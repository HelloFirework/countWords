Orthogonal Decomposition Network for Pixel-wise Binary Classiﬁcation

Chang Liu†, Fang Wan†, Wei Ke†,

Zhuowei Xiao†‡, Yuan Yao†, Xiaosong Zhang† and Qixiang Ye†§∗

†University of Chinese Academy of Sciences, Beijing, China

‡Institute of Geology and Geophysics, Chinese Academy of Sciences

§Peng Cheng Laboratory, Shenzhen, Guangdong, China

{liuchang615,wanfang13,kewei11,yaoyuan17,zhangxiaosong18}@mails.ucas.ac.cn

xiaozhuowei@mail.iggcas.ac.cn, qxye@ucas.ac.cn

Abstract

The weight sharing scheme and spatial pooling oper-
ations in Convolutional Neural Networks (CNNs) intro-
duce semantic correlation to neighboring pixels on fea-
ture maps and therefore deteriorate their pixel-wise clas-
siﬁcation performance.
In this paper, we implement an
Orthogonal Decomposition Unit (ODU) that transforms a
convolutional feature map into orthogonal bases targeting
at de-correlating neighboring pixels on convolutional fea-
tures. In theory, complete orthogonal decomposition pro-
duces orthogonal bases which can perfectly reconstruct any
binary mask (ground-truth).
In practice, we further de-
sign incomplete orthogonal decomposition focusing on de-
correlating local patches which balances the reconstruction
performance and computational cost. Fully Convolutional
Networks (FCNs) implemented with ODUs, referred to as
Orthogonal Decomposition Networks (ODNs), learn de-
correlated and complementary convolutional features and
fuse such features in a pixel-wise selective manner. Over
pixel-wise binary classiﬁcation tasks for two-dimensional
image processing, speciﬁcally skeleton detection, edge de-
tection, and saliency detection, and one-dimensional key-
point detection, speciﬁcally S-wave arrival time detection
for earthquake localization, ODNs consistently improves
the state-of-the-arts with signiﬁcant margins.

1. Introduction

Pixel-wise binary classiﬁcation tasks, e.g., skeleton de-
tection, edge detection, and saliency detection, are funda-
mentally important for computer vision and pattern recog-
nition. Skeleton is one of the most representative visual
properties, describing objects with compact but informative

∗Corresponding author.

Figure 1: Orthogonal Decomposition Unit (ODU) trans-
forms a convolutional feature map to orthogonal bases,
which can be used to perfectly reconstruct complex binary
masks (ground-truth) with convolutional reconstruction.

curves. Such curves constitute a continuous decomposition
of object shapes [25], providing valuable cues for both ob-
ject representation and recognition. Edge can be converted
into descriptive features and spatial constraints, which en-
force object grouping [38], semantic segmentation [21], and
object localization [13]. Saliency represents the most con-
spicuous and attractive regions in an image, and saliency
detection serves as the ﬁrst step to a variety of computer
vision applications [8].

Pixel-wise binary classiﬁcation tasks have the common
goal about predicting a mask of interest given a color input
image. In the deep learning era, the fully convolutional neu-
ral networks (FCNs) [27] have been widely applied to solve
pixel-wise classiﬁcation problems due to their end-to-end
training manner and ﬂexibility to the image size. Recent
FCN-based approaches, e.g., holistically-nested edge de-

6064

10.90.10.6100000.900000.10*10/60010000.6Ground-truthInputConvolutional ReconstructionOrthogonal Decomposition1001enhance pixel-wise positivesuppress pixel-wise negative1010Outputtection (HED) and side-output residual network (SRN) [11]
root in multi-layer feature fusion with the motivation that
low-level features focus on details while high-level features
are rich in semantics [11]. Linear Span Network (LSN) [19]
uses linear span theory to de-correlate the convolutional fea-
ture channels and increase their independence.

The multi-layer feature fusion and channel-level de-
correlation have been extensively explored. However, pixel-
level de-correlation remains unsolved. The backbone CNNs
used in pixel-wise binary classiﬁcation approaches are usu-
ally pre-trained on image classiﬁcation and have pixel-wise
semantic correlation which caused by the weight sharing
scheme and pooling operations notably deteriorates the net-
work’s capability about pixel-wise classiﬁcation.

In this paper, we propose an Orthogonal Decomposition
Unit (ODU) which transforms a convolutional feature map
into orthogonal channels, Fig. 1, and targets at alleviating
semantic correlation of neighboring pixels on convolutional
the feature map. The ODU is based on a mathematical prin-
ciple that any vector, e.g., a binary ground-truth mask, can
be perfectly reconstructed by a set of complete orthogonal
bases (orthogonal feature channels). When fusing the de-
composed feature maps with a convolutional operation, the
convolutional weights can be solved independently, which
endows ODU the capability of pixel-wise reﬁnement over
feature maps. Considering that neighboring pixels suffer
semantic correlation more critical, we utilize incomplete or-
thogonal decomposition on local feature patches. Besides
the de-correlation of local patches, incomplete orthogonal
decomposition also decreases the computational cost com-
pared with complete orthogonal decomposition.

The ODU can be added atop the output layer of FCNs
and update them to Orthogonal Decomposition Networks
(ODNs) which facilitate learning de-correlated and comple-
mentary features. In contrast to existing FCNs, e.g., SRN
[11], LSN [19], and deeply supervised short-connections
(DSS) [8] that focus on learning channel-level comple-
mentary features, ODNs pursue pixel-level spatial de-
correlation and feature complementarity in a more effective
way.

The contributions of this paper include:

• A plug-and-play module named Orthogonal Decom-
position Unit (ODU) is designed to partition neighbor-
ing pixels on a feature map into orthogonal channels
that targets to alleviate the fundamental drawback of
CNNs about pixel-level semantic correlation.

• With ODU plugged atop the output layer, success-
ful fully convolutional networks, including VGG [33],
HED [39], and SRN [11], are upgraded to Orthogo-
nal Decomposition Networks (ODNs), which facilitate
learning de-correlated and complementary features.

• With negligible computation cost, ODNs consistently

improves the state-of-the-arts with signiﬁcant mar-
gins on pixel-wise binary classiﬁcation tasks for two-
dimensional image processing, speciﬁcally skeleton
detection, edge detection, and saliency detection, and
one-dimensional keypoint detection, speciﬁcally S-
wave arrival time detection for earthquake localization.

2. Related Work

In this section, we review approaches about pixel-wise
binary classiﬁcation and methods about learning comple-
mentary and/or de-correlated features with CNNs.

Pixel-wise binary classiﬁcation. The early pixel-wise
binary classiﬁcation approaches rooted in hand-crafted im-
age processing methods [3, 12, 18, 24], which used morpho-
logical operations to localize pixels of interest, e.g., edge
or skeleton pixels. Recently, learning based methods were
proposed for pixel-wise binary classiﬁcation. The multiple
instance learning method [36] was used to learn a true skele-
ton pixel from a bag of pixels. The structured random for-
est [35] and subspace multiple instance learning [28] were
employed to perform skeleton detection and localization.

With the rise of deep learning, researchers have formu-
lated pixel-wise binary classiﬁcation as an image-to-mask
mapping problem and focusing on fusing the multi-layer
convolutional features in an end-to-end manner.

HED [39] introduced deeply supervised side-output net-
work to learn a pixel-wise classiﬁer for edge detection.
Multiscale deep features (MSD) [14] extracted and fused
features from multiple convolutional layers for saliency de-
tection, and leverages image segmentation to further boost
the performance. Fusing scale-associated deep side-outputs
(FSDS) [31] learned multi-scale skeleton representation
given scale-associated ground-truth. SRN [11] and DSS [8]
leveraged the side-output residual units to ﬁt the errors be-
tween the object symmetry/skeleton ground-truth and the
side-outputs of multiple convolutional layers. To improve
the pixel-wise classiﬁcation accuracy, a segment-wise spa-
tial pooling were proposed for saliency detection [16].

It has been extensively explored that how to fuse multi-
layer convolutional features to predict binary mask. Never-
theless, researchers barely investigate de-correlating neigh-
boring pixels in a convolutional layer. With strong semantic
correlation, CNN’s capability for pixel-wise classiﬁcation
remains limited.

Convolutional feature de-correlation. From a broad
view of convolutional feature de-correlation, canonical cor-
relation analysis (CCA) [4, 34] and singular vector decom-
position (SVD) [34] were implemented into CNNs with
specially designed de-correlation layers. These approaches
validated the feasibility of using a single de-correlation
layer to push the network learning complementary features.
For object saliency detection, a super-pixel approach was
combined with FCNs to reduce the correlation of deep pix-

6065

Figure 2: ODU eases the semantic correlation among neighbouring pixels by decomposing the input map to orthogonal
channels, which are further fused in the convolutional reconstruction to perform pixel-wise reﬁnement.

els [7]. For multiple pixel-wise classiﬁcation tasks, SRN
[11] and DSS [8] used residual models to “force” the side-
output features from different convolutional layers to be
complementary. A recent research, linear span network
(LSN) [19] linked the multi-layer convolutional features
with the linear span theory to reduce the correlation and in-
crease the complementary of multi-channel features. How-
ever, the pixel-wise de-correlation problem remains un-
solved, despite of the effort on sub-pixel operations [32].

3. Orthogonal Decomposition Unit

An Orthogonal Decomposition Unit (ODU) is made up
of two components of orthogonal decomposition and con-
volutional reconstruction, Fig. 2. Orthogonal decomposi-
tion is utilized to decompose an input map to a set of or-
thogonal channels, which have the same size to the input
map. Convolutional reconstruction is then utilized to fuse
the orthogonal channels to an output map with pixel-wise
reﬁnement.

3.1. Orthogonal Decomposition for Spatial De 

correlation

Formally, we denote an m × n input map as A = (aij),
where i = 1, 2, · · · m and j = 1, 2, · · · n. aij is the pixel
at i-th row and j-th column on the map. The input map A
is transformed to an m × n × mn map with complete or-
thogonal decomposition, denoted as D, on the left of Fig 2.
Supposing the k-th channel of D as D(k) = (d(k)
ij ), where
k = 1, 2, · · · , mn, the pixel-wise correspondence between
A and D is deﬁned as

(d(k)

ij = aij,
d(k)
ij = 0,

if k = (i − 1) × n + j

else

.

(1)

According to Eq. 1, it can be easily concluded that complete
orthogonal decomposition has two elegant properties.

Property 1: sparsity. Neighboring pixels in the input
map are decomposed to different channels with sparse non-
zero elements, i.e., a channel has at most one non-zero
pixel. The element-wise sum of the orthogonal channels
is equivalent to the input map, as

mn

A =

D(k).

(2)

Xk=1

Property 2: orthogonality. As the locations of non-zero
pixels from any two channels are different with each other,
any two channels of the output map are orthogonal to each
other, as

D(i) · D(j) = 0, 1 ≤ i 6= j ≤ mn.

(3)

3.2. Convolutional Reconstruction for Pixel wise

Reﬁnement

In convolutional reconstruction, a 1 × 1 convolutional
layer is inserted atop the orthogonal decomposition layer
to ﬁt the ground-truth. Denoting the m × n output map
and binary ground-truth as B = (bij) and G = (gij), the
convolutional reconstruction is formulated as

mn

λkD(k) = B ≈ G, k = 1, 2, · · · , mn,

(4)

Xk=1

where k = 1, 2, · · · , mn and λk denotes the convoltuional
reconstruction coefﬁcient to be learned. Eq. 4 can be solved
as

λ(i−1)n+j = gij/aij, aij 6= 0,

(5)

which implies that with back-propagation each channel is
assigned an independent weight corresponding to the single
non-zero pixel. In this way, we can operate each pixel on
the input map in the convolutional reconstruction for pixel-
wise reﬁnement. As the condition aij 6= 0 can be easily
satisﬁed in the back-propagation process, ODU can com-
pletely reconstruct an arbitrary given binary mask with the
orthogonal features.

6066

ODConvConvolutional ReconstructionOrthogonal Decomposition𝑎𝑖𝑗𝜆i−1n+j𝜆1𝜆𝑚𝑛𝐴𝑚×𝑛𝐵𝑚×𝑛𝐷(𝑚𝑛)𝐷(1)𝐷(i−1n+𝑗)b𝑖𝑗𝐷𝑚×𝑛×𝑚𝑛(orthonormal bases)Orthogonal Decomposition UnitInput mapOutput map𝑎𝑖𝑗same size and perform the complete orthogonal decompo-
sition deﬁned in Eq. 1 on all patches. With incomplete or-
thogonal decomposition, the pixel correspondence between
the input map and the channels of the output feature map is
deﬁned as

(d(k)

ij = aij,
d(k)
ij = 0,

if k = (i

′

− 1) × pw + j

else

′

,

(6)

′

where aij and dij are deﬁned in Eq. 1, i
j
m, 1 ≤ j ≤ n.

≡ j(modpw), 1 ≤ i

≤ ph, 1 ≤ j

′

′

′

≡ i(modph),
≤ pw, 1 ≤ i ≤

Figure 3: The orthogonal coordinate system generated from
the input map by ODU can span the whole space.

3.3. Geometric Interpretation

To understand ODU, we visualize a two dimensional ex-
ample, Fig. 3. A single channel input map can be repre-
sented as a vector α = (a1, a2) in the plane, Fig. 3(1).
According to Eq. 1, vector α is decomposed into two or-
thogonal vectors, i.e., α1 = (a1, 0) and α2 = (0, a2), in the
orthogonal decomposition. These two orthogonal vectors
establish an orthogonal coordinate system, Fig. 3(2).

As shown in Fig. 3(2), α1 and α2 can span the whole
plane, where any vector can be perfectly reconstructed by
α1 and α2. In convolutional reconstruction, the recon-
struction coefﬁcients can be directly computed by the coor-
dinates of β, i.e., a ground-truth mask. According to Eq. 5,
α1, and α2 are transformed to β1 and β2, which are equiv-
alent to the orthogonal projection of β to α1 and α2, Fig.
3(3). β is then reconstructed as β = β1 + β2, Fig. 3(4).
Therefore, ODU can reconstruct β by independently ﬁt-
ting β1 and β2 in two orthogonal directions, which ensures
the capability of pixel-wise reﬁnement through spacial de-
correlation of the input map.

4. Orthogonal Decomposition Network

Considering that neighboring pixels suffer semantic cor-
relation most, we introduce ODU with incomplete orthog-
onal decomposition (IOD) which focuses on de-correlating
the neighboring pixels within local patches. With IOD, we
build Orthogonal Decomposition Network (ODNs) using
single-ODU or multi-ODU to approximate the sparsity and
orthogonality of features.

4.1. Incomplete Orthogonal Decomposition

The incomplete orthogonal decomposition, Fig. 4(a), is
proposed to perform spacial de-correlation on local patches.
Speciﬁcally, we use a pw × ph non-overlap densely sliding
window to partition an input map into local patches with the

The incomplete orthogonal decomposition approximates
the properties of sparsity and orthogonality deﬁned in Eq. 2
and Eq. 3 while reducing the channels of the output feature
map to pw · ph. Contrarily, according to Eq. 1, an m × n
input map requires mn channels to implement the complete
orthogonal decomposition. When m and n are large, there
is a curse of dimensionality.

According to Eq. 5, the essence of ODU with complete
orthogonal decomposition is reﬁning each pixel with an in-
dependent weight.
In incomplete orthogonal decomposi-
tion, there is weight sharing that non-zero pixels in the same
channel share the reconstruction coefﬁcient. Therefore, we
appropriately use ﬁlter size larger than the size of orthogo-
nal decomposition patch in the convolutional reconstruction
to ease the weight sharing.

4.2. ODN Exemplars

By integrating the ODU with incomplete orthogonal de-
composition, we update FCNs including VGG [33], HED
[39] and SRNs [11] to OD-VGG, OD-HED, and OD-SRN,
as shown in Fig. 4(b). For OD-VGG, a single ODU is added
atop the last convolutional layer of VGG-16. For OD-HED
and OD-SRN, six ODUs are inserted in the ﬁve side-output
branches and one fusion branch. HED is an effective archi-
tecture for multi-scale convolutional feature fusion, while
SRN updated the fusion strategy by introducing residual
modules between the adjacent side-output branches. With
these ODN exemplars, we target at validating the general
applicability of ODUs to pixel-wise reﬁnement in popular
FCN architectures.

Single-ODU: spatial de-correlation. It is known that
neighboring pixels on CNN feature maps have strong se-
mantic correlation for the weight sharing scheme and spatial
pooling operations. Such correlation causes falsely classify-
ing background pixels near a true positive to false positives,
and vice versa. With the sparsity (Property 1) of orthog-
onal decomposition, convolutional reconstruction can inde-
pendently pinpoint the label of each pixel. In other words, a
single ODU implement pixel-wise reﬁnement through spa-
tial de-correlation.

Multi-ODU: complementary feature learning. State-
of-the-art approaches usually perform channel-wise feature

6067

Establish orthogonal coordinate system𝛼=(𝑎1,𝑎2)inputplane𝛼1=(𝑎1,0)𝛼2=(0 , 𝑎2)Fittheorthogonal projection of GTβ1=𝑏1𝑎1𝛼1=(𝑏1,0)β2=𝑏2𝑎2𝑎2=(0 , 𝑏2)β=β1+β2=(𝑏1,𝑏2)ground-truth1234Linear reconstruct  GTOrthogonal DecompositionConvolutionalReconstructionFigure 4: (a) Incomplete orthogonal decomposition with an 3 × 5 input map and 2 × 2 patch size. (b) With ODUs, plug-and-
play modules added atop the output layers, VGG-16 [33], HED [39] and SRN [11] are updated to ODNs.

integration, but fail to differentiate positive and negative
pixels in the same channels, which inevitably introduces
noise to the ﬁnal outputs. With multiple ODUs, the orthog-
onality (Property 2) drives the network to learn comple-
mentary features in the linear span view [19]. Meanwhile,
the sparsity (Property 1) make the feature fusion proce-
dure more selective, i.e., different stages enhance/suppress
pixels in different areas. In this way, the potential of the
backbone network to learn complementary feature is fur-
ther explored which further promote the ODN’s capability
of pixel-wise reﬁnement.

5. Experiments

In this section, the experimental settings are ﬁrst intro-
duced. The effects of ODU on feature de-correlation and
pixel-level reﬁnement are then validated. Finally, the per-
formance of ODNs is reported on pixel-wise binary classiﬁ-
cation tasks for two-dimensional image processing, speciﬁ-
cally skeleton detection, edge detection, and saliency detec-
tion, and one-dimensional keypoint detection, speciﬁcally
S-wave arrival time detection for earthquake localization.

All the experiments run on a Tesla K40 GPU. The mini-
batch size is set to 1, the loss-weight to 1 for each output
layer, the momentum to 0.9, the weight decay to 0.002, and
the initial learning rate to 1e-6, which decreases one mag-
nitude for every 10,000 iterations.

5.1. ODU Effect

From left to right in each row of Fig. 5, it can be seen that
the output maps of OD-HED are reﬁned and much back-
ground noise is suppressed. Particularly, the zigzag noise
caused by up-sampling is eased and the fused output (the
last row of OD-HED) is closer to the ground-truth. By com-
paring the outputs of all ﬁve stages of HED and OD-HED
in Fig. 5 from top to down, we conclude that the output
maps of OD-HED are more complementary than those of
HED. In shallow stages of OD-HED, the ﬁne details are

Figure 5: With ODU, each input feature map (ﬁrst col-
umn) is decomposed into orthogonal channels (middle col-
umn) that facilitate de-correlated and complementary fea-
ture learning and pixel-wise reﬁnement (last column).

richer than that of HED. The torso of the horse is suppressed
while the slim parts including the tail and legs are enhanced.
In deep stages of OD-HED, the torso of the horse is en-
hance while slim parts are suppressed. These validate that
ODUs can drive multi-layer learning complementary con-
volutional features. With t-SNE analysis, it’s illustrated that
with the features learned by OD-HED, the background and
foreground pixels are more separable, validating the effec-
tiveness of ODUs for semantic de-correlation, Fig. 6.

We give quantitative comparison of patch sizes and
convolutional reconstruction ﬁlter size of incomplete or-
thogonal decomposition on SK-LARGE skeleton detection
dataset, Table 1 and Table 2.
In Table 1, it can be seen

6068

OD-SRNVGG-16dataVGG-16OD-HEDVGG-16dataOrthogonal Decomposition UnitLoss LayerVGG-16OD-VGGVGG-16dataIncomplete Orthogonal Decompositionstage1stage2stage3stage4stage5fusionfusionConvolutional LayerIODPatch(𝑝𝑤= 𝑝ℎ= 2) 3 x 53 x 5 x 4(a)(b)side-output1HEDdataGTOD-HEDorthogonal decompositionof fusion-outputside-output2side-output3side-output4side-output5fusion-output(𝑝𝑤= 𝑝ℎ= 5)Figure 6:
t-SNE of foreground/background pixels shows
that the features of OD-HED incorporate less semantic cor-
relation than those of HED. (Best viewed in color)

Table 1: Comparison of patch sizes of incomplete orthogo-
nal decomposition on SK-LARGE with 11 × 11 ﬁlter size.

Patch size
F-measure

w/o
0.489

3 × 3
0.604

5 × 5
0.606

7 × 7
0.600

Table 2: Comparison of ﬁlter sizes for convolutional recon-
struction on SK-LARGE with 5 × 5 patch size.

Filter size
F-measure

3×3
0.511

7×7
0.575

11×11
0.606

15×15
0.620

17×17
0.619

Table 3: Skeleton detection on SKLARGE.

Methods

F-measure ∆ F-measure

VGG-16
OD-VGG
HED [39]
OD-HED
SRN [11]
OD-SRN

0.489
0.620
0.495
0.644
0.655
0.676

0.131

0.149

0.021

that the F-measure signiﬁcantly increases after applying
the ODU. Compared with VGG-16 without ODU, VGG-16
with incomplete orthogonal decomposition patch size 3 × 3,
achieve 21.5% (0.489 vs. 0.604) performance gain. As se-
mantic correlation mainly exits within local patches, the F-
measure increases to 0.606 using patch size 5 × 5, but stops
increase when the size becomes larger. In all the following
experiments, we use the patch size 5 × 5. In Table 2, we
evaluate reconstruction convolutional ﬁlters under different
sizes. From 3 × 3 to 15 × 15, the performance keeps in-
creasing, but drops a little at 17 × 17. The reason for this
phenomenon is that larger ﬁlter eases the weight sharing
of non-zero pixels of the incomplete decomposed orthog-
onal features while aggravates the weight learning burden.
Therefore the 15 × 15 convolutional ﬁlter is selected for re-
construction.

5.2. Skeleton Detection

Five skeleton detection datasets, including SYMMAX
[36], WH-SYMMAX [28], SK-SMALL [31], SK-LARGE

Figure 7: Pixel-wise reﬁnement of skeleton detection ex-
amples by ODNs. It can be seen that with ODUs inserted,
zigzag noise and blur are suppressed and ﬁne details of ob-
ject skeletons are detected.

[30], and Sym-PASCAL [11] are used to evaluate ODNs.
SYMMAX contains 200/100 training/test images. SK-
SMALL involves skeletons about 16 classes of objects with
300/206 training/test images. Based on SK-SMALL, SK-
LARGE is extended to 746/745 training/test images. Sym-
PASCAL is derived from the PASCAL-VOC-2011 segmen-
tation dataset [5] which contains 14 object classes with
648/787 images for training and test. By changing thresh-
old values on output masks we get multiple predicted bi-
nary skeleton masks, which is compared with the ground-
truth pixel-by-pixel to compute precision (P) and recall
(R). The F-measure is used to evaluate the performance
of the different detection approaches, which is calculated
with the optimal threshold values over the whole dataset as
F = 2P · R/(P + R).

The performance of three ODNs including OD-VGG,
OD-HED, and OD-SRN and the comparisons with the base-
line networks are shown in Table 3. It can be seen that OD-
VGG outperforms VGG by 13.1%, OD-HED outperforms
HED by 14.9%. The reason for the large performance gain
lies in that a single ODU in VGG, Fig. 4(b), effectively de-
correlates convolutional features while multiple ODUs in
HED, Fig. 4(b), drive learning multi-layer complementary
features. With the capability of pixel-wise label reﬁnement,

6069

HEDOD-HEDVGGOD-VGGHEDOD-HEDSRNOD-SRNGTDataTable 4: Performance comparison of state-of-the-art approaches on ﬁve commonly used skeleton detection datasets.

Lindeberg [18] MIL [36] HED [39]

FSDS [31] LSN [19]

SRN [11] OD-SRN (ours)

WH-SYMMAX [28]
SK-SMALL [31]
SYMMAX [36]
SK-LARGE [30]
Sym-PASCAL [11]

0.277
0.227
0.360
0.270
0.138

0.365
0.392
0.362
0.293
0.174

0.743
0.542
0.427
0.495
0.369

0.769
0.623
0.467

–

0.418

0.797
0.633
0.480
0.668
0.425

0.780
0.609
0.446
0.655
0.443

0.804
0.624
0.489
0.676
0.444

Table 5: Performance comparison on the BSDS500 edge
detection dataset.

Methods

ODS

OIS

AP

DC [29]
HED [39]
SRN [11]
LSN [19]
OD-SRN(ours)
Human

0.757
0.780
0.782
0.790
0.798
0.800

0.776
0.797
0.800
0.806
0.814
0.800

0.790
0.814
0.779
0.618
0.782

–

OD-SRN outperforms SRN that uses residual modules to
learn complementary features, aggregating the performance
by 2.1%. The pair-wise comparison of skeleton detection
results is illustrated in Fig. 7.

On the ﬁve commonly used skeleton detection datasets,
Table 4, the OD-SRN respectively outperforms the baseline
SRN 2.4%, 1.5%, 4.3%, 2.1%, and 0.1% and beats the state-
of-the-art approaches.

5.3. Edge Detection

Edge detection is another typical pixel-wise binary clas-
siﬁcation task. The BSDS500 [2] dataset that is composed
of 200 training images, 100 validation images, and 200 test-
ing images is used to evaluated the ODN. In BSDS 500,
each edge mask is manually annotated by ﬁve persons on
average. For training images, we preserve their positive
labels annotated by at least three human annotators. The
F-measures when choosing an optimal scale for the entire
dataset (ODS) or per image (OIS), and the average preci-
sion (AP) are used as the evaluation metrics.

As shown in Table 5, the DeepContour (DC) [29] sets a
solid baseline. The HED [39] approach that fuses multi-
scale convolutional features reports higher performance
with ODS=0.780. The state-of-the-art SRN [11] achieves
ODS=0.782, and the LSN [19] achieves ODS=0.790. OD-
SRN achieves the best performance, ODS=0.798 and OIS
=0.814, which is even comparable to human performance.

5.4. Saliency Detection

We evaluate OD-SRN on ﬁve object saliency detection
datasets, including MSRA-B [20], ECSSD [40], HKU-IS
[15], PASCALS [17], SOD [22] [23]. MSRA-B contains

5,000 images with single objects. ECSSD contains 1,000
images with complex backgrounds. HKU-IS contains 4000
images for multi-object saliency. PASCALS contains 850
images. SOD is a subset of the BSDS dataset and contains
300 images, most of which has more than one salient ob-
jects. We train OD-SRN with the MSRA-B dataset and test
the model on all ﬁve datasets. F-measure and the mean ab-
solute error (MAE) are used as performance metrics [9].

In Table 6, OD-SRN consistently outperforms the base-
line SRN on the ﬁve datasets. The advantage of our ap-
proach lies in pixel-wise label reﬁnement, so it mainly ag-
gregates the saliency detection results at object boundaries,
as shown in Fig. 8. For holistic saliency regions, the perfor-
mance improvement is moderate.

5.5. One dimensional Keypoint Detection: S wave

Arrival Time Detection

Task and dataset. As a general approach for pixel-wise
binary classiﬁcation, ODN is extended to one-dimensional
keypoint detection, i.e., detecting S-wave arrival times of
an earthquake in enormous seismograms, Fig. 9. These
arrivals are keypoints of time when S-wave reaches seis-
mometers, which are essential for accurate earthquake lo-
calization [6] and earth interior imaging [10] [37]. Such
arrivals are usually annotated by experts with great efforts
[1] [26]. The training dataset includes 20,000 S-wave pick-
waveform pairs provided by human experts1. The testing
dataset includes 1512 records from 782 stations of 97 earth-
quakes in Japan. Each record is discretized as a 1D vector
with 1200 points and two channels corresponding to a radial
component R and a transverse component T , Fig. 9.

Performance and analysis. The S-wave arrival time de-
tection performance is evaluated by calculating the propor-
tions of samples under different time deviation between the
prediction and the ground-truth, Table 7. It can be seen that
OD-SRN outperforms the SRN baseline, i.e., with higher
proportion under the same time deviation. Specially, with
deviation ≤ 0.2s, OD-SRN achieves 76.7% accuracy, which
signiﬁcantly outperforms SRN by 4.5%. In Fig. 9, SRN re-
ports a false positive (red dotted box), while OD-SRN can
precisely detect the arrival time of the S-wave by predicting

1Research Center for Prediction of Earthquakes and Volcanic Erup-

tions, Tohoku University and Hi-net

6070

Table 6: Performance comparison of the state-of-the-art approaches on commonly used saliency detection datasets. (Smaller
MAE indicates better performance.)

Methods

SRN [11]

OD-SRN (ours)

MSRA-B [20]

ECSSD [40]

HKU-IS [15]

PASCALS [17]

SOD [22] [23]

Fβ

0.888

0.899

MAE

0.063

0.058

Fβ

0.872

0.883

MAE

0.084

0.078

Fβ

0.871

0.882

MAE

0.065

0.060

Fβ

0.771

0.786

MAE

0.129

0.121

Fβ

0.803

0.815

MAE

0.132

0.129

Table 7: Performance comparison on S-wave arrival time detection (sample proportions under time deviation).

Deviation(s)
SRN [11]
OD-SRN(ours)

0.00
0.030
0.035

0.01
0.065
0.072

0.02
0.063
0.079

0.03
0.073
0.053

0.04
0.051
0.058

0.05
0.057
0.054

0.06-0.10

0.11-0.20

0.192
0.211

0.190
0.204

≤ 0.20
0.722
0.767

Figure 8: Pixel-wise reﬁnement of saliency by OD-SRN.

Figure 9: An S-wave arrival time example is detected by
SRN and OD-SRN. A false positive (red dotted box) pre-
dicted by SRN is suppressed by OD-SRN.

the maximum keypoint, which shows the pixel-level reﬁne-
ment capability of OD-SRN.

Fig. 10 shows S-wave arrival time detection examples
predicted by OD-SRN (red lines) which achieves close per-
formance to the ground-truth ( blue lines) annotated by hu-
man experts.

6. Conclusion

We proposed the Orthogonal Decomposition Unit
(ODU) targeting at de-correlating neighboring pixels on
convolutional features. In theory, the complete orthogonal

Figure 10: S-wave arrival times in an earthquake detected
by OD-SRN . Red lines are arrivals detected by OD-SRN
while blue ones are ground-truth. (Best viewed in color)

decomposition produced orthogonal bases that can perfectly
reconstruct any binary mask (ground-truth). In practice, in-
complete orthogonal decomposition with proper patch sizes
can effectively and efﬁciently approximate the complete or-
thogonal decomposition. We updated successful FCNs in-
cluding VGG-16, HED, and SRN to Orthogonal Decompo-
sition Networks (ODNs) and applied them on typical pixel-
wise binary classiﬁcation tasks including skeleton, edge,
and saliency detection to validate the generality of ODUs
for pixel-level spacial de-correlation and pixel-wise reﬁne-
ment. The extension of ODN to 1D keypoint detection, i.e.,
S-wave arrival time detection for earthquake localization
provides fresh insight about the application of deep learn-
ing in the area of geoscience.

Acknowledgments. The authors are very grateful to the
support by NSFC grant 61836012 and 61671427, and Bei-
jing Municipal Science and Technology Commission grant
Z181100008918014.

6071

0246810121416time(s)0246810121416SRNOD-SRNRTS-wave0246810121416GT02468101214160246024601time(s)0246810121416EventTime:2016-1-1520:5:11.02Latitude:35.162Longitude:137.064 Depth: 14.3 km mag: 3.1References

[1] Jubran Akram and David W Eaton. A review and appraisal
of arrival-time picking methods for downhole microseismic
dataarrival-time picking methods. Geophysics, 81(2):KS71–
KS91, 2016.

[2] Pablo Arbelaez, Michael Maire, Charless C. Fowlkes, and
Jitendra Malik. Contour detection and hierarchical image
segmentation.
IEEE Trans. Pattern Anal. Mach. Intell.,
33(5):898–916, 2011.

[3] John F. Canny. A computational approach to edge detec-
tion. IEEE Trans. Pattern Anal. Mach. Intell., 8(6):679–698,
1986.

[4] Xiaobin Chang, Tao Xiang, and Timothy M. Hospedales.
In
Scalable and effective deep cca via soft decorrelation.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1488–1497, 2018.

[5] Mark Everingham, Luc J. Van Gool, Christopher K. I.
Williams, John M. Winn, and Andrew Zisserman. The pascal
visual object classes (VOC) challenge. International Journal
of Computer Vision, 88(2):303–338, 2010.

[6] Jeanne L Hardebeck and Peter M Shearer. A new method for
determining ﬁrst-motion focal mechanisms. Bulletin of the
Seismological Society of America, 92(6):2264–2276, 2002.

[7] Shengfeng He, Rynson W. H. Lau, Wenxi Liu, Zhe Huang,
and Qingxiong Yang. Supercnn: A superpixelwise convolu-
tional neural network for salient object detection. Interna-
tional Journal of Computer Vision, 115(3):330–344, 2015.

[8] Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji,
Zhuowen Tu, and Philip Torr. Deeply supervised salient ob-
ject detection with short connections. In IEEE CVPR, pages
3203–3212, 2017.

[9] Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji,
Zhuowen Tu, and Philip Torr. Deeply supervised salient ob-
ject detection with short connections. In 2017 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 5300–5309. IEEE, 2017.

[10] Chengxin Jiang, Brandon Schmandt, Steven M Hansen,
Sara L Dougherty, Robert W Clayton, Jamie Farrell, and
Fan-Chi Lin. Rayleigh and s wave tomography constraints
on subduction termination and lithospheric foundering in
central california. Earth and Planetary Science Letters,
488:14–26, 2018.

[11] Wei Ke, Jie Chen, Jianbin Jiao, Guoying Zhao, and Qixiang
Ye. SRN: side-output residual network for object symme-
try detection in the wild. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 302–310, 2017.

[12] Louisa Lam, Seong-Whan Lee, and Ching Y. Suen. Thin-
ning methodologies - A comprehensive survey. IEEE Trans.
Pattern Anal. Mach. Intell., 14(9):869–885, 1992.

[13] Tom Sie Ho Lee, Sanja Fidler, and Sven J. Dickinson. Learn-
ing to combine mid-level cues for object proposal genera-
tion. In IEEE International Conference on Computer Vision,
pages 1680–1688, 2015.

[14] Guanbin Li and Yizhou Yu. Visual saliency based on multi-
scale deep features. In IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2015, Boston, MA, USA,
June 7-12, 2015, pages 5455–5463, 2015.

[15] Guanbin Li and Yizhou Yu. Visual saliency based on multi-
scale deep features. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 5455–
5463, 2015.

[16] Guanbin Li and Yizhou Yu. Deep contrast learning for salient
object detection. In IEEE Conference on Computer Vision
and Pattern Recognition, CVPR, pages 478–487, 2016.

[17] Yin Li, Xiaodi Hou, Christof Koch, James M Rehg, and
Alan L Yuille. The secrets of salient object segmentation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 280–287, 2014.

[18] Tony Lindeberg. Edge detection and ridge detection with
automatic scale selection. International Journal of Computer
Vision, 30(2):117–156, 1998.

[19] Chang Liu, Wei Ke, Fei Qin, and Qixiang Ye. Linear span
network for object skeleton detection. In European Confer-
ence on Computer Vision, pages 133–148, 2018.

[20] Tie Liu, Zejian Yuan, Jian Sun, Jingdong Wang, Nanning
Zheng, Xiaoou Tang, and Heung-Yeung Shum. Learning to
detect a salient object. IEEE Transactions on Pattern analy-
sis and machine intelligence, 33(2):353–367, 2011.

[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 3431–3440, 2015.

[22] David Martin, Charless Fowlkes, Doron Tal, and Jitendra
Malik. A database of human segmented natural images and
its application to evaluating segmentation algorithms and
measuring ecological statistics. In Computer Vision, 2001.
ICCV 2001. Proceedings. Eighth IEEE International Con-
ference on, volume 2, pages 416–423. IEEE, 2001.

[23] Vida Movahedi and James H Elder. Design and percep-
tual validation of performance measures for salient object
segmentation. In Computer Vision and Pattern Recognition
Workshops (CVPRW), 2010 IEEE Computer Society Confer-
ence on, pages 49–56. IEEE, 2010.

[24] Punam K. Saha, Gunilla Borgefors, and Gabriella Sanniti di
Baja. A survey on skeletonization algorithms and their ap-
plications. Pattern Recognition Letters, 76:3–12, 2016.

[25] Thomas B. Sebastian, Philip N. Klein, and Benjamin B.
Kimia. Recognition of shapes by editing their shock graphs.
IEEE Trans. Pattern Anal. Mach. Intell., 26(5):550–571,
2004.

[26] BK Sharma, Amod Kumar, and VM Murthy. Evaluation of
seismic events detection algorithms. Journal of the Geologi-
cal Society of India, 75(3):533–538, 2010.

[27] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully
IEEE

convolutional networks for semantic segmentation.
Trans. Pattern Anal. Mach. Intell., 39(4):640–651, 2017.

[28] Wei Shen, Xiang Bai, Zihao Hu, and Zhijiang Zhang. Multi-
ple instance subspace learning via partial random projection
tree for local reﬂection symmetry in natural images. Pattern
Recognition, 52:306–316, 2016.

[29] Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, and Zhi-
jiang Zhang. Deepcontour: A deep convolutional feature
learned by positive-sharing loss for contour detection.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR, pages 3982–3991, 2015.

6072

[30] Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Xiang Bai,
and Alan L. Yuille. Deepskeleton: Learning multi-task
scale-associated deep side outputs for object skeleton ex-
traction in natural images. IEEE Trans. Image Processing,
26(11):5298–5311, 2017.

[31] Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Zhijiang Zhang,
and Xiang Bai. Object skeleton extraction in natural images
by fusing scale-associated deep side outputs. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
222–230, 2016.

[32] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,
Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1874–1883, 2016.

[33] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In IEEE
International Conference on Computer Vision, 2015.

[34] Yifan Sun, Liang Zheng, Weijian Deng, and Shengjin Wang.
Svdnet for pedestrian retrieval. In IEEE International Con-
ference on Computer Vision, ICCV, pages 3820–3828, 2017.
[35] Ching Lik Teo, Cornelia Ferm¨uller, and Yiannis Aloimonos.
Detection and segmentation of 2d curved reﬂection symmet-
ric structures.
In IEEE International Conference on Com-
puter Vision, ICCV, pages 1644–1652, 2015.

[36] Stavros Tsogkas and Iasonas Kokkinos. Learning-based
In European Con-

symmetry detection in natural images.
ference on Computer Vision (ECCV), pages 41–54, 2012.

[37] Zewei Wang, Dapeng Zhao, Xin Liu, Chuanxu Chen, and
Xibing Li. P and s wave attenuation tomography of the japan
subduction zone. Geochemistry, Geophysics, Geosystems,
18(4):1688–1710, 2017.

[38] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser
Sheikh. Convolutional pose machines. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 4724–
4732, 2016.

[39] Saining Xie and Zhuowen Tu. Holistically-nested edge de-
tection. In IEEE International Conference on Computer Vi-
sion, pages 1395–1403, 2015.

[40] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical
saliency detection. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1155–
1162, 2013.

6073

