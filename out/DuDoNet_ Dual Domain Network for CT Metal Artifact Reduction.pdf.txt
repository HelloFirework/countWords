DuDoNet: Dual Domain Network for CT Metal Artifact Reduction

Wei-An Lin*1 Haofu Liao*2 Cheng Peng1 Xiaohang Sun3

Jingdan Zhang4

Jiebo Luo2 Rama Chellappa1 Shaohua Kevin Zhou5,6

1University of Maryland, College Park 2University of Rochester
4Z2W Corporation 5Chinese Academy of Sciences

3Princeton University
6Peng Cheng Laboratory, Shenzhen

Abstract

Computed tomography (CT) is an imaging modality
widely used for medical diagnosis and treatment. CT im-
ages are often corrupted by undesirable artifacts when
metallic implants are carried by patients, which creates the
problem of metal artifact reduction (MAR). Existing meth-
ods for reducing the artifacts due to metallic implants are
inadequate for two main reasons. First, metal artifacts
are structured and non-local so that simple image domain
enhancement approaches would not sufÔ¨Åce. Second, the
MAR approaches which attempt to reduce metal artifacts
in the X-ray projection (sinogram) domain inevitably lead
to severe secondary artifact due to sinogram inconsistency.
To overcome these difÔ¨Åculties, we propose an end-to-end
trainable Dual Domain Network (DuDoNet) to simultane-
ously restore sinogram consistency and enhance CT images.
The linkage between the sinogram and image domains is
a novel Radon inversion layer that allows the gradients to
back-propagate from the image domain to the sinogram do-
main during training. Extensive experiments show that our
method achieves signiÔ¨Åcant improvements over other single
domain MAR approaches. To the best of our knowledge, it
is the Ô¨Årst end-to-end dual-domain network for MAR.

1. Introduction

Computed tomography (CT) images reconstructed from
X-ray projections allow effective medical diagnosis and
treatment. However, due to increasingly common metallic
implants, CT images are often adversely affected by metal
artifacts which not only exhibit undesirable visual effects
but also increase the possibility of false diagnosis. This
creates the problem of metal artifact reduction (MAR), for
which existing solutions are inadequate.

Unlike typical image restoration tasks such as super-
re-
resolution [14, 32, 25, 35], compression artifact
moval [31, 7], and denoising [3, 16, 15], metal artifacts

(a) CT with metal artifacts

(b) RDN2 [32]

(c) CNNMAR [33]

(d) DuDoNet (Ours)

Figure 1: (a) Sample MAR results for a CT image with
intense metal artifact. Metal implants are colored in yel-
low. (b) Artifacts are not fully reduced and a ‚Äòwhite band‚Äô
is present between the two implants. (c) Organ boundaries
on the right are smeared out. (d) DuDoNet effectively re-
duces metal shadows and recovers Ô¨Åne details.

are often structured and non-local (e.g.
streaking and
shadowing artifacts as in Figure 1a). Modeling such ar-
tifacts in image domain is extremely difÔ¨Åcult. There-
fore, before the emergence of deep learning, most existing
works [12, 4, 18, 17] proposed to reduce metal artifact in the
X-ray projection (sinogram) domain. The metal-corrupted
regions are viewed as missing, and replaced by interpolated
values. However, as the projections are taken from a single
object under certain geometry, physical constraints should
be satisÔ¨Åed by the enhanced sinogram. Otherwise, severe
secondary artifacts can be introduced in the reconstructed
CT images.

2The residual dense network (RDN) proposed in [32] without up-

* First two authors contributed equally.

scaling layers.

10512

ùëå

ùëÄ$

Linear

Interpolation

Radon Inversion 

Layer

ùëã&‚Äô ùëã(

IE-Net

ùëã"#$

Radon Inversion Layer

ùëå&‚Äô

SE-Net

ùëå"#$

Radon Inversion 

Layer

Parallel-Beam

Conversion

Ram-Lak Filtering

Backprojection

Figure 2: The proposed Dual Domain Network (DuDoNet) for MAR. Given a degraded sinogram Y and a metal trace mask
Mt, DuDoNet reduces metal artifacts by simultaneously reÔ¨Åning in the sinogram and image domains.

Recently, motivated by the success of deep learning in
solving ill-posed inverse problems [32, 25, 15, 19, 30, 23],
several works have been proposed to overcome the difÔ¨Åcul-
ties in MAR. Wang et al. [24] applied the pix2pix model [9]
to reduce metal artifact in the CT image domain. Zhang et
al. [33] proposed to Ô¨Årst estimate a prior image by a con-
volutional neural network (CNN). Based on the prior im-
age, metal-corrupted regions in the sinogram are Ô¨Ålled with
surrogate data through several post-processing steps for re-
duced secondary artifact. Park et al. [20] applied U-Net [22]
to directly restore metal-corrupted sinograms. Although
metal artifacts can be reduced by these deep learning ap-
proaches, we will show that, despite the strong expressive
power of deep neural networks, either image domain en-
hancement or sinogram domain enhancement is limited in
being able to restore metal shadows and secondary artifact.

We hereby propose Dual Domain Network (DuDoNet)
to address these problems by learning two CNNs on dual
domains to restore sinograms and CT images simultane-
ously. Our intuition is that image domain enhancement can
be improved by fusing information from the sinogram do-
main, and inconsistent sinograms can be corrected by the
learning signal back-propagated from the image domain to
reduce secondary artifacts. SpeciÔ¨Åcally, we propose a novel
network (Figure 2) consisting of three parts: a sinogram
enhancement network (SE-Net), a Radon inversion layer
(RIL), and an image enhancement network (IE-Net). To
address the issue that in the sinogram domain, information
about small metal implants tends to vanish in higher layers
of the network due to down-sampling, we propose a mask
pyramid U-Net architecture for SE-Net, which retains metal
mask information across multiple scales. The key to our
dual-domain learning is RIL that reconstructs CT images
using the Ô¨Åltered back-projection (FBP) algorithm and efÔ¨Å-

ciently back-propagates gradients from the image domain to
the sinogram domain. Based on RIL, we introduce a Radon
consistency loss to penalize secondary artifacts in the im-
age domain. Finally, IE-Net reÔ¨Ånes CT images via resid-
ual learning. Extensive experiments on CT images from
hundreds of patients demonstrate that dual domain enhance-
ment generates superior artifact-reduced CT images.

In summary, we make the following contributions:
‚Ä¢ We propose an end-to-end trainable dual-domain re-
Ô¨Ånement network for MAR. The network is able to re-
cover details corrupted by metal artifacts.

‚Ä¢ We propose a mask pyramid (MP) U-Net to improve
sinogram reÔ¨Ånement. The MP architecture improves
performance especially when small metallic implants
are dominated by the non-metal regions.

‚Ä¢ We propose a Radon inversion layer (RIL) to enable ef-
Ô¨Åcient end-to-end dual domain learning. RIL can ben-
eÔ¨Åt the community through its ubiquitous use in vari-
ous reconstruction algorithms [26, 10, 1, 34].

‚Ä¢ We propose a Radon consistency (RC) loss to penal-
ize secondary artifacts in the image domain. Gradients
of the loss in the image domain are back-propagated
through RIL to the sinogram domain for improved con-
sistency.

2. Backgrounds and Related Works

Tissues inside the human body such as bones and mus-
cles have different X-ray attenuation coefÔ¨Åcients ¬µ. If we
consider a 2D slice of human body, the distribution of the
attenuation coefÔ¨Åcients X = ¬µ(x, y) represents the under-
lying anatomical structure. The principle of CT imaging
is based on the fundamental Fourier Slice Theorem, which
guarantees that the 2D function X can be reconstructed

10513

solely from its dense 1D projections. In CT imaging, pro-
jections of the anatomical structure X are inferred by the
emitted and received X-ray intensities through the Lambert-
Beer Law [2]. We consider the following CT model under a
polychromatic X-ray source with energy distribution Œ∑(E):

Y = ‚àí logZ Œ∑(E) exp {‚àíPX(E)} dE,

(1)

where P is the projection generation process, and Y repre-
sents the projection data (sinogram). The 2D X(E) is the
anatomical structure (CT image) we want to recover from
the measured projection data Y .

For normal body tissues, X(E) is almost constant with

respect to the X-ray energy E. If we let X = X(E), then

Y = PX.

(2)

Therefore, given measured projection data Y , the CT image
ÀÜX can be inferred by using a reconstruction algorithm P ‚Ä†3:
ÀÜX = P ‚Ä†Y [11].

However, when metallic implants IM (E) are present,
X(E) = X + IM (E), where X(E) has large variations
with respect to E due to IM . Eq. (1) becomes

Y = PX ‚àí logZ Œ∑(E) exp{‚àíPIM (E)}dE,

(3)

where the region of PIM in Y will be referred to as metal
trace in the rest of the paper. When the reconstruction algo-
rithm P ‚Ä† is applied,

P ‚Ä†Y = ÀÜX ‚àí P ‚Ä† logZ Œ∑(E) exp{‚àíPIM (E)}dE.

(4)

The term after ÀÜX in (4) is the metal artifact. It is clear that
perfect MAR can be achieved only if the last term in Eq. (4)
is suppressed while the term ÀÜX is unaffected. However, it is
generally an ill-posed problem since both terms contribute
to the region of metal trace.

2.1. Inpainting based Methods

One commonly adopted strategy in MAR is to formu-
late sinogram completion as an image inpainting task. Data
within the metal trace are viewed as missing and Ô¨Ålled
through interpolation. Linear interpolation (LI) [12] is a
widely used method in MAR due to its simplicity. Meyer
et al. [18] proposed the NMAR algorithm, where sino-
grams are normalized by tissue priors before performing
LI. NMAR requires proper tissue segmentation in the im-
age domain, which is unreliable when severe metal artifacts
are present. Mehranian et al. [17] restored sinograms by en-
forcing sparsity constraints in the wavelet domain. In gen-
eral, inpainting-based approaches fail to replace the data of
PX in (3) within metal trace by consistent values. It is this
introduced inconsistency in sinogram data that leads to no-
ticeable secondary artifacts after reconstruction.

3We use P ‚Ä† to denote the linear operation for reconstruction.

2.2. MAR by Iterative Reconstruction

In iterative reconstruction, MAR can be formulated as

the following optimization problem:

ÀÜX = min
X

k(1 ‚àí Mt) ‚äô (PX ‚àí Y )k2 + ŒªR(X),

(5)

where Mt is the metal trace mask. Mt = 1 on the metal
trace and Mt = 0 otherwise. R is some regularization
total variation (TV) [8] and sparsity con-
function, e.g.
straints in the wavelet domain [29]. Eq. (5) is often solved
through iterative approaches such as the split Bregman al-
gorithm. Iterative reconstruction usually suffers from long
processing time as they require multiplying and inverting
huge matrices in each iteration. More importantly, hand-
crafted regularization R(X) does not capture the structure
of metal artifacts and would result in an over-smoothed re-
construction. Recently, Zhang et al. [29] proposed a re-
weighted JSR method which combines NMAR into (5) and
jointly solves for X and interpolated sinogram. Similar to
NMAR, the weighting strategy in re-weighted JSR requires
tissue segmentation. In phantom study, better performance
against NMAR is achieved by re-weighted JSR. However,
the improvements remain limited for non-phantom CT im-
ages.

2.3. Deep Learning based Methods for MAR

Convolutional neural networks have the ability to model
complex structures within data. Motivated by the success
of DNNs in solving inverse problems, Gjesteby et al. [6]
and Park et al. [20] proposed to reÔ¨Åne sinograms using a
CNN for improved consistency. Zhang et al. [33] proposed
a CNNMAR model to Ô¨Årst estimate a prior image by a
CNN and then correct sinogram similar to NMAR. How-
ever, even with the strong expressive power of CNNs, these
approaches still suffer from secondary artifacts due to in-
consistent sinograms.

Gjesteby et al. [5], Xu et al. [27] and Wang et al. [24]
proposed to reduce metal artifact directly in the CT image
domain. The metal artifacts considered in these works are
mild and thus can be effectively reduced by a CNN. We will
show in our experiments that image domain enhancement is
not sufÔ¨Åcient for mitigating intense metal shadows.

3. Proposed Method

As shown in Figure 2, our proposed model consists of
three parts: (a) a sinogram enhancement network (SE-Net),
(b) a Radon inversion layer (RIL), and (c) an image en-
hancement network (IE-Net). Inputs to the model include
a degraded sinogram Y ‚àà RHs√óWs and the corresponding
metal trace mask Mt ‚àà {0, 1}Hs√óWs . Notice that we use
Hs to represent the detector size and Ws to represent the
number of projection views. The region where Mt = 1

10514

is the metal trace. Given the inputs, we Ô¨Årst apply LI [12]
to generate an initial estimate for the sinogram data within
metal trace. The resulting interpolated sinogram is denoted
by YLI . SE-Net then restores YLI within the metal trace
through a mask pyramid U-Net architecture. To maintain
sinogram consistency, we introduce a Radon consistency
(RC) loss. A sinogram will be penalized by the RC loss
if it leads to secondary artifacts in the image domain after
passing through RIL. Finally, the reconstructed CT image
ÀÜX ‚àà RHc√óWc is reÔ¨Åned by IE-Net via residual learning.

3.1. Sinogram Enhancement Network

Sinogram enhancement is extremely challenging since
geometric consistency should be retained to prevent sec-
ondary artifact in the reconstructed CT image, so prior
works only replace data within the metal trace. Similarly,
given a metal-corrupted sinogram Y and metal trace mask
Mt, SE-Net Gs learns to restore the region of YLI in Mt =
1. In sinogram domain enhancement, when the metal size
is small, or equivalently, the metal trace is small, informa-
tion about metal trace is dominated by non-metal regions
in higher layers of network due to down-sampling. To re-
tain the mask information, we propose to fuse Mt through
a mask pyramid U-Net architecture. The output of SE-Net
is written as

Yout = Mt ‚äô Gs(YLI , Mt) + (1 ‚àí Mt) ‚äô YLI .

(6)

We use an L1 loss to train SE-Net:

LGs = kYout ‚àí Ygtk1,

(7)

where Ygt is the ground truth sinogram without metal arti-
fact.

3.2. Radon Inversion Layer

Although sinogram inconsistency is reduced by SE-Net,
there is no existing mechanism to penalize secondary arti-
facts in the image domain. The missing key element is an
efÔ¨Åcient and differentiable reconstruction layer. Therefore,
we propose a novel RIL fR to reconstruct CT images from
sinograms and at the same time allow back-propagation of
gradients. We hightlight that trivially inverting P in existing
deep learning frameworks would require a time and space
complexity of O(HsWsHcWc), which is prohibitive due to
limited GPU memory.

In this work, we consider the projection process P as the
Radon transform under fan-beam geometry with arc detec-
tors [11]. The distance between an X-ray source and its ro-
tation center is D. The resulting fan-beam sinograms Yf an
are represented in coordinates (Œ≥, Œ≤). To reconstruct CT im-
ages from Yf an(Œ≥, Œ≤), we adopt the fan-beam Ô¨Åltered back-
projection (FBP) algorithm as the forward operation of RIL.

Our RIL consists of three modules: (a) a parallel-beam
conversion module, (b) a Ô¨Åltering module, and (c) a back-
projection module. The parallel-beam conversion mod-
ule transforms Yf an(Œ≥, Œ≤) to its parallel-beam counterpart
Ypara(t, Œ∏) through a change of variables. The FBP al-
gorithm in coordinate (t, Œ∏) becomes more effective and
memory-efÔ¨Åcient than in (Œ≥, Œ≤). Parallel-beam FBP is then
realized by the subsequent Ô¨Åltering and back-projection
modules.

Parallel-beam Conversion Module. We utilize the
property that a fan beam sinogram Yf an(Œ≥, Œ≤) can be con-
verted to its parallel beam counterpart Ypara(t, Œ∏) through
the following change of variables [11]:

(t = D sin Œ≥,

Œ∏ = Œ≤ + Œ≥.

(8)

The change of variable in (8) is implemented by grid sam-
pling in (t, Œ∏), which allows back-propagation of gradients.
With Ypara, CT images can be reconstructed through the
following Ram-Lak Ô¨Åltering and back-projection modules.
Ram-Lak Filtering Module. We apply the Ram-Lak

Ô¨Åltering to Ypara in the Fourier domain.

Q(t, Œ∏) = F ‚àí1

t

{|œâ| ¬∑ Ft {Ypara(t, Œ∏)}} ,

(9)

where Ft and F ‚àí1
are the Discrete Fourier Transform
(DFT) and inverse Discrete Fourier Transform (iDFT) with
respect to the detector dimension.

t

Backprojection Module. The Ô¨Åltered parallel-beam
sinogram Q is back-projected to the image domain for every
projection angle Œ∏ by the following formula:

X(u, v) =Z œÄ

0

Q(u cos Œ∏ + v sin Œ∏, Œ∏)dŒ∏.

(10)

It is clear from (10) that the computation is highly parallel.
We make a remark here regarding the property of RIL fR.
Due to the back-projection nature of fR, the derivative with
respect to the input Yout is actually the projection operation
P. That is, any loss in the image domain will be aggregated
and projected to the sinogram domain. This desirable prop-
erty enables joint learning in sinogram and image domains.
Radon Consistency Loss. With the differentiable RIL,
we introduce the following Radon consistency (RC) loss to
penalize secondary artifacts in ÀÜX = fR(Yout) after recon-
struction.

LRC = kfR(Yout) ‚àí Xgtk1,

(11)

where Xgt is the ground truth CT image without metal arti-
fact.

Difference from DL-based Reconstruction. Our RIL
is designed to combine the image formation process (CT
reconstruction) with deep neural networks and achieve im-
proved MAR by dual-domain consistency learning. Meth-
ods in [26, 10, 1, 34] target image formation via deep learn-
ing, which is not the main focus of this work.

10515

Ground truth CT image

Figure 3: Sample simulated metal artifact on patient CT. The X-ray spectrum is shown in the upper-left corner. Metallic
implants are colored in yellow for better visualization.

3.3. Image Enhancement Network

Since our ultimate goal is to reduce visually undesirable
artifacts in image domain, we further apply a U-Net Gi to
enhance ÀÜX by residual learning:

Xout = XLI + Gi( ÀÜX, XLI ),

(12)

where XLI = fR(YLI ) is reconstructed from YLI , the lin-
early interpolated sinogram. Gi is also optimized by L1 loss.

LGi = kXout ‚àí Xgtk1.

The full objective function of our model is:

L = LGs + LRC + LGi .

(13)

(14)

One could tune and balance each term in (14) for better per-
formance. However, we found that the default setting works
sufÔ¨Åciently well.

4. Experimental Results

Following the de facto practice in the literature [33], our
evaluations consider simulated metal artifacts on real pa-
tient CTs. Various effects are considered including poly-
chromatic X-ray, partial volume effect, and Poisson noise.
The simulated artifacts exhibit complicated structures and
cannot be easily modelled by a very deep CNN. All the
compared approaches are evaluated on the same dataset,
and superior performance is achieved by our method. Eval-
uations on clinical data is presented in the supplementary
material.

Metal Artifact Dataset. Recently, Yan et al. [28] re-
leased a large-scale CT dataset DeepLesion for lesion de-
tection. Due to its high diversity and quality, we use a sub-
set of images from DeepLesion to synthesize metal artifact.
4,000 images from 320 patients are used in the training set

and 200 images from 12 patients are used in the test set. All
images are resized to 416 √ó 416. We collect a total of 100
metal shapes. 90 metal shapes are paired with the 4,000 im-
ages, yielding 360,000 combinations in the training set. 10
metal shapes are paired with the 200 images, yielding 2,000
combinations in the test set. In the training set, the sizes of
the metal implants range from 16 to 4967 pixels. In the test
set, the sizes of the metal implants range from 32 to 2054
pixels.

We adopt similar procedures as in [33] to synthesize
metal-corrupted sinograms and CT images. We assume a
polychromatic X-ray source with spectrum Œ∑(E) in Fig-
ure 3. To simulate Poisson noise in the sinogram, we as-
sume the incident X-ray has 2 √ó 107 photons. Metal partial
volume effect is also considered. The distance from the X-
ray source to the rotation center is set to 39.7cm, and 320
projection views are uniformly spaced between 0-360 de-
grees. The resulting sinograms have size 321 √ó 320. Fig-
ure 3 shows some sample images with simulated metal arti-
facts.

Evaluation Metrics. We choose peak signal-to-noise ra-
tio (PSNR) and structured similarity index (SSIM) for quan-
titative evaluations. In DeepLesion, each CT image is pro-
vided with a dynamic range, within which the tissues are
clearly discernible. We use the dynamic range as the peak
signal strength when calculating PSNR.

Implementation Details. We implement our model us-
ing the PyTorch [21] framework. All the sinograms have
size 321 √ó 320, and all the CT images have size 416 √ó 416.
To train the model, we use the Adam [13] optimizer with
(Œ≤1, Œ≤2) = (0.5, 0.999), and a batch size of 8. The learning
rate starts from 2 √ó 10‚àí4, and is halved for every 30 epochs.
The model is trained on two Nvidia 1080Ti for 380 epochs.

10516

PSNR(dB)/SSIM

Large Metal ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí Small Metal

A) SE-Net0
B) SE-Net
C) IE-Net
D) SE-Net0+IE-Net
E) SE-Net+IE-Net
F) SE-Net0+IE-Net+RCL
G) SE-Net+IE-Net+RCL

22.88/0.7850
23.06/0.7868
27.54/0.8840
28.46/0.8938
28.28/0.8921
28.97/0.8970
29.02/0.8972

24.52/0.8159
24.71/0.8178
29.49/0.9153
30.67/0.9232
30.49/0.9221
31.14/0.9254
31.12/0.9256

27.38/0.8438
27.66/0.8463
31.96/0.9368
33.71/0.9458
33.76/0.9456
34.21/0.9476
34.32/0.9481

28.61/0.8549
28.91/0.8575
34.38/0.9498
36.17/0.9576
36.26/0.9576
36.58/0.9590
36.72/0.9595

28.93/0.8581
29.19/0.8604
33.90/0.9489
35.74/0.9571
36.01/0.9574
36.15/0.9586
36.36/0.9592

Average

26.46/0.8315
26.71/0.8337
31.45/0.9269
32.95/0.9355
32.96/0.9350
33.41/0.9375
33.51/0.9379

Table 1: Quantitative evaluations for different components in DuDoNet.

4.1. Ablation Study

In this section, we evaluate the effectiveness of different
components in the proposed approach. Performance is eval-
uated on the artifact-reduced CT images. When evaluating
SE-Nets without image domain reÔ¨Ånement, we use the re-
constructed CT images ÀÜX. We experiment on the following
conÔ¨Ågurations:

A) SE-Net0: The sinogram enhancement network without

mask pyramid network.

B) SE-Net: The full sinogram enhancement module.

C) IE-Net: Image enhancement module. IE-Net is applied

to enhance XLI without ÀÜX.

D) SE-Net0+IE-Net: Dual domain learning with SE-Net0

and IE-Net.

E) SE-Net+IE-Net: Dual domain learning with SE-Net

and IE-Net.

F) SE-Net0+IE-Net+RCL: Dual domain learning with

Radon consistency loss.

G) SE-Net+IE-Net+RCL: Our full network.

Notice that the conÔ¨Ågurations including SE-Net0, SE-Net
and IE-Net are single domain enhancement approaches.

Table 1 summarizes the performance of different mod-
els. Since there are totally 10 metal implants in the test set,
for conciseness, we group the results according to the size
of metal implants. The sizes of the 10 metal implants are:
[2054, 879, 878, 448, 242, 115, 115, 111, 53, 32] in pixels.
We simply put every two masks into one group.

From E and G, it is clear that the use of the RC loss
improves the performance over all metal sizes for at least
0.3 dB. In Figure 4, the model trained with RC loss better
recovers the shape of the organ.

From F and G, we observe an interesting trend that the
proposed mask pyramid architecture results in ‚àº0.2 dB
gain when the metal size is small, and the performance is
nearly identical when the metal is large. The reason is that
the mask pyramid retains metal information across multiple
scales. Figure 5 demonstrates that in the proximity of small

metal implants, the model with mask pyramid recovers the
Ô¨Åne details.

Without RC loss

With RC loss

Ground Truth

Figure 4: Visual comparisons between models without RC
loss (E in Table 1) and our full model (G in Table 1).

Without MP

With MP

Ground Truth

Figure 5: Visual comparisons between models without MP
(F in Table 1) and our full model (G in Table 1).

XLI

IE-Net

IE-Net-RDN

ÀÜX

Xout

Ground Truth

Figure 6: Visual comparisons between models without SE-
Net (top row IE-Net and IE-Net-RDN) and our full model
(bottom row ÀÜX and Xout).

Effect of Dual Domain Learning.

In the proposed
framework, IE-Net enhances XLI by fusing information

10517

PSNR(dB)/SSIM

LI [12]
NMAR [18]
cGAN-CT [24]
RDN-CT [32]
CNNMAR [33]
DuDoNet (Ours)

Large Metal ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí Small Metal

20.20/0.8236
21.95/0.8333
26.71/0.8265
28.61/0.8668
23.82/0.8690
29.02/0.8972

22.35/0.8686
24.43/0.8813
24.71/0.8507
28.78/0.9027
26.78/0.9097
31.12/0.9256

26.76/0.9098
28.63/0.9174
29.80/0.8911
32.40/0.9264
30.92/0.9394
34.32/0.9481

28.50/0.9252
30.84/0.9281
31.47/0.9104
34.95/0.9446
32.97/0.9513
36.72/0.9595

29.53/0.9312
31.69/0.9402
27.65/0.8876
34.00/0.9376
33.11/0.9520
36.36/0.9592

Average

25.47/0.8917
27.51/0.9001
28.07/0.8733
31.74/0.9156
29.52/0.9243
33.51/0.9379

Table 2: Quantitative evaluation of MAR approaches in terms of PSNR and SSIM.

Ground Truth
PSNR/SSIM

With Metal Artifact

10.98/0.1485

LI [12]

20.62/0.5462

NMAR [18]
23.21/0.6336

(a) Small metallic implants.

cGAN-CT [24]
15.12/0.2678

RDN-CT [32]
20.88/0.5353

CNNMAR [33]

23.11/0.6405

DuDoNet

26.91/0.7258

Ground Truth
PSNR/SSIM

With Metal Artifact

9.67/0.1137

LI [12]

18.36/0.6628

NMAR [18]
19.08/0.6697

(b) Medium metallic implants.

cGAN-CT [24]
28.15/0.7328

RDN-CT [32]
21.52/0.6966

CNNMAR [33]

19.66/0.6370

DuDoNet

28.72/0.8108

Ground Truth
PSNR/SSIM

With Metal Artifact

12.15/0.1519

LI [12]

19.27/0.6260

NMAR [18]
20.20/0.6597

(c) Large metallic implants.

cGAN-CT [24]
18.68/0.4460

RDN-CT [32]
26.28/0.6946

CNNMAR [33]

20.92/0.6916

DuDoNet

27.31/0.7947

Figure 7: Visual comparisons on MAR for different types of metallic implants.

from SE-Net. We study the effect of dual domain learn-
ing by visually comparing our full pipeline (G in Table 1)
with single domain enhancement IE-Net (C in Table 1). In
addition to the U-Net architecture, we also consider IE-Net
with RDN architecture, which is denoted as IE-Net-RDN.
Visual comparisons are shown in Figure 6. We observe that

single domain models IE-Net and IE-Net-RDN fail to re-
cover corrupted organ boundaries in XLI . In our dual do-
main reÔ¨Ånement network, SE-Net Ô¨Årst recovers inconsistent
sinograms and reduces secondary artifacts as in ÀÜX. IE-Net
then reÔ¨Ånes ÀÜX to recover the Ô¨Åne details.

Effect of LI sinogram. The inputs to our network are

10518

the linear interpolated sinogram YLI and its reconstructed
CT XLI . One possible alternative is to directly input the
metal corrupted sinogram and CT, and let the network learn
to restore the intense artifacts. However, we experimen-
tally found out this alternative approach does not perform
well. Metal shadows and streaking artifacts are not fully
suppressed.

4.2. Comparison with State of the Art Methods

In this section, we compare our model with the follow-
ing methods: LI [12], NMAR [18], cGAN-CT [24], RDN-
CT [32] and CNNMAR [33]. We use cGAN-CT to refer
the approach by Wang et al. [24] which applies cGAN for
image domain MAR. RDN [32] was originally proposed for
image super-resolution (SR). The fundamental building unit
of RDN is the residual dense block (RDB). Recently, it has
been shown that by stacking multiple RDBs or its variant,
the residual in residual dense blocks (RRDBs) [25], local
details in natural images can be effectively recovered. We
build a very deep architecture with 10 RDBs (‚àº80 conv
layers) for direct image domain enhancement, which is de-
noted by RDN-CT. SpeciÔ¨Åcally, we select D = 10, C =
8, G = 64, following the notations in [32]. Inputs to RDN-
CT are 128 √ó 128 patches.

Quantitative Comparisons. Table 2 shows quantitative
comparisons. We observe that the state-of-the-art sinogram
inpainting approach CNNMAR achieves higher SSIM than
image enhancement approaches (e.g. RDN and cGAN-CT)
especially when the size of metal is small. The reason
is that sinogram inpainting only modiÔ¨Åes data within the
metal trace and recovers the statistics reasonably well. In
most of the cases, CNNMAR also outperforms cGAN-CT
in terms of PSNR. However, when CNN is sufÔ¨Åciently deep
(e.g. RDN-CT), image enhancement approaches generally
achieve higher PSNR. Our dual domain learning approach
jointly restores sinograms and CT images, which attains the
best performance in terms of both PSNR and SSIM consis-
tently in all categories.

Visual Comparisons. Figure 7 shows visual compar-
isons. Figure 7a considers metal artifacts resulted from
two small metallic implants. From the zoomed Ô¨Ågure (with
metal artifact), we can perceive severe streaking artifacts
and intense metal shadows between the two implants. We
observe that sinogram inpainting approaches such as LI,
NMAR and CNNMAR effectively reduce metal shadows.
However, Ô¨Åne details are either corrupted by secondary arti-
facts as in LI or blurred as in NMAR and CNNMAR. Image
domain approaches such as cGAN-CT and RDN-CT pro-
duce sharper CT images but fail to suppress metal shadows.
Our method effectively reduces metal shadows and at the
same time retains Ô¨Åne details. Figure 7b shows a degraded
CT image with long metal implants. We observe similar
trend that sinogram inpainting approaches do not perform

well in regions with intense streaking artifact. In this ex-
ample, image domain methods reduce most of the artifacts.
It is possibly due to that fact that the pattern of the artifact
in Figure 7b is monotonous compared to Figures 7a and 7c.
However, noticeable speckle noise is present in the result by
cGAN-CT, and RDN-CT does not fully recover details in
the middle. Figure 7c considers metal artifacts result from
two large metallic implants. Likewise, sinogram inpainting
methods and direct image domain enhancement have lim-
ited capability of suppressing metal artifacts. More visual
comparisons are presented in the supplemental material.

4.3. Running Time Comparisons

On an Nvidia 1080Ti GPU, it takes 0.24 ms for RIL to
reconstruct a sinogram of size 321 √ó 320 to a CT image of
size 416√ó416, and 11.40 ms for back-propagation of gradi-
ents. RIL requires 16 MB of memory for forward pass and
25 MB for back-propagation. In Table 3 we compare the
running time of different MAR approaches. With the run-
ning time of LI included, DuDoNet runs almost 4√ó faster
than the very deep architecture RDN while achieving supe-
rior performance.

LI
[12]

NMAR

cGAN-CT

RDN-CT

CNNMAR

DuDoNet

[18]

[24]

[32]

[33]

(Ours)

0.0832 0.4180

0.0365

0.5150

0.6043

0.1335

Table 3: Comparison of running time measured in seconds.

5. Conclusion

In this paper, we present the Dual Domain Network
for metal artifact reduction.
In particular, we propose to
jointly improve sinogram consistency and reÔ¨Åne CT images
through a novel Radon inversion layer and a Radon consis-
tency loss, along with a mask pyramid U-Net. Experimen-
tal evaluations demonstrate that while state-of-the-art MAR
methods suffer from secondary artifacts and very-deep neu-
ral networks have limited capability of directly reducing
metal artifacts in image domain, our dual-domain model can
effectively suppress metal shadows and recover details for
CT images. At the same time, our network is computation-
ally more efÔ¨Åcient. Future work includes investigating the
potential of the dual-domain learning framework for other
signal recovery tasks, such as super-resolution, noise reduc-
tion, and CT reconstruction from sparse X-ray projections.

Acknowledgements: This research was supported by
MURI from the Army Research OfÔ¨Åce under the Grant No.
W911NF-17-1-0304, NSF award #17228477, and the Mor-
ris K. Udall Center of Excellence in Parkinson‚Äôs Disease
Research by NIH.

10519

References

[1] J. Adler and O. ¬®Oktem. Learned primal-dual reconstruction.
IEEE Transactions on Medical Imaging, 37(6):1322‚Äì1332,
June 2018. 2, 4

[2] Beer. Bestimmung der absorption des rothen lichts in
farbigen Ô¨Ç¬®ussigkeiten. Annalen der Physik und Chemie,
162(5):78‚Äì88, 1852. 3

[3] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image
denoising by sparse 3-d transform-domain collaborative Ô¨Ål-
tering. IEEE Transactions on image processing, 16(8):2080‚Äì
2095, 2007. 1

[4] X. Duan, L. Zhang, Y. Xiao, J. Cheng, Z. Chen, and Y. Xing.
Metal artifact reduction in ct images by sinogram tv inpaint-
ing.
In Nuclear Science Symposium Conference Record,
2008. NSS‚Äô08. IEEE, pages 4175‚Äì4177. IEEE, 2008. 1

[5] L. Gjesteby, Q. Yang, Y. Xi, B. Claus, Y. Jin, B. De Man,
and G. Wang. Reducing metal streak artifacts in ct images
via deep learning: Pilot results.
In The 14th International
Meeting on Fully Three-Dimensional Image Reconstruction
in Radiology and Nuclear Medicine, pages 611‚Äì614, 2017.
3

[6] L. Gjesteby, Q. Yang, Y. Xi, Y. Zhou, J. Zhang, and G. Wang.
Deep learning methods to guide ct image reconstruction and
reduce metal artifacts. In SPIE Medical Imaging, 2017. 3

[7] J. Guo and H. Chao. Building dual-domain representations
for compression artifacts reduction. In European Conference
on Computer Vision, pages 628‚Äì644. Springer, 2016. 1

[8] H. Gupta, K. H. Jin, H. Q. Nguyen, M. T. McCann, and
M. Unser. Iterative metal artifact reduction for x-ray com-
puted tomography using unmatched projector/backprojector
pairs. IEEE Transactions on Medical Imaging, 43(6):3019‚Äì
3033, 2016. 3

[9] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks.
In The
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 1125‚Äì1134, 2017. 2

[10] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser. Deep
convolutional neural network for inverse problems in imag-
ing. IEEE Transactions on Image Processing, 26(9):4509‚Äì
4522, Sept 2017. 2, 4

[11] A. C. Kak and M. Slaney. Principles of computerized tomo-
graphic imaging. Society for Industrial and Applied Mathe-
matics, 2001. 3, 4

[12] W. A. Kalender, R. Hebel, and J. Ebersberger. Reduc-
tion of ct artifacts caused by metallic implants. Radiology,
164(2):576‚Äì577, 1987. 1, 3, 4, 7, 8

[13] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 5

[14] C. Ledig, L. Theis, F. Husz¬¥ar, J. Caballero, A. Cunning-
ham, A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang,
et al. Photo-realistic single image super-resolution using a
generative adversarial network.
In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), vol-
ume 2, page 4, 2017. 1

[15] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Kar-
ras, M. Aittala, and T. Aila. Noise2Noise: Learning image

restoration without clean data. In International Conference
on Machine Learning (ICML), volume 80, pages 2965‚Äì2974,
2018. 1, 2

[16] M. Makitalo and A. Foi. Optimal inversion of the anscombe
transformation in low-count poisson image denoising. IEEE
transactions on Image Processing, 20(1):99‚Äì109, 2011. 1

[17] A. Mehranian, M. R. Ay, A. Rahmim, and H. Zaidi. X-ray
ct metal artifact reduction using wavelet domain l0 sparse
regularization.
IEEE Transactions on Medical Imaging,
32:1707‚Äì1722, 2013. 1, 3

[18] E. Meyer, R. Raupach, M. Lell, B. Schmidt, and M. Kachel-
rie√ü. Normalized metal artifact reduction (nmar) in com-
puted tomography. Medical physics, 37(10):5482‚Äì5493,
2010. 1, 3, 7, 8

[19] J. Pan, W. Ren, Z. Hu, and M. Yang. Learning to deblur im-
ages with exemplars. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 2018. 2

[20] H. S. Park, Y. E. Chung, S. M. Lee, H. P. Kim, and J. K.
Seo. Sinogram-consistency learning in ct for metal artifact
reduction. arXiv preprint arXiv:1708.00607, 2017. 2, 3

[21] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. In NIPS-W, 2017. 5

[22] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In Med-
ical Image Computing and Computer Assisted Intervention
(MICCAI), pages 234‚Äì241. Springer, 2015. 2

[23] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep image prior.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018. 2

[24] J. Wang, Y. Zhao, J. H. Noble, and B. M. Dawant. Condi-
tional generative adversarial networks for metal artifact re-
duction in ct images of the ear. In Medical Image Computing
and Computer Assisted Intervention (MICCAI), 2018. 2, 3,
7, 8

[25] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao,
and C. C. Loy. Esrgan: Enhanced super-resolution genera-
tive adversarial networks. In The European Conference on
Computer Vision Workshops (ECCVW), September 2018. 1,
2, 8

[26] T. W¬®urÔ¨Ç, F. C. Ghesu, V. Christlein, and A. Maier. Deep
In Medical Image Com-
learning computed tomography.
puting and Computer Assisted Intervention (MICCAI), pages
432‚Äì440. Springer International Publishing, 2016. 2, 4

[27] S. Xu and H. Dang. Deep residual learning enabled metal
artifact reduction in ct. In Medical Imaging 2018: Physics
of Medical Imaging, volume 10573, page 105733O. Interna-
tional Society for Optics and Photonics, 2018. 3

[28] K. Yan, X. Wang, L. Lu, L. Zhang, A. P. Harrison,
M. Bagheri, and R. M. Summers. Deep lesion graphs in
the wild: Relationship learning and organization of signiÔ¨Å-
cant radiology image Ô¨Åndings in a diverse large-scale lesion
database. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018. 5

[29] H. Zhang, B. Dong, and B. Liu. A reweighted joint spatial-
radon domain ct image reconstruction model for metal ar-
tifact reduction. SIAM J. Imaging Sciences, 11:707‚Äì733,
2018. 3

10520

[30] H. Zhang and V. M. Patel. Densely connected pyramid de-
hazing network. In The IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2018. 2

[31] X. Zhang, W. Yang, Y. Hu, and J. Liu. Dmcnn: Dual-domain
multi-scale convolutional neural network for compression ar-
tifacts removal. In IEEE International Conference on Image
Processing (ICIP), 2018. 1

[32] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Resid-
ual dense network for image super-resolution. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 1, 2, 7, 8

[33] Y. Zhang and H. Yu. Convolutional neural network based
metal artifact reduction in x-ray computed tomography.
IEEE Transactions on Medical Imaging, 2018. 1, 2, 3, 5,
7, 8

[34] Z. Zhang, X. Liang, X. Dong, Y. Xie, and G. Cao. A
sparse-view ct reconstruction method based on combination
of densenet and deconvolution. IEEE Transactions on Med-
ical Imaging, 37(6):1407‚Äì1417, June 2018. 2, 4

[35] Z. Zhong, T. Shen, Y. Yang, Z. Lin, and C. Zhang. Joint
sub-bands learning with clique structures for wavelet domain
super-resolution. arXiv preprint arXiv:1809.04508, 2018. 1

10521

