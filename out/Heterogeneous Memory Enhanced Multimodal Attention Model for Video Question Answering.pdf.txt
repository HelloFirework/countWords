Heterogeneous Memory Enhanced Multimodal Attention Model for

Video Question Answering

Chenyou Fan1

,

∗, Xiaofan Zhang1, Shu Zhang1, Wensheng Wang1, Chi Zhang1, Heng Huang1

2

,

∗

,

1JD.COM, 2 JD Digits

∗

chenyou.fan@jd.com,

∗

heng.huang@jd.com

Abstract

In this paper, we propose a novel end-to-end trainable
Video Question Answering (VideoQA) framework with three
major components: 1) a new heterogeneous memory which
can effectively learn global context information from ap-
pearance and motion features; 2) a redesigned question
memory which helps understand the complex semantics of
question and highlights queried subjects; and 3) a new mul-
timodal fusion layer which performs multi-step reasoning
by attending to relevant visual and textual hints with self-
updated attention. Our VideoQA model ﬁrstly generates
the global context-aware visual and textual features respec-
tively by interacting current inputs with memory contents.
After that, it makes the attentional fusion of the multimodal
visual and textual representations to infer the correct an-
swer. Multiple cycles of reasoning can be made to itera-
tively reﬁne attention weights of the multimodal data and
improve the ﬁnal representation of the QA pair. Experimen-
tal results demonstrate our approach achieves state-of-the-
art performance on four VideoQA benchmark datasets.

1. Introduction

Video Question Answering (VideoQA) is to learn a
model that can infer the correct answer for a given ques-
tion in human language related to the visual content of a
video clip. VideoQA is a challenging computer vision task,
as it requires to understand a complex textual question ﬁrst,
and then to ﬁgure out the answer that can best associate the
semantics to the visual contents in an image sequence.

Recent work [2,3,10,11,15,29] proposed to learn models
of encoder-decoder structure to tackle the VideoQA prob-
lem. A common practice is to use LSTM-based encoders
to encode CNN features of video frames and embeddings
of question words into encoded visual sequence and word
sequence. Proper reasoning is then performed to produce
the correct answer, by associating the relevant visual con-
tents with the question. For example, learning soft weights

Figure 1. VideoQA is a challenging task as it requires the model to
associate relevant visual contents in frame sequence with the real
subject queried in question sentence. For a complex question such
as “Who drives by a hitchhiking man who is smoking?”, the model
needs to understand that the driver is the queried person and then
localize the frames in which the driver is driving in the car.

of frames will help attend to events that are queried by the
questions, while learning weights of regions in every single
frame will help detect details and localize the subjects in
the query. The former one aims to ﬁnd relevant frame-level
details by applying temporal attention to encoded image se-
quence [10, 15, 27]. The latter one aims to ﬁnd region-level
details by spatial attention [2, 12, 26, 29].

Jang et al. [10] applied spatiotemporal attention mecha-
nism on both spatial and temporal dimension of video fea-
tures. They also proposed to use both appearance (e.g.,
VGG [22]) and motion features (e.g., C3D [24]) to better
represent video frames. Their practice is to make early fu-
sion of the two features and feed the concatenated feature
to a video encoder. But such straightforward feature inte-
gration leads to suboptimal results. Gao et al. [5] proposed
to replace the early fusion with a more sophisticated co-
memory attention mechanism. They used one type of fea-
ture to attend to the other and fused the ﬁnal representations
of these two feature types at the ﬁnal stage. However, this
method doesn’t synchronize the attentions detected by ap-
pearance and motion features, thus could generate incorrect
attentions. Meanwhile, this method will also miss the atten-
tion which can be inferred by the combined appearance and

11999

A: Our model: woman     Existing model: man......Q: Who drives by a hitchhiking man who is smoking? (answer: woman) Video AttentionQuestion Attentionmotion features, but not individual ones. The principal rea-
son for the existing approaches to fail to identify the correct
attention is that they separate feature integration and atten-
tion learning steps. To address this challenging problem,
we propose a new heterogeneous memory to integrate
appearance and motion features and learn spatiotempo-
ral attention simultaneously. In our new memory model,
the heterogeneous visual features as multi-input will co-
learn the attention to improve the video understanding.

On the other hand, VideoQA becomes very challenging
if the question has complex semantics and requires multiple
steps of reasoning. Several recent work [5, 15, 32] tried to
augment VideoQA with differently embodied memory net-
works [23, 25, 26]. Xu et al. [27] proposed to reﬁne the
temporal attention over video features word by word with
a conventional LSTM question encoder plus an additional
LSTM based memory unit to store and update the attention.
However, this model is easily trapped into irrelevant local
semantics, and cannot understand the question based on the
global context. Both Zeng et al. [32] and Gao et al. [5]
used external memory (memory network [23] and episodic
memory [26] respectively) to make multiple iterations of in-
ference by interacting the encoded question representation
with video features conditioning on current memory con-
tents. However, similar to many other work [2, 10, 26], the
question representation used in these approaches is only a
single feature vector encoded by an LSTM (or GRU) which
lacks capability to capture complex semantics in questions
such as shown in Fig. 1. Thus, it is desired to design a new
powerful model for understanding the complex semantics
of questions in VideoQA. To tackle this problem, we de-
sign novel network architecture to integrate both ques-
tion encoder and question memory which can augment
each other. The question encoder learns meaningful
representation of question and the re-designed question
memory understands the complex semantics and high-
lights queried subjects by storing and updating global
contexts.

Moreover, we design a multimodal fusion layer which
can attend to visual and question hints simultaneously by
aligning relevant visual contents with key question words.
After gradually reﬁning the joint attention over video and
question representations and fusing them with learned soft
modality weights, the multi-step reasoning is achieved to
infer the correct answer from the complex semantics.

Our major contributions can be summarized as follows:
1) we introduce a heterogeneous external memory module
with attentional read and write operations such that the mo-
tion and appearance features are integrated to co-learn at-
tention; 2) we utilize the interaction of visual and ques-
tion features with memory contents to learn global context-
aware representations; 3) we design a multimodal fusion
layer which can effectively combine visual and question

features with softly assigned attentional weights and also
support multi-step reasoning; and 4) our proposed model
outperforms the state-of-the-art methods on four VideoQA
benchmark datasets.

2. Related Work

Visual Question Answering (VQA) is an emerging re-
search area [1–3, 11, 15, 17, 29] to reason the correct answer
of a given question which is related to the visual content
of an image. Yang et al. [29] proposed to encode question
words into one feature vector which is used as query vec-
tor to attend to relevant image regions with stack attention
mechanism. Their method supports multi-step reasoning by
repeating the query process while reﬁning the query vector.
Anderson et al. [2] proposed to align questions with relevant
object proposals in images generated by Faster R-CNN [20]
and compute the visual feature as a weighted average over
all proposals. Xiong et al. [26] proposed to encode image
and question features as facts and attend to relevant facts
through attention mechanism to generate a contextual vec-
tor. Ma et al. [15] proposed a co-attention model which can
attend to not only relevant image regions but also important
question words simultaneously. They also suggested to use
external memory [21] to memorize uncommon QA pairs.

Video Question Answering (VideoQA) extends VQA
to video domain which aims to infer the correct answer
given a relevant question of the visual content of a video
clip. VideoQA is considered to be a challenging problem as
reasoning on video clip usually requires memorizing con-
textual information in temporal scale. Many models have
been proposed to tackle this problem [5, 10, 27, 30–32].
Many work [5, 10, 30] utilized both motion (i.e. C3D [24])
and appearance (i.e. VGG [22], ResNet [8]) features to bet-
ter represent video frames. Similar to the spatial mecha-
nism widely used in VQA methods to ﬁnd relevant image
regions, many VideoQA work [5, 10, 27, 30] applied tempo-
ral attention mechanism to attend to most relevant frames
of a video clip.
Jang [10] utilized both appearance and
motion features as video representations and applied spatial
and temporal attention mechanism to attend to both relevant
regions of a frame and frames of a video. Xu et al. [27] pro-
posed to reﬁne the temporal attention over frame features at
each question encoding step word by word. Both Zeng et
al. [32] and Gao et al. [5] proposed to use external memory
(Memory Network [23] and Episodic Memory [26] respec-
tively) to make multiple iterations of inference by interact-
ing the encoded question feature with video features condi-
tioning on current memory contents. Their memory designs
maintain a single hidden state feature of current step and
update it through time steps. However, this could hardly es-
tablish long-term global context as the hidden state feature
is updated at every step. Neither are their models able to
synchronize appearance and motion features.

22000

Please see Fig 3 for details.

Visual Memory

Please see Fig 4 for details.

Question Memory 

Please see Fig 5 for details.

Multi-modal Fusion

⨁

F

T
e
m
p
o
r
a

l
 

A
T
T

.

Label

SL

STA

0.2

0.5

0.1

0.2

Mv

Mv

Mv

Mq

Mq

Mq

Motion
 Net 

 
 
 

Appearance 

 
 
 

Net 

LSTM

LSTM

LSTM

Motion Encoder 

LSTM

LSTM

LSTM

Appearance Encoder 

Video Encoder LSTMs

⨁

LSTM

LSTM

LSTM

How

many ...

laughing

Question Encoder LSTM

FC + Softmax

Figure 2. Our proposed VideoQA pipeline with highlighted visual memory, question memory, and multimodal fusion layer.

Our model differs from existing work such that 1) we de-
sign a heterogeneous external memory module with atten-
tional read and write operations that can efﬁciently combine
motion and appearance features together; 2) we allow inter-
action of visual and question features with memory contents
to construct global context-aware features; and 3) we design
a multimodal fusion layer which can effectively combine
visual and question features with softly assigned attentional
weights and also support multi-step reasoning.

3. Our Approach

In this section, we illustrate our network architecture for
VideoQA. We ﬁrst introduce the LSTM encoders for video
features and question embeddings. Then we elaborate on
the design of question memory and heterogeneous video
memory. Finally, we demonstrate how our designed multi-
modal fusion layer can attend to relevant visual and textual
hints and combine to form the ﬁnal answer representation.

3.1. Video and text representation

Video representation. Following previous work [5, 10,
27], we sample a ﬁxed number of frames (e.g., 35 for TGIF-
QA) for all videos in that dataset. We then apply pre-
trained ResNet [8] or VGG [22] network on video frames
to extract video appearance features, and use C3D [24] net-
work to extract motion features. We denote appearance
features as f a = [f a
], and motion features as
f m = [f m
], in which Nv is number of frames. The
dimensions of ResNet, VGG and C3D features are 2048,
4096 and 4096. We use two separate LSTM encoders to

1 , · · · , f m
Nv

1 , · · · , f a
Nv

process motion and appearance features individually ﬁrst,
and late fuse them in the designed memory module which
will be discussed in §3.2. In Fig. 2, we highlight the ap-
pearance encoder in blue and the motion encoder in orange.
The inputs fed into the two encoders are raw CNN mo-
tion features f m and appearance features f a, and the out-
puts are encoded motion and appearance features denoted
as om = [om

] and oa = [oa

1 , · · · , om
Nv

1, · · · , oa
Nv

].

Question representation. Each VideoQA dataset has
a pre-deﬁned vocabulary which is composed of the top K
most frequent words in the training set. The vocabulary
size K of each dataset is shown in Table 1. We repre-
sent each word as a ﬁxed-length learnable word embed-
ding and initialize with the pre-trained GloVe 300-D [19]
feature. We denote the question embedding as a sequence
of word embeddings f q = [f q
], in which Nq is
number of words in the question. We use another LSTM
encoder to process question embedding f q, as highlighted
in red in Fig. 2. The outputs are the encoded text features
oq = [oq

1 , · · · , f q
Nq

] .

1, · · · , oq
Nq

3.2. Heterogeneous video memory

Both motion and appearance visual features are crucial
for recognizing the objects and events associated with the
questions. Because these two types of features are heteroge-
neous, the straightforward combination cannot effectively
learn the video content. Thus, we propose a new hetero-
geneous memory to integrate motion and appearance visual
features, learn the joint attention, and enhance the spatial-
temporal inference.

32001

read 
head

β

t

write 
head

ɑ

t

read 
vector
r
t

content 
vectors

cm
t

ca
t

m1

m2

.
.
.

mS

memory slots M

ha
t

hm
t

hv
t

 om

 oa

Figure 3. Our designed heterogeneous visual memory which con-
tains memory slots M , read and write heads α, β, and three hidden
states hm, ha and hv.

Different to the standard external memory, our new het-
erogeneous memory accepts multiple inputs including en-
coded motion features om and appearance features oa, and
uses multiple write heads to determine the content to write.
Fig. 3 illustrates the memory structure, which is composed
of memory slots M = [m1, · · · , mS] and three hidden
states hm, ha and hv. We use two hidden states hm and ha
to determine motion and appearance contents which will be
written into memory, and use a separate global hidden state
hv to store and output global context-aware feature which
integrates motion and appearance information. We denote
the number of memory slots as S, and sigmoid function as
σ. For simplicity, we combine superscript m and a for iden-
tical operations on both motion and appearance features.

Write operation. Firstly we deﬁne the motion and ap-
to write to memory at t-th time as

pearance content cm/a
non-linear mappings from input and previous hidden state

t

cm/a

t

= σ(Wm/a

oc

om/a
t + Wm/a

hc

hm/a
t-1 + bm/a

c

)

(1)

Then we deﬁne αm/a
weights of cm/a

t

t

={αm/a

t,1 , . . . , αm/a

t,S } as the write

to each of S memory slot given by

am/a

t

=v

⊤

a tanh(Wm/a

hm/a
t-1 + bm/a

a

)

ha

cm/a
t + Wm/a
ca
exp(am/a
t,i )
j=1 exp(am/a
t,j )

PS

αm/a

t,i =

for i = 1 . . . S

(2)

t

satisfying αm/a
sum to 1. Uniquely, we also need to inte-
grate motion and appearance information and make a uni-
ﬁed write operation into current memory. Thus we estimate
the weights ǫt ∈ R3 of motion content αm
t , appearance
content αa

t and current memory content Mt-1 given by

et =v

⊤

e tanh(Whehv

t-1 + (Wmecm

t + Waeca

t ) + be)

ǫt,i =

exp(et,i)
j=1 exp(et,j)

P3

for i = 1 . . . 3

(3)

Figure 4. Our re-designed question memory with memory slots
M , read and write heads α, β, and hidden states hq.

The memory M can be updated at each time step by

Mt = ǫt,1αm
t

cm
t + ǫt,2αa
t

ca
t + ǫt,3Mt-1

(4)

t

in which the write weights αm/a
for memory slots deter-
mine how much attention should different slots pay to cur-
rent inputs, while the modality weights ǫt determine which
of motion or appearance feature (or none of them if non-
informational) from current inputs should the memory pay
more attention to. Through this designed memory-write
mechanism, we are able to integrate motion and appear-
ance features to learn joint attention, and memorize differ-
ent spatio-temporal patterns of this video in a synchronized
and global context.

Read operation. The next step is to perform an atten-
tional read operation from the memory M to update mem-
ory hidden states. We deﬁne the weights of reading from
memory slots as βt={βt,1, . . . , βt,S} given by

bt =v

⊤

b tanh(Whbhv

t-1 + (Wmbcm

t + Wabca

t ) + bb)

βt,i =

exp(bt,i)
j=1 exp(bt,j)

PS

for i = 1 . . . S

(5)

The content rt read from memory is the weighted sum of
each memory slot rt = PS
i=1 βt,i ·mi in which both motion
and appearance information has been integrated.

Hidden states update. The ﬁnal step is to update all

three hidden states ha, hm and hv

hm/a

t

= σ(Wm/a

hh hm/a

oh om/a
t-1 + Wm/a
)

rh rt + bm/a
h
rhrt + bv
t-1 + Wv
h)

+ Wm/a
hhhv

t

t = σ(Wv
hv

(6)

(7)

The global memory hidden state at all time steps hv
will
be taken as our ﬁnal video features.
In next section, we
will discuss how to generate global question features.
In
Section 3.4, we will introduce how to interact video and
question features for answer inference.

1:Nv

42002

mSm2m1htctrtmemory slots MInput otβtɑtwrite headread headcontent vectorread vector...q3.3. External question memory

The existing deep learning based VideoQA methods of-
ten misunderstand the complex questions because they un-
derstand the questions based on local word information. For
example, for question “Who drives by a hitchhiking man
who is smoking?”, traditional methods are easily trapped
by the local words and fail to generate the right attention to
the queried person (the driver or the smoker). To address
this challenging problem, we introduce the question mem-
ory to learn context-aware text knowledge. The question
memory can store the sequential text information, learn rel-
evance between words, and understand the question from
the global point of view.

We redesign the memory networks [6, 16, 23, 25] to per-
sistently store previous inputs and enable interaction be-
tween current inputs and memory contents. As shown in
Fig. 4, the memory module is composed of memory slots
M = [m1, m2, · · · , mS] and memory hidden state hq. Un-
like the heterogeneous memory discussed previously, one
hidden state hq is necessary for the question memory. The
inputs to the question memory are the encoded texts oq.

Write operation. We ﬁrst deﬁne the content to write to

the memory at t-th time step as cq

t which is given by

cq
t = σ(Wocoq

t + Whchq

t-1 + bc)

(8)

as a non-linear mapping from current input oq
t and pre-
vious hidden state hq
t-1 to content vector cq
Then
t .
we deﬁne the weights of writing to all memory slots
αt={αt,1...αt,i...αt,S} such that
a tanh(Wcacq

t + Whahq

at = v⊤

t-1 + ba)

αt,i =

exp(at,i)
j=1 exp(at,j)

PS

for i = 1 . . . S

(9)

satisfying αt sum to 1. Then each memory slot mi is up-
dated by mi = αt,ict + (1 − αt,i)mi for i = 1 . . . S.

Read operation. The next step is to perform atten-
tional read operation from the memory slots M. We deﬁne
the normalized attention weights βt={βt,1...βt,i...βt,S} of
reading from memory slots such that

Figure 5. Multimodal fusion layer. An LSTM controller with hid-
den state st attends to relevant visual and question features, and
combines them to update current state.

3.4. Multimodal fusion and reasoning

In this section, we design a dedicated multimodal fusion
and reasoning module for VideoQA, which can attend to
multiple modalities such as visual and textual features, then
make multi-step reasoning with reﬁned attention for each
modality. Our design is inspired by Hori et al. [9] which
proposed to generate video captions by combining different
types of features such as video and audio.

Fig. 5 demonstrates our designed module. The hidden

1:Nv

and question memory hq

states of video memory hv
1:Nq
are taken as the input features. The core part is an LSTM
controller with its hidden state denoted as s. During each it-
eration of reasoning, the controller attends to different parts
of the video features and question features with temporal
attention mechanism, and combines the attended features
with learned modality weights φt, and ﬁnally updates its
own hidden state st.

Temporal attention. At t-th iteration of reasoning, we
t and cq
t by attending to
t and question features
are

ﬁrst generate two content vectors cv
different parts of visual features hv
hq
t . The temporal attention weights γv
computed by

and γq

1:Nv

1:Nq

bt = v⊤

b tanh(Wcbcq

t + Whbhq

t-1 + bb)

gv/q = v

v/q ⊤
g

βt,i =

exp(bt,i)
j=1 exp(bt,j)

PS

for i = 1 . . . S

(10)

γv/q
i =

tanh(Wv/q
exp(gv/q
Nv/q
j=1 exp(gv/q

)

j

i

P

g st-1 + Vv/q

g hv/q + bv/q

g

)

for i = 1 . . . Nv/q

(12)

)

The content rt read from memory is the weighted sum of
each memory slot content rt = PS

i=1 βt,i · mi.

Hidden state update. The ﬁnal step of t-th iteration is

to update the hidden state hq

t as

hq
t = σ(Wohoq

t + Wrhrt + Whhhq

t-1 + bh)

(11)

cv/q
t =

We take the memory hidden state of all time steps hq
1:Nq
as the global context-aware question features which will be
used for inference in Section 3.4.

and shown by the dashed lines in Fig. 5. Then the attended
content vectors cv/q

and the transformed dv/q

are

t

t

Nv/q
X

i=1

γv/q
i

hv/q

i

, dv/q

t = ReLU(Wv/q

d

cv/q
t +bv/q

d

) (13)

Multimodal fusion. The multimodal attention weights
t } are obtained by interacting the previous hid-

t , φq

φt = {φv

52003

qqhNqh1cvcq⨁ dvdqst-1ϕvϕqxtγ1γNv    γ1γNqVisual featuresQuestion features...vh1hNvvvvq...qstttttden state st-1 with the transformed content vectors dv/q

t

3.6. Implementation details

pv/q
t = v⊤

p tanh(Wv/q

p st-1 + Vv/q

p dv/q

t + bv/q

p

φv/q
t =

)

exp(pv/q
t ) + exp(pq
t )

t

exp(pv

)

(14)

The fused knowledge xt is computed by the sum of dv/q
with multimodal attention weights φv/q such that

t

xt = φv
t

dv
t + φq
t

dq
t

(15)

Multi-step reasoning. To complete t-th iteration of rea-
soning, the hidden state st of LSTM controller is updated
by st = LSTM(xt, st-1). This reasoning process is iterated
for L times and we set L = 3. The optimal choice for L
is discussed in §4.4. The hidden state sL at last iteration is
the ﬁnal representation of the distilled knowledge. We also
apply the standard temporal attention on encoded video fea-
tures om and oa as in ST-VQA [10], and concatenate with
sL to form the ﬁnal answer representation sA.

3.5. Answer generation

We now discuss how to generate the correct answers

from answer features sA.

Multiple-choice task is to choose one correct answer out
of K candidates. We concatenate the question with each
candidate answer individually, and forward each QA pair to
obtain the ﬁnal answer feature {sA}K
i=1, on top of which we
use a linear layer to provide scores for all candidate answers
s = {sp, sn
K−1} in which sp is the correct answer’s
score and the rest are K − 1 incorrect ones. During training,
we minimize the summed pairwise hinge loss [10] between
the positive answer and each negative answer deﬁned as

1 , · · · , sn

Lmc =

K−1

X

i=1

max(0, m − (sp − sn

i ))

(16)

and train the entire network end-to-end. The intuition of
Lmc is that the score of the true QA pair should be larger
than any negative pair by a margin m. During testing, we
choose the answer of highest score as the prediction.
In
Table 1, we list the number of choices K for each dataset.

Open-ended task is to choose one correct word as the
answer from a pre-deﬁned answer set of size C. We ap-
ply a linear layer and softmax function upon sA to pro-
vide probabilities for all candidate answers such that p =
p sL + bp) in which p ∈ RC . The training
softmax(W⊤
error is measured by cross-entropy loss such that

Lopen = −

C

X

c=1

1{y = c} log(pc)

(17)

in which y is the ground truth label. By minimizing Lopen
we can train the entire network end-to-end. In testing phase,
the predicted answer is provided by c∗ = arg maxc(p).

We implemented our neural networks in PyTorch [18]
and updated network parameters by Adam solver [13] with
batch size 32 and ﬁxed learning rate 10−3. The video and
question encoders are two-layer LSTMs with hidden size
512. The dimension D of the memory slot and hidden state
is 256. We set the video and question memory sizes to 30
and 20 respectively, which are roughly equal to the maxi-
mum length of the videos and questions. We have released
our code for boosting further research1.

4. Experiments and Discussions

We evaluate our model on four benchmark VideoQA

datasets and compare with the state-of-the-art techniques.

4.1. Dataset descriptions

In Table 1, we show the statistics of the four VideoQA
benchmark datasets and the experimental settings from their
original paper including feature types, vocabulary size,
sampled video length, number of videos, size of QA splits,
answer set size for open-ended questions, and number of
options for multiple-choice questions.

TGIF-QA [10] contains 165K QA pairs associated with
72K GIF images based on the TGIF dataset [14]. TGIF-QA
includes four types of questions: 1) counting the number of
occurrences of a given action; 2) recognizing a repeated ac-
tion given its count; 3) identifying the action happened be-
fore or after a given action, and 4) answering image-based
questions. MSVD-QA and MSRVTT-QA were proposed
by Xu et al. [27] based on MSVD [4] and MSVTT [28]
video sets respectively. Five different question types ex-
ist in both datasets, including what, who, how, when and
where. The questions are open-ended with pre-deﬁned an-
swer sets of size 1000. YouTube2Text-QA [30] collected
three types of questions (what, who and other) from the
YouTube2Text [7] video description corpus. The video
source is also MSVD [4]. Both open-ended and multiple-
choice tasks exist.

4.2. Result analysis

TGIF-QA result. Table 2 summarizes the experiment re-
sults of all four tasks (Count,Action,Trans.,FrameQA) on
TGIF-QA dataset. We compare with state-of-the-art meth-
ods ST-VQA [10] and Co-Mem [5] and list the reported ac-
curacy in the original paper. For repetition counting task
(column 1), our method achieves the lowest average L2 loss
compared with ST-VQA and Co-Mem (4.02 v.s. 4.28 and
4.10). For Action and Trans. tasks (column 2,3), our method
signiﬁcantly outperforms the other two by increasing accu-
racy from prior best 0.682 and 0.743 to 0.739 and 0.778 re-
spectively. For FrameQA task (column 4), our method also

1https://github.com/fanchenyou/HME-VideoQA

62004

Dataset

Feature

Vocab size Video len Video num

Question num

Ans size MC num

Train

Val

Test

TGIF-QA [10]
MSVD-QA [27]
MSRVTT-QA [27]
Youtube2Text-QA [30] ResNet+C3D

VGG+C3D
VGG+C3D

ResNet+C3D

8,000
4,000
8,000
6,500

35
20
20
40

71,741
1,970
10,000
1,970

125,473
30,933
158,581
88,350

13,941
6,415
12,278
6,489

25,751
13,157
72,821
4,590

1746
1000
1000
1000

5
NA
NA
4

Table 1. Dataset statistics of four VideoQA benchmark datasets. The columns from left to right indicate dataset name, feature types,
vocabulary size, sampled video length, number of videos, size of QA splits, answer set size (Ans size) for open-ended questions, and
number of options for multiple-choice questions (MC num).

Method

ST-VQA [10]
Co-Mem [5]

Ours

Question type

Count (loss)

Action

Trans.

FrameQA

4.28
4.10

4.02

0.608
0.682

0.671
0.743

0.739

0.778

0.493
0.515

0.538

Table 2. Experiment results on TGIF-QA dataset.

with the ST-VQA [10], Co-Mem [5] and AMU [27] on
MSRVTT-QA. Similar to the trend on MSVD-QA, our
method outperforms the other models on three major ques-
tion types (what, who, how), and achieves the best overall
accuracy of 0.330.

achieves the best accuracy of 0.538 among all three meth-
ods, outperforming the Co-Mem by 4.7%.

Question type and # instances

Method

What Who
8419
4552

How When Where
370

58

28

ST-VQA [10]
Co-Mem [5]
AMU [27]

0.181
0.196
0.206

0.500
0.487
0.475

0.838
0.816
0.835

0.724
0.741
0.724

0.286
0.317
0.536

All

13427

0.313
0.317
0.320

Ours

0.224

0.501

0.730

0.707

0.429

0.337

Table 3. Experiment results on MSVD-QA dataset.

MSVD-QA result. Table 3 summarizes the experiment re-
sults on MSVD-QA. It’s worth mentioning that there is high
class imbalance in both training and test sets, as more than
95% questions are what and who while less than 5% are
how, when and where. We list the numbers of their test in-
stances in the table for reference. We compare our model
with the ST-VQA [10], Co-Mem [5] and current state-of-
the-art AMU [27] on MSVD-QA. We show the reported ac-
curacy of AMU in [27], while we accommodate the source
code of ST-VQA and implement Co-Mem from scratch to
obtain their numbers. Our method outperforms all the oth-
ers on both what and who tasks, and achieves best over-
all accuracy of 0.337 which is 5.3% better than prior best
(0.320). Even though our method slightly underperforms
on the How, When and Where questions, the difference are
minimal (40,2 and 3) regarding the absolute number of in-
stances due to class imbalance.

Method

Question type

What Who

How When Where

All

ST-VQA [10]
Co-Mem [5]
AMU [27]

0.245
0.239
0.262

0.412
0.425
0.430

0.780
0.741
0.802

0.765
0.690
0.725

0.349
0.429
0.300

0.309
0.320
0.325

Ours

0.265

0.436

0.824

0.760

0.286

0.330

Table 4. Experiment results on MSRVTT-QA dataset.

MSRVTT-QA result. In Table 4, we compare our model

Task

Method

Question type and # instances

What Who Other
2489

2004

97

All

4590

Avg. Per-class

Multi-choice

Open-ended

r-ANL [30]

0.633

0.364

0.845

0.520

Ours

0.831

0.778

0.866

0.808

r-ANL [30]

0.216

0.294

0.804

0.262

Ours

0.292

0.287

0.773

0.301

0.614

0.825

0.438

0.451

Table 5. Experiment results on YouTube2Text-QA dataset.

YouTube2Text-QA result.
In Table 5, we compare
our methods with the state-of-the-art
r-ANL [30] on
YouTube2Text-QA dataset.
It’s worth mentioning that r-
ANL utilized frame-level attributes as additional supervi-
sion to augment learning while our method does not. For
multiple-choice questions, our method signiﬁcantly outper-
forms r-ANL on all three types of questions (What, Who,
Other) and achieves a better overall accuracy (0.808 v.s.
0.520). For open-ended questions, our method outperforms
r-ANL on what queries and slightly underperforms on the
other two types. Still, our method achieves a better over-
all accuracy (0.301 v.s. 0.262). We also report the per-
class accuracy to make direct comparison with [30], and
our method is better than r-ANL in this evaluation method.

4.3. Attention visualization and analysis

In Figs. 1 and 6, we demonstrate three QA examples with
highlighted key frames and words which are recognized by
our designed attention mechanism. For visualization pur-
pose, we extract the visual and textual attention weights
from our model (Eq. 12) and plot them with bar charts.
Darker color stands for larger weights, showing that the cor-
responding frame or word is relatively important.

Fig. 1 shows the effectiveness of understanding complex
question with our proposed question memory. This question
intends to query the female driver though it uses another
relative clause to describe the man. Our model focuses on
the correct frames in which the female driver is driving in
the car and also focuses on the words which describe the
woman but not the man. In contrast, ST-VQA [10] fails to

72005

...

...

Q: What does a man cut into thin slivers with a cleaver? (answer: onion)

to 0.306 when the number of reasoning iteration L increases
from 1 to 3, and seems to saturate at L = 5 (0.307), while
drops to 0.304 at L = 7. To balance performance and speed,
we choose L = 3 for our experiments throughout the paper.

Visual 
Attention

Question 
Attention

Visual 
Attention

Question 
Attention

A: Our model: onion        Existing model: potato

(a)

...

...

Q: What does a woman demonstrate while a man narrates? (answer: exercise)

A: Our model: exercise     Existing model: barbell

(b)

Figure 6. Visualization of multimodal attentions learned by our
model on two QA exemplars. Highly attended frames and words
are highlighted.

identify the queried person as its simple temporal attention
is not able to gather semantic information in the context of
a long sentence.

In Fig. 6(a), we provide an example showing that our
video memory is learning the most salient frames for the
given question while ignoring others.
In the ﬁrst half of
the video, it’s difﬁcult to know whether the vegetable is
onion or potato, due to the lighting condition and camera
view. However, our model smartly pays attention to frames
in which the onion is cut into pieces by combining both
question words “a man cut” and the motion features, and
thus determines the correct object type by onion pieces (but
not potato slices) from appearance hint.

Fig. 6(b) shows a typical example illustrating that jointly
learning motion and appearance features as our heteroge-
neous memory design is superior to attending to them sepa-
rately such as Co-Mem [5]. In this video, a woman is doing
yoga in a gym, and there is a barbell rack at the background.
Our method successfully associated the woman with the ac-
tion of exercising, while Co-Mem [5] incorrectly pays at-
tention to the barbell and fails to utilize motion information
as they separately learn motion and appearance attentions.

4.4. Ablation study

We perform two ablation studies to investigate the effec-
tiveness of each component of our model. We ﬁrst study
how many iterations of reasoning is sufﬁcient in the de-
signed multimodal fusion layer. After that, we make a com-
parison of variants of our model to evaluate the contribution
of each component.
Reasoning iterations. To understand how many iterations
of reasoning are sufﬁcient for our VideoQA tasks, we test
different numbers and report their accuracy. The valida-
tion accuracy on MSVD-QA dataset increases from 0.298

Dataset
MSVD

MSRVTT

EF

LF

0.313
0.309

0.315
0.321
Table 6. Ablation study of different architectures.

0.315
0.312

E-M
0.318
0.319

V-M Q-M V+Q
0.337
0.320
0.330
0.325

Different architectures. To understand the effectiveness of
our designed memory module, we compare several variants
of our models and evaluate on MSVD-QA and MSRVTT-
QA, as shown in Table 6. Early Fusion (EF) is indeed ST-
VQA [10] which concatenates raw video appearance and
motion features at an early stage, before feeding into the
LSTM encoder. Late Fusion (LF) model uses two separate
LSTM encoders to encode video appearance and motion
features and then fuses them by concatenation. Episodic
Memory (E-M) [26] is a simpliﬁed memory network em-
bodiment and we use it as the visual memory to compare
against our design. Visual Memory (V-M) model uses our
designed heterogeneous visual memory (M v in Fig. 2) to
fuse appearance and motion features and generate global
context-aware video features. Question Memory (Q-M)
model uses our redesigned question memory only (M q in
Fig. 2) to better capture complex question semantics. Fi-
nally, Visual and Question Memory (V+Q M) is our full
model which has both visual and question memory.

In Table 6, we observe consistent trend that using mem-
ory networks (e.g., E-M,V-M,V+Q) to align and integrate
multimodal visual features is generally better than simply
concatenating them (e.g., EF,LF). In addition, our designed
visual memory (V-M) has shown its strengths over episodic
memory (E-M) and other memory types (Table 3-5). Fur-
thermore, using both visual memory and question memory
(V+Q) increases the performance by 2-7%.

5. Conclusion

In this paper, we proposed a novel end-to-end deep learn-
ing framework for VideoQA, with designing new external
memory modules to better capture global contexts in video
frames, complex semantics in questions, and their interac-
tions. A new multimodal fusion layer was designed to fuse
visual and textual modalities and perform multi-step reason-
ing with gradually reﬁned attention. In empirical studies,
we visualized the attentions generated by our model to ver-
ify its capability of understanding complex questions and
attending to salient visual hints. Experimental results on
four benchmark VideoQA datasets show that our new ap-
proach consistently outperforms state-of-the-art methods.

82006

ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.

[19] Jeffrey Pennington, Richard Socher, and Christopher Man-
In

ning. GloVe: Global vectors for word representation.
EMNLP, 2014.

[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015.

[21] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan
Wierstra, and Timothy Lillicrap. Meta-learning with
memory-augmented neural networks. In ICML, 2016.

[22] Karen Simonyan and Andrew Zisserman. Very deep convo-

lutional networks for large-scale image recognition. 2014.

[23] Sainbayar Sukhbaatar, Jason Weston, and Rob Fergus. End-

to-end memory networks. In NIPS, 2015.

[24] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatio-temporal features with
3d convolutional networks. In ICCV, 2015.

[25] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory

networks. In ICLR, 2015.

[26] Caiming Xiong, Stephen Merity, and Richard Socher. Dy-
namic memory networks for visual and textual question an-
swering. In ICML, 2016.

[27] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, and Yueting Zhuang. Video question answer-
ing via gradually reﬁned attention over appearance and mo-
tion. In ACMMM, 2017.

[28] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A
large video description dataset for bridging video and lan-
guage. In CVPR, 2016.

[29] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and
Alex Smola. Stacked attention networks for image question
answering. In CVPR, 2016.

[30] Yunan Ye, Zhou Zhao, Yimeng Li, Long Chen, Jun Xiao,
and Yueting Zhuang. Video question answering via attribute-
augmented attention network learning. In SIGIR, 2017.

[31] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee
Kim. End-to-end concept word detection for video caption-
ing, retrieval, and question answering. In CVPR, 2016.

[32] Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang,
Yuan-Hong Liao, Juan Carlos Niebles, and Min Sun. Lever-
aging video descriptions to learn video question answering.
In AAAI, 2017.

References

[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret
Mitchell, C Lawrence Zitnick, Dhruv Batra, and Devi Parikh.
VQA: Visual question answering. In ICCV, 2015.

[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
visual question answering. In CVPR, 2018.

[3] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan

Klein. Neural module networks. In CVPR, 2016.

[4] David L Chen and William B Dolan. Collecting highly par-

allel data for paraphrase evaluation. In ACL, 2011.

[5] Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia.
Motion-appearance co-memory networks for video question
answering. In CVPR, 2018.

[6] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing

machines. In arXiv preprint arXiv:1410.5401, 2014.

[7] Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkar-
nenkar, Subhashini Venugopalan, Raymond Mooney, Trevor
Darrell, and Kate Saenko. Youtube2text: Recognizing and
describing arbitrary activities using semantic hierarchies and
zero-shot recognition. In ICCV, 2013.

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[9] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang,
Bret Harsham, John R Hershey, Tim K Marks, and Kazuhiko
Sumi. Attention-based multimodal fusion for video descrip-
tion. In ICCV, 2017.

[10] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and
Gunhee Kim. TGIF-QA: Toward spatio-temporal reasoning
in visual question answering. In CVPR, 2017.

[11] Aniruddha Kembhavi, MinJoon Seo, Dustin Schwenk,
Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Text-
book question answering for multimodal machine compre-
hension. In CVPR, 2017.

[12] Jin-Hwa Kim, Sang-Woo Lee, Donghyun Kwak, Min-Oh
Heo, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang.
Multimodal residual learning for visual QA. In NIPS, 2016.
[13] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015.

[14] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault,
Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. Tgif: A
new dataset and benchmark on animated gif description. In
CVPR, 2016.

[15] Chao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng
Wang, Anton van den Hengel, and Ian Reid. Visual question
answering with memory-augmented networks.
In CVPR,
2018.

[16] Ying Ma and Jose Principe. A taxonomy for neural memory

networks. In arXiv preprint arXiv:1805.00327, 2018.

[17] Mateusz Malinowski and Mario Fritz. A multi-world ap-
proach to question answering about real-world scenes based
on uncertain input. In NIPS, 2014.

[18] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-

92007

