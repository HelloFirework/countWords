Propagation Mechanism for Deep and Wide Neural Networks

Dejiang Xu

Mong Li Lee

Wynne Hsu

School of Computing,

National University of Singapore

{xudj,leeml,whsu}@comp.nus.edu.sg

Abstract

Recent deep neural networks (DNN) utilize identity map-
pings involving either element-wise addition or channel-
wise concatenation for the propagation of these identity
mappings.
In this paper, we propose a new propagation
mechanism called channel-wise addition (cAdd) to deal
with the vanishing gradients problem without sacriﬁcing
the complexity of the learned features. Unlike channel-
wise concatenation, cAdd is able to eliminate the need to
store feature maps thus reducing the memory requirement.
The proposed cAdd mechanism can deepen and widen exist-
ing neural architectures with fewer parameters compared to
channel-wise concatenation and element-wise addition. We
incorporate cAdd into state-of-the-art architectures such as
ResNet, WideResNet, and CondenseNet and carry out ex-
tensive experiments on CIFAR10, CIFAR100, SVHN and
ImageNet to demonstrate that cAdd-based architectures are
able to achieve much higher accuracy with fewer parame-
ters compared to their corresponding base architectures.

1. Introduction

After the impressive performance of deep neural network
[17] at the ImageNet [3] 2012 competition, there has been
a rapid introduction of new neural network architectures
with improved performance. These architectures include
ResNet [7], Wide-ResNet [32], ResNeXt [31], Pyramid-
Net [6], DenseNet [12], Dual Path Network [2], MobileNet
[10], Shake-Shake Net [4], ShufﬂeNet [33], CondenseNet
[11] etc. Recent attempts to use the sheer power of numer-
ous GPU servers to automatically search for good neural
network architectures have led to NASNet [34], EAS [1],
ENAS [22] and AmoebaNets [24]).

One trend that is consistent across these neural network
architectures is that a deeper and wider neural network often
yields better performance. However, a deep and wide net-
work suffers from the problem of vanishing gradient as well
as a quadratic growth in the number of parameters. Further,
the computational complexity and memory requirements

also escalate in these architectures which are formidable for
scalable learning in real world applications.

It remains non-trivial to design neural architectures that
can address the vanishing gradient problem, yet are capable
of capturing complex features to signiﬁcantly lift the per-
formance of the learned models which are also sufﬁciently
small in size to reduce power consumption and potentially
be deployable on IoT devices and mobile platforms.

We observe that the depth of a neural architecture is key
to its performance. Current neural architectures use iden-
tity mappings in the form of skip connections to increase
their depth. This allows the gradient to be passed back-
wards directly thus allowing the increase in depth without
the issue of vanishing gradients. The propagation of these
identity mappings from one block to the next is achieved
either via element-wise addition (eAdd) [7] or channel-wise
concatenation (cCon) [12]. Figure 1 shows these propaga-
tion mechanisms.
In eAdd, addition is performed on the
corresponding elements, hence the input size for each unit
remains the same. On the other hand, cCon concatenates the
inputs from all the preceding units, thus increasing the in-
put size quadratically for each subsequent units. As a result,
cCon can learn more complex features, however, it needs
more memory to store the concatenated inputs [23] .

(a) Element-wise addition

(b) Channel-wise concatenation

Figure 1. Propagation mechanism of element-wise Addition
(eAdd) and channel-wise Concatenation (cCon).

9220

Figure 2. General architecture of a deep neural network using cAdd.

In this work, we propose a novel propagation mecha-
nism, called channel-wise addition (cAdd), that can be eas-
ily incorporated into any of the state-of-art neural architec-
tures to reduce the computational and memory requirements
while achieving high accuracy. In order to keep the memory
requirement small, we sequentially produce small residual
part and add them to part of channels of the identity part in
one unit. The unit is repeated multiple times until all the
channels are added. With this, the depth of a network is
increased while the number of parameters is reduced.

Figure 2 shows a general architecture of a neural net-
work using cAdd. It has several stages and the cAdd units
within each stage have the same resolution for both input
and output feature maps to allow for channel-wise addition.
The resolution across the stages may be different as they
could be down-sampled by transition units. This design has
several advantages:

1. Vanishing gradient can be alleviated since cAdd also
has a shortcut that allows the gradient to bypass the
unit directly.

2. Less memory is needed since cAdd adds back the out-
put features instead of concatenation, thus keeping the
input size the same for each unit.

3. More complex features can be generated as cAdd sig-

niﬁcantly increases the width and depth of CNNs.

4. Fewer parameters and FLOPs compared to existing

neural networks with the same width and height.

Extensive experiments on CIFAR10 [16], CIFAR100
[16], SVHN [21] and ImageNet [3] datasets demon-
strate that cAdd-based neural networks consistently achieve
higher accuracy with fewer number of parameters compared
to the original networks that use either eAdd or cCon.

2. Related Work

Neural Networks using eAdd. Depth is vital for neural
networks to achieve higher performance. However, it is
hard to optimize deep neural networks. Element-wise ad-
dition was introduced in ResNet [7] to signiﬁcantly deepen
the neural network and ease the training process [8]. It has
been widely used in many deep neural networks, including
Inception-ResNet [29], Wide-ResNet [32], ResNeXt [31],
PyramidNet [6], Shake-Shake Net [4], and ShufﬂeNets

[33]. It is also adopted by AlphaGo [26] and the automati-
cally designed architectures, like NASNet [34], ENAS [22],
and AmoebaNets [24].

The width of a neural network is also crucial to gain
accuracy. Unlike ResNet, which achieves higher perfor-
mance by simply stacking element-wise addition, Wide-
ResNet widens the network by increasing the input chan-
nels along the depth. Experimental results show a 16-layer
Wide-ResNet can outperform a thousand-layer ResNet in
both accuracy and efﬁciency. For Wide-ResNet, the in-
crease in width occurs only between stages, and the input
size within a stage remains the same. PyramidNet gradually
increases its width in a pyramid-like shape with a widen-
ing step factor, which has been experimentally proven to
improve generalization ability. ResNext uses multi-branch
element-wise additions, by replacing the only branch with
a set of small homogeneous branches. Simply adding more
branches can improve the the performance of ResNext. In-
stead of directly summing up all the small branches, Shake-
shake Net uses a stochastic afﬁne combination to signiﬁ-
cantly improve the generalization ability.

Unlike the manually designed architectures which need
human expertise, automatically designed architectures
search the entire architecture space to ﬁnd the best de-
sign. Although the learned architectures have many differ-
ent small branches, the distinct characteristic is that they all
use eAdd to sum up the branches. Since eAdd requires the
output size to be at least the same or larger than the input
size, a neural network can go deeper or wider, but not both
when the number of parameters is limited. In contrast, the
proposed cAdd can both deepen and widen a neural network
for the same number of parameters.

Neural Networks using cCon. Channel-wise concatena-
tion was ﬁrst used in DenseNet [12]. Features from all pre-
ceding units are used as inputs to generate a small number
of outputs, which are passed to subsequent units. While this
strengthens feature propagation and reuse, not all prior fea-
tures need to be used as inputs to every subsequent layer.
As such, CondenseNet selects only the most relevant in-
puts through an learned group convolution [11].
It spar-
siﬁes the convolutional layer by pruning away unimportant
ﬁlters during the condensing stage, and optimizes the spar-
siﬁed model in the second half of the training process. Con-
denseNet is more efﬁcient than the compact MobileNes [10]
and ShufﬂeNets [33], which are designed for mobile de-
vices using depth-wise separable convolutions [15].

9221

Figure 3. Propagation mechanism within four cAdd units.

The best models obtained from automatically designed
architectures all utilize cCon, especially for the combination
of all the cell outputs. Since concatenation increases the in-
put size linearly, this also increases the number of param-
eters and memory requirements. In contrast, the proposed
cAdd is able to keep the input size constant by adding back
outputs to selected inputs.

3. Channel-wise Addition

The proposed cAdd propagation mechanism combines
the beneﬁts of eAdd and cCon to deepen and widen neural
networks yet using fewer parameters. The key idea is that
each unit must generate a small number of output channels
whereby the generated outputs are then added back to the
corresponding skipped connections to form the inputs to the
next unit. Figure 3 shows the propagation across 4 units
using cAdd. The ﬁrst cAdd unit generates 3 outputs which
are then added back to the ﬁrst 3 skipped connections to
form the inputs to the second cAdd unit.

Suppose M is the number of input channels. To ensure
all the skipped connections are covered, we group the in-
put channels of each unit into non-overlapping parts. The
size of each part is controlled by a parameter α such that
each part has exactly ⌊M/α⌋ channels except the ﬁnal part
which has ⌊M/α⌋ + R channels where R is the remaining
In Figure 3, we see that the second input part
channels.
(green parallelogram) has ⌊M/α⌋ = 3 channels and these
are covered by the addition to the outputs of the cAdd2 unit,
while the third and ﬁnal input part (blue parallelogram) has
⌊M/α⌋ + R = 4 channels and they are covered by the ad-
dition to the outputs of the cAdd3 unit.
In order for the addition operation to make sense, the
number of generated outputs from a unit must match the
number of channels to be covered in the next unit. Mathe-
matically, the number of output channels for the kth cAdd
unit is given by:

(⌊M/α⌋ ,
⌊M/α⌋ + M % α,

k% α 6= 0
otherwise

(1)

We show that the cAdd propagation mechanism is able
to alleviate the vanishing gradient problem. Let X =
[x1, x2,··· , xM ] be the input to a cAdd unit, and Y =
[y1, y2,··· , yN ] be the output of X after passing through
the non-linear transformation function F(·) of the convo-
lutional block, which may have different layers consisting
of batch normalization [14] (BN), rectiﬁed linear units [5]
(ReLU), and convolution layers (Conv). that is,

Y = F(X)

(2)

The cAdd unit adds back its outputs Y into part of its
for the next unit as follows:

inputs X to form the inputs X

′

X′ = X + TY

(3)

where T is a M × N sparse matrix, Tij = 1 if yj is to be
added to xi.

With Equations 2 and 3, we have:

X′ = X + T · F(X)

(4)

Let us consider the propagation from cAdd unit s to
cAdd unit e whose corresponding inputs are Xs and Xe
respectively. We have

Xe = Xs +

e−1

Xi=s

Ti · F(Xi)

(5)

Let E be the error loss. The gradient on Xs can be ex-

pressed as:

e−1

∂E
∂Xe

∂E
∂Xs =

∂Xe
∂Xs =

∂E

∂Xe 1 +

∂Xs ! (6)
∂F(Xi)

Ti ·

Since it is not possible for all the training samples within

Xi=s
i=s Ti · ∂F (Xi)
a batch to have the componentPe−1

in Equa-
tion (6) to be always equal to -1, this implies that gradient
is unlikely to be 0, thus alleviating the vanishing gradient
problem.

∂Xs

9222

4. Architectures using cAdd

The proposed cAdd propagation mechanism can be eas-
ily incorporated into existing neural networks. There are
two kinds of units in neural networks, namely, basic and
bottleneck units. We use the following notations for the dif-
ferent layers in a unit:

• Conv(I, O, L, L). Convolution layer with I input
channels, O output channels, and kernel size L × L.
• BN(I). Batch normalization with I input channels.
• ReLU. Rectiﬁed linear unit.
We ﬁrst consider networks that use eAdd propagation
mechanism. In the eAdd basic unit, the number of output
channels must be the same as that of the input channels for
element-wise addition. This constraint is no longer required
when we replace eAdd by cAdd. Recall Equation 1 where
the number of output channels in cAdd is determined by α.
A large α will imply a signiﬁcant reduction in the number of
output channels. Figure 4 shows that the initial convolution
layer of cAdd basic unit is Conv(M , M/α, L, L) instead of
eAdd basic unit from Conv(M , M , L, L).

(a) eAdd

(b) cAdd

Figure 4. Basic unit using eAdd vs cAdd

The eAdd bottleneck unit uses convolution layer with
kernel size 1×1 [19] to spatially combine large numbers of
input feature maps with few parameters (see Figure 5(a)).
Due to the element-wise addition requirement, an additional
convolution layer is needed to expand the size of the out-
put channels back to M . However, this is not needed for
channel-wise addition. Figure 5(b) shows the correspond-
ing bottleneck unit that uses cAdd.

Adapting cCon-based neural networks to use the cAdd
propagation mechanism is straightforward. Instead of using
the growth rate g to determine the number of output chan-
nels in both the basic and bottleneck units, we use Equa-
tion 1. Figure 6 shows the basic unit using cAdd vs cCon
where the convolution layer is Conv(M , M/α, L, L) in-
stead of Conv(M , g, L, L).

Similar adaptations can be made to neural architecture

variants such as PyramidNet [6].

(a) eAdd

(b) cAdd

Figure 5. Bottleneck unit using eAdd vs cAdd

(a) cCon

(b) cAdd

Figure 6. Basic unit using cCon vs cAdd

We analyze the number of parameters required in a neu-
ral architecture that uses cAdd vs eAdd or cCon. We assume
that the neural architecture has U basic units, and each unit
has M input channels with size of h × w. For fair compar-
ison, we assume that the growth rate g for a cCon unit is
M/α so that the cCon unit has the same number of output
as cAdd. Table 1 gives the number of parameters required.

eAdd basic unit
corresponding cAdd unit U ∗ M 2 ∗ L2 ∗ (1/α + 1/α2)
cCon basic unit

U ∗ M 2/α ∗ L2 +

Number of Parameters
2 ∗ U ∗ M 2 ∗ L2

corresponding cAdd unit U ∗ M 2/α ∗ L2

Table 1. Comparison of number of parameters required.

(M/α)2 ∗ L2 ∗ (U 2 − U )/2

We see that a neural network using cAdd has approxi-
mately 2α times fewer parameters compared to a network
that uses eAdd. That is, with the same number of param-
eters, the depth of a neural network using cAdd can be in-

creased by 2α, or the width can be increased by √2α com-

pared to using eAdd. Such an increase can improve the
generalization ability of the neural networks, thus leading
higher accuracy. Clearly, the number of parameters required
by cCon in Table 1 have more parameters than cAdd. The

residual part of (M/α)2 ∗ L2 ∗ (U 2 − U )/2 is introduced

by concatenation operation.

9223

We also compare the number of FLOPs required Table 2
shows that a neural network using cAdd requires approxi-
mately 2α and (1 + U −1
2α ) times fewer FLOPs compared to
a network that uses eAdd and cCon respectively.

eAdd basic unit
corresponding cAdd unit

cCon basic unit

Number of FLOPs
2M 2L2hwU + M hwU
(1/α + 1/α2)M 2L2hwU +

1/αM hwU
1/αM 2L2hwU +

1/α2M 2L2hw(U 2 − U )/2

corresponding cAdd unit

1/αM 2L2hwU + 1/αM hwU

Table 2. Comparison of FLOPs required.

5. Experimental Evaluation

We carry out experiments to compare the performances
of neural architectures that use cAdd, eAdd and cCon. We
incorporate cAdd into three widely used CNN architectures,
namely ResNet, WRN and CondenseNet, and call them
cResNet, cWRN and cCondenseNet respectively. Each ar-
chitecture has 3 stages.

We train these networks using stochastic gradient de-
scent with nesterov momentum [28] of 0.9 without damp-
ening, and a weight decay of 10−4. For fair comparison, all
the training settings (learning rate, batch size, epochs, and
data augmentations) are the same as in the original papers,
unless otherwise speciﬁed. The following datasets are used:

• CIFAR10 [16]:

It has 10 object classes with 6,000
32x32 color images for each class. There are 50,000
images for training and 10,000 for testing.

• CIFAR100 [16]: It has 100 classes with 600 32x32
color images for each class. The training and testing
sets contain 50,000 and 10,000 images respectively.

• SVHN [21]: This has over 600,000 32x32 images of
real-world house numbers. There are 73,257 images
for training, 26,032 for testing, and additional 531,131
for extra training.

• ImageNet [3]: It has 1,000 classes. The training set
has 1.2 millon images and validaiton set has 50,000
images.

5.1. ResNet vs cResNet

In this set of experiments, we examine the performance
of ResNet with cResNet. Like ResNet, we train all the
cResNet (α = 7) for 300 epochs with batch size of 128.
The learning rate starts from 0.1 and is reduced by 10 after
the 150th and 225th epoch. For the 1224-layer cResNet, the
initial learning rate is 0.01 for the ﬁrst 20 epochs, and then
go back to 0.1 to continue the training.

Table 3 gives the results of ResNet, pre-activation
ResNet, and cResNet on CIFAR10, CIFAR100, and SVHN

datasets. ResNet-20 with 0.27 million parameters has a
depth of 20, and its width for three stages are 16, 32, and
64 respectively. In contrast, cResNet-86 with comparable
number of parameters (0.21 million) has a depth of 86, and
its corresponding width are 84, 112, and 140. The increased
width and depth in cResNet-86 over ResNet-20 enables it
to have a much higher accuracy on CIFAR10. In fact, the
accuracy of cResNet-86 beats ResNet-56 on CIFAR10, CI-
FAR100 and SVHN datasets, which has four times the num-
ber of parameters.

When we increase the width of cResNet-86 to 168-196-
308 so that it has comparable number of parameters (0.84
million) as ResNet-56, the gap in accuracy widens signiﬁ-
cantly. cResNet-86 even outperforms ResNet-110, ResNet-
164 and pre-activation ResNet-164, which have twice the
number of parameters. We see that cResNet-170 with
1.65 million parameters gives the best results over all the
ResNets and pre-activation ResNets.

We observe that ResNet-1202 has 19.4 million parame-
ters, yet its error rate is higher than ResNet-110, possibly
due to over-ﬁtting [7]. On the other hand, our cResNet-
1224, which is much wider and deeper than ResNet-1202,
achieves the lowest top-1 error rate of 4.06 on CIFAR10.

Figure 7 shows the top-1 error rates of the cResNet and
ResNet on CIFAR10 dataset as we vary the number of pa-
rameters. Clearly, the error rate of cResNet is always lower
than ResNet for the same number of parameters. We ob-
serve that ResNet at its lowest error rate has 8 times more
parameters than cResNet.

Figure 7. ResNet vs. cResNet on CIFAR10.

5.2. WRN vs cWRN

Next, we compare the performance of WRN with
cWRN. Similar to WRN, we train cWRN (α = 7) for 200
epochs with batch size of 128. The learning rate starts from
0.1, annealed by a factor of 5 times after the 60th, 120th,
and 160th epochs for CIFAR10 and CIFAR100 datasets.
For SVHN dataset, cWRN are trained for 160 epochs with
batch size of 128, and is optimized by dividing the initial
learning rate of 0.01 by 10 after the 80th and 120th epochs.

9224

Architecture

Width

# Params

CIFAR10

CIFAR100

SVHN

ResNet-20 [7]
ResNet-32 [7]
ResNet-44 [7]
ResNet-56 [7]
ResNet-110 [7]
ResNet-164 [7]
ResNet-1001 [7]
ResNet-1202 [7]
Pre-activation ResNet -164 [8]
Pre-activation ResNet -1001 [8]
cResNet-86
cResNet-86
cResNet-170
cResNet-1224

16-32-64
16-32-64
16-32-64
16-32-64
16-32-64
16-32-64
16-32-64
16-32-64

64-128-256
64-128-256
84-112-140
168-196-308
196-224-280
196-224-280

-
-

8.75
7.51
7.17
6.97

0.27M
0.46M
0.66M
0.85M
1.73M 6.61 ± 0.16
1.70M
10.2M
19.4M
1.7M
10.2M
0.21M 6.37 ± 0.09
0.84M 4.76 ± 0.07
1.65M 4.33 ± 0.04
13.185M

7.93
5.46
4.92

4.06

-
-
-

28.25 ∗
27.22 †
25.16
27.82

-

24.33
22.71

-
-
-

2.49 ∗
2.01 †

-
-
-
-
-

27.45 ± 0.11
23.35 ± 0.17
21.33 ± 0.20

-

2.09 ± 0.07
2.04 ± 0.07
1.92 ± 0.06

-

Table 3. Top-1 error rate of ResNet and cResNet. Width is the number of input channels in the three stages. ∗ indicates results are from
[30] and † shows results are from [13], Results for cResNet are averaged over 5 runs in the format of “mean±std”.

Architecture

Width

# Params

CIFAR10

CIFAR100

SVHN

WRN-52-1 [32]
WRN-16-4 [32]
WRN-40-4 [32]
WRN-16-8 [32]
cWRN-130-2
cWRN-130-4
cWRN-172-6
cWRN-172-8
cWRN-88-13

16-32-64

64-128-256
64-128-256
128-256-512
98-126-154
196-252-308
294-378-462
392-504-616
637-819-1001

6.43
5.02
4.53
4.27

0.76M
2.75M
8.95M
11.00M
0.39M 6.32 ± 0.10
1.52M 4.87 ± 0.09
4.41M 4.34 ± 0.09
7.80M 4.26 ± 0.07
10.90M 4.04 ± 0.09

28.89
24.03
21.18
20.43

2.08
1.85

-
-

26.75 ± 0.20
22.4 ± 0.19
20.87 ± 0.13
19.78 ± 0.17
19.33 ± 0.13

1.99 ± 0.06
1.81 ± 0.05

-
-
-

Table 4. Top-1 error rate of WRN and cWRN. Width is the number of input channels in the three stages. Results for cWRN are averaged
over 5 runs in the format of “mean±std”.

Table 4 gives the results. All the cWRN are much wider
and deeper compared to the corresponding WRN, and are
able to achieve lower top-1 error rates with fewer parame-
ters on all three datasets. Speciﬁcally, cWRN-130-2 outper-
forms WRN-52-1 with half the parameters (0.39 million vs.
0.76 million) on all three datasets. Overall, cWRN-88-13
gives the best performance.

Figure 8 shows the top-1 error rates of the cWRN and
WRN on CIFAR10 dataset as we vary the number of pa-
rameters. We see that cWRN has 1.4 times fewer parame-
ters than WRN for the same error rate.

5.3. CondenseNet vs cCondenseNet

Figure 8. WRN vs. cWRN on CIFAR10.

Finally, we examine the performance of using cAdd in
CondenseNet. We train all the cCondenseNet (α = 6) for
300 epochs with a batch size of 64, and use a cosine-shaped
learning rate from 0.1 to 0. For cCondenseNet-254, we train
for 600 epochs with a dropout rate [27] of 0.1 to ensure fair
comparison with CondenseNet-182.

Table 5 shows the results with cCondenseNet-254 giv-

ing the best performance on both CIFAR10 and CIFAR100.
It has 456 input channels which is 38 times the width of
CondenseNet-182, and 254 convolutional layers which is
1.4 times the depth of CondenseNet-182. We see that
cCondenseNet-146 and cCondenseNet-110 are much wider
and deeper with fewer parameters compared to their coun-
terparts CondenseNet-86 and CondenseNet-50. In partic-

9225

Architecture

Width

# Params

CIFAR10

CIFAR100

CondenseNet-50 [11]
CondenseNet-74 [11]
CondenseNet-86 [11]
CondenseNet-98 [11]
CondenseNet-110 [11]
CondenseNet-122 [11]
CondenseNet-182 [11]
cCondenseNet-110
cCondenseNet-146
cCondenseNet-254

8-16-32
8-16-32
8-16-32
8-16-32
8-16-32
8-16-32
12-24-48

96-144-192
168-216-264
456-504-576

6.22
5.28
5.06
4.83
4.63
4.48
3.76

0.22M
0.41M
0.52M
0.65M
0.79M
0.95M
4.22M
0.19M 5.74 ± 0.08
0.50M 4.64 ± 0.08
4.16M 3.40 ± 0.09

-
-

23.64

-
-
-

18.47

27.40 ± 0.15
23.44 ± 0.11
18.20 ± 0.13

Table 5. Top-1 error rate of CondenseNet and cCondenseNet. Width is the number of input channels or growth rate in the three stages.
Results for cCondenseNet are averaged over 5 runs in the format of “mean±std”.

Architecture

Width

# Params Top-1 error rate Top-5 error rate

CondenseNet-74 (G=C=4) [11]
cCondenseNet-246 (G=C=4)
ResNet-50 [7]
ResNet-101 [7]
cResNet-72

8-16-32-64-128

192-288-384-552-768
64-256-512-1024-2048
64-256-512-1024-2048
280-280-560-1120-2240

4.8M
4.7M
25.6M
44.5M
23.3M

26.2%
25.4%
24.7%
23.6%
23.7%

8.3%
7.7%
7.8%
7.1%
7.1%

Table 6. One-crop validation results on ImageNet. Width is the number of input channels or growth rate in the ﬁve stages

1 and top-5 error rates, we see that cAdd-based architec-
ture requires only half the number of parameters. Similarly,
cCondenseNet-246 with 0.1 million fewer parameters out-
performs CondenseNet-74.

5.5. Depth vs. Width

Depth and width are vital dimensions for neural net-
works to achieve higher performance. Depth controls the
complexity of the learned features. A deeper neural network
can learn more complex features, while a wider network en-
ables more features to be involved in the ﬁnal classiﬁcation.
For cAdd based architectures, we have the ﬂexibility of
either increasing the depth or the width or both and still
retain approximately the same number of parameters. Here,
we investigate the impact of the depth and width of a cAdd
based architecture on its classiﬁcation accuracy.

We use ResNet-56 with 0.85 million parameters, and
CondenseNet-86 with 0.52 million parameters as the base-
lines, and design different cResNet and cCondenseNet with
approximately the same number of parameters at varying
depth and width. Table 7 shows the results on both CI-
FAR10 and CIFAR100 datasets.

We observe that the best performances are attained when
the increase in depth is balanced with the increase in width,
indicating that both depth and width are equally important.
This makes sense as the performance of a neural net de-
pends both on the number of features as well as the com-
plexity of these features.

9226

Figure 9. CondenseNet vs. cCondenseNet on CIFAR10.

ular, although cCondenseNet-110 has 0.03 million fewer
parameters than CondenseNet-50, its top-1 error rate is
smaller than that of CondenseNet-50, 5.74% versus 6.22%.
Figure 9 shows the top-1 error rates on CIFAR10. We
see that cCondenseNet has 1.4 times fewer parameters than
CondenseNet for the same error rate.

5.4. Experiments on ImageNet

We also compare the performances of the various neu-
ral architectures on the ImageNet dataset. Table 6 shows
the results. We observe that cResNet-72 achieves much
lower top-1 and top-5 error rates compared to ResNet-
50 with similar number of parameters. When we com-
pare ResNet-101 and cResNet-72 which has similar top-

Architecture

# Params

Width

Depth CIFAR10 CIFAR100

ResNet-56 [7] (Base-line)
cResNet-44
cResNet-86
cResNet-128
cResNet-170
cResNet-212
cResNet-254
cResNet-296
cResNet-338
CondenseNet-86 [11] (Base-line)
cCondenseNet-38
cCondenseNet-74
cCondenseNet-110
cCondenseNet-182
cCondenseNet-218
cCondenseNet-254
cCondenseNet-290
cCondenseNet-326

16-32-64

0.85M
0.86M 280-308-336
0.81M 196-224-252
0.89M 168-196-224
0.88M 140-168-196
0.89M 126-154-182
0.88M 112-140-168
0.86M 100-128-156
0.82M
91-119-147
0.52M
0.51M 312-360-384
0.49M 240-288-312
0.51M 216-240-288
0.51M 168-192-240
0.51M 144-192-216
0.49M 120-168-216
0.51M 120-168-192
0.51M 120-144-192

8-16-32

56
44
86
128
170
212
254
296
338
86
38
74
110
182
218
254
290
326

6.97
6.20
5.91
5.84
5.66
5.50
5.88
5.95
5.94
5.06
5.08
4.89
4.73
4.61
4.94
4.89
4.86
5.11

28.25
27.14
27.09
26.94
27.04
26.93
27.39
27.77
27.55
23.64
25.29
24.19
24.02
23.46
23.56
23.74
24.19
24.24

Table 7. Top-1 error rate of cResNet, and cCondenseNets on CIFAR10, and CIFAR100 datasets.

(a) eAdd vs. cAdd

(b) cCon vs. cAdd

Figure 10. Neuron weights in the convolutional layer of architectures using cAdd, eAdd and cCon.

5.6. Norm of Weights

6. Conclusion

Weight norm measures the activeness of neurons dur-
ing feature learning [9, 11, 18, 20]. Figure 10 shows the
mean and standard deviation of the neuron weights within
each convolutional layer of the trained neural networks us-
ing cAdd (ResNet-26 and DenseNet-28), eAdd (ResNet-
26), and cCon (DenseNet-28). We observe that the neurons
in cAdd based networks have larger weights than eAdd and
cCon based networks. This indicates that cAdd neurons are
more active compared to eAdd and cCon neurons during
feature learning. One possible reason could be that many
of the weights in eAdd and cCon are close to zero and can
be pruned without sacriﬁcing accuracy [9, 18, 20]. With
cAdd, we are able to reduce the number of weights, leading
to fewer parameters and higher accuracy.

In this paper, we have proposed a new channel-wise ad-
dition propagation mechanism to deepen and widen neu-
ral networks with signiﬁcantly fewer parameters. We have
described how we can adapt state-of-the-art deep neu-
ral networks, namely, ResNet, WRN and CondenseNet
to use cAdd. Extensive comparative experiments on CI-
FAR10, CIFAR100, SVHN and ImageNet datasets show
that cAdd based neural architectures (cResNet, cWRN and
cCondenseNet) consistently outperform their correspond-
ing counterparts with higher accuracy, fewer parameters and
lower computational costs. Future work includes investigat-
ing how channel-wise addition can be incorporated to fur-
ther enhance the compact neural architectures for real world
deployment.

9227

References

[1] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun
Wang. Efﬁcient architecture search by network transforma-
tion. In AAAI, 2018.

[2] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin,
Shuicheng Yan, and Jiashi Feng. Dual path networks. CoRR,
abs/1707.01629, 2017.

[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
Imagenet: A large-scale hierarchical image database. IEEE
Computer Vision and Pattern Recognition (CVPR), 2009.

[4] Xavier Gastaldi.

Shake-shake regularization.

ICLR,

abs/1705.07485, 2017.

[5] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep
sparse rectiﬁer neural networks. In International Conference
on Artiﬁcial Intelligence and Statistics, volume 15, pages
315–323, 2011.

[6] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyra-
midal residual networks. the IEEE Conference on Computer
Vision and Pattern Recognition, 2016.

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. CVPR, 2015.

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

Identity mappings in deep residual networks. ECCV, 2016.

[9] Yihui He, Xiangyu Zhang, and Jian Sun. Channel prun-
ing for accelerating very deep neural networks. CoRR,
abs/1707.06168, 2017.

[10] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. CoRR,
abs/1704.04861, 2017.

[11] Gao Huang, Shichen Liu, Laurens van der Maaten, and Kil-
ian Q. Weinberger. Condensenet: An efﬁcient densenet using
learned group convolutions. CVPR, 2017.

[12] Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely

connected convolutional networks. CVPR, 2016.

[13] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-
ian Q. Weinberger. Deep networks with stochastic depth.
CoRR, abs/1603.09382, 2016.

[14] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. International Conference on Machine Learn-
ing, 2015.

[15] Lukasz Kaiser, Aidan N. Gomez, and Franc¸ois Chollet.
Depthwise separable convolutions for neural machine trans-
lation. CoRR, abs/1706.03059, 2017.

[16] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Master’s thesis, Department of
Computer Science, University of Toronto, 2009.

[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, pages 1097–1105, 2012.

[18] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. CoRR,
abs/1608.08710, 2016.

[19] Min Lin, Qiang Chen, and Shuicheng Yan. Network in net-
International Conference on Learning Representa-

work.
tions, 2013.

[20] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,
Shoumeng Yan, and Changshui Zhang. Learning efﬁcient
convolutional networks through network slimming. CoRR,
abs/1708.06519, 2017.

[21] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural
images with unsupervised feature learning. NIPS, 2011.

[22] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and
Jeff Dean. Efﬁcient neural architecture search via parame-
ter sharing. International Conference on Machine Learning,
2018.

[23] Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li,
Laurens van der Maaten, and Kilian Q. Weinberger.
Memory-efﬁcient
CoRR,
abs/1707.06990, 2017.

implementation of densenets.

[24] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V.
Le. Regularized evolution for image classiﬁer architecture
search. CoRR, abs/1802.01548, 2018.

[25] L. Sifre. Rigid-motion scattering for image classiﬁcation.

Ph. D. thesis, 2014.

[26] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis
Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lu-
cas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timo-
thy Lillicrap, Fan Hui, Laurent Sifre, George van den Driess-
che, Thore Graepel, and Demis Hassabis. Mastering the
game of go without human knowledge. Nature, 550,354-
359, 2017.

[27] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: A simple
way to prevent neural networks from overﬁtting. Journal of
Machine Learning Research, 15:1929–1958, 2014.

[28] Ilya Sutskever, James Martens, George Dahl, and Geoffrey
Hinton. On the importance of initialization and momen-
tum in deep learning. volume 28 of Proceedings of Ma-
chine Learning Research, pages 1139–1147, Atlanta, Geor-
gia, USA, 17–19 Jun 2013. PMLR.

[29] Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke.
Inception-v4, inception-resnet and the impact of residual
connections on learning. AAAI, 2017.

[30] Yan Wang, Lingxi Xie, Chenxi Liu, Ya Zhang, Wenjun
Zhang, and Alan L. Yuille. SORT: second-order response
transform for visual recognition. CoRR, abs/1703.06993,
2017.

[31] Saining Xie, Ross B. Girshick, Piotr Doll´ar, Zhuowen Tu,
and Kaiming He. Aggregated residual transformations for
deep neural networks. CVPR, 2017.

[32] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-

works. BMVA, 2016.

[33] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. CoRR, abs/1707.01083, 2017.

[34] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V.
Le. Learning transferable architectures for scalable image
recognition. CoRR, abs/1707.07012, 2017.

9228

