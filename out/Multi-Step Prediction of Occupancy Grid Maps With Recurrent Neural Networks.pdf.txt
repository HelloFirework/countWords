Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural Networks

Nima Mohajerin and Mohsen Rohani∗

Huawei Noah’s Ark, Markham, ON, Canada

{nima.mohajerin, mohsen.rohani}@huawei.com

Abstract

We investigate the multi-step prediction of the driv-
able space, represented by Occupancy Grid Maps (OGMs),
for autonomous vehicles. Our motivation is that accu-
rate multi-step prediction of the drivable space can efﬁ-
ciently improve path planning and navigation resulting in
safe, comfortable and optimum paths in autonomous driv-
ing. We train a variety of Recurrent Neural Network (RNN)
based architectures on the OGM sequences from the KITTI
dataset. The results demonstrate signiﬁcant improvement
of the prediction accuracy using our proposed difference
learning method, incorporating motion related features,
over the state of the art. We remove the egomotion from
the OGM sequences by transforming them into a common
frame. Although in the transformed sequences the KITTI
dataset is heavily biased toward static objects, by learn-
ing the difference between consecutive OGMs, our pro-
posed method provides accurate prediction over both the
static and moving objects. A video of the performance of
our method on the KITTI dataset is available at https:
//youtu.be/Bskd0Z7eLFE.

1. Introduction

Determining the environment state is a crucial ability for
autonomous vehicles to have. Particularly for path planning
and navigation, the state of the environment is required to
determine safe areas to drive, that is, the drivable space. In a
driving scenario, the classic approach is to detect and track
objects, and based on the state of the tracked objects, deter-
mine the drivable space [21, 5]. Figure 1(a) illustrates the
general pipeline of the classic approach. As the new data
comes in, the objects in the environment are detected (and
classiﬁed). To keep track of objects, each object is given an
ID. The Data Association module assigns existing IDs to the
detected objects, or initiates new IDs if there is no matched
tracks. The states of the tracked objects are updated using

∗The authors contributed equally.

data

Object

Detection

Data

Association

Model

Assignment

Object

Classiﬁer

State
update

Drivable

space

to path planning

(a) Classic approach for drivable space detection.

data

Convert
to OGM

OGM

Multi-Step
Prediction

Predicted OGMs

. . .

to path planning

(b) OGM multi-step prediction.

Figure 1. Classic approach versus ours for providing the drivable
space to a motion planner in autonomous driving.

models which are assigned to them based on their class. Fi-
nally, the updated states (possibly along with the assigned
models) are used to determine the drivable space.

The various stages in the classic approach require hand-
engineered features. Additionally, any error occurred at an
early stage of the pipeline can propagate towards the next
stages leading to an overall performance degradation. An
alternative approach is to predict the drivable space at a level
closer to the sensory information, requiring fewer abstrac-
tion levels. However, for such prediction to be useful, ﬁrst a
proper representation of the drivable space should be chosen
which can be directly obtained from sensor measurements.
One such proper means in this context is an Occupancy Grid
Map (OGM) [3, 24]. An OGM divides the space around
the ego-vehicle into equal cells which represent the occu-
pancy state of surrounding regions, i.e., free or occupied.
The state of the cells can be constructed using Laser range
ﬁnders (and images [22]).

Given an OGM, methods such as Probabilistic Road-
Maps and Rapidly exploring Random Trees can be em-
ployed to generate a collision-free path [24]. In the presence
of dynamic objects, however, a single OGM is insufﬁcient

110600

for guaranteeing a safe passage in general, as it does not
properly represent the moving objects. Planning based on
the updated OGMs results in inefﬁcient and uncomfortable
paths requiring frequent steering corrections or even inad-
missible paths. In such scenarios, one common approach
is to “grow” the occupied regions by padding the occupied
cells, which artiﬁcially creates a safe distance from obsta-
cles. However, the amount of padding is difﬁcult to deter-
mine a priori and depends on the objects’ type, velocity,
etc. Without proper object detection and classiﬁcation, the
padding size has to be chosen above a safe threshold which
may result in severely restricting the ego-vehicle movement
in crowded areas, such as urban environments, and failing
to detect useful drivable spaces in highway scenarios where
the objects may move relatively fast.

Alternatively, it is possible to provide multi-steps of
OGM prediction into the future [2, 4, 10]. In a dynamic en-
vironment, such predictions can provide the drivable space
to a planning algorithm without the need for the several
stages required in the classic approach. The idea is to en-
code the observed states of the environment, represented
as OGMs, into latent variables of a recursive model, from
which the future OGMs can be produced recursively. Re-
current Neural Networks (RNNs) are, therefore, a potential
model candidate, which have also been successfully applied
to various other problems with dynamic nature, such as un-
supervised video prediction [12, 7], multi-step prediction of
mobile robots [15] and visual odometry [25]. Moreover, it
is shown that RNNs can produce promising results in the
context of OGM prediction [2, 4].

OGMs may also be looked upon as black-and-white im-
ages. However, predicting images in a video stream, i.e.,
video prediction [7, 13, 23], can be studied either as a
regression problem, or multinomial classiﬁcation (with at
least 256 classes for gray-scale images), where OGM pre-
diction is a binary classiﬁcation problem. As a sanity check,
we employed PredNet [12] and the CDNA model [7], but
both methods failed to provide a reliable OGM prediction.
Particularly, the occlusions that occur within OGMs, due to
the nature of range ﬁnder sensors, are difﬁcult to be dealt
with in the video prediction task where the primary goal
is to predict the future images as similar as possible to the
ground truth. Therefore, in this work we do not consider the
multi-step prediction of OGMs as a video prediction task.

In [2], a network of Gated Recurrent Units [1] (GRUs),
with dilated convolutions, is used whose states are initial-
ized as it receives and reconstructs observed OGMs, for a
limited number of steps. Then, the initialized states are em-
ployed to generate next OGMs, while the input OGMs are
Inspired by [2], we propose an approach to learn
blank.
the difference between consecutive OGMs as a compensa-
tion matrix. The current OGM, modiﬁed by the compensa-
tion matrix, provides the features for a predictive classiﬁca-

tion. This choice stems from the fact that the current OGM
provides a reliable prior for the drivable space. The ma-
jor source of discrepancy between consecutive OGMs is the
egomotion, which presumably is available (e.g., from the
Drive-By-Wire system or GPS). Therefore, in this work, the
observed OGMs are transferred to a common frame using
egomotion information. The common frame corresponds to
the ego-vehicle coordinate system at the present time, i.e.,
the frame at which we want to predict future OGMs.

The OGMs are also predicted into the future with respect
to the common frame. In fact, in our approach the drivable
space is predicted as OGMs in a frame which is absolute
over the observed history and the prediction period, but is
attached to the ego-vehicle. This is different to the approach
taken in [2] where a Spatial Transformer Module (STM)
[11] is employed to take account for the egomotion. Since
our intention is to predict OGMs suitable for planning, it
is reasonable to predict the OGMs regardless of the future
egomotion. Providing the environment state in an absolute
frame makes it possible for a planning algorithm to examine
and plan based on virtually all ego trajectories that are safe,
i.e., those which do not interfere with other objects’ paths
in the environment.

Training a network for OGM prediction is supervised as
the target output is the OGM. However, the targets directly
come from the sensors and do not need human supervision
to be generated. Such datasets are easy to collect which sig-
niﬁcantly reduces the human labor and improves the time-
to-market. One difﬁculty is that the number of static ob-
jects is quite often dominant in OGM datasets. Conse-
quently, the network can easily trap in a local minimum
where it assumes the surrounding environment is basically
static. Learning the difference between OGMs partially pre-
vents such local minima as the network only needs to com-
pensate for the moving objects. Additionally, we propose
to incorporate motion relevant information as inputs to the
networks, either by using standard optical-ﬂow extraction
algorithms such as the Farneback method [6], or by feed-
ing the OGM differences. Both methods are considered and
evaluated for performance and computation time.
Our contributions are summarized as follows,
• A novel RNN-based architecture is proposed to predict
OGMs over multi-steps into the future, based on learn-
ing the OGM differences between consecutive frames,
• Motion related features are employed which enhances

OGM prediction over dynamic objects signiﬁcantly,

• A conclusive comparison between various RNN-based

architectures is presented for the problem at hand.

In the following, relevant works are reviewed in Sec-
tion 2. Section 3 formulates the OGM prediction problem
and describes our methods to address it. Section 4 presents
the experimental results and discussions. We conclude the
paper in Section 5.

10601

2. Related Work

In [20], the authors propose a deep tracking scheme,
where a simple RNN is leveraged to learn OGM-to-OGM
mappings. The training data consists of OGMs that are built
by synthesized laser readings collected from a static sen-
sor. This work is improved and expanded in [2], where the
egomotion is handled with STMs, and convolutional GRUs,
with dilation, replace the vanilla RNN architecture. How-
ever, the two approaches in [20] and [2] are similar: a se-
quence of observed OGMs, followed by blank OGMs, are
fed to an RNN, whose output is trained on the observed
OGMs for the entire sequence. Therefore, the RNN acts as
an autoencoder while it receives the observed OGM, to ini-
tialize its states. The accumulated information in the states
is employed to predict the OGM evolution as the network
receives blank OGMs. However, feeding blank inputs cause
the output OGMs to fade out rapidly. Also, approaches
in [20] and [2] become computationally inhibitive as the
OGM size increases unless the OGMs are either resized or
an encoder-decoder architectures is employed.

Authors in [10] and [4] employ Dynamic OGMs
(DOGMa). DOGMa is the result of fusing a variety of
sensor readings using Bayesian ﬁltering, which associates
dynamic information to each cell as well as the occupancy
state [18]. The dynamic information contains the veloc-
ity and its uncertainty. An encoder-decoder structure with
Convolutional Long-Short-Term-Memory network (Con-
vLSTM) [27] receives DOGMa and produces the occu-
pancy probability of static regions alongside the anchor
boxes for dynamic objects [4]. An automatic output label
generation is also used which can have a “relatively high
false negative rate” [10]. Additionally, the datasets in [10]
and [4] are collected from a static sensor (parked car).

OGM prediction has already been used in path planning
In [17], Variable Length Markov
of autonomous robots.
Model is employed to predict the OGM in an environment
whose main occupants are humans. However, in [17], the
experiments are restricted to cases where there is only one
human exists in the vicinity of the autonomous robot. Sim-
ilarly in [19], multi-steps of predicted OGMs are stacked in
an XYT space (T stands for time) which is then used for
path planning of rescue robots. The XYT stack of predicted
OGMs is also used in other works, such as [9], however,
the OGMs are either updated using object models (classic
approach) or the main occupant are of the same object type,
e.g. human in [17] and [19].

3. OGM Prediction in Autonomous Driving

In practice, an OGM cell can be either fully occupied,
partially occupied or completely free. However, in this
work, it is assumed that a cell can be either fully occupied or
free, with a probability of occupancy associated to it. For-

mally, the state of the cell located at ith row and jth col-
umn of the OGM, at time-step k, is represented by a binary
random variable ck(i, j) ∈ {0, 1}. For now, assume that
the OGM at time-step k, Ok, is the probability matrix of
ck(i, j), that is,

Ok =hp(cid:0)ck(i, j)(cid:1)i =


...

p(cid:0)ck(1, 1)(cid:1)
p(cid:0)ck(Y, 1)(cid:1)

...

p(cid:0)ck(1, X)(cid:1)
. . .
. . .
. . . p(cid:0)ck(Y, X)(cid:1)




,

(1)
where X and Y indicate the grid size.
In order not to
lose generality, we assume that at each time-step, ck(i, j)s,
where i = 1, ..., Y and j = 1, ..., X, are independent with
possibly different distributions. However, ck(i, j)s are de-
pendent over time. In general, the probability of ck(i, j) is
conditioned on the set of random variables Ci,j ,

Ci,j =(cid:8)cv(m, n)|v = k − 1, k − 2, ...

m = i − αi, ..., i + βi,
n = j − αj, ..., , j + βj,
0 ≤ αi < i, 0 ≤ βi ≤ X − i,

(2)

0 ≤ αj ≤ j, 0 ≤ βj < Y − j(cid:9),

where αi, βi and αj, βj are integers and indicate some
neighborhood around i and j. In general, the values αi, βi
and αj, βj depend on a number of factors, such as the ego-
vehicle’s velocity and/or the speed of the surrounding ob-
jects. Therefore, we assign the extreme possible values to
them, in which case, (m, n) covers the entire OGM. There-
fore, the elements of OGM matrix in (1) are, in fact, condi-
tional probabilities,

p(cid:0)ck(i, j)|Ck−1, Ck−2, ...(cid:1),

Ck =(cid:8)ck(m, n)|m = 1, ..., Y ; n = 1, ..., X(cid:9),

and we deﬁne the OGM as the matrix of the conditional
probabilities (3),

(3)

(4)

Ok =hp(cid:0)ck(i, j)|Ck−1, Ck−2, ...(cid:1)i, i = 1, ..., Y ; j = 1, ...X.

(5)
Our goal is to predict the OGMs, as depicted in (5), over
multi-steps into the future. To this end, we employ and train
a variety of RNN-based architectures. The feedback con-
nections in an RNN are particularly useful to establish the
recursive dependency represented in (5). The RNN-based
models in this work are employed as sequence-to-sequence
maps, i.e., they receive a sequence of input OGMs and pro-
duce a sequence of OGMs, of the same length. Therefore,
given a ﬁxed sequence length, T , a prediction task is deﬁned
as feeding the input OGM sequence, O, to an RNN-based
model which generates the output OGM sequence, ˆO,

O = {Ok}, k = 1, ..., T,
ˆO = { ˆOk}, k = 1, ..., T,

(6a)

(6b)

10602

where ˆOk is the model output at time-step k whose form
depends on the model architecture. Based on [2] we initially
form the input sequence as follows,

Ok =(O∗

k, k = 1, ..., τI ,

[0], k = τI + 1, ...T,

(7)

k is the observed OGM at k and [0] represents a
where O∗
matrix of zeros whose size is the same as O∗
k, i.e., a blank
OGM. Later we will modify the input sequence. Equa-
tion (6) and (7) indicate that a prediction task consists of
two phases. During the ﬁrst phase, where k = 1, ..., τI , the
observed OGMs are given to the model. The ﬁrst phase
is mainly intended for RNN state initialization [16], and
therefore, we will refer to it as the initialization phase, or
init-phase for short. The second phase, namely the predic-
tion phase, starts at k = τ + 1 and is intended for multi-step
prediction, as the input OGMs are blank. The length of the
prediction phase is τP = T − τI .

It is worthwhile to mention that in [2], authors train their
network to reconstruct the input during the init-phase, there-
fore, their architecture behaves similar to an autoencoder.
From our point of view, such approach does not necessar-
ily result in extracting features that are useful for predic-
tion, rather, those features are useful for reconstructing the
inputs. In contrary, our models are trained to generate one-
step-ahead prediction during the init-phase. Therefore, we
force the models to learn useful features for prediction.

3.1. Base Architecture

To handle large OGMs, an encoder/decoder architecture
is employed, as depicted in Figure 2. The encoder mod-
ule consists of a number of convolutional layers, each fol-
lowed by a max pooling, to reduce the input OGM size and
extract features. Then, the features are passed through an
RNN, followed by a decoder module. The decoder module
upsamples the RNN output using transposed convolutional
layers. The number of layers in the encoder and the decoder
is the same. Also, the decoder’s output has the same XY di-
mension as the input OGM.

Figure 2. Base Architecture.

To account for the egomotion, the OGMs are transferred
to the frame at which the prediction starts, i.e., k = τ . We
use standard geometric image transformation using the dis-

placement in x, y and the heading directions over consec-
utive frames. This transformation is applied to all of the
OGMs in each prediction task. As a result, the OGMs (ob-
served and predicted per each prediction task) are treated as
if they are seen from a stationary sensor whose location is
the same as the sensor location at k = τ . To avoid notation
clutter, we use the same notation for the OGMs as before,
keeping in mind that they are all transformed accordingly.

3.2. Extended Architectures

We extend the base architecture by feeding the output

back to the network. Therefore, (7) becomes,

Ok =(cid:2)1 − u(k − τ − 1)(cid:3)O∗

k + u(k − τ − 1) ˆOk,

(8)

where ˆOk, is the predicted OGM at time-step k and u(.) is
the standard step function1. The output feedback connec-
tion incorporates the information, that are coded inside the
decoder module, back into the network.

The second extension is to include motion-related fea-
tures using Motion-Flow (MF) extraction algorithms (e.g.,
Farneback algorithm [6]), or a two-channel difference
method. An MF extraction algorithm, receives two con-
secutive OGMs and provides a tensor of the same size and
depth of two. Each of the two channels correspond to the
movement in X and Y directions. The two-channel differ-
ence method takes the difference between two past consec-
utive OGMs and places the result into two channels; one
channel for the positive values and the other for (the ab-
solute value of) the negative values. The resulting two-
channel tensor, from either of the methods, is then passed
through an encoder module, whose output is stacked with
the output of the OGM encoder. The extended architectures
is illustrated in Figures 3.

D

MFE

δk

Ok

O∗
k

ˆOk

En.

En.

S

Core
RNN

D

D : Delay

S : Stack

ˆOk+1

De.

Figure 3. Extensions over the base architecture. “MFE” stands for
Motion-related Feature Extraction. The switch δk changes posi-
tion at k = τI +1. “En.” and “De.” stand for Encoder and Decoder,
respectively.

3.3. Difference Learning Architecture

With the current technology, high resolution OGMs can
be constructed at 10 Hz rate and faster. In a normal driving
scenario, it is quite unlikely that the consecutive OGMs at

1 u(x) returns 1 for x ≥ 0 and 0 otherwise.

10603

……EncoderDecoderFeaturesPrediction in feature spaceInput OGMPredictedOGMCore RNN such rates differ dramatically. In other words, current OGM
provides a reasonable prior for predicting the next OGM.

We embed the above idea into a difference learning ar-
chitecture, which is based on a two-stage process as illus-
trated in Figure 4. The ﬁrst stage, depicted by Diff. Learn,
generates a compensation matrix, ∆Ok, whose size is the
same as the input OGM. The Diff. Learn module em-
ploys the extended architecture depicted in Figure 3. The
result of adding ∆Ok to the input OGM provides the ba-
sis on which the Classiﬁer module produces the next OGM
prediction. Therefore, the Diff. Learn module should im-
plicitly distinguish between the static and dynamic objects
which is then reﬂected in ∆Ok. The elements of ∆Ok are
real values in [−1, 1]. A value near zero corresponds to a
cell whose occupancy should not be altered, i.e., a free cell
or a cell occupied by a static object). Similarly, a value
closer to 1 (or -1) attempts to add (or clear) occupancy to
(from) the corresponding cells, indicating the cell is being
occupied by (or freed from) a dynamic object. The Clas-
siﬁer module reﬁnes the modiﬁed OGM further to predict
the next OGM, i.e., ˆOk+1. Figure 5 illustrated the detailed
difference-learning model. Note that the Classiﬁer module
can be a simple feed-forward or a recurrent network.

δk

Ok

O∗
k

ˆOk

Diff.
Learn

∆Ok

D

+

Classiﬁer

ˆOk+1

Figure 4. Difference OGM learning model.

D

MFE

Diff. Learn

δk

Ok

En.

En.

S

O∗
k

ˆOk

Core
RNN

D

De.

∆Ok+1

ˆOk+1

+

Classi-

ﬁer

Figure 5. Detailed architecture for the difference learning model.

3.4. Training Cost

As the problem at hand is classiﬁcation (occlusion vs.
free), we employ pixel-wise Cross-Entropy cost between
the output, ˆOk, and the target, O∗
k. Because the majority
of the cells are normally unoccupied, the cost is normalized
with respect to the number of occupied/free cells. Also, the
pixel-wise cost is multiplied by the visibility matrix as in
[2], to let the network handle occlusions.

To make sure the Diff. Learn module is effectively em-
ployed, two extra terms are added for the difference learn-
ing models. First, based on the idea that most of the objects
are static in a driving scenario, an L2 norm on the com-
pensation matrix, ∆Ok+1, is added to the total cost. This

term ensures that the non-zero elements inside ∆Ok+1 cor-
respond to the cells whose occupancy should be changed
during the prediction task, i.e., the dynamic objects. To
force the classiﬁer inputs towards the target, a multi-scale
Structural Similarity Index Metric [26] (SSIM) between the
summation result and the target OGM is added to the cost.

4. Experimental Results

The OGM sequences are obtained from the KITTI raw
dataset [8]. The raw Lidar point-clouds, collected at 10Hz,
are converted to bird’s eye view OGMs (with ground re-
moval) [14]. Then, the OGM sequences are partitioned into
segments of 20 frames, preserving the order within each
segment. Every segment corresponds to 2 seconds of OGM
measurement and is regarded as one sample. The init-phase
length is set to 10 steps, i.e., τ = 10. Since the models’
ability to handle occlusion is important to us, in addition to
the natural occlusions that exist in the raw data, we impose
a constant occlusion pattern over the OGMs by ﬁltering out
occupancies outside the front camera ﬁeld of view (FOV).
With a cell size equal to 20(cm)×20(cm), each OGM frame
consists of 256 × 256 cells, arranged in a square matrix,
corresponding to occupancies over 50 meters in front of the
car. The OGMs are visualized as 256 × 256 pixel, black and
white or gray-scale, images, where the former shows the ac-
tual occupancy and the latter illustrates the discretized oc-
cupancy probabilities. Each pixel represents one OGM cell.
Table 1 lists the models, and their conﬁgurations, trained
for this work. The ED (Encoder-Decoder) models provide
a basis for evaluation. The ED-Di (Encoder-Decoder with
Dilated convolutions) models correspond to the model pro-
posed in [2] along with two improvements based on using
the motion-related features. The Diff.1 and Diff.2 models
represent our proposed difference learning architecture.

The encoders’ architecture is identical across the mod-
els; they produce 64 × 64 × 32 features from 256 × 256 × n
inputs, where n = 1 for the OGM and n = 2 for motion-
ﬂow related features. The decoders are also similar across
the models; they convert back the 64 × 64 × 32 tensors to
256 × 256 × n, where n = 1 for Diff.1 and Diff.2 models,
and n = 2 for ED and ED-Di models. Also, the decoders in
Diff.1 and Diff.2 models employ a tanh() activation func-
tion on their last layer to produce compensation matrix val-
ues in [−1, 1] while the decoders in ED and ED-Di models
employ a linear activation to produce logits. The Core RNN
module consists of 4 ConvLSTM layers, where dilation at
the kth ConvLSTM layer is equal to k for the ED-Di model
and one for the rest. Therefore, in our settings, the ED-
Di model at #4 row in Table 1 is an attempt to replicate the
model proposed in [2]. Models #5 and #6 represent our sug-
gested improvements on #4. All of the models are trained
to predict one-step-ahead during the init-phase.

Table 2 lists the results. We evaluate the prediction accu-

10604

#
1
2
3
4
5
6
7
8
9
10
11
12

Model

ED

ED-Di

Diff.1

Diff.2

Cfg.
Base
Ext1.
Ext2.
Base
Ext1.
Ext2.
Base
Ext1.
Ext2.
Base
Ext1.
Ext2.

Arch.
Fig. 2
Fig. 3
Fig. 3
Fig. 2
Fig. 3
Fig. 3
Fig. 4
Fig. 5
Fig. 5
Fig. 4
Fig. 5
Fig. 5

Input
Eq. (7)
Eq. (8)
Eq. (8)
Eq. (7)
Eq. (8)
Eq. (8)
Eq. (8)
Eq. (8)
Eq. (8)
Eq. (8)
Eq. (8)
Eq. (8)

MFE
N/A

Farenback

Two-channel difference

N/A

Farenback

Two-channel difference

N/A

Farenback

Two-channel difference

N/A

Farenback

Two-channel difference

Di.
No
No
No
Yes
Yes
Yes
No
No
No
No
No
No

Diff. Learn

Classiﬁer

N/A
N/A
N/A
N/A
N/A
N/A

N/A
N/A
N/A
N/A
N/A
N/A

Base (Fig. 2)

Shaded area in Fig. 5
Shaded area in Fig. 5

Base (Fig. 2)

Shaded area in Fig. 5
Shaded area in Fig. 5

2 Layer Conv.
2 Layer Conv.
2 Layer Conv.
1 ConvLSTM
1 ConvLSTM
1 ConvLSTM

Table 1. The models trained in this work. ED models constitute the base line over each conﬁguration (Cfg.), ED-Di models are the closest
replica of the model proposed in [2], in our framework, Diff.1 and Diff.2 models represent our proposed models.

Architectures

Model

Cfg.

ED

ED-Di

Diff.1

Diff.2

Base
Ext.1
Ext2.
Base
Ext.1
Ext.2
Base
Ext.1
Ext.2
Base
Ext.1
Ext.2

Whole OGM

Objects only

Whole seq.

Prediction seq.

Whole seq.

Prediction seq.

TP

87.51
87.87
89.48
86.16
88.09
88.60
85.39
88.58
88.14
85.37
91.02
90.31

TN

97.85
87.29
98.69
97.41
89.81
97.90
99.35
99.07
97.86
99.39
99.69
98.53

S100
93.91
93.75
94.85
93.89
94.02
94.57
95.35
97.11
98.10
95.55
99.47
98.97

TP

85.37
85.25
87.50
86.01
85.89
87.15
84.01
85.03
86.67
81.60
88.36
88.89

TN

97.38
99.07
98.21
97.68
99.10
97.52
98.89
98.29
98.33
99.09
99.28
98.91

S100
92.95
95.49
94.31
93.10
96.92
95.52
94.26
97.85
97.12
94.45
98.41
97.83

TP

77.07
77.19
78.95
77.83
78.24
76.52
78.90
79.80
79.96
73.37
82.97
81.09

TN

97.88
98.94
98.77
98.01
98.55
96.26
99.34
99.12
98.89
99.30
98.81
99.02

S100
95.71
96.79
96.55
95.91
97.01
95.25
97.44
97.12
95.99
97.39
97.37
97.01

TP

70.39
70.28
72.76
71.84
71.25
69.42
72.06
77.02
74.41
69.91
79.91
77.43

TN

97.56
98.59
98.39
97.89
98.88
97.93
98.92
98.56
97.92
99.00
98.37
98.11

S100
95.01
96.02
95.77
95.14
97.03
93.87
96.39
97.00
96.28
96.57
96.73
96.97

t/f
(ms)

Ord.

3.8
9.2
4.2
4.0
9.8
4.7
4.0
10.2
5.7
4.3
12.1
6.1

11
12
5
9
7
10
6
3
4
8
1
2

Table 2. The OGM prediction accuracy measured by %True-Positive (%TP), %True-Negative (TN) and S100 (100×[standard SSIM])
measures. The decreasing order of prediction accuracy is listed under “Ord.”, where 1 corresponds to the best overall accuracy. The
bold-faced numbers correspond to the best performance on each column. Within each 3×3 block, the shaded row corresponds to the best
performance over the block. The “t/f” column lists the time it takes to generate one frame of prediction using GTX 1080Ti GPU.

racy using %True-Positive (TP), %True-Negative (TN) and
S100 measures. The S100 is the standard SSIM measure mul-
tiplied by 100, in order to be on the same scale as TP and
TN. The evaluation horizon is either the entire sequence (20
frames) listed under “Whole seq.”, or only over the predic-
tion length (the second 10 frames) listed under “Prediction
seq.”. Also, to evaluate the prediction accuracy over the ar-
eas occupied by the objects only, we employ the tracklets
in the KITTI dataset to mask the target and the predicted
OGMs and then compute the measures. The obtained re-
sults are listed under “Objects only” and partially reﬂect the
prediction accuracy of dynamic objects. Recall that the la-
bels and/or tracklets are not used for training.

Over each 3-by-3 block the shaded row highlights the re-
sult with the highest sum (TP+TN+S100), reﬂecting the best
performance among the three corresponding conﬁgurations.
The last column (titled Ord.) lists the order at which the pre-
diction accuracy decreases, that is, number 1 corresponds to
the best and 12 to the worst performance, respectively.

Based on the results, our proposed difference learning
model, using Farneback algorithm to extract motion-ﬂow
features, (model #11) outperforms the other models, how-
ever, it is also computationally the most expensive one. If

we switch to the two-channel difference method (model
#12), we gain a considerable improvement over the compu-
tation time, while losing a small amount of accuracy. Con-
sidering online deployment, model #12 can be readily em-
ployed onboard of an autonomous vehicle as it takes about
60ms to predict OGM over 1 second into the future. Addi-
tionally, it is clear that inclusion of the motion-related fea-
tures improves the prediction accuracy for all of the models
we consider in this work.

Figure 6. One image from the ego-vehicle POV corresponding to
the results shown in Figures 7 and 8. The arrows indicate the mov-
ing objects.

To study the predicted OGMs qualitatively, Figure 7 il-
lustrates 10 frames of prediction generated by the four mod-
els in their best conﬁguration. A dark red grid is overlaid on
every OGM as a reference for the inertial coordinate system.
Also, the dashed yellow lines show the border of the FOV.
The initial OGMs (under the column “Init-phase”) are bi-

10605

k Init-phase k Diff.2

Comp. Mat.

Diff.1

Comp. Mat.

ED-Di

ED

Figure 7. Qualitative results of the four models in their best conﬁguration. The numbers below k indicate time-step.

10606

Init-phase (k=9)

Diff.2 (k=19)

Diff.1 (k=19)

ED-Di (k=19)

ED (k=19)

Figure 8. Last frame of OGM prediction extracted from Figure 7 along with the last frame of initial OGMs.

nary matrices. Therefore, they are illustrated as black and
white images. The Init-phase OGMs indicate that there are
three moving objects, two cars followed by a bus, high-
lighted by red arrows. One frame of the actual scene is
illustrated in Figure 6. The two cars are turning to their
right. However, by the end of the Init-phase, it is difﬁcult to
determine the turning direction for the bus.

The generated compensation matrix from Diff.1 and
Diff.2 models are illustrated under “Comp. Mat.” for each
of the two models. In the compensation matrices, green and
red pixels correspond to numbers in (0, 1] and [−1, 0), re-
spectively, and black pixels correspond to 0. To assess the
prediction quality, the pixels of predicted OGMs are color
coded as green, blue and red which correspond to true pos-
itive, false positive and false negative results.

The qualitative results conﬁrm the superiority of Diff.2
model over the rest; the moving objects are tracked pre-
cisely and occlusions are handled properly. Figure 8 shows
the last predicted frame extracted from Figure 7. An impor-
tant factor to notice is the inadequacy of the ground truth as
a target which the prediction should be compared against. In
fact, an OGM only shows the visible border of the object.
Therefore, a false positive is not undesirable everywhere on
the OGMs. Over the predicted OGM by Diff.2 in Figure 8,
the areas highlighted by the dashed pink rectangles (A and
B) show a relatively large area of false positive. Rectangle
A shows the border of the bus, grabbed from the tracklets.
The ground truth OGM (green and red pixels together) only
provides the visible border of the bus, however, intuitively
we can say the area inside the rectangle A is fully occupied.
In this case, the false positive over A is desirable. The B
rectangle shows the other possible direction of the bus.

The box, labeled C, illustrates another area where a false
positive is desirable.
In fact, this box is out of the FOV
where no visible ground truth is provided. However, the
network is managed to track the moving object out of the
FOV and into the occluded area. In Diff.1 predicted OGM,
however, some of the false positive areas are likely undesir-
able. For instance, the circle D on Diff.1 OGM shows that
the areas between two static objects (notice the circle D on
the Init-phase OGM) and the bus are joined and the network
has failed to properly predict occluded parts of the scene.

The predicted OGM from ED and ED-Di models are

considerably less accurate than Diff.2 and Diff.1. Partic-
ularly, the area under the rectangle F (on ED-Di) is wrongly
predicted occupied. Also, the occupancies corresponding
to the moving bus is not accurately predicted as indicated
by the red areas. The ED result is slightly better than ED-
Di, however, the occluded areas are not handled as well as
Diff.2 model. Diff.1 model is a slightly more “cautious”
predictor, among others, due to the more false positive it
generated. One interesting difference is highlighted by the
arrow, where the object’s border is only properly deter-
mined by Diff.1 model.

Interesting patterns emerge from the compensation ma-
trices. For instance, looking at the compensation matrices,
on Figure 7, for the Diff.2 model over k > 11, we may
infer that there are three moving objects in the scene, two
of which are smaller in length. In fact, when the compen-
sation matrix is applied to an OGM, a green area followed
by a red area is a pattern which encourages the occupancies
in those areas to move from the red area towards the green
area. Therefore, both the dynamic objects and their approx-
imate headings may be interpreted from the compensation
matrix. However, since the compensation matrix is a by-
product of our approach, in this work we opt not to employ
any inference on the compensation matrix. Illustrating the
compensation matrix, however, is a supporting evidence for
the difference learning idea.

5. Conclusion

Accurate multi-step prediction of OGMs, as representa-
tions of the future drivable space, is useful for path plan-
ning algorithms and does not require multi stages of en-
gineered features in the classic approach of object detec-
tion and tracking.
In this work, a difference learning ar-
chitecture, based on RNNs, is proposed to predict OGMs
multi-steps into the future. We have shown that our pro-
posed architecture outperforms the state of the art in OGM
multi-step prediction and is accurate in predicting the static
and moving objects as well as handling occlusions. Further-
more, as a future work, the generated features by the com-
pensation matrix from our proposed scheme may provide
interesting features for label-less detection and tracking of
dynamic objects.

10607

Neural Networks (IJCNN), International Joint Conference
on, pages 2330–2337. IEEE, 2017. 4

[17] H. Noguchi, T. Yamada, T. Mori, and T. Sato. Mobile robot
path planning using human prediction model based on mas-
sive trajectories. In Networked Sensing Systems (INSS), 2012
Ninth International Conference on, pages 1–7. IEEE, 2012.
3

[18] D. Nuss, S. Reuter, M. Thom, T. Yuan, G. Krehl, M. Maile,
A. Gern, and K. Dietmayer. A random ﬁnite set approach
for dynamic occupancy grid maps with real-time application.
The International Journal of Robotics Research, 37(8):841–
866, 2018. 3

[19] T. Ohki, K. Nagatani, and K. Yoshida. Collision avoid-
ance method for mobile robot considering motion and per-
sonal spaces of evacuees. In Intelligent Robots and Systems
(IROS), 2010 IEEE/RSJ International Conference on, pages
1819–1824. IEEE, 2010. 3

[20] P. Ondruska and I. Posner. Deep tracking: Seeing be-
yond seeing using recurrent neural networks. arXiv preprint
arXiv:1602.00991, 2016. 3

[21] A. Petrovskaya and S. Thrun. Model based vehicle detec-
tion and tracking for autonomous urban driving. Autonomous
Robots, 26(2-3):123–139, 2009. 1

[22] A. M. Santana, K. R. Aires, R. M. Veras, and A. A. Medeiros.
An approach for 2d visual occupancy grid map using monoc-
ular vision. Electronic Notes in Theoretical Computer Sci-
ence, 281:175–191, 2011. 1

[23] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsu-
pervised learning of video representations using lstms.
In
International conference on machine learning, pages 843–
852, 2015. 2

[24] E. Tsardoulias, A. Iliakopoulou, A. Kargakos, and L. Petrou.
A review of global path planning methods for occupancy grid
maps regardless of obstacle density. Journal of Intelligent &
Robotic Systems, 84(1-4):829–858, 2016. 1

[25] S. Wang, R. Clark, H. Wen, and N. Trigoni. End-to-end,
sequence-to-sequence probabilistic visual odometry through
deep neural networks. The International Journal of Robotics
Research, 37(4-5):513–542, 2018. 2

[26] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc-
tural similarity for image quality assessment. In The Thrity-
Seventh Asilomar Conference on Signals, Systems & Com-
puters, 2003, volume 2, pages 1398–1402. Ieee, 2003. 5

[27] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong,
and W.-c. Woo. Convolutional lstm network: A machine
learning approach for precipitation nowcasting. In Advances
in neural information processing systems, pages 802–810,
2015. 3

References

[1] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase
representations using rnn encoder-decoder for statistical ma-
chine translation. arXiv preprint arXiv:1406.1078, 2014. 2

[2] J. Dequaire, P. Ondr´uˇska, D. Rao, D. Wang, and I. Posner.
Deep tracking in the wild: End-to-end tracking using recur-
rent neural networks. The International Journal of Robotics
Research, 37(4-5):492–512, 2018. 2, 3, 4, 5, 6

[3] A. Elfes. Occupancy grids: A stochastic spatial represen-
tation for active robot perception.
In Proceedings of the
Sixth Conference on Uncertainty in AI, volume 2929, page 6,
1990. 1

[4] N. Engel, S. Hoermann, P. Henzler, and K. Dietmayer. Deep
object tracking on dynamic occupancy grid maps using rnns.
arXiv preprint arXiv:1805.08986, 2018. 2, 3

[5] A. Ess, K. Schindler, B. Leibe, and L. Van Gool. Object de-
tection and tracking for autonomous navigation in dynamic
environments. The International Journal of Robotics Re-
search, 29(14):1707–1725, 2010. 1

[6] G. Farneb¨ack. Two-frame motion estimation based on poly-
In Scandinavian conference on Image

nomial expansion.
analysis, pages 363–370. Springer, 2003. 2, 4

[7] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learn-
ing for physical interaction through video prediction. In Ad-
vances in neural information processing systems, pages 64–
72, 2016. 2

[8] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets
robotics: The kitti dataset. International Journal of Robotics
Research (IJRR), 2013. 5

[9] O. K. Gupta and R. A. Jarvis. Optimal global path plan-
ning in time varying environments based on a cost evalua-
tion function. In Australasian Joint Conference on Artiﬁcial
Intelligence, pages 150–156. Springer, 2008. 3

[10] S. Hoermann, M. Bach, and K. Dietmayer. Dynamic oc-
cupancy grid prediction for urban autonomous driving: A
deep learning approach with fully automatic labeling. arXiv
preprint arXiv:1705.08781, 2017. 2, 3

[11] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in neural information

transformer networks.
processing systems, pages 2017–2025, 2015. 2

[12] W. Lotter, G. Kreiman, and D. Cox. Deep predictive cod-
ing networks for video prediction and unsupervised learning.
arXiv preprint arXiv:1605.08104, 2016. 2

[13] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale
video prediction beyond mean square error. arXiv preprint
arXiv:1511.05440, 2015. 2

[14] I. Miller and M. Campbell. A mixture-model based algo-
rithm for real-time terrain estimation. In The 2005 DARPA
Grand Challenge, pages 407–436. Springer, 2007. 5

[15] N. Mohajerin, M. Moziﬁan, and S. L. Waslander. Deep learn-
ing a quadrotor dynamic model for multi-step prediction. In
Robotics and Automation (ICRA), IEEE International Con-
ference on. IEEE, 2018. 2

[16] N. Mohajerin and S. L. Waslander. State initialization for
In

recurrent neural network modeling of time-series data.

10608

