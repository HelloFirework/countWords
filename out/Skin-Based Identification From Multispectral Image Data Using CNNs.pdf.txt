Skin-based identiﬁcation from multispectral image data using CNNs

Takeshi Uemori1

Atsushi Ito2

Yusuke Moriuchi2

Alexander Gatto1

Jun Murayama2

1Sony Europe B.V., Stuttgart, Germany

2Sony Corporation, Tokyo, Japan

{Takeshi.Uemori,Atsushi.C.Ito,Yusuke.Moriuchi,Alexander.Gatto,Jun.Murayama}@sony.com

Abstract

User identiﬁcation from hand images only is still a chal-
lenging task.
In this paper, we propose a new biometric
identiﬁcation system based solely on a skin patch from a
multispectral image. The system is utilizing a novel modi-
ﬁed 3D CNN architecture which is taking advantage of mul-
tispectral data. We demonstrate the application of our sys-
tem for the example of human identiﬁcation from multispec-
tral images of hands. To the best of our knowledge, this
paper is the ﬁrst to describe a pose-invariant and robust
to overlapping real-time human identiﬁcation system using
hands. Additionally, we provide a framework to optimize
the required spectral bands for the given spatial resolution
limitations.

1. Introduction

Personal identiﬁcation using unique physiological fea-
tures such as a face, an iris, ﬁngerprints or vein, is re-
quired in a wide variety of systems and applications. Es-
pecially in the past couple of years, there has been sig-
niﬁcant increase of practical logon applications for differ-
ent types of devices such as cellular phones [3, 28], lap-
tops [24], and video game consoles [33].
In the case of
tabletop devices, which fall into the category of a so-called
tangible user interface [18, 34], identifying a user from his
hands only is desired for individual access controls and nat-
ural user interfaces. The system is required to work with-
out any constraints on hand pose, in contrast to ﬁngerprint
or vein which usually forces users to pause at the ideal
pose. In recent years, thanks to progress in deep learning,
there has been outstanding progress in a variety of com-
puter vision tasks. The standard way to perform image-
based recognition is to use geometric information. While
this approach is suitable for relatively rigid objects such
as faces, hand recognition often requires a particular hand
pose [29, 5, 43, 30, 13]. Our method is pose-invariant and
can deal with occlusions.

Recently, multispectral image acquisition systems which
capture data at several speciﬁc wavelength ranges (usually

more narrowly than RGB image acquisition systems) have
become easier available. Due to the complexity of tra-
ditional multispectral acquisition systems, proposed appli-
cations have been limited to very speciﬁc ﬁelds [16, 22].
However, various types of systems [7, 11] are being devel-
oped recently and spreading to wider commercial applica-
tions [27, 6] including biometrics [45, 1]. Especially the
advent of multispectral mosaic-array sensors [42, 21, 15]
enables the acquisition of video as a sequence of single-
snapshots. However, the disadvantage of these sensors is
the trade-off between spatial resolution and the number of
spectral bands, which means the spatial resolution is sac-
riﬁced if the number of spectral bands is increased or vice
versa.

Several skin spectra models have been proposed in early
works [2, 40]. According to their optical considerations,
perceived color is mainly composed of dermis scattering,
melanin and vascular absorption which are different among
individuals due to the differences of skin chromophore con-
centrations [35, 40]. Our approach has been motivated by
knowledge from these studies.

In this paper, we propose a framework of hand identiﬁ-
cation using spatial and spectral distributions of skin, with-
out using any geometric information, as shown in Figure 1.
From a small patch (i.e. 16x16 pixels) of a hand, our CNN
model can distinguish among registered users, without us-
ing any additional information about a hand’s shape. One
advantage of our patch-based identiﬁcation is that it works
even when a hand of a user to be identiﬁed is overlapped
by a hand of a different user, or a part of a hand is out of
the view. Additionally, our model can distinguish between
the left and right hand of a person because our CNN model
learns different spectral and spatial features of skin for each
hand. Finally our approach works in a frame-by-frame fash-
ion, making real time processing more feasible. We demon-
strate this user identiﬁcation framework in the scenario of a
tabletop projection system as shown in Figure 2. A multi-
spectral camera, including a projection system, is mounted
over the tabletop and acquires images of hands which are
moving without any constraints on the table. These capabil-
ities may provide a novel natural user interaction for many

12349

Figure 1. Skin-based user identiﬁcation with local spatial-spectral features: We propose a novel framework for pose-invariant user identi-
ﬁcation by the combination of multispectral image data and an algorithm based on CNN.

conventional RGB image skin-based identiﬁcation when us-
ing the same amount of data.

• Demonstrating feasibility of hand identiﬁcation based
on spatial-spectral features of skin using CNNs with syn-
thetic and real datasets.

• Proposing a novel 3D CNN which enhances relevant

spectral bands for skin-based identiﬁcation.

• Providing multispectral

image dataset generating
pipelines for ﬁnding an optimal shape of spatial-spectral
data cube.

Outline This paper is organized as follows. We begin with
reviewing prior work in Section 2. In Section 3, we intro-
duce our network architecture utilizing a multispectal data
cube as input. We explain our strategy of generating syn-
thetic datasets in Section 4 and show superiority of multi-
spectral image input via multiple experiments with our syn-
thetic datasets in Section 5. The feasibility with real mul-
tispectral data is shown in Section 6. In Section 7, we dis-
cuss supportive evidence of our proposal with an explana-
tion tool for deep networks. Conclusions, limitations and
future works are provided in Section 8.

2. Prior Work

Identiﬁcation approach using hands: There are al-
ready many commercial biometric user authentication sys-
tems which require an image of hands. Most of them can
be categorized into ﬁngerprint, vein and geometric iden-
tiﬁcation. As an example of ﬁngerprints identiﬁcation,
in [32], the authors extracted ridge ending and ridge bi-
furcation of ﬁngerprint as feature values. In [26], the au-
thors claimed multispectral ﬁngerprint image acquisition
improved robustness against environmental and physiolog-
ical conditions like bright ambient lighting, wetness, poor
contact between the ﬁnger and sensor. In the case of vein
authentication, vascular patterns are recognized by analyz-
ing deoxidized hemoglobin absorption of near-infrared light

12350

Figure 2. Use case in the scenario of a tabletop interface: In this
case, users’ hands are moving without any shape constraints and
without any restriction in partial overlapping each other.

applications.

In this work, we are dealing with a mosaic-array based
multispectral sensor where its spatial resolution is divided
into spectral bands. Thus, by increasing the number of spec-
tral bands we reduce the resolution of each band. This paper
provides a framework for ﬁnding the best trade-off between
the number of spectral bands and the spectral spatial reso-
lution in order to maximize classiﬁcation accuracy. In other
words, given the ﬁxed 3D spectral data cube volume (speci-
ﬁed by sensor’s basic resolution), our framework optimizes
the shape of the cube with the same volume which provides
the best classiﬁcation accuracy. This optimization frame-
work requires input from various conﬁgurations with dif-
ferent numbers of spectral bands. This would require data
capture with multiple different multispectral cameras with
the same basic sensor resolution, which is not practical. In-
stead, we propose to use a simulation approach, which we
describe in detail in Section 4.

Contributions The key technical contributions of this pa-
per are summarized below.

• Showing superiority of our approach with respect to

RGBMultispectralFeature extraction Information integration Classification Data sensing Feature map Prediction Multispectral images Input: Spectral data cube Output: Skin identification Subject    Probability skin patch Input Soft max FC 3D  Conv 3D  Conv 3D  Conv User 3,Left handUser 2,Left handUser 1,Left and right handsSkin-based user identificationSpectral data cubeSkin patchMosaic-array sensorTabletop interactionin [39]. The other category is the physical dimensions of
a human hand.
In [5], a user identiﬁcation approach us-
ing 25 geometric features of ﬁnger and palm was proposed.
In [43], features of hand silhouette extracted by using in-
dependent component analysis showed satisfactory perfor-
mance for groups of about 500 users. A user authentica-
tion system with RGB camera on multi-touch tables was
proposed in [30]. The authors used a support vector ma-
chine classiﬁer with features of palm width, ﬁnger length
and breadth. In [13], a non-contact identiﬁcation method
with CNN was proposed. Here, users hold their hands in
front of a ToF camera and they are classiﬁed by shape fea-
tures from their palm.

Multispectral image capturing system: A traditional
multispectral imaging system operates in a sweeping man-
ner and utilizes a prism or grating to disperse light [25].
The next category of multispectral imaging system employs
either liquid-crystal tunable ﬁlters or acousto-optic tunable
ﬁlters to modulate the input spectrum over time [14]. Re-
cently, as the newest category, single-snapshot multispec-
tral imaging systems are being developed to rapidly acquire
a 3D spectral data cube which allow to avoid motion arti-
facts and thus enabling video acquisition [41, 42, 21, 15].
However, this category of mosaic-array multispectral imag-
ing system usually sacriﬁces its spatial resolution for spec-
tral resolution. To overcome this problem, some papers re-
cently proposed the framework of combining image sensor
architecture and image signal reconstruction [38, 12].

Potential of skin spectra for user identiﬁcation: Early
works have shown that skin has much personal informa-
tion. The history began with the famous skin model in [2].
In [37], the authors proposed a novel skin model with two-
region chromophore ﬁtting and estimated consistency of
pigments such as melanin, oxy- and deoxy-hemoglobin, by
measuring the spectra of skin optical properties of 18 sub-
jects of different skin phototypes I–VI [35] in the range
from 500 to 1000 nm.
the au-
thors showed that absorption spectra and scattering spectra
properties of skin sub-surface scattering are very different
among 149 subjects.

In another work [40],

3. Network architecture for hand identiﬁcation

Recently, CNNs using 3D convolutions have been suc-
cessful in various applications [10, 8, 23] with high dimen-
sional data. In our skin identiﬁcation task, with a spectral
data cube, 3D convolution is expected to extract spectral-
spatial features more efﬁciently than 2D convolution. The
proposed network architecture is shown in Figure 3. Al-
though any architecture can be used as the base network
for extending to 3D, we selected the wide residual net-
works (Wide-ResNet) [44] because the ResNet architecture
and its variants are commonly used in image classiﬁcation
ﬁeld. The difference from a normal 3D Wide-ResNet is that

Figure 3. Network architecture: Our architecture is based on the
Wide-ResNet [44] which has two types of residual blocks with
different skip connections (left). These skip connections of block
A and B are the projection shortcut and the identity shortcut re-
spectively. We used a 3D convolution kernel and extracted the
characteristics of a spectral band.
In addition, to enhance rele-
vant spectral bands, we added squeeze-and-excitation (SE) blocks
(right).

spectral attention is involved to enhance the relevant spec-
tral bands.
It was inspired by the squeeze-and-excitation
block (SE-block) [17]. SE block enhances the performance
with small computational effort, and pays attention to a sin-
gle weight for each channel of the feature maps. Position-
SE block for facial attribute analysis was proposed in [46],
which focuses on highlighting the relevant spatial position.
Unlike [46], our SE block pays attention to spectra as well
as channels of feature maps.

In our implementation we replaced all 2D convolutions
of Wide-ResNet by 3D convolutions, while keeping the di-
mension of spectral band constant until the last global pool-
ing layer. SE blocks were applied to each residual module.
Concerning the global pooling layer in the SE blocks, only
spatial axes were averaged. At the end of the SE blocks,
weights for each spectral band, as well as each channel of
feature maps, were obtained.

4. Generating synthetic datasets

We need datasets for evaluating the identiﬁcation perfor-
mance in various types of input data cubes. Therefore, we
provide two types of synthetic multispectral dataset gener-
ating pipelines. One of them is for creating datasets which
include actual hand skin textures. The other is for evalu-
ating performance with a large number of subjects. In this
framework, 1D spectral proﬁles are converted to 3D data
cubes with measured distributions of skin textures.

12351

 Technology Center�3D SE Residual Block A3D SE Residual Block A,128, /23D SE Residual Block B, 1283D SE Residual Block B, 1283x3x3 Conv, 16BN + ReLUGlobal poolingFCSoftmax3D SE Residual Block A, 2563D SE Residual Block B, 2563D SE Residual Block B, 2563D SE Residual Block A, 5123D SE Residual Block B, 5123D SE Residual Block B, 512outputInput×××1x1x1 Conv, C, /2SE Block1×1××Global pooling1×1××1x1x3 Conv, C/r1×1××1x1x3 Conv, CSigmoidReLU×××3x3x3 Conv, C, /2Dropout×××3x3x3 Conv, CBN + ReLU×××ScaleBN + ReLUSkip connection Figure 4. Pipelines of synthetic multispectral skin dataset genera-
tion: We constructed two types of framework. One of them is for
creating datasets which include real hand skin textures. Another is
a framework that converts 1D spectral proﬁles into 3D data cubes
by incorporating measured distributions of skin.

Figure 5. Sensor sensitivity characteristics: (a) represents the spec-
tral proﬁle of the acA2500-14gc of Basler AG. (b) shows the spec-
tral proﬁle of the multispectral camera CMS-C of SILIOS Tech-
nologies. Both cameras were used in the experiment described in
Section 6.

4.1. Dataset #1 based on 2D spectral measurement

In order to create this dataset #1, we utilized a 2D spec-
trometer which has been prototyped internally in our afﬁl-
iation.
It acquires hyperspectral images in steps of 1 nm
with 168x128 pixel resolution in the visible. 20 hands of
skin from 12 subjects are in the source data and each of
the hands is represented by 18 to 30 hyperspectral images
with various poses. All subjects were Asian males. Here,
we can emulate images according to a sensor speciﬁcation.
The upper line of Figure 4 shows the pipeline of generat-
ing our skin multispectral dataset #1. In general, an image
capturing system with multiple wavelength channels is rep-
resented by equation 1:

Ic = Z 700

400

R(λ)L(λ)Sc(λ)dλ + n

(1)

Where Ic means the intensity of spectral band c, λ is the
wavelength over which is integrated and n is the noise. R
is the spectral reﬂectance of a target in the scene and L is
the spectral distribution of the illumination. Finally, Sc rep-
resents the sensor’s spectral sensitivity of spectral band c.
To acquire the correct reﬂectance R for each pixel, we had
to normalize the illumination under which we collected the
data. We captured the spectral response of a gray uniform
board whose reﬂectance is known in advance. Then, we

normalized the skin spectra by using equation 2:

R(λ) = Ms(λ) ⊘ Mg(λ)

(2)

Where Ms and Mg denote the measured data of skin and the
gray board. The symbol ⊘ represents the Kronecker divi-
sion. We mainly assumed a white illumination which has a
ﬂat spectral distribution over all wavelengths as L. With the
sensor spectral sensitivities Sc, we mainly assumed proﬁles
as shown in Figure 5 (a) and (b) respectively for a multi-
spectral and RGB sensor which were used in actual camera
experiments in Section 6. The ﬁnal stage of the pipeline
consists of adding sensor noise. We adopted a noise model
described in equation 3 and 4:

ˆIc(x) = G(Ic(x), sn(x))
sn(x) = α ×pIc(x)

(3)

(4)

G(m, s) denotes a Gaussian distribution function with a
mean value m and a standard deviation s. ˆIc(x) indicates
the intensity in a patch image at position x and sn(x) is the
standard deviation of noise. α is a noise scaling coefﬁcient.
We usually set α = 0.25 as a base. In this case, the standard
deviation s becomes 0.78% in a bright region of a 10-bit im-
age. According to the procedure described above, we could
generate a skin dataset #1 which was assumed to be cap-
tured by a multispectral sensor and an RGB sensor. Some
examples of this dataset #1 are shown in our Supplementary
Material.

4.2. Dataset #2 based on large scale spectral proﬁles

For generating a large scale skin multispectral dataset,
using the pipeline of dataset #1 is a hard task. Especially
collecting the source data with the 2D spectrometer is taking
a lot of time. Therefore, we used the standard object color
spectra database (SOCS) [19] as the source of our dataset.
SOCS contains only 1D skin spectral proﬁles which were
acquired at a point using a spectrometer. We picked up bare
skin spectral proﬁles from the forehead of 123 Japanese fe-
males. These proﬁles are shown in Figure 6. We intended
to generate a skin dataset #2 which was assumed to be cap-
tured by any formats of multispectral sensors. The pipeline
is shown at the bottom of Figure 4. The biggest difference
with the pipeline of dataset #1 is that R in equation1 has
only spectral distributions, but does not have spatial distri-
butions. Hence, we synthesized skin textures based on mea-
surement of real skin. We measured the standard deviation
of real skin sr from the source of dataset #1. This stan-
dard deviation sr is shown in Figure 6 as the yellow band.
Then, we calculated the desired texture pixel number using
equation 5:

ˆR(λ) = G(R(λ), sr(λ))

(5)
Here, we could acquire a 3D data cube ˆR(λ) as input of
the pipeline. The subsequent procedure is the same as with

12352

Sensor simulationSkin texture generationHyperspectral imageSpectral profileDataset #1Dataset #2Skin distributionIlluminationpropertySensornoiseSensorsensitivity400450500550600650700Wavelength (nm)01020304050Quantum Efficiency (%)400450500550600650700Wavelength (nm)01020304050Quantum Efficiency (%)(b) Multispectral (8 bands)(a) RGB (3 bands)y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

0.97

0.95

0.93

0.91

0.89

0.87

0.85

Classification accuracy

1.2

1.0

n
o

0.8

0.6

0.4

0.2

0.0

Spatial resolution

3 4

6 8

32

16
Spectral bands

301

i
t

l

u
o
s
e
r
 
l

a

i
t

a
p
s
 
f

o
o

 

i
t

a
R

Figure 6. Skin spectral proﬁles in the SOCS dataset: 123 subjects’
spectral skin proﬁles were picked up for generating dataset #2.
The wide yellow band means a spatial standard deviations of real
skin. Because distribution among individuals are less than a spatial
distribution, it looks difﬁcult for a human to distinguish the person
from them.

Figure 8. Performance comparison under same amount of data:
The amount of data is deﬁned by the multiplication of the num-
ber of spectral bands × the spatial resolution ratio. We evaluated
performances under this trade-off conditions and found that at 16
spectral bands the combination of spectral and spatial information
is optimal.

Figure 7. Samples of synthetic dataset #2: Multispectral data
patches were generated from 1D spectral proﬁles from the SOCS
dataset. Here, 16×16×3 (RGB) samples are shown. They look
very similar and it seems to be difﬁcult for a human to distinguish
a subject.

Section 4.1. We mainly generated patches with skin texture
having a size of 16x16 pixels, but the size is ﬂexible depend-
ing on a requirement of each experiment. Some samples of
this dataset #2 are shown in Figure 7.

5. Evaluation on synthetic datasets

In this section, we analyzed identiﬁcation performances
with synthetic datasets which were generated as described
in the previous section. To validate superiority of a multi-
spectral image as input in various aspects, we evaluated on
data trade-off conditions (Section 5.1), different numbers of
classes (Section 5.2) and different noise conditions (Section
5.3). Finally, we compared the performance between 2D
and 3D CNNs including our proposed network architecture
(Section 5.4).

5.1. Data cube trade off comparison

In the case of a mosaic-array sensor, the number of spec-
tral bands and the spatial resolution are in a trade-off rela-
tionship, if keeping the amount of data (image width × im-
age height × number of bands) constant. In this experiment,
we evaluated performances among this trade-off conditions.
Experimental setup: We conducted this experiment
with the generation pipeline of dataset #1. At ﬁrst, 7 types

of multi-band sensors which have respectively 3, 4, 6, 8, 16,
32 and 301 bands were generated. Their sensor sensitivities
were deﬁned by dividing the range 400 − 700 nm into their
band numbers equally (whose spectral proﬁles shaped into
squares). An ideal white illumination was assumed. From
the obtained multispectral images, we cropped 16x16xD
(D=3, 4, 6, 8, 16, 32, 301) data cubes of hand skin re-
gions which have been detected by thresholding in HSV
color space in advance.
In order to align the amount of
data among cubes, they were reduced by skip down-scaling

with the ratio p3/D from all of their original data amount

of 16x16xD. Finally, sensor noise was added with coefﬁ-
cient α = 0.25 in equation 4. Then, data cubes were up-
scaled again by ﬁltering for evaluation with the same net-
work. We used the Wide-ResNet [44], which was imple-
mented by [36]. Implementation details with network pa-
rameters are explained in our Supplementary Material. We
split the created dataset into training and evaluation data in
the ratio of 7 to 3. We randomly cropped 500 patches for
each hand from the training data, and in the same manner,
we prepared 215 patches per hand for the evaluation.

Analysis: Figure 8 shows the classiﬁcation accuracy for
each input. The classiﬁcation accuracy is increasing with
the number of spectral bands up to 95.7% at 16 bands. Ba-
sically, at 16 spectral bands the combination of spectral and
spatial information is optimal. Even though the maximum
classiﬁcation accuracy is achievable at 16 spectral bands, 8
bands provide sufﬁciently good results. Thus, taking into
account the current mutispectral cameras market, we de-
cided to use SILIOS Technologies’ CMS-C multispectral
camera with 8 narrow spectral bands.

5.2. Performance scalability in large scale datasets

We show the scalability of performance related to the

number of classes in the identiﬁcation.

12353

Subject 1 Subject 2 Subject 3 Subject 4 Subject 5 0.99 

0.99 

0.97 

0.93 

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

1.00

0.90

0.80

0.70

RGB (3 bands)
Multispectral (8 bands)

1.0

0.94 

Accuracy ratio

0.88 

0.9

0.84 

0.73 

0.8

0.7

o
i
t
a
r
 
y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

0.97

0.87

0.77

0.67

0.57

0.47

0.96 

0.91 

0.89 

RGB (3 bands)
Multispectral (8 bands)

0.81 

0.81 

0.73 

Accuracy ratio

0.68 

0.59 

1.1

1.0

0.9

0.8

0.7

0.6

o
i
t
a
r
 
y
c
a
r
u
c
c
A

16

32

64

123

Identification class number

0.00

0.25

0.50
α
Noise level (  )

1.00

Figure 9. Classiﬁcation accuracy with different number of sub-
jects: The performance gap between RGB and multispectral in-
put becomes larger with an increasing class number. Overall the
identiﬁcation performance with multispectral input remains more
stable with varying number of classes.

Experimental setup: To facilitate this experiment, we
prepared skin data cubes by utilizing the procedure as de-
scribed in Section 4.2 which was used previously for gener-
ation dataset #2. In this experiment, we intended to emulate
and compare existing sensors, therefore we adopted sensor
sensitivities of the 3-band sensor (Figure 5 (a)) and the 8-
band sensor (Figure 5 (b)). An ideal white illumination was
assumed and the noise level was set to α = 0.5 in equation
4. Then we got 16x16x8 data cubes for the 8-band sensor
and 23x23x3 data cubes for the RGB sensor. Their data
volumes are almost the same when considering the Bayer
pattern [4] in RGB. We prepared sub-datasets with differ-
ent numbers of subjects (16, 32, 64 and 123) based on the
original dataset #2 containing 123 subjects. The number
of patches for each subject were the same in the training
and evaluation. The network architecture and its parameters
were as in the previous experiment explained in Section 5.1.
The results of classiﬁcation accuracy are
shown in Figure 9. In the case of 16 classes, high accuracy
over 99.0% was achieved with both RGB and multispectral
inputs. This means that this dataset with 16 classes might
be simpler than the experiment in Section 5.1. However, the
performance gap between them became larger as the num-
ber of classes increased. The accuracy with multispectral
input was still kept 88.3% even in the case of 123 classes,
though the accuracy with RGB was declined to 72.9%.

Analysis:

5.3. Robustness for noise

We also compared the robustness for sensor noise be-

tween RGB and multispectral input.

Experimental setup: Here, we regenerated dataset #1
with various noise levels. We set the noise scaling coefﬁ-
cient in equation 4 as α = 0, 0.25, 0.5, 1.0, for creating dif-
ferent levels of noise. We used the same sensor sensitivities
as in Section 5.2. Except for noise and sensor sensitivity,
all other conditions were the same as in the experiment in

Figure 10. Classiﬁcation accuracy at different noise levels: As
noise level becomes bigger, the performance gap between with
RGB and with multispectral input becomes larger.

Table 1. Classiﬁcation accuracy with 2D and 3D based networks
RGB (3 bands) (%) Multispectral (8 bands) (%)

Approach

2D CNN [44]

3D CNN

3D CNN with SE (Ours)

81.0
80.0
83.2

88.6
88.0
91.1

Section 5.1. The network, its parameters and training pro-
cedures were also according to Section 5.1.

Analysis: The comparison with different noise levels
is shown in Figure 10. The performance with multispec-
tral input was superior to the one with RGB input at every
noise level. Although both decreased, the performance gap
increased with the noise level. This result implies that spec-
tral information keep contributing to the performance, even
when spatial information is degraded by noise.

5.4. Comparison between 2D and 3D CNNs

In this experiment, we show the performance improve-

ment by our network architecture described in Section 3.

Experimental setup: We used the same dataset with

noise level α = 0.25 which was generated in Section 5.3.

We compared the performances of 2D and 3D convolu-
tions in Wide-ResNet architecture. Then, we evaluated the
effectiveness of the SE-blocks which pay attention to rele-
vant spectral bands.

Analysis: Table 1 shows the results obtained from RGB
and multispectral input with different networks. In the com-
parison between different inputs in 2D CNN, multispectral
input led to 7.6% improvement. However, from the compar-
ison between 2D and 3D convolutions, 3D convolutions did
not enhance the performance for each input. As mentioned
in [20], this might due to the fact that 3D convolutions do
not work well with insufﬁcient number of training data. On
the other hand, our proposal with multispectral input had
2.5% improvement from the 2D CNN. From this result, SE-
blocks enhanced the performance of 3D CNN even with the
insufﬁcient amount of training data. Visualized results are
shown in our Supplementary Material.

12354

4

5

7

8

4

5

7

8

Input

result

Input (visualized as RGB)

result

RGB (3 bands)

Multispectral (8 bands)

Figure 11. Result on the actual camera dataset of 10 hands: The
ﬁrst and second columns show the RGB inputs and the prediction
results with 2D CNN. The third and fourth columns show the mul-
tispectral inputs and the prediction results with Ours. For each
prediction result, each color means the predicted class and the la-
bel at the upper-right corner shows the ground truth color.

6. Evaluation on an actual camera dataset

To validate experiments with synthetic data in the pre-
vious section, we did an experiment with an actual camera
setup. We made a real hand skin dataset which was ac-
quired by a multispectral and RGB camera, and compared
the identiﬁcation performance achieved with each camera
dataset against each other.

6.1. Experimental setup

We used a commercially available multispectral camera,
the CMS-C of SILIOS Technologies, for the image acquisi-
tion. This camera can acquire 8 narrow spectral bands and a
broad monochrome band. In our experiment, we used the 8
color spectral bands only. For the RGB image acquisition,
we used the acA2500-14gc of Basler AG. Both cameras
were synchronized by software with a framerate of about
5 frames per second. Their spectral sensitivities have been
already shown in Figure 5. Other camera speciﬁcations and
the experimental setup are explained in our Supplementary
Material. We acquired images of both hands from 5 males,
1 Asian and 4 Caucasians. During the acquisition, the sub-
jects moved their hands freely on the wall which was 0.8
meters away from the cameras. A conventional 100W bulb
was used as a light source. As pre-processing before in-
putting to CNN, illumination normalization represented by
equation 2 were applied to both datasets. In addition to that,
we adjusted the ﬁeld of view of a pixel by nearest neighbor
resizing and the bit depth between both cameras by round-
ing RGB images. Measured noise levels were correspond-
ing to α = 0.36 in an RGB image and α = 0.43 in a multi-
spectral image on average of spectral bands.

Table 2. Classiﬁcation accuracy with the actual camera dataset

Approach

RGB (3 bands) (%) Multispectral (8 bands) (%)

2D CNN [44]

3D CNN

3D CNN with SE (Ours)

88.3
87.9
89.1

91.5
92.5
93.1

Sub1_L

0.8960 0.0717 0.0164 0.0006 0.0040 0.0012 0.0029 0.0017 0.0040 0.0014

Sub1_L

0.9381 0.0429 0.0153 0.0032 0.0003 0.0000 0.0000 0.0003 0.0000 0.0000

Sub1_R 0.1178 0.6770 0.0288 0.0389 0.0190 0.0568 0.0043 0.0297 0.0107 0.0170

Sub1_R 0.1340 0.7724 0.0063 0.0395 0.0075 0.0277 0.0003 0.0035 0.0032 0.0058

Sub2_L

0.0112 0.0066 0.9309 0.0046 0.0141 0.0061 0.0058 0.0035 0.0084 0.0089

Sub2_L

0.0052 0.0017 0.9821 0.0020 0.0023 0.0003 0.0003 0.0003 0.0043 0.0014

Sub2_R 0.0000 0.0084 0.0144 0.9167 0.0040 0.0193 0.0084 0.0150 0.0023 0.0115

Sub2_R 0.0003 0.0040 0.0144 0.9718 0.0012 0.0049 0.0006 0.0009 0.0000 0.0020

l

e
b
a

l
 

e
u
r
T

Sub3_L

0.0023 0.0017 0.0092 0.0037 0.8773 0.0444 0.0173 0.0084 0.0135 0.0222

Sub3_R 0.0026 0.0052 0.0017 0.0078 0.0297 0.9009 0.0058 0.0075 0.0049 0.0340

l

e
b
a

l
 

e
u
r
T

Sub3_L

0.0043 0.0023 0.0115 0.0081 0.8963 0.0346 0.0037 0.0043 0.0187 0.0161

Sub3_R 0.0023 0.0029 0.0029 0.0098 0.0118 0.9490 0.0000 0.0055 0.0023 0.0135

Sub4_L

0.0014 0.0017 0.0167 0.0086 0.0101 0.0023 0.9009 0.0429 0.0075 0.0078

Sub4_L

0.0000 0.0003 0.0003 0.0000 0.0009 0.0000 0.9167 0.0804 0.0014 0.0000

Sub4_R 0.0012 0.0058 0.0058 0.0320 0.0061 0.0078 0.0536 0.8698 0.0046 0.0135

Sub4_R 0.0000 0.0006 0.0012 0.0017 0.0012 0.0012 0.0441 0.9490 0.0003 0.0009

Sub5_L

0.0104 0.0026 0.0098 0.0020 0.0167 0.0072 0.0095 0.0026 0.9124 0.0268

Sub5_L

0.0046 0.0006 0.0017 0.0023 0.0063 0.0026 0.0026 0.0035 0.9614 0.0144

Sub5_R 0.0026 0.0006 0.0009 0.0023 0.0063 0.0213 0.0029 0.0072 0.0052 0.9507

Sub5_R 0.0006 0.0000 0.0000 0.0017 0.0014 0.0176 0.0003 0.0014 0.0049 0.9721

S u b 1 _ L

S u b 1 _ R

S u b 2 _ L

S u b 2 _ R

S u b 3 _ L

S u b 3 _ R

S u b 4 _ L

S u b 4 _ R

S u b 5 _ L

S u b 5 _ R

S u b 1 _ L

S u b 1 _ R

S u b 2 _ L

S u b 2 _ R

S u b 3 _ L

S u b 3 _ R

S u b 4 _ L

S u b 4 _ R

S u b 5 _ L

S u b 5 _ R

Predicted label
Predicted label

(a) RGB (3 bands)

Predicted label
Predicted label

(b) Multispectral (8 bands)

Figure 12. Confusion matrices comparison between (a) 2D CNN
with RGB input and (b) Ours with multispectral input: More con-
fusion among subjects as well as between hands of a subject are
found in (a). Ours with multispectral input distinguishes among
hands more accurately.

Then, we split images of each hand into training data
and evaluation data in the ratio of 7 to 3. In order to detect
skin pixels, we preprocessed both RGB and multispectral
images by transforming them into the HSV color space and
identiﬁed skin pixels by a reference sub-color space. From
the training data, we randomly extracted 8100 patches of
size 16x16 pixels, centered around the detected pixels. In
the same manner, we prepared 3471 patches per hand for
the evaluation. We evaluated the performance with each
input using the same three networks described in Section
5.4.

6.2. Result

Figure 11 shows selected results for comparing between
the 2D CNN with RGB input and our proposed 3D CNN
added SE with multispectral input. Since our goal is to
identify a person from just a small skin area, this perfor-
mance gap is considerable. Table 2 shows the quantita-
tive results obtained from actual RGB and multispectral in-
puts with different networks. When comparing the perfor-
mances between 2D CNN and 3D CNN without SE, mul-
tispectral based performance was improved with more than
eight times the amount of training data to Section 5.4. On
the other hand, RGB based performance was decreased. We
believe that this result comes from 3D convolution, which
beneﬁt more from the relevant spectral information of mul-
tispectral data, and not only from the model capacity in-
crease. We also conﬁrmed that our proposed network led to
more improvement by involving SE-blocks. The ﬁnal per-
formance gap was 4.8%. When considering the experimen-
tal setup of the class number and noise levels, this result is
reasonable. Figure 12 shows another comparison with con-
fusion matrices. This result also supports our conclusion

12355

“W” curve around 550nm due to underlying blood condi-
tions which are depending on individuals. This is consistent
with our Grad-CAM analysis, showing that spectral bands
of this wavelength region were the most relevant for skin-
based identiﬁcation. Our 3D CNN with SE architecture en-
hanced the relevant spectral characteristics. Also, it could
learn from the underlying hemoglobin more than other ap-
proaches. We consider that these are the main reasons for
higher performance.

8. Conclusion

In this paper we have presented a novel framework for
skin-based user identiﬁcation, by combining multispectral
imaging and CNNs. We showed superiority of our ap-
proach, with respect to conventional RGB imaging, when
using the same amount of data. Feasibility of the approach
was demonstrated with synthetic and actual image datasets.
We proposed a novel 3D CNN model which is enhancing
the inﬂuence of spectral image data. This paper is the ﬁrst to
involve SE-blocks for boosting relevant wavelengths. Ad-
ditionally, we developed multispectral image dataset gen-
eration pipelines for ﬁnding an optimal shape of spatial-
spectral data cubes.

Limitations and future work: One limitation is that
we do not consider variations in illumination. A solution
would be to add information from a spectrometer which
senses the illumination spectra of a light source in a scene.
Spectrometers recently became affordable devices, making
this approach feasible. Another limitation is that we do not
account for a sudden change of skin color due to sunburn
or coloration by hand cream. Finally, the current work is
restricted to the back-side of the hand. This is mainly due
to the targeted use case scenario of a tabletop device. In
principle, our approach can be also applied to the palm or
other parts of the human body.

Optimizing spectral combinations of a mosaic-array sen-
sor is the most interesting future work. Recently, there
are several proposals which enable to customize capturing
spectral bands such as [42]. We consider that our synthetic
data generation framework and the CNN visualization tech-
nique described in Section 7 are valid for this work. Ad-
ditionally, there should be many spatial clues of personal
identiﬁcation in infrared spectral bands, although our cur-
rent camera conﬁguration is limited to visible wavelengths
only. We are looking forward to ﬁnding the best combina-
tion of bands and extending wavelengths for improving the
performance of our identiﬁcation framework.

Acknowledgement

We are grateful to our colleagues from Sony Europe B.V.
and Sony Corporation for their fruitful discussions and sup-
port.

12356

Figure 13. Analysis with Grad-CAM: The center bar graph shows
a histogram of the most contributing spectral band which had the
largest weight in 8 bands. The top plots show skin proﬁles with
different underlying blood conditions. Bottom the series of images
show some heat map examples of visualization.

that the experimental results with synthetic dataset was val-
idated with the actual dataset.

7. Discussion with network explanation tool

In this section, we reveal the contributing factors to iden-
tiﬁcation performances with Grad-CAM [31] which is a
technique to produce visual explanations of decisions from
a large class of CNN-based models.

Contribution degree of each spectral band: We can
analyze contributing spectral bands of input. To facilitate
this evaluation, we applied Grad-CAM to the 3D Wide-
ResNet with SE model which was trained in Section 6.
Then, we observed feature maps output from the last resid-
ual block. They had (No. of channels, height, width, No.
of spectral bands) = (512, 4, 4, 8) dimensions and got
weights for each spectral band. Figure 13 shows the his-
togram of the most contributing spectral band. From the
aggregated result, we found that band #5 (center wavelength
λc = 572nm) and #4 (λc = 541nm) contributed more than
other bands. Some Grad-CAM visualizations as heat maps
are shown in the bottom of Figure 13. We can see that the
peak of contribution is located mostly within band #4 and
#5.

Discussion: As shown above, there were signiﬁcant
bias among contributions of each input spectral bands. The
plots in the top of Figure 13 are quoted from [9]. They
are typical cases of skin reﬂectance with different condi-
tions of the underlying blood. They indicate cases of high
and low hemoglobin concentration as well as high and low
oxygenation. [9] claims that a skin spectrum shapes into a

0500100015002000Band 1(433nm)Band 2(466nm)Band 3(503nm)Band 4(542nm)Band 5(572nm)Band 6(611nm)Band 7(652nm)Band 8(692nm)CountHand 1Hand 2Hand 3Hand 4Hand 5Hand 6Hand 7Hand 8Hand 9Hand 10Patch APatch BLow HbconcentrationHigh HbconcentrationHigh oxygenationLow oxygenationSkin reflectanceReferences

[1] Faisal AlGashaam, Kien Nguyen, Mohamed Alkanhal,
Vinod Chandran, Wageeh W. Boles, and Jasmine Banks.
Multispectral periocular classiﬁcation with multimodal com-
pact multi-linear pooling.
IEEE Access, 5:14572–14578,
2017.

[2] R. R. Anderson and J. A. Parrish. The optics of human skin.

Journal of Investigative Dermatology, 77:13–19, 1981.

[3] Apple Inc. Face ID, 2017.

[4] B.E. Bayer. Color imaging array.

1976. US Patent,

US05685824.

[5] G Boreki and A Zimmer. Hand geometry: A new approach
for feature extraction. IEEE Workshop on Automatic Iden-
tiﬁcation Advanced Technologies, pages 149–154, October
2005.

[6] A. Burns and W. U. Bajwa. Multispectral imaging for im-
proved liquid classiﬁcation in security sensor systems. Pro-
ceedings of SPIE Conference on Algorithms and Technolo-
gies for Multispectral, Hyperspectral, and Ultraspectral Im-
agery XXIV, pages 1–7, Apr. 2018.

[7] Zach D. Caratao, Kelsey F. Gabel, Abijit Arun, Brett My-
ers, David L. Swartzendruber, and Christopher W. Lum. Mi-
casense aerial pointing and stabilization system: Dampen-
ing in-ﬂight vibrations for improved agricultural imaging.
2018 AIAA Information Systems-AIAA Infotech @ Aerospace
AIAA SciTech Forum, 2018.

[8] Jo˜ao Carreira and Andrew Zisserman. Quo vadis, action
recognition? A new model and the kinetics dataset. IEEE
Conference on Computer Vision and Pattern Recognition,
pages 4724–4733, 2017.

[9] Mark Changizi. The vision revolution: How the latest re-
search overturns everything we thought we knew about hu-
man vision. BenBella Books, 2009.

[10] Lele Chen, Yue Wu, Adora M. DSouza, Anas Z. Abidin,
Axel Wism¨uller, and Chenliang Xu. MRI tumor segmenta-
tion with densely connected 3d CNN. Proceedings of SPIE
Image Processing, 2018.

[11] Valerie C. Coffey. Multispectral imaging moves into the

mainstream. Optics and Photonics News, 23, 2012.

[12] K. Degraux, V. Cambareri, B. Geelen, L. Jacques, and G.
Lafruit. Multispectral compressive imaging strategies using
fabry-perot ﬁltered sensors. IEEE Transactions on Compu-
tational Imaging, Pre-print, 2018.

[13] DArmin Dietz, Joachim Hienzsch, and Eduard Reithmeier.
Contactless hand identiﬁcation using machine learning.
CMBBE 2018, 15th International Symposium on Com-
puter Methods in Biomechanics and Biomedical Engineer-
ing, 2018.

[14] N. Gat. Imaging spectroscopy using tunable ﬁlters: a review.

Proceedings of the SPIE, 4056:50–64, 2000.

[15] Bert Geelen, Nicolaas Tack, and Andy Lambrechts. A com-
pact snapshot multispectral imager with a monolithically in-
tegrated per-pixel ﬁlter mosaic. SPIE The International So-
ciety for Optical Engineering, (1), 2014.

astronomy and space surviellance. Proceeding of Imaging
Spectrometry IX, 5159, 2004.

[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
IEEE Conference on Computer Vision and Pattern

works.
Recognition, 2018.

[18] Hiroshi Ishii. Tangible bits: beyond pixels. Invited paper in
Proceedings of the 2nd International Conference on Tangible
and Embedded Interaction, 2008.

[19] ISO/TR16066:2003. Graphic technology - standard ob-
ject colour spectra database for colour reproduction evalu-
ation (socs). Technical report (International Organization
for Standardization), 2003.

[20] Will Kay, Jo˜ao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Apostol Natsev, Mustafa Suley-
man, and Andrew Zisserman. The kinetics human action
video dataset. CoRR, abs/1705.06950, 2017.

[21] Pierre-Jean Lapray, Xingbo Wang, Jean-Baptiste Thomas,
and Pierre Gouton. Multispectral ﬁlter arrays: Re-
cent advances and practical
Sensors,
14(11):21626, 2014.

implementation.

[22] G. Lu and B. Fei. Medical hyperspectral imaging: a review.

Journal of Biomedical Optics, 19, 2014.

[23] Yanan Luo, Jie Zou, Chengfei Yao, Tao Li, and Gang Bai.
HSI-CNN: A novel convolution neural network for hyper-
spectral image. Proceedings of International Conference on
Audio, Language and Image Processing, 2018.

[24] Microsoft Corporation. Windows Hello face authentication,

2017.

[25] P. Mouroulis, R. O. Green, and T. G. Chrien. Design of
pushbroom imaging spectrometers for optimum recovery of
spectroscopic and spatial information. OSA Applied Optics,
39:2210–2220, 2000.

[26] R. Rowe, K. Nixon, and P. Butler. Multispectral ﬁnger-
print image acquisition. Advances in biometrics, pages 3–23,
2008.

[27] Inkyu Sa, Marija Popovic, Raghav Khanna, Zetao Chen,
Philipp Lottes, Frank Liebisch, Juan I. Nieto, Cyrill Stach-
niss, Achim Walter, and Roland Siegwart. Weedmap: A
large-scale semantic weed mapping framework using aerial
multispectral imaging and deep neural network for precision
farming. Remote Sensing, 10(9):1423, 2018.

[28] Samsung Electronics Co., Ltd. Security - Iris Scanner, 2017.
[29] R Sanchez-Reillo, C Sanchez-Avila, and A Gonzalez-
Marcos. Biometric identiﬁcation through hand geometry
measurements. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 22(10):1168–1171, 2000.

[30] Dominik Schmidt, Ming Ki Chong, and Hans Gellersen.
Handsdown: hand-contour-based user identiﬁcation for in-
teractive surfaces. NordiCHI, pages 432–441, 2010.

[31] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek
Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-
tra. Grad-cam: Visual explanations from deep networks via
gradient-based localization. IEEE International Conference
on Computer Vision, pages 618–626, 2017.

[16] E. Keith Hege, Dan O’Connell, William Johnson, Shridhar
Basty, and Eustace L. Dereniak. Hyperspectral imaging for

[32] Tatsuya Shimahara. Technologies for improving the speed
and accuracy of ﬁngerprint identiﬁcation systems in support

12357

of public bodies. NEC Technical Journal, 9(1):128–131,
2015.

[33] Sony Interactive Entertainment Inc. PlayStation4 User’s

Guide, 2013.

[34] Jim Spadaccini and Hugh McDonald. The evolution of tan-
gible User interfaces on touch tables: New frontiers in UI &
UX design. Technical report, Ideum, 2017.

[35] B. Thomas and MD Fitzpatrick. The validity and practical-
ity of sun-reactive skin types i through vi. Arch Dermatol,
124(6):869–871, April 1988.

[36] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clay-
ton. Chainer: a next-generation open source framework for
deep learning. Proceedings of Workshop on Machine Learn-
ing Systems (LearningSys) in The Twenty-ninth Annual Con-
ference on Neural Information Processing Systems (NIPS),
2015.

[37] Sheng-Hao Tseng, Paulo Bargo, Anthony Durkin, and Niki-
foros Kollias. Chromophore concentrations, absorption and
scattering properties of human skin in-vivo. Optic Express,
(17):14599–14617, 2009.

[38] P. Wang and R. Menon. Computational multispectral video
Journal of the Optical Society of America A,

imaging.
35(1):189–199, 2018.

[39] Masaki Watanabe. Palm vein. Encyclopedia of Biometrics,

pages 1027–1033, 2009.

[40] Tim Weyrich, Wojciech Matusik, Hanspeter Pﬁster, Bernd
Bickel, Craig Donner, Chien Tu, Janet McAndless, Jinho
Lee, Addy Ngan, Henrik Wann Jensen, and Markus Gross.
Analysis of human faces using a measurement-based skin re-
ﬂectance model. ACM Transactions on Graphics (Proceed-
ings on SIGGRAPH 2006), 25(3):1013–1024, 2006.

[41] Fumihito Yasuma, Tomoo Mitsunaga, Daisuke Iso, and
Shree K. Nayar. Generalized assorted pixel camera: Post-
capture control of resolution, dynamic range, and spectrum.
IEEE Transactions on Image Processing, 19(9):2241–2253,
2010.

[42] Sozo Yokogawa, Stanley P. Burgos, and Harry A. Atwater.
Plasmonic color ﬁltersfor cmos image sensor applications.
Nano Letter, 12:4349–4354, 2012.

[43] Erdem Yoruk, Ender Konukoglu, Jerome Darbon, and Bulent
Sankur. Shape-based hand recognition. IEEE Transaction on
Image Processing, 15:1803–1815, 2006.

[44] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. Proceedings of the British Machine Vision Confer-
ence, 2016.

[45] David Zhang, Zhenhua Guo, and Yazhuo Gong. Multispec-
tral Biometrics: Systems and Applications. Springer, 2015.
[46] Yan Zhang, Wanxia Shen, Li Sun, and Qingli Li. Position-
squeeze and excitation block for facial attribute analysis. The
British Machine Vision Conference, page 279, 2018.

12358

