Content Authentication for Neural Imaging Pipelines: End-to-end

Optimization of Photo Provenance in Complex Distribution Channels

New York University1, AGH University of Science and Technology2

Pawe l Korus1,2 and Nasir Memon1

http://kt.agh.edu.pl/~korus

Abstract

Forensic analysis of digital photo provenance relies
on intrinsic traces left in the photograph at the time
of its acquisition. Such analysis becomes unreliable af-
ter heavy post-processing, such as down-sampling and
re-compression applied upon distribution in the Web.
This paper explores end-to-end optimization of the en-
tire image acquisition and distribution workﬂow to fa-
cilitate reliable forensic analysis at the end of the dis-
tribution channel. We demonstrate that neural imag-
ing pipelines can be trained to replace the internals of
digital cameras, and jointly optimized for high-ﬁdelity
photo development and reliable provenance analysis. In
our experiments, the proposed approach increased im-
age manipulation detection accuracy from 45% to over
90%. The ﬁndings encourage further research towards
building more reliable imaging pipelines with explicit
provenance-guaranteeing properties.

1. Introduction

Ensuring integrity of digital images is one of the
most challenging and important problems in multime-
dia communications. Photographs and videos are com-
monly used for documentation of important events, and
as such, require eﬃcient and reliable authentication
protocols. Our current media acquisition and distribu-
tion workﬂows are built with entertainment in mind,
and not only fail to provide explicit security features,
but actually work against them.
Image compression
standards exploit heavy redundancy of visual signals
to reduce communication payload, but optimize for hu-
man perception alone. Security extensions of popular
standards lack in adoption [20].

Two general approaches to assurance and veriﬁca-
tion of digital image integrity include [24, 30, 32]: (1)
pro-active protection methods based on digital signa-
tures or watermarking; (2) passive forensic analysis

which exploits inherent statistical properties result-
ing from the photo acquisition pipeline. While the
former provides superior performance and allows for
advanced protection features (like precise tampering
localization [38], or reconstruction of tampered con-
tent [25]), it failed to gain widespread adoption due to
the necessity to generate protected versions of the pho-
tographs, and the lack of incentives for camera vendors
to modify camera design to integrate such features [4].
Passive forensics, on the other hand, relies on our
knowledge of the photo acquisition pipeline, and statis-
tical artifacts introduced by its successive steps. While
this approach is well suited for potentially analyzing
any digital photograph, it often falls short due to the
complex nature of image post-processing and distribu-
tion channels. Digital images are not only heavily com-
pressed, but also enhanced or even manipulated before,
during or after dissemination. Popular images have
many online incarnations, and tracing their distribu-
tion and evolution has spawned a new ﬁeld of image
phylogeny [13, 14] which relies on visual diﬀerences be-
tween multiple images to infer their relationships and
editing history. However, phylogeny does not provide
any tools to reason about the authenticity or history
of individual images. Hence, reliable authentication of
real-world online images remains untractable [39].

At the moment, forensic analysis often yields useful
results in near-acquisition scenarios. Analysis of native
images straight from the camera is more reliable, and
even seemingly benign implementation details - like the
rounding operators used in the camera’s image signal
processor [1] - can provide useful clues. Most forensic
traces quickly become unreliable as the image under-
goes further post-processing. One of the most reliable
tools at our disposal involves analysis of the imaging
sensor’s artifacts (the photo response non-uniformity
pattern) which can be used both for source attribution
and content authentication problems [10].

In the near future, rapid progress in computational
imaging will challenge digital image forensics even in

8621

Cross entropy loss

Classification labels: pristine, post­proc. A ­ Z

MUX

Downscale

JPEG 

Compression 

FAN

=

RAW 

measurements 

NIP

Developed

image

L2 loss

=

Desired

output image 

Post­proc. 

A 

Post­proc. 

B

Post­proc. 

Y 

Post­proc. 

Z

Acquisition

Post-processing / editing

Distribution

Analysis

Figure 1. Optimization of the image acquisition and distribution channel to facilitate photo provenance analysis. The neural
imaging pipeline (NIP) is trained to develop images that both resemble the desired target images, but also retain meaningful
forensic clues at the end of complex distribution channels.

near-acquisition authentication. In the pursuit of bet-
ter image quality and convenience, digital cameras and
smartphones employ sophisticated post-processing di-
rectly in the camera, and soon few photographs will
resemble the original images captured by the sensor(s).
Adoption of machine learning has recently challenged
many long-standing limitations of digital photogra-
phy,
(1) high-quality low-light photogra-
phy [9]; (2) single-shot HDR with overexposed con-
tent recovery [15]; (3) practical high-quality digital
zoom from multiple shots [37]; (4) quality enhancement
of smartphone-captured images with weak supervision
from DSLR photos [18].

including:

These remarkable results demonstrate tangible ben-
eﬁts of replacing the entire acquisition pipeline with
neural networks. As a result, it will be necessary to
investigate the impact of the emerging neural imaging
pipelines on existing forensics protocols. While impor-
tant, such evaluation can be seen rather as damage
assessment and control and not as a solution for the
future. We believe it is imperative to consider novel
possibilities for security-oriented design of our cameras
and multimedia dissemination channels.

In this paper, we propose to optimize neural imaging
pipelines to improve photo provenance in complex dis-
tribution channels. We exploit end-to-end optimization
of the entire photo acquisition and distribution chan-
nel to ensure that reliable forensics decisions can be
made even after complex post-processing, where clas-
sical forensics fails (Fig. 1). We believe that immi-
nent revolution in camera design creates a unique op-
portunity to address some of the long-standing limi-
tations of the current technology. While adoption of
digital watermarking in image authentication was lim-

ited by the necessity to modify camera hardware, our
approach exploits the ﬂexibility of neural networks to
learn relevant integrity-preserving features within the
expressive power of the model. We believe that with
solid security-oriented understanding of neural imaging
pipelines, and with the rare opportunity of replacing
the well-established and security-oblivious pipeline, we
can signiﬁcantly improve digital image authentication
capabilities.

We aim to inspire discussion about novel camera de-
signs that could improve photo provenance analysis ca-
pabilities. We demonstrate that it is possible to opti-
mize an imaging pipeline to signiﬁcantly improve de-
tection of photo manipulation at the end of a complex
real-world distribution channel, where state-of-the-art
deep-learning techniques fail. The main contributions
of our work include:

1. The ﬁrst end-to-end optimization of the imaging
pipeline with explicit photo provenance objectives;

2. The ﬁrst security-oriented discussion of neural

imaging pipelines and the inherent trade-oﬀs;

3. Signiﬁcant improvement of forensic analysis per-
formance in challenging, heavily post-processed
conditions;

4. A neural model of the entire photo acquisition and
distribution channel with a fully diﬀerentiable ap-
proximation of the JPEG codec.

To facilitate further research in this direction, and
enable reproduction of our results, our neural imaging
toolbox is available at https://github.com/pkorus/
neural-imaging.

8622

2. Related Work

Trends in Pipeline Design Learning individual
steps of the imaging pipeline (e.g., demosaicing) has
a long history [21] but regained momentum in the re-
cent years thanks to adoption of deep learning. Nat-
urally, the research focused on the most diﬃcult op-
erations, i.e., demosaicing [17, 23, 34, 35] and denois-
ing [6, 26, 40]. Newly developed techniques delivered
not only improved performance, but also additional fea-
tures. Gharbi et al. proposed a convolutional neural
network (CNN) trained for joint demosaicing and de-
noising [17]. A recent work by Syu et al. proposes to
exploit CNNs for joint optimization of the color ﬁlter
array and a corresponding demosaicing ﬁlter [34].

Optimization of digital camera design can go even
further. The recently proposed L3 model by Jiang et al.
replaces the entire imaging pipeline with a large collec-
tion of local linear ﬁlters [19]. The L3 model reproduces
the entire photo development process, and aims to facil-
itate research and development eﬀorts for non-standard
camera designs. In the original paper, the model was
used for learning imaging pipelines for RGBW (red-
green-blue-white) and RGB-NIR (red-green-blue-near-
infra-red) color ﬁlter arrays.

Replacing the entire imaging pipeline with a mod-
ern CNN can also overcome long-standing limitations
of digital photography. Chen et al.
trained a UNet
model
[31] to develop high-quality photographs in
low-light conditions [9] by exposing it to paired ex-
amples of images taken with short and long expo-
sure. The network learned to develop high-quality well-
exposed color photographs from underexposed raw in-
put, and yielded better performance than traditional
image post-processing based on brightness adjustment
and denoising. Eilertsen et al. also trained a UNet
model to develop high-dynamic range images from a
single shot [15]. The network not only learned to cor-
rectly perform tone mapping, but was also able to re-
cover overexposed highlights. This signiﬁcantly sim-
pliﬁes HDR photography by eliminating the need for
bracketing and dealing with ghosting artifacts.

Trends in Forensics The current research in foren-
sic image analysis focuses on two main directions: (1)
learning deep features relevant to low-level forensic
analysis for problems like manipulation detection [3,
41], identiﬁcation of the social network of origin [2],
camera model identiﬁcation [11], or detection of arti-
ﬁcially generated content [27]; (2) adoption of high-
level vision to automate manual analysis that exposes
physical inconsistencies, such as reﬂections [33, 36], or
shadows [22]. To the best of our knowledge, there are
currently no eﬀorts to either assess the consequences

of the emerging neural imaging pipelines, or to exploit
this opportunity to improve photo reliability.

3. End-to-end Optimization of Photo

Provenance Analysis

Digital image forensics relies on intrinsic statisti-
cal artifacts introduced to photographs at the time of
their acquisition. Such traces are later used for reason-
ing about the source, authenticity and processing his-
tory of individual photographs. The main problem is
that contemporary media distribution channels employ
heavy compression and post-processing which destroy
the traces and inhibit forensic analysis.

The core of the proposed approach is to model the
entire acquisition and distribution channel, and opti-
mize the neural imaging pipeline (NIP) to facilitate
photo provenance analysis after content distribution
(Fig. 1). The analysis is performed by a forensic anal-
ysis network (FAN) which makes a decision about the
authenticity/processing history of the analyzed photo-
graph. In the presented example, the model is trained
to perform manipulation detection, i.e., aims to clas-
sify input images as either coming straight from the
camera, or as being aﬀected by a certain class of post-
processing. The distribution channel mimics the be-
havior of modern photo sharing services and social net-
works which habitually down-sample and re-compress
the photographs. As will be demonstrated later, foren-
sic analysis in such conditions is severely inhibited.

The parameters of the NIP are updated to guaran-
tee both faithful representation of a desired color pho-
tograph (L2 loss), and accurate decisions of forensics
analysis at the end of the distribution channel (cross-
entropy loss). Hence, the parameters of the NIP and
FAN models are chosen as:

nip = argmin
θ∗

(1a)

θnip Xn (cid:16)kyn − nip(xn | θnip)k2
log(cid:0)fanc(dc(nip(xn | θnip)) | θfan)(cid:1)(cid:17) (1b)
log(cid:0)fanc(dc(nip(xn | θnip)) | θfan)(cid:1)

+Xc
θfan Xn Xc

fan = argmin
θ∗

where: θnip/fan are the parameters of the NIP and FAN
networks, respectively; xn are the raw sensor measure-
ments for the n-th example patch; yn is the correspond-
ing target color image; nip(xn) is the color RGB image
developed by NIP from xn; dc() denotes a color image
patch processed by manipulation c; fanc() is the prob-
ability that an image belongs to the c-th manipulation
class, as estimated by the FAN model.

8623

RAW 
(h, w, 1) 

LUT 

mapping 

Normalization
& calibration 

White

balancing 

Demosaicing 

Denoising 

Color space
conversion 

Gamma 
correction 

RGB 
(h, w, 3)

½h, ½w, 4

NIP 

RGB 
(h, w, 3) 

Figure 2. Adoption of a neural imaging pipeline to develop raw sensor measurements into color RGB images: (top) the
standard imaging pipeline; (bottom) adoption of the NIP model.

3.1. The Neural Imaging Pipeline

We replace the entire imaging pipeline with a CNN
which develops raw sensor measurements into color
RGB images (Fig. 2). Before feeding the images to
the network, we pre-process them by reversing the non-
linear value mapping according to the camera’s LUT,
subtracting black levels from the edge of the sensor,
normalizing by sensor saturation values, and apply-
ing white-balancing according to shot settings. We
also standardized the inputs by reshaping the tensors
to have feature maps with successive measured color
channels. This ensures a well-formed input of shape
2 , w
( h
2 , 4) with values normalized to [0, 1]. All of the
remaining steps of the pipeline are replaced by a NIP.
See Section 4.1 for details on the considered pipelines.

3.2. Approximation of JPEG Compression

To enable end-to-end optimization of the entire ac-
quisition and distribution channel, we need to ensure
that every processing step remains diﬀerentiable.
In
the considered scenario, the main problem is JPEG
compression. We designed a dJPEG model which ap-
proximates the standard codec, and expresses its suc-
cessive steps as matrix multiplications or convolution
layers that can be implemented in TensorFlow (see sup-
plementary materials for a detailed network deﬁnition):

• RGB to/from YCbCr color-space conversions are

implemented as 1 × 1 convolutions.

• Isolation of 8×8 blocks for independent processing
is implemented by a combination of space-to-depth
and reshaping operations.

• Forward/backward 2D discrete cosine transforms
are implemented by matrix multiplication accord-
ing to DxDT where x denotes a 8 × 8 input, and
D denotes the transformation matrix.

• Division/multiplication of DCT coeﬃcients by the
corresponding quantization steps are implemented
as element-wise operations with properly tiled and
concatenated quantization matrices (for both the
luminance and chrominance channels).

• The actual quantization is approximated by a con-

tinuous function ρ(x)(see details below).

The key problem in making JPEG diﬀerentiable lies
in the rounding of DCT coeﬃcients. We considered
two approximations (Fig. 3a). Initially, we used a Tay-
lor series expansion, which can be made arbitrarily
accurate by including more terms. Finally, we used
a smoother, and simpler sinusoidal approximation ob-
tained by matching its phase with the sawtooth func-
tion:

ρ(x) = x −

sin(2πx)

2π

(2)

We validated our dJPEG model by comparing pro-
duced images with a reference codec from libJPEG. The
results are shown in Fig. 3bcd for a standard rounding
operation, and the two approximations, respectively.
We used 5 terms for the harmonic rounding. The devel-
oped module produces equivalent compression results
with standard rounding, and a good approximation for
its diﬀerentiable variants. Fig. 3e-h show a visual com-
parison of an example image patch, and its libJPEG
and dJPEG-compressed counterparts.

In our distribution channel, we used quality level 50.

3.3. The Forensic Analysis Network

The forensic analysis network (FAN) is implemented
as a CNN following the most recent recommendations
on construction of neural networks for forensics analy-
sis [3]. Bayar and Stamm proposed a new layer type,
which constrains the learned ﬁlters to be valid residual
ﬁlters [3]. Adoption of the layer helps ignore visual con-
tent and facilitates extraction of forensically-relevant
low-level features. In summary, our network operates
on 128 × 128 × 3 patches in the RGB color space and
includes (see supplement for full network deﬁnition):

• A constrained convolutions layer learning 5 × 5

residual ﬁlters and with no activation function.

• Four 5×5 convolutional layers with doubling num-
ber of output feature maps (starting from 32). The
layers use leaky ReLU activation and are followed
by 2 × 2 max pooling.

8624

(a) rounding approximations

(b) libJPEG vs dJPEG (standard)

(c) libJPEG vs dJPEG (harmonic)

(d) libJPEG vs dJPEG (sin)

round
harmonic
sinus

−1.5 −1 −0.5

0

0.5

1

1.5

Q10

30

Q

Q

Q90

Q80

Q70

Q60

Q50

Q40

Q30

Q20

Q90

Q80

Q70

Q60

Q50

Q40

Q30

Q20

Q10

35

40

45

50

55

30

35

40

45

50

55

30

Q90

Q80

Q70

Q60

Q50

Q40

Q30

Q20

]

B
d

[

G
E
P
J
b

i
l

r
o
f

R
N
S
P

40

35

30

40

45

50

55

Q10

35

(e) uncompressed (96x96px)

(f) libJPEG(50), PSNR=34.4 dB

(g) dJPEG(50), PSNR=35.5 dB

(h) dJPEG(50), PSNR=37.9 dB

PSNR for dJPEG [dB]

PSNR for dJPEG [dB]

PSNR for dJPEG [dB]

Figure 3. Implementation of JPEG compression as a fully diﬀerentiable dJPEG module: (a) continuous approximations
of the rounding function; (b)-(d) validation of the dJPEG module against the standard libJPEG library with standard
rounding, and the harmonic and sinusoidal approximations; (e) an example image patch; (f) standard JPEG compression
with quality 50; (g)-(h) dJPEG-compressed patches with the harmonic and sinusoidal approximations.

• A 1 × 1 convolutional layer mapping 256 features

into 256 features.

• A global average pooling layer reducing the num-

ber of features to 256.

• Two fully connected layers with 512 and 128 nodes

activated by leaky ReLU.

• A fully connected layer with N = 5 output nodes

and softmax activation.

The network has 1,341,990 parameters in total, and
outputs probabilities of 5 possible processing histories
(4 manipulation classes & straight from camera).

4. Experimental Evaluation

We started our evaluation by using several NIP mod-
els to reproduce the output of a standard imaging
pipeline (Sec. 4.1). Then, we used the FAN model to
detect popular image manipulations (Sec. 4.2).
Ini-
tially, we validated that the models work correctly by
using it without a distribution channel (Sec. 4.3). Fi-
nally, we performed extensive evaluation of the entire
acquisition and distribution network (Sec. 4.4).

We collected a data-set with RAW images from 8
cameras (Table 1). The photographs come from two
public (Raise [12] and MIT-5k [7]) and from one private
data-set. For each camera, we randomly selected 150
images with landscape orientation. These images will
later be divided into separate training/validation sets.

Table 1. Digital cameras used in our experiments

Camera

SR1 #Images2 Source Bayer

Canon EOS 5D
Canon EOS 40D
Nikon D5100
Nikon D700
Nikon D7000
Nikon D750
Nikon D810
Nikon D90

12
10
16
12
16
24
36
12

864 dng MIT-5k RGGB
313 dng MIT-5k RGGB
288 nef
Private RGGB
590 dng MIT-5k RGGB
Raise RGGB
>1k nef
312 nef
Private RGGB
Private RGGB
205 nef
>1k nef
Raise GBRG

1 Sensor Resolution in Megapixels [Mpx]
2 RAW ﬁle formats: nef (Nikon); dng (generic, Adobe)

4.1. Neural Imaging Pipelines

We considered three NIP models with various com-
plexity and design principles (Table 2): INet - a sim-
ple convolutional network with layers corresponding to
successive steps of the standard pipeline; UNet - the
well-known UNet architecture [31] adapted from [9];
DNet - adaptation of the model originally used for
joint demosaicing and denoising [17]. Details of the
networks’ architectures are included in the supplement.
We trained a separate model for each camera. For
training, we used 120 full-resolution images.
In each
iteration, we extracted random 128 × 128 patches and
formed a batch with 20 examples (one patch per im-
age). For validation, we used a ﬁxed set of 512×512 px
patches extracted from the remaining 30 images. The
models were trained to reproduce the output of a stan-

8625

Native patch

Sharpen

Gaussian ﬁltering

JPEGNet

Resampling

optimization target bilinear demosaicing

converged pipeline

(a) Improved demosaicing example (Canon EOS 40D)

optimization target

converged pipeline

i

n
o
i
t
a
l
u
p
n
a
m
y
l
n
O

)
a
(
n
o
i
t
u
b

i
r
t
s
i
d

r
e
t
f
A
)
b
(

(b) Improved denoising example (Nikon D5100)

Figure 4. Examples of serendipitous image quality improve-
ments obtained by neural imaging pipelines: (a) better de-
mosaicing performance; (b) better denoising.

Table 2. Considered neural imaging pipelines

# Parameters
PSNR
SSIM
Train. speed [it/s]
Train. time

INet

321
42.8
0.989
8.80

17 - 26 min

UNet

DNet

7,760,268

493,976

44.3
0.990
1.75
2-4 h

46.2
0.995
0.66

12 - 22 h

dard imaging pipeline. We used our own implemen-
tation based on Python and rawkit [8] wrappers over
libRAW [28]. Demosaicing was performed using an
adaptive algorithm by Menon et al. [29].

All NIPs successfully reproduced target images with
high ﬁdelity. The resulting color photographs are vi-
sually indistinguishable from the targets. Objective ﬁ-
delity measurements for the validation set are collected
in Table 2 (average for all 8 cameras). Interestingly, the
trained models often revealed better denoising and de-
mosaicing performance, despite the lack of a denoising
step in the simulated pipeline, and the lack of explicit
optimization objectives (see Fig. 4).

Of all of the considered models, INet was the easi-
est to train - not only due to its simplicity, but also
because it could be initialized with meaningful pa-
rameters that already produced valid results and only
needed ﬁne-tuning. We initialized the demosaicing ﬁl-
ters with bilinear interpolation, color space conversion
with a known multiplication matrix, and gamma cor-
rection with a toy model separately trained to repro-
duce this non-linearity. The UNet model was initial-
ized randomly, but improved rapidly thanks to skip
connections. The DNet model took the longest and
for a long time had problems with faithful color ren-
dering. The typical training times are reported in Ta-

Figure 5. An example image patch with all of the considered
manipulations: (a) only manipulation; (b) after the distri-
bution channel (down-sampled and JPEG compressed).

ble 2. The measurements were collected on a Nvidia
Tesla K80 GPU. The models were trained until the rel-
ative change of the average validation loss for the last 5
dropped below 10−4. The maximum number of epochs
was 50,000. For the DNet model we adjusted the stop-
ping criterion to 10−5 since the training progress was
slow, and often terminated prematurely with incorrect
color rendering.

4.2. Image Manipulation

Our experiment mirrors the standard setup for im-
age manipulation detection [3, 5, 16]. The FAN simply
decides which manipulation class the input patch be-
longs to. This approximates identiﬁcation of the last
post-processing step, which is often used to mask traces
of more invasive modiﬁcations.

We consider four mild post-processing operations:
sharpening - implemented as an unsharp mask operator
with the following kernel:

1

6"−1 −4 −1
−1 −4 −1#

26 −4

−4

(3)

applied to the luminance channel in the HSV color
space; resampling - implemented as successive 1:2
down-sampling and 2:1 up-sampling using bilinear in-
terpolation; Gaussian ﬁltering - implemented using a
convolutional layer with a 5 × 5 ﬁlter and standard
deviation 0.83; JPG compression - implemented using
the dJPEG module with sinusoidal rounding approx-
imation and quality level 80. Fig. 5 shows the post-
processed variants of an example image patch: (a) just
after manipulation; and (b) after the distribution chan-
nel (as seen by the FAN module).

4.3. FAN Model Validation

To validate our FAN model, we initially imple-
mented a simple experiment, where analysis is per-
formed just after image manipulation (no distribution
channel distortion, as in [3]). We used the UNet model

8626

.
s
e
r

11
*
18
12
21

.
s
e
r

4
*
10
4
77

.
s
e
r

*

99

43

41

39

37

35

33

1.0

.8

.6

.4

.2

]

B
d

[

R
N
S
P

]

%

[

y
c
a
r
u
c
c
a
N
A
F

FAN only

Joint FAN+NIP

Table 3. Typical confusion matrices (Nikon D90). Entries
≈ 0 are not shown; entries / 3% are marked with (*).

(a) standalone FAN optimization (UNet) → 44.2%

True

native
sharpen
gaussian
jpg
resample

Predicted

.
t
a
n

27
*
7
26
14

.
a
h
s

24
93
4
23
7

.
u
a
g

18
3
59
18
38

g
p
j

20
*
12
21
20

(b) joint FAN+NIP optimization (INet) → 55.2%

0

2

4

6

8

10

12

14

16

18

20

Predicted

Training progress [sampling steps]

Figure 6. Typical progression of validation metrics (Nikon
D90) for standalone FAN training (F) and joint optimiza-
tion of FAN and NIP models (F+N).

True

native
sharpen
gaussian
jpg
resample

.
t
a
n

41
7
18
41
5

.
a
h
s

16
85
8
16
*

.
u
a
g

22
3
54
21
14

g
p
j

18
4
10
19
*

to develop smaller image patches to guarantee the same
size of the inputs for the FAN model (128×128×3 RGB
images). In such conditions, the model works just as
expected, and yields classiﬁcation accuracy of 99% [3].

4.4. Imaging Pipeline Optimization

In this experiment, we perform forensic analysis at
the end of the distribution channel. We consider two
optimization modes: (F) only the FAN network is op-
timized given a ﬁxed NIP model; (F+N) both the FAN
and NIP models are optimized jointly. In both cases,
the NIPs are pre-initialized with previously trained
models (Section 4.1). The training was implemented
with two separate Adam optimizers, where the ﬁrst
one updates the FAN (and in the F+N mode also the
NIP) and the second one updates the NIP based on the
image ﬁdelity objective.

Similarly to previous experiments, we used 120 full-
resolution images for training, and the remaining 30
images for validation. From training images, in each
iteration we randomly extract new patches. The val-
idation set is ﬁxed at the beginning and includes 100
random patches per each image (3,000 patches in to-
tal) for classiﬁcation accuracy assessment. To speed-up
image ﬁdelity evaluation, we used 2 patches per im-
age (60 patches in total).
In order to prevent over-
representation of empty patches, we bias the selection
by outward rejection of patches with pixel variance <
0.01, and by 50% chance of keeping patches with vari-
ance < 0.02. More diverse patches are always accepted.
Due to computational constraints, we performed the
experiment for 4 cameras (Canon EOS 40D and EOS
5D, and Nikon D7000, and D90, see Table 1) and for
the INet and UNet models only. (Based on prelimi-
nary experiments, we excluded the DNet model which
rapidly lost and could not regain image representation

(c) joint FAN+NIP optimization (UNet) → 94.0%

True

native
sharpen
gaussian
jpg
resample

Predicted

.
a
h
s

100

.
t
a
n

90

13

.

u
a
g

*

97
3
*

g
p
j

9

*
84

ﬁdelity.) We ran the optimization for 1,000 epochs,
starting with a learning rate of 10−4 and systematically
decreasing by 15% every 100 epochs. Each run was re-
peated 10 times. The typical progression of validation
metrics for the UNet model (classiﬁcation accuracy
and distortion PSNR) is shown in Fig. 6 (sampled ev-
ery 50 epochs). The distribution channel signiﬁcantly
disrupts forensic analysis, and the classiﬁcation accu-
racy drops to ≈ 45% for standalone FAN optimization
(F). In particular, the FAN struggles to identify three
low-pass ﬁltering operations (Gaussain ﬁltering, JPEG
compression, and re-sampling; see confusion matrix in
Tab. 3a), which bear strong visual resemblance at the
end of the distribution channel (Fig. 5b). Optimization
of the imaging pipeline signiﬁcantly increases classiﬁca-
tion accuracy (over 90%) and makes the manipulation
paths easily distinguishable (Tab. 3c). Most mistakes
involve the native and jpeg compressed classes. Fig. 7
collects the obtained results for both the UNet and INet
models. While UNet delivers consistent and signiﬁcant
beneﬁts, INet is much simpler and yields only modest
improvements in accuracy - up to ≈ 55%. It also lacked
consistency and for one of the tested cameras yielded
virtually no beneﬁts.

The observed improvement in forensic accuracy
comes at the cost of image distortion, and leads to
artifacts in the developed photographs. In our experi-
ments, photo development ﬁdelity dropped to ≈36 dB

8627

(a) the INet model

(b) the UNet model

Nikon D90 (F+N)

Nikon D90 (F)

Nikon D7000 (F+N)

Nikon D7000 (F)

Canon EOS 40D (F+N)

Canon EOS 40D (F)

Canon EOS 5D (F+N)

Canon EOS 5D (F)

.4

.5

.6

.8
Classiﬁcation accuracy [%]

.7

.9

1

.4

.5

.6

.8
Classiﬁcation accuracy [%]

.7

.9

1

Figure 7. Validation accuracy for image manipulation detection after the distribution channel: (F) denotes standalone FAN
training given a ﬁxed NIP; (F+N) denotes joint optimization of the FAN and NIP models.

e
c
n
e
r
e
f
e
r

t
e
N
U
)
F
(

t
s
r
o
w

-

t
e
N
U
)
N
+
F
(

t
s
e
b

-

t
e
N
U
)
N
+
F
(

e
c
n
e
r
e
f
e
r

t
e
N
I

)
F
(

l
a
c
i
p
y
t

-

t
e
N
I

)
N
+
F
(

Figure 8. Example image patches developed with joint NIP
and FAN optimization (F+N) for Nikon D90. The UNet
examples show the best/worst artifacts. The INet examples
show typical outputs.

(PSNR) / 0.96 (SSIM). Detailed results are collected
in Tab. 4. The severity of the distortions varied with
training repetitions. Qualitative illustration of the ob-
served artifacts is shown in Fig. 8. The ﬁgure shows
diverse image patches developed by several NIP vari-
ants (diﬀerent training runs). The artifacts vary in
severity from disturbing to imperceptible, and tend to
be well masked by image content. INet’s artifacts tend
to be less perceptible, which compensates for modest
improvements in FAN’s classiﬁcation accuracy.

Table 4. Fidelity-accuracy trade-oﬀ for joint optimization.

Camera

PSNR [dB]

SSIM

Acc. [%]

42.7 → 35.9
D90
43.0 → 35.6
D7000
EOS 40D 42.8 → 36.1
EOS 5D
43.0 → 36.2

43.3 → 37.2
D90
D7000
40.6 → 35.4
EOS 40D 41.6 → 33.1
41.5 → 40.7
EOS 5D

UNet model

0.990 → 0.960
0.990 → 0.955
0.990 → 0.962
0.989 → 0.961

INet model

0.992 → 0.969
0.985 → 0.959
0.985 → 0.934
0.986 → 0.984

0.45 → 0.91
0.42 → 0.91
0.39 → 0.92
0.45 → 0.89

0.44 → 0.55
0.43 → 0.53
0.38 → 0.50
0.42 → 0.42

5. Discussion and Future Work

We replaced the photo acquisition pipeline with a
CNN, and developed a fully-diﬀerentiable model of the
entire acquisition and distribution workﬂow. This al-
lows for joint optimization of photo analysis and acqui-
sition. Our results show that it is possible to optimize
the imaging pipeline to facilitate provenance analysis at
the end of complex distribution channels. We observed
signiﬁcant improvements in manipulation detection ac-
curacy w.r.t state-of-the-art classical forensics [3] (in-
crease from ≈ 45% to over 90%).

Competing optimization objectives lead to imaging
artifacts which tend to be well masked in textured ar-
eas, but are currently too visible in ﬂat regions. Sever-
ity of the artifacts varies between training runs and NIP
architectures, and suggests that better conﬁgurations
should be achievable by learning to control the ﬁdelity-
accuracy trade-oﬀ. Since ﬁnal decisions are often taken
by pooling predictions for many patches, we believe a
good balance should be achievable even with relatively
simple models. Further improvements may also possi-
ble by exploring diﬀerent NIP architectures, or explic-
itly modeling HVS characteristics. Future work should
also assess generalization capabilities to other manip-
ulations, and more complex forensic problems. It may
also be interesting to optimize other components of the
workﬂow, e.g., the lossy compression in the channel.

8628

References

[1] S. Agarwal and H. Farid. Photo forensics from jpeg dimples.
In Information Forensics and Security (WIFS), 2017 IEEE
Workshop on, pages 1–6. IEEE, 2017.

[2] I. Amerini, T. Uricchio, and R. Caldelli. Tracing images
back to their social network of origin: A cnn-based ap-
proach. In IEEE Int. Workshop Information Forensics and
Security, 2017.

[3] B. Bayar and M. Stamm. Constrained convolutional neural
networks: A new approach towards general purpose image
manipulation detection. IEEE Tran. Information Forensics
and Security, 13(11):2691–2706, 2018.

[4] P. Blythe and J. Fridrich. Secure digital camera. In Digital

Forensic Research Workshop, pages 11–13, 2004.

[5] M. Boroumand and J. Fridrich. Deep learning for detecting
processing history of images. Electronic Imaging, 2018(7):1–
9, 2018.

[6] H. Burger, C. Schuler, and S. Harmeling. Image denoising:
Can plain neural networks compete with bm3d?
In Int.
Conf. Computer Vision and Pattern Recognition (CVPR),
pages 2392–2399. IEEE, 2012.

[7] V. Bychkovsky, S. Paris, E. Chan, and E. Durand. Learning
photographic global tonal adjustment with a database of
input/output image pairs. In IEEE Conf. Computer Vision
and Pattern Recognition, 2011.

[8] P. Cameron and S. Whited. Rawkit. https://rawkit.

readthedocs.io/en/latest/. Visited Nov 2018.

neural networks. In Applications of Artiﬁcial Neural Net-
works in Image Processing V, volume 3962, pages 112–121,
2000.

[22] E. Kee, J. O’brien, and H. Farid. Exposing photo manip-
ulation from shading and shadows. ACM Trans. Graph.,
33(5), 2014.

[23] F. Kokkinos and S. Lefkimmiatis. Deep image demosaick-
ing using a cascade of convolutional residual denoising net-
works. arXiv, 2018. arXiv:1803.05215v4.

[24] P. Korus. Digital image integrity–a survey of protection and
veriﬁcation techniques. Digital Signal Processing, 71:1–26,
2017.

[25] P. Korus, J. Bialas, and A. Dziech. Towards practical
self-embedding for JPEG-compressed digital images. IEEE
Trans. Multimedia, 17(2):157–170, Feb 2015.

[26] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T.
Karras, M. Aittala, and T. Aila. Noise2noise: Learn-
ing image restoration without clean data. arXiv preprint
arXiv:1803.04189, 2018.

[27] H. Li, B. Li, S. Tan, and J. Huang. Detection of deep
network generated images using disparities in color compo-
nents. arXiv preprint arXiv:1808.07276, 2018.

[28] LibRaw LLC. libRAW. https://www.libraw.org/. Visited

Nov 2018.

[29] D. Menon, S. Andriani, and G. Calvagno. Demosaicing with
directional ﬁltering and a posteriori decision. IEEE Tran.
Image Processing, 16(1):132–141, 2007.

[30] A. Piva. An overview on image forensics.

ISRN Signal

[9] C. Chen, Q. Chen, J. Xu, and V. Koltun. Learning to see

Processing, 2013, 2013. Article ID 496701.

in the dark. arXiv, 2018. arXiv:1805.01934.

[10] M. Chen, J. Fridrich, M. Goljan, and J. Lukas. Determining
image origin and integrity using sensor noise. IEEE Trans.
Inf. Forensics Security, 3(1):74–90, 2008.

[11] D. Cozzolino

and L. Verdoliva.

cnn-based camera model ﬁngerprint.
arXiv:1808.08396, 2018.

a
Noiseprint:
arXiv preprint

[12] D. Dang-Nguyen, C. Pasquini, V. Conotter, and G. Boato.
RAISE - a raw images dataset for digital image forensics.
In Proc. of ACM Multimedia Systems, 2015.

[13] Z. Dias, S. Goldenstein, and A. Rocha. Large-scale image
IEEE

phylogeny: Tracing image ancestral relationships.
Multimedia, 20(3):58–70, 2013.

[14] Z. Dias, S. Goldenstein, and A. Rocha. Toward image phy-
logeny forests: Automatically recovering semantically sim-
ilar image relationships. Forensic Science International,
231(1):178–189, 2013.

[15] G. Eilertsen, J. Kronander, G. Denes, R. Mantiuk, and J.
Unger. HDR image reconstruction from a single exposure
using deep CNNs. ACM Tran. Graphics, 36(6):1–15, nov
2017.

[16] W. Fan, K. Wang, and F. Cayre. General-purpose image
forensics using patch likelihood under image statistical mod-
els. In Proc. of IEEE Int. Workshop on Inf. Forensics and
Security, 2015.

[17] M. Gharbi, G. Chaurasia, S. Paris, and F. Durand. Deep
joint demosaicking and denoising. ACM Tran. Graphics,
35(6):1–12, nov 2016.

[18] A. Ignatov, N. Kobyshev, R. Timofte, K. Vanhoey, and L.
Van Gool. Wespe: weakly supervised photo enhancer for
digital cameras. arXiv preprint arXiv:1709.01118, 2017.

[19] H. Jiang, Q. Tian, J. Farrell, and B. A. Wandell. Learning
the image processing pipeline. IEEE Tran. Image Process-
ing, 26(10):5032–5042, oct 2017.

[20] Joint Photographic Experts Group.

JPEG Privacy
& Security. https://jpeg.org/items/20150910_privacy_
security_summary.html. Visited Oct 2018.

[21] O. Kapah and H. Z. Hel-Or. Demosaicking using artiﬁcial

[31] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In Int.
Conf. Medical Image Computing and Computer-assisted In-
tervention. Springer, 2015.

[32] M. Stamm, M. Wu, and KJ Liu. Information forensics: An
overview of the ﬁrst decade. IEEE Access, 1:167–200, 2013.
[33] Z. H Sun and A. Hoogs. Object insertion and removal in
images with mirror reﬂection. In IEEE Workshop on Infor-
mation Forensics and Security, pages 1–6, 2017.

[34] N.-S. Syu, Y.-S. Chen, and Y.-Y. Chuang.

Learning
deep convolutional networks for demosaicing. arXiv, 2018.
arXiv:1802.03769.

[35] R. Tan, K. Zhang, W. Zuo, and L. Zhang. Color image
demosaicking via deep residual learning. In IEEE Int. Conf.
Multimedia and Expo, 2017.

[36] E. Wengrowski, Z. Sun, and A. Hoogs. Reﬂection corre-
spondence for exposing photograph manipulation. In IEEE
Int. Conf. Image Processing, pages 4317–4321, 2017.

[37] B. Wronski and P. Milanfar. See better and further with su-
per res zoom on the pixel 3. https://ai.googleblog.com/
2018/10/see-better-and-further-with-super-res.html.
Visited in Oct 2018.

[38] C. P. Yan and C. M. Pun. Multi-scale diﬀerence map
fusion for tamper localization using binary ranking hash-
ing.
IEEE Tran. Information Forensics and Security,
12(9):2144–2158, 2017.

[39] M. Zampoglou, S. Papadopoulos, and Y. Kompatsiaris.
Large-scale evaluation of splicing localization algorithms
for web images. Multimedia Tools and Applications,
76(4):4801–4834, 2017.

[40] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang.
learning of deep
IEEE Tran. Image Processing,

Beyond a gaussian denoiser: Residual
cnn for image denoising.
26(7):3142–3155, 2017.

[41] P. Zhou, X. Han, V. Morariu, and L. S. Davis. Learning
rich features for image manipulation detection.
In IEEE
Int. Conf. Computer Vision and Pattern Recognition, pages
1053–1061, 2018.

8629

