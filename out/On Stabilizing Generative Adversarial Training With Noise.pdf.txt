On Stabilizing Generative Adversarial Training with Noise

Simon Jenni

Paolo Favaro

University of Bern

{simon.jenni,paolo.favaro}@inf.unibe.ch

Abstract

We present a novel method and analysis to train gener-
ative adversarial networks (GAN) in a stable manner. As
shown in recent analysis, training is often undermined by
the probability distribution of the data being zero on neigh-
borhoods of the data space. We notice that the distribu-
tions of real and generated data should match even when
they undergo the same ﬁltering. Therefore, to address the
limited support problem we propose to train GANs by us-
ing different ﬁltered versions of the real and generated data
distributions. In this way, ﬁltering does not prevent the ex-
act matching of the data distribution, while helping training
by extending the support of both distributions. As ﬁltering
we consider adding samples from an arbitrary distribution
to the data, which corresponds to a convolution of the data
distribution with the arbitrary one. We also propose to learn
the generation of these samples so as to challenge the dis-
criminator in the adversarial training. We show that our
approach results in a stable and well-behaved training of
even the original minimax GAN formulation. Moreover, our
technique can be incorporated in most modern GAN formu-
lations and leads to a consistent improvement on several
common datasets.

1. Introduction

Since the seminal work of [6], generative adversarial net-
works (GAN) have been widely used and analyzed due to
the quality of the samples that they produce, in particular
when applied to the space of natural images. Unfortunately,
GANs still prove difﬁcult to train. In fact, a vanilla imple-
mentation does not converge to a high-quality sample gen-
erator and heuristics used to improve the generator often ex-
hibit an unstable behavior. This has led to a substantial work
to better understand GANs (see, for instance, [23, 19, 1]). In
particular, [1] points out how the unstable training of GANs
is due to the (limited and low-dimensional) support of the
data and model distributions. In the original GAN formu-
lation, the generator is trained against a discriminator in a
minimax optimization problem. The discriminator learns

(a)

(b)

(c)

Figure 1: (a) When the probability density functions of the
real pd and generated data pg do not overlap, then the dis-
criminator can easily distinguish samples. The gradient of
the discriminator with respect to its input is zero in these
regions and this prevents any further improvement of the
generator. (b) Adding samples from an arbitrary pǫ to those
of the real and the generated data results in the ﬁltered ver-
sions pd ∗ pǫ and pg ∗ pǫ. Because the supports of the ﬁl-
tered distributions overlap, the gradient of the discriminator
is not zero and the generator can improve. However, the
high-frequency content of the original distributions is miss-
ing. (c) By varying pǫ, the generator can learn to match the
data distribution accurately thanks to the extended supports.

to distinguish real from fake samples, while the generator
learns to generate fake samples that can fool the discrimi-
nator. When the support of the data and model distributions
is disjoint, the generator stops improving as soon as the dis-
criminator achieves perfect classiﬁcation, because this pre-
vents the propagation of useful information to the generator
through gradient descent (see Fig. 1a).

The recent work by [1] proposes to extend the support
of the distributions by adding noise to both generated and
real images before they are fed as input to the discrimi-
nator. This procedure results in a smoothing of both data

112145

and model probability distributions, which indeed increases
their support extent (see Fig. 1b). For simplicity, let us
assume that the probability density function of the data is
well deﬁned and let us denote it with pd. Then, samples
˜x = x + ǫ, obtained by adding noise ǫ ∼ pǫ to the data sam-
ples x ∼ pd, are also instances of the probability density
function pd,ǫ = pǫ ∗ pd, where ∗ denotes the convolution
operator. The support of pd,ǫ is the Minkowski sum of the
supports of pǫ and pd and thus larger than the support of
pd. Similarly, adding noise to the samples from the gen-
erator probability density pg leads to the smoothed proba-
bility density pg,ǫ = pǫ ∗ pg. Adding noise is a quite well-
known technique that has been used in maximum likelihood
methods, but is considered undesirable as it yields approx-
imate generative models that produce low-quality blurry
samples. Indeed, most formulations with additive noise boil
down to ﬁnding the model distribution pg that best solves
pd,ǫ = pg,ǫ. However, this usually results in a low qual-
ity estimate pg because pd ∗ pǫ has lost the high frequency
content of pd. An immediate solution is to use a form of
noise annealing, where the noise variance is initially high
and is then reduced gradually during the iterations so that
the original distributions, rather than the smooth ones, are
eventually matched. This results in an improved training,
but as the noise variance approaches zero, the optimization
problem converges to the original formulation and the algo-
rithm may be subject to the usual unstable behavior.

In this work, we design a novel adversarial training pro-
cedure that is stable and yields accurate results. We show
that under some general assumptions it is possible to mod-
ify both the data and generated probability densities with
additional noise without affecting the optimality conditions
of the original noise-free formulation. As an alternative to
the original formulation, with z ∼ N (0, Id) and x ∼ pd,

min

G

max

D

Ex[log D(x)] + Ez[log(1 − D(G(z)))],

(1)

where D denotes the discriminator, we propose to train a
generative model G by solving instead the following opti-
mization

min

G

max

D Xpǫ∈S

Eǫ∼pǫ [Ex∼pd [log D(x + ǫ)]] +

(2)

Eǫ∼pǫ (cid:2)Ez∼N (0,Id)[log(1 − D(G(z) + ǫ))](cid:3) ,

where we introduced a set S of probability density func-
tions. If we solve the innermost optimization problem in
Problem (2), then we obtain the optimal discriminator

D(x) =

,

(3)

Ppǫ∈S pd,ǫ(x)

Ppǫ∈S pd,ǫ(x) + pg,ǫ(x)

where we have deﬁned pg as the probability density of
G(z), where z ∼ N (0, Id).
If we substitute this in the

z<

noise
sample

Generator G

noise
sample

Generator N

<

G(z)

G(z) + 

x + 

real image

sample

x<

Discriminator D

fake

real

Figure 2: Simpliﬁed scheme of the proposed GAN training.
We also show a noise generator N that is explained in detail
in Section 3.1. The discriminator D needs to distinguish
both noise-free and noisy real samples from fake ones.

problem above and simplify we have

min

G

JSD(cid:16) 1

|S| Ppǫ∈S pd,ǫ, 1

|S| Ppǫ∈S pg,ǫ(cid:17) ,

(4)

where JSD is the Jensen-Shannon divergence. We show
that, under suitable assumptions, the optimal solution of
Problem (4) is unique and pg = pd. Moreover, since

1/|S|Ppǫ∈S pd,ǫ enjoys a larger support than pd, the opti-

mization via iterative methods based on gradient descent is
more likely to achieve the global minimum, regardless of
the support of pd. Thus, our formulation enjoys the follow-
ing properties: 1) It deﬁnes a ﬁtting of probability densities
that is not affected by their support; 2) It guarantees the ex-
act matching of the data probability density function; 3) It
can be easily applied to other GAN formulations. A sim-
pliﬁed scheme of the proposed approach is shown in Fig. 2.

In the next sections we introduce our analysis in detail
and then devise a computationally feasible approximation
of the problem formulation (2). Our method is evaluated
quantitatively on CIFAR-10 [12], STL-10 [5], and CelebA
[15], and qualitatively on ImageNet [20] and LSUN bed-
rooms [24].

2. Related Work

The inherent instability of GAN training was ﬁrst ad-
dressed through a set of techniques and heuristics [22] and
careful architectural design choices and hyper-parameter
tuning [18].
[22] proposes the use of one-sided label
smoothing and the injection of Gaussian noise into the lay-
ers of the discriminator. A theoretical analysis of the un-
stable training and the vanishing gradients phenomena was
introduced by Arjovsky et al. [1]. They argue that the main
source of instability stems from the fact that the real and
the generated distributions have disjoint supports or lie on
low-dimensional manifolds. In the case of an optimal dis-
criminator this will result in zero gradients that then stop the

12146

l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
C
/
G
3
N
f
a
c
E
T
g
j
b
T
b
h
t
V
7
R
e
j
a
z
c
f
U
=
"
>
A
A
A
B
8
H
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
E
O
S
J
c
x
O
Z
p
M
h
8
1
h
m
Z
o
W
w
5
C
u
8
e
F
D
E
q
5
/
j
z
b
9
x
N
t
m
D
J
h
Y
0
F
F
X
d
d
H
d
F
C
W
f
G
+
v
6
3
V
1
p
b
3
9
j
c
K
m
9
X
d
n
b
3
9
g
+
q
h
0
d
t
o
1
J
N
a
I
s
o
r
n
Q
3
w
o
Z
y
J
m
n
L
M
s
t
p
N
9
E
U
i
4
j
T
T
j
S
5
z
f
3
O
E
9
W
G
K
f
l
g
p
w
k
N
B
R
5
J
F
j
O
C
r
Z
M
e
+
z
Q
x
j
C
t
Z
G
V
R
r
f
t
2
f
A
6
2
S
o
C
A
1
K
N
A
c
V
L
/
6
Q
0
V
S
Q
a
U
l
H
B
v
T
C
/
z
E
h
h
n
W
l
h
F
O
Z
5
V
+
a
m
i
C
y
Q
S
P
a
M
9
R
i
Q
U
1
Y
T
Y
/
e
I
b
O
n
D
J
E
s
d
K
u
p
E
V
z
9
f
d
E
h
o
U
x
U
x
G
5
T
o
H
t
2
C
x
7
u
f
i
f
1
0
t
t
f
B
1
m
T
C
a
p
p
Z
I
s
F
s
U
p
R
1
a
h
/
H
s
0
Z
J
o
S
y
6
e
O
Y
K
K
Z
u
x
W
R
M
d
a
Y
W
J
d
R
H
k
K
w
/
P
I
q
a
V
/
U
A
7
8
e
3
F
/
W
G
j
d
F
H
G
U
4
g
V
M
4
h
w
C
u
o
A
F
3
0
I
Q
W
E
B
D
w
D
K
/
w
5
m
n
v
x
X
v
3
P
h
a
t
J
a
+
Y
O
Y
Y
/
8
D
5
/
A
I
R
B
k
D
Q
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
C
/
G
3
N
f
a
c
E
T
g
j
b
T
b
h
t
V
7
R
e
j
a
z
c
f
U
=
"
>
A
A
A
B
8
H
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
E
O
S
J
c
x
O
Z
p
M
h
8
1
h
m
Z
o
W
w
5
C
u
8
e
F
D
E
q
5
/
j
z
b
9
x
N
t
m
D
J
h
Y
0
F
F
X
d
d
H
d
F
C
W
f
G
+
v
6
3
V
1
p
b
3
9
j
c
K
m
9
X
d
n
b
3
9
g
+
q
h
0
d
t
o
1
J
N
a
I
s
o
r
n
Q
3
w
o
Z
y
J
m
n
L
M
s
t
p
N
9
E
U
i
4
j
T
T
j
S
5
z
f
3
O
E
9
W
G
K
f
l
g
p
w
k
N
B
R
5
J
F
j
O
C
r
Z
M
e
+
z
Q
x
j
C
t
Z
G
V
R
r
f
t
2
f
A
6
2
S
o
C
A
1
K
N
A
c
V
L
/
6
Q
0
V
S
Q
a
U
l
H
B
v
T
C
/
z
E
h
h
n
W
l
h
F
O
Z
5
V
+
a
m
i
C
y
Q
S
P
a
M
9
R
i
Q
U
1
Y
T
Y
/
e
I
b
O
n
D
J
E
s
d
K
u
p
E
V
z
9
f
d
E
h
o
U
x
U
x
G
5
T
o
H
t
2
C
x
7
u
f
i
f
1
0
t
t
f
B
1
m
T
C
a
p
p
Z
I
s
F
s
U
p
R
1
a
h
/
H
s
0
Z
J
o
S
y
6
e
O
Y
K
K
Z
u
x
W
R
M
d
a
Y
W
J
d
R
H
k
K
w
/
P
I
q
a
V
/
U
A
7
8
e
3
F
/
W
G
j
d
F
H
G
U
4
g
V
M
4
h
w
C
u
o
A
F
3
0
I
Q
W
E
B
D
w
D
K
/
w
5
m
n
v
x
X
v
3
P
h
a
t
J
a
+
Y
O
Y
Y
/
8
D
5
/
A
I
R
B
k
D
Q
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
C
/
G
3
N
f
a
c
E
T
g
j
b
T
b
h
t
V
7
R
e
j
a
z
c
f
U
=
"
>
A
A
A
B
8
H
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
E
O
S
J
c
x
O
Z
p
M
h
8
1
h
m
Z
o
W
w
5
C
u
8
e
F
D
E
q
5
/
j
z
b
9
x
N
t
m
D
J
h
Y
0
F
F
X
d
d
H
d
F
C
W
f
G
+
v
6
3
V
1
p
b
3
9
j
c
K
m
9
X
d
n
b
3
9
g
+
q
h
0
d
t
o
1
J
N
a
I
s
o
r
n
Q
3
w
o
Z
y
J
m
n
L
M
s
t
p
N
9
E
U
i
4
j
T
T
j
S
5
z
f
3
O
E
9
W
G
K
f
l
g
p
w
k
N
B
R
5
J
F
j
O
C
r
Z
M
e
+
z
Q
x
j
C
t
Z
G
V
R
r
f
t
2
f
A
6
2
S
o
C
A
1
K
N
A
c
V
L
/
6
Q
0
V
S
Q
a
U
l
H
B
v
T
C
/
z
E
h
h
n
W
l
h
F
O
Z
5
V
+
a
m
i
C
y
Q
S
P
a
M
9
R
i
Q
U
1
Y
T
Y
/
e
I
b
O
n
D
J
E
s
d
K
u
p
E
V
z
9
f
d
E
h
o
U
x
U
x
G
5
T
o
H
t
2
C
x
7
u
f
i
f
1
0
t
t
f
B
1
m
T
C
a
p
p
Z
I
s
F
s
U
p
R
1
a
h
/
H
s
0
Z
J
o
S
y
6
e
O
Y
K
K
Z
u
x
W
R
M
d
a
Y
W
J
d
R
H
k
K
w
/
P
I
q
a
V
/
U
A
7
8
e
3
F
/
W
G
j
d
F
H
G
U
4
g
V
M
4
h
w
C
u
o
A
F
3
0
I
Q
W
E
B
D
w
D
K
/
w
5
m
n
v
x
X
v
3
P
h
a
t
J
a
+
Y
O
Y
Y
/
8
D
5
/
A
I
R
B
k
D
Q
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
C
/
G
3
N
f
a
c
E
T
g
j
b
T
b
h
t
V
7
R
e
j
a
z
c
f
U
=
"
>
A
A
A
B
8
H
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
E
O
S
J
c
x
O
Z
p
M
h
8
1
h
m
Z
o
W
w
5
C
u
8
e
F
D
E
q
5
/
j
z
b
9
x
N
t
m
D
J
h
Y
0
F
F
X
d
d
H
d
F
C
W
f
G
+
v
6
3
V
1
p
b
3
9
j
c
K
m
9
X
d
n
b
3
9
g
+
q
h
0
d
t
o
1
J
N
a
I
s
o
r
n
Q
3
w
o
Z
y
J
m
n
L
M
s
t
p
N
9
E
U
i
4
j
T
T
j
S
5
z
f
3
O
E
9
W
G
K
f
l
g
p
w
k
N
B
R
5
J
F
j
O
C
r
Z
M
e
+
z
Q
x
j
C
t
Z
G
V
R
r
f
t
2
f
A
6
2
S
o
C
A
1
K
N
A
c
V
L
/
6
Q
0
V
S
Q
a
U
l
H
B
v
T
C
/
z
E
h
h
n
W
l
h
F
O
Z
5
V
+
a
m
i
C
y
Q
S
P
a
M
9
R
i
Q
U
1
Y
T
Y
/
e
I
b
O
n
D
J
E
s
d
K
u
p
E
V
z
9
f
d
E
h
o
U
x
U
x
G
5
T
o
H
t
2
C
x
7
u
f
i
f
1
0
t
t
f
B
1
m
T
C
a
p
p
Z
I
s
F
s
U
p
R
1
a
h
/
H
s
0
Z
J
o
S
y
6
e
O
Y
K
K
Z
u
x
W
R
M
d
a
Y
W
J
d
R
H
k
K
w
/
P
I
q
a
V
/
U
A
7
8
e
3
F
/
W
G
j
d
F
H
G
U
4
g
V
M
4
h
w
C
u
o
A
F
3
0
I
Q
W
E
B
D
w
D
K
/
w
5
m
n
v
x
X
v
3
P
h
a
t
J
a
+
Y
O
Y
Y
/
8
D
5
/
A
I
R
B
k
D
Q
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
i
q
D
j
c
D
4
i
Q
U
5
3
B
c
g
d
k
h
W
U
U
D
S
x
F
I
E
=
"
>
A
A
A
B
6
3
i
c
b
V
B
N
S
w
M
x
E
J
2
t
X
7
V
+
V
T
1
6
C
R
a
h
X
s
q
u
F
P
R
Y
9
K
D
H
C
v
Y
D
2
q
V
k
0
2
w
b
m
m
S
X
J
C
v
U
p
X
/
B
i
w
d
F
v
P
q
H
v
P
l
v
z
L
Z
7
0
N
Y
H
A
4
/
3
Z
p
i
Z
F
8
S
c
a
e
O
6
3
0
5
h
b
X
1
j
c
6
u
4
X
d
r
Z
3
d
s
/
K
B
8
e
t
X
W
U
K
E
J
b
J
O
K
R
6
g
Z
Y
U
8
4
k
b
R
l
m
O
O
3
G
i
m
I
R
c
N
o
J
J
j
e
Z
3
3
m
k
S
r
N
I
P
p
h
p
T
H
2
B
R
5
K
F
j
G
C
T
S
b
f
V
p
/
N
B
u
e
L
W
3
D
n
Q
K
v
F
y
U
o
E
c
z
U
H
5
q
z
+
M
S
C
K
o
N
I
R
j
r
X
u
e
G
x
s
/
x
c
o
w
w
u
m
s
1
E
8
0
j
T
G
Z
4
B
H
t
W
S
q
x
o
N
p
P
5
7
f
O
0
J
l
V
h
i
i
M
l
C
1
p
0
F
z
9
P
Z
F
i
o
f
V
U
B
L
Z
T
Y
D
P
W
y
1
4
m
/
u
f
1
E
h
N
e
+
S
m
T
c
W
K
o
J
I
t
F
Y
c
K
R
i
V
D
2
O
B
o
y
R
Y
n
h
U
0
s
w
U
c
z
e
i
s
g
Y
K
0
y
M
j
a
d
k
Q
/
C
W
X
1
4
l
7
Y
u
a
5
9
a
8
+
3
q
l
c
Z
3
H
U
Y
Q
T
O
I
U
q
e
H
A
J
D
b
i
D
J
r
S
A
w
B
i
e
4
R
X
e
H
O
G
8
O
O
/
O
x
6
K
1
4
O
Q
z
x
/
A
H
z
u
c
P
Q
B
C
N
t
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
i
q
D
j
c
D
4
i
Q
U
5
3
B
c
g
d
k
h
W
U
U
D
S
x
F
I
E
=
"
>
A
A
A
B
6
3
i
c
b
V
B
N
S
w
M
x
E
J
2
t
X
7
V
+
V
T
1
6
C
R
a
h
X
s
q
u
F
P
R
Y
9
K
D
H
C
v
Y
D
2
q
V
k
0
2
w
b
m
m
S
X
J
C
v
U
p
X
/
B
i
w
d
F
v
P
q
H
v
P
l
v
z
L
Z
7
0
N
Y
H
A
4
/
3
Z
p
i
Z
F
8
S
c
a
e
O
6
3
0
5
h
b
X
1
j
c
6
u
4
X
d
r
Z
3
d
s
/
K
B
8
e
t
X
W
U
K
E
J
b
J
O
K
R
6
g
Z
Y
U
8
4
k
b
R
l
m
O
O
3
G
i
m
I
R
c
N
o
J
J
j
e
Z
3
3
m
k
S
r
N
I
P
p
h
p
T
H
2
B
R
5
K
F
j
G
C
T
S
b
f
V
p
/
N
B
u
e
L
W
3
D
n
Q
K
v
F
y
U
o
E
c
z
U
H
5
q
z
+
M
S
C
K
o
N
I
R
j
r
X
u
e
G
x
s
/
x
c
o
w
w
u
m
s
1
E
8
0
j
T
G
Z
4
B
H
t
W
S
q
x
o
N
p
P
5
7
f
O
0
J
l
V
h
i
i
M
l
C
1
p
0
F
z
9
P
Z
F
i
o
f
V
U
B
L
Z
T
Y
D
P
W
y
1
4
m
/
u
f
1
E
h
N
e
+
S
m
T
c
W
K
o
J
I
t
F
Y
c
K
R
i
V
D
2
O
B
o
y
R
Y
n
h
U
0
s
w
U
c
z
e
i
s
g
Y
K
0
y
M
j
a
d
k
Q
/
C
W
X
1
4
l
7
Y
u
a
5
9
a
8
+
3
q
l
c
Z
3
H
U
Y
Q
T
O
I
U
q
e
H
A
J
D
b
i
D
J
r
S
A
w
B
i
e
4
R
X
e
H
O
G
8
O
O
/
O
x
6
K
1
4
O
Q
z
x
/
A
H
z
u
c
P
Q
B
C
N
t
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
i
q
D
j
c
D
4
i
Q
U
5
3
B
c
g
d
k
h
W
U
U
D
S
x
F
I
E
=
"
>
A
A
A
B
6
3
i
c
b
V
B
N
S
w
M
x
E
J
2
t
X
7
V
+
V
T
1
6
C
R
a
h
X
s
q
u
F
P
R
Y
9
K
D
H
C
v
Y
D
2
q
V
k
0
2
w
b
m
m
S
X
J
C
v
U
p
X
/
B
i
w
d
F
v
P
q
H
v
P
l
v
z
L
Z
7
0
N
Y
H
A
4
/
3
Z
p
i
Z
F
8
S
c
a
e
O
6
3
0
5
h
b
X
1
j
c
6
u
4
X
d
r
Z
3
d
s
/
K
B
8
e
t
X
W
U
K
E
J
b
J
O
K
R
6
g
Z
Y
U
8
4
k
b
R
l
m
O
O
3
G
i
m
I
R
c
N
o
J
J
j
e
Z
3
3
m
k
S
r
N
I
P
p
h
p
T
H
2
B
R
5
K
F
j
G
C
T
S
b
f
V
p
/
N
B
u
e
L
W
3
D
n
Q
K
v
F
y
U
o
E
c
z
U
H
5
q
z
+
M
S
C
K
o
N
I
R
j
r
X
u
e
G
x
s
/
x
c
o
w
w
u
m
s
1
E
8
0
j
T
G
Z
4
B
H
t
W
S
q
x
o
N
p
P
5
7
f
O
0
J
l
V
h
i
i
M
l
C
1
p
0
F
z
9
P
Z
F
i
o
f
V
U
B
L
Z
T
Y
D
P
W
y
1
4
m
/
u
f
1
E
h
N
e
+
S
m
T
c
W
K
o
J
I
t
F
Y
c
K
R
i
V
D
2
O
B
o
y
R
Y
n
h
U
0
s
w
U
c
z
e
i
s
g
Y
K
0
y
M
j
a
d
k
Q
/
C
W
X
1
4
l
7
Y
u
a
5
9
a
8
+
3
q
l
c
Z
3
H
U
Y
Q
T
O
I
U
q
e
H
A
J
D
b
i
D
J
r
S
A
w
B
i
e
4
R
X
e
H
O
G
8
O
O
/
O
x
6
K
1
4
O
Q
z
x
/
A
H
z
u
c
P
Q
B
C
N
t
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
i
q
D
j
c
D
4
i
Q
U
5
3
B
c
g
d
k
h
W
U
U
D
S
x
F
I
E
=
"
>
A
A
A
B
6
3
i
c
b
V
B
N
S
w
M
x
E
J
2
t
X
7
V
+
V
T
1
6
C
R
a
h
X
s
q
u
F
P
R
Y
9
K
D
H
C
v
Y
D
2
q
V
k
0
2
w
b
m
m
S
X
J
C
v
U
p
X
/
B
i
w
d
F
v
P
q
H
v
P
l
v
z
L
Z
7
0
N
Y
H
A
4
/
3
Z
p
i
Z
F
8
S
c
a
e
O
6
3
0
5
h
b
X
1
j
c
6
u
4
X
d
r
Z
3
d
s
/
K
B
8
e
t
X
W
U
K
E
J
b
J
O
K
R
6
g
Z
Y
U
8
4
k
b
R
l
m
O
O
3
G
i
m
I
R
c
N
o
J
J
j
e
Z
3
3
m
k
S
r
N
I
P
p
h
p
T
H
2
B
R
5
K
F
j
G
C
T
S
b
f
V
p
/
N
B
u
e
L
W
3
D
n
Q
K
v
F
y
U
o
E
c
z
U
H
5
q
z
+
M
S
C
K
o
N
I
R
j
r
X
u
e
G
x
s
/
x
c
o
w
w
u
m
s
1
E
8
0
j
T
G
Z
4
B
H
t
W
S
q
x
o
N
p
P
5
7
f
O
0
J
l
V
h
i
i
M
l
C
1
p
0
F
z
9
P
Z
F
i
o
f
V
U
B
L
Z
T
Y
D
P
W
y
1
4
m
/
u
f
1
E
h
N
e
+
S
m
T
c
W
K
o
J
I
t
F
Y
c
K
R
i
V
D
2
O
B
o
y
R
Y
n
h
U
0
s
w
U
c
z
e
i
s
g
Y
K
0
y
M
j
a
d
k
Q
/
C
W
X
1
4
l
7
Y
u
a
5
9
a
8
+
3
q
l
c
Z
3
H
U
Y
Q
T
O
I
U
q
e
H
A
J
D
b
i
D
J
r
S
A
w
B
i
e
4
R
X
e
H
O
G
8
O
O
/
O
x
6
K
1
4
O
Q
z
x
/
A
H
z
u
c
P
Q
B
C
N
t
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
G
V
M
N
I
X
D
F
9
l
c
F
4
K
N
r
L
D
w
k
c
p
l
a
v
U
I
=
"
>
A
A
A
B
9
X
i
c
b
V
B
N
S
w
M
x
E
M
3
W
r
1
q
/
q
h
6
9
B
I
t
Q
E
c
q
u
C
H
o
s
e
t
B
j
B
f
s
B
7
V
q
y
6
b
Q
N
z
S
Z
L
k
l
X
q
0
v
/
h
x
Y
M
i
X
v
0
v
3
v
w
3
Z
t
s
9
a
O
u
D
g
c
d
7
M
8
z
M
C
y
L
O
t
H
H
d
b
y
e
3
t
L
y
y
u
p
Z
f
L
2
x
s
b
m
3
v
F
H
f
3
G
l
r
G
i
k
K
d
S
i
5
V
K
y
A
a
O
B
N
Q
N
8
x
w
a
E
U
K
S
B
h
w
a
A
a
j
q
9
R
v
P
o
D
S
T
I
o
7
M
4
7
A
D
8
l
A
s
D
6
j
x
F
j
p
/
r
r
8
d
H
z
S
g
U
g
z
L
k
W
h
W
y
y
5
F
X
c
K
v
E
i
8
j
J
R
Q
h
l
q
3
+
N
X
p
S
R
q
H
I
A
z
l
R
O
u
2
5
0
b
G
T
4
g
y
j
H
K
Y
F
D
q
x
h
o
j
Q
E
R
l
A
2
1
J
B
Q
t
B
+
M
r
1
6
g
o
+
s
0
s
N
9
q
W
w
J
g
6
f
q
7
4
m
E
h
F
q
P
w
8
B
2
h
s
Q
M
9
b
y
X
i
v
9
5
7
d
j
0
L
/
y
E
i
S
g
2
I
O
h
s
U
T
/
m
2
E
i
c
R
o
B
7
T
A
E
1
f
G
w
J
o
Y
r
Z
W
z
E
d
E
k
W
o
s
U
G
l
I
X
j
z
L
y
+
S
x
m
n
F
c
y
v
e
7
V
m
p
e
p
n
F
k
U
c
H
6
B
C
V
k
Y
f
O
U
R
X
d
o
B
q
q
I
4
o
U
e
k
a
v
6
M
1
5
d
F
6
c
d
+
d
j
1
p
p
z
s
p
l
9
9
A
f
O
5
w
8
r
k
J
G
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
G
V
M
N
I
X
D
F
9
l
c
F
4
K
N
r
L
D
w
k
c
p
l
a
v
U
I
=
"
>
A
A
A
B
9
X
i
c
b
V
B
N
S
w
M
x
E
M
3
W
r
1
q
/
q
h
6
9
B
I
t
Q
E
c
q
u
C
H
o
s
e
t
B
j
B
f
s
B
7
V
q
y
6
b
Q
N
z
S
Z
L
k
l
X
q
0
v
/
h
x
Y
M
i
X
v
0
v
3
v
w
3
Z
t
s
9
a
O
u
D
g
c
d
7
M
8
z
M
C
y
L
O
t
H
H
d
b
y
e
3
t
L
y
y
u
p
Z
f
L
2
x
s
b
m
3
v
F
H
f
3
G
l
r
G
i
k
K
d
S
i
5
V
K
y
A
a
O
B
N
Q
N
8
x
w
a
E
U
K
S
B
h
w
a
A
a
j
q
9
R
v
P
o
D
S
T
I
o
7
M
4
7
A
D
8
l
A
s
D
6
j
x
F
j
p
/
r
r
8
d
H
z
S
g
U
g
z
L
k
W
h
W
y
y
5
F
X
c
K
v
E
i
8
j
J
R
Q
h
l
q
3
+
N
X
p
S
R
q
H
I
A
z
l
R
O
u
2
5
0
b
G
T
4
g
y
j
H
K
Y
F
D
q
x
h
o
j
Q
E
R
l
A
2
1
J
B
Q
t
B
+
M
r
1
6
g
o
+
s
0
s
N
9
q
W
w
J
g
6
f
q
7
4
m
E
h
F
q
P
w
8
B
2
h
s
Q
M
9
b
y
X
i
v
9
5
7
d
j
0
L
/
y
E
i
S
g
2
I
O
h
s
U
T
/
m
2
E
i
c
R
o
B
7
T
A
E
1
f
G
w
J
o
Y
r
Z
W
z
E
d
E
k
W
o
s
U
G
l
I
X
j
z
L
y
+
S
x
m
n
F
c
y
v
e
7
V
m
p
e
p
n
F
k
U
c
H
6
B
C
V
k
Y
f
O
U
R
X
d
o
B
q
q
I
4
o
U
e
k
a
v
6
M
1
5
d
F
6
c
d
+
d
j
1
p
p
z
s
p
l
9
9
A
f
O
5
w
8
r
k
J
G
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
G
V
M
N
I
X
D
F
9
l
c
F
4
K
N
r
L
D
w
k
c
p
l
a
v
U
I
=
"
>
A
A
A
B
9
X
i
c
b
V
B
N
S
w
M
x
E
M
3
W
r
1
q
/
q
h
6
9
B
I
t
Q
E
c
q
u
C
H
o
s
e
t
B
j
B
f
s
B
7
V
q
y
6
b
Q
N
z
S
Z
L
k
l
X
q
0
v
/
h
x
Y
M
i
X
v
0
v
3
v
w
3
Z
t
s
9
a
O
u
D
g
c
d
7
M
8
z
M
C
y
L
O
t
H
H
d
b
y
e
3
t
L
y
y
u
p
Z
f
L
2
x
s
b
m
3
v
F
H
f
3
G
l
r
G
i
k
K
d
S
i
5
V
K
y
A
a
O
B
N
Q
N
8
x
w
a
E
U
K
S
B
h
w
a
A
a
j
q
9
R
v
P
o
D
S
T
I
o
7
M
4
7
A
D
8
l
A
s
D
6
j
x
F
j
p
/
r
r
8
d
H
z
S
g
U
g
z
L
k
W
h
W
y
y
5
F
X
c
K
v
E
i
8
j
J
R
Q
h
l
q
3
+
N
X
p
S
R
q
H
I
A
z
l
R
O
u
2
5
0
b
G
T
4
g
y
j
H
K
Y
F
D
q
x
h
o
j
Q
E
R
l
A
2
1
J
B
Q
t
B
+
M
r
1
6
g
o
+
s
0
s
N
9
q
W
w
J
g
6
f
q
7
4
m
E
h
F
q
P
w
8
B
2
h
s
Q
M
9
b
y
X
i
v
9
5
7
d
j
0
L
/
y
E
i
S
g
2
I
O
h
s
U
T
/
m
2
E
i
c
R
o
B
7
T
A
E
1
f
G
w
J
o
Y
r
Z
W
z
E
d
E
k
W
o
s
U
G
l
I
X
j
z
L
y
+
S
x
m
n
F
c
y
v
e
7
V
m
p
e
p
n
F
k
U
c
H
6
B
C
V
k
Y
f
O
U
R
X
d
o
B
q
q
I
4
o
U
e
k
a
v
6
M
1
5
d
F
6
c
d
+
d
j
1
p
p
z
s
p
l
9
9
A
f
O
5
w
8
r
k
J
G
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
G
V
M
N
I
X
D
F
9
l
c
F
4
K
N
r
L
D
w
k
c
p
l
a
v
U
I
=
"
>
A
A
A
B
9
X
i
c
b
V
B
N
S
w
M
x
E
M
3
W
r
1
q
/
q
h
6
9
B
I
t
Q
E
c
q
u
C
H
o
s
e
t
B
j
B
f
s
B
7
V
q
y
6
b
Q
N
z
S
Z
L
k
l
X
q
0
v
/
h
x
Y
M
i
X
v
0
v
3
v
w
3
Z
t
s
9
a
O
u
D
g
c
d
7
M
8
z
M
C
y
L
O
t
H
H
d
b
y
e
3
t
L
y
y
u
p
Z
f
L
2
x
s
b
m
3
v
F
H
f
3
G
l
r
G
i
k
K
d
S
i
5
V
K
y
A
a
O
B
N
Q
N
8
x
w
a
E
U
K
S
B
h
w
a
A
a
j
q
9
R
v
P
o
D
S
T
I
o
7
M
4
7
A
D
8
l
A
s
D
6
j
x
F
j
p
/
r
r
8
d
H
z
S
g
U
g
z
L
k
W
h
W
y
y
5
F
X
c
K
v
E
i
8
j
J
R
Q
h
l
q
3
+
N
X
p
S
R
q
H
I
A
z
l
R
O
u
2
5
0
b
G
T
4
g
y
j
H
K
Y
F
D
q
x
h
o
j
Q
E
R
l
A
2
1
J
B
Q
t
B
+
M
r
1
6
g
o
+
s
0
s
N
9
q
W
w
J
g
6
f
q
7
4
m
E
h
F
q
P
w
8
B
2
h
s
Q
M
9
b
y
X
i
v
9
5
7
d
j
0
L
/
y
E
i
S
g
2
I
O
h
s
U
T
/
m
2
E
i
c
R
o
B
7
T
A
E
1
f
G
w
J
o
Y
r
Z
W
z
E
d
E
k
W
o
s
U
G
l
I
X
j
z
L
y
+
S
x
m
n
F
c
y
v
e
7
V
m
p
e
p
n
F
k
U
c
H
6
B
C
V
k
Y
f
O
U
R
X
d
o
B
q
q
I
4
o
U
e
k
a
v
6
M
1
5
d
F
6
c
d
+
d
j
1
p
p
z
s
p
l
9
9
A
f
O
5
w
8
r
k
J
G
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
7
m
i
i
r
s
u
w
j
r
0
d
t
Q
T
p
a
M
k
v
C
w
Q
Z
d
E
g
=
"
>
A
A
A
B
8
n
i
c
b
V
B
N
S
8
N
A
E
N
3
U
r
1
q
/
q
h
6
9
B
I
s
g
C
C
U
R
Q
Y
9
F
L
x
4
r
2
F
p
o
Q
9
l
s
J
+
3
S
z
W
7
Y
n
Y
g
l
9
G
d
4
8
a
C
I
V
3
+
N
N
/
+
N
m
z
Y
H
b
X
0
w
8
H
h
v
h
p
l
5
Y
S
K
4
Q
c
/
7
d
k
o
r
q
2
v
r
G
+
X
N
y
t
b
2
z
u
5
e
d
f
+
g
b
V
S
q
G
b
S
Y
E
k
p
3
Q
m
p
A
c
A
k
t
5
C
i
g
k
2
i
g
c
S
j
g
I
R
z
f
5
P
7
D
I
2
j
D
l
b
z
H
S
Q
J
B
T
I
e
S
R
5
x
R
t
F
L
3
6
a
w
H
i
e
F
C
y
U
q
/
W
v
P
q
3
g
z
u
M
v
E
L
U
i
M
F
m
v
3
q
V
2
+
g
W
B
q
D
R
C
a
o
M
V
3
f
S
z
D
I
q
E
b
O
B
E
w
r
v
d
R
A
Q
t
m
Y
D
q
F
r
q
a
Q
x
m
C
C
b
n
T
x
1
T
6
w
y
c
C
O
l
b
U
l
0
Z
+
r
v
i
Y
z
G
x
k
z
i
0
H
b
G
F
E
d
m
0
c
v
F
/
7
x
u
i
t
F
V
k
H
G
Z
p
A
i
S
z
R
d
F
q
X
B
R
u
f
n
/
7
o
B
r
Y
C
g
m
l
l
C
m
u
b
3
V
Z
S
O
q
K
U
O
b
U
h
6
C
v
/
j
y
M
m
m
f
1
3
2
v
7
t
9
d
1
B
r
X
R
R
x
l
c
k
S
O
y
S
n
x
y
S
V
p
k
F
v
S
J
C
3
C
i
C
L
P
5
J
W
8
O
e
i
8
O
O
/
O
x
7
y
1
5
B
Q
z
h
+
Q
P
n
M
8
f
y
3
m
Q
6
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
7
m
i
i
r
s
u
w
j
r
0
d
t
Q
T
p
a
M
k
v
C
w
Q
Z
d
E
g
=
"
>
A
A
A
B
8
n
i
c
b
V
B
N
S
8
N
A
E
N
3
U
r
1
q
/
q
h
6
9
B
I
s
g
C
C
U
R
Q
Y
9
F
L
x
4
r
2
F
p
o
Q
9
l
s
J
+
3
S
z
W
7
Y
n
Y
g
l
9
G
d
4
8
a
C
I
V
3
+
N
N
/
+
N
m
z
Y
H
b
X
0
w
8
H
h
v
h
p
l
5
Y
S
K
4
Q
c
/
7
d
k
o
r
q
2
v
r
G
+
X
N
y
t
b
2
z
u
5
e
d
f
+
g
b
V
S
q
G
b
S
Y
E
k
p
3
Q
m
p
A
c
A
k
t
5
C
i
g
k
2
i
g
c
S
j
g
I
R
z
f
5
P
7
D
I
2
j
D
l
b
z
H
S
Q
J
B
T
I
e
S
R
5
x
R
t
F
L
3
6
a
w
H
i
e
F
C
y
U
q
/
W
v
P
q
3
g
z
u
M
v
E
L
U
i
M
F
m
v
3
q
V
2
+
g
W
B
q
D
R
C
a
o
M
V
3
f
S
z
D
I
q
E
b
O
B
E
w
r
v
d
R
A
Q
t
m
Y
D
q
F
r
q
a
Q
x
m
C
C
b
n
T
x
1
T
6
w
y
c
C
O
l
b
U
l
0
Z
+
r
v
i
Y
z
G
x
k
z
i
0
H
b
G
F
E
d
m
0
c
v
F
/
7
x
u
i
t
F
V
k
H
G
Z
p
A
i
S
z
R
d
F
q
X
B
R
u
f
n
/
7
o
B
r
Y
C
g
m
l
l
C
m
u
b
3
V
Z
S
O
q
K
U
O
b
U
h
6
C
v
/
j
y
M
m
m
f
1
3
2
v
7
t
9
d
1
B
r
X
R
R
x
l
c
k
S
O
y
S
n
x
y
S
V
p
k
F
v
S
J
C
3
C
i
C
L
P
5
J
W
8
O
e
i
8
O
O
/
O
x
7
y
1
5
B
Q
z
h
+
Q
P
n
M
8
f
y
3
m
Q
6
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
7
m
i
i
r
s
u
w
j
r
0
d
t
Q
T
p
a
M
k
v
C
w
Q
Z
d
E
g
=
"
>
A
A
A
B
8
n
i
c
b
V
B
N
S
8
N
A
E
N
3
U
r
1
q
/
q
h
6
9
B
I
s
g
C
C
U
R
Q
Y
9
F
L
x
4
r
2
F
p
o
Q
9
l
s
J
+
3
S
z
W
7
Y
n
Y
g
l
9
G
d
4
8
a
C
I
V
3
+
N
N
/
+
N
m
z
Y
H
b
X
0
w
8
H
h
v
h
p
l
5
Y
S
K
4
Q
c
/
7
d
k
o
r
q
2
v
r
G
+
X
N
y
t
b
2
z
u
5
e
d
f
+
g
b
V
S
q
G
b
S
Y
E
k
p
3
Q
m
p
A
c
A
k
t
5
C
i
g
k
2
i
g
c
S
j
g
I
R
z
f
5
P
7
D
I
2
j
D
l
b
z
H
S
Q
J
B
T
I
e
S
R
5
x
R
t
F
L
3
6
a
w
H
i
e
F
C
y
U
q
/
W
v
P
q
3
g
z
u
M
v
E
L
U
i
M
F
m
v
3
q
V
2
+
g
W
B
q
D
R
C
a
o
M
V
3
f
S
z
D
I
q
E
b
O
B
E
w
r
v
d
R
A
Q
t
m
Y
D
q
F
r
q
a
Q
x
m
C
C
b
n
T
x
1
T
6
w
y
c
C
O
l
b
U
l
0
Z
+
r
v
i
Y
z
G
x
k
z
i
0
H
b
G
F
E
d
m
0
c
v
F
/
7
x
u
i
t
F
V
k
H
G
Z
p
A
i
S
z
R
d
F
q
X
B
R
u
f
n
/
7
o
B
r
Y
C
g
m
l
l
C
m
u
b
3
V
Z
S
O
q
K
U
O
b
U
h
6
C
v
/
j
y
M
m
m
f
1
3
2
v
7
t
9
d
1
B
r
X
R
R
x
l
c
k
S
O
y
S
n
x
y
S
V
p
k
F
v
S
J
C
3
C
i
C
L
P
5
J
W
8
O
e
i
8
O
O
/
O
x
7
y
1
5
B
Q
z
h
+
Q
P
n
M
8
f
y
3
m
Q
6
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
7
m
i
i
r
s
u
w
j
r
0
d
t
Q
T
p
a
M
k
v
C
w
Q
Z
d
E
g
=
"
>
A
A
A
B
8
n
i
c
b
V
B
N
S
8
N
A
E
N
3
U
r
1
q
/
q
h
6
9
B
I
s
g
C
C
U
R
Q
Y
9
F
L
x
4
r
2
F
p
o
Q
9
l
s
J
+
3
S
z
W
7
Y
n
Y
g
l
9
G
d
4
8
a
C
I
V
3
+
N
N
/
+
N
m
z
Y
H
b
X
0
w
8
H
h
v
h
p
l
5
Y
S
K
4
Q
c
/
7
d
k
o
r
q
2
v
r
G
+
X
N
y
t
b
2
z
u
5
e
d
f
+
g
b
V
S
q
G
b
S
Y
E
k
p
3
Q
m
p
A
c
A
k
t
5
C
i
g
k
2
i
g
c
S
j
g
I
R
z
f
5
P
7
D
I
2
j
D
l
b
z
H
S
Q
J
B
T
I
e
S
R
5
x
R
t
F
L
3
6
a
w
H
i
e
F
C
y
U
q
/
W
v
P
q
3
g
z
u
M
v
E
L
U
i
M
F
m
v
3
q
V
2
+
g
W
B
q
D
R
C
a
o
M
V
3
f
S
z
D
I
q
E
b
O
B
E
w
r
v
d
R
A
Q
t
m
Y
D
q
F
r
q
a
Q
x
m
C
C
b
n
T
x
1
T
6
w
y
c
C
O
l
b
U
l
0
Z
+
r
v
i
Y
z
G
x
k
z
i
0
H
b
G
F
E
d
m
0
c
v
F
/
7
x
u
i
t
F
V
k
H
G
Z
p
A
i
S
z
R
d
F
q
X
B
R
u
f
n
/
7
o
B
r
Y
C
g
m
l
l
C
m
u
b
3
V
Z
S
O
q
K
U
O
b
U
h
6
C
v
/
j
y
M
m
m
f
1
3
2
v
7
t
9
d
1
B
r
X
R
R
x
l
c
k
S
O
y
S
n
x
y
S
V
p
k
F
v
S
J
C
3
C
i
C
L
P
5
J
W
8
O
e
i
8
O
O
/
O
x
7
y
1
5
B
Q
z
h
+
Q
P
n
M
8
f
y
3
m
Q
6
w
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
a
U
2
p
y
s
M
3
B
H
f
N
5
U
L
2
5
f
i
u
M
E
T
I
3
U
w
=
"
>
A
A
A
B
6
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
R
i
8
c
q
t
h
b
a
U
D
b
b
T
b
t
0
s
w
m
7
E
7
G
E
/
g
M
v
H
h
T
x
6
j
/
y
5
r
9
x
0
+
a
g
r
Q
8
G
H
u
/
N
M
D
M
v
S
K
Q
w
6
L
r
f
T
m
l
l
d
W
1
9
o
7
x
Z
2
d
r
e
2
d
2
r
7
h
+
0
T
Z
x
q
x
l
s
s
l
r
H
u
B
N
R
w
K
R
R
v
o
U
D
J
O
4
n
m
N
A
o
k
f
w
j
G
1
7
n
/
8
M
i
1
E
b
G
6
x
0
n
C
/
Y
g
O
l
Q
g
F
o
2
i
l
u
6
d
K
v
1
p
z
6
+
4
M
Z
J
l
4
B
a
l
B
g
W
a
/
+
t
U
b
x
C
y
N
u
E
I
m
q
T
F
d
z
0
3
Q
z
6
h
G
w
S
S
f
V
n
q
p
4
Q
l
l
Y
z
r
k
X
U
s
V
j
b
j
x
s
9
m
l
U
3
J
i
l
Q
E
J
Y
2
1
L
I
Z
m
p
v
y
c
y
G
h
k
z
i
Q
L
b
G
V
E
c
m
U
U
v
F
/
/
z
u
i
m
G
l
3
4
m
V
J
I
i
V
2
y
+
K
E
w
l
w
Z
j
k
b
5
O
B
0
J
y
h
n
F
h
C
m
R
b
2
V
s
J
G
V
F
O
G
N
p
w
8
B
G
/
x
5
W
X
S
P
q
t
7
b
t
2
7
P
a
8
1
r
o
o
4
y
n
A
E
x
3
A
K
H
l
x
A
A
2
6
g
C
S
1
g
E
M
I
z
v
M
K
b
M
3
Z
e
n
H
f
n
Y
9
5
a
c
o
q
Z
Q
/
g
D
5
/
M
H
G
s
y
N
E
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
a
U
2
p
y
s
M
3
B
H
f
N
5
U
L
2
5
f
i
u
M
E
T
I
3
U
w
=
"
>
A
A
A
B
6
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
R
i
8
c
q
t
h
b
a
U
D
b
b
T
b
t
0
s
w
m
7
E
7
G
E
/
g
M
v
H
h
T
x
6
j
/
y
5
r
9
x
0
+
a
g
r
Q
8
G
H
u
/
N
M
D
M
v
S
K
Q
w
6
L
r
f
T
m
l
l
d
W
1
9
o
7
x
Z
2
d
r
e
2
d
2
r
7
h
+
0
T
Z
x
q
x
l
s
s
l
r
H
u
B
N
R
w
K
R
R
v
o
U
D
J
O
4
n
m
N
A
o
k
f
w
j
G
1
7
n
/
8
M
i
1
E
b
G
6
x
0
n
C
/
Y
g
O
l
Q
g
F
o
2
i
l
u
6
d
K
v
1
p
z
6
+
4
M
Z
J
l
4
B
a
l
B
g
W
a
/
+
t
U
b
x
C
y
N
u
E
I
m
q
T
F
d
z
0
3
Q
z
6
h
G
w
S
S
f
V
n
q
p
4
Q
l
l
Y
z
r
k
X
U
s
V
j
b
j
x
s
9
m
l
U
3
J
i
l
Q
E
J
Y
2
1
L
I
Z
m
p
v
y
c
y
G
h
k
z
i
Q
L
b
G
V
E
c
m
U
U
v
F
/
/
z
u
i
m
G
l
3
4
m
V
J
I
i
V
2
y
+
K
E
w
l
w
Z
j
k
b
5
O
B
0
J
y
h
n
F
h
C
m
R
b
2
V
s
J
G
V
F
O
G
N
p
w
8
B
G
/
x
5
W
X
S
P
q
t
7
b
t
2
7
P
a
8
1
r
o
o
4
y
n
A
E
x
3
A
K
H
l
x
A
A
2
6
g
C
S
1
g
E
M
I
z
v
M
K
b
M
3
Z
e
n
H
f
n
Y
9
5
a
c
o
q
Z
Q
/
g
D
5
/
M
H
G
s
y
N
E
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
a
U
2
p
y
s
M
3
B
H
f
N
5
U
L
2
5
f
i
u
M
E
T
I
3
U
w
=
"
>
A
A
A
B
6
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
R
i
8
c
q
t
h
b
a
U
D
b
b
T
b
t
0
s
w
m
7
E
7
G
E
/
g
M
v
H
h
T
x
6
j
/
y
5
r
9
x
0
+
a
g
r
Q
8
G
H
u
/
N
M
D
M
v
S
K
Q
w
6
L
r
f
T
m
l
l
d
W
1
9
o
7
x
Z
2
d
r
e
2
d
2
r
7
h
+
0
T
Z
x
q
x
l
s
s
l
r
H
u
B
N
R
w
K
R
R
v
o
U
D
J
O
4
n
m
N
A
o
k
f
w
j
G
1
7
n
/
8
M
i
1
E
b
G
6
x
0
n
C
/
Y
g
O
l
Q
g
F
o
2
i
l
u
6
d
K
v
1
p
z
6
+
4
M
Z
J
l
4
B
a
l
B
g
W
a
/
+
t
U
b
x
C
y
N
u
E
I
m
q
T
F
d
z
0
3
Q
z
6
h
G
w
S
S
f
V
n
q
p
4
Q
l
l
Y
z
r
k
X
U
s
V
j
b
j
x
s
9
m
l
U
3
J
i
l
Q
E
J
Y
2
1
L
I
Z
m
p
v
y
c
y
G
h
k
z
i
Q
L
b
G
V
E
c
m
U
U
v
F
/
/
z
u
i
m
G
l
3
4
m
V
J
I
i
V
2
y
+
K
E
w
l
w
Z
j
k
b
5
O
B
0
J
y
h
n
F
h
C
m
R
b
2
V
s
J
G
V
F
O
G
N
p
w
8
B
G
/
x
5
W
X
S
P
q
t
7
b
t
2
7
P
a
8
1
r
o
o
4
y
n
A
E
x
3
A
K
H
l
x
A
A
2
6
g
C
S
1
g
E
M
I
z
v
M
K
b
M
3
Z
e
n
H
f
n
Y
9
5
a
c
o
q
Z
Q
/
g
D
5
/
M
H
G
s
y
N
E
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
a
U
2
p
y
s
M
3
B
H
f
N
5
U
L
2
5
f
i
u
M
E
T
I
3
U
w
=
"
>
A
A
A
B
6
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
R
i
8
c
q
t
h
b
a
U
D
b
b
T
b
t
0
s
w
m
7
E
7
G
E
/
g
M
v
H
h
T
x
6
j
/
y
5
r
9
x
0
+
a
g
r
Q
8
G
H
u
/
N
M
D
M
v
S
K
Q
w
6
L
r
f
T
m
l
l
d
W
1
9
o
7
x
Z
2
d
r
e
2
d
2
r
7
h
+
0
T
Z
x
q
x
l
s
s
l
r
H
u
B
N
R
w
K
R
R
v
o
U
D
J
O
4
n
m
N
A
o
k
f
w
j
G
1
7
n
/
8
M
i
1
E
b
G
6
x
0
n
C
/
Y
g
O
l
Q
g
F
o
2
i
l
u
6
d
K
v
1
p
z
6
+
4
M
Z
J
l
4
B
a
l
B
g
W
a
/
+
t
U
b
x
C
y
N
u
E
I
m
q
T
F
d
z
0
3
Q
z
6
h
G
w
S
S
f
V
n
q
p
4
Q
l
l
Y
z
r
k
X
U
s
V
j
b
j
x
s
9
m
l
U
3
J
i
l
Q
E
J
Y
2
1
L
I
Z
m
p
v
y
c
y
G
h
k
z
i
Q
L
b
G
V
E
c
m
U
U
v
F
/
/
z
u
i
m
G
l
3
4
m
V
J
I
i
V
2
y
+
K
E
w
l
w
Z
j
k
b
5
O
B
0
J
y
h
n
F
h
C
m
R
b
2
V
s
J
G
V
F
O
G
N
p
w
8
B
G
/
x
5
W
X
S
P
q
t
7
b
t
2
7
P
a
8
1
r
o
o
4
y
n
A
E
x
3
A
K
H
l
x
A
A
2
6
g
C
S
1
g
E
M
I
z
v
M
K
b
M
3
Z
e
n
H
f
n
Y
9
5
a
c
o
q
Z
Q
/
g
D
5
/
M
H
G
s
y
N
E
A
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
D
z
X
c
h
l
s
P
l
m
u
E
y
Z
Z
/
9
z
F
J
+
i
V
C
6
I
=
"
>
A
A
A
B
6
H
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
R
i
8
c
W
7
A
e
0
o
W
y
2
k
3
b
t
Z
h
N
2
N
0
I
N
/
Q
V
e
P
C
j
i
1
Z
/
k
z
X
/
j
t
s
1
B
W
x
8
M
P
N
6
b
Y
W
Z
e
k
A
i
u
j
e
t
+
O
4
W
1
9
Y
3
N
r
e
J
2
a
W
d
3
b
/
+
g
f
H
j
U
0
n
G
q
G
D
Z
Z
L
G
L
V
C
a
h
G
w
S
U
2
D
T
c
C
O
4
l
C
G
g
U
C
2
8
H
4
d
u
a
3
H
1
F
p
H
s
t
7
M
0
n
Q
j
+
h
Q
8
p
A
z
a
q
z
U
e
O
q
X
K
2
7
V
n
Y
O
s
E
i
8
n
F
c
h
R
7
5
e
/
e
o
O
Y
p
R
F
K
w
w
T
V
u
u
u
5
i
f
E
z
q
g
x
n
A
q
e
l
X
q
o
x
o
W
x
M
h
9
i
1
V
N
I
I
t
Z
/
N
D
5
2
S
M
6
s
M
S
B
g
r
W
9
K
Q
u
f
p
7
I
q
O
R
1
p
M
o
s
J
0
R
N
S
O
9
7
M
3
E
/
7
x
u
a
s
J
r
P
+
M
y
S
Q
1
K
t
l
g
U
p
o
K
Y
m
M
y
+
J
g
O
u
k
B
k
x
s
Y
Q
y
x
e
2
t
h
I
2
o
o
s
z
Y
b
E
o
2
B
G
/
5
5
V
X
S
u
q
h
6
b
t
V
r
X
F
Z
q
N
3
k
c
R
T
i
B
U
z
g
H
D
6
6
g
B
n
d
Q
h
y
Y
w
Q
H
i
G
V
3
h
z
H
p
w
X
5
9
3
5
W
L
Q
W
n
H
z
m
G
P
7
A
+
f
w
B
6
U
G
M
/
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
D
z
X
c
h
l
s
P
l
m
u
E
y
Z
Z
/
9
z
F
J
+
i
V
C
6
I
=
"
>
A
A
A
B
6
H
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
R
i
8
c
W
7
A
e
0
o
W
y
2
k
3
b
t
Z
h
N
2
N
0
I
N
/
Q
V
e
P
C
j
i
1
Z
/
k
z
X
/
j
t
s
1
B
W
x
8
M
P
N
6
b
Y
W
Z
e
k
A
i
u
j
e
t
+
O
4
W
1
9
Y
3
N
r
e
J
2
a
W
d
3
b
/
+
g
f
H
j
U
0
n
G
q
G
D
Z
Z
L
G
L
V
C
a
h
G
w
S
U
2
D
T
c
C
O
4
l
C
G
g
U
C
2
8
H
4
d
u
a
3
H
1
F
p
H
s
t
7
M
0
n
Q
j
+
h
Q
8
p
A
z
a
q
z
U
e
O
q
X
K
2
7
V
n
Y
O
s
E
i
8
n
F
c
h
R
7
5
e
/
e
o
O
Y
p
R
F
K
w
w
T
V
u
u
u
5
i
f
E
z
q
g
x
n
A
q
e
l
X
q
o
x
o
W
x
M
h
9
i
1
V
N
I
I
t
Z
/
N
D
5
2
S
M
6
s
M
S
B
g
r
W
9
K
Q
u
f
p
7
I
q
O
R
1
p
M
o
s
J
0
R
N
S
O
9
7
M
3
E
/
7
x
u
a
s
J
r
P
+
M
y
S
Q
1
K
t
l
g
U
p
o
K
Y
m
M
y
+
J
g
O
u
k
B
k
x
s
Y
Q
y
x
e
2
t
h
I
2
o
o
s
z
Y
b
E
o
2
B
G
/
5
5
V
X
S
u
q
h
6
b
t
V
r
X
F
Z
q
N
3
k
c
R
T
i
B
U
z
g
H
D
6
6
g
B
n
d
Q
h
y
Y
w
Q
H
i
G
V
3
h
z
H
p
w
X
5
9
3
5
W
L
Q
W
n
H
z
m
G
P
7
A
+
f
w
B
6
U
G
M
/
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
D
z
X
c
h
l
s
P
l
m
u
E
y
Z
Z
/
9
z
F
J
+
i
V
C
6
I
=
"
>
A
A
A
B
6
H
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
R
i
8
c
W
7
A
e
0
o
W
y
2
k
3
b
t
Z
h
N
2
N
0
I
N
/
Q
V
e
P
C
j
i
1
Z
/
k
z
X
/
j
t
s
1
B
W
x
8
M
P
N
6
b
Y
W
Z
e
k
A
i
u
j
e
t
+
O
4
W
1
9
Y
3
N
r
e
J
2
a
W
d
3
b
/
+
g
f
H
j
U
0
n
G
q
G
D
Z
Z
L
G
L
V
C
a
h
G
w
S
U
2
D
T
c
C
O
4
l
C
G
g
U
C
2
8
H
4
d
u
a
3
H
1
F
p
H
s
t
7
M
0
n
Q
j
+
h
Q
8
p
A
z
a
q
z
U
e
O
q
X
K
2
7
V
n
Y
O
s
E
i
8
n
F
c
h
R
7
5
e
/
e
o
O
Y
p
R
F
K
w
w
T
V
u
u
u
5
i
f
E
z
q
g
x
n
A
q
e
l
X
q
o
x
o
W
x
M
h
9
i
1
V
N
I
I
t
Z
/
N
D
5
2
S
M
6
s
M
S
B
g
r
W
9
K
Q
u
f
p
7
I
q
O
R
1
p
M
o
s
J
0
R
N
S
O
9
7
M
3
E
/
7
x
u
a
s
J
r
P
+
M
y
S
Q
1
K
t
l
g
U
p
o
K
Y
m
M
y
+
J
g
O
u
k
B
k
x
s
Y
Q
y
x
e
2
t
h
I
2
o
o
s
z
Y
b
E
o
2
B
G
/
5
5
V
X
S
u
q
h
6
b
t
V
r
X
F
Z
q
N
3
k
c
R
T
i
B
U
z
g
H
D
6
6
g
B
n
d
Q
h
y
Y
w
Q
H
i
G
V
3
h
z
H
p
w
X
5
9
3
5
W
L
Q
W
n
H
z
m
G
P
7
A
+
f
w
B
6
U
G
M
/
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
H
D
z
X
c
h
l
s
P
l
m
u
E
y
Z
Z
/
9
z
F
J
+
i
V
C
6
I
=
"
>
A
A
A
B
6
H
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
R
i
8
c
W
7
A
e
0
o
W
y
2
k
3
b
t
Z
h
N
2
N
0
I
N
/
Q
V
e
P
C
j
i
1
Z
/
k
z
X
/
j
t
s
1
B
W
x
8
M
P
N
6
b
Y
W
Z
e
k
A
i
u
j
e
t
+
O
4
W
1
9
Y
3
N
r
e
J
2
a
W
d
3
b
/
+
g
f
H
j
U
0
n
G
q
G
D
Z
Z
L
G
L
V
C
a
h
G
w
S
U
2
D
T
c
C
O
4
l
C
G
g
U
C
2
8
H
4
d
u
a
3
H
1
F
p
H
s
t
7
M
0
n
Q
j
+
h
Q
8
p
A
z
a
q
z
U
e
O
q
X
K
2
7
V
n
Y
O
s
E
i
8
n
F
c
h
R
7
5
e
/
e
o
O
Y
p
R
F
K
w
w
T
V
u
u
u
5
i
f
E
z
q
g
x
n
A
q
e
l
X
q
o
x
o
W
x
M
h
9
i
1
V
N
I
I
t
Z
/
N
D
5
2
S
M
6
s
M
S
B
g
r
W
9
K
Q
u
f
p
7
I
q
O
R
1
p
M
o
s
J
0
R
N
S
O
9
7
M
3
E
/
7
x
u
a
s
J
r
P
+
M
y
S
Q
1
K
t
l
g
U
p
o
K
Y
m
M
y
+
J
g
O
u
k
B
k
x
s
Y
Q
y
x
e
2
t
h
I
2
o
o
s
z
Y
b
E
o
2
B
G
/
5
5
V
X
S
u
q
h
6
b
t
V
r
X
F
Z
q
N
3
k
c
R
T
i
B
U
z
g
H
D
6
6
g
B
n
d
Q
h
y
Y
w
Q
H
i
G
V
3
h
z
H
p
w
X
5
9
3
5
W
L
Q
W
n
H
z
m
G
P
7
A
+
f
w
B
6
U
G
M
/
g
=
=
<
/
l
a
t
e
x
i
t
>
training of the generator. More importantly, they also pro-
vide a way to avoid such difﬁculties by introducing noise
and considering “softer” metrics such as the Wasserstein
[23] makes similar observations and also pro-
distance.
posed the use of “instance noise” which is gradually re-
duced during training as a way to overcome these issues.
Another recent work stabilizes GAN training in a similar
way by transforming examples before feeding them to the
discriminator [21]. The amount of transformation is then
gradually reduced during training. They only transform the
real examples, in contrast to [23], [1] and our work.
[2]
builds on the work of [1] and introduces the Wasserstein
GAN (WGAN). The WGAN optimizes an integral prob-
ability metric that is the dual to the Wasserstein distance.
This formulation requires the discriminator to be Lipschitz-
continuous, which is realized through weight-clipping. [7]
presents a better way to enforce the Lipschitz constraint via
a gradient penalty over interpolations between real and gen-
erated data (WGAN-GP). [19] introduces a stabilizing reg-
ularizer based on a gradient norm penalty similar to that by
[7].
Its formulation however is in terms of f-divergences
and is derived via an analytic approximation of adversarial
training with additive Gaussian noise on the datapoints. An-
other recent GAN regularization technique that bounds the
Lipschitz constant of the discriminator is the spectral nor-
malization introduced by [17]. This method demonstrates
state-of-the-art in terms of robustness in adversarial train-
ing. Several alternative loss functions and GAN models
have been proposed over the years, claiming superior sta-
bility and sample quality over the original GAN (e.g., [16],
[25], [3], [2], [25], [11]). Adversarial noise generation has
previously been used in the context of classiﬁcation to im-
prove the robustness against adversarial perturbations [13].

3. Matching Filtered Distributions

We are interested in ﬁnding a formulation that yields as
optimal generator G a sampler of the data probability den-
sity function (pdf) pd, which we assume is well deﬁned.
The main difﬁculty in dealing with pd is that it may be zero
on some neighborhood in the data space. An iterative op-
timization of Problem (1) based on gradient descent may
yield a degenerate solution, i.e., such that the model pdf
pg only partially overlaps with pd (a scenario called mode
collapse). It has been noticed that adding samples of an ar-
bitrary distribution to both real and fake data samples dur-
ing training helps reduce this issue. In fact, adding samples
ǫ ∼ pǫ corresponds to blurring the original pdfs pd and pg,
an operation that is known to increase their support and thus
their likelihood to overlap. This increased overlap means
that iterative methods can exploit useful gradient directions
at more locations and are then more likely to converge to
the global solution. By building on this observation, we
propose to solve instead Problem (2) and look for a way to

increase the support of the data pdf pd without losing the
optimality conditions of the original formulation of Prob-
lem (1).

Our result below proves that this is the case for some
choices of the additive noise. We consider images of m × n
pixels and with values in a compact domain Ω ⊂ Rm×n,
since image intensities are bounded from above and below.
Then, also the support of the pdf pd is bounded and con-
tained in Ω. This implies that pd is also L2(Ω).

Theorem 1. Let us choose S such that Problem (4) can be
written as

min
pg

JSD(cid:18) 1

2

(pd + pd ∗ pǫ),

1
2

(pg + pg ∗ pǫ)(cid:19) ,

(5)

where pǫ is a zero-mean Gaussian function with a positive
deﬁnite covariance Σ. Let us also assume that the domain
of pg is restricted to Ω (and thus pg ∈ L2(Ω)). Then, the
global optimum of Problem (5) is pg(x) = pd(x), ∀x ∈ Ω.

Proof. The global minimum of the Jensens-Shannon diver-
gence is achieved if and only if

pd + pd ∗ pǫ = pg + pg ∗ pǫ.

(6)

Let pg = pd + ∆. Then, we have R |∆(x)|2dx < ∞. By

substituting pg in eq. (6) we obtain ∆ ∗ pǫ = −∆. Since ∆
and pǫ are in L2(Ω), we can take the Fourier transform of
both sides, compute their absolute value and obtain

,

(7)

(cid:12)(cid:12)(cid:12)

∀ω ∈ ˆΩ.

ˆ∆(ω)(cid:12)(cid:12)(cid:12)

|ˆpǫ(ω)| = (cid:12)(cid:12)(cid:12)
Because pg and pd integrate to 1, R ∆(x)dx = 0 and
ˆ∆(0) = 0. Suppose ∃ω 6= 0 such that ˆ∆(ω) 6= 0. Since
pǫ is Gaussian, |ˆpǫ(ω)| = (cid:12)(cid:12)(cid:12)
< 1, which contra-
dicts the optimality condition (7). Thus, ∆(x) = 0, ∀x ∈ Ω
and we can conclude that pg(x) = pd(x), ∀x ∈ Ω.

ˆ∆(ω)(cid:12)(cid:12)(cid:12)
2 ω⊤Σ−1ω(cid:12)(cid:12)(cid:12)

e− 1

3.1. Formulation

Based on the above theorem we consider two cases:

1. Gaussian noise with a ﬁxed/learned standard devia-

tion σ: pǫ(ǫ) = N (ǫ; 0, σId);

2. Learned noise from a noise generator network N with
parameters σ: pǫ(ǫ) such that ǫ = N (w, σ), with w ∼
N (0, Id).

In both conﬁgurations we can learn the parameter(s) σ. We
do so by minimizing the cost function after the maximiza-
tion with respect to the discriminator. The minimization
encourages large noise since this would make pd,ǫ(ω) more
similar to pg,ǫ(ω) regardless of pd and pg. This would not be
very useful to gradient descent. Therefore, to limit the noise

12147

Algorithm 1: Distribution Filtering GAN (DFGAN)
Input: Training set D ∼ pd, number of discriminator
updates ndisc, number of training iterations N ,
batch-size m, learning rate α, noise penalty λ

Output: Generator parameters θ
Initialize generator parameters θ, discriminator

parameters φ and noise-generator parameters ω ;

for 1 . . . N do

for 1 . . . ndisc do

Sample {x1, . . . , xm} ∼ pd, {˜x1, . . . , ˜xm} ∼ pg

and {ǫ1, . . . , ǫm} ∼ pǫ ;
D = Pm
Lr
i=1 ln(D(xi)) + ln(D(xi + ǫi));
D = Pm
Lf
i=1 ln(1−D(˜xi))+ln(1−D(˜xi +ǫi));
Lǫ = Pm
i=1 |ǫi|2;
φ ← φ + ∇φLr
ω ← ω − ∇ω(cid:0)Lr

D(φ, ω) + ∇φLf
D(φ, ω) + Lf

D(φ, ω) + λLǫ(ω)(cid:1);

D(φ, ω);

end
Sample {˜x1, . . . , ˜xm} ∼ pg and {ǫ1, . . . , ǫm} ∼ pǫ ;
Lf
θ ← θ + ∇θLf

i=1 ln(D(˜xi)) + ln(D(˜xi + ǫi));

G = Pm

G(θ);

end

magnitude we introduce as a regularization term the noise
variance Γ(σ) = σ2 or the Euclidean norm of the noise out-
put image Γ(σ) = Ew∼N (0,Id)|N (w, σ)|2, and multiply it
by a positive scalar λ, which we tune.

The proposed formulations can then be written in a uni-

ﬁed way as:

min

G

σ

D

min

max

λΓ + Exh log D(x) + Eǫ log D(x + ǫ)i+
Ezh log[1 − D(G(z))] + Eǫ log[1 − D(G(z) + ǫ)]i.

(8)

3.2. Implementation

Implementing our algorithm only requires a few minor
modiﬁcations of the standard GAN framework. We perform
the update for the noise-generator and the discriminator in
the same iteration. Mini-batches for the discriminator are
formed by collecting all the fake and real samples in two
separate batches, i.e., {x1, . . . , xm, x1 + ǫ1, . . . , xm + ǫm}
is the batch with real examples and {˜x1, . . . , ˜xm, ˜x1 +
ǫ1, . . . , ˜xm + ǫm} the fake examples batch. The com-
plete procedure is outlined in Algorithm 1. The noise-
generator architecture is typically the same as the gener-
ator, but with a reduced number of convolutional ﬁlters.
Since the inputs to the discriminator are doubled when com-
pared to the standard GAN framework, the DFGAN frame-
work can be 1.5 to 2 times slower. Similar and more
severe performance drops are present in existing variants
(e.g., WGAN-GP). Note that by constructing the batches
as {x1, . . . , xm/2, xm/2+1 + ǫ1, . . . , xm + ǫm} the training

(a)

(b)

Figure 3: Illustration of how separate normalization of fake
and real mini-batches discourages mode collapse.
In (a)
no normalization is applied and mode collapse is observed.
Since the covered modes are indistinguishable, the genera-
tor receives no signal that encourages better mode coverage.
In (b) separate normalization of the real and fake data is ap-
plied. The mismatch in the batch statistics (mean and stan-
dard deviation) can now be detected by the discriminator,
forcing the generator to improve.

time is instead comparable to the standard framework, but
it is much more stable and yields an accurate generator. For
a comparison of the runtimes, see Fig. 4.

3.3. Batch Normalization and Mode Collapse

The current best practice is to apply batch normalization
to the discriminator separately on the real and fake mini-
batches [4]. Indeed, this showed much better results when
compared to feeding mini-batches with a 50/50 mix of real
and fake examples in our experiments. The reason for this
is that batch normalization implicitly takes into account the
distribution of examples in each mini-batch. To see this,
consider the example in Fig. 3.
In the case of no sepa-
rate normalization of fake and real batches we can observe
mode-collapse. The modes covered by the generator are in-
distinguishable for the discriminator, which observes each
example independently. There is no signal to the genera-
tor that leads to better mode coverage in this case. Since
the ﬁrst two moments of the fake and real batch distribution
are clearly not matching, a separate normalization will help
the discriminator distinguish between real and fake exam-
ples and therefore encourage better mode coverage by the
generator.

Using batch normalization in this way turns out to be cru-
cial for our method as well. Indeed, when no batch normal-
ization is used in the discriminator, the generator will often
tend to produce noisy examples. This is difﬁcult to detect
by the discriminator, since it judges each example indepen-
dently. To mitigate this issue we apply separate normal-
ization of the noisy real and fake examples before feeding
them to the discriminator. We use this technique for models
without batch normalization (e.g. SNGAN).

12148

Table 1: Network architectures used for experiments on
CIFAR-10 and STL-10. Images are assumed to be of size
32 × 32 for CIFAR-10 and 64 × 64 for STL-10. We set
M = 512 for CIFAR-10 and M = 1024 for STL-10. Lay-
ers in parentheses are only included for STL-10. The noise-
generator network follows the generator architecture with
the number of channels reduced by a factor of 8. BN indi-
cates the use of batch-normalization [9].

Generator CIFAR-10/(STL-10)

Discriminator CIFAR-10/(STL-10)

z ∈ R128 ∼ N (0, I)
fully-conn. BN ReLU 4 × 4 × M
(deconv 4 × 4 str.=2 BN ReLU 512)
deconv 4 × 4 str.=2 BN ReLU 256
deconv 4 × 4 str.=2 BN ReLU 128
deconv 4 × 4 str.=2 BN ReLU 64
deconv 3 × 3 str.=1 tanh 3

conv 3 × 3 str.=1 lReLU 64
conv 4 × 4 str.=2 BN lReLU 64
conv 4 × 4 str.=2 BN lReLU 128
conv 4 × 4 str.=2 BN lReLU 256
conv 4 × 4 str.=2 BN lReLU 512
(conv 4 × 4 str.=2 BN lReLU 1024)
fully-connected sigmoid 1

also produce noisy samples since there is no incentive
to remove the noise. Annealing the added noise dur-
ing training as proposed by [1] and [23] leads to an
improvement over the standard GAN. This is demon-
strated in experiment (c). The added Gaussian noise
is linearly annealed during the 100K iterations in this
case;

(d)-(i) Both noisy and clean samples: The second set of
experiments consists of variants of our proposed
model. Experiments (d) and (e) use a simple Gaus-
sian noise model; in (e) the standard deviation of the
noise σ is learned. We observe a drastic improvement
in the quality of the generated examples even with this
simple modiﬁcation. The other experiments show re-
sults of our full model with a separate noise-generator
network. We vary the weight λ of the L2 norm of the
noise in experiments (f)-(h). Ablation (i) uses the al-
ternative mini-batch construction with faster runtime
as described in Section 3.2;

Application to Different GAN Models. We investigate the
possibility of applying our proposed training method to sev-
eral standard GAN models. The network architectures are
the same as proposed in the original works with only the
necessary adjustments to the given image-resolutions of the
datasets (i.e., truncation of the network architectures). The
only exception is SVM-GAN, where we use the architecture
in Table 1. Note that for the GAN with minimax loss (MM-
GAN) and WGAN-GP we use the architecture of DCGAN.
Hyper-parameters are kept at their default values for each
model. The models are evaluated on two common GAN
benchmarks: CIFAR-10 [12] and CelebA [15]. The im-
age resolution is 32 × 32 for CIFAR-10 and 64 × 64 for
CelebA. All models are trained for 100K generator itera-
tions. For the alternative objective function of LSGAN and

12149

Figure 4: A comparison of wall clock time vs IS for GANs
with and without distribution ﬁltering. The models use
the architecture speciﬁed in Table 1 and were trained on
CIFAR-10. The computational overhead introduced by our
method does not negatively affect the speed of convergence.

4. Experiments

We compare and evaluate our model using two common
GAN metrics: the Inception score IS [22] and the Fr´echet
Inception distance FID [8]. Throughout this section we use
10K generated and real samples to compute IS and FID.
In order to get a measure of the stability of the training
we report the mean and standard deviation of the last ﬁve
checkpoints for both metrics (obtained in the last 10% of
training). More reconstructions, experiments and details are
provided in the supplementary material.

4.1. Ablations

To verify our model we perform ablation experiments
on two common image datasets: CIFAR-10 [12] and STL-
10 [5]. For CIFAR-10 we train on the 50K 32 × 32 RGB
training images and for STL-10 we resize the 100K 96 ×
96 training images to 64 × 64. The network architectures
resemble the DCGAN architectures of [18] and are detailed
in Table 1. All the models are trained for 100K generator
iterations using a mini-batch size of 64. We use the ADAM
optimizer [10] with a learning rate of 10−4 and β1 = 0.5.
Results on the following ablations are reported in Table 2:

(a)-(c) Only noisy samples: In this set of experiments we
only feed noisy examples to the discriminator. In ex-
periment (a) we add Gaussian noise and in (b) we add
learned noise.
In both cases the noise level is not
annealed. While this leads to stable training, the re-
sulting samples are of poor quality which is reﬂected
by high FID and low IS. The generator will tend to

Table 2: We perform ablation experiments on CIFAR-10 and STL-10 to demonstrate the effectiveness of our proposed
algorithm. Experiments (a)-(c) show results where only ﬁltered examples are fed to the discriminator. Experiment (c)
corresponds to previously proposed noise-annealing and results in an improvement over the standard GAN training. Our
approach of feeding both ﬁltered and clean samples to the discriminator shows a clear improvement over the baseline.

Experiment

Standard GAN

(a) Noise only: ǫ ∼ N (0, I)
(b) Noise only: ǫ learned
(c) Noise only: ǫ ∼ N (0, σI), σ → 0

(d) Clean + noise: ǫ ∼ N (0, I)
(e) Clean + noise: ǫ ∼ N (0, σI) with learnt σ
(f) DFGAN (λ = 0.1)
(g) DFGAN (λ = 1)
(h) DFGAN (λ = 10)

(i) DFGAN alt. mini-batch (λ = 1)

CIFAR-10

STL-10

FID

46.1 ± 0.7

94.9 ± 4.9
69.0 ± 3.4
44.5 ± 3.2

29.7 ± 0.6
28.8 ± 0.7
27.7 ± 0.8
26.5 ± 0.6
29.8 ± 0.4

28.7 ± 0.6

IS

FID

IS

6.12 ± .09

4.68 ± .12
5.05 ± .14
6.85 ± .20

7.16 ± .05
7.23 ± .14
7.31 ± .06
7.49 ± .04
6.55 ± .08

7.3 ± .05

78.4 ± 6.7

107.9 ± 2.3
107.2 ± 3.4
75.9 ± 1.9

66.5 ± 2.3
71.3 ± 1.7
63.9 ± 1.7
64.0 ± 1.4
66.9 ± 3.2

67.8 ± 3.2

8.22 ± .37

6.48 ± .19
6.39 ± .22
8.49 ± .19

8.64 ± .17
8.30 ± .12
8.81 ± .07
8.52 ± .16
8.38 ± .20

8.30 ± .11

SVM-GAN we set the loss of the noise generator to be the
negative of the discriminator loss, as is the case in our stan-
dard model. The results are shown in Table 3. We can ob-
serve that applying our training method improves perfor-
mance in most cases and even enables the training with the
original saturation-prone minimax GAN objective, which
is very unstable otherwise. Note also that applying our
method to SNGAN [17] (the current state-of-the-art) leads
to an improvement on both datasets. We also evaluated
SNGAN with and without our method on 64 × 64 images of
STL-10 (same as in Table 2) where our method boosts the
performance from an FID of 66.3 ± 1.1 to 58.3 ± 1.4. We
show random CelebA reconstructions from models trained
with and without our approach in Fig. 5.

Robustness to Hyperparameters. We test the robust-
ness of DFGANs with respect to various hyperparamters by
training on CIFAR-10 with the settings listed in Table 4.
The network is the same as speciﬁed in Table 1. The noise
penalty term is set to λ = 0.1. We compare to a model with-
out our training method (Standard), a model with the gradi-
ent penalty regularization proposed by [19] (GAN+GP) and
a model with spectral normalization (SNGAN). To the best
of our knowledge, these methods are the current state-of-
the-art in terms of GAN stabilization. Fig. 7 shows that our
method is stable and accurate across all settings.

Robustness to Network Architectures. To test the ro-
bustness of DFGANs against non-optimal network archi-
tectures we modiﬁed the networks in Table 1 by doubling
the number of layers in both generator and discriminator.
This leads to signiﬁcantly worse performance in terms of
FID in all cases: 46 to 135 (Standard), 33 to 111 (SNGAN),
28 to 36 (GAN+GP), and 27 to 60 (DFGAN). However,

Table 3: We apply our proposed GAN training to various
previous GAN models trained on CIFAR-10 and CelebA.
The same network architectures and hyperparameters as in
the original works are used (for SVM-GAN we used the
network in Table 1). We observe that our method increases
performance in most cases even with the suggested hyper-
parameter settings. Note that our method also allows suc-
cessful training with the original minimax MMGAN loss as
opposed to the commonly used heuristic (e.g., in DCGAN).

Model

MMGAN [6]
DCGAN [18]
WGAN-GP [7]
LSGAN [16]
SVM-GAN [14]
SNGAN ([17]

MMGAN +DF (λ = 0.1)
DCGAN + DF (λ = 10)
LSGAN + DF (λ = 10)
SVM-GAN + DF (λ = 1)
SNGAN + DF (λ = 1)

CIFAR-10

FID

> 450

33.4 ± 0.5
37.7 ± 0.4
38.7 ± 1.8
43.9 ± 1.0
29.1 ± 0.4

33.1 ± 0.7
31.2 ± 0.3
36.7 ± 1.2
28.7 ± 1.1
25.9 ± 0.3

IS

∼ 1

6.73 ± .07
6.55 ± .08
6.73 ± .12
6.25 ± .09
7.26 ± .06

6.91 ± .05
6.95 ± .11
6.63 ± .17
7.31 ± .11
7.47 ± .08

CelebA

FID

> 350

25.4 ± 2.6
15.5 ± 0.2
21.4 ± 1.1
26.5 ± 1.9
13.2 ± 0.3

16.6 ± 1.9
14.7 ± 1.0
19.9 ± 0.4
12.7 ± 0.7
10.5 ± 0.4

SNGAN+DF leads to good results with a FID of 27.6.

4.2. Qualitative Results

We trained DFGANs on 128 × 128 images from two
large-scale datasets: ImageNet [20] and LSUN bedrooms
[24]. The network architecture is similar to the one in Ta-
ble 1 with one additional layer in both networks. We trained
the models for 100K iterations on LSUN and 300K iter-

12150

(a) Original GAN without DF

(b) Original GAN with DF

(c) DCGAN without DF

(d) DCGAN with DF

Figure 5: Left column: Random reconstructions from models trained on CelebA without distribution ﬁltering (DF). Right
column: Random reconstructions with our proposed method.

Figure 6: Reconstructions from DFGANs trained on 128 × 128 images from the LSUN bedrooms dataset (top) and ImageNet
(bottom).

12151

Figure 7: Results of the robustness experiments in Table 4 on CIFAR-10. We compare the standard GAN (1st column), a
GAN with gradient penalty (2nd column), a GAN with spectral normalization (3rd column) and a GAN with our proposed
method (4th column). Results are reported in Fr´echet Inception Distance FID (top) and Inception Score IS (bottom).

Table 4: Hyperparameter settings used to evaluate the ro-
bustness of our proposed GAN training method. We vary
the learning rate α, the normalization in G, the optimizer,
the activation functions, the number of discriminator itera-
tions ndisc and the number of training examples ntrain.

Exp.

a)
b)
c)
d)
e)
f)

LR α
2 · 10−4
2 · 10−4
1 · 10−3
1 · 10−2
2 · 10−4
2 · 10−4

BN in G

Opt.

ActFn

ndisc

ntrain

FALSE
TRUE
TRUE
TRUE
TRUE
TRUE

tanh

ADAM (l)ReLU
ADAM
ADAM (l)ReLU
(l)ReLU
ADAM (l)ReLU
ADAM (l)ReLU

SGD

1
1
1
1
5
1

50K
50K
50K
50K
50K
5K

Figure 8: Examples of the generated noise (top row) and
corresponding noisy training examples (rows 2 to 4). The
columns correspond to different iterations. The noise varies
over time to continually challenge the discriminator.

ations on ImageNet. Random samples of the models are
shown in Fig. 6. In Fig. 8 we show some examples of the
noise that is produced by the noise generator at different
stages during training. These examples resemble the image
patterns that typically appear when the generator diverges.

5. Conclusions

We have introduced a novel method to stabilize genera-
tive adversarial training that results in accurate generative
models. Our method is rather general and can be applied to
other GAN formulations with an average improvement in
generated sample quality and variety, and training stability.
Since GAN training aims at matching probability density
distributions, we add random samples to both generated

and real data to extend the support of the densities and
thus facilitate their matching through gradient descent.
We demonstrate the proposed training method on several
common datasets of real images.

Acknowledgements. This work was supported by the
Swiss National Science Foundation (SNSF) grant num-
ber 200021 169622. We also wish to thank Abdelhak
Lemkhenter for discussions and for help with the proof of
Theorem 1.

12152

References

[1] Martin Arjovsky and L´eon Bottou. Towards principled
methods for training generative adversarial networks. arXiv
preprint arXiv:1701.04862, 2017. 1, 2, 3, 5

[2] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein generative adversarial networks.
In Interna-
tional Conference on Machine Learning, pages 214–223,
2017. 3

[3] David Berthelot, Tom Schumm, and Luke Metz.

Be-
gan: Boundary equilibrium generative adversarial networks.
arXiv preprint arXiv:1703.10717, 2017. 3

[4] Soumith Chintala, Emily Denton, Martin Arjovsky, and
Michael Mathieu. How to train a gan? tips and tricks to
make gans work. https://github.com/soumith/
ganhacks, 2016. 4

[5] Adam Coates, Andrew Ng, and Honglak Lee. An analy-
sis of single-layer networks in unsupervised feature learning.
In Proceedings of the fourteenth international conference on
artiﬁcial intelligence and statistics, pages 215–223, 2011. 2,
5

[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014. 1, 6

[7] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron C Courville.
Improved training of
wasserstein gans. In Advances in Neural Information Pro-
cessing Systems, pages 5769–5779, 2017. 3, 6

[8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, G¨unter Klambauer, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a
nash equilibrium. arXiv preprint arXiv:1706.08500, 2017. 5
[9] Sergey Ioffe and Christian Szegedy. Batch normalization:
accelerating deep network training by reducing internal co-
variate shift. In Proceedings of the 32nd International Con-
ference on International Conference on Machine Learning-
Volume 37, pages 448–456. JMLR. org, 2015. 5

[10] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 5

In 2017 IEEE International Confer-
adversarial networks.
ence on Computer Vision (ICCV), pages 2813–2821. IEEE,
2017. 3, 6

[17] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks. arXiv preprint arXiv:1802.05957, 2018.
3, 6

[18] Alec Radford, Luke Metz, and Soumith Chintala. Un-
supervised representation learning with deep convolu-
tional generative adversarial networks.
arXiv preprint
arXiv:1511.06434, 2015. 2, 5, 6

[19] Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and
Thomas Hofmann. Stabilizing training of generative ad-
versarial networks through regularization.
In Advances in
Neural Information Processing Systems, pages 2015–2025,
2017. 1, 3, 6

[20] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211–252, 2015. 2, 6

[21] Mehdi SM Sajjadi and Bernhard Sch¨olkopf. Tempered ad-
versarial networks. arXiv preprint arXiv:1802.04374, 2018.
3

[22] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In Advances in Neural Information Pro-
cessing Systems, pages 2234–2242, 2016. 2, 5

[23] Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wen-
zhe Shi, and Ferenc Husz´ar. Amortised map inference for
image super-resolution. arXiv preprint arXiv:1610.04490,
2016. 1, 3, 5

[24] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianx-
iong Xiao.
Lsun: Construction of a large-scale image
dataset using deep learning with humans in the loop. CoRR,
abs/1506.03365, 2015. 2, 6

[25] Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-
arXiv preprint

based generative adversarial network.
arXiv:1609.03126, 2016. 3

[11] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt
arXiv preprint

How to train your dragan.

Kira.
arXiv:1705.07215, 2017. 3

[12] Alex Krizhevsky. Learning multiple layers of features from

tiny images. 2009. 2, 5

[13] Hyeungill Lee, Sungyeob Han, and Jungwoo Lee. Genera-
tive adversarial trainer: Defense to adversarial perturbations
with gan. arXiv preprint arXiv:1705.03387, 2017. 3

[14] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv

preprint arXiv:1705.02894, 2017. 6

[15] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), De-
cember 2015. 2, 5

[16] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares generative

12153

