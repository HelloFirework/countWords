Hierarchical Discrete Distribution Decomposition for Match Density Estimation

Zhichao Yin

Trevor Darrell

Fisher Yu

UC Berkeley

Abstract

Explicit representations of the global match distributions
of pixel-wise correspondences between pairs of images are
desirable for uncertainty estimation and downstream ap-
plications. However, the computation of the match den-
sity for each pixel may be prohibitively expensive due to
the large number of candidates. In this paper, we propose
Hierarchical Discrete Distribution Decomposition (HD3),
a framework suitable for learning probabilistic pixel corre-
spondences in both optical Ô¨Çow and stereo matching. We
decompose the full match density into multiple scales hi-
erarchically, and estimate the local matching distributions
at each scale conditioned on the matching and warping at
coarser scales. The local distributions can then be com-
posed together to form the global match density. Despite
its simplicity, our probabilistic method achieves state-of-
the-art results for both optical Ô¨Çow and stereo matching on
established benchmarks. We also Ô¨Ånd the estimated uncer-
tainty is a good indication of the reliability of the predicted
correspondences.

1. Introduction

Finding dense pixel correspondences between two im-
ages, typically for stereo matching or optical Ô¨Çow, is one of
the earliest problems studied in the computer vision litera-
ture. Dense correspondences have wide application includ-
ing for activity recognition [36], video interpolation [23],
scene geometry perception [13], and many others. Chal-
lenges when solving this problem include texture ambigu-
ity, complex object motion, illumination change, and entan-
gled occlusion estimation.

Classic approaches jointly optimize local texture match-
ing and neighbor afÔ¨Ånity on images [17] possibly in a
coarse-to-Ô¨Åne fashion [6, 20, 42]. While these methods can
achieve impressive correspondence accuracy, the optimiza-
tion step may be too slow for downstream applications. Re-
cent works using deep convolutional networks (ConvNets)
have achieved similar or even better matching results with-
out an optimization step [10, 21, 40]. Pixel features learned
directly from correspondence supervision can capture both
local appearance and global context information due to the

large network receptive Ô¨Åelds. With GPU acceleration, it is
possible to use these networks to regress the pixel displace-
ments in real time [10, 40].

However,

the estimation uncertainty inherent in cor-
respondence estimation is neglected by displacement re-
gression approaches. Though post-hoc conÔ¨Ådence mea-
sures [27, 29] can recover the uncertainty to some de-
gree, they are independent of model training; uncertainty
is ignored in the training process. Recognizing the miss-
ing uncertainty measures in optical Ô¨Çow methods, some
works [12, 43] propose probabilistic frameworks for joint
correspondence and uncertainty estimation. Due to con-
straints on computation and parameter number, they rely on
the local Gaussian noise assumption to represent the match
distribution. Consequently, they cannot model complicated
distributions on a large image area. Early works in stereo
matching show that we can build a complete match cost vol-
ume as a proxy to estimate the match density, but were not
applicable for high-resolution stereo matching nor general
optical Ô¨Çow due to the excessive amount of computation
needed for the complete cost volume.

In this work, we propose Hierarchical Discrete Distribu-
tion Decomposition (HD3), a general probabilistic frame-
work for match density estimation. We aim to Ô¨Ånd the dis-
crete distribution of possible correspondences with a large
support deÔ¨Åned on the image grids for each pixel. We adopt
a general model to represent pixel-level match probabil-
ity without any parametric distribution assumption. The
model-inherent uncertainty measures can be naturally de-
rived from our estimated match densities.

HD3 decomposes the full match density into multiple
levels of local distributions similar to quadtrees. To extract
discriminative features for matching, we use networks with
Deep Layer Aggregation (DLA) [50] to build the multi-
scale feature pyramid. The DLA framework provides us
with feature networks of different computation-accuracy
trade-offs, which can be easily integrated with other recog-
nition tasks in complex applications. We estimate the match
density of the residual motion in each scale, conditioned on
match densities at coarser scales. We can propagate the con-
ditional information from previous levels to the prediction
at the current level through iterative feature warping and

16044

Reference Frame

Target Frame

Figure 1: Illustration of HD3. We aim to estimate discrete match distribution in this work. For reducing the infeasible computation cost,
the overall distribution is decomposed into multiple scales hierarchically at learning time. The full match information can be recovered by
composing predictions from all levels. Please refer to Sec. 3.2 for more details.

density bypass connections. The multi-scale match densi-
ties can then be used to recover the complete match density.
We can easily convert between point estimates and match
densities to train our models on existing datasets with anno-
tations in the form of motion vectors.

We evaluate our framework extensively in two appli-
cations: stereo matching and optical Ô¨Çow. Our method
achieves state-of-the-art results on both the synthetic dataset
MPI Sintel [7] and the real dataset KITTI [13]. Our method
not only surpasses all two-frame based optical Ô¨Çow methods
by large margins but also beats some competitive scene Ô¨Çow
methods on both KITTI 2012 & 2015. We also evaluate our
uncertainty estimation and demonstrate the error-awareness
of our method in its predictions. Our code is available at
https://github.com/ucbdrive/hd3.

2. Related Work

Great efforts have been devoted to the problems of Ô¨Ånd-
ing dense correspondences in the past four decades. For a
thorough review, we refer to popular benchmarks includ-
ing Middlebury [4], MPI Sintel [7], and KITTI [32] bench-
marks for both the classical methods and the latest advances
in these areas. We will discuss the most related ideas in this
section.

Correspondence Estimation. Classical stereo matching
usually involves local correspondence extraction and semi-
global regularization [16]. On the other hand, optical Ô¨Çow
methods typically adopt MRFs [28] to jointly reason about
the displacements, occlusions, and symmetries [20, 46] for
tackling the more unconstrained and challenging 2D cor-
respondence problem. Despite the distinct differences be-
tween the search space dimensions, stereo matching and
optical Ô¨Çow share similar assumptions such as brightness
constancy and edge-preserving continuity [17, 34, 40].

With the success of deep learning, end-to-end mod-
els have been designed for these dense prediction tasks.

BeneÔ¨Åting from pretraining on a large corpus of synthetic
data [30], these methods achieve impressive results on par
with classical methods [10, 21]. Furthermore, recent ad-
vances emphasize the incorporation of classical principles
into network designs, such as pyramid matching, feature
warping, and contextual regularizer [19, 24, 40]. These im-
provements contribute to the superior performance of deep
learning models, allowing them to surpass classical meth-
ods. However, such learning methods neglect the model-
inherent uncertainty estimation, i.e. they are agnostic of the
prediction failure, which is quite important for applications
such as autonomous driving and medical imaging. In con-
trast, our work focuses on the probabilistic correspondence
estimation, which can naturally convey the conÔ¨Ådence of
the predictions.

Uncertainty Measures. Various uncertainty measures have
been proposed for classical optical Ô¨Çow estimation. Bar-
ron et al. [5] proposed a simple method based on the in-
put data characteristics while ignoring the estimated optical
Ô¨Çow itself. Kondermann et al. [27] learned a probabilistic
Ô¨Çow model and obtained uncertainty estimation through hy-
pothesis testing. Mac Aodha et al. [29] trained a classiÔ¨Åer
to assess the prediction quality in terms of end-point-error.
These methods either leverage only part of the input infor-
mation, such as images or predicted Ô¨Çow, for uncertainty
estimation, or require post-processing steps independent of
model inference itself.

Recently, Gast et al. [12] recognized the importance
of model-inherent uncertainty measures for deep networks.
They proposed probabilistic output layers and employed as-
sumed density Ô¨Åltering to propagate activation uncertainties
through the network. For computational tractability, they
assumed Gaussian noise and adopted a parametric distribu-
tion. Though it can be easily adapted for use with existing
regression networks, their performance is only competitive
with the deterministic counterparts. Our method provides
inherent uncertainty estimation as well as new state-of-the-

6045

	ùîº[ùêü ",-]

ùúë

V2D

"
ùëù$%

Motion Vector

Upsample

Vector to Density

Match Density

"
ùêπ(

W

"
ùêπ‚Äô(

"
ùêü$%

Pyramid Feature

Warping

Warped Feature

Motion Vector

L

Loss

D2V

"

ùëù345

"
ùêπ-

Pyramid Feature

",-
ùê∏*+

C

Correlation

ùê∑"

Match Density

Density to Vector

"

ùê∏*+

	ùîº[ùê† "]

Density Embedding

Density Decoder

Density Embedding

Motion Vector

Figure 2: Overview of our architecture. The submodule at the lth level is presented here. F l and ÀúF l denotes the lth level of original
and warped pyramid features of image pair I. E l
up denotes upsampled density embeddings between different levels as density bypass
connections. f l and gl denote motion vectors and pl corresponds to match density. Their conversion is fulÔ¨Ålled by our D2V and V 2D
modules. For details please refer to our method part. This Ô¨Ågure is best viewed in color.

art results for both optical Ô¨Çow and stereo matching.

Coarse-to-Fine. Because of the complexity of Ô¨Ånding 2D
correspondences for each pixel in optical Ô¨Çow, it is natu-
ral to match the pixels from coarse to Ô¨Åne resolutions in
an image or feature pyramid. This method can be used ef-
fectively in the optimization methods [1, 2, 38] as well as
patch matching [18]. Its effectiveness is also veriÔ¨Åed by re-
cent deep learning approaches such as SpyNet [33], PWC-
Net [40], and LiteFlowNet [19]. We also estimate our hier-
archically decomposed match densities based on the feature
pyramid representation [50]. Our contribution lies in the de-
composition of the discrete probability distribution instead
of the feature representations.

3. Method

In this section, we discuss our probabilistic framework
for match density estimation. Without loss of generality,
we focus on solving 2D correspondences for optical Ô¨Çow in
this section, which can be easily adapted to the 1D case for
stereo matching.

3.1. Preliminary

We Ô¨Årst introduce the notations and basic concepts used.
Given a pair of images I = {I1, I2}, we denote the mo-
tion Ô¨Åeld as f = {fij} where fij = (uij, vij)T for pixels
(i, j), i = 1, . . . , n, j = 1, . . . , m.
In contrast to Wan-
nenwetsch et al. [43] where {fij} are continuous, we treat
them as discrete random variables. We call their density
functions match densities. We use p(f |I) to denote the joint
probability distribution of {fij}. For brevity, we omit the
conditional I in the following discussion when there is no

ambiguity. Finally, we introduce a √ó2 upsampling operator
œï and an opposite downsampling operator œï‚àí1.

3.2. Match Density Decomposition

The main challenge of estimating the full match den-
sity is the prohibitive computational cost. Assume we have
an image with size 1000 √ó 1000 and displacement range
[‚àí50, 50]. The cardinality of {fij} would be 106 and the
support size of each fij could be 104. In this case, the en-
tire distribution volume would have 10 billion cells, which
is intractable to generate.

Our key observation is that the full match density can be
decomposed hierarchically into multiple levels of distribu-
tions. Fig. 1 provides an intuitive illustration. Let us con-
sider multi-scale motion Ô¨Åelds {f l} (l = 0, . . . , L), where
higher level f l has half of the resolution of the lower level
f l+1 and f L is identical as f . We introduce a transformation
gl = f l ‚àí œï(f l‚àí1) to shift the absolute multi-scale motion
Ô¨Åelds to residual ones. We can recover the original motion
Ô¨Åeld f from {gl} (l = 0, . . . , L) via

f =

L

Xl=0

œïL‚àíl(gl).

(1)

Naturally, we have the decomposition of p(f ) as

L

p(f ) = X{gl}‚ààF

Yl=0

p(gl|Gl‚àí1),

(2)

s=0, and F is the set of all possible {gl}
where Gl = {gs}l
that satisÔ¨Åes Eq. 1. Therefore, we can in turn estimate the
decomposed match densities p(gl|Gl‚àí1) and recover full

6046

match density p(f ) through Eq. 2 afterward. The beneÔ¨Åt
of adopting such decomposition lies in that match density
p(gl|Gl‚àí1) actually has quite low variance, i.e. probabili-
ties concentrate on a small subset R‚Ä≤
gl of the entire support
Rgl . Without loss of much information, we can focus on
solving p(gl|Gl‚àí1) with gs ‚àà R‚Ä≤
gs for s = 0, . . . , l ‚àí 1.
Consequently, for maximizing the posterior distribution
p(f ), we achieve satisfactory approximation through max-
imizing each of the decomposed match densities. We will
discuss our selection of support subsets in the next section.

3.3. Learning Decomposed Match Density

Our objective becomes estimating multi-scale decom-
posed match densities p(gl|Gl‚àí1). We propose to learn
such information through multiple levels of ConvNets. At
each level, a ConvNet is designed to estimate the decom-
posed match density. Note that gl is conditioned on Gl‚àí1,
while theoretically we can sample gs ‚àà Gl‚àí1 according to
predicted densities at coarser levels.

In this section, we Ô¨Årst discuss how to transform point es-
timate into match density, which is adopted for generating
our distribution supervision for each level. Let us consider
a general motion vector fij ‚àà f and its density function
p(fij). As stated in Sec. 3.2, we prefer p(fij) to possess
a low variance, which would greatly reduce the computa-
tion cost through our decomposition. We observe that real-
valued fij uniquely falls into a 2 √ó 2 window Wij in the
image grid. This inspires us to splat the bilinear weights of
fij w.r.t. coordinates in Wij to p(fij). Concretely, for any
d ‚àà Z2, we have

P(fij = d) =( 0

œÅ(fij ‚àí Àúd)

d /‚àà Wij

d ‚àà Wij,

(3)

where œÅ(¬∑) means the product of elements in the vector, and
Àúd is the diagonal opposite coordinate of d in Wij . We call
such conversion as V2D (see Fig. 3), which depicts our as-
sumption for the ground-truth match density.

As seen from Eq. 3, the support of p(fij) is indeed Wij
which has a maximum size of 4. Ideally, we can sample
gs ‚àà Gl‚àí1 in a quadtree fashion during estimating the
match density of gl. However, such computation is still
heavy for both training and evaluation. For trade-off, we can
discard samplings with minor probabilities. A trivial prac-
tice is always taking arg max at each level. As a substitu-
tion, we propose local expectation to further reduce the loss
of information. SpeciÔ¨Åcally, for any general match density
p(fij), we deÔ¨Åne W ‚àó
ij as the 2 √ó 2 window over which the
integral of p(fij) maximizes among all candidate windows.
We only retain the probabilities of p(fij) in W ‚àó
ij and nor-
malize it into p‚àó(fij). The local expectation is deÔ¨Åned as
E[fij] w.r.t. p‚àó(fij). In the following, we use expectation to
denote local expectation by default. We call this conversion

-1

0

0

0.2

1

0

1

0

0.2

0.2

0.4

-1

0

0

0

D2V

-1

(0.5, 0.25)

1

1

-1

1

-1

-1

V2D

1

(0.6, -0.5)

-1

0

0

0

0

0

1

0

0.2

0.3

0.2

0.3

1

0

-1

Figure 3: Conversion between motion vectors and match densi-
ties. The support is taken as 3 √ó 3 here for illustration.

D2V (see Fig. 3). Therefore, at each level, instead of ex-
haustive sampling we always take the max posterior of gl as
E[gl], and we only estimate p(gl|G(l‚àí1)‚àó) (pl
res for short in
the following) in each level, where G(l‚àí1)‚àó = {E[gs]}l‚àí1
s=0.
This enables us to get rid of expensive training and test time
sampling.

3.4. Network Architecture Design

Finally, we discuss the network architecture design for
estimating the decomposed match densities. We achieve
this objective via stacking multiple levels of ConvNets,
which we call density decoders {Dl}. Dl infers the match
density pl
res in its respective level l. A single level of the
entire network is illustrated in Fig. 2. In the following, we
discuss the details of our subnetworks.

1, F l

The architecture design of our density decoder Dl is
motivated by the close relationship between the targeted
output pl
res and the similarity information between image
pairs, or their embedded representations. Our match den-
sity estimation operates on multi-scale feature embeddings
{F l
2}, which are extracted via a DLA [50] network over
{I1, I2}. AfÔ¨Ånity information can be obtained through the
correlation [10] of feature embeddings between different
frames. For performing long-range correlation and impos-
ing conditional priors from previous levels, we always warp
2 according to œï(E[f l‚àí1]) before correlation.
the feature F l
1, œï(E[f l‚àí1]), and
The cost volume is concatenated with F l
the upsampled density embedding El‚àí1
from the previous
level, then fed into our density decoder Dl. The decoder Dl
produces the density embedding, from which we obtain the
match density via a classiÔ¨Åer. Also, we upsample the den-

up

6047

Left Image

Right Image

Ground Truth

Prediction

Error Map

ConÔ¨Ådence Map

Figure 4: Visualized stereo results on the validation set of FlyingThings3D. Cold colors in the error map denote correct predictions while
warm colors mean the contrary. Our network gives accurate results in most regions, while errors tend to occur at boundaries and occlusions.

sity embedding to El
up and feed it to the next level as density
bypass connections. Both of the pyramid feature extractor
and the density decoder are jointly trained in an end-to-end
manner.

At inference time, we calculate E[gl] from each pre-
dicted pl
res and compose them via Eq. 1 to produce the point
estimate of f . While during training time, we downsample
the ground-truth motion Ô¨Åeld into f l
gt. The residual motion
w.r.t. œï(E[f l‚àí1]) is converted into pl
gt. The entire training
loss comes in the form of Kullback‚àíLeibler divergence

mantic segmentation accuracy on small datasets with much
less computation than the deeper alternatives. The features
at the coarsest level are √ó64 downsampled. The density
decoder Dl consists of two residual blocks plus one aggre-
gation node [15, 50], except for the last level when it is ful-
Ô¨Ålled via a dilated convolutional network [49] as a context
module. We adopt batch normalization [22] in all of our
models to stabilize the training. Predictions are upsampled
from the lowest level with highest output resolution to full
resolution during evaluation.

L =Xl Xg‚ààR

g

l

pl
gt(g)(log pl

gt(g) ‚àí log pl

res(g)).

(4)

4. Experiments

HD3 provides hierarchically decomposed match densi-
ties. It can be used for different tasks, such as stereo match-
ing and optical Ô¨Çow. The probability of point estimates can
be used as uncertainty estimation. It is hard to evaluate the
quality of the learned distribution directly, but we can inves-
tigate its performance being applied to these speciÔ¨Åc tasks.

4.1. Implementation Details

Network Variants. We can apply our models to stereo
matching and optical Ô¨Çow. The networks are called HD3S
and HD3F. The two variants differ slightly: we adopt 1D
correlation for HD3S and 2D correlation for HD3F. The cor-
relation range is always 4 for both tasks at different levels,
which is consistent with the size of match density support.
Since we treat stereo matching as 1D Ô¨Çow estimation, we
clip the positive values in converted point estimates at each
level for HD3S. The pyramid level is set to 5 for HD3F and
6 for HD3S based on experiment results.

Module Details. We select DLA-34-Up [50] as our pyra-
mid feature extractor, because it can achieve competitive se-

Training Details. We train our models on 8 GPUs with-
out synchronized batch normalization. The weights of pyra-
mid feature extractor are initialized from the ImageNet pre-
trained model. The network is optimized by Adam [25],
where Œ≤1 = 0.9, Œ≤2 = 0.999. For all of our pretraining
experiments on synthetic datasets, models are trained for
200 epochs, and the learning rate is decayed by 0.5 every
30 epochs after 70 epochs for 4 times in total. As for data
augmentation, besides random cropping, we adopt random
resizing and color perturbation [31] during the Ô¨Åne-tuning
stage, and introduce random Ô¨Çipping for optical Ô¨Çow exper-
iments. The dense and sparse annotations, as supervision
at different scales, are bilinearly downsampled and average
pooled from the ground-truth map respectively. In this sec-
tion, unless otherwise stated, conÔ¨Ådence maps are obtained
through aggregating the probabilities within W ‚àó
i of the last
level match density, and uncertainty maps are the opposite.

4.2. Stereo Matching

To evaluate the performance of our HD3S model, we
benchmark our result on the KITTI stereo dataset [13]. Due
to the limited amount of training data in KITTI, we pretrain
our model on the FlyingThings3D dataset [30].

FlyingThings3D. We use the FlyingThings3D dataset as

6048

Image 1

Image 2

Ground Truth

Prediction

Error Map

ConÔ¨Ådence Map

Figure 5: Qualitative multi-scale Ô¨Çow result on the validation set of FlyingThings3D dataset. Bilinearly downsampled raw images, coarser
level Ô¨Çows, error maps and conÔ¨Ådence maps are enlarged via nearest neighbor upsampling for visualization purpose. Our network gives
precise predictions in most regions, while occasionally presents confusion in occluded regions and disappearing parts.

KITTI 2012

KITTI 2015

Time

Methods Out-Noc Out-All D1-bg D1-fg D1-all

(s)

SPS-st [46]
Displets v2 [14]
MC-CNN-acrt [51]
SGM-Net [35]
L-ResMatch [37]
GC-Net [24]
EdgeStereo [39]
PDSNet [41]
PSMNet [8]
SegStereo [47]

HD3S (Ours)

3.39
2.37
2.43
2.29
2.27
1.77
1.73
1.92
1.49
1.68

1.40

4.41
3.09
3.63
3.50
3.40
2.30
2.18
2.53
1.89
2.03

3.84 12.67 5.31
3.00 5.56 3.43
2.89 8.88 3.88
2.66 8.64 3.66
2.72 6.95 3.42
2.21 6.16 2.87
2.27 4.18 2.59
2.29 4.05 2.58
1.86 4.62 2.32
1.88 4.07 2.25

2.00
265
67.0
67.0
48.0
0.90
0.27
0.50
0.41
0.60

1.80

1.70 3.63 2.02

0.14

Table 1: Stereo matching results on KITTI test set. All of the num-
bers denote percentages of disparity outliers. The ofÔ¨Åcial leader-
board ranks methods according to ‚ÄúOut-Noc‚Äù for KITTI 2012 and
‚ÄúD1-all‚Äù for KITTI 2015.

t
e
N
C
G

s
r
u
O

t
e
N
C
G

s
r
u
O

t
e
N
C
G

s
r
u
O

Figure 6: Example stereo error maps on KITTI 2015 test set. We
contrast our method with GC-Net [24]. Orange corresponds to
erroneous prediction. This Ô¨Ågure is best viewed in color.

training data. Following the training protocol of the orig-
inal FlowNet2 model [21], we use a subset of the dataset
which omits some extremely hard samples. We train our
model with a batch size of 32 and an initial learning rate of
2 √ó 10‚àí4. The image crop size is 320 √ó 896. Qualitative ex-
amples, as well as the conÔ¨Ådence maps, are shown in Fig. 4.
We Ô¨Ånd low conÔ¨Ådence correlates well with prediction er-
rors, which generally occurs at boundaries and occlusions.

KITTI. During Ô¨Åne-tuning stage, we leverage the available
394 image pairs from KITTI 2012 & 2015 as training data.
Training is performed for 2000 epochs, with batch size 16

and image crop size 320 √ó 896. The initial learning rate is
1 √ó 10‚àí5 and decayed by 0.5 at the 1000th and the 1500th
epoch.

As shown in Tab. 1, our method achieves the lowest per-
centages of disparity outliers in both non-occluded (Out-
Noc) and total regions (Out-All), background (D1-bg) and
foreground regions (D1-fg), among all of the competitive
baselines on both datasets. We also hold the lowest infer-
ence time for processing a standard KITTI stereo pair. Note
that we do not leverage the entire Scene Flow dataset [30]
for training as [8, 24], nor do we utilize additional seman-
tic or edge cues as in [39, 47]. Qualitative comparisons are

6049

Training

Test

Time

KITTI 2012

KITTI 2015

Methods Clean

Final

Clean Final

(s)

Methods AEPE AEPE F1-Noc AEPE F1-all F1-all

PatchBatch [11]
EpicFlow [34]
CPM-Ô¨Çow [18]
FullFlow [9]
FlowFields [2]
MRFlow [44]
FlowFieldsCNN [3]
DCFlow [45]
SpyNet-ft [33]
FlowNet2 [21]
FlowNet2-ft [21]
LiteFlowNet [19]
LiteFlowNet-ft [19]
PWC-Net [40]
PWC-Net-ft [40]

HD3F (Ours)
HD3F-ft (Ours)

-
-
-
-
-

-
-
-

3.60

-

1.83

3.59

-
-

(3.17)
2.02
(1.45)
2.52
(1.64)
2.55
(2.02)

3.84
(1.70)

-
-

(4.32)
3.14
(2.01)
4.05
(2.23)
3.93
(2.08)

8.77
(1.17)

5.79
4.12
3.56
2.71
3.75
2.53
3.78
3.54
6.64
3.96
4.16

-

6.78
6.29
5.96
5.90
5.81
5.38
5.36
5.12
8.36
6.02
5.74

-

4.86

6.09

-

-

4.39

5.04

-

-

4.79

4.67

50.0
15.0
4.30
240
28.0
480
23.0
8.60
0.16
0.12
0.12
0.09
0.09
0.03
0.03

0.08
0.08

train

test

test

train

train

test

-

-

3.3

3.8

EpicFlow [34]
FullFlow [9]
PatchBatch [11]
FlowFields [2]
DCFlow [45]
MirrorFlow [20]
PRSM [42]

7.88% -
-
5.29% -
-
-
4.38% -
2.46% -
SpyNet-ft [33] (4.13) 4.7 12.31% -
FlowNet2 [21] 4.09

-
-
-
-
-
-
-

2.6
1.0

-
-

-
-

-

-

-
-
-
-

26.29%
23.37%
21.07%
19.80%
15.09% 14.83%
9.93% 10.29%
6.68%
35.07%

-
-

10.06 30.37%

-

FlowNet2-ft [21] (1.28) 1.8
LiteFlowNet [19] 4.25
LiteFlowNet [19] (1.26) 1.7

-

PWC-Net [40] 4.14

-

4.82% (2.30) (8.61%) 10.41%

-
-
-

10.46 29.30%
(2.16) (8.16%) 10.24%
10.35 33.67%

-

-

PWC-Net-ft [40] (1.45) 1.7

4.22% (2.16) (9.80%) 9.60%

HD3F (Ours) 4.65

-
HD3F-ft (Ours) (0.81) 1.4

-

13.17 23.99%

-

2.26% (1.31) (4.10%) 6.55%

Table 2: Average EPE results on MPI Sintel dataset. ‚Äú-ft‚Äù means
Ô¨Ånetuning on the Sintel training set and numbers in the parenthesis
are results on data the method has been trained on.

Table 3: Optical Ô¨Çow results on KITTI dataset. ‚Äú-ft‚Äù means Ô¨Åne-
tuning on the KITTI training set. Numbers in parenthesis are re-
sults on data the network has been trained on.

shown in Fig. 6. Our method exhibits better performance
in regions with complex and ambiguous textures. This indi-
cates the effectiveness of hierarchical match density learn-
ing based on pyramid feature representations, which ex-
hibits robustness to local noise.

4.3. Optical Flow

We pretrain our HD3F on synthetic data from Fly-
ingChairs [10] and FlyingThings3D [21], then investigate
the effectiveness of our model on established optical Ô¨Çow
benchmarks including MPI Sintel [7] and KITTI [13].

FlyingChairs. We train our network on FlyingChairs with
batch size 64 and initial learning rate 4 √ó 10‚àí4. Images are
randomly resized and cropped to 384 √ó 512 patches. We
Ô¨Ånd larger crop size can improve the network performance.

FlyingThings3D. We further Ô¨Åne-tune the model on the
FlyingThings3D data, the same subset in our stereo match-
ing experiments, with batch size 32, learning rate 4 √ó 10‚àí5
and image crop size 384 √ó 832. We visualize examples of
multi-scale predictions in Fig. 5. The results indicate that
our model is able to progressively reÔ¨Åne the prediction from
coarse to Ô¨Åne scales. Though we adopt the discrete distri-
bution, our model can still capture very detailed displace-
ments.

MPI Sintel. Finally, we Ô¨Åne-tune our model on MPI Sin-
tel [7] for 1200 epochs with batch size 32 and image crop
size 384 √ó 768. The initial learning rate is 2 √ó 10‚àí5 and

C
W
P

s
r
u
O

C
W
P

s
r
u
O

C
W
P

s
r
u
O

Figure 7: Example Ô¨Çow error maps on KITTI 2015 test set. We
compare our method with PWC-Net [40]. Orange corresponds to
erroneous prediction. This Ô¨Ågure is best viewed in color.

decayed by 0.5 at the 600th and the 900th epoch. Though
the dataset provides training data of different subsets (clean
& Ô¨Ånal passes), we only adopt the Ô¨Ånal pass as training data
rendered with motion blur, defocus blur, and atmospheric
effects. As shown in Tab. 2, we can obtain the lowest av-
erage EPE in the Ô¨Ånal pass, and compelling results on the
clean pass, though our model does not see the clean pass
data during training.
In the model generalization experi-
ment, our pretrained HD3F estimates the Ô¨Çow accurately
near the occlusion boundary, resulting in the lowest out-

6050

Input Image

ConÔ¨Ådence Map

Error Map

Figure 8: Example conÔ¨Ådence maps of our predictions and error
maps w.r.t. ground-truth. In conÔ¨Ådence maps, white colors mean
conÔ¨Ådent predictions while dark colors denote uncertain ones. In
the error maps, warmer colors indicate inaccurate predictions.

lier percentage on KITTI (see the ‚ÄúHD3F (Ours)‚Äù entry in
Tab. 3). The metric of EPE emphasizes large motion error.
This inÔ¨Çuence makes our pretrained HD3F achieve higher
EPE on MPI Sintel (see the ‚ÄúHD3F (Ours)‚Äù entry in Tab. 2).

KITTI. Alternatively, we can Ô¨Ånetune our pretrained model
on KITTI dataset. We follow the conÔ¨Ågurations of our
stereo experiment. Tab. 3 summarize the results. We can
obtain the lowest F1-Noc on KITTI 2012 test set and the
lowest F1-all on KITTI 2015 test set. At the time of writ-
ing, HD3F outperforms all two-frame optical Ô¨Çow methods
by large margins on both KITTI 2012 & 2015.
It even
surpasses some competitive scene Ô¨Çow methods such as
PRSM [42], which use additional stereo data. This reveals
the suitability of our probabilistic method in challenging
real-world cases. We show qualitative comparisons against
PWC-Net in Fig. 7. Our method appear to have advantages
in estimating many thin structures.

4.4. Uncertainty Estimation

We also conduct quantitative analysis of uncertainty es-
timation. We compute the log likelihoods of our network
predictions and compare HD3F with probabilistic Ô¨Çow net-
works [12]. FlowNetDropOut uses variational Gaussian
dropout layers [26]. While FlowNetProbOut replaces deter-
ministic outputs with probabilistic output layers. FlowNe-
tADF propagates uncertainty through the entire network us-
ing ADF. During the evaluation, we recover the full match
density through composing the multi-scale match densi-
ties. This can be achieved through iteratively sampling from
coarse to Ô¨Åne, and we assume a discrete non-uniform distri-
bution for sampling outside W ‚àó
i (see Sec. 3.3). As shown
in Tab. 4, HD3F achieves the best average log likelihoods
against all of the baselines.

Furthermore, we measure the reliability of network pre-
diction based on uncertainty. We treat predictions with un-
certainty greater than a certain threshold (œÉ = 0.3) as out-

Methods

Sintel clean Sintel Ô¨Ånal Chairs

FlowNetDropOut [12]
FlowNetProbOut [12]
FlowNetADF [12]

HD3F(Ours)

-7.106
-6.888
-3.878

-1.487

-10.820
-7.621
-4.186

-6.176
-3.591
-3.348

-1.821

-0.872

Table 4: Average log likelihoods of probabilistic Ô¨Çow methods on
MPI Sintel training set and FlyingChairs test set.

Classes

Methods

Outlier

Inlier

Mean

Consistency

Ours

Consistency

Ours

Consistency

Ours

Noc

All

IoU

17.5
37.6

84.2
96.1

50.9
66.9

Acc

64.9
57.8

85.8
97.8

75.4
77.8

IoU

23.3
44.1

75.6
91.8

49.5
68.0

Acc

81.9
76.4

76.9
93.7

79.4
85.1

Table 5: ClassiÔ¨Åcation result of inlier and outlier predictions on
KITTI 2015 training set. Noc denotes evaluation only in the non-
occluded area, while All denotes evaluation in the overall region.

liers and compare such criterion with the forward-backward
consistency check [48] which is popularly adopted for point
estimate. Both methods use the same HD3F model for in-
ference. As shown in Tab. 5, our uncertainty estimation
gives the highest mean IoU and mean accuracy in both non-
occluded and overall regions. Fig. 8 presents visualization
of the conÔ¨Ådence and error maps. We can observe the posi-
tive correlation between our estimated uncertainty and pre-
diction error.

5. Conclusion

We proposed Hierarchical Discrete Distribution Decom-
position (HD3) for estimating the match density. Our ap-
proach decomposed the match density into multiple scales
and learned the decomposed match densities in an end-to-
end manner. The predicted match densities can be converted
into point estimates, while providing model-inherent uncer-
tainty measures at the same time. Our experiments demon-
strated the advantages of our method on established bench-
marks.

In the future, we hope to integrate more information into
our framework such as the pixel assignment probabilities
from segmentation. Currently, we do not consider relation-
ships between match densities of adjacent pixels, but this
may help remove match uncertainty in challenging cases.

Acknowledgments This work was supported by Berkeley
AI Research and Berkeley DeepDrive.

6051

References

[1] P. Anandan. A computational framework and an algorithm

for the measurement of visual motion. IJCV, 1989. 3

[2] C. Bailer, B. Taetz, and D. Stricker. Flow Ô¨Åelds: Dense corre-
spondence Ô¨Åelds for highly accurate large displacement op-
tical Ô¨Çow estimation. In ICCV, 2015. 3, 7

[3] C. Bailer, K. Varanasi, and D. Stricker. Cnn-based patch
matching for optical Ô¨Çow with thresholded hinge embedding
loss. In CVPR, 2017. 7

[4] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and
R. Szeliski. A database and evaluation methodology for op-
tical Ô¨Çow. IJCV, 2011. 2

[5] J. L. Barron, D. J. Fleet, and S. S. Beauchemin. Performance

of optical Ô¨Çow techniques. IJCV, 1994. 2

[6] A. Behl, O. Hosseini Jafari, S. Karthik Mustikovela,
H. Abu Alhaija, C. Rother, and A. Geiger. Bounding boxes,
segmentations and object coordinates: How important is
recognition for 3d scene Ô¨Çow estimation in autonomous driv-
ing scenarios? In ICCV, 2017. 1

[7] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical Ô¨Çow evaluation.
In ECCV, 2012. 2, 7

[8] J.-R. Chang and Y.-S. Chen. Pyramid stereo matching net-

work. In CVPR, 2018. 6

[9] Q. Chen and V. Koltun. Full Ô¨Çow: Optical Ô¨Çow estimation
by global optimization over regular grids. In CVPR, 2016. 7
[10] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.
FlowNet: Learning optical Ô¨Çow with convolutional net-
works. In ICCV, 2015. 1, 2, 4, 7

[11] D. Gadot and L. Wolf. Patchbatch: a batch augmented loss

for optical Ô¨Çow. In CVPR, 2016. 7

[12] J. Gast and S. Roth. Lightweight probabilistic deep net-

works. In CVPR, 2018. 1, 2, 8

[13] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets

robotics: The kitti dataset. IJRR, 2013. 1, 2, 5, 7

[14] F. Guney and A. Geiger. Displets: Resolving stereo ambigu-

ities using object knowledge. In CVPR, 2015. 6

[15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in

deep residual networks. In ECCV, 2016. 5

[16] H. Hirschmuller and D. Scharstein. Evaluation of stereo
matching costs on images with radiometric differences.
PAMI, 2009. 2

[17] B. K. Horn and B. G. Schunck. Determining optical Ô¨Çow.

ArtiÔ¨Åcial intelligence, 1981. 1, 2

[18] Y. Hu, R. Song, and Y. Li. EfÔ¨Åcient coarse-to-Ô¨Åne patch-
match for large displacement optical Ô¨Çow. In CVPR, 2016.
3, 7

[19] T.-W. Hui, X. Tang, and C. C. Loy. LiteFlowNet: A
lightweight convolutional neural network for optical Ô¨Çow es-
timation. In CVPR, 2018. 2, 3, 7

[20] J. Hur and S. Roth. MirrorÔ¨Çow: Exploiting symmetries in
joint optical Ô¨Çow and occlusion estimation. In ICCV, 2017.
1, 2, 7

[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 5

[23] H. Jiang, D. Sun, V. Jampani, M.-H. Yang, E. Learned-
Miller, and J. Kautz. Super slomo: High quality estimation
of multiple intermediate frames for video interpolation. In
CVPR, 2018. 1

[24] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry,
R. Kennedy, A. Bachrach, and A. Bry. End-to-end learn-
ing of geometry and context for deep stereo regression. In
ICCV, 2017. 2, 6

[25] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, 2014. 5

[26] D. P. Kingma, T. Salimans, and M. Welling. Variational
In NIPS,

dropout and the local reparameterization trick.
2015. 8

[27] C. Kondermann, R. Mester, and C. Garbe. A statistical con-

Ô¨Ådence measure for optical Ô¨Çows. In ECCV, 2008. 1, 2

[28] S. Z. Li. Markov random Ô¨Åeld models in computer vision. In

ECCV, 1994. 2

[29] O. Mac Aodha, A. Humayun, M. Pollefeys, and G. J. Bros-
tow. Learning a conÔ¨Ådence measure for optical Ô¨Çow. PAMI,
2013. 1, 2

[30] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers,
A. Dosovitskiy, and T. Brox. A large dataset to train convo-
lutional networks for disparity, optical Ô¨Çow, and scene Ô¨Çow
estimation. In CVPR, 2016. 2, 5, 6

[31] S. Meister, J. Hur, and S. Roth. UnFlow: Unsupervised
learning of optical Ô¨Çow with a bidirectional census loss. In
AAAI, 2018. 5

[32] M. Menze and A. Geiger. Object scene Ô¨Çow for autonomous

vehicles. In CVPR, 2015. 2

[33] A. Ranjan and M. J. Black. Optical Ô¨Çow estimation using a

spatial pyramid network. In CVPR, 2017. 3, 7

[34] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicÔ¨Çow: Edge-preserving interpolation of correspondences
for optical Ô¨Çow. In CVPR, 2015. 2, 7

[35] A. Seki and M. Pollefeys. Sgm-nets: Semi-global matching

with neural networks. In CVPR, 2017. 6

[36] L. Sevilla-Lara, Y. Liao, F. G¬®uney, V. Jampani, A. Geiger,
and M. J. Black. On the integration of optical Ô¨Çow and action
recognition. In GCPR, 2018. 1

[37] A. Shaked and L. Wolf. Improved stereo matching with con-
stant highway networks and reÔ¨Çective conÔ¨Ådence learning.
In CVPR, 2017. 6

[38] E. P. Simoncelli, E. H. Adelson, and D. J. Heeger. Probability

distributions of optical Ô¨Çow. In CVPR, 1991. 3

[39] X. Song, X. Zhao, H. Hu, and L. Fang. Edgestereo: A con-
text integrated residual pyramid network for stereo matching.
In ACCV, 2018. 6

[40] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. PWC-Net: CNNs
for optical Ô¨Çow using pyramid, warping, and cost volume. In
CVPR, 2018. 1, 2, 3, 7

[21] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet 2.0: Evolution of optical Ô¨Çow estimation
with deep networks. In CVPR, 2017. 1, 2, 6, 7

[41] S. Tulyakov, A. Ivanov, and F. Fleuret. Practical deep stereo
(pds): Toward applications-friendly deep stereo matching. In
NeurIPS, 2018. 6

6052

[42] C. Vogel, K. Schindler, and S. Roth. 3d scene Ô¨Çow estimation

with a piecewise rigid scene model. IJCV, 2015. 1, 7, 8

[43] A. S. Wannenwetsch, M. Keuper, and S. Roth. ProbÔ¨Çow:
Joint optical Ô¨Çow and uncertainty estimation. In ICCV, 2017.
1, 3

[47] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia. Segstereo:
Exploiting semantic information for disparity estimation. In
ECCV, 2018. 6

[48] Z. Yin and J. Shi. GeoNet: Unsupervised Learning of Dense

Depth, Optical Flow and Camera Pose. In CVPR, 2018. 8

[44] J. Wulff, L. Sevilla-Lara, and M. J. Black. Optical Ô¨Çow in

[49] F. Yu and V. Koltun. Multi-scale context aggregation by di-

mostly rigid scenes. In CVPR, 2017. 7

lated convolutions. In ICLR, 2015. 5

[45] J. Xu, R. Ranftl, and V. Koltun. Accurate optical Ô¨Çow via

[50] F. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer

direct cost volume processing. CoRR, 2017. 7

aggregation. In CVPR, 2018. 1, 3, 4, 5

[46] K. Yamaguchi, D. McAllester, and R. Urtasun. EfÔ¨Åcient joint
segmentation, occlusion labeling, stereo and Ô¨Çow estimation.
In ECCV, 2014. 2, 6

[51] J. Zbontar and Y. LeCun. Stereo matching by training a con-
volutional neural network to compare image patches. JMLR,
2016. 6

6053

