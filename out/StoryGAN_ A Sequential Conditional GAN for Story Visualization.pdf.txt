StoryGAN: A Sequential Conditional GAN for Story Visualization

Yitong Li∗1, Zhe Gan2, Yelong Shen4, Jingjing Liu2, Yu Cheng2, Yuexin Wu5,

Lawrence Carin1, David Carlson1 and Jianfeng Gao3

1Duke University, 2Microsoft Dynamics 365 AI Research, 3Microsoft Research

4Tencent AI Research, 5Carnegie Mellon University

Abstract

In this work, we propose a new task called Story Vi-
sualization. Given a multi-sentence paragraph, the story
is visualized by generating a sequence of images, one for
each sentence.
In contrast to video generation, story vi-
sualization focuses less on the continuity in generated im-
ages (frames), but more on the global consistency across dy-
namic scenes and characters – a challenge that has not been
addressed by any single-image or video generation meth-
ods. Therefore, we propose a new story-to-image-sequence
generation model, StoryGAN, based on the sequential con-
ditional GAN framework. Our model is unique in that it
consists of a deep Context Encoder that dynamically tracks
the story ﬂow, and two discriminators at the story and im-
age levels, to enhance the image quality and the consistency
of the generated sequences. To evaluate the model, we mod-
iﬁed existing datasets to create the CLEVR-SV and Pororo-
SV datasets. Empirically, StoryGAN outperformed state-
of-the-art models in image quality, contextual consistency
metrics, and human evaluation.

Figure 1: The input story is “Pororo and Crong are ﬁshing to-
gether. Crong is looking at the bucket. Pororo has a ﬁsh on his
ﬁshing rod.” Each sentence is visualized with one image. In this
work, the image generation for each sentence is enriched with con-
textual information from the Context Encoder. Two discriminators
at different levels guide the generation process.

1. Introduction

Learning to generate meaningful and coherent sequences
of images from a natural language story is a challenging
task that requires understanding and reasoning on both nat-
ural language and images. In this work, we propose a new
Story Visualization task. Speciﬁcally, the goal is to generate
a sequence of images to describe a story written in a multi-
sentence paragraph, as shown in Figure 1.

There are two main challenges in this task. First, the se-
quence of images must consistently and coherently depict
the whole story. This task is highly related to text-to-image
generation [35, 28, 17, 36, 34], where an image is generated

∗This work was done while the ﬁrst author was an intern at Microsoft

Dynamics 365 AI Research.

based on a short description. However, by sequentially ap-
plying text-to-image methods to a story will not generate a
coherent image sequence, failing on the story visualization
task. For instance, consider the story “A red metallic cylin-
der cube is at the center. Then add a green rubber cube at
the right.” The second sentence alone does not capture the
entire scene.

The second challenge is how to display the logic of the
storyline. Speciﬁcally, the appearance of objects and the
layout in the background must evolve in a coherent way
as the story progresses. This is similar to video genera-
tion. However, story visualization and video generation dif-
fer as: (i) Video clips are continuous with smooth motion
transitions, so video generation models focus on extract-
ing dynamic features to maintain realistic motions [32, 31].

16329

Pororoand Crongare fishing together. Crongis looking at the bucket. Pororohas a fish on his fishing rod.Real/Fake? Coherent to text? Consistent?“PororoandCrongare fishing together. Crongis looking at the bucket. Pororohas a fish on hisfishing rod.”“PororoandCrongare fishing together. Crongislooking at thebucket. Pororohas a fish on his fishing rod.”“PororoandCrongare fishing together. Crongis looking at the bucket. Pororohas a fish on his fishing rod.”Image GeneratorImage GeneratorImage Generator+++In contrast, the goal of story visualization is to generate
a sequence of key static frames that present correct story
plots where motion features are less important. (ii) Video
clips are often based on simple sentence input and typically
have a static background, while complex stories require the
model to capture scene changes necessary for the plot line.
In that sense, story visualization could also be viewed as
a critical step towards real-world long-video generation by
capturing sharp scene changes. To tackle these challenges,
we propose a StoryGAN framework, inspired by Genera-
tive Adversarial Networks (GANs) [10], a two-player game
between a generator and a discriminator. To take into ac-
count the contextual information in the sequence of sen-
tence inputs, StoryGAN is designed as a sequential condi-
tional GAN model.

Given a multi-sentence paragraph (story), StoryGAN
uses a recurrent neural network (RNN) to incorporate the
previously generated images into the current sentence’s im-
age generation. Contextual information is extracted with
our Context Encoder module, including a stack of a GRU
cell and our newly proposed Text2Gist cell. The Context
Encoder transforms the current sentence and a story encod-
ing vector into a high-dimensional feature vector (Gist) for
further image generation. As the story proceeds, the Gist
is dynamically updated to reﬂect the change of objects and
scenes in the story ﬂow. In the Text2Gist component, the
sentence description is transformed into a ﬁlter and adapted
to the story, so that we can optimize the mixing process by
tweaking the ﬁlter. Similar ideas are also used in dynamic
ﬁltering [18], attention models [34] and meta-learning [27].
To ensure consistency across the sequence of generated
images, we adopt a two-level GAN framework. We use an
image-level discriminator to measure the relevance of a sen-
tence and its generated image, and a story-level discrimina-
tor to measure the global coherence between the generated
image sequence and the whole story.

We created two datasets from the existing CLEVR [19]
and Pororo [21] datasets for our story visualization task,
called CLEVR-SV and Pororo-SV, respectively. Empiri-
cally, StoryGAN more efﬁciently captures the full picture
of the story and how it evolves, compared to existing base-
lines [36, 24]. Equipped with the deep Context Encoder
module and the two-level discriminators, StoryGAN signif-
icantly outperforms previous state-of-the-art models, gener-
ating a sequence of higher quality images that are coherent
with the story in both image quality and global consistency
metrics, as well as human evaluation.

2. Related Work

Variational AutoEncoders (VAEs) [23], Generative Ad-
versarial Nets (GANs) [10], and ﬂow-based generative
models [7, 8]) have been widely applied to a wide range of
generation tasks including text-to-image generation, video

generation, style transfer, and image editing. Story visu-
alization falls into this broad categorization of generative
tasks, but has several distinct aspects.

Very relevant for the story visualization task is condi-
tional text-to-image transformation [28, 17, 38, 35], which
can now generate high-resolution realistic images [36, 34].
A key task in text-to-image generation is understanding
longer and more complex input texts. For example, this
has been explored in dialogue-to-image generation, where
the input is a complete dialogue session rather than a sin-
gle sentence [29]. Another related task is textual image
editing, which edits an input image according to a textual
editing query [3, 30, 4, 9]. This task requires consistency
between the original image and the output image. Finally,
there is the task of placing pre-speciﬁed images and objects
in a picture from a text description [20]. This task also re-
lates text to a consistent image, but does not require a full
image generation procedure.

A second closely related task to story visualization is
video generation, especially that of text-to-video [24, 13]
or image-to-video generation [1, 31, 32]. Existing ap-
proaches only generate short video clips [13, 5, 12] without
scene changes. The biggest challenge in video generation
is how to ensure a smooth motion transition across succes-
sive video frames. Trajectory, skeleton or simple landmarks
are used in existing works to help model the motion fea-
ture [12, 37, 33]. To this end, researchers disentangle dy-
namic and static features for motion and background, re-
spectively [32, 24, 31, 6]. In our modeling of story visu-
alization, the whole story sets the static features and each
input sentence encodes dynamic features. However, there
are several differences: (i) conditional video generation has
only one input, while our task has sequential, evolving in-
puts; and (ii) the motion in video clips is continuous, while
images visualizing a story are discrete and often with dif-
ferent scene views.

There are also several other related tasks in the litera-
ture. For instance, story image retrieval from a pre-collected
training set rather than image generation [26]. Cartoon
generation has been explored with a “cut and paste” tech-
nique [11]. However, both of these techniques require large
amounts of labeled training data. An inverse task to story vi-
sualization is visual storytelling, where the output is a para-
graph describing a sequence of input images. Text genera-
tion models or reinforcement learning are often highlighted
for visual storytelling [16, 25, 15].

3. StoryGAN

StoryGAN is designed to create a sequence of images
to describe an input story S. The story S consists of a
sequence of sentences S = [s1, s2, · · · , sT ], where the
length T may vary. There is one generated image per sen-
tence, denoted as ˆX = [ ˆx1, ˆx2, · · · , ˆxT ], that are both

26330

Image Discriminator

Generated Sequence of 
Images

Story Encoder

𝒉0

…

𝑺Full Story
𝐾𝐿 𝒩𝝁(𝑺),diag(𝝈𝟐𝐒)||𝒩𝟎,𝑰

Image 

ො𝑥1
𝒐1
GRU𝒊1
𝒔𝟏 & 𝝐𝟏

Image 

ො𝑥2
𝒐2
GRU𝒊2
𝒔𝟐 & 𝝐𝟐

𝒉1
𝒈1

Image 

ො𝑥3
𝒐3
GRU𝒊3
𝒔𝟑 & 𝝐𝟑

𝒉2
𝒈2

Story Discriminator

…
…
…

…

𝒉𝑇−1
𝒈𝑇

Generator

Image 

ො𝑥𝑇
𝒐𝑇
GRU𝒊𝑇
𝒔𝑻 & 𝝐𝑻

Text2Gist

Generator

Generator

Generator

Text2Gist

Text2Gist

Text2Gist

Figure 2: The framework of StoryGAN. The variables in gray solid circles are the input story S and individual sentences s1, . . . , sT
with random noise ✏1, . . . , ✏T . The generator network contains the Story Encoder, Context Encoder and image generator. The proposed
component Text2Gist is introduced in detail in Section 3.2. There are two discriminators on top, which discriminate whether each image-
sentence pair and each image-sequence-story pair are real or fake.

locally (sentence-image) and globally (story-images) con-
sistent. For training, ground truth images are denoted as
X = [x1, x2, · · · , xT ]. The image sequence is locally
consistent if each image matches its corresponding sentence
semantically. The image sequence is globally consistent if
all the images globally hold together as coherent to the full
story S it visualizes. In our approach, each sentence in story
S has been encoded into an embedding vector using a pre-
trained sentence encoder [2]. With slight abuse of notation,
each sentence is an encoded via vector st 2 R128. In the
following, we assume st and S are both encoded feature
vectors instead of raw text.

The overall architecture of StoryGAN is presented in
Figure 2. It is implemented as a sequential GAN model,
which consists of (i) a Story Encoder that encodes S into a
low-dimensional vector h0; (ii) a two-layer recurrent neural
network (RNN) based Context Encoder that encodes input
sentence st and its contextual information into a vector ot
(Gist) for each time point t; (iii) an image generator that
generates image ˆxt based on ot for each time step t; and
(iv) an image discriminator and a story discriminator that
guide the image generation process so as to ensure the gen-
erated image sequence ˆX is locally and globally consistent,
respectively.

3.1. Story Encoder

The Story Encoder is given in the dotted pink box of
Figure 2. Following the conditioning mechanism in Stack-
GAN [36], the Story Encoder E(·) learns a stochastic map-
ping from story S to an low-dimensional embedding vector
h0. h0 encodes the whole story and it serves as the initial
state of the hidden cell of the Context Encoder. Speciﬁcally,
the Story Encoder samples an embedding vector h0 from

a normal distribution h0 ⇠ E(S) = N (µ(S), Σ(S)),
with µ(·) and Σ(·) implemented as two neural networks.
In this work, we restrict Σ(S) = diag( 2(S)) to a diag-
onal matrix for computational tractability. With the repa-
rameterization trick, the encoded story h0 can be written as
h0 = µ(S)+ 2(S)
2  ✏S, where ✏S ⇠ N (0, I).   repre-
sents elementwise multiplication, and the square root is also
taken elementwise. µ(S) and  2(S) are parameterized as
Multi-Layer Perceptrons (MLPs) with a single hidden layer.
Convolutional networks could also be used depending on
the structure of S. The sampled h0 is provided to the RNN-
based Context Encoder as the initial state vector.

1

By using stochastic sampling, the Story Encoder deals
with the discontinuity problem in the original story space,
thus not only leading to a compact, semantic representa-
tion of S for story visualization, but also adding random-
ness to the generation process. The encoder’s parameters
are optimized jointly with the other modules of StoryGAN
via back propagation. Therefore, to enforce the smoothness
over the conditional manifold in latent semantic space and
avoid collapsing to a single generative point rather than a
distribution, we add the regularization term [36],

LKL = KL N  µ(S), diag( 2(S))  ||N (0, I)  ,

(1)

which is the Kullback-Leibler (KL) divergence between the
learned distribution and the standard Gaussian distribution.

3.2. Context Encoder

Video generation is closely related to story visualization,
and it typically assumes a static background with smooth
motion transitions, requiring a disjoint embedding of static
and dynamic features [32, 16, 31]. In story visualization, the

36331

challenge differs in that the characters, motion, and back-
ground often change from image to image, as illustrated in
Figure 1. This requires us to address two problems: (i) how
to update the contextual information to effectively capture
background changes; and (ii) how to combine new inputs
and random noise when generating each image to visualize
the change of characters, which may shift dramatically.

We address these issues by proposing a deep RNN based
Context Encoder to capture contextual information during
sequential image generation, shown in the red box in Fig-
ure 2. The context can be deﬁned as any related information
in the story that is useful for the current generation. The
deep RNN consists of two hidden layers. The lower layer is
implemented using standard GRU cells and the upper layer
using the proposed Text2Gist cells, which are a variant to
GRU cells and are detailed below. At time step t, the GRU
layer takes as input the concatenation of the sentence st and
isometric Gaussian noise ✏t, and outputs the vector it. The
Text2Gist cell combines the GRU’s output it with the story
context ht (initialized by Story Encoder) to generate ot that
encodes all necessary information for generating an image
at time t. ht is updated by the Text2Gist cell to reﬂect the
change of potential context information.

Let gt and ht denote the hidden vectors of the GRU and
Text2Gist cells, respectively. The Context Encoder works
in two steps to generate its output:

it, gt = GRU(st, ✏t, gt−1),
ot, ht = Text2Gist(it, ht−1).

(2)

(3)

We call ot the “Gist” vector since it combines all the global
and local context information, from ht−1 and it respec-
tively, at time step t (i.e.
it captures the “gist” of the in-
formation). The Story Encoder initializes h0, while g0 is
randomly sampled from an isometric Gaussian distribution.
Next, we give the underlying updates of Text2Gist.
Given ht−1 and it at time step t, Text2Gist generates a hid-
den vector ht and an output vector ot as follows:

zt = σz (Wzit + Utht−1 + bz) ,
rt = σr (Writ + Urht−1 + br) ,
ht = (1   zt)   ht−1

+ zt   σh (Whit + Uh(rt   ht−1) + bh) ,

ot = Filter(it) ⇤ ht,

(4)

(5)

(6)

(7)

where zt and rt are the outputs from the update and reset
gates, respectively. The update gate decides how much in-
formation from the previous step should be kept, and the re-
set gate determines what to forget from ht−1. σz, σr and σh
are sigmoid non-linearity functions. In contrast to standard
GRU cells, output ot is the convolution between Filter(it)
and ht. The ﬁlter it is learned to adapt to ht. Speciﬁcally,
Filter(·) transforms vector it to a multi-channel ﬁlter of size

Cout ⇥ 1 ⇥ 1 ⇥ len(ht) using a neural network, where Cout
is the number of output channels. Since ht is a vector, this
ﬁlter is used as a 1D ﬁlter as in a standard convolutional
layer.

The convolution operator in Eq. (7) infuses the global
contextual information from ht and local information from
it. ot is the output of the Text2Gist cell at time step t. Since
it encodes information from st and ht from S, which re-
ﬂects the whole picture of the story, the convolutional op-
eration in Eq. (7) can be seen as helping st to pick out the
important part from the story in the process of generation.
Empirically, we ﬁnd that Text2Gist is more effective than
traditional RNNs for story visualization.

3.3. Discriminators

StoryGAN uses two discriminators, an image and a story
discriminator, to ensure the local and global consistency of
the story visualization, respectively. The image discrim-
inator measures whether the generated image ˆxt matches
the sentence st given its initial context information en-
coded in h0. It does this by comparing the generated triplet
{st, h0, ˆxt} to the real triplet {st, h0, xt}. In contrast to
prior work on text-to-image generation [36, 28], the same
sentence can have a signiﬁcantly different generated image
depending on the context, so it is important to give the en-
coded context information to the discriminator as well. For
example, consider the example given in Section 1, “A red
metallic cylinder cube is at the center. Then add a green
rubber cube at the right of it.” The second image will vary
wildly without the context (i.e. the ﬁrst sentence).

Figure 3: Structure of the story discriminator. The feature vectors
of the images/sentences in the story are concatenated.   means el-
ementwise product. The product of image and text features are in-
put to a fully connected layer with sigmoid non-linearity to predict
whether it is a fake or real story pair.

The story discriminator helps enforce the global consis-
tency of the generated image sequence given story S.
It
differs from the discriminators used for video generation,
which often use 3D convolution [32, 31, 24] to smooth

46332

“Pororoand Crongfishing together.” Text Encoder“Crongis looking at the bucket.” “Pororohas a fish on his fishing rod.”⨀Real / Fake?Text EncoderText Encoderthe changes between frames. The overall architecture of
the story discriminator is illustrated in Figure 3. The left
part is an image encoder, which encodes an image se-
quence into a sequence of feature vectors Eimg(X) =
[Eimg(x1), · · · , Eimg(xT )], where X are either real or
generated images (which are denoted by ˆX). These vectors
are concatenated into a single vector, shown as the blue rect-
angular in Fig. 3. Similarly, the right part is a text encoder,
which encodes the multi-sentence story S into a sequence
of feature vectors Etxt(S) = [Etxt(s1), · · · , Etxt(sT )].
Likewise, these are concatenated into one big vector, shown
as the red rectangle in Fig. 3. The image encoder is im-
plemented as a deep convolutional network and the text en-
coder as a multi-layer perceptron. Both output a same di-
mensional vector.

The global consistency score is computed as

DS = σ (w| (Eimg(X)   Etxt(S)) + b) ,

(8)

where   is element-wise product. The weights w and bias b
are learned in the output layer. σ is a sigmoid function that
normalizes the score to a value in [0, 1]. By pairing each
sentence and image, the story discriminator can consider
both local matching and global consistency jointly.

Both image and story discriminators are trained on posi-
tive and negative pairs. The latter are generated by replacing
the image (sequence) in the positive pairs with generated
ones.

3.4. Algorithm Outlines

Let ✓,  I , and  S denote the parameters of the whole
generator G(·; ✓), the image discriminator, and the story
discriminator, respectively. The objective function for Sto-
ryGAN is

min

✓

max
 I , S

αLImage + βLStory + LKL,

(9)

where α and β balance the three loss terms. LKL is the reg-
ularization term of the Story Encoder deﬁned in (1). LImage
and LStory are deﬁned as
LImage = PT

t=1(E(xt,st) [log DI (xt, st, h0;  I )]

+ E(✏t,st) [log(1   DI (G(✏t, st; ✓), st, h0;  I ))])
(10)

LStory = E(X,S) [log DS(X, S;  S)]

+ E(✏,S) hlog(1   DS([G(✏t, st; ✓)]T

t=1), S;  S))i .
(11)

DI (·;  I ) and DS(·;  S) are the image and story discrimi-
nator, parameterized by  I and  S, respectively.

The pseudo-code for training StoryGAN is given in Al-
gorithm 1. The parameters of the story and image discrim-
inators,  I and  S, are updated in two separate for loops,

respectively, while the parameters of the image generator ✓
are updated in both loops. The initial hidden state of the
Text2Gist layer is the encoded story feature vector h0 pro-
duced by the Story Encoder. The detailed conﬁguration of
the network is provided in Appendix A.

Algorithm 1 Training Procedure of StoryGAN

sentence
and

Input:
Encoded
[sn1, sn2, · · · , snT ]
Xn = [xn1, · · · , xnT ] for n = 1, · · · , N .
Output: Generator parameters ✓ and discriminator pa-
rameters  I and  S.

corresponding

vectors Sn

=
images

for iter = 1 to max iter do

for iterI = 1 to kI do

story-sentence pairs

Sample a mini-batch of
{(st, S, xt)} from the training set.
Compute h0 as the initialization of the Text2Gist
layer and the KL regularization term as Eq. (1).
Generate a single output image ˆx.
Update  I and ✓.

end for
for iterS = 1 to kS do

Sample a mini-batch of story-image pair {(S, X)}
from training set.
Compute h0 and update ht at each time step t
Generate image sequence ˆX.
Update  S and ✓.

end for

end for

In our experiments, we use Adam [22] for parameter up-
dates1. We also ﬁnd that using different mini-batch sizes
for image and story discriminators may accelerate training
convergence, and that it is beneﬁcial to update generator and
discriminator in different time steps in one epoch.

4. Experiment

In this section, we evaluate the StoryGAN model on one
toy and one cartoon dataset. To the best of our knowledge,
there is no existing work on our proposed story visualization
task. The closest alternative for story visualization is con-
ditional video generation [24], where the story is treated as
single input and a video is generated in lieu of the sequence
of images. However, we empirically found that the video
generation result is too blurry and not comparable to Story-
GAN. Thus, our comparisons are mainly to ablated versions
of our proposed model. For a fair comparison, all models
use the same structure of the image generator, Context En-
coder and discriminators when applicable. The compared

is

available

at https://github.com/yitong91/

1Code

StoryGAN

56333

baseline models are:

ImageGAN: ImageGAN follows the work in [28, 36]
and does not use the story discriminator, story encoder and
Context Encoder. Each image is generated independently.
However, for a reasonable comparison, we concatenate st,
encoded story S and a noise term as input. Otherwise, the
model fails on the task. This is the simplest version of Sto-
ryGAN.

SVC: In “Story Visualization by Concatenation” (SVC),
the Text2Gist cell in StoryGAN is replaced by simple con-
catenation of the encoded story and description feature vec-
tors [31]. Compared to ImageGAN, SVC includes the addi-
tional story discriminator, and is visualized in Figure 4.

Figure 4: The framework of the baseline model SVC, where the
story and individual sentence are concatenated to form the input.

SVFN: In “Story Visualization by Filter Network”
(SVFN), the concatenation in SVC is replaced by a ﬁlter
network. Sentence st is transformed into a ﬁlter and con-
volved with the encoded story. Speciﬁcally, the image gen-
erator input is ot = Filter(it) ⇤ h0 instead of Eq. 7.

4.1. CLEVR-SV Dataset

The CLEVR [19] dataset was originally used for visual
question answering. We modiﬁed this data for story visual-
ization by generating images from randomly assigned lay-
outs of the object (examples in the top row of Figure 5). We
named this dataset CLEVR-SV to distinguish it from the
existing CLEVR dataset. Speciﬁcally, four rules were used
to construct the CLEVR-SV: (i) The maximum number of
objects in one story is limited to four. (ii) Objects are made
of metallic/rubber with eight different colors and two dif-
ferent sizes. (iii) The object shape can be cylinder, cube or
sphere. (iv) The object is added one at a time, resulting in a
four-image sequence per story. We generated 10, 000 image
sequences for training and 3, 000 for testing. For our task,
the story is the layout descriptions of objects.

The input st is the current object’s attribute and the rel-
ative position given by two real numbers indicating its co-
ordinates. For instance, the ﬁrst image of the left column
of Fig. 5 is generated from “yellow, large, metal, sphere, (-
2.1, 2.4).” The following objects are described in the same
way. Given the description, the generated objects’ appear-
ance should have little variation from the ground truth and

their relative positions should be similar.

Figure 5 gives the results comparison. ImageGAN [28]
fails to keep the consistency of the ‘story’ and it mixes up
the attributes when the number of objects increases. SVC
solves this consistency problem by including the story dis-
criminator and GRU cell at the bottom, as the third row
of Figure 5 has consistent objects in the image sequence.
However, SVC generates an implausible forth image in the
sequence. We hypothesize that using simple vector concate-
nation cannot effectively balance the importance of the cur-
rent description with the whole story. SVFN can alleviate
this problem to some extent, but not completely. In con-
trast, StoryGAN generates more feasible images than the
competitors. We attribute the performance improvement to
three components: (i) Text2Gist cell tracks the progress of
story; (ii) story and image discriminators keep the consis-
tency of objects in the generation process; (iii) using the
Story Encoder to initialize the Text2Gist cell gives better
result on ﬁrst generated image. Greater empirical evidence
for this ﬁnal point appears in the cartoon dataset in Sec-
tion 4.2.

In order to further validate the StoryGAN model, we de-
signed a task to evaluate whether the model can generate
consistent images by changing the ﬁrst sentence descrip-
tion. Speciﬁcally, we randomly replaced the ﬁrst object’s
description while keeping the other three the same during
generation, which we visualize in Supplemental Figure 8
in Appendix B. This comparison shows that only Story-
GAN can keep the story consistency by correctly utilizing
the attributes of the ﬁrst object in later frames, as discussed
above. In Supplemental Figure 9, we give additional exam-
ples on changing the initial attributes only using StoryGAN.
Regardless of the initial attribute, StoryGAN is consistent
between frames.

ImageGAN [28]

SSIM

0.596

SVC
0.641

SVFN StoryGAN
0.654

0.672

Table 1: SSIM comparison on CLEVR-SV dataset.

We also compare the Structural Similarity Index (SSIM)
score between the generated images and ground truth [14].
SSIM was originally used to measure the recovery result
from distorted images. Here, it is used to determine whether
the generated images are aligned with the input description.
Table 1 gives the SSIM metric for each method on the test
set. Note that though this is a generative task, using SSIM to
measure the structure similarity is reasonable because there
is little variation given the description. In this task, Story-
GAN signiﬁcantly outperforms the other baselines.

4.2. Cartoon Dataset

The Pororo dataset [21] was originally used for video
question answering, where each one second video clip is as-

66334

𝒔𝟏& 𝝐𝟏𝑥1𝑥2𝑥𝑇…Frame DiscriminatorStory DiscriminatorGenerated Sequence of Images𝑺Full StoryGRU𝒔𝟐& 𝝐𝟐GRU𝒔𝑻& 𝝐𝑻GRU……Image GeneratorImage GeneratorImage GeneratorStory Encoder…𝐾𝐿𝒩𝝁(𝑺),diag(𝝈𝟐𝐒)||𝒩𝟎,𝑰Ground Truth

ImageGAN

SVC

SVFN

StoryGAN

Figure 5: Comparison among different methods on CLEVR-SV dataset.

Figure 6: Two generated samples on the Pororo-SV dataset.

sociated with more than one manually written description.
About 40 video clips forms a complete story. Each story has
several QA pairs. In total, the Pororo dataset contains 16K
clips of one second videos about 13 distinct characters. The
manually written description has an average length of 13.6
words that describes what is happening and which charac-
ters are in each video clip. These 16K video clips are sorted
into 408 movie stories [21].

We modiﬁed the Pororo dataset to ﬁt story visualization
task by considering the description for each video clip as the
story’s text input. For each video clip, we randomly pick
out one frame (sampling rate is 30Hz) during training as
the real image sample. Five continuous images form a sin-
gle story. Finally, we end up with 15, 336 description-story
pairs, where 13, 000 pairs are used as training, the remain-
ing 2, 336 pairs for testing. We call this dataset Pororo-SV
to differ it from the original Pororo QA dataset [21].

The text encoder uses universal encoding [2] with ﬁxed
pre-trained parameters. Training a new text encoder empir-
ically gave little performance gain. Two visualized stories

from the competing methods are given in Figure 6. The text
input is given on the top. ImageGAN does not generate con-
sistent image sequences; for instance, the generated images
switch from indoors to outdoors randomly. Additionally,
the characters’ appearance is inconsistent in the sequence of
images (e.g. Pororo’s hat). SVC and SVFN can improve the
consistency to some extent, but their limitations can be seen
in the unsatisfactory ﬁrst images. In contrast, StoryGAN’s
ﬁrst image has a much higher quality than other baselines
because of the use of the Story Encoder to initialize the re-
current cell. This shows the advantage of using the output
of the Story Encoder as ﬁrst hidden state over random ini-
tialization.

To explore how different models represent the story, we
ran experiments where only the character names in the story
were changed, shown in Figure 7. Visually, StoryGAN out-
performs the other baselines on the image quality and con-
sistency.

Further, we perform two distinct quantitative tasks. The
ﬁrst is to determine whether the generation is able to cap-

76335

Loopy laughs but tends to be angry.Pororois singing and dancing and loopy is angry.Loopy says stop to Pororo. Pororostops.Loopy asks reason to pororo. pororois startled.Pororois making an excuse to loopy.Ground TruthImageGANSVCSVFNStoryGANGround TruthImageGANSVCSVFNStoryGANEddy is shocked at what happened now.Pororotells Eddy that Crongwas cloned.Pororotells Eddy that Cronggot into the machine.Eddy says it is not a problem.Eddy tells them that Eddy made a machine to reversethe cloning.Input Story: c1 and c2 are standing in the snow. 
c1 tells a story to c3. c3 wants to joint c1
and c2. c1 continuous to talk. c1 looks down. 
They suddenly noticed that there is something 
lying on the snow.

C1 = Pororo, C2 = Loppy, C3 = Crong

C1 = Pororo, C2 = Eddy, C3 = Rody

GT

Image 
GAN

SVC

SVFN

Story 
GAN

Figure 7: Generation result by changing character names in the
same story. The story template is given at the top, with the charac-
ter names c1, c2 and c3 in the two instances of the story, each one
shown in a column.

ture the relevant characters in the story. The nine most com-
mon characters are selected from the dataset. Their names
and pictures are provided in Supplemental Figure 9 in Ap-
pendix D. Next, a character image classiﬁer is trained on
real images from training set and applied on both real and
generated images from the test set. We compare the classi-
ﬁcation accuracy (only exact matches across all characters
counts as correct) of each image/story pair as an indicator
of whether the generation is coherent to the story descrip-
tion. The classiﬁer’s performance on the test set is 86%,
which is considered an upper bound for the task. From
these results, it is clear that StoryGAN has increased char-
acter consistency compared to the baseline models. Note
that there is peculiarity in the labels, as the human labeled
description can sometimes include characters not shown in
the frame. Further, the training classiﬁer is on real images.
Domain gap between real and generated images also harm
the performance. However, these should hurt all algorithms
equally and it is a fair comparison.

Upper Bound

ImageGAN [28]

Acc.

0.86

0.23

SVC SVFN StoryGAN
0.21

0.24

0.27

Table 2: Character classiﬁcation accuracy (exact match ratio)
comparison on Pororo-SV dataset. The upper bound is the clas-
siﬁer accuracy on the real images associated with the stories.

Human Evaluation Automatic metrics cannot fully eval-
uate the performance of StoryGAN. Therefore, we per-
formed both pairwise and ranking-based human evaluation
studies on Amazon Mechanical Turk on Pororo-SV. For
both tasks, we use 170 generated image sequences sampled
from the test set, each assigned to 5 workers to reduce hu-

Table 3: Results of pairwise human evaluation. The ± denotes
standard error on the metrics.

StoryGAN vs ImageGAN

Choice (%)

Visual Quality

Consistence
Relevance

StoryGAN ImageGAN
74.17 ±1.38
18.60 ±1.38
15.28 ±1.27
79.15 ±1.27
78.08 ±1.34
17.65±1.34

Tie
7.23
5.57
4.27

Table 4: Results of ranking-based human evaluation. The ± de-
notes standard error on the metrics.

Method

ImageGAN

SVC

Rank

2.91±0.05

2.42±0.04

SVFN
2.77±0.04

StoryGAN

1.94±0.05

man variance. The order of the options within each assign-
ment is shufﬂed to make a fair comparison.

We ﬁrst performed a pairwise comparison between Sto-
ryGAN and ImageGAN. For each input story, the worker
is presented with two generated image sequences and asked
to make decisions from the three aspects: visual quality2,
consistency3, and relevance4. Results are summarized in
Table 3. The standard error on these estimates is small,
demonstrating that StoryGAN drastically outperformed Im-
ageGAN on this task.

We next performed ranking-based human evaluation.
For each input story, the worker is asked to rank images gen-
erated from the four compared models on their overall qual-
ity. Results are summarized in Table 4. StoryGAN achieves
the highest average rank, while ImageGAN performs the
worst. There is little uncertainty in these estimates, so we
are conﬁdent that humans prefer StoryGAN on average.

5. Conclusion

We studied the story visualization task as a sequential
conditional generation problem. The proposed StoryGAN
model deals with the task by jointly considering the cur-
rent input sentence with the contextual information. This is
achieved by the proposed Text2Gist component in the Con-
text Encoder. From the ablation test, the two-level discrim-
inator and the recurrent structure on the inputs helps ensure
the consistency across the generated images and the story
to be visualized, while the Context Encoder efﬁciently pro-
vides the image generator with both local and global condi-
tional information. Both quantitative and human evaluation
studies show that StoryGAN improves the generation com-
pared to the baseline models. As image generators improve,
the story visualization’s quality will improve also.

2The generated images look visually appealing, rather than blurry and

difﬁcult to understand.

3The generated images are consistent with each other, have a common
topic hidden behind, and naturally forms a story, rather than looking like 5
independent images.

4The generated image sequence accurately reﬂects the input story and

covers the main characters mentioned in the story.

86336

References

[1] Haoye Cai, Chunyan Bai, Yu-Wing Tai, and Chi-Keung
Tang. Deep video generation, prediction and completion of
human action sequences. arXiv preprint arXiv:1711.08682,
2018. 2

[2] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole
Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-
Cespedes, Steve Yuan, Chris Tar, et al. Universal sentence
encoder. arXiv preprint arXiv:1803.11175, 2018. 3, 7

[3] Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu, and
Xiaodong Liu. Language-based image editing with recurrent
attentive models. CVPR, 2018. 2

[4] Yu Cheng, Zhe Gan, Yitong Li, Jingjing Liu, and Jianfeng
Gao. Sequential attention gan for interactive image editing
via dialogue. arXiv preprint arXiv:1812.08352, 2018. 2

[5] Emily Denton and Rob Fergus. Stochastic video generation

with a learned prior. In ICML, 2018. 2

[18] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V

Gool. Dynamic ﬁlter networks. In NIPS, 2016. 2

[19] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr:
A diagnostic dataset for compositional language and elemen-
tary visual reasoning. In CVPR, 2017. 2, 6

[20] Jin-Hwa Kim, Devi Parikh, Dhruv Batra, Byoung-Tak
Zhang, and Yuandong Tian. Codraw: Visual dialog for col-
laborative drawing. arXiv preprint arXiv:1712.05558, 2017.
2

[21] Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and
Byoung-Tak Zhang. Deepstory: Video story qa by deep em-
bedded memory networks. In IJCAI, 2017. 2, 6, 7

[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 5

[23] Diederik P Kingma and Max Welling. Auto-encoding varia-

[6] Emily L Denton et al. Unsupervised learning of disentangled

tional bayes. arXiv preprint arXiv:1312.6114, 2013. 2

representations from video. In NIPS, 2017. 2

[7] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:
arXiv

Non-linear independent components estimation.
preprint arXiv:1410.8516, 2014. 2

[8] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-
arXiv preprint

gio. Density estimation using real nvp.
arXiv:1605.08803, 2016. 2

[9] Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, De-
von Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua
Bengio, and Graham W Taylor. Keep drawing it: Iterative
language-based image generation and editing. arXiv preprint
arXiv:1811.09845, 2018. 2

[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
2

[11] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem,
and Aniruddha Kembhavi. Imagine this! scripts to composi-
tions to videos. ECCV, 2018. 2

[12] Zekun Hao, Xun Huang, and Serge Belongie. Controllable
video generation with sparse trajectories. In CVPR, 2018. 2
[13] Jiawei He, Andreas Lehrmann, Joseph Marino, Greg Mori,
and Leonid Sigal. Probabilistic video generation using holis-
tic attribute control. arXiv preprint arXiv:1803.08085, 2018.
2

[14] Alain Hore and Djemel Ziou. Image quality metrics: Psnr

vs. ssim. In ICPR, 2010. 6

[15] Qiuyuan Huang, Zhe Gan, Asli Celikyilmaz, Dapeng Wu,
Jianfeng Wang, and Xiaodong He. Hierarchically structured
reinforcement learning for topically coherent visual story
generation. arXiv preprint arXiv:1805.08191, 2018. 2

[16] Ting-Hao Kenneth Huang,

Francis Ferraro, Nasrin
Mostafazadeh,
Ishan Misra, Aishwarya Agrawal, Jacob
Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli,
Dhruv Batra, et al. Visual storytelling. In NAACL, 2016. 2,
3

[17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. CVPR, 2017. 1, 2

[24] Yitong Li, Martin Renqiang Min, Dinghan Shen, David Carl-
son, and Lawrence Carin. Video generation from text. AAAI,
2018. 2, 4, 5

[25] Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, and
Eric P Xing. Recurrent topic-transition gan for visual para-
graph generation. arXiv preprint arXiv:1703.07022, 2017.
2

[26] Hareesh Ravi, Lezi Wang, Carlos Muniz, Leonid Sigal, Dim-
itris Metaxas, and Mubbasir Kapadia. Show me a story: To-
wards coherent neural story illustration. In CVPR, 2018. 2

[27] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi.
Efﬁcient parametrization of multi-domain deep neural net-
works. In CVPR, 2018. 2

[28] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
ICML, 2016. 1, 2, 4, 6,
versarial text to image synthesis.
8

[29] Shikhar Sharma, Dendi Suhubdy, Vincent Michalski,
Samira Ebrahimi Kahou, and Yoshua Bengio. Chatpainter:
Improving text to image generation using dialogue. arXiv
preprint arXiv:1802.08216, 2018. 2

[30] Rakshith Shetty, Mario Fritz, and Bernt Schiele. Adversarial
scene editing: Automatic object removal from weak supervi-
sion. arXiv preprint arXiv:1806.01911, 2018. 2

[31] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
Kautz. Mocogan: Decomposing motion and content for
video generation. CVPR, 2018. 1, 2, 3, 4, 6

[32] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
Generating videos with scene dynamics. In NIPS, 2016. 1,
2, 3, 4

[33] Wei Wang, Xavier Alameda-Pineda, Dan Xu, Pascal Fua,
Elisa Ricci, and Nicu Sebe.
Every smile is unique:
Landmark-guided diverse smile generation. In CVPR, 2018.
2

[34] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. CVPR, 2018. 1, 2

96337

[35] Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee.
Attribute2image: Conditional image generation from visual
attributes. In ECCV, 2016. 1, 2

[36] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei
Huang, Xiaogang Wang, and Dimitris Metaxas. Stackgan:
Text to photo-realistic image synthesis with stacked genera-
tive adversarial networks. In ICCV, 2017. 1, 2, 3, 4, 6

[37] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dim-
itris Metaxas. Learning to forecast and reﬁne residual motion
for image-to-video generation. In ECCV, 2018. 2

[38] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. ICCV, 2017. 2

106338

