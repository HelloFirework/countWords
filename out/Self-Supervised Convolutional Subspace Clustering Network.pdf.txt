Self-Supervised Convolutional Subspace Clustering Network

Junjian Zhang†, Chun-Guang Li†, Chong You‡, Xianbiao Qi♯,

Honggang Zhang†, Jun Guo†, and Zhouchen Lin§

† SICE, Beijing University of Posts and Telecommunications

‡ EECS, University of California, Berkeley

♯ Shenzhen Research Institute of Big Data

§ Key Laboratory of Machine Perception (MOE), School of EECS, Peking University

Abstract

Subspace clustering methods based on data self-
expression have become very popular for learning from
data that lie in a union of low-dimensional linear sub-
spaces. However, the applicability of subspace clustering
has been limited because practical visual data in raw for-
m do not necessarily lie in such linear subspaces. On the
other hand, while Convolutional Neural Network (ConvNet)
has been demonstrated to be a powerful tool for extract-
ing discriminative features from visual data, training such
a ConvNet usually requires a large amount of labeled data,
which are unavailable in subspace clustering application-
s. To achieve simultaneous feature learning and subspace
clustering, we propose an end-to-end trainable framework,
called Self-Supervised Convolutional Subspace Clustering
Network (S2ConvSCN), that combines a ConvNet module
(for feature learning), a self-expression module (for sub-
space clustering) and a spectral clustering module (for self-
supervision) into a joint optimization framework. Particu-
larly, we introduce a dual self-supervision that exploits the
output of spectral clustering to supervise the training of the
feature learning module (via a classiﬁcation loss) and the
self-expression module (via a spectral clustering loss). Our
experiments on four benchmark datasets show the effective-
ness of the dual self-supervision and demonstrate superior
performance of our proposed approach.

1. Introduction

In many real-world applications such as image and video
processing, we need to deal with a large amount of high-
dimensional data. Such data can often be well approxi-
mated by a union of multiple low-dimensional subspaces,
where each subspace corresponds to a class or a category.
For example, the frontal facial images of a subject taken un-
der varying lighting conditions approximately span a linear
subspace of dimension up to nine [11]; the trajectories of

feature points related to a rigidly moving object in a video
sequence span an afﬁne subspace of dimension up to three
[41]; the set of handwritten digit images of a single digit al-
so approximately span a low-dimensional subspace [8]. In
such cases, it is important to segment the data into multi-
ple groups where each group contains data points from the
same subspace. This problem is known as subspace cluster-
ing [42, 45], which we formally deﬁne as follows.

Problem (Subspace Clustering). Let X ∈ IRD×N be
a real-valued matrix whose columns are drawn from a u-
nion of n subspaces of IRD, ∪n
i=1{Si}, of dimensions
di ≪ min{D, N }, for i = 1, . . . , n. The goal of subspace
clustering is to segment the columns of X into their corre-
sponding subspaces.

In the past decade, subspace clustering has become an
important topic in unsupervised learning and many sub-
space clustering algorithms have been developed [2, 23, 4,
27, 22, 26, 5, 17, 37, 53, 51, 18]. These methods have been
successfully applied to various applications such as motion
segmentation [44, 40], face image clustering [3], genes ex-
pression microarray clustering [28, 20] and so on.

Despite the great success in the recent development of
subspace clustering, its applicability to real applications is
very limited because practical data do not necessarily con-
form with the linear subspace model. In face image cluster-
ing, for example, practical face images are often not aligned
and often contain variations in pose and expression of the
subject. Subspace clustering cannot handle such cases as
images corresponding to the same face no longer lie in lin-
ear subspaces. While there are recently developed tech-
niques for joint image alignment and subspace clustering
[21], such a parameterized model is incapable of handling a
broader range of data variations such as deformation, trans-
lation and so on.
It is also possible to use manually de-
signed invariance features such as SIFT [25], HOG [1] and
PRICoLBP [38] of the images before performing subspace
clustering, e.g., in [36, 35]. However, there has been neither
theoretical nor practical evidence to show that such features
follow the linear subspace model.

15473

Recently, Convolutional Neural Networks (ConvNets)
have demonstrated superior ability in learning useful image
representations in a wide range of tasks such as face/object
classiﬁcation and detection [15, 31].
In particular, it is
shown in [16] that when applied to images of different class-
es, ConvNets are able to learn features that lie in a union of
linear subspaces. The challenge for training such a Con-
vNet, however, is that it requires a large number of labeled
training images which is often unavailable in practical ap-
plications.

In order to train ConvNet for feature learning without
labeled data, many methods have been recently proposed
by exploiting the self-expression of data in a union of sub-
spaces [36, 14, 35, 54]. Speciﬁcally, these methods super-
vise the training of ConvNet by inducing the learned fea-
tures to be such that each feature vector can be expressed as
a linear combination of the other feature vectors. However,
it is difﬁcult to learn good feature representations in such an
approach due to the lack of effective supervision.

Paper contribution.
In this paper, we develop an end-
to-end trainable framework for simultaneous feature learn-
ing and subspace clustering, called Self-Supervised Con-
volutional Subspace Clustering Network (S2ConvSCN). In
this framework, we use the current clustering results to
self-supervise the training of feature learning and self-
expression modules, which is able to signiﬁcantly improve
the subspace clustering performance. In particular, we in-
troduce the following two self-supervision modules:

1. We introduce a spectral clustering module which
uses the current clustering results to supervise the
learning of the self-expression coefﬁcients. This is
achieved by inducing the afﬁnity generated from the
self-expression to form a segmentation of the data that
aligns with the current class labels generated from clus-
tering.

2. We introduce a classiﬁcation module which uses the
current clustering results to supervise the training of
feature learning. This is achieved by minimizing the
classiﬁcation loss between the output of a classiﬁer
trained on top of the feature learning module and the
current class labels generated from clustering.

We propose a training framework where the feature repre-
sentation, the data self-expression and the data segmenta-
tion are jointly learned and alternately reﬁned in the learn-
ing procedure. Conceptually, the initial clustering results
do not align exactly with the true data segmentation, there-
fore the initial self-supervision incurs errors to the training.
Nonetheless, the feature learning is still expected to ben-
eﬁt from such self-supervision as there are data with cor-
rect labels that produce useful information. An improved
feature representation subsequently helps to learn a better

self-expression and consequently produce a better data seg-
mentation (i.e., with less wrong labels). Our experiments on
four benchmark datasets demonstrate superior performance
of the proposed approach.

2. Related Work

In this section, we review the relevant prior work in sub-
space clustering. For clarity, we group them into two cate-
gories: a) subspace clustering in original space; and b) sub-
space clustering in feature space.

2.1. Subspace Clustering in Original Space

In the past years, subspace clustering has received a lot of
attention and many methods have been developed. Among
them, methods based on spectral clustering are the most
popular, e.g., [2, 23, 4, 27, 3, 22, 26, 5, 17, 37, 51, 53, 18,
50]. These methods divide the task of subspace clustering
into two subproblems. The ﬁrst subproblem is to learn a da-
ta afﬁnity matrix from the original data, and the second sub-
problem is to apply spectral clustering on the afﬁnity matrix
to ﬁnd the segmentation of the data. The two subproblems
are solved successively in one-pass [2, 23, 4, 27, 26, 51] or
solved alternately in multi-pass [5, 17, 7, 53, 18].

Finding an informative afﬁnity matrix is the most crucial
step. Typical methods to ﬁnd an informative afﬁnity matrix
are based on the self-expression property of data [2, 45],
which states that a data point in a union of subspaces can be
expressed as a linear combination1 of other data points, i.e.,
xj = ∑i̸=j cij xi + ej , where ej is used to model the noise
or corruption in data. It is expected that the linear combi-
nation of data point xj uses the data points that belong to
the same subspace as xj . To achieve this objective, differ-
ent types of regularization terms on the linear combination
coefﬁcients are used. For example, in [2] the ℓ1 norm is
used to ﬁnd sparse linear combination; in [23] the nuclear
norm of the coefﬁcients matrix is used to ﬁnd low-rank rep-
resentation; in [46, 51] the mixture of the ℓ1 norm and the ℓ2
norm or the nuclear norm is used to balance the sparsity and
the denseness of the linear combination coefﬁcients; and in
[48] a data-dependent sparsity-inducing regularizer is used
to ﬁnd sparse linear combination. On the other hand, differ-
ent ways to model the noise or corruptions in data have also
been investigated, e.g., the vector ℓ1 norm is used in [2], the
ℓ2,1 norm is adopted in [23], and the correntropy term is
used in [9].

2.2. Subspace Clustering in Feature Space

For subspace clustering in feature space, we further di-
vide the existing methods into two types. The ﬁrst type uses
latent feature space, which is induced via a Mercer kernel,

1If data points lie in a union of afﬁne subspaces [19], then the linear

combination will be modiﬁed to afﬁne combination.

5474

Figure 1. Architecture of the proposed Self-Supervised Convolutional Subspace Clustering Network (S2ConvSCN). It consists of mainly
ﬁve modules: a) stacked convolutional encoder module, which is used to extract convolutional features; b) stacked convolutional decoder
module, which is used with the encoder module to initialize the convolutional module; c) self-expression module, which is used to learn
the self-expressive coefﬁcient matrix and also takes the self-supervision information from the result of spectral clustering to reﬁne the
self-expressive coefﬁcients matrix; d) FC-layers based self-supervision module, which builds a self-supervision path back to the stacked
convolutional encoder module; e) spectral clustering module, which provides self-supervision information to guide the self-expressive
model and FC-layers module. The modules with solid line box are the backbone components; whereas the modules in dashed box are the
auxiliary components to facilitate the training of the whole network.

e.g., [34, 32, 49, 47], or constructed via matrix decomposi-
tion, e.g., [24], [33]. The second type use explicit feature
space, which is designed by manual feature extraction, e.g.,
[36], or is learned from data, e.g., [14, 54].

Latent Feature Space. Many recent works have em-
ployed the kernel trick to map the original data into a high-
dimensional latent feature space, in which subspace cluster-
ing is performed, e.g., [34, 32, 49, 47]. For example, pre-
deﬁned polynomial and Gaussian kernels are used in the k-
ernel sparse subspace clustering method [34, 32] and the
kernel low-rank representation method [49, 47, 12]. Un-
fortunately, it is not guaranteed that the data in the latent
feature space induced with such predeﬁned kernels lie in
low-dimensional subspaces.2

On the other hand, the latent feature space has also been
constructed via matrix decomposition, e.g., [24], [33]. In
[24], a linear transform matrix and a low-rank representa-
tion are computed simultaneously; in [33], a linear transfor-
m and a sparse representation are optimized jointly. Howev-
er, the representation power of the learned linear transform
is still limited.

Explicit Feature Space. Deep learning has gained a lot
of research interests due to its powerful ability to learn hi-
erarchical features in an end-to-end trainable way [10, 15].
Recently, there are a few works that use techniques in deep
learning for feature extraction in subspace clustering. For
example, in [36, 35], a fully connected deep auto-encoder
network with hand-crafted features (e.g., SIFT or HOG
features) combined with a sparse self-expression model is
developed; in [14], a stacked convolutional auto-encoder
network with a plus-in self-expression model is proposed.
While promising clustering accuracy has been reported,
these methods are still suboptimal because neither the po-
tentially useful supervision information from the clustering
result has been taken into the feature learning step nor a join-
t optimization framework for fully combining feature learn-
ing and subspace clustering has been developed. More re-
cently, in [54], a deep adversarial network with a subspace-
speciﬁc generator and a subspace-speciﬁc discriminator is
adopted in the framework of [14] for subspace clustering.
However, the discriminator need to use the dimension of
each subspace, which is usually unknown.

2In [12], while the data matrix in the latent feature space is encouraged
to be low-rank, it is not necessary that the data in feature space are encour-
aged to align with a union of linear subspaces.

In this paper, we attempt to develop a joint optimization
framework for combining feature learning and subspace
clustering, such that the useful self-supervision information

5475

from subspace clustering result could be used to guide the
feature learning and to reﬁne the self-expression model. In-
spired by the success of Convolutional Neural Networks in
recent years for classiﬁcation tasks on images and videos
datasets [15] and the recent work [14], we integrate the con-
volutional feature extraction module into subspace cluster-
ing to form an end-to-end trainable joint optimization frame-
work, called Self-Supervised Convolutional Subspace Clus-
tering Network (S2ConvSCN). In S2ConvSCN, both the s-
tacked convolutional layers based feature extraction and the
self-expression based afﬁnity learning are effectively self-
supervised by exploiting the feedback from spectral cluster-
ing.

3. Our Proposal: Self-Supervised Convolution-

al Subspace Clustering Network

In this section, we present our S2ConvSCN for joint fea-
ture learning and subspace clustering. We start with intro-
ducing our network formulation (see Fig. 1), then introduce
the self-supervision modules. Finally, we present an effec-
tive procedure for training the proposed network.

3.1. Network Formulation

As aforementioned, our network is composed of a fea-
ture extraction module, a self-expression module and self-
supervision modules for training the former two modules.

Feature Extraction Module. A basic component of our
proposed S2ConvSCN is the feature extraction module,
which is used to extract features from raw data that are a-
menable to subspace clustering. To extract localized fea-
tures while preserving spatial locality, we adopt the con-
volutional neural network which is comprised of multiple
convolutional layers. We denote the input to the network
as h(0) = x where x is the image. A convolutional lay-
(ℓ)
er ℓ contains a set of ﬁlters w
and the associated bias-
i
, i = 1, · · · , m(ℓ), and produces m(ℓ) feature maps
es b
from the output of the previous layer. The feature maps
(L)
{h
i }i=1,··· ,m(L) in the top layer L of the network are then
used to form a representation of the input data x. Speciﬁ-
cally, the m(L) feature maps {h
i=1 are vectorized and
concatenated to form a representation vector z, i.e.,

i }m(L)

(ℓ)
i

(L)

z = [h

(L)
1

(:), · · · , h

(L)

m(L) (:)]⊤

,

(1)

(L)
1

where h

(:), · · · , h

(L)
m(L)(:) are row vectors denoting the
(L)
vectorization of the feature maps h
m(L) . These
vectors are horizontally concatenated and then transposed
to form the vector z.

, · · · , h

(L)
1

To ensure that the learned representation z contains
meaningful information from the input data x, the feature

(L)
1

, · · · , h

(L)
maps h
m(L) are fed into a decoder network to re-
construct an image ˆx. The loss function for this encoder-
decoder network is the reconstruction error:

L0 =

1
2N

N

∑

j=1

∥xj − ˆxj∥2

2 =

1
2N

∥X − ˆX∥2
F ,

(2)

where N is the number of images in the training set.

Self-Expression Module. State-of-the-art subspace cluster-
ing methods are based on the self-expression property of
data, which states that each data point in a union of sub-
spaces can be expressed as a linear combination of other
data points [2, 45]. In order to learn feature representation-
s that are suitable for subspace clustering, we adopt a self-
expression module that imposes the following loss function:

λ∥C∥ℓ +

1
2

∥Z − ZC∥2

F s.t. diag(C) = 0,

(3)

where Z = [z1, · · · , zN] is a matrix containing features
from the feature extraction module as its columns, ∥C∥ℓ
is a properly chosen regularization term,
the constraint
diag(C) = 0 is optionally used to rule out a trivial solution
of C = I, and λ > 0 is a tradeoff parameter.

Self-Supervision Modules. Once the self-expression coef-
ﬁcient matrix C is obtained, we can compute a data afﬁnity
matrix as A = 1
2 (|C| + |C ⊤|). Subsequently, spectral clus-
tering can be applied on A to obtain a segmentation of the
data by minimizing the following cost:

Q ∑
min

i,j

aij∥qi − qj∥2

2, s.t. Q ∈ Q,

(4)

where Q = {Q ∈ {0, 1}n×N : 1⊤Q = 1⊤ and rank(Q) =
n} is a set of all valid segmentation matrices with n groups,
and qi and qj are respectively the i-th and j-th columns
of Q indicating the membership of each data point to the
assigned cluster. In practice, since the search over all Q ∈
Q is combinatorial, spectral clustering techniques usually
relax the constraint Q ∈ Q to QQ⊤ = I.

Observe that the spectral clustering produces a labeling
of the data set which, albeit is not necessarily the correct
class label for all the data points, contains meaningful infor-
mation about the data. This motivates us to supervise the
training of the feature extraction and self-expression mod-
ules using the output of spectral clustering. In principle, the
features learned from the feature extraction module should
contain enough information for predicting the class labels of
the data points. Therefore, we introduce a classiﬁcation lay-
er on top of the feature extraction module which is expect-
ed to produce labels that aligns with the labels generated
in spectral clustering. Furthermore, the segmentation pro-
duced by spectral clustering can also be used to construct a
binary segmentation matrix, which contains information re-
garding which data points should be used in the expression

5476

of a particular data point. Therefore, we incorporate the ob-
jective function of spectral clustering as a loss function in
our network formulation, which has the effect of supervis-
ing the training of the self-expression module. We present
the details of these two self-supervision modules in the fol-
lowing two subsections.

3.2. Self Supervision for Self Expression

To exploit the information in the labels produced by spec-
tral clustering, we incorporate spectral clustering as a mod-
ule of the network which provides a feedback to the self-
expression model (see Fig. 1).

To see how the objective function of spectral clustering
in (4) provides such feedback, we rewrite (4) to a weighted
ℓ1 norm of C as in [17], that is,

1
2 ∑

i,j

aij∥qi − qj∥2

2 = ∑

i,j

|cij|

∥qi − qj∥2
2

2

:= ∥C∥Q, (5)

where we have used the fact that aij = 1
2 (|cij| + |cji|). It
can be seen from (5) that ∥C∥Q measures the discrepancy
between the coefﬁcients matrix C and the segmentation ma-
trix Q. When Q is provided, minimizing the cost ∥C∥Q has
the effect of enforcing the self-expression matrix C to be
such that an entry cij is nonzero only if the i-th and j-th
data points have the same class labels. Therefore, incorpo-
rating the term ∥C∥Q in the network formulation helps the
training of the self-expression module. That is, the result
of previous spectral clustering can be incorporated into the
self-expression model to provide self-supervision for reﬁn-
ing the self-expression matrix C.

3.3. Self Supervision for Feature Learning

We also use the class labels generated from spectral clus-
tering to supervise the training of the feature extraction mod-
ule. Notice that the output of spectral clustering is an n-
dimensional vector which indicates the membership to n
subspaces (i.e., clusters). Thus, we design FC layers as
p × N1 × N2 × n, where p is the dimension of the extracted
convolutional feature, which is deﬁned as the concatenation
of the different feature maps of the last convolutional layer
in the encoder block, and N1 and N2 are the numbers of
neurons in the two FC layers, respectively.

Denote y as the n-dimensional output of the FC layers,
where y ∈ IRn. Note that the output {qj}N
j=1 of spectral
clustering will be treated as the target output of the FC lay-
ers. To exploit the self-supervision information to train the
convolutional encoder, we deﬁne a mixture of cross-entropy
loss and center loss (CEC) as follows:

L4 =

1
N

N

∑

j=1

(ln(1 + e−˜y⊤

j qj ) + τ ∥yj − µπ(yj )∥2

2),

(6)

where ˜yj is a normalization of yj via softmax, µπ(yj) denotes
the cluster center which corresponds to yj , π(yj) is to take
the index of yj from the output of spectral clustering, and
0 ≤ τ ≤ 1 is a tradeoff parameter. The ﬁrst term of L4 is
effectively a cross-entropy loss and the second term of L4 is
a center loss which compresses the intra-cluster variations.
An important issue in deﬁning such a loss function is that
the output of spectral clustering {qj}N
j=1 provides merely
pseudo labels for the input data. That is, the label index as-
signed to a cluster in the returned result of spectral cluster-
ing is up to an unknown permutation. Therefore, the class la-
bels from two successive epochs might not be consistent. To
address this issue, we propose to perform a permutation of
the new pseudo labels via Hungarian algorithm [29] to ﬁnd
an optimal assignment between the pseudo labels of succes-
sive iterations before feeding them into the self-supervision
module with the cross-entropy loss in (6).

Remark 1. Note that the output of spectral clustering is
used in two interrelated self-supervision modules and thus
we call it a dual self-supervision mechanism.3

3.4. Training S2ConvSCN

To obtain an end-to-end trainable framework, we design
the total cost function of S2ConvSCN by putting together
the costs in (2), (3), (5), and (6) as follows:

L = L0 + γ1L1 + γ2L2 + γ3L3 + γ4L4,

(7)

2 ∥Z − ZC∥2

where L1 = ∥C∥ℓ, L2 = 1
F , L3 = ∥C∥Q, and
γ1, γ2, γ3 and γ4 are four tradeoff parameters. The tradeoff
parameters are set roughly to be inversely proportional to
the value of each cost in order to obtain a balance amongst
them.

To train S2ConvSCN, we propose a two-stage strategy
as follows: a) pre-train the stacked convolutional layers to
provide an initialization of S2ConvSCN; b) train the whole
network with the assistance of the self-supervision informa-
tion provided by spectral clustering.

Stage I: Pre-Training Stacked Convolutional Module.
The pre-training stage uses the cost L0. In this stage, we
set the weights in the two FC layers as zeros, which yield
zeros output. Meanwhile, we also set the output of spectral
clustering as zero vectors, i.e., qj = 0 for j = 1, · · · , N .
By doing so, the two FC layers are “sleeping” during this
pre-training stage. Moreover, we set the coefﬁcient ma-
trix C as an identity matrix, which is equivalent to training
S2ConvSCN without the self-expression layer. As an op-
tional pre-training, we can also use the pre-trained stacked

3 While it is also sensible to term our approach with “self-training”,
we prefer to use the term “self-supervision” in order to emphasizes on
the mechanism of guiding the training of the whole framework, that is
to make each component as consistent as possible (i.e., be separable, self-
expressive, and block diagonal).

5477

Algorithm 1 Procedure for training S2ConvSCN
Require: Input data, tradeoff parameters,

maximum iteration Tmax, T0, and t=1.

1: Pre-train the stacked convolutional

module via stacked CAE.

2: (Optional) Pre-train the stacked

convolutional module with the

self-expressive layer.

3: Initialize the FC layers.
4: Run self-expressive layer.
5: Run spectral clustering layer to get the

segmentation Q.

6: while t ≤ Tmax do
7:

Fixed Q, update the other parts T0

epoches.

8:

Run spectral clustering once to update
Q and set t ← t+1.

9: end while

Ensure: trained S

2

ConvSCN and Q.

Extended Yale B

ORL

Layers

kernel size

channels

kernel size

channels

encoder-1
encoder-2
encoder-3
decoder-1
decoder-2
decoder-3

3
3
5
5
3
3
Table 1. Network settings for Extended Yale B and ORL.

3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3

5 × 5
3 × 3
3 × 3
3 × 3
3 × 3
5 × 5

10
20
30
30
20
10

CAE to train the stacked CAE with the self-expression lay-
er.

Stage II: Training the Whole S2ConvSCN. In this stage,
we use the total cost L to train the whole S2ConvSCN as a
stacked CAE assisted with the self-expression module and
dual self-supervision. To be more speciﬁc, given the spec-
tral clustering result Q, we update the other parameters in
S2ConvSCN for T0 epoches, and then perform spectral clus-
tering to update Q. For clarity, we provide the detailed pro-
cedure to train S2ConvSCN in Algorithm 1.

Remark 2.
In the total cost function as (7), if we set
γ3 = γ4 = 0, then the two self-supervision blocks will dis-
appear and our S2ConvSCN reduces to DSCNet [14]. Thus,
it would be interesting to add an extra pre-training stage,
i.e., using the cost function L0 + γ1L1 + γ2L2 to train the
stacked convolutional module and the self-expressive lay-
er together before evoking the FC layers and the spectral
clustering layer. This is effectively a DSCNet [14]. In ex-
periments, as used in [14], we stop the training by setting a
maximum number of epoches Tmax.

4. Experimental Evaluations

of

To

the

our

evaluate

performance

proposed
S2ConvSCN, we conduct experiments on four bench-
mark data sets:
two face image data sets, the Extended
Yale B [6] and ORL [39], and two object image data sets,
COIL20 and COIL100 [30]. We compare our proposed
S2ConvSCN with the following baseline algorithms,
including Low Rank Representation (LRR) [23], Low
Rank Subspace Clustering (LRSC) [43], Sparse Subspace
Clustering (SSC) [3], Kernel Sparse Subspace Cluster-
ing (KSSC) [34], SSC by Orthogonal Matching Pursuit
(SSC-OMP) [52], Efﬁcient Dense Subspace Clustering
(EDSC) [13], Structured SSC (S3C) [18], SSC with the
pre-trained convolutional auto-encoder features (AE+SSC),
EDSC with the pre-trained convolutional auto-encoder
features (AE+EDSC), Deep Subspace Clustering Networks
(DSCNet) [14] and Deep Adversarial Subspace Clustering
(DASC) [54]. For EDSC, AE+EDSC, DSCNet and DASC,
we directly cite the best results reported in [14] and [54].
For S3C, we use soft S3C with a ﬁxed parameter α = 1.

The architecture speciﬁcation of S2ConvSCN used in our
experiments for each dataset are listed in Table 1 and Ta-
ble 4. In the stacked convolutional layers, we set the kernel
stride as 2 in both horizontal and vertical directions, and use
Rectiﬁed Linear Unit (ReLU) [15] as the activation function
σ(·). In addition, the learning rate is set to 1.0 × 10−3 in
all our experiments. The whole data set is used as one batch
input. For the FC layers, we set N1 = N

2 and N2 = n.

To ﬁnd informative afﬁnity matrix, we adopt the vector
ℓ1 norm and the vector ℓ2 norm to deﬁne ∥C∥ℓ and denote
as S2ConvSCN-ℓ1 and S2ConvSCN-ℓ2, respectively. In the
second training stage, we update the stacked convolutional
layers, the self-expression model, and the FC layers for T0
epochs and then update the spectral clustering module once,
where T0 is set to 5 ∼ 16 in our experiments.

4.1. Experiments on Extended Yale B

The Extended Yale B database [6] consists of face im-
ages of 38 subjects, 2432 images in total, with approxi-
mately 64 frontal face images per subject taken under d-
ifferent illumination conditions, where the face images of
each subject correspond to a low-dimensional subspace. In
our experiments, we follow the protocol used in [14]: a)
each image is down-sampled from 192 × 168 to 48 × 42
pixels; b) experiments are conducted using all choices of
n ∈ {10, 15, 20, 25, 30, 35, 38}.

To make a fair comparison, we use the same setting as
that used in DSCNet [14], in which a three-layer stacked
convolutional encoders is used with {10, 20, 30} channels,
respectively. The detailed settings for the stacked convolu-
tional network used on Extended Yale B are shown Table
1. The common parameters γ1 and γ2 are set the same as
that in DSCNet, where γ1 = 1 (for the term ∥C∥ℓ) and

5478

Methods
10 subjects

Mean
Median
15 subjects

Mean
Median
20 subjects

Mean
Median
25 subjects

Mean
Median
30 subjects

Mean
Median
35 subjects

Mean
Median
38 subjects

Mean
Median

LRR

LRSC

SSC

AE+ SSC

KSSC

SSC-OMP

soft S3C†

EDSC

AE+ EDSC

DSC-ℓ1

DSC-ℓ2

Ours (ℓ2)

Ours (ℓ1)

19.76
18.91

25.82
26.30

31.45
32.11

28.14
28.22

38.59
36.98

40.61
40.71

35.12
35.12

30.95
29.38

31.47
31.64

28.76
28.91

27.81
26.81

30.64
30.31

31.35
31.74

29.89
29.89

8.80
9.06

12.89
13.23

20.11
21.41

26.30
26.56

27.52
27.97

29.19
29.51

29.36
29.36

17.06
17.75

18.65
17.76

18.23
16.80

18.72
17.88

19.99
20.00

22.13
21.74

25.33
25.33

14.49
15.78

16.22
17.34

16.55
17.34

18.56
18.03

20.49
20.94

26.07
25.92

27.75
27.75

12.08
8.28

14.05
14.69

15.16
15.23

18.89
18.53

20.75
20.52

20.29
20.18

23.52
23.52

6.34
3.75

11.01
10.89

14.07
13.98

16.79
17.13

20.46
21.15

20.38
20.47

19.45
19.45

5.64
5.47

7.63
6.41

9.30
10.31

10.67
10.84

11.24
11.09

13.10
13.10

11.64
11.64

5.46
6.09

6.70
5.52

7.67
6.56

10.27
10.22

11.56
10.36

13.28
13.21

12.66
12.66

2.23
2.03

2.17
2.03

2.17
2.11

2.53
2.19

2.63
2.81

3.09
3.10

3.33
3.33

1.59
1.25

1.69
1.72

1.73
1.80

1.75
1.81

2.07
2.19

2.65
2.64

2.67
2.67

1.18
1.09

1.14
1.14

1.31
1.32

1.32
1.34

1.71
1.77

1.67
1.69

1.56
1.56

1.18
1.09

1.12
1.14

1.30
1.25

1.29
1.28

1.67
1.72

1.62
1.60

1.52
1.52

Table 2. Clustering Error (%) on Extended Yale B. The best results are in bold and the second best results are underlined.

n

10 −3. For the speciﬁc parameters used
γ2 = 1.0 × 10
in S2ConvSCN, we set γ3 = 16 for the term ∥C∥Q and
γ4 = 72 for the cross-entropy term, respectively. We set
T0 = 5 and Tmax = 10 + 40n.

The experimental results are presented in Table 2. We ob-
serve that our proposed S2ConvSCN-ℓ1 and S2ConvSCN-ℓ2
remarkably reduced the clustering errors and yield the low-
est clustering errors with n ∈ {10, 15, 20, 25, 30, 35, 38}
than all the listed baseline methods. We note that DASC
[54] reported a clustering error of 1.44% on Extended Yale
B with n = 38, which is slightly better than our results.

To gain further understanding of the proposed dual self-
supervision, we use S2ConvSCN-ℓ1 as an example and eval-
uate the effect of using the dual self-supervision modules
via an ablation study. Due to space limitation, we only list
the experimental results of using a single self-supervision
via L3, using a single self-supervision via L4, and using du-
al self-supervision of L3 plus L4 on datasets Extended Yale
B in Table 3. As a baseline, we show the experimental re-
sults of DSCNet [14], which uses the loss L0 + L1 + L2.
As could be read from Table 3 that, using only a single
self-supervision module, i.e., L0 + L1 + L2 plus L3, or
L0 + L1 + L2 plus L4, the clustering errors are reduced.
Compared to using the self-supervision via a spectral clus-
tering loss L3 in the self-expression module, using the self-
supervision via the classiﬁcation loss L4 in FC block is
more effective. Nonetheless, using the dual supervision
modules further reduces the clustering errors.

4.2. Experiments on ORL

The ORL dataset [39] consists of face images of 40 dis-
tinct subjects, each subjects having 10 face images under
varying lighting conditions, with different facial expression-
s (open/closed eyes, smiling/not smiling) and facial details
(glasses / no glasses) [39]. As the images were took under

variations of facial expressions, this data set is more chal-
lenging for subspace clustering due to the nonlinearity and
small sample size per subject.

In our experiments, each image is down-sampled from
112×92 to 32×32. We reduce the kernel size in convolution
module to 3 × 3 due to small image size and set the number
of channels to {3, 3, 5}. The speciﬁcation of the network
structure is shown in Table 1. For the tradeoff parameters,
we set γ1 = 0.1, γ2 = 0.01, γ3 = 8, and γ4 = 1.2 for our
S2ConvSCN. For the ﬁne-tuning stage, we set T0 = 5 and
Tmax = 940. Experimental results are shown in Table 5.
Again, our proposed approaches yield the best results.

4.3. Experiments on COIL20 and COIL100

To further verify the effectiveness of our proposed
S2ConvSCN, we conduct experiments on dataset COIL20
and COIL100 [30]. COIL20 contains 1440 gray-scale im-
ages of 20 objects; whereas COIL100 contains 7200 images
of 100 objects. Each image was down-sampled to 32 × 32.
The settings of the stacked convolutional networks used for
COIL20 and COIL100 are listed in Table 4.

For the tradeoff parameters on COIL20, we set γ1 = 1,
γ2 = 30 as same as used in DSC-Net [14], and γ3 = 8,
γ4 = 6, T0 = 4, and Tmax = 80 in our S2ConvSCN. For the
tradeoff parameters on COIL100, we set γ1 = 1, γ2 = 30
as same as used in DSC-Net [14], and γ3 = 8, γ4 = 7,
T0 = 16, and Tmax = 110 in our S2ConvSCN.

For experiments on COIL20 and COIL100, we initial-
ize the convolutional module with stacked CAE at ﬁrst, and
then train a stacked CAE assisted with a self-expressive
model. This is effectively DSCNet [14]. And then, we
train the whole S2ConvSCN. Experimental results are list-
ed in Table 5. As could be read, our S2ConvSCN-ℓ1 and
S2ConvSCN-ℓ2 reduce the clustering errors signiﬁcantly.
This result conﬁrms the effectiveness of the designed dual

5479

No. Subjects

10 subjects

15 subjects

20 subjects

25 subjects

30 subjects

35 subjects

38 subjects

Mean Median Mean Median Mean Median Mean Median Mean Median Mean Median Mean Median
2.23
1.58
1.32
1.18

2.03
1.25
1.09
1.09
Table 3. Ablation Study on S2ConvSCN-ℓ1 on Extended Yale B.

2.81
1.82
1.61
1.72

2.17
1.67
1.54
1.30

2.11
1.57
1.48
1.25

2.53
1.61
1.48
1.29

2.63
2.74
1.87
1.67

3.09
2.64
1.82
1.62

3.10
2.65
1.84
1.60

3.33
2.75
1.92
1.52

2.17
1.63
1.31
1.12

2.03
1.55
1.30
1.14

2.19
1.63
1.98
1.28

3.33
2.75
1.92
1.52

Losses
L0 + L1 + L2(DSC-ℓ1 [14])

L0 + L1 + L2 + L3
L0 + L1 + L2 + L4

L0 + L1 + L2 + L3 + L4

6

10

6

5

4

3

2

1

0

3100

2600

2100

1600

1100

0

100

200

300

400

0

100

200

300

400

(a) L, L0 and L2

(b) L1 and L3

4

10

7.5

6

4.5

3

1.5

0

0

100

200

300

400

90

80

70

60

50

40

30

20

10

0

0

Clustering Error (%)

100

200

300

400

(d) L4

(e)

140

120

100

80

60

40

20

0

0

0.9

0.85

0.8

0.75

0.7

0

100

200

300

400

(c) L4

100

200

300

400

(f)

L3
L1

Figure 2. The cost functions and clustering error of S2ConvSCN-
ℓ1 during training period on Extended Yale B (n = 10).

self-supervision components for the proper use of the useful
information from the output of spectral clustering.

COIL20

COIL100

Layers

kernel size

channels

kernel size

channels

encoder-1
decoder-1

3 × 3
3 × 3

15
15

5 × 5
5 × 5

50
50

Table 4. Network settings for COIL20 and COIL100.

Methods

LRR
LRSC
SSC

AE+SSC

KSSC

SSC-OMP

EDSC

AE+EDSC
soft S3C†
DSC-ℓ1
DSC-ℓ2

DASC [54]

S2ConvSCN-ℓ2
S2ConvSCN-ℓ1

ORL
33.50
32.50
29.50
26.75
34.25
37.05
27.25
26.25
26.00
14.25
14.00
11.75
11.25
10.50

COIL20

COIL100

30.21
31.25
14.83
22.08
24.65
29.86
14.86
14.79
11.87
5.65
5.42
3.61
2.33
2.14

53.18
50.67
44.90
43.93
47.18
67.29
38.13
38.88
41.71
33.62
30.96

-

27.83
26.67

4.4. Convergence Behaviors

Table 5. Clustering Error (%) on ORL, COIL20 and COIL100.

To show the convergence behavior during training iter-
ations, we conduct experiments on Extended Yale B with
n = 10. We record the clustering errors and each cost func-
tion during training period, and show them as a function of
the number of epoches in Fig. 2. As could be observed from
Fig. 2(a), (c), (d) and (e), the cost functions L, L0, L2, and
L4, and the cluster error decrease rapidly and tend to “ﬂat”.
To show more details in the iterations, in Fig. 2 (b) and (f),
we show the curves of ∥C∥1, ∥C∥Q and ∥C∥Q
. Note that
∥C∥1
∥C∥Q and ∥C∥Q
are the cost and the relative cost of spec-
∥C∥1
tral clustering, respectively. Compared to ∥C∥Q, we argue
that ∥C∥Q
is more indicative to the clustering performance.
∥C∥1
As could be observed, while ∥C∥1 and ∥C∥Q are increas-
ing4, the curve of ∥C∥Q
tends to “ﬂat”—which is largely
∥C∥1
consistent to the curve of the clustering error in Fig. 2 (e).

5. Conclusion

We have proposed an end-to-end trainable framework
for simultaneous feature learning and subspace clustering,
called Self-Supervised Convolutional Subspace Clustering

4The observation that the curves of L1 and L3 go up is because the
entries of the extracted feature Z are slowly shrinking and thus the abso-
lute values of entries of C are slowly increasing, due to the absence of
normalization step in feature learning at each epoch.

Network (S2ConvSCN). Speciﬁcally, in S2ConvSCN, the
feature extraction via stacked convolutional module, the
afﬁnity learning via self-expression model, and the data
segmentation via spectral clustering are integrated into a
joint optimization framework. By exploiting a dual self-
supervision mechanism, the output of spectral clustering are
effectively used to improve the training of the stacked con-
volutional module and to reﬁne the self-expression model,
leading to superior performance. Experiments on bench-
mark datasets have validated the effectiveness of our pro-
posed approach.

Acknowledgment

J. Zhang and C.-G. Li are supported by the National
Natural Science Foundation of China (NSFC) under Grant
No. 61876022, and the Open Project Fund from Key Lab-
oratory of Machine Perception (MOE), Peking Universi-
ty. H. Zhang is partially supported by NSFC under Grant
Nos. 61701032 and 61806184. X. Qi is supported by
Shenzhen Fundamental Research Fund under Grants Nos.
ZDSYS201707251409055 and 2017ZT07X152. Z. Lin
is supported by 973 Program of China under Grant No.
2015CB352502, NSFC under Grant Nos. 61625301 and
61731018, Qualcomm, and Microsoft Research Asia.

5480

References

[1] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection. In IEEE Conference on Computer Vision
and Pattern Recognition, 2005. 1

[2] E. Elhamifar and R. Vidal. Sparse subspace clustering. In
Proceedings of IEEE International Conference on Computer
Vision and Pattern Recognition, pages 2790–2797, 2009. 1,
2, 4

[3] E. Elhamifar and R. Vidal. Sparse subspace clustering: Al-
gorithm, theory, and applications. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 35(11):2765–2781,
2013. 1, 2, 6

[4] P. Favaro, R. Vidal, and A. Ravichandran. A closed form
In
solution to robust subspace estimation and clustering.
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1801 –1807, 2011. 1, 2

[5] J. Feng, Z. Lin, H. Xu, and S. Yan. Robust subspace seg-
mentation with block-diagonal prior. In IEEE Conference on
Computer Vision and Pattern Recognition, pages 3818–3825,
2014. 1, 2

[6] A.-S. Georghiades, P.-N. Belhumeur, and D.-J. Kriegman.
From few to many: Illumination cone models for face recog-
nition under variable lighting and pose. IEEE Transaction-
s on Pattern Analysis and Machine Intelligence, 23(6):643–
660, 2001. 6

[7] X. Guo. Robust subspace segmentation by simultaneously
In
learning data representations and their afﬁnity matrix.
Proceedings of the 24th International Joint Conference on
Artiﬁcial Intelligence, pages 3547–3553, 2015. 2

[8] T. Hastie and P.-Y. Simard. Metrics and models for handwrit-
ten character recognition. Statistical Science, pages 54–65,
1998. 1

[9] R. He, L. Wang, Z. Sun, Y. Zhang, and B. Li. Information
theoretic subspace clustering. IEEE Transactions on Neural
Networks and Learning Systems, 27(12):2643–2655, 2016. 2

[10] G. Hinton, L. Deng, D. Yu, G.-E. Dahl, A.-R. Mohamed,
N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, and T. N.
Sainath. Deep neural networks for acoustic modeling in
speech recognition: The shared views of four research group-
s. IEEE Signal Processing Magazine, 29(6):82–97, 2012. 3

[11] J. Ho, M.-H. Yang, J. Lim, K.-C. Lee, and D.-J. Kriegman.
Clustering appearances of objects under varying illumination
conditions.
In Proceedings of IEEE International Confer-
ence on Computer Vision and Pattern Recognition, pages 11–
18, 2003. 1

[12] P. Ji, I. Reid, R. Garg, H. Li, and M. Salzmann. Adaptive
low-rank kernel subspace clustering. arXiv:1707.04974v4,
2019. 3

[13] P. Ji, M. Salzmann, and H. Li. Efﬁcient dense subspace clus-
tering. In IEEE Winter conferance on Applications of Com-
puter Vision, pages 461–468, 2014. 6

[14] P. Ji, T. Zhang, H. Li, M. Salzmann, and I. Reid. Deep sub-
space clustering networks. In Neural Information Processing
Systems (NIPS), 2017. 2, 3, 4, 6, 7, 8

[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.

Imagenet
In

Neural Information Processing Systems, pages 1097–1105,
2012. 2, 3, 4, 6

[16] J. Lezama, Q. Qiu, P. Muse, and G. Sapiro. Ole: Orthogo-
nal low-rank embedding - a plug and play geometric loss for
deep learning.
In Proceedings of IEEE International Con-
ference on Computer Vision and Pattern Recognition, pages
8109–8118, 2018. 2

[17] C.-G. Li and R. Vidal. Structured sparse subspace clustering:
A uniﬁed optimization framework. In Proceedings of IEEE
International Conference on Computer Vision and Pattern
Recognition, pages 277–286, 2015. 1, 2, 5

[18] C.-G. Li, C. You, and R. Vidal. Structured sparse subspace
clustering: A joint afﬁnity learning and subspace cluster-
ing framework.
IEEE Transactions on Image Processing,
26(6):2988–3001, 2017. 1, 2, 6

[19] C.-G. Li, C. You, and R. Vidal. On geometric analysis of
afﬁne sparse subspace clustering. IEEE Journal on Selected
Topics in Signal Processing, 12(6), 2018. 2

[20] C.-G. Li, J. Zhang, and J. Guo. Constrained sparse sub-
space clustering with side information.
In Proceedings of
the 24th International Conference on Pattern Recognition
(ICPR), pages 2093–2099, August 2018. 1

[21] Q. Li, Z. Sun, Z. Lin, R. He, and T. Tan. Transformation in-
variant subspace clustering. Pattern Recognition, pages 142–
155, 2016. 1

[22] G. Liu, Z. Lin, S.-C. Yan, J. Sun, Y. Yu, and Y. Ma. Robust
recovery of subspace structures by low-rank representation.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 35(1):171–184, 2013. 1, 2

[23] G. Liu, Z. Lin, and Y. Yu. Robust subspace segmentation
by low-rank representation. In Proceedings of International
Conference on Machine Learning, pages 663–670, 2010. 1,
2, 6

[24] G. Liu and S. Yan. Latent low-rank representation for sub-
space segmentation and feature extraction.
In IEEE Inter-
national Conference on Computer Vision, pages 1615–1622,
2011. 3

[25] D. Lowe. Distinctive image features from scale-invariant
keypoints. International Journal of Computer Vision, 20:91–
110, 2004. 1

[26] C. Lu, Z. Lin, and S. Yan. Correlation adaptive subspace
segmentation by trace lasso. In Proceedings of IEEE Inter-
national Conference on Computer Vision, pages 1345–1352,
2014. 1, 2

[27] C.-Y. Lu, H. Min, Z.-Q. Zhao, L. Zhu, D.-S. Huang, and S.-
C. Yan. Robust and efﬁcient subspace segmentation via least
squares regression. Proceedings of European Conference on
Computer Vision, pages 347–360, 2012. 1, 2

[28] B. McWilliams and G. Montana. Subspace clustering of high
dimensional data: a predictive approach. Data Mining and
Knowledge Discovery, 28(3):736–772, 2014. 1

[29] J. Munkres. Algorithms for the assignment and transporta-
tion problems. Journal of the Society for Industrial and Ap-
plied Mathematics, 5(1):32–38, 1957. 5

[30] S.-A. Nene, S.-K. Nayar, and H. Murase. Columbia object

image library. Columbia University, 1996. 6, 7

[31] O. M. Parkhi, A. Vedaldi, A. Zisserman, et al. Deep face

recognition. In BMVC, volume 1, page 6, 2015. 2

5481

[50] C. You, C. Li, D. P. Robinson, and R. Vidal. Scalable
exemplar-based subspace clustering on class-imbalanced da-
ta.
In Proceedings of European Conference on Computer
Vision, September 2018. 2

[51] C. You, C.-G. Li, D. Robinson, and R. Vidal. Oracle based
active set algorithm for scalable elastic net subspace cluster-
ing.
In Proceedings of IEEE International Conference on
Computer Vision and Pattern Recognition, pages 3928–3937,
2016. 1, 2

[52] C. You, D. Robinson, and R. Vidal. Scalable sparse subspace
clustering by orthogonal matching pursuit. In Proceedings
of IEEE International Conference on Computer Vision and
Pattern Recognition, pages 3918–3927, 2016. 6

[53] J. Zhang, C.-G. Li, H. Zhang, and J. Guo. Low-rank and
structured sparse subspace clustering. In Proceeding of IEEE
Visual Communication and Image Processing, 2016. 1, 2

[54] P. Zhou, Y. Hou, and J. Feng. Deep adversarial subspace
clustering. In Proceedings of IEEE International Conference
on Computer Vision and Pattern Recognition, June 2018. 2,
3, 6, 7, 8

[32] V. M. Patel, H. V. Nguyen, and R. Vidal. Latent space sparse
and low-rank subspace clustering. IEEE Journal of Selected
Topics in Signal Processing, 9(4):691–701, 2015. 3

[33] V.-M. Patel, H. V. Nguyen, and R. Vidal. Latent space sparse
subspace clustering. In Proceedings of IEEE International
Conference on Computer Vision, pages 225–232, Dev 2013.
3

[34] V.-M. Patel. and R. Vidal. Kernel sparse subspace clustering.
In Proceedings of IEEE International Conference on Image
Processing, pages 2849–2853, 2014. 3, 6

[35] X. Peng, J. Feng, S. Xiao, J. Lu, Z. Yi, and S. Yan. Deep s-
parse subspace clustering. arXiv preprint arXiv:1709.08374,
2017. 1, 2, 3

[36] X. Peng, S. Xiao, J. Feng, W. Y. Yau, and Z. Yi. Deep sub-
space clustering with sparsity prior.
In International Join-
t Conference on Artiﬁcial Intelligence, pages 1925–1931,
2016. 1, 2, 3

[37] X. Peng, Z. Yu, Z. Yi, and H. Tang. Constructing the l2-graph
for robust subspace learning and subspace clustering. IEEE
Transactions on Cybernetics, 47(4):1053–1066, 2017. 1, 2

[38] X. Qi, R. Xiao, C.-G. Li, Y. Qiao, J. Guo, and X. Tang.
Pairwise rotation invariant co-occurrence local binary pat-
tern. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 36(11):2199–2213, 2014. 1

[39] F.-S. Samaria and A.-C. Harter. Harter, a.: Parameterisation
of a stochastic model for human face identiﬁcation. In Pro-
ceedings of the Second IEEE Workshop on Applications of
Computer Vision, pages 138–142, 1994. 6, 7

[40] R. Shankar, T. Roberto, R. Vidal, and Y. Ma. Motion segmen-
tation in the presence of outlying, incomplete, or corrupted
trajectories. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 32(10):1832–1845, 2010. 1

[41] C. Tomasi and T. Kanade. Shape and motion from image
streams under orthography: a factorization method. Interna-
tional Journal on Computer Vision, 9(2):137–154, 1992. 1

[42] R. Vidal. Subspace clustering. IEEE Signal Processing Mag-

azine, 28(2):52–68, 2011. 1

[43] R. Vidal and P. Favaro. Low rank subspace clustering (lrsc).

Pattern Recognition Letters, 43:47–61, 2014. 6

[44] R. Vidal, Y. Ma, and S. Sastry. Generalized Principal Compo-
nent Analysis (GPCA). IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(12):1–15, 2005. 1

[45] R. Vidal, Y. Ma, and S. Sastry. Generalized Principal Com-

ponent Analysis. Springer Verlag, 2016. 1, 2, 4

[46] Y. X. Wang, H. Xu, and C. Leng. Provable subspace cluster-
ing: when lrr meets ssc. In Neural Information Processing
Systems (NIPS), pages 64–72, 2013. 2

[47] S. Xiao, M. Tan, D. Xu, and Z.-Y. Dong. Robust kernel low-
rank representation. IEEE Transactions on Neural Networks
and Learning Systems, 27(11):2268–2281, 2016. 3

[48] B. Xin, Y. Wang, W. Gao, and D. Wipf. Building invariances
into sparse subspace clustering. IEEE Transactions on Signal
Processing, 66(2):449–462, 2018. 2

[49] H. N. W. Yang, F. Shen, and C. Sun. Kernel low-rank repre-
sentation for face recognition. Neurocomputing, 155:32–42,
2015. 3

5482

