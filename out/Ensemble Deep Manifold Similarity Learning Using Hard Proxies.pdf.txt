Ensemble Deep Manifold Similarity Learning using Hard Proxies

Nicolas Aziere and Sinisa Todorovic

School of EECS, Oregon State University

azieren@oregonstate.edu, sinisa@oregonstate.edu

Abstract

This paper is about deep image-similarity learning such
that images of the same class have more similar deep
feature representations than those belonging to different
classes. For learning, prior work typically speciﬁes loss
in terms of ℓ2-distances or dot-products between deep fea-
tures, despite the well-known non-Euclidean nature of deep
feature spaces. Our ﬁrst contribution is in specifying the
N -pair loss using a manifold similarity of deep features.
We introduce a new time- and memory-efﬁcient estimation
of the manifold similarities that uses a closed-form conver-
gence solution of the Random Walk algorithm. We randomly
partition the deep feature space, and express the manifold
similarities via representatives of the resulting subspaces,
a.k.a. proxies. Multiple random partitions of the deep fea-
ture space gives an ensemble of proxies which can be jointly
used for estimating image similarity. Our second contri-
bution is aimed at reducing overﬁtting by estimating hard
proxies that are as close to one another as possible, but re-
main in their respective subspaces. We outperform the state
of the art in both image retrieval and clustering on the CUB-
200-2011, Cars196, and Stanford Online Products datasets
with the same complexity as related ensemble methods.

1. Introduction

This paper presents an approach to deep metric learn-
ing. Our objective is to learn deep representations of im-
ages such that images belonging to the same class have
more similar representations than those belonging to differ-
ent classes. This is an important problem with a wide range
of applications, including image retrieval [16, 17], image
clustering [18], and ﬁne-grained image classiﬁcation [4,11].
Recent work uses a convolutional neural network (CNN)
to compute the image’s deep feature which satisﬁes the
above objective. The CNN is typically trained with the
contrastive loss [2], triplet loss [7, 21], or N -pair loss [17].
These loss functions are usually speciﬁed in terms of ℓ2-
distances or dot-products which are ill-suited to the highly
non-Euclidean deep feature space.

Recent works address this issue by: (i) Specifying loss as
a function of manifold distances of deep features [1,8,9]; or
(ii) Partitioning the deep feature space ﬁrst into subspaces,
then projecting data to a new space spanned by represen-
tatives of the subspaces, and ﬁnally estimating loss in the
new space [10, 12, 13, 23]. Both groups of methods have
shortcomings that we seek to alleviate.

The ﬁrst group of methods uses the Random Walk algo-
rithm [24] for estimating a geodesic distance (or similarity)
between data points on a manifold, referred to as manifold
distance (or manifold similarity). However, incorporating
Random Walk in deep learning is difﬁcult, because the for-
mer requires access to all data whereas deep learning orga-
nizes training in mini-batches. Also, Random Walk, being
iterative, signiﬁcantly increases complexity of training.

Weaknesses of the second group of methods pertain to
speciﬁcation of the new embedding space. For example,
deep features are projected onto the new space spanned by:
(a) One-hot vectors [23], or (b) Randomly sampled vectors,
a.k.a. proxies [12]. Both one-hot vectors and randomly
sampled proxies are heuristic and not learned end-to-end.

Toward addressing the aforementioned shortcomings,
we make two contributions in training of our CNN, as il-
lustrated in Fig. 1 (left). Our ﬁrst contribution is in speci-
fying a time- and memory-efﬁcient algorithm for estimating
loss in terms of manifold similarities between deep features.
Unlike prior work, we adapt Random Walk to estimate the
manifold similarities of only a small number of data in each
mini-batch of deep learning, rather than on all training data.
This allows for efﬁciently computing the manifold similari-
ties using the closed-form convergence solution of Random
Walk, rather than running its many iterations.

Following the above second group of approaches, we
randomly partition the training dataset, where each partition
represents a meta-class of images. A meta-class may com-
prise only a part of images belonging to one class, or images
from several distinct image classes. Similar to [12], we take
representatives of the meta-classes to stand in as proxies for
images when estimating the N-pair loss in training. Specif-
ically, the N-pair loss on all images that got assigned to a
meta-class is computed as a loss of the proxy vector repre-

7299

Figure 1. (Left) An overview of our two contributions in training of a CNN whose output deep representations of images x should satisfy
the objective that images belonging to the same class have more similar deep representations than those belonging to different classes.
(Right) Our training produces an ensemble of CNNs, where each is learned on a particular random partition of the deep feature space into
meta-classes. For testing, we integrate all learners in the ensemble for computing similarity between the query and other test images.

senting that meta-class. As the proxy set is much smaller
than the training dataset, the number of image triplets com-
monly considered when estimating the N-pair loss can be
signiﬁcantly reduced. Also, as shown in [12], minimiz-
ing loss expressed in terms of proxies in place of images
amounts to minimizing an upper bound of the ranking loss
between images, and hence effectively enforces the desired
distance relationships of images.

Our second contribution is in specifying a new algo-
rithm for estimating hard proxies. This is aimed at tight-
ening the upper bound on the desired distance relationships
between training images. When the related work [12, 23]
randomly samples proxies, they risk overﬁtting to a rel-
atively “loose” upper bound of the ranking loss between
images. Rather than using random sampling, we optimize
proxies so that they incur a maximum N-pair loss. Essen-
tially, this means that we search in the deep space for prox-
ies that are as close to one another as possible, but still be-
long to their respective subspaces of the deep feature space
corresponding to the meta-classes. Because the proxies are
made very similar to one another, violating the distance re-
lationships between the images is made easier, and thus our
end-to-end training of deep features is enforced to be more
precise toward reducing these violations.

Some of the issues related to overﬁtting and random par-
titioning of the deep feature space can be overcome with
ensemble learning [10, 13, 23]. We follow this line of work,
and resort to multiple random splits of the deep feature
space, as the optimal partitioning is unknown. Each ran-
dom partitioning gives the corresponding dictionary of hard
proxies that are used to deﬁne the N-pair loss for learning
the CNN – that is, one learner in the ensemble.

In testing, the deep image representation is computed by
concatenating or averaging the deep features produced by
every CNNs of the ensemble, as illustrated in Fig. 1 (right).

Our evaluation demonstrates that we outperform the

state of the art in both image retrieval and clustering on
the benchmark datasets,
including CUB-200-2011 [20],
Cars196 [11], and Stanford Online Products [19], with the
same computational complexity per one CNN in the ensem-
ble as non-ensemble methods.

In the following, Sec. 2 reviews prior work, Sec. 3 gives
an overview of our approach, Sec. 4 formulates hard prox-
ies, Sec. 5 explains how to compute manifold similarities
with Random Walk, Sec. 6 formulates two manifold loss
functions, Sec. 7 describes complexity, Sec. 8 speciﬁes im-
plementation details, and Sec. 9 presents evaluation.

2. A Review of Related Work

Distance Metric Learning is a long-standing problem.

The section reviews only the most closely related work.

Loss formulations: Loss is usually deﬁned on triplets of
images for taking into account image distances both within
a class and across distinct classes in training. For exam-
ple, the N-pair loss [17, 19] is computed on a mini-batch
of training images comprising one anchor, one positive im-
age from the same class of the anchor’s, and many negative
images from different classes. The angular loss [22] rep-
resents a variation of the N-pair loss, aimed at both min-
imizing the angles between intra-class features and maxi-
mizing the angles between inter-class features. The facility
location function is speciﬁed to improve image clustering
quality measured by normalized mutual information (NMI)
rather than directly optimize image distances [18]. A class-
level tree capturing intrinsic and contextual information of
the dataset is used to adaptively estimate the margin in the
triplet loss [5]. These loss functions are usually speciﬁed in
terms of ℓ2-distances or dot-products of deep features. In
this work, we use the N-pair loss for training, and advance
the related work by specifying the loss in terms of manifold
similarities between deep features as more appropriate for
the highly non-Euclidean deep feature space.

7300

Addressing the large number of image triplets: Se-
lecting optimal image triplets for more efﬁcient and effec-
tive distance metric learning can be done, e.g., by smart
mining in the large space of training triplets [7], or by adver-
sarial metric learning on synthetic hard negatives generated
from the observed negative samples [3].

The most closely related method to ours in this group
uses proxies to substitute for the original data points (hence
reducing the large sampling space of image triplets), such
that a loss over the proxies is a tight upper bound of the orig-
inal loss over the images [12]. However, they use the same
or double the number of proxies as the number of images
classes, whereas we use signiﬁcantly fewer proxies to avoid
overﬁtting. Also, their randomly sampled proxies are ﬁxed,
while learning enforces the distribution of deep features to
choose the proxies as their cluster centers. In contrast, we
optimize the proxies to become hard examples that are difﬁ-
cult to learn. That is, we enforce the proxies to move away
from centers of their respective subspaces, and maximize
the N-pair loss, in order to avoid overﬁtting.

Manifold distance learning: A few approaches seek to
estimate a manifold structure of the image dataset for dis-
tance metric learning. To this end, they use the PageRank
algorithm [24], or the diffusion process on a region man-
ifold [1, 9]. Some of these methods also consider selec-
tion of optimal training examples via unsupervised mani-
fold guided selection of image triplets [8]. However, all
these approaches estimate the manifold distances in a post-
processing step by ﬁxing the deep features. In contrast, we
integrate estimation of the manifold distances into our end-
to-end training of deep features.

Ensemble Learning: Ensemble learning is aimed at re-
ducing variance among a family of learners, which typi-
cally leads to performance improvements. In deep metric
learning, each member of the ensemble votes for a distance
between two points, and the ﬁnal distance is estimated by
integrating all of the votes. For example, the last embed-
ding layer of a deep network can be divided into an embed-
ding ensemble, and trained using the online gradient boost-
ing [13]. Also, different learners can be deﬁned using a
family of attention masks, resulting in an attention-based
ensemble [10]. Similar to [23], we use randomized ensem-
bles, where each learner is deﬁned by a particular random
partitioning of the deep feature space.

3. Our Approach

an

Our

learns

approach

ensemble

of CNNs,
E = {CNN(e)
: e = 1, . . . , E}, where each CNN(e)
embeds an input image, I, to the normalized deep fea-
ture x(e) = ˜x(e)
|˜x(e)| , as illustrated in Fig. 1 (right). Thus,
in this work, all image embeddings are normalized to
the unit sphere. The CNNs in E have the same deep

architecture, but are independently learned on a given
random partitioning of the training set of images. Ran-
domized ensembles like ours, have been shown to improve
performance over individual members of the ensemble [23].

In testing, we ﬁrst pass every test image In through E, re-
sulting in the deep representation xn = [x(1)
n , · · · , x(E)
n ].
These deep features are then used for estimating image
similarities, and ﬁnally evaluation on the image retrieval
or clustering problems. We specify similarity between the
query image Iq and another test image In as

s(xq, xn) =

E

X

e=1

α(e) s(x(e)

q , x(e)

n ),

(1)

where {α(e)} are relative importance weights of the CNNs
in the ensemble, and s(xq, xn) is deﬁned as the dot-product
of input features:

s(xq, xn) = xq · xn.

(2)

Note that when α(e) = 1, for e = 1, . . . , E, our ensem-
ble integration amounts to concatenating all outputs of the
CNNs in the ensemble for computing the image similarity
in (1), s(xq, xn) = xq · xn.

While {α(e)} could be estimated on a validation set
using various boosting algorithms, in our experiments, we
did not observe signiﬁcant differences in our performance
from the case when the relative weights are all set to 1.
Therefore, in this paper, we use α(e) = 1, for e = 1, . . . , E.

In training, we learn independently each of the CNNs in
the ensemble on a given training set of images and their
class labels, D = {(In, yn)}. After the CNN computes
deep features of training images, we estimate their mani-
fold similarity relationships. Any violations of the desired
manifold similarity relationships incurs loss, which is then
backpropagated for training the CNN.

Following recent approaches [17, 19, 22], in this paper,
we use the smooth, differentiable N-pair loss which efﬁ-
ciently takes advantage of all training images in a mini-
batch, rather than taking into account individual image
triplets. Speciﬁcally, in [17, 19, 22], each training mini-
batch consists of N samples, where one image x is called
anchor, another positive image x+ comes from the same
class as the anchor, and the remaining N − 2 negative im-
ages {x−
n } belong to classes that are different from the an-
chor’s. These approaches deﬁne the N-pair loss so as to
reduce similarity between the anchor x and the negatives
{x−
n }, and simultaneously increase similarity between the
anchor x and the positive image x+:

LN-pair(x, x+, {x−

n }) = log (cid:16)1+

N −2

X

n=1

es(x,x−

n )−s(x,x+)+m(cid:17),
(3)

7301

where m ≥ 0 is a constant margin, and s(·) is a similarity
function, e.g., given by (2).

Our extension of prior work is two-fold. Our N-pair loss
uses manifold similarity instead of their similarity, and an
optimized set of proxies in place of actual training images.
Before we specify our N-pair loss in Sec. 6, we ﬁrst describe
how to estimate the proxies in Sec. 4, and how to compute
manifold similarities of a training mini-batch in Sec. 5.

the desired similarity relationships between training im-
ages. This is because, our LP represents an upper bound of
LN-pair, so minimizing LP effectively reduces LN-pair. This
is straightforward to show by following very similar deriva-
tion steps to those presented in [12]. From (3)–(4) and the
triangle inequality, an absolute difference of the two losses
∆L = |LN-pair − LP | for an image triplet (x, x+, x−) can
be bounded as

4. Hard Proxies

We seek to address the following two challenges in our
CNN training: (1) How to accurately estimate similarity be-
tween images for computing the N-pair loss given by (3) in
the highly non-Euclidean deep feature space; and (2) How
to efﬁciently select optimal training images for (3) from the
large sampling space of image triplets.

4.1. Proxy N pair Loss

To address the ﬁrst challenge, we follow [23] and ran-
domly partition the training dataset D into K disjoint sub-
sets, D = ∪K
k=1Dk, where K is signiﬁcantly less than the
number of image classes (e.g., 10%). We expect that deep
features of each partition Dk will exhibit properties closer
to a Euclidean space than the entire deep space of D. Im-
ages in Dk may come from one or more classes, and we
say that Dk deﬁnes a meta-class. Under such a partitioning,
we generalize the notion of positive and negative images of
an anchor, mentioned in Sec. 3. Speciﬁcally, for an anchor
image x from Dk, positive images x+ belong to the same
subset Dk, and negative images x− belong to the other sub-
sets Dj , j 6= k.

For the second challenge, we use a similar strategy as
that introduced in [12]. In every subset Dk, we randomly
select an image to represent this meta-class, and use its nor-
malized deep feature pk = ˜pk
| ˜pk| as a proxy for all other im-
ages in Dk when estimating the N-pair loss. In this way, we
form the initial set of proxies P = {pk : k = 1, . . . , K}.

After obtaining P, we estimate the proxy N-pair loss
LP (x, x+, {x−
n }) in a similar way to the expression in (3).
For an anchor image x in Dk, we replace its positive x+
with pk, and the negatives {x−
n } with their respective prox-
ies {pj}, j 6= k, resulting in the proxy N-pair loss:

K

j=1,j6=k

X

LP (x, x+, {x−

n }) = log (cid:16)1 +

es(x,pj )−s(x,pk)+m(cid:17).
(4)
From (4), we effectively alleviate the issue of optimal se-
lection of the positive and negative images for the training
mini-batch, since the loss depends only of the anchor im-
age and the signiﬁcantly fewer proxies than the number of
original image classes, LP (x, x+, {x−

n }) = LP (x).

Importantly, the proxy loss LP in (4) maintains the char-
acteristics of the N-pair loss LN-pair in (3) and enforces

∆L = (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

log

1 + es(x,x−)−s(x,x+)+m
1 + es(x,pj )−s(x,pk)+m

,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≈ |[s(x, x−) − s(x, x+)] − [s(x, pj) − s(x, pk)]|,
= |[d(x, x+) − d(x, x−)] − [d(x, pk) − d(x, pj)]|,
≤ 2ǫ,

(5)
where we deﬁne feature distance d(x, x′) = 1 − s(x, x′)
as all our features are normalized to the unit sphere, and
ǫ = maxx d(x, p(x)), and p(x) is the proxy of x. It follows
that the expectation of the N-pair loss can be bounded over
training images as

E[LN-pair] ≤ E[LP ] + P r[|d(x, pk) − d(x, pj)| ≤ 2ǫ].

(6)
Since our deep features and proxies are normalized to the
unit sphere, the upper bound in (6) is tight.

4.2. Estimation of Hard Proxies

In this work, the initial set of images selected as proxies
is ﬁxed for the entire duration of our training. But, in ev-
ery epoch of training, we ﬁrst recompute their deep features
P, and then estimate an optimal set of proxies P ∗ for com-
puting the optimal proxy loss LP ∗ over all training mini-
batches in the next epoch.

This is our main difference from prior work [12], since
their proxies P remain unchanged in learning, and their
CNN is trained to produce deep features which cluster well
around the proxies with respect to a distance metric. In our
experiments, however, we observe that this leads to overﬁt-
ting, because, in part, clustering in the non-Euclidean deep
space using a distance metric gives suboptimal results. An-
other reason for overﬁtting comes from the random outcome
of selecting proxies, which may make the proxy loss LP a
looser upper bound of the N-pair loss LN-pair. Hence, min-
imizing such a LP may have little effect on enforcing the
desired similarity relationships between training images.

To address overﬁtting, our key idea is to estimate optimal
proxies, P ∗ = {p∗
k : k = 1, . . . , K}, so as to maximally
reduce the difference in (6), |d(x, p∗
j )|, j 6= k,
and in this way make LP ∗ a tighter upper bound of LN-pair
than the initial LP . One way to achieve this is to make
all proxies similarly distant from all data points in the deep
feature space, resulting in ideal |d(x, pk) − d(x, pj)| ≈ 0,
for all x and pk 6= pj .

k) − d(x, p∗

7302

Figure 2. Optimization of proxies:
the white lines mark meta-
classes, black dots represent images xn, yellow dots mark the ini-
tial randomly sampled proxies pk, and white dots indicate the esti-
mated hard proxies p∗
k away from
images xn it represents toward the other meta-classes, while regu-
larizes that p∗

k. Our optimization “pushes” p∗

k remains close to pk.

As illustrated in Fig. 2, this objective can be achieved
by maximizing distances between p∗
k and images in Dk,
for every meta-class Dk. In turn, this will make every p∗
k,
k = 1, . . . , N , closer to the other meta-classes. To avoid
a trivial solution, where all proxies are equal, we regularize
this objective such that the optimal p∗
k is not too far from the
initial pk. This gives the following optimization for every
meta-class Dk:

p∗
k = arg min
p∈Rl

log (cid:16)1 + X

xn∈Dk\{pk}

es(p,xn)−s(p,pk)(cid:17), (7)

where l is the length of deep features, the ﬁrst term in the
exponent “pushes” the optimal proxy away from images xn
it represents toward other meta-classes, and the second term
in the exponent regularizes against trivial solutions. We efﬁ-
ciently solve (7) with gradient descent, with the initial value
set to pk of a randomly selected image in Dk.

One consequence of making every p∗

k become closer to
k ∈ P ∗ be-
the other meta-classes Dj , j 6= k, is that all p∗
come close to each other. This makes minimization of the
optimal proxy loss LP ∗ given by (4) difﬁcult, because the
CNN has to produce more accurate deep features for re-
specting the desired similarity relationships that x from Dk
should be more similar to pk than to the other proxies pj ,
j 6= k. Therefore, we call P ∗ the set of hard proxies.

5. Manifold Similarity Estimation

We have empirically observed that estimating similari-
ties between images and the proxies in (4), exp(s(x, p∗
j ) −
s(x, p∗
k)+m), j 6= k, often gives inaccurate results, because
meta-classes {Dk} are highly non-convex sets in the deep
feature space. Therefore, rather than using the dot-product
for estimating s(xn, p∗
k), our next contribution is to estimate
geodesic similarities, {fnp∗
k) : k = 1, . . . , K},
on a manifold, which we call manifold similarities, and

= f (xn, p∗

k

Figure 3. Illustration of vectors of manifold similarities Fn, fn,
and fp∗
for estimating the manifold
k
proxy loss.

, where we use fn and fp∗
k

use them for computing the manifold proxy loss. Impor-
tantly, our key novelty is in estimating {f (xn, p∗
k)} for ev-
ery mini-batch of our end-to-end training, rather than com-
puting manifold similarities pre- or post-training on all data,
as common in prior work [1, 8, 9].

We compute the manifold similarities of images in a
training mini-batch B with the Random Walk algorithm [24]
on a nearest neighbor graph. Each B is constructed from N
training images and K hard proxies:

B = {x1, . . . , xn, . . . , xN , p∗

1, . . . , p∗

k, . . . , p∗

K}.

(8)

k) = xn · p∗

For B, we ﬁrst compute the (N + K) × (N + K) symmet-
rically normalized adjacency matrix, ¯S = D−1/2SD−1/2,
where elements of S are dot-products s(xn, xn′ ) = xn · xn′
or s(xn, p∗
k, and D is the diagonal degree ma-
trix with elements D(n, n) equal to a sum of the nth row
in S. As similarity of each xn or p∗
k to itself is irrelevant
(and also to avoid loops in Random Walk), we set all diag-
onal elements of ¯S to 0. Then, for each image xn ∈ B, we
estimate a vector of its manifold similarities to images in
B, Fn = [fn1, . . . , fnN , fnp1 , . . . , fnpK ], using the closed-
form convergence solution of Random Walk [24] as

Fn = (1 − α)(I − α ¯S)−1en

(9)

where α ∈ (0, 1) is a probability of restarting Random Walk
from nth query point, en is the query one-hot vector with 1
at nth location, and I is the (N + K) × (N + K) identity
matrix. Since the size of mini-batch (N + K) is relatively
small, designed to ﬁt the available RAM memory, comput-
ing the inverse matrix in (9) can be done efﬁciently.

As explained in the next section, the manifold proxy loss
, which are parts of their
consist of only
k to the proxies, respec-

is deﬁned in terms of fn and fp∗
respective vectors Fn and Fp∗
manifold similarities of xn and p∗
tively, as illustrated in Fig. 3:

. fn and fp∗

k

k

k

fn = [fnp∗
= [fp∗
fp∗

1

k

kp∗
1

, . . . , fnp∗
, . . . , fp∗

k

, . . . , fnp∗
, . . . , fp∗

K

kp∗
k

],

kp∗
K

].

(10)

7303

6. Two Manifold Proxy Loss Functions

We extend the optimal proxy loss LP ∗ given by (4) to
account for manifold similarities between images and the
hard proxies in a mini-batch of N training images. In this
paper, we consider two extensions: (1) Intrinsic Lint, and
(2) Contextual Lcxt loss functions.

The intrinsic manifold proxy loss is computed as

Lint({xn}) =

1
N

N

X

n=1

log (cid:16)1 +

K

X

j=1,
j6=k

fnp∗
j

−fnp∗
k

e

+m(cid:17), (11)

where p∗
k is the hard proxy of image xn if xn ∈ Dk, and
fnp∗
· are elements of the manifold similarity vector fn given
by (10). Having the similar formulation as LN-pair and LP ∗ ,
the intrinsic loss Lint({xn}N
1 ) in (11) inherits advantages of
the N-pair loss and the proxy loss in addressing the large
sampling space of image triplets, discussed in Sec. 3.

The contextual manifold loss introduces additional con-
straints relative to Lint for a stronger enforcement of the de-
sired similarity relationships. We additionally constrain that
each image xn ∈ Dk “sees” the set of hard proxies P ∗ as
its proxy p∗
k “sees” P ∗. That is, if P ∗ represents well the
entire set of data D, then the desired similarity relationships
between every xn ∈ Dk and other images in D should be
close to the similarity relationships between p∗

k and P ∗.

Lcxt({xn})=

1
N

N

X

n=1

log (cid:0)1+

e

K

X

j=1,
j6=k

s(fn,fp∗
j

)−s(fn,fp∗
k

)+m(cid:1),

(12)
k is the hard proxy of image xn if xn ∈ Dk, fn and

are given by (10), and s(fn, fp∗

) = fn · fp∗

.

k

k

where p∗
fp∗

k

Tab. 1 compares our training complexity with that of the
state of the art. As can be seen, our training complexity is
the same as that of [23], and higher than that of [12] only
for the additional ensemble learning.

Methods

Training Complexity

N-Pair [17]

Proxy-NCA [12]

DREML [23]
Our approach

O(M 3)
O(M K 2)
O(EM K 2)
O(EM K 2)

Table 1. Training complexity of the most related methods to ours.

8. Implementation Details

For implementation, we use Pytorch [14].

In pre-
processing, images are normalized and re-sized to 256×256
pixels. We used standard data augmentation techniques in-
cluding random image cropping and rotation. The mini-
batch size is set to N = 128. The size of our ensemble of
CNNs is E = 25. The Random Walk parameter α = 0.8,
and the margin m = 5×10−4. For the hard proxy optimiza-
tion, the learning rate is set to 10−3. The empirically-found
best number of proxies (one per meta-class) is K = 50
for all datasets. For comparison with prior work, we im-
plemented both ResNet18 and GoogLeNet pre-trained on
ImageNet [15]. The last fully-connected layer is modiﬁed
to set the dimension of deep features l = 128. We used an
Adam optimizer parameterized with the weight decay fac-
tor of 10−5. The learning rate is initialized at 10−4, and
decreased by a factor of 0.1 every three epochs, over a total
of 10 epochs per training. Overall, the training of one CNN
takes about 1 hour on a Tesla K80 GPU.

7. Complexity Analysis

9. Results

We are given a training dataset, D, with size |D| = M ,
and M 3 image triplets for computing the triplet loss. We use
E random partitions of D into K meta classes for ensemble
learning. The E distinct CNNs are trained in parallel, so
our runtime actually does not increase E times.

Our training complexity per one epoch of a single CNN
is derived as follows. As illustrated in Fig. 1, we have
three main computational steps. First, we optimize K prox-
ies over M data by minimizing Eq. 7, which amounts to
O(M K). Second, in each iteration over M
N batches, where
N is the mini-batch size, we compute an inverse of the
(N+K)x(N+K) manifold-similarity matrix, which amounts
N (N + K)3) = O(M K 2), as we set K ≈ N
to O( M
2
(not sensitive, see Sec. 8). Third, we compute the N-pair
loss over M training data and K proxies, which amounts
to O(M K 2). Finally, our total complexity is O(M K) +
O(M K 2) + O(M K 2) = O(M K 2).

Datasets: Evaluation is performed on the image retrieval
and clustering problems using the following three bench-
mark datasets. CUB 200-2011 [20] has 11,788 images
showing 200 bird classes. The data is split into 5,864 im-
ages of the ﬁrst 100 classes are used for training, and 5,924
images of the remaining classes for testing. Cars196 [11]
has 16,185 images of 196 car classes. The data is split into
8,054 training images of the ﬁrst 96 classes and 8,131 test
images of the remaining car classes. Stanford Online Prod-
uct [19] has 120,053 images with 22,634 classes. The data
is split into 59,551 training images of the ﬁrst 11,318 classes
and 60,502 testing images of the remaining classes from the
dataset. The aforementioned training-test splits are standard
and used by all prior work that we compare with.

Evaluation Metrics: In image retrieval, given a query
test image, we ﬁnd its K nearest neighbors from the test
set. We calculate a percentage of the retrieved images R@K

7304

that have the same class as the query. Image clustering is
performed using the K-means algorithm, where K is the
number of image classes, and evaluated with the normal-
ized mutual information (NMI). For the obtained set of clus-
ters, ˆΩ = {ˆω1, ..., ˆωK}, and the ground-truth image cluster-
ing by their classes, Ω = {ω1, ..., ωK}, NMI is deﬁned as
NMI( ˆΩ, Ω) = 2I( ˆΩ,Ω)
, where I(·) denotes the mutual
information, and H(·) is entropy.

H( ˆΩ)+H(Ω)

Ablation Study: We test performance effects of various
components of our approach, especially our claimed contri-
butions, using the following variants of our approach:

• EDMS (RW, P*) is our full approach Ensemble Deep
Manifold Similarity learning using Random Walk
(RW) and the hard proxies for estimating Lcxt. For
testing, we use the dot-product similarity, where the
ensemble of our CNNs is fused as speciﬁed in (1) (see
Sec. 3 and Fig. 1).

• EDMS (RW, P) does not optimize the proxies, but
computes Lcxt using the initial P , and thus tests the
effect of using P ∗ vs. P on performance.

• EDMS (RW-int, P*) replaces Lcxt with Lint, and thus

evaluates the contextual vs. intrinsic manifold loss.

• EDMS (P*) replaces Lcxt with LP ∗ given by (4), and
thus tests the effect of Random Walk on performance.

• EDMS (w/o) replaces Lcxt with LN-pair given by (3).
We still perform random partitioning of the training
set, and compute LN-pair with respect to meta-classes.

• EDMS (RW) is similar to EDMS (w/o) but computes
LN-pair with manifold similarities f (xn, xn′ ) with re-
spect to meta-classes.

Baselines: We compare with with the following state-
of-the-art approaches using the same testing setup as theirs.
Proxy-NCA [12] uses the proxies to estimate the NCA loss
[6]. Lifted Structure [19], N-pair [17] and Angular [22] use
the N-pair loss. BIER [13], ABE [10] and DREML [23] use
ensemble learning. For comparison with DREML [23], we
use their latest results published on arXive and linked on the
Github page.

Method

NMI R@1 R@2 R@4 R@8

EDMS (w/o)
EDMS (RW)
EDMS (RW, P)
EDMS (RW-int, P*)
EDMS (P*)
EDMS (RW, P*)

66.4
63.4
66.8
66.9
67.2
68.9

58.7
56.1
60.3
61.1
63.7
66.1

71.4
69.2
71.5
72.2
74.2
76.7

81.2
79.5
81.3
81.7
82.9
85.5

89.4
87.6
89.1
89.3
89.7
91.4

Table 2. Our ablation study on the CUB dataset: Image clustering
and retrieval results for different variants of our approach using
ResNet18.

9.1. Quantitative Results

Fig. 4 shows how performance of our EDMS (RW, P*)
changes as a function of the ensemble size, number of prox-
ies, and deep-feature dimension on CUB-200-2011. We ob-
serve on Fig. 4 (left) that the accuracy saturates after a cer-
tain ensemble size, and as a good trade off between com-
plexity and accuracy we set the number of CNNs in the
ensemble E = 25. From Fig. 4 (right), we get the best
results for K = 50 proxies, and deep-feature dimension of
l = 128. We use these parameters for all variants of our
approach on all three datasets.

Tab. 2 presents our ablation study with the six variants
of our approach on CUB-200-2011. For EDMS (RW) our
recall decreases relative to EDMS (w/o) when no prox-
ies are used. This suggests that our estimation of mani-
fold similarities on a relatively small mini-batch may not
be able to reliably capture the true geodesic distances be-
tween images without the help of the proxies. This is fur-
ther seen in EDMS (RW, P), where by adding the prox-
ies to EDMS (RW), we get performance improvement over
both EDMS (w/o) and EDMS (RW). A good performance
of EDMS (P*) suggests that our random partitioning of the
training set and the use of proxies help estimate image simi-
larities reliably, even without Random Walk. Using the con-
textual loss in EDMS (RW, P*) gives much better recall than
using the intrinsic loss in EDMS (RW-int, P*). Finally, our
optimization of hard proxies in EDMS (RW, P*) improves
performance relative to that of EDMS (RW, P).

Tables 3 and 4 compare our best performing EDMS (RW,
P*) with the baselines. When using ResNet18 as the CNN,
we outperform the state-of-the-art ensemble learning meth-
ods BIER, ABE and DREML. We observe that GoogLeNet
gives lower results than ResNet18 for our approach.

From Fig. 4, we can use small E < 5, and still sig-
niﬁcantly outperform [12] (see Tab. 3). Our gains in per-
formance over non-ensemble-learning methods justify the
slight increase in runtime for fusing E CNNs.

Figure 4. Optimal selection of the ensemble size, number of prox-
ies, and deep-feature dimension for EDMS (RW, P*) on CUB-200-
2011 with respect to Recall. (Left) Our recall as a function of the
ensemble size. (Right) Recall@1 as a function of the number of
proxies, and size of deep features.

7305

Dataset

CUB-200-2011

Car196

Method

Network

NMI

R@1 R@2 R@4 R@8

NMI R@1 R@2 R@4 R@8

Lifted [19]
Proxy-NCA [12]
N-pair [17]
Angular [22]
BIER [13]
ABE [10]
DREML [23]

GoogLeNet
InceptionBN
GoogLeNet
GoogLeNet
GoogLeNet
GoogLeNet
ResNet18

EDMS (RW, P*)
EDMS (RW, P*)

GoogLeNet
ResNet18

55.38
59.5
60.4
61.1

-
-

67.8

64.5
68.9

47.2
49.2
51.0
54.7
55.3
60.6
63.9

61.6
66.1

58.9
61.9
63.3
66.3
68.4
71.5
75.0

72.1
76.7

70.2
67.9
74.3
76.0
76.9
79.8
83.1

81.8
85.5

80.2
72.4
83.2
83.9
85.1
87.7
89.7

88.9
91.4

55.1
64.9
64.0
63.2

-
-

76.4

75.1
76.7

48.3
73.2
71.1
71.4
78.0
85.2
86.0

85.6
87.6

61.1
82.4
79.7
81.4
85.8
90.5
91.7

90.8
92.1

71.8
86.4
86.5
87.5
91.1
94.0
95.0

94.8
95.2

81.1
88.7
91.6
92.1
95.1
96.1
97.2

96.1
97.3

Table 3. Image clustering and retrieval results on the CUB-200-2011 and Cars196 datasets.

Method

NMI R@1 R@10 R@100 R@1000

Lifted [19]
N-pair [17]
Angular [22]
BIER [13]
ABE [10]

Ours+G
Ours+R

87.4
87.9
88.6

-
-

89.0
90.1

63.0
67.7
70.9
74.2
76.3

77.2
78.5

80.5
83.8
85.0
86.9
88.4

89.1
90.7

91.7
93.0
93.5
94.0
94.8

94.9
95.2

97.5
97.8
98.0
97.8
98.2

98.1
98.5

Table 4. Image clustering and retrieval results on Stanford On-
line Products. All the competing approaches use the GoogLeNet.
Ours+G = EDMS (RW, P*) with GoogLeNet, and Ours+R =
EDMS (RW, P*) with ResNet18.

Figure 5. Our sample retrieval results on the CUB-200-2011
(Left) and Car196 datasets (Right). For each query, we show
4 top retrieved images, where the top row shows the results
with EDMS (P*), and the bottom row shows the results with
EDMS (RW, P*). Using manifold similarities gives more visu-
ally accurate results. Errors in our retrieval are highlighted with
the black frame.

9.2. Qualitative Results

Fig. 5 shows a few sample retrieval results on CUB-200-
2011 and Car196 for EDMS (P*) and EDMS (RW, P*). As
can be seen, using manifold similarities in EDMS (RW, P*)
gives more visually accurate results not only in terms of the
correctly retrieved images from the same class as the query,

but also regarding the 3D pose and background. There are
also some failure cases, which seem to be due to confusing
foreground and background in the images. This could be
addressed in the future by incorporating recent visual atten-
tion techniques.

10. Conclusion

We have presented a new approach to ensemble learn-
ing of deep image representations that should respect their
desired similarity relationships within and across image
classes. Toward addressing the non-Euclidean properties
of the deep feature space, we have made two key contri-
butions in training. First, we have speciﬁed two new loss
functions, called contextual and intrinsic manifold loss, in
terms of geodesic similarity of images on a manifold, which
is efﬁciently estimated for each training mini-batch using
the closed-form solution of Random Walk. For computing
our manifold loss, training images are partitioned into sub-
sets, and their manifold similarity is estimated via randomly
selected representatives of the subsets, called proxies. Our
second contribution pertains to optimizing the proxies such
that the proposed manifold loss enforces stronger con-
straints on learning of the desired similarity relationships.
We have presented an ablation study and comparison with
the state of the art on image retrieval and clustering using
the CUB-200-2011, Cars196, and Stanford Online Prod-
ucts datasets. Our results suggest that estimation of man-
ifold similarities on a relatively small mini-batch may not
be able to reliably capture the true geodesic distances be-
tween images without the help of the proxies. Also, our
random partitioning of the training set and the use of proxies
help estimate image similarities reliably, leading to a com-
petitive performance, even without Random Walk. Our full
approach outperforms the state of the art on both image re-
trieval and clustering, on all three datasets.

Acknowledgment

This work was supported in part by DARPA XAI Award

N66001-17-2-4029.

7306

References

[1] S. Bai, Z. Zhou, J. Wang, X. Bai, L. J. Latecki, and Q. Tian.
Ensemble diffusion for retrieval. In ICCV, pages 774–783,
2017. 1, 3, 5

[2] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similar-
ity metric discriminatively, with application to face veriﬁca-
tion.
In Computer Vision and Pattern Recognition, 2005.
CVPR 2005. IEEE Computer Society Conference on, vol-
ume 1, pages 539–546. IEEE, 2005. 1

[3] Y. Duan, W. Zheng, X. Lin, J. Lu, and J. Zhou. Deep ad-
versarial metric learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2780–2789, 2018. 3

[4] J. Fu, H. Zheng, and T. Mei. Look closer to see better: Recur-
rent attention convolutional neural network for ﬁne-grained
image recognition. In CVPR, volume 2, page 3, 2017. 1

[5] W. Ge, W. Huang, D. Dong, and M. R. Scott. Deep met-
ric learning with hierarchical triplet loss.
In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 269–285, 2018. 2

[6] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov.

Neighbourhood components analysis. In NIPS, 2004. 7

[7] B. Harwood, V. K. BG, G. Carneiro, I. Reid, and T. Drum-
space,

Smart mining for deep metric learning.

mond.
9(13):22. 1, 3

[8] A. Iscen, G. Tolias, Y. Avrithis, and O. Chum. Mining on
manifolds: Metric learning without labels. arXiv preprint
arXiv:1803.11095, 2018. 1, 3, 5

[9] A. Iscen, G. Tolias, Y. S. Avrithis, T. Furon, and O. Chum.
Efﬁcient diffusion on region manifolds: Recovering small
objects with compact cnn representations.
In CVPR, vol-
ume 1, page 4, 2017. 1, 3, 5

[10] W. Kim, B. Goyal, K. Chawla, J. Lee, and K. Kwon.
Attention-based ensemble for deep metric learning. arXiv
preprint arXiv:1804.00382, 2018. 1, 2, 3, 7, 8

[11] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In 4th Interna-
tional IEEE Workshop on 3D Representation and Recogni-
tion (3dRR-13), Sydney, Australia, 2013. 1, 2, 6

[12] Y. Movshovitz-Attias, A. Toshev, T. K. Leung, S. Ioffe, and
S. Singh. No fuss distance metric learning using proxies.
arXiv preprint arXiv:1703.07464, 2017. 1, 2, 3, 4, 6, 7, 8

[13] M. Opitz, G. Waltner, H. Possegger, and H. Bischof. BIER–
boosting independent embeddings robustly. In International
Conference on Computer Vision (ICCV), 2017. 1, 2, 3, 7, 8

[14] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017. 6

[15] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015. 6

[16] O. Seddati, S. Dupont, and S. Mahmoudi. Quadruplet net-
works for sketch-based image retrieval.
In Proceedings of
the 2017 ACM on International Conference on Multimedia
Retrieval, pages 184–191. ACM, 2017. 1

[17] K. Sohn. Improved deep metric learning with multi-class n-
pair loss objective. In Advances in Neural Information Pro-
cessing Systems, pages 1857–1865, 2016. 1, 2, 3, 6, 7, 8

[18] H. O. Song, S. Jegelka, V. Rathod, and K. Murphy. Deep
metric learning via facility location. In Computer Vision and
Pattern Recognition (CVPR), 2017. 1, 2

[19] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016. 2, 3, 6, 7, 8

[20] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical Re-
port CNS-TR-2011-001, California Institute of Technology,
2011. 2, 6

[21] J. Wang, T. Leung, C. Rosenberg, J. Wang, J. Philbin,
B. Chen, Y. Wu, et al. Learning ﬁne-grained image simi-
larity with deep ranking. arXiv preprint arXiv:1404.4661,
2014. 1

[22] J. Wang, F. Zhou, S. Wen, X. Liu, and Y. Lin. Deep met-
ric learning with angular loss. In 2017 IEEE International
Conference on Computer Vision (ICCV), pages 2612–2620.
IEEE, 2017. 2, 3, 7, 8

[23] H. Xuan, R. Souvenir, and R. Pless.

ized ensembles for metric learning.
arXiv:1808.04469, 2018. 1, 2, 3, 4, 6, 7, 8

Deep random-
arXiv preprint

[24] D. Zhou,

J. Weston, A. Gretton, O. Bousquet, and
B. Sch¨olkopf. Ranking on data manifolds.
In Advances
in neural information processing systems, pages 169–176,
2004. 1, 3, 5

7307

