Pay attention! - Robustifying a Deep Visuomotor Policy through

Task-Focused Visual Attention

Pooya Abolghasemi ∗, Amir Mazaheri ∗, Mubarak Shah and Ladislau B¨ol¨oni

University of Central Florida, Orlando, FL 32816

pooya.abolghasemi, amirmazaheri@knights.ucf.edu, shah@crcv.ucf.edu, lboloni@cs.ucf.edu

https://pouyaab.github.io/pay-attention/

Abstract

Several recent studies have demonstrated the promise
of deep visuomotor policies for robot manipulator control.
Despite impressive progress, these systems are known to be
vulnerable to physical disturbances, such as accidental or
adversarial bumps that make them drop the manipulated ob-
ject. They also tend to be distracted by visual disturbances
such as objects moving in the robot’s ﬁeld of view, even if the
disturbance does not physically prevent the execution of the
task. In this paper, we propose an approach for augmenting
a deep visuomotor policy trained through demonstrations
with Task Focused visual Attention (TFA). The manipula-
tion task is speciﬁed with a natural language text such as
“move the red bowl to the left”. This allows the visual at-
tention component to concentrate on the current object that
the robot needs to manipulate. We show that even in benign
environments, the TFA allows the policy to consistently out-
perform a variant with no attention mechanism. More im-
portantly, the new policy is signiﬁcantly more robust: it reg-
ularly recovers from severe physical disturbances (such as
bumps causing it to drop the object) from which the baseline
policy, i.e. with no visual attention, almost never recovers.
In addition, we show that the proposed policy performs cor-
rectly in the presence of a wide class of visual disturbances,
exhibiting a behavior reminiscent of human selective visual
attention experiments.

1. Introduction

Many recent researches show the possibility of end-to-
end training of deep visuomotor policies that perform ob-
ject manipulation tasks such as pick-and-place, push-to-
location, stacking and pouring. These systems perform all
the components of the task (vision processing, grasp and

∗Equal Contribution.

Figure 1. The robot performs a given command. Our proposed net-
work attends the image regions that matter the most, and is robust
to physical and visual disturbance.

trajectory planning and robot control) using a neural net-
work trained by variations of deep reinforcement learning
and learning from demonstration (supervised learning).

Deep visuomotor policies for manipulator control are
neural network architectures that have as input an obser-
vation composed of an image or video frame and possibly
other sensor data, ot, a task (or goal) speciﬁcation, g, and
output robot commands, at = π(ot, g). The robot executes
these commands, enacting a change in the external environ-
ment, which creates a new observation ot+1, and the cycle
repeats. Architecturally, most currently proposed systems
follow variations of the generic model of Figure 1, which
posits the existence of a primary latent encoding, z, the re-
sult of the visual processing of the input by a specialized
visual network. This encoding, of dimensionality orders
of magnitude smaller than the input, is then used by the
motor network to generate the next state joint angles com-
mand, a. While most demonstrations (supervised data) had
been made in unstructured but relatively benign environ-

14254

Primary latentEncoding z=z(o, g)JointCommand a“Pick up the box”Visual Network generating the Task-Focused AttentionVisual/PhysicalDisturbanceTask Spec gRGB CameraRobot Moves(Updates the State) Next FrameVisualVisual observation oMotor Network generating the robot commandsPhysicalments, our own experiments and personal communication
with other researchers had shown that task independent vi-
sual networks for visuomotor policies are highly vulnerable
to physical and visual disturbances. An example of physical
disturbance is the robot arm being bumped such that it drops
the manipulated object. The desired behavior would be for
the robot to immediately notice this, change its trajectory,
pick up the dropped object and continue with the manipu-
lation task. Instead, with an otherwise reliably performing
policy, we notice situations where the robot arm, having lost
the object, continue to go empty-handed through the full tra-
jectory of the manipulation, recovering either much later, or
not at all. A visual disturbance may involve distracting mo-
bile objects appearing in the robot’s ﬁeld of view. Clearly, if
the visual disturbance prevents the execution of the task, for
instance, by blocking the view of the manipulated object, it
is acceptable for the robot to stop or even cancel the manip-
ulation. There are, however, visual disturbances that should
not prevent the execution of the task: for instance, hands
waving in the visual ﬁeld of the robot but not covering the
manipulated object or the robot arm. We have found that in
the case of an task independent visual network, even such
visual disturbances cause the robot to behave erratically –
possibly due to the robot interpreting the situation as a state
never encountered before.

In engineered robot architectures such problems can be
dealt by developing explicit models of the possible distur-
bances, which may allow the robot to reason around the sit-
uation. In deep learning systems, one possible brute-force
solution is to gather more training data containing physical
and visual disturbance events; however, data collection for
robotic tasks is time consuming. Also, there are unlimited
visual and physical disturbance scenarios for a single task.
It is impossible for to record demonstrations to cover all
possible scenarios of physical and visual disturbances.

Pay attention! Task dependent visual network: The
principal idea of this paper is that performance beneﬁts can
be obtained if we make the vision system pay attention to
relevant regions of each frame regarding the current task or
user command. Humans are known to exhibit selective at-
tention - when observing a scene with a particular task in
mind, features of the scene relevant to the task are given
particular attention, while other features are de-emphasized
or even ignored. This had been illustrated in the famous
experiments of Chabris and Simmons [1]. In this paper we
propose Task Focused (Visual) Attention (TFA) as an auxil-
iary network to increase the robustness of the robot manip-
ulator network to physical and visual disturbances, without
the need of any additional training data. Thus, our objective
is to create a system that implements a selective visual at-
tention similar to what human perception is doing: we want
the robot to focus on the objects of the scene that are rel-

evant to the current manipulation task. We conjecture that
using TFA, z will better represent the objects and colors that
are the subject of the attention, allowing for more precision
in grasping and manipulation (See Figure 2).

Our Contributions: The contributions of the paper are as
follows: 1- We describe a novel architecture for a visuomo-
tor policy trained end-to-end from demonstrations, which
features a task focused visual attention system. The visual
attention system is guided by a natural language description
of the task and focuses on the currently manipulated object.
2- We show that, under benign conditions, the new policy
outperforms a closely related baseline policy without the
attention model over pick-up and push tasks using a variety
of objects. 3- We show that in the case of a severe phys-
ical disturbance, when an external intervention causes the
robot to miss the grasp or drop the already grasped object,
the new policy recovers in the majority of situations, while
the baseline policy almost never recovers. 4- We show that
the task focused visual attention allows the policy to ignore
a large class of visual disturbances, that interfere with the
task for the baseline policy. We show experimentally that
the system exhibits the “invisible gorilla” phenomenon [1]
from the classic selective attention test. 5- The teacher net-
work for the task focused visual attention can be trained of-
ﬂine, does not require additional training data or pixel level
annotation of objects.

2. Related Work

A deep visuomotor policy for robotic manipulation
transforms an input video stream (possibly combined with
other sensory input) into robot commands by the means of
a single deep neural network. Such a system had been ﬁrst
demonstrated in [2] using guided policy search, a method
that transforms policy search into supervised learning, with
supervision provided by a trajectory-centric reinforcement
learning method.
In recent years, several alternative ap-
proaches have been proposed using variations of both deep
reinforcement learning and deep learning from demonstra-
tion (as well as combinations of these).

Deep reinforcement

learning is powerful paradigm
which, in applications where exploration can be performed
in a simulated environment allowing millions of trial runs,
can train systems that perform at superhuman level [3], even
when no human knowledge is used for bootstrapping [4].
Unfortunately, for training visuomotor policies controlling
real robots, it is very difﬁcult to perform reinforcement runs
on these scales. Even the most extensive projects could only
collect several orders of magnitude lower number of exper-
iments: for example, in [5] 14 robotic manipulators were
used over the period of two months to gather 800,000 grasp
attempts. Even this number of experimental tries are unre-
alistic in many practical settings.

4255

Figure 2. The proposed visuomotor architecture. Given an image captured from the scene and a command sentence provided by the
user, the Encoder (E) produces the Primary Latent Encoding (z). z is the input to the Motor Network, which decides the next state of
the robot joint angles. Also, z is the input to a Generator (G), which produces “Fake” frame and masked frame. A pre-trained Visual
Attention Teacher Module masks the original frame by an spatial attention computed employing the textual input. The Discriminator (D)
must discriminate between real/fake frames and masked frames, and also classify the object and color of the object being manipulated.

Thus, many efforts focus on reducing the number of ex-
perimental runs necessary to train an end-to-end visuomotor
controller. One obvious direction is to learn a better encod-
ing of the input data, which can improve the learning rate.
In [6], a set of visual features were extracted from the im-
age to be used as state representation for a reinforcement
learning algorithm.

Another direction involves the use of learning from
demonstration instead (or in combination with) of rein-
forcement learning. The demonstrations can be performed
in real [7] or simulated [8, 9] environments. Meta-
learning [10] and related approaches promise to drastically
lower the amount of training data needed to learn a speciﬁc
task from a class of related tasks (possibly, down to a single
task speciﬁc demonstration). However, they still require a
costly meta-learning phase.

An approach that is similar to ours in objective, but dif-
ferent in implementation, is described in [11]. Consider-
ing manipulation tasks, the authors implement two layers
of attention. The ﬁrst, a task independent visual attention
semantically identiﬁes labels and localizes objects in the
scene. This labeling relies on training on an external la-
beled dataset, thus in this respect the approach is not “end-
to-end”. The second, a task-speciﬁc attention is learned by
selecting from the segmented objects, by the task indepen-
dent attention, those objects that contribute most to the cor-
rect prediction of demonstrated trajectories.

Another point concerns the way in which the task is spec-
iﬁed to the robot. Specifying the task in the form of a human
readable sentence is a natural choice [12], as creating such a
command is very easy for a human user. In the general case,

however, translating a command into a task is not yet fea-
sible with an end-to-end learned controller. In this paper,
we assume the existence of the command, but only as an
additional input that helps the creation of the task-focused
attention. Alternative ways of specifying the task are pos-
sible. A purely visual speciﬁcation was proposed in [13],
where the user identiﬁes a pixel in the image and speciﬁes
where it should be moved. A technique of control based on
visual images was also demonstrated in [14].

One component of our work has its roots in recent work
on visual attention networks. These networks often ap-
pear as components of larger networks, solving problems
like image captioning [15, 16], visual question answer-
ing [17, 18, 19] or visual expression localization [20]. Al-
though the applications are different, the role of attention
networks, i.e., focusing on information-rich parts of the vi-
sual input, remains the same. Our proposed attention mech-
anism is most similar to [17]. However, in our model we
train the attention network with word selection objective.
The objective is to select some regions on a video frame re-
garding a textual input, such that it be able to regenerate the
words in the input sentence just based on the visual features
of selected image regions.

3. Approach

As shown in Figure 2, our architecture contains a Motor
Network and Visual Network.The Motor Network, often
but not always, contains a recurrent neural network and is
trained on a loss that favors the execution of the speciﬁed
task, g. This training may take several forms. In the case of
RL we need a source of rewards. If the task is speciﬁed by

4256

Motor NetworkVisual NetworkInputs:“Push the QR Box from left to right”Visual Attention Teacher Module(pre-trained)Command Sentence:RGB Camera Frame :“Fake”Frame (𝑥′)“Fake” MaskedFrame (𝑚′) “Real” MaskedFrame (𝑚) “Real” Frame (𝓍)“Real”Frame (𝑥)Real (𝓧)Fake (𝓧 )#Objects +1#Colors +1αμσSamplingRobot Next State Joint Angles (a) LSTMLSTMLSTMStacked Layers of LSTMs (With skip connection)Mixture of DensitiesEncoding(z)Primary Latent Encoding(z)Last layer features:Encoder   (E) Generator   (G) fD(x)Discriminator(D)Textual Encoding′Figure 3. Examples of task focused visual attention. We provide
the command sentence on top of each column. The ﬁrst row shows
frames from RGB camera and the second row is the same image
masked by the attention, produced by teacher network. We de-
note the ﬁrst/second row images by x/m in our equations.

demonstrations (our case), the training may be executed in
a supervised fashion using a behavioral cloning loss.

The Visual Network contains an Encoder module that en-
codes the input frame into the Primary Latent Variable, z.
To get a richer representation z, we incorporate two other
modules. First, a teacher network which computes an atten-
tion map and masks the input frame. We train the teacher
network separately (Section 3.1). Second, a GAN network
that takes z as input and generates two reconstructed frames,
the input frame and the masked input frame.

3.1. A Teacher Network for TFA

We consider robot manipulation commands expressed in
natural language such as, “Push the red plate to the left”,
“Push the blue box to the left”, and “Pick up the red ring”.
The goal of the TFA is to identify the parts of the visual
input, where objects relevant to the task appear, that is, to
focus the attention on the red plate, blue box and blue ring
respectively (see Figure 3).

A TFA system could be trained as a supervised learning
model, if we can create a sufﬁcient amount of training data.
However, this would require us to label with attention blobs
on an unrealistically large number of input video frames.
Our approach is to generate our own labels by implement-
ing a teacher network that provides training data for the con-
troller. Our approach ﬁts in the established technique of
student-teacher network training models [21, 22, 23], with
the qualiﬁcation that the attention teacher only teaches one
particular aspect of the ﬁnal controller. In the remainder of
this section, we describe the implementation of a teacher
network which computes the TFA as shown in Figure 4.

The proposed approach allows us to train the TFA with-
out pixel level annotations. The principal idea is that the
attention should be on those regions that allow us to recon-
struct the input text based on those regions only. The overall

Figure 4. Proposed visual attention network. The network uses the
pre-trained VGG19 [24] network’s last convolution layer output
as the visual spatial features. The attention module combines the
spatial and textual features, and assigns one probability to each
spatial region. To train the attention network, ﬁrst we pool the
visual features by the attention probabilities (weighted average),
and second, we use an auxiliary classiﬁer to reconstruct the input
text’s words based on the pooled visual features.

architecture is described in Figure 4.

We divid the visual ﬁeld (video frame), x, into k regions.
The visual attention we aim to obtain is a vector of proba-
bilities, pTFA ∈ (0, 1)k, with a probability for each of the
k regions. The higher the probability, the more attention is
paid to the speciﬁc region. In general, our goal is to focus
the attention on a small number of regions.

The ﬁrst step is to encode the text and image inputs.

Text input: Let {v1, v2, . . . , vn} be the textual input with
n words, with one-hot indicators vi ∈ {0, 1}|V |, where V is
the dictionary of the words in our dataset. Thus, a word-to-
vector encoding is employed:

wi = vi × Wω,

(1)

where Wω ∈ R|V |×dv , and dv is the length of encoded word
vectors. To encode a whole sentence, we feed the series
of word vectors to an LSTM. To obtain the text encoding,
we extract the last hidden state of the LSTM, u ∈ Rdh ,
where dh is cell size of the LSTM. We observed better per-
formance by concatenating the LSTM output with a binary
vector indicating objects’ shape and color.

Visual input: We use the last Convolution layer of a pre-
trained VGG19 [24] network to obtain k spatial visual fea-
ture vectors. The resulting spatial visual features have the
form φf ∈ Rk×dφ , where k is the number of spatial regions
and dφ is the length of feature vector for each region.

We combine the textual and visual encodings through a
technique similar to [17]. We learn a mapping on both vi-
sual and text data and combine them through an element-
wise summation:

ψ = tanh(φf × Wf ⊕ u × Wu),

(2)

where Wu ∈ Rdh×dψ and Wf ∈ Rdφ×dψ are mapping
matrices, ⊕ is element-wise summation. ψ ∈ Rk×dψ is the
combination matrix of textual and visual inputs. Note that,
u is a vector, while φf is a matrix. We augment the u vector

4257

Pick up red bowlPush blue box from left to rightPick up blue ringAttention OutputText encoderVGG19Spatial FeaturesSpatial AttentionInput TextVideo FrameSpatial FeaturesPoolingAux. classifier for attention trainingInput text wordsWord selection[h x w][h x w x d][d]by repeating it for k times. To compute the ﬁnal attention
probabilities, the model must assign higher scores to a few
spatial regions.

pTFA = softmax(ψ × Wp),

(3)

where Wp ∈ Rdψ×1 is trainable weights vector, which is
used to assign a score to each region. The ﬁnal pTFA ∈
(0, 1)k is the vector containing attention scores of all k re-
gions’. We use a softmax non-linearity to force the network
to attend to a few number of regions.

Our method does not need any spatial pixel level anno-
tation to compute the attention. The attention in our for-
mulation is a latent variable dependent on the input text
and frame (See Figure 4). The main idea which allows us
to train the attention network, is that from a pooled spa-
tial features weighted by the latent variable attention, pTFA,
we should be able to reconstruct the input text (user com-
mand sentence) words V ∈ {0, 1}|V |. Here, we deﬁne the
weighted pooled features u ∈ Rdφ :

pTFAi φf i.

u =Xi∈k

(4)

Basically, given a video frame and sentence, we force
the network to select a few regions of the input frame, and
reconstruct the input text just based on the selected regions.
As a result, the only way that the network can reconstruct
the original input text, is by selecting the relevant regions of
the frame:

ˆV = σ(τ (u)),

(5)
where τ (.) is a multi-layer perceptron. ˆV ∈ (0, 1)|V | con-
tains the predicted set of words. We optimize the entropy
loss function Latt = −Vlog( ˆV).

In Figure 3, we show RGB frames and the masked frame
using computed attention pTFA. To mask the RGB frames,
we reshape and re-size pTFA (using bi-linear interpolation) to
the same size of the input frame (x); followed by smoothing
the mask by applying a Gaussian ﬁlter on it. We denote the
masked RGB input frame with the attention pTFA by m.

3.2. The visual and motor networks

Our architecture follows the generic architecture for the
visuomotor policy in Figure 1. It consists of a Visual Net-
work sub-module that extracts a primary latent encoding, z,
and a Motor Network that transforms z into actions, which
in our case are joint angle commands (next state of the robot
arms). However, our architecture makes several speciﬁc de-
cisions with the aim to take advantage of the available the
text description of the current task and the TFA.

of the current task. An ongoing problem is that the encoding
needs to work within a certain limited dimensionality bud-
get.
Intuitively, general purpose visual features extracted
from the image would waste space by encoding aspects of
the image that are not relevant to the task. On the other
hand, focusing only on the attention ﬁeld may ignore parts
of the image that are important for the task. For instance, in
Figure 3- bottom right masked frame, the robot arm itself is
not visible.

Our proposed architecture for the visual network, shown
in Figure 2, incorporates several techniques that allows it
to learn a representation that efﬁciently encodes the parts
of the input that are relevant to the current task. The overall
architecture follows the idea of a VAE-GAN [25]: it is com-
posed of an encoder, a generator and a discriminator. The
Primary Latent Encoding (z) is extracted from the output of
the visual encoder (E).

The visual network receives a raw frame x and a one-
hot representation of the user command (input sentence),
denoted by Ic ∈ {0, 1}|V |. In fact, Ic is indicates which
words of the dictionary are appearing in the textual input
command. We assume that z ∼ N (µz, σz), and:

[µz|σz] = E(x, Ic),

(6)

where µz, σz ∈ Rdz , and dz is the length of the Primary
Latent Encoding (z). In fact, E is a multi-layer convolu-
tional neural network with a 2dz dimensional vector which
splits into µz and σz.

Unlike traditional GAN discriminators,

The generator, (G), takes the Primary Latent Encoding z
as input, and produces two images, a reconstruction frame,
and a reconstructed frame masked with attention (“Fake
Frame” and “Fake masked frame” in Figure 2). Notice that
a novel aspect of our proposed architecture is that the gen-
′,
erator does not only create a reconstruction of the input, x
but also an approximation of the faked masked frame, m′.
the discrimi-
nator D employed in our architecture performs a more
complicated classiﬁcation [26]. Masked and unmasked
frames(m/m′, x/x′) are both inputs to the discriminator,
and it classiﬁes the objects (s) and color (c) of the object
of interest, as well as whether the input was fake or real.
The discriminator has two outputs of lengths of |s| + 1 and
|c| + 1. |s| and |c| are respectively the number of colors and
objects in the vocabulary |V | and the “+1” is for the “fake”
class. We make the set of s and c tags by parsing all the
input sentences (user’s textual commands) in the training.

3.2.2 Motor Network

3.2.1 Visual Network

The objective of the Visual Network is to create a compact
primary latent encoding that captures the important aspects

The motor network in our architecture (see Figure 2) con-
tains both recurrent and stochastic components.
It takes
as input the primary latent encoding, z, which is pro-
cessed through a 3-layer LSTM network with skip connec-

4258

tions [27]. Note that the memory cells of LSTMs get up-
dated through the time by doing the task (frame by frame).
The output of the ﬁnal LSTM layer is fed into a mixture
density network (MDN) [28]. MDN provides a set of Gaus-
sian kernels parameters namely µi, σi and the mixing prob-
abilities αi(x), all ∈ R|J|, and 1 ≤ i ≤ NG. Here, |J| is
the number of robot joints (speciﬁc to the robot) and NG is
the number of Gaussian components. The |J|-dimensional
vector describing the next joint angles is sampled from this
mixture of Gaussians. We provide the detailed architectures
of D, G, E, and motor sub-networks in the Supplementary
Material.

3.3. Loss Function and Training

In this section, we describe the discriminator loss func-
tion LD, and the generator loss function LG. All the pa-
rameters in the Discriminator have been optimized to mini-
mize LD, and parameters of the visual Encoder, Generator,
and Motor network are optimized by the loss value LG in
a GAN training manner. In following, to prevent repetition
of equations, we use the unifying tuples X ′ = (x′, m′) and
X = (x, m) as fake and real data respectively. To clarify,
(x′, m′) = G(z ∼ E(x, Ic)), while x is the real frame from
RGB camera, and m is the masked real frame by the teacher
network (Section 3.1).

3.3.1 Discriminator Loss

If the discriminator D is receiving real data X , it needs to
classify the object and color contained in the user’s textual
command input:

Lreal = − EX ,s∼pdata [log (PD (s(cid:12)(cid:12)X ))]
− EX ,c∼pdata [log (PD (c(cid:12)(cid:12)X ))],

where PD is the class probabilities produced by the discrim-
inator for both colors and objects. Similarly, if D receives
X ′, it should classify them as fake:

(7)

Finally, if D receives raw and masked faked frames, gen-

erated by G with the latent representation z ∼ N (0, 1):

Lfake = − EX ′∼G[log (PD (|s| + 1(cid:12)(cid:12)X ′))]
− EX ′∼G[log (PD (|c| + 1(cid:12)(cid:12)X ′))].
Lnoise =− Ez∼noise [log (PD (|s| + 1(cid:12)(cid:12)G(z)))]
− Ez∼noise [log (PD(|c|+1(cid:12)(cid:12)G(z)))].

The overall loss of the discriminator is thus LD =

(9)

Lreal + Lfake + Lnoise .

3.3.2 Generator Loss

The Generator (G) must reconstruct a real looking frame
and masked frame by attention that contains the object of in-
terest. In fact, G tries not only to look real, but also presents

the correct object in both of its outputs. Hence, it has to fool
the discriminator which tries to distinguish between fake
frames and different objects and colors:

(10)

LGD = − EX ′,s∼pG[log pD (s(cid:12)(cid:12)X ′)]
− EX ′,c∼pG[log pD (c(cid:12)(cid:12)X ′)].

The training of GANs is notoriously unstable. A possi-
ble technique to improve stability is feature matching [29]–
forcing G to generate images that match the statistics of the
real data. Here, we use features extracted by the last con-
volution layer of D for this purpose and we call it fD(x).
The generator must produce outputs that have similar fD
representation to real data. We deﬁne the loss term Lfea as
a distance between the real inputs x/m and generated ones
x′/m′ features [26]:

Lfea = ||fD(x) − fD(x′)||2 + ||fD(m) − fD(m′)||2.

(11)

To regularize the Primary Latent Encoding (z), we min-

imize the KL-divergence between z and N (0, 1):

Lprior = DKL(E(x, Ic) || N (0, 1)).

(12)

Additionally,

error
Frame/Masked generated by G is deﬁned by:

reconstruction

a

of

“fake”

Lrec = ||x′ − x||2 + ||m′ − m||2.

(13)

Motor Network Loss: The motor loss is calculated ac-
cording to the MDN negative log-likelihood loss formula
over the supervised data based on the demonstrations (be-
havioral cloning loss):

Lmotor = −log  NG
Xi=1

αi(x) · P∼N (µi,σi)(J)!.

(14)

Finally, we write the Generator loss as LG = LDG +

(8)

Lrec + Lprior + Lmotor .

4. Experiments

We collected demonstrations for the tasks of picking
up and pushing objects using an inexpensive Lynxmotion-
AL5D robot. We controlled the robot using a PlayStation
controller. For each task and object combination we col-
lected 150 demonstrations. The training data consists of
joint-angle commands plus the visual input recorded in 10
fps rate by a PlayStation Eye camera mounted over the work
area. The training data thus collected was used to train both
the Visual and the Motor Networks. Note that this robot
does not have proprioception – any collision or manipula-
tion error needs to be detected solely from the visual input.

4259

Figure 5. An execution of the pushing task with the sentence “Push the red bowl from right to left”. Top row: original input image, middle
row: fake frame generated by the Generator(G), bottom row: fake masked image with TFA generated by G. You can compare the fake
masked frames presented in this ﬁgure with attention maps generated by the teacher network in Figure 3. Notice that visual disturbances
such as the hand and the gorilla do not appear in the reconstructed image.

Textual

Command
Sentences

B
o
w

l

R
e
d

T
o
w
e
l

W
h
i
t
e

R
i
n
g

B
l
u
e

Pick up ...

D
u
m
b
b
e
l
l

B
l
a
c
k

Push ... from left to right

P

l
a
t
e

W
h
i
t
e

B
u
b
b
l
e
s

R
e
d

p
i
c
k
u
p

M

e
a
n

B
o
w

l

R
e
d

P

l
a
t
e

W
h
i
t
e

B
o
x

B
l
u
e

Q
R
-
b
o
x

/

B
W

P
u
s
h

M

e
a
n

M

e
a
n

Method

Just Encoder (%)

Traditional VAE (%)

(w/o TFA) (%)
with TFA (%)

(w/o TFA) (%)
with TFA (%)

20
60
70
80

10
70

20
60
50
80

10
80

0
20
30
60

0
60

40
20
40
50

0
60

Benign Condition

0
50
60
80

0
40

10
30
10
40

15.0
40.0
43.3
65.0

40
50
80
100

With Disturbance

0
40

3.3
58.3

0
90

10
60
60
60

30
50

0
30
10
30

0
30

0
30
20
60

0
50

12.5
42.5
42.5
62.5

14.0
41.0
43.0
64.0

7.5
55.0

5.0
57.0

Table 1. The upper half of the table shows the rate of successfully performing the desired manipulation with different sentence commands.
The model with TFA has superior results to a model without it [7]. We also train a version of our model without the Discriminator, named
Traditional VAE. The model trained without D cannot effectively perform the manipulations since the adversarial loss helps to learn rich
Primary Latent Variable (z). Also, in Just Encoder experiment, we just use the Encoder as the visual network. The lower half of the table
shows the rate of successfully performing the desired command, while being disturbed by an external agent. The model with TFA is by far
better than a model without it [7] in all cases.

4.1. Performance under benign conditions

The ﬁrst set of experiments studies the performance of
the visuomotor controller under benign conditions, that is,
under situations when the robot is given a textual command,
Ic in Sec. 3.2.1, and it is left alone to perform the task in an
undisturbed environment. To compare our approach against
a baseline, we have reimplemented and trained the network
described in [7], which can be used in the same experimen-
tal setup, but it does not feature a task focused visual atten-
tion. Note that the success rates are not directly comparable
with [7], due to the more complex objects used here and the
different camera position and environment of our robot. We

trained the [7] model on our own dataset, tuned its hyper-
parameters and also tried to get the best possible results by
adding all the loss terms explained in Sec. 3.2.

Table 1 compares the performance of the four ap-
proaches for all the tasks, averaged over 10 tries each. We
note that the proposed architecture using “TFA” outper-
forms the “w/o TFA” on all tasks. As an ablation study,
we remove the discriminator and train the system as a tra-
ditional VAE (compared to VAE-GAN). Also, in another
experiment we trained the E just by using the motor net-
work loss without any GAN. We conﬁrm the contribution
of the adversarial loss and the GAN network to produce a

4260

rich primary latent variable z. We observe that not having
the adversarial loss will reduce the sharpness of the recon-
structed images and fade out the details. Note that the model
without adversarial loss fails to manipulate objects that re-
quire precise positioning like the black dumbbell or the blue
ring, however, it can push the white plate much better as the
plate is a big symmetric object. Please refer to the sup-
plementary materials to compare the reconstructed images
with and without the adversarial loss.

4.2. Recovery after disturbance

In the second series of experiments, we investigate the
controller’s ability to recover from a physical and visual
disturbance. We are comparing the baseline model and our
model which uses TFA. Physically disturbing means to dis-
turbed the robot either by (a) pushing the object just when
the robot was about to pick it up or (b) forcefully taking
away the object from the robot after a successful grasp. For
the push tasks, we bring in one or two hands into the scene
(Figure 5). We make different visual disturbances by bring-
ing in the hand in random positions, waving it, sometimes
covering whole top part of the scene. In some cases we even
put other random objects like a paper gorilla.

Under the described situations we count as success, if the
robot notices the disturbance and recovers by successfully
redoing the task. We remind the audience of the paper that
due to the limitations of the Lynxmotion-AL5D robot, the
only way the robot can detect the disturbance is through its
visual system.

Table 1 shows the experimental results for scenarios with
physical/visual disturbance. We notice that the results here
are drastically better than the baseline. In the absence of
TFA, the recovery rate is close to zero. In most cases, af-
ter loosing the object, the robot tried to execute the manip-
ulation without noticing that it does not grasp the object.
With the help of TFA, however, the robot almost always no-
tices the disturbance, turns back and tries to redo the grasp.
This phenomena is illustrated in our supplementary mate-
rial video. Averaged over all the objects, the recovery rate
is only 5% for the baseline policy in pickup and push tasks,
while it is 57% for the policy with the TFA (see Tables 1).
Note that physical disturbance doesn’t necessarily drop the
robot’s success rate since disturbing the robot occurs only
when it is about to successfully perform the task, therefore
the robot’s success rate with and without the physical dis-
turbance are not comparable. In other words, robot starts
doing the task, a human judge decides if the robot is doing
well and if it is, the human judge starts to disturbing the
robot. We discard any tries that the robot is likely to fail
even without disturbance.

The disappearing gorilla: The proposed architecture al-
lows us to ignore many of the possible visual disturbances.

Experiments comparing the architecture to one without TFA
conﬁrm that this is indeed the case. Another way to study
whether the policy ignores the visual disturbance is to re-
connect the generator during test time as well, and study
the reconstituted video frames (which are a good represen-
tation of the information content of primary latent encod-
ing). Figure 5 shows the input video frames (ﬁrst row), the
reconstructed video frames (second row) and the generated
masked frames (third row). While the robot was executing
the task of pushing the red bowl to the left, we added some
disturbances such as waving a hand or inserting a cutout
gorilla ﬁgure in the visual ﬁeld of robot. Notice that in
the reconstructed frames, the hand and the gorilla disap-
pear, while the subject matter is reconstructed accurately.
As these disturbing visual objects are ignored by the encod-
ing, the task execution proceeds without disturbance. While
we must be careful about making claims on the biological
plausibility of the details of our architecture, we note that
the overall effect implements a behavior similar to the se-
lective attention experiments1 of Chabris and Simmons [1],
purely as a side effect of an architecture implemented for a
completely different goal.

5. Conclusion

In this paper, we proposed a method for augmenting a
deep visuomotor policy learned from demonstration with a
task focused visual attention model. The attention is guided
by a natural language description of the task – it effectively
tells the policy to “Pay Attention!” to the task and object at
hand. Our experiments show that under benign situations,
the resulting policy consistently outperforms a related base-
line policy. More importantly, paying attention has signif-
icant robustness beneﬁts. In severe adversarial situations,
where a bump or human intervention forces the robot to
miss the grasp or drop the object, we demonstrated through
experiments that the proposed policy recovers quickly in the
majority of cases, while the baseline policy almost never re-
covers. In the case of visual disturbances such as moving
foreign objects in the visual ﬁeld of the robot, the new pol-
icy is able to ignore these disturbances which in the baseline
policy often trigger erratic behavior.

Future work includes attention systems that can simulta-
neously focus on multiple objects, shift from object to ob-
ject according to the requirements of the task, and work in
severe clutter.
Acknowledgments: This work had been supported in
part by the National Science Foundation under grant num-
bers IIS-1409823 and IIS-1741431. Any opinions, ﬁnd-
ings, and conclusions or recommendations expressed in
this material are those of the authors and do not neces-
sarily reﬂect the views of the National Science Founda-
tion.

1https://youtu.be/vJG698U2Mvo

4261

References

[1] C. Chabris and D. Simons, The invisible gorilla: And other

ways our intuitions deceive us. Harmony, 2010. 2, 8

[2] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end
training of deep visuomotor policies,” Journal of Machine
Learning Research, vol. 17, no. 1, pp. 1334–1373, 2016. 2

[3] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,
G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, et al., “Mastering the game
of Go with deep neural networks and tree search,” Nature,
vol. 529, no. 7587, p. 484, 2016. 2

[4] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton,
et al., “Mastering the game of Go without human knowl-
edge,” Nature, vol. 550, no. 7676, p. 354, 2017. 2

[5] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen,
“Learning hand-eye coordination for robotic grasping with
deep learning and large-scale data collection,” International
Journal of Robotics Research, vol. 37, no. 4-5, pp. 421–436,
2018. 2

[6] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and
P. Abbeel, “Deep spatial autoencoders for visuomotor learn-
ing,” arXiv preprint arXiv:1509.06113, 2015. 3

[7] R. Rahmatizadeh, P. Abolghasemi, L. B¨ol¨oni, and S. Levine,
“Vision-based multi-task manipulation for
inexpensive
robots using end-to-end learning from demonstration,” in
Proc. of IEEE Int’l Conference on Robotics and Automation
(ICRA-2018), 2018, pp. 3758 – 3765. 3, 7

[8] S. James, A. J. Davison, and E. Johns, “Transferring end-to-
end visuomotor control from simulation to real world for a
multi-stage task,” arXiv preprint arXiv:1707.02267, 2017. 3

[9] Y. Zhu, Z. Wang, J. Merel, A. Rusu, T. Erez, S. Cabi, S. Tun-
yasuvunakool, J. Kram´ar, R. Hadsell, N. de Freitas, et al.,
“Reinforcement and imitation learning for diverse visuomo-
tor skills,” arXiv preprint arXiv:1802.09564, 2018. 3

[10] Y. Duan, M. Andrychowicz, B. Stadie, O. J. Ho, J. Schnei-
der, I. Sutskever, P. Abbeel, and W. Zaremba, “One-shot imi-
tation learning,” in Advances in Neural Information Process-
ing Systems, 2017, pp. 1087–1098. 3

[11] C. Devin, P. Abbeel, T. Darrell, and S. Levine, “Deep
object-centric representations for generalizable robot learn-
ing,” arXiv preprint arXiv:1708.04225, 2017. 3

[12] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Baner-
jee, S. Teller, and N. Roy, “Understanding natural language
commands for robotic navigation and mobile manipulation,”
in Proc. of the Nat’l Conf. on Artiﬁcial Intelligence (AAAI-
2011), San Francisco, CA, August 2011, pp. 1507–1514. 3

[13] C. Finn and S. Levine, “Deep visual foresight for planning
robot motion,” in IEEE Int’l Conf. on Robotics and Automa-
tion (ICRA-2017), 2017, pp. 2786–2793. 3

[14] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller,
“Embed to control: A locally linear latent dynamics model
for control from raw images,” in Advances in neural infor-
mation processing systems, 2015, pp. 2746–2754. 3

[15] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi-
nov, R. Zemel, and Y. Bengio, “Show, attend and tell: Neu-
ral image caption generation with visual attention,” in Proc.
of Int’l Conf. on Machine Learning (ICML-2015), 2015, pp.
2048–2057. 3

[16] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, “Image cap-
tioning with semantic attention,” in Proc. of IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR-2016),
2016, pp. 4651–4659. 3

[17] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked
attention networks for image question answering,” in Proc.
of IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR-2016), 2016, pp. 21–29. 3, 4

[18] A. Mazaheri, D. Zhang, and M. Shah, “Video ﬁll in the blank
using LR/RL LSTMs with spatial-temporal attentions,” in
Proc of IEEE Int’l Conf. on Computer Vision (ICCV-2017),
Oct 2017. 3

[19] D. Yu, J. Fu, T. Mei, and Y. Rui, “Multi-level attention net-
works for visual question answering,” in Proc. of IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR-2017),
2017, pp. 4187–4195. 3

[20] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L.
Berg, “MAttNet: Modular attention network for referring ex-
pression comprehension,” in Proc. of IEEE Conf. on on Com-
puter Vision and Pattern Recognition (CVPR-2018), 2018. 3

[21] S. Lawrence, C. L. Giles, and A. C. Tsoi, “Lessons in neural
network training: Overﬁtting may be harder than expected,”
in Proc. of the Fourteenth Nat’l Conf. on Artiﬁcial Intelli-
gence (AAAI-97), 1997, pp. 540–545. 4

[22] C. Bucilu, R. Caruana, and A. Niculescu-Mizil, “Model
compression,” in Proc. of the 12th ACM SIGKDD Int’l Conf.
on Knowledge Discovery and Data Mining, 2006, pp. 535–
541. 4

[23] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowl-
edge in a neural network,” arXiv preprint arXiv:1503.02531,
2015. 4

[24] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” arXiv preprint
arXiv:1409.1556, 2014. 4

[25] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and
O. Winther, “Autoencoding beyond pixels using a learned
similarity metric,” arXiv preprint arXiv:1512.09300, 2015.
5

[26] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Chen, “Improved techniques for training
GANs,” in Advances in Neural Information Processing Sys-
tems, 2016, pp. 2234–2242. 5, 6

[27] A. Graves, “Generating sequences with recurrent neural net-

works,” arXiv preprint arXiv:1308.0850, 2013. 6

[28] C. M. Bishop, “Mixture density networks,” Aston University,

Tech. Rep., 1994. 6

[29] J. Bao, D. Chen, F. Wen, H. Li, and G. Hua, “CVAE-GAN:
ﬁne-grained image generation through asymmetric training,”
arXiv preprint arXiv:1703.10155, 2017. 6

4262

