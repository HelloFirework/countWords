A Decomposition Algorithm for the Sparse Generalized Eigenvalue Problem

Ganzhao Yuan1

3

,

,

4, Li Shen2, Wei-Shi Zheng3

4

,

1 Center for Quantum Computing, Peng Cheng Laboratory, Shenzhen 518005, China

2 Tencent AI Lab, Shenzhen, China

3 School of Data and Computer Science, Sun Yat-sen University, China

4 Key Laboratory of Machine Intelligence and Advanced Computing (Sun Yat-sen University), Ministry of Education, China

yuanganzhao@foxmail.com, mathshenli@gmail.com, wszheng@ieee.org

Abstract

The sparse generalized eigenvalue problem arises in a
number of standard and modern statistical learning mod-
els, including sparse principal component analysis, sparse
Fisher discriminant analysis, and sparse canonical corre-
lation analysis. However, this problem is difﬁcult to solve s-
ince it is NP-hard. In this paper, we consider a new effective
decomposition method to tackle this problem. Speciﬁcally,
we use random or/and swapping strategies to ﬁnd a work-
ing set and perform global combinatorial search over the
small subset of variables. We consider a bisection search
method and a coordinate descent method for solving the
quadratic fractional programming subproblem.
In addi-
tion, we provide some theoretical analysis for the proposed
method. Our experiments on synthetic data and real-world
data have shown that our method signiﬁcantly and consis-
tently outperforms existing solutions in term of accuracy.

1. Introduction

In this paper, we mainly focus on the following sparse

generalized eigenvalue problem (‘,’ means deﬁne):
minx6=0,x∈Ω f (x) , h(x)

g(x) , with Ω , {x | kxk0 ≤ s},

h(x) , 1

2 xT Ax, g(x) , 1

2 xT Cx.

(1)

Here, x ∈ Rn, and k·k0 is a function that counts the number
of nonzero elements in a vector. A ∈ Rn×n and C ∈ Rn×n
are some symmetry matrices. We assume that C is strictly
positive deﬁnite and s ∈ [1, n] is a positive integer.
The sparse generalized eigenvalue problem in (1) de-
scribes many applications of interest in both computer vi-
sion and machine learning, including object recognition
[26], visual tracking [21], object detection [27, 28, 31], pix-
el/part selection [25], and text summarization [44]. We no-
tice that the objective function and sparsity constraint in
(1) is scale-invariant (multiplying x with a positive constant
does not change the value of the objective function and the

satisﬁability of the sparsity constraint). Thus, it is equiva-
lent to the following problem: minx xT Ax, s.t. xT Cx =
1, kxk0 ≤ s. Moreover, without the sparsity constraint,
Problem (1) reduces to the minimum generalized eigenval-
ue problem and it has several equivalent formulations [4]:

minx6=0 (xT Ax)/(xT Cx) = min{xT Ax : xT Cx =
1} = max{λ : A − λC (cid:23) 0} = λmin(C−1/2AC−1/2),
where λmin(X) is the smallest eigenvalue of a given matrix
X, and X (cid:23) 0 denotes X is positive semideﬁnite.

Problem (1) is closely related to the classical matrix
computation in the literature [13, 11, 1]. Imposing an addi-
tional sparsity constraint on the solution reduces over-ﬁtting
and improves the interpretability of the model for high-
dimensional data analysis. The work of [18] successively
choose a sparse principle direction that maximizes the vari-
ance by enforcing a sparsity constraint using a bounded ℓ1
norm. The work of [45] reformulates the principle com-
ponent analysis problem as a elastic-net regularized ridge
regression problem, which can be solved efﬁciently using
least angle regression. The work of [9] proposes a convex
relaxation for the sparse principle component analysis prob-
lem problem based on semideﬁnite programming.

One difﬁculty of solving Problem (1) comes from the
combinational nature of the cardinality constraint. A con-
ventional way to solve this problem is to simply replace
ℓ0 norm by its convex relaxation. Recently, non-convex
approximation methods such as Schatten ℓp norm, re-
weighted ℓ1 norm, Capped ℓ1 function have been proposed
for acquiring better accuracy [32]. However, all these ap-
proximation methods fails to directly control the sparsity of
the solution. In contrast, iterative hard thresholding main-
tain the sparsity of the solution by iteratively setting the s-
mall elements (in magnitude) to zero in a gradient descent
manner. Due to its simplicity, it has been widely used and
incorporated into the truncated power method [41] and trun-
cated Rayleigh ﬂow method [35].

Another difﬁculty of solving Problem (1) is due to
the non-convexity of the objective function. One popular
method to overcome this difﬁculty is removing the quadrat-

6113

ic term using semideﬁnite programming lifting technique
and reformulating (1) into the following low-rank sparse op-
timization problem: minX6=0 tr(AX)/tr(CX), s.t. X (cid:23)
0, rank(X) = 1, kXk0 ≤ s2. We remark that the ob-
jective function is quasilinear (hence both quasiconvex and
quasiconcave), and one can constrain the denominator to
be a positive constant using the scale-invariant property of
the problem. Recently, convex semideﬁnite programming
method drops the rank constraint and considers ℓ1 relax-
ation for the sparsity constraint [9, 8, 20, 43]. It has been
shown to achieve strong guarantee under suitable assump-
tions. However, such a matrix lifting technique will incur
expensive computation overhead.

In summary, existing methods for solving Problem (1)
suffer from the following limitations. (i) Semideﬁnite pro-
gramming methods [9, 8, 20] are not scalable due to its
high computational complexity for the eigenvalue decom-
position. (ii) Convex/Non-convex approximation methods
[34, 36, 33] fail to directly control the low-rank and sparse
(iii) Hard thresholding methods
property of the solution.
[41, 19] only obtain weak optimality guarantee and result
in poor accuracy in practice [3, 40].

Recently, the work of [5] considers a new optimality cri-
terion which is based on Coordinate-Wise Optimality (C-
WO) condition for sparse optimization. It is proven that C-
WO condition is stronger than the optimality criterion based
on hard thresholding. The work of [40] presents a new
block-k optimal condition for general discrete optimization.
It is shown to be stronger than CWO condition since it in-
cludes CWO condition as a special case with k = 1. In-
spired by these works, we propose a new decomposition
method for the sparse generalized eigenvalue problem, a-
long with using a greedy method based on CWO [5] for
ﬁnding the working set.

Contributions: This paper makes the following contri-
butions. (i) We propose a new decomposition algorithm for
solving the sparse generalized eigenvalue problem (see Sec-
tion 3). (ii) We discuss two strategies to ﬁnd the working set
for our decomposition algorithm (see Section 4). (iii) We
propose two methods to solve the sparse quadratic fraction-
al programming subproblem (see Section 5). (iv) A conver-
gence analysis for the decomposition method is provided
(see Section 6). (v) Our experiments have shown that our
method outperforms existing solutions in term of accuracy.
(see Section 7).

Notation: All vectors are column vectors and super-
script T denotes transpose. Xi,j denotes the (ith, jth) ele-
ment of matrix X and xi denotes the i-th element of vector
x. ei is a unit vector with a 1 in the ith entry and 0 in al-
l other entries. diag(x) is a diagonal matrix formed with
x as its principal diagonal. For any partition of the index

vector [1, 2, ..., n] into [B, N ] with B ∈ Nk, N ∈ Nn−k,
we deﬁne UB ∈ Rn×k, UN ∈ Rn×(n−k) as:

(UB )j,i =

0,

else.

, (UN )j,l = (cid:26) 1, N (l) = j;
Bx and x = Ix = (UBUT

(cid:26) 1, B(i) = j;
have xB = UT
UBxB + UN xN . Finally, C k
sible combinations choosing k items from n.

. Therefore, we
B + UN UT
N )x =
n denotes the number of pos-

else.

0,

2. Generalized Eigenvalue Problems

A number of standard and modern statistical learning
models can be formulated as the sparse generalized eigen-
value problem, which we present some instances below.

m−1 Pm

CA can be cast into the following optimization problem:

• Principle Component Analysis (PCA). Consider a
data matrix Z ∈ Rm×d, where each row represents an in-
dependent sample. The covariance matrix Σ is computed
by Σ = 1
i=1(zi − µ)(zi − µ)T ∈ Rd×d, where
zi denotes ith column of Z and µ = Pm
i=1 zi ∈ Rd. P-
minx6=0 (−xT Σx)/(xT x).
• Fisher Discriminant Analysis (FDA). Given obser-
vations with two distinct classes with µ(i) and Σ(i) be-
ing the mean vector and covariance matrix of class i
(i = 1 or 2), respectively. FDA seeks a projection vec-
tor such that the between-class variance is large relative to
the within-class variance, leading to the following problem:

.

xT (Σ(1)+Σ(2))x

minx6=0 −xT ((µ(1)−µ(2))(µ(1)−µ(2))T )x
• Canonical Correlation Analysis (CCA). Given two
classes of data X ∈ Rm1×d and Y ∈ Rm2×d, the covari-
ance matrix between samples from X and Y can be con-
Σyx Σyy) ∈ R(m1+m2)×(m1+m2). C-
structed as Σ , (Σxx Σxy
CA exploits the relation of the samples by solving the fol-
lowing problem: maxu6=0, v6=0 uT Σxyv, s.t. uT Σxxu =
vT Σyyv = 1, where A , ( 0
Σyy) ∈
R(m1+m2)×(m1+m2), and x , [uT vT ]T . One can rewrite
CCA as the following equivalent problem: minx −xT Ax
xT Cx .
Incorporated with the sparsity constraint, the applica-
tions listed above become special cases of the general opti-
mization models in (1).

0 ), C , (Σxx
Σxy

Σyx

0

0

3. The Proposed Decomposition Algorithm

This section presents our decomposition algorithm for
solving (1), which is based on the following notation of
block-k optimality [40] for general non-convex constrained
optimization.

Deﬁnition 1. (Block-k Optimal Solution and Block-k Opti-
mality Measure ) (i) We denote B ∈ Nk as a vector contain-
ing k unique integers selected from {1, 2, ..., n}. We deﬁne
N , {1, 2, ..., n} \ B, x = UBxB + UN xN and let
P(B, x) , arg minxB f (UBxB + UN xN ) ,

(2)

s.t. (UBxB + UN xN ) ∈ Ω.

A solution ¯x is the block-k optimal solution if and only if
¯xB = P(B, ¯x) for all |B| = k. In other words, a solution

6114

n

Ck

n PCk

is the block-k optimal solution if and only if every block co-
ordinate of size k achieves the global optimal solution. (ii)
We deﬁne M(x) , 1
2 with
Ck
i=1 being all the possible combinations of the index
n
{B(i)}
vectors choosing k items from n with B(i) ∈ Nk for all i.
M(x) is an optimality measure for Problem 1 in the sense
that M(¯x) = 0 if and only if ¯x is the block-k optimal solu-
tion.

i=1 kP(B(i), x) − xB(i)k2

We describe the basic idea of the decomposition method.
In each iteration t, the indices {1, 2, ..., n} of decision vari-
able are separated to two sets Bt and N t, where Bt is the
working set and N t = {1, 2, ..., n} \ Bt. To simplify the
notation, we use B instead of Bt. Therefore, we can rewrite
h(·) and g(·) in Problem (1) as:
2 xT
h(xB, xN ) = 1
BABBxB + 1
2 xT
BCBBxB + 1
g(xB, xN ) = 1

N CN N xN + hxB, CBN xN i.

N AN N xN + hxB, ABN xN i,

2 xT
2 xT

The vector xN is ﬁxed so the objective value becomes a
subproblem with the variable xB. Our proposed algorithm
iteratively solves the small-sized optimization problem with
respect to the variable xB as in (3) until convergence. We
summarize our method in Algorithm 1.

Algorithm 1 A Decomposition Algorithm for Sparse
Generalized Eigenvalue Problem as in (1).

1: Specify the working set parameter k and the proximal
term parameter θ. Find an initial feasible solution x0
and set t = 0.

2: while not converge do
3:

(S1) Use some strategy to ﬁnd a working set B
whose size is k. Deﬁne N , {1, 2, ..., n} \ B.
(S2) Solve the following subproblem with the vari-
able xB using combinatorial search:

4:

xt+1
B ⇐ arg minxB

h(xB ,xt

2 kxB−xt

Bk2

2

N )+ θ
g(xB ,xt

N )
Nk0 ≤ s

s.t. kxBk0 + kxt

(3)

(S3) Increment t by 1.

5:
6: end while

Remarks. (i) The concept of block-k optimality has been
introduced in [40]. This paper extends their method for min-
imizing convex functions to handle general non-convex ob-
jective functions. (ii) Algorithm 1 relies on solving a small-
sized quadratic fractional problem as in (3). However, us-
ing the speciﬁc structure of the objective function and the
sparsity constraint, we can develop an efﬁcient and practi-
cal algorithm to solve it globally. (iii) We propose a new
proximal strategy when solving the subproblem as in (3).
Note that the proximal strategy is only applied to the nu-
merator instead of to the whole objective function. This is

to guarantee sufﬁcient descent condition and global conver-
gence of Algorithm 1 (see Lemma 2 and Theorem 2). (iv)
When the dimension n is small 1 and the parameter setting
θ = 0, k = n is used, the subproblem in (3) is equivalent
to Problem (1).

4. Finding the Working Set

This section shows how to ﬁnd the working set (refer
to Step S1 in Algorithm 1). This problem is challenging for
two aspects. (i) Unlike convex methods that one can ﬁnd the
working set using the ﬁrst-order optimal condition or KKT
primal-dual residual [17, 7], there is no general criteria to
ﬁnd the working set for non-convex problems. (ii) There
are C k
n possible combinations of choice for the working set
of size k. One cannot expect to use the cyclic strategy and
alternatingly minimize over all the possible combinations
Ck
i=1 ) due to its high computational complexity
(i.e., {B(i)}
n
when k is large. We propose the following two strategies to
ﬁnd the working set:

• Random Strategy. We uniformly select one combina-
Ck
tion (which contains k coordinates) from {B(i)}
i=1 . In ex-
n
pectation, our algorithm is still guaranteed to ﬁnd the block-
k stationary point.

• Swapping Strategy. We denote S(x) and Z(x) as the
index of non-zero elements and zero elements of x, respec-
tively. Based on the current solution xt, our method enu-
merates all the possible pairs (i, j) with i ∈ S(xt), j ∈
Z(xt) that lead to the greatest descent Di,j by changing

the two coordinates from zero/non-zero to non-zero/zero,
as follows:

Di,j = minβ f (xt + βei − xt

j ej) − f (xt).

(4)

We then pick the top pairs of coordinates that lead to the

greatest descent by measuring D ∈ R|S(x)|×|Z(x)|. Specif-
ically, we sort the elements in D with DP1,S1 ≤ DP2,S2 ≤
DP3,S3 ≤, ..., DPn,Sn , where P ∈ Nn and S ∈ Nn are
the index vectors. Assuming that k is an even number, we
simply pick the top-(k/2) nonoverlapping elements of the
sequence P and S respectively as the working set.

We now discuss how to solve (4) to obtain Di,j . We start

from the following lemma.

Lemma 1. We consider the following one-dimensional op-
timization problem:

β∗ = arg minβ ψ(β) ,

1
2 ¯aβ2+¯bβ+¯c
1
2 ¯rβ2+¯sβ+¯t

, s.t. β ≥ ¯L

(5)

2 ¯rβ2 + ¯sβ + ¯t > 0 and the
optimal solution is bounded. We have: (i) Problem (5) ad-

Assume that ∀β ≥ ¯L, τ , 1
mits a closed-form solution as: β∗ = arg minβ f (β), β ∈

1For example, the popular pit props data set [16, 24] only contains 13

dimensions.

6115

{Π(β1), Π(β2)}, where β1 = (−ϑ − √ϑ2 − 2πι)/π, β2 =
(−ϑ + √ϑ2 − 2πι)/π, π , ¯a¯s − ¯b¯r, ϑ , ¯a¯t − ¯c¯r, ι ,

¯t¯b + ¯c¯s, and Π(β) , max( ¯L, β). (ii) Problem (5) contains
one unique optimal solution.

2 ¯rβ2 + ¯sβ + ¯t)− ( 1

2 ¯rβ2 + ¯sβ + ¯t)− ( 1

Proof. (i) Dropping the bound constraint and setting the
gradient of ψ(β) to zero, we have 0 = ψ′(β) = ((¯aβ +
¯b)( 1
2 ¯aβ2 + ¯bβ + ¯c)(¯rβ + ¯s))/τ 2. Notic-
ing τ > 0, we obtain the following ﬁrst-order optimal con-
2 ¯aβ2 + ¯bβ +
dition for ψ: 0 = (¯aβ + ¯b)( 1
¯c)(¯rβ + ¯s). It can be simpliﬁed as: 0 = 1
2 β2π + βϑ + ι.
Solving this equation, we have two solutions β1 and β2.
We select the one between Π(β1) and Π(β2) that leads to
a lower objective value as the optimal solution. (ii) It can
be proven by contradiction. We omit the one-sided bound
constraint since it does not effect the uniqueness of the opti-
mal solution. Assume that there exist two optimal solutions
x and y to (1) that lead to the same objective value ϑ. Ac-
cording to the ﬁrst-order and second-order optimal condi-

tion [10, 42], we have: (¯a− ϑ¯r)x = −¯b + ϑ¯s, (¯a− ϑ¯r)y =
−¯b+ϑ¯s, (¯a−ϑ¯r) > 0, which leads to the following contra-
diction: ϑ¯s−¯b
¯a−ϑ¯r . Therefore, (11) contains
one unique optimal solution. Please refer to Figure 1.

¯a−ϑ¯r = x 6= y = ϑ¯s−¯b

Problem (6) is equally NP-hard due to the combinatori-
al constraint kzk0 ≤ q. Inspired by the work of [40], we
develop an exhaustive tree/combinatorial search algorithm
to solve it. Speciﬁcally, we consider to solve the follow-
ing optimization problem: minz∈Rk p(z), s.t. zK = 0,
where K has Pq
k possible choices for the coordinates.
We systematically enumerate the full binary tree for K to
obtain all possible candidate solutions for z and then pick
the best one that leads to the lowest objective value as the
optimal solution.
In other words, we need to solve the
following quadratic fractional programming problem with

i=0 C i

m , k − |K| variables:
y∗ = arg miny L(y) , u(y)
where y ∈ Rm. The optimal solution of (6) can be com-
puted as z∗K = 0, z∗¯K = y∗ with ¯K = {1, 2, ..., k} \ K.

2 yT Qy+pT y+w
2 yT Ry+cT y+v ,

Therefore, if we ﬁnd the global optimal solution of (7), we
ﬁnd the global optimal solution of (6) as well.

(7)

q(y)

,

1

1

The non-convex problem in (7) is still challenging. For
solving it, we present two methods, namely a bisection
search method and a coordinate descent method, which are
of independent research interest.

case 1

case 2

case 3

5.1. A Bisection Search Method

Figure 1: Geometric interpretation for the one-dimensional
quadratic fractional problem. Using the l’Hopital’s Rule,
we have limβ→+∞ ψ(β) = limβ→−∞ ψ(β) = ¯a
¯r . Since
the optimal solution is bounded and the problem at most
contains two critical points, we only have the three cases
above. Clearly, there exists one unique optimal solution.

1
2
1
2

(v+βei )T A(v+βei )
(v+αei )T C(v+βei )

Letting v , xt − xt
minβ
−∞, ¯a = Ai,i, ¯b = (Av)i, ¯c = 1
(Cv)i, ¯t = 1
for (4).

j ej , we obtain: minβ f (v + βei) =
. By applying Lemma 1 with ¯L =
2 vT Av, ¯r = Ci,i, ¯s =
2 vT Cv, we obtain the global optimal solution

5. Solving the Subproblem

The subproblem (3) in Algorithm 1 (refer to Step S2 in
Algorithm 1) reduces to the following quadratic fractional
programming problem:

z∗ = arg minkzk0≤q p(z) ,

1
2 zT ¯Qz+¯pT z+ ¯w
1
2 zT ¯Rz+¯cT z+¯v

,

(6)

1

where z ∈ Rk, ¯Q = ABB + θI, ¯p = ABN xN − θxt
2 xT
2 xT

N AN N xN + θ
N CN N xN , q = s − kxNk0.

B, ¯w =
2, ¯R = CBB, ¯c = CBN xN , , ¯v =

2kxt

Bk2

1

This subsection presents a bisection search method for

ﬁnding the global optimal solution of Problem (7).

We now discuss the relationship between this fraction-
al programming problem and the following parametric pro-
gramming problem [10]:

J (α) = 0, with J (α) , miny u(y) − αq(y)

This is a feasibility problem with respect to α: J (α) =
u(y∗(α)) − αq(y∗(α)) = 0, where y∗(α) ∈ Rm is deﬁned
as y∗(α) , arg miny u(y) − αq(y).

The following theorem sheds some theoretic lights for

the original non-convex problem in (7).

Theorem 1. We have the following results.

(i) It hold-
s that: λmin (Z) ≤ miny L(y) < λmin (O), with O ,
R−1/2QR−1/2, γ , 2v−kR−1/2ck2
2 > 0, g , R−1/2p−
R−1/2QR−1c, δ , cT R−1QR−1c−2cT R−1p+2w, and
g/√γ
Z , (cid:16) O
δ/γ (cid:17). (ii) Let O = Udiag(d)UT be the
gT /√γ
eigenvalue decomposition of O. The function J (α) can be
rewritten as

a2
i

i

2 Pm

2 δ − 1

2 αγ − 1

di−α , with a = UT g

J (α) = 1
and it is monotonically decreasing on the range λmin(Z) ≤
α < λmin(O). The optimal solution can be computed as
y∗ = R−1/2(u∗ − R−1/2c), with u∗ = −(O − α∗I)−1g
and α∗ being the unique root of the equation J (α) = 0 on
the range λmin(Z) ≤ α < λmin(O).

(8)

6116

Proof. (i) Firstly, it is not hard to notice that Program (7) is
equivalent to the following problem:

miny L(y) = mind

= mind

= mind

= minu

1

2 (R−1/2d)T Q(R−1/2d)+pT (R−1/2d)+w
1
2 (R−1/2d)T R(R−1/2d)+cT (R−1/2d)+v
dT Od+dT (R−1/2p)+w
1
2
1
2 kdk2
2+dT (R−1/2c)+v
dT Od+dT (R−1/2p)+w

1
2

1

2 kd+R−1/2ck2
uT Ou+uT g+ 1
2

1
2

1

2 kuk2

2+ 1
2

γ

2+v− 1

2 kR−1/2ck2

2

δ

,

(9)

where the ﬁrst step uses the variable substitution that y =
R−1/2d; the third step uses the transformation that u = d+
R−1/2c. We notice that the denominator is always strictly
positive for all decision variables. Letting u = 0 in (9), we
obtain 1
2 γ > 0. We naturally obtain the upper bound for
miny L(y):
miny L(y) = minu,η=√γ

η2

1
2

1

1
2

η2

uT Ou+ 1√γ
uT gη+ δ
2γ
2+ 1
2 kuk2
2
uT gη+ δ
2γ
2+ 1
2
Z [uT | ηT ]
2+ 1
2

uT Ou+ 1√γ
2 kuk2
2 [uT | ηT ]T
2 kuk2

η2

η2

η2

1

1

1

≥ minu,η

= minu,η

= λmin(Z),

where the ﬁrst inequality uses the fact that minx f (x) ≤
minx∈Ψ f (x) for all f (·) and Ψ.
We now derive the upper bound of miny L(y). Since the
objective function J (α) is always bounded, there must exist
α with Q−αR ≻ 0, such that the value of y minimizing the
function (u(y) − αq(y)). Therefore, we have Q − αR ≻
0 ⇒ Q − αR1/2IR1/2 ≻ 0 ⇒ R−1/2QR−1/2 − αI ≻
0 ⇒ α < λmin(O).

(ii) Using the result of (9), we can rewrite J (α) as:
2 + 1
2kuk2
J (α) = minu
2 δ − αγ
2 .
= minu

2 uT Ou + uT g + 1
2 δ − α(cid:0) 1
2 uT (O − αI)u + uT g + 1

2 γ(cid:1)

1

1

Solving the quadratic optimization with respect to u we

2 δ − αγ

2 gT (O − αI)−1g + 1

have u∗ = −(O − αI)g. Thus, we can repress J (α) as:
J (α) = − 1
2 . Since it hold-
s that gT (O − αI)−1g = gT UT diag(1 ÷ (d − α))Ug
with ÷ denoting the element-wise division between two
vectors, we obtain (8). Noticing that the ﬁrst-order and
second-order gradient of J (α) with respect to α can be
computed as: J ′(α) = − 1
2 , J ′′(α) =
−Pm
i /(di − α)3) and γ > 0, we obtain J ′(α) < 0
and J ′′(α) ≤ 0. Thus, the function J (α) is concave and
monotonically decreasing on the range λmin(Z) ≤ α <
λmin(O), and there exists a unique root of the equation
J (α) = 0 on the range λmin(Z) ≤ α < λmin(O).

di−α )2 − γ

2 Pm

i ( ai

i (a2

Based on Theorem 1, we now present a bisection method
for solving Problem (7). For notation convenience, we de-

ﬁne α , λmin(Z) and α , λmax(O) − ǫ, where ǫ de-

notes the machine precision parameter which is sufﬁcient-
ly small. Due to the monotonically decreasing property of
J (α), we can solve (8) by checking where the sign of the
left-hand side changes. Speciﬁcally, we consider the fol-
lowing three cases for J (α) on the range α ≤ α ≤ α:
(a) J (α) ≥ J (α) ≥ 0, (b) 0 ≥ J (α) ≥ J (α), and (c)
J (α) ≥ 0 ≥ J (α). For case (a) and (b), we can directly
return α and α as the optimal solution, respectively. We now
consider case (c). By the Rolle mean value theorem, there
always exists an α∗ ∈ [α, α] such that J (α∗) = 0. Thus,
we can deﬁne and initialize the lower bound lb = α and the
upper bound ub = α. We then perform the following loop
until the optimal solution α∗ = mid with J (mid) ≈ 0
is found: {mid = (lb + ub)/2,
if(J (mid) > 0) lb =
mid; else ub = mid;}. Such a bisection scheme is guaran-
teed to ﬁnd the optimal solution within O(log2((α− α)/ε))
iterations that ub ≤ lb + ε [6].
Remarks. (i) To our knowledge, this is the ﬁrst algorith-
m for unconstrained quadratic fractional programming with
global optimal guarantee. The work of [12] also discusses a
bisection search method for the ratio of trace problem, but it
can not solve our general quadratic fractional programming
problem. The classical Dinkelbach’s method [10, 39] can
solve our problem, but it only ﬁnds a stationary solution
for the non-convex problem. Our results are based on the
monotone property of the associated parametric program-
ming problem in a restricted domain. (ii) The unconstrained
fractional quadratic program can be solved to optimality by
linear semideﬁnite programming and it is related to the S-
lemma for the quadratically constrained quadratic program
[29]. In this paper, we show that it can be solved using a
bisection search method. This method has the merit that it
is simple and easy to implement. In addition, it is efﬁcient
and it does not require iterative eigenvalue decomposition
as in the semideﬁnite programming lifting techniques. (iii)
The matrix O is a n × n principal sub-matrix of Z. Us-
ing Theorem 4.3.17 in [13], it always holds that λ1(Z) ≤
λ1(O) ≤ λ2(Z) ≤ ... ≤ λn−1(Z) ≤ λn−1(O) ≤ λn(Z),
where λ(X) denotes the eigenvalues of X in increasing or-
der. Thus, the bound for the α∗ is tight.

5.2. A Coordinate Descent Method

This subsection presents a simple coordinate descen-
t method [37, 14, 15, 38, 22] for solving Problem (7). Al-
though it can not guarantee to ﬁnd the global optimal solu-
tion, it has many merits. (i) It is able to incorporate addition-
al bound constraints. (ii) It is numerically robust and does
not require additional eigenvalue solvers. (iii) It is guaran-
teed to converge to a coordinate-wise minimum point for
our speciﬁc problem (see Proposition 1 below).

To illustrate the merits of the coordinate descent method,

we consider incorporating the bound constraint x ≥ ¯L on

6117

the solution for Problem (1) 2. Our decomposition algorith-
m for ﬁnding the working set and strategies for handling the
NP-hard ℓ0 norm directly follow and what one needs is to
replace (7) and solve the following problem:

miny∈Rm L(y) ,
for some constant ˆL.

1

2 yT Qy+pT y+w
2 yT Ry+cT y+v , s.t. y ≥ ˆL

1

(10)

The coordinate descent method iteratively picks a coor-
dinate i ∈ {1, 2, ..., m} and solves the following one di-
mensional subproblem based on its current solution yj with
j = 0, 1, ...∞:

β∗ = arg minβ L(yj + βei), s.t. yj

i + β ≥ ˆL

(11)

where j is the iteration counter for the coordinate descen-
t algorithm. Problem (11) reduces to the one-dimensional
subproblem as in Lemma 1 with suitable parameters.
In
every iteration j, once the optimal solution β∗ in (11) is
found, the intermediate solution for (10) is updated via
yj+1
i ⇐ yj
i + β∗. There are several ways and orders to
(i)
decide which coordinates to update in the literature.
Cyclic order strategy runs all coordinates in cyclic order,
i.e., 1 → 2 → ... → m → 1. (ii) Random sampling s-
trategy randomly selects one coordinate to update (sample
with replacement). (iii) Gauss-Southwell strategy picks co-

ordinate i such that i = arg max1≤t≤m | ¯∇L(xj)|t, with
¯∇L(x) ∈ Rm being the projected gradient of L at x [23]:
and ∇L(x) being the gra-
¯∇L(x)i = (cid:26) ∇L(x)i ,
dient of ∇L at x. Note that ¯∇L(`x) = 0 implies `x is a

min(0, ∇L(x)i ),

xi > ˆL;
xi = ˆL;

ﬁrst-order stationary point.

We now present our convergence result of the coordinate
descent method for solving (10), which is an extension of
Theorem 4.1 in [37]. Some proofs can be found in the Ap-
pendix.

Proposition 1. When the cyclic order strategy is used, co-
ordinate descent method is guaranteed to converge to a

coordinate-wise minimum of Problem (10) that ∀i, y∗i =
arg minα≥ ˆL L(y∗i + αei).

Remarks.
(i) Convergence of the coordinate descen-
t method requires a unique solution in each minimization
step; otherwise, it may cycle indeﬁnitely. A simple but
intriguing example is given in [30]. One good feature of
the non-convex problem in (10) is that its associated one-
dimensional subproblem in (11) only contains one unique
optimal solution (see part (ii) in Lemma 1). This is dif-
ferent from the work of [38] where their one-dimensional
subproblem may have multiple optimal solutions and cause
(ii) Coordinate descent method is guaran-
divergence.
teed to produce a coordinate-wise stationary point which
is stronger than the full gradient projection method. Note

2This is useful in sparse non-negative PCA [2].

that any coordinate-wise stationary point x∗ that ∀i, x∗i =
arg minα≥ ˆL L(x∗i + αei) also satisﬁes the ﬁrst-order opti-
mal condition with ¯∇L(x∗) = 0. However, the reverse is

not true. This implies that the coordinate descent method
can exploit possible higher order derivatives to escape sad-
dle points for the non-convex problem.

6. Convergence Analysis of Algorithm 1

This section presents the convergence analysis of Algo-

rithm 1. We assume that {f (xt)}∞t=0 is generated by Algo-
rithm 1 and the solution is bounded with 0 < kxtk < ∞ for
all t throughout this section. We ﬁrst present the following
lemma.

Lemma 2. (Sufﬁcient Decrease Condition) It holds that:

f (xt+1) − f (xt) ≤ −θkxt+1−xtk2
(xt+1)T Cxt+1 .

2

Remarks. The proximal term in the numerator in (3) is
necessary for our non-convex problem since it guarantees
sufﬁcient decrease condition which is important for conver-
gence.

Now we present our main convergence result.

Theorem 2. Convergence Properties of Algorithm 1. As-
sume that the subproblem in (3) is solved globally, and there

exists a constant σ such that xtCxt ≥ σ > 0 for all t. We

have the following results.

(i) When the random strategy is used to ﬁnd the working

set, we have limt→∞ E[kxt+1 − xtk] = 0 and Algorithm 1
converges to the block-k stationary point in expectation.

(ii) When the swapping strategy is used to ﬁnd the work-

ing set with k ≥ 2, we have limt→∞ kxt+1 − xtk = 0

and Algorithm 1 converges to the block-2 stationary point
deterministically.

Remarks. (i) Thanks to the fact that the denominator is pos-
itive and the objective function is quadratic fractional, our
algorithm is still guaranteed to convergence even in the p-
resence of non-convexity. (ii) We propose using a swapping
strategy to ﬁnd the working set which enumerates all pos-
sible swaps for all pairs of coordinates to ﬁnd the greatest
descent. One good feature of this strategy is that it achieves
optimality guarantee which is no worse than Beck and Vais-
bourd’s coordinate-wise optimality condition [5].

7. Experiments

This section demonstrates the efﬁcacy of the proposed
decomposition algorithm by considering three importan-
t applications (i.e., sparse PCA, sparse FDA, and sparse C-
CA) on synthetic and real-world data sets.

• Data Sets. (i) We consider four real-world data sets:
‘a1a’, ‘w1a’, ‘w2a’, and ‘madelon’. We randomly select a

6118

-0.5

-1

e
v
i
t
c
e
b
O

j

-1.5

0

TPM
CWA
TRF
DEC-B(R6S0)
DEC-B(R10S0)
DEC-B(R0S6)
DEC-B(R0S10)
DEC-B(R4S4)
DEC-B(R6S6)

e
v
i
t
c
e
b
O

j

-0.2

-0.4

-0.6

-0.8

TRF
DEC-B(R6S0)
DEC-B(R10S0)
DEC-B(R0S6)
DEC-B(R0S10)
DEC-B(R4S4)
DEC-B(R6S6)
DEC-B(R8S8)

e
v
i
t
c
e
b
O

j

0

-0.1

-0.2

-0.3

1

2

Time(s)

3

1

2

3

4

0

1

Time(s)

TRF
DEC-B(R6S0)
DEC-B(R10S0)
DEC-B(R0S6)
DEC-B(R0S10)
DEC-B(R4S4)
DEC-B(R6S6)
DEC-B(R8S8)

2
3
Time(s)

4

5

(a) sparse PCA, ‘w1a’, s=15

(b) sparse FDA, ‘randn-500’, s=15

(c) sparse CCA, ‘randn-500’, s=15

-1

e
v
i
t
c
e
b
O

j

-1.5

-2

-2.5

TPM
CWA
TRF
DEC-B(R6S0)
DEC-B(R10S0)
DEC-B(R0S6)
DEC-B(R0S10)
DEC-B(R4S4)
DEC-B(R6S6)

e
v
i
t
c
e
b
O

j

-0.2

-0.4

-0.6

-0.8

-1

-1.2

TRF
DEC-B(R6S0)
DEC-B(R10S0)
DEC-B(R0S6)
DEC-B(R0S10)
DEC-B(R4S4)
DEC-B(R6S6)
DEC-B(R8S8)

e
v
i
t
c
e
b
O

j

0

-0.05

-0.1

-0.15

-0.2

TRF
DEC-B(R6S0)
DEC-B(R10S0)
DEC-B(R0S6)
DEC-B(R0S10)
DEC-B(R4S4)
DEC-B(R6S6)
DEC-B(R8S8)

1

2

3

4

5

6

1

2

3

4

Time(s)

Time(s)

2

4

6

Time(s)

(d) sparse PCA, ‘randn-2000’, s=15

(e) sparse FDA, ‘randn-2000’, s=15

(f) sparse CCA, ‘randn-2000’, s=15

Figure 1 Convergence behavior of different methods for sparse PCA (left column), sparse FDA (middle column), and sparse CCA (right column).

subset of examples from the original data sets 3. The size
of the data sets used in our experiments are 2000 × 119,
2000 × 300, 2000 × 300, 2000 × 112, respectively.
(ii)
We also use a similar method as in [5] to generate synthet-
ic Gaussian random data sets. Speciﬁcally, we produce the

feature matrix X ∈ Rm×d and the label vector y ∈ Rm as
follows: X = randn(m, d), y = sign(randn(m, 1)), where
randn(m, d) is a function that returns a standard Gaussian
random matrix of size m × d and sign is a signum func-
tion. We ﬁx m = 300 and consider different values for
d = {100, 500, 1500, 2000}. We denote the data sets as
‘randn-d’ and place the results in the Appendix.
Based on X and y, we generate the matrices A and C in
Problem (1) for different applications (see Section 2). Note
that the resulting size of the sparse generalized eigenvalue
problem for sparse PCA, sparse FDA, and sparse CCA are
d, d, and m, respectively. We vary the sparsity parame-
ter s ∈ {4, 8, 12, ..., 40} and report the objective values for
Problem (1).
• Compared Methods. We compare the following
(i) Truncated Power Method (TPM) [41] 4 it-
methods.
eratively and greedily decreases the objective while main-
taining the desired sparsity for the solutions by hard thresh-
olding truncation. (ii) Coordinate-Wise Algorithm (CWA)
[3, 5] 5 iteratively performs an optimization step with re-
spect to two coordinates, where the coordinates that need to
be altered are chosen to be the ones that produce the max-
(iii) Trun-
imal decrease among all possible alternatives.
cated Rayleigh Flow (TRF) [35] iteratively updates the so-
lution using the gradient of the generalized Rayleigh quo-
tient and performs a truncation operation to achieve spar-
(iv) Quadratic Majorization Method (QMM) [32] 6
sity.
approximates the ℓ0-norm by a continuous surrogate func-
tion and iteratively majorizes the surrogate function by a

3https://www.csie.ntu.edu.tw/˜cjlin/libsvm/
4code: sites.google.com/site/xtyuan1980/
5code: sites.google.com/site/amirbeck314/
6code: https://junxiaosong.github.io/

quadratic separable function, which at each iteration re-
duces to a regular generalized eigenvalue problem. Using
different smooth non-convex approximation functions, they
develop different versions of QMM (QMM-exp, QMM-log,
QMM-ℓp, QMM-ℓ0). Since their methods only solve a reg-
ularized problem and fail to control the sparsity of the so-
lution, we use a simple bisection search to ﬁnd the best
regulation parameter and report the lowest objective val-
(v) The proposed decompo-
ue after hard thresholding.
sition method (denoted as DEC) is included for compar-
isons. We use DEC-B(Ri-Sj) and DEC-C(Ri-Sj) to de-
note our method based on a Bisection search method and
a Coordinate descent method, respectively, along with se-
lecting i coordinate using the Random strategy and j coor-
dinates using the Swapping strategy. In each iteration, we

compute rt = (f (xt) − f (xt+1))/f (xt). We let Algorith-
m 1 run up to T iterations and stop it at iteration t < T if
mean([rt−min(t,M )+1, rt−min(t,M )+2, ..., rt]) ≤ ǫ. The de-
fault parameter (θ, ǫ, M, T ) = (10−5, 10−5, 50, 1000)
is used. All codes are implemented in MATLAB on an Intel
3.20GHz CPU with 8 GB RAM 7. Only DEC-C is develope-
d in C and wrapped into our MATLAB code, since it uses an
elementwise loop which is inefﬁcient in native MATLAB.

We remark that both (a) and (b) are only designed for
sparse PCA with C = I. We do not compare against the DC
programming algorithms [34, 36] since they fail to control
the sparsity of the solution and result in worse accuracy than
QMM (see [32]).

• Convergence Behavior. We show the convergence
behavior for different methods in Figure 1. We do not in-
clude the results of QMM since it fails to control the spar-
sity of the solution. Due to space limitation, we only report
the results of DEC-B in this set of experiments. We have
the following observations. (i) The methods {TPM, CWA,
TRF} converge within one second and they are faster than
7For the purpose of reproducibility, we provide our code in the authors’

research webpage.

6119

e
v
i
t
c
e
b
O

j

-2.5

-3

-3.5

-4

-4.5

-5

-5.5

-6

QMM-exp
QMM-log
QMM-lp
QMM-10
TRF
TPM
CWA
DEC-B(R6S6)
DEC-C(R6S6)

-0.5

e
v
i
t
c
e
b
O

j

-1

-1.5

-2

QMM-exp
QMM-log
QMM-lp
QMM-10
TRF
TPM
CWA
DEC-B(R6S6)
DEC-C(R6S6)

e
v
i
t
c
e
b
O

j

-0.2

-0.4

-0.6

-0.8

-1

-1.2

-1.4

-1.6

-1.8

QMM-exp
QMM-log
QMM-lp
QMM-10
TRF
TPM
CWA
DEC-B(R6S6)
DEC-C(R6S6)

e
v
i
t
c
e
b
O

j

-2

-4

-6

-8

-10

QMM-exp
QMM-log
QMM-lp
QMM-10
TRF
TPM
CWA
DEC-B(R6S6)
DEC-C(R6S6)

4

8

12

16

20
24
Sparsity

(a) a1a

28

32

36

40

4

8

12

16

20
24
Sparsity

28

32

36

40

4

8

12

16

20
24
Sparsity

28

32

36

40

4

8

12

16

20
24
Sparsity

28

32

36

40

(b) w1a

(c) w2a

(d) madelon

Figure 2: Accuracy of different methods on different data sets for sparse PCA problem with varying the cardinalities.

e
v
i
t
c
e
b
O

j

-0.6

-0.8

-1

-1.2

QMM-lp
QMM-log
QMM-exp
TRF
DEC-B(R6S6)
DEC-C(R6S6)

e
v
i
t
c
e
b
O

j

-0.2

-0.3

-0.4

-0.5

QMM-lp
QMM-log
QMM-exp
TRF
DEC-B(R6S6)
DEC-C(R6S6)

e
v
i
t
c
e
b
O

j

-0.3

-0.4

-0.5

-0.6

-0.7

QMM-lp
QMM-log
QMM-exp
TRF
DEC-B(R6S6)
DEC-C(R6S6)

×10-3

e
v
i
t
c
e
b
O

j

-2

-4

-6

-8

-10

-12

QMM-lp
QMM-log
QMM-exp
TRF
DEC-B(R6S6)
DEC-C(R6S6)

4 8 12 16 20 24 28 32 36 40

4 8 12 16 20 24 28 32 36 40

4 8 12 16 20 24 28 32 36 40

4

8 12 16 20 24 28 32 36 40

Sparsity

(a) a1a

Sparsity

(b) w1a

Sparsity

(c) w2a

Sparsity

(d) madelon

Figure 3: Accuracy of different methods on different data sets for sparse FDA problem with varying cardinalities.

e
v
i
t
c
e
b
O

j

-0.5

-0.6

-0.7

-0.8

-0.9

QMM-lp
QMM-log
QMM-exp
TRF
DEC-B(R6S6)
DEC-C(R6S6)

e
v
i
t
c
e
b
O

j

-0.5

-0.55

-0.6

-0.65

-0.7

-0.75

QMM-lp
QMM-log
QMM-exp
TRF
DEC-B(R6S6)
DEC-C(R6S6)

e
v
i
t
c
e
b
O

j

-0.4

-0.5

-0.6

QMM-lp
QMM-log
QMM-exp
TRF
DEC-B(R6S6)
DEC-C(R6S6)

e
v
i
t
c
e
b
O

j

-0.75

-0.8

-0.85

-0.9

-0.95

QMM-lp
QMM-log
QMM-exp
TRF
DEC-B(R6S6)
DEC-C(R6S6)

4 8 12 16 20 24 28 32 36 40

4 8 12 16 20 24 28 32 36 40

4 8 12 16 20 24 28 32 36 40

4 8 12 16 20 24 28 32 36 40

Sparsity

(a) a1a

Sparsity

(b) w1a

Sparsity

(c) w2a

Sparsity

(d) madelon

Figure 4: Accuracy of different methods on different data sets for sparse CCA problem with varying cardinalities.

1

10

0

10

)
s
(
e
m
T

i

−1

10

−2

10

 

4

QMM−exp
QMM−log
QMM−lp
QMM−10
TRF
TPM
CWA
DEC−B(R6S6)
DEC−C(R6S6)

8

12

16

 

2

1.5

1

0.5

)
s
(
e
m
T

i

QMM−lp
QMM−log
QMM−exp
TRF
DEC−B(R6S6)
DEC−C(R6S6)

 

 

QMM−lp
QMM−log
QMM−exp
TRF
DEC−B(R6S6)
DEC−C(R6S6)

)
s
(
e
m
T

i

60

50

40

30

20

10

20
24
Sparsity

28

32

36

40

(a) PCA

0

 

4

8 12 16 20 24 28 32 36 40

Sparsity

(b) FDA

0

 

4

8 12 16 20 24 28 32 36 40

Sparsity

(c) CCA

Figure 5 Comparison of the computing time for different methods on ‘w1a’ data set with varying cardinalities.

DEC. However, they get stuck into poor local minima and
result in much worse accuracy than DEC. (ii) The objective
values of DEC stabilize after less than 5 seconds, which
means it has converged, and the decrease of the objective
value is negligible afterwards. This implies that one may
use a looser stopping criterion without sacriﬁcing accura-
cy. (iii) DEC-B(R6S0) and DEC-B(R8S8) converge slow-
ly, and it seems that DEC-B(R6S6) ﬁnds a good trade-off
between efﬁciency and effectiveness. (iv) DEC-B(R10S0)
achieves a lower objective value than DEC-B(R6S0). This
is reasonable since a larger k in the block-k optimality
(v) {DEC-
condition implies a stronger stationary point.
B(R6S0), DEC-B(R10S0)} achieve larger objective values
than {DEC-B(R0S6), DEC-B(R0S10)}, which implies that
the swapping strategy plays an indispensable role in DEC.

• Experimental Results. We show the experimental re-
sults for sparse PCA, sparse FDA, and sparse CCA in Figure
2, 3 and 4, respectively. Several conclusions can be drawn.
(i) CWA generally outperforms {TPM, TRF, QMM}.
(i-

i) CWA is not stable and generates much worse results on
‘w1a’ and ‘w2a’. (iii) The proposed method DEC still out-
performs CWA and achieves the lowest objective values.
(iv) Both DEC-B and DEC-C perform similarly. This is be-
cause coordinate descent methods ﬁnd a desirable solution
for the quadratic fractional programming problem.

• Computational Efﬁciency. We demonstrate a com-
parison of the actual computing time for different method-
s on ‘w1a’ data set in Figure 5. Two conclusions can be
drawn. (i) DEC takes less than 15 seconds to converge in
all our instances. (ii) DEC is practical and it is much more
efﬁcient than QMM.

Acknowledgments. This work was supported by the Na-
tional Key Research and Development Program of China
(2018YFB1004903), NSF-China (61772570), Pearl River
S&T Nova Program of Guangzhou (201806010056), and
Guangdong Natural Science Funds for Distinguished Young
Scholar (2018B030306025).

6120

References

[1] Megasthenis

Asteris,

Anastasios

Kyrillidis,
Oluwasanmi Koyejo, and Russell Poldrack.
A
simple and provable algorithm for sparse diagonal c-
ca. In International Conference on Machine Learning
(ICML), pages 1148–1157, 2016. 1

[2] Megasthenis Asteris, Dimitris S. Papailiopoulos, and
Alexandros G. Dimakis. Nonnegative sparse PCA
with provable guarantees.
In International Confer-
ence on Machine Learning (ICML), pages 1728–1736,
2014. 6

[3] Amir Beck and Yonina C Eldar.

Sparsity con-
strained nonlinear optimization: Optimality condi-
tions and algorithms. SIAM Journal on Optimization,
23(3):1480–1509, 2013. 2, 7

[4] Amir Beck and Marc Teboulle. On minimizing
quadratically constrained ratio of two quadratic func-
tions.
Journal of Convex Analysis, 17(3):789–804,
2010. 1

[5] Amir Beck and Yakov Vaisbourd. The sparse principal
component analysis problem: Optimality conditions
and algorithms. Journal of Optimization Theory and
Applications, 170(1):119–143, 2016. 2, 6, 7

[6] Stephen Boyd and Lieven Vandenberghe. Convex op-

timization. Cambridge university press, 2004. 5

[7] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a li-
brary for support vector machines. ACM transactions
on Intelligent Systems and Technology (TIST), 2(3):27,
2011. 3

[8] Alexandre dAspremont, Francis Bach, and Laurent El
Ghaoui. Optimal solutions for sparse principal compo-
nent analysis. Journal of Machine Learning Research,
9(Jul):1269–1294, 2008. 2

[9] Alexandre d’Aspremont, Laurent E Ghaoui, Michael I
Jordan, and Gert R Lanckriet. A direct formulation
for sparse pca using semideﬁnite programming.
In
Advances in Neural Information Processing Systems
(NIPS), pages 41–48, 2005. 1, 2

[10] Werner Dinkelbach. On nonlinear fractional program-
ming. Management Science, 13(7):492–498, 1967. 4,
5

[11] Rong Ge, Chi Jin, Praneeth Netrapalli, Aaron Sid-
ford, et al. Efﬁcient algorithms for large-scale gener-
alized eigenvector computation and canonical correla-
tion analysis. In International Conference on Machine
Learning (ICML), pages 2741–2750, 2016. 1

[12] Yue-Fei Guo, Shijin Li, Jing-Yu Yang, Ting-Ting Shu,
and Lide Wu. A generalized foley-sammon transform
based on generalized ﬁsher discriminant criterion and
its application to face recognition. Pattern Recogni-
tion Letters, 24(1-3):147–158, 2003. 5

[13] Roger A Horn and Charles R Johnson. Matrix analy-

sis. Cambridge university press, 1990. 1, 5

[14] Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,

S. Sathiya Keerthi, and S. Sundararajan. A dual coor-
dinate descent method for large-scale linear SVM. In
International Conference on Machine Learning (ICM-
L), pages 408–415, 2008. 5

[15] Cho-Jui Hsieh and Inderjit S. Dhillon. Fast coordi-
nate descent methods with variable selection for non-
negative matrix factorization.
In ACM International
Conference on Knowledge Discovery and Data Min-
ing (SIGKDD), pages 1064–1072, 2011. 5

[16] JNR Jeffers. Two case studies in the application
of principal component analysis. Applied Statistics,
pages 225–236, 1967. 3

[17] Thorsten Joachims. Making large-scale svm learning
practical. Technical report, Technical Report, SFB
475: Komplexit¨atsreduktion in Multivariaten Daten-
strukturen, Universit¨at Dortmund, 1998. 3

[18] Ian T Jolliffe, Nickolay T Trendaﬁlov, and Mudassir
Uddin. A modiﬁed principal component technique
based on the lasso.
Journal of computational and
Graphical Statistics, 12(3):531–547, 2003. 1

[19] Michel Journ´ee, Yurii Nesterov, Peter Richt´arik, and
Rodolphe Sepulchre. Generalized power method for
sparse principal component analysis. Journal of Ma-
chine Learning Research, 11(Feb):517–553, 2010. 2

[20] Robert Krauthgamer, Boaz Nadler, Dan Vilenchik,
et al. Do semideﬁnite relaxations solve sparse pca up
to the information limit?
The Annals of Statistics,
43(3):1300–1322, 2015. 2

[21] Junseok Kwon and Kyoung Mu Lee. Visual tracking
decomposition. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2010. 1

[22] Qi Lei, Kai Zhong,

and Inderjit S. Dhillon.
Coordinate-wise power method. In Advances in Neu-
ral Information Processing Systems (NIPS), pages
2056–2064, 2016. 5

[23] Chih-Jen Lin. Projected gradient methods for non-
negative matrix factorization. Neural Computation,
19(10):2756–2779, 2007. 6

[24] Lester W Mackey. Deﬂation methods for sparse p-
In Advances in Neural Information Processing

ca.
Systems, pages 1017–1024, 2009. 3

[25] Bernard Moghaddam, Yair Weiss, and Shai Avidan.
Fast pixel/part selection with sparse eigenvectors. In
IEEE International Conference on Computer Vision
(ICCV), 2007. 1

[26] Nikhil Naikal, Allen Y. Yang, and Shankar Sastry. In-
formative feature selection for object recognition vi-
a sparse PCA. In IEEE International Conference on
Computer Vision (ICCV), 2011. 1

[27] Sakrapee Paisitkriangkrai, Chunhua Shen, and Jian
Zhang. Efﬁciently training a better visual detector
with sparse eigenvectors.
In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR),
2009. 1

6121

Machine Learning Research, 14(Apr):899–925, 2013.
1, 2, 7

[42] Ailing Zhang and Shunsuke Hayashi. Celis-dennis-
tapia based approach to quadratic fractional program-
ming problems with two quadratic constraints. Nu-
merical Algebra, Control and Optimization, 1(1):83–
98, 2011. 4

[43] Youwei Zhang, Alexandre dAspremont, and Lauren-
t El Ghaoui. Sparse pca: Convex relaxations, algo-
rithms and applications. In Handbook on Semideﬁnite,
Conic and Polynomial Optimization, pages 915–940.
Springer, 2012. 2

[44] Youwei Zhang and Laurent E Ghaoui. Large-scale s-
parse principal component analysis with application to
text data. In Advances in Neural Information Process-
ing Systems (NIPS), 2011. 1

[45] Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse
principal component analysis. Journal of computa-
tional and graphical statistics, 15(2):265–286, 2006.
1

[28] Sakrapee Paisitkriangkrai, Chunhua Shen, and Jian
Incremental training of a detector using on-
IEEE Transactions

Zhang.
line sparse eigendecomposition.
on Image Processing (TIP), 2011. 1

[29] Imre P´olik and Tam´as Terlaky. A survey of the s-

lemma. SIAM Review, 49(3):371–418, July 2007. 5

[30] M. J. D. Powell. On search directions for minimization
algorithms. Mathematical Programming, 4(1):193–
201, Dec 1973. 6

[31] Chunhua Shen, Sakrapee Paisitkriangkrai, and Jian
Zhang. Efﬁciently learning a detection cascade with
sparse eigenvectors. IEEE Transactions on Image Pro-
cessing (TIP), 2011. 1

[32] Junxiao Song, Prabhu Babu, and Daniel P´erez Palo-
mar. Sparse generalized eigenvalue problem via s-
mooth optimization.
IEEE Transactions on Signal
Processing, 63(7):1627–1642, 2015. 1, 7

[33] Bharath K Sriperumbudur, David A Torres, and
Gert RG Lanckriet. A majorization-minimization ap-
proach to the sparse generalized eigenvalue problem.
Machine learning, 85(1):3–39, 2011. 2

[34] Bharath K. Sriperumbudur, David A. Torres, and Gert
R. G. Lanckriet. Sparse eigen methods by d.c. pro-
gramming. In International Conference on Machine
Learning (ICML), pages 831–838, 2007. 2, 7

[35] Kean Ming Tan, Zhaoran Wang, Han Liu, and Tong
Zhang. Sparse generalized eigenvalue problem: Opti-
mal statistical rates via truncated rayleigh ﬂow. Jour-
nal of the Royal Statistical Society: Series B, 2018. 1,
7

[36] Mamadou Thiao, Pham D Tao, and Le An. A dc pro-
gramming approach for sparse eigenvalue problem. In
International Conference on Machine Learning (ICM-
L), pages 1063–1070, 2010. 2, 7

[37] P. Tseng. Convergence of a block coordinate descent
method for nondifferentiable minimization. Journal
of Optimization Theory and Applications, 109(3):475–
494, Jun 2001. 5, 6

[38] Arnaud Vandaele, Nicolas Gillis, Qi Lei, Kai Zhong,
and Inderjit S. Dhillon. Efﬁcient and non-convex coor-
dinate descent for symmetric nonnegative matrix fac-
torization. IEEE Transactions on Signal Processing,
64(21):5571–5584, 2016. 5, 6

[39] Huan Wang, Shuicheng Yan, Dong Xu, Xiaoou Tang,
and Thomas Huang. Trace ratio vs. ratio trace for di-
mensionality reduction. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2007.
5

[40] Ganzhao Yuan, Li Shen, and Wei-Shi Zheng. A hybrid
method of combinatorial search and coordinate de-
scent for discrete optimization. arXiv preprint, 2017.
2, 3, 4

[41] Xiao-Tong Yuan and Tong Zhang. Truncated power
method for sparse eigenvalue problems. Journal of

6122

