Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking

Department of Computer and Information Sciences, Temple University, Philadelphia, PA USA

Heng Fan

Haibin Ling∗

{hengfan,hbling}temple.edu

Abstract

Recently, the region proposal networks (RPN) have been
combined with the Siamese network for tracking, and shown
excellent accuracy with high efﬁciency. Nevertheless, previ-
ously proposed one-stage Siamese-RPN trackers degenerate
in presence of similar distractors and large scale variation.
Addressing these issues, we propose a multi-stage tracking
framework, Siamese Cascaded RPN (C-RPN), which con-
sists of a sequence of RPNs cascaded from deep high-level
to shallow low-level layers in a Siamese network. Com-
pared to previous solutions, C-RPN has several advantages:
(1) Each RPN is trained using the outputs of RPN in the
previous stage. Such process stimulates hard negative sam-
pling, resulting in more balanced training samples. Con-
sequently, the RPNs are sequentially more discriminative
in distinguishing difﬁcult background (i.e., similar distrac-
tors). (2) Multi-level features are fully leveraged through a
novel feature transfer block (FTB) for each RPN, further im-
proving the discriminability of C-RPN using both high-level
semantic and low-level spatial information. (3) With mul-
tiple steps of regressions, C-RPN progressively reﬁnes the
location and shape of the target in each RPN with adjusted
anchor boxes in the previous stage, which makes localiza-
tion more accurate. C-RPN is trained end-to-end with the
multi-task loss function. In inference, C-RPN is deployed as
it is, without any temporal adaption, for real-time tracking.
In extensive experiments on OTB-2013, OTB-2015, VOT-
2016, VOT-2017, LaSOT and TrackingNet, C-RPN consis-
tently achieves state-of-the-art results and runs in real-time.

1. Introduction

Visual tracking is one of the most fundamental problems
in computer vision, and has a long list of applications such
as robotics, human-machine interaction, intelligent vehicle,
surveillance and so forth. Despite great advances in recent
years, visual tracking remains challenging due to many fac-

∗Corresponding author.

GT

C-RPN

Siamese-RPN

Figure 1. Comparisons between one-stage Siamese-RPN [23] and
C-RPN on two challenging sequences: Bolt2 (the top row) with
similar distractors and CarScale (the bottom row) with large scale
changes. We observe that C-RPN can distinguish the target from
distractors, while Siamese-RPN drifts to the background in Bolt2.
In addition, compared to using a single regressor in Siamese-RPN,
multi-regression in C-RPN can better localize the target in pres-
ence of large scale changes in CarScale. Best viewed in color.

tors including occlusion, scale variation, etc.

Recently, Siamese network has drawn great attention in
the tracking community owing to its balanced accuracy and
speed. By formulating object tracking as a matching prob-
lem, Siamese trackers [2, 17, 19, 23, 45, 46, 51, 59] aim to
learn ofﬂine a generic similarity function from a large set of
videos. Among these methods, the work of [23] proposes a
one-stage Siamese-RPN for tracking by introducing the re-
gional proposal networks (RPN), originally used for object
detection [38], into Siamese network. With the proposal
extraction by RPN, this approach simultaneously performs
classiﬁcation and localization from multiple scales, achiev-
ing excellent performance. Besides, the use of RPN avoids
applying the time-consuming pyramid for target scale esti-
mation [2], leading to a super real-time solution.

1.1. Problem and Motivation

Despite having achieved promising result, Siamese-RPN
may drift to the background especially in presence of sim-
ilar semantic distractors (see Fig. 1). We identify two rea-
sons accounting for this.

First, the distribution of training samples is imbalanced:

7952

(1) positive samples are far less than negative samples, lead-
ing to ineffective training of the Siamese network; and (2)
most negative samples are easy negatives (non-similar non-
semantic background) that contribute little useful informa-
tion in learning a discriminative classiﬁer [29]. As a con-
sequence, the classiﬁer is dominated by the easily classiﬁed
background samples, and degrades when encountering dif-
ﬁcult similar semantic distractors.

Second, low-level spatial features are not fully explored.
In Siamese-RPN (and other Siamese trackers), only features
of the last layer, which contain more semantic information,
are explored to distinguish target/background. In tracking,
nevertheless, background distractors and the target may be-
long to the same category, and/or have similar semantic fea-
tures [49]. In such case, the high-level semantic features are
less discriminative in distinguishing target/background.

In addition to the issues above, the one-stage Siamese-
RPN applies a single regressor for target localization using
pre-deﬁned anchor boxes. These anchors are expected to
work well when having a high overlap with the target. How-
ever, for model-free visual tracking, no prior information re-
garding the target object is known, and it is hard to estimate
how the scale of target changes. Using pre-deﬁned coarse
anchor boxes in a single step regression is insufﬁcient for
accurate localization [3, 14] (see again Fig. 1).

The class imbalance issue is addressed in two-stage ob-
ject detectors (e.g., Faster R-CNN [38]). The ﬁrst proposal
stage rapidly ﬁlters out most background samples, and then
the second classiﬁcation stage adopts sampling heuristics
such as a ﬁxed foreground-to-background ratio to maintain
a manageable balance between foreground and background.
In addition, two steps of regressions achieve accurate local-
ization even for objects with extreme shapes.

Inspired by the two-stage detectors, we propose a multi-
stage tracking framework by cascading a sequence of RPNs
to solve the class imbalance problem, and meanwhile fully
explore features across layers for robust visual tracking.

1.2. Contribution

As the ﬁrst contribution, we introduce a novel multi-
stage tracking framework, the Siamese Cascaded RPN (C-
RPN), to solve the problem of class imbalance by perform-
ing hard negative sampling [40, 48]. C-RPN consists of a
sequence of RPNs cascaded from the high-level to the low-
level layers in the Siamese network. In each stage (level),
an RPN performs classiﬁcation and localization, and out-
puts the classiﬁcation scores and the regression offsets for
the anchor boxes in this stage. The easy negative anchors
are then ﬁltered out, and the rest, treated as hard examples,
are used as training samples for the RPN of the next stage.
Through such process, C-RPN performs stage by stage hard
negative sampling. Consequently, the distributions of train-
ing samples are sequentially more balanced, and the clas-

siﬁers of RPNs are sequentially more discriminative in dis-
tinguishing more difﬁcult distractors (see Fig. 1).

Another beneﬁt of C-RPN is more accurate target local-
ization compared to the one-stage SiamRPN [23]. Instead
of using the pre-deﬁned coarse anchor boxes in a single re-
gression step, C-RPN consists of multiple steps of regres-
sions due to multiple RPNs. In each stage, the anchor boxes
(including locations and sizes) are adjusted by the regressor,
which provides better initialization for the regressor of next
stage. As a result, C-RPN can progressively reﬁne the target
bounding box for better localization as shown in Fig. 1.

Leveraging features from different layers in the networks
has been proven to be beneﬁcial for improving model dis-
criminability [27, 28, 30]. To fully explore both the high-
level semantic and the low-level spatial features for visual
tracking, we make the second contribution by designating
a novel feature transfer block (FTB). Instead of separately
using features from a single layer in one RPN, FTB enables
us to fuse the high-level features into low-level RPN, which
further improves its discriminative power to deal with com-
plex background, resulting in better performance of C-RPN.
Fig. 2 illustrates the framework of C-RPN.

In extensive experiments on six benchmarks, including
OTB-2013 [52], OTB-2015 [53], VOT-2016 [20], VOT-
2017 [21], LaSOT [10] and TrackingNet [34], our C-RPN
achieves the state-of-the-art results and runs in real-time1.

2. Related Work

Visual tracking has been extensively researched in recent
decades. In the following we discuss the most related work,
and refer readers to [25, 41, 54] for recent surveys.

Deep tracking. Inspired by the successes in image classi-
ﬁcation [18, 22], deep convolutional neural network (CNN)
has been introduced into visual tracking and demonstrated
excellent performances [7, 8, 12, 32, 35, 43, 49, 50]. Wang
et al. [50] propose a stacked denoising autoencoder to learn
generic feature representation for object appearance mod-
eling in tracking. Wang et al. [49] present a fully convo-
lutional network tracking approach by transferring the pre-
trained deep features to improve tracking. Ma et al. [32]
apply deep feature for correlation ﬁlter tracking, achieving
remarkable gains. Nam and Han [35] propose a light archi-
tecture of CNN with online update to learn generic feature
for tracking target. Fan and Ling [12] extend this approach
by introducing a recurrent neural network (RNN) to capture
object structure. Song et al. [43] apply adversary learning in
CNN to learn richer representation for tracking. Danelljan
et al. [8] propose continuous convolution ﬁlters for correla-
tion ﬁlter tracking, and later optimize this method in [7].

Siamese tracking. Siamese network has attracted increas-

1The project is at http://www.dabi.temple.edu/˜hbling/

code/CRPN/crpn.htm

7953

Figure 2. Illustration of the architecture of C-RPN, including a Siamese network for feature extraction and cascaded regional proposal
networks for sequential classiﬁcations and regressions. The FTB transfers the high-level semantic features for the low-level RPN, and “A”
represents the set of anchor boxes, which are gradually reﬁned stage by stage. Best viewed in color.

ing interest for tracking because of its balanced accuracy
and efﬁciency. Tao et al. [45] use Siamese network to learn
a matching function from videos, then use the ﬁxed match-
ing function to search for the target. Bertinetto et al. [2]
present a fully convolutional Siamese network (SiamFC) for
tracking by measuring the region-wise feature similarity be-
tween the target and the candidate. Owing to its light struc-
ture and without model update, SiamFC runs efﬁciently at
80 fps. Held et al. [19] propose the GOTURN approach by
learning a motion prediction model with the Siamese net-
work. Valmadre et al. [46] use a Siamese network to learn
the feature representation for correlation ﬁlter tracking. He
et al. [17] introduce a two-fold Siamese network for track-
ing. Later in [16], they improve this two-fold Siamese track-
ing by incorporating angle estimation and spatial mask-
ing. Wang et al. [51] introduce an attention mechanism
into Siamese network to learn a more discriminative met-
ric for tracking. Notably, Li et al. [23] combine Siamese
network with RPN and propose a one-stage Siamese-RPN
tracker, achieving excellent performance. Zhu et al. [59]
utilize more negative samples to improve the Siamese-RPN
tracker. Despite improvement, this approach requires large
extra training data from other domains.
Multi-level features. The features from different layers in
the neural network contain different information. The high-
level feature consists of more abstract semantic cues, while
the low-level layers contains more detailed spatial informa-
tion [30]. It has been proven that tracking can be beneﬁted
using multi-level features. In [32], Ma et al. separately use
features in three different layers for three correlation mod-
els, and fuse their outputs for the ﬁnal tracking result. Wang
et al. [49] develop two regression models with features from
two layers to distinguish similar semantic distractors.
Cascaded structure. Cascaded structures have been a pop-
ular strategy to improve performance. Viola et al. [48] pro-
pose a boosted cascade of simple feature for efﬁcient object
detection. Li et al. [24] present a cascaded structure built

on CNN for face detection and achieve powerful discrimi-
native capability with high efﬁciency. Cai et al. [3] propose
a multi-stage object detection framework, cascade R-CNN,
aiming at high quality detection by sequentially increasing
IoU thresholds. Zhang et al. [55] utilize a cascade to reﬁne
detection results by adjusting anchors.

Our approach. In this paper, we focus on solving the prob-
lem of class imbalance to improve model discriminability.
Our approach is related but different from the Siamese-RPN
tracker [23], which applies one-stage RPN for classiﬁcation
and localization and skips the data imbalance problem. In
contrast, our approach cascades a sequence of RPNs to ad-
dress the data imbalance by performing hard negative sam-
pling, and progressively reﬁnes anchor boxes for better tar-
get localization using multi-regression. Our method is also
related to [32, 49] using multi-level features for tracking.
However, unlike [32, 49] in which multi-level features are
separately used for independent models (i.e., decision-level
fusion), we propose a feature transfer block to fuse the fea-
tures across layers for each RPN (i.e., feature-level fusion),
improving its discriminative power in distinguishing the tar-
get object from complex background.

3. Siamese Cascaded RPN (C-RPN)

In this section, we detail the Siamese Cascaded RPN (re-

ferred to as C-RPN) as shown in Fig. 2.

C-RPN contains two subnetworks: the Siamese network
and the cascaded RPN. The Siamese network is utilized to
extract the features of the target template x and the search
region z. Afterwards, C-RPN receives the features of x and
z for each RPN. Instead of only using the features from one
layer, we apply feature transfer block (FTB) to fuse the fea-
tures from high-level layers for RPN. An RPN simultane-
ously performs classiﬁcation and localization on the feature
maps of z. Based on the classiﬁcation scores and regression
offsets, we ﬁlter out easy negative anchors (e.g., an anchor

7954

Sharing WeightsSiamese NetworkRPN(stage 1)Feature Transfer BlockFeature Transfer BlockRPN(stage 3)Feature Transfer BlockFeature Transfer BlockRPN(stage 2)A2A1Cascaded Regional Proposal NetworksA3A4………𝜑1(𝐳)Φ1(𝐳)𝜑2(𝐳)𝜑3(𝐳)𝜑3(𝐱)𝜑2(𝐱)𝜑1(𝐱)Φ1(𝐳)Φ2(𝐳)Φ2(𝐳)Φ3(𝐳)Φ3(𝐳)Φ1(𝐱)Φ1(𝐱)Φ2(𝐱)Φ2(𝐱)Φ3(𝐱)Φ3(𝐱)GT

C-RPN

Siamese-RPN

Figure 4. Localization using a single regressor and multiple re-
gressors.The multiple regressors in C-RPN can better handle large
scale changes for more accurate localization. Best viewed in color.

Figure 3. Architecture of RPN. Best viewed in color.

for each anchor can be computed as

with negative conﬁdence is larger than a preset threshold θ)
and reﬁne the rest for training RPN in the next stage.

3.1. Siamese Network

As in [2], we adopt the modiﬁed AlexNet [22] to develop
our Siamese network. The Siamese network comprises two
identical branches, the z-branch and the x-branch, which are
employed to extract features from z and x, respectively (see
Fig. 2). The two branches are designed to share parameters
to ensure the same transformation applied to both z and x,
which is crucial for the similarity metric learning. More
details about the Siamese network can be referred to [2].

Different from [23] that only uses the features from the
last layer of the Siamese network for tracking, we leverage
the features from multiple levels to improve model robust-
ness. For convenience in next, we denote ϕi(z) and ϕi(x)
as the feature transformations of z and x from the conv-i
layer in the Siamese network with N layers2.

3.2. One Stage RPN in Siamese Network

Before describing C-RPN, we ﬁrst review the one-stage
Siamese RPN tracker [23], which consists of two branches
of classiﬁcation and regression for anchors, as depicted in
Fig. 3. It takes as inputs the feature transformations ϕ1(z)
and ϕ1(x) of z and x and outputs classiﬁcation scores and
regression offsets for anchors. For simplicity, we remove
the subscripts in feature transformations in next.

To ensure classiﬁcation and regression for each anchor,
two convolution layers are utilized to adjust the channels of
ϕ(z) into suitable forms, denoted as [ϕ(z)]cls and [ϕ(z)]reg,
for classiﬁcation and regression, respectively. Likewise, we
apply two convolution layers for ϕ(x) but keep the channels
unchanged, and obtain [ϕ(x)]cls and [ϕ(x)]reg. Therefore,
the classiﬁcation scores {ci} and the regression offsets {ri}

2For notation simplicity, we name each layer in the Siamese network in
an inverse order, i.e., conv-N , conv-(N − 1), · · · , conv-2, conv-1 for the
low-level to the high-level layers.

{ci} = corr([ϕ(z)]cls, [ϕ(x)]cls)
{ri} = corr([ϕ(z)]reg, [ϕ(x)]reg)

(1)

where i is the anchor index, and corr(a, b) denotes correla-
tion between a and b where a is served as the kernel. Each
ci is a 2d vector, representing for negative and positive con-
ﬁdences of the ith anchor. Similarly, each ri is a 4d vector
which represents the offsets of center point location and size
of the anchor to groundtruth. Siamese RPN is trained with a
multi-task loss consisting of two parts, i.e., the classiﬁcation
loss (i.e., softmax loss) and the regression loss (i.e., smooth
L1 loss). We refer readers to [23, 38] for further details.

3.3. Cascaded RPN

As mentioned earlier, previous Siamese trackers mostly
ignore the problem of class imbalance, resulting in degen-
erated performance in presence of similar semantic distrac-
tors. Besides, they only use the high-level semantic features
from the last layer, which does not fully explore multi-level
features. To address these issues, we propose a multi-stage
tracking framework by cascading a set of L (L ≤ N ) RPNs.
For RPNl in the lth (1 < l ≤ L) stage, it receives fused
features Φl(z) and Φl(x) of conv-l layer and high-level lay-
ers from FTB. The Φl(z) and Φl(x) are obtained as follows,

Φl(z) = FTB(cid:0)Φl−1(z), ϕl(z)(cid:1)
Φl(x) = FTB(cid:0)Φl−1(x), ϕl(x)(cid:1)

(2)

where FTB(·, ·) denotes FTB as described in Section 3.4.
For RPN1, Φ1(z) = ϕ1(z) and Φ1(x) = ϕ1(x). Therefore,
i} and the regression offsets {rl
the classiﬁcation scores {cl
i}
for anchors in stage l are calculated as

{cl
{rl

i} = corr([Φl(z)]cls, [Φl(x)]cls)
i} = corr([Φl(z)]reg, [Φl(x)]reg)

(3)

where [Φl(z)]cls, [Φl(x)]cls, [Φl(z)]reg and [Φl(x)]reg are
derived by performing convolutions on Φl(z) and Φl(x).

Let Al denote the anchor set in stage l. With classiﬁca-
tion scores {cl
i}, we can ﬁlter out anchors in Al whose neg-
ative conﬁdences are larger than a preset threshold θ, and

7955

Conv.Conv.Conv.Conv.Corr.cls. scoresCorr.reg. scoresclassificationregressionloss𝜑(𝐱)𝜑(𝐳)[𝜑𝐳]𝑐𝑙𝑠[𝜑𝐳]𝑟𝑒𝑔[𝜑𝐱]𝑟𝑒𝑔[𝜑𝐱]𝑐𝑙𝑠𝐻′×𝑊′×𝐶′Φ(𝐱)Φ(𝐳)[Φ𝐳]𝑐𝑙𝑠[Φ𝐳]𝑟𝑒𝑔[Φ𝐱]𝑟𝑒𝑔[Φ𝐱]𝑐𝑙𝑠ℎ×𝑤×2𝑘ℎ×𝑤×4𝑘𝜑(𝐱)𝜑(𝐳)[𝜑𝐳]𝑐𝑙𝑠[𝜑𝐳]𝑟𝑒𝑔[𝜑𝐱]𝑟𝑒𝑔[𝜑𝐱]𝑐𝑙𝑠ℎ×𝑤×2𝑘ℎ×𝑤×4𝑘loss(a) Region of interest

(b) Response map in stage 1

(c) Response map in stage 2

(d) Response map in stage 3

(a) Region of interest

(b) From left to right: response maps of stage 1, stage 2 and stage 3

Figure 5. Response maps in different stages. Image (a) is the re-
gion of interest, and (b) shows the response maps obtained by RPN
in three stages. We can see that RPN is sequentially more discrim-
inative in distinguishing distractors. Best viewed in color.

the rest are formed into a new set of anchor Al+1, which
is employed for training RPNl+1. For RPN1, A1 is pre-
deﬁned. Besides, in order to provide a better initialization
for regressor of RPNl+1, we reﬁne the anchors in Al+1 us-
ing regression results {rl
i} in RPNl, thus generate more ac-
curate localization compared to a single step regression in
Siamese RPN [23], as illustrated in Fig. 4. Fig. 2 shows the
cascade architecture of C-RPN.

The loss function ℓRPNl for RPNl is composed of classi-
ﬁcation loss function Lcls (softmax loss) and regression loss
function Lloc (smooth L1 loss) as follows,

ℓRPNl ({c

l

i}, {r

l

i}) = X

Lcls(c

l
i, c

l∗

i ) + λ X

l∗

i Lloc(r

l
i, r

l∗

i )

c

i

i

(4)
where i is the anchor index in Al of stage l, λ a weight to
balance losses, cl∗
the true
i
distance between anchor i and groundtruth. Following [38],
rl∗
i = (rl∗

the label of anchor i, and rl∗
i

i(h)) is a 4d vector, such that

i(w), rl∗

i(x), rl∗

i(y), rl∗

rl∗
i(x) = (x∗ − xl
a)/wl
rl∗
i(w) = log(w∗/wl
a)

a

rl∗
i(y) = (y∗ − yl
rl∗
i(h) = log(y∗/hl
a)

a)/hl

a

(5)

where x, y, w and h are center coordinates of a box and its
width and height. Variables x∗ and xl
a are for groundtruth
and anchor of stage l (likewise for y, w and h). It is worth
noting that, different from [23] using ﬁxed anchors, the an-
chors in C-RPN are progressively adjusted by the regressor
in the previous stage, and computed as

a = xl−1
xl
a = wl−1

rl−1
a + wl−1
i(x)
exp(rl−1
i(w))

wl

a

a

a = yl−1
yl
a = hl−1
hl

a + hl−1
exp(rl−1
i(h))

a rl−1
i(y)

a

(6)

For the anchor in A1, x1

a, y1

a, w1

a and h1

a are pre-deﬁned.

The above procedure forms the proposed cascaded RPN.
Due to the rejection of easy negative anchors, the distribu-
tion of training samples for each RPN is gradually more bal-
anced. As a result, the classiﬁer of each RPN is sequentially
more discriminative in distinguishing difﬁcult distractors.
Besides, multi-level feature fusion further improves the dis-
criminability in handing complex background. Fig. 5 shows
the discriminative powers of different RPNs by demonstrat-
ing detection response map in each stage.

Figure 6. Overview of feature transfer block. Best viewed in color.

The loss function ℓCRPN of C-RPN consists of the loss
functions of all RPNl. For each RPN, loss function is com-
puted using Eq. (4), and ℓCRPN is expressed as

ℓCRPN =

L

X

l=1

ℓRPNl

(7)

3.4. Feature Transfer Block

To effectively leverage multi-level features, we introduce
FTB to fuse features across layers so that each RPN is able
to share high-level semantic feature to improve the discrim-
inability. In detail, a deconvolution layer is used to match
the feature dimensions of different sources. Then, different
features are fused using element-wise summation, followed
a ReLU layer. In order to ensure the same groundtruth for
anchors in each RPN, we apply the interpolation to rescale
the fused features such that the output classiﬁcation and re-
gression maps have the same resolution for all RPN. Fig. 6
shows the feature transferring for RPNl (l > 1).

3.5. Training and Tracking

Training. The training of C-RPN is performed on the image
pairs that are sampled from the same sequence as in [23].
The multi-task loss function in Eq. (7) enables us to train
C-RPN in an end-to-end manner. Considering that the scale
of target changes smoothly in two consecutive frames, we
employ one scale with different ratios for each anchor. The
ratios of anchors are set to [0.33, 0.5, 1, 2, 3] as in [23].

For each RPN, we adopt the strategy as in object detec-
tion [38] to determine positive and negative training sam-
ples. We deﬁne the positive samples as anchors whose In-
tersection over union (IOU) with groundtruth is larger than
a threshold τpos, and negative samples as anchors whose
IoU with groundtruth bounding box is less than a threshold
τneg. We generate at most 64 samples from one image pair.
Tracking. We formulate tracking as multi-stage detection.
For each video, we pre-compute feature embeddings for the
target template in the ﬁrst frame. In a new frame, we extract
a region of interest according to the result in last frame, and

7956

𝐻×𝑊×𝐶𝐻′×𝑊′×𝐶′𝑘×𝑘×𝐶′3×3×𝐶′ReLU3×3×𝐶′3×3×𝐶′𝐻′×𝑊′×𝐶′𝐻×𝑊×𝐶′Deconv𝑘×𝑘×𝐶′Conv 3×3×𝐶′Conv 3×3×𝐶′Conv 3×3×𝐶′𝐻×𝑊×𝐶𝐻′×𝑊′×𝐶′𝜑𝑙Φ𝑙−1Eltw sumReLUInterpolation𝐻′×𝑊′×𝐶′𝐻×𝑊×𝐶′Algorithm 1: Tracking with C-RPN
1 Input: frame sequences {Xt}T

t=1 and groundtruth

bounding box b1 of X1, trained model C-RPN;

t=2;

2 Output: Tracking results {bt}T
3 Extract target template z in X1 using b1 ;
4 Extract features {ϕl(z)}L
5 for t = 2 to T do
6

l=1 for z from C-RPN;

Extract the search region x in Xt using bt−1 ;
Extract features {ϕl(x)}L
Initialize anchors A1;
for l = 1 to L do

l=1 for x from C-RPN;

if l equals to 1 then

Φl(z) = ϕl(z), Φl(x) = ϕl(x);

else

Φl(z), Φl(x) ← Eq. (2) ;

i}, {rl

i} ← Eq. (3);

end
{cl
Remove any anchor i from Al with negative
conﬁdence cl
Al+1 ← Reﬁne the rest anchors in Al with
{rl

i(neg) > θ ;

i} using Eq. (6);

end
Target proposals ← AL+1 ;
Select the best proposal as tracking result bk

using strategies in [23];

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21 end

then perform detection using C-RPN on this region. In each
stage, an RPN outputs classiﬁcation scores and regression
offsets for anchors. The anchors with negative scores larger
than θ are discarded, and the rest are reﬁned and taken over
by RPN in next stage. After the last stage L, the remained
anchors are regarded as target proposals, from which we se-
lect the best one as the ﬁnal tracking result using strategies
in [23]. Alg. 1 summarizes the tracking process by C-RPN.

4. Experiments

Implementation detail. We implement C-RPN in Matlab
using MatConvNet [47] on a single Nvidia GTX 1080 with
8GB memory. The backbone Siamese network adopts the
modiﬁed AlexNet [22]. Instead of training from scratch, we
borrow the parameters from the pretrained model on Ima-
geNet [9]. During training, the parameters of ﬁrst two lay-
ers are frozen. The number L of stages is 3. The thresholds
θ, τpos and τneg are empirically set to 0.95, 0.6 and 0.3. C-
RPN is trained end-to-end over 50 epochs using SGD, and
the learning rate is annealed geometrically at each epoch
from 10−2 to 10−6. We train our C-RPN using the training
data from [10] for experiments on LaSOT [10], and using
VID [39] and YT-BB [37] for other experiments.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 
s
s
e
c
c
u
S

Success plots of OPE

SA-Siam [0.676]
C-RPN [0.675]
CREST [0.673]
PTAV [0.663]
SiamRPN [0.658]
ACT [0.657]
DaSiamRPN [0.655]
ECO-HC [0.652]
TRACA [0.652]
BACF [0.648]
SINT [0.635]
SiamFC [0.607]
HCFT [0.605]
HDT [0.603]
CFNet [0.603]
Staple [0.600]

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 
s
s
e
c
c
u
S

Success plots of OPE

C-RPN [0.663]
DaSiamRPN [0.658]
SA-Siam [0.656]
ECO-HC [0.643]
SiamRPN [0.637]
PTAV [0.635]
ACT [0.625]
CREST [0.623]
BACF [0.617]
TRACA [0.603]
SiamFC [0.582]
Staple [0.581]
SINT [0.580]
CFNet [0.566]
HDT [0.564]
HCFT [0.562]

0

0

0.1

0.2

0.4

0.3
0.7
Overlap threshold

0.5

0.6

0.8

0.9

1

0

0

0.1

0.2

0.4

0.3
0.7
Overlap threshold

0.5

0.6

0.8

0.9

1

Figure 7. Comparisons with stage-of-the-art tracking approaches
on OTB-2013 [52] and OTB-2015 [53]. C-RPN achieves the best
results on both benchmarks. Best viewed in color.

4.1. Experiments on OTB 2013 and OTB 2015

We conduct experiments on the popular OTB-2013 [52]
and OTB-2015 [53] which consist of 51 and 100 fully an-
notated videos, respectively. C-RPN runs at around 36 fps.
Following [52], we employ the success plot in one-pass
evaluation (OPE) to assess different trackers. The compari-
son with 15 state-of-the-art trackers (SiamRPN [23], DaSi-
amRPN [59], TRACA [6], ACT [4], BACF [13], ECO-
HC [7], CREST [42], SiamFC [2], Staple [1], PTAV [11],
SINT [45], CFNet [46], SA-Siam [17], HDT [36] and
HCFT [32]) is shown in Fig. 7. C-RPN achieves promising
performance on both two benchmarks. In speciﬁc, we ob-
tain the 0.675 and 0.663 precision scores on OTB-2013 and
OTB-2015, respectively. In comparison with the baseline
SiamRPN with 0.658 and 0.637 precision scores, we obtain
improvements by 1.9% and 2.6%, showing the advantages
of multi-stage RPNs in accurate localization. DaSiamRPN
uses extra negative training data from other domains to im-
prove the ability to handle similar distractors, and obtains
0.655 and 0.658 precision scores. Without using extra train-
ing data, C-RPN outperforms DaSiamRPN by 2.0% and
0.5%. More results and comparisons on OTB-2013 [52] and
OTB-2015 [53] are shown in the supplementary material.

4.2. Experiments on VOT 2016 and VOT 2017

VOT-2016 [20] consists of 60 sequences, aiming at as-
sessing the short-term performance of trackers. The overall
performance of a tracking algorithm is evaluated using Ex-
pected Average Overlap (EAO) which takes both accuracy
and robustness into account. The speed of a tracker is rep-
resented with a normalized speed (EFO).

We evaluate C-RPN on VOT-2016, and compare it with
11 trackers including the baseline SiamRPN [23] and other
top ten approaches in VOT-2016. Fig. 8 shows the EAO
of different trackers. C-RPN achieves the best results, sig-
niﬁcantly outperforming SiamRPN and other approaches.
Tab. 1 lists the comparisons of different trackers on VOT-
2016, and we can see that C-RPN outperforms other track-

7957

Expected overlap scores for baseline

0.38

0.36

0.34

0.32

0.3

0.28

p
a
l
r
e
v
o

d
e

t
c
e
p
x
e
e
g
a
r
e
v
A

C-RPN
SiamRPN
CCOT
TCNN
SSAT
MLDF
Staple
DDC
EBT
SRBT
STAPLEp
DNT

12

11

10

9

8

7
6
Order

5

4

3

2

1

Figure 8. Comparisons on VOT-2016 [20]. Larger (right) value in-
dicates better performance. Our C-RPN signiﬁcantly outperforms
the baseline and other approaches. Best viewed in color.

Table 1. Detailed comparisons on VOT-2016 [20]. The best two
results are highlighted in red and blue fonts, respectively.

Tracker

EAO

Accuracy

Failure

EFO

C-RPN 0.363

SiamRPN [23]
C-COT [8]
TCNN [20]
SSAT [20]
MLDF [20]
Staple [1]
DDC [20]
EBT [58]
SRBT [20]
STAPLEp [20]
DNT [5]

0.344
0.331
0.325
0.321
0.311
0.295
0.293
0.291
0.290
0.286
0.278

0.594

0.560
0.539
0.554
0.577
0.490
0.544
0.541
0.465
0.496
0.557
0.515

0.95

1.12
0.85
0.96
1.04
0.83
1.35
1.23
0.90
1.25
1.32
1.18

9.3

23.0
0.5
1.1
0.5
1.2
11.1
0.2
3.0
3.7
44.8
1.1

e

t

a
r
 
s
s
e
c
c
u
S

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Table 2. Comparisons on VOT-2017 [21]. The best two results are
highlighted in red and blue fonts, respectively.

Tracker Baseline EAO Real-time EAO

C-RPN

SiamRPN [23]
LSART [44]
CFWCR [21]
CFCF [15]
ECO [7]
Gnet [21]
MCCT [21]
C-COT [8]
CSRDCF [31]
SiamDCF [21]
MCPF [56]

0.289

0.243
0.323
0.303
0.286
0.280
0.274
0.270
0.267
0.256
0.249
0.248

0.273

0.244
0.055
0.062
0.059
0.078
0.060
0.061
0.058
0.100
0.135
0.060

Success plots of OPE on LaSOT

[0.459] C-RPN
[0.440] SiamRPN
[0.413] MDNet
[0.412] VITAL
[0.358] SiamFC
[0.356] StructSiam
[0.353] DSiam
[0.340] ECO
[0.339] SINT
[0.315] STRCF
[0.311] ECO_HC
[0.296] CFNet
[0.285] TRACA
[0.280] MEEM
[0.277] BACF
[0.272] HCFT
[0.271] SRDCF
[0.269] PTAV
[0.266] Staple
[0.263] CSRDCF
[0.262] Staple_CA
[0.258] SAMF
[0.246] LCT
[0.234] Struck
[0.233] DSST
[0.232] fDSST
[0.228] TLD
[0.214] SCT4
[0.211] ASLA
[0.211] KCF
[0.186] CN
[0.178] CT
[0.172] CSK
[0.168] L1APG
[0.163] MIL
[0.151] STC
[0.136] IVT

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e

t

a
r
 
s
s
e
c
c
u
S

Success plots of OPE on LaSOT Testing Set

[0.455] C-RPN
[0.433] SiamRPN
[0.397] MDNet
[0.390] VITAL
[0.336] SiamFC
[0.335] StructSiam
[0.333] DSiam
[0.324] ECO
[0.314] SINT
[0.308] STRCF
[0.304] ECO_HC
[0.275] CFNet
[0.259] BACF
[0.257] TRACA
[0.257] MEEM
[0.250] HCFT
[0.250] PTAV
[0.245] SRDCF
[0.244] CSRDCF
[0.243] Staple
[0.238] Staple_CA
[0.233] SAMF
[0.221] LCT
[0.212] Struck
[0.210] TLD
[0.207] DSST
[0.203] fDSST
[0.194] ASLA
[0.191] SCT4
[0.178] KCF
[0.170] CN
[0.158] CT
[0.155] L1APG
[0.149] CSK
[0.139] MIL
[0.138] STC
[0.118] IVT

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Overlap threshold

Overlap threshold

ers in both accuracy and robustness, and runs efﬁciently.

VOT-2017 [21] contains 60 sequences, which are devel-
oped by replacing the 10 least challenging videos in VOT-
2016 [20] with 10 difﬁcult sequences. Different from VOT-
2016 [20], VOT-2017 [21] introduces a new real-time ex-
periment by taking into both tracking performance and efﬁ-
ciency. We compare C-RPN with SiamRPN [23] and other
top ten approaches in VOT-2017 using the EAO of base-
line and real-time experiments. As shown in Tab. 2, C-RPN
achieves the EAO score of 0.289, which signiﬁcantly out-
performs SiamRPN [23] with 0.243 EAO score. Besides,
compared to LSART [44] and CFWCR [21], C-RPN shows
competitive performance. In real-time experiment, C-RPN
obtains the best performance with EAO score of 0.273, out-
performing all other trackers.

4.3. Experiment on LaSOT

LaSOT [10] is a recent large-scale dataset aiming at both
training and evaluating trackers. There are 1,400 videos in
LaSOT. We compare the proposed C-RPN to 35 approaches
in LaSOT, including ECO [7], MDNet [35], SiamFC [2],
VITAL [43], StructSiam [57], TRACA [6], BACF [13] and
so on. We refer readers to [10] for more details about these

Figure 9. Comparisons with state-of-the-art tracking methods on
LaSOT [10]. C-RPN outperforms existing approaches on success
by large margins under all two protocols. Best viewed in color.

trackers. In addition, we also compare C-RPN with the re-
cent SiamRPN [23] tracker as it is an important baseline.

Following [10], we report the results of success (SUC)
for different trackers as shown in Fig. 9. It shows that our C-
RPN outperforms all other trackers under two protocols. We
achieve SUC scores of 0.459 and 0.455 under protocol I and
II, outperforming the second best tracker SiamRPN with
SUC scores 0.44 and 0.433 by 1.9% and 2.2%, respectively.
Compared to SiamFC with 0.358 and 0.336 SUC scores,
C-RPN gains the improvements by 11.1% and 11.9%. C-
RPN runs at around 23 fps on LaSOT. We refer readers to
supplementary material for more details about results and
comparisons on LaSOT.

4.4. Experiment on TrackingNet

TrackingNet [34] is proposed to assess the performance
of a tracker in the wild. We evaluate C-RPN on its testing
set with 511 videos. Following [34], we utilize three metrics
precision (PRE), normalized precision (NPRE) and success
(SUC) for evaluation. Tab. 3 demonstrates the comparison

7958

Table 3. Comparisons on TrackingNet [34] with the best two re-
sults highlighted in red and blue fonts, respectively.

PRE

NPRE

SUC

C-RPN 0.619

MDNet [35]
CFNet [46]
SiamFC [2]
ECO [7]
CSRDCF [31]
SAMF [26]
ECO-HC [7]
Staple [1]
Staple CA [33]
BACF [13]

0.565
0.533
0.533
0.492
0.48
0.477
0.476
0.470
0.468
0.461

0.746

0.705
0.654
0.663
0.618
0.622
0.598
0.608
0.603
0.605
0.580

0.669

0.606
0.578
0.571
0.554
0.534
0.504
0.541
0.528
0.529
0.523

Table 4. Effect on the number of stages in C-RPN.

# Stages One stage

Two stages

Three stages

SUC on LaSOT
Speed on LaSOT

0.417
48 fps

EAO on VOT-2017

0.248

0.446
37 fps

0.278

0.455
23 fps

0.289

Table 5. Effect on negative anchor ﬁltering (NAF) in C-RPN.

C-RPN w/o NAF C-RPN w/ NAF

SUC on LaSOT

EAO on VOT-2017

0.439

0.282

0.455

0.289

Table 6. Effect on feature transfer block in C-RPN. “S” and “M”
indicate using single and multiple layers, respectively.

C-RPN w/o

C-RPN w/o

FTB (S)

FTB (M)

C-RPN w/
FTB (M)

SUC on LaSOT

EAO on VOT-2017

0.442

0.278

0.449

0.282

0.455

0.289

results to trackers with top PRE scores3, showing that C-
RPN achieves the best results on all three metrics. In spe-
ciﬁc, C-RPN obtains the PRE score of 0.619, NPRE score
of 0.746 and SUC score of 0.669, outperforming the second
best tracker MDNet with PRE score of 0.565, NPRE score
of 0.705 and SUC score of 0.606 by 5.4%, 4.1% and 6.3%,
respectively. Besides, C-RPN runs efﬁciently at a speed of
around 32 fps.

4.5. Ablation Experiment

To validate the impact of different components, we con-
duct ablation experiments on LaSOT (Protocol II) [10] and
VOT-2017 [21].
Number of stages? As shown in Tab. 4, adding the second
stage signiﬁcantly improves one-stage baseline. The SUC
on LaSOT is improved by 2.9% from 0.417 to 0.446, and

3The result of C-RPN on TrackingNet [34] is evaluated by the server
provided by the organizer at http://eval.tracking-net.org/
web/challenges/challenge-page/39/leaderboard/42.
The results of compared trackers are reported from [34]. Full comparison
is shown in the supplementary material.

the EAO on VOT-2017 is increased by 3.5% from 0.248 to
0.283. The third stage produces 0.9% and 0.6% improve-
ments on LaSOT and VOT-2017, respectively. We observe
that the improvement by the second stage is higher than that
by the third stage. This suggests that most difﬁcult back-
ground is handled in the second stage. Adding more stages
may lead to further improvements, but also the computation.
Negative anchor ﬁltering? Filtering out the easy negatives
aims to provide more balanced training samples for RPN in
next stage. To show its effectiveness, we set threshold θ to
1 such that all reﬁned anchors will be send to the next stage.
Tab. 5 shows that removing negative anchors in C-RPN can
improve the SUC on LaSOT by 1.6% from 0.439 to 0.455,
and the EAO on VOT-2017 by 0.7% from 0.282 to 0.289,
respectively, which evidences balanced training samples are
crucial for training more discriminative RPN.
Feature transfer block? We conduct experiments on two
baselines to show the effect of FTB on performance of C-
RPN: (a) we only employ the features from one single con-
volution layer (the last layer) in C-RPN; (b) we utilize mul-
tiple layers (the last three layers) in C-RPN but ﬁrstly per-
form correlation for each layer and then fuse the results of
all layers (i.e., decision-level fusion). Note that, for both
baselines (a) and (b), we adopt the cascading strategy. The
comparisons of these two baselines with the proposed ap-
proach is demonstrated in Tab. 6. We observe that the use
of features from multiple layers helps improve the SUC on
LaSOT by 0.7% from 0.442 to 0.449, and the EAO on VOT-
2017 by 0.4% from 0.278 to 0.282. Further, the combina-
tion of FTB and multi-layer features boosts the performance
to 0.455 and 0.289 on LaSOT and VOT-2017, respectively,
validating the effectiveness of multi-level feature fusion in
improving performance.

These studies show that each ingredient brings individual
improvement, and all of them work together to produce the
excellent tracking performance.

5. Conclusion

In this paper, we propose a novel multi-stage framework
C-RPN for tracking. Compared with previous state-of-the-
arts, C-RPN demonstrates more robust performance in han-
dling complex background such as similar semantic distrac-
tors by performing hard negative sampling within a cascade
architecture. In addition, we present a novel FTB module
that enables effective feature usage across layers for more
discriminative representation. Moreover, C-RPN progres-
sively reﬁnes the target bounding box using multiple steps
of regressions, leading to more accurate localization. In our
extensive experiments on six popular benchmarks, C-RPN
consistently achieves the state-of-the-art results and runs in
real-time.

Acknowledgement. This work was supported in part by
US NSF grants 1814745, 1618398, 1407156, and 1350521.

7959

References

[1] Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej
Miksik, and Philip HS Torr. Staple: Complementary learners
for real-time tracking. In CVPR, 2016. 6, 7, 8

[2] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea
Vedaldi, and Philip HS Torr. Fully-convolutional siamese
networks for object tracking. In ECCVW, 2016. 1, 3, 4, 6, 7,
8

[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving

into high quality object detection. In CVPR, 2018. 2, 3

[4] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and
Huchuan Lu. Real-time actor-critictracking. In ECCV, 2018.
6

[5] Zhizhen Chi, Hongyang Li, Huchuan Lu, and Ming-Hsuan
TIP,

Dual deep network for visual

tracking.

Yang.
26(4):2005–2015, 2017. 7

[6] Jongwon Choi, Hyung Jin Chang, Tobias Fischer, Sangdoo
Yun, Kyuewang Lee, Jiyeoup Jeong, Yiannis Demiris, and
Jin Young Choi. Context-aware deep feature compression
for high-speed visual tracking. In CVPR, 2018. 6, 7

[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan,
Michael Felsberg, et al. Eco: Efﬁcient convolution opera-
tors for tracking. In CVPR, 2017. 2, 6, 7, 8

[8] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,
and Michael Felsberg. Beyond correlation ﬁlters: Learn-
ing continuous convolution operators for visual tracking. In
ECCV, 2016. 2, 7

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 6

[10] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia
Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.
Lasot: A high-quality benchmark for large-scale single ob-
ject tracking. arXiv, 2018. 2, 6, 7, 8

[11] Heng Fan and Haibin Ling. Parallel tracking and verifying:
A framework for real-time and high accuracy visual tracking.
In ICCV, 2017. 6

[12] Heng Fan and Haibin Ling. Sanet: Structure-aware network

for visual tracking. In CVPRW, 2017. 2

[19] David Held, Sebastian Thrun, and Silvio Savarese. Learning
to track at 100 fps with deep regression networks. In ECCV,
2016. 1, 3

[20] Matej Kristan et al. The visual object tracking vot2016 chal-

lenge results. In ECCVW, 2016. 2, 6, 7

[21] Matej Kristan et al. The visual object tracking vot2017 chal-

lenge results. In ICCVW, 2017. 2, 7, 8

[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012. 2, 4, 6

[23] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.
High performance visual tracking with siamese region pro-
posal network. In CVPR, 2018. 1, 2, 3, 4, 5, 6, 7

[24] Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and
Gang Hua. A convolutional neural network cascade for face
detection. In CVPR, 2015. 3

[25] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep
visual tracking: Review and experimental comparison. PR,
76:323–338, 2018. 2

[26] Yang Li and Jianke Zhu. A scale adaptive kernel correlation

ﬁlter tracker with feature integration. In ECCVW, 2014. 8

[27] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian D
Reid. Reﬁnenet: Multi-path reﬁnement networks for high-
resolution semantic segmentation. In CVPR, 2017. 2

[28] Tsung-Yi Lin, Piotr Doll´ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, 2017. 2

[29] Tsung-Yi Lin, Priyal Goyal, Ross Girshick, Kaiming He, and
Piotr Doll´ar. Focal loss for dense object detection. In ICCV,
2017. 2

[30] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015. 2, 3

[31] Alan Lukezic, Tomas Vojir, Luka Cehovin Zajc, Jiri Matas,
and Matej Kristan. Discriminative correlation ﬁlter with
channel and spatial reliability. In CVPR, 2017. 7, 8

[32] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan
Yang. Hierarchical convolutional features for visual tracking.
In ICCV, 2015. 2, 3, 6

[13] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.
Learning background-aware correlation ﬁlters for visual
tracking. In ICCV, 2017. 6, 7, 8

[33] Matthias Mueller, Neil Smith, and Bernard Ghanem.
In CVPR, 2017.

Context-aware correlation ﬁlter tracking.
8

[14] Spyros Gidaris and Nikos Komodakis. Object detection via
a multi-region and semantic segmentation-aware cnn model.
In ICCV, 2015. 2

[15] Erhan Gundogdu and A Aydın Alatan. Good features to cor-

relate for visual tracking. TIP, 27(5):2526–2540, 2018. 7

[16] Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. To-
wards a better match in siamese network based visual object
tracker. In ECCVW, 2018. 3

[17] Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. A
In

twofold siamese network for real-time object tracking.
CVPR, 2018. 1, 3, 6

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 2

[34] Matthias M¨uller, Adel Bibi, Silvio Giancola, Salman Al-
Subaihi, and Bernard Ghanem. Trackingnet: A large-scale
dataset and benchmark for object tracking in the wild.
In
ECCV, 2018. 2, 7, 8

[35] Hyeonseob Nam and Bohyung Han. Learning multi-domain
convolutional neural networks for visual tracking. In CVPR,
2016. 2, 7, 8

[36] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao,
Qingming Huang, Jongwoo Lim, and Ming-Hsuan Yang.
Hedged deep tracking. In CVPR, 2016. 6

[37] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan,
and Vincent Vanhoucke. Youtube-boundingboxes: A large
high-precision human-annotated data set for object detection
in video. In CVPR, 2017. 6

7960

[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015. 1, 2, 4, 5

[57] Yunhua Zhang, Lijun Wang, Jinqing Qi, Dong Wang,
Mengyang Feng, and Huchuan Lu. Structured siamese net-
work for real-time visual tracking. In ECCV, 2018. 7

[58] Gao Zhu, Fatih Porikli, and Hongdong Li. Beyond local
search: Tracking objects everywhere with instance-speciﬁc
proposals. In CVPR, 2016. 7

[59] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and
Weiming Hu. Distractor-aware siamese networks for visual
object tracking. In ECCV, 2018. 1, 3, 6

[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge.
IJCV, 115(3):211–252,
2015. 6

[40] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.
Training region-based object detectors with online hard ex-
ample mining. In CVPR, 2016. 2

[41] Arnold WM Smeulders, Dung M Chu, Rita Cucchiara, Si-
mone Calderara, Afshin Dehghan, and Mubarak Shah. Vi-
sual tracking: An experimental survey. TPAMI, 36(7):1442–
1468, 2014. 2

[42] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Ryn-
son WH Lau, and Ming-Hsuan Yang. Crest: Convolutional
residual learning for visual tracking. In ICCV, 2017. 6

[43] Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao
Bao, Wangmeng Zuo, Chunhua Shen, Rynson Lau, and
Ming-Hsuan Yang. Vital: Visual tracking via adversarial
learning. In CVPR, 2018. 2, 7

[44] Chong Sun, Huchuan Lu, and Ming-Hsuan Yang. Learning
spatial-aware regressions for visual tracking. In CVPR, 2018.
7

[45] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders.
In CVPR, 2016. 1,

Siamese instance search for tracking.
3, 6

[46] Jack Valmadre, Luca Bertinetto, Jo˜ao Henriques, Andrea
Vedaldi, and Philip HS Torr. End-to-end representation
learning for correlation ﬁlter based tracking. In CVPR, 2017.
1, 3, 6, 8

[47] Andrea Vedaldi and Karel Lenc. Matconvnet: Convolutional

neural networks for matlab. In ACM MM, 2015. 6

[48] Paul Viola, Michael Jones, et al. Rapid object detection using

a boosted cascade of simple features. In CVPR, 2001. 2, 3

[49] Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan
In

Lu. Visual tracking with fully convolutional networks.
ICCV, 2015. 2, 3

[50] Naiyan Wang and Dit-Yan Yeung. Learning a deep compact

image representation for visual tracking. In NIPS, 2013. 2

[51] Qiang Wang, Zhu Teng, Junliang Xing, Jin Gao, Weiming
Hu, and Stephen Maybank. Learning attentions: residual
attentional siamese network for high performance online vi-
sual tracking. In CVPR, 2018. 1, 3

[52] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object

tracking: A benchmark. In CVPR, 2013. 2, 6

[53] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-

ing benchmark. TPAMI, 37(9):1834–1848, 2015. 2, 6

[54] Alper Yilmaz, Omar Javed, and Mubarak Shah. Object track-

ing: A survey. ACM CSUR, 38(4):13, 2006. 2

[55] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and
Stan Z Li. Single-shot reﬁnement neural network for object
detection. In CVPR, 2018. 3

[56] Tianzhu Zhang, Changsheng Xu, and Ming-Hsuan Yang.
Multi-task correlation particle ﬁlter for robust object track-
ing. In CVPR, 2017. 7

7961

