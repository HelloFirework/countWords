Probabilistic End-to-end Noise Correction for Learning with Noisy Labels

Kun Yi

Jianxin Wu∗

National Key Laboratory for Novel Software Technology

Nanjing University, Nanjing, China

yik@lamda.nju.edu.cn, wujx2001@nju.edu.cn

Abstract

Deep learning has achieved excellent performance in var-
ious computer vision tasks, but requires a lot of training
examples with clean labels. It is easy to collect a dataset
with noisy labels, but such noise makes networks overﬁt se-
riously and accuracies drop dramatically. To address this
problem, we propose an end-to-end framework called PEN-
CIL, which can update both network parameters and label
estimations as label distributions. PENCIL is independent
of the backbone network structure and does not need an aux-
iliary clean dataset or prior information about noise, thus
it is more general and robust than existing methods and is
easy to apply. PENCIL outperforms previous state-of-the-art
methods by large margins on both synthetic and real-world
datasets with different noise types and noise rates. Exper-
iments show that PENCIL is robust on clean datasets, too.

1. Introduction

Deep learning has shown very impressive performance
on various vision problems, e.g., classiﬁcation, detection and
semantic segmentation. Although there are many factors for
the success of deep learning, one of the most important is
the availability of large-scale datasets with clean annotations
like ImageNet [3].

However, collecting a large scale dataset with clean labels
is expensive and time-consuming. On one hand, expert
knowledge is necessary for some datasets such as the ﬁne-
grained CUB-200 [26], which demands knowledge from
ornithologists. On the other hand, we can easily collect a
large scale dataset with noisy annotations through image
search engines [4, 11, 20]. These noisy annotations can
be obtained by extracting labels from the surrounding texts
or using the searching keywords [28]. For a huge dataset
like JFT300M (which contains 300 million images), it is

∗This research was partially supported by the National Natural Science
Foundation of China (61772256, 61422203). J. Wu is the corresponding
author.

impossible to manually label it and inevitably about 20%
noisy labels exist in this dataset [22]. Hence, being able to
deal with noisy labels is essential.

The label noise problem has been studied for a long
time [1, 17]. Along with the recent successes of various deep
learning methods, noise handling in deep learning has gained
momentum, too [18, 21, 28]. However, existing methods
often have prerequisites that may not be practical in many
applications, e.g., an auxiliary set with clean labels [28] or
prior information about the noise [16]. Some methods are
very complex [29], which hurts their deployment capabil-
ity. Overﬁtting to noise is another serious difﬁculty. For a
DNN with enough capacity, it can memorize the random
labels [30]. Thus, some noise handling methods may ﬁnally
still overﬁt and their performance decline seriously, i.e., they
are not robust. Their accuracies on the clean test set reach a
peak in the middle of the training process, but will degrade
afterwards and the accuracies after the ﬁnal training epoch
are poor [16, 24].

We attack the label noise problem from two aspects. First,
we model the label for an image as a distribution among
all possible labels [6] instead of a ﬁxed categorical value.
This probabilistic modeling lends us the ﬂexibility to handle
noise-contaminated and noise-free labels in a uniﬁed man-
ner. Second, inspired by [23], we maintain and update the
label distributions in both network parameter learning (in
which label distributions act as labels) and label learning (in
which label distributions are updated to correct noise). Un-
like [23] which updates labels simply by using the running
average of network predictions, we correct noise and update
our label distributions in a principled end-to-end manner.
The proposed framework is called PENCIL, meaning proba-
bilistic end-to-end noise correction in labels. The PENCIL
framework only uses the noisy labels to initialize our label
distributions, then iteratively correct the noisy labels by up-
dating the label distributions, and the network loss function
is computed using the label distributions rather than the noisy
labels.

Our contributions are as follows.

• We propose an end-to-end framework PENCIL for

7017

noisy label handling. PENCIL is independent of the
backbone network structure and does not need an auxil-
iary clean dataset or prior information about noise, thus
it is easy to apply. PENCIL utilizes back-propagation
to probabilistically update and correct image labels be-
yond updating the network parameters. To the best of
our knowledge, PENCIL is the ﬁrst method in this line.

• We propose a variant of the DLDL method [6], which is
essential for correcting noise contained in our label dis-
tributions. PENCIL achieves state-of-the-art accuracy
on datasets with both synthetic and real-world noisy
labels (e.g., CIFAR-10, CIFAR-100 and Clothing1M).

• PENCIL is robust. It is not only robust in learning with
noisy labels, but also robust enough to apply in datasets
with zero or small amount of potential label noise (e.g.,
CUB-200) to improve accuracy.

2. Related Works

We ﬁrst brieﬂy introduce related works that inspired this

work and other noise handling methods in the literature.

Deep label distribution learning was introduced in [6]
(called DLDL), which was proposed to handle label uncer-
tainty by converting a categorical label (e.g., 25 years old)
into a label distribution (e.g., a normal distribution whose
mean is 25 and standard deviation is 3). The DLDL method
uses constant label distributions and the Kullback-Leibler
divergence to compute the network loss. In PENCIL, we
use label distributions for a different purpose such that the
label distributions can be updated and hence noise can be
probabilistically corrected. The original DLDL method did
not work in our setup and we designed a new loss function
in PENCIL to overcome this difﬁculty.

For deep learning methods, [30] showed that a deep net-
work with large enough capacity can memorize the training
set labels even when they are randomly generated. Hence,
they are particularly susceptible to noisy labels. Label noise
can lead to serious overﬁtting and dramatically reduce net-
work accuracy. However, [23] observed that when the learn-
ing rate is high, DNNs may maintain relatively high accuracy
(i.e., the impact of label noise is not signiﬁcant). This ob-
servation was utilized in [23] to maintain an estimate of the
labels using the running average of network predictions with
a large learning rate. Then, these estimates were used as
supervision signals to train the network. PENCIL is inspired
by this observation and [23], too.

Label noise is an important issue and has long been re-
searched [1, 17]. There are mainly two types of label noise:
symmetric noise and asymmetric noise, which are modeled
in [13] and [21], respectively. [5] is a survey of relatively
early methods. [19] argued that deep neural networks are
inherently robust to label noise to some extent. And, deep

methods have achieved state-of-the-art results in recent years.
Hence, we mainly focus on noise handling in deep learning
models in this section.

One intuitive and easy solution is to delete all the samples
which are considered as unreliable [2]. However, many difﬁ-
cult samples will be deleted, but these samples are important
to algorithm’s accuracy [8]. Thus, more profound noisy label
handling methods become necessary.

There are mainly two lines of attack to the the noisy
label problem: constructing a special model based on noisy
labels or using a robust loss function. The objective of these
methods is to construct a noise-aware model which explicitly
deals with noisy labels. [28] constructed a model to deal with
noisy labels, and tested their method on a real-world dataset
collected by them. [24] proposed a framework called CNN-
CRF, which combined convolutional neural networks (CNN)
with conditional random ﬁelds (CRF) to characterize noisy
labels. [29] utilized similar ideas to determine the conﬁdence
of each label. This approach is gaining popularity in recent
years (e.g., in [14, 15, 25]), and different techniques such
as local inherent dimensionality have been brought into the
noisy label learning domain.

Another effective approach is to design robust loss func-
tions in order for a noise-tolerant model. Forward and back-
ward methods [16] explicitly modeled the noise transition
matrix in loss computation. [7] investigated the robustness
of different loss functions, such as the mean squared loss,
mean absolute loss and cross entropy loss. [31] combined
advantages of the mean absolute loss and cross entropy loss
to obtain a better loss function.

[23] did not fall in these two categories. It is special
in the sense that it replaced the noisy label with their own
estimate of the label (i.e., running average of the network’s
predictions). This approach is effective in noise handling but
ad-hoc. PENCIL is partly inspired by this work, but more
principled and effective.

Existing methods usually have prerequisites that are im-
practical, such as demanding an additional clean dataset (e.g.,
to curb overﬁtting) or a groundtruth noise transition matrix.
When these prerequisites are not satisﬁed, they often fail
to produce robust models. These methods are sometimes
too complex to be deployed in real-world applications. In
contrast, the proposed PENCIL method does not require
additional information, and it can be easily applied to any
backbone network.

3. The Proposed PENCIL Method

First of all, we deﬁne the notations for our study. Column
vectors are denoted in bold (e.g., x) and matrices in capital
form (e.g., X). Speciﬁcally, 1 is a vector of all-ones. We
use both hard labels and soft labels. The hard-label space is
H = {y : y ∈ {0, 1}c, 1⊤y = 1}, and the soft-label space
is S = {y : y ∈ [0, 1]c, 1⊤y = 1}. That is, a soft-label is a

7018

label distribution.

3.1. Probabilistic modeling of noisy labels

In a c-class classiﬁcation problem, we have a training set
X = {x1, x2, . . . , xn}. In the ideal scenario, every image
xi has a clean label yi ∈ H, which is a one-hot vector (i.e.,
equivalent to an integer between 1 and c). In our noisy label
problem, the labels might be wrong with relatively high
probability and we use ˆyi ∈ H to denote labels which may
contain noise. Using cross entropy, the loss function is

L = −

1
n

n

c

Xi=1

Xj=1

ˆyij log fj(xi; θ) ,

(1)

where ˆyij is the j’th element of ˆyi, f is a model’s predic-
tion (processed by the softmax function) and θ is the set of
network parameters.

In PENCIL, we maintain a label distribution yd

i ∈ S =
{y : y ∈ [0, 1]c, 1⊤y = 1} for every image xi, which is
our estimate of the underlying noise-free label for xi. yd
i is
used as the pseudo-groundtruth label in our learning, which
is initialized based on the noisy label ˆyi. It is continuously
updated (i.e., the noise is gradually corrected) through back-
propagation. This probabilistic setting allows ample ﬂexibil-
ity for noise correction. Note that our probabilistic modeling
of the noisy labels is different from that in DLDL [6]. Label
distributions in DLDL are ﬁxed and cannot be updated.

In [6], the loss function is KL-divergence:

L =

1
n

n

Xi=1

KL(yd

i ||f (xi; θ)) =

KL(yd

i ||f (xi; θ)), and

yd

ij log  yd

fj(xi; θ)! .

ij

c

Xj=1

(2)

(3)

This loss is used in [23], too. However, KL-divergence is
an asymmetric function. Hence, if we exchange the two
operands in Eq. 2, we obtain a new loss function

L =

1
n

n

Xi=1

KL(f (xi; θ)||yd

i ) =

KL(f (xi; θ)||yd

i ), and

(4)

c

Xj=1

fj(xi; θ) log  fj(xi; θ)

ij ! .

yd

(5)

input 
image

࢞

backbone

CNN

network
prediction

݂ሺ࢞; ࣂሻ

label

initialization

࢟෥

softmax

label 

distribution

࢟ௗ

noisy
label

࢟ෝ

entropy

loss
ℒ௘

classification

compatibility

loss
ℒ௖

loss
ℒ௢

Figure 1. The PENCIL learning framework. We use label dis-
d (which is the softmax transformed version of label
tributions y
initialization variables ˜y) to replace noisy labels ˆy. The label dis-
tributions are updated in every iteration using three loss functions,
among which the classiﬁcation loss and compatibility loss updates
d by requiring the label distributions produce both smooth models

y
and not too distant from the noisy labels.

3.2. End to end noise correction in labels

Our label distribution yd models the unknown noise-free
label for xi. Hence, we need to estimate these distributions
in our learning process. Let X and Y d be the union of xi
and yd
i (for all 1 ≤ i ≤ n), respectively. Inspired by [23],
we let Y d be part of the parameters that are to be updated
in the back-propagation process. That is, PENCIL not only
updates the network parameters θ as in traditional networks,
but also updates Y d (i.e., yd
i ) in every iteration. Therefore,
we optimize both network parameters and label distributions
as follows:

L(θ, Y d|X)

min
θ,Y d

(6)

The overall architecture of PENCIL is shown in Fig. 1.

In the PENCIL framework, three types of “labels” (yd,
ˆy and ˜y) are involved. Label distribution yd is updated by
back-propagation. In the end, yd will be a good estimate
of the underlying unknown noise-free label (i.e., noise cor-
rected label). ˜y is a variable that assists yd to be normalized
to a probability distribution, by

yd = softmax(˜y) .

(7)

Hence, ˜y is not constrained and can be updated freely using
back-propagation, but yd is always a valid distribution.

The original noisy label ˆy does not directly impact the
parameter (θ) learning. However, it is useful because we
use it to indirectly initialize our label distribution yd. At the
start of PENCIL, ˜y is initialized by ˆy as follows:

We will soon show that Eq. 4 is more suitable for noise
handling. In fact, Eq. 2 led to very poor results in our ex-
periments and we propose to use Eq. 4 as one of the loss
functions in PENCIL. More details will be discussed in Sec-
tion 3.4.

˜y = K ˆy ,

(8)

where K is a large constant (K = 10 in our experiments),
and hence from Eq. 7 we have yd ≈ ˆy after this initializa-
tion.

7019

3.3. Compatibility loss

The noisy label ˆy is also useful in PENCIL’s loss compu-
tation. In fact, there are lots of (e.g., 80% of) correct labels
even in datasets with noisy labels. Therefore, we should not
let the estimated label distribution yd be completely different
from those noisy labels ˆy.

We deﬁne a compatibility loss Lo( ˆY , Y d) to enforce this

requirement, as

Lo( ˆY , Y d) = −

1
n

n

c

Xi=1

Xj=1

ˆyij log yd

ij ,

(9)

which is a classic cross entropy loss between label distribu-
tion and noisy label.

3.4. Classiﬁcation loss

The deviation between our label distribution yd and the
network prediction f (x; θ) guides how the network param-
eters θ should be updated.
In DLDL [6] and a similar
work [23], the classic KL-loss (Eq. 2) is used to calculate
the distance between these two distributions. However, we
ﬁnd that Eq. 2 works poorly in PENCIL and propose to use
Eq. 4 instead, as a new classiﬁcation loss (which we denote
as Lc).

Because we need to update the label distribution, we need
∂yd . If Eq. 2 is used as the classiﬁcation loss

to calculate ∂Lc
Lc, then

∂Lc
∂yd
ij

= 1 +

c

Xj=1

log

yd
ij

fj(xi; θ)

.

(10)

And, if we use Eq. 4 as Lc, we have

(Eq. 10) will not (Case 1). Now consider the updating of yd
Eq. 2 (Eq. 10) will only decrease yd
and Eq. 4 (Eq. 11) will keep yd

i,3.
i,3 by a medium amount,

i,3 almost intact (Case 2).

Combining these observations altogether, we believe that
although the classic KL-loss (Eq. 2) is a good ﬁt for other
applications, our proposed Eq. 4 is more suitable for correct-
ing the noise in labels. Hence, we use the variant of KL-loss
in Eq. 4 as our classiﬁcation loss Lc.

3.5. Entropy loss

Obviously, when the prediction f (x; θ) is the same as
the label distribution yd, the network will stop updating.
However, f (x; θ) tend to approach yd fairly quickly, be-
cause label distributions are used as the supervision signal
for learning network parameters θ. Following [23], we add
an additional loss (regularization) term to avoid this problem.
The entropy loss can force the network to peak at only one
category rather than being ﬂat because the one-hot distribu-
tion has the smallest possible entropy value. This property is
advantageous for classiﬁcation problems. The entropy loss
is deﬁned as

Le(f (x; θ)) = −

1
n

n

c

Xi=1

Xj=1

fj(x; θ) log fj(x; θ) .

(12)

At the same time, it also helps avoid the training from being
stalled in our PENCIL framework, because the label distribu-
tion is not going to be a one-hot distribution and then f (x; θ)
will be different from yd.

3.6. The overall PENCIL framework

With all components ready, the PENCIL loss function is

∂Lc
∂yd
ij

= −

c

Xj=1

fj(xi; θ)

yd
ij

.

(11)

L =

1
c

Lc(f (x; θ), Y d) + αLo( ˆY , Y d) +

β
c

Le(f (x; θ)) ,

Then, we have the following observations for a ﬁxed

training example i and any class index j.

Case 1 If the prediction fj(xi; θ) is much larger than label
distribution yd
ij , Eq. 10 leads to a medium negative
gradient (because of the log), but Eq. 11 leads to a large
negative gradient for updating yd
ij .

Case 2 If fj(xi; θ) is much smaller than yd

j , Eq. 10 leads to
a medium positive gradient while Eq. 11 leads to a
gradient which is almost zero.

Suppose for xi the noisy label ˆyi is peaked at j = 3
(i.e., ˆyi,3 = 1) but the true label is 7. Thus, initially yd
i,3
will be the peak in our label distribution yd
i . The internal
smoothness inside the network may make the prediction
f (xi; θ) to (correctly) peak at j = 7. Hence, we have
f7(xi; θ) ≫ ˆyi,7 and f3(xi; θ) ≪ ˆyi,3. Eq. 4 (Eq. 11) will
then (correctly) increase yd
i,7 by a large amount, while Eq. 2

in which α and β are two hyperparameters. Using this loss
function and the PENCIL framework’s architecture in Fig. 1,
we can use any deep neural network as the backbone net-
work in Fig. 1, and then equip it with the PENCIL network to
handle learning problems with noisy labels. The relationship
between variables and loss functions are clearly visualized in
Fig. 1 as arrows. Forward computations are visualized by red
solid arrows, while back-propagation computations are visu-
alized as blue dashed arrows. The algorithmic description of
the PENCIL framework is shown in Algorithm 1.

We want to add two notes about PENCIL. First, the error
back-propagation process in PENCIL is pretty straightfor-
ward. For example, it can be done automatically in deep
learning packages that support automatic gradient compu-
tation. Second, after the network has been fully trained (cf.
Section 4), those PENCIL-related components in Fig. 1 are
not needed at all—the backbone network alone can perform
prediction for future test examples.

7020

Algorithm 1 The proposed PENCIL framework
Input: the noisy training set {xi, ˆyi} (1 ≤ i ≤ n), and the
number of training epochs T

1: initialize ˜yi (1 ≤ i ≤ n) by Eq. 8
2: t ← 1
3: while t ≤ T do
4:

update θ and yd

i (1 ≤ i ≤ n) by forward computa-
tion and backward propagation in the mini-batch fashion
using all n training examples (i.e., to ﬁnish one epoch)

5:

t ← t + 1

Output: the trained network model θ, and the noise cor-
rected labels yd

i (1 ≤ i ≤ n).

Similar to [23], we implement our PENCIL training

through 3 steps.

Backbone learning: We ﬁrstly train the backbone net-
work with a large ﬁxed learning rate from scratch without
noise handling. As aforementioned, it is observed that when
the learning rate is high, a DNN often does not overﬁt the
label noise. Therefore, in this step, we use a ﬁxed high learn-
ing rate with only the cross-entropy loss function in Eq. 1.
The resulted DNN is the backbone network in Fig. 1.

PENCIL learning: Then, we use the PENCIL frame-
work to update both network parameters and label distribu-
tions. The learning rate is still a ﬁxed high value. Therefore,
the network will not overﬁt label noise and the label distri-
butions will correct noise in the original labels. At the end
of this step, we obtain a label distribution vector for every
image. Algorithmic details are shown in Algorithm 1. Note
that in practice we ﬁnd that updating ˜y requires a learning
rate that is much larger than that used for updating other
parameters. Because the overall learning rate is ﬁxed in this
step, we simply use one single hyperparameters λ to update
˜y (i.e., do not use PENCIL’s overall learning rate), as

˜y ← ˜y − λ

∂L
∂ ˜y

.

(13)

Final ﬁne-tuning: Lastly, we use the learned label distri-
butions to ﬁne-tune the network using only the classiﬁcation
loss Lc (i.e., α = β = 0). In this step, the label distributions
will not be updated and the learning rate will be gradually
reduced as in common neural network training.

4. Experiments

We tested the proposed PENCIL framework on both syn-
thetic and real-world datasets: CIFAR-100 [12], CIFAR-
10 [12], CUB-200 [26] and Clothing1M [28]. All experi-
ments were implemented using the PyTorch framework.

4.1. Datasets

CIFAR-100: Following [31], we retained 10% of the
training data as the validation set, and both train and val-

idation sets were noise contaminated. However, note that
we did not use the validation set in our method, because
PENCIL does not need a validation set.

There are two types of noises: symmetric and asymmetric.
Following [31], in the symmetric noise setup, label noise
is uniformly distributed among all categories, and the label
noise percentage is r ∈ [0, 1]. For every example, if the
correct label is i, then the noise-contaminated label has 1 − r
probability to remain correct, but has r probability to be
drawn uniformly from the c labels. The asymmetric noise
label was generated by ﬂipping each class to the next class
circularly with noise rate r ∈ [0, 1].

CIFAR-10: Following [23], we retained 10% of the
CIFAR-10 training data as the validation set and modify
the original correct labels to obtain different noisy label
datasets. The setting for symmetric noise is the same as
that in CIFAR-100. As for asymmetric noise, following
[16] the noisy labels were generated by mapping truck
→ automobile, bird→ airplane, deer → horse
and cat ↔ dog with probability r. These noise genera-
tion methods are in coincidence with confusions that often
happen in the real world.

Clothing1M: Clothing1M is a large-scale dataset with
noisy labels. It consists of more than one million images
from 14 classes with many wrong labels. Images were ob-
tained from several online shopping websites and labels were
generated by their surrounding texts. The estimated noise
level is roughly 40% [28]. This dataset is seriously imbal-
anced and the label mistakes mostly happen between similar
classes (i.e., asymmetric). There exist additional training,
validation and test sets with 50k, 14k and 10k examples
whose labels are believed to be clean, respectively.

CUB-200: We tested the robustness of our framework
in a ﬁne-grained classiﬁcation dataset CUB-200. CUB-200
contains 11788 images of 200 species of birds, which is not
considered to have the noisy label difﬁculty. Therefore, we
tested our framework on this dataset to show that PENCIL
is robust. In addition, there is probably a small percentage
of noisy labels in CUB-200 [27]. It is interesting to observe
whether PENCIL is robust and effective in such a dataset.

4.2. Implementation details

Next, we describe more implementation details for each

dataset.

CIFAR-100: We used ResNet-34 [9] as the backbone
network for fair comparison with existing methods. The
learning rate was 0.35, α = 0.1, β = 0.4, and λ = 10000.
Mean subtraction, horizontal random ﬂip and 32×32 random
crops after padding 4 pixels on each side were performed as
data preprocessing and augmentation. We used SGD with
0.9 momentum, a weight decay of 10−4, and batch size of
128. Following [23], the epoch numbers for three steps were
70, 130 and 120, respectively. In the last step, we used the

7021

Table 1. Hyperparameters for CIFAR-10 experiments. 3000 → 0
means that λ decreases from 3000 to 0 linearly.

Symmetric Noise

noise rate (%)

learning rate

10
30
50
70
90

0.02
0.03
0.04
0.08
0.12

α
0.1
0.1
0.1
0.1
0.1

Asymmetric Noise

noise rate (%)

learning rate

10
20
30
40
50

0.06
0.06
0.06
0.03
0.03

α
0.1
0.1
0.1
0
0

β
0.8
0.8
0.8
0.8
0.4

β
0.4
0.4
0.4
0.4
0.4

λ
200
300
400
800
1200

λ
600
600
600

3000 → 0
4000 → 0

learning rate of 0.2 and divided it by 10 after 40 and 80
epochs [23]. All experiments on CIFAR-100 used the same
settings as described above. In fact, we can obtain better
results by further tuning the hyperparameters (e.g., as what
we will soon introduce for CIFAR-10). However, we choose
to use the same set of hyperparameters to demonstrate the
robustness of our framework.

CIFAR-10: We used PreAct ResNet-32 [10] as the back-
bone network for fair comparison with existing methods.
We used the same settings as those for CIFAR-100, except
the overall learning rate, α, β and λ hyperparameters. On
CIFAR-10, these hyperparameters are shown in Table 1.

As shown in Table 1, the learning rate increases as the
noise rate increases for symmetric noise. This is reasonable,
because when noise rate gets higher, we need stronger ro-
bustness and we can increase the learning rate to prevent our
network from overﬁtting. And, when the noise rate is very
high (e.g., 50% asymmetric), there are too many noisy labels.
Hence, we can remove the effect of noisy labels by removing
Lo (i.e., set α to 0). At the same time, we require a large λ
to correct these noisy labels quickly. However, after a few
epochs, the noisy labels were quickly corrected to a stable
state (cf. Fig. 2 and Fig. 3). Hence, we need to decrease λ
linearly to prevent wrong updates in later epochs.

CUB-200: On this dataset, we used ResNet-50 [9] pre-
trained on ImageNet. Data preprocessing and augmentation
is also applied, including performing mean subtraction, hor-
izontal random ﬂip, resizing the image to 256 × 256 and
224 × 224 random crops. We used SGD with 0.9 momen-
tum, a weight decay of 10−4, and batch size of 16. The
number of epochs for the three steps are 35, 65 and 60, re-
spectively. The learning rate of the ﬁrst and second step
is 2 × 10−3. In the last step, the learning rate is 10−3 and
divided by 10 after 20 epochs and 40 epochs. β is 0.8 and we
reported results for different values of α and λ as ablation
studies.

Clothing1M: We used ResNet-50 pre-trained on Ima-
geNet as the backbone network for fair comparison with

existing methods. Data preprocessing and augmentation are
the same as those in CUB-200. We used SGD with 0.9 mo-
mentum, a weight decay of 10−3, and batch size of 32. The
epoch numbers of three steps are 5, 10 and 10, respectively.
The ﬁrst step learning rate is 1.6 × 10−3 and the second
step learning rate is 8 × 10−4. The last step learning rate
is 5 × 10−4 and divided by 10 after 5 epochs. α = 0.08,
β = 0.8. In ﬁrst 5 epochs of second step λ = 3000, and in
last 5 epochs of second step λ = 500.

This dataset exists serious data imbalance. Therefore, we
randomly selected a small balanced subset (using the noisy
labels) to relieve the difﬁculty caused by imbalance. The
small subset includes about 260k images and all classes have
the same number of images. All our experiments on Cloth-
ing1M were done with this subset in this study. However,
note that this subset is not truly balanced, because the labels
are noisy.

4.3. Results on CIFAR 100

Firstly we tested PENCIL on CIFAR-100. The results are
shown in Table 2. All dataset settings followed [31]. The
method “Forward T [16]” used the groundtruth noise tran-
sition matrix (which is not available in real-world datasets),
hence its numbers were not compared with other methods.
Except for the 80% symmetric noise case, PENCIL signiﬁ-
cantly outperformed previous methods in all symmetric and
asymmetric noise cases. Even if “Forward T ” used strong
prior information which should not have been used, our
PENCIL method still outperformed it in most cases.

As for the 80% symmetric noise case, it revealed a failure
mode of the proposed PENCIL method. When the noise
rate is too high (e.g., 80%), the correct labels only form a
minority group and they are too weak to bootstrap the noise
correction process. Hence, PENCIL tends to fail in such
high noise rate problems. Fortunately, we hardly deal with
such high noise rate in real-world applications. For example,
the large scale real-world image dataset JFT300M [22] only
includes about 20% noisy labels.

We have intentionally chosen the same set of hyperpa-
rameters in all experiments on this dataset, and the results
demonstrate the robustness of our PENCIL framework to
these hyperparameters. We can obtain better accuracy by
using different hyperparameters for different noise rate and
noise type, as shown in Table 1 on the CIFAR-10 dataset.

4.4. Experiments on CIFAR 10

Next, we evaluated the performance of our PENCIL
framework on CIFAR-10. All the settings have been de-
scribed in Section 4.2. On the original noise-free CIFAR-10
dataset, the result of our backbone network (PreAct ResNet-
32) is 94.05%. Our setup followed that in [23]. However, re-
sults in [23] used a prior knowledge (i.e., all categories have
the same number of noise-free training examples), which

7022

Table 2. Results on CIFAR-100. We report the average accuracy and standard deviation of 5 trials. #1 to #5 are quoted from [31]. PENCIL
(#6) is the result of last epoch (without using the validation set). The row with a star * (#2) did not participate in comparison for fairness.
#

method

noise rate (%)

20

80

10

Symmetric Noise
40
60

Asymmetric Noise
20
30

1 Cross Entropy Loss
Forward T * [16]
2
Forward ˆT [16]
3
4
5
6

Trunc Lq [31]
PENCIL (last)

Lq [31]

58.72±0.26
63.16±0.37
39.19±2.61
66.81±0.42
67.61±0.18
73.86±0.34

48.20±0.65
54.65±0.88
31.05±1.44
61.77±0.24
62.64±0.33
69.12±0.62

37.41±0.94
44.62±0.82
19.12±1.95
53.16±0.78
54.04±0.56
57.79±3.86

18.10±0.82
24.83±0.71
8.99±0.58
29.16±0.74
29.60±0.51

fail

66.54±0.42
71.05±0.30
45.96±1.21
68.36±0.42
68.86±0.14
75.93±0.20

59.20±0.18
71.08±0.22
42.46±2.16
66.59±0.22
66.59±0.23
74.70±0.56

51.40±0.16
70.76±0.26
38.13±2.97
61.45±0.26
61.87±0.39
72.52±0.38

40

42.74±0.61
70.82±0.45
34.44±1.93
47.22±1.15
47.66±0.69
63.61±0.23

Table 3. Test accuracy on CIFAR-10 with symmetric noise. We
reported the average result of 5 trials. All results in this table were
based on our own implementation.

#

method

Symmetric Noise

noise rate (%)

10

30

50

70

90

1 Cross Entropy Loss

2

3

Tanaka et al. [23]

PENCIL

best 91.66 89.00 85.15 78.09 50.74
last 88.43 72.78 53.11 33.32 16.30
best 93.23 91.23 88.50 84.51 54.36
last 93.23 91.22 88.51 84.59 53.49
best 93.26 92.09 90.29 87.10 61.21
last 93.28 92.24 90.36 87.18 60.80

should not be used. For fair comparison, we implemented
the “Tanaka et al. [23]” method and in our implementation
we did not use this prior knowledge.

Table 3 lists results of symmetric noise for CIFAR-10. In
Table 3, “best” denotes the test accuracy of the epoch where
the validation accuracy was optimal and “last” denotes the
test accuracy of the last epoch. As aforementioned, when the
learning rate is small, the deep neural network’s accuracy
will decline because the network memorizes all the (noisy)
labels, i.e., the network is overﬁtting. As shown in row
#1, the traditional neural network using the classic cross
entropy loss is heavily affected by this difﬁculty. Its best-
epoch test accuracy was signiﬁcantly better than that of the
last-epoch one. And, as the noise rate increased, the gap
was even larger because the overﬁtting to noise became
more serious as expected. On the contrary, our method and
the Tanaka et al. [23] did not have obvious accuracy drop
between best- and last-epochs. Therefore, the proposed
PENCIL method has strong robustness. As for the test set
accuracy, PENCIL had a clear advantage than competing
methods in Table 3. The winning gap became especially
apparent when the noise rate increased to larger values. For
example, when the noise rate was 90%, PENCIL obtained
roughly 7% higher accuracy than that of Tanaka et al. and
10% higher than that of cross entropy.

Table 4 lists results of asymmetric noise for CIFAR-10.
In terms of robustness, methods shown in row #1, #2 and
#3 had the overﬁtting problem and their test accuracies
had large gaps between the best- and last-epochs. The
Tanaka et al. method experienced the same issue when the
noise rate was high (50%), but was robust in other cases.
Our PENCIL method, however, remained robust throughout

Table 4. Test accuracy on CIFAR-10 with asymmetric noise. We
reported the average result of 5 trials. Rows #1, #4 and #5
were based on our own implementation. Rows #2 and #3 were
quoted from [23]. The methods marked with a “*” used additional
information that should not be used, and need to be excluded in a
fair comparison.

#

method

Asymmetric Noise

noise rate (%)

10

20

30

40

50

1 Cross Entropy Loss

2

3

4

5

Forward T * [16]

CNN-CRF * [24]

Tanaka et al. [23]

PENCIL

91.4
89.7
91.5
86.6

best 91.09 89.94 88.78 87.78 77.79
last 85.24 80.74 76.09 76.12 71.05
best 92.4
last 91.7
best 92.0
last 90.3
best 92.53 91.89 91.10 91.48 75.81
last 92.64 91.92 91.18 91.55 68.35
best 93.00 92.43 91.84 91.01 80.51
last 93.04 92.43 91.80 91.16 80.06

91.0
88.0
90.7
83.6

90.3
86.4
89.5
79.7

83.8
80.9
84.0
76.4

all the experiments.

The Forward [16] and CNN-CRF [24] methods both
require the ground-truth noise transition matrix, which is
hardly available in applications. Our method does not re-
quire any prior information about noise labels. Table 4 shows
that PENCIL has been robust and is the overall accuracy win-
ner on CIFAR-10.

We recorded the number of correct labels in PENCIL’s
second step.
In a label distribution vector, the category
corresponding to the maximum value in the probability dis-
tribution was identiﬁed as the label estimated by PENCIL. If
this label was the same as the noise-free groundtruth label,
we say it was correct. The results for 70% symmetric and
30% asymmetric noise on CIFAR-10 are shown in Fig. 2 and
Fig. 3, respectively. We can observe that PENCIL effectively
and stably estimated correct labels for most examples even
with high noise rates. For example, with 70% symmetric
noise rate, originally only about 16000 labels were correct,
but after PENCIL’s learning process there are about 39000
correct labels.

4.5. Experiments on CUB 200

We performed additional experiments on CUB-200 with
different hyperparameters α and λ. This dataset is generally
considered to contain no or only few noisy labels. There-
fore, we use it to further test the robustness of PENCIL on

7023

CIFAR10 with 70% symmetric noise

Table 5. Test accuracy on CUB-200 with different hyperparameters.
The accuracy of PENCIL does not decline in standard datasets with
clean labels.

90

80

70

60

50

40

)

%

(
 
l
e
b
a
l
 
t
c
e
r
r
o
c

0

20

40

60
epoch

80

100

120

Figure 2. Correct labels on CIFAR-10 with 70% symmetric noise.

CIFAR10 with 30% asymmetric noise

)

%

(
 
l
e
b
a
l
 
t
c
e
r
r
o
c

95.0

92.5

90.0

87.5

85.0

82.5

80.0

77.5

0

20

40

60
epoch

80

100

120

Figure 3. Correct labels on CIFAR-10 with 30% asymmetric noise.

problems not affected by noisy labels.

The results are listed in Table 5. Row #1 is the baseline
(classic method) and rows #2 to #7 are PENCIL results.
For a wide range of α and λ values, PENCIL consistently
exhibited competitive results (i.e., without obvious degrada-
tion). Furthermore, we observed the ﬁnal label distributions,
and the maximum values of all label distributions are correct
(i.e., same as the correct labels). This observation shows that
PENCIL works robustly in clean datasets, too.

In the settings of rows #4 to #7, PENCIL achieved
higher accuracy than the baseline. In particular, row #4
is 0.71% higher. A small percentage of label noise may
exist in this dataset [27]. Our hypothesis is that by replacing
the original one-hot label with probabilistic modeling in
PENCIL, we obtained better robustness and consequently a
small edge in accuracy.

4.6. Experiments on Clothing1M

Finally, we tested PENCIL on Clothing1M, which is a
real-world noisy label dataset. It includes a lot of unknown
structure (asymmetric) noise.

The results are shown in Table 6. All results are best
test accuracy. Rows #1 and #2 were quoted from [16],
and row #3 was reported in [23]. Although these baseline
models were trained on the whole Clothing1M training set,
our PENCIL used a randomly sampled pseudo-balanced
subset, including about 260k images. The backbone network
was ResNet-50 for all methods.

#
1

2
3
4
5
6
7

method

Test Accuracy (%)

Cross Entropy Loss

81.93

PENCIL

λ

1000
2000
3000
1000
2000
3000

α
0
0
0
0.1
0.1
0.1

81.91
81.84
82.64
82.09
82.21
82.22

Table 6. Test accuracy on the Clothing1M dataset. Rows #1 and
#2 were quoted from [16] and #3 was quoted from [23]. These
baseline methods used the complete Clothing1M training data,
but our method only used a small pseudo-balanced subset (i.e.,
balanced in terms of noisy labels). Our method achieved state-of-
the-art result in this real-world dataset.

#
1
2
3
4

method

Test Accuracy (%)

Cross Entropy Loss

Forward [16]

Tanaka et al. [23]

PENCIL

68.94
69.84
72.16
73.49

In Table 6, only noisy labeled examples were used (i.e.,
without using the clean training subset). The Forward [16]
method required the ground-truth noise transition matrix,
which is not available. Hence, it used an estimated matrix
instead. The Tanaka et al. [23] method used the distribution
of noisy labels to relieve the imbalanced problem. In our
PENCIL method, we did not use any extra prior informa-
tion. PENCIL achieved 1.33% higher accuracy than that
of Tanaka et al. [23], 3.65% higher than Forward [16] and
4.55% than cross entropy.

5. Conclusion

We proposed a framework named PENCIL to solve the
noisy label problem. PENCIL adopted label probability
distributions to supervise network learning and to update
these distributions through back-propagation end-to-end in
every epoch. We proposed a KL-loss, which is different from
previous methods but is robust for noisy label handling. The
proposed PENCIL framework is end-to-end and independent
of the backbone network structure, thus it is easy to deploy.
We tested PENCIL with synthetic label noise on CIFAR-
100 and CIFAR-10 with different noise types and noise rates,
and outperformed current state-of-the-art methods by large
margins. We also experimented on CUB-200, which is con-
sidered to be noise free. The results show that PENCIL is
robust for different datasets and hyperparameters. Lastly,
we tested PENCIL on the real-world large scale label noise
dataset Clothing1M. On this dataset, we achieved 1.33%
higher accuracy than previous state-of-the-art.

7024

[19] David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit.
Deep learning is robust to massive label noise. arXiv preprint
arXiv:1705.10694, 2017.

[20] Florian Schroff, Antonio Criminisi, and Andrew Zisserman.
Harvesting image databases from the web. IEEE Trans. Pat-
tern Anal. Mach. Intell., 33(4):754–766, 2011.

[21] Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir
Bourdev, and Rob Fergus. Training convolutional networks
with noisy labels. arXiv preprint arXiv:1406.2080, 2014.

[22] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav
Gupta. Revisiting unreasonable effectiveness of data in deep
learning era. In ICCV, pages 843–852, 2017.

[23] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiy-
oharu Aizawa. Joint optimization framework for learning
with noisy labels. In CVPR, pages 5552–5560, 2018.

[24] Arash Vahdat. Toward robustness against label noise in train-
In NIPS, pages

ing deep discriminative neural networks.
5601–5610, 2017.

[25] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey,
Hongyuan Zha, Le Song, and Shu-Tao Xia. Iterative learning
with open-set noisy labels. In CVPR, pages 8688–8696, 2018.
[26] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-
longie, and P. Perona. Caltech-UCSD Birds 200. Technical
Report CNS-TR-2010-001, California Institute of Technol-
ogy, 2010.

[27] Michael J. Wilber, Iljung S. Kwak, David J. Kriegman, and
Serge J. Belongie. Learning concept embeddings with com-
bined human-machine expertise. In ICCV, pages 981–989,
2015.

[28] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang
Wang. Learning from massive noisy labeled data for image
classiﬁcation. In CVPR, pages 2691–2699, 2015.

[29] Jiangchao Yao, Jiajie Wang, Ivor W Tsang, Ya Zhang, Jun
Sun, Chengqi Zhang, and Rui Zhang. Deep learning from
noisy image labels with quality embedding. IEEE Transac-
tions on Image Processing, 2018, accepted.

[30] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,
and Oriol Vinyals. Understanding deep learning requires
rethinking generalization. In ICLR, 2017.

[31] Zhilu Zhang and Mert R. Sabuncu. Generalized cross entropy
loss for training deep neural networks with noisy labels. In
NIPS, 2018.

References

[1] Dana Angluin and Philip D. Laird. Learning from noisy

examples. Machine Learning, 2(4):343–370, 1988.

[2] Carla E. Brodley and Mark A. Friedl. Identifying mislabeled

training data. J. Artif. Intell. Res., 11:131–167, 1999.

[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
ImageNet: A large-scale hierarchical image

Fei-Fei Li.
database. In CVPR, pages 248–255, 2009.

[4] Robert Fergus, Fei-Fei Li, Pietro Perona, and Andrew Zisser-
man. Learning object categories from Internet image searches.
Proceedings of the IEEE, 98(8):1453–1466, 2010.

[5] Benoˆıt Fr´enay and Michel Verleysen. Classiﬁcation in the
presence of label noise: A survey. IEEE Trans. Neural Netw.
Learning Syst., 25(5):845–869, 2014.

[6] Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, and Xin
Geng. Deep label distribution learning with label ambiguity.
IEEE Trans. Image Processing, 26(6):2825–2838, 2017.

[7] Aritra Ghosh, Himanshu Kumar, and P. S. Sastry. Robust
loss functions under label noise for deep neural networks. In
AAAI, pages 1919–1925, 2017.

[8] Isabelle Guyon, Nada Matic, and Vladimir Vapnik. Discov-
ering informative patterns and data cleaning. In KDD, pages
181–203, 1996.

[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR, pages
770–778, 2016.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In ECCV, vol-
ume 9908 of LNCS, pages 630–645. Springer, 2016.

[11] Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard
Zhou, Alexander Toshev, Tom Duerig, James Philbin, and
Li Fei-Fei. The unreasonable effectiveness of noisy data for
ﬁne-grained recognition. In ECCV, volume 9907 of LNCS,
pages 301–320. Springer, 2016.

[12] Alex Krizhevsky. Learning multiple layers of features from

tiny images. Master’s thesis, University of Toronto, 2009.

[13] Jan Larsen, Lars Nonboe Andersen, Mads Hintz-Madsen, and
Lars Kai Hansen. Design of robust neural network classiﬁers.
In ICASSP, pages 1205–1208, 1998.

[14] Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang.
CleanNet: Transfer learning for scalable image classiﬁer train-
ing with label noise. In CVPR, pages 5447–5456, 2018.

[15] Xingjun Ma, Yisen Wang, Michael E. Houle, Shuo Zhou,
Sarah M. Erfani, Shu-Tao Xia, Sudanthi N. R. Wijewickrema,
and James Bailey. Dimensionality-driven learning with noisy
labels. In ICML, pages 3355–3364, 2018.

[16] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon,
Richard Nock, and Lizhen Qu. Making deep neural networks
robust to label noise: A loss correction approach. In CVPR,
pages 1944–1952, 2017.

[17] J. Ross Quinlan. Induction of decision trees. Machine Learn-

ing, 1(1):81–106, 1986.

[18] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian
Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training
deep neural networks on noisy labels with bootstrapping. In
ICLR, 2015.

7025

