Deep Multimodal Clustering for Unsupervised Audiovisual Learning

Di Hu, Feiping Nie, Xuelong Li∗

School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL),

Northwestern Polytechnical University, Xi’an 710072, P. R. China

hdui831@mail.nwpu.edu.cn, feipingnie@gmail.com, xuelong li@ieee.org

Abstract

The seen birds twitter, the running cars accompany with
noise, etc. These naturally audiovisual correspondences
provide the possibilities to explore and understand the out-
side world. However, the mixed multiple objects and sounds
make it intractable to perform efﬁcient matching in the un-
constrained environment. To settle this problem, we pro-
pose to adequately excavate audio and visual components
and perform elaborate correspondence learning among
them. Concretely, a novel unsupervised audiovisual learn-
ing model is proposed, named as Deep Multimodal Clus-
tering (DMC), that synchronously performs sets of cluster-
ing with multimodal vectors of convolutional maps in differ-
ent shared spaces for capturing multiple audiovisual cor-
respondences. And such integrated multimodal clustering
network can be effectively trained with max-margin loss in
the end-to-end fashion. Amounts of experiments in feature
evaluation and audiovisual tasks are performed. The re-
sults demonstrate that DMC can learn effective unimodal
representation, with which the classiﬁer can even outper-
form human performance. Further, DMC shows noticeable
performance in sound localization, multisource detection,
and audiovisual understanding.

1. Introduction

When seeing a dog, why the sound emerged in our mind
is mostly barking instead of miaow or others? It seems
easy to answer “we can only catch the barking dog in our
daily life”. As speciﬁc visual appearance and acoustic sig-
nal usually occur together, we can realize that they are
strongly correlated, which accordingly makes us recognize
the sound-maker of visual dog and the distinctive barking
sound. Hence, the concurrent audiovisual message provides
the possibilities to better explore and understand the outside
world [19].

The cognitive community has noticed such phenomenon

∗Corresponding author

in the last century and named it as multisensory process-
ing [19]. They found that some neural cells in superior
temporal sulcus (a brain region in the temporal cortex)
can simultaneously response to visual, auditory, and tac-
tile signal [18]. When the concurrent audiovisual message
is perceived by the brain, such neural cells could provide
corresponding mechanism to correlate these different mes-
sages, which is further reﬂected in various tasks, such as
lip-reading [8] and sensory substitute [30].

In view of the merits of audiovisual learning in human
beings, it is highly expected to make machine possess sim-
ilar ability, i.e., exploring and perceiving the world via the
concurrent audiovisual message. More importantly, in con-
trast to the expensive human-annotation, the audiovisual
correspondence can also provide valuable supervision, and
it is pervasive, reliable, and free [27]. As a result, the
audiovisual correspondence learning has been given more
and more attention recently. In the beginning, the coher-
ent property of audiovisual signal is supposed to provide
cross-modal supervision information, where the knowledge
of one modality is transferred to supervise the other primi-
tive one. However, the learning capacity is obviously lim-
ited by the transferred knowledge and it is difﬁcult to ex-
pand the correspondence into unexplored cases. Instead of
this, a natural question emerges out: can the model learn the
audiovisual perception just by their correspondence with-
out any prior knowledge? The recent works give deﬁni-
tive answers [3, 26]. They propose to train an audiovisual
two-stream network by simply appending a correspondence
judgement on the top layer. In other words, the model learns
to match the sound with the image that contains correct
sound source. Surprisingly, the visual and auditory subnets
have learnt to response to speciﬁc object and sound after
training the model, which can be then applied for unimodal
classiﬁcation, sound localization, etc.

The

correspondence

assumption behind previous
works [27, 3, 6] rely on speciﬁc audiovisual scenario
where the sound-maker should exist
in the captured
visual appearance and single sound source condition is
expected. However, such rigorous scenario is not entirely

9248

suitable for the real-life video. First, the unconstrained
visual scene contains multiple objects which could be
sound-makers or not, and the corresponding soundscape
is a kind of multisource mixture.
Simply performing
the global corresponding veriﬁcation without having an
insight into the complex scene components could result in
the inefﬁcient and inaccurate matching, which therefore
need amounts of audiovisual pairs to achieve acceptable
performance [3] but may still generate semantic irrelevant
matching [33]. Second, the sound-maker does not always
produce distinctive sound, such as honking cars, barking
dogs, so that
the current video clip does not contain
any sound but next one does, which therefore creates
inconsistent conditions for the correspondence assumption.
Moreover, the sound-maker may be even out of the screen
so we cannot see it in the video, e.g., the voiceover of
photographer. The above intricate audiovisual conditions
make it extremely difﬁcult
to analyze and understand
the realistic environment, especially to correctly match
different sound-makers and their produced sounds. So, a
kind of elaborate correspondence learning is expected.

As each modality involves multiple concrete components
in the unconstrained scene, it is difﬁcult to correlate the
real audiovisual pairs. To settle this problem, we propose
to disentangle each modality into a set of distinct compo-
nents instead of the conventional indiscriminate fashion.
Then, we aim to learn the correspondence between these
distributed representations of different modalities. More
speciﬁcally, we argue that the activation vectors across con-
volution maps have distinct responses for different input
components, which just meets the clustering assumption.
Hence, we introduce the Kmeans into the two-stream au-
diovisual network to distinguish concrete objects or sounds
captured by video. To align the sound and its correspond-
ing producer, sets of shared spaces for audiovisual pairs are
effectively learnt by minimizing the associated triplet loss.
As the clustering module is embedded into the multimodal
network, the proposed model is named as Deep Multimodal
Clustering (DMC). Extensive experiments conducted on
wild audiovisual pairs show superiority of our model on
unimodal features generation, image/acoustic classiﬁcation
and some audiovisual tasks, such as single sound localiza-
tion and multisource Sound Event Detection (SED). And
the ultimate audiovisual understanding seems to have pre-
liminary perception ability in real-life scene.

2. Related Works

Audiovisual correspondence is a kind of natural phe-
nomena, which actually comes from the fact that “Sound
is produced by the oscillation of object”. The simple phe-
nomena provides the possibilities to discover audiovisual
appearances and build their complex correlations. That’s
why we can match the barking sound to the dog appear-

Source Supervis.

Task

Reference

Sound Vision
Vision
Sound
Sound

&

Match

Vision

Acoustic Classif.

[5, 15, 14, 6]

Image Classif.
Classiﬁcation

[27, 6]

[3]

Sound Localization
Source Separation

[4, 33, 26, 38]
[9, 26, 13, 38]

Table 1. Audiovisual learning settings and relevant tasks.

ance from numerous audio candidates (sound separation)
and ﬁnd the dog appearance according to the barking sound
from the complex visual scene (sound source localization).
As usually, the machine model is also expected to possess
similarly ability as human.

In the past few years, there have been several works that
focus on audiovisual machine learning. The learning set-
tings and relevant tasks can be categorized into three phases
according to source and supervision modality, as shown in
Table 1. The early works consider that the audio and visual
messages of the same entity should have similar class in-
formation. Hence, it is expected to utilize the well-trained
model of one modality to supervise the other one with-
out additional annotation. Such “teacher-student” learning
fashion has been successfully employed for image classiﬁ-
cation by sound [27] and acoustic recognition by vision [5].
Although the above models have shown promised cross-
modal learning capacity, they actually rely on stronger su-
pervision signal than human. That is, we are not born with
a well-trained brain that have recognized kinds of objects
or sounds. Hence, recent (almost concurrent) works pro-
pose to train a two-stream network just by given the audio-
visual correspondence, as shown in Table 1. Arandjelovi´c
and Zisserman [3] train their audiovisual model to judge
whether the image and audio clip are corresponding. Al-
though such model is trained without the supervision of any
teacher-model, it has learnt highly effective unimodal rep-
resentation and cross-modal correlation [3]. Hence, it be-
comes feasible to execute relevant audiovisual tasks, such
as sound localization and source separation. For the ﬁrst
task, Arandjelovi´c and Zisserman [4] revise their previous
model [3] to ﬁnd the visual area with the maximum simi-
larity for the current audio clip. Owens et al. [26] propose
to adopt the similar model as [3] but use 3D convolution
network for the visual pathway instead, which can capture
the motion information for sound localization. However,
these works rely on simple global correspondence. When
there exist multiple sound-producers in the shown visual
modality, it becomes difﬁcult to exactly locate the correct
producer. Recently, Senocak et al. [33] introduce the at-
tention mechanism into the audiovisual model, where the
relevant area of visual feature maps learn to attend speciﬁc
input sound. However, there still exists another problem

9249

Figure 1. An illustration of activation distributions. It is obvious
that different visual components have distinct activation vectors
across the feature maps. Such property helps to distinguish differ-
ent visual components. Best viewed in color.

that the real-life acoustic environment is usually a mixture
of multiple sounds. To localize the source of speciﬁc sound,
efﬁcient sound separation is also required.

In the sound separation task, most works propose to
reconstruct speciﬁc audio streams from manually mixed
tracks with the help of visual embedding. For example,
Zhao et al.[38] focus on the musical sound separation,
while Casanovas et al. [9], Owens et al. [26], and Ariel et
al. [11] perform the separation for mixed speech messages.
However, the real-life sound is more complex and general
than the speciﬁc imitated examples, which even lacks the
groundtruth for the separated sound sources. Hence, our
proposed method jointly disentangles the audio and visual
components, and establishes elaborate correspondence be-
tween them, which naturally covers both of the sound sepa-
ration and localization task.

3. The Proposed Model

3.1. Visual and Audio subnet

Visual subnet. The visual pathway directly adopts the off-
the-shelf VGG16 architecture but without the fully con-
nected and softmax layers [34]. As the input to the net-
work is resized into 256 × 256 image, the generated 512
feature maps fall into the size of 8 × 8. To enable the
efﬁcient alignment across modalities, the pixel values are
scaled into the range of [−1, 1] that have comparable scale
to the log-mel spectrogram of audio signal. As the asso-
ciated visual components for the audio signal have been
encoded into the feature maps, the corresponding entries
across all the maps can be viewed as their feature repre-
sentations, as shown in Fig. 1. In other words, the original
feature maps of 8 × 8 × 512 is reshaped into 64 × 512,
where each row means the representations for speciﬁc vi-
sual area. Hence, the ﬁnal visual representations become

p|uv

2, ..., uv

(cid:8)uv
1, uv

i ∈ Rn(cid:9), where p = 64 and n = 512.

Audio subnet. The audio pathway employs the VGGish
model to extract the representations from the input log-mel
spectrogram of mono sound [17]. In practice, different from
the default conﬁgurations in [17], the input audio clip is

extended to 496 frames of 10ms each but other parame-
ters about short-time Fourier transform and mel-mapping
are kept. Hence, the input to the network becomes 496 × 64
log-mel spectrogram, and the corresponding output feature
maps become 31 × 4 × 512. To prepare the audio repre-
sentation for the second-stage clustering, we also perform
the same operation as the visual ones. That is, the audio

feature maps are reshaped into (cid:8)ua

where q = 124 and n = 512.

1, ua

2, ..., ua

q |ua

i ∈ Rn(cid:9),

3.2. Multimodal clustering module

As the convolutional network shows strong ability in
describing the high-level semantics for different modali-
ties [34, 17, 37], we argue that the elements in the feature
maps have similar activation probabilities for the same uni-
modal component, as shown in Fig. 1.
It becomes pos-
sible to excavate the audiovisual entities by aggregating
their similar feature vectors. Hence, we propose to clus-
ter the unimodal feature vectors into object-level represen-
tations, and align them in the coordinated audiovisual en-
vironment, as shown in Fig. 2. For simplicity, we take
{u1, u2, ..., up|ui ∈ Rn} for feature representation without
regard to the type of modality.

To cluster the unimodal features into k clusters, we
propose to perform Kmeans to obtain the centers C =
{c1, c2, ..., ck|cj ∈ Rm}, where m is the center dimension-
ality. Kmeans aims to minimize the within-cluster distance
and assign the feature points into k-clusters [20], hence, the
objective function can be formulated as,

F(C) =

p

Xi=1

k

min
j=1

d (ui, cj),

(1)

where

k

min
j=1

d (ui, cj) means the distance between current

point and its closest center. However, simply introducing
Eq.1 into the deep networks will make it difﬁcult to opti-
mize by gradient descent, as the minimization function in
Eq.1 is a hard assignment of data points for clusters and
not differentiable. To solve this intractable problem, one
way is to make a soft assignment for each point. Particu-
larly, Expectation-Maximization (EM) algorithm for Gaus-
sian Mixture Models (GMMs) makes a soft assignment
based on the posterior probabilities and converges to a local
optimum [25].

In this paper, we propose another perspective to trans-
form the hard assignment in Eq.(1) to a soft assignment
problem and be a differentiable one. The minimization op-
eration in Eq.(1) is approximated via utilizing the following
equation,

max {di1, di2, ..., dik} ≈

1
z

log


k

Xj=1

edij z
 ,

(2)

9250

Figure 2. The diagram of the proposed deep multimodal clustering model. The two modality-speciﬁc ConvNets ﬁrst process the pairwise
visual image and audio spectrogram into respective feature maps, then these maps are co-clustered into corresponding components that
indicate concrete audiovisual contents, such as baby and its voice, drumming and its sound. Finally, the model takes the similarity across
modalities as the supervision for training.

where z is a parameter about magnitude and dij = d (ui, cj)
for simplicity. Eq.2 shows that the maximum value of a
given sequence can be approximated by the log-summation
of corresponding exponential functions. Intuitively, the dif-
ferences in the original sequence are ampliﬁed sharply with
the exponential projection, which tends to ignore the tiny
ones and remain the largest one. Then, the reversed log-
arithm projection gives the approximated maximum value.
The rigorous proof for Eq.2 can be found in the materials.

As we aim to ﬁnd the minimum value of the distance

sequence, Eq. 2 is modiﬁed into

min {di1, di2, ..., dik} ≈ −

1
z

k

log
Xj=1


e−dij z
 .

(3)

Then, the objective function of clustering becomes

F (C) = −

1
z

p

Xi=1

log


k

Xj=1

e−dij z
.

As Eq. 4 is differentiable everywhere, we can directly com-
pute the derivative w.r.t. each cluster center. Concretely, for
the center cj , the derivative is written as

∂F
∂cj

=

p

Xi=1

e−dij z
k

e−dilz

Pl=1

∂dij
∂cj

=

p

Xi=1

sij

∂dij
∂cj

,

(5)

Pk

e−dij z
where sij =
l=1 e−dil z = sof tmax (−dijz). The soft-
max coefﬁcient performs like the soft-segmentation over the
whole visual area or audio spectrogram for different centers,
and we will give more explanation about it in the following
sections.

In practice, the distance dij between each pair of fea-
ture point ui and center cj can be achieved in different
ways, such as Euclidean distance, cosine proximity, etc.
In this paper, inspired by the capsule net1 [32, 35], we
choose the inner-product for measuring the agreement, i.e.,

dij = −Dui, cj

kcj kE. By taking it into Eq. 5 and setting the

derivative to zero, we can obtain2

cj
kcjk

=

p

p

Pi=1
Pi=1

(cid:13)(cid:13)(cid:13)(cid:13)

sijui

sijui(cid:13)(cid:13)(cid:13)(cid:13)

,

(6)

which means the center and the integrated features lie in the
same direction. As the coefﬁcients s·j are the softmax val-
ues of distances, the corresponding center cj emerges in the
comparable scope as the features and Eq. 6 is approxima-

(4)

tively computed as cj =

sijui for simplicity. However,

p

Pi=1

there remains another problem that the computation of sij
depends on the current center cj , which makes it difﬁcult
to get the direct update rules for the centers. Instead, we
choose to alternatively update the coefﬁcient s(r)
ij and cen-
ter c(r+1)

, i.e.,

j

c(r+1)
j

=

p

Xi=1

s(r)
ij ui.

(7)

Actually, the updating rule is much similar to the EM algo-
rithm that maximizes posterior probabilities in GMMs [7].

1A discussion about capsule and DMC is provided in the materials.
2Detailed derivation is shown in the materials.

9251

Pi=1

Speciﬁcally, the ﬁrst step is the expectation step or E step,
which uses the current parameters to evaluate the posterior
probabilities, i.e., re-assigns data points to the centers. The
second step is the maximization step or M step that aims to
re-estimate the means, covariance, and mixing coefﬁcients,
i.e., update the centers in Eq.(7).

The aforementioned clusters indicate a kind of soft as-
signment (segmentation) over the input image or spectro-
gram, where each cluster mostly corresponds to certain con-
tent (e.g., baby face and drum in image, voice and drum-
beat in sound in Fig. 2), hence they can be viewed as the
distributed representations of each modality. And we ar-
gue that audio and visual messages should have similar
distributed representations when they jointly describe the
same natural scene. Hence, we propose to perform differ-
ent center-speciﬁc projections {W1, W2, ..., Wk} over the
audio and visual messages to distinguish the representa-
tions of different audiovisual entities, then cluster these
projected features into the multimodal centers for seeking
concrete audiovisual contents. Formally, the distance dij

and center updating become dij = −DWjui, cj
c(r+1)
j

s(r)
ij Wjui, where the projection matrix Wj is

kcj kE and

=

p

shared across modalities and considered as the association
with concrete audiovisual entity. Moreover, Wj also per-
forms as the magnitude parameter z when computing the
distance dij . We show the complete multimodal clustering
in Algorithm 1.

We employ the cosine proximity to measure the differ-
i ), where ca
ence between audiovisual centers, i.e., s (ca
i
and cv
i are the i-center for audio and visual modality, respec-
tively. To efﬁciently train the two-stream audiovisual net-
work, we employ the max-margin loss to encourage the net-
work to give more conﬁdence to the realistic image-sound
pair than mismatched ones,

i , cv

loss =

k

Xi=1,i6=j

max(cid:0)0, s(cid:0)ca

j , cv

i(cid:1) − s (ca

i , cv

i ) + ∆(cid:1),

(8)

i , cv

where ∆ is a margin hyper-parameter and ca
j means the
negative audio sample for the positive audiovisual pair of
(ca
i ). In practice, the negative example is randomly sam-
pled from the training set but different from the positive
one. The Adam optimizer with the learning rate of 10−4 is
used. Batch-size of 64 is selected for optimization. And we
train the audiovisual net for 25,000 iterations, which took 3
weeks on one K80 GPU card.

4. Feature Evaluation

Ideally, the unimodal networks should have learnt to re-
spond to different objects or sound scenes after training the
DMC model. Hence, we propose to evaluate the learned au-
dio and visual representations of the CNN internal layers.

2, ..., ua

Algorithm 1 Deep Multimodal Clustering Algorithm
Input: The feature vectors for each modality:
2, ..., uv

i ∈ Rn(cid:9)
p|uv
Output: The center vectors for each modality:
j ∈ Rm(cid:9)
k|cv

i ∈ Rn(cid:9), (cid:8)uv
q |ua
j ∈ Rm(cid:9), (cid:8)cv
k|ca
1, cv
ij = dv
ij = 0
1: for t = 1 to T , x in {a, v} do
2:

(cid:8)ua
1, ua
(cid:8)ca
1, ca

for i = 1 to q (p), j = 1 to k do

Initialize the distance da

2, ..., ca

2, ..., cv

1, uv

3:

4:

5:

Update weights: sx

Update centers: cx

j =

Update distances: dx

ij(cid:1)
ij = sof tmax(cid:0)−dx

p

ijWjux
sx
Pi=1
i
ij = −(cid:28)Wjux

i ,

cx
j
kcx

j k(cid:29)

end for

6:
7: end for

For efﬁciency, the DMC model is trained with 400K unla-
beled videos that are randomly sampled from the SoundNet-
Flickr dataset [5]. The input audio and visual message are
the same as [5], where pairs of 5s sound clip and corre-
sponding image are extracted from each video with no over-
lap. Note that, the constituted ∼1.6M audiovisual pairs are
about 17 times less than the ones in L3 [3] and 5 times less
than SoundNet [5].

4.1. Audio Features

The audio representation is evaluated in the complex en-
vironmental sound classiﬁcation task. The adopted ESC-50
dataset [29] is a collection of 2000 audio clips of 5s each.
They are equally partitioned into 50 categories. Hence, each
category contains 40 samples. For fairness, each sample is
also partitioned into 1s audio excerpts for data argumenta-
tion [5], and these overlapped subclips constitute the audio
inputs to the VGGish network. The mean accuracy is com-
puted over the ﬁve leave-one-fold-out evaluations. Note
that, the human performance on this dataset is 0.813.

The audio representations are extracted by pooling the
feature maps3. And a multi-class one-vs-all linear SVM
is trained with the extracted audio representations. The ﬁ-
nal accuracy of each clip is the mean value of its subclip
scores. To be fair, we also modify the DMC model into the
“teacher-student” scheme (‡DMC) where the VGG net is
pretrained with ImageNet and kept ﬁxed during training. In
Table 2 (a), it is obvious that the DMC model exceeds all the
previous methods except Audio-Visual Temporal Synchro-
nization (AVTS) [22]. Such performance is achieved with
less training data (just 400K videos), which conﬁrms that
our model can utilize more audiovisual correspondences in
the unconstrained videos to effectively train the unimodal
network. We also note that AVTS is trained with the whole

3Similarly with SoundNet [5], we evaluate the performance of different

VGGish layers and select conv4 1 as the extraction layer.

9252

(a) ESC-50

(b) Pascal VOC 2007

Methods

Accuracy

Methods

Accuracy

Autoencoder [5]
Rand. Forest [29]

ConvNet [28]
SoundNet [5]

L3 [3]
†L3 [3]

†AVTS [22]

DMC
‡DMC

Human Perfor.

0.399
0.443
0.645
0.742
0.761
0.793
0.823
0.798
0.826
0.813

0.375
Taxton. [27]
0.348
Kmeans [23]
0.422
Tracking [36]
0.467
Patch. [10]
Egomotion [2] 0.311
Sound(spe.) [27] 0.440
Sound(clu.) [27] 0.458
Sound(bia.) [27] 0.467
0.514
0.672

ImageNet

DMC

Table 2. Acoustic Scene Classiﬁcation on ESC-50 [29] and Im-
age Classiﬁcation on Pascal VOC 2007[12]. (a) For fairness, we
provide a weakened version of L3 that is trained with the same
audiovisual set as ours, while †L3 is trained with more data in [3].
†AVTS is trained with the whole SoundNet-Flickr dataset [5].
‡DMC takes supervision from the well-trained vision network for
training the audio subnet. (b) The shown results are the best ones
reported in [27] except the ones with FC features.

2M+ videos in [5], which is 5 times more than DMC.
Even so, DMCs still outperform AVTS on the DCASE2014
benchmark dataset (More details can be found in the mate-
rials). And the cross-modal supervision version ‡DMC im-
proves the accuracy further, where the most noticeable point
is that ‡DMC outperforms human [29] (82.6% vs 81.3%).
Hence, it veriﬁes that the elaborative alignment efﬁciently
works and the audiovisual correspondence indeed helps to
learn the unimodal perception.

4.2. Visual Features

The visual representation is evaluated in the object
recognition task. The chosen PASCAL VOC 2007 dataset
contains 20 object categories that are collected in realistic
scenes [12]. We perform global-pooling over the conv5 1
features of VGG16 net to obtain the visual features. A
multi-class one-vs-all linear SVM is also employed as the
classiﬁer, and the results are evaluated using Mean Average
Precision (mAP). As the DMC model does not contain the
standard FC layer as previous works, the best conv/pooling
features of other methods are chosen for comparison, which
have been reported in [27]. To validate the effectiveness of
multimodal clustering in DMC, we choose to compare with
the visual model in [27], which treats the separated sound
clusters as object indicators for visual supervision. In con-
trast, DMC model jointly learns the audio and visual rep-
resentation rather than the above single-ﬂow from sound to
vision, hence it is more ﬂexible for learning the audiovisual
correspondence. As shown in Table 2 (b), our model in-
deed shows noticeable improvement over the simple cluster

Figure 3. Qualitative examples of sound source localization. Af-
ter feeding the audio and visual messages into the DMC model,
we visualize the soft assignment that belongs to the most related
visual cluster to the audio messages. Note that, the visual scene
becomes more complex from top to bottom, and the label is just
for visualization purpose.

supervision, even its multi-label variation (in binary) [27].
Moreover, we also compare with the pretrained VGG16 net
on ImageNet. But, what surprises us is that the DMC model
is comparable the human performance in acoustic classiﬁ-
cation but it has a large gap with the image classiﬁcation
benchmark. Such differences may come from the complex-
ity of visual scene compared with the acoustic ones. Nev-
ertheless, our model still provides meaningful insights in
learning effective visual representation via audiovisual cor-
respondence.

5. Audiovisual Evaluation

5.1. Single Sound Localization

In this task, we aim to localize the sound source in the vi-
sual scene as [4, 26], where the simple case of single source
is considered. As only one sound appears in the audio track,
the generated audio features should share identical center.
In practice, we perform average-pooling over the audio cen-
ters into ca, then compare it with all the visual centers via
cosine proximity, where the number of visual centers is set
to 2 (i.e., sound-maker and other components) in this sim-
ple case. And the visual center with the highest score is
considered as the indicator of corresponding sound source.
In order to visualize the sound source further, we resort to
j , i.e., sv
the soft assignment of the selected visual center cv
·j .
As the assignment sv
·j is
reshaped back to the size of original feature map and viewed
as the heatmap that indicates the cluster property.

ij ∈ [0, 1], the coefﬁcient vector sv

In Fig. 3, we show the qualitative examples with respect
to the sound-source locations of different videos from the
SoundNet-Flickr dataset. It is obvious that the DMC model
has learnt to distinguish different visual appearances and
correlate the sound with corresponding visual source, al-
though the training phase is entirely performed in the un-

9253

supervised fashion. Concretely, in the simple scene, the
visual source of baby voice and car noise are easy to lo-
calize. When the visual scene becomes more complex, the
DMC model can also successfully localize the correspond-
ing source. The dog appearance highly responses to the
barking sound while the cat does not. In contrast to the au-
dience and background, only the onstage choruses response
to the singing sound. And the moving vehicles are success-
fully localized regardless of the driver or other visual con-
tents in the complex trafﬁc environment.

Apart from the qualitative analysis, we also provide the
quantitative evaluation. We directly adopt the annotated
sound-sources dataset [33], which are originally collected
from the SoundNet-Flickr dataset. This sub-dataset con-
tains 2,786 audio-image pairs, where the sound-maker of
each pair is individually located by three subjects. 250 pairs
are randomly sampled to construct the testing set (with sin-
gle sound). By setting an arbitrary threshold over the as-
signment sv
·j , we can obtain a binary segmentation over the
visual objects that probably indicate the sound locations.
Hence, to compare the automatic segmentation with hu-
man annotations, we employ the consensus Intersection
over Union (cIoU) and corresponding AUC area in [33] as
the evaluation metric. As shown in Table. 3, the proposed
DMC model is compared with the recent sound location
model with attention mechanism [33]. First, it is obvious
that DMC shows superior performance over the unsuper-
vised attention model. Particularly, when the cIoU thresh-
old becomes larger (i.e., 0.7), DMC even outperforms the
supervised ones. Second, apart from the most related visual
centers to the sound track, the unrelated one is also evalu-
ated. The large decline of the unrelated center indicates that
the clustering mechanism in DMC can effectively distin-
guish different modality components and exactly correlate
them among different modalities.

5.2. Real-Life Sound Event Detection

In this section, in contrast to speciﬁc sound separation
task4, we focus on a more general and complicated sound
task, i.e., multisource SED. In the realistic environment,
multiple sound tracks usually exist at the same time, i.e., the
street environment may be a mixture of people speaking,
car noise, walking sound, brakes squeaking, etc. It is ex-
pected to detect the existing sounds in every moment, which
is much more challenging than the previous single acoustic
recognition [16]. Hence, it becomes more valuable to eval-
uate the ability of DMC in learning the effective represen-
tation of multi-track sound.
In the DCASE2017 acoustic
challenges, the third task5 is exactly the multisource SED.

4The sound separation task mostly focuses speciﬁc task scenarios and
need effective supervision from the original sources[9, 26, 38], which goes
beyond our conditions.

5

http://www.cs.tut.ﬁ/sgn/arg/dcase2017/challenge/task-sound-event-

detection-in-real-life-audio

Methods

Random

Unsupervised† [33]
Unsupervised [33]

Supervised [33]
Sup.+Unsup. [33]
DMC (unrelated)

DMC (related)

cIoU(0.5)

cIoU(0.7) AUC

12.0
52.4
66.0
80.4
82.8
10.4
67.1

-
-

∼18.8
∼25.5
∼28.8

5.2
26.2

32.3
51.2
55.8
60.3
62.0
21.1
56.8

Table 3. The evaluation of sound source localization. The cIoUs
with threshold 0.5 and 0.7 are shown. The area under the cIoU
curve by varying the threshold from 1 to 0 (AUC) is also provided.
The unsupervised† method in [33] employs a modiﬁed attention
mechanism.

The audio dataset used in this task focuses on the complex
street acoustic scenes that consist of different trafﬁc levels
and activities. The whole dataset is divided for develop-
ment and evaluation, and each audio is 3-5 minutes long.
Segment-based F-score and error rate are calculated as the
evaluation metric.

As our model provides elaborative visual supervision for
training the audio subnet, the corresponding audio repre-
sentation should provide sufﬁcient description for the multi-
track sound. To validate the assumption, we directly replace
the input spectrum with our generated audio representation
in the baseline model of MLP [16]. As shown in Table. 4,
the DMC model is compared with the top ﬁve methods in
the challenge, the audiovisual net L3 [3], and the VGGish
net [17]. It is obvious that our model takes the ﬁrst place
on F1 metric and is comparable to the best model in error
rate. Speciﬁcally, there are three points we should pay atten-
tion to. First, by utilizing the audio representation of DMC
model instead of raw spectrum, we can have a noticeable
improvement. Such improvement indicates that the corre-
spondence learning across modalities indeed provides effec-
tive supervision in distinguishing different audio contents.
Second, as the L3 net simply performs global matching be-
tween audio and visual scene without exploring the concrete
content inside, it fails to provide effective audio representa-
tion for multisource SED. Third, although the VGGish net
is trained on a preliminary version of YouTube-8M (with
labels) that is much larger than our training data, our model
still outperforms it. This comes from the more efﬁcient au-
diovisual correspondence learning of DMC model.

5.3. Audiovisual Understanding

As introduced in the Section 1, the real-life audiovisual
environment is unconstrained, where each modality con-
sists of multiple instances or components, such as speak-
ing, brakes squeaking, walking sound in the audio modality
and building, people, cars, road in the visual modality of

9254

Methods

Segment F1

Segment Error

J-NEAT-E [24]

SLFFN [24]

ASH [1]

MICNN [21]

MLP [16]
§MLP [16]

L3 [3]

VGGish [17]

DMC

44.9
43.8
41.7
40.8
42.8
39.1
43.24
50.96
52.14

0.90
1.01
0.79
0.81
0.94
0.90
0.89
0.86
0.83

Table 4. Real life sound event detection on the evaluation dataset
of DCASE 2017 Challenge. We choose the default STFT parame-
ters of 25ms window size and 10 window hop [17]. The same pa-
rameters are also adopted by §MLP, L3, and VGGish, while other
methods adopt the default parameters in [16].

the street environment. Hence, it is difﬁcult to disentangle
them within each modality and establish exact correlations
between modalities, i.e., audiovisual understanding. In this
section, we attempt to employ the DMC model to perform
the audiovisual understanding in such cases where only the
qualitative evaluation is provided due to the absent annota-
tions. To illustrate the results better, we turn the soft assign-
ment of clustering into a binary map via a threshold of 0.7.
And Fig. 4 shows the matched audio and visual clustering
results of different real-life videos, where the sound is rep-
resented in spectrogram.
In the “baby drums” video, the
drumming sound and corresponding motion are captured
and correlated, meanwhile the baby face and people voice
are also picked out from the intricate audiovisual content.
These two distinct centers jointly describe the audiovisual
structures. In more complex indoor and outdoor environ-
ments, the DMC model can also capture the people yelling
and talking sound from background music and loud envi-
ronment noise by clustering the audio feature vectors, and
correlate them with the corresponding sound-makers (i.e.,
the visual centers) via the shared projection matrix. How-
ever, there still exist some failure cases. Concretely, the
out-of-view sound-maker is inaccessible for current visual
clustering. In contrast, the DMC model improperly corre-
lates the background music with kitchenware in the second
video. Similarly, the talking sound comes from the visible
woman and out-of-view photographer in the third video, but
our model simply extracts all the human voice and assigns
them to the visual center of woman. Such failure cases also
remind us that the real-life audiovisual understanding is far
more difﬁcult than what we have imagined. Moreover, to
perceive the audio centers more naturally, we reconstruct
the audio signal from the masked spectrogram information
and show them in the released video demo.

Figure 4. Qualitative examples of complex audiovisual under-
standing. We ﬁrst feed the audiovisual messages into the DMC
model, then the corresponding audiovisual clusters are captured
and shown, where the assignments are binarized into the masks
over each modality via a threshold of 0.7. The labels in the ﬁgure
indicate the learned audiovisual content, which is not used in the
training procedure.

6. Discussion

In this paper, we aim to explore the elaborate correspon-
dence between audio and visual messages in unconstrained
environment by resorting to the proposed deep multimodal
clustering method. In contrast to the previous rough corre-
spondence, our model can efﬁciently learn more effective
audio and visual features, which even exceed the human
performance. Further, such elaborate learning contributes
to noticeable improvements in the complicated audiovisual
tasks, such as sound localization, multisource SED, and au-
diovisual understanding.

Although the proposed DMC shows considerable supe-
riority over other methods in these tasks, there still remains
one problem that the number of clusters k is pre-ﬁxed in-
stead of automatically determined. When there is single
sound, it is easy to set k = 2 for foreground and back-
ground. But when multiple sound-makers emerge, it be-
comes difﬁcult to pre-determine the value of k. Although
we can obtain distinct clusters after setting k = 10 in the
audiovisual understanding task, more reliable method for
determining the number of audiovisual components is still
expected [31], which will be focused in the future work.

7. Acknowledgement

This work was supported in part by the National Natural
Science Foundation of China grant under number 61772427
and 61751202. We thank Jianlin Su for the constructive
opinion, and thank Zheng Wang and reviewers for refresh-
ing the paper.

9255

References

[1] Sharath Adavanne and Tuomas Virtanen. A report on sound
event detection with different binaural features. Technical
report, DCASE2017 Challenge, September 2017.

[2] Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning
to see by moving. In Computer Vision (ICCV), 2015 IEEE
International Conference on, pages 37–45. IEEE, 2015.

[3] Relja Arandjelovic and Andrew Zisserman. Look, listen and
learn. In 2017 IEEE International Conference on Computer
Vision (ICCV), pages 609–617. IEEE, 2017.

[4] Relja Arandjelovi´c and Andrew Zisserman. Objects that

sound. arXiv preprint arXiv:1712.06651, 2017.

[5] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Sound-
net: Learning sound representations from unlabeled video.
In Advances in Neural Information Processing Systems,
pages 892–900, 2016.

[6] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. See,
hear, and read: Deep aligned representations. arXiv preprint
arXiv:1706.00932, 2017.

[7] Christopher M Bishop. Pattern recognition and machine

learning. springer, 2006.

[8] Gemma A Calvert, Edward T Bullmore, Michael J Brammer,
Ruth Campbell, Steven CR Williams, Philip K McGuire, Pe-
ter WR Woodruff, Susan D Iversen, and Anthony S David.
Activation of auditory cortex during silent lipreading. sci-
ence, 276(5312):593–596, 1997.

[9] Anna Llagostera Casanovas, Gianluca Monaci, Pierre Van-
dergheynst, and R´emi Gribonval. Blind audiovisual source
separation based on sparse redundant representations. IEEE
Transactions on Multimedia, 12(5):358–371, 2010.

[10] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-
vised visual representation learning by context prediction. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1422–1430, 2015.

[11] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin
Wilson, Avinatan Hassidim, William T Freeman, and
Michael Rubinstein. Looking to listen at the cocktail party:
A speaker-independent audio-visual model for speech sepa-
ration. arXiv preprint arXiv:1804.03619, 2018.

[12] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge.
International journal of computer
vision, 88(2):303–338, 2010.

[13] Ruohan Gao, Rogerio Feris, and Kristen Grauman. Learning
to separate object sounds by watching unlabeled video. arXiv
preprint arXiv:1804.01665, 2018.

[14] David Harwath and James R Glass. Learning word-like
arXiv preprint

units from joint audio-visual analysis.
arXiv:1701.07481, 2017.

[15] David Harwath, Antonio Torralba, and James Glass. Unsu-
pervised learning of spoken language with visual context. In
Advances in Neural Information Processing Systems, pages
1858–1866, 2016.

[17] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F
Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal,
Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn ar-
chitectures for large-scale audio classiﬁcation. In Acoustics,
Speech and Signal Processing (ICASSP), 2017 IEEE Inter-
national Conference on, pages 131–135. IEEE, 2017.

[18] KAZUO Hikosaka, EIICHI Iwai, H Saito, and KEIJI Tanaka.
Polysensory properties of neurons in the anterior bank of
the caudal superior temporal sulcus of the macaque monkey.
Journal of neurophysiology, 60(5):1615–1637, 1988.

[19] Nicholas P Holmes and Charles Spence. Multisensory inte-
gration: space, time and superadditivity. Current Biology,
15(18):R762–R764, 2005.

[20] Anil K Jain. Data clustering: 50 years beyond k-means. Pat-

tern recognition letters, 31(8):651–666, 2010.

[21] Il-Young Jeong, Subin Lee, Yoonchang Han, and Kyogu
Lee. Audio event detection using multiple-input convolu-
tional neural network. Technical report, DCASE2017 Chal-
lenge, September 2017.

[22] Bruno Korbar, Du Tran, and Lorenzo Torresani. Co-training
of audio and video representations from self-supervised tem-
poral synchronization.
arXiv preprint arXiv:1807.00230,
2018.

[23] Philipp Kr¨ahenb¨uhl, Carl Doersch, Jeff Donahue, and Trevor
Darrell. Data-dependent initializations of convolutional neu-
ral networks. arXiv preprint arXiv:1511.06856, 2015.

[24] Christian Kroos and Mark D. Plumbley. Neuroevolution for
sound event detection in real life audio: A pilot study. Tech-
nical report, DCASE2017 Challenge, September 2017.

[25] Brian Kulis and Michael I Jordan. Revisiting k-means:
New algorithms via bayesian nonparametrics. arXiv preprint
arXiv:1111.0352, 2011.

[26] Andrew Owens and Alexei A Efros. Audio-visual scene
analysis with self-supervised multisensory features. arXiv
preprint arXiv:1804.03641, 2018.

[27] Andrew Owens, Jiajun Wu, Josh H McDermott, William T
Freeman, and Antonio Torralba. Ambient sound provides
supervision for visual learning. In European Conference on
Computer Vision, pages 801–816. Springer, 2016.

[28] Karol J Piczak. Environmental sound classiﬁcation with con-
volutional neural networks.
In Machine Learning for Sig-
nal Processing (MLSP), 2015 IEEE 25th International Work-
shop on, pages 1–6. IEEE, 2015.

[29] Karol J Piczak. Esc: Dataset for environmental sound classi-
ﬁcation. In Proceedings of the 23rd ACM international con-
ference on Multimedia, pages 1015–1018. ACM, 2015.

[30] Michael J Proulx, David J Brown, Achille Pasqualotto, and
Peter Meijer. Multisensory perceptual learning and sensory
substitution. Neuroscience & Biobehavioral Reviews, 41:16–
25, 2014.

[31] Siddheswar Ray and Rose H Turi. Determination of number
of clusters in k-means clustering and application in colour
image segmentation. In Proceedings of the 4th international
conference on advances in pattern recognition and digital
techniques, pages 137–143. Calcutta, India, 1999.

[16] Toni Heittola and Annamaria Mesaros. DCASE 2017 chal-
lenge setup: Tasks, datasets and baseline system. Technical
report, DCASE2017 Challenge, September 2017.

[32] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dy-
namic routing between capsules. In Advances in Neural In-
formation Processing Systems, pages 3859–3869, 2017.

9256

[33] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan
Yang, and In So Kweon. Learning to localize sound source
in visual scenes. arXiv preprint arXiv:1803.03849, 2018.

[34] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[35] Dilin Wang and Qiang Liu. An optimization view on dy-

namic routing between capsules. 2018.

[36] Xiaolong Wang and Abhinav Gupta. Unsupervised learn-
ing of visual representations using videos. arXiv preprint
arXiv:1505.00687, 2015.

[37] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level
convolutional networks for text classiﬁcation. In Advances
in neural information processing systems, pages 649–657,
2015.

[38] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Von-
drick, Josh McDermott, and Antonio Torralba. The sound of
pixels. arXiv preprint arXiv:1804.03160, 2018.

9257

