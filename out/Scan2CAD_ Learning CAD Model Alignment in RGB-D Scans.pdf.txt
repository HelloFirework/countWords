Scan2CAD: Learning CAD Model Alignment in RGB-D Scans

Armen Avetisyan1

Manuel Dahnert1

Angela Dai1

Manolis Savva2

Angel X. Chang2

Matthias Nießner1

1Technical University of Munich

2Simon Fraser University

Figure 1: Scan2CAD takes as input an RGB-D scan and a set of 3D CAD models (left). We then propose a novel 3D CNN
approach to predict heatmap correspondences between the scan and the CAD models (middle). From these predictions, we
formulate an energy minimization to ﬁnd optimal 9 DoF object poses for CAD model alignment to the scan (right).

Abstract

1. Introduction

We present Scan2CAD1, a novel data-driven method
that learns to align clean 3D CAD models from a shape
database to the noisy and incomplete geometry of an RGB-
D scan. For a 3D reconstruction of an indoor scene, our
method takes as input a set of CAD models, and predicts a
9DoF pose that aligns each model to the underlying scan
geometry. To tackle this problem, we create a new scan-
to-CAD alignment dataset based on 1506 ScanNet scans
with 97607 annotated keypoint pairs between 14225 CAD
models from ShapeNet and their counterpart objects in the
scans. Our method selects a set of representative keypoints
in a 3D scan for which we ﬁnd correspondences to the CAD
geometry. To this end, we design a novel 3D CNN archi-
tecture to learn a joint embedding between real and syn-
thetic objects, and thus predict a correspondence heatmaps.
Based on these correspondence heatmaps, we formulate
a variational energy minimization that aligns a given set
of CAD models to the reconstruction. We evaluate our
approach on our newly introduced Scan2CAD benchmark
where we outperform both handcrafted feature descriptor
as well as state-of-the-art CNN based methods by 21.39%.

1The Scan2CAD dataset is publicly released along with an automated

benchmark script for testing under www.Scan2CAD.org

In recent years, the wide availability of consumer-grade
RGB-D sensors, such as the Microsoft Kinect, Intel Real
Sense, or Google Tango, has led to signiﬁcant progress
in RGB-D reconstruction. We now have 3D reconstruc-
tion frameworks, often based on volumetric fusion [6],
that achieve impressive reconstruction quality [18, 29, 30,
40, 21] and reliable global pose alignment [40, 5, 8]. At
the same time, deep learning methods for 3D object clas-
siﬁcation and semantic segmentation have emerged as a
primary consumer of large-scale annotated reconstruction
datasets [7, 2]. These developments suggest great potential
in the future of 3D digitization, for instance, in applications
for virtual and augmented reality.

Despite these improvements in reconstruction quality,
the geometric completeness and ﬁne-scale detail of indoor
scene reconstructions remain a fundamental limitation. In
contrast to artist-created computer graphics models, 3D
scans are noisy and incomplete, due to sensor noise, motion
blur, and scanning patterns. Learning-based approaches for
object and scene completion [9, 37, 10] cannot reliably re-
cover sharp edges or planar surfaces, resulting in quality far
from artist-modeled 3D content.

One direction to address this problem is to retrieve a set
of CAD models from a shape database and align them to
an input scan, in contrast to a bottom-up reconstruction of

2614

the scene geometry. If all objects are replaced in this way,
we obtain a clean and compact scene representation, pre-
cisely serving the requirements for many applications rang-
ing from AR/VR scenarios to architectural design. Unfor-
tunately, matching CAD models to scan geometry is an ex-
tremely challenging problem: While high-level geometric
structures might be similar, the low-level geometric fea-
tures differ signiﬁcantly (e.g., surface normal distributions).
This severely limits the applicability of handcrafted geo-
metric features, such as FPFH [33], SHOT [35], point-pair-
features [11], or SDF-based feature descriptors [25]. While
learning-based approaches like random forests [28, 36] ex-
ist, their model capacity remains relatively low, especially
in comparison to more modern methods based on deep
learning, which can achieve signiﬁcantly higher accuracy,
but remain at their infancy. We believe this is in large part
attributed to the lack of appropriate training data.

In this paper, we make the following contributions:

• We introduce the Scan2CAD dataset, a large-scale
dataset comprising 97607 pairwise keypoint corre-
spondences and 9DoF alignments between 14225 in-
stances of 3049 unique synthetic models, between
ShapeNet [3] and reconstructed scans in ScanNet [7],
as well as oriented bounding boxes for each object.

• We propose a novel 3D CNN architecture that learns a
joint embedding between real and synthetic 3D objects
to predict accurate correspondence heatmaps between
the two domains.

• We present a new variational optimization formulation
to minimize the distance between scan keypoints and
their correspondence heatmaps, thus obtaining robust
9DoF scan-to-CAD alignments.

2. Related work

RGB-D Scanning and Reconstruction The availability
of low-cost RGB-D sensors has led to signiﬁcant research
progress in RGB-D 3D reconstruction. A very prominent
line of research is based on volumetric fusion [6], where
depth data is integrated in a volumetric signed distance
function. Many modern real-time reconstruction methods,
such as KinectFusion [18, 29], are based on this surface
representation.
In order to make the representation more
memory-efﬁcient, octree [4] or hash-based scene represen-
tations have been proposed [30, 21]. An alternative fusion
approach is based on points [22]; the reconstruction qual-
ity is slightly lower, but it has more ﬂexibility when han-
dling scene dynamics and can be adapted on-the-ﬂy for loop
closures [40]. Very recent RGB-D reconstruction frame-
works combine efﬁcient scene representations with global
pose estimation [5], and can even perform online updates
with global loop closures [8]. A closely related direction to
ours (and a possible application) is recognition of objects as

a part of a SLAM method, and using the retrieved objects
as part of a global pose graph optimization [34, 27].

3D Features for Shape Alignment and Retrieval Geo-
metric features have a long-established history in computer
vision, such as Spin Images [20], Fast Point Feature His-
tograms (FPFH) [33], or Point-Pair Features (PPF) [11].
Based on these descriptors or variations of them,
re-
searchers have developed shape retrieval and alignment
methods. For instance, Kim et al. [24] learn a shape prior in
the form of a deformable part model from input scans to ﬁnd
matches at test time; or AA2h [23] use a similar approach
to PPF, where a histogram of normal distributions of sam-
ple points is used for retrieval. Li et al. [25] propose a for-
mulation based on a hand-crafted TSDF feature descriptor
to align CAD models in real-time to RGB-D scans. While
these retrieval approaches based on hand-crafted geomet-
ric features show initial promise, they struggle to generalize
matching between the differing data characteristics of clean
CAD models and noisy, incomplete real-world data.

An alternative direction is learned geometric feature de-
scriptors. For example, Nan et al. [28] use a random deci-
sion forest to classify objects on over-segmented input ge-
ometry from high-quality scans. Shao et al. [36] introduce
a semi-automatic system to resolve segmentation ambigui-
ties, where a user ﬁrst segments a scene into semantic re-
gions, and then shape retrieval is applied. 3DMatch [43]
leverage a Siamese neural network to match keypoints in
3D scans for pose estimation. Zhou et al. [44] is of similar
nature, proposing a view consistency loss for 3D keypoint
prediction network on RGB-D image data. Inspired by such
approaches, we develop a 3D CNN-based approach target-
ing correspondences between the synthetic domain of CAD
models and the real domain of RGB-D scan data.

Other approaches retrieve and align CAD models given
single RGB [26, 19, 38, 17] or RGB-D [12, 45] images.
These methods are related, but our focus is on geomet-
ric alignment independent of RGB information, rather than
CAD-to-image.

Shape Retrieval Challenges and RGB-D Datasets
Shape retrieval challenges have recently been organized
as part of the Eurographics 3DOR [16, 32]. Here, the
task was formulated as matching of object instances from
ScanNet [7] and SceneNN [15] to CAD models from the
ShapeNetSem dataset [3]. Evaluation only considered
binary in-category vs out-of-category (and sub-category)
match as the notion of relevance. As such, this evaluation
does not address the alignment quality between scan objects
and CAD models, which is our focus.

ScanNet [7] provides aligned CAD models for a small
subset of the annotated object instances (for only 200 ob-
jects out of the total 36000). Moreover, the alignment

2615

quality is low with many object category mismatches and
alignment errors, as the annotation task was performed by
crowdsourcing. The PASCAL 3D+ [42] dataset annotates
13898 objects in the PASCAL VOC images with coarse 3D
poses deﬁned against representative CAD models. Object-
Net3D [41] provides a dataset of CAD models aligned to
2D images, approximately 200K object instances in 90K
images. The IKEA objects [26] and Pix3D [38] datasets
similarly provide alignments of a small set of identiﬁable
CAD models to 2D images of the same objects in the real
world; the former has 759 images annotated with 90 mod-
els, the latter has 10069 annotated with 395 models.

No existing dataset provides ﬁne-grained object instance
alignments at the scale of our Scan2CAD dataset with
14225 CAD models (3049 unique instances) annotated to
their scan counterpart distributed on 1506 3D scans.

3. Overview

Task We address alignment between clean CAD models
and noisy, incomplete 3D scans from RGB-D fusion, as il-
lustrated in Fig. 1. Given a 3D scene S and a set of 3D CAD
models M = {mi}, the goal is to ﬁnd a 9DoF transforma-
tion Ti (3 degrees for translation, rotation, and scale each)
for every CAD model mi such that it aligns with a semanti-
cally matching object O = {oj} in the scan. One important
note is that we cannot guarantee the existence of 3D models
which exactly matches the geometry of the scan objects.

Dataset and Benchmark In Sec. 4, we introduce the con-
struction of our Scan2CAD dataset. We propose an anno-
tation pipeline designed for use by trained annotators. An
annotator ﬁrst inspects a 3D scan and selects a model from a
CAD database that is geometrically similar to a target object
in the scan. Then, for each model, the annotator deﬁnes cor-
responding keypoint pairs between the model and the object
in the scan. From these keypoints, we compute ground truth
9DoF alignments. We annotate the entire ScanNet dataset
and use the original training, validation, and test splits to
establish our alignment benchmark.

Heatmap Prediction Network In Sec. 5, we propose a
3D CNN taking as input a volume around a candidate key-
point in a scan and a volumetric representation of a CAD
model. The network is trained to predict a correspondence
heatmap over the CAD volume, representing the likelihood
that the input keypoint in the scan is matching with each
voxel. The heatmap prediction is formulated as a classiﬁ-
cation problem, which is easier to train than regression, and
produces sparse correspondences needed for pose optimiza-
tion.

Alignment Optimization Sec. 6 describes our variational
alignment optimization. To generate candidate correspon-
dence points in the 3D scan, we detect Harris keypoints, and
predict correspondence heatmaps for each Harris keypoint

(a) First step: Retrieval view.

(b) Second step: Alignment view.

Figure 2: Our annotation web interface is a two-step pro-
cess. (a) After the user places an anchor on the scan surface,
class-matching CAD models are displayed on the right. (b)
Then the user annotates keypoint pairs between the scan and
CAD model from which we derive the ground truth 9DoF
transformation.

and CAD model. Using the predicted heatmaps we ﬁnd op-
timal 9DoF transformations. False alignments are pruned
via a geometric conﬁdence metric.

4. Dataset

Our Scan2CAD dataset builds upon the 3D scans from
ScanNet [7] and CAD models from ShapeNet [3]. Each
scene S contains multiple objects O = {oi}, where each ob-
ject oi is matched with a ShapeNet CAD model mi and both
share multiple keypoint pairs (correspondences) and one
transformation matrix Ti deﬁning the alignment. Note that
ShapeNet CAD models have a consistently deﬁned front
and upright orientation which induces an amodal tight ori-
ented bounding box for each scan object, see Fig. 3.

4.1. Data Annotation

The annotation is done via a web application that allows
for simple scaling and distribution of annotation jobs; see
Fig. 2. The annotation process is separated into two steps.
The ﬁrst step is object retrieval, where the user clicks on a
point on the 3D scan surface, implicitly determining an ob-
ject category label from the ScanNet object instance anno-
tations. We use the instance category label as query text in
the ShapeNet database to retrieve and display all matching

2616

Figure 3: (Left) Oriented bounding boxes (OBBs) com-
puted from the instance segmentation of ScanNet [7] are
often incomplete due to missing geometry (e.g., in this case,
missing chair legs). (Right) Our OBBs are derived from the
aligned CAD models and are thus complete.

CAD models in a separate window as illustrated in Fig. 2a.
After selecting a CAD model the user performs alignment.
In the alignment step, the user sees two separate win-
dows in which the CAD model (left) and the scan object
(right) are shown (see Fig. 2b). Keypoint correspondences
are deﬁned by alternately clicking paired points on the CAD
model and scan object. We require users to specify at least
6 keypoint pairs to determine a robust ground truth trans-
formation. After keypoint pairs are speciﬁed, the alignment
computation is triggered by clicking a button. This align-
ment (given exact 1-to-1 correspondences) is solved with
the genetic algorithm CMA-ES [14, 13] that minimizes the
point-to-point distance over 9 parameters. In comparison
to gradient-based methods or Procrustes superimposition
method, we found this approach to perform signiﬁcantly
better in reliably returning high-quality alignments regard-
less of initialization.

The quality of these keypoint pairs and alignments was
veriﬁed in several veriﬁcation passes, with re-annotations
performed to ensure a high quality of the dataset. The veri-
ﬁcation passes were conducted by the authors of this work.
A subset of the ShapeNet CAD models have symme-
tries that play an important role in making correspondences.
Hence, we annotated all ShapeNet CAD models used in
our dataset with their rotational symmetries to prevent false
negatives in evaluations. We deﬁned 2-fold (C2), 4-fold
(C4) and inﬁnite (C∞) rotational symmetries around a
canonical axis of the object.

4.2. Dataset Statistics

The annotation process yielded 97607 keypoint pairs on
14225 (3049 unique) CAD models with their respective
scan counterpart distributed on a total of 1506. Approxi-
mately 28% out of the 3049 CAD models have a symmetry
tag (either C2, C4 or C∞).

Given the complexity of the task and to ensure high qual-
ity annotations, we employed 7 part-time annotators (in
contrast to crowd-sourcing). On average, each scene has
been edited 1.76 times throughout the re-annotation cycles.
The top 3 annotated model classes are chairs, tables and
cabinets which arises due to the nature of indoor scenes in

ScanNet. The number of objects aligned per scene ranges
from 1 to 40 with an average of 9.3. It took annotators on
average of 2.48min to align each object, where the time to
ﬁnd an appropriate CAD model dominated the time for key-
point placement. The average annotation time for an entire
scene is 20.52min.

It is interesting to note that manually placed keypoint
correspondences between scans and CAD models differ sig-
niﬁcantly from those extracted from a Harris corner detec-
tor. Here, we compare the mean distance from the anno-
tated CAD keypoint to: (1) the corresponding annotated
scan keypoint (= 3.5cm) and (2) the nearest Harris key-
point in the scan (= 12.8cm).

4.3. Benchmark

Using our annotated dataset, we designed a benchmark
to evaluate scan-to-CAD alignment methods. A model
alignment is considered successful only if the category of
the CAD model matches that of the scan object and the pose
error is within translation, rotational, and scale bounds rel-
ative to the ground truth CAD. We do not enforce strict in-
stance matching (i.e., matching the exact CAD model of the
ground truth annotation) as ShapeNet models typically do
not identically match real-world scanned objects. Instead,
we treat CAD models of the same category as interchange-
able (according to the ShapeNetCorev2 top-level synset).

Once a CAD model is determined to be aligned correctly,
the ground truth counterpart is removed from the candidate
pool in order to prevent multiple alignments to the same
object. Alignments are fully parameterized by 9 pose pa-
rameters. A quantitative measure based on bounding box
overlap (IoU) can be readily calculated with these parame-
ters as CAD models are deﬁned on the unit box. The error
thresholds for a successful alignment are set to ǫt ≤ 20cm,
ǫr ≤ 20◦, and ǫs ≤ 20% for translation, rotation, and scale
respectively (for extensive error analysis please see the sup-
plemental). The rotation error calculation takes C2, C4 and
C∞ rotated versions into account.

The Scan2CAD dataset and associated symmetry anno-
tations are available to the community. For standardized
comparison of future approaches, we operate an automated
test script on a hidden test set that can be found under
www.Scan2CAD.org.

5. Correspondence Prediction Network

5.1. Data Representation

Scan data is represented by its signed distance ﬁeld
(SDF) encoded in a volumetric grid and generated through
volumetric fusion [6] from the depth maps of the RGB-D re-
construction (voxel resolution = 3cm, truncation = 15cm).
For the CAD models, we compute unsigned distance ﬁelds
(DF) using the level-set generation toolkit by Batty [1].

2617

Figure 4: 3D CNN architecture of our Scan2CAD approach: we take as input SDF chunks around a given keypoint from a 3D
scan and the DF of a CAD model. These are encoded with 3D CNNs to learn a shared embedding between the synthetic and
real data; from this, we classify whether there is semantic compatibility between both inputs (top), predict a correspondence
heatmap in the CAD space (middle) and the scale difference between the inputs (bottom).

5.2. Network Architecture

Our architecture takes as input a pair of voxel grids: A
SDF centered at a point in the scan with a large receptive
ﬁeld at 643 size, and a DF of a particular CAD model at 323
size. We use a series of convolutional layers to separately
encode each input stream (see Fig. 4). The two encoders
compress the volumetric representation into compact fea-
ture volumes of 43 × 64 (scan) and 43 × 8 (CAD) which
are then concatenated before passing to the decoder stage.
The decoder stage predicts three output targets, heatmap,
compatibility, and scale, described as follows:

Heatmap The ﬁrst output is a heatmap H : Ω → [0, 1]
over the 323 voxel domain Ω ∈ N3 of the CAD model pro-
ducing the voxel-wise correspondence probability. This in-
dicates the probability of matching each voxel in Ω to the
center point of the scan SDF. We train our network using a
combined binary cross-entropy (BCE) loss and a negative
log-likelihood (NLL) to predict the ﬁnal heatmap H. The
raw output S : Ω → R of the last layer in the decoder is
used to generate the heatmaps:

is constructed with an element-wise multiplication of both
heatmap variations: H = H1 ◦ H2.

Compatibility The second prediction target is a single
probability score ∈ [0, 1] indicating semantic compatibil-
ity between scan and CAD. This category equivalence score
is 0 when the category labels are different (e.g., scan table
and CAD chair) and 1 when the category labels match (e.g.,
scan chair and CAD chair). The loss function for this output
is a sigmoid function followed by a BCE loss:

Lcompat. = BCE(sigmoid(x), xGT)

Scale The third output predicts the scale ∈ R3 of the CAD
model to the respective scan. Note that we do not explicitly
enforce positivity of the predictions. This loss term is a
mean-squared-error (MSE) for a prediction x ∈ R3:

Lscale = MSE(x, xGT) = kx − xGTk2

2

Finally, to train our network, we use a weighted combi-

nation of the presented losses:

L = 1.0LH + 0.1Lcompat. + 0.2Lscale

H1 : Ω → [0, 1],

x 7→ sigmoid(S(x))

H2 : Ω → [0, 1],

x 7→ softmax(S(x))

LH = Xx∈Ω

w(x) · BCE(H1, HGT) + Xx∈Ω

v · NLL(H2, HGT)

where the weighting of each loss component was empiri-
cally determined for balanced convergence.

5.3. Training Data Generation

where w(x) = 64.0 if HGT(x) > 0.0 else 1.0, v = 64 are
weighting factors to increase the signal of the few sparse
positive keypoint voxels in the voxel grid (≈ 99% of the
target voxels have a value equal to 0). The combination of
the sigmoid and softmax terms is a compromise between
high recall but low precision using sigmoid, and more lo-
cally sharp keypoint predictions using softmax over all vox-
els. The ﬁnal target heatmap, used later for alignment,

Voxel Grids Centered scan volumes are generated by pro-
jecting the annotated keypoint into the scan voxel grid and
then cropping around it with a crop window of 633. Ground
truth heatmaps are generated by projecting annotated key-
points (and any symmetry-equivalent keypoints) into the
CAD voxel grid. We then use a Gaussian blurring kernel
(σ = 2.0) on the voxel grid to account for small keypoint
annotation errors and to avoid sparsity in the loss residuals.

2618

Training Samples With our annotated dataset we gen-
erate NP,ann. = 97607 positive training pairs where one
pair consists of an annotated scan keypoint and the corre-
sponding CAD model. Additionally, we create NP,aug. =
10 · NP,ann., augmented positive keypoint pairs by randomly
sampling points on the CAD surface, projecting them to the
scan via the ground truth transformation and rejecting if the
distance to the surface in the scan ≥ 3cm. In total we gen-
erate NP = NP,ann. + NP,aug. positive training pairs.

Negative pairs are generated in two ways: (1) Randomly
choosing a voxel point in the scan and a random CAD
model (likelihood of false negative is exceedingly low). (2)
Taking an annotated scan keypoint and pairing it with a ran-
dom CAD model of different class. We generate NN = NP
negative samples with (1) and NHN = NP with (2).

Hence, the training set has a positives-to-negatives ratio
of 1:2 (NP : NN +NHN ). We found an over-representation
of negative pairs gives satisfactory performance on the com-
patibility prediction.

5.4. Training Process

We use an SGD optimizer with a batch size of 32 and
an initial learning rate of 0.01, which is decreased by 1/2
every 50K iterations. We train for 250K iterations (≈ 62.5
hours). The weights are initialized randomly. The losses
of the heatmap prediction stream and the scale prediction
stream are masked such that only positive samples make up
the residuals for back-propagation.

The CAD encoder is pre-trained with an auto-encoder on
ShapeNet models with a reconstruction task and a M SE as
loss function. All models of ShapeNetCore (≈ 55K) are
used for pre-training and the input and output dimensions
are 323 distance ﬁeld grids. The network is trained with
SGD until convergence (≈ 50 epochs).

6. Alignment Optimization

Filtering The input to our alignment optimization is a
representative set of Harris keypoints K = {pj}, j =
1 . . . N0 from a scene S and a set of CAD models M =
{mi}. The correspondences between K and M were estab-
lished by the correspondence prediction from the previous
stage (see Sec. 5) where each keypoint pj is tested against
every model mi.

Since not every keypoint pj semantically matches to ev-
ery CAD model mi, we reject correspondences based on
the compatibility prediction of our network. The thresh-
old for rejecting pj is determined by the Otsu thresholding
scheme [31]. In practice this method turned out to be much
more effective than a ﬁxed threshold. After the ﬁltering
there are N ≤ N0 (usually N ≈ 0.1N0) correspondence
pairs to be used for the alignment optimization.

Variational Optimization From the remaining Kﬁlter. ⊂
K Harris keypoints, we construct point-heatmap pairs
(pj, Hj) for each CAD model mi, with pj ∈ R3 a point
in the scan and Hj : Ω → [0, 1] a heatmap.

In order to ﬁnd an optimal pose we construct the follow-

ing minimization problem:

cvox = Tworld→vox · Tmi (a, s) · pj

f = min
a,s

N

Xj

(1 − Hj(cvox))2 + λsksk2

2

(1)

where cvox is a voxel coordinate, Tworld→vox denotes a trans-
formation that maps world points into the voxel grid for
look-ups, a denotes the coordinates of the Lie algebra (for
rotation and translation), s deﬁnes the scale, and λs deﬁnes
the scale regularization strength. a, s compose a transfor-
mation matrix Tmi = ψ(ami , smi ):

ψ : R6 × R3 → R4×4,

a, s 7→ expm(cid:18)(cid:20)Γ(a1,2,3) a4,5,6

0 (cid:21)(cid:19) ·(cid:20)s

0 1(cid:21)

0

0

where Γ is the hat map, expm is the matrix exponential.

We solve Eq. 1 using the Levenberg-Marquardt (LM) al-
gorithm. As we can suffer from zero-gradients (especially
at bad initialization), we construct a scale-pyramid from the
heatmaps which we solve in coarse-to-ﬁne fashion.

In each LM step we optimize over the incremental
change and update the parameters as following: T k+1
mi ←
φ(a∗, s∗) · T k
mi where a∗, s∗ are the optimal parameters. As
seen in Eq. 1, we add a regularization on the scale in order
to prevent degenerate solutions which can appear for very
large scales.

By restarting the optimization with different translation
parameters (i.e., varying initializations), we obtain multiple
alignments per CAD model mi. We then generate as many
CAD model alignments as required for a given scene in the
evaluation. Note, in a ground truth scene one unique CAD
model mi can appear in multiple locations e.g., chairs in
conference rooms.

Pruning Finally, there will be alignments of various CAD
models into a scene where a subset will be misaligned. In
order to select only the best alignments and prune potential
misalignments we use a conﬁdence metric similar to [25];
for more detail, we refer to the appendix.

7. Results

7.1. Correspondence Prediction

To quantify the performance of correspondence heatmap
predictions, we evaluate the voxel-wise F1-score for a pre-
diction and its Gaussian-blurred target. The task is chal-
lenging and by design 2
3 test samples are false correspon-
dences, ≈ 99% of the target voxels are 0-valued, and only a

2619

base [+variations, ...]

bath

bookshelf

cabinet

chair

display

sofa

table

trash bin

other

class avg.

+sym
+sym,+scale
+sym,+CP
+scale,+CP
+sym,+scale,+CP
+sym,+scale,+CP,+PT (3/3 ﬁx)
+sym,+scale,+CP,+PT (1/3 ﬁx)

46.88
51.35
59.32
45.24
56.05
57.03
60.08

44.39
45.46
51.93
45.85
51.28
50.63
58.62

40.49
45.24
55.11
47.16
57.45
56.76
56.35

64.46
66.94
70.99
61.55
72.64
70.39
73.92

26.85
29.88
41.58
27.65
36.36
39.74
44.19

56.26
64.78
66.77
51.92
70.63
65.00
75.08

47.15
48.30
53.74
41.21
52.28
52.03
56.80

38.43
38.00
43.39
31.13
46.80
46.87
45.78

24.68
28.65
42.93
29.62
43.32
41.83
46.53

43.29
46.51
53.97
42.37
54.09
53.36
57.48

avg.

48.01
50.85
60.44
47.64
60.43
58.61
63.94

Table 1: Correspondence prediction F1-scores in % for variations of our correspondence prediction network. We evaluate the
effect of symmetry (sym), predicting scale (scale), predicting compatibility (CP), encoder pre-training (PT), and pre-training
with parts of the encoder ﬁxed (#ﬁx), see Sec. 5 for more detail regarding our network design and training scheme.

single 1-valued voxel out of 323 voxels exists. The F1-score
will increase only by identifying true correspondences. As
seen in Tab. 1, our best 3D CNN achieves 63.94%.

Tab. 1 additionally addressed our design choices; in par-
ticular, we evaluate the effect of using pre-training (PT), us-
ing compatibility (CP) as a proxy loss (deﬁned in Sec. 5.2),
enabling symmetry awareness (sym), and predicting scale
(scale). Here, a pre-trained network reduces overﬁtting, en-
hancing generalization capability. Optimizing for compati-
bility strongly improves heatmap prediction as it efﬁciently
detects false correspondences. While predicting scale only
slightly inﬂuences the heatmap predictions, it becomes very
effective for the later alignment stage. Additionally, incor-
porating symmetry enables signiﬁcant improvement by ex-
plicitly disambiguating symmetric keypoint matches.

7.2. Alignment

In the following, we compare our approach to other
handcrafted feature descriptors: FPFH [33], SHOT [39], Li
et al. [25] and a learned feature descriptor: 3DMatch [43]

(trained on our Scan2CAD dataset). We combine these de-
scriptors with a RANSAC outlier rejection method to obtain
pose estimations for an input set of CAD models. A detailed
description of the baselines can be found in the appendix.
As seen in Tab. 2, our best method achieves 31.68% and
outperforms all other methods by a signiﬁcant margin. We
additionally show qualitative results in Fig. 5. Compared to
state-of-the-art handcrafted feature descriptors, our learned
approach powered by our Scan2CAD dataset produces con-
siderably more reliable correspondences and CAD model
alignments. Even compared to the learned descriptor ap-
proach of 3DMatch, our explicit learning across the syn-
thetic and real domains coupled with our alignment op-
timization produces notably improved CAD model align-
ment.

Fig. 6 shows the capability of our method to align in an
unconstrained real-world setting where ground truth CAD
models are not given, we instead provide a set of 400 ran-
dom CAD models from ShapeNet [3].

Figure 5: Qualitative comparison of alignments on four different test ScanNet [7] scenes. Our approach to learning geometric
features between real and synthetic data produce much more reliable keypoint correspondences, which coupled with our
alignment optimization, produces signiﬁcantly more accurate alignments.

2620

bath

bookshelf

cabinet

chair

display

sofa

table

trash bin

other

class avg.

FPFH (Rusu et al. [33])
SHOT (Tombari et al. [39])
Li et al. [25]
3DMatch (Zeng et al. [43])
Ours: +sym
Ours: +sym,+scale
Ours: +sym,+CP
Ours: +scale,+CP
Ours: +sym,+scale,+CP
Ours: +sym,+scale,+CP,+PT (3/3 ﬁx)
Ours: +sym,+scale,+CP,+PT (1/3 ﬁx)

0.00
0.00
0.85
0.00
24.30
18.99
35.90
34.18
36.20
37.97
34.81

1.92
1.43
0.95
5.67
10.61
13.61
32.35
31.76
36.40
30.15
36.40

0.00
1.16
1.17
2.86
5.97
7.24
28.64
21.82
34.00
28.64
29.00

10.00
7.08
14.08
21.25
9.49
14.73
40.48
37.02
44.26
41.55
40.60

0.00
0.59
0.59
2.41
3.90
9.76
18.85
14.75
17.89
19.51
23.25

5.41
3.57
6.25
10.91
25.26
41.05
60.00
50.53
70.63
57.89
66.00

2.04
1.47
2.95
6.98
12.34
14.04
33.11
32.31
30.66
33.85
37.64

1.75
0.44
1.32
3.62
10.74
5.26
28.42
31.05
30.11
20.00
24.32

2.00
0.75
1.50
4.65
3.58
6.29
16.89
11.59
20.60
17.22
22.81

2.57
1.83
3.30
6.48
11.80
14.55
32.74
29.45
35.64
31.86
34.98

avg.

4.45
3.14
6.03
10.29
8.772
11.48
29.42
26.75
31.68
29.27
31.22

Table 2: Accuracy comparison (%) on our CAD alignment benchmark. While handcrafted feature descriptors can achieve
some alignment on more featureful objects (e.g., chairs, sofas), they do not tolerate well the geometric discrepancies between
scan and CAD data – which remains difﬁcult for the learned keypoint descriptors of 3DMatch. Scan2CAD directly addresses
this problem of learning features that generalize across these domains, thus signiﬁcantly outperforming state of the art.

respondence prediction, as RGB data is typically higher-
resolution than depth or geometry, and could potentially im-
prove alignment results.

9. Conclusion

In this work, we presented Scan2CAD, which aligns a set
of CAD models to 3D scans by predicting correspondences
in form of heatmaps and then optimizes over these corre-
spondence predictions. First, we introduce a new dataset of
9DoF CAD-to-scan alignments with 97607 pairwise key-
point annotations deﬁning the alignment of 14225 objects.
Based on this new dataset, we design a 3D CNN to predict
correspondence heatmaps between a CAD model and a 3D
scan. From these predicted heatmaps, we formulate a vari-
ational cost minimization that then ﬁnds the optimal 9DoF
pose alignments between CAD models and the scan, en-
abling effective transformation of noisy, incomplete RGB-
D scans into a clean, complete CAD model representation.
This enables us to achieve signiﬁcantly more accurate re-
sults than state-of-the-art approaches, and we hope that our
dataset and benchmark will inspire future work towards
bringing RGB-D scans to CAD or artist-modeled quality.

Acknowledgements

We would like to thank the expert annotators Soh Yee
Lee, Rinu Shaji Mariam, Suzana Spasova, Emre Taha, Se-
bastian Thekkekara, and Weile Weng for their efforts in
building the Scan2CAD dataset. We thank valuable discus-
sions with J¨urgen Sturm. This work is supported by Occipi-
tal, the ERC Starting Grant Scan2CAD (804724), a Google
Faculty Award, and the ZD.B. We would also like to thank
the support of the TUM-IAS, funded by the German Ex-
cellence Initiative and the European Union Seventh Frame-
work Programme under grant agreement n 291763, for the
TUM-IAS Rudolf M¨oßbauer Fellowship and Hans-Fisher
Fellowship (Focus Group Visual Computing).

2621

Figure 6: Unconstrained scenario where instead of having a
ground truth set of CAD models given, we use a set of 400
randomly selected CAD models from ShapeNetCore [3],
more closely mimicking a real-world application scenario.

8. Limitations

While the focus of this work is mainly on the alignment
between 3D scans and CAD models, we only provide a ba-
sic algorithmic component for retrieval (ﬁnding the most
similar model). This necessitates an exhaustive search over
a set of CAD models. We believe that one of the immediate
next steps in this regard would be designing a neural net-
work architecture that is speciﬁcally trained on shape sim-
ilarity between scan and CAD geometry to introduce more
efﬁcient CAD model retrieval. Additionally, we currently
only consider geometric information, and it would also be
intresting to introduce learned color features into the cor-

References

[1] C. Batty.

SDFGen.

https://github.com/

christopherbatty/SDFGen. 4

[2] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner,
M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3D:
Learning from RGB-D data in indoor environments. Inter-
national Conference on 3D Vision (3DV), 2017. 1

[3] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich
3D Model Repository. Technical Report arXiv:1512.03012
[cs.GR], Stanford University — Princeton University —
Toyota Technological Institute at Chicago, 2015. 2, 3, 7,
8

[4] J. Chen, D. Bautembach, and S. Izadi. Scalable real-time
volumetric surface reconstruction. ACM Transactions on
Graphics (TOG), 32(4):113, 2013. 2

[5] S. Choi, Q.-Y. Zhou, and V. Koltun. Robust reconstruction
of indoor scenes.
In 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 5556–5565.
IEEE, 2015. 1, 2

[6] B. Curless and M. Levoy. A volumetric method for building
complex models from range images. In Proceedings of the
23rd annual conference on Computer graphics and interac-
tive techniques, pages 303–312. ACM, 1996. 1, 2, 4

[7] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Nießner. ScanNet: Richly-annotated 3D reconstruc-
tions of indoor scenes. In Proc. Computer Vision and Pattern
Recognition (CVPR), IEEE, 2017. 1, 2, 3, 4, 7

[8] A. Dai, M. Nießner, M. Zollh¨ofer, S. Izadi, and C. Theobalt.
Bundlefusion: Real-time globally consistent 3d reconstruc-
tion using on-the-ﬂy surface reintegration. ACM Transac-
tions on Graphics (TOG), 36(3):24, 2017. 1, 2

[9] A. Dai, C. R. Qi, and M. Nießner. Shape completion us-
ing 3d-encoder-predictor cnns and shape synthesis. In Proc.
Computer Vision and Pattern Recognition (CVPR), IEEE,
2017. 1

[10] A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and
M. Nießner. Scancomplete: Large-scale scene completion
and semantic segmentation for 3d scans.
arXiv preprint
arXiv:1712.10215, 2018. 1

[11] B. Drost and S. Ilic. 3d object detection and localization
using multimodal point pair features. In 2012 Second Inter-
national Conference on 3D Imaging, Modeling, Processing,
Visualization & Transmission, pages 9–16. IEEE, 2012. 2

[12] S. Gupta, P. Arbel´aez, R. Girshick, and J. Malik. Aligning
3D models to RGB-D images of cluttered scenes.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4731–4740, 2015. 2

[13] N. Hansen. Benchmarking a bi-population cma-es on the
bbob-2009 function testbed.
In Proceedings of the 11th
Annual Conference Companion on Genetic and Evolution-
ary Computation Conference: Late Breaking Papers, pages
2389–2396. ACM, 2009. 4

with covariance matrix adaptation (cma-es). Evolutionary
computation, 11(1):1–18, 2003. 4

[15] B.-S. Hua, Q.-H. Pham, D. T. Nguyen, M.-K. Tran, L.-F. Yu,
and S.-K. Yeung. Scenenn: A scene meshes dataset with
annotations. In 3D Vision (3DV), 2016 Fourth International
Conference on, pages 92–101. IEEE, 2016. 2

[16] B.-S. Hua, Q.-T. Truong, M.-K. Tran, Q.-H. Pham,
A. Kanezaki, T. Lee, H. Chiang, W. Hsu, B. Li, Y. Lu, et al.
Shrec17: Rgb-d to cad retrieval with objectnn dataset. 2

[17] S. Huang, S. Qi, Y. Zhu, Y. Xiao, Y. Xu, and S.-C. Zhu.
Holistic 3D scene parsing and reconstruction from a single
RGB image. In European Conference on Computer Vision,
pages 194–211. Springer, 2018. 2

[18] S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. Newcombe,
P. Kohli, J. Shotton, S. Hodges, D. Freeman, A. Davison,
et al. Kinectfusion: real-time 3d reconstruction and inter-
action using a moving depth camera. In Proceedings of the
24th annual ACM symposium on User interface software and
technology, pages 559–568. ACM, 2011. 1, 2

[19] H. Izadinia, Q. Shan, and S. M. Seitz. Im2cad. In Computer
Vision and Pattern Recognition (CVPR), 2017 IEEE Confer-
ence on, pages 2422–2431. IEEE, 2017. 2

[20] A. E. Johnson. Spin-images: a representation for 3-d surface

matching. 1997. 2

[21] O. K¨ahler, V. A. Prisacariu, C. Y. Ren, X. Sun, P. Torr, and
D. Murray. Very high frame rate volumetric integration of
depth images on mobile devices. IEEE transactions on visu-
alization and computer graphics, 21(11):1241–1250, 2015.
1, 2

[22] M. Keller, D. Leﬂoch, M. Lambers, S. Izadi, T. Weyrich,
and A. Kolb. Real-time 3d reconstruction in dynamic scenes
using point-based fusion. In 3D Vision-3DV 2013, 2013 In-
ternational Conference on, pages 1–8. IEEE, 2013. 2

[23] Y. M. Kim, N. J. Mitra, Q. Huang, and L. Guibas. Guided
real-time scanning of indoor objects. In Computer Graphics
Forum, volume 32, pages 177–186. Wiley Online Library,
2013. 2

[24] Y. M. Kim, N. J. Mitra, D.-M. Yan, and L. Guibas. Acquiring
3D indoor environments with variability and repetition. ACM
Transactions on Graphics (TOG), 31(6):138, 2012. 2

[25] Y. Li, A. Dai, L. Guibas, and M. Nießner. Database-assisted
object retrieval for real-time 3D reconstruction. In Computer
Graphics Forum, volume 34, pages 435–446. Wiley Online
Library, 2015. 2, 6, 7, 8

[26] J. J. Lim, H. Pirsiavash, and A. Torralba. Parsing ikea ob-
jects: Fine pose estimation.
In Proceedings of the IEEE
International Conference on Computer Vision, pages 2992–
2999, 2013. 2, 3

[27] J. McCormac, R. Clark, M. Bloesch, A. Davison, and
S. Leutenegger. Fusion++: Volumetric object-level slam. In
2018 International Conference on 3D Vision (3DV), pages
32–41. IEEE, 2018. 2

[28] L. Nan, K. Xie, and A. Sharf. A search-classify approach for
cluttered indoor scene understanding. ACM Transactions on
Graphics (TOG), 31(6):137, 2012. 2

[14] N. Hansen, S. D. M¨uller, and P. Koumoutsakos. Reducing
the time complexity of the derandomized evolution strategy

[29] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and

2622

[43] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and
T. Funkhouser. 3dmatch: Learning local geometric descrip-
tors from rgb-d reconstructions. In Computer Vision and Pat-
tern Recognition (CVPR), 2017 IEEE Conference on, pages
199–208. IEEE, 2017. 2, 7, 8

[44] X. Zhou, A. Karpur, C. Gan, L. Luo, and Q. Huang. Un-
supervised domain adaptation for 3d keypoint estimation via
view consistency. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pages 137–153, 2018. 2

[45] C. Zou, R. Guo, Z. Li, and D. Hoiem. Complete 3D scene
International Journal of

parsing from an RGBD image.
Computer Vision (IJCV), 2018. 2

A. Fitzgibbon. Kinectfusion: Real-time dense surface map-
ping and tracking. In Mixed and augmented reality (ISMAR),
2011 10th IEEE international symposium on, pages 127–
136. IEEE, 2011. 1, 2

[30] M. Nießner, M. Zollh¨ofer, S. Izadi, and M. Stamminger.
Real-time 3d reconstruction at scale using voxel hashing.
ACM Transactions on Graphics (TOG), 2013. 1, 2

[31] N. Otsu. A threshold selection method from gray-level his-
tograms. IEEE transactions on systems, man, and cybernet-
ics, 9(1):62–66, 1979. 6

[32] Q.-H. Pham, M.-K. Tran, W. Li, S. Xiang, H. Zhou, W. Nie,
A. Liu, Y. Su, M.-T. Tran, N.-M. Bui, et al. Shrec18: Rgb-d
object-to-cad retrieval. 2

[33] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature
histograms (fpfh) for 3d registration.
In Robotics and Au-
tomation, 2009. ICRA’09. IEEE International Conference
on, pages 3212–3217. Citeseer, 2009. 2, 7, 8

[34] R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H.
Kelly, and A. J. Davison. Slam++: Simultaneous localisa-
tion and mapping at the level of objects. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, pages 1352–1359, 2013. 2

[35] S. Salti, F. Tombari, and L. Di Stefano. Shot: Unique signa-
tures of histograms for surface and texture description. Com-
puter Vision and Image Understanding, 125:251–264, 2014.
2

[36] T. Shao, W. Xu, K. Zhou, J. Wang, D. Li, and B. Guo. An
interactive approach to semantic modeling of indoor scenes
with an RGBD camera. ACM Transactions on Graphics
(TOG), 31(6):136, 2012. 2

[37] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and
T. Funkhouser. Semantic scene completion from a single
depth image. Proceedings of 30th IEEE Conference on Com-
puter Vision and Pattern Recognition, 2017. 1

[38] X. Sun, J. Wu, X. Zhang, Z. Zhang, C. Zhang, T. Xue, J. B.
Tenenbaum, and W. T. Freeman. Pix3D: Dataset and meth-
ods for single-image 3D shape modeling.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2974–2983, 2018. 2, 3

[39] F. Tombari, S. Salti, and L. Di Stefano. Unique signatures
of histograms for local surface description.
In K. Dani-
ilidis, P. Maragos, and N. Paragios, editors, Computer Vision
– ECCV 2010, pages 356–369, Berlin, Heidelberg, 2010.
Springer Berlin Heidelberg. 7, 8

[40] T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker,
and A. J. Davison. Elasticfusion: Dense slam without a pose
graph. Proc. Robotics: Science and Systems, Rome, Italy,
2015. 1, 2

[41] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mot-
taghi, L. Guibas, and S. Savarese. Objectnet3d: A large scale
database for 3D object recognition. In European Conference
on Computer Vision, pages 160–176. Springer, 2016. 3

[42] Y. Xiang, R. Mottaghi, and S. Savarese. Beyond pascal: A
benchmark for 3d object detection in the wild. In Applica-
tions of Computer Vision (WACV), 2014 IEEE Winter Con-
ference on, pages 75–82. IEEE, 2014. 3

2623

