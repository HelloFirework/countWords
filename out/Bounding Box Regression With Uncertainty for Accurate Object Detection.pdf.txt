Bounding Box Regression with Uncertainty for Accurate Object Detection

Yihui He1

Chenchen Zhu1
1Carnegie Mellon University

Jianren Wang1 Marios Savvides1

Xiangyu Zhang2

2Megvii Inc. (Face++)

{he2,chenchez,jianrenw,marioss}@andrew.cmu.edu

zhangxiangyu@megvii.com

(a)

(b)

(c)

(d)

Figure 1: In object detection datasets, the ground-truth bounding boxes have inherent ambiguities in some cases. The
bounding box regressor is expected to get smaller loss from ambiguous bounding boxes with our KL Loss.
(a)(c) The
ambiguities introduced by inaccurate labeling. (b) The ambiguities introduced by occlusion. (d) The object boundary itself is
ambiguous. It is unclear where the left boundary of the train is because the tree partially occludes it. (better viewed in color)

Abstract

Large-scale object detection datasets (e.g., MS-COCO)
try to deﬁne the ground truth bounding boxes as clear as
possible. However, we observe that ambiguities are still in-
troduced when labeling the bounding boxes. In this paper,
we propose a novel bounding box regression loss for learn-
ing bounding box transformation and localization variance
together. Our loss greatly improves the localization accura-
cies of various architectures with nearly no additional com-
putation. The learned localization variance allows us to
merge neighboring bounding boxes during non-maximum
suppression (NMS), which further improves the localization
performance. On MS-COCO, we boost the Average Preci-
sion (AP) of VGG-16 Faster R-CNN from 23.6% to 29.1%.
More importantly, for ResNet-50-FPN Mask R-CNN, our
method improves the AP and AP90 by 1.8% and 6.2% re-
spectively, which signiﬁcantly outperforms previous state-
of-the-art bounding box reﬁnement methods. Our code and
models are available at github.com/yihui-he/KL-Loss

1. Introduction

Large scale object detection datasets like ImageNet [6],
MS-COCO [35] and CrowdHuman [47] try to deﬁne the

ground truth bounding boxes as clear as possible.

However, we observe that the ground-truth bounding
boxes are inherently ambiguous in some cases. The ambi-
guities makes it hard to label and hard to learn the bounding
box regression function. Some inaccurately labeled bound-
ing boxes from MS-COCO are shown in Figure 1 (a)(c).
When the object is partially occluded, the bounding box
boundaries are even more unclear, shown in Figure 1 (d)
from YouTube-BoundingBoxes [40].

Object detection is a multi-task learning problem con-
sisting of object
localization and object classiﬁcation.
Current state-of-the-art object detectors (e.g., Faster R-
CNN [42], Cascade R-CNN [2] and Mask R-CNN [17]) rely
on bounding box regression to localize objects.

However, the traditional bounding box regression loss
(i.e., the smooth L1 loss [13]) does not take such the am-
biguities of the ground truth bounding boxes into account.
Besides, bounding box regression is assumed to be accurate
when the classiﬁcation score is high, which is not always
the case, illustrated in Figure 2.

To address these problems, we propose a novel bounding
box regression loss, namely KL Loss, for learning bound-
ing box regression and localization uncertainty at the same
time. Speciﬁcally, to capture the uncertainties of bound-
ing box prediction, we ﬁrst model the bounding box pre-

2888

1.00

0.35

0.32

0.96

(a)

(b)

Figure 2: Illustration of failure cases of VGG-16 Faster R-
CNN on MS-COCO. (a) both candidate boxes are inaccu-
rate in a certain coordinate.
(b) the left boundary of the
bounding box which has the higher classiﬁcation score is
inaccurate. (better viewed in color)

diction and ground-truth bounding box as Gaussian distri-
bution and Dirac delta function respectively. Then the new
bounding box regression loss is deﬁned as the KL diver-
gence of the predicted distribution and ground-truth distri-
bution. Learning with KL Loss has three beneﬁts: (1) The
ambiguities in a dataset can be successfully captured. The
bounding box regressor gets smaller loss from ambiguous
bounding boxes. (2) The learned variance is useful during
post-processing. We propose var voting (variance voting)
to vote the location of a candidate box using its neighbors’
locations weighted by the predicted variances during non-
maximum suppression (NMS). (3) The learned probability
distribution is interpretable. Since it reﬂects the level of un-
certainty of the bounding box prediction, it can potentially
be helpful in down-stream applications like self-driving cars
and robotics [7, 16, 21].

To demonstrate the generality of KL Loss and var voting,
we evaluate various CNN-based object detectors on both
PASCAL VOC 2007 and MS-COCO including VGG-CNN-
M-1024, VGG-16, ResNet-50-FPN, and Mask R-CNN. Our
experiments suggest that our approach offers better object
localization accuracy for CNN-based object detectors. For
VGG-16 Faster R-CNN on MS-COCO, we improve the AP
from 23.6% to 29.1%, with only 2ms increased inference
latency on the GPU (GTX 1080 Ti). Furthermore, we apply
this pipeline to ResNet-50-FPN Mask R-CNN and improve
the AP and AP90 by 1.8% and 6.2% respectively, which
outperforms the previous state-of-the-art bounding box re-
ﬁnement algorithm [27].

2. Related Work

Two-stage Detectors: Although one-stage detection al-
gorithms [36, 41, 30, 51] are efﬁcient, state-of-the-art object
detectors are based on two-stage, proposal-driven mecha-
nism [42, 4, 5, 17, 31, 2]. Two-stage detectors generate
cluttered object proposals, which result in a large number
of duplicate bounding boxes. However, during the standard
NMS procedure, bounding boxes with lower classiﬁcation
scores will be discarded even if their locations are accurate.
Our var voting tries to utilize neighboring bounding boxes
based on localization conﬁdence for better localization of
the selected boxes.

Object Detection Loss: To better learn object detection,
different kind of losses have been proposed. UnitBox [49]
introduced an Intersection over Union (IoU) loss function
for bounding box prediction. Focal Loss [34] deals with
the class imbalance by changing the standard cross entropy
loss such that well-classiﬁed examples are assigned lower
[39] optimizes for the mAP via policy gradient
weights.
for learning globally optimized object detector. [28] intro-
duces uncertainties for depth estimation. The idea is further
extended to the 3D object detection [10, 9]. [29] proposes
to weight multi-task loss for scene understanding by con-
sidering the uncertainty of each task. With KL Loss, our
model can adaptively adjust variances for the boundaries of
every object during training, which can help to learn more
discriminative features.

Non-Maximum Suppression: NMS has been an essen-
tial part of computer vision for many decades. It is widely
used in edge detection [44], feature point detection [37] and
objection detection [13, 12, 42, 45]. Recently, soft NMS
and learning NMS [1, 24] are proposed to improve NMS
results. Instead of eliminating all lower scored surrounding
bounding boxes, soft-NMS [1] decays the detection scores
of all other neighbors as a continuous function of their
overlap with the higher scored bounding box. Learning
NMS [24] proposed to learn a new neural network to per-
form NMS using only boxes and their classiﬁcation scores.

Bounding Box Reﬁnement: MR-CNN [11] is ﬁrst pro-
posed to merge boxes during iterative localization. Relation
network [25] proposes to learn the relation between bound-
ing boxes. Recently, IoU-Net [27] proposes to learn the IoU
between the predicted bounding box and the ground-truth
bounding box. IoU-NMS is then applied to the detection
boxes, guided by the learned IoU. Different from IoU-Net,
we propose to learn the localization variance from a proba-
bilistic perspective. It enables us to learn the variances for
the four coordinates of a predicted bounding box separately

2889

instead of only IoU. Our var voting determine the new loca-
tion of a selected box based on the variances of neighboring
bounding boxes learned by KL Loss, which can work to-
gether with soft-NMS (Table 1 and Table 6).

3. Approach

In this section, we ﬁrst introduce our bounding box pa-
rameterization. Then we propose KL Loss for training de-
tection network with localization conﬁdence. Finally, a new
NMS approach is introduced for improving localization ac-
curacy with our conﬁdence estimation.

3.1. Bounding Box Parameterization

Based on a two-stage object detector Faster R-CNN or
Mask R-CNN [42, 17] shown in Figure 3, we propose to
regress the boundaries of a bounding box separately. Let

(x1, y1, x2, y2) ∈ R4 be the bounding box representation

as a 4-dimensional vector, where each dimension is the box
boundary location. We adopt the parameterizations of the
(x1, y1, x2, y2) coordinates instead of the (x, y, w, h) coor-
dinates used by R-CNN [13]:
x1 − x1a
y1 − y1a

x2 − x2a
wa
y2 − y2a

, tx2 =

tx1 =

wa

, ty2 =

ty1 =

ha
x∗
1 − x1a
wa
y∗
1 − y1a
ha

, t∗

x2 =

, t∗

y2 =

ha
x∗
2 − x2a
wa

y∗
2 − y2a
ha

x1 =
t∗

t∗
y1 =

, t∗
y2

, t∗
where tx1 , ty1 , tx2 , ty2 are the predicted offsets. t∗
,
y1
x1
t∗
are the ground-truth offsets. x1a, x2a, y1a, y2a,
x2
wa, ha are from the anchor box. x1, y1, x2, y2 are from
the predicted box. In the following discussions, a bounding
box coordinate is denoted as x for simplicity because we
can optimize each coordinate independently.

We aim to estimate the localization conﬁdence along
with the location. Formally, our network predicts a prob-
ability distribution instead of only bounding box location.
Though the distribution could be more complex ones like
multivariate Gaussian or a mixture of Gaussians, in this pa-
per we assume the coordinates are independent and use sin-
gle variate gaussian for simplicity:

PΘ(x) =

1

√2πσ2

(x−xe)2

2σ2

e−

(2)

where Θ is the set of learnable parameters. xe is the esti-
mated bounding box location. Standard deviation σ mea-
sures uncertainty of the estimation. When σ → 0, it means
our network is extremely conﬁdent about estimated bound-
ing box location. It is produced by a fully-connected layer
on top of the fast R-CNN head (fc7). Figure 3 illustrates

(1)

where xg is the ground-truth bounding box location.

RoI

fc6

fc7

Lcls

Class

Box

Box std

Lreg
(KL loss)

Figure 3: Our network architecture for estimating local-
ization conﬁdence. Different from standard fast R-CNN
head of a two stage detection network, our network esit-
mates standard deviations along with bounding box loca-
tions, which are taken into account in our regression loss
KL Loss

the fast R-CNN head of our network architecture for object
detection.

The ground-truth bounding box can also be formulated
as a Gaussian distribution, with σ → 0, which is a Dirac
delta function:

PD(x) = δ(x − xg)

(3)

3.2. Bounding Box Regression with KL Loss

The goal of object localization in our context is to esti-
mate ˆΘ that minimize the KL-Divergence between PΘ(x)
and PD(x) [43] over N samples:

ˆΘ = arg min

Θ

1

N X DKL(PD(x)||PΘ(x))

(4)

We use the KL-Divergence as the loss function Lreg for
bounding box regression. The classiﬁcation loss Lcls re-
mains the same. For a single sample:

Lreg = DKL(PD(x)||PΘ(x))

+

=

2σ2

log(2π)

log(σ2)

(xg − xe)2

= Z PD(x) log PD(x)dx − Z PD(x) log PΘ(x)dx
− H(PD(x))
(5)
Shown in Figure 4, when the location xe is estimated inac-
curately, we expect the network to be able to predict larger
variance σ2 so that Lreg will be lower.
log(2π)/2 and
H(PD(x)) do not depend on the estimated parameters Θ,
hence:

+

2

2

Lreg ∝

(xg − xe)2

2σ2

+

1
2

log(σ2)

(6)

2890

𝛿(𝑥 − 𝑥+)

Shown in Algorithm 1, we change NMS with three lines of
code:

𝑁(𝑥$, 𝜎2)

Figure 4: The Gaussian distributions in blue and gray are
our estimations. The Dirac delta function in orange is the
distribution of the ground-truth bounding box. When the
location xe is estimated inaccurately, we expect the network
to be able to predict larger variance σ2 so that Lreg will be
lower (blue)

Algorithm 1 var voting
B is N × 4 matrix of initial detection boxes. S contains
corresponding detection scores. C is N × 4 matrix of cor-
responding variances. D is the ﬁnal set of detections. σt is
a tunable parameter of var voting. The lines in blue and in
green are soft-NMS and var voting respectively.

B = {b1, .., bN}, S = {s1, .., sN}, C = {σ2
D ← {}
T ← B
while T 6= empty do
m ← argmax S
T ← T − bm
S ← Sf (IoU (bm, T ))
idx ← IoU (bm, B) > 0
p ← exp(−(1 − IoU (bm,B[idx]))2/σt)
bm ← p(B[idx]/C[idx])/p(1/C[idx])
D ← DS bm

1, .., σ2
N}

⊲ soft-NMS
⊲ var voting

When σ = 1, KL Loss degenerates to the standard Eu-
clidean loss:

Lreg ∝

(xg − xe)2

2

(7)

end while
return D,S

The loss is differentiable w.r.t location estimation xe and
localization standard deviation σ:
xe − xg
σ2
(xe − xg)2

Lreg =

(8)

Lreg = −

σ−3

d
dxe
d
dσ

1
σ

−

However, since σ is in the denominators, the gradient some-
times can explode at the beginning of training. To avoid gra-
dient exploding, our network predicts α = log(σ2) instead
of σ in practice:

(xg − xe)2 +
We convert α back to σ during testing.

Lreg ∝

e−α
2

1
2

α

(9)

For |xg − xe| > 1, we adopt a loss similar to the smooth

L1 loss deﬁned in Fast R-CNN [12]:

Lreg = e−α(|xg − xe| −

1
2

) +

1
2

α

(10)

We initialize the weights of the FC layer for α prediction
with random Gaussian initialization. The standard deviation
and mean are set to 0.0001 and 0 respectively, so that KL
Loss will be similar to the standard smooth L1 loss at the
beginning of training. (Equation 9 and Equation 10).

3.3. Variance Voting

After we obtain the variance of predicted location, it is
intuitive to vote candidate bounding box location accord-
ing to the learned variances of neighboring bounding boxes.

We vote the location of

selected boxes within
Af-
the loop of standard NMS or soft-NMS [1].
selecting the detection with maximum score b,
ter
{x1, y1, x2, y2, s, σx1 , σy1 , σx2 , σy2},
its new location is
computed according to itself and its neighboring bound-
ing boxes. Inspired by soft-NMS, we assign higher weights
for boxes that are closer and have lower undertainties. For-
mally, let x be a coordinate (e.g., x1) and xi be the coordi-
nate of ith box. The new coordinate is computed as follow:

pi = e−(1−IoU (bi,b))2/σt
x = Pi pixi/σ2
Pi pi/σ2

x,i

x,i

subject to IoU (bi, b) > 0

(11)

σt is a tunable parameter of var voting. Two types of neigh-
boring bounding boxes will get lower weights during vot-
ing: (1) Boxes with high variances.
(2) Boxes that have
small IoU with the selected box. Classiﬁcation score is not
involved in the voting, since lower scored boxes may have
higher localization conﬁdence. In Figure 5, we provide a
visual illustration of var voting. With var voting, the two
situations as mentioned earlier in Figure 2 that lead to de-
tection failure can sometimes be avoided.

4. Experiments

To demonstrate our method for accurate object detection,
we use two datasets: MS-COCO [35] and PASCAL VOC

2891

1.08

Person 0.42

Person 1.00

0.46

0.28

Train 0.99

1.06

0.53

0.71
0.65

0.81

1.01

0.21

0.19

0.19

0.62

0.28
0.44

1.06

0.56

0.21

0.66

acquire variances with KL Loss

0.46

Person 
0.42

0.37

Person 0.36

Person 0.93

0.25

0.42

0.48

Airplane 1.00

Train 0.35

0.30

0.30

1.25

1.17

0.31

0.27

0.36

0.30

Airplane 0.30

0.33

0.32

0.42

0.36

var voting

0.19

0.18

0.37

Person 0.36

Person 0.93

0.25

1.08

Person 1.00

0.46

Person 0.42

0.28

1.06

0.81

1.01

0.21

0.19

0.62

0.44

Train 0.99

Train 0.35

0.53

0.65

0.66

0.56

0.30

0.30

Airplane 1.00

0.48

0.42

Airplane 0.30

1.25

1.17

0.31

0.27

0.36

0.30

0.33

0.32

0.19

0.18

0.36

0.42

(a)                                                    (b) 

(c)                                                  (d)

Figure 5: Results of var voting with VGG-16 Faster R-CNN on MS-COCO. The green textbox in the middle of each boundary
is the corresponding standard deviation σ we predicted (Equation 2). Two failure situations corresponding to Figure 2 that can
be improved by var voting: (a) When each candidate bounding box is inaccurate in some coordinates (women on the right),
our var voting can incorporate their localization conﬁdence and produce better boxes. (b) The bounding box with a higher
classiﬁcation score (train 0.99) actually has lower localization conﬁdence than the bounding box with a lower classiﬁcation
score (train 0.35). After var voting, the box scored 0.99 moves towards the correct location. (better viewed in color)

2007 [8]. We use four GPUs for our experiments. The train-
ing schedule and batch size are adjusted according to the
linear scaling rule [15]. For VGG-CNN-M-1024 and VGG-
16 Net [48], our implementation is based on Caffe [26]. For
ResNet-50 FPN [18, 33] and Mask R-CNN [17], our im-
plementation is based on Detectron [14]. For VGG-16 [48]
Faster R-CNN, following py-faster-rcnn1, we train
on train2014 and test on val2014. For other object
detection networks, we train and test on the newly deﬁned
train2017 and val2017 respectively. We set σt to
0.02. Unless speciﬁed, all hyper-parameters are set to de-
fault2.

4.1. Ablation Study

We evaluate the contribution of each element in our de-
tection pipeline: KL Loss, soft-NMS and var voting with
VGG-16 Faster R-CNN. The detailed results are shown in

1github.com/rbgirshick/py-faster-rcnn
2github.com/facebookresearch/Detectron

Table 1.

KL Loss: Surprisingly, simply training with KL Loss
greatly improves the AP by 2.8%, which is also observed
on ResNet-50 Faster R-CNN and Mask R-CNN (1.5% and
0.9% improvement respectively, shown in Table 3 and Ta-
ble 4). First, by learning to predict high variances for sam-
ples with high uncertainties during training, the network
can learn more from useful samples. Second, the gradient
for localization can be adaptively controlled by the network
during training (Equation 8), which encourages the network
to learn more accurate object localization. Third, KL Loss
incorporates learning localization conﬁdence which can po-
tentially help the network to learn more discriminative fea-
tures.

The learned variances through our KL Loss are inter-
pretable. Our network will output higher variances for chal-
lenging object boundaries, which can be useful in vision ap-
plications like self-driving cars and robotics. The ﬁrst row

2892

KL Loss

soft-NMS

var voting

AP

AP50 AP75 APS APM APL AR1 AR10 AR100

X

X

X

X

X

X

X

23.6
24.8
26.4
27.8
27.8
29.1

44.6
45.6
47.9
48.0
49.0
49.1

22.8
24.6
26.4
28.9
28.5
30.4

6.7
7.6
7.4
8.1
8.4
8.7

25.9
27.2
29.3
31.4
30.9
32.7

36.3
37.6
41.2
42.6
42.7
44.3

23.3
23.4
25.2
26.2
25.3
26.2

33.6
39.2
36.1
37.5
41.7
42.5

34.3
42.2
36.9
38.3
44.9
45.5

X

X

Table 1: The contribution of each element in our detection pipeline on MS-COCO. The baseline model is VGG-16 Faster
R-CNN

method
baseline

ours

latency (ms)

99
101

Table 2: Inference time comparison on MS-COCO with
VGG-16 Faster R-CNN on a GTX 1080 Ti GPU, CUDA
8 [38] and CUDNN 6 [3]

fast R-CNN head

backbone KL Loss

AP

2mlp head

FPN

2mlp head + mask

FPN

conv5 head

RPN

X

X

X

37.9
38.5+0.6
38.6
39.5+0.9
36.5
38.0+1.5

Table 3: Comparison of different fast R-CNN heads. The
model is ResNet-50 Faster R-CNN

of Figure 5 shows some qualitative examples of the standard
deviation learned through our KL Loss.

Soft-NMS: As expected, soft-NMS performs consistently
on both baseline and our network trained with KL Loss. It
improves the AP by 1.2% and 1.4% on the baseline and our
network respectively, shown in Table 1.

Variance Voting: Finally, with var voting, the AP is fur-
ther improved to 29.1%. We made the observation that im-
provement mainly comes from the more accurate localiza-
tion. Notice that the AP50 is only improved by 0.1%. How-
ever, AP75, APM and APL are improved by 1.8%, 1.8%,
and 1.6% respectively, shown in Table 1. This indicates that
classiﬁcation conﬁdence is not always associated with lo-
calization conﬁdence. Therefore, learning localization con-
ﬁdence apart from classiﬁcation conﬁdence is important for
more accurate object localization.

Figure 6: Varying σt for var voting with ResNet-50 Faster
R-CNN. (better viewed in color)

We also found that var voting and soft-NMS can work
well together. Applying var voting with the standard NMS
improves the AP by 1.4%. Applying var voting after soft-
NMS still can improve the AP by 1.3%. We argue that soft-
NMS is good at scoring candidate bounding boxes which
improve overall performance, whereas var voting is good
at reﬁning those selected bounding boxes for more accurate
object localization. The second row of Figure 5 shows some
qualitative examples of our var voting.

Shown in Figure 6, we test the sensitivity of the tunable
parameter σt for var voting. When σt = 0, var voting is not
activated. We observe that the AP75, AP80 and AP90 can
be signiﬁcantly affected by σt, while AP50 is less sensitive
to σt. Acceptable values of σt varies from around 0.005 ∼
0.05. We use σt = 0.02 in all experiments.

Inference Latency: We also evaluate the inference time
of our improved VGG-16 Faster R-CNN on a single GTX
1080 Ti GPU with CUDA 8 and CUDNN 6, as it is cru-
cial for resource-limited applications [50, 20, 23, 19, 32].

2893

0.00.0050.010.0250.050.1t1.00.50.00.51.0 AP (%)APAP50AP75AP80AP90baseline [14]
MR-CNN [11]
soft-NMS [1]
IoU-NMS+Reﬁne [27]
KL Loss
KL Loss+var voting
KL Loss+var voting+soft-NMS

AP

38.6
38.9
39.3
39.2
39.5+0.9
39.9+1.3
40.4+1.8

AP50 AP60 AP70
59.8
47.7
59.8
48.1
48.9
59.7
57.9
47.4
47.6
58.9
47.7
58.9
58.7
48.5

55.3
55.5
55.6
53.6
54.4
54.4
54.6

AP80

34.4
34.8+0.4
35.9+1.5
36.5+2.1
36.0+1.6
36.4+2.0
37.5+3.3

AP90

11.3
11.9+0.6
12.0+0.7
16.4+5.1
15.8+4.5
17.0+5.7
17.5+6.2

Table 4: Comparisons of different methods for accurate object detection on MS-COCO. The baseline model is ResNet-50-
FPN Mask R-CNN. We improve the baseline by ≈ 2% in AP

Shown in Table 2, our approach only increases 2ms latency
on GPU. Different from IoUNet [27] which uses 2mlp head
for IoU prediction, our approach only requires a 4096× 324
fully-connected layer for the localization conﬁdence predic-
tion.

RoI Box Head: We test the effectiveness of KL Loss with
different RoI box heads on a deeper backbone: ResNet-
50. res5/conv5 head consists of 9 convolutional lay-
ers which can be applied to each RoI as fast R-CNN head.
2mlp head consists of two fully connected layers. res5
head can learn more complex representation than the com-
monly used 2mlp head. Shown in Table 3, KL Loss can
improve the AP by 0.9% with mask. KL Loss can further
improve the AP by 1.5% with conv5 head. We hypoth-
esize that the localization variance is much more challeng-
ing to learn than localization, therefore KL Loss can ben-
eﬁt more from the expressiveness of conv5 head. Since
conv5 head is not commonly used in recent state-of-the-
art detectors, we still adopt the 2mlp head in the following
experiments.

4.2. Accurate Object Detection

Table 4 summarizes the performance of different meth-
ods for accurate object detection on ResNet-50-FPN Mask
R-CNN. With KL Loss, the network can learn to adjust the
gradient for ambiguous bounding boxes during training. As
a result, Mask R-CNN trained with KL Loss performs sig-
niﬁcantly better than the baseline for high overlap metrics
like AP90. Variance Voting improves the localization results
by voting the location according to the localization conﬁ-
dences of neighboring bounding boxes. AP80 and AP90 are
further improved by 0.4% and 1.2% respectively. Variance
Voting is also compatible with soft-NMS. Variance Voting
combined with soft-NMS improves the AP90 and the over-
all AP of the ﬁnal model by 6.2% and 1.8% respectively.
Compared with IoUNet [27]: (1) our variance and localiza-
tion are learned together with KL Loss, which improves the
performance. (2) KL Loss does not require a separate 2mlp

backbone

method

baseline
VGG-CNN- KL Loss

M-1024

VGG-16

KL Loss+var voting
KL Loss+var voting+soft-NMS
baseline
QUBO (tabu) [46]
QUBO (greedy) [46]
soft-NMS [1]
KL Loss
KL Loss+var voting
KL Loss+var voting+soft-NMS

mAP
60.4
62.0
62.8
63.6
68.7
60.6
61.9
70.1
69.7
70.2
71.6

Table 5: Comparisons of different approaches on PASCAL
VOC 2007 with Faster R-CNN.

head for learning localization conﬁdence, which introduces
nearly no additional computation. (3) var voting does not
require iterative reﬁnement, which is much faster.

We further evaluate our approach on the feature pyramid

network (ResNet-50 FPN) [33, 18], shown in Table 6.

For fast R-CNN version, training with KL Loss increases
the baseline by 0.4%. After applying var voting along with
soft-NMS, our model achieves 38.0% in AP, which outper-
forms both IoU-NMS and soft-NMS baselines. Training
end-to-end with KL Loss can help the network learn more
discriminative features, which improves the baseline AP by
0.6%. The ﬁnal model achieves 39.2% in AP, which im-
proves the baseline by 1.3%.

4.3. Experiments on PASCAL VOC 2007

Even though our approach is designed for

large
scale object detection,
it could also generalize well on
small datasets. We perform experiments with Faster
R-CNN [42] on PASCAL VOC 2007, which consists
of about 5k voc_2007_trainval images and 5k
voc_2007_test images over 20 object categories.
Backbone networks: VGG-CNN-M-1024 and VGG-16

2894

type

method

fast R-CNN

Faster R-CNN

baseline (1x schedule) [14]
baseline (2x schedule) [14]
IoU-NMS [27]
soft-NMS [1]
KL Loss
KL Loss+var voting
KL Loss+var voting+soft-NMS

baseline (1x schedule) [14]
IoU-Net [27]
IoU-Net+IoU-NMS [27]
baseline (2x schedule) [14]
IoU-Net+IoU-NMS+Reﬁne [27]
soft-NMS[1]
KL Loss
KL Loss+var voting
KL Loss+var voting+soft-NMS

AP
36.4
36.8
37.3
37.4
37.2
37.5
38.0

36.7
37.0
37.6
37.9
38.1
38.6
38.5
38.8
39.2

39.3
39.5

39.8
39.5

20.3
19.8

AP50 AP75 APS APM APL
58.4
48.1
58.4
49.5
56.0
58.2
57.2
56.5
56.4

20.3
19.8
19.4
19.8

41.0
39.9
40.1
41.2

40.2
39.7
40.2
40.6

50.1
50.1
51.6
52.3

-

-

-

-

58.4
58.3
56.2
59.2
56.3
59.3
57.8
57.8
57.6

39.6

21.1

39.8

48.1

-
-

-
-

-
-

-
-

41.1

21.5

41.1

49.9

-

42.4
41.2
41.6
42.5

-

21.9
20.9
21.0
21.2

-

41.9
41.2
41.5
41.8

-

50.7
51.5
52.0
52.5

Table 6: Performance comparison with FPN ResNet-50 on MS-COCO

Net [48] are tested.

Acknowledgement

Shown in Table 5, we compare our approach with
soft-NMS and quadratic unconstrained binary optimization
(QUBO [46]). For QUBO, we test both greedy and clas-
sical tabu solver (we manually tuned the penalty term for
both solvers to get better performance). We observe that it
is much worse than the standard NMS, though it was re-
ported to be better for pedestrian detection. We hypothesize
that QUBO is better at pedestrian detection since there are
more occluded bounding boxes [47]. For VGG-CNN-M-
1024, training with var voting improves the mAP by 1.6%.
var voting further improves the mAP by 0.8%. For VGG-
16, our approach improves the mAP by 2.9%, combined
with soft-NMS. We notice that var voting could still im-
prove performance even after soft-NMS is applied to the
initial detection boxes. This observation is consistent with
our experiments on MS-COCO (Table 1).

5. Conclusion

To conclude, the uncertainties in large-scale object de-
tection datasets can hinder the performance of state-of-the-
art object detectors. Classiﬁcation conﬁdence is not always
strongly related to localization conﬁdence. In this paper, a
novel bounding box regression loss with uncertainty is pro-
posed for learning more accurate object localization. By
training with KL Loss, the network learns to predict local-
ization variance for each coordinate. The resulting vari-
ances empower var voting, which can reﬁne the selected
bounding boxes via voting. Compelling results are demon-
strated for VGG-16 Faster R-CNN, ResNet-50 FPN and
Mask R-CNN on both MS-COCO and PASCAL VOC 2007.

This research was partially supported by National Key

R&D Program of China (No. 2017YFA0700800).

We would love to express our appreciation to Prof. Kris
Kitani and Dr. Jian Sun for the useful discussions during
this research.

References

[1] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and
Larry S Davis. Soft-nms – improving object detection with
one line of code.
In Computer Vision (ICCV), 2017 IEEE
International Conference on, pages 5562–5570. IEEE, 2017.
2, 4, 7, 8

[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-
arXiv preprint

ing into high quality object detection.
arXiv:1712.00726, 2017. 1, 2

[3] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch,
Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan
Shelhamer. cudnn: Efﬁcient primitives for deep learning.
arXiv preprint arXiv:1410.0759, 2014. 6

[4] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object
detection via region-based fully convolutional networks. In
Advances in neural information processing systems, pages
379–387, 2016. 2

[5] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. CoRR, abs/1703.06211, 1(2):3, 2017. 2

[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei.
Imagenet: A large-scale hierarchical im-
age database. In Computer Vision and Pattern Recognition,
2009. CVPR 2009. IEEE Conference on, pages 248–255.
IEEE, 2009. 1

2895

[7] Nemanja Djuric, Vladan Radosavljevic, Henggang Cui, Thi
Nguyen, Fang-Chieh Chou, Tsung-Han Lin, and Jeff Schnei-
der. Motion prediction of trafﬁc actors for autonomous
driving using deep convolutional networks. arXiv preprint
arXiv:1808.05819, 2018. 2

[8] M. Everingham, L. Van Gool, C. K. I. Williams, J.
Winn, and A. Zisserman. The PASCAL Visual Object
Classes Challenge 2007 (VOC2007) Results.
pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.
5

[9] Di Feng, Lars Rosenbaum, and Klaus Dietmayer. Towards
safe autonomous driving: Capture uncertainty in the deep
neural network for lidar 3d vehicle detection. In 2018 21st
International Conference on Intelligent Transportation Sys-
tems (ITSC), pages 3266–3273. IEEE, 2018. 2

[10] Di Feng, Lars Rosenbaum, Fabian Timm, and Klaus Diet-
mayer. Leveraging heteroscedastic aleatoric uncertainties
for robust real-time lidar 3d object detection. arXiv preprint
arXiv:1809.05590, 2018. 2

[11] Spyros Gidaris and Nikos Komodakis. Object detection via
a multi-region and semantic segmentation-aware cnn model.
In Proceedings of the IEEE International Conference on
Computer Vision, pages 1134–1142, 2015. 2, 7

[12] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 1440–1448,
2015. 2, 4

[13] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
580–587, 2014. 1, 2, 3

[14] Ross Girshick,

Ilija Radosavovic, Georgia Gkioxari,
Piotr Doll´ar,
Detectron.
and Kaiming He.
github.com/facebookresearch/detectron,
7,
8

2018.

5,

[15] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noord-
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
large mini-
Yangqing Jia, and Kaiming He. Accurate,
batch sgd:
arXiv preprint
arXiv:1706.02677, 2017. 5

training imagenet in 1 hour.

[16] Marcus Gualtieri and Robert Platt. Learning 6-dof grasp-
ing and pick-place using attention focus. arXiv preprint
arXiv:1806.06134, 2018. 2

[17] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE
International Conference on, pages 2980–2988. IEEE, 2017.
1, 2, 3, 5

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016. 5, 7

[19] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
Song Han. Amc: Automl for model compression and ac-
celeration on mobile devices.
In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 784–
800, 2018. 6

[20] Yihui He, Xianggen Liu, Huasong Zhong, and Yuchun Ma.
Addressnet: Shift-based primitives for efﬁcient convolu-
tional neural networks. In 2019 IEEE Winter Conference on
Applications of Computer Vision (WACV), pages 1213–1222.
IEEE, 2019. 6

[21] Yihui He, Xiaobo Ma, Xiapu Luo, Jianfeng Li, Mengchen
Zhao, Bo An, and Xiaohong Guan. Vehicle trafﬁc driven
camera placement for better metropolis security surveillance.
arXiv preprint arXiv:1705.08508, 2017. 2

[22] Yihui He, Xiangyu Zhang, Marios Savvides, and Kris Ki-
tani. Softer-nms: Rethinking bounding box regression for
accurate object detection. arXiv preprint arXiv:1809.08545,
2018.

[23] Yihui He, Xiangyu Zhang, and Jian Sun. Channel prun-
ing for accelerating very deep neural networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1389–1397, 2017. 6

[24] Jan Hendrik Hosang, Rodrigo Benenson, and Bernt Schiele.
Learning non-maximum suppression. In CVPR, pages 6469–
6477, 2017. 2

[25] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen
Wei. Relation networks for object detection. arXiv preprint
arXiv:1711.11575, 8, 2017. 2

[26] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional architecture for fast
feature embedding. In Proceedings of the 22nd ACM inter-
national conference on Multimedia, pages 675–678. ACM,
2014. 5

[27] Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, and Yun-
ing Jiang. Acquisition of localization conﬁdence for accurate
object detection. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 784–799, 2018. 2, 7, 8

[28] Alex Kendall and Yarin Gal. What uncertainties do we need
in bayesian deep learning for computer vision? In Advances
in neural information processing systems, pages 5574–5584,
2017. 2

[29] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geom-
etry and semantics. arXiv preprint arXiv:1705.07115, 3,
2017. 2

[30] Hei Law and Jia Deng. Cornernet: Detecting objects as
paired keypoints. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pages 734–750, 2018. 2

[31] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yang-
dong Deng, and Jian Sun. Light-head r-cnn: In defense of
two-stage object detector. arXiv preprint arXiv:1711.07264,
2017. 2

[32] Yudong Liang, Ze Yang, Kai Zhang, Yihui He, Jinjun Wang,
and Nanning Zheng. Single image super-resolution via a
lightweight residual convolutional neural network. arXiv
preprint arXiv:1703.08173, 2017. 6

[33] Tsung-Yi Lin, Piotr Doll´ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, page 3, 2017. 5, 7

[34] Tsung-Yi Lin, Priyal Goyal, Ross Girshick, Kaiming He, and
IEEE

Piotr Doll´ar. Focal loss for dense object detection.

2896

transactions on pattern analysis and machine intelligence,
2018. 2

work. In Proceedings of the 2016 ACM on Multimedia Con-
ference, pages 516–520. ACM, 2016. 2

[50] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018. 6
[51] Chenchen Zhu, Yihui He, and Marios Savvides. Feature se-
lective anchor-free module for single-shot object detection.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019. 2

[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, Cham, 2014. 1, 4

[36] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In European con-
ference on computer vision, pages 21–37. Springer, 2016. 2
[37] David G Lowe. Distinctive image features from scale-
International journal of computer vi-

invariant keypoints.
sion, 60(2):91–110, 2004. 2

[38] John Nickolls,

Ian Buck, Michael Garland, and Kevin
Skadron. Scalable parallel programming with cuda. In ACM
SIGGRAPH 2008 classes, page 16. ACM, 2008. 6

[39] Yongming Rao, Dahua Lin, Jiwen Lu, and Jie Zhou. Learn-
ing globally optimized object detector via policy gradient.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 6190–6198, 2018. 2

[40] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan,
and Vincent Vanhoucke. Youtube-boundingboxes: A large
high-precision human-annotated data set for object detec-
tion in video.
In Computer Vision and Pattern Recogni-
tion (CVPR), 2017 IEEE Conference on, pages 7464–7473.
IEEE, 2017. 1

[41] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 779–788, 2016. 2

[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, pages 91–99, 2015. 1, 2, 3, 7

[43] Christian Robert. Machine learning, a probabilistic perspec-

tive, 2014. 3

[44] Azriel Rosenfeld and Mark Thurston. Edge and curve detec-
tion for visual scene analysis. IEEE Transactions on com-
puters, (5):562–569, 1971. 2

[45] Rasmus Rothe, Matthieu Guillaumin, and Luc Van Gool.
Non-maximum suppression for object detection by passing
messages between windows. In Asian Conference on Com-
puter Vision, pages 290–306. Springer, 2014. 2

[46] Sitapa Rujikietgumjorn and Robert T Collins. Optimized
pedestrian detection for multiple and occluded people.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3690–3697, 2013. 7, 8

[47] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu,
Xiangyu Zhang, and Jian Sun. Crowdhuman: A bench-
mark for detecting human in a crowd.
arXiv preprint
arXiv:1805.00123, 2018. 1, 8

[48] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 5, 8

[49] Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and
Thomas Huang. Unitbox: An advanced object detection net-

2897

