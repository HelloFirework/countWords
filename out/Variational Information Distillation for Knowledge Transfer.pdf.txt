Variational Information Distillation for Knowledge Transfer

Korea Advanced Institute of Science and Technology

Sungsoo Ahn ∗

Daejeon, Korea

sungsoo.ahn@kaist.ac.kr

Shell Xu Hu ∗

´Ecole des Ponts ParisTech
Champs-sur-Marne, France

hus@imagine.enpc.fr

Andreas Damianou

Amazon

Neil D. Lawrence

Amazon

Zhenwen Dai

Amazon

Cambridge, United Kingdom

Cambridge, United Kingdom

Cambridge, United Kingdom

damianou@amazon.com

lawrennd@amazon.com

zhenwend@amazon.com

Abstract

Transferring knowledge from a teacher neural network
pretrained on the same or a similar task to a student neural
network can signiﬁcantly improve the performance of the
student neural network. Existing knowledge transfer ap-
proaches match the activations or the corresponding hand-
crafted features of the teacher and the student networks. We
propose an information-theoretic framework for knowledge
transfer which formulates knowledge transfer as maximiz-
ing the mutual information between the teacher and the
student networks. We compare our method with existing
knowledge transfer methods on both knowledge distillation
and transfer learning tasks and show that our method con-
sistently outperforms existing methods. We further demon-
strate the strength of our method on knowledge transfer
across heterogeneous network architectures by transferring
knowledge from a convolutional neural network (CNN) to a
multi-layer perceptron (MLP) on CIFAR-10. The resulting
MLP signiﬁcantly outperforms the-state-of-the-art methods
and it achieves similar performance to the CNN with a sin-
gle convolutional layer.

1. Introduction

Deep neural networks (DNNs) play important roles in
various computer vision tasks, e.g., depth estimation [8],
pose estimation [26], optical ﬂow [7], object classiﬁcation
[11], detection [10], and segmentation [25]. A typical DNN
approach for a computer vision task is to train a sophis-
ticated end-to-end neural network with a large amount of
labeled data. Such an approach often delivers state-of-the-
art performance if a sufﬁcient amount of data is available.

∗Contributed during an internship at Amazon.

Figure 1: Conceptual diagram of the proposed knowledge
transfer method. The student network efﬁciently learns the
target task by minimizing the cross-entropy (CE) loss while
retaining high mutual information (MI) with the teacher net-
work. The mutual information is maximized by learning to
estimate the distribution of the activations in the teacher net-
work, provoking the transfer of knowledge.

However, in many cases, it is impossible to gather sufﬁ-
ciently large data to train a DNN. For example, in many
medical image applications [24], the amount of available
data is constrained by the number of patients of a particular
disease.

A popular approach for handling such lack of data is
transfer learning [19], where the goal is to transfer knowl-
edge from the source task to facilitate learning on the tar-
get task. Typically, one considers the source task to be

43219163

generic with a larger amount of available data that contains
useful knowledge for learning the target task, e.g., knowl-
edge from natural image classiﬁcation [23] is likely to be
useful for ﬁne-grained bird classiﬁcation [29]. Hinton et
al. [12] proposed the teacher-student framework for trans-
ferring such knowledge between DNNs being trained on the
source and target tasks respectively. The high-level idea is
to introduce an additional regularization for the DNN being
trained on the target task, i.e., the student network, which al-
lows learning the knowledge existing in the DNN that was
pre-trained on the source task, i.e., the teacher network.
While the framework was originally designed for knowl-
edge transfer between DNNs on the same dataset, recent
works [30, 31] started exploiting its potential for more gen-
eral transfer learning tasks, i.e., when the source data and
the target data are different.

Many knowledge transfer methods have been proposed
with various intuitions. Hinton et al. [12] and Ba and Caru-
ana [2] propose to match the ﬁnal layers of the teacher and
the student network, as the outputs from the ﬁnal layer of
the teacher network provide more information than raw la-
bels. Romero et al. [22] proposes to match intermediate
layers of the student network to the corresponding layers of
the teacher network. Recent works [3, 6, 13, 30, 31] relax
the regularization of matching the entire layer by matching
carefully designed features/statistics extracted from inter-
mediate layers of the teacher and the student networks, e.g.,
attention maps [31] and maximum mean discrepancy [13].
Evidently, there is no commonly agreed theory behind
knowledge transfer. This causes difﬁculty in understand-
ing empirical results and in developing new methods in
a more principled way.
In this paper, we propose varia-
tional information distillation (VID) as an attempt towards
this direction in which we formulate the knowledge trans-
fer as maximization of the mutual information between the
teacher and the student networks. This framework proposes
an actionable objective for knowledge transfer and allows
us to quantify the amount of information that is transferred
from a teacher network to a student network. Since the mu-
tual information is computationally intractable, we employ
a variational information maximization [1] scheme to max-
imize the variational lower bound instead. See Figure 1 for
the conceptual diagram of the proposed knowledge transfer
method. We further show that several existing knowledge
transfer methods [16, 22] can be derived as speciﬁc imple-
mentations of our framework by choosing different forms
of the variational lower bound. We empirically validate the
VID framework, which signiﬁcantly outperforms existing
methods. We observe the gap is especially large in the cases
of small data and heterogeneous architectures.

In summary, the overall contributions of our paper are as

follows:

• We propose variational information distillation, a prin-

cipled knowledge transfer framework through max-
imizing mutual information between two networks
based on the variational information maximization
technique.

• We demonstrate that VID generalizes several existing
knowledge transfer methods. In addition, our imple-
mentation of the framework empirically outperforms
state-of-the-art knowledge transfer methods on vari-
ous knowledge transfer experiments, including knowl-
edge transfer between (heterogeneous) DNNs on the
same dataset or on different datasets.

• Finally, we demonstrate that heterogeneous knowl-
edge transfer between a convolutional neural networks
(CNN) and a multilayer perceptrons (MLP) is pos-
sible on CIFAR-10. Our method yields a student
MLP that signiﬁcantly outperforms the best-reported
MLPs [17, 27] in the literature.

2. Variational information distillation (VID)

In this section, we describe VID as a general framework
for knowledge transfer in the teacher-student framework.
Speciﬁcally, consider training a student neural network on a
target task, given another teacher neural network pre-trained
on a similar (or related) source task. Note that the source
task and the target task could be the same, e.g., for model
compression or knowledge distillation. The underlying as-
sumption is that the layers in the teacher network have been
trained to represent certain attributes of given inputs that
exist in both the source task and the target task. For a suc-
cessful knowledge transfer, the student network must learn
how to incorporate the knowledge of such attributes to its
own learning.

From a perspective of information theory, knowledge
transfer can be expressed as retaining high mutual infor-
mation between the layers of the teacher and the student
networks. More speciﬁcally, consider an input random vari-
able x drawn from the target data distribution p(x) and K
pairs of layers R = {(T (k), S (k))}K
k=1, where each pair
(T (k), S (k)) is selected from the teacher network and the
student network respectively. Feedforwarding the input x
through the networks induces K pairs of random variables
{(t(k), s(k))}K
k=1 which indicate activations of the selected
layers, e.g., t(k) = T (k)(x). The mutual information be-
tween the pair of random variables (t, s) is deﬁned by:

I(t; s) = H(t) − H(t|s)

= −Et[log p(t)] + Et,s[log p(t|s)],

(1)

where the entropy H(t) and the conditional entropy H(t|s)
are derived from the joint distribution p(t, s). Empirically,
the joint distribution p(t, s) is a result of aggregation over
the layers with input x sampled from the input distribution

43229164

p(x). Intuitively, the deﬁnition of I(t; s) can be understood
as a reduction in uncertainty in the knowledge of the teacher
encoded in its layer t when the the student layer s is known.
We now deﬁne the following loss function which aims to
learn a student network for the target task while encourag-
ing high mutual information with the teacher network:

L = LS −

KX

k=1

λkI(t(k), s(k)),

(2)

where LS is the task-speciﬁc loss function for the target task
and λk > 0 is a hyper-parameter introduced for regulariza-
tion of the mutual information in each layer. Equation (2)
needs to be minimized with respect to the parameters of the
student network. However, the minimization is hard since
exact computation of the mutual information is intractable.
We instead propose a variational lower bound for each mu-
tual information term I(t; s), in which we deﬁne a varia-
tional distribution q(t|s) that approximates p(t|s):

I(t; s) = H(t) − H(t|s)
= H(t) + Et,s[log p(t|s)]
= H(t) + Et,s[log q(t|s)] + Es[DKL(p(t|s)||q(t|s))]
≥ H(t) + Et,s[log q(t|s)],

(3)

where the expectations are over the distribution p(t, s)
and the last inequality is due to the non-negativity of
the Kullback-Leiber divergence DKL(·). This technique
is known as the variational information maximization [1].
Finally, we obtain VID by applying the variational in-
formation maximization to each mutual information term
I(t(k), s(k)) in (2), leading to a minimization of the follow-
ing loss function:

µ(·) and homoscedastic variance σ as the variational dis-
tribution q(t|s), i.e., the mean µ(·) is a function of s and
the standard deviation σ is not. Next, the parameterization
of µ(·) and σ is further speciﬁed by the type of layer cor-
responding to t. When t corresponds to intermediate layer
of the teacher network with spatial dimensions indicating
channel, height and width respectively, i.e., t ∈ RC×H×W ,
our choice of variational distribution is expressed as fol-
lows:

− log q(t|s) = −

CX

HX

WX

c=1

h=1

w=1

log q(tc,h,w|s)

(5)

=

CX

HX

WX

c=1

h=1

w=1

log σc +

(tc,h,w − µc,h,w(s))2

2σ2
c

+ constant,

where tc,h,w denote scalar components of t indexed by
(c, h, w). Further, µc,h,w represents the output of a single
unit from the neural network µ(·) consisting of convolu-
tional layers and the variance is ensured to be positive us-
ing the softplus function, i.e., σ2
c = log(1 + exp(αc)) + ǫ
where αc ∈ R being the parameter to be optimized and
ǫ > 0 is minimum variance introduced for numerical stabil-
ity. Typically, one can choose s from the student network
with similar hierarchy and spatial dimension as t. When
spatial dimension of two layers are equal, 1 × 1 convolu-
tional layers are typically used for efﬁcient parameterization
of µ(·). Otherwise, convolution or transposed convolution
with larger kernel size could be used to match the spatial
dimensions.

We additionally consider the case when the layer t =
T (logit)(x) ∈ RN corresponds to the logit layer of the
teacher network. Here, our choice of the variational dis-
tribution is expressed as follows:

eL = LS −

KX

k=1

λkE

t(k),s(k) [log q(t(k)|s(k))].

(4)

− log q(t|s) = −

NX

n=1

log q(tn|s)

(6)

The objective eL is jointly minimized over the parameters of

the student network and the variational distribution q(t|s).
Note that the entropy term H(t) has been removed from
the equation (3) since it is constant with respect to the pa-
rameters to be optimized. Alternatively, one could interpret
the objective (4) as jointly training the student network for
the target task and maximization of the conditional likeli-
hood to ﬁt the activations of the selected layers from the
teacher network. By doing so, the student network obtains
the “compressed” knowledge required for recovering acti-
vations of the selected layers in the teacher network.

2.1. Algorithm formulation

We further specify our framework by choosing a form
made for the variational distribution q(t|s). In general, we
employ a Gaussian distribution with heteroscedastic mean

=

NX

n=1

log σn +

(tn − µn(s))2

2σ2
n

+ constant,

where tn indicates the n-th entry of the vector t, µn repre-
sents the output of a single unit of neural network µ(·) and
σn is, again, parameterized by softplus function to enforce
positivity. For this case, the corresponding layer s in the
student network is the penultimate layer S (pen) instead of
the logit layer to match the hierarchy of two layers without
being too restrictive on the output of the student network.
Furthermore, we found that using a simple linear transfor-
mation for the parameterization of the mean function was
sufﬁcient in practice, i.e., µ(s) = Ws for some weight
matrix W.

The aforementioned implementations turned out to per-
form satisfactorily during the experiments. We also consid-

43239165

(a) input

(b) 0-th epoch

(c) 40-th epoch

(d) 160-th epoch

(e) no transfer

(f) magnitude of th,w

Figure 2: Plots for the heat maps corresponding to the variational distribution evaluated for spatial dimensions of the inter-

mediate layer in the teacher network, i.e., log q(th,w|s) = Pc log q(tc,h,w|s). Each ﬁgure corresponds to (a) original input

image, (b, c, d) log-likelihood log q(th,w|s) that was normalized and interpolated to ﬁt the spatial dimension of the input im-
age (red pixels correspond to high probability), (d) log-likelihood of variational distribution optimized for the student network
trained without any knowledge transfer applied and (f) magnitude of the layer t averaged for each spatial dimensions.

ered using heteroscedastic variance σ(·), but it gave unsta-
ble training with ignorable improvements. Other types of
parameterizations such as a heavy-tailed distribution or the
mixture density network [5] could be used to gain additional
performance. We leave these ideas for future exploration.

See Figure 2 for an illustration of the training VID using
the implementation based on equation (5). Here, we display
the change in the evaluated log-likelihood of the variational
distribution aggregated over channels, i.e., log q(th,w|s) =

Pc log q(tc,h,w|s), given input x (Figure 2a) throughout

the VID training process. One observes that the student
network is trained gradually for the variational distribution
to estimate the density of the intermediate layer from the
teacher network (Figure 2b, 2c and 2d). As a comparison,
we also optimize the variational distribution for the student
network trained without knowledge transfer, (Figure 2e).
For this case, we observe that this particular instance of the
variational distribution fails to obtain high log-likelihoods,
indicating low mutual information between the teacher and
the student networks.
Interestingly, the parts that corre-
spond to the background achieve higher magnitudes com-
pared to that of the foreground in general. Our explanation
is that the output of layers corresponding to the background
that mostly corresponds to zero activations (Figure 2f) and
contains less information, being a relatively easier target for

maximizing the log-likelihood of the variational distribu-
tion.

2.2. Connections to existing works

The infomax principle. We ﬁrst describe the relationship
between our framework and the celebrated infomax princi-
ple [18] applied to representation learning [28], stating that
“good representation” is likely to contain much informa-
tion in the corresponding input. Especially, such a principle
has been successfully applied to semi-supervised learning
for neural networks by maximizing the mutual information
between the input and output of the intermediate layer as a
regularization to learning the target task, e.g., learning to re-
construct input based on autoencoders [21]. Our framework
can be viewed similarly as an instance of semi-supervised
learning with modiﬁcation of the infomax principle: layers
of the teacher network contain important information for the
target task, and a good representation of the student network
is likely to retain much of their information. One recovers
the traditional semi-supervised learning infomax principle
when we set t(k) = x in the equation (2).

Generalizing mean squared error matching. Next, we
explain how existing knowledge transfer methods based on

43249166

mean squared error matching can be seen as a speciﬁc in-
stance of the proposed framework. In general, the methods
will be induced from the equation (4) by making a speciﬁc
choice of the layers R = {(T (k), S (k))}K
k=1 for knowledge
transfer and parameterization of heteroscedastic mean µ(·)
in the variational distribution:

− log q(t|s) =

NX

n=1

(tn − µn(s))2

2

+ constant.

(7)

Note that Equation (7) corresponds to a Gaussian distribu-
tion with unit variance over every dimension of the layer
in the teacher network. Ba and Caruana [2] showed that
knowledge can be transferred between the teacher and the
student networks that were designed for the same task, by
matching the output of logit layers T (logit), S (logit) from
the teacher and the student networks with respect to mean
squared error. Such a formulation is induced from the equa-
tion (7) by letting R = {(T (logit), S (logit))}, and µ(s) = s
in the equation (7). This was later extended for knowl-
edge transfer between the teacher and the student net-
works designed for different tasks by Li and Hoiem [16],
through adding an additional linear layer on top of the
penultimate layer S (pen) in the student network to match-
ing with logit layer T (logit) in the teacher network. This
is induced similarly from the equation (7) by letting R =
{(T (logit), S (pen))}, and µ(·) being a linear transformation,
i.e., µ(s) = Ws. Next, Romero et al. [22] proposed a
knowledge transfer loss for minimizing the mean squared
error between intermediate layers from the teacher and the
student networks, with additional convolutional layer intro-
duced for adapting different dimension size between each
pair of matched layers. This is recovered from the regular-
ization term in the equation (7) by choosing layers for the
knowledge transfer to be intermediate layers of the teacher
and the student networks, and µ(·) being a linear transfor-
mation corresponding to a single 1 × 1 convolutional layer.
These methods are all similar to our implementation of
the framework in that they all use Gaussian distribution as
the variational distribution. However, our method differs in
two key ways: (a) allowing the use of a more ﬂexible non-
linear functions for heteroscedastic mean and (b) modeling
different variances for each dimension in the variational dis-
tribution. This allows transferring mutual information in a
more ﬂexible manner without wasting model capacity. Es-
pecially, modeling unit variance for all dimensions of the
layer t in the teacher network could be highly restrictive for
the student network. To illustrate, the layer of the teacher
network might include an activation tn that contains infor-
mation irrelevant to the task of the student network, yet re-
quires much capacity for regression of µn(s) to tn. This
would raise over-regularization issues, i.e., wasting the ma-
jority of the student network’s capacity on trying to ﬁt such
a unit. Instead, modeling high homoscedastic variance σn

for such dimension make its contribution ignorable to the
overall loss, allowing one to “ﬁlter” out such unit in an efﬁ-
cient way.

Comparison with feature matching. Besides the knowl-
edge transfer methods based on mean squared error match-
ing, several works [6, 13, 30, 31] have proposed indi-
rectly matching the handcrafted features extracted from
intermediate layers. More speciﬁcally, Zagoruyko and
Komodakis [31] proposed matching the “attention maps”
generated from activations from the layers. Huang and
Wang [13] later generalized the attention map to matching
the maximum mean discrepancy of the activations. Yim
et al. [30] proposed matching the feature called the Flow
of Solution Procedure (FSP) deﬁned by the Gram matrix
of layers adjacent in the same network. Chen et al. [6]
considered matching the reconstructed input image from
the intermediate layers of the teacher and the student net-
works. These methods could be seen as smartly avoiding
the aforementioned over-regularization issue by ﬁltering out
information in the teacher network using expert knowledge.
However, such methods potentially lead to suboptimal re-
sults when the feature extraction method is not apt for the
particular knowledge transfer task and may discard impor-
tant information from the layer of the teacher network in an
irreversible way.

3. Experiments

We demonstrate the performance of the proposed knowl-
edge transfer framework by comparing VID to state-of-
the-art knowledge transfer methods on image classiﬁca-
tion. We apply VID to two different locations: (a) VID
between intermediate layers of the teacher and the student
network (VID-I) and (b) VID between the logit layer of
the teacher network and the penultimate layer of the stu-
dent network (VID-LP). For comparison, we consider the
following knowledge transfer methods: the original knowl-
edge distillation (KD) [12],
learning without forgetting
(LwF) [16], hint based transfer (FitNet) [31], activation-
based attention transfer (AT) [31] and polynomial kernel-
based neural selectivity transfer (NST) [13]. Note that we
consider FitNet as a regularization for training the student
network [31] instead of a stage-wise training procedure as
ﬁrst proposed in [22]. We compare knowledge transfer
methods for knowledge transfer between same and different
datasets, which is commonly referred to as the knowledge
distillation and transfer learning tasks respectively.

In all the experiments, we select the same pairs of in-
termediate layers for knowledge transfer based on VID-I,
FitNet, AT and NST. Similarly, the same pairs of layers for
knowledge transfer are used for LwF and VID-LP. All the
hyper-parameters of all the methods are chosen according
to the performance on a validation set, which is 20% of

43259167

M

Teacher
Student

KD
FitNet
AT
NST
VID-I

KD + AT
KD + VID-I

5000

1000

500

100

94.26
90.72

91.27
90.64
91.60
91.16
91.85

91.81
91.7

-

-

-

84.67

79.63

58.84

86.11
84.78
87.26
86.55
89.73

87.34
88.59

82.23
80.73
84.94
82.61
88.09

85.01
86.53

64.24
68.90
73.40
64.53
81.59

76.29
78.48

(d, w)

Teacher
Student

KD
FitNet
AT
NST
VID-I

KD + AT
KD + VID-I

(40,2)

(16, 2)

(40, 1)

(16, 1)

74.16
74.34

75.80
74.29
74.76
74.81
75.25

75.86
76.11

-

-

-

70.42

68.79

65.46

72.87
70.89
71.06
71.19
73.31

73.13
73.69

70.99
68.66
69.85
68.00
71.51

71.4
72.16

66.03
65.38
65.31
64.95
66.32

67.07
67.19

Table 1: Experimental results (test accuracy) of knowledge
distillation on the CIFAR-10 dataset from teacher network
(WRN-40-2) to student network (WRN-16-1) with varying
number of data points per class (denoted by M ).

Table 2: Experimental results (test accuracy) of knowl-
edge distillation on the CIFAR-100 dataset from the teacher
network (WRN-40-2) to the student networks (WRN-d-w)
with varying factor of depth d and width w.

the training set. We carefully pick the set of candidate val-
ues of hyper-parameters such that all the values proposed
in the original works are included. The presented perfor-
mances are the average of three repeated runs. More details
about experiments are included in the supplementary ma-
terial. The implementation of the algorithm will be made
publicly available shortly.

3.1. Knowledge distillation

We ﬁrst compare knowledge transfer methods on the tra-
ditional knowledge distillation task, where a student net-
work is trained on the same task as the teacher network. By
distilling the knowledge from a large teacher network into
a small student network, we can speed up the computation
for prediction. We further investigate two problems for this
task: whether we can beneﬁt from knowledge transfer in the
small data regime and how much performance we lose by
reducing the size of the student network? Note that we do
not evaluate the performance of VID-LP and LwF as they
are designed for transfer learning. When applied, KD, VID-
LP and LwF delivered similar performance.

Reducing training data. Knowledge transfer can be a
computationally expensive task. Given a pre-trained teacher
network on the whole training data set, we explore the pos-
sibility of using a small portion of the training set for knowl-
edge transfer. We demonstrate the effect of a reduced train-
ing set by applying knowledge distillation on CIFAR-10
[15] with four different sizes of training data. We employ
wide residual networks (WRN) [15] for the teacher network
(WRN-40-2) and the student network (WRN-16-1), where
the teacher network is pre-trained on the whole training set
of CIFAR-10. Knowledge distillation is applied to four dif-
ferent sizes of training set: 5000 (the full size), 1000, 500,
100 data points per class.

We compare VID-I with KD, FitNet, AT and NST. We
also provide performances of the teacher network (Teacher)
and the student network trained without any knowledge
transfer (Student) as baselines. We choose four pairs of in-
termediate layers similarly to [31], each of which is located
at the end of a group of residual blocks. We implemented
VID-I using three 1 × 1 convolutional layers with hidden
channel size as twice of the output channel size.
The
results are shown in Table 1. Our method, VID-I, outper-
forms other knowledge transfer methods consistently across
all regimes. The performance gap increases as the size of
dataset get smaller, e.g., VID-I only drops 10.26% of accu-
racy even when 100 data points per each class are provided
to the student network. There is a 31.88% drop without
knowledge transfer and a 15.52% drop for the best baseline,
i.e., KD + AT.

Varying the size of the student network. The size of the
student network gives a trade-off between the speed and the
performance in knowledge transfer. We evaluate the per-
formance of knowledge transfer methods on different sizes
of the student network. The teacher network (WRN-40-2)
is pre-trained on the whole training set of CIFAR-100. A
student network with four choices of size, i.e., WRN-40-
2, WRN-16-2, WRN-40-1, WRN-16-1, is trained on the
whole training set of CIFAR-100. We compare our VID-
I with KD, FitNet, AT and NST along with the Teacher and
Student baselines. The choices of intermediate layers are
the same as the previous experiment.

The results are shown in in Table 1. As also noticed
by Furlanello et al. [9], the student network with the same
size as the teacher network outperforms the teacher network
with all the knowledge transfer methods. One observes
that VID-I consistently outperforms FitNet, AT and NST,
which correspond to the same choice of layers for knowl-
edge transfer. It also outperforms KD except for the case

43269168

when the structure of the student network is identical to that
of the teacher network, i.e., WRN-40-2, where two methods
can be combined to yield the best performance.

3.2. Transfer learning

We evaluate knowledge transfer methods on transfer
learning.
The teacher network is a residual network
(ResNet-34) [11] pre-trained on the ImageNet dataset [23].
We apply transfer learning to improve the performance of
two separate image classiﬁcation tasks. The ﬁrst task is a
ﬁne-grained bird species classiﬁcation based on the CUB-
200-2011 dataset [29], which contains 11,788 images in
total for 200 bird species. The second task is an indoor
scene classiﬁcation based on the MIT-67 dataset [20], which
contains 15,620 images for 67 classes of indoor scenes.
For both tasks, there are a relatively few images per class,
which can signiﬁcantly beneﬁt from knowledge transfer
from the ImageNet classiﬁcation task. To evaluate the per-
formance at various levels of data scarcity, we subsample
both datasets into three different sizes (50, 25, 10 per class
for MIT-67 and 20, 10, 5 per class for CUB-200-2011) and
compare the knowledge transfer methods.

We evaluate the knowledge transfer methods in two sce-
narios: a smaller student network of the same architecture
(ResNet-18) and different architecture (VGG-9) [25]. We
compare our VID-I and VID-LP with LwF, FitNet, AT and
NST. We evaluate the performance of the student network
without transfer learning (Student) as a baseline. For the
teacher and the student network with ResNet architecture,
we choose the outputs of the third and fourth groups of
residual blocks (from the input) as the intermediate layers
for knowledge transfer. In the case of the VGG-9 student
network, we choose the fourth and ﬁfth max-pooling lay-
ers as the intermediate layers for knowledge transfer, which
corresponds to the same spatial dimension as the intermedi-
ate layers selected from the teacher network. For applying
VID-I to the ResNet-18 student network, we use two 1 × 1
convolutional layers with the size of intermediate channels
as half of the output channel size. When the student net-
work is VGG-9, a single 1 × 1 convolutional layer without
non-linearity is used.

The results are shown in Table 3. The knowledge trans-
fer from ResNet-34 to VGG-9 gives very similar perfor-
mance to the transfer from ResNet-34 to ResNet-18 for all
the knowledge transfer methods. This shows that knowl-
edge transfer methods are robust against small architecture
changes. Our methods outperform other knowledge trans-
fer methods in all regions of comparison. Both VID-I and
VID-LP outperforms baselines that correspond to the same
choice of layers for knowledge transfer. For the MIT-67
dataset, we observe that our algorithm outperforms even the
ﬁnetuning method, which requires pre-training of the stu-
dent network on the source task.

3.3. Knowledge transfer from CNN to MLP

The transfer learning experiments show the robustness
of the knowledge transfer method against small architec-
ture changes. This leads to an interesting question: whether
a knowledge transfer method can work between two com-
pletely different network architectures. A solution to this
question can open a new direction of knowledge trans-
fer and potentially offer solutions to many problems, e.g.,
speeding up prediction of recurrent neural networks (RNNs)
by transferring knowledge from a RNN to a CNN, speed-
ing up prediction of CNN on CPU or low-energy device by
transferring knowledge from a CNN to a multi-layer per-
ceptron (MLP).

In this paper, we evaluate the performance of knowl-
edge transfer from CNN to MLP on CIFAR-10. There is
a well-known performance gap between CNN and MLP
on CIFAR-10 [17, 27]. The state-of-the-art performance
on CIFAR-10 with MLP is 78.62% with initialization from
auto-encoders [17] and 74.32% using knowledge distilla-
tion [27]. Urban et al. [27] also trained a single convo-
lutional layer achieving the performance of 84.6% using
knowledge distillation.

We apply the knowledge transfer methods in the knowl-
edge distillation setting as mentioned in Section 3.1. We
use a teacher network with convolutional layers (WRN-40-
2) pre-trained on CIFAR-10. We use a MLP with ﬁve fully
connected hidden layers as the student network, constructed
by stacking one linear layer, three bottleneck linear layers
and one linear layer in sequence. Each is followed by a non-
linearity activation in between. Here, the bottleneck layer
indicates a composition of two linear layers without non-
linearity that is introduced to speed up learning by reducing
the number of parameters. All the hidden layers have the
same h number of units and the bottleneck linear layer is
composed of two linear layers with a size of h × h
4 and
h
4 × h.

The knowledge transfer between intermediate layers is
deﬁned between the outputs of four residual groups of the
teacher network and the outputs of the ﬁrst four fully con-
nected layers of the student network. We compare VID-I
with KD and FitNet since these knowledge transfer meth-
ods do not rely on spatial structures. For the same reason,
AT and NST are not applicable to multilayer perceptrons.
VID-I is implemented with multiple transposed convolu-
tional layers without non-linearities. Speciﬁcally, the inputs
for the variational distributions, i.e., the hidden layers of the
MLP are treated as a tensor with 1 × 1 spatial dimensions.
Single transposed convolutional layer with a 4 × 4 kernel,
unit stride and zero padding is followed by multiple trans-
posed convolutional layers with a 4 × 4 kernel, two strides,
and single padding to match the spatial dimension of the
corresponding layer of the teacher network for knowledge
transfer. More details on implementations of the student

43279169

M

≈80

50

25

10

M

≈80

50

25

10

Student
ﬁne-tuning

LwF
FitNet
AT
NST
VID-LP
VID-I

LwF + FitNet
VID-LP + VID-I

48.13
70.97

63.43
71.34
58.21
55.52
67.91
71.34

70.97
71.87

37.69
66.04

51.79
60.45
48.66
46.34
58.51
63.66

60.37
65.75

27.01
58.13

41.04
54.78
43.66
33.21
47.09
60.07

54.48
61.79

14.25
47.91

22.76
36.94
27.01
20.82
31.94
50.97

38.73
50.37

Student
ﬁne-tuning

LwF
FitNet
AT
NST
VID-LP
VID-I

LwF + FitNet
VID-LP + VID-I

53.58
65.97

60.90
70.90
60.90
55.60
68.88
72.01

70.52
71.72

43.96
58.51

52.01
64.70
52.16
46.04
61.64
67.01

64.10
66.49

29.70
51.72

41.57
54.48
42.76
35.22
50.22
59.33

54.63
58.96

15.97
39.63

27.76
40.82
25.60
21.64
39.25
45.90

40.15
45.89

(a) MIT-67, ResNet-34 to ResNet-18

(b) MIT-67, ResNet-34 to VGG-9

M

≈29.95

20

10

5

M

≈29.95

20

10

5

Student
ﬁne-tuning

LwF
FitNet
AT
NST
VID-LP
VID-I

LwF + FitNet
VID-LP + VID-I

37.22
76.69

55.18
66.63
54.62
55.01
65.59
73.25

68.69
69.71

24.33
71.00

42.13
56.63
41.44
41.87
54.12
67.20

58.81
63.94

12.00
59.25

26.23
46.68
28.90
23.76
39.20
56.86

48.86
52.87

7.09
44.07

14.27
31.04
16.55
15.63
27.86
46.21

31.30
41.12

Student
ﬁne-tuning

LwF
FitNet
AT
NST
VID-LP
VID-I

LwF + FitNet
VID-LP + VID-I

44.59
60.96

52.18
68.96
56.28
56.55
66.82
71.51

70.56
70.00

32.10
51.86

38.05
61.52
43.96
44.95
55.94
65.69

62.44
65.14

15.69
46.88

25.57
48.04
28.33
28.43
38.10
53.29

47.36
53.78

9.66
39.98

13.93
32.89
13.98
14.66
30.47
38.09

30.52
38.76

(c) CUB-200-2011, ResNet-34 to ResNet-18

(d) CUB-200-2011, ResNet-34 to VGG-9

Table 3: Experimental results (test accuracy) of transfer learning from the teacher network (ResNet-34) to the student network
(ResNet-18/VGG-9) for the MIT-67/CUB-200-2011 dataset with varying number of data points per class (denoted by M ).
We use M ≈ Mavg to denote the setting where the number of data points per class is non-uniform and Mavg in average.
Fine-tuning gives good results on transfer learning, but is not directly comparable as it is not a knowledge transfer method.

Network

MLP-4096 MLP-2048 MLP-1024

Student
KD
FitNet
VID-I

Urban et al. [27]
Lin et al. [17]

70.60
70.42
76.02
85.18

70.78
70.53
74.08
83.47

74.32
78.62

70.90
70.79
72.91
78.57

Table 4: Experimental result (test accuracy) of distilla-
tion on CIFAR-10 from the convolutional teacher network
(WRN-40-2) to the fully connected student network (MLP-
h) with varying size of hidden dimensions h.

network and the auxiliary distribution are in the supplemen-
tary material.

The results are shown in Table 4. Both FitNet and VID-
I improve the performance comparing the baseline of di-
rectly training the intermediate layers of the student net-

work. VID-I signiﬁcantly outperforms FitNet on MLPs
with different sizes. Furthermore, MLP-4096 outperforms
the the state-of-the-art performance with MLP reported by
Lin et al. [17] (78.62%) and Ba et al. [27] (74.32%) signif-
icantly. More importantly, our method bridges the perfor-
mance gap between CNN (84.6% using one convolutional
layer [27]) and MLP shown in previous works.

4. Conclusion

In this work, we proposed the VID framework for ef-
fective knowledge transfer by maximizing the variational
lower bound of the mutual information between two neural
networks. The implementation of our algorithm is based on
Gaussian observation models and is empirically shown to
outperform other benchmarks in the distillation and trans-
fer learning tasks. Using more ﬂexible recognition models,
e.g., [14], for accurate maximization of mutual information
and alternative estimation of mutual information, e.g., [4],
are both ideas of future interest.

43289170

[19] S. J. Pan, Q. Yang, et al. A survey on transfer learning. 2010.
[20] A. Quattoni and A. Torralba. Recognizing indoor scenes.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 413–420. IEEE, 2009.

[21] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and
T. Raiko. Semi-supervised learning with ladder networks. In
Advances in Neural Information Processing Systems, pages
3546–3554, 2015.

[22] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv
preprint arXiv:1412.6550, 2014.

[23] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015.

[24] T. Schlegl, J. Ofner, and G. Langs. Unsupervised pre-training
across image domains improves lung tissue classiﬁcation. In
International MICCAI Workshop on Medical Computer Vi-
sion, pages 82–93. Springer, 2014.

[25] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[26] A. Toshev and C. Szegedy. Deeppose: Human pose esti-
mation via deep neural networks.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1653–1660, 2014.

[27] G. Urban, K. J. Geras, S. E. Kahou, O. Aslan, S. Wang,
A. Mohamed, M. Philipose, M. Richardson, and R. Caru-
ana. Do deep convolutional nets really need to be deep and
convolutional? In ICLR, 2017.

[28] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.
Manzagol. Stacked denoising autoencoders: Learning useful
representations in a deep network with a local denoising cri-
terion. Journal of machine learning research, 11(Dec):3371–
3408, 2010.

[29] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-
longie, and P. Perona. Caltech-UCSD Birds 200. Technical
Report CNS-TR-2010-001, California Institute of Technol-
ogy, 2010.

[30] J. Yim, D. Joo, J. Bae, and J. Kim. A gift from knowl-
edge distillation: Fast optimization, network minimization
and transfer learning.

[31] S. Zagoruyko and N. Komodakis. Paying more attention to
attention: Improving the performance of convolutional neu-
ral networks via attention transfer. In ICLR, 2016.

References

[1] D. B. F. Agakov. The IM algorithm: a variational approach

to information maximization. 2004.

[2] J. Ba and R. Caruana. Do deep nets really need to be deep?
In Advances in neural information processing systems, pages
2654–2662, 2014.

[3] V. Belagiannis, A. Farshad, and F. Galasso. Adversarial net-
In European Conference on Computer

work compression.
Vision, pages 431–449. Springer, 2018.

[4] I. Belghazi, S. Rajeswar, A. Baratin, R. D. Hjelm, and
A. Courville. Mine: mutual information neural estimation.
arXiv preprint arXiv:1801.04062, 2018.

[5] C. M. Bishop. Mixture density networks. Technical report,

Citeseer, 1994.

[6] S. Chen, C. Zhang, and M. Dong. Coupled end-to-end trans-
In Com-

fer learning with generalized Fisher information.
puter Vision and Pattern Recognition, 2018.

[7] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In Proceedings of the IEEE International Conference on
Computer Vision, pages 2758–2766, 2015.

[8] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in neural information processing systems, pages
2366–2374, 2014.

[9] T. Furlanello, Z. C. Lipton, M. Tschannen, L. Itti, and
In ICML,

A. Anandkumar. Born again neural networks.
2018.

[10] R. Girshick. Fast r-CNN. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015.

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[12] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[13] Z. Huang and N. Wang. Like what you like: Knowl-
edge distill via neuron selectivity transfer. arXiv preprint
arXiv:1707.01219, 2017.

[14] D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen,
I. Sutskever, and M. Welling. Improved variational inference
with inverse autoregressive ﬂow. In Advances in Neural In-
formation Processing Systems, pages 4743–4751, 2016.

[15] A. Krizhevsky. Learning multiple layers of features from

tiny images. Technical report, Citeseer, 2009.

[16] Z. Li and D. Hoiem. Learning without forgetting.

IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2017.

[17] Z. Lin, R. Memisevic, and K. Konda. How far can we go
without convolution: Improving fully-connected networks.
arXiv preprint arXiv:1511.02580, 2015.

[18] R. Linsker. An application of the principle of maximum in-
formation preservation to linear systems. In Advances in neu-
ral information processing systems, pages 186–194, 1989.

43299171

