Synthesizing Environment-Aware Activities via Activity Sketches

Yuan-Hong Liao1
1 University of Toronto, 2 Vector Institute, 3 MIT, 4 University of Ljubljana, 5 Abelium d.o.o
{andrew, fidler}@cs.toronto.edu, {xavierpuig,torralba}@csail.mit.edu, marko.boben@fri.uni-lj.si

5, Antonio Torralba3, Sanja Fidler1

2∗, Xavier Puig3∗, Marko Boben4

2

,

,

,

Abstract

In order to learn to perform activities from demonstra-
tions or descriptions, agents need to distill what the essence
of the given activity is, and how it can be adapted to new
environments.
In this work, we address the problem of
environment-aware program generation. Given a visual
demonstration or a description of an activity, we gener-
ate program sketches representing the essential instructions
and propose a model to transform these into full programs
representing the actions needed to perform the activity un-
der the presented environmental constraints. To this end,
we build upon VirtualHome [15] to create a new dataset
VirtualHome-Env, where we collect program sketches to
represent activities and match programs with environments
that can afford them. Furthermore, we construct a knowl-
edge base to sample realistic environments and another
knowledge base to seek out the programs under the sampled
environments. Finally, we propose ResActGraph, a network
that generates a program from a given sketch and an envi-
ronment graph and tracks the changes in the environment
induced by the program.

1. Introduction

We want agents to be able to perform everyday tasks
(such as setting up the table, preparing coffee, or even sit on
the couch and watch TV). An agent should learn to perform
these tasks from high-level descriptions or visual demon-
strations. The challenge is on how to generalize the ac-
quired knowledge to new environments. For instance, if we
want to learn to make coffee, an agent could ﬁrst watch a
video of someone making coffee in order to extract the se-
quence of steps (program) that need to be executed. How-
ever, when trying to make coffee in an environment that dif-
fers from the environment in which the demonstration took
place, the agent has to adjust the program so that it can be
executed. For instance, the agent could ﬁnd that the coffee
machine is unplugged, or that it is in the living room instead

∗ indicates equal contribution

Activity (a): Watch TV

Description (da):

Go to the living room. Sit on the couch and watch TV.

Visual Demonstration (ia):

(a) 

New Environment (e)

Activity Sketch (sa)

Sit

Couch

Watch

TV

Environment-aware 

Walk

Find

Find

Grab

Put

Sit

Program (p)

Living room

TV

Couch

Cat

Cat

Couch

TV

(b)

Changes: Cat on sofa

Watch

Figure 1. Overview of the environment-aware program generation.
Our goal is (a) generating a sketch sa distilling the essential steps
of the given demonstration ia or description da and (b) given a
new environment e, generating a program p, adapting the sketch
to e. The program contains the instructions to perform the activity
(blue blocks) as well as instructions to deal with the environment
(red blocks, grabbing the cat to sit).

of in the kitchen, or that someone else is currently using it
and the agent needs to wait. Being able to perform these ad-
justments requires the access to a common-sense database
of knowledge that allows the agent to decide which steps
in the demonstration are essential in the task deﬁnition, and
how the program needs to be modiﬁed (by adding/removing
steps) in order to accomplish the task in a new environment.
To address the generalization problem we represent ac-
tivities with sketches, representations inspired from work in
programming languages [16, 13]. Sketches are high-level
representations of the steps needed to perform a task but
leaving holes that need to be completed for the sketch to
become executable in a particular environment.

Fig. 1 illustrates our goal. Given a visual demonstration
or a description of someone going to watch TV, we want to
extract the sketch of the activity (ﬁg. 1a). In this example,
the sketch consists of two steps: Sit Couch, Watch TV.
The fact that the demonstration had the person going to the

6291

living-room is not important. Watching TV requires us to
go to the room that contains the TV. Executing this activity
in a new environment requires expanding the sketch into a
complete program (ﬁg. 1b). The sketch expansion depends
on the state of the environment. In this example, it turns out
that there is a cat on the couch. To sit on the couch, we need
to push away the cat ﬁrst. We show the automatically gener-
ated program that includes all the steps needed to complete
the task and to deal with the cat.

To expand sketches into programs, it is necessary to
have access to commonsense knowledge about the world
and how to deal with typical situations. This knowledge
provides ways to solve situations that can be reused in a
multitude of tasks. Since no such information, with di-
verse programs and environments, exists to date, we col-
lect a new dataset, built upon VirtualHome [15], contain-
ing around 3k home activities. We extend the VirtualHome
dataset to include more actions and over 30k programs and
collect sketches of the activities by crowdsourcing. Each
environment is represented as a graph with 300 objects and
4000 spatial relations on average.

We then propose a model to generate programs by se-
lecting, for every step, a node in the environment graph
representing an object of interaction. To do so, we exploit
Graph Neural Networks (GNNs) to reason about the states
and relations between the objects in the environments. Fur-
thermore, we propose ResActGraph, a model that reasons
about the changes in the graph induced by the agents previ-
ous steps to generate the goal program.

The main contributions of this work are:

introducing
sketches as environment-independent representations of an
activity, a database of commonsense knowledge of activi-
ties, sketches and how to deal with a variety of situations,
and a method to generate programs from sketches that ac-
complish the task in a new environment. We show results in
VirtualHome [15].

2. Related Work

Learning from demonstrations. Learning from visual
demonstrations or language descriptions has been of in-
creasing interest in both robotics and computer vision.
However, this has been mostly focused on learning low-
level tasks rather than the high semantic level activities
that we tackle in this work. For example, [18, 11, 1] fo-
cus on learning to navigate in environments or manipulate
objects, while in visual imitation learning, multiple works
have used videos to learn to manipulate objects under low
supervision regimes [14, 7] or imitate kinetic human be-
haviors [12]. Our work is more closely related to semantic
planning [9, 20, 3], which focus on modeling sequences of
composite and semantically loaded actions. Learning those
requires inferring and modeling the sub-goals of a given
task. [9] represents such goals as a graph, with nodes be-

ing actions and edges being precondition states, while we
propose program representations. Furthermore, to perform
such tasks, it is necessary to know what the constraints of
the given environment where it will be executed are. Sim-
ilarly to [3], we propose to encode knowledge of the en-
vironments and how they can constrain the activities to be
performed.

Program Synthesis by Sketching. Our approach for
environment-aware program generation is partially inspired
by performing program synthesis by sketching. In [16], a
sketch expresses the high-level structure of an implemen-
tation but leaves holes in place of low-level details, which
corresponds to our model that derives the details based on
the environmental constraints so as to execute the programs
smoothly.A recent body of work has developed neural ap-
proaches to program generation using user-provided exam-
ples [8, 4], visual demonstrations [17], and descriptions [6].
The work [13] is the most related to ours. They learn a
model that predicts sketches of programs relevant to a label
and the predicted sketches are concretized into code using
combinatorial techniques. The main difference between our
work and theirs is not only that our sketches are inferred
from visual or textual data, but that we focus on how to
incorporate the environmental constraints in program gen-
eration.

3. Problem formulation

The goal of environment-aware program generation is to
predict, given a demonstration or description of an activity
and an environment, a program that can execute the activ-
ity in such environment. We deﬁne this task and the corre-
sponding notation in this section.

Let A and E be the universe of activities and environ-
ments. An activity a (e.g. watch TV) can be represented as
a set of programs Pa containing a sequence of instructions
(e.g. TurnOn TV, Sit Sofa, Watch TV), which vary
depending on how the activity is performed. Let ✶(p, e) be
an indicator function determining if p can be executed in e
(e.g. an agent can not grab cups inside a closed cabinet).
Given an activity a ∈ A and an environment e ∈ E, our
goal is to learn a model that generates p such that

p ∈ Pa, ✶(p, e) = 1

(1)

We use the corresponding visual demonstrations ia ∈ Ia
or descriptions da ∈ Da to specify a. However, ia and da
are implicitly conditioned on certain environments which
might differ from the current one e. Therefore, directly in-
ferring ˆp does not ensure that ˆp satisﬁes eq. 1 given e.

Inspired by [16], we introduce program sketches sa ∈ S
as environment-independent representations of the activi-
ties. We thus change the constraint in eq. 1 to be:

6292

(a)

(b)

propagation

(cid:13)(cid:20)(cid:20)(cid:21)
(cid:23)(cid:10)(cid:11)(cid:18)(cid:14)
(cid:7)(cid:8)
(cid:12)(cid:10)(cid:23)

(cid:12)(cid:20)(cid:24)(cid:12)(cid:16)

…
(cid:10)(cid:15)(cid:14)(cid:19)(cid:23)

(cid:3)(cid:17)(cid:19)(cid:13)

(cid:4)(cid:21)(cid:10)(cid:11)

(cid:5)(cid:24)(cid:23)

…

…

…

h

h

h

GRU enc

GRUdec

GRU

dec

GRU dec

(cid:1)(cid:22)(cid:20)(cid:22)(cid:2) (cid:10)(cid:15)(cid:14)(cid:19)(cid:23)

(cid:3)(cid:17)(cid:19)(cid:13)

(cid:12)(cid:20)(cid:24)(cid:12)(cid:16)

(cid:4)(cid:21)(cid:10)(cid:11)

(cid:12)(cid:10)(cid:23)

(cid:6)(cid:17)(cid:23)

(cid:12)(cid:20)(cid:24)(cid:12)(cid:16)

(cid:9)(cid:10)(cid:23)(cid:12)(cid:16)

(cid:7)(cid:8)

Copy

Learned state changes 

Differentiable path

Non-differential path

Figure 2. (a) We extract the ground truth environment graph from VirtualHome and perform message passing on the graph. (b) At every
time step, the decoder perform sequential classiﬁcation over the hidden states of the graphs (top row). The selected nodes are shown in
bold border blocks. We also model the environment changes induced by the generated programs (solid red arrows).

p ∈ Psa , ✶(p, e) = 1

(2)

With the program sketches as proxy representations, we
can divide the task into two sub-problems: a model that
predicts a sketch ˆsa from a demonstration ia or a description
da and another model that predicts a program ˆp given the
predicted sketch ˆsa and an environment e:

ˆp = fsketch2prog( ˆsa, e), where
ˆsa = fdemo2sketch(ia) or ˆsa = fdesc2sketch(da)

(3)

Here, p and sa are a sequence of instructions. Each instruc-
tion is represented by an action and up to two arguments
representing objects of interaction (α, β1, β2).1

4. Model

In this section, we present our approach to the
environment-aware program generation task. First, we in-
troduce ResActGraph to generate programs from sketches
and target environments. Later on, we describe how we pre-
dict sketches from demonstrations or descriptions.

4.1. Program generation from sketches and graphs

We frame the program generation task as a seq2seq prob-
lem, where an encoder encodes the input sketch and the de-
coder generates the target program one instruction at a time,
composed by an action and object arguments. Given that

the program must be grounded in a target environment, in-
stead of predicting the objects from a ﬁxed taxonomy, the
model predicts for each instruction object instances that are
present in the environment. This has two beneﬁts: (1) It
avoids referring to object instances that do not exist in the
environment. (2) It allows the model to use information of
each instance within the environment, such as its state or
relations with other objects, to predict the appropriate in-
struction.

To do that, we encode the scene as a graph G = (V, R)
modeling the dependencies of the object instances. The
node v ∈ V indicates the object instance and each node
has a label, including the object class cv, its states lv, and
properties propv. Note that V includes a node for the agent
itself. The edge r ∈ R encodes the spatial relations, includ-
ing ON, IN OBJ, IN ROOM, CLOSE TO, and FACE AT,
between every two object instances.

The node labels and relations are used to obtain vector
embeddings for each instance which are used by the decoder
to predict the environment-aware program, as we describe
in the following section.

4.2. ResActGraph

We adopt the GGNN [10] framework to obtain the hid-
den states of the nodes and capture the object relations in
the environment graph. The hidden states of each node v
are initialized by its label (cv, lv, propv):

h0
v = tanh(ginit([Wccv, Wllv, Wproppropv])

(4)

1The number of arguments depends on the type of the action.

We apply one-hot encoding to the label and set

6293

Wc, Wl, Wprop as learnable weights. ginit is a network
composed of fully connected layers that combine all the in-
formation.

At propagation step k, each node’s incoming informa-
v is determined by aggregating the hidden states of its

tion xk
neighbors v′ ∈ N (v) at the previous step k − 1:

v = X
xk
j∈L(R)

X

Wpj hk−1

v′ + bpj

(5)

v′∈Nj (v)

L(R) denotes the set of edge labels and the linear layer Wpj
and bias bpj are shared across all nodes.

After aggregating the information, the hidden states of
the nodes are updated through a gating mechanism similar
to Gated Recurrent Unit (GRU) [5] as follows:

zk
v = ρ(Wzxk
v = ρ(Wrxk
rk
ˆhk
v = tanh(Whxk
hk
v = (1 − zk

v + Uzhk−1
v + Urhk−1

v + bz),
v + br),

v + Uh(rk
v + zk

v ⊙ hk−1
v ⊙ ˆhk

v

v

v ) ⊙ hk−1

(6)

) + bh),

This results in a vector embedding for each object hk
v , with
information about its state and relationship with the envi-
ronment.

We use one GRU to encode the sketches and another one
to generate the program one instruction at a time. For time
t, let f eatsa = enc(sa) be the output of the sketch encoder,
and ht
dec the hidden states of the decoder. To predict the
program instruction, ( ˆαt, ˆβ1
t ), we predict the ﬁrst object
argument β1
t over the graph nodes, use it to predict the ac-
tion ˆαt and combine this information to predict the second
argument ˆβ2
t :

t , ˆβ2

ˆβ1
t = arg max

v∈V

ˆαt = arg max

α∈A

ˆβ2
t = arg max

v∈V

σ(gβ 1 (ht

dec, hK

v , f eatsa ))

σ(gα(ht

dec, hK
ˆβ 1

t

, f eatsa ))

(7)

σ(gβ 2 (ht

dec, hK

v , f eatsa

, hK
ˆβ 1

t

, ˆαt))

where A is all the possible actions and σ denotes the
softmax function.

Note that so far the hidden states of the nodes hK

v are
constant over t, but we would like them to change according
to the program being executed. To do that, at time t, we
use the previously generated instructions ( ˆα<t, ˆβ1
<t) to
update the hidden states of the nodes. We set the initial
state of each node as hK0
v and update the state hKt
v
at time t if v is interacted by the agent at the previous time
step or is the agent itself. For example, if the instruction
at the previous time step is grab mug, we use the action
embeddings of grab to change the hidden states of agent

v = hK

<t, ˆβ2

and mug. Let ˆv correspond to the agent node or one of the
previous arguments ˆβ1
is updated
as follows:

t−1. The state hKt−1

t−1, ˆβ2

ˆv

r = tanh(gres(hKt−1

ˆv

⊙ ge(Emb( ˆαt−1), mt−1)))

ˆv = hKt−1
hKt

ˆv

+ r

(8)

ˆαt−1. ge and gres consider the change of the hKt−1

where Emb( ˆαt−1) is the embedding of
ˆαt−1 and mt−1 is
a one-hot encoding denoting if v is the subject or the object
of
and
predict the residuals. Note that since the node agent is
involved at every time step, it tracks the progress through
the generation. The model overview is shown in Fig. 2.
Learning. We use the cross-entropy loss function for pro-
gram prediction. The GGNN and GRUs are then trained
with the back-propagation through time (BPTT).

ˆv

4.3. Inferring sketches

Activities speciﬁed by demonstrations. We use key
frames i = [in]n=1:Ndemo as the representations of the
demonstrations, where Ndemo is the length of the key
frames. To be speciﬁc, we take the bird-eye view of each
in. Besides, we also use the ground truth semantic segmen-
tation map iseg = [iseg n]n=1:Ndemo as input. Two CNNs
are used to extract the features separately, and we apply late
fusion to extract the nth visual features f eatn as follows:

f eatn = gf use([CN Ni(in), CN Nseg(iseg n)])

(9)

where [, ] denotes concatenation. Later on, we max-pool the
features over the different steps time steps and apply a GRU
to decode the sketches.
Activities speciﬁed by descriptions. We adopt the seq2seq
model with a GRU encoding each word in the description
and a GRU to decode each of the sketch instructions.

5. Dataset: VirtualHome-Env

Our goal is to generate a sketch sa from a demonstration
ia or description da and induce a program p from sa and a
target environment e. In this section, we describe how we
obtain the dataset containing (a, ia, da, sa, e, p). We brieﬂy
introduce VirtualHome as the playground for the dataset
collection, we describe how we extend the dataset with ac-
tivity sketches and ﬁnally describe how we pair the pro-
grams with arbitrary environments where they can be per-
formed, motivating the creation of a common sense knowl-
edge base of activities2.
Background. VirtualHome is a dataset and simulator to
represent human household activities. The dataset was col-
lected by asking annotators to come up with various ac-
In a second
tivities and provide a description for them.

2The details of the collected knowledge base are described in ??

6294

Description:

(1)

Pick up my dirty clothes 
from the bedroom, load the 
washer, add detergent, and 
turn on washer

(2)

Clothes  in bedroom

Washing  machine  in 

entrance  hall

Washing  machine  off

No  clothes  inside 
washing  machine

Detergent near 
washing  machine

Preconditions

Programs

B edroom

Clothes

Clothes

Entrance hall

Washing machine

Walk

Walk

Grab

Walk

Walk

Open

Washing machine

Put

Grab

Put

Run

Clothes

Detergent

Detergent

Washing machine

Figure 3. The annotators label the descriptions and programs
with certain environments in mind (bottom left), resulting in the
environment-dependent descriptions and programs. The blocks
colored in blue are considered as environment-dependent compo-
nents.

stage, annotators were shown a description of an activity
and were asked to write a program from it. With activities
represented as programs, a Unity simulator executes them
in some predeﬁned apartments and renders the programs as
videos. This allows us to obtain activities and programs to-
gether with descriptions and demonstrations (a, ia, da, p).

5.1. Collecting sketches

When collecting each activity in VirtualHome, annota-
tors imagine an environment where the activity could take
place and provide a description according to it. As a re-
sult, the description and subsequent program is speciﬁc to a
given environment, but may not be doable when presented
with new environments or constraints (see Fig. 3). There-
fore, we need a more abstract representation of an activity,
which can be consistent with multiple environments and the
information (a, ia, da, p).

Inspired by [16], we collect the sketches of the activi-
ties to abstract out the components that are environment-
dependent and informally deﬁne the sketches as the envi-
ronment independent representations. Different from pro-
gramming languages, it is highly non-trivial to deﬁne the
sketches of the activities since they depend on the common-
sense of each individual. Therefore, we manually collect
the sketches and get the information (a, ia, da, sa, p).

5.2. Pairing programs with environments

We ﬁnally need to add an environment that pairs with the
activity programs. Since we do not know the environment
that each annotator had in mind, we need to infer it from the
program. We ﬁrst extract the preconditions of the programs
and use them to sample feasible environments. We deﬁne
preconditions of a program as the conditions that have to be
true in the environment in order to execute the program in it.

For example, in order to execute watch TV, the tv should
be on. We construct a function Φ that infers the precondi-
tions from p and sample e from the set of environments that
satisfy such preconditions EΦ(p):

e ∼ EΦ(p) ⊂ E, s.t. ∀j ∈ Φ(p), e satisﬁes j

(10)

The above is a weak constraint since it does not inform
about objects that are not speciﬁed in Φ(p). To get real-
istic environments, these objects should follow some priors.
For example, couches can be occupied, but they can not be
cold. Apples can be stored inside fridges, but they are sel-
dom found in bathtubs. We build collect these rules in a
knowledge base, KB-RealEnv and use them to build the en-
vironment, starting from the environments provided in Vir-
tualHome.

5.3. Extending programs to diverse environments

We now have the information (a, ia, da, sa, e, p). How-
ever, relying solely on the original programs results in a lim-
ited set of preconditions and thus environments. One possi-
ble reason is that when describing activities, annotators tend
to assume the simplest setting to perform the given activity.
For example, when thinking of doing the laundry, it is com-
mon to imagine that the washing machine is idle or empty.
To address that, we build a simulator Ψ that takes a pro-
gram p and environment graph e and outputs the graph cor-
responding to the environment after executing the program,
or raises an exception if the program is not executable at a
certain step. Given a program with preconditions Φ(p), we
start by randomly perturbing them into Φ(p)′ and use eq. 10
to obtain e′ as an environment satisfying Φ(p)′. Then, we
execute p in the simulator with the environment e′. Given
that the environment and preconditions have changed but
the program is still the same, as the program is executed,
some exception will be raised from the simulator. Then,
a subroutine is called to modify the program p into p′, by
inserting or removing instructions to correct the exception,
obtaining the extended program.

For example, when we executing Sit Sofa, if Sofa
is occupied, the subroutine is expected to perform actions
to remove things on the Sofa until there is enough space
to Sit. We manually compose the subroutines based on
different types of exception, forming the knowledge base,
KB-ExceptionHandler. We show more details of how we
augment the programs in the supplementary materials.

This way, we augment over 30k tuples of sketches, envi-
ronments, and programs (sa, e′, p′). Note that the sketches
sa are environment-independent, so there is no need to
change them after applying the subroutines.

6295

Figure 4. The effect of the dataset augmentation: changes in the
distribution of preconditions for the objects in the environment.

5.4. Dataset Analysis

From the original 2807 programs in VirtualHome, we
trim out the programs that can not be executed in the simula-
tor with the environment sampled via preconditions, obtain-
ing 1387 executable programs. Using the process described
in Sec. 5.3, we extend these programs to our ﬁnal dataset
with around 30k programs. The signiﬁcant increase in the
program length is induced by the modiﬁed preconditions.
For example, the agent needs to open containers to reach
objects or make space to sit on a sofa. In Table ??, we show
the statistics of the new dataset. Fig. 4 shows the change
in the distribution of preconditions of the cup and the sauce
pan. The programs after augmentation show a less skewed
distribution of preconditions and therefore more diverse en-
vironments. Finally, we use the simulator to generate snap-
shots of the environment after executing each instruction of
a program, as show in ﬁg. ??. Note that some of the objects
in the program do not have a model in the simulator, so we
generate frames for a subset of 8421 programs.

6. Experiments

We split the dataset into train and test set in terms of dif-
ferent types of activities with ratio 7:3 and leave one apart-
ment for the test set. We aim to test the capability of our
model with novel activities and environments. We follow
the same split for sketch prediction, where we only keep the
original programs for the desc2sketch, since they contain
the collected descriptions, and use the available frames for
the demo2sketch task.

In this section, we describe the evaluation metrics, base-
lines. Next, we show the extensive experiment results of
ResActGraph. Finally, we analyze the extent to which the
proposed method is environment-aware. We will describe
the implementation details in supplementary materials.

6.1. Evaluation Metrics

We analyze the performance of sketch prediction and
program generation by measuring the normalized longest
common subsequence (LCS) between the generated and

ground truth sequences. LCS is sensitive to the order of the
sequences and allows gaps in between. To further measure
if the generated programs achieve the speciﬁed activities,
we compute the differences between the ﬁnal environment
graphs ˆG = Ψ(ˆp, e) and G = Ψ(p, e) using F1 scores3. In
particular, we only compared the sub-graph containing the
object instances mentioned in p and ˆp. We describe the de-
tails of F1( ˆG, G) in the supplementary materials. Inspired
by [2], we also compute F1-state and F1-relation.

Furthermore, inspired by program synthesis, we care
whether the generated programs are “compilable” as well.
We evaluate if the generated programs can be parsed
(parsibility) and executed (executability) by the simulators.
We will describe the detailed deﬁnition of them in the sup-
plementary materials.

6.2. Baselines

We implement ﬁve different baselines to compare with

the proposed ResActGraph.
Nearest Neighbors: For every example in the testing set,
we retrieve the training sample that has a sketch with the
highest LCS. In case of a tie, we pick the one with the most
similar initial graph.
Unaries: We set K = 0 in Eq. 7. This model does not
consider the relations of the objects. We use it to showcase
the beneﬁts of modeling object relations.
Graph: This model does not consider the change of graphs
induced by programs (Eq. 8).
FCActGraph:
model
[hKt−1
GRUActGraph: This model
as
[Emb( ˆαt−1), mt−1] as inputs and considers hKt−1
hidden state to output hKt
v .

the graph changes.
, Emb( ˆαt−1), mt−1] as inputs and outputs hKt
v .

treats the graph changes
a GRU to ingest
as

to
takes the

This model uses

a FC layer

Speciﬁcally,

and uses

sequence

another

it

v

v

6.3. Results

We show the results of ResActGraph quantitatively and
qualitatively. Next, we show the ablation study of the num-
ber of the graph propagation steps. Finally, we show the
prediction results of the whole system.

Program generation from sketches and graphs. The
results are shown in Table 1. By comparing the Graph and
Unaries, we show that aggregating information from neigh-
boring nodes increases performances in nearly all metrics.
The three bottom rows of Table 1 show the results of
models that consider graph changes induced by programs.
The F1 scores and executability beneﬁt the most, which
is expected. For example, suppose there is a glass near
an opened cabinet in the environment and the model pre-
dicts (Grab Glass, Put Glass Cabinet, Close

3If the generated programs cannot be parsed or cannot be executed, the

F1 is set to be 0.

6296

0.00.20.40.6inside coffe makernear dishwashernear sinkin home officeinside dishwashernear coffe makernear chairin kitcheninside coffe makerin living roomnear chairnear microwavein dining roomnear stoveinside stovenear coffe makerinside microwaveDistribution for coffee cuporiginalaugmented0.00.20.40.6near food foodin kitchennear dry pastafood chicken insidenear food eggnear cabinetnear stovenear food chickenfood food insidemilk insidenear kitchen counternear dry pastain dining roomclosednear fryingpannear food foodfood egg insidenear food foodnear milknear dishwasherDistribution for sauce panoriginalaugmentedLCS

F1-relation

F1-state

F1

Executability

Parsability

Nearest Neighbors

0.463

Unaries
Graph
FCActGraph
GRUActGraph
ResActGraph

0.39
0.526
0.515
0.517
0.519

0.537

0.188

0.4

0.423
0.426
0.432

0.051

0.179
0.397

0.4

0.428
0.436

0.100

0.188

0.4
0.4

0.429
0.436

-

22.93%
46.79%
48.23%
50.28%
51.1%

-

79.16%
78.9%
84.44%
84.74%
83.66%

Table 1. Induce program from ground truth sketches and ground truth graphs. (K = 2)

Sketch

GT Program

Generated Program

[Open] <washing machine>  
[Put] <basket> <washing machine> 
[Put] <soap>  <washing machine> 
[SwitchOn] <washing machine> 

Environment

Washing machine (1001) is closed
Washing machine (1001) is off
Washing machine (1001) is unplugged 
Washing machine (1001) in bathroom (1)
Soap (1002) inside Washing machine (1001)

[Walk] <bedroom> (273)
[Walk] <basket> (1000)
[Find] <basket> (1000)
[Grab] <basket> (1000)
[Walk] <bathroom> (1)
[Walk] <washing machine> (1001)
[Find] <washing machine> (1001)
[Open] <washing machine> (1001)
[Put] <basket> (1000) <washing machine> (1001)
[Find] <soap> (1002)
[Grab] <soap> (1002)
[Put] <soap> (1002) <washing machine> (1001)
[Find] <washing machine> (1001)
[Close] <washing machine> (1001)
[PlugIn] <washing machine> (1001)
[SwitchOn] <washing machine> (1001)

[Walk] <bedroom> (273) 
[Walk] <basket> (1000) 
[Find] <basket > (1000) 
[Grab] <basket > (1000)
[Find] <washing machine> (1001)
[Open] <washing machine> (1001)
[Put] <basket> (1000) <washing machine> (1001) 
[Find] <soap> (1002) 
[Grab] <soap> (1002) 
[Put] <soap> (1002) <washing machine> (1001) 
[Close] <washing machine> (1001)
[Plugin] <washing machine> (1001) 
[SwitchOn] <washing machine> (1001)

Figure 5. An example of the prediction of ResActGraph. We colore the LCS between the prediction and ground truth in light green. Note
that the sketch is environment agnostic, so it does not specify the ‘id’ (the number in the parentheses) of the object instances.

LCS

F1-relation F1-state

Unaries (K=0)
Graph (K=1)
Graph (K=2)
Graph (K=3)

ResActGraph (K=1)
ResActGraph (K=2)
ResActGraph (K=3)

0.39
0.48
0.526
0.527

0.49
0.519
0.536

0.18
0.34
0.4
0.39

0.38
0.432
0.415

0.17
0.36
0.397
0.39

0.39
0.436
0.418

F1
0.18
0.35
0.4
0.39

0.39
0.436
0.419

Table 2. Ablation study of the propagation steps K.

Cabinet) at the ﬁrst three steps, if the model wants to
grab other things from the cabinet without opening it after
t ≥ 4, it fails since the cabinet is closed at t = 3.

Among the three bottom rows of Table 1, the proposed
ResActGraph performs the best in F1. The reason is that us-
ing the residual architecture is easier for the model to learn
the state “changes” compared to using FC. Using GRU to
encode the state changes is also an alternative, but we ob-
serve that it converges slower since it has more number of
parameters to learn and does not perform better.

In Fig. 5, we show the qualitative results of ResAct-
Graph. Even though the generated program does not exactly

match the ground truth, it reaches nearly the same environ-
ment state. In Fig. 6, we show the results with the same
sketch, but different initial environment states. Note that we
only show the states and relations related to the programs.
The model correctly induces correct actions w.r.t.
the en-
vironment changes and the two generated programs nearly
reach the same environment states.

Ablation studies. We show the effect of the number of
propagation steps K in Table 2. Both the baseline and the
proposed model beneﬁt as K increases, and the proposed
model performs better than the baseline regardless of dif-
ferent K. We found that the performance saturates when
K = 2, so we ﬁxed it for all other experiments.

Combining predicted sketches with ResActGraph.
The LCS of fdemo2sketch and fdesc2sketch are 0.16 and 0.33
respectively. The reason why the LCS is low is that they are
signiﬁcantly shorter (on average 2.4 instructions) than the
programs (on average 18.79). This makes the sketch predic-
tion a quite challenging task where LCS is highly penalized
even under small errors.

With the trained model, we can directly generate the pro-
grams from the demonstrations or descriptions. Note that
we do not re-train the program generation model. In Ta-

6297

Sketch

[Sit] <sofa>
[Read] <book>

Environment 1

Generated Program 1 Environment 2

Generated Program 2

Book (263) in bedroom (23)
Bookmark (27)  near  book (263) 
Bookmark (27) in bedroom (23)
Sofa (101) in bedroom (23)
Sofa (101) near bookmark (275) 
Sofa (101) is free
Book (263) near sofa (1001) 

[Walk] <bedroom> (23)
[Walk] <book>  (263) 
[Find] <book> (263) 
[Grab] <book> (263) 
[Find] <sofa>  (101) 
[Sit] <sofa>  (101) 
[Find] <bookmark>  (27) 
[Read] <book> (263) 

Sofa (101) in bedroom (23)
Book (263) in bedroom (23)
Bookmark (27) in bedroom (23)
Sofa (101) occupied
Phone (75) on sofa (101) 
Cards (261) on sofa (101) 
Game (231) on sofa (101) 
Check (213)  on sofa (101) 

[Walk] <bedroom>  (23)
[Walk] <sofa>  (101) 
[Find] <sofa>  (101) 
[Find] <phone>  (75) 
[Grab] <phone> (75) 
[Release] <phone> (75) 
[Find] <cards>  (261) 
[Grab] <cards> (261) 
[Release] <cards> (261) 
[Find] <game> (231)

[Grab] <game>  (231) 
[Release] <game> (231) 
[Find] <check> (213)
[Grab] <check>  (213)
[Release] <check> (213)
[Sit] <sofa> (101) 
[Standup]  
[Walk] <book>  (263) 
[Find] <book>  (263) 
[Grab] <book>  (263) 
[Read] <book> (263) 

Figure 6. An example of the prediction of ResActGraph with the same sketch, but different environments. We highlight the difference
between two environments with orange and color the LCS between two predictions in light green.

Demonstration

Environment

Generated Sketch Generated Program

Phone (10) in living room (1)

[Grab] <phone>

[Walk] <living room> (1)
[Walk] <phone> (10)
[Find] <phone> (10)

[Walk] <phone> (10)
[TurnTo] <phone> (10)
[LookAt] <phone> (10)
[Grab] <phone> (10)

Description

Environment

Generated Sketch Generated Program

Walk into the home office. Walk up to 
the chair, sit down in the chair. Type 
with the keyboard.

Chair (29) close to keyboard (2)
Chair (29) is free
Keyboard (2) close to computer
Computer (31) in living room (1)

[Sit] <chair>
[Type] <keyboard>

[Walk] <living room> (1)
[Walk] <desk> (137)
[Find] <chair> (29)

[Sit] <chair> (29)
[Find] <keyboard> (2)
[Type] <keyboard> (2)

Figure 7. Predictions from the ResActGraph given sketches from descriptions and demonstrations.

Unaries
Graph
ResActGraph

LCS F1-relation F1-state
0.25
0.4
0.4

0.16
0.33
0.32

0.16
0.32
0.34

F1
0.16
0.33
0.33

Executability

Parsability

32.05%
43.69%
47.22%

86.71%
82.5%
80.94%

Table 3. Induce program from sketches predicted from decsrip-
tions and ground truth graphs. (K = 2)

Unaries
Graph
ResActGraph

LCS F1-relation F1-state
0.23
0.4
0.3

0.25
0.45
0.39

0.24
0.39
0.36

F1
0.24
0.4
0.35

Executability

Parsability

97.77%
77.77%
93.33%

68.88%

60%

66.66%

Table 4. Inducing program from sketches predicted from demon-
strations and ground truth graphs. (K = 2)

ble 3 and Table 4, we show the results of the program gen-
eration with the sketches predicted from descriptions and
demonstrations respectively. The performance gap between
the proposed model and the baselines becomes small. The
reason is that the model is confused when the non-perfect
sketches are given, resulting in similar performance. Note
that all models still perform better than Unaries. Qualitative
results are shown in Fig. 7, showing that the model predicts
the plausible sketch and ResActGraph generates plausible
programs w.r.t to the sketch and the graph.

7. Conclusion

In this work, we propose the environment-aware
program generation task. We introduce sketches as
environment-independent activity representations and ad-
dress the problem in two steps: generating sketches from
demonstrations or descriptions and generating programs
from sketches and graphs. To this end, we propose a novel
model, ResActGraph and create a dataset VirtualHome-Env,
with sketches, environments, and programs.

The environment-aware program generation is far from
being solved and opens exciting research directions. While
we access the truth state of the environment graph, one natu-
ral extension would be to predict it from environment obser-
vations [19], but this would still require an oracle provid-
ing images of the interior of closed objects or unexplored
areas. Additionally, though the ResActGraph updates the
hidden states of nodes at each time step, the graph structure
is not changed, which impedes from performing message
passing at each time step. Finally, our setting assumes a
fully observable environment. Working with partially ob-
servable environments opens up exciting directions, such as
dealing with environment changes due to factors external to
the agent or modelling the agent’s theory of mind.

Acknowledgement X.P. is supported by La Caixa Fellowship.

6298

[16] A. Solar-Lezama. Program synthesis by sketching. Citeseer,

2008. 1, 2, 5

[17] S.-H. Sun, H. Noh, S. Somasundaram, and J. Lim. Neural
program synthesis from diverse demonstration videos.
In
Proceedings of the 35th International Conference on Ma-
chine Learning, 2018. 2

[18] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Baner-
jee, S. Teller, and N. Roy. Understanding natural language
commands for robotic navigation and mobile manipulation.
In Proceedings of the Twenty-Fifth AAAI Conference on Arti-
ﬁcial Intelligence, AAAI’11, pages 1507–1514. AAAI Press,
2011. 2

[19] D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei. Scene graph gen-
eration by iterative message passing. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5410–5419, 2017. 8

[20] Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta,
R. Mottaghi, and A. Farhadi. Visual semantic planning using
deep successor representations. In Proceedings of the IEEE
International Conference on Computer Vision, pages 483–
492, 2017. 2

References

[1] M. Alomari, P. Duckworth, M. Hawasly, D. C. Hogg, and
A. G. Cohn. Natural language grounding and grammar in-
duction for robotic manipulation commands. In Proceedings
of the First Workshop on Language Grounding for Robotics,
pages 35–43, 2017. 2

[2] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice:
Semantic propositional image caption evaluation.
In Eu-
ropean Conference on Computer Vision, pages 382–398.
Springer, 2016. 6

[3] M. Beetz, U. Klank,

I. Kresse, A. Maldonado,
L. M¨osenlechner, D. Pangercic, T. R¨uhr, and M. Tenorth.
Robotic roommates making pancakes.
In Humanoid
Robots (Humanoids), 2011 11th IEEE-RAS International
Conference on, pages 529–536. IEEE, 2011. 2

[4] M. Boˇsnjak, T. Rockt¨aschel, J. Naradowsky, and S. Riedel.
Programming with a differentiable forth interpreter. arXiv
preprint arXiv:1605.06640, 2016. 2

[5] K. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio.
On the properties of neural machine translation: Encoder-
decoder approaches. arXiv preprint arXiv:1409.1259, 2014.
4

[6] L. Dong and M. Lapata. Coarse-to-ﬁne decoding for neural
semantic parsing. arXiv preprint arXiv:1805.04793, 2018. 2

[7] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot
In Conference

visual imitation learning via meta-learning.
on Robot Learning, pages 357–368, 2017. 2

[8] A. L. Gaunt, M. Brockschmidt, R. Singh, N. Kushman,
P. Kohli, J. Taylor, and D. Tarlow. Terpret: A probabilis-
tic programming language for program induction.
arXiv
preprint arXiv:1608.04428, 2016. 2

[9] D.-A. Huang, S. Nair, D. Xu, Y. Zhu, A. Garg, L. Fei-Fei,
S. Savarese, and J. C. Niebles. Neural task graphs: Gener-
alizing to unseen tasks from a single video demonstration,
2018. 2

[10] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel.
arXiv preprint

Gated graph sequence neural networks.
arXiv:1511.05493, 2015. 3

[11] H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and
walk: Neural mapping of navigational instructions to action
sequences, 2015. 2

[12] J. Merel, Y. Tassa, S. Srinivasan, J. Lemmon, Z. Wang,
G. Wayne, and N. Heess. Learning human behaviors from
motion capture by adversarial imitation.
arXiv preprint
arXiv:1707.02201, 2017. 2

[13] V. Murali, L. Qi, S. Chaudhuri, and C. Jermaine. Neural
sketch learning for conditional program generation. arXiv
preprint arXiv:1703.05698, 2017. 1, 2

[14] A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Ma-
lik, and S. Levine. Combining self-supervised learning and
imitation for vision-based rope manipulation.
In Robotics
and Automation (ICRA), 2017 IEEE International Confer-
ence on, pages 2146–2153. IEEE, 2017. 2

[15] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and
A. Torralba. Virtualhome: Simulating household activities
via programs. arXiv preprint arXiv:1806.07011, 2018. 1, 2

6299

