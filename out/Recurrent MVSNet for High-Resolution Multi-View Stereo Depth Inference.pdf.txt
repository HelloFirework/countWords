Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference

Yao Yao1∗

Zixin Luo1

Shiwei Li1

Tianwei Shen1

Tian Fang2†

Long Quan1

1The Hong Kong University of Science and Technology
{yyaoag, zluoag, slibc, tshenaa, quan}@cse.ust.hk

2Shenzhen Zhuke Innovation Technology (Altizure)

fangtian@altizure.com

Abstract

Deep learning has recently demonstrated its excellent
performance for multi-view stereo (MVS). However, one
major limitation of current learned MVS approaches is the
scalability: the memory-consuming cost volume regulariza-
tion makes the learned MVS hard to be applied to high-
resolution scenes.
In this paper, we introduce a scalable
multi-view stereo framework based on the recurrent neu-
ral network. Instead of regularizing the entire 3D cost vol-
ume in one go, the proposed Recurrent Multi-view Stereo
Network (R-MVSNet) sequentially regularizes the 2D cost
maps along the depth direction via the gated recurrent
unit (GRU). This reduces dramatically the memory con-
sumption and makes high-resolution reconstruction feasi-
ble. We ﬁrst show the state-of-the-art performance achieved
by the proposed R-MVSNet on the recent MVS benchmarks.
Then, we further demonstrate the scalability of the pro-
posed method on several large-scale scenarios, where pre-
vious learned approaches often fail due to the memory con-
straint. Code is available at https://github.com/
YoYo000/MVSNet.

1. Introduction

Multi-view stereo (MVS) aims to recover the dense repre-
sentation of the scene given multi-view images and cali-
brated cameras. While traditional methods [24, 10, 29, 9]
have achieved excellent reconstruction performance, recent
works [14, 13, 30] show that learned approaches are able to
produce results comparable to the traditional state-of-the-
arts. In particular, MVSNet [30] proposed a deep architec-
ture for depth map estimation, which signiﬁcantly boosts
the reconstruction completeness and the overall quality.

One of the key advantages of learning-based MVS is
the cost volume regularization, where most networks ap-

∗Intern at Shenzhen Zhuke Innovation Technology (Altizure).
†Corresponding author.

ply multi-scale 3D CNNs [14, 15, 30] to regularize the 3D
cost volume. However, this step is extremely memory ex-
pensive:
it operates on 3D volumes and the memory re-
quirement grows cubically with the model resolution (Fig. 1
(d)). Consequently, current learned MVS algorithms could
hardly be scaled up to high-resolution scenarios.

Recent works on 3D with deep learning also acknowl-
edge this problem. OctNet [23] and O-CNN [27] exploit the
sparsity in 3D data and introduce the octree structure to 3D
CNNs. SurfaceNet [14] and DeepMVS [13] apply the engi-
neered divide-and-conquer strategy to the MVS reconstruc-
tion. MVSNet [30] builds the cost volume upon the ref-
erence camera frustum to decouple the reconstruction into
smaller problems of per-view depth map estimation. How-
ever, when it comes to a high-resolution 3D reconstruction
(e.g., volume size > 5123 voxels), these methods will either
fail or take a long time for processing.

To this end, we present a novel scalable multi-view
stereo framework, dubbed as R-MVSNet, based on the re-
current neural network. The proposed network is built upon
the MVSNet architecture [30], but regularizes the cost vol-
ume in a sequential manner using the convolutional gated
recurrent unit (GRU) rather than 3D CNNs. With the se-
quential processing, the online memory requirement of the
algorithm is reduced from cubic to quadratic to the model
resolution (Fig. 1 (c)). As a result, the R-MVSNet is appli-
cable to high resolution 3D reconstruction with unlimited
depth-wise resolution.

We ﬁrst evaluate the R-MVSNet on DTU [1], Tanks and
Temples [17] and ETH3D [25] datasets, where our method
produces results comparable or even outperforms the state-
of-the-art MVSNet [30]. Next, we demonstrate the scal-
ability of the proposed method on several large-scale sce-
narios with detailed analysis on the memory consumption.
R-MVSNet is much more efﬁcient than other methods in
GPU memory and is the ﬁrst learning-based approach ap-
plicable to such wide depth range scenes, e.g., the advance
set of Tanks and Temples dataset [17].

5525

Required Memory: H × W

H × W

H × W

H × W × D

dmin

dmax

(a) Winner-take-all

dmin
dmax
(b) Spatial Regularization

dmin

dmax

(c) Recurrent Regularization (Proposed)

(d) 3D CNNs Regularization

Figure 1: Illustrations of different regularization schemes. For the interest red voxel, we use voxels in blue to denote its
receptive ﬁeld during the cost volume regularization. The runtime memory requirement is also listed on top of the volume,
where H, W and D denote the image height, width and depth sample number respectively. The 3D CNNs gather the cost
information across the whole space, however, requires a runtime memory cubical to the model resolution

2. Related Work

Learning-based MVS Reconstruction Recent learning-
based approaches have shown great potentials for MVS re-
construction. Multi-patch similarity [11] is proposed to re-
place the traditional cost metric with the learned one. Sur-
faceNet [14] and DeepMVS [13] pre-warp the multi-view
images to 3D space, and regularize the cost volume us-
ing CNNs. LSM [15] proposes differentiable projection
operations to enable the end-to-end MVS training. Our
approach is mostly related to MVSNet [30], which en-
codes camera geometries in the network as differentiable
homography and infers the depth map for the reference im-
age. While some methods have achieved excellent per-
formance in MVS benchmarks, aforementioned learning-
based pipelines are restricted to small-scale MVS recon-
structions due to the memory constraint.

Scalable MVS Reconstruction The memory require-
ment of learned cost volume regularizations [14, 15, 13, 5,
30] grows cubically with the model resolution, which will
be intractable when large image sizes or wide depth ranges
occur. Similar problem also exists in traditional MVS re-
constructions (e.g., semi-global matching [12]) if the whole
volume is taken as the input to the regularization. To mit-
igate the scalability issue, learning-based OctNet [23] and
O-CNN [27] exploit the sparsity in 3D data and introduce
the octree structure to 3D CNNs, but are still restricted to
reconstructions with resolution < 5123 voxels. Heuristic
divide-and-conquer strategies are applied in both classical
[18] and learned MVS approaches [14, 13], however, usu-
ally lead to the loss of global context information and the
slow processing speed.

On the other hand, scalable traditional MVS algorithms
all regularize the cost volume implicitly. They either ap-
ply local depth propagation [19, 9, 10, 24] to iteratively re-
ﬁne depth maps/point clouds, or sequentially regularize the

cost volume using simple plane sweeping [7] and 2D spatial
cost aggregation with depth-wise winner-take-all [28, 31].
In this work, we follow the idea of sequential processing,
and propose to regularize the cost volume using the con-
volutional GRU [6]. GRU is a RNN architecture [8] ini-
tially proposed for learning sequential speech and text data,
and is recently applied to 3D volume processing, e.g., video
sequence analysis [3, 34]. For our task, the convolutional
GRU gathers spatial as well as temporal context information
in the depth direction, which is able to achieve comparable
regularization results to 3D CNNs.

3. Network Architecture

This section describes the detailed network architecture of
R-MVSNet. Our method can be viewed as an extension to
the recent MVSNet [30] with cost volume regularization us-
ing convolutional GRU. We ﬁrst review the MVSNet archi-
tecture in Sec. 3.1, and then introduce the recurrent regular-
ization in Sec. 3.2 and the corresponding loss formulation
in Sec. 3.3.

3.1. Review of MVSNet

Given a reference image I1 and a set of its neighboring
source images {Ii}N
i=2, MVSNet [30] proposes an end-to-
end deep neural network to infer the reference depth map
D. In its network, deep image features {Fi}N
i=1 are ﬁrst
extracted from input images through a 2D network. These
2D image features will then be warped into the reference
camera frustum by differentiable homographies to build the
feature volumes {Vi}N
i=1 in 3D space. To handle arbitrary
N -view image input, a variance based cost metric is pro-
posed to map N feature volumes to one cost volume C.
Similar to other stereo and MVS algorithms, MVSNet reg-
ularizes the cost volume using the multi-scale 3D CNNs,
and regresses the reference depth map D through the soft
argmin [16] operation. A reﬁnement network is applied at

5526

…

Cr(0)

Cr(1)

Cr(2)

Cr(D-1)

Softmax

…
…
…
…

C(0)

C(1)

C(2)

C(D-1)

M

M

M

M

Loss

P

Q

One-hot

Feature Extraction

Recurrent  Regularization

Loss Computation

Conv + BN + ReLU, stride = 1
Conv + BN + ReLU, stride = 2

Conv, stride = 1
GRU unit

Cost Maps
Regularized Cost Maps

M Differentiable Homography Warping 

&Variance Cost Metric

GT Depth Map

Figure 2: The R-MVSNet architecture. Deep image features are extracted from input images and then warped to the fronto-
parallel planes of the reference camera frustum. The cost maps are computed at different depths and are sequentially regular-
ized by the convolutional GRU. The network is trained as a classiﬁcation problem with the cross-entropy loss

the end of MVSNet to further enhance the depth map qual-
ity. As deep image features {Fi}N
i=1 are downsized during
the feature extraction, the output depth map size is 1/4 to
the original image size in each dimension.

MVSNet has shown state-of-the-art performance on
DTU dataset [1] and the intermediate set of Tanks and
Temples dataset [17], which contain scenes with outside-
looking-in camera trajectories and small depth ranges.
However, MVSNet can only handle a maximum reconstruc-
tion scale at H × W × D = 1600× 1184× 256 with the 16
GB large memory Tesla P100 GPU, and will fail at larger
scenes e.g., the advanced set of Tanks and Temples. To
resolve the scalability issue especially for the wide depth
range reconstructions, we will introduce the novel recurrent
cost volume regularization in the next section.

3.2. Recurrent Regularization

Sequential Processing An alternative to globally regu-
larize the cost volume C in one go is to sequentially pro-
cess the volume through the depth direction. The simplest
sequential approach is the winner-take-all plane sweeping
stereo [7], which crudely replaces the pixel-wise depth
value with the better one and thus suffers from noise (Fig. 1
(a)). To improve, cost aggregation methods [28, 31] ﬁlter
the matching cost C(d) at different depths (Fig. 1 (b)) so as
to gather spatial context information for each cost estima-
tion. In this work, we follow the idea of sequential process-
ing, and propose a more powerful recurrent regularization
scheme based on convolutional GRU. The proposed method

is able to gather spatial as well as the uni-directional con-
text information in the depth direction (Fig. 1 (c)), which
achieves regularization results comparable to the full-space
3D CNNs but is much more efﬁcient in runtime memory.

Convolutional GRU Cost volume C could be viewed as
D cost maps {C(i)}D
i=1 concatenated in the depth direc-
If we denote the output of regularized cost maps
tion.
as {Cr(i)}D
i=1, for the ideal sequential processing at the
tth step, Cr(t) should be dependent on cost maps of the
current step C(t) as well as all previous steps {C(i)}t−1
i=1.
Speciﬁcally, in our network we apply a convolutional vari-
ant of GRU to aggregate such temporal context information
in depth direction, which corresponds to the time direction
in language processing. In the following, we denote ‘⊙’ as
the element-wise multiplication, ‘[]’ the concatenation and
‘∗’ the convolution operation. Cost dependencies are for-
mulated as:

Cr(t) = (1 − U(t)) ⊙ Cr(t − 1) + U(t) ⊙ Cu(t)

(1)

where U(t) is the update gate map to decide whether to up-
date the output for current step, Cr(t− 1) is the regularized
cost map of late step, and Cu(t) could be viewed as the
updated cost map in current step, which is deﬁned as:

Cu(t) = σc(Wc ∗ [C(t), R(t) ⊙ Cr(t − 1)] + bc)

(2)

R(t) here is the reset gate map to decide how much the pre-
vious Cr(t − 1) should affect the current update. σc(·) is

5527

(a) Reference Image

(b) Initial Depth Map

(c) Final Depth Map

(d) GT Depth Map

(e) Final Point Cloud

(f) Probability Map

(g) Depth Gradient

(h) Depth Gradient after Refinement

Figure 3: Reconstruction pipeline. (a) Image 24 of DTU [1] scan 15. (b) Initial depth map from the network. (c) Final
depth map (Sec. 4.3). (d) Ground truth depth map. (e) Point cloud output. (f) Probability estimation for depth map ﬁltering
(Sec. 4.3). (g) The gradient visualization of the initial depth map. (h) The gradient visualization after the reﬁnement (Sec. 4.2)

the nonlinear mapping, which is the element-wise sigmoid
function. The update gate and reset gate maps are also re-
lated to the current input and previous output:

R(t) = σg(Wr ∗ [C(t), Cr(t − 1)] + br)

U(t) = σg(Wu ∗ [C(t), Cr(t − 1)] + bu)

(3)

(4)

W and b are learned parameters. The nonlinear σg(·) is the
hyperbolic tangent to make soft decisions for the updates.

The convolutional GRU architecture not only spatially
regularizes the cost maps through 2D convolutions, but also
aggregates the temporal context information in depth di-
rection. We will show in the experiment section that our
GRU regularization can signiﬁcantly outperform the simple
winner-take-all or only the spatial cost aggregation.

Stacked GRU The basic GRU model is comprised of a
single layer. To further enhance the regularization ability,
more GRU units could be stacked to make a deeper network.
In our experiments, we adopt a 3-layer stacked GRU struc-
ture (Fig. 2). Speciﬁcally, we ﬁrst apply a 2D convolutional
layer to map the 32-channel cost map C(t) to 16-channel
as the input to the ﬁrst GRU layer. The output of each GRU
layer will be used as the input to the next GRU layer, and
the output channel numbers of the 3 layers are set to 16, 4, 1
respectively. The regularized cost maps {Cr(i)}D
i=1 will ﬁ-
nally go through a softmax layer to generate the probability
volume P for calculating the training loss.

3.3. Training Loss

Most deep stereo/MVS networks regress the disparity/depth
outputs using the soft argmin operation [16], which can be
interpreted as the expectation value along the depth direc-
tion [30]. The expectation formulation is valid if depth val-
ues are uniformly sampled within the depth range. However,
in recurrent MVSNet, we apply the inverse depth to sample
the depth values in order to efﬁciently handle reconstruc-
tions with wide depth ranges. Rather than treat the problem
as a regression task, we train the network as a multi-class
classiﬁcation problem with cross entropy loss:

Loss = X

(cid:16)

p

D

X

i=1

−P(i, p) · log Q(i, p)(cid:17)

(5)

where p is the spatial image coordinate and P(i, p) is a
voxel in the probability volume P. Q is the ground truth
binary occupancy volume, which is generated by the one-
hot encoding of the ground truth depth map. Q(i, p) is the
corresponding voxel to P(i, p).

One concern about the classiﬁcation formulation is the
discretized depth map output [32, 20, 13]. To achieve sub-
pixel accuracy, a variational depth map reﬁnement algo-
rithm is proposed in Sec. 4.2 to further reﬁne the depth map
output. In addition, while we need to compute the whole
probability volume during training, for testing, the depth
map can be sequentially retrieved from the regularized cost
maps using the winner-take-all selection.

5528

4. Reconstruction Pipeline

The proposed network in the previous section generates
the depth map per-view. This section describes the non-
learning parts of our 3D reconstruction pipeline.

4.1. Preprocessing

To estimate the reference depth map using R-MVSNet, we
need to prepare: 1) the source images {Ii}N
i=2 of the given
reference image I1, 2) the depth range [dmin, dmax] of the
reference view and 3) the depth sample number D for sam-
pling depth values using the inverse depth setting.

For selecting the source images, we follow MVSNet [30]
to score each image pair using a piece-wise Gaussian func-
tion w.r.t. the baseline angle of the sparse point cloud [33].
The neighboring source images are selected according to
the pair scores in descending order. The depth range is also
determined by the sparse point cloud with the implementa-
tion of COLMAP [24]. Depth samples are chosen within
[dmin, dmax] using the inverse depth setting and we deter-
mine the total depth sample number D by adjusting the tem-
poral depth resolution to the spatial image resolution (de-
tails are described in the supplementary material).

4.2. Variational Depth Map Reﬁnement

As mentioned in Sec. 3.3, a depth map will be retrieved
from the regularized cost maps through the winner-take-all
selection. Compare to the soft argmin [16] operation, the
argmax operation of winner-take-all cannot produce depth
estimations with sub-pixel accuracy. To alleviate the stair
effect (see Fig. 3 (g) and (h)), we propose to reﬁne the
depth map in a small depth range by enforcing the multi-
view photo-consistency.

Given the reference image I1, the reference depth map
D1 and one source image Ii, we project Ii to I1 through D1
to form the reprojected image Ii→1. The image reprojection
error between I1 and Ii→1 at pixel p is deﬁned as:

Ei(p) = Ei

photo(p) + Ei

smooth(p)
= C(I1(p), Ii→1(p)) + X

′∈N (p)

p

S(p, p′)

(6)

where Ei
photo is the photo-metric error between two pixels,
Ei
smooth is the regularization term to ensure the depth map
smoothness. We choose the zero-mean normalized cross-
correlation (ZNCC) to measure the photo-consistency C(·),
and use the bilateral squared depth difference S(·) between
p and its neighbors p′ ∈ N (p) for smoothness.
During the reﬁnement, we iteratively minimize the to-
tal image reprojection error between the reference image
and all source images E = Pi Pp Ei→1(p) w.r.t. depth
map D1. It is noteworthy that the initial depth map from
R-MVSNet has already achieved satisfying result. The pro-

posed variational reﬁnement only ﬁne-tunes the depth val-
ues within a small range to achieve sub-pixel depth accu-
racy, which is similar to the quadratic interpolation in stereo
methods [32, 20] and the DenseCRF in DeepMVS [13].

4.3. Filtering and Fusion

Similar to other depth map based MVS approaches[10, 24,
30], we ﬁlter and fuse depth maps in R-MVSNet into a sin-
gle 3D point cloud. The photo-metric and the geometric
consistencies are considered in depth map ﬁltering. As de-
scribed in previous sections, the regularized cost maps will
go through a softmax layer to generate the probability vol-
ume. In our experiments, we take the corresponding proba-
bility of the selected depth value as its conﬁdence measure-
ment (Fig. 3 (f)), and we will ﬁlter out pixels with probabil-
ity lower than a threshold of 0.3. The geometric constraint
measures the depth consistency among multiple views, and
we follow the geometric criteria in MVSNet [30] that pixels
should be at least three view visible. For depth map fusion,
we apply the visibility-based depth map fusion [21] as well
as the mean average fusion [30] to further enhance the depth
map quality and produce the 3D point cloud. Illustrations
of our reconstruction pipeline are shown in Fig. 3.

5. Experiments

5.1. Implementation

Training We train R-MVSNet on the DTU dataset [1],
which contains over 100 scans taken under 7 different light-
ing conditions and ﬁxed camera trajectories. While the
dataset only provides the ground truth point clouds, we
follow MVSNet [30] to generate the rendered depth maps
for training. The training image size is set to W × H =
640 × 512 and the input view number is N = 3. The
depth hypotheses are sampled from 425mm to 905mm with
D = 192. In addition, to prevent depth maps from being
biased on the GRU regularization order, each training sam-
ple is passed to the network with forward GRU regulariza-
tion from dmin to dmax as well as the backward regulariza-
tion from dmax to dmin. The dataset is split into the same
training, validation and evaluation sets as previous works
[14, 30]. We choose TensorFlow [2] for the network imple-
mentation, and the model is trained for 100k iterations with
batch size of 1 on a GTX 1080Ti graphics card. RMSProp is
chosen as the optimizer and the learning rate is set to 0.001
with an exponential decay of 0.9 for every 10k iterations.

Testing For testing, we use N = 5 images as input, and
the inverse depth samples are adaptively selected as de-
scribed in Sec. 4.1. For Tanks and Temples dataset, the
camera parameters are computed from OpenMVG [22] as
suggested by MVSNet [30]. Depth map reﬁnement, ﬁlter-
ing and fusion are implemented using OpenGL on the same
GTX 1080Ti GPU.

5529

5.2. Benchmarks

Mean Acc. Mean Comp. Overall (mm)

We ﬁrst demonstrate the state-of-the-art performance of the
proposed R-MVSNet, which produces results comparable
to or outperforms the previous MVSNet [30].

DTU Dataset [1] We evaluate the proposed method on
the DTU evaluation set. To compare R-MVSNet with
MVSNet [30], we set [dmin, dmax] = [425, 905] and D =
256 for all scans. Quantitative results are shown in Table 1.
The accuracy and the completeness are calculated using the
matlab script provided by the DTU dataset. To summa-
rize the overall reconstruction quality, we calculate the av-
erage of the mean accuracy and the mean completeness as
the overall score. Our R-MVSNet produces the best recon-
struction completeness and overall score among all meth-
ods. Qualitative results can be found in Fig. 4.

Tanks and Temples Benchmark [17] Unlike the indoor
DTU dataset, Tanks and Temples is a large dataset captured
in more complex environments. Speciﬁcally, the dataset is
divided into the intermediate and the advanced sets. The in-
termediate set contains scenes with outside-look-in camera
trajectories, while the advanced set contains large scenes
with complex geometric layouts, where almost all previous
learned algorithms fail due to the memory constraint.

The proposed method ranks 3rd on the intermediate set,
which outperforms the original MVSNet [30]. Moreover,
R-MVSNet successfully reconstructs all scenes and also
ranks 3rd on the advanced set. The reconstructed point
clouds are shown in Fig. 5. It is noteworthy that the bench-
marking result of Tanks and Temples is highly dependent on
4 × W
the point cloud density. Our depth map is of size H
4 ,
which is relatively low-resolution and will result in low re-
construction completeness. So for the evaluation, we lin-
early upsample the depth map from the network by two
( H
2 × W
2 ) before the depth map reﬁnement. The f scores
of intermediate and advanced sets increase from 43.48 to
48.40 and from 24.91 to 29.55 respectively.

ETH3D Benchmark [25] We also evaluate our method
on the recent ETH3D benchmark. The dataset is divided
into the low-res and the high-res scenes, and provides the
ground truth depth maps for MVS training. We ﬁrst ﬁne-
tune the model on the ETH3D low-res training set, however,
observe no performance gain compared to the model only
pre-trained on DTU. We suspect the problem may be some
images in low-res training set are blurred and overexposed
as they are captured using hand-held devices. Also, the
scenes of ETH3D dataset are complicated in object occlu-
sions, which are not explicitly handled in the proposed net-
work. We evaluate on this benchmark without ﬁne-tuning
the network. Our method achieves similar performance to
MVSNet [30] and ranks 6th on the low-res benchmark.

Camp [4]
Furu [9]
Tola [26]

Gipuma [10]
Colmap [10]

SurfaceNet [14]

MVSNet (D=256) [30]

R-MVSNet (D=256)
R-MVSNet (D=512)

0.835
0.613
0.342
0.283
0.400
0.450
0.396
0.385
0.383

0.554
0.941
1.19
0.873
0.664
1.04
0.527
0.459
0.452

0.695
0.777
0.766
0.578
0.532
0.745
0.462
0.422
0.417

Table 1: Quantitative results on the DTU evaluation scans
[1]. R-MVSNet outperforms all methods in terms of recon-
struction completeness and overall quality

 

0
1
n
a
c
S

 

3
2
n
a
c
S

R-MVSNet

Ground Truth

Figure 4: Our results and the ground truth point clouds of
scans 10 and 23, DTU [1] dataset

5.3. Scalability

Next, we demonstrate the scalability of R-MVSNet from:
1) wide-range and 2) high-resolution depth reconstructions.

Wide-range Depth Reconstructions The memory re-
quirement of R-MVSNet is independent to the depth sample
number D, which enables the network to infer depth maps
with large depth range that is unable to be recovered by pre-
vious learning-based MVS methods. Some large scale re-
constructions of Tanks and Temples dataset are shown in
Fig. 5. Table 2 compares MVSNet [30] and R-MVSNet
in terms of benchmarking rankings, reconstruction scales
and memory requirements. We deﬁne the algorithm’s mem-
ory utility (Mem-Util) as the size of volume processed per
memory unit ( H
4 × D / runtime memory size). R-
MVSNet is ×8 more efﬁcient than MVSNet in Mem-Util.
High-resolution Depth Reconstructions R-MVSNet
can also produce high-resolution depth reconstructions by
sampling denser in depth direction. For the DTU evaluation
in Sec. 5.2, if we ﬁx the depth range and change the depth
sample number from D = 256 to D = 512, the overall
distance score will be reduced from 0.422mm to 0.419mm
(see last row of Table 1).

4 × W

5530

Dataset

DTU [1]
T. Int. [17]
T. Adv. [17]
ETH3D [25]

Rank

H

W Ave. D Mem. Mem-Util Rank

H

MVSNet[30]

R-MVSNet (Ours)
W Ave. D Mem. Mem-Util

Mem-Util

Ratio

2
4
-
5

1600
1920

-

928

1184
1072

-

480

256
256

-

320

15.4 GB
15.3 GB

-

1.97 M
2.15 M

-

8.7 GB

1.02 M

1
3
3
6

1600
1920
1920
928

1200
1080
1080
480

512
898
698
351

6.7 GB
6.7 GB
6.7 GB
2.1 GB

9.17 M
17.4 M
13.5 M
4.65 M

4.7
8.1

-

4.6

Table 2: Comparisons between MVSNet [30] and the proposed R-MVSNet on benchmarking rankings, reconstruction scales
and GPU memory requirements on the three MVS datasets [1, 17, 25]. The memory utility (Mem-Util) measures the data
size processed per memory unit, and the high ratio between the two algorithms reﬂects the scalability of R-MVSNet

(a) Courtroom

(b) Panther

(c) Ballroom

(d) Palace

(e) Horse

(f) Train

Figure 5: Point cloud reconstructions of Tanks and Temples dataset [17]

5.4. Ablation studies

5.4.1 Networks

This section studies how different components in the net-
work affect the depth map reconstruction. We perform
the study on DTU validation set with W × H × D =
640×512×256, and use the average absolute difference be-
tween the inferred and the ground truth depth maps for the
quantitative comparison. We denote the learned 2D image
features as 2D CNNs. The comparison results of following
settings are shown Fig. 6 and Fig. 7:

2D CNNs + 3D CNNs Replace the GRU regularization
with the same 3D CNNs regularization in MVSNet [30].
As shown in Fig. 6 and Fig. 7, 3D CNNs produces the best
depth map reconstructions.

2D CNNs + GRU The setting of the proposed R-
MVSNet, which produces the 2nd best depth map results
among all settings. The qualitative comparison between 3D
CNNs and GRU is shown in Fig. 7 (d) and (e).

2D CNNs + Spatial Replace the GRU regularization with
the simple spatial regularization. We approach the spatial
regularization by a simple 3-layer, 32-channel 2D network
on the cost map. The depth map error of spatial regulariza-
tion is larger than the GRU regularization.

2D CNNs + Winner-Take-All Replace the GRU regular-
ization with simple the winner-take-all selection. We apply
a single layer, 1-channel 2D CNN to directly map the cost
map to the regularized cost map. The depth map error is
further larger than the spatial regularization.

ZNCC + Winner-Take-All Replace the learned image
feature and cost metric with the engineered ZN CC (win-
dow size of 7 × 7). This setting is also referred to the clas-
sical plane sweeping [7]. As expected, plane sweeping pro-
duces the highest depth map error among all methods.

5.4.2 Post-processing

Next, we study the inﬂuences of post processing steps on the
ﬁnal point cloud reconstruction. We reconstruct the DTU
evaluation without the variational reﬁnement, photo-metric
ﬁltering, geometric ﬁltering or depth map fusion. Quantita-
tive results are shown in Table 3.

Without Variational Reﬁnement This setting is similar
to the post-processing of MVSNet [30]. The f score is
changed to a larger number of 0.465, demonstrating the ef-
fectiveness of the proposed depth map reﬁnement.

Without Photo-metric Filtering Table 3 shows that the
f score without photo-metric ﬁltering is increased to a larger

5531

Error 
/ mm
64.0

32.0

16.0

8.0

4.0

2D CNNs + 3D CNNs
2D CNNs + WTA

2D CNNs + GRU
ZNCC + WTA

2D CNNs + Spatial

20k

40k

60k

# Iterations

80k

100k

Ref.
√
×
√
√
√
×
×

Pho. Geo.
√
√
√
√
√
×
√
×
√
√
√
√
√
×

Fus. Acc.
√
0.385
√
0.444
√
0.550
√
0.479
×
0.498
×
0.605
√
0.591

Comp. Overall
0.422
0.459
0.465
0.486
0.384
0.467
0.432
0.385
0.431
0.364
0.489
0.373
0.411
0.501

Figure 6: Ablation studies on network architectures, which
demonstrate the importance of learned features and learned
regularization. WTA is referred to Winner-Take-All and
the ﬁgure records testing depth map errors during training

Table 3: Ablation studies on different combinations of
variational Refinement, Photo-metic ﬁltering, Geometry
ﬁltering and depth map Fusion for post-processing. Tested
on DTU [1] evaluation set

(a) ZNCC + WTA

(b) 2D CNNs + WTA

(c) 2D CNNs + spatial

(d) 2D CNNs + GRU

(e) 2D CNNs + 3D CNNs

Figure 7: Depth map reconstructions of scan 11, DTU dataset [1] using different image features and cost volume regulariza-
tion methods. All models are trained for 100k iterations

number of 0.467, which demonstrates the importance of the
probability map for photo-metric ﬁltering (Fig. 3 (f)).

Without Geo-metric Filtering The f score is increased
to 0.432, showing the effectiveness of depth consistency.

Without Depth Map Fusion The f score is also in-
creased to 0.431, showing the effectiveness of depth fusion.

5.5. Discussion

Running Time For DTU evaluation with D = 256, R-
MVSNet generates the depth map at a speed of 9.1s / view.
Speciﬁcally, it takes 2.9s to infer the initial depth map and
6.2s to perform the depth map reﬁnement. It is noteworthy
that the runtime of depth map reﬁnement only relates to re-
ﬁnement iterations and the input image size. Filtering and
fusion takes neglectable runtime.

Generalization R-MVSNet is trained with ﬁxed input
size of N×W×H×D = 3×640×512×256, but it is appli-
cable to arbitrary input size during testing. It is noteworthy
that we use the model trained on the DTU dataset [1] for all
our experiments without ﬁne-tuning. While R-MVSNet has
shown satisfying generalizability to the other two datasets
[17, 25], we hope to train R-MVSNet on a more diverse
MVS dataset, and expect better performances on Tanks and
Temples [17] and ETH3D [25] benchmarks in the future.

Limitation on Image Resolution While R-MVSNet is
applicable to reconstructions with unlimited depth-wise res-
olution, the reconstruction scale is still restricted to the input
image size. Currently R-MVSNet can handle a maximum
input image size of 3072 × 2048 on a 11GB GPU, which
covers all modern MVS benchmarks except for the ETH3D
high-res benchmark (6000 × 4000).
6. Conclusions

We presented a scalable deep architecture for high-
resolution multi-view stereo reconstruction. Instead of us-
ing 3D CNNs, the proposed R-MVSNet sequentially regu-
larizes the cost volume through the depth direction with the
convolutional GRU, which dramatically reduces the mem-
ory requirement for learning-based MVS reconstructions.
Experiments show that with the proposed post-processing,
R-MVSNet is able to produce high quality benchmarking
results as the original MVSNet [30]. Also, R-MVSNet is
applicable to large-scale reconstructions which cannot be
handled by the previous learning-based MVS approaches.

7. Acknowledgement

This work is supported by Hong Kong RGC GRF
16203518, Hong Kong T22-603/15N, ITC PSKL12EG02.
We thank the support of Google Cloud Platform.

5532

References

[1] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B.
Dahl. Large-scale data for multiple-view stereopsis. Inter-
national Journal of Computer Vision (IJCV), 2016.

[2] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org.

[3] N. Ballas, L. Yao, C. Pal, and A. Courville. Delving deeper
into convolutional networks for learning video representa-
tions. International Conference on Learning Representations
(ICLR), 2016.

[4] N. D. Campbell, G. Vogiatzis, C. Hern´andez, and R. Cipolla.
Using multiple hypotheses to improve depth-maps for multi-
view stereo. European Conference on Computer Vision
(ECCV), 2008.

[5] J.-R. Chang and Y.-S. Chen. Pyramid stereo matching net-
work. Computer Vision and Pattern Recognition (CVPR),
2018.

[6] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase
representations using rnn encoder–decoder for statistical ma-
chine translation. Empirical Methods in Natural Language
Processing (EMNLP), 2014.

[7] R. T. Collins. A space-sweep approach to true multi-
image matching. Computer Vision and Pattern Recognition
(CVPR), 1996.

[8] J. L. Elman. Finding structure in time. Cognitive science,

1990.

[9] Y. Furukawa and J. Ponce. Accurate, dense, and robust multi-
view stereopsis. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI), 2010.

[10] S. Galliani, K. Lasinger, and K. Schindler. Massively paral-
lel multiview stereopsis by surface normal diffusion. Inter-
national Conference on Computer Vision (ICCV), 2015.

[11] W. Hartmann, S. Galliani, M. Havlena, L. Van Gool, and
K. Schindler. Learned multi-patch similarity. International
Conference on Computer Vision (ICCV), 2017.

[12] H. Hirschmuller. Stereo processing by semiglobal match-
ing and mutual information. IEEE Transactions on Pattern
Analysis and Machine Intelligence (TPAMI), 2008.

[13] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang.
Deepmvs: Learning multi-view stereopsis. Computer Vision
and Pattern Recognition (CVPR), 2018.

[16] A. Kendall, H. Martirosyan, S. Dasgupta, and P. Henry. End-
to-end learning of geometry and context for deep stereo re-
gression. Computer Vision and Pattern Recognition (CVPR),
2017.

[17] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun. Tanks
and temples: Benchmarking large-scale scene reconstruc-
tion. ACM Transactions on Graphics (TOG), 2017.

[18] A. Kuhn, H. Hirschm¨uller, D. Scharstein, and H. Mayer. A tv
prior for high-quality scalable multi-view stereo reconstruc-
tion. International Journal of Computer Vision (IJCV), 2017.
[19] M. Lhuillier and L. Quan. A quasi-dense approach to surface
reconstruction from uncalibrated images.
IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
2005.

[20] W. Luo, A. G. Schwing, and R. Urtasun. Efﬁcient deep learn-
ing for stereo matching. Computer Vision and Pattern Recog-
nition (CVPR), 2016.

[21] P. Merrell, A. Akbarzadeh, L. Wang, P. Mordohai, J.-M.
Frahm, R. Yang, D. Nist´er, and M. Pollefeys. Real-time
visibility-based fusion of depth maps.
International Con-
ference on Computer Vision (ICCV), 2007.

[22] P. Moulon, P. Monasse, R. Marlet, and Others. Openmvg. an
open multiple view geometry library. https://github.
com/openMVG/openMVG.

[23] G. Riegler, A. O. Ulusoy, and A. Geiger. Octnet: Learning
deep 3d representations at high resolutions. Computer Vision
and Pattern Recognition (CVPR), 2017.

[24] J. L. Sch¨onberger, E. Zheng, J.-M. Frahm, and M. Pollefeys.
Pixelwise view selection for unstructured multi-view stereo.
European Conference on Computer Vision (ECCV), 2016.

[25] T. Sch¨ops, J. L. Sch¨onberger, S. Galliani, T. Sattler,
K. Schindler, M. Pollefeys, and A. Geiger. A multi-view
stereo benchmark with high-resolution images and multi-
camera videos. Computer Vision and Pattern Recognition
(CVPR), 2017.

[26] E. Tola, C. Strecha, and P. Fua. Efﬁcient large-scale multi-
view stereo for ultra high-resolution image sets. Machine
Vision and Applications (MVA), 2012.

[27] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong.
O-cnn: Octree-based convolutional neural networks for 3d
shape analysis. ACM Transactions on Graphics (TOG),
2017.

[28] Q. Yang. A non-local cost aggregation method for stereo
Computer Vision and Pattern Recognition

matching.
(CVPR), 2012.

[29] Y. Yao, S. Li, S. Zhu, H. Deng, T. Fang, and L. Quan. Rela-
tive camera reﬁnement for accurate dense reconstruction. 3D
Vision (3DV), 2017.

[30] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan. Mvsnet: Depth
inference for unstructured multi-view stereo. European Con-
ference on Computer Vision (ECCV), 2018.

[14] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang. Surfacenet:
An end-to-end 3d neural network for multiview stereopsis.
International Conference on Computer Vision (ICCV), 2017.

[31] K.-J. Yoon and I. S. Kweon. Adaptive support-weight ap-
IEEE Transactions on

proach for correspondence search.
Pattern Analysis and Machine Intelligence (TPAMI), 2006.

[15] A. Kar, C. H¨ane, and J. Malik. Learning a multi-view stereo
machine. Advances in Neural Information Processing Sys-
tems (NIPS), 2017.

[32] J. Zbontar and Y. LeCun. Stereo matching by training a con-
volutional neural network to compare image patches. Jour-
nal of Machine Learning Research (JMLR), 2016.

5533

[33] R. Zhang, S. Li, T. Fang, S. Zhu, and L. Quan. Joint cam-
era clustering and surface segmentation for large-scale multi-
view stereo. International Conference on Computer Vision
(ICCV), 2015.

[34] X. Zhu, J. Dai, X. Zhu, Y. Wei, and L. Yuan. Towards
high performance video object detection for mobiles. arXiv
preprint arXiv 1804.05830, 2018.

5534

