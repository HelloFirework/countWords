On Exploring Undetermined Relationships for Visual Relationship Detection

Yibing Zhan1, Jun Yu∗1, Ting Yu1 and Dacheng Tao2

1Key Laboratory of Complex Systems Modeling and Simulation,

School of Computer Science and Technology, Hangzhou Dianzi University, China

2UBTECH Sydney AI Centre, School of Computer Science, FEIT, University of Sydney,

{zybjy,yujun}@hdu.edu.cn, yuting@zufedfc.edu.cn, dacheng.tao@sydney.edu.au

Darlington, NSW 2008, Australia

Abstract

In visual relationship detection, human-notated relation-
ships can be regarded as determinate relationships. How-
ever, there are still large amount of unlabeled data, such as
object pairs with less signiﬁcant relationships or even with
no relationships. We refer to these unlabeled but potentially
useful data as undetermined relationships. Although a vast
body of literature exists, few methods exploit these undeter-
mined relationships for visual relationship detection.

In this paper, we explore the beneﬁcial effect of unde-
termined relationships on visual relationship detection. We
propose a novel multi-modal feature based undetermined
relationship learning network (MF-URLN) and achieve
great improvements in relationship detection. In detail, our
MF-URLN automatically generates undetermined relation-
ships by comparing object pairs with human-notated data
according to a designed criterion. Then, the MF-URLN
extracts and fuses features of object pairs from three com-
plementary modals: visual, spatial, and linguistic modals.
Further, the MF-URLN proposes two correlated subnet-
works: one subnetwork decides the determinate conﬁdence,
and the other predicts the relationships. We evaluate the
MF-URLN on two datasets: the Visual Relationship De-
tection (VRD) and the Visual Genome (VG) datasets. The
experimental results compared with state-of-the-art meth-
ods verify the signiﬁcant improvements made by the unde-
termined relationships, e.g., the top-50 relation detection
recall improves from 19.5% to 23.9% on the VRD dataset.

1. Introduction

Visual relationships have been widely used in multiple
image understanding tasks, such as object categorization

∗Jun Yu is the corresponding author

Figure 1. This ﬁgure shows four images with different relation-
ships. (a) and (b) surround the descriptions of object pairs with
determinate relationships. In addition, (b) presents an unlabeled
relationship: person-on-street. (c) and (d) outline the descriptions
of object pairs with undetermined relationships. In (d), the two
objects are detected by Faster R-CNN [29].

[8], object detection [14], image segmentation [11], image
captioning [6], and human-object interactions [10]. Due to
its wide applications, visual relationship detection has at-
tracted increasingly more attention. The goal of visual rela-
tionship detection is to detect pairs of objects, meanwhile
to predict the object pairs’ relationships.
In visual rela-
tionship detection, visual relationships are generally repre-
sented as subject-predicate-object triplets, such as person-
wear-hat and person-on-horse, which are shown in Fig. 1
(a) and (b). Since relation triplets are compositions of ob-
jects and predicates, their distribution is long-tailed. For N
objects and M predicates, the number of all possible rela-
tion triplets is O(N 2M ). Therefore, taking relation triplets

5128

(a) Person–Wear–HatPersonHat(b) Person–On–HorsePersonHorse(d) Monitor–LampFalsely Detected LampMonitorLampBikeCar(c) Bike–CarIrrelevant ObjectsPersonStreetas a whole learning task requires a very large amount of
labeling data [4, 7]. A better strategy is to build separate
modules for objects and predicates. Such a strategy reduces
the complexity to O(N + M ) and increases the detection
performance on large-scale datasets [23]. Even so, visual
relationship detection is still data-hungry. Another solution
is to obtain more human annotations. However, labeling
relation triplets is truly expensive, as it requires tedious in-
spections of huge numbers of object interactions [38].

We notice that in addition to the human-notated relation-
ships, there is still much unexploited data in images. We
illuminate these data in Fig. 1. The human-notated relation-
ships can be regarded as determinate relationships, such as
Fig. 1 (a) and (b). Contrarily, we refer to the other rela-
tionships constructed from unlabeled object pairs as unde-
termined relationships. These undetermined relationships
include 1) object pairs with relationships but not labeled
by humans, e.g., the unlabeled person-on-street shown in
Fig. 1 (b), and 2) object pairs with no relationships, such
as Fig. 1 (c). Further, object pairs with falsely detected
objects are also classiﬁed as undetermined relationships,
such as Fig. 1 (d).
Intuitively, these undetermined rela-
tionships can be used as complements of determinate rela-
tionships for the following reasons. First, they contain neg-
ative samples, such as object pairs without relationships and
with falsely detected objects. Second, they reﬂect humans’
dislike preferences, e.g., the less signiﬁcant unlabeled rela-
tionships and relationships with unusual expressions (e.g.,
we prefer to say cup-on-table instead of table-under-cup),
which are considered as undetermined relationships. More-
over, they require no human annotation and have beneﬁcial
regular effects for visual relationship detection [39].

Therefore, in this paper, we explore how to utilize these
unlabeled undetermined relationships to improve relation-
ship detection, and propose a multi-modal feature based
undetermined relationship learning network (MF-URLN).
In the MF-URLN, a generator is proposed to automatically
produce useful undetermined relationships. Speciﬁcally, we
use an object detector to detect objects, and two differ-
ent objects compose an object pair; then, this object pair
is compared with human-notated relationships using a de-
signed criterion. Those object pairs without corresponding
determinate relationships are classiﬁed as undetermined re-
lationships. For each object pair, the MF-URLN extracts
and fuses features from three different modals: the visual
modal, the spatial modal, and the linguistic modal. These
features comprehensively gather information on one rela-
tionship. Afterwards, the MF-URLN constructs two cor-
related subnetworks: one depicts the object pairs as either
determinate or undetermined, and the other predicts the re-
lationships. In addition, the second subnetwork uses infor-
mation from the ﬁrst subnetwork. The ﬁnal relationships
are decided according to the scores from the two subnet-

works. We perform experiments on two relationship detec-
tion datasets, namely VRD [23] and VG [18, 37], to ver-
ify the effectiveness of the MF-URLN. The experimental
results demonstrate that the MF-URLN achieves great im-
provements on both datasets by using undetermined rela-
tionships, e.g., the top-50 phrase detection recall improves
from 25.2% to 31.5 % in the VRD dataset.

Our contributions can be summarized as: 1) we explore
undetermined relationships to improve visual relationship
detection. We propose an automatic method to obtain effec-
tive undetermined relationships and a novel model to utilize
these undetermined relationships for visual relationship de-
tection. 2) We propose a novel and competitive visual rela-
tionship detection method, the MF-URLN, by using multi-
modal features based on determinate and undetermined re-
lationships. The experimental results, when compared with
state-of-the-art methods, demonstrate the capability of the
MF-URLN for visual relationship detection.

2. Related Work

Visual Relationship Detection. Earlier works on visual
relationship detection treated object and predicate detection
as a single task. These methods required a large amount of
training data, but could be applied only to limited situations
[4, 7]. Then, Lu et al. [23] proposed an efﬁcient strategy
for detecting the objects and predicates separately. Later,
linguistic knowledge showed its power. Yu et al. [34] com-
bined rich visual and linguistic representations using the
teacher-student deep learning framework. Deep structural
learning is another recent attempt.
In [21], a deep struc-
tural ranking model was proposed by integrating multiple
cues to predict the relationships. The methods mentioned
above contained two steps for objects and predicates.
In
contrast, other methods had end-to-end models. In [37], the
authors proposed a end-to-end visual translation embedding
network for relationship detection. Although the previous
methods performed satisfactorily, few of them took unde-
termined relationships into account.

Positive Unlabeled Learning. Utilization of undeter-
mined relationships is related to positive unlabeled (PU)
learning. PU learning refers to the task of learning a bi-
nary classiﬁer from only positive and unlabeled data [5].
PU learning has been used in a variety of tasks, such as
matrix completion [13], multi-view learning [40], and data
mining [19]. Most PU learning methods emphasize only
binary classiﬁcation [27]; e.g., [30] proposed an unlabeled
data in sequential minimal optimization (USMO) algorithm
to learn a binary classiﬁer from an unlabeled dataset. How-
ever, visual relationship detection is a multi-label classiﬁca-
tion task. Therefore, this paper is one of the works for PU
learning on multi-label tasks, similar to [16, 17, 39]. Fol-
lowing [39], we consider the beneﬁcial effect of unlabeled
relationships to improve visual relationship detection.

5129

Figure 2. The framework of the MF-URLN. The MF-URLN detects objects through an object detector. Then, determinate and undetermined
relationships are generated from the proposed generator. Afterwards, the MF-URLN extracts and fuses features from three modals to
describe each object pairs. Finally, the relationships are predicted based on the determinate conﬁdence subnetwork and the relationship
detection subnetwork.

3. MF-URLN

3.1. Object Detector

Our novel multi-modal feature based undetermined rela-
tionship learning network (MF-URLN) is presented in this
section. We suppose s, p, o, d, and R to represent the sub-
ject, predicate, object, determinate conﬁdence, and relation-
ship, respectively. Therefore, the probabilistic model of the
MF-URLN for visual relationship detection is deﬁned as:

P (R) = P (p|s, o, d)P (d|s, o)P (s|Bs)P (o|Bo).

(1)

Here, Bs and Bo are two individual boxes for subject
and object, which compose an object pair. P (s|Bs) and
P (o|Bo) represent the subject box’s and object box’s prob-
abilities for belonging to an object category. Since object
detection is not a key topic in this paper, the MF-URLN di-
rectly obtains these object probabilities from an object de-
tector, following [23, 34]. P (d|s, o) represents the probabil-
ity of the object pair having a determinate relationship; in
other words, P (d|s, o) reﬂects the probability of one object
pair being manually selected and labeled. P (p|s, o, d) is the
probability of the object pair belonging to a predicate cate-
gory. Note that only P (s|Bs) and P (o|Bo) are independent.
The rest of the factors are correlated. P (d|s, o) depends on
the subject and the object, and P (p|s, o, d) relies on the sub-
ject, the object, and the determinate conﬁdence.

As shown in Fig. 2, the MF-URLN uses an object detec-
tor to detect objects and to provide the scores of P (s|Bs)
and P (o|Bo). Then, an undetermined relationship gener-
ator is utilized. For training, object pairs are classiﬁed as
determinate relationships and undetermined relationships.
For testing, all object pairs are directly used as testing data.
Finally, an undetermined relationship learning network is
proposed to extract and fuse multi-modal features and to
calculate the scores of P (d|s, o) and P (p|s, o, d). More de-
tails about the object detector, the undetermined relation-
ship generator, and the undetermined relationship learning
network are explained in the following subsections.

In the MF-URLN, we use the Faster R-CNN [29] with
the VGG-16 network to locate and detect objects. Specif-
ically, we ﬁrst sample 300 proposed regions generated by
the RPN with IoU > 0.7. Then, after classiﬁcation, we per-
form the NMS with IoU > 0.4 on the 300 proposals. The
retained proposals with category probabilities higher than
0.05 are regarded as the detected objects in one image. The
Faster R-CNN with the VGG-16 is selected, because it is
commonly used in visual relationship detection [32, 33, 37].
Note that the MF-URLN uses the same parameter settings
of Faster R-CNN for different datasets, but these parameters
can be adjusted to obtain better object proposals, following
[37, 41]. In addition, the MF-URLN can be married to any
object detector such as the fast RCNN [9] and YOLO [28].

3.2. Undetermined Relationship Generator

Different datasets of undetermined Relationships seri-
ously affect the detection performance. Therefore, in this
subsection, we introduce a fast manner to automatically
generate useful undetermined relationships.

Speciﬁcally, two different detected objects compose one
object pair. Afterwards, all of the object pairs are com-
pared with the manually annotated relationships (i.e., the
ground truth) for categorization purposes. We suppose ls,
lp, and lo represent the labels for the s, p, and o. A rep-
resents the set of manually annotated relationships, and D
denotes the set of object pairs constructed from the de-
tected objects. An object pair (si, oi) ∈ D is determinate
only if ∃(sk, pk, ok) ∈ A, in which lsi = lsk , loi = lok ,
IoU (si, sk) > 0.5, and IoU (oi, ok) > 0.5. In this way,
lpi = lpk . Otherwise, (si, oi) is classiﬁed as an undeter-
mined relationship, and lpi is unknown and probably does
not belong to any predicates. Here, IoU (a, b) is the inter-
section over union (IoU) between the objects a and b.

We choose this generator because most of the generated

5130

0.11Spatial LocationsLinguistic FeaturesVisual AppearancesCFc𝑷(𝒅)FcFcFuseObject DetectorUndetermined Relationship Generator< Sub-Pre-Obj>InternalWikipediaExternalEmbedding Vectors……StreetClockDogPersonPerson–Has–HandPerson–On–HorseKeyboard–On–LaptopPerson–Wear–HatTraining TripletsFrequencies of TriplesOnWithRideHold…CNN𝑷(𝒑)On0.14Has0.09Near0.89......PredicatesDeterminate ConfidenceSubnetworkRelationship Detection SubnetworkDeterminate Confidence10undetermined relationships belong to the situations men-
tioned in the introduction. In addition, such a method also
yields a good empirical performance. Note that we use the
same object detector to detect objects and generate unde-
termined relationships, so as to make the generated unde-
termined relationships highly correlated with the detected
objects of the MF-URLN.

3.3. Undetermined Relationship Learning Network

The undetermined relationship learning network of the
MF-URLN includes two parts: the multi-modal feature ex-
traction network and the relationship learning network.

3.3.1 Multi-modal Feature Extraction Network

The MF-URLN depicts a full picture of one relationship
from features of three different modals: visual modal, spa-
tial modal, and linguistic modal.

Visual Modal Features. Visual modal features are use-
ful to collect the category characteristics as well as the di-
versity of objects from the same category in different sit-
uations. Following [34], the MF-URLN directly uses the
VGG-16 with ROI pooling from the Faster R-CNN to ex-
tract the visual features from the individual boxes of sub-
jects and objects, and the union box of subject and object
in object pairs. In this way, our learning network is highly
correlated with the object detector and the generated unde-
termined relationships.

Spatial modal

Spatial Modal Features.

features
are complementary to visual modal features, because
information of ob-
ROI pooling deletes
max),
ject pairs.
(xo
min, yo
max)
denote the locations of the subject box, the object box, and
the union box of subject and object in one image, respec-
tively. The spatial modal features are calculated as:

the spatial
We suppose (xs
max, yo
max), and (xu

max, ys
max, yu

min, xs
min, xu

min, ys
min, yu

min, xo

[

min

xs
min − xu
max − xu
xu
xo
min − xu
max − xu
xu

min

min

min

,

,

min

ys
min − yu
max − yu
yu
yo
min − yu
max − yu
yu

min

min

min

,

,

xs
max − xu
max − xu
xu
max − xu
xo
max − xu
xu

max

min

max

min

,

,

max

min

max

ys
max − yu
max − yu
yu
max − yu
yo
max − yu
yu

min

,

].

(2)

Linguistic Modal Features. Linguistic modal features
provide similarities among objects from linguistic knowl-
edge, which are difﬁcult to obtain from visual appearances
and spatial locations. In the MF-URLN, object categories
are obtained from the object detector; then, two kinds of lin-
guistic modal features are extracted based on the labels as-
sociated with the classiﬁer: external linguistic features and
internal linguistic features. For external linguistic features,
we employ the pretrained word2vec model of Wikipedia
2014 [25] to extract the semantic representations of subject
and object. However, such external linguistic features may

contain noise because the training texts are not limited to
relationships. Thus, the internal linguistic features are pro-
posed as complements. For internal linguistic features, we
count the frequencies of all relation triplets in the training
set, and transform these frequencies into probability distri-
butions according to the subject’s and object’s categories,
based on Naive Bayes with Laplace smoothing [24]. The
Laplace smoothing is used to consider the zero-shot data
inﬂuence [23].

Feature Fusion.

In previous methods, the commonly
used feature fusion method is directly concatenating fea-
tures [34, 37]. However, as different features’ dimen-
sions vary widely, high-dimensional features, such as the
4096-dimensional visual features, easily overwhelm low-
dimensional features, such as the 8-dimensional spatial fea-
tures. To alleviate such problems, in the MF-URLN, all
individual features of the same modalities are transformed
into the same dimensions and concatenated;
then, these
concatenated features of single modalities are again trans-
formed into the same dimensions before being concatenated
for multi-modal feature fusion.

3.3.2 Relationship Learning Network

Previous methods considered only determinate relation-
ships. Contrarily,
the MF-URLN predicts relationships
from two kinds of data: determinate relationships and un-
determined relationships.
In detail, two subnetworks are
proposed: a determinate conﬁdence subnetwork and a rela-
tionship detection subnetwork.

Determinate Conﬁdence Subnetwork. The determi-
nate conﬁdence subnetwork decides the determinate con-
ﬁdence of an object pair, which reﬂects the probability of
the object pair being manually selected and labeled. As
shown in Fig. 2, our determinate conﬁdence subnetwork
uses multi-modal features. In the MF-URLN, we use sig-
moid cross entropy loss [3], which is deﬁned as:

CE(p, y) =(−log(p)

−log(1 − p)

if y = 1

otherwise.

(3)

We deﬁne (sd, pd, od) as determinate relationships, and
(si, pi, oi) as undetermined relationships. For a determinate
relationship, y = 1, and its determinate conﬁdence loss is
deﬁned as: Ld
det = CE(P (dd|sd, od), 1). For an undeter-
mined relationship, y = 0, and its determinate conﬁdence
loss is deﬁned as: Li
det = CE(P (di|si, oi), 0). The ﬁnal
determinate conﬁdence loss is weighted calculated as:

Ldet = Ld

det + αLi

det,

(4)

where α is the parameter to adjust the relative importance of
undetermined relationships and determinate relationships.
We believe that determinate relationships and undetermined

5131

relationships contribute equally to the determinate conﬁ-
dence loss and thus set α = 1.

Relationship Detection Subnetwork. The relationship
detection subnetwork predicts the relationship of all object
pairs. As shown in Fig. 2, our relationship detection sub-
network depends on multi-modal features and the determi-
nate conﬁdence subnetwork. In this manner, the two sub-
networks are correlated. In addition, the determinate conﬁ-
dence experimentally improves the relationship detection.

Determinate relationships contain clear human-notated
predicates. Therefore, the relationship detection loss from a
determinate relationship is deﬁned as:

Ld

rel =

M

Xk=1

CE(P (pd

k|sd, od, dd), yk),

(5)

where pk and yk are the kth predicate and the corresponding
label. yk = 1 means the kth predicate’s label is human-
notated; otherwise, yk = 0. M is the number of predicate
categories.

The undetermined relationships with unlabeled predi-
cates should have at least one predicate, whereas the un-
determined relationships without any relationships or with
falsely detected objects should have no predicates. There
is currently no reliable methods to automatically label these
undetermined relationships. Therefore, we treat these data
as having no predicates, following [39]. This method is
naive but experimentally useful. The relationship detection
loss from an undetermined relationship is deﬁned as:

Li

rel =

M

Xk=1

CE(P (pi

k|si, oi, di), 0).

(6)

The relationship detection loss is ﬁnally calculated as:

Lrel = Ld

rel + λ1Li

rel.

(7)

Here, λ1 is the parameter to adjust the relative signiﬁ-
cance of undetermined relationships and determinate rela-
tionships for the relationship detection loss.

Joint loss function. Finally, a joint loss function is
proposed to simultaneously calculate the determinate conﬁ-
dence loss and the relationship detection loss. The joint loss
function is deﬁned as follows:

L = Lrel + λ2Ldet.

(8)

Here, λ2 is the parameter used to trade off between two
groups of objectives: the determinate conﬁdence loss and
the relationship detection loss. By combining Eq. (4), Eq.
(7), and Eq. (8), the joint loss function is rewritten as:

L = Ld

rel + λ1Li

rel + λ2Ld

det + λ2Li

det.

(9)

4. Experiments

In this section, we conduct experiments to validate the
effectiveness of the MF-URLN and the usefulness of un-
determined relationships by answering the following ques-
tions. Q1: is the proposed MF-URLN competitive, when
compared with the state-of-the-art visual relation detection
methods? Q2: what are the inﬂuences of the features on the
proposed MF-URLN? Q3: are undetermined relationships
beneﬁcial to visual relationship detection?

4.1. Datasets, Evaluation Tasks, and Metrics

Datasets. Two public datasets are used for algorithm
validation: the Visual Relationship Detection dataset [23]
and the Visual Genome dataset [18].

The Visual Relationship Detection (VRD) dataset con-
sists of 5,000 images with 100 object categories and 70
predicate categories. In total, the VRD contains 37,993 re-
lationships with 6,672 types. The default dataset split in-
cludes 4,000 training images and 1,000 test images. There
are 1,169 relation triplets that appear only in the test set,
which are further used for zero-shot relationship detection.
We split the default training images from the VRD into two
parts: 3,700 images for training and 300 for validation.

The Visual Genome (VG) dataset is one of the largest
relationship detection datasets. We note that there are mul-
tiple versions of VG datasets [20, 33, 34, 37]. In this paper,
we use the pruned version of the VG dataset provided by
[37]. This VG was also used in [12, 21, 32, 38, 41].
In
summary, this VG contains 99,652 images with 200 object
categories and 100 predicates. The VG contains 1,090,027
relation annotations with 19,561 types. The default dataset
split includes 73,794 training and 25,858 testing images.
We split the default training images from the VG into two
parts: 68,794 images for training and 5,000 for validation.

Evaluation Tasks. Three commonly used tasks are
adopted: predicate detection, phrase detection, and relation
detection, following [33, 34]. In predicate detection, we are
given an input image and ground truth bounding boxes with
corresponding object categories. The outputs are predicates
that describe each pair of objects. In phrase detection, we
are given an input image. The output is a set of relation
triplets and localization of the entire bounding box for each
relation, which overlap at least 0.5 with the ground-truth
joint subject and object boxes. In relation detection, we are
given an input image. The output is a set of relation triplets
and localization of individual bounding boxes for subjects
and objects in each relation, which overlap at least 0.5 with
the ground-truth subject and object boxes.

Evaluation Metrics. We follow the precedent set by [23,
34] by using Recall as our evaluation metric. The top-N
recall is denoted as RN . To be more speciﬁc, for one image,
the outputs are the aggregation of the ﬁrst k top-conﬁdence
predicates from all the potential visual relation triplets in the

5132

image. The RN metric ranks all the outputs from an image
and computes the recall of the top N . We use R50 and R100
for our evaluations. For both datasets, k = 1.

Table 1. Performance comparison of visual relationship detection
methods on the VRD dataset. Pre., Phr., and Rel. represent pred-
ication detection, phrase detection, and relation detection, respec-
tively. “-” denotes that the result is unavailable.

4.2. Implementation Details

We ﬁrst train Faster R-CNN object detectors [1] for the
VRD and VG datasets individually. Then, undetermined
and determinate relationships are generated from the pro-
posed undetermined relationship generator.
In our unde-
termined relationship learning network, the dimensions of
all features’ transforming layers are set as 500, following
[37]. The determinate conﬁdence subnetwork includes two
layers: a 100-dimensional fully connected feature-fusing
layer and a sigmoid classiﬁcation layer. The relationship
detection subnetwork includes three layers: a concatenat-
ing layer for features of multi modals and the determinate
conﬁdence subnetwork, a 500-dimensional fully connected
feature-fusing layer, and a sigmoid classiﬁcation layer. We
use the relu function and the Adam Optimizer with an ex-
ponential decay learning rate to train the MF-URLN. For
the VRD dataset, the initial learning rate is set at 0.0003,
and it decays 0.5 every 4,000 steps. For the VG dataset, the
initial learning rate is set at 0.0003, and it decays 0.7 every
35,000 steps. For the predicate detection task, we do not
use undetermined relationships and set λ1=λ2=0. For the
phrase and relation detection tasks, in each batch, the ratio
of undetermined and determinate relationships is set at 3:1,
following [29]. We set λ1=0.5 and λ2=1. The training sets
are used to train the Faster R-CNN and the MF-URLN 1.
The validation sets are used only to determine parameters.

4.3. Performance Comparisons (Q1)

In this subsection, we compare the MF-URLN with
state-of-the-art relationship detection models to show the
competitiveness of the MF-URLN. We ﬁrst compare the
proposed MF-URLN with ﬁfteen methods on the VRD
dataset. The ﬁfteen methods include: linguistic knowledge
methods, such as VRD-Full [23], LKD: S [34], LKD: T
[34], and LKD: S+T [34]; end-to-end network methods,
such as VTransE [37], VIP-CNN [20], DVSRL [22], and
TFR [15]; deep structural learning methods, such as DSL
[41]; and some other visual relationship detection meth-
ods, such as Weak-S [26], PPRFCN [38], STA [32], Zoom-
Net[33], CAI+SCA-M [33], and VSA [12]. These methods
encompass distinct and different properties. The results are
provided in Table 12. The best methods are highlighted in
bold font. From Table 1, it is apparent that the MF-URLN
outperforms all the other methods in all tasks. In predicate
detection, the MF-URLN outperforms the second-best com-
petitor by 3.9% on R50/100. In phrase detection, compared

1Code available on https://github.com/Atmegal/
2In predicate detection, R50=R100, because there are not enough ob-

jects in ground truth to produce more than 50 pairs.

Pre.

R50/100

Phr.

Rel.

R50

R100

R50

R100

47.9
44.8

-

52.6
47.4
47.5
54.1
55.2

-

52.3

-

48.0
50.7
56.0
49.2

58.2

16.2
19.4
22.8
17.9
19.6
19.2
22.5
23.1
21.4
17.4
22.7

-

24.8
25.2
19.1

31.5

17.0
22.4
27.9
19.5
23.2
20.0
23.6
24.0
22.6
19.1
24.0

-

28.1
28.9
21.7

36.1

13.9
14.1
17.3
15.8
14.4
16.6
18.6
19.2
18.2
15.2
17.4

-

18.9
19.5
16.0

23.9

14.7
15.2
20.0
17.1
15.7
17.7
20.6
21.3
20.8
16.8
18.3

-

21.4
22.4
17.7

26.8

VRD-Full [23]
VTransE [37]
VIP-CNN [20]
Weak-S [26]
PPRFCN [38]
LKD:S [34]
LKD:T [34]
LKD:S+T [34]
DVSRL [22]
TFR [15]
DSL [41]
STA [32]
Zoom-Net [33]
CAI+SCA-M [33]
VSA [12]

MF-URLN

Table 2. Performance comparison of six methods on the VG
dataset. “-” denotes that the result is unavailable.

Pre.

Phr.

Rel.

R50

R100

R50

R100

R50

R100

VTransE [37]
PPRFCN [38]
DSL [41]
STA [32]
VSA [12]

MF-URLN

62.6
64.2

-

62.7
64.4

71.9

62.9
64.9

-

62.9
64.5

72.2

9.5
10.6
13.1

-

9.7

26.6

10.5
11.1
15.6

-

10.0

32.1

5.5
6.0
6.8

-

6.0

6.0
6.9
8.0

-

6.3

14.4

16.5

with the second-best method, the MF-URLN increases R50
and R100 by 25.0% and 24.9%, respectively.
In relation
detection, compared with the second-best method, the MF-
URLN improves upon R50 and R100 by 22.6% and 19.6%,
respectively. These high performances demonstrate the ca-
pacity of the MF-URLN for relationship detection.

Table 2 provides the performance of the MF-URLN and
ﬁve competitive methods (VtransE, PPRFCN, DSL, STA,
and VSA-Net) on the VG dataset. The results from the other
methods are not provided, because these methods were not
tested on the version of VG dataset [37] in their correspond-
ing papers. In Table 2, the top methods are highlighted in
boldface. The MF-URLN performs the best in all of the
tasks no matter the evaluation criteria. For predicate detec-
tion, The MF-URLN yields 11.6% and 11.2% gains on R50
and R100 for predicate detection, 103.1% and 105.8% on
R50 and R100 for phrase detection, and 111.8% and 106.3%
on R50 and R100 for relation detection, respectively. These
improvements verify that the MF-URLN can be applied to

5133

Table 3. Performance comparison on the zero-shot set of the VRD
dataset. “-” denotes that the result is unavailable.

Table 4. R50 predicate detection and relation detection of the MF-
URLN and its eight variants on the VRD dataset.

Pre.

R50/100

12.3

-

21.6
17.0
8.8

-

17.3
20.6

26.9
27.2

R50

5.1
2.7
6.8
10.4
6.5
9.2
5.8

-

5.9
6.2

VRD-Full [23]
VTransE [37]
Weak-S [26]
LKD:S [34]
LKD:T [34]
DVSRL [22]
TFR [15]
STA [32]

MF-URLN
MF-URLN-IM

Phr.

Rel.

R100

R50

R100

Transforming
Pre.

Rel.

Concatenating
Pre.

Rel.

5.7
3.5
7.8
10.9
6.7
10.3
7.1

-

7.9
9.2

4.8
1.7
6.4
8.9
6.1
7.9
5.3

-

4.3
4.5

5.4
2.1
7.4
9.1
6.4
8.5
6.5

-

5.5
6.4

Baseline: V
Baseline: Lex,in
Baseline: S
V +S
V +Lex,in
Lex,in+S
V +S+Lin
V +S+Lex
MF-URLN

52.29
53.39
43.43
54.66
57.27
57.10
56.87
57.69
58.22

22.64
18.49
17.94
23.15
23.21
23.67
23.15
23.50
23.89

53.01
53.94
43.44
52.36
55.45
56.04
53.25
55.29
55.77

22.85
18.07
17.95
22.75
22.62
23.29
22.51
22.83
22.61

large-scale datasets with complex situations.

We also provide the zero-shot detection performances of
ten methods in Table 32 to evaluate the ability of the MF-
URLN to tackle zero-shot data. These methods include:
VRD-Full, VtransE, Weak-S, LKD: S, LKD: T, DVSRL,
TFR, the MF-URLN, and the MF-URLN-IM. The remain-
ing methods are not compared, because the correspond-
ing papers did not consider zero-shot relationship detection.
Here, the MF-URLN-IM is the MF-URLN with the infer-
ring model3. In Table 3, the best methods are highlighted
in boldface. The MF-URLN still performs nearly the best
on predicate detection. However, on phrase detection and
relation detection, the MF-URLN does not perform as well.
This result may occur because some unseen determinate re-
lationships have been mistakenly classiﬁed as undetermined
and consequently inﬂuence the zero-shot detection perfor-
mance. The MF-URLN-IM improves the performance of
the MF-URLN, because of the inferring model. However,
this inferring model is not proper for seen data. The R50/100
predicate relation on the VRD of the MF-URLN-IM is only
57.2. Better strategies to generate and utilize undetermined
relationships are still necessary.

4.4. Discussion of Multi modal Features (Q2)

In this subsection, the effects of multi-modal features
on the MF-URLN are discussed. The MF-URLN is com-
pared with its eight variants by conducting predicate and
relation detection on the VRD dataset. These eight vari-
ants include three baselines of single-modal features, the
“V ”, the “S”, and the “Lex,in”, in which the MF-URLN
uses only visual modal features, spatial modal features,
and linguistic modal features, respectively; three methods
of bi-modal features, the “V +S”, the “V +Lex,in”, and the
“Lex,in+S”, in which the MF-URLN uses visual and spa-
tial modal features, visual and linguistic modal features, and
linguistic and spatial modal features, respectively; and two

3The inferring model is explained in the supplement materials.

methods of multi-modal features, the “V +S+Lin” and the
“V +S+Lex”, in which the MF-URLN uses internal and ex-
ternal linguistic features, respectively. Additionally, we dis-
cuss two kinds of feature fusion methods: the “Transforma-
tion”, in which methods transform features into the same di-
mensions before concatenating, and the “Concatenating”, in
which methods directly concatenate features. Note that con-
catenating methods have the same layer number and layer
dimensions as their corresponding transforming methods to
ignore the improvements caused by deepening the net.

The performances of all compared methods are provided
in Table 4. We draw the following conclusions. 1) By com-
paring the MF-URLN with methods having different fea-
tures, we can see that the MF-URLN obtains the best perfor-
mance. Features from different modalities are complemen-
tary and all contribute to the performance of the MF-URLN.
2) By comparing concatenating methods with transforming
methods, we can conclude that directly concatenating fea-
tures is a low effective feature fusion strategy and that trans-
forming all features into the same dimensions improves the
performance. However, we notice that concatenating meth-
ods slightly outperforms transforming methods on single
modal features. Better feature fusion strategy is still nec-
essary and remains a future topic.

4.5. Analysis of undetermined Relationships (Q3)

In this subsection, we ﬁrst validate the usefulness of un-
determined relationships in visual relationship detection.
We compare the MF-URLN with its three variants by
conducting relation detection on the VRD dataset. The
three variants include the baseline “MFLN”, which is the
MF-URLN without using undetermined relationships; the
“MFLN-Triplet NMS”, which is the MFLN with triplets
NMS [20]; and the “MFLN-Pair Filtering”, which is the
MFLN that uses pair ﬁltering [2]. The triplets NMS and
pair ﬁltering have both been proposed to delete negative
object pairs. The performances of these four methods are
compared in Table 5. We see that “MFLN-Triplet NMS”
decreases the performance of “MFLN”, partly because the

5134

Table 5. Relation detection of four methods on the VRD dataset.

Baseline: MFLN
MFLN-Triplet NMS
MFLN-Pair Filtering
MF-URLN

Entire Set

Unseen Set

R50

17.36
15.53
21.58
23.89

R100

21.76
17.95
23.39
26.79

R50

4.02
3.76
3.59
4.28

R100

4.96
4.19
3.93
5.47

(a) The effects of λ1 and λ2.

(b) MF-URLN vs MF-URLN-NS.

Figure 3. (a) Detection performance of the MF-URLN with dif-
ferent values of modal parameters: λ1 and λ2. (b) Performance
comparison of the MF-URLN and the MF-URLN-NS.

NMS has already been used in our object detector. “MFLN-
Pair Filtering” increases the performance of “MFLN”, be-
cause it excludes some undetermined object pairs. The MF-
URLN achieves the best performance. The improvement of
the MF-URLN verify the utility of undetermined relation-
ships for visual relationship detection.

Then, we discuss the beneﬁcial effects of undetermined
relationships on the MF-URLN by studying the perfor-
mance impacts of the modal parameters, λ1 and λ2. The
R50 relation detection on the VRD dataset is used as the
evaluation criterion. Speciﬁcally, the evaluation is con-
ducted by changing one of the observed parameters while
ﬁxing the other, as in [36]. We set the ranges of λ1 and λ2
as {0, 0.3, 0.5, 0.7, 1} and {0, 0.01, 0.1, 1, 10}, respec-
tively. Fig. 3 (a)4 shows the performance. We can observe
that when λ1=0.5 and λ2=1, the MF-URLN yields the best
performance. λ1=0.5 reveals that labeling undetermined re-
lationships as having no predicates is a useful strategy; un-
determined relationships do have beneﬁcial regular effects
on the relationship detection. λ2=1 reveals that both of the
subnetworks contribute to the detection of the MF-URLN.
Next, we show the beneﬁt of sharing information from
the determinate conﬁdence subnetwork with the relation-
ship detection subnetwork. We compare the MF-URLN
with the MF-URLN-NS, in which the relationship detec-
tion subnetwork does not use information from the conﬁ-
dence subnetwork. The comparing performances are shown
in Fig. 3 (b). It can be seen that the MF-URLN performs
better. This result veriﬁes that the determinate conﬁdence

4λ1=0 and λ2=0 denote the MF-URLN without using undetermined

relationships, the result is 17.36.

Figure 4. Visualization of detection results. For predicate detec-
tion, the top-3 predicates of the MF-URLN are provided. For rela-
tion detection, the top-5 triplets of the MFLN and the MF-URLN
are provided. The √ represents the correct results.

also contributes to the relationship detection.

We also provide the quantitative performances of the
MF-URLN in Fig. 4. For predicate detection, it can be
seen that the MF-URLN yields accurate predictions, which
reveals the ability of the MF-URLN. For relation detec-
tion, the MF-URLN obtains much better detections than the
MFLN. Utilization of undetermined relationships highlights
object pairs with determinate relationships.

5. Conclusion

In this paper, we explore the role of undetermined re-
lationships in visual relationship detection. And accord-
ingly, we propose a novel relationship detection method,
MF-URLN, which extracts and fuses multi-modal features
based on determinate and undetermined relationships. The
experimental results, when compared with state-of-the-
art methods, demonstrate the competitiveness of the MF-
URLN and the usefulness of undetermined relationships.
Our future works include better utilizing undetermined rela-
tionships for relationship detection and promoting undeter-
mined relationships to scene graph generation [15, 31, 35].
Acknowledgment. This work was supported in part by
the National Natural Science Foundation of China under
Grant No.: 61836002 and 61622205, and in part by the
Australian Research Council Projects FL-170100117, DP-
180103424, and IH-180100002.

5135

RefrigeratorStoveSinkFaucetCabinetFaucet–Above–SinkRefrigerator–Nextto–SinkRefrigerator–Nextto–StoveSink–In–CabinetStove–Nextto–RefrigeratorBusRoadStreetWheelTreeCarBus–On–StreetCar–Behind–BusBus–Has–WheelTree–Behind–BusBus–On–RoadTrainStreetBusPersonCarSkySky–Above–BusSky–Above–PersonSky–Above–CarBus–On–StreetSky–Above–TrainRefrigeratorCabinetSinkCounterCabinetSink–In–CabinetCabinet–Above–SinkCabinet–Above–RefrigeratorSink–In–CounterCabinet–Above–CabinetBusStreetCarSkyClockRoofSky–Above–BusBus–On–StreetClock–Above–BusSky–Above–CarRoof–Above–BusBusPersonCarSkyCarCarSky–Above–CarSky–Above–CarSky–Above–PersonSky–Above–BusSky–Above–CarTop-5 Visual Relation Triplets of the MFLNTop-5 Visual Relation Triplets of the MF-URLNRelation DetectionUnseen Visual RelationshipTop-3 Predicates of MF-IRLNOnNexttoUnderTop-3 Predicates of MF-IRLN  HasHoldInKeyboardLaptopKeyboard–On–LaptopPersonHandPerson–Has–HandTop-3 Predicates of MF-IRLNUnderNexttoBehindMotorcycleGrassGrass–Under–MotorcycleSeen Visual RelationshipsPredicate DetectionReferences

[1] X. Chen and A. Gupta. Spatial memory for context reasoning
in object detection. arXiv preprint arXiv:1704.04224, 2017.

[2] B. Dai, Y. Zhang, and D. Lin. Detecting visual relationships
with deep relational networks. Computer vision and pattern
recognition, pages 3298–3308, 2017.

[3] P.-T. De Boer, D. P. Kroese, S. Mannor, and R. Y. Rubin-
stein. A tutorial on the cross-entropy msethod. Annals of
operations research, 134(1):19–67, 2005.

[4] C. Desai and D. Ramanan. Detecting actions, poses, and
objects with relational phraselets. In European Conference
on Computer Vision, pages 158–172. Springer, 2012.

[5] M. Du Plessis, G. Niu, and M. Sugiyama. Convex formu-
lation for learning from positive and unlabeled data. In In-
ternational Conference on Machine Learning, pages 1386–
1394, 2015.

[6] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng,
P. Dollar, J. Gao, X. He, M. Mitchell, J. C. Platt, C. L. Zit-
nick, and G. Zweig. From captions to visual concepts and
back.
In 2015 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), volume 00, pages 1473–1482,
June 2015.

[7] A. Farhadi and M. A. Sadeghi. Recognition using visual
In CVPR 2011(CVPR), volume 00, pages 1745–

phrases.
1752, 06 2011.

[8] C. Galleguillos, A. Rabinovich, and S. Belongie. Object cat-
egorization using co-occurrence, location and appearance.
In Computer Vision and Pattern Recognition, 2008. CVPR
2008. IEEE Conference on, pages 1–8. IEEE, 2008.

[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 580–587,
2014.

[10] G. Gkioxari, R. Girshick, P. Doll´ar, and K. He. Detecting
and recognizing human-object interactions. arXiv preprint
arXiv:1704.07333, 2017.

[11] S. Gould, J. Rodgers, D. Cohen, G. Elidan, and D. Koller.
Multi-class segmentation with relative location prior. Inter-
national Journal of Computer Vision, 80(3):300–316, 2008.

[12] C. Han, F. Shen, L. Liu, Y. Yang, and H. T. Shen. Vi-
sual spatial attention network for relationship detection. In
2018 ACM Multimedia Conference on Multimedia Confer-
ence, pages 510–518. ACM, 2018.

[13] C.-J. Hsieh, N. Natarajan, and I. S. Dhillon. Pu learning for

matrix completion. In ICML, pages 2445–2453, 2015.

[14] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation networks
for object detection. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.

[15] S. Jae Hwang, S. N. Ravi, Z. Tao, H. J. Kim, M. D. Collins,
and V. Singh. Tensorize, factorize and regularize: Robust vi-
sual relationship learning. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018.

mental state prediction. In 2018 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP),
pages 2301–2305. IEEE, 2018.

[17] A. Kanehira and T. Harada. Multi-label ranking from pos-
itive and unlabeled data. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5138–5146, 2016.

[18] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al. Vi-
sual genome: Connecting language and vision using crowd-
sourced dense image annotations. International Journal of
Computer Vision, 123(1):32–73, 2017.

[19] X.-L. Li, P. S. Yu, B. Liu, and S.-K. Ng. Positive unlabeled
learning for data stream classiﬁcation. In Proceedings of the
2009 SIAM International Conference on Data Mining, pages
259–270. SIAM, 2009.

[20] Y. Li, W. Ouyang, X. Wang, and X. Tang. Vip-cnn: Visual
phrase guided convolutional neural network. Computer vi-
sion and pattern recognition, pages 7244–7253, 2017.

[21] K. Liang, Y. Guo, H. Chang, and X. Chen. Visual relation-
ship detection with deep structural ranking. In AAAI Confer-
ence on Artiﬁcial Intelligence, 2018.

[22] X. Liang, L. Lee, and E. P. Xing. Deep variation-structured
reinforcement learning for visual relationship and attribute
detection. Computer vision and pattern recognition, pages
4408–4417, 2017.

[23] C. Lu, R. Krishna, M. S. Bernstein, and L. Feifei. Visual
relationship detection with language priors. European con-
ference on computer vision, pages 852–869, 2016.

[24] A. Y. Ng and M. I. Jordan. On discriminative vs. generative
classiﬁers: A comparison of logistic regression and naive
bayes.
In Advances in neural information processing sys-
tems, pages 841–848, 2002.

[25] J. Pennington, R. Socher, and C. Manning. Glove: Global
vectors for word representation. In Proceedings of the 2014
conference on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543, 2014.

[26] J. Peyre, I. Laptev, C. Schmid, and J. Sivic. Weakly-
international con-

supervised learning of visual relations.
ference on computer vision, pages 5189–5198, 2017.

[27] E. Platanios, H. Poon, T. M. Mitchell, and E. J. Horvitz. Es-
timating accuracy from unlabeled data: A probabilistic logic
approach.
In Advances in Neural Information Processing
Systems, pages 4361–4370, 2017.

[28] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Uniﬁed, real-time object detection. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 779–788, 2016.

[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: to-
wards real-time object detection with region proposal net-
works. IEEE Transactions on Pattern Analysis & Machine
Intelligence, (6):1137–1149, 2017.

[30] E. Sansone, F. G. De Natale, and Z.-H. Zhou. Efﬁcient train-
IEEE Transactions on

ing for positive unlabeled learning.
Pattern Analysis and Machine Intelligence, 2018.

[16] H. Kaji, H. Yamaguchi, and M. Sugiyama. Multi task learn-
ing with positive and unlabeled data and its application to

[31] D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei. Scene graph gen-
eration by iterative message passing. In Proceedings of the

5136

IEEE Conference on Computer Vision and Pattern Recogni-
tion, volume 2, 2017.

[32] X. Yang, H. Zhang, and J. Cai.

Shufﬂe-then-assemble:
learning object-agnostic visual relationship features. arXiv
preprint arXiv:1808.00171, 2018.

[33] G. Yin, L. Sheng, B. Liu, N. Yu, X. Wang, J. Shao, and C. C.
Loy. Zoom-net: Mining deep feature interactions for visual
relationship recognition. arXiv preprint arXiv:1807.04979,
2018.

[34] R. Yu, A. Li, V. I. Morariu, and L. S. Davis. Visual relation-
ship detection with internal and external linguistic knowl-
edge distillation. International conference on computer vi-
sion, pages 1068–1076, 2017.

[35] R. Zellers, M. Yatskar, S. Thomson, and Y. Choi. Neural
motifs: Scene graph parsing with global context.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 5831–5840, 2018.

[36] Y. Zhan, J. Yu, Z. Yu, R. Zhang, D. Tao, and Q. Tian.
Comprehensive distance-preserving autoencoders for cross-
modal retrieval.
In 2018 ACM Multimedia Conference on
Multimedia Conference, pages 1137–1145. ACM, 2018.

[37] H. Zhang, Z. Kyaw, S. Chang, and T. Chua. Visual transla-
tion embedding network for visual relation detection. Com-
puter vision and pattern recognition, pages 3107–3115,
2017.

[38] H. Zhang, Z. Kyaw, J. Yu, and S. Chang. Ppr-fcn: Weakly su-
pervised visual relation detection via parallel pairwise r-fcn.
International conference on computer vision, pages 4243–
4251, 2017.

[39] X. Zhang and Y. LeCun. Universum prescription: Regular-
In AAAI, pages 2907–2913,

ization using unlabeled data.
2017.

[40] J. T. Zhou, S. J. Pan, Q. Mao, and I. W. Tsang. Multi-view
positive and unlabeled learning. In Asian Conference on Ma-
chine Learning, pages 555–570, 2012.

[41] Y. Zhu and S. Jiang. Deep structured learning for visual re-
lationship detection. In AAAI Conference on Artiﬁcial Intel-
ligence, 2018.

5137

