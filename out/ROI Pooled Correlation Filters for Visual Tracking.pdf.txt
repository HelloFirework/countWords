ROI Pooled Correlation Filters for Visual Tracking

Yuxuan Sun1, Chong Sun2, Dong Wang1 ∗, You He3, Huchuan Lu1

4

,

1School of Information and Communication Engineering, Dalian University of Technology, China

2Tencent Youtu Lab, China

3Naval Aviation University, China
4Peng Cheng Laboratory, China

rumsyx@mail.dlut.edu.cn, waynecsun@tencent.com, heyou f@126.com, {wdice,lhchuan}@dlut.edu.cn

Abstract

The ROI (region-of-interest) based pooling method per-
forms pooling operations on the cropped ROI regions for
various samples and has shown great success in the ob-
ject detection methods. It compresses the model size while
preserving the localization accuracy, thus it is useful in
the visual tracking ﬁeld. Though being effective, the ROI-
based pooling operation is not yet considered in the cor-
relation ﬁlter formula. In this paper, we propose a novel
ROI pooled correlation ﬁlter (RPCF) algorithm for ro-
bust visual tracking. Through mathematical derivations,
we show that the ROI-based pooling can be equivalently
achieved by enforcing additional constraints on the learned
ﬁlter weights, which makes the ROI-based pooling feasi-
ble on the virtual circular samples. Besides, we develop
an efﬁcient joint training formula for the proposed corre-
lation ﬁlter algorithm, and derive the Fourier solvers for
efﬁcient model training. Finally, we evaluate our RPCF
tracker on OTB-2013, OTB-2015 and VOT-2017 benchmark
datasets. Experimental results show that our tracker per-
forms favourably against other state-of-the-art trackers.

1. Introduction

Visual tracking aims to localize the manually speciﬁed
target object in the successive frames, and it has been
densely studied in the past decades for its broad applica-
tions in the automatic drive, human-machine interaction,
behavior recognition, etc. Till now, visual tracking is still
a very challenging task due to the limited training data and
plenty of real-world challenges, such as occlusion, defor-
mation and illumination variations.

In recent years, the correlation ﬁlter (CF) has become
one of the most widely used formulas in visual tracking
for its computation efﬁciency. The success of the corre-

∗Corresponding Author: Dr. Wang

Figure 1. Visualized tracking results of our method and other four
competing algorithms. Our tracker performs favourably against
the state-of-the-art.

lation ﬁlter mainly comes from two aspects: ﬁrst, by ex-
ploiting the property of circulant matrix, the CF-based al-
gorithms do not need to construct the training and testing
samples explicitly, and can be efﬁciently optimized in the
Fourier domain, enabling it to handle more features; sec-
ond, optimizing a correlation ﬁlter can be equivalently con-
verted to solving a system of linear functions, thus the ﬁl-
ter weights can either be obtained with the analytic solu-
tion (e.g., [9, 8]) or be solved via the optimization algo-
rithms with quadratic convergence [9, 7]. As is well rec-
ognized, the primal correlation ﬁlter algorithms have lim-
ited tracking performance due to the boundary effects and
the over-ﬁtting problem. The phenomenon of boundary ef-
fects is caused by the periodic assumptions of the training
samples, while the over-ﬁtting problem is caused by the un-
balance between the numbers of model parameters and the

5783

RPCFECOC-COTKCFCF2training samples. Though the boundary effects have been
well addressed in several recent papers (e.g., SRDCF [9],
DRT [29], BACF [12] and ASRCF [5]), the over-ﬁtting
problem is still not paid much attention to and remains to
be a challenging research hotspot.

The average/max-pooling operation has been widely
used in the deep learning methods via the pooling layer,
which is shown to be effective in handling the over-ﬁtting
problem and deformations. Currently, two kinds of pooling
operations are widely used in deep learning methods. The
ﬁrst one performs average/max-pooling on the entire input
feature map and obtains a feature map with reduced spatial
resolutions. In the CF formula, the pooling operation on the
input feature map can lead to fewer available synthetic train-
ing samples, which limits the discriminative ability of the
learned ﬁlter. Also, the smaller size of the feature map will
signiﬁcantly inﬂuence the localization accuracy. However,
the ROI (Region of Interest)-based pooling operation is an
alternative, which has been successfully embedded into sev-
eral object detection networks (e.g., [14, 26]). Instead of
directly performing the average/max-pooling on the entire
feature map, the ROI-based pooling method ﬁrst crops large
numbers of ROI regions, each of which corresponds to a tar-
get candidate, and then performs average/max-pooling for
each candidate ROI region independently. The ROI-based
pooling operation has the merits of a pooling operation as
mentioned above, and at the same time retains the number
of training samples and the spatial information for localiza-
tion, thus it is meaningful to introduce the ROI-based pool-
ing into the CF formula. Since the CF algorithm has no
access to real-world samples, it remains to be investigated
on how to exploit the ROI-based pooling in a correlation
ﬁlter formula.

In this paper, we study the inﬂuence of the pooling op-
eration in visual tracking, and propose a novel ROI pooled
correlation ﬁlters algorithm. Even though the ROI-based
pooling algorithm has been successfully applied in many
deep learning-based applications, it is seldom considered in
the visual tracking ﬁeld, especially in the correlation ﬁlter-
based methods. Since the correlation ﬁlter formula does not
really extract positive and negative samples, it is infeasible
to perform the ROI-based pooling like Fast R-CNN [14].
Through mathematical derivation, we provide an alterna-
tive solution to implement the ROI-based pooling. We pro-
pose a correlation ﬁlter algorithm with equality constraints,
through which the ROI-based pooling can be equivalently
achieved. We propose an Alternating Direction Method Of
Multipliers (ADMM) algorithm to solve the optimization
problem, and provide an efﬁcient solver in the Fourier do-
main. Large number of experiments on the OTB-2013 [31],
OTB-2015 [32] and VOT-2017 [20] datasets validate the ef-
fectiveness of the proposed method (see Figure 1 and Sec-
tion 5). The contributions of this paper are three-fold:

• This paper is the ﬁrst attempt to introduce the idea
of ROI-based pooling in the correlation ﬁlter formula.
It proposes a correlation ﬁlter algorithm with equality
constraints, through which the ROI-based pooling op-
eration can be equivalently achieved without the need
for real-world ROI sample extraction. The learned ﬁl-
ter weights are insusceptible to the over-ﬁtting prob-
lem and are more robust to deformations.

• This paper proposes a robust ADMM method to op-
timize the proposed correlation ﬁlter formula in the
Fourier domain. With the computed Lagrangian mul-
tipliers, the paper aims to use the conjugate gradient
method for ﬁlter learning, and develops efﬁcient opti-
mization strategy for each step.

• This paper conducts large amounts of experiments on
three available public datasets. The experimental re-
sults validate the effectiveness of the proposed method.

2. Related Work

The recent papers on visual tracking are mainly based
on the correlation ﬁlters and deep networks [21], many of
which have impressive performance. In this section, we pri-
marily focus on the algorithms based on the correlation ﬁl-
ters and brieﬂy introduce related issues of the pooling oper-
ations.

Discriminative Correlation Filters. Trackers based on
correlation ﬁlters have been the focus of researchers in re-
cent years, which have achieved the top performance in
various datasets. The correlation ﬁlter algorithm in visual
tracking can be dated back to the MOSSE tracker [2], which
takes the single-channel gray-scale image as input. Even
though the tracking speed is impressive, the accuracy is not
satisfactory. Based on the MOSSE tracker, Henriques et
al. advance the state-of-the-art by introducing the kernel
functions [18] and higher dimensional features [19]. Ma et
al. [24] exploit the rich representation information of deep
features in the correlation ﬁlter formula, and fuse the re-
sponses of various convolutional features via a coarse-to-
ﬁne searching strategy. Qi et al. [25] extend the work
of [24] by exploiting the Hedge method to learn the im-
portance for each kind of feature adaptively. Apart from
the MOSSE tracker, the aforementioned algorithms learn
the ﬁlter weights in the dual space, which have been at-
tested to be less effective than the primal space-based al-
gorithms [8, 9, 19]. However, correlation ﬁlters learned in
the primal space are severely inﬂuenced by the boundary ef-
fects and the over-ﬁtting problem. Because of this, Danell-
jan et al. [9] introduce a weighted regularization constraint
on the learned ﬁlter weights, encouraging the algorithm to
learn more weights on the central region of the target ob-
ject. The SRDCF tracker [9] has become a baseline algo-
rithm for many latter trackers, e.g., CCOT [11] and SRD-

5784

CFDecon [10]. The BACF tracker [12] provides another
feasible way to address the boundary effects, which gener-
ates real-world training samples and greatly improves the
discriminant power of the learned ﬁlter. Though the above
methods have well addressed the boundary effects, the over-
ﬁtting problem is rarely considered. The ECO tracker [7]
jointly learns a projection matrix and the ﬁlter weights,
through which the model size is greatly compressed. Differ-
ent from the ECO tracker, our method introduces the ROI-
based pooling operation into a correlation ﬁlter formula,
which does not only address the over-ﬁtting problem but
also makes the learned ﬁlter weights more robust to defor-
mations.

Pooling Operations. The idea of the pooling opera-
tion has been used in various ﬁelds in computer vision,
e.g., feature extraction [6, 22], convolutional neural net-
works [27, 16], to name a few. Most of the pooling op-
erations are performed on the entire feature map to either
obtain more stable feature representations or rapidly com-
press the model size. In [6], Dalal et al. divide the image
window into dozens of cells, and compute the histogram of
gradient directions in each divided cell. The computed fea-
ture representations are more robust than the ones based on
individual pixels. In most deep learning-based algorithms
(e.g., [6, 22]), the pooling operations are performed via
a pooling layer, which accumulates the multiple response
activations over a small neighbourhood region. The lo-
calization accuracy of the network usually decreases after
the pooling operation. Instead of the primal max/average-
pooling layer, the faster R-CNN method [14] exploits the
ROI pooling layer to ensure the localization accuracy and at
the same time compress the model size. The method ﬁrstly
extracts the ROI region for each candidate target object via
a region of proposal network (RPN), and then performs the
max-pooling operation on the ROI region to obtain more
robust feature representations. Our method is inspired by
the ROI pooling proposed in [14], and is the ﬁrst attempt to
introduce the ROI-based pooling operation into the correla-
tion ﬁlter formula.

3. Correlation Filter and Pooling

In this section, we brieﬂy revisit the two key technolo-
gies closely related to our approach (i.e., the correlation ﬁl-
ter and pooling operation).

3.1. Revisit of Correlation Filter

To help better understand our method, we ﬁrst introduce
the primal correlation ﬁlter algorithm. Given an input fea-
ture map, a correlation ﬁlter algorithm aims at learning a set
of ﬁlter weights to regress the Gaussian-shaped response.
We use yd ∈ RN to denote the desired Gaussian-shaped
response, and x to denote the input feature map with D
feature channels x1, x2, ..., xD. For each feature channel

Figure 2. Illustration showing that ROI pooled features are more
robust to target deformations than the original ones. For both fea-
tures, we compute the ℓ2 loss between features extracted from
Frames 2-20 and Frame 1, and visualize the distances via red and
blue dots respectively.

xd ∈ RN , a correlation ﬁlter algorithm computes the re-
sponse by convolving xd with the ﬁlter weight wd ∈ RN .
Based on the above-mentioned deﬁnitions and descriptions,
the optimal ﬁlter weights can be obtained by optimizing the
following objective function:

E(w) =

1
2

y −

D

X

d=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

wd ∗ xd(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2

2

+

λ
2

D

X

d=1

kwdk2
2 ,

(1)

where ∗ denotes the circular convolution operator, w =
[w1, w2, ..., wD] is concatenated ﬁlter vector, λ is a trade-off
parameter to balance the importance between the regression
and the regularization losses. According to the Parseval’s
theorem, Eq. 1 can be equivalently written in the Fourier
domain as

E( ˆw) =

1
2

ˆy −

D

X

d=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

ˆwd ⊙ ˆxd(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2

2

+

λ
2

D

X

d=1

k ˆwdk2
2 ,

(2)

where ⊙ is the Hadamard product. We use ˆy, ˆwd, ˆxd to
denote the Fourier domain of vector y, wd and xd.

3.2. Pooling Operation in Visual Tracking

As is described by many deep learning methods [27, 13],
the pooling layer plays a crucial rule in addressing the over-
ﬁtting problem. Generally speaking, a pooling operation
tries to fuse the neighbourhood response activations into
one, through which the model parameters can be effectively
compressed. In addition to addressing the over-ﬁtting prob-
lem, the pooled feature map becomes more robust to defor-
mations (Figure 2). Currently, two kinds of pooling opera-
tions are widely used, i.e., the pooling operation based on
the entire feature map (e.g., [27, 16]) and the pooling op-
eration based on the candidate ROI region (e.g. [26]). The

5785

140120100806040200L2 Norm DistanceFrame15101520Frame1Frame5Frame10Frame15Frame20the correlation ﬁlter does not explicitly extract the training
samples, it is impossible to perform the ROI-based pooling
operation following the pipeline in Figure 3. In this paper,
we derive that the ROI-based pooling operation can be im-
plemented by adding additional constraints on the learned
ﬁlter weights.

Given a candidate feature vector v corresponding to the
target region with L elements, we perform the average-
pooling operation on it with the pooling kernel size e. For
simplicity, we set L = eM , where M is a positive integer
(the padding operation can be used if L cannot be divided
by e evenly). The pooled feature vector v′ ∈ RM can be
computed as v′ = 1
e U v, where the matrix U ∈ RM ×M e is
constructed as:

U =

1e 0e
0e 1e
...
...
0e 0e
0e 0e





···
···
. . .
···
···

0e 0e
0e 0e

0e 0e
1e 0e
0e 1e





,

(3)

where 1e ∈ R1×e denotes a vector with all the entries set
as 1, and 0e ∈ R1×e is a zero vector. Based on the pooled

vector, we compute the response as:

r = w′⊤v′ = w′⊤U v/e = (cid:0)U ⊤w′(cid:1)

⊤

v/e,

(4)

wherein w′ is the weight corresponding to the pooled fea-
ture vector, U ⊤w′ = [w′(1)1e, w′(2)1e, ..., w′(M )1e]⊤. It
is easy to conclude that average-pooling operation can be
equivalently achieved by constraining the ﬁlter weights in
each pooling kernel to have the same value. Based on the
discussions above, we deﬁne our ROI pooled correlation ﬁl-
ter as follows:

2

+ λ
2

D

Pd=1kgd ⊙ wdk2

2

E(w) = 1

y −

D

Pd=1

2 (cid:13)(cid:13)(cid:13)(cid:13)

(pd ⊙ wd) ∗ xd(cid:13)(cid:13)(cid:13)(cid:13)

2

s.t. wd(iη) = wd(jη), (iη, jη) ∈ P, η = 1, .., K

(5)
where we consider K equality constraints to ensure that ﬁl-
ter weights in each pooling kernel have the same value, P
denotes the set that two ﬁlter elements belong to the same
pooling kernel, iη and jη denote the indexes of elements
in weight vector wd. In Eq. 5, pd ∈ RN is a binary mask
which crops the ﬁlter weights corresponding to the target
region. By introducing pd, we make sure that the ﬁlter only
has the response for the target region of each circularly con-
structed sample [12]. The vector gd ∈ RN is a regulariza-
tion weight that encourages the ﬁlter to learn more weights
in the central part of the target object. The idea to intro-
duce pd and gd has been previously proposed in [9, 12],
while our tracker is the ﬁrst attempt to integrate them. In
the equality constraints, we consider the relationships be-
tween two arbitrary weight elements in a pooling kernel,

5786

Figure 3. Illustration showing the difference between the feature
map based and the ROI-based pooling operations. For clarity, we
use 8 as the stride for sample extraction on the original image. This
corresponds to a stride = 2 feature extraction in the HOG feature
with 4 as the cell size. The pooling kernel size is set as e = 2 in
this example.

former one has been widely used in the CF trackers with
deep features, as a contrast, the ROI-based pooling oper-
ation is seldom considered. As is described in Section 1,
directly performing average/max-pooling on the input fea-
ture map will result in fewer training/testing samples and
worse localization accuracy. We use an example to show
how different pooling methods inﬂuence the sample extrac-
tion process in Figure 3, wherein the extracted samples are
visualized on the right-hand side. For simplicity, this ex-
ample is based on the dense sampling process. The conclu-
sion is also applicable to the correlation ﬁlter method, which
is essentially trained via densely sampled circular candi-
dates. In the feature map based pooling operation, the fea-
ture map size is ﬁrst reduced to W/e× H/e, thus leading to
fewer samples. However, the ROI-based pooling ﬁrst crop
samples from the W × H feature map and then performs
pooling operations upon them, thus does not inﬂuence the
training number. Fewer training samples will lead to infe-
rior discrimination ability of the learned ﬁlter, while fewer
testing samples will result in inaccurate target localizations.
Thus, it is meaningful to introduce the ROI-based pooling
operation into the correlation ﬁlter algorithms. Since the
max-pooling operation will introduce the non-linearity that
makes the model intractable to be optimized, the ROI-based
average-pooling operation is preferred in this paper.

4. Our Approach

4.1. ROI Pooled Correlation Filter

In this section, we propose a novel correlation tracking
method with ROI-based pooling operation. Like the previ-
ous methods [18, 11], we introduce our CF-based tracking
algorithm in the one-dimensional domain, and the conclu-
sions can be easily generalized to higher dimensions. Since

Feature map based pooling operationPoolingCropVisualization of extracted samplesROI-based pooling operationPoolingCropVisualization of extracted samplesW*HW*HW/e * H/ee!

(e−2)!2! (⌊(L − e)/e⌋ + 1) for each channel d,
thus K =
where L is the number of nonzero values in pd. Note that
the constraints are only performed in the ﬁlter coefﬁcients
corresponding to the target region of each sample, and the
computed K is based on the one-dimensional case.

According to the Parseval’s formula, the optimization in

Eq. 5 can be equivalently written as:

D

2 (cid:13)(cid:13)(cid:13)(cid:13)
E( ˆw) = 1
d F −1

s.t. V 1

ˆy −

Pd
d ˆwd = V 2

ˆPd ˆwd ⊙ ˆxd(cid:13)(cid:13)(cid:13)(cid:13)
d F −1

d ˆwd

2

2

+ λ
2

D

Pd=1

(cid:13)(cid:13)(cid:13)

ˆGdwd(cid:13)(cid:13)(cid:13)

2

2

,

(6)

d

where Fd denotes the Fourier transform matrix, and F −1
denotes the inverse transform matrix. The vectors ˆpd ∈
CN ×1, ˆy ∈ CN ×1, ˆxd ∈ CN ×1 and ˆwd ∈ CN ×1 de-
note the Fourier coefﬁcients of the corresponding signal
vectors y, xd, pd and wd. Matrices ˆPd and ˆGd are the
Toeplitz matrices, whose (i, j)-th elements are ˆpd((N + i−
j)%N + 1) and ˆgd((N + i − j)%N + 1), where % de-
notes the modulo operation. They are constructed based on
the convolution theorem to ensure that ˆPd ˆwd = ˆpd ∗ ˆwd,
ˆGdwd = ˆgd ∗ ˆwd. Since the discrete Fourier coefﬁ-
cients of a real-valued signal are Hermitian symmetric, i.e.,
ˆpd((N + i − j)%N + 1) = ˆpd((N + j − i)%N + 1)∗
in our case, we can easily conclude that ˆPd = ˆP H
d and
ˆGd = ˆGH
d , where H denotes the conjugate-transpose of
d ∈ RK×N
a complex matrix.
and V 2
d ∈ RK×N are index matrices with either 1 or
0 as the entries, V 1
d ˆwd = [wd(i1), ..., wd(iK)]⊤ and
d F −1
V 2
d ˆwd = [wd(j1), ..., wd(jK)]⊤.

In the constraint term, V 1

Eq. 6 can be rewritten in a compact formula as:

d F −1

D

2 (cid:13)(cid:13)(cid:13)(cid:13)
E( ˆw) = 1
s.t. VdF −1

ˆy −

Pd=1
d ˆwd = 0

ˆEd ˆwd(cid:13)(cid:13)(cid:13)(cid:13)

2

2

+ λ
2

D

Pd=1

(cid:13)(cid:13)(cid:13)

ˆGd ˆwd(cid:13)(cid:13)(cid:13)

2

2

,

(7)

where ˆEd = ˆXd ˆPd, ˆXd = diag(ˆxd(1), ..., ˆxd(N )) is a di-
agonal matrix, Vd = V 1

d − V 2
d .

4.2. Model Learning

Since Eq. 7 is a quadratic programming problem with
linear constraints, we use the Augmented Lagrangian
Method for efﬁcient model learning. The Lagrangian func-
tion corresponding to Eq. 7 is deﬁned as:

2

D

2 (cid:13)(cid:13)(cid:13)(cid:13)
L( ˆw, ξ) = 1
Pd=1
ˆy −
d VdF −1
d ˆwd + 1
ξ⊤
+
2

Pd=1

ˆEd ˆwd(cid:13)(cid:13)(cid:13)(cid:13)
Pd=1

D

D

D

2

+ λ
2

(cid:13)(cid:13)(cid:13)
Pd=1
γd (cid:13)(cid:13)VdF −1
d ˆwd(cid:13)(cid:13)

2
2,

ˆGd ˆwd(cid:13)(cid:13)(cid:13)

2

2

(8)

where ξd ∈ RK denotes the Lagrangian multipliers for the
d-th channel, γd is the penalty parameter, ξ = [ξ⊤
D]⊤.
The ADMM method is used to alternately optimize ˆw and ξ.

1 , ..., ξ⊤

(a)

(b)

Figure 4. Comparison between ﬁlter weights of the baseline
method (i.e., the correlation ﬁlter algorithm without ROI-based
pooling) and the proposed method. (a) A toy model showing that
our learned ﬁlter elements are identical in each pooling kernel. (b)
Visualizations of the ﬁlter weights learned by the baseline and our
method. Our algorithm learns more compact ﬁlter weights than
the baseline method, and thus can better address the over-ﬁtting
problem.

Though the optimization objective function is non-convex,
it becomes a convex function when either ˆw or ξ is ﬁxed.

When ξ is ﬁxed, ˆw can be computed via the conjugate
gradient descent method [4]. We compute the gradient of
the objective function with respects to ˆwd in Eq. 8 and ob-
tain a number of linear equations by setting the gradient to
be a zero vector:

⊤

( ˆA + FV

V F −1 + λ ˆGH ˆG) ˆw = ˆEH y − FV ⊤ξ,

(9)

where F ∈ CDN ×DN , ˆG ∈ CDN ×DN , V ∈ RDK×DN
and V ∈ RDK×DN are block diagonal matrices with the
d-th matrix block set as Fd, ˆGd, Vd and √γdVd, E =
[E1, E2, ..., ED], ˆA = EH E.
In the conjugate gradi-
ent method, the computation load lies in the three terms

⊤

1 , ..., u⊤

V F −1 ˆu and λ ˆGH ˆGˆu given the search direction
ˆAˆu, FV
ˆu = [u⊤
D]⊤. In the following, we present more de-
tails on how we compute these three terms efﬁciently. Each
of the three terms can be regarded as a vector constructed
with D sub-vectors. The d-th sub-vector of ˆAˆu is computed
as ˆP H

ˆXj( ˆPj ˆuj) wherein P H

d = Pd as described

d X H

D

d

Pj=1

above. Since the Fourier coefﬁcients of pd (a vector with
binary values) are densely distributed, it is time consuming
to directly compute ˆPdˆv given an arbitrary complex vector
ˆv.
In this work, the convolution theorem is used to efﬁ-
ciently compute ˆPdˆv. The d-th sub-vector of the second
⊤Vdud. As the matrices
term is FdV d
Vd and V ⊤
d only consists of 1 and −1, thus the computation
of V ⊤
d Vdud can be efﬁciently conducted via table lookups.
The third term corresponds to the convolution operation,
whose convolution kernel is usually smaller than 5, thus it
can also be efﬁciently computed.

V dud = γdFdVd

⊤

5787

OursBaselineInput ImageHigh confidenceLow confidenceTarget RegionWhen ˆw is computed, ξd can be updated via:

5. Experiments

ξi+1
d = ξi

d + γdVdF −1

d ˆwd,

(10)

where we use ξi
tion. According to [3], the value of γd can be updated as:

d to denote the value of ξd in the i-th itera-

In this section, we evaluate the proposed RPCF tracker
on the OTB-2013 [31], OTB-2015 [32] and VOT2017 [20]
datasets. We ﬁrst evaluate the effectiveness of the method,
and then further compare our tracker with the recent state-
of-the-art.

γi+1
d = min(γmax, αγi

d),

(11)

5.1. Experimental Setups

again we use i to denote the iteration index.

4.3. Model Update

To learn more robust ﬁlter weights, we update the pro-
posed RPCF tracker based on several training samples (T
samples in total) like [11, 7]. We extend the notations ˆA
and ˆE in Eq. 9 with superscript t, and reformulate Eq. 9 as
follows:

T

(

X

t=1

µt ˆAt + FV ⊤V F −1 + λ ˆGH ˆG) ˆw = b,

(12)

T

H

µt( ˆEt)

Pt=1

where b =

y − FV ⊤ξ, and µt denotes the
importance weight for each training sample t. Most pre-
vious correlation ﬁlter trackers update the model iteratively
via a weighted combination of the ﬁlter weights in various
frames. Different from them, we exploit the sparse update
mechanism, and update the model every Nt frames [7]. In
each updating frame, the conjugate gradient descent method
is used, and the search direction of the previous update pro-
cess is input as a warm start. Our training samples are gen-
erated following [7], and the weight (i.e., learning rate) for
the newly added sample is set as ω, while the weights of
previous samples are decayed by multiplying 1− ω. In Fig-
ure 4, we visualize the learned ﬁlter weights of different
trackers with and without ROI-based pooling, our tracker
can learn more compact ﬁlter weights and focus on the reli-
able regions of the target object.

4.4. Target Localization

In the target localization process, we ﬁrst crop the can-
didate samples with different scales, i.e., xs
d, s ∈ {1, ..., S}.
Then, we compute the response ˆrs for the feature in each
scale in the Fourier domain:

ˆrs =

D

X

d=1

ˆxs
d ˆwd.

(13)

The computed responses are then interpolated with
trigonometric polynomial following [9] to achieve the sub-
pixel target localization.

Implementation Details. The proposed RPCF method is
mainly implemented in MATLAB on a PC with an i7-
4790K CPU and a Geforce 1080 GPU. Similar to the ECO
method [7], we use a combination of CNN features from
two convolution layers, HOG and color names for target
representation. For efﬁciency, the PCA method is used to
compress the features. We set the learning rate ω, the max-
imum number of training samples T , γmax and α as 0.02,
50, 1000 and 10 respectively, and we update the model in
every Nt frame. As to γd, we set a relative small value γ1
(e.g., 0.1) for the high-level feature (i.e., the second con-
volution layer), and a larger value γ2 = 3γ1 for the other
feature channels. The kernel size e is set as 2 in the imple-
mentation. We use the conjugate gradient descent for model
initialization and update, 200 iterations are used in the ﬁrst
frame, and the following update frame uses 6 iterations. Our
tracker runs at about 5fps without optimization.
Evaluation Metric. We follow the one-pass evaluation
(OPE) rule on the OTB-2013 and OTB-2015 datasets, and
report the precision plots as well as the success plots for the
performance measure. The success plots demonstrate the
overlaps between tracked bounding boxes and ground truth
with varying thresholds, while the precision plots measure
the accuracy of the estimated target center positions. In the
precision plots, we exploit the distance precision (DP) rate
at 20 pixels for the performance report, while we exploit
the area-under-curve (AUC) score for performance report
in success plots. On the VOT-2017 dataset, we evaluate our
tracker in terms of the Expected Average Overlap (EAO),
accuracy raw value (A) and robustness raw value (R) mea-
sure the overlap, accuracy and robustness respectively.

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Precision plots of OPE

RPCF [0.929]
Baseline [0.884]
Baseline + AP [0.881]
Baseline + MP [0.877]

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 
s
s
e
c
c
u
S

Success plots of OPE

RPCF [0.690]
Baseline [0.670]
Baseline + MP [0.654]
Baseline + AP [0.650]

10
Location error threshold

20

30

40

50

0

0

0.2

0.4

0.6

Overlap threshold

0.8

1

(a)

(b)

Figure 5. Precision and success plots of 100 sequences on the
OTB-2015 dataset. The distance precision rate at the threshold of
20 pixels and the AUC score for each tracker is presented in the
legend.

5788

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Precision plots of OPE

RPCF-NC [0.954]
RPCF [0.943]
LSART [0.935]
ECO [0.930]
CCOT [0.899]
CF2 [0.891]
ECO-HC [0.874]
MEEM [0.830]
Staple [0.793]
KCF [0.740]
30
40

10
Location error threshold

20

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
t
a
r
 
s
s
e
c
c
u
S

50

0

0

Success plots of OPE

RPCF-NC [0.713]
RPCF [0.709]
ECO [0.709]
LSART [0.677]
CCOT [0.672]
ECO-HC [0.652]
CF2 [0.605]
Staple [0.600]
MEEM [0.566]
KCF [0.514]

0.2

0.4

0.6

Overlap threshold

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Precision plots of OPE

RPCF-NC [0.932]
RPCF [0.929]
LSART [0.923]
ECO [0.910]
CCOT [0.898]
ECO-HC [0.856]
CF2 [0.837]
Staple [0.784]
MEEM [0.781]
KCF [0.696]
30
40

10
Location error threshold

20

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
t
a
r
 
s
s
e
c
c
u
S

50

0

0

0.8

1

Success plots of OPE

RPCF-NC [0.696]
ECO [0.691]
RPCF [0.690]
LSART [0.672]
CCOT [0.671]
ECO-HC [0.643]
Staple [0.581]
CF2 [0.562]
MEEM [0.530]
KCF [0.477]

0.2

0.4

0.6

0.8

1

Overlap threshold

(a)

(b)

(a)

(b)

Figure 6. Precision and success plots of 50 sequences on the OTB-
2013 dataset. The distance precision rate at the threshold of 20
pixels and the AUC score for each tracker is presented in the leg-
end.

Figure 7. Precision and success plots of 100 sequences on the
OTB-2015 dataset. The distance precision rate at the threshold of
20 pixels and the AUC score for each tracker is presented in the
legend.

5.2. Ablation Study

In this subsection, we conduct experiments to validate
the contributions of the proposed RPCF method. We set the
tracker that does not consider the pooling operation as the
baseline method, and use Baseline to denote it. It essentially
corresponds to Eq. 5 without equality constraints. To vali-
date the superiority of our ROI-based pooling method over
feature map based average-pooling and max-pooling, we
also implement the trackers that directly performs average-
pooling and max-pooling on the input feature map, which
are named as Baseline+AP and Baseline+MP.

We ﬁrst compare the Baseline method with Baseline+AP
and Baseline+MP, which shows that the tracking perfor-
mance decreases when feature map based pooling opera-
tions are performed. Directly performing pooling opera-
tions on the input feature map will not only inﬂuence the
extraction of the training samples but also lead to worse tar-
get localization accuracy. In addition, the over-ﬁtting prob-
lem is not well addressed in such methods since the ratio
between the numbers of model parameters and available
training samples do not change compared with the Base-
line method. We validate the effectiveness of the proposed
method by comparing our RPCF tracker with the Baseline
method. Our tracker improves the Baseline method by 4.4%
and 2.0% in precision and success plots respectively. By
exploiting the ROI-based pooling operations , our learned
ﬁlter weights are insusceptible to the over-ﬁtting problem
and are more robust to deformations.

5.3. State of the art Comparisons

OTB-2013 Dataset. The OTB-2013 dataset contains 50
videos annotated with 11 various attributes including illu-
mination variation, scale variation, occlusion, deformation
and so on. We evaluate our tracker on this dataset and com-
pare it with 8 state-of-the-art methods that are respectively
ECO [7], CCOT [11], LSART [28], ECO-HC [7], CF2 [24],
Staple [1], MEEM [33] and KCF [19]. We demonstrate the
precision and success plots for different trackers in Figure 6.

p
a
l
r
e
v
o
 
d
e
t
c
e
p
x
E

0.6

0.5

0.4

0.3

0.2

0.1

0

Expected overlap curves for baseline

RPCF[0.3157]
CFWCR[0.3026]
CFCF[0.2857]
ECO[0.2805]
Gnet[0.2737]
MCCT[0.2703]
CCOT[0.2671]
CSR[0.2561]
MCPF[0.2478]
Staple[0.1694]

50
Sequence length

100

200

500

1000

Figure 8. Expected Average Overlap (EAO) curve for 10 state-of-
the-art trackers on the VOT-2017 dataset.

Our RPCF method has a 94.3% DP rate at the threshold of
20 pixels and a 70.9% AUC score. Compared with other
correlation ﬁlter based trackers, the proposed RPCF method
has the best performance in terms of both precision and suc-
cess plots. Our method improves the second best tracker
ECO by 1.9% in terms of DP rates, and has comparable
performance according to the success plots. When the fea-
tures are not compressed via PCA, the tracker (denoted as
RPCF-NC) has a 95.4% DP rate at the threshold of 20 pix-
els and a 71.3% AUC score in success plots, and it runs at
2fps without optimization.

OTB-2015 Dataset. The OTB-2015 dataset is an exten-
sion of the OTB-2013 dataset and contains 50 more video
sequences. On this dataset, we also compare our tracker
with the above mentioned 8 state-of-the-art trackers, and
present the results in Fiugre 7(a)(b). Our RPCF tracker has
a 92.9% DP rate and a 69.0% AUC score. It improves the
second best tracker ECO by 1.9% in terms of the precision
plots. With the non-compressed features, our RPCF-NC
tracker achieves the 93.2% DP rate and 69.6% AUC score,
which again has the best performance among all the com-
pared trackers.

The OTB-2015 dataset divides the image sequences into
11 attributes, each of which corresponds to a challenging
factor. We compare our RPCF tracker against other 8 state-

5789

Precision plots of OPE - illumination variation (38)

1

i

i

n
o
s
c
e
r
P

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

RPCF-NC [0.937]
RPCF [0.924]
LSART [0.915]
ECO [0.914]
CCOT [0.875]
ECO-HC [0.820]
CF2 [0.817]
Staple [0.791]
MEEM [0.740]
KCF [0.719]
30
40

10
Location error threshold

20

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

50

0

0

Precision plots of OPE - scale variation (65)

RPCF [0.917]
RPCF-NC [0.917]
LSART [0.901]
ECO [0.881]
CCOT [0.876]
ECO-HC [0.824]
CF2 [0.802]
MEEM [0.740]
Staple [0.731]
KCF [0.639]
30
40

10
Location error threshold

20

i

i

n
o
s
c
e
r
P

Precision plots of OPE - in-plane rotation (51)
1

Precision plots of OPE - out-of-plane rotation (62)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

RPCF-NC [0.923]
RPCF [0.917]
LSART [0.910]
ECO [0.892]
CCOT [0.868]
CF2 [0.854]
ECO-HC [0.800]
MEEM [0.794]
Staple [0.770]
KCF [0.701]
30
40

10
Location error threshold

20

i

i

n
o
s
c
e
r
P

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

50

0

0

RPCF-NC [0.941]
RPCF [0.934]
LSART [0.915]
ECO [0.906]
CCOT [0.890]
ECO-HC [0.832]
CF2 [0.804]
MEEM [0.791]
Staple [0.734]
KCF [0.672]
30
40

10
Location error threshold

20

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

50

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

50

0

0

Precision plots of OPE - occlusion (49)

RPCF-NC [0.934]
RPCF [0.919]
ECO [0.908]
CCOT [0.902]
LSART [0.897]
ECO-HC [0.848]
CF2 [0.767]
MEEM [0.741]
Staple [0.726]
KCF [0.630]
30
40

10
Location error threshold

20

Precision plots of OPE - fast motion (42)

RPCF-NC [0.888]
RPCF [0.887]
LSART [0.878]
CCOT [0.868]
ECO [0.865]
ECO-HC [0.819]
CF2 [0.792]
MEEM [0.728]
Staple [0.696]
KCF [0.620]
30
40

10
Location error threshold

20

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

50

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

i

n
o
s
c
e
r
P

50

0

0

Precision plots of OPE - motion blur (31)

RPCF-NC [0.924]
RPCF [0.916]
ECO [0.904]
CCOT [0.903]
LSART [0.890]
ECO-HC [0.815]
CF2 [0.797]
Staple [0.726]
MEEM [0.722]
KCF [0.618]
30
40

10
Location error threshold

20

Precision plots of OPE - deformation (44)

LSART [0.908]
RPCF [0.902]
RPCF-NC [0.894]
CCOT [0.860]
ECO [0.859]
ECO-HC [0.806]
CF2 [0.791]
MEEM [0.754]
Staple [0.748]
KCF [0.617]
30
40

10
Location error threshold

20

50

50

Figure 9. Precision plots of different algorithms on 8 attributes, which are respectively illumination variation, scale variation, occlusion,
motion blur, in-plane rotation, out-of-plane rotation, fast motion and deformation.

Table 1. Performance evaluation for 10 state-of-the-art algorithms on the VOT-2017 public dataset. The best three results are marked in
red, blue and green fonts, respectively.

RPCF CFWCR CFCF
0.286
0.509
0.281

0.303
0.484
0.267

EAO 0.316
0.500
A
R
0.234

ECO
0.281
0.483
0.276

Gnet MCCT CCOT
0.274
0.267
0.502
0.494
0.276
0.318

0.270
0.525
0.323

CSR MCPF
0.256
0.248
0.510
0.491
0.356
0.427

Staple
0.169
0.530
0.688

of-the-art trackers and present the precision plots for dif-
ferent trackers in Figure 9. As is illustrated in the ﬁgure,
our RPCF tracker has good tracking performance in all the
listed attributes. Especially, the RPCF tracker improves the
ECO method by 3.6%, 2.5%, 2.8%, 2.2% and 4.3% in the
attributes of scale variation, in-plane rotation, out-of-plane
rotation, fast motion and deformation. The ROI pooled fea-
tures become more consistent across different frames than
the original ones, which contributes to robust target repre-
sentation when the target appearance dramatically changes
(see Figure 2 for example). In addition, by exploiting the
ROI-based pooling operations, the model parameters are
greatly compressed, which makes the proposed tracker in-
susceptible to the over-ﬁtting problem. In Figure 9, we also
present the results of our RPCF-NC tracker for reference.

VOT-2017 Dataset. We test the proposed tracker on the
VOT-2017 dataset for more thorough performance evalu-
ations. The VOT-2017 dataset consists of 60 sequences
with 5 challenging attributes, i.e., occlusion, illumination
change, motion change, size change, camera motion. Dif-
ferent from the OTB-2013 and OTB-2015 datasets, it fo-
cuses on evaluating the short-term tracking performance
and introduces a reset based experiment setting. We com-
pare our RPCF tracker with 9 state-of-the-art trackers in-
cluding CFWCR [17], ECO [7], CCOT [11], MCCT [30],
CFCF [15], CSR [23], MCPF [34], Gnet [20] and Sta-
ple [1]. The tracking performance of different trackers in

terms of EAO, A and R are provided in Table 1 and Figure 8.
Among all the compared trackers, our RPCF method has
a 31.6% EAO score which improves the ECO method by
3.5%. Also, our tracker has the best performance in terms
of robustness measure among all the compared trackers.

6. Conclusion

In this paper, we propose the ROI pooled correlation ﬁl-
ters for visual tracking. Since the correlation ﬁlter algo-
rithm does not extract real-world training samples, it is in-
feasible to perform the pooling operation for each candidate
ROI region like the previous methods. Based on the math-
ematical derivations, we provide an alternative solution for
the ROI-based pooling with the circularly constructed vir-
tual samples. Then, we propose a correlation ﬁlter formula
with equality constraints, and develop an efﬁcient ADMM
solver in the Fourier domain. Finally, we evaluate the pro-
posed RPCF tracker on OTB-2013, OTB-2015 and VOT-
2017 benchmark datasets. Extensive experiments demon-
strate that our method performs favourably against the state-
of-the-art algorithms on all the three datasets.

Acknowledgement. This paper is supported in part by
National Natural Science Foundation of China #61725202,
#61829102, #61872056 and #61751212, and in part by the
Fundamental Research Funds for the Central Universities
under Grant #DUT18JC30. This work is also sponsored by
CCF-Tencent Open Research Fund.

5790

[19] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge
Batista. High-speed tracking with kernelized correlation ﬁl-
ters.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 37(3):583–596, 2015.

[20] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg,
Roman P. Pﬂugfelder, Luka Cehovin Zajc, Tom´as Voj´ır, and
Gustav H¨ager. The visual object tracking vot2017 challenge
results. In ICCV Workshops, 2017.

[21] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep
visual tracking: Review and experimental comparison. Pat-
tern Recognition, 76:323–338, 2018.

[22] David G Lowe. Distinctive image features from scale-
International journal of computer vi-

invariant keypoints.
sion, 60(2):91–110, 2004.

[23] Alan Lukezic, Tomas Vojir, Luka Cehovin Zajc, Jiri Matas,
and Matej Kristan. Discriminative correlation ﬁlter with
channel and spatial reliability. In CVPR, 2017.

[24] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan
Yang. Hierarchical convolutional features for visual tracking.
In ICCV, 2015.

[25] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao,
Qingming Huang, Jongwoo Lim, and Ming-Hsuan Yang.
Hedged deep tracking. In CVPR, 2016.

[26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015.

[27] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[28] Chong Sun, Huchuan Lu, and Ming-Hsuan Yang. Learning
spatial-aware regressions for visual tracking. In CVPR, 2018.
[29] Chong Sun, Dong Wang, Huchuan Lu, and Ming-Hsuan
Yang. Correlation tracking via joint discrimination and re-
liability learning. In CVPR, pages 489–497, 2018.

[30] Ning Wang, Wengang Zhou, Qi Tian, Richang Hong, Meng
Wang, and Houqiang Li. Multi-cue correlation ﬁlters for ro-
bust visual tracking. In CVPR, 2018.

[31] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object

tracking: A benchmark. In CVPR, 2013.

[32] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-
ing benchmark. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 37(9):1834–1848, 2015.

[33] Jianming Zhang, Shugao Ma, and Stan Sclaroff. Meem: ro-
bust tracking via multiple experts using entropy minimiza-
tion. In ECCV, 2014.

[34] Tianzhu Zhang, Changsheng Xu, and Ming-Hsuan Yang.
Multi-task correlation particle ﬁlter for robust object track-
ing. In CVPR, 2017.

References

[1] Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej
Miksik, and Philip HS Torr. Staple: Complementary learners
for real-time tracking. In CVPR, 2016.

[2] David S. Bolme, J. Ross Beveridge, Bruce A. Draper, and
Yui Man Lui. Visual object tracking using adaptive correla-
tion ﬁlters. In CVPR, 2010.

[3] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,
Jonathan Eckstein, et al. Distributed optimization and statis-
tical learning via the alternating direction method of multi-
pliers. Foundations and Trends in Machine learning, 3(1):1–
122, 2011.

[4] Angelika Bunse-Gerstner and Ronald St¨over. On a conjugate
gradient-type method for solving complex symmetric linear
systems. Linear Algebra and its Applications, 287(1-3):105–
123, 1999.

[5] Kenan Dai, Dong Wang, Huchuan Lu, Chong Sun, and Jian-
hua Li. Visual tracking via adaptive spatially-regularized
correlation ﬁlters. In CVPR, 2019.

[6] Navneet Dalal and Bill Triggs. Histograms of oriented gra-

dients for human detection. In CVPR, 2005.

[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan,
Michael Felsberg, et al. Eco: Efﬁcient convolution opera-
tors for tracking. In CVPR, 2017.

[8] Martin Danelljan, Gustav H¨ager, Fahad Khan, and Michael
Felsberg. Accurate scale estimation for robust visual track-
ing. In BMVC, 2014.

[9] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and
Michael Felsberg. Learning spatially regularized correlation
ﬁlters for visual tracking. In ICCV, 2015.

[10] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and
Michael Felsberg. Adaptive decontamination of the training
set: A uniﬁed formulation for discriminative visual tracking.
In CVPR, 2016.

[11] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,
and Michael Felsberg. Beyond correlation ﬁlters: Learn-
ing continuous convolution operators for visual tracking. In
ECCV, 2016.

[12] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.
Learning background-aware correlation ﬁlters for visual
tracking. In ICCV, 2017.

[13] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-
In

age style transfer using convolutional neural networks.
CVPR, 2016.

[14] Ross Girshick. Fast r-cnn. In ICCV, 2015.
[15] Erhan Gundogdu and A Aydın Alatan. Good features to cor-
relate for visual tracking. IEEE Transactions on Image Pro-
cessing, 27(5):2526–2540, 2018.

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[17] Zhiqun He, Yingruo Fan, Junfei Zhuang, Yuan Dong, and
HongLiang Bai. Correlation ﬁlters with weighted convolu-
tion responses. In ICCV Workshops, 2017.

[18] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge
Batista. Exploiting the circulant structure of tracking-by-
detection with kernels. In ECCV, 2012.

5791

