Object-driven Text-to-Image Synthesis via Adversarial Training

Wenbo Li†∗1

,

2 Pengchuan Zhang∗2 Lei Zhang3

Qiuyuan Huang2 Xiaodong He4 Siwei Lyu1

Jianfeng Gao2

1University at Albany, SUNY 2Microsoft Research AI

3Microsoft

4JD AI Research

{wli20,slyu}@albany.edu, {penzhan,leizhang,qihua,jfgao}@microsoft.com, xiaodong.he@jd.com

Abstract

In this paper, we propose Object-driven Attentive Gen-
erative Adversarial Newtorks (Obj-GANs)
that allow
object-centered text-to-image synthesis for complex scenes.
Following the two-step (layout-image) generation process,
a novel object-driven attentive image generator is pro-
posed to synthesize salient objects by paying attention to
the most relevant words in the text description and the
pre-generated semantic layout.
In addition, a new Fast
R-CNN based object-wise discriminator is proposed to
provide rich object-wise discrimination signals on whether
the synthesized object matches the text description and the
pre-generated layout. The proposed Obj-GAN signiﬁcantly
outperforms the previous state of the art in various metrics
on the large-scale COCO benchmark,
increasing the
Inception score by 27% and decreasing the FID score by
11%. A thorough comparison between the traditional grid
attention and the new object-driven attention is provided
through analyzing their mechanisms and visualizing their
attention layers, showing insights of how the proposed
model generates complex scenes in high quality.

1. Introduction

Synthesizing images from text descriptions (known as
Text-to-Image synthesis) is an important machine learning
task, which requires handling ambiguous and incomplete
information in natural language descriptions and learning
across vision and language modalities. Approaches based
on Generative Adversarial Networks (GANs) [5] have re-
cently achieved promising results on this task [23, 22, 32,
33, 29, 16, 9, 12, 34]. Most GAN based methods synthe-
size the image conditioned only on a global sentence vec-
tor, which may miss important ﬁne-grained information at
the word level, and prevents the generation of high-quality
images. More recently, AttnGAN [29] is proposed which
introduces the attention mechanism [28, 30, 2, 27] into the
GAN framework, thus allows attention-driven, multi-stage

† Work was performed when was an intern with Microsoft Research AI.
* indicates equal contributions.

Figure 1: Top: AttnGAN [29] and its grid attention visualization.
Middle: our modiﬁed implementation of two-step (layout-image)
generation proposed in [9]. Bottom: our Obj-GAN and its object-
driven attention visualization. The middle and bottom generations
use the same generated semantic layout, and the only difference is
the object-driven attention.

reﬁnement for ﬁne-grained text-to-image generation.

Although images with realistic texture have been synthe-
sized on simple datasets, such as birds [29, 16] and ﬂower-
s [33], most existing approaches do not speciﬁcally mod-
el objects and their relations in images and thus have d-
ifﬁculties in generating complex scenes such as those in
the COCO dataset [15]. For example, generating images
from a sentence “several people in their ski gear are in the
snow” requires modeling of different objects (people, ski
gear) and their interactions (people on top of ski gear), as
well as ﬁlling the missing information (e.g., the rocks in the
In the top row of Fig. 1, the image gener-
background).
ated by AttnGAN does contain scattered texture of people
and snow, but the shape of people are distorted and the pic-
ture’s layout is semantically not meaningful. [9] remedies
this problem by ﬁrst constructing a semantic layout from the
text and then synthesizing the image by a deconvolutional

112174

image generator. However, the ﬁne-grained word/object-
level information is still not explicitly used for generation.
Thus, the synthesized images do not contain enough details
to make them look realistic (see the middle row of Fig. 1).
In this study, we aim to generate high-quality complex
images with semantically meaningful layout and realistic
objects. To this end, we propose a novel Object-driven At-
tentive Generative Adversarial Networks (Obj-GAN) that
effectively capture and utilize ﬁne-grained word/object-
level information for text-to-image synthesis. The Obj-
GAN consists of a pair of object-driven attentive image
generator and object-wise discriminator, and a new object-
driven attention mechanism. The proposed image genera-
tor takes as input the text description and a pre-generated
semantic layout and synthesize high-resolution images vi-
a multiple-stage coarse-to-ﬁne process. At every stage, the
generator synthesizes the image region within a bounding
box by focusing on words that are most relevant to the ob-
ject in that bounding box, as illustrated in the bottom row
of Fig. 1. More speciﬁcally, using a new object-driven at-
tention layer, it uses the class label to query words in the
sentences to form a word context vector, as illustrated in
Fig. 4, and then synthesizes the image region conditioned
on the class label and word context vector. The object-wise
discriminator checks every bounding box to make sure that
the generated object indeed matches the pre-generated se-
mantic layout. To compute the discrimination losses for all
bounding boxes simultaneously and efﬁciently, our object-
wise discriminator is based on a Fast R-CNN [4], with a
binary cross-entropy loss for each bounding box.

The contribution of this work is three-folded.

(i) An
Object-driven Attentive Generative Network (Obj-GAN) is
proposed for synthesizing complex images from text de-
scriptions. Speciﬁcally, two novel components are pro-
posed, including the object-driven attentive generative net-
work and the object-wise discriminator. (ii) Comprehensive
evaluation on a large-scale COCO benchmark shows that
our Obj-GAN signiﬁcantly outperforms previous state-of-
the-art text-to-image synthesis methods. Detailed ablation
study is performed to empirically evaluate the effect of d-
ifferent components in Obj-GAN. (iii) A thorough analy-
sis is performed through visualizing the attention layers of
the Obj-GAN, showing insights of how the proposed model
generates complex scenes in high quality. Compared with
the previous work, our object-driven attention is more ro-
bust and interpretable, and signiﬁcantly improves the object
generation quality in complex scenes.

2. Related Work

through different approaches, such as variational infer-
ence [17, 6], approximate Langevin process [24], condition-
al PixelCNN via maximal likelihood estimation [26, 24],
and conditional generative adversarial networks [23, 22, 32,
33]. Compared with other approaches, Generative Adver-
sarial Networks (GANs) [5] have shown better performance
in image generation [21, 3, 25, 13, 11, 10]. However, ex-
isting GAN based text-to-image synthesis is usually condi-
tioned only on the global sentence vector, which misses im-
portant ﬁne-grained information at the word level, and thus
lacks the ability to generate high-quality images. [29] uses
the traditional grid visual attention mechanism in this task,
which enables synthesizing ﬁne-grained details at different
image regions by paying attentions to the relevant words in
the text description.

To explicitly encode the semantic layout into the gen-
erator, [9] proposes to decompose the generation process
into two steps, in which it ﬁrst constructs a semantic lay-
out (bounding boxes and object shapes) from the text and
then synthesizes an image conditioned on the layout and
text description. [12] also proposes such a two-step process
to generate images from scene graphs, and their process can
be trained end-to-end. In this work, the proposed Obj-GAN
follows the two-step generation process as [9]. However,
[9] encodes the text into a single global sentence vector,
which loses word-level ﬁne-grained information. More-
over, it uses the image-level GAN loss for the discriminator,
which is less effective at providing object-wise discrimina-
tion signal for generating salient objects. We propose a new
object-driven attention mechanism to provide ﬁne-grained
information (words in the text description and objects in the
layout) for different components, including an attentive se-
q2seq bounding box generator, an attentive image generator
and an object-wise discriminator.

The attention mechanism has recently become a crucial
part of vision-language multi-modal intelligence tasks. The
traditional grid attention mechanism has been successfully
used in modeling multi-level dependencies in image cap-
tioning [28], image question answering [30], text-to-image
generation [29], unconditional image synthesis [31] and
image-to-image translation [16], image/text retrieval [14].
In 2018, [1] proposes a bottom-up attention mechanis-
m, which enables attention to be calculated over semantic
meaningful regions/objects in the image, for image caption-
ing and visual question-answering. Inspired by these work-
s, we propose Obj-GAN which for the ﬁrst time develops
an object-driven attentive generator plus an object-wise dis-
criminator, thus enables GANs to synthesize high-quality
images of complicated scenes.

Generating photo-realistic images from text description-
s, though challenging, is important to many real-world ap-
plications such as art generation and computer-aided de-
sign. There has been much research effort for this task

3. Object-driven Attentive GAN

As illustrated in Fig. 2, the Obj-GAN performs text-to-
image synthesis in two steps: generating a semantic lay-

12175

Figure 2: Obj-GAN completes the text-to-image synthesis in two steps: the layout generation and the image generation. The layout
generation contains a bounding box generator and a shape generator. The image generation uses the object-driven attentive image generator.

out (class labels, bounding boxes, shapes of salient object-
s), and then generating the image. In the image generation
step, the object-driven attentive generator and object-wise
discriminator are designed to enable image generation con-
ditioned on the semantic layout generated in the ﬁrst step.

The input of Obj-GAN is a sentence with Ts tokens.
With a pre-trained bi-LSTM model, we encode its words
as word vectors e ∈ RD×Ts and the entire sentence as a
global sentence vector ¯e ∈ RD. We provide details of this
pre-trained bi-LSTM model and the implementation details
of other modules of Obj-GAN in § ??.

3.1. Semantic layout generation

In the ﬁrst step, the Obj-GAN takes the sentence as input
and generates a semantic layout, a sequence of objects spec-
iﬁed by their bounding boxes (with class labels) and shapes.
As illustrated in Fig. 2, a box generator ﬁrst generates a se-
quence of bounding boxes, and then a shape generator gen-
erates their shapes. This part resembles the bounding box
generator and shape generator in [9], and we put our imple-
mentation details in § ??.
Box generator. We train an attentive seq2seq model [2],
also referring to Fig. 2, as the box generator:

B1:T := [B1, B2, . . . , BT ] ∼ Gbox(e).

(1)

Here, e are the pre-trained bi-LSTM word vectors, Bt =
(lt, bt) are the class label of the t’s object and its bounding
box b = (x, y, w, h) ∈ R4. In the rest of the paper, we will
also call the label-box pair Bt as a bounding box when no
confusion arises. Since most of the bounding boxes have
corresponding words in the sentence, the attentive seq2seq
model captures this correspondence better than the seq2seq
model used in [9].
Shape generator. Given the bounding boxes B1:T , the
shape generator predicts the shape of each object in its
bounding box, i.e.,

cM1:T = Gshape(B1:T , z1:T ).

(2)

where zt ∼ N (0, 1) is a random noise vector. Since the
generated shapes not only need to match the location and
category information provided by B1:T , but also should be
aligned with its surrounding context, we build Gshape based
on a bi-directional convolutional LSTM, as illustrated in
Fig. 2. Training of Gshape is based on the GAN framework
[9], in which a perceptual loss is also used to constrain the
generated shapes and to stabilize the training.

3.2. Image generation

3.2.1 Attentive multistage image generator

As shown in Fig. 3, the proposed attentive multistage gen-
erative network has two generators (G0, G1). The base gen-

erator G0 ﬁrst generates a low-resolution image bx0 condi-

tioned on the global sentence vector and the pre-generated
semantic layout. The reﬁner G1 then reﬁnes details in d-
ifferent regions by paying attention to most relevant words
and pre-generated class labels and generates a higher reso-

lution image bx1. Speciﬁcally,

bx0 = G0(h0),
bx1 = G1(h1),

¯e, Enc(M 0), cobj, clab),
h0 = F0(z,
h1 = F1(cpat, h0 + Enc(M 1), cobj, clab),
where (i) z is a random vector with standard normal distri-
bution; (ii) Enc(M 0) ( Enc(M 1) ) is the encoding of low-
resolution shapes M 0 (higher-resolution shapes M 1); (iii)
cpat = F grid
attn (e, h0) are the patch-wise context vectors from
the traditional grid attention, (iv) cobj = F obj
attn(e, eg, lg, M )
are the object-wise context vectors from our new object-
driven attention, and clab = clab(lg, M ) are the label context
vectors from class labels. We can stack more reﬁners to the
generation process and get higher and higher resolution im-
ages. In this paper, we have two reﬁners (G1 and G2) and
ﬁnally generate images with resolution 256 × 256.
Compute context vectors via attention. Both patch-wise
context vectors cpat and object-wise context vectors cobj are
attention-driven context vectors for speciﬁc image regions,
and encode information from the words that are most rel-
evant to that image region. Patch-wise context vectors are

12176

Figure 3: The object-driven attentive image generator.

For the traditional grid attention, we use the image region
feature hj , which is one column in the previous hidden lay-
er h ∈ RDpat×N pat
, to query the pre-trained bi-LSTM word
vectors e. For the new object-driven attention, we use the
GloVe embedding of object class label lg
t to query the GloVe
embedding of the words in the sentence, as illustrated in the
lower part of Fig. 4.
Feature map concatenation. The patch-wise context vec-
tor cpat
j can be directly concatenated with the image feature
vector hj in the previous layer. However, the object-wise
context vector cobj
cannot, because they are associated with
t
bounding boxes instead of pixels in the hidden feature map.
We propose to copy the object-wise context vector cobj
to
t
every pixel where the t’th object is present, i.e., Mt ⊗ cobj
where ⊗ is the vector outer-product, as illustrated in the
upper-right part of Fig. 4. 1

t

If there are multiple bounding boxes covering the same
pixel, we have to decide whose context vector should be
used on this pixel. In this case, we simply do a max-pooling
across all the bounding boxes:

cobj = max

t :1≤t≤T

Mt ⊗ cobj

t

.

(6)

Then cobj can be concatenated with the feature map h and
patch-wise context vectors cpat for next-stage generation.
Label context vectors. Similarly, we distribute the class
label information to the entire hidden feature map to get the
label context vectors, i.e.,

clab = max

t : 1≤t≤T

Mt ⊗ eg
t .

(7)

Finally, we concatenate h, cpat, cobj and clab and pass the

1This operation can be viewed as an inverse of the pooling operator.

12177

Figure 4: Object-driven attention.

for uniform-partitioned image patches determined by the u-
niform down-sampling/up-sampling structure of CNN, but
these patches are not semantically meaningful. Object-wise
context vectors are for semantically meaningful image re-
gions speciﬁed by bounding boxes, but these regions are at
different scales and may have overlaps.

Speciﬁcally,

the patch-wise context vector cpat
j

(

objective-wise context vector cobj
t ) is a dynamic represen-
tation of word vectors relevant to patch j (bounding box
Bt), which is calculated by

cpat
j =

βpat
j,i ei,

cobj
t =

βobj
t,i ei.

(3)

TsX

i=1

TsX

i=1

j,i ( βobj

Here, βpat
t,i ) indicates the weight the model attends to
the i’th word when generating patch j (bounding box Bt)
and is computed by

βpat
j,i =

βobj
t,i =

exp(spat
j,i)
PTs
k=1 exp(spat
j,k)
exp(sobj
t,i )
PTs
k=1 exp(sobj
t,k)

,

,

spat
j,i = (hj)T ei,

(4)

sobj
t,i = (lg

t )T eg
i .

(5)

concatenated tensor through one up-sampling layer and sev-
eral residual layers to generate a higher-resolution image.
Grid attention vs. object-driven attention. The process
to compute the patch-wise context vectors above is the tra-
ditional grid attention mechanism used in AttnGAN [29].
Note that its attention weights βpat
j,i and context vector cpat
are useful only when the hidden feature hpat
in the G0 stage
j
correctly captures the content to be drawn in patch j. This
essentially assumes that the generation in the G0 stage al-
ready captures a rough sketch (semantic layout). This as-
sumption is valid for simple datasets like birds [29], but
fails for complex datasets like COCO [15] where the gen-

j

erated low-resolution image bx0 typically does not have a

meaningful layout. In this case, the grid attention is even
harmful, because patch-wise context vector is attended to a
wrong word and thus generate the texture associated with
that wrong word. This may be the reason why AttnGAN’s
generated image contains scattered patches of realistic tex-
ture but overall is semantically not meaningful; see Fig. 1
for example. Similar phenomenon is also observed in Deep-
Dream [20]. On the contrary, in our object-driven attention,
the attention weights βobj
rely on the
class label lg
t of the bounding box and are independent of
the generation in the G0 stage. Therefore, the object-wise
context vectors are always helpful to generate images that
are consistent with the pre-generated semantic layout. An-
other beneﬁt of this design is that the context vector cobj
t can
also be used in the discriminator, as we present in § 3.2.2.

t,i and context vector cobj

t

3.2.2 Discriminators

We design patch-wise and object-wise discriminators to
train the attentive multi-stage generator above. Given a
patch from uniformly-partitioned image patches determined
by the uniform down-sampling structure of CNN, the patch-
wise discriminator is trying to determine whether this patch
is realistic or not (unconditional) and whether this patch is
consistent with the sentence description or not (condition-
al). Given a bounding box and the class label of the object
within it, the object-wise discriminator is trying to deter-
mine whether this region is realistic or not (unconditional)
and whether this region is consistent with the sentence de-
scription and given class label or not (conditional).
Patch-wise discriminators. Given an image-sentence pair
x, ¯e (¯e is the sentence vector), the patch-wise unconditional
and text discriminator can be written as

ppat,un = Dpat

ppat,con = Dpat

uncond.(Enc(x)),

text(Enc(x), ¯e),
(8)
where Enc is a convolutional feature extractor that extracts
patch-wise features, Duncond. ( Dpat
text ) determine whether the
patch is realistic (consistent with the text description) or not.
Shape discriminator.
In a similar manner, we have our
patch-wise shape discriminator

Figure 5: Object-wise discriminator.

ppix = Dpix(Enc(x, M )),

(9)

where we ﬁrst concatenate the image x and shapes M in the
channel dimension, and then extracts patch-wise features by
another convolutional feature extractor Enc. The probabili-
ties ppix determine whether the patch is consistent with the
given shape. Our patch-wise discriminators Dpat
uncond., Dpat
text
and Dpix resembles the PatchGAN [11] for the image-to-
image translation task. Compared with the global discrim-
inators in AttnGAN [29], the patch-wise discriminators not
only reduce the model size and thus enable generating high-
er resolution images, but also increase the quality of gener-
ated images; see Table 1 for experimental evidence.

Object-wise discriminators. Given an image x, bounding
boxes of objects B1:T and their shapes M , we propose the
following object-wise discriminators:

t }T

{hobj
uncond.(hobj

t=1 =FastRCNN(x, M, B1:T ),
, eg

t ), pobj,con

= Dobj(hobj
t

t

t , cobj

t ).

pobj,un
t

= Dobj

(10)
Here, we ﬁrst concatenate the image x and shapes M and
extract a region feature vector hobj
for each bounding box
t
through a Fast R-CNN model [4] with an ROI-align lay-
er [7]; see Fig. 5(a). Then similar to the patch-wise dis-
criminator (8), the unconditional (conditional) probabilities
pobj,un
) determine whether the t’th object is real-
t
istic (consistent with its class label eg
t and its text context
information cobj
t is the GloVe
embedding of the class label and cobj
is its text context in-
t
formation deﬁned in (3).

t ) or not; see Fig. 5(b). Here, eg

( pobj,con

t

All discriminators are trained by the traditional cross en-

tropy loss [5].

12178

3.2.3 Loss function for the image generator

The generator’s GAN loss is a weighted sum of these dis-
criminators’ loss, i.e.,

LGAN(G) = −

λobj
T

TX

t=1

−

1

N pat

N patX

j=1


log ppat,un
|
{z
}

uncond. loss

j




t

obj uncond. loss


 log pobj,un
{z
}
|
|
+ λtxt log ppat,con
|
}

text cond. loss

{z

j

+ log pobj,con

t

obj cond. loss

}

{z
+ λpix log ppix
|
}

shape cond. loss

{z

j


 .

Here, T is the number of bounding boxes, N pat is the num-
ber of regular patches, (λobj, λtxt, λpix) are the weights of the
object-wise GAN loss, patch-wise text conditional loss and
patch-wise shape conditional loss, respectively. We tried
combining our discriminators with the spectral normalized
projection discriminator [18, 19], but did not see signiﬁcant
performance improvement. We report performance of the
spectral normalized version in § 4.1 and provide model ar-
chitecture details in § ??.

Combined with the deep multi-modal attentive similarity
model (DAMSM) loss introduced in [29], our ﬁnal image
generator’s loss is

LG = LGAN + λDAMSMLDAMSM

(11)

where λdamsm is a hyper-parameter to be tuned. Here,
the DAMSM loss is a word level ﬁne-grained image-text
matching loss computed, which will be elaborated in § ??.
Based on the experiments on a held-out validation set, we
set the hyperparameters in this section as: λobj = 0.1, λtxt =
0.1, λpix = 1 and λdamsm = 100.

Remark 3.1. Both the patch-wise and object-wise discrim-
inators can be applied to different stages in the generation.
We apply the patch-wise discriminator for every stage of the
generation, following [33, 11], but only apply the object-
wise discriminator at the ﬁnal stage.

4. Experiments

Dataset. We use the COCO dataset [15] for evaluation. It
contains 80 object classes, where each image is associat-
ed with object-wise annotations (i.e., bounding boxes and
shapes) and 5 text descriptions. We use the ofﬁcial 2014
train (over 80K images) and validation (over 40K images)
splits for training and test stages, respectively.
Evaluation metrics. We use the Inception score [25] and
Fr´echet inception distance (FID) [8] score as the quantita-
tive evaluation metrics. In our experiments, we found that
Inception score can be saturated, even over-ﬁtted, while FID
is a more robust measure and aligns better with human qual-
itative evaluation. Following [29], we also use R-precision,

Table 1: The quantitative experiments. Methods marked with 0, 1
and 2 respectively represent experiments using the predicted boxes
and shapes, the ground-truth boxes and predicted shapes, and the
ground-truth boxes and shapes. We use bold, ∗, and ∗∗ to high-
light the best performance under these three settings, respectively.
The results of methods marked with † are those reported in the
original papers. ↑ (↓) means the higher (lower), the better.

Methods
Obj-GAN0
Obj-GAN1
Obj-GAN2
P-AttnGAN w/ Lyt0
P-AttnGAN w/ Lyt1
P-AttnGAN w/ Lyt2
P-AttnGAN
Obj-GAN w/ SN0
Obj-GAN w/ SN1
Obj-GAN w/ SN2
Reed et al. [23]†
StackGAN [32]†
AttnGAN [29]
vmGAN [35]†
Sg2Im [12]†
Infer [9]0†
Infer [9]1†
Infer [9]2†
Obj-GAN-SOTA0
Obj-GAN-SOTA1
Obj-GAN-SOTA2

Inception ↑

FID ↓

R-prcn (%) ↑

27.37 ± 0.22
27.96 ± 0.39∗
29.89 ± 0.22∗∗

25.85
24.19∗
20.75∗∗

86.20 ± 2.98
88.36 ± 2.82
89.59 ± 2.67
65.71 ± 3.74
68.40 ± 3.79
70.94 ± 3.70
86.71 ± 2.97
86.84 ± 2.82
88.70 ± 2.65∗
89.97 ± 2.56∗∗

n/a
n/a

59.02
54.96
48.47
41.51
29.07
27.26
23.37

n/a
n/a

28.76

82.98 ± 3.15

n/a
n/a
n/a
n/a
n/a

25.64
24.28
21.21

n/a
n/a
n/a
n/a
n/a

91.05 ± 2.34
92.54 ± 2.16
93.39 ± 2.08

18.84 ± 0.29
19.32 ± 0.29
20.81 ± 0.16
26.31 ± 0.43
26.97 ± 0.31
27.41 ± 0.17
28.75 ± 0.32

7.88 ± 0.07
8.45 ± 0.03
23.79 ± 0.32
9.94 ± 0.12

6.7 ± 0.1

11.46 ± 0.09
11.94 ± 0.09
12.40 ± 0.08
30.29 ± 0.33
30.91 ± 0.29
32.79 ± 0.21

a common evaluation metric for ranking retrieval results, to
evaluate whether the generated image is well conditioned
on the given text description. More speciﬁcally, given a
pre-trained image-to-text retrieval model, we use generat-
ed images to query their corresponding text descriptions.

First, given generated image bx conditioned on sentence s

and 99 random sampled sentences {s′
i : 1 ≤ i ≤ 99}, we
rank these 100 sentences by the pre-trained image-to-text
If the ground truth sentence s is ranked
retrieval model.
highest, we count this a success retrieval. For all the im-
ages in the test dataset, we perform this retrieval task once
and ﬁnally count the percentage of success retrievals as the
R-precision score.

It is important to point out that none of these quantita-
tive metrics are perfect. Better metrics are required to eval-
uate image generation qualities in complicated scenes. In
fact, the Inception score completely fails in evaluating the
semantic layout of the generated images. The R-precision s-
core depends on the pre-trained image-to-text retrieval mod-
el it uses, and can only capture the aspects that the retrieval
model is able to capture. The pre-trained model we use
is still limited in capturing the relations between objects in
complicated scenes, so is our R-precision score.
Quantitative evaluation. We compute these three metrics
under two settings for the full validation dataset.
Qualitative evaluation. Apart from the quantitative evalu-
ation, we also visualize the outputs of all ablative version-
s of Obj-GAN and the state-of-the-art methods (i.e., [29])
whose pre-trained models are publicly available.

12179

Figure 6: The overall qualitative comparison. All images are generated without the usage of any ground-truth information.

4.1. Ablation study

In this section, we ﬁrst evaluate the effectiveness of the
object-driven attention. Next, we compare the object-driven
attention mechanism with the grid attention mechanism.
Then, we evaluate the impact of the spectral normalization
for Obj-GAN. We use Fig. 6 and the higher half of Table 1 to
present the comparison among different ablative versions of
Obj-GAN. Note that all ablative versions have been trained
with batch size 16 for 60 epochs. In addition, we use the
lower half of Table 1 to show the comparison between Obj-
GAN and previous methods. Finally, we validated the Obj-
GAN’s generalization ability on the novel text descriptions.
Object-driven attention. To evaluate the efﬁcacy of the
object-driven attention mechanism, we implement a base-
line, named P-AttnGAN w/ Lyt, by disabling the object-
driven attention mechanism in Obj-GAN. In essence, P-
AttnGAN w/ Lyt can be considered as an improved version
of AttnGAN with the patch-wise discriminator (abbreviat-
ed as the preﬁx “P-” in name) and the modules (e.g., shape
discriminator) for handling the conditional layout (abbre-
viated as “Lyt”). Moreover, it can also be considered as a
modiﬁed implementation of [9], which resembles their two-
step (layout-image) generation. Note that there are three
key differences between P-AttnGAN w/ Lyt and [9]: (i)
P-AttnGAN w/ Lyt has a multi-stage image generator that
gradually increases the generated resolution and reﬁnes the
generated images, while [9] has a single-stage image gen-
erator. (ii) With the help of the grid attentive module, P-
AttnGAN w/ Lyt is able to utilize the ﬁne-grained word-
level information, while [9] conditions on the global sen-
tence information. (iii) The third difference lies in their loss

functions: P-AttnGAN w/ Lyt uses the DAMSM loss in (11)
to penalize the mismatch between the generated images and
the input text descriptions, while [9] uses the perceptual loss
to penalize the mismatch between the generated images and
the ground-truth images. As shown in Table 1, P-AttnGAN
w/ Lyt yields higher Inception score than [9] does.

We compare Obj-GAN with P-AttnGAN w/ Lyt under
three settings, with each corresponding to a set of condi-
tional layout input, i.e., the predicted boxes & shapes, the
ground-truth boxes & predicted boxes, and the ground-truth
boxes & shapes. As presented in Table 1, Obj-GAN con-
sistently outperforms P-AttnGAN w/ Lyt on all three met-
rics. In Fig. 7, we use the same layout as the conditional
input, and compare the visual quality of their generated im-
ages. An interesting phenomenon shown in Fig. 7 is that
both the foreground objects (e.g., airplane and train) and the
background (e.g., airport and trees) textures synthesized by
Obj-GAN are much richer and smoother than those using
P-AttnGAN w/ Lyt. The effectiveness of the object-driven
attention for the foreground objects is easy to understand.
The beneﬁts for the background textures using the object-
driven attention mechanism is probably due to the fact that
it implicitly provides stronger signal that distinguishes the
foreground. As such, the image generator may have richer
guidance and clearer emphasis when synthesizing textures
for a certain region.

Grid attention vs. object-driven attention. We compare
Obj-GAN with P-AttnGAN herein, so as to compare the ef-
fects of the object-driven and the grid attention mechanism-
s. In Fig. 8, we show the generated image of each method
as well as the corresponding attention maps aligned on the

12180

Figure 7: Qualitative comparison with P-AttnGAN w/ Lyt.

Figure 8: Qualitative comparison with P-AttnGAN. The attention
maps of each method are shown beside the generated image.

right side. In a grid attention map, the brightness of a region
reﬂects how much this region attended to the word above the
map. As for the object-driven attention map, the word above
each attention map is the most attended word by the high-
lighted object. The highlighted region of an object-driven
attention map is the object shape.

As analyzed in § 3.2.1, the reliability of grid attention
weights depends on the quality of the previous layer’s im-
age region features. This makes the grid attention unreliable
sometimes, especially for complex scenes. For example,
the grid attention weights in Fig. 8 are unreliable because
they are scattered (e.g., the attention map for “man”) and
inaccurate. However, this is not a problem for the object-
driven attention mechanism, because its attention weights
are directly calculated from embedding vectors of word-
s and class labels and are independent of image features.
Moreover, as shown in Fig. 4 and Equ. (6), the impact re-
gion of the object-driven attention context vector is bounded
by the object shapes, which further enhances its semantics
meaningfulness. As a result, the instance-driven attention
signiﬁcantly improves the visual quality of the generated
images, as demonstrated in Fig. 8. Moreover, the perfor-
mance can be further improved if the semantic layout gen-
eration is improved. In the extreme case, Obj-GAN based
on ground truth layout (Obj-GAN2) has the best visual qual-
ity (the rightmost column of Fig. 8) and the best quantitative
evaluation (Table 1).
Obj-GAN w/ SN vs. Obj-GAN. We present the compari-
son between the cases with or without spectral normaliza-
tion in the discriminators in Table 1 and Fig. 6. We observe
that there is no obvious improvement on the visual quali-
ty, but slightly worse on the quantitative metrics. We show

Figure 9: Generated images for novel descriptions.

more results and discussions in § ??.
Comparison with previous methods. To compare Obj-
GAN with the previous methods, initialized by the Obj-
GAN models in the ablation study, we trained Obj-GAN-
SOTA with batch size 64 for 10 more epochs. In order to
evaluate AttnGAN on FID, we conducted the evaluation on
the ofﬁcially released pre-trained model. Note that the S-
g2Im [12] focuses on generating images from scene graphs
and conducted the evaluation on a different split of COCO.
However, we still included Sg2Im’s results to reﬂect the
broader context of the related topic. As shown in Table 1,
Obj-GAN-SOTA outperforms all previous methods signif-
icantly. We notice that the increment of batch size does
boost the Inception score and R-precision, but does not im-
prove FID. The possible explanation is: with a larger batch
size, the DAMSM loss (a ranking loss in essence) in (11)
plays a more important role and improves Inception and R-
precision, but it does not focus on reducing FID between
the generated images and the real ones.
Generalization ability. We further investigate if Obj-GAN
just memorizes the scenarios in COCO or it indeed learn-
s the relations between the objects and their surroundings.
To this end, we compose several descriptions which reﬂect
novel scenarios that are unlikely to happen in the real-world,
e.g., a decker bus is ﬂoating on top of a lake, or a cat is
catching a frisbee. We use Obj-GAN to synthesize images
for these rare scenes. The results in Fig. 9 further demon-
strate the good generalization ability of Obj-GAN.

5. Conclusions

In this paper, we have presented a multi-stage Object-
driven Attentive Generative Adversarial Networks (Obj-
GANs) for synthesizing images with complex scenes from
the text descriptions. With a novel object-driven attention
layer at each stage, our generators are able to utilize the
ﬁne-grained word/object-level information to gradually re-
ﬁne the synthesized image. We also proposed the Fast R-
CNN based object-wise discriminators, each of which is
paired with a conditional input of the generator and provides
object-wise discrimination signal for that condition. Our
Obj-GAN signiﬁcantly outperforms previous state-of-the-
art GAN models on various metrics on the large-scale chal-
lenging COCO benchmark. Extensive experiments demon-
strate the effectiveness and generalization ability of Obj-
GAN on text-to-image generation for complex scenes.

12181

References

[1] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
S. Gould, and L. Zhang. Bottom-up and top-down attention
for image captioning and vqa. CVPR, 2018.

[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate. arX-
iv:1409.0473, 2014.

[3] E. L. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep
generative image models using a laplacian pyramid of adver-
sarial networks. In NIPS, 2015.

[4] R. B. Girshick. Fast R-CNN. In ICCV, 2015.

[5] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio.
Generative adversarial nets. In NIPS, 2014.

[6] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and
D. Wierstra. DRAW: A recurrent neural network for image
generation. In ICML, 2015.

[7] K. He, G. Gkioxari, P. Doll´ar, and R. B. Girshick. Mask

R-CNN. In ICCV, 2017.

[8] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. K-
lambauer, and S. Hochreiter. GANs trained by a two time-
scale update rule converge to a nash equilibrium. NIPS,
2017.

[9] S. Hong, D. Yang, J. Choi, and H. Lee. Inferring semantic
layout for hierarchical text-to-image synthesis. CVPR, 2018.

[10] Q. Huang, P. Zhang, D. O. Wu, and L. Zhang. Turbo learning

for captionbot and drawingbot. In NeurIPS, 2018.

[11] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017.

[12] J. Johnson, A. Gupta, and L. Fei-Fei. Image generation from

scene graphs. In CVPR, 2018.

[13] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Aitken, A. Te-
jani, J. Totz, Z. Wang, and W. Shi. Photo-realistic single im-
age super-resolution using a generative adversarial network.
In CVPR, 2017.

[14] K. Lee, X. Chen, G. Hua, H. Hu, and X. He. Stacked cross

attention for image-text matching. ECCV, 2018.

[15] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Dollr, and C. L. Zitnick. Microsoft coco: Common
objects in context. In ECCV, 2014.

[16] S. Ma, J. Fu, C. W. Chen, and T. Mei. DA-GAN: Instance-
level image translation by deep attention generative adver-
sarial networks. In CVPR, 2018.

[17] E. Mansimov, E. Parisotto, L. J. Ba, and R. Salakhutdinov.
In ICLR,

Generating images from captions with attention.
2016.

[18] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spec-
tral normalization for generative adversarial networks. ICLR,
2018.

[19] T. Miyato and M. Koyama. cgans with projection discrimi-

nator. ICLR, 2018.

[20] A. Mordvintsev, C. Olah, and M. Tyka. Deep dream, 2015,

2017.

[21] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. In ICLR, 2016.

[22] S. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and

H. Lee. Learning what and where to draw. In NIPS, 2016.

[23] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and
H. Lee. Generative adversarial text-to-image synthesis. In
ICML, 2016.

[24] S. E. Reed, A. van den Oord, N. Kalchbrenner, S. G. Col-
menarejo, Z. Wang, Y. Chen, D. Belov, and N. de Fre-
itas. Parallel multiscale autoregressive density estimation.
In ICML, 2017.

[25] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung,
A. Radford, and X. Chen. Improved techniques for training
gans. In NIPS, 2016.

[26] A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt,
A. Graves, and K. Kavukcuoglu. Conditional image genera-
tion with pixelcnn decoders. In NIPS, 2016.

[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all
you need. NIPS, 2017.

[28] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhut-
dinov, R. S. Zemel, and Y. Bengio. Show, attend and tell:
Neural image caption generation with visual attention.
In
ICML, 2015.

[29] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and
X. He. Attngan: Fine-grained text to image generation with
attentional generative adversarial networks. CVPR, 2018.

[30] Z. Yang, X. He, J. Gao, L. Deng, and A. J. Smola. Stacked
attention networks for image question answering. In CVPR,
2016.

[31] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-
attention generative adversarial networks. arXiv preprint
arXiv:1805.08318, 2018.

[32] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and
D. Metaxas. Stackgan: Text to photo-realistic image synthe-
sis with stacked generative adversarial networks. In ICCV,
2017.

[33] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and
D. N. Metaxas. Stackgan++: Realistic image synthesis with
stacked generative adversarial networks. TPAMI, 2018.

[34] S. Zhang, H. Dong, W. Hu, Y. Guo, C. Wu, D. Xie, and
F. Wu. Text-to-image synthesis via visual-memory creative
adversarial network. In PCM, 2018.

[35] S. Zhang, H. Dong, W. Hu, Y. Guo, C. Wu, D. Xie, and
F. Wu. Text-to-image synthesis via visual-memory creative
adversarial network. In PCM, 2018.

12182

