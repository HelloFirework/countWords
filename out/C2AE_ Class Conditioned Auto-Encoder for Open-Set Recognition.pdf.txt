C2AE: Class Conditioned Auto-Encoder for Open-set Recognition

Poojan Oza and Vishal M. Patel

Department of Electrical and Computer Engineering

Johns Hopkins University, 3400 N. Charles St, Baltimore, MD 21218, USA

poza2@jhu.edu, vpatel36@jhu.edu

Abstract

Models trained for classiﬁcation often assume that all
testing classes are known while training. As a result,
when presented with an unknown class during testing, such
closed-set assumption forces the model to classify it as one
of the known classes. However, in a real world scenario,
classiﬁcation models are likely to encounter such exam-
ples. Hence, identifying those examples as unknown be-
comes critical to model performance. A potential solu-
tion to overcome this problem lies in a class of learning
problems known as open-set recognition.
It refers to the
problem of identifying the unknown classes during testing,
while maintaining performance on the known classes. In
this paper, we propose an open-set recognition algorithm
using class conditioned auto-encoders with novel training
and testing methodologies. In this method, training proce-
dure is divided in two sub-tasks, 1. closed-set classiﬁcation
and, 2. open-set identiﬁcation (i.e.
identifying a class as
known or unknown). Encoder learns the ﬁrst task following
the closed-set classiﬁcation training pipeline, whereas de-
coder learns the second task by reconstructing conditioned
on class identity. Furthermore, we model reconstruction er-
rors using the Extreme Value Theory of statistical modeling
to ﬁnd the threshold for identifying known/unknown class
samples. Experiments performed on multiple image classi-
ﬁcation datasets show that the proposed method performs
signiﬁcantly better than the state of the art methods. The
source code is available at: github.com/otkupjnoz/c2ae.

1. Introduction

Recent advancements in computer vision have resulted
in signiﬁcant improvements for tasks such as image classi-
ﬁcation [16], [24], [17], [48], Detection [41], [40], [4], [49],
Clustering [19], [1], [8], [2], etc. Speciﬁcally, for classiﬁ-
cation task the rise of Deep Convolutional Neural Network
has resulted in error rates surpassing the human-level per-
formance [15]. These promising results, enable their poten-
tial use in many real world applications. However, when

Known 
Classes 

? 

? 

? 

? 

? 
Unknown 
Classes 
? 

? 

? 

? 

? 

? 

Known 
Classes 

? 

Figure 1: Open-set recognition problem: Data samples
from Blue Jay, Seal, Dog and Penguin are from the known
class set (K). Also, many classes not known during train-
ing, will be present at testing, i.e., samples from unknown
class set (U ). The goal is to correctly classify any sample
coming from set K, as either Blue Jay, Seal, Dog or Penguin
and identify samples coming from U as unknown.

deployed in a real world scenario, such systems are likely
to observe samples from classes not seen during training
(i.e. unknown classes also referred as “unknown unknowns”
[44]). Since, the traditional training methods follow this
closed-set assumption, the classiﬁcation systems observing
any unknown class samples are forced to recognize it as
one of the known classes. As a result, it affects the perfor-
mance of these systems, as evidenced by Jain et al. [18]
with digit recognition example. Hence, it becomes criti-
cal to correctly identify test samples as either known or un-
known for a classiﬁcation model. This problem setting of
identifying test samples as known/unknown and simultane-
ously correctly classifying all of known classes, is referred
to as open-set recognition [44]. Fig. 1 illustrates a typical
example of classiﬁcation in the open-set problem setting.

In an open-set problem setting, it becomes challenging to
identify unknown samples due to the incomplete knowledge
of the world during training (i.e. only the known classes
are accessible). To overcome this problem many open-set
methods in the literature [7], [45], [50], [47] adopt recog-

12307

nition score based thresholding models. However, when
using these models one needs to deal with two key ques-
tions, 1) what is a good score for open-set identiﬁcation?
(i.e., identifying a class as known or unknown), and given a
score, 2) what is a good operating threshold for the model?.
There have been many methods that explore these questions
in the context of traditional methods such as Support Vec-
tor Machines [44], [45], [18], Nearest Neighbors [21], [6]
and Sparse Representation [50]. However, these questions
are relatively unexplored in the context of deep neural net-
works.

Even-though deep neural networks are powerful in learn-
ing highly discriminative representations, they still suffer
from performance degradation in the open-set setting [7].
In a naive approach, one could apply a thresholding model
on SoftMax scores. However, as shown by experiments in
[7], that model is sub-optimal for open-set identiﬁcation. A
few methods have been proposed to better adapt the Soft-
Max scores for open-set setting. Bendale et al. proposed
a calibration strategy to update SoftMax scores using ex-
treme value modeling [7]. Other strategies, Ge et al. [11]
and Lawrence et al. [29] follow data augmentation tech-
nique using Generative Adversarial Networks (GANs) [13].
GANs are used to synthesize open-set samples and later
used to ﬁne-tuning to adapt SoftMax/OpenMax scores for
open-set setting. Shu et al. [47] introduced a novel sigmoid-
based loss function for training the neural network to get
better scores for open-set identiﬁcation.

All of these methods modify the SoftMax scores, so that
it can perform both open-set identiﬁcation and maintain
its classiﬁcation accuracy. However, it is extremely chal-
lenging to ﬁnd a single score measure, that can perform
both. In contrast to these methods, in the proposed approach
the training procedure for open-set recognition using class
conditional auto-encoders, is divided it into two sub-tasks,
1. closed-set classiﬁcation, and 2. open-set identiﬁcation.
These sub-tasks are trained separately in a stage-wise man-
ner. Experiments show that such approach provides good
open-set identiﬁcation scores and it is possible to ﬁnd a
good operating threshold using the proposed training and
testing strategy.

In summary, this paper makes following contributions,

• A novel method for open-set recognition is proposed with
novel training and testing algorithm based on class con-
ditioned auto-encoders.

• We show that dividing open-set problem in sub-tasks can

help learn better open-set identiﬁcation scores.

• Extensive experiments are conducted on various image
classiﬁcation datasets and comparisons are performed
against several recent state-of-the-art approaches. Fur-
thermore, we analyze the effectiveness of the proposed
method through ablation experiments.

2. Related Work

Open-set Recognition. The open-set recognition meth-
ods can be broadly classiﬁed in to two categories, tradi-
tional methods and neural network-based methods. Tra-
ditional methods are based on classiﬁcation models such
as Support Vector Machines (SVMs), Nearest Neighbors,
[45] extended
Sparse Representation etc. Scheirer et al.
the SVM for open-set recognition by calibrating the deci-
sion scores using the extreme value distribution. Speciﬁ-
cally, Scheirer et al. [45] utilized two SVM models, one
for identifying a sample as unknown (referred as CAP
models) and other for traditional closed-set classiﬁcation.
PRM Junior et al. [20] proposed a nearest neighbor-based
open-set recognition model utilizing the neighbor similar-
ity as a score for open-set identiﬁcation. PRM Junior et
later also presented specialized SVM by constraining
al.
the bias term to be negative. This strategy in the case of
Radial Basis Function kernel, yields an open-set recogni-
tion model. Zhang and Patel [50] proposed an extension
of the Sparse Representation-based Classiﬁcation (SRC) al-
gorithm for open-set recognition. Speciﬁcally, they model
residuals from SRC using the Generalized-Pareto extreme
value distribution to get score for open-set identiﬁcation.

In neural network-based methods, one of the earliest
works by Bendale et al. [7] introduced an open-set recog-
nition model based on “activation vectors” (i.e. penulti-
mate layer of the network). Bendale et al. utilized meta-
recognition for multi-class classiﬁcation by modeling the
distance from “mean activation vector” using the extreme
value distribution. SoftMax scores are calibrated using
these models for each class. These updated scores, termed
as OpenMax, are then used for open-set identiﬁcation. Ge
et al. [11] introduced a data augmentation approach called
G-OpenMax. They generate unknown samples from the
known class training data using GANs and use it to ﬁne-
tune the closed-set classiﬁcation model. This helps in im-
proving the performance for both SoftMax and OpenMax
based deep network. Along the similar motivation, Neal
[29] proposed a data augmentation strategy called
et al.
counterfacutal image generation. This strategy also utilizes
GANs to generate images that resemble known class im-
ages but belong to unknown classes. In another approach,
Shu et al. [47] proposed a k-sigmoid activation-based novel
loss function to train the neural network. Additionally, they
perform score analysis on the ﬁnal layer activations to ﬁnd
an operating threshold, which is helpful for open-set identi-
ﬁcation. There are some related problems such as anomaly
detection [31], [32], [36] and novelty detection [34], [39],
[33], [43] etc., which are relaxed version of the open-set
recognition formulation. But, for this paper we only focus
on the open-set recognition problem.
Extreme Value Theory. Extreme value modeling is a
branch of statistics that deals with modeling of statistical

2308

2 Open­set Training

Open­set Recognition Model

 

y

pred

Classifier

X

Encoder

z

z

l

m

 

g
n
i
n
o
i
t
i
d
n
o
C

 
r
e
y
a
L

l

l

m

nm

z

l

nm

Known 
Classes 

Match Condition 

 Vectors 

1

Closed­set Training

[
1
 
­
1
 
.
 
.
 
.
 
­
1
]

[
­
1
 
­
1
 
.
 
.
 
.
 
1
]

[
­
1
 
1
 
.
 
.
 
.
 
­
1
]

Encoder

z

C
l
a
s
s
i
f
i
e
r

Known 
Classes 

Closed­Set Model

C

k

Decoder

Non­match

Reconstructions 

nm

X

Reconstruction Error 

Non­matched 
Histogram 

EVT 

Modelling 

τ

m

X

Non­match Condition

Vectors 

Match 

Reconstructions 

Reconstruction Error 

Matched 
Histogram 

3 Open­set Testing (k­Inference)
 

pred

y

[
­
1
 
­
1
 
.
 
.
 
.
 
1
]

[
1
 
­
1
 
.
 
.
 
.
 
­
1
]

[
­
1
 
­
1
 
.
 
.
 
­
1
]

k
­
C
l
a
s
s
 

Open­set
Recognition

Model

C

1.
..
.
.

C
l
a
s
s
i
f
i
c
a
t
i
o
n

 

1

Recmin

Recmin  τ

<

Rec

.
..
.
.

T rue

y

pred

 

 Unk

False

l

l

l

1

2

k

Real
World 

[
1

 
­
1

 
.
 
.
 
.
 
­
1
]

[
­
1

 

1

 
.
 
.
 
.
 
­
1
]

. . . . . .

[
­
1

 
­
1

 
.
 
.
 
.
 

1
]

Rec

k

τ

EVT 
Model 

Figure 2: Block diagram of the proposed method: 1) Closed-set training, Encoder (F ) and Classiﬁer (C) are trained with
the traditional classiﬁcation loss. 2) Open-set Training, To train an open-set identiﬁcation model, auto-encoder network
Encoder (F ) with frozen weights, and Decoder (G), are trained to perfectly or poorly reconstruct the images depending on
the label condition vector. Reconstruction errors are then modeled using the extreme value distribution to ﬁnd the operating
threshold of the method. 3) Open-set Testing, Open-set recognition model produces the classiﬁcation prediction (ypred) and
k reconstruction errors, conditioned with each condition vector. If the minimum reconstruction error is below the threshold
value obtained from the EVT model, the test sample is classiﬁed as one of the k classes, or else it is classiﬁed as unknown.

extremes. The use of extreme value theory in vision tasks
largely deals with post recognition score analysis [35], [45].
Often for a given recognition model the threshold to re-
ject/accept lies in the overlap region of extremes of match
and non-match score distributions [46].
In such cases, it
makes sense to model the tail of the match and non-match
recognition scores as one of the extreme value distributions.
Hence, many visual recognition methods including some
described above, utilize extreme value models to improve
the performance further [50], [45].

3. Proposed Method

The proposed approach divides the open-set recognition
problem into two sub-tasks, namely, closed-set classiﬁca-
tion and open-set identiﬁcation. The training procedure
for these tasks are shown in Fig. 2 as stage-1 and stage-
2. Stage-3 in Fig. 2 provides overview of the proposed ap-
proach at inference. In what follows, we present details of
these stages.

3.1. Closed set Training (Stage 1)

Given images in a batch {X1, X2, ..., XN } ∈ K, and
their corresponding labels {y1, y2, ..., yN }. Here, N is the
batch size and ∀yi ∈ {1, 2, .., k}. The encoder (F ) and the
classiﬁer (C) with parameters Θf and Θc, respectively are
trained using the following cross entropy loss,

Lc({Θf , Θc}) = −

1
N

N

k

Xi=1

Xj=1

Iyi (j) log[pyi (j)],

(1)

where, Iyi is an indicator function for label yi (i.e., one hot
encoded vector) and pyi = C(F(Xi)) is a predicted prob-
ability score vector. pyi (j) is probability of the ith sample
being from the jth class.

3.2. Open set Training (Stage 2)

There are two major parts in open-set training, condi-
tional decoder training, followed by EVT modeling of the
reconstruction errors. In this stage, the encoder and classi-
ﬁer weights are ﬁxed and don’t change during optimization.

2309

3.2.1 Conditional Decoder Training

For any batch described in Sec. 3.1, F is used to extract the
latent vectors as, {z1, z2, ..., zN }. This latent vector batch
is conditioned using the work by Perez et al. [37] called
FiLM. FiLM inﬂuences the input feature map by applying
a feature-wise linear modulations based on conditioning in-
formation. For an input feature z and vector lj containing
conditioning information we get following equations,

γj = Hγ(lj), βj = Hβ(lj),
zlj = γj ⊙ z + βj,

(2)

(3)

where,

lj(x) =(+1, x = j,

−1, x 6= j,

x, j ∈ {1, 2, ..., k}.

Here, Hγ and Hβ are neural networks with parameters
Θγ and Θβ. Tensors zlj , γj , βj have the same shape and ⊙
represents the Hadamard product. lj is used for condition-
ing, and referred to as label condition vector in the paper.
Also, the notation zlj is used to describe the latent vector z
conditioned on the label condition vector lj , i.e, z|lj .

The decoder (G with parameters Θg) is expected to per-
fectly reconstruct the original input when conditioned on
the label condition vector matching the class identity of
the input, referred here as the match condition vector (lm).
However, here G is additionally trained to poorly recon-
struct the original input when conditioned on the label con-
dition vector, that does not match the class identity of the in-
put, referred here as the non-match condition vector (lnm).
The importance of this additional constraint on the decoder
is discussed in Sec. 3.2.3 while modeling the reconstruction
errors using EVT. For the rest of this paper, we use super-
script m and nm to indicate match and non-match, respec-
tively.

j

, for any random ynm

For a given input Xi from the batch and lm = lym

and
lnm = lynm
sampled from
{1, 2, .., k}, be its corresponding match and non-match con-
dition vectors, the feed forward path for stage-2 can be sum-
marized through the following equations,

6= ym
i

i

i

zi = F(Xi),
= Hγ(lym

i

γym

i

βym

i

= Hβ(lym

i

),

),

γynm

i

= Hγ(lynm

i

),

βynm

i

= Hβ(lynm

i

),

zilm = γym
˜X m

i = G(zlm ).

i

⊙ zi + βym

i

,

zilnm = γynm
˜X nm

i = G(zlnm).

i

⊙ zi + βynm

i

,

Following the above feed-forward path, the loss func-
tions in the second stage of training to train the decoder (G
with parameters Θg) and conditioning layer (with parame-
ters Θγ and Θβ) are given as follows,

Lnm

r

({Θg, Θγ , Θβ}) =

1
N

N

Xi=1

||X nm

i − ˜X nm

i

||1,

(5)

min

{Θg ,Θγ ,Θβ }

αLm

r ({Θg, Θγ , Θβ})

+(1 − α)Lnm

r

({Θg, Θγ , Θβ}).

i

i

2

1

r

, X nm

, ..., X nm

Here, the loss function Lm

(6)
r corresponds to the constraint
that output generated using match condition vector ˜X m
i ,
should be perfect reconstruction of Xi. Whereas, the loss
function Lnm
corresponds to the constraint that output gen-
erated using non match condition vector ˜X nm
, should have
poor reconstruction. To enforce the later condition, another
batch {X nm
N }, is sampled from the train-
ing data, such that new batch does not have class identity
consistent with the match condition vector. This in effect
achieves the goal of poor reconstruction when conditioned
lynm
. This conditioning strategy in a way, emulates open-
set behavior (as will be discussed further in Sec. 3.2.3).
Here, the network is speciﬁcally trained to produce poor
reconstructions when class identity of an input image does
not match the condition vector. So, when encountered with
an unknown class test sample, ideally none of the condi-
tion vector would match the input image class identity. This
will result in poor reconstruction for all condition vectors.
While, when encountered with the known test sample, as
one of the condition vector will match the input image class
identity, it will produce a perfect reconstruction for that par-
ticular condition vector. Hence, training with the non-match
loss helps the network adapt better to open-set setting. Here,
Lnm
r are weighted with α ∈ [0, 1] to get the the ﬁ-
nal training objective.

and Lm

r

3.2.2 EVT Modeling

Extreme Value Theory. Extreme value theory is often used
in many visual recognition systems and is an effective tool
for modeling post-training scores [45], [46].
It has been
used in many applications such as ﬁnance, railway track in-
[28], [3], [12] as well as open-set recogni-
spection etc.
tion [7], [45], [50]. In this paper we follow the Picklands-
Balkema-deHaan formulation [38], [5] of the extreme value
theorem.
It considers modeling probabilities conditioned
on random variable exceeding a high threshold. For a given
random variable W with a cumulative distribution function
(CDF) FW (w) the conditional CDF for any w exceeding
the threshold u is deﬁned as,

FU (w) = P(w−u ≤ w|w > u) =

FW (u + w) − FW (u)

1 − FW (u)

,

Lm

r ({Θg, Θγ , Θβ}) =

1
N

N

Xi=1

||Xi − ˜X m

i ||1,

(4)

where, P(·) denotes probability measure function. Now,
given I.I.D. samples, {Wi, ..., Wn}, the extreme value the-

2310

(a)

(b)

Figure 3: (a) Normalized histogram of match and non-match reconstruction errors. The match histogram in yellow shows
distribution of elements in set Sm in Sec. 3.2.3. The non-match histogram showed in blue shows distribution of elements
in set Snm. (b) Normalized histogram of known and unknown reconstruction errors. The known histogram in green shows
the distribution of known class and unknown histogram in red shows the distribution of unknown class reconstruction errors.
Here, the histograms are computed using SVHN dataset.

orem [38] states that, for large class of underlying distribu-
tions and given a large enough u, FU can be well approxi-
mated by the Generalized Pareto Distribution (GPD),

G(w; ζ, µ) =(1 − (1 + ζ·w

1 − e

w
µ

µ )

1

ζ , if ζ 6= 0,
, if ζ = 0,

(7)

such that −∞ < ζ < +∞, 0 < µ < +∞, w > 0 and
ζw > −µ. G(.) is CDF of GPD and for ζ = 0, reduces to
the exponential distribution with parameter µ and for ζ 6= 0
takes the form of Pareto distribution [10].

Parameter Estimation. When modeling the tail of any
distribution as GPD, the main challenge is in ﬁnding the
tail parameter u to get the conditional CDF. However, it is
possible to ﬁnd an estimated value of u using mean excess
function (MEF), i.e., E[W − u|W > u] [46]. It has been
shown that for GPD, MEF holds a linear relationship with
u. Many researchers use this property of GPD to estimate
the value of u [46], [35]. Here, the algorithm for ﬁnding u,
introduced in [35] for GPD is adopted with minor modiﬁ-
cations. See [35], [46] for more details regarding MEF or
tail parameter estimation. After getting an estimate for u,
since from extreme value theorem [38], we know that set
{w ∈ W | w > u}, follows GPD distribution, rest of the
parameters for GPD, i.e. ζ and µ can be easily estimated
using the maximum likelihood estimation techniques [14],
except for some rarely observed cases [9].

3.2.3 Threshold Calculation

After the training procedure described in previous sec-
tions, Sec. 3.1 and Sec. 3.2, a set of match and non-
match reconstruction errors are created from training set,
{X1, X2, ..., XNtrain } ∈ K, and their corresponding

2

1

, ynm

1 , ym

, ..., ynm

match and non match labels, {ym
} and
{ynm
i be the match reconstruc-
tion error and rnm
be the non match reconstruction error for
the input Xi, then the set of match and non match errors can
be calculated as,

}. Let, rm

2 , ..., ym

Ntrain

Ntrain

i

) ⊙ F(Xi) + Hβ(lym

)),

i

˜X m
˜X nm

i = G(Hγ(lym
i = G(Hγ(lynm

i

i

Sm = {rm
Snm = {rnm

i ∈ R+ ∪ {0} | rm
i ∈ R+ ∪ {0} | rnm

)),

) ⊙ F(Xi) + Hβ(lynm
i = ||Xi − ˜X m
i ||1 },
i = ||Xi − ˜X nm

i

i

||1 },

∀i ∈ {1, 2, ..., Ntrain}.

Typical histograms of Sm (set of match reconstruction
errors) shown in Fig. 3a with color yellow, and Snm (set
of non-match reconstruction errors) shown in Fig. 3a with
color blue. Note that the elements in these sets are calcu-
lated solely based on what is observed during training (i.e.,
without utilizing any unknown samples). Fig. 3b shows the
histogram of reconstruction errors observed during infer-
ence from the test samples of known class set (K) (shown
in green), and unknown class set (U ) (shown in red). Com-
paring Fig. 3a and Fig. 3b, it can be observed that the distri-
bution of Sm and Snm computed during training, provides
a good approximation for the error distributions observed
during inference, for test samples from known set (K) and
unknown set (U ). This observation also validates that non
match training emulates an open-set test scenario (also dis-
cussed in Sec. 3.2) where the input does not match any of
the class labels. This motivates us to use Sm and Snm to
ﬁnd an operating threshold for open-set recognition to make
a decision about any test sample being known/unknown.

We can assume that the optimal operating threshold (τ ∗)
lies in the region Sm ∩ Snm. The underlying distributions

2311

00.10.20.30.40.50.60.7Reconstruction Error00.020.040.060.080.10.12Normalized HistogramNon MatchMatch00.10.20.30.40.50.60.7Reconstruction Error00.010.020.030.040.050.060.070.080.09Normalized HistogramKnownUnknownof Sm and Snm are not known. However, as explained in
Sec. 3.2.2, it is possible to model the tails of Sm (right tail)
and Snm (left tail) with GPD as Gm and Gnm with G(·) be-
ing a CDF. Though, GPD is only deﬁned for modeling max-
ima, before ﬁtting Gnm left tail of Snm we perform inverse
transform as S′
nm = −Snm. Assuming the prior probabil-
ity of observing unknown samples is pu, the probability of
errors can be formulated as a function of the threshold τ ,

τ ∗ = min

τ

Perror(τ )

6:

= min

τ

= min

τ

[(1 − pu) ∗ Pm(r > τ ) + pu ∗ Pnm(−r < −τ )]

[(1 − pu) ∗ (1 − Gm(τ )) + pu ∗ (1 − Gnm(τ ))].

Solving the above equation should give us an operating
threshold that can minimize the probability of errors for a
given model and can be solved by a simple line search algo-
rithm by searching for τ ∗ in the range {Sm ∩ Snm}. Here,
the accurate estimation of τ ∗ depends on how well Sm and
Snm represent the known and unknown error distributions.
It also depends on the prior probability pu, effect of this
prior will be further discussed in Sec. 4.3.

3.3. Open set Testing by k inference (Stage 3)

Here, we introduce the open-set testing algorithm for
proposed method. The testing procedure is described in
Algo. 1 and an overview of this is also shown in Fig. 2. This
testing strategy involves conditioning the decoder k-times
with all possible condition vectors to get k reconstruction
errors. Hence, it is referred as k-inference algorithm.

4. Experiments and Results

In this section we evaluate performance of the proposed
approach and compare it with other open-set recognition
methods. The experiments in Sec. 4.2, we measure the
ability of algorithm to identify test samples as known or
unknown without considering operating threshold. In sec-
ond set of experiments, we measure overall performance of
open-set recognition algorithm. Additionally through abla-
tion experiments, we analyze contribution from each com-
ponent of the proposed method.

4.1. Implementation Details

We use Adam optimizer [22] with learning rate 0.0003
and batch size, N =64. The parameter α, described in
Sec. 3.2, is set equal to 0.9. For all the experiments, condi-
tioning layer networks Hγ and Hβ are a single layer fully
connected neural networks. Another important factor af-
fecting open-set performance is openness of the problem.
Deﬁned by Scheirer et al. [44], it quantiﬁes how open the
problem setting is,

Algorithm 1 k-Inference Algorithm

Require: Trained network models F , C, G, Hγ , Hβ
Require: Threshold τ from EVT model
Require: Test image X, k condition vectors {l1, . . . , lk}

1: Latent space representation, z = F(X)
2: Prediction probabilities, py = C(z)
3: predict known label, ypred = argmax(py)
4: for i = 1, . . . , k do
5:

zli = Hγ(li) ⊙ z + Hβ(li)
˜Xi = G(zli )

Rec(i) = ||X − ˜Xi||1

7:
8: end for
9: Recmin = sort(Rec)
10: if Recmin < τ do
11:
12: else do
13:
14: end if

predict X as Unknown

predict X as Known, with label ypred

O = 1 − s 2 × Ntrain

Ntest + Ntarget

,

(8)

where, Ntrain is the number of training classes seen dur-
ing training, Ntest is the number of test classes that will
be observed during testing, Ntarget is the number of target
classes that needs to be correctly recognized during testing.
We evaluate performance over multiple openness value de-
pending on the experiment and dataset.

4.2. Experiment I : Open set Identiﬁcation

The evaluation protocol deﬁned in [29] is considered and
area under ROC (AUROC) is used as evaluation metric.
AUROC provides a calibration free measure and character-
izes the performance for a given score by varying threshold.
The encoder, decoder and classiﬁer architecture for this ex-
periment is similar to the architecture used by [29] in their
experiments. Following the protocol in [29], we report the
AUROC averaged over ﬁve randomized trials.

4.2.1 Datasets

MNIST, SVHN, CIFAR10. For MNIST [26], SVHN [30]
and CIFAR10 [23], openness of the problem is set to O =
13.39%, by randomly sampling 6 known classes and 4 un-
known classes.
CIFAR+10, CIFAR+50. For CIFAR+M experiments, 4
known classes are sampled from CIFAR10. M non over-
lapping classes are used as the unknowns, sampled from the
CIFAR100 [23]. Openness of the problem for CIFAR+10
and CIFAR+50 is O = 33.33% and 62.86%, respectively.
TinyImageNet. For experiments with the TinyImageNet

2312

Method
SoftMax

MNIST SVHN CIFAR10 CIFAR+10 CIFAR+50 TinyImageNet

G-OpenMax [11] (BMVC’17)

OpenMax [7] (CVPR’16)

0.577
0.576
0.580
0.586
0.748
Table 1: AUROC for open-set identiﬁcation, values other than the proposed method are taken from [29].

0.978
0.981
0.984
0.988
0.989

0.886
0.894
0.896
0.910
0.922

0.677
0.695
0.675
0.699
0.895

0.816
0.817
0.827
0.838
0.955

0.805
0.796
0.819
0.827
0.937

Proposed Method

OSRCI [29] (ECCV’18)

[25], 20 known classes and 180 unknown classes with open-
ness O = 57.35% are randomly sampled for evaluation.

4.2.2 Comparison with state-of-the-art

For comparing the open-set identiﬁcation performance, we
consider the following methods:
I. SoftMax : SoftMax score of a predicted class is used for
open-set identiﬁcation.
II. OpenMax [7]: The score of k+1th class and score of
the predicted class is used for open-set identiﬁcation.
III. G-OpenMax [11]: It is a data augmentation technique,
which utilizes the OpenMax scores after training the
network with the generated data.
IV. OSRCI [29]: Another data augmentation technique
called counterfactual image generation is used for training
the network for k+1 class classiﬁcation. We refer to this
method as Open-set Recognition using Counterfactual
Images (OSRCI). The score value P (yk+1) − max
P (yi)
i≤k

is used for open-set identiﬁcation.

Results corresponding to this experiment are shown in
Table 1. As seen from this table, the proposed method out-
perform the other methods, showing that open-set identiﬁ-
cation training in proposed approach learns better scores for
identifying unknown classes. From the results, we see that
our method on the digits dataset produces a minor improve-
ment compared to the other recent methods. This is mainly
do the reason that results on the digits dataset are almost
saturated. On the other hand, our method performs signif-
icantly better than the other recent methods on the object
datasets such as CIFAR and TinyImageNet.

4.3. Experiment II : Open set Recognition

This experiment shows the overall open-set recognition
performance evaluated with F-measure. For this experiment
we consider LFW dataset [27]. We extend the protocol in-
troduced in [44] where, 12 classes containing more than 50
images are considered as known classes and divided into
80/20 train-test split. Image size is kept to 64×64. We vary
the openness from 0% to 93% by taking 18 to 5705 un-
known classes during testing. Since, many classes contain
only one image, instead of random sampling, we sort them
according to the number of images per class and add it se-

quentially to increase the openness. It is obvious that with
the increase in openness, the probability of observing un-
known will also increase. Hence, it is reasonable to assume
that prior probability pu will be a function of openness. For
this experiment we set pu = 0.5 ∗ O.

4.3.1 Comparison with state-of-the-art

For comparing the open-set recognition performance, we
consider the following methods:
I. W-SVM (PAMI’14) : W-SVM is used as formulated
in [44], which trains Weibull calibrated SVM classiﬁer for
open set recognition.
II. SROR (PAMI’16) : SROR is used as formulated in [50].
It uses sparse representation-based framework for open-set
recognition.
III. DOC (EMNLP’16) : It utilizes a novel sigmoid-based
loss function for training a deep neural network [47].

To have a fair comparison with these methods, we use
features extracted from the encoder (F ) to train W-SVM
and SROR. For DOC, the encoder (F ) is trained with the
loss function proposed in [47]. Experiments on LFW are
performed using a U-Net [42] inspired encoder-decoder ar-
chitecture. More details regarding network architecture is
included in the supplementary material.

Results corresponding to this experiment is shown in
Fig. 4a. From this ﬁgure, we can see that the proposed ap-
proach remains relatively stable with the increase in open-
ness, outperforming all other methods. One interesting
trend noticed here is, that DOC initially performs better than
the statistical methods such as W-SVM and SROR. How-
ever with openness more than 50% the performance suffers
signiﬁcantly. While the statistical methods though initially
perform poor compared to DOC, but remain relatively sta-
ble and performs better than DOC as the openness is in-
creased (especially over O >50%).

4.3.2 Ablation Study

In this section, we present ablation analysis of the proposed
approach on the LFW dataset. The contribution of individ-
ual components is reported by creating multiple baselines of
the proposed approach. Starting with the most simple base-
line, i.e., thresholding SoftMax probabilities of a closed-
set model, each component is added building up to the pro-

2313

(a) F-measure comparisons for the open-set recognition experiment.

(b) F-measure comparisons for the ablation study.

Figure 4: Performance evaluation on the LFW dataset.

posed approach and are described as follows,
I. CLS : Encoder (F) and the classiﬁer (C) are trained for
k-class classiﬁcation. Samples with probability score pre-
diction less than 0.5 are classiﬁed as unknown.
II. CLS+DEC : In this baseline, only the networks F , C and
the decoder (G) are trained as described in Sec. 3, except G
is only trained with match loss function, Lm
r . Samples with
more than 95% of maximum train reconstruction error ob-
served, are classiﬁed as unknown.
III. Naive : Here, the networks F , C and G and the condi-
tioning layer networks (Hγ and Hβ) are trained as described
in Sec. 3, but instead of modeling the scores using EVT as
described in Sec. 3.2.3, threshold is directly estimated from
the raw reconstruction errors.
IV. Proposed method (pu = 0.5) : F , C, G and condi-
tion layer networks (Hγ and Hβ) are trained as described
in Sec. 3 and to ﬁnd the threshold prior probability of ob-
serving unknown is set to pu = 0.5.
V. Proposed method: Method proposed in this paper, with
pu set as described in Sec. 4.3.

Results corresponding to the ablation study are shown
in Fig. 4b. Being a simple SoftMax thresholding baseline,
CLS has weakest performance. However, when added with
a match loss function (Lm
r ) as in CLS+DEC, the open-
set identiﬁcation is performed using reconstruction scores.
Since, it follows a heuristic way of thresholding, the perfor-
mance degrades rapidly as openness increases. However,
addition of non match loss function (Lnm
), as in the Naive
baseline, helps ﬁnd a threshold value without relying on
heuristics. As seen from the Fig. 4b performance of Naive
baseline remains relatively stable with increase in openness,
showing the importance of loss function Lnm
. Proposed
method with pu ﬁxed to 0.5, introduces EVT modeling on
reconstruction errors to calculate a better operating thresh-
old. It can be seen from the Fig. 4b, such strategy improves
over ﬁnding threshold based on raw score values. This
shows importance applying EVT models on reconstruction

r

r

errors. Now, if pu is set to 0.5 ∗ O, as in the proposed
method, there is a marginal improvement over the ﬁxed pu
baseline. This shows beneﬁt of setting pu as a function of
openness. It is interesting to note that for large openness
values (as 0.5 ∗ O → 0.5), both ﬁxed pu baseline and pro-
posed method achieve similar performance.

5. Conclusion

We presented an open-set recognition algorithm based
on class conditioned auto-encoders. We introduced train-
ing and testing strategy for these networks.
It was also
shown that dividing the open-set recognition into sub tasks
helps learn a better score for open-set identiﬁcation. During
training, enforcing conditional reconstruction constraints
are enforced, which helps learning approximate known
and unknown score distributions of reconstruction errors.
Later, this was used to calculate an operating threshold for
the model. Since inference for a single sample needs k
feed-forwards, it suffers from increased test time. How-
ever, the proposed approach performs well across multi-
ple image classiﬁcation datasets and providing signiﬁcant
improvements over many state of the art open-set algo-
rithms. In our future research, generative models such as
GAN/VAE/FLOW can be explored to modify this method.
We will revise the manuscript with such details in the con-
clusion.

Acknowledgements

This research is based upon work supported by the Of-
ﬁce of the Director of National Intelligence (ODNI), Intel-
ligence Advanced Research Projects Activity (IARPA), via
IARPA R&D Contract No. 2014-14071600012. The views
and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the ofﬁcial policies or endorsements, either expressed or im-
plied, of the ODNI, IARPA, or the U.S. Government.

2314

0102030405060708090100Openness (%)00.10.20.30.40.50.60.70.80.91F-measure0102030405060708090100Openness (%)00.10.20.30.40.50.60.70.80.91F-measureReferences

[1] Mahdi Abavisani and Vishal M Patel. Adversarial domain
adaptive subspace clustering.
In 2018 IEEE 4th Interna-
tional Conference on Identity, Security, and Behavior Analy-
sis (ISBA), pages 1–8. IEEE, 2018.

[2] Mahdi Abavisani and Vishal M Patel. Deep multimodal sub-
space clustering networks. IEEE Journal of Selected Topics
in Signal Processing, 12(6):1601–1614, 2018.

[3] Isabel Fraga Alves and Cl´audia Neves. Extreme value distri-
butions. In International encyclopedia of statistical science,
pages 493–496. Springer, 2011.

[4] Yancheng Bai, Yongqiang Zhang, Mingli Ding, and Bernard
Ghanem. Finding tiny faces in the wild with generative ad-
versarial network. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 21–30,
2018.

[5] August A Balkema and Laurens De Haan. Residual life time
at great age. The Annals of probability, pages 792–804, 1974.

[6] Abhijit Bendale and Terrance Boult. Towards open world
recognition. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1893–1902,
2015.

[7] Abhijit Bendale and Terrance E Boult. Towards open set
deep networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1563–1572,
2016.

[8] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pages 132–149, 2018.

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. arXiv preprint arXiv:1709.01507, 2017.

[18] Lalit P Jain, Walter J Scheirer, and Terrance E Boult. Multi-
class open set recognition using probability of inclusion. In
European Conference on Computer Vision, pages 393–409.
Springer, 2014.

[19] Pan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, and
Ian Reid. Deep subspace clustering networks. In Advances in
Neural Information Processing Systems, pages 24–33, 2017.
[20] Pedro Ribeiro Mendes J´unior, Terrance E Boult, Jacques
Specialized support vec-
arXiv preprint

Wainer, and Anderson Rocha.
tor machines for open-set recognition.
arXiv:1606.03802, 2016.

[21] Pedro R Mendes J´unior, Roberto M de Souza, Rafael de O
Werneck, Bernardo V Stein, Daniel V Pazinato, Waldir R
de Almeida, Ot´avio AB Penatti, Ricardo da S Torres, and
Anderson Rocha. Nearest neighbors distance ratio open-set
classiﬁer. Machine Learning, 106(3):359–386, 2017.

[22] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. 2015.

[23] Alex Krizhevsky. Learning multiple layers of features from

tiny images. Technical report, Citeseer, 2009.

[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, pages 1097–1105, 2012.

[25] Ya Le and Xuan Yang. Tiny imagenet visual recognition

challenge. CS 231N, 2015.

[9] Vartan Choulakian and Michael A Stephens. Goodness-of-ﬁt
tests for the generalized pareto distribution. Technometrics,
43(4):478–484, 2001.

[26] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist hand-
written digit database. AT&T Labs [Online]. Available:
http://yann. lecun. com/exdb/mnist, 2, 2010.

[10] Herbert Aron David and Haikady Navada Nagaraja. Order

statistics. Wiley Online Library, 1970.

[11] ZongYuan Ge, Sergey Demyanov, Zetao Chen, and Rahil
Garnavi. Generative openmax for multi-class open set clas-
siﬁcation. arXiv preprint arXiv:1707.07418, 2017.

[12] Xavier Gibert, Vishal M Patel, and Rama Chellappa. Deep
multitask learning for railway track inspection. IEEE Trans-
actions on Intelligent Transportation Systems, 18(1):153–
164, 2017.

[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014.

[14] Scott D Grimshaw. Computing maximum likelihood esti-
mates for the generalized pareto distribution. Technometrics,
35(2):185–191, 1993.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation.
In Proceedings of the
IEEE international conference on computer vision, pages
1026–1034, 2015.

[27] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
In Proceedings
Deep learning face attributes in the wild.
of the IEEE International Conference on Computer Vision,
pages 3730–3738, 2015.

[28] Gerald A Meehl, Thomas Karl, David R Easterling, Stanley
Changnon, Roger Pielke Jr, David Changnon, Jenni Evans,
Pavel Ya Groisman, Thomas R Knutson, Kenneth E Kunkel,
et al. An introduction to trends in extreme weather and cli-
mate events: observations, socioeconomic impacts, terres-
trial ecological impacts, and model projections. Bulletin of
the American Meteorological Society, 81(3):413–416, 2000.
[29] Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen
Wong, and Fuxin Li. Open set learning with counterfac-
tual images. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 613–628, 2018.

[30] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural
images with unsupervised feature learning. In NIPS work-
shop on deep learning and unsupervised feature learning,
volume 2011, page 5, 2011.

[31] Poojan Oza and Vishal M Patel. Active authentication us-
ing an autoencoder regularized cnn-based one-class classi-
ﬁer. arXiv preprint arXiv:1903.01031, 2019.

2315

Human Identiﬁcation V, volume 6944, page 69440O. Inter-
national Society for Optics and Photonics, 2008.

[47] Lei Shu, Hu Xu, and Bing Liu. Doc: Deep open classiﬁ-
cation of text documents. arXiv preprint arXiv:1709.08716,
2017.

[48] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[49] Vishwanath Sindagi and Vishal Patel. Dafe-fd: Den-
In 2019
sity aware feature enrichment for face detection.
IEEE Winter Conference on Applications of Computer Vision
(WACV), pages 2185–2195. IEEE, 2019.

[50] He Zhang and Vishal M Patel. Sparse representation-based
open set recognition. IEEE transactions on pattern analysis
and machine intelligence, 39(8):1690–1696, 2017.

[32] Poojan Oza and Vishal M Patel. One-class convolutional
neural network. IEEE Signal Processing Letters, 26(2):277–
281, 2019.

[33] Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. Oc-
gan: One-class novelty detection using gans with constrained
latent representations. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.

[34] Pramuditha Perera and Vishal Patel. Deep transfer learning
for multiple class novelty detection. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2019.

[35] Pramuditha Perera and Vishal M Patel. Extreme value anal-
ysis for mobile active user authentication.
In 2017 12th
IEEE International Conference on Automatic Face & Ges-
ture Recognition (FG 2017), pages 346–353. IEEE, 2017.

[36] Pramuditha Perera and Vishal M Patel.

features for one-class classiﬁcation.
arXiv:1801.05365, 2018.

Learning deep
arXiv preprint

[37] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-
Film: Visual reason-
arXiv preprint

moulin, and Aaron Courville.
ing with a general conditioning layer.
arXiv:1709.07871, 2017.

[38] James Pickands III et al. Statistical inference using extreme
order statistics. the Annals of Statistics, 3(1):119–131, 1975.
[39] Stanislav Pidhorskyi, Ranya Almohsen, and Gianfranco
Doretto. Generative probabilistic novelty detection with ad-
versarial autoencoders. In Advances in Neural Information
Processing Systems, pages 6822–6833, 2018.

[40] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.

[41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, pages 91–99, 2015.

[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention, pages 234–241.
Springer, 2015.

[43] Lukas Ruff, Nico G¨ornitz, Lucas Deecke, Shoaib Ahmed
Siddiqui, Robert Vandermeulen, Alexander Binder, Em-
manuel M¨uller, and Marius Kloft. Deep one-class classiﬁ-
cation. In International Conference on Machine Learning,
pages 4390–4399, 2018.

[44] Walter J Scheirer, Anderson de Rezende Rocha, Archana
Sapkota, and Terrance E Boult. Toward open set recogni-
tion.
IEEE transactions on pattern analysis and machine
intelligence, 35(7):1757–1772, 2013.

[45] Walter J Scheirer, Lalit P Jain, and Terrance E Boult. Prob-
ability models for open set recognition. IEEE transactions
on pattern analysis and machine intelligence, 36(11):2317–
2324, 2014.

[46] Zhixin Shi, Frederick Kiefer, John Schneider, and Venu
Govindaraju. Modeling biometric systems using the gen-
eral pareto distribution (gpd). In Biometric Technology for

2316

