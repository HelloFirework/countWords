Meta-SR: A Magniﬁcation-Arbitrary Network for Super-Resolution

Xuecai Hu∗1

,

2 , Haoyuan Mu∗ 4, Xiangyu Zhang3, Zilei Wang1, Tieniu Tan1

,

2, Jian Sun3

1 University of Science and Technology of China

2 Center for Research on Intelligent Perception and Computing, NLPR, CASIA

3 Megvii Inc (Face++) 4 Tsinghua University

huxc@mail.ustc.edu.cn, muhy17@mails.tsinghua.edu.cn

{zhangxiangyu, sunjian}@megvii.com, zlwang@ustc.edu.cn, tnt@nlpr.ia.ac.cn

Abstract

Recent research on super-resolution has achieved great
success due to the development of deep convolutional neu-
ral networks (DCNNs). However, super-resolution of arbi-
trary scale factor has been ignored for a long time. Most
previous researchers regard super-resolution of different
scale factors as independent tasks. They train a speciﬁc
model for each scale factor which is inefﬁcient in comput-
ing, and prior work only take the super-resolution of sev-
eral integer scale factors into consideration. In this work,
we propose a novel method called Meta-SR to ﬁrstly solve
super-resolution of arbitrary scale factor (including non-
integer scale factors) with a single model. In our Meta-SR,
the Meta-Upscale Module is proposed to replace the tradi-
tional upscale module. For arbitrary scale factor, the Meta-
Upscale Module dynamically predicts the weights of the up-
scale ﬁlters by taking the scale factor as input and use these
weights to generate the HR image of arbitrary size. For any
low-resolution image, our Meta-SR can continuously zoom
in it with arbitrary scale factor by only using a single model.
We evaluated the proposed method through extensive exper-
iments on widely used benchmark datasets on single image
super-resolution. The experimental results show the superi-
ority of our Meta-Upscale.

1. Introduction

Single image super-resolution (SISR) aims to reconstruct
a visually natural high-resolution image from its degraded
low-resolution (LR) image. And it has very wide applica-
tion on security and surveillance imaging [8, 37], medical
imaging [23], as well as satellite and aerial imaging [32]. In
real-world scenarios, it is very common and necessary for
SISR to zoom in the LR image with the user-customized

∗This work is conducted during Xuecai Hu’s and Haoyuan Mu’s intern-

ship at Megvii Inc, two authors contributed equally to this work.

scale factor. As with the common image viewer, the user
can arbitrarily zoom in the viewed image by rolling the
mouse wheel to see the local details of the viewed image.
The customized scale factor for super-resolution also can
be any positive number. And it should not be ﬁxed to some
certain integers. Thus, a method to solve super-resolution of
arbitrary scale factor is important for putting the SISR into
more practical use. If we train a speciﬁc model for each pos-
itive scale factor, it is impossible to store all these models
and it is inefﬁcient in computing. Thus, the more important
thing is that whether we can solve the super-resolution of
arbitrary scale factor with a single model.

However, as we all known, the most existing SISR meth-
ods only consider super-resolution of some certain integer
scale factors (X2, X3, X4). And these methods treat super-
resolution of different scale factors as independent tasks.
Few previous work has discussed how to implement super-
resolution of arbitrary scale factor. As for the state-of-the-
art SISR methods, such as ESPCNN [22], EDSR [18], RDN
[36] and RCAN [35], these methods zoom in the feature
maps at the end of networks with the sub-pixel convolution
[18]. Unfortunately, these methods have to design a spe-
ciﬁc upscale module for each scale factor. Each upscale
module can only zoom in the image with the ﬁxed integer
scale factor. And the sub-pixel convolution only works for
the integer scale factors. These disadvantages limit the use
of SISR to real-world scenarios. Although, we could imple-
ment super-resolution of non-integer scale factors by prop-
erly upscaling the input image. However, the repeated com-
putation and the upscaled input make these methods very
time-consuming and hard to put into pratical use.

To solve these drawbacks and put SISR into more prac-
tical use, an efﬁcient and novel method for super-resolution
of arbitrary scale factor with a single model is necessary.
If we want to solve the super-resolution of arbitrary scale
factor with a single model, a group of weights for upscale
ﬁlters is necessary for each scale factor.
Inspired by the
meta-learning, we propose a network to dynamically pre-

1575

dict the weights of ﬁlters for each scale factor. Thus, we
no longer need to store weights for each scale factor. Com-
pared with storing the weights for each scale factor, storing
the small weight prediction network is more convenient.

r ⌋, ⌊ j

We call this method Meta-SR. There are two modules in
our Meta-SR, the Feature Learning Module and the Meta-
Upscale Module. The Meta-Upscale Module is proposed
to replace the typical upscale module. For each pixel (i, j)
on the generated HR image, we project it onto the LR im-
age based on the scale factor r. The projection coordinate
is (⌊ i
r ⌋) on the LR image. Our Meta-Upscale Module
takes this coordinate-related and scale-related vector as in-
put and predicts the weights for the ﬁlters. For each pixel
(i, j) on the generated SR image, a convolution operation
is conducted between the feature at the corresponding pro-
jection coordinate on the LR image and the weights of the
ﬁlters to generate the pixel value on (i, j). The proposed
Meta-Upscale Module could dynamically predict the vari-
ant number of weights for the convolution ﬁlters by taking
a sequence of scale-related and coordinate-related vectors
as input. Through this way , our Meta-Upscale Module can
zoom in the feature maps of arbitrary scale factor with a
single model. Actually, our Meta-Upscale Module can be
incorporated into most previous methods [36, 35, 18] by re-
placing the typical upscale module.

We present extensive experiments on multiple bench-
mark datasets for single image super-resolution to evalu-
ate our method. We show that: 1) For super-resolution of
single integer scale factor, our Meta-SR could achieve the
comparable results with the corresponding baseline which
re-trained the model for each integer scale factor. Note that
our Meta-SR is trained a single model for super-resolution
of arbitrary scale factor together. 2) For super-resolution of
arbitrary scale factor with a single model, our Meta-SR is
better than these methods based on properly zooming in the
input images or the output images, or interpolating on the
feature maps. 3) Our Meta-Upscale Module only consists
of several fully connected layers and it is fast enough. The
running time of our Meta-Upscale is about 1% of the time
consumed by the Feature Learning Module (RDN [36]).

2. Related Work

2.1. Single Image Super Resolution

Early SISR methods are exemplar or dictionary based
super-resolution [3, 27, 25].
These methods require
a database of external
images and generate the high-
resolution images by transfering the relevant patches in the
database images. The performance is limited by the size of
the database or the dictionary. These traditional methods are
very time-consuming and they have limited performance.

With the rapid development of the deep learning, nu-
merous deep learning based methods have been proposed.

A three-layers convolutional neural network is ﬁrstly pro-
posed by Dong et al. [4] called SRCNN. The SRCNN up-
scaled the low-resolution image with bicubic interpolation
before feeding into the network. Kim et al. [14] increased
the depth of the network and used the residual learning for
stable training. Kim et al. [15] ﬁrstly introduced the recur-
sive learning to SISR, called DRCN. Tai et al. [24] proposed
DRRN by introducing the recursive blocks with shared pa-
rameters to make the training stable. Tai et al. also intro-
duced the memory block called Memnet [25]. However, the
input of these networks have the same size as the ﬁnal high-
resolution image, these methods are time-consuming.

Shi et al.

[22] ﬁrstly proposed a real-time super-
resolution algorithm ESPCNN by proposing the sub-pixel
convolution layer. The ESPCNN [22] upscaled the image
at the end of the network to reduce the computation. Ledig
et al. [16] introduced the residual block and the adversarial
learning [7, 6] to make the generated images more realistic
and natural. Lim et al. [18] used the deeper and wider resid-
ual networks called EDSR. The EDSR [18] removed the BN
layer and used the residual scaling to speedup the training.
Lim also ﬁrstly trained single model for multiple scale fac-
tors (X2, X3, X4) called MDSR. The MDSR has different
image processing blocks and upscale modules for each scale
factor. Zhang et al. [36] proposed a residual dense network
(RDN) which combines the advantage of the residual block
and the dense connected block. Then Zhang et al.
[35]
introduced the residual channel attention to the SR frame-
work. Wang et al. [28] proposed a novel deep spatial feature
transform to recover textures conditioned on the categorical
priors. Both DBPN [10] and DSRN [9] made use of the
mutual dependencies of low- and high-resolution images.
DBPN exploited iterative up-sampling and down-sampling
layers to provide an error feedback mechanism for each
[13] introduced the dynamic upsampling
stage. Jo et al.
ﬁlters for video super-resolution. The dynamic upsampling
ﬁlters were generated locally and dynamically depending
on the spatial-temporal neighborhood of each pixel in LR
frames. Different from this work, our Meta-Upscale Mod-
ule predicted the weights of the convolution kernel depend-
ing on the varying scale factors for SISR. Moreover, our
Meta-Upscale could generate variant number and variant
weights of the convolution kernel depend on the scale fac-
tor. Instead of using the spatial-temporal feature blocks, the
input of the our Meta-Upscale Module is the scale-related
and coordinate-related vector. Moreover, our Meta-Upscale
Module is proposed to solve the arbitrary scale.

2.2. Meta Learning

The meta-learning, or learning to learn, is the science of
observing how different machine learning approaches per-
form on a wide range of learning tasks, and then learn-
ing from this experience, or meta-data. The meta-learning

1576

Figure 1. An instance of our Meta-SR based on RDN [36]. We also call the network Meta-RDN. (a) The Residual Dense Block proposed
by RDN [36]. (b) The Feature Learning Module which generates the shared feature maps for arbitrary scale factor. (c) For each pixel on the
SR image, we project it onto the LR image. The proposed Meta-Upscale Module takes a sequence of coordinate-related and scale-related
vectors as input to predict the weights for convolution ﬁlters. By doing the convolution operation, our Meta-Upscale ﬁnally generate the
HR image.

is mainly used in few-shot/zero-shot learning [1, 21] and
transfer learning [29]. The detailed survey of the meta-
learning can be found in [17]. Here we only discuss the
weight prediction related work.

The weight prediction is one of meta-learning strategy in
the neural network [17]. The weights of the neural network
are predicted by another neural network rather than directly
learned from the training dataset. Cai et al. [2] predicted the
parameters W of the classiﬁer to adapt to the new categories
without back propagation for few-shot learning. The pa-
rameters were predicted conditioned on the memory of the
support set. In the object detection task, Hu et al. [11] pro-
posed to predict the mask weights from box weights. And
Yang et al. [31] proposed a novel and ﬂexible anchor mech-
anism for object detection. The anchor functions could be
dynamically generated from the arbitrary customized prior
boxes. In the video super resolution, Jo et al. [13] proposed
a dynamic upsampling ﬁlters. The dynamic upsampling ﬁl-
ters were generated locally and dynamically depending on
the spatial-temporal neighborhood of each pixel in multiple
LR frames. Unlike this work, we take the advantage of the
meta-learning to predict weights of the ﬁlters for each scale
factor. We no long need to store the weights of the ﬁlters
for each scale factor. Our Meta-SR can train a single model
for super-resolution of arbitrary scale. It is convenient and
efﬁcient for practical use.

The most related work is the Parameterized Image Oper-

ators [5] which took advantage of the weight prediction to
dynamically adjust the weights of a deep network for image
operators (image ﬁltering or image restoration). Different
from this work, our Meta-SR focuses on reformulation of
the Upscale Module by taking both the coordinate and scale
factor as inputs.

3. Our Approach

In this section, we describe the proposed model archi-
tectures. As shown in Fig.1. In our Meta-SR, the Feature
Learning Module extracts the feature of the low-resolution
image and the Meta-Upscale Module upscales the feature
map with arbitrary scale factor. We introduce our Meta-
Upscale at ﬁrst, then we describe the architecture details of
our Meta-SR.

3.1. Meta Upscale Formulation

Given an LR image ILR which is downscaled from the
corresponding original HR image IHR, the task of SISR is
to generate a HR image ISR whose ground-truth is IHR.
We choose the RDN [36] as our Feature Learning Module.
As shown in Fig.1(b). Here, we focus on formulating the
Meta-Upscale Module.

Let FLR denote the feature extracted by the Feature
Learning Module. Suppose the scale factor is r. For each
pixel (i, j) on the SR image, we think that it is decided

1577

by the feature of the pixel (i′, j ′) on the LR image and the
weights of the corresponding ﬁlter. From this perspective,
the upscale module can be seen as a mapping function to
map ISR and FLR. At ﬁrst, the upscale module should map
the pixel (i, j) to the pixel (i′, j ′). Then, the upscale module
needs a speciﬁc ﬁlter to map the feature of the pixel (i′, j ′)
to generate the value of this pixel (i, j). We formulate the
upscale module as:

ISR(i, j) = Φ(FLR(i′, j ′), W(i, j))

(1)

where ISR(i, j) denotes the pixel value at (i, j) on SR im-
age. FLR(i′, j ′) denotes the feature of pixel (i′, j ′) on the
LR image. W(i, j) is the weights of ﬁlter for pixel (i, j).
Φ(.) is the feature mapping function to calculate the pixel
value.

Since each pixel on the SR image corresponds to a ﬁl-
ter. For different scale factors, both the number of the ﬁlters
and the weights of the ﬁlters are different from the other
scale factor. In order to solve the super-resolution of arbi-
trary scale factor with a single model, we propose the Meta-
Upscale Module to dynamically predict the weights W(i, j)
based on the scale factor and coordinate information.

For the Meta-Upscale Module, there are three important
functions. That is, the Location Projection, the Weight Pre-
diction and the Feature Mapping. As shown in the Fig.2.
The Location Projection projects pixel onto the LR image.
And the Weight Prediction Module predicts the weights of
the ﬁlter for each pixel on the SR image. At last, the Feature
Mapping function maps the feature on the LR image with
the predicted weights back to the SR image to calculate the
value of the pixel.

Location Projection For each pixel (i, j) on the SR im-
age, the location projection is to ﬁnd the (i′, j ′) on the LR
image. We think the value of the pixel (i, j) is decided by
the feature of (i′, j ′) on the LR image. We do the following
projection operator to map these two pixels:

(i′, j ′) = T (i, j) = (cid:18)(cid:22) i

r(cid:23) ,(cid:22) j

r(cid:23)(cid:19)

(2)

where T is the transformation function. ⌊⌋ is ﬂoor function.
The Location Projection can be seen as a kind of variable
fractional stride [19] mechanism which could upscale the
feature maps with arbitrary scale factor. As shown in the
Fig 2, if the scale factor r is 2, each pixel (i′, j ′) determines
two points. However, if the scale factor is non-integer, such
as r = 1.5, some pixels determine two pixels and some pixels
determine one pixel. For each pixel (i, j) on the SR image,
we could ﬁnd a unique pixel (i′, j ′) on the LR image and
we think these two pixels are most related.

Weight Prediction For the typical upscale module, it
predeﬁnes the number of ﬁlters for each scale factor and
learns W from the training dataset. Different from the typ-

Figure 2. The schematic diagram for how to upscale the feature
map with the non-integer scale factor r = 1.5. Here we only show
the one-dimensional case for simplify.

ical upscale module, our Meta-Upscale Module uses a net-
work to predict the weights of the ﬁlters. We can formulate
the weight prediction as:

W(i, j) = ϕ(vij; θ)

(3)

where W(i, j) are the weights of ﬁlter for pixel (i, j) on the
SR image, vij is a vector related with i, j. ϕ(.) is weight
prediction network and takes the vij as input. θ is the pa-
rameters of the weight prediction network.

As for the input of ϕ(.) for pixel (i, j), the proper choice
is the relative offset to the (i′, j ′), the vij can be formulated
as:

vij = (cid:18) i

r

−(cid:22) i

r(cid:23) ,

j
r

−(cid:22) j

r(cid:23)(cid:19)

(4)

In order to train the multiple scale factor together, it is
better to add the scale factor into the vij to differentiate the
weights for different scale factor. For example, if we want
to upscale the image with scale factor 2 and 4, and we de-
note them as I SR
respectively. The pixel (i, j) on
I SR
2 would have the same weights of the ﬁlter and the same
projection coordinate with the pixel (2i, 2j) on I SR
. That
means that I SR
. It would limit
the performance. Thus, we redeﬁne the vij as:

is the subimage of the I SR

and I SR

2

4

4

2

4

vij = (cid:18) i

r

−(cid:22) i

r(cid:23) ,

j
r

−(cid:22) j

r(cid:23) ,

1

r(cid:19)

(5)

Feature Mapping We extract the feature of the (i′, j ′)
on the LR image from FLR. And we predict the weights
of the ﬁlters with weight prediction network. The last thing

1578

we need to do is mapping feature to the value of the pixel on
the SR image. We choose the matrix product as the Feature
Mapping function. We formulate the Φ(.)as:

Φ(FLR(i′, j ′), W(i, j)) = FLR(i′, j ′)W(i, j)

(6)

Our Meta-Upscale Module is shown in Algorithm 1.

Algorithm 1 Meta Upscale Module
Input: scale:r, the size of input image: (inH, inW ), the

weight prediction function:W, the feature: FLR

Output: the upscale image

1: Calculate the output size outH = int(inH × r), outW

= int(inW × r)

2: for i = 0 : 1 : outH do
3:

r ⌋, 1
r )

r − ⌊ j
r ⌋)

for j = 0 : 1 : outW do
r ⌋, j
r ⌋, ⌊ j

r − ⌊ i
vij = ( i
(i′, j ′) = (⌊ i
the feature on (i′, j ′): FLR(i′, j ′)
weight predicted by ϕ: W(i, j)
pv = FLR(i′, j ′) · W(i, j)
the pixel value on (i, j) is pv

4:

5:

6:

7:

8:

9:

end for

10:
11: end for

3.2. Architecture Details of Meta SR

There are two modules in our Meta-SR network, the
Feature Learning Module, and the Meta-Upscale Module.
Most the state-of-the-art methods [22, 36, 18, 16, 34] could
be selected as our Feature Learning Module. The pro-
posed Meta-Upscale Module could be applied to these net-
works by simply replacing the traditional upscale module
(sub-pixel convolution [22]). We choose the state-of-the-art
SISR network, called residual dense network ( RDN [36]
) as our Feature Learning Module. Note that our Meta-SR
can also work with EDSR or MDSR [18] or RCAN [35].
For the RDN [36], there are 3 convolutional layers and 16
residual dense blocks (RDBs). Each RDB has 8 convolu-
tional layers. The growth rate for the dense block is 64.
And the extracted feature map has 64 channels. The de-
tailed structure is shown in Fig.1. More details can be found
in RDN [36].

For the Meta-Upscale Module,

it consists of several
fully connected layers and several activation layers. Each
input will output one group of weights with the shape
(inC, outC, k, k). Here the inC is the number of channels
of the extracted feature map, and the inC = 64 in the paper.
The outC is the number of channels of the predicted HR im-
age. Generally, outC = 3 for color images and outC = 1
for grayscale image. The k represents the size of the convo-
lution kernel.

Here we want to describe the parameters of the proposed
Meta-Upscale Module including the number of hidden neu-
rons, the number of the fully connected layers, the choice
of activation function and the kernel size of the convolu-
tion layer. Since the output size (k2 × inC × outC) is very
large compared with the input size (3), we set the number
of the hidden neurons as 256. Continuing to increase the
number of the hidden neurons has no improvements. And
the activation function is ReLU. We conduct experiments
and ﬁnd that the best number of the fully connected layer
is 2 with the balance of the speed and the performance.
As for the kernel size, 3 × 3 is the best size . Conduct-
ing 5 × 5 convolution operation on the large feature maps
is more time-consuming.

4. Experiments

4.1. Datasets and Metrics

In the NTIRE 2017 Challenge on Single Image Super
Resolution, a high-quality dataset DIV2K [26] is newly re-
leased. There are 1000 images in DIV2K database, 800 im-
ages for training, 100 images for validation and 100 images
for test. All of our models are trained with DIV2K train-
ing images set. For testing, we use four standard bench-
mark datasets: Set14 [33], B100 [20], Manga109 [12] and
DIV2K [26] . Note that the ground truth of the DIV2K test
set is not publicly available. Therefore, we report the results
on the DIV2K validation set. The super-resolution results
are evaluated with PSNR and SSIM [30]. Following the
setting in [36], we only consider the PSNR and SSIM [30]
on the Y channel of the transformed YCbCr color space.

As for the degradation methods to generate the low-
resolution images, following [18, 36], we use the bicubic
interpolation by adopting the Matlab function imresize to
simulate the LR images.

4.2. Training Details

In the single image super-resolution, the traditional loss
function is L2 loss. Following the setting of [18], we train
our network using L1 loss instead of the L2 for better con-
vergence.

During training the network, we randomly extract 16
LR RGB patches with the size of 50*50 as a batch input.
Following the setting in [36], we randomly augment the
patches by ﬂipping horizontally or vertically and rotating
90◦. The optimizer is Adam. The learning rate is initialized
to 10−4 for all the layers and decreases by half for every
200 epochs. All experiments run in parallel on 4 GPUs.
The training scale factors of the Meta-SR vary from 1 to 4
with stride 0.1, and the distribution of the scale factors is
uniform. Each patch image in a batch has the same scale
factor. Our Meta-SR is trained with Meta-Upscale Module
from scratch.

1579

Table 1. Results of arbitrary upscale on different methods. The EDSR is based on residual block. And the RDN is based on the dense
connection block. The test dataset is B100 [20]. The Best results is black.

Scale

Methods

bicubic
RDN(x1)
RDN(x2)
RDN(x4)
BicuConv
Meta-Bicu

Meta-RDN(our)

EDSR(x1)
EDSR(x2)
EDSR(x4)

Meta-EDSR(our)

Scale

Methods

bicubic
RDN(x1)
RDN(x2)
RDN(x4)
BicuConv
Meta-Bicu

Meta-RDN(our)

EDSR(x1)
EDSR(x2)
EDSR(x4)

Meta-EDSR(our)

Scale

Methods

bicubic
RDN(x1)
RDN(x2)
RDN(x4)
BicuConv
Meta-Bicu

Meta-RDN(our)

EDSR(x1)
EDSR(x2)
EDSR(x4)

Meta-EDSR(our)

X1.1

X1.2

X1.3

X1.4

X1.5

X1.6

X1.7

X1.8

X1.9

X2.0

36.56
42.41
41.84
39.71
41.86
42.11
42.82
42.42
41.79
39.61
42.72

35.01
39.76
39.34
38.48
39.16
39.58
40.40
39.79
39.11
38.41
39.92

33.84
38.00
37.87
37.33
37.88
38.07
38.28
38.08
37.79
37.27
38.16

32.93
36.68
36.63
36.29
29.86
36.83
36.95
36.73
36.51
36.24
36.84

32.14
35.57
35.56
35.34
35.68
35.81
35.86
35.65
35.40
35.30
35.78

31.49
34.64
34.63
34.52
34.77
34.86
34.90
34.73
34.49
34.46
34.83

30.90
33.87
33.83
33.81
33.95
34.03
34.13
33.83
33.81
33.75
34.06

30.38
33.19
33.1
33.14
33.18
33.24
33.45
33.27
33.11
33.09
33.36

29.97
32.60
32.52
32.60
32.60
32.63
32.86
32.67
32.57
32.56
32.78

29.55
32.08
32.11
32.09
31.85
32.18
32.35
32.15
32.09
32.04
32.26

X2.1

X2.2

X2.3

X2.4

X2.5

X2.6

X2.7

X2.8

X2.9

X3.0

29.18
31.63
31.61
31.61
31.53
31.59
31.82
31.69
31.57
31.56
31.73

28.87
31.23
31.24
31.23
31.11
31.21
31.41
31.29
31.15
31.17
31.31

28.57
30.86
30.82
30.88
37.87
30.91
31.06
30.91
30.81
30.82
30.87

28.31
30.51
30.44
30.52
30.38
30.54
30.62
30.56
30.47
30.46
30.60

28.13
30.23
30.23
30.31
30.16
30.34
30.45
30.28
30.22
30.24
30.40

27.89
29.95
29.71
29.99
29.81
30.01
30.13
29.98
29.91
29.93
30.09

27.66
29.68
29.65
29.75
29.55
29.76
29.82
29.73
29.66
29.68
29.83

27.51
29.45
29.43
29.53
29.28
29.54
29.67
29.49
29.45
29.47
29.61

27.31
29.21
29.20
29.26
29.05
29.28
29.40
29.25
29.19
29.20
29.34

27.19
29.03
29.05
29.14
28.91
29.22
29.30
29.07
29.09
29.08
29.22

X3.1

X3.2

X3.3

X3.4

X3.5

X3.6

X3.7

X3.8

X3.9

X4.0

26.98
28.81
28.71
28.89
28.64
28.89
28.87
28.85
28.78
28.82
28.95

26.89
28.67
28.69
28.75
28.51
28.75
28.79
28.69
28.64
28.69
28.82

26.59
28.47
28.51
28.57
28.28
28.54
28.68
28.51
28.45
28.49
28.63

26.60
28.30
28.49
28.42
28.13
28.39
28.54
28.36
28.34
28.35
28.48

26.42
28.15
28.18
28.19
27.91
28.17
28.32
28.18
28.11
28.13
28.27

26.35
28.00
28.17
28.16
27.84
28.11
28.27
28.04
28.06
28.09
28.21

26.15
27.86
28.09
27.93
27.61
27.87
28.04
27.91
27.83
27.85
27.98

26.07
27.72
27.84
27.81
27.49
27.75
27.92
27.77
27.73
27.73
27.86

26.01
27.59
27.61
27.70
27.37
27.64
27.82
27.64
27.61
27.63
27.75

25.96
27.47
27.51
27.64
27.29
27.58
27.75
27.52
27.49
27.56
27.67

4.3. Single Model For Arbitrary Scale Factor

Since no previous approach has focused on the super-
resolution of arbitrary scale factor with a single model, we
need to design several baselines. We compare our approach
with these baselines to prove the superiority of Meta-SR.

Suppose we want to zoom in the LR image with scale
r ∈ (1, 4]. Before we feed it into the network, we could
upscale it with bicubic interpolation. Thus, the ﬁrst baseline

simply upscales the LR image with bicubic interpolation as
the ﬁnal HR image, called bicubic baseline. The second
approach upscales the LR image r times at ﬁrst, and then
input it into a CNN to generate the ﬁnal HR image, called
EDSR(x1) and RDN(x1) respectively. These two methods
are very time-consuming and hard to put into practical use.

The third baseline downscales the generated HR image.
Suppose there is a network G to implement the k times up-

1580

scale. Thus we could input the LR image into the network
G to generate the HR image. And then we downscale the
HR image with scale factor r
k to predict the ﬁnal results. If
the k = 2, we call them RDN(x2), and EDSR(x2) respec-
tively. For the scale r > k, we have to upscale the LR im-
age before feeding it into the network. If the k = 4, it is
the fourth baseline. We call them RDN(x4) and EDSR(x4)
respectively.

In order to prove the superiority of the Weight Prediction
and the Location Projection, we design the ﬁfth baseline
(BicuConv): we use the interpolation to upscale the ﬁnal
feature maps, the upscale module is ﬁxed convolution layer
for all scale factor. And the sixth baseline (Meta-Bicu) is
interpolating the feature maps to the needed size. We use
the Weight Prediction network to predict the weights of the
convolution ﬁlter for each scale factor. We train all these
models on arbitrary scale factor together.

The experimental results are shown in Table 1. For
the bicubic interpolation baseline, simply upscaling the LR
image with bicubic interpolation could not introduce any
texture or details to the HR images.
It has very lim-
ited performance. For the RDN(x1) and EDSR(x1), it has
low-performance on the large scale factors. And the up-
scaled input makes it time-consuming. For the RDN(x4)
and EDSR(x4), the performance has huge gap between our
Meta-RDN and RDN(x4) (or Meta-EDSR and EDSR(x4))
for scale factor close to 1. Moreover, EDSR(x4) and
RDN(x4) also have to upscale the LR image before feed-
ing it into the network when the the scale r > k.

Thanks to the Weight Prediction, both Meta-Bicu and
our Meta-SR could learn the best weights of the ﬁlter for
each scale factor while BicuConv shares the same weights
of the ﬁlter for all the scale factors. The experimental results
show that Meta-Bicu is signiﬁcantly better than BicuConv
which proves the superiority of the Weight Prediction Mod-
ule. At the same time, our Meta-RDN is also better than the
Meta-Bicu. For the interpolation on the feature maps, the
larger the scale factor is, the smaller the valid Filed Of View
(FOV) is. However, each scale factor has the same FOV in
our Meta-SR methods. Beneﬁted from the proposed Meta-
Upscale, our Meta-RDN achieves the better performance on
almost all scale factors than the other baselines.

4.4. The Inference Time

SISR is low-level image processing task and has very
high practical use. In real-world scenarios, the time require-
ments are very important and strict for SISR. We measure
the computing efﬁciency using Tesla P40 with Intel Xeon
E5-2670v3@2.30GHz. We choose the B100 [20] as the test
dataset. Here, we do not take the image pre-processing time
into consideration.

We conduct experiments to calculate the running time of
each module in our Meta-SR and the baselines. As shown

Table 2. Comparison running time with the baselines. FL rep-
resents the Feature Learning Module. WP is Weight Prediction
Module of our Meta-SR. Upscale is the Upscale Module. We test
on the B100 and the test scale factor is 2.

Methods

FL

WP

Upscale

3.66e-2s
RDN(x1)
3.29e-2s
RDN(x2)
RDN(x4)
3.13e-2s
Meta-RDN 3.28e-2s

-
-
-

1.5e-4s

1.7e-4s
1.9e-4s
3.2e-4s
3.6e-4s

in the Table.2. Compared with the Feature Learning Mod-
ule, the running time of our Weight Prodiction Module can
be neglected. Because there are only two fully connected
layers in our Meta-Upscale Module.

Although the computing efﬁciency of our Meta-SR on
single scale has no advantage when we compare with the
baselines RDN(x1), RDN(x2) and RDN(x4) on scale r = 2.
If we increase the scale factor to 8 or 16, our Meta-SR is less
time-consuming than these baselines. Moreover, if we want
to continuously zoom in the same image with different scale
factors like the common image viewer, our Meta-SR is the
fastest. Since our Meta-SR method only need to run the
Meta-Upscale Module for each scale factor. RDN(x1) and
RDN(x2) have to upscale the input image at ﬁrst. And then
they feed the upscaled image into whole network for each
scale factor. Thus, we claim that our Meta-RDN is more
efﬁcient and has better performance than these baselines.

4.5. Comparison With The SOTA Methods

We apply the proposed Meta-Upscale Module to the
RDN [36] by replacing the typical upscale module, called
Meta-RDN . We train our Meta-RDN on the DIV2K train-
ing images with random scale factor r ∈ (1, 4]. We com-
pare the Meta-RDN with the corresponding baseline RDN
[36]. For fair comparison, we also try to ﬁnetune our Meta-
RDN for each single scale factor. However, ﬁne-tuning on
each single integer scale factor have few improvement to
the ﬁnal performance. And the RDN re-trained the model
for each scale factor with different upscale module, includ-
ing X2, X3, X4. We test our Meta-RDN on four different
benchmarks with PSNR and SSIM metrics.

As shown in Table 3, the Meta-RDN achieve the compa-
rable or even better results compared with the correspond-
ing baseline RDN [36]. Since the proposed Meta-Upscale
could dynamically predict weights of the ﬁlter for each scale
and thanks to the weight prediction, we could train a single
model for multiple scale factors and work well at the ar-
bitrary scale factor. Moreover, our Meta-SR network only
need to save one model for test, but the typical model needs
to save several models. Our Meta-SR network is more efﬁ-
cient for SR of multiple scale factors.

1581

Table 3. Compared with the state-of-the-art methods on X2,X3,X4. The reported results of the state-of-the art methods are re-trained for
each scale factor.

Methods

Metric

bicubic

RDN

Meta-RDN

PSNR
SSIM

PSNR
SSIM
PSNR
SSIM

X2

30.24
0.8688

34.01
0.9212
34.04
0.9213

Set14

X3

27.55
0.7742

30.57
0.8468
30.55
0.8466

X4

26.00
0.7227

28.81
0.7871
28.84
0.7872

X2

29.56
0.8431

32.34
0.9017
32.35
0.9019

B100

X3

27.21
0.7385

29.26
0.8093
29.30
0.8096

X4

25.96
0.6675

27.72
0.7419
27.75
0.7423

X2

30.80
0.9339

39.18
0.9780
39.18
0.9782

Manga109

X3

26.95
0.8556

34.13
0.9484
34.14
0.9483

X4

224.89
0.7866

31.00
0.9151
31.03
0.9154

X2

31.35
0.9076

35.17
0.9483
35.18
0.9484

DIV2K

X3

28.49
0.8339

31.39
0.8931
31.42
0.8935

X4

26.92
0.7774

29.34
0.8446
29.36
0.8448

Figure 3. The visual comparison of an single image upsampling with different scale factors by our Meta-RDN.

4.6. Visual Results

In this section, we show the visual results in Fig.3 and
Fig.4 . As shown in Fig.4, we compare with the RDN(x1),
RDN(x2) and RDN(x4) for super-resolution of arbitrary
scale factor. Our Meta-SR has better performance for the
structure part. Since the RDN(x1), RDN(x2) and RDN(x4)
share the same the weights of ﬁlters for all the scale fac-
tors, the texture of the SR image generated by these baseline
methods is worse than our Meta-RDN. Thanks to the weight
prediction, our Meta-SR can predict a group of independent
weights for each scale factor.

5. Conclusion

We propose a novel upscale module named Meta-
Upscale to solve the super-resolution of arbitrary scale fac-
tor with a single model. The proposed Meta-Upscale Mod-
ule could dynamically predict the weights of the ﬁlters. For
each scale factor, the proposed Meta-Upscale Module gen-
erates a group of weights for the upscale module. By doing
convolution operation between the feature maps and the ﬁl-
ters, we generate the HR image of arbitrary size. Thanks
to the weight prediction, we can train a single model for
super-resolution of arbitrary scale factor. Especially, our
Meta-SR can continuously zoom in the same image with
multiple scale factors.

Figure 4. The visual comparison with four baselines. Our Meta-
RDN has better performance.

Acknowledgement

This research is supported by National Key R&D
Program of China (2017YFA0700800, 2016YFB1001002,
2016YFB1001000), National Natural Science Foundation
of China (61525306, 61633021, 61721004, 61420106015),
Capital Science and Technology Leading Talent Train-
ing Project (Z181100006318030), and Beijing Science and
Technology Project (Z181100008918010).

1582

LRX1.5X2.0X2.5X3.0X3.5X4.0HRPSNR/SSIMBicubic19.65/0.4390RDN(x1)16.96/0.2235RDN(x2)16.81/0.2361RDN(x4)18.40/0.4503Meta-RDN23.92/0.9109r=2img002 from Set14HRPSNR/SSIMBicubic21.78/0.7384RDN(x1)32.75/0.9484RDN(x2)29.22/0.98866RDN(x4)32.80/0.9446Meta-RDN33.61/0.9510img042 from B100r=1.5References

[1] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman,
D. Pfau, T. Schaul, B. Shillingford, and N. De Freitas. Learn-
ing to learn by gradient descent by gradient descent. In Ad-
vances in Neural Information Processing Systems, 2016. 3

[2] Q. Cai, Y. Pan, T. Yao, C. Yan, and T. Mei. Memory match-
ing networks for one-shot image recognition. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018. 3

[3] H. Chang, D.-Y. Yeung, and Y. Xiong. Super-resolution
through neighbor embedding. In Computer Vision and Pat-
tern Recognition, 2004. CVPR 2004. Proceedings of the
2004 IEEE Computer Society Conference on, volume 1,
pages I–I. IEEE, 2004. 2

[4] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep
convolutional network for image super-resolution. In Euro-
pean conference on computer vision. Springer, 2014. 2

[5] Q. Fan, D. Chen, L. Yuan, G. Hua, N. Yu, and B. Chen. De-
couple learning for parameterized image operators. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 442–458, 2018. 3

[6] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
by backpropagation. arXiv preprint arXiv:1409.7495, 2014.
2

[7] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014. 2

[8] B. K. Gunturk, Y. Altunbasak, and R. M. Mersereau.
Super-resolution reconstruction of compressed video using
transform-domain statistics.
IEEE Transactions on Image
Processing. 1

[9] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S.
Huang. Image super-resolution via dual-state recurrent net-
works.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018. 2

[10] M. Haris, G. Shakhnarovich, and N. Ukita. Deep back-
projection networks for super-resolution.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018. 2

[11] R. Hu, P. Doll´ar, K. He, T. Darrell, and R. Girshick. Learning
to segment every thing. Cornell University arXiv Institution:
Ithaca, NY, USA, 2017. 3

[12] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015. 5

[13] Y. Jo, S. Wug Oh, J. Kang, and S. Joo Kim. Deep video
super-resolution network using dynamic upsampling ﬁlters
without explicit motion compensation. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2018. 2, 3

[14] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image super-
resolution using very deep convolutional networks. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, 2016. 2

[15] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive con-
volutional network for image super-resolution. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, 2016. 2

[16] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In CVPR, 2017. 2, 5

[17] C. Lemke, M. Budka, and B. Gabrys. Metalearning: a sur-
vey of trends and technologies. Artiﬁcial intelligence review,
2015. 3

[18] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
deep residual networks for single image super-resolution. In
The IEEE conference on computer vision and pattern recog-
nition (CVPR) workshops, 2017. 1, 2, 5

[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. In CVPR, 2015. 4

[20] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of
human segmented natural images and its application to eval-
uating segmentation algorithms and measuring ecological
statistics. In Computer Vision, 2001. ICCV 2001. Proceed-
ings. Eighth IEEE International Conference on, volume 2,
pages 416–423. IEEE, 2001. 5, 6, 7

[21] S. Ravi and H. Larochelle. Optimization as a model for few-

shot learning. 2016. 3

[22] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
2016. 1, 2, 5

[23] W. Shi, J. Caballero, C. Ledig, X. Zhuang, W. Bai, K. Bha-
tia, A. M. S. M. de Marvao, T. Dawes, D. ORegan, and
D. Rueckert. Cardiac image super-resolution with global cor-
respondence using multi-atlas patchmatch. In International
Conference on Medical Image Computing and Computer-
Assisted Intervention. Springer, 2013. 1

[24] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep
recursive residual network. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, vol-
ume 1, page 5, 2017. 2

[25] Y. Tai, J. Yang, X. Liu, and C. Xu. Memnet: A persistent
memory network for image restoration. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4539–4547, 2017. 2

[26] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang,
L. Zhang, B. Lim, S. Son, H. Kim, S. Nah, K. M. Lee,
et al. Ntire 2017 challenge on single image super-resolution:
Methods and results. In Computer Vision and Pattern Recog-
nition Workshops (CVPRW), 2017 IEEE Conference on,
pages 1110–1121. IEEE, 2017. 5

[27] R. Timofte, V. De Smet, and L. Van Gool. Anchored neigh-
borhood regression for fast example-based super-resolution.
In Proceedings of the IEEE international conference on com-
puter vision, pages 1920–1927, 2013. 2

[28] X. Wang, K. Yu, C. Dong, and C. Change Loy. Recover-
ing realistic texture in image super-resolution by deep spa-

1583

tial feature transform. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018. 2

[29] Y.-X. Wang and M. Hebert. Learning to learn: Model regres-
sion networks for easy small sample learning. In European
Conference on Computer Vision, pages 616–634. Springer,
2016. 3

[30] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-
from error visibility to
IEEE transactions on image process-

Image quality assessment:

celli.
structural similarity.
ing, 13(4):600–612, 2004. 5

[31] T. Yang, X. Zhang, W. Zhang, and J. Sun. Metaanchor:
Learning to detect objects with customized anchors. NIPS,
2018. 3

[32] D. Yıldırım and O. G¨ung¨or. A novel image fusion method us-
ing ikonos satellite images. Journal of Geodesy and Geoin-
formation, 2012. 1

[33] R. Zeyde, M. Elad, and M. Protter. On single image scale-up
using sparse-representations. In International conference on
curves and surfaces, pages 711–730. Springer, 2010. 5

[34] K. Zhang, W. Zuo, and L. Zhang. Learning a single convo-
lutional super-resolution network for multiple degradations.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018. 5

[35] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu. Image
super-resolution using very deep residual channel attention
networks. arXiv preprint arXiv:1807.02758, 2018. 1, 2, 5

[36] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Resid-
ual dense network for image super-resolution. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 1, 2, 3, 5, 7

[37] W. W. Zou and P. C. Yuen. Very low resolution face recog-
IEEE Transactions on Image Processing,

nition problem.
2012. 1

1584

