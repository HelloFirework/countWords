Few-shot Adaptive Faster R-CNN

Tao Wang1

Xiaopeng Zhang1,2

Li Yuan1

Jiashi Feng1

1Department of Electrical and Computer Engineering, National University of Singapore, Singapore

2Huawei Noah’s Ark Lab, Shanghai, China

twangnh@gmail.com zhangxiaopeng12@huawei.com ylustcnus@gmail.com elefjia@nus.edu.sg

Abstract

To mitigate the detection performance drop caused by
domain shift, we aim to develop a novel few-shot adapta-
tion approach that requires only a few target domain im-
ages with limited bounding box annotations. To this end,
we ﬁrst observe several signiﬁcant challenges. First, the
target domain data is highly insufﬁcient, making most ex-
isting domain adaptation methods ineffective. Second, ob-
ject detection involves simultaneous localization and clas-
siﬁcation, further complicating the model adaptation pro-
cess. Third, the model suffers from over-adaptation (similar
to overﬁtting when training with a few data example) and
instability risk that may lead to degraded detection perfor-
mance in the target domain. To address these challenges,
we ﬁrst introduce a pairing mechanism over source and tar-
get features to alleviate the issue of insufﬁcient target do-
main samples. We then propose a bi-level module to adapt
the source trained detector to the target domain: 1) the split
pooling based image level adaptation module uniformly ex-
tracts and aligns paired local patch features over loca-
tions, with different scale and aspect ratio; 2) the instance
level adaptation module semantically aligns paired object
features while avoids inter-class confusion. Meanwhile, a
source model feature regularization (SMFR) is applied to
stabilize the adaptation process of the two modules. Com-
bining these contributions gives a novel few-shot adaptive
Faster-RCNN framework, termed FAFRCNN, which effec-
tively adapts to target domain with a few labeled samples.
Experiments with multiple datasets show that our model
achieves new state-of-the-art performance under both the
interested few-shot domain adaptation(FDA) and unsuper-
vised domain adaptation(UDA) setting.

1. Introduction

Humans can easily recognize familiar objects from new
domains, while current object detection models suffer sig-
niﬁcant performance drop in unseen environments due to
domain shift. Poor adaptability to new domains severely
limits the applicability and efﬁcacy of these models. Pre-

Figure 1. Illustration of our main idea. Middle row shows two
images from Cityscapes and Foggy Cityscapes respectively. Top
row shows background patches reﬂecting image level domain shift
and bottom row shows independent objects (cars) reﬂecting object
instance level domain shift.

vious works tackling domain shift issues for deep CNN
models [12, 42, 29, 1] are mainly targeted at the unsuper-
vised domain adaptation (UDA) setting, which requires a
large amount of target domain data and comparatively long
adaptation time. Only a few works consider the supervised
domain adaptation (SDA) [39, 7, 32] setting. However, as
UDA methods, they mainly focus on the simple task of clas-
siﬁcation, and may not apply well to more complex tasks
like object detection that involves localizing and classifying
all individual objects over high resolution inputs.

In this paper, we explore the possibility of adapting an
object detector trained with source domain data to target
domain with only a few loosely annotated target image sam-
ples (not all object instances are annotated). This is based
on our key observation that limited target samples can still
largely reﬂect major domain characteristics, e.g. illumina-
tion, weather condition, individual object appearance, as
shown in Fig. 1. Also, the setting is appealing in practice
as collecting a few representative data from a new domain
needs negligible effort, meanwhile can reduce the inevitable
noise brought by large amount of samples. However, it is
very challenging to learn domain invariant representation
with only a few target data samples, and detectors require

7173

Image level shiftInstance level shiftﬁne-grained high resolution features for reliable localiza-
tion and classiﬁcation.

To address this challenge, we proposed a novel frame-
work that consists of two level of adaptation modules cou-
pled with a feature pairing mechanism and a strong regu-
larization for stable adaptation. The pairing process pairs
feature samples into two groups to effectively augment lim-
ited target domain data, pairs in ﬁrst group consist of one
sample from target domain and one from the source do-
main, and pairs in the second group are both from the source
domain. Similar approach has been used in [31] for aug-
menting image samples, while we augment local feature
patches and object features in the two adaptation module
respectively. With the introduced pairing mechanism, the
image-level module uniformly extracts and aligns paired
multi-grained patch features to address the global domain-
shift like illumination; the instance-level module semanti-
cally matches paired object features while avoids confu-
sion between classes as well as reduced discrimination abil-
ity. Both of these two modules are trained with a domain-
adversarial learning method. We further propose a strong
regularization method, termed source model feature reg-
ularization (SMFR), to stabilize training and avoid over-
adaptation by imposing consistency between source and
adapted models on feature response of foreground anchor
locations. The bi-level adaptation modules combined with
SMFR can robustly adapt source trained detection model to
new target domain with only few target sample data. The re-
sulted framework, termed few shot adaptive Faster R-CNN
(FAFRCNN), offers a number of advantages:

• Fast adaptation. For a source trained model, our
framework empirically only needs hundreds step of
adaptation updates to reach desirable performance un-
der all established scenarios.
In contrast previous
methods under UDA setting [43, 5] requires tens of
thousands of steps to train.

• Less data collection cost. With only few represen-
tative data sample, the FAFRCNN model can greatly
boost source detector on target domain, drastically mit-
igating data collection cost. Under the devised loosely
annotation process, the amount of human annotating
time is reduced signiﬁcantly.

• Training stability. Fine-tuning with limited target
data sample can lead to severe over-ﬁtting. Also, do-
main adaptation approaches relying on adversarial ob-
jective might be unstable and sensitive to initialization
of model parameters. This issue greatly limits their ap-
plicability. The proposed SMFR approach enables the
model to avoid over-ﬁtting and beneﬁt from the few
target data samples. For the two adversarial adapta-
tion modules, although imposing SMFR could not sig-
niﬁcantly boost their performance, the variance over

different runs is drastically reduced. Thus SMFR pro-
vides much more stable and reliable model adaptation.

To demonstrate the efﬁcacy of the proposed FAFR-
CNN for cross-domain object detection, we conduct the
few-shot adaptation experiments under various scenarios
constructed with multiple datasets including Cityscapes,
SIM10K, Udacity self-driving and Foggy Cityscapes. Our
model signiﬁcantly surpasses compared methods and out-
performs state-of-art method using full target domain data.
When applied to UDA setting, our method generates new
state-of-art result for various scenarios.

2. Related Work

Object Detection Recent years have witnessed remark-
able progress on object detection with deep CNNs and
various large-scale datasets. Previous detection architec-
tures are grouped into two- or multi-stage models like
R-CNN [15], Fast R-CNN [14], Faster R-CNN [37] and
Cascaded R-CNN [3], as well as single-stage models like
YOLO [35], YOLOv2 [36], SSD [28] and Retinanet [27].
However, all of them require a large amount of training data
with careful annotations, thus are not directly applicable to
object detection in unseen domains.

Cross-domain Object Detection Recent works on do-
main adaptation with CNNs mainly address the simple task
of classiﬁcation [29, 11, 13, 2, 26, 18, 30], and only a few
consider object detection. [45] proposed a framework to
mitigate the domain shift problem of deformable part-based
model (DPM). [34] developed subspace alignment based
domain adaptation for the R-CNN model. A recent work
[20] used a two-stage iterative domain transfer and pseudo-
labeling approach to tackle cross-domain weakly super-
vised object detection. [5] designed three modules for un-
supervised domain adaptation of the object detector. In this
work, we aim at adapting object detectors with a few target
image samples and build a framework for robust adaptation
of state-of-the-art Faster R-CNN models under this setting.

Few-shot Learning Few-shot learning [9] was proposed
to learn a new category with only a few examples, just
as humans do. Many works are based on Bayesian infer-
ence [25, 24], and some leverage memory machines [17,
41]. Later, [19] proposed to transfer the base class feature
to a new class; a recent work [10] proposed a meta learn-
ing based approach which achieves state-of-the-art. Incor-
porating few-shot learning into object detection was pre-
viously explored. [8] proposed to learn an object detector
with a large pool of unlabeled images and only a few anno-
tated images per category, termed few-shot object detection
(FSOD); [4] tackled the setting of few-shot object detection
with a low-shot transfer detector (LSTD) coupled with de-
signed regularization. Our FDA setting differs in that target

7174

data distribution changed but task remain the same, while
few-shot learning aims at a new tasks.

3. Method

In this section, we elaborate on our proposed few-shot
domain adaptation approach for detection. To tackle the
issue brought by insufﬁcient target domain samples, we in-
troduce a novel feature pairing mechanism built upon fea-
tures sampled by split pooling and instance ROI sampling.
Our proposed approach performs domain adaptation over
the paired features at both image and object-instance levels
through domain-adversarial learning, where the ﬁrst level
alleviates global domain shift and the second level seman-
tically aligns object appearance shift while avoiding con-
fusion between classes. To stabilize the training and avoid
over-adaptation, we ﬁnally introduce the source model fea-
ture regurgitation technique. We apply these three novel
techniques to Faster R-CNN model and obtain the few-shot
adaptive Faster-RCNN (FAFRCNN), which is able to adapt
to novel domains with only a few target domain examples.

3.1. Problem Setup

Suppose we have a large set of source domain training
data (XS, YS) and a very small set of target data (XT , YT ),
where XS and XT are input images, YS denotes complete
bounding box annotation for XS, and YT denotes loose an-
notation for XT . With only a few object instances in the
target domain images annotated, our goal is to adapt a de-
tection model trained on source training data to the target
domain with minimal performance drop. We only consider
loose bounding box annotation to reduce annotation effort.

3.2. Image level Adaptation

Inspired by the superior result of the patch based domain
classiﬁer compared to its full image counterpart in previous
seminal works [21, 46] for image to image translation. We
propose split pooling (SP) to uniformly extract local feature
patches across locations with different aspect ratio and scale
for domain adversarial alignment.

Speciﬁcally, given grid width w and height h, the pro-
posed split pooling ﬁrst generates random offsets sx and sy
for x- and y-axis ranging from 0 to the full grid width w and
height h respectively (i.e., 0<sx<w, 0<sy<h, sx, sy ∈ N),
as shown in the top left panel of Fig. 2. A random grid is
formed on the input image with the offset of (sx, sy) start-
ing from the top left corner of the input image. This random
sampling scheme gives a trade-off between static grid that
may generate biased sampling, and exhausting all grid loca-
tions that suffers redundancy and over-sampling.

The grid window width w and height h are set with scales
and ratios as anchor boxes in Faster R-CNN. We empiri-
cally choose 3 scales (large scale 256, medium scale 160,
and small scale 96, corresponding to feature size 16, 10

and 6 on relu 5 3 of VGG16 network) and 3 aspect ratios
(0.5, 1, 2), resulting in 9 pairs of w and h. For each pair,
gird is generated then non-border rectangles in the grid are
pooled into ﬁxed sized features with ROI pooling. Pooling
enables different sized grids to be compatible with a single
domain classiﬁer without changing the patch-wise charac-
teristics of the extracted features. Formally, let f be the
feature extractor and X be the set of input images. We
perform split pooling at three scales, result in the features
spl(f (X)), spm(f (X)), and sps(f (X)) respectively. We
separate them according to scales as we want to investigate
the contribution of different scales independently. These lo-
cal patch features can reﬂect image-level domain shifts like
varied illumination, weather change, etc. Since those shifts
spread on the whole image, the phenomenon is more evi-
dent for object detection as input images are usually large.
We then develop image-level adaptation module which
performs multi-scale alignment with paired local fea-
tures. Speciﬁcally, it tackles image-level shift by ﬁrst pair-
ing the extracted local features from split pooling to form
two groups for each of the three scales. e.g., for the small
scale patch, Gs1 = {(gs, gs)}, Gs2 = {(gs, gt)}, where
gs ∼ sps(f (XS)) and gt ∼ sps(f (XT )). Here the pairs
within the ﬁrst group Gs1 consist of samples from the
source domain only, and pairs within the second group Gs2
consist of one sample from source and another from the tar-
get domain. Such a pairing scheme effectively augments the
limited target domain feature samples.

To adapt the detection model, domain-adversarial learn-
ing objective is imposed to align the constructed two groups
of features. The domain-adversarial learning [11, 42, 43]
employs the principle in generative adversarial learning [16]
to minimize an approximated domain discrepancy distance
through adversarial objective on feature generator and do-
main discriminator. Thus the data distribution is aligned
and source task network can be employed for the target do-
main. Speciﬁcally the domain discriminator tries to classify
the feature to source and target domain while the feature
generator tries to confuse the discriminator. The learning
objective of small scale discriminator Dsps is to minimize

Lspsd = −Ex∼Gs1 [log Dsps (x)]

− Ex∼Gs2 [log(1 − Dsps (x))],

such that the discriminator can tell clearly the source-source
feature pairs apart from source-target feature pairs. The ob-
jective of the generator is to transform the features from
both domains such that they are not distinguishable to the
discriminator, by maximizing the above loss.

We can similarly get losses for medium and large scale
discriminator as Lspmd and Lspld . We use 3 separate dis-
criminators for each scale. In addition, this module operates
requiring no supervision. Thus it can be used for unsuper-
vised domain adaptation (UDA). Together, the image level

7175

Figure 2. Framework of the proposed few shot adversarial adaptive Faster R-CNN model(FAFRCNN). We address the domain shift with
image level and instance level adaptation modules, the former with different grid size adapts multi-grained feature patches and latter seman-
tically aligns independent object appearance, the modules augmented with the proposed pairing mechanism result in effective alignment
of feature representation in such few shot scenario(refer to Section 3 for details), we further developed source model feature regulariza-
tion(SMFR) which dramatically stabilizes the adaptation process.

discriminator’s objective is to minimize:

puts with a following objective to minimize:

Limd = Lspsd + Lspmd + Lspld ,

and the feature generator’s objective is to maximize Limd .

3.3. Instance level Adaptation

To mitigate object instance level domain shift, we pro-
pose the instance-level adaptation module which semanti-
cally aligns paired object features.

Speciﬁcally, we extend the Faster R-CNN ROI sampling
to instance ROI sampling. The Faster R-CNN ROI sam-
pling scheme samples ROIs to create training data for clas-
siﬁcation and regression heads. It by default separates fore-
ground and background ROIs with an IOU threshold of
0.5 and samples them at a speciﬁc ratio (e.g., 1:3). Dif-
ferently, our proposed instance ROI sampling keeps all the
foreground ROIs with higher IOU threshold (i.e., 0.7 in our
implementation) to ensure the ROIs are closer to real ob-
ject regions and suitable for alignment. The foreground
ROI features of source and target domain images, accord-
ing to their class, are passed through the intermediate layers
(i.e., the layers after ROI pooling but before classiﬁcation
and regression heads) to get sets of source object features
Ois and target object features Oit. Here i ∈ [0, C] is the
class label and C is the total number of classes. Then they
are further paired into two groups the same way as image
level patch features, resulting in Ni1 = {(nis, nis)} and
Ni2 = {(nis, nit)}. Here nis ∼ Ois and nit ∼ Oit. The
multi-way instance-level discriminator Dins has 2 × C out-

Linsd =

C

X

i=1

−Ex∼Ni1 [log Dins(x)i1]

− Ey∼Ni2 [log Dins(y)i2].

Here Dins(x)i1 denotes discriminator output over the i-th
class of ﬁrst group. Correspondingly, the objective of fea-
ture generator is to minimize

Linsg =

C

X

i=1

−Ex∼Ni1 [log Dins(x)i2]

− Ey∼Ni2 [log Dins(y)i1],

which aims to confuse the discriminator between two do-
mains while avoid misclassiﬁcation to other classes.

3.4. Source Model Feature Regularization

Training instability is a common issue for adversarial
learning and is more severe for cases of insufﬁcient training
data, which may result in over-adaptation. Fine-tuning with
limited target data would also unavoidably lead to overﬁt-
ting. We resort to a strong regularization to address the in-
stability by forcing the adapted model to produce consistent
feature response on source input with the source model in
the sense of ℓ2 difference. The purpose is to avoid over-
updating learned representation towards limited target sam-
ples that degrades the performance. A similar form of ℓ2
penalty on the feature map was used in image to image
translation method [1, 21] to constrain content change.

Formally, Let fs and ft be the feature extractors of the
source model and the adapted model respectively. Then the

7176

Regression HeadClassification HeadRPNROI_PoolingROI_PoolingSource & Target DataSource DataSMFRInstance Roi SamplingSP_PoolingObject-level domain classifierPairing,SamplingSource carsTarget carsTarget personsSource personsSharing ParametersPairing,SamplingImage-level domain classifiersource model feature regularization (SMFR) term is

Lreg = Exs∼XS

1
wh

||fs(xs) − ft(xs)||2
2,

where w and h are the width and height of the feature map.
However, object detection cares more about local fore-
ground feature regions while background area is usually
unfavorably dominant and noisy. We ﬁnd directly impos-
ing the regularization on global feature map leads to severe
deterioration when adapting to the target domain. Thus we
propose to estimate those foreground regions on the fea-
ture map as the anchor locations that have IOU with ground
truth boxes larger than a threshold (0.5 is used in implemen-
tation). Denote M as the estimated foreground mask. Then
we modify the proposed regularization as follows:

Lreg = Exs∼XS

1
k

||(fs(xs) − ft(xs)) ∗ M ||2
2,

where k is the number of positive mask locations. This is
partially inspired by the “content-similarity loss” from [1]
that employs available rendering information to impose ℓ2
penalty on foreground regions of the generated image.

3.5. Training of FAFRCNN

The framework is initialized with the source model
and optimized by alternating between following objectives:
Step 1. Minimize the following loss w.r.t. full detection
model: Lg = α(Limg +Linsg )+βLdet+λLreg, where Ldet
denotes Faster R-CNN detection training loss on source
data, α, β and λ are balancing hyperparameters controlling
interaction between losses. Step 2. Minimize following
loss w.r.t. domain discriminators: Ld = Limd + Linsd .

4. Experiments

In this section, we present evaluation results of the pro-
posed method on adaptation scenarios capturing different
domain shift constructed with multiple datasets. In experi-
ments, VGG16 network based Faster-RCNN is used as the
detection model.

4.1. Datasets and Setting

Datasets We adopt following four datasets to establish the
cross-domain adaptation scenarios for evaluating the adap-
tation ability of our model and comparing methods. The
SIM10K [23] dataset contains 10k synthetic images with
bounding box annotation for car, motorbike and person.
The Cityscapes dataset contains around 5000 accurately
annotated real world images with pixel-level category la-
bels. Following [5], we take box envelope of instance mask
for bounding box annotations. The Foggy Cityscapes [40]
dataset is generated from Cityscapes with simulated fog.
The Udacity self-driving dataset (Udacity for short) [44]
is an open source dataset collected with different illumina-
tion, camera condition and surroundings as Cityscapes.

Evaluation scenarios The
established cross-domain
adaptation scenarios include Scenario-1:
SIM10K to
Udacity (S → U); Scenario-2: SIM10K to Cityscapes
(S → C); Scenario-3: Cityscapes to Udacity (C → U);
Scenario-4: Udacity to Cityscapes (U → C); Scenario-5:
Cityscapes to Foggy Cityscapes (C → F). The ﬁrst two sce-
narios capture synthetic to real data domain shift, which is
important as learning from synthetic data is very promising
way to address the lack of labeled training data [6, 38, 33];
Scenario-3 and Scenario-4 constructed with both real
world collected datasets mainly aim for domain shift like
illumination, camera condition, etc., which is important for
practical applications; And the last scenario captures the
extreme weather change of normal to foggy condition. We
sample from target train set and test on target val set, the
source model is trained with full source dataset.

Baselines We compare our method with following base-
lines: (1) Source training model. The model trained with
source data only and directly evaluated on target domain
data. (2) ADDA [43]. ADDA is a general framework for ad-
dressing unsupervised adversarial domain adaptation. Last
feature map is aligned in experiments. (3) Domain trans-
fer and Fine-tuning (DT+FT). The method has been used
as a module in [20] for adapting object detector to target
domain. In UDA setting, we use CycleGAN [46] to train
and transform source image to target domain. In FDA set-
ting, since very few target domain samples are available,
we employ method in [22] that needs only one target style
image to train the transformation. This baseline is denoted
as DTf +FT. (4) Domain Adaptive Faster R-CNN [5]. The
method is deliberately developed for unsupervised domain
adaptation, denoted as FRCNN UDA.

4.2. Quantitative Results

We evaluate the proposed method by conducting exten-
sive experiments on the established scenarios. To quantify
the relative effect of each step, the performances of are ex-
amined with different conﬁgurations. We also evaluate pro-
posed split pooling based image level adaptation in the un-
supervised domain adaptation (UDA) setting, where large
amount of unlabeled target images are available.

Speciﬁcally, for the few-shot domain adaptation (FDA)
setting, we perform the following steps for each run: (1)
Randomly sample ﬁxed number of target domain images,
ensure that needed class are presented; (2) Simulate loosely
annotating process to get annotated target domain images,
i.e., only randomly annotate ﬁxed number of object in-
stances; (3) Gradually combine each component of our
method, run the adaptation and record performance (AP);
(4) Run compared methods on the same sampled images
and record performance. For the UDA setting, only pro-
posed split pooling based adaptation component is used as
no annotation is available in the target domain.

7177

sps spm spl ins ft

S→U

S →C

sps spm spl ins ft C→U

U→C

Source

FDA setting

ADDA [43]

DTf +FT

FRCNN UDA [5]

X

X

34.1

33.5

34.3±0.9 34.4±0.7

35.2±0.3 35.6±0.6

33.8±1.0 33.1±0.4

35.1±0.6 35.4±0.8

34.9±0.6 34.8±0.5

Source only

FDA setting

ADDA [43]

DTf +FT

FRCNN UDA [5]

X

X

44.5

44.0

44.3±0.9 44.2±1.2

44.9±0.6 45.1±0.5

43.0±0.8 43.3±0.8

45.9±0.7 47.2±0.3

46.1±0.4 47.6±0.5

X

36.0±1.0 34.8±0.8

X

45.3±0.4 48.1±0.7

X X

35.2±1.0 35.8±0.6

X X

45.9±0.6 48.0±0.5

Ours

X X X

36.8±0.6 37.0±0.9

Ours

X X X

46.8±0.3 48.8±0.9

X

37.2±0.9 37.1±0.6

X

46.4±0.5 47.1±0.7

X X X X

38.8±0.3 39.2± 0.5

X X X X

47.8±0.4 49.2±0.4

X 34.8± 0.4 34.6± 0.5

X 45.5±0.8 45.0± 0.6

X X X X X 39.3± 0.3 39.8± 0.6

X X X X X 48.4±0.4 50.6± 0.6

UDA setting

ADDA [43]

DT+FT

FRCNN UDA [5]

Ours (SP only)

35.2

36.1

36.7

40.5

36.1

36.8

38.9

41.2

UDA setting

ADDA [43]

DT+FT

FRCNN UDA[5]

Ours (SP only)

46.5

46.1

47.9

48.5

47.5

47.8

49.0

50.2

Table 1. Quantitative results of our method on Scenario-1 and
Scenario-2, in terms of average precision for car detection. UDA
denotes traditional setting where large amount of unlabeled target
images are available, and FDA indicates the proposed few shot do-
main adaptation setting. sps, spm and spl denote small, medium
and large scale split pooling respectively. “ins” indicates object
instance level adaptation and “ft” denotes adding ﬁne-tuning loss
with available target domain annotations. For FDA setting, both
S→U and S→C samples 8 images per experiment round and an-
notate 3 car objects per image.

Results for Scenario-1 As summarized in Table 1, un-
der FDA setting, comparing to source training model, the
three different scaled image level adaptation modules inde-
pendently provide favourable gain. Further combining them
gives higher improvement (2.7 AP gain on mean value), in-
dicating the complementary effect of alignments at differ-
ent scales. The object instance level adaptation component
independently generates 3.1 AP improvement. Combining
image level components with instance level module further
enhance the detector by 1.6 AP over instance level module
only and 2.0 AP over the image level adaptation only, sug-
gesting complementing effect of the two modules. Fune-
tuning with the limited loosely annotated target samples
brings minor improvement, but the gain is orthogonal to the
adversarial adaptation modules. The combination of all pro-
posed components brings 5.2 AP boost over the raw source
model, which already outperforms state-of-art method [5]
under UDA setting.

It is clearly observable that baseline methods generate
less improvement. The ADDA [43] and FRCNN UDA [5]

Table 2. Quantitative results of our method on Scenario-3 and
Scenario-4. For FDA setting, C→U samples 16 images per experi-
ment round, and U→C samples 8 images per round, both annotate
3 car objects per image.

methods barely brings any gains for the detector, suggest-
ing they cannot effectively capture and mitigate the domain
shift with only s few target data samples. The DTf +FT
method results in about 1.0 AP gain, suggesting the style
transfer method only weakly captures the domain shift in
our setting where there is no such drastic style discrepancy
as between those real images and comic or art works [22].

For the UDA setting, as sufﬁcient target domain data are
available, the three compared methods all get better results.
While our proposed split pooling based adaptation brings
much better results. We observe 6.4 AP gain over the base-
line source model, indicating the module effectively cap-
tures and mitigates domain shift, for both cases where a few
or sufﬁcient target domain images are available.

Result for other four scenarios As presented in Table 1
to Table 3, for all the other scenarios, the results share sim-
ilar trend with scenario-1. For FDA setting, our method
provides effective adaptation for the source training model,
signiﬁcantly surpassing all baselines and outperforms state-
of-the-art method under UDA setting. For UDA setting,
our method generates SOTA performance with the proposed
split pooling based adaptation.
It is interesting to note
the performance of Scenario-1 (S→U) is much lower than
Scenario-3 (C→U) though they share same test set. This is
because the visual scene in SIM10K dataset is much sim-

7178

Source

FDA setting
ADDA [43]
DTf +FT
FRCNN UDA [5]

Ours

UDA setting
ADDA [43]
DT+FT
FRCNN UDA [5]
Ours (SP only)

sps spm spl ins ft person
24.1

rider

29.9

car

32.7

truck

10.9

bus

13.8

train

5.0

mcycle

bicycle mAP

14.6

27.9

19.9

X

X

X

24.4± 0.3 29.1± 0.9 33.7± 0.5 11.9± 0.5 13.3± 0.8 7.0± 1.5 13.6± 0.6 27.6± 0.2 20.1± 0.8
23.5± 0.5 28.5± 0.6 30.1± 0.8 11.4± 0.6 26.1± 0.9 9.6± 2.1 17.7± 1.0 26.2± 0.6 21.7± 0.6
24.0± 0.8 28.8± 0.7 27.1± 0.7 10.3± 0.7 24.3± 0.8 9.6± 2.8 14.3± 0.8 26.3± 0.8 20.6± 0.8
25.7± 0.8 35.6± 1.0 35.8± 0.8 17.7± 0.3 31.9± 0.5 9.4± 2.5 21.6± 1.5 30.3± 0.5 26.0± 1.0
27.8± 1.0 34.4± 0.8 41.3± 1.0 19.6± 0.8 31.9± 1.2 12.2± 2.1 18.3± 1.2 29.2± 0.5 26.9± 0.5
27.4± 0.8 36.3± 1.1 39.7± 0.9 19.4± 0.9 34.8± 1.5 10.0± 2.0 19.6± 1.1 30.3± 0.7 27.2± 0.3
27.8± 0.4 36.4± 0.4 39.4± 1.0 18.1± 0.2 33.8± 1.5 10.9± 1.9 18.8± 1.3 30.1± 0.2 26.9± 0.6
25.7± 0.9 36.3± 1.1 40.4± 0.7 20.1± 0.3 34.5± 1.3 12.8± 2.2 24.1± 1.6 30.3± 0.4 28.0± 0.5
23.7± 1.0 30.2± 0.9 30.1± 0.4 11.5± 0.6 25.8± 1.1 11.2± 2.5 15.8± 1.3 28.5± 0.7 22.1± 0.4
26.7± 0.6 36.2± 1.2 41.0± 0.6 20.3± 0.7 32.8± 1.9 18.7± 2.6 21.1± 1.4 29.8± 0.6 28.3± 0.5
X 23.5± 0.7 29.0± 0.6 27.1± 0.5 10.9± 0.2 23.2± 1.0 9.8± 2.6 16.0± 1.4 26.4± 0.2 20.8± 0.8
X X X X X 27.9± 0.6 37.8± 0.6 42.3± 0.7 20.1± 0.5 31.9± 1.1 13.1± 1.5 24.9± 1.3 30.6± 0.9 28.6± 0.5

X X X X

X X X

X X

X

25.7
25.3
25.0
29.1

35.8
35.0
31.0
39.7

38.5
35.9
40.5
42.9

12.6
18.7
22.1
20.8

25.2
32.1
35.3
37.4

9.1
9.8
20.2
24.1

21.5
20.9
20.0
26.5

30.8
30.9
27.1
29.9

24.9
26.1
27.6
31.3

Table 3. Quantitative results of our method on Scenario-5. 8 images(1 image per class) are sampled for each experiment round, and 1
object bounding box is annotated for corresponding class per image.

pler than that in Cityscapes, where more diverse car object
instances are presented, providing better training statistics.
Similar trend is observed in Scenario-2 and Scenario-4.

4.3. Qualitative Results

Figure 3 shows some qualitative result from Scenario
2 (S→C). It can be clearly observed that 1) the adapted
model outputs tighter bounding boxes for each object, in-
dicating better localization ability; 2) the adapted model
places higher conﬁdence on detected objects, especially for
those harder objects (e.g., the car in the ﬁrst image occluded
by the road sign); 3) the source model missed some small
objects, while the adapted model can detect them.

4.4. Ablation Analysis

Effect of pairing As shown in Table 4, we independently
examine the pairing effect on split pooling module and ob-
ject instance level adaptation module. When not paired, we
reduce input channel number of corresponding discrimina-
tor and remain the other parts unchanged. Without the intro-
duced pairing, the performance of adaptation drops signif-
icantly. This indicates effectiveness of the pairing for aug-
menting the input data for discriminator learning.

S→C

C→U U→ C

sps spm spl ins S→U
34.1

44.5

33.5

44.0

source
pairing X X X
36.8± 0.4 37.0± 0.7 46.8± 0.6 48.8± 0.5
X X X
34.8± 0.5 34.3± 0.6 44.6± 0.3 45.8± 0.4
w/o
X 37.2± 0.9 37.1± 0.6 46.4± 0.5 47.1± 0.2
pairing
X 35.7± 0.6 34.9± 0.5 44.1± 0.6 45.3± 0.8
w/o
pairing X X X X 39.3± 0.6 39.8± 0.7 48.4± 0.7 50.6± 0.5
X X X X 36.1± 0.6 36.8± 0.6 44.5± 0.3 45.5± 0.4
w/o

Table 4. The effect of the introduced pairing mechanism.

small set of images contain less than 6 car objects). We
vary the number of target images from 1 to 8 exponentially.
For Scenario-5, as for most classes (like truck, bus, train,
rider) there is only 1 instance in an image, we only annotate
1 box for each image. We do not examine beyond 8 images
as there are already at most 48 (6 boxes* 8 images) and 64
(1 box*8 classes*8 images) object instances in Fig.4(a)(b)
and Fig.4(c) involved, which can be deemed as sufﬁciently
many for FDA evaluation. As shown in Figure 4, the re-
sults suggest common phenomenon that using more image
and more box generates higher adaptation results. As im-
age number increases exponentially, the roughly linear im-
provement suggests saturating effect.

Number of sample images and annotated boxes We ex-
amine the effect of varying the number of target domain
images and annotating bounding boxes under Scenario-1, 4
and 5. We draw the mean value curve across all the sam-
pling rounds. As car is abundant class for target domain
of Scenario-1 and Scenario-4, we vary the annotated boxes
number from 1 to up to 6 (at most 6 boxes considering a

Sharing parameters among discriminators For split
pooling based adaptation, we use the same discriminator ar-
chitecture with shared parameters for different scale. While
the discriminators could also be independent and not shar-
ing parameters. As shown in Table 6, it is clearly observed
that sharing the discriminator between small, medium and
large scales provides much better results. Such interesting

7179

Figure 3. Qualitative result. The results are sampled from S→U scenario, we set a bounding box visualization threshold of 0.05. The ﬁrst
row are sample output from unadapted source training model, and second raw are corresponding detection output from adapted model.

u6-box
3-box
1-box
source

40

39

38

37

36

35

P
A
 
r
a
c

u6-box
3-box
1-box
source

50

49

48

P
A
 
r
a
c

47

46

45

44

1

2
4
target image number

8

1

2
4
target image number

8

1-box
source

30.0

29.5

29.0

28.5

P
A
m

28.0
19.0
18.8
18.6
18.4
18.2
18.0

1

2

4

target image number per class

8

(a)

(b)

(c)

Figure 4. Varying target sample image number and annotation boxes number. (a) S→U. (b) U→C. (c)C→F. 1-box, 3-box denote annotating
only 1 or 3 box each sampled image, and u6-box means annotating at most 6 boxes as some images does not contain enough to 6 car objects.

sps

spm spl

ins

ft mean

source
SMFR
w/o
SMFR X
X
w/o

33.5
X 34.6
X 30.1
39.6
39.4

X

X

X

X

X

X

std

-
0.2
1.8
0.3
2.1

Table 5. The effect SMFR, with S→C scenario, mean and std de-
note mean and standard derivation of APs for the 10 runs.

S→U

S→C

C→U

U→ C

34.1

source
36.8± 0.6 37.0± 0.6 46.8± 0.3 48.8± 0.4
SP share
SP not share 35.1± 0.3 35.3± 0.6 45.2± 0.7 46.8± 0.8

33.5

44.5

44.0

feature regularization (SMFR), within one round of sample,
we measure the standard derivation of the adapted model
performance over 10 runs with different random parameter
initialization. Table 5 illustrates that 1) Fine-tuning directly
result in very large variance and suffer from severe over-
ﬁtting, the tunned model performs worse than the source
training model; Imposing SMFR drastically reduces vari-
ance, and the model actually beneﬁts from the the limited
target sample data. 2) While SMFR does not improve much
of the overall performance of proposed components (i.e.,
sps, spm,spl,ins), the variance is dramatically reduced.

5. Conclusion

Table 6. The effect of sharing/not sharing discriminator paramters
between different scales’ split pooling adaptation module.

phenomenon suggests that image patches at different scales
share similar representation characteristics for the image-
level domain shift. They are complementary and combin-
ing them further strengthens the discriminator, resulting in
better domain invariant representation.

Stability gain from SMFR Fine-tuning on small set of
data unavoidably result in serve over-ﬁtting, and instabil-
ity is a common annoying feature of adversarial training.
To evaluate the importance of the proposed source model

In this paper, we explored the possibility of exploiting
only few sample of target domain loosely annotated im-
ages to mitigate the performance drop of object detector
caused by domain shift. Built on Faster R-CNN, by care-
fully designing the adaptation modules and imposing proper
regularization, our framework can robustly adapt a source
trained model to target domain with very few target samples
and still outperforms state-of-art methods accessing full un-
labeled target set.

Acknowledgement
Jiashi Feng was partially supported
by NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-
133 and MOE Tier-II R-263-000-D17-112.

7180

References

[1] Konstantinos Bousmalis, Nathan Silberman, David Dohan,
Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-
level domain adaptation with generative adversarial net-
works.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), volume 1, page 7, 2017.

[2] Pau Panareda Busto and Juergen Gall. Open set domain

adaptation. In ICCV, pages 754–763, 2017.

[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-
arXiv preprint

ing into high quality object detection.
arXiv:1712.00726, 2017.

[4] Hao Chen, Yali Wang, Guoyou Wang, and Yu Qiao. Lstd: A
low-shot transfer detector for object detection. arXiv preprint
arXiv:1803.01529, 2018.

[5] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and
Luc Van Gool. Domain adaptive faster r-cnn for object detec-
tion in the wild. In Computer Vision and Pattern Recognition
(CVPR), 2018.

[6] Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schnei-
der, Trevor Blackwell, Joshua Tobin, Pieter Abbeel, and Wo-
jciech Zaremba. Transfer from simulation to real world
through learning deep inverse dynamics model.
arXiv
preprint arXiv:1610.03518, 2016.

[7] Sailesh Conjeti, Amin Katouzian, Abhijit Guha Roy, Lo¨ıc
Peter, Debdoot Sheet, St´ephane Carlier, Andrew Laine, and
Nassir Navab. Supervised domain adaptation of decision
forests: Transfer of models trained in vitro for in vivo in-
travascular ultrasound tissue characterization. Medical im-
age analysis, 32:1–17, 2016.

[8] Xuanyi Dong, Liang Zheng, Fan Ma, Yi Yang, and
arXiv preprint

Deyu Meng. Few-shot object detection.
Arxiv:1706.08249, pages 1–11, 2017.

[9] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning
of object categories. IEEE transactions on pattern analysis
and machine intelligence, 28(4):594–611, 2006.

[10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
arXiv preprint arXiv:1703.03400, 2017.

[11] Yaroslav Ganin and Victor Lempitsky.
domain adaptation by backpropagation.
arXiv:1409.7495, 2014.

Unsupervised
arXiv preprint

[12] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. The Journal of Machine Learning
Research, 17(1):2096–2030, 2016.

[13] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang,
David Balduzzi, and Wen Li.
Deep reconstruction-
classiﬁcation networks for unsupervised domain adaptation.
In European Conference on Computer Vision, pages 597–
613. Springer, 2016.

[14] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015.

[15] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection

and semantic segmentation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
580–587, 2014.

[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014.

[17] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing

machines. arXiv preprint arXiv:1410.5401, 2014.

[18] Philip Haeusser, Thomas Frerix, Alexander Mordvintsev,
and Daniel Cremers. Associative domain adaptation.
In
International Conference on Computer Vision (ICCV), vol-
ume 2, page 6, 2017.

[19] Bharath Hariharan and Ross B Girshick. Low-shot vi-
sual recognition by shrinking and hallucinating features. In
ICCV, pages 3037–3046, 2017.

[20] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and
Kiyoharu Aizawa. Cross-domain weakly-supervised ob-
ject detection through progressive domain adaptation. arXiv
preprint arXiv:1803.11365, 2018.

[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. arXiv preprint, 2017.

[22] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution.
In
European Conference on Computer Vision, pages 694–711.
Springer, 2016.

[23] Matthew Johnson-Roberson, Charles Barto, Rounak Mehta,
Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan.
Driving in the matrix: Can virtual worlds replace human-
generated annotations for real world tasks? arXiv preprint
arXiv:1610.01983, 2016.

[24] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B
Tenenbaum. Human-level concept learning through proba-
bilistic program induction. Science, 350(6266):1332–1338,
2015.

[25] Brenden M Lake, Ruslan R Salakhutdinov, and Josh Tenen-
baum. One-shot learning by inverting a compositional causal
process. In Advances in neural information processing sys-
tems, pages 2526–2534, 2013.

[26] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M
Hospedales. Deeper, broader and artier domain generaliza-
tion. In Computer Vision (ICCV), 2017 IEEE International
Conference on, pages 5543–5551. IEEE, 2017.

[27] Tsung-Yi Lin, Priyal Goyal, Ross Girshick, Kaiming He, and
Piotr Doll´ar. Focal loss for dense object detection.
IEEE
transactions on pattern analysis and machine intelligence,
2018.

[28] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In European con-
ference on computer vision, pages 21–37. Springer, 2016.

[29] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I
Jordan. Learning transferable features with deep adaptation
networks. arXiv preprint arXiv:1502.02791, 2015.

7181

[45] Jiaolong Xu, Sebastian Ramos, David V´azquez, and Anto-
nio M L´opez. Domain adaptation of deformable part-based
models. IEEE transactions on pattern analysis and machine
intelligence, 36(12):2367–2380, 2014.

[46] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. arXiv preprint, 2017.

[30] Hao Lu, Lei Zhang, Zhiguo Cao, Wei Wei, Ke Xian, Chun-
hua Shen, and Anton van den Hengel. When unsupervised
domain adaptation meets tensor representations. In The IEEE
International Conference on Computer Vision (ICCV), vol-
ume 2, 2017.

[31] Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gian-
franco Doretto. Few-shot adversarial domain adaptation. In
Advances in Neural Information Processing Systems, pages
6670–6680, 2017.

[32] Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gi-
anfranco Doretto. Uniﬁed deep supervised domain adapta-
tion and generalization. In The IEEE International Confer-
ence on Computer Vision (ICCV), volume 2, page 3, 2017.

[33] Weichao Qiu and Alan Yuille. Unrealcv: Connecting com-
In European Conference on

puter vision to unreal engine.
Computer Vision, pages 909–916. Springer, 2016.

[34] Anant Raj, Vinay P Namboodiri, and Tinne Tuytelaars. Sub-
space alignment based domain adaptation for rcnn detector.
arXiv preprint arXiv:1507.05578, 2015.

[35] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 779–788, 2016.

[36] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,

stronger. arXiv preprint, 2017.

[37] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, pages 91–99, 2015.

[38] Andrei A Rusu, Matej Vecerik, Thomas Roth¨orl, Nicolas
Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot
learning from pixels with progressive nets. arXiv preprint
arXiv:1610.04286, 2016.

[39] Avishek Saha, Piyush Rai, Hal Daum´e, Suresh Venkata-
subramanian, and Scott L DuVall. Active supervised do-
main adaptation. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pages 97–
112. Springer, 2011.

[40] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Se-
mantic foggy scene understanding with synthetic data. Inter-
national Journal of Computer Vision, 126(9):973–992, Sep
2018.

[41] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan
Wierstra, and Timothy Lillicrap.
learning
with memory-augmented neural networks. arXiv preprint
arXiv:1605.06065, 2016.

One-shot

[42] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko.
Simultaneous deep transfer across domains and tasks.
In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 4068–4076, 2015.

[43] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
Adversarial discriminative domain adaptation. In Computer
Vision and Pattern Recognition (CVPR), volume 1, page 4,
2017.

[44] udacity. Udacity annotated driving data.

https://
github.com/udacity/self-driving-car, 2018.

7182

