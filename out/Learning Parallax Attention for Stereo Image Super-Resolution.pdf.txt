Learning Parallax Attention for Stereo Image Super-Resolution

Longguang Wang1, Yingqian Wang1, Zhengfa Liang2, Zaiping Lin1, Jungang Yang1, Wei An1, Yulan Guo1∗

1College of Electronic Science and Technology, National University of Defense Technology, China

2National Key Laboratory of Science and Technology on Blind Signal Processing, China

{wanglongguang15,yulan.guo}@nudt.edu.cn

Abstract

Stereo image pairs can be used to improve the perfor-
mance of super-resolution (SR) since additional informa-
tion is provided from a second viewpoint. However, it is
challenging to incorporate this information for SR since
disparities between stereo images vary signiﬁcantly.
In
this paper, we propose a parallax-attention stereo super-
resolution network (PASSRnet) to integrate the information
from a stereo image pair for SR. Speciﬁcally, we intro-
duce a parallax-attention mechanism with a global recep-
tive ﬁeld along the epipolar line to handle different stereo
images with large disparity variations. We also propose a
new and the largest dataset for stereo image SR (namely,
Flickr1024). Extensive experiments demonstrate that the
parallax-attention mechanism can capture correspondence
between stereo images to improve SR performance with a
small computational and memory cost. Comparative results
show that our PASSRnet achieves the state-of-the-art per-
formance on the Middlebury, KITTI 2012 and KITTI 2015
datasets.

1. Introduction

aims

Super-resolution (SR)

to reconstruct high-
resolution (HR) images from their low-resolution (LR)
counterparts. Recovering an HR image from a single shot
is a long-standing problem [1, 2, 3]. Recently, dual cameras
are becoming increasingly popular in mobile phones and
autonomous vehicles.
is already demonstrated that
subpixel shifts contained in LR stereo images can be used
to improve SR performance [4]. However, since disparities
between stereo images can vary signiﬁcantly for different
baselines, focal lengths, depths and resolutions, it is highly
challenging to incorporate stereo correspondence for SR.

It

Traditional multi-image SR methods [7, 8] use patch re-
currence across images to obtain correspondence. However,
these methods cannot exploit sub-pixel correspondence and
their computational cost is high. Recent CNN-based frame-
works [9, 10, 11] incorporate optical ﬂow estimation and SR

Bicubic

SRCNN

LapSRN

StereoSR

Ours

Groundtruth

Figure 1: Visual results achieved by bicubic interpolation,
SRCNN [1], LapSRN [5], StereoSR [6] and our network for
2× SR. These results are achieved on “test image 002” of
the KITTI 2015 dataset.

in uniﬁed networks to solve the video SR problem. How-
ever, these methods cannot be directly applied to stereo im-
age SR since the disparity can be much larger than their
receptive ﬁeld.

Stereo matching has been investigated to obtain corre-
spondence between a stereo image pair [12, 13, 14]. Re-
cent CNN-based methods [15, 16, 17, 18] use 3D or 4D
cost volumes in their networks to model long-range depen-
dency between stereo image pairs. Intuitively, these CNN-
based stereo matching methods can be integrated with SR
to provide accurate correspondence. However, 4D cost vol-
ume based methods [15, 16] suffer from a high computa-
tional and memory burden, which is unbearable for stereo
image SR. Although the efﬁciency of 3D cost volume based
methods [17, 18] is improved, these methods cannot handle
stereo images with large disparity variations since a ﬁxed
maximum disparity is used to construct a cost volume.

Recently, Jeon et al. proposed a stereo SR network
(StereoSR) [6] to provide correspondence cues for SR using
an image stack. Speciﬁcally, the image stack is obtained by
concatenating the left image and the images generated by
shifting the right image with different intervals. A direct
mapping between parallax shifts and an HR image is then
obtained. However, the ﬂexibility of this method for differ-
ent sensors and scenes is limited since the largest allowed
disparity is ﬁxed (i.e., 64 in [6]) in this algorithm.

112250

In this paper, we propose a parallax-attention stereo SR
network (PASSRnet) to incorporate stereo correspondence
for the SR task. Given a stereo image pair, a residual atrous
spatial pyramid pooling (ASPP) module is ﬁrst used to gen-
erate multi-scale features. Then, these features are fed to
a parallax-attention module (PAM) to capture stereo corre-
spondence. For each pixel in the left image, its feature sim-
ilarities with all possible disparities in the right image are
computed to generate an attention map. Consequently, our
PAM can capture global correspondence while maintaining
high ﬂexibility. Afterwards, attention-driven feature aggre-
gation is performed to update the features of the left image.
Finally, these features are used to generate the SR result.
Ablation study is performed on the KITTI 2015 dataset to
test our PASSRnet. Comparative experiments are further
conducted on the Middlebury, KITTI 2012 and KITTI 2015
datasets to demonstrate the superior performance of our net-
work (as shown in Fig. 1).

Our main contributions can be summarized as fol-
lows: 1) We propose a PASSRnet for SR by incorpo-
rating stereo correspondence; 2) We introduce a generic
parallax-attention mechanism with a global receptive ﬁeld
along the epipolar line to handle different stereo images
with large disparity variations.
It is demonstrated that
reliable correspondence can be efﬁciently generated by
the parallax-attention mechanism for the improvement of
SR performance; 3) We propose a new dataset, namely
Flickr1024, for the training of stereo image SR networks.
The Flickr1024 dataset consists of 1024 high-quality stereo
image pairs and covers diverse scenes; 4) Our PASSRnet
achieves the state-of-the-art performance as compared to re-
cent single image SR and stereo image SR methods.

2. Related Work

In this section, we brieﬂy review several major works for

SR and long-range dependency learning.

2.1. Super resolution

Single Image SR Since the seminal work of super-
resolution convolutional neural network (SRCNN) [1],
learning-based methods have dominated the research of sin-
gle image SR. Kim et al. [19] proposed a very deep super-
resolution network (VDSR) with 20 convolutional layers.
Tai et al. [20] developed a deep recursive residual network
(DRRN) to control model parameters. Recently, Zhang et
al. [21] proposed a residual dense network (RDN) to facili-
tate effective feature learning through a contiguous memory
mechanism.
Video SR Liao et al.
[22] introduced the ﬁrst CNN for
video SR. They performed motion compensation to gener-
ate an ensemble of SR-drafts, and then employed a CNN to
reconstruct HR frames from the ensemble. Caballero et al.
[9] proposed an end-to-end video SR framework by incor-

porating a motion compensation module with an SR mod-
ule. Tao et al. [10] integrated an encoder-decoder network
with LSTM to fully use temporal correspondence. This ar-
chitecture further facilitates the extraction of temporal con-
text. Since correspondence between adjacent frames mainly
exists within a local region, video SR methods focus on the
exploitation of local dependency. Therefore, they cannot be
directly applied to stereo image SR due to the non-local and
long-range dependency in stereo images.
Light-ﬁeld Image SR Light-ﬁled imaging can capture ad-
ditional angular information of light at the cost of spatial
resolution. To enhance spatial resolution, Yoon et al. [23]
introduced the ﬁrst light-ﬁeld convolutional neural network
(LFCNN). Yuan et al.
[24] proposed a CNN framework
with a single image SR module and an epipolar plane im-
age enhancement module. To model the correspondence be-
tween images of adjacent sub-apertures, Wang et al. [25]
developed a bidirectional recurrent CNN. Their network
uses an implicit multi-scale feature fusion scheme to ac-
cumulate contextual information for SR. Note that, these
methods are speciﬁcally proposed for light-ﬁeld imaging
with short baselines. Since stereo imaging usually has a
much larger baseline than light-ﬁeld imaging, these meth-
ods are unsuitable for stereo image SR.
Stereo Image SR Bhavsar et al.
[26] argued that image
SR and HR depth estimation are intertwined under stereo
setting. Therefore, they proposed an integrated approach to
jointly estimate the SR image and HR disparity from LR
stereo images. Recently, Jeon et al. [6] proposed a Stere-
oSR to employ parallax prior. Given a stereo image pair, the
right image is shifted with different intervals and concate-
nated with the left image to generate a stereo tensor. The
tensor is then fed to a plain CNN to generate the SR result
by detecting similar patches within the disparity channel.
However, StereoSR cannot handle different stereo images
with large disparity variations since the number of shifted
right images is ﬁxed.

2.2. Long range Dependency Learning

To handle different stereo images with varying dispari-
ties for SR, long-range dependency in stereo images should
be captured. In this section, we review two types of meth-
ods for long-range dependency learning.
Cost Volume Cost volume is widely applied in stereo
matching [15, 16, 17] and optical ﬂow estimation [27, 28].
For stereo matching, several methods [15, 16] use naive
concatenation to construct 4D cost volumes. These meth-
ods concatenate left feature maps with their corresponding
right feature maps across all disparities to obtain a 4D cost
volume (i.e., height×width×disparity×channel). Then, 3D
CNNs are usually used for matching cost learning. How-
ever, learning matching costs from 4D cost volumes suf-
fers from a high computational and memory burden. To

12251

Figure 2: An overview of our PASSRnet.

achieve higher efﬁciency, dot product is used to reduce fea-
ture dimension [17, 18], resulting in 3D cost volumes (i.e.,
height×width×disparity). However, due to the ﬁxed max-
imum disparity in 3D cost volumes, these methods are un-
able to handle different stereo image pairs with large dispar-
ity variations.

Self-attention Mechanisms Attention mechanisms have
been widely used to capture long-range dependency [29,
30]. For self-attention mechanisms [31, 32, 33], a weighted
sum of all positions in spatial and/or temporal domain is
calculated as the response at a position. Through matrix
multiplication, self-attention mechanisms can capture the
interaction between any two positions. Consequently, long-
range dependency can be modeled with a small increase
in computational and memory cost. Self-attention mech-
anisms have been successfully applied in image modeling
[32] and semantic segmentation [33]. Recent non-local net-
works [34, 35] share a similar idea and can be considered
as a generalization of self-attention mechanisms. Note that,
since self-attention mechanisms model dependency across
the whole image, directly applying these mechanisms to
stereo image SR involves unnecessary calculations.

Inspired by self-attention mechanisms, we develop a
parallax-attention mechanism to model global dependency
in stereo images. Compared to cost volumes, our parallax-
attention mechanism is more ﬂexible and efﬁcient. Com-
pared to self-attention mechanisms, our parallax-attention
mechanism takes full use of epipolar constraints to re-
duce search space and improve efﬁciency. Moreover, the
parallax-attention mechanism enforces our network to focus
on the most similar feature rather than collecting all similar
features for correspondence generation. It is demonstrated
that the parallax-attention mechanism can generate reliable

correspondence to improve SR performance (Section 4.3.1).

3. Method

Our PASSRnet takes a stereo image pair as input and
super-resolves the left image. The architecture of our PASS-
Rnet is shown in Fig. 2 and Table 1.

3.1. Residual Atrous Spatial Pyramid Pooling

(ASPP) Module

Feature representation with rich context information is
important for correspondence estimation [16]. Therefore,
both large receptive ﬁled and multi-scale feature learning
are required to obtain a discriminative representation. To
this end, we propose a residual ASPP module to enlarge the
receptive ﬁeld and extract hierarchical features with dense
pixel sampling rate and scales.

As shown in Fig. 2 (a), our residual ASPP module is
constructed by alternately cascading a residual ASPP block
with a residual block. Input features are ﬁrst fed to a resid-
ual ASPP block to generate multi-scale features. These re-
sulting features are then sent to a residual block for feature
fusion. This structure is repeated twice to produce ﬁnal fea-
tures. Within each residual ASPP block (as shown in Fig. 2
(b)), we ﬁrst combine three dilated convolutions (with dila-
tion rates of 1, 4, 8) to form an ASPP group, and then cas-
cade three ASPP groups in a residual manner. Our residual
ASPP block not only enlarges the receptive ﬁeld, but also
enriches the diversity of convolutions, resulting in an en-
semble of convolutions with different receptive regions and
dilation rates. The highly discriminative feature learned by
our residual ASPP module is beneﬁcial to the overall SR
performance, as demonstrated in Sec. 4.3.1.

12252

Bconv0resblock0resASPP1_aresblock1_aresASPP1_bresblock1_bconv0resblock0resASPP1_aresblock1_aresASPP1_bresblock1_bPAMreblock3_areblock3_dSub-pixelconv3_bShared WeightsResidual ASPP ModuleShared WeightsLeft ImageRight ImageSuper-resolved Left Image...Convd=1Convd=4Convd=8Convd=1Convd=4Convd=8Convd=1Convd=4Convd=8elementwise additionconv2_aconv2_bcon2_cABQSRMB→AShared WeightsfusionValid Mask GenerationOresblock2resblock21x1Conv1x1Conv1x1Convmatrix multiplicationsoftmaxdilated conv(b) Residual ASPP Block(c) Parallax-attention Module (PAM)4 residual blocks(a) Overall FrameworkMA→BABTable 1: The detailed architecture of our PASSRnet.
LReLU represents leaky ReLU with a leakage factor of 0.1,
dila stands for dilation rate, ⊗ denotes batch-wise matrix
multiplication, and s is the upscaling factor.

Name
input

conv0

resblock0

"

"

resASPP

1 a

resblock

1 a

resASPP

1 b

resblock

1 b

resblock2

conv2 a
conv2 b
conv2 c

att map

mult

fusion

resblock3

×4

sub-pixel
conv3 b

Input

Output

H ×W ×3

H ×W ×3

H ×W ×3 H ×W ×64

H ×W ×64 H ×W ×64

Setting

3×3
LReLU

h3×3
3×3i

h3×3
3×3i
h3×3
3×3i

1×1

Residual ASPP Module

3×3
LReLU
dila=1

,

,

3×3
LReLU
dila=4
1×1

3×3
LReLU
dila=8

#×3 H ×W ×64 H ×W ×64

3×3
LReLU
dila=1

,

h3×3
3×3i

,

3×3
LReLU
dila=4
1×1

3×3
LReLU
dila=8

H ×W ×64 H ×W ×64

#×3 H ×W ×64 H ×W ×64

H ×W ×64 H ×W ×64

Parallax-Attention Module

1 × 1, reshape

1×1

conv2 a ⊗ conv2 b

att map ⊗ conv2 c

1×1

CNN

h3×3
3×3i

1×1, pixel shufﬂe

3×3

H ×W ×64 H ×W ×64

H ×W ×64 H ×W ×64
H ×W ×64 H ×64×W
H ×W ×64 H ×W ×64
H ×W ×64
H ×64×W
H ×W ×W
H ×W ×64
H ×W ×129 H ×W ×64

H ×W ×64

H ×W ×W

H ×W ×64 H ×W ×64

H ×W ×64 sH ×sW ×64
sH ×sW ×64 sH ×sW ×3

3.2. Parallax attention Module (PAM)

Inspired by self-attention mechanisms [32, 33], we de-
velop PAM to capture global correspondence in stereo im-
ages. Our PAM efﬁciently integrates the information from
a stereo image pair.

Parallax-attention Mechanism The architecture of our
PAM is illustrated in Fig. 2 (c). Given two feature maps
A, B ∈ RH×W ×C , they are fed to a transition residual block
to generate A0 and B0. Then, A0 is fed to a 1 × 1 convo-
lution layer to produce a query feature map Q ∈ RH×W ×C .
Meanwhile, B0 is fed to another 1 × 1 convolution layer
to produce S ∈ RH×W ×C , which is then reshaped to
RH×C×W . Batch-wise matrix multiplication is then per-
formed between Q and S and a softmax layer is applied,
resulting in a parallax attention map MB→A ∈ RH×W ×W .
For more details, please refer to the supplemental mate-
rial. Next, B is fed to a 1 × 1 convolution to generate
R ∈ RH×W ×C , which is further multiplied by MB→A to
produce features O ∈ RH×W ×C . As a weighted sum of
features at all possible disparities, O is then integrated with

ours

groundtruth

Figure 3: Visual comparison between parallax-attention
maps Mright→left generated by our PAM and the groundtruth.
These attention maps (100×100) correspond to the regions
(1×100) marked by blue and pink strokes in the left image.

its corresponding local features A. Since PAM can gradu-
ally focus on the features at accurate disparities using fea-
ture similarities, correspondence can then be captured. Note
that, once MB→A is ready, A and B are exchanged to pro-
duce MA→B for valid mask generation (as described below).
Finally, stacked features and a valid mask are fed to a 1 × 1
convolution layer for feature fusion.

Different from self-attention mechanisms [32, 33], our
parallax-attention mechanism enforces our network to fo-
cus on the most similar feature along the epipolar line rather
than collecting all similar features, resulting in sparse atten-
tion maps. A comparison between parallax-attention maps
generated by our PAM and the groundtruth is shown in Fig.
3. Note that, Mright→left(i, j, k) represents the contribution
of position (i, k) in the right image to position (i, j) in the
left image. Consequently, the patterns in an attention map
can reﬂect the correspondence between stereo pairs and also
encode disparity information. For more details, please refer
to the supplemental material. It can be observed that our
PAM produces patterns similar to the groundtruth, which in-
dicates that reliable stereo correspondence can be captured
by our PAM. It should be noted that our PASSRnet can be
considered as a multi-task network to learn both stereo cor-
respondence and SR. However, using shared features for
different tasks usually suffers from training conﬂict [36].
Therefore, a transition block is introduced in our PAM to
alleviate this problem. The effectiveness of the transition
block is demonstrated in Sec. 4.3.1.
Left-right Consistency & Cycle Consistency Given deep
features extracted from an LR stereo image pair (IL
left
and IL
two parallax-attention maps (Mleft→right and
Mright→left) can be generated by PAM. Ideally, the following
left-right consistency can be obtained if our PAM captures
accurate correspondence:

right),

( IL

left = Mright→left ⊗ IL
IL
right = Mleft→right ⊗ IL

left

right

,

(1)

where ⊗ denotes batch-wise matrix multiplication. Based

12253

Figure 4: Visualization of valid masks. Two left images and
their occluded regions (i.e., yellow regions) are illustrated.

on Eq. (1), we can further derive a cycle consistency:

( IL

left = Mleft→right→left ⊗ IL
right = Mright→left→right ⊗ IL
IL

left

right

,

the

where
Mright→left→right can be calculated as:

cycle-attention maps Mleft→right→left

(2)

and

( Mleft→right→left = Mright→left ⊗ Mleft→right

Mright→left→right = Mleft→right ⊗ Mright→left

.

(3)

Here, we introduce left-right consistency and cycle consis-
tency to regularize the training of our PAM for the genera-
tion of reliable and consistent correspondence.
Valid Masks Since left-right consistency and cycle consis-
tency do not hold for occluded regions, we use an occlusion
detection method to generate valid masks. We only enforce
consistency on valid regions. In the parallax-attention map
generated by our PAM (e.g., Mleft→right), it is observed that
pixels in occluded regions are usually assigned with small
weights. Therefore, a valid mask Vleft→right ∈ RH×W can be
obtained by:

0,

Vleft→right(i, j) =(cid:26) 1,

ifPk∈[1, W ] Mleft→right(i, k, j) > τ ,

otherwise,
(4)
where τ is a threshold (empirically set to 0.1) and W is the
width of stereo images. Two examples of valid masks are
shown in Fig. 4. According to the parallax-attention mech-
anism, Mleft→right(i, k, j) represents the contribution of po-
sition (i, j) in the left image to position (i, k) in the right
image. Since occluded pixels in the left image cannot ﬁnd
their correspondences in the right image, their values (i.e.,

Pk∈[1, W ] Mleft→right(i, k, j)) are usually low. Thus, we con-

sider these pixels as occluded ones. In practice, we use sev-
eral morphological operations to handle isolated pixels and
holes in valid masks. Note that, occluded regions in the left
image cannot obtain additional information from the right
image. Therefore, valid mask Vleft→right is further used to
guide feature fusion, as shown in Fig. 2 (c).

3.3. Losses

We design four losses for the training of our PASSRnet.
Other than an SR loss, we introduce three additional losses,

including photometric loss, smoothness loss and cycle loss,
to help the network to fully exploit the correspondence be-
tween stereo images. The overall loss function is formu-
lated as:

L = LSR + λ(Lphotometric + Lsmooth + Lcycle),

(5)

where λ is empirically set to 0.005. The performance of our
network with different losses will be analyzed in Sec. 4.3.2.
SR Loss The mean square error (MSE) loss is used as the
SR loss:

,

(6)

left − IH

LSR =(cid:13)(cid:13)ISR

left

2

2

left(cid:13)(cid:13)

represent

left and IH

the SR result and HR

where ISR
groundtruth of the left image, respectively.
Photometric Loss Since collecting a large stereo dataset
with densely labeled groundtruth disparities is highly chal-
lenging, we train our PAM in an unsupervised manner. Note
that, if the groundtruth disparities are available, we can gen-
erate the groundtruth attention maps accordingly (see the
supplemental material for more details) and train our PAM
in a supervised manner. Following [37], we introduce a
photometric loss using the mean absolute error (MAE) loss.
Note that, since the left-right consistency deﬁned in Eq. (1)
only holds in non-occluded regions, we introduce a photo-
metric loss as:

Lphotometric = Xp∈Vleft→right(cid:13)(cid:13)IL
+ Xp∈Vright→left(cid:13)(cid:13)IL

left(p)−(Mright→left ⊗ IL

right(p)−(Mleft→right ⊗ IL

right)(p)(cid:13)(cid:13)1
left)(p)(cid:13)(cid:13)1

(7)

,

where p represents a pixel with a valid mask value.
Smoothness Loss To generate accurate and consistent at-
tention in textureless regions, a smoothness loss is deﬁned
on the attention maps Mleft→right and Mright→left:

Lsmooth =XM Xi,j,k

( kM(i, j, k)−M(i+1, j, k)k1+

kM(i, j, k) −M(i, j +1, k+1)k1),

(8)
where M ∈ {Mleft→right, Mright→left}. The ﬁrst and second
terms in Eq. (8) are used to achieve vertical and horizontal
attention consistency, respectively.
Cycle Loss In addition to photometric loss and smoothness
loss, we further introduce a cycle loss to achieve cycle con-
sistency. Since Mleft→right→left and Mright→left→right in Eq.
(2) can be considered as identity matrices, we design a cy-
cle loss as:

Lcycle = Xp∈Vleft→right
Xp∈Vright→left

kMleft→right→left(p) − I(p)k1 +

kMright→left→right(p) − I(p)k1 ,

(9)

where I ∈ RH×W ×W is a stack of H identity matrices.

12254

Table 2: Comparative results achieved on the KITTI 2015 dataset by PASSRnet with different settings for 4× SR.

Model
PASSRnet with single input
PASSRnet with replicated inputs
PASSRnet without residual manner
PASSRnet without atrous convolution
PASSRnet without PAM
PASSRnet without transition residual block
PASSRnet

Input
Left

Left-Left
Left-Right
Left-Right
Left-Right
Left-Right
Left-Right

PSNR
25.27
25.29
25.40
25.38
25.28
25.36
25.43

SSIM Params.
0.770
0.771
0.774
0.773
0.771
0.773
0.776

Time
1.32M 114ms
1.42M 176ms
1.42M 176ms
1.42M 176ms
1.32M 135ms
1.34M 160ms
1.42M 176ms

4. Experimental Results

not provide further consistent improvement.

In this section, we ﬁrst introduce the datasets and imple-
mentation details, and then conduct ablation experiments to
test our network. We further compare our network to recent
single image SR and stereo image SR methods.

4.1. Datasets

For training, we followed [6] and downsampled 60 Mid-
dlebury [38] images by a factor of 2 to generate HR images.
We further collected 1024 stereo images from Flickr to con-
struct a new Flickr1024 dataset. This dataset was used as the
augmented training data for our PASSRnet. Please see the
supplemental material for more details about the Flickr1024
dataset. For test, we used 5 images from the Middlebury
dataset, 20 images from the KITTI 2012 dataset [39] and
20 images from the KITTI 2015 dataset [40] as benchmark
datasets. We further collected 10 close-shot stereo images
(with disparities larger than 200) from Flickr to test the ﬂex-
ibility of our network to large disparity variations. For vali-
dation, we selected another 20 images from the KITTI 2012
dataset.

4.2. Implementation Details

During the training phase, we ﬁrst downsampled HR im-
ages using bicubic interpolation to generate LR images, and
then cropped 30 × 90 patches with a stride of 20 from these
LR images. Meanwhile, their corresponding patches in HR
images were also cropped. The horizontal patch size was
increased to 90 to cover most disparities (∼96%) in our
training dataset. These patches were randomly ﬂipped hori-
zontally and vertically for data augmentation. Note that, ro-
tation was not performed to maintain epipolar constraints.
We used peak signal-to-noise ratio (PSNR) and structural
similarity index (SSIM) to test SR performance. Similar to
[6], we cropped borders to achieve fair comparison.

Our PASSRnet was implemented in Pytorch on a PC
with an Nvidia GTX 1080Ti GPU. All models were opti-
mized using the Adam method [41] with β1 = 0.9, β2 =
0.999 and a batch size of 32. The initial learning rate was
set to 2×10−4 and reduced to half after every 30 epochs. The
training was stopped after 80 epochs since more epochs do

4.3. Ablation Study

In this section, we present ablation experiments to justify
our design choices, including the network architecture and
the losses.

4.3.1 Network Architecture

Single Input vs. Stereo Input Compared to single images,
stereo image pairs provide additional information observed
from a different viewpoint. To demonstrate the effective-
ness of stereo information for SR performance improve-
ment, we removed PAM from our PASSRnet and retrained
the network with single images (i.e., the left images). For
comparison, we also used pairs of replicated left images as
the input to the original PASSRnet. Results achieved on the
KITTI 2015 dataset are listed in Table 2.

Compared to the original PASSRnet, the network trained
with single images suffers a decrease of 0.16 dB (25.43 to
25.27) in PSNR. Further, if pairs of replicated left images
are fed to the original PASSRnet, the PSNR value is de-
creased to 25.29 dB. Without extra information introduced
by stereo images, our PASSRnet with replicated images
achieves comparable performance to the network trained
with single images. This clearly demonstrates that stereo
images can be used to improve the performance of PASSR-
net.
Residual ASPP Module Residual ASPP module is used in
our network to extract multi-scale features. To demonstrate
the effectiveness of residual ASPP, two variants were intro-
duced. First, to test the effectiveness of residual connec-
tions, we removed them to obtain a cascading ASPP mod-
ule. Then, to test the effectiveness of atrous convolutions,
we replaced them with ordinary convolutions.

From the comparative results shown in Table 2, we can
see that SR performance beneﬁts from both residual con-
nections and atrous convolutions.
If residual connections
are removed, the PSNR value is decreased from 25.43 dB to
25.40 dB. That is because, residual connections enable our
residual ASPP module to extract features at more scales,

12255

resulting in more robust feature representations. Further-
more, if atrous convolutions are replaced by ordinary ones,
the PSNR value is decreased from 25.43 dB to 25.38 dB.
That is because, large receptive ﬁeld of atrous convolutions
facilitates our PASSRnet to employ context information in
a large area. Therefore, more accurate correspondence can
be obtained to improve SR performance.
Parallax-attention Module PAM is introduced to integrate
the information from stereo images. To demonstrate its ef-
fectiveness, we introduced a variant by removing PAM and
directly stacking the output features of the residual ASPP
module.
It can be observed from Table 2 that the PSNR
value is decreased from 25.43 dB to 25.28 dB if PAM is
removed. That is because, long spatial distance between lo-
cal features in the left image and their dependency in the
right image hinders plain CNNs to integrate these features
effectively.
Transition Block in PAM Transition block in PAM is in-
troduced to alleviate the training conﬂict in shared layers.
To demonstrate the effectiveness of transition block, we re-
moved it from our PAM and retrained the network. It can
be observed from Table 2 that the PSNR value is decreased
from 25.43 dB to 25.36 dB if the transition block is re-
moved. That is because, the transition block enhances task-
speciﬁc feature learning in our PAM and alleviates train-
ing conﬂict in shared layers. Therefore, more representative
features can be learned in shared layers.
PAM vs. Cost Volume Cost volume and 3D convolu-
tions are commonly used to obtain stereo correspondence
[15, 16]. To demonstrate the efﬁciency of our PAM in stereo
correspondence generation, we replaced PAM with a 4D
cost volume and two 3D convolutional layers (3 × 3 × 3).
It can be observed from Table 3 that our PAM has less than
half of the parameters in the cost volume formation. More-
over, our PAM achieves superior computational efﬁciency,
with FLOPs being reduced by over 150 times. With PAM,
our PASSRnet achieves better SR performance (i.e., PSNR
value is increased from 25.23 dB to 25.43 dB) and efﬁ-
ciency (i.e., running time is decreased by 1.5 times). That
is because, two 3D convolutional layers are insufﬁcient to
capture long-range correspondence within the cost volume.
However, adding more layers will lead to a signiﬁcant in-
crease of computational cost.

4.3.2 Losses

To test the effectiveness of our losses, we retrained PASSR-
net using different losses.

It can be observed from Table 4 that the PSNR value of
our PASSRnet is decreased from 25.43 to 25.35 if PASS-
Rnet is trained with only SR loss. That is because, with
only this loss, our PAM learns to collect all similar fea-
tures along the epipolar line and cannot focus on the most

Table 3: Comparison between our PAM and the cost volume
formation for 4× SR. FLOPs are calculated on 128×128×64
input features, while Time/PSNR/SSIM values are achieved
on the KITTI 2015 dataset.

Model
PAM

Cost Volume

Params. FLOPs Time PSNR SSIM
1× 25.43 0.776
151× 1.5× 25.23 0.768

94K
221K

1×

Table 4: Comparative results achieved on KITTI 2015 by
our PASSRnet trained with different losses for 4× SR.

Model LSR Lphotometric Lsmooth Lcycle PSNR SSIM
25.35 0.771
25.38 0.773
25.40 0.774
25.43 0.776

PASSRnet X
PASSRnet X
PASSRnet X
PASSRnet X

X

X

X

X

X

X

similar feature to provide accurate correspondence. Fur-
ther, the performance is gradually improved if photomet-
ric loss, smoothness loss and cycle loss are added. That is
because, these losses encourage our PAM to generate reli-
able and consistent correspondence. Overall, our PASSRnet
achieves the best performance (i.e., PSNR=25.43 dB and
SSIM=0.776) when it is trained with all these losses.

4.4. Comparison to State of the arts

We compared our PASSRnet to a number of CNN-based
SR methods on three benchmark datasets. Recent single
image SR methods under comparison include SRCNN [1],
VDSR [19], DRCN [42], LapSRN [5] and DRRN [20]. We
also compared our PASSRnet to the latest stereo image SR
method StereoSR [6]. The codes provided by the authors
of these methods were used to conduct experiments. Note
that, similar to [6, 43], EDSR [44], RDN [21] and D-DBPN
[45] are not included in our comparison since their model
sizes are larger than our PASSRnet by at least 8 times.
Quantitative Results The quantitative results are shown in
Table 5.
It can be observed that our PASSRnet achieves
the best performance on the Middlebury, KITTI 2012 and
KITTI 2015 datasets. Speciﬁcally, compared to single im-
age SR methods, our PASSRnet outperforms the second
best approach (i.e., DRRN) by 1.04 dB in terms of PSNR
on the Middlebury dataset for 2× SR. Moreover, the PSNR
value achieved by our network is higher than that of Stere-
oSR by 1.00 dB. That is because, more reliable correspon-
dence can be captured by our parallax-attention mechanism.
Qualitative Results Figure 5 illustrates the qualitative re-
sults achieved on two scenarios. It can be observed from
zoom-in regions that single image SR methods cannot re-

12256

Table 5: Comparative PSNR/SSIM values achieved on the Middlebury, KITTI 2012 and KITTI 2015 datasets. Results
marked with * are directly copied from the corresponding paper. Note that, only 2× SR results of StereoSR are presented on
the KITTI 2012 and KITTI 2015 datasets since a 4× SR model is unavailable.

Dataset

Scale

Middlebury
(5 images)
KITTI 2012
(20 images)
KITTI 2015
(20 images)

×2
×4
×2
×4
×2
×4

Single Image SR

SRCNN [1] VDSR [19] DRCN [42] LapSRN [5] DRRN [20]
32.91/0.945
32.05/0.935
27.93/0.855
27.46/0.843
30.16/0.908
29.75/0.901
25.53/0.764
25.94/0.773
29.00/0.906
28.77/0.901
24.68/0.744
25.05/0.756

32.75/0.940
27.98/0.861
30.10/0.905
25.96/0.779
28.97/0.903
25.03/0.760

32.82/0.941
27.93/0.856
30.19/0.906
25.92/0.777
29.04/0.904
25.04/0.759

32.66/0.941
27.89/0.853
30.17/0.906
25.93/0.778
28.99/0.904
25.01/0.760

Stereo Image SR

StereoSR [6]
33.05/0.955*
26.80/0.850*
30.13/0.908

-

29.09/0.909

-

Ours

34.05/0.960
28.63/0.871
30.65/0.916
26.26/0.790
29.78/0.919
25.43/0.776

Bicubic

29.07/0.904

SRCNN

30.46/0.921

VDSR

DRCN

31.00/0.927

31.04/0.927

LapSRN

30.91/0.926

StereoSR

31.22/0.929

Ours

31.71/0.936

Groundtruth

Bicubic

28.28/0.875

SRCNN

29.28/0.894

VDSR

DRCN

29.54/0.898

29.66/0.898

LapSRN

29.52/0.897

StereoSR

29.73/0.905

Ours

30.50/0.913

Groundtruth

Figure 5: Visual comparison for 2× SR. These results are achieved on “test image 013” of the KITTI 2012 dataset and
“test image 019” of the KITTI 2015 dataset.

Table 6: Comparison between our PASSRnet and StereoSR
[6] on stereo images with different resolutions for 2× SR.

StereoSR [6]

Ours

Resolution

PSNR FLOPs
High (500 × 500)
39.27
Middle (100 × 100) 34.21
Low (20 × 20)
29.48

FLOPs
1× 41.45(↑ 2.18) 0.57×
1× 35.04(↑ 0.83) 0.58×
1× 29.88(↑ 0.40) 0.36×

PSNR

cover reliable details. In contrast, our PASSRnet uses stereo
correspondence to produce ﬁner details with fewer artifacts,
such as the railings and stripe in Fig. 5. Compared to Stere-
oSR, our PASSRnet explicitly captures stereo correspon-
dence for SR. Consequently, superior visual performance
is achieved.
Flexibility We further tested the ﬂexibility of our PASSR-
net and StereoSR [6] with respect to large disparity varia-
tions. Results achieved on images with different resolutions
are shown in Table 6. More results under different base-
lines and depths are available in the supplemental material.
It can be observed that our PASSRnet is signiﬁcantly bet-

ter than StereoSR in terms of efﬁciency (i.e., FLOPs) on
low resolution images. Meanwhile, our PASSRnet outper-
forms StereoSR by a large margin in terms of PSNR on high
resolution images. That is because, StereoSR needs to per-
form padding for images with horizontal resolution lower
than 64 pixels, which involves unnecessary calculations.
For high resolution images, the ﬁxed maximum disparity
hinders StereoSR to capture longer-range correspondence.
Therefore, the SR performance of StereoSR is limited.

5. Conclusion

In this paper, we propose a parallax-attention stereo
super-resolution network (PASSRnet) to incorporate stereo
correspondence for the SR task. Our PASSRnet introduces
a parallax-attention mechanism with global receptive ﬁeld
to handle different stereo images with large disparity vari-
ations. We also introduce a new and the largest dataset for
stereo image SR. It is demonstrated that our PASSRnet can
effectively capture stereo correspondence for the improve-
ment of SR performance. Comparison to recent single im-
age SR and stereo image SR methods has shown that our
network achieves the state-of-the-art performance.

12257

References

[1] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Learning a deep convolutional network for image
super-resolution. In ECCV, pages 184–199, 2014.

[2] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,
Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In
CVPR, pages 1874–1883, 2016.

[3] Zheng Hui, Xiumei Wang, and Xinbo Gao. Fast and accu-
rate single image super-resolution via information distilla-
tion network. In CVPR, 2018.

[4] Sung Cheol Park, Min Kyu Park, and Moon Gi Kang. Super-
resolution image reconstruction: a technical overview. IEEE
signal processing magazine, 20(3):21–36, 2003.

[5] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-
Hsuan Yang. Deep laplacian pyramid networks for fast and
accurate super-resolution. In CVPR, pages 5835–5843, 2017.

[6] Daniel S. Jeon, Seung-Hwan Baek, Inchang Choi, and
Min H. Kim. Enhancing the spatial resolution of stereo im-
ages using a parallax prior. In CVPR, 2018.

[7] Matan Protter, Michael Elad, Hiroyuki Takeda, and Pey-
man Milanfar. Generalizing the nonlocal-means to super-
resolution reconstruction.
IEEE Trans. Image Processing,
18(1):36–51, 2009.

[8] Hiroyuki Takeda, Peyman Milanfar, Matan Protter, and
Super-resolution without explicit sub-
IEEE Trans. Image Processing,

Michael Elad.
pixel motion estimation.
18(9):1958–1975, 2009.

[9] Jose Caballero, Christian Ledig, Andrew P. Aitken, Alejan-
dro Acosta, Johannes Totz, Zehan Wang, and Wenzhe Shi.
Real-time video super-resolution with spatio-temporal net-
works and motion compensation.
In CVPR, pages 2848–
2857, 2017.

[10] Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, and Jiaya
Jia. Detail-revealing deep video super-resolution. In ICCV,
pages 4482–4490, 2017.

[11] Longguang Wang, Yulan Guo, Zaiping Lin, Xinpu Deng, and
Wei An. Learning for video super-resolution through HR
optical ﬂow estimation. In ACCV, 2018.

[12] Stephen T. Barnard and Martin A. Fischler. Computational

stereo. ACM Comput. Surv., 14(4):553–572, 1982.

[13] Dan. Scharstein and R. Szeliski. A taxonomy and evalua-
tion of dense two-frame stereo correspondence algorithms.
International Journal of Computer Vision, 47(1-3), 2002.

[14] Wenjie Luo, Alexander G. Schwing, and Raquel Urtasun. Ef-
ﬁcient deep learning for stereo matching. In CVPR, pages
5695–5703, 2016.

[15] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, and
Peter Henry. End-to-end learning of geometry and context
for deep stereo regression. In ICCV, pages 66–75, 2017.

[16] Jia-Ren Chang and Yong-Sheng Chen.

Pyramid stereo

matching network. In CVPR, 2018.

[17] Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Linbo
Qiao, Wei Chen, Li Zhou, and Jianfeng Zhang. Learning
for disparity estimation through feature constancy. In CVPR,
2018.

[18] Zequn Jie, Pengfei Wang, Yonggen Ling, Bo Zhao, Yunchao
Wei, Jiashi Feng, and Wei Liu. Left-right comparative recur-
rent model for stereo matching. In CVPR, 2018.

[19] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In CVPR, pages 1646–1654, 2016.

[20] Ying Tai, Jian Yang, and Xiaoming Liu.

resolution via deep recursive residual network.
pages 2790–2798, 2017.

Image super-
In CVPR,

[21] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image super-resolution.
In CVPR, 2018.

[22] Renjie Liao, Xin Tao, Ruiyu Li, Ziyang Ma, and Jiaya Jia.
Video super-resolution via deep draft-ensemble learning. In
ICCV, pages 531–539, 2015.

[23] Youngjin Yoon, Hae-Gon Jeon, Donggeun Yoo, Joon-Young
Lee, and In So Kweon. Learning a deep convolutional net-
work for light-ﬁeld image super-resolution. In ICCV Work-
shops, pages 57–65, 2015.

[24] Yan Yuan, Ziqi Cao, and Lijuan Su. Light-ﬁeld image super-
resolution using a combined deep CNN based on EPI. IEEE
Signal Process. Lett., 25(9):1359–1363, 2018.

[25] Yunlong Wang, Fei Liu, Kunbo Zhang, Guangqi Hou,
Zhenan Sun, and Tieniu Tan. Lfnet: A novel bidirec-
tional recurrent convolutional neural network for light-ﬁeld
image super-resolution.
IEEE Trans. Image Processing,
27(9):4274–4286, 2018.

[26] Arnav V. Bhavsar and A. N. Rajagopalan. Resolution en-
hancement in multi-image stereo. IEEE Trans. Pattern Anal.
Mach. Intell., 32(9):1721–1728, 2010.

[27] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
PWC-Net: CNNs for optical ﬂow using pyramid, warping,
and cost volume. In CVPR, 2017.

[28] Jia Xu, Rene Ranftl, and Vladlen Koltun. Accurate opti-
cal ﬂow via direct cost volume processing. In CVPR, pages
5807–5815, 2017.

[29] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez
Rezende, and Daan Wierstra. DRAW: A recurrent neural
network for image generation. In ICML, volume 37, pages
1462–1471, 2015.

[30] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C. Courville, Ruslan Salakhutdinov, Richard S.
Zemel, and Yoshua Bengio. Show, attend and tell: Neural
image caption generation with visual attention.
In ICML,
volume 37, pages 2048–2057, 2015.

[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NIPS, pages 6000–
6010, 2017.

12258

[32] Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and
Augustus Odena. Self-attention generative adversarial net-
works. In NIPS, 2018.

[33] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing
Lu. Dual attention network for scene segmentation. arXiv
preprint arXiv:1809.02983, 2018.

[34] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-

ing He. Non-local neural networks. In CVPR, 2018.

[35] Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and
Thomas S. Huang. Non-local recurrent network for image
restoration. In NIPS, 2018.

[36] Ozan Sener and Vladlen Koltun. Multi-task learning as

multi-objective optimization. In NIPS, 2018.

[37] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J. Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. In CVPR, pages 6602–6611, 2017.

[38] Daniel Scharstein, Heiko Hirschm¨uller, York Kitajima,
Greg Krathwohl, Nera Nesic, Xi Wang, and Porter West-
ling. High-resolution stereo datasets with subpixel-accurate
ground truth. In GCPR, volume 8753, pages 31–42, 2014.

[39] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the KITTI vision benchmark
suite. In CVPR, pages 3354–3361, 2012.

[40] Moritz Menze and Andreas Geiger. Object scene ﬂow for

autonomous vehicles. In CVPR, pages 3061–3070, 2015.

[41] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015.

[42] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-
recursive convolutional network for image super-resolution.
In CVPR, pages 1637–1645, 2016.

[43] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Fast,
accurate, and lightweight super-resolution with cascading
residual network. In ECCV, 2018.

[44] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for single
image super-resolution. In CVPR Workshops, 2017.

[45] Muhammad Haris, Greg Shakhnarovich, and Norimichi
Ukita. Deep back-projection networks for super-resolution.
In CVPR, 2018.

12259

