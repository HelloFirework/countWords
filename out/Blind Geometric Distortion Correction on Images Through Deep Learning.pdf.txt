Blind Geometric Distortion Correction on Images Through Deep Learning

Xiaoyu Li1

Bo Zhang1

Pedro V. Sander1

Jing Liao2

1The Hong Kong University of Science and Technology

2City University of Hong Kong

Abstract

We propose the ﬁrst general framework to automatically
correct different types of geometric distortion in a single
input image. Our proposed method employs convolutional
neural networks (CNNs) trained by using a large synthetic
distortion dataset to predict the displacement ﬁeld between
distorted images and corrected images. A model ﬁtting
method uses the CNN output to estimate the distortion pa-
rameters, achieving a more accurate prediction. The ﬁnal
corrected image is generated based on the predicted ﬂow
using an efﬁcient, high-quality resampling method. Experi-
mental results demonstrate that our algorithm outperforms
traditional correction methods, and allows for interesting
applications such as distortion transfer, distortion exagger-
ation, and co-occurring distortion correction.

1. Introduction

Geometric distortion is a common problem in digital im-
agery and occurs in a wide range of applications. It can be
caused by the acquisition system (e.g., optical lens, imaging
sensor), imaging environment (e.g., motions of the platform
or target, viewing geometry) and image processing opera-
tions (e.g., image warping). For example, camera lenses
often suffer from optical aberrations, causing barrel distor-
tion (B), common in wide angle lenses, where the image
magniﬁcation decreases with distance from the optical axis,
and pincushion distortion (Pi), where it increases. While
lens distortions are intrinsic to the camera, extrinsic geo-
metric distortions like rotation (R), shear (S) and perspec-
tive distortion (P) may also arise from the improper pose
or the movement of cameras. Furthermore, a wide number
of distortion effects, such as wave distortion (W), can be
generated by image processing tools. We aim to design an
algorithm that can automatically correct images with these
distortions and can be generalized to a wide range of distor-
tions easily (see Figure 2).

Geometric distortion correction is highly desired in both
photography and computer vision applications. For exam-

Figure 1. Our proposed learning-based method can blindly correct
images with different types of geometric distortion (ﬁrst row) pro-
viding high-quality results (second row).

ple, lens distortion violates the pin-hole camera model as-
sumption which many algorithms rely on. Second, remote
sensing images usually contain geometric distortions that
cannot be used with maps directly before correction [34].
Third, skew detection and correction is an important pre-
processing step in document analysis and has a direct ef-
fect on the reliability and efﬁciency of the segmentation
and feature extraction stages [1]. Finally, photos often con-
tain slanted buildings, walls, and horizon lines due to im-
proper camera rotation. Our visual system expects man-
made structures to be straight, and horizon lines to be hori-
zontal [24].

Completely blind geometric distortion correction is a
challenging problem, which is under-constrained given that
the input is only a single distorted image. Therefore, many
correction methods have been proposed by using multiple
images or additional information. Multiple views meth-
ods [4, 15, 23] for radial lens distortion use point correspon-
dences of two or more images. These methods can achieve
impressive results. However, they cannot be applied when
multiple images under camera motion are unavailable.

To address these limitations, distortion correction from
a single image has also been explored. Methods for radial
lens distortion based on the plumb line approach [39, 5, 33]
assume that straight lines are projected to circular arcs in
the image plane caused by radial lens distortion. There-
fore, accurate line detection is a very important aspect for
the robustness and ﬂexibility of these methods. Correction
methods for other distortions [13, 24, 7, 31] also rely on

4855

the detection of special low-level features such as vanishing
points, repeated textures, and co-planar circles. But these
special low-level features are not always frequent enough
for distortion estimation in some images, which greatly re-
strict the versatility of the methods. Moreover, all of the
methods focus on a speciﬁc distortion. To our knowledge,
there is no general framework which can address different
types of geometric distortion from a single image.

In this paper, we propose a learning-based method to
achieve this goal. We use the displacement ﬁeld between
distorted images and corrected images to represent a wide
range of distortions. The correction problem is then con-
verted to the pixel-wise prediction of this displacement
ﬁeld, or ﬂow, from a single image. Recently, CNNs have
become a powerful method in many ﬁelds of computer vi-
sion and outperform many traditional methods, which moti-
vated us to use a similar network structure for training. The
predicted ﬂow is then further improved by our model ﬁtting
methods which estimate the distortion parameters. Lastly,
we use a modiﬁed resampling method to generate the out-
put undistorted image from the predicted ﬂow.

Overall, our learning-based method does not make
strong assumptions on the input images while generating
high-quality results with few visible artifacts as shown in
Figure 1. Our main contribution is to propose the ﬁrst
learning-based methods to correct a wide range of geomet-
ric distortions blindly. More speciﬁcally, we propose:

1. A single-model network, which implicitly learns the

distortion parameters given the distortion type.

2. A multi-model network, which performs type classi-
ﬁcation jointly with ﬂow regression without knowing
the distortion type, followed by an optional model ﬁt-
ting method to further improve the accuracy of the es-
timation.

3. A new resampling method based on an iterative search

with faster convergence.

4. Extended applications that can directly use this frame-
work, such as distortion transfer, distortion exaggera-
tion, and co-occurring distortion correction.

2. Related Work

Geometric distortion correction. For camera lens dis-
tortions, pre-calibration techniques have been proposed for
correction with known distortion parameters [32, 10, 35, 18,
44]. However, they are unsuitable for zoom lenses, and the
calibration process is usually tedious. On the other hand,
auto-calibration methods do not require special calibration
patterns and automatically extracts camera parameters from
multi-view images [12, 15, 23, 28, 19]. But for many appli-
cation scenarios, multiple images with different views are

B

Pi

R

S

P

W

Figure 2. Our system is trained to correct barrel distortion (B),
pincushion (P i), rotation (R), shear (S), perspective (P) and wave
distortion (W).

unavailable. To address these limitations, automatic dis-
tortion correction from a single image has gained more re-
search interest recently. Fitzgibbon [12] proposes a division
model to approximate the radial distortion curve with higher
accuracy and fewer parameters. Wang et al. [39] studied
the geometry property of straight lines under the division
model and proposed to estimate the distortion parameters
through arc ﬁtting. Since plumb line methods rely on ro-
bust line detection, Aleman-Flores et al. [2] used an im-
proved Hough Transform to improve the robustness while
Bukhari and Dailey [5] proposed a sampling method that
robustly chooses the circular arcs and determines distortion
parameters that are insensitive to outliers.

For other distortions, such as rotation and perspective,
most of the image correction methods rely on the detection
of low-level features such as vanishing point, repeated tex-
tures, and co-planar circles [13, 24, 7, 31]. Recently, Zhai et
al. [43] proposed to use deep convolutional neural networks
to estimate the horizon line by aggregating the global image
context with the clue of the vanishing point. Workman et
al. [40] goes further and directly estimates the horizon line
in the single image. Unlike these specialized methods, our
approach is generalizable for multi-type distortion correc-
tion using a single network.

Deformation estimation. There has been recent work on
automatic detection of geometric deformation or variations
in a single image. Dekel et al. [9] use a non-local variations
algorithm to automatically detect and correct small defor-
mations between repeating structures from a single image.
Wadhwa et al. [37] ﬁt parametric models to compute the ge-
ometric deviations and exaggerate the departure from ideal
geometries. Estimating deformations has also been studied
in the context of texture images [22, 16, 27]. None of these
techniques are learning-based and are mostly for specialized
domains.

Neural networks for pixel-wise prediction. Recently,
convolutional neural networks have been used in many
pixel-wise prediction tasks from a single image, such as se-

4856

mantic segmentation [25], depth estimation [11] and mo-
tion prediction [38]. One of the main problems for dense
prediction is how to combine multi-scale contextual rea-
soning with the full-resolution output. Long et al. [25]
proposed fully convolutional networks which popularized
CNNs for dense predictions without fully connected layers.
Some methods focus on dilated or atrous convolution [42, 8]
which supports exponential expansion the receptive ﬁeld
and systematically aggregate multi-scale contextual infor-
mation without losing resolution. Another strategy is to
use the encoder-decoder architecture [26, 3, 29]. The en-
coder gradually reduces the spatial dimension to increase
the receptive ﬁeld of the neuron, while the decoder maps the
low-resolution feature maps to full input resolution maps.
Noh et al. [26] developed deconvolution and unpooling lay-
ers for the decoder part. Badrinarayanan et al. [3] used
pooling indices to connect the encoder and the correspond-
ing decoder, making the architecture more memory efﬁ-
cient. Another popular network is U-net [29], which uses
the skip connections to combine the contracting paths with
the upsampled feature maps. Our networks use an encoder-
decoder architecture with residual connection design and
achieve more accurate results.

3. Network Architectures

Geometrically distorted images usually exhibit unnatu-
ral structures that can serve as clues for distortion correc-
tion. As a result, we presume that the network can po-
tentially recognize the geometric distortions by extracting
the features from the input image. We, therefore, propose
a network to learn the mapping from the image domain I
to the ﬂow domain F . The ﬂow is the 2D vector ﬁeld that
speciﬁes where pixels in the input image should move in or-
der to get the corrected image. It deﬁnes a non-parametric
transformation, thus being able to represent a wide range of
distortions. Since the ﬂow is a forward map from the dis-
torted image to the corrected image, a resampling method
is needed to produce the ﬁnal result.

This strategy follows learning methods of other applica-
tions which have observed that it is often simpler to predict
the transformation from input to output rather than predict-
ing the output directly (e.g., [14, 20]). Thus, we designed
our architecture to learn an intermediate ﬂow representa-
tion. Additionally, the forward mapping indicates where
each pixel with a known color in the distorted image maps
to. Therefore, all pixels in the input image learn a distortion
ﬂow prediction directly associated to them, which would
not be the case if we were attempting to learn a backward
mapping, where some input regions could not have corre-
spondences. It can be a serious problem when the distortion
changes the image shape greatly. Furthermore, our resam-
pling method that is required to generate the ﬁnal image is
fast and accurate.

We propose two networks by considering whether the
user has prior knowledge of the distortion type. Our net-
works are trained in a supervised manner. Therefore, we
ﬁrst introduce how the paired datasets have been con-
structed (Section 3.1) and then we introduce our two net-
works, for single-model and multi-model distortion estima-
tion (Sections 3.2 and 3.3, respectively).

3.1. Dataset construction

We generate the distorted image ﬂow pair by warping
an image with a given mapping, thus constructing the dis-
torted image dataset I and its corresponding distortion ﬂow
dataset F , where Ij ∈ I and Fj ∈ F are paired.

We consider six geometric distortion models in our net-
work. However, the architecture is not specialized to these
types of distortion and can potentially be further extended.
Each distortion type β = 1, ..., 6 has a model Mβ, which
deﬁnes the mapping from the distorted image lattice to the
original one. Bilinear interpolation is used if the corre-
sponding point in the original image is not on the integer
grid. The ﬂow F = Mβ(ρβ), F ∈ F , is generated in the
meantime to record how the pixel in the distorted image
should be moved to the corresponding point in the original
image. ρβ is the distortion parameter that controls the dis-
tortion effect. For instance, in the rotation distortion model,
ρβ is the rotation angle while in the barrel and pincushion
model, ρβ represents the parameter in Fitzgibbon’s single
parameter division model [12]. All distortion parameters ρβ
in different distortion models are randomly sampled using
a uniform distribution within a speciﬁed range. As Figure 2
shows, the geometric distortions change the image shapes.
Thus we crop the images and ﬂows to remove empty re-
gions.

3.2. Single model distortion estimation

We ﬁrst introduce a network N β parameterized by θβ to
estimate the ﬂow for distorted images with a known distor-
tion type β. N β learns the mapping from I β to F β with
sub-datasets where I β ⊂ I and F β ⊂ F are sub-domains
containing the images and ﬂows of distortion type β.

Architecture. A possible architecture choice is to directly
regress the distortion ﬂow according to the ground truth
with an auto-encoder-like structure. However, the network
would only be optimized with the pixel-wise ﬂow error,
without taking advantage of global constraints imposed by
the known distortion model. Instead, we design a network
to ﬁrst predict the model parameter ρβ directly. This param-
eter is then used to generate the ﬂow F = Mβ(ρβ) in the
network. Though the network should implicitly learn the
distortion parameter, there is no explicit constraint for the
network to do so exactly.

4857

Residual Block

Fully Connected Layer

Convolutional Layer

Distortion Model Layer 

64

128

256

CNNs

Resampling

GeoNetS

512

512

GeoNetM

Distortion
Parameter

EPE loss

64

128

256

512

512

512

512

512

512

Model Fitting

EPE loss

512

512

Distortion type

Cross-entropy loss

Figure 3. Overview of our entire framework, including the single-model (GeoNetS) and multi-model (GeoNetM) distortion networks
(Section 3), and resampling (Section 4). Each box represents some conv layers, with vertical dimension indicating feature map spatial
resolution, and horizontal dimension indicating the output channels of each conv layer in the box.

The network architecture, referred to as GeoNetS, is
shown in Figure 3. It has three conv layers at the very be-
ginning and ﬁve residual blocks ([17]) to gradually down-
size the input image and extract the features. Each resid-
ual contains two conv layers and has a shortcut connection
from input to output. The shortcut connection helps ease the
gradient ﬂow, achieving a lower loss according to our ex-
periments. Downsampling in spatial resolution is achieved
using conv layers with a stride of 2 and 3 × 3 kernels.
Batch normalization layers and ReLU function are added
after each conv layer, which signiﬁcantly improves training.
After the residual blocks, two conv layers are used to
downsize the features further, and a fully-connected layer
converts the 3D feature map into a 1D vector ρβ. With the
distortion parameter ρβ, the corresponding distortion model
Mβ analytically generates the distortion ﬂow. The network
is optimized with the pixel-wise ﬂow error between the gen-
erated ﬂow and the ground truth.

Loss We train the network to minimize the loss L, which
measures the distance between the estimated distortion ﬂow
and the ground truth ﬂow:

∗θβ = arg min
θβ

L(N β(I; θβ), F )

N β(I; θβ) = Mβ(nβ(I; θβ))

(1)

where nβ is the sub-network of N β, represents the part to
regress the distortion parameter implicitly. Here we choose
the endpoint error (EPE) as our loss function. The EPE
is deﬁned as the Euclidean distance between the predicted
ﬂow vector and the ground truth averaged over all pix-
els. Because the estimated distortion ﬂow is explicitly con-
strained by the distortion model, it is naturally smooth.

Since the geometric distortion models Mβ we consider

are differentiable, the backward gradient of each layer can
be computed using the chain rule:

∂L
∂θβ =

∂L
∂Mβ

∂Mβ
∂nβ

∂nβ
∂θβ

(2)

Our trained network can estimate the distortion ﬂow blindly
from an input image for each distortion type and achieve
comparable performance as traditional methods.

3.3. Multi model distortion estimation

The GeoNetS network is only able to capture a speciﬁc
distortion type with a distortion model at a time. For a new
type, the entire network has to be retrained. Furthermore,
the distortion type and model can be unknown in some
cases. In view of these limitations, we designed a second
network for multi-model distortion estimation. However,
since the distortion model and the parameters ρβ can vary
drastically across types, it is impossible to train a multi-
model network with the model constraints. We train a
network to regress the distortion ﬂow without model con-
straints and at the same time classify the distortion type.
The network is illustrated in Figure 3. The multi-model
network N parameterized by θ is jointly trained for two
tasks. The ﬁrst task estimates the distortion ﬂow, learning
the mapping from the image domain I to the ﬂow domain
F . The second task classiﬁes the distortion type, learning
the mapping from image domain I to type domain T .

Architecture The entire network adopts an encoder-
decoder structure, which includes an encoder part, a de-
coder part, and a classiﬁcation part. The input image is fed
into an encoder to encode the geometric features and cap-
ture the unnatural structures. Then two branches follow: In
the ﬁrst branch, a decoder is used to regress the distortion

4858

ﬂow, while in the second branch a classiﬁcation subnet is
used to classify the distortion type. The encoder part is the
same as GeoNetS, and the decoder part is symmetric to the
encoder. Downsampling/Upsampling in spatial resolution
is achieved using conv/upconv layers with a stride of 2. The
classiﬁcation part also has two conv layers to downsize the
features further, and a fully-connected layer converts the 3D
feature map into a 1D score vector of each type.

Loss We use the EPE loss Lﬂow in the ﬂow regression
branch, and a cross entropy loss in the classiﬁcation branch.
The two branches are jointly optimized by minimizing the
total loss:

∗θ = arg min

θ

(Lﬂow + λLclass)

(3)

where the weight λ provides a trade-off between the ﬂow
prediction and the distortion type classiﬁcation.

We observe that jointly learning the distortion type helps
reduce the ﬂow prediction error as well. These two branches
share the same encoder, and the classiﬁcation branch helps
the encoder learn the geometric features for different distor-
tion types better. Please refer to Section 5 for direct com-
parisons.

Model ﬁtting Our multi-model network simultaneously
predicts the ﬂow and the distortion type from the input im-
age. Based on this information, we can estimate the actual
distortion parameters in the model and regenerate the ﬂow
to obtain a more accurate result.

The Hough Transform is a widely used technique to ex-
tract features in an image.
It is robust to noise by elim-
inating the outliers in the ﬂow using a voting procedure.
Moreover, it is a non-iterative approach. Each data point is
treated independently, and therefore parallel processing of
all points is possible. This makes it more computationally
efﬁcient.

For an input image I, given its distortion type β and dis-
tortion ﬂow N (I;∗ θ) predicted by our network, we want
to ﬁt the corresponding distortion model Mβ with the dis-
tortion parameter ρβ. In our scenario, we map each data
point Nij in ﬂow N (I;∗ θ) at position (i, j) to a point in the
distortion parameter space. The transform is given by

ρij = M−1(Nij)

(4)

We assume the distortion parameter ρ has a range from
ρmin to ρmax and split the range into M cells uniformly.
All the points ρij belong to a cell according to the parameter
values. The cell receiving the maximum number of counts
determine the best ﬁtting result, and the ﬁnal result is the
average of all the points in this cell. We let M = 100 in our
experiments.

Distorted

IS (5)

IS (10)

IS (15)

Ours (5)

Figure 4. Comparison of the convergence using the traditional it-
erative search (IS) and our approach. Two examples with different
distortion levels are used. Our method converges to good resam-
pling result with 5 iterations.

Once model ﬁtting is completed, we have a reﬁned and
smoother ﬂow F = Mβ(ρβ). With model ﬁtting, the ef-
ﬁciency in correcting of higher resolution images can be
greatly improved. This is because we can estimate the ﬂow
and obtain the distortion parameter ρ at a much smaller res-
olution, and generate the full resolution ﬂow directly ac-
cording to the distortion parameter.

4. Resampling

Given the distortion ﬂow, we employ a pixel evaluation
algorithm to determine the backward-mapping and resam-
ple the ﬁnal undistorted image. The approach is inspired by
the bidirectional iterative search algorithm in [41]. Unlike
mesh rasterization approaches, this iterative method runs
entirely independently and in parallel for each pixel, fetch-
ing the color from the appropriate location of the source
image.

The traditional backward mapping algorithm of [41]
seeks to ﬁnd a point p in the source image that maps to q.
Since we only have the forward distortion ﬂow, this method
essentially inverts this mapping using an iterative search un-
til the location p converges:

p(0) = q

p(i+1) = q − f (p(i))

(5)

where f (p) is the computed forward ﬂow from the source
pixel p to the undistorted image.

Since the application in this paper often involves large,
smooth distortions, we propose a modiﬁcation that signiﬁ-
cantly improves the convergence rate and quality. The tra-
ditional method initializes p based on the ﬂow at q. More
speciﬁcally, p(1) = q − f (p(0)). If f (p(1)) ≈ f (p(0)), then
the iterative search converges quickly. However, in the pres-
ence of large distortions, kf (p(0))k is large, p(0) and p(1) are
distant, and thus, f (p(1)) and f (p(0)) can be very different,
making it a poor initialization and decreasing conversion
speed.

4859

Instead of assuming that the ﬂow in p(0) and p(1) are the
same, we compute the local derivative of the ﬂow at p(0)
using the ﬁnite difference method, and use this derivative to
estimate the ﬂow at p(1). We let fx(p) and fy(p) represent
the horizontal and vertical ﬂow respectively. Formally,

dfx
dx

=

fx(p(0)

nx ) − fx(p(0))
(x + 1) − x

(6)

where p(0) is at coordinates (x, y), and its horizontal pixel
neighbor pnx is at coordinates (x + 1, y). We then use this
derivative to approximate the ﬂow at p(1) = (x′, y′):

dfx
dx

=

fx(p(1)) − fx(p(0))

x′ − x

(7)

By the deﬁnition of forward ﬂow, we have fx(p(1)) = x −
x′. Therefore we can compute x′ combining Equation 6 and
Equation 7:

x′ = x −

fx(p(0))
nx ) − fx(p(0))

1 + fx(p(0)

(8)

We compute y′ similarly and proceed with the iterative
search. Note that we only use this ﬁnite difference method
in the ﬁrst iteration to get a coarse initial estimation. The
traditional, faster iterative search is used to ﬁnetune until
convergence.

5. Experiments

In this section, we report the results of our work. We ﬁrst
analyze the results of our proposed networks in Section 5.1.
Then we discuss the results of our resampling method in
Section 5.2. In Section 5.3, we show qualitative and quanti-
tative comparisons of our approach to previous methods for
correcting speciﬁc distortion types. In Section 5.4, we show
some applications of our method. CNNs training details are
given in the supplementary material.

5.1. Networks

GeoNetM is 97.3% for these six kinds of distortion. Table 1
also shows that single-type achieves lower ﬂow error than
multi-type since additional information needs to be learned
for the multi-type task.

We also examine whether the model ﬁtting method im-
proves prediction accuracy for GeoNetM. The last two rows
in Table 1 shows that the Hough transform based model
ﬁtting provides more accurate results. More results from
GeoNetM are shown in Figure 5. For each example, the
distorted image is shown on the left, the corrected output
image in the middle, and the three ﬂows (before ﬁtting, af-
ter ﬁtting, and ground truth) on the right. More real image
results and detailed discussion of model ﬁtting, GeoNetS
and GeoNetM are given in the supplementary material.

5.2. Resampling

Next we present the results of our resampling strategy.
Figure 4 shows results applied to images with two differ-
ent distortion levels. Note that, on the top row, it takes
roughly 10 iterations to converge using the traditional itera-
tive search approach (IS), whereas 5 iterations sufﬁce when
using our initialization. On the second row, with a more
severe distortion, even after 15 iterations, the traditional
method has not satisfactorily converged, whereas with our
initialization the results also converge within 5 iterations.

Figure 6 demonstrates how our method more quickly
converges to the ground truth (left), and how the vast major-
ity of pixels already have an endpoint error lower than 1/5
of a pixel after just 5 iterations. A parallel version of our
approach has been implemented on the GPU using an Intel
Xeon E5-2670 v3 2.3 GHz machine with Nvidia Tesla K80
GPU. It can resample the image under 50 ms.

5.3. Comparison with previous techniques

Next, we compare our distortion correction technique to
some existing methods that are specialized to some distor-
tion types. Note that, unlike these methods, our learning-
based approach is able to handle different distortion types.

To evaluate the performance of GeoNetS, we compare
with GeoNetM without classiﬁcation branch and train these
two networks on the same dataset with only single-type dis-
tortion. The ﬁrst two rows in Table 1 show that by ex-
plicitly considering the distortion model in the network,
GeoNetS achieves better results. Moreover, the ﬂow is
globally smooth due to the restriction given by the distor-
tion model.

Second, we examine how the joint learning with classi-
ﬁcation improves the distortion ﬂow prediction. The third
and fourth rows in Table 1 show that GeoNetM with joint
learning using the classiﬁcation branch has more accurate
prediction than GeoNetM w/o classiﬁcation training in the
multi-type distortion dataset. The classiﬁcation accuracy of

Lens distortion. Figure 7 compares our approach with [2]
and [30], which are specialized for lens distortion. Note
that for cases where the image has obvious distortion (e.g.,
ﬁrst row), all methods can correct accurately. However, in
cases where the distortion is more subtle or does not ex-
hibit highly distorted lines (e.g., bottom two rows), our ap-
proach yields improved results. Figure 8 shows a quan-
titative comparison based on 50 randomly chosen images
from the dataset of [21]. These images include a variety
of scene types (e.g., nature, man-made, water) and are dis-
torted with random distortion parameters to generate our
synthetic dataset. All of these methods use Fitzgibbon’s sin-
gle parameter division model [12], therefore we can calcu-
late the relative error of the distortion correction parameters

4860

Conﬁguration

EPE

Architecture

Training dataset

Single-type
GeoNetS
Single-type
GeoNetM w/o Clas
GeoNetM w/o Clas Multi-type
GeoNetM
Multi-type
GeoNetM w/ Hou Multi-type

B

1.43
1.57
3.07
2.72
1.78

Pi

0.79
1.12
2.24
2.03
1.34

R

2.41
3.01
3.75
3.68
2.77

S

P

W Average

2.19
2.91
4.99
3.12
2.27

0.89
1.01
3.35
3.29
2.25

1.06
1.32
1.73
1.67
1.22

1.46
1.82
3.19
2.75
1.94

Table 1. EPE and classiﬁcation statistics of our approach using 500 test images per distortion.

Source

Corrected

Flow

Source

Corrected

Flow

Source

Corrected

Flow

Figure 5. Results of distortions that we considered. Top row: Barrel distortion, pincushion distortion and shear distortion. Bottom row:
rotation, wave distortion and perspective distortion. The ﬂows refer to the ﬂow before model ﬁtting (top), after model ﬁtting (middle), and
ground truth (bottom).

0.98

0.92

Iterative search
Ours

Iterative search
Ours

e
g
a
t
n
e
c
r
e
P

5

10

15

20

0

1

2

3

4

Iterations

Endpoint error at 5th iteration

 

E
P
E
e
g
a
r
e
v
A

0

0

Figure 6. Convergence of EPE (left) and its histogram after ﬁve
iterations (right) for traditional iterative search and our method.
Test on 10 images with different distortion levels.

Table 2. Angle deviation of detected lines with the vertical angle.

method

baseline (input)

[6]

ours

angle deviation

6.44◦

3.03◦

2.81◦

Source

Ours

Result of [2]

Result of [30]

Figure 7. Qualitative comparison with state-of-the-art lens distor-
tion correction methods.

for comparison. Note that, with our approach, the number
of sample images (y-axis) that lie within the error thresholds
(x-axis) is signiﬁcantly higher than the other methods.

Perspective distortion. For the perspective distortion, we
compare with [6]. Here we use angle deviation as the

metric. We collect 30 building images under orthographic
projection and distort them with different Homography
matrices. We control the distorted vertical lines within
[70◦, 110◦]. Then we detect straight lines [36] in the correc-
tion results using the line segment detector within this range
and assume that the angle of these lines should be 90◦ after

4861

Ours
Aleman et al. 2014
Santana et al. 2015

50

40

30

20

10

l

s
e
p
m
a
S
 
f
o
 
r
e
b
m
u
N

0
0.0

0.2

0.4

0.6

0.8

1.0

Relative Error Threshold

Figure 8. Quantitative comparison on lens distortion correction
methods. Given relative error threshold, our method gives more
accurate pixels than the other methods.

Source

Ours

Result of [6]

Figure 9. Comparison with previous perspective correction
method.

Reference

Target

Result

Figure 10. Examples of distortion transfer. Perspective and barrel
distortions are used.

correction and calculate their average angle deviation. As
shown in Table 2 and Figure 9, our method outperforms the
previous approach [6].

Source

Shrunk

Expanded

Figure 11. Example of distortion exaggeration.

Distortion transfer. Our system can detect the distortion
from a reference image and transfer to a target image. We
can estimate the forward ﬂow from the reference image to
the corrected version and then directly apply it to the target
by bilinear interpolation. Figure 10 shows two examples
of transferring distortion from a reference image to a target
image, in order to accentuate the perspective of a house pho-
tograph (upper row) or apply aggressive barrel distortion to
a portrait (lower row).

Distortion exaggeration. To achieve distortion exagger-
ation, we can reverse the direction of estimated ﬂow ﬁeld to
make the pixels further away from its undistorted position,
and use our resampling approach to generate an exaggerated
distortion output. Figure 11 shows a building with perspec-
tive effect, we can adjust the level of distortion to exaggerate
the effect by amplifying or reversing the ﬂow, respectively.

Co-occurring distortion correction. Sometimes an im-
age could have more than one type of distortion. We can
correct the distorted image simply by running our correction
algorithm twice iteratively. For each iteration, it detects and
corrects the most severe type of distortion that it encounters.
See the supplementary material for some examples and re-
sults.

6. Conclusion

In conclusion, we present the ﬁrst approach to blindly
correct several types of geometric distortions from a single
image. Our approach uses a deep learning method trained
on several common distortions to detect the distortion ﬂow
and type. Our model ﬁtting and parameter estimation ap-
proach then accurately predicts the distortion parameters.
Finally, we present a fast parallel approach to resample the
distortion-corrected images. We compare our techniques
to recent specialized methods for distortion correction and
present applications such as distortion transfer, distortion
exaggeration, and co-occurring distortion correction.

5.4. Applications

In addition, we explored applications that can beneﬁt

from our distortion correction method directly.

Acknowledgements: This work was partly supported by
CityU of Hong Kong Start-up Grant No. 7200607/CS, and
Hong Kong GRF Grant No. 16208814.

4862

References

[1] A. M. Al-Shatnawi and K. Omar. Skew detection and correc-
tion technique for arabic document images based on centre
of gravity. Journal of Computer Science, 5(5):363, 2009. 1

[2] M. Alem´an-Flores, L. Alvarez, L. Gomez, and D. Santana-
Cedr´es. Automatic lens distortion correction using one-
parameter division models.
Image Processing On Line,
4:327–343, 2014. 2, 6, 7

[3] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. arXiv preprint arXiv:1511.00561, 2015. 3

[4] J. P. Barreto and K. Daniilidis. Fundamental matrix for cam-
eras with radial distortion. In Computer Vision, 2005. ICCV
2005. Tenth IEEE International Conference on, volume 1,
pages 625–632. IEEE, 2005. 1

[5] F. Bukhari and M. N. Dailey. Automatic radial distortion
estimation from a single image. Journal of mathematical
imaging and vision, 45(1):31–45, 2013. 1, 2

[6] K. Chaudhury, S. DiVerdi, and S. Ioffe. Auto-rectiﬁcation of
user photos. In IEEE Image Processing (ICIP), pages 3479–
3483, 2014. 7, 8

[7] K. Chaudhury and S. J. DiVerdi. Automatic rectiﬁcation of
distortions in images, June 23 2015. US Patent 9,064,309. 1,
2

[8] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE transactions on pattern analysis and ma-
chine intelligence, 40(4):834–848, 2018. 3

[9] T. Dekel, T. Michaeli, M. Irani, and W. T. Freeman. Re-
vealing and modifying non-local variations in a single image.
ACM Transactions on Graphics (TOG), 34(6):227, 2015. 2

[10] C. B. Duane. Close-range camera calibration. Photogramm.

Eng, 37(8):855–866, 1971. 2

[11] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in neural information processing systems, pages
2366–2374, 2014. 3

[12] A. W. Fitzgibbon. Simultaneous linear estimation of multi-
ple view geometry and lens distortion. In Computer Vision
and Pattern Recognition, 2001. CVPR 2001. Proceedings of
the 2001 IEEE Computer Society Conference on, volume 1,
pages I–I. IEEE, 2001. 2, 3, 6

[13] A. C. Gallagher. Using vanishing points to correct camera
rotation in images.
In Computer and Robot Vision, 2005.
Proceedings. The 2nd Canadian Conference on, pages 460–
467. IEEE, 2005. 1, 2

[14] M. Gharbi, Y. Shih, G. Chaurasia, J. Ragan-Kelley, S. Paris,
and F. Durand. Transform recipes for efﬁcient cloud photo
enhancement.
ACM Transactions on Graphics (TOG),
34(6):228, 2015. 3

[15] R. Hartley and S. B. Kang. Parameter-free radial distor-
tion correction with center of distortion estimation.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
29(8):1309–1321, 2007. 1, 2

[16] J. Hays, M. Leordeanu, A. A. Efros, and Y. Liu. Discovering
texture regularity as a higher-order correspondence problem.

In European Conference on Computer Vision, pages 522–
535. Springer, 2006. 2

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 4

[18] J. Heikkila and O. Silven. A four-step camera calibra-
tion procedure with implicit image correction. In Computer
Vision and Pattern Recognition, 1997. Proceedings., 1997
IEEE Computer Society Conference on, pages 1106–1112.
IEEE, 1997. 2

[19] J. Henrique Brito, R. Angst, K. Koser, and M. Pollefeys. Ra-
dial distortion self-calibration. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1368–1375, 2013. 2

[20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.

Image-
to-image translation with conditional adversarial networks.
arXiv preprint, 2017. 3

[21] H. Jegou, M. Douze, and C. Schmid. Hamming embedding
and weak geometric consistency for large scale image search.
In European conference on computer vision, pages 304–317.
Springer, 2008. 6

[22] V. G. Kim, Y. Lipman, and T. A. Funkhouser. Symmetry-
guided texture synthesis and manipulation. ACM Trans.
Graph., 31(3):22–1, 2012. 2

[23] Z. Kukelova and T. Pajdla. A minimal solution to radial dis-
tortion autocalibration. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 33(12):2410–2422, 2011. 1,
2

[24] H. Lee, E. Shechtman, J. Wang, and S. Lee. Automatic
upright adjustment of photographs.
In Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conference on,
pages 877–884. IEEE, 2012. 1, 2

[25] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3431–3440, 2015. 3

[26] H. Noh, S. Hong, and B. Han. Learning deconvolution net-
work for semantic segmentation. In Proceedings of the IEEE
international conference on computer vision, pages 1520–
1528, 2015. 3

[27] M. Park, K. Brocklehurst, R. T. Collins, and Y. Liu. De-
formed lattice detection in real-world images using mean-
shift belief propagation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 31(10):1804–1816, 2009. 2

[28] S. Ramalingam, P. Sturm, and S. K. Lodha. Generic self-
calibration of central cameras. Computer Vision and Image
Understanding, 114(2):210–219, 2010. 2

[29] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation.
In
International Conference on Medical image computing and
computer-assisted intervention, pages 234–241. Springer,
2015. 3

[30] D. Santana-Cedr´es, L. Gomez, M. Alem´an-Flores, A. Sal-
gado, J. Esclar´ın, L. Mazorra, and L. Alvarez.
Invertibil-
ity and estimation of two-parameter polynomial and division
lens distortion models. SIAM Journal on Imaging Sciences,
8(3):1574–1606, 2015. 6, 7

4863

[31] D. Santana-Cedr´es, L. Gomez, M. Alem´an-Flores, A. Sal-
gado, J. Esclar´ın, L. Mazorra, and L. Alvarez. Automatic
correction of perspective and optical distortions. Computer
Vision and Image Understanding, 161:1–10, 2017. 1, 2

[32] J.-P. Tardif, P. Sturm, M. Trudeau, and S. Roy. Calibra-
tion of cameras with radially symmetric distortion.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
31(9):1552–1566, 2009. 2

[33] T. Thorm¨ahlen, H. Broszio, and I. Wassermann. Robust line-
based calibration of lens distortion from a single view. Mi-
rage 2003, pages 105–112, 2003. 1

[34] T. Toutin. Geometric processing of remote sensing images:
International journal of

models, algorithms and methods.
remote sensing, 25(10):1893–1924, 2004. 1

[35] R. Tsai. A versatile camera calibration technique for high-
accuracy 3d machine vision metrology using off-the-shelf tv
cameras and lenses. IEEE Journal on Robotics and Automa-
tion, 3(4):323–344, 1987. 2

[36] R. G. Von Gioi, J. Jakubowicz, J.-M. Morel, and G. Ran-
dall. Lsd: A fast line segment detector with a false detection
control. IEEE transactions on pattern analysis and machine
intelligence, 32(4):722–732, 2010. 7

[37] N. Wadhwa, T. Dekel, D. Wei, F. Durand, and W. T. Freeman.
Deviation magniﬁcation: revealing departures from ideal ge-
ometries. ACM Transactions on Graphics (TOG), 34(6):226,
2015. 2

[38] J. Walker, A. Gupta, and M. Hebert. Dense optical ﬂow pre-
diction from a static image. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 2443–2451,
2015. 3

[39] A. Wang, T. Qiu, and L. Shao. A simple method of radial dis-
tortion correction with centre of distortion estimation. Jour-
nal of Mathematical Imaging and Vision, 35(3):165–172,
2009. 1, 2

[40] S. Workman, M. Zhai, and N. Jacobs. Horizon lines in the

wild. arXiv preprint arXiv:1604.02129, 2016. 2

[41] L. Yang, Y.-C. Tse, P. V. Sander, J. Lawrence, D. Ne-
hab, H. Hoppe, and C. L. Wilkins.
Image-based bidirec-
tional scene reprojection. ACM Trans. Graph., 30(6):150:1–
150:10, 2011. 5

[42] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. arXiv preprint arXiv:1511.07122, 2015.
3

[43] M. Zhai, S. Workman, and N. Jacobs. Detecting vanishing
points using global image context in a non-manhattanworld.
In Computer Vision and Pattern Recognition (CVPR), 2016
IEEE Conference on, pages 5657–5665. IEEE, 2016. 2

[44] Z. Zhang. Flexible camera calibration by viewing a plane
from unknown orientations. In Computer Vision, 1999. The
Proceedings of the Seventh IEEE International Conference
on, volume 1, pages 666–673. Ieee, 1999. 2

4864

