Learning View Priors for Single-view 3D Reconstruction

Hiroharu Kato1 and Tatsuya Harada1,2
1The University of Tokyo, 2RIKEN
{kato,harada}@mi.t.u-tokyo.ac.jp

Abstract

2D image

Reconstructed 3D model

There is some ambiguity in the 3D shape of an object
when the number of observed views is small. Because of
this ambiguity, although a 3D object reconstructor can be
trained using a single view or a few views per object, re-
constructed shapes only ﬁt the observed views and appear
incorrect from the unobserved viewpoints. To reconstruct
shapes that look reasonable from any viewpoint, we propose
to train a discriminator that learns prior knowledge regard-
ing possible views. The discriminator is trained to distin-
guish the reconstructed views of the observed viewpoints
from those of the unobserved viewpoints. The reconstructor
is trained to correct unobserved views by fooling the dis-
criminator. Our method outperforms current state-of-the-
art methods on both synthetic and natural image datasets;
this validates the effectiveness of our method.

1. Introduction

Humans can estimate the 3D structure of an object in a
single glance. We utilize this ability to grasp objects, avoid
obstacles, create 3D models using CAD, and so on. This is
possible because we have gained prior knowledge about the
shapes of 3D objects.

Can machines also acquire this ability? This problem
is called single-view 3D object reconstruction in computer
vision. A straightforward approach is to train a reconstruc-
tor using 2D images and their corresponding ground truth
3D models [3, 5, 9, 13, 16, 27, 30]. However, creating 3D
annotations requires extraordinary effort from professional
3D designers. Another approach is to train a reconstructor
using a single view or multiple views of an object without
explicit 3D supervision [15, 17, 18, 31, 36]. We call this
approach view-based training. This approach typically re-
quires annotations of silhouettes of objects and viewpoints,
which are relatively easy to obtain.

Because a ground truth 3D shape is not given in view-
based training, there is some ambiguity in the possible
shapes.
In other words, several different 3D shapes can
be projected into the same 2D view, as shown in the up-

From original viewpoint

Unobserved views

+ View prior learning

Figure 1. When a 3D reconstructor is trained using only a single
view per object, because of ambiguity in the 3D shape of an object,
it reconstructs a shape which only ﬁts the observed view and looks
incorrect from unobserved viewpoints (upper). By introducing a
discriminator that learns prior knowledge of correct views, the re-
constructor is able to generate a shape that is viewed as reasonable
from any viewpoint (lower).

per half and lower half of Figure 1. To reduce this ambi-
guity, twenty or more views per object are typically used in
training [18, 36]. However, this is not practical in many
cases in terms of feasibility and scalability. When creating
a dataset by taking photos, if an object is moving or deform-
ing, it is difﬁcult to take photos from many viewpoints. In
addition, when creating a dataset using a large number of
photos from the Internet, it is not always possible to collect
multiple views of an object. Therefore, it is desirable that
a reconstructor can be trained using a few views or even a
single view of an object.

In this work, we focus on training a reconstructor using a
single view or a few views for single-view 3D object recon-
struction. In this case, the ambiguity in shapes in training
is not negligible. The upper half of Figure 1 shows the re-
sult of single-view 3D reconstruction using a conventional
approach [18]. Although this method originally uses mul-
tiple views per object for training, a single view is used in
this experiment. As a result, the reconstructed shape looks

9778

Work
[13, 37]
[9, 34]

(a)
(b)
(c) Ours
(d)

-

Class A
Predicted 3D shapes
Predicted 3D shapes
Views of predicted 3D shapes from observed viewpoints Views of predicted 3D shapes from random viewpoints
Views of predicted 3D shapes

Class B
Their corresponding ground truth 3D shapes
3D shape collections

Views in a training dataset

Table 1. Summary of discriminators in learning-based 3D reconstruction. Discriminator (d) is described in Section 3.2.

correct when viewed from the same viewpoint as the input
image, however, it looks incorrect from other viewpoints.
This is because the reconstructor is unaware of unobserved
views and generates shapes that only ﬁt the observed views.

How can a reconstructor overcome shape ambiguity and
correctly estimate shapes? The hint is shown in Figure 1.
Humans can recognize that the three views of the chair in
the upper-right of Figure 1 are incorrect because we have
prior knowledge of how a chair looks, having seen many
chairs in the past. If machines also have knowledge regard-
ing the correct views, they would use it to estimate shapes
more accurately.

We implement this idea on machines by using a discrim-
inator and adversarial training [7]. One can see from the
upper half of Figure 1 that, with the conventional method,
views of estimated shapes from observed viewpoints con-
verge to the correct views, while unobserved views do not
always become correct. Therefore, we train the discrimi-
nator to distinguish the observed views of estimated shapes
from the unobserved views. This results in the discriminator
obtaining knowledge regarding the correct views. By train-
ing the reconstructor to fool the discriminator, reconstructed
shapes from all viewpoints become indistinguishable and to
be viewed as reasonable from any viewpoint. The lower half
of Figure 1 shows the results from the proposed method.

Learning prior knowledge of 3D shapes using 3D mod-
els was tackled in other publications [9, 34]. In contrast,
we focus on prior knowledge of 2D views rather than 3D
shapes. Because our method does not require any 3D mod-
els for training, ours can scale to various categories where
3D models are difﬁcult to obtain.

The major contributions can be summarized as follows.

• In view-based training of single-view 3D reconstruc-
tion, we propose a method to predict shapes which are
viewed as reasonable from any viewpoint by learning
prior knowledge of object views using a discriminator.
Our method does not require any 3D models for train-
ing.

• We conducted experiments on both synthetic and nat-
ural image datasets and we observed a signiﬁcant per-
formance improvement for both datasets. The advan-
tages and limitations of the method are also examined
via extensive experimentation.

2. Related work

A simple and popular approach for learning-based 3D re-
construction is to use 3D annotations. Recent studies focus
on integrating multiple views [3, 16], memory efﬁciency
problem of voxels [30], point cloud generation [5], mesh
generation [8, 32], advanced loss functions [13], and neural
network architectures [27].

To reduce the cost of 3D annotation, view-based train-
ing has recently become an active research area. The key
of training is to deﬁne a differentiable loss function for
view reconstruction. A loss function of silhouettes us-
ing chamfer distance [17], differentiable projection of vox-
els [31, 33, 36, 38], point clouds [11, 23], and meshes [18] is
proposed. Instead of using view reconstruction, 3D shapes
can be reconstructed via view synthesis [29].

As mentioned in the previous section, it is not easy to
train reconstructors using a small number of views. For this
problem, some methods use human knowledge of shapes
as regularizers or constraints. For example,
the graph
Laplacian of meshes was regularized [15, 32], and shapes
were assumed to be symmetric [15].
Instead of using
manually-designed constraints, others attempted to acquire
prior knowledge of shapes from data. Learning category-
speciﬁc mean shapes [15, 17] is an example. Adversar-
ial training is another way to learn shape priors. Yang et
al. [37] and Jiang et al. [13] used discriminators on an es-
timated shape and its corresponding ground truth shape to
make the estimated shapes more realistic. Gwak et al. [9]
and Wu et al. [34] used discriminators on generated shapes
and a shape collection. In contrast, our method does not re-
quire 3D models to learn prior knowledge. Table 1 lists a
summary of these discriminators.

3. View-based training of single-view 3D object

reconstructors with view prior learning

In this section, we introduce a simple view-based method
to train 3D reconstructors based on [18]. Then, we describe
our main technique, called view prior learning (VPL). We
also explain a technique to further improve reconstruction
accuracy by applying internal pressure to shapes. Figure 2
shows the architecture of our method.

For training, our method requires a dataset that con-
tains single or multiple views of objects, and their silhou-
ette and viewpoint annotations, similar to previous stud-
ies [15, 18, 31, 36]. Additionally, ours can also use class

9779

Image x

ij

Encoder Enc

Shape decoder Dec

s

Texture decoder Dec

t

3D model

Renderer 

 P

View

Gradient reversal

View comparison

Reconstruction loss L

r

Corresponding viewpoint v

ij

Random viewpoint v

kl

Discriminator Dis

View discrimination loss L

d

Renderer 

 P

View

Gradient reversal

Internal pressure 

Internal pressure loss L

p

Input

Loss

Trainable function

Other function

Figure 2. Architecture of the proposed method. The main point of our method is the use of discrimination loss to learn priors of views.
While the discriminator aims to minimize discrimination loss, the encoder and decoders try to maximize it using a gradient reversal layer.

View comparison 

Reconstruction loss

View comparison 

Image A

Viewpoint A

Renderer 

Renderer 

Encoder & Decoder

Image B

Viewpoint B

Figure 3. Reconstruction loss in multi-view training. Images A and
B are views of the same object. Although only the loss with respect
to a view reconstructed from image A is shown in this ﬁgure, the
loss with respect to image B is also computed.

labels of views if they are available. After training, recon-
struction is performed without silhouette, viewpoint, and
class label annotations.

3.1. View based training for 3D reconstruction

In this section, we describe our baseline method for 3D
reconstruction. We extend a method which uses silhouettes
in training [18] to handle textures using a texture decoder
and perceptual loss [14].

Overview. The common approach to view-based training
of 3D reconstructors is to minimize the difference between
views of a reconstructed shape and views of a ground truth
shape. Let xij be the view of an object oi from a viewpoint
vij , No be the number of objects in the training dataset, Nv
be the number of viewpoints per object, R(·) be a recon-
structor that takes an image and outputs a 3D model, P (·, ·)
be a renderer that takes a 3D model and a viewpoint and out-
puts the view of the given model from the given viewpoint,
and Lv(·, ·) be a function that measures the difference be-
tween two views. Then, reconstruction loss is deﬁned as

Lr(x, v) =

No

Nv

Nv

Xi=1

Xj=1

Xk=1

Lv(P (R(xij), vik), xik).

(1)

i=1 Lv(P (R(xi1), vi1), xi1). We call the case where 2 ≤

PNo

Nv multi-view training.

3D representation and renderer. Some works use vox-
els as a 3D representation in view-based training [31, 36].
However, voxels are not well suited to view-based training
because using high-resolution views of voxels is difﬁcult as
voxels are memory inefﬁcient. Recently, this problem was
overcome by Kato et al. [18] by using a mesh as a 3D repre-
sentation and a differentiable mesh renderer. Following this
work, we also use a mesh and their renderer1.

Reconstructor.
In this work, a 3D model is represented
by a pair of a shape and a texture. Our reconstructor R(·)
uses an encoder-decoder architecture. An encoder Enc(·)
encodes an input image, and a shape decoder Decs(·) and
texture decoder Dect(·) generate a 3D mesh and a texture
image, respectively. Following recent learning-based mesh
reconstruction methods [15, 18, 32], we generate a shape by
moving the vertices of a pre-deﬁned mesh. Therefore, the
output of the shape decoder is the coordinates of the esti-
mated vertices. The details of the encoder and the decoders
are described in the supplementary material.

View comparison function. Color images (RGB chan-
nels) and silhouettes (alpha channels) are processed sepa-
rately in Lv(·, ·). Let x and ˆx = P (R(x), v) be a ground
truth view and an estimated view, xc, ˆxc be the RGB chan-
nels of x, ˆx, and xs, ˆxs be the alpha channels of x, ˆx. The
silhouette at the i-th pixel xsi is set to one if an object ex-
ists at the pixel and to zero if the pixel is part of the back-
ground. xs can take a value between zero and one owing
to anti-aliasing of the renderer. To compare color images
xc, ˆxc, we use perceptual loss Lp [14] with additional fea-
ture normalization. Let Fm(·) be the m-th feature map of
Nf maps in a pre-trained CNN for image classiﬁcation. In
addition, let Cm, Hm, Wm be the channel size, height, and
width of Fm(·), respectively. Speciﬁcally, we use the ﬁve

We call the case where Nv = 1 single-view training. In
this case, the reconstruction loss is simpliﬁed to Lr(x, v) =

1We modiﬁed the approximate differentiation of the renderer. Details

are described in the supplementary material.

9780

feature maps after convolution layers of AlexNet [20] for
Fm(·). Then, using Dm = CmHmWm, the perceptual loss
is deﬁned as

that the view is correct, and V be the set of all viewpoints
in the training dataset. Using cross-entropy, we deﬁne view
disrcimination loss as

Lc( ˆxc, xc) =

Nf

Xm=1

Fm(ˆxc)
|Fm(ˆxc)|

−

1

Dm (cid:12)(cid:12)(cid:12)(cid:12)

Fm(xc)

|Fm(xc)|(cid:12)(cid:12)(cid:12)(cid:12)

2

.

(2)

For silhouettes xs, ˆxs, we use their multi-scale cosine dis-
tance. Let xi
s be an image obtained by down-sampling xs
2i−1 times, and Ns be the number of scales. We deﬁne the
loss function as

Ls(xs, ˆxs) =

Ns

Xi=1

(cid:18)1 −

xi
s · ˆxi
s
|xi
s||ˆxi

s|(cid:19) .

(3)

We also use negative intersection over union (IoU) of sil-
houettes, as was used in [18]. Let ⊙ be an elementwise
product. This loss is deﬁned as

Ls(xs, ˆxs) = 1 −

|xs ⊙ ˆxs|1

|xs + ˆxx − xs ⊙ ˆxs|1

.

(4)

The total reconstruction loss is Lv = Ls + λcLc. λc is a
hyper-parameter.

Training. We optimize R(·) using mini-batch gradient
descent. Figure 2 shows the architecture of single-view
training.
In multi-view training, we randomly take two
views of an object in one minibatch. The architecture for
computing Lr in this case is shown in Figure 3.

3.2. View prior learning

As described in Section 1, in view-based training, a re-
constructor can generate a shape that looks unrealistic from
unobserved viewpoints. In order to reconstruct a shape that
is viewed as realistic from any viewpoint, it is necessary to
(1) learn the difference between correct views and incorrect
views, and (2) tell the reconstructor how to modify incorrect
views.
In view-based training, reconstructed views from
observed viewpoints converge to the real views in a training
dataset by minimizing the reconstruction loss, and views
from unobserved viewpoints do not always become correct.
Therefore, the former can be regarded as correct and real-
istic views, and the latter can be regarded as incorrect and
unrealistic views. Based on this assumption, we propose
to train a discriminator that distinguishes estimated views
at observed viewpoints from estimated views at unobserved
viewpoints to learn the correctness of views. The discrimi-
nator can pass this knowledge to the reconstructor by back-
propagating the gradient of the discrimination loss into the
reconstructor via estimated views and shapes as with adver-
sarial training in image generation [7] and domain adapta-
tion [6].

Concretely, let Dis(·, ·) be a trainable discriminator that
takes a view and its viewpoint and outputs the probability

Ld(xij , vij) = − log(Dis(P (R(xij), vij), vij))

− Xvu∈V,vu6=vij

log(1 − (Dis(P (R(xij), vu), vu)))

|V − 1|

.

(5)

In minibatch training, we sample one random view for each
reconstructed object to compute Ld.

Stability of training. Although adversarial training is
generally not stable, training of our proposed method is sta-
ble. It is known that training of GANs fails when the dis-
criminator is too strong to be fooled by the generator. This
problem is explained from the distinction of the supports of
real and fake samples [1]. However, in our case, it is very
difﬁcult to distinguish views correctly in the earlier train-
ing stage because view reconstruction is not accurate and
views are incorrect from any viewpoint. Even in the later
stage, the reconstructor can easily fool the discriminator by
slightly breaking the correct views. Therefore, the discrim-
inator cannot be dominant in our method.

Optimization of the reconstructor. The original proce-
dure of adversarial training requires optimizing a discrim-
inator and a generator iteratively [7]. Subsequently, Ganin
et al. [6] proposed to train a generator using the reversed
gradient of discrimination loss. The proposed gradient re-
versal layer does nothing in the forward pass, although it
reverses the sign of gradients and scales them λd times in
the backward pass. This layer is posed on the right before
a discriminator. Because this optimization procedure is not
iterative, the training time is shorter than in iterative opti-
mization. Furthermore, we experimentally found that the
performance of the gradient reversal and iterative optimiza-
tion are nearly the same in our problem. Therefore, we use
the gradient reversal layer for training the reconstructor.

Image type for the discriminator. The discriminator can
take both RGBA images and silhouette images. We give it
RGBA images when texture prediction is conducted, other-
wise we give it silhouettes.

Class conditioning.
In addition, a discriminator can be
conditioned on class labels using the conditional GAN [24]
framework. When class labels are known, view discrimi-
nation becomes easier and the discriminator becomes more
reliable. We use the projection discriminator [26] for class
conditioning. Note that the test phase does not require class
labels even in this case.

Another possible discriminator. We propose to train a
discriminator on views of reconstructed shapes at observed
and unobserved viewpoints. Another possible approach is

9781

to distinguish reconstructed views from real views in a train-
ing dataset. In fact, this discriminator does not work well
because generating a view that is difﬁcult to distinguish
from the real view is very difﬁcult. This is caused by the
limitation of the representation ability of the reconstructor
and renderer. Table 1 shows a summary of the discrimina-
tors we have explained thus far.

3.3. Internal pressure

One of the most popular methods in multi-view 3D re-
construction is visual hull [21]. In visual hull, a point inside
all silhouettes is assumed to be inside the object. In other
words, in terms of shape ambiguity, visual hull produces the
shape with the largest volume. Following this policy, we
inﬂate the volume of the estimated shapes by giving them
internal pressure in order to maximize their volume. Con-
cretely, we add a gradient along the normal of the face for
each vertex of a triangle face. Let pi be one of the vertices
of a triangle face, and n be the normal of the face. We add
a loss term Lp that satisﬁes ∂Lp(pi)

= −n.

∂pi

3.4. Summary

In addition to using reconstruction loss Lr = Ls + λcLc,
we propose to use view discrimination loss Ld to recon-
struct realistic views and internal pressure loss Lp to inﬂate
reconstructed shapes. The total loss is L = Ls + λcLc +
Ld + λpLp. The hyperparameters of loss weighting are λc,
λp, and λd. Because λd is used in the gradient reversal
layer, it does not appear in L. The entire architecture is
shown in Figure 2.

4. Experiments

We tested our proposed view prior learning (VPL) on
synthetic and natural image datasets. We conducted an ex-
tensive evaluation of our proposed method using a synthetic
dataset because it consists of a large number of objects with
accurate silhouette and viewpoint annotations.

As a metric of the reconstruction accuracy, we used
intersection over union (IoU) of a predicted shape and
a ground truth that was used in many previous publica-
tions [3, 5, 15, 16, 18, 27, 30, 31, 36]. To fairly compare
our results with those in the literature, we computed IoU
after converting a mesh into a volume of 323 voxels2.

4.1. Synthetic dataset

As a synthetic dataset, we used ShapeNet [2], a large-
scale dataset of 3D CAD models. We use 43, 784 objects in
thirteen categories from ShapeNet. By using ShapeNet and

2Another popular metric is the chamfer distance of point clouds. How-
ever, this metric is not suitable for use in view-based learning. Because
it commonly assumes that points are distributed on surfaces, it is inﬂu-
enced by invisible structures inside shapes, which are impossible to learn
in view-based training. This problem does not arise when using IoU be-
cause it commonly assumes that the interior of a shape is ﬁlled.

Baseline

Proposed

Baseline

Proposed

Baseline

Proposed

(a)

(b)

(c)

(d)

(e)

Figure 4. Examples of single-view training on the ShapeNet
dataset. (a) Input images. (b) Reconstructed shapes viewed from
the original viewpoints. (c–e) Reconstructed shapes viewed from
other viewpoints.

a renderer, a dataset of views, silhouettes, viewpoints, and
ground truth 3D shapes can be synthetically created. We
used ground truth 3D shapes only for validation and testing.
We used rendered views and train/val/test splits provided by
Kar et al. [16]. In this dataset, each 3D model is rendered
from twenty random viewpoints. Each image has a resolu-
tion of 224 × 224. We augmented the training images by
random color channel ﬂipping and horizontal ﬂipping, as
was used in [16, 27]3. We use all or a subset of views for
training, and all views were used for testing.

We used Batch Normalization [12] and Spectral Normal-
ization [25] in the discriminator. The parameters were op-
timized with the Adam optimizer [19]. The architecture of
the encoder and decoders, hyperparameters, and optimizers
are described in the supplementary material. The hyper-
parameters were tuned using the validation set. We used
Equation 3 as the view comparison function for silhouettes.

4.1.1 Single-view training

At ﬁrst, we trained reconstructors in single-view training
described in Section 3.1. Namely, we used only one ran-
domly selected view out of twenty views for each object in
training.

Figure 4 shows examples of reconstructed shapes with
and without VPL. When viewed from the original view-
points (b), the estimated shapes appear valid in all cases.
However, without VPL, the shapes appear incorrect when

3When ﬂipping images, we also ﬂip the corresponding viewpoints.

9782

viewed from other viewpoints (c–e). For example, the back-
rest of the chair is too thick, the car is completely broken,
and the airplane has a strange prominence in the center.
When VPL is used, the shapes look reasonable from any
viewpoint. These results clearly indicate that the discrim-
inator informed the reconstructor regarding knowledge of
feasible views.

Table 2 shows a quantitative evaluation of single-view
training. VPL provides signiﬁcantly improved reconstruc-
tion performance. This improvement is further boosted
when the discriminator is class conditioned. We can tell
that conducting texture prediction also helps train accurate
reconstructors.

VPL is particularly effective with the phone, display,
bench, and sofa categories. In contrast, VPL is not effec-
tive with the lamp category. Typical examples in these cate-
gories are shown in the supplementary material. In the case
of phone and display categories, because the silhouettes are
very simple, the shapes are ambiguous and various shapes
can ﬁt into one view. Although integrating texture predic-
tion reduces the ambiguity, VPL is much more effective. In
the case of bench and sofa categories, learning their long
shapes is difﬁcult without considering several views. Be-
cause the shapes in the lamp category are diverse and the
training dataset is relatively small, the discriminator cannot
learn meaningful priors.

4.1.2 Multi-view training

Second, we trained reconstructors using multi-view training
as described in Section 3.1. Namely, we used two or more
views out of twenty views for each object in training.

Table 3 shows the relationship between the reconstruc-
tion accuracy and the number of views per object Nv
used for training. Texture prediction was not conducted in
this experiment, and the difference between the proposed
method and the baseline is the use of VPL with class con-
ditioning. Our proposed method outperforms the baseline
in all cases, which indicates that VPL is also effective in
multi-view training. The effect of VPL increases as Nv de-
creases, as expected. Figure 5 shows reconstructed shapes
with texture prediction when Nv = 2. When VPL is used,
the shape details become more accurate.

4.1.3 Discriminator and optimization

We discussed two types of discriminators in the last para-
graph of Section 3.2 and emphasized the importance of dis-
criminating between estimated views rather than estimated
views and real views. We validated this statement with an
experiment. We ran experiments in single-view training us-
ing the discriminator of Table 1 (d). We also tested the iter-
ative optimization used in GAN [7] instead of using a gra-
dient reversal layer [6]. However, in both cases, we were

Baseline

Proposed

Baseline

Proposed

Baseline

Proposed

(a)

(b)

(c)

(d)

(e)

Figure 5. Examples of multi-view training on ShapeNet (Nv = 2).
Panels (a–e) are the same as in Figure 4.

unable to observe any meaningful improvements from the
baseline by tuning λd. This fact indicates that the discrim-
inator in Figure 1 (d) does not work well in practice, and
discriminating estimated views is key to effective training.

4.1.4 Comparison with manually-designed priors

Our proposed internal pressure (IP) loss and some regulariz-
ers and constraints used in [15, 32] were designed using hu-
man knowledge regarding shapes. Table 4 shows a compar-
ison with VPL. This experiment was conducted in single-
view training without texture prediction.

This result shows that IP loss improves performance.
The symmetricity constraint also improves the perfor-
mance, however, some objects in ShapeNet are actually not
symmetric. By regularizing the graph Laplacian and the
edge length of meshes, although the visual quality of the
generated meshes became better, improvement of IoU was
not observed.

VPL cannot be compared with the learning-based 3D
shape priors detailed by Gwak et al. [9] and Wu et al. [34]
because these methods require additional 3D models for
training, and their methods are applicable to voxels rather
than meshes.

4.1.5 Comparison with state-of-the-arts

Our work also shows the effectiveness of view-based train-
ing. Table 5 shows the reconstruction accuracy (IoU) on the
ShapeNet dataset using our method and that presented in re-

9783

C
C

P
T

L
P
V

X

X X

X

X

X

e
n
a
l
p
r
i
a

.479

.500

.513

.483

.524

h
c
n
e
b

.266

.347

.376

.284

.378

r
e
s
s
e
r
d

.466

.583

.591

.544

.581

r
a
c

.550

.673

.701

.535

.705

r
i
a
h
c

.367

.413

.444

.356

.442

X X X .531

.385

.591

.701

.454

y
a
l
p
s
i
d

.265

.399

.425

.372

.422

.423

p
m
a
l

.454

.443

.422

.443

.441

.441

r
e
k
a
e
p
s

.524

.578

.596

.534

.561

.570

e
ﬂ
i
r

.382

.481

.479

.386

.510

a
f
o
s

.367

.464

.500

.370

.475

e
l
b
a
t

.342

.423

.436

.361

.443

e
n
o
h
p

.337

.583

.595

.529

.625

l
e
s
s
e
v

.439

.486

.485

.448

.490

l
l
a

.403

.490

.505

.434

.508

.521

.508

.444

.601

.498

.513

Table 2. IoU of single-view training on the ShapeNet dataset. VPL: proposed view prior learning. CC: class conditioning in the discrimi-
nator. TP: texture prediction.

Nv
Baseline
Proposed

2

3

5

10

20

.575

.596

.620

.641

.652

.583

.600

.624

.644

.655

Table 3. The relation between the number of views per object Nv
and the reconstruction accuracy (IoU) in multi-view training.

Prior
None
Internal pressure (IP, ours)
IP & Symmetricity [15]
IP & Regularizing graph Laplacian [15, 32]
IP & Regularizing edge length [32]
IP & View prior learning (ours)

IoU
.387

.403

.420

.403 ∗
.403 ∗
.505

Table 4. Comparison of our learning-based prior with manually-
designed shape regularizers and constraints. ∗No meaningful im-
provement was observed.

Single-view training
Our best model♯
Multi-view training
PTN [36]
NMR [18]
Our best model♯
3D supervision
3D-R2N2♯ [16]
3D-R2N2♭ [3]
OGN♭ [30]
LSM♯ [16]
Matryoshka♭ [27]
PSGN♭ [5]
VTN♭ [27]

Nv

IoU

1

.513

24

24

20

20

24

24

20

24

24

24

.574

.602

.655

.551

.560

.596

.615

.635

.640

.641

Table 5. Comparison of our method and state-of-the-art methods
on ShapeNet (3D-R2N2) dataset using IoU. Although supervision
is weaker, our proposed method outperforms the other models
trained using 3D models. ♯♭Models denoted with the same symbol
use the same rendered images.

cent papers45. Our method outperforms existing view-based

4The most commonly used dataset of ShapeNet for 3D reconstruction
was provided by Choy et al. [3]. However, we found that this dataset is not

training methods [18, 36]. The main differences between
our baseline and [18] are the internal pressure and the train-
ing dataset. Because the resolution of our training images
(224 × 224) is larger than theirs (64 × 64) and the elevation
range in the viewpoints ([−20◦, 30◦]) is wider than that of
theirs (30◦ only), more accurate and detailed 3D shapes can
be learned in our experiments.

It may be surprising that our view-based method outper-
forms reconstructors trained using 3D models. Although
view-based training is currently less popular than 3D-based
training, one can say that view-based training has much
room for further study.

4.2. Natural image dataset

If a 3D model is available, we can synthetically create
multiple views with accurate silhouette and viewpoint anno-
tations. However, in practical applications, it is not always
possible to obtain many 3D models, and datasets must be
created using natural images. In this case, generally, multi-
view training is not possible, and silhouette and viewpoint
annotations are noisy. Therefore, to measure the practicality
of a given method, it is important to evaluate such a case.

Thus, we used the PASCAL dataset preprocessed by Tul-
siani et al. [31]. This dataset is composed of images in
PASCAL VOC [4], annotations of 3D models, silhouettes,
and viewpoints in PASCAL 3D+ [35], and additional im-
ages in ImageNet [28] with silhouette and viewpoint an-
notations automatically created using [22]. We conducted
single-view training because there is only one view per ob-
ject. Because this dataset is not large, the variance in the
training results is not negligible. Therefore, we report the
mean accuracy from ﬁve runs with different random seeds.
We used the pre-trained ResNet-18 model [10] as the en-
coder as with [15, 31]. The parameters were optimized with

suitable for view-based training because there are large occluded regions
in the views owing to the narrow range of elevation in the viewpoints.
Therefore, we used a dataset by Kar et al. [16], in which images were
rendered from a variety of viewpoints. A comparison of the results from
both datasets is not so unfair because the performance of 3D-R2N2 [3] is
close in both datasets.

5This table only compares papers that report IoU on 3D-R2N2 dataset.
On other metrics and datasets, some works [8, 32] outperform PSGN [5].

9784

airplane

car

chair

mean

Category-agnostic models
DRC [31]
Baseline (s)
Proposed (s)
Baseline
Proposed
Category-speciﬁc models
CSDM [17]
CMR [15]
Baseline (s)
Proposed (s)
Baseline
Proposed

.415

.448

.450

.440

.460

.398

.46

.449

.472

.450

.475

.666

.652

.672

.640

.662

.600

.64

.679

.689

.669

.679

.247

.272

.292

.280

.443

.458

.471

.454

.296

.473

.291

n/a
.289

.303

.293

.304

.429

n/a
.472

.488

.470

.486

Table 6. IoU of single-view 3D reconstruction on the PASCAL
dataset. The difference between the proposed method and the
baseline is the use of view prior learning. (s) indicates silhouette
only training without texture prediction (λc = 0).

the Adam optimizer [19]. The architecture of decoders, dis-
criminators, and other hyperparameters are described in the
supplementary material. We constrained estimated shapes
to be symmetric, as was the case in a previous study [15].
We used Equation 4 as the view comparison function for
silhouettes.

Table 6 shows the reconstruction accuracy on the PAS-
CAL dataset. Our proposed method consistently outper-
forms the baseline and provides state-of-the-art perfor-
mance for this dataset, which validates the effectiveness
of our proposed method. Category-speciﬁc models outper-
form category-agnostic models because the object shapes
in these three categories are not very similar and multitask
learning is not beneﬁcial. The performance difference when
texture prediction is used is primarily caused by the relative
weight of the internal pressure loss.

Figure 6 shows typical improvements that can be gained
using our method.
Improvements are prominent on the
wings of the airplane, the tires of the car, and the front legs
of the chair when viewed from unobserved viewpoints.

In this experiment, internal pressure loss plays an im-
portant role because observed viewpoints are not diverse.
Figure 7 shows a reconstructed shape without internal pres-
sure. The trunk of the car is hollowed, and this hollow can-
not be ﬁlled by VPL because there are few images taken
from viewpoints such as (c–e) in the dataset.

5. Conclusion

In this work, we proposed a method to learn prior knowl-
edge of views for view-based training of 3D object recon-
structors. We veriﬁed our approach in single-view training
on both synthetic and natural image datasets. We also found
that our method is effective, even when multiple views are
available for training. The key to our success involves us-

Baseline

Proposed

Baseline

Proposed

Baseline

Proposed

(a)

(b)

(c)

(d)

(e)

Figure 6. Examples on the PASCAL dataset. Panels (a–e) are the
same as in Figure 4.

Proposed
w/o IP

(a)

(b)

(c)

(d)

(e)

Figure 7. An example of reconstruction without internal pressure
(IP). Panels (a–e) are the same as in Figure 4.

ing a discriminator with two estimated views from observed
and unobserved viewpoints. Our data-driven method works
better than existing manually-designed shape regularizers.
We also showed that view-based training works as well as
methods that use 3D models for training. The experimental
results clearly validate these statements.

Our method signiﬁcantly improves reconstruction accu-
racy, especially in single-view training. This is important
progress because it is easier to create a single-view dataset
than to create a multi-view dataset. This fact may enable
3D reconstruction of diverse objects beyond the existing
synthetic datasets. The most important limitation of our
method is that it requires silhouette and viewpoint anno-
tations. Training end-to-end 3D reconstruction, viewpoint
prediction, and silhouette segmentation would be a promis-
ing future direction.

Acknowledgment

This work was partially funded by ImPACT Program of Coun-
cil for Science, Technology and Innovation (Cabinet Ofﬁce, Gov-
ernment of Japan) and partially supported by JST CREST Grant
Number JPMJCR1403, Japan. We would like to thank Antonio
Tejero de Pablos, Atsuhiro Noguchi, Kosuke Arase, and Takuhiro
Kaneko for helpful discussions.

9785

References

[1] Martin Arjovsky and L´eon Bottou. Towards principled meth-
In ICLR,

ods for training generative adversarial networks.
2017. 4

[2] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat
Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-
lis Savva, Shuran Song, Hao Su, et al.
Shapenet: An
information-rich 3d model repository. arXiv, 2015. 5

[3] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese. 3d-r2n2: A uniﬁed approach for
single and multi-view 3d object reconstruction.
In ECCV,
2016. 1, 2, 5, 7

[4] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. IJCV, 88(2):303–338, 2010. 8

[5] Haoqiang Fan, Hao Su, and Leonidas Guibas. A point set
generation network for 3d object reconstruction from a single
image. In CVPR, 2017. 1, 2, 5, 7

[6] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. JMLR, 17(1):2096–2030, 2016. 4,
7

[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
2, 4, 7

[8] Thibault Groueix, Matthew Fisher, Vladimir G Kim,
Bryan C Russell, and Mathieu Aubry. Atlasnet: A papier-
m\ˆ ach\’e approach to learning 3d surface generation. In
CVPR, 2018. 2, 7

[9] JunYoung Gwak, Christopher B Choy, Manmohan Chan-
draker, Animesh Garg, and Silvio Savarese. Weakly super-
vised 3d reconstruction with adversarial constraint. In 3DV,
2017. 1, 2, 7

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 8, 16

[11] Eldar Insafutdinov and Alexey Dosovitskiy. Unsupervised
learning of shape and pose with differentiable point clouds.
In NIPS, 2018. 2

[12] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, 2015. 5, 17

[13] Li Jiang, Shaoshuai Shi, Xiaojuan Qi, and Jiaya Jia. Gal:
Geometric adversarial loss for single-view 3d-object recon-
struction. In ECCV, 2018. 1, 2

[14] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
ECCV, 2016. 3

[15] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and
Jitendra Malik. Learning category-speciﬁc mesh reconstruc-
tion from image collections. In ECCV, 2018. 1, 2, 3, 5, 7,
8

[16] Abhishek Kar, Christian H¨ane, and Jitendra Malik. Learning

a multi-view stereo machine. In NIPS, 2017. 1, 2, 5, 7

[17] Abhishek Kar, Shubham Tulsiani, Joao Carreira, and Jiten-
dra Malik. Category-speciﬁc object reconstruction from a
single image. In CVPR, 2015. 1, 2, 8

[18] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu-

ral 3d mesh renderer. In CVPR, 2018. 1, 2, 3, 4, 5, 7, 13

[19] Diederik Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015. 5, 8, 16

[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012. 4

[21] Aldo Laurentini. The visual hull concept for silhouette-based

image understanding. PAMI, 16(2):150–162, 1994. 5

[22] Ke Li, Bharath Hariharan, and Jitendra Malik. Iterative in-

stance segmentation. In CVPR, 2016. 8

[23] Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learning
efﬁcient point cloud generation for dense 3d object recon-
struction. In AAAI, 2018. 2

[24] Mehdi Mirza and Simon Osindero. Conditional generative

adversarial nets. arXiv, 2014. 4

[25] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks. In ICLR, 2018. 5, 17

[26] Takeru Miyato and Masanori Koyama. cgans with projection

discriminator. In ICLR, 2018. 4

[27] Stephan R Richter and Stefan Roth. Matryoshka networks:
Predicting 3d geometry via nested shape layers. In CVPR,
2018. 1, 2, 5, 7

[28] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Imagenet large
Aditya Khosla, Michael Bernstein, et al.
scale visual recognition challenge.
IJCV, 115(3):211–252,
2015. 8

[29] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.
Multi-view 3d models from single images with a convolu-
tional network. In ECCV, 2016. 2

[30] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.
Octree generating networks: Efﬁcient convolutional archi-
tectures for high-resolution 3d outputs. In ICCV, 2017. 1, 2,
5, 7

[31] Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Ji-
tendra Malik. Multi-view supervision for single-view recon-
struction via differentiable ray consistency. In CVPR, 2017.
1, 2, 3, 5, 8

[32] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei
Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh
models from single rgb images. In ECCV, 2018. 2, 3, 7

[33] Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, Bill
Freeman, and Josh Tenenbaum. Marrnet: 3d shape recon-
struction via 2.5 d sketches. In NIPS, 2017. 2

[34] Jiajun Wu, Chengkai Zhang, Xiuming Zhang, Zhoutong
Zhang, William T Freeman, and Joshua B Tenenbaum.
Learning shape priors for single-view 3d completion and re-
construction. In ECCV, 2018. 2, 7

[35] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond
pascal: A benchmark for 3d object detection in the wild. In
WACV, 2014. 8

9786

[36] Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and
Honglak Lee. Perspective transformer nets: Learning single-
view 3d object reconstruction without 3d supervision.
In
NIPS, 2016. 1, 2, 3, 5, 7

[37] B. Yang, S. Rosa, A. Markham, N. Trigoni, and H. Wen.
Dense 3d object reconstruction from a single depth view.
PAMI, pages 1–1, 2018. 2

[38] Rui Zhu, Hamed Kiani Galoogahi, Chaoyang Wang, and Si-
mon Lucey. Rethinking reprojection: Closing the loop for
pose-aware shape reconstruction from a single image.
In
ICCV, 2017. 2

9787

