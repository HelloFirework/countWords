Fully Learnable Group Convolution for Acceleration of Deep Neural Networks

Xijun Wang1,2

Meina Kan1

Shiguang Shan1,2,3

Xilin Chen1,2

1Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS),

Institute of Computing Technology, CAS, Beijing 100190, China

2University of Chinese Academy of Sciences, Beijing 100049, China

3CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai 200031, China

xijun.wang@vipl.ict.ac.cn

{kanmeina,sgshan,xlchen}@ict.ac.cn

Abstract

Beneﬁtted from its great success on many tasks, deep
learning is increasingly used on low-computational-cost de-
vices, e.g. smartphone, embedded devices, etc. To reduce
the high computational and memory cost, in this work, we
propose a fully learnable group convolution module (FLGC
for short) which is quite efﬁcient and can be embedded into
any deep neural networks for acceleration. Speciﬁcally, our
proposed method automatically learns the group structure
in the training stage in a fully end-to-end manner, lead-
ing to a better structure than the existing pre-deﬁned, two-
steps, or iterative strategies. Moreover, our method can be
further combined with depthwise separable convolution, re-
sulting in 5× acceleration than the vanilla Resnet50 on sin-
gle CPU. An additional advantage is that in our FLGC the
number of groups can be set as any value, but not necessar-
ily 2k as in most existing methods, meaning better tradeoff
between accuracy and speed. As evaluated in our experi-
ments, our method achieves better performance than exist-
ing learnable group convolution and standard group convo-
lution when using the same number of groups.

1. Introduction

Since the Alexnet proposed by Krizhevsky et al. [23]
achieved breakthrough results in the 2012 ImageNet Chal-
lenge, deeper and larger convolutional neural networks
(CNNs) have become a ubiquitous setting for better per-
formance, especially on tasks with big data [5, 26]. How-
ever, even an ordinary CNNs is usually with dozens, hun-
dreds or even thousands of layers and thousands of channels
[12, 35, 18]. Such huge parameters and high computational
cost make it insupportable on devices with limited hard-
ware resources or applications with strict latency require-
ments. In [6], Misha Denil et al. found that there is signif-
icant redundancy in CNNs, and the accuracy will not drop

even many of the network parameters are not learned or re-
moved. After that, various methods of reducing redundancy
have emerged. These methods can be roughly grouped into
two categories, post-processing methods such as pruning or
quantizing a pre-trained deep model, and efﬁcient architec-
ture design methods attempting to design fast and compact
deep network.

1.1. Post processing methods

A straightforward strategy is to post-process a pre-
trained model, such as pruning the parameters, or quantiz-
ing the model by using fewer bits.

Parameter Pruning. Some ﬁne-grained methods at-
tempt to prune the wispy connections between two neural
nodes based on its importance, and thus convert a dense
network to a sparse one [27, 11, 10, 24]. A typical one is
[11], in which Han et al. proposed to learn the importance
of each connection and then those unimportant connections
are removed to reduce the operations. Furthermore, Guo et
al. [10] proposed an on-the-ﬂy connection pruning method
named dynamic network surgery, which can avoid incorrect
pruning and make it as a continual network maintenance by
incorporating connection splicing into the whole process.
The sparse network achieved from the ﬁne-grained prun-
ing methods has much lower computation cost theoretically.
Unfortunately, there is no mature framework or hardware
for sparse network, and thus only limited speed up can be
obtained practically.

There are also some other methods attempting to do
coarse-grained pruning by cutting off the entire ﬁlters,
channels or even layers. In [14], He et al. proposed an it-
erative two-step algorithm to effectively prune each layer
by using a LASSO regression, which based on the chan-
nel selection and least square reconstruction. In [25], Li et
al. applied L1 regularization to prune ﬁlters to induce spar-
sity. More generally, Wen et al. [38] proposed a structured
sparsity learning method to reduce redundant ﬁlters, chan-
nels, and layers in a uniﬁed manner. Coarse-grained prun-

9049

(cid:9)

(cid:10)

(cid:11)

(cid:12)

(cid:13)

(cid:14)

(cid:15)

(cid:16)

(cid:26)

(cid:9)

(cid:10)

(cid:11)

(cid:12)

(cid:13)

(cid:14)

(cid:9)

(cid:10)

(cid:11)

(cid:12)

(cid:13)

(cid:14)

(cid:15)

(cid:16)

(cid:26)

(cid:9)

(cid:10)

(cid:11)

(cid:12)

(cid:13)

(cid:14)

(cid:9)

(cid:10)

(cid:11)

(cid:12)

(cid:13)

(cid:14)

(cid:15)

(cid:16)

(cid:26)

(cid:9)

(cid:10)

(cid:11)

(cid:12)

(cid:13)

(cid:14)

(cid:37)
(cid:33)
(cid:31)
(cid:34)
(cid:34)
(cid:22)
(cid:35)

(cid:10)

(cid:12)

(cid:9)

(cid:13)

(cid:14)

(cid:11)

(cid:9)

(cid:10)

(cid:11)

(cid:12)

(cid:13)

(cid:14)

(cid:15)

(cid:16)

(cid:26)

(cid:9)

(cid:10)

(cid:11)

(cid:12)

(cid:13)

(cid:14)

(cid:10)

(cid:11)

(cid:13)

(cid:26)

(cid:9)

(cid:15)

(cid:12)

(cid:13)

(cid:16)

(cid:10)

(cid:12)

(cid:9)

(cid:13)

(cid:14)

(cid:11)

(cid:1)(cid:17)(cid:3)(cid:4)(cid:18)(cid:19)(cid:20)(cid:21)(cid:17)(cid:22)(cid:4)(cid:23)(cid:19)(cid:24)(cid:25)

(cid:1)(cid:27)(cid:3)(cid:4)(cid:28)(cid:29)(cid:17)(cid:24)(cid:2)(cid:17)(cid:20)(cid:2)(cid:4)(cid:30)(cid:20)(cid:19)(cid:31)(cid:32)(cid:4)(cid:23)(cid:19)(cid:24)(cid:25)

(cid:1)(cid:23)(cid:3)(cid:4)(cid:28)(cid:33)(cid:31)(cid:34)(cid:34)(cid:22)(cid:35)(cid:18)(cid:35)(cid:29)(cid:4)(cid:30)(cid:20)(cid:19)(cid:31)(cid:32)(cid:4)(cid:23)(cid:19)(cid:24)(cid:25)(cid:4)(cid:31)(cid:24)(cid:36)(cid:29)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)

(cid:29)(cid:20)(cid:17)(cid:36)(cid:24)(cid:36)(cid:24)(cid:30)

(cid:29)(cid:35)(cid:37)(cid:29)(cid:36)(cid:24)(cid:30)

Figure 1. An overview of different group convolution mechanisms. (a) is a normal convolution. (b) is a standard group convolution, in
which input channels and ﬁlters in each group are both ﬁxed. (c) is ShufﬂeNet group convolution unit, in which input channels are ﬁxed.
(d) is our FLGC convolution, in which the grouping structure including both input channels and ﬁlters in each group is dynamically learnt.
The octagons represent the input channels and the circles represent the ﬁlters.

ing methods directly remove the ﬁlters or channels and thus
effectively accelerate the network.

Quantization. Network quantization is to reduce the
number of bits used to represent the parameters or features.
Han et al. [1] proposed a deep compression method that
ﬁrstly pruned the insigniﬁcant connections, and secondly
quantized the left connections by using weight sharing and
Huffman coding. Moreover, INQ [47] and ShiftCNN [9]
quantize a full-precision CNN model to a low-precision
model whose parameters (i.e. weights) are either zero
or powers of two. With the powers of two representa-
tion, the multiplication operations can be replaced by shift
operations which is quite efﬁcient. Besides these post-
quantization methods, there are also some methods attempt-
ing to directly train a binary network, such as BinaryCon-
nect [3], BNNs [4] and XNORNetworks [30]. As quite
fewer bits used, all these methods can obtain faster net-
works, but correspondingly the accuracy usually is signif-
icantly decreased when dealing with large networks.

Other Methods. In addition to the methods described
above, some other approaches explored how to use low-
rank factorization, knowledge distillation for deep network
acceleration. In Low-rank decomposition methods [7, 22],
the convolutional ﬁlters structured in 4D tensors are decom-
posed to lower-rank ﬁlters, which removes the redundancy
in convolution inducing fewer calculations. In knowledge
distillation methods [15, 31], the knowledge learnt from a
deep and wide network is shifted into shallower and nar-
rower ones by making the output distribution of the two
networks the same.

The post-processing methods are simple and intuitive,
but obviously have some limitations. Most above methods
are in two or multiple steps manner, the objective of the net-
work (such as classiﬁcation or detection) and the objective
of acceleration are separately optimized. Therefore, the ac-
celeration does not necessarily ensure excellent classiﬁca-
tion or detection accuracy. Besides, most pruning methods
determine the importance of a connection or layer by only
considering its magnitude and its contribution to several ad-
jacent layers, but not its inﬂuence on the whole network.
As veriﬁed in [43], pruning without considering the global
impact will result in signiﬁcant error propagation, causing
performance degeneration especially in deep networks.

1.2. Design Efﬁcient Architectures

Considering the above mentioned limitations, some re-
searchers go other way to directly design efﬁcient network
architectures, such as smaller ﬁlters, separable convolution,
group convolution, and etc.

Separable Convolution. Some early works straight-
1×1, 3×3) to re-
forwardly employ small ﬁlters (e.g.
5×5, 7×7) for accelera-
place those large ones (e.g.
tion [33, 12, 20, 18]. However, even if only with 3×3 and
1×1 ﬁlters, an ordinary deep network is still time consum-
ing, such as the ResNet50 needs about 4G MAdds1 and
VGG16 needs 16G MAdds for calculating a 224 × 224 im-
age. In order to get further acceleration, some works ex-
plore separable convolution which uses multiple 2D con-

1In this paper, MAdds refers to the number of multiplication-addition

operations.

9050

volutions to replace the time-intensive 3D convolutions. In
aspect of spatial separation, Inception V3 [37] factorizes a
h × w × c ﬁlter into two ones, i.e. one h × 1 × c ﬁlter
and another 1 × w × c ﬁlter. In aspect of channel separa-
tion, Xception [2] and MobileNets [16] employ depthwise
separable convolution. This kind of separable convolution
can speed up the computing exponentially, and thus they are
widely used in many modern networks.

Group Convolution. Separable convolution achieves
the acceleration by factorizing the ﬁlters. Differently, the
group convolution speed up the network by dividing all ﬁl-
ters into several groups, such as [21, 34, 40, 45, 41, 46].
The concept of group convolution was ﬁrst proposed in
Alexnet [23], and then it is further successfully adopted in
ResNeXt [41], making it popular in recent network design.
However, standard group convolutions do not communicate
between different groups, which restricts their representa-
tion capability. To solve this problem, in ShufﬂeNet [46],
a channel shufﬂe unit is proposed to randomly permute the
output channels of group convolution to make the output
better related to the input. In these methods, the elements
(i.e.
input channels and ﬁlters) in each group are ﬁxed
or randomly deﬁned. Furthermore, in Condensenet [17] a
learnable group convolution was proposed to automatically
select the input channels for each group.

Although the existing group convolution methods have
advanced the acceleration very effectively, there are still
several limitations to solve: 1) The ﬁlters used for group
convolution in each group are pre-deﬁned and ﬁxed, and
this hard assignment hinders its representation capability
even with random permutation after group convolution; 2)
In some works the groups are learnable, but usually are de-
signed as a tedious multiple-stage or iterative manner. In
each stage, the network from previous stage is ﬁrstly pruned
and then ﬁne-tuned to recover the accuracy.

To deal with all above limitations once for all,

in
this work we propose a fully learnable group convolution
(FLGC) method.
In our proposed FLGC, the grouping
structure including the input channels and ﬁlters in one
group is dynamically optimized. What’s more, this mod-
ule can be embedded into any existing deep network and
easily optimized in an end-to-end manner. At test time, the
learnt model is calculated similar as the standard group con-
volution which allows for efﬁcient computation in practice.
A brief comparison of different group convolution meth-
ods are shown in Figure 1. Overall, the advantages of our
method are as follows:

(1) The element including input channels and ﬁlters in
each group are both learnable, allowing for ﬂexible group-
ing structure and inducing better representation capability;
(2) The group structure in all layers are simultaneously
optimized in an end-to-end manner, rather than a multiple-
stage or iterative manner (i.e. pruning layer by layer.);

(3) The numbers of input channels and ﬁlters in each
group are both ﬂexible, while the two numbers must be di-
visible by the group number in conventional group convo-
lution.

2. Fully Learnable Group Convolution(FLGC)

In modern deep networks, the size of ﬁlters is mostly
3 × 3 or 1 × 1, and the main computational cost is from
the convolution layer. The 3 × 3 convolutions can be easily
accelerated by using the depthwise separable convolution
(DSC). And the separation of 3 × 3 convolutions will come
along with additional 1 × 1 convolutions. After DSC, the
1 × 1 convolutions contribute the major time-cost, e.g. for a
Resnet50 network, after applying DSC to the 3×3 convolu-
tions, the computational cost of 1 × 1 convolutions accounts
for 83.6% in the whole network. Therefore, how to speed
up the 1 × 1 convolution becomes an urgent problem and
attracts increasing attentions.

Since the 1 × 1 ﬁlters are non-separable, group convo-
lution becomes a hopeful and feasible solution. However,
simply applying group convolution will result in drastic pre-
cision degradation. As analyzed in [17], this is caused by
the fact that the input channels to the 1 × 1 convolutional
layer have an intrinsic order or they are far more diverse.
This implies that the hard assignment grouping mechanism
in standard group convolution is incompetent. For a better
solution, our proposed method dynamically determines the
input channels and ﬁlters for each group, forming a ﬂexible
and efﬁcient grouping mechanism.

Brieﬂy, in our FLGC the input channels and ﬁlters in one
group (i.e. the group structure) are both dynamically deter-
mined and updated according to the gradient of the overall
loss of the network through back propagation. And thus it
can be optimized in an end-to-end manner.

2.1. Method

In a deep network, the convolution layer is computed as
convolving the input feature maps with ﬁlters. Taking the
kth layer for an example, the input of the kth layer can be
1, xk
denoted as X k = {xk
C}, where C is the num-
i is the ith feature map. The ﬁlters
ber of channels and xk
of the kth layer are denoted as W k = {wk
N },
where N denotes the number of ﬁlters, i.e. number of out-
put channels, and wk
i is the ith 3D convolutional ﬁlter. The
output2 of this convolution layer is calculated as follows:

2 , · · · , wk

2, · · · , xk

1 , wk

X k+1 = W k ⊗ X k

= {wk

1 ∗ X k, wk

2 ∗ X k, · · · , wk

N ∗ X k}, (1)

where ⊗ in this work denotes the convolution between two
sets, ∗ denotes the convolution operation between a ﬁlter
and the input feature maps.

2We omit the bias b for simplicity.

9051

In group convolution, the input channels and ﬁlters are
divided into G groups respectively, denoted as X k =
{X k
G}4.
Now, X k+1 is reformulated as below:

G}3 and W k = {W k

2 , · · · , W k

2 , · · · , X k

1 , W k

1 , X k

belong to the jth group. Then the jth group of ﬁlters, i.e.
W k

j , can be represented as:

W k

j = W k ⊙ T k(:, j)T , j ∈ [1, G].

(7)

X k+1 = {W k

1 ⊗ X k

1 , W k

2 ⊗ X k

2 , · · · , W k

G ⊗ X k

G}.

(2)

As a result, the overall group convolution in Eq.(2) can

be re-formulated as follows:

Typically, in standard group convolution the input chan-
nels and ﬁlters are evenly divided into G groups in a hard
assignment manner, i.e. C
G ﬁlters in
each group. Therefore, the number of channels used in each
ﬁlter is reduced to 1
G of original ones, resulting in a accel-
eration rate as below:

G input channels and N

MAdds(W k ⊗ X k)

MAdds(PG

i=1 W k

i ⊗ X k
i )

= G.

(3)

As can be seen, this group convolution from hard as-
signment can easily bring about considerable acceleration
of G×. However, it is not necessarily a promising approach
for accuracy. Therefore, the goal of our method is to design
a fully learnable grouping mechanism, where the grouping
structure is dynamically optimized for favorable accelera-
tion as well as accuracy.

Firstly, we formulate the grouping structure in the kth
layer as two binary selection matrices for input channels
and ﬁlters respectively, denoting as Sk and T k.

The Sk is a matrix for channel selection in shape of C ×

G, with each element deﬁned as:

Sk(i, j) = 


1, if xk

i ∈ X k
j ,

0, if xk

i /∈ X k
j ,

i = [1, C]; j ∈ [1, G].

(4)

in which Sk(i, j) = 1 means the ith input channel is se-
lected into the jth group. As can be seen, the jth column of
Sk indicates which input channels belong to the jth group.
Then, X k

j can be simply represented as follows:

X k

j = X k ⊙ Sk(:, j)T , j ∈ [1, G],

(5)

where ⊙ denotes the element-wise selection operator and
the element here means ∀xk
i ∈ X k, and T denotes the trans-
pose of a vector.

Similarly, for ﬁlter selection we deﬁne a matrix T k in

shape of N × G , with each element deﬁned as:

T k(i, j) = 


1, if wk

i ∈ W k
j ,

0, if wk

i /∈ W k
j ,

i = [1, N ]; j ∈ [1, G].

(6)

in which T k(i, j) = 1 means the ith ﬁlter is selected into
the jth group. The jth column of T k indicates which ﬁlters

3X k
4W k

1

1

∪ X k
∪ W k

2

2

∪ · · · ∪ X k
G
∪ · · · ∪ W k
G

= X k
= W k

X k+1 =W k ⊗ X k
1 ⊗ X k

={W k
G}
={W k ⊙ T k(:, 1)T ⊗ X k ⊙ Sk(:, 1)T , · · · ,

2 , · · · , W k

2 ⊗ X k

G ⊗ X k

1 , W k

W k ⊙ T k(:, G)T ⊗ X k ⊙ Sk(:, G)T }.

(8)

With Eq.(8), the structure of group convolution is param-
eterized by two binary selection matrix Sk and T k. There-
fore, this parameterized group convolution can be embed-
ded in any existing deep networks with the objective as:

min

W k,S k,T k|K

k=1

1
n

n

X

i=1

L(Yi; ˆY |Xi, W k, Sk, T k),

(9)

in which Xi denotes the ith input sample, n indicates the
number of training data, Yi indicates the ith sample’s true
category label, K is the number of layers, and ˆY is the label
predicted from a network with our group convolution pa-
rameterized by W k, Sk, T k. L(, ) denotes the loss function
(e.g. cross entropy loss) for classiﬁcation or detection etc.

In the above objective, the ﬁlters W k, the group structure
including Sk and T k can be all automatically optimized ac-
cording to the overall objective function. However, binary
variables are notorious for its non-differential feature. So,
we further design an ingenious approximation to make it
differentiable for better optimization in section 2.2.

As can be seen from Eq.(9), the group structure in our
method is automatically optimized rather than manually de-
ﬁned. Furthermore, different from those methods only con-
sidering the magnitude and impact of the connection in one
or two layers, the group structure in our method is deter-
mined according to the objective loss of the whole network.
Therefore, the group structures of all layers in our method
are jointly optimized implying a superior solution.

2.2. Optimization

In Eq.(9), the ﬁlters W k can be easily optimized as most
deep networks by using the stochastic gradient descent,
while the binary parameters are hard to optimize as they
are non-differentiable. To solve this problem, we approx-
imate the Sk and T k by applying a softmax function to a
meta selection matrix to make it differentiable.

Speciﬁcally, we introduce a meta selection matrix ¯Sk for
channel selection with the same shape as Sk. And then the
softmax function is applied to each row of ¯Sk, which can
map it to (0,1) as below:

ˆSk(i, :) = sof tmax( ¯Sk(i, :)), i ∈ [1, C].

(10)

9052

Here, the meta selection matrix ¯Sk can be initialized as
Gaussian distribution or results from other methods. After
softmax, the ith row of ˆSk indicates the probability that the
ith input channel belongs to each group. So, the ith input
channel can be simply selected into the group with highest
probability. That is to say, the binary selection matrix Sk
can be approximated as:

Sk(i, j) = 


1, if ˆSk(i, j) = max( ˆSk(i, :)),

(11)

0, otherwise.

The reason of using softmax function is that with soft-
max operation the meta selection matrix ¯Sk can be updated
to make the output ˆSk approximating 0 or 1 as close as pos-
sible. As a result, the quantization error between ¯Sk and Sk
is largely narrowed.

Similarly, the binary selection matrix T k is approxi-
mated by applying softmax function on a meta selection
matrix ¯T k for ﬁlter selection as follows:

T k(i, j) = 


with

1, if ˆT k(i, j) = max( ˆT k(i, :)),

0, otherwise,

ˆT k(i, :) = sof tmax( ¯T k(i, :)), i ∈ [1, N ].

(13)

Here, the ith row of ˆT k indicates the probability that the ith
ﬁlter belongs to each group.

In summary, with the above Eq.(10), Eq.(11), Eq.(12)and
Eq.(13), the differentiation of the binary Sk and T k are
shifted to the differentiation of the meta selection variable
¯Sk and ¯T k which are non-binary, yet with small quantiza-
tion error.

Furthermore, for easy implementation, Eq.(8) is equiva-

lently transformed to the following formulation:

X k+1 ={W k ⊙ T k(:, 1)T ⊗ X k ⊙ Sk(:, 1)T , · · · ,

W k ⊙ T k(:, G)T ⊗ X k ⊙ Sk(:, G)T }

=(W k ⊙ M k) ⊗ X k,

(14)

with M k = T k(Sk)T , and the shape of M k is N × C that
is the same as W k.

Finally, the objective function is re-formulated as below:

(12)

in which η indicates the learning rate. The overall procedure
is summarized in Algorithm 1.

W k

(i,j) ← W k

(i,j) − η

∂L

∂ (cid:16)W k

(i,j) ⊙ M k

(i,j)(cid:17)

, ∀i, j ∈ I, (16)

∂L

∂L

(i,j) ← ¯Sk
¯Sk

(i,j) − η

∂ (cid:16)W k

(i,j)(cid:17)

∂ (cid:16)W k
(i,j) ⊙ M k
∂M k

(i,j)

(i,j) ⊙ M k
(i,j)(cid:17)

∂M k
∂ ˆSk

∂ ˆSk
∂ ¯Sk

(i,j)

(i,j)

(i,j)

(i,j)

¯T k
(i,j) ← ¯T k

(i,j) − η

∂ (cid:16)W k

(i,j)(cid:17)

∂ (cid:16)W k
(i,j) ⊙ M k
∂M k

(i,j)

(i,j) ⊙ M k
(i,j)(cid:17)

∂M k
∂ ˆT k

∂ ˆT k
∂ ¯T k

(i,j)

(i,j)

(i,j)

(i,j)

,

(17)

,

(18)

Algorithm 1 FLGC: solving the optimization problem in
Eq.(15) via SGD
Input: X: training data, Y: lable
Output: {W k, Sk, T k : k ∈ [1, K]}
1: Initialize W k ← msra; ¯Sk, ¯T k ← Gaussian;

Sk, T k ← 0

2: for each batch Xi do
3:

//Forward propagation:
for i = 1 → C do

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

ˆSk(i, :) ← sof tmax( ¯Sk(i, :))
Sk(i, j) ← 1 ,if ˆSk(i, j) = max( ˆSk(i, :))

end for
for i = 1 → N do

ˆT k(i, :) ← sof tmax( ¯T k(i, :))
T k(i, j) ← 1 ,if ˆT k(i, j) = max( ˆT k(i, :))

end for
M k ← T k(Sk)T
Get loss: L = L(Yi, Xi(W k ⊙ M k) + b)
//Backward propagation:
W k ← W k − η
¯Sk ← ¯Sk − η
¯T k ← ¯T k − η

∂(W k⊙M k)

∂(W k⊙M k)

∂(W k⊙M k)

∂(W k⊙M k)

∂M k

∂L

∂L

∂L

∂M k
∂ ˆS k
∂M k
∂ ˆT k

∂(W k⊙M k)

∂M k

∂ ˆS k
∂ ¯S k
∂ ˆT k
∂ ¯T k

min
W, ¯S, ¯T

1
n

n

X

i=1

L(Yi, Xi(W ⊙ M ) + b),

(15)

18: end for

where W = {W k|K

k=1}, ¯S = { ¯Sk|K

k=1}, ¯T = { ¯T k|K

k=1}.

The objective in Eq.(15) can be easily optimized as
most deep network by using the stochastic gradient descent
method, with the parameters of each layer are updated as
follows:

2.3. Inference with Index Reordering

After the group structure is learnt, the input channels and
ﬁlters usually need to be re-organized for fast inference. A
naive method is to add an index layer to re-order the input

9053

channels according to the group information, and another
index layer to re-order the ﬁlters. Then, the output channels
are re-ordered back to the original order, as shown in Fig-
ure 2(a). Unfortunately, such frequent re-order operations
on memory will signiﬁcantly increase the inference time.

Therefore, we propose an efﬁcient strategy for index re-
ordering as shown in Figure 2(b). Firstly, the ﬁlters are re-
ordered to make those ﬁlters in one group arranged together.
Secondly, considering that the input channels are also the
output channels of previous layer, we merge the index of
the output from previous layer and index of input channels
in this layer as single index to obtain correct order of input
channels. The detail is shown in Figure 2(c). As designed
like above, the operations on memory are reduced a lot and
all these re-ordering index can be obtained ofﬂine, so it is
quite efﬁcient at the inference stage.

As a result, at the inference time our FLGC can be as

efﬁcient as the standard group convolution.

3. Experiments

In this section, we investigate the effectiveness of
our proposed FLGC by embedding it into the existing
popular CNNs networks including Resnet50 [13], Mo-
bileNetV2 [32] and Condensenet [17]. Firstly, we conduct
ablation study of FLGC on CASIA-WebFace [42], and then
compare it with existing competitive methods on CASIA-
WebFace, CIFAR-10 and ImageNet (ILSVRC 2012) [5] in
terms of face veriﬁcation and image classiﬁcation.

3.1. Embedding into the state of the art CNNs

We select

three state-of-the-art architectures includ-
ing Resnet50, MobileNetV2 and CondenseNet to embed
the proposed fully learnable group convolution(FLGC) for
evaluation.

Resnet50 with FLGC. The Resnet50 is a powerful net-
work which achieves prominent accuracy on many tasks.
Nevertheless, it is quite time-consuming. As shown in Fig-
ure 3(blue line), the major computation cost falls on the 3×3

(cid:1)(cid:8)(cid:3)(cid:5)(cid:3)(cid:9)(cid:3)(cid:10)(cid:3)(cid:4)(cid:3)(cid:2)(cid:6)

(cid:1)(cid:9)(cid:3)(cid:2)(cid:3)(cid:10)(cid:6)(cid:3)(cid:7)(cid:1)(cid:5)(cid:3)(cid:8)(cid:6)(cid:3)(cid:7)(cid:1)(cid:4)(cid:6)

(cid:22)(cid:14)(cid:23)(cid:24)(cid:25)(cid:19)(cid:14)(cid:7)(cid:26)(cid:23)(cid:27)(cid:28)

(cid:22)(cid:20)(cid:29)(cid:24)(cid:14)(cid:19)(cid:28)

(cid:22)(cid:14)(cid:23)(cid:24)(cid:25)(cid:19)(cid:14)(cid:7)(cid:26)(cid:23)(cid:27)(cid:28)

(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)(cid:14)(cid:13)(cid:16)(cid:22)(cid:20)(cid:29)(cid:24)(cid:14)(cid:19)(cid:28)

(cid:22)(cid:14)(cid:23)(cid:16)(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:22)(cid:20)(cid:29)(cid:16)(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:14)(cid:18)(cid:19)(cid:13)(cid:14)(cid:19)(cid:20)(cid:12)(cid:21)

(cid:30)(cid:18)(cid:12)(cid:31)(cid:16)(cid:18)(cid:27)(cid:14)(cid:19)(cid:23)(cid:24)(cid:20)(cid:18)(cid:12)

(cid:13)(cid:14)(cid:16)(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:30)(cid:18)(cid:12)(cid:31)(cid:16)(cid:18)(cid:27)(cid:14)(cid:19)(cid:23)(cid:24)(cid:20)(cid:18)(cid:12)

(cid:22)(cid:14)(cid:23)(cid:24)(cid:25)(cid:19)(cid:14)(cid:7)(cid:26)(cid:23)(cid:27)(cid:28)

(cid:22)(cid:20)(cid:29)(cid:24)(cid:14)(cid:19)(cid:28)

(cid:22)(cid:14)(cid:23)(cid:24)(cid:25)(cid:19)(cid:14)(cid:7)(cid:26)(cid:23)(cid:27)(cid:28)

(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)(cid:14)(cid:13)(cid:16)(cid:22)(cid:20)(cid:29)(cid:24)(cid:14)(cid:19)(cid:28)

(cid:22)(cid:14)(cid:23)(cid:16)(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:22)(cid:20)(cid:29)(cid:16)(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:14)(cid:18)(cid:19)(cid:13)(cid:14)(cid:19)(cid:20)(cid:12)(cid:21)

(cid:30)(cid:18)(cid:12)(cid:31)(cid:16)(cid:18)(cid:27)(cid:14)(cid:19)(cid:23)(cid:24)(cid:20)(cid:18)(cid:12)

(cid:13)(cid:14)(cid:16)(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:22)(cid:30)

(cid:30)(cid:18)(cid:12)(cid:31)(cid:16)(cid:18)(cid:27)(cid:14)(cid:19)(cid:23)(cid:24)(cid:20)(cid:18)(cid:12)

(cid:13)(cid:14)(cid:16)(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:22)(cid:30)

(cid:1)

(cid:2)

(cid:3)

(cid:4)

(cid:5)

(cid:6)

(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:14)(cid:18)(cid:19)(cid:13)(cid:14)(cid:19)(cid:20)(cid:12)(cid:21)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:3)(cid:5)(cid:6)(cid:3)(cid:7)(cid:1)(cid:8)(cid:3)(cid:9)(cid:6)(cid:3)(cid:7)(cid:1)(cid:10)(cid:6)

(cid:3)

(cid:6)

(cid:4)

(cid:2)

(cid:1)

(cid:5)

(cid:20)(cid:24)(cid:35)(cid:7)(cid:29)(cid:23)(cid:36)(cid:14)(cid:19)(cid:7)(cid:18)(cid:25)(cid:24)(cid:27)(cid:25)(cid:24)

(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:32)(cid:20)(cid:37)(cid:9)(cid:33)(cid:24)(cid:35)(cid:7)(cid:29)(cid:23)(cid:36)(cid:14)(cid:19)
(cid:20)(cid:12)(cid:27)(cid:25)(cid:24)(cid:7)(cid:20)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:32)(cid:23)(cid:33)(cid:7)(cid:12)(cid:23)(cid:20)(cid:31)(cid:14)(cid:7)(cid:20)(cid:12)(cid:22)(cid:14)(cid:19)(cid:14)(cid:12)(cid:30)(cid:14)

(cid:32)(cid:34)(cid:33)(cid:7)(cid:18)(cid:25)(cid:19)(cid:7)(cid:20)(cid:12)(cid:22)(cid:14)(cid:19)(cid:14)(cid:12)(cid:30)(cid:14)

(cid:32)(cid:30)(cid:33)(cid:7)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:14)(cid:18)(cid:19)(cid:13)(cid:20)(cid:12)(cid:21)(cid:7)(cid:18)(cid:22)(cid:7)(cid:18)(cid:25)(cid:19)(cid:7)(cid:20)(cid:12)(cid:22)(cid:14)(cid:19)(cid:14)(cid:12)(cid:30)(cid:14)

Figure 2. Illustration of index re-ordering for efﬁcient inference.
(a) is a naive inference method, (b) is our efﬁcient inference
method, (c) is Index-Reordering unit.

convolutions, and thus we ﬁrstly use the DSC to separate the
3 × 3 convolutions following MobileNet [16]. After DSC,
there are a large number of 1 × 1 convolutions, which com-
putational cost accounts for 83.6% of the whole network.
Therefore, we further replace all the 1 × 1 layers in the net-
work with our FLGC layers. Besides, we simply double the
stride of the ﬁrst layer and add a fc layer.

MobileNetV2 with FLGC. The MobileNetV2 is a state-
of-the-art efﬁcient architecture with elaborative design.
This architecture achieves satisfactory accuracy on many
tasks with favorable computational cost, e.g.
classiﬁca-
tion, detection and segmentation. But still, the intensive
1 × 1 convolutions takes great majority of computational
cost, leaving much room for further acceleration. There-
fore, we replace those 1 × 1 convolution layers, of which
the ﬁlters number is larger than 96, with our FLGC layer.

CondenseNet with FLGC. CondenseNet proposed a
learnable group convolution which can automatically select
the input channels for each group. However, the ﬁlters in
each group are ﬁxed, and the process are designed as a te-
dious multiple-stage or iterative manner. Besides, the im-
portance of each input channel is determined according to
the magnitude of the connections between the input and the
ﬁlters, but without considering its impact on the overall net-
work, leading to a sub-optimal solution. We substitute all
the FLGC for the LGC in CondenseNet.

3.2. Ablation Study

The ablation experiment

is conducted on CASIA-
WebFace with Resnet50 in terms of face veriﬁcation. The
experimental setting on this dataset is the same as that in
section 3.1.

Firstly, we analyze the speedup with DSC and our FGLC
by comparing with the standard convolution. The time cost
of each layer in all methods are shown in Figure 3. As can
be seen, in the standard Resnet50 denoted in the blue line,
3 × 3 convolution layer is the major time-consuming part.
After applying DSC, 3 × 3 convolution time cost is signiﬁ-
cantly reduced as shown in the orange line, and the orange
line also highlights that 1 × 1 convolution layer is the ma-
jor time cost part now. By further applying FLGC, the time
cost of 1 × 1 convolution layer is successfully reduced as
shown in the green line, resulting in a quite efﬁcient archi-
tecture with comparable accuracy as the baseline(standard
Resnet50). For overall procedure, our method achieves a
signiﬁcant improvement of time cost.

Besides the efﬁciency, we further explore the accuracy
of standard group convolution and our FLGC w.r.t. differ-
ent number of groups, and the result are shown in Figure 4,
Table 1 and Table 2. As can be seen, the accuracy drops dra-
matically when the standard group convolution is applied
to the 1 × 1 convolution, mainly due to the loss of repre-
sentation capability from hard assignment. Differently, our

9054

Figure 3. The time cost of each convolutional layer in Resnet50 with different convolution mechanisms on a single CPU. The blue line is
the Standard Resnet50. The orange line is the Resnet50 with 3 × 3 convolutions replaced by DSC. The green line is the Resnet50 with
1 × 1 convolutions further replaced by FLGC.

Figure 4. Compare our FLGC with standard group convolu-
tion(SGC) in terms of face veriﬁcation accuracy of Resnet50 on
CASIA-WebFace w.r.t. different group numbers.

FLGC successfully maintains the accuracy even with large
number of groups, beneﬁtted from the fully learnable mech-
anism for grouping structure.

3.3. Comparison with competitive approaches

Results on CASIA-WebFace. The CASIA-WebFace is
a commonly used wild dataset for face veriﬁcation, consist-
ing of 494,414 face images from 10,575 subjects. All faces
are detected and aligned by using [39] and [44], and then
the detected faces are cropped out in resolution of 256×256.
This dataset is used for training. Following the mainstream
works, the well-known LFW [19] dataset is used for face
veriﬁcation evaluation. LFW includes 13,233 face images
from 5749 different identities, and the standard protocol de-
ﬁnes 3,000 positive pairs and 3,000 negative pairs for veri-
ﬁcation testing.

On this dataset, we embed the proposed FLGC into the
Resnet50 as described in section 3.1. For optimization of
our method, we initialize the meta selection matrix ¯Sk and
¯T k with Gaussian distribution, and simply set the hyperpa-
rameters of momentum as 0.9, weight decay as 5 × 10−4,

batch size as 80, and iterations as 120,000. Two versions
of our FLGC with group number as 4 and 8 are evaluated
respectively.

Our accelerated network is compared with several state-
of-the-art methods on this dataset including [42, 28, 8, 29].
All methods for comparison including ours employ soft-
max loss for optimization. The experimental results are
shown in Table 1. As can be seen, the standard Resnet50
achieves better veriﬁcation rate with giant architecture than
[42, 28, 8, 29], inevitably leading to high computational
cost. Expectedly, our modiﬁed Resnet50 achieves about
18x speed up over standard Resnet50 without accuracy
drop, which is also much faster than [42, 28, 8, 29]. In prac-
tical evaluation on single CPU(Intel(R) Xeon(R) CPU E5-
2620 v3 @2.40GHz), our modiﬁed Resnet50 runs 5x faster
than standard Resnet50, demonstrating the effectiveness of
our method.

Results on CIFAR-10. We further compare our FLGC
with other acceleration approaches on CIFAR-10 dataset.
CIFAR-10 consists of 10 classes and 60,000 images in res-
olution of 32×32 pixels. Among them, 50,000 images are
used for training and 10,000 for testing.

Since the image resolution on this dataset is small, the
modiﬁed Resnet50 in Section 3.1 used for 224×224 image
is too large and redundant. So, we replace the 7×7 convo-
lution layer with 3×3 convolution layer to suit the smaller
input images. Based on this baseline architecture, we re-
place the 1x1 convolution layers with FLGC layers, and
the number of group is set as 4. For clear comparison, two
versions of FLGC with different MAdds by changing num-
ber of ﬁlters is proposed, referred to as ResNet50-FLGC1
and ResNet50-FLGC2. Besides Resnet50, we also embed
our FLGC in the state-of-the-art acceleration architecture

9055

Table 1. Face veriﬁcation accuracy (%) and time complexity on
LFW, all the medels are trained on CASIA-WebFace. The arch-
tecture of ResNet50-FLGC and ResNet50-SGC are introduced in
Section 3.1. (SGC: standard group convolution)

Model

MAdds Params Acc

Yi et al. [42]
64layer+Softmax [28]
Ding et al. [8]
Liu et al. [29]
ResNet50(stardand)
ResNet50-SGC(G=2)
ResNet50-FLGC(G=2)
ResNet50-SGC(G=4)
ResNet50-FLGC(G=4)
ResNet50-SGC(G=8)
ResNet50-FLGC(G=8)

770M 1.75M
97.73
28460M 37.16M 97.88
98.43
2874M 3.76M
10194M 6.78M
98.71
3727M 20.69M 98.82
363M 5.35M
98.81
363M 5.35M 98.82
203M 2.70M
98.78
203M 2.70M 98.82
124M 1.37M
98.30
124M 1.37M 98.73

MobileNetV2, referred to as MobileNetV2-FLGC. For op-
timization of our method, all hyperparameters is the same
as that used on CASIA-WebFace.

On this dataset, we compare the FLGC with state-of-the-
art ﬁlter level pruning methods and the state-of-the-art ar-
chitecture MobileNetV2. The comparison results are shown
in Table 2. Comparing with the pruning methods [14, 25]
which also employ the Resnet architecture, we can get lower
classiﬁcation error with 3× fewer MAdds. Besides, our
FLGC can be ﬂexibly embedded into any efﬁcient architec-
tures such as MobileNetV2, leading to further speedup. As
can be seen in Table 2, MobileNetV2 with FLGC achieves
better accuracy w.r.t different group number, further demon-
strating the superiority of our proposed FLGC.

Table 2. Image classiﬁcation error(%) and time complexity of dif-
ferent methods on CIFAR-10.(G:group number)

Model

MAdds Params Err

ResNet56-pruned [14]
ResNet50-FLGC1(ours)
ResNet56-pruned [25]
ResNet50-FLGC2(ours)
MobileNetV2-SGC(G=2)
MobileNetV2-FLGC(G=2)
MobileNetV2-FLGC(G=3)
MobileNetV2-SGC(G=4)
MobileNetV2-FLGC(G=4)
MobileNetV2-FLGC(G=5)
MobileNetV2-FLGC(G=6)
MobileNetV2-FLGC(G=7)
MobileNetV2-SGC(G=8)
MobileNetV2-FLGC(G=8)

8.2

—
62M
0.22M 7.95
23M
0.73M 6.94
90M
44M
0.68M 6.77
158M 1.18M 6.04
158M 1.18M 5.89
122M 0.85M 5.80
103M 0.68M 6.64
103M 0.68M 5.84
0.58M 6.12
92M
0.51M 6.33
85M
0.46M 6.34
80M
0.43M 7.51
76M
0.43M 6.91
76M

Results on ImageNet. To further validate the effec-
tiveness of our proposed FLGC, we compare our FLGC
with state-of-the-art learnable group convolution which pro-

Table 3. Comparison of Top-1 and Top-5 classiﬁcation error rate
(%) with other state-of-the-art compact models on ImageNet.

Model

MAdds Params Top1 Top5

Inception V1[36]
1.0 MobileNet-224[16]
ShufﬂeNet 2x[46]
NASNet-A (N=4)[48]
NASNet-B (N=4)[48]
NASNet-C (N=4)[48]
CondenseNet (G=4)[17]
CondenseNet-SGC

1448M 6.6M 30.2 10.1
569M 4.2M 29.4 10.5
524M 5.3M 26.3 —
564M 5.3M 26.0 8.4
488M 5.3M 27.2 8.7
558M 4.9M 27.5 9.0
529M 4.8M 26.2 8.3
529M 4.8M 29.0 9.9

CondenseNet-FLGC

529M 4.8M 25.3 7.9

posed in CondenseNet [17] on ImageNet.

For a fair comparison, we used the same network struc-
ture as CondenseNet. Based on this baseline architecture,
we replace the LGC layers in CondenseNet with our FLGC
layers and standard group convolution (SGC) layers re-
spectively, and the number of group is set as 4. What’s
more, we keep the hyperparameters the same as that used
in CondenseNet. All models are trained for 120 epochs,
with a cosine shape learning rate which starts from 0.2 and
gradually reduces to 0. As can be seen in Table 3, our
FLGC achieves better accuracy than CondenseNet’s LGC
and SGC. Moreover, Our FLGC even achieves a favorable
performance compared with competitive MobileNet, Shuf-
ﬂeNet and NASNet-A.

4. Conclusion

In this work, we propose a fully learnable group convo-
lution module which is quite efﬁcient and can be embedded
into any layer of any deep neural networks for acceleration.
Instead of the existing pre-deﬁned, two-steps, or iterative
acceleration strategies, FLGC can automatically learn the
group structure at the training stage according to the over-
all loss of the network in a fully end-to-end manner, and
run as efﬁcient as standard group convolution at the infer-
ence stage. The number of input channels and ﬁlters in
each group are ﬂexible, which ensures better representa-
tion capability and well solves the problem of uneven infor-
mation distribution encountered in standard group convo-
lution. Furthermore, compared with LGC of CondenseNet
and standard group convolution, our FLGC can better main-
tain the accuracy while achieve signiﬁcant acceleration even
with large number of groups.

Acknowledgements

This work is partially supported by the National Key
R&D Program of China (No. 2017YFA0700800), Natural
Science Foundation of China (Nos. 61650202, 61772496
and 61532018).

9056

References

[1] W. Chen, J. Wilson, S. Tyree, K. Weinberger, and Y. Chen.
Compressing neural networks with the hashing trick. In In-
ternational Conference on Machine Learning (ICML), pages
2285–2294, 2015.

[2] F. Chollet. Xception: Deep learning with depthwise separa-
ble convolutions. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 1251–1258, 2017.

[3] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In Advances in Neural Information Processing
Systems (NIPS), pages 3123–3131, 2015.

[4] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and
Y. Bengio. Binarized neural networks: Training deep neu-
ral networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016.

[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 248–255, 2009.

[6] M. Denil, B. Shakibi, L. Dinh, N. De Freitas, et al. Pre-
dicting parameters in deep learning. In Advances in Neural
Information Processing Systems (NIPS), pages 2148–2156,
2013.

[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-
gus. Exploiting linear structure within convolutional net-
works for efﬁcient evaluation. In Advances in Neural Infor-
mation Processing Systems (NIPS), pages 1269–1277, 2014.
[8] C. Ding and D. Tao. Robust face recognition via multimodal
deep face representation. IEEE Transactions on Multimedia
(TMM), pages 2049–2058, 2015.

[9] D. A. Gudovskiy and L. Rigazio. Shiftcnn: Generalized low-
precision architecture for inference of convolutional neural
networks. arXiv preprint arXiv:1706.02393, 2017.

[10] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for
efﬁcient dnns. In Advances in Neural Information Process-
ing Systems (NIPS), pages 1379–1387, 2016.

[11] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights
and connections for efﬁcient neural network. In Advances in
Neural Information Processing Systems (NIPS), pages 1135–
1143, 2015.

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 770–778, 2016.
[13] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks.
In European conference on com-
puter vision (ECCV), LNCS 9908, Part IV, pages 630–645,
2016.

[14] Y. He, X. Zhang, and J. Sun. Channel pruning for accel-
In IEEE International

erating very deep neural networks.
Conference on Computer Vision (ICCV), 2017.

[15] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[16] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. arXiv preprint arXiv:1704.04861, 2017.

[17] G. Huang, S. Liu, L. Van der Maaten, and K. Q. Weinberger.
Condensenet: An efﬁcient densenet using learned group con-
volutions. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2018.

[18] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2017.

[19] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller. La-
beled faces in the wild: A database forstudying face recog-
nition in unconstrained environments. In Workshop on faces
in’Real-Life’Images: detection, alignment, and recognition,
2008.

[20] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and< 0.5 mb model size. arXiv
preprint arXiv:1602.07360, 2016.

[21] Y. Ioannou, D. Robertson, R. Cipolla, A. Criminisi, et al.
Deep roots: Improving cnn efﬁciency with hierarchical ﬁlter
groups. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017.

[22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up
convolutional neural networks with low rank expansions. In
British Machine Vision Conference (BMVC), 2014.

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems (NIPS),
pages 1097–1105, 2012.

[24] V. Lebedev and V. Lempitsky. Fast convnets using group-
wise brain damage. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2554–2564, 2016.

[25] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf.
Pruning ﬁlters for efﬁcient convnets. In International Con-
ference on Learning Representations (ICLR), 2017.

[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European Conference on Com-
puter Vision (ECCV), LNCS 8693, Part V, pages 740–755,
2014.

[27] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.
Sparse convolutional neural networks. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
806–814, 2015.

[28] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
Sphereface: Deep hypersphere embedding for face recogni-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017.

[29] W. Liu, Y. Wen, Z. Yu, and M. Yang. Large-margin soft-
max loss for convolutional neural networks. In International
Conference on Machine Learning (ICML), pages 507–516,
2016.

[30] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In European Conference on Computer Vision
(ECCV), LNCS 9908, Part IV, pages 525–542, 2016.

[31] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv
preprint arXiv:1412.6550, 2014.

9057

[47] A. Zhou, A. Yao, Y. Guo, L. Xu, and Y. Chen. Incremen-
tal network quantization: Towards lossless cnns with low-
precision weights. In International Conference on Learning
Representations (ICLR), 2017.

[48] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning
transferable architectures for scalable image recognition. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 8697–8710, 2018.

[32] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
Chen. Mobilenetv2: Inverted residuals and linear bottle-
necks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 4510–4520, 2018.

[33] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[34] K. Sun, M. Li, D. Liu, and J. Wang. Igcv3: Interleaved low-
rank group convolutions for efﬁcient deep neural networks.
In British Machine Vision Conference (BMVC), 2018.

[35] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI Conference on Artiﬁcial
Intelligence (AAAI), 2017.

[36] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions.
In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015.

[37] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2818–2826, 2016.

[38] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
structured sparsity in deep neural networks. In Advances in
Neural Information Processing Systems (NIPS), pages 2074–
2082, 2016.

[39] S. Wu, M. Kan, Z. He, S. Shan, and X. Chen. Funnel-
structured cascade for multi-view face detection with
alignment-awareness. Neurocomputing, 2017.

[40] G. Xie, J. Wang, T. Zhang, J. Lai, R. Hong, and G.-J. Qi.
Interleaved structured sparse convolutional neural networks.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 8847–8856, 2018.

[41] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggre-
gated residual transformations for deep neural networks. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 5987–5995, 2017.

[42] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-
tation from scratch. arXiv preprint arXiv:1411.7923, 2014.

[43] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han,
M. Gao, C.-Y. Lin, and L. S. Davis. Nisp: Pruning networks
using neuron importance score propagation. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2018.

[44] J. Zhang, M. Kan, S. Shan, and X. Chen. Occlusion-free
face alignment: deep regression networks coupled with de-
corrupt autoencoders.
In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 3428–3437,
2016.

[45] T. Zhang, G.-J. Qi, B. Xiao, and J. Wang. Interleaved group
In IEEE International Conference on Com-

convolutions.
puter Vision (ICCV), 2017.

[46] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An ex-
tremely efﬁcient convolutional neural network for mobile de-
vices. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 6848–6856, 2017.

9058

