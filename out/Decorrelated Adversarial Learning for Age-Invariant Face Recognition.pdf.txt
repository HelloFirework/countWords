Decorrelated Adversarial Learning for Age-Invariant Face Recognition

Hao Wang, Dihong Gong, Zhifeng Li, Wei Liu

Tencent AI Lab

hawelwang@tencent.com, gongdihong@gmail.com, michaelzfli@tencent.com, wl2223@columbia.edu

Abstract

There has been an increasing research interest in age-
invariant face recognition. However, matching faces with
big age gaps remains a challenging problem, primarily due
to the signiﬁcant discrepancy of face appearance caused
by aging. To reduce such discrepancy, in this paper we
present a novel algorithm to remove age-related compo-
nents from features mixed with both identity and age infor-
mation. Speciﬁcally, we factorize a mixed face feature into
two uncorrelated components: identity-dependent compo-
nent and age-dependent component, where the identity-
dependent component contains information that is useful
for face recognition. To implement this idea, we propose
the Decorrelated Adversarial Learning (DAL) algorithm,
where a Canonical Mapping Module (CMM) is introduced
to ﬁnd maximum correlation of the paired features gener-
ated by the backbone network, while the backbone network
and the factorization module are trained to generate fea-
tures reducing the correlation. Thus, the proposed model
learns the decomposed features of age and identity whose
correlation is signiﬁcantly reduced. Simultaneously, the
identity-dependent feature and the age-dependent feature
are supervised by ID and age preserving signals respec-
tively to ensure they contain the correct information. Ex-
tensive experiments have been conducted on the popular
public-domain face aging datasets (FG-NET, MORPH Al-
bum 2, and CACD-VS) to demonstrate the effectiveness of
the proposed approach.

1. Introduction

With the impressive advancement driven by deep learn-
ing [22, 36, 17, 47], current face recognition methods[37,
39, 38, 46, 42, 40, 28, 10] have achieved excellent perfor-
mance. Many of these models are even more accurate than
humans in various scenarios. However, identifying faces
across a wide range of ages remains under-exploring.

Recently, modern advances [41, 46, 28, 42, 40, 10] in-
troduce the margin-based metrics and normalization mech-
anism to train the models in order to improve the face recog-

Figure 1. We show a typical example for AIFR, where the intra-
identity distance is greater than the inter-identity distance due to
the large age variations. As a result, many current face recognition
systems fail to identify faces across big age gaps.

nition performance. However, most of these methods usu-
ally lack the discriminating power for face identiﬁcation in
the scenario of Age Invariant Face Recognition (AIFR). The
crucial challenge for AIFR is subject to the signiﬁcant dis-
crepancy resulting from the aging process. Figure 1 shows
an example that face images have great variations within the
same identity across different ages, while those of different
identities share similar age-related information. As a result,
those faces with big age gaps serve as hard examples that the
current face recognition systems cannot identify correctly.
In particular, the intra-identity distance is increasing larger
if there are more faces of the child and the elderly.

In the meanwhile, increasing research attentions have
been attracted to the age-invariant face recognition (AIFR).
Recent research studies on AIFR mainly focus on the de-
sign of either generative models or discriminative models.
The generative methods [12, 23, 32] propose to synthesize
face images of different ages to assist the face recognition.
Very recently, several studies [53, 2, 11] aim at improving
the quality of generated aging faces by utilizing the power-
ful GAN-based models. However, accurately modeling the
aging process is difﬁcult and complicated. The unstable ar-

3527

Intra-identity distanceInter-identity distancesarial training, we wish the xid and xage will be sufﬁciently
uncorrelated, and the age information in xid can be signiﬁ-
cantly reduced.

Our major contributions are summarized as follows:
1. We propose a novel Decorrelated Adversarial Learn-
ing (DAL) algorithm based on the linear feature factoriza-
tion, in order to regularize the learning of decomposed fea-
tures.
In this way, we wish to capture the ID-preserving
while age invariant features for AIFR. To the best of our
knowledge, this is the ﬁrst work to introduce decorrelated
adversarial feature learning to AIFR.

2. We present the Batch Canonical Correlation Analysis
(BCCA), an extension of CCA in the fashion of stochastic
gradient decent optimization. The proposed BCCA can be
integrated to the deep neural networks for correlation regu-
larization.

3. The proposed method has signiﬁcantly improved the
state-of-the-art performance on the AIFR datasets includ-
ing MORPH Album2[34], FG-NET[1] and CACD-VS[5],
which strongly demonstrates its effectiveness.

2. Related Work

Age-Invariant Face Feature Learning. Many prior
studies[15, 24, 26, 7, 25, 27, 5, 6, 13] in the literature ex-
tracted hand-craft features with heuristic methods. For ex-
ample, the [25] developed a multi-feature discriminant anal-
ysis method with local feature descriptions. The [13] pro-
posed the hidden factor analysis (HFA) to model the fea-
ture factorization and reduce the age variations in identity-
related features. The [15] introduced an effective maximum
entropy feature descriptor and a robust identity matching
framework for AIFR. Several recent methods [45, 55, 43]
are mainly based on deep neural networks. The [45] devel-
oped the Latent Factor guided Convolutional Neural Net-
work (LF-CNN) to improve the HFA. The [55] introduced
the Age Estimation guided CNN (AE-CNN) method for
AIFR. The OE-CNN [43] proposed the orthogonal embed-
ding decomposition such that the identity information is en-
coded in the angular space while the age information is rep-
resented in the radial direction. Our work presents a DAL
algorithm with the linear residual decomposition.

Canonical Correlation Analysis. Canonical Correla-
tion Analysis (CCA) [18] is a well-known algorithm to
measure the linear relationship between two multidimen-
sional variables. Some previous works have introduced this
method to face recognition in various scenarios. For exam-
ple, the [49] proposed a 2D-3D face matching method using
the CCA. The [14] developed a multi-feature CCA method
for face-sketch recognition. Compared to these typical CCA
based methods, our work presents the extension of CCA to
deep neural network as a regularization method for AIFR.

Adversarial Approaches. Generative adversarial net-
works (GAN) [16] have shown effective in various gen-

3528

Figure 2. The face features are decomposed into the identity-
dependent component and the age-dependent component. Only
the identity features participate the testing of face recognition.

tifacts in the synthesized faces can signiﬁcantly affect the
performance of face recognition. In contrast, discriminative
methods draw increasing interest in recent studies. For ex-
ample , the [13] separates the identity-related information
and the age-related information through the hidden factor
analysis (HFA). The [45] is based on similar analysis and
extends the HFA to the deep learning framework. More re-
cently, the OE-CNN [43] presents the orthogonal feature de-
composition to solve the AIFR. According to all these stud-
ies, feature decomposition plays a key role in invariant fea-
ture learning under the assumption that facial information
can be perfectly modeled by the decomposed components.
However, the decomposed components practically have la-
tent relationship with each other and the identity-dependent
component may still contain age information.

In this paper, we introduce a deep feature factoriza-
tion learning framework that factorizes the mixed face fea-
tures into two uncorrelated components: identity-dependent
component (xid) and age-dependent component (xage).
Figure 2 illustrates our feature factorization schema. We
implement such factorization through a residual mapping
module inspired by [4]. This means that, the age-dependent
embeddings are encoded through a residual mapping func-
tion xage = R(x). We have the following formulation:
x = xid + R(x), where x is the initial face feature, and
xid is the identity-dependent feature.

To reduce the mutual variations in the decomposed
components, we propose a novel Decorrelated Adversarial
Learning (DAL) algorithm that adversarially minimizes the
correlation between xid and xage. Speciﬁcally, a Canon-
ical Mapping Module is introduced to ﬁnd maximum cor-
relations between xid and xage, while the backbone and
factorization module aims to reduce the correlation. In the
meanwhile, xid and xage are learned by the identity and
age classiﬁcation signals respectively. Through the adver-

Original feature spaceIdentity feature spaceAge feature spaceerative tasks, such as face aging [53, 2, 11], face super-
resolution [51, 8], etc. Besides, the adversarial networks has
also been explored to the improve the discriminative mod-
els. For example, the [3] utilized GAN to generate high-
resolution of small faces in order to improve face detection.
The [9] developed an adversarial UV completion frame-
work (UV-GAN) to solve the pose invariant face recog-
nition problem. The [29] proposed to learn the identity-
distilled features and the identity-dispelled features in an
adversarial autoencoder framework. The [54] proposed an
adversarial network to generate hard triplet feature exam-
ples. In this work, we propose a decorrelated adversarial
learning method to signiﬁcantly minimize the correlation
between the decoupled components of identity and age, thus
the identity-dependent features are age invariant.

3. Method

3.1. Feature Factorization

As faces contain intrinsic identity information and age
information, they can be jointly represented by the identity-
dependent features and the age-dependent features. Mo-
tivated by this, we design a linear factorization module
that decomposes the initial features into these two unre-
lated components. Formally, given an initial feature vector
x ∈ Rd that extracted from an input image p by a backbone
CNN F (i.e, x = F(p)), we deﬁne the linear factorization
as follows:

x = xid + xage,

(1)

where xid denotes the identity-dependent component, and
xage denotes the age-dependent component. We design
a deep residual mapping module similar to [4] to imple-
ment this. Speciﬁcally, we obtain the age-dependent feature
through a mapping function R, and the residual part is re-
garded as the identity-dependent feature. We refer to this as
Residual Factorization Module (RFM), which is formulated
as:

xage = R (x),

xid = x − R (x).

(2)

At testing stage, only the identity-dependent features are
used for face recognition. It is desirable that xid encodes the
identity information while xage draws the age variations.
We simultaneously put the identity discriminating signal
and the age discriminating signal onto these two decoupled
features to respectively supervise the multi-task learning of
these two components. Figure 3 shows the overall frame-
work of our work. The resnet-like backbone extracts the
initial features, upon which we build the residual module
for feature factorization. Based on such factorization, we
propose the Decorrelated Adversarial Learning, which is in-
troduced in the following section.

Figure 3. An overview of the proposed method. The initial features
are extracted by backbone net, followed by the residual factoriza-
tion module. The two factorized components xid and xage are
then used for classiﬁcation and DAL regularization.

3.2. Decorrelated Adversarial Learning

Through feature factorization, it is crucial for AIFR that
the xid should be identity preserving and necessarily age-
invariant. Unfortunately, the xid and xage practically have
latent relationship with each other. For example, xid and
xage may have high linear correlation with each other.
Thus, the xid may partially involve the age variation, which
leads to negative effect on face recognition. On the other
hand, the xid and xage should be mutually uncorrelated to
force the non-trivial learning such that they both improve
themselves.

To this end, we design a regularization algorithm that is
helpful to reduce the correlation between the decomposed
features, namely Decorrelated Adversarial Learning (DAL).
The DAL basically calculates the canonical correlation be-
tween the paired features of the decomposed components.

Formally, given paired features xid, xage, we design
a linear Canonical Mapping Module (CMM) that maps
xid, xage to the canonical variables vid, vage:

∀t ∈ {id, age} : vt = C(xt) = wT

t xt,

(3)

where the wid, wage are the learning parameters for canon-
ical mapping. After that, we deﬁne the canonical correlation
as:

ρ =

Cov(vid, vage)

pVar(vid)Var(vid)

.

(4)

Based on such deﬁnition, we ﬁrst ﬁnd maximum of |ρ| by
updating CMM with respect to wid, wage, and then try to
reduce the correlation by training the backbone and RFM.
That is, on the one hand, we freeze F , R and train C in
the canonical correlation maximizing process. On the other
hand, we update F , R with C ﬁxed in the feature correlation
minimizing process. Obviously, they compete with each
other playing a two-player min-max game during the ad-
versarial training procedure. In this way, our goal is to min-
imize the correlation between xid, xage by always decreas-
ing their maximum canonical correlation. In other words,

3529

BackboneInitial FeatureID DiscriminatorDAL RegularizerInput Image-1Age DiscriminatorCanonical Mapping ModuleResidual Factorization ModuleIdentity-dependent FeatureAge-dependent FeatureFRCCthe optimal feature projections having maximum correla-
tion act as the primary target to be decorrelated. Thus, xid
and xage learns continuously to have small correlation and
ﬁnally they are signiﬁcantly uncorrelated.

Overall, the objective function for DAL is formulated as:

LDAL = min
F ,R

max

C

(|ρ(C(F(p) − R (F(p)), C(R (F(p)))|)

(5)

3:

We believe the strong decorrelation enhanced by DAL will
encourage the xid and xage to be sufﬁciently invariant with
each other. Importantly, this will improve robustness of xid
for age-invariant face recognition.

3.3. Batch Canonical Correlation Analysis

In contrast to the typical canonical correlation analysis
(CCA) methods, our work introduces the canonical correla-
tion Analysis (BCCA) based on stochastic gradient decent
(SGD) optimization. Since the correlation statistics on the
entire dataset is practically impossible, we follow similar
strategy of batch normalization [20] to compute the correla-
tion statistics based on mini-batches. Thus, it naturally suits
the deep learning framework.

Given a mini-batch size of m, we have two sets of
} and Bage =
}. Thus, the canonical correlation can be written

the decomposed features: Bid = {x1,...,m
{x1,...,m
as:

age

id

1

m Σm

ρ =

age − µage)

id − µid)(vi
id + ǫ qσ2

i=1(vi
pσ2
id are the mean and variance of vid re-
age. The ǫ is a con-

age + ǫ

(6)

.

Here, the µid and σ2
spectively, and similarly for µage and σ2
stant parameter for numerical stability.

Equation 6 serves as the objective function for BCCA
and we leverage the SGD based algorithm to optimize it.
Note that the canonical correlation |ρ| is demanded to be
necessarily maximized when updating the C., while being
minimized when training the F , R. The derivation of gra-
dients are:

∂ρ
∂vi
id

∂ρ
∂vi

age

=

=

1
m

1
m

vi
age − µage
(
id + ǫ qσ2
pσ2
vi
id − µid
id + ǫ qσ2

(
pσ2

age + ǫ

age + ǫ

−

−

(vi

id − µid) · ρ
σ2
id + ǫ

),

(vi

age − µage) · ρ

σ2

age + ǫ

).

(7)

Thus, the optimization consists of a forward propagation
that outputs the ρ, and a backward propagation that calcu-
late the gradients for updating. The detailed learning algo-
rithm of BCCA is described in Algorithm 1.

Algorithm 1 Learning algorithm of BCCA for each itera-
tion.
Input: Bid = {x1,...,m
Output: the canonical correlation ρ for forward pass; the

}; Bage = {x1,...,m

age

};

id

gradients for backward pass.

1: for each t ∈ {id, age} do
2:

CMM forward: vi
t = wT
Compute means: µt = 1
Compute variances: σ2

t xi
m Σm
t = 1

t for i = 1 . . . m;
i=1vi
t;
i=1(vi
m Σm

t − µt)2;

4:
5: end for
6: Forward propagation: Compute ρ with Equation 6.
7: for each t ∈ {id, age} do
8:

Compute ∂ρ
∂vt
CMM backward: ∂L
∂xi
t
CMM backward: ∂L
∂wi
t

with Equation 7;
∂L
∂vi
t
∂L
∂vi
t

= wi
t
= xi
t

9:

10:

; for i = 1 . . . m;

; for i = 1 . . . m;

11: end for

3.4. Multi task Training

In this section, we describe the multi-task training strat-
egy to supervise the learning of the decomposed features.
As shown in Figure 3, there are three basic supervision
modules: age discriminator, identity discriminator and DAL
regularizer.

Age Discriminator. For the learning of age information,
we feed xage into an age discriminator to ensure the age
discriminating information. Since age labels are rough with
uncertain noises in practice, we follow [13, 45] and per-
form classiﬁcations on ages by dividing them into different
groups. We use the softmax layer with cross-entropy loss
for the age classiﬁcation.

Identity Discriminator. Following the recent [42, 40],
we utilize the CosFace loss to supervise the learning of xid
and ensure the identity-preserving information. The Cos-
Face loss is formulated as:

,

escos(θj,i)

LID =

1
N X

i

− log

es(cos(θyi ,i)−m)
es(cos(θyi ,i)−m) + Pj6=yi

kxi

WT
j

id
idk

is the cosine

kWjk · xi

(8)
where N is the number of identities, yi is the correspond-
ing identity label, cos(θj, i) =
of angle between the i-th feature xi
id and the j-th weight
vector Wj of the classiﬁer. The m a constant margin term
controlling the cosine margin and the s is a constant scal-
ing factor s. The CosFace loss aims to introduce much
strict constraints to the identity classiﬁcation such that the
learned features are encouraged to be separated by a margin
between different identities. A properly large m will en-
courage powerful discriminating information in the learned
features for face recognition.

DAL Regularizer. The proposed DAL regularization

3530

also participants the joint supervision to guide the feature
learning such that the correlations between the paired de-
composed features can be signiﬁcantly reduced. Through
the joint supervision, the model simultaneously learns to
encourage both the discriminating power of xid, xage, and
decorrelation information between of these two decom-
posed components.

In summary, the training is supervised by the following

combined multi-task loss:

L = LID(xid) + λ1LSM (xage) + λ2LDAL(xid, xage),
(9)
where LID denotes the CosFace loss, LSM denotes the soft-
max with cross-entropy loss, λ1 and λ2 are scalar hyper-
parameters to balance these three losses.
In the testing
phase, we extract the identity-dependent features xid for
AIFR evaluations.

3.5. Discussion

The proposed method has the following advantages.
First, the DAL regularization on features is helpful to en-
courage the uncorrelated and co-invariant information be-
tween the decomposed components. Related works such
as HFA[13], LF-CNN[45] and OE-CNN[43] have neglected
the underlying correlation. Instead, we aim to minimize the
classiﬁcation error as well as the correlation effect simulta-
neously. Second, the BCCA provides an extension of CCA
that is inserted to the deep learning framework such that the
entire model can be trained in an end-to-end process. Fi-
nally, our method can be easily generalized to other compo-
nents factorization model, such as pose, illumination, emo-
tion, etc. To the best of our knowledge, we are the ﬁrst to
develop the decorrelated adversarial regularization frame-
work to AIFR.

4. Experiments

4.1. Implementation Details

Network Architecture.

(1) Backbone: our backbone
network is a 64-layer CNN similar to [43] .
It consists
of 4 stages with respectively 3, 4, 10, 3 stacked residual
blocks. Every residual block has 3 stacked units of “3x3
Conv + BN + ReLU”. Finally, a FC layer outputs the initial
face features of 512 dimension. (2)Residual factorization
Module (RFM): the initial face features are mapped to form
the age-dependent feature through 2 “FC +ReLU”, and the
residual part is regarded as the identity-dependent feature.
(3) Age discriminator: we stack 3 “FC +ReLU” upon xage,
and perform age classiﬁcation. (4) Identity discriminator:
we directly use xid for identiﬁcation by CosFace loss. (5)
DAL regularizer: we feed the xage and xid into the FC lay-
ers respectively and output their linear combinations, which
are then used for the BCCA calculation and optimization.

y
c
n
e
u
q
e
r
F

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0~12

13~18 19~25 26~35 36~45 46~55 56~65 >=66

Age Group

Figure 4. The age distribution of our small training dataset. It
contains 0.5M face images covering large age variations.

Data Preprocessing. We use MTCNN [52] to detect
face areas and facial landmarks on both the training and
testing sets. Then, similarity transformation is performed
according to the 5 facial key points (two eyes, nose and two
mouth corners) in order to crop the face patch to 112×96
. Finally, each pixel ([0,255]) of the cropped face patch is
normalized by subtracting 127.5 then divided by 128.

Training Details. Our training data includes the Cross-
Age Face (CAF) dataset provided by [43] and other com-
mon face datasets such as CASIA-WebFace [50], VGG
Face [33] and celebrity+ [30].
It totally contains about
1.7M images from 19.9k individuals, which is similar to
[43]. Meanwhile, we build a subset containing about 0.5M
images from 12k individuals following [43] in order to con-
duct fair experimental comparisons. We refer to this subset
as small training dataset and our whole training dataset as
large training dataset for clarify. We adopt the pre-trained
age estimation model [35] to generate predicted age labels
for the face images of the entire training set. Note that
only those predicted ages with relatively high conﬁdence
(i.e. more likely to be true label) are considered valid and
will participate the age-classiﬁcation. After that, the pre-
dicted ages are divided into 8 groups: 0-12, 13-18, 19-25,
26-35, 36-45, 46-55, 56-65, ≥ 66. The grouped age labels
are then used for the age-classiﬁcation training. The joint
supervision in Equation 9 guides the DALtraining process
in an adversarial manner. More speciﬁcally, in an adversar-
ial loop, we alternately run the canonical correlation maxi-
mizing process for 20 iterations and then change to feature
correlation minimizing process for 50 iterations. The em-
pirically setting of hyper-parameters λ1 and λ2 in Equation
9 are: λ1 = 0.1, λ2 = 0.1, m = 0.35, s = 64. All our
experiment models are trained through stochastic gradient
descent (SGD), with batch size of 512. The whole train-
ing procedure is about 40-th epochs and the learning rate is

3531

y
c
n
e
u
q
e
r
F

y
c
n
e
u
q
e
r
F

0.12

0.1

0.08

0.06

0.04

0.02

0

0

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

0

Age Group 1  (0~12)

w/o DAL
DAL

Age Group 2  (13~18)

w/o DAL
DAL

0.035

0.03

0.025

0.02

0.015

0.01

0.005

y
c
n
e
u
q
e
r
F

0.2

0.4

0.6

0.8

1

Cosine

0

0

0.2

0.4

0.6

0.8

1

Cosine

Age Group 5  (36~45)

w/o DAL
DAL

Age Group 6  (46~55)

w/o DAL
DAL

0.03

0.025

0.02

0.015

0.01

0.005

y
c
n
e
u
q
e
r
F

y
c
n
e
u
q
e
r
F

0.02

0.018

0.016

0.014

0.012

0.01

0.008

0.006

0.004

0.002

0

0

0.035

0.03

0.025

0.02

0.015

0.01

0.005

y
c
n
e
u
q
e
r
F

Age Group 3  (19~25)

w/o DAL
DAL

0.2

0.4

0.6

0.8

1

Cosine

Age Group 7  (56~65)

w/o DAL
DAL

0.06

0.05

0.04

0.03

0.02

0.01

y
c
n
e
u
q
e
r
F

0

0

0.25

0.2

0.15

0.1

0.05

y
c
n
e
u
q
e
r
F

Age Group 4  (26~35)

w/o DAL
DAL

0.2

0.4

0.6

0.8

1

Cosine

Age Group 8  (>=66)

w/o DAL
DAL

0.2

0.4

0.6

0.8

1

Cosine

0

0

0.2

0.4

0.6

0.8

1

Cosine

0

0

0.2

0.4

0.6

0.8

1

Cosine

0
0.1

0.2

0.3

0.4

0.5
Cosine

0.6

0.7

0.8

Figure 5. The distribution of the cosine similarity between features and their class center at different age groups. Our DAL model
consistently increases the cosine similarity compared against the baseline model without DAL across all the age groups, which demonstrates
the effectiveness of our method to encourage less intra-identity variations. Best viewed in colors.

Model

FG-NET

FG-NET

(MF1)

(MF2)

Baseline

+Age

+Age+DAL

58.85%
55.86%
55.84%
58.64%
57.92% 60.01%

FG-NET
(leave-
one-out)

93.4%
93.6%
94.5%

MORPH
Album 2

CACD

-VS

99.07%
98.21%
98.11%
99.05%
98.93% 99.40%

Table 1. Comparison of our method against the baseline models.
The evaluation results are rank-1 face identiﬁcation rate on FG-
NET, under protocols of MF1, MF2 and leave-one-out.

initially set to 0.1 and reduced by a factor of 0.1 at 22-th,
33-th, 38-th epoch.

Testing Details. We conduct evaluation experiments on
the well-known public AIFR face datasets: FG-NET[1],
MORPH Album 2[34] and CACD-VS[5].
In the testing
process, we extract the identity-dependent features and con-
catenate features of the original image and the ﬂipped im-
age to form the ﬁnal representation. The cosine similarity
of these representations are then used to conduct face veri-
ﬁcation and identiﬁcation.

4.2. Ablation Study

In this subsection, we study the different variants of the

proposed models to show the effectiveness of our method.

Visualization of Cosine Similarity. For a better under-
standing of the DAL and its ability to improve the identity-
preserving information, we conduct an experiment to vi-
sualize the cosine similarities across different age groups.

Given the learned identity-dependent features xid, we ﬁrst
calculate their class centers by clustering every identity in
the identity feature space, and then compute the cosine sim-
ilarity between each sample and its class center. After that,
we plot the distribution of cosine similarity across differ-
ent age groups. In this study, we conduct such visualiza-
tion analysis on the small training dataset which contains
0.5M face images covering various age differences. Fig-
ure 4 shows the age distribution of this dataset. We present
a comparison between the “w/o DAL” model (trained by
the joint supervision signals of age and identity but without
DAL) and our proposed DAL model. As shown in Figure
5, compared against the “w/o DAL” model, the DAL model
consistently increases the cosine similarity between xid and
its class center across all the age groups. This observation
proves that our method encourages features to have small
intra-identity variations and thus the samples of the same
identity but different ages are pulled together in the feature
space. Thus, the discriminating power of the learned iden-
tity features can be effectively improved by the proposed
DAL method.

Quantitative Evaluation. To show the impact of the
joint learning framework with our proposed DAL method,
we conduct the ablative evaluations on several public AIFR
datasets including FG-NET, MORPH Album 2 and CACD-
VS. Moreover, we also test our models on FG-NET follow-
ing the protocols of Megaface challenge 1 (MF1) [21] and
Megaface challenge 2 (MF2) [31]. Both the MF1 and the

3532

Method

HFA [13]
CARC [5]
MEFA [15]

MEFA+SIFT+MLBP [15]

LPS+HFA [24]
LF-CNNs [45]

OE-CNNs

Ours

GSM [26]

AE-CNNs [55]
OE-CNNs [43]

Ours

#Test Subjects

Rank-1

Method

10,000
10,000
10,000
10,000
10,000
10,000
10,000
10,000
3,000
3,000
3,000
3,000

91.14%
92.80%
93.80%
94.59%
94.87%
97.51%
98.55%
98.93%
94.40%
98.13%
98.67%
98.97%

High-Dimensional LBP [7]

HFA [13]
CARC [5]

LF-CNNs [45]

Human, Average [6]
Human, Voting [6]

Softmax

A-Softmax

OE-CNNs [43]

Ours

Acc.

AUC.

81.6% 88.8%
84.4% 91.7%
87.6% 94.2%
98.5% 99.3%
85.7% 94.6%
94.2% 99.0%
98.4% 99.4%
98.7% 99.5%
99.2% 99.5%
99.4% 99.6%

Table 3. Evaluation results on the CACD-VS dataset.

Table 2. Evaluation results on the MORPH Album 2 dataset.

MF2 include an additional distractor set respectively that
contains 1 million face distractors, making the benchmarks
much more difﬁcult. The MF2 provides a training dataset
such that all the evaluation methods should be trained on
the same dataset and without any additional training data.
We consider the following models for ablative comparison
in this study: (1) Baseline: the baseline model is trained by
the identiﬁcation loss only and without any extra age super-
vision. (2) +Age: this model is trained by the joint super-
vision of the identiﬁcation signal and the age classiﬁcation
signal. (3) +Age+DAL: our proposed model that is trained
simultaneously by the DAL regularization and the joint su-
pervision signals. As reported in Table 1, without DAL the
joint supervision model achieves comparable results with
the baseline model. On the contrary, our “+Age+DAL”
model improves the performance of FG-NET on all the
schemes. The improvement on FG-NET with the scheme
of MF2 is relatively limited compared with that of MF1 and
’leave-one-out’, mainly due to the less aging variations of
MF2 training dataset. Nevertheless, the consistently perfor-
mance improvement demonstrates the effectiveness of our
method. Moreover, our method improves the baseline mod-
els by more than 0.7% on MORPH Album 2, and more than
0.3% on CACD-VS, which are remarkable improvements at
the high accuracy level above 98% and 99%.

4.3. Experiments on the MORPH Album 2 Dataset

The MORPH Album 2 dataset consists of 78,000 face
images of 20,000 individuals across different ages. For fair
comparison, we follows [43] and conduct evaluations under
two benchmark schemes where the testing set consists of
10,000 subjects and 3,000 subjects respectively. In the test-
ing sets, two face images of each subjects with the largest
age gaps are selected to compose the probe set and the
gallery set. We train the model with our proposed DAL on
the large training dataset(1.7M images). Note that we have
not conducted any training or ﬁnetuning on the MORPH
Album 2.

Method

Park et al. [32] (2010)

Li et al. [25] (2011)

HFA [13] (2013)
MEFA [15] (2015)

CAN [48]

LFCNNs [11]

Ours

Rank-1

37.4%
47.5%
69.0%
76.2%
86.5%
88.1%
94.5%

Table 4. Evaluation results on the FG-NET dataset under the pro-
tocol of leave-one-out.

In this experiment, we compare our DAL model against
the recently AIFR algorithms in the literature. As shown in
Table 2, the proposed method has effectively improved the
rank-1 identiﬁcation performance. Particularly, our method
outperforms the recent top-performing AIFR methods by a
clear margin, setting new state-of-the-art on the MORPH
Album 2 database.

4.4. Experiments on the CACD VS Dataset

As a public released dataset for AIFR, the CACD dataset
is composed of 163,446 images from 2,000 celebrities with
age variations. The collected face images also include dif-
ferent illumination, various poses and makeup. The sub-
set CACD-VS consists of 4000 face image pairs for face
veriﬁcation, and the face pairs are divided into 2,000 pos-
itive pairs and 2,000 negative pairs.
In our experiment,
we strictly follow [5, 43] to perform the 10-fold cross-
validation for fair comparisons. We use the same trained
models in Sec 4.3 to evaluate the performance on the
CACD-VS Dataset. Table 3 shows the veriﬁcation accu-
racy of our models compared against the other state-of-the-
art AIFR methods. Not surprisingly, the proposed DAL
model obtains consistent improvement over the prior meth-
ods, demonstrating the superiority of our method again.

3533

Method

Protocol

Rank-1

Method

LFW

MF1-Facescrub

FUDAN-CS SDS [44]

SphereFace [28]

TNVP [11]

Softmax

A-Softmax

OE-CNNs [43]

Ours

Small
Small
Small
Small
Small
Small
small

25.56%
47.55%
47.72%
35.11%
46.77%
52.67%
57.92%

Table 5. Evaluation results on the FG-NET dataset under the pro-
tocol of MF1.

Method

GRCCV

NEC
3DiVi

GT-CMU-SYSU
OE-CNNs [43]

Ours

Protocol

Rank-1

Large
Large
Large
Large
Large
Large

21.04%
29.29%
35.79%
38.21%
53.26%
60.01%

Table 6. Evaluation results on the FG-NET dataset under the pro-
tocol of MF2

4.5. Experiments on the FG NET Dataset

Compared to MORPH Album 2 and CACD-VS, the FG-
NET dataset is much more challenging containing a wide
covering of ages from 0 to 69. It has 1002 face images from
82 individuals.The dataset includes lots of face images at
the age phase of the child and the elderly. We conducted ex-
periments under three different evaluation schemes for over-
all fair benchmark comparison:
leave-one-out, MegaFace
challenge 1 (MF1) and MegaFace challenge 2 (MF2).

Evaluation with leave-one-out. We directly use the
DAL model trained on the small training set (0.5M images)
and test on the FG-NET dataset. The evaluation is con-
ducted by leave-one-out. It is noticeable that we have not
used any data of FG-NET for training or ﬁnetuning. The
performance comparisons are given in Table . We can see
that our method has improved the priors [13] by a signiﬁ-
cant margin.

Evaluation with MF1. The MF1 [21] contains 1 million
distractor images from 690K different individuals. Accord-
ing to [21], evaluations are conducted under the two proto-
cols: large or small training set. The training set less than
0.5M is considered small. We strictly follow the protocol
of small training set to train the model and conduct eval-
uations on FG-NET. The experimental results are reported
in Table 5. The performance improvement over the other
methods strongly demonstrates the effectiveness of the pro-
posed DAL method.

Evaluation with MF2. We also conducte experiments
on the MF2 [31], which has 1 million distractors as well.
But the distractors of MF1 and MF2 are totally different.

SphereFace[28]

CosFace[42]
OE-CNNs[43]

Ours

99.42%
99.33%
99.35%
99.47%

72.73%
77.11%

N/A

77.58%

Table 7. Evaluation results on LFW and MF1-Facescrub dataste.
The reported results are veriﬁcation rate for LFW, and rank-1 iden-
tiﬁcation rate for MF1-Facescrub.

Unlike the MF1, the MF2 requires that all the models should
be trained on the same training set, thus yields very fair
comparisons. The training set provided by MF2 contains
4.7 million faces from 672K identities. Following this pro-
tocol, we train our models and conduct evaluations on the
MF2. Table 6 shows the performance comparisons between
ours and the previous methods. Again, our DAL method
signiﬁcantly improves the identiﬁcation accuracy and set
new state-of-the-art on the MF2 dataset.

4.6. Experiments on the General Face Recognition

Datasets

To compare against the state-of-the-art methods in Gen-
eral Face Recognition(GFR), we further conduct experi-
mental evaluations on the LFW and the MegaFace Chal-
lenge 1 Facescrub (MF1-Facescrub) datasets. The LFW
[19] is a public benchmark for GFR that has 13,233 face
images from 5,749 subjects. The MF1-Facescrub [21] in-
cludes the Facescrub (containing 106,863 face images from
530 celebrities) as a probe set and contains a million dis-
tractors in the gallery set. We strictly follow the same train-
ing and evaluation procedure in OE-CNNs [43]. That is, our
training data contains 0.5M images that are the same as OE-
CNNs [43]. Table 7 reports the veriﬁcation rate on LFW and
the rank-1 identiﬁcation rate in MF1-Facescrub. Our model
outperforms the [43] as well as the state-of-the-art General
Face Recognition (GFR) models [28, 42] on both datasets,
which demonstrates the strong generalization ability of our
proposed approach.

5. Conclusion

In this paper, we propose the decorrelated adversarial
learning method for AIFR. Our model learns to minimize
the correlation between the paired decomposed features of
identity and age in an adversarial process. We present the
BCCA algorithm as an extension of CCA in deep learning.
Besides the DAL, we simultaneously train the model with
the joint supervision of identiﬁcation and age classiﬁcation.
In the testing, only the identity features are used for face
recognition. Evaluations conducted on the AIFR bench-
marks demonstrate the superiority of our method.

3534

References

[1] FG-NET Aging Database,http://www.fgnet.rsunit.com/. 2, 6

[2] G. Antipov, M. Baccouche, and J.-L. Dugelay. Face Aging
With Conditional Generative Adversarial Networks.
IEEE
International Conference on Image Processing (ICIP), 2017.
1, 3

[3] Y. Bai, Y. Zhang, M. Ding, and B. Ghanem. Finding tiny
faces in the wild with generative adversarial network.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR’18), 2018. 3

[4] K. Cao, Y. Rong, C. Li, X. Tang, and C. C. Loy. Pose-robust
face recognition via deep residual equivariant mapping. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR’18), 2018. 2, 3

[5] B.-C. Chen, C.-S. Chen, and W. H. Hsu. Cross-age reference
coding for age-invariant face recognition and retrieval.
In
European Conference on Computer Vision (ECCV), 2014. 2,
6, 7

[6] B.-C. Chen, C.-S. Chen, and W. H. Hsu. Face recognition
and retrieval using cross-age reference coding with cross-
age celebrity dataset.
IEEE Transactions on Multimedia,
17(6):804–815, 2015. 2, 7

[7] D. Chen, X. Cao, F. Wen, and J. Sun. Blessing of dimension-
ality: High-dimensional feature and its efﬁcient compres-
sion for face veriﬁcation. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 3025–3032,
2013. 2, 7

[8] Y. Chen, Y. Tai, X. Liu, C. Shen, and J. Yang. FSRNet: End-
to-end learning face super-resolution with facial priors.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR’18), 2018. 3

[9] J. Deng, S. Cheng, N. Xue, Y. Zhou, and S. Zafeiriou. Uv-
gan: Adversarial facial uv map completion for pose-invariant
face recognition. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR’18), 2018. 3

[10] J. Deng, J. Guo, and S. Zafeiriou. Arcface: Additive angular
margin loss for deep face recognition. arXiv:1801.07698,
2018. 1

[11] C. N. Duong, K. G. Quach, K. Luu, M. Savvides, et al.
Temporal Non-Volume Preserving Approach to Facial Age-
Progression and Age-Invariant Face Recognition.
IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2017. 1, 3, 7, 8

[12] X. Geng, Z.-H. Zhou, and K. Smith-Miles. Automatic age
estimation based on facial aging patterns. IEEE Transactions
on pattern analysis and machine intelligence (TPAMI), 2007.
1

[13] D. Gong, Z. Li, D. Lin, J. Liu, and X. Tang. Hidden factor
analysis for age invariant face recognition. In International
Conference on Computer Vision (ICCV), 2013. 2, 4, 5, 7, 8

[14] D. Gong, Z. Li, J. Liu, and Y. Qiao. Multi-feature Canon-
ical Correlation Analysis for Face Photo-Sketch Image Re-
trieval. In Proceedings of ACM international conference on
Multimedia, pages 617–620, 2013. 2

[15] D. Gong, Z. Li, D. Tao, J. Liu, and X. Li. A maximum en-
tropy feature descriptor for age invariant face recognition. In

IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 5289–5297, 2015. 2, 7

[16] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, , and Y. Bengio.
Generative Adversarial Nets. In Advances in Neural Infor-
mation Processing Systems, 2014. 2

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning
In IEEE Conference on Computer

for Image Recognition.
Vision and Pattern Recognition (CVPR), 2016. 1

[18] H. Hotelling.

Relations between two sets of variates.

Biometrika,, 1936. 2

[19] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.
Labeled faces in the wild: A database for studying face
recognition in unconstrained environments. In Technical Re-
port 07-49, University of Massachusetts, Amherst, 2007. 8

[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
Proceedings of International Conference on Machine Learn-
ing, 2015. 4

[21] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and
E. Brossard. The megaface benchmark: 1 million faces for
recognition at scale. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2016. 6, 8
[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems (NIPS),
2012. 1

[23] A. Lanitis, C. J. Taylor, and T. F. Cootes. Toward automatic
simulation of aging effects on face images. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
2002. 1

[24] Z. Li, D. Gong, X. Li, and D. Tao. Aging face recognition: A
hierarchical learning model based on local patterns selection.
IEEE Transactions on Image Processing (TIP), 25(5):2146–
2154, 2016. 2, 7

[25] Z. Li, U. Park, and A. K. Jain. A discriminative model for age
invariant face recognition. IEEE transactions on Information
Forensics and Security (TIFS), 2011. 2, 7

[26] L. Lin, G. Wang, W. Zuo, X. Feng, and L. Zhang. Cross-
domain visual matching via generalized similarity measure
and feature learning. IEEE Transactions on Pattern Analysis
and Machine Intelligence (TPAMI), 39(6):1089–1102, 2017.
2, 7

[27] H. Ling, S. Soatto, N. Ramanathan, and D. W. Jacobs.
Face veriﬁcation across age progression using discrimina-
tive methods. IEEE transactions on Information Forensics
and Security (TIFS), 2010. 2

[28] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
SphereFace: Deep Hypersphere Embedding for Face Recog-
nition. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 1, 8

[29] Y. Liu, F. Wei, J. Shao, L. Sheng, J. Yan, and X. Wang. Ex-
ploring Disentangled Feature Representation Beyond Face
Identiﬁcation. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2018. 3

[30] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face
attributes in the wild. In International Conference on Com-
puter Vision (ICCV), 2015. 5

3535

[46] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In Euro-
pean Conference on Computer Vision (ECCV), pages 499–
515, 2016. 1

[47] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. arXiv
preprint arXiv:1611.05431, 2016. 1

[48] C. Xu, Q. Liu, and M. Ye. Age invariant face recognition and
retrieval by coupled auto-encoder networks. Neurocomput-
ing, 222:62–71, 2017. 7

[49] W. Yang, D. Yi, Z. Lei, J. Sang, and S. Z. Li. 2D-3D Face

Matching using CCA. 2

[50] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face representa-
tion from scratch. In arXiv preprint arXiv:1411.7923, 2014.
5

[51] K. Zhang, Z. Zhang, C.-W. Cheng, W. H. Hsu, Y. Qiao,
W. Liu, and T. Zhang. Super-Identity Convolutional Neural
Network for Face Hallucination. In European Conference on
Computer Vision (ECCV), 2018. 3

[52] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao.

Joint Face De-
tection and Alignment using Multi-task Cascaded Convolu-
tional Networks. Signal Processing Letters, 23(10):1499–
1503, 2016. 5

[53] Z. Zhang, Y. Song, and H. Qi. Age Progression/Regression
by Conditional Adversarial Autoencoder. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2017. 1, 3

[54] Y. Zhao, Z. Jin, G. jun Qi, H. Lu, and X. sheng Hua. An
adversarial approach to hard triplet generation. In European
Conference on Computer Vision (ECCV), 2018. 3

[55] T. Zheng, W. Deng, and J. Hu. Age Estimation Guided Con-
volutional Neural Network for Age-Invariant Face Recogni-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW), 2017. 2, 7

[31] A. Nech and I. Kemelmacher-Shlizerman. Level playing
ﬁeld for million scale face recognition. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2017.
6, 8

[32] U. Park, Y. Tong, and A. K. Jain. Age-invariant face recog-
nition. IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI), 2010. 1, 7

[33] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face
recognition. In British Machine Vision Conference (BMVC),
2015. 5

[34] K. Ricanek and T. Tesafaye. Morph: A longitudinal im-
age database of normal adult age-progression.
In Interna-
tional Conference on Automatic Face and Gesture Recogni-
tion, 2006. 2, 6

[35] R. Rothe, R. Timofte, and L. V. Gool. Dex: Deep expectation
of apparent age from a single image. In International Con-
ference on Computer Vision Workshops (ICCVW), December
2015. 5

[36] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations (ICLR), 2015. 1

[37] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning
face representation by joint identiﬁcation-veriﬁcation. In Ad-
vances in Neural Information Processing Systems (NIPS),
2014. 1

[38] Y. Sun, D. Liang, X. Wang, and X. Tang. Deepid3: Face
In arXiv

recognition with very deep neural networks.
preprint arXiv:1502.00873, 2015. 1

[39] Y. Sun, X. Wang, and X. Tang. Deeply learned face represen-
tations are sparse, selective, and robust. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2015.
1

[40] F. Wang, J. Cheng, W. Liu, and H. Liu. Additive margin
softmax for face veriﬁcation. IEEE Signal Processing Let-
ters, 25:926–930, 2018. 1, 4

[41] F. Wang, X. Xiang, J. Cheng, and A. L. Yuille. NormFace:
L 2 Hypersphere Embedding for Face Veriﬁcation. In Pro-
ceedings of the 2017 ACM on Multimedia Conference (ACM
MM), 2017. 1

[42] H. Wang, Y. Wang, Z. Zhou, X. Ji, Z. Li, D. Gong, J. Zhou,
and W. Liu. Cosface: Large margin cosine loss for deep
face recognition. Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2018. 1, 4, 8

[43] Y. Wang, D. Gong, Z. Zhou, X. Ji, , H. Wang, Z. Li, W. Liu,
and T. Zhang. Orthogonal Deep Features Decomposition for
Age-Invariant Face Recognition. In European Conference on
Computer Vision (ECCV), 2018. 2, 5, 7, 8

[44] Z. Wang, K. He, Y. Fu, R. Feng, Y.-G. Jiang, and X. Xue.
Multi-task Deep Neural Network for Joint Face Recognition
and Facial Attribute Prediction. In Proceedings of the 2017
ACM on International Conference on Multimedia Retrieval
(ICMR), 2017. 8

[45] Y. Wen, Z. Li, and Y. Qiao. Latent factor guided convolu-
tional neural networks for age-invariant face recognition. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016. 2, 4, 5, 7

3536

