Image-Question-Answer Synergistic Network for Visual Dialog

Dalu Guo, Chang Xu, Dacheng Tao

UBTECH Sydney AI Centre, School of Computer Science, FEIT,

University of Sydney, Darlington, NSW 2008, Australia
{dguo8417@uni., c.xu@, dacheng.tao@}sydney.edu.au

Abstract

The image, question (combined with the history for de-
referencing), and the corresponding answer are three vital
components of visual dialog. Classical visual dialog sys-
tems integrate the image, question, and history to search for
or generate the best matched answer, and so, this approach
signiﬁcantly ignores the role of the answer. In this paper, we
devise a novel image-question-answer synergistic network
to value the role of the answer for precise visual dialog. We
extend the traditional one-stage solution to a two-stage so-
lution.
In the ﬁrst stage, candidate answers are coarsely
scored according to their relevance to the image and ques-
tion pair. Afterward, in the second stage, answers with high
probability of being correct are re-ranked by synergizing
with image and question. On the Visual Dialog v1.0 dataset,
the proposed synergistic network boosts the discriminative
visual dialog model to achieve a new state-of-the-art of
57.88% normalized discounted cumulative gain. A gener-
ative visual dialog model equipped with the proposed tech-
nique also shows promising improvements.

1. Introduction

Visual dialog is an emerging research theme lying at the
intersection between computer vision and natural language
processing. Given the capacities of reasoning, grounding,
recognition, and translating, a visual dialog agent is ex-
pected to answer questions based on an image, caption,
and history. For example, in Figure 1, the agent ﬁrst rea-
sons what the word ‘their’ refers to in the current question
based on history, locates the bounding boxes of the ﬁve peo-
ple in the picture, recognizes the color of their jackets, and
then translates the visual information into human language.
Hence, a visual dialog task can also be regarded as: (i) vi-
sual grounding [33], which further converts visual informa-
tion in located bounding boxes into human language; (ii)
visual question answering (VQA) [2], which includes extra
dialog history and caption as the input; and (iii) image cap-
tioning [25], which generates a description not only based
on visual information but also the history and question.

Figure 1. A general visual dialog task. The correct answer is
picked from a candidate set by investigating the given image, cap-
tion, history, and question.

A general visual dialog model has two components: an
encoder to embed inputs (e.g., images and questions) into
vectors and fuse them to create a uniﬁed representation, and
a decoder either to translate the encoded vector directly into
words for an answer or to rank the given candidate answers.
Both VQA and visual dialog involve the fusion job of multi-
ple modalities. However, as a multi-turn VQA task, in each
turn, a visual dialog system must also integrate the caption
and dialog history from past turns. Visual dialog systems
can be grouped into two main categories according to dif-
ferent decoders: generative models and discriminative mod-
els. Generative models usually employ seq2seq [24] or ad-
vanced reinforcement learning [29] techniques to generate
the answer set, where the highest probability answer is cho-

110434

1.Black and grey2.Dark blue3.Red green grey and black4.Grey, black and white5.Blue, red, black, grey and greenCaption5 people skiing in a snowy area surrounded by trees.HistoryQ: Can you see more than 5 people?A:No.Q:Are they wearing helmets?A:No.QuestionWhat colorare their jackets?Candidateswe usually ﬁrst exclude obviously wrong answers before
paying more effort to compare the remaining answers that
are more likely to be correct. We ﬁll each answer into the
question blank and judge whether the complete sentence is
more suitable than the others. In addition, we address the
class imbalance problem in the primary stage for a strong
discriminative model. As a large number of easy negative
candidate answers dominate the loss function, a tempera-
ture factor is considered in the loss function to discount the
contribution of easy samples.

Our model is evaluated on the Visual Dialog v1.0 dataset
[4]. In validation, our primary stage with a loss-balanced
discriminative model improves the mean reciprocal rank
(MRR) by 0.71% compared to the non-balanced model, and
the synergistic stage gives additional 0.91% improvement
on MRR. Furthermore, the synergistic stage in our gener-
ative model improves MRR by 4.7% and recall on top-5
responses (R@5) by 9.2% compared to the primary stage,
which provides a different way to generate descriptive an-
swers other than GAN and reinforcement learning [29].
On the test-standard dataset, our two-stage model outper-
forms the baselines and achieves state-of-art performance,
higher than the other entries in the Visual Dialog Challenge
2018 with a 57.88% normalized discounted cumulative gain
(NDCG).

2. Related Work

Visual Question Answering (VQA): VQA is the ﬁrst
task undertaken when querying images for text answer gen-
eration.
It is a classiﬁcation problem in which candidate
answers are restricted to the most common answers appear-
ing in the dataset. Current models can be classiﬁed into
three main categories: early fusion models, later fusion
models, and external knowledge-based models.
In early
fusion models, the input queries are regarded as parame-
ters of conditional batch normalization [6, 22] in the detec-
tion network [9], which leads a pre-trained ResNet to the
proposed MODERN architecture; this method affects less
than 1% of parameters in the pre-trained model, which re-
duces the risk of over-ﬁtting. Later fusion models mainly
concentrate on how to represent the answer vector by joint-
ing the question and global image feature [11, 2]. How-
ever, a lot of visual information is irrelevant to the input
query, resulting in signiﬁcant noise during reasoning. Thus,
the attention mechanism is introduced to solve this prob-
lem. This starts with the linear combination of two fea-
tures, such as the stacked attention network [30] by learn-
ing the attention using multi-step reasoning and the dual at-
tention network [20] that learns both the visual and textual
attention. Then, a bilinear pooling method like multi-modal
compact bilinear pooling (MCB) [7] is applied by project-
ing the outer product of two features into the high dimen-
sion of quadratic expansion. However, MCB needs to sam-

10435

Figure 2. Candidate answers synergized with image and question
are re-scored. The synergistic stage refers the answers back to the
question and re-matches the image.

sen as the output. Discriminative models tend to calculate
the similarity between the latent output of the encoder and
the embedding of candidate answers [4, 18, 12, 29], with the
correct answer expected to have the highest score. However,
existing generative models aim to generate a single word of
high probability at each step but omit the meaning of the
whole answer sentence. The discriminative models are ben-
eﬁcial to understanding the answer sentence through long
short-term memory (LSTM) [10], but the scoring method
is insufﬁcient to capture the similarity between inputs and
answers, since the vector of inputs and answers have been
separately learned without deep fusion. Furthermore, both
generative and discriminative models tend to give short and
safe answer, such as ‘Yes’ or ‘No’, as their fusion methods
focus on the major signal in short answer but will not look
into details in a longer one.

To highlight the role of answer and its integration with
other ingredients (e.g., images and questions) in visual dia-
log, we propose an image-question-answer synergistic net-
work. However, not all answers are plausible for the image
and question. According to the experimental results of [4],
recall at 10 (R@10) can already be higher than 85%, which
implies that the answers ranked below 10 are unlikely to
be correct. For example, in Figure 1, only answers about
color in the image are related to the question, while the
others are unreasonable and probably lead to network side
effects. Hence, we extend the traditional one-stage model
composed of an encoder and a decoder into a two-stage
model containing a primary stage and a synergistic stage.
The primary stage can be any existing model that coarsely
scores all candidate answers or generates some high prob-
ability candidates. The synergistic stage selects answers of
high probability based on certain policies, ﬁnely synergizes
them with the question and then re-ranks the candidate an-
swers according to the relevance of the synergies to the im-
age, as shown in Figure 2. The proposed method is consis-
tent with human practice. In multiple choice examinations,

⚫Black and grey⚫Dark blue⚫Red green grey and black⚫Grey, black and white⚫Blue, red, black, grey and greenWhat colorare their jackets?Primary StageThe colorof their jackets are black and greyThe colorof their jackets are dark blueThe colorof their jackets are red green grey and blackThe colorof their jackets are grey, black and whiteThe colorof their jackets are blue, red, black, grey and green00001SynergisticStage5.315.295.285.265.22Scoreple features, which is computationally intractable and has
very large projected dimension, so low-rank bilinear pool-
ing (MLB) [15] and multi-factor bilinear pooling (MFB)
[32] have been proposed to project the two features into a
common low-rank space, and the bilinear attention network
(BAN) [14] builds the interaction attention between multi-
modal inputs.
In contrast to early and later fusion mod-
els, external knowledge-based models suppose that com-
mon sense or information not given in the image is required
to infer the right answer. [28] uses DBpedia [3] to broaden
the range of answers. [21] queries the triplet (visual con-
cept, relation, attribute) in Fvqa [26] to score the retrieved
facts. However, this method is not good enough to reason
complex facts and requires further development.

Visual Dialog: Extending the single turn dialog task
(VQA) to a multi-turn one, we introduce visual dialog.
Before the dataset proposed by [4], [5] used a dataset
that located the object in an image in which one person
gave a question about the image and the other provided a
‘Yes/No/NA’ answer based on the truth. The current dataset
expands the range of questions and answers. Question types
about image can be various including color, number, rela-
tionships, etc., while the answer could be simple as a ‘No’
or a complex description of the image. Three encoding
methods were provided in [4] as baselines, namely late fu-
sion, the hierarchical recurrent encoder, and the memory
network, as well as two decoding methods: LSTM and
softmax.
Inspired by the generative adversarial networks
(GANs) [8] and the performance gap between discrimina-
tive and generative networks, [18] transferred knowledge
from a pre-trained discriminative network to a generative
network with a Gumbel-softmax [13] LSTM encoder us-
[29] combined GAN and reinforce-
ing perceptual loss.
ment learning [31] to train the generator with a co-attention
encoder, thereby allowing the discriminator to directly ac-
cess the generated response to evaluate its quality, and used
Monte Carlo (MC) searching with a roll-out policy to com-
pute the intermediate reward for each word. [12] merged
the fusion step and the scoring step into a single step, which
is similar to our synergistic stage, but involved all the im-
age information containing noise and the whole history con-
taining topics irrelevant to current question. Furthermore, it
simply arrayed the isolated vectors of the image, question,
history, and answer for their representation.

3. Synergistic Network

In a candidate answer set, some of the answers are hard
samples that are close or equal to the correct answer, while
others are easy samples. Our framework shown in Figure 3
has two stages, the primary stage and the synergistic stage.
In the primary stage, we learn representative vectors of the
image, history, and question using a co-attention module
and then calculates the score of each candidate answer to

In the synergistic
separate hard answers from easy ones.
stage, we select hard answers together with their questions
to form question-answer pairs. These pairs further coordi-
nate with the image and history to predict scores.

We ﬁrst formally deﬁne the visual dialog problem and
then introduce our new synergistic strategy. Given an im-
age I and caption C, we collect historical questions and
their corresponding answers as H. At turn t, our model
gives a score for each answer at,i in candidate set At based
on question qt. To describe input image I, we detect objects
and their features using the Faster-RCNN model [1] and ap-
ply CNN to encode them into V = (v1, . . . , vn), where
vi ∈ Rd and n is the number of objects. Question qt is
a sequence of words, which can be encoded using LSTM,
i.e., mq
t = LSTM(qt). We also organize the previous di-
alog (including caption) as history H = (H0, . . . , Ht−1),
where H0 = C, and Hi = (qi, ai,gt) for i ∈ {1, . . . , t − 1},
which is the concatenation of the question and correct an-
swer at each turn before time t. Similar to the question,
we use another LSTM to extract the history features as
U = (u0, . . . , ut−1), where ui = LSTM(Hi). mq
t ∈ Rd
and ui ∈ Rd correspond to the last state of LSTM.
3.1. Primary Stage

An encoder-decoder solution is adopted in the primary
stage [4, 29, 18]. The encoder contains two main tasks,
one is how to de-reference in the multi-turn conversations
(98% of dialogs contain at least one pronoun), and the other
is to locate the objects in the image mentioned in the cur-
rent question. The attention mechanism [18] is commonly
used to tackle the tasks. Instead of linear concatenation, we
use multi-modal factorized bilinear pooling (MFB) [32], as
it can overcome the difference between distributions of the
two features (two LSTMs to encode question and history,
respectively; LSTMs for text feature and CNNs for image
feature). MFB is expected to provide a richer representation
than other bilinear methods, such as MLB [15] and MCB
[7]. In MFB, the fusion of two features X, Y ∈ Rd is cal-
culated by:

z = MFB(X, Y ) =

k

X

(U⊤

i X ◦ V⊤

i Y ),

(1)

i=1

where U, V ∈ Rd×l×k are the parameters to be learned,
k is the number of factors, l is the hidden size, and ◦ is
the Hadamard product (element-wise multiplication). How-
ever, Y sometimes represents multiple channel input, e.g.,
detected objects or history in our model, so the formula be-
comes:

k

z = MFB(X, Y ) =

X

((U⊤

i X · 1⊤) ◦ (V⊤

i Y )),

(2)

i=1

where 1 ∈ Rφ is the vector with all elements equal to one,
and φ is the channel number of Y . To stabilize the output
neurons, we use power normalization (z ← sign(z)|z|0.5)
and ℓ2 normalization (z ← z/kzk).

10436

Figure 3. Architecture of our model. All candidate answers are scored in the primary stage, and some selected answers are re-scored in the
synergistic stage.

We utilize MFB to learn the uniﬁed vector of the ques-
tion and history, denoted as zh
t , U ), where
zh
t ∈ Rl×t. Then, we learn the attention weight and vector
by:

t = MFBh(mq

αh
t = softmax(w⊤

α zh

t ),

mh

t =

t−1

X

i=0

(αh

t,iui),

(3)

(4)

α ∈ Rl is a learned parameter, and αh

t ∈ Rt is the cal-
w⊤
culated weight implying which history the question should
refer to. The attended history vector mh
t can be concate-
nated with the question vector and then fused with image
features, zv
t ], V ). The image at-
tention vector mv
t can be obtained in a similar approach
as Eq.(3) and Eq.(4) by taking zv
t as the input. Finally,
we learn the representation of text and visual features with
ep
t = MFBe([mq

t = MFBv([mq

t : mh

t ], mv

: mh

t ).

t

The decoder encodes each candidate answer at,i ∈ At to
t,i using LSTM and calculates the dot similarity score by:

ma

t,i = ep
sd

t ⊤fd(ma

t,i),

(5)

where fd is a one-layer MLP with activation tanh to project
the answer encoding ma
t,i ∈ Rd to the space of input em-
bedding ep
t .

The correct answer at,gt should have a higher score than
the others. Thus, we use N-pair loss [23] to measure the
error. Most of the 100 candidate answers are easy samples,
which are irrelevant to the inputs, and contribute to no use-
ful learning signal in this loss (score difference less than
zero in Figure 5). To solve the imbalance problem, we em-
ploy temperature τ to reduce the imbalance impact:

LD = log(

100

X

i=1

exp

sd
t,i − sd

t,gt

τ

),

(6)

t,i − sd

where τ ≤ 1.
If the candidate answer at,i is correctly
scored lower than that of the ground truth answer at,gt, loss
lt,i = sd
t,gt will be less than 0, and τ can reduce the
contribution of answer at,i. For instance, with τ = 0.25
and lt,i = −1, about 20 such items make the same loss as
the normal N-pair loss. Otherwise, it ampliﬁes the loss of
incorrectly scored answers.

3.2. Synergistic Stage

In the primary stage, some answers are improperly
scored due to the limitations of the scoring method. In this
synergistic stage, answers are coordinated with the ques-
tion and image for re-rank. However, easy candidate an-
swers are not needed in further analysis, and we want our
second stage model to fully focus on hard answers within
its modeling capacity. Therefore, we select the answers
with higher probability of being correct based on the pre-
dicted scores from the primary stage. As seen from the
recall of our best method in Table 1, the top ten answers
predicted in the primary stage covered nearly 90% of the
ground truth, which means that the remaining 90 answers
are of lower probability and can easily be discriminated.
Based on this phenomenon, we ﬁrst pick the top N an-
swers from At to organize a new candidate set Bt, where
Bt = (bt,1, . . . , bt,N ), Bt ⊂ At.

The selected answers are often ambiguous for describing
the whole meaning of sentences (such as ‘No’ and ‘Black
and grey’ in Figure 1), so they must work with the corre-
sponding question to make complete sentences. Thus, we
append question qt to each answer bt,j, j ∈ {1, . . . , N } as
a question-answer pair and encode it using LSTM to obtain
a vector:

mb

t,j = LSTM([qt : bt,j]).

(7)

Extra history is required to remedy the reference problem

10437

Caption𝑞1:𝑎(1,𝑔𝑡)…𝑞𝑡−1:𝑎(𝑡−1,𝑔𝑡)𝑞𝑡𝑠𝑡,1𝑑…𝑠𝑡,100𝑑𝑎𝑡,1,…,𝑎𝑡,100CNNLSTM𝑞𝑡CNNAnswerSelection00001Primary StageSynergistic StageCo-attention modularCo-attention modularFCCaption𝑞1:𝑎(1,𝑔𝑡)…𝑞𝑡−1:𝑎(𝑡−1,𝑔𝑡)LSTMLSTMLSTMLSTMof the question. Therefore, we use mb
tor combined with attended history mh
attention parameters:

t,j as a question vec-
t to learn the image’s

(8)

t ], V )

t,j : mh

zr
t,j = MFBa([mb
and the attended image feature mr
t,j using Eq.(3) and Eq.(4)
for selected answer bt,j . Similar to the primary stage,
we obtain the fusion embedding er
:
mh
t,j), which represents the answer vector synergized
with the image, question, and history, which is directly used
to calculate the score by:

t,j = MFBr([mb
t,j

t ], mr

sr
t,j = fr(er

t,j),

(9)

where fr can be a one-layer MLP. An answer in candidate
set containing more details and better matching the inputs
should earn a higher score than ordinary ones.

Here, we reuse the attended history vector mh

t from the
primary stage, since the co-reference problem in the ques-
tion can be resolved without knowing the answer. For ex-
ample, as shown in Figure 1, we can refer ‘their’ to the ‘ﬁve
people’ regardless of the colors of their jackets. The im-
age feature V is shared with the primary stage, because we
want the image feature to be represented universally in the
two stages. But the attention weights are learned at each
stage, since each candidate answer, as well as the question,
depicts its own attention map.

We treat this stage as a classiﬁcation problem, where the

correct answer should have the highest probability:

pr
t = softmax(sr

t ),

LR =

N

X

j=1

−yj log(pr

t,j),

(10)

(11)

where ygt is equal to 1 and the others are zeros. We note that
this formula can be easily extended to soft cross entropy,
where yi is the probability marking this answer as correct if
a dense annotation dataset is available in the future.

4. Extension to the Generative Model

Besides discriminative model in the primary stage, the
generative model can also be used to score the candidate
answers and seamlessly works with the proposed image-
question-answer synergistic method. The encoder of the
generative model is the same as that of the discriminative
model in the primary stage in Section 3.1. Accordingly, we
still use ep
t to represent the common vector of image I, his-
tory H, and question qt at turn t. The decoder interprets ep
t
into answers and calculates the probability of answer at,i
by:

sg
t,i = p(w1

t,i, . . . , wT ′

t,i |qt, H, I)

=

T ′
Y

j=1

p(wj

t,i|(w1

t,i, . . . , wj−1

t,i ), ep

t ),

Figure 4. Primary stage of our generative model. The score of each
answer is its probability of word sequence.

which is also regarded as score in the primary stage as
shown in Figure 4, where w1
t,i is the word se-
quence of the answer at,i, and T ′ is word number. For each
word, its probability is given by:

t,i, . . . , wT ′

p(wj

t,i|(w1

t,i, . . . , wj−1

t,i ), ep

t ) = fg(MFBg(hj, ep

t )),

(13)

where h0 is the last hidden state of LSTM for the ques-
tion, hj = LSTM(hj−1, wj−1
t,i ) is the state of LSTM, and
fg maps the fusion vector to the word space. Instead of ini-
tializing the LSTM of decoder with encoded vector ep
t [4],
we regard it as a context vector [19]. This is for three main
reasons: ﬁrst, the encoded vector and the LSTM decoder
have different distributions, and all gates and the hidden
state in LSTM are learned by linear combination; second,
the salient objects that should be attended by each token
are already chosen by the question, and the aim of the de-
coder is to translate the visual information of salient objects
into text, such that the context vector is ﬁxed for each to-
ken; and third, instead of learning a joint vector with hj−1,
the context vector could be considered as compensatory in-
formation to the current hidden state hj , which reduces the
uncertainty for next word prediction.

To make the correct answer at,gt score higher in the
primary stage, we maximize the conditional probability
p(w1
t,gt|qt, H, I). Thus, the loss function is the
sum of the negative log likelihood of the correct word in
each step:

t,gt, . . . , wT ′

LG = −

log p(at,gt).

(14)

i=1

If the candidate answers are given in the generative
model as in the discriminative model, we can collect the
score for each answer using Eq.(12). Otherwise, we can
generate some candidate answers with high probability by
beam search [24]. After collecting the score for each answer
in the primary stage, we follow the strategy in Section 3.2
to pick the top N answers and then synergize them with the
image, question, and history to learn a better representation
for re-scoring.

T ′
X

5. Experiments

(12)

In this section, we evaluate our synergistic strategy on a
visual dialog dataset. We introduce the dataset and evalua-

10438

𝐻𝑞𝑡CNN𝑝(𝑤𝑡,𝑖1)…𝑠𝑡,𝑖𝑔LSTMLSTMLSTMS𝑤𝑡,𝑖1𝑤𝑡,𝑖𝑇′LSTMLSTM𝐼……𝑝(𝑤𝑡,𝑖2)𝑝(𝐸)tion metric, before describing our experimental setting and
results, and ﬁnally the qualitative analysis.

5.1. Dataset and Evaluation Metric

Our model is trained on the Visual Dialog v1.0 dataset
[4], which contains about 120k images from COCO-trainval
[17]. Each image has one caption and 10 turn dialogs,
i.e., about 1.2 million question-answer pairs. To organize
this dataset, two people chatted on the Amazon Mechanical
Turk. The questioner could not see the image and asked a
question based on the given caption and previous context to
better understand the scene, while the answerer could see
both the image and caption and replied to the question as
naturally and conversationally as possible. Each question
has 100 candidate answers containing one correct answer,
50 answers to similar questions, 30 popular answers, and
some randomly picked answers from the dataset. For the
validation and test datasets, 10K COCO-like images were
collected from Flickr. The test dataset was densely anno-
tated in v1.0 by four people to allow application for the
more robust evaluation metric NDCG [27] rather than tradi-
tional retrieval metrics, such as mean rank, R@1,5,10, and
MRR. There could be more than one correct answer to each
question in the candidate set, such as ‘yeah’ and ‘yes’. In
this situation, NDCG is invariant to the order of options
with identical relevance. For each candidate answer, its rel-

annotators who marked answer as relevent

evance is

metric is given by:

DCG@k =

k

X

4

relevancei
log2(i + 1)

,

NDCG@k =

i=1
DCG@k for submited ranking

DCG@k for ideal ranking

. The

(15)

, (16)

where k is the number of answer options whose relevance is
greater than zero. Of these metrics, a higher score is better
for NDCG, MRR, and R@1,5,10, but a lower score is better
for mean rank.

5.2. Implementation Details

We ﬁrst constructed the vocabulary, which contains the
words appearing in the questions, correct answers, and
captions more than four times in the train dataset. This
made 11,213 words with the padding word ‘PAD’, out-of-
vocabulary word ‘UNK’, start symbol ‘START’, and end
symbol ‘END’. Then, each word was embedded within a
300-dimension vector shared across caption, history, ques-
tion, and answer. The maximum lengths of the cap-
tion, question, answer, and history were 40, 20, 20, and
40, respectively. For each candidate answer, we inserted
‘START’ at the head and appended ‘END’ at the tail. The
LSTMs of the question and history are two layered, while
it is one layered for the answer in the primary stage and
the question-answer pair in the synergistic stage. The hid-

Model
LF[4]
HRE[4]
MN[4]
MN-att[4]
LF-att[4]
Technion
MS AI
USTC-YTH
Single (ours)
Ensemble (ours)

NDCG MRR R@1 R@5 R@10 Mean
45.31
5.95
6.41
45.46
5.92
47.50
5.59
49.58
5.41
49.76
54.46
3.55
4.15
55.35
4.65
56.47
4.17
57.32
57.88
3.97

55.42
54.16
55.49
56.90
57.07
67.25
63.27
61.44
62.20
63.42

40.95
39.93
40.98
42.43
42.08
53.40
49.53
47.65
47.90
49.30

72.45
70.45
72.30
74.00
74.83
85.28
80.40
78.13
80.43
80.77

82.83
81.50
83.30
84.35
85.05
92.70
89.60
87.88
89.95
90.68

Table 1. Performance of discriminative models on test-standard
server of Visual Dialog Challenge 2018. We ensemble 10 models
with different seeds and varying M from 25 to 35.

den state dimension d for all LSTMs and CNN is 512. For
bilinear pooling, we set k to 5 and l to 1000 as Yu [32].

We start training the primary stage with loss LD (LG) for
7 epochs to arrange the top ranked answers relative to the in-
puts. The synergistic stage follows with loss LD(LG) + LR
for another 15 epochs. When training the synergistic stage,
our policy is to randomly sample N − 1 answers from the
top M ranked in the ﬁrst stage combined with the right an-
swer at,gt to organize Bt. During testing, we only select the
top N answers. For this dataset, we choose N = 10 or 15
and M varying from 10 to 40 for the discriminative model,
then N = 10, 20, 30 and M ﬁxed to 30 for the generative
model, and we analyze the effect of different N and M on
performance in Section 5.4. Our model is trained using the
Adam solver [16] with β1 = 0.9, β2 = 0.99, initial learn-
ing rate 10−3, and decay every 7 epochs with an exponential
rate of 0.25.

5.3. Comparison with the State of the art

We compare our discriminative model with baselines [4]
and other methods: Later Fusion (LF), which encodes the
question, image, and history respectively and projects their
concatenation into a joint embedding; Hierarchical Recur-
rent Encoder (HRE), which uses a hierarchical architec-
ture to encode the dialog history; Memory Network (MN),
which maintains a memory bank storing previous dialog,
which is attended by a question; and MN-att and LF-att,
which add an attention mechanism for image to their base
methods. From Table 1, it can be seen that our best sin-
gle model improves NDCG by 7.56% and MRR by 5.13%
compared with LF-att. To improve accuracy, we ensem-
ble 10 models with different seeds and M . We rank the
answers by summing scores of all models and achieve the
highest NDCG on the test-standard server of Visual Dialog
Challenge 2018.

5.4. Ablation Study

We conduct several ablation studies to verify the contri-
bution of each module to our discriminative model. The
ﬁrst three lines in Table 2 show the performance of the pri-
mary stage with different τ varying from 1 to 0.25. The

10439

N M τ
-
-
-
10
10
10
10
15

-
-
-
10
20
30
40
30

1.00
0.50
0.25
0.25
0.25
0.25
0.25
0.25

MRR R@1 R@5 R@10 Mean
4.42
61.92
4.27
62.51
4.23
62.63
4.22
62.31
62.83
4.11
4.09
63.54
4.13
63.14
63.16
4.12

79.78
80.05
80.39
79.45
80.70
81.01
80.97
80.75

47.53
48.30
48.31
48.18
48.45
49.21
48.77
48.91

89.28
89.57
89.65
90.21
90.28
90.32
90.02
90.22

Table 2. Performance of our discriminative model on validation
dataset.

Model
MN-att [4]
Primary (ours)
Synergistic (ours)
Synergistic (ours)
Synergistic (ours)

N M MRR R@1 R@5 R@10 Mean
-
17.61
16.69
-
16.51
10
15.87
20
15.12
30

37.48
38.54
40.77
41.42
41.28

58.56
59.82
63.58
67.22
69.01

65.57
66.94
67.00
72.91
75.85

47.94
49.01
51.62
53.23
53.73

-
-
30
30
30

Table 3. Performance of our generative model on validation dataset
with candidate set.

that the primary stage outperforms baseline by 1.1% with
respect to MRR, which generates a strong candidate set for
the next stage. Our synergistic stage further improves MRR
by 2.6% when N = 10, since the primary stage focuses
on each word but lacks the understanding of whole answer
sequence. In contrast to the discriminative model in which
about 90% of correct answers are top ten ranked in the pri-
mary stage, only 2 out of 3 correct answers are top sorted
by the generative model, so we increase the selected answer
number N from 10 to 30 in the second stage, which further
increases MRR by 2.1%. Furthermore, our model improves
R@5 by 9.2% and R@10 by 8.9%.

Figure 5. Cumulative normalized loss for different τ .

5.5. Qualitative Analysis

performance improves as τ decays, because most candidate
answers are easily discriminated. The summed loss of these
easy negative answers whose scores are lower than the cor-
rect answer by one consumes almost 30% of the model’s
energy at τ = 1.00, as shown in Figure 5. This kind of loss
is reduced as τ decreases and is nearly zero when τ = 0.25
, saturating our primary stage. This makes the model pay
more attention to incorrect answers scored near or higher
than the correct answer. The last ﬁve lines show the results
of the synergistic stage with different settings. MRR of this
stage drops at N = 10 and M = 10, as the top answers
of the primary stage become stable after several epochs, re-
sulting in the synergistic stage learning bias. Feeding more
samples by increasing M improves performance to produce
the best model at M = 30, showing that synergy can learn
a better representation of the image, question, and answer.
What of interest is that the performance drops at M = 40 or
N = 15, possibly because the selected answers become less
relevant to the inputs, and the second stage model’s abil-
ity is to score nearly correct answers but is sensitive to un-
related ones. Conversely, it also addresses the importance
of the primary stage to the synergistic stage. And the pri-
mary stage is also necessary to balance the memory cost,
since each answer learns its own attention map and fusion
with the image in the synergistic stage, while in the primary
stage, only one attention map is required for the question
and image.

For the generative model, the ﬁrst line in Table 3 shows
the performance of the memory network initialized by the
common vector of inputs, and the second line shows the
result of our model using only the primary stage, leaving
other three lines for the synergistic stage.
It can be seen

To further demonstrate the effects of our synergistic
model, we present some examples from the validation
dataset. Figure 6 shows the results of the discriminative
model ranked by only the primary stage and our two-stage
network. The answer in red is correct, while the others
are top-ranked candidate answers. It can be seen that the
one-stage model tries to give a safe answer such as ‘No’
to the top-left and middle-left images, while our model de-
picts more details by adding ‘only the giraffe’ and ‘another
pair of feed’. Biased answers are given in middle-right im-
age, since for a binary question with choices, the answer
word is always contained in the question. There is also bias
in bottom-right image, because only humans tend to wear
hats. Our model gives unbiased answers based on the im-
ages, and furthermore, can detect the discrepancies between
similar words in the bottom-left image.

In order to apply our method to realistic applications, we
abandon the prepared candidate set in the ﬁrst stage and
generate another within our primary model by beam search
[24], which maintains a partial sequence list of size B. At
each step, all partial sequences are extended with the whole
vocabulary, and only the top B sequences with the highest
probability are retained for next step. Sentences meeting the
‘END’ symbol are moved from the partial list to the com-
plete one. Starting with the ‘START’ symbol and iterating
for a maximum 20 steps, we obtain a candidate set of size
B with all complete sentences complemented by some par-
tial ones. In Figure 7, we show the top generated answers
with B = 15 for the primary and synergistic stage, the pre-
set answer is below the question. The primary stage always
ranks short answers having one or two words higher than
the long sequences that depict images with more informa-

10440

42024score difference between candidate and correct answer0.00.20.40.60.81.0cumulative normalized loss = 1.00 = 0.50 = 0.25Figure 6. Qualitative comparison for discriminative model.

Figure 7. Qualitative comparison for generative model without candidate answers.

tion, because the generative method calculates the score for
each answer by product probability of its words. In the syn-
ergistic stage, this problem is overcome, since the extra at-
tribute information, e.g., ‘a white visor’ (middle-left image)
and ‘white with a blue tail’ (top-right image), can have a
higher score than a simple answer. Surprisingly, our model
can sometimes even generate better answers than those pro-
vided, such as ‘Just the legs of someone’ vs. ‘Part of a per-
son’ in the bottom-left image and ‘behind the truck’ vs. ‘In
the background’ in the bottom-right image.

6. Conclusions

The limitations of previous input-answer fusion meth-
ods mean that they cannot correctly represent the common
vector of these features. As a result, they omit detailed in-

formation and focus on short and safe answers. In this pa-
per, we develop a synergistic network that jointly learns the
representation of the image, question, answer, and history
in a single step. We also improve the N-pair loss function
to solve the class-imbalanced problem in the discriminative
model. Our ﬁnal proposed discriminative model achieves
state-of-the-art performance on the Visual Dialog v1.0 test-
standard server with the robust NDCG evaluation metric.
The results of our generative model are also encouraging.

7. Acknowledgement

This work was

supported

Research

Australian
170100117, DP-180103424,
180101438.

Council

in

part

the
FL-
IH-180100002, and DE-

Projects

by

10441

A1:No signsA2:No signs presentA3:There are no signsA4:NoA5:No I do not see any signsA6:No, only the giraffeA1:No, only the giraffeA2:No I do not see any signsA3:No signsA4:No signs presentA5:There are no signsA6:NoIs there any signs around?PrimaryA1:0 of the 3 guys are wearing hatsA2:Yes, the man is wearing a hatA3:2 of them are wearing hatsA4:Only 1 is wearing a hatA1:No plants or treesA2:I see some grassA3:Dried up grass or weeds, yesA4:Just rocks and grassA5:No grassA6:Just dirt Is there trees and animals?PrimarySynergisticSynergisticA1:DayA2:DaytimeA3:It is dayA4:NightA5:EveningA1:EveningA2:NightA3:DayA4:DaytimeA5:It is dayCan you tell if it’s day or night?A1:No one else is visibleA2:NoA3:Nobody in the photoA4:Not that I can seeA5:Yes, there are another pair of feedA1:Yes, there are another pair of feedA2:Not that I can seeA3:No one else is visibleA4:NoA5:Nobody in the photoIs there anyone near him?PrimarySynergisticPrimarySynergisticWhat’s the desk made out of?PrimaryA1:WoodA2:LaminateA3:Dark brownA4:Cheap plastic or woodA1:WoodA2:Cheap plastic or woodA3:LaminateA4:Dark brownSynergisticAre they both wearing a hat?A1:Just rocks and grassA2:No grassA3:No plants or treesA4:Dried up grass or weeds, yesA5:Just dirt A6:I see some grassA1:Only 1 is wearing a hatA2:2 of them are wearing hatsA3:0 of the 3 guys are wearing hatsA4:Yes, the man is wearing a hatPrimarySynergisticA1:YesA2:Yes, it isA3:Yes, it is outdoorA4:Yes, on a beachA5:NoA6:Yes, in a parkA1:YesA2:Yes, on a beachA3:Yes, it is outdoorA4:Yes, it is outdoorsA5,Yes, it isA6:NoQ:Is this outdoors?A:No, a beachPrimaryA1:Yes, in the backgroundA2:Yes, there areA3:Yes, in the distanceA4:Yes, there 1 in the backgroundA5:Yes, there areA1:WhiteA2:White and blueA3:White with a blue tailA4:White and blackA5:Blue and whiteA6:Grey and blueQ:What coloris the jet?A:White and bluePrimarySynergisticSynergisticA1:YesA2:Yes, it looks like it A3:Yes, it isA4:No, it looks like stoneA5:NoA6:I can’t tellA1:No, it looks like its made of stoneA2:No, it looks like stoneA3:Yes, it appears to be brickA4:No, it looks like concreteA5:Yes, it is brickA6:YesQ:Is the church brick?A:It looks made of stoneA1:VisorA2:YesA3:A visorA4:Yes, a visorA5:No, a visorA6:NoA1:Yes, a white visorA2:Visor, yesA3:A visorA4:Yes, a visorA5:VisorA6:No, a visorQ:Is she wearing a hat?A:A visorPrimarySynergisticPrimarySynergisticQ:Can you see any people? A:Part of a personPrimaryA1:YesA2:Just 1A3:Part of 1A4:1A5:Yes 1A6:Just the legs of someoneA1:Just the legs of someoneA2:Just the legs of 1 personA3:I can see the legs of 1 personA4:Just legsA5:Just the legs of 1A6: YesSynergisticQ:Are there any buildings?A:In the background, I can see 1A1:WhiteA2:GrayA3:GreyA4:White and blueA5:SilverA6:Blue and whiteA1:Yes, there is 1 behind the truckA2:Yes, I see a fewA3:Yes, there is a building behind itA4:Yes, there is 1 behind the busA5:Yes, I see 1PrimarySynergisticReferences

[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
visual question answering. In CVPR, volume 3, page 6, 2018.
3

[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
Vqa: Visual question answering. In Proceedings of the IEEE
international conference on computer vision, pages 2425–
2433, 2015. 1, 2

[3] S¨oren Auer, Christian Bizer, Georgi Kobilarov,

Jens
Lehmann, Richard Cyganiak, and Zachary Ives. Dbpedia:
A nucleus for a web of open data.
In The semantic web,
pages 722–735. Springer, 2007. 3

[4] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,
Deshraj Yadav, Jos´e MF Moura, Devi Parikh, and Dhruv Ba-
tra. Visual dialog. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, volume 2, 2017.
2, 3, 5, 6, 7

[5] Harm De Vries, Florian Strub, Sarath Chandar, Olivier
Pietquin, Hugo Larochelle, and Aaron C Courville. Guess-
what?! visual object discovery through multi-modal dia-
logue. In CVPR, volume 1, page 3, 2017. 3

[6] Harm De Vries, Florian Strub,

J´er´emie Mary, Hugo
Larochelle, Olivier Pietquin, and Aaron C Courville. Mod-
ulating early visual processing by language. In Advances in
Neural Information Processing Systems, pages 6594–6604,
2017. 2

[7] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,
Trevor Darrell, and Marcus Rohrbach. Multimodal com-
pact bilinear pooling for visual question answering and vi-
sual grounding. arXiv preprint arXiv:1606.01847, 2016. 2,
3

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014. 3

[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 2

[10] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term

memory. Neural computation, 9(8):1735–1780, 1997. 2

[11] Allan Jabri, Armand Joulin, and Laurens van der Maaten.
Revisiting visual question answering baselines. In European
conference on computer vision, pages 727–739. Springer,
2016. 2

[12] Unnat Jain, Svetlana Lazebnik, and Alexander G Schwing.
Two can play this game: Visual dialog with discriminative
question generation and answering.
In Proc. CVPR, vol-
ume 1, 2018. 2, 3

[13] Eric Jang, Shixiang Gu, and Ben Poole.

reparameterization with gumbel-softmax.
arXiv:1611.01144, 2016. 3

Categorical
arXiv preprint

[14] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bi-
linear attention networks. arXiv preprint arXiv:1805.07932,
2018. 3

[15] Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee
Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Hadamard
product for low-rank bilinear pooling.
arXiv preprint
arXiv:1610.04325, 2016. 3

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 6

[17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, 2014. 6

[18] Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, and
Dhruv Batra. Best of both worlds: Transferring knowledge
from discriminative learning to a generative visual dialog
model. In Advances in Neural Information Processing Sys-
tems, pages 314–324, 2017. 2, 3

[19] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher.
Knowing when to look: Adaptive attention via a visual sen-
tinel for image captioning.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), volume 6, page 2, 2017. 5

[20] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual
attention networks for multimodal reasoning and matching.
arXiv preprint arXiv:1611.00471, 2016. 2

[21] Medhini Narasimhan and Alexander G Schwing. Straight to
the facts: Learning knowledge base retrieval for factual vi-
sual question answering. arXiv preprint arXiv:1809.01124,
2018. 3

[22] Ethan Perez, Harm De Vries, Florian Strub, Vincent Du-
moulin, and Aaron Courville. Learning visual reasoning
without strong priors.
arXiv preprint arXiv:1707.03017,
2017. 2

[23] Kihyuk Sohn.

Improved deep metric learning with multi-
class n-pair loss objective. In Advances in Neural Informa-
tion Processing Systems, pages 1857–1865, 2016. 4

[24] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to
sequence learning with neural networks. In Advances in neu-
ral information processing systems, pages 3104–3112, 2014.
1, 5, 7

[25] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-
mitru Erhan. Show and tell: A neural image caption gen-
erator. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 3156–3164, 2015. 1

[26] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and An-
ton van den Hengel. Fvqa: Fact-based visual question an-
swering. IEEE transactions on pattern analysis and machine
intelligence, 2017. 3

[27] Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen,
and Tie-Yan Liu. A theoretical analysis of ndcg ranking
measures.
In Proceedings of the 26th Annual Conference
on Learning Theory (COLT 2013), volume 8, 2013. 6

[28] Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and
Anton van den Hengel. Ask me anything: Free-form vi-
sual question answering based on knowledge from external

10442

In Proceedings of the IEEE Conference on Com-
sources.
puter Vision and Pattern Recognition, pages 4622–4630,
2016. 3

[29] Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, and Anton
van den Hengel. Are you talking to me? reasoned visual di-
alog generation through adversarial learning. arXiv preprint
arXiv:1711.07613, 2017. 1, 2, 3

[30] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and
Alex Smola. Stacked attention networks for image question
answering. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 21–29, 2016. 2
[31] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan:
Sequence generative adversarial nets with policy gradient. In
AAAI, pages 2852–2858, 2017. 3

[32] Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi-
modal factorized bilinear pooling with co-attention learning
for visual question answering. IEEE International Confer-
ence on Computer Vision (ICCV), pages 1839–1848, 2017.
3, 6

[33] Zhou Yu, Jun Yu, Chenchao Xiang, Zhou Zhao, Qi Tian,
and Dacheng Tao. Rethinking diversiﬁed and discrimina-
tive proposal generation for visual grounding. arXiv preprint
arXiv:1805.03508, 2018. 1

10443

