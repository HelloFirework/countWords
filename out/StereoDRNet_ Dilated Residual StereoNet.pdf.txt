StereoDRNet: Dilated Residual StereoNet

Rohan Chabra 1†

Julian Straub 2

Chris Sweeney2

Richard Newcombe2

Henry Fuchs1

1University of North Carolina at Chapel Hill

2Facebook Reality Labs

1{rohanc, fuchs}@cs.unc.edu

2julian.straub@oculus.com, {sweenychris, richard.newcombe}@fb.com

Figure 1: StereoDRNet enables estimation of high quality depth maps that opens the door to high quality reconstruction by
passive stereo video. In this ﬁgure we compare the output from dense reconstruction [15] built form depth maps generated
by StereoDRNet, PSMNet [2] and a structured light system [23] (termed Ground Truth). We report and visualize point-to-
plane distance RMS error on the reconstructed meshes with respect to the ground truth demonstrating the improvement in
reconstruction over the state-of-the-art.

Abstract

We propose a system that uses a convolution neural net-
work (CNN) to estimate depth from a stereo pair followed
by volumetric fusion of the predicted depth maps to pro-
duce a 3D reconstruction of a scene. Our proposed depth
reﬁnement architecture, predicts view-consistent disparity
and occlusion maps that helps the fusion system to produce
geometrically consistent reconstructions. We utilize 3D di-
lated convolutions in our proposed cost ﬁltering network
that yields better ﬁltering while almost halving the compu-
tational cost in comparison to state of the art cost ﬁltering
architectures. For feature extraction we use the Vortex Pool-
ing architecture [24]. The proposed method achieves state
of the art results in KITTI 2012, KITTI 2015 and ETH 3D
stereo benchmarks. Finally, we demonstrate that our sys-
tem is able to produce high ﬁdelity 3D scene reconstructions
that outperforms the state of the art stereo system.

1. Introduction

Depth from stereo vision has been heavily studied in
computer vision ﬁeld for the last few decades. Depth es-

†Work performed during internship at Facebook Reality Labs.

timation has various applications in autonomous driving,
dense reconstruction and 3D objects and human tracking.
Virtual Reality and Augmented Reality systems require
depth estimations to build dense spatial maps of the environ-
ment for interaction and scene understanding. For proper
rendering and interaction between virtual and real objects
in an augmented 3D world, the depth is expected to be both
dense and correct around object boundaries. Depth sensors
such as structured light and time of ﬂight sensors are of-
ten used to build such spatial maps of indoor environments.
These sensors often use illumination sources which require
power and space that exceeds the expected budget of an en-
visioned AR system. Since these sensors use infrared vi-
sion, they do not work well in bright sun light environment
or in presence of other infrared sources.
On the other hand, the depth from stereo vision systems
have a strong advantage of working in both indoors and
in sunlight environments. Since these systems use passive
image data, they do not interfere with each other or with
the environment materials. Moreover, the resolution of pas-
sive stereo systems is typically greater than the sparse pat-
terns used in structured light depth sensors, so these meth-
ods have capabilities to produce depth with accurate ob-

111786

ject boundaries and corners. Due to recent advancements in
camera and mobile technology the image sensors have dra-
matically reduced in size and have signiﬁcantly improved
in resolution and image quality. All these qualities makes
passive stereo system a better ﬁt for being a depth estima-
tor for a AR or VR system. However, stereo systems have
their own disadvantages, such as ambiguous predictions in
texture-less or repeating/confusing textured surfaces.
In
order to deal with these homogeneous regions traditional
methods make use of handcrafted functions and optimize
the parameters globally on the entire image. Recent meth-
ods use machine learning to derive the functions and it’s
parameters from the data that is used in training. As these
functions tend to be highly non-linear, they tend to yield
reasonable approximations even on the homogeneous and
reﬂective surfaces.

Our key contributions are as follows:

• Novel Disparity Reﬁnement Network: The main mo-
tivation of our work is to predict geometrically consistent
disparity maps for stereo input that can be directly used
by TSDF-based fusion system like KinectFusion [15] for
simultaneous tracking and mapping.
Surface normals
are an important factor in fusion weight computation in
KinectFusion-like systems, and we observed that state of
the art stereo systems such as PSMNet produces disparity
maps that are not geometrically consistent which negatively
affect TSDF fusion. To address this issue, we propose a
novel reﬁnement network which takes geometric error Eg,
photometric error Ep and unreﬁned disparity as input and
produces reﬁned disparity (via residual learning) and the
occlusion map.
• 3D Dilated Convolutions in Cost Filtering: State of
the art stereo systems such as PSMNet[2] and GC-Net[7]
that use 3D cost ﬁltering approach use most of the compu-
tational resources in the ﬁltering module of their system.
We observe that using 3D dilated convolutions in all three
dimensions i.e (width, height, and disparity channels) in a
structure shown in Fig. 4 gave us better results with less
compute (refer to Table.1).
• Other Contributions: We observe that Vortex Pooling
compared to spatial pyramid pooling (used in PSMNet)
provides better results (refer to ablation study 2). We found
the exclusion masks used to ﬁlter non-conﬁdent regions
of ground truth for ﬁne-tuning our model as discussed in
Sec 4.4 to be very useful in obtaining sharp edges and
ﬁne details in disparity predictions. We achieve 1.3 - 2.1
cm RMSE on 3D reconstructions of three scenes that we
prepared using structured light system proposed in [23].

2. Related Work

Depth from stereo has been widely explored in the
literature, we refer interested readers to surveys and meth-

ods described in [20]. Broadly speaking stereo matching
can be categorized into computation of cost metrics, cost
aggregation, global or semi-global optimization [4] and
reﬁnement or ﬁltering processes. Traditionally global cost
ﬁltering approaches used discrete labeling methods such
as Graph Cuts [11] or used belief propagation techniques
described in [10] and [1]. Total Variation denoising [19]
has been used in cost ﬁltering by methods described in
[26], [16] and [14].
The state of the art in disparity estimation techniques use
CNNs. MC-CNN [27] introduced a Siamese network to
compare two image patches. The scores on matching was
used along with the semi-global matching process [4]
to predict consistent disparity estimation. DispNet [13]
demonstrates an end-to-end disparity estimation neural
network with a correlation layer (dot product of features)
for stereo volume construction. Liang et al. [12] improved
DispNet by introducing novel iterative ﬁltering process.
GC-Net [7] introduces a method to ﬁlter 4D cost using a
3D cost ﬁltering approach and the soft argmax process to
regress depth. PSMNet [2] improved GC-Net by enriching
features with better global context using pyramid spatial
pooling process. They also show effective use of stacked
residual networks in cost ﬁltering process.
Xie et al. [24] introduce vortex pooling which is an
improvement of the atrous spatial pooling approach used
in Deep lab [3]. Atrous pooling uses convolutions with
various dilation steps to increase receptive ﬁelds of a CNN
ﬁlter. The vortex pooling technique uses average pooling
in grids of varying dimensions before dilated convolutions
to utilize information from the pixels which were not used
in bigger dilation steps. The size of average pool grids
grows with the increase in dilation size. We use the feature
extraction described in Vortex pooling and improve the cost
ﬁltering approach described by PSMNet.
Our proposed reﬁnement network takes geometric error
Eg, photometric error Ep and unreﬁned disparity as input
and produces reﬁned disparity (via residual learning) and
the occlusion map. Reﬁnement procedures proposed in
CRL [17], iResNet [12], StereoNet [8] and FlowNet2 [5]
only use photometeric error (either in image or feature
domain) as part of the input in the reﬁnement networks. To
the best of our knowledge we are the ﬁrst to explore the
importance of geometric error and occlusion training for
disparity reﬁnement.

3. Algorithm

In this section we describe our architecture that predicts
disparity for the input stereo pair. Instead of using a generic
encoder-decoder CNN we break our algorithm into feature
extraction, cost volume ﬁltering and reﬁnement procedures.

11787

3.1. Feature Extraction

The feature extraction starts with a small shared weight
Siamese network which takes input as images and encodes
the input to a set of features. As these features will be used
for stereo matching we want them to have both local and
global contextual information. To encode local spatial in-
formation in our feature maps we start by downsampling
the input by use of convolutions with stride of 2. Instead of
having a large 5 × 5 convolution we use three 3 × 3 ﬁlters
where ﬁrst convolution has stride of 2. We bring the reso-
lution to a fourth by having two of such blocks. In order
to encode more contextual information we choose Vortex
Pooling [24] on the learned local feature maps Fig. 3. Each
of our convolutions are followed by batch normalization and
ReLU activation except on the last 3x3 convolution on the
spatial pooling output. In order to keep the feature infor-
mation compact we keep the feature dimension size as 32
throughout the feature extraction process.

3.2. Cost Volume Filtering

We use the features extracted in the previous step to pro-
duce a stereo cost volume. While several approaches in the
literature ([7],[13]) use concatenation or dot products of the
stereo features to obtain the cost volume, we found simple
arithmetic difference to be just as effective.

While the simple argmin on the cost should in princi-
ple lead to the correct local minimum solution, it has been
shown several times in literature [16], [4],[20] that it is
common for the solution to have several local minima. Sur-
faces with homogeneous or repeating texture are particu-
larly prone to this problem. By posing the cost ﬁltering as a
deep learning process with multiple convolutions and non-
linear activations we attempt to resolve these ambiguities
and ﬁnd the correct local minimum.

We start by processing our cost volume with a 3 × 3 × 3
convolution along the width, height and depth dimensions.
We then reduce the resolution of the cost by a convolution
with stride of 2 followed by convolutions with dilation 1, 2,
4 in parallel. A convolution on the concatenation of the di-
lated convolution ﬁlters is used to combine the information
fetched from varying receptive ﬁelds.

Residual learning has been shown to be very effective
in disparity reﬁnement process so we propose a cascade of
such blocks to iteratively improve the quality of our dispar-
ity prediction. We depict the entire cost ﬁltering process as
Dilated Residual Cost Filtering in Fig. 4. In this ﬁgure no-
tice how our network is designed to produce k = 3 disparity
maps labeled as dk.

Our network architecture that supports reﬁnement
predicts disparities for both left and right view as separate
channels in disparity predictions dk. Note that we construct
the cost for both left and right views and concatenate them
before ﬁltering; this ensures that the cost ﬁltering method

is provided with cost information for both views. Please
refer to Table 3 in supplementary text for exact architecture
details.

3.3. Disparity Regression

In order to have a differentiable argmax we use soft
argmax as proposed by GC-Net [7]. For each pixel i the
regressed disparity estimation di is deﬁned as a weighted
softmax function:

di =

N

X

d=1

d

e−Ci (d)

N
P
d′=1

e−Ci (d′ )

,

(1)

where Ci is the cost at pixel i and N is the maximum dis-
parity. The loss Lk for each of the proposed disparity maps
dk (as shown in Fig. 4) in our dilated residual cost ﬁltering
architecture, relies on the Huber loss ρ and is deﬁned as:

M

Lk =

X

ρ(dk

i , ˆdi) ,

i

(2)

i and ˆdi are the estimated and ground truth disparity
where dk
at pixel i, respectively and M is the total number of pixels.
The total data loss Ld is deﬁned as:

Ld =

3

X

k=1

wkLk ,

(3)

where wk is the weight for each disparity map dk.

3.4. Disparity Reﬁnement

In order to make the disparity estimation robust to oc-
clusions and view consistency we further optimize the es-
timate. For brevity we label the third disparity prediction
d3 (k = 3) described in Sec. 3.2 for left view as Dl and for
right view as Dr. In our reﬁnement network we warp the
right image Ir to left view via the warp W and evaluate the
image reconstruction error map Ep for the left image Il as:

Ep = |Il − W (Ir, Dr)| .

(4)

By warping Dr to the left view and using the left disparity
Dl we can evaluate the geometric consistency error map Eg
as:

Eg = |Dl − W (Dr, Dl)| .

(5)

While we could just reduce these error terms directly into
a loss function, we observed signiﬁcant improvement by us-
ing photo-metric and geometric consistency error maps as
input to the reﬁnement network as these error terms are only
meaningful for non occluding pixels (only pixels for which
the consistency errors can be reduced).

Our reﬁnement network takes as input left image Il, left
disparity map Dl, image reconstruction error map Ep and

11788

Figure 2: StereoDRNet network architecture pipeline.

Our total loss function L is deﬁned as

L = Ld + λ1Lr + λ2Lo ,

(8)

where λ1 and λ2 are scalar weights.

3.5. Training

We implemented our neural network code in PyTorch.
We tried to keep the training of our neural network similar
to one described in PSMNet [2] for ease of comparison. We
used Adam optimizer [9] with β1 = 0.9 and β2 = 0.999 and
normalized the image data before passing it to the network.
In order to optimize the training procedure we cropped the
images to 512x256 resolution. For training we used a mini-
batch size of 8 on 2 Nvidia Titan-Xp GPUs. We used w1 =
0.2, w2 = 0.4, w3 = 0.6, λ1 = 1.2 and λ2 = 0.3 weights in
our proposed loss functions Eq. 3 and Eq. 8.

4. Experiments

We tested our architecture on rectiﬁed stereo datasets
such as SceneFlow, KITTI 2012, KITTI 2015 and ETH3D.
We also demonstrate the utility of our system in building
3D reconstruction of indoor scenes. See the supplementary
material for additional visual comparisons.

4.1. SceneFlow Dataset

SceneFlow [13] is a synthetic dataset with over 30, 000
stereo pairs for training and around 4000 stereo pairs for
evaluation. We use both left and right ground truth dispari-
ties for training our network. We compute the ground truth
occlusion map by deﬁning as occluded any pixel with dis-
parities inconsistency larger than 1 px. This dataset is chal-
lenging due to presence of occlusions, thin structures and
large disparities.

In Fig. 6 we visually compare our results with PSM-
Net [2]. Our system infers better structural details in the
disparity image and also produces consistent depth maps
with signiﬁcantly less errors in homogeneous regions. We
further visualize the effect of our reﬁnement network in our
supplementary section.

11789

Figure 3: StereoDRNet Vortex Pooling architecture derived
from [24].

geometric error map Eg. We ﬁrst ﬁlter left image and recon-
struction error and left disparity and geometric error map
Eg independently by using one layer of convolution fol-
lowed by batch normalization. Both these results are then
concatenated and followed by atrous convolution [18] to
sample from a larger context without increasing the network
size. We used dilations with rate 1, 2, 4, 8, 1, and 1 respec-
tively. Finally a single 3 × 3 convolution without ReLU or
batch normalization is used to output an occlusion map O
and a disparity residual map R. Our ﬁnal reﬁned dispar-
ity map is labeled as Dref . We demonstrate our reﬁnement
network in Fig. 5 and provide exact architecture details in
Table 2 of our supplementary text.

We compute the cross entropy loss on the occlusion map

O as Lo

Lo = H(O, ˆO) ,

where ˆO is the ground truth occlusion map.

The reﬁnement loss Lr is deﬁned as

M

Lr =

X

ρ(dr

i , ˆdi) ,

i

(6)

(7)

where dr
map Dref and M is the total number of pixels.

i is the value for a pixel i in our reﬁned disparity

Figure 4: Proposed dilated cost ﬁltering approach with residual connections.

Figure 5: StereoDRNet reﬁnement architecture.

Figure 6: Disparity prediction comparison between our net-
work (Stereo-DRNet) and PSMNet [2] on the SceneFlow
dataset. The top row shows disparity and the bottom row
shows the EPE map. Note how our network is able to re-
cover thin and small structures and at the same times shows
lower error in homogeneous regions.

Table 1 shows a quantitative analysis of our architec-
ture with and without reﬁnement network. Stereo-DRNet
achieves signiﬁcantly lower end point error while reduc-
ing computation time. Our proposed cost ﬁltering ap-
proach achieves better accuracy with signiﬁcantly less com-
pute, demonstrating the effectiveness of the proposed di-
lated residual cost ﬁltering approach.
Ablation study: In Table 2 we show a complete EPE break-
down for different parts of our network on the SceneFlow
dataset. Both vortex pooling and reﬁnement procedure add
marginal performance gains. Co-training occlusion map
with residual disparity drastically improves the mean end
point disparity error of the ﬁnal disparity from 0.93 px to
0.86 px. Passing only the photometric error into the reﬁne-
ment network actually degrades the performance.

Method

EPE Total FLOPS 3D-Conv FLOPS FPS

CRL[17]

1.32

-

-

GC-Net[7] 2.51

8789 GMac

8749 GMac

PSMNet[2] 1.09

2594 GMac

2362 GMac

Ours

0.98 1410 GMac

1119 GMac

Ours-Ref

0.86 1711 GMacs

1356 GMacs

2.1

1.1

2.3

4.3

3.6

Table 1: Quantitative comparison of the proposed Stereo-
DRNet with the state of the art methods on the SceneFlow
dataset. EPE represent the mean end point error in dispar-
ity. FPS and FLOPS (needed by the convolution layers) are
measured on full 960 × 540 resolution stereo pairs. No-
tice even our unreﬁned disparity architecture outperforms
the state of the art method PSMNet [2] while requiring sig-
niﬁcantly less computation.

4.2. KITTI Datasets

We evaluated our method on both KITTI 2015 and
KITTI 2012 datasets. These data sets contain stereo pairs
with semi-dense depth images acquired using a LIDAR sen-
sor that can be used for training. The KITTI 2012 dataset
contains 194 training and 193 test stereo image pairs from
static outdoor scenes. The KITTI 2015 dataset contains 200
training and 200 test stereo image pairs from both static and

11790

Network Architecture

SceneFlow KITTI-2015

Pooling

Cost Filtering Reﬁnement
d1 d2

d3 E

E

L

p

g

o

EPE

Val Error(%)

Pyramid X

Vortex X

Vortex X X

Vortex X X X

Pyramid X X X

Vortex X X X X

Vortex X X X

X

Vortex X X X X X

Vortex X X X X X X

Pyramid X X X X X X

1.17

1.13

0.99

0.98

1.00

1.03

0.95

0.93

0.86

0.96

2.28

2.14

1.88

1.74

1.81

-

-

-

-

-

Table 2: Ablation study of network architecture settings on
SceneFlow and KITTI-2015 evaluation dataset.

Method

2px

3px

Avg Error

Noc All Noc All Noc All

Time(s)

GC-NET[7]

2.71 3.46 1.77 2.30 0.6

EdgeStereo[21] 2.79 3.43 1.73 2.18 0.5

PDSNet[22]

3.82 4.65 1.92 2.53 0.9

SegStereo[25]

2.66 3.19 1.68 2.03 0.5

PSMNet[2]

2.44 3.01 1.49 1.89 0.5

Ours

2.29 2.87 1.42 1.83 0.5

0.7

0.6

1.0

0.6

0.6

0.5

0.90

0.48

0.50

0.60

0.41

0.23

Table 3: Comparison of disparity estimation from Stereo-
DRNet with state of the art published methods on KITTI
2012 dataset.

Method

DN-CSS[6]

GC-NET[7]

CRL[17]

EdgeStereo[21]

PDSNet[22]

PSMNet[2]

SegStereo[25]

Ours

All(%)

Noc(%)

D1-bg D1-fg D1-all D1-bg D1-fg D1-all

Time(s)

2.39

2.21

2.48

2.27

2.29

1.86

1.88

1.72

5.71

6.16

3.59

4.18

4.05

4.62

4.07

4.95

2.94

2.87

2.67

2.59

2.58

2.32

2.25

2.26

2.23

2.02

2.32

2.12

2.09

1.71

1.76

1.57

4.96

5.58

3.12

3.85

3.69

4.31

3.70

4.58

2.68

2.61

2.45

2.40

2.36

2.14

2.08

2.06

0.07

0.90

0.47

0.27

0.50

0.41

0.60

0.23

Table 4: Comparison of disparity estimation from Stereo-
DRNet with state of the art published methods on KITTI
2015 dataset.

dynamic outdoor scenes.
Training and ablation study: Since KITTI data sets con-
tain only limited amount of training data, we ﬁne tuned our
model on the SceneFlow dataset. In our training we used
80% stereo pairs for training and 20% stereo pairs for eval-
uation. We demonstrate the ablation study of our proposed
method on KITTI 2015 dataset Table 2. Note how our pro-
posed dilated residual architecture and the use of Vortex
pooling for feature extraction consistently improve the re-

Method

All

Noc

1px 2px 4px RMSE 1px

2px 4px RMSE

PSMNet[2] 5.41 1.31 0.54

iResNet[12] 4.04 1.20 0.34

DN-CSS[6] 3.00 0.96 0.34

Ours

4.84 0.96 0.30

0.75

0.59

0.56

0.55

5.02 1.09 0.41

3.68 1.00 0.25

2.69 0.77 0.26

4.46 0.83 0.24

0.66

0.51

0.48

0.50

Table 5: Comparison of disparity estimation from Stereo-
DRNet with state of the art published methods on ETH 3D
dataset.

sults. We did not achieve signiﬁcant gains by doing reﬁne-
ment on KITTI datasets as these datasets only contain la-
beled depth for sparse pixels. Our reﬁnement procedure im-
proves disparity predictions using view consistency checks
and sparsity in ground truth data affected the training pro-
cedure. We demonstrate that data sets with denser training
data enabled the training and ﬁne-tuning of our reﬁnement
model.
Results: We evaluated our Dilated residual network with-
out ﬁltering on both these datasets and achieved state of the
art results on KITTI 2012 Table 3 and comparable results
with best published method on KITTI 2015 Table 4. On
KITTI 2015 dataset the three columns D1-bg, D1-fg and
D1-all mean that the pixels in the background, foreground,
and all areas, respectively, were considered in the estimation
of errors. We perform consistently well in D1-bg meaning
background areas, we achieve comparable results with state
of art method in all pixels and better results in non-occluded
regions. On KITTI 2012 dataset ”Noc” means non occluded
regions and ”All” mean all regions. Notice, that we perform
comparable against SegStereo [25] on KITTI 2015 but way
better in KITTI 2012 dataset.

4.3. ETH3D Dataset

We again used our pre-trained network trained on Scene-
ﬂow dataset and ﬁne-tuned it on the training set provided
in the dataset. ETH dataset contains challenging scenes of
both outside and indoor environment. According to our Ta-
ble 5 we perform best on almost half of the evaluation met-
rics, our major competitor in this evaluation was DN-CSS
[6]. Although, we observe that this method did not perform
well on KITTI 2015 data set Table 4. Notice, as this data
set contained dense training disparity maps of both stereo
views we were able to train and evaluate our reﬁnement net-
work on this data set.

4.4. Indoor Scene Reconstruction

We use the scanning rig used in recent work [23] for
preparing ground truth dataset for supervised learning of
depth and added one more RGB camera to the rig to obtain
a stereo image pair. We kept the baseline of the stereo pair
to be about 10cm. We trained our StereoDRNet network on

11791

SceneFlow as described in section 4.1 and then ﬁne tuned
the pre-trained network on 250 stereo pairs collected in the
indoor area by our scanning rig. We observed that the net-
work to quickly adapted to our stereo rig with a minimal
amount of ﬁne-tuning.

For preparing ground truth depth we found rendered
depth from complete scene reconstruction to be a better es-
timate than the live sensor depth which usually suffers from
occlusions and depth uncertainties. Truncated signed dis-
tance function (TSDF) was used to fuse live depth maps
into a scene as described in [15].

Figure 7: We show a training example with the left image,
ground truth depth and the exclusion mask. Note that the
glass, mirrors and the sharp corners of the table are excluded
from training as indicated by the yellow pixels in the occlu-
sion mask. Note, that this example was not part of our actual
training set.

Figure 8: This ﬁgure demonstrates that our StereoDRNet
network produces better predictions on thin reﬂective legs
of the chair and some portions of the glass. We used oc-
clusion mask predicted by our network to clip occluding
regions. Yellow region in the ground truth are the regions
that belong to our proposed exclusion mask.

The infrared-structure light depth sensors are known
to be unresponsive to dark and highly reﬂective surfaces.
Moreover, the quality of TSDF fusion is limited to the res-

Figure 9: This ﬁgure demonstrates 3D reconstruction of a
living room in an apartment prepared by TSDF fusion of
the predicted depth maps from our system. We visualize
two views of the textured mesh and surface normals in top
and bottom rows respectively.

olution of the voxel size. Hence we expect the reconstruc-
tions to be overly smooth in some areas such as table cor-
ners or sharp edges of plant leaves. In order to avoid con-
taminating our training data with false depth estimation, we
use a simple photometric error threshold to mask out the
pixels from training where the textured model projection
color disagrees with the real images. We show one such ex-
ample in Fig. 7 where glass, mirrors and the sharp corners
of the table are excluded from training. Although, the sys-
tem from Whelan et al. [23] can obtain ground truth planes
of mirrors and glass we avoid depth supervision on them in
this work as it is beyond the scope of a stereo matching pro-
cedure to obtain depth on reﬂectors.
We demonstrate visualizations of the depth predictions from
the stereo pair in Fig. 8. Notice, our prediction is able to re-
cover sharp corners of the table, thin reﬂective legs of the
chair and several thin structures in kitchen dataset as a re-
sult of ﬁltering process used in training. It is interesting to
see that we recover the top part of the glass correctly but not
the bottom part of the glass which suffers from reﬂections.
The stereo matching model simply treats reﬂectors as win-
dows in presence of reﬂections.
Results and evaluations: We demonstrate visualizations of
full 3D reconstruction of a living room in an apartment pre-
pared by TSDF fusion of the predicted depth maps from
our system in Fig. 9. For evaluation study we prepared
three small data sets that we refer as “Sofa and cushions”
demonstrated in Fig. 1, “Plants and couch” and “Kitchen
and bike” demonstrated in Fig. 10. We report point-to-
plane root mean squared error (RMSE) of the reconstructed
3D meshes from fusion of depth maps obtained from PSM-
Net [2] and our reﬁned network. We obtain a RMSE of
1.3 cm on the simpler “Sofa and cushions” dataset. Note
that our method captured high frequency structural details

11792

Figure 10: Comparison of 3D reconstruction using fusion of depth maps from our StereoDRNet network (middle), PSM-
Net [2] (right) and depth maps from the structured light system (left) described in [23] (termed Ground Truth). We report and
visualize point-to-plane distance RMS error on the reconstructed meshes with respect to the ground truth mesh. Dark yellow
boxes represent the regions where our reconstruction yields details that the structured light sensor or PSMNet were not able
to capture. Light yellow boxes represent regions where StereoDRNet outperforms PSMNet.

on the cushions which were not captured by PSMNet or
the structured light sensor. “Plants and couch” represents
a more difﬁcult scene as it contained a directed light source
casting shadows. For this dataset StereoDRNet obtained
2.1 cm RMSE whereas PSMNet obtained 2.5 cm RMSE.
Notice, that our reconstruction is not only cleaner but pro-
duces minimal errors in the shadowed areas (shadows cast
by book shelf and left plant). “Kitchen and bike” dataset
cluttered and contains reﬂective objects making it the hard-
est dataset. While our system still achieved 2.1 cm RMSE,
the performance of PSMNet degraded to 2.8 cm RMSE. No-
tice, that our reconstruction contains the faucet (highlighted
by yellow box) in contrast to the structured light sensor and
PSMNet reconstructions. For all evaluations we used ex-
actly the same training dataset for ﬁne-tuning our Stereo-
DRNet and PSMNet.

5. Conclusion

Depth estimation from passive stereo images is a chal-
lenging task. Systems from related work suffer in regions
with homogeneous texture or surfaces with shadows and
specular reﬂections. Our proposed network architecture
uses global spatial pooling and dilated residual cost ﬁltering
techniques to approximate the underlying geometry even in
above mentioned challenging scenarios. Furthermore, our
reﬁnement network produces geometrically consistent dis-
parity maps with the help of occlusion and view consistency
cues. The use of perfect synthetic data and careful ﬁltering
of real training data enabled us to recover thin structures and
sharp object boundaries. Finally, we demonstrate that our
passive stereo system, when used for building 3D scene re-
constructions in challenging indoor scenes, approaches the
quality of state-of-the-art structured light systems [23].

11793

References

[1] Michael Bleyer, Christoph Rhemann, and Carsten Rother.
Patchmatch stereo-stereo matching with slanted support win-
dows. In Bmvc, volume 11, pages 1–11, 2011. 2

[2] Jia-Ren Chang and Yong-Sheng Chen.

Pyramid stereo
matching network. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5410–
5418, 2018. 1, 2, 4, 5, 6, 7, 8

[3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence, 40(4):834–848, 2018. 2

[4] Heiko Hirschmuller. Stereo processing by semiglobal match-
ing and mutual information. IEEE Transactions on pattern
analysis and machine intelligence, 30(2):328–341, 2008. 2,
3

[5] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keu-
per, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0:
Evolution of optical ﬂow estimation with deep networks. In
IEEE conference on computer vision and pattern recognition
(CVPR), volume 2, page 6, 2017. 2

[6] Eddy Ilg, Tonmoy Saikia, Margret Keuper, and Thomas
Brox. Occlusions, motion and depth boundaries with a
generic network for disparity, optical ﬂow or scene ﬂow
estimation.
In European Conference on Computer Vision
(ECCV), 2018. 6

[7] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter
Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry.
End-to-end learning of geometry and context for deep stereo
regression. CoRR, vol. abs/1703.04309, 2017. 2, 3, 5, 6

[8] Sameh Khamis, Sean Fanello, Christoph Rhemann, Adarsh
Kowdle, Julien Valentin, and Shahram Izadi. Stereonet:
Guided hierarchical reﬁnement for real-time edge-aware
depth prediction. arXiv preprint arXiv:1807.08865, 2018.
2

[9] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 4

[10] Andreas Klaus, Mario Sormann, and Konrad Karner.
Segment-based stereo matching using belief propagation and
a self-adapting dissimilarity measure.
In Pattern Recogni-
tion, 2006. ICPR 2006. 18th International Conference on,
volume 3, pages 15–18. IEEE, 2006. 2

[11] Vladimir Kolmogorov and Ramin Zabih. Computing vi-
sual correspondence with occlusions using graph cuts.
In
Computer Vision, 2001. ICCV 2001. Proceedings. Eighth
IEEE International Conference on, volume 2, pages 508–
515. IEEE, 2001. 2

[12] Zhengfa Liang, Yiliu Feng, YGHLW Chen, and LQLZJ
Zhang. Learning for disparity estimation through feature
constancy. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 2811–2820,
2018. 2, 6

[13] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,

optical ﬂow, and scene ﬂow estimation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4040–4048, 2016. 2, 3, 4

[14] Richard Newcombe. Dense visual SLAM. PhD thesis, Impe-

rial College London, UK, 2012. 2

[15] Richard A Newcombe, Shahram Izadi, Otmar Hilliges,
David Molyneaux, David Kim, Andrew J Davison, Pushmeet
Kohi, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon.
Kinectfusion: Real-time dense surface mapping and track-
ing.
In Mixed and augmented reality (ISMAR), 2011 10th
IEEE international symposium on, pages 127–136. IEEE,
2011. 1, 2, 7

[16] Richard A Newcombe, Steven J Lovegrove, and Andrew J
Davison. Dtam: Dense tracking and mapping in real-time.
In Computer Vision (ICCV), 2011 IEEE International Con-
ference on, pages 2320–2327. IEEE, 2011. 2, 3

[17] Jiahao Pang, Wenxiu Sun, Jimmy SJ Ren, Chengxi Yang, and
Qiong Yan. Cascade residual learning: A two-stage convo-
lutional neural network for stereo matching. In ICCV Work-
shops, volume 7, 2017. 2, 5, 6

[18] George Papandreou, Iasonas Kokkinos, and Pierre-Andr´e
Savalle. Modeling local and global deformations in deep
learning: Epitomic convolution, multiple instance learning,
and sliding window detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 390–399, 2015. 4

[19] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear
total variation based noise removal algorithms. Physica D:
nonlinear phenomena, 60(1-4):259–268, 1992. 2

[20] Daniel Scharstein and Richard Szeliski. A taxonomy and
evaluation of dense two-frame stereo correspondence algo-
rithms. International journal of computer vision, 47(1-3):7–
42, 2002. 2, 3

[21] Xiao Song, Xu Zhao, Hanwen Hu, and Liangji Fang.
Edgestereo: A context integrated residual pyramid network
for stereo matching. arXiv preprint arXiv:1803.05196, 2018.
6

[22] Stepan Tulyakov, Anton Ivanov, and Francois Fleuret. Prac-
tical deep stereo (pds): Toward applications-friendly deep
stereo matching. arXiv preprint arXiv:1806.01677, 2018. 6

[23] Thomas Whelan, Michael Goesele, Steven J Lovegrove, Ju-
lian Straub, Simon Green, Richard Szeliski, Steven Butter-
ﬁeld, Shobhit Verma, and Richard Newcombe. Reconstruct-
ing scenes with mirror and glass surfaces. ACM Transactions
on Graphics (TOG), 37(4):102, 2018. 1, 2, 6, 7, 8

[24] Chen-Wei Xie, Hong-Yu Zhou, and Jianxin Wu. Vortex pool-
ing: Improving context representation in semantic segmen-
tation. arXiv preprint arXiv:1804.06242, 2018. 1, 2, 3, 4

[25] Guorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong
Segstereo: Exploiting seman-
arXiv preprint

Deng, and Jiaya Jia.
tic information for disparity estimation.
arXiv:1807.11699, 2018. 6

[26] Christopher Zach, Thomas Pock, and Horst Bischof. A glob-
ally optimal algorithm for robust tv-l 1 range image integra-
tion. In Computer Vision, 2007. ICCV 2007. IEEE 11th In-
ternational Conference on, pages 1–8. IEEE, 2007. 2

11794

[27] Jure Zbontar and Yann LeCun. Stereo matching by training
a convolutional neural network to compare image patches.
Journal of Machine Learning Research, 17(1-32):2, 2016. 2

11795

