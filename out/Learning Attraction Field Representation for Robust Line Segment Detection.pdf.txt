Learning Attraction Field Representation for Robust Line Segment Detection

Nan Xue1

,

2, Song Bai3, Fudong Wang1, Gui-Song Xia1 ∗, Tianfu Wu2, Liangpei Zhang1

1Wuhan University, China

2NC State University, USA

3University of Oxford, UK

{xuenan, fudong-wang, guisong.xia, zlp62}@whu.edu.cn,

songbai.site@gmail.com, tianfu wu@ncsu.edu

Abstract

This paper presents a region-partition based attraction
ﬁeld dual representation for line segment maps, and thus
poses the problem of line segment detection (LSD) as the
region coloring problem. The latter is then addressed by
learning deep convolutional neural networks (ConvNets)
for accuracy, robustness and efﬁciency. For a 2D line seg-
ment map, our dual representation consists of three com-
ponents: (i) A region-partition map in which every pixel is
assigned to one and only one line segment; (ii) An attrac-
tion ﬁeld map in which every pixel in a partition region is
encoded by its 2D projection vector w.r.t.
the associated
line segment; and (iii) A squeeze module which squashes
the attraction ﬁeld to a line segment map that almost per-
fectly recovers the input one. By leveraging the duality,
we learn ConvNets to compute the attraction ﬁeld maps
for raw input images, followed by the squeeze module for
LSD, in an end-to-end manner. Our method rigorously ad-
dresses several challenges in LSD such as local ambiguity
and class imbalance. Our method also harnesses the best
practices developed in ConvNets based semantic segmen-
tation methods such as the encoder-decoder architecture
and the a-trous convolution.
In experiments, our method
is tested on the WireFrame dataset [12] and the YorkUrban
dataset [6] with state-of-the-art performance obtained. Es-
pecially, we advance the performance by 4.5 percents on the
WireFrame dataset. Our method is also fast with 6.6 ∼ 10.4
FPS, outperforming most of the existing line segment detec-
tors. The source code of this paper is available at https:
//github.com/cherubicXN/afm_cvpr2019.

1. Introduction

1.1. Motivation and Objective

Line segment detection (LSD) is an important yet chal-
lenging low-level task in computer vision. The resulting

∗Corresponding author

(a) Attraction ﬁeld map representation for line segments

(b) Our approach for line segment detection

Figure 1. Illustration of the proposed method. (a) The proposed
attraction ﬁeld dual representation for line segment maps. A line
segment map can be almost perfectly recovered from its attraction
ﬁled map (AFM), by using a simple squeeze algorithm. (b) The
proposed formulation of posing the LSD problem as the region
coloring problem. The latter is addressed by learning ConvNets.

line segment maps provide compact structural information
that facilitate many up-level vision tasks such as 3D recon-
struction [6, 8], image partition [7], stereo matching [32],
scene parsing [34, 33], camera pose estimation [27], and
image stitching [25].

LSD usually consists of two steps: line heat map gen-
eration and line segment model ﬁtting. The former can
be computed either simply by the gradient magnitude map
(mainly used before the recent resurgence of deep learning)
[23, 31, 5], or by a learned convolutional neural network
(ConvNet) [26, 18] in state-of-the-art methods [12]. The
latter needs to address the challenging issue of handling un-
known multi-scale discretization nuisance factors (e.g., the
classic zig-zag artifacts of line segments in digital images)
when aligning pixels or linelets to form line segments in
the line heat map. Different schema have been proposed,
e.g., the ǫ-meaningful alignment method proposed in [23]
and the junction [24] guided alignment method proposed

1595

Line SegmentsRegion-Partition MapAttraction Field Map ConvNetAFM PredictionDetected Line Segmentsin [12]. The main drawbacks of existing two-stage methods
are in two-fold: lacking elegant solutions to solve the local
ambiguity and/or class imbalance in line heat map genera-
tion, and requiring extra carefully designed heuristics or su-
pervisedly learned contextual information in inferring line
segments in the line heat map.

In this paper, we focus on learning based LSD frame-
work and propose a single-stage method which rigorously
addresses the drawbacks of existing LSD approaches. Our
method is motivated by two observations,

• The duality between region representation and bound-
ary contour representation of objects or surfaces,
which is a well-known fact in computer vision.

• The recent remarkable progresses for image semantic
segmentation by deep ConvNet based methods such as
U-Net [22] and DeepLab V3+ [11].

So, the intuitive idea of this paper is that if we can bridge
line segment maps and their dual region representations,
we will pose the problem of LSD as the problem of re-
gion coloring, and thus open the door to leveraging the best
practices developed in state-of-the-art deep ConvNet based
image semantic segmentation methods to improve perfor-
mance for LSD. By dual region representations, it means
they are capable of recovering the input line segment maps
in a nearly perfect way via a simple algorithm. We present
an efﬁcient and straightforward method for computing the
dual region representation. By re-formulating LSD as the
equivalent region coloring problem, we address the afore-
mentioned challenges of handling local ambiguity and class
imbalance in a principled way.

1.2. Method Overview

Figure 1 illustrates the proposed method. Given a 2D
line segment map, we represent each line segment by its
geometry model using the two end-points1. In computing
the dual region representation, there are three components
(detailed in Section 3).

• A region-partition map. It is computed by assigning
every pixel to one and only one line segment based on
a proposed point to line segmentation distance func-
tion. The pixels associated with one line segment form
a region. All regions represent a partition of the image
lattice (i.e., mutually exclusive and the union occupies
the entire image lattice).

• An attraction ﬁeld map. Each pixel in a partition region
has one and only one corresponding projection point
on the geometry line segment (but the reverse is often

a one-to-many mapping). In the attraction ﬁeld map,
every pixel in a partition region is then represented by
its attraction/projection vector between the pixel and
its projection point on the geometry line segment 2.

• A light-weight squeeze module. It follows the attrac-
tion ﬁeld to squash partition regions in an attraction
ﬁeld map to line segments that almost perfectly recov-
ers the input ones, thus bridging the duality between
region-partition based attraction ﬁeld maps and line
segment maps.

The proposed method can also be viewed as an intuitive
expansion-and-contraction operation between 1D line seg-
ments and 2D regions in a simple projection vector ﬁeld:
The region-partition map generation jointly expands all line
segments into partition regions, and the squeeze module de-
generates regions into line segments.

With the duality between a line segment map and the cor-
responding region-partition based attraction ﬁeld map, we
ﬁrst convert all line segment maps in the training dataset to
their attraction ﬁeld maps. Then, we learn ConvNets to pre-
dict the attraction ﬁeld maps from raw input images in an
end-to-end way. We utilize U-Net [22] and a modiﬁed net-
work based on DeepLab V3+ [11] in our experiments. Af-
ter the attraction ﬁeld map is computed, we use the squeeze
module to compute its line segment map.

In experiments, the proposed method is tested on the
WireFrame dataset [12] and the YorkUrban dataset [6]
with state-of-the-art performance obtained comparing with
[12, 5, 1, 23]. In particular, we improve the performance
by 4.5% on the WireFrame dataset. Our method is also fast
with 6.6 ∼ 10.4 FPS, outperforming most of line segment
detectors.

2. Related Work and Our Contributions

The study of line segment detection has a very long his-
tory since 1980s [2]. The early pioneers tried to detect line
segments based upon the edge map estimation. Then, the
perception grouping approaches based on the Gestalt The-
ory are proposed. Both of these methods concentrate on
the hand-crafted low-level features for the detection, which
have become a limitation. Recently, the line segment detec-
tion and its related problem edge detection have been stud-
ied under the perspective of deep learning, which dramati-
cally improved the detection performance and brings us of
great practical importance for real applications.

2.1. Detection based on Hand crafted Features

In a long range of time, the hand-crafted low-level fea-
tures (especially for image gradients) are heavily used for

1We will have discrepancy for some intermediate points of a line seg-
ment between their annotated pixel locations and the geometric locations
when the line segment is not strictly horizontal or vertical.

2They are the same point when the pixel is on the geometry line seg-
ment, and thus we will have a zero vector. We observed that the total
number of those points are negligible in our experiments.

1596

line segment detection. These approaches can be divided
into edge map based approaches [9, 14, 28, 29, 30, 1] and
perception grouping approaches [3, 23, 5]. The edge map
based approaches treat the visual features as a discriminated
feature for edge map estimation and subsequently applying
the Hough transform [2] to globally search line conﬁgura-
tions and then cutting them by using thresholds. In contrast
to the edge map based approaches, the grouping methods
directly use the image gradients as local geometry cues to
group pixels into line segment candidates and ﬁlter out the
false positives [23, 5].

Actually, the features used for line segment detection can
only characterize the local response from the image appear-
ance. For the edge detection, only local response without
global context cannot avoid false detection. On the other
hand, both the magnitude and orientation of image gradi-
ents are easily affected by the external imaging condition
(e.g. noise and illumination). Therefore, the local nature
of these features limits us to extract line segments from im-
ages robustly. In this paper, we break the limitation of lo-
cally estimated features and turn to learn the deep features
that hierarchically represent the information of images from
low-level cues to high-level semantics.

2.2. Deep Edge and Line Segment Detection

Recently, HED [26] opens up a new era for edge per-
ception from images by using ConvNets. The learned
multi-scale and multi-level features dramatically addressed
the problem of false detection in the edge-like texture re-
gions and approaching human-level performance on the
BSDS500 dataset [20]. Followed by this breakthrough, a
tremendous number of deep learning based edge detection
approaches are proposed [18, 15, 17, 16, 19, 11]. Under
the perspective of binary classiﬁcation, the edge detection
has been solved to some extent.
It is natural to upgrade
the traditional edge map based line segment detection by
alternatively using the edge map estimated by ConvNets.
However, the edge maps estimated by ConvNets are usually
over-smoothed, which will lead to local ambiguities for ac-
curate localization. Further, the edge maps do not contain
enough geometric information for the detection. Accord-
ing to the development of deep learning, it should be more
reasonable to propose an end-to-end line segment detector
instead of only applying the advances of deep edge detec-
tion.

Most recently, Huang et al. [12] have taken an important
step towards this goal by proposing a large-scale dataset
with high quality line segment annotations and approach-
ing the problem of line segment detection as two parallel
tasks, i.e., edge map detection and junction detection. As a
ﬁnal step for the detection, the resulted edge map and junc-
tions are fused to produce line segments. To the best of our
knowledge, this is the ﬁrst attempt to develop a deep learn-

ing based line segment detector. However, due to the so-
phisticated relation between edge map and junctions, it still
remains a problem unsolved. Beneﬁting from our proposed
formulation, we can directly learn the line segments from
the attraction ﬁeld maps that can be easily obtained from
the line segment annotations without the junction cues.

Our Contributions The proposed method makes the fol-
lowing main contributions to robust line segment detection.

• A novel dual representation is proposed by bridging
line segment maps and region-partition-based attrac-
tion ﬁeld maps. To our knowledge, it is the ﬁrst work
that utilizes this simple yet effective representation in
LSD.

• With the proposed dual representation, the LSD prob-
lem is re-formulated as the region coloring problem,
thus opening the door to leveraging state-of-the-art se-
mantic segmentation methods in addressing the chal-
lenges of local ambiguity and class imbalance in exist-
ing LSD approaches in a principled way.

• The proposed method obtains state-of-the-art perfor-
mance on two widely used LSD benchmarks, the Wire-
Frame dataset (with 4.5% signiﬁcant improvement)
and the YorkUrban dataset.

3. The Attraction Field Representation

In this section, we present details of the proposed region-

partition representation for LSD.

3.1. The Region Partition Map

i , xe

i and xe

Let Λ be an image lattice (e.g., 800 × 600). A line seg-
ment is denote by li = (xs
i ) with the two end-points
being xs
i (non-negative real-valued positions due to
sub-pixel precision is used in annotating line segments) re-
spectively. The set of line segments in a 2D line segment
map is denoted by L = {l1, · · · , ln} . For simplicity, we
also denote the line segment map by L. Figure 2 illustrates
a line segment map with 3 line segments in a 10 × 10 image
lattice.

(a) Support regions

(b) Attraction vectors

(c) Squeeze module

Figure 2. A toy example illustrating a line segment map with 3
line segments, its dual region-partition map, selected vectors of
the attraction ﬁeld map and the squeeze module for obtaining line
segments from the attraction ﬁeld map. See text for details.

1597

i , xe

Computing the region-partition map for L is assigning
every pixel in the lattice to one and only one of the n line
segments. To that end, we utilize the point-to-line-segment
distance function. Consider a pixel p ∈ Λ and a line seg-
ment li = (xs
i ) ∈ L, we ﬁrst project the pixel p to
the straight line going through li in the continuous geom-
etry space.
If the projection point is not on the line seg-
ment, we use the closest end-point of the line segment as
the projection point. Then, we compute the Euclidean dis-
tance between the pixel and the projection point. Formally,
we deﬁne the distance between p and li by

d(p, li) = min
t∈[0,1]

||xs

i + t · (xe

i − xs

i ) − p||2
2,

t∗
p = arg min

t

d(p, li),

(1)

where the projection point is the original point-to-line pro-
jection point if t∗
p ∈ (0, 1), and the closest end-point if
t∗
p = 0 or 1.

So, the region in the image lattice for a line segment li is

deﬁned by

Ri = {p | p ∈ Λ; d(p, li) < d(p, lj), ∀j 6= i, lj ∈ L}.

(2)

It is straightforward to see that Ri ∩ Rj = ∅ and ∪n
i=1Ri =
Λ, i.e., all Ri’s form a partition of the image lattice. Fig-
ure 2(a) illustrates the partition region generation for a
line segment in the toy example (Figure 2). Denote by
R = {R1, · · · , Rn} the region-partition map for a line seg-
ment map L.

3.2. Computing the Attraction Field Map

Consider the partition region Ri associated with a line
segment li, for each pixel p ∈ Ri, its projection point p′ on
li is deﬁned by

p′ = xs

i + t∗

p · (xe

i − xs

i ),

(3)

We deﬁne the 2D attraction or projection vector for a

pixel p as,

a(p) = p′ − p,

(4)

where the attraction vector is perpendicular to the line seg-
ment if t∗
p ∈ (0, 1) (see Figure 2(b)). Figure 1 shows exam-
ples of the x- and y-component of an attraction ﬁeld map
(AFM). Denote by A = {a(p) | p ∈ Λ} the attraction ﬁeld
map for a line segment map L.

3.3. The Squeeze Module

Given an attraction ﬁeld map A, we ﬁrst reverse it by
computing the real-valued projection point for each pixel p
in the lattice,

v(p) = p + a(p),

(5)

and its corresponding discretized point in the image lattice,

vΛ(p) = ⌊v(p) + 0.5⌋.

(6)

where ⌊·⌋ represents the ﬂoor operation, and vΛ(p) ∈ Λ.

Then, we compute a line proposal map in which each
pixel q ∈ Λ collects the attraction ﬁeld vectors whose dis-
cretized projection points are q. The candidate set of attrac-
tion ﬁeld vectors collected by a pixel q is then deﬁned by

C(q) = {a(p) | p ∈ Λ, vΛ(p) = q},

(7)

where C(q)’s are usually non-empty for a sparse set of pix-
els q’s which correspond to points on the line segments. An
example of the line proposal map is shown in Figure 2(c),
which project the pixels of the support region for a line seg-
ment into pixels near the line segment.

With the line proposal map, our squeeze module utilizes
an iterative and greedy grouping algorithm to ﬁt line seg-
ments, similar in spirit to the region growing algorithm used
in [23].

• Given the current set of active pixels each of which has
a non-empty candidate set of attraction ﬁeld vectors,
we randomly select a pixel q and one of its attraction
ﬁeld vector a(p) ∈ C(q). The tangent direction of the
selected attraction ﬁeld vector a(p) is used as the initial
direction of the line segment passing the pixel q.

• Then, we search the local observation window cen-
tered at q (e.g., a 3 × 3 window is used in this paper) to
ﬁnd the attraction ﬁeld vectors which are aligned with
a(p) with angular distance less than a threshold τ (e.g.,
τ = 10◦ used in this paper).

– If the search fails, we discard a(p) from C(q),
and further discard the pixel q if C(q) becomes
empty.

– Otherwise, we grow q into a set and update
its direction by averaging the aligned attraction
vectors. The aligned attraction vectors will be
marked as used (and thus inactive for the next
round search). For the two end-points of the set,
we recursively apply the greedy search algorithm
to grow the line segment.

q, xe

• Once terminated, we obtain a candidate line segment
lq = (xs
q) with the support set of real-valued pro-
jection points. We ﬁt the minimum outer rectangle us-
ing the support set. We verify the candidate line seg-
ment by checking the aspect ratio between width and
length of the approximated rectangle with respect to a
predeﬁned threshold to ensure the approximated rect-
angle is “thin enough”. If the checking fails, we mark
the pixel q inactive and release the support set to be
active again.

1598

i

i

n
o
s
c
e
r
P

0.997

0.994

1

0.97

l
l

a
c
e
R

0.991

0.5

1.5

1
Scale

2

0.93

0.5

1.5

1
Scale

2

values in a small and thus numerically unstable in training.
We apply a point-wise invertible value stretching transfor-
mation for the size-normalized AFM

z′ := S(z) = −sign(z) · log(|z| + ε),

(9)

Figure 3. Veriﬁcation of the duality between line segment maps
and attraction ﬁeld maps, and its scale invariance.

where ε = 1e−6 to avoid log(0). The inverse function
S−1(·) is deﬁned by

3.4. Verifying the Duality and its Scale Invariance

We test the proposed attraction ﬁeld representation on
the WireFrame dataset [12]. We ﬁrst compute the attrac-
tion ﬁeld map for each annotated line segment map and then
compute the estimated line segment map using the squeeze
module. We run the test across multiple scales, ranging
from 0.5 to 2.0 with step-size 0.1. We evaluate the esti-
mated line segment maps by measuring the precision and
recall following the protocol provided in the dataset. Fig-
ure 3 shows the precision-recall curves. The average preci-
sion and recall rates are above 0.99 and 0.93 respectively,
thus verifying the duality between line segment maps and
corresponding region-partition based attractive ﬁeld maps,
as well as the scale invariance of the duality.

So, the problem of LSD can be posed as the region
coloring problem almost without hurting the perfor-
mance. In the region coloring formulation, our goal is to
learn ConvNets to infer the attraction ﬁeld maps for input
images. The attraction ﬁeld representation eliminates local
ambiguity in traditional gradient magnitude based line heat
map, and the predicting attraction ﬁeld in learning gets rid
of the imbalance problem in line v.s. non-line classiﬁcation.

4. Robust Line Segment Detector

In this section, we present details of learning ConvNets
for robust LSD. ConvNets are used to predict AFMs from
raw input images under the image-to-image transformation
framework, and thus we adopt encoder-decoder network ar-
chitectures.

4.1. Data Processing

Denote by D = {(Ii, Li); i = 1, · · · , N } the provided
training dataset consisting of N pairs of raw images and an-
notated line segment maps. We ﬁrst compute the AFMs
for each training image. Then, let D = {(Ii, ai); i =
1, · · · , N } be the dual training dataset. To make the AFMs
insensitive to the sizes of raw images, we adopt a simple
normalization scheme. For an AFM a with the spatial di-
mensions being W × H, the size-normalization is done by

ax := ax/W, ay := ay/H,

(8)

where ax and ay are the component of a along x and y axes
respectively. However, the size-normalization will make the

z := S−1(z′) = sign(z′)e(−|z ′|).

(10)

For notation simplicity, denote by R(·) the composite re-
verse function, and we still denote by D = {(Ii, ai); i =
1, · · · , N } the ﬁnal training dataset.

4.2. Inference

Denote by fΘ(·) a ConvNet with the parameters col-
lected by Θ. As illustrated in Figure 1(b), for an input image
IΛ, our robust LSD is deﬁned by

ˆa = fΘ(IΛ)
ˆL = Squeeze(R(ˆa))

(11)

(12)

where ˆa is the predicted AFM for the input image (the
size-normalized and value-stretched one), Squeeze(·) the
squeeze module and ˆL the inferred line segment map.

4.3. Network Architectures

We utilize two network architectures to realize fΘ(): one
is U-Net [22], and the other is a modiﬁed U-Net, called
a-trous Residual U-Net which uses the ASSP module pro-
posed in DeepLab v3+ [4] and the skip-connection as done
in ResNet [10].

Table 1 shows the conﬁgurations of the two architec-
tures. The network consists of 5 encoder and 4 decoder
stages indexed by c1, . . . , c5 and d1, . . . , d4 respectively.

• For U-Net, the double conv operator that contains two
convolution layers is applied and denoted as {·}. The
{·}∗ operator of di stage upscales the output feature
map of its last stage and then concatenate it with the
feature map of ci stage together before applying the
double conv operator.

• For the a-trous Residual U-Net, we replace the dou-
ble conv operator to the Residual block, denoted as [·].
Different from the ResNet, we use the plain convolu-
tion layer with 3 × 3 kernel size and stride 1. Similar
to {·}∗, the operator [·]∗ also takes the input from two
sources and upscales the feature of ﬁrst input source.
The ﬁrst layer of [·]∗ contains two parallel convolution
operators to reduce the depth of feature maps and then
concatenate them together for the subsequent calcula-
tions. In the stage d4, we apply the 4 ASPP operators

1599

Table 1. Network architectures we investigated for the attraction
ﬁeld learning. {} and [] represent the double conv in U-Net and
the residual block. Inside the brackets are the shape of convolution
kernels. The sufﬁx ∗ represent the bilinear upsampling operator
with the scaling factor 2. The number outside the brackets is the
number of stacked blocks on a stage.

stage

U-Net

a-trous Residual U-Net

(cid:26) 3 × 3, 64

3 × 3, 64 (cid:27)

3 × 3, 64, stride 1

2 × 2 max pool, stride 2

3 × 3 max pool, stride 2

c1

c2

c3

c4

c5

d4

d3

d2

d1

output

(cid:26) 3 × 3, 128

3 × 3, 128 (cid:27)

2 × 2 max pool, stride 2

(cid:26) 3 × 3, 256

3 × 3, 256 (cid:27)

2 × 2 max pool, stride 2

(cid:26) 3 × 3, 512

3 × 3, 512 (cid:27)

2 × 2 max pool, stride 2

(cid:26) 3 × 3, 512

3 × 3, 512 (cid:27)

(cid:26) 3 × 3, 256

3 × 3, 256 (cid:27) ∗

(cid:26) 3 × 3, 128

3 × 3, 128 (cid:27) ∗

(cid:26) 3 × 3, 64

3 × 3, 64 (cid:27) ∗










1 × 1, 64
3 × 3, 64
1 × 1, 256
1 × 1, 128
3 × 3, 128
1 × 1, 512
1 × 1, 256
3 × 3, 256
1 × 1, 1024
1 × 1, 512
3 × 3, 512
1 × 1, 2048
ASPP










× 3

× 4

× 6

× 3

1 × 1, 256; 1 × 1, 256

3 × 3, 512
1 × 1, 512

1 × 1, 128; 1 × 1, 128

3 × 3, 256
1 × 1, 256

1 × 1, 64; 1 × 1, 64

3 × 3, 128
1 × 1, 128

1 × 1, 32; 1 × 1, 32










∗

∗










(cid:26) 3 × 3, 64

3 × 3, 64 (cid:27) ∗

3 × 3, 64
1 × 1, 64
1 × 1, stride 1, w.o. BN and ReLU

with the output channel size 256 and the dilation rate
1, 6, 12, 18 and then concatenate their outputs. The
output stage use the convolution operator with 1 × 1
kernel size and stride 1 without batch normalization
[13] and ReLU [21] for the attraction ﬁeld map pre-
diction.

4.4. Training

We follow standard deep learning protocol to estimate

the parameters Θ.

Loss function. We adopt the l1 loss function in training.

ℓ(ˆa, a) = X

ka(x, y) − ˆa(x, y)k1.

(13)

(x,y)∈Λ

Implementation details. We train the two networks (U-
Net and a-trous Residual U-Net) from scratch on the train-
ing set of Wireframe dataset [12]. Similar to [12], we fol-
low the standard data augmentation strategy to enrich the
training samples with image domain operations including
mirroring and ﬂipping upside-down. The stochastic gradi-
ent descent (SGD) optimizer with momentum 0.9 and initial
learning rates 0.01 is applied for network optimization. We

train these networks with 200 epochs and the learning rate
is decayed with the factor of 0.1 after every 50 epochs. In
training phase, we resize the images to 320 × 320 and then
generate the offset maps from resized line segment annota-
tions to form the mini batches. As discussed in Section 3,
the rescaling step with reasonable factor will not affect the
results. The mini-batch sizes for the two networks are 16
and 4 respectively due to the GPU memory.

In testing, a test image is also resized to 320 × 320 as
input to the network. Then, we use the squeeze module
to convert the attraction ﬁeld map to line segments. Since
the line segments are insensitive to scales, we can directly
resize them to original image size without loss of accuracy.
The squeeze module is implemented with C++ on CPU.

5. Experiments

In this section, we evaluate the proposed line segment
detector and make the comparison with existing state-of-
the-art line segment detectors [12, 5, 1, 23]. As shown
below, our proposed line segment detector outperforms
these existing methods on the WireFrame dataset [12] and
YorkUrban dataset [6].

∗

∗

5.1. Datasets and Evaluation Metrics

We follow the evaluation protocol from the deep wire-
frame parser [12] to make a comparison. Since we train on
the Wreframe dataset [12], it is necessary to evaluate our
proposed method on its testing dataset, which includes 462
images for man-made environments (especially for indoor
scenes). To validate the generalization ability, we also eval-
uate our proposed approach on the YorkUrban Line Seg-
ment Dataset [6].

All methods are evaluated quantitatively by the precision
and recall as described in [12, 20]. The precision rate indi-
cates the proportion of positive detection among all of the
detected line segments whereas recall reﬂects the fraction
of detected line segments among all in the scene. The de-
tected and ground-truth line segments are digitized to image
domain and we deﬁne the “positive detection” pixel-wised.
The line segment pixels within 0.01 of the image diagonal
is regarded as positive. After getting the precision (P) and
recall (R), we compare the performance of algorithms with
F-measure F = 2 · P ·R
P +R .

5.2. Comparisons for Line Segment Detection

We compare our proposed method with Deep Wireframe
Parser3 [12], Linelet4 [5], the Markov Chain Marginal Line
Segment Detector5(MCMLSD) [1] and the Line Segment

3https://github.com/huangkuns/wireframe
4https://github.com/NamgyuCho/

Linelet-code-and-YorkUrban-LineSegment-DB
5http://www.elderlab.yorku.ca/resources/

1600

i

i

n
o
s
c
e
r
P

1

0.8

0.6

0.4

0.2

0

0

[F=.773] Ours (a-trous)
[F=.752] Ours (U-Net)
[F=.728] Wireframe
[F=.647] LSD
[F=.644] Linelet
[F=.566] MCMLSD

0.2

0.4

0.6

0.8

1

Recall

i

i

n
o
s
c
e
r
P

1

0.8

0.6

0.4

0.2

0

0

[F=.646] Ours (a-trous)
[F=.639] Ours (U-Net)
[F=.627] Wireframe
[F=.591] LSD
[F=.585] Linelet
[F=.564] MCMLSD

0.2

0.4

0.6

0.8

1

Recall

Figure 4. The PR curves of different line segment detection meth-
ods on the WireFrame [12] and YorkUrban [6] datasets.

Table 2. F-measure evaluation with state-of-the-art approaches on
the WireFrame dataset and York Urban dataset. The last column
reports the averaged speed of different methods in frames per sec-
ond (FPS) on the WireFrame dataset.

Methods

LSD [23]
MCMLSD [1]
Linelet [5]
Wireframe parser [12]
Ours (U-Net)
Ours (a-trous)

Wireframe

York Urban

dataset
0.647
0.566
0.644
0.728
0.752
0.773

dataset
0.591
0.564
0.585
0.627
0.639
0.646

FPS

19.6
0.2
0.14
2.24
10.3
6.6

Detector (LSD)6 [23]. The source codes of compared meth-
ods are obtained from the authors provide links. It is no-
ticeable that the authors of Deep Wireframe Parser do not
provide the pre-trained model for line segment detection,
we reproduced their result by ourselves.

Threshold Conﬁguration In our proposed method, we
ﬁnally use the aspect ratio to ﬁlter out false detections.
Here, we vary the threshold of the aspect ratio in the
range (0, 1] with the step size ∆τ = 0.1. For com-
parison, the LSD is implemented with the − log(NFA) in
0.01 × {1.750, . . . , 1.7519} where NFA is the number of
false alarm. Besides, Linelet [5] use the same thresholds as
the LSD to ﬁlter out false detection. For the MCMLSD [1],
we use the top K detected line segments for comparison.
Due to the architecture of Deep Wireframe Parser [12], both
the threshold for the junction localization conﬁdence and
the orientation conﬁdence of junctions branches are ﬁxed to
0.5. Then, we use the author recommended threshold array
[2, 6, 10, 20, 30, 50, 80, 100, 150, 200, 250, 255] to binarize
the line heat map and detect line segments.

Precision & Recall To compare our method with state-of-
the-arts [12, 5, 1, 23], we evaluate the proposed method on
the Wireframe dataset [12] and YorkUrban dataset [6]. The
precision-recall curves and F-measure are reported in Fig-
ure 4 and Table 2. Without bells and whistles, our proposed
method outperforms all of these approaches on Wireframe
and YorkUrban datasets by a signiﬁcant margin even with a
18-layer network. Deeper network architecture with ASPP

6http://www.ipol.im/pub/art/2012/gjmr-lsd/

module further improves the F-measure performance. Due
to the YorkUrban dataset aiming at Manhattan frame esti-
mation, some line segments in the images are not labeled,
which causes the F-measure performance of all methods on
this dataset decreased.

Speed We evaluate the computational time consuming for
the abovementioned approaches on the Wireframe dataset.
We run 462 frames with image reading and result writing
steps and count the averaged time consuming because the
size of testing images are not equal. As reported in Table
2, our proposed method can detect line segments fast (out-
performs all methods except for the LSD) while getting the
best performances. All experiments perform on a PC work-
station, which is equipped with an Intel Xeon E5-2620 2.10
GHz CPU and 4 NVIDIA Titan X GPU devices. Only one
GPU is used and the CPU programs are executed in a single
thread.

Beneﬁting from the simplicity of original U-Net, our
method can detect line segments fast. The deep wireframe
parser [12] spends much time for junction and line map fu-
sion. On the other hand, beneﬁting from our novel formula-
tion, we can resize the input images into 320 × 320 and then
transform the output line segments to the original scales,
which can further reduce the computational cost.

Visualization Further, we visualize the detected line seg-
ments with different methods on Wireframe and YorkUrban
datasets (see Figure 5). The threshold conﬁgurations for vi-
sualization are as follow:

1. The a-contrario validation of LSD and Linelet are set

as − log ǫ = 0.01 · 1.758;

2. The top 90 detected line segments for the MCMLSD

are visualized;

3. The threshold for line heat map is 10 for the deep wire-

frame parser;

4. The upper bound of aspect ratio is set as 0.2 for our

results.

By observing these ﬁgures, it is easy to ﬁnd that Deep
Wireframe Parser [12] can detect more complete line seg-
ments compared with the previous methods, however, our
proposed approach can get better result in the perspective
of completeness. On the other hand, this junction driven
approach indeed induces some uncertainty for the detec-
tion. The orientation of line segments estimated by junc-
tion branches is not accurate, which will affect the orienta-
tion of line segments. Meanwhile, some junctions are mis-
connected to get false detections. In contrast, our proposed
method gets rid of junction detection and directly detect the
line segments from images.

Comparing with the rest of approaches [23, 1, 5], the
deep learning based methods (including ours) can utilize

1601

LSD

MCMLSD

Linelet

Wireframe

Ours

GT

Figure 5. Some Results of line segment detection on Wireframe [12] and YorkUrban [6] datasets with different approaches LSD [23],
MCMLSD [1], Linelet [5], Deep Wireframe Parser [12] and ours with the a-trous Residual U-Net are shown from left to right. The ground
truths are listed in last column as reference.

the global information to get complete results in the low-
contrast regions while suppressing the false detections in
the edge-like texture regions. Due to the limitation of lo-
cal features, the approaches [23, 1, 5] cannot handle the re-
sults with global information and still get some false detec-
tions even with powerful validation approaches. Although
the overall F-measure of LSD is slightly better than Linelet,
the visualization results of Linelet are cleaner.

6. Conclusion

In this paper, we proposed a method of building the dual-
ity between the region-partition based attraction ﬁeld repre-
sentation and the line segment representation. We then pose
the problem of line segment detection (LSD) as the region
coloring problem which is addressed by learning convolu-

tional neural networks. The proposed attraction ﬁeld rep-
resentation rigorously addresses several challenges in LSD
such as local ambiguity and class imbalance. The region
coloring formulation of LSD harnesses the best practices
developed in ConvNets based semantic segmentation meth-
ods such as the encoder-decoder architecture and the a-trous
convolution. In experiments, our method is tested on two
widely used LSD benchmarks, the WireFrame dataset [12]
and the YorkUrban dataset [6], with state-of-the-art perfor-
mance obtained and 6.6 ∼ 10.4 FPS speed.

Ackowledgement

This work is supported by NSFC-projects under con-
tracts No.61771350 and 61842102. Nan Xue is supported
by China Scholarship Council. Tianfu Wu is supported by
ARO grants W911NF1810295 and W911NF1810209.

1602

References

[1] Emilio J Almazan, Ron Tal, Yiming Qian, and James H El-
der. MCMLSD: A Dynamic Programming Approach to Line
Segment Detection. In Proc. of CVPR, 2017. 2, 3, 6, 7, 8

[2] Dana H Ballard. Generalizing the Hough transform to detect
arbitrary shapes. Pattern Recognition, 13(2):111–122, 1981.
2, 3

[3] J. Brian Burns, Allen R. Hanson, and Edward M. Riseman.
Extracting straight lines. IEEE TPAMI, 8(4):425–455, 1986.
3

[4] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-Decoder with Atrous
Separable Convolution for Semantic Image Segmentation. In
Proc. of ECCV, 2018. 5

[5] Nam Gyu Cho, Alan Yuille, and Seong Whan Lee. A Novel
Linelet-Based Representation for Line Segment Detection.
IEEE TPAMI, 40(5):1195–1208, 2018. 1, 2, 3, 6, 7, 8

[6] Patrick Denis, James H. Elder, and Francisco J. Estrada.
Efﬁcient Edge-Based Methods for Estimating Manhattan
Frames in Urban Imagery.
In Proc. of ECCV, pages 197–
210, 2008. 1, 2, 6, 7, 8

[7] Liuyun Duan and Florent Lafarge. Image Partitioning Into
In Proc. of CVPR, pages 3119–3127,

Convex Polygons.
2015. 1

[8] Olivier D Faugeras, Rachid Deriche, Herv´e Mathieu,
Nicholas Ayache, and Gregory Randall. The Depth and
Motion Analysis Machine.
International Journal of Pat-
tern Recognition and Artiﬁcial Intelligence, 6(2&3):353–
385, 1992. 1

[9] Yasutaka Furukawa and Yoshihisa Shinagawa. Accurate
and robust line segment extraction by analyzing distribution
around peaks in hough space. CVIU, 92(1):1–25, 2003. 3

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Sun Jian.
Deep Residual Learning for Image Recognition. In Proc. of
CVPR, 2016. 5

[11] Qibin Hou, Jiangjiang Liu, Ming-Ming Cheng, Ali Borji, and
Philip H S Torr. Three Birds One Stone: A Uniﬁed Frame-
work for Salient Object Segmentation, Edge Detection and
Skeleton Extraction. In Proc. of ECCV, 2018. 2, 3

[12] Kun Huang, Yifan Wang, Zihan Zhou, Tianjiao Ding,
Shenghua Gao, and Yi Ma. Learning to Parse Wireframes
in Images of Man-Made Environments. In Proc. of CVPR,
2018. 1, 2, 3, 5, 6, 7, 8

[13] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, 2015. 6

[14] Varsha Kamat-Sadekar and Subramaniam Ganesan. Com-
plete description of multiple line segments using the hough
transform. Image Vision Comput., 16(9-10):597–613, 1998.
3

[15] Iasonas Kokkinos. Pushing the Boundaries of Boundary De-

tection Using Deep Learning. In Proc. of ICLR, 2016. 3

[16] Iasonas Kokkinos. UberNet: Training a ‘Universal’ Con-
volutional Neural Network for Low-, Mid-, and High-Level
Vision using Diverse Datasets and Limited Memory. In Proc.
of CVPR, 2017. 3

[17] Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, and
Xiang Bai. Richer Convolutional Features for Edge Detec-
tion. In Proc. of CVPR, 2017. 3

[18] Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbel´aez,
and Luc Van Gool. Convolutional Oriented Boundaries. In
Proc. of ECCV, 2016. 1, 3

[19] Kevis Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbel´aez,
and Luc Van Gool. Convolutional Oriented Boundaries:
From Image Segmentation to High-Level Tasks.
IEEE
TPAMI, 40(4):819–833, 2018. 3

[20] David R. Martin, Charless C. Fowlkes, and Jitendra Ma-
lik. Learning to detect natural image boundaries using local
brightness, color, and texture cues. IEEE TPAMI, 26(5):530–
549, 2004. 3, 6

[21] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units

improve restricted boltzmann machines. In ICML, 2010. 6

[22] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
Net: Convolutional Networks for Biomedical Image Seg-
mentation. In Proc. of MICCA, 2015. 2, 5

[23] R G von Gioi, J Jakubowicz, J M Morel, and G Randall.
LSD: A Fast Line Segment Detector with a False Detection
Control. IEEE TPAMI, 32(4):722–732, 2010. 1, 2, 3, 4, 6, 7,
8

[24] Gui-Song Xia, Julie Delon, and Yann Gousseau. Accurate
Junction Detection and Characterization in Natural Images.
IJCV, 106(1):31–56, 2014. 1

[25] Tianzhu Xiang, Gui-Song Xia, Xiang Bai, and Liangpei
Image stitching by line-guided local warping with
Zhang.
global similarity constraint. Pattern Recognition, 83:481–
497, 2018. 1

[26] Saining Xie and Zhuowen Tu. Holistically-Nested Edge De-

tection. In Proc. of ICCV, 2015. 1, 3

[27] Chi Xu, Lilian Zhang, Li Cheng, and Reinhard Koch. Pose
Estimation from Line Correspondences: A Complete Anal-
ysis and a Series of Solutions.
IEEE TPAMI, 39(6):1209–
1222, 2017. 1

[28] Zezhong Xu, Bok-Suk Shin, and Reinhard Klette. Accurate
and robust line segment extraction using minimum entropy
with hough transform. IEEE TIP, 24(3):813–822, 2015. 3

[29] Zezhong Xu, Bok-Suk Shin, and Reinhard Klette. Closed
form line-segment extraction using the hough transform. Pat-
tern Recognition, 48(12):4012–4023, 2015. 3

[30] Zezhong Xu, Bok-Suk Shin, and Reinhard Klette. A statis-
tical method for line segment detection. CVIU, 138:61–73,
2015. 3

[31] Nan Xue, Gui-Song Xia, Xiang Bai, Liangpei Zhang, and
Weiming Shen. Anisotropic-Scale Junction Detection and
Matching for Indoor Images. IEEE TIP, 27(1):78 – 91, 2018.
1

[32] Zhan Yu, Xinqing Gao, Haibing Lin, Andrew Lumsdaine,
and Jingyi Yu. Line Assisted Light Field Triangulation and
Stereo Matching. In Proc. of ICCV, 2013. 1

[33] Yibiao Zhao and Song-Chun Zhu. Scene Parsing by Integrat-
ing Function, Geometry and Appearance Models. In Proc. of
CVPR, pages 3119–3126, 2013. 1

[34] Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem.
LayoutNet: Reconstructing the 3D Room Layout from a Sin-
gle RGB Image. CoRR, abs/1803.08999, 2018. 1

1603

