DVC: An End-to-end Deep Video Compression Framework

Guo Lu1, Wanli Ouyang2, Dong Xu3, Xiaoyun Zhang1, Chunlei Cai1, and Zhiyong Gao ∗1

1Shanghai Jiao Tong University, {luguo2014, xiaoyun.zhang, caichunlei, zhiyong.gao}@sjtu.edu.cn

2The University of Sydney, SenseTime Computer Vision Research Group, Australia

3The University of Sydney, {wanli.ouyang, dong.xu}@sydney.edu.au

Abstract

Conventional video compression approaches use the pre-
dictive coding architecture and encode the corresponding
motion information and residual information. In this paper,
taking advantage of both classical architecture in the con-
ventional video compression method and the powerful non-
linear representation ability of neural networks, we pro-
pose the ﬁrst end-to-end video compression deep model that
jointly optimizes all the components for video compression.
Speciﬁcally, learning based optical ﬂow estimation is uti-
lized to obtain the motion information and reconstruct the
current frames. Then we employ two auto-encoder style
neural networks to compress the corresponding motion and
residual information. All the modules are jointly learned
through a single loss function, in which they collaborate
with each other by considering the trade-off between reduc-
ing the number of compression bits and improving quality
of the decoded video. Experimental results show that the
proposed approach can outperform the widely used video
coding standard H.264 in terms of PSNR and be even on
par with the latest standard H.265 in terms of MS-SSIM.
Code is released at https://github.com/GuoLusjtu/DVC.

1. Introduction

Nowadays, video content contributes to more than 80%
internet trafﬁc [26], and the percentage is expected to in-
crease even further. Therefore, it is critical to build an efﬁ-
cient video compression system and generate higher quality
frames at given bandwidth budget. In addition, most video
related computer vision tasks such as video object detection
or video object tracking are sensitive to the quality of com-
pressed videos, and efﬁcient video compression may bring
beneﬁts for other computer vision tasks. Meanwhile, the
techniques in video compression are also helpful for action
recognition [41] and model compression [16].

However, in the past decades, video compression algo-
rithms [39, 31] rely on hand-crafted modules, e.g., block
based motion estimation and Discrete Cosine Transform
(DCT), to reduce the redundancies in the video sequences.

∗Corresponding author

(a) Original frame (Bpp/MS-SSIM) (b) H.264 (0.0540Bpp/0.945)

(c) H.265 (0.082Bpp/0.960)

(d) Ours ( 0.0529Bpp/ 0.961)

Figure 1: Visual quality of the reconstructed frames from
different video compression systems.
(a) is the original
frame.
(b)-(d) are the reconstructed frames from H.264,
H.265 and our method. Our proposed method only con-
sumes 0.0529pp while achieving the best perceptual qual-
ity (0.961) when measured by MS-SSIM. (Best viewed in
color.)

Although each module is well designed, the whole com-
pression system is not end-to-end optimized.
It is desir-
able to further improve video compression performance by
jointly optimizing the whole compression system.

Recently, deep neural network (DNN) based auto-
encoder for image compression [34, 11, 35, 8, 12, 19, 33,
21, 28, 9] has achieved comparable or even better perfor-
mance than the traditional image codecs like JPEG [37],
JPEG2000 [29] or BPG [1]. One possible explanation is
that the DNN based image compression methods can ex-
ploit large scale end-to-end training and highly non-linear
transform, which are not used in the traditional approaches.

However, it is non-trivial to directly apply these tech-
niques to build an end-to-end learning system for video
compression. First, it remains an open problem to learn
how to generate and compress the motion information tai-
lored for video compression. Video compression methods
heavily rely on motion information to reduce temporal re-
dundancy in video sequences. A straightforward solution
is to use the learning based optical ﬂow to represent mo-
tion information. However, current learning based optical
ﬂow approaches aim at generating ﬂow ﬁelds as accurate
as possible. But, the precise optical ﬂow is often not opti-

11006

mal for a particular video task [42]. In addition, the data
volume of optical ﬂow increases signiﬁcantly when com-
pared with motion information in the traditional compres-
sion systems and directly applying the existing compression
approaches in [39, 31] to compress optical ﬂow values will
signiﬁcantly increase the number of bits required for stor-
ing motion information. Second, it is unclear how to build
a DNN based video compression system by minimizing the
rate-distortion based objective for both residual and motion
information. Rate-distortion optimization (RDO) aims at
achieving higher quality of reconstructed frame (i.e., less
distortion) when the number of bits (or bit rate) for com-
pression is given. RDO is important for video compression
performance. In order to exploit the power of end-to-end
training for learning based compression system, the RDO
strategy is required to optimize the whole system.

In this paper, we propose the ﬁrst end-to-end deep video
compression (DVC) model that jointly learns motion es-
timation, motion compression, and residual compression.
The advantages of the this network can be summarized as
follows:

• All key components in video compression, i.e., motion
estimation, motion compensation, residual compres-
sion, motion compression, quantization, and bit rate
estimation, are implemented with an end-to-end neu-
ral networks.

• The key components in video compression are jointly
optimized based on rate-distortion trade-off through a
single loss function, which leads to higher compres-
sion efﬁciency.

• There is one-to-one mapping between the components
of conventional video compression approaches and our
proposed DVC model. This work serves as the bridge
for researchers working on video compression, com-
puter vision, and deep model design. For example, bet-
ter model for optical ﬂow estimation and image com-
pression can be easily plugged into our framework.
Researchers working on these ﬁelds can use our DVC
model as a starting point for their future research.

Experimental results show that estimating and compress-
ing motion information by using our neural network based
approach can signiﬁcantly improve the compression perfor-
mance. Our framework outperforms the widely used video
codec H.264 when measured by PSNR and be on par with
the latest video codec H.265 when measured by the multi-
scale structural similarity index (MS-SSIM) [38].

2. Related Work

2.1. Image Compression

A lot of image compression algorithms have been pro-
posed in the past decades [37, 29, 1]. These methods heav-

ily rely on handcrafted techniques. For example, the JPEG
standard linearly maps the pixels to another representation
by using DCT, and quantizes the corresponding coefﬁcients
before entropy coding[37]. One disadvantage is that these
modules are separately optimized and may not achieve op-
timal compression performance.

Recently, DNN based image compression methods have
attracted more and more attention [34, 35, 11, 12, 33, 8,
21, 28, 24, 9]. In [34, 35, 19], recurrent neural networks
(RNNs) are utilized to build a progressive image compres-
sion scheme. Other methods employed the CNNs to de-
sign an auto-encoder style network for image compres-
sion [11, 12, 33]. To optimize the neural network, the
work in [34, 35, 19] only tried to minimize the distortion
(e.g., mean square error) between original frames and re-
constructed frames without considering the number of bits
used for compression. Rate-distortion optimization tech-
nique was adopted in [11, 12, 33, 21] for higher compres-
sion efﬁciency by introducing the number of bits in the opti-
mization procedure. To estimate the bit rates, context mod-
els are learned for the adaptive arithmetic coding method in
[28, 21, 24], while non-adaptive arithmetic coding is used
in [11, 33]. In addition, other techniques such as general-
ized divisive normalization (GDN) [11], multi-scale image
decomposition [28], adversarial training [28], importance
map [21, 24] and intra prediction [25, 10] are proposed to
improve the image compression performance. These exist-
ing works are important building blocks for our video com-
pression network.

2.2. Video Compression

In the past decades, several traditional video compres-
sion algorithms have been proposed, such as H.264 [39] and
H.265 [31]. Most of these algorithms follow the predictive
coding architecture. Although they provide highly efﬁcient
compression performance, they are manually designed and
cannot be jointly optimized in an end-to-end way.

For the video compression task, a lot of DNN based
methods have been proposed for intra prediction and resid-
ual coding[13], mode decision [22], entropy coding [30],
post-processing [23]. These methods are used to improve
the performance of one particular module of the traditional
video compression algorithms instead of building an end-
In [14], Chen et al. pro-
to-end compression scheme.
posed a block based learning approach for video compres-
sion. However, it will inevitably generate blockness artifact
in the boundary between blocks. In addition, they used the
motion information propagated from previous reconstructed
frames through traditional block based motion estimation,
which will degrade compression performance. Tsai et al.
proposed an auto-encoder network to compress the residual
from the H.264 encoder for speciﬁc domain videos [36].
This work does not use deep model for motion estimation,

11007

motion compensation or motion compression.

3.1. Brief Introduction of Video Compression

The most related work is the RNN based approach in
[40], where video compression is formulated as frame in-
terpolation. However, the motion information in their ap-
proach is also generated by traditional block based mo-
tion estimation, which is encoded by the existing non-deep
learning based image compression method [5].
In other
words, estimation and compression of motion are not ac-
complished by deep model and jointly optimized with other
components. In addition, the video codec in [40] only aims
at minimizing the distortion (i.e., mean square error) be-
tween the original frame and reconstructed frame without
considering rate-distortion trade-off in the training proce-
dure.
In comparison, in our network, motion estimation
and compression are achieved by DNN, which is jointly
optimized with other components by considering the rate-
distortion trade-off of the whole compression system.

2.3. Motion Estimation

Motion estimation is a key component in the video com-
pression system. Traditional video codecs use the block
based motion estimation algorithm [39], which well sup-
ports hardware implementation.

In the computer vision tasks, optical ﬂow is widely used
to exploit temporal relationship. Recently, a lot of learning
based optical ﬂow estimation methods [15, 27, 32, 17, 18]
have been proposed. These approaches motivate us to inte-
grate optical ﬂow estimation into our end-to-end learning
framework. Compared with block based motion estima-
tion method in the existing video compression approaches,
learning based optical ﬂow methods can provide accurate
motion information at pixel-level, which can be also op-
timized in an end-to-end manner. However, much more
bits are required to compress motion information if optical
ﬂow values are encoded by traditional video compression
approaches.

3. Proposed Method

V

Let

Introduction

of Notations.

=
{x1, x2, ..., xt−1, xt, ...} denote the current video se-
quences, where xt is the frame at time step t. The predicted
frame is denoted as ¯xt and the reconstructed/decoded
frame is denoted as ˆxt. rt represents the residual (error)
between the original frame xt and the predicted frame
¯xt. ˆrt represents the reconstructed/decoded residual.
In
order to reduce temporal redundancy, motion information
is required. Among them, vt represents the motion vector
or optical ﬂow value. And ˆvt is its corresponding recon-
structed version. Linear or nonlinear transform can be
used to improve the compression efﬁciency. Therefore,
residual information rt is transformed to yt, and motion
information vt can be transformed to mt, ˆrt and ˆmt are the
corresponding quantized versions, respectively.

In this section, we give a brief introduction of video com-
pression. More details are provided in [39, 31]. Gener-
ally, the video compression encoder generates the bitstream
based on the input current frames. And the decoder recon-
structs the video frames based on the received bitstreams.
In Fig. 2, all the modules are included in the encoder side
while blue color modules are not included in the decoder
side.

The classic framework of video compression in Fig. 2(a)
follows the predict-transform architecture. Speciﬁcally, the
input frame xt is split into a set of blocks, i.e., square re-
gions, of the same size (e.g., 8 × 8). The encoding proce-
dure of the traditional video compression algorithm in the
encoder side is shown as follows,

Step 1. Motion estimation. Estimate the motion be-
tween the current frame xt and the previous reconstructed
frame ˆxt−1. The corresponding motion vector vt for each
block is obtained.

Step 2. Motion compensation. The predicted frame ¯xt
is obtained by copying the corresponding pixels in the pre-
vious reconstructed frame to the current frame based on the
motion vector vt deﬁned in Step 1. The residual rt between
the original frame xt and the predicted frame ¯xt is obtained
as rt = xt − ¯xt.

Step 3. Transform and quantization. The residual rt
from Step 2 is quantized to ˆyt. A linear transform (e.g.,
DCT) is used before quantization for better compression
performance.

Step 4. Inverse transform. The quantized result ˆyt in
Step 3 is used by inverse transform for obtaining the recon-
structed residual ˆrt.

Step 5. Entropy coding. Both the motion vector vt in
Step 1 and the quantized result ˆyt in Step 3 are encoded into
bits by the entropy coding method and sent to the decoder.
Step 6. Frame reconstruction. The reconstructed
frame ˆxt is obtained by adding ¯xt in Step 2 and ˆrt in Step
4, i.e. ˆxt = ˆrt + ¯xt. The reconstructed frame will be used
by the (t + 1)th frame at Step 1 for motion estimation.

For the decoder, based on the bits provided by the en-
coder at Step 5, motion compensation at Step 2, inverse
quantization at Step 4, and then frame reconstruction at Step
6 are performed to obtain the reconstructed frame ˆxt.

3.2. Overview of the Proposed Method

Fig. 2 (b) provides a high-level overview of our end-
to-end video compression framework. There is one-to-one
correspondence between the traditional video compression
framework and our proposed deep learning based frame-
work. The relationship and brief summarization on the dif-
ferences are introduced as follows:

Step N1. Motion estimation and compression. We use
a CNN model to estimate the optical ﬂow [27], which is

11008

*#(46&6-%(/ C64$- !-)8#$++6-% ’#()$B-#1

7%4D&-D$%4 C64$- !-)8#$++6-% ’#()$B-#1

*#(%+,-#)

>

&"

!

!"
!"##$%&
’#()$

#!"

5-&6-%2

!-)8$%+(&6-%

9%:$#+$

*#(%+,-#)
’&"

%!"

%$"

7%&#-8;2
!-46%<

*"

*"

./-0123(+$4

5-&6-%27+&6)(&6-%

%!"()

?$+64"(/

7%0-4$# @$&

>

$"

&"

!

!"
!"##$%&
’#()$

%$"
?$+64"(/

=$0-4$# @$&

’&"

%$"

#!"

5-&6-%

!-)8$%+(&6-% @$&

%*"

5C =$0-4$# @$&

+,"
>
,"

5C 7%0-4$# @$&

*"

A8&60(/ ’/-B @$&

%!"()

%!"

.6& ?(&$

7+&6)(&6-% @$&

+,"

G-++

=$0-4$42’#()$+ .",,$#

=$0-4$42’#()$+ .",,$#

E(F

E3F

Figure 2: (a): The predictive coding architecture used by the traditional video codec H.264 [39] or H.265 [31]. (b): The
proposed end-to-end video compression network. The modules with blue color are not included in the decoder side.

considered as motion information vt. Instead of directly en-
coding the raw optical ﬂow values, an MV encoder-decoder
network is proposed in Fig. 2 to compress and decode the
optical ﬂow values, in which the quantized motion repre-
sentation is denoted as ˆmt. Then the corresponding recon-
structed motion information ˆvt can be decoded by using the
MV decoder net. Details are given in Section 3.3.

Step N2. Motion compensation. A motion compensa-
tion network is designed to obtain the predicted frame ¯xt
based on the optical ﬂow obtained in Step N1. More infor-
mation is provided in Section 3.4.

Step N3-N4. Transform, quantization and inverse
transform. We replace the linear transform in Step 3 by us-
ing a highly non-linear residual encoder-decoder network,
and the residual rt is non-linearly maped to the representa-
tion yt. Then yt is quantized to ˆyt. In order to build an end-
to-end training scheme, we use the quantization method in
[11]. The quantized representation ˆyt is fed into the resid-
ual decoder network to obtain the reconstructed residual ˆrt.
Details are presented in Section 3.5 and 3.6.

Step N5. Entropy coding. At the testing stage, the
quantized motion representation ˆmt from Step N1 and the
residual representation ˆyt from Step N3 are coded into bits
and sent to the decoder. At the training stage, to estimate
the number of bits cost in our proposed approach, we use
the CNNs (Bit rate estimation net in Fig. 2) to obtain the
probability distribution of each symbol in ˆmt and ˆyt. More
information is provided in Section 3.6.

Step N6. Frame reconstruction. It is the same as Step

6 in Section 3.1.

3.3. MV Encoder and Decoder Network

In order to compress motion information at Step N1, we
design a CNN to transform the optical ﬂow to the corre-

!"

#!"

(

,
*
+
*
)
’
&
%
$
#
"

(

(

(

,
*
*
’
&
%
$
#
1
0
.

/
.
-

/
.
-

2

!"#$%&’()*

(

,
*
+
*
)
’
&
%
$
#
"

(

(

(

,
*
+
*
)
’
&
%
$
#
1
0
.

/
.
-

/
.
-

2

(

,
*
+
*
)
’
&
%
$
#
"

(

(

(

,
*
+
*
)
’
&
%
$
#
1
0
.

/
.
-

/
.
-

2

(

,
*
+
*
)
’
&
%
$
#
"

(

(

(

,
*
+
*
)
’
&
%
$
#
1
0
.

$"

!

%$"

!"#+)&’()*

3:

Our MV Encoder-decoder
represents

network.
Figure
Conv(3,128,2)
convoluation operation
with the kernel size of 3x3, the output channel of 128
and the stride of 2. GDN/IGDN [11] is the nonlinear
transform function. The binary feature map is only used for
illustration.

the

sponding representations for better encoding. Speciﬁcally,
we utilize an auto-encoder style network to compress the
optical ﬂow, which is ﬁrst proposed by [11] for the image
compression task. The whole MV compression network is
shown in Fig. 3. The optical ﬂow vt is fed into a series of
convolution operation and nonlinear transform. The num-
ber of output channels for convolution (deconvolution) is
128 except for the last deconvolution layer, which is equal
to 2. Given optical ﬂow vt with the size of M × N × 2,
the MV encoder will generate the motion representation mt
with the size of M/16 × N/16 × 128. Then mt is quantized
to ˆmt. The MV decoder receives the quantized representa-
tion and reconstruct motion information ˆvt. In addition, the
quantized representation ˆmt will be used for entropy cod-
ing.

11009

3.4. Motion Compensation Network

Given the previous reconstructed frame ˆxt−1 and the mo-
tion vector ˆvt, the motion compensation network obtains the
predicted frame ¯xt, which is expected to as close to the cur-
rent frame xt as possible. First, the previous frame ˆxt−1
is warped to the current frame based on the motion infor-
mation ˆvt. The warped frame still has artifacts. To remove
the artifacts, we concatenate the warped frame w(ˆxt−1, ˆvt),
the reference frame ˆxt−1, and the motion vector ˆvt as the
input, then feed them into another CNN to obtain the re-
ﬁned predicted frame ¯xt. The overall architecture of the
proposed network is shown in Fig. 4. The detail of the
CNN in Fig. 4 is provided in supplementary material. Our
proposed method is a pixel-wise motion compensation ap-
proach, which can provide more accurate temporal informa-
tion and avoid the blockness artifact in the traditional block
based motion compensation method. It means that we do
not need the hand-crafted loop ﬁlter or the sample adaptive
offset technique [39, 31] for post processing.

3.5. Residual Encoder and Decoder Network

The residual information rt between the original frame
xt and the predicted frame ¯xt is encoded by the residual en-
coder network as shown in Fig. 2. In this paper, we rely
on the highly non-linear neural network in [12] to trans-
form the residuals to the corresponding latent representa-
tion. Compared with discrete cosine transform in the tra-
ditional video compression system, our approach can bet-
ter exploit the power of non-linear transform and achieve
higher compression efﬁciency.

3.6. Training Strategy

Loss Function. The goal of our video compression
framework is to minimize the number of bits used for en-
coding the video, while at the same time reduce the dis-
tortion between the original input frame xt and the recon-
structed frame ˆxt. Therefore, we propose the following
rate-distortion optimization problem,

λD + R = λd(xt, ˆxt) + (H( ˆmt) + H(ˆyt)),

(1)

where d(xt, ˆxt) denotes the distortion between xt and ˆxt,
and we use mean square error (MSE) in our implementa-
tion. H(·) represents the number of bits used for encoding
the representations. In our approach, both residual represen-
tation ˆyt and motion representation ˆmt should be encoded
into the bitstreams. λ is the Lagrange multiplier that deter-
mines the trade-off between the number of bits and distor-
tion. As shown in Fig. 2(b), the reconstructed frame ˆxt, the
original frame xt and the estimated bits are input to the loss
function.

Quantization. Latent representations such as residual
representation yt and motion representation mt are required

!"#$*+ ,#"-*

())

’$#

!$#%&

!"#$%&’

!"#

Figure 4: Our Motion Compensation Network.

to be quantized before entropy coding. However, quantiza-
tion operation is not differential, which makes end-to-end
training impossible. To address this problem, a lot of meth-
ods have been proposed [34, 8, 11]. In this paper, we use
the method in [11] and replace the quantization operation
by adding uniform noise in the training stage. Take yt as
an example, the quantized representation ˆyt in the train-
ing stage is approximated by adding uniform noise to yt,
i.e., ˆyt = yt + η, where η is uniform noise.
In the in-
ference stage, we use the rounding operation directly, i.e.,
ˆyt = round(yt).

Bit Rate Estimation.

In order to optimize the whole
network for both number of bits and distortion, we need to
obtain the bit rate (H(ˆyt) and H( ˆmt)) of the generated la-
tent representations (ˆyt and ˆmt). The correct measure for
bitrate is the entropy of the corresponding latent represen-
tation symbols. Therefore, we can estimate the probability
distributions of ˆyt and ˆmt, and then obtain the correspond-
ing entropy. In this paper, we use the CNNs in [12] to esti-
mate the distributions.

Buffering Previous Frames. As shown in Fig. 2, the
previous reconstructed frame ˆxt−1 is required in the motion
estimation and motion compensation network when com-
pressing the current frame. However, the previous recon-
structed frame ˆxt−1 is the output of our network for the
previous frame, which is based on the reconstructed frame
ˆxt−2, and so on. Therefore, the frames x1, . . . , xt−1 might
be required during the training procedure for the frame xt,
which reduces the variation of training samples in a mini-
batch and could be impossible to be stored in a GPU when t
is large. To solve this problem, we adopt an on line updating
strategy. Speciﬁcally, the reconstructed frame ˆxt in each it-
eration will be saved in a buffer. In the following iterations,
ˆxt in the buffer will be used for motion estimation and mo-
tion compensation when encoding xt+1. Therefore, each
training sample in the buffer will be updated in an epoch. In
this way, we can optimize and store one frame for a video
clip in each iteration, which is more efﬁcient.

4. Experiments

4.1. Experimental Setup

Datasets. We train the proposed video compression
framework using the Vimeo-90k dataset [42], which is re-
cently built for evaluating different video processing tasks,

11010

39

38

37

36

35

34

)

B
d
(
R
N
S
P

UVG dataset

HEVC Class B dataset

HEVC Class E dataset

)

B
d
(
R
N
S
P

36

35

34

33

32

31

Wu_ECCV2018
H.264
H.265
Proposed

H.264
H.265
Proposed

0.0

0.1

0.2

Bpp

0.3

0.4

0.1

0.2

0.3

0.4

0.5

0.6

Bpp

)

B
d
(
R
N
S
P

41

40

39

38

37

36

35

H.264
H.265
Proposed

0.05

0.10

0.15

Bpp

0.20

0.25

0.30

0.980

0.975

0.970

0.965

0.960

0.955

0.950

0.945

UVG dataset

Wu_ECCV2018
H.264
H.265
Proposed

0.980

0.975

0.970

0.965

0.960

0.955

0.950

0.945

0.940

I

M
S
S
S
M

-

HEVC Class B dataset

HEVC Class E dataset

I

M
S
S
S
M

-

0.990

0.988

0.986

0.984

0.982

0.980

0.978

0.976

H.264
H.265
Proposed

H.264
H.265
Proposed

I

M
S
S
S
M

-

0.0

0.1

0.2

Bpp

0.3

0.4

0.1

0.2

0.3

0.4

0.5

0.6

Bpp

0.05

0.10

0.15

Bpp

0.20

0.25

0.30

Figure 5: Comparsion between our proposed method with the learning based video codec in [40], H.264 [39] and H.265 [31].
Our method outperforms H.264 when measured by both PSNR ans MS-SSIM. Meanwhile, our method achieves similar or
better compression performance when compared with H.265 in terms of MS-SSIM.

such as video denoising and video super-resolution. It con-
sists of 89,800 independent clips that are different from each
other in content.

To report the performance of our proposed method, we
evaluate our proposed algorithm on the UVG dataset [4],
and the HEVC Standard Test Sequences (Class B, Class C,
Class D and Class E) [31]. The content and resolution of
these datasets are diversiﬁed and they are widely used to
measure the performance of video compression algorithms.
Evaluation Method To measure the distortion of the re-
constructed frames, we use two evaluation metrics: PSNR
and MS-SSIM [38]. MS-SSIM correlates better with hu-
man perception of distortion than PSNR. To measure the
number of bits for encoding the representations, we use bits
per pixel(Bpp) to represent the required bits for each pixel
in the current frame.

Implementation Details We train four models with dif-
ferent λ (λ = 256, 512, 1024, 2048). For each model, we
use the Adam optimizer [20] by setting the initial learning
rate as 0.0001, β1 as 0.9 and β2 as 0.999, respectively. The
learning rate is divided by 10 when the loss becomes stable.
The mini-batch size is set as 4. The resolution of training
images is 256 × 256. The motion estimation module is ini-
tialized with the pretrained weights in [27]. The whole sys-
tem is implemented based on Tensorﬂow and it takes about
7 days to train the whole network using two Titan X GPUs.

4.2. Experimental Results

also included for comparison. To generate the compressed
frames by the H.264 and H.265, we follow the setting in
[40] and use the FFmpeg with very fast mode. The GOP
sizes for the UVG dataset and HEVC dataset are 12 and
10, respectively. Please refer to supplementary material for
more details about the H.264/H.265 settings.

Fig.

5 shows the experimental results on the UVG
dataset, the HEVC Class B and Class E datasets. The re-
sults for HEVC Class C and Class D are provided in sup-
plementary material.
It is obvious that our method out-
performs the recent work on video compression [40] by a
large margin. On the UVG dataset, the proposed method
achieved about 0.6dB gain at the same Bpp level. It should
be mentioned that our method only uses one previous ref-
[40] utilizes
erence frame while the work by Wu et al.
bidirectional frame prediction and requires two neighbour-
ing frames. Therefore, it is possible to further improve the
compression performance of our framework by exploiting
temporal information from multiple reference frames.

On most of the datasets, our proposed framework out-
performs the H.264 standard when measured by PSNR and
MS-SSIM. In addition, our method achieves similar or bet-
ter compression performance when compared with H.265
in terms of MS-SSIM. As mentioned before, the distortion
term in our loss function is measured by MSE. Neverthe-
less, our method can still provide reasonable visual quality
in terms of MS-SSIM.

In this section, both H.264 [39] and H.265 [31] are in-
cluded for comparison. In addition, a learning based video
compression system in [40], denoted by Wu ECCV2018, is

4.3. Ablation Study and Model Analysis

Motion Estimation. In our proposed method, we exploit
the advantage of the end-to-end training strategy and opti-

11011

35

34

33

32

31

30

)

B
d
(
R
N
S
P

HEVC Class B dataset

Fix ME

W/O MVC

Ours

Bpp
0.044

PSNR Bpp
27.33
0.20

PSNR
24.32

Bpp
0.029

PSNR
28.17

Table 1: The bit cost for encoding optical ﬂow and the cor-
responding PSNR of warped frame from optical ﬂow for
different setting are provided.

Proposed
W/O update
W/O MC
W/O Joint Traning
W/O MVC
W/O Motion Information

(a) Frame No.5

(b) Frame No.6

0.10

0.15

0.20

Bpp

0.25

0.30

0.35

Figure 6: Ablation study. We report the compression per-
formance in the following settings. 1. The strategy of
buffering previous frame is not adopted(W/O update). 2.
Motion compensation network is removed (W/O MC). 3.
Motion estimation module is not jointly optimized (W/O
Joint Training). 4. Motion compression network is removed
(W/O MVC). 5. Without relying on motion information
(W/O Motion Information).

mize the motion estimation module within the whole net-
work. Therefore, based on rate-distortion optimization, the
optical ﬂow in our system is expected to be more compress-
ible, leading to more accurate warped frames. To demon-
strate the effectiveness, we perform a experiment by ﬁxing
the parameters of the initialized motion estimation module
in the whole training stage. In this case, the motion estima-
tion module is pretrained only for estimating optical ﬂow
more accurately, but not for optimal rate-distortion. Ex-
perimental result in Fig. 6 shows that our approach with
joint training for motion estimation can improve the per-
formance signiﬁcantly when compared with the approach
by ﬁxing motion estimation, which is the denoted by W/O
Joint Training in Fig. 6 (see the blue curve).

We report the average bits costs for encoding the optical
ﬂow and the corresponding PSNR of the warped frame in
Table 1. Speciﬁcally, when the motion estimation module is
ﬁxed during the training stage, it needs 0.044bpp to encode
the generated optical ﬂow and the corresponding PSNR of
the warped frame is 27.33db. In contrast, we need 0.029bpp
to encode the optical ﬂow in our proposed method, and the
PSNR of warped frame is higher (28.17dB). Therefore, the
joint learning strategy not only saves the number of bits re-
quired for encoding the motion, but also has better warped
image quality. These experimental results clearly show that
putting motion estimation into the rate-distortion optimiza-
tion improves compression performance.

In Fig. 7, we provide further visual comparisons. Fig. 7
(a) and (b) represent the frame 5 and frame 6 from the Ki-

(c) Reconstructed optical ﬂow when
ﬁxing ME Net.

(d) Reconstructed optical ﬂow with
the joint training strategy.

Flow Magnitude Distribution

Flow Magnitude Distribution

0.15

0.10

0.05

y
t
i
l
i

b
a
b
o
r
P

0.00

0

5
Flow Magnitude

10

15

y
t
i
l
i

b
a
b
o
r
P

0.4

0.3

0.2

0.1

0.0

0

5
Flow Magnitude

10

15

(e) Magnitude distribution of the op-
tical ﬂow map (c).

(f) Magnitude distribution of the op-
tical ﬂow map (d).

Figure 7: Flow visualize and statistic analysis.

mono sequence. Fig. 7 (c) denotes the reconstructed optical
ﬂow map when the optical ﬂow network is ﬁxed during the
training procedure. Fig. 7 (d) represents the reconstructed
optical ﬂow map after using the joint training strategy. Fig.
7 (e) and (f) are the corresponding probability distributions
of optical ﬂow magnitudes. It can be observed that the re-
constructed optical ﬂow by using our method contains more
pixels with zero ﬂow magnitude (e.g., in the area of human
body). Although zero value is not the true optical ﬂow value
in these areas, our method can still generate accurate motion
compensated results in the homogeneous region. More im-
portantly, the optical ﬂow map with more zero magnitudes
requires much less bits for encoding. For example, it needs
0.045bpp for encoding the optical ﬂow map in Fig. 7 (c)
while it only needs 0.038bpp for encoding optical ﬂow map
in Fig. 7 (d).

It should be mentioned that in the H.264 [39] or H.265
[31], a lot of motion vectors are assigned to zero for achiev-
ing better compression performance. Surprisingly, our pro-
posed framework can learn a similar motion distribution
without relying on any complex hand-crafted motion esti-
mation strategy as in [39, 31].

Motion Compensation. In this paper, the motion com-

11012

0.14

0.11

0.08

0.05

)
p
p
B
(
e
a
r
t
i

t

B

 
l

a
u
t
c
A

0.02

0.02

0.05

0.08

0.11

0.14

Estimated Bitrate(Bpp)

)

B
d
(
R
N
S
P

35

34

33

32

The percentage of motion information

(2048, 26.9%)

(1024, 34.6%)

(512, 36.0%)

(256, 38.8%)

0.1

0.2
Bpp

0.3

(a) Actual and estimated bit rate.

(b) Motion information percentages.

Figure 8: Bit rate analysis.

pensation network is utilized to reﬁne the warped frame
based on the estimated optical ﬂow. To evaluate the effec-
tiveness of this module, we perform another experiment by
removing the motion compensation network in the proposed
system. Experimental results of the alternative approach de-
noted by W/O MC (see the green curve in Fig. 6) show that
the PSNR without the motion compensation network will
drop by about 1.0 dB at the same bpp level.

Updating Strategy. As mentioned in Section 3.6, we
use an on-line buffer to store previously reconstructed
frames ˆxt−1 in the training stage when encoding the cur-
rent frame xt. We also report the compression performance
when the previous reconstructed frame ˆxt−1 is directly re-
placed by the previous original frame xt−1 in the training
stage. This result of the alternative approach denoted by
W/O update (see the red curve ) is shown in Fig. 6.
It
demonstrates that the buffering strategy can provide about
0.2dB gain at the same bpp level.

MV Encoder and Decoder Network. In our proposed
framework, we design a CNN model to compress the optical
ﬂow and encode the corresponding motion representations.
It is also feasible to directly quantize the raw optical ﬂow
values and encode it without using any CNN. We perform a
new experiment by removing the MV encoder and decoder
network. The experimental result in Fig. 6 shows that the
PSNR of the alternative approach denoted by W/O MVC
(see the magenta curve ) will drop by more than 2 dB af-
ter removing the motion compression network. In addition,
the bit cost for encoding the optical ﬂow in this setting and
the corresponding PSNR of the warped frame are also pro-
vided in Table 1 (denoted by W/O MVC). It is obvious that
it requires much more bits (0.20Bpp) to directly encode raw
optical ﬂow values and the corresponding PSNR(24.43dB)
is much worse than our proposed method(28.17dB). There-
fore, compression of motion is crucial when optical ﬂow is
used for estimating motion.

Motion Information. In Fig. 2(b), we also investigate
the setting which only retains the residual encoder and de-
coder network. Treating each frame independently without
using any motion estimation approach (see the yellow curve
denoted by W/O Motion Information) leads to more than
2dB drop in PSNR when compared with our method.

Running Time and Model Complexity. The total num-

ber of parameters of our proposed end-to-end video com-
pression framework is about 11M. In order to test the speed
of different codecs, we perform the experiments using the
computer with Intel Xeon E5-2640 v4 CPU and single Ti-
tan 1080Ti GPU. For videos with the resolution of 352x288,
the encoding (resp. decoding) speed of each iteration of Wu
et al.’s work [40] is 29fps (resp. 38fps), while the overall
speed of ours is 24.5fps (resp. 41fps). The correspond-
ing encoding speeds of H.264 and H.265 based on the ofﬁ-
cial software JM [2] and HM [3] are 2.4fps and 0.35fps, re-
spectively. The encoding speed of the commercial software
x264 [6] and x265 [7] are 250fps and 42fps, respectively.
Although the commercial codec x264 [6] and x265 [7] can
provide much faster encoding speed than ours, they need
a lot of code optimization. Correspondingly, recent deep
model compression approaches are off-the-shelf for mak-
ing the deep model much faster, which is beyond the scope
of this paper.

Bit Rate Analysis. In this paper, we use a probability es-
timation network in [12] to estimate the bit rate for encoding
motion information and residual information. To verify the
reliability, we compare the estimated bit rate and the actual
bit rate by using arithmetic coding in Fig. 8(a). It is ob-
vious that the estimated bit rate is closed to the actual bit
rate. In addition, we further investigate on the components
of bit rate. In Fig. 8(b), we provide the λ value and the per-
centage of motion information at each point. When λ in our
objective function λ∗D +R becomes larger, the whole Bpp
also becomes larger while the corresponding percentage of
motion information drops.

5. Conclusion

In this paper, we have proposed the fully end-to-end deep
learning framework for video compression. Our framework
inherits the advantages of both classic predictive coding
scheme in the traditional video compression standards and
the powerful non-linear representation ability from DNNs.
Experimental results show that our approach outperforms
the widely used H.264 video compression standard and the
recent learning based video compression system. The work
provides a promising framework for applying deep neural
network for video compression. Based on the proposed
framework, other new techniques for optical ﬂow, image
compression, bi-directional prediction and rate control can
be readily plugged into this framework.

Acknowledgement This work was supported in part
by National Natural Science Foundation of China
(61771306) Natural Science Foundation of Shang-
hai(18ZR1418100), Chinese National Key S&T Special
Program(2013ZX01033001-002-002),
Shanghai Key
Laboratory of Digital Media Processing and Transmis-
sions(STCSM 18DZ2270700).

11013

References

[1] F. bellard, bpg image format. http://bellard.org/

bpg/. Accessed: 2018-10-30. 1, 2
[2] The h.264/avc reference software.

http://iphome.

hhi.de/suehring/. Accessed: 2018-10-30. 8

[3] Hevc

test model

(hm).

https://hevc.hhi.

fraunhofer.de/HM-doc/. Accessed: 2018-10-30. 8

[4] Ultra video group test sequences. http://ultravideo.

cs.tut.fi. Accessed: 2018-10-30. 6

[5] Webp.

https://developers.google.com/

speed/webp/. Accessed: 2018-10-30. 3
encoder.

h.264/avc

best

the

[6] x264,

https:

//www.videolan.org/developers/x264.html.
Accessed: 2018-10-30. 8

[7] x265 hevc encoder / h.265 video codec. http://x265.

org. Accessed: 2018-10-30. 8

[8] E. Agustsson, F. Mentzer, M. Tschannen, L. Cavigelli,
R. Timofte, L. Benini, and L. V. Gool. Soft-to-hard vector
quantization for end-to-end learning compressible represen-
tations. In NIPS, pages 1141–1151, 2017. 1, 2, 5

[9] E. Agustsson, M. Tschannen, F. Mentzer, R. Timo-
fte, and L. Van Gool. Generative adversarial networks
for extreme learned image compression.
arXiv preprint
arXiv:1804.02958, 2018. 1, 2

[10] M. H. Baig, V. Koltun, and L. Torresani. Learning to inpaint
for image compression. In NIPS, pages 1246–1255, 2017. 2
End-
arXiv preprint

[11] J. Ball´e, V. Laparra, and E. P. Simoncelli.

to-end optimized image compression.
arXiv:1611.01704, 2016. 1, 2, 4, 5

[12] J. Ball´e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston.
Variational image compression with a scale hyperprior. arXiv
preprint arXiv:1802.01436, 2018. 1, 2, 5, 8

[13] T. Chen, H. Liu, Q. Shen, T. Yue, X. Cao, and Z. Ma. Deep-
coder: A deep neural network based video compression. In
VCIP, pages 1–4. IEEE, 2017. 2

[14] Z. Chen, T. He, X. Jin, and F. Wu. Learning for video com-

pression. arXiv preprint arXiv:1804.09869, 2018. 2

[15] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In ICCV, pages 2758–2766, 2015. 3

[16] S. Han, H. Mao, and W. J. Dally.

Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015. 1

[17] T.-W. Hui, X. Tang, and C. Change Loy. Liteﬂownet: A
lightweight convolutional neural network for optical ﬂow es-
timation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 8981–8989,
2018. 3

[18] T.-W. Hui, X. Tang, and C. C. Loy. A lightweight optical
ﬂow cnn–revisiting data ﬁdelity and regularization. arXiv
preprint arXiv:1903.07414, 2019. 3

[19] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh,
T. Chinen, S. Jin Hwang, J. Shor, and G. Toderici. Improved
lossy image compression with priming and spatially adaptive
bit rates for recurrent networks. In CVPR, June 2018. 1, 2

[20] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 6

[21] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang. Learning con-
volutional networks for content-weighted image compres-
sion. In CVPR, June 2018. 1, 2

[22] Z. Liu, X. Yu, Y. Gao, S. Chen, X. Ji, and D. Wang. Cu par-
tition mode decision for hevc hardwired intra encoder using
convolution neural network. TIP, 25(11):5088–5103, 2016.
2

[23] G. Lu, W. Ouyang, D. Xu, X. Zhang, Z. Gao, and M.-T. Sun.
Deep kalman ﬁltering network for video compression artifact
reduction. In ECCV, September 2018. 2

[24] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and
L. Van Gool. Conditional probability models for deep image
compression. In CVPR, number 2, page 3, 2018. 2

[25] D. Minnen, G. Toderici, M. Covell, T. Chinen, N. Johnston,
J. Shor, S. J. Hwang, D. Vincent, and S. Singh. Spatially
adaptive image compression using a tiled deep network. In
ICIP, pages 2796–2800. IEEE, 2017. 2

[26] C. V. networking Index. Forecast and methodology, 2016-

2021, white paper. San Jose, CA, USA, 1, 2016. 1

[27] A. Ranjan and M. J. Black. Optical ﬂow estimation using a
spatial pyramid network. In CVPR, volume 2, page 2. IEEE,
2017. 3, 6

[28] O. Rippel and L. Bourdev. Real-time adaptive image com-

pression. In ICML, 2017. 1, 2

[29] A. Skodras, C. Christopoulos, and T. Ebrahimi. The jpeg
IEEE Signal Pro-

2000 still image compression standard.
cessing Magazine, 18(5):36–58, 2001. 1, 2

[30] R. Song, D. Liu, H. Li, and F. Wu. Neural network-based
arithmetic coding of intra prediction modes in hevc. In VCIP,
pages 1–4. IEEE, 2017. 2

[31] G. J. Sullivan, J.-R. Ohm, W.-J. Han, T. Wiegand, et al.
Overview of the high efﬁciency video coding(hevc) standard.
TCSVT, 22(12):1649–1668, 2012. 1, 2, 3, 4, 5, 6, 7

[32] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. Pwc-net: Cnns
for optical ﬂow using pyramid, warping, and cost volume. In
CVPR, pages 8934–8943, 2018. 3

[33] L. Theis, W. Shi, A. Cunningham, and F. Husz´ar. Lossy
image compression with compressive autoencoders. arXiv
preprint arXiv:1703.00395, 2017. 1, 2

[34] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent,
D. Minnen, S. Baluja, M. Covell, and R. Sukthankar. Vari-
able rate image compression with recurrent neural networks.
arXiv preprint arXiv:1511.06085, 2015. 1, 2, 5

[35] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Min-
nen, J. Shor, and M. Covell. Full resolution image compres-
sion with recurrent neural networks. In CVPR, pages 5435–
5443, 2017. 1, 2

[36] Y.-H. Tsai, M.-Y. Liu, D. Sun, M.-H. Yang, and J. Kautz.
Learning binary residual representations for domain-speciﬁc
video streaming. In Thirty-Second AAAI Conference on Ar-
tiﬁcial Intelligence, 2018. 2

[37] G. K. Wallace. The jpeg still picture compression standard.
IEEE Transactions on Consumer Electronics, 38(1):xviii–
xxxiv, 1992. 1, 2

11014

[38] Z. Wang, E. Simoncelli, A. Bovik, et al. Multi-scale struc-
tural similarity for image quality assessment. In ASILOMAR
CONFERENCE ON SIGNALS SYSTEMS AND COMPUT-
ERS, volume 2, pages 1398–1402. IEEE; 1998, 2003. 2,
6

[39] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra.
Overview of the h. 264/avc video coding standard. TCSVT,
13(7):560–576, 2003. 1, 2, 3, 4, 5, 6, 7

[40] C.-Y. Wu, N. Singhal, and P. Krahenbuhl. Video compres-
sion through image interpolation. In ECCV, September 2018.
3, 6, 8

[41] C.-Y. Wu, M. Zaheer, H. Hu, R. Manmatha, A. J. Smola,
and P. Kr¨ahenb¨uhl. Compressed video action recognition. In
CVPR, pages 6026–6035, 2018. 1

[42] T. Xue, B. Chen, J. Wu, D. Wei, and W. T. Freeman.
Video enhancement with task-oriented ﬂow. arXiv preprint
arXiv:1711.09078, 2017. 2, 5

11015

