Gradient Matching Generative Networks for Zero-Shot Learning

Mert Bulent Sariyildiz

Bilkent University

Ramazan Gokberk Cinbis

Middle East Technical University (METU)

Department of Computer Engineering

Department of Computer Engineering

mert.sariyildiz@bilkent.edu.tr

gcinbis@metu.edu.tr

Abstract

Zero-shot learning (ZSL) is one of the most promising
problems where substantial progress can potentially be
achieved through unsupervised learning, due to distribu-
tional differences between supervised and zero-shot classes.
For this reason, several works investigate the incorporation
of discriminative domain adaptation techniques into ZSL,
which, however, lead to modest improvements in ZSL ac-
curacy. In contrast, we propose a generative model that
can naturally learn from unsupervised examples, and syn-
thesize training examples for unseen classes purely based
on their class embeddings, and therefore, reduce the zero-
shot learning problem into a supervised classiÔ¨Åcation task.
The proposed approach consists of two important compo-
nents: (i) a conditional Generative Adversarial Network that
learns to produce samples that mimic the characteristics of
unsupervised data examples, and (ii) the Gradient Matching
(GM) loss that measures the quality of the gradient signal
obtained from the synthesized examples. Using our GM loss
formulation, we enforce the generator to produce examples
from which accurate classiÔ¨Åers can be trained. Experimental
results on several ZSL benchmark datasets show that our
approach leads to signiÔ¨Åcant improvements over the state of
the art in generalized zero-shot classiÔ¨Åcation.

1. Introduction

There has been tremendous progress in visual recognition
models over the past several years, primarily driven by the
advances in deep learning. The state-of-the-art approaches
in deep learning, however, predominantly rely on the avail-
ability of a large set of carefully annotated training examples.
The need for such large-scale datasets poses a signiÔ¨Åcant
bottleneck against building comprehensive recognition mod-
els of the visual world, especially due to the long-tailed
distribution of object categories [1].

Recently, there has been a signiÔ¨Åcant research interest
in overcoming this difÔ¨Åculty. Prominent approaches for

Figure 1: Illustration of our approach. We propose the
Gradient Matching Network (GMN) which learns to produce
synthetic examples for a class given it‚Äôs semantic embedding.
By using the GMN we generate training samples for zero-
shot (unseen) classes, then train a supervised classiÔ¨Åer over
the union of this synthetic set and the training set of seen
classes.

this purpose include semi-supervised learning, i.e. improv-
ing supervised classiÔ¨Åcation through leveraging unlabeled
data [2, 3], few-shot learning, i.e. learning from few labeled
samples [4, 5] and zero-shot learning (ZSL) for modeling
novel classes without training samples [6, 7, 8]. In our pa-
per, we focus on the ZSL problem, where the goal is to
extrapolate a classiÔ¨Åcation model learned from seen classes,
i.e. those with labeled examples, to unseen classes with
no labeled training samples. In order to relate classes to
each other, they are commonly represented as class embed-
ding vectors constructed from side information. Such class
embedding vectors can be constructed in several different
ways, such as by manually deÔ¨Åning attributes that character-
ize visual and semantic properties of objects [9, 10, 11]
or by adapting vector-space embeddings of class names
[12, 13, 14] or by representing the position of classes in
a relevant taxonomy tree as vectors [15]. Given the class
embeddings, the ZSL problem boils down to modeling rela-
tions between a visual feature space, i.e. images or features
extracted from some deep convolutional network, and a class
embedding space [16, 15, 17, 18, 19, 20, 21, 22].

However, ZSL models typically suffer from the domain
shift problem [23] due to distributional differences between
seen and unseen classes. This can signiÔ¨Åcantly limit the
generalized zero-shot learning (GZSL) accuracy where test

12168

SEEN CLASSESbirdcowùõ©All-ClassesSupervised TrainingCNNGMNUNSEEN CLASSEShas-armhas-tail...bathas-winghas-teeth...monkeysamples may belong to any of the seen or unseen classes [24].
Towards addressing this problem, several recent work have
proposed generative models that can synthesize training sam-
ples for unseen classes and learn a classiÔ¨Åer from real and/or
synthesized examples [25, 26, 27, 28]. Therefore, a bias
towards seen classes can be reduced considerably.

Similarly, in this work, our goal is to learn a generative
model that can synthesize samples for any class of interest,
purely based on the embedding vector of the class. Once
the generative model is learned, we augment the set of seen
class examples by the set of unseen class examples sampled
from the generative model. The Ô¨Ånal classiÔ¨Åcation model is
then built by training a classiÔ¨Åer over the real and synthetic
training examples, Therefore, in a sense, we reduce ZSL to
a supervised learning problem, as illustrated in Fig 1.

Just like any other example-synthesis based ZSL ap-
proach, however, the accuracy of the resulting classiÔ¨Åer
heavily depends on the diversity and Ô¨Ådelity of the training
examples synthesized by the generative model. For this rea-
son, we speciÔ¨Åcally focus on two directions: (i) leveraging
unseen examples to implicitly model the manifold of each
unseen class, (ii) ensuring that generative model produces
data using which we can train an accurate classiÔ¨Åer.

In order to leverage unlabeled examples, we propose
as a Generative Adversarial Network (GAN) [29] based
formulation. In particular, in contrast to recent GAN-based
example synthesis approaches [26, 27, 28], our approach
allows utilizing an unconditional GAN discriminator, which
naturally extends to incorporating over unlabeled training
examples. In this way, we aim to learn a generator that
produces samples that mimic the characteristics of both seen
and unseen classes.

In order to learn to generate better training data, we pro-
pose a novel loss function that behaves as a quality inspector
on the synthetic samples. More speciÔ¨Åcally, we aim to guide
the generator towards minimizing the classiÔ¨Åcation loss of
synthetic example-driven classiÔ¨Åcation models. For this pur-
pose, we derive the gradient matching loss as a proxy for the
classiÔ¨Åcation loss, which measures the discrepancy between
the gradient vectors obtained using the real versus synthetic
samples. We refer to our complete model that incorporates
this loss term as Gradient Matching Network (GMN).

We show that our Ô¨Ånal classiÔ¨Åcation models lead to state-
of-the-art results on ZSL benchmark datasets Caltech-UCSD
Birds (CUB) [30], SUN Attributes (SUN) [31] and Animals
with Attributes (AWA) [11] using the challenging and realis-
tic Generalized ZSL (GZSL) evaluation protocols [24, 32].

The rest of the paper is organized as follows: Section 2
provides an overview of the most relevant previous work,
Section 3 explains the details of the proposed approach, and
Section 4 presents empirical evaluations of the method. Fi-
nally, Section 5 concludes the paper with a brief discussion.

2. Related Work

In this section, we provide an overview of the most related

work on zero-shot learning.

Over the years, a number of ZSL approaches have been
proposed. For example, [6] models the joint probability of
attributes, [8] models the conditional distribution of features
given attributes, [33] uses semantic knowledge bases for at-
tribute classiÔ¨Åcation, [34, 35, 36] build convex combinations
of seen class classiÔ¨Åers, [15, 17, 18, 19, 20, 21, 22] learn a
compatibility function between features and class embed-
dings. Similarly, [37, 38, 39] learn a mapping from semantic
embeddings to visual features, and, [40, 41, 42] learn a data-
driven metric for comparing similarities between features
and semantic embeddings. Alternatively, transductive ap-
proaches have been proposed to beneÔ¨Åt from unlabeled data
[43, 23, 44, 39]. Such discriminative techniques, however,
typically assume that each unlabeled example belongs to one
of the unseen (or seen) classes, which can be an unrealistic
assumption in practice.

Recently, the use of contemporary generative models
[45]
in zero-shot learning settings has gained attention.
proposes training a conditional Variational Auto-Encoder
(cVAE), that learns to generate samples according to given
class embeddings. [44] extends this notion with trainable
class conditional latent spaces. [28] also develops a cVAE ex-
cept that their model learns a separate semantic embedding
regressor/discriminator. [25] evaluates several generative
[27]
models for learning to generate training examples.
adopts cycle consistency loss of cycle-GAN into zero-shot
learning to regularize feature synthesis network. [46] uses
a separate reconstructor, discriminator and classiÔ¨Åer all tar-
geting at visual features to remedy domain-shift problem.
Slightly different from mainstream approaches, [47] intro-
duces diffusion regularization to increase utility of features.
[26] proposes a WGAN [48] based formulation that uses a
discriminative supervised loss function, in addition to the
unsupervised adversarial loss. In this model, the supervised
loss enforces the WGAN generator to produce samples that
are correctly classiÔ¨Åed according to a pre-trained classiÔ¨Åer
of seen classes.

Among the aforementioned works, [26] is the one closest
to ours in the sense that we also train a conditional WGAN
towards synthesizing training samples. However, our ap-
proach has two major differences. First, we use the proposed
gradient matching loss, which aims to directly maximize the
value of the produced training examples by measuring the
quality of the gradient signal obtained over the synthesized
examples. Second, our model learns an unconditional dis-
criminator i.e., the discriminator network does not rely on a
semantic embedding vector. This permits us to explore the
incorporation of unlabeled training examples into training in
a semi-supervised fashion.

2169

œï

ÃÉ¬†

x

Óà≥

x

bill_shape¬†:¬†cone¬†

¬†¬†¬†¬†¬†¬†wing_color¬†:¬†iridescent¬†

¬†¬†¬†¬†¬†¬†¬†¬†¬†...¬†

Óà∏

Óà∞

f (. ; Œ∏)

‚àá

Óà∏

WGAN

Œ∏

GM

‚àá

f (x, a; Œ∏)

Œ∏

‚àá

f ( , a; Œ∏)

x

ÃÉ¬†

Œ∏

Figure 2: Illustration of the gradient matching loss. œÜ is a pre-trained CNN. G is the generator and it synthesizes features for
any class using its semantic embedding. D represents the discriminator network. f is the compatibility function. ‚àá denotes
gradient operator in the compute graph. Paths through which only data of seen classes Ô¨Çow when D is unconditional are
colored in green. (Best viewed in color.)
3. Method

¬†¬†¬†¬†¬†¬†wing_color¬†:¬†iridescent¬†

bill_shape¬†:¬†cone¬†

¬†¬†¬†¬†¬†¬†¬†¬†¬†...¬†

Óà≥

x

x

ÃÉ¬†

Óà∏

Óà∞

WGAN

In ZSL, the goal is to learn a classiÔ¨Åer on a set of seen
classes for which we have training samples, and then to
use this function to predict the class labels of test samples
belonging to unseen classes for which we have no training
data. In addition to the conventional ZSL, in GZSL, the
test samples may also belong to the seen classes. To enable
knowledge transfer to novel classes, one can deÔ¨Åne an aux-
iliary (semantic embedding) space A, in which both seen
and unseen classes can be uniquely identiÔ¨Åed. This way,
the classiÔ¨Åer can be formulated as a compatibility function
f (x, a; Œ∏f ) : X √ó A ‚Üí R, which estimates the degree of
conÔ¨Ådence that the input image (or its representation) x ‚àà X
belongs to the class represented by the embedding a ‚àà A,
using the model with parameters Œ∏f . Given the compatibility
function, the classiÔ¨Åer over all classes can be constructed.

We start by deÔ¨Åning a set of seen classes Ys =
{1, . . . , Cs} and a set of unseen classes Yu = {Cs +
1, . . . , Cs + Cu} such that Ys ‚à© Yu = ‚àÖ and Yall = Ys ‚à™ Yu.
For each class in Yall, there is a unique class embedding
vector a ‚àà Rda , and we denote the set of all class embed-
dings by Aall. Thus As and Au represent embeddings of
seen and unseen classes, respectively. Dtrain = {(x, a) | x ‚àà
Xs, a ‚àà As}, is the training set containing N examples,
where each training example consists of the feature repre-
sentation x ‚àà Rdx extracted using a pre-trained CNN, and
the corresponding class embedding vector a. Here, Xs de-
notes the set of all labeled data points. During training, our
approach can optionally utilize a set of unlabeled examples,
denoted by Xu.

3.1. Unsupervised GAN

Our generative model is built upon the WGAN [49] as in
[26]. Different from vanilla GAN [29], WGAN optimizes
the Wasserstein distance using Kantorovich Rubinstein du-
ality, instead of optimizing the Jensen-Shannon divergence.
It is shown that enforcing discriminators to be 1-Lipschitz
provides more stable gradients for generators. Even though
clipping the weights of discriminators serves this purpose,

it leads to unstable training for the WGAN. Instead, [48]
propose to apply gradient penalty to discriminators to control
their Lipschitz norm, which we use as our starting point:

f ( , a; Œ∏)

‚àá

x

ÃÉ¬†

Œ∏

f (. ; Œ∏)

‚àá

LWGAN = E

Œ∏

x‚àºPr

[D(x)] ‚àí E

f (x, a; Œ∏)

‚àá

Œ∏

Àúx‚àºPg

[D(Àúx)] +

GM

Óà∏

(1)

Œª E

ÀÜx‚àºP ÀÜx(cid:2)(k‚àáÀÜxD(ÀÜx)k2 ‚àí 1)2(cid:3) ,

where, Pr is the true data distribution, Pg denotes generator
outputs and ÀÜx is the interpolation between x and Àúx.

Note that Eq. 1 does not involve any label information
regarding either the real samples from the data distribution
x ‚àº Pr or fake ones synthesized by the generator Àúx ‚àº Pg.
In order to generate a sample Àúx, a noise vector shall be sam-
pled from a prior distribution and then fed into the generator
in a purely unsupervised manner. In our case, however, we
aim to produce training samples for the unseen classes using
the generative model. For this purpose, we need to train a
generator network such that it takes a combination of the
noise vector and class embeddings as input, and therefore,
produces class-speciÔ¨Åc samples according to the side infor-
mation given by the class embedding.

A simple scheme for combining the noise and class em-
bedding vectors is to concatenate them [26]. However, we
can also aim to model the latent distributions corresponding
to classes and then take samples from these latent distri-
butions instead. For this purpose, inspired from [44], we
propose to deÔ¨Åne a conditional multivariate Gaussian dis-
tribution N (¬µ(a), Œ£(a)), where ¬µ(a) = Wmua + bmu and
Œ£(a) = exp(Wcova + bcov) estimate an Rdz dimensional
Gaussian noise mean and covariance conditioned on class
embeddings, Wmu and Wcov are linear transformation ma-
trices and bmu and bcov are bias vectors. Therefore, in order
to generate a sample of class j, we Ô¨Årst compute ¬µ(aj) and
Œ£(aj), take a noise sample from N (¬µ(a), Œ£(a)) and then
feed the noise into the generator network. To make the sam-
pling process differentiable, we use the re-parameterization
trick [50, 51]. In this manner, we make Wmu, Wcov, bmu and
bcov end-to-end differentiable and train them as an integral
part of the generative network.

2170

3.2. Gradient Matching Loss

So far the aforementioned approach lacks deÔ¨Ånition of
any supervisory signal, which is crucial for learning a correct
conditional generative model (See the Ablation Study in
Sec. 4). One possible solution is to measure the correctness
of the resulting samples for the seen classes using the loss
function of a pre-trained classiÔ¨Åcation model, which is the
approach used in [26]. However, we argue that classiÔ¨Åcation
guidance does not necessarily lead to the synthesis of a
good training set as it measures the loss of the samples
w.r.t. the pre-trained model, rather than the expected loss of
the model trained on them. For instance, if the generator
learns to generate only conÔ¨Ådently classiÔ¨Åed examples, the
classiÔ¨Åcation loss given by the pre-trained model will be
low, even though the resulting training set lacks examples
near class boundaries, i.e. the support vectors. In fact, [52,
53] report that conditional GAN models tend to produce
degenerate class conditional examples when they are trained
to minimize the loss of a pre-trained classiÔ¨Åer.

Based on these observations we propose that instead of
aiming to produce samples that are correctly classiÔ¨Åed by a
pre-trained model, we should focus on learning to generate
training examples that lead to accurate classiÔ¨Åcation models.
For this purpose, one can consider training the generative
model by minimizing the Ô¨Ånal loss of a tentative classiÔ¨Å-
cation model trained over the synthetic samples. Here, the
tentative classiÔ¨Åer would be iteratively trained via a gradient
based optimizer over a number of model update steps, within
each training iteration for the generative model. Since all
computational blocks are differentiable, such an approach
would allow training the generative model in an end-to-end
fashion such that it learns to generate training examples from
which accurate classiÔ¨Åcation models can be built.

However, based on our preliminary experiments, we have
observed that such a naive strategy performs poorly for two
important reasons. First, normally a large number of model
update steps are needed to be able to train the tentative
classiÔ¨Åer. However, integrating a long compute chain of
model update steps within the generative model training
procedure not only slows down the training procedure very
signiÔ¨Åcantly, but also leads to vanishing gradient problems.
Second, using an unrealistically small number of classiÔ¨Åer
update steps due to the aforementioned problems, on the
other hand, encourages the generative model to produce
unrealistic samples that aim to ‚Äúquickly‚Äù minimize the Ô¨Ånal
loss over the few classiÔ¨Åcation model update steps.

Instead, we address these issues by focusing on maximiz-
ing the correctness of individual model updates. We observe
the simple fact that in the case where a generative model
learns true class manifolds, partial derivatives of a loss func-
tion with respect to classiÔ¨Åcation model parameters over a
large set of synthetic examples would be highly correlated
with those over a large set of real training examples.

Following these observations, we propose to minimize
the approximation error of the gradients obtained over the
synthetic samples of seen classes. More speciÔ¨Åcally, we
propose to learn a generative model G such that it maximizes
the correlation between gradients over the synthetic samples
and those over the real samples. To formalize this idea, we
Ô¨Årst deÔ¨Åne the aliases gr and gs for the expected gradient
vectors over the real and synthesized examples, respectively:

gr(Œ∏) =

E

(x,a)‚àºD

gs(Œ∏) =

E

Àúx‚àºG(a‚àºA

s(cid:2)‚àáŒ∏f LCLS(f, x, a; Œ∏f = Œ∏)(cid:3) ,
s)(cid:2)‚àáŒ∏f LCLS(f, Àúx, a; Œ∏f = Œ∏)(cid:3) .

(2)

(3)

Here, LCLS(f, x, a) is the loss function used in training the
compatibility function f (x, a; Œ∏f ). Throughout the training
procedure, we approximate gr and gs over sample batches.
Since the most important information conveyed by the
gradient vector is the direction towards the local minima
rather than the absolute scale of the gradient vectors, we
measure the discrepancy between gr and gs via the cosine
similarity between two vectors. Finally, we formalize the
gradient matching loss LGM as the expected cosine distance
between the gr and gs, computed over all possible compati-
bility model parameters Œ∏:

LGM = E

Œ∏ (cid:20)1 ‚àí

gr(Œ∏)Tgs(Œ∏)

kgr(Œ∏)k2kgs(Œ∏)k2(cid:21) ,

(4)

In our experiments, we approximate the expectation by sam-
pling Œ∏f vectors obtained over the training iterations while
learning the compatibility function via gradient descent over
real training examples. Then our Ô¨Ånal objective becomes

Œ∏‚àó
G, Œ∏‚àó

D = arg min
Œ∏G ,Œ∏D

{LWGAN + Œ≤LGM},

(5)

where Œ≤ is a simple weight hyper-parameter to be tuned on a
validation set. We refer to a generative model trained within
this framework as Gradient Matching Network (GMN).

Given the true generative model of the data distribution
and a representative train set, the correlation between gr(Œ∏)
and gs(Œ∏) is expected to be high, independent of the com-
patibility model parameters Œ∏. Therefore, in principle, any
compatibility function model can be utilized within the gradi-
ent matching loss. In our experiments, we use cross-entropy
as LCLS and implement the compatibility function f as a
bilinear model:

f (x, a; W, b) = xTW a + b.

(6)

The compatibility matrix W and bias vector b corresponds to
Œ∏. We note that while optimizing LGM by a batch gradient de-
scent update rule, it is important to compute gr(Œ∏) and gs(Œ∏)
over real and synthetic samples of the same class, respec-
tively. This makes sure that the genarator effectively learns
class manifolds separately. Otherwise, although matching
the aggregated gradients ‚àáŒ∏f of a batch of samples that be-
long to different classes is still a valid supervision for the
generator to learn the data distribution, it becomes difÔ¨Åcult

2171

for the generator to learn individual class distributions.

4. Experiments

Furthermore, thanks to our gradient matching loss, we can
decouple the class label supervision from LWGAN objective.
This way, depending on the availability of unlabeled training
data, LWGAN term in Eq. 5 can be computed either over seen
class embeddings and samples (LS
WGAN), or, over all classes
(LS+U
LS

WGAN) possibly in a transductive way:
WGAN =

[D(x)] + ŒªLGP

(7)

E

Àúx‚àºG(a‚àºA

s)

[D(Àúx)] ‚àí E
x‚àºX

s

LS+U

WGAN =

E

Àúx‚àºG(a‚àºA

all)

[D(Àúx)] ‚àí E
x‚àºX

all

[D(x)] + ŒªLGP,

(8)

where LGP is the gradient penalty term in Eq. 1. Here, in
the case of Eq. 8, Dtrain also includes Xu. Unlike most of
the transductive zero-shot learning approaches, we do not
assume that unseen examples belong solely to the unseen
classes: while such an assumption can possibly provide a
signiÔ¨Åcant advantage in training, it is unrealistic in most
scenarios. The compute graph summarizing our approach is
depicted in Fig. 2.

3.3. Supervision by Conditional Discriminator

Up to now, the source of supervision for a generator
network is deÔ¨Åned by an auxiliary loss function minimized
by the generator network itself during training. However, we
can also condition a discriminator network on either one-hot
class labels or semantic embedding vectors so that it can
also learn relations between visual features and semantic
embeddings [26, 27, 28, 46]. To do that we slightly change
Eq. 1 as follows:

LS

cWGAN = E [D(x, a)] ‚àí E [D(Àúx, a)] +

(9)

Œª E(cid:2)(k‚àáÀÜxD(ÀÜx, a)k2 ‚àí 1)2(cid:3) .

We note that this conditional form of the discriminator net-
work can only be trained using training samples of seen
classes. In other words, it cannot be utilized over unsuper-
vised samples in a semi-supervised or transductive settings.

In our experiments, we comprehensively evaluate the
impact of training with different GAN loss versions
(LS
WGAN, LS
cWGAN) and their combinations with gra-
dient matching loss.

WGAN, LS+U

3.4. Feature Synthesis

Once we train our generative model, we synthesize train-
ing examples for both seen and unseen classes by providing
their class embeddings into the generative network as input,
and then we combine the resulting Dfake with Dtrain to form
our Ô¨Ånal training set ÀúD = Dtrain ‚à™ Dfake. Once all samples
are generated, we train the multi-class classiÔ¨Åcation model
based on the compatibility function by simply minimizing
the cross-entropy loss over all (seen+unseen) classes. Finally,
we utilize the resulting f to perform ZSL and GZSL.

In this section, we present an experimental evaluation of
the proposed approach. First we brieÔ¨Çy explain our experi-
mental setup, then we evaluate important GMN variants and
compare with the state of the art. We additionally analyze our
model via a detailed ablation study, including an evaluation
on the effect of using synthesized training examples.

nattr

|Yu|

|Ys|

|Xu|

|Xs|

ANOSPC

CUB [30]

SUN [31]

AWA [11]

312
102
85

50
72
10

150
645
40

2967
1440
5685

8821
12900
24790

59
20
609

Table 1: Statistics for the benchmark datasets. nattr denotes
number of attributes and |.| indicates cardinality of a set. The
last column shows average number of samples per class over
each of the datasets. We use the splits proposed in [32].

Datasets. We evaluate our model in the three commonly
used benchmark datasets, namely Caltech-UCSD Birds-200-
2011 (CUB) [30], SUN Attribute (SUN) [31] and Animals
with Attributes (AWA) [11]. CUB and SUN are Ô¨Åne-grained
image datasets which contain 200 bird species and 717 scene
categories, respectively. They are particularly challenging
for ZSL & GZSL as they contain relatively fewer images
per class, making it difÔ¨Åcult to model intra-class variations
efÔ¨Åciently. AWA is a coarse-grained dataset consisting of
images belonging to 50 animals. AWA contains a relatively
small set of classes, which makes generalization to unseen
classes more difÔ¨Åcult. A summary is given in Table 1.

In our comparisons, we utilize the splits, class embed-
dings and evaluation metrics proposed in [32] for standard-
ized ZSL and GZSL evaluation. We use class-level attributes
as class embeddings. For CUB experiments, we addition-
ally use 1024-dimensional character-based CNN-RNN fea-
tures [54] as in [32, 27]. As a pre-processing step, we ‚Ñì2
normalize the class embeddings. Following [32, 25, 26], we
use the 2048-dimensional top pooling units of a ResNet-101
pretrained on ImageNet-1K as the image representation. We
do not apply any pre-processing to these features.

fake, Ds

fake, Du

Evaluation. Once we train a GMN on a particular dataset
Dtrain, we synthesize nzsl, ngzsl-u and ngzsl-s-many samples
for each unseen class to create a separate augmented dataset
Dzsl
fake for training separate models for the ZSL,
GZSL-u and GZSL-s evaluations, respectively. Additionally,
we create Da
fake containing na synthetic sample per unseen
class, to train a single model that performs all tasks, i.e. clas-
sifying seen and unseen class examples. Exceptionally, only
on the AWA dataset, where there is a signiÔ¨Åcant imbalance
among training classes, we additionally synthesize examples
for the seen classes to obtain equivalent number of training

2172

Zero-Shot Learning

Generalized Zero-Shot Learning

CUB SUN AWA

CUB

SUN

AWA

Method

T-1

T-1

T-1

u

s

h

u

s

h

u

s

h

Train only with real samples (Ds)
LS
LS

WGAN + LCLS

cWGAN

LS
LS
LS+U

WGAN + LGM
cWGAN + LGM
WGAN + LGM (transductive)

56.8
58.3
60.6

61.9
64.6
64.6

60.7
61.4
62.6

63.8
64.1
64.3

62.3
70.0
72.0

70.4
73.9
82.5

26.9
47.0
55.9

55.8
57.9
60.2

67.6
71.0
71.1

70.7
71.2
70.6

38.4
56.5
62.6

62.4
63.9
65.0

23.4
47.7
53.6

53.8
55.2
57.1

36.3
41.2
41.1

40.9
40.8
40.7

28.4
44.2
46.5

46.5
46.9
47.5

13.4
47.8
55.2

78.1
78.7
79.1

22.9
59.5
65.0

78.8
78.8

52.1
62.7
63.2
70.1
70.8 79.2 74.8

Table 2: Quantitative evaluation of GMN over the strong baselines.

examples for the seen classes. These numbers are considered
as hyper-parameters and therefore tuned on the validation
splits. We set sample spaces in a way that they are compara-
ble with the ANOSPC value of each dataset (Table 1). Then
we stack each Dfake together with the training set Dtrain to
form ÀúD on which we train f deÔ¨Åned in Eq. 6. We also tune
the hyper-parameters of this classiÔ¨Åer, i.e. learning rate and
number of training iterations, for each experimental setup
separately on the validation splits as well. Once the Ô¨Ånal
classiÔ¨Åer is trained, we assign a label to a test sample by
considering only the scores of unseen classes Yu for ZSL;
we consider the scores of all classes Yall when determin-
ing the label for GZSL. As evaluation scores we compute
normalized mean top-1 accuracies T-1. We compute two
metrics for GZSL experiments: GZSL-u (u) and GZSL-s
(s) are normalized GZSL T-1 accuracies of unseen and seen
classes, respectively . Finally we compute their harmonic
means by h = 2√óu√ós
u+s

[32].

Implementation details. In our experiments, we realize
G and D as simple MLPs that have 1 or 2 hidden layers
with 1024, 2048 or 4096 units. Both networks have ReLU
activation functions. We consider the dimension of noise
spaces dz as another hyper-parameter. While minimizing
LGM, we also update the classiÔ¨Åcation model, but unlike the
G and D models, we regularly re-initialize the classiÔ¨Åcation
model every N iterations. We tune all the hyper-parameters
on the validation sets. While developing the model, we
observed the followings: (i) Constraining noise means by
applying tanh activation, i.e. ¬µ(a) = tanh(Wmua + bmu),
slightly improves generalization performance. (ii) Using an
identity covariance matrix, ie. Œ£(a) = I, performs equally
well. (iii) Minimizing lp loss between the gradient vectors,
i.e. kgr(Œ∏) ‚àí gs(Œ∏)kp in addition to maximizing their cosine
similarity leads to slightly better results. Therefore, we tune
these design choices on the validation sets as well.

GMN versus strong baselines. In Table 2, we present a
detailed evaluation of the GMN variants and compare them
against strong baselines. In order to carefully observe the
variants on each task, we train a separate model for each

task following the evaluation scheme described above. The
Ô¨Årst part of the table shows baselines with no GM-loss: (i)
training f with available seen class samples only, (ii) train-
ing G model w.r.t. unconditional discriminator based on the
seen class examples (LS
WGAN) plus the loss of a pre-trained
classiÔ¨Åer (LCLS), (iii) training G model with conditional dis-
criminator over the seen classes (LS
cWGAN). The second part
of the table shows GMN variants, where the GM-loss is used
in combination with (i) an unconditional seen-class only dis-
criminator (LS
WGAN), (ii) a conditional discriminator over the
seen classes (LS
cWGAN), (iii) an unconditional discriminator
over seen and unseen classes (LS+U
WGAN). In all cases (except
the very Ô¨Årst one), the resulting G model is used for generat-
ing nzsl, ngzsl-u and ngzsl-s-many synthetic training examples
for each unseen class. Only the last experiment runs in a
transductive setting. Note that we do not report any results
with LS
WGAN-only training as they lead to
unusable G due to lack any class supervision (see Fig.5).

WGAN-only or LS+U

From the results, we observe that all generative model
based methods signiÔ¨Åcantly improve over the simple real
data only baseline. This result shows that generating unseen
class examples via G helps training f models that are par-
ticularly better at Generalized ZSL. We also observe that
using a conditional discriminator leads to better results than
training with the loss function of a pre-trained classiÔ¨Åer, as
we hypothesize in Section 3.

From the results in the second part of Table 2, we observe
that the proposed gradient matching loss consistently im-
proves all ZSL and u scores by marginally sacriÔ¨Åcing the s
scores. In particular we observe that GM loss is signiÔ¨Åcantly
a better way for training G than minimizing the classiÔ¨Åcation
loss of a pre-trained classiÔ¨Åer. In addition, we observe that
LGM improves over the conditional discriminator based G
training. Finally, we observe that unsupervised discriminator
(over all classes) combined with the GM loss (over seen
class examples only) gives a strong approach for training the
conditional generative model in a transductive way.

GMN versus state of the art. Finally, we present a com-
parison of GMN with the recently proposed state-of-the-art
non-transductive ZSL approaches on the benchmarks from

2173

Zero-Shot Learning

Generalized Zero-Shot Learning

CUB SUN AWA

CUB

SUN

AWA

Method

Zhang et al. [46] ‚Äô18

Bucher et al. [25] ‚Äô17

Xian et al. [26] - DEVISE ‚Äô18

Xian et al. [26] - ALE ‚Äô18

Verma et al. [28] ‚Äô18

Felix et al. [27] - cycle-WGAN ‚Äô18

Felix et al. [27] - cycle-CLSWGAN ‚Äô18

LS
LS

cWGAN + LCLS [26]
cWGAN + LGM (Ours)

LS

cWGAN + LGM ‚Ä° (Ours)

T-1

T-1

T-1

u

s

h

u

s

h

u

s

h

52.6
57.8
60.3
61.5
59.6
57.8
58.4

61.7
60.4
60.9
62.1
63.4
59.7
60.0

67.4
66.3
66.9
68.2
69.5
65.6
66.3

31.5
28.8
52.2
40.2
41.5
46.0
45.7

62.2
64.3

62.7
63.6

69.4
71.9

51.1
56.1

40.2
55.7
42.4
59.3
53.3
60.3
61.0

54.9
54.3

35.3
38.0
46.7
47.9
46.7
52.2
52.3

41.2
40.5
38.4
41.3
40.9
48.3
49.4

52.9
55.2

50.6
53.2

26.7
37.2
25.4
31.1
30.5
33.1
33.6

30.3
33.0

32.4
38.8
30.6
35.5
34.9
39.2
40.0

38.7
2.3
35.0
47.6
56.3
56.4
56.9

37.3
40.7

57.5
61.1

74.6
90.2
62.8
57.2
67.8
63.5
64.0

66.8
71.3

51.0
4.5
45.0
52.0
61.5
59.7
60.2

61.8
65.8

64.6

64.1

73.9

57.9

71.2

63.9

55.2

40.8

46.9

63.2

78.8

70.1

Table 3: Comparison of GMN against the baselines and the state-of-the-art on the benchmarks from [32]. The results are taken
from the papers, except for [25] (using the the authors‚Äô implementation) and LS

cWGAN + LCLS (our implementation).

[32], in Table 3. GMN, ALE and DEVISE models from
[32], cycle-WGAN model from [27] and our implemen-
tation of [32] by using a bilinear compatibility function
(LS
cWGAN + LCLS) employ the same generative model i.e.
WGAN conditioned on semantic embeddings (except that
GMN and LS
cWGAN + LCLS explicitly learn a noise space con-
ditioned on the embeddings) but optimize different auxiliary
loss functions aside from the WGAN objective. Therefore,
by comparing them we can see that gradient matching out-
performs both minimizing the loss of a pre-trained classiÔ¨Åer
and minimizing the cycle-consistency loss in the context of
zero-shot learning either when a single model is trained to
perform all tasks (LS
cWGAN + LGM) or a separate model is
trained to perform individual tasks (LS
cWGAN + LGM ‚Ä°). Over-
all, we observe that GMN leads to signiÔ¨Åcant improvements
over the state of the art in terms of h-scores on all datasets.

Effect of the feature generation. Having veriÔ¨Åed the ef-
fectiveness of our framework, we now evaluate how the
size of the synthetic train data affect the Ô¨Ånal ZSL and
GZSL performances. For this purpose, we train two gen-
erative models on each dataset by optimizing LS
WGAN and
LS+U
WGAN, respectively. Then we create six different synthetic
datasets ÀúDi
u with each model by sampling for each class
{10, 25, 50, 150, 200, 250} features on CUB and SUN and
{500, 600, 700, 800, 900, 1000} features on AWA. In Fig. 3,
we see that, as the number of features synthesized increases,
u scores on all datasets increase rapidly, ZSL scores develop
progressively on the SUN and remain almost Ô¨Åxed on the
others. In addition, we observe a trade-off between s scores
and the amount of synthesized features, i.e. s scores decrease
dramatically as f is trained with more synthesized features.
This is an expected result since (i) f is no longer biased to-
wards seen classes, (ii) the pooled sets ÀúDi become dominated
by the synthesized sets ÀúDi
u. In fact s scores on the AWA sup-

port this claim i.e. slope of the decrease in s scores is much
smaller, because in AWA there are 609 samples per class on
average. We observe that synthesizing more features does
not necessarily increase the ZSL scores. This might indicate
that class speciÔ¨Åc noise spaces become less discriminative
due to the WGAN updates or the generator might be learning
to model feature space irrespective of the noise distributions.
And Ô¨Ånally, the gap between LS
WGAN on AWA
suggests that GMN struggles in Ô¨Ånding generalizable latent
spaces when there are small set of classes and attributes.

WGAN and LS+U

WGAN + LGM and LS+U

Ablation study. We perform additional analyses to gain
further insight about GMN. We begin by elaborating more
on the supervision signal conveyed by LGM, when discrim-
inator is unconditional, and the effect of utilizing samples
of unseen classes on u scores. To do that we train four dif-
ferent generators over SUN dataset by optimizing LS
WGAN,
LGM, LS
WGAN + LGM, respectively. Be-
sides, while training the generators we compute u scores
of samples in the validation set. Not surprisingly, when a
generator optimizes LS
WGAN alone, it cannot distinguish class
modes separately. Therefore features synthesized by this
network results in a poor classiÔ¨Åer. However, a generator
optimizing only LGM starts learning a mapping from seman-
tic embeddings to prominent visual features. Furthermore,
LS
WGAN + LGM slightly improves the mapping by most likely
modeling data manifold more effectively. Finally, as an
unconditional discriminator captures statistics of samples
belonging to unseen classes by means of optimizing LS+U
WGAN,
classiÔ¨Åer performance peaks. We share plots corresponding
to each experiment in Fig. 5.

Next, we show that conditioning a discriminator on se-
mantic embeddings and gradient matching are complemen-
tary sources of supervision for a generator network. In our
experiments, we see that optimizing LS
cWGAN + LGM, com-
pared to optimizing only LS
cWGAN, improves almost all perfor-

2174

Figure 3: Analysis of the impact of the number of synthesized features on (from left to right) T-1, u and s scores on CUB,
SUN and AWA. Top row is obtained by optimizing LS

WGAN, while bottom row is LS+U

WGAN.

Figure 5: u scores of validation samples from the SUN
dataset over the training iterations of LS
WGAN (GMNin¬ß),
LGM (GMNin‚Ä°), LS
WGAN + LGM
(GMNtr). (Better viewed in color.)

WGAN + LGM (GMNin) and LS+U

learning better relations between visual features and seman-
tic embeddings. Some of examples are shown in Fig. 4.

5. Conclusion

Figure 4: Examples from unseen CUB classes that are mis-
classiÔ¨Åed with a cWGAN-based classiÔ¨Åer but correctly rec-
ognized when we include LGM into generator training. From
top to bottom the classes are groove billed ani, wilson war-
bler, mallard, le conte sparrow and tropical kingbird.

mance metrics signiÔ¨Åcantly. We perform a rather qualitative
evaluation on CUB dataset to examine the improvements
that result from introducing LGM term in a plain conditional
WGAN setup. For this purpose, we inspect classes that are
improved by introducing gradient matching. SigniÔ¨Åcant im-
provements in certain classes suggest that GMN can help

In this work, we propose a novel loss function based
generative model for capturing modalities of CNN features
given semantic class embeddings. We show that our gen-
erative model is able to synthesize discriminative features
which can be used to train a state-of-the-art classiÔ¨Åer for
generalized zero-shot classiÔ¨Åcation.

Acknowledgements. This work was supported in part by
the TUBITAK Grant 116E445. The numerical calculations
reported in this paper were partially performed at TUBITAK
ULAKBIM, High Performance and Grid Computing Center
(TRUBA resources).

2175

102550150200250Numberoffeaturesperclass0204060T-1Accuracy(in%)CUBZSLus102550150200250Numberoffeaturesperclass0204060T-1Accuracy(in%)SUNZSLus5006007008009001000Numberoffeaturesperclass0255075T-1Accuracy(in%)AWAZSLus102550150200250Numberoffeaturesperclass0204060T-1Accuracy(in%)CUBZSLus102550150200250Numberoffeaturesperclass0204060T-1Accuracy(in%)SUNZSLus5006007008009001000Numberoffeaturesperclass0255075T-1Accuracy(in%)AWAZSLus0255075100Epoch2040uaccuracy(in%)SUNGMNtrGMNinGMNin‚Ä°GMNin¬ßReferences

[1] Antonio Torralba, Rob Fergus, and William T Freeman.
80 million tiny images: A large data set for nonparamet-
ric object and scene recognition. IEEE Trans. Pattern
Anal. Mach. Intell., 2008. 1

[2] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. Semi-supervised learning
with deep generative models. In Advances in Neural
Information Processing Systems, 2014. 1

[3] Tim Salimans, Ian Goodfellow, Wojciech Zaremba,
Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen.
Improved techniques for training gans. In Advances in
Neural Information Processing Systems 29. 2016. 1

[4] Antreas Antoniou, Amos Storkey, and Harrison Ed-
wards. Data Augmentation Generative Adversarial
Networks. arXiv:1711.04340, 2017. 1

[5] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang,
Philip H. S. Torr, and Timothy M. Hospedales. Learn-
ing to Compare: Relation Network for Few-Shot Learn-
ing. arXiv:1711.06025, 2017. 1

[6] Christoph H Lampert, Hannes Nickisch, and Stefan
Harmeling. Attribute-based classiÔ¨Åcation for zero-shot
visual object categorization.
IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2014. 1, 2

[7] Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio.

Zero-data learning of new tasks. In AAAI, 2008. 1

[8] Xiaodong Yu and Yiannis Aloimonos. Attribute-based
transfer learning for object categorization with zero/one
training example. In Proc. European Conf. on Com-
puter Vision, 2010. 1, 2

[9] Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. Describing objects by their attributes.
In
Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2009. 1

[10] Ali Farhadi, Ian Endres, and Derek Hoiem. Attribute-
centric recognition for cross-category generalization.
In Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2010.
1

[11] Christoph H Lampert, Hannes Nickisch, and Stefan
Harmeling. Learning to detect unseen object classes by
between-class attribute transfer. In Proc. IEEE Conf.
Comput. Vis. Pattern Recog., 2009. 1, 2, 5

[12] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. EfÔ¨Åcient estimation of word representations in
vector space. arXiv preprint arXiv:1301.3781, 2013. 1

[13] Jeffrey Pennington, Richard Socher, and Christopher
Manning. Glove: Global vectors for word represen-
tation. In Proc. of the Empiricial Methods in Natural
Language Processing, 2014. 1

[14] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. Distributed representations of
words and phrases and their compositionality. In Proc.
Adv. Neural Inf. Process. Syst., 2013. 1

[15] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee,
and Bernt Schiele. Evaluation of output embeddings
for Ô¨Åne-grained image classiÔ¨Åcation. In Proc. IEEE
Conf. Comput. Vis. Pattern Recog., 2015. 1, 2

[16] Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and
Cordelia Schmid. Label-embedding for image classiÔ¨Å-
cation. IEEE Trans. Pattern Anal. Mach. Intell., 2016.
1

[17] Andrea Frome, Greg S Corrado, Jon Shlens, Samy
Bengio, Jeff Dean, Marc' Aurelio Ranzato, and Tomas
Mikolov. Devise: A deep visual-semantic embedding
model. In Advances in Neural Information Processing
Systems. 2013. 1, 2

[18] Richard Socher, Milind Ganjoo, Christopher D Man-
ning, and Andrew Ng. Zero-shot learning through
cross-modal transfer. In Advances in Neural Informa-
tion Processing Systems. 2013. 1, 2

[19] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh
Nguyen, Matthias Hein, and Bernt Schiele. Latent
embeddings for zero-shot classiÔ¨Åcation. In Proc. IEEE
Conf. Comput. Vis. Pattern Recog., 2016. 1, 2

[20] Bernardino Romera-Paredes and Philip Torr. An em-
barrassingly simple approach to zero-shot learning. In
Proc. Int. Conf. Mach. Learn., 2015. 1, 2

[21] Yanwei Fu and Leonid Sigal.

vocabulary-informed learning.
Comput. Vis. Pattern Recog., 2016. 1, 2

Semi-supervised
In Proc. IEEE Conf.

[22] Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, and Anton
van den Hengel. Less is more: Zero-shot learning from
online textual documents with noise suppression. In
Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2016.
1, 2

[23] Yanwei Fu, Timothy M Hospedales, Tao Xiang, Zheny-
ong Fu, and Shaogang Gong. Transductive multi-view
embedding for zero-shot recognition and annotation.
In Proc. European Conf. on Computer Vision, 2014. 1,
2

2176

[24] Wei-Lun Chao, Soravit Changpinyo, Boqing Gong, and
Fei Sha. An empirical study and analysis of generalized
zero-shot learning for object recognition in the wild. In
Proc. European Conf. on Computer Vision, 2016. 2

[35] Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, and
Fei Sha. Synthesized classiÔ¨Åers for zero-shot learning.
In Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2016.
2

[25] Maxime Bucher, Stephane Herbin, and Frederic Jurie.
Generating visual representations for zero-shot classiÔ¨Å-
cation. In IEEE International Conference on Computer
Vision Workshops, 2017. 2, 5, 7

[26] Yongqin Xian, Tobias Lorenz, Bernt Schiele, and
Zeynep Akata. Feature generating networks for zero-
shot learning. In Proc. IEEE Conf. Comput. Vis. Pattern
Recog., 2018. 2, 3, 4, 5, 7

[27] Rafael Felix, Vijay B. G. Kumar, Ian Reid, and Gus-
tavo Carneiro. Multi-modal cycle-consistent general-
ized zero-shot learning. In Proc. European Conf. on
Computer Vision, 2018. 2, 5, 7

[36] Ziming Zhang and Venkatesh Saligrama. Zero-shot
learning via semantic similarity embedding. In Proc.
IEEE Int. Conf. on Computer Vision, 2015. 2

[37] Li Zhang, Tao Xiang, and Shaogang Gong. Learning a
deep embedding model for zero-shot learning. In Proc.
IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2

[38] Yang Long, Li Liu, Ling Shao, Fumin Shen, Guiguang
Ding, and Jungong Han. From zero-shot learning to
conventional supervised classiÔ¨Åcation: Unseen visual
data synthesis. In Proc. IEEE Conf. Comput. Vis. Pat-
tern Recog., 2017. 2

[28] Vinay Kumar Verma, Gundeep Arora, Ashish Mishra,
and Piyush Rai. Generalized zero-shot learning via
synthesized examples. In Proc. IEEE Conf. Comput.
Vis. Pattern Recog., 2018. 2, 5, 7

[39] Elyor Kodirov, Tao Xiang, Zhenyong Fu, and Shaogang
Gong. Unsupervised domain adaptation for zero-shot
learning. In Proc. IEEE Int. Conf. on Computer Vision,
2015. 2

[29] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial
nets. In Advances in Neural Information Processing
Systems. 2014. 2, 3

[40] Thomas Mensink, Jakob Verbeek, Florent Perronnin,
and Gabriela Csurka. Metric learning for large scale
image classiÔ¨Åcation: Generalizing to new classes at
near-zero cost. In Proc. European Conf. on Computer
Vision. 2012. 2

[30] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-
longie. The Caltech-UCSD Birds-200-2011 Dataset.
Technical Report CNS-TR-2011-001, California Insti-
tute of Technology, 2011. 2, 5

[41] Alina Kuznetsova, Sung Ju Hwang, Bodo Rosenhahn,
and Leonid Sigal. Exploiting view-speciÔ¨Åc appearance
similarities across classes for zero-shot pose prediction:
A metric learning approach. In AAAI, 2016. 2

[31] Genevieve Patterson and James Hays. Sun attribute
database: Discovering, annotating, and recognizing
scene attributes.
In Proc. IEEE Conf. Comput. Vis.
Pattern Recog., 2012. 2, 5

[32] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-
shot learning - the good, the bad and the ugly. In Proc.
IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2, 5, 6,
7

[33] Mark Palatucci, Dean Pomerleau, Geoffrey E Hinton,
and Tom M Mitchell. Zero-shot learning with semantic
output codes. In Proc. Adv. Neural Inf. Process. Syst.,
2009. 2

[34] Mohammad Norouzi, Tomas Mikolov, Samy Bengio,
Yoram Singer, Jonathon Shlens, Andrea Frome, Greg
Corrado, and Jeffrey Dean. Zero-shot learning by con-
vex combination of semantic embeddings. In Interna-
tional Conference on Learning Representations, 2014.
2

[42] Maxime Bucher, St√©phane Herbin, and Fr√©d√©ric Ju-
rie. Improving semantic embedding consistency by
metric learning for zero-shot classifÔ¨Åcation. In Proc.
European Conf. on Computer Vision, 2016. 2

[43] Jie Song, Chengchao Shen, Yezhou Yang, Yang Liu,
and Mingli Song. Transductive unbiased embedding
for zero-shot learning. In Proc. IEEE Conf. Comput.
Vis. Pattern Recog., 2018. 2

[44] Wenlin Wang, Yunchen Pu, Vinay Kumar Verma,
Kai Fan, Yizhe Zhang, Changyou Chen, Piyush Rai,
and Lawrence Carin. Zero-shot learning via class-
conditioned deep generative models. In AAAI, 2017. 2,
3

[45] Ashish Mishra, M Reddy, Anurag Mittal, and Hema A
Murthy. A generative model for zero shot learning
using conditional variational autoencoders.
arXiv
preprint arXiv:1709.00663, 2017. 2

2177

[46] Haofeng Zhang, Yang Long, Li Liu, and Ling Shao.
Adversarial unseen visual feature synthesis for zero-
shot learning. Neurocomputing, 2018. 2, 5, 7

[47] Y. Long, L. Liu, F. Shen, L. Shao, and X. Li. Zero-Shot
Learning Using Synthesised Unseen Visual Data with
Diffusion Regularisation. IEEE Trans. Pattern Anal.
Mach. Intell., 2018. 2

[48] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-
cent Dumoulin, and Aaron C Courville.
Improved
training of wasserstein gans. In Advances in Neural
Information Processing Systems. 2017. 2, 3

[49] Martin Arjovsky, Soumith Chintala, and L√©on Bottou.
Wasserstein generative adversarial networks. In Proc.
Int. Conf. Mach. Learn., 2017. 3

[50] Diederik P Kingma and Max Welling. Auto-encoding
variational bayes. In Proc. Int. Conf. Learn. Represent.,
2014. 3

[51] Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. In Proc. Int. Conf.
Mach. Learn., 2014. 3

[52] Augustus Odena, Christopher Olah, and Jonathon
Shlens. Conditional image synthesis with auxiliary
classiÔ¨Åer GANs.
In Proc. Int. Conf. Mach. Learn.,
2017. 4

[53] Takeru Miyato and Masanori Koyama. cGANs with
In Proc. Int. Conf. Learn.

projection discriminator.
Represent., 2018. 4

[54] Scott Reed, Zeynep Akata, Honglak Lee, and Bernt
Schiele. Learning deep representations of Ô¨Åne-grained
visual descriptions. In Proc. IEEE Conf. Comput. Vis.
Pattern Recog., 2016. 5

2178

