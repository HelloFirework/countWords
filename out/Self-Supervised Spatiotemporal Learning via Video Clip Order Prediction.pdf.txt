Self-supervised Spatiotemporal Learning via Video Clip Order Prediction

Dejing Xu1

Jun Xiao1 Zhou Zhao1
1Zhejiang University

Jian Shao1 Di Xie2 Yueting Zhuang1

2Hikvision Research Institute

{xudejing,junx,zhaozhou,jshao,yzhuang}@zju.edu.cn; xiedi@hikvision.com

Abstract

We propose a self-supervised spatiotemporal

learn-
ing technique which leverages the chronological order of
videos. Our method can learn the spatiotemporal represen-
tation of the video by predicting the order of shufﬂed clips
from the video. The category of the video is not required,
which gives our technique the potential to take advantage
of inﬁnite unannotated videos. There exist related works
which use frames, while compared to frames, clips are more
consistent with the video dynamics. Clips can help to re-
duce the uncertainty of orders and are more appropriate to
learn a video representation. The 3D convolutional neu-
ral networks are utilized to extract features for clips, and
these features are processed to predict the actual order. The
learned representations are evaluated via nearest neighbor
retrieval experiments. We also use the learned networks
as the pre-trained models and ﬁnetune them on the action
recognition task. Three types of 3D convolutional neural
networks are tested in experiments, and we gain large im-
provements compared to existing self-supervised methods.

1. Introduction

3D convolutional neural networks (CNNs) have been
explored for the action recognition task in many previ-
ous works [38, 2, 30]. While many high-level tasks have
been proposed such as captioning [3] and question answer-
ing [43], action recognition is always signiﬁcant for its
foundation status. However, compared to the progress made
by 2D CNNs over images, the improvements of 3D CNNs
over videos are much slower. Until recently, the 2D CNNs
that take both the RGB and ﬂow streams [33] as inputs
still behave competitively with 3D CNNs in action recog-
nition. The primary reason is that most of the existing
video datasets are relatively small-scale which may not be
able to optimize the immense number of parameters in 3D
CNNs [13]. ImageNet [5] plays an important role in vari-
ous tasks in the image domain, but there is a lack of sim-

Figure 1. Illustration of the necessity to use clips. The top three
rows show examples of frame-based order prediction. For the
shufﬂed frames, both order 1 and order 2 has the same correctness
since we cannot tell the direction of the gymnast in the balance
beam. However, for the clip-based order prediction in the bottom
rows, the judgment is simpler. The dynamics in each clip will help
to reveal the correct order.

ilar dataset in the video domain. In [13], several success-
ful model architectures in image classiﬁcation are extended
and trained on Kinetics [2] video dataset. The authors con-
clude that 3D CNNs and Kinetics may have the potential to
contribute to signiﬁcant progress in ﬁelds related to various
video tasks as the 2D CNNs and ImageNet do.

Though nowadays the large-scale video datasets begin
to appear [12, 2], annotating new video datasets are always
required to tackle the problem in new domains. It needs a
wealth of resources and meticulous design to annotate such
one. Therefore it is valuable if we can leverage the unla-
beled videos to facilitate learning. Self-supervised learning
is one kind of technique where the supervisory signal can be
obtained easily from the data itself. Researches have been
done in this area, in which both the images and videos are
exploited. For image data, there exist self-supervised tasks
such as predicting relative positions of image patches [6],
solve jigsaw puzzles [27], image inpainting [29] and image
color channel prediction [21]. Since the particular prop-

10334

?Xshuffled framesorder 2shuffled clipsorder 1order 1√order 2erty of the video is temporal information, recent works also
attempt to leverage the temporal relations among frames,
such as order veriﬁcation [26, 9] and order prediction [22]
of frames.

The existing self-supervised works that utilize the video
have the framework as follows: ﬁrst use 2D CNNs to extract
features from the frames, then concatenate these features
and predict the veriﬁcation result or the actual order of the
input frames. The whole framework is trained end-to-end.
After the training, the learned 2D CNNs can be used as an
image feature extractor or ﬁnetuned to other tasks such as
image classiﬁcation and detection. Compared to order ver-
iﬁcation [26, 9], order prediction [22] contains much richer
supervisory signals and shows better performance in several
validation experiments.

However, the order is not uniquely determined when re-
ferring to frames merely. As Figure 1 shows, given the shuf-
ﬂed frames, it is hard to tell the correct order of frames since
both directions of the gymnast in the balance beam seems
possible. To mitigate the defect of this, [22] groups both the
forward and backward orders as the same class. It is a com-
promise under the circumstances that only frames and 2D
CNNs are used. In contrast, we propose to directly use clips
and 3D CNNs to make the task more well-deﬁned. From the
ﬁgure, we can get that if the shufﬂed clips are provided, the
order will be more speciﬁc because of the inner dynamics
contained in each clip. Besides, the 3D CNNs are always
believed to be hard to optimize due to the lack of labeled
videos [13]. With the assistance of the clip order prediction
task, the 3D CNNs can leverage numerous videos without
any labels. The 3D CNNs thus can be easily pre-trained
to adapt to different video distributions in new application
domains, which is a prerequisite to gain good performance.
In this paper, we integrate 3D CNNs into the clip order
prediction task. First several ﬁxed-length clips are sampled
from the video and shufﬂed randomly, then 3D CNNs are
used to extract features for these clips, and ﬁnally, a simple
neural network is employed to predict the actual order of the
shufﬂed clips. The learned 3D CNNs can be either used as a
clip feature extractor or a pre-trained model to be ﬁnetuned
on other tasks. The main contributions of the paper can be
summarized as follows:

• We propose to utilize 3D CNNs and video clips to do
order prediction task, which is more consistent with
the video dynamics;

• C3D, R3D, and R(2+1)D networks are tested to prove
that the proposed self-supervised learning framework
is available widely;

• We validate the learned representations via the nearest
neighbor retrieval experiments, and also ﬁnetune the
learned 3D CNNs on action recognition, both experi-
ments show promising results;

The rest of the paper is organized as follows. We ﬁrst re-
view related works in Section 2, then the details of the pro-
posed method are explained in Section 3. In Section 4, the
implementation and results of the experiments are provided
and analyzed. Finally, we conclude our works in Section 5.

2. Related Work

In this section, we ﬁrst introduce the recent progress in
action recognition, then we discuss recent works on self-
supervised representation learning.

Action Recognition Action recognition is one of the
classic problems in computer vision area.
The basic
pipeline to tackle the task is ﬁrst extracting features then
doing classiﬁcation. From traditional hand-crafted fea-
tures [32, 18, 4, 7, 31, 41] to deep neural networks derived
features [44, 11, 8, 35, 15, 23], the improvements are obvi-
ous.

Lots of researches relating to applying 2D CNNs on
videos are proposed since the breakthrough on image clas-
siﬁcation made by AlexNet [19]. Many of them extract
features for frames sampled from the video and fuse these
features as a representation of the video. In [33], the in-
put video is decomposed into the spatial stream and opti-
cal ﬂow stream. Each stream are processed by a deep 2D
CNNs and the prediction is made by late fusion. [17] pro-
poses three kinds of fusion methods to integrate the tempo-
ral information of the video. It also implements multireso-
lution by splitting the inputs frames into context stream and
fovea stream. Both streams are processed using 2D CNNs.
3D CNNs is a natural extension of 2D CNNs over tempo-
ral data such as videos. [38] proposes the C3D architecture
where 3D convolution kernels are stacked followed by fully
connected layers. In [2], the successful 2D CNNs trained
on ImageNet are converted into 3D CNNs via inﬂating all
the ﬁlters and pooling kernels. The proposed I3D model
is based on Inception-v1 [37] and take both RGB and ﬂow
as inputs. The ResNet [14] architecture is also extended
in [30, 39, 13] by adapting the 2D convolution kernels to 3D
ones. [30] designs three types of bottleneck building blocks
and interleaves these blocks to form the P3D ResNet. The
decomposition of 3D convolution to 2D spatial and 1D tem-
poral convolutions are adopted in both [30, 39].
In [13],
they focus on the training of very deep 3D CNNs from
scratch and indicate that deeper 3D CNNs trained on large
datasets can be more effective.

Self-Supervised Representation Learning With the
availability of large-scale data and abundant computing
power, deep neural networks show promising results in
computer vision tasks such as image classiﬁcation and
video recognition. CNNs are one of the critical factors to
gain such improvements. It learns hierarchical representa-
tions of the input data which can be used in other related
tasks directly or after ﬁnetuning. Though there exists a large

10335

Figure 2. Overview of Clip Order Prediction Framework. (a) Sample and Shufﬂe: Sample non-overlapping clips and shufﬂe them to a
random order. (b) Feature Extraction: Use the 3D ConvNets to extract the feature of all clips. The 3D ConvNets is not pre-trained in any
datasets. (c) Order Prediction: The extracted features are pairwise concatenated, and fully connected layers are placed on top to predict the
actual order. The dashed lines mean that the corresponding weights are shared among clips. The framework can be trained end-to-end, and
the 3D ConvNets can be used as a video feature extractor or pre-trained weights after training.

amount of data, it takes great efforts to annotate such mas-
sive data, which is necessary to enable the supervised train-
ing of the CNNs. Self-supervised representation learning is
one type of techniques that learn representations by solv-
ing a surrogate task, where the supervisory signals can be
obtained for free.

There exist many works that leverage the unlabeled im-
ages. [6] proposes to learn image representation by predict-
ing the relative positions between two image patches. The
patches are sampled from the same image in eight spatial ar-
rangements. In [27], nine tiles are extracted from the image
and shufﬂed according to a predeﬁned permutation set to
make jigsaw puzzles. The permutation set is determined via
a greedy algorithm based on Hamming distance between all
possible permutations. [29] use an encoder-decoder archi-
tecture to tackle the image inpainting task, while [21] focus
on image colorization, where the color components of an
image are predicted given its intensity.

Videos are also utilized because of the temporal coher-
ence and dynamics they have. [42] exploits different self-
supervised approaches to learn representations invariant to
inter-instance and intra-instance variations among object
patches. The object patches are extracted from unlabeled
videos using motion cues. [10] use the ranking machines to
capture the evolution of appearances among frames, and the
learned functional parameters can be used as the video rep-
resentation. In [26, 9], the chronological order of frames in
the video are exploited, and the method is required to tell
whether the frame sequence is ordered or not. [22] propose
another related task, in which the actual order of input frame
sequences should be predicted. The task is formulated as

a multi-class classiﬁcation problem, and both the forward
and backward orders are grouped into the same class. Since
the number of possible permutations or orders are exploded
when patches or frames are increased, the permutations are
always predeﬁned as we mentioned before [27]. While in
[1], a reinforcement learning algorithm is used to propose
permutations for the 2D CNNs training.

Though the above works make use of videos for self-
supervised representation learning, the actual inputs are
frames. As a result, the learned CNNs are only capable of
extract features for still images. We extend the order predic-
tion task proposed in [22] from frames to clips, which can
help to leverage the strength of 3D CNNs and inner dynam-
ics of clips. The details of our method are explained in the
next section.

3. Clip Order Prediction

In this section, we will ﬁrst give a brief overview of the
proposed method, then each part of the method will be clar-
iﬁed in detail. Figure 2 presents the overall framework,
which is composed of mainly three procedures. In sample
and shufﬂe, several clips are uniformly sampled and shuf-
ﬂed; in feature extraction, 3D CNNs are used to extract fea-
tures for the clips, and all 3D CNNs used here shared same
weights; in order prediction, we resolve the task via clas-
siﬁcation as in [22]. The extracted features are pairwise
concatenated and forwarded through two linear layers, after
which a softmax layer is applied to output the probability
distribution over the possible orders.

In order to make the following descriptions more clear,

10336

(a) Sample and Shuffle(b) Feature Extraction(c) Order Prediction………3211323D ConvNets3D ConvNets3D ConvNets1 2 31 3 22 1 32 3 13 1 23 2 1𝑓23𝑓21𝑓31𝑓2𝑓3𝑓1…we ﬁrst introduce several deﬁnitions. A clip is made up
of continuous frames sampled from the video with the size
c×l×h×w, where c is the number of frame channels, l is the
number of frames, h and w indicates the height and width
of frames. A 3D convolution kernel has the size t × d × d,
where t is the temporal depth and both d are spatial size.
We deﬁne a tuple of ordered clips as C = hc1, c2, . . . , cni,
and the features extracted by 3D CNNs are represented as
F = hf1, f2, . . . , fni. The subscript here indicates the
chronological order.

3.1. Sample and Shufﬂe

For N clips, there exist N ! possible orders. The num-
ber of orders overgrows with the increase in the number of
clips. For example 7! = 5040, which already makes the
classiﬁcation task very hard. Previous works [27, 1] select
several particular orders from all possible orders either de-
terminately or adaptively. Since clip order prediction is just
a proxy task and we focus on the learning of 3D CNNs, the
task should be solvable. Otherwise, if the whole task is too
hard to solve, almost nothing can be learned. We restrict the
number of clips between 2 to 5, which makes the number of
order classes less than 120.

The clips are sampled uniformly from the video, with
an interval of m frames. The clips are forced to be non-
overlapping, which can avoid the situation that the whole
framework tackles the task by comparing lower character-
istics such as texture and color. For an extreme case, if the
clips are overlapped by 1 frame, a simple comparison of
frames pixels can solve the task.

After the clips are sampled, they are shufﬂed to form the
input data while the actual order is served as the target, as
shown in Figure 2 (a). The shufﬂe step should be random,
and no particular permutations are preferred. During the
training step, the number of generated samples belonging
to each order class is roughly the same.

3.2. Feature Extraction

Once the shufﬂed clips are prepared, 3D CNNs are used
to extract features for each clip. The same 3D CNNs are
used for all clips in one tuple, as Figure 2 (b) shows. We
choose three different 3D CNNs as the feature extractor,
which are C3D [38], R3D and R(2+1)D [39] networks. The
structure of distinct convolution blocks are presented in Fig-
ure 3. We will discuss the architecture of each network con-
cretely in the following.

C3D [38] The model is a natural extension from 2D
CNNs over videos. 3D CNNs is well-suited for spatiotem-
poral learning since it can model the temporal information
and dynamics of the video [15, 38]. C3D network includes
8 convolution layers stacked one by one, with 5 pooling lay-
ers interleaved, and followed by two fully connected layers
terminally. The size of all convolution kernels are 3 × 3 × 3,

Figure 3. Three types of 3D Conv Blocks. (a) C3D Conv Blocks:
the classic 3D convolution kernel with size t × d × d, which are
stacked to form the C3D network. (a) R3D Conv Blocks: classic
3D convolution kernels with a shortcut connection. (c) R(2+1)D
Conv Blocks:
the 3D kernel are decomposed into a spatial 2D
kernel (1 × d × d) and a temporal 1D kernel (t × 1 × 1). Batch
normalization and ReLU layers are omitted for clarity.

which is the best practice gained from their experiments.

R3D [39] Residual learning principle [14] is a milestone
for the architecture design of 2D CNNs. ResNet pushes the
performance of many image-related tasks such as classiﬁ-
cation, detection, and segmentation to state-of-the-art. R3D
network is the 3D CNNs with residual connections. The
operations of the basic convolution block are as follows:

xo = F2(F1(xi)) + H(xi)

(1)

where xi and xo stands for the input and output of the
block, F is the 3D convolution operation, and H is a fucn-
tion to scale the xi to the size of xo when necessary. The
convolution block consists of two 3D kernels, with batch
normalization and ReLU layers appended. There are 5 con-
volution layers in total, the speciﬁcation can be refered in
Table 1 of [39].

R(2+1)D [39] The 3D convolution kernel is convolved
over a volume both spatially and temporally. The procedure
can be refactored by ﬁrst applying spatial convolution then
temporal convolution [30, 36]. The concrete operations in
the convolution block are as follows:

xm = T1(S1(xi))
xo = T2(S2(xm)) + H(xi)

(2)

where xi, xm and xo correspond to the input, middle and
output of the block, S stands for spatial convolution, T
stands for temoporal convolution, and H is the same func-
tion as mentioned before. The overall architecture is the
same as R3D, except that more ReLU layers are inserted
in the block, which means the number of nonlinearties are

10337

ddtddtddt(a) C3D Conv Blocks(b) R3D Conv Blocks(c) R(2+1)D Conv Blocksdd1t11dd1t11doubled while the number of parameters are almost the
same. The R(2+1)D network also leads to state-of-art re-
sults on four action recognition benchmarks.

Both R3D and R(2+1)D networks use a global spa-
tiotemporal pooling layer to aggregate the activations after
the convolution layers. The obtained vector is treated as the
extracted feature for the input clip. To have a fair compari-
son, we modify the original C3D implementation to follow-
ing the same design. The batch normalization is also added
after each convolution layer.

3.3. Order Prediction

The order prediction is formulated as a classiﬁcation
task. The input is a tuple of clip features, and the output
is a probability distribution over different orders. We use
a simple multi-layer perceptron, and the extracted features
are pairwise concatenated, which is proved to be better for
both order prediction and the learning of underlying feature
extractors [22]. Given the extracted features, the operations
are as follows:

hk = gθ(W1(fikfj) + b1)
a = W2kN

k=1hk + b2
exp(ai)
j=1 exp(aj))

PC

pi =

(3)

where k means the concatenation of vectors, gθ is a nonlin-
ear function, W and b are the parameters of linear transfor-
mation, hk captures the relationship between fi and fj , a
is the logits and pi is the probability that the order belongs
to class i.

Assume a tuple contains 3 clips, after shufﬂing, we get
C = hc2, c3, c1i and the corresponding extracted features
F = hf2, f3, f1i. As Figure 2 (c) shows, the features
are ﬁrst pairwise concatenated as hf23, f21, f31i and then
transformed to form a tuple of three vectors which capture
the relationship between each clip. These vectors are con-
catenated again and a fully-connected layer with softmax
are applied over to output the ﬁnal prediction. The target
classes are permutations of h1, 2, 3i, one of which is the ac-
tual order h2, 3, 1i.

The correctness of the prediction is measured using

cross-entropy as follows,

L = −

C

X

i=1

yilog(pi)

(4)

where yi and pi are the probability of the sample belong-
ing to order class i in groundtruth and prediction, and C
is number of all possible orders. The loss L is then back-
propagated to optimize the whole framework. When the
framework is trained to predict the order of clips, the 3D
CNNs are trained to extract the meaningful features of clips
meanwhile.

4. Experiments

In this section, we will ﬁrst describe the concrete set-
tings of the clip order prediction experiments and their re-
sults, then the learned 3D CNNs are evaluated both quan-
titatively and qualitatively via nearest neighbor retrieval in
Section 4.1 and action recognition in Section 4.2.

Though our self-supervised method is designed to learn
from unlabeled videos, we choose to experiment based
on UCF101 [34] since its diverse enough and well orga-
nized. Besides, it has many reported results to compare.
HMDB51 [20] are also utilized to test the generalizability
of the proposed method. The details of both datasets are
described in the following.

UCF101 is an action recognition dataset of realistic ac-
tion videos, collected from YouTube, having 101 action
categories. The action categories can be divided into ﬁve
types: 1) Human-Object Interaction 2) Body-Motion Only
3) Human-Human Interaction 4) Playing Musical Instru-
ments 5) Sports. With 13,320 videos from 101 action cat-
egories, UCF101 has large diversity regarding actions and
with the presence of large variations in camera motion, ob-
ject appearance and pose, object scale, viewpoint, cluttered
background, illumination conditions, etc.

HMDB51 is collected from various sources, mostly from
movies, and a small proportion from public databases such
as the Prelinger archive, YouTube and Google videos. The
actions categories can be grouped in ﬁve types: 1) Gen-
eral facial actions 2) Facial actions with object manipula-
tion 3) General body movements 4) Body movements with
object interaction 5) Body movements for human interac-
tion. The dataset contains 6,849 clips divided into 51 action
categories, each containing a minimum of 101 clips.

We use PyTorch [28] to implement the whole frame-
work. Since the C3D network contains 8 convolution lay-
ers, we implement the R3D network with no repetitions in
conv{2-5} x, which results in 9 convolution layers in total.
C3D network is also modiﬁed by replacing the two fully
connected layers with a global spatiotemporal pooling layer
as used in the R3D network. R(2+1)D network follows
the same architecture as the R3D network, with only 3D
kernels decomposed. Dropout layers are applied between
fully-connected layers with p = 0.5. All nonlinearities are
ReLU.

The split 1 of UCF101 is used to train and test our self-
supervised learning method. We choose clip length as 16
frames since most 3D CNNs [30, 39, 13, 38] requires a
16-frames clip as input. The interval is set to be 8 frames,
which is required to avoid trivial solutions of the task. For
tuple length, 3 clips per tuple are reasonable since 2 clips or-
der prediction is more like an order veriﬁcation, while more
than 3 tuples become a relatively hard task. This decision
is also based on the conclusion from [27] that a good self-
supervised task is neither simple nor ambiguous. On-the-

10338

3D CNNs C3D R3D R(2+1)D

Accuracy

68.5

68.4

64.2

Table 1. Clip order prediction results on UCF101. C3D, R3D
and R(2+1)D networks are trained with clip order prediction
framework separately.

ﬂy data augmentation is used to prepare the input data. We
randomly split 800 videos from the training set to do valida-
tion during training. The input video clips are ﬁrst resized
to 128 × 171, then randomly cropped to 112 × 112 during
training. During validation or testing, the clip is cropped in
the center.

To optimize the framework, we use mini-batch stochas-
tic gradient descent. Memory consumption is always a
trouble when training neural networks with large batches,
especially for 3D CNNs. Recently [25] shows that small
mini-batch sizes provide more up-to-date gradient calcula-
tions and yields more stable and reliable training. Thus we
choose 8 tuples per batch. The learning rate is set to 0.001,
while the momentum is 0.9 and weight decay is 0.0005. The
training process lasts for 300 epochs, and the model with the
lowest validation loss is saved to be the best model.

As Table 1 shows, with the clip as 16 frames, the interval
as 8 frames and the tuple as 3 clips, the clip order predic-
tion task can reach an accuracy higher than 64% for all three
3D CNNs. Considering that the accuracy of random guess-
ing for the task is 16.7%, the framework indeed learns to
analyze the content of clips and reason the order out. We
also test the 4 clips per tuple with the C3D network. During
the training phase, the accuracy increases very slowly but
continuously, while a larger learning rate causes unstable
training at present. As a result, for the following validation
experiments, we use the ones trained under 3 clips per tuple
setting if not particularly speciﬁed.

4.1. Nearest Neighbor Retrieval

As mentioned before, to accomplish the clip order pre-
diction task, the framework needs to analyze and understand
the content of clips. As the feature extractor, the 3D CNNs
are trained together with the whole framework. To evaluate
the learned representations, we choose the nearest neighbor
retrieval method since it is also used in [1, 26].

We basically follow the experiment settings in [1]. The
split 1 of UCF101 is used for validation. In their experi-
ment, 10 frames are extracted per video, and the pool5 layer
of CaffeNet [16] is selected as the representation. In our
experiment, we extract 10 clips per video likewise. Since
the pool5 representation of CaffeNet has the dimension of
256×6×6, we apply a max pooling operation instead of the
original global spatiotemporal pooling in three 3D CNNs to
get a 512 × 2 × 3 × 3 spatiotemporal representation, which

is the same size as the other one. The clips extracted from
the test set are used to query the clips from the training set.
The cosine distance of representations between the query
clip and all clips in the training set are computed. When the
class of a test clip appears in the classes of k nearest training
clips, it is considered to be correctly predicted.

We show the accuracies for k = 1, 5, 10, 20, 50 and com-
pare with the other self-supervised methods on UCF101 in
Table 2. The top row in the table are those which use 2D
CNNs, speciﬁcally, CaffeNet as the feature extractor, and
the bottom shows 3D CNNs trained by our self-supervised
method. The results of random initialized 3D CNNs are also
presented. As we can see, our self-supervised trained 3D
CNNs perform better than random initialized ones and self-
supervised trained 2D CNNs especially when k becomes
larger. B¨uchler et al. [1] presents a competitive result when
k is less than 10. Their method focuses on adjusting the
permutation set based on network states, which we can ex-
pect to apply in our method and get a promotion as well
when we use more clips per tuple. We also test the trained
3D CNNs on split 1 of HMDB51, the results are presented

Methods

Top1 Top5 Top10 Top20 Top50

Jigsaw [27]

OPN [22]

B¨uchler et al. [1]

C3D (random)

C3D

R3D (random)

R3D

19.7

19.9

25.7

16.0

12.5

10.5

14.1

R(2+1)D (random)

10.2

R(2+1)D

10.7

28.5

28.7

36.2

22.5

29.0

17.2

30.3

17.3

25.9

33.5

34.0

42.2

26.7

39.0

21.5

40.0

21.9

35.4

40.0

40.6

49.2

31.4

50.6

27.0

51.1

27.7

47.3

49.4

51.6

59.5

39.3

66.9

36.7

66.5

38.5

63.9

Table 2. Frame and clip retrieval results on UCF101. The meth-
ods in top row are based on 2D CNNs while 3D CNNs in our
framework are presented in bottom row.

Methods

Top1 Top5 Top10 Top20 Top50

C3D (random)

C3D

R3D (random)

R3D

R(2+1)D (random)

R(2+1)D

7.7

7.4

5.5

7.6

4.6

5.7

12.5

22.6

11.3

22.9

11.1

19.5

17.3

34.4

16.5

34.4

16.3

30.7

24.1

48.5

23.8

48.8

23.9

45.8

37.8

70.1

37.2

68.9

39.3

67.0

Table 3. Clip retrieval results on HMDB51. The 3D CNNs used
here are self-supervised trained on split 1 of UCF101 merely.

10339

Figure 4. Video retrieval samples on UCF101. The ﬁrst column contains query clips from the test split, and the remaining columns
indicate top2 nearest clips retrieved by different trained models from the training split. The class of each video is displayed in bottom.

in Table 3. Since these feature extractors are trained on
UCF101 merely, which means they never see any videos
from HMDB51 theoretically (actually there may be some
duplicated videos between two datasets since we ﬁnd one
in samples). The results are even better which indicates the
generalizability of our trained feature extractors.

Note that for k = 1, the self-supervised trained 3D
CNNs do not show noticeable improvement. We ﬁnd that
the top1 accuracy is sensitive to the clip sample rate. To fur-
ther evaluate the learned representation, we adopt the same
experiment in video level. The video representation is the
average of the 10 extracted clip features. As shown in Fig-
ure 5, compared to randomly initialized networks, the accu-
racy of all trained 3D CNNs are higher for all k consistently
in both datasets, among which R3D and C3D networks are
slightly better than R(2+1)D network.

To have an intuitive understanding of the video retrieval
results, we also visualize the top2 retrieved videos from
UCF101 in Figure 4. The videos are represented by their

Figure 5. Video retrieval results. The clip features are averaged
to form the video representation.

central frames, and the actual classes are displayed un-
der the ﬁgures. The leftmost columns are videos used for
the query, and the remaining columns show top2 retrieved
videos by different feature extractors. As we can see, the
self-supervised trained 3D CNNs can ﬁnd videos with sim-
ilar meanings. For query video of basketball, R(2+1)D net-
work also ﬁnds volleyball spiking video which both is also
sports and contains balls. For balance beam video, R3D net-
work also retrieves uneven bars video, which is also gym-
nastics.

To test the generalizability of our trained feature ex-
tractors, we also adopt cross-dataset video retrieval be-
tween UCF101 and HMDB51 using the trained R3D net-
work. Since the two datasets have different classes, we can-
not evaluate the video retrieval performance quantitatively.
Several results are showed in Figure 6 to understand the sta-
tus qualitatively. We use one dataset for query and the other
dataset for retrieval. Though the classes are not the same,
we can see that the retrieved videos have the classes relating
to the query one more or less.

From the above experimental results, we can conclude
that the clip order prediction task indeed encourages the 3D
CNNs to learn more general spatiotemporal representations
for clips and videos.

4.2. Action Recognition

In addition to acting as feature extractors, we can also
take the trained 3D CNNs as initializations and ﬁnetune the
networks on other supervised tasks. Here we ﬁnetune the
network on action recognition task on both UCF101 and
HMDB51.

The three networks all output a 512-dimension vector af-
ter the global spatiotemporal pooling layer, and we append
a fully-connected layer with softmax on top of it as in [39].
Only the fully-connected layers are randomly initialized,
other layers are initialized from the self-supervised training
one correspondingly. The hyperparameters and data pre-

10340

QueryC3D (random)R(2+1)DC3DR3DBasketballHorse RaceRock Climbing IndoorBasketballSurfingBasketballBasketballBasketballVolleyball SpikingPlaying CelloRope ClimbingBikingPlaying ViolinPlaying CelloPlaying CelloApply Eye MakeupArcheryPlaying CelloBalance BeamPunchBalance BeamFront CrawlBalance BeamUneven BarsBalance BeamBalance BeamBalance Beam0102030405060708001020304050Accuracy (%)# of videosC3D (random)C3DR3D (random)R3DR(2+1)D (random)R(2+1)D0102030405060708001020304050Accuracy (%)# of videos(a) UCF101(b) HMDB51Method

UCF101 HMDB51

Shufﬂe&Learn [26]

VGAN [40]

Luo et al. [24]

OPN [22]

Jigsaw [27]

B¨uchler et al. [1]

ImageNet pre-trained

C3D (random)

C3D

R3D (random)

R3D

R(2+1)D (random)

R(2+1)D

Kinetics pre-trained

50.2

52.1

53.0

56.3

51.5

58.6

67.1

61.6

65.6

54.4

64.9

56.2

72.4

96.8

18.1

-

-

22.1

22.5

25.0

28.5

23.2

28.4

21.5

29.5

22.0

30.9

74.5

Table 4. Action recognition results on UCF101 and HMDB51.
The top row is frame-based methods and the bottom row is clip-
based methods.

5. Conlusions

In this paper, we introduce the clip order prediction task
to leverage the inner dynamics of the video better. The task
is very suitable for 3D CNNs which can model the spa-
tiotemporal information. We experiment with three types
of 3D CNNs and evaluate them using nearest neighbor re-
trieval and ﬁnetuning on action recognition. From the ex-
perimental results, we can get that the clip order prediction
task can encourage the 3D CNNs to learn a general spa-
tiotemporal representation as well as a good initialization.
We hope that our work will inspire more research inter-
ests on self-supervised learning of 3D CNNs. While our
study shows promising results, the ﬁnetuning from super-
vised pre-training on larger datasets such as Kinetics still
act as the best. Future direction will be the combination
of our method with more unlabeled videos and search for
diverse task settings.

6. Acknowledgements

National Key Research and Development Program
of China (2017YFB0203001), Zhejiang Natural Science
Foundation (LR19F020002, LZ17F020001), National Nat-
ural Science Foundation of China (61572431), Chi-
nese Knowledge Center for Engineering Sciences and
Technology, Key R&D Program of Zhejiang Province
(2018C03055), the Fundamental Research Funds for the
Central Universities and Joint Research Program of ZJU &
Hikvision Research Institute.

10341

Figure 6. Cross-dataset retrieval samples. The dataset names in
left and top indicates the query and retrieval sources correspond-
ingly. The class of each video is displayed in bottom.

processing steps are the same as before. All networks are
ﬁnetuned for another 150 epochs. To get the action recog-
nition result for a video, we follow the method from [39].
10 clips are sampled from the video to get clip predictions,
which are then averaged to obtain the video prediction.

We report the average classiﬁcation accuracy over 3
splits and compare with other ﬁnetuning results from exist-
ing self-supervised methods in Table 4. The training from
randomly initialized 2D CNNs and 3D CNNs are reported
for reference. We also show the accuracies from ﬁnetuned
models which are pre-trained on large supervised datasets
such as ImageNet and Kinetics. As we can see, the 3D
CNNs trained from scratch already beats several 2D CNNs
after ﬁnetuning, which indicates the capability of 3D CNNs
over videos. The ﬁnetuned C3D network gives 4.0% and
5.2% improvements on UCF101 and HMDB51 compared
to the randomly initialized one. R3D and R(2+1)D net-
works gain lower accuracy if only trained from scratch on
both datasets. However, with the initialization from our
self-supervised training, the R3D and R(2+1)D networks
achieve even better accuracies than the C3D network. The
best-performed R(2+1)D network gets 16.2% and 8.9% pro-
motions on both datasets than random initialization. The
model also beats the state-of-the-art from B¨uchler et al. [1]
by 13.8% on UCF101 and 5.9% on HMDB51.

Since our initialization networks are only trained on split
1 of UCF101, the same improvements gained by all ﬁne-
tuned networks on 3 splits of the UCF101 and HMDB51
datasets prove that our self-supervised learning technique is
widely applicable and has good generalizability.

页脚HMDB51UCF101HMDB51UCF101ride_bikeDivingBikingHorse RidingHammer ThrowGolf SwingGolf Swingkick_ballkick_balljumpjumpthrowpickhitSoccer PenaltyBaseball PitchQueryReferences

[1] Uta Buchler, Biagio Brattoli, and Bjorn Ommer. Improving
spatiotemporal self-supervision by deep reinforcement learn-
ing.
In Proceedings of the European Conference on Com-
puter Vision, pages 770–786, 2018. 3, 4, 6, 8

[2] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6299–6308, 2017. 1, 2

[3] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian
Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and
channel-wise attention in convolutional networks for image
captioning. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 5659–5667,
2017. 1

[4] Navneet Dalal, Bill Triggs, and Cordelia Schmid. Human de-
tection using oriented histograms of ﬂow and appearance. In
Proceedings of the European conference on computer vision,
pages 428–441, 2006. 2

[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE conference on Com-
puter Vision and Pattern Recognition, pages 248–255, 2009.
1

[6] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-
vised visual representation learning by context prediction. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1422–1430, 2015. 1, 3

[7] Piotr Doll´ar, Vincent Rabaud, Garrison Cottrell, and Serge
Belongie. Behavior recognition via sparse spatio-temporal
features. In IEEE International Workshop on Visual Surveil-
lance and Performance Evaluation of Tracking and Surveil-
lance, pages 65–72, 2005. 2

[8] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko,
and Trevor Darrell. Long-term recurrent convolutional net-
works for visual recognition and description.
In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 2625–2634, 2015. 2

[9] Basura Fernando, Hakan Bilen, Efstratios Gavves, and
Stephen Gould. Self-supervised video representation learn-
ing with odd-one-out networks.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3636–3645, 2017. 2, 3

[10] Basura Fernando, Efstratios Gavves, Jos´e Oramas, Amir
Ghodrati, and Tinne Tuytelaars. Rank pooling for action
recognition. IEEE transactions on pattern analysis and ma-
chine intelligence, 39(4):773–787, 2017. 3

[11] Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic,
and Bryan Russell. Actionvlad: Learning spatio-temporal
aggregation for action classiﬁcation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 971–980, 2017. 2

[12] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-
ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,
Valentin Haenel,
Ingo Fruend, Peter Yianilos, Moritz
Mueller-Freitag, et al. The something something video

database for learning and evaluating visual common sense.
In Proceedings of the IEEE International Conference on
Computer Vision, volume 2, page 8, 2017. 1

[13] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can
spatiotemporal 3d cnns retrace the history of 2d cnns and im-
agenet? In Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition, pages 6546–6555, 2018. 1,
2, 5

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 2, 4

[15] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-
tional neural networks for human action recognition. IEEE
transactions on pattern analysis and machine intelligence,
35(1):221–231, 2013. 2, 4

[16] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional architecture for fast
feature embedding. In Proceedings of the 22nd ACM inter-
national conference on Multimedia, pages 675–678, 2014.
6

[17] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video
classiﬁcation with convolutional neural networks.
In Pro-
ceedings of the IEEE conference on Computer Vision and
Pattern Recognition, pages 1725–1732, 2014. 2

[18] Alexander Klaser, Marcin Marszałek, and Cordelia Schmid.
A spatio-temporal descriptor based on 3d-gradients. In 19th
British Machine Vision Conference, 2008. 2

[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, pages 1097–1105, 2012. 2

[20] Hildegard Kuehne, Hueihan Jhuang, Est´ıbaliz Garrote,
Tomaso Poggio, and Thomas Serre. Hmdb: a large video
database for human motion recognition.
In Proceedings
of the IEEE International Conference on Computer Vision,
pages 2556–2563, 2011. 5

[21] Gustav

Larsson, Michael Maire,

and Gregory
Shakhnarovich. Colorization as a proxy task for visual
understanding.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
6874–6883, 2017. 1, 3

[22] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-
Hsuan Yang. Unsupervised representation learning by sort-
ing sequences.
In Proceedings of the IEEE International
Conference on Computer Vision, pages 667–676, 2017. 2,
3, 5, 6, 8

[23] An-An Liu, Yu-Ting Su, Wei-Zhi Nie, and Mohan Kankan-
halli. Hierarchical clustering multi-task learning for joint hu-
man action grouping and recognition. IEEE transactions on
pattern analysis and machine intelligence, 39(1):102–114,
2017. 2

[24] Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, and
Li Fei-Fei. Unsupervised learning of long-term motion dy-
namics for videos. In Proceedings of the IEEE Conference

10342

3d convolutional networks. In Proceedings of the IEEE inter-
national conference on computer vision, pages 4489–4497,
2015. 1, 2, 4, 5

[39] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 6450–6459, 2018. 2, 4, 5, 7, 8

[40] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
In Advances In
Generating videos with scene dynamics.
Neural Information Processing Systems, pages 613–621,
2016. 8

[41] Heng Wang and Cordelia Schmid. Action recognition with
improved trajectories.
In Proceedings of the IEEE inter-
national conference on computer vision, pages 3551–3558,
2013. 2

[42] Xiaolong Wang, Kaiming He, and Abhinav Gupta. Tran-
sitive invariance for self-supervised visual representation
learning. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 1329–1338, 2017. 3

[43] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, and Yueting Zhuang. Video question answer-
ing via gradually reﬁned attention over appearance and mo-
tion. In Proceedings of the 25th ACM international confer-
ence on Multimedia, 2017. 1

[44] Zhongwen Xu, Yi Yang, and Alex G Hauptmann. A discrim-
inative cnn video representation for event detection. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1798–1807, 2015. 2

on Computer Vision and Pattern Recognition, pages 2203–
2212, 2017. 8

[25] Dominic Masters and Carlo Luschi.

batch training for deep neural networks.
arXiv:1804.07612, 2018. 6

Revisiting small
arXiv preprint

[26] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuf-
ﬂe and learn: unsupervised learning using temporal order
veriﬁcation. In Proceedings of the European Conference on
Computer Vision, pages 527–544, 2016. 2, 3, 6, 8

[27] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of
visual representations by solving jigsaw puzzles. In Proceed-
ings of the European Conference on Computer Vision, pages
69–84, 2016. 1, 3, 4, 5, 6, 8

[28] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017. 5

[29] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2536–2544, 2016. 1, 3

[30] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-
temporal representation with pseudo-3d residual networks.
In Proceedings of the IEEE International Conference on
Computer Vision, pages 5533–5541, 2017. 1, 2, 4, 5

[31] Sreemanananth Sadanand and Jason J Corso. Action bank: A
high-level representation of activity in video. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1234–1241, 2012. 2

[32] Paul Scovanner, Saad Ali, and Mubarak Shah. A 3-
dimensional sift descriptor and its application to action
recognition. In Proceedings of the 15th ACM international
conference on Multimedia, pages 357–360, 2007. 2

[33] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos. In Ad-
vances in neural information processing systems, pages 568–
576, 2014. 1, 2

[34] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402, 2012. 5

[35] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-
nov. Unsupervised learning of video representations using
lstms.
In International conference on machine learning,
pages 843–852, 2015. 2

[36] Lin Sun, Kui Jia, Dit-Yan Yeung, and Bertram E Shi. Human
action recognition using factorized spatio-temporal convolu-
tional networks. In Proceedings of the IEEE International
Conference on Computer Vision, pages 4597–4605, 2015. 4
[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1–9, 2015.
2

[38] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with

10343

