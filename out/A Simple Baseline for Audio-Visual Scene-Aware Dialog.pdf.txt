A Simple Baseline for Audio-Visual Scene-Aware Dialog

Idan Schwartz1, Alexander Schwing2, Tamir Hazan1

1Technion

2UIUC

idanschwartz@gmail.com, aschwing@illinois.edu, tamir.hazan@technion.ac.il

Abstract

The recently proposed audio-visual scene-aware dialog
task paves the way to a more data-driven way of learning
virtual assistants, smart speakers and car navigation sys-
tems. However, very little is known to date about how to
effectively extract meaningful information from a plethora of
sensors that pound the computational engine of those devices.
Therefore, in this paper, we provide and carefully analyze a
simple baseline for audio-visual scene-aware dialog which
is trained end-to-end. Our method differentiates in a data-
driven manner useful signals from distracting ones using an
attention mechanism. We evaluate the proposed approach on
the recently introduced and challenging audio-visual scene-
aware dataset, and demonstrate the key features that permit
to outperform the current state-of-the-art by more than 20%
on CIDEr.

1. Introduction

We are interacting with a dynamic environment which
constantly stimulates our brain via visual and auditory sig-
nals. Despite the huge amount of different information that
is permanently occupying our nervous system, we are often
easily able to quickly discern important cues from data that
is irrelevant. Telling apart useful information from distract-
ing aspects is also an important ability for virtual assistants,
car navigation systems, or smart speakers. However present
day technology uses a chain of components from speech
recognition and dialog management to sentence generation
and speech synthesis, making it hard to design a holistic and
entirely data-driven approach.

For instance, in computer vision, a tremendous amount of
recent work has focused on image captioning [68, 30, 11, 16,
75, 45, 77, 31, 69, 4, 15, 10], visual question generation [36,
48, 47, 28], visual question answering [5, 19, 59, 54, 44, 73,
74, 76, 57, 58, 49, 50], and very recently visual dialog [13,
14, 27, 46]. While those meticulously engineered algorithms
have shown promising results in their speciﬁc domain, little
is known about the end-to-end performance of an entire
system. This is partly due to the fact that little data is publicly
available to design such an end-to-end algorithm.

Recent work on audio-visual scene aware dialog [2, 25]
partly addresses this shortcoming and proposes a novel

Figure 1: We present 4 different questions and the generated an-
swer. Our attention unit is illustrated as well. Our model samples 4
frames, and attends to each frame separately, along with the ques-
tion and the audio. We observe attention for each frame to differ,
where ﬁrst and fourth frames are widespread, while the second
and third are more speciﬁc. Also, the question attention attends to
relevant words. We also include the audio modality as input to the
attention computation.

dataset. Different from classical datasets like MSCOCO [39],
VQA [5] or Visual Dialog [13], this new dataset contains
short video clips, the corresponding audio stream and a se-
quence of question-answer pairs. While development of an
end-to-end data driven system isn’t feasible just yet due to
the missing speech signal, the new audio-visual scene aware
dialog dataset at least permits to develop a holistic dialog
management and sentence generation approach taking audio
and video signals into account.

In recent work [2, 25], a baseline for a system based on
audio, video and language data was proposed. Compelling
results were achieved, demonstrating accurate question an-
swering. The authors demonstrate that multimodal features
based on I3D-Kinetics (RGB+Flow) [9] reﬁned via a care-
fully designed attention-based mechanism improve the qual-
ity of the generated dialog.

However, since much effort was dedicated to collecting
the dataset, little analysis of such a holistic system was pro-
vided. Moreover, due to tremendous amounts of available
data (certainly a ten-fold increase compared to classical vi-

112548

Question: what color is the rag ?Answer: it appears to be white .MultiModal-Attention:Question:where is the video taking place ? MultiModal-Attention:Answer: the video starts with a manin the kitchen .Question:doeshe speak at all ?Answer: no he does not speak.MultiModal-Attention:Question:do they get up from the chair?MultiModal-Attention:Answer: no , they stay sitting in the chair .sual dialog data) this is by no means trivial. To provide
this missing information and to share some insights with the
community about how and where to improve, in this paper,
we follow the spirit of [26] and demonstrate (1) that simply
using the question as a signal already permits to outperform
the current state-of-the-art; (2) that it is crucial to maintain
spatial features for the video signal (either VGG19 [63] or
I3D-Kinetics [9]). Reducing every video frame into a single
representation drops performance signiﬁcantly; (3) that tem-
porally subsampling the video frames improves the accuracy;
(4) that using attention over all available data (including dif-
ferent frames) is beneﬁcial. To this end we analyze how to
fuse the attended vectors for different data modalities.

Our simple baseline, which consists of three jointly
trained components (data representation extraction, atten-
tion and answer generation) outperforms state-of-the-art by
a large margin of 20% on CIDEr.
Improvements of the
proposed approach are largely due to the aforementioned
four points. Results of generated answers are contrasted to
the current state-of-the-art in Fig. 1. We observe plausible
answers to many questions and attention that focuses on
important parts in both video and text.

2. Related Work

A signiﬁcant amount of research has been conducted re-
garding image captioning, visual question generation, visual
question answering, visual dialog, video data, audio data
and multimodal attention models. We brieﬂy review those
related areas in the following.
Image Captioning: Originally image captioning was for-
mulated as a retrieval problem. The best ﬁtting caption
from a set of considered options was found by matching fea-
tures obtained from the available textual descriptions and the
given image. Importantly, the matching function is typically
learned using a dataset of image-caption pairs. While such a
formulation permits end-to-end training, assessing the ﬁt of
image descriptors to a large pool of captions is computation-
ally expensive. Moreover, it’s likely prohibitive to construct
a database of captions that is sufﬁcient for describing even a
modestly large fraction of plausible images.

To address this challenge, recurrent neural nets (RNNs)
decompose captions into a product space of individual words.
This technique has recently found widespread use for image
captioning because remarkable results have been demon-
strated which are, despite being constructed word by word,
syntactically correct most of the time. For instance, a CNN
to extract image features and a language RNN that shares a
joint embedding layer was trained [45]. Joint training of a
CNN with a language RNN to generate sentences one word
at a time was demonstrated in [75], and subsequently ex-
tended [75] using additional attention parameters which iden-
tify salient objects for caption generation.A bi-directional
RNN was employed along with a structured loss function in a
shared vision-language space [31]. Diversity was considered,

e.g., by Wang et al. [69] and Deshpande et al. [15].

Visual Question Answering: Beyond generating a caption
for an image, a large amount of work has focused on an-
swering a question about a given image. On a plethora of
datasets [43, 54, 5, 19, 81, 29], models with multi-modal
attention [41, 76, 3, 12, 18, 59, 74, 57, 58], deep net archi-
tecture developments [8, 44, 42] and memory nets [73] have
been investigated.

Visual Question Generation: In spirit similar to question
answering is the task of visual question generation, which
is still very much an open-ended topic. For example, Ren
et al. [54] discuss a rule-based method, converting a given
sentence into a corresponding question which has a single
word answer. Mostafazadeh et al. [48] learned a question
generation model with human-authored questions rather than
machine-generated descriptions. Vijayakumar et al. [67]
have shown results for this task as well. Different from the
two aforementioned techniques, Jain et al. [28] argued for
more diverse predictions and use a variational auto-encoder
approach. Li et al. [36] discuss VQA and VQG as dual tasks
and suggest a joint training. They take advantage of the
state-of-the art VQA model by Ben-younes et al. [8] and
report improvements for both VQA and VQG.

Visual Dialog: Visual dialog [13] combines the three afore-
mentioned tasks. Strictly speaking it requires both gener-
ation of questions and corresponding answers. Originally,
visual dialog required to only predict the answer for a given
question, a given image and a provided history of question-
answer pairs. While this resembles the VQA task, different
approaches, e.g., also based on reinforcement learning, have
been proposed recently [35, 14, 27, 46, 72].

Video Data: A variety of tasks like video paragraph cap-
tioning [78], video object segmentation [53], pose esti-
mation [79], video classiﬁcation [32], and action recogni-
tion [62] have used video data for a long time. Probably
most related to our approach are video classiﬁcation and
action recognition since both techniques also extract a repre-
sentation from a video. While the extracted representation is
subsequently used for either classiﬁcation or action recog-
nition, we employ the representation to more accurately
answer a question. Commonly used feature representations
for either video classiﬁcation or action recognition are I3D-
based features by Carreira et al. [9], extracted from an action
recognition dataset. With proper ﬁne-tuning the I3D-based
features proved to be better than the classical approaches,
such as C3D [65] that capture spatiotemporal information
via a 3D CNN. In this work, we assess a na¨ıve feature ex-
tractor based on VGG [63], and demonstrate that for video-
reasoning, careful reduction of the spatial dimension is more
crucial than the type of extracted features used to embed the
video frames. Wang et al. [70] showed that working with
video frame samples, achieves not only efﬁciency, but also
improves performance compared to a conservative dense
temporal representation. Recently, Zhou et al. [80] further

12549

Sec 3.4

V
G
G
s
h

i

r

A

×n

d

ℝ

A

A

V
G
G
1
9

r

V

F× ×n

d

ℝ

V

V

does more than one person 
appear in the video ?

L
S
T
M

r

Q

Sec 3.3

M
u

l
t
i

m
o
d
a

l
 

A

t
t

e
n

t
i

o
n

Fig. 4

a

A

d

ℝ

A

a

V

1

a

V

F

F×d

ℝ

V

a

Q

×n

d

d

Q

Q

Q

ℝ

ℝ

Q: is there a person in the video?  
A: yes there is 
Q: is it a male or female ? 
A: the person is male <eos>

L
S
T
M

r

H

d

ℝ

H

Sec 3.2 - Fig. 3

L
S
T
M

 

A
u
d
-
V
s
 

i

C
o
n
c
a

t

 there  is  only  one  person

y

y

1

2

...

y

5

Ans-Generation 

LSTM 

a

a

y

y

T

T

0

1

...

a

y

T

4

Word 

Embedding 

 <S> there  is  only  one

Figure 2: Overview of our approach for the AVSD task. More details can be found in Sec. 3.

extended those ideas, and suggested to capture relational
temporal relationships between the sampled frames, relying
on the relational-networks concept [56]. We follow those
ideas by also sub-sampling a small set of frames uniformly.
Our model further advances those concepts, by exploiting
spatial relationships between sampled temporal frames via a
high-order multimodal attention module, where each video
frame is treated as a separate modality. Li et al. [37] propose
the Video-LSTM model, which uses attention to emphasis
relevant locations, during LSTM video encoding. Our ap-
proach differs in that attention on one frame can inﬂuence
attention on other frames which isn’t the case in their model.
Audio Data: Audio data gained popularity in the vision
community recently. For instance, prediction of pose given
audio input [60], learning of audio-visual object models
from unlabeled video for audio source separation in novel
videos [20, 51], use of video and audio data for acoustic
scene/object classiﬁcation [6], source separation was also
considered in [17] and learning to see using audio [52].
Multimodal Attention: Multimodal attention has been a
prominent component in tasks which operate on different
input data. Xu et al. [75] showed an encoder decoder at-
tention model for image captioning, which was extended to
visual question answering [74]. Yang et al. [76] propose a
multi-step reasoning system using an attention model. Mul-
timodal pooling methods were also explored [18, 33]. Lu et
al. [41] suggest to produce co-attention for the image and
question separately, using a hierarchical and parallel formu-
lation. Schwartz et al. [57, 58] later extend this approach to
high-order attention applied over image, question and answer
modalities via potentials. Similarly, in the visual dialog task,
co-attention models have held the state-of-the-art [71, 40]
attending over image, question and history in hierarchical
manner. For audio-visual scene-aware dialog, [25] also use
a sum-pooling type of attention, using the question feature

|

 

⋅ )

p(y

1

|

p(y

2

⋅ )

p(y

n

|

⋅ )

FC

FC

FC

LSTM 
T=1 

LSTM 
T=2 

(

h

,

c

)

0

0

LSTM 
T=F+1 

LSTM 
T=1 

LSTM 
T=2 

LSTM 
T=N 

a

a

a

a

A

T

V

V

1

F

<S>

a

a

y

y

T

T

1

n−1

Figure 3: Our decoder for audio-visual scene-aware dialog. We
start with encoding of attended audio and video vectors using the
Aud-Vis LSTM (orange colored), followed by the Ans-Generation
LSTM that receives the textual data concatenated with the previous
answer word (green colored).

along with audio and video modalities separately. In contrast,
here we compute attention over each modality via local and
cross data evidence, letting all the modalities interact with
each other.

3. Audio Visual Scene-Aware Dialog Baselines

Our method has three building blocks: answer generation,

attention and data representation as shown in Fig. 2.

3.1. Answer Generation

We are interested in predicting an answer y =
(y1, . . . , yn) consisting of n words yi ∈ Yi = {1, . . . , |Yi|}
each arising from a vocabulary of possible words Yi. Given
data x = (Q, V, A, H) which subsumes, a question Q, a sub-
sampled video V = (V1, . . . , VF ) composed of F frames,
the corresponding audio signal A, and a history of past
question-answer pairs H, we construct a probability model
over the set of possible words for the answer generation
task. To this end, we formulate prediction of the answer as
inference in a recurrent model where the joint probability is

12550

a

A

a

a

Q

V

1

a

V

F

3.2. Attention

A

1

...

A

n

A

Q

1

...

Q

n

Q

V

F,1

...

V

1,1

V

F,n

V

V

1,n

V

r

r

A

Q

r

V

Figure 4: Multimodal Attention model for audio-visual scene-aware
dialog. We treat each frame as a modality, along with audio and
question modality, to total of 6 modalities. Each element attention
score is affected not only from local evidence, but also via cross-
data interactions of all other elements.

given by the product of conditionals, i.e.,

p(y|x) =

n

Yi=1

p(yi|y<i, x).

Note that, for now, we condition on all the data x for read-
ability and provide details later. Instead of conditioning the
probability of the current word p(yi|y<i, x) on its entire past
y<i, we combine two recurrent nets: an audio-visual recur-
rent net that generates the temporal information which is fed
as an initialization to the answer generating recurrent net.
See Fig. 3 for a schematic.
Audio-visual LSTM-net: It operates on an attended audio
embedding aA and attended video embeddings aV1 , ..., aVF
for each of the F frames f ∈ {1, . . . , F }. This LSTM-net
has F +1 units, the ﬁrst unit’s input is the attended audio vec-
tor, and the input to the F subsequent units are the attended
video representations aV1 , . . . , aVF . The context vector that
is generated from this LSTM, i.e., (h0, c0) summarizes the
audio-visual attention and is provided as input to the answer
generation LSTM-net.
Answer generation LSTM-net: It computes conditional
probabilities for the possible words yi ∈ Yi of the answer
y = (y1, . . . , yn). This probability considers the last word
and captures context via a representation hi−1 obtained from
the previous time-step.

p(yi|yi−1, hi−1, x) = gw(yi, yi−1, hi−1, x).

We illustrate the LSTM-net gw in Fig. 3. Using the initial
state (h0, c0), the LSTM-net gw predicts in its i-th step a
probability distribution p(yi|yi−1, hi−1, x) over words yi ∈
Yi using as input yi−1 and the textual attention vector aT =
(aQ, rH ): the attended textual vector is a concatenation of
the attended question vector aQ and the history vector rH ,
which represents information about question and history
data. The output of the LSTM-net is transformed via a FC-
layer with a dropout and a softmax to obtain the probability
distribution p(yi|yi−1, hi−1, x).

The attention step provides an attended representation
for the data components, i.e., aVf ∈ RdV for frame f ∈
{1, . . . , F } of the video data, aA ∈ RdA for the audio data,
and aT ∈ RdT for the textual data. These attended repre-
sentations are obtained by transforming the representations
extracted from the raw data, i.e., rVf ∈ RnV ×dV for the
video data, rA ∈ RnA×dA for the audio data, and for the
textual data, rQ ∈ RnQ×dQ as well as rH ∈ RdH which
capture signals from the question and history respectively.
We outline the general procedure in Fig. 4.

Formally, we obtain the attended representation

nα

aα =

αkpα(k),

Xk=1

where α ∈ {A, Q, V1, . . . , VF } is used to index the available
data components (audio, question, visual frames), nα is the
number of entities in a data component (e.g., the number
of words in a question), and pα(k) ≥ 0 ∀α is a probability
k=1 pα(k) = 1 ∀α) over the nα entity repre-
sentations of data α. For instance, if we let α = A we obtain

distribution (Pnα
the attended audio representation aA =PnA

We compute the attention via a factor graph attention
approach [57, 58]. The attention probability distribution
over a data source α consists of a log-prior distribution πα, a
local evidence lα that relies solely on its data representation
rα and a cross data evidence cα that accounts for correlations
between the different data representations rα, rβ, for β ∈
{A, Q, V1, . . . , VF }. This probability distribution takes the
form:

k=1 AkpA(k).

pα(k) ∝ exp ( ˆwαπα(k) + lα(k) + cα(k)) .

log-prior is πα(k) and the cross data evidence is

The local evidence is lα(k) = wα(cid:0)v⊤
Xj=1 (cid:18) Lααk
cα(k) = Xβ∈D

wα,β
nβ

α relu(Vααk)(cid:1), the
kRββjk(cid:19)! .

kLααkk(cid:19)⊤(cid:18) Rββj

nβ

The set D = {A, Q, V1, . . . , VF } consists of the possible
data types. The trainable parameters of the model are: (1)
Vα, Lα, Rα which re-embed the data representation to tune
the attention; (2) vα which scores the local modality; and
(3) ˆwα, wα, wα,β which weight the three components with
respect to each other.

We found the use of attention for history to not yield im-
provements. Therefore, we obtain the attended textual repre-
sentation aT ∈ RdT by concatenating the attended question
representation aQ ∈ RdQ with the history representation
rH ∈ RdH . Consequently, dT = dQ + dH .

3.3. Data Representation

The proposed approach relies on representations rα ob-
tained for a variety of data components which we brieﬂy
discuss subsequently.

12551

Video: Containing both temporal and spatial information,
video data is among the most memory consuming. Common
practice is to reduce the spatial information while maintain-
ing attention over the temporal dimension. Instead, we ﬁrst
reduce the temporal dimension, maintaining the ability for
spatial attention to reason about the video content. To ensure
fast training, we reduce the temporal dimension by sampling
F frames uniformly. For each sampled frame we extract a
representation from a deep net trained on ImageNet (in our
case VGG19). We then ﬁne tune the representation of each
frame using a 1D conv layer with a bias term. This conv
layer is identical for all the F frames. Consequently, we ob-
tain the video representation rV ∈ RF ×nV ×dV , where F is
the number of sampled frames, nV is the spatial dimension
and dV is the embedding dimension.
Audio: For audio, we extracted features from a strong audio
classiﬁcation model (i.e., VGGish [24]) by taking the last
representation before the ﬁnal FC-layer. This representa-
tion has adaptive temporal length. For each batch we ﬁnd
the maximal temporal length of the audio signal, and zero-
padded the shorter audio representations. We then ﬁne-tune
each audio ﬁle using a 1D conv layer with a bias. We ob-
tain the audio representation rA ∈ RnA×dA , where nA is
the maximal temporal length of a given batch and dA is the
embedding dimension.
Question: We start with an adaptive-length list of 1-hot
word-representations. For each batch we ﬁnd the longest
sentence, and zero-pad shorter ones. We embed each word
using a linear-embedding layer, followed by a single layer
LSTM-net with dropout. The last hidden state of the LSTM
is the question representation rQ ∈ RnQ×dQ , where nQ is
the length of the maximal sentence for the given batch and
dQ is the embedding dimension.
History: The history data source consists of the past
T question-answer pairs, which we denote by H =
(Q, A)t∈{1,...,T }. The history embedding consists of two
components: we ﬁrst embed each question-answer pair
(Q, A)t using a LSTM-net to get T representations of the
history. We then feed these representations into another
LSTM-net to obtain the vector representation rH ∈ RdH ,
where dH is the history embedding dimension.

We embed each question-answer pair (Q, A)t following
the question embedding above. A question-answer pair starts
with a list of 1-hot word-representations of the words in
the question followed by 1-hot word-representations of the
words in the answer. For each batch we ﬁnd the longest
question-answer sequence, and zero-pad the shorter ones.
We embed each 1-hot vector using a linear-embedding layer,
followed by a two layer LSTM-net with a dropout. The last
hidden state of this LSTM-net is the vector representation of
(Q, A)t, which we denote by rt.

We embed the history by feeding r1, . . . , rT to a one layer
LSTM-net with dropout, in order to capture the temporal
aspect of the question-answer history. To deal with the

Figure 5: Perplexity values for our model vs. baseline [25].

adaptive length of history interactions, for each batch we
ﬁnd the interaction with the longest history, and zero-pad
question-answer pairs with shorter history. The ﬁnal LSTM-
net hidden state is the history representation rH ∈ RdH ,
where dH is the history embedding dimension.

4. Results

In the following we evaluate the discussed baseline on
the Audio Visual Scene-Aware Dialog (AVSD) dataset. We
follow the proposed protocol and assess the generated an-
swers to a user question given a dialog context [2, 25]. This
context consists of a dialog history (previous questions and
answers) in addition to video and audio information about
the scene. Our code is publicly available1.

4.1. AVSD v0.1 Dataset

The AVSD dataset consists of annotated conversations
about short videos. The dataset contains 9,848 videos taken
from CHARADES, a multi-action dataset with 157 action
categories [61]. Each dialog is obtained from two Amazon
Mechanical Turk (AMT) workers, who discuss about events
in a video. One of the workers takes the role of an answerer
who had already watched the video. The answerer replies to
questions asked by another AMT worker, the questioner.

The questioner was not shown the whole video but only
the ﬁrst, middle and last frames of the video. The dialog
revolves around the events in and other aspects of the video.
The AVSD v0.1 dataset is split into 7,659 train dialogs, 1,787
validation and 1,710 test dialogs. Because the test set doesn’t
currently include ground truth, we follow [25] and evaluate
on the ‘prototype test-set’ with 733 dialogs. Because the
‘prototype test-set’ is part of the ‘v0.1 validation-set,’ we
use the ‘prototype validation-set’ with 732 dialogs, which
doesn’t overlap with the ‘prototype test-set.’

4.2. Implementation Details

Our system relies on textual, visual and audio data rep-
resentations, i.e., rα for α ∈ {A, Q, V1, . . . , VF }. For the
video representation we randomly sample F = 4 equally
spaced frames, and use the last conv layer of a VGG19
having a dimensions of 7 × 7 × 512. Therefore the visual
embedding dimension is dV = 512. After ﬂattening the 2D

1https://github.com/idansc/simple-avsd

12552

01234567Epoch1520253035PerplexityBaselineOursBaselineOursTable 1: Results for the AVSD dataset for CIDEr, BLEU1, . . . ,
BLEU4, ROUGE-L, METEOR. We provide a comparison to the
baseline and a detailed ablation study separated into categories and
discussed in Sec. 4.5. We also report the number of parameters for
each baseline.

Model
baseline[25]2

C

B4

B3

B2

B1

R

M

P

0.766

0.084

0.117

0.173

0.273

0.291

0.117

6.15M

basic baselines

q
q+h
q+h+vgg-spatial
q+h+vgg-spatial+audio

q+att
q+h+att
q+h+vgg-spatial+att

0.088
0.089
0.089
0.091

0.815
0.843
0.869
0.874

0.122
0.123
0.124
0.125
basic baselines+attention
0.124
0.124
0.129

0.090
0.090
0.093

0.849
0.861
0.908

attention-model

w/o-cross-data-evidence
w/o-local-evidence
w/o-question-prior
sharing–weights

0.896
0.917
0.906
0.923

0.095
0.096
0.096
0.097

temporal-attention
summation
weighted-summation
video-audio-lstm

video-fusion

0.877
0.890
0.876
0.865

0.091
0.093
0.094
0.076

0.131
0.132
0.132
0.133

0.126
0.128
0.130
0.101

decoder-input

q-ﬁrst-state
all-ﬁrst-state
all-concat-decoder-input
q+h+a-concat-input

0.704
0.714
0.797
0.857

0.078
0.079
0.089
0.090

0.110
0.114
0.125
0.123

0.178
0.178
0.180
0.182

0.179
0.177
0.185

0.190
0.191
0.190
0.191

0.182
0.183
0.187
0.141

0.163
0.171
0.183
0.177

i3d-features-&-spatial-temporal

i3d-rgb-temporal
i3d-rgb-ﬂow-temporal
i3d-rgb-spatial-10
vgg-spatial-1
vgg-spatial-16

default
xavier
he

w/o beam
2-width
4-width
5-width
Ours

0.886
0.851
0.928
0.919
0.903

0.094
0.091
0.097
0.095
0.093

initialization

0.877
0.848
0.913

0.090
0.087
0.095

0.130
0.127
0.133
0.130
0.128

0.123
0.119
0.131

0.188
0.185
0.190
0.187
0.186

0.178
0.171
0.189

beam-search hyper-parameters

0.924
0.934
0.931
0.926
0.941

0.082
0.094
0.096
0.096
0.096

0.109
0.128
0.131
0.132
0.131

0.152
0.183
0.188
0.188
0.187

0.279
0.277
0.279
0.282

0.278
0.271
0.283

0.292
0.293
0.292
0.293

0.281
0.283
0.289
0.210

0.257
0.271
0.285
0.274

0.289
0.286
0.290
0.287
0.287

0.274
0.262
0.290

0.226
0.279
0.287
0.289
0.285

0.297
0.296
0.302
0.305

0.298
0.298
0.307

0.309
0.309
0.309
0.309

0.302
0.303
0.304
0.286

0.279
0.276
0.297
0.298

0.306
0.303
0.310
0.309
0.307

0.300
0.297
0.308

0.298
0.311
0.310
0.309
0.311

0.121
0.122
0.123
0.124

0.121
0.122
0.125

0.128
0.128
0.127
0.127

0.124
0.124
0.126
0.108

0.112
0.113
0.121
0.121

0.126
0.125
0.127
0.126
0.127

0.121
0.119
0.127

0.114
0.126
0.127
0.127
0.128

3.1M
4.51M
5.12M
5.23M

3.35M
4.57M
7.4M

7.5M
8.35M
8.35M
6.18M

8.4M
7.35M
7.85M
8.35M

8.35M
10.1M
9.53M
7.72M

7.23M
7.82M
6.58M
6.18M
28.88M

8.35M
8.35M
8.35M

8.35M
8.35M
8.35M
8.35M
8.35M

spatial dimension, we obtain the spatial dimension nV = 49.
For audio features we use VGGish that operates on 0.96s
log-Mel spectrogram patches extracted from 16kHz audio,
and outputs a dA = 128 dimensional vector. VGGish inputs
overlap by 50%, therefore an output is provided every 0.48s.
Dropout parameters before the last FC layer, and the LSTM
layers are set to 0.5. For the question representation we set
the word embedding dimension to 128. The questions are
embedded to dQ = 256 dimensional vectors, extracted from
the last hidden state of their LSTM-net. The history con-
sists of T = 10 question-answer pairs, which we denote by
H = (Q, A)t∈{1,...,T }. We use an LSTM-net with a hidden
state of dH = 128 to encode the history.

4.3. Training

We use a cross-entropy loss on the probabilities,
p(yi|y<i, x) to train the answer generator, the attention and
the embedding layers jointly end-to-end. The total amount
of trainable parameters are 8,359,107. We use the Adam
optimizer [34] with a learning rate of 0.001 and a batch size
of 64. During training after each epoch we evaluate our
performance on the validation set using a perplexity metric.
We stop our training after two consecutive epochs with no
improvement.

We use a standard machine with an Nvidia Tesla M40
GPU for all our experiments. Training our system takes 4
epochs to converge vs. 9 epochs for the baseline (see Fig. 5).
Each epoch takes 8 minutes vs. 13 minutes for the baseline.
In total, training our model takes approximately 30 minutes.

4.4. Performance Evaluation:

We evaluate the performance of our system using several
metrics. Our prime metric is CIDEr, the Consensus-based
Image Description Evaluation, which measures the similar-
ity of a sentence to the consensus [66]. We also evaluate
our performance on the ROUGE-L metric (Recall Oriented
Understudy of Gisting Evaluation). This is a recall-based
metric that measures the longest common subsequence of
tokens [38]. The METEOR metric is a unigram precision
and recall that allows for matchings between candidates and
references [7]. We also evaluate our performance using the
traditional BLEU score, which measures the effective over-
lap between a reference sentence and a candidate sentence.
We measure the geometric mean of the effective n-gram
precision scores, for n = 1, . . . , 4 and refer to these as
BLEU1,. . ., BLEU4.

4.5. Quantitative Results and Insights for a Good

Baseline

We compare to the baseline discussed in [25]. In the fol-
lowing we explore the various components of audio-visual
dialog systems and present our insights for constructing a
simple and effective baseline. These insights cover all as-
pects of our system: feature embedding, attention, fusion
and training techniques. We particularly emphasize the im-
portance of spatial features for AVSD, which we contrast
with the action recognition based I3D features.
Question Bias and Basic Baselines: We revisit the scores
published by [25] and assess a basic seq2seq-type baseline,
with no attention [64]. In this variant, which we call q in
Tab. 1, we encode the question using a word embedding
(with embedding dimension of 128) and a 1-layer LSTM-net
(with hidden state dimension of 256 compared to a dimen-
sion of 128 in the baseline), without any video or history
related features. For decoding, another 1-layer LSTM-net
(with hidden state dimension of 256 compared to a dimen-
sion of 128 in the baseline) is used. Surprisingly, this model
alone was able to surpass the current baseline of [25]. Sim-
ilar results are also reported in [55]. This indicates that
there might be bias-problem within the AVSD dataset, no
visual information is needed. For instance a common ques-
tion is “How many people are in the video?”, but videos
in many cases feature only one person. Another example
are questions of the form “is it indoor?” which are mean-
ingless since the CHARADES dataset focuses on indoor
activities. Another possible explanation for this good result
is the encoding of the answer in the question. For instance,
a question “this person is standing in a kitchen correct?” is

12553

Figure 6: An illustration of out 4-framed samples from a video along with the relevant attention variables. Our attention treats any frame as
different component. This allows the attention module to learn different attention behaviors for different temporal locations. We observe
the ﬁrst and fourth samples are noisier, while the second and third attend to speciﬁc interesting locations. Our multimodal attention also
generates attention for questions, illustrated over the question via a word heat map. We provide generated answers for different baseline
models: q+h+att, is a model with only history and question input; i3d-rgb-temporal is a model with temporal features instead of spatial;
q+h+vgg-spatial+audio is a model without attention. We also compare to the generated answer by [25]. the ground-truth is denoted by GT,
and our ﬁnal model denoted by Ours.

answered with “yes he is in the kitchen.” Moreover, genera-
tive evaluation is also more prone to biases, as the evaluation
emphasizes correct sentence structure rather than correctness
of the answer. Very recently, a discriminative approach was
proposed [1]. The bias problem is not unique to AVSD, and
was also discussed for Visual Question Answering [22].

To further improve the most basic baseline q, we add
more modalities. We use the fusion and embedding tech-
niques of the proposed model but omit attention. Instead of
attention, we use a mean over the representation for visual
and auditory data sources, and the last hidden state of the
LSTM-net is used to represent the question data source. We
found that our model can utilize any modality supplement,
even without attention. In the ‘basic baselines+attention’
section of Tab. 1 we assess versions with attention, which
brings us closer to our full model.

Spatial vs. Temporal Information: Current methods focus
on temporal models and often na¨ıvely reduce the spatial di-
mension [25, 70, 80]. In contrast, for closely related visual
reasoning tasks, such as visual dialog and visual question
answering, it is broadly accepted that spatial attention is
necessary. Therefore, it is unlikely that video reasoning is ef-
fective when simply reducing the spatial dimension. Indeed,
we ﬁnd better results when reducing the temporal dimen-
sion with sampling techniques and employing attention to
reduce the spatial dimension. In Fig. 6 we observe that a
small subset of frames (e.g., 4) is usually enough for an

almost complete understanding of the video. In the ‘i3d-
features-&-spatial-temporal’ section of Tab. 1, we compare
spatial-based features to temporal-based ones. The temporal
features are computed on a stack of 16 video frames, and are
treated as an input modality to our attention mechanism. At-
tention choses the relevant temporal locations. The temporal
attended representation was fed to the Aud-Vis LSTM-net
along with the audio attended-features. For the i3d-rgb-ﬂow
version we also use the I3D model based on optical ﬂow
features as an additional data component. This resulted in
a drop in performance compared to the spatial-based i3d-
features reported in the i3d-rgb-spatial-10 line of Tab. 1. We
also test different number of sampled frames. Interestingly,
only one frame is already very useful for AVSD, and too
many VGG-frames harm performance. Note that each frame
is coupled to an attention-score and treated as a modality,
which explains why too many frames can add noise to the
inferred multimodal probability.

I3D Features vs. VGG: I3D features are widely used as
video-based feature extractor (cf . [9]), discarding the classi-
cal image-based features, e.g., VGG. They are extracted from
a model trained on the Kinetics Dataset, a dataset for action
recognition, and have been shown to improve many video
tasks. We ﬁnd that while I3D features have repeatedly been
shown to improve on action-recognition tasks, they are not
as useful in the answer generation task of AVSD. Equipped
with VGG features we were able to achieve comparable re-

12554

GT:  the pillow is brown in the video .Ours: it appears to be a brown pillow . q+h+att: the pillow is white in the video . baseline: it appears to be black .q+h+vgg-spatial+audio: the blanket is white and white .i3d-rgb-temporal: it looks to be white  .Q:what color is the pillow ? Q: is he holding something in his hand ? GT: a sandwich and then he gets a drink .Ours: yes , he is holding a sandwich in his hand .q+h+att: yes , he is holding a broom . baseline: yes , he is holding a book .i3d-rgb-temporal: yes , he is holding a cell phone in his hands .q+h+vgg-spatial+audio: yes , he is holding a bag of chips . GT: ithink sheswearing a sweatshirt and sweatpantsOurs: she is wearing a black sweatshirt and jeans q+h+att: she is wearing a blue shirt and jeans .baseline: she is wearing a blue shirt q+h+vgg-spatial+audio: she is wearing ablue shirt and jeans.i3d-rgb-temporal:she is wearing a blue shirt and black pantsQ: whatsshe wearing ? GT: he looks like he is watching television at the end .Ours: yes he is watching television .q+h+att: yes , he is watching tv . baseline:  no , he is not talking in the video q+h+vgg-spatial+audio: yes he is watching something on the laptop . i3d-rgb-temporal: no , he is not watching tv .Q:is he watching tv ?GT: she looks like she is aloneOurs: no , she is alone in the video .q+h+att: no she is the only one in the video .Baseline: no , she is the only one in the video q+h+vgg-spatial+audio: she is the only one in the videoi3d-rgb-temporal:no one else is in the video Q: is anyone else with her ? GT: no ido not see that .Ours: no he does not put it in the sink .q+h+att: yes , he puts it down on the counter .Baseline: no he does not put it on the table .q+h+vgg-spatial+audio: no he does not put the cup down .i3d-rgb-temporal:no he does not put it down .Q: does he put the glass in the sink ? sults to the i3d-rgb-spatial-20 version. The i3d-rgb-spatial
features are 4 times bigger (7x7x512 vs. 2x7x7x1024), as
well as more complicated to extract. Seeking simplicity,
we report scores with the VGG-based features subsequently.
This may also indicate a weakness in the dataset, as this
solution seems to be sub-optimal for action-related ques-
tions (e.g., classifying sequences of actions). Not only do
we na¨ıvely sample temporal frames, but also do we not use
I3D features that were extracted from a network trained for
action-recognition, yet we achieve good results.

Attention Model: We assess different components of the
attention model. See Sec. 3.2 for details about local evidence
and cross data evidence. We found that every component
contributes to the model, especially the cross-data compo-
nent. The cross-data component determines the attention
score of an element by considering interactions with other
modalities. For instance, a region in the second frame can
affect a region in the third frame, or perhaps a word in the
question.

To ﬁnd the simplest attention module, we also explored
the option of grouping together the parameters for all video
frames, i.e., VV1 = . . . = VVF , LV1 = . . . = LVF , and
RV1 = . . . = RVF , which yields good results despite 2
million fewer parameters. This version allows to increase
the number of processed frames, with no additional memory
cost. Those results are reported in the ‘sharing-weights’ line
of Tab. 1.

Multimodal Decoding Fusion: We experimented with sev-
eral variants that reduce aA, aV1 , . . . , aVF . In Tab. 1, section
‘decoder-input,’ we show a version that uses an additional
multimodal attention step over the video-related attended
vector, called temporal-attention. Another attempt is sum-
mation polling of the vectors, and weighted summation
with scalers. Instead, we note the sequential information
of aV1 , . . . , aVF that naturally calls for the use of an ad-
ditional LSTM unit, which we call Aud-Vis (see Fig. 4).
We think audio is a more general cue while frames have
more speciﬁc information. Ordering is guided by the intu-
ition that LSTM-based encoding commonly starts with more
general information. To verify this intuition, in video-audio-
lstm, we performed additional experiments with ordering of
aV1 , . . . , aVF , aA.

Next we ﬁnd a good way to input elements into the answer
generation LSTM-net. We ﬁrst analyze the basic q model. A
classic decoder, where encoded q are fed as ﬁrst hidden state
to the LSTM-net is reported in the ‘q-ﬁrst-state’ row in Tab. 1
(decoder-input section). This suggest that textual data should
be concatenated to the decoder inputs. Concatenating all
modalities to the input, which is reported in the ‘all-concat-
input’ line in Tab. 1 drops the performance, suggesting that
a dichotomy of video-related and textual-related features is
useful. To incorporate the audio signal, we ﬁnd it’s best to
use it as a ﬁrst state in the Aud-Vis LSTM-net. A version
where we concatenated the audio attended vector to aT is

referred to as ‘q+h+a-concat-input+s-ﬁrst-state.’ The model
behaves the best when the fused video related features were
used as the initial state h0 of the Ans-Generation LSTM-
net. Our state-of-the-art model further improves the fusion
technique by using the Aud-Vis LSTM-net to generate h0
which captures the temporal information of audio attention
aA and the visual attention aV1 , . . . , aVF .
Weight Initialization: An important aspect is the initializa-
tion of the deep net parameters. We observed a signiﬁcant
improvement using Kaiming normal initialization or Xavier
initialization for all LSTM models [23, 21].
Beam Search Width: In an attempt to improve the overall
evaluation time, we experimented with different beam width.
We found that although beam search is useful for generation,
a width of 2 achieves almost as good results. Our version
use 3-width beam search.

4.6. Qualitative Results

In Fig. 6, we show several examples of generated answers
of ﬁve models, our ﬁnal model, a version without any at-
tention (q+h+vgg-spatial+audio), a version with temporal
I3D features (i3d-rgb-temporal), a version with only textual
modalities (q+h+att), and the baseline [25]. The ground-
truth is referred to via GT. Additionally, we take advantage
of the interpretability of attention modules to also illustrate
the attention probabilities of our ﬁnal modal on 5 different
modalities, i.e., our 4-frames, and the question. First, we
observe an interesting behavior of our attention model: each
sampled frame is attended a differently, which captures dif-
ferent features from different frames. The ﬁrst and fourth
frames are noisier and extract general concepts, while the
second and third capture unique aspects of the video, e.g., a
person, a couch. This behavior can be associated with the
temporal aspect of the frames. Meaning it is more important
to capture general aspects at the end and at the beginning,
but in the middle we reveal the important speciﬁc concepts.
Additionally, the question attention attends to the informa-
tive words. Our generated answers are usually more aware
of the scene, and less prone to bias. For instance, in the ﬁrst
row, the question is “what color is the pillow?.” We observe
our model to be able to answer the correct color, while all
other model variants answer with white, the most-common
color of a pillow. In another question “whats she is wearing,”
our model was the only one to relate to her black sweatshirt.

5. Conclusion

We propose a simple baseline for Audio-Visual Scene-
Aware Dialog that surpasses current techniques by 20% on
the CIDEr metric. Pioneering on this task, we carefully
evaluated our approach. We hope our analysis can bridge the
gap between video-reasoning and image-reasoning.
Acknowledgments: This work is supported in part by NSF
under Grant No. 1718221, Samsung, and 3M.

12555

References

[1] H. Alamri, V. Cartillier, A. Das, J. Wang, S. Lee, P. Anderson,
I. Essa, D. Parikh, D. Batra, A. Cherian, T. K. Marks, and
C. Hori. Audio-visual scene-aware dialog. arXiv preprint
arXiv:1901.09107, 2019. 7

[2] H. Alamri, V. Cartillier, R. G. Lopes, A. Das, J. Wang, I. Essa,
D. Batra, D. Parikh, A. Cherian, T. K. Marks, and C. Hori. Au-
dio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7.
In https://arxiv.org/abs/1806.00525, 2018. 1, 5

[3] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Deep com-
positional question answering with neural module networks.
In Proc. CVPR, 2016. 2

[4] J. Aneja, A. Deshpande, and A. G. Schwing. Convolutional

Image Captioning. In Proc. CVPR, 2018. 1

[17] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Has-
sidim, W. T. Freeman, and M. Rubinstein. Looking to Listen
at the Cocktail Party: A Speaker-Independent Audio-Visual
Model for Speech Separation. In Proc. SIGGRAPH, 2018. 3

[18] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and
M. Rohrbach. Multimodal compact bilinear pooling for visual
question answering and visual grounding. In Proc. EMNLP,
2016. 2, 3

[19] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.
Are you talking to a machine? Dataset and Methods for
Multilingual Image Question Answering.
In Proc. NIPS,
2015. 1, 2

[20] R. Gao, R. S. Feris, and K. Grauman. Learning to Separate
Object Sounds by Watching Unlabeled Video. In Proc. CVPR
Workshop, 2018. 3

[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh. VQA: Visual question answering. In
Proc. ICCV, 2015. 1, 2

[21] X. Glorot and Y. Bengio. Understanding the difﬁculty of
training deep feedforward neural networks. In Proc. AISTATS,
2010. 8

[6] Y. Aytar, C. Vondrick, and A. Torralba. Soundnet: Learning
sound representations from unlabeled video. In Proc. NIPS,
2016. 3

[7] S. Banerjee and A. Lavie. Meteor: An automatic metric for
mt evaluation with improved correlation with human judg-
ments. In Proceedings of the acl workshop on intrinsic and
extrinsic evaluation measures for machine translation and/or
summarization, 2005. 6

[8] H. Ben-younes, R. Cadene, M. Cord, and N. Thome. Mutan:
Multimodal tucker fusion for visual question answering. In
Proc. ICCV, 2017. 2

[9] J. Carreira and A. Zisserman. Quo vadis, action recognition?
a new model and the kinetics dataset. In Proc. CVPR, 2017.
1, 2, 7

[10] M. Chatterjee and A. G. Schwing. Diverse and Coherent
Paragraph Generation from Images. In Proc. ECCV, 2018. 1

[11] X. Chen and C. L. Zitnick. Mind’s eye: A recurrent visual
representation for image caption generation. In Proc. CVPR,
2015. 1

[12] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Batra.
Human attention in visual question answering: Do humans
and deep networks look at the same regions? In Proc. EMNLP,
2016. 2

[13] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. F.
Moura, D. Parikh, and D. Batra. Visual Dialog. In Proc.
CVPR, 2017. 1, 2

[14] A. Das, S. Kottur, J. M. F. Moura, S. Lee, and D. Batra. Learn-
ing cooperative visual dialog agents with deep reinforcement
learning. In Proc. ICCV, 2017. 1, 2

[15] A. Deshpande, J. Aneja, L. Wang, A. G. Schwing, and D. A.
Forsyth. Diverse and Controllable Image Captioning with
Part-of-Speech Guidance. In Proc. CVPR, 2019. 1, 2

[16] J. Donahue, L. Anne Hendricks, S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell.
Long-term recurrent convolutional networks for visual
recognition and description. In Proc. CVPR, 2015. 1

[22] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.
Making the v in vqa matter: Elevating the role of image
understanding in visual question answering. In Proc. CVPR,
2017. 7

[23] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In Proc. ICCV, 2015. 8

[24] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke,
A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous,
B. Seybold, M. Slaney, R. J. Weiss, and K. Wilson. Cnn
architectures for large-scale audio classiﬁcation.
In Proc.
ICASSP, 2017. 5

[25] C. Hori, H. Alamri, J. Wang, G. Wichern, T. Hori, A. Cherian,
T. K. Marks, V. Cartillier, R. G. Lopes, A. Das, I. Essa, D. Ba-
tra, and D. Parikh. End-to-End Audio Visual Scene-Aware
Dialog using Multimodal Attention-Based Video Features. In
https://arxiv.org/abs/1806.08409, 2018. 1, 3, 5, 6, 7, 8

[26] A. Jabri, A. Joulin, and L. van der Maaten. Revisiting Visual

Question Answering Baselines. In Proc. ECCV, 2016. 2

[27] U. Jain, S. Lazebnik, and A. G. Schwing. Two can play this
Game: Visual Dialog with Discriminative Question Genera-
tion and Answering. In Proc. CVPR, 2018. 1, 2

[28] U. Jain∗, Z. Zhang∗, and A. G. Schwing. Creativity: Gener-
ating Diverse Questions using Variational Autoencoders. In
Proc. CVPR, 2017. ∗ equal contribution. 1, 2

[29] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L.
Zitnick, and R. Girshick. Clevr: A diagnostic dataset for
compositional language and elementary visual reasoning. In
Proc. CVPR, 2017. 2

[30] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully
convolutional localization networks for dense captioning. In
Proc. CVPR, 2016. 1

[31] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments
for generating image descriptions. In Proc. CVPR, 2015. 1, 2

[32] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-Scale Video Classiﬁcation with Convo-
lutional Neural Networks. In Proc. CVPR, 2014. 2

12556

[33] J.-H. Kim, K.-W. On, W. Lim, J. Kim, J.-W. Ha, and B.-
T. Zhang. Hadamard product for low-rank bilinear pooling.
arXiv preprint arXiv:1610.04325, 2016. 3

[50] M. Narasimhan and A. G. Schwing. Straight to the Facts:
Learning Knowledge Base Retrieval for Factual Visual Ques-
tion Answering. In Proc. ECCV, 2018. 1

[34] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint 2014. 6

[35] S. Kottur, J. M. F. Moura, D. Parikh, D. Batra, and
M. Rohrbach. Visual coreference resolution in visual dia-
log using neural module networks. In Proc. ECCV, 2018.
2

[36] Y. Li, N. Duan, B. Zhou, X. Chu, W. Ouyang, and X. Wang.
Visual Question Generation as Dual Task of Visual Question
Answering. In https://arxiv.org/abs/1709.07192, 2017. 1, 2

[37] Z. Li, K. Gavrilyuk, E. Gavves, M. Jain, and C. G. M. Snoek.
Videolstm convolves, attends and ﬂows for action recognition.
CVIU, 2018. 3

[38] C.-Y. Lin. Rouge: A package for automatic evaluation of

summaries. Text Summarization Branches Out, 2004. 6

[39] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common
objects in context. In Proc. ECCV, 2014. 1

[40] J. Lu, A. Kannan, J. Yang, D. Parikh, and D. Batra. Best
of both worlds: Transferring knowledge from discriminative
learning to a generative visual dialog model. In Proc. NIPS,
2017. 3

[41] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical question-
image co-attention for visual question answering. In Proc.
NIPS, 2016. 2, 3

[42] L. Ma, Z. Lu, and H. Li. Learning to answer questions from
image using convolutional neural network. In Proc. AAAI,
2016. 2

[43] M. Malinowski and M. Fritz. A Multi-World Approach to
Question Answering about Real-World Scenes based on Un-
certain Input. In Proc. NIPS, 2014. 2

[44] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neu-
rons: A neural-based approach to answering questions about
images. In Proc. ICCV, 2015. 1, 2

[45] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
Deep Captioning with Multimodal Recurrent Neural Net-
works (m-rnn). In Proc. ICLR, 2015. 1, 2

[46] D. Massiceti, N. Siddharth, P. K. Dokania, and P. H. S. Torr.
FlipDial: A Generative Model for Two-Way Visual Dialogue.
In https://arxiv.org/abs/1802.03803, 2018. 1, 2

[47] N. Mostafazadeh, D. Brockett, B. Dolan, M. Galley, J. Gao,
G. P. Spithourakis, and L. Vanderwende. Image-grounded
conversations: Multimodal context for natural question and
response generation. arXiv preprint arXiv:1701.08251, 2017.
1

[48] N. Mostafazadeh, I. Misra, J. Devlin, M. Mitchell, X. He,
and L. Vanderwende. Generating natural questions about an
image. In Proc. ACL, 2016. 1, 2

[49] M. Narasimhan, S. Lazebnik, and A. G. Schwing. Out of the
Box: Reasoning with Graph Convolution Nets for Factual
Visual Question Answering. In Proc. NIPS, 2018. 1

[51] A. Owens and A. A. Efros. Audio-Visual Scene Analysis
with Self-Supervised Multisensory Features. In Proc. ECCV,
2018. 3

[52] A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and
A. Torralba. Learning Sight from Sound: Ambient Sound
Provides Supervision for Visual Learning. IJCV, 2018. 3

[53] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,
M. Gross, and A. Sorkine-Hornung. A benchmark dataset
and evaluation methodology for video object segmentation.
In Proc. CVPR, 2016. 2

[54] M. Ren, R. Kiros, and R. Zemel. Exploring models and data

for image question answering. In Proc. NIPS, 2015. 1, 2

[55] R. Sanabria, S. Palaskar, and F. Metze. Cmu sinbadas sub-
mission for the dstc7 avsd challenge. In DSTC7 at AAAI2019
workshop, 2019. 6

[56] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pas-
canu, P. Battaglia, and T. Lillicrap. A simple neural network
module for relational reasoning. In Proc. NIPS, 2017. 3

[57] I. Schwartz, A. G. Schwing, and T. Hazan. High-Order Atten-
tion Models for Visual Question Answering. In Proc. NIPS,
2017. 1, 2, 3, 4

[58] I. Schwartz, S. Yu, T. Hazan, and A. G. Schwing. Factor

Graph Attention. In Proc. CVPR, 2019. 1, 2, 3, 4

[59] K. J. Shih, S. Singh, and D. Hoiem. Where to look: Focus
regions for visual question answering. In Proc. CVPR, 2016.
1, 2

[60] E. Shlizerman, L. Dery, H. Schoen, and I. Kemelmacher-
Shlizerman. Audio to Body Dynamics. In Proc. CVPR, 2018.
3

[61] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev,
and A. Gupta. Hollywood in homes: Crowdsourcing data
collection for activity understanding. In Proc. ECCV, 2016. 5

[62] K. Simonyan and A. Zisserman. Two-Stream Convolutional
Networks for Action Recognition in Videos. In Proc. NIPS,
2014. 2

[63] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In Proc. ICLR,
2015. 2

[64] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence

learning with neural networks. In Proc. NIPS, 2014. 6

[65] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In Proc. ICCV, 2015. 2

[66] R. Vedantam, L.-C. Zitnick, and D. Parikh. Cider: Consensus-
based image description evaluation. In Proc. CVPR, 2015.
6

[67] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun,
S. Lee, D. Crandall, and D. Batra. Diverse Beam Search:
Decoding Diverse Solutions from Neural Sequence Models.
In Proc. AAAI, 2018. 2

12557

[68] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
tell: A neural image caption generator. In Proc. CVPR, 2015.
1

[69] L. Wang, A. G. Schwing, and S. Lazebnik. Diverse and Ac-
curate Image Description Using a Variational Auto-Encoder
with an Additive Gaussian Encoding Space. In Proc. NIPS,
2017. 1, 2

[70] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang,
and L. Van Gool. Temporal segment networks: Towards
good practices for deep action recognition. In Proc. ECCV.
Springer, 2016. 2, 7

[71] Q. Wu, P. Wang, C. Shen, I. Reid, and A. van den Hengel. Are
you talking to me? reasoned visual dialog generation through
adversarial learning. arXiv preprint arXiv:1711.07613, 2017.
3

[72] Q. Wu, P. Wang, C. Shen, I. Reid, and A. van den Hengel.
Are you talking to me? reasoned visual dialog generation
through adversarial learning. In Proc. CVPR, 2018. 2

[73] C. Xiong, S. Merity, and R. Socher. Dynamic memory net-
works for visual and textual question answering. In Proc.
ICML, 2016. 1, 2

[74] H. Xu and K. Saenko. Ask, attend and answer: Exploring
question-guided spatial attention for visual question answer-
ing. In Proc. ECCV, 2016. 1, 2, 3

[75] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,
R. Zemel, and Y. Bengio. Show, Attend and Tell: Neural
Image Caption Generation with Visual Attention. In Proc.
ICML, 2015. 1, 2, 3

[76] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked
attention networks for image question answering. In Proc.
CVPR, 2016. 1, 2, 3

[77] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image caption-

ing with semantic attention. In Proc. CVPR, 2016. 1

[78] H. Yu, J. Wang, Z. Huang, Y. Yang, and W. Xu. Video para-
graph captioning using hierarchical recurrent neural networks.
In Proc. CVPR, 2016. 2

[79] D. Zhang and M. Shah. Human Pose Estimation in Videos.

In Proc. ICCV, 2015. 2

[80] B. Zhou, A. Andonian, and A. Torralba. Temporal relational
reasoning in videos. arXiv preprint arXiv:1711.08496, 2017.
2, 7

[81] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7W:
Grounded Question Answering in Images. In Proc. CVPR,
2016. 2

12558

