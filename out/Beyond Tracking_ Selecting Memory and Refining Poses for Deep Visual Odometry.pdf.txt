Selecting Memory and Reﬁning Poses for Deep Visual Odometry

Beyond Tracking:

Fei Xue1,3, Xin Wang1,3, Shunkai Li1,3, Qiuyuan Wang1,3, Junqiu Wang2, and Hongbin Zha1,3

1Key Laboratory of Machine Perception (MOE), School of EECS, Peking University

2Beijing Changcheng Aviation Measurement and Control Institute, AVIC

3PKU-SenseTime Machine Vision Joint Lab

{feixue, xinwang cis, lishunkai, wangqiuyuan}@pku.edu.cn

jerywangjq@foxmail.com, zha@cis.pku.edu.cn

Abstract

Most previous learning-based visual odometry (VO)
methods take VO as a pure tracking problem. In contrast,
we present a VO framework by incorporating two additional
components called Memory and Reﬁning. The Memory
component preserves global information by employing an
adaptive and efﬁcient selection strategy. The Reﬁning com-
ponent ameliorates previous results with the contexts stored
in the Memory by adopting a spatial-temporal attention
mechanism for feature distilling. Experiments on the KITTI
and TUM-RGBD benchmark datasets demonstrate that our
method outperforms state-of-the-art learning-based meth-
ods by a large margin and produces competitive results
against classic monocular VO approaches. Especially, our
model achieves outstanding performance in challenging
scenarios such as texture-less regions and abrupt motions,
where classic VO algorithms tend to fail.

1. Introduction

Visual Odometry (VO) and Visual Simultaneous Local-
ization And Mapping (V-SLAM) estimate camera poses
from image sequences by exploiting the consistency be-
tween consecutive frames. As an essential task in au-
tonomous driving and robotics, VO has been studied for
decades and many outstanding algorithms have been devel-
oped [7, 8, 10, 20, 30]. Recently, as Convolutional Neural
Networks (CNNs) and Recurrent Neural Networks (RNNs)
achieve impressive performance in many computer vision
tasks [4,6,12,34], a number of end-to-end models have been
proposed for VO estimation. These methods either learn
depth and ego-motion jointly with CNNs [16,19,36,37,39],
or leverage RNNs to introduce temporal information [14,

Figure 1. Overview of our framework. Compared with existing
learning-based methods which formulate VO task as a pure track-
ing problem, we introduce two useful components called Memory
and Reﬁning. The Memory module preserves longer time informa-
tion by adopting an adaptive context selection strategy. The Reﬁn-
ing module ameliorates previous outputs by employing a spatial-
temporal feature reorganization mechanism.

22, 31–33]. Due to the high dimensionality of depth maps,
the number of frames is commonly limited to no more than
5. Although temporal information is aggregated through re-
current units, RNNs are incapable of remembering previous
observations for long time [27], leading to the limited usage
of historical information. Besides, the above methods pay
little attention to the signiﬁcance of incoming observations
for reﬁning previous results, which is crucial for VO tasks.
Direct estimation of camera motions from image snip-
pets is prone to large errors due to the geometric uncertainty
caused by small baselines (especially for handheld devices).
Consequently, error accumulation is getting increasingly se-
vere over time, as global poses are integrated from frame-
wise poses. In classic VO/SLAM systems [20], a local map

18575

02t-1t,,−is established according to the co-visibility graph over up
to hundreds of frames, on which bundle adjustment is ex-
ecuted to jointly optimize all corresponding poses. There-
fore, both previous and new observations are incorporated
for optimization, and accumulated errors are thus alleviated.
Inspired by classic VO/SLAM systems [7, 20], we in-
troduce an effective component, called Memory, which ex-
plicitly preserves the accumulated information adaptively.
Owing to the high sample frequency, contents between con-
secutive frames are much overlapped. Rather than keep-
ing accumulated information per time step with brute force,
an intuitive and efﬁcient strategy is utilized to reduce the
redundancy. As errors of previous poses will propagate
over time to current estimation, reﬁning previous results be-
comes necessary. The Memory contains more global infor-
mation, which can be leveraged naturally to reﬁne previous
results. Therefore, a Reﬁning component is introduced. The
Reﬁning module takes the global pose estimation as a regis-
tration problem by aligning each view with the Memory. A
spatial-temporal attention mechanism is applied to the con-
texts stored in the Memory for feature selection.

The overview of our framework is illustrated in Fig. 1.
The encoder encodes paired images into high-level fea-
tures. The Tracking module accepts sequential features as
input, fuses current observation into accumulated informa-
tion using convolution LSTMs [25] for preserving spatial
connections, and produces relative poses. Hidden states of
the Tracking RNN are adaptively preserved in the Memory
slots. The Reﬁning module ameliorates previous results us-
ing another convolutional LSTM, enabling reﬁned results
passing through recurrent units to improve the following
outputs. Our contributions can be summarized as follows:

• We propose a novel end-to-end VO framework consist-
ing of the Tracking, Memory and Reﬁning components;

• An adaptive and efﬁcient strategy is adopted for the
Memory component to preserve accumulated informa-
tion;

• A spatial-temporal attention mechanism is employed
for the Reﬁning component to distill valuable features.

Our method outperforms state-of-the-art learning-based
methods and produces competitive results against classic al-
gorithms. Additionally, it works well in challenging condi-
tions where classic algorithms tend to fail due to insufﬁ-
cient textures or abrupt motions. The rest of this paper is
organized as follows. In Sec. 2, related works on monoc-
ular odometry are discussed. In Sec. 3, our architecture is
described in detail. The performance of the proposed ap-
proach is compared with current state-of-the-art methods in
Sec. 4. We conclude the paper in Sec. 5.

2. Related Works

Visual odometry has been studied for decades, and many
excellent approaches have been proposed. Traditionally,
VO is tackled by minimizing geometric reprojection
errors [10, 18, 20] or photometric errors [7, 8, 30]. These
methods mostly work in regular environments, but will
fail in challenging scenarios such as textureless scenes or
abrupt motions. After the advent of CNNs and RNNs, the
VO task has been explored with deep learning techniques.
A number of approaches have been proposed to deal with
the challenges in classic monocular VO/SLAM systems
such as feature detection [1], depth initialization [28, 34],
scale correction [35], depth representation [2] and data
association [3, 17]. Despite their promising performance,
they utilize the classic framework as backend, and thus
cannot been deployed in an end-to-end fashion.
In this
paper, we mainly focus on learning-based end-to-end
monocular VO works.

Unsupervised methods Mimicking the conventional struc-
ture from motion, SfmLearner [39] learns the single view
depth and ego-motion from monocular image snippets us-
ing photometric errors as supervisory signals. Following the
same scenario, Vid2Depth [19] adopts a differential ICP (It-
erative Closest Point) loss executed on estimated 3D point
clouds to enforce the consistency of predicted depth maps of
two consecutive frames. GeoNet [36] estimates the depth,
optical ﬂow and ego-motion jointly from monocular views.
To cope with the scale ambiguity of motions recovered from
monocular image sequences, Depth-VO-Feat [37] and Un-
DeepVO [16] extend the work of SfmLearner to accept
stereo image pairs as input and recover the absolute scale
with the known baseline.

Although these unsupervised methods break the limi-
tation of requiring massive labeled data for training, only
a limited number of consecutive frames can be processed
in a sequence due to the fragility of photometric losses,
leading to high geometric uncertainty and severe error
accumulation.

Supervised methods DeMoN [29] jointly estimates the
depth and camera poses in an end-to-end fashion by for-
mulating structure from motion as a supervised learning
problem. DeepTAM [38] extends DTAM [21] via two in-
dividual subnetworks indicating tracking and mapping for
the pose and depth estimation respectively. Both DeMoN
and DeepTAM achieve promising results, yet require highly
labeled data (depth, optical ﬂow and camera pose) for train-
ing. MapNet [12] presents an allocentric spatial memory
for localization, but only discrete directions and positions
can be obtained in synthetic environments.

VO can be formulated as a sequential learning prob-
lem via RNNs. DeepVO [31] harnesses the LSTM [13] to

28576

introduce historical knowledge for current relative motion
prediction. Based on DeepVO, ESP-VO [32] infers poses
and uncertainties in a uniﬁed framework. GFS-VO [33]
considers the discriminability of features to different mo-
tion patterns and estimates the rotation and translation sep-
arately with a dual-branch LSTM. In addition, the ConvL-
STM unit [25] is adopted to retain the spatial connections
of features. There are some other works focusing on reduc-
ing localization errors by imposing constraints of relative
poses [4, 14, 22].

Geometric uncertainty can be partially reduced by aggre-
gating more temporal information using RNNs or LSTMs.
Unfortunately, RNNs or LSTMs are limited for remember-
ing long historical knowledge [27]. Here, we extend the
ﬁeld of view by adaptively preserving hidden states of re-
current units as memories. Therefore, previous valuable in-
formation can be inherited longer than being kept in only
the single current hidden state. Besides, all these methods
ignore the importance of new observations for reﬁning pre-
vious poses, which is essential for VO tasks. By incorporat-
ing the Reﬁning module, previous poses can be updated by
aligning ﬁltered features with the Memory. Therefore, error
accumulation is further mitigated.

3. Approach

The encoder extracts high-level features from consecu-
tive RGB images in Sec. 3.1. The Tracking module accepts
sequential features as input, aggregates temporal informa-
tion, and produces relative poses in Sec. 3.2. Hidden states
of the Tracking RNN are adaptively selected to construct
the Memory (Sec. 3.3) for further Reﬁning previous results
in Sec. 3.4. We design the loss function considering both
relative and absolute pose errors in Sec. 3.5.

3.1. Encoder

We harness CNNs to encode images into high-level fea-
tures. Optical ﬂow has been proved useful for estimat-
ing frame-to-frame ego-motion by lots of current works
[22,31–33,38]. We design the encoder based on the Flownet
[6] which predicts optical ﬂow between two images. The
encoder retains the ﬁrst 9 convolutional layers of Flownet
encoding a pair of images, concatenated along RGB chan-
nels, into a 1024-channel 2D feature-map. The process can
be described as:

Xt = F(It−1, It) .

(1)

Xt ∈ RC×H×W denotes the encoded feature-map at time
t by function F from two consecutive images It−1 and It.
H, W and C represent the height, width and channel of
obtained feature maps.

Figure 2. The Tracking module of our framework is implemented
on a convolutional LSTM [25]. Relative camera poses are pro-
duced by the SE (3) layer [5] from the outputs of recurrent units.
Temporal information is preserved in the hidden states.

3.2. Tracking

The Tracking module fuses current observations into ac-
cumulated information and calculates relative camera mo-
tions between two consecutive views as shown in Fig. 2.

Sequence modeling We adopt the prevent LSTM [13]
to model the image sequence. In this case, the feature ﬂow
passing through recurrent units carries rich accumulated in-
formation of previous inputs to infer the current output.
Note that the standard LSTM unit used by DeepVO [31]
and ESP-VO [32] requires 1D vector as input in which the
spatial structure of features is ignored. The ConvLSTM
unit [25], an extension of LSTM with convolution under-
neath, is adopted in the Tracking RNN for preserving the
spatial formulation of visual cues and expanding the ca-
pacity of recurrent units for remembering more knowledge.
The recurrent process can be controlled by

Ot, Ht = U (Xt, Ht−1) .

(2)

Ot denotes the output at time t. Ht and Ht−1 are the hidden
states at current and the last time step.

Relative pose estimation Relative motions can be di-
rectly recovered from paired images. Unfortunately, direct
estimation is prone to error accumulation due to the geo-
metric uncertainty brought by short baselines. The problem
can be mitigated by introducing more historical informa-
tion. Inheriting accumulated knowledge, the output of re-
current unit at each time step is naturally used for pose es-
timation. The SE (3) [5] layer generates the 6-DoF motion
Pt,t−1 from the output at time t.

Theoretically, the global pose of each view can be re-
covered by integrating predicted relative poses as Pt =
Qt
i=1 Pi,i−1P0 (P0 denotes the origin pose of the world co-
ordinate) just as DeepVO [31] and ESP-VO [32]. The ac-
cumulated error, however, will get increasingly severe, and

38577

EncoderConvLSTM SE3 layer,,−,−,−(a)

(b)

(a)

(b)

Figure 3. (a) The Reﬁning module aligns current observation with
the contexts stored in the Memory module for absolute pose es-
timation.
(b) Both contexts and the current observation are re-
organized utilizing the last output as guidance.

Figure 4. Extracting features from Memory using the last output as
guidance. We consider the correlation of both each context stored
in the Memory in (a) and every channel of the context in (b).

thus degrades the performance of the entire system. Due
to the lack of explicit geometric representation of the 3D
environments, neural networks, however, are incapable of
building a global map to assist tracking. Fortunately, the
temporal information is recorded in the hidden states of re-
current units. Although the information is short-time, these
hidden states at different time points can be gathered and re-
organized as parts of an implicit map (discussed in Sec. 3.3).

3.3. Remembering

The Memory module is a neural analogue of the local
map commonly used in classic VO/SLAM systems [20].
Considering the LSTM cannot remember information for
long time [27], we explicitly store hidden states of recur-
rent units at different time points to extend the time span.

A vanilla choice is to take each time step into account
via storing all hidden states over the whole sequence as
M = {m1, m2, ..., mN −1, mN }, where mi denotes the ith
hidden state in the sequence, and N is the size of the mem-
ory buffer. Since contents of two consecutive images are
much overlapped, it’s redundant to remember each hidden
state. Instead, only key states are selected. As the difference
between two frames coincides with the poses, we utilize the
motion distance as a metric to decide if current hidden state
should be stored.

Speciﬁcally, the current hidden state would not be put
into the Memory, unless the parallax between the current
and the latest view in the slot is large enough. Here, the
rotational and translational distances are utilized:

||Rotmi − Rotmi−1 ||2 ≥ θRot ,

||T ransmi − T ransmi−1 ||2 ≥ θT rans .

(3)

(4)

This strategy guarantees both the co-visibility of different
views and the existence of global information. As both pre-
vious and new observations are gathered, the Memory can
be used to optimize previous poses.

3.4. Reﬁning

Once the Memory is constructed, the Reﬁning module
estimates the absolute pose of each view by aligning corre-
sponding observation with the Memory, as shown in Fig. 3.
We adopt another recurrent branch using ConvLSTM, en-
abling previously reﬁned outputs passing through recurrent
units to improve the next estimation, as:

OA

t , H A

t = U A(X A

t , H A

t−1) .

(5)

t , OA

t and H A

X A
t are the input, output and hidden state at
time t. H A
t−1 denotes the hidden state at time t − 1. The U A
indicates the recurrent branch for the Absolute pose estima-
tion. All these variables are 3D tensors to be discussed in
the following sections.

Spatial-temporal attention Although all observations
are fused and distributed in N hidden states, each hidden
state stored in the Memory contributes discriminatively to
different views. In order to distinguish related information,
an attention mechanism is adopted. We utilize the last out-
put OA
t−1 as guidance, since motions between two consecu-
tive views in a sequence are very small.

In speciﬁc, we generate selected memories M

′

t for cur-

rent view t with the function G as:

M

′

t = G(OA

t−1, M ) .

(6)

′

The temporal attention aims to re-weight elements in
the Memory considering the contribution of each mi to the
pose estimation of speciﬁc views. Therefore, as shown in
Fig. 4(a), M
t can be deﬁned as the linear averaging of all el-
ements in M as M
k=1 exp(wi)
denotes the normalized weight. The wi = S(OA
t−1, mi)
is the weight computed according to the cosine similarity
function denoted as S.

i=1 αimi. The αi = exp(wi)

t = PN

PN

′

As all elements in the Memory are formulated as 3D ten-
sors, spatial connections are retained. In this framework,
we focus on not only which element in the Memory plays a

48578

1′2′2′′′12−112…′′00′′′−1……×1×1′−1more important role but also where each element inﬂuences
the ﬁnal results more signiﬁcantly. We try to ﬁnd corre-
sponding co-visible contents at the feature level. Hence, we
extend the attention mechanism from the temporal domain
to the spatial-temporal domain incorporating an additional
channel favored feature attention mechanism. Feature-map
of each channel is taken as a unit and re-weighted for each
view according to the last output. As shown in Fig. 4(b), the
process is described as:

M

′

t =

N

X

i=1

αiC(βi1mi1, βi2mi2, ..., βiC miC) .

(7)

The mij ∈ RH×W denotes the jth channel of the ith el-
ement in the Memory. The βij is the normalized weight
deﬁned on the correlation between the jth channel of Ot−1
and mi. C concatenates all reweighted feature maps along
the channel dimension. We calculate the cosine similarity
between two vectorized feature-maps to assign the correla-
tion weights.

′

Absolute pose estimation The guidance is also executed
on the observations encoded as high-level features to dis-
till related visual cues, denoted as X
t . Both reorganized
memories and observations are stacked along channels and
passed through two convolutional layers with kernel size of
3 for fusion. The fused feature denoted as X A
is the ﬁnal in-
t
put to be fed into convolutional recurrent units. Then the SE
(3) layer calculates the absolute pose from the output OA
t .
Note that, through recurrent units, the hidden state prop-
agating reﬁned results to next time point further improves
the following prediction.

3.5. Loss Function

Our model learns relative and absolute poses in the
Tracking and Reﬁning modules separately. Therefore, con-
sisting of both relative and absolute pose errors, the loss
functions are deﬁned as:

Llocal =

Lglobal =

1
t

t

X

i=1

t

X

i=1

1
i

|| ˆpi−1,i − pi−1,i||2 + k|| ˆφi−1,i − φi−1,i||2,

(|| ˆp0,i − p0,i||2 + k|| ˆφ0,i − φ0,i||2), (9)

(8)

Ltotal = Llocal + Lglobal,

(10)

where ˆpi−1,i, pi−1,i, ˆφi−1,i, and φi−1,i respectively repre-
sent the predicted and ground-truth relative translations and
rotations in three directions; ˆp0,i, p0,i, ˆφ0,i, and φ0,i rep-
resent the predicted and ground-truth absolute translations
and rotations. Llocal, Lglobal and Ltotal denote the local,
global, and total losses respectively. t is the current frame
index in a sequence. k is a ﬁxed parameter for balancing the
rotational and translational errors.

4. Experiments

We ﬁrst discuss the implementation details of our frame-
work in Sec. 4.1. Next, we compare our method with state-
of-the-art approaches on the KITTI [9] and TUM-RGBD
[26] datasets in Sec. 4.2 and Sec. 4.3, respectively. Finally,
an ablation study is performed in Sec. 4.4.

4.1. Implementation

Training Our network takes monocular RGB image se-
quences as input. The image size can be arbitrary because
our model has no requirement of compressing features into
vectors as DeepVO [31] and ESP-VO [32]. We use 11 con-
secutive images to construct a sequence, yet our model can
accept dynamic lengths of inputs. The parameter k is set to
100 and 1 for the KITTI and TUM-RGBD dataset. The θRot
and θT rans are set to 0.005 (rad) and 0.6 (m) for the KITTI
dataset. While for the TUM-RGBD dataset, the values are
0.01 (rad) and 0.01 (m). The buffer size N is initialized
with the sequence length, yet the buffer can be used without
being fully occupied.

Network The encoder is pretrained on the FlyingChairs
dataset [6], while other parts of the network are initialized
with MSRA [11]. Our model is implemented by PyTorch
[23] on an NVIDIA 1080Ti GPU. Adam [15] with β1 =
0.9, β2 = 0.99 is used as the optimizer. The network is
trained with a batch size of 4, a weight decay of 4 × 10−4
for 150,000 iterations in total. The initial learning rate is set
to 10−4 and reduced by half every 60,000 iterations.

4.2. Results on the KITTI Dataset

The KITTI dataset [9], one of the most inﬂuential out-
door VO/SLAM benchmark datasets, is widely used in both
classic [10, 20] and learning-based works [16, 19, 31, 32,
36, 37, 39]. It consists of 22 sequences captured in urban
and highway environments at a relatively low sample fre-
quency (10 fps) at the speed up to 90km/h. Seq 00-10
provide raw data with ground-truth represented as 6-DoF
motion parameters considering the complicated urban envi-
ronments, while Seq 11-21 provide only raw data. In our
experiments, the left RGB images are resized to 1280 x 384
for training and testing. We adopt the same train/test split
as DeepVO [31] and GFS-VO [33] by using Seq 00, 02, 08,
09 for training and Seq 03, 04, 05, 06, 07, 10 for evaluation.
Baseline methods The learning-based baselines include
supervised approaches such as DeepVO [31], ESP-VO
[32], GFS-VO [33], and unsupervised approaches such
as SfmLearner [39], Depth-VO-Feat [37], GeoNet [36],
Vid2Depth [19] and UndeepVO [16]. Monocular VISO2
[10] (VISO2-M) and ORB-SLAM2 [20] are used as clas-
sic baselines. The error metrics, i.e., averaged Root Mean
Square Errors (RMSE) of the translational and rotational
errors, are adopted for all the test sequences of the lengths
ranging from 100, 200 to 800 meters.

58579

Method

03

04

05

Sequence

06

07

10

Avg

trel

rrel

trel

rrel

trel

rrel

trel

rrel

trel

rrel

trel

rrel

trel

rrel

5.00 6.17

3.40 1.50
4.94 2.35
9.09 7.54 20.12 7.67

UnDeepVO [16]
5.49 2.13
Depth-VO-Feat [37] 15.58 10.69 2.92 2.06
GeoNet [36]
Vid2Depth [19]
SfmLearner [39]
DeepVO [31]
ESP-VO [32]
GFS-VO-RNN [33]
GFS-VO [33]
Ours

19.21 9.78
27.02 10.39 18.92 1.19 51.13 21.86 58.07 26.83 51.22 36.64 21.54 12.54 37.98 18.24
15.91 4.67
10.78 3.92
5.96 6.12
8.49 6.89
6.15 6.66
6.72 6.46
7.68 3.22
6.36 3.62
5.44 3.32
4.97 2.26
3.32 2.10 2.96 1.76 2.59 1.25 4.93 1.90 3.07 1.76 3.94 1.72 3.47 1.75

4.49 5.24 18.67 4.10
2.62 3.61
7.19 6.97
6.33 6.08
3.35 4.93
5.85 2.55
5.95 2.36
2.91 1.30
3.27 1.62

25.88 4.80
5.42 5.82
7.24 7.29
14.58 4.98
8.50 2.74

21.33 6.65
3.91 4.60
3.52 5.02
5.88 2.64
3.37 2.25

14.33 3.30
8.11 8.83
9.77 10.2
7.44 3.19
6.32 2.33

5.65 3.15
7.98 4.04
13.12 7.38

10.63 4.65
12.45 3.46
20.73 9.04

6.20 1.98
5.80 2.07
9.28 4.34

3.15 2.48
6.48 3.60
8.27 5.93

trel : average translational RMSE drift (%) on length from 100, 200 to 800 m.
rrel : average rotational RMSE drift (◦/100m) on length from 100, 200 to 800 m.

Table 1. Results on the KITTI dataset. DeepVO [31], ESP-VO [32], GFS-VO [33] and our model are supervised methods trained on Seq
00, 02, 08 and 09. SfmLearner [39], GeoNet [36], Vid2Depth [19], Depth-VO-Feat [37], and UndeepVO [16] are trained on Seq 00-08 in
an unsupervised manner. The results of SfmLearner and UnDeepVO are from [34], while for GeoNet, Vid2Depth and Depth-VO-Feat, the
poses are recovered from ofﬁcially released pretrained models. The best results are highlighted.

(a) Translation against path length.

(b) Rotation against path length.

(c) Translation against speed.

(d) Rotation against speed.

Figure 5. Average errors on translation and rotation against differ-
ent path lengths and speeds.

Comparison with learning-based methods As shown
in Table 1, our method outperforms DeepVO [31], ESP-
VO [32] and GFS-VO-RNN [33] (without motion decou-
pling) on all of the test sequences by a large margin. Since
DeepVO, ESP-VO and GFS-VO only consider historical
knowledge stored in a single hidden state, error accumulates
severely. The problem is partially mitigated by considering
the discriminative ability of features to different motion pat-
terns in GFS-VO, while our method is more effective.

Meanwhile, we provide the results of unsupervised ap-
proaches in Table 1. As monocular VO methods includ-

ing SfmLearner [39], GeoNet [36], Vid2Depth [19] suffer
from scale ambiguity, frame-to-frame motions of short se-
quence snippets are aligned individually with ground-truths
to ﬁx scales. Although they achieve promising performance
on sequences consisting of 5 (SfmLearner, GeoNet) or 3
(Vid2Depth) frames, they suffer from heavy error accumu-
lation when integrating poses over the entire sequence. Ben-
eﬁted from stereo images in scale recovery, UnDeepVO
[16] and Depth-VO-Feat [37] obtain competitive results
against DeepVO, ESP-VO, and GFS-VO, while our results
are still much better. Note that only monocular images are
used in our model.

We further evaluate the average rotation and translation
errors for different path lengths and speeds in Fig. 5. The
accumulated errors over long path lengths are effectively
mitigated by our method owing to the new information for
reﬁning previous results. Moreover, this advantage of our
algorithm can also be seen in handling high speed situa-
tions. GFS-VO [33] also achieves promising rotation esti-
mation by decoupling the motions. Unfortunately, it does
not provide robust translation results.

Comparison with classic methods The results of
VISO2-M [10], ORB-SLAM2 [20] (with and without loop
closure), and our method are shown in Table 2. VISO2-
M is a pure monocular VO algorithm recovering frame-
wise poses. ORB-SLAM2, however, is a strong baseline,
because both versions utilize local bundle adjustment for
jointly optimizing poses and a global map. Our model
outperforms VISO2-M consistently. ORB-SLAM2 [20]
achieves superior performance in terms of rotation esti-
mation owing to the global explicit geometric constraints.
However, it suffers more from error accumulation in trans-
lation on long sequences (Seq 05, 06, 07) than our approach,

68580

Seq Ours

VISO2-M

[10]

Method
ORB-SLAM2

[20]

trel rrel

trel

rrel

trel

rrel

ORB-SLAM2

(LC) [20]
rrel

trel

03 3.32 2.10 8.47 8.82 2.28
0.40 2.17
04 2.96 1.76 4.69 4.49 1.41
0.14 1.07
05 2.59 1.25 19.22 17.58 13.21 0.22 1.86
06 4.93 1.90 7.30 6.14 18.68 0.26 4.96
07 3.07 1.76 23.61 19.11 10.96 0.37 1.87
10 3.94 1.72 41.56 32.99 3.71
0.30 3.76
0.28 2.62
Avg 3.47 1.75 17.48 16.52 8.38

0.39
0.17
0.24
0.18
0.39
0.29
0.28

Table 2. Results of VISO2-M [10], ORB-SLAM2 (with and with-
out loop closure) [20] and our method on the KITTI dataset.

which is reduced by global bundle adjustment. While for
short sequences (Seq 03, 04, 10), performances of the two
versions and our method are very close. The small differ-
ences between the results of ORB-SLAM2 with loop close
and our method suggest that global information is retained
and effectively used by our novel framework.

A visualization of the trajectories estimated by Depth-
VO-Feat, GFS-VO, ORB-SLAM2 and our method is illus-
trated in Fig. 6. Depth-VO-Feat suffers from sever error
accumulation though trained on stereo images. GFS-VO
and ORB-SLAM2 produces close results with our model in
simple environments (Seq 03, 10), while our method out-
performs them in complicated scenes (Seq 05, 07).

4.3. Results on the TUM RGBD Dataset

We test the generalization ability of our model on the
TUM-RGBD dataset [26], a prevalent public benchmark
used by a number of VO/SLAM algorithms [8, 20, 38].
The dataset was collected by handheld cameras in in-
door environments with various conditions including dy-
namic objects, textureless regions and abrupt motions. The
dataset provides both color and depth images, while only
the monocular RGB images are used in our experiments.
Different from datasets captured by moving cars, motions
in this benchmark contain complicated patterns due to the
handheld capture mode. We select some sequences for
training and others for testing (The details can be found in
the supplementary material), and evaluate the performance
in both regular and challenging conditions using the aver-
aged Absolute Trajectory Errors (ATE).

Comparison with classic methods Since few monocu-
lar learning-based VO algorithms have attempted to handle
complicated motions recorded by handheld cameras, we al-
ternatively compare our approach against current state-of-
the-art classic methods including ORB-SLAM2 [20] and
DSO [7]. As shown in Table 3, they yield promising re-
sults on scenes with rich textures (fr2/desk, fr2/360 kidnap,
fr3/sitting static, fr3/nstr tex near loop, fr3/str tex far), yet

Figure 6. The trajectories of ground-truth, ORB-SLAM2 [20],
Depth-VO-Feat [37], GFS-VO [33] and our model on Seq 03, 05,
07 and 10 (from left to right) of the KITTI benchmark.

our results are comparable.

As ORB-SLAM2 [20] relies on ORB [24] features
to establish correspondences,
it fails in scenes with-
out rich textures (fr3/nstr ntex near loop, fr3/str ntex far,
fr2/large cabinet). Utilizing pixels with large gradients
for tracking, DSO [7] works well in scenes with struc-
tures or edges (fr3/str ntex far, fr3/str tex far).
It can-
not achieve good performance when textures are insuf-
ﬁcient. Both ORB-SLAM2 and DSO can hardly work
in scenes without texture and structure (fr2/large cabinet,
fr3/nstr ntex near loop) and tend to fail when facing abrupt
motions (fr2 pioneer 360, fr2/ pioneer slam3). In contrast,
our method is capable of dealing with these challenges ow-
ing to the ability of deep learning in extracting high-level
features, and the efﬁcacy of our proposal for error reduc-
tion. A visualization of trajectories is shown in Fig. 7.

4.4. Ablation Study

Table 3 also shows an ablation study illustrating the im-
portance of each component in our framework. The base-
line is our model removing the Memory and Reﬁning mod-
ules, similar to [31–33]. The Tracking model works poorly
in both regular and challenging conditions, because histor-
ical knowledge in a single hidden state is inefﬁcient to re-
duce accumulated errors. Fortunately, the Memory compo-
nent mitigates the problem by explicitly introducing more
global information and considerably improves results of the
Tracking model both on regular and challenging sequences.
We further test the spatial-temporal attention strategy
adopted for selecting features from memories and ob-
servations by removing the temporal attention and spa-
tial attention progressively. We observe that both of the
two attention techniques are crucial to improve the re-

78581

Figure 7. The raw images (top) and trajectories (bottom) recovered by our method on the TUM-RGBD dataset [26] (from left to right:
fr3/str tex far, fr2/poineer 360, fr3/str ntex far, fr3/nstr ntex near loop). Trajectories are aligned with ground-truths for scale recovery.

Sequence

fr2/desk

fr2/360 kidnap
fr2/pioneer 360
fr2/pioneer slam3
fr2/large cabinet
fr3/sitting static

fr3/nstr ntex near loop
fr3/nstr tex near loop

fr3/str ntex far
fr3/str tex far

Desc.

str/tex/abrupt motion

Frames

Y/Y/N
Y/Y/N
Y/Y/Y
Y/Y/Y
Y/N/N
Y/Y/N
N/N/N
N/Y/N
Y/N/N
Y/Y/N

2965
1431
1225
2544
1011
707
1125
1682
814
938

ORB-SLAM2

[20]
0.041
0.184

X
X
X
X
X

0.057

X

0.018

DSO
[7]
X

0.197

X

0.737

X

0.082

X

0.093
0.543
0.040

Ours

Ours

Ours

(tracking)

(w/o temp atten)

(w/o spat atten)

0.183
0.313
0.241
0.149
0.193
0.017
0.371
0.046
0.069
0.080

0.164
0.225
0.1338
0.1065
0.193
0.018
0.195
0.011
0.047
0.049

0.159
0.224
0.076
0.085
0.177
0.017
0.157
0.010
0.039
0.046

Ours

0.153
0.208
0.056
0.070
0.172
0.015
0.123
0.007
0.035
0.042

Table 3. Evaluation on the TUM-RGBD dataset [26]. The values describe the translational RMSE in [m/s]. Results of ORB-SLAM2 [20]
and DSO [7] are generated from the ofﬁcially released source code with recommended parameters. Ours (tracking) is a network which
contains only the tracking component. Ours (w/o temp atten) indicates the model averaging the all memories as input without temporal
attention. Ours (w/o spat atten) is the model removing the spatial attention yet retaining the temporal attention.

sults, especially in challenging conditions (fr2/pioneer 360,
fr2/pioneer slam3, fr3/nstr ntex near loop).

5. Conclusion

In this paper, we present a novel framework for learn-
ing monocular visual odometry in an end-to-end fashion.
In the framework, we incorporate two helpful components
called Memory and Reﬁning, which focus on introducing
more global information and ameliorating previous results
with these information respectively. We utilize an adap-
tive and efﬁcient selection strategy to construct the Mem-
ory. Besides, a spatial-temporal attention mechanism is em-
ployed for feature selection when recovering the absolute
poses in the Reﬁning module. The reﬁned results propa-
gating information through recurrent units, further improve
the following estimation. Experiments demonstrate that our

model outperforms previous learning-based monocular VO
methods and gives competitive results against classic VO
approaches on the KITTI and TUM-RGBD benchmarks re-
spectively. Moreover, our model obtains outstanding results
under challenging conditions including texture-less regions
and abrupt motions, where classic methods tend to fail.

In the future, we consider to extend the work to a full
SLAM system consisting tracking, mapping and global op-
timization. Moreover, auxiliary information, such as IMU
and GPS data will also be introduced to enhance the system.

Acknowledgement

The work is supported by the National Key Research and
Development Program of China (2017YFB1002601) and
National Natural Science Foundation of China (61632003,
61771026).

88582

References

[1] P. Agrawal, J. Carreira, and J. Malik. Learning to See by

Moving. In ICCV, 2015. 2

[2] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and
A. J. Davison. CodeSLAM-Learning a Compact, Optimis-
able Representation for Dense Visual SLAM.
In CVPR,
2018. 2

[3] S. L. Bowman, N. Atanasov, K. Daniilidis, and G. J. Pap-
pas. Probabilistic Data Association for Semantic SLAM. In
ICRA, 2017. 2

[4] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz. Map-
Net: Geometry-aware Learning of Maps for Camera Local-
ization. In CVPR, 2018. 1, 3

[5] R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen.
VidLoc: A Deep Spatio-temporal Model for 6-DoF Video-
clip Relocalization. In CVPR, 2017. 3

[6] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
Flownet: Learning Optical Flow with Convolutional Net-
works. In ICCV, 2015. 1, 3, 5

[7] J. Engel, V. Koltun, and D. Cremers. Direct Sparse Odome-

try. TPAMI, 2018. 1, 2, 7, 8

[8] J. Engel, T. Sch¨ops, and D. Cremers. LSD-SLAM: Large-

scale Direct Monocular SLAM. In ECCV, 2014. 1, 2, 7

[9] A. Geiger, P. Lenz, and R. Urtasun. Are We Ready for Au-
tonomous Driving? The KITTI Vision Benchmark Suite. In
CVPR, 2012. 5

[10] A. Geiger, J. Ziegler, and C. Stiller. Stereoscan: Dense 3D

Reconstruction in Real-time. In IV, 2011. 1, 2, 5, 6, 7

[11] K. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rec-
tiﬁers: Surpassing Human-level Performance on Imagenet
Classiﬁcation. In ICCV, 2015. 5

[12] J. F. Henriques and A. Vedaldi. MapNet: An Allocentric
Spatial Memory for Mapping Environments. In CVPR, 2018.
1, 2

[13] S. Hochreiter and J. Schmidhuber. Long Short-term Mem-

ory. Neural Computation, 1997. 2, 3

[14] G. Iyer, J. K. Murthy, K. Gunshi Gupta, and L. Paull. Ge-
ometric Consistency for Self-supervised End-to-end Visual
Odometry. In CVPR Workshops, 2018. 1, 3

[15] D. P. Kingma and J. Ba. Adam: A Method for Stochastic

Optimization. In ICLR, 2015. 5

[16] R. Li, S. Wang, Z. Long, and D. Gu. UnDeepVO: Monocular
Visual Odometry through Unsupervised Deep Learning. In
ICRA, 2018. 1, 2, 5, 6

[17] K.-N. Lianos, J. L. Sch¨onberger, M. Pollefeys, and T. Sattler.

VSO: Visual Semantic Odometry. In ECCV, 2018. 2

[18] H. Liu, M. Chen, G. Zhang, H. Bao, and Y. Bao. ICE-BA:
Incremental, Consistent and Efﬁcient Bundle Adjustment for
Visual-Inertial SLAM. In CVPR, 2018. 2

[19] R. Mahjourian, M. Wicke, and A. Angelova. Unsupervised
Learning of Depth and Ego-motion from Monocular Video
Using 3D Geometric Constraints. In CVPR, 2018. 1, 2, 5, 6
[20] R. Mur-Artal and J. D. Tard´os. ORB-SLAM2: An Open-
source SLAM System for Monocular, Stereo, and RGB-D
Cameras. T-RO, 2017. 1, 2, 4, 5, 6, 7, 8

[21] R. A. Newcombe, S. J. Lovegrove, and A. J. Davison.
In

DTAM: Dense Tracking and Mapping in Real-time.
ICCV, 2011. 2

[22] E. Parisotto, D. Singh Chaplot, J. Zhang, and R. Salakhutdi-
nov. Global Pose Estimation with an Attention-based Recur-
rent Network. In CVPR Workshops, 2018. 1, 3

[23] A. Paszke, S. Gross, S. Chintala, and G. Chanan. Pytorch.
https://github.com/pytorch/pytorch, 2017. 5
[24] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski. ORB:
An Efﬁcient Alternative to SIFT or SURF. In ICCV, 2011. 7
[25] X. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong, and W. Woo.
Convolutional LSTM Network: A Machine Learning Ap-
proach for Precipitation Nowcasting. In NIPS, 2015. 2, 3

[26] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers. A Benchmark for the Evaluation of RGB-D SLAM
Systems. In IROS, 2012. 5, 7, 8

[27] S. Sukhbaatar, a. szlam, J. Weston, and R. Fergus. End-to-

end Memory Networks. In NIPS, 2015. 1, 3, 4

[28] K. Tateno, F. Tombari, I. Laina, and N. Navab. CNN-SLAM:
Real-time Dense Monocular SLAM with Learned Depth Pre-
diction. In CVPR, 2017. 2

[29] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg,
A. Dosovitskiy, and T. Brox. DeMoN: Depth and Motion
Network for Learning Monocular Stereo. In CVPR, 2017. 2
[30] R. Wang, M. Schworer, and D. Cremers. Stereo DSO: Large-
scale Direct Sparse Visual Odometry with Stereo Cameras.
In ICCV, 2017. 1, 2

[31] S. Wang, R. Clark, H. Wen, and N. Trigoni. DeepVO:
Towards End-to-end Visual Odometry with Deep Recurrent
Convolutional Neural Networks. In ICRA, 2017. 1, 2, 3, 5,
6, 7

[32] S. Wang, R. Clark, H. Wen, and N. Trigoni.

End-to-
end, Sequence-to-sequence Probabilistic Visual Odometry
through Deep Neural Networks. IJRR, 2018. 1, 3, 5, 6, 7

[33] F. Xue, Q. Wang, X. Wang, W. Dong, J. Wang, and H. Zha.
In

Guided Feature Selection for Deep Visual Odometry.
ACCV, 2018. 1, 3, 5, 6, 7

[34] N. Yang, R. Wang, J. St¨uckler, and D. Cremers. Deep Vir-
tual Stereo Odometry: Leveraging Deep Depth Prediction
for Monocular Direct Sparse Odometry. In ECCV, 2018. 1,
2, 6

[35] X. Yin, X. Wang, X. Du, and Q. Chen. Scale Recovery for
Monocular Visual Odometry Using Depth Estimated with
Deep Convolutional Neural Fields. In ICCV, 2017. 2

[36] Z. Yin and J. Shi. GeoNet: Unsupervised Learning of Dense
Depth, Optical Flow and Camera Pose. In CVPR, 2018. 1,
2, 5, 6

[37] H. Zhan, R. Garg, C. Saroj Weerasekera, K. Li, H. Agarwal,
and I. Reid. Unsupervised Learning of Monocular Depth
Estimation and Visual Odometry with Deep Feature Recon-
struction. In CVPR, 2018. 1, 2, 5, 6, 7

[38] H. Zhou, B. Ummenhofer, and T. Brox. DeepTAM: Deep

Tracking and Mapping. In ECCV, 2018. 2, 3, 7

[39] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsu-
pervised Learning of Depth and Ego-motion from Video. In
CVPR, 2017. 1, 2, 5, 6

98583

