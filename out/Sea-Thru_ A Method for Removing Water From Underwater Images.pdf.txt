Sea-thru: A Method For Removing Water From Underwater Images

Derya Akkaynak

Tali Treibitz

University of Haifa

derya.akkaynak@gmail.com, ttreibitz@univ.haifa.ac.il

Abstract

Robust recovery of lost colors in underwater images re-
mains a challenging problem. We recently showed that
this was partly due to the prevalent use of an atmospheric
image formation model for underwater images and pro-
posed a physically accurate model. The revised model
showed: 1) the attenuation coefﬁcient of the signal is not
uniform across the scene but depends on object range and
reﬂectance, 2) the coefﬁcient governing the increase in
backscatter with distance differs from the signal attenua-
tion coefﬁcient. Here, we present the ﬁrst method that re-
covers color with our revised model, using RGBD images.
The Sea-thru method estimates backscatter using the dark
pixels and their known range information. Then, it uses
an estimate of the spatially varying illuminant to obtain
the range-dependent attenuation coefﬁcient. Using more
than 1,100 images from two optically different water bod-
ies, which we make available, we show that our method with
the revised model outperforms those using the atmospheric
model. Consistent removal of water will open up large un-
derwater datasets to powerful computer vision and machine
learning algorithms, creating exciting opportunities for the
future of underwater exploration and conservation.

1. Introduction

Reconstructing colors in underwater images is a chal-
lenging task for which no robust algorithm currently ex-
ists. We recently showed that the commonly used image
formation model was partly to blame [1], because it was
derived for the atmosphere [48] and neglected the strong
wavelength dependency of light underwater. We proposed
a revised model that showed: 1) direct and backscattered
signals are governed by distinct coefﬁcients (the old model
assumed them to be the same), and 2) each of these coef-
ﬁcients have dependencies on factors other than the opti-
cal properties of the water (the old model ignored them).
While the revised model is physically more accurate, it has
extra parameters making its application difﬁcult. Here, we
present the Sea-thru method that outlines how to estimate
these parameters for better scene recovery.

Figure 1. The Sea-thru method removes water from underwater
images. Best viewed online for color and details.

Large image datasets like ImageNet [20] have been in-
strumental in igniting the artiﬁcial intelligence boom, which
fueled many important discoveries in science and indus-
try in the last two decades [39]. The underwater domain,
which has no shortage of large image datasets, however,
has not beneﬁted from the full power of computer vision
and machine learning methods which made these discov-
eries possible, partly because water masks many computa-
tionally valuable features of a scene. An underwater photo
is the equivalent of one taken in air, but covered in thick,
colored fog, subject to an illuminant whose white point and
intensity changes as a function of distance.
It is difﬁcult
to train learning-based methods for different optical condi-
tions that represent the global ocean, because calibrated un-
derwater datasets are expensive and logistically difﬁcult to

11682

ronment [52]. For scene recovery, these methods required
more than one frame of the scene, or extra information such
as 3D structure. This model was further simpliﬁed to in-
clude only one attenuation coefﬁcient, uniform across all
color channels. This was done to enable recovery from sin-
gle images in haze [8, 26, 33, 57] and later used also for
underwater recovery [17, 19, 21, 44, 49]. While using the
same coefﬁcient for all color channels in underwater scenes
is a very crude approximation [1], using a coefﬁcient per
channel can yield decent results [9, 13, 52, 56, 58]. Nev-
ertheless, as we further show their accuracy is inherently
limited by the model.

Backscatter was previously estimated from single im-
ages using DCP [33], some variants of it [17, 19, 21, 44],
or other priors [9, 49]. Attenuation coefﬁcients can be mea-
sured by ocean optics instruments such as transmissiome-
ters or spectrometers [11]. However, they cannot be used
as-is for imaging because of differences in spectral sensitiv-
ity and acceptance angle. Plus, these instruments are expen-
sive and cumbersome to deploy. Thus, it is best to estimate
the attenuation coefﬁcients directly from images. The most
basic method for that is to photograph a calibration target at
known distances [60]. In [63], coefﬁcients are taken from
the estimated veiling-light, ignoring the illumination color.
In [9] the attenuation coefﬁcients per channel were esti-
mated using the grey-world assumption. Others [19, 44, 55]
alleviate this problem by using ﬁxed attenuation coefﬁcients
measured for just one water type.

Known distances slightly simplify the problem and were
used to estimate backscatter together with attenuation by
ﬁtting data from multiple images to the image formation
model [13, 51, 55]. Deep networks were recently used for
reconstructing underwater scenes [43, 53]. Their training,
however, relies on purely synthetic data, and thus highly
depends on the quality of the simulation models. All the
methods so far assume the attenuation coefﬁcients are only
properties of the water and are uniform across the scene per
color channel, but as we showed in [1, 2], this is an incorrect
assumption that leads to errors in reconstruction.

3. Scientiﬁc Background

Underwater image formation is governed by:

Ic = Dc + Bc ,

(1)

where c = R,G,B is the color channel, Ic is the image
captured by the camera (with distorted colors), Dc is the di-
rect signal which contains the information about the (atten-
uated) scene, and Bc is the backscatter, an additive signal
that degrades the image due to light reﬂected from particles
suspended in the water column (Fig. 2). The components
Dc and Bc are governed by two distinct coefﬁcients βD
c and
βB
c , which are wideband (RGB) attenuation and backscatter
coefﬁcients, respectively [1, 2].

1683

Figure 2. Underwater image formation is governed by an equation
of the form Ic = Dc + Bc. Dc contains the scene with attenuated
colors, and Bc is a degrading signal that strongly depends on the
optical properties of the water, and eventually dominates the image
(shown here for a gray patch). Insets show relative magnitudes of
Dc and Bc for a Macbeth chart imaged at 27m in oceanic water.

acquire. Existing methods that attempt to reverse the degra-
dation due to water are either unstable, too sensitive, or only
work for short object ranges. Thus, the analysis of large un-
derwater datasets often requires costly manual effort. On
average, a human expert spends over 2 hours identifying
and counting ﬁsh in a video that is one hour long [59].

The Sea-thru method aims to consistently remove
water from underwater images, so large datasets can
be analyzed with increased efﬁciency.
It works as fol-
lows: given an RGBD image,
it estimates backscatter
in a way inspired from the Dark Channel Prior (DCP)
developed for haze [34], but utilizing the known range
map. Next, it uses an optimization framework to esti-
mate the range-dependent attenuation coefﬁcient using
an illumination map obtained using local space average
color [23] as input. We show that the distance-dependent
attenuation coefﬁcient can be modeled as a 2-term ex-
ponential, which greatly reduces the unknowns in the
optimization step. We contribute more than 1,100 images
acquired in two optically different water types (avail-
able at http://csms.haifa.ac.il/profiles/
tTreibitz/datasets/sea_thru/index.html).
On these images and another underwater RGBD dataset
contributed by [7], we show qualitatively and quantitatively
that Sea-thru, which is the ﬁrst to utilize the revised image
formation model, outperforms others that use the old
model.

2. Related Works

The image formation model for bad weather was devel-
oped by Nayar and Narasimhan [48]. It was assumed that
the scattering coefﬁcient is constant over the camera sensi-
tivity range in each color channel, resulting in a coefﬁcient
per wavelength. This model then became extensively used
for bad weather, and later adapted for the underwater envi-

The expanded form of Eq. 1 is given as [1]:

Ic = Jce−βD

c (vD)·z + B∞

c (cid:16)1 − e−βB

c (vB )·z(cid:17) ,

(2)

c and βB

where z is range (distance) between the camera and the
objects in the scene along the line of sight, B∞
is veil-
c
ing light, and Jc is the unattenuated scene that would be
captured at the location of the camera had there been no
attenuation along z. Vectors vD = {z, ρ, E, Sc, β} and
vB = {E, Sc, b, β} represent the dependencies of the co-
efﬁcients βD
c on range z, reﬂectance ρ, spectrum of
ambient light E, spectral response of the camera Sc, and
the physical scattering and beam attenuation coefﬁcients
of the water body, b and β, all of which are functions of
wavelength λ. Previously, it was assumed that βD
c = βB
c ,
and that these coefﬁcients had a single value for a given
scene [9], but in [1] we have shown that they are distinct,
and furthermore, that they had dependencies on different
factors. Eq. 2 is formulated for imaging in the horizontal
direction. However, throughout this work, we apply it to
scenes captured in different directions with the assumption
that the deviations are small. Future work should test the
applicability of Eq. 2 to different imaging directions.

The equations connecting RGB coefﬁcients βD

c and βB
c

to wavelength dependent physical quantities are [1]:

.∆z ,



#.z .

(3)

Sc(λ)ρ(λ)E(d, λ)e−β(λ)zdλ

Sc(λ)ρ(λ)E(d, λ)e−β(λ)(z+∆z)dλ

βD

c = ln


λ1

Z λ2
Z λ2

λ1

βB

c = − ln"1 − R λ2

λ1

Sc(λ)B∞(λ)(1 − e−β(λ)z)dλ
R λ2

B∞(λ)Sc(λ)dλ

λ1

(4)
Here, λ1 and λ2 are the limits of the visible range (400
and 700nm), E is the spectrum of ambient light at depth
d. Light penetrating vertically attenuates based on the dif-
fuse downwelling attenuation Kd(λ) [47], different than the
beam attenuation coefﬁcient β(λ) [1, 47] which is solely a
function of the type, composition, and density of dissolved
substances in the ocean [47]. If E(0, λ) is light at the sea
surface, then E(d, λ) at depth d is [2]:

E(d, λ) = E(0, λ)e−Kd(λ)d .

The veiling light B∞
c

in Eq. 2 is given as:

B∞

c =Z λ2

λ1

Sc(λ)B∞(λ)dλ ,

where

B∞(λ) =(cid:2)b(λ)E(d, λ)(cid:3)(cid:14)β(λ) .

(5)

(6)

(7)

Figure 3. a) A 3D model created from 68 photographs using Pho-
toscan Professional (Agisoft LLC). b) Range map z (in meters) for
the image in Fig. 1 obtained from this model. We placed a color
chart on the seaﬂoor to set the scale.

4. The Sea-thru Method

Based on Eqs. 2-4, to recover Jc the following parame-
ters need to be known or estimated: optical water type de-
termined by b, β, and Kd; light Ed, distance between the
camera and scene along the line-of-sight z, depth at which
the photo was taken d, the reﬂectance of each object in the
scene ρ, and the spectral response of the camera Sc. These
parameters are rarely, if ever, known at the time an under-
water photo is taken. In [1] we showed that βD
c was most
strongly governed by z, and βB
c was most affected by the
optical water type and illumination E. Therefore, we tai-
lor the Sea-thru method to tackle these speciﬁc dependen-
cies. Since the coefﬁcients vary with imaging angle and ex-
posure [1], we assume that they generally cannot be trans-
ferred across images, even those taken sequentially with the
same camera, and we estimate the relevant parameters for a
given image from that image only.

4.1. Imaging and Range Map Generation

As βD

c heavily depends on z we require having a range
map of the scene, which we obtain using structure-from-
motion (SFM), commonly used underwater to measure
structural complexity of reefs (e.g., [12, 15, 16, 27, 28, 42])
and in archaeology (e.g., [45, 46]). Our method requires an
absolute value for z, whereas SFM provides range only up
to scale, so we placed objects of known sizes in the scene
(Fig. 3). When imaging from underwater vehicles, their
navigation sensors can provide velocity or altitude. An al-
ternative is stereo imaging (e.g., [7, 9, 35, 54]), which re-
quires the use of two synchronized cameras, and a straight-
forward in-water calibration before imaging survey begins.

4.2. Scene Reconstruction

From Eqs. 1 & 2 we have:

Jc = DceβD

c (z)z ,

(8)

where Dc = Ic − Bc. Here we explicitly kept the z depen-
dency of βD
c as we must account for it, but we will ignore
other dependencies. Jc is an image whose colors are only
corrected along z, and depending on the imaging geome-
try, it may need further correction to achieve the colors of

1684

we model as:

ˆBc = B∞

c z) + J

′

ce−βD′

c z

,

(10)

c (1 − e−βB
ce−βD′

′

c z represents a small residual
where the expression J
term that behaves like the direct signal. Using non-linear
least squares ﬁtting, we estimate parameters B∞
c , J ′
c,
and βD
′ subject to the bounds [0, 1], [0, 5], [0, 1], and [0, 5],
c
respectively. For this step, we ignore the z-dependency of
βD
′. If information about the camera sensor, water type,
c
etc., is available, the bounds for βD
c can be further
reﬁned using the loci described in [1, 2].

c and βB

c , βB

Figure 4. A color chart imaged at various ranges. Top row in a
shows the raw images Ic, and bottom row shows the corresponding
Dc, or backscatter-removed images. b) Bc calculated for each
color channel with the method described here (x’s), and the color-
chart based backscatter calculation method described in [1] (o’s);
values are almost identical.

an image that was taken at sea surface. Let Js denote the
image taken at the surface. Then,

Js = Jc/Wc ,

(9)

where Wc is the white point of the ambient light at the cam-
era (i.e., at depth d), and Js is Jc globally white balanced.

4.3. Backscatter Estimation

Backscatter increases exponentially with z, and eventu-
ally saturates (Fig. 2). Where scene reﬂectance ρc → 0
(all light absorbed), or E → 0 (complete shadow), the cap-
tured RGB intensity Ic → Bc. We search the image for very
dark or shadowed pixels, and use them to get an initial es-
timate of backscatter. Our approach is similar to DCP in
that it attempts to ﬁnd the backscattered signal where the
Dc is minimum, but it differs in the fundamental way that
we utilize a known range map, rather than try to estimate it.
Additionally, we search for the darkest RGB triplets rather
than ﬁnding the darkest pixels independently in each color
channel and we do not form a dark channel image. The
small number of unconnected pixels our method identiﬁes
is sufﬁcient, because we have the corresponding range in-
formation, and a physical model of how Bc behaves with z
(but see note about imaging angle in Sec. 3).

We estimate backscatter as follows: ﬁrst we partition the
range map into 10 evenly spaced clusters spanning the min-
imum and maximum values of z. In each range cluster, we
search Ic for the RGB triplets in the bottom 1 percentile,
which we denote by Ω. Then across the whole image,
ˆBc(Ω) ≈ Ic(Ω) is an overestimate of backscatter, which

Depending on the scene, the residual can be left out of
Eq. 10 if the reﬂectance of found dark pixels are perfect
black; if they are under a shadow; if z is large; or if the water
is extremely turbid (Bc ≫ Dc). In all other cases, the inclu-
sion of the residual term is important. In reef scenes, due to
their complex 3D structure, there are often many shadowed
pixels which provide direct estimates of backscatter.

Fig. 4 demonstrates the performance of this method in
a calibrated experiment. We mounted a chart on a buoy
line in blue water (to minimize interreﬂections from to the
seaﬂoor and surface), and photographed as we swam to-
wards it. The veiling effect of backscatter is clearly visible
in the far images, decreasing as z between the camera and
the chart decreases (Fig. 4a). For each image, we calculated
the ground-truth backscatter using the achromatic patches
of the chart as described in [1], and also estimated it using
the method described here (Fig. 4b). The resulting Bc val-
ues are almost identical; no inputs (e.g., water type) other
than Ic and z were needed to obtain this result. Note that
the black patch of the color chart was not picked up in Ω
in any of the images, indicating that it is indeed a just dark
gray and much lighter than true black or shadowed pixels.

4.4. Attenuation Coefﬁcient Estimation

4.4.1 βD

c as a Function of z

We previously showed that βD
c varies most strongly with
range z [1, 2]. Inspecting Eq. 3 suggests that this variation
is in the form of an exponential decay. Before we describe
how to extract βD
c (z) from images, we formulate the rela-
tionship between βD

c and z.

Fig. 5 shows an experiment from [2] where we mounted
a color chart and a Nikon D90 camera on a frame roughly
20cm apart, and lowered this conﬁguration from surface to
30m depth while taking photographs. Backscatter and at-
tenuation between the camera and the chart are both negli-
gible since the distance z between the chart and the camera
was small, yielding Ic → Jc. In this setup, the color loss
is due to the effective attenuation coefﬁcient acting in the
vertical distance d from the sea surface, and is captured in
the white point of ambient light Wc at each depth.

1685

Figure 5. Using a dataset from [2] we calculate βD
c (z) three different ways: extracting it from photos of the same object at two different
distances; calculating it using the white point of the ambient light using Eq. 9; and simulating it using Eq. 3. For this dataset, a color chart
(DGK Color Tools) and the camera were mounted on a frame very close to each other so that Ic → Jc. The frame was imaged from surface
to 30m depth in the Red Sea using a Nikon D90. a) Raw images captured by the camera (top row; not all are shown), and same images
after white balancing using the achromatic patch (bottom row). Brightness in each image was manually adjusted for visualization. b) For
simulating the value of βD
c , we used the spectral response of Nikon D90 from [38], assumed CIE D65 light at the surface [61], measured
the reﬂectance of the second brightest gray patch (note that it does not reﬂect uniformly), and for the optical water type, used the diffuse
downwelling attenuation coefﬁcient Kd(λ) we measured in situ (magenta curve in c). This curve agrees well with the oceanic water types
deﬁned by Jerlov (black curves in c) [36, 37]. d) βD

c decays as a 2-term exponential with z as shown by all three methods.

From each image, we calculated the effective βD

c in the
vertical direction two different ways: from pairwise images
as described in [2], and by using Eq. 9 with Wc extracted
from the intensity of the second (24%) gray patch in the
color chart. Additionally, we used Eq. 3 to calculate the
theoretical value of βD
c in that water type using the spectral
response of the camera from [38], and the Kd(λ) of the
water body (which acts in the vertical direction) which we
had measured. All three ways of estimating βD
in Fig. 5
c
demonstrate that βD
c decays with distance, in this case d.
Based on the data in Fig. 5 and additional simulations we
describe the dependency of βD
c on any range z using a 2-
term exponential in the form of:

βD
c (z) = a ∗ exp(b ∗ z) + c ∗ exp(d ∗ z) .

(11)

For short ranges, βD

c (z) can also be modeled as a line.

4.4.2 Coarse Estimate of βD

c (z) From an Image

Assuming Bc is successfully removed from image Ic, we
can now proceed to estimating βD
c (z) from the direct signal
Dc. Note from Eq. 2 that the direct signal is the product
of the scene Jc (at the location of the camera) attenuated
by e−βD
c (z)z. Thus, the recovery of the scene Jc reduces to
a problem of the estimation of the illumiant map between
the camera and the scene, which varies spatially. Given an
estimate of the local illuminant map ˆEc(z), we can obtain

an estimate of βD

c (z) as follows:
ˆβD
c (z) = − log ˆEc(z)/z.

(12)

Estimation of an illuminant locally is a well-studied topic
in the ﬁeld of computational color constancy (e.g., [6, 10,
22, 29, 32, 41]). Several methods, most notably the Retinex
model which mimics a human’s ability to discount vary-
ing illuminations, have been applied on underwater imagery
(e.g., [30, 62]), and a recent work showed that there is a di-
rect linear relationship between atmospheric image dehaz-
ing and Retinex [31].
If backscatter is properly removed
from original images, we can expect many of the multi-
illuminant estimation methods to work well on underwater
images. Here, we adopt a variant of the local space aver-
age color (LSAC) method described in [24], as it utilizes
a known range map. This method works as follows: for a
given pixel (x, y) in color channel c, local space average
color ac(x, y) is estimated iteratively through updating the
equations:

a′
c(x, y) =

ac(x′, y′)

1

Ne XNe

(13)

(14)

ac(x, y) = Dc(x, y)p + a′

c(x, y)(1 − p) ,

where the neighborhood Ne is deﬁned as the 4-connected
pixels neighboring the pixel at (x, y) which are closer to it
than a range threshold ∈:
Ne(x′, y′) = (x′, y′) with kz(x, y) − z(x′, y′)k ≤∈. (15)

1686

Set
D1
D2
D3
D4
D5

Scene Depth
10m
reef
10m
reef
4m
reef
4-9m
5m

canyon

reef

Angle
down
down

Bc Water Type
low
high
low
high
forward med

clear
clear
clear
turbid
clear

down

all

# images

Camera

Lens

559
318
68
153
59

Sony α7R Mk III
Sony α7R Mk III
Sony α7R Mk III

Sony FE 16-35mm f/2.8GM
Sony FE 16-35mm f/2.8GM
Sony FE 16-35mm f/2.8GM

Nikon D810
Nikon D810

Nikkor 35mm f1.8
Nikkor 35mm f1.8

Table 1. Datasets we contribute with SFM-based range maps for each image. Each set contains multiple images with a color chart.

Here, the initial value of a(x, y) is taken as zero for all
pixels, since after a large number of iterations the start-
ing value will be insigniﬁcant. The parameter p describes
the local area of support over which the average is com-
puted and depends on the size of the image; large p means
that local space average color will be computed for a small
neighborhood. Then, the local illuminant map is found as
ˆEc = f ac, where f is a factor based on geometry scaling all
color channels equally and can be found based on the scene
viewed. We use f = 2 following [23] for a perpendicular
orientation between the camera and the scene.

4.4.3 Reﬁned Estimate of βD

c (z)

Next, we make use of the known range map z and reﬁne the
estimate of βD
c (z) found from Eqs. 12-15 to obey the given
z. We rewrite Eq. 12 as:

and minimize:

c (z) ,

ˆz = − log ˆEc(cid:14)βD
min
c (z)kz − ˆzk,
βD

(16)

(17)

where βD
c (z) is deﬁned in the form of Eq. 11 with
parameters a, b, c, d. The lower and upper bounds for
these parameters to obtain a decaying exponential will be
[0,−∞, 0,−∞], and [∞, 0,∞, 0], respectively; but can be
narrowed using the coarse estimate obtained from Eq. 12.
Using the reﬁned estimate of βD
c (z), we recover Jc using
Eq. 8. In Jc, spatial variation of ambient light has already
been corrected, so all that remains is the estimation of the
global white point Wc. This can be done using statistical
or learning based methods (see [32] for a survey); here for
the scenes that contain a sufﬁciently diverse set of colors,
we adopt the simple and fast Gray World Hypothesis [14],
and for monochromatic scenes (such as in our dataset D4
we introduce next), we use a spatial-domain method from
[18] that does not rely on color information.

4.5. Photoﬁnishing

We use the camera pipeline manipulation platform de-
scribed in [40] to convert Sea-thru outputs to a standard
color space, inserting them into the pipeline before Step 6
and specifying an identity matrix for white balance.

5. Datasets

We contribute ﬁve underwater RGBD datasets (Table 1).
All were acquired under natural illumination, in raw format,
with constant exposure settings for a given set, and contain
multiple images with color charts.

6. Results: Validation and Error Analysis

We use our dataset from Table 1 and the stereo RGBD

dataset from [7] to test the following scenarios:
S1. Simple contrast stretch.
S2. Former model with an incorrect estimate of Bc.
Speciﬁcally, we use DCP [34], which typically overes-
timates Bc in underwater scenes. We used the built-in
imreducehaze function in Matlab.

S3. Former model, with a correct estimate of Bc (i.e., cor-

rect B∞

c and βB

c ), and assuming βc = βD

c = βB
c .

S4. Revised model, with a correct estimate of Bc, and Jc
obtained as Jc = Dc/ ˆEc without explicitly computing
βD
c (z).

c = βD

c , and βD
βD

c (z) from Eq. 11.

S5. Sea-thru, which uses the revised model where βB
c

6=
Since Sea-thru is the ﬁrst algorithm to use the revised un-
derwater image formation model and has the advantage of
having a range map, we do not test its performance against
single image color reconstruction methods that also try to
estimate the range/transmission. After a meticulous sur-
vey of these methods, authors in [7] found that DCP-based
ones [21, 50] were not able to consistently correct colors,
and others [3, 4, 5, 25] were designed to enhance images
rather than achieve physically accurate corrections. The
proposed method in [7] does aim to recover physically ac-
curate colors (using the former model), but only works for
horizontal imaging with sufﬁciently large distances in the
scene, making it unsuitable for many of our images.

We present raw images, range maps, and the correspond-
ing S1-S5 results on D1-5 in Fig. 6, and on the stereo
database of [7] in Fig. 7. For evaluation, we used RGB angu-
lar error ¯ψ between the six grayscale patches of each chart
and a pure gray color, averaged per chart

¯ψ = (1/6) cos−1(cid:2)Ic/(√3kIck)(cid:3) ,

as also done by [7]. Lower ¯ψ value indicates better correc-
tion (though see exceptions below); errors (in degrees) are

(18)

1687

Figure 6. Results on D1-5 (Table 1). For each chart and method, ¯ψ rounded to the nearest integer is given in inset; Chart #1 is closest to
the camera. Average errors for the dataset are: raw: 20.57, S1: 12.49, S2: 14.38, S3: 21.77, S4: 4.13, S5: (Sea-thru) 6.33.

listed in the insets of Figs. 6 & 7 per color chart for scenes
that had them, rounded to the nearest integer.

In all cases, the simple contrast stretch S1, which is
global, works well when scene distances are more or
less uniform. The DCP method (S2) often overestimates
backscatter (which can improve visibility), and generally
distorts and halucinates colors. For example what should
be uniformly colored sand appears green and purple in both
datasets. In D1 3272, the gray patches of the color chart in
S2 have visible purple artifacts, yet their ¯ψ error is lower
than that of S5, suggesting that ¯ψ is not an optimal met-
ric for quantifying color reconstruction error. In S3-S5, the

correct amount of Bc is subtracted. In S3 attenuation is cor-
rected with a constant βD
c , as had been done by methods us-
ing the former model. When there is large variation in range
(e.g., Fig. 7), the failure of the constant βD
c assumption is
most evident, and this is also where S5 has the biggest ad-
vantage (though S3 also fails on scenes with short ranges,
e.g., D3&4). Range maps often tend to be least accurate
furthest from the camera, which also adds to the difﬁculty
of reconstructing colors at far ranges. S4 sometimes yields
lower errors on the color cards than S5. This makes sense
as it is easier to calculate the illuminant on the cards; how-
ever S5 results are better on the complete scenes. S4 can be

1688

Figure 7. Results on the stereo dataset of [7]. Their range maps were further processed to remove spurious pixels. ¯ψ rounded to the nearest
integer is given in inset; chart #1 is closest to the camera. S1 and S2 do not utilize range maps; for others, lack of ¯ψ values indicate missing
range information. The average errors across all images are: raw: 22.28, S1: 6.83, S2: 10.03, S3: 12.04, S4: 4.46, S5: (Sea-thru) 4.94.

used for a ﬁrst-order correction that is better than previous
methods.

multiple color charts in the scene, but evaluation limited to
the charts does not always tell the full story.

7. Conclusions

c are distinct, and the z-dependency of βD

Taking its strength from an image formation model de-
rived for the ocean, Sea-thru offers a glimpse into the un-
derwater world without skewed colors. It highlights that βD
c
and βB
c cannot be
ignored. We focused on recovering the z-dependency as it is
the most prominent, but in the future plan to also recover the
ρ-dependency to improve correction. As recovering these
intricate dependencies is extremely challenging, deep nets
should perform better than the estimation methods we used.
Since ground-truth cannot be attained for this environment
their training has to be conducted with carefully designed
simulations based on the correct image formation models.
Careful simulations can also help with another challenge
that arose in this work; the evaluation of results. The dataset
published in [7] was acquired with signiﬁcant effort to place

Sea-thru is a signiﬁcant step towards opening up large
underwater datasets to powerful computer vision and ma-
chine learning algorithms, and will help boost underwater
research at a time when our oceans are increasing stress
from pollution, overﬁshing, and climate change.

8. Acknowledgments

This work was supported by the The Leona M. and Harry
B. Helmsley Charitable Trust, the Maurice Hatter Foun-
dation, Ministry of Science, Technology and Space grant
#3 − 12487, ISF grant #680/18, the Technion Ollendorff
Minerva Center for Vision and Image Sciences, the Univer-
sity of Haifa institutional postdoctoral program. We thank
Tom Shlesinger, Deborah Levy, Matan Yuval, Ben Singer,
H. Can Karaimer, and the Interuniversity Institute of Marine
Sciences in Eilat.

1689

References

[1] D. Akkaynak and T. Treibitz. A revised underwater image

formation model. In Proc. IEEE CVPR, 2018. 1, 2, 3, 4

[2] D. Akkaynak, T. Treibitz, T. Shlesinger, R. Tamir, Y. Loya,
and D. Iluz. What is the space of attenuation coefﬁcients in
underwater computer vision? In Proc. IEEE CVPR, 2017. 2,
3, 4, 5

[3] C. Ancuti, C. O. Ancuti, C. De Vleeschouwer, R. Garcia,
and A. C. Bovik. Multi-scale underwater descattering.
In
Proc. IEEE Int. Conf. on Pattern Recognition (ICPR), pages
4202–4207, 2016. 6

[4] C. O. Ancuti, C. Ancuti, C. De Vleeschouwer, and
P. Bekaert. Color balance and fusion for underwater image
enhancement.
IEEE Trans. Image Processing, 27(1):379–
393, 2018. 6

[5] C. O. Ancuti, C. Ancuti, C. De Vleeschouwer, L. Neumann,
and R. Garcia. Color transfer for underwater dehazing and
depth estimation. In Proc. IEEE Int. Conf. Image Processing
(ICIP), pages 695–699, 2017. 6

[6] S. Beigpour, C. Riess, J. Van De Weijer, and E. An-
gelopoulou. Multi-illuminant estimation with conditional
random ﬁelds. IEEE Trans. Image Processing, 23(1):83–96,
2014. 5

[7] D. Berman, D. Levy, S. Avidan, and T. Treibitz. Underwa-
ter single image color restoration using haze-lines and a new
quantitative dataset, 2018. 2, 3, 6, 8

[8] D. Berman, T. Treibitz, and S. Avidan. Non-local image de-

hazing. In Proc. IEEE CVPR, 2016. 2

[9] D. Berman, T. Treibitz, and S. Avidan. Diving into haze-
In Proc.

lines: Color restoration of underwater images.
British Machine Vision Conference (BMVC), 2017. 2, 3

[10] M. Bleier, C. Riess, S. Beigpour, E. Eibenberger, E. An-
gelopoulou, T. Tr¨oger, and A. Kaup. Color constancy and
non-uniform illumination: Can existing algorithms work? In
Proc. IEEE ICCVWorkshops, pages 774–781, 2011. 5

[11] D. L. Bongiorno, M. Bryson, and S. B. Williams. Dy-
namic spectral-based underwater colour correction. In Proc.
MTS/IEEE OCEANS, 2013. 2

[12] M. Bryson, R. Ferrari, W. Figueira, O. Pizarro, J. Madin,
S. Williams, and M. Byrne. Characterization of measurement
errors using structure-from-motion and photogrammetry to
measure marine habitat structural complexity. Ecology and
Evolution, 7(15):5669–5681, 2017. 3

[13] M. Bryson, M. Johnson-Roberson, O. Pizarro, and S. B.
Williams. True color correction of autonomous underwater
vehicle imagery. J. Field Robotics, 2015. 2

[14] G. Buchsbaum. A spatial processor model for object colour

perception. J. the Franklin institute, 310(1):1–26, 1980. 6

[15] J. Burns, D. Delparte, R. Gates, and M. Takabayashi. Inte-
grating structure-from-motion photogrammetry with geospa-
tial software as a novel technique for quantifying 3d ecolog-
ical characteristics of coral reefs. PeerJ, 3:e1077, 2015. 3

[16] J. Burns, D. Delparte, L. Kapono, M. Belt, R. Gates, and
M. Takabayashi. Assessing the impact of acute disturbances
on the structure and composition of a coral community us-
ing innovative 3D reconstruction techniques. Methods in
Oceanography, 15:49–59, 2016. 3

[17] N. Carlevaris-Bianco, A. Mohan, and R. M. Eustice.

tial results in underwater single image dehazing.
MTS/IEEE OCEANS, 2010. 2

Ini-
In Proc.

[18] D. Cheng, D. K. Prasad, and M. S. Brown. Illuminant estima-
tion for color constancy: why spatial-domain methods work
and the role of the color distribution. JOSA A, 31(5):1049–
1058, 2014. 6

[19] J. Y. Chiang and Y.-C. Chen. Underwater image enhance-
IEEE

ment by wavelength compensation and dehazing.
Trans. Image Processing, 21(4):1756–1769, 2012. 2

[20] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR09, 2009. 1

[21] P. Drews, E. Nascimento, F. Moraes, S. Botelho, and
M. Campos. Transmission estimation in underwater single
images. In Proc. IEEE ICCV Underwater Vision Workshop,
pages 825–830, 2013. 2, 6

[22] M. Ebner. Color constancy, volume 6. John Wiley & Sons,

2007. 5

[23] M. Ebner. Color constancy based on local space average
color. Machine Vision and Applications, 20(5):283–301,
2009. 2, 6

[24] M. Ebner and J. Hansen. Depth map color constancy. Bio-

Algorithms and Med-Systems, 9(4):167–177, 2013. 5

[25] S. Emberton, L. Chittka, and A. Cavallaro. Underwater im-
age and video dehazing with pure haze region segmentation.
Computer Vision and Image Understanding, 168:145–156,
2018. 6

[26] R. Fattal. Dehazing using color-lines. ACM Trans. on Graph-

ics (TOG), 34(1):13, 2014. 2

[27] R. Ferrari, D. McKinnon, H. He, R. N. Smith, P. Corke,
M. Gonz´alez-Rivero, P. J. Mumby, and B. Upcroft. Quantify-
ing multiscale habitat structural complexity: a cost-effective
framework for underwater 3D modelling. Remote Sensing,
8(2):113, 2016. 3

[28] W. Figueira, R. Ferrari, E. Weatherby, A. Porter, S. Hawes,
and M. Byrne. Accuracy and precision of habitat structural
complexity metrics derived from underwater photogramme-
try. Remote Sensing, 7(12):16883–16900, 2015. 3

[29] G. D. Finlayson, B. V. Funt, and K. Barnard. Color constancy
under varying illumination. In Proc. IEEE ICCV, pages 720–
725, 1995. 5

[30] X. Fu, P. Zhuang, Y. Huang, Y. Liao, X.-P. Zhang, and
X. Ding. A retinex-based enhancing approach for single un-
derwater image. In Proc. IEEE Int. Conf. Image Processing
(ICIP), pages 4572–4576, 2014. 5

[31] A. Galdran, A. Alvarez-Gila, A. Bria, J. Vazquez-Corral, and
M. Bertalm´ıo. On the duality between retinex and image
dehazing. In Proc. IEEE CVPR, pages 8212–8221, 2018. 5
[32] A. Gijsenij, T. Gevers, J. Van De Weijer, et al. Computational
color constancy: Survey and experiments. IEEE Trans. Im-
age Processing, 20(9):2475–2489, 2011. 5, 6

[33] K. He, J. Sun, and X. Tang. Single image haze removal using

dark channel prior. In Proc. IEEE CVPR, 2009. 2

[34] K. He, J. Sun, and X. Tang. Single image haze removal using
dark channel prior. Trans. IEEE PAMI, 33(12):2341–2353,
2011. 2, 6

1690

[53] Y.-S. Shin, Y. Cho, G. Pandey, and A. Kim. Estimation of
ambient light and transmission map with common convolu-
tional architecture. In Proc. MTS/IEEE OCEANS, 2016. 2

[54] M. Shortis and E. H. D. Abdo. A review of underwater
stereo-image measurement for marine biology and ecology
applications.
In Oceanography and marine biology, pages
269–304. CRC Press, 2016. 3

[55] K. A. Skinner, E. Iscar, and M. Johnson-Roberson. Auto-
matic color correction for 3D reconstruction of underwater
scenes.
In Proc. IEEE International Conference Robotics
and Automation (ICRA), pages 5140–5147, 2017. 2

[56] O. Spier, T. Treibitz, and G. Gilboa. In Situ target-less cali-

bration of turbid media. In Proc. IEEE ICCP, 2017. 2

[57] R. T. Tan. Visibility in bad weather from a single image. In

Proc. IEEE CVPR, 2008. 2

[58] T. Treibitz and Y. Y. Schechner. Active polarization descat-

tering. IEEE Trans. PAMI, 31(3):385–399, 2009. 2

[59] K. Williams, C. Rooper, and J. Harms. Report of the na-
tional marine ﬁsheries service automated image processing
workshop. NOAA Technical Memorandum NMFS-F/SPO-
121, 2012. 2

[60] G. Winters, R. Holzman, A. Blekhman, S. Beer, and Y. Loya.
Photographic assessment of coral chlorophyll contents: im-
plications for ecophysiological studies and coral monitoring.
J. Exp. Marine Biology and Ecology, 380(1):25–35, 2009. 2
[61] G. Wyszecki and W. S. Stiles. Color science, volume 8. Wi-

ley New York, 1982. 5

[62] S. Zhang, T. Wang, J. Dong, and H. Yu. Underwater image
enhancement via extended multi-scale retinex. Neurocom-
puting, 245:1–9, 2017. 5

[63] X. Zhao, T. Jin, and S. Qu. Deriving inherent optical proper-
ties from background color and underwater image enhance-
ment. Ocean Engineering, 94:163–172, 2015. 2

[35] J. Henderson, O. Pizarro, M. Johnson-Roberson, and I. Ma-
hon. Mapping submerged archaeological sites using stereo-
vision photogrammetry.
Int. J. Nautical Archaeology,
42(2):243–256, 2013. 3

[36] N. Jerlov. Irradiance Optical Classiﬁcation. Elsevier, 1968.

5

[37] N. G. Jerlov. Marine optics, volume 14. Elsevier, 1976. 5
[38] J. Jiang, D. Liu, J. Gu, and S. Susstrunk. What is the space
of spectral sensitivity functions for digital color cameras? In
IEEE Workshop Applications of Computer Vision (WACV),
pages 168–179, 2013. 5

[39] M. I. Jordan and T. M. Mitchell. Machine learning: Trends,
perspectives, and prospects. Science, 349(6245):255–260,
2015. 1

[40] H. C. Karaimer and M. S. Brown. A software platform for
manipulating the camera imaging pipeline. In Proc. ECCV,
pages 429–444. Springer, 2016. 6

[41] E. H. Land and J. J. McCann. Lightness and retinex theory.

J. Optical Society of America, 61(1):1–11, 1971. 5

[42] J. X. Leon, C. M. Roelfsema, M. I. Saunders, and S. R.
Phinn. Measuring coral reef terrain roughness using
structure-from-motionclose-range photogrammetry. Geo-
morphology, 242:21–28, 2015. 3

[43] Y. Li, H. Lu, J. Li, X. Li, Y. Li, and S. Serikawa. Under-
water image de-scattering and classiﬁcation by deep neural
network. Computers & Electrical Engineering, 54:68–77,
2016. 2

[44] H. Lu, Y. Li, L. Zhang, and S. Serikawa. Contrast enhance-
ment for images in turbid water. JOSA A, 32(5):886–893,
2015. 2

[45] J. McCarthy and J. Benjamin. Multi-image photogrammetry
for underwater archaeological site recording: an accessible,
diver-based approach. J. Maritime Archaeology, 9(1):95–
114, 2014. 3

[46] J. Mertes, T. Thomsen, and J. Gulley. Evaluation of struc-
ture from motion software to create 3d models of late nine-
teenth century great lakes shipwrecks using archived diver-
acquired video surveys. J. Maritime Archaeology, 9(2):173–
189, 2014. 3

[47] C. D. Mobley. Light and water: radiative transfer in natural

waters. Academic press, 1994. 3

[48] S. K. Nayar and S. G. Narasimhan. Vision in bad weather. 1,

2

[49] Y.-T. Peng, X. Zhao, and P. C. Cosman. Single underwater
image enhancement using depth estimation based on blurri-
ness. In Proc. IEEE ICIP, pages 4952–4956, 2015. 2

[50] Y.-T. Peng, X. Zhao, and P. C. Cosman. Single underwater
image enhancement using depth estimation based on blur-
riness.
In Proc. IEEE Int. Conf. Image Processing (ICIP),
pages 4952–4956, 2015. 6

[51] M. Roser, M. Dunbabin, and A. Geiger. Simultaneous un-
derwater visibility assessment, enhancement and improved
stereo. In IEEE Conf. Robotics and Automation, pages 3840–
3847, 2014. 2

[52] Y. Y. Schechner and N. Karpel. Recovery of underwater visi-
bility and structure by polarization analysis. IEEE J. Oceanic
Engineering, 30(3):570–587, 2005. 2

1691

