Efﬁcient Decision-based Black-box Adversarial Attacks on Face Recognition

Yinpeng Dong1, Hang Su1, Baoyuan Wu2, Zhifeng Li2, Wei Liu2, Tong Zhang3, Jun Zhu1 ∗
1 Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys.,

1 Institute for AI, THBI Lab, Tsinghua University, Beijing, 100084, China
3 Hong Kong University of Science and Technology
2 Tencent AI Lab

dyp17@mails.tsinghua.edu.cn, suhangss@mail.tsinghua.edu.cn, wubaoyuan1987@gmail.com

michaelzﬂi@tencent.com, wl2223@columbia.edu, tongzhang@tongzhang-ml.org, dcszj@mail.tsinghua.edu.cn

Abstract

Face recognition has obtained remarkable progress in
recent years due to the great improvement of deep convo-
lutional neural networks (CNNs). However, deep CNNs are
vulnerable to adversarial examples, which can cause fateful
consequences in real-world face recognition applications
with security-sensitive purposes. Adversarial attacks are
widely studied as they can identify the vulnerability of the
models before they are deployed. In this paper, we evaluate
the robustness of state-of-the-art face recognition models in
the decision-based black-box attack setting, where the at-
tackers have no access to the model parameters and gradi-
ents, but can only acquire hard-label predictions by sending
queries to the target model. This attack setting is more prac-
tical in real-world face recognition systems. To improve the
efﬁciency of previous methods, we propose an evolutionary
attack algorithm, which can model the local geometry of the
search directions and reduce the dimension of the search
space. Extensive experiments demonstrate the effectiveness
of the proposed method that induces a minimum perturba-
tion to an input face image with fewer queries. We also ap-
ply the proposed method to attack a real-world face recog-
nition system successfully.

1. Introduction

Recent progress in deep convolutional neural networks
(CNNs) [26, 29, 11] has led to substantial performance im-
provements in a broad range of computer vision tasks. Face
recognition, as one of the most important computer vision
tasks, has been greatly facilitated by deep CNNs [31, 28, 23,
33, 16, 32, 5]. There are usually two sub-tasks in face recog-
nition: face veriﬁcation and face identiﬁcation [12, 15]. The
former distinguishes whether a pair of face images repre-
sent the same identity, while the latter classiﬁes an image

∗Corresponding author.

Original 
Images

1,000 
Queries

10,000 
Queries

100,000 
Queries

False

True

True

True

A “Black-box” Model

Figure 1. Demonstration of the decision-based black-box attack
setting. Given a black-box model, the attackers use queries to gen-
erate adversarial examples with minimum perturbations.

to an identity. The state-of-the-art face recognition models
realize these two tasks by using deep CNNs to extract face
features that have minimum intra-class variance and maxi-
mum inter-class variance. Due to the excellent performance
of these models, face recognition has been widely used for
identity authentication in enormous applications, such as ﬁ-
nance/payment, public access, criminal identiﬁcation, etc.

Despite the great success in various applications, deep
CNNs are known to be vulnerable to adversarial exam-
ples [30, 9, 19, 6]. These maliciously generated adversarial
examples are often indistinguishable from legitimate ones
for human observers by adding small perturbations. But
they can make deep models produce incorrect predictions.
The face recognition systems based on deep CNNs have
also been shown their vulnerability against such adversar-
ial examples. For instance, adversarial perturbations can
be made to the eyeglass that, when worn, allows attackers
to evade being recognized or impersonate another individ-
ual [24, 25]. The insecurity of face recognition systems in
real-world applications, especially those with sensitive pur-
poses, can cause severe consequences and security issues.

To evaluate the robustness of face recognition systems in
real-world applications, adversarial attacks can serve as an
important surrogate, as they can identify the vulnerability of

7714

these systems [2] and help to improve the robustness [9, 18].
However, existing attack methods [24, 25] for face recogni-
tion are mainly based on the white-box scenario, where the
attackers know the internal structure and parameters of the
system being attacked. Accordingly, the attack objective
function can be directly optimized by gradient-based meth-
ods. This setting is clearly impractical in real-world cases,
when the attackers cannot get access to the model details.
Instead, we focus on a more realistic and general decision-
based black-box setting [1], where no model information
is exposed except that the attackers can only query the tar-
get model and obtain corresponding hard-label predictions.
The goal of attacks is to generate adversarial examples with
minimum perturbations by limited queries. This attack sce-
nario is much more challenging, since that the gradient can-
not be directly computed and the predicted probability is not
provided. On the other hand, it is much more realistic and
important, because most of the real-world face recognition
systems are black-box and only provide hard-label outputs.
To the best of our knowledge, it is the ﬁrst attempt to con-
duct adversarial attacks on face recognition in this setting.

Several methods [1, 14, 4] have been proposed to per-
form decision-based black-box attacks. However, they lack
the efﬁciency in the sense that they usually require a tremen-
dous number of queries to converge, or get a relatively large
perturbation given a limited budget of queries. Therefore,
we consider how to efﬁciently generate adversarial exam-
ples for decision-based black-box attacks by inducing a
smaller perturbation to each sample with fewer queries.

To address the aforementioned issues, we propose an
evolutionary attack method for query-efﬁcient adversarial
attacks in the decision-based black-box setting. Given the
attack objective function, the proposed method is able to op-
timize it in the black-box manner through queries only. Our
method can ﬁnd better search directions by modeling their
local geometry. It further improves the efﬁciency by reduc-
ing the dimension of the search space. We apply the pro-
posed method to comprehensively study the robustness of
several state-of-the-art face recognition models, including
SphereFace [16], CosFace [32], and ArcFace [5], under the
decision-based black-box scenario. Extensive experiments
conducted on the most popular public-domain face recogni-
tion datasets such as Labeled Face in the Wild (LFW) [12]
and MegaFace Challenge [15] demonstrate the effectiveness
of the proposed method. We further apply our method to at-
tack a real-world face recognition system to show its practi-
cal applicability. In summary, our major contributions are:

• We propose a novel evolutionary attack method un-
der the decision-based black-box scenario, which can
model the local geometry of the search directions and
meanwhile reduce the dimension of the search space.
The evolutionary attack method is generally applica-
ble for any image recognition task, and signiﬁcantly

improves the efﬁciency over existing methods.

• We thoroughly evaluate the robustness of several state-
of-the-art face recognition models by decision-based
black-box attacks in various settings. We demonstrate
the vulnerability of these face models in this setting.

• We show the practical applicability of the proposed
method by successfully attacking a real-world face
recognition system.

2. Related Work

Deep face recognition. DeepFace [31] and DeepID [28]
treat face recognition as a multi-class classiﬁcation prob-
lem and use deep CNNs to learn features supervised by the
softmax loss. Triplet loss [23] and center loss [33] are pro-
posed to increase the Euclidean margin in the feature space
between classes. The angular softmax loss is proposed in
SphereFace [16] to learn angularly discriminative features.
CosFace [32] uses the large margin cosine loss to maximize
cosine margin. The additive angular margin loss is proposed
in ArcFace [5] to learn highly discriminative features.

Adversarial attacks on face recognition. Deep CNNs
are highly vulnerable to adversarial examples [30, 9, 19].
Face recognition has also been shown the vulnerability
against attacks. In [24], the perturbations are constrained to
the eyeglass region and generated by gradient-based meth-
ods, which fool face recognition systems even in the physi-
cal world. The adversarial eyeglasses can also be produced
by generative networks [25]. However, these methods rely
on the white-box manipulations of face recognition models,
which is unrealistic in real-world applications. Instead, we
focus on evaluating the robustness of face recognition mod-
els in the decision-based black-box attack setting.

Black-box attacks. Black-box attacks can be divided
into transfer-based, score-based and decision-based attacks.
Transfer-based attacks generate adversarial examples for a
white-box model and attack the black-box model based on
the transferability [17, 6]. In score-based attacks, the pre-
dicted probability is given by the model. Several methods
rely on approximated gradients to generate adversarial ex-
amples [3, 14]. In decision-based attacks, we can only ob-
tain the hard-label predictions. The boundary attack method
is based on random walk on the decision boundary [1]. The
optimization-based method [4] formulates this problem as a
continuous optimization problem and estimate the gradient
for optimization. However, it needs to calculate the distance
to the decision boundary along a direction by binary search.
In [14], the predicted probability is estimated by hard-label
predictions. Then, the natural evolution strategy (NES) is
used to maximize the target class probability or minimize
the true class probability. These methods generally require
a large number of queries to generate an adversarial exam-
ple with a minimum perturbation, or converge to a large
perturbation with few queries .

7715

3. Methodology

In this section, we ﬁrst introduce the decision-based
black-box attack setting against a face recognition model,
and then detail the proposed evolutionary attack method.

3.1. Attack Setting

Let f (x) : X → Y (X ⊂ Rn) denote the face recogni-
tion model that predicts a label for an input face image. For
face veriﬁcation, the model relies on another face image to
identify whether the pair of images belong to the same iden-
tity, and outputs a binary label in Y = {0, 1}. For face
identiﬁcation, the model f (x) compares the input image x
with a gallery set of face images, and then classiﬁes x as a
speciﬁc identity. So it can be viewed as a multi-class clas-
siﬁcation task, where Y = {1, 2, ..., K}, with K being the
number of identities. Although the face recognition model
f (x) uses an additional face image or a set of face images
for recognizing x, we do not explicitly describe the depen-
dency of f (x) on the compared images for simplicity.

Given a real face image x, the goal of attacks is to gen-
∗ in the vicinity of x but is
erate an adversarial face image x
misclassiﬁed by the model. It can be obtained by solving a
constrained optimization problem

min

x∗

D(x

∗, x), s.t. C(f (x

∗)) = 1,

(1)

where D(·, ·) is a distance metric, and C(·) is an adversar-
ial criterion that takes 1 if the attack requirement is satisﬁed
and 0 otherwise. We use the L2 distance as D. The con-
strained problem in Eq. (1) can be equivalently reformu-
lated as the following unconstrained optimization problem

min

x∗

L(x

∗) = D(x

∗, x) + δ(cid:0)C(f (x

∗)) = 1(cid:1),

(2)

where δ(a) = 0 if a is true, otherwise δ(a) = +∞. By
∗ with a min-
optimizing Eq. (2), we can obtain an image x
imum perturbation, which is also adversarial according to
the criterion. Note that in the above objective functions, C
cannot be deﬁned as a continuous criterion such as cross-
entropy loss, since that the model f (x) only gives discrete
hard-label outputs in this problem. In particular, we specify
C according to the following two types of attacks.

Dodging attack corresponds to generating an adversarial
image that is recognized wrong or not recognized. Dodg-
ing attack could be used to protect personal privacy against
excessive surveillance. For face veriﬁcation, given a pair
of face images belonging to the same identity, the attacker
seeks to modify one image and make the model recog-
nize them as not the same identity. So the criterion is
∗) = 0), where I is the indicator func-
C(f (x
tion. For face identiﬁcation, the attacker generates an ad-
versarial face image with the purpose that it is recognized as
∗) 6=
any other identity. The criterion is C(f (x
y), where y is the true identity of the real image x.

∗)) = I(f (x

∗)) = I(f (x

Impersonation attack works as seeking an adversarial
image recognized as a speciﬁc identity, which could be used
to evade the face authentication systems. For face veriﬁca-
tion, the attacker tries to ﬁnd an adversarial image that is
recognized as the same identity of another image, while the
original images are not from the same identity. The crite-
∗) = 1). For face identiﬁcation,
rion is C(f (x
the generated adversarial image needs to be classiﬁed as a
speciﬁc identity y∗, so C(f (x

∗)) = I(f (x

∗)) = I(f (x

∗) = y∗).

3.2. Evolutionary Attack

Since we cannot get access to the conﬁguration and pa-
rameters of f (x) but can only send queries to probe the
model, we resort to black-box optimization techniques to
minimize the objective function in Eq. (2). Gradient estima-
tion methods [20, 8, 7] approximate the gradient of the ob-
jective function by ﬁnite difference and update the solution
by gradient descent, which are commonly used for score-
based black-box attacks, when the predicted probability is
given by the model [3, 14]. However, in the case of hard-
label output, the attack objective function is discontinuous
and the output is insensitive to small input perturbations.
So the gradient estimation methods cannot be directly used.
Some methods [4, 14] successfully reformulate the discon-
tinuous optimization problem in Eq. (2) as some continuous
optimization problems and use gradient estimation methods
for optimization. But they need to calculate the distance of
a point to the decision boundary or estimate the predicted
probability by the hard-label outputs, which are less efﬁ-
cient as demonstrated in the experiments. Therefore, we
consider how to directly optimize Eq. (2) efﬁciently.

In this paper, we propose a novel evolutionary attack
method to solve the black-box optimization problem. Our
method is based on a simple and efﬁcient variant of covari-
ance matrix adaptation evolution strategy (CMA-ES) [10],
which is the (1+1)-CMA-ES [13]. In each update iteration
of the (1+1)-CMA-ES, a new offspring (candidate solution)
is generated from its parent (current solution) by adding a
random noise, the objective of these two solutions are eval-
uated, and the better one is selected for the next iteration.
This method is capable for solving black-box optimization
problems. However, directly applying the (1+1)-CMA-ES
to optimize Eq. (2) is inefﬁcient due to the high dimension
∗. Considering the query limit in decision-based black-
of x
box attacks for face images, the original (1+1)-CMA-ES
may be infeasible. To accelerate this algorithm, we design
an appropriate distribution to sample the random noise in
each iteration, which can model the local geometry of the
search directions. We also propose several techniques to re-
duce the dimension of the search space by considering the
special characteristics of this problem.

The overall evolutionary attack algorithm is outlined in
Algorithm 1. Rather than the original n-dimensional input

7716

Algorithm 1 The evolutionary attack algorithm
Input: The attack objective function L(x

∗); the original face im-
age x; the dimension n ∈ N+ of the input space (x
∗ ∈ Rn);
the dimension m ∈ N+ of the search space; the number of
coordinates k ∈ N+ for stochastic coordinate selection.

Input: The total number of queries T .
1: Initialize C = Im, pc = 0, σ, µ, cc, ccov ∈ R+, ˜x
2: for t = 1 to T do
3:

Sample z ∼ N (0, σ2C);
Select k coordinates from m with probability proportional

∗ ∈ Rn;

to each diagonal element in C;

Set the non-selected coordinates of z to 0;
Upscale z to Rn by bilinear interpolation and obtain ˜z;
˜z ← ˜z + µ(x − ˜x
if L( ˜x
˜x
Update pc and C by z according to Eq. (3) and Eq. (4);

∗);
∗ + ˜z) < L( ˜x
∗ ← ˜x

∗) then

∗ + ˜z;

4:

5:

6:

7:

8:

9:

10:

end if

11:
12: end for
13: return ˜x

∗.

space, we perform search in a lower dimensional space Rm
with m < n. In each iteration, we ﬁrst sample a random
vector z from N (0, σ2C) such that z ∈ Rm, where C is
a diagonal covariance matrix to model the local geometry
of the search directions. We then select k coordinates ran-
domly for search, according to the assumption that only a
fraction of pixels are important for ﬁnding an adversarial
image. We keep the value of the selected k coordinates of z
by setting the others to 0. We upscale z to the input space
by bilinear interpolation and get ˜z ∈ Rn. We further add a
bias to ˜z to minimize the distance between the adversarial
and original images. We ﬁnally test whether we get a better
solution. If we indeed ﬁnd a better solution, we jump to it
and update the covariance matrix. In the following, we will
give a detailed description of each step in the algorithm.

3.2.1

Initialization

∗ should be initialized at ﬁrst (in Step 1).
In Algorithm 1, ˜x
∗ does not satisfy the adversarial criterion,
If the initial ˜x
∗) equals to +∞. For subsequent iterations, adding a
L( ˜x
random vector can rarely make the search point adversar-
ial due to that deep CNNs are generally robust to random
noises [30], and thus the loss function will keep being +∞.
∗ with a sample that already satisﬁes the
So we initialize ˜x
adversarial criterion. The following updates will also keep
∗ adversarial, and at the same time minimize the distance
˜x
∗ can
between ˜x
be simply set as a random vector. For impersonation attack,
we use the target image as the initial point of ˜x

∗ and x. For dodging attack, the initial ˜x

∗.

3.2.2 Mean of Gaussian Distribution

We explain why we need to add a bias term to the random
vector in Step 7. Assume now that the dimension of the

search space is the same as that of the input space and we
select all coordinates for search (i.e., k = m = n). In each
iteration, a random vector z is sampled from a Gaussian
distribution. In general, the distribution should be unbiased
(with zero mean) for better exploration in the search space.
But in our problem, sampling the random vector from a zero
mean Gaussian distribution will result in nearly zero proba-
bility of updates as n → ∞, given by Theorem 1.

Theorem 1 (Proof in Appendix A) Assume that the covari-
ance matrix C is positive deﬁnite. Let λmax and λmin(> 0)
be the largest and smallest eigenvalues of C, respectively.
Then, we have

Pz∼N (0,σ2C)(cid:0)L( ˜x

∗ + z) < L( ˜x

∗)(cid:1) ≤

∗ − xk2

4λmaxk ˜x
σ2λ2

minn2

.

From Theorem 1, we need to draw O(n2) samples from
the zero mean Gaussian distribution for only one success-
ful update, which is inefﬁcient and costly when n is large.
This happens because in high dimensional search space, a
∗ − x,
randomly drawn vector z is almost orthogonal to ˜x
∗ + z, x) will be rarely smaller than
thus the distance D( ˜x
∗, x). To address this problem, the random vector z
D( ˜x
should be sampled from a biased distribution towards min-
∗ from the original image x. So
imizing the distance of ˜x
∗) to ˜z (the same as z when
we add a bias term µ(x − ˜x
k = m = n) in Step 7, where µ is a critical hyper-parameter
controlling the strength of going towards the original image
x. We will specify the update procedure of µ in Sec. 3.2.6.

3.2.3 Covariance Matrix Adaptation

The adaptation of covariance matrix C is suitable for solv-
ing non-separable optimization problems since it can model
the local geometry of the search directions [10]. For ex-
ample, an appropriately set covariance matrix can make the
random vectors generated predominantly in the direction of
narrow valleys. In learning all pair-wise dependencies be-
tween dimensions, the storage and computation complexity
of the covariance matrix is at least O(m2), which is unac-
ceptable when m is large. For black-box adversarial attacks,
the dimension of the search space is extremely large (e.g.,
m = 45 × 45 × 3 in our experiments). Therefore, we relax
the covariance matrix to be a diagonal matrix for efﬁcient
computation. Inspired by [22] which uses a diagonal co-
variance matrix for CMA-ES, we design an update rule for
the adaptation of the diagonal covariance matrix C (in Step
10) after each successful trial as

pc = (1 − cc)pc + pcc(2 − cc)

z
σ
cii = (1 − ccov)cii + ccov(pc)2
i ,

,

(3)

(4)

where pc ∈ Rm is called the evolution path as it stores
the exponentially decayed successful search directions; for
i = 1, ..., m, cii is the diagonal element of C and (pc)i is
the i-th element of pc. cc and ccov are two hyper-parameters

7717

Model
Queries

Dodging

Impersonation

Boundary [1]

Optimization [4]

NES-LO [14]
Evolutionary
Boundary [1]

Optimization [4]

NES-LO [14]
Evolutionary

SphereFace [16]
5,000

10,000

100,000

1,000

CosFace [32]
10,000

5,000

100,000

1,000

ArcFace [5]
10,000

5,000

100,000

9.3e-3
2.9e-3
3.8e-2
8.9e-5
6.3e-3
3.3e-3
2.6e-2
7.2e-5

7.0e-4
1.3e-3
2.4e-2
3.4e-5
5.7e-4
1.3e-3
1.7e-2
2.9e-5

1.9e-5
7.1e-5
7.4e-3
1.3e-5
1.6e-5
6.1e-5
5.5e-3
1.2e-5

2.0e-2
1.1e-2
1.4e-1
1.7e-3
1.1e-2
7.7e-3
9.3e-2
6.5e-4

7.5e-3
2.9e-3
3.5e-2
9.1e-5
2.9e-3
1.9e-3
2.0e-2
3.7e-5

7.7e-4
1.3e-3
2.0e-2
3.3e-5
2.8e-4
7.1e-4
1.2e-2
1.5e-5

1.6e-5
6.6e-5
6.5e-3
1.1e-5
7.4e-6
2.8e-5
3.1e-3
5.3e-6

2.4e-2
1.5e-2
1.4e-1
2.8e-3
2.0e-2
1.6e-2
9.3e-2
2.3e-3

1.6e-2
5.4e-3
3.9e-2
1.5e-4
9.2e-3
7.0e-3
3.0e-2
1.2e-4

1.5e-3
2.6e-3
2.3e-2
5.2e-5
1.2e-3
3.3e-3
1.9e-2
3.9e-5

2.3e-5
9.9e-5
1.5e-2
1.6e-5
1.7e-5
7.7e-5
8.1e-3
1.2e-5

1,000

2.3e-2
1.2e-2
1.4e-1
1.6e-3
1.5e-2
1.1e-2
8.4e-2
1.2e-3

Table 1. The results on face veriﬁcation. We report the average distortion (MSE) of the adversarial images generated by different methods
for SphereFace, CosFace, and ArcFace given 1,000, 5,000, 10,000, and 100,000 queries, based on the LFW dataset.

of CMA. An intuitive explanation of this update is that the
variance along the past successful directions should be en-
larged for future search.

3.2.4 Stochastic Coordinate Selection

For adversarial attacks, the perturbations added to the im-
ages could be very sparse to fool deep CNNs [27], indi-
cating that only a fraction of coordinates (pixels) are suf-
ﬁcient for ﬁnding the adversarial images. We can also ac-
celerate the black-box optimization if we could identify the
important coordinates. However, this is non-trivial in the
decision-based black-box attack setting. Fortunately, our
algorithm provides a natural way to ﬁnd the useful coor-
dinates for search since the elements in the diagonal co-
variance matrix C represent the preferred coordinates of the
past successful trials, i.e., larger cii indicates that searching
along the i-th coordinate may induce a higher success rate
based on the past experience. According to this, in each it-
eration we select k (k ≪ m) coordinates to generate the
random vector z with the probability of selecting the i-th
coordinate being proportional to cii (in Step 4-5).

3.2.5 Dimensionality Reduction

It has been proved that the dimensionality reduction of
the search space is useful for acceleration of black-box at-
tacks [3]. Based on this, we sample the random vector z in
a lower dimensional space Rm with m < n (in Step 3). We
then adopt an upscaling operator to project z to the original
space Rn (in Step 6). Note that we do not change the dimen-
sion of an input image but only reduce the dimension of the
search space. Speciﬁcally, we use the bilinear interpolation
method as the upscaling operator.

3.2.6 Hyper-parameter Adjustment

There are also several hyper-parameters in the proposed al-
gorithm, including σ, µ, cc, and ccov. We simply set cc =
∗, x) based
0.01 and ccov = 0.001. σ is set as 0.01 · D( ˜x
on the intuition that σ should shrink gradually when the dis-
tance from x decreases. µ is a critical hyper-parameter that
If µ is too large, the search
needs to be tuned carefully.
point may probably violate the adversarial criterion and the
success rate of updates is low. On the other hand, if µ is too

small, we would make little progress towards minimizing
∗ and x although the success rate is
the distance between ˜x
high. So we adopt the 1/5th success rule [21], which is a
traditional method for hyper-parameter control in evolution
strategies, to update µ as µ = µ · exp(Psuccess − 1/5), where
Psuccess is the success rate of several past trials.

4. Experiments

In this section, we present the experimental results to
demonstrate the effectiveness of the proposed evolutionary
attack method. We comprehensively evaluate the robustness
of several state-of-the-art face recognition models under the
decision-based black-box attack scenario. We further apply
the proposed method to attack a real-world face recognition
system to demonstrate its practical applicability.

4.1. Experimental Settings

Target models. We study three state-of-the-art face recog-
nition models, including SphereFace [16], CosFace [32] and
ArcFace [5]. In testing, the feature representation for each
image is ﬁrst extracted by these models. Then, the cosine
similarity between feature representations of different im-
ages are calculated. Finally, we use the thresholding strat-
egy and nearest neighbor classiﬁer for face veriﬁcation and
identiﬁcation, respectively.
Datasets. We conduct experiments on the Labeled Face in
the Wild (LFW) [12] and MegaFace [15] datasets. For face
veriﬁcation, in each dataset, we select 500 pairs of face im-
ages for dodging attack, in which each pair represent the
same identity. And, we select another 500 pairs of face im-
ages for impersonation attack, in which the images of each
pair are from different identities. For face identiﬁcation, in
each dataset, we select 500 images of 500 different identi-
ties to form a gallery set, and corresponding 500 images of
the same identities to form a probe set. We perform dodging
and impersonation attacks for images in the probe set. For
impersonation attack, the target identity is chosen randomly.
The input image size (i.e., the dimension of the input space
n) is 112×112×3. All the selected images can be correctly
recognized by the three face recognition models.
Compared methods. We compare the performance of the
evolutionary attack method with all existing methods for

7718

Model
Queries

Dodging

Impersonation

Boundary [1]

Optimization [4]

NES-LO [14]
Evolutionary
Boundary [1]

Optimization [4]

NES-LO [14]
Evolutionary

SphereFace [16]
5,000

10,000

100,000

1,000

CosFace [32]
10,000

5,000

100,000

1,000

ArcFace [5]
10,000

5,000

100,000

6.5e-3
2.1e-3
4.0e-2
6.6e-5
1.1e-2
7.7e-3
3.8e-2
1.6e-4

4.7e-4
8.3e-4
2.5e-2
2.5e-5
1.7e-3
3.7e-3
2.8e-2
6.3e-5

1.4e-5
4.6e-5
5.5e-3
9.9e-6
3.6e-5
1.6e-4
1.0e-2
2.3e-5

2.0e-2
1.0e-2
1.5e-1
1.2e-3
2.5e-2
1.9e-2
8.8e-2
2.2e-3

5.1e-3
2.0e-3
3.6e-2
6.2e-5
8.9e-3
7.1e-3
3.7e-2
1.3e-4

5.4e-4
8.2e-4
2.2e-2
2.3e-5
1.3e-3
3.3e-3
2.7e-2
4.6e-5

1.2e-5
4.0e-5
4.7e-3
7.5e-6
2.3e-5
1.1e-4
8.8e-3
1.5e-5

3.1e-2
2.0e-2
1.5e-1
3.2e-3
2.5e-2
2.0e-2
8.8e-2
3.7e-3

1.7e-2
6.1e-3
4.5e-2
1.6e-4
1.3e-2
1.1e-2
3.4e-2
2.5e-4

1.6e-3
2.7e-3
3.1e-2
5.4e-5
2.5e-3
6.0e-3
2.3e-2
8.8e-5

2.3e-5
9.8e-5
1.3e-2
1.6e-5
3.8e-5
3.5e-4
1.1e-2
2.6e-5

1,000

2.4e-2
1.1e-2
1.4e-1
1.3e-3
2.4e-2
1.9e-2
7.9e-2
2.5e-3

Table 2. The results on face identiﬁcation. We report the average distortion (MSE) of the adversarial images generated by different methods
for SphereFace, CosFace, and ArcFace given 1,000, 5,000, 10,000, and 100,000 queries, based on the LFW dataset.

Dodging Attack

Impersonation Attack

Dodging Attack

Impersonation Attack

Boundary
Optimization
NES-LO
Evolutionary

0

1

2

3

4

5

6

7

8

Queries

9
#10

10
4

Boundary
Optimization
NES-LO
Evolutionary

0

1

2

3

4

5

6

7

8

Queries

9
#10

10
4

Boundary
Optimization
NES-LO
Evolutionary

e
c
a
F
e
r
e
h
p
S

n
o
i
t
r
o
t
s
i
D

e
c
a
F
s
o
C

n
o
i
t
r
o
t
s
i
D

0

10

-1

10

-2

10

-3

10

-4

10

-5

10

0
10

-1

10

-2

10

-3

10

-4

10

-5

10

0
10

-1

10

-2

10

-3

10

n
o
i
t
r
o
t
s
i
D

e
c
a
F
c
r
A

-4

10

-5

10

0

10

-1

10

-2

10

-3

10

-4

10

-5

10

0
10

-1

10

-2

10

-3

10

-4

10

-5

10

0
10

-1

10

-2

10

-3

10

-4

10

-5

10

n
o
i
t
r
o
t
s
i
D

n
o
i
t
r
o
t
s
i
D

n
o
i
t
r
o
t
s
i
D

Boundary
Optimization
NES-LO
Evolutionary

0

1

2

3

4

5

6

7

8

Queries

9
#10

10
4

Boundary
Optimization
NES-LO
Evolutionary

0

1

2

3

4

5

6

7

8

Queries

9
#10

10
4

Boundary
Optimization
NES-LO
Evolutionary

e
c
a
F
e
r
e
h
p
S

n
o
i
t
r
o
t
s
i
D

e
c
a
F
s
o
C

n
o
i
t
r
o
t
s
i
D

e
c
a
F
c
r
A

n
o
i
t
r
o
t
s
i
D

0

10

-1

10

-2

10

-3

10

-4

10

-5

10

0

10

-1

10

-2

10

-3

10

-4

10

-5

10

0

10

-1

10

-2

10

-3

10

-4

10

-5

10

Boundary
Optimization
NES-LO
Evolutionary

0

1

2

3

4

5

6

7

8

Queries

9
10
4
#10

Boundary
Optimization
NES-LO
Evolutionary

0

1

2

3

4

5

6

7

8

Queries

9
#10

10
4

Boundary
Optimization
NES-LO
Evolutionary

Boundary
Optimization
NES-LO
Evolutionary

0

1

2

3

4

5

6

7

8

Queries

9
10
4
#10

Boundary
Optimization
NES-LO
Evolutionary

0

1

2

3

4

5

6

7

8

Queries

9
10
4
#10

Boundary
Optimization
NES-LO
Evolutionary

n
o
i
t
r
o
t
s
i
D

0

10

-1

10

-2

10

-3

10

-4

10

-5

10

0
10

-1

10

-2

10

-3

10

n
o
i
t
r
o
t
s
i
D

-4

10

-5

10

0
10

-1

10

-2

10

-3

10

-4

10

-5

10

n
o
i
t
r
o
t
s
i
D

0

1

2

3

4

5

6

7

8

Queries

9
#10

10
4

0

1

2

3

4

5

6

7

8

Queries

9
10
4
#10

0

1

2

3

4

5

6

7

8

Queries

9
10
4
#10

0

1

2

3

4

5

6

7

8

Queries

9
#10

10
4

Figure 2. The results on face veriﬁcation. We show the curves of
the average distortion (MSE) of the adversarial images generated
by different attack methods for SphereFace, CosFace, and ArcFace
over the number of queries, based on the LFW dataset.

Figure 3. The results on face identiﬁcation. We show the curves of
the average distortion (MSE) of the adversarial images generated
by different attack methods for SphereFace, CosFace, and ArcFace
over the number of queries, based on the LFW dataset.

decision-based black-box attacks, including the boundary
attack method [1], optimization-based method [4] and an
extension of NES in the label-only setting (NES-LO) [14].

Evaluation metrics. For all methods, the generated ad-
versarial examples are guaranteed to be adversarial. So we
measure the distortion between the adversarial and original
images by mean square error (MSE) to evaluate the perfor-
mance of different methods1. We set a maximum number
of queries to be 100,000 for each image across all experi-
ments. Due to the space limitation, we leave the results on
the MegaFace dataset in Appendix B. The results on both
datasets are consistent. Our method is generally applicable
beyond face recognition. We further present the results on

1Images are normalized to [0, 1].

the ImageNet dataset in Appendix C.

4.2. Experimental Results

We report the results on the LFW dataset in this section.
We perform dodging attack and impersonation attack by
Boundary, Optimization, NES-LO and the proposed Evolu-
tionary method against SphereFace, CosFace, and ArcFace,
respectively. For our method, we set the dimension of the
search space as m = 45 × 45 × 3, and k = m/20 for stochas-
tic coordinate selection. For other methods, we adopt the
default settings. We calculate the distortion (MSE) of the
adversarial images generated by each method averaged over
the selected 500 images. And, the distortion curves over
the number of queries for face veriﬁcation are shown in
Fig. 2, while those for face identiﬁcation in Fig. 3. Besides,

7719

k
c
a
t
t

A
 
g
n
i
g
d
o
D

k
c
a
t
t

 

A
n
o
i
t
a
n
o
s
r
e
p
m

I

Original pair

0 queries

100 queries

1,000 queries

2,000 queries

10,000 queries 100,000 queries

Original pair

0 queries

100 queries

1,000 queries

2,000 queries

10,000 queries 100,000 queries

1.3e-1

1.7e-2

3.3e-3

1.0e-3

7.4e-5

2.4e-5

6.1e-2

1.2e-2

1.4e-3

4.1e-4

2.5e-5

9.9e-6

Figure 4. Examples of dodging and impersonation attacks on face veriﬁcation for the ArcFace [5] model. The initial adversarial image is a
random noise or the target image for each kind of attacks. The distortion between the adversarial image and the original image decreases
gradually. We show the total number of queries and the mean square error until each point.

SphereFace

CosFace

ArcFace

wo/ CMA, wo/ SCS
w/ CMA, wo/ SCS
w/ CMA, w/ SCS (C)
w/ CMA, w/ SCS (In)

2.6e-4/1.9e-4
2.4e-4/1.8e-4
1.7e-4/1.3e-4
2.0e-4/1.5e-4

2.5e-4/9.2e-5
2.3e-4/8.5e-5
1.6e-4/6.4e-5
1.9e-4/7.5e-5

4.2e-4/2.6e-4
3.8e-4/2.5e-4
2.6e-4/1.7e-4
3.0e-4/2.0e-4

Table 3. Comparisons of the evolutionary method with four set-
tings: without CMA or SCS; with CMA, without SCS; with CMA
and SCS where the selection probability is proportional to the ele-
ments in C; with CMA and SCS where the selection probability is
set equally. We report the average distortion (MSE) given 10,000
queries for dodging/impersonation attacks on face veriﬁcation.

for 1,000, 5,000, 10,000, and 100,000 queries, we report
the corresponding distortion values of different methods for
face veriﬁcation in Table 1, while those for face identiﬁca-
tion in Table 2. Two visual examples are also presented in
Fig. 4 for dodging and impersonation attacks.

Above results demonstrate that our method converges
much faster and achieves smaller distortions compared with
other methods consistently across both tasks (i.e., face ver-
iﬁcation and identiﬁcation), both attack settings (i.e., dodg-
ing and impersonation) and all face models. For example,
as shown in Table 1 and 2, given 5,000 queries our method
obtains the distortions which are about 30 times smaller
than those generated by the second best method (i.e., Opti-
mization), which validates the effectiveness of the proposed
method. From Fig. 4, it can be seen that 2,000 queries are
sufﬁcient to generate visually indistinguishable adversarial
examples. For NES-LO, the hard-label predictions are ﬁrst
used to estimate the predicted probability (e.g., 25 queries)
and then it approximates the gradient by NES (e.g., 40 tri-
als). In consequence, this method requires more than 1,000
queries for only one update, which leads to the worst results.

It should be noted that the face recognition models are
extremely vulnerable to adversarial examples. These mod-
els can be fooled in the black-box manner by adversarial
examples with about only 1e−5 distortions, which are visu-
ally imperceptible for humans, as shown in Fig. 4.

Dodging Attack

Impersonation Attack

e
c
a
F
e
r
e
h
p
S

n
o
i
t
r
o
t
s
i
D

e
c
a
F
s
o
C

n
o
i
t
r
o
t
s
i
D

e
c
a
F
c
r
A

n
o
i
t
r
o
t
s
i
D

-1

10

-2

10

-3

10

-4

10

-5

10

0

-1

10

-2

10

-3

10

-4

10

-5

10

0

-1

10

-2

10

-3

10

-4

10

-5

10

0

m=15#15#3
m=30#30#3
m=45#45#3
m=60#60#3
m=112#112#3

2000

4000

6000

8000

10000

Queries

m=15#15#3
m=30#30#3
m=45#45#3
m=60#60#3
m=112#112#3

2000

4000

6000

8000

10000

Queries

m=15(cid:1)15(cid:1)3
m=30(cid:1)30(cid:1)3
m=45(cid:1)45(cid:1)3
m=60(cid:1)60(cid:1)3
m=112(cid:1)112(cid:1)3

2000

4000

6000

8000

10000

Queries

-1

10

-2

10

-3

10

-4

10

n
o
i
t
r
o
t
s
i
D

-5

10

0

-1

10

-2

10

-3

10

-4

10

n
o
i
t
r
o
t
s
i
D

-5

10

0

-1

10

-2

10

-3

10

-4

10

n
o
i
t
r
o
t
s
i
D

-5

10

0

m=15#15#3
m=30#30#3
m=45#45#3
m=60#60#3
m=112#112#3

2000

4000

6000

8000

10000

Queries

m=15#15#3
m=30#30#3
m=45#45#3
m=60#60#3
m=112#112#3

2000

4000

6000

8000

10000

Queries

m=15(cid:1)15(cid:1)3
m=30(cid:1)30(cid:1)3
m=45(cid:1)45(cid:1)3
m=60(cid:1)60(cid:1)3
m=112(cid:1)112(cid:1)3

2000

4000

6000

8000

10000

Queries

Figure 5. We show the curves of the average distortion (MSE)
of the adversarial images generated by the evolutionary method
with different dimensions of the search space over the number of
queries. We perform dodging and impersonation attacks against
SphereFace, CosFace, and ArcFace on face veriﬁcation.

4.3. Ablation Study

We perform ablation study in this section to validate the
effectiveness of each component in the proposed method.
We conduct experiments based on face veriﬁcation on the
LFW dataset. In particular, we study the effects of covari-
ance matrix adaptation, stochastic coordinate selection and
dimensionality reduction respectively.

Covariance matrix adaptation (CMA). To examine the

7720

usefulness of CMA, we compare CMA with a baseline
method that the covariance matrix is set to In without up-
dating. We do not include stochastic coordinate selection or
dimensionality reduction in this part for solely examining
the effect of CMA. We show the results of the average dis-
tortion given 10,000 queries in the ﬁrst two rows of Table 3.
CMA improves the results over the baseline method.

Stochastic coordinate selection (SCS). We study two
aspects of SCS. The ﬁrst is whether SCS is useful. The sec-
ond is whether we should select the coordinates with prob-
ability being proportional to the diagonal elements in the
covariance matrix C. We further perform experiments with
SCS, where we compare the performance of SCS with the
selection probability of each coordinate being proportional
to each diagonal element in C or In (equal probability for
each coordinate). By comparing the 2-4 rows of Table 3, it
can be seen that SCS is beneﬁcial for obtaining better results
and sampling coordinates with probability proportional to
cii is better than sampling with equal probability.

Dimensionality reduction. We ﬁnally study the inﬂu-
ence of dimensionality reduction. We set the dimension m
of the search space as 15 × 15 × 3, 30 × 30 × 3, 45 × 45 × 3,
60×60×3, and 112×112×3. We perform dodging and im-
personation attacks against SphereFace, CosFace, and Arc-
Face with each m, and compare the results in Fig. 5. It can
be seen that the evolutionary method converges faster in a
lower dimensional search space. However, if the dimension
of the search space is too small (e.g., 15 × 15 × 3), the at-
tack results in relatively large distortions. So we choose a
medium dimension as 45×45×3 in the above experiments.

4.4. Attacks on a Real World Application

In this section, we apply the evolutionary attack method
to the face veriﬁcation API in Tencent AI Open Platform2.
This face veriﬁcation API allows users to upload two face
images, and outputs a similarity score of them. We set the
threshold to be 90, i.e., if the similarity score is larger than
90, the two images are predicted to be the same identity;
and if not, they are predicted to be different identities.

We choose 10 pairs of images from the LFW dataset to
perform impersonation attack. The original two face images
of each pair are from different identities. We generate a per-
turbation for one of them and make the API recognize the
adversarial image to be the same identity as the other im-
age. We set the maximum number of queries to be 10,000.
We use the proposed evolutionary method to attack the face
veriﬁcation API and compare the results with Boundary [1]
and Optimization [4]. We do not present the result of NES-
LO [14], as it fails to generate an adversarial image within
10,000 queries. We show the average distortion between the
adversarial and original images in Table 4. Our method still
obtains a smaller distortion than other baseline methods.

2https://ai.qq.com/product/face.shtml#compare

Attack Method

Distortion (MSE)

Boundary [1]

Optimization [4]

Evolutionary

1.63e-2
1.71e-2
2.54e-3

Table 4. The results of impersonation attack on the real-world face
veriﬁcation API. We report the average distortion (MSE) of the
selected 10 pairs of images by different attack methods.

Original pair

Evolutionary

Boundary

Optimization

Figure 6. Examples of impersonation attack on the real-world face
veriﬁcation API. We show the original pairs of images as well as
the adversarial images generated by each method.

We also show two examples in Fig. 6. It can be seen that
the adversarial images generated by our method are more
visually similar to the original images, while those gener-
ated by other methods have large distortions, making them
distinguishable from the original images.

5. Conclusion

In this paper, we proposed an evolutionary attack al-
gorithm to generate adversarial examples in the decision-
based black-box setting. Our method improves the efﬁ-
ciency over the other methods by modeling the local ge-
ometry of the search directions and meanwhile reducing
the dimension of the search space. We applied the pro-
posed method to comprehensively study the robustness of
several state-of-the-art face recognition models, and com-
pared against the other methods. The extensive experiments
consistently demonstrate the effectiveness of the proposed
method. We showed that the existing face recognition mod-
els are extremely vulnerable to adversarial attacks in the
black-box manner, which raises security concerns for de-
veloping more robust face recognition models. We ﬁnally
attacked a real-world face recognition system by the pro-
posed method, demonstrating its practical applicability.

Acknowledgements

Most of this work was done when Yinpeng Dong was
intern at Tencent AI Lab, supported by the Tencent Rhino-
Bird Elite Training Program. Yinpeng Dong, Hang Su, and
Jun Zhu are supported by the National Key Research and
Development Program of China (No. 2017YFA0700904),
NSFC Projects (Nos.
61620106010, 61621136008,
61571261), Beijing NSF Project (No. L172037), Tiangong
Institute for Intelligent Computing, NVIDIA NVAIL Pro-
gram, DITD Program JCKY2017204B064, and the projects
from Siemens and Intel.

7721

References

[1] W. Brendel, J. Rauber, and M. Bethge. Decision-based ad-
versarial attacks: Reliable attacks against black-box machine
learning models. In ICLR, 2018. 2, 5, 6, 8

[2] N. Carlini and D. Wagner. Towards evaluating the robustness
In IEEE Symposium on Security and

of neural networks.
Privacy, 2017. 2

[3] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh. Zoo:
Zeroth order optimization based black-box attacks to deep
neural networks without training substitute models. In Pro-
ceedings of the 10th ACM Workshop on Artiﬁcial Intelligence
and Security, pages 15–26. ACM, 2017. 2, 3, 5

[4] M. Cheng, T. Le, P.-Y. Chen, J. Yi, H. Zhang, and
Query-efﬁcient hard-label black-box at-
arXiv preprint

C.-J. Hsieh.
tack: An optimization-based approach.
arXiv:1807.04457, 2018. 2, 3, 5, 6, 8

[5] J. Deng, J. Guo, and S. Zafeiriou. Arcface: Additive an-
gular margin loss for deep face recognition. arXiv preprint
arXiv:1801.07698, 2018. 1, 2, 5, 6, 7

[6] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li.
In CVPR,

Boosting adversarial attacks with momentum.
2018. 1, 2

[7] A. D. Flaxman, A. T. Kalai, and H. B. Mcmahan. Online
convex optimization in the bandit setting:gradient descent
without a gradient. In Sixteenth ACM-SIAM Symposium on
Discrete Algorithms, pages 385–394, 2005. 3

[8] S. Ghadimi and G. Lan. Stochastic ﬁrst- and zeroth-order
methods for nonconvex stochastic programming. SIAM Jour-
nal on Optimization, 23(4):2341–2368, 2013. 3

[9] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and

harnessing adversarial examples. In ICLR, 2015. 1, 2

[10] N. Hansen and A. Ostermeier. Completely derandomized
self-adaptation in evolution strategies. Evolutionary compu-
tation, 9(2):159–195, 2001. 3, 4

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1

[12] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller. La-
beled faces in the wild: A database forstudying face recog-
nition in unconstrained environments. In Workshop on faces
in’Real-Life’Images: detection, alignment, and recognition,
2008. 1, 2, 5

[13] C. Igel, T. Suttorp, and N. Hansen. A computational efﬁ-
cient covariance matrix update and a (1+ 1)-cma for evolu-
tion strategies. In Proceedings of the 8th annual conference
on Genetic and evolutionary computation, pages 453–460.
ACM, 2006. 3

[14] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Black-box
adversarial attacks with limited queries and information. In
ICML, 2018. 2, 3, 5, 6, 8

[15] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and
E. Brossard. The megaface benchmark: 1 million faces for
recognition at scale. In CVPR, 2016. 1, 2, 5

[16] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
Sphereface: Deep hypersphere embedding for face recogni-
tion. In CVPR, 2017. 1, 2, 5, 6

[17] Y. Liu, X. Chen, C. Liu, and D. Song. Delving into transfer-
able adversarial examples and black-box attacks. In ICLR,
2017. 2

[18] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. Towards deep learning models resistant to adver-
sarial attacks. In ICLR, 2018. 2

[19] S. M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi,

and
P. Frossard. Universal adversarial perturbations. In CVPR,
2017. 1, 2

[20] Y. Nesterov and V. Spokoiny. Random gradient-free mini-
mization of convex functions. Foundations of Computational
Mathematics, 17(2):527–566, 2017. 3
[21] I. Rechenberg. Evolutionsstrategien.

In Simulationsmeth-
oden in der Medizin und Biologie, pages 83–114. Springer,
1978. 5

[22] R. Ros and N. Hansen. A simple modiﬁcation in cma-es
achieving linear time and space complexity. In International
Conference on Parallel Problem Solving from Nature, pages
296–305. Springer, 2008. 4

[23] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In CVPR,
2015. 1, 2

[24] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter. Ac-
cessorize to a crime: Real and stealthy attacks on state-
of-the-art face recognition.
In ACM Sigsac Conference on
Computer and Communications Security, pages 1528–1540,
2016. 1, 2

[25] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter. Ad-
versarial generative nets: Neural network attacks on state-of-
the-art face recognition. arXiv preprint arXiv:1801.00349,
2017. 1, 2

[26] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
1

[27] J. Su, D. V. Vargas, and S. Kouichi. One pixel attack for fool-
ing deep neural networks. arXiv preprint arXiv:1710.08864,
2017. 5

[28] Y. Sun, X. Wang, and X. Tang. Deep learning face repre-
sentation from predicting 10,000 classes. In CVPR, 2014. 1,
2

[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 1

[30] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. In ICLR, 2014. 1, 2, 4

[31] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR, 2014. 1, 2

[32] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li,
and W. Liu. Cosface: Large margin cosine loss for deep face
recognition. In CVPR, 2018. 1, 2, 5, 6

[33] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In ECCV,
2016. 1, 2

7722

