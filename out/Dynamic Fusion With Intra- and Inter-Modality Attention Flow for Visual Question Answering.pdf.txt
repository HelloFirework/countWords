Dynamic Fusion with Intra- and Inter-modality Attention

Flow for Visual Question Answering

Peng Gao1, Zhengkai Jiang3, Haoxuan You4,

Pan Lu4, Steven Hoi 2, Xiaogang Wang 1, Hongsheng Li 1

1CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong

2Singapore Management University 3NLPR, CASIA 4Tsinghua University

{1155102382@link, xgwang@ee, hsli@ee}.cuhk.edu.hk

Abstract

Learning effective fusion of multi-modality features is
at the heart of visual question answering. We propose a
novel method of dynamically fusing multi-modal features
with intra- and inter-modality information ﬂow, which al-
ternatively pass dynamic information between and across
the visual and language modalities.
It can robustly cap-
ture the high-level interactions between language and vi-
sion domains, thus signiﬁcantly improves the performance
of visual question answering. We also show that the pro-
posed dynamic intra-modality attention ﬂow conditioned
on the other modality can dynamically modulate the intra-
modality attention of the target modality, which is vital
for multimodality feature fusion. Experimental evaluations
on the VQA 2.0 dataset show that the proposed method
achieves state-of-the-art VQA performance. Extensive ab-
lation studies are carried out for the comprehensive analy-
sis of the proposed method.

1. Introduction

Visual Question Answering [2] aims at automatically an-
swering a natural language question related to the contents
of a given image. It has extensive applications in practice,
such as assisting blind people [12] and education of young
children, and therefore become a hot research topic recently.
The performance of Visual Question Answering (VQA) has
been substantially improved in recent years thanks to three
lines of works. Firstly, better visual and language feature
representations are at the core for boosting VQA perfor-
mance. The feature learning capability from VGG [35],
ResNet [13], FishNet [36] to the recent bottom-up & top-
down features [1] increases the VQA performance signif-
icantly. Secondly, different variants of attention mecha-
nisms [40] can adaptively select important features which
can help deep learning achieve better recognition accuracy.

Thirdly, better multi-modality fusion approaches, such Bi-
linear Fusion [9], MCB [7] and MUTAN [4], have been
proposed for better capturing the high-level interactions be-
tween language and visual features.

Despite being studied extensively, most existing VQA
approaches focus on learning inter-modality relations be-
tween visual and language features. Bilinear feature fusion
approaches [9] focus on capturing the higher order relations
between language and visual modalities by feature outer
product. Co-attention [39, 28, 24] or bilinear attention-
based approaches
[19] learn the inter-modality relations
between word-region pairs to identify key pairs for ques-
tion answering. On the other hand, there exist computer
vision and natural language processing algorithms focus-
ing on learning intra-modality relations. Hu et al. [14] pro-
posed to explore intra-modality object-to-object relations to
boost object detection accuracy. Yao et al. [42, 26] mod-
eled intra-modality object-to-object relations for improv-
ing image captioning performance.
In the recently pro-
posed BERT algorithm [6] for natural language process-
ing, intra-modality word relations are modelled by self-
attention mechanism to learn state-of-the-art word embed-
ding. However, the inter- and intra-modality relations were
never jointly investigated in a uniﬁed framework for solv-
ing the VQA problem. We argue that, for the VQA problem,
the intra-modality relations within each modality is comple-
mentary to the inter-modality relations, which were mostly
ignored by existing VQA methods. For instance, for the im-
age modality, each image region should obtain information
not only from its associate words/phrases in the question
but also from related image regions to infer the answer of
the question. For the question modality, better understand-
ing of question can be acquired by inferring other words.
Such cases motivate us to propose a uniﬁed framework for
modelling both inter- and intra-modality information ﬂow.

To overcome the limitations, we propose a novel Dy-
namic Fusion with Intra- and Inter-modality Attention Flow

43216639

Intra-Inter Modality Attention Module

Intra-Inter Modality Attention Module

Inter-Modality Attention Flow Intra-Modality Attention Flow

Inter-Modality Attention Flow Intra-Modality Attention Flow

Visual 
Feature

RCNN

What is the man 

pointing at

GRU

(cid:17)Element-wise Product

Visual 
Feature
µ
   × 2048

Question 
Feature
14 × 1280

...

...

...

(cid:17)

×<

Classﬁer

w
h
a
t

i

s

t
h
e

m
a
n

a
t

i

p
o
n
t
i
n
g

w
h
a
t

i

s

t
h
e

m
a
n

a
t

i

p
o
n
t
i
n
g

w
h
a
t

i

s

t
h
e

m
a
n

a
t

i

p
o
n
t
i
n
g

w
h
a
t

i

s

t
h
e

m
a
n

a
t

i

p
o
n
t
i
n
g

Question 
Feature

Figure 1: Illustration of the proposed Dynamic Fusion with Intra- and Inter-modality Attention Flow (DFAF) for visual
question answering. Each DFAF module contains one Inter-Modality Attention Flow and one of Intra-Modality Attention
Flow Module. Stacking several blocks of DFAF can help the network gradually focus on important image regions , question
words and the latent alignments.

(DFAF) framework for efﬁcient multi-modality feature fu-
sion to accurately answer visual questions. The overall
diagram is shown in Figure 1. Our DFAF framework
integrates cross-modal self-attention and cross-modal co-
attention mechanisms to achieve effective information ﬂows
within and between the image and language modalities.
Given visual and question features encoded by deep neu-
ral networks, the DFAF framework ﬁrst generates inter-
modality attention ﬂow (InterMAF) to pass information be-
tween image and language. In the InterMAF module, vi-
sual and language features generate a joint-modality co-
attention matrix. Each visual region would select question
features according to the joint-modality co-attention matrix
and vice versa. The InterMAF module fuses and updates
each image region and each word’s features according to the
attention-weighted information ﬂows from the other modal-
ity. Following the InterMAF module, DFAF calculates the
dynamic intra-modality attention ﬂow (DyIntraMAF) for
passing information ﬂows within each modality to capture
the complex intra-modality relations. Visual regions and
sentence words generate self-attention weights and aggre-
gate attention-weighted information from other instances in
the same modality. More importantly, although the infor-
mation are only propagated within the same modalities, in-
formation of the other modality is considered and used to
modulate intra-modality attention weights and ﬂows. With
such an operation, the attention ﬂows within each modal-
ity are dynamically conditioned on the other modality and
is the key difference compared with existing intra-modality
message passing methods on object detection [14] and im-
age captioning [42]. DyIntraMAF is shown to be substan-
tially better than its variant using only internal information
for intra-modality information ﬂow and is the key to the

success of the proposed framework. We alternatively use
InterMA and DyIntraMA modules to create the basic blocks
of the DFAF. Multiple stacks of DFAF blocks are shown to
further improve the VQA performance.

Our contributions can be summarized into threefold. (1)
A novel Dynamic Fusion with Intra- and Inter-modality
Attention Flow (DFAF) framework is proposed for multi-
modality fusion by interleaving intra- and inter-modality
feature fusion. Such a framework for the ﬁrst time inte-
grates inter-modality and dynamic intra-modality informa-
tion ﬂow in a uniﬁed framework for tackling the VQA task.
(2) Dynamic Intra-modality Attention Flow (DyIntraMAF)
module is proposed for generating effective attention ﬂows
within each modality, which are dynamically conditioned
on the information of the other modality. It is one of the
core novelties of our proposed framework.
(3) Extensive
experiments and ablation studies are performed to exam-
ine the effectiveness of the proposed DFAF framework, in
which state-of-the-art VQA performance is achieved by our
proposed DFAF framework.

2. Related Work

Representation learning for VQA. The recent boost of
VQA performance is due to the success of deep represen-
tation learning.
In the early stage of VQA methods, the
VGG [35] network was commonly used. With the introduc-
tion of ResNet [13], the VQA community shift to ResNet
networks, which outperform VGG by large margins. Re-
cently, the bottom-up and top-down network [1] derived
from faster RCNN [33] are shown to be suitable for VQA
and image captioning tasks. Feature learning is an essential
component for the development of VQA algorithms.

Bilinear Fusion for VQA. Solving VQA requires un-

43226640

l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
derstanding of visual and language contents as well as the
relation between them.
In early VQA methods, simple
concatenation or element-wise multiplication [45] between
visual and language are used for cross-modal feature fu-
sion. To capture the high-level interactions between the two
modalities, Bilinear Fusion [9] has been proposed to adopt
bilinear pooling to fuse features from the two modalities.
To overcome the limitation of high computational cost of
bilinear pooling, many approximated fusion methods, in-
cluding MCB [7], MLB [20] and MUTAN [4], were pro-
posed, which have shown better performance than bilinear
fusion [9] with much fewer parameters.

Self-attention-based methods. The attention mecha-
nism in deep learning tries to mimic how human vision
works. By automatically ignoring irrelevant information
from the data, neural networks can selectively focus on
important features. This approach has achieved great suc-
cess in Natural Language Processing (NLP) [3], image cap-
tioning [40] and VQA [46]. There are many variants of
the attention mechanism. Our approach are mainly mo-
tivated by self-attention and co-attention based methods.
The self-attention mechanism [37] transforms features into
query, key and value features. The attention matrix be-
tween different features are then calculated by the inner
product of query and key features. After acquiring the
attention matrix, features are aggregated as the attention-
weighted summation of the original features. Motivated
by the self-attention mechanism, many vision tasks’ per-
formances were improved signiﬁcantly. Non-local neural
network [38] proposed a non-local module for aggregat-
ing information between different frames within one video
and achieved state-of-the-art performance in video classi-
ﬁcation. Relation Network learn [14] the relationship be-
tween object proposals by adopting the self-attention mech-
anism. The in-place module can boost Faster RCNN [33]
and Non-Maximum-Suppression (NMS) performance.

The

Co-attention-based methods.

co-attention
based [39, 28] vision and language methods model the
interactions across the two modalities. For each word,
every image region features are aggregated to the word
according to the co-attention weights. The co-attention
mechanism has been widely used in NLP and VQA tasks.
In [29], Dense Symmetric Co-attention (DCN) has been
proposed.
It achieved state-of-the-art performances on
VQAv1 and VQAv2 datasets without using any bottom-up
and top-down features. The success of DCN is due to dense
concatenation [16] of symmetric co-attention.

Other works for language and vision tasks. Be-
yond above mentioned methods, many algorithms have
also been proposed for fusion of cross-modal language
and visual features. Dynamic Parameter Prediction [30]
and Question-guided Hybrid Convolution [8] utilized dy-
namically predicted parameters for feature fusion. Adap-

tive attention [27] introduced a visual sensual which can
skip attention during image captioning. Structured atten-
tion [21] adopted the MRF model over attention maps for
better modelling better spatial attention distributions. Lo-
cally weighted deformable neighbours [18] proposed to pre-
dict offset and modulation weight.

3. Dynamic Fusion with Intra- and Inter-

modality Attention Flow for VQA

3.1. Overview

The proposed approach consists of a series of DFAF
modules. The whole pipeline is illustrated at ﬁgure 1 Visual
and language features between the two modalities are ﬁrst
weighted with the co-attention mechanism and aggregated
between the modalities to each image region and each word
by the proposed Inter-modality Attention Flow (InterMAF)
module, which learns the cross-modal interactions between
the image regions and question words. Following the inter-
modality module, to model the relationships within each
modality, i.e., word-to-word relations and region-to-region
relations, the Dynamic Intra-modality Attention Flow (Dy-
IntraMAF) module is adopted. It weights words and regions
within each modality and aggregates their features to the
words and regions again, which could be viewed as pass-
ing information ﬂows within each modality. Importantly, in
our proposed intra-modality module, the attention ﬂows are
dynamically conditioned on the information from the other
modality, which is a key difference compared with existing
self-attention based methods. Such InterMAF and DyIntra-
MAF modules could be stacked multiple times to pass the
information ﬂows among words and regions iteratively to
model the latent alignments for visual question answering.

3.2. Base visual and language feature extraction

To obtain base visual and language features, we ex-
tract image features from bottom-up & top-down attention
model [1]. The visual region features are obtained from a
Faster RCNN [33] model pretrained on Visual Genome [23]
dataset. For each image, we extract 100 region propos-
als and its associated region features. Given the input im-
age I, the obtained region visual features are denoted as
R ∈ Rµ×2048, where the ith region feature is denoted as
ri ∈ R2048 and there are µ object regions in total. The ob-
ject visual features are ﬁxed during training.
We adopt GLoVe word embeddings [32] as the inputs of
the Gated Recurrent Unit (GRU) [5] for encoding question
word features. Given the question Q, we obtain word-level
features E ∈ R14×1280 from GRU, where the jth word fea-
ture is denoted as ej ∈ R1280 and all questions are padded
and truncated to the same length 14.
The obtained visual object region features R and ques-

43236641

tion features E could be denoted as

R = RCNN(I; θRCNN),
E = GRU(Q; θGRU).

(1)

(2)

where visual feature parameters θRCNN are ﬁxed while ques-
tion features θGRU are learned from scratch and updated to-
gether when training our proposed framework.

3.3. Inter modality Attention Flow

The Inter-modality Attention Flow (InterMAF) module
as shown in Figure 1 ﬁrst learns to capture the importance
between each pair of visual region and word features.
It
then passes information ﬂows between the two modalities
according to the learned importance weights and aggregate
features to update each word feature and image region fea-
ture. Such an information ﬂow process is able to identify
cross-modal relations between visual regions and words.

Given visual region and word features, we ﬁrst calcu-
late the association weights between every pair of visual re-
gion and word. Each visual region and word features are
ﬁrst transformed into query, key and value features follow-
ing [34, 41], where the transformed region features are de-

noted as RK , RQ, RV ∈ Rµ×dim; Transformed word fea-
tures are denoted as EK , EQ and EV ∈ R14×dim,

RK = Linear(R; θRK), EK = Linear(R; θEK),
RQ = Linear(R; θRQ), EQ = Linear(E; θEQ),
RV = Linear(R; θRV ), EV = Linear(E; θEV ).

(3)

(4)

(5)

where “Linear” denotes a fully-connected layer with param-
eter θ, and dim represents the common dimension of trans-
formed features from both modalities.

By calculating the inner product RQET

K between ev-
ery pair of visual region feature RQ and word key feature
EK , we obtain the raw attention weights for aggregating
information from word features to each of the visual fea-
tures, and vice versa. After normalizing the raw weights
with the square root of the dimension number and a soft-
max non-linearity function, we obtain the two sets of atten-

mation ﬂow transmitted from words to image regions, and

tion weights, InterMAFR←E ∈ Rµ×14 for weighting infor-
InterMAFR→E ∈ R14×µ for weighting information ﬂow

transmitted from image regions to sentence words,

InterMAFR←E = softmax(

InterMAFR→E = softmax(

RQET
K√dim
EQRT
K√dim

),

).

(6)

(7)

The inner product values are proportional to the dimension
of hidden feature space, thus need to be normalized by the
square root of hidden dimension. The softmax non-linearity
function is applied row-wisely.

The two bi-directional InterMAF matrices capture the
importances between every image region and word pairs.
Take the InterMAFR←E for example, each row stands for
the attention weights between one visual region and all
word embeddings. Information from all word embeddings
to this one image region feature could be aggregated as
the weighted summation of the word value features EV .
We denote the information ﬂows to update visual region
features and word features by the InterMAF module as

Rupdate ∈ Rµ×dim and Eupdate ∈ R14×dim, respectively,

Rupdate = InterMAFR←E × EV ,
Eupdate = InterMAFR→E × RV .

(8)

(9)

where EV and RV are the un-weighted information
ﬂows(value features) to update visual region features and
word features in Eq. (5), and the two InterMAF matrices
are used to weight such information ﬂows.

After acquiring the updated visual and word features, we
concatenate them with original visual features R and word
features E. A fully connected layer is utilized to transform
the concatenated features into output features,

R = Linear([R, Rupdate]T ; θRT ),
E = Linear([E, Eupdate]T ; θET ).

(10)

(11)

The output features by the InterMAF module would then
be fed into the following Dynamic Intra-modality Attention
Flow module for learning intra-modality information ﬂows
to further update the visual region and word features for
capturing region-to-region and word-to-word relations.

3.4. Dynamic Intra modality Attention Flow

The input visual regions and word features of DyIntra-
MAF have encoded cross-modal relations between visual
regions and words. However, we argue that relationships
within each modality are complementary to the cross-modal
relations and should be taken into account for improving
the VQA accuracy. For example, for the question, “who is
above the skateboard ?”, the intra-modality module should
relate the region above the skateboard and the skateboard
region to infer the ﬁnal answer. Therefore, we propose
the Dynamic Intra-modality Attention Flow (DyIntraMAF)
module for modelling such within-modality relations with a
dynamic attention mechanism. The implementation of Dy-
IntraMAF is illustrated at Figure 2.

The naive intra-modality matrices to capture the impor-
tance between regions and between words could be deﬁned
similarly to Eq. (5) as,

IntraMAFR←R = softmax(

IntraMAFE←E = softmax(

RQRT
K√dim
EQET
K√dim

),

).

(12)

(13)

43246642

Average
Pooling

σ

Expand

Conditional 
Gating Vector

(cid:17)

Element-wise Product

×<

Matrix Multiplication

σ

Sigmoid

dimensional feature vector of each modality is then pro-
cessed by a sigmoid non-linearity function σ(·) to generate
channel-wise conditioning gates for the other modality,

GR→E = σ(Linear(Avg Pool(R); θRP )),
GR←E = σ(Linear(Avg Pool(E); θEP )).

(14)

(15)

w
h
a
t

i

s

t
h
e

m
a
n

a
t

i

p
o
n
t
i
n
g

Question 
Feature

Region
Update

The query and key features from both modalities are then
modulated by the conditional gates from the other modality

(cid:17)

Attention

Mask

Key

Softmax

×<

(cid:17)

×<

Region 
Feature

Query

Value

Figure 2:
Illustration of the proposed Dynamic Intra-
Modality Attention Flow module. Only intra-modality at-
tention ﬂow within the visual modality conditioned on ques-
tion are shown. By average pooling over question features,
the conditional gating vector can be acquired for control-
ling the information ﬂows among region features. Attention
will focus on question related information ﬂows. Row-wise
softmax is applied to obtain the attention weight.

The dot products are utilized to estimate their within-
modality importance between the same modality’s query
and key features. Such weight matrices could then be used
to weight the information ﬂows transmitted within each
modality. Modelling within-modality relationships have
been shown to be effective in object detection [14], image
captioning and BERT word embedding pretraining [6].

However,

the naive IntraMAF module only utilizes
within-modality information for estimating the region-to-
region and word-to-word importance. Some relations are
important but could only be identiﬁed conditioned on infor-
mation from the other modality. For instance, even for the
same input image, relations between different visual region
pairs should be weighted differently according to different
questions. We therefore propose a Dynamic Intra-modality
Attention Flow (DyIntraMAF) module to estimate within-
modality relation importance conditioned on the informa-
tion from the other modality.

To summarize the conditioning information from the
other modality, we average pool the visual region features
along the object-index dimension and the word features
along the word-index dimension. The average pooled fea-
tures of both modalities are then transformed to a dim-
dimensional feature vector to match the dimension of
the query and key features RQ, RK , EQ, EK . The dim-

ˆRQ = (1 + GR←E) ⊙ RQ, ˆRK = (1 + GR←E) ⊙ RK ,
ˆEQ = (1 + GR←E) ⊙ RQ, ˆEK = (1 + GR←E) ⊙ EK .

(16)

where ⊙ denotes element-wise multiplication. Channels of
query and key features would be activated or deactivated by
channel-wise gates conditioned on the other modality. Such
a design of the two gating vectors share the similar spirit
with Squeeze and Excitation Network [15] and the Gated
Convolution [10]. The key difference is that the channel-
wise gating vector is created based on cross-modal infor-
mation.

The dynamic intra-modality attention ﬂow matrices

DyIntraMAFR←R ∈ Rµ×µ and DyIntraMAFE←E ∈
R14×14 are then obtained by the gated query and key fea-
tures to weight different within-modality relations,

DyIntraMAFR←R = softmax(

DyIntraMAFE←E = softmax(

ˆRQ ˆRT
K√dim
ˆEQ ˆET
K√dim

),

).

(17)

(18)

Visual region and word features are then updated by the

weighted value features RV and EV via residual,

R = Linear(R + Rupdate; θRD),
E = Linear(E + Eupdate; θED).

where

Rupdate = DyIntraMAFR←R × RV ,
Eupdate = DyIntraMAFE←E × EV .

(19)

(20)

(21)

(22)

Note that here we only make key and query features con-
ditioned on the other modality to adaptively weight within-
modality information ﬂows.
In our ablation studies, we
observe that the proposed DyIntraMAF module by a large
margin outperforms the naive IntraMAF module.

3.5. The Framework with Intra  and Inter modality

Attention Flow

In this section, we introduce how to integrate intra-
and inter-modality attention ﬂow modules into our pro-
posed framework. The whole pipeline is illustrated in Fig-
ure 1.The proposed framework ﬁrst extracts visual region

43256643

l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
E
K
J
y
6
r
j
X
h
i
A
K
r
B
U
X
G
S
g
3
J
2
V
q
H
F
Q
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
g
F
4
8
R
z
A
O
S
J
c
x
O
Z
p
M
x
s
z
P
L
T
K
8
Q
Q
v
7
B
i
w
d
F
v
P
o
/
3
v
w
b
J
8
k
e
N
L
G
g
o
a
j
q
p
r
s
r
S
q
W
w
6
P
v
f
X
m
F
t
f
W
N
z
q
7
h
d
2
t
n
d
2
z
8
o
H
x
4
1
r
c
4
M
4
w
2
m
p
T
b
t
i
F
o
u
h
e
I
N
F
C
h
5
O
z
W
c
J
p
H
k
r
W
h
0
O
/
N
b
T
9
x
Y
o
d
U
D
j
l
M
e
J
n
S
g
R
C
w
Y
R
S
c
1
u
y
g
S
b
n
v
l
i
l
/
1
5
y
C
r
J
M
h
J
B
X
L
U
e
+
W
v
b
l
+
z
L
O
E
K
m
a
T
W
d
g
I
/
x
X
B
C
D
Q
o
m
+
b
T
U
z
S
x
P
K
R
v
R
A
e
8
4
q
q
h
b
E
k
7
m
1
0
7
J
m
V
P
6
J
N
b
G
l
U
I
y
V
3
9
P
T
G
h
i
7
T
i
J
X
G
d
C
c
W
i
X
v
Z
n
4
n
9
f
J
M
L
4
O
J
0
K
l
G
X
L
F
F
o
v
i
T
B
L
U
Z
P
Y
6
6
Q
v
D
G
c
q
x
I
5
Q
Z
4
W
4
l
b
E
g
N
Z
e
g
C
K
r
k
Q
g
u
W
X
V
0
n
z
o
h
r
4
1
e
D
+
s
l
K
7
y
e
M
o
w
g
m
c
w
j
k
E
c
A
U
1
u
I
M
6
N
I
D
B
I
z
z
D
K
7
x
5
2
n
v
x
3
r
2
P
R
W
v
B
y
2
e
O
4
Q
+
8
z
x
+
2
K
4
8
0
<
/
l
a
t
e
x
i
t
>
features and word features from the input image and ques-
tion by utilizing the Faster RCNN and GRU models, re-
spectively. Faster R-CNN model weights are ﬁxed during
training our proposed framework, while GRU weights are
updated with our framework from scratch.

After visual region features and word features being
transformed into vectors of the same dimension by fully
connected layers. The InterMAF module passes informa-
tion ﬂows between each pair of visual region and question
word and aggregates updated features to each region and
each word. Such aggregated features integrate information
from the other modality to update the visual and word fea-
tures according to the cross-modal relations.

Given the InterMAF outputs, the DyIntraMAF module
is utilized for dynamically passing information ﬂows within
each modality. The visual region and word features would
be updated again with information within the same modality
via residual connections.

We use one InterMAF module followed by one DyIntra-
MAF module to form a basic block in our proposed DFAF
framework. Multiple blocks could be stacked thanks to the
feature concatenation and residual connection in the feature
updating procedures. Very deep intra- and inter-modality
information ﬂows can be effectively trained with stochastic
gradient descent. In addition, we utilize multi-head atten-
tion in practice. The original features are split along channel
dimensions into groups and different groups would gener-
ate parallel attentions to update visual and word features in
different groups independently.

3.6. Answer Prediction Layer and Loss Function

After several blocks of feature updating by InterMAF
and DyIntraMAF modules, we obtain the ﬁnal visual re-
gion and word features encoding inter-modality and intra-
modality relations for VQA. By average pooling over re-
gion features and over word features, we could obtain dis-
criminative representations for image and question, respec-
tively. Such features could then be fused via either feature
concatenation, or feature element-wise product, or feature
addition to obtain fused features. We experiment with the
three fusion approaches in which the element-wise product
between visual and language representations achieves the
best performance with a trivial margin.

Similar to state-of-the-art VQA approaches, we treat
VQA as a classiﬁcation problem. The fused multi-modal
features are transformed into a probability vector by a 2-
layer multi-layer perceptron with ReLU non-linearity func-
tion between the layers and a ﬁnal softmax function. The
ground-truth answers are extracted from annotated answers
that appear for more than 5 times. Cross-entropy loss func-
tion is adopted as the objective function.

4. Experiments

4.1. Datasets

We used VQA version 2.0 [11] for our experiment. VQA
dataset contains human annotated question-answer pairs for
images from Microsoft COCO dataset
[25]. VQA 2.0 is
an updated of previous VQA 1.0 with much more anno-
tations and less dataset bias. VQA 2.0 is split into train,
validation and test-standard sets. Among test-standard
test, 25% are served as test-dev set. All questions types
are divided into Yes/No, Number and other categories.
Train, validation and test-standard contains 82,783, 40,504
and 81,434 images, with 443,757, 214,354, 447,793 ques-
tions,respectively. Each question contains 10 answers from
different annotators. Answers with the highest frequency
are treated as the ground-truth. Following previous ap-
proaches, we perform ablation studies over the validation
set and utilize the train and validation splits for test.

4.2. Experimental Setup

Visual features of dimension 2048 are extracted from
Faster R-CNN [33] while word features are encoded into
features of dimension 1280 by GRU[5]. The visual features
and word features are then embedded into 512 dimensions
by a fully-connected layer, respectively. Inside InterMAF,
features are transformed into 8 multi-head attention with
64 dimensions for each head. For DyIntraMAF, the av-
erage pooled features from both modality are transformed
into 512 dimensions by MLP followed by element-wise sig-
moid activation to obtain the conditioning gating vectors.
They are then multiplied with 512 dimension visual key and
query features at every position of visual and word features
for dynamic attention ﬂows. Previous approaches achieve
signiﬁcantly better results with sentinel and relative posi-
tion information. However, sentinel and relative position do
not affect the performance of our method.

All fully connected layers have the same dropout rate
0.1. All gradients are clipped to 0.25. Batch size is set as
512. Adamax optimizer [22], a variant of Adam, is used.
The learning rate is set as 10−3 for the ﬁrst 2 epoch, 2 ×
10−3 for the next 8 epochs and decayed by 1/4 for the rest
epochs. Our method is implemented with Pytorch [31]. All
initilizations are Pytorch default initilization.

All ablation studies are conducted on the validation
dataset, while train, validation datasets and extra visual
genome dataset are combined for testing on test-dev.

4.3. Ablation study of DFAF

We perform extensive ablation studies on the VQA 2.0
validation dataset [11]. The results are shown in Table 1.
Our default setting only has 1 block of DFAF module. Re-
gion features with 2,048 dimensions are extracted from the
input image by Faster RCNN [33], word features with 1,024

43266644

Component

Bottom-up [1]

Bilinear
Attention [19]

Default

# of stacked blocks

Attention type

Attention Direction
inside InterMAF

Embedding
dimension

Cross-model
feature fusion

Visual Sentinel

Bounding Box
Embedding

Parallel Heads

Setting

Accuracy

Bottom-up

BAN-1
BAN-4
BAN-12
DFAF-1
DFAF-2
DFAF-5
DFAF-8

InterMAF only
IntraMAF only

DyIntraMAF only

InterMAF +
DyIntraMAF

Parallel

R → E, E → R
E → R, R → E

512
1024

Multiplication

Addition

Concatenation

None

1
3

None

Absolute Position
Relative Position
1 head each 512
4 heads each 128
8 heads each 64

63.37
65.36
65.81
66.04
66.21
66.43
66.58
66.66
64.37
62.34
65.51
66.21

65.99
66.21
66.19
66.21
65.89
66.21
66.11
66.14
66.21
66.01
66.02
66.21
65.88
65.23
65.84
66.17
66.21

Table 1: Ablation studies of our proposed DFAF on VQA
2.0 validation dataset. R stands for region features while E
stands for word embedding features

dimensions are extracted by GRU [5]. By default, all mod-
ules inside DFAF has 512 dimensions. In the ﬁnal fusion
layer, feature multiplication is employed, which shows a
trivial improvement. Visual sentinel [27] and bounding box
position embedding are also tested which give a slight drop
in the ﬁnal performance. 8 parallel attention heads with di-
mensions 64 for each head is utilized in the default setting.

We ﬁrst

investigate the inﬂuence of the number of
stacked DFAF blocks. The default setting has one stack.
As one can see from Table 1, more stacks can improve the
performance thanks to the residual connection [13]. Differ-
ent from ResNet, we do not employ any normalization [17]
technique during residual connection. The performance of
single layer DFAF is comparable with BAN-12 [19].

bution between each word and region pairs. By adding
the InterMAF, performance can improve by 1% because
of the modelling the inter-modality relations between im-
age regions and question words.
Integrating only the In-
traMAF module would harm the performance because too
many unrelated information ﬂows hinder the learning pro-
cess. By adding dynamically conditioned information ﬂow
DyIntra MAF module, we achieve a 2.15% performance im-
provement. By combining Intra- and Inter-modality atten-
tion ﬂows, we signiﬁcantly outperform the baseline [1] by
2.83% and previous state-of-the-art BAN-1 [19] by 0.85%.
There are several orders for passing information within
the InterMAF module, namely, parallel and sequential [39,
28]. For parallel InterMAF, both region and word features
are updated at the same time. For the sequential informa-
tion ﬂow, we experiment with passing attention ﬂow from
regions to words ﬁrst, which updates word features, and
then passing message from words to regions, which then
update region features, and vice versa. We denote the ﬁrst
sequential order as R → E, E → R, and the second one as
E → R, R → E. Sequential update outperforms parallel
update way, while the speciﬁc order does not matter.

Next, we perform ablation study on the inﬂuence of em-
bedding dimension and cross-model feature fusion. 512
dimensions result in better performance than 1024 dimen-
sions. For the fusion method, multiplication shows a slight
better performance than feature addition and concatenation.
Visual sentinel [27, 39] were utilized in many previous
VQA methods, which was shown to increase the VQA accu-
racy. We treat sentinel as a general 512 dimension features
and concatenate sentinel with all region and word features.
Previous µ region features and 14 word features become
into µ + 1 and 15 respectively. In our experiments, adding
visual sentinel do not show improvement.

The positions of bounding boxes were widely utilized
as a part of image region features in previous methods.
Absolute position embedding has been employed in Trans-
former [37], BERT [6] and Gated CNN [10] in NLP. Rela-
tive position was adopted in relation network [14] for object
detection. In our experiment, adding absolute or relative po-
sitions would drop the performance.

At last, we experiment on the inﬂuence of multi-head
attention [37]. We keep the overall dimensions to be 512.
1, 4 and 8 attention heads are experimented. As can be seen
in Table 1, 8 attention can achieve better performance at the
same number of parameters.

4.4. Visualisation of the proposed Attention Flow

Weights

Then, we investigate the inﬂuence of attention type. The
attention mechanism in Bottom up [1] utilizes simple at-
tention methods. Bilinear attention network [19] proposed
a bilinear attention which learns the joint attention distri-

In Figure 3, we visualise the intra-modality attention
ﬂow weights to analyse VQA model. The attention weights
modulate information ﬂow from contextual regions(orange,
blue and green) to center region(red). The left column

43276645

IntraMAF weights

DyIntraMAF weights

DyIntraMAF weights

DyIntraMAF weights

Q: Is the skier wearing goggles? A:No
Q: What is the person standing? A:Skis

Q: Is the skier wearing goggles?
A: Yes

Q: Is this skier using poles?
A: Yes

Q: What is the person standing on?
A: Skis

Q: Is this skier using poles? A:No

Q: How many vehicles are in the street? A:1 Q: How many vehicles are in street? Q: Are there any people on street? Q: Where will the bus go?
Q: Are there any people on the street? A:No A: 2

A: School

A: No

Q: Where will the bus go? A:School

Figure 3: Visualisation of IntraMAF and DyIntraMAF attention weights between central region(red) and other related re-
gions. (Left) The IntraMAF module treats different questions equally and generate uninformative weight for different ques-
tions. (Right) The proposed DyIntraMAF module dynamically changes attention weights according to input questions.

stands for the attention ﬂow weights in the IntraMAF mod-
ule. While the rest columns represent dynamic attention
ﬂow weights in the DyIntraMAF module. In the DyIntra-
MAF module, unrelated information ﬂow are ﬁltered out by
question features and thus generate the correct answer.

4.5. Comparison with State of the arts methods

Table 2 shows the performance of our proposed algo-
rithm trained with extra visual genome dataset and state-of-
the-art methods on VQA. Bottom up in Table 2 is the win-
ner of VQA challenge 2017. This approach proposed to use
features based on Faster RCNN [33] instead of ResNet [13].
Multi-modal Factorized High-order Pooling (MFH) [43]
is a state-of-the-art bilinear pooling methods. Dense Co-
Attention Network (DCN) [29] utilized dense stack of mul-
tiple layers of Co-attention mechanism which signiﬁcantly
outperform previous methods with ResNet features. Count-
ing methods [44] are good at counting questions by utilizing
the information of bounding boxes. Bilinear Attention Net-
work (BAN) [19] is a state-of-the-art approach on VQA 2.0
which has 12 stacked blocks of BAN modules.

5. Conclusions

In this paper, we proposed a novel framework Dy-
namic Fusion with Intra- and Inter-modality Attention Flow
(DFAF) for visual question answering. The DFAF frame-
work alternatively passes information within and across
different modalities based on an inter-modality and intra-

Model

Bottom-up [1]
MFH [11]
DCN [29]
Counter [44]
MFH+Bottom-Up [11]
BAN+Glove [19]
DFAF(ours)

test-dev

test-std

Y/N
81.82

n/a

83.51
83.14
84.27
85.46
86.09

No.
44.21

n/a

46.61
51.62
49.56
50.66
53.32

Other
56.05

n/a

57.26
58.97
59.89
60.50
60.49

All

65.32
66.12
66.87
68.09
68.76
69.66
70.22

All

65.67

n/a

66.97
68.41

n/a
n/a

70.34

Table 2: Comparison with previous state-of-the-art methods
on VQA 2.0 test dataset.

modality attention mechanisms. The information ﬂow in-
side visual features are dynamically conditioned on the
question features. Stacking multiple blocks of DFAF are
shown to improve the performance of VQA.

6. Acknowledgment

This work is supported in part by SenseTime Group
Limited, in part by the General Research Fund through
the Research Grants Council of Hong Kong under Grants
CUHK14202217, CUHK14203118, CUHK14205615,
CUHK14207814, CUHK14213616, CUHK14208417,
CUHK14239816, and in part by CUHK Direct Grant.

References

[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and

43286646

visual question answering. In IEEE Conference on Computer
Vision and Pattern Recognition, 2018.

[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
Vqa: Visual question answering. In IEEE International Con-
ference on Computer Vision, pages 2425–2433, 2015.

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate.
In International Conference on Learning Repre-
sentations, 2015.

[4] Hedi Ben-younes, Remi Cadene, Matthieu Cord, and Nico-
las Thome. MUTAN: Multimodal Tucker Fusion for Visual
Question Answering. In IEEE International Conference on
Computer Vision, 2017.

[5] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and
Yoshua Bengio. Empirical evaluation of gated recurrent
neural networks on sequence modeling.
arXiv preprint
arXiv:1412.3555, 2014.

[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Pre-training of deep bidirectional
arXiv preprint

Toutanova.
transformers for language understanding.
arXiv:1810.04805, 2018.

Bert:

[7] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,
Trevor Darrell, and Marcus Rohrbach. Multimodal com-
pact bilinear pooling for visual question answering and vi-
sual grounding. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing, pages
457–468, 2016.

[8] Peng Gao, Hongsheng Li, Shuang Li, Pan Lu, Yikang Li,
Steven CH Hoi, and Xiaogang Wang. Question-guided hy-
brid convolution for visual question answering.
In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 469–485, 2018.

[9] Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell.
Compact bilinear pooling. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 317–326, 2016.

[10] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,
and Yann N Dauphin. Convolutional sequence to sequence
learning. In International Conference on Machine Learning,
pages 1243–1252, 2017.

[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the V in VQA matter: Ele-
vating the role of image understanding in Visual Question
Answering.
In IEEE Conference on Computer Vision and
Pattern Recognition, 2017.

[12] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi
Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
Vizwiz grand challenge: Answering visual questions from
blind people. In IEEE Conference on Computer Vision and
Pattern Recognition, 2018.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
770–778, 2016.

[14] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen
Wei. Relation networks for object detection. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, volume 2,
2018.

[15] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In IEEE Conference on Computer Vision and Pattern
Recognition, 2017.

[16] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In IEEE Conference on Computer Vision and Pattern
Recognition, volume 1, page 3, 2017.

[17] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. arXiv preprint arXiv:1502.03167, 2015.

[18] Zhengkai Jiang, Peng Gao, Chaoxu Guo, Qian Zhang, Shim-
ing Xiang, and Chunhong Pan. Video object detection with
locally-weighted deformable neighbors. 2019.

[19] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bi-
linear attention networks. arXiv preprint arXiv:1805.07932,
2018.

[20] Jin-Hwa Kim, Kyoung Woon On, Woosang Lim, Jeonghee
Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Hadamard
product for low-rank bilinear pooling. In International Con-
ference on Learning Representations, 2017.

[21] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M
Rush. Structured attention networks. In International Con-
ference on Learning Representations, 2017.

[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations.
International Journal of Computer Vi-
sion, 123(1):32–73, 2017.

[24] Shuang Li, Tong Xiao, Hongsheng Li, Wei Yang, and Xi-
aogang Wang. Identity-aware textual-visual matching with
latent co-attention. In Computer Vision (ICCV), 2017 IEEE
International Conference on, pages 1908–1917. IEEE, 2017.
[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 740–755. Springer, 2014.

[26] Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, and Xi-
aogang Wang. Show, tell and discriminate: Image caption-
ing by self-retrieval with partially labeled data. In European
Conference on Computer Vision, pages 353–369. Springer,
2018.

[27] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher.
Knowing when to look: Adaptive attention via a visual sen-
tinel for image captioning. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 3242–3250. IEEE,
2017.

[28] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
Hierarchical question-image co-attention for visual question
answering. In Advances In Neural Information Processing
Systems, pages 289–297, 2016.

[29] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion
of visual and language representations by dense symmetric

43296647

[44] Yan Zhang, Jonathon Hare, and Adam Pr¨ugel-Bennett.
Learning to count objects in natural images for visual ques-
tion answering.
In International Conference on Learning
Representations, 2018.

[45] Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur
Szlam, and Rob Fergus. Simple baseline for visual question
answering. arXiv preprint arXiv:1512.02167, 2015.

[46] Chen Zhu, Yanpeng Zhao, Shuaiyi Huang, Kewei Tu, and Yi
Ma. Structured attentions for visual question answering. In
IEEE International Conference on Computer Vision (ICCV),
pages 1300–1309. IEEE, 2017.

co-attention for visual question answering. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2018.

[30] Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. Im-
age question answering using convolutional neural network
with dynamic parameter prediction.
In IEEE Conference
on Computer Vision and Pattern Recognition, pages 30–38,
2016.

[31] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.

[32] Jeffrey Pennington, Richard Socher, and Christopher Man-
ning. Glove: Global vectors for word representation. In The
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 1532–1543, 2014.

[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, pages 91–99, 2015.

[34] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-
attention with relative position representations. In The Con-
ference of the North American Chapter of the Association for
Computational Linguistics, volume 2, pages 464–468, 2018.
[35] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[36] Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, and
Wanli Ouyang. Fishnet: A versatile backbone for image,
region, and pixel level prediction.
In Advances in Neural
Information Processing Systems, pages 760–770, 2018.

[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems, pages 5998–6008, 2017.

[38] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In IEEE Conference on
Computer Vision and Pattern Recognition, 2018.

[39] Caiming Xiong, Victor Zhong, and Richard Socher. Dy-
namic coattention networks for question answering. In In-
ternational Conference on Learning Representations, 2017.
[40] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In International Conference on
Machine Learning, pages 2048–2057, 2015.

[41] Kui Xu, Zhe Wang, Jiangping Shi, Hongsheng Li, and
Qiangfeng Cliff Zhang. A2-net: Molecular structure es-
timation from cryo-em density volumes.
arXiv preprint
arXiv:1901.00785, 2019.

[42] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Explor-
ing visual relationship for image captioning. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 684–699, 2018.

[43] Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and
Dacheng Tao. Beyond bilinear: Generalized multimodal
factorized high-order pooling for visual question answering.
IEEE Transactions on Neural Networks and Learning Sys-
tems, (99):1–13, 2018.

43306648

