Single Image Reﬂection Removal Exploiting Misaligned Training Data and

Network Enhancements

Kaixuan Wei1

Jiaolong Yang2 Ying Fu1 ∗ David Wipf2 Hua Huang1

1Beijing Institute of Technology

2Microsoft Research

Abstract

Removing undesirable reﬂections from a single image
captured through a glass window is of practical impor-
tance to visual computing systems. Although state-of-the-
art methods can obtain decent results in certain situations,
performance declines signiﬁcantly when tackling more gen-
eral real-world cases. These failures stem from the intrin-
sic difﬁculty of single image reﬂection removal – the funda-
mental ill-posedness of the problem, and the insufﬁciency of
densely-labeled training data needed for resolving this am-
biguity within learning-based neural network pipelines. In
this paper, we address these issues by exploiting targeted
network enhancements and the novel use of misaligned
data. For the former, we augment a baseline network archi-
tecture by embedding context encoding modules that are ca-
pable of leveraging high-level contextual clues to reduce in-
determinacy within areas containing strong reﬂections. For
the latter, we introduce an alignment-invariant loss func-
tion that facilitates exploiting misaligned real-world train-
ing data that is much easier to collect. Experimental results
collectively show that our method outperforms the state-of-
the-art with aligned data, and that signiﬁcant improvements
are possible when using additional misaligned data.

1. Introduction

Reﬂection is a frequently-encountered source of image
corruption that can arise when shooting through a glass sur-
face. Such corruptions can be addressed via the process of
single image reﬂection removal (SIRR), a challenging prob-
lem that has attracted considerable attention from the com-
puter vision community [22, 25, 39, 2, 5, 47, 44, 38]. Tra-
ditional optimization-based methods often leverage manual
intervention or strong prior assumptions to render the prob-
lem more tractable [22, 25]. Recently, alternative learning-
based approaches rely on deep Convectional Neural Net-
works (CNNs) in lieu of the costly optimization and hand-
crafted priors [5, 47, 44, 38]. But promising results notwith-

∗Corresponding author: fuying@bit.edu.cn

standing, SIRR remains a largely unsolved problem across
disparate imaging conditions and varying scene content.

For CNN-based reﬂection removal, our focus herein, the
challenge originates from at least two sources: (i) The ex-
traction of a background image layer devoid of reﬂection
artifacts is fundamentally ill-posed, and (ii) Training data
from real-world scenes, are exceedingly scarce because of
the difﬁculty in obtaining ground-truth labels.

Mathematically speaking, it is typically assumed that a
captured image I is formed as a linear combination of a
background or transmitted layer T and a reﬂection layer R,
i.e., I = T + R. Obviously, when given access only to I,
there exists an inﬁnite number of feasible decompositions.
Further compounding the problem is the fact that both T
and R involve content from real scenes that may have over-
lapping appearance distributions. This can make them difﬁ-
cult to distinguish even for human observers in some cases,
and simple priors that might mitigate this ambiguity are not
available except under specialized conditions.

On the other hand, although CNNs can perform a wide
variety visual tasks, at times exceeding human capabilities,
they generally require a large volume of labeled training
data. Unfortunately, real reﬂection images accompanied
with densely-labeled, ground-truth transmitted layer inten-
sities are scarce. Consequently, previous learning-based ap-
proaches have resorted to training with synthesized images
[5, 38, 47] and/or small real-world data captured from spe-
cialized devices [47]. However, existing image synthesis
procedures are heuristic and the domain gap may jeopardize
accuracy on real images. On the other hand, collecting suf-
ﬁcient additional real data with precise ground-truth labels
is tremendously labor-intensive.

This paper is devoted to addressing both of the afore-
mentioned challenges. First, to better tackle the intrinsic
ill-posedness and diminish ambiguity, we propose to lever-
age a network architecture that is sensitive to contextual in-
formation, which has proven useful for other vision tasks
such as semantic segmentation [11, 48, 46, 13]. Note that
at a high level, our objective is to efﬁciently convert prior
information mined from labeled training data into network
structures capable of resolving this ambiguity. Within a tra-

8178

ditional CNN model, especially in the early layers where
the effective receptive ﬁeld is small, the extracted features
across all channels are inherently local. However, broader
non-local context is necessary to differentiate those features
that are descriptive of the desired transmitted image, and
those that can be discarded as reﬂection-based. For ex-
ample, in image neighborhoods containing a particularly
strong reﬂection component, accurate separation by any
possible method (even one trained with arbitrarily rich train-
ing data) will likely require contextual information from re-
gions without reﬂection. To address this issue, we utilize
two complementary forms of context, namely, channel-wise
context and multi-scale spatial context. Regarding the for-
mer, we apply a channel attention mechanism to the fea-
ture maps from convolutional layers such that different fea-
tures are weighed differently according to global statistics
of the activations. For the latter, we aggregate information
across a pyramid of feature map scales within each chan-
nel to reach a global contextual consistency in the spatial
domain. Our experiments demonstrate that signiﬁcant im-
provement can be obtained by these enhancements, leading
to state-of-the-art performance on two real-image datasets.

Secondly, orthogonal to architectural considerations, we
seek to expand the sources of viable training data by facil-
itating the use of misaligned training pairs, which are con-
siderably easier to collect. Misalignment between an input
image I and a ground-truth reﬂection-free version T can be
caused by camera and/or object movements during the ac-
quisition process. In the previous works [37, 46], data pairs
(I, T ) were obtained by taking an initial photo through a
glass plane, followed by capturing a second one after the
glass has been removed. This process requires that the
camera, scene, and even lighting conditions remain static.
Adhering to these requirements across a broad acquisition
campaign can signiﬁcantly reduce both the quantity and di-
versity of the collected data. Additionally, post-processing
may also be necessary to accurately align I and T to com-
pensate for spatial shifts caused by the refractive effect [37].
In contrast, capturing unaligned data is considerably less
burdensome, as shown in Fig. 1. For example, there is no
need for a tripod, table, or other special hardware; the cam-
era can be hand-held and the pose can be freely adjusted;
dynamic scenes in the presence of vehicles, humans, etc.
can be incorporated; and ﬁnally no post-processing of any
type is needed.

To handle such misaligned training data, we require a
loss function that is, to the extent possible, invariant to the
alignment, i.e., the measured image content discrepancy be-
tween the network prediction and its unaligned reference
should be similar to what would have been observed if
the reference was actually aligned.
In the context of im-
age style transfer [17] and others, certain perceptual loss
functions have been shown to be relatively invariant to var-

[46]

Ours

Figure 1: Comparison of the reﬂection image data collection meth-
ods in [46] and this paper.

ious transformations. Our study shows that the using only
the highest-level feature from a deep network (VGG-19 in
our case) leads to satisfactory results for our reﬂection re-
moval task. In both simulation tests and experiments us-
ing a newly collected dataset, we demonstrate for the ﬁrst
time that training/ﬁne-tuning a CNN with unaligned data
improves the reﬂection removal results by a large margin.

2. Related Work

This paper is concerned with reﬂection removal from
a single image. Previous methods utilizing multiple input
images of, e.g., ﬂash/non-ﬂash pairs [1], different polariza-
tion [20], multi-view or video sequences [6, 35, 30, 7, 24,
34, 9, 43, 45] will not be considered here.

Traditional methods. Reﬂection removal from a single im-
age is a massively ill-posed problem. Additional priors are
needed to solve the otherwise prohibitively-difﬁcult prob-
lem in traditional optimization-based method [22, 25, 39,
2, 36].
In [22], user annotations are used to guide layer
separation jointly with a gradient sparsity prior [23]. [25]
introduces a relative smoothness prior where the reﬂections
are assumed to be blurry thus their large gradients are penal-
ized. [39] explores a variant of the smoothness prior where
a multi-scale Depth-of-Field (DoF) conﬁdence map is uti-
lized to perform edge classiﬁcation. [31] exploits the ghost
cues for layer separation. [2] proposes a simple optimiza-
tion formulation with an l0 gradient penalty on the transmit-
ted layer inspired by image smoothing algorithms [42]. De-
spite decent results can be obtained by these methods where
their assumptions hold, the vastly-different imaging condi-
tions and complex scene content in the real world render
their generalization problematic.

Deep learning based methods.
Recently, there is an
emerging interest in applying deep convolutional neural net-
works for single image reﬂection removal such that the
handcrafted priors can be replaced by data-driven learn-
ing [5, 38, 47, 44]. The ﬁrst CNN-based method is due
to [5], where a network structure is proposed to ﬁrst pre-
dict the background layer in the edge domain followed by

8179

reconstructing it the color domain. Later, [38] proposes to
predict the edge and image intensity concurrently by two
cooperative sub-networks. The recent work of [44] presents
a cascade network structure which predicts the background
layer and reﬂection layer in an interleaved fashion. The ear-
lier CNN-based methods typical use the raw image intensity
discrepancy such as mean squared error (MSE) to train the
networks. Several recent works [47, 16, 3] adopt the per-
ceptual loss [17] which uses the multi-stage features of a
deep network pre-trained on ImageNet [29]. [47]. Adver-
sarial loss is investigated in [47, 21] to improve the realism
of the predicted background layers.

3. Approach

Given an input image I contaminated with reﬂections,
our goal is to estimate a reﬂection-free trasmitted image ˆT .
To achieve this, we train a feed-forward CNN GθG parame-
terized by θG to minimize a reﬂection removal loss function
l. Given training image pairs {(In, Tn)}, n = 1, · · · , N ,
this involves solving:

ˆθG = arg minθG

1

N PN

n=1 l(GθG (In), Tn).

(1)

We will ﬁrst introduce the details of network architecture
GθG followed by the loss function l applied to both aligned
data (the common case) and newly proposed unaligned data
extensions. The overall system is illustrated in Fig. 2.

3.1. Basic Image Reconstruction Network

Our starting point can be viewed as the basic image re-
construction neural network component from [5] but mod-
iﬁed in three aspects: (1) We simplify the basic residual
block [12] by removing the batch normalization (BN) layer
[14]; (2) we increase the capacity by widening the network
from 64 to 256 feature maps; and (3) for each input image
I, we extract hypercolumn features [10] from a pretrained
VGG-19 network [32], and concatenate these features with
I as an augmented network input. As explained in [47],
such an augmentation strategy can help enable the network
to learn semantic clues from the input image.

Note that removing the BN layer from our network turns
out to be critical for optimizing performance in the present
context. As shown in [41], if batch sizes become too small,
prediction errors can increase precipitously and stability is-
sues can arise. Moreover, for a dense prediction task such as
SIRR, large batch sizes can become prohibitively expensive
in terms of memory requirements. In our case, we found
that within the tenable batch sizes available for reﬂection re-
moval, BN led to considerably worse performance, includ-
ing color attenuation/shifting issues as sometimes observed
in image-to-image translation tasks [5, 15, 49]. BN layers
have similarly been removed from other dense prediction
tasks such as image super-resolution [26] or deblurring [28].

At this point, we have constructed a useful base archi-
tecture upon which other more targeted alterations will be
applied shortly. This baseline, which we will henceforth
refer to as BaseNet, performs quite well when trained and
tested on synthetic data. However, when deployed on real-
world reﬂection images we found that its performance de-
graded by an appreciable amount, especially on the 20 real
images from [47]. Therefore, to better mitigate the tran-
sition from the make-believe world of synthetic images to
real-life photographs, we describe two modiﬁcations for in-
troducing broader contextual information into otherwise lo-
cal convolutional ﬁlters.

3.2. Context Encoding Modules

As mentioned previously, we consider both context be-

tween channels and multi-scale context within channels.

Channel-wise context. The underlying design princi-
ple here is to introduce global contextual
information
across channels, and a richer overall structure within resid-
ual blocks, without dramatically increasing the parameter
count. One way to accomplish this is by incorporating a
channel attention module originally developed in [13] to re-
calibrate feature maps using global summary statistics.

Let U = [u1, . . . , uc, . . . , uC] denote original, uncali-
brated activations produced by a network block, with C
feature maps of size of H × W . These activations gener-
ally only reﬂect local information residing within the corre-
sponding receptive ﬁelds of each ﬁlter. We then form scalar,
channel-speciﬁc descriptors zc = fgp(uc) by applying a
global average pooling operator fgp to each feature map
uc ∈ RH×W . The vector z = [z1, . . . , zC] ∈ RC represents
a simple statistical summary of global, per-channel activa-
tions and, when passed through a small network structure,
can be used to adaptively predict the relative importance of
each channel [13].

More speciﬁcally,

the channel attention module ﬁrst
computes s = σ(WU δ(WDz)) where WD is a trainable
weight matrix that downsamples z to dimension R < C,
δ is a ReLU non-linearity, WU represents a trainable up-
sampling weight matrix, and σ is a sigmoidal activation.
Elements of the resulting output vector s ∈ RC serve
as channel-speciﬁc gates for calibrating feature maps via
ˆuc = sc · uc.

Consequently, although each individual convolutional
ﬁlter has a local receptive ﬁeld, the determination of which
channels are actually important in predicting the transmis-
sion layer and suppressing reﬂections is based on the pro-
cessing of a global statistic (meaning the channel descrip-
tors computed as activations pass through the network dur-
ing inference). Additionally, the parameter overhead intro-
duced by this process is exceedingly modest given that WD
and WU are just small additional weight matrices associated
with each block.

8180

𝐺(cid:3087)(cid:3256)

VGG19-features 

Channel Attention

…
…

13 blocks

1/32

1/16

1/8

1/4

𝑃𝑖𝑥𝑒𝑙 𝐿𝑜𝑠𝑠 𝑙(cid:3043)(cid:3036)(cid:3051)(cid:3032)(cid:3039)
F𝑒𝑎𝑡𝑢𝑟𝑒 𝐿𝑜𝑠𝑠 𝑙(cid:3033)(cid:3032)(cid:3028)(cid:3047)
Align.𝐼𝑛𝑣𝑎𝑟𝑖𝑎𝑛𝑡 𝐿𝑜𝑠𝑠 𝑙(cid:3036)(cid:3041)(cid:3049)
𝐷(cid:3087)(cid:3253)
𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑖𝑎𝑙 𝐿𝑜𝑠𝑠 𝑙(cid:3028)(cid:3031)(cid:3049)

Loss for

aligned Data

Loss for

Unaligned Data

𝑇(cid:3552)
𝑇

Upsample

Convolution ReLU

Residual
Block

Sigmoid

Pyramid
Pooling

Global
Pooling

Figure 2: Overview of our approach for single image reﬂection removal.

Multi-scale spatial context. Although we have found that
encoding the contextual information across channels al-
ready leads to signiﬁcant empirical gains on real-world im-
ages, utilizing complementary multi-scale spatial informa-
tion within each channel provides further beneﬁt. To ac-
complish this, we apply a pyramid pooling module [11],
which has proven to be an effective global-scene-level rep-
resentation in semantic segmentation [48]. As shown in
Fig. 2, we construct such a module using pooling opera-
tions at sizes 4, 8, 16, and 32 situated in the tail of our net-
work before the ﬁnal construction of ˆT . Pooling in this way
fuses features under four different pyramid scales. After
harvesting the resulting sub-region representations, we per-
form a non-linear transformation (i.e. a Conv-ReLU pair) to
reduce the channel dimension. The reﬁned features are then
upsampled via bilinear interpolation. Finally, the different
levels of features are concatenated together as a ﬁnal repre-
sentation reﬂecting multi-scale spatial context within each
channel; the increased parameter overhead is negligible.

3.3. Training Loss for Aligned Data

In this section, we present our loss function for aligned
training pairs (I, T ), which consists of three terms similar
to previous methods [47, 44].

Pixel loss. Following [5], we penalize the pixel-wise in-
tensity difference of T and ˆT via lpixel = αk ˆT − T k2
2 +
β(k∇x ˆT − ∇xT k1 + k∇y ˆT − ∇yT k1) where ∇x and ∇y
are the gradient operator along x- and y-direction, respec-
tively. We set α = 0.2 and β = 0.4 in all our experiments.

Feature loss. We deﬁne the feature loss based on the
activations of the 19-layer VGG network [33] pretrained
on ImageNet [29]. Let φl be the feature from the l-th
layer of VGG-19, we deﬁne the feature loss as lf eat =
Pl λlkφl(T ) − φl( ˆT )k1 where {λl} are the balancing
weights. Similar to [47], we use the layers ‘conv2 2’,
‘conv3 2’, ‘conv4 2’, and ‘conv5 2’ of VGG-19 net.

Adversarial loss. We further add an adversarial loss to

improve the realism of the produced background images.
We deﬁne an opponent discriminator network DθD and
minimize the relativistic adversarial loss [18] deﬁned as
adv = − log(DθD (T, ˆT ))−log(1−DθD ( ˆT , T )) for
ladv = lG
adv = − log(1 − DθD (T, ˆT )) − log(DθD ( ˆT , T ))
GθG and lD
for DθD where DθD (T, ˆT ) = σ(C(T ) − C( ˆT )) with σ(·)
being the sigmoid function and C(·) the non-transformed
discriminator function (refer to [18] for details).

To summarize, our loss for aligned data is deﬁned as:

laligned = ω1lpixel + ω2lf eat + ω3ladv

(2)

where we empirically set the weights as ω1 = 1, ω2 = 0.1,
and ω3 = 0.01 respectively throughout our experiments.

3.4. Training Loss for Unaligned Data

To use misaligned data pairs (I, T ) for training, we need
a loss function that is invariant to the alignment, such that
the true similarity between T and the prediction ˆT can be
reasonably measured. In this regard, we note that human
observers can easily assess the similarity of two images
even if they are not aligned. Consequently, designing a
loss measuring image similarity on the perceptual-level may
serve our goal. This motivates us to directly use a deep fea-
ture loss for unaligned data.

Intuitively, the deeper the feature, the more likely it is
to be insensitive to misalignment. To experimentally ver-
ify this and ﬁnd a suitable feature layer for our purposes,
we conducted tests using a pre-trained VGG-19 network as
follows. Given an unaligned image pair (I, T ), we use gra-
dient descent to ﬁnetune the weights of our network GθG
to minimize the feature difference of T and ˆT , with features
extracted at different layers of VGG-19. Figure 3 shows that
using low-level or middle-level features from ‘conv2 2’ to
‘conv4 2’ leads to blurry results (similar to directly using a
pixel-wise loss), although the reﬂection is more thoroughly
removed. In contrast, using the highest-level feature from
‘conv5 2’ gives rise to a striking result: the predicted back-
ground image is sharp and almost reﬂection-free.

8181

(a) Input

(b) Unaligned Ref.

(c) Pretrained

(d) lpixel

(e) conv2 2

(f) conv3 2

(g) conv4 2

(h) conv5 2

(i) Loss of [27]

(a) and (b) are the unaligned image pair (I, T ).

Figure 3: The effect of using different loss to handle misaligned
real data.
(c)
shows the reﬂection removal result of our network trained on syn-
thetic data and a small number of aligned real data (see Section 4
for details). Reﬂection can still be observed in the predicted back-
(d) is the result ﬁnetuned on (I, T ) with pixel-
ground image.
wise intensity loss. (e)-(h) are the results ﬁnetuned with features
at different layers of VGG-19. Only the highest-level feature from
‘conv5 2’ yields satisfactory result. (i) shows the results ﬁnetuned
with the loss of [27]. (Best viewed on screen with zoom)

Recently, [27] introduced a “contextual loss” which is
also designed for training deep networks with unaligned
data for image-to-image translation tasks like image style
transfer. In Fig 3, we also present the ﬁnetuned result us-
ing this loss for our reﬂection removal task. Upon visual
inspection, the results are similar to our highest-level VGG
feature loss (quantitative comparison can be found in the
experiment section). However, our adopted loss (formally
deﬁned below) is much simpler and more computationally
efﬁcient than the loss from [27].

Alignment-invariant loss. Based on the above study, we
now formally deﬁne our invariant loss component designed
for unaligned data as linv = kφh(T ) − φh( ˆT )k1, where
φh denotes the ‘conv5 2’ feature of the pretrained VGG-19
network. For unaligned data, we also apply an adversarial
loss which is not affected by misalignment. Therefore, our
overall loss for unaligned data can be written as

lunaligned = ω4linv + ω5ladv

(3)

where we set the weights as ω4 = 0.1 and ω5 = 0.01.

4. Experiments

4.1. Implementation Details

Training data. We adopt a fusion of synthetic and real data
as our train dataset. The images from [5] are used as sythetic

Table 1: Comparison of different settings. Our full model (i.e.
ERRNet) leads to best performance among all comparisons.

Model

PSNR

SSIM PSNR

SSIM

Synthetic

Real20

CEILNet-F [5]

24.70

0.884

20.32

0.739

BaseNet only
BaseNet + CSC
BaseNet + MSC
ERRNet

25.71
27.64
26.03
27.88

0.926
0.940
0.928
0.941

21.51
22.61
21.75
22.89

0.780
0.796
0.783
0.803

data, i.e. 7,643 cropped images with size 224 × 224 from
PASCAL VOC dataset [4]. 90 real-world training images
from [47] are adopted as real data. For image synthesis,
we use the same data generation model as [5] to create our
synthetic data. In the following, we always use the same
dataset for training, unless speciﬁcally stated.

Training details. Our implementation1 is based on Py-
Torch. We train the model with 60 epoch using the Adam
optimizer [19]. The base learning rate is set to 10−4 and
halved at epoch 30, then reduced to 10−5 at epoch 50. The
weights are initialized as in [26].

4.2. Ablation Study

In this section, we conduct an ablation study for our
method on 100 synthetic testing images from [5] and 20
real testing images from [47] (denoted by ‘Real20’).

Component analysis.
To verify the importance of our
network design, we compare four model architectures as
described in Section 3, including (1) Our basic image re-
construction network BaseNet; (2) BaseNet with channel-
wise context module (BaseNet + CWC); (3) BaseNet with
multi-scale spatial context module (BaseNet + MSC); and
(4) Our enhanced reﬂection removal network, denoted ER-
RNet, i.e., BaseNet + CWC + MSC. The result from the
CEILNet [5] ﬁne-tuned on our training data (denoted by
CEILNet-F) is also provided as an additional reference.

As shown in Table 1, our BaseNet has already achieved
a much better result than CEILNet-F. The performance of
our BaseNet could be obviously boosted by using channel-
wise context and multi-scale spatial context modules, espe-
cially by using them together, i.e. ERRNet. Figure 4 visu-
ally shows the results from BaseNet and our ERRNet. It can
be observed that BaseNet struggles to discriminate the re-
ﬂection region and yields some obvious residuals, while the
ERRNet removes the reﬂection and produces much cleaner
transmitted images. These results suggest the effectiveness
of our network design, especially the components tailored
to encode the contextual clues.

Efﬁcacy of the training loss for unaligned data.

In this

1Code is released at https://github.com/Vandermode/ERRNet

8182

Input

BaseNet

ERRNet

4.3. Method Comparison on Benchmarks

In this section, we compare our ERRNet against state-of-
the-art methods including the optimization-based method of
[25] (LB14) and the learning-based approaches (CEILNet
[5], Zhang et al. [47], and BDN [44]). For fair comparison,
we ﬁnetune these models on our training dataset and report
results of both the original pretrained model and ﬁnetuned
version (denoted with a sufﬁx ’-F’).

The comparison is conducted on four

real-world
datasets, i.e. 20 testing images in [47] and three sub-datasets
from SIR2 [37]. These three sub-datasets are captured under
different conditions: (1) 20 controlled indoor scenes com-
posed by solid objects; (2) 20 different controlled scenes
on postcards; and (3) 55 wild scenes3 with ground truth
provided.
In the following, we denote these datasets by
‘Real20’, ‘Objects’, ‘Postcard’, and ‘Wild’, respectively.

Table 3 summarizes the results of all competing meth-
ods on four real-world datasets. The quality metrics include
PSNR, SSIM [40], NCC [43, 37] and LMSE [8]. Larger
values of PSNR, SSIM, and NCC indicate better perfor-
mance, while a smaller value of LMSE implies a better re-
sult. Our ERRNet achieves the state-of-the-art performance
in ‘Real20’ and ‘Objects’ datasets. Meanwhile, our result
is comparable to the best-performing BDN-F on ‘Postcard’
data. The quantitative results on ‘Wild’ dataset reveal a
frustrating fact, namely, that no method could outperform
the naive baseline ’Input’, suggesting that there is still large
room for improvement.

Figure 5 displays visual results on real-world images. It
can be seen that all compared methods fail to handle some
strong reﬂections, but our network more accurately removes
many undesirable artifacts, e.g. removal of tree branches re-
ﬂected on the building window in the fourth photo of Fig 5.

4.4. Training with Unaligned Data

To test our alignment-invariant loss on real-world un-
aligned data, we ﬁrst collected a dataset of unaligned im-
age pairs with cameras and a portable glass, as shown in
Fig. 1 . Both a DSLR camera and a smart phone are used to
capture the images. We collected 450 image pairs in total,
and some samples are shown in Fig 6. These image pairs
are randomly split into a training set of 400 samples and a
testing set with 50 samples.

We conduct experiments on the BDN-F and ERRNet
models, each of which is ﬁrst trained on aligned dataset
(w/o unaligned) as in Section 4.3, and then ﬁnetuned with
our alignment-invariant loss and unaligned training data.
The resulting pairs before and after ﬁnetuning are assem-
bled for human assessment, as no existing numerical metric
is available for evaluating unaligned data.

We asked 30 human observers to provide a preference

Figure 4: Comparison of the results with (ERRNet) and without
(BaseNet) the context encoding modules.

Table 2:
alignment-invariant loss

Simulation experiment

to verify the efﬁcacy our

Training Scheme

PSNR

SSIM

Synthetic only
+ 50 aligned
+ 90 aligned

19.79
22.00
22.89

0.741
0.785
0.803

+ 50 aligned, + 40 unaligned trained with:

lpixel
linv
lcx
linv + lcx

21.85
22.38
22.47
22.43

0.766
0.797
0.796
0.796

experiment, we ﬁrst train our ERRNet with only ‘synthetic
data’, ‘synthetic + 50 aligned real data’, and ‘synthetic +
90 aligned real data’. The loss function in Eq. (2) is used
for aligned data. We can see that the testing results become
better with the increasing real data in Table 2.

Then, we synthesize misalignment through performing
random translations within [−10, 10] pixels on real data2,
and train ERRNet with ‘synthetic + 50 aligned real data +
40 unaligned data’. Pixel-wise loss lpixel and alignment-
invariant loss linv are used for 40 unaligned images. Ta-
ble 2 shows employing 40 unaligned data with lpixel loss
degrades the performance, even worse than that from 50
aligned images without additional unaligned data.

In addition, we also investigate the contextual loss lcx
of [27]. Results from both contextual loss lcx and our
alignment-invariant loss linv (or combination of them linv +
lcx) surpass analogous results obtained with only aligned
images by appreciable margins, indicating that these losses
provide useful supervision to networks granted unaligned
data. Note although linv and lcx perform equally well, our
linv is much simpler and computationally efﬁcient than lcx,
suggesting linv is lightweight alternative to lcx in terms of
our reﬂection removal task.

2Our alignment-invariant loss linv can handle shifts of up to 20 pixels.

See suppl. material for more details.

3Images indexed by 1, 2, 74 are removed due to misalignment.

8183

Input

LB14 [25]

CEILNet-F [5] Zhang et al. [47]

BDN-F [44]

ERRNet

Reference

Figure 5: Visual comparison on real-world images. The images are obtained from ‘Real20’ (Rows 1-3) and our collected unaligned dataset
(Rows 4- 5). More results can be found in the suppl. material.

Table 3: Quantitative results of different methods on four real-world benchmark datasets. The best results are indicated by red color and
the second best results are denoted by blue color. The results of ‘Average’ are obtained by averaging the metric scores of all images from
these four real-world datasets.

Dataset

Index

Input

Real20

Objects

Postcard

Wild

Average

PSNR
SSIM
NCC
LMSE
PSNR
SSIM
NCC
LMSE
PSNR
SSIM
NCC
LMSE
PSNR
SSIM
NCC
LMSE
PSNR
SSIM
NCC
LMSE

19.05

0.733

0.812

0.027

23.74

0.878

0.981

0.004

21.30

0.878

0.947

0.005

26.24

0.897

0.941

0.005

22.85

0.874

0.955

0.006

LB14
[25]
18.29

0.683

0.789

0.033

19.39

0.786

0.971

0.007

14.88

0.795

0.929

0.008

19.05

0.755

0.894

0.027

17.51

0.781

0.937

0.011

Methods

CEILNet

CEILNet

[5]

18.45

0.690

0.813

0.031

23.62

0.867

0.972

0.005

21.24

0.834

0.945

0.008

22.36

0.821

0.918

0.013

22.30

0.841

0.948

0.009

F

20.32

0.739

0.834

0.028

23.36

0.873

0.974

0.005

19.17

0.793

0.926

0.013

22.05

0.844

0.924

0.009

21.41

0.832

0.943

0.010

Zhang

et al. [47]

21.89

0.787

0.903

0.022

22.72

0.879

0.964

0.005

16.85

0.799

0.886

0.007

21.56

0.836

0.919

0.010

20.22

0.838

0.925

0.007

BDN
[44]
18.41

0.726

0.792

0.032

22.73

0.856

0.978

0.005

20.71

0.859

0.943

0.005

22.36

0.830

0.932

0.009

21.70

0.848

0.951

0.007

BDN

F

20.06

0.738

0.825

0.027

24.00

0.893

0.978

0.004

22.19

0.881

0.941

0.004

22.74

0.872

0.922

0.008

22.96

0.879

0.950

0.006

ERRNet

22.89

0.803

0.877

0.022

24.87

0.896

0.982

0.003

22.04

0.876

0.946

0.004

24.25

0.853

0.917

0.011

23.59

0.879

0.956

0.005

score among {-2,-1,0,1,2} with 2 indicating the ﬁnetuned
result is signiﬁcantly better while -2 the opposite. To avoid
bias, we randomly switch the image positions of each pair.

In total, 3000 human judgments are collected (2 methods,
30 users, 50 images pairs). More details regarding this eval-
uation process can be found in the suppl. material.

8184

Figure 6: Image samples in our unaligned image dataset. Our dataset covers a large variety of indoor and outdoor environments including
dynamic scenes with vehicles, human, etc.

Score Range Ratio BDN-F ERRNet
(0.25, 2]
54%
[−0.25, 0.25]
36%
[−2, −0.25)
10%

78%
18%
4%

2

1

0

Average Score

0.62

0.51

-1

2

1

0

-1

10

20

30

40

50

10

20

30

40

50

Table 4: Human preference scores of self-comparsion experiments. Left: results of BDN-F; Right: results of ERRNet. X axis of each
sub-ﬁgure represents the image # of testing images (50 in total).

BDN-F

ERRNet

input

reference

w/o unaligned

w. unaligned

w/o unaligned

w. unaligned

Figure 7: Results of training with and without unaligned data. See suppl. material for more examples. (Best view on screen with zoom)

Table 4 shows the average of human preference scores
for the resulting pairs of each method. As can be seen, hu-
man observers clearly tend to prefer the results produced
by the ﬁnetuned models over the raw ones, which demon-
strates the beneﬁt of leveraging unaligned data for training
independent of the network architecture. Figure 7 shows
some typical results of the two methods; the results are sig-
niﬁcantly improved by training on unaligned data.

5. Conclusion

We have proposed an enhanced reﬂection removal net-
work together with an alignment-invariant loss function to
help resolve the difﬁculty of single image reﬂection re-
moval. We investigated the possibility to directly utilize
misaligned training data, which can signiﬁcantly alleviate
the burden of capturing real-world training data. To efﬁ-

ciently extract the underlying knowledge from real train-
ing data, we introduce context encoding modules, which
can be seamlessly embedded into our network to help dis-
criminate and suppress the reﬂection component. Extensive
experiments demonstrate our approach set a new state-of-
the-art on real-world benchmarks of single image reﬂection
removal, both quantitatively and visually.

Acknowledgments

We thank Yunhao Zou for great help collecting the re-
ﬂection image dataset. This work was supported by the Na-
tional Natural Science Foundation of China under Grants
No. 61425013 and No. 61672096.

8185

References

[1] A. Agrawal, R. Raskar, S. K. Nayar, and Y. Li. Remov-
ing photography artifacts using gradient projection and ﬂash-
exposure sampling. ACM Transactions on Graphics (TOG),
24(3):828–835, 2005.

[2] N. Arvanitopoulos, R. Achanta, and S. Susstrunk. Single im-
age reﬂection suppression. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), July 2017.

[3] Z. Chi, X. Wu, X. Shu, and J. Gu. Single image reﬂection
removal using deep encoder-decoder network. arXiv preprint
arXiv:1802.00094, 2018.

[4] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The pascal visual object classes (voc)
challenge. International Journal of Computer Vision (IJCV),
88(2):303–338, 2010.

[5] Q. Fan, J. Yang, G. Hua, B. Chen, and D. Wipf. A generic
deep architecture for single image reﬂection removal and im-
age smoothing.
In The IEEE International Conference on
Computer Vision (ICCV), Oct 2017.

[6] H. Farid and E. H. Adelson. Separating reﬂections and light-
ing using independent components analysis. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
July 1999.

[7] K. Gai, Z. Shi, and C. Zhang. Blind separation of superim-
posed moving images using image statistics. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
34(1):19–32, 2012.

[8] R. Grosse, M. K. Johnson, E. H. Adelson, and W. T. Free-
man. Ground truth dataset and baseline evaluations for in-
trinsic image algorithms. In IEEE International Conference
on Computer Vision (ICCV). IEEE, Oct 2009.

[9] X. Guo, X. Cao, and Y. Ma. Robust separation of reﬂection
In IEEE Conference on Computer

from multiple images.
Vision and Pattern Recognition (CVPR), July 2014.

[10] B. Hariharan, P. Arbelaez, R. Girshick, and J. Malik. Hy-
percolumns for object segmentation and ﬁne-grained local-
ization.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2015.

[11] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling
in deep convolutional networks for visual recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 37(9):1904–1916, 2015.

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.

[13] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-
In The IEEE Conference on Computer Vision and

works.
Pattern Recognition (CVPR), June 2018.

[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015.

[15] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks.
In The
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), July 2017.

[16] M. Jin, S. Ssstrunk, and P. Favaro. Learning to see through
reﬂections. In IEEE International Conference on Computa-
tional Photography (ICCP), May 2018.

[17] J. Johnson, A. Alahi, and L. Feifei. Perceptual losses for
real-time style transfer and super-resolution. European Con-
ference on Computer Vision (ECCV), pages 694–711, 2016.
[18] A. Jolicoeur-Martineau. The relativistic discriminator: a key
element missing from standard GAN. In International Con-
ference on Learning Representations (ICLR), 2019.

[19] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[20] N. Kong, Y.-W. Tai, and J. S. Shin. A physically-based
approach to reﬂection separation: from physical modeling
to constrained optimization. IEEE Transactions on Pattern
Analysis and Machine Intelligence (TPAMI), 36(2):209–221,
2014.

[21] D. Lee, M.-H. Yang, and S. Oh. Generative single image re-
ﬂection separation. arXiv preprint arXiv:1801.04102, 2018.
[22] A. Levin and Y. Weiss. User assisted separation of reﬂections
from a single image using a sparsity prior. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
29(9):1647–1654, 2007.

[23] A. Levin, A. Zomet, and Y. Weiss. Learning to perceive
transparency from the statistics of natural scenes.
In Ad-
vances in Neural Information Processing Systems (NIPS).
December 2002.

[24] Y. Li and M. S. Brown. Exploiting reﬂection change for au-
tomatic reﬂection removal. In The IEEE International Con-
ference on Computer Vision (ICCV), December 2013.

[25] Y. Li and M. S. Brown. Single image layer separation us-
ing relative smoothness. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 2752–2759,
2014.

[26] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
deep residual networks for single image super-resolution.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops, July 2017.

[27] R. Mechrez, I. Talmi, and L. Zelnik-Manor. The contextual
loss for image transformation with non-aligned data. In The
European Conference on Computer Vision (ECCV), Septem-
ber 2018.

[28] S. Nah, T. Hyun Kim, and K. Mu Lee. Deep multi-scale
convolutional neural network for dynamic scene deblurring.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.

[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge. In-
ternational Journal of Computer Vision (IJCV), 115(3):211–
252, 2015.

[30] B. Sarel and M. Irani. Separating transparent layers through
In European Conference on

layer information exchange.
Computer Vision (ECCV), September 2004.

[31] Y. Shih, D. Krishnan, F. Durand, and W. T. Freeman. Reﬂec-
tion removal using ghosting cues. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2015.

8186

[32] K. Simonyan and A. Zisserman. Very deep convolutional
International

networks for large-scale image recognition.
Conference on Learning Representations (ICLR), 2015.

[48] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene
parsing network. In The IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), July 2017.

[49] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In The IEEE International Conference on Computer
Vision (ICCV), Oct 2017.

[33] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Machine Learning (ICLR), 2015.

[34] S. N. Sinha, J. Kopf, M. Goesele, D. Scharstein, and
R. Szeliski.
Image-based rendering for scenes with reﬂec-
tions. ACM Transactions on Graphics (TOG), 31(4):100–1,
2012.

[35] R. Szeliski, S. Avidan, and P. Anandan. Layer extrac-
tion from multiple images containing reﬂections and trans-
parency. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), July 2000.

[36] R. Wan, B. Shi, L. Duan, A. Tan, W. Gao, and A. C. Kot.
Region-aware reﬂection removal with uniﬁed content and
gradient priors.
IEEE Transactions on Image Processing,
27(6):2927–2941, 2018.

[37] R. Wan, B. Shi, L.-Y. Duan, A.-H. Tan, and A. C. Kot.
Benchmarking single-image reﬂection removal algorithms.
In The IEEE International Conference on Computer Vision
(ICCV), Oct 2017.

[38] R. Wan, B. Shi, L.-Y. Duan, A.-H. Tan, and A. C. Kot. Crrn:
Multi-scale guided concurrent reﬂection removal network.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.

[39] R. Wan, B. Shi, T. A. Hwee, and A. C. Kot. Depth of ﬁeld
guided reﬂection removal. In IEEE International Conference
on Image Processing, September 2016.

[40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
from error visibility to struc-
IEEE Transactions on Image Processing,

Image quality assessment:
tural similarity.
13(4):600–612, 2004.

[41] Y. Wu and K. He. Group normalization. In European Con-

ference on Computer Vision (ECCV), September 2018.

[42] L. Xu, C. Lu, Y. Xu, and J. Jia.

gradient minimization.
(TOG), volume 30, page 174, 2011.

Image smoothing via L0
In ACM Transactions on Graphics

[43] T. Xue, M. Rubinstein, C. Liu, and W. T. Freeman. A com-
putational approach for obstruction-free photography. ACM
Transactions on Graphics (TOG), 34(4):79, 2015.

[44] J. Yang, D. Gong, L. Liu, and Q. Shi. Seeing deeply and
bidirectionally: A deep learning approach for single image
reﬂection removal.
In The European Conference on Com-
puter Vision (ECCV), September 2018.

[45] J. Yang, H. Li, Y. Dai, and R. T. Tan. Robust optical ﬂow
estimation of double-layer images under transparency or re-
ﬂection. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.

[46] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and
A. Agrawal. Context encoding for semantic segmentation.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.

[47] X. Zhang, R. Ng, and Q. Chen. Single image reﬂection
separation with perceptual losses. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2018.

8187

