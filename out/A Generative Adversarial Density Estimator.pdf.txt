A Generative Adversarial Density Estimator

M. Ehsan Abbasnejad, Javen Shi, Anton van den Hengel, Lingqiao Liu

{ehsan.abbasnejad,javen.shi,anton.vandenhengel,lingqiao.liu}@adelaide.edu.au

Australian Institute of Machine Learning & The University of Adelaide, Australia

Abstract

Density estimation is a challenging unsupervised learn-
ing problem. Current maximum likelihood approaches for
density estimation are either restrictive or incapable of pro-
ducing high-quality samples. On the other hand, likelihood-
free models such as generative adversarial networks, pro-
duce sharp samples without a density model. The lack of
a density estimate limits the applications to which the sam-
pled data can be put, however. We propose a Generative Ad-
versarial Density Estimator, a density estimation approach
that bridges the gap between the two. Allowing for a prior
on the parameters of the model, we extend our density es-
timator to a Bayesian model where we can leverage the
predictive variance to measure our conﬁdence in the like-
lihood. Our experiments on challenging applications such
as visual dialog where the density and the conﬁdence in pre-
dictions are crucial shows the effectiveness of our approach.

1. Introduction

Generative modelling is amongst the longest-standing
problems in machine learning, and one that has been draw-
ing increasing attention. This is at least partly due to
the shortcomings of the predominant discriminative deep
learning-based models. These shortcomings include the
failure to generalise, a lack of robustness to data distribution
changes, and the need for large volumes of training data.

Deep generative models have been successful in address-
ing some of these shortcomings. In particular, deep maxi-
mum likelihood models such as deep Boltzmann machines
[40], variational autoencoders (VAEs) [23], autoregressive
models [17, 36], real non-volume preserving transforma-
tions [13] etc. have demonstrated an impressive ability to
model complex densities.

Likelihood-free approaches [20, 16] such as generative
adversarial networks (GANs) [16] have outperformed pre-
vious deep generative models in their ability to model com-
plex distributions.
In image-based problems they have
shown a particular ability to consistently generate sharp and

realistic looking samples [22, 48, 34]. GANs are one of the
implicit generative models wherein density is not explicitly
deﬁned [11]. Unfortunately, GANs are incapable of evalu-
ating densities by-design.

Maximum likelihood models have the advantage that
they are able to represent probability density explicitly, but
either perform poorly in terms of the quality of samples they
generate or are relatively inefﬁcient to train.

As modern datasets are typically large, high-dimensional
and highly structured, the challenge is building models that
are powerful enough to capture the underlying complexity
yet still efﬁcient to train in estimating the density of in-
stances.

Density estimates are essential in a wide range of practi-
cal problems in Computer Vision, particularly when likeli-
hoods over the generated samples are critical. This occurs,
for example, when it is important to both explore and opti-
mise over a search space, when conﬁdence estimates about
a hypothesis are required, or when control over the level
of generalisation is important. The typical sample qual-
ity metrics are inadequate since the generative model may
simply memoraize the data or miss important modes [45].
Moreover, in an application such as one where the virtual
agent is engaged in a dialog with the user about an im-
age, the estimate of how likely an answer is in addition to
the agent’s conﬁdence improves both dialogue ﬂuidity and
method performance. Alternatively, density estimates such
as autoregressive models [17], ﬂow-based methods [12, 13]
or non-parameteric methods such as kernel density estima-
tion (KDE) [16] are either overly computationally demand-
ing or rely on heavy engineering of the neural networks in-
volved.

In this paper, we introduce a Generative Adversarial
Density Estimator that is both easy to train and expres-
sive enough to model high-dimensional data.
In particu-
lar, we bridge the gap between the maximum likelihood
and likelihood-free models by explicitly modeling the like-
lihood while using adversarial samples to compute the nor-
malizer. As a by-product of this model, we show the local
properties of the density function are captured and allow
for diverse samples to be generated. Further, we show our

110782

approach is capable of modeling the likelihood of complex
data, such as images, from which realistic looking samples
can be taken.

In addition, our approach easily extends to Bayesian esti-
mation where the distribution of the parameters are incorpo-
rated in the generative model. This allows us to compute the
predictive variance which uncovers the “uncertainty” in the
prediction. This uncertainty can be used in an applications
such as Visual Dialog [10] where an agent is trained to re-
spond to questions when conﬁdent or reply “I don’t know”
for uncertain answers. This is essential if such agents are to
be of practical utility.

This paper makes the following contributions:

1. We propose a generative adversarial density estimator
network able to perform efﬁcient sampling as well as
likelihood evaluation without the need for an explicit in-
vertible generator.

2. We propose a learning objective, based on sound theo-
retical grounds, for our adversarial density estimator that
attains good log-likelihoods and generates high-quality
samples on multiple datasets as well as estimating uncer-
tainty in a vision-and-language problem.

3. Our approach is naturally extended to a Bayesian setting.
In particular we investigate a conjugate prior and an efﬁ-
cient gradient based Monte Carlo method to estimate the
posterior.

4. We propose a method to efﬁciently regularize the Jaco-
bian of the generator function to evaluate the likelihoods
of the model.

5. We provide a theoretical apparatus for future research
in density estimation, in particular, utilizing adversarial
training.

2. Related Work

Deep Boltzmann machines [40, 3] represent one of the
earlier approaches to maximum likelihood modelling, but
due to their intractable nature, they are hard to train. Re-
cently, autoregressive approaches [36, 41] have been used
to model the distribution of each image pixel value directly.
As such, the ordering of the dimensions can be critical to the
training of the model. The sequential nature of these mod-
els signiﬁcantly limits their computational efﬁciency. VAEs
[23, 15, 1] have shown to be successful in modeling a la-
tent variable from which instances from the distribution can
be reconstructed. While these approaches have been very
successful, image samples generated by VAEs are generally
blurry and exhibit unrealistic artefacts.

Generative Adversarial Networks (GANs) [16] avoid
the maximum likelihood principle altogether. Instead, the
generative network is associated with a discriminator net-
work whose task is to distinguish between samples and real
data.Successfully trained GAN models consistently outper-
form their counterparts in producing high-quality, sharp and

realistic image samples [46, 22]. However, GANs can-
not estimate the density function over the sampled data in-
stances, even though approaches such as Wasserstein dis-
tance (WGAN) [6, 19, 2] and f-GAN [35] are similar to
ours. Recently, [38, 18] considered likelihood maximisa-
tion with a change of variable. However, these approaches
require designing a function that is both invertible and that
has an easily computable Jacobian. These designs typically
produce low quality samples, or are inefﬁcient.

3. Generative Adversarial Density Estimator

3.1. Adversarial Formulation

We consider the problem of estimating the density for an

observation sample x (e.g. an image) deﬁned by

p(x|w) = exp(φ(w, x) − A(w)),

A(w) = log(cid:18)Z exp(φ(w, x))dx(cid:19) .

(1)

Here, φ(w, x) is the energy function for the Boltzmann dis-
tribution and the parameter w fully speciﬁes the density
function. We use a deep neural network with linear out-
put to model this function, i.e. φ(w, x) = w⊤
1 φ′(w2, x),
where w = {w1, w2} and feature vector φ′(w2, x) is the
sufﬁcient statistics that we learn. Alternatively φ can be
seen as a function that maps the input x to a feature space
where the density is easy to formulate. In addition, A(w) is

the normalizer that ensuresR p(x|w)dx = 1.
Given a dataset X = {x1 . . . , xn} where samples are
drawn i.i.d from the underlying distribution p, we are inter-
ested in ﬁnding parameter w in Equation 1. To that end, we
maximize the expected log-likelihood of these observations
in the dataset with respect to this density model to obtain
the parameter w, that is, maxw Ep [log p(x|w)], where

Ep [log p(x|w)] = Z log(p(x|w))p(x|w)dx

= Ep [φ(w, x)] − A(w)

(2)

Computing the normalizer A(w) is intractable, and thus we
resort to approximations. In particular, we utilize the varia-
tional principle to obtain an upper bound on A(w) using an
alternative distribution q parameterized by θ as

E

q [φ(w,x)]

Aq(w) = sup

q (cid:26)

{
z
Z φ(w, x)q(x|θ)dx +

}|
Z −q(x|θ) log (q(x|θ)) dx
|
}

{z

Hx(q)

(cid:27) (3)

Note that direct maximization of Aq(w) yields the opti-
mal value when p = q, i.e. when q takes the same form
as p. Classical variational inference methods (see e.g. [23])

10783

deﬁne a class of distributions from which a q is selected
(or rather parameter θ is obtained). Here, motivated by the
approach used in GANs, we specify q through a likelihood-
free deterministic function which we use to generate sam-
ples (hence called generator). Assuming this generator
g (z), z ∼ pz is invertible1, by the calcu-
function x = gθ
lus of variable change [7] Eq[φ(w, x)] in Equation 3 is:

Z φ(w, x)q(x)dx =
Z φ(w, gθ

g (z))pz(g−1

θ

g

(x)|θz, θg)(cid:12)(cid:12)det(cid:0)J ⊤

1/2

1/2

dz

x Jx(cid:1)(cid:12)(cid:12)
dz = pz(z|θz)dz,

and pz(gθ

g (x)−1|θz, θg)(cid:12)(cid:12)det(cid:0)J ⊤

x Jx(cid:1)(cid:12)(cid:12)

where pz is the prior distribution with parameter θz and Jx
g (x)−1. Here we use the convention
is the Jacobian of gθ
that θ is composed of two parts: θg, θz for the parameters
g (·) and the distribution of
of the transformation function gθ
z, respectively. In the second line of Equation 4, we spec-
g preserves the volume with respect to φ(w, x).
ify that gθ
Note in practice we use φ′(w2, x) as a surrogate for g−1 and
regularize it to have a relatively constant determinant (by
adding a regularization term for the gradients of φ′(w2, x)).
To compute the entropy of q in Hx(q) in Equa-
tion 3, again using variable change, we have log pz(z) =
) and q(g(z)) = q(x)

1/2

log q(g(z)) + log((cid:12)(cid:12)det(cid:0)J ⊤

for which the expectation yields,

z Jz(cid:1)(cid:12)(cid:12)

entropy of prior noise

local geometry of the transformed space

Hx(q) =

+

z }| {H(pz)
z}|{Hg
where Hg = −Epz hlog(cid:12)(cid:12) det(J ⊤

,

z Jz)(cid:12)(cid:12)

1/2i (5)

where Jz is the Jacobian of gθ
g (the generator). Intuitively,
this equality states that the entropy with respect to the distri-
bution of the generated samples is the sum of two terms (1)
the entropy of the noise prior which captures the informa-
tion present in the noise distribution (speciﬁed by parameter
θz) and (2) the expected local behavior in the transformed
space (speciﬁed by parameter θg). From the information
geometric perspective, the latter term captures the volume
of the geometric space that the transformation function gθ
produces.

g

Remark 1. Explicitly constraining the generator to be in-
vertible is infeasible, however, in practice we can use net-
works with inverted architecture to mediate this. Further-
more, when the matrix J ⊤
z Jz is full-rank, it indicates the
mapping is one-to-one, i.e.
there is an invertible function
that maps samples from z to observation samples x.

Since computing the log det term in Equation 5 is com-
putationally expensive, we resort to an approximation that

1This is merely for theoretical analysis. In practice, we don’t need to

explicitly deﬁne an invertible function

is easier to optimize with stochastic gradient descent meth-
ods. We utilize the following Lemma2:
Lemma 2. For a matrix A ∈ Rd×d and Id the identity
matrix of size d where λ1(A) denotes the largest eigenvalue
and λ1(A) < α we have

log det(A) = d log(α) +

∞

Xk=1

(4)

(−1)k+1 tr(cid:16)(cid:0)A/α − Id(cid:1)k(cid:17)

.

k

(6)

Proof. Refer to the supplementary material.

Employing Lemma 2, we are able to substitute the com-
putation of the determinant for a more efﬁcient alternative
approximation. This approximation is arbitrarily close to
the true value depending on the number of steps in the se-
ries and the value chosen for α.

3.2. Main Algorithm

Taking the derivative of the normalizer w.r.t. the param-
eter w1 in Equation 1 yields the ﬁrst moment of this dis-
tribution, i.e. E[φ′(w2, x)] = ∂A(w)
. Furthermore, the
∂w1
second derivative is the covariance (and in this case Fisher
information specifying the sensitivity of the parameter to
the input). As such, it matches the moments from the true
distribution and its estimate with respect to the sufﬁcient
statistics φ′(w2, x) when using maximum likelihood.

Putting Equation 1, 3, 4 and 5 together, we have a max-
min problem to solve for two sets of parameters: w and θ.
Finding the saddle point solution for this problem is chal-
lenging, as such, we employ an alternating optimization us-
ing stochastic gradient descent to update the parameters w
and θ iteratively, as

w ← w + ηw∇w(cid:18)Ephφ(w, x)i − Epzhφ(w, gθ
θ ← θ − ηg∇θ(cid:18)Epzhφ(w, gθ

g (z))i(cid:19)
g (z))i + H(pz) + Hg(cid:19),

(7)

In general, we need to optimize w at each step until conver-
gence using the θ found in the previous iteration. In prac-
tice, we use a smaller learning rate to update θ (or fewer
rounds of update) to guarantee convergence [25].

The transformation function acts similar to the generator
in GANs is mode-seeking and it is complemented by the
ability of maximum likelihood to cover the space of X . In
particular, maximizing the entropy in the input noise and
the output ensures a transformation function gθ
g that does
not collapse.

Based on the deﬁnition we provided in Equation 4, the
ideal distribution for pz is one that follows a similar dis-
tribution. In particular, z is more likely for the most fre-
quent samples in the true distribution for which w⊤φ(x) is
the highest. However, as a prior we can choose a simple
distribution such as a Gaussian whose entropy H(pz) can

2A randomized version of this lemma is proposed in [8]

10784

be analytically computed in closed-form (e.g. for Guassian

log(σ√2πe) for standard deviation σ).

4,

=

Eq[w⊤

In practice, constraining gθ

1 φ′(w2, x)]
Epz [w⊤
1

from Equation
1 φ′(w2, gθ
g (z))]

g (·) = φ′−1(·)
Remark 3. It is interesting to note that if gθ
=
then
Epz [w⊤
z],
hence
Eq[φ′(w2, x)] = Epz [z] and Ep[φ′(w2, x)] = Epz [z].
This entails distribution of prior z should match that of
φ in expectation.
g to be an
inverse of φ (even in expectation) is too restrictive and
hard to implement3. However, it is fruitful to know when
constructing neural networks for this task, an inverse
architecture for the networks learning gθ
g and φ has
the best chance of success. Similarly, it is based on this
argument and the entropy in Equation 5 that it is better to
learn the distribution of z. However, it should be noted
that since gθ
g is a deterministic function, changes to the
distribution of z can negatively impact our generator
or decrease the convergence speed of gθ
In
practice, we make sure the learning rate of θz is smaller
than θg, its counterpart for gθ

g network.

g .

To estimate the integrals in the normalizer Aq(w) we
employ Monte Carlo method using samples the generator
produces and Lemma 2 to obtain ˆA(w), i.e.

ˆA(w) =

1
m

m

Xj=1

φ(w, xj) + H(pz) + ˆHg

where xj = gθ

g (zj), zj ∼ q(zj|θz), j = 1, . . . , m (8)
Here, ˆHg is the empirical estimate of Hg using samples zj
for the Jacobian of the transformation function gθ

g .

The summary of the algorithm is shown in Algorithm
In our
1 where we employ this alternating optimization.
approach density estimator maximizes the log-likelihood of
the given data with respect to the model in Equation 1. Gen-
erated samples from the transformation function gθ
g are the
byproduct of this maximization in the normalizer.

We note that our approach in maximizing the Hg in
Equation 5 resembles the ones in volume preserving or non-
volume preserving generative models [13, 12] and normal-
ized ﬂows [18, 38] that have been recently investigated.
These approaches maximize the likelihood of the data (cor-
responding to only maximizing Hg) directly by carefully
designing a function that is both invertible and its Jaco-
bian is easy to compute. These designs typically lead to
approximations that either produce subpar samples or are
inefﬁcient. Alternatively, our approach does not require an

3We observed in practice that if the generator function gz shares
weights with that of φ (i.e. second layer of gz with last layer of φ and
so forth) the approach does not perform well. In addition, even if we con-
strain each layer of the gz network to match in distribution to layers of φ
the quality of the generated samples deteriorate.

Algorithm 1 Our density estimation algorithm.
1: Input: X , learning rates ηw, ηg, ηz, ηz < ηg < ηw,
2: while θ is not converged do
3:

regularization parameter λ

m Pi (φ(w, gθ(zi)))(cid:3)

1, . . . , z′

m ∼ pz(z|θz)
n Pi (φ(w, xi)) − 1
m ∼ pz(z|θz)
2 log det(J ⊤
z Jz)
m Pihφ(w, gθ
1
= ∇θ

Sample uniformly x1, . . . , xn from X
Sample z′
w = ∇w(cid:2) 1
ρ′
R = −k∇w2 φ′(w2, x)k2
w = w + ηw.Adam(w, ρ′
Sample z′
ˆHg = 1
ρ′
θg = θg − ηg.Adam(θg, ρ′
θz = θz + ηz.Adam(θz,∇θ

1, . . . , z′

w + λ∇wR)

{g,z}

{g,z}

)

θ

θ

g

z(cid:0)H(pz) + ρ′

θ

⊲ Using Lemma 2

{g,z} (zi)) + ˆHgi

12:
13: end while

z(cid:1))

3
x

10

8

6

4

2

0

4:

5:

6:

7:

8:

9:

10:

11:

2
z

3

2

1

0

−1

−2

−3

−3

−2

−1

0

z1
z1

1

2

3

−2

x

0
1

2

0

20

15

10

x 2

5

Figure 1. Samples from z are transformed by g(z).
Red
dashed-line indicate the transformation function that has higher
log det(J ⊤
z Jz). This transformation induces a higher entropy in
the output space and hence better spread out.

invertible function. In addition, we apply the transforma-
tion in the dual space rather than directly on the input data.

1 φ(w2, x)(cid:3) ≤ A(w),
On the other hand, we know Ep(cid:2)w⊤
then for a given distribution q we have, Ep [φ(w, x)] −
g (z))(cid:3) ≤ H(pz) + Hg where the entropies
Ez∼pz (cid:2)φ(w, gθ
provide the bound on the difference between the moments
under p and q.

3.3. Bayesian Extension

One beneﬁt of such density estimation is that we can eas-
ily extend the model to a Bayesian one where a distribution
over parameters given the observations is taken rather than
the point estimate. As such, we employ prior distribution
over the parameters and the Bayes rule to derive the pos-
terior distribution over the parameters. For the exponential
distribution in this paper we use a conjugate prior for pa-
rameter w1: p(w1) = exp(w⊤
1 β − β0A(w1)). The poste-
rior is then computed in a closed-form: then

1 β + φ′(w2, x) − (1 + β0)A(w))
p(w1|x) = exp(w⊤
For w2 we use the same prior, however, the posterior
does not have a closed-form solution. As such, we em-
ploy Stochastic Gradient Langevin Dynamics (SGLD) [47]

10785

to sample from this posterior. SGLD is an efﬁcient algo-
rithm that is easy to implement when the gradient of the
log of the posterior is easy to compute (as it is in our case
where the density is estimated using a deep neural network).
SGLD performs Monte Carlo sampling by adding a Gaus-
sian noise with a variance proportionate to the learning rate
to the gradient updates of the log posterior. Therefore the
predictive distribution for a test instance x∗ is

p(x∗|X ) = Z p(x∗|w)p(w|X )dw

(9)

where we employ Monte Carlo estimate of this integral
along with the samples of the posterior to estimate. Pre-
dicting the output in Equation 9 needs to be weighted by
the step size in the Langevin dynamics to adjust for the de-
crease in the step size which in turn decreases the mixing
rate as explained in [47].

Note in practice, we found it helps to speed up the “burn-
in ”process where we use Adam for the ﬁrst few iterations
(in larger datasets), after which we revert to SGD and sam-
ple according to SGLD update rule in Algorithm 1. As was
shown in [39, 5, 4] such Bayesian approaches can signif-
icantly improve the performance due to the ability of the
model in exploring the space and averaging parameter val-
ues.

1.6

1.4

1.2

1.0

0.8

0.6

0.4

0.2

0.0

Figure 2. A simple density estimation: the ﬁrst row shows a multi-
modal distribution and the distribution of samples from the gener-
ator at various stages; and, in the second diagram, the convergence
of the estimated density to its true value is shown.

DCGAN [37]
ALI [14]
Unrolled GAN [33]
Ours
Ours Bayesian

Modes

99
16
48.7
125
142

Table 1. Degree of mode collapse on Stacked-MNIST.

3.4. Mode Collapse

4. Experiments

g

Mode collapse is a phenomenon in which function gθ
maps all zs to a small number of points in the space of X .
This can happen in similar algorithms such as GANs where
mode collapse is a major problem. Our approach is less
likely to suffer from this phenomenon since we are maxi-
mizing the Hg for the generated samples. As shown in Fig-
ure 1 we transform the input z to the output space as the lo-
cal geometry of the output space better spread out because
we are enforcing the local geometry of a point mapped in
the output space x to encourage higher volume.

The interplay between the entropy of the distribution of
z and the local characteristics of the transformation func-
tion heavily contribute to the performance of the quality of
the samples generated. In particular, in case the distribu-
tion of z is Gaussian, maximizing the entropy amount to
increasing its corresponding variance. Staring with a large
variance and transforming to a spread-out space in p, leads
to an estimate of the likelihood that covers all the space with
relatively similar value. Hence, the samples generated this
way exhibit poor quality and as such under-perform in esti-
mating the correct likelihood. On the other hand, when the
initial variance of the distribution of z is small and gradu-
ally this variance is increased, the samples generated by gθ
cover the mode of the distribution p ﬁrst and subsequently
learns the less frequent samples. In practice, we take the
latter approach and start with a small variance for the pz so
that the mode of p is learnt ﬁrst.

g

Our approach provides a path for density estimation
while we have the capacity to draw samples from the dis-
tribution learned from the dataset. As such, we direct our
focus towards various applications where various aspects of
our approach is evaluated. We use Algorithm 1 to train our
density model in which a generator is also learnt that we
can sample from. When the distribution of the output for
the generator is too large, computing the Jacobian is expen-
sive. As such, during training we sample a set of dimensions
from the output and compute the gradients. To estimate the
Jacobian we use Lemma 2 and found the value of α set to
the dimension of the Jacobian performs well in practice. In
addition, we update the parameter w twice as often as that
of θ while regularize the gradients to ensure stable training
that converges to an optimum solution.

4.1. Simulation

For the ﬁrst experiment, we use samples from a Gaussian
mixture model to simulate a dataset we are interested in its
density model. Each component is a Gaussian distribution
with standard deviation 0.1. The generated dataset is shown
in Figure 2 on top. We then use a simple two fully con-
nected layer neural network for the generator and φ(w, x).
In the top row of Figure 2 the generated samples are plot-
ted and as shown over time our approach is able to capture
the density model and generate samples from all the modes.
Furthermore, at the bottom of Figure 2 the convergence of

10786

step 800step 1500step 2200step 3000Real Samples1
7
4
6
9
2
8
5
0
3

0.000

0.002

0.004

0.006

0.008

0.010

Figure 3. Average posterior probability for each digit sorted from
the highest variance to the lowest.

MNIST

VAE [23]
IWAE (IW = 50) [9]
VAE+NF [38]
ASY-IWAE [49]
Auxiliary VAE [31]
DARN [17]
IWAE+ConvFlow [50]

Ours
Ours-Bayesian

− log p(x)
87.88
86.10
85.10
85.76
84.59
84.13
79.78

82.39
83.12

Figure 4. Samples from MNIST sorted based on their density from high
to low.

Table 2. MNIST test set negative log-likelihood with generative
models (lower is better). These values are estimates since the true
density is intractable.

the density estimate to its true value is shown. As observed,
as the distribution of the samples from the generator look
more similar to the real ones, the estimate of the density (as
measured by the expected log-likelihood with respect to the
true distribution) approaches the true value.

4.2. Image Generation and Density Estimation

In this section, we examine the quality of the generated
images using our approach as well as its density estimation
ability. We use DCGAN’s [37] architecture for image gen-
eration.

MNIST: To evaluate the effect of the Jacobian on the
mode collapse, we have conducted an experiment similar to
that of [33] where we select 3 digits at random and build
a 3-channel “stacked-MNIST” by concatenating them. Us-
ing a pre-trained classiﬁer, we ﬁnd the number of modes
for which the generator produced at least one sample. The
results of running this experiment is shown in Table 4. As
observed our approach outperforms its counterparts indicat-
ing the ability of our approach in avoiding mode-collapse.

We use MNIST, a dataset of handwritten characters, to
generate samples from q. We show samples both from max-
imum log-likelihood and Bayesian. We use the predictive
probability obtained from Equation 9 is shown in Figure 5.
It is interesting to observe that 1 and 7 are the least likely
digits because they have straight lines that have structural
differences with other digits. In addition, we compare the
likelihood estimated by our approach compared to the-state-
of-the-art. The evaluation of log-likelihood (LL) in nats is
reported in Table 2. All log-likelihood values were esti-
mated as the mean of 5000 instances drawn randomly from
the test set. Since we use the predictive distribution in Equa-
tion 9, the likelihood is not as low as when we compute the
likelihood directly. Hence, the reported values are stochas-
tic lower bounds on the true value. Figure 4 depicts sam-
ples drawn from the generator sorted by the likelihood using

(a) Max LL

(b) Bayesian

(c) Max LL

(d) Bayesian

Figure 5. Samples drawn from the generator using MNIST: 5(a)
and 5(b); and, CIFAR-10 5(c) and 5(d). Alternatives (e.g. [24, 41])
can generate visually better quality images.

our Bayesian approach.

CIFAR-10:We have trained our approach on CIFAR-10
dataset [26] composed of 50, 000 RGB images of natural
scenes. Samples from this dataset is shown in Figure 5.
We achieve the inception score of 7.84 using a modiﬁed
ResNet.

CelebA: While MNIST and CIFAR-10 are smaller di-
mensions, we evalue our approach on CelebA [28] with size
256 × 256. While our approach performs density estima-
tion, we can generate high-quality images to estimate the
normalizer. In this experiment, we utilize the Progressive-
GAN [22] architecture and learning procedure to produce
images and evaluate their density estimation. We compare
the output of our density estimator versus WGAN-GP’s [19]
discriminator output as shown in Figure 6. The histogram
distribution of the fake samples in WGAN-GP looks very
similar to the Normal distribution similar to the prior noise.
In addition, as expected with most GAN-based methods, the
discriminator value overlaps between real and fake samples
indicating the discriminator’s confusion. Our approach on
the other hand, shows a better spread histograms. Further-
more, the images in the graph shows samples of generated
images with their probability as computed by our approach.
It is observed that more typical looks that are more repre-
sentative of the dataset concentrated in a high density area.
In the progressive training, we freeze the update of the noise

10787

Real

Fake from Normalizer

Real

Fake

r
o
t
a
m

i
t
s
E
 
y
t
i
s
n
e
D
 
r
u
O

P
G

-

N
A
G
W

g (z)) for 500
Figure 6. Histogram of the evaluation of φ(w, gθ
samples generated from the CelebA dataset with 256×256 dimen-
sions (on top) compared to the discriminator value of WGAN-GP
(on bottom). On the left corner of each plot, the distribution of the
φ(w, gθ
g (z)) is shown vs. the output of the discriminator in the
WGAN-GP. As shown, discriminator in WGAN-GP ﬁnds a distri-
bution that has around half overlap between reals and the fakes.

entropy for the higher quality images to avoid changing
the networks signiﬁcantly. The Frechet Inception Distance
(FID) computed from 50K images is 5.96 for this dataset.

4.3. Uncertainty in Visual Question Answering

We utilize our approach in the visual dialog problem in-
troduced in [10]. The task is to design an agent capable of
replying to questions based on image content and history of
a dialog with a human user. The objective is to generate
responses to questions that are most likely to be given by a
human. The transformation function gθ
g is responsible for
generating a suitable answer for the user’s question, given
the previous history of agent-user conversations, the user
question and the visual content (rather than just a noise dis-
tribution in the previous experiment). Then, the likelihood
of the human responses is higher.

Our approach is well embedded into a larger dialog sys-
tem built upon that of [42, 29]. We use a hierarchical setup
where a neural network for sequence models (LSTM in our

case) encodes the word sequence and another one captures
the context through historical dependencies of the conver-
sions. A common practice is to use an attention mecha-
nism with the encoder to implicitly identifying parts of di-
alog history relevant to the current question [42, 10]. It is
expected that a similar mechanism is able to localize the
relevant regions in the image consistent with the attended
history. Since the output is discrete, we use Gumbel-Max
trick [21, 32] which illustrates the ability of our approach in
maximizing likelihood for discrete data.

This task is speciﬁcally suitable for Bayesian formula-
tion of our approach since the space in which the function
gθ
g formulates the responses is large and as such this causes
uncertainty in the prediction. Hence, our approach lever-
ages this uncertainty in the predictive distribution when re-
sponding by providing “I don’t know” statements when the
predictive variance is high.

The visual dialog [10] dataset we use was collected
by pairing two Amazon Mechanical Turk users convers-
ing about an image. The “questioner” user is shown the
captions of the image (from COCO dataset [27]) while the
image itself remains hidden. The “answerer” user sees the
image and provides responses about the image to the ques-
tioner. This dialog is carried out for 10 rounds on the COCO
dataset leading to 83, 000 training dialogs and 40, 000 vali-
dation dialog. Following the baseline, the likelihood score
is used in ranking amongst 100 candidate answers.

We set the parameter for the Gumbel temperature to 0.1.
Our LSTMs are single layer with 512-dimensional hidden
states. Further, we use VGG-19 [44] to represent images
and Adam optimization with the learning-rate of 4e-4. For
the Bayesian case, we use SGD and set the learning rate to
decrease from 1e-3 to 1e-4. Following the problem setup in
[29], we pre-train our transformation function gθ
g by maxi-
mizing the score it generates compared to the ground-truth.
Similarly, we pre-train the density estimator using samples
from 100 rounds of the question-answers such that the score
of the ground-truth is higher than non-matching responses
in the training set. We pre-train both for 20 epochs. More-
over, we regularize the latent space from which the answers
are generated with its entropy (we assumed the latent space
is a zero-mean Gaussian). We draw 5 samples from which
we pick the most likely.

In Table 3 we provide the results of running our approach
on the visual dialog dataset compared to the state-of-the-
art GAN methods and other baselines in [43, 29]. Mod-
els are evaluated on standard retrieval metrics: mean rank,
recall @k, and mean reciprocal rank (MRR) of human re-
sponse. The results from the pre-training stage is indicated
as HCIAE in the table and as is shown GAN training im-
proves the performance. In addition, our approach outper-
forms the baseline with a good margin and is more stable
in training due to its regularization in the output. Moreover,

10788

Discriminative

Generative

Model

MRR

R@1

R@5

R@10 Mean MRR

R@1

R@5

R@10 Mean

HieCoAtt [30]
LF [10]
HRE [10]
MN [10]
HCIAE [29]

GAN1 [29]
GAN2 [29]
WGAN

Ours
Ours Bayesian

0.5788
0.5807
0.5868
0.5965
0.6140

0.2177
0.6050
0.5524

0.6125
0.6204

43.51
43.82
44.82
45.55
47.73

8.82
46.20
43.77

47.17
47.65

74.49
74.68
74.81
76.22
77.50

32.97
77.92
67.75

77.25
78.91

83.96
84.07
84.36
85.37
86.35

52.14
87.20
72.32

87.65
88.30

5.84
5.78
5.66
5.46
5.15

18.53
4.97
13.18

4.82
4.61

-

0.5199
0.5242
0.5259
0.5386

0.5298
0.5459
0.5306

0.5448
0.5513

-

41.83
42.28
42.29
44.06

43.12
44.33
43.49

44.37
45.03

-

61.78
62.33
62.85
63.55

62.74
65.05
62.38

64.34
65.25

-

67.59
68.17
68.88
69.24

68.58
71.40
67.73

71.81
71.93

-

17.07
16.79
17.06
16.01

16.25
14.34
17.15

14.93
14.01

Table 3. Results on Visual Dialog dataset with Late Fusion (LF), Hierarchical Recurrent Encoder (HRE), Memory Network (MN). GAN1
is devised by using generated fake samples in the discriminator and GAN2, in addition, includes negative answers. Higher values are better
except for the mean. Best numbers are boldfaced.

Q: is the train old?
A: i think so
Sample A: yes (0.03 ± 0.2)
Q: is the train moving?
A: no but it does have some steam com-
ing
Sample A: no (0.02 ± 0.1)

Q: is this in color ?
A: yes
Sample A: yes (0.1 ± 0.05)
Q: how old does the boy look?
A: he is not facing me, but maybe
Sample A: 3 (0.1 ± 0.4)

Q: what vegetables are there?
A: carrots, cauliﬂower, broccoli
Sample A: carrots , carrots , and cucum-
bers (0.1 ± 0.01)
Q: what color is the table ?
A: dark brown
Sample A: white (0.1 ± 0.02)

Table 4. Qualitative comparison from the Visual Dialog dataset. (Q) is the question, (A) is the ground-truth human response, (Sample A):
is the generated answer from the generator. Red response indicates lack of conﬁdence, which will be substituted by “I don’t know”.

we devised the Bayesian variant (as in Section ) to compute
the predictive distribution and pick the one with the high-
est conﬁdence according to Equation 9 (highest mean plus
standard deviation). Since the space of answers generated
is typically large even with pre-training, performance of the
generator and discriminator can deteriorate. This is because
the noisy samples from the generator lead to decreased the
performance in the discriminator.

In Table 4 we show samples from the visual dialog
dataset. We evaluate the generated answers by sampling
the discriminator function and evaluating the variance of
the prediction (likelihood of the generated answer being
human-response in the log space). As is shown in the table,
we observe the variance of the prediction is generally higher
for the wrong responses. We can use a simple thresholding
on the predictive variance to determine the answers to be
substituted by “I don’t know”. For instance, in Table 4 we
show an example where a highly uncertain answer is sub-
stituted. Even though for frequent answers such as yes/no
responses the variances are less reliable.
In addition, for
longer sentences, the variances are generally higher. This is

expected due to the potential diversity.

5. Conclusion

In this paper, we introduced generative adversarial den-
sity estimator. Our approach estimates the density of data
using a lower bound on its normalizer. We formalized the
problem as a maximum likelihood for the density estima-
tor in which a transformation function generates samples
for approximating the normalizer. We showed that having a
density model and the prior on the parameters, we are able
to build a Bayesian alternative using which we can measure
our conﬁdence in the likelihood. Our experiments on chal-
lenging applications such as image density or visual dialog
where both the likelihood and conﬁdence in predictions are
crucial shows the effectiveness of our approach. Moreover,
the samples drawn from the estimated density is less sus-
ceptible to mode collapse which is a desired property for
generative models.

We believe our approach opens new avenues for further
research in density estimation and its marriage with adver-
sarial models.

10789

References

[1] E. Abbasnejad, S. Sanner, E. V. Bonilla, and
P. Poupart. Learning community-based preferences
via dirichlet process mixtures of gaussian processes.
In Proceedings of
the Twenty-Third International
Joint Conference on Artiﬁcial Intelligence, IJCAI ’13,
pages 1213–1219, 2013. 2

[2] E. Abbasnejad, J. Shi, and A. van den Hengel. Deep

lipschitz networks and dudley GANs, 2018. 2

[3] I. Abbasnejad, S. Sridharan, D. Nguyen, S. Denman,
C. Fookes, and S. Lucey. Using synthetic data to im-
prove facial expression analysis with 3d convolutional
networks.
In Proceedings of the IEEE International
Conference on Computer Vision, pages 1609–1618,
2017. 2

[4] M. E. Abbasnejad, A. Dick, Q. Sh i, and A. van den
Hengel. Active learning from noisy tagged images.
2018. 5

[5] M. E. Abbasnejad, Q. Shi, I. Abbasnejad, A. van den
Hengel, and A. R. Dick. Bayesian conditional gen-
erative adverserial networks. CoRR, abs/1706.05477,
2017. 5

[6] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein

gan. ArXiv e-prints, Jan. 2017. 2

[7] A. Ben-Israel. The change-of-variables formula using
matrix volume. SIAM Journal on Matrix Analysis and
Applications, 21(1):300–312, 1999. 3

[8] C. Boutsidis, P. Drineas, P. Kambadur, E.-M. Kon-
topoulou, and A. Zouzias. A randomized algorithm
for approximating the log determinant of a symmetric
positive deﬁnite matrix. Linear Algebra and its Appli-
cations, 533, 2017. 3

[9] Y. Burda, R. Grosse, and R. Salakhutdinov.

Impor-

tance weighted autoencoders. 09 2015. 6

[10] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M.
In
Moura, D. Parikh, and D. Batra. Visual Dialog.
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2017. 2, 7, 8

[11] P. J. Diggle and R. J. Gratton. Monte Carlo methods
of inference for implicit statistical models. Journal of
the Royal Statistical Society., pages 193–227, 1984. 1

[12] L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-
linear independent components estimation. CoRR,
abs/1410.8516, 2014. 1, 4

[13] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density

estimation using real nvp. 2016. 1, 4

[14] V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversari-
ally learned inference. In International Conference on
Learning Representation, 2017. 5

[15] M. Ehsan Abbasnejad, A. Dick, and A. van den
Hengel.
Inﬁnite variational autoencoder for semi-
supervised learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition,
pages 5888–5897, 2017. 2

[16] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-
gio. Generative adversarial nets. In International Con-
ference on Neural Information Processing Systems,
2014. 1, 2

[17] K. Gregor, I. Danihelka, A. Mnih, C. Blundell, and
D. Wierstra. Deep autoregressive networks. In The In-
ternational Conference on Machine Learning (ICML),
2014. 1, 6

[18] A. Grover, M. Dhar, and S. Ermon. Flow-gan: Com-
bining maximum likelihood and adversarial learning
in generative models.
In Proceedings of the Thirty-
Second AAAI Conference on Artiﬁcial Intelligence,
(AAAI-18), the 30th innovative Applications of Arti-
ﬁcial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artiﬁcial Intel-
ligence (EAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018, pages 3069–3076, 2018. 2, 4

[19] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin,
and A. C. Courville. Improved training of wasserstein
gans. CoRR, abs/1704.00028, 2017. 2, 6

[20] M. U. Gutmann, R. Dutta, S. Kaski, and J. Corander.
Likelihood-free inference via classiﬁcation. Statistics
and Computing, 2018. 1

[21] E. Jang, S. Gu, and B. Poole. Categorical reparame-

terization with gumbel-softmax. CoRR, 11 2016. 7

[22] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Pro-
gressive growing of gans for improved quality, stabil-
ity, and variation. In The International Conference on
Learning Representations (ICLR), 2017. 1, 2, 6

[23] D. P. Kingma and M. Welling. Auto-encoding vari-
In The International Conference on

ational bayes.
Learning Representations (ICLR), 2014. 1, 2, 6

[24] A. Kolesnikov and C. H. Lampert. PixelCNN models
with auxiliary variables for natural image modeling.
In D. Precup and Y. W. Teh, editors, Proceedings of
the 34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learning
Research, pages 1905–1914, International Convention
Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. 6

[25] V. R. Konda and J. N. Tsitsiklis. Convergence rate of
linear two-time-scale stochastic approximation. Ann.
Appl. Probab., 14(2):796–819, 05 2004. 3

[26] A. Krizhevsky and G. Hinton. Learning multiple lay-

ers of features from tiny images. 2009. 6

10790

[27] T.-Y. Lin, M. Michael, B. Serge, H. James, P. Pietro,
R. Deva, D. Piotr, and Z. C. Lawrence. Microsoft
coco: Common objects in context. In Computer Vi-
sion – ECCV 2014. Springer International Publishing,
2014. 7

[28] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning
face attributes in the wild. In Proceedings of Interna-
tional Conference on Computer Vision (ICCV), 2015.
6

[29] J. Lu, A. Kannan, , J. Yang, D. Parikh, and D. Ba-
tra. Best of both worlds: Transferring knowledge from
discriminative learning to a generative visual dialog
model. NIPS, 2017. 7, 8

[30] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchi-
cal question-image co-attention for visual question an-
swering.
In Proceedings of the 30th International
Conference on Neural Information Processing Sys-
tems, NIPS’16, pages 289–297, USA, 2016. Curran
Associates Inc. 8

[31] L. Maaløe, C. K. Sønderby, S. K. Sønderby, and
In
O. Winther. Auxiliary deep generative models.
Proceedings of
the 33rd International Conference
on International Conference on Machine Learning,
ICML’16, pages 1445–1454, 2016. 6

[32] C. J. Maddison, A. Mnih, and Y. W. Teh. The con-
crete distribution: A continuous relaxation of discrete
random variables. CoRR, 2016. 7

[33] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Un-
rolled generative adversarial networks. International
Conference on Learning Representations, 2016. 5, 6

[34] A. Nguyen, J. Clune, Y. Bengio, A. Dosovitskiy, and
J. Yosinski. Plug & play generative networks: Con-
ditional iterative generation of images in latent space.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. IEEE, 2017. 1

[35] S. Nowozin, B. Cseke, and R. Tomioka.

f-GAN:
Training Generative Neural Samplers using Varia-
tional Divergence Minimization. ArXiv e-prints, 2016.
2

[36] A. v. d. Oord, N. Kalchbrenner, O. Vinyals, L. Espe-
holt, A. Graves, and K. Kavukcuoglu. Conditional
image generation with pixelcnn decoders.
In Pro-
ceedings of the 30th International Conference on Neu-
ral Information Processing Systems, NIPS’16, pages
4797–4805, 2016. 1, 2

[37] A. Radford, L. Metz, and S. Chintala. Unsupervised
representation learning with deep convolutional gen-
erative adversarial networks. CoRR, 2015. 5, 6

[38] D. J. Rezende and S. Mohamed. Variational inference
with normalizing ﬂows. In Proceedings of the 32Nd
International Conference on International Conference

on Machine Learning, ICML’15, pages 1530–1538.
JMLR.org, 2015. 2, 4, 6

[39] Y. Saatchi and A. G. Wilson. Bayesian GAN.

In
Advances in Neural Information Processing Systems,
2017. 5

[40] R. Salakhutdinov and G. E. Hinton. Deep boltzmann
machines. In International conference on artiﬁcial in-
telligence and statistics, AISTATS’09, 2009. 1, 2

[41] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma.
Pixelcnn++: A pixelcnn implementation with dis-
cretized logistic mixture likelihood and other modiﬁ-
cations. In The International Conference on Learning
Representations (ICLR), 2017. 2, 6

[42] I. V. Serban, A. Sordoni, Y. Bengio, A. Courville, and
J. Pineau. Building end-to-end dialogue systems us-
ing generative hierarchical neural network models. In
Proceedings AAAI, pages 3776–3783, 2016. 7

[43] R. Shetty, M. Rohrbach, L. A. Hendricks, M. Fritz,
and B. Schiele. Speaking the same language: Match-
ing machine to human captions by adversarial train-
ing. CoRR, abs/1703.10476, 2017. 7

[44] K. Simonyan and A. Zisserman. Very deep convolu-
tional networks for large-scale image recognition. In
arXiv, 2014. 7

[45] L. Theis, A. van den Oord, and M. Bethge. A note on
the evaluation of generative models. ArXiv e-prints,
Nov. 2015. 1

[46] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz,
and B. Catanzaro. High-resolution image synthe-
sis and semantic manipulation with conditional gans.
2017. 2

[47] M. Welling and Y. W. Teh. Bayesian learning via
stochastic gradient langevin dynamics.
In Proceed-
ings of the 28th International Conference on Interna-
tional Conference on Machine Learning, pages 681–
688, 2011. 4, 5

[48] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang,
and D. Metaxas. Stackgan: Text to photo-realistic im-
age synthesis with stacked generative adversarial net-
works. In ICCV, 2017. 1

[49] G. Zheng, Y. Yang, and J. Carbonell. Likelihood al-

most free inference networks. 11 2017. 6

[50] G. Zheng, Y. Yang, and J. Carbonell. Convolutional
In ICML Workshop on Theoreti-
normalizing ﬂows.
cal Foundations and Applications of Deep Learning,
2018. 6

10791

