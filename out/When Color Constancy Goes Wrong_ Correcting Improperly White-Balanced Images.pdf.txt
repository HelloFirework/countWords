When Color Constancy Goes Wrong:

Correcting Improperly White-Balanced Images

Mahmoud Aﬁﬁ
York University

mafifi@eecs.yorku.ca

Brian Price

Adobe Research
bprice@adobe.com

Scott Cohen

Adobe Research
scohen@adobe.com

Michael S. Brown
York University

mbrown@eecs.yorku.ca

Abstract

WB: Fluorescent 

This paper focuses on correcting a camera image that
has been improperly white-balanced. This situation oc-
curs when a camera’s auto white balance fails or when the
wrong manual white-balance setting is used. Even after
decades of computational color constancy research, there
are no effective solutions to this problem. The challenge
lies not in identifying what the correct white balance should
have been, but in the fact that the in-camera white-balance
procedure is followed by several camera-speciﬁc nonlin-
ear color manipulations that make it challenging to cor-
rect the image’s colors in post-processing. This paper in-
troduces the ﬁrst method to explicitly address this problem.
Our method is enabled by a dataset of over 65,000 pairs
of incorrectly white-balanced images and their correspond-
ing correctly white-balanced images. Using this dataset,
we introduce a k-nearest neighbor strategy that is able to
compute a nonlinear color mapping function to correct the
image’s colors. We show our method is highly effective and
generalizes well to camera models not in the training set.

1. Introduction

When taking a photograph, we expect our images to be
correctly white-balanced. Computer vision algorithms im-
plicitly assume a correct white balance (WB) by expecting
their input image colors to be correct. What happens when
the WB is not correct? In such cases, the images have the
familiar bluish/reddish color casts that not only are undesir-
able from a photography standpoint but also can adversely
affect the performance of vision algorithms.

Correcting improperly white-balanced images is poorly
understood. Sources such as Matlab [1] purports misleading
solutions that suggest the problem is a matter of identifying
what the correct WB should have been and then applying
this as a post-correction. However, this solution is not ef-
fective and does not consider WB within the full context of
the in-camera processing pipeline.

WB: Shade

Images with 

incorrect WB applied

Correction using 

diagonal 
correction

Correction using 
“linearization” and 
diagonal correction

Our results

Ground truth

(correct camera WB)

Figure 1. Two incorrectly white-balanced images produced by dif-
ferent cameras and attempts to correct them using (1) a linear WB
correction, (2) correction by ﬁrst applying a gamma linearization
( [3,17]), and (3) our results. Also shown is the ground truth image
produced by the camera using the correct WB.

WB is applied on board cameras to remove the color
cast in images caused by the scene’s illumination. WB falls
into the research area of computational color constancy that
aims to mimic the human visual system’s ability to perceive
scene colors similarly even when observed under different
illuminations [21]. WB correction is performed with a sim-
ple linear diagonal 3×3 matrix. The WB transform is ap-
plied to the camera’s raw-RGB image and is often one of the
ﬁrst steps in the in-camera processing pipeline. After WB
is applied, several additional transforms are applied to con-
vert the image values from a sensor-speciﬁc raw-RGB color
space to an output-referred color space–namely standard
RGB (sRGB). These additional transforms include camera-
speciﬁc nonlinear color manipulations that make up a cam-
era’s photo-ﬁnishing routines (for more details, see [26]).
A simple model for the in-camera color manipulation from

11535

sensor raw-RGB to sRGB can be expressed as:

Scene (bridge) 
reference white

R      G       B
170   201   254
76    115   185

R      G       B
255  201   158
115  115    115

IsRGB = fXYZ→sRGB(Traw→XYZDWBIraw),

(1)

where IsRGB and Iraw are 3×N matrices containing the image
values in the sRGB and raw-RGB spaces respectively, N is
the total number of pixels, D represents the 3×3 diagonal
WB matrix, T is a 3 × 3 linear transform that maps from
white-balanced raw-RGB to a device-independent color
space, such as CIE-XYZ (or one of its derivatives), and f (·)
is a nonlinear function that compounds various operations,
including color enhancement, tone-manipulation, and a ﬁ-
nal sRGB gamma encoding. We can think of f (·) as the
collective in-camera color rendering operation performed
on the camera after WB. Prior works [11,27,29] have shown
that f (·) not only is speciﬁc to camera models but also de-
pends on the camera settings used during image capture.

From Eq. 1, it is clear that because WB is applied early in
the processing chain, attempting to correct it using a diago-
nal matrix will not work. Matlab suggests using an optional
pre-linearization step using a 2.2 gamma [3, 17]. However,
it has long been known that a 2.2 gamma does not reﬂect the
true nature of the f (·) function [15]. Fig. 1 shows examples.

Contribution We propose a data-driven approach to cor-
rect images that have been improperly white-balanced. As
part of this effort, we have generated a new dataset of over
65,000 images from different cameras that have been ren-
dered to sRGB images using each camera’s pre-deﬁned WB
settings and picture styles. Each incorrect white-balanced
image in the dataset has a corresponding correct white-
balanced sRGB image rendered to a standard picture style.
Given an improperly white-balanced camera image, we out-
line a k-nearest neighbor strategy that is able to ﬁnd simi-
lar incorrectly white-balanced images in the dataset. Based
on these similar example images, we describe how to con-
struct a nonlinear color correction transform that is used to
remove the color cast. Our approach gives good results and
generalizes well to camera makes and models not found in
the training data. In addition, our solution requires a small
memory overhead (less than 24 MB) and is computationally
fast.

2. Related Work

As far as we are aware, this paper is the ﬁrst to directly
address the problem of correcting colors in an incorrectly
white-balanced image. The problem is related to three ar-
eas in computer vision: (i) computational color constancy,
(ii) radiometric calibration, and (iii) general color manipula-
tion. These are discussed within the context of our problem.

Computational Color Constancy WB is performed to
mimic our visual system’s ability to perceive objects as hav-

Color chart reference white

(A) sRGB rendered image with incorrect 

white balance

R      G       B
201  201    201
90   115    146

Color chart white is correct.
Scene white is incorrect.

(B) Diagonal correction of (A) using the 
color chart's patch as a white reference  
R      G       B
232   229   227
112   116   159

Scene white is correct.
Color chart white is incorrect.
(C) Diagonal correction of (A) using the 

scene (bridge) as a white reference

Scene white is correct.
Color chart white is incorrect.

(D) Auto-color correction

(Adobe Photoshop)

R      G       B
208  212   214
120  121   119

R      G       B
213   213   213
121   121   121

Scene white and color chart 
white are both improved

(E) Our result

Scene and color chart white 
are both correct.

(F) sRGB rendered image with correct 

white balance (ground truth image)

Figure 2. (A) A camera sRGB image with a wrong white balance
applied.
(B) and (C) show traditional white balance correction
applied to the image using different reference white points manu-
ally selected from the image (note: this would represent the best
solution for an automatic illumination estimation algorithm). (D)
shows the result of auto-color correction from Adobe Photoshop.
(E) Result from our approach. (F) The ground truth camera image
with the correct white balance applied.

ing the same color even when viewed under different illu-
minations. WB requires the camera’s sensor response to
the scene’s illumination. Once the color of the illumination
is known, a 3 × 3 diagonal matrix is used to normalize the
illumination’s colors by mapping them to the achromatic
line in the camera’s raw-RGB color space (i.e., the raw-
RGB values corresponding to the the scene’s illumination
are mapped to lie on the R=G=B “white” line). The vast
majority of computational color constancy research is fo-
cused on illumination estimation. Representative examples
include [5, 6, 8, 10, 12, 20–22, 25, 35]. Illumination estima-
tion is computed in the camera’s raw-RGB color space and
makes up the camera’s auto WB mechanism.

Note that none of the aforementioned illumination esti-
mation methods are intended to be applied to sRGB images.
Even if an illumination estimation method could be used to
determine the color of an achromatic region in a sRGB im-
age, this information is still insufﬁcient to correct an incor-
rectly white-balanced image. To stress this point, we pro-
vide an example in Fig. 2. Fig. 2-(A) shows a camera image
rendered through a camera pipeline to an sRGB output with
the incorrect WB. Two achromatic regions in the scene are
highlighted: (i) a patch from a white bridge and (ii) a neutral
patch from the color chart. The same scene is rendered with
the correct WB in Fig. 2-(F). Because the WB was applied
correctly in Fig. 2-(F), both the scene achromatic regions lie
on the white line (i.e., R=G=B). Fig. 2-(B) and (C) show
attempts at using standard diagonal WB correction using

1536

Training data (for each training image, we pre-calculate its mapping function M (j), its histogram, and PCA feature vector) 

(i)

It

It
(1)

…

…

…

…

It
(l)

…

…

It
(l+i)

…

…

M (1)

M (i)

M (l)

M(l+i)

D
0
0
6
n
o
n
a
C

0
0
2
5
D
n
o
k
N

i

 

(j)

It

…

…

It
(j+i)

…

…

It
(n)

h(It

(1))

M (j)

M (j+i)

…

h(It

(n))

v(It

Corrected images 
Igt (ground truth)

v(It

(1))

(cid:851)(cid:1709)(cid:851)
(cid:851)
(cid:851)(cid:1709)(cid:851)
(cid:851)

…

(n))

Camera: Unknown
WB: Unknown 
Camera Style: Unknown
Correction map: Unknown

Incorrect WB imagee Iin

RGB-uv histogram

h(Iin)

(cid:851)(cid:1709)(cid:851)

CA feat
PCA feature
v(Iin)

Compute a weighted mapping function M from 

the k similar training examples Ms
M = (cid:68)1Ms
(k)

(2) … + (cid:68)k Ms

(1) + (cid:68)2 Ms

(i)

…

Find k similar images 
based on PCA feature

Camera: Canon 1Ds Mark III
WB: Fluorescent
Style: Portrait
Correction map: Ms

(1)

Camera: Canon 600D
WB: Incandescent
Style: Neutral
Correction map: Ms

(2)

Camera: Nikon D5200
WB: Incandescent
Style: Landscape
Correction map: Ms
(k)

Final corrected image
g

Icorr = M Φ(Iin)

Figure 3. An overview diagram of our overall procedure. For the input sRGB image and our training data, we ﬁrst extract the histogram
feature of the input image, followed by generating a compact PCA feature to ﬁnd the most similar k nearest neighbors to the input image
in terms of colors. Based on the retrieved similar images, a color transform M is computed to correct the input image.

the color chart’s patch and bridge scene region as reference
white, respectively. We can see that in both cases, only the
selected reference white region is corrected, while the other
region remains incorrect.

Radiometric Calibration Radiometric calibration is the
process of parameterizing a camera’s nonlinear color ma-
nipulation in order to reverse it. Radiometric calibration
essentially attempts to approximate f −1 from Eq. 1. There
have been a number of methods addressing radiometric cali-
bration (e.g., [11,23,27–31,38]). Radiometric calibration is
performed to linearize the photo-ﬁnished camera sRGB im-
ages in order to aid low-level computer vision tasks that re-
quire a linear response to scene irradiance (e.g., photomet-
ric stereo, image deblurring, HDR imaging) [33, 34]. When
radiometric calibration data is available, it can be used to
undo the photo-ﬁnishing in an sRGB image in order to cor-
rect WB, as demonstrated by [27]. However, performing
radiometric calibration requires a tedious calibration pro-
cedure. The radiometric metadata needs to be stored and
associated with each captured image [34]. As a result, ra-
diometric calibration data is rarely available for most users.

Color Transformations Another topic related to our
work is color transformations that are used to map an
input image’s color space to some desired target color
space. These methods are typically used for the purpose
of colorimetric calibration. Representative examples in-
clude [2, 7, 16, 18, 19, 24]. Most of these methods rely on
explicit color correspondences between the input and target
color spaces. The most effective methods are those based on
polynomial color correction [19, 24] and recent root poly-
nomial color correction [19]. These approaches use kernel
functions that project the original color data from the three-
channel RGB color space to a higher-dimensional space.

Additional ad hoc methods include routines in software
such as Adobe Photoshop that perform auto-color and auto-
tone manipulation [14]. Fig. 2-(D) shows correction based
on Adobe Photoshop’s auto-color. Similar to Fig. 2-(B) and
(C), this can correct one of the regions, but not the other.

To date, there are no effective methods for correcting in-
correctly white-balanced camera images and often images
like Fig. 2-(A) are simply discarded.

3. Proposed Method

We begin with an overview of our approach followed by
speciﬁc implementation details. Our method is designed
with the additional constraints of fast execution and a small
memory overhead to make it suitable for incorporation as
a mobile app or as a software plugin. Alternative designs
and additional evaluation of parameters are provided in the
supplemental material.

3.1. Method Overview

Fig. 3 provides an overviews our framework. Given an
incorrectly white-balanced sRGB image, denoted as Iin, our
goal is to compute a mapping M that can transform the in-
put image’s colors to appear as if the WB was correctly ap-
plied.

(1)
t

, ..., I

Our method relies on a large set of n training images ex-
(n)
pressed as It = {I
t } that have been generated
using the incorrect WB settings. Each training image has
a corresponding correct white-balanced image (or ground
(i)
truth image), denoted as I
gt . Note that multiple training
images may share the same target ground truth image. Sec-
tion 3.2 details how we generated this dataset.

For each pair of training image I

and its ground truth
(i)
image I
gt , we compute a nonlinear color correction ma-
trix M(i) that maps the incorrect image’s colors to its target

(i)
t

1537

sRGB rendered images

(different white-balance settings and picture styles)

…

Ground truth

Captured  raw-RGB image + camera 

render settings

WB: Incandescent
Style: Landscape

…

White balance settings:

WB: Incandescent

Style: Portrait 

…

…

Auto 

Cloudy Shade

Incandescent

white balance
Picture styles:
Landscape, Vivid,  … Portrait

…

WB: Cloudy

Style: Landscape

WB: Manual

(Color chart white is reference)

Style: Adobe Standard

WB: Cloudy
Style: Portrait 

Figure 4. Example of rendering sRGB training images. Working
directly from the raw-RGB camera image, we render sRGB output
images using the camera’s pre-deﬁned white balance settings and
different picture styles. A target white balance sRGB image is also
rendered using the color rendition chart in the scene to provide the
ground truth.

ground truth image’s colors. The details of this mapping are
discussed in Section 3.3.

Given an input image, we search the training set to ﬁnd
images with similar color distributions. This image search
is performed using compact features derived from input and
training image histograms as described in Section 3.4. Fi-
nally, we obtain a color correction matrix M for our input
image by blending the associated color correction matrices
of the similar training image color distributions, denoted as
Ms. This is described in Section 3.5.

3.2. Dataset Generation

Our training images are generated from two pub-
the NUS
licly available illumination estimation datasets:
dataset [12] and the Gehler dataset [20].
Images in
these datasets were captured using digital single-lens re-
ﬂex (DSLR) cameras with a color rendition chart placed in
the scene that provides ground truth reference for illumina-
tion estimation. Since these datasets are intended for use
in illumination estimation, they were captured in raw-RGB
format. Because the images are in the camera’s raw-RGB
format, we can convert them to sRGB output emulating dif-
ferent WB settings and picture styles on the camera. To
do this, we use the Adobe Camera Raw feature in Photo-
shop to render different sRGB images using different WB
presets in the camera. In addition, each incorrect WB can
be rendered with different camera picture styles (e.g., Vivid,
Standard, Neutral, Landscape). Depending on the make and
model of the camera, a single raw-RGB image can be ren-
dered to more than 25 different camera-speciﬁc sRGB im-
ages. These images make up our training images {I
, ...,
(n)
t }.
I

(1)
t

To produce the correct target image, we manually select
the “ground truth” white from the middle gray patches in
the color rendition chart, followed by applying a camera-
independent rendering style—namely, Adobe Standard.

This provides the target ground truth sRGB image Igt. Fig.
4 illustrates an example of a raw-RGB image from the NUS
dataset and the corresponding sRGB images rendered with
different WB settings and picture styles.
In the end, we
generated 62,535 images from these datasets (there is an
additional set generated for cross-dataset validation using
the same approach; more details are given in Sec. 4.1).

3.3. Color Correction Transform

images representing an incorrect WB image I

After generating our training images, we have n pairs of
and its cor-
(i)
gt . These are represented as 3×N matrices,
rect WB image I
where N is the total number of pixels in the image and the
three rows represent the red, green, and blue values in the
camera’s output sRGB color space.

(i)
t

We can compute a color correction matrix M(i), which

(i)
maps I
t

to I

(i)
gt , by minimizing the following equation:

,

(2)

arg min

M(i) (cid:13)(cid:13)(cid:13)

M(i) Φ(cid:16)I

(i)

(i)

gt (cid:13)(cid:13)(cid:13)F
t (cid:17) − I

where k.kF is the Frobenius norm and Φ is a kernel function
that projects the sRGB triplet to a high-dimensional space.
We have examined several different color transformation
mappings and found the polynomial kernel function pro-
posed by Hong et al. [24] provided the best results for our
task (additional details are given in the supplemental mate-
rials). Based on [24], Φ:[R, G, B]T → [R, G, B, RG, RB,
GB, R2, G2, B2, RGB, 1]T and M(i) is represented as a
3 × 11 matrix. Note that spatial information is not consid-
ered when estimating the M(i).

3.4. Image Search

Since our color correction matrix is related to the image’s
color distribution, our criteria for ﬁnding similar images are
based on the color distribution. We also seek compact repre-
sentation as these features represent the bulk of information
that will need to be stored in memory.

Inspired by prior work [5, 6], we construct a histogram
feature from the log-chrominance space, which represents
the color distribution of an image I as an m×m×3 tensor
that is parameterized by uv. We refer to this as an RGB-uv
histogram. This histogram is generated by the function h(I)
described by the following equations:

R(i) + I2

G(i) + I2

B(i),

Iy(i) =qI2
Iu1(i) = log(cid:0)I
Iv1(i) = log(cid:0)I

R(i)(cid:1) − log(cid:0)I
R(i)(cid:1) − log(cid:0)I

G(i)(cid:1),
B(i)(cid:1),

Iu2 = −Iu1 , Iv2 = −Iu1 + Iv1,
Iu3 = −Iv1 , Iv3 = −Iv1 + Iu1.

(3)

H (I)(u,v,C) =Pi

Iy(i)(cid:2)(cid:12)(cid:12)

2 ∧(cid:12)(cid:12)
IuC(i) − u(cid:12)(cid:12) 6 ε

IvC(i) − v(cid:12)(cid:12) 6 ε

2(cid:3) , (4)

1538

Input image e Iin

3.5. Final Color Correction

Given a new input image, we compute its PCA feature
and search the training dataset for images with similar fea-
tures. We extract the set of color correction matrices Ms
associated with the k similar PCA features. The ﬁnal cor-
rection matrix M is then computed as a weighted linear
combination of the correction matrices Ms as follows:

…

Similar    training 

images(cid:1863)

M =

k

Xj=1

αjM(j)

s

,

(7)

where α is a weighting vector represented as a radial basis
function:

Training imageses It

αj =

, j ∈ [1, ..., k],

(8)

Figure 5. Visualization of the training images based on their corre-
sponding PCA feature vectors. In this ﬁgure t-SNE [32] is used to
aid visualization of the training space. Shown is an example input
image and several of the nearest images retrieved using the PCA
feature.

h(I)(u,v,C) =s

H (I)(u,v,C)

Pu′ Pv′ H (I)(u′ ,v′ ,C)

,

(5)

where i = {1, ..., N }, R, G, B represent the color channels
in I, C ∈ {1, 2, 3} represents each color channel in the
histogram, and ε is the histogram bin’s width. Taking the
square root after normalizing H increases the discrimina-
tory ability of our projected histogram feature [4, 5].

For the sake of efﬁciency, we apply a dimensionality re-
duction step in order to extract a compact feature represent-
ing each RGB-uv histogram. We found that the linear trans-
formation is adequate for our task to map the vectorized his-
togram vec(h (I)) ∈ Rm×m×3 to a new lower-dimensional
space. The principal component analysis (PCA) feature
vector is computed as follows:

v(I) = WT (vec (h (I)) − b) ,

(6)

where v (I) ∈ Rc is the PCA feature vector containing c
principal component (PC) coefﬁcients, c ≪ m × m × 3,
W = [w1, w2, ..., wc], w ∈ Rm×m×3 is the PC coefﬁcient
matrix computed by the singular value decomposition, and
b ∈ Rm×m×3 is the mean histogram vector. As a result, each
training image I
can be represented by a small number of

(i)
t

(i)

PC coefﬁcients v(cid:16)I

t (cid:17). The input image is ﬁnally repre-

sented by v(Iin). The L2 distance is used to measure the
similarity between the PCA feature vectors. Fig. 5 visual-
izes the training images based on their corresponding PCA
features.

j /2σ2(cid:1)
exp(cid:0)−d2
k′ =1 exp(cid:16)−d2
k′ /2σ2(cid:17)
Pk

where σ is the radial fall-off factor and d represents a vector
containing the L2 distance between the given input feature
and the similar k training features.

As shown in Fig. 3, the ﬁnal color transformation is
generated based on correction transformations associated
with training images taken from different cameras and ren-
der styles (see supplemental materials for a study of the ef-
fect of having different picture styles on the results). By
blending the mapping functions from images produced by
a wide range of different cameras and their different photo-
ﬁnishing styles, we can interpret M correction as mapping
the input image to a meta-camera’s output composed from
the most similar images to the input.

Lastly, the corrected image Icorr is produced by the fol-

lowing equation:

Icorr = M Φ (Iin) .

(9)

Since our training data includes examples of WB set-
tings that are close to the manual ground truth, our method
implicitly can also deal with test images that were rendered
with the correct WB settings (see supplemental materials
for examples).

3.6. Implementation Details

Our Matlab implementation requires approximately 0.54
seconds to compute the histogram feature. Once the PCA
histogram feature is computed, the correction process takes
an average of 0.73 seconds; this process includes the PCA
feature extraction, the brute-force search of the k nearest
neighbors, blending the correction matrix, and the ﬁnal im-
age correction. All the reported runtimes were computed
on an Intelr Xeonr E5-1607 @ 3.10 GHz machine and
for a 12 mega-pixel image. The accelerated GPU imple-
mentation runs on average in 0.12 seconds to correct a 12
mega-pixel image using GTX 1080 GPU.

1539

Our method requires 23.3 MB to store 62,535 feature
vectors, mapping matrices, the PCA coefﬁcient matrix, and
the mean histogram vector using single-precision ﬂoating-
point representation without affecting the accuracy.

In our implementation, each PCA feature vector was rep-
resented by 55 PC scores (i.e., c = 55), the PC coefﬁcient
matrix W was represented as a (60 × 60 × 3) × 55 matrix
(i.e., m = 60), and the mean vector b ∈ R60×60×3. We used
a fall-off factor σ = 0.25 and k = 25. Comparisons us-
ing different parameter values are given in the supplemental
materials.

4. Experimental Results

Our method is compared with common approaches that
are currently used to correct an improperly white-balanced
sRGB image. We ﬁrst describe the data used to test our
method (Sec. 4.1). Afterwards, we show both quantitative
(Sec. 4.2) and qualitative results (Sec. 4.3).

4.1. Dataset

As described in Sec. 3.2, we have generated a dataset of
62,535 images that contains pairs of incorrect and correct
(ground truth) images. An additional set has been gener-
ated using the same procedure containing 2,881 images.
Intrinsic set (Set 1): consists of 62,535 images generated
from the raw-RGB images provided in the NUS dataset
[12], its extension [13], and the Gehler dataset [20].
Extrinsic set (Set 2): is generated for cross-dataset valida-
tion, where it consists of 2,881 sRGB images rendered from
four mobile phones (iPhone 7, Google Pixel, LG G4, and
Galaxy S6 Edge) and one of the DSLR cameras (Olympus)
from the NUS dataset that was excluded from Set 1. Set 2
does not contain any cameras from Set 1. There are 1,874
DSLR images and 1,007 mobile phone images (468 of them
rendered from raw-RGB images provided by Karaimer and
Brown [9]).

We use Set 1 for training and evaluation, using three-fold
validation, such that the three folds are disjointed in regards
to the imaged scenes, meaning if the scene (i.e., original
raw-RGB image) appears in a fold, it is excluded from the
other folds. The color rendition chart is masked out in the
image and ignored during training and testing. For evalua-
tion on Set 2, we use the entire training from Set 1 of ∼62K.

4.2. Quantitative Results

We compared our results against a diagonal WB correc-
tion that is computed using the center gray patch in the color
checker chart placed in the scene. We refer to this as the ex-
act achromatic reference point, as it represents a true neu-
tral point found in the scene. This exact white point repre-
sents the best results that an illumination estimation algo-
rithm could achieve when applied to our input in order to

determine the diagonal WB matrix. This is equivalent to
the example in Fig. 2-(B).

For the sake of completeness, we also compared our
results against a “linearized” diagonal correction that ap-
plies an inverse gamma operation [3, 17], then performed
WB using the exact reference point, and then reapplied the
gamma to produce the result in the sRGB color space. We
also include results using Adobe Photoshop corrections—
speciﬁcally,
the auto-color function (AC) and auto-tone
function (AT).

We adopted three commonly used error metrics for the
evaluation, which are: (i) mean squared error (MSE), (ii)
mean angular error (MAE), and (iii) △E, which is widely
used to measure changes in visual perception between two
colors. There are different versions of △E; we report the
results of △E 2000 [37] (results of using △E 76 [36] are
also reported in the supplemental materials).
In Table 1,
the mean, lower quartile (Q1), median (Q2), and the upper
quartile (Q3) of the error between the corrected images and
the corresponding ground truth images are reported.

Table 1 shows that our proposed method consistently

outperforms the other approaches in all metrics.

4.3. Qualitative Results and User Study

As mentioned in Sec. 2, the inverse of the nonlinear
photo-ﬁnishing in an sRGB image can be performed by ap-
plying a full radiometric calibration [27] or from radiomet-
ric metadata that has been embedded in the sRGB rendered
image to restore the original raw-RGB image [34]. Fig. 6
shows a comparison between our result and applying diag-
onal WB correction to the reconstructed raw-RGB image
proposed in [34]. We are able to achieve comparable results
without the need for radiometric calibration.

Qualitative visual results for Set 1 and Set 2 are shown
in Fig. 7.
It is arguable that our results are the most vi-
sually similar to the ground truth images. To conﬁrm this
independently, we have conducted a user study of 35 partic-
ipants (18 males and 17 females), ranging in age from 21 to
46. Each one was asked to choose the most visually simi-
lar image to the ground truth image between the results of
our method and the diagonal correction with the exact ref-
erence point. Experiments were carried out in a controlled
environment. The monitor was calibrated using a Spyder5
colorimeter. Participants were asked to compare 24 pairs
of images, such that for each quartile, based on the MSE
of each method, 4 images were randomly picked from Set
1 and Set 2. That means the selected images represent the
best, median, and worst results of each method and for each
set. On average, 93.69% of our results were chosen as the
most similar to the ground truth images. Fig. 8 illustrates
that the results of this study are statistically signiﬁcant with
p-value < 0.01.

We note that our algorithm does fail on certain types of

1540

(cid:959)E= 10.82

(cid:959)E= 6.52

(cid:959)E= 5.92

(A) Input image

(B) Diagonal correction using the 

sRGB image

(C) Diagonal correction using the 

reconstructed raw-RGB

(D) Ours

(E) Ground truth

Figure 6. Comparison of our results against applying white balance to the reconstructed raw-RGB image as proposed by [34]. The work
in [34] is able to reconstruct the raw-RGB image by embedding radiometric calibration metadata in the sRGB data. By reversing back
to the raw-RGB, they can re-apply white balance correctly and then re-render the sRGB image. (A) Input sRGB image. (B) Result of
diagonal correction on sRGB color space. (C) Result of diagonal correction on the reconstructed raw-RGB image [34]. (D) Our result. (E)
Ground truth image. (A)-(C) and (E) are adapted from [34].

Set1/DSLR

Set1/DSLR

Set1/DSLR

Set1/DSLR

Set2 /DSLR

AC

AC

AC

AT

AT

(cid:959)E= 21.92

LRGB

(cid:959)E= 9.33

(cid:959)E= 6.59

LRGB

(cid:959)E= 15.04

LRGB

(cid:959)E= 17.36

sRGB

(cid:959)E= 13.57

LRGB

(cid:959)E= 5.98

(cid:959)E= 6.04

(cid:959)E= 7.66

(cid:959)E= 7.09

Set2/Mobile

AC

(cid:959)E= 13.78

LRGB

(cid:959)E= 11.56

(cid:959)E= 3.58

(cid:959)E= 3.55

(cid:959)E= 2.27

(cid:959)E= 3.18

(cid:959)E= 3.87

(cid:959)E= 5.68

(A) Input image

(B) Ps correction

(C) Diagonal correction

(D) Ours

(E) Ground truth

Figure 7. Comparisons between the proposed approach and other techniques on Set 1 (ﬁrst four rows) and Set 2 (last two rows). (A)
Input image in sRGB. (B) Results of Adobe Photoshop (Ps) color correction functions. (C) Results of diagonal correction using the exact
reference point obtained directly from the color chart. (D) Our results. (E) Ground truth images. In (B) and (C), we pick the best result
between the auto-color (AC) and auto-tone (AT) functions and between the sRGB (sRGB) and “linearized” sRGB (LRGB) [3, 17] based
on △E values, respectively.

1541

Table 1. Comparisons between our method with existing solutions for white balancing sRGB images. We compare our results against the
diagonal white balance correction using an exact achromatic reference point obtained from the color chart in the image. The diagonal
correction is applied directly to the sRGB images, denoted as (sRGB) and on the “linearized” sRGB [3, 17], denoted as (LRGB). Also,
we compare our results against the Adobe Photoshop functions: auto-color (AC) and auto-tone (AT). The terms Q1, Q2, and Q3 denote
the ﬁrst, second (median), and third quartile, respectively. The terms MSE and MAE stand for mean square error and mean angular error,
respectively. The top results are indicated with yellow and bold.

MSE

MAE

△E

Method

Photoshop-AC
Photoshop-AT
Diagonal WB (sRGB)
Diagonal WB (LRGB)
Ours

Mean

780.52
1002.93
135.77
130.01
77.79

Q2

Q1

Q3

Q2

Q1

Mean

Q3
Intrinsic set (Set 1): DSLR multiple cameras (62,535 images)
157.39
238.33
20.20
19.73
13.74

991.28
1245.51
196.15
183.65
94.01

430.96
606.74
71.74
68.54
39.62

7.96°
7.56°
4.63°
4.29°
3.06°

3.43°
3.08°
1.99°
1.85°
1.74°

5.59°
5.75°
3.56°
3.35°
2.54°

10.58°
10.83°
6.14°
5.70°
3.76°

Extrinsic set (Set 2): DSLR and mobile phone cameras (2,881 images)

Photoshop-AC
Photoshop-AT
Diagonal WB (sRGB)
Diagonal WB (LRGB)
Ours

745.49
953.85
422.31
385.23
171.09

240.58
386.7
110.70
99.05
37.04

514.33
743.84
257.76
230.86
87.04

968.27
1256.94
526.16
475.72
190.88

10.19°
11.91°
7.99°
7.22°
4.48°

5.25°
7.01°
4.36°
3.80°
2.26°

8.60°
10.70°
7.11°
6.34°
3.64°

14.13°
15.92°
10.57°
9.54°
5.95°

Mean

Q1

Q2

Q3

10.06
11.12
4.69
4.59
3.58

11.71
13.12
8.53
8.15
5.60

5.75
6.55
2.25
2.24
2.07

7.56
9.63
5.52
5.07
3.43

8.92
10.54
4.00
3.89
3.09

11.41
13.18
8.38
7.88
4.90

13.30
14.68
6.68
6.51
4.55

15.00
16.5
11.11
10.68
7.06

)

%

(
e
c
o
h
c

i

 

r
e
s
u
e
g
a
r
e
v
A

100

90

80

70

60

50

40

30

20

10

0

)

%

(
e
c
o
h
c

i

 

r
e
s
u
e
g
a
r
e
v
A

Ours

Diagonal WB

(A) Overall results

Lower quartile (Q1)

Interquartile range (Q2)

Upper quartile (Q3)

100

90

80

70

60

50

40

30

20

10

0

)

%

(
e
c
o
h
c

i

 

r
e
s
u
e
g
a
r
e
v
A

Ours

Diagonal WB

100

90

80

70

60

50

40

30

20

10

0

)

%

(
e
c
o
h
c

i

 

r
e
s
u
e
g
a
r
e
v
A

Ours

Diagonal WB

(B) Results of 3-quartiles

100

90

80

70

60

50

40

30

20

10

0

Ours

Diagonal WB

Figure 8. The results of a user study with 35 people in which users are asked which output is most visually similar to the ground truth
image. An equal number of images are selected randomly from the different quartiles. The outcome of the user study is shown via interval
plots, with error bars shown at a 99% conﬁdence interval. (A) shows the overall user preference of all results, while (B) shows the results
categorized by Q1, Q2, and Q3.

inputs. These are generally images with strong color casts
that have a large number of saturated colors (see Fig. 9).

5. Concluding Remarks

White balance is a critical low-level computer vision
step that is often taken for granted. Most computer vi-
sion datasets, especially those composed of images crawled
from the web, are implicitly biased towards correctly white-
balanced images as improperly white-balanced images are
rarely uploaded in the ﬁrst place. As discussed in this pa-
per, correcting an sRGB image that has been rendered by a
camera with the wrong WB setting is challenging.

This paper has proposed a data-driven method to correct
improperly white-balanced images. Extensive quantitative
and qualitative experiments, including a user study, demon-
strate the effectiveness of this approach. More importantly,
our approach is practical, requiring less than 24 MB of data
and less than 1.5 seconds to correct a full-resolution image.

In addition, our approach generalizes well to images not
contained within our training set. We believe this dataset,
the only one of its kind, will serve as a useful resource for
future work on this and related topics.

Acknowledgment This study was funded in part by the
Canada First Research Excellence Fund for the Vision: Sci-
ence to Applications (VISTA) programme, an NSERC Dis-
covery Grant, and an Adobe Gift Award.

(cid:959)E= 14.49

(cid:959)E= 18.83

(A) Input image

(B) Diagonal WB 

correction

(C) Ours

(D) Ground truth

Figure 9. Failure example of the proposed method.
(A) Input
sRGB image. (B) Results using a diagonal white balance correc-
tion based on the exact achromatic reference obtained from the
color chart. (C) Our result. (D) Ground truth image.

1542

References

[1] Mathworks:

Adjust

color balance of RGB image
https:

with chromatic adaptation (chromadapt).
//www.mathworks.com/help/images/ref/
chromadapt.html. Accessed: 2019-04-05.

[2] Casper Find Andersen and David Connah. Weighted con-
strained hue-plane preserving camera characterization. IEEE
Transactions on Image Processing, 25(9):4329–4339, 2016.

[3] Matthew Anderson, Ricardo Motta, Srinivasan Chan-
drasekar, and Michael Stokes. Proposal for a standard default
color space for the Internet—sRGB. In Color and Imaging
Conference, 1996.

[4] Relja Arandjelovic and Andrew Zisserman. Three things ev-
eryone should know to improve object retrieval. In CVPR,
2012.

[5] Jonathan T Barron. Convolutional color constancy. In ICCV,

2015.

[6] Jonathan T Barron and Yun-Ta Tsai. Fast fourier color con-

stancy. In CVPR, 2017.

[7] Roy S Berns and M James Shyu. Colorimetric character-
ization of a desktop drum scanner using a spectral model.
Journal of Electronic Imaging, 4(4):360–373, 1995.

[8] Simone Bianco and Raimondo Schettini. Adaptive color
constancy using faces. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 36(8):1505–1518, 2014.

[9] Michael S Brown and Hakki Can Karaimer. Improving color

reproduction accuracy on cameras. In CVPR, 2018.

[10] Gershon Buchsbaum. A spatial processor model for ob-
Journal of the Franklin Institute,

ject colour perception.
310(1):1–26, 1980.

[11] A. Chakrabarti, Ying Xiong, Baochen Sun, T. Darrell, D.
Scharstein, T. Zickler, and K. Saenko. Modeling radiometric
uncertainty for vision with tone-mapped color images. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
36(11):2185–2198, 2014.

[12] Dongliang Cheng, Dilip K Prasad, and Michael S Brown.
Illuminant estimation for color constancy: Why spatial-
domain methods work and the role of the color distribution.
Journal of the Optical Society of America A, 31(5):1049–
1058, 2014.

[13] Dongliang Cheng, Brian Price, Scott Cohen, and Michael S
Brown. Beyond white: Ground truth colors for color con-
stancy correction. In ICCV, 2015.

[14] Brad Dayley and DaNae Dayley. Adobe Photoshop CS6

Bible. John Wiley & Sons, 2012.

[15] Paul E. Debevec and Jitendra Malik. Recovering high dy-
In SIG-

namic range radiance maps from photographs.
GRAPH, 1997.

[16] Mark S Drew and Brian V Funt. Natural metamers. CVGIP:

Image Understanding, 56(2):139–151, 1992.

[17] Marc Ebner. Color Constancy. John Wiley & Sons, 2007.

[18] Graham Finlayson, Han Gong, and Robert B Fisher. Color
homography: Theory and applications. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 41(1):20–33,
2019.

[19] Graham D Finlayson, Michal Mackiewicz, and Anya Hurl-
bert. Color correction using root-polynomial regression.
IEEE Transactions on Image Processing, 24(5):1460–1470,
2015.

[20] Peter Vincent Gehler, Carsten Rother, Andrew Blake, Tom
Minka, and Toby Sharp. Bayesian color constancy revisited.
In CVPR, 2008.

[21] Arjan Gijsenij, Theo Gevers, and Joost Van De Weijer. Com-
putational color constancy: Survey and experiments. IEEE
Transactions on Image Processing, 20(9):2475–2489, 2011.
[22] Arjan Gijsenij, Theo Gevers, and Joost Van De Weijer. Im-
proving color constancy by photometric edge weighting.
IEEE Transactions on Pattern Analysis and Machine Intel-
ligence, 34(5):918–929, 2012.

[23] M.D. Grossberg and S.K. Nayar. What is the space of camera

response functions? In CVPR, 2003.

[24] Guowei Hong, M Ronnier Luo, and Peter A Rhodes. A
study of digital camera colorimetric characterisation based
on polynomial modelling. Color Research & Application,
26(1):76–84, 2001.

[25] Yuanming Hu, Baoyuan Wang, and Stephen Lin. FC4: Fully
convolutional color constancy with conﬁdence-weighted
pooling. In CVPR, 2017.

[26] Hakki Can Karaimer and Michael S Brown. A software
platform for manipulating the camera imaging pipeline. In
ECCV, 2016.

[27] Seon Joo Kim, Hai Ting Lin, Zheng Lu, S. S¨usstrunk, S.
Lin, and M. S. Brown. A new in-camera imaging model for
color computer vision and its application. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 34(12):2289–
2302, 2012.

[28] Seon Joo Kim and M. Pollefeys. Robust radiometric calibra-
tion and vignetting correction. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 30(4):562–576, 2008.

[29] Haiting Lin, Seon Joo Kim, S. S¨usstrunk, and M. S. Brown.
Revisiting radiometric calibration for color computer vision.
In ICCV, 2011.

[30] S. Lin, Jinwei Gu, S. Yamazaki, and Heung-Yeung Shum.
Radiometric calibration from a single image. In CVPR, 2004.
[31] S. Lin and L. Zhang. Determining the radiometric response

function from a single grayscale image. In CVPR, 2005.

[32] Laurens van der Maaten and Geoffrey Hinton. Visualizing
data using t-SNE. Journal of Machine Learning Research,
9:2579–2605, 2008.

[33] Seonghyeon Nam and Seon Joo Kim. Modelling the scene
dependent imaging in cameras with a deep neural network.
In ICCV, 2017.

[34] Rang M. H. Nguyen and Michael S. Brown. Raw image re-
construction using a self-contained sRGB–JPEG image with
small memory overhead. International Journal of Computer
Vision, 126(6):637–650, 2018.

[35] Seoung Wug Oh and Seon Joo Kim. Approaching the
computational color constancy as a classiﬁcation problem
through deep learning. Pattern Recognition, 61:405–416,
2017.

[36] Gaurav Sharma and Raja Bala. Digital Color Imaging Hand-

book. CRC press, 2017.

1543

[37] Gaurav Sharma, Wencheng Wu,

and Edul N Dalal.
The CIEDE2000 color-difference formula: Implementation
notes, supplementary test data, and mathematical observa-
tions. Color Research & Application, 30(1):21–30, 2005.

[38] Ying Xiong, K. Saenko, T. Darrell, and T. Zickler. From
pixels to physics: Probabilistic color de-rendering. In CVPR,
2012.

1544

