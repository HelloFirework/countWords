Detection based Defense against Adversarial Examples from the Steganalysis

Point of View

Jiayang Liu1

Weiming Zhang2

Yiwei Zhang1

Dongdong Hou1

Yujia Liu1
Nenghai Yu2
University of Science and Technology of China

Hongyue Zha1

1{ljyljy,zywvvd,houdd,yjcaihon,zha00}@mail.ustc.edu.cn, 2{zhangwm,ynh}@ustc.edu.cn

Abstract

Deep Neural Networks (DNNs) have recently led to sig-
niﬁcant improvements in many ﬁelds. However, DNNs are
vulnerable to adversarial examples which are samples with
imperceptible perturbations while dramatically misleading
the DNNs. Moreover, adversarial examples can be used
to perform an attack on various kinds of DNN based sys-
tems, even if the adversary has no access to the underlying
model. Many defense methods have been proposed, such
as obfuscating gradients of the networks or detecting ad-
versarial examples. However it is proved out that these de-
fense methods are not effective or cannot resist secondary
adversarial attacks.
In this paper, we point out that ste-
ganalysis can be applied to adversarial examples detection,
and propose a method to enhance steganalysis features by
estimating the probability of modiﬁcations caused by adver-
sarial attacks. Experimental results show that the proposed
method can accurately detect adversarial examples. More-
over, secondary adversarial attacks are hard to be directly
performed to our method because our method is not based
on a neural network but based on high-dimensional artiﬁ-
cial features and Fisher Linear Discriminant ensemble.

1. Introduction

Deep Neural Networks (DNNs) have recently led to sig-
niﬁcant improvements in many ﬁelds, such as image classi-
ﬁcation [28, 15] and speech recognition [1]. However, the
generalization properties of the DNNs have been recently
questioned because these machine learning models are vul-
nerable to adversarial examples [31]. An adversarial exam-
ple is a slightly modiﬁed sample that is intended to cause
an error output of the DNN based model.
In the context
of classiﬁcation task, the adversarial example is crafted to
force a model to classify it into a class different from the le-
gitimate class. In addition, adversarial examples have cross-
model generalization property [12], so the attacker can even

generate adversarial examples without the knowledge of the
DNN. Adversarial attacks are divided into two types: tar-
geted attack and untargeted attack. In targeted attack, the
attacker generates adversarial examples which are misclas-
siﬁed by the classiﬁer into a particular class. In untargeted
attack, the attacker generates adversarial examples which
are misclassiﬁed by the classiﬁer into any class as long as it
is different from the true class.

There are many studies which focus on methods of gen-
erating adversarial examples. Some attack methods are
based on calculating the gradient of the network, such as
Fast Gradient Sign Method (FGSM) [12], Basic Iterative
Method (BIM) [18] and Jacobian Saliency Map Attack
Method [25]. While other methods are based on solving op-
timization problems, such as L-BFGS [31], Deepfool [23]
and Carlini & Wagner (C&W) attack [6].

Many defenses are proposed to mitigate adversarial ex-
amples against the above attacks. They make it harder for
the adversary to craft adversarial examples using existing
techniques or make the DNNs still give correct classiﬁca-
tions on adversarial examples. These defenses are mainly
divided into two categories.

One way is preprocessing the input image before classi-
ﬁcation, taking advantage of the spatial instability of adver-
sarial examples. Defenders can perform some operations
on the input image in spatial domain before giving the in-
put image to a DNN, such as JPEG compression, scaling,
adding noise, etc. Gu et al. [14] propose to use an autoen-
coder to remove adversarial perturbations from inputs.

The other way is to modify the network architecture, the
optimization techniques or the training process. Goodfel-
low et al.
[12] propose to augment the training set with
adversarial examples to increase the model’s robustness a-
gainst a speciﬁc adversarial attack. However, this approach
faces difﬁculties because the dimension of the images and
features in networks means a great quantity of training data
is required. Defensive distillation [26] is another technique
against certain adversarial attacks. This form of network
can prevent the model from ﬁtting too tightly to the original

4825

data. Unfortunately, most of these defenses are not very ef-
fective against adversarial examples in classiﬁcation tasks.

Obfuscated gradients appear to be robust against adver-
sarial attacks. Obfuscated gradients can be deﬁned as a spe-
cial case of gradient masking [24], in which the attackers
cannot compute out the feasible gradient to generate adver-
sarial examples. However, Athalye et al. [2] proposed at-
tack techniques to overcome the obfuscated gradient based
defenses.

Due to the difﬁculty of classifying adversarial exam-
ples correctly, recent work has turned to detecting them.
Hendrycks & Gimpel [16], Li et al.
[19] and Bhagoji et
al. [3] use PCA to detect statistical properties of the images
or network parameters. Feinman et al. [9] perform another
statistical test to detect adversarial examples. Grosse et al.
[13], Gong et al. [11] and Metzen et al. [22] utilize a second
neural network to classify images as normal or adversarial.
Lu et al. [21] detect adversarial examples by hypothesiz-
ing that adversarial examples produce different patterns of
ReLU activations in networks than what is produced by nor-
mal images. Xu et al. [32] propose a method, called Feature
Squeezing, to detect adversarial examples by measuring the
disagreement among the prediction vectors of the original
and squeezed examples. Liang et al. [20] detect the adver-
sarial example by comparing the classiﬁcation results of the
input and its denoised version.

Unfortunately, Carlini and Wagner perform experiments
to prove that most of these detecting methods are only ef-
fective on image datasets with small size or only several
classes [5]. Moreover, Grosse’s [13], Gong’s [11] and Met-
zen’s [22] defenses use a second neural network to classify
images as normal or adversarial. However, neural network-
s used for detecting adversarial examples can also be by-
passed [5]. In fact, given an adversarial method can fool the
original neural network, Carlini et al. [5] show that with a
similar method we can also fool the extended network for
detection, which we call secondary adversarial attacks.

In this paper we propose to detect adversarial examples
from the view of steganalysis [27] which is the technolo-
gy for detecting steganography. In fact, Goodfellow et al.
[12] have provided the insight on one essence of adversari-
al examples such that “the adversarial attack can be treated
as a sort of accidental steganography”. Furthermore, we
propose a method to enhance steganalysis features by es-
timating the probability of modiﬁcations caused by adver-
sarial attacks. Experimental results show that the proposed
method can accurately detect adversarial examples. More-
over, secondary adversarial attacks are hard to be directly
performed to our method because our method is not based
on a neural network but based on high-dimensional artiﬁcial
features and FLD (Fisher Linear Discriminant) ensemble.

2. Related Work

2.1. Adversarial Attacks

2.1.1 Fast Gradient Sign Method

Goodfellow et al.
[12] propose the Fast Gradient Sign
Method (FGSM) for generating adversarial examples. This
method uses the derivative of the loss function of the net-
work pertaining to the input feature vector. Given the input
image X, FGSM is to perturb the gradient direction of each
feature by the gradient. Then the classiﬁcation result of the
input image will be changed. For a neural network with
cross-entropy cost function J(X, y) where X is the input
image and yt is the target class for the input image, the ad-
versarial example is generated as

X adv = X − ǫsign(∇X J(X, yt)),

(1)

where ǫ is a parameter to determine the perturbation size.

2.1.2 Basic Iterative Method

The Basic Iterative Method (BIM) is the iterative version
of FGSM. This method applies FGSM many times with s-
mall perturbation size instead of applying adversarial noise
with one large perturbation size. The adversarial example
of BIM is generated as

X adv

N +1 = ClipX,ǫ{X adv

0 = X,

X adv
N −αsign(∇X J(X adv

N , yt))},

(2)

where ClipX,ǫ{X ′} represents a clipping of the pixel val-
So the results stay in the ǫ-
ues after each iteration.
neighbourhood of the input image X. This attack is more
powerful because the attacker can control how far the ad-
versarial example past the classiﬁcation boundary. It was
demonstrated that the attack of BIM was better than FGSM
on ImageNet [18].

2.1.3 Deepfool

Deepfool is an untargeted attack method to generate an ad-
versarial example by iteratively perturbing an image [23].
This method explores the nearest decision boundary. The
image is modiﬁed a little to reach the boundary in each iter-
ation. The algorithm stops once the modiﬁed image changes
the classiﬁcation of the network.

2.1.4 Carlini & Wagner Method

This method is named after its authors [6]. The attack can
be targeted or untargeted, and has three metrics to measure
its distortion (l0 norm, l2 norm and l∞ norm). The authors
point out that the untargeted l2 norm version has the best

4826

performance. It generates adversarial examples by solving
the following optimization problem:

minimize

δ

kδk2 + c · f (x + δ)

s.t. x+δ ∈ [0, 1]n

(3)

This attack is to look for the smallest perturbation mea-
sured by l2 norm and make the network classify the image
incorrectly at the same time. c is a hyperparameter to bal-
ance the two parts of equation (3). The best way to choose
c is to use the smallest value of c for which the resulting
solution x + δ has f (x + δ) ≤ 0. f (x) is the loss func-
tion to measure the distance between the input image and
the adversarial image. f (x) is deﬁned as:

f (x) = max(Z(x)true − max
i6=true

{Z(x)i}, −κ).

(4)

Z(x) is the pre-softmax classiﬁcation result vector. κ is
a hyper-parameter called conﬁdence. Higher conﬁdence en-
courages the attack to search for adversarial examples that
are stronger in classiﬁcation conﬁdence. High-conﬁdence
attacks often have larger perturbations and better transfer-
ability to other models. The C&W method is a strong attack
which is difﬁcult to defend.

2.2. Robustness Based Defense

Robustness based defense aims at classifying adversari-
al examples correctly. There are many methods to achieve
robustness based defense. Adversarial training is to train a
better network by using a mixture of normal and adversarial
examples in the training set for data augmentation [12]. Pre-
processing the input images is to perform some operations
to remove adversarial perturbations, such as principal com-
ponent analysis (PCA) [3], JPEG compression [7], adding
noise, cropping, rotating and so on. Defensive distillation
hides the gradient between the pre-softmax layer and soft-
max outputs by leveraging distillation training techniques
[26]. Obfuscated gradients make the attackers hard to com-
pute out the feasible gradient to generate adversarial exam-
ples [2].

2.3. Detection Based Defense

Detection based defense aims at distinguishing normal

images and adversarial examples.

Hendrycks & Gimpel [16] leverage PCA to detect ad-
versarial examples, ﬁnding that adversarial examples place
a higher weight on the larger principal components than
normal images. However, Carlini and Wagner prove that
Hendrycks’s defense is only effective on MNIST [5].

Li et al. [19] apply PCA to the values after inner con-
volutional layers of the neural network, and use a cascade
classiﬁer to detect adversarial examples. Speciﬁcally, they
propose building a cascade classiﬁer that accepts the input

as normal only if all classiﬁers accept the input, but reject-
s it if any do. However, Carlini and Wagner perform ex-
periments to prove that Li’s defense fails against the C&W
attack [5].

Grosse et al. [13] propose a variant on adversarial re-
training.
Instead of attempting to classify the adversarial
examples correctly, they introduce an additional class, sole-
ly for adversarial examples, and retrain the network to clas-
sify adversarial examples as the new class. Gong et al. [11]
propose a very similar defense method. However, Carlini
and Wagner re-implement these two defenses and ﬁnd that
they are only effective on MNIST [5].

Bhagoji et al. [3] leverage PCA to reduce the dimension-
ality of the images. Then instead of training on the original
images, they train a classiﬁer on images which have been
processed with dimensionality reduction. However, this de-
fense is only effective on MNIST [5].

Feinman et al. [9] utilize a Gaussian Mixture Model to
model outputs from the ﬁnal hidden layer of a neural net-
work, and claim that adversarial examples belong to a d-
ifferent distribution than that of normal images. However,
Carlini and Wagner prove that Feinman’s defense is only
effective on MNIST [5].

Metzen et al. [22] detect adversarial examples by look-
ing at the inner convolutional layers of the network. They
augment the classiﬁcation neural network with a detection
neural network that takes its input from various intermediate
layers of the classiﬁcation network. However, this defense
is only effective on CIFAR-10 [5].

Lu et al. [21] hypothesize that adversarial examples pro-
duce different patterns of ReLU activations in networks than
what is produced by normal images. Based on this hypoth-
esis, they propose the Radial Basis Function SVM (RBF-
SVM) classiﬁer which takes advantage of discrete codes
computed by the late stage ReLUs of the network to detect
adversarial examples on CIFAR-10 and ImageNet.

Xu et al. [32] propose a method, called Feature Squeez-
ing (FS), to detect adversarial examples. They reduce the
color bit depth of each pixel and smooth it by a spatial ﬁlter
to squeeze the features of an image. Then the adversari-
al examples are identiﬁed by measuring the disagreement
among the prediction vectors of the original and squeezed
examples.

Liang et al. [20] regard the adversarial perturbation as
a kind of noise and use scalar quantization and smoothing
spatial ﬁlter to reduce its adversarial effect. Then the adver-
sarial example can be detected by comparing the classiﬁca-
tion results of the input and its denoised version. We refer
to this method as Noise Reduction (NR).

For practical applications, we can deploy detection based
defense combing with robustness based defense. First of all,
we use detection based defense to detect the input image. If
it is a normal image, we will directly feed it to the original

4827

DNN. Otherwise we can take advantage of robustness based
defense to mitigate adversarial examples.

3. Proposed Method

Both adversarial attacks and steganography on images
make perturbations on the pixel values, which alter the de-
pendence between pixels. However, steganalysis can ef-
fectively detect modiﬁcations caused by steganography via
modeling the dependence between adjacent pixels in natu-
ral images. So we can also take advantage of steganalysis
to identify deviations due to adversarial attacks.

Assuming that we have known the attacking method used
by the attacker, we construct a detector to detect whether
the input image is an adversarial example or not. In prac-
tice, we don’t know the method used by the attacker, but we
can deploy a series of detectors trained for various main-
stream adversarial attacks. Our detection method exploits
the fact that the perturbation of pixel values by adversarial
attack alters the dependence between pixels. By modeling
the differences between adjacent pixels in natural images,
we can identify deviations due to adversarial attacks.
In
the beginning, we use a ﬁlter to suppress the content of the
input image. Dependence between adjacent pixels of the
ﬁltered image is modeled as a higher order Markov chain
[30]. Then the transition probability matrix is used as a vec-
tor feature for a feature based detector implemented using
machine learning algorithms.

We recommend two kinds of steganalysis feature sets for
detecting adversarial examples: one is the low-dimensional
model SPAM with 686 features [27]; the other is the high-
dimensional model Spatial Rich Model (SRM) with 34671
features [10].

3.1. Features Extraction

3.1.1 SPAM

SPAM is described as follows. First, we calculate the
transition probabilities between pixels in eight direction-
s {←, →, ↓, ↑, տ, ց, ւ, ր} in the spatial domain. We
always compute the differences and the transition proba-
bility along the same direction. For example, the hori-
zontal direction from left-to-right differences are calculat-
ed by A→
i,j = Xi,j − Xi,j+1, where X is an image with
size of m × n, and Xi,j is the pixel at the position (i, j)
for i ∈ {1, . . . , m}, j ∈ {1, . . . , n − 1}. Second, we
use a Markov chain between pairs of differences (ﬁrst or-
der chain) or triplets (second order chain) to model pixel
dependence along the eight directions. The ﬁrst-order de-
tecting features, F 1st, model the difference arrays A via
a ﬁrst-order Markov process. For the horizontal direction,
this leads to

where x, y ∈ {−T, . . . , T }. The second-order detecting
features, F 2nd, model the difference arrays A via a second-
order Markov process. For the horizontal direction, this
leads to

M →

x,y,z = P (A→

i,j+2 = x|A→

i,j+1 = y, A→

i,j = z)

(6)

where x, y, z ∈ {−T, . . . , T }. To reduce the dimension-
ality of the transition probability matrix, we only consider
differences within a limited range. Thus, we just calculate
the transition probability matrix for pairs within [−T, T ].
We separately average the horizontal and vertical matrices
and then the diagonal matrices to form the ﬁnal feature sets,
F 1st, F 2nd. The expression of the average sample Markov
transition probability matrices is

F1,...,k = (M → + M ← + M ↑ + M ↓)/4

Fk+1,...,2k = (M ր + M ւ + M ց + M տ)/4

(7)

where k = (2T + 1)2 for the ﬁrst-order detecting features
and k = (2T + 1)3 for the second-order detecting features.
We can see that the order of Markov model and the range
of differences T control the dimensionality of our detecting
model. We use T = 3 for second order, resulting in 2k =
686 features [27].

3.1.2 Spatial Rich Model

Spatial Rich Model (SRM) can be viewed as an extend-
ed version of SPAM by extracting residuals from images
[10]. We use a pixel predictor from the pixel’s immediate
neighborhood to obtain a residual which is an estimate of
the image noise component. SRM uses 45 different pixel
predictors. The pixel predictor is linear or non-linear. Each
linear predictor is a shift-invariant ﬁnite-impulse response
ﬁlter which is described by a kernel matrix K (pred). The
residual Z = (zkl) is a matrix which has the same dimen-
sion as X:

Z = K (pred) ∗ X − X ∆= K ∗ X,

(8)

where the symbol ∗ denotes the convolution with X mirror-
padded. Thus, K ∗ X has the same dimension as X.

For example, one simple linear residual

is zij =
Xi,j+1 − Xi,j , which is the difference between a pair of
horizontally adjacent pixels. In this case, the residual ker-
nel is K = (cid:0) −1 1 (cid:1), which means that the pixel value is
predicted as its horizontally neighboring pixel.
SRM obtains non-linear predictors by taking the mini-
mum or maximum of up to ﬁve residuals which are obtained
by using linear predictors. For example, we can predict
pixel Xi,j from its horizontal or vertical neighboring pix-
els, obtaining thus one horizontal and one vertical residual

Z (h) = (cid:0)zh

ij(cid:1), Z (v) = (cid:0)zv

ij(cid:1):

M →

x,y = P r(A→

i,j+1 = x|A→

i,j = y)

(5)

z(h)
ij = Xi,j+1 − Xi,j,

(9)

4828

z(v)
ij = Xi+1,j − Xi,j.

(10)

We can use these two residuals to compute two nonlinear

“minmax” residuals as:

z(min)

ij

z(max)

ij

= minnz(h)

ij , z(v)

ij o ,

= maxnz(h)

ij , z(v)

ij o .

(11)

(12)

After that, quantize Z with a quantizer Q−T ,T with
centroids Q−T ,T = {−T q, (−T + 1) q, . . . , T q}, where
T > 0 is an integer threshold and q > 0 is a quantization
step:

rij

∆= Q−T ,T (zij) , ∀i, j.

(13)

The next step is that a co-occurrence matrix of fourth or-
der, C (SRM ) ∈ Q4
−T ,T , is computed from four (horizontal-
ly and vertically) adjacent values of the quantized residual
rij from the entire image:

C SRM

d0d1d2d3 =

m,n−3

X

i,j=1

[ri,j = dk, ∀k = 0, . . . , 3],

(14)

where dk ∈ Q−T ,T and [B] is the Iverson bracket, which is
1 if the statement B is true and 0 otherwise. The union of
all co-occurrence matrices has a total dimension of 34671.

3.2. Features Enhancement

The above methods of extracting steganalysis features
do not consider the location of modiﬁed pixels caused by
adversarial attacks. Obviously, the detection rate will be
improved if we assign larger weight to the features of the
modiﬁed location. Although we cannot obtain the accurate
modiﬁed location, we can estimate the relative modiﬁca-
tion probability of each pixel. In order to further improve
the accuracy of detection, we propose to enhance steganal-
ysis features by estimating the probability of modiﬁcations
caused by adversarial attacks.

We take advantage of the gradient amplitude to estimate
the modiﬁcation probability because the pixels with larger
gradient amplitude have larger probability to be modiﬁed.
Assume that the neural network divides images into N cat-
egories. Although we cannot know which target class will
be selected by the attacker, we can randomly select L cate-
gories to generate L targeted adversarial examples and then
estimate the modiﬁcation probability of each pixel accord-
ing to these targeted adversarial examples. So we take the
t-th (1 ≤ t ≤ L) class as the target class to calculate the
gradient of the input image X. We refer to the matrix of all
pixels’ modiﬁcation probabilities as Modiﬁcation Probabil-
ity Map (MPM). Note that these targeted adversarial exam-
ples generated by us are only used to estimate MPM which
can be used to enhance the detection of adversarial attacks.

For FGSM and BIM, when generating the adversar-
ial example of the target class yt for the input image,
we save absolute values of the gradient of each pixel
|∇X J(X, yt)|, and then normalize them to obtain the gradi-
ent map fnor (|∇X J(X, yt)|) where fnor () is the function
to normalize all elements in the matrix to (0, 1). Finally,
calculate the mean value of the gradient maps of L adver-
sarial examples to get MPM P :

P =

1
L

L

X

t=1

fnor (|∇X J(X, yt)|),

(15)

where P is a m × n matrix in which the element Pi,j is the
modiﬁcation probability of the pixel Xi,j .

For C&W which does not generate adversarial examples
by gradient, the estimation of MPM starts by computing the
difference array Dt between the normal image X and the
adversarial example X adv

:

t

Dt = X adv

t − X.

(16)

Then save the absolute values of all elements in the differ-
ence array Dt and normalize them to obtain the difference
map fnor (|Dt|). Finally, calculate the mean value of the
difference maps of L adversarial examples to get MPM:

P =

1
L

L

X

t=1

fnor (|Dt|).

(17)

For Deepfool which can only generate untargeted adver-
sarial examples, we estimate MPM by computing the dif-
ference array D between the normal image X and the ad-
versarial example X adv:

D = X adv − X.

(18)

Then save the absolute values of all elements in the differ-
ence array D and normalize them to obtain MPM:

P = fnor (|D|) .

(19)

The above description is the estimation of MPM based
on normal images. In practice, the detector may receive an
adversarial example. The results of our experiments show
that the MPM of one normal image and its adversarial im-
age is quite similar. Figure 1 shows an example of a normal
image, an adversarial image and their MPM (normalized to
(0, 255) to show more clearly).

3.2.1 Enhanced SPAM

Considering the impact of MPM, Enhanced SPAM (ES-
PAM) is proposed. The difference between SPAM and ES-
PAM is that we construct a new Markov transition proba-
bility based on MPM. For example, in the horizontal direc-
tion, the Markov transition probability M →
x,y is related to

4829

(a) Normal image

(b) MPM of normal image

(c) Adversarial image

(d) MPM of adversarial image

Figure 1. Illustrations of a normal image, an adversarial image and
their MPM.

the pixel Xi,j , Xi,j+1 and Xi,j+2. So we calculate the new
Markov transition probability M ′→

x,y in this way:

M ′→

x,y = M →

x,y · Pi,j · Pi,j+1 · Pi,j+2.

(20)

Similarly, for the second-order detecting features, the new
Markov transition probability M ′→

x,y,z is

M ′→

x,y,z = M →

x,y,z · Pi,j · Pi,j+1 · Pi,j+2 · Pi,j+3.

(21)

The rest of the process of forming ESPAM is the same
with SPAM. ESPAM has the same dimensionality as SPAM,
which is 686.

3.2.2 Enhanced SRM

The Enhanced Spatial Rich Model (ESRM) is built in the
similar manner as SRM. The difference is that ESRM mod-
iﬁes the process of forming the co-occurrence matrices to
consider the impact of MPM:

C ESRM

d0d1d2d3 =

m,n−3

X

i,j=1

max

k=0,...,3

Pi,j+k [ri,j = dk, ∀k = 0, . . . , 3],

(22)
where C (ESRM )
is the enhanced version of the co-
occurrence C (SRM ). In other words, instead of increasing
the corresponding co-occurrence bin by 1, we add the max-
imum of the modiﬁcation probabilities taken across the four
residuals to the bin [8]. Thus, if a group has four pixels with

small modiﬁcation probabilities, it has smaller effect on the
co-occurrence values than the group with at least one pixel
likely to be changed. The rest of the process of forming ES-
RM stays exactly the same with SRM. ESRM has the same
dimensionality as SRM, which is 34671.

3.3. Training Detector

The construction of our detectors based on features relies
on pattern-recognition classiﬁers. The detectors are trained
as binary classiﬁers implemented using the FLD (Fisher
Linear Discriminant) ensemble [17] with default settings.
The ensemble by default minimizes the total classiﬁcation
error probability under equal priors. We ﬁnd the number
of base learners and the random subspace dimensionality
via minimizing the out-of-bag estimate of the testing error
calculated from the training set because it is an unbiased
estimate of the testing error on unseen data [4].

4. Experimental Results

We construct the detectors by modeling the statistical d-
ifferences between adjacent pixels in natural images. There-
fore, our method can not achieve very good performance
on MNIST and CIFAR-10 because images with small sizes
cannot provide enough samples to construct efﬁcient fea-
tures. However, it has good performance on ImageNet. Pre-
vious work showed that untargeted attack is easier to suc-
ceed, results in smaller perturbations, and transfers better to
different models. So we detect untargeted adversarial ex-
amples to see the performance of our method.

We test our detection method against untargeted attacks
by FGSM, BIM, Deepfool and C&W. Our experiments are
performed on 40000 images randomly selected from Ima-
geNet (ILSVRC-2016) using a pretrained VGG-16 model
[29] as classiﬁcation network which is evaluated with top-
1 accuracy. This results in a train set of 25000 images, a
validation set of 5000 images, and a test set of 10000 im-
ages. The values of pixels per color channel of these 40000
images range from 0 to 255. For BIM, we use α = 1 to en-
sure that we change each pixel by 1 on each step and ǫ ≤ 8
where ǫ is a parameter to determine the perturbation size.
For Deepfool, we apply the l2 norm version. For C&W, we
use the l2 norm version and set κ = 0. In the process of
estimating MPM, we set L = 100.

At ﬁrst, the 40000 images from ImageNet are classiﬁed
by the network to obtain their true labels. Then we use these
40000 images to generate 40000 adversarial images as ad-
versarial samples of our experiments. To prove that MPM
is effective when detecting adversarial examples, we per-
form comparative experiments. We construct two pairs of
detectors: SPAM and ESPAM, SRM and ESRM. The only
difference between each pair is one detector with MPM and
the other detector without MPM. All detectors are trained
and tested on the same adversarial method.

4830

Table 1. Preliminary evaluations on previous detection methods.

detection
method

Hendrycks’s [16]

Li’s [19]

Grosse’s [13]
Gong’s [11]
Bhagoji’s [3]
Feinman’s [9]
Metzen’s [22]
RBF-SVM [21]

FS [32]
NR [20]

effective

on

ImageNet

effective
against
C&W

without second
neural network

✗

X

✗

✗

✗

✗

✗

X

X

X

✗

✗

X

X

✗

✗

✗

unknown

X

X

X

X

✗

✗

X

X

✗

X

X

X

Table 2. Detection rate of normal images and their adversarial im-
ages generated by FGSM.

RBF-SVM [21]
normal images

adversarial images

FS [32]

normal images

adversarial images

NR [20]

normal images

adversarial images

SPAM

normal images

adversarial images

ESPAM

normal images

adversarial images

SRM

normal images

adversarial images

ESRM

normal images

adversarial images

ǫ = 2
0.8340
0.8258
ǫ = 2
0.9460
0.4029
ǫ = 2
0.8774
0.7752
ǫ = 2
0.9488
0.9432
ǫ = 2
0.9725
0.9704
ǫ = 2
0.9757
0.9785
ǫ = 2
0.9809
0.9811

ǫ = 4
0.8913
0.8936
ǫ = 4
0.9472
0.2856
ǫ = 4
0.8670
0.6908
ǫ = 4
0.9570
0.9559
ǫ = 4
0.9758
0.9719
ǫ = 4
0.9814
0.9822
ǫ = 4
0.9839
0.9866

ǫ = 6
0.9305
0.9243
ǫ = 6
0.9441
0.2078
ǫ = 6
0.8538
0.6324
ǫ = 6
0.9651
0.9628
ǫ = 6
0.9812
0.9751
ǫ = 6
0.9831
0.9861
ǫ = 6
0.9900
0.9905

ǫ = 8
0.9487
0.9541
ǫ = 8
0.9455
0.1715
ǫ = 8
0.8513
0.5587
ǫ = 8
0.9713
0.9709
ǫ = 8
0.9868
0.9806
ǫ = 8
0.9887
0.9903
ǫ = 8
0.9931
0.9938

Carlini and Wagner point out that it is necessary to eval-
uate defenses using a strong attack on harder datasets (such
as ImageNet) [5]. Moreover, Carlini and Wagner prove that
using a second neural network to identify adversarial exam-
ples is the least effective defense [5]. Therefore we only
compare our method with the defense which is effective a-
gainst C&W on ImageNet and not based on another neural
network. Table 1 illustrates preliminary evaluations on pre-
vious detection methods: whether effective on ImageNet,
whether effective against C&W and whether without second
neural network. As shown in Table 1, Li’s defense [19] fails
against the C&W attack. Hendrycks’s [16], Bhagoji’s [3]
and Feinman’s [9] defenses are only effective on MNIST.

Table 3. Detection rate of normal images and their adversarial im-
ages generated by BIM.

RBF-SVM [21]
normal images

adversarial images

FS [32]

normal images

adversarial images

NR [20]

normal images

adversarial images

SPAM

normal images

adversarial images

ESPAM

normal images

adversarial images

SRM

normal images

adversarial images

ESRM

normal images

adversarial images

ǫ = 2
0.7749
0.7975
ǫ = 2
0.9457
0.6281
ǫ = 2
0.8802
0.8210
ǫ = 2
0.9402
0.9411
ǫ = 2
0.9708
0.9638
ǫ = 2
0.9667
0.9697
ǫ = 2
0.9712
0.9716

ǫ = 4
0.8660
0.8752
ǫ = 4
0.9440
0.3547
ǫ = 4
0.8742
0.7411
ǫ = 4
0.9485
0.9474
ǫ = 4
0.9737
0.9675
ǫ = 4
0.9706
0.9724
ǫ = 4
0.9754
0.9767

ǫ = 6
0.9145
0.9072
ǫ = 6
0.9466
0.2592
ǫ = 6
0.8599
0.6725
ǫ = 6
0.9559
0.9545
ǫ = 6
0.9749
0.9725
ǫ = 6
0.9753
0.9762
ǫ = 6
0.9811
0.9820

ǫ = 8
0.9362
0.9330
ǫ = 8
0.9451
0.2134
ǫ = 8
0.8530
0.6143
ǫ = 8
0.9606
0.9601
ǫ = 8
0.9760
0.9745
ǫ = 8
0.9802
0.9812
ǫ = 8
0.9878
0.9879

Table 4. Detection rate of normal images and their adversarial im-
ages generated by Deepfool.

normal images

adversarial images

RBF-SVM [21]

FS [32]
NR [20]
SPAM
ESPAM

SRM
ESRM

0.5838
0.9476
0.9021
0.8553
0.8690
0.9445
0.9498

0.6012
0.7441
0.9208
0.8481
0.8572
0.9491
0.9527

Table 5. Detection rate of normal images and their adversarial im-
ages generated by C&W.

normal images

adversarial images

RBF-SVM [21]

FS [32]
NR [20]
SPAM
ESPAM

SRM
ESRM

0.5332
0.9485
0.9110
0.6957
0.7467
0.8814
0.9233

0.5187
0.8933
0.9226
0.6778
0.7563
0.9092
0.9341

Table 6. Detection rate of secondary adversarial attacks yielded by
C&W.

adversarial images

SPAM ESPAM SRM ESRM
0.6669
0.9150

0.7246

0.8944

Grosse’s [13], Gong’s [11] and Metzen’s [22] defenses use

4831

a second neural network to classify images as normal or
adversarial. RBF-SVM [21] has good performance on Im-
ageNet even though its performance against C&W is not
evaluated. FS [32] and NR [20] claim good performance
when detecting C&W. Therefore, we compare our detectors
with RBF-SVM, FS and NR.

Figure 2. Average detection rate for detectors against FGSM.

Figure 3. Average detection rate for detectors against BIM.

The experimental results of detecting adversarial exam-
ples are shown in Table 2,3,4,5. The data of Table 2,3,4,5
is the detection rate of normal images and adversarial im-
ages. Figure 2 and Figure 3 illustrate these detectors’ per-
formance by averaging the detection rate of detecting nor-
mal images and adversarial images. First of all, the results
reveal that the detectors with MPM have higher detection
rates. Moreover, MPM has stronger enhancing effect on S-
PAM than SRM. When detecting FGSM and BIM, ESPAM
even has comparable performance as SRM. That is to say,
we can even use the low-dimensional model to achieve com-
parable performance as the high-dimensional model via the

enhancing method. Experimental results show that it is dif-
ﬁcult to detect adversarial examples generated by the C&W
method. RBF-SVM is almost invalid against C&W. SPAM
and SRM achieve relatively low detection rate when detect-
ing C&W. However, MPM improves SPAM by more than 7
percent and the detection rate of ESRM reaches 93 percent
on detecting adversarial examples yielded by C&W. FS and
NR achieve comparable performance with ESRM against
C&W. Unfortunately, the detection rates of FS and NR are
much lower against FGSM and BIM. We suspect the reason
why FS and NR are less effective against FGSM and BIM is
that FS and NR are only well suited to mitigating small ad-
versarial perturbations. On the contrary, the detection rate
of ESRM is the highest on detecting adversarial examples
generated by FGSM, BIM, Deepfool and C&W. However,
the computation time of SRM and ESRM is much longer
because of their high-dimensional features.

5. Secondary Adversarial Attacks

The secondary adversarial attacks are hard to be directly
performed to our method because the structure of our detec-
tion model is not a neural network. A direct idea of escap-
ing the detection of our method is to reduce the number of
adversarial perturbations. However, this strategy will also
weaken the ability of adversarial examples to mislead the
DNN. To verify this point, we try to perform the secondary
attacks by removing 10% of the untargeted adversarial per-
turbations generated by C&W. As shown in Table 6, the de-
tection rate of ESRM drops from 93.41% to 91.50%. How-
ever, the success rate of secondary attack adversarial exam-
ples to deceive the network drops from 99.03% to 45.27%.

6. Conclusions

Inspired by the insight of Goodfellow et al.

[12] that
“adversarial examples can be thought of as a sort of acci-
dental steganography”, we propose to apply steganalysis to
detecting adversarial examples. We also propose a method
to enhance steganalysis features. The experimental results
show that the enhanced scheme can accurately detect vari-
ous kinds of adversarial attacks including the C&W method.
Moreover, the secondary adversarial attacks [5] are hard to
be directly performed to our method because the structure
of our detection model is not a neural network. Our method
draws a relevant connection between adversarial examples
of computer vision and steganalysis and could kindle more
promising work in this direction. As an idea for future work,
a better secondary attack could try to add perturbations that
maintain the dependence between neighboring pixels.
Acknowledgement This work was supported in part by
the Natural Science Foundation of China under Grant
U1636201 and 61572452, and by Anhui Initiative in Quan-
tum Information Technologies under Grant AHY150400.

4832

References

[1] Dario Amodei, Rishita Anubhai, Eric Battenberg, Car-
l Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike
Chrzanowski, Adam Coates, Greg Diamos, et al. Deep
speech 2: End-to-end speech recognition in english and man-
darin.
In International Conference on Machine Learning,
pages 173–182, 2016.

[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfus-
cated gradients give a false sense of security: Circumvent-
ing defenses to adversarial examples. arXiv preprint arX-
iv:1802.00420, 2018.

[3] Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal.
Dimensionality reduction as a defense against evasion at-
tacks on machine learning classiﬁers. arXiv preprint arX-
iv:1704.02654, 2017.

[4] Leo Breiman. Bagging predictors. Machine learning,

24(2):123–140, 1996.

[5] Nicholas Carlini and David Wagner. Adversarial examples
are not easily detected: Bypassing ten detection methods. In
Proceedings of the 10th ACM Workshop on Artiﬁcial Intelli-
gence and Security, pages 3–14. ACM, 2017.

[6] Nicholas Carlini and David Wagner. Towards evaluating
the robustness of neural networks. In Security and Privacy,
2017.

[7] Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred
Hohman, Li Chen, Michael E Kounavis, and Duen Horng
Chau. Keeping the bad guys out: Protecting and vaccinating
deep learning with jpeg compression. arXiv preprint arX-
iv:1705.02900, 2017.

[8] Tomas Denemark, Vahid Sedighi, Vojtech Holub, R´emi
Cogranne, and Jessica Fridrich. Selection-channel-aware
rich model for steganalysis of digital images.
In Informa-
tion Forensics and Security (WIFS), 2014 IEEE Internation-
al Workshop on, pages 48–53. IEEE, 2014.

[9] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and An-
drew B Gardner. Detecting adversarial samples from arti-
facts. arXiv preprint arXiv:1703.00410, 2017.

[10] Jessica Fridrich and Jan Kodovsky. Rich models for steganal-
IEEE Transactions on Information

ysis of digital images.
Forensics and Security, 7(3):868–882, 2012.

[11] Zhitao Gong, Wenlu Wang, and Wei-Shinn Ku. Adver-
sarial and clean data are not twins. arXiv preprint arX-
iv:1704.04960, 2017.

[12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
arXiv

Explaining and harnessing adversarial examples.
preprint arXiv:1412.6572, 2014.

[13] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot,
Michael Backes, and Patrick McDaniel. On the (statisti-
cal) detection of adversarial examples. arXiv preprint arX-
iv:1702.06280, 2017.

[14] Shixiang Gu and Luca Rigazio. Towards deep neural net-
work architectures robust to adversarial examples. Interna-
tional Conference on Learning Representations, 2015.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016.

[16] Dan Hendrycks and Kevin Gimpel. Early methods for detect-
ing adversarial images. arXiv preprint arXiv:1608.00530,
2016.

[17] Jan Kodovsky, Jessica Fridrich, and Vojtˇech Holub. Ensem-
ble classiﬁers for steganalysis of digital media. IEEE Trans-
actions on Information Forensics and Security, 7(2):432–
444, 2012.

[18] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adver-
sarial examples in the physical world. International Confer-
ence on Learning Representations, 2017.

[19] Xin Li and Fuxin Li. Adversarial examples detection in deep
networks with convolutional ﬁlter statistics. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 5764–5772, 2017.

[20] Bin Liang, Hongcheng Li, Miaoqiang Su, Xirong Li, Wen-
chang Shi, and XiaoFeng Wang. Detecting adversarial im-
age examples in deep neural networks with adaptive noise
reduction.
IEEE Transactions on Dependable and Secure
Computing, 2018.

[21] Jiajun Lu, Theerasit Issaranon, and David Forsyth. Safe-
tynet: Detecting and rejecting adversarial examples robust-
ly. In IEEE International Conference on Computer Vision,
pages 446–454, 2017.

[22] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and
Bastian Bischoff. On detecting adversarial perturbations. In-
ternational Conference on Learning Representations, 2017.
[23] Seyed Mohsen Moosavidezfooli, Alhussein Fawzi, and Pas-
cal Frossard. Deepfool: A simple and accurate method to
fool deep neural networks. In Computer Vision and Pattern
Recognition, pages 2574–2582, 2015.

[24] Nicolas Papernot, Patrick McDaniel,

Ian Goodfellow,
Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practi-
cal black-box attacks against machine learning. In Proceed-
ings of the 2017 ACM on Asia Conference on Computer and
Communications Security, pages 506–519. ACM, 2017.

[25] Nicolas Papernot, Patrick Mcdaniel, Somesh Jha, Mat-
t Fredrikson, Z. Berkay Celik, and Ananthram Swami. The
limitations of deep learning in adversarial settings. In IEEE
European Symposium on Security and Privacy, pages 372–
387, 2016.

[26] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha,
and Ananthram Swami. Distillation as a defense to adversar-
ial perturbations against deep neural networks. In Security
and Privacy (SP), 2016 IEEE Symposium on, pages 582–
597. IEEE, 2016.

[27] Tom´aˇs Pevny, Patrick Bas, and Jessica Fridrich. Steganalysis
by subtractive pixel adjacency matrix. IEEE Transactions on
information Forensics and Security, 5(2):215–224, 2010.

[28] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large s-
cale visual recognition challenge. International Journal of
Computer Vision, 115(3):211–252, 2015.

[29] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. Com-
puter Science, 2014.

[30] Kenneth Sullivan, Upamanyu Madhow, Shivkumar Chan-
drasekaran, and Bangalore S Manjunath. Steganalysis of

4833

In
spread spectrum data hiding exploiting cover memory.
Electronic Imaging 2005, pages 38–46. International Soci-
ety for Optics and Photonics, 2005.

[31] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-
triguing properties of neural networks. International Confer-
ence on Learning Representations, 2014.

[32] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing:
Detecting adversarial examples in deep neural networks. In
Proceedings of the 2018 Network and Distributed Systems
Security Symposium (NDSS), 2018.

4834

