Training deep learning based image denoisers from undersampled

measurements without ground truth and without image prior

Magauiya Zhussip,

Shakarim Soltanayev,

Se Young Chun

Ulsan National Institute of Science and Technology (UNIST), Republic of Korea

{mzhussip,shakarim,sychun}@unist.ac.kr

Abstract

Compressive sensing is a method to recover the origi-
nal image from undersampled measurements. In order to
overcome the ill-posedness of this inverse problem, image
priors are used such as sparsity, minimal total-variation,
or self-similarity of images. Recently, deep learning based
compressive image recovery methods have been proposed
and have yielded state-of-the-art performances. They used
data-driven approaches instead of hand-crafted image pri-
ors to regularize ill-posed inverse problems with under-
sampled data.
Ironically, training deep neural networks
(DNNs) for them requires “clean” ground truth images,
but obtaining the best quality images from undersampled
data requires well-trained DNNs. To resolve this dilemma,
we propose novel methods based on two well-grounded
theories: denoiser-approximate message passing (D-AMP)
and Stein’s unbiased risk estimator (SURE). Our proposed
methods were able to train deep learning based image de-
noisers from undersampled measurements without ground
truth images and without additional image priors, and
to recover images with state-of-the-art qualities from un-
dersampled data. We evaluated our methods for various
compressive sensing recovery problems with Gaussian ran-
dom, coded diffraction pattern, and compressive sensing
MRI measurement matrices. Our proposed methods yielded
state-of-the-art performances for all cases without ground
truth images. Our methods also yielded comparable perfor-
mances to the methods with ground truth data.

1. Introduction

Compressive sensing (CS) has provided ways to sam-
ple and to compress signals at the same time with rela-
tively long signal reconstruction time [10, 15]. The idea
of combining signal acquisition and compression immedi-
ately drew great attention in the application areas such as
MRI [29, 38], CT [11], hyperspectral imaging [49, 50],
coded aperture imaging [2], radar imaging [36] and radio

astronomy [41]. CS applications have been investigated
extensively for the last decade and now some systems are
commercialized for practical usages such as low-dose CT
and accelerated MR.

CS is modeled as a linear equation for the measurement:

y = Ax + ǫ

(1)

where y ∈ RM is a measurement vector, A ∈ RM ×N is
a sensing matrix with M ≪ N , x ∈ RN is an unknown
signal to reconstruct, and ǫ ∈ RM is a noise vector. It is
a challenging ill-posed inverse problem to estimate x from
the undersampled measurements y with M ≪ N .
Sparsity has been investigated as prior to regularize the
ill-posed problem of CS recovery. CS theories allow to
use l1 norm for good signal recovery instead of l0 norm
[10, 15]. Minimizing l1 norm is advantageous for large-
scale inverse problems since l1 norm is convex so that con-
ventional convex optimization can be used. There have been
many convex optimization algorithms for solving CS recov-
ery problems with non-differentiable l1 norm such as itera-
tive shrinkage-thresholding algorithm (ISTA), fast iterative

Ground Truth

TVAL3 (35.74 dB)

Proposed (38.67 dB)

Figure 1: Ground truth MR image from fully-sampled data
(left), reconstructed MR images from 50%-sampled data
using conventional TV image prior (middle, [29]) and our
proposed deep learning based method without ground truth
(right). Our proposed method yielded signiﬁcantly better
result than conventional method and result comparable to
the ground truth with very small residual error (red box).

10255

shrinkage-thresholding algorithm (FISTA) [3], alternating
direction minimization (ADM) [44], and approximate mes-
sage passing (AMP) [16], to name a few.

The signal x itself is not usually sparse, but a trans-
formed signal is often sparse. Signals/images are sparse in
the wavelet domain and/or discrete cosine transform (DCT)
domain.
In high-resolution imaging, images have sparse
edges that are often promoted by minimizing total variation
(TV) [28]. Sparse MR image recovery used both wavelet
and TV priors [29] or dictionary learning prior from highly
undersampled measurements [38].
Similarly, CS color
video and depth recovery used both wavelet and DCT [46].
Hyperspectral imaging utilized manifold-structured sparsity
prior [49] or reweighted Laplace prior [50]. Self-similarity
is also used for CS image recovery such as NLR-CS [14]
and denoiser-AMP (D-AMP) [32]. D-AMP utilized power-
ful modern denoisers such as BM3D [12] and has recently
been extended to sparse MRI [17].

1.1. Deep learning in compressive image recovery

Deep learning with massive amount of data has revo-
lutionized many computer vision tasks [26].
It has also
inﬂuenced many low level computer vision tasks such as
image denoising [8, 22, 40, 42, 48, 35] and CS recov-
ery [20, 21, 23, 25, 33, 43, 47]. There are largely two differ-
ent approaches using deep neural networks (DNNs) for CS
recovery. One is to use a deep network to directly map from
an initially recovered low-quality image from compressive
samples to a high-quality ground truth image [23, 25]. The
other approach for deep learning based CS image recov-
ery is to use DNN structures that unfold optimization algo-
rithms and learned image priors, inspired by learned ISTA
(LISTA) [19]. In sparse MRI, ADMM-Net [45] and vari-
ational network [21] were proposed with excellent per-
formances. Both methods learned parametrized shrinkage
functions as well as transformation operators for sparse rep-
resentation from training data. Recently, instead of us-
ing explicit parametrization in shrinkage operator, DNNs
were used to unfold optimizations for CS recovery such
as learned D-AMP (LDAMP) [33], ISTA-Net [47], CNN-
projected gradient descent for CT [20], and Laplacian pyra-
mid reconstructive adversarial network [43]. Utilizing gen-
erative adversarial network (GAN) for CS was also investi-
gated [4]. These methods have one important requirement:
“clean” ground truth images must be available for training.

acquire clean data, for example, in medical imaging (long
acquisition for MR, high radiation dose for CT) or hyper-
spectral imaging. Here, we address this dilemma.

Recently, there have been a few attempts to train DNNs
for low-level computer vision tasks in unsupervised ways.
Lehtinen et al. proposed noise2noise to train DNNs for im-
age denoising, inpainting, and MR reconstruction [27]. This
work implemented MR reconstruction using a direct map-
ping instead of unfolding optimization scheme. However,
this was not evaluated with various CS applications and it
requires two contaminated data for each image, which may
not be available in some cases. Bora et al. proposed Am-
bientGAN, a training method for GAN with contaminated
images and applied it to CS image recovery [4, 5]. How-
ever, AmbientGAN was trained with artiﬁcially contami-
nated images, rather than with CS measurements. More-
over, the method of [4] is limited to i.i.d. Gaussian mea-
surement matrix theoretically and was evaluated with rela-
tively low-resolution images. Soltanayev et al. proposed
a Stein’s unbiased risk estimator (SURE) based training
method for deep learning based denoisers [39]. This method
requires only one realization, but it is limited to i.i.d. Gaus-
sian noise. Moreover, it is not straightforward to extend this
work to CS image recovery with measurements.

We propose unsupervised training methods for deep
learning based CS image recovery based on two well-
grounded theories: D-AMP and SURE. Our proposed meth-
ods were able to train DNN based image denoisers from
undersampled measurements without ground truth images,
and to recover images with state-of-the-art qualities from
undersampled data. Here are the contributions of this work:
1) Proposing a method to train DNN denoisers from un-
dersampled measurements without ground truth and with-
out additional image priors. Only one realization for each
measurement was required. An accurate noise estimation
method was also developed for training deep denoisers.

2) Proposing a CS image recovery method by modifying
LDAMP to have up to1 denoiser instead of 9 denoisers with
comparable performance to reduce training time.

3) Extensive evaluations of the proposed method using
high-resolution natural images and MR images for the CS
recovery problems with Gaussian random, coded diffraction
pattern, and realistic CS MR measurement matrices.

2. Background

1.2. Deep learning without ground truth

2.1. Denoiser based AMP (D AMP)

Most deep learning based data-driven approaches for CS
image recovery solve the ill-posed inverse problem with un-
dersampled data by using DNNs. Ironically, training DNNs
for them requires “clean” ground truth images, but obtain-
ing the best quality images from undersampled data requires
well-trained DNNs.
It is often expensive or infeasible to

D-AMP is an algorithm designed to solve CS problems
where one needs to recover image vector x from the set of
measurements y using prior information about x. Based on
the model (1), the problem can be formulated as:

x ky − Axk2
min

2

subject to x ∈ C

(2)

10256

Algorithm 1: (Learned) D-AMP algorithm [32, 33]

input : x0 = 0, y, A

1 for t = 0 to T do

bt ← zt−1divDw(ˆσt−1)(xt−1 + AH zt−1)/M
zt ← y − Axt + bt
ˆσt ← kztk2/√M
xt+1 ← Dw( ˆσt)(xt + AH zt)

2

3

4

5

6 end

output: xT

where C is the set of natural images. D-AMP solves (2)
relying on approximate message passing (AMP) theory. It
employs appropriate Onsager correction term bt at each it-
eration, so that xt + AH zt in Algorithm 1 becomes close to
the ground truth image plus i.i.d. Gaussian noise. D-AMP
can utilize any denoiser as a mapping operator Dw( ˆσt)(·)
in CS recovery (Algorithm 1) for reducing i.i.d. Gaussian
noise as far as the divergence of denoiser can be obtained.

D-AMP [31] ﬁrst utilized conventional state-of-the-art
denoisers such as BM3D [12] for Dw( ˆσt)(·) in Algorithm 1.
Given a standard deviation of noise ˆσt at iteration t, BM3D
was applied to a noisy image xt + AH zt to yield estimated
image xt+1. Since BM3D can not be represented as a lin-
ear function, analytical form for divergence of this denoiser
is not available for obtaining Onsager term. This issue was
resolved by using Monte-Carlo (MC) approximation for di-
vergence term divDw(ˆσt)(·) [37]: For ǫ > 0 and ˜n is a
standard normal random vector,

(3)

˜n′
ǫ (cid:0)Dw(ˆσt)(· + ǫ ˜n)−Dw(ˆσt)(·)(cid:1) .

divDw(ˆσt)(·) ≈
Recently, LDAMP was proposed to use deep learning
based denoisers for Dw( ˆσt)(·) in Algorithm 1. Nine DNN
denoisers were trained with noiseless ground truth data for
different noise levels. LDAMP consists of 10 D-AMP lay-
ers (iterations) and DnCNN [48] was used in each layer as
a denoiser operator. Unlike other unrolled neural network
versions of iterative algorithms such as learned-AMP [6]
and LISTA [19], LDAMP exploited imaging system mod-
els, which are ﬁxed A and AH operators while the param-
eters for DnCNN denoisers were trained with ground truth
data in image domain.

2.2. Stein’s unbiased risk estimator (SURE) based

deep neural network denoisers

Over the past years, DNN based denoisers have been
well investigated [8, 22, 40, 42, 48, 35] and often outper-
formed conventional denoisers such as BM3D [12] and non-
local ﬁltering [7, 34]. DNN denoisers such as DnCNN [48]
yielded state-of-the-art denoising performance at multiple
noise levels and are typically trained by minimizing the

mean square error (MSE) between the output image of de-
noiser and the noiseless ground truth image:

1
K

K

X

j=1

kDw(σ)(z(j)) − x(j)k2

(4)

where z ∈ RN is a noisy image of the ground truth im-
age x contaminated with i.i.d. Gaussian noise with zero
mean and ﬁxed σ2 variance, Dw(σ)(·) is a deep learning
based denoiser with large-scale parameters w to train, and
(z(1), x(1)), . . . , (z(K), x(K)) is a training dataset with K
samples in image domain.

Recently, a method to train deep learning based denois-
ers only with noisy images was proposed [39]. Instead of
minimizing MSE, the following Monte-Carlo Stein’s un-
biased risk estimator (MC-SURE) that approximates MSE
was minimized with respect to large-scale weights w in the
DNN without noiseless ground truth images:

1
K

K

X

j=1

kz(j) − Dw(σ)(z(j))k2 − N σ2+

(5)

2σ2 ˜n′

ǫ

(cid:16)Dw(σ)(z(j) + ǫ ˜n) − Dw(σ)(z(j))(cid:17) .

In compressive image recovery applications, there are
often cases where no ground truth data or no Gaussian
contaminated images are available, but only compressive
samples in measurement domain are available for training.
However, it is not straightforward to use MSE or MC-SURE
based deep denoiser networks for CS image recovery unless
additional image priors are used. The goal of this article is
to propose a method to train DNN denoisers directly from
compressive samples without additional image prior and to
simultaneously recover images.

3. Method

3.1. Training deep denoisers from undersampled

measurements without ground truth

Our proposed method exploits D-AMP (LDAMP) [32,
33] to yield Gaussian noise contaminated images during
compressive image recovery from large-scale undersampled
measurements and train a single DNN denoiser with these
noisy images at different noise levels using MC-SURE
based denoiser learning [39]. Since Onsager correction
term in D-AMP allows x + AH z term to be close to the
ground truth image plus Gaussian noise, we conjecture that
these can be utilized for MC-SURE based denoiser train-
ing. We further investigated this in the next subsection.
Our joint algorithm is detailed in Algorithm 2. Note that
for large-scale compressive measurements y(1), . . . , y(K),
both images ˆx(1)
L and trained denoising deep net-
work DwL(σ)(·) were able to be obtained. After training,

L , . . . , ˆx(K)

10257

fast and high performance compressive image recovery was
possible without further training of deep denoising network.
The original LDAMP [33] utilized 9 DnCNN denois-
ers trained on “clean” images for different noise levels
(σ = 0 ∼ 10, 10 ∼ 20, 20 ∼ 40, 40 ∼ 60, 60 ∼
80, 80 ∼ 100, 100 ∼ 150, 150 ∼ 300, 300 ∼ 500).
However, we found that training a single DnCNN denoiser
is enough to achieve almost the same results (see Table
2 in the supplementary material). The DnCNN network
was pre-trained with reconstructed images using D-AMP
with BM3D plus Gaussian noise with σ ∈ [0, 55]. The
pre-trained DnCNN blind denoiser Dwl( ˆσt) cleans xt−1 +
AH zt−1 with noise level between [0, 55] (line 8 in Algo-
rithm 2), while BM 3D ˆσt is utilized for higher level noise
reduction (line 10 in Algorithm 2). Depending on a sam-
pling ratio and forward operator A, initial 2-4 iterations are
usually required for BM3D to decrease the noise level sufﬁ-
cient enough for DnCNN. Then, after T iterations, the set of
training data s(1)
can be generated using LDAMP
with pre-trained deep denoiser. Those noisy training images
were used for further training the DnCNN with MC-SURE.
Worth to note that the noise level range for DnCNN is
subject to change depending on a particular problem. For
example, we found that for i.i.d. Gaussian and CDP matri-
ces, training DnCNN with σ ∈ [0, 55] is optimal, while for
CS MRI case, the range is shortened to σ ∈ [0, 10].
3.2. Accuracy of standard deviation estimation for

, . . . , s(K)

l

l

MC SURE based denoiser learning

In D-AMP and LDAMP [32, 33], noise level was esti-

mated in measurement domain using

ˆσt ← kztk2/√M .

(6)

The accuracy of this estimation was not critical for D-AMP
or LDAMP since denoisers in both methods were not sensi-
tive to different noise levels. However, accurate noise level
estimation was quite important for MC-SURE based deep
denoiser network learning. We investigated the accuracy of
(6). It turned out that the accuracy of noise level estimation
depends on measurement matrices.

With i.i.d. Gaussian measurement matrix A, (6) was
very accurate and comparable to the ground truth stan-
dard deviation that was obtained from the true residual

(xt + AH zt)− xtrue. However, with coded diffraction pat-

tern measurement matrix A that yields complex measure-
ments, it turned out that (6) yielded over-estimated noise
level for multiple examples. Since the image xt is real, we
propose a new standard estimation method for D-AMP:

Algorithm 2: Simultaneous LDAMP and MC-SURE
deep denoiser learning algorithm

input : y(1), . . . , y(K), A

1 for l = 1 to L do
2

for k = 1 to K do

3

4

5

6

7

8

9

10

11

12

13

14

15

16

for t = 0 to T do

bt ←
zt−1divDwl(ˆσt−1)(xt−1 + AH zt−1)/M
zt ← y(k) − Axt + bt
ˆσt ← kztk2/√M
if ˆσt ≤ 55. then
else

xt+1 ← Dwl( ˆσt)(xt + AH zt)
xt+1 ← BM 3D ˆσt (xt + AH zt)

end

end
ˆx(k)
l ← xT +1
s(k)
l ← xT + AH zT
Train Dwl(σ)(·) with s(1)

end

noise levels σ

l

, . . . , s(K)

l

at different

17 end

output: ˆx(1)

L , . . . , ˆx(K)

L , DwL(σ)(·)

and found that they are all very similar for i.i.d. Gaussian
measurement matrix, but our proposed method (7) yielded
more accurate estimates of standard deviation than previous
method (6). Figure 2 illustrates the accuracy of our estima-
tor compared to previous one. When normalizing the true
residual, using accurate sigma estimation yields good ﬁt-
ting to standard normal density (red line). Normalized his-
togram of true residual using ground truth and our proposed
standard deviation estimation yielded good ﬁtting to that,
but previous estimation method yielded sharper histogram,
which indicates that previous method overestimates noise
level. Our proposed estimation was critical for the high per-
formance of our proposed method with CDP measurement
matrix (see Table 1 in the supplementary material).

Moreover, we found out that proposed noise estimator
(7) also can be applied to a CS-MRI case, when k-space
data is not highly undersampled. Therefore, for a sampling
rate of larger than 35-40%, true residual follows a Gaussian
noise, which can be accurately measured by (7) and further
utilized for training deep denoisers with MC-SURE.

4. Simulation Results

ˆσt ← k Re(AH zt)k2/√N .

(7)

4.1. Setup

We performed comparison studies between (6), (7), and

the ground truth from true residual (xt + AH zt) − xtrue

Datasets We used images from DIV2K [1], Berkeley’s
BSD-500 [30] datasets, and standard test images for train-

10258

Figure 2: Normalized residual histograms of “Boat” image after 10 iterations using LDAMP-BM3D for CDP matrix. Nor-
malization was done with estimated sigma from (a) true residual (b) zT (D-AMP) and (c) Re(AH zT ) (Proposed).

ing and testing our proposed method on i.i.d. Gaussian and
CDP matrices. Training dataset was comprised of all 500
images from BSD-500, while a test set of 100 images in-
cluded 75 randomly chosen images from DIV2K dataset
and 25 standard test images. Since the proposed method
used measurement data and ﬁxed linear operator for im-
age reconstruction, all test and train images had to have the
same size. Thus, all images were subsampled and cropped
to the size of 180 ×180 and then compressively sampled us-
ing the forward model A to generate CS measurement data.
For CS-MRI reconstruction, Stanford dataset with 3D
FSE (fast spin echo) [18] was pulled from the open reposi-
tory at http://mridata.org/. The knee dataset included 20 pa-
tients each having 256 slices of 320 × 320 images. Among
20 cases of knee data, 3 cases were used for training and 1
case for testing. Images were transformed to k-space mea-
surements and then subsampled with realistic radial sam-
pling patterns at various sampling rates.

We implemented all methods on the Tensorﬂow frame-
work and used Adam optimization [24] with the learning
rate of 10-3, which was dropped to 10-4 after 40 epochs and
further trained for 10 epochs. The batch size was set to 128
and training the DnCNN denoiser took approximately 12-
14 hours on one NVIDIA Titan X (Pascal).

Initialization of DnCNN denoiser For given measure-
ment data y from BSD-500 and linear operator A, initial
images were ﬁrstly reconstructed using a conventional CS
recovery algorithm, BM3D-AMP. Even though the qual-
ity of these initial images were not close to the ground
truth images,
they still provided good pre-training data
for DnCNN denoisers. Recovered images were rescaled,
cropped, ﬂipped, and rotated to generate 298,000 image
patches whose sizes are 50×50 . These patches were used
as a ground truth to pre-train DnCNN denoiser with MSE.
Since our approach does not require dataset with ground
truth, it is possible to use measurement data from the test
set. Thus, we also generated 357,600 50×50 patches from
reconstructed test and train images. Our DnCNN denoiser
was trained for σ ∈ [0, 55] noise level range with either

training patches only or training and testing patches to-
gether. The former pre-trained DnCNN denoiser in the
LDAMP framework is denoted by “LDAMP BM3D” and
the latter pre-trained DnCNN with LDAMP is denoted
by“LDAMP BM3D-T”.

BM3D-AMP-MRI was speciﬁcally tailored for CS-MRI
reconstruction [17] and thus yielded signiﬁcantly better re-
sults than conventional BM3D-AMP. Therefore, k-space
knee dataset was reconstructed using it and then the re-
sulted images were rescaled, cropped, ﬂipped, and rotated
to generate 267,240 and 350,320 50×50 patches for training
LDAMP BM3D and LDAMP BM3D-T, respectively. We
trained DnCNN denoisers for σ ∈ [0, 10] noise range.
Training LDAMP SURE Firstly, LDAMP SURE was
run T = 10 iterations using pre-trained DnCNN denoiser
and BM3D. At the last iteration, we collected images and
estimated noise standard deviation with (7). Then, all im-
ages with noise levels in [0, 55] range (CS-MRI case: σ ∈
[0, 10]) were grouped into one set, while images with larger
noise levels were replaced by Gaussian noise added BM3D-
AMP recovered images. Thus, we have the dataset of all
images with σ ∈ [0, 55] (CS-MRI case: σ ∈ [0, 10]) to
train DnCNN denoiser with MC-SURE. These steps were
repeated L times to further improve the performance of our
proposed method. Although training DnCNN with MC-
SURE involves estimation of a noise standard deviation for
an entire image, we assume that a patch from an image has
the same noise level as the image itself. Thus, we generated
patches without using rescaling to avoid noise distortion to
train LDAMP SURE.

To train DnCNN with SURE, we initialized DnCNN
with the weights of pre-trained DnCNN and trained it using
Adam optimizer [24] with learning rate of 10-4 and batch
size 128 for 10 epochs. Then, we decreased learning rate to
10-5 and trained it for another 10 epochs. Training process
took about 3 hours for LDAMP SURE and about 4 hours for
LDAMP SURE-T. We empirically found that after L=2 it-
erations (line 1 in Algorithm 2) of training LDAMP SURE,
the results converge for both CDP and i.i.d. Gaussian cases,

10259

while for CS-MRI, L = 1.

The accuracy of MC-SURE approximation depends on
the selection of constant value ǫ, which is directly propor-
tional to σ [13, 39]. Therefore, for training DnCNN with
SURE, ǫ value was calculated for each patch based on its
noise level (see Section 1 in the supplementary material).

4.2. Results

Gaussian measurement matrix We compared our pro-
posed LDAMP SURE with the state-of-the-art CS meth-
ods that do not require ground truth data such as BM3D-
AMP[31], NLR-CS[14], and TVAL3[28]. BM3D-AMP
was used with default parameters and run for 30 iterations
to reduce high variation in the results, although PSNR1 ap-
proached its maximum after 10 iterations [32]. The pro-
posed LDAMP SURE algorithm was run 30 iterations but
also showed convergence after 8-10 iterations. NLR-CS
was initialized with 8 iterations of BM3D as justiﬁed in
[32], while TVAL3 was set to its default parameters. Also,
we included the results of LDAMP with a single DnCNN
(denoted as LDAMP MSE) that was trained on ground truth
images to see the performance gap.

From Table 1, proposed LDAMP SURE and LDAMP
SURE-T outperformed other methods at higher CS ratios
by 0.26-0.46 dB, while at a highly undersampled case, it
is inferior to NLR-CS. Nevertheless, it is clear that SURE
based LDAMP is able to improve the performance of pre-
trained LDAMP BM3D and surpasses BM3D-AMP by
0.28-1.56 dB. In Figure 3, reconstructions of all methods
on a test image are represented for 25% sampling ratio.
Proposed LDAMP SURE and LDAMP SURE-T provide
sharper edges and preserve more details.

In terms of run time, the dominant source of computation
comes from using BM3D denoiser at initial iterations, while
DnCNN takes less than a second for inference. LDAMP
SURE utilizes CPU for BM3D and GPU for DnCNN. Con-
sequently, proposed LDAMP SURE was faster than BM3D-
AMP, NLR-CS, and TVAL3 methods.

Coded diffraction pattern measurements LDAMP
SURE was tested with randomly sampled coded diffraction
pattern [9] and yielded the best quantitative performance at
higher sampling rates (see Table 2 and Figure 4). LDAMP
SURE and LDAMP SURE-T achieved about 1.8 dB perfor-
mance gain over BM3D-AMP. However, at extremely low
sampling ratio, our method slightly falls behind TVAL3.
LDAMP SURE requires better data than BM3D-AMP re-
constructed images from highly undersampled data to pre-
train DnCNN. Therefore, one way to surpass TVAL3 at
the highly undersampled case is to pretrain DnCNN with
TVAL3 images (see Table 3 in supplementary material).

1PSNR stands for peak signal-to-noise ratio and is calculated by fol-
mean(ˆx−xgt)2 ) for pixel range ∈ [0 − 255]

lowing expression: 10log10(

2552

CS MR measurement matrix LDAMP SURE was ap-
plied to CS MRI reconstruction problem to demonstrate
its generality and to show its performance on images that
contain structures different from natural image dataset.
We compared LDAMP SURE with state-of-the-art BM3D-
AMP-MRI [17] for CS-MR image reconstruction without
ground truth data along with TVAL3, BM3D-AMP, and dic-
tionary learning based DL-MRI [38]. Average image re-
covery PSNRs and run times are tabulated in Table 3. Fig-
ure 5 shows that our proposed method yielded state-of-the-
art performance, close to the ground truth. The results re-
veal that proposed LDAMP SURE-T outperforms existing
algorithms in all sampling ratios.

5. Discussion and Conclusion

We proposed methods for unsupervised training of im-
age denoisers with undersampled CS measurements. Our
methods simultaneously performed CS image recovery and
DNN denoiser learning. Our proposed method yielded bet-
ter image quality than conventional methods at higher sam-
pling rates for i.i.d Gaussian, CDP, and CS MR measure-
ments. Thus, it may be possible that this work can be help-
ful for areas where obtaining ground truth images is chal-
lenging such as hyperspectral or medical imaging.

Note that training deep learning based image denoisers
from undersampled data still seems to require to contain
enough information in the undersampled measurements.
Tables 1 and 2 show that only 5% of the full samples was
not enough to achieve state-of-the-art performance possi-
bly due to lack of information in the measurement. Note
also that since we assume a single CS measurement for
each image and evaluated with various CS matrices with
high-resolution images,
it was not possible to compare
our method to noise2noise [27] and AmbientGAN [4, 5].
Lastly, our proposed method can potentially be used with
more advanced deep denoisers for potentially better perfor-
mance as far as they are trainable with MC SURE loss [39].

Acknowledgments

This work was supported partly by Basic Science Re-
search Program through the National Research Foundation
of Korea(NRF) funded by the Ministry of Education(NRF-
2017R1D1A1B05035810), the Technology Innovation Pro-
gram or Industrial Strategic Technology Development Pro-
gram (10077533, Development of robotic manipulation al-
gorithm for grasping/assembling with the machine learn-
ing using visual and tactile sensing information) funded by
the Ministry of Trade, Industry & Energy (MOTIE, Korea),
and a grant of the Korea Health Technology R&D Project
through the Korea Health Industry Development Institute
(KHIDI), funded by the Ministry of Health & Welfare, Re-
public of Korea (grant number: HI18C0316).

10260

Method

Training Time

TVAL3
NLR-CS
BM3D-AMP

LDAMP BM3D
LDAMP BM3D-T
LDAMP SURE
LDAMP SURE-T

LDAMP MSE

N/A
N/A
N/A

10.90 hrs
14.30 hrs
15.05 hrs
17.97 hrs

10.17 hrs

M
N = 5%

M
N = 15%

M
N = 25%

PSNR

Time

PSNR

Time

PSNR

Time

20.46
21.88
21.40

21.41
21.42
21.44
21.68

22.07

9.71

128.73
25.98

8.98
8.98
8.98
8.98

8.98

24.14
27.58
26.74

27.54
27.61
27.65
27.84

27.78

22.96
312.92
24.21

3.94
3.94
3.94
3.94

3.94

26.77
31.20
30.10

31.20
31.32
31.46
31.66

31.65

34.87
452.23
23.08

2.89
2.89
2.89
2.89

2.89

Table 1: Average PSNRs (dB) and run times (sec) of 100 180×180 image reconstructions for i.i.d. Gaussian measurements
case (no measurement noise) at various sampling rates (M/N × 100%).

Method

Training Time

M
N = 5%

M
N = 15%

M
N = 25%

PSNR Time

PSNR Time

PSNR

Time

TVAL3
NLR-CS
BM3D-AMP

LDAMP BM3D
LDAMP BM3D-T
LDAMP SURE
LDAMP SURE-T

N/A
N/A
N/A

10.56 hrs
12.67 hrs
15.22 hrs
17.61 hrs

22.57
19.00
21.66

21.97
21.93
22.18
22.06

0.85
93.05
22.15

23.43
23.43
23.43
23.43

27.99
22.98
27.29

28.04
28.01
29.14
29.17

LDAMP MSE

10.17 hrs

22.12

23.43

28.87

0.75
86.90
22.28

7.01
7.01
7.01
7.01

7.01

32.82
31.24
31.40

31.65
32.12
33.26
33.51

33.88

0.67

119.70
17.00

2.71
2.71
2.71
2.71

2.71

Table 2: Average PSNRs (dB) and run times (sec) of 100 180x180 image reconstructions for CDP measurements case (no
measurement noise) at various sampling rates (M/N × 100%).

Method

Training Time

TVAL3
DL-MRI
BM3D-AMP-MRI
BM3D-AMP

LDAMP BM3D
LDAMP BM3D-T
LDAMP SURE
LDAMP SURE-T

N/A
N/A
N/A
N/A

9.31 hrs
12.41 hrs
12.04 hrs
16.05 hrs

M
N = 40%

M
N = 50%

M
N = 60%

PSNR Time

PSNR Time

PSNR Time

36.76
36.60
37.42
36.15

37.12
37.65
37.40
37.77

0.58
98.51
14.76
96.23

6.26
6.26
6.26
6.26

37.13
37.81
38.94
36.29

38.63
38.92
38.70
39.09

0.24
97.58
15.00
84.34

6.14
6.14
6.14
6.14

38.35
39.13
40.51
39.53

39.53
39.87
40.62
40.71

0.21
99.44
15.36
98.01

6.10
6.10
6.10
6.10

Table 3: Average PSNRs (dB) and run times (sec) of 100 180x180 image reconstructions for CS-MRI measurements case
(no measurement noise) at various sampling rates (M/N × 100%).

10261

Ground truth

TVAL3

BM3D-AMP

NLR-CS

LDAMP SURE

LDAMP SURE-T

(a) PSNR

(b) 26.10 dB

(c) 31.90 dB

(d) 33.59 dB

(e) 34.53dB

(f) 35.19 dB

Figure 3: Reconstructions of 180×180 test “Butterﬂy” image with i.i.d. Gaussian matrix with M/N = 0.25 sampling rate.
LDAMP SURE-T

BM3D-AMP

Ground truth

LDAMP SURE

NLR-CS

TVAL3

(a) PSNR

(b) 27.52 dB

(c) 24.08 dB

(d) 22.29 dB

(e) 29.17 dB

(f) 28.92 dB

Figure 4: Reconstructions of 180×180 test image with CDP measurement matrix for M/N = 0.15 sampling rate.
Ground truth

BM3D-AMP-MRI

BM3D-AMP

DL-MRI

TVAL3

LDAMP SURE-T

(a) PSNR

(b) 37.44 dB

(c) 36.54 dB

(d) 36.76 dB

(e) 37.85 dB

(f) 38.22 dB

Figure 5: Reconstructions of 180×180 test image with CS-MRI measurement matrix for M/N = 0.40 sampling rate.
Residual errors are shown in red boxes.

10262

References

[1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 chal-
lenge on single image super-resolution: Dataset and study.
In IEEE Conference on Computer Vision and Pattern Recog-
nition Workshop (CVPRW), 2017. 4

[2] Gonzalo R Arce, David J Brady, Lawrence Carin, Henry Ar-
guello, and David S Kittle. Compressive Coded Aperture
Spectral Imaging: An Introduction. IEEE Signal Processing
Magazine, 31(1):105–115, Nov. 2013. 1

[3] Amir Beck and Marc Teboulle. A Fast Iterative Shrinkage-
Thresholding Algorithm for Linear Inverse Problems. SIAM
Journal on Imaging Sciences, 2(1):183–202, Jan. 2009. 2

[4] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Di-
makis. Compressed sensing using generative models. In In-
ternational Conference on Machine Learning (ICML), pages
537–46, 2017. 2, 6

[5] A Bora, E Price, and A G Dimakis. AmbientGAN: Gen-
erative models from lossy measurements.
In International
Conference on Learning Representations (ICLR), 2018. 2, 6
[6] Mark Borgerding, Philip Schniter, and Sundeep Ran-
gan. AMP-inspired deep networks for sparse linear in-
verse problems.
IEEE Transactions on Signal Processing,
65(16):4293–4308, 2017. 3

[7] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel. A
non-local algorithm for image denoising. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 60–65, 2005. 3

[8] Harold C Burger, Christian J Schuler, and Stefan Harmel-
ing. Image denoising: Can plain neural networks compete
with BM3D? In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 2392–2399, 2012. 2, 3

[9] Emmanuel

J Candes, Xiaodong Li,

and Mahdi
Soltanolkotabi.
Phase retrieval from coded diffraction
patterns. Applied and Computational Harmonic Analysis,
39(2):277–299, 2015. 6

[10] E J Candes, J Romberg, and T Tao. Robust uncertainty prin-
ciples: exact signal reconstruction from highly incomplete
frequency information.
IEEE Transactions on Information
Theory, 52(2):489–509, Jan. 2006. 1

[11] Kihwan Choi, Jing Wang, Lei Zhu, Tae-Suk Suh, Stephen
Boyd, and Lei Xing. Compressed sensing based cone-
beam computed tomography reconstruction with a ﬁrst-order
method. Medical Physics, 37(9):5113–5125, Aug. 2010. 1

[12] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and
Karen Egiazarian. Image denoising by sparse 3-d transform-
domain collaborative ﬁltering. IEEE Transactions on Image
Processing, 16(8):2080–2095, 2007. 2, 3

[13] Charles-Alban Deledalle, Samuel Vaiter, Jalal Fadili, and
Gabriel Peyr´e. Stein Unbiased GrAdient estimator of the
Risk (SUGAR) for multiple parameter selection. SIAM Jour-
nal on Imaging Sciences, 7(4):2448–2487, 2014. 6

[14] Weisheng Dong, Guangming Shi, Xin Li, Yi Ma, and
Feng Huang. Compressive sensing via nonlocal low-rank
regularization.
IEEE Transactions on Image Processing,
23(8):3618–3632, 2014. 2, 6

[15] D L Donoho. Compressed sensing. IEEE Transactions on

Information Theory, 52(4):1289–1306, Mar. 2006. 1

[16] D L Donoho, A Maleki, and A Montanari. Message-passing
algorithms for compressed sensing. Proceedings of the Na-
tional Academy of Sciences (PNAS), 106(45):18914–18919,
Nov. 2009. 2

[17] Ender M Eksioglu and A Korhan Tanc. Denoising AMP for
MRI reconstruction: BM3D-AMP-MRI. SIAM Journal on
Imaging Sciences, 11(3):2090–2109, 2018. 2, 5, 6

[18] Kevin Epperson, Anne Marie Sawyer, Michael Lustig, Mar-
cus Alley, Martin Uecker, Patrick Virtue, Peng Lai, and
Shreyas Vasanawala. Creation of Fully Sampled MR Data
Repository for Compressed Sensing of the Knee. In SMRT
Conference, 2013. 5

[19] Karol Gregor and Yann LeCun. Learning fast approxima-
tions of sparse coding. In International Conference on In-
ternational Conference on Machine Learning (ICML), pages
399–406, 2010. 2, 3

[20] Harshit Gupta, Kyong Hwan Jin, Ha Q Nguyen, Michael T
McCann, and Michael Unser. CNN-Based Projected Gradi-
ent Descent for Consistent CT Image Reconstruction. IEEE
transactions on medical imaging, pages 1–1, May 2018. 2

[21] Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P
Recht, Daniel K Sodickson, Thomas Pock, and Florian
Knoll. Learning a variational network for reconstruction
of accelerated MRI data. Magnetic Resonance in Medicine,
79(6):3055–3071, Nov. 2017. 2

[22] Viren Jain and Sebastian Seung. Natural image denoising
with convolutional networks. In Advances in Neural Infor-
mation Processing Systems (NIPS), pages 769–776, 2009. 2,
3

[23] Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey,
and Michael Unser. Deep Convolutional Neural Network for
Inverse Problems in Imaging. IEEE Transactions on Image
Processing, 26(9):4509–4522, Sept. 2017. 2

[24] Diederik P. Kingma and Jimmy Lei Ba. ADAM: A method
for stochastic optimization. In International Conference on
Learning Representations (ICLR), 2015. 5

[25] Kuldeep Kulkarni, Suhas Lohit, Pavan K Turaga, Ronan Ker-
viche, and Amit Ashok. ReconNet: Non-iterative reconstruc-
tion of images from compressively sensed measurements. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 449–458, 2016. 2

[26] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep

learning. Nature, 521(7553):436–444, May 2015. 2

[27] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli
Laine, Tero Karras, Miika Aittala,
and Timo Aila.
Noise2Noise: Learning image restoration without clean data.
In International Conference on Machine Learning (ICML),
pages 2965–74, 2018. 2, 6

[28] Chengbo Li, Wotao Yin, and Yin Zhang. Users guide for
TVAL3: TV minimization by augmented Lagrangian and al-
ternating direction algorithms. CAAM report, 20(46-47):4,
2009. 2, 6

[29] Michael Lustig, David Donoho, and John M Pauly. Sparse
MRI: The application of compressed sensing for rapid MR
imaging. Magnetic Resonance in Medicine, 58(6):1182–
1195, 2007. 1, 2

[30] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to

10263

evaluating segmentation algorithms and measuring ecologi-
cal statistics. In IEEE International Conference on Computer
Vision (ICCV), volume 2, pages 416–423, July 2001. 4

[44] Junfeng Yang and Yin Zhang. Alternating Direction Algo-
rithms for ℓ1-Problems in Compressive Sensing. SIAM Jour-
nal on Scientiﬁc Computing, 33(1):250–278, Jan. 2011. 2

[45] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Deep
ADMM-Net for compressive sensing MRI. In Advances in
Neural Information Processing Systems (NIPS), pages 10–
18, 2016. 2

[46] Xin Yuan, Patrick Llull, Xuejun Liao, Jianbo Yang, David J
Brady, Guillermo Sapiro, and Lawrence Carin. Low-cost
compressive sensing for color video and depth.
In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3318–25, 2014. 2

[47] Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable
optimization-inspired deep network for image compressive
sensing. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 1828–1837, 2018. 2

[48] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a Gaussian denoiser: Residual learning
of deep CNN for image denoising.
IEEE Transactions on
Image Processing, 26(7):3142–3155, 2017. 2, 3

[49] Lei Zhang, Wei Wei, Yanning Zhang, Fei Li, Chunhua Shen,
and Qinfeng Shi. Hyperspectral compressive sensing us-
ing manifold-structured sparsity prior. In IEEE International
Conference on Computer Vision (ICCV), pages 3550–3558,
2015. 1, 2

[50] Lei Zhang, Wei Wei, Yanning Zhang, Chunna Tian, and
Fei Li. Reweighted Laplace prior based hyperspectral com-
pressive sensing for unknown sparsity. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
2274–2281, 2015. 1, 2

[31] Christopher A Metzler, Arian Maleki, and Richard G Bara-
niuk. BM3D-AMP: A new image recovery algorithm based
on BM3D denoising. In IEEE International Conference on
Image Processing (ICIP), pages 3116–3120, 2015. 3, 6

[32] Christopher A Metzler, Arian Maleki, and Richard G Bara-
niuk. From denoising to compressed sensing. IEEE Trans-
actions on Information Theory, 62(9):5117–5144, 2016. 2,
3, 4, 6

[33] Christopher A Metzler, Arian Maleki, and Richard G Bara-
niuk. Learned D-AMP: Principled neural network based
compressive image recovery. In Advances in Neural Infor-
mation Processing Systems (NIPS), pages 1770–1781, 2017.
2, 3, 4

[34] Minh Phuong Nguyen and Se Young Chun. Bounded Self-
Weights Estimation Method for Non-Local Means Image
Denoising Using Minimax Estimators.
IEEE Transactions
on Image Processing, 26(4):1637–1649, Feb. 2017. 3

[35] Dongwon Park, Kwanyoung Kim, and Se Young Chun. Efﬁ-
cient module based single image super resolution for multi-
ple problems. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops, pages 995–1003,
2018. 2, 3

[36] Lee C Potter, Emre Ertin, Jason T Parker, and M¨ujdat Cetin.
Sparsity and Compressed Sensing in Radar Imaging. Pro-
ceedings of the IEEE, 98(6):1006–1020, May 2010. 1

[37] Sathish Ramani, Thierry Blu, and Michael Unser. Monte-
Carlo SURE: A black-box optimization of regularization pa-
rameters for general denoising algorithms.
IEEE Transac-
tions on Image Processing, 17(9):1540–1554, 2008. 3

[38] Saiprasad Ravishankar and Yoram Bresler. MR image re-
construction from highly undersampled k-space data by dic-
tionary learning.
IEEE Transactions on Medical Imaging,
30(5):1028–1041, 2011. 1, 2, 6

[39] Shakarim Soltanayev and Se Young Chun. Training deep
learning based denoisers without ground truth data. In Ad-
vances in Neural Information Processing Systems (NIPS),
2018. 2, 3, 6

[40] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua
Bengio, and Pierre Antoine Manzagol. Stacked denoising
autoencoders: Learning Useful Representations in a Deep
Network with a Local Denoising Criterion. Journal of Ma-
chine Learning Research, 11:3371–3408, Dec. 2010. 2, 3

[41] Y Wiaux, L Jacques, G Puy, A M M Scaife, and P Van-
dergheynst. Compressed sensing imaging techniques for ra-
dio interferometry. Monthly Notices of the Royal Astronom-
ical Society, 395(3):1733–1742, May 2009. 1

[42] Junyuan Xie, Linli Xu, and Enhong Chen. Image denoising
In Advances in
and inpainting with deep neural networks.
Neural Information Processing Systems (NIPS), pages 341–
349, 2012. 2, 3

[43] Kai Xu, Zhikang Zhang, and Fengbo Ren. LAPRAN: A scal-
able Laplacian pyramid reconstructive adversarial network
for ﬂexible compressive sensing reconstruction.
In Euro-
pean Conference on Computer Vision (ECCV), pages 491–
507, 2018. 2

10264

