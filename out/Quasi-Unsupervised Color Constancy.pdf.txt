Quasi-Unsupervised Color Constancy

Simone Bianco

University of Milano-Bicocca

Claudio Cusano

University of Pavia

simone.bianco@unimib.it

claudio.cusano@unipv.it

Abstract

We present here a method for computational color con-
stancy in which a deep convolutional neural network is
trained to detect achromatic pixels in color images after
they have been converted to grayscale. The method does not
require any information about the illuminant in the scene
and relies on the weak assumption, fulﬁlled by almost all
images available on the Web, that training images have
been approximately balanced. Because of this requirement
we deﬁne our method as quasi-unsupervised. After training,
unbalanced images can be processed thanks to the prelimi-
nary conversion to grayscale of the input to the neural net-
work. The results of an extensive experimentation demon-
strate that the proposed method is able to outperform the
other unsupervised methods in the state of the art being, at
the same time, ﬂexible enough to be supervisedly ﬁne-tuned
to reach performance comparable with those of the best su-
pervised methods.

1. Introduction

Computational color constancy is a longstanding prob-
lem which consists in correcting images to make them ap-
pear as if they were taken under a neutral illuminant. Com-
putational color constancy can beneﬁt the solution to many
computer vision problems such as visual recognition [14],
surveillance [22], etc., where color is an important feature
for distinguishing objects. Despite its apparent simplicity,
this problem is very challenging for both human and com-
puter vision systems [25, 20].

The last decade has witnessed a remarkable improve-
ment in our capability of addressing many computer vision
problems. The main factor behind this is the development
of deep learning algorithms that make it possible to follow
a very effective data-driven approach [35].
It is not sur-
prising therefore that several attempts have been made to
exploit this machine learning paradigm for computational
color constancy as well [7, 37, 28, 38]. In our opinion, how-
ever, these approaches only partially took advantage of the
potential of deep learning.

The main difﬁculty in applying deep learning methods
to color constancy consists in the lack of large data sets
annotated with ground truth illuminants. In fact, data sets
used for this purpose are usually obtained by taking pic-
tures of scenes where a standard object of known chromatic
properties (e.g. a color target) has been placed. This pro-
cedure is clearly impractical for the collection of the large
data sets needed by supervised deep learning. Another issue
with color constancy methods based on machine learning is
that the learned models are often specialized for images ac-
quired with the same devices used to collect the training
set. Their application to images taken with other devices
requires some form of adaptation or retraining [2].

We present here a method for computational color con-
stancy that is based on a deep convolutional neural net-
work1.
The method leverages large datasets of pub-
licly available images to train the network in a quasi-
unsupervised setup. No ground truth about the color of
the illuminant is needed. Instead, the method exploits the
assumption that training images have been approximately
balanced either manually or by unspeciﬁed automatic pro-
cessing pipelines. Because of this assumption (that, as we
will see, is easily fulﬁlled in practice) we deﬁne our method
as “quasi-unsupervised”, instead of just “unsupervised”.

More in detail, the neural network is trained to detect
achromatic pixels. To do so, only a grayscale version of the
input image is taken into account. This way, the output is
independent on the actual color of the illuminant and, there-
fore, the network can be later applied to both balanced and
unbalanced images. The weighted average of the detected
pixels is the estimate of the illuminant that is ﬁnally used to
correct the input color image.

We veriﬁed the feasibility of the approach by training
several neural networks on three large datasets commonly
used for image recognition and retrieval. The evaluation
on two annotated datasets of raw images demonstrates that
very accurate results can be obtained even if no images from
those datasets were used for training.

The novel design of the proposed method determines re-

1Source code and trained models available at ‡ https://

claudio-unipv.github.io/quasi-unsupervised-cc/

112212

markable advantages over competing methods in the state
of the art: (i) the method exploits a complex neural network
architecture, without requiring a large training set of anno-
tated images; (ii) the trained model can be applied to unbal-
anced images acquired with any camera without the need
of any kind of adaptation. Despite the complexity of the
setup, the accuracy in estimating the illuminant compares
favorably with respect to those reported in the literature. In
particular, the proposed method is able to outperform the
other unsupervised methods in the state of the art. More-
over, it optionally supports a supervised ﬁne tuning on spe-
ciﬁc datasets that makes it possible to reach performance
comparable with those of the best supervised methods.

2. Related work

The computational color constancy methods in the state
of the art are usually divided into two categories: statistics-
based and learning-based. Methods in the former category
make assumptions about the statistical properties of natu-
ral scenes, and estimate the color of the illuminant as the
deviation from such assumptions [46]. Methods in the lat-
ter category estimate the color of the illuminant using a
model learned from training data. The great majority of
recent methods are learning-based, because this approach
allows to reach a generally higher accuracy with respect to
statistics-based methods. Many of these methods employ
models trained on handcrafted features extracted from the
input image, e.g [18, 11, 21, 41, 16], while the most recent
works learn features by using deep convolutional neural net-
works, e.g. [7, 37, 8, 44, 28].

The main difﬁculty in applying the above deep learning
methods consists in the lack of large scale data sets anno-
tated with ground truth illuminants. As a reference, the
largest data set available for color constancy is three orders
of magnitude smaller than those available for other com-
puter vision tasks such as visual class recognition [42]. Fur-
thermore, such methods tend to degrade their performance
when used in a cross-dataset setup, requiring a ﬁne-tuning
phase to adapt to the new dataset.

These reasons motivate the research of new algorithms
that do not need a dataset with annotated illuminant ground
truths, that we call unsupervised methods, and that produce
results comparable to learning-based methods in the state
of the art. To this end, we propose an alternative catego-
rization of color constancy algorithms into three different
classes: parametric, including methods that rely on a very
small set of parameters to be tuned, e.g. [4, 19, 46]; super-
vised, including methods that need a proper training phase,
e.g. [23, 12, 6]; unsupervised, including methods that do
not need an annotated dataset and can be readily applied to
new datasets without any form of adaptation, e.g. [34, 9].

In the following we review these latter works, which are
the most relevant to the aim of this paper. We refer the

readers to the surveys [26, 25] for additional background.

Interestingly, the ﬁrst color constancy algorithms that
have been proposed in the state of the art are unsupervised
algorithms. For example the White Point (or MaxRGB)
[34] algorithm assumes that the maximum values obtained
independently from each of the three color channels repre-
sents the color of the illumination. Gray World [9] is based
on the assumption that the average color in the image is
gray and that the illuminant color can be estimated as the
shift from gray of the averages in the image color chan-
nels. Recently Buzzelli et al. [10] proposed a deep learning
method that is not trained with illuminant annotations, but
with the objective of improving performance on an auxil-
iary task such as object recognition. The method therefore
learns to predict illuminant color in absence of any illumi-
nation ground truth data, but it requires label information
for the auxiliary task. Banic and Loncaric [3] proposed an
heuristic named green stability assumption, that can be used
to ﬁne-tune the values of the parameters of the statistics-
based methods by using only raw images without known
ground-truth illumination. In [2] an unsupervised learning-
based method is proposed that learns its parameter values
after approximating the unknown ground-truth illumination
of the training images. Therefore [2] and [3] do not need the
illuminant ground-truth information to be available, but re-
quire a raw training dataset. The same data is also needed by
Qian et al. [40] that proposed a statistical color constancy
method that relies on a novel gray pixel detection followed
by mean shift clustering.

3. Method

Computational color constancy is usually tackled in two
steps: ﬁrst the color of the illuminant is estimated, then the
estimate is used to correct the input image. In this work we
propose an illuminant estimation method based on a convo-
lutional neural network trained on large datasets of pictures.
The method is “quasi-unsupervised” since its training pro-
cess does not rely on the knowledge of the actual color of
the illuminant in the scene. The method, instead, is based
on the assumption that the training images have been ap-
proximately balanced by their owners before publication.
As a consequence of that, we expect that in most cases the
color of the illuminant appears close to gray. We deﬁne the
method as “quasi-unsupervised” (instead of just “unsuper-
vised”) because the assumed preliminary color correction
entails some form of weak supervision, even though such
a correction has not been performed explicitly to achieve
color constancy.

After the training, in order to make it possible to apply
the resulting model to unbalanced raw images, two main
issues need to be addressed: (i) these images will be of dif-
ferent kind with respect to those used for training, and (ii)
an actual ground truth will be available for the evaluation,

12213

Input

image x

Equalized
grayscale

Weights w

Achromatic

loss LA

P ·

Illuminant
estimate I

Figure 1. Schematic view of the proposed method. The input image x is ﬁrst converted to grayscale and then fed to a convolutional neural
network. Then, the color of the illuminant is estimated as the sum of the input RGB pixels, weighted by the output w of the network.
During training the estimate is evaluated with an achromatic loss function.

but not for training. We solve the ﬁrst issue by converting
the images to grayscale before passing them to the network,
so that they are almost independent on the color of the scene
illuminant. The lack of a ground truth is addressed by train-
ing the network to solve a problem that can be considered a
proxy of illuminant estimation: the detection of achromatic
pixels. An overview of the method is given in Figure 1,
details are explained in the following sections.

Once an estimate of the color of the illuminant has been
computed, it can be used to correct the input image. To do
so we apply the von Kries model [47], that consists in scal-
ing the color components of the pixels by the corresponding
components of the estimate.

3.1. Illuminant estimation

Many illuminant estimation methods are based on the
simple fact that the color of the illuminant directly con-
tributes to the color of the image pixels. These methods
expect that by averaging all or some of the pixels the corre-
sponding reﬂectances in the scene cancel out, leaving as a
result the color of the light source. Examples of this strat-
egy are the Gray World algorithm (that takes the average
over the whole image), the White Patch algorithm (that av-
erages a group of the brightest pixels), and the White Point
algorithm (that takes just the pixel with the highest level of
brightness).

Here we propose to train a convolutional neural network
to select which pixels should be used to estimate the color
of the illuminant. More precisely, the estimate will be a
weighted sum of the input pixels where the weights are the
output of the network. For a H × W input RGB image
x (xij ∈ R3) the network yields a weight map w (wij ∈
[0, 1]) which is used to compute the scene illuminant I as:

I = PH

i=1PW

j=1
Z

xijwij

,

(1)

where Z is a factor that normalizes the vector to make it
have unit Euclidean norm.

When fed with public images, taken from the web or
from datasets used by the computer vision community, in
accordance with the balancing assumption we want the net-
work to produce an achromatic estimate. The divergence
of the estimated color from the gray axis can then be used
as a loss function to train the network. We call this loss
the achromatic loss LA(I), and we deﬁne it in terms of
the cosine of the angle between the network estimate I =
(Ir, Ig, Ib)T and the gray axis:

LA(I) = 1 −

,

(2)

Ir + Ig + Ib

ǫ +q3(I 2

r + I 2

g + I 2
b )

where ǫ = 10−4 is a small quantity used to stabilize the
ratio. It can be easily shown that the loss is non-negative,
and that it is close to zero only when Ir ≃ Ig ≃ Ib.

The key idea is that by minimizing LA(I) the network
will learn to assign high weights to the pixels that are likely
to be achromatic. Of course this task would be trivial if the
network was able to see the input RGB image. To force
the network to learn a more elaborate strategy we feed it,
instead, with a gray-level version of the image. In fact, it
has already been shown that is possible to train a network
to predict colors from gray-level images (i.e. to perform an
automatic colorization [49]). Here, through the achromatic
loss, we implicitly train it to identify gray pixels.

The advantage of this approach is that what the network
learns can be used to estimate the illuminant even when the
balancing assumption does not hold. In particular it is pos-
sible to apply it to the unbalanced images that form the
datasets commonly used to evaluate color constancy algo-
rithms. For these images we expect that the network selects
pixels that appear with the same color of the illuminant be-
cause they are part of matte gray objects in the scene, of
highlights reﬂecting the light source, etc. In other words, by
making use of information that is substantially independent
on the color of the illuminant, we expect that the network
will learn to make accurate estimates for both balanced and
unbalanced images.

12214

However, to make this possible care has to be taken to
make the network work equally well for both the public bal-
anced images used for training and the unbalanced images
used for the evaluation. In fact, the two kind of images usu-
ally have a very different dynamic range, may have clipped
values or not, etc. Moreover, public images are likely to
be in the sRGB color space, while test images are in the
raw format of the acquisition device. To make their gray-
level versions comparable we compute them as the average
of the RGB color channels (that, with respect to more so-
phisticated conversions, makes as less assumptions as pos-
sible), and we apply a histogram equalization. Equalization
adaptively distorts the gray levels, reducing the differences
between images taken with different devices, or processed
by different pipelines.

Moreover, before their conversion to grayscale training
images, that are assumed to be in the sRGB color space, are
preliminary processed with a gamma removal to make their
pixels’ values linear with respect to energy:

c′ =(c/12.92

((c + 0.055)/1.055)2.4

if c ≤ 0.04045,
otherwise,

(3)

where c and c′ represent one of the three color channels
before and after the transformation [29].

A difﬁculty in minimizing LA(I) is that, due to the nor-
malization in (1), the estimate I is invariant under scalings
of w. A consequence of this invariance is that the neural
network is not encouraged to use the whole [0, 1] range for
w, since it can assign tiny weights to the pixels without
changing the ﬁnal estimate. This negatively affects the sta-
bility of the optimization algorithm. To push the network to
use larger weights we introduced an additional noise term
n ∈ R3 in Equation (1):

I =

n +PH

i=1PW

Z

j=1

xijwij

,

(4)

where the three components of n are normally distributed
with zero mean and σ2 variance. The larger the variance,
the larger the average weight assigned by the network to
make the contribution of noise negligible. The noise term
also acts as regularizer and is used only during training.

3.2. Extensions and variations

The proposed method is quite ﬂexible, and can easily ac-
commodate several variations. In particular, the grayscale
image can be replaced by, or combined with other informa-
tion, provided that it is independent on the color of the il-
luminant. We experimented with information derived from
the spatial gradient computed on each color channel. Since
the magnitude of the gradient is strongly correlated with
the color of the illuminant, we consider only the direction.

More precisely, for each color channel we compute the hor-
izontal and the vertical spatial derivatives by applying the
Sobel operators [45]. Then, the two derivatives are nor-
malized to form a unit-length vector. This procedure yields
a six-channel image (two derivatives times three channels)
that can be used as input of the neural network.

3.3. Supervised ﬁne tuning

Even though the main focus of this work is the quasi-
unsupervised setting, it is possible to adapt the method to
supervised learning as well. To do so it is sufﬁcient to re-
place the achromatic loss in (2) with a chromatic loss LC ,
deﬁned in terms of the cosine of the angle between the esti-
mated illuminant I and the target illuminant ˆI = ( ˆIr, ˆIg, ˆIb):

LC(I, ˆI) = 1−

Ir · ˆIr + Ig · ˆIg + Ib · ˆIb
r + ˆI 2

g + I 2

r + I 2

b )( ˆI 2

g + ˆI 2
b )

ǫ +q(I 2

. (5)

Supervised training of a deep learning model requires a
large dataset annotated with a suitable ground truth. Since
this is hard to achieve, we propose to follow a ﬁne tuning
procedure [36]. In this case, the parameters of the network
are initialized by the quasi-unsupervised learning on a large
dataset. Then the training process continues with super-
vised learning over a smaller annotated dataset with a small
learning rate.

3.4. Neural network architecture

The architecture of the neural network used in this work
is an adaptation of that proposed by Isola et al. [30] for
image-to-image translation. We chose to take inspiration
from that architecture because it demonstrated to be suit-
able for image colorization, a task that is somewhat related
to the detection of achromatic pixels.

The network takes as input a 256 × 256 graylevel image
(possibly augmented with gradient information) and pro-
duces as output a 256 × 256 weight map. The layers form
a U-shaped encoder/decoder with skip connections. There
are eight convolutions with kernel size 4 × 4 and stride 2
that are paired to eight deconvolutions (transposed convolu-
tions) with the same kernel size and stride. All these oper-
ations (with exception of the ﬁrst and the last) are followed
by a batch normalization and other non-linearities (leaky
ReLUs with slope 0.2 for convolutions and conventional
ReLUs for deconvolutions). For the last deconvolution the
ReLU is replaced by a sigmoid that yields the weights as-
signed to the input pixels. During training, the ﬁrst three de-
convolutional blocks include dropout with probability 0.5.
The entire net include about 54 millions of learnable param-
eters. Preliminary tests shown that simplifying the architec-
ture slightly reduces the ﬁnal accuracy.

12215

Input

Output

⊕

C64l

D1σ

⊕

C512bl

D512br

⊕

C128bl

D64br

C512bl

D512bdr

⊕

⊕

C256bl

D128br

C512bl

D512bdr

⊕

⊕

C512bl

D256br

C512bl

D512bdr

Figure 2. Structure of the neural network. In the diagram Ck de-
notes a convolution with k output channels. Similarly, Dk repre-
sents a deconvolution (transposed convolution). All convolutions
and deconvolutions have a kernel size of 4 × 4 and stride 2. The
other operations are denoted by: l → leaky ReLU, r → ReLU,
b → batch normalization, d → dropout, σ → sigmoid, ⊕ →
concatenation along channels. Operations in the same group are
executed from left to right.

4. Experimentation

The neural network has been trained by running 300 000
iterations of the Adam optimization algorithm [33]. The ob-
jective function was the achromatic loss, as deﬁned in Equa-
tion (4), with the standard deviation of the noise term set to
100. Each iteration analyzed a mini-batch of 16 images; the
learning rate was 10−4 and the coefﬁcient for weight decay
was 10−5. All the parameters have been empirically set on
the basis of a few preliminary experiments.

For deep learning applications the quality of training data
is of paramount importance.
In this work we decided to
adopt three large datasets widely used to train image recog-
nition and retrieval systems. The Ilsvrc12 is the dataset
that has been made publicly available for the Internet Large
Scale Visual Recognition Challenge [42] which probably
represents the most popular benchmark for image recog-
nition. The dataset consists of approximately 1.2 millions
samples taken from of 1000 different categories among
those collected for the ImageNET initiative [17].

The second dataset is Places365 [50] which includes
about 1.8 millions images representing 365 different cate-
gories of scenes. Images were obtained by querying several
search engines with terms taken from WordNet, and then
manually annotated. The main purpose of the dataset is to
serve as benchmark for scene recognition systems.

The last dataset we considered is the Flickr100k
dataset [39]. It consists of 100 071 images collected from

the Flickr photo sharing service by searching for the 146
most popular tags. The dataset has been collected to evalu-
ate image retrieval algorithms.

We chose three diverse datasets with the aim of assessing
how much the nature of the training images inﬂuences the
quality of the learned model. Ilsvrc12 and Places365 con-
sist of images taken from search engines, while Flickr100k
includes images from a single source.
Ilsvrc12 includes
many “object centric” images with little or no background,
while Places365 focuses on whole scenes.
Images from
Flickr100k seem, on average, of higher quality than those
from the other two datasets.

Figure 3 reports some examples of images from the
three datasets processed by the trained networks (images
have been taken from the validation sets of Ilsvrc12 and
Places365 and from the training set of Flickr100k). The
strategy followed in selecting the pixels for the estimation
of the illuminant can be inferred by looking at the weights.
The network often selects light sources such as lamps, the
sky or the sun. Windows are often selected in indoor scenes
with the light coming from outside. The network seems also
quite good in identifying highlights and surfaces diffusing
the light directly from the source. It is quite common the
case in which dark areas are selected: this is due to their
limited impact on the sum in Equation (1).

Figure 3 also shows that not all the images are well bal-
anced. Some of them present a strong non-neutral color cast
which is very evident in the case of sunsets and nighttime
images, and for some indoor images. However, the illumi-
nant estimate provided by the method seems coherent with
the content of the images. Even though we cannot quanti-
tatively evaluate the estimates due to the lack of a ground
truth, we can observe that the images balanced according to
the estimates look pleasantly natural. This suggests that the
network learned how to balance the outliers by modeling a
large number of “almost balanced” images.

4.1. Evaluation

The aim of the proposed method is to achieve a high
accuracy in estimating the color of the illuminant in un-
balanced pictures. To assess this we processed two dif-
ferent datasets of raw images commonly used to evaluate
color constancy algorithms. Both datasets contain high-
resolution photographs, representing scenes including a
color calibration target (the Macbeth ColorChecker). For
each image a ground truth illuminant has been computed by
analyzing the gray patches in the color target.

The ﬁrst test dataset is the Color Checker (CC), in the
variant reprocessed by Shi and Funt [23, 43].
It consists
of 568 images acquired with a Canon 1D, and a Canon
5D cameras. The second dataset has been collected by
a research group in the National University of Singapore
(NUS) [15] and includes 1853 images acquired with 9 dif-

12216

Ilsvrc12

Ilsvrc12

Ilsvrc12

Places365 Places365 Places365 Flickr100k Flickr100k Flickr100k

0.4

2.7

12.5

0.4

0.1

0.4

0.4

5.1

13.2

Figure 3. Examples of images from the training sets, processed by the trained networks. The top row shows the input images, each one with
superimposed a circle representing the estimated color of the illuminant. Inside the circle is reported the angular difference (in degrees)
between the estimate and the gray axis. The second row reports the weights assigned by the network to the pixels (blue → 0, yellow → 1).
The third row reports the images balanced with the estimated illuminant.

ferent cameras. As suggested by Hordley and Finlayson the
error metric we considered is the angle between the esti-
mated and the ground truth illuminants [27].

The results we obtained by applying the trained models
to the two test datasets are summarized in Table 1. The
mean and median angular errors are quite uniform over the
three training sets with differences of about 0.2 degrees or
less. This is very important since it demonstrates that the
type of photographs used for training is not of primary im-
portance.
It also suggests that our method is relying on
assumptions (i.e. that training images have been balanced)
that are easily met in practice.

For each combination of training and test datasets we
evaluated three variants differing in the kind of data pro-
cessed by the network.
The ﬁrst processes equalized
grayscale images, the second analyzes gradient directions,
and the third is based on a combination of the two. In all the
cases using gradient directions, alone or in combination, al-
lowed to obtain better results than using just the grayscale
image. For the CC dataset the lowest median angular error
has been obtained by the model trained on Ilsvrc12 using
both grayscale and directions. For NUS the best combina-
tion in terms of median angular error was to use the model
trained on Flickr100k using just the gradient directions. For
the rest of the experiments we consider as reference version
the one trained on Ilsvrc12 with grayscale and directions.

Figure 4 shows the result of processing some images
from the test sets. It can be noticed how, even in the case of
unbalanced images, the network selects meaningful regions
such as those representing the light sources or highlights.
Differently than with training images, this time the selected
pixels are not achromatic. They appear, instead, approxi-

Training set

Test set

Input

Mean Median Max

Ilsvrc12

Places365

Flickr100k

Ilsvrc12

Places365

Flickr100k

CC
CC
CC

CC
CC
CC

CC
CC
CC

NUS
NUS
NUS

NUS
NUS
NUS

NUS
NUS
NUS

Grayscale
Directions
Both

Grayscale
Directions
Both

Grayscale
Directions
Both

Grayscale
Directions
Both

Grayscale
Directions
Both

Grayscale
Directions
Both

4.04
3.67
3.46

4.01
3.43
3.60

4.09
3.70
3.59

3.14
2.97
3.00

3.24
2.91
3.07

3.27
2.95
2.98

2.67
2.53
2.23

2.60
2.38
2.45

2.67
2.48
2.25

2.24
2.15
2.27

2.32
2.24
2.20

2.38
2.12
2.16

27.88
17.62
21.17

27.72
18.31
21.47

27.09
20.86
20.04

22.39
15.89
19.16

22.66
16.05
17.12

21.28
16.40
15.86

Table 1. Statistics of angular errors (in degrees) obtained by varia-
tions of the proposed method on the CC and NUS datasets. Train-
ing has been performed on three datasets with different inputs:
equalized grayscale, gradient directions, and their combination.

mately with the color of the ground truth illuminant. As a
result, the images balanced according to the estimates ap-
pear as if they were taken under a neutral illuminant.

4.2. Fine tuning

When an annotated training set is available it is possible
to improve the performance of the neural network by ﬁne
tuning its parameters. This is done by continuing the train-

12217

CC

CC

CC

CC

NUS

NUS

NUS

NUS

NUS

5.2

0.2

10.4

5.3

2.7

2.8

3.9

0.9

0.6

Figure 4. Examples of test images processed by the network trained on ILSVRC12 (grayscale version). From top to bottom the rows show
the input image, the weights assigned to the pixels, and the images balanced according to the illuminant estimates. The colors of the circle
over the input images show the illuminant estimate (upper half) and the ground truth (lower half). The number in the circle is the angular
error expressed in degrees. For visualization purposes, input and balanced images have been gamma corrected.

Dataset

Mean

Median

Max

CC
NUS

2.91 (-0.55)
1.97 (-1.03)

1.98 (-0.25)
1.41 (-0.86)

19.9 (-1.2)
20.5 (+1.6)

Table 2. Statistics of angular errors (in degrees) obtained by the
network trained on Ilsvrc12 and ﬁne tuned on the two test datasets.
The values in brackets report the difference with respect to those
obtained in the quasi-unsupervised setting.

ing in a supervised way using a small learning rate. Here
we performed 250 000 additional iterations, with a learning
rate of 10−7, and without the noise term in Equation (4).
We repeated the experiment for both the Color Checker and
the NUS datasets. In both cases we evaluated the ﬁnal per-
formance with a three-fold cross validation.

Table 2 reports the results we obtained by ﬁne-tuning the
neural network trained on Ilsvrc12 processing the combina-
tion of grayscale image and gradient directions (for the sake
of brevity we omit the performance obtained by the other
variants). For both test datasets the mean and the median
angular error decreased. In the case of the NUS dataset the
improvement was particularly noticeable, with more than
one degree of difference in the mean error.

color constancy dataset on which is tested; “cross dataset”,
meaning that the method is trained/tuned on one color con-
stancy dataset and tested on a different one; “no dataset”,
meaning that the method is not trained/tuned on any color
constancy dataset.

From the results reported in Table 3 and Figure 5 it is
possible to notice that the proposed method is able to out-
perform all the purely unsupervised algorithms (i.e. Un-
supervised no-db) in the state of the art by a large margin
with a reduction of the median angular error by 37.9% and
9.6% on the CC and NUS datasets respectively, showing at
the same time a more stable performance across all the dif-
ferent cameras. In the cross-dataset setting, the proposed
method is able to outperform all the supervised methods.
Concerning parametric methods, our method outperforms
all of them on all the error statistics considered except for
the median error on NUS. Interestingly, parametric methods
have shown to perform better than supervised methods in
this setting. In the completely supervised setting, the ﬁne-
tuned version of the proposed method is able to outperform
all the parametric methods and to compete with the super-
vised ones obtaining the best mean error in the state of the
art on NUS, and the second best median.

4.3. Comparison with the state of the art

5. Conclusions

Table 3 reports the statistics of the angular errors for sev-
eral methods in the state of the art. The values have been
taken from the literature or obtained by executing publicly
available implementations. The methods are divided in un-
supervised, parametric and supervised ones. The three cat-
egories are further split into: “in dataset”, meaning that the
method is trained/tuned with cross validation on the same

We presented here a method for computational color
constancy which exploits a deep convolutional neural net-
work and leverages large unannotated datasets thanks to a
quasi-unsupervised learning procedure. We trained several
variants of the method differing in the kind of information
processed and in the training dataset. The experimental re-
sults showed that the proposed method is able to outperform

12218

Figure 5. Visual summary of the median errors of the proposed method, with and without ﬁne tuning, on CC, NUS and individual NUS
cameras. The method is compared with three groups of algorithms. From left to right: unsupervised (no-db), parameteric (cross-db) and
supervised (cross-db). For each group are drawn the best and worst median error, the interquartile range, the median ad the mean.

20mm

20mm

20mm

Method

Mean Med. Max Mean Med. Max

C1 C600

Color Checker

NUS

NUS median, camera-by-camera
Fuj. N52 Oly.

Pan.

Sam.

Unsupervised (in-db)

Unsupervised (no-db)

Parametric (in-db)

Parametric (cross-db)

Supervised (in-db)

Supervised (cross-db)

SoG [19] with GSA [3]
gGW [4] with GSA [3]
GE1[46] with GSA [3]
GE2[46] with GSA [3]
Banic and Loncaric [2]
WP [34]
GW [9]
Buzzelli et al. (gl. norm)[10]
Buzzelli et al. (ch. norm)[10]
Proposed

SoG [19]
gGW [4]
GE1 [46]
GE2 [46]
BP[32]
Cheng et al.[15]
Grey Pixel (edge) [48]
SoG [19]
gGW [4]
GE1 [46]
GE2 [46]
Qian et al. [40]

Bayesian [23]
Spatio-Spectral (ML) [13]
Spatio-Spectral (GP) [13]
Natural Image Statistics [24]
Exemplar-based [31]
Chakrabarti (Empirical)[12]
Chakrabarti (End-to-end)[12]
Cheng et al. [16]
Color Dog [1]
Bianco et al. [8]
FFCC [6]
Oh and Kim [38]
CCC (dist+ext) [5]
FC4(AlexNet) [28]
DS-Net (HypNet+SelNet) [44]
Proposed + Fine Tuning
Bayesian [23]
Exemplar-based [31]
Chakrabarti (Empirical) [12]
Chakrabarti (End-to-end) [12]
Cheng et al. [16]
FFCC [6]

4.05
4.05
4.03
4.13

5.97
4.76
4.84
5.48
3.46

3.85
4.12
4.06
4.18
3.98
3.52
4.60
6.08
4.66
4.06
4.26
3.65

4.70
3.55
3.47
4.09
2.89
2.89
2.56
2.42

2.36
1.78
2.16
1.95
1.77
1.90
2.91
4.75
6.50
3.87
3.89
5.52
3.91

21.87
21.15
18.89
17.78

45.00
24.92
20.80
19.88
21.17

20.89
22.51
23.05
24.05

28.35

37.24
31.59
23.05
23.45
26.12

16.98
16.25

19.9

2.54
2.58
3.08
3.34

3.74
3.59
4.12
4.81
2.23

2.43
2.52
2.67
2.68
2.61
2.14
3.10
3.85
2.84
2.67
2.82
2.38

3.44
2.93
2.90
3.13
2.27
1.89
1.67
1.65
1.49
1.44
0.96
1.47
1.22
1.11
1.12
1.98
3.11
5.10
3.25
3.10
4.52
3.15

3.31
3.45
3.18
3.41
2.96
3.57
4.17
4.88
4.32
3.00

3.42
3.37
3.18
3.19

3.02
3.15
3.44
3.53
3.18
3.53
3.16

1.99
2.41
2.38
2.12
2.24
1.97
3.65

3.49
3.52
4.86
3.19

2.58
2.68
2.48
2.52
1.70
2.49
3.17
4.17
3.37
2.25

2.45
2.49
2.18
2.18
2.48
2.12
2.20
2.59
2.71
2.18
2.62
2.15

2.81
2.54
2.39
2.69

1.58
1.76
1.77
1.34
2.15
1.48
1.53
1.46
1.41
3.08

2.87
2.71
4.40
2.33

21.01
22.48
24.16
31.21

26.87
22.34
18.70
22.36
19.16

26.27
23.73
21.81
24.29

23.28

18.40
19.87
21.81
23.00
21.93

19.80

2.37
2.45
2.26
2.27
1.79
2.28
3.84
4.12
3.18
2.27

2.28
2.43
2.37
2.29
2.45
2.01

2.73
2.71
2.37
2.92
2.22

2.80
2.80
2.67
3.04

1.57
1.72
1.71
1.34
2.18

2.57
2.61
2.72
2.59
1.66
2.24
3.13
4.00
3.15
2.09

2.24
2.35
2.00
1.86
2.48
1.89

2.43
2.53
2.00
2.62
2.07

2.35
2.32
2.03
2.46

1.62
1.85
1.85
1.33
1.75

2.38
2.61
2.33
2.39
1.72
2.74
3.29
3.43
3.06
2.24

2.69
2.50
2.06
2.13
2.67
2.15

2.70
2.81
2.06
2.58
1.91

3.20
2.70
2.45
2.95

1.58
1.81
1.75
1.35
2.75

2.54
2.59
2.70
2.66
1.70
2.48
3.39
4.19
3.08
2.21

2.36
2.29
2.23
2.16
2.30
2.08

2.58
2.67
2.23
2.62
2.18

3.10
2.43
2.26
2.40

1.65
1.94
1.88
1.45
2.00

2.51
2.71
2.72
2.71
1.71
2.07
2.63
3.83
3.06
2.36

2.17
2.50
1.98
1.94
2.18
1.87

2.47
2.62
1.98
2.63
2.06

2.81
2.24
2.21
2.17

1.41
1.46
1.65
1.16
2.22

2.51
2.77
2.62
2.61
1.63
2.62
3.07
3.87
3.26
1.98

2.42
2.39
2.02
2.01
2.15
2.02

2.41
2.56
2.02
2.33
2.11

2.41
2.28
2.22
2.28

1.61
1.69
1.59
1.28
1.53

2.51
2.63
2.33
2.63
1.61
2.61
2.98
4.37
3.77
2.01

2.53
2.61
2.13
2.07
2.49
2.03

2.41
2.48
2.13
2.34
2.14

3.00
2.51
2.29
2.77

1.78
1.89
1.88
1.47
1.65

Son. N40

2.54
2.55
2.36
2.45
1.60
2.44
2.94
4.34
3.02
2.27

2.39
2.58
2.34
2.31
2.62
2.33

2.68
2.75
2.34
2.66
2.14

2.36
2.70
2.58
2.88

1.48
1.77
1.63
1.35
3.11

2.46
2.66
2.36
2.37

3.67
3.50
5.37
4.76
2.97

3.71
3.69
2.70
2.72
3.13
2.72

2.94
3.14
2.70
3.20
3.05

3.53
2.99
2.89
3.51

2.00
1.36
2.68

20.50

1.59

1.26

1.34

1.52

1.35

1.29

1.30

1.52

1.84

2.39
2.18

3.03
2.42

3.72
3.01

3.07
3.17

4.30
3.29

2.00
2.33

3.15
3.13

3.92
4.32

Table 3. Performance comparison with the state of the art in terms of angular error on the CC and NUS datasets.

the other unsupervised methods in the state of the art, being
at the same time ﬂexible enough to be supervisedly ﬁne-
tuned on a speciﬁc dataset reaching performance compara-
ble with those of the top supervised methods. In this work
we focused on the quasi-unsupervised setting.
In the fu-
ture we plan to explore more thoroughly the supervised ﬁne
tuning step, possibly by experimenting with more complex

techniques taken from the literature on transfer learning and
on domain adaptation.

Acknowledgment

We gratefully acknowledge the support of NVIDIA Cor-
poration with the donation of the Titan Xp GPU used for
this research.

12219

References

[1] Nikola Banic and Sven Loncaric. Color dog-guiding the
global illumination estimation to better accuracy. In Inter-
national Conference on Computer Vision Theory and Appli-
cations, pages 129–135, 2015. 8

[2] Nikola Bani´c and Sven Lonˇcari´c. Unsupervised learning for
color constancy. arXiv preprint arXiv:1712.00436, 2017. 1,
2, 8

[3] Nikola Bani´c and Sven Lonˇcari´c. Green stability assump-
tion: Unsupervised learning for statistics-based illumination
estimation. arXiv preprint arXiv:1802.00776, 2018. 2, 8

[4] Kobus Barnard, Vlad Cardei, and Brian Funt. A compari-
son of computational color constancy algorithms. ii: Exper-
iments with image data. IEEE Transactions on Image Pro-
cessing, 11(9):985–994, 2002. 2, 8

[5] Jonathan T Barron. Convolutional color constancy. In IEEE
International Conference on Computer Vision, pages 379–
387, 2015. 8

[6] Jonathan T Barron and Yun-Ta Tsai. Fast fourier color con-
stancy. In IEEE Conference on Computer Vision and Pattern
Recognition, 2017. 2, 8

[7] Simone Bianco, Claudio Cusano, and Raimondo Schettini.
Color constancy using cnns. In IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops, pages 81–
89, 2015. 1, 2

[8] Simone Bianco, Claudio Cusano, and Raimondo Schettini.
Single and multiple illuminant estimation using convolu-
tional neural networks.
IEEE Transactions on Image Pro-
cessing, 26(9):4347–4362, 2017. 2, 8

[9] Gershon Buchsbaum. A spatial processor model for ob-
Journal of the Franklin institute,

ject colour perception.
310(1):1–26, 1980. 2, 8

[10] Marco Buzzelli, Joost van de Weijer, and Raimondo Schet-
tini. Learning illuminant estimation from object recognition.
arXiv preprint arXiv:1805.09264, 2018. 2, 8

[11] Vlad C Cardei, Brian Funt, and Kobus Barnard. Estimat-
ing the scene illumination chromaticity by using a neural
network.
Journal of the Optical Society of America A,
19(12):2374–2386, 2002. 2

[16] Dongliang Cheng, Brian Price, Scott Cohen, and Michael S
Brown. Effective learning-based illuminant estimation using
simple features.
In IEEE Conference on Computer Vision
and Pattern Recognition, pages 1000–1008, 2015. 2, 8

[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 248–255, 2009. 5

[18] Graham D Finlayson, Steven D Hordley, and Paul M Hubel.
Color by correlation: A simple, unifying framework for color
constancy. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 23(11):1209–1221, 2001. 2

[19] Graham D Finlayson and Elisabetta Trezzi. Shades of gray
and colour constancy.
In Color and Imaging Conference,
volume 2004, pages 37–41. Society for Imaging Science and
Technology, 2004. 2, 8

[20] David H Foster.

Color constancy.

Vision research,

51(7):674–700, 2011. 1

[21] Brian Funt and Weihua Xiong. Estimating illumination chro-
maticity via support vector regression. In Color and Imaging
Conference, volume 2004, pages 47–52. Society for Imaging
Science and Technology, 2004. 2

[22] Hiren Galiyawala, Kenil Shah, Vandit Gajjar, and Mehul S
Raval. Person retrieval in surveillance video using height,
color and gender. arXiv preprint arXiv:1810.05080, 2018. 1
[23] Peter Vincent Gehler, Carsten Rother, Andrew Blake, Tom
Minka, and Toby Sharp. Bayesian color constancy revisited.
In IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 1–8. IEEE, 2008. 2, 5, 8

[24] Arjan Gijsenij and Theo Gevers. Color constancy using natu-
ral image statistics and scene semantics. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 33(4):687–
698, 2011. 8

[25] Arjan Gijsenij, Theo Gevers, Joost Van De Weijer, et al.
Computational color constancy: Survey and experiments.
IEEE Transactions on Image Processing, 20(9):2475–2489,
2011. 1, 2

[26] Steven D Hordley.

present, and future.
31(4):303–314, 2006. 2

Scene illuminant estimation: past,
Color Research & Application,

[12] Ayan Chakrabarti. Color constancy by learning to predict
chromaticity from luminance. In Advances in Neural Infor-
mation Processing Systems, pages 163–171, 2015. 2, 8

[27] Steven D. Hordley and Graham D. Finlayson. Reevaluation
of color constancy algorithm performance. Journal of the
Optical Society of America A, 23(5):1008–1020, 2006. 6

[13] Ayan Chakrabarti, Keigo Hirakawa, and Todd Zickler. Color
constancy with spatio-spectral statistics. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 34(8):1509–
1519, 2012. 8

[14] Yu-Hsiu Chen, Ting-Hsuan Chao, Sheng-Yi Bai, Yen-Liang
Lin, Wen-Chin Chen, and Winston H Hsu. Filter-invariant
image classiﬁcation on social media photos. In ACM inter-
national conference on Multimedia, pages 855–858, 2015.
1

[15] Dongliang Cheng, Dilip K Prasad, and Michael S Brown. Il-
luminant estimation for color constancy: why spatial-domain
methods work and the role of the color distribution. Journal
of the Optical Society of America A, 31(5):1049–1058, 2014.
5, 8

[28] Yuanming Hu, Baoyuan Wang, and Stephen Lin. Fc4: Fully
convolutional color constancy with conﬁdence-weighted
pooling. In IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 4085–4094, 2017. 1, 2, 8

[29] Multimedia systems and equipment — colour measurements
and management — part 2-1: Colour management — de-
fault RGB color space — sRGB’. Standard, International
Electrotechnical Commission, IEC, 1999. 4

[30] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-
In IEEE Conference on Computer Vision

Efros.
sarial networks.
and Pattern Recognition, pages 5967–5976, 2017. 4

[31] Hamid Reza Vaezi Joze and Mark S Drew. Exemplar-
IEEE

based color constancy and multiple illumination.

12220

[47] J von Kries. Chromatic adaptation, festschrift der albercht-

ludwig-universit¨at, 1902. 3

[48] Kai-Fu Yang, Shao-Bing Gao, and Yong-Jie Li. Efﬁcient il-
luminant estimation for color constancy using grey pixels. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2254–2263, 2015. 8

[49] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
image colorization. In European Conference on Computer
Vision, pages 649–666, 2016. 3

[50] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 2017. 5

Transactions on Pattern Analysis and Machine Intelligence,
36(5):860–873, 2014. 8

[32] Hamid Reza Vaezi Joze, Mark S Drew, Graham D Finlayson,
and Perla Aurora Troncoso Rey. The role of bright pixels in
illumination estimation. In Color and Imaging Conference,
volume 2012, pages 41–46. Society for Imaging Science and
Technology, 2012. 8

[33] Diederik P Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In International Conference on
Learning Representation, 2015. 5

[34] Edwin H Land and John J McCann. Lightness and retinex
theory. Journal of the Optical Society of America A, 61(1):1–
11, 1971. 2, 8

[35] Y LeCun, Y Bengio, and G Hinton. Deep learning. Nature,

521(7553):436–444, 2015. 1

[36] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 40(12):2935–2947, 2017. 4

[37] Zhongyu Lou, Theo Gevers, Ninghang Hu, Marcel P Lu-
cassen, et al. Color constancy by deep learning. In BMVC,
pages 76–1, 2015. 1, 2

[38] Seoung Wug Oh and Seon Joo Kim. Approaching the
computational color constancy as a classiﬁcation problem
through deep learning. Pattern Recognition, 61:405–416,
2017. 1, 8

[39] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and
Andrew Zisserman. Object retrieval with large vocabularies
and fast spatial matching. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 1–8, 2007. 5

[40] Yanlin Qian, Ke Chen, Jarno Nikkanen, Joni-Kristian
K¨am¨ar¨ainen, and Jiri Matas. Revisiting gray pixel for sta-
tistical illumination estimation. In International Conference
of Computer Vision Theory and Applications, 2019. 2, 8

[41] Charles Rosenberg, Martial Hebert, and Sebastian Thrun.
Color constancy using kl-divergence. In IEEE International
Conference on Computer Vision, volume 1, pages 239–246.
IEEE, 2001. 2

[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpa-
thy, Aditya Khosla, Michael Bernstein, Alexander C. Berg,
and Li Fei-Fei.
ImageNet Large Scale Visual Recogni-
tion Challenge. International Journal of Computer Vision,
115(3):211–252, 2015. 2, 5

[43] Lilong Shi and Brian Funt.

Shi’s re-processed ver-
sion of the gehler color constancy dataset of 568 im-
ages. http://www.cs.sfu.ca/˜colour/data, Ac-
cessed: 12/11/2018. 5

[44] Wu Shi, Chen Change Loy, and Xiaoou Tang. Deep special-
ized network for illuminant estimation. In European Confer-
ence on Computer Vision, pages 371–387. Springer, 2016. 2,
8

[45] Irwin Sobel and Gary Feldman. A 3 × 3 isotropic gradient
operator for image processing. Talk at Stanford Artiﬁcial
Intelligence Project (SAIL), 1968. 4

[46] Joost Van De Weijer, Theo Gevers, and Arjan Gijsenij. Edge-
based color constancy. IEEE Transactions on image process-
ing, 16(9):2207–2214, 2007. 2, 8

12221

