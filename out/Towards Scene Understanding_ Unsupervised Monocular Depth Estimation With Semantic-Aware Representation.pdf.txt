Unsupervised Monocular Depth Estimation with Semantic-aware Representation

Towards Scene Understanding:

Po-Yi Chen1

3∗

,

Alexander H. Liu1

3∗

,

1National Taiwan University

Yen-Cheng Liu2
2Georgia Institute of Technology

Yu-Chiang Frank Wang1

3

,

3MOST Joint Research Center for AI Technology and All Vista Healthcare

pychen0@ntu.edu.tw, r07922013@ntu.edu.tw, ycliu@gatech.edu, ycwang@ntu.edu.tw

Abstract

Monocular depth estimation is a challenging task in
scene understanding, with the goal to acquire the geomet-
ric properties of 3D space from 2D images. Due to the lack
of RGB-depth image pairs, unsupervised learning methods
aim at deriving depth information with alternative supervi-
sion such as stereo pairs. However, most existing works fail
to model the geometric structure of objects, which gener-
ally results from considering pixel-level objective functions
during training. In this paper, we propose SceneNet to over-
come this limitation with the aid of semantic understanding
from segmentation. Moreover, our proposed model is able
to perform region-aware depth estimation by enforcing se-
mantics consistency between stereo pairs.
In our experi-
ments, we qualitatively and quantitatively verify the effec-
tiveness and robustness of our model, which produces fa-
vorable results against the state-of-the-art approaches do.

1. Introduction

With the development of robotics and autonomous driv-
ing, scene understanding has become a crucial yet challeng-
ing problem. One goal of scene understanding is to recog-
nize and analyze 3D geometric information from a 2D scene
image. Toward this end, several methods [5, 14, 12] attempt
to estimate depth information from a monocular image by
learning a supervised regression model with a great amount
of 2D-3D image pairs or multiple observations from dif-
ferent viewpoints. However, as most supervised learning
methods, collecting ground truth data is costly and time-
consuming. Thus, recent works attempted to learn unsuper-
vised depth estimation models based on either stereo image
pairs [8] or video sequences [27].

Most unsupervised depth estimation methods derive

∗ Indicates equal contribution.

Figure 1: Integrating depth estimation and semantic seg-
mentation towards scene understanding. With image repre-
sentation jointly learned from the above objectives preserv-
ing geometric/semantic information, unsupervised depth es-
timation can be realized.

depth information by reconstructing the geometric structure
of a scene, while in addition to the geometric cue, we human
estimate depth information according to semantic informa-
tion of a scene. For example, we know that pixels labeled as
“sky” must accompany with large values of depth. Further-
more, the depth values of the pixels within a segmentation
mask (i.e., an object) should be close and relative, and sig-
niﬁcant changes of depth between adjacent pixels implicitly
indicate the boundary of an object. Based on these proper-
ties, several works [13, 20, 4, 18] have explored to mutually
positive transfer between semantic segmentation and depth
estimation, while the requirement of pairwise depth and se-
mantic labels limits the applicability of these models.

In this paper, we ﬁrst point out the current state-of-the-
arts like [8] predict the disparity maps for stereo views only
based on one monocular image. This results in unaware-
ness of structural information from the other view in the in-
ference stage and further affects the performance of dispar-
ity prediction. With the proposed SceneNet, the mismatch-
ing problem can be signiﬁcantly alleviated by our training

12624

strategy. We will verify our design is more reasonable by
comparing the performances with the state-of-the-art unsu-
pervised depth estimation.

More importantly, our model further achieves improved
depth estimation by leveraging semantic understanding.
Fig. 1 illustrates the idea of SceneNet to learn semantic-
aware scene representation to advance our depth estimation.
SceneNet is an encoder-decoder based network that takes
scene images and encodes them into representations. The
decoder acts as a multi-task yet shared classiﬁer that trans-
forms scene representation into the prediction of depth or
segmentation. This is accomplished by a unique task iden-
tity mechanism, which allows the shared decoder to switch
the outputs between semantic segmentation and depth es-
timation. Base on the conditioned task identity infor-
mation, SceneNet thus can be viewed as a cross-modal
network model, bonding depth and segmentation modali-
ties together. To further strengthen the bonding between
geometric and semantic understanding, we introduce left-
right semantic consistency and semantics-guided disparity
smoothness, two self-supervised objective functions that re-
ﬁne depth estimation with semantic prediction.

In our experiments, we demonstrate that SceneNet not
only produces satisfactory results on depth estimation, its
integration of geometric and semantic information also re-
alizes general scene understanding. With a small amount of
data with annotated semantic ground truth labels, our model
gains signiﬁcant improvement over depth estimation.

We highlight the contributions of our work as follow:

• We point out possible mismatch problems in recent un-
supervised monocular depth estimation methods utiliz-
ing left-right consistency.

• Our proposed SceneNet work towards scene under-
standing by integrating both geometric and seman-
tic information, with our proposed modules preserv-
ing task identity, left-right semantic consistency and
semantics-guided disparity smoothness.

• The end-to-end learning procedure allows our model
to learn from disjoint cross-modal datasets of stereo
images and semantically labeled images.

• In our experiments, we qualitatively and quantitatively
verify the effectiveness and robustness of our model
over state-of-the-art methods on benchmark datasets.

2. Related Work

Depth Estimation

Generally, depth information can be represented in an ab-
solute depth value or a disparity value (the former is in-
versely proportional to the latter). Traditional methods re-

lied on additional observations such as multi-view from sev-
eral cameras [21] and motion cue from video frames [9]
to derive the corresponding depth of a scene. With only a
single monocular image during the inference stage, Liu et
al. [14] used a deep convolution neural network and con-
tinuous condition random ﬁeld as patch-wise depth predic-
tor to estimate the depth information. Eigen et al. [5] in-
corporated the coarse and ﬁne cues to predict the depth
map. With sparse ground-truth depth map, Kuznietsovet et
al. [12] learned to predict the dense depth map in a semi-
supervised manner. Although promising results were re-
ported, their requirement of a large amount of pixel-level
annotation and lack to ability in handling noisy depth sen-
sory data would be concerns.

On the other hand, unsupervised depth estimation meth-
ods rely on the supervision from either stereo image
pairs [6, 8, 25] or video sequences [27, 19, 16, 23, 25]. With
the stereo images in the training stage, Garg et al. [6] ap-
plied the inverse warping loss to learn a monocular depth
estimation CNN. Godard et al. [8] inferred the disparities
by warping the left-viewpoint image to match the right-
viewpoint one (and vice versa) with a left-right consistency
objective function. As noted previously, the derived dispari-
ties map could later be converted into the depth map. On the
other hand, some works [27, 19] explored image sequences
and proposed the temporal temporal photometric warp loss
between the adjacent frames to derive the depth informa-
tion. Mahjourian et al. [16] similarly used temporal consis-
tency and further imposed more 3D geometric constraints.
Yin et al. [23] learned depth information together with opti-
cal ﬂow and camera pose by taking advantage of the nature
of 3D scene geometry. Zhan et al. [25] further proposed
spatial and temporal warp objective function for learning
the depth map using both temporal and stereo views.

Leveraging Semantic Segmentation

Since monocular depth estimation methods rely heav-
ily on the property of perspective geometry or annotated
ground truth, seeking assistance from semantic segmenta-
tion of image has been an inevitable direction of research.
Prior works [13, 20, 4, 18] explored the possibility to com-
bine supervised depth estimation and semantic segmen-
tation with multi-task learning. Either through a hierar-
chical network, multi-stage training or sharing latent fea-
ture, they all found the two tasks are indeed strongly cor-
Jiao et al. [10] studied
related and mutually beneﬁcial.
the long tail property of the distribution of depth and im-
proved supervised depth estimation with attention and se-
mantic segmentation. Zhang et al. [26] proposed a joint
task-recursive learning framework to recursively reﬁne the
results of both semantic segmentation and supervised depth
estimation through serialized task-level interactions. Chen
et al. [2] proposed a self-supervised proxy task predicting

2625

Figure 2: Architecture of our proposed SceneNet. SceneNet takes an image I as input and encodes it into a scene represen-
tation z. This representation can be decode into the output ˜Y along with the introduced task identity layer t. Based on the
conditioned t, ˜Y can later be transformed into pixel-wise prediction of semantic segmentation output s or depth estimation
output d, while these two outputs would be properly aligned based on the corresponding semantic information.

relative depth for urban scenes, which can then be adapted
to semantic segmentation or depth estimation through ﬁne-
tuning model (with ground truth provided).

While these prior works were closely related to ours in
terms of pursuing a more general scene understanding for
cross depth estimation and segmentation, we state the dif-
ference between our work and the previous works as follow-
ing: First, unlike the aforementioned works, we choose to
build a uniﬁed model to jointly exploit both tasks. Second,
our method does not require paired training data to learn
shared scene representation for depth estimation and seman-
tic segmentation (i.e., training data of these two tasks can be
completely disjoint). Third, depth estimation remains unsu-
pervised with our proposed model, we do not use any given
disparity map or sparse ground truth. Last, while learning
shared representations for different downstream tasks, our
approach remains end-to-end trainable. Neither pre-training
nor ﬁne-tuning model is required.

3. Proposed Method

The goal of our proposed model, SceneNet, is to pre-
dict a dense depth map directly from a monocular image.
During training, our model is trained on stereo pairs and
RGB-segmentation pairs. Unlike existing multi-task learn-
ing models like [13, 20, 4, 18], our model does not re-
quire the stereo images and semantic-annotated images to
be paired.

a task identity t (detailed in Sect. 3.1) as input, and out-
puts the cross-modal prediction ˜Y . To train SceneNet, we
apply the objective functions for unsupervised depth pre-
diction and supervised semantic segmentation in Sect. 3.2.
Later in Sect. 3.3, we reﬁne the cross-modal prediction by
introducing two self-supervised objective functions – left-
right semantic consistency and semantic grounded disparity
smoothness. In Sect. 3.4, we summarize the learning objec-
tive and detail the inference procedure of SceneNet.

3.1. Task Identity for Cross modal Prediction

Most existing works that jointly learn disparity estima-
tion and semantic segmentation use task-speciﬁc classiﬁca-
tion/regression sub-networks to obtain disparity maps and
segmentation masks. However, hyper-parameters such as
the number of sharing/non-sharing layers across different
branches are required to be tuned and decided according to
the task shift. This restricts the practicality of the model,
especially when adapting to different datasets.

To address the limitation, we merge cross-modal predic-
tions by utilizing a uniﬁed decoder conditioned on a task
identity t (as shown in Fig. 2). In practice, we set the task of
disparity estimation as t = 1 and task of semantic segmen-
tation as t = 0. Our decoder further generates the cross-
modal prediction ˜Y from the scene representation z and the
task identity t:

˜Y = D(δ(z, t)),

(1)

As illustrated in Fig. 2, the encoder of our model ﬁrst
converts a scene image I into a scene representation z. Our
decoder further takes both the scene representation z and

where δ is a operation of concatenation and D is our cross-
modal decoder with no activation function in last layer.

Speciﬁcally, the semantic segmentation prediction s (red

2626

lines in Fig. 2) is computed as:

s = σc( ˜Ys),

(2)

where ˜Ys = D(δ(z, t = 1)) and σc is a softmax function.
The disparity map prediction d (green lines in Fig 2) is de-
rived as:

d = σb(fµ( ˜Yd)),

(3)

where ˜Yd = D(δ(z, t = 0)), fµ refers to pixel-wise average
pooing and σb is the sigmoid function.

Note that since ˜Y is conditioned on the task identity t,
our model is able to arbitrarily switch the output between
different tasks by assigning a different value to t. We note
that the use of a uniﬁed decoder allows sharing geometric
and semantic information across different modalities and
contributes positive transfer for both tasks. We would later
verify the effectiveness of this uniﬁed decoder in our exper-
iments.

3.2. Depth Estimation & Semantic Segmentation

Unsupervised Depth Estimation Inspired by existing un-
supervised models on depth estimation [6, 8], we utilize the
stereo image pairs I l, I r as supervision during training in
order to derive a disparity map from a monocular image in
inference stage.

Given an RGB monocular image, our model predicts a
pixel-wise disparity map, which is used to warp an image
from one viewpoint to another. To be more speciﬁc, we
input left-view image I l and predicts its corresponding dis-
parity map dl, which is applied to warp the right-view image
I r and reconstruct the left-view image I r→l.

To learn our disparity prediction model, we compute the

image reconstruction loss Lre with element-wise L1 loss:

Lre = (cid:13)

(cid:13)I l − I r→l(cid:13)

(cid:13) + (cid:13)

(cid:13)I r − I l→r(cid:13)
(cid:13) .

(4)

where I r→l is obtained from warping the right image I r
based on the left-view disparity dl.

To further match the consistency between right and left
disparities and maintain the smoothness of predicted dis-
parity maps, we apply the left-right disparity consistency
loss and disparity smoothness loss introduced by Godard
et al. [8]. Thus, our entire objective function for learning
depth estimation can be deﬁned as:

Ldepth = Lre + αlr(cid:0) (cid:13)

(cid:13)dr − dl→r(cid:13)
(cid:13) (cid:1)
+ αds(cid:0) k∂xdk e−k∂xdk + k∂ydk e−k∂y dk(cid:1),

(cid:13)dl − dr→l(cid:13)

(cid:13) + (cid:13)

(5)

where αlr and αds are the weights for the associated terms.
Note that dr→l can be obtained by warping right-view
disparity dr according to left-view disparity dl (similar
remarks can be applied to dl→r).

Figure 3: Model design differences between [8] and ours.
Note that [8] predicts both disparity maps dl and dr given
only the input left-view image I l, causing dr to align with I l
instead of I r, the mismatching problem therefore arises. We
predict a disparity map given the input image, and advance
the same warping techniques to preserve left-right predic-
tion consistency via image ﬂipping. This not only avoids
possible mismatch but also simpliﬁes the learning process.

The Mismatching Problem It is worth noting that Godard
et al. [8] predicts both disparity maps dl and dr from one
input image I l as shown in Fig. 3. We show that this might
not properly maintain the structural alignment between the
right-view RGB image I r and the right-view disparity map
dr. This is because that, without the structural and textural
information of right-view image I r, it would be difﬁcult to
accurately estimate the right-view disparity dr from a single
left-view image I l.

Instead of predicting both disparity maps from a single
view, we choose to output only one disparity map which
aligns with the input image. To obtain the right disparity
map dr, we horizontally ﬂip the right-view image I r.

Supervised Semantic Segmentation Existing depth esti-
mation methods generally focus on pixel-wise disparity es-
timation [6, 8, 25] and regard all pixels within an image as
spatial homogeneity, which would lead to unfavorable dis-
parity estimation along object boundaries. To overcome the
limitation, we perform disparity estimation by leveraging
semantics information from segmentation-image pairs. We
thus deﬁne the semantic segmentation loss Lseg as:

Lseg = H(sgt, s),

(6)

where H indicates the cross-entropy loss and sgt denotes
the ground truth labels from additional disjoint dataset.

2627

3.3. Self supervised Learning of SceneNet

4. Experiments

To reinforce the semantic awareness when estimat-
ing disparity, we further introduce two self-supervised
regularization losses, left-right semantic consistency and
semantics-guided disparity smoothness.

Left-Right Semantic Consistency In Sect. 3.2, we con-
sider the left-right consistency loss between RGB stereo im-
age pairs. However, such consistency over the color value
of each pixel is likely to be affected by optical changes be-
tween left-right views. For instance, specular reﬂection on
a glass would vary across different viewpoints. To mitigate
the problem, we further observe such left-right consistency
at the semantic level, since semantic segmentation is less
sensitive to optical changes.

By replacing stereo image I r and I l in (4) into their se-
mantic segmentation sr and sl, the left-right semantic con-
sistency can be deﬁned as:

Llrsc = (cid:13)

(cid:13)sl − sr→l(cid:13)

(cid:13) + (cid:13)

(cid:13)sr − sl→r(cid:13)
(cid:13) ,

(7)

where sr→l can be obtained by warping sr according to dl
and we follow the same rule to obtain sl→r.

Semantics-Guided Disparity Smoothness In addition to
left-right semantic consistency, we also regularize the
smoothness of disparity values within each segmentation
mask. This semantics-guided disparity smoothness is de-
ﬁned as:

Lsmooth = kd − f7→(d)k⊗(1−kψ(s) − f7→(ψ(s))k), (8)

where ψ is the operation which sets the maximize value
along each channel as 1 and sets the remaining values as
0, ⊗ denotes element-wise multiplication, and f7→ is the
operation of shifting input one pixel along the horizontal
axis. The second term is similar to applying an edge detec-
tor to identify edges of segmentation masks. Note that the
smoothness loss is also calculated along the vertical axis,
but here we omit it in (8) for simplicity.

3.4. Learning of SceneNet

During training, SceneNet takes either single view image
with semantic label or stereo view image as input. The full
objective of SceneNet can be deﬁned as

L = Ldepth + αsegLseg + αlrscLlrsc + αsmoothLsmooth,
(9)
where αseg, αlrsc and αsmooth are the weights for each loss.
During the inference stage, SceneNet takes a monocular im-
age to produce both semantic segmentation and disparity
map (which can then be transformed into depth as speciﬁed
in [8]) by manipulating the task identity.

In order to quantitatively and qualitatively evaluate our
model and to fairly compare with recent works, we train
our SceneNet on the stereo image pairs from the KITTI
dataset [7]. As for learning semantic segmentation abil-
ity, we use the fully annotated images of the Cityscapes
dataset [3]. Note that we do not require any images to have
both stereo image pairs and the ground truth semantic seg-
mentation map. The detail of datasets used in our experi-
ments are given as follow:
Eigen Split Eigen et al. [5] selected 697 images from the
KITTI dataset [7] as test set for single view depth estima-
tion. To fairly compare with the prior works, we followed
their setting to use 22,600 images for training and the rest
for evaluation.
KITTI Split To further recognize the scene understanding
ability of SceneNet, we also evaluate our method on the
KITTI split of KITTI dataset following the work of Godard
et al. [8]. The training set of KITTI split contains 29,000
image pairs from various scenes and 200 images for the test
set. Moreover, the test set not only provides ground truth
disparities, but also comes along with ground truth semantic
segmentation labels, which are consistent with the annota-
tions used in the Cityscapes Dataset. Although no semantic
annotation from KITTI split is utilized during training, it
allows us to evaluate both depth prediction and semantic
segmentation abilities of our model on the test set.
Cityscapes Dataset The Cityscapes Dataset [3] provides
images of urban street scenes that is paired with pixel-
wise segmentation masks. This dataset is used as our only
segmentation data for training SceneNet. The provided
training set contains 2,975 images and the corresponding
ground truth semantic labels. Note that the amount of
training data we used to train SceneNet for semantic seg-
mentation is about 10 times less than the amount used for
depth. As for evaluation, the testing set contains 500 anno-
tated images. To understand the scene as much as possible,
SceneNet uses up to 19 semantic classes, which are com-
monly shared among segmentation works.

4.1. Implementation Details

Network Architecture Our proposed SceneNet
is com-
posed of a pair of encoder and decoder modiﬁed from
DispNet [17]. As dilated residual networks (DRNs) [24]
has shown promising results, our encoder utilizes dilated
Resnet layers to obtain better scene understanding features.
With the encoder extracts scene representations from the
input image, the task identity t will be appended to these
features. Depending on which task is performed, the values
of the layer are either all 1s or 0s. These features will later
be decoded by our decoder, which is inspired by Godard et
al. [8]. Our decoder uses four skip connections [15] from
the encoder to enhance the resolution of our predictions.

2628

Table 1: Quantitative results of depth estimation on the Eigen split of KITTI dataset. Following previous works, we
conduct experiments capped at 80/50 meters in depth.

Method

cap Abs Rel

Sq Rel RMSE RMSE log

δ < 1.25

δ < 1.252

δ < 1.253

(Lower is better)

(Higher is better)

Zhou et al. [27]
Yang et al. [22]
Mahjourian et al. [16]
Yin et al. [23]
Garg et al. [6]
Zou et al. [28]
Godard et al. [8]
Zhan et al. [25]
Ours (w/o seg)
Ours

Zhou et al. [27]
Garg et al. [6]
Yin et al. [23]
Godard et al. [8]
Zhan et al. [25]
Ours (w/o seg)
Ours

80m

50m
50m

0.208
0.182
0.163
0.155
0.152
0.150
0.141
0.135
0.128
0.118

0.201
0.169
0.147
0.134
0.128
0.122
0.112

1.768
1.481
1.240
1.296
1.226
1.124
1.186
1.132
0.996
0.905

1.391
1.080
0.936
0.872
0.815
0.742
0.673

6.856
6.501
6.220
5.857
5.849
5.507
5.677
5.585
5.444
5.096

5.181
5.104
4.348
4.305
4.204
4.103
3.871

0.283
0.267
0.250
0.233
0.246
0.223
0.238
0.229
0.226
0.211

0.264
0.273
0.218
0.224
0.216
0.212
0.198

0.678
0.725
0.762
0.793
0.784
0.806
0.809
0.820
0.820
0.839

0.696
0.740
0.810
0.824
0.835
0.835
0.852

0.885
0.906
0.916
0.931
0.921
0.933
0.928
0.933
0.936
0.945

0.900
0.904
0.941
0.937
0.941
0.944
0.951

0.957
0.963
0.968
0.973
0.967
0.973
0.969
0.971
0.972
0.977

0.966
0.962
0.977
0.973
0.975
0.977
0.980

2 , 1

4 , 1

Also, the outputs of our model are in four different scales
(1, 1
8 ) of the input image size. The outputs with
lower resolution are only used for loss calculation. We
also adopt the exponential linear units (ELU) interlaced
with the convolutional
layers within our model except
for the prediction layers. At last, the predicted outputs
will be sent through either pixel-wise average pooling or
softmax depending on the task identity t. As a reference,
the proposed SceneNet contains about 15 million trainable
parameters. A more detailed description of our model is
available in the supplementary material.

Training Details We implement the proposed model using
the TensorFlow framework [1]. During training, we resize
the input images to a resolution of 256×512. Data augmen-
tation is also performed to avoid overﬁtting. To be more
speciﬁc, we perform the augmentation (with a ﬁfty percent
chance) by sampling three numbers from uniform distribu-
tions in ranges of [0.8, 1.2], [0.5, 2.0] and [0.8, 1.2] respec-
tively. The sampled numbers will be used to shift gammas,
brightness and three channels of RGB colors respectively.
Our SceneNet is optimized by Adam [11], with the initial
learning rate λ = 1e-4, β1 = 0.9, β2 = 0.999, and ǫ = 1e-
5. The weights for different terms in the objective function
are set as αlr = 0.2, αds = 0.02, αseg = 0.1, αsmooth =
0.2 and αsmooth = 2.0. Since our self-supervising losses
rely on the quality of both depth estimation and semantic
segmentation, we only apply them after both Ldepth and
Lseg start converging. The training procedure requires 32

hours on a single GTX 1080 GPU to train on a total of 22
thousand paired images and 2,975 annotated images for 20
epochs with batch size set as 4. At the inference stage, we
input image I and obtain both d and s by changing the val-
ues in the task identity t. We also input the horizontally
ﬂipped I ′ and obtain the ﬂipped outputs d′, s′. By ﬂipping
back the outputs we obtain d′′, s′′ that aligns with the orig-
inal predictions d, s. For disparity maps, we follow similar
post-processing technique of Godard et al. [8]. As for seg-
mentation maps, we simply take the average of s and s′′ as
our ﬁnal results.

4.2. Quantitative Results

We ﬁrst evaluate our model on the testing set of Eigen
split as shown in Table 1. Notice that even without seman-
tic segmentation data, our SceneNet surpasses the state-of-
the-art unsupervised depth estimation. Later in our ablation
study, we verify that this is achieved by addressing the mis-
matching problem as noted in Sect. 3.2.

With the auxiliary semantic annotated data (2,975 im-
ages from Cityscapes), a signiﬁcant improvement made by
SceneNet can be observed. For more detailed studies on
varying the volume of training data and evaluation over dif-
ferent semantic classes, please refer to the appendix.

4.3. Ablation Study

To verify the impact of each idea we proposed and each
decision we made, we perform ablation studies and list the
results in Table 2, with examples shown in Figures 4 and 5.

2629

Table 2: Ablation study of our model on the KITTI dataset. The baseline model is SceneNet with separate decoders for
each task. Note that K denotes stereo images from KITTI, and CS denotes semantically annotated images from Cityscapes.
We have HF indicate the use of the proposed horizontally ﬂipping technique to address the mismatching problem, U for the
use of uniﬁed classiﬁer, t as task identity, Lsc for left-right semantic consistency, and Lss for semantics-guided disparity
smoothness.
In addition to depth estimation, semantic segmentation results in terms of mean Intersection-Over-Union
(mIOU) on both KITTI and Cityscapes are presented.

Data

Improvement

(Lower is better)

(Higher is better)

(Higher is better)

Depth

Segmentation

Method

K CS HF U

t

Lsc Lss

X
Godard
et al.† [8] X
X

Baseline

X

X

X

X

X X

X

X X

X X

X X

X X X

Ours

X X

X X X

X

X X

X X X

X X

X X X

X

X

X

Rel

Abs / Sq

RMSE
raw / log

0.117 / 1.177
0.114 / 1.086
0.116 / 1.145
0.112 / 1.111

5.804 / 0.206
5.776 / 0.204
5.762 / 0.208
5.812 / 0.204

δ <

mIOU

1.25 / 1.252 / 1.253

0.848 / 0.943 / 0.977
0.849 / 0.944 / 0.977
0.843 / 0.941 / 0.977
0.848 / 0.941 / 0.977

K

-
-
-
-

CS

-
-
-
-

-

-

-

0.112 / 0.999
0.111 / 1.216
0.104 / 0.913
0.104 / 0.940
0.104 / 0.913
0.102 / 0.890

5.564 / 0.197
5.585 / 0.197
5.286 / 0.185
5.340 / 0.187
5.276 / 0.187
5.203 / 0.183

0.854 / 0.944 / 0.979
0.855 / 0.945 / 0.979
0.862 / 0.953 / 0.983
0.863 / 0.952 / 0.982
0.861 / 0.953 / 0.983
0.863 / 0.955 / 0.984

33.83% 41.36%
5.45%
47.44%
14.93% 46.81%
39.13% 48.39%
38.49% 47.81%
38.95% 48.23%
37.69% 47.87%

† Results are better than those reported in the cited paper since we applied the post-processing method [8] for the sake of fairness.

(a) Input Image

(b) Baseline Disparity Map

(a) Input Image

(b) Baseline Semantic Map

(c) SceneNet Semantic Map

(d) SceneNet Disparity Map

(c) SceneNet Disparity Map

(d) SceneNet Semantic Map

Figure 4: Ablation study on depth estimation, the baseline
model is SceneNet trained w/o semantic segmentation. For
the same input image, we can observe that SceneNet is able
produces better depth map ((b)v.s.(d)) with the aid of its
semantic understanding (as demonstrated in (c)), especially
for the trafﬁc light in the ﬁgure.

Figure 5: Ablation study on semantic segmentation, the
baseline model is SceneNet trained w/o depth estimation.
Even though SceneNet targeted on depth estimation, im-
provement over semantic segmentation can still be observed
((b)v.s.(d)) with the aid of geometric understanding, espe-
cially for the vehicles in the ﬁgure.

The study is performed on the KITTI split, this allows us to
evaluate the performance of SceneNet on semantic segmen-
tation. The baseline model shares the encoder architecture
with SceneNet but uses a separate decoder for each task.

We ﬁrst evaluate the contribution and effectiveness of
addressing the mismatching problem with the horizontally
ﬂipping (HF) technique (as noted in Sect. 3.2 and Fig. 3).

More speciﬁcally, we apply the exact network architecture
of Godard et al. [8] with and without using our proposed
HF technique; we also additionally evaluate our architec-
ture without HF (and without segmentation either). We see
that HF successfully addressed the mismatching problem
with satisfactory performances, making our method state-
of-the-art on monocular depth estimation.

2630

Figure 6: Example results of SceneNet on KITTI split. Leveraging semantic understanding, our model is able to provide clear
and disparity map on smaller objects such as trafﬁc signs and trunk. As the results showed, SceneNet successfully derived a
robust scene representation for depth estimation and semantic segmentation.

Next, it is clear that employing the uniﬁed classiﬁer with
task identity not only improves the performance of depth
estimation, but also enables our model to predict satisfying
semantic segmentation on the stereo dataset. This veriﬁed
our assumption that using separate classiﬁer limits the ca-
pacity to jointly learn from both tasks. Note that with the
separately trained classiﬁer for each task, the semantic clas-
siﬁer failed to produce acceptable results on images from
stereo datasets, indicating that it may be overﬁtting the seg-
mentation dataset without learning robust scene represen-
tation. It is also worth noting that, although our goal is to
advance unsupervised depth estimation with semantic seg-
mentation, results show the fact that segmentation perfor-
mance also beneﬁts from depth estimation by sharing in-
formation through SceneNet. Finally, with each component
of loss functions being gradually added to our architecture,
the full version of our SceneNet is obtained and compared
to others in Table 1.

4.4. Visualization

Examples of the depth and semantic prediction are pro-
vided in Fig. 6, along with the corresponding input and
ground truth from KITTI Split test set. It is apparent that our
SceneNet not only performs favorable results of disparity
prediction, but also provides satisfying quality of semantic

masks in complicated scenes. At last, it is worth mentioning
that we do not require any ground truths of KITTI dataset.
Due to the space limit, please refer to the appendix for more
qualitative results and comparison against prior works.

5. Conclusion

In this paper, we propose SceneNet to address the mis-
matching problem of existing unsupervised depth estima-
tion models. Our model advances depth estimation by lever-
aging semantic segmentation, with our proposed task iden-
tity enables SceneNet to perform both semantic segmenta-
tion and depth estimation with a uniﬁed structure. In ad-
dition, our self-supervised regularization (left-right seman-
tic consistency and semantics-guided disparity smoothness)
further allows performance improvements via semantic un-
derstanding. Our SceneNet can be trained in an end-to-
end manner without ground truth depths map and any pre-
trained models; moreover, it can be learned from disjoint
stereo pairs and segmentation datasets without the require-
ment of paired training instances. In our experiments, our
model performed favorably against state-of-the-art methods
on unsupervised depth estimation.

Acknowledgement. This work is supported in part by the
Ministry of Science and Technology of Taiwan under grant MOST
108-2634-F-002-018.

2631

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-
ﬂow: a system for large-scale machine learning. In OSDI,
2016. 6

[2] W. Chen and J. Deng. Learning single-image depth from
videos using quality assessment networks.
In Proceedings
of the European Conference on Computer Vision (ECCV),
2018. 2

[3] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016. 5

[4] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2015. 1, 2, 3

[5] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in Neural Information Processing Systems (NIPS),
2014. 1, 2, 5

[6] R. Garg, B. V. Kumar, G. Carneiro, and I. Reid. Unsuper-
vised cnn for single view depth estimation: Geometry to the
rescue. In Proceedings of the European Conference on Com-
puter Vision (ECCV), 2016. 2, 4, 6

[7] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-
tonomous driving? the kitti vision benchmark suite. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). IEEE, 2012. 5

[8] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsuper-
vised monocular depth estimation with left-right consistency.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017. 1, 2, 4, 5, 6, 7

[9] H. Ha, S. Im, J. Park, H.-G. Jeon, and I. So Kweon. High-
quality depth from uncalibrated small motion clip. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016. 2

[10] J. Jiao, Y. Cao, Y. Song, R. W. H. Lau, and R. Lau. Look
deeper into depth: Monocular depth estimation with seman-
tic booster and attention-driven loss. In Proceedings of the
European Conference on Computer Vision (ECCV), 2018. 2
[11] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 6

[12] Y. Kuznietsov, J. Stuckler, and B. Leibe. Semi-supervised
deep learning for monocular depth map prediction. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017. 1, 2

[13] L. Ladicky, J. Shi, and M. Pollefeys. Pulling things out of
perspective. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2014. 1, 2, 3
[14] F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from sin-
gle monocular images using deep convolutional neural ﬁelds.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence (TPAMI), 2016. 1, 2

[15] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2015. 5

[16] R. Mahjourian, M. Wicke, and A. Angelova. Unsupervised
learning of depth and ego-motion from monocular video us-
ing 3d geometric constraints.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 2, 6

[17] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers,
A. Dosovitskiy, and T. Brox. A large dataset to train convo-
lutional networks for disparity, optical ﬂow, and scene ﬂow
estimation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2016. 5

[18] A. Mousavian, H. Pirsiavash, and J. Koˇseck´a. Joint semantic
segmentation and depth estimation with deep convolutional
networks. In International Conference on 3D Vision (3DV),
2016. 1, 2, 3

[19] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar,
and K. Fragkiadaki. Sfm-net: Learning of structure and mo-
tion from video. arXiv preprint arXiv:1704.07804, 2017. 2
[20] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L.
Yuille. Towards uniﬁed depth and semantic prediction from
a single image. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015. 1,
2, 3

[21] T.-C. Wang, M. Srikanth, and R. Ramamoorthi. Depth from
semi-calibrated stereo and defocus.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016. 2

[22] Z. Yang, P. Wang, W. Xu, L. Zhao, and R. Nevatia. Unsuper-
vised learning of geometry with edge-aware depth-normal
consistency. arXiv preprint arXiv:1711.03665, 2017. 6

[23] Z. Yin and J. Shi. Geonet: Unsupervised learning of dense
depth, optical ﬂow and camera pose. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 2, 6

[24] F. Yu, V. Koltun, and T. A. Funkhouser. Dilated residual net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017. 5

[25] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, and
I. Reid. Unsupervised learning of monocular depth estima-
tion and visual odometry with deep feature reconstruction.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018. 2, 4, 6

[26] Z. Zhang, Z. Cui, C. Xu, Z. Jie, X. Li, and J. Yang. Joint
task-recursive learning for semantic segmentation and depth
estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), 2018. 2

[27] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsuper-
vised learning of depth and ego-motion from video. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017. 1, 2, 6

[28] Y. Zou, Z. Luo, and J.-B. Huang. Df-net: Unsupervised joint
learning of depth and ﬂow using cross-task consistency. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), 2018. 6

2632

