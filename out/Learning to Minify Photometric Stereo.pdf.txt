Learning to Minify Photometric Stereo

Junxuan Li1,2

Antonio Robles-Kelly3

Shaodi You1,2

Yasuyuki Matsushita4

1Australian National University, College of Eng. and Comp. Sci., Acton, ACT 2601, Australia

2Data61-CSIRO, Black Mountain Laboratories, Acton, ACT 2601, Australia

3Deakin University, Faculty of Sci., Eng. and Built Env., Waurn Ponds, VIC 3216, Australia

4Osaka University, Graduate School of Information Science and Technology, Osaka 565-0871, Japan

Abstract

Photometric stereo estimates the surface normal given
a set of images acquired under different illumination con-
ditions. To deal with diverse factors involved in the image
formation process, recent photometric stereo methods de-
mand a large number of images as input. We propose a
method that can dramatically decrease the demands on the
number of images by learning the most informative ones un-
der different illumination conditions. To this end, we use a
deep learning framework to automatically learn the criti-
cal illumination conditions required at input. Furthermore,
we present an occlusion layer that can synthesize cast shad-
ows, which effectively improves the estimation accuracy. We
assess our method on challenging real-world conditions,
where we outperform techniques elsewhere in the literature
with a signiﬁcantly reduced number of light conditions.

1. Introduction

Photometric stereo is a technique that estimates the sur-
face normal of an object from numbers of photographs
taken under different illumination conditions from a ﬁxed
viewpoint. Due to the diverse appearances of real-world
materials, a method for dealing with general reﬂectances is
desired. Recent methods toward this direction improve the
estimation accuracy by robust estimation at the cost of in-
creasing the number of images, such as [21]; however, it
complicates the data acquisition setup and calibration pro-
cedure.

Recently, deep learning-based methods appeared in the
context of photometric stereo [16, 8, 3]. These methods are
shown effective for surfaces with diverse reﬂectances, indi-
cating that the mapping from input images to surface normal
can be well established even with the diversity. Unlike these
methods that primarily focus on the estimation accuracy, we
study the problem of reducing the number of required im-

GT

Ours

PS-FCN

CNN-PS

Figure 1. Performance with only 8 inputs for our method, PS-
FCN [3] and CNN-PS [8] on the “pot1” from DiLiGenT [17]. Note
we outperform the alternatives.

ages so as to minify the photometric stereo input. We ap-
proach this problem by learning the relative importance of
different illuminant directions, which opens up a possibility
of optimally selecting input illuminant directions for light-
weight data acquisition.

However, reducing the number of input illuminant direc-
tions without loss in performance is a non-trivial task be-
cause the surface radiance is determined by the illumination
intensity, illuminant direction, surface normal and BRDF
function. For instance, Argyriou et al. [2] use a sparse rep-
resentation of illuminations for reducing the required num-
ber of lights to 5 under the assumption of Lambertian sur-
faces. The task is further complicated by the presence of
cast shadows, specularities, and inter-reﬂections.

In this paper, we propose a deep learning approach,
which applies a connection table that can select those illu-
minant directions that are most relevant to the surface nor-
mal prediction process. To this end, we employ a connec-
tion table so as to introduce a trainable input mapping. This
mapping is trained by making use of the ℓ1-norm and a
sparsity-loss. Once the model is trained, it can efﬁciently
estimate the surface normal only given a much-reduced
number of input images without loss in accuracy. For the
sake of the scalability of our method, here we select the il-
luminant directions making use of an observation map [8]
and estimate the surface normal in a pixel-wise fashion.

17568

Furthermore, to account for the effect of global illumi-
nation, we tackle the problem of cast shadows explicitly.
Speciﬁcally, by regarding cast shadows as a localized zero
pattern on the observation map, we introduce an occlusion
layer into the network to deal with cast shadows. In sum-
mary, our contributions are:

1. We propose a connection table in the network input to-
gether with a suitable loss function and a rank selection
process so as to select those illuminant directions that
are the most relevant to the surface normal prediction
process.

2. We propose an occlusion layer that can simulate object
cast shadows. This occlusion layer can be applied on
both augmented and real-world datasets, and enhance
performance on shadowed areas.

3. Our end-to-end deep neural network for photometric
stereo can predict the surface normal with a much-
reduced number of light source directions. This is
consistent with our results on the DiLiGenT bench-
mark [17], where we can predict the surface normal
with as few as 8 input images.

2. Related Work

The literature of photometric stereo is vast, it can be di-

vided, however, into the following groups:

Least-squares methods: Introduced by Woodham [20],
least-squares approaches aim at solving the problem un-
der the Lambertian assumption, i.e., the pixel intensity is
proportional to the cosine of the angle between the illumi-
nant direction and the surface normal. Moreover, they often
assume the surface is convex and devoid of cast shadows
with a uniform, Lambertian reﬂectance. The Lambertian
assumption is important since it allows for the image for-
mation model to be cast into a linear system of equations
that can be solved in a closed-form manner.

Robust approaches: These approaches can cope with
specularities and cast shadows on the object under study by
viewing non-Lambertian regions as outliers. Wu et al. [21]
add an additional term to the image formation model so as to
naturally represent the error, which accounts for those pix-
els that deviate from the Lambertian assumption and solve
the photometric stereo problem using rank minimization.
Many other techniques are also used for solving the prob-
lem under the assumptions above, such as RANSAC [14],
expectation maximization [22], sparse regression [9] and
variational optimization approaches [15].

Example based methods: These approaches take ex-
emplars of materials and surface geometry as the reference
to estimate the surface normal of an unknown object with
the same or similar BRDF function which are in the ex-
ample set. Hertzmann and Seitz [5] propose a method that
ﬁrst clusters the materials on the object. More recently, Hui

and Sankaranarayanan [7] use a virtually rendered sphere
instead of a physical one for normal estimation.

Deep learning: These methods are the ones that, re-
cently, have achieved the best performance in photometric
stereo. Santo et al. [16] propose the ﬁrst deep network-
based method for pixel-wise estimation.
In their model,
they assume that the light directions are all known and
consistent between the training and prediction phases.
Nonetheless effective, this assumption on the consistency of
the training and testing set greatly limits the generalization
capabilities of the method and makes it overly sensitive to
the training set. Ikehata [8] has proposed a deep net which
employs an observation map for unstructured photometric
stereo inputs. This map is intended to account for every
illuminant direction at each surface pixel. Despite effec-
tive, the author assumes a dense illumination map, making
the method prone to corruption with a lesser amount of il-
luminant directions at input. In a different approach, Taniai
and Maehara [18] propose an unsupervised network that can
take the whole images at input and predict the dense normal
in an end-to-end fashion. This method has the drawback,
however, of being computationally very costly. More re-
cently, Xu et al. [23] propose a deep net to learn the optimal
samples for image-based relighting.

3. Preliminaries

In this section, we commence by providing the basic un-

derpinnings and notations regarding photometric stereo.

3.1. Reﬂected Irradiance

Assume that a light source illuminates a surface point
from the direction l ∈ S 2 (the space of 3-dimensional unit
vectors) and its irradiance is e. And, respectively, the re-
ﬂected radiance of this surface is denoted as r. The surface
normal is denoted as n ∈ S 2. The BRDF function is then
denoted as ρ(l, v, n), i.e., a function of the incident and re-
ﬂected radiance vectors l, v and the surface normal n.

Due to the variation of the normals and light source di-
rections, shadows may appear at the reﬂecting surface. If
the normal is opposite to the light direction, i.e., l · n ≤ 0,
the attached shadow occurs. If the surface is occluded by
the object itself, the cast shadow also occurs, as shown in
Fig. 2. As a result, the observed reﬂected radiance r(v) can
be written as

r(v) = Q (eρ(l, v, n) max(l · n, 0)) ,

(1)

where Q ∈ {0, 1} is a binary variable with a value of 0 at
cast shadows, assuming that there are no inter-reﬂections
in shadowed areas. Hence, as long as the illumination is
occluded by objects, the indicator variable Q will be set to
zero regardless.

Following the conventional calibrated photometric
stereo problem, we assume that the light direction l and

27569

Light source

Camera

Observation map

Cast shadow

Attached shadow

Figure 2. Shadowed area on “harvest” from the DiLiGenT [17]
dataset. Note cast shadows are caused by occlusions, while at-
tached shadowed areas are facing back with respect to the light
source. On the right-hand side of the ﬁgure, we show two obser-
vation maps obtained from cast shadows. The red dash line on the
observation map indicates the shape that occludes the lights.

intensity e are given at each illumination condition. Also,
we assume that the light source is in distance over the im-
ages. In order to simplify the ensuing equations, we con-
sider the camera to be in orthographic position and, hence,
v is [0, 0, 1]. Finally, here we use, without any loss of gen-
erality, the relative intensity m = r
e to rewrite the Eq. (1) as
follows

m = Q(cid:0)ρ(l, v, n) max(l · n, 0)(cid:1).

3.2. Observation Map Computation

(2)

We follow Ikehata [8] and adopt the use of an obser-
vation map. Recall that an observation map is a 2-D pro-
jection of all the lighting directions in a 3-D hemisphere.
More formally, assume that a light direction is given in the
x, y, z -coordinate system by the vector l = [lx, ly, lz]⊤.
In contrast with Ikehata [8], we do not use the Carte-
sian coordinates directly, but rather compute the observa-
tion map by ﬁrst projecting the Cartesian vector l into a
polar coordinate system whose components are given by
z). Here,
we have used a different size of observation maps as com-
pared to Ikehata [8], and have noted that the performance
of the polar coordinates outperforms Cartesian ones for this
size of observation maps.

z), ϕ = arcsin(ly/ql2

θ = arcsin(lx/pl2

x + l2

y + l2

The entry indexed int(cid:0) θ+b

∆θ(cid:1) , int(cid:16) ϕ+b

∆ϕ(cid:17) of the observa-

tion map O ∈ Rw×w with a window size w can be then
set to m. In the indexes above, b is the upper and lower
bound of the range of θ and φ and int(·) is an operator the
rounding operator that delivers the closest integer to its ar-
gument, and ∆θ and ∆ϕ are the gap between each of the
map indexes given by ∆θ = ∆ϕ = 2b

w−1 .

In this manner, the observation map converts un-ordered
inputs into a meaningful feature map which captures the re-

lationship between the reﬂectance properties of the object
under different illuminant directions. As shown in Fig. 2,
each value on the map naturally encodes the information of
the surface reﬂectance r, illuminant direction l and inten-
sity e. Moreover, the map can also be viewed as a group of
functions of the normal n, the BRDF ρ and the cast shadow
indicator variable Q.

3.3. illuminant direction Selection

We now turn our attention to the learning of those illumi-
nant directions that are the most relevant to the photomet-
ric stereo process so as to greatly minimize the number of
views required to compute the surface normal. Mathemat-
ically, if we have k different illuminations with directions
L = [l(1), l(2), · · · , l(k)]⊤ ∈ Rk×3, and for each direction
of the lights the relative intensities on a surface point are
given by m = [m(1), m(2), · · · , m(k)]⊤ ∈ Rk. Then for
each surface point we have a system of equations given by

m = Q ◦ ρ(l, v, n) ◦ vmax(Ln, 0),

(3)

where Q ∈ {0, 1}k is a vector encoding the cast shadow
information under different illuminations, ◦ denotes the
Hadamard product, i.e., an element-wise multiplication op-
erator, and similarly, vmax is an element-wise max opera-
tor. Our goal is hence to solve this system of equation with
k being as small as possible.

4. Proposed Method

Here, we propose a neural network that can take sparse
illuminations as inputs and predict the surface normal with
a marginal loss in accuracy. As shown in Fig. 3, the obser-
vation map can encode different lighting directions into a
feature map, making the selection over the input space fea-
sible. Then an occlusion layer renders the system robust to
cast shadows. For the network itself, we have used a varia-
tion of the DenseNet [6] architecture.

To the effect of selecting the relevant illuminant direc-
tions, we employ a learnable connection table at the input of
our deep network. As the network undergoes training, our
method can predict the surface normal accurately by taking
at input a signiﬁcantly less number of illuminant directions
than other methods elsewhere in the literature.

4.1. Connection Table

To the best of our knowledge, there are no learning based
approaches aiming to ﬁnd the relevant input feature maps
for photometric stereo. However, as related to deep net-
works, Alvarez et al. [1] use sparse constraints for regular-
izing the training process of a deep neural network. Ko-
ray et al. [11] reduce the number of parameters and, hence,
the complexity of deep networks using a binary connec-
tion table to “disconnect” a random set of feature maps.

37570

Objects under 

different illuminations

Illumination 

directions

Observation

map

Occlusion

layer

Connection 

table

CNN blocks

Output

Observation map 

computation

Figure 3. Overview of our network. Given the images of objects under different illuminations, we can compute its corresponding observa-
tion map at each pixel. Then we apply an occlusion layer which operates as a binary ﬁlter mask so as to zero out an area of the observation
map. We then employ a learned connection table to weigh the feature map at input. After training, only a few illuminations corresponding
to the non-zero weights of the connection table are necessary for our network to predict the surface normal.

Wei et al. [19] employ a one-dimensional connection table
to perform band reduction in imaging spectroscopy. These
contrast with our method, which aims at recovering a two-
dimensional sparse input connection table with non-binary
weights over the observation map.

4.1.1 Sparse Table Computation

Let the input connection table be denoted by C. This table
is applied to the observation map after an occlusion layer.
For now, we concentrate on the connection table and will
elaborate further on the occlusion layer later in the paper.
This table effectively acts as an input selector whereby the
connection weights are learned at training. Note that the
connection table has the same size as the observation map,
C ∈ Rw×w, with all its parameters larger or equal to zero,
Ci,j > 0. The sparse observation map is then yielded by
an element-wise product with the observation map after it
has been “ﬁltered” by the occlusion layer. This generates a
sparse observation map as the input for the network.

The loss function of the network is then given by two
terms. The ﬁrst term accounts for the surface normal pre-
diction, where we use the mean square error. The second
term is a regularization one over the connection table. These
yield the loss given by

L = kn − n′k2

2 + λg(C),

(4)

where n and n′ are the predicted and ground truth surface
normals, respectively, g(·) denotes the regularization func-
tion and λ is a hyper-parameter that controls the contribu-
tion of the regularize to the overall loss. Thus, a large λ will
enforce a more sparse connection table and, conversely, a
smaller λ will yield a denser one.

Note that, in this manner, the connection table can be
used to select the most relevant illuminant directions at in-
put. These directions in observation map are effectively de-

ﬁned by the angular intervals i.e., ∆θ = ∆ϕ = 2b
w−1 with
respect to the illuminant direction. This is an important ob-
servation since the connection table can also be interpreted
as relevance for the corresponding range of the illuminant
direction, which renders the method more robust to small
variations in the illuminant direction.

4.1.2 Connection Table Training

Here, to make the connection table sparse, we employ the
regularization function given by

g(C) =Xi,j  2Ci,j −

C 2
i,j

2α ! ,

(5)

where α is the maximum value of
α = max(Ci,j) ∀ i, j.

the map,

i.e.,

Note that the function above is minimum as the connec-
tion table weights tend to zeros. Moreover, at each step of
the back-propagation, the connection table can be updated
in a straightforward manner using the derivative given by

∂g(C)
∂Ci,j

= 2 −

Ci,j
α

.

(6)

Since the connection table by deﬁnition is non-negative, the
derivative ∂g(C)
∂Ci,j

takes values in the range of [1, 2].

With the connection table in hand, we can apply a se-
lection strategy to select the k-most important inputs to the
network. To this end, we use a recurrent training scheme
and, at each training operation, we apply a rank selection
with exponential decay, i.e., k = int(c + τ e−βt), where
t = {0, 1, 2, . . .} is the training operation index and c, β
and τ are scalar parameters. It is worth noting in passing
that our choice of exponential decay in the number of ta-
ble entries that are not null is reminiscent of the exponential
cooling strategy in annealing methods and rank selection
approaches elsewhere in the literature.

47571

λ = 10!" MAE=6.88

λ = 10!# MAE=7.78

λ = 10!$ MAE=9.19

Occlusion layer

Random zero

Figure 4. Observation map processed by an occlusion layer as
compared to being randomly zeroed [16]. Note that the occlu-
sion layer effectively simulates the pattern in the observation map
induced by cast shadows.

4.2. Occlusion Layer

Recall that cast shadows are one of the most challenging
problems in photometric stereo. It is particularly true when
the number of input illuminant directions is small. Santo et
al. [16], tackle this problem by making use of a shadow
layer in their network which randomly sets some of the in-
puts to zero.

Despite effective, their approach does not take into ac-
count that cast shadows are often consistent with respect to
the illuminant direction rather than occurring randomly. To
illustrate this, we show, in Fig. 2 the observation map ob-
tained from a shadowed area in the “harvest” object. The
cast shadow is caused by the wrinkles on the cloth and
shows a consistent pattern with a relatively sharp bound-
ary. Locally, a smooth or ﬂat occlusion results in a straight
boundary while sudden changes in the object geometry of-
ten yield more abrupt boundaries.

Thus, here we employ an occlusion layer that can ef-
fectively simulate the cast shadows by randomly drawing a
line across the observation map to obtain a mask that can
then be used to set the corresponding entries of the map to
zero. Speciﬁcally, the layer randomly selects two sides of
the map, and randomly picks a point on each side. Then
the line connecting these two points divides the map into
two regions. The smaller of these two regions is then set
to zero. This is illustrated in Fig. 4. By training the net-
work with this occlusion layer, our proposed method can
effectively learn the pattern of cast shadows and predict the
surface normal more robustly.

5. Implementation

Observation Map Parameters For the observation map,
we have set the window size w to 14. And the upper and
lower bounds of θ and ϕ to π
4 . With these parameters in
hand, we then randomly select 5% of the observation maps
to be put through our occlusion layer, which simulates the
effect of cast shadows. Besides, we also randomly set about
5% points of observation map to be zero. We discuss fur-
ther the choice of this fraction of the observation maps in
Sec. 6.2.

λ = 10!% MAE=10.87

λ = 1 MAE=11.03  λ = 10 MAE=11.06

Figure 5. Visualization of the connection tables (cropped) and the
mean angular errors on the validation set as a function of λ.

55

30

16

10

8

6

&
%
$
#
-
1
!

)
#
(
!

Figure 6. Connection tables (cropped) after rank selection. From
left-to-right, each panel shows the table as the number of non-zero
values decreases down to 6. The top row shows the maps yielded
using an ℓ1-norm regularizer. The second row shows the maps
obtained using the regularization function in Eq. (5).

Pre-training For the connection table, as mentioned ear-
lier, we perform rank selection as we recursively train the
network. To this end, we initialize the table to be all ones
and proceed to pre-train it. For the pre-training, we use an
alternative loss function given by

L = kn − n′k2

2 + λf (C),

(7)

where f (·) is given by the ℓ1-norm regularizer.

The reason for using the ℓ1-norm at pre-training and the
regularizer g(·) later is due to the fact that, in contrast with
the behavior of g(·), the ℓ1-norm decreases as the magni-
tude of the connection table entries tends to zero. Thus, we
have employed the ℓ1-norm to provide a good initialization
for the training and employed the regularizer in Eq. (5) to
favor values of the table that are either close to unity or to
zero. Moreover, nonetheless λ can control the sparsity of
the connection table when the ℓ1-norm is used, it quickly
becomes ineffective as it increases. To illustrate this behav-
ior, we show, in Fig. 5, the connection table obtained with
different values of λ at pre-training. We can see from the
ﬁgure that, as λ increases beyond 10−1, the connection ta-
ble shows virtually no change.

Connection Table Rank Selection As mentioned above,
we commence by pre-training the network. Here, we have
set λ = 10−3 for the pre-training operation and then ap-
plied, rank selection to the connection table over a set of
subsequent, recurrent training steps. As described earlier,

57572

Inputs

Methods
Ours-g(·)
Ours-L1

Ours-Random
PS-FCN [3]
CNN-PS [8]
L1-RES [9]
Baseline [20]

96

16

10

8

6

8.43

8.39
7.20
14.08
15.39

9.66
10.29
10.50
9.37
10.49
15.47
16.65

10.02
11.06
11.39
10.51
14.34
16.37
17.31

10.39
11.33
11.85
11.42
19.50
16.84
17.47

12.16
12.57
12.58
12.54
30.28
18.45
18.60

Table 1. Performance on the DiLiGenT dataset with different num-
ber of input light directions (see Sec. 6.1 for more details).

in this manner we select the top k largest weights, with k
steadily decreasing over the training process from all illumi-
nations in the dataset (96) down to 6 following the sequence
55, 30, 16, 10, 8, 6 in k.

In Fig. 6 we present the connection tables at each train-
ing step after the rank selection has been affected. In the
left-hand panel, we show the connection table obtained at
the ﬁrst training step after the 55 most signiﬁcant elements
of the learned table are selected and the rest are set to zero.
The second panel, from left-to-right, shows the table where
the 30 most signiﬁcant entries have been selected after the
second training operation and so on. For all training oper-
ations, we have set λ = 10−3. Note this is consistent with
our pre-training operation.

5.1. Dataset

To train our network, we generate a synthetic dataset us-
ing the 10 shapes from the Blobby Shape Dataset [10] and
100 reﬂectances from MERL BRDF dataset [13]. To do
this, we randomly select 8 BRDFs to repeatedly render each
object using 144 randomly distributed light directions. For
the sake of consistency, we have used the same range of
light directions as that used in the DiLiGenT [17] dataset.

We choose 9 shapes for training and the remaining one
for validation. Thus, our synthetic dataset comprises ap-
proximately 9 × 104 × 8 = 720, 000 observation maps
for training and about 80, 000 maps for validation. For the
testing, we use the DiLiGenT [17] dataset, which is a real-
world public benchmark containing the images of 10 dis-
tinct objects. Each object is observed under 96 different
known light directions, of which the horizontal direction
ranges from −45◦ to 45◦ and vertically from −30◦ to 30◦
over a hemisphere centered at the viewing direction. The
ground truth surface normals have been acquired using a
laser scanner and are available for quantitative evaluation.

6. Experiments

For all our experiments we have trained our network us-
ing Adam [12] optimizer with the default setting. The net-
work is implemented by Keras with Tensorﬂow as the back-
end on a workstation using Ubuntu-14 with an NVIDIA
GTX 1080Ti 11G.

Training

Methods

Synthesis

Real-
world

Occlusion layer
Random zeroing
Occlusion layer
Random zeroing

0%

11.42

8.28

5%
10.42
10.75
7.99
8.33

10%
10.68
10.87
8.04
8.13

15%
10.57
10.94
7.85
8.37

Table 2. Comparison of our occlusion layer and random zeroing
[16] on the DiLiGenT dataset for 10 illumination directions.

For the quantiﬁcation of the error, in all our experiments
we have used the mean angular error (MAE) for surface nor-
mal evaluation. Here, we compare the results by our method
to those obtained using a number of alternatives [16, 3, 8, 9]
that can be used with sparse inputs. We also illustrate the
effectiveness of our loss function and our occlusion layer
and show results on pixel-wise normal estimation.

6.1. Effectiveness of the Regularization Function

To illustrate the effectiveness of our regularizer g(·) in
the loss function, we now compare the performance of our
network when other regularization functions are used. In
Table 1, “Ours-g(·)” denotes the network trained using the
function g(·) described in Eq. (5). “Ours-L1” corresponds
to the results yielded by our network when the ℓ1-norm reg-
ularizer is used as an alternative to g(·). Finally, the “Ours-
Random” corresponds to the case where no regularizer is
included and the loss function and the connection table is
randomly initialized rather than pre-trained. All the other
methods are tested with randomly selected inputs with all
the results shown are averaged over 10 trails. As we can see
from the table, our model with the sparsity regularization
function g(·) performs the best and clearly outperforms the
ℓ1-norm and random initialization.

6.2. Evaluation of the Occlusion Layer

Now we turn our attention to the effects of the occlusion
layer. To this end, we have trained the network with both,
our occlusion layer and that proposed by Santo et al. [16]
and set k = 10, i.e., used 10 light directions at input. Here,
we have trained the network on both, our synthetic dataset
and DiLiGenT.

In Table 2, we show the effects of our occlusion layer and
that proposed by Santo et al. [16]. Note that in our synthetic
dataset there are no cast shadows. This contrasts with the
real-world objects in the DiLiGenT dataset, which exhibit a
diversity of shadows and inter-reﬂections. The purpose of
the comparison shown here is not only showing the beneﬁts
of our occlusion layer as compared to that in [16] but also
showing that it can beneﬁt the training of real-world and
synthetic datasets alike.

Also, note that the percentages in the table have a dif-
ferent meaning for both approaches. For our occlusion
layer, the percentages indicate how many of the observation
maps are “masked” with our occlusion layer during train-
ing. While the percentages of randomly zeroing follow the

67573

25 (Worse)

0 [deg.]

-25 (Better)

Figure 7. Performance comparison of our occlusion layer with respect to random zeroing [16]. The blue color in the panels corresponds to
the area where the occlusion layer outperforms the random zeroing. The converse applies to the red regions. We can see that, in the “Cat”
image, the occlusion layer is better in most of the shoulder region. In the “Buhhda” image, which is quite textured, our method outperforms
the random zeroing on all the cloth wrinkles. All of these areas correspond to cast shadows.

methods

Ours

PS-FCN[3]
CNN-PS[8]
L1-RES[9]

Baseline [20]

Ours

PS-FCN[3]
CNN-PS[8]
L1-RES[9]

Baseline [20]

s
t
u
p
n
I
0
1

s
t
u
p
n
I
8

ball
3.97
4.02
9.11
4.92
5.09

3.65
5.14
22.74
4.70
5.50

cat
6.69
8.30
11.71
9.68
9.66

7.07
9.47
21.06
9.57
9.89

pot1
7.30
10.14
13.23
10.69
11.32

8.11
10.65
18.87
11.96
12.01

bear
8.73
7.18
14.08
9.58
11.59

8.91
9.16
16.89
10.10
11.57

pot2
9.74
9.85
14.65
17.64
18.03

9.92
10.05
19.18
17.33
17.49

buddha
11.36
9.79
14.58
14.53
16.25

12.03
9.66
19.95
15.62
16.53

goblet
10.46
11.58
15.48
18.39
19.97

10.67
12.39
18.62
19.07
19.73

reading
14.37
15.03
16.99
18.23
19.86

15.27
16.47
17.77
18.96
20.41

cow
10.19
10.51
14.04
27.01
27.90

10.14
11.14
16.04
28.23
28.64

harvest
17.33
18.70
19.56
33.05
33.41

18.15
20.05
23.90
32.82
32.96

Avg.
10.02
10.51
14.34
16.37
17.31

10.39
11.42
19.50
16.84
17.47

Table 3. Results on the DiLiGenT dataset with 10 and 8 input illumination directions. All the alternatives are tested with randomly selected
inputs. All the mean angular errors shown correspond to the average performance over 10 trails.

setting in [16], indicating how many pixels at each input
map are randomly set to zero.

As we can see from the table, our occlusion layer per-
forms better when 5% of the input maps are occluded. For
the random zeroing strategy, the best performance appears
to be also about 5%. Nonetheless, regardless of the per-
centage used, the lowest error is always achieved by our
occlusion layer. Finally, we show the error map for a num-
ber of sample objects in the DiLiGenT dataset in Fig. 7.
The error maps shown correspond to the difference between
the MAE yielded by our occlusion layer and random zero-
ing. Hence the negative areas, i.e., blue regions in Fig. 7 are
those where our occlusion layer outperforms the alternative.
From the ﬁgure, it is clear that the occlusion layer performs
much better at most of the rugged and wrinkled areas, where
cast shadows are stronger. This is consistent with the notion
that our occlusion layer can help the network cope with cast
shadows.

6.3. Benchmark Comparison Under Sparse Inputs

We commence by comparing our method with other
state-of-art approaches using the DiLiGenT dataset. For the
results yielded by the alternatives in sparse illuminations,
and in contrast with our method, we have randomly selected
input illuminant directions up to the k under consideration.
Note that this is in accordance with the sparse input setting
proposed by the authors.

The MAE for the dataset over 10 trails is shown in Ta-

ble 1. Note that in Table 1, all the methods except PS-
FCN [3] estimate surface normal in a pixel-wise fashion.
Since pixel-wise methods naturally discard the connection
between pixels and shape, a sparse input setting like the
one used here is expected to decrease the performance dra-
matically. As we can see, the performance of CNN-PS [8]
decreases from 7.20 to 30.28 at 6 inputs. In comparison,
our method do not exhibit such loss in performance, out-
performing the other pixel-wise methods. We can see that
from 16 inputs down, our method starts to show its ability
to select the illuminant directions to avoid an overly loss
in performance. At 10 and 8 input directions, our method
shows a large improvement in performance with respect to
the other methods. Moreover, our method shows a decrease
in performance as low as 1.25 degrees on MAE from 96
to 16 input directions and only 0.3 degrees from 16 to 10
inputs.

Also, note that PS-FCN [3] is a dense normal estima-
tion method which takes the whole image into account.
Hence, PS-FCN [3] naturally has advantages on sparse in-
puts as it can utilize the structural information across pix-
els. Nonetheless, our method can perform better than PS-
FCN [3] in highly sparse illumination settings with 10 or
less light source directions. In Table 3, we also show the
performance of our method and the alternatives on each ob-
ject in DiLiGenT dataset for 10 and 8 inputs.
In Fig. 8,
we show the surface normal and error maps of the “Goblet”
object from DiLiGenT.

77574

GT

Ours

PS-FCN

CNN-PS

L1-RES

Baseline

s
t
u
p
n
I
 
0
1

s
t
u
p
n
I
 
8

10.46

11.21

15.41

18.92

18.90

10.67

12.29

18.60

18.81

19.56

Figure 8. Performance of “Goblet” in DiLiGenT dataset in only 10 and 8 inputs.

90 [deg.]

0 [deg.]

90 [deg.]

0 [deg.]

Light-Conﬁg

Ours

Random

Ours

Optimal [4]

10.02

PS-FCN[3]
10.51
11.35
8.73

CNN-PS[8]
14.34
13.02
13.35

L1-RES[9]
16.37
15.83
15.50

Baseline[20]

17.31
17.12
16.57

Table 4. Mean angular errors on the DiLiGenT benchmark us-
ing the same 10 lights directions obtained using random sampling
(Random), our approach (Ours) and the method in [4].

Table 4 shows the mean angular errors for our and other
methods under similar light conﬁgurations. The ﬁrst row
(Random) shows the result when 10 lights are randomly
sampled from a uniform distribution over the hemisphere.
The second row (Ours) is the result under the light direc-
tions that correspond to the non-zero entries of the connec-
tion map learned by our method. The third row (Optimal)
correponds to the optimal light conﬁguration proposed by
Drbohlav and Chantler [4]. Note that, these light directions
do not precisely match those provided in DiLiGenT, we use
the closest ones in the DiLiGenT dataset for testing. Our
method outperforms others except for the PS-FCN [3] un-
der the optimal light conﬁguration.

Pixel-wise normal estimation. Finally, we present a
comparison between pixel-wise and dense normal estima-
tion. In Fig. 9, we show the results yielded by our method
and PS-FCN [3] on a challenging synthetic object.
In
Fig. 9, we use one object from Blobby [10] which has been
excluded from our training dataset and render it by ran-
domly selecting a BRDF function from MERL [13] for each
pixel. We then test both methods using only 8 inputs. Note
that PS-FCN [3] suffers from the relationship between the
BRDF and the geometry of the object whereas our method,

Inputs

GT

Ours

Err:13.77

PS-FCN

Err:18.03

Figure 9. Performance of our method and PS-FCN for a spatially
varying BRDF when 8 input light source directions are shown.

despite some noise, still recovers a surface normal in good
accordance with the object itself, outperforming the PS-
FCN [3].

7. Conclusion

In this paper, we have proposed a deep learning approach
which applies a connection table that can select those illu-
minant directions that are most relevant to the surface nor-
mal prediction process. To this end, we have employed
a connection table and showed how, once the model is
trained, it can efﬁciently estimate the surface normal only
given a much-reduced number of input images without loss
in accuracy. Furthermore, we have tackled the problem of
cast shadows explicitly by introducing an occlusion layer
into the network. We have illustrated the utility of our
method for photometric stereo by comparing our results
with those yielded by a number of alternatives.

Acknowledgments: Part of this work was supported by
JST CREST Grant Number JP17942373, Japan.

87575

photometric stereo under inaccurate lighting. In Computer
Vision and Pattern Recognition (CVPR), 2017 IEEE Confer-
ence on, pages 350–359. IEEE, 2017. 2

[16] Hiroaki Santo, Masaki Samejima, Yusuke Sugano, Boxin
Shi, and Yasuyuki Matsushita. Deep photometric stereo net-
work. In Computer Vision Workshop (ICCVW), 2017 IEEE
International Conference on, pages 501–509. IEEE, 2017. 1,
2, 5, 6, 7

[17] Boxin Shi, Zhipeng Mo, Zhe Wu, Dinglong Duan, Sai Kit
Yeung, and Ping Tan. A benchmark dataset and evalua-
tion for non-lambertian and uncalibrated photometric stereo.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2018. 1, 2, 3, 6

[18] Tatsunori Taniai and Takanori Maehara. Neural inverse ren-
dering for general reﬂectance photometric stereo. In Interna-
tional Conference on Machine Learning, pages 4864–4873,
2018. 2

[19] Ran Wei, Antonio Robles-Kelly, and Jos´e ´Alvarez. Context
free band reduction using a convolutional neural network.
In Joint IAPR International Workshops on Statistical Tech-
niques in Pattern Recognition (SPR) and Structural and Syn-
tactic Pattern Recognition (SSPR), pages 86–96. Springer,
2018. 4

[20] Robert J Woodham. Photometric method for determining
surface orientation from multiple images. Optical engineer-
ing, 19(1):191139, 1980. 2, 6, 7, 8

[21] Lun Wu, Arvind Ganesh, Boxin Shi, Yasuyuki Matsushita,
Yongtian Wang, and Yi Ma. Robust photometric stereo via
low-rank matrix completion and recovery. In Asian Confer-
ence on Computer Vision, pages 703–717. Springer, 2010. 1,
2

[22] Tai-Pang Wu and Chi-Keung Tang. Photometric stereo via
IEEE transactions on pattern

expectation maximization.
analysis and machine intelligence, 32(3):546–560, 2010. 2

[23] Zexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi
Ramamoorthi. Deep image-based relighting from optimal
sparse samples. ACM Transactions on Graphics (TOG),
37(4):126, 2018. 2

References

[1] Jose M Alvarez and Mathieu Salzmann. Learning the num-
ber of neurons in deep networks. In Advances in Neural In-
formation Processing Systems, pages 2270–2278, 2016. 3

[2] Vasileios Argyriou, Stefanos Zafeiriou, Barbara Villarini,
and Maria Petrou. A sparse representation method for de-
termining the optimal illumination directions in photometric
stereo. Signal Processing, 93(11):3027–3038, 2013. 1

[3] Guanying Chen, Kai Han, and Kwan-Yee K Wong. Ps-fcn: A
ﬂexible learning framework for photometric stereo. In Euro-
pean Conference on Computer Vision, pages 3–19. Springer,
2018. 1, 6, 7, 8

[4] Ondrej Drbohlav and Mike Chantler. On optimal light con-
In Tenth IEEE Interna-
ﬁgurations in photometric stereo.
tional Conference on Computer Vision (ICCV’05) Volume 1,
volume 2, pages 1707–1712. IEEE, 2005. 8

[5] Aaron Hertzmann and Steven M Seitz. Example-based pho-
tometric stereo: Shape reconstruction with general, varying
brdfs. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27(8):1254–1264, 2005. 2

[6] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Computer Vision and Pattern Recognition (CVPR),
2017 IEEE Conference on, pages 2261–2269. IEEE, 2017. 3

[7] Zhuo Hui and Aswin C Sankaranarayanan.

Shape and
spatially-varying reﬂectance estimation from virtual exem-
plars. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 39(10):2060–2073, 2017. 2

[8] Satoshi Ikehata. Cnn-ps: Cnn-based photometric stereo for
general non-convex surfaces.
In European Conference on
Computer Vision, pages 3–19. Springer, 2018. 1, 2, 3, 6, 7, 8

[9] Satoshi Ikehata, David Wipf, Yasuyuki Matsushita, and Kiy-
oharu Aizawa. Robust photometric stereo using sparse
regression.
In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on, pages 318–325. IEEE,
2012. 2, 6, 7, 8

[10] Micah K Johnson and Edward H Adelson. Shape estima-
tion in natural illumination.
In Computer Vision and Pat-
tern Recognition (CVPR), 2011 IEEE Conference on, pages
2553–2560. IEEE, 2011. 6, 8

[11] Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol
Gregor, Micha¨el Mathieu, and Yann L Cun. Learning con-
volutional feature hierarchies for visual recognition.
In
Advances in neural information processing systems, pages
1090–1098, 2010. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 6

[13] Wojciech Matusik, Hanspeter Pﬁster, Matt Brand, and
Leonard McMillan. A data-driven reﬂectance model. ACM
Transactions on Graphics, 22(3):759–769, July 2003. 6, 8

[14] Yasuhiro Mukaigawa, Yasunori Ishii, and Takeshi Shaku-
naga. Analysis of photometric factors based on photometric
linearization. JOSA A, 24(10):3326–3334, 2007. 2

[15] Yvain Qu´eau, Tao Wu, Franc¸ois Lauze, Jean-Denis Durou,
and Daniel Cremers. A non-convex variational approach to

97576

