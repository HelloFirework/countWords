Multi-Person Pose Estimation

with Enhanced Channel-wise and Spatial Information

Kai Su†,

1

,

2, Dongdong Yu†,

2, Zhenqi Xu2, Xin Geng∗,

1, Changhu Wang∗,

2

1School of Computer Science and Engineering, Southeast University, Nanjing, China

{sukai,xgeng}@seu.edu.cn

2ByteDance AI Lab, Beijing, China

{sukai,yudongdong,xuzhenqi,wangchanghu}@bytedance.com

Abstract

Multi-person pose estimation is an important but chal-
lenging problem in computer vision. Although current ap-
proaches have achieved signiﬁcant progress by fusing the
multi-scale feature maps, they pay little attention to en-
hancing the channel-wise and spatial information of the
feature maps.
In this paper, we propose two novel mod-
ules to perform the enhancement of the information for the
multi-person pose estimation. First, a Channel Shufﬂe Mod-
ule (CSM) is proposed to adopt the channel shufﬂe oper-
ation on the feature maps with different levels, promoting
cross-channel information communication among the pyra-
mid feature maps. Second, a Spatial, Channel-wise At-
tention Residual Bottleneck (SCARB) is designed to boost
the original residual unit with attention mechanism, adap-
tively highlighting the information of the feature maps both
in the spatial and channel-wise context. The effectiveness
of our proposed modules is evaluated on the COCO key-
point benchmark, and experimental results show that our
approach achieves the state-of-the-art results.

1. Introduction

Multi-Person Pose Estimation aims to locate body parts
for all persons in an image, such as keypoints on the arms,
torsos, and the face. It is a fundamental yet challenging task
for many computer vision applications like activity recog-
nition [22] and human re-identiﬁcation [28]. Achieving ac-

† Equal contribution.
∗ X. Geng and C. Wang are the corresponding authors.
This work was performed while Kai Su worked as an intern at

ByteDance AI Lab.

This research was partially supported by the National Key Research &
Development Plan of China (No. 2017YFB1002801), the National Science
Foundation of China (61622203), the Collaborative Innovation Center of
Novel Software Technology and Industrialization, and the Collaborative
Innovation Center of Wireless Communications Technology.

Figure 1. The example of an input image (left) from the COCO
test-dev dataset [12] and its estimated pose (right) from our model.

curate localization results, however, is difﬁcult due to the
close-interaction scenarios, occlusions and different human
scales.

Recently, due to the involvement of deep convolutional
neural networks [10, 7], there has been signiﬁcant progress
on the problem of multi-person pose estimation [23, 16, 4,
3, 1, 15, 26]. Existing approaches for multi-person pose es-
timation can be roughly classiﬁed into two frameworks, i.e.,
top-down framework [23, 16, 4, 3] and bottom-up frame-
work [1, 15, 26]. The former one ﬁrst detects all human
bounding boxes in the image and then estimates the pose
within each box independently. The latter one ﬁrst detects
all body keypoints independently and then assembles the
detected body joints to form multiple human poses.

Although great progress has been made, it is still an open
problem to achieve accurate localization results. First, on
the one hand, high-level feature maps with larger receptive
ﬁelds are required in some challenging cases to infer the in-
visible and occluded keypoints, e.g., the right knee of the
human in Fig. 1. On the other hand, low-level feature maps
with larger resolutions are also helpful to the detailed reﬁne-
ment of the keypoints, e.g., the right ankle of the human in
Fig. 1. The trade-off between the low-level and high-level
feature maps is more complex in real scenarios. Second, the
feature fusion is even dynamic and the fused feature maps
always remain redundant. Therefore, the information which
is more important to the pose estimation should be adap-

5674

R-Conv-1

R-Conv-2

R-Conv-3

R-Conv-4

R-Conv-5

Conv 1x1

Conv 1x1

Conv 1x1

Conv 1x1

Conv and Downsample

Conv-2

Conv-3

Conv-4

Conv-5

Conv 1x1

Channel Shuffle Module

S-Conv-2

S-Conv-3

S-Conv-4

S-Conv-5

Concat

+

2x

Concat

+

t
a
c
n
o
C

Concat

Concat

+

4x

8x

loss

loss*

dimension reduction conv

S-Conv-x

shuffled feature maps

+

elem-sum

Spatial, Channel-wise 
Attention Residual
Bottleneck

Figure 2. Overview of our architecture. R-Conv-1∼5 are the last residual blocks of different feature maps from the ResNet backbone [7].
R-Conv-2∼5 are ﬁrst reduced to the same channel dimension of 256 by 1 × 1 convolution, denoted as Conv-2∼5. S-Conv-2∼5 means
the corresponding shufﬂed feature maps after the Channel Shufﬂe Module. S-Conv-2∼5 are then concatenated with Conv-2∼5 as the ﬁnal
enhanced pyramid features. Moreover, a Spatial, Channel-wise Attention Residual Bottleneck is proposed to adaptively enhance the fused
pyramid feature responses. Loss denotes the L2 loss and loss* means the L2 loss with Online Hard Keypoints Mining [3].

tively highlighted, e.g., with the help of attention mecha-
nism. According to the above analysis, in this paper, we
propose a Channel Shufﬂe Module (CSM) to further en-
hance the cross-channel communication between the fea-
ture maps across all scales. Moreover, a Spatial, Channel-
wise Attention Residual Bottleneck (SCARB) is designed
to adaptively enhance the fused feature maps both in the
spatial and channel-wise context.

To promote the information communication across the
channels among the feature maps at different resolution lay-
ers, we further exploit the channel shufﬂe operation pro-
posed in the ShufﬂeNet [27]. Different from ShufﬂeNet, in
this paper, we creatively adopt the channel shufﬂe operation
to enable the cross-channel information ﬂow among the fea-
ture maps across all scales. To the best of our knowledge,
the use of the channel shufﬂe operation to enhance the in-
formation of the feature maps is rarely mentioned in previ-
ous work for the multi-person pose estimation. As shown
in Fig. 2, the proposed Channel Shufﬂe Module (CSM)
performs on the feature maps Conv-2∼5 of different res-
olutions to obtain the shufﬂed feature maps S-Conv-2∼5.
The idea behind the CSM is that the channel shufﬂe oper-
ation can further recalibrate the interdependencies between
the low-level and high-level feature maps.

Moreover, we propose a Spatial, Channel-wise Attention
Residual Bottleneck (SCARB), integrating the spatial and
channel-wise attention mechanism into the original residual
unit [7]. As shown in Fig. 2, by stacking these SCARBs
together, we can adaptively enhance the fused pyramid fea-
ture responses both in the spatial and channel-wise context.

There is a trend of designing networks with attention mech-
anism, as it is effective in adaptively highlighting the most
informative components of an input feature map. However,
spatial and channel-wise attention has little been used in the
multi-person pose estimation yet.

As one of the classic methods belonging to the top-down
framework, Cascaded Pyramid Network (CPN) [3] was the
winner of the COCO 2017 keypoint Challenge [13]. Since
CPN is an effective structure for the multi-person pose
estimation, we apply it as the basic network structure in
our experiments to investigate the impact of the enhanced
channel-wise and spatial information. We evaluate the two
proposed modules on the COCO [12] keypoint benchmark,
and ablation studies demonstrate the effectiveness of the
Channel Shufﬂe Module and the Spatial, Channel-wise At-
tention Residual Bottleneck from various aspects. Experi-
mental results show that our approach achieves the state-of-
the-art results.

In summary, our main contributions are three-fold as fol-

lows:

• We propose a Channel Shufﬂe Module (CSM), which
can enhance the cross-channel information commu-
nication between the low-level and high-level feature
maps.

• We propose a Spatial, Channel-wise Attention Resid-
ual Bottleneck (SCARB), which can adaptively en-
hance the fused pyramid feature responses both in the
spatial and channel-wise context.

5675

• Our method achieves the state-of-the-art results on the

COCO keypoint benchmark.

The rest of this paper is organized as follows. First, re-
lated work is reviewed. Second, our method is described in
details. Then ablation studies are performed to measure the
effects of different parts of our system, and the experimental
results are reported. Finally, conclusions are given.

2. Related Work

This section reviews two aspects related to our method:

multi-scale fusion and visual attention mechanism.

2.1. Multi scale Fusion Mechanism

In the previous work for the multi-person pose estima-
tion, large receptive ﬁled is achieved by a sequential ar-
chitecture in the Convolutional Pose Machines [23, 1] to
implicitly capture the long-range spatial relations among
multi-parts, producing the increasingly reﬁned estimations.
However, low-level information is ignored along the way.
Stacked Hourglass Networks [16, 15] processes the feature
maps across all scales to capture various spatial relation-
ships of different resolutions, and adopt the skip layers to
preserve spatial information at each resolution. Moreover,
the Feature Pyramid Network architecture [11] is integrated
in the GlobalNet of the Cascaded Pyramid Network [3], to
maintain both the high-level and low-level information from
the feature maps of different scales.

2.2. Visual Attention Mechanism

Visual attention has achieved great success in various
tasks, such as the network architecture design [8], image
caption [2, 25] and pose estimation [4]. SE-Net [8] pro-
posed a “Squeeze-and-Excitation” (SE) block to adaptively
highlight the channel-wise feature maps by modeling the
channel-wise statistics. However, SE block only consid-
ers the channel-wise relationship and ignores the impor-
tance of the spatial attention in the feature maps. SCA-
CNN [2] proposed Spatial and Channel-wise Attentions in a
CNN for image caption. Spatial and channel-wise attention
not only encodes where (i.e., spatial attention) but also in-
troduces what (i.e., channel-wise attention) the important
visual attention is in the feature maps. However, spatial
and channel-wise attention has little been used in the multi-
person pose estimation yet. Chu et al. [4] proposed the ef-
fective multi-context attention model for the human pose es-
timation. However, our proposed spatial and channel-wise
attention residual bottleneck for the multi-person pose esti-
mation has not been mentioned in [4] yet.

3. Method

An overview of our proposed framework is illustrated
in Fig. 2. We adopt the effective Cascaded Pyramid Net-

Conv-2

Conv-3

Conv-4

Conv-5

Upsample

Upsample

Upsample

Concat

Channel Shuffle

Split

Downsample

Downsample

Downsample

C-Conv-2

C-Conv-3

C-Conv-4

C-Conv-5

Conv 1x1

Conv 1x1

Conv 1x1

Conv 1x1

S-Conv-2

S-Conv-3

S-Conv-4

S-Conv-5

Figure 3. Channel Shufﬂe Module. The module adopts the channel
shufﬂe operation on the pyramid features Conv-2∼5 to achieve the
shufﬂed pyramid features S-Conv-2∼5 with cross-channel com-
munication between different levels. The groups g is set as 4 here.

work (CPN) [3] as the basic network structure to explore
the effects of the Channel Shufﬂe Module and the Spatial,
Channel-wise Attention Residual Bottleneck for the multi-
person pose estimation. We ﬁrst brieﬂy review the structure
of the CPN, and then the detailed descriptions of our pro-
posed modules are presented.

3.1. Revisiting Cascaded Pyramid Network

Cascaded Pyramid Network (CPN) [3] is a two-step net-
work structure for the human pose estimation. Given a hu-
man box, ﬁrst, CPN uses the GlobalNet to locate some-
what “simple” keypoints based on the FPN architecture
[11]. Second, CPN adopts the ReﬁneNet with the Online
Hard Keypoints Mining mechanism to explicitly address the
“hard” keypoints.

As shown in Fig. 2, in this paper, for the GlobalNet,
the feature maps with different scales (i.e., R-Conv-2∼5)
extracted from the ResNet [7] backbone are ﬁrst reduced to
the same channel dimension of 256 by 1×1 convolution, de-
noted as Conv-2∼5. The proposed Channel Shufﬂe Module
then performs on the Conv-2∼5 to obtain the shufﬂed fea-
ture maps S-Conv-2∼5. Finally, S-Conv-2∼5 are concate-
nated with the original pyramid features Conv-2∼5 as the
ﬁnal enhanced pyramid features, which will be used as the
U-shape FPN architecture. In addition, for the ReﬁneNet,
a boosted residual bottleneck with spatial, channel-wise at-
tention mechanism is proposed to adaptively highlight the
feature responses transferred from the GlobalNet both in the
spatial and channel-wise context.

5676

3.2. CSM: Channel Shufﬂe Module

As the levels of the feature maps are greatly enriched
by the depth of the layers in the deep convolutional neu-
ral networks, many visual tasks have made signiﬁcant im-
provements, e.g., image classiﬁcation [7]. However, for the
multi-person pose estimation, there are still limitations in
the trade-off between the low-level and high-level feature
maps. The channel information with different characteris-
tics among different levels can complement and reinforce
with each other. Motivated by this, we propose the Channel
Shufﬂe Module (CSM) to further recalibrate the interdepen-
dencies between the low-level and high-level feature maps.
As shown in Fig. 3, assuming that the pyramid features
extracted from the ResNet backbone are denoted as Conv-
2∼5 (with the same channel dimension of 256). Conv-3∼5
are ﬁrst upsampled to the same resolution as the Conv-
2, and then these feature maps are concatenated together.
After that, the channel shufﬂe operation is performed on
the concatenated features to fuse the complementary chan-
nel information among different levels. The shufﬂed fea-
tures are then split and downsampled to the original resolu-
tion separately, denoted as C-Conv-2∼5. C-Conv-2∼5 can
be viewed as the features consisting of the complementary
channel information from feature maps among different lev-
els. After that, we perform 1× 1 convolution to further fuse
C-Conv-2∼5, and obtain the shufﬂed features, denoted as S-
Conv-2∼5. We then concatenate the shufﬂed feature maps
S-Conv-2∼5 with the original pyramid feature maps Conv-
2∼5 to achieve the ﬁnal enhanced pyramid feature repre-
sentations. These enhanced pyramid feature maps not only
contain the information from the original pyramid features,
but also the fused cross-channel information from the shuf-
ﬂed pyramid feature maps.

3.2.1 Channel Shufﬂe Operation

As described in the ShufﬂeNet [27], a channel shufﬂe op-
eration can be modeled as a process composed of “reshape-
transpose-reshape” operations. Assuming the concatenated
features from different levels as Ψ, and the channel dimen-
sion of Ψ in this paper is 256 ∗ 4 = 1024. We ﬁrst reshape
the channel dimension of Ψ into (g, c), where g is the num-
ber of groups, c = 1024/g. Then, we transpose the channel
dimension to (c, g), and ﬂatten it back to 1024. After the
channel shufﬂe operation, Ψ is fully related in the channel
context. The number of groups g will be discussed in the
ablation studies of the experiments.

3.3. ARB: Attention Residual Bottleneck

Based on the enhanced pyramid feature representations
introduced above, we attach our boosted Attention Resid-
ual Bottleneck to adaptively enhance the feature responses
both in the spatial and channel-wise context. As shown in

!

Residual

+

"!

!

Residual

#×%×&

Conv

#×%×1

Sigmoid

#×%×1

Spatial 
Attention

)

Residual Bottleneck

Scale

#×%×&

Global Pooling

1×1×&

FC

1×1×&

ReLU

1×1×&

FC

1×1×&

Sigmoid

1×1×&

Channel-wise 
Attention

(

Scale

#×%×&

+

"!

#×%×&

Attention Residual Bottleneck

Figure 4. The schema of the original Residual Bottleneck (left) and
the Spatial, Channel-wise Attention Residual Bottleneck (right),
which is composed of the spatial attention and channel-wise atten-
tion. Dashed links indicate the identity mapping.

Fig. 4, our Attention Residual Bottleneck learns the spatial
attention weights β and the channel-wise attention weights
α respectively.

3.3.1 Spatial Attention

Applying the whole feature maps may lead to sub-optimal
results due to the irrelevant regions. Different from paying
attention to the whole image region equally, spatial attention
mechanism attempts to adaptively highlight the task-related
regions in the feature maps.

Assuming the input of the spatial attention is V ∈
RH×W ×C , and the output of the spatial attention is V
∈
RH×W ×C , then we can get V
= β ∗ V , where ∗ means
the element-wise multiplication in the spatial context. The
spatial-wise attention weights β ∈ RH×W is generated by
a convolutional operation W ∈ R1×1×C followed by a sig-
moid function on the input V , i.e.,

′

′

β = Sigmoid(W V ),

(1)

where W denotes the convolution weights, and Sigmoid
means the sigmoid activation function.

Finally the learned spatial attention weights β is rescaled

on the input V to achieve the output V

′

.

v

′

i,j = βi,j ∗ vi,j,

(2)

where ∗ means the element-wise multiplication between the
i, j-th element of β and V in the spatial context.

5677

3.3.2 Channel-wise Attention

3.3.3 SCARB: Spatial, Channel-wise Attention Resid-

Since convolutional ﬁlters perform as a pattern detector, and
each channel of a feature map after the convolutional oper-
ation is the feature activations of the corresponding convo-
lutional ﬁlters. The channel-wise attention mechanism can
be viewed as a process of adaptively selecting the pattern
detectors, which are more important to the task.

′

Assuming the input of the channel-wise attention is U ∈
RH×W ×C , and the output of the channel-wise attention is
∈ RH×W ×C , then we can get U
= α ∗ U , where ∗
U
means the element-wise multiplication in the channel-wise
context, α ∈ RC is the channel-wise attention weights. Fol-

lowing the SE-Net [8], channel-wise attention can be mod-
eled as a process consisting of two steps, i.e., squeeze and
excitation step respectively.

′

In the squeeze step, global average pooling operation is
ﬁrst performed on the input U to generate the channel-wise
statistics z ∈ RC , where the c-th element of z is calculated

by

ual Bottleneck

The ﬁrst
type applies the spatial attention before the
channel-wise attention, as shown in Fig. 4. All processes
are summarized as follows:

′

= F (X),

X
Y = α ∗ (β ∗ X
fX = σ(X + Y ),

′

),

(6)

where the function F (X) represents the residual mapping

to be learned in the ResNet [7], fX is the output attention

feature maps with the enhanced spatial and channel-wise
information.

3.3.4 CSARB: Channel-wise, Spatial Attention Resid-

ual Bottleneck

Similarly, the second type is a model with channel-wise at-
tention implemented ﬁrst, i.e.,

= F (X),

X
Y = β ∗ (α ∗ X
fX = σ(X + Y ).

′

),

(7)

The choice of the SCARB and CSARB will be discussed

in the ablation studies of the experiments.

zc =

1

H×W

HX

WX

i=1

j=1

uc(i, j),

(3)

′

where uc ∈ RH×W is the c-th element of the input U .

In the excitation step, a simple gating mechanism with a
sigmoid activation is performed on the channel-wise statis-
tics z, i.e.,

α = Sigmoid(W2(σ(W1(z)))),

(4)

4. Experiments

where W1 ∈ RC×C and W2 ∈ RC×C denotes two fully
connected layers, σ means the ReLU activation function
[14], and Sigmoid means the sigmoid activation function.
Finally, the learned channel-wise attention weights α is
rescaled on the input U to achieve the output of the channel-
wise attention U

, i.e.,

′

Our multi-person pose estimation system follows the
top-down pipeline. First, a human detector is applied to
generate all human bounding boxes in the image. Then for
each human bounding box, we apply our proposed network
to predict the corresponding human pose.

u

′

c = αc ∗ uc,

4.1. Experimental Setup

(5)

4.1.1 Datasets and Evaluation Criterion

where ∗ means the element-wise multiplication between the
c-th element of α and U in the channel-wise context.

As shown in Fig. 4, assuming the input of the residual

bottleneck is X ∈ RH×W ×C , the attention mechanism is

performed on the non-identity branch of the residual mod-
ule, and the spatial, channel-wise attention act before the
summation with the identity branch. There are two different
implementation orders of the spatial attention and channel-
wise attention in the residual bottleneck [7], i.e., SCARB:
Spatial, Channel-wise Attention Residual Bottleneck and
CSARB: Channel-wise, Spatial Attention Residual Bottle-
neck respectively, which are described as follows.

We evaluate our model on the challenging COCO keypoint
benchmark [12]. Our models are only trained on the COCO
trainval dataset (includes 57K images and 150K person in-
stances) with no extra data involved. Ablation studies are
validated on the COCO minival dataset (includes 5K im-
ages). The ﬁnal results are reported on the COCO test-dev
dataset (includes 20K images) compared with the public
state-of-the-art results. We use the ofﬁcial evaluation met-
ric [12] that reports the OKS-based AP (average precision)
in the experiments, where the OKS (object keypoints simi-
larity) deﬁnes the similarity between the predicted pose and
the ground truth pose.

5678

Table 1. Ablation study on the Channel Shufﬂe Module (CSM)
with different groups g on the COCO minival dataset. CSM-g
denotes the Channel Shufﬂe Module with g groups. The Attention
Residual Bottleneck is not used in this experiment.

Table 4. Comparison with the 8-stage Hourglass [16], CPN [3]
and Simple Baselines [24] on the COCO minival dataset. Their
results are cited from [3, 24]. “*” means the model training with
the Online Hard Keypoints Mining.

Method

CPN (baseline)
CPN + CSM-2
CPN + CSM-4
CPN + CSM-8
CPN + CSM-16
CPN + CSM-32
CPN + CSM-64
CPN + CSM-128
CPN + CSM-256

AP
69.4
70.4
71.7
71.4
71.2
70.1
70.7
71.0
71.6

Table 2. Ablation study on the Attention Residual Bottleneck on
the COCO minival dataset. SCARB denotes the Spatial, Channel-
wise Attention Residual Bottleneck, CSARB denotes the Channel-
wise, Spatial Attention Residual Bottleneck. The Channel Shufﬂe
Module is not used in this experiment.

Method

AP
69.4
CPN (baseline)
CPN + CSARB 70.4
CPN + SCARB 70.8

Table 3. Component analysis on the Channel Shufﬂe Module with
4 groups (CSM-4) and the Spatial, Channel-wise Attention Resid-
ual Bottleneck (SCARB) on the COCO minival dataset. Based on
the baseline CPN [3], we gradually add the CSM-4 and SCARB
for ablation studies. The last line shows the total improvement
compared with the baseline CPN.

Method

CPN (baseline)
CPN + CSM-4
CPN + SCARB

CPN + CSM-4 + SCARB

CSM-4

√

√

SCARB AP
69.4
71.7
70.8
72.1

√
√

4.1.2 Training Details

Our pose estimation model is implemented in Pytorch [18].
For the training, 4 V100 GPUs on a server are used. Adam
[9] optimizer is adpoted. The base learning rate is set to
5e − 4, and is decreased by a factor of 0.1 at 90 and 120
epochs, and ﬁnally we train for 140 epochs. The input size
of the image for the network is made to a ﬁxed aspect ratio
of height : width = 4 : 3, e.g., 256 × 192 is used as the de-
fault resolution, the same as the CPN [3]. L2 loss is used for
the GlobalNet, and following the CPN, we only punish the
top 8 keypoints losses in the Online Hard Keypoint Mining
of the ReﬁneNet. Data augmentation during the training in-
cludes the random rotation (−40◦ ∼ +40◦) and the random
scale (0.7 ∼ 1.3).
Our ResNet backbone is initialized with the weights
of the public-released Imagenet [20] pre-trained model.
ResNet backbones with 50, 101 and 152 layers are exper-
imented. ResNet-50 is used by default, unless otherwise
noted.

Method

Backbone

8-stage Hourglass
8-stage Hourglass

-
-

ResNet-50
CPN (baseline)
ResNet-50
CPN (baseline)
ResNet-50
CPN* (baseline)
ResNet-50
CPN* (baseline)
Simple Baselines ResNet-50
Simple Baselines ResNet-50
ResNet-50
ResNet-50

Ours*
Ours*

Input Size
256 × 192
256 × 256
256 × 192
384 × 288
256 × 192
384 × 288
256 × 192
384 × 288
256 × 192
384 × 288

AP
66.9
67.1
68.6
70.6
69.4
71.6
70.6
72.2
72.1
73.8

CPN

Ours

Figure 5. Visual heatmaps of CPN and our model on the COCO
minival dataset. From left to right are input images, predicted
heatmaps and predicted poses. Best viewed in zoom and color.

4.1.3 Testing Details

For the testing, a top-down pipeline is applied. For the
COCO minival dataset, we use the human detection results
provided by the CPN [3] for the fair comparison, which re-
ports the human detection AP 55.3. For the COCO test-dev
dataset, we adopt the SNIPER [21] as the human detector,
which achieves the human detection AP 58.1. Following
the common practice in [3, 16], the keypoints are estimated
on the averaged heatmaps of the original and ﬂipped image.
A quarter offset in the direction from the highest response
to the second highest response is used to obtain the ﬁnal
keypoints.

4.2. Component Ablation Studies

In this section, we conduct the ablation studies on the
Channel Shufﬂe Module and the Attention Residual Bottle-
neck on the COCO minival dataset. The ResNet-50 back-
bone and the input size of 256 × 192 are used by default in
the all ablation studies.

5679

Table 5. Comparison of ﬁnal results on the COCO test-dev dataset. Top: methods in the literature, trained only with the COCO trainval
dataset. Middle: results submitted to the COCO test-dev leaderboard [13]. “*” means that the method involves extra data for the training.
“+” indicates the results using the ensembled models. Bottom: the results of our single model, trained only with the COCO trainval dataset.
> indicates the results using the single model with ﬂip and rotation testing strategy.
AP .5
84.9
87.3

CMU-Pose [1]
Mask-RCNN [6]

AP .75
67.5
68.7

AP (L)
68.2
71.4

ResNet-50-FPN

AP
61.8
63.1

57.1
57.8

AR
66.5

Input Size

Backbone

Method

AP (M)

-
-

-

-

Associative Embedding

[15]

G-RMI [17]

CPN [3]

Simple Baselines [24]
Simple Baselines [24]
FAIR Mask R-CNN*

[13]

G-RMI* [13]

oks* [13]

-

ResNet-101

ResNet-Inception

ResNet-101
ResNet-152

512 × 512
353 × 257
384 × 288
384 × 288
384 × 288

ResNet-101-FPN

-

ResNet-152

-

bangbangren*+ [13]

ResNet-101

CPN+ [13]

ResNet-Inception

Ours
Ours
Ours
Ours
Ours
Ours
Ours>
Ours>

ResNet-50
ResNet-50
ResNet-101
ResNet-101
ResNet-152
ResNet-152
ResNet-101
ResNet-152

353 × 257

-
-

384 × 288
256 × 192
384 × 288
256 × 192
384 × 288
256 × 192
384 × 288
384 × 288
384 × 288

65.5

64.9
72.1
73.2
73.8

69.2

71.0
72.0
72.8
73.0
71.4
73.2
71.8
73.8
72.3
74.3
74.1
74.6

86.8

85.5
91.4
91.4
91.7

90.4

87.9
90.3
89.4
91.7
91.3
91.9
91.3
91.7
91.4
91.8
91.8
91.8

72.3

71.3
80.0
80.9
81.2

77.0

77.7
79.7
79.6
80.9
79.8
81.0
80.1
81.4
80.6
81.9
81.7
82.1

60.6

62.3
68.7
69.7
70.3

64.9

69.0
67.6
68.6
69.5
68.3
69.6
68.7
70.4
69.2
70.7
70.6
70.9

72.6

70.0
77.2
79.5
80.0

76.3

75.2
78.4
80.0
78.1
77.1
79.3
77.3
79.6
77.8
80.2
80.0
80.6

70.2

69.7
78.5
78.6
79.1

75.2

75.8
77.1
78.7
79.0
77.1
78.5
78.8
80.3
79.2
80.5
80.4
80.7

4.2.1 Groups g in the Channel Shufﬂe Module

4.2.3 Component Analysis

In this experiment, we explore the performances of the
Channel Shufﬂe Module with different groups on the
COCO minival dataset. CSM-g denotes the Channel Shufﬂe
Module with g groups and the groups g controls the degree
of the cross-channel feature maps fusion. The ResNet-50
backbone and the input size of 256×192 are used by default,
and the Attention Residual Bottleneck is not used here. As
the Table 1 shows, 4 groups achieves the best AP of 71.7. It
indicates that when only using the Channel Shufﬂe Module
with 4 groups (CSM-4), we can achieve 2.3 AP improve-
ment compared with the baseline CPN. Therefore, 4 groups
(CSM-4) is selected ﬁnally.

4.2.2 Attention Residual Bottleneck: SCARB and

CSARB

In this experiment, we explore the effects of different imple-
mentation orders of the spatial attention and the channel-
wise attention in the Attention Residual Bottleneck, i.e.,
SCARB and CSARB. The ResNet-50 backbone and the in-
put size of 256 × 192 are used by default, and the Channel
Shufﬂe Module is not used here. As shown in Table 2, the
SCARB achieves the best AP of 70.8. It indicates that when
only using the SCARB, our model outperforms the baseline
CPN by 1.4 AP. Therefore, SCARB is selected by default.

In this experiment, we analyze the importance of each pro-
posed component on the COCO minival dataset, i.e., the
Channel Shufﬂe Module and the Attention Residual Bot-
tleneck. According to Table 1 and 2, the Channel Shufﬂe
Module with 4 groups (CSM-4) and the Spatial, Channel-
wise Attention Residual Bottleneck (SCARB) are selected
ﬁnally. Accroding to Table 3, compared with the 69.4 AP
of the baseline CPN, with only the CSM-4 used, we can
achieve 71.7 AP, and with only the SCARB used, we can
achieve 70.8 AP. With all the proposed components used,
we can achieve 72.1 AP, with the improvement of 2.7 AP
over the baseline CPN.

4.3. Comparisons on COCO minival dataset

Table 4 compares our model with the 8-stage Hourglass
[16], CPN [3] and Simple Baselines [24] on the COCO
minival dataset. The human detection AP of the 8-stage
Hourglass and CPN are the same 55.3 as ours. The hu-
man detection AP reported in the Simple Baselines is 56.4.
Compared with the 8-stage Hourglass, both methods use an
input size of 256 × 192, our model has an improvement of
5.2 AP. CPN and our model both use the Online Hard Key-
points Mining, our model outperforms the CPN by 2.7 AP
for the input size of 256 × 192 and 2.2 AP for the input size
of 384 × 288. Compared with the Simple Baselines, our
model outperforms 1.5 AP for the input size of 256 × 192,
and 1.6 AP for the input size of 384 × 288. Fig. 6 demon-
strates the visual heatmaps of CPN and our model on the

5680

Figure 6. Qualitative results of our model on the COCO test-dev dataset. Our model deals well with the diverse poses, occlusions and
cluttered scenes.

Table 6. Comparison between the human detection performance
and the pose estimation performance on the COCO test-dev
dataset. All pose estimation methods are trained with the ResNet-
152 backbone and the 384 × 288 input size.

Pose Method

Det Method

Human AP

Pose AP

Simple Baselines [24]

Faster-RCNN [19]

Ours
Ours
Ours

Deformable [5]

- [3]

SNIPER [21]

60.9
45.8
57.2
58.1

73.8
72.9
73.8
74.3

COCO minival dataset. As shown in Fig. 6, our model still
works in the scenarios (e.g., close-interactions, occlusions)
where CPN can not well deal with.

4.4. Experiments on COCO test dev dataset

In this section, we compare our model with the state-of-
the-art methods on the COCO test-dev dataset, and analyze
the relationships between the human detection performance
and the corresponding pose estimation performance.

4.4.1 Comparison with the state-of-the-art Methods

Table 5 compares our model with other state-of-the-art
methods on the COCO test-dev dataset. For the CPN, a hu-
man detector with human detection AP 62.9 on the COCO
minival dataset is used. For the Simple Baselines, a human
detector with human detection AP 60.9 on the COCO test-
dev dataset is used. Without extra data for training, our sin-
gle model can achieve 73.8 AP with the ResNet-101 back-
bone, and 74.3 AP with the ResNet-152 backbone, which
outperform both CPN’s single model 72.1 AP, ensembled
model 73.0 AP and Simple Baselines 73.8 AP. Moreover,
when using the averaged heatmaps of the original, ﬂipped
and rotated (+30◦, −30◦ is used here) images, our single
model can achieve 74.1 AP with the ResNet-101 backbone,
and 74.6 AP with the ResNet-152 backbone. Fig. 6 demon-
strates the poses predicted by our model on the COCO test-
dev dataset.

4.4.2 Human Detection Performance

Table 6 shows the relationships between the human detec-
tion performance and the corresponding pose estimation
performance on the COCO test-dev dataset. Our model
and Simple Baselines [24] are compared in this experiment.
Both models are trained with the ResNet-152 backbone and
the 384 × 288 input size. The Simple Baselines adopts the
Faster-RCNN [19] as the human detector, which reports the
human detection AP 60.9 in their paper. For our model,
we adopt the SNIPER [21] as the human detector, which
achieves the human detection AP 58.1. Moreover, we also
use the Deformable Convolutional Networks [5] (achieves
the human detection AP 45.8) and the human detection re-
sults provided by the CPN [3] (reports the human detection
AP 57.2) for comparison.

From the table, we can see that the pose estimation AP
gains increasingly when the human detection AP increases.
For example, when the human detection AP increases from
57.2 to 58.1, the pose estimation AP of our model increases
from 73.8 to 74.3. However, although the human detection
AP 60.9 of the Simple Baselines is higher than ours 58.1
AP, the pose estimation AP 73.8 of the Simple Baselines is
lower than ours 74.3 AP. Therefore, we can conclude that
it is more important to enhance the accuracy of the pose
estimator than the human detector.

5. Conclusions

In this paper, we tackle the multi-person pose estima-
tion with the top-down pipeline. The Channel Shufﬂe Mod-
ule (CSM) is proposed to promote the cross-channel infor-
mation communication among the feature maps across all
scales, and a Spatial, Channel-wise Attention Residual Bot-
tleneck (SCARB) is designed to adaptively highlight the
fused pyramid feature maps both in the spatial and channel-
wise context. Overall, our model achieves the state-of-the-
art performance on the COCO keypoint benchmark.

5681

[17] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tomp-
son, C. Bregler, and K. Murphy. Towards accurate multi-
person pose estimation in the wild.
In CVPR, volume 3,
page 6, 2017.

[18] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017.

[19] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in neural information processing systems, pages
91–99, 2015.

[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015.

[21] B. Singh, M. Najibi, and L. S. Davis. Sniper: Efﬁcient multi-
scale training. In Advances in Neural Information Processing
Systems, pages 9333–9343, 2018.

[22] C. Wang, Y. Wang, and A. L. Yuille. An approach to pose-
based action recognition.
In Computer Vision and Pattern
Recognition (CVPR), 2013 IEEE Conference on, pages 915–
922. IEEE, 2013.

[23] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con-
volutional pose machines. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
4724–4732, 2016.

[24] B. Xiao, H. Wu, and Y. Wei. Simple baselines for human
pose estimation and tracking. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 466–
481, 2018.

[25] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image caption-
ing with semantic attention. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
4651–4659, 2016.

[26] A. Zanﬁr, E. Marinoiu, M. Zanﬁr, A.-I. Popa, and C. Smin-
chisescu. Deep network for the integrated 3d sensing of mul-
tiple people in natural images. In Advances in Neural Infor-
mation Processing Systems, pages 8420–8429, 2018.

[27] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An ex-
tremely efﬁcient convolutional neural network for mobile de-
vices. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 6848–6856, 2018.

[28] L. Zheng, Y. Huang, H. Lu, and Y. Yang. Pose invariant
embedding for deep person re-identiﬁcation. arXiv preprint
arXiv:1701.07732, 2017.

References

[1] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-
person 2d pose estimation using part afﬁnity ﬁelds. In CVPR,
volume 1, page 7, 2017.

[2] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and
T.-S. Chua. Sca-cnn: Spatial and channel-wise attention in
convolutional networks for image captioning.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5659–5667, 2017.

[3] Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun.
Cascaded pyramid network for multi-person pose estimation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 7103–7112, 2018.

[4] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and
X. Wang. Multi-context attention for human pose estima-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1831–1840, 2017.

[5] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.
Deformable convolutional networks. In Proceedings of the
IEEE international conference on computer vision, pages
764–773, 2017.

[6] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.
In Computer Vision (ICCV), 2017 IEEE International Con-
ference on, pages 2980–2988. IEEE, 2017.

[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[8] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7132–7141, 2018.

[9] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[11] T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and
S. Belongie. Feature pyramid networks for object detection.
In CVPR, volume 1, page 4, 2017.

[12] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European conference on computer
vision, pages 740–755. Springer, 2014.

[13] MS-COCO.

Coco keypoint

leaderboard.

http://

cocodataset.org/.

[14] V. Nair and G. E. Hinton. Rectiﬁed linear units improve
restricted boltzmann machines. In Proceedings of the 27th
international conference on machine learning (ICML-10),
pages 807–814, 2010.

[15] A. Newell, Z. Huang, and J. Deng. Associative embedding:
End-to-end learning for joint detection and grouping.
In
Advances in Neural Information Processing Systems, pages
2274–2284, 2017.

[16] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In European Conference
on Computer Vision, pages 483–499. Springer, 2016.

5682

