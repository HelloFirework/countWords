Perceive Where to Focus: Learning Visibility-aware Part-level Features

for Partial Person Re-identiﬁcation

Yifan Sun1, Qin Xu1, Yali Li1, Chi Zhang2, Yikang Li1, Shengjin Wang1 ∗, Jian Sun2

1Tsinghua University 2Megvii Technology

{sunyf15, xuq16, liyk11}@mails.tsinghua.edu.cn

{liyali13, wgsgj}@tsinghua.edu.cn

{zhangchi, sunjian}@megvii.com

Abstract

This paper considers a realistic problem in person re-
identiﬁcation (re-ID) task, i.e., partial re-ID. Under par-
tial re-ID scenario, the images may contain a partial ob-
servation of a pedestrian.
If we directly compare a par-
tial pedestrian image with a holistic one, the extreme spa-
tial misalignment signiﬁcantly compromises the discrimi-
native ability of the learned representation. We propose
a Visibility-aware Part Model (VPM), which learns to per-
ceive the visibility of regions through self-supervision. The
visibility awareness allows VPM to extract region-level fea-
tures and compare two images with focus on their shared
regions (which are visible on both images). VPM gains
two-fold beneﬁt toward higher accuracy for partial re-ID.
On the one hand, compared with learning a global fea-
ture, VPM learns region-level features and beneﬁts from
ﬁne-grained information. On the other hand, with visibility
awareness, VPM is capable to estimate the shared regions
between two images and thus suppresses the spatial mis-
alignment. Experimental results conﬁrm that our method
signiﬁcantly improves the learned representation and the
achieved accuracy is on par with the state of the art.

1. Introduction

Person re-identiﬁcation (re-ID) aims to spot the appear-
ances of same person in different observations by com-
paring the query image with the gallery images (i.e., the
database). In spite that the re-ID research community has
achieved signiﬁcant progress during the past few years, re-
ID systems are still faced with a series of realistic difﬁcul-
ties. A prominent challenge is the partial re-ID problem
[34, 7, 33, 36], which requires accurate retrieval with partial
observation of the pedestrian. More concretely, in realistic
re-ID systems, a pedestrian may happen to be partially oc-
cluded or be walking out of the ﬁeld of camera view, and

∗Corresponding author.

(a)

(b)

shared region features

(c)

Figure 1. Two challenges related to partial-re-ID and our solution
with the proposed VPM. (a) aggravation of spatial misalignment,
(b) distracting noises from unshared regions (the blue region on the
left image) and (c) VPM locates visible regions on a given image
and extracts region-level features. With visibility-awareness, VPM
compares two images by focusing on their shared regions.

the camera fails to capture the holistic pedestrian.

Intuitively, partial re-ID increases the difﬁculty to make
correct retrieval. Analytically, we ﬁnd that partial re-
ID raises two more unique challenges, compared with the
holistic person re-ID, as illustrated in Fig. 1.

• First, partial re-ID aggravates the spatial misalignment
between probe and gallery images. Under holistic re-
ID setting, the spatial misalignment mainly originates
from the articulate movement of pedestrian and the
viewpoint variation. Under partial re-ID setting, even
when two pedestrian with same pose are captured from
same viewpoints, there still exists severe spatial mis-
alignment between the two images (Fig. 1 (a)).

• Second, when we directly compare a partial pedestrian
against a holistic one, the unshared body regions in the
holistic pedestrian become distracting noises, rather

393

than discriminative clues. We note that the same sit-
uation also happens when any two compared images
contain different proportion of the holistic pedestrian
(Fig. 1 (b)).

We propose the Visibility-aware Part Model (VPM) for
partial re-ID. VPM avoids or alleviates the two unique dif-
ﬁculties related to partial re-ID by focusing on their shared
regions, as shown in Fig. 1 (c). More speciﬁcally, we ﬁrst
deﬁne a set of regions on the holistic person image. Dur-
ing training, given partial pedestrian images, VPM learns to
locate all the pre-deﬁned regions on convolutional feature
maps. After locating each region, VPM perceives which re-
gions are visible and learns region-level features. During
testing, given two images to be compared, VPM ﬁrst cal-
culates the local distances between their shared regions and
then concludes the overall distance.

VPM gains two-fold beneﬁt toward higher accuracy for
partial re-ID. On the one hand, compared with learning a
global feature, VPM learns region-level features and thus
beneﬁts from ﬁne-grained information, which is similar to
the situation in holistic person re-ID [23, 12]. On the other
hand, with visibility-awareness, VPM is capable to estimate
the shared regions between two images and thus suppresses
the spatial misalignment as well as the noises originated
from unshared regions. Experimental results conﬁrm that
VPM achieves signiﬁcant improvement on partial re-ID ac-
curacy, compared with a global feature learning baseline
[32], as well as a strong part-based convolutional baseline
[23]. The achieved performance are on par with the state of
the art.

Moreover, VPM is

featured for employing self-
supervision for learning the region visibility awareness. We
randomly crop partial pedestrian images from the holistic
ones and automatically generate region labels, yielding the
so-called self-supervision. Self-supervision enables VPM
to learn locating pre-deﬁned regions. It also helps VPM to
focus on visible regions during feature learning, which is
critical to the discriminative ability of the learned features,
as to be accessed in Section 4.4.

The main contributions of this paper are summarized as

follows:

• We propose a visibility-aware part model (VPM) for
partial re-ID task. VPM learns to locate the visible re-
gions on pedestrian images through self-supervision.
Given two images to be compared, VPM conducts
a region-to-region comparison within their shared re-
gions, and thus signiﬁcantly suppresses the spatial mis-
alignment as well as the distracting noises originated
from unshared regions.

• We conduct extensive partial re-ID experiments on
both synthetic datasets and realistic datasets and val-
idate the effectiveness of VPM. On two realistic

dataset, Partial-iLIDs and Partial-ReID, VPM achieves
performance on par with the state of the art. So far as
we know, few previous works on partial re-ID reported
the performance on synthetic large-scale datasets e.g.,
Market-1501 or DukeMTMC-ReID. We experimen-
tally validate that VPM can be easily scaled up to
large-scale (synthetic) partial re-ID datasets, due to its
fast matching capacity.

2. Related Works

2.1. Deeply learned part features for re ID

Deep learning methods currently dominate the re-ID re-
search community with signiﬁcant superiority on retrieval
accuracy [32]. Recent works [23, 12, 26, 29, 22, 28, 16]
further advance the state of the art on holistic person re-ID,
through learning part-level deep features. For example, Wei
et al. [26], Kalayeh et al. [12] and Sun et al. [23] extract
several region parts, with pose estimation [17, 27, 10, 18, 1],
human parsing [2, 5] and uniform partitioning, respectively.
Then they learn a respective feature for each part and as-
semble the part-level features to form the ﬁnal descriptor.
These progresses motivate us to extend learning part-level
features to the speciﬁed problem of partial re-ID.

However, learning part-level features does not naturally
improve partial re-ID. We ﬁnd that PCB [23], which main-
tains the latest state of the art on holistic person re-ID, en-
counters a substantial performance decrease when applied
in partial re-ID scenario. The achieved retrieval accuracy
even drops below the global feature learning baseline (to be
accessed in Sec. 4.2). Arguably, it is because part mod-
els rely on precisely locating each part and are inherently
more sensitive to the severe spatial misalignment problem
in partial re-ID.

Our method is similar to PCB in that both methods per-
form uniform division instead of semantic body parts for
part extraction. Moreover, similar to SPReID [12], our
method also uses probability maps to extract each part dur-
ing inference. However, while SPReID requires an extra
human parser and human parsing dataset (strong supervi-
sion) for learning part extraction, our method relies on self-
supervision. During matching stage, both PCB and SPReID
adopt the common strategy of concatenating part features.
In contrast, VPM ﬁrst measures the region-to-region dis-
tance and then conclude the overall distance by dynamically
crediting the local distances with high visibility conﬁdence.

2.2. Self supervised learning

Self-supervision learning is a speciﬁed unsupervised
learning approach. It explores the visual information to au-
tomatically generate surrogate supervision signal for feature
learning [19, 25, 13, 3, 14]. Larsson et al. [13] train the deep
model to predict per-pixel color histograms and consequen-

394

R1

R2

R3

Pixel g

conv

Region  locator 

1X1 conv

Softmax

Holistic image

Partial image

Tensor T

	
!	
	

Feature extractor

WP

WP

WP

C1

C2

C3

Region 
features

Visibility 
scores

probability maps

Figure 2. The structure of VPM. We ﬁrst deﬁne p = m×n (3× 1 in the ﬁgure for instance) densely aligned rectangle regions on the holistic
pedestrian. VPM resizes a partial pedestrian image to ﬁxed size, inputs it into a stack of convolutional layers (“conv”) and transforms it
into a 3D tensor T . Upon T , VPM appends a region locator to discover each regions through pixel-wise classiﬁcation. By predicting
a probability of belonging to each region for every pixel g, the region locator generates p probability maps to infer the location of each
region. It also generates p visibility scores through “P” operation over each probability map. Given the predicted probability maps, the
feature extractor extracts a respective feature for each pre-deﬁned region through weighted pooling (“WP”). VPM, as a whole, outputs p
region-level features and p visibility scores for inference.

tially facilitate automatic colorization. Doersch et al. [3]
and Noroozi et al. [19] propose to predict the relative posi-
tion of image patches. Gidaris et al. train the deep model to
recognize the rotation applied to original images.

Self-supervision is an elemental tool in our work. We
employ self-supervision to learn visibility awareness. VPM
is especially close to [3] and [19] in that all the three meth-
ods employ the position information of patches for self-
supervision. However, VPM signiﬁcantly differs from them
in the following aspects.

Self-supervision signal. [3] randomly samples a patch
and one of its eight possible neighbors, and then trains the
deep model to recognize the spatial conﬁguration. Simi-
larly, [19] encodes the neighborhood relationship into a jig-
saw puzzle. Different from [3] and [19], VPM does not
explore the spatial relationship between multiple images or
patches. VPM pre-deﬁnes a division on the holistic pedes-
trian image and then assigns an independent label to each
region. Then VPM learns to directly predict which regions
are visible on a partial pedestrian image, without comparing
it against the holistic one.

Usage of the self-supervision. Both [3] and [19] trans-
fer the model trained through self-supervision to the object
detection or classiﬁcation task. In comparison, VPM uti-
lizes self-supervision in a more explicit manner: with the
visibility awareness gained from self-supervision, VPM de-
cides which regions to focus when comparing two images.

3. Proposed Method

3.1. Structure of VPM

VPM is designed as a fully convolutional network, as
illustrated in Fig. 2. It takes a pedestrian image as the input
and outputs a constant number of region-level features, as
well as a set of visibility scores indicating which regions
are visible on the input image.

We ﬁrst deﬁne p = m × n densely aligned rectangle
regions on the holistic pedestrian image through uniform
division. Given a partial pedestrian image, we resize it to a
ﬁxed size, i.e., H × W and input it into VPM. Through a
stack of convolutional layers (“conv” in Fig. 2, which uses
all the convolutional layers in ResNet-50 [6]), VPM trans-
fers the input image into a 3D tensor T . The size of T is
c × h × w (which are the number of channels, height and
width, respectively), and we view the c − dim vector g as
a pixel on T . On tensor T , VPM appends a region locator
and a region feature extractor. The region locator discov-
ers regions on tensor T . Then the region feature extractor
generates a respective feature for each region.

A region locator perceives which regions are visible and
predicts their locations on tensor T . To this end, the region
locator employs a 1 × 1 convolutional layer and a following
Softmax function to classify each pixel g on T into the pre-
deﬁned regions, which in formulated by,

P (Ri|g) = sof tmax(W T g) =

exp W T
p

i g

exp W T

j g

P

j=1

,

(1)

where P (Ri|g) is the predicted probability of g belonging
to Ri, W is the learnable weight matrix of the 1 × 1 convo-

395

lutional layer, p is the total number of pre-deﬁned regions.
By sliding over every pixel g on T , the region loca-
tor predicts g as belonging to all the pre-deﬁned regions
with corresponding probabilities, and thus gets p probabil-
ity maps (one h × w map for each region), as shown in Fig.
2. Each probability map indicates the location of a corre-
sponding region on T , which allows region feature extrac-
tion.

The region locator also predicts the visibility score C for
each region, by accumulating P (Ri|g) over all the g on T ,
which is formulated by,

Ci = X

P (Ri|g),

f ∈T

(2)

Eq. 2 is natural in that if considerable pixels on T be-
longs to Ri (with large probability), it indicates that Ri is
visible on the input image and is assigned with a relatively
large Ci. In contrast, if a region is actually invisible, the re-
gion locator will still return a probability map (with all the
values approximating 0). In this case, Ci will be very small,
indicating possibly-invisible region. The visibility score is
important for calculating the distance between two images,
as to be detailed in Section 3.2.

A region feature extractor generates a respective fea-
ture f for a region by weighted pooling, which is formulated
by,

P (Ri|g)g

P

g∈T

Ci

fi =

, ∀i ∈ {1, 2, · · · , p},

(3)

where the division of Ci is to maintain the norm invariance
against the size of the region.

The region locator returns a probability map for each re-
gion, even if the region is actually invisible on the input
image. Correspondingly, we can see from Eq. 3 that the re-
gion feature extractor always generates a constant number
(i.e., p) of region features for any input image.

3.2. Employing VPM

Given two images to be compared, i.e., I k and I l, VPM
extracts their region features and predicts the region vis-
2, respectively.
ibility scores through Eq.
i , C k
With region features and region visibility scores {f k
i }
, {f l
i}, VPM ﬁrst calculates region-to-region euclidean
distances Dkl
i k(i = 1, 2, · · · , p). Then VPM
concludes the overall distance from the local distances by,

i = kf k

3 and Eq.

i − f l

i , C l

Dkl =

p

P

i=1

C k

i C l

iDkl
i

p

P

i=1

C k

i C l
i

.

(4)

In Eq. 4, the visible regions are with relative large vis-
ibility scores. The local distances between shared regions

are highly credited by VPM and thus dominate the overall
distance Dkl. In contrast, if a region is invisible in any one
of the compared images, its region feature is considered as
unreliable and the corresponding local distance contributes
little to Dkl.

Employing VPM adds very light computational cost,
compared with popular part-based deep learning methods
[23, 29, 12]. While some prior partial re-ID methods re-
quire pairwise comparison before feature extraction and
may have efﬁciency problems, VPM presents high scalabil-
ity, which allows experiments on large re-ID datasets such
as Market-1501 [31] and DukeMTMC-reID [35], as to be
accessed in Section 4.2.

3.3. Training VPM

Training VPM consists of training the region locator and
training the region feature extractor. The region locator and
the region feature extractor share the convolutional layers
before tensor T , and are trained end to end in a multi-task
training manner. Training VPM is also featured for employ-
ing auxiliary self-supervision.

Self-supervision is critical to VPM. It supervises VPM
to learn region visibility awareness, as well as to focus on
visible regions during feature learning. Speciﬁcally, given a
holistic pedestrian image, we randomly crop a patch and re-
size it to H × W . The random crop operation excludes sev-
eral pre-deﬁned regions and the remaining regions are re-
shaped during the resizing. Then, we project the regions on
the input image to tensor T through ROI projection [11, 20].
To be concrete, let us assume a region with its up-left cor-
ner located at (x1, y1) and its bottom-right corner located
at (x2, y2) on the input image. Then the ROI projection
deﬁnes a corresponding region on tensor T with its up-left
corner located at ([x1/S] , [y1/S]) and its right-bottom cor-
ner located at ([x2/S] , [y2/S]), in which the [•] denotes the
rounding and S is the down-sampling rate from input image
to T . Finally, we assign every pixel g on T with a region
label L(L ∈ 1, 2, · · · , p) to indicate which region g belongs
to. We also record all the visible regions in a set V . As we
will see, self-supervision contributes to training VPM in the
following three aspects:

• First, self-supervision generates the ground truth of re-

gion labels for training the region locator.

• Second, self-supervision enables VPM to focus on vis-
ible regions when learning feature through classiﬁca-
tion loss (cross-entropy loss).

• Finally, self-supervision enables VPM to focus on the
shared regions when learning features through triplet
loss.

Without the auxiliary self-supervision, VPM encounters

396

Figure 3. VPM learns region-level features with auxiliary self-
supervision. Only features corresponding to visible regions con-
tribute to the cross-entropy loss. Only features corresponding to
shared regions contribute to the deducing of triplet loss.

dramatic performance decrease, as to be accessed in Section
4.4.

The region locator is trained through cross-entropy loss
with the self-supervision signal L as the ground truth, which
is formulated by,

LR = − X

✶i=Llog(P (Ri|g)),

(5)

g∈T

where ✶Ri=L returns 1 only when i equals the ground truth
region label L and returns 0 in any other cases.

The region feature extractor is trained with the combi-
nation of cross-entropy loss and triplet loss, as illustrated in
3. Recall that the region feature extractor always generates
p region features for any input image. It leads to a nontrivial
problem during feature learning: only features of visible re-
gions should be allowed to contribute to the training losses.
With self-supervision signals V , we dynamically select the
visible regions for feature learning.

The cross-entropy loss is commonly used in learning fea-
tures for pedestrian under the IDE [30] mode. We append
a respective identity classiﬁer i.e., IPi(fi)(i = 1, 2, · · · , p)
upon each region feature fi, to predict the identity of train-
ing images. The identity classiﬁer consists of two sequen-
tial fully-connected layers and a Softmax function. The ﬁrst
fully-connected layers reduces the dimension of the input
region feature, and the second one transforms the feature di-
mension to K (K is the total identities of training images).
Then the cross-entropy loss is formulated by,

LID = − X

i∈V

✶k=ylog(sof tmax(IPi(fi))),

(6)

where k is the predicted identity and y is the ground truth
label. With Eq. 6, self-supervision enforces focus on visible
regions for learning region features through cross-entropy
loss.

The triplet loss pushes the features from a same pedes-
trian close to each other and pulls the features from differ-
ent pedestrians far away. Given a triplet of images, i.e., an

anchor image I a, a positive image I p and a negative image
I n, we deﬁne a region-selective triplet loss derived from the
canonical one by,

Ltri = [Dap − Dan + α]+ ,
i − f p
i k

kf a

P

i∈(V a∩V p)

Dap =

Dan =

|V a ∩ V p|
kf a
P

i − f n
i k

i∈(V a∩V n)

|V a ∩ V n|

,

,

(7)

i , f p

i and f n

where f a
i are the region features from anchor
image, positive image and negative image, respectively. V a,
V p and V n are the visible region sets for anchor image, pos-
itive image and negative image, respectively. |•| denotes the
operation of counting the elements of a set, i.e., the number
of shared regions in the two compared images. α is the mar-
gin for training triplet, and is set to 1 in our implementation.
With Eq. 7, self-supervision enforces focus on the shared

regions when calculating the distances of two images.

The overall training loss is the sum of region prediction
loss, the identity classiﬁcation loss and the region-selective
triplet loss, which is formulated by,

L = LR + LID + Ltri

(8)

We also note that Eq. 4 and Eq. 7 share a similar pattern.
Training with the modiﬁed triplet loss (Eq. 7) mimics the
matching strategy (Eq. 4) and is thus specially beneﬁcial
(to be detailed in Table 3). The difference is that, during
training, the focus is enforced through “hard” visibility la-
bels, while during testing, the focus is regularized through
predicted “soft” visibility scores.

4. Experiment

4.1. Settings

to evaluate our method.

Datasets. We use four datasets,

i.e., Market-1501
[31], DukeMTMC-reID [21, 35], Partial-REID and Partial-
iLIDS,
Market-1501 and
DukeMTMC-reID are two large scale holistic re-ID dataset.
The Market-1501 dataset contains 1,501 identities ob-
served from 6 camera viewpoints, 19,732 gallery images
and 12,936 training images detected by DPM [4]. The
DukeMTMC-reID dataset contains 1,404 identities, 16,522
training images, 2,228 queries, and 17,661 gallery im-
ages. We crop certain patches from the query images dur-
ing testing stage to imitate the partial re-ID scenario and
get a comprehensive evaluation of our method on large-
scale (synthetic) partial re-ID datasets. We note that few
prior works on partial re-ID evaluated their methods on
large-scale dataset, mainly because of low computing ef-
ﬁciency. Partial-REID [34] and Partial-iLIDS [33] are

397

Dataset

γ

Market-1501

0.5
0.6
0.7
0.8
0.9
1.0
0.5
0.6
DukeMTMC-reID 0.7
0.8
0.9
1.0

baseline

PCB

VPM

R-1
64.5
79.0
83.9
85.7
87.1
86.8
65.0
76.2
76.3
76.3
77.0
76.2

R-5
82.2
91.4
93.9
94.3
95.5
95.3
81.1
87.3
87.3
88.3
88.1
87.3

R-10 mAP
44.4
88.1
57.9
94.3
95.9
63.7
66.1
96.4
67.8
97.4
67.7
97.4
47.2
86.7
90.4
55.4
90.6
90.6
58.8
91.9
59.0
91.7
91.2
58.6

R-1
0.9
8.1
36.5
71.9
88.8
93.4
5.0
13.1
35.9
64.0
81.6
84.1

R-5
3.2
16.5
58.9
87.3
95.8
97.8
10.1
25.6
57.0
82.6
90.4
92.4

R-10 mAP
1.7
5.6
6.6
23.2
67.4
26.8
56.8
91.4
77.2
97.1
83.0
98.4
4.0
13.6
33.5
10.5
28.4
65.4
52.3
87.7
70.3
93.0
94.5
73.2

R-1
70.9
84.4
88.2
90.1
91.7
93.0
69.5
78.2
80.3
80.3
81.7
83.6

R-5
86.5
94.3
95.8
95.8
96.6
97.8
83.1
89.0
89.5
89.3
90.9
91.7

R-10 mAP
48.8
92.1
62.5
96.1
97.2
71.7
74.7
97.7
78.7
98.0
80.8
98.8
52.2
87.9
91.3
60.9
63.1
92.0
63.5
92.4
70.7
93.1
94.2
72.6

Table 1. Comparison between VPM, baseline and PCB. For VPM, we use p = 6 × 1 pre-deﬁned regions. For PCB, we adopt the code
released by the authors and append an extra triplet loss, for fair comparison with VPM. On Market-1501, the extra triplet loss enables PCB
to gain +5.6% mAP over the original 77.4% reported by the authors [23].

two commonly-used datasets for partial re-ID. Partial-REID
contains 600 images and 60 identities, every one of which
has 5 holistic images and 5 partial images. Partial-iLIDS
is derived from iLIDS [24], which is collected in an airport
and the lower-body of a pedestrian is frequently occluded
by the luggage. Partial-iLIDS crops the non-occluded re-
gion from these images and get 238 images from 119 identi-
ties. Both Partial-REID and Partial-iLIDS offer only testing
images. When evaluating our method on these two public
datasets, we train VPM on the training set of Market-1501,
for fair comparison with other competitive methods, includ-
ing MTRC [15], AMC+SWM [34], DSR [7], and SFR [8].
Implementation Details. Training VPM relies on the
assumption that the original training images all contain
holistic pedestrian and the pedestrian are tightly bounded
by bounding boxes. On two holistic re-ID datasets, Market-
1501 and DukeMTMC-reID, there do exist some images
which contain either partial pedestrian or oversized bound-
ing boxes. We consider these images as tolerable noises.

To generate the partial image for training VPM, we crop
a patch from the holistic image with random area ratio γ.
We set γ to be uniformly distributed between 0.5 and 1.
VPM is not necessarily bounded with any speciﬁed crop
strategy and we may consider the prior knowledge for op-
timization. We argue that choosing the detailed crop strat-
egy according to the realistic condition is reasonable be-
cause partial re-ID is a realistic challenge, and the occlusion
fashion is usually predictable. We also experimentally val-
idate that choosing an appropriate crop strategy to imitate
the confronted partial re-ID condition beneﬁts the retrieval
accuracy, as to be detailed in Section 4.3. That being said,
VPM is still general in that it may adopt any crop strategy
to conduct self-supervision.

VPM is trained with the combination of cross-entropy

loss and triplet loss. We use the standard Stochastic Gra-
dient Descent (SGD) optimization strategy, initialize the
learning rate as 0.1 and decay it to 0.01 after 30 epochs. We
construct each mini-batch with 64 images from 8 identities
(8 images per identity) and use the hard mining strategy [9]
for deducing the triplet loss.

4.2. Evaluation on large scale partial re ID datasets

We evaluate the effectiveness of VPM with experiment
on the synthetic partial datasets derived from two large-
scale re-ID datasets, Market-1501 and DukeMTMC-reID.
We differ the ratio γ of the cropped patches from 0.5 to 1.0
during testing. For comparison with VPM, we implement a
baseline which learns global feature through the combina-
tion of cross-entropy loss and triplet loss. We also imple-
ment a part-based feature learning method PCB [23]. For
fair comparison, we enhance PCB with an extra triplet loss
during training, and achieve slightly higher performance
than [23]. The results are summarized in Table 1.

VPM signiﬁcantly improves partial re-ID perfor-
mance over the baseline. On Market-1501, VPM sur-
passes the baseline by +6.4%, +5.4%, +4.3%, +4.4%,
+4.6%, +6.2% rank-1 accuracy and +4.4%, +4.6%, +8.0%,
+ 8.6%, +10.9%, +13.1% mAP when γ is set from 0.5 to 1,
respectively. The superiority of VPM against the baseline,
which learns a global feature representation, is derived from
two-fold beneﬁt. On the one hand, VPM learns region-level
features and beneﬁts from ﬁne-grained information. On
the other hand, with visibility awareness, VPM conducts a
region-level alignment and eliminates the distracting noises
originated from unshared regions.

VPM increases the robustness of part features un-
der partial re-ID scenario. Comparing VPM with PCB,
a state-of-the-art part feature learning method for holistic

398

baseline

p=2

p=3

p=4

p=6

p=8

1
-
k
n
a
R

93

88

83

78

73

68

63

0.5

0.6

0.7

0.8

0.9

1

γ

Figure 4. Impact of p on the partial-reID accuracy. We set p to
2,3,4,6 and 8, respectively. We use Market-1501 for training and
differ the crop ratio γ during testing.

person re-ID task, we observe that as γ decreases, the re-
trieval accuracy achieved by PCB dramatically drops (e.g.,
0.9% rank-1 accuracy at γ = 0.5 ), implying that PCB
is extremely vulnerable to the spatial misalignment in par-
tial re-ID. By contrast, the retrieval accuracy achieved by
VPM decreases much slower as γ decreases. We infer that
VPM facilitates region-to-region comparison within shared
regions of two images and thus gains strong robustness.

We also notice that under γ = 1.0, i.e., the holistic per-
son re-ID scenario, VPM achieves comparable retrieval ac-
curacy with PCB.

In Table 1, we use 6 pre-deﬁned parts to construct VPM.
Moreover, we analyze the impact of the part numbers p on
Market-1501, with results shown in Fig. 4. Under all set-
tings of p and γ, VPM consistently surpasses the baseline,
which further conﬁrms the superiority of VPM. We also ob-
serve that larger p generally brings higher (rank-1) retrieval
accuracy. Larger p allows VPM to learn the region-level
features in ﬁner granularity and thus beneﬁts the discrim-
inative ability, which is consistent with the observation in
holistic person re-ID works [29, 23]. Larger p also allows
more accurate region alignment when comparing a partial
person image against a holistic one. We suggest choosing p
with the joint consideration of retrieval accuracy and com-
puting efﬁciency, and set p = 6 in most of our experiments
(if not specially mentioned).

4.3. Comparison with the state of the art

We compare VPM with the state-of-the-art methods on
two public datasets, i.e., Partial-REID and Partial-iLIDS.
We train three different versions of VPM with different crop
strategies for preparing training patches, i.e., top crop (the
top regions are always visible), bottom crop (the bottom re-
gions are always visible) and bilateral crop (top crop + bot-
tom crop). The results are presented in Table 2, from which

Methods

Partial-REID Partial-iLIDS
R-1
MTRC [15]
23.7
AMC+SWM [34] 37.3
DSR [7]
50.7
56.9
SFR [8]
53.2
VPM (Bottom)
64.3
VPM (Top)
67.7
VPM (Bilateral)

R-3
27.3
46.0
70.0
78.5
73.2
83.6
81.9

R-3
26.1
32.8
67.2
74.8
62.3
76.5
74.8

R-1
17.7
21.0
58.8
63.9
53.6
67.2
65.5

Table 2. Evaluation of VPM on Partial-REID and Partial-iLIDS.
Three VPMs trained with different crop strategies are evaluated.

two observations are drawn.

First, comparing three editions of VPM against each
other, we ﬁnd that the crop strategy matters to VPM. On
Partial-iLIDS, all query images of which are cropped from
the top side of holistic pedestrian images, VPM (Top)
achieves the highest retrieval accuracy. On Partial-REID,
which contains images cropped from different directions,
VPM (Bilateral) achieves the highest retrieval accuracy.
VPM (Bottom) always performs the worst due to two rea-
sons. First, retaining the bottom regions severely deviates
from the testing condition. Second, the bottom regions
(mainly containing legs) inherently offers relatively weak
discriminative clues. We note that when solving the prob-
lem of partial-reID, the realistic partial condition is usually
estimable. We recommend analyzing the partial condition
and choosing a similar crop strategy for training VPM. That
being said, VPM is general in that it is able to cooperate
with various crop strategies.

Second, given appropriate crop strategies, VPM achieves
very competitive performance compared with the state of
the art. On Partial-REID, VPM (Bilateral) surpasses the
strongest competing method SFR by +10.6% Rank-1 ac-
curacy. On Partial-iLIDS, VPM (Top) surpasses SFR by
+3.3% Rank-1 accuracy. Even with no prior knowledge of
partial condition on testing set, we may eclectically choose
VPM (Bilateral), which considers both top and down occlu-
sions and thus maintains stronger robustness.

4.4. The importance of self supervision

We conduct ablation study to analyze the impact of self-
supervision on VPM. We train four Malfunctioned VPM for
comparison:

• MVPM-1 is trained as a normal VPM, but abandons
the visibility awareness during testing, i.e., MVPM-
1 concludes the overall distance with all region-level
features, even if some regions are invisible.

• MVPM-2 abandons self-supervision on triplet loss
during training, i.e., all region features equally con-
tribute to deducing the triplet loss Ltri.

399

Partial-iLIDS

Market-1501

Methods

R-1 R-3 R-5 R-1 R-5 mAP
VPM.
67.2 76.5 82.4 93.0 97.8 80.8
VPM (no triplet) 57.1 73.9 79.0 91.3 97.0 77.8
MVPM-1
63.0 74.8 82.4 93.0 96.3 79.7
MVPM-2
61.3 73.1 79.0 92.8 97.4 80.1
58.8 74.8 82.4 91.4 96.5 75.5
MVPM-3
59.7 74.8 78.2 90.4 96.6 75.7
MVPM-4

Table 3. Ablation study on VPM. “VPM (no triplet)” is trained
with no triplet loss. On Market-1501, we only analyze the holistic
person re-ID mode.

• MVPM-3 abandons self-supervision on identiﬁcation
loss LID during training, i.e., all region features are
supervised by the training identity label through LID.

• MVPM-4 abandons self-supervision on both triplet

loss and identiﬁcation loss.

Figure 5. Region visualization. We train VPM with 3 × 2 pre-
deﬁned regions. For each image, VPM discovers 6 regions with
6 probability maps, as detailed in Section 3.1. For better visual-
ization, we assign each pixel to its closest region and achieve the
partitioning effect. Images on the ﬁrst and the second row are from
(synthetic) Market-1501 and Partial-REID, respectively.

Moreover, we also analyze the impact of the modiﬁed
triplet loss with dynamic region selection (Eq. 7) in training
VPM. The results are summarized in Table 3, from which
we draw three observations.

First, comparing “VPM (no triplet)” with “VPM”, we
ﬁnd the modiﬁed triplet loss with dynamic region selec-
tion is vital for VPM, especially under partial re-ID sce-
nario. Without triplet loss, the rank-1 accuracy slightly
decreases by -1.7% on (holistic) Market-1501, while on
Partial-iLIDS, the rank-1 accuracy dramatically decreases
by -10.1%. It is because training with the modiﬁed triplet
loss mimics the matching strategy (Eq. 4) for partial re-ID
and is thus especially important.

Second, comparing “MVPM-1” with “VPM”, we ob-
serve a dramatic performance decrease on Partial-iLIDS.
Both models are trained in exactly the same procedure. The
difference is that “MPVM-1” employs all the region fea-
tures to conclude the overall distance, while VPM focuses
on the shared regions between two images. On Market-
1501, all the regions are visible and two models achieves
very close retrieval accuracy. We thus infer that the visibil-
ity awareness is critical for VPM under partial re-ID sce-
nario.

Third, comparing last three editions of MVPM with
“VPM” as well as “MVPM-1”, we observe further per-
formance decreases on Partial-iLIDS. The last three edi-
tions abandon self-supervision to regularize the learning of
region-level features (either on the cross-entropy loss or
triplet loss or both). Learning features from invisible re-
gions brings about larger sample noises. Consequentially,
the learned region features are signiﬁcantly compromised.
We thus conclude that enforcing VPM to focus on visible
regions through self-supervision is critical for learning re-
gion features.

4.5. Visualization of discovered regions

We visualize the regions discovered by VPM (the re-
gion locator, in particular) in Fig. 5. We use p = 3 × 2
pre-deﬁned regions to facilitate both horizontal and verti-
cal visibility awareness. It is observed that VPM conducts
adaptive partition with visibility awareness. Given holis-
tic images (the ﬁrst column), VPM successfully discovers
all the 3 × 2 regions. Given partial pedestrian images with
horizontal occlusion (the second column), VPM favors the
dominating regions (left regions in Fig. 5). Given partial
pedestrian images with lower-body occluded (the last two
columns), VPM roughly discovers 4 visible regions, and
perceives that the bottom 2 regions are invisible. These ob-
servations conﬁrm that VPM gains robust region-level visi-
bility awareness and is capable to locate the visible regions
through self-supervised learning.

5. Conclusion

In this paper, we propose a region-based feature learn-
ing method, VPM for partial re-ID task. Given a set of
pre-deﬁned regions on the holistic pedestrian image, VPM
learns to perceive which regions are visible on a partial im-
age through self-supervision. VPM locates each region on
the convolutional feature maps and then extracts region-
level features. With visibility awareness, VPM compares
two pedestrian images with focus on their shared regions
and correspondingly suppresses the severe spatial misalign-
ment in partial re-ID. Experimental results conﬁrm that
VPM surpasses both the global feature learning baseline
and part-based convolutional methods, and the achieved
performance is on par with the state of the art.

400

References

[1] Zhe Cao, Tomas Simon, Shih En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In CVPR, 2017. 2

[2] L. C. Chen, G Papandreou, I Kokkinos, K Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected
crfs.
IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834–
848, 2018. 2

[3] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual
representation learning by context prediction. In IEEE Inter-
national Conference on Computer Vision, pages 1422–1430,
2015. 2, 3

[4] Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan. A discriminatively trained, multiscale, deformable
part model. In CVPR, 2008. 5

[5] Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen,
and Liang Lin. Look into person: Self-supervised structure-
sensitive learning and a new benchmark for human parsing.
In 2017 IEEE Conference on Computer Vision and Pattern
Recognition, 2017. 2

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 3

[7] Lingxiao He,

Jian Liang, Haiqing Li,

and Zhenan
Sun. Deep spatial feature reconstruction for partial per-
son re-identiﬁcation: Alignment-free approach. CoRR,
abs/1801.00881, 2018. 1, 6, 7

[8] Lingxiao He, Zhenan Sun, Yuhao Zhu,

and Yunbo
Wang. Recognizing partial biometric patterns. CoRR,
abs/1810.07399, 2018. 6, 7

[9] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-
fense of the triplet loss for person re-identiﬁcation. arXiv
preprint arXiv: 1703.07737, 2017. 6

[10] Eldar

Insafutdinov, Leonid Pishchulin, Bjoern Andres,
Mykhaylo Andriluka, and Bernt Schiele. Deepercut: A
deeper, stronger, and faster multi-person pose estimation
model. In ECCV, 2016. 2

[11] He Kaiming, Zhang Xiangyu, Ren Shaoqing, and Jian Sun.
Spatial pyramid pooling in deep convolutional networks for
visual recognition.
In European Conference on Computer
Vision, 2014. 4

[12] Mahdi M. Kalayeh, Emrah Basaran, Muhittin Gokmen,
Mustafa E. Kamasak, and Mubarak Shah. Human seman-
tic parsing for person re-identiﬁcation. In CVPR, 2018. 2,
4

[13] Gustav

Larsson, Michael Maire,

and Gregory
Learning representations for automatic
Shakhnarovich.
colorization. In Computer Vision - ECCV 2016 - 14th Eu-
ropean Conference, Amsterdam, The Netherlands, October
11-14, 2016, Proceedings, Part IV, pages 577–593, 2016. 2

[14] Quoc V. Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu
Devin, Greg Corrado, Kai Chen, Jeffrey Dean, and An-
drew Y. Ng. Building high-level features using large scale
unsupervised learning. In ICML, 2012. 2

[15] Shengcai Liao, Anil K. Jain, and Stan Z. Li. Partial face
recognition: Alignment-free approach. IEEE Trans. Pattern
Anal. Mach. Intell., 35(5):1193–1205, 2013. 6, 7

[16] Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao,
Shuai Yi, Junjie Yan, and Xiaogang Wang. Hydraplus-net:
Attentive deep features for pedestrian analysis.
In ICCV,
2017. 2

[17] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015. 2

[18] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In ECCV, 2016.
2

[19] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of
visual representations by solving jigsaw puzzles.
In IEEE
International Conference on Computer Vision, pages 69–84,
2016. 2, 3

[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: towards real-time object detection with region
proposal networks. IEEE Transactions on Pattern Analysis
and Machine Intelligence, pages 1137–1149, 2017. 4

[21] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,
and Carlo Tomasi. Performance measures and a data set for
multi-target, multi-camera tracking.
In European Confer-
ence on Computer Vision workshop on Benchmarking Multi-
Target Tracking, 2016. 5

[22] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao,
and Qi Tian. Pose-driven deep convolutional model for per-
son re-identiﬁcation. In ICCV, 2017. 2

[23] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin
Wang. Beyond part models: Person retrieval with reﬁned
part pooling. In ECCV, 2018. 2, 4, 6, 7

[24] Taiqing Wang, Shaogang Gong, Xiatian Zhu, and Shengjin
Wang. Person re-identiﬁcation by video ranking. In Com-
puter Vision - ECCV 2014 - 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part IV, 2014. 6

[25] Xiaolong Wang, Kaiming He, and Abhinav Gupta. Tran-
sitive invariance for self-supervised visual representation
learning. In IEEE International Conference on Computer Vi-
sion, ICCV 2017, Venice, Italy, October 22-29, 2017, 2017.
2

[26] Longhui Wei, Shiliang Zhang, Hantao Yao, Wen Gao, and Qi
Tian. GLAD: Global-local-alignment descriptor for pedes-
trian retrieval. ACM Multimedia, 2017. 2

[27] Shih En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser

Sheikh. Convolutional pose machines. In CVPR, 2016. 2

[28] Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li,
and Qi Tian. Deep representation learning with part loss for
person re-identiﬁcation. arXiv preprint arXiv:1707.00798,
2017. 2

[29] Liming Zhao, Xi Li, Jingdong Wang, and Yueting Zhuang.
Deeply-learned part-aligned representations for person re-
identiﬁcation. In ICCV, 2017. 2, 4, 7

[30] Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su,
Shengjin Wang, and Qi Tian. Mars: A video benchmark for
large-scale person re-identiﬁcation. In ECCV, 2016. 5

401

[31] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiﬁcation:
A benchmark. In ICCV, 2015. 4, 5

[32] Liang Zheng, Yi Yang, and Alexander G. Hauptmann. Per-
son re-identiﬁcation: Past, present and future. arXiv preprint
arXiv:1610.02984, 2016. 2

[33] Wei-Shi Zheng, Shaogang Gong, and Tao Xiang. Person re-
identiﬁcation by probabilistic relative distance comparison.
In The 24th IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2011, Colorado Springs, CO, USA,
20-25 June 2011, 2011. 1, 5

[34] Wei-Shi Zheng, Xiang Li, Tao Xiang, Shengcai Liao,
Jian-Huang Lai, and Shaogang Gong. Partial person re-
identiﬁcation.
In 2015 IEEE International Conference on
Computer Vision, ICCV 2015, Santiago, Chile, December 7-
13, 2015, 2015. 1, 5, 6, 7

[35] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identiﬁcation
baseline in vitro. In ICCV, 2017. 4, 5

[36] Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai, and Guangcong
Wang. Occluded person re-identiﬁcation. In 2018IEEE In-
ternational Conference of Multimedia and Expo, 2018. 1

402

