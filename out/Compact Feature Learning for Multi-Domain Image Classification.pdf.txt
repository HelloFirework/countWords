Compact Feature Learning for Multi-domain Image Classiﬁcation

Yajing Liu1, Xinmei Tian1(B), Ya Li2, Zhiwei Xiong1, Feng Wu1
1University of Science and Technology of China, Hefei, China

2iFLYTEK Research, Hefei, China

lyj123@mail.ustc.edu.cn, xinmei@ustc.edu.cn,

yali8@iflytek.com, zwxiong@ustc.edu.cn, fengwu@ustc.edu.cn

Abstract

The goal of multi-domain learning is to improve the per-
formance over multiple domains by making full use of all
training data from them. However, variations of feature dis-
tributions across different domains result in a non-trivial
solution of multi-domain learning. The state-of-the-art
work regarding multi-domain classiﬁcation aims to extract
domain-invariant features and domain-speciﬁc features in-
dependently. However, they view the distributions of fea-
tures from different classes as a general distribution and try
to match these distributions across domains, which lead to
the mixture of features from different classes across domains
and degrade the performance of classiﬁcation. Addition-
ally, existing works only force the shared features among
domains to be orthogonal to the features in the domain-
speciﬁc network. However, redundant features between the
domain-speciﬁc networks still remain, which may shrink the
discriminative ability of domain-speciﬁc features. There-
fore, we propose an end-to-end network to obtain the more
optimal features, which we call compact features. We pro-
pose to extract the domain-invariant features by matching
the joint distributions of different domains, which have dis-
tinct boundaries between different classes. Moreover, we
add an orthogonal constraint between the private features
across domains to ensure the discriminative ability of the
domain-speciﬁc space. The proposed method is validated
on three landmark datasets, and the results demonstrate the
effectiveness of our method.

1. Introduction

Image classiﬁcation is one of the fundemental branches
of computer vision tasks and has achieved impressive suc-
cess with the advances of deep learning [11, 13]. How-
ever, due to the various exteral factors, such as viewpoint
changes and background noise, in the real world, a classiﬁer
trained on one domain is likely to perform poorly on another
domain. What’s more, labeling enough samples in each

Figure 1. The two curves in (a) represent two classiﬁers in two
domains. Features in each domain can be well-separated by its
own classiﬁer. However, when features from different domains
are put together, the features from different classes across domains
are mixed up. Therefore, we apply a shared classiﬁer to match the
conditional distributions P (Y |Fs(X)) across domains. As shown
in (b), features from different classes are well separated.

domain is time consuming. To address these drawbacks,
multi-domain learning aims to make full use of the training
data to simulataneously improve the general classiﬁcation
performance over all domains [15]. Previous works regard-
ing multi-domain learning trains classiﬁers in a collabrative
way based on multi-task learning [23]. These approaches
decompose the classiﬁer in each domain into a shared part
and a domain-speciﬁc part. The sharing knowledge used in
the network signiﬁcantly improves the classiﬁcation perfor-
mance over all domains. Unfortunately, these networks do
not consider the matching of distributions across domains.
Consequently, the shared knowledge cannot realize its full
potential to improve the general classiﬁcation performance
in related domains.

Recent works [4, 17, 3] use a network with the adver-
sarial loss to overcome the aforementioned drawbacks. Let
X, Fs(X) and Y denote the images, shared features after
transformation and category labels, respectively. Adversar-
ial training is applied to match the marginal distributions of
shared features Fs(X). Under the assumption that the cor-
relations P (Y |Fs(X)) between the marginal distributions

17193

Figure 2. Redundant common features probably arise in the private
space in previous works as shown in (a). The redundancy exists in
the private network, then the private network does not make full
use of the parameters and learning spaces. This can decrease the
discriminability of the learned features, and thus increases the dif-
ﬁculty of accurate classiﬁcation. In (b), we apply the orthogonal
restriction between private features across domains in order to pre-
vent the redundant features between domain-speciﬁc spaces.

P (Fs(X)) and P (Y ) keep stable across domains, early ap-
proaches deem the distributions of features in the same class
are matched. Unfortunately, the assumption does not al-
ways hold in practice. As shown in Figure 1(a), although
the features in each domain can be well-separated by its own
classiﬁer, when they are put together only with the match-
ing of marginal distributions Fs(X), features from different
classes across domains can be mixed up. To separate sam-
ples from different classes, we apply a shared classiﬁer that
matches the conditional distributions P (Y |Fs(X)) of the
features. In this way, joint distributions P (Fs(X), Y ) of
different domains are matched, and samples from different
classes are well separated as shown in Figure 1(b).

Another problem that arises in existing works [4, 17, 3]
is that they make assumptions that domain-speciﬁc features
can be learned automatically just utilizing the orthogonal
constraint between the domain-speciﬁc space and the shared
space. However, the orthogonal property between domain-
speciﬁc spaces cannot be guaranteed, which results in the
redundant features between domain-speciﬁc spaces. As
shown in Figure 2(a), if the redundancy exists in the private
network, the private network does not make full use of the
parameters and learning spaces. This can decrease the dis-
criminability of the learned features, and thus increases the
difﬁculty of accurate classiﬁcation. Furthermore, the shared
space is not well learned for the reason that the shared fea-
tures occur in the private space instead. Considering these
drawbacks, as in Figure 2(b),we apply an orthogonal reg-
ularization between private features across domains to pre-
vent the redundant features between domain-speciﬁc spaces
and then ensure the uniqueness of each private space.

In this paper, we propose a compact feature learning
method that guarantees the uniqueness of each domain-
speciﬁc space and prevents the mixture of samples from dif-
ferent classes across domains during domain-invariant fea-

ture learning. The contributions of our work can be summa-
rized as follows:

• The proposed network realizes compact feature learn-
ing, which extracts independent domain-speciﬁc fea-
tures and the domain-invariant features with distinct
class boundaries.

• We train an adversarial network with a shared classi-
ﬁer across domains, in which the joint distributions
P (Fs(X), Y ) can be automatically matched. Then the
features in different classes across domains can be well
separated.

• We propose an orthogonal regularization between pri-
vate spaces to ensure the uniqueness of the private
space and eradicate the redundant information in the
network.

We conduct extensive experiments on several multi-
domain datasets and the results demonstrate the effective-
ness of our approach.

2. Related Work

Multi-domain learning aims to improve the classiﬁcation
performance in general domains by making full use of the
information in each domain. Previous works have proposed
to improve the performance of multi-domain learning using
multi-task learning.

Existing approaches in multi-task learning based on neu-
ral networks can be broadly categorized into two meth-
ods: the parameter sharing methods [19], which entail the
parameter restriction of hidden layers; the feature sharing
methods, in which task-invariant representations can au-
tomatically be learned through the architecture of the net-
work. In parameter restriction studies, Caruna et al. [2] re-
alizes complete parameter sharing across tasks in lower lay-
ers, while task-speciﬁc output layers are maintained at the
end of each task. Additionally, Duong et al. [6] and Yang et
al. [24] respectively apply l2 regularizations and trace norm
regularizations between parameters to encourage the learn-
ing of common knowledge across tasks. However, the loca-
tion of the shared layers in parameter restriction approaches
is determined before training, which constrains the ﬂexi-
bility of the network [8]. For feature sharing approaches,
the cross-stitch network [18] aims to linearly combine the
outputs from the identical layers in each task-speciﬁc net-
work, while the cross-stitch units determine the inﬂuence
degree of the shared knowledge from each task. Moreover,
cross-connect network [8] uses 1×1 convolution layers to
connect the identical layers in each network for different
tasks, which increases the ﬂexibility of the network. How-
ever, all of these networks have no consider matching of
distributions of the task-invariant features across tasks, sub-
sequently degrading the performance of these works.

7194

Unlike above methods, recent works aim to extract
the shared knowledge through Bregman Divergence-Based
Regularization [21] and the adversarial training network,
which performs well in transfer learning and multi-domain
learning. Transfer learning generally aims to learn invariant
representations or parameters in different domains. How-
ever, multi-domain classiﬁcation aims to learn invariant rep-
resentations while preserving the private representations.
Liu et al.
[17] obtains the domain-invariant information
in multiple related domains through the adversarial train-
ing strategy while eliminating redundant features between
the private and shared spaces via orthogonal regularization.
Chen et al. [3] proposes to use the negative log-likelihood
loss and the l2 loss instead of the adversarial loss alone.
The combination of domain-invariant features and domain-
speciﬁc features signiﬁcantly improves the performance in
text classiﬁcation for multi-domain learning. In this paper,
we focus on the multi-domain learning for image classiﬁ-
cation problems and realize the compact feature learning,
consequently obtaining a remarkable result.

3. Approach

We ﬁrst introduce multi-domain learning and deﬁne the
notation used in this paper. Then we present details of the
proposed multi-domain learning approach.

3.1. Multi domain learning

Suppose the feature and the label spaces are represented
by X and Y respectively. A domain deﬁned on X × Y can
be represented by a joint probability distribution P (X, Y ).
For simplicity, Pm(X, Y ) denotes the joint distribution of
the datasets in the m-th domain and Pm(X) denotes the
marginal distribution of the datasets. Each dataset is asso-
ciated with a sample Dm = {xi, yi}Nm
i=1, where Nm is the
sample size of the m-th domain. Given C related domains
P1(X, Y ), P2(X, Y ), . . . , PC(X, Y ) and their correspond-
ing datasets Dm = {xi, yi}Nm
i=1, the goal of multi-domain
learning discussed in this paper is to learn a multi-branch
model f : Xm → Ym, m = {1, 2, . . . , C} to classify
all datasets correctly in parallel. Multi-domain learning
considers the distribution bias between different domains,
which might be caused by the camera’s viewpoint, back-
ground noise or image style, etc. And then it aims to learn
a general multi-branch model to improve the performance
over all domains simultaneously.

3.2. Proposed multi domain classiﬁcation model

As shown in Figure 3, our multi-domain classiﬁcation
model is composed of three components: the shared feature
learning network, the private feature learning network and
the classiﬁcation network for each domain.

To extract domain-invariant features through the match-
ing of joint distributions of features P (Fs(X), Y ) across

domains, the joint adversarial shared network is applied. It
is in the middle of the architecture with blue colors in Fig-
ure 3. Image features are extracted with several convolution
neural networks. Besides, the matching of feature distri-
butions is guaranteed by one image classiﬁcation network
and one discriminator simultaneously. The multi-player ad-
versarial discriminator is applied to ensure the match of
marginal distributions P (Fs(X)) of features [16]. Addi-
tionally, the shared classiﬁer matches the conditional dis-
tributions P (Y |Fs(X)) across domains. Consequently, the
joint distributions of shared features P (Y, Fs(X)) in differ-
ent domains are matched.

The private network aims to learn the domain-speciﬁc
information which preserves the characteristics of each do-
main.
It is shown at two sides of the shared network in
Figure 3. To obtain the domain-speciﬁc knowledge and fur-
ther promoting domain-invariant feature learning, we intro-
duce two types of orthogonal regularizations: the regular-
ization between private features and shared features in each
domain; the regularization between private features across
domains. Consequently, the independence of each feature
space can be guaranteed. At training and testing process, the
learned shared features Fs(x) and private features Fp(x)
are concatenated and fed into the classiﬁcation network for
each domain. We call the shared features and private fea-
tures as compact features Fcompact:

Fcompact = [Fs(x), Fp(x)]

(1)

The main learning goal in our multi-domain learning archi-
tecture can be formulated as:

Ldomain = −

C

X

m=1

1
Nm

Nm

X

i=1

log[Cm(ym

i |Fcompact)]

(2)

we denote the classiﬁer in each domain by Cm. The pro-
posed compact feature learning improves the classiﬁcation
performance of each domain.

3.3. Joint adversarial shared network

Previous methods are proposed to obtain domain-
invariant features through matching the marginal distribu-
tions of features P (Fs(X)). Each domain has its own clas-
siﬁer, which has no restriction of the matching of condi-
tional distributions P (Y |Fs(X)) across domains. With-
out considering their class labels in the adversarial training,
domain-invariant features in different classes are probably
mixed up. To address this drawback, our approach separates
shared features from different classes through applying an
adversarial training algorithm with a shared classiﬁer.

This shared classiﬁer and the domain adversarial net-
work cope with all domain-invariant features in the same
manner. The domain adversarial network is connected with
the feature exacting network through a gradient reversal
layer (GRL), which layer [9] forwards the input to the fol-
lowing layers but reverses the gradient during the backward

7195

Figure 3. The proposed multi-domain classiﬁcation model. The joint adversarial shared network is in the middle of the architecture with
blue colors. A domain discriminator and a shared classiﬁer are applied at the end of the shared feature extracting network to guarantee the
joint distribution matching of features. The private network is shown at two sides of the shared network. Feature orthogonal regularization
is applied between private features across domains, in addition to private features and shared features in each domain. Compared with [3],
we add the joint adversarial loss and the orthogonal regularizations between private features across domains.

propagation. We deﬁne the minimax game of joint adver-
sarial learning(jal) as follow:

improve its performance and the domain-invariant features
in joint distribution matching are obtained.

log[Cs(yi|(Fs(xi))]

• Proof of joint distribution matching

Ljal = min
Fs,Cs

max
d1,...,dc

(−

C

X

m=1

1
Nm

Nm

X

i=1

+

C

X

m=1

1
Nm

Nm

X

i=1

log[dm(Fs(xi))])

Under

(Y |X).

respectively.

the assumption that

(3)
where dm is the m-th domain discriminator and Fs, Cs
the shared feature extracting network and
represent
the shared classiﬁcation network,
Since
we constrain Cs to a simple linear transformation or
shallow network, Cs(Y |Fs(X)) can be simpliﬁed as
P i
the shared
Fs
classiﬁer predicts the samples from the same class in
different domains as the same accurate one-hot
label,
the network can perfectly match the conditional distribu-
tions across different domains.
It can be formulated as
P 1
(Y |X) = PFs (Y |X) . At the
Fs
same time, under the restriction of the negative gradients,
the shared feature extracting network expects that the ex-
acted features can mislead the domain classiﬁcation results,
but the discriminator tries its best to correctly classify
the domain category of features [10]. Consequently, the
domain discriminator results in the minimax game in the
network, which leads to the domain-invariant property
of the shared features. At the end of the training phase,
the network will reach a point that each network cannot

(Y |X) = . . . = P C
Fs

The cost function of the minimax game in the network

can be formulated as:

φ = min
Fs

max
d1,...,dc

C

X

m=1

1
Nm

Nm

X

i=1

log[dm(Fs(xi))],

s.t.

C

X

m=1

dm((Fs(x))) = 1

(4)

The marginal distribution and the joint distribution of
shared features after transformation in the i-th domain are
denoted by P i
Fs

(X), respectively.

(X) and P i
Fs

The optimal procedure in the network is consist of two
In Stage 1, the discriminator is unrelated to the
stages.
distribution of Y , then, we provide the optimal discrimina-
tor under a ﬁxed transformation Fs from [10, 16] directly.
In Stage 2, we obtain the optimal domain-invariant feature
transformation under the shared classiﬁer and the ﬁxed dis-
criminator in Stage 1.

Stage 1. Let x′, y = Fs(x). For a ﬁxed transforma-
, . . . , dC
Fs

tion Fs, the optimal prediction probabilities d1
Fs
of discriminator d are:

dm∗
Fs

(x′) =

7196

P m∗
(x′)
Fs
m=1 P m∗
Fs

PC

(x′)

(5)

Stage 2.When we obtain the optimal point of d, let

φ′ = min
Fs

= min
Fs

C

X

m=1

C

X

m=1

1
Nm

1
Nm

Nm

X

i=1

Nm

X

i=1

log[dm(Fs(xi))],

log[

(x′)
P m
Fs
m=1 P m
Fs

PC

(x′)

(6)

]

φ′ achieves the value −ClogC in the global mini-
mum, and the balanced point is achieved if and only if
P 1
Fs

(X, Y ) = · · · = P C
Fs

(X, Y )

Proof. Under the assumption that we can obtain the
optimal shared classiﬁer, we can arrive the optimal point
P 1
Then we
Fs
can obtain:

(Y |X) = · · · = P C
Fs

(Y |X) = PFs (Y |X).

φ′ =

=

C

X

m=1

C

X

m=1

C

1
Nm

1
Nm

Nm

X

i=1

Nm

X

i=1

Nm

1
Nm

=

X

X
i=1
= C · J SD(P 1
Fs

m=1

[log[

] + logC] − ClogC

PC

(x′)
P m
Fs
m=1 P m
Fs
(x′)PFs (y|x′)

(x′)

(x′)PFs (y|x′)

] − ClogC

log[

1

P m
Fs
C PC
m=1 P m
Fs
P m
Fs
C PC
m=1 P m
Fs
(X, Y ), . . . , P C
Fs

1

log[

(x′, y)

] − ClogC

(x′, y)

(X, Y )) − ClogC

(7)

(X, Y ) = · · · = P C
Fs

since the Jensen-Shannon divergence(JSD) loss is non-
negative, the only zero solution arrives at P 1
(X, Y ) =
Fs
P 2
(X, Y ), which implies that the
Fs
joint distributions of learned feature representations across
domains are perfectly matched. Moreover, φ′ achieves the
global minimum −ClogC.

3.4. Orthogonal regularization

Private neural networks are shown on two sides of the
shared network in Figure 3. They aim to learn domain-
speciﬁc information that can not be learned in the shared
network, which mostly preserve the discriminative ability
and provide strong support for the domain-invariant fea-
tures. To obtain the domain-speciﬁc knowledge, we in-
troduce two types of orthogonal regularizations:
the or-
thogonal regularization between domain-invariant features
and domain-speciﬁc features in each domain; the orthogo-
nal regularization between domain-speciﬁc features across
domains. We call the ﬁrst kind of regularization as intra-
domain orthogonal regularization and the second as extra-
domain orthogonal regularization.

In previous works, they only apply a soft subspace or-
thogonal regularization [1] between the private and the
shared space in each domain to ensure their independence,
which can be formulated as follows:

Rintra orth = X

C

m=1

kS⊤

mHmk2
F

(8)

where Sm is the matrix whose rows are the domain-invariant
features in the m-th domain, and Hm is the matrix whose
rows are the domain-speciﬁc features in the m-th domain.

There exists a potential drawback in this model:

the
domain-invariant features can simultaneously occur in mul-
tiple private networks among different domains.
Let
H1, H1
p represents the entire feature space and private fea-
ture space in domain 1, H2, H2
p represents the entire fea-
ture space and the private feature space in domain 2, and S
represents the shared space between domains; then, we can
formulate the loss in previous work as:

The solution is not unique and can be represented as:

H1 = H1
H2 = H2
H1
H2

p + S
p + S
p ∩ S = {0}
p ∩ S = {0}

p + S ′
p + S ′

p = H1∗
p = H2∗

H1
H2
S = S ∗ − S ′






(9)

(10)

p , H2∗

where H1∗
p , S ∗ represent the optimal feature spaces;
S ′ is the complementary subspace of S in S ∗. The duplicate
subspace S ′ probably arises in the private feature spaces,
then the individual network loses its own characteristics.
Therefore, we apply the subspace orthogonal regularization
between private features across domains. We express the
extra-domain regularization as:

Rextra orth =

C

X

m1=1

C

X

m2=1,
m26=m1

kH⊤

m1 Hm2 k2
F

(11)

where we denote the domain-speciﬁc features in the m1-
th and m2-th domains as Hm1 and Hm2 . Then, by forcing
p , S ∗.
S ′ = {0}, we obtain the optimal solutions H1∗
Furthermore, the disappearance of domain-invariant fea-
tures in the domain-speciﬁc feature space promotes the
learning of the shared network.

p , H2∗

To reduce the correlation between private features and
shared features in each domain, we add the procedure of
training domain-speciﬁc features individually. We set all
domain-invariant features to zero and only train the domain-
speciﬁc features in this procedure:

Fzeros private = [zeros, Fp(x)]

(12)

The private feature learning can be formulated as:

Lp = min
Fp

(−

C

X

m=1

1
Nm

Nm

X

i=1

log[Cm(ym

i |Fzeros private)])

(13)
where Fp represents parameters in the private feature ex-
tracting network. The entire network realizes the optimal
independent feature learning with the well-designed regu-

7197

larizations and losses.

3.5. Training procedure

The aforementioned losses and regularizations can be

linearly combined as follows:
Lall = Ldomain + λ1Ljal + λ2Rintra orth + λ3Rextra orth
(14)

where λi denotes the weight of each restriction. The
stochastic gradient descent is applied to update the param-
eters in the network and the gradient reversal layer reverses
the gradient from the domain discriminative network to up-
date the parameters in the feature extracting network. The
entire training procedure is shown in Algorithm 1, the opti-
mization process stops until ﬁnding the saddle point in neu-
ral networks.

Algorithm 1 The training procedure for proposed compact
feature learning
Input:

C labeled datasets {Xm, Ym}C
m=1; Initiated shared fea-
ture extractor Fs; Initiated private feature extractor Fp;
Shared category classiﬁer Cs; Domain discriminator D;
Domain category classiﬁer {Cm}C

m=1;

Output:

Well-trained shared feature extractor F ∗
vate feature extractor F ∗
{Cm}C

s ; Initiated pri-
p ; Domain category classiﬁer

m=1;

1: while not converged do
2:

Sample mini-batch from {Xm, Ym}C
for m = 1 : C do

m=1

Update Fp, Fs, Cm by Eq.2, Eq.8, Eq.11;
Update Fp by Eq.13, ﬁx the classiﬁers in each do-
main and set the shared features to zeros, then up-
date the parameters in Fp;
Update Fs, Cs, D by Eq.3, reverse the gradient
from the discriminator during the backward prop-
agation to update the parameters in the Fs;

end for
7:
8: end while
9: return F ∗

p = Fp; F ∗

s = Fs; C∗

m = Cm;

4. Experiments

We evaluate our proposed method on three image clas-
siﬁcation datasets: the MNIST dataset, the VLCS dataset
[22, 12] and the PACS [14] dataset. We compare our pro-
posed methods with these following works:

• Indiv: Different networks are applied to deal with dif-
ferent domains. Each network is trained individually
without any connection with the networks for related
domains.

3:

4:

5:

6:

Table 1. Performance comparison between different methods for
multi-domain learning with respect to accuracy(%) on MNIST
dataset.

Method

Indiv

Indiv l2

Cross stitch

Cross connect

Share
MAN

JARN
JOARN

Domain 1

Mnist
96.01
96.32
96.38
96.28
96.48
96.48

96.56
97.10

Domain 2
Mnist m

85.76
85.93
86.34
87.09
86.16
86.62

88.54
89.34

Figure 4. Visualization of examples in MNIST-M and MNIST
dataset.

• Indiv l2 [6]: Each domain has an individual network,
and the l2 distance regularization among parameters
between the networks for different domains is applied.

• Cross stitch [18]: Different networks are applied to
address different domains. The outputs in identical
shallow layers of each domain are linearly combined
for feature sharing.

• Cross connect [8]: Different networks are applied to
address different domains. 1×1 convolution layers be-
tween identical layers in each domain are applied to
learn the inﬂuence degree of each feature map in the
related domains.

• Share: Instead of the individual network for each do-
main, a single network is applied to simultaneously ad-
dress all related domains.

• MAN [3]: The adversarial training strategy is applied
to obtain domain-invariant information and orthogonal
regularizations are applied to eliminate redundant fea-
tures between the private and shared feature spaces.

• JARN (joint adversarial restriction network): The
joint adversarial loss is applied to obtain the joint dis-
tribution matching and orthogonal regularizations are
applied to eliminate redundant features between the
private and shared feature spaces.

• JOARN (joint orthogonal and adversarial restriction
network): Losses and regularizations in Eq.14 are ap-
plied in JOARN.

To ensure the fairness of the experiments, the same archi-
tecture is applied in these aforementioned works.

7198

(a) Initial shared features

(b) Shared features with restriction

(c) Initial private features

(d) Private features with restriction

Figure 5. The visualization of extracted features in MNIST and MNIST-M. Different colors refer to different classes. From Figure 5(a) and
Figure 5(b), we can observe that with the help of the discriminator and the shared classiﬁer, the distribution of shared features becomes
nondiscriminative across domains and has a clear and distinct boundaries between classes. From Figure 5(c) and Figure 5(d), the domain-
speciﬁc features obtain uniqueness and discriminability in each domain with the orthogonal restriction between them.

(a) Accuracy curve of voc2007

(b) Accuracy curve of sun09

Figure 6. Accuracy curves in VLCS. The combination of domain-
invariant features and domain-speciﬁc features can signiﬁcantly
improve the classiﬁcation performance in each domain.

4.1. MNIST and MNIST M Dataset

4.1.1 Settings

Examples from MNIST-M and MNIST datasets are shown
in Figure 4. The MNIST-M dataset is composed of MNIST
dataset and patches randomly extracted from BSD500. The
variation of backgrounds means the change of image do-
mains. Obviously, the classiﬁcation in MNIST-M is more
difﬁcult than that in MNIST because of the more compli-
cated background in the pictures. We randomly select 1000
training samples in each domain. All 10000 test examples
in each dataset are used. The architecture that we used is
identical to that in [1]. It has two convolution layers and
three fully connected layers. We connect the GRL layer
to the ﬁrst fully connected layer in the main network. The
domain-invariant features and domain-speciﬁc features are
extracted at the same position.

4.1.2 Visualization and analysis

cannot be well learned. MAN achieves better performance
since it obtains domain-invariant features through adversar-
ial training, and the domain-speciﬁc features and domain-
invariant features are extracted individually to prevent the
interfering between them. Our JARN outperforms MAN.
The shared classiﬁer is applied in JARN to get the joint dis-
tribution matching of domain-invariant features. JOARN
obtains a better result and outperforms all other methods.

To obtain an intuitive observation of the inﬂuence of
our proposed regularizations in the network, we use the t-
SNE projection to visualize the domain-invariant features
and domain-speciﬁc features in different situations. From
Figure 5(a), we can observe two drawbacks of the shared
features with no restriction. One drawback is the network
can only learn the partial matching between the distribu-
tions of shared features across domains. The other is the
cross-domain features in different classes are mismatched
at the interfacial boundaries, which produces the difﬁculty
in classiﬁcation. As shown in Figure 5(b), with the ad-
versarial loss and the shared classiﬁer, the distribution of
shared features becomes nondiscriminative across domains
and has a clear and distinguishable boundary for the main
learning goal. On the other side, the network prefers to learn
domain-speciﬁc features of different properties in different
domains in Figure 5(c). Along with the increasing com-
plexity of the dataset, there is probable intersection between
domain-speciﬁc feature spaces. In Figure 5(d) the private
features in each domain obtain the uniqueness and discrim-
inability with the orthogonal restriction between them.

4.2. VLCS Dataset

4.2.1 Setting

As indicated in Table 1, we can see that the improvement
in different domains is unbalanced since the shared fea-
tures account for different importance in the classiﬁcation
problems across datasets. Previous works such as indiv l2,
cross stitch, and cross connect only obtain limited improve-
ment in multi-domain learning. This is because they do not
consider the matching of domain-invariant feature distribu-
tions across domains. Then, the domain-invariant features

VLCS is a real world image classiﬁcation dataset. We
select 3 different types of subdatasets in it for three do-
mains: PASCAL VOC2007 (V)
[20]
and SUN09 (S)
[5]. Each subdataset contains ﬁve com-
mon classes: “bird”, “car”, “chair”, “dog” and “person”.
Each subdataset is randomly split into two parts: 70% for
training and 30% for testing. To be consistent with earlier
researches and facilitate comparison experiments, we only

[7], LabelMe (L)

7199

Table 2. Performance comparison between different methods for multi-domain learning with respect to accuracy(%) on VLCS dataset.

Dataset pair 1

Dataset pair 2

Dataset pair 3

Voc2007

Sun09

Voc2007
Labelme

Sun09

Labelme

Indiv
78.13
78.40
78.13
73.13
78.40
73.13

Indiv l2

Cross stitch

Cross connect

78.13
78.60
78.62
73.5
78.80
72.75

77.73
78.80
77.73
74.13
78.90
72.63

78.52
78.90
78.42
73.25
78.60
72.75

Share MAN
78.72
78.42
78.80
78.90
77.93
77.93
73.88
74.13
78.49
78.30
73.5
72.88

JARN
79.31
81.03
78.62
74.5
78.70
74.13

JOARN

80.00
81.54
79.41
76.13
80.02
75.75

Table 3. Performance comparison between different methods for multi-domain learning with respect to accuracy(%) on PACS dataset.

Dataset pair 1

Dataset pair 2

Dataset pair 3

Cartoon

Art painting

Cartoon
Sketch

Art painting

Sketch

Indiv
91.65
87.38
91.65
90.65
87.38
90.65

Indiv l2

Cross stitch

Cross connect

91.94
87.54
91.94
90.74
87.70
90.91

92.26
87.06
92.08
90.91
87.70
91.16

91.80
87.54
91.94
90.65
87.86
91.08

Share MAN
91.80
91.65
88.19
87.86
91.51
91.51
91.59
91.08
86.73
87.54
91.93
91.33

JARN
92.65
89.00
92.93
92.44
88.51
92.10

JOARN

93.35
89.97
93.21
93.12
89.42
93.54

use two subdatasets each time. However, new domains can
be easily extended by appending the afﬁliated private net-
work. Following previous works, the structures of shared
network and the private network in our multi-domain model
are the same as AlexNet[13], and the convolution layers are
initial with the pretrained Alexnet. Moreover, we extract the
FC6 features as the domain-invariant features and domain-
speciﬁc features[14], and we use three fully connected lay-
ers (1024 − 1024 − 2) in our domain discriminative network
then connect it with the FC6 layer in the main network via
the GRL.

4.2.2 Analysis

As shown in Figure 6, the domain-invariant features and
domain-speciﬁc features can efﬁciently classify the images,
and the combination of shared features and private features
signiﬁcantly improves the classiﬁcation performance. Ad-
ditionally, along with the training, the performance of each
network gradually improves and arrives at a stable stage.
The entire network realizes the optimal independent feature
learning. The experimental results are summarized in Ta-
ble 2. Similar conclusions can be obtained as in the exper-
iments of MNIST and MNIST-M. Note that the result of
MAN is worse than previous works sometimes. This out-
come is because the shared features in MAN only match
the marginal distributions, and the accuracy varies signiﬁ-
cantly in the training process since the changes of the con-
ditional distributions P (Y |Fs(X)) across domains. More-
over, the redundant features in the domain-speciﬁc network
lead to the discriminablity of private features. Our network
extracts the shared features through matching joint distri-
butions P (Fs(X), Y ) across domains and obtains indepen-
dent private features, thus performs better than other ones.

4.3. PACS Dataset

We select three different image styles in PACS: art-
painting (A), cartoon (C) and sketch (S). Each image style

can be viewed as one domain. The image styles across do-
mains in PACS are of marked difference. We also split each
subdataset into two parts randomly: 70% for training and
30% for testing. There are 7 common categories in each
dataset: “dog”, “elephant”, “giraffe”, “guitar”, “horse”,
“house”, “person”. Additionally, the training architecture
of the PACS is identical to that of VLCS, except the features
are extracted from the FC7 layer [14] and the GRL layer is
connected to the FC7 layer. Note that PACS has a bigger do-
main bias across subdatasets than VLCS. Consequently, the
conditional distributions P (Y |Fs(X)) across domains vary
signiﬁcantly and the matching of joint distributions of fea-
tures is of great important. From the results presented in Ta-
ble 3, we can observe that the proposed JARN and JOARN
algorithm achieve the better performance than others, which
demonstrate the effectiveness of our method.

5. Conclusion

In this paper, we propose compact feature learning to in-
dividually extract more optimal domain-invariant features
and domain-speciﬁc features. We train the adversarial net-
work with a shared classiﬁer across domains, where the
joint distribution of each domain can be matched. More-
over, the orthogonal loss is applied to ensure the unique-
ness of each private space. Compact feature learning sig-
niﬁcantly improves the general classiﬁcation performance
over related domains, as the results demonstrate the effec-
tiveness of our method.

Acknowledgements

We acknowledge funding from National Key R&D
Program of China under Grants 2017YFA0700800 and
2017YFB1002203, and Natural Science Foundation of
China (NSFC) under Grant 61872329.

7200

ation for Computational Linguistics on Human Language
Technologies: Short Papers, pages 257–260. Association for
Computational Linguistics, 2008.

[16] Y. Li, X. Tian, M. Gong, Y. Liu, T. Liu, K. Zhang, and
D. Tao. Deep domain generalization via conditional invariant
adversarial networks. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 624–639, 2018.
[17] P. Liu, X. Qiu, and X. Huang. Adversarial multi-task learn-
ing for text classiﬁcation. arXiv preprint arXiv:1704.05742,
2017.

[18] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-
stitch networks for multi-task learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3994–4003, 2016.

[19] S. Ruder. An overview of multi-task learning in deep neural

networks. arXiv preprint arXiv:1706.05098, 2017.

[20] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Free-
man. Labelme: a database and web-based tool for image
annotation. International journal of computer vision, 77(1-
3):157–173, 2008.

[21] S. Si, D. Tao, and B. Geng. Bregman divergence-based regu-
larization for transfer subspace learning. IEEE Transactions
on Knowledge and Data Engineering, 22(7):929–942, 2010.
[22] A. Torralba and A. A. Efros. Unbiased look at dataset bias.
In Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, pages 1521–1528. IEEE, 2011.

[23] F. Wu and Y. Huang. Collaborative multi-domain sentiment
In 2015 IEEE International Conference on

classiﬁcation.
Data Mining (ICDM), pages 459–468. IEEE, 2015.

[24] Y. Yang and T. M. Hospedales. Trace norm regularised deep
multi-task learning. arXiv preprint arXiv:1606.04038, 2016.

References

[1] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and
D. Erhan. Domain separation networks. In Advances in Neu-
ral Information Processing Systems, pages 343–351, 2016.

[2] R. Caruna. Multitask learning: A knowledge-based source
of inductive bias. In Machine Learning: Proceedings of the
Tenth International Conference, pages 41–48, 1993.

[3] X. Chen and C. Cardie. Multinomial adversarial net-
works for multi-domain text classiﬁcation. arXiv preprint
arXiv:1802.05694, 2018.

[4] X. Chen, Z. Shi, X. Qiu, and X. Huang. Adversarial
multi-criteria learning for chinese word segmentation. arXiv
preprint arXiv:1704.07556, 2017.

[5] M. J. Choi, J. J. Lim, A. Torralba, and A. S. Willsky. Ex-
ploiting hierarchical context on a large database of object
categories. 2010.

[6] L. Duong, T. Cohn, S. Bird, and P. Cook. Low resource
dependency parsing: Cross-lingual parameter sharing in a
neural network parser.
In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Lan-
guage Processing (Volume 2: Short Papers), volume 2, pages
845–850, 2015.

[7] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. International journal of computer vision, 88(2):303–
338, 2010.

[8] S. Fukuda, R. Yoshihashi, R. Kawakami, S. You, M. Iida,
and T. Naemura. Cross-connected networks for multi-task
learning of detection and segmentation.
arXiv preprint
arXiv:1805.05569, 2018.

[9] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky. Domain-
adversarial training of neural networks. The Journal of Ma-
chine Learning Research, 17(1):2096–2030, 2016.

[10] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[12] A. Khosla, T. Zhou, T. Malisiewicz, A. A. Efros, and A. Tor-
ralba. Undoing the damage of dataset bias.
In European
Conference on Computer Vision, pages 158–171. Springer,
2012.

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.

[14] D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Deeper,
broader and artier domain generalization. In Computer Vi-
sion (ICCV), 2017 IEEE International Conference on, pages
5543–5551. IEEE, 2017.

[15] S. Li and C. Zong. Multi-domain sentiment classiﬁcation.
In Proceedings of the 46th Annual Meeting of the Associ-

7201

