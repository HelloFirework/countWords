Simultaneously Optimizing Weight and Quantizer of Ternary Neural Network

using Truncated Gaussian Approximation

Zhezhi He

Deliang Fan

University of Central Florida

University of Central Florida

Orlando, FL 32816

Orlando, FL 32816

Elliot.He@Knights.ucf.edu

dfan@ucf.edu

Abstract

In the past years, deep convolution neural network has
achieved great success in many artiﬁcial intelligence ap-
plications. However, its enormous model size and massive
computation cost have become the main obstacle for the de-
ployment of such powerful algorithm in the low power and
resource-limited mobile systems. As the countermeasure to
this problem, deep neural networks with ternarized weights
(i.e., -1, 0, +1) have been widely explored to greatly re-
duce the model size and computational cost, with limited
accuracy degradation.
In this work, we propose a novel
ternarized neural network training method which simulta-
neously optimizes both weights and quantizer during train-
ing, differentiating from prior works. Instead of ﬁxed and
uniform weight ternarization, we are the ﬁrst to incorpo-
rate the thresholds of weight ternarization into a closed-
form representation using truncated Gaussian approxima-
tion, enabling simultaneous optimization of weights and
quantizer through back-propagation training. With both of
the ﬁrst and last layer ternarized, the experiments on the Im-
ageNet classiﬁcation task show that our ternarized ResNet-
18/34/50 only has ∼3.9/2.52/2.16% accuracy degradation
in comparison to the full-precision counterparts.

1. Introduction

Artiﬁcial intelligence is nowadays one of the hottest re-
search topics, which has drawn tremendous efforts from
various ﬁelds in the past years. While computer scientists
have succeeded to develop Deep Neural Networks (DNN)
with transcendent performance in the domains of computer
vision, speech recognition, big data processing and etc.
[13]. The state-of-the-art DNN evolves into structures with
larger model size, higher computational cost and denser
layer connections [8, 25, 24, 11]. Such evolution brings
great challenges to the computer hardware in terms of both
computation and on-chip storage [10], which leads to great

research effort on the topics of model compression in recent
years, including channel pruning [9, 29], weight sparsiﬁca-
tion [7], weight quantization [6] and etc [10].

Weight ternarization, as a special case of weight quan-
tization technique to efﬁciently compress DNN model,
mainly provides three beneﬁts: 1) it converts the ﬂoating-
point weights into the ternary format (i.e., -1, 0, +1), which
can signiﬁcantly reduce the model size by 16×. With proper
sparse encoding technique, such model compression rate
can be further boosted. 2) Besides the model size reduc-
tion, the ternarized weight enables elimination of hardware-
expensive ﬂoating-point multiplication operations, while re-
placing with hardware friendly addition/subtraction opera-
tions. Thus, it could signiﬁcantly reduce the inference la-
tency. 3) The ternarized weights with zero values intrin-
sically prune network connections, thus the computations
related to those zero weights can be simply skipped.

In the previous low bit-width quantization works, such
as TTN [15], TTQ [28] and BNN [5], they do re-train the
models’ weights but a ﬁxed weight quantizer is used and
not properly updated together with other model parame-
ters, which leads to accuracy degradation and slow con-
vergence of training.
In this work, we have proposed a
network ternarization method which simultaneously update
both weights and quantizer (i.e. thresholds) during training,
where our contributions can be summarized as:

• We propose a fully trainable DNN ternarization
method that can jointly train the quantizer threshold,
layer-wise scaling factor, and weights to minimize the
accuracy degradation caused by the model compres-
sion.

• Rather than utilizing the ﬁxed and uniform ternar-
izer, we are the ﬁrst to incorporate the thresholds of
weight ternarization into a closed-form expression us-
ing truncated Gaussian approximation, which can be
optimized through back-propagation together with net-
work’s other parameters through the end-to-end train-
ing.

11438

• We further optimize the widely used Straight-
Through-Estimator (STE) [2, 5] with gradient correct-
ness technique. It gives better gradient approximation
for the non-differential staircase ternarization function,
which leads to faster convergence speed and higher in-
ference accuracy.

• In order to validate the effectiveness of our proposed
methods, we apply the proposed model ternarization
method on CIFAR-10 and ImageNet datasets for object
classiﬁcation task.

The rest of this paper is organized as follows. We ﬁrst
give a brief introduction to the related works regarding the
topics of model compression. Then the proposed network
ternarization method and the applied tricks are explained in
details. In the following section, experiments are performed
on both small and large scale dataset with the various DNN
architectures, to evaluate the effectiveness of our proposed
method. After that, the conclusion is drawn in the end.

2. Related Works

Recently, model compression on deep convolutional
neural network has emerged as one hot topic in the hard-
ware deployment of artiﬁcial intelligence. There are various
techniques, including network pruning [17], knowledge dis-
tillation [18], weight sparsiﬁcation [7], weight quantization
[6] and etc. [22], to perform network model compression.

As one of the most popular technique, weight quan-
tization techniques are widely explored in many related
works which can signiﬁcantly shrink the model size and re-
duce the computation complexity [10]. The famous deep
compression technique [6] adopts the scheme that optimiz-
ing weight quantizer using K-means clustering on the pre-
trained model. Even though the deep compression tech-
nique can achieve negligible accuracy degradation with 8-
bit quantized weight, its performance on low-bit quantized
case is non-ideal. Thereafter, many works are devoted to
quantize the model parameters into binary [5, 20] or ternary
formats [28], not only for its extremely model size reduc-
tion (16× ∼ 32×), but also the computations are simpliﬁed
from ﬂoating-point multiplication (i.e. mul) operations into
addition/subtraction (i.e. add/sub). BinaryConnect [4] is
the ﬁrst work of binary CNN which can get close to the
state-of-the-art accuracy on CIFAR-10, whose most effec-
tive technique is to introduce the gradient clipping. After
that, both BWN in [20] and DoreFa-Net [27] show better
or close validation accuracy on ImageNet dataset. In order
to reduce the computation complexity, XNOR-Net [20] bi-
narizes the input tensor of convolution layer which further
converts the Add/Sub operations into bit-wise Xnor and
bit-count operations.

trained scaling factors [28]. Leng et al. employ ADMM
method to optimize neural network weights in conﬁgurable
discrete levels to trade off between accuracy and model size
[14]. ABC-Net in [16] proposes multiple parallel binary
convolution layers to improve the network model capac-
ity and accuracy, while maintaining binary kernel. All the
aforementioned aggressive DNN binarization or ternariza-
tion methods sacriﬁce inference accuracy, in comparison
with the full precision counterpart, to achieve large model
compression rate and computation cost reduction.

3. Methodology

3.1. Problem Deﬁnition

As for weight quantization of neural networks, the state-
of-the-art work [26] typically divides it
into two sub-
problems: 1) minimizing the quantization noise (i.e., Mean-
Square-Error) between ﬂoating-point weights and quan-
tized weights, and 2) minimizing the inference error of
DNN w.r.t the deﬁned objective function of DNN infer-
ence. In this work, instead of optimizing two sub-problems
separately, we mathematically incorporate the thresholds of
weight quantizer into neural network forward path, thus en-
abling the simultaneous optimization of weights and thresh-
olds through back-propagation method. In this work, given
the vectorized input x and target t, the network optimiza-
tion problem can be described as:

{wl,Sl,∆

±

L(f (x; {w′

arg min
l }L
l = T ern(wl, Sl, ∆±
l )

s.t. w′

l}L

l=1

l=1), t)

(1)

l}L

l}L

where f (x; {w′
l=1) calculates the outputs of DNN, which
is parameterized by the ternarized weight, w.r.t the input
x. {w′
l=1. L is the number of layers in DNN. wl is
the ﬂoating-point weights in l-th layer, before ternarization.
L(·, ·) is the deﬁned loss function. The ternarization func-
tion T ern() in Eq. (1) is parameterized by the ternarized
value Sl and thresholds ∆±
l , where the equation detail is
given as Eq. (2) in the following section.

3.2. Trainable ternarization under Gaussian ap 

proximation

In this subsection, we will ﬁrst introduce our weight
ternarization methodology. Then, our proposed method
to incorporate ternarization thresholds into neural network
inference path, which makes it trainable through back-
propagation, is discussed particularly.

3.2.1 Network Ternarization:

Besides weight binarization, there are also recent works
proposing to ternarize the weights of neural network using

For the sake of obtaining a DNN with ternarized weight
and minimized accuracy gap w.r.t to its full-precision coun-

11439

(a) processes to only update thresholds δ

(b) processes to only update weights w

Figure 1. The ﬂowchart of network ternarization, where
solid/dashed line
step transition.
1)⇒2)⇒3)⇒2)⇒4) steps are iteratively operated during training.

activate/inactive

indicate

terpart, the training scheme for one iteration (as shown in
Fig. 1) can be generally enumerated as four steps:

1) Initialize the weight with full-precision pre-trained
model. Previous works have experimentally demon-
strated that ﬁne-tuning the pre-trained model with
small learning rate normally generates a quantized
model with higher accuracy. More importantly, with
the pre-trained model as parameter initialization, much
less number of training epochs is required to get model
converged in comparison to training from scratch.

2) Ternarize the full-precision weight wl,i w.r.t the layer-
wise thresholds ∆±
l and quantized value Sl (aka. scal-
ing coefﬁcient) in real-time. The weight ternarization
function can be described as:

w′

l,i = Sl · T ern(wl,i, ∆±
l )

= Sl ·




+1 wl,i > ∆+
l
0
−1 wl,i < ∆−
l

∆−

l ≤ wl,i ≤ ∆+

l

(2)

Note that, the scaling coefﬁcient Sl can be written in
a closed-form function with threshold, which is key to
incorporate the the quantizer optimization into DNN
training without modifying the loss function. The for-
mula derivation of Sl will be speciﬁed in Section 3.2.2.
Moreover, since we propose to use symmetric thresh-
olds centered by µl for weight ternarization, thus we
reformat ∆±
l = µl ±δl, where µl is the statistical mean
of wl.

3) With one given input batch, this step only updates the
thresholds {δl}L
l=1 through back-propagation. Mean-
while, the update of weight is suspended in the current
step.

l}L

4) With the identical input batch, it repeats step-2 to
synchronize the ternarized weights {w′
l=1 w.r.t the
updated thresholds {δl}L
l=1 in step-3. Then, it sus-
pends the update of thresholds and only allows full-
precision weight base to be updated 1. Since the stair-
case ternarization function (T ern(·) in Eq. (2)) is non-
differential owing to its zero derivatives almost ev-
erywhere, we adopt the method of Straight-Through-
Estimator (STE) [2] similar as previous network quan-
tization works [28]. It is noteworthy that we propose
and apply the gradient correctness technique on STE,
which is critical to improving the convergence speed
for weight retraining (see details in Section 3.3).

Now with the ternarized weights, the major computa-
tion of DNN is converted from the computational expensive
ﬂoating-point Multiplication-and-Accumulation (MAC) to
more efﬁcient and less complex addition and subtraction
(Add/Sub). The computation can be expressed as2:

l · w′
xT

l = xT

l · (Sl · T ern(wl)) = Sl · (xT

l · T ern(wl)) (3)

where xl and w′
l are the vectorized input and ternarized
weight of l-th layer respectively.
In the state-of-the-art
DNN architectures, convolution/fully-connected layers nor-
mally follows a batch-normalization layer [12] (i.e., Afﬁne
function) or ReLU, where both of them perform element-
wise multiplication on their input tensor (i.e., xT
l).
l
in Eq. (3)
Therefore,
can be emitted and integrated with the following batch-
norm/RELU layer in the forward path. In addition to the
above description, we formalize the operations in Algo-
rithm 1 as well for clariﬁcation.

the element-wise scaling with Sl

· w′

3.2.2 Trainable thresholds utilizing truncated Gaus-

sian distribution approximation:

It has been discussed in previous works [3, 1] that the
weighted distributions of spatial convolution layers and
fully-connected layers are intending to follow Gaussian dis-
tribution, whose histogram is in bell-shape, owing to the
regularization effect of L2-norm weight penalty. For exam-
ple, in Fig. 2, we have shown the weight distributions and
their corresponding Probability Density Function (PDF) us-
ing the calculated mean and standard deviation for each
parametric layer (i.e., convolution and fully-connected lay-
ers) in ResNet-18b [8]. Meanwhile, the Shapiro-Wilk nor-
mality test [23] is conducted to identify whether the weight
sample originated from Gaussian distribution quantitatively.
The given test statistic Ws of Shapiro-Wilk normality test
indicate a good normally distribution match with minimum

1During the training, ternarized weights are calculated from the full-
precision weight base in real-time, thus the weight update is performed on
the full-precision weight instead of its ternarized counterpart.

2For simplicity, we neglect the bias term.

11440

BatchLoss'___1)2)3)4)BatchLoss'___1)2)3)4)Figure 2. The histogram of weights wl (blue shadow) along with the PDF curve (red line) of Gaussian distribution N (µl, σ2
convolution, fully-connected and residual layers in ResNet-18b [8]. µl and σ2
layers with more number of weights (#w), the weights distribution conforms to the Gaussian distribution more precisely.

l ), for each
l are the statistical mean and variance of wl, respectively. For

0.82 value. Note that, the asymmetry (i.e., Skewness) of
the last fully-connected layer is due to the existence of bias
term. In this work, we consider the weight of parametric
layers (i.e., convolution and fully-connected layers) approx-
imately following Gaussian distribution, then we perform
the weight ternarization based on such approximation.

In order to make the thresholds {δl}L

l=1 as trainable
parameters that can be updated through back-propagation,
there are two criteria that have to meet:

• Thresholds {δl}L

l=1 have to be parameters within the

DNN inference path in a closed-form expression.

• Such closed-form expression is differentiable w.r.t the

thresholds.

Hereby, we ﬁrst make the assumption that:

Assumption 1 the weights of designated layer l are ap-
proximately following Gaussian distribution (i.e., wl ∼
N (µl, σ2
l )), where µl and σl are the calculated mean and
standard deviation of the weight sample wl.

where such assumption is the key to incorporate the thresh-
olds into DNN inference path in a differentiable closed-
form.

For the quantizer design of either uniformly or non-
uniformly distributed data, the centroid is normally taken
as the quantized value to minimize the quantization error
[19]. Thus, for weight ternarization, the layerwise scaling
coefﬁcient (i.e., quantized value) can be described as:

Sl(wl, ∆±

−
l

−∞

φc(x) · xdx + Z +∞

l ) = Z ∆
= E(|wl,i|(cid:12)(cid:12)(wl,i > ∆+

∆l

φc(x) · xdx

(4)

l ) ∪ (wl,i < ∆−

l ))

where φc(x) is the conditional PDF under the condition of (x >
∆+
l = µl ± δl, we can
approximate the Eq. (4) and reformat it into:

l ). In this work, by setting ∆±

l ) ∨ (x < ∆−

Sl(µl, σl, δl) = Z b=+∞

a=µl+δl

φ(x(cid:12)(cid:12)µl, σl)

Φ(b(cid:12)(cid:12)µl, σl) − Φ(a(cid:12)(cid:12)µl, σl)

· xdx (5)

where φ(x(cid:12)(cid:12)µl, σl) and Φ(x(cid:12)(cid:12)µl, σl) are the PDF and CDF for

Gaussian distribution N (µl, σ2
l ) . Such calculation can directly
utilize the closed-form expression of mathematical expectation
for truncated Gaussian distribution with lower bound a and up-
per bound b. Thus, we ﬁnally obtain a closed-form expression of
scaling factor embedding trainable thresholds δl:

α =

a − µl

σl

=

δl
σl

; β =

b − µl

σl

= +∞

(6)

Sl(µl, σl, δl) = µl − σl ·

φ(β|0, 1) − φ(α|0, 1)
Φ(β|0, 1) − Φ(α|0, 1)

= µl + σl ·

φ(α|0, 1)

1 − Φ(α|0, 1)

(7)

where φ(·(cid:12)(cid:12)0, 1) and Φ(·(cid:12)(cid:12)0, 1) are PDF and CDF of standard nor-

mal distribution N (0, 1).

(a)

(b)

Figure 3. The forward and backward curves for (a) Sl(µl, σl, δl)
and (b) Sl(µl, σl, δc
l is δl with clipping con-
straints . Note that, we choose µl = 0 and σl = 1 as the example
for visualization.

l ) w.r.t δl, where δc

As shown in Fig. 3a, we plot the function of Sl in the for-
ward and backward paths w.r.t the variation of δl for visualiza-
tion. Since most of the popular deep learning frameworks using

11441

0.50.00.51.001020conv1=2.94E-05=1.30E-01#w=9408Ws=0.860.50.00.502550layer1.0.conv1=-3.09E-03=5.34E-02#w=36864Ws=0.820.40.20.00.20510layer1.0.conv2=-8.89E-04=4.52E-02#w=36864Ws=0.950.50.00.50510layer1.1.conv1=-2.42E-03=5.08E-02#w=36864Ws=0.900.40.20.00.20510layer1.1.conv2=-1.26E-03=4.40E-02#w=36864Ws=0.960.20.00.20510layer2.0.conv1=-1.45E-03=4.16E-02#w=73728Ws=0.950.20.00.20.4010layer2.0.conv2=-1.25E-03=3.40E-02#w=147456Ws=0.940.50.00.50510layer2.0.downsample.0=-2.59E-03=7.06E-02#w=8192Ws=0.860.40.20.00.20.4010layer2.1.conv1=-1.53E-03=3.42E-02#w=147456Ws=0.950.20.00.2010layer2.1.conv2=-1.27E-03=3.01E-02#w=147456Ws=0.970.40.20.00.2010layer3.0.conv1=-1.37E-03=2.90E-02#w=294912Ws=0.950.20.00.201020layer3.0.conv2=-7.87E-04=2.50E-02#w=589824Ws=0.960.20.00.2010layer3.0.downsample.0=-1.90E-03=3.29E-02#w=32768Ws=0.960.20.00.201020layer3.1.conv1=-1.66E-03=2.24E-02#w=589824Ws=0.970.20.00.201020layer3.1.conv2=-1.44E-03=2.07E-02#w=589824Ws=0.980.20.00.20.401020layer4.0.conv1=-1.56E-03=1.99E-02#w=1179648Ws=0.980.20.00.201020layer4.0.conv2=-1.30E-03=1.73E-02#w=2359296Ws=0.990.50.00.5010layer4.0.downsample.0=-8.43E-04=3.28E-02#w=131072Ws=0.960.20.00.201020layer4.1.conv1=-2.26E-03=1.78E-02#w=2359296Ws=0.990.10.00.10.2020layer4.1.conv2=-1.08E-04=1.32E-02#w=2359296Ws=0.980.250.000.250.500.7505fc=5.85E-08=6.95E-02#w=512000Ws=0.915.02.50.02.55.0l024ForwardSl(l=0,l=1,l)5.02.50.02.55.0l0.00.51.01.52.0BackwardSl(l=0,l=1,l)l5.02.50.02.55.0l012345ForwardSl(l=0,l=1,cl)5.02.50.02.55.0l1012BackwardSl(l=0,l=1,cl)lnumerical method (e.g., Monte-Carlo method) for distribution re-
lated calculation, there will be error for calculating Sl and ∂Sl/∂δl
at the tail of distribution (i.e., δl > 3σl). For ensuring the cor-
rectness of Sl in both forward and backward path and prevent the
framework convergence issue, we perform the clipping on δl, thus
|δl| ∈ (0, 3σl). Such clipping operation is functionally equivalent
as propagating δl through the hard-tanh function, which is piece-
wise linear activation function with upper-limit j and lower-limit
k, then the trainable thresholds with clipping constraints can be
expressed as:

hardtanh(x, j, k) = Clip(x, j, k) = max(j, min(x, k))

δc
l = hardtanh(abs(δl), 0, 3σl)

(8)

(9)

After the substitution of δl with its clipped version δc
l , the forward
and backward function of Sl is transformed from Fig. 3a to Fig. 3b.
Beyond that, since the weight decay tends to push the trainable
threshold of δl close to zero which biases the ternary weight rep-
resentation towards the binary counterpart, thus we do not apply
weight decay on threshold δl during training.

In summary, we ﬁnalize the scaling factor term and weight
ternarization function to substitute the original full-precision
weight in the forward propagation path:

(a)

(b)

Figure 4. Analysis about the quantizer’s straight-through-estimator
design for (a) ro = sign(ri) for [5] and (b) ro = T ern(ri) in this
work.

T ern(wl), where both terms can pass back gradients to update
the embedding parameters. For assigning a proper gradient to the
T ern(wl,i), we follow STE design rule which leads to the follow-
ing expression:

∂w′
l,i
∂wl,i

=

∂Sl · T ern(wl,i)

∂wl,i

= Sl

∂T ern(wl,i)

∂wl,i

= 1

(14)

Sl(µl, σl, δl) = µl + σl ·

φ(δc

l /σl|0, 1)

1 − Φ(δc

l /σl|0, 1)

T ern(wl,i, µl, δl) = 


+1 wl,i > µl + δc
l
0
−1 wl,i < µl − δc
l

µl − δc

l ≤ wl,i ≤ µl + δc

l

3.3. STE with Gradient Correctness

(10)

(11)

Thus, the STE for ternarization function can be derived Eq. (14)
as:

∂T ern(wl,i)

∂wl,i

=

1
Sl

(15)

As seen in Eq. (15), instead of simply assigning the gradient as 1,
we scale the ∂T ern(wl,i)/∂wl,i w.r.t the value of Sl(µl, σl, δl)
in real time. As shown in Fig. 4b, STE could better approximate
the gradient with adjustable gradient correctness term.

Almost for any quantization function which maps the continu-
ous values into discrete space, it has encountered the same prob-
lem that such stair-case function is non-differentiable. Thus, a
widely adopted solution is using the so-called Straight-Through-
Estimator (STE) to manually assign an approximated gradient to
the quantization function. We take the STE in famous binarized
neural network [5] as an example to perform the analysis (Fig. 4a),
where the forward and backward of binarization function are de-
ﬁned as:

Forward : ro = sgn(ri)

(12)

Backward :

∂L
∂ro

STE=

=⇒

= 1

(13)

∂L

∂ri(cid:12)(cid:12)(cid:12)(cid:12)|ri|≤1

∂ro

∂ri (cid:12)(cid:12)(cid:12)(cid:12)|ri|≤1

where L is the DNN inference loss. The rule behind such STE
setup is that the output of quantization function ro can effec-
tively represent the full-precision input value ri. Thus, Sign(·)
performs the similar function as f (ri) = ri whose derivative is
∂f (ri)/∂ri = 1. However, the rough approximation in Eq. (12)
and Eq. (13) leads to signiﬁcant quantization error and hamper the
network training. When ri is either too large or too small (ri << 1
or ri >> 1), the gradients of ri will be stationary if binarized
value ro is not changed.

In order to encounter the drawback of naive STE design dis-
cussed above, we propose a method called gradient correctness for
better gradient approximation. For our weight ternarization case,
the full-precision weight base wl is represented by Sl(µl, σl, δl) ·

4. Experiment and Result Evaluation

4.1. Experiment setup

In this work, we evaluate our proposed network ternarization
method for object classiﬁcation task with CIFAR-10 and ImageNet
datasets. All the experiments are performed under Pytorch deep
learning framework using 4-way NVIDIA Titan-XP GPUs. For
clariﬁcation, in this work, both the ﬁrst and last layer are ternarized
during the training and test stage.

CIFAR-10 contains 50 thousands training samples and 10 thou-
sands test samples with 32×32 image size. The data augmentation
method is identical as used in [8]. For ﬁne-tuning, we set the initial
learning rate as 0.1, which is scheduled to scale by 0.1 at epoch 80,
120 respectively. The mini-batch size is set to 128. In order to pro-
vide a more comprehensive experimental results on large dataset,
we examine our model ternarization techniques on image classiﬁ-
cation task with ImageNet [21] (ILSVRC2012) dataset. ImageNet
contains 1.2 million training images and 50 thousands validation
images, which are labeled with 1000 categories. For the data pre-
processing, we choose the scheme adopted by ResNet [8]. Aug-
mentations applied to the training images can be sequentially enu-
merated as: 224 × 224 randomly resized crop, random horizontal
ﬂip, pixel-wise normalization. All the reported classiﬁcation ac-
curacy on validation dataset is single-crop result. The mini-batch
size is set to 256.

11442

+1-1+1-1iroriorr+1-1+1-1ilorsr1iorrirorAlgorithm 1 Training both the weights and thresholds of
ternarized network under the assumption that weights are
following Gaussian distribution.
Require: : a mini-batch of inputs x and its correspond-
ing targets yt, number of layers N , full-precision pre-
trained weights ¯w, initial thresholds δ full-precision
weight base wt and layer-wise thresholds δt from last
training iteration t, learning rate η, network inference
function f (·).

Ensure: for current iteration index of t + 1, updated full-
precision weights wt+1, updated layer-wise thresholds
δt+1.
{Step-1. Initialization:}

Table 1. Ablation study of proposed method using ResNet-20 on
CIFAR-10 dataset.

Conﬁgurations

Accuracy

full-precision (baseline)

w/ gradient correctness
w/o gradient correctness

vanilla SGD

Adam

Initialize with δl = 0.05max(|wl|)
Initialize with δl = 0.1max(|wl|)
Initialize with δl = 0.15max(|wl|)

91.7%

90.39%
87.89%

90.39%
56.31%

89.96%
90.24%
90.12%

w ← ¯w; δ ← ¯δ

⊲ This is the ﬁrst training iteration
⊲ load pretrained model

w ← wt; δ ← δt

⊲ load from last iteration

1: if t = 0 then
2:
3: else
4:
5: end if

{Step-2. Weight ternarization:}

6: for l := 1 to N do
7:

µl ← wl.mean(); σl ← wl.std()
w′

l ← Sl(µl, σl, δl) · T ern(wl, µl, δl) ⊲ Eqs. (10)

8:

and (11)
9: end for

{Step-3. Update thresholds δ only:}

10: y ← f (x, w′)
11: L ← Loss(y, yt)
12: for l := N to 1 do
gδl ← ∂L/∂δl
13:
δl ← Update(δl, gδl , η)

14:
15: end for

⊲ forward propagation, Eq. (3)
⊲ get inference error

⊲ back-propagate for gradients
⊲ Using vanilla SGD

{Repeat Step-2: from op-6 to op-10} ⊲ important step!
{Step-4. Update weights w only:}

16: y ← f (x, w′)
17: L ← Loss(y, yt)
18: for l := N to 1 do
19:

20:
21: end for

gwl ← ∂L/∂wl
wl ← Update(wl, gwl , η)

⊲ back-propagate for gradients
⊲ Using SGD/Adam

return wt+1 ← w; δt+1 ← δ

4.2. Ablation studies

In order to exam the effectiveness of our proposed methods, we
have performed the following ablation studies. The experiments
are conducted with ResNet-20 [8] on CIFAR-10 dataset, where
the differences are signiﬁcant enough to tell the effectiveness.

Figure 5. The accuracy evolution curve for train and test for the
cases w/ or w/o gradient correctness.

main reason cause the convergence speed degradation is that when
layer-wise scaling factor is less than 1, without gradient correct-
ness, the gradient of the loss function w.r.t the weights is scaled
by the scaling factor due to the chain-rule. Thus, weights are up-
dated with a much smaller step-size in comparison to the thresh-
olds, when optimized are set up with identical parameters (e.g.,
learning rate, etc.).

4.2.2 Optimizer on thresholds

The vanilla SGD and Adam are two most adopted optimizers for
quantized neural network training. Hereby, we took those two op-
timizers as an example to show the training evolution. Note that,
since weights and thresholds are iteratively updated for each input
mini-batch, we can use different optimizer for weights and thresh-
olds.
In this experiment, we use SGD for weight optimization,
while using SGD and Adam on thresholds. The result depicted in
Fig. 6 shows that it is better to use the same SGD optimizers to
achieve higher accuracy.

4.2.1 Gradient Correctness

4.2.3 Thresholds Initialization

We compare the accuracy curve convergence speed between the
STE with or without the gradient correctness. As shown in Fig. 5,
the network training speed with gradient correctness is much faster
in comparison with the case without gradient correctness. The

In order
to exam how the threshold initialization affects
the network training, we initialize the threshold as δl =
{0.05, 0.1, 0.15} · max(|wl|) for all the layers. The experimental
results reported in Fig. 7 shows that the initialization does not play

11443

0255075100125150Epoch405060708090Accuracy (%)w/ gradient correctness-testw/ gradient correctness-trainw/o gradient correctness-testw/o gradient correctness-trainTable 2. Validation accuracy (top1/top5 %) of ResNet-18/34/50b [8] on ImageNet using various model quantization methods.

Quan.
scheme

First
layer

Last
layer

Accuracy
(top1/top5)

Comp.

rate

Full precision

BWN[20]

ABC-Net[16]
ADMM[14]
TWN[15, 14]

TTN[28]

ADMM[14]

APPRENTICE[18]

this work
this work

Full precision

APPRENTICE[18]

this work

Full precision

APPRENTICE[18]

this work

-

Bin.
Bin.
Bin.
Tern.
Tern.
Tern.
Tern.
Tern.
Tern.

-

Tern.
Tern.

-

Tern.
Tern.

ResNet-18b

FP
FP
FP*
FP*
FP
FP
FP*
FP*
FP
Tern

FP
FP
FP*
FP*
FP
FP
FP*
FP*
FP
Tern

ResNet-34b

69.75/89.07

1×

60.8/83.0
68.3/87.9
64.8/86.2
61.8/84.2
66.6/87.2
67.0/87.5

∼32×
∼6.4×
∼32×
∼16×
∼16×
∼16×
∼16×
68.09/87.90 ∼16×
65.83/86.68 ∼16×

68.5/-

FP
FP*
Tern

FP
FP*
Tern

73.31/91.42

1×

72.8/-

∼16×
70.79/89.89 ∼16×

ResNet-50b

FP
FP*
Tern

FP
FP*
Tern

76.13/92.86

1×

74.7/-

∼16×
73.97/91.65 ∼16×

Figure 6. The accuracy evolution curve for train and test for the
cases with vanilla SGD and Adam optimizer

Figure 7. The accuracy evolution curve for train and test for the
cases with various threshold initialization.

an important role for network ternarization in our case. The reason
of that may comes to twofolds: 1) on one hand, all the layer-wise
ternarization thresholds are initialized with small values where the
difference is not signiﬁcant. 2) on the other hand, all the thresh-
olds are fully trainable which will mitigate the difference during
training.

4.3. Performance on ImageNet dataset

Beyond the ablation studies we performed on the CIFAR-10
dataset, we also conduct the experiment on large scale ImageNet
dataset with ResNet-18/34/50 (type-b residual connection) net-
work structures. The experimental results are listed in Table 2

together the methods adopted in related works. Since for the realis-
tic case that neural network operating on the speciﬁcally designed
hardware, it is expected that all the layers are ternarized. The re-
sults shows that, our result can achieve the state-of-the-art results.
The layer-wise thresholds are initialized as δl = 0.1×|max(wl)|.
We use the full-precision pre-trained model for weight initializa-
tion as described in Fig. 1. The learning rate starts from 1e-4, then
change to 2e-5, 4e-6, 2e-6 at epoch 30, 40, 45 correspondingly.

11444

0255075100125150Epoch20406080Accuracy (%)vanilla SGD-testvanilla SGD-trainAdam-testAdam-train0255075100125150Epoch405060708090Accuracy (%)initial with 0.05max(|w|)-testinitial with 0.05max(|w|)-traininitial with 0.1max(|w|)-testinitial with 0.1max(|w|)-traininitial with 0.15max(|w|)-testinitial with 0.15max(|w|)-train5. Conclusion and future works

In this work, we have proposed a neural network ternariza-
tion method which incorporate thresholds as trainable parameter
within the network inference path, thus both weights and thresh-
olds are updated through back-propagation. Furthermore, we ex-
plicitly discuss the importance of straight-through-estimator de-
sign for approximating the gradient for staircase function. In gen-
eral, our work is based on the assumption that the weight of deep
neural network is tend to following Gaussian distribution. It turns
out that such assumption somehow successfully returns a abstract
model for network ternarization purpose.

Acknowledgement: This work is supported in part by the Na-
tional Science Foundation under Grant No. 1740126 and Semi-
conductor Research Corporation nCORE.

References

[1] C. Baskin, E. Schwartz, E. Zheltonozhskii, N. Liss,
R. Giryes, A. M. Bronstein, and A. Mendelson. Uniq: uni-
form noise injection for the quantization of neural networks.
arXiv preprint arXiv:1804.10969, 2018. 3

[2] Y. Bengio, N. L´eonard, and A. Courville. Estimating or prop-
agating gradients through stochastic neurons for conditional
computation. arXiv preprint arXiv:1308.3432, 2013. 2, 3

[3] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wier-
stra. Weight uncertainty in neural networks. arXiv preprint
arXiv:1505.05424, 2015. 3

[4] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In Advances in neural information processing
systems, pages 3123–3131, 2015. 2

[5] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and
Y. Bengio. Binarized neural networks: Training deep neu-
ral networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016. 1, 2, 5

[6] S. Han, H. Mao, and W. J. Dally.

Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015. 1, 2

[7] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights
and connections for efﬁcient neural network.
In Advances
in neural information processing systems, pages 1135–1143,
2015. 1, 2

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 1, 3, 4, 5, 6, 7

[9] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerat-
ing very deep neural networks. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1389–
1397, 2017. 1

[10] Z. He, B. Gong, and D. Fan. Optimize deep convolutional
neural network with ternarized weights and high accuracy. In
2019 IEEE Winter Conference on Applications of Computer
Vision (WACV), pages 913–921. IEEE, 2019. 1, 2

[11] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten.
Densely connected convolutional networks.
In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, volume 1, page 3, 2017. 1

[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015. 3

[13] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature,

521(7553):436, 2015. 1

[14] C. Leng, H. Li, S. Zhu, and R. Jin. Extremely low bit neural
network: Squeeze the last bit out with admm. arXiv preprint
arXiv:1707.09870, 2017. 2, 7

[15] F. Li, B. Zhang, and B. Liu. Ternary weight networks. arXiv

preprint arXiv:1605.04711, 2016. 1, 7

[16] X. Lin, C. Zhao, and W. Pan. Towards accurate binary convo-
lutional neural network. In Advances in Neural Information
Processing Systems, pages 344–352, 2017. 2, 7

[17] J.-H. Luo, J. Wu, and W. Lin. Thinet: A ﬁlter level pruning
method for deep neural network compression. arXiv preprint
arXiv:1707.06342, 2017. 2

[18] A. Mishra and D. Marr. Apprentice: Using knowledge dis-
tillation techniques to improve low-precision network accu-
racy. arXiv preprint arXiv:1711.05852, 2017. 2, 7

[19] J. G. Proakis, M. Salehi, N. Zhou, and X. Li. Communication
systems engineering, volume 2. Prentice Hall New Jersey,
1994. 4

[20] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In European Conference on Computer Vision,
pages 525–542. Springer, 2016. 2, 7

[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015. 5

[22] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
Chen. Mobilenetv2: Inverted residuals and linear bottle-
necks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 4510–4520, 2018. 2

[23] S. S. Shapiro and M. B. Wilk. An analysis of variance test
for normality (complete samples). Biometrika, 52(3/4):591–
611, 1965. 3

[24] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2818–2826, 2016. 1

[25] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In Com-
puter Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on, pages 5987–5995. IEEE, 2017. 1

[26] D. Zhang, J. Yang, D. Ye, and G. Hua. Lq-nets: Learned
quantization for highly accurate and compact deep neural
networks. arXiv preprint arXiv:1807.10029, 2018. 2

[27] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou.
Dorefa-net: Training low bitwidth convolutional neural
networks with low bitwidth gradients.
arXiv preprint
arXiv:1606.06160, 2016. 2

11445

[28] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary
quantization. arXiv preprint arXiv:1612.01064, 2016. 1, 2,
3, 7

[29] Z. Zhuang, M. Tan, B. Zhuang, J. Liu, Y. Guo, Q. Wu,
J. Huang, and J. Zhu. Discrimination-aware channel pruning
for deep neural networks. In Advances in Neural Information
Processing Systems, pages 875–886, 2018. 1

11446

