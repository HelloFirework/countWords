DuLa-Net: A Dual-Projection Network for Estimating Room Layouts from a

Single RGB Panorama

Shang-Ta Yang1

2

,

Fu-En Wang1

sundadenny@gapp.nthu.edu.tw

fulton84717@gapp.nthu.edu.tw

Chi-Han Peng2
pchihan@asu.edu

Peter Wonka2

Min Sun1

Hung-Kuo Chu1

pwonka@gmail.com

sunmin@ee.nthu.edu.tw

hkchu@cs.nthu.edu.tw

1National Tsing Hua University

2KAUST

Abstract

We present a deep learning framework, called DuLa-Net,
to predict Manhattan-world 3D room layouts from a sin-
gle RGB panorama. To achieve better prediction accuracy,
our method leverages two projections of the panorama at
once, namely the equirectangular panorama-view and the
perspective ceiling-view, that each contains different clues
about the room layouts. Our network architecture con-
sists of two encoder-decoder branches for analyzing each
of the two views. In addition, a novel feature fusion struc-
ture is proposed to connect the two branches, which are
then jointly trained to predict the 2D ﬂoor plans and lay-
out heights. To learn more complex room layouts, we in-
troduce the Realtor360 dataset that contains panoramas
of Manhattan-world room layouts with different numbers
of corners. Experimental results show that our work out-
performs recent state-of-the-art in prediction accuracy and
performance, especially in the rooms with non-cuboid lay-
outs.

1. Introduction

Inferring high-quality 3D room layouts from indoor
panoramic images plays a crucial role in indoor scene un-
derstanding and can be beneﬁcial to various applications,
including virtual/augmented reality and robotics. To that
end, recent methods recover 3D room layouts by using deep
learning to predict the room corners and boundaries on the
input panorama. For example, LayoutNet [33] achieved
impressive reconstruction accuracy for Manhattan world-
constrained rooms. However, the clutter in the room, e.g.
furniture, poses a challenge to extract critical corners and
edges that are occluded in the input panorama. In addition,
estimating 3D layouts from 2D corner and edge maps is an
ill-posed problem and thus imposing extra constraints in the

Figure 1. 3D room layouts with different complexity are estimated
from a single RGB panorama using our system.
(Left to right)
Room layout with a ﬂoor plan of 6 corners, 8 corners, and 10 cor-
ners. The checkerboard patterns on the walls indicate the missing
textures due to occlusion.

optimization. Therefore, it remains challenging to process
complex room layouts.

In this work, we present a novel end-to-end framework
to estimate a 3D room layout from a single RGB panorama.
By the intuition that a neural network may extract differ-
ent kinds of features given the same panorama but in dif-
ferent projections, we propose to predict the room lay-
outs from two distinct views of the panoramas, namely the
equirectangular panorama-view and the perspective ceiling-
view. The network architecture follows the encoder-decoder
scheme and consists of two branches, the panorama-branch
and the ceiling-branch, for respectively analyzing images
of the panorama-view and the ceiling-view. The outputs
of panorama-branch include a ﬂoor-ceiling probability map
and a layout height, while the ceiling-branch outputs a
ﬂoor plan probability map. To share information between

13363

branches, we employ a feature fusion scheme to connect the
ﬁrst few layers of decoders through a E2P conversion that
transforms intermediate feature maps from equirectangular
projection to perspective ceiling-view. We ﬁnd that better
prediction performance is achieved by jointly training the
two connected branches. The ﬁnal 2D ﬂoor plan is then
obtained by ﬁtting an axis-aligned polygon to a fused ﬂoor
plan probability map (see Figure 3 for details) and then ex-
truded by the estimated layout height.

To learn from panoramas with complex layouts, we need
a proper dataset for network training and testing. How-
ever, existing public datasets, such as PanoContext [30]
dataset, provide mostly labeled 3D layouts with simple
cuboid shapes. To learn more complex layouts, we intro-
duce a new dataset, Realtor360, which includes a subset of
SUN360 [24] dataset (593 living rooms and bedrooms) and
1980 panoramas collected from a real estate database. We
annotated the whole dataset with a custom-made interactive
tool to obtain the ground-truth 3D layouts.

A key feature of our dataset is that it contains rooms with
more complex shapes in terms of the numbers of the cor-
ners. The experimental results demonstrate that our method
outperforms the current state-of-the-art method ([33]) in
prediction accuracy, especially with rooms with more than
four corners. Our method also takes much less time to com-
pute the ﬁnal room layouts. Fig. 1 shows some room lay-
outs estimated by our method. Our contributions are sum-
marized as follows:

• We propose a novel network architecture that con-
tains two encoder-decoder branches to analyze the in-
put panorama in two different projections. These two
branches are further connected through a feature fu-
sion scheme. This dual-projection architecture can in-
fer room layouts with more complex shapes beyond
cuboids and L-shapes.

• Our neural network is an important step towards build-
ing an end-to-end architecture. Our network directly
outputs a probability map of the 2D ﬂoor plan. This
output requires signiﬁcantly less post-processing to
obtain the ﬁnal 3D room layout than the output of the
current state of the art.

• We introduce a new data set, called Realtor360, that
contains 2573 panoramas depicting rooms with 4 to 12
corners. To the best of our knowledge, this is largest
data set of indoor images with room layout annotations
currently available.

2. Related Work

There are multiple papers that propose a solution to esti-
mate room layouts from a single image taken in an indoor

environment. They mainly differ in three aspects: 1) the as-
sumptions of the room layouts, 2) the types of the input im-
ages, and 3) the methods. In terms of room layout assump-
tions, a popular choice is the ”Manhattan world” assump-
tion [4], meaning that all walls are aligned with a global
coordinate system [4, 23]. To make the problem easier to
solve, a more restrictive assumption is that the room is a
cuboid [8, 5, 13], i.e., there exist exactly four room corners.
Our method adopts the Manhattan world assumption but al-
lows for arbitrary numbers of corners.

In terms of types of input images, the images may differ
in the FoV (ﬁeld of view) - ranging from being monocu-
lar (i.e., taken from a standard camera) to 360◦ panoramas,
and whether depth information is provided. The methods
then largely depend on the input image types. The problem
is probably most difﬁcult to solve when only a monocu-
lar RGB image is given. Typically, geometric (e.g., lines
and corners) [14, 8, 22] and/or semantic (e.g., segmentation
into different regions [9, 10] and volumetric reasoning [7])
”cues” are extracted from the input image, a set of room
layout hypotheses is generated, and then an optimization or
voting process is taken to rank and select one among the
hypotheses. Recently, neural network-based methods took
stride in tackling this problem. A trend is that the neural
networks generate higher and higher levels of information
- starting from line segments [17, 31], surface labels [5], to
room types [13] and room boundaries and corners [33], to
make the ﬁnal layout generation process increasingly eas-
ier to solve. Our method pushes this trend one step further
by using neural networks to directly predict a 2D ﬂoor plan
probability map that requires only a 2D polygon ﬁtting pro-
cess to produce the ﬁnal 2D room layout.

If depth information is provided, there exist methods that
estimate scene annotations including room layouts [28, 15,
29]. A deeper discussion is beyond the scope of this paper.

Closely related problems include depth estimation from
a given image [32, 21] and scene reconstructions from point
clouds [19, 18, 16]. Note that neither estimated depths nor
reconstructed 3D scenes necessarily equate a clean room
layout as such inputs may contain clutters.
360◦ panorama: The seminal work by Zhang et al. [30]
advocates the use of 360◦ panoramas for indoor scene un-
derstanding for the reason that the FOV of 360◦ panoramas
is much more expansive. Work in this direction ﬂourished,
including methods based on optimization approaches over
geometric [6, 21, 26] and/or semantic cues [25, 27] and
later based on neural networks [13, 33]. Except for Lay-
outNet [33], most methods rely on leveraging existing tech-
niques for single perspective images on samples taken from
the input panorama. We believe that this is a major reason
of LayoutNet’s superior performance since it performs pre-
dictions on the panorama as a whole, thus extracting more
global information that the input panorama might contain.

3364

Figure 2. Our network architecture follows the encoder-decoder scheme and consists of two branches. Given a panorama in equirectan-
gular projection, we additionally create a perspective ceiling-view image through a equirectangular-to-perspective (E2P) conversion. The
panorama and the ceiling-view images are then fed to the panorama-view (upper) and ceiling-view (lower) branches. A E2P-based feature
fusion scheme is employed to connect the two branches, which are jointly trained by the network to predict: 1) probability maps of the
ﬂoor and ceiling in panorama view, 2) a ﬂoor plan in ceiling view, and 3) a layout height. Then, our system estimates a 2D ﬂoor plan
by ﬁtting a Manhattan-world aligned polygon to a weighted average of the three ﬂoor plans, which is further extruded using the predicted
layout height to obtain the ﬁnal 3D room layout.

A further step in this direction can be found in [21], in which
the input panorama is projected to a 2D ”ﬂoor” view in
which the camera position is mapped to the center of the
image and the vertical lines in the panorama become radial
lines emanated from the image center. An advantage of this
approach is that the room layout becomes a 2D closed loop
that can be extracted more easily. We derived our ”ceiling”
view idea here - instead of looking downward toward the
ﬂoor in which all the clutter in the room is included, we
look upward toward the ceiling and got a more clutter-free
view of the room layout.

3. Overview

Fig. 2 illustrates the overview of our framework. Given
the input as an equirectangular panoramic image, we follow
the same pre-processing step used in PanoContext [30] to
align the panoramic image with a global coordinate system,
i.e. we make a Manhattan world assumption. Then, we
transform the panoramic image into a perspective ceiling-
view image through an equirectangular to perspective (E2P)
conversion (Sec. 4). The panorama-view and ceiling-view
images are then fed to a network consisting of two encoder-
decoder branches. These two branches are connected via
a E2P-based feature fusion scheme and jointly trained to
predict a ﬂoor plan probability map, a ﬂoor-ceiling proba-
bility map, and the layout height (Sec. 5). Two intermediate
probability maps are derived from the ﬂoor-ceiling proba-
bility map using E2P conversion and combined with ﬂoor
plan probability map to obtain a fused ﬂoor plan probabil-

ity map. The ﬁnal 3D Manhattan layout is determined by
extruding a 2D Manhattan ﬂoor plan estimated on the fused
ﬂoor plan probability map using the predicted layout height
(Sec. 6).

4. E2P conversion

In this section, we explain the formulation of E2P con-
version that transforms an equirectangular panorama to a
perspective image. We assume the perspective image is
square with dimension w × w. For every pixel in the per-
spective image at position (px, py), we derive the position
of the corresponding pixel in the equirectangular panorama,
(p′
y ≤ 1, as follows. First,
we deﬁne the ﬁeld of view of the pinhole camera of the
perspective image as F oV . Then, the focal length can be
derived as:

x ≤ 1, −1 ≤ p′

y), −1 ≤ p′

x, p′

f = 0.5 ∗ w ∗ cot(0.5 ∗ FoV) .

(px, py, f ), the 3D position of the pixel in the perspec-
tive image in the camera space, is then rotated by 90◦ or
-90◦ along the x-axis (counter-clockwise) if the camera is
looking upward (looking at the ceiling) or downward (look-
ing at the ﬂoor), respectively.

Next, we project the rotated 3D position to the equirect-
angular space. To do so, we ﬁrst project it onto a unit sphere
by vector normalization, (sx, sy, sz), and apply the follow-
ing formula:

(p′

x, p′

y) = (

arctan2( sx
sz

)

π

,

arcsin(sy)

0.5π

),

(1)

3365

x, p′

to project (sx, sy, sz), the 3D position on the unit sphere,
back to (p′
y), the corresponding 2D position in the
equirectangular panorama. Finally, we use (p′
y) to in-
terpolate a pixel value from the panorama. We note that this
process is differentiable so it can be used in conjunction
with backpropagation.

x, p′

5. Network architecture

Our network architecture is illustrated in Fig. 2. It con-
sists of two encoder-decoder branches, for the panorama-
view and the ceiling-view input images. We denote the
panorama-view branch as BP and the ceiling-view branch
as BC . The encoder and decoder of BP are denoted as
EBP and DBP and for BC they are denoted as EBC and
DBC . A key concept is that our network predicts the ﬂoor
plan and the layout height. With these two predictions, we
can reconstruct a 3D room layout in a post-process (Sec. 6).

5.1. Encoder

We use ResNet-18 as the architecture for both EBP and
EBC . The input dimension of EBP is 512 × 1024 × 3 (the
dimension of the input panorama) and the output dimension
is 16×32×512. For EBC , the input and output dimensions
are 512 × 512 × 3 and 16 × 16 × 512. Note that the input of
EBC is a perspective ceiling-view image generated by ap-
plying E2P conversion to the input panorama with F oV set
to 160◦ and w set to 512. We also tried other more computa-
tionally expensive network architectures such as ResNet-50
for the encoders. However, we ﬁnd no improvements in ac-
curacy so we chose to work with ResNet-18 for simplicity.

5.2. Decoder

Both DBP and DBC consist of six convolutional layers.
The ﬁrst ﬁve layers are 3 × 3 resize convolutions [1] with
ReLU activations. The last layer is a regular 3 × 3 convo-
lution with sigmoid activation. The numbers of channels
of the six layers are 256, 128, 64, 32, 16, and 1. To infer
the layout height, we add three fully connected layers to the
middlemost feature of BP . The dimensions of the three
layers are 256, 64, and 1. To make the regression of the
layout height more robust, we add dropout layers after the
ﬁrst two layers. To take the middlemost feature as input, we
ﬁrst apply global average pooling along both x and y dimen-
sions, which produces an 1-D feature with 512 dimension,
and take it as the input of the fully connected layers.

The output of BP is a probability map of the ﬂoor and
the ceiling in the equirectangular projection, denoted as the
ﬂoor-ceiling probability map (MF C ). For BC , the output
is a probability map of the ﬂoor plan in the ceiling view,
denoted as the ﬂoor plan probability map (MF P ). Note that
BP also outputs a predicted layout height (H).

5.3. Feature fusion

We ﬁnd that applying fusion techniques to merge the fea-
tures in both BP and BC increases the prediction accuracy.
We conjecture a reason as follows. In a ceiling-view im-
age, the areas near the image boundary (where some useful
visual clues such as shadows and furniture arrangements ex-
ist) are more distorted, which can have a detrimental effect
for the ceiling-view branch to infer room structures. By
fusing features from the panorama-view branch (in which
distortion is less severe), performance of the ceiling-view
branch can be improved.

We apply fusions before each of the ﬁrst ﬁve layers of
DBP and DBC . For each fusion connection, a E2P con-
version (Sec. 4) with F oV set to 160◦ is taken to project the
features in DBP , which are originally in the equirectangu-
lar view, to the perspective ceiling view. Each fusion works
as follows:

f ∗
BC = fBC +

α
βi × fBP , i ∈ {0, 1, 2, 3, 4},

(2)

where fBC is the feature from BC and fBP is the feature
from BP after applying the E2P conversion. α and β are
the decay coefﬁcients. i is the index of the layer. After each
fusion, the merged feature, f ∗
BC , is sent into the next layer
of DBC . The performance improvement of this technique
is discussed in Sec. 8.

5.4. Loss function

For MF C and MF P , we apply binary cross entropy loss:

Eb(x, x∗) = − X

x∗
i log(xi) + (1 − x∗

i ) log(1 − xi).

i

For H (layout height), we use L1-loss:

EL1(x, x∗) = X

|xi − x∗

i |.

i

The overall loss function is:

(3)

(4)

L = Eb(MF C, M ∗

F C) + Eb(MF P , M ∗

F P ) + γEL1(H, H ∗),
(5)

where M ∗
MF P , and H.

F C , M ∗

F P and H ∗ are the ground truth of MF C ,

5.5. Training details

We implement our method with PyTorch[20]. We use the
Adam[11] optimizer with β1 = 0.9 and β2 = 0.999. The
learning rate is 0.0003 and batch size is 4. Our training loss
converges after about 120 epochs. For each training itera-
tion we augment the input panorama with random ﬂipping
and horizontal rotatations by 0◦, 90◦, 180◦, and 270◦. For

3366

Table 1. Statistics of the Realtor360 dataset.

4 corners

6 corners

8 corners

10+ corners Total

1246

950

316

61

2573

divide the bounding rectangle into several disjoint grid cells
(see Fig. 3 (c)). We deﬁne the shape of the 2D ﬂoor plan as
the union of grid cells where the ratio of ﬂoor plan area is
greater than 0.5 (see Fig. 3 (d)).

Figure 4. Few example panoramas in Realtor360. The annotated
3D room layouts are drawn as blue wireframes.

7. Realtor360 dataset

A dataset that contains a sufﬁcient number of 3D room
layouts with different numbers of corners is crucial for
training as well as testing our network. Unfortunately,
existing public domain datasets, such as the PanoCon-
text [30] dataset and the Stanford 2D-3D dataset labeled by
Zou et al. [33], contain mostly layouts with a simple cuboid
shape. To prove that our framework is ﬂexible enough to
deal with rooms with an arbitrary number of corners, we in-
troduce a new dataset, named Realtor360, that contains over
2500 indoor panoramas and annotated 3D room layouts. We
classify each room according to its layout complexity mea-
sured by the number of corners in the ﬂoor plan. Table 1
shows the statistics of the dataset and a few visual examples
can be found in Fig. 4. The source panoramic images in
the Realtor360 dataset are collected from two sources. The
ﬁrst one is a subset of the SUN360 dataset [24], which con-
tains 593 living rooms and bedrooms panoramas. The other
source is a real estate database with 1980 indoor panora-
mas acquired from a real-estate company. We annotate the
3D layouts of these indoor panoramas using a custom-made
interactive tool as explained below.

Annotation tool. To annotate the 2D indoor panoramas
with high-quality 3D room layouts, we developed an in-
teractive tool to facilitate the labeling process. The tool
ﬁrst leverages existing automatic methods to extract a depth
map [12] and line segments [30] from the input panorama.

3367

Figure 3. 2D ﬂoor plan ﬁtting. (a) The probability maps that our
network outputs are fused to a ﬂoor plan probability map M f use
F P .
(b) We apply image thresholding to M f use
F P and ﬁt a polygon shape
to the ﬂoor plan region. (c) The polygon edges are regressed and
clustered into two sets of horizontal lines (red) and vertical lines
(green). (d) The ﬁnal ﬂoor plane shape is deﬁned by grids in (c)
where the ratio of ﬂoor plan area is greater than 0.5.

fusion, we set α and β in Eqn. 2 to be 0.6 and 3. We set the
γ in Eqn. 5 to be 0.5. Because we estimate the ﬂoor plan
probability map in the ceiling view, we assume the distance
between the camera and the ceiling to be 1.6 meters, and
use this constant to normalize the ground truth.

6. 3D layout estimation

Given the probability maps (MF C and MF P ) and the
layout height (H) predicted by the network, we reconstruct
the ﬁnal 3D layout in the following two steps:

1. Estimating a 2D Manhattan ﬂoor plan shape using the

probability maps.

2. Extruding the ﬂoor plan shape along its normal accord-

ing to the layout height.

For step 1, two intermediate maps, denoted as M C
F C and
M F
F C , are derived from ceiling pixels and ﬂoor pixels of
the ﬂoor-ceiling probability map using the E2P conversion.
We further use a scaling factor, 1.6/(H − 1.6), to register
the M F
F C , where the constant 1.6 is the distance
between the camera and the ceiling. Finally, a fused ﬂoor
plan probability map is computed as follows:

F C with M C

M f use

F P = 0.5 ∗ MF P + 0.25 ∗ M C

F C + 0.25 ∗ M F

F C. (6)

F P

Fig. 3 (a) illustrates the above process. The probability map
M f use
is binarized using a threshold of 0.5. A bounding
rectangle of the largest connected component is computed
for later use. Next, we convert the binary image to a densely
sampled piece-wise linear closed loop and simplify it using
the Douglas-Peucker algorithm (see Fig. 3 (b)). We run a
regression analysis on the edges and cluster them into sets
of axis-aligned horizontal and vertical lines. These lines

Method

2D IoU (%)

3D IoU (%)

2D IoU (%)

3D IoU (%)

2D IoU (%)

3D IoU (%)

2D IoU (%)

3D IoU (%)

2D IoU (%)

3D IoU (%)

Average

4 corners

6 corners

8 corners

10+ corners

LayoutNet [33]
ours (fc-only)
ours (fp-only)

ours (w/o fusion)

ours (full)

65.84
75.2
75.75
78.52
80.53

62.77
72.02
72.18
74.8
77.2

80.41
76.75
79.66
81.77
82.63

76.6
73.27
75.54
77.57
78.91

60.5
76.04
75.42
78.5
80.72

57.87
73.06
72.23
75.1
77.79

41.16
70.8
70.51
73.61
78.12

38.61
67.89
67.39
70.37
74.86

22.35
56.42
51.03
57.01
63.1

21.52
54.2
48.57
54.12
59.72

Table 2. Quantitative evaluation on the Realtor360 dataset. We compare our method with the LayoutNet [33], and conduct an ablation
study using different conﬁgurations of our method. Bold numbers indicate the best performance.

Method

2D IoU (%)

3D IoU (%)

2D IoU (%)

3D IoU (%)

2D IoU (%)

3D IoU (%)

2D IoU (%)

3D IoU (%)

2D IoU (%)

3D IoU (%)

Average

4 corners

6 corners

8 corners

10+ corners

LayoutNet [33]

Ours (full)

71.31
77.87

67.91
74.16

80.69
82.42

76.82
78.3

68.95
77.19

65.83
73.74

50.31
70.81

47.23
67.55

44.53
54.05

42.51
50.96

Table 3. Quantitative evaluation on the subset of Realtor360 dataset. We compare with LayoutNet [33] using a training set that contains
only rooms with cuboid layout (4 corners). Bold numbers indicate the best performance.

Then, an initial 3D Manhattan-world layout is created by
sampling the depth along the horizontal line in the middle
of the panorama. The tool allows the users to reﬁne the ini-
tial 3D layout through a set of intuitive operations, including
(i) pushing/pulling a wall; (ii) merging multiple walls; and
(iii) splitting a wall. It also offers a handy function to snap
the layout edges to the estimated line segments during the
interactive editing to improve the accuracy.

8. Experiments

We compare our method to LayoutNet [33], a state-of-
the-art method in room layout estimation, through a se-
ries of quantitative and qualitative experiments on our Re-
altor360 dataset and the PanoContext [30] dataset. We also
conduct ablation study with several alternative conﬁgura-
tions of our method. We adopt 2D and 3D Intersection over
Union (IoU) to evaluate the accuracy of the estimated 2D
ﬂoor plans and 3D layouts, which is a standard metric in
similar tasks [3]. All the experiments used the same hyper-
parameter discussed in Sec. 5.5. Fig. 5 shows a few 3D
room layouts with different numbers of corners estimated
using our method. Please refer to the supplementary mate-
rials for more results in the following experiments.

Evaluation on the Realtor360 dataset. To train both
LayoutNet [33] and our DuLa-Net on the Realtor360
dataset, we randomly selected 2169 panoramas for training
and took the remaining 404 panoramas for testing. We fur-
ther classify the testing panoramas according to their num-
bers of corners. We run LayoutNet using the codes and de-
fault hyper-parameter released by the authors. The quan-
titative comparison with LayoutNet is shown in Table 2.
We observe that LayoutNet delivers good performance on
cuboid-shaped rooms (4 corners), similar to the numbers
reported in their paper. However, the accuracy drops signif-

icantly as the number of corners increases. In comparison,
Our DuLa-Net not only outperforms LayoutNet on cuboid-
shaped rooms by a small margin (around 2%), but also per-
forms well on rooms with larger numbers of corners. This
leads to an overall performance gain of ∼ 14% in both 2D
and 3D metrics when compared to LayoutNet.

Since the 3D layout optimization and the hyper-
parameter of LayoutNet were tuned on a dataset that con-
tains mostly cuboid-shaped rooms, we conducted another
experiment by training both networks on a revised training
set that excludes rooms of non-cuboid layouts, while keep-
ing the testing set untouched. Table 3 shows the quantitative
results. Note that while the performance of LayoutNet im-
proves, our method still outperforms on all kinds of rooms.
From the qualitative comparison shown in Fig. 6, we
can observe a strong tendency of LayoutNet to predict the
rooms to be cuboid-shaped, possibly due to the constraints
imposed in their 3D layout optimization.
In comparison,
our method simpliﬁes the problem by directly predicting a
Manhattan-world ﬂoor plan without any assumptions about
the numbers of corners. We conjecture that this is a main
reason why our method outperforms LayoutNet, especially
with rooms with more than four corners.

We also conducted an ablation study that evaluates the
performance of our method in different conﬁgurations as
follows: 1) ours(fc-only): only panorama-view branch, 2)
ours(fp-only): only ceiling-view branch, and 3) ours(w/o fu-
sion): our full model but without feature fusion. The quan-
titative results in Table 2 shows that jointly training both
branches leads to better performance than training only one
of them. In addition, adding feature fusion between the two
branches further improves the performance.

Evaluation on the PanoContext and Stanford 2D/3D
datasets. LayoutNet provided quantitative results on the
PanoContext [30] dataset with 414 panoramas for training

3368

Figure 5. Visual results. Given a single RGB panorama, our method automatically estimates the corresponding 3D room layout. Our
method is ﬂexible to handle more complex room layout beyond the simple cuboid room. The checkerboard patterns on the walls indicate
the missing textures due to occlusion.

and 53 panoramas for testing. All rooms are labeled as
cuboid-shape. To compare, we trained our network on the
same dataset. The quantitative comparison is shown in Ta-
ble 4. Our model outperforms LayoutNet by a small margin.

We also evaluate our model on the Stanford 2D-3D [2]
dataset with annotations labeled by LayoutNet [33]. The
dataset
includes 404 panoramas for training and 113
panoramas for testing. The last column in Table 4 shows
the quantitative result on the Stanford 2D-3D [2] dataset.

Table 4. Quantitative evaluation on the PanoContext [30] and
Stanford 2D/3D [2] datasets in 3D IoU (%)

Method

PanoContext

Stanford 2D-3D

LayoutNet [33]

Ours (full)

74.48
77.42

76.33
79.36

Timing. An end-to-end computation takes three main
steps - 1) an alignment process to align the input panorama
with a global coordinate system, 2) ﬂoor plan probabil-
ity map prediction by our neural network, and 3) 2D ﬂoor
plan ﬁtting. Step 1) is most time-consuming, which takes
about 13.37s measured on a machine with a single NVIDIA
1080ti GPU and Intel i7-7700 3.6GHZ CPU. Step 2) takes
only 34.68ms and step 3) takes only 21.71ms.

Compared to LayoutNet, they carry out the same align-
ment process and their neural network prediction is also
very fast (39ms). However, they needed another very time-
consuming 3D layout optimization step in the end, which
takes 30.5s.
In summary, an end-to-end computation by
LayoutNet takes about 43.9s while our method takes about
13.4s, a speed up of 3.28X.

9. Conclusion

We present an end-to-end deep learning framework,
called DuLa-Net, for estimating 3D room layouts from a

3369

Figure 6. Qualitative comparison with LayoutNet [33]. The 3D room layouts generated by LayoutNet[33] (green lines) and our method
(orange lines). Results are displayed on both the equirectangular panorama-view (left) and ﬂoor plan view (right), where the blue lines and
yellow solid shapes represent the ground truth, respectively.

single RGB panorama. We propose a new network architec-
ture that consists of two encoder-decoder branches for ana-
lyzing features from two distinct views of the input panora-
mas, namely the equirectangular panorama-view and the
perspective ceiling-view. The two branches are connected
through a novel feature fusion scheme and jointly trained
to achieve the best accuracy in the prediction of 2D ﬂoor
plan and layout height. To learn from complex layouts, we
introduce a new dataset, Realtor360, which contains 2573
indoor panoramas of Manhattan-world room layouts with
various complexity. Both the quantitative and qualitative
results demonstrate that our method outperforms the cur-

Figure 7. Limitations. Two failure cases generated by our method
(orange lines) due to the lack of object semantics.
(Top) Our
method is misled by the reﬂection of mirror. (Bottom) The bound-
ary of ﬂoor plan is occluded by the refrigerator. The ground truth
layout is rendered in blue.

rent state-of-the-art in prediction accuracy, especially with
rooms with more than four corners, and take much less time
to compute the ﬁnal 3D room layouts.

Limitations and future work. Our method has the fol-
lowing limitations:
i) without knowing the object se-
mantics, our network might get confused with the rooms
that contains mirrors or large occluding objects as shown
in Fig. 7; and ii) our approach of 3D layout estimation in-
volves heuristics and assumptions that might over- or under-
estimate the underlying ﬂoor plan probability map and also
restrain the results to Manhattan world. We propose to ex-
plore the following directions in the near future. First, in-
troducing the object semantics, i.e., segmentation and la-
bels, to the network architecture could potentially improve
the accuracy by ignoring those distracting and occluding
objects from the ﬂoor plan prediction. Second, designing
a principled algorithm for a more robust 3D layout esti-
mation, e.g., no Manhattan-world assumption and support
rooms with curve shapes. Last but not the least, we believe
that even better results can be achieved by experimenting
with a larger range of encoders for our network architec-
ture.

Acknowledgements. The project was funded in part by
the KAUST Ofﬁce of Sponsored Research (OSR) under
Award No. URF/1/3426-01-01, and the Ministry of Sci-
ence and Technology of Taiwan (107-2218-E-007-047- and
107-2221-E-007-088-MY3).

3370

References

[1] Andrew P. Aitken, Christian Ledig, Lucas Theis, Jose Ca-
ballero, Zehan Wang, and Wenzhe Shi. Checkerboard ar-
tifact free sub-pixel convolution: A note on sub-pixel con-
volution, resize convolution and convolution resize. CoRR,
abs/1707.02937, 2017. 4

[2] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2D-
3D-Semantic Data for Indoor Scene Understanding. ArXiv
e-prints, Feb. 2017. 7

[3] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV, 2018. 6

[4] James M. Coughlan and A. L. Yuille. Manhattan world:
Compass direction from a single image by bayesian infer-
ence. pages 941–, 1999. 2

[5] S. Dasgupta, K. Fang, K. Chen, and S. Savarese. Delay:
Robust spatial layout estimation for cluttered indoor scenes.
In 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 616–624, June 2016. 2

[6] Kosuke Fukano, Yoshihiko Mochizuki, Satoshi Iizuka,
Edgar Simo-Serra, Akihiro Sugimoto, and Hiroshi Ishikawa.
Room reconstruction from a single spherical
image by
higher-order energy minimization. 2016 23rd International
Conference on Pattern Recognition (ICPR), pages 1768–
1773, 2016. 2

[7] Abhinav Gupta, Martial Hebert, Takeo Kanade, and
David M. Blei. Estimating spatial layout of rooms using vol-
umetric reasoning about objects and surfaces. In J. D. Laf-
ferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A.
Culotta, editors, Advances in Neural Information Process-
ing Systems 23, pages 1288–1296. Curran Associates, Inc.,
2010. 2

[8] V. Hedau, D. Hoiem, and D. Forsyth. Recovering the spa-
tial layout of cluttered rooms. In 2009 IEEE 12th Interna-
tional Conference on Computer Vision, pages 1849–1856,
Sept 2009. 2

[9] D. Hoiem, A. A. Efros, and M. Hebert. Geometric context
from a single image. In Tenth IEEE International Conference
on Computer Vision (ICCV’05) Volume 1, volume 1, pages
654–661 Vol. 1, Oct 2005. 2

[10] Derek Hoiem, Alexei A. Efros, and Martial Hebert. Recov-
ering surface layout from an image. International Journal of
Computer Vision, 75(1):151–172, Oct 2007. 2

[11] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. CoRR, abs/1412.6980, 2014. 4

[12] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks.
In 3DV, pages
239–248. IEEE Computer Society, 2016. 5

[13] Chen-Yu Lee, Vijay Badrinarayanan, Tomasz Malisiewicz,
and Andrew Rabinovich. Roomnet: End-to-end room layout
estimation. CoRR, abs/1703.06241, 2017. 2

[14] D. C. Lee, M. Hebert, and T. Kanade. Geometric reason-
ing for single image structure recovery. In 2009 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2136–2143, June 2009. 2

[15] C. Liu, P. Kohli, and Y. Furukawa. Layered scene decom-
position via the occlusion-crf.
In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
165–173, June 2016. 2

[16] Chen Liu, Jiaye Wu, and Yasutaka Furukawa. Floornet:
A uniﬁed framework for ﬂoorplan reconstruction from 3d
scans. European Conference on Computer Vision (ECCV),
2018, 2018. 2

[17] Arun Mallya and Svetlana Lazebnik. Learning informative
edge maps for indoor scene layout prediction. In Proceed-
ings of the 2015 IEEE International Conference on Com-
puter Vision (ICCV), ICCV ’15, pages 936–944, Washing-
ton, DC, USA, 2015. IEEE Computer Society. 2

[18] Aron Monszpart, Nicolas Mellado, Gabriel J. Brostow, and
Niloy J. Mitra.
Rapter: Rebuilding man-made scenes
with regular arrangements of planes. ACM Trans. Graph.,
34(4):103:1–103:12, July 2015. 2

[19] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D.
Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A.
Fitzgibbon. Kinectfusion: Real-time dense surface mapping
and tracking. In 2011 10th IEEE International Symposium
on Mixed and Augmented Reality, pages 127–136, Oct 2011.
2

[20] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS-W, 2017. 4

[21] G. Pintore, V. Garro, F. Ganovelli, E. Gobbetti, and M. Agus.
Omnidirectional image capture on mobile devices for fast au-
tomatic generation of 2.5d indoor maps. In 2016 IEEE Win-
ter Conference on Applications of Computer Vision (WACV),
pages 1–9, March 2016. 2, 3

[22] Srikumar Ramalingam and Matthew Brand. Lifting 3d man-
hattan lines from a single image. 2013 IEEE International
Conference on Computer Vision, pages 497–504, 2013. 2

[23] S. Ramalingam, J. K. Pillai, A. Jain, and Y. Taguchi. Manhat-
tan junction catalogue for spatial reasoning of indoor scenes.
In 2013 IEEE Conference on Computer Vision and Pattern
Recognition, pages 3065–3072, June 2013. 2

[24] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba. Recogniz-
ing scene viewpoint using panoramic place representation.
In 2012 IEEE Conference on Computer Vision and Pattern
Recognition, pages 2695–2702, June 2012. 2, 5

[25] J. Xu, B. Stenger, T. Kerola, and T. Tung. Pano2cad: Room
layout from a single panorama image. In 2017 IEEE Win-
ter Conference on Applications of Computer Vision (WACV),
pages 354–362, March 2017. 2

[26] H. Yang and H. Zhang. Efﬁcient 3d room shape recovery
from a single panorama. In 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 5422–
5430, June 2016. 2

[27] Yang Yang, Shi Jin, Ruiyang Liu, Sing Bing Kang, and
Jingyi Yu. Automatic 3d indoor scene modeling from sin-
gle panorama. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018. 2

[28] J. Zhang, C. Kan, A. G. Schwing, and R. Urtasun. Estimating
the 3d layout of indoor scenes and its clutter from depth sen-

3371

sors. In 2013 IEEE International Conference on Computer
Vision, pages 1273–1280, Dec 2013. 2

[29] Yinda Zhang, Mingru Bai, Pushmeet Kohli, Shahram Izadi,
and Jianxiong Xiao. Deepcontext: Context-encoding neural
pathways for 3d holistic scene understanding. International
Conference on Computer Vision (ICCV 2017), 2017. 2

[30] Yinda Zhang, Shuran Song, Ping Tan, and Jianxiong Xiao.
Panocontext: A whole-room 3d context model for panoramic
scene understanding.
In Computer Vision - ECCV 2014 -
13th European Conference, Zurich, Switzerland, September
6-12, 2014, Proceedings, Part VI, pages 668–686, 2014. 2,
3, 5, 6, 7

[31] Hao Zhao, Ming Lu, Anbang Yao, Yiwen Guo, Yurong
Chen, and Li Zhang. Physics inspired optimization on se-
mantic transfer features: An alternative method for room lay-
out estimation. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017. 2

[32] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas,
and Petros Daras. Omnidepth: Dense depth estimation for
indoors spherical panoramas. In The European Conference
on Computer Vision (ECCV), September 2018. 2

[33] Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem.
Layoutnet: Reconstructing the 3d room layout from a single
rgb image. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018. 1, 2, 5, 6, 7, 8

3372

