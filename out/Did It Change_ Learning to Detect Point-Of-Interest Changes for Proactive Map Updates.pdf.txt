Learning to Detect Point-of-Interest Changes for Proactive Map Updates

Did it change?

J´erˆome Revaud† Minhyeok Heo§ Rafael S. Rezende† Chanmi You§ Seong-Gyun Jeong§

†NAVER LABS Europe

§NAVER LABS

Abstract

Maps are an increasingly important tool in our daily
lives, yet their rich semantic content still largely depends
on manual input. Motivated by the broad availability of
geo-tagged street-view images, we propose a new task aim-
ing to make the map update process more proactive. We
focus on automatically detecting changes of Points of Inter-
est (POIs), speciﬁcally stores or shops of any kind, based on
visual input. Faced with the lack of an appropriate bench-
mark, we build and release a large dataset, captured in two
large shopping centers, that comprises 33K geo-localized
images and 578 POIs. We then design a generic approach
that compares two image sets captured in the same venue
at different times and outputs POI changes as a ranked list
of map locations. In contrast to logo or franchise recogni-
tion approaches, our system does not depend on an external
franchise database.
It is instead inspired by recent deep
metric learning approaches that learn a similarity function
ﬁt to the task at hand. We compare various loss functions to
learn a metric aligned with the POI change detection goal,
and report promising results.

1. Introduction

Maps have become a useful companion in our daily lives,
as they provide a convenient and searchable representation
of our physical world. To achieve map usability, raw geo-
graphical data is manually enriched with associated seman-
tic information [18, 38]. This way, maps can be queried and
explored in an effective and user-friendly manner.

Points of interest (POIs), i.e. well-localized landmarks
that one may ﬁnd useful or interesting, typically constitute
an important part of this semantic content. POIs can be
shops or stores of all kinds, including restaurants, caf´es,
banks, and so forth. Currently, most of this content is
gathered and entered mostly by hand in the map database
[18, 27, 41], but this process is tedious and expensive. This
is problematic as map semantic content is alive by deﬁnition
and, thus, subject to frequent changes. In addition, out-of-

Figure 1. Overview of the proposed system. We detect changes
of Points-of-Interest (POIs) as a ﬁrst step towards the long-term
goal of fully-automated map updates. To that purpose, we compare
two sets of images captured at different moments.
Image pairs
corresponding to the same physical location are ﬁrst formed and
then compared using specially trained embeddings extracted by a
deep network. The comparison is simply carried out as an inner-
product between the two ℓ2-normalized embeddings.

date POI information can be a source of accidents and user
frustration [8, 37].

In this paper, we ask the following question: Could the
map maintenance burden be alleviated by leveraging recent
advances in computer vision? We notice that, much like hu-
mans, who unconsciously build a mental map of their sur-
rounding relying heavily on their ocular sense, machines
should also be able to perform semantic mapping-related

4086

(a) POI appearance

(b) POI replacement

(c) POI disappearance

(d) no change

Figure 2. The different cases of POI changes. The ﬁrst three columns show the cases we wish to detect. The last case is not a POI change,
but is challenging due to the lack of clearly identiﬁable brand or logo and the strong viewpoint and lighting changes.

tasks from visual input. We make one step towards the am-
bitious goal of fully-automated map maintenance and tackle
the detection of POI changes based on spatio-temporally
localized scene photographs. This task more speciﬁcally
consists in notifying a database operator whenever a POI is
changed, which happens when a new POI appears, a POI is
replaced by another one or a POI disappears, see Fig. 2.a–c.
It implies comparing two sets of images captured at dis-
tinct moments and is challenging for several reasons. First,
the comparison must be robust to various sources of noise,
e.g. lighting, reﬂection, shadows, occlusions and viewpoint
changes. Image poses and scaling can dramatically differ
between the two captures, as exempliﬁed in Fig. 2.d. Sec-
ond, substantial intrinsic variations of POI appearance hap-
pen over time due to seasonal changes, special sales, etc.
Third, the system is expected to produce an output aligned
with the ﬁnal goal, i.e. a list of geographical POI spots likely
to have changed, which requires a novel framework.

This paper is also motivated by the recent explosion of
initiatives to capture photos spanning all areas of the real-
world: nowadays, online map services offer immersive ex-
periences regarding street views [3]. Given the existence
and availability of this data [4, 48], it is somewhat surpris-
ing that no work, to the best of our knowledge, has yet fo-
cused on this important aspect of map content update. One
potential reason for this absence is the lack of an appro-
priate benchmark. Although new datasets related to urban
localization [4, 5, 48], place attributes [54–56] and land-
marks [7,21,29–31] have recently emerged, information re-
lated to both POIs and time-stamps is unavailable.

As a ﬁrst contribution, we provide a new and challeng-
ing annotated dataset to bring this interesting task to the
attention of researchers. The dataset is composed of thou-
sands of photographs, captured in two large shopping malls,
each comprising hundreds of POIs. By focusing on in-
door images, which are simpler to analyze and parse than

outdoor scenes, we isolate the problem at hand from other
out-of-scope challenges. Since change detection assumes
temporality, photographs are divided into two groups cap-
tured at different time-stamps separated by several months.
We also provide the precise geo-localization of each photo,
estimated at acquisition time using a LIDAR. Overall, our
dataset comprises 578 POI instances and more than 33K
images with relative 6-DoF camera pose information. More
importantly, we have annotated POI changes at two differ-
ent levels: at the image-level, but also at a geographical
level.

Our second contribution is a novel and generic approach
for POI change detection. Our system is based on three key
stages: (1) temporally distant images are matched to form
pairs if their poses overlap; (2) for each pair, the images are
compared to detect POI changes; (3) pairwise predictions
are aggregated at the scale of the entire venue. The pro-
posed system only makes loose assumptions on the quality,
content and exhaustiveness of the input photographs. Re-
markably, it is able to detect changes even in the absence of
clearly identiﬁable logos or signage.

Our third contribution is a benchmark of several alterna-
tive techniques for the image pair comparison step (second
stage above). These range from keypoint-matching with ge-
ometric veriﬁcation to state-of-the-art deep learning tech-
niques for metric learning, see Fig. 1. The method that
works best, based on deep embedding trained with a triplet
loss, achieves promising results.

The rest of the paper is organized as follows. We dis-
cuss related work in Section 2 and present our new dataset,
coined MallScape, in Section 3. We describe the proposed
overall approach for POI change detection in Section 4 and
benchmarks different methods and options in Section 5.

4087

2. Related Work

Change detection in images
has been a long topic of
interest [35] in multiple domains such as medical imag-
ing [28], remote sensing [11, 12], camera surveillance [15,
23, 53] or aerial image analysis [2, 9, 22, 42]. In the medical
ﬁeld, change detection has mainly been seen as a way of
monitoring the appearance of tumors or other anomalies in
X-ray or MRI images. To that aim, several images taken at
different moments are ﬁrst carefully aligned and then pixel-
wise compared [28]. Similar problems arise in the con-
text of aerial images [2, 9, 42] and remote sensing [11, 12],
where the goal is again to observe the evolution of speciﬁc
regions or constructions between images precisely aligned
beforehand. These approaches rely on speciﬁc constraints
and conditions and are not easily generalizable to other
cases [15].

Closer to our problem,

the task of detecting struc-
tural changes in outdoor urban scenes has recently devel-
opped [2, 9, 16, 22, 42]. Similar to our case, these meth-
ods feed on geo-localized photographs, typically captured
by vehicle-mounted cameras equipped with GPS tracking
devices. By comparing pairs of pictures of the same lo-
cation captured at different moments, they aim to predict
a binary pixel mask indicating structural changes, e.g. dis-
placed objects or road works. For successful predictions,
image pairs must ﬁrst undergo an accurate aligment pro-
cedure using complex and error-prone 3D reconstruction
techniques [2, 6]. In this work, we do not make assump-
tions on the precise alignment of images. More impor-
tantly, these methods are blind to the nature of the change
and it is unclear if they would detect a POI change, which
does not necessarily involves any structural changes (e.g.
see Fig. 2.b). On the contrary they would incorrectly detect
structural changes due to periodical store-front rearrange-
ments or special events like Christmas. Finally, these meth-
ods are optimizing pixel-level metrics that are not suited
to the goal of map updates. We detect and aggregate POI
changes at a geographical scale inside entire venues.

Logo and franchise detection. Another category of work
has focused on recognizing POIs, or rather what makes
them identiﬁable: logos or representative symbols of their
brands. Deep approaches for logo detection [44, 45] have
recently outperformed former hand-crafted approaches [39,
40] and larger datasets have been collected by fetching im-
ages on social media such as Flickr and Twitter [49]. While
it is theoretically possible to detect POI changes based on
tracking brand logos or signage over time, it is subject to
two major issues: (i) it implies that an exhaustive database
of logos and brands is available and up-to-date, which is
unrealistic given that new brands appear every day, and
(ii) it overlooks the fact that in practice, due to occlusions,
changes in viewpoint and image framing, logos and sig-

nages are often absent (e.g. see Fig. 2.d). Moreover many
POIs do not belong to any franchise. In this work, we pro-
pose a framework that makes none of the two above as-
sumptions and is still able to accurately detect POI changes.

Image retrieval and metric learning. Our approach is
inspired by recent progress in image retrieval.
In partic-
ular, recognizing places despite appearance and illumina-
tion changes over time and seasons is typically cast as an
image retrieval problem [4, 5, 48]. Image retrieval aims to
deﬁne a distance measure between images so that, given
an image query, similar images can be retrieved from a
large collection [21, 29, 31].
Ideally, this distance metric
should be invariant to semantically meaningless variations
induced by lighting or viewpoint changes. Recent works
have shown that this metric can be learned with supervi-
sion using deep Siamese networks [25]. In fact, deep metric
learning has proven overwhelmingly effective for image re-
trieval [14, 34], person re-identiﬁcation [51], ﬁne-grained
image classiﬁcation [52], 3D object retrieval [20] and place
recognition [5]. We follow this example and learn, using
minimal supervision, a distance metric that ﬁts POI change
detection.

In more detail, deep metric learning consists of learn-
ing an embedding function that project images in a space
where Euclidean distance is an acurate measure of their se-
mantic similarity. Many variants of the objective loss func-
tion have been developed, e.g. the contrastive [17], double-
margin [24], triplet [43], and quadruplet [10] losses. Each
has its own speciﬁcity (see a review in [25, 50]) but they
have the common goal of pushing apart points belonging to
different classes in the embedding space while at the same
time attracting points having the same label. We experiment
with several of these techniques for our problem.

3. The MallScape dataset

In this section, we describe a new dataset speciﬁcally tai-
lored to the task of POI change detection. It is composed of
indoor scene photographs captured in real shopping centers.
Each image comes with a precise 6 degrees-of-freedom (6-
DoF) localization pose obtained by LIDAR. To observe real
exemplars of POI changes, two distinct acquisition sessions
separated by several months have been conducted. The
dataset can be downloaded at [1].

Acquisition scenario. Our mapping device sweeps the
venue and captures photos densely enough so that every
portion of the wall appears at least in one photo in close-up,
without any other special contraints. In order to ensure that
POIs are well visible even when the device is very close to
them, we mount cameras such that they pitch slightly up-
wards. This lazy acquisition scenario makes the capture
system easier to implement and more scalable in realistic

4088

Table 1. Summary of MallScape-A and MallScape-B datasets.

Dataset

Floor

# of images
# of POI
# of changes

MallScape-A

MallScape-B

Total

B1
696
13
0

1F

5640
106

6

2F

3F

5736

3912

87
6

73
4

4F
156

2
0

B1

17531

297

6

-

33671

578
22

(a) MallScape-A

(b) MallScape-B

Figure 3. Parts of the ﬂoor maps corresponding to the ﬁrst ﬂoor
of MallScape-A (a) and the retail areas of MallScape-B (b). Each
POI is represented by a distinct color.

conditions. We now describe the two large shopping cen-
ters that have served for the acquisition campaign.

MallScape-A is a ﬁve-story building comprising 281
POIs, where the total retail ﬂoor area is about 460, 000 m2.
We captured data twice with a 6-month time gap and took
360-degree panoramic images every 7 meters. Images were
then mapped back to standard rectilinear camera lenses with
12 equidistant horizontal viewpoints in portrait mode, each
having horizontal and vertical ﬁelds of view of 70 and 85
degrees, respectively.

MallScape-B covers about 144, 000 m2, and contains 297
POIs on a single underground ﬂoor. About 17K pictures
were captured during two sessions separated by 3 months.
Unlike the MallScape-A dataset, we used standard cameras
equipped with ﬁsh-eye lenses, resulting in photographs in
landscape mode with horizontal and vertical ﬁelds of view
of respectively 107 and 70 degrees.

After the collection, images were carefully reviewed and
annotated by semi-automatic methods in order to label the
POI visible in each image as well as record POI changes.
We provide with the dataset the labels of the POIs appear-
ing in each image. Overall, the dataset contains a total of
578 POIs and 22 POI changes. Details about the number
of images, number of POIs and number of POI changes are
summarized in Table 1. Excerpts of the ﬂoor maps can be
seen in Fig. 3, each color denoting a different POI.

Some image pairs showing the same POI at two different
moments are exempliﬁed in Fig. 5. Statistics computed over
the camera pose for all such pairs are displayed in Fig. 4. On
MallScape-A, the average distance between the two cameras
is 4 meters, while the average intersection-over-union of the
visual content is 0.66 (computation of these metrics is de-
tailed later in Section 4.2). This shows that image pairs have
substantially different viewpoints overall.

5.5
4.4
3.3
2.2
1.1

)
3
0
1
(
 
s
r
i
a
p

 
f
o
#

 

0.5

0.6

0.7

3.0
2.4
1.8
1.2
0.6

)
3
0
1
(
 
s
r
i
a
p

 
f
o
#

 

IoU

100
90
80
70
60

)
3
0
1
(
 
s
r
i
a
p

 
f
o
#

 

0.8

0.9

1.0

0.5

0.6

0.7

120
96
72
48
24

)
3
0
1
(
 
s
r
i
a
p

 
f
o
#

 

0.8

0.9

1.0

IoU

0

1

2

3

5

4
6
distance (m)

7

8

9 10

0

1

2

3

5

4
6
distance (m)

7

8

9 10

(a) MallScape-A

(b) MallScape-B

Figure 4. Statistics on positive image pairs for both shopping cen-
ters MallScape-Aand MallScape-B. Positive pairs, formally de-
ﬁned in Eq. (7), are the ones showing the same part of the same
POI. We present histograms of the geometric overlap (computed
according to Eq. (5)) between these pairs and their geometric dis-
tances (in meters). Cameras can be up to 10 meters away, resulting
in signiﬁcant viewpoint difference.

(a)

(b)

(c)

(d)

Figure 5. Examples of matching images pairs (each column shows
the same place seen from different viewpoints and moments). Our
proposed deep metric learning-based approach can accomodate
images showing a single POI (a), multiple POIs (b), and no POI at
all (c). We show in column (d) an example of dramatic appearance
variations due to advertisement.

4. POI Change Detection

We describe in this section the problem that we want to
solve. We then propose a generic metric learning approach
to solve it, and discuss different options for training the cen-
tral pairwise image similarity.

4.1. Problem formulation

We are interested in automatically determining, for each
location within a certain area, if a POI at this location has
changed or not over a period of time. Let Dt denote a
dataset of geo-localized images captured at time t1, i.e.

1for simplicity, we assume simultaneity of the capture.

4089

i , Θt

i)}i where I t

i is an image and Θt

Dt = {(I t
i its asso-
ciated 6-DoF camera pose. We further assume the existence
of a second dataset Dt′
captured at a different time t′ > t.
Note that we do not make any assumption on the correspon-
dences between images and poses between Dt and Dt′
ex-
cept that both image sets are captured in the same area.

Our ﬁrst goal is to learn a function ς that predicts the

similarity between two localized images:

ς : (I t

i , Θt

i) × (I t′

j , Θt′

j ) 7→ [0, 1].

(1)

We design ς such that the similarity is high when the two
images show the same POI, and low otherwise. We inves-
tigate various ways of accomplishing this goal in the next
sections.

Eventually, we want to ﬁnd all modiﬁcations or alter-
ations of POIs in the target area, regardless of the rea-
son. Formally, our ﬁnal goal is to score each potential
POI location with a corresponding change likelihood. Let
g : P 7→ [0, 1] denote such POI change scoring function,
where P ⊂ R3 denotes the coordinate space of all locations
addressed by latitude, longitude and elevation. In practice,
we implement g(·) straightforwardly by max-pooling the
pairwise similarity scores output by ς(·):

Figure 6. The knowledge of the ﬂoor map combined with simple
ray-casting technique allows us to compute the wall points U (Θ)
visible from camera pose Θ.

a potential POI location p (or more accurately, a potential
store-front facade), and their union constitutes U (Θ). The
unions of the visible facades from all viewpoints deﬁnes the
set of all potential POI locations:

U (Θi).

P =[i

(3)

Conversely, the set of all camera poses seeing point p is
stored as its visibility set V(p) = {Θi|p ∈ U(Θi)}.

To simplify the similarity function ς (Eq. (1)), we before-

hand exclude all image pairs having inconsistent poses:

g(p) = 1 −

max

i ∈ V t(p)
Θt
Θt′
j ∈ V t′
(p)

ς(cid:16)(I t

i , Θt

i), (I t′

j , Θt′

j )(cid:17) ,

(2)

ς((I t

i , Θt

i), (I t′

j , Θt′

j )) =(s(I t

0

i , I t′
j )

if is valid(Θi, Θj),
otherwise,

where V t(p) is the visibility set of p, i.e. the set of image
poses {Θt
i}i from which one can directly see location p (and
likewise for V t′
). Otherwise stated, function g(p) predicts a
POI change if not a single pair of images showing location
p has high similarity.

The ﬁnal decision of whether or not a given map spot p
has undergone a change of POI is made by comparing g(p)
with a threshold τ . In practice, we beforehand smooth g(·)
using a Gaussian kernel of spatial radius σ = 2 meters to
remove noise. In our experiments, we evaluate the accuracy
of g(·) after thresholding as well as the average precision of
the spots ranked by g(·) (see Section 5.2).

4.2. Visibilty sets and pose based constraints

The formulation above assumes that we can compute the
set of locations visible for each image pose. We achieve this
by leveraging the ﬂoor map M that speciﬁes the positions
of all walls. Speciﬁcally, we use a ray-casting technique
to compute the set of wall points that are visible from each
camera pose. The process is illustrated in Fig. 6. Each ray
starts from the camera center at P (Θ) ∈ R3, passes through
the camera lens and continues until it hits a wall. The set of
rays form a conical region of width corresponding to the
ﬁeld of view of the camera. Each hit point is labeled as

(4)
where s is a trained similarity function purely relying on
visual input and deﬁned in Section 4.3. An image pair
is deemed consistent, or valid, if its overlap is substan-
tial and if both images are close enough to a wall, as im-
ages far from store-fronts are generally poorly informative.
First, the overlap between two images is computed as the
intersection-over-union of their respective visibility sets:

O(Θi, Θj) =

|U (Θi) ∩ U (Θj)|
|U (Θi) ∪ U (Θj)|

,

(5)

Then, the average wall distance for a given image is com-
puted based on U (Θ) as D(Θ) =
P (Θ)k (in practice, we use 10 meters as distance thresh-
old). These two conditions practically remove many irrel-
evant pairs in Eq. (2) and considerably speed up the infer-
ence.

|U (Θ)|Pp∈U (Θ) kp −

1

4.3. Learning a similarity function

The goal of metric learning is to learn a similarity mea-
sure between images under some supervision. It has been
applied successfully to various ﬁelds, such as image re-
trieval [14,32] and person re-identiﬁcation [10,51]. In prac-
tice, it is often formulated as the task of learning an image

4090

embedding function f (I) = x ∈ X , with X ⊂ RN an ℓ2-
normalized embedding space of dimension N . Similarity s
from Eq. (4) is computed as the inner product between two
embeddings, i.e. we have

s(Ii, Ij) = max(cid:0)0, f (Ii)⊤f (Ij)(cid:1) .

(6)

We can now learn an embedding function that suits
our deﬁnition of similarity.
let
yi = {o1 . . . omi } and yj = {o1 . . . omj } denote the set of
POIs visible in image Ii and Ij . We deﬁne the ground-truth
similarity Y (i, j) as follows:

More speciﬁcally,

Y (i, j) =(1

0

if O(Θi, Θj) > 0.4 and |yi ∩ yj| > 0,
otherwise,

(7)
i.e. two images are not similar unless they show at least one
common part of the same POI. Fig. 5 shows several exam-
ples of matching (i.e. positive) image pairs.

Loss function. We use a deep network to learn function
f (·), trained with stochastic gradient descent. Gradients are
computed at each iteration according to the agreement be-
tween the ground-truth Y and the current similarity, com-
puted by Eq. (6), thanks to an appropriate loss function.

For instance, the contrastive loss [17, 33] denoted by Lc
attracts positive pairs (Y (i, j) = 1) to each other while
pushing negative pairs (Y (i, j) = 0) apart:

Lc(i, j) = Y (i, j) (1 − s(Ii, Ij))

+ (1 − Y (i, j)) max (0, s(Ii, Ij) − τc) ,

(8)

where τc is a similarity threshold between negative pairs
below which the loss has no effect.

Ideally, we want to discriminate between changes and
non-changes by thresholding g(·) with τ . The most direct
way to achieve that is by penalizing any training image pairs
that deviates from this behavior. This corresponds to the
double-margin pairwise loss Ldm proposed in [24]:

Ldm(i, j) = Y (i, j) max(cid:16)0, (cid:16)τ +

+ (1 − Y (i, j)) max(cid:16)0, s(Ii, Ij) −(cid:16)τ −

m

2 (cid:17) − s(Ii, Ij)(cid:17)
2 (cid:17)(cid:17) .

m

(9)

where m is a tunable margin. Alternatively, we can use the
contrastive loss Lc with τc = τ − m
2 .

Finally,

another popular

loss is based on image

triplets [43]:

Lt(i, j, k) = max (0, m − s(Ii, Ij) + s(Ii, Ik)) ,

with Y (i, j) = 1 and Y (i, k) = 0.

(10)

This loss has been shown to be easier to train because it
only enforces the relative ordering of the positive (i, j) and
negative (i, k) similarities [25, 50], whereas previous for-
mulations enforce an absolute similarity threshold. While
there is no guarrantee that the triplet loss will preserve a
global threshold τ suitable for g(·), in practice we observe
good performance.

The procedure for training is to repeatedly sample ran-
dom pairs or triplets of images (depending on the loss), and
to compute the loss for each. If the loss is non-zero, then the
loss gradient is computed and serves to update the network
weights. In practice, we sample as many positive pairs as
negative ones to balance the training.

5. Experimental results

5.1. Protocol and metrics

We now introduce evaluation metrics and experimental
protocols tailored to our problem and data. We perform the
evaluation at two different levels: on image pairs (interme-
diate goal in Section 4.1) and at the geographical level (our
ﬁnal goal).

Ground-truth. The quality of the similarity measure on
image pairs is evaluated with respect to the ground-truth
Y (i, j) ∈ {0, 1} deﬁned in Eq. (7). For the geographi-
cal level, we annotate the set of potential POI locations P
(Eq. (3)) as follows: each location p ∈ P belonging to a
true POI change is marked as positive, and negative oth-
erwise. For convenience we abuse notations and denote it
as Y (p) ∈ {0, 1} in the following. We now introduce the
different metrics.

ROC curve. Detecting POI changes can be seen as a bi-
nary classiﬁcation task: change (Y (p) = 1) vs. no change
(Y (p) = 0). The ROC curve allows to measure the overall
performance of a binary classiﬁer at multiple thresholds. It
is produced by computing the true and false positive rates
(TPR and FPR, respectively) for all thresholds:

TPR(τ ) = Pp

I[g(p) ≥ τ ]

Pp Y (p)

, FPR(τ ) = Pp

I[g(p) < τ ]

Pp 1 − Y (p)

We also report the area under the curve (AUC). We compute
these metrics both for image pairs (denoted as pROC and
pAUC) and for geographic locations (denoted as gROC and
gAUC).

Average Precision (AP). Our ﬁnal goal can also be for-
mulated as globally ranking all map locations in terms of
POI change likelihood. We measure the system perfor-
mance in terms of ranking using the AP. The AP computed
for image pairs is denoted as pAP and the geographic-level
AP is denoted as gAP in the following.

4091

Train and test splits. MallScape consists of two sub
datasets captured in different venues. Despite the large
number of dataset images, there are relatively few instances
of POI changes. This can increase the variance of the noise
in the evaluation metrics and negatively affect the evalua-
tion. To smooth the performance, we therefore train and
test on one and the other venue in turns and ﬁnally average
the results:

• Split 1: train on MallScape-A and test on MallScape-B;

• Split 2: train on MallScape-B and test on MallScape-A

Since the styles of the two venues are signiﬁcantly different,
these splits also ensures that good performance is not due to
training set overﬁtting.

5.2. Quantitative results

Implementation details All our models are formed by a
ResNet-101 [19] backbone to which we append a global
generalized-mean pooling layer [33]. The embedding di-
mension is N = 2048. After some preliminar experiments,
we decide to use a weight decay of 0 and a learning rate
of 10−5, decreased by 2 every 2000 iterations. We em-
ploy standard data augmentation during training to increase
generalization performance [19]. To improve training speed
and test performance, we follow standard practice for pre-
training and hard-negative mining [14,32,34]. We also tune
the margin m and threshold τc parameters separately for
each loss.

Results. We study the performance of the different loss
functions presented in Section 4.3. We also compare to
three baseline approaches. The ﬁrst one relies on keypoint-
matching followed by geometric veriﬁcation [26] to com-
pute the similarity s from Eq. (4) using a sigmoid. The
second one is based on a state-of-the-art logo detector [46]
able to recognize 352 common logos in real-world images.
In this case, images are thus compared in terms of detected
brands. Lastly, we also include the performance of embed-
dings extracted from the last convolutional layer of a net-
work pretrained on ImageNet [47] for reference.

Results for all methods with all metrics are presented in
Table 2.
It is clear that off-the-shelf features trained on
ImageNet produce poor embeddings, highlighting the im-
portance of learning the metric. Similarly, SIFT-based fea-
tures are unable to analyze the complex semantic changes
involved in the POI detection task. In contrast, the logo-
based baseline speciﬁcally focuses on semantic aspects that
are key to the task, yet, it performs more poorly than the Im-
ageNet baseline. After inspection, we ﬁnd that many POI
images do not contain any logos for which the detector was
trained. These 352 known logos form, after all, a rather
small part of all logos potentially appearing in real POI im-
ages. We therefore believe that approaches explicitly recog-
nizing brands are rather impractical, as noted in Section 2.

(a) Split 1

(b) Split 2

Figure 7. ROC curves for all methods on both splits at the geo-
graphic level (gROC).

Unsurprisingly, the three methods based on metric learn-
ing (ML) perform similarly in terms of average pAUC
and gAUC. However the quality of the ranking of POI
scores (gAP) is signiﬁcantly better for the triplet loss Lt.
This is in agreement with previous ﬁndings that the triplet
loss better behaves during training in general than pair-
wise losses [14, 50]. Interestingly, we note that image-level
metrics can be in complete disagreement with geograph-
ical metrics. This is for instance the case on the second
split where ImageNet features yield, at the same time, the
best pAP score and the worse gAP score by far among
deep methods. This underlines the fact that detecting POI
changes should be evaluated with respect to its ﬁnal goal (at
the geographical level) instead of relying on simpler image-
level criteria (e.g. pixelwise or image-level metrics applied
on image pairs).

We also plot the gROC curve, for both splits, in Fig. 7.
It is interesting to note that the difﬁculty of the splits is not
homogeneous, and that methods behave quite differently on
both splits. While we expect the double-margin loss to per-
form better in term of gAUC, a detection metric, due to the
perfect alignment with the task at hand (Section 4.3), this is
only true for the ﬁrst split. In contrast, the triplet and con-
trastive losses yield more stable performance on both splits.

5.3. Qualitative results on map database

We present quantitative results of POI change predic-
tions generated at the map level for each of the learned mod-
els in Fig. 8. Each row shows a part of the ﬂoop map aug-
mented with the geographical likelihood of POI changes, as
output by g(p) from Eq. (2). The likelihood is color-coded
from green (no change) to red (change).

We also show some image pairs corresponding to the
POIs pinned on the map. Some of them present dramatic
viewpoint changes, e.g. Fig. 8.a. The learned approaches
most correctly predict the absence of changes up to some
extent, e.g. in the case of ﬁrst row. Conversely, the view-
point difference in the second row of Fig. 8.a exceeds the
tolerance range of the models and results in a false predic-
tion of POI change.

4092

(a)

(b)

Figure 8. We present some examples of POI change likelihood output at the geographical level according to Eq. (2). The two rows are
from MallScape-A (top) and MallScape-B (bottom), respectively. We show two corresponding image pairs from the locations pinned on
the partial ﬂoor map (a). The change likelihood is color-coded as changed and not-changed.

Table 2. Performance evaluation for all methods and all metrics (see text for details).

Parameters

Split 1

Split 2

Overall

name value

pAUC pAP

gAUC gAP

pAUC pAP

gAUC gAP

pAUC pAP

gAUC gAP

Local descriptor
SIFT [26]+RANSAC

Logo detection
YOLOv2 [36]+CAL [46]

Global representations
ImageNet [13]

-

-

-

-

-

-

0.533

0.080

0.731

0.101

0.823

0.101

0.795

0.167

0.678

0.091

0.763

0.134

0.690

0.005

0.711

0.077

0.642

0.003

0.638

0.058

0.666

0.004

0.675

0.068

0.932

0.386

0.758

0.201

0.932

0.245

0.827

0.391

0.932

0.316

0.793

0.296

Deep metric learning
ML+Lc (8)
ML+Ldm (9)
ML+Lt (10)

τc
m, τ
m

0.5
0.1, 0.1
0.1

0.970
0.961
0.973

0.591
0.556
0.582

0.787
0.793
0.786

0.393
0.330
0.412

0.959
0.930
0.970

0.214
0.105
0.228

0.898
0.868
0.905

0.508
0.408
0.557

0.965
0.946
0.972

0.403
0.331
0.405

0.843
0.831
0.846

0.451
0.369
0.485

Since our framework allows to output results directly at
a geographical level, visualization is easy and straightfor-
ward. A human operator can rapidly understand the posi-
tions of all POI changes in a glance and update them ac-
cordingly, which can greatly ease the update process. Ul-
timately, the update process could become fully automatic
if logo or franchise recognition [44, 45, 49] would be per-
formed on the corresponding POI images.

6. Conclusion

We have presented a novel generic approach, based on
the deep metric learning framework, that can detect POI
changes from a set of spatiotemporally localized scene pho-
tographs. Several metric learning formulations were thor-
oughly evaluated and tested, which conﬁrmed their overall
effectiveness for this problem. In particular, the triplet loss
seems best for this problem from an empirical perspective.
To enable training and evaluation, we have introduced a
new dataset dedicated to the POI change detection task. It

contains thousands of images and hundreds of POIs, mak-
ing it suitable for training deep models in realistic settings.
Not only can this dataset also serve as a benchmark suite for
other researchers interested in this task, but we believe that
it can also help to further develop new exciting tasks related
to automatic map creation and maintenance, thanks to the
rich information encompassed in the dataset.

We indeed acknowledge that the approach proposed here
calls for further research. For instance, it does not allow us
to understand if a single photo contains several POIs, and if
so, what their boundaries are. Yet automatic shop segmen-
tation is an important milestone to understand the spatial
extent of each POI and thus to map them to a geographic
location, which would certainly help to better detect and lo-
calize POI changes. We leave these issues for future work.

Acknowledgements This work was partly supported by Insti-
tute of Information & Communications Technology Planning &
Evaluation (IITP) grant funded by the Korea government (MSIT)
(No.R0132-15-1005, Content visual browsing technology in the
online and ofﬂine environments)

4093

References

[1] http://rebrand.ly/mallscape. 3
[2] P. F. Alcantarilla, S. Stent, G. Ros, R. Arroyo, and R. Gher-
ardi. Street-view change detection with deconvolutional net-
works. Auton. Robots, 42:1301–1322, 2016. 3

[3] D. Anguelov, C. Dulong, D. Filip, C. Frueh, S. Lafon,
R. Lyon, A. Ogale, L. Vincent, and J. Weaver. Google
street view: Capturing the world at street level. Computer,
43(6):32–38, June 2010. 2

[4] A. T. R. Arandjelovic, J. S. M. Okutomi, and T. Pajdla. 24/7
place recognition by view synthesis. In IEEE CVPR, 2015.
2, 3

[5] R. Arandjelovi, P. Gronat, A. Torii, T. Pajdla, and J. Sivic.
NetVLAD: CNN architecture for weakly supervised place
recognition. In IEEE CVPR, 2016. 2, 3

[6] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, and E. Romera.
Are you able to perform a life-long visual topological local-
ization? Autonomous Robots, 42(3):665–685, Mar 2018. 3

[7] Y. Avrithis, Y. Kalantidis, G. Tolias, and E. Spyrou. Re-
trieving landmark and non-landmark images from commu-
nity photo collections. In ACM Multimedia, 2010. 2

[8] J. Baus, K. Cheverst, and C. Kray. A Survey of Map-based
Mobile Guides, pages 193–209. Springer Berlin Heidelberg,
2005. 1

[9] K.-T. Chen, F.-E. Wang, J.-T. Lin, F.-H. Chan, and M. Sun.
In

The world is changing: Finding changes on the street.
ACCV Workshop, 2016. 3

[10] W. Chen, X. Chen, J. Zhang, and K. Huang. Beyond triplet
loss: A deep quadruplet network for person re-identiﬁcation.
In IEEE CVPR, 2017. 3, 5

[11] R. C. Daudt, B. L. Saux, and A. Boulch. Fully convolutional
siamese networks for change detection. In IEEE ICIP, 2018.
3

[12] R. C. Daudt, B. L. Saux, A. Boulch, and Y. Gousseau. High
resolution semantic change detection. arXiv:1810.08452,
2018. 3

[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. ImageNet: A large-scale hierarchical image database.
In IEEE CVPR, 2009. 8

[14] A. Gordo, J. Almaz´an, J. Revaud, and D. Larlus. Deep image
retrieval: Learning global representations for image search.
In ECCV, 2016. 3, 5, 7

[15] N. Goyette, P. Jodoin, F. Porikli, J. Konrad, and P. Ishwar.
Changedetection.net: A new change detection benchmark
dataset. In IEEE CVPR Workshop, 2012. 3

[16] E. Guo, X. Fu, J. Zhu, M. Deng, Y. Liu, Q. Zhu, and
H. Li.
Learning to measure change: Fully convolu-
tional siamese metric networks for scene change detection.
arXiv:1810.09111, 2018. 3

[17] R. Hadsell, S. Chopra, and Y. Lecun. Dimensionality reduc-
tion by learning an invariant mapping. In IEEE CVPR, 2006.
3, 6

[18] M. Haklay and P. Weber. Openstreetmap: User-generated
street maps. IEEE Pervasive Computing, 7(4):12–18, 2008.
1

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In IEEE CVPR, 2016. 7

[20] X. He, Y. Zhou, Z. Zhou, S. Bai, and X. Bai. Triplet-center
loss for multi-view 3d object retrieval. In IEEE CVPR, 2018.
3

[21] H. Jegou, M. Douze, and C. Schmid. Hamming embedding
and weak geometric consistency for large scale image search.
In ECCV, 2008. 2, 3

[22] J. Koˇsecka. Detecting changes in images of street scenes. In

ACCV, 2012. 3

[23] L. A. Lim and H. Y. Keles. Learning multi-scale features for

foreground segmentation. arXiv:1808.01477, 2018. 3

[24] J. Lin, O. Morere, A. Veillard, L. Duan, H. Goh, and V. Chan-
drasekhar. Deephash for image instance retrieval: Getting
regularization, depth and ﬁne-tuning right. In ICMR, 2017.
3, 6

[25] T.-Y. Liu. Learning to rank for information retrieval. Found.

Trends Inf. Retr., 2009. 3, 6

[26] D. G. Lowe. Distinctive image features from scale-invariant

keypoints. IJCV, 60(2):91–110, 2004. 7, 8

[27] L. N. Mummidi and J. Krumm. Discovering points of interest
from users’ map annotations. GeoJournal, 72(3):215–227,
2008. 1

[28] A. Naitsat, E. Saucan, and Y. Zeevi. A differential geom-
In

etry approach for change detection in medical images.
ISCBMS, 2017. 3

[29] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han.
Largescale image retrieval with attentive deep local features.
In IEEE CVPR, 2017. 2, 3

[30] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisser-
man. Object retrieval with large vocabularies and fast spatial
matching. In IEEE CVPR, 2007. 2

[31] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman.
Lost in quantization: Improving particular object retrieval in
large scale image databases. In IEEE CVPR, 2008. 2, 3

[32] F. Radenovi´c, G. Tolias, and O. Chum. Cnn image retreival
learns from BoW: Unsupervised ﬁne-tuning with hard exam-
ples. In ECCV, 2016. 5, 7

[33] F. Radenovi´c, G. Tolias, and O. Chum. Fine-tuning CNN
image retrieval with no human annotation. TPAMI, 2018. 6,
7

[34] F. Radenovi, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.
Revisiting oxford and paris: Large-scale image retrieval
benchmarking. In IEEE CVPR, 2018. 3, 7

[35] R. J. Radke, S. Andra, O. Al-Kofahi, and B. Roysam. Im-
age change detection algorithms: A systematic survey. TIP,
2005. 3

[36] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

In IEEE CVPR, 2017. 8

[37] K. Rehrl, E. H¨ausler, R. Steinmann, S. Leitinger, D. Bell, and
M. Weber. Pedestrian Navigation with Augmented Reality,
Voice and Digital Map: Results from a Field Study assessing
Performance and User Experience, pages 3–20. Springer
Berlin Heidelberg, 2012. 1

[38] D. Reilly, M. Rodgers, R. Argue, M. Nunes, and K. Inkpen.
Marked-up maps: Combining paper maps and electronic
information resources.
Personal Ubiquitous Comput.,
10(4):215–226, 2006. 1

4094

[39] J. Revaud, M. Douze, and C. Schmid. Correlation-Based
Burstiness for Logo Retrieval. In ACM Multimedia, 2012. 3
[40] S. Romberg, L. G. Pueyo, R. Lienhart, and R. van Zwol.
In ICMR,

Scalable logo recognition in real-world images.
2011. 3

[41] M. Ruta, F. Scioscia, S. Ieva, G. Loseto, and E. Di Sciascio.
Semantic annotation of openstreetmap points of interest for
mobile discovery and navigation. In 2012 IEEE First Inter-
national Conference on Mobile Services, pages 33–39, 2012.
1

[42] K. Sakurada and T. Okatani. Change detection from a street
image pair using cnn features and superpixel segmentation.
In BMVC, 2015. 3

[43] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In IEEE
CVPR, 2015. 3, 6

[44] H. Su, S. Gong, X. Zhu, et al. Weblogo-2m: Scalable logo
detection by deep learning from the web. In ICCV Workshop
on Web-scale Vision and Social Media, 2018. 3, 8

[45] H. Su, X. Zhu, and S. Gong. Deep learning logo detection
with data expansion by synthesising context. In IEEE WACV,
2017. 3, 8

[46] H. Su, X. Zhu, and S. Gong. Open logo detection challenge.

In BMVC, 2018. 7, 8

[47] G. Tolias, R. Sicre, and H. J´egou. Particular object retrieval
In ICLR,

with integral max-pooling of CNN activations.
2016. 7

[48] A. Torii, J. Sivic, M. Okutomi, and T. Pajdla. Visual place
recognition with repetitive structures. In IEEE CVPR, 2013.
2, 3

[49] A. T¨uzk¨o, C. Herrmann, D. Manger, and J. Beyerer. Open

Set Logo Detection and Retrieval. In VISAPP, 2018. 3, 8

[50] E. Ustinova and V. Lempitsky. Learning deep embeddings

with histogram loss. In NIPS, 2016. 3, 6, 7

[51] F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang. Joint
learning of single-image and cross-image representations for
person re-identiﬁcation. In IEEE CVPR, 2016. 3, 5

[52] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang,
J. Philbin, B. Chen, and Y. Wu. Learning ﬁne-grained im-
age similarity with deep ranking. In IEEE CVPR, 2014. 3

[53] Y. Wang, Z. Luo, and P.-M. Jodoin. Interactive deep learning
method for segmenting moving objects. Pattern Recognition
Letters, 96:66 – 75, 2017. Scene Background Modeling and
Initialization. 3

[54] J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba. SUN
Database: Large-scale scene recognition from abbey to zoo.
In IEEE CVPR, 2010. 2

[55] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba.
Places: A 10 million image database for scene recognition.
TPAMI, 2017. 2

[56] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In NIPS, 2014. 2

4095

