Leveraging the Invariant Side of Generative Zero-Shot Learning

Jingjing Li1, Mengmeng Jing1, Ke Lu1, Zhengming Ding2, Lei Zhu3, Zi Huang4

1 University of Electronic Science and Technology of China; 3 Shandong Normal Unversity

2 Indiana University-Purdue University Indianapolis; 4 University of Queensland

lijin117@yeah.net

Abstract

Seen Samples

Semantic Attributes

Generated fake samples

Conventional zero-shot learning (ZSL) methods gener-
ally learn an embedding, e.g., visual-semantic mapping,
to handle the unseen visual samples via an indirect man-
ner.
In this paper, we take the advantage of generative
adversarial networks (GANs) and propose a novel method,
named leveraging invariant side GAN (LisGAN), which can
directly generate the unseen features from random noises
which are conditioned by the semantic descriptions. Specif-
ically, we train a conditional Wasserstein GANs in which
the generator synthesizes fake unseen features from noises
and the discriminator distinguishes the fake from real via a
minimax game. Considering that one semantic description
can correspond to various synthesized visual samples, and
the semantic description, ﬁguratively, is the soul of the gen-
erated features, we introduce soul samples as the invariant
side of generative zero-shot learning in this paper. A soul
sample is the meta-representation of one class. It visualizes
the most semantically-meaningful aspects of each sample in
the same category. We regularize that each generated sam-
ple (the varying side of generative ZSL) should be close to
at least one soul sample (the invariant side) which has the
same class label with it. At the zero-shot recognition stage,
we propose to use two classiﬁers, which are deployed in
a cascade way, to achieve a coarse-to-ﬁne result. Experi-
ments on ﬁve popular benchmarks verify that our proposed
approach can outperform state-of-the-art methods with sig-
niﬁcant improvements 1.

1. Introduction

In general, a computer vision algorithm can only handle
the objects which appeared in the training dataset. In other
words, an algorithm can only recognize the objects which
are seen before. However, for some speciﬁc real-world ap-
plications, we either do not have the training sample of one
object or the sample is too expensive to be labeled. For

1Codes and datasets are available at github.com/lijin118/LisGAN

Train

G

A

N

Input

Unseen Sample

Recognize

Figure 1. Zero-shot learning with GANs, i.e., generative ZSL.

instance, we want the approach to trigger a message when
it encounters a sample with a rare gene mutation from one
species. Unfortunately, we did not have the visual features
of the sample for training. The things we have are merely
the images taken from normal instances and some semantic
descriptions which describe the characteristics of the muta-
tion and how it differs from normal ones. Conventional ma-
chine learning algorithms would fail in this task, but a hu-
man being would not. A human being is able to recognize
an unseen object at the ﬁrst glance by only reading some
semantic descriptions. Inspired by this, zero-shot learning
(ZSL) [5,32,35,36] is proposed to handle unseen objects by
the model which is trained on only seen objects and seman-
tic descriptions about both seen and unseen categories.

Since the seen and unseen classes are connected by the
semantic descriptions, a natural idea is to learn a visual-
semantic mapping so that both seen and unseen samples can
be compared in the semantic space. For instance, previous
works [5, 37–39] learn either shallow or deep embeddings
for zero-shot learning. These methods handle the unseen
samples via an indirect way. Considering that one semantic
description can correspond to enormous number of visual
samples, the performance of zero-shot learning is restricted
with the limited semantic information.

Recently, thanks to the advances in generative adversar-
ial networks (GANs) [8], a few approaches are proposed to
directly generate unseen samples from the random noises
and semantic descriptions [24, 34, 40], as shown in Fig. 1.
With the generated unseen samples, zero-shot learning can
be transformed to a general supervised machine learning

7402

problem. In such a learning paradigm, however, the chal-
lenges of zero-shot learning have been also passed on to the
GANs. In the GANs based paradigms for zero-shot learn-
ing, we have to address the spurious and soulless generating
problem. Speciﬁcally, we generally have only one seman-
tic description, e.g., one attributes vector, one article or one
paragraph of texts, for a speciﬁc category, but the semantic
description is inherently related to a great mass of images in
the visual space. For instance, “a tetrapod with a tail” can
be mapped to many animals, e.g., cats, dogs and horses. At
the same time, some objects from different categories have
very similar attributes, such as “tigers” and “ligers”. Thus,
the generative adversarial networks for zero-shot learning
must challenge two issues: 1) how to guarantee the genera-
tive diversity based on limited and even similar attributes?
2) how to make sure that each generated sample is highly
related with the real samples and corresponding semantic
descriptions? However, since deploying GANs to address
the ZSL problem is a new topic, most of existing works did
not explicitly address the two issues. In this paper, we pro-
pose a novel approach which takes the two aspects into con-
sideration and carefully handles them in the formulation.

At ﬁrst, to guarantee that the generated samples are
meaningful, we propose to generate samples from random
noises which are conditioned with the class semantic de-
scriptions. At the same time, we also introduce the super-
vised classiﬁcation loss in the GAN discriminator to pre-
serve the inter-class discrimination during the adversarial
training. Furthermore, to ensure that each synthesized sam-
ple (the varying side of generative zero-shot learning) is
highly related with the real ones and corresponding seman-
tic descriptions (the invariant side), we introduce soul sam-
ples in this paper, as shown in Fig. 3. For unseen classes,
the visual characteristics of a generated sample only depend
on the semantic descriptions. Thus, the semantic informa-
tion is the soul of the generated samples. The soul sample
must be not very speciﬁc so that it can plainly visualize the
most semantically-meaningful aspects and relate to as many
samples as possible. For the seen images, therefore, we de-
ﬁne that a soul sample is an average representation of them.
For the generated samples, we regularize them to be close to
soul samples. Thus, we can guarantee that each generated
sample is highly related with the real ones and correspond-
ing semantic descriptions.

To summarize, the main contributions of this paper are:

1) We propose a novel ZSL method LisGAN which takes
advantage of generative adversarial networks. Speciﬁ-
cally, we deploy the conditional GANs to tackle the two
issues: generative diversity and generative reliability. To
improve the quality of generated features, we introduce
soul samples which are deﬁned as the representations of
each category. By further considering the multi-view na-
ture of different images, we propose to deﬁne multiple

soul samples for each class. We regularize each gener-
ated sample to be close to at least one soul sample so that
the varying side in generative zero-shot learning would
not be divorced from the invariant side.

2) At the zero-shot recognition stage, we propose that if we
have high conﬁdence in recognizing an unseen sample,
the sample (with its assigned pseudo label) will be lever-
aged as the reference to recognize other unseen samples.
Speciﬁcally, we propose to use two classiﬁers, which are
deployed in a cascade way, to achieve a coarse-to-ﬁne
result. We also report a simple yet efﬁcient method to
measure the classiﬁcation conﬁdence in this paper.

3) Extensive experiments on ﬁve widely used datasets ver-
ify that our proposed method can outperform state-of-
the-art methods with remarkable improvements.

2. Related Work

2.1. Zero Shot Learning

Inspired by the human ability that one can recognize an
object at the ﬁrst glance by only knowing some semantic
descriptions of it, zero-shot learning [4, 6, 13, 17, 35] aims
to learn a model with good generalization ability which can
recognize unseen objects by only giving some semantic at-
tributes. A typical zero-shot learning model is trained on
visual features which only contain the seen samples and se-
mantic features which contain both seen and unseen sam-
ples. Since the seen objects and unseen ones are only con-
nected in the semantic space and the unseen objects need
to be recognized by the visual features, zero-shot learning
methods generally learn a visual-semantic embedding with
the seen samples. At the zero-shot classiﬁcation stage, un-
seen samples are projected into the semantic space and la-
beled by semantic attributes [5, 15, 16, 29]. Instead of learn-
ing a visual-semantic embedding, some previous works also
propose to learn a semantic-visual mapping so that the un-
seen samples can be represented by the seen ones [12, 30].
In addition, there are also some works to learn an interme-
diate space shared by the visual features and semantic fea-
tures [4, 38, 39]. Besides, ZSL is also related with domain
adaptation and cold-start recommendation [18–21].

From the recent literatures, typical zero-shot learning
tasks are zero-shot classiﬁcation [11, 36], zero-shot re-
trieval [22] and generalized zero-shot recognition [32]. The
main difference between zero-shot learning and generalized
zero-shot recognition is that the former only classiﬁes the
unseen samples in the unseen category and the latter rec-
ognizes samples, which can be either seen ones and unseen
ones, in both seen and unseen categories.

It is easy to observe that conventional zero-shot learning
methods are indirect. They usually need to learn a space

7403

Random Noises

Soul Samples Regularization

Fake visual samples

Real Seen Samples

Semantic Attributes

(Seen)

(Unseen)

GAN Generator

Unseen Samples

Real/Fake?

GAN Discriminator

Classifier

Classification Loss

Zebra

Leverage the confident results

Figure 2. Idea illustration of our LisGAN (Leveraging invariant side GAN). We train a conditional WGAN to generate fake unseen images
from random noises and semantic attributes. Multiple soul samples for each class are introduced to regularize the generator. Unseen
samples classiﬁed with high conﬁdence are leveraged to ﬁne-tune ﬁnal results.

mapping function. Recently, by taking advantage of gener-
ative adversarial networks [3, 8], several methods [34, 40]
are proposed to directly generate unseen samples from their
corresponding attributes, which converts the conventional
zero-shot learning to a classic supervised learning problem.

2.2. Generative Adversarial Nets

A typical generative adversarial networks (GANs) [8]
consists of two components: a generator and a discrimina-
tor. The two players are trained in an adversarial manner.
Speciﬁcally, the generator G tries to generate fake images
from input noises to fool the discriminator, while the dis-
criminator D attempts to distinguish real images and fake
ones. In general, the input of G is random noise and the
output is the synthesized image. The inputs of D are both
real images and fake images, the output is a probability dis-
In this paper, we deploy G to generate sample
tribution.
features instead of image pixels.

Although GANs has shown quite impressive results and
profound impacts, the vanilla GAN is very hard to train.
Wasserstein GANs (WGANs) [3] presents an alternative to
traditional GAN training. WGANs can improve the sta-
bility of learning, get rid of problems like mode collapse,
and provide meaningful learning curves useful for debug-
ging and hyperparameter searches. In addition, conditional
GANs [23] are proposed to enhance the outputs of tradi-
tional GANs. With conditional GANs, one can incorporate
the class labels and other information into the generator and
discriminator to synthesize speciﬁed samples.

3. The Proposed Method

3.1. Deﬁnitions and Notations

Given n labeled seen samples with both visual features
X ∈ Rd×n and semantic descriptions A ∈ Rm×n for train-
ing, zero-shot learning aims to recognize nu unknown vi-
sual samples Xu ∈ Rd×nu which only have semantic at-
tributes Au ∈ Rm×nu for training. Let Y and Yu be the

label space of X and Xu, respectively, in zero-shot learning
we have Y ∩ Yu = ∅. Suppose that we have C and Cu cat-
egories in total for seen data and unseen data, respectively,
classical zero-shot learning recognizes Xu by only search-
ing in Cu, while generalized zero-shot learning searches in
C ∪ Cs. The semantic descriptions A and Au are either pro-
vided as binary/numerical vectors or word embedding/RNN
features. Each semantic description a corresponds to a cat-
egory y. Formally, given {X, A, Y } and {Au, Yu} for
training, the task of zero shot learning is to learn a function
f : Xu → Yu and generalized zero shot learning is to learn
a function f : {X , X u} → Y ∪ Yu .

3.2. Overall Idea

In this paper, we take advantage of GANs to directly gen-
erate fake visual features for unseen samples from random
noises and the semantic descriptions. Then, the synthesized
visual features are used as references to classify real unseen
samples. Since we only have Au and the GAN discrim-
inator cannot access Xu in the training stage, the real or
fake game, therefore, cannot be played. Thus, we mainly
train our GAN on the seen classes. At the same time, we
deploy the conditional GANs so that the class embedding
can be incorporated into both generator G and discrimina-
tor D. Since {A, Y } and {Au, Yu} are interconnected,
i.e., A and Au have the same semantic space, the condi-
tional GAN which generates high-quality samples for seen
classes is also expected to generate high-quality samples for
unseen categories. The main idea of this paper is illustrated
in Fig 2. Compared with existing methods which also de-
ploy GANs for zero-shot learning, our novelty comes from
two aspects. The ﬁrst one is that we introduce multiple soul
samples per class to regularize the generator. The second
is that we leverage the unseen samples which are classi-
ﬁed with high conﬁdence to facilitate the subsequent un-
seen samples. Experiments reported in section 5 show that
we can achieve a signiﬁcant improvement against state-of-
the-art methods on various datasets.

7404

Figure 3. Soul samples of the horse category. Considering the na-
ture multi-view property of visual objects, e.g., real images of an
object are usually captured from different views, we propose to
learn multiple soul samples for each class. By such a formulation,
the domain shift issue caused by different views can be alleviated.

3.3. Train the LisGAN

Given the seen samples {X, A, Y }, the attributes Au
of the unseen sample and random noises z ∼ N (0, 1), the
GAN generator G uses the input a and nosies z to synthe-
size fake features. At the same time, the GAN discriminator
D takes the features of real image x and G(z, a) as inputs
to discriminate whether an input feature is real or fake. For-
mally, the loss of G can be formulated as follows:

LG = −E[D(G(z, a))] − λE[logP (y|G(z, a))],

(1)

where the ﬁrst term is the Wasserstein loss [3] and the sec-
ond one is the supervised classiﬁcation loss on the synthe-
sized features, λ > 0 is a balancing parameter.

Similarly, the loss of the discriminator can be formulated

as follows:

LD = E[D(G(z, a))] − E[D(x)]

− λ(E[logP (y|G(z, a))] + E[logP (y|x)])
− βE[(k∇ˆxD(ˆx)k2 − 1)2],

(2)

where β > 0 is a hyper-parameter. The fourth term, simi-
lar with the third one, is a supervised classiﬁcation loss on
real samples. The last term is used to enforce the Lipschitz
constraint [9], in which ˆx = µx + (1 − µ)G(z, a) with
µ ∼ U (0, 1). As suggested in [9], we ﬁx β = 10.

In our model, we take the CNN features of samples as
the visual input X. Both the generator and discriminator
are implemented with fully connected layers and ReLU ac-
tivations. Thus, the model is feasible to incorporate into
different CNN architectures. At the same time, the output
of the generator is directly visual features rather than im-
age pixels. By optimizing the above two-player minimax
game, the conditional GAN generator is able to synthesize
fake features of the seen images with the class embedding
A. Since the unseen objects share the same semantic space
with the seen samples, the conditional GAN generator can
also synthesize visual features for unseen categories via Au.
With the optimization problem in Eq. (1) and Eq. (2), our
model can guarantee the generative diversity with similar
attributes. With the supervised classiﬁcation loss, it can
also ensure that the learned features are discriminative for
further classiﬁcation. However, the model does not explic-
itly address the quality of the generated features.
In this

paper, to make sure that each generated feature is highly re-
lated with the semantic descriptions and real samples, we
introduce soul samples to regularize the generator. Since
the soul samples of a category should reﬂect the most re-
markable characteristics of the class as much as possible,
we deploy the average representation of all samples from
the category c to deﬁne the soul sample of c, which is sim-
ilar with prototypical networks for few-shot learning [31].
Furthermore, considering the nature multi-view property of
real samples, as shown in Fig. 3, we further propose that a
category c should have multiple soul samples to address the
multi-view issue. To this end, we ﬁrst group the real fea-
tures of one seen class into k clusters. For simplicity, we
ﬁx k = 3 in this paper. Then, we calculate a soul sample
for each cluster. Let {X c
k} be the k clusters
of category c, the soul samples Sc = {sc
k} are
deﬁned as:

2, · · · , X c

2, · · · , sc

1, X c

1, sc

k = 1
sc
|X c

k| P
xi∈X c
k

xi.

(3)

Similarly, for the generated fake features, we can also

deﬁne the soul sample ˜sc

k as:

k = 1
˜sc
| ˜X c
k|

P

˜xi,

˜xi∈ ˜X c

k

(4)

where ˜xi = G(z, a) is a generated fake feature.

In this paper, we encourage that each generated sample
˜x for class c should be close to at least one soul sample sc.
Formally, we introduce the following regularization:

LR1 = 1
n1

n1
P
i=1

min
j∈[1,k]

k˜xi − sc

jk2
2,

(5)

where n1 is the number of generated samples and k is the
number of soul samples per class. At the same time, since
the soul samples can also be seen as the centroid of one
cluster, we encourage that the fake soul samples should be
close to at least one real soul sample from the same class,
which can be formulated as:

LR2 = 1
C

C
P
c=1

min
j∈[1,k]

k˜sc

j − sc

jk2
2,

(6)

where C is the number of total categories. With the two
regularizations LR1 and LR2, our model avoids to generate
soulless features. Each of the generated features would be
close to the real ones, which guarantees the quality of the
fake features. From another perspective, LR1 is an individ-
ual regularization which addresses single samples and LR2
is a group regularization which takes care of a cluster.

3.4. Predict Unseen Samples

Once the GAN is trained to be able to generate visual
features for seen classes, it can also synthesize visual fea-
tures for the unseen ones with random noises and semantic

7405

attributes Au. Then, the zero-shot learning is automatically
converted to a supervised learning problem. Speciﬁcally,
we can train a softmax classiﬁer on the generated features
and classify the real unseen features. The softmax is formu-
lated as minimizing the following negative log likelihood:

Table 1. Dataset statistics. The (number) in # Seen Classes indi-
cates the number of seen classes used for test in the GZSL.

Dataset
# Samples
# Attributes
# Seen Classes
# Unseen Classes

aPaY
15,339

64

AwA
30,475

85

CUB
11,788

312

20 (5)

40 (13)

150 (50)

12

10

50

FLO
8,189
1,024
82 (20)

20

SUN
14,340

102

645 (65)

72

min

θ

− 1

|X | P

(x,y)∈(X ,Y)

logP (y|x; θ),

where θ is the training parameter and

P (y|x; θ) =

exp(θ⊤
y x)
i=1 exp(θ⊤

i x)

PN

.

(7)

(8)

In this paper, we further propose that we can leverage an
unseen sample if we have sufﬁcient conﬁdence in believing
that the sample has been correctly classiﬁed. Since the out-
put of the softmax layer is a vector which contains the prob-
abilities of all possible categories, the entropy of the vector
can be used to measure the certainty of the results. If a prob-
ability vector has lower entropy, we have more conﬁdence
of the results. Therefore, we leverage the samples which
have low classiﬁcation entropy and deploy them as refer-
ences to classify the other unseen samples. Speciﬁcally, we
calculate the sample entropy by:

E(y) = −

C
P
c=1

yc log yc.

(9)

In our model, we deploy two classiﬁers via a cascade
manner to predict the unseen samples. The ﬁrst classiﬁer is
used to evaluate the classiﬁcation conﬁdence and the second
is used to leverage the correctly classiﬁed samples. In our
zero-shot recognition, the ﬁrst classiﬁer is a softmax trained
on the generated fake features, while second classiﬁer can
be either a trained classiﬁer, e.g., softmax classiﬁer, SVM,
or just a training-free classiﬁer, e.g., NNC.

4. Experiments

4.1. Datasets

APascal-aYahoo (aPaY) contains 32 categories from
both PASCAL VOC 2008 dataset and Yahoo image search
engine. Speciﬁcally, 20 classes are from PASCAL and
12 classes are from Yahoo. The total number of aPaY is
15,339. Following previous work [35, 40], we deploy the
PASCAL VOC 2008 as seen dataset and the Yahoo as un-
seen one. An additional 64-dimensional attribute vector is
annotated for each category.

Animals with Attributes (AwA) [14] consists of 30,475
images of 50 animals classes. The animals classes are
aligned with Osherson’s classical class/attribute matrix,
thereby providing 85 numeric attribute values for each class.
Caltech-UCSD Birds-200-2011 (CUB) [33] is an ex-
tended version of the CUB-200 dataset. CUB is a chal-
lenging dataset which contains 11,788 images of 200 bird

species. Each species is associated with a Wikipedia arti-
cle and organized by scientiﬁc classiﬁcation (order, family,
genus, species). A vocabulary of 28 attribute groupings and
312 binary attributes were associated with the dataset based
on an online tool for bird species identiﬁcation.

Oxford Flowers (FLO) [25] dataset consists of 8,189 im-
ages which comes from 102 ﬂower categories. Each class
consists of between 40 and 258 images. The images have
large scale, pose and light variations. In addition, there are
categories that have large variations within the category and
several very similar categories. For this dataset, we use the
same semantic descriptions provided by Reed et al. [28].

SUN attributes (SUN) [27] is a large-scale scene attribute
dataset, which spans 717 categories and 14,340 images in
total. Each category includes 102 attribute labels.

For clarity, we report the dataset statistics and zero-shot
split settings in Table 1. The zero-shot splits of aPaY, AwA,
CUB and SUN are same with previous work [35] and the
splits of FLO is same with [28]. For the real CNN features,
we follow previous work [34] to extract 2048-dimensional
features from ResNet-101 [10] which is pre-trained on Im-
ageNet. For the semantic descriptions, we use the default
attributes included in the datasets. Speciﬁcally, since FLO
did not provide attributes with the dataset, we use the 1024-
dimensional RNN descriptions via the model of [28]. For
fair comparisons, all of our experimental settings are same
with the protocols reported in previous work [34].

4.2. Implementation and Compared Methods

In our model, the GAN is implemented via multilayer
perceptron with Rectiﬁed Linear Unit (ReLU) activation.
Speciﬁcally, the generator G contains a fully connected
layer with 4,096 hidden units. The noise z is conditioned
by the semantic description a and then severed as the inputs
of G. An additional ReLU layer is deployed as the output
layer of G which outputs the synthesized fake features. The
discriminator D takes the real features and the synthesized
fake features from G and processes them via an FC layer, a
Leaky ReLU layer, an FC layer and a ReLU layer. The dis-
criminator has two branches for output. One is used to tell
fake from real and the other is a standard n-ways classiﬁer
to predict the correct category of each sample. In this paper,
we set λ = 0.01 and β = 10. The weight for two regular-
izations are all set to 0.01. The sample entropy threshold is
set to be smaller than the median of all entropies. One can
also tune the hyper-parameters by cross-validation.

The compared methods are representative ones pub-

7406

Table 2. The top-1 accuracy (%) of zero-shot learning on different
datasets. The best results are highlighted with bold numbers.

Methods
DAP [15]
CONSE [26]
SSE [38]
DeViSE [7]
SJE [2]
ESZSL [29]
ALE [1]
SYNC [4]
SAE [13]
DEM [37]
GAZSL [40]
f-CLSWGAN [34]
LisGAN [Ours]

aPaY
33.8
26.9
34.0
39.8
32.9
38.3
39.7
23.9
8.3
35.0
41.1
40.5
43.1

AwA
44.1
45.6
60.1
54.2
65.6
58.2
59.9
54.0
53.0
68.4
68.2
68.2
70.6

CUB
40.0
34.3
43.9
52.0
53.9
53.9
54.9
55.6
33.3
51.7
55.8
57.3
58.8

FLO

-
-
-

45.9
53.4
51.0
48.5

-
-
-

60.5
67.2
69.6

SUN
39.9
38.8
51.5
56.5
53.7
54.5
58.1
56.3
40.3
61.9
61.3
60.8
61.7

learning on aPaY dataset.

Table 3. The results (top-1 accuracy %) of generalized zero-
The Mean in this table
shot
is the harmonic mean of seen and unseen samples,
i.e.,
Mean=(2*Unseen*Seen)/(Unseen+Seen).

Methods

DAP [15]
CONSE [26]
SSE [38]
DeViSE [7]
SJE [2]
ESZSL [29]
ALE [1]
SYNC [4]
SAE [13]
DEM [37]
GAZSL [40]
f-CLSWGAN [34]
LisGAN [Ours]

Unseen

4.8
0.0
0.2
4.9
3.7
2.4
4.6
7.4
0.4
11.1
14.2
32.9
34.3

aPaY
Seen
78.3
91.2
78.9
76.9
55.7
70.1
73.7
66.3
80.9
75.1
78.6
61.7
68.2

Mean
9.0
0.0
0.4
9.2
6.9
4.6
8.7
13.3
0.9
19.4
24.0
42.9
45.7

lished in the fast few years and the state-of-the-art ones re-
ported very recently. Speciﬁcally, we compare our approach
with: DAP [15], CONSE [26], SSE [38], DeViSE [7],
SJE [2], ESZSL [29], ALE [1], SYNC [4], SAE [13],
DEM [37], GAZSL [40] and f-CLSWGAN [34].

Following previous work [34, 40], we report the average
per-class top-1 accuracy for each of the evaluated method.
Speciﬁcally, for classic zero-shot learning, we report the
top-1 accuracy of unseen samples by only searching the un-
seen label space. However, for the generalized zero-shot
learning, we report the accuracy on both seen classes and
unseen classes with the same settings in [35]. Some of the
results reported in this paper are also cited from [35].

4.3. Zero shot Learning

We report the zero-shot learning results on the ﬁve
datasets in Table 2. In these experiments, the possible cate-
gories of unseen samples are searched from only Yu. It can
be seen that our method achieves the best on four of the ﬁve
evaluations. We also achieved state-of-the-art result on the
last dataset. Speciﬁcally, we achieved 2.6% improvement
over the state-of-the-art method on aPaY dataset. We also
achieved 2.4%, 1.5% and 2.4% on AWA, CUB and FLO.

(a) f-CLSWGAN

(b) Ours

Figure 4. The confusion matrix on the evaluation of aPaY dataset.

From the results, we can also observe that the GAN-
based methods, e.g., GAZSL, f-CLSWGAN and ours, gen-
erally perform better than embedding ones, e.g., SSE, ALE
and SAE. The embedding methods handle the unseen sam-
ples via an indirect manner, while the GAN method directly
handle it by converting it to a supervised learning task. The
results suggest that GAN could be a promising way to ad-
dress zero-shot learning problem in the future. Apart from
generating visual features from noises, GANs can also be
used for semantic augmentation in zero-shot learning. In
our future work, we will incorporate semantic data augmen-
tation in our model to cover more unseen samples.

4.4. Generalized Zero shot Learning

We further report the experiment results of generalized
zero-shot learning in Table 3 and Table 4. Table 3 shows the
results on aPaY dataset and Table 4 shows the results on the
other 4 datasets. In generalized zero-shot learning, the seen
classes are split into two parts: one for training and the other
for test. At the test stage, both seen and unseen samples
are recognized by searching the possible categories from
Y ∪ Yu. The splits of seen classes can be seen in Table 1
and more details can be found in previous work [35]. Since
both seen and unseen classes are tested in generalized zero-
shot learning, we also report the harmonic mean of seen
accuracy and unseen accuracy in the tables.

From the results in Table 3 and Table 4, we can draw
the similar conclusions as from Table 2. Our approach per-
forms better than existing methods. Our results are signif-
icantly better on the unseen samples and harmonic mean,
which means our proposed method has a much better gen-
eralized ability. It is able to classify the samples into the
true category. Our approach is stably dependable on both
seen and unseen classes. Although some previous meth-
ods, e.g., DAP, ESZSL and SAE, perform well on the con-
ventional zero-shot learning setting with unseen samples,
their performances degrade dramatically on the generalized
zero-shot learning. They tend to mess up when the possible
categories of unseen samples become large. Thus, the ap-
plicability of these methods is limited in real applications.

The harmonic mean is more stable regarding outliers

7407

cowhorsemotorbikepersonpottedplantsheeptraintvmonitordonkeygoatjetskistatuePredicted labelcowhorsemotorbikepersonpottedplantsheeptraintvmonitordonkeygoatjetskistatueTrue label0.030.100.020.030.040.160.030.020.350.180.00.060.010.130.030.040.040.170.010.020.260.130.090.070.00.00.770.00.020.00.030.040.00.00.130.020.010.00.070.160.140.010.020.470.00.00.090.020.00.00.040.050.170.010.020.660.00.00.050.00.030.050.00.030.050.050.020.00.180.390.00.200.00.00.030.00.010.00.900.040.00.00.020.00.00.00.010.00.050.010.050.850.010.00.010.00.010.270.00.00.010.020.00.010.360.260.050.010.030.120.00.00.00.040.00.00.580.180.00.050.00.00.010.00.020.010.140.090.00.00.680.040.00.010.00.290.040.040.070.270.00.00.070.190.00.20.40.60.81.0cowhorsemotorbikepersonpottedplantsheeptraintvmonitordonkeygoatjetskistatuePredicted labelcowhorsemotorbikepersonpottedplantsheeptraintvmonitordonkeygoatjetskistatueTrue label0.010.050.020.030.040.200.030.010.380.180.00.060.010.040.040.030.040.220.010.010.310.130.090.080.00.00.790.00.020.00.010.020.00.00.140.020.00.00.060.180.150.010.010.460.00.00.110.020.00.00.050.040.200.00.020.640.00.00.040.00.020.00.00.040.070.040.010.00.180.430.00.200.00.00.020.00.010.00.890.060.00.00.020.00.00.00.010.00.020.010.010.950.010.00.010.00.00.120.00.00.010.020.00.00.450.330.060.010.020.040.00.010.00.040.00.00.630.230.00.030.00.00.010.00.020.010.060.020.00.00.840.040.00.00.00.290.030.010.040.200.00.00.130.290.00.20.40.60.81.0Table 4. The results (top-1 accuracy %) of generalized zero-shot learning. The Mean in this table is the harmonic mean of seen and unseen
samples, i.e., Mean=(2*Unseen*Seen)/(Unseen+Seen). The best results are highlighted with bold numbers.

Methods

DAP [15]
CONSE [26]
SSE [38]
DeViSE [7]
SJE [2]
ESZSL [29]
ALE [1]
SYNC [4]
SAE [13]
DEM [37]
GAZSL [40]
f-CLSWGAN [34]
LisGAN [Ours]

Unseen

0.0
0.4
7.0
13.4
11.3
5.9
14.0
10.0
1.1
30.5
19.2
57.9
52.6

AwA
Seen Mean Unseen
88.7
88.6
80.5
68.7
74.6
77.8
81.8
90.5
82.2
86.4
86.5
61.4
76.3

0.0
0.8
12.9
22.4
19.6
11.0
23.9
18.0
2.2
45.1
31.4
59.6
62.3

1.7
1.6
8.5
23.8
23.5
2.4
4.6
7.4
0.4
11.1
23.9
43.7
46.5

-
-
-

CUB
Seen Mean Unseen
67.9
72.2
46.9
53.0
59.2
70.1
73.7
66.3
80.9
75.1
60.6
57.7
57.9

3.3
3.1
14.4
32.8
33.6
4.6
8.7
13.3
0.9
19.4
34.3
49.7
51.6

9.9
13.9
11.4
13.3

28.1
59.0
57.7

-
-
-

FLO
Seen Mean Unseen

-
-
-

44.2
47.6
56.8
61.6

-
-
-

77.4
73.8
83.8

-
-
-

16.2
21.5
19.0
21.9

-
-
-

41.2
65.6
68.3

4.2
6.8
2.1
16.9
14.7
11.0
21.8
7.9
8.8
20.5
21.7
42.6
42.9

SUN
Seen Mean
25.2
39.9
36.4
27.4
30.5
27.9
33.1
43.3
18.0
34.3
34.5
36.6
37.8

7.2
11.6
4.0
20.9
19.8
15.8
26.3
13.4
11.8
25.6
26.7
39.4
40.2

than the arithmetic mean and geometric mean. Thus, from
the results reported in Table 3 and Table 4, we can also
observe that our method is more stable than the compared
methods. It avoids extreme results on different evaluations.
In terms of the harmonic mean, we achieved up to 2.8%,
2.7%, 1.9%, 2.7% and 0.8% improvements on aPaY, AwA,
CUB, FLO and SUN, respectively. The average is over the
ﬁve is 2.2%. Although our method did not perform the best
on some seen categories, it performs almost neck to neck
with the previous state-of-the-arts. These results veriﬁed
the outstanding generalization ability of our method.

Considering the fact that both GAZSL and f-CLSWGAN
leverage GANs to synthesize unseen samples, the perfor-
mance boost of our method can be attributed to two aspects.
One is that we introduce soul samples to guarantee that each
generated sample is highly related with the semantic de-
scription. The soul samples regularizations also address the
multi-view characteristic. As a result, it can automatically
take care of the domain-shift problem caused by different
views in zero-shot learning. The other aspect is that our
cascade classiﬁer is able to leverage the results from the ﬁrst
classiﬁer and strengthen the second one. Such a formulation
provides the results via a coarse-to-ﬁne manner. The results
verify that it is beneﬁcial to leverage the invariant side of
generative ZSL. The invariant side regularizations guaran-
tee that each synthesized sample is highly related with the
real ones and corresponding semantic descriptions.

4.5. Model Analysis

In this section, we analyze our model under different set-
tings. Since our GAN generates visual features rather than
image pixels, it is inappropriate to show the synthesized re-
sults with images. We will analyze our model in terms of
the generalization ability and stability. The sensitivity of
hyper-parameters are also discussed.

4.5.1 Class-wise Accuracy

To show the experimental results of our method in a more
ﬁne-grained scale, we report the confusion matrix of f-
CLSGAN and our method on the aPaY dataset in Fig. 4.
Compared with Fig. 4(a) and Fig. 4(b), we can see that
our method generally has better accuracy on most of the
categories. Notably, we can see that the accuracy on cate-
gory “tvmonitor”, “donkey” and “jetski” are boosted around
10% against f-CLSWGAN. There is also a common phe-
nomenon that the ZSL methods perform poorly on some
unseen categories. We will investigate ﬁne-grained / class-
wise zero-shot learning in our future work.

4.5.2 Parameter Sensitivity

In our model, we have several hype-parameters to tune. The
parameter β controls the Lipschitz constraint. As suggested
in [9], we ﬁx β = 10 in this paper. The parameter λ bal-
ances the supervised classiﬁcation loss, its inﬂuence is re-
ported in Fig. 5(a). In our formulation, we also introduced
a weight coefﬁcient to adjust the contribution of soul sam-
ple regularizations. Its sensitivity is reported in Fig. 5(b).
Similarly, Fig. 5(c) and Fig. 5(d) show the effects of sam-
ple entropy threshold and synthesized sample numbers per
class, respectively. From the results, we can see that the
weight parameters for classiﬁcation loss and soul sample
regularization should be relatively small. The sample en-
tropy threshold is recommended to set to be smaller than
the median of all samples. The more synthesized samples,
the better results generally there will be. However, more
samples also introduce more noises and need more training
costs. In practice, we suggest to split the seen categories as
training set and validation set for cross-validation. Speciﬁ-
cally. we report the sensitivity of k in Fig. 7(a). Since k is
not sensitive, we ﬁx k = 3 to reduce the computation cost.

7408

(a) Classiﬁcation loss (λ)

(b) Soul sample regularization

(c) Sample entropy threshold

(d) Synthesized sample numbers

)

%

(
y
c
a
r
u
c
c
A

80

70

60

50

40

30

20

aPaY
AwA

)

%

(
y
c
a
r
u
c
c
A

80
75
70
65
60
55
50
45
40
35
30

aPaY
AwA

0.001

0.01

0.05

0.1

0.2

0.5

1

2

5

10

aPaY
AwA

)

%

(
y
c
a
r
u
c
c
A

80

70

60

50

40

30

20

)

%
(
y
c
a
r
u
c
c
A

80

70

60

50

40

30

20

10

aPaY

AwA

10 20 30 40 50 60 70 80 90

10 50 100 150 250 300 350 400 450

Weigh of soul samples loss

Ratio (%) of confident samples

# Generated features per class

0.001

0.01

0.05

0.1

0.2

0.5

1

2

5

10

Weigh of classification loss

Figure 5. Parameter sensitivity. The horizontal axis of (c) indicates the sample entropy threshold is not larger than the entropy of x%
samples where all sample entropies are sorted from small to large., e.g., 50 indicates the sample entropy threshold is set as the median of
all sample entropies. The horizontal axis of (d) indicates synthesized sample numbers per class.

(a) Zero-shot learning

(b) Generalized ZSL

aPaY

AwA

)

%

(
s
r
o
r
r
E

100

90

80

70

60

50

40

30

20

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

# epoch

)

%

(
s
r
o
r
r
E

100

90

80

70

60

50

40

30

20

aPaY

AwA

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

# epoch 

Figure 6. The trends of training stability. For GZSL in (b), we
report the harmonic mean on both seen and unseen samples.

)

%

(
y
c
a
r
u
c
c
A

80

75

70

65

60

55

50

45

40

35

30

aPaY

AwA

)

%

(
y
c
a
r
u
c
c
A

50

40

30

20

10

0

38.5

40.3

42.8

43.2

S1 (Setting 1): 

Plain  Conditional WGAN

S2 (Setting 2):

S1 + Classification Loss

S3 (Setting 3): 

S2 + Soul Samples 

Regularizations

S4 (Setting 4): 

S3 + Cascade Classifier

1

2

3

4

5

6

7

8

9

10

S1

S2

S3

S4

The values of k

(a)

Different settings

(b)

Figure 7. The results of different k (number of clusters) and abla-
tion analysis of ZSL with aPaY.

4.5.3 Model Stability

Since our approach deploys an adversarial training manner,
it needs several epochs to achieve the balance between the
generator and the discriminator.
In Fig. 6, we report the
zero-shot learning and generalized zero-shot learning re-
sults of our method with different epochs in terms of testing
error. The results reﬂect the training stability of our model.
It can be seen that our model shows a stable training trend
with the increasing of training epochs. Although there are
small ﬂuctuations, our model can achieve a stable results
with 30 epochs. For different real-world applications, one
can deploy cross-validation to choose the optimal epoch.

4.5.4 Ablation Analysis

Conditional WGAN has been a cutting-edge but popular
technique in computer vision tasks. It is more like an in-
frastructure in the community. Thus, we ﬁx the conditional
WGAN and focus on soul sample regularization and the

cascade classiﬁer in this section. We ﬁrst report the results
of plain conditional WGAN. Then, we introduce additional
components into the model and observe the effects of them.
The results of ablation analysis are reported in Fig. 7(b).
The ﬁve settings demonstrate that different components in
our framework are all signiﬁcant. The supervised loss guar-
antees that the generated features are discriminative. The
soul samples regularizations constrain that each synthesized
sample is close to the very semantic descriptions. Multiple
soul samples per class provide a relaxed solution to handle
domain shit problem caused by the multi-view issue. The
cascade classiﬁer leverages the result of sample entropy and
presents a more ﬁne accuracy.

5. Conclusion

In this paper, we propose a novel zero-shot learning
method by taking advantage of generative adversarial net-
works. Specially, we deploy conditional WGAN to syn-
thesize fake unseen samples from random noises. To guar-
antee that each generated sample is close to real ones and
their corresponding semantic descriptions, we introduce
soul samples regularizations in the GAN generator. At the
zero-shot recognition stage, we further propose to use a cas-
cade classiﬁer to ﬁne-tune the accuracy. Extensive experi-
ments on ﬁve popular benchmarks veriﬁed that our method
can outperform previous state-of-the-art ones with remark-
able advances.
In our future work, we will explore data
augmentation with GAN which can be used to synthesize
more semantic descriptions to cover more unseen samples.

Acknowledgments

This work was supported in part by the National Nat-
ural Science Foundation of China under Grant 61806039,
61832001, 61802236, 61572108 and 61632007, in part by
the ARC under Grant FT130101530, in part by the National
Postdoctoral Program for Innovative Talents under Grant
BX201700045, and in part by the China Postdoctoral Sci-
ence Foundation under Grant 2017M623006.

7409

References

[1] Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and
Cordelia Schmid. Label-embedding for image classiﬁcation.
IEEE TPAMI, 38(7):1425–1438, 2016.

[2] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and
Bernt Schiele. Evaluation of output embeddings for ﬁne-
grained image classiﬁcation.
In CVPR, pages 2927–2936,
2015.

[3] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein generative adversarial networks. In ICML, pages
214–223, 2017.

[4] Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, and Fei
Sha. Synthesized classiﬁers for zero-shot learning. In CVPR,
pages 5327–5336, 2016.

[5] Zhengming Ding, Ming Shao, and Yun Fu. Low-rank em-
bedded ensemble semantic dictionary for zero-shot learning.
In CVPR. IEEE, 2017.

[6] Zhengming Ding, Ming Shao, and Yun Fu. Generative zero-
shot learning via low-rank embedded semantic dictionary.
IEEE TPAMI, 2018.

[7] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio,
Jeff Dean, Tomas Mikolov, et al. Devise: A deep visual-
semantic embedding model.
In NIPS, pages 2121–2129,
2013.

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, pages
2672–2680, 2014.

[9] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Improved training of

Dumoulin, and Aaron C Courville.
wasserstein gans. In NIPS, pages 5767–5777, 2017.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[11] Huajie Jiang, Ruiping Wang, Shiguang Shan, Yi Yang, and
Xilin Chen. Learning discriminative latent attributes for
zero-shot classiﬁcation. In ICCV, pages 4223–4232, 2017.

[12] Elyor Kodirov, Tao Xiang, Zhenyong Fu, and Shaogang
Gong. Unsupervised domain adaptation for zero-shot learn-
ing. In ICCV, pages 2452–2460, 2015.

[13] Elyor Kodirov, Tao Xiang, and Shaogang Gong.

Se-
mantic autoencoder for zero-shot learning. arXiv preprint
arXiv:1704.08345, 2017.

[14] Christoph H Lampert, Hannes Nickisch, and Stefan Harmel-
ing. Learning to detect unseen object classes by between-
class attribute transfer.
In CVPR, pages 951–958. IEEE,
2009.

[15] Christoph H Lampert, Hannes Nickisch, and Stefan Harmel-
ing. Attribute-based classiﬁcation for zero-shot visual object
categorization. IEEE TPAMI, 36(3):453–465, 2014.

[18] Jingjing Li, Ke Lu, Zi Huang, and Heng Tao Shen. Two birds
one stone: on both cold-start and long-tail recommendation.
In ACM MM, pages 898–906. ACM, 2017.

[19] Jingjing Li, Ke Lu, Zi Huang, Lei Zhu, and Heng Tao
Shen. Heterogeneous domain adaptation through progres-
sive alignment. IEEE TNNLS, 2018.

[20] Jingjing Li, Ke Lu, Zi Huang, Lei Zhu, and Heng Tao Shen.
Transfer independently together: A generalized framework
for domain adaptation. IEEE TCYB, 2018.

[21] Jingjing Li, Lei Zhu, Zi Huang, Ke Lu, and Jidong Zhao. I
read, i saw, i tell: Texts assisted ﬁne-grained visual classiﬁ-
cation. In ACM MM. ACM, 2018.

[22] Yang Long, Li Liu, Yuming Shen, Ling Shao, and J Song.
Towards affordable semantic searching: Zero-shot. retrieval
via dominant attributes. In AAAI, 2018.

[23] Mehdi Mirza and Simon Osindero. Conditional generative

adversarial nets. arXiv preprint arXiv:1411.1784, 2014.

[24] Ashish Mishra, M Reddy, Anurag Mittal, and Hema A
Murthy. A generative model for zero shot learning using
conditional variational autoencoders. In CVPR, 2018.

[25] M-E. Nilsback and A. Zisserman. Automated ﬂower clas-
siﬁcation over a large number of classes. In ICVGIP, Dec
2008.

[26] Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram
Singer, Jonathon Shlens, Andrea Frome, Greg S Corrado,
and Jeffrey Dean. Zero-shot learning by convex combination
of semantic embeddings. arXiv preprint arXiv:1312.5650,
2013.

[27] Genevieve Patterson and James Hays. Sun attribute database:
Discovering, annotating, and recognizing scene attributes. In
CVPR, pages 2751–2758. IEEE, 2012.

[28] Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele.
Learning deep representations of ﬁne-grained visual descrip-
tions. In CVPR, pages 49–58, 2016.

[29] Bernardino Romera-Paredes and Philip Torr. An embarrass-
ingly simple approach to zero-shot learning. In ICML, pages
2152–2161, 2015.

[30] Yutaro Shigeto,

Ikumi Suzuki, Kazuo Hara, Masashi
Shimbo, and Yuji Matsumoto. Ridge regression, hubness,
and zero-shot learning.
In ECML-KDD, pages 135–151.
Springer, 2015.

[31] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical
networks for few-shot learning. In NIPS, pages 4077–4087,
2017.

[32] V Kumar Verma, Gundeep Arora, Ashish Mishra, and Piyush
Rai. Generalized zero-shot learning via synthesized exam-
ples. In CVPR, 2018.

[33] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011.

[16] Jimmy Lei Ba, Kevin Swersky, Sanja Fidler, et al. Predicting
deep zero-shot convolutional neural networks using textual
descriptions. In ICCV, pages 4247–4255, 2015.

[34] Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep
Akata. Feature generating networks for zero-shot learning.
In CVPR, 2018.

[17] Jingjing Li, Mengmeng Jing, Ke Lu, Lei Zhu, Yang Yang,
and Zi Huang. From zero-shot learning to cold-start recom-
mendation. In AAAI, 2019.

[35] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-
shot learning-the good, the bad and the ugly. arXiv preprint
arXiv:1703.04394, 2017.

7410

[36] Meng Ye and Yuhong Guo. Zero-shot classiﬁcation with
In CVPR,

discriminative semantic representation learning.
2017.

[37] Li Zhang, Tao Xiang, Shaogang Gong, et al. Learning a deep

embedding model for zero-shot learning. 2017.

[38] Ziming Zhang and Venkatesh Saligrama. Zero-shot learning
via semantic similarity embedding. In ICCV, pages 4166–
4174, 2015.

[39] Ziming Zhang and Venkatesh Saligrama. Zero-shot learning
via joint latent similarity embedding. In CVPR, pages 6034–
6042, 2016.

[40] Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, Xi Peng,
and Ahmed Elgammal. A generative adversarial approach
for zero-shot learning from noisy texts. In CVPR, 2018.

7411

