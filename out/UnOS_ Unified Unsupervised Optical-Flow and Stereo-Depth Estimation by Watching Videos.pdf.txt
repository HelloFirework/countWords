UnOS: Uniﬁed Unsupervised Optical-ﬂow and Stereo-depth Estimation by

Watching Videos

Yang Wang1 Peng Wang1 Zhenheng Yang2 Chenxu Luo3 Yi Yang1 Wei Xu1
1Baidu Research 2 University of Southern California
3 Johns Hopkins University

{wangyang59, wangpeng54, yangyi05, wei.xu}@baidu.com zhenheny@usc.edu chenxuluo@jhu.edu

Abstract

In this paper, we propose UnOS, an uniﬁed system for
unsupervised optical ﬂow and stereo depth estimation using
convolutional neural network (CNN) by taking advantages
of their inherent geometrical consistency based on the rigid-
scene assumption [31]. UnOS signiﬁcantly outperforms
other state-of-the-art (SOTA) unsupervised approaches that
treated the two tasks independently. Speciﬁcally, given two
consecutive stereo image pairs from a video, UnOS esti-
mates per-pixel stereo depth images, camera ego-motion
and optical ﬂow with three parallel CNNs. Based on these
quantities, UnOS computes rigid optical ﬂow and com-
pares it against the optical ﬂow estimated from the FlowNet,
yielding pixels satisfying the rigid-scene assumption. Then,
we encourage geometrical consistency between the two es-
timated ﬂows within rigid regions, from which we derive
a rigid-aware direct visual odometry (RDVO) module. We
also propose rigid and occlusion-aware ﬂow-consistency
losses for the learning of UnOS. We evaluated our results
on the popular KITTI dataset over 4 related tasks, i.e. stereo
depth, optical ﬂow, visual odometry and motion segmenta-
tion.

1. Introduction

Estimating stereo depth [23] and optical ﬂow [19] are
two fundamental problems in computer vision. Jointly con-
sidering the two provides dense 3D scene ﬂow [34], which
enables numerous applications such as autonomous driving
[34], robot navigation [7, 12] and video analysis [42, 25].

Current state-of-the-art (SOTA) strategies for both tasks
rely on the advance of CNNs with supervised learning, e.g.
PSMNet [5] and PWCNet [40], which depend heavily on
the availability of training data[11, 32]. However, videos
are from various scenes when considering open-world prob-
lems [58], so it is not practical to collect dense ground truths
for these tasks at every place. Therefore, lots of efforts and
progresses have been made recently in unsupervised learn-
ing of stereo depth/matching [57], monocular depth estima-
tions [60] and optical ﬂow [38] with CNNs by just providing

Figure 1. Comparison between UnOS and other unsupervised
methods.
(a) left image, (b) optical ﬂow from [33], (d) UnOS
optical ﬂow, (c) stereo depth from [14], (e) UnOS stereo, It can be
seen for both optical ﬂow and stereo depth, UnOS generated re-
sults are more regularized and have sharper boundaries following
scene structures as shown in zoomed regions (best view in color).

stereo pairs or videos. These methods vastly improve the
generalization ability of the learned models. Nevertheless,
in those works, the two tasks were mostly treated indepen-
dently in their pipeline, although it has shown to be very
useful to consider both of the tasks as a whole in the aspect
of 3D scene ﬂow in traditional methods [43, 35, 41].

This paper completes this missing piece of unsupervised
learning by proposing joint learning of the two tasks, which
explores their geometrical relationship during training, and
boosts the performance on both sides as illustrated in Fig. 1.
We provide an overview of our system in Fig. 2. During
training, given two consecutive stereo image pairs, i.e. (Lt,
Rt) and (Ls, Rs), UnOS jointly outputs stereo depth esti-
mation (Dt), camera ego-motion (Tt→s) and optical ﬂow
(Ft→s) from StereoNet, MotionNet and FlowNet respec-
tively. Then, a rigid-aware direct visual odometry (RDVO)
module is applied after the MotionNet, which reﬁnes and
updates the camera motion (Tu
t→s). Next, we use Dt and
Tu
t→s to compute rigid ﬂow Fr
t→s representing the motion
induced solely by the camera, which is compared against
Ft→s and yields a rigid mask M. In addition to the individ-
ual loss for each network, Fr
t→s and Ft→s are encouraged
to be consistent within rigid regions M, yielding more ro-
bust estimation for both tasks.

Our contributions are summarized as below,

8071

Figure 2. UnOS system. Given two consecutive stereo pairs, optical ﬂow Ft→s, stereo depth Dt, camera motion Tt→s are predicted from
three networks. Potentially rigid pixels are then discovered, and a rigid-aware direct visual odometry (RDVO) module is designed to reﬁne
the camera motion. All of the information is sent to our full set of losses LU nOS with rigid awareness, occlusion (Occ.) awareness and
ﬂow-depth consistency (Details in Sec. 4.1). Please note that here the term FlowNet refers to the network for estimating optical ﬂow which
we used PWCNet [40] in our work.

1. We design an uniﬁed framework for unsupervised
learning of optical ﬂow and stereo depth, named as
“UnOS”, by explicitly encouraging their geometrical
consistency with automatically found rigid regions,
yielding SOTA performance on both tasks.

2. We design a rigid-aware direct visual odomotry
(RDVO) module that carefully handles rigid regions
using optical ﬂow matching, yielding more accurate
camera motion estimations.

3. We jointly include the properties of rigidity and oc-
clusion in our training schema, which is effective for
learning the CNNs.

UnOS signiﬁcantly improves over other unsupervised
stereo depth and optical ﬂow methods. For example, on
KITTI 2012 benchmark, UnOS reduces the optical ﬂow er-
ror from a previous unsupervised method [33] by 50%. For
stereo depth, it also outperforms the SOTA method taken
stereo video as an input [57]. UnOS also achieves bet-
ter performance for unsupervised moving object segmen-
tation when comparing against [52]. The code and models
of our method can be found at https://github.com/
baidu-research/UnDepthflow .

2. Related Work

Stereo matching and optical ﬂow estimation have long
been important problems for computer vision. Here we
summarize the closely related works using deep CNNs due
to space limitation. We refer readers to survey papers on
both tasks [9, 16] for broader understanding.

Supervised optical ﬂow and stereo depth. In general, the
two tasks share the same methodology for ﬁnding dense
pixel correspondences, where stereo depth is a more con-
strained problem. Therefore, here we review them as a
whole since an optical ﬂow method can be easily extended
to stereo matching by limiting the search within a dispar-
ity line. Based on CNNs, early works [55, 8, 28, 39, 15]
started to learn matching for stereo using various losses with
image patches as input, which might be time consuming

during both training and inference. Recently, works like
SPyNet [36] was designed to ﬁnd 2D optical ﬂow by ex-
plicitly using image warping in the architecture to enable
efﬁcient learning. PWCNet [40] built a 3D cost volume
calculated within a local region. Despite the limited range
in matching, PWCNet achieved SOTA optical ﬂow results
thanks to the coarse-to-ﬁne scheme used.

To fully exploit the limited dimension and matching
range in stereo depth, researchers built more speciﬁc ar-
chitectures and losses. GCNet [23] proposed to generate
a 3D cost volume by densely comparing the feature at a
pixel from the reference image to all possible matching pix-
els at the target image. The network ﬁnds the best match-
ing through a sof t-argmin operation. PSMNet [5] adopted
pyramid spatial pooling and hourglass networks for exploit-
ing image context. Later work [6] appended a post process-
ing module, yielding better recovered details. These net-
work architectures provide strong foundation for develop-
ing unsupervised learning methods.

Unsupervised optical ﬂow. To reduce the requirements
for large amount of training data, unsupervised optical ﬂow
learning was introduced recently in [38] and [22], where the
basic idea was to use the spatial transform network [20] to
backprop the photometric matching error from comparing
the original and warped target images. Later works [46, 33]
improved their results by explicitly handling the occlusions.
In our case, we introduce geometrical regularization by
jointly considering stereo depth, which produces further im-
provements.

Unsupervised monocular and stereo depth. Unsuper-
vised learning of depth was ﬁrst introduced for monocu-
lar images based on the supervision via stereo image pairs.
Speciﬁcally, recent works [50, 10] adopted a CNN to take a
single image as input and predict its disparity, where the su-
pervision came from the photometric comparison. It was
later improved by using inherent geometrical regulariza-
tions [14, 29]. Zhou et al. [60] incorporated camera ego-
motion into the training pipeline using structure from mo-

8072

tion (SfM) [48], which made depth learning possible from
monocular videos. Later works improved the performance
by regularizing scene structures [53], reﬁning camera ego-
motion [45, 30], and jointly using stereo and monocular
video for learning [26, 13]. Here, our RDVO is motivated
by Differentiable Direct Visual Odometry (DDVO) [45].
However, rather than using photometric distance in solv-
ing relative camera pose, RDVO relies on estimated optical
ﬂow within rigid regions for pixel matching.

Due to the success of monocular depth estimation, re-
searchers extended the corresponding losses to the problem
of stereo depth estimation [14, 59], where the correspond-
ing network architectures were borrowed from those shown
to be effective with supervised learning such as GCNet [23].
These methods showed signiﬁcant performance boost over
the traditional unsupervised stereo algorithms based on lo-
cal patch-wise matching and smoothing [18, 4, 27]. In our
case, we adopt more light-weight PWCNet for stereo depth
estimation by limiting the matching space.

Most recently, leveraging video for unsupervised stereo
depth estimation was also proposed by Zhong et al. [58],
where a RNN was used to implicitly aggregate the informa-
tion from previous frames. Therefore, a video sequence is
necessary for testing. In our case, we explicitly account for
the depth transformation between consecutive frames with
camera motion and optical ﬂow in training, and only need a
stereo pair for testing.

Joint unsupervised learning of depth and ﬂow. Under-
standing depth and ﬂow jointly from a video is commonly
known as 3D scene ﬂow estimation [43, 44], where 2D op-
tical ﬂow is explained with 3D scene structures and cam-
era geometry. Recently traditional methods for scene ﬂow
estimation using stereo videos rely on bottom-up super-
pixel piece-wise planar matching [34], or top-down recog-
nition [2, 49]. Taniai et al. [41] accelerated these algorithms
with per-pixel scene ﬂow understanding by jointly enforc-
ing consistency among stereo depth, camera motion and op-
tical ﬂow. However, there were no learning components in-
troduced in their systems.

Within the scope of unsupervised deep learning, joint
depth and optical ﬂow learning was studied based on
monocular videos. GeoNet [54] used a residual FlowNet to
reﬁne the rigid ﬂow from depth and ego-motion to the full
optical ﬂow, but no explicit geometry consistency was con-
sidered and it did not explicitly distinguish between static
and moving regions. EPC [52] discovered rigid regions and
encourage consistency between depth and ﬂow estimations,
but it did not do joint learning. Recent work [37] pieced the
optical ﬂow and rigid ﬂow together and did iterative learn-
ing for reﬁnement. DF-net [61] also proposed a consistency
loss between rigid ﬂow and optical ﬂow. However, neither
of them [37, 61] showed much improvements on the ﬂow
task due to the intrinsic limitation of the monocular depth

accuracy. As mentioned in Sec. 1, including stereo depth es-
timations in our system fundamentally facilitates the learn-
ing of the two tasks.

3. Learning with self-supervision

In order to make the paper self-contained, we ﬁrst intro-
duce the preliminaries for unsupervised stereo depth [14],
monocular depth [60] and optical ﬂow [38] estimation,
which share similar underlying idea of supervision by syn-
thesis.

Finding corresponding pixels. As introduced in Sec. 1, we
take consecutive stereo image pairs, (Lt, Rt) and (Ls, Rs)
as inputs, where L, R indicate the left and right image re-
spectively, and t, s indicate the target and source image. The
networks estimate a stereo depth map Dt using (Lt, Rt), a
relative camera pose Tt→s ∈ SE(3), and an optical ﬂow
map Ft→s using (Lt, Ls). For each pixel pt in a target im-
age Lt, we can ﬁnd the corresponding source pixels by,

prs = π(K[Tt→sφ(pt|K, Dt)]),
pf s = pt + Ft→s(pt),
px
ss = px

t − f · B/Dt(pt)

(1)

where prs represents the pixel found at Ls based on the rigid
scene assumption and camera motion, pf s represents the
pixel found at Ls through optical ﬂow, and pss represents
the pixel found at Rt via stereo disparity (the superscript x
speciﬁes the horizontal component). Here, φ(pt|K, Dt) =
Dt(pt)K−1h(pt) is a back-projection function mapping a
2D pixel to a 3D point. h(pt) is the homogeneous coor-
dinate of pt. π([x, y, d]) = [x/d, y/d]T returns 2D non-
homogeneous coordinates. K is the camera intrinsic ma-
trix, and f, B are the focal length and baseline of the stereo
image pair.

Supervise with view synthesis. Given the corresponding
pixel pairs pt and p∗s (∗ could be r, f or s in Eq. (1)),
we may generate synthesized target images ˆL∗t from var-
ious source images using a differentiable bilinear interpo-
lation [20], and the system can be trained by minimizing
photometric error. The corresponding loss function term is
deﬁned as,

L∗v(O) = Xpt

V∗(pt, O)|Lt(pt) − ˆL∗t(pt, O)|.

(2)

where V∗(pt, O) is a visibility mask, indicating whether pt
can ﬁnd a valid matching pixel given certain information O
and a source image. O could be depth Dt or optical ﬂow
Ft→s. Here, the visibility mask V∗ is computed by forward
warping of the reverse optical ﬂow as proposed in [46].
Thus, adopting different matching pairs triggers different
unsupervised learning pipelines, e.g. using prs, pss or pf s

8073

Figure 3. An example of our rigid potential. (a) Image. (b) Flow consistency map. (c) Visibility mask Vf . (d) Rigid potential. (e) Ground
truth rigid mask. We can see ﬂow consistency falsely indicates rigid potential in occluded regions.

induces mono-depth [60], stereo-depth [14], and optical-
ﬂow [38] respectively. Using both prs and pss leads to deep
visual odometry with stereo video [26].

Regularization with edge-aware smoothness. Pixel color
matching alone is unstable and ambiguous. Therefore, an
edge-aware smoothness term is often applied for each pre-
diction. Speciﬁcally,

Ls(O, W, o) = X

X

pt

d∈x,y

W(pt)|∇o

dO(pt)|e−β|∇2

dLt(pt)|

(3)

where O represents the type of the input, W is a weight
map, and o is the order of smoothness gradient. For exam-
ple, Ls(Dt, 1, 2) is a spatial smoothness term penalizing the
L1 norm of second-order gradients of depth Dt along both
x and y directions over all images, as proposed by [14].

4. Unifying optical ﬂow and stereo depth

One possible approach for unifying the learning of depth
and ﬂow is to use matching pixels of prs, pss and pf s to-
gether during training. However, it may not work well, as
also mentioned in prior works [26, 30], since errors from
one task may negatively impact the other. This is mainly
because there are moving things from t to s, and pixels be-
longing to those regions fail the one rigid motion assump-
tion handling only the ego-motion. Thus, the discovered
prs will be different from pixels found by optical ﬂow pf s.
This systematic error will affect the learning of the whole
model. Therefore, one key to successfully unify learning of
both tasks is to ﬁnd pixels having high potential satisfying
the rigid assumption.

Locating rigid regions with soft potential. Here, rather
than using a hard binary rigid mask as in [37], we consider
using a soft rigid region mask [60], where each pixel has
a potential of satisfying the rigid assumption. This will be
useful in our RDVO module and losses later. In particular,
the rigid potential at a pixel pt is computed as,

Rt(pt) = max{1 − Vf (pt), exp{−γ(|pf s − prs|)}}

(4)

where γ is a hyper parameter. Here, We ﬁrst check the con-
sistency between pf s and prs, and also consider the regions
that are occluded 1 − Vf (pt) as rigid. For example, re-
gions that become occluded at image boundaries or road oc-
cluded by moving cars should be considered as rigid. Fig. 3
visualizes an example of our soft rigid mask, where more
complete rigid regions are discovered using the two criteria.
One possible error here could be having mutual occlusions

between moving objects, which will be considered in our
future work.

From Eq. (1) by letting prs = pf s, we can see optical
ﬂow Ft→s, depth Dt and camera motion Tt→s turn out to
be three conjugated quantities within rigid regions. Given
Dt and Ft→s, we can apply the n-point algorithm [17] to
solve for Tt→s with a closed-form using SVD, based on
which we later propose rigid-aware direct visual odometry
(RDVO) for pose reﬁnement. It reﬁnes the camera pose ob-
tained from the MotionNet. Given the reﬁned camera pose
Tu
t→s, Dt(pt) and Ft→s, we propose to include geometri-
cal consistency in our loss design. These two components
will be elaborated below.

4.1. Rigid aware direct visual odometry (RDVO)

In this module, given estimated Dt, Ft→s and an initial
estimation of camera pose Tt→s from MotionNet, our tar-
get is to ﬁnd a relative pose ∆Tt→s to reﬁne the pose Tt→s.
This is necessary because MotionNet itself lacks geometri-
cal constraints, which was also mentioned in [45, 30]. Here
we propose a simpler and more efﬁcient solution using the
discovered rigid potential.

Speciﬁcally, the target of RDVO based on the notation

from Eq. (1) is,

min
∆Tt→s

X

pt∈S

kprs − pf sk2

(5)

By substituting corresponding items in Eq. (1) yields,

pf s − prs = pf s − π(K[∆Tt→sTt→sφ(pt|K, Dt)])

⇔ φ(pf s|K, Ds) − ∆Tt→sTt→sφ(pt|K, Dt)
= ψ(pf s|Ds)K−1h(pf s) − ∆Tt→sTt→sDtK−1h(pt)

(6)

which means we back project 2D pixels to 3D point cloud
for optimization. Here, ψ(pf s|Ds) is a bilinear interpola-
tion operation returning depth value at ﬂoat coordinate pf s
using the depth map Ds from source images. Note pf s does
not necessarily have discrete values, therefore an interpola-
tion is needed. ⇔ means 2D to 3D projection. Now, Eq. (5)
is a standard L2 minimization problem which can be easily
solved using SVD [3]. In practice, computing pose can be
more accurate by selecting the most reliable matching for
visual odometry [1] rather than using all pixels. Therefore,
S in Eq. (5) is chosen with two criteria, (1) Vf (pt) > 0.75
since only pixels without occlusion are valid for matching.
(2) the potential Rt(pt) is within top 25%. Here we choose
these parameters based on the corresponding validation set.

8074

Figure 4. Left column: left target images (Lt). Right column: the
regions selected in the RDVO module (i.e., region S described in
Eq. (5)) in green overlaying on the ground truth moving object
mask in grey.

Fig. 4 visualizes the selected pixels (green) in S for min-
imization. We see that the selected pixels (green) clearly
separate from the moving objects (grey).

After RDVO, we obtain an updated camera motion
Tu
t→s = ∆Tt→sTt→s, which we can feed back to calcu-
late rigid matching of prs, yielding a better rigid potential
(Eq. (4)). We may iterate this process till convergence, and
use the updated pu
rs for generating various losses. In prac-
tice, we iterate twice for each sample and found it is already
good enough in achieving SOTA results.

Finally, based on our rigid potential, we may generate a

rigid segmentation mask with a threshold,

Mt = Rt(pt) > 0.5,

(7)

which distinguishes the regions of static background and
moving objects, and will be applied later for training the
networks.

4.2. Learning with geometrical consistency

In this section, we discuss on how to leverage consis-
tency in our losses and network architectures to effectively
supervise UnOS.

4.2.1 Training losses

Rigid and occlusion-aware structural matching. As dis-
cussed in Sec. 3, photometric matching Eq. (2) follows
Lambersian assumption based on pixel colors, which is
not robust against illumination variations. To capture lo-
cal structures, following [14], we add structural matching
cost from SSIM [47]. Speciﬁcally, our pixel matching loss
is,

L∗v(O) = Xpt
where, s(L(p), ˆL(p)) = (1 − α) · |L(p) − ˆL(p)|+

V∗(pt, O) · s(Lt(pt), ˆL∗t(pt, O)),

α · (1 −

1
2

SSIM(L(p), ˆL(p))).

(8)

Here, α is a balancing hyper-parameter. Same as in Eq. (2),
O represents the type of output we need to supervise,
which could be stereo depth estimations Dt or optical ﬂow
Ft→s. V∗ indicates visibility mask depending on the type

of source image for synthesis. Speciﬁcally, for supervis-
ing with stereo pairs, ˆL∗t is from pss, Vs is computed
using disparity. For supervising optical ﬂow, ˆL∗t is from
pf s, Vf is computed based on using backward optical ﬂow,
i.e. Fs→t [46]. For supervising with consecutive images,
i.e. ˆL∗t is from prs (before RDVO), Vr represents rigid
and non-occluded regions, which is computed as Vr =
Vf ⊙ Mt. We denote different view synthesis loss terms
as Lsv, Lf v, Lrv respectively.

In addition, as mentioned in Sec. 4.1, we also obtain a
rs after RDVO through an optimized
t→s yielding a new structural matching
r is computed accord-

better matching pixel pu
camera motion Tu
loss, which we denote as Lu
ingly.

rv and Vu

Edge-aware local smoothness. We adopt similar smooth-
ness loss functions as formulated in Eq. (3). Speciﬁcally,
for depth, we follow [14] and use Lss = Ls(Dt, 1, 2),
which penalize the second-order gradient of depth. For op-
tical ﬂow, we choose to smooth over the moving regions,
i.e. Lf s(Ft→s, 1 − Mt, 2).

Rigid-aware ﬂow consistency Given updated camera mo-
tion Tu
t→s after RDVO, we then further encourage consis-
tency between rigid ﬂow and optical ﬂow. The consistency
loss is formulated as,

Lf c(F, D, Tu) = X

Mt(pt)|pu

rs − pf s|

(9)

pt

where Mt(pt) is the rigid mask computed in Eq. (7). Since
our RDVO is not differentiable, this consistency loss only
supervises FlowNet and StereoNet.

Left-right consistency Given stereo pairs, Godard et
al. [14] showed that jointly predicting depth for both left
and right images, and checking their consistency helps
depth learning. We also include such loss for our StereoNet,
which is denoted as Lsc.

In summary, our loss functional for UnOS is written as,

LU nOS = (Lf v + λf sLf s) + λrv(Lrv + Lu
+ (λsvLsv + λssLss + λscLsc) + λf cLf c

rv)

(10)

λ = [λf s, λrv, λsv, λss, λsc, λf c]
parameters balancing different losses.

is the set of hyper-

4.2.2 Network Architectures.

As reviewed in Sec. 2, SOTA stereo depth and optical
ﬂow algorithms are able to share similar architecture and
methodology.
In our work, due to joint multi-task train-
ing, we prefer more light-weighted architectures in order
to ﬁt everything into a single GPU. Therefore, for handling
stereo matching, rather than using a stronger but relative
heavy network, e.g. GCNet [23] or PSMNet [5], we choose
PWCNet[40] used in the optical ﬂow estimation, which is

8075

Method

Train
Stereo

Test
Stereo

Super-
vised

train
Noc

X

X

X

X

–
–
–
–

1.26

–
–
–
–
–

–
–
–
–
–
–
–
–
–
–

KITTI 2012
train
train
Occ
All
4.09
(1.28)
4.14
(1.45)
3.29

–
–

3.55

–

3.54
2.68
2.86
1.93
1.64

KITTI 2015
train
all

train
static

train
move

–
–
–
–
–
–

–
–
–
–
–
–

6.35

6.16

–
–
–

5.92
35.9
36.5
5.30

–
–
–

7.68
4.53
2.99
5.39

10.06
(2.3)
10.35
(2.16)
8.10
10.81
7.76
8.88
6.59
8.98
7.88
11.9
10.69
5.58

test
All
–
1.8
–
1.7
–
–
–
4.2
–
4.4
3.2
3.1
2.1
1.8

test
all
–

11.48%

–

9.60%

–
–
–

31.2%
22.94%
25.70%
23.75%
43.86%
32.34%
18.00 %

X

X

X

X

X

1.15
2.27
1.46
1.04

11.2
6.67
4.88
5.18

Flownet2
Flownet2+ft
PWC-Net
PWC-Net+ft
UnFlow-CSS [33]
Geonet [54]
Ranjan et al. [37]
Wang et al. [46]
Janai et al. [21]
DF-net [61]
UnOS (FlowNet-only)
UnOS (Ego-motion)
UnOS (Ego+RDVO)
UnOS (Full)

Table 1. Quantitative evaluation on the optical ﬂow task. The numbers reported here are all average end-point-error (EPE) except for the
last column (KITTI2015 test) which is the percentage of erroneous pixels (Fl-all). A pixel is considered to be correctly estimated if the
ﬂow end-point error is <3px or <5%.

light-weighted and also achieves good performance. Dif-
ferently, we modify PWCNet [40] to exploit the epipolar
geometry constraints, i.e. pss in Eq. (1) can only be found
along the horizontal axis and at the left side of pt. There-
fore, we limit the search range to the horizontal line in the
cost volume, and the value of horizontal ﬂow to be negative.
The structure of MotionNet is similar to the one used in
[60] except our network only takes two consecutive images
as the input instead of three or ﬁve images, and has two
more convolutional layers. We use PWCNet for optical ﬂow
estimation.

The whole training process includes three stages. First,
we train the FlowNet using Lf v, Lf s. At the second
stage, we train the StereoNet and MotionNet jointly us-
ing Lsv, Lss, Lsc, Lrv without RDVO or ﬂow consistency.
The two pre-training stages provide us a reasonable opti-
cal ﬂow and stereo depth estimation. At the last stage, we
add our RDVO module and consistency terms, and train all
networks together using the total loss LU nOS.

For inference, we obtain optical ﬂow Ft→s and stereo
depth Dt directly from the corresponding networks, and
obtain camera motion Tu
t→s after RDVO. Moving object
segmentation is computed by 1 − Mt.

rameters, and [λf s, λrv, λsv, λss, λsc, λf c] are set to be
[10.0, 10.0, 1.0, 10.0, 1.0, 0.01] by balancing the scale of
various losses without too much tuning.

During training, we use a batch size of 4. In each stage,
we train for around 15 epochs and choose the model with
the best validation accuracy for the start of next stage train-
ing. Images are scaled to have values between 0 and 1, and
size of 832 × 256. The only data augmentation we perform
is random left-right ﬂipping and random time order switch-
ing.

Dataset Following previous works [14, 46, 60, 52], for the
depth, optical ﬂow and segmentation tasks, we train our
networks using all of the raw data in KITTI excluding the
scenes appeared in the training set of KITTI 2015 [34],
which we adopt as our validation set and use to compare
with other methods. We also evaluate UnOS on KITTI 2012
[11] to additionally verify our algorithm. For segmentation,
we only evaluate on KITTI 2015 since there is no moving
things in KITTI 2012. For the odometry task, we use the
ofﬁcial odometry split, i.e. using sequences 00-08 as train-
ing and sequences 09, 10 as validation. All of our models
are trained from scratch in a pure unsupervised manner.

5. Experiments

5.1. Evaluation

We evaluate UnOS on the KITTI dataset with multiple
types of ground truth, and compare our results to existing
supervised and unsupervised SOTA methods on the tasks
of optical ﬂow, stereo depth, visual odometry and motion
segmentation.

Training Details In all of training stages, we used Adam
optimizer [24] with β1 = 0.9 and β2 = 0.999. The
learning rate is set to be 10−4. The hyper-parameters
β = 10.0 in Eq. (3), α = 0.85 in Eq. (8), γ =
0.17 in Eq. (4).
For the loss functional in Eq. (10),
we borrow the parameters from [14] for stereo losses pa-

Optical ﬂow. We evaluate our method on the optical
ﬂow estimation task using both KITTI 2012 and KITTI
2015, and the quantitative results are shown in Tab. 1.
UnOS (FlowNet-only) is our baseline model after training
FlowNet in the ﬁrst stage. We could see that it is better than
one of the unsupervised optical ﬂow method UnFlow-CSS
[33] demonstrating the effectiveness of our occlusion-aware
loss and PWC network structure. “UnOS (Ego-motion)” is
the result of rigid ﬂow, i.e. computing ﬂow using prs − pt,
at the end of the second stage training. The rigid ﬂow is
shown to be better than the previous general optical ﬂow in

8076

Method

EPC [52]
Zhou et al. [59]
SegStereo [51]
Godard et al. [14]
Zhong et al. [57]
OpenWorld [58]
UnOS (Stereo-only)
UnOS (Ego-motion)
UnOS (Full)
PSMNet

X

X

X

X

X

X

X

X

X

X

Train
Stereo

Test
Stereo

Super-
vised

Lower the better

Higher the better

Abs Rel

0.109

Sq Rel
1.004

RMSE
6.232

RMSE log

D1-all

δ < 1.25

δ < 1.252

δ < 1.253

0.203

–

0.853

0.937

0.975

X

X

X

X

X

X

X

X

X

–
–

0.068
0.075
(0.056)
0.060
0.052
0.049

–

–
–

0.835
1.726
(0.692)
0.833
0.593
0.515

–

–
–

4.392
4.857
(3.176)
4.187
3.488
3.404

–

–
–

0.146
0.165
(0.125)
0.135
0.121
0.121

–

9.41%
8.79%
9.194%
6.424%
(5.140%)
7.073%
6.431%
5.943%
1.83%

–
–

0.942
0.956
(0.967)
0.955
0.964
0.965

–

–
–

0.978
0.976

–

0.981
0.985
0.984

–

X

–
–

0.989
0.985

–

0.990
0.992
0.992

–

Table 2. Quantitative evaluation of the stereo depth task on the KITTI2015 training set. Abs Rel, Sq Rel, RMSE, RMSE log, δ <
1.25, 1.252, 1.253 are standard metrics for depth evaluation [60]. We capped the depth to be between 0-80 meters to compare with existing
literature. D1-all is the error rate of the disparity. Please note that the OpenWorld results were obtained by directly training on the
KITTI2015 training set by replicating the images 200 times to form a pseudo-video, therefore not directly comparable with other methods
which held out that dataset.

Method

frames

Stereo

Sequence 09

Sequence 10

ORB-SLAM(Full)

All

0.014 ± 0.008

0.012 ± 0.011

Zhou et al. [60]

Geonet [54]

Mahjourianet al. [30]

Adv. [37]

UnOS (MotionNet)

UnOS (+RDVO)

5

5

3

5

2

2

0.016 ± 0.009

0.013 ± 0.009

0.012 ± 0.007

0.012 ± 0.009

0.013 ± 0.010

0.012 ± 0.011

0.012 ± 0.007

0.012 ± 0.008

X

X

0.023 ± 0.010

0.022 ± 0.016

0.013 ± 0.006

0.015 ± 0.010

Figure 5. Visualization of the ﬂow error map for different stages in
the training. Color legend for errors is plotted at the bottom.

occluded (6.67 vs. 11.2) and static (4.53 vs. 7.68) regions.
This observation is consistent with our assumption about
the advantage of the rigid ﬂow in those areas, and provides
motivation for our proposed ﬂow consistency loss (Lf c).
The rigid ﬂow is much worse in moving regions which is
expected since it is only supposed to be accurate in static
regions. “UnOS (Ego+RDVO)” is the result of reﬁned rigid
ﬂow, i.e. computing ﬂow using pu
rs − pt after RDVO with-
out the third stage training. The result shows that the rigid
alignment module signiﬁcantly improves the rigid ﬂow in
static regions (1.93 vs. 2.86 and 2.99 vs. 4.53). “UnOS
(Full)” represents our optical ﬂow estimation at the end of
the third training stage with ﬂow consistency.
It is still
worse than rigid ﬂow after RDVO in static regions but has
the best overall performance. For KITTI 2012, our method
reduces the error from previous unsupervised method [33]
by 50%, and reaches similar performance of the supervised
methods [19], which demonstrates the beneﬁts of our pro-
posed method and the utilization of stereo data. For KITTI
2015, our method also outperforms previous unsupervised
methods by a large margin, although it still lags behind the
corresponding supervised methods [40]. Visualization of
our estimated optical ﬂow can be found in Fig. 6, and we
can see that our results are more regularized with sharper
boundaries.

We also show the error map of optical ﬂow from dif-

UnOS (Full)

0.013 ± 0.008
Table 3. Quantitative evaluation of the odometry task using the
metric of the absolute trajectory error.

0.012 ± 0.006

2

X

Method

Sequence 09

Sequence 10

terr% rerr(◦/100)

terr% rerr(◦/100)

ORB-SLAM(Full)

Zhan et al. [56]

UnOS (MotionNet)

UnOS (+RDVO)

UnOS (Full)

15.30

11.92

13.98

8.15

5.21

0.26

3.60

5.36

3.02

1.80

3.68

12.62

19.67

9.54

5.20

0.48

3.43

9.13

4.80

2.18

Table 4. Quantitative evaluation of the odometry task using the
metric of average translational and rotational errors. Numbers of
ORB-SLAM (Full) are adopted from [56].

Initially,

ferent training stages in Fig. 5 (bluer means better while
redder means worse).
rigid ﬂow from ego-
motion ‘UnOS(Ego-motion)’ has worse performance than
‘UnOS(FlowNet-only)’. After adding RDVO, we can see
that the ﬂow estimations in the static region are greatly
improved (comparing green circles in (b) and (c)). After
adding the moving object mask and applying the consis-
tency loss, ‘UnOS(Full)’ shows even better results in both
static and occluded regions compared to our baseline (com-
paring green circles in (a) and (d)).

Stereo-depth. We evaluate our depth estimation on the
KITTI 2015 dataset, and show the resutls in Tab. 2. Here,
the numbers of Zhong et al. [57] and OpenWorld [58] were
obtained through private communications with the authors.
“UnOS (StereoNet-only)” is the StereoNet trained using
only stereo images, and is our baseline algorithm. It is al-

8077

Figure 6. Qualitative results of UnOS. We compare each of our output to previous SOTA results. Speciﬁcally, (a) Godard et al. [14], (b)
UnFlow-CSS [33], (c) EPC [52].

ready better than some of the existing unsupervised stereo
depth algorithms [14, 51] demonstrating the effectiveness of
our StereoNet. Our stereo depth also performs much better
than the SOTA monocular depth method [52]. “UnOS (Ego-
motion)” shows the results at the end of our second training
stage. After adding the data of time consecutive images,
the depth accuracy improves especially in the large distance
regions (0.593 vs. 0.833). “UnOS (Full)” shows the re-
sults after using RDVO with rigid-aware ﬂow consistency,
and gives the best performance. However, its performance
is still worse than the supervised method like PSMNet [5].
We provide 3D scene ﬂow evaluation on KITTI 2015 test
set in the supplementary materials, and the qualitative re-
sults of our estimated depth are shown in Fig. 6, where
UnOS ﬁgures out better scene structures with less noise.

Visual odometry. We evaluate camera motion using two
commonly adopted metrics. The ﬁrst one was proposed in
SfMLearner [60] which measures the absolute trajectory er-
ror averaged over all overlapping 5-frame snippets after fac-
tor rescaling with the ground truth. In our case, we only
have two frames as input to the MotionNet to predict their
relative pose. For evaluation, we accumulate 4 consecutive
predictions to get the result for the 5-frame snippet. The
other metric was proposed in [56] which measures the av-
erage translation and rotation errors for all sub-sequences
of length (100, 200, ..., 800). For this metric, we accu-
mulate all of two frames estimations together for the entire
sequence without any post-processing. The results for the
two metrics are shown in Tab. 3 and Tab. 4 respectively.
We can see direct output from MotionNet (UnOS (Motion-
Net)) is not satisfying, and is much worse than other SOTA
methods. However, after RDVO module, we see signiﬁcant

Method
EPC [52]

UnOS (Full)

Pixel Acc. Mean Acc. Mean IoU f.w. IoU

0.89
0.90

0.75
0.82

0.52
0.56

0.87
0.88

Table 5. Motion segmentation evaluation. The metrics are pixel
accuracy, mean pixel accuracy, mean IoU, and frequency weighted
IoU.

improvements (UnOS (MotionNet+RDVO)). After training
with the ﬂow consistency and RDVO, the results can be fur-
ther improved, and on par with other SOTA methods de-
spite using stereo info. In Tab. 4, UnOS is worse than tradi-
tional ORB-SLAM, we argue that it uses bundle adjustment
to avoid drifting error, which is complementary to UnOS.

Motion segmentation. The motion segmentation task is
evaluated using the object map provided by the KITTI 2015
dataset [52], where the moving objects are manually seg-
mented. We follow the metrics used in [52] including
pixel accuracy and mean intersect-over-union. As shown
in Tab. 5, we also outperform their method. The qualitative
results are shown in Fig. 6, where UnOS discovers more
compact and cleaner segments for moving objects.

6. Conclusion

In summary, our paper propose an uniﬁed system
(UnOS) to learn optical ﬂow and stereo-depth, which mutu-
ally leverages stereo and temporal information in a video.
Speciﬁcally, it automatically discovers rigid regions, and
substantially improves unsupervised learning of stereo-
depth, optical ﬂow, visual odometry and motion segmen-
tation on the KITTI dataset.

8078

References

[1] Dan Barnes, Will Maddern, Geoffrey Pascoe, and Ingmar
Posner. Driven to distraction: Self-supervised distractor
learning for robust monocular visual odometry in urban en-
vironments. In ICRA, pages 1894–1900. IEEE, 2018.

[2] Aseem Behl, Omid Hosseini

Jafari,

Siva Karthik
Mustikovela, Hassan Abu Alhaija, Carsten Rother, and
Andreas Geiger. Bounding boxes, segmentations and object
coordinates: How important is recognition for 3d scene
ﬂow estimation in autonomous driving scenarios?
In
International Conference on Computer Vision, 2017.

[3] Paul J Besl and Neil D McKay. Method for registration of
3-d shapes.
In Sensor Fusion IV: Control Paradigms and
Data Structures, volume 1611, pages 586–607. International
Society for Optics and Photonics, 1992.

[4] Michael J Black and Paul Anandan. The robust estimation
of multiple motions: Parametric and piecewise-smooth ﬂow
ﬁelds. Computer vision and image understanding, 63(1):75–
104, 1996.

[5] Jia-Ren Chang and Yong-Sheng Chen.

Pyramid stereo
matching network. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5410–
5418, 2018.

[6] Xinjing Cheng, Peng Wang, and Ruigang Yang. Depth es-
timation via afﬁnity learned with convolutional spatial prop-
agation network. In European Conference on Computer Vi-
sion, pages 108–125. Springer, Cham, 2018.

[7] Guilherme N DeSouza and Avinash C Kak. Vision for
mobile robot navigation: A survey.
IEEE transactions on
pattern analysis and machine intelligence, 24(2):237–267,
2002.

[8] Yiliu Feng, Zhengfa Liang, and Hengzhu Liu. Efﬁcient deep
learning for stereo matching with larger image patches. In
Image and Signal Processing, BioMedical Engineering and
Informatics (CISP-BMEI), 2017 10th International Congress
on, pages 1–5. IEEE, 2017.

[9] Denis Fortun, Patrick Bouthemy, and Charles Kervrann. Op-
tical ﬂow modeling and computation: a survey. Computer
Vision and Image Understanding, 134:1–21, 2015.

[10] Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, and Ian
Reid. Unsupervised cnn for single view depth estimation:
Geometry to the rescue. In European Conference on Com-
puter Vision, pages 740–756. Springer, 2016.

[11] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Computer Vision and Pattern Recognition (CVPR),
2012 IEEE Conference on, pages 3354–3361. IEEE, 2012.

[12] Andrea Giachetti, Marco Campani, and Vincent Torre. The
use of optical ﬂow for road navigation. IEEE transactions
on robotics and automation, 14(1):34–48, 1998.

[15] Fatma Guney and Andreas Geiger. Displets: Resolving
stereo ambiguities using object knowledge. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4165–4175, 2015.

[16] Rostam Affendi Hamzah and Haidi Ibrahim. Literature sur-
vey on stereo vision disparity map algorithms. Journal of
Sensors, 2016, 2016.

[17] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision. Cambridge university press,
2003.

[18] Berthold KP Horn and Brian G Schunck. Determining opti-

cal ﬂow. Artiﬁcial intelligence, 17(1-3):185–203, 1981.

[19] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keu-
per, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0:
Evolution of optical ﬂow estimation with deep networks. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), volume 2, 2017.

[20] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Advances in Neural Infor-
mation Processing Systems, pages 2017–2025, 2015.

[21] Joel Janai, Fatma G¨uney, Anurag Ranjan, Michael Black,
and Andreas Geiger. Unsupervised learning of multi-frame
optical ﬂow with occlusions.
In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 690–
706, 2018.

[22] J Yu Jason, Adam W Harley, and Konstantinos G Derpanis.
Back to basics: Unsupervised learning of optical ﬂow via
brightness constancy and motion smoothness. In Computer
Vision–ECCV 2016 Workshops, pages 3–10. Springer, 2016.
[23] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter
Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry.
End-to-end learning of geometry and context for deep stereo
regression. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 66–75, 2017.

[24] Diederik Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[25] Junghwan Ko and Jungsuk Lee. Stereo camera-based intelli-
gence surveillance system. Journal of Automation and Con-
trol Engineering Vol, 3(3), 2015.

[26] Ruihao Li, Sen Wang, Zhiqiang Long, and Dongbing Gu.
Undeepvo: Monocular visual odometry through unsuper-
vised deep learning. arXiv preprint arXiv:1709.06841, 2017.
[27] Ce Liu, Jenny Yuen, and Antonio Torralba. Sift ﬂow: Dense
correspondence across scenes and its applications.
IEEE
transactions on pattern analysis and machine intelligence,
33(5):978–994, 2011.

[28] Wenjie Luo, Alexander G Schwing, and Raquel Urtasun. Ef-
ﬁcient deep learning for stereo matching.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5695–5703, 2016.

[13] Cl´ement Godard, Oisin Mac Aodha, and Gabriel Brostow.
Digging into self-supervised monocular depth estimation.
arXiv preprint arXiv:1806.01260, 2018.

[29] Yue Luo, Jimmy Ren, Mude Lin, Jiahao Pang, Wenxiu Sun,
Hongsheng Li, and Liang Lin. Single view stereo matching.
In CVPR, pages 155–163, 2018.

[14] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. In CVPR, volume 2, page 7, 2017.

[30] Reza Mahjourian, Martin Wicke, and Anelia Angelova. Un-
supervised learning of depth and ego-motion from monoc-
ular video using 3d geometric constraints.
In Proceedings

8079

of the IEEE Conference on Computer Vision and Pattern
Recognition, 2018.

[31] David Marr. A computational investigation into the human
representation and processing of visual information. Free-
man, San Francisco, CA, 1982.

[32] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical ﬂow, and scene ﬂow estimation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4040–4048, 2016.

[33] Simon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Un-
supervised learning of optical ﬂow with a bidirectional cen-
sus loss. In AAAI, New Orleans, Louisiana, Feb. 2018.

[34] Moritz Menze and Andreas Geiger. Object scene ﬂow for au-
tonomous vehicles. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 3061–
3070, 2015.

[35] Jean-Philippe Pons, Renaud Keriven, and Olivier Faugeras.
Multi-view stereo reconstruction and scene ﬂow estimation
with a global image-based matching score.
International
Journal of Computer Vision, 72(2):179–193, 2007.

[36] Anurag Ranjan and Michael J Black. Optical ﬂow estima-
tion using a spatial pyramid network. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), vol-
ume 2, page 2. IEEE, 2017.

[37] Anurag Ranjan, Varun Jampani, Kihwan Kim, Deqing Sun,
Jonas Wulff, and Michael J Black. Adversarial collabo-
ration: Joint unsupervised learning of depth, camera mo-
tion, optical ﬂow and motion segmentation. arXiv preprint
arXiv:1805.09806, 2018.

[38] Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang,
and Hongyuan Zha. Unsupervised deep learning for optical
ﬂow estimation. In AAAI, pages 1495–1501, 2017.

[39] Amit Shaked and Lior Wolf. Improved stereo matching with
constant highway networks and reﬂective conﬁdence learn-
ing. In Proc. of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4641–4650, 2017.

[40] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
Pwc-net: Cnns for optical ﬂow using pyramid, warping, and
cost volume. arXiv preprint arXiv:1709.02371, 2017.

[41] Tatsunori Taniai, Sudipta N Sinha, and Yoichi Sato. Fast
multi-frame stereo scene ﬂow with motion segmentation. In
CVPR, pages 6891–6900. IEEE, 2017.

[42] Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J Black.
Video segmentation via object ﬂow. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3899–3908, 2016.

[43] Sundar Vedula, Simon Baker, Peter Rander, Robert Collins,
and Takeo Kanade. Three-dimensional scene ﬂow. In Com-
puter Vision, 1999. The Proceedings of the Seventh IEEE In-
ternational Conference on, volume 2, pages 722–729. IEEE,
1999.

[44] Sundar Vedula, Peter Rander, Robert Collins, and Takeo
Kanade. Three-dimensional scene ﬂow. IEEE transactions
on pattern analysis and machine intelligence, 27(3):475–
480, 2005.

[45] Chaoyang Wang, Jos´e Miguel Buenaposada, Rui Zhu, and
Simon Lucey. Learning depth from monocular videos us-
ing direct methods. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2022–
2030, 2018.

[46] Yang Wang, Yi Yang, Zhenheng Yang, Peng Wang, Liang
Zhao, and Wei Xu. Occlusion aware unsupervised learn-
ing of optical ﬂow. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4884–
4893, 2018.

[47] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing,
13(4):600–612, 2004.

[48] Changchang Wu et al. Visualsfm: A visual structure from

motion system. 2011.

[49] Jonas Wulff, Laura Sevilla-Lara, and Michael J. Black. Op-

tical ﬂow in mostly rigid scenes. In CVPR, July 2017.

[50] Junyuan Xie, Ross Girshick, and Ali Farhadi. Deep3d:
Fully automatic 2d-to-3d video conversion with deep convo-
lutional neural networks. In European Conference on Com-
puter Vision, pages 842–857. Springer, 2016.

[51] Guorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong
Segstereo: Exploiting seman-
arXiv preprint

Deng, and Jiaya Jia.
tic information for disparity estimation.
arXiv:1807.11699, 2018.

[52] Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, and Ram
Nevatia. Every pixel counts: Unsupervised geometry learn-
ing with holistic 3d motion understanding. arXiv preprint
arXiv:1806.10556, 2018.

[53] Zhenheng Yang, Peng Wang, Wei Xu, Liang Zhao, and
Ramakant Nevatia. Unsupervised learning of geometry
with edge-aware depth-normal consistency. arXiv preprint
arXiv:1711.03665, 2017.

[54] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learn-
ing of dense depth, optical ﬂow and camera pose. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), volume 2, 2018.

[55] Jure Zbontar and Yann LeCun. Stereo matching by training
a convolutional neural network to compare image patches.
Journal of Machine Learning Research, 17(1-32):2, 2016.

[56] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera,
Kejie Li, Harsh Agarwal, and Ian Reid. Unsupervised learn-
ing of monocular depth estimation and visual odometry with
deep feature reconstruction.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 340–349, 2018.

[57] Yiran Zhong, Yuchao Dai, and Hongdong Li.

Self-
supervised learning for stereo matching with self-improving
ability. arXiv preprint arXiv:1709.00930, 2017.

[58] Yiran Zhong, Hongdong Li, and Yuchao Dai. Open-
world stereo video matching with deep rnn. arXiv preprint
arXiv:1808.03959, 2018.

[59] Chao Zhou, Hong Zhang, Xiaoyong Shen, and Jiaya Jia.
Unsupervised learning of stereo matching. In International
Conference on Computer Vision, 2017.

8080

[60] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In CVPR, volume 2, page 7, 2017.

[61] Yuliang Zou, Zelun Luo, and Jia-Bin Huang. Df-net: Un-
supervised joint learning of depth and ﬂow using cross-task
consistency. In European Conference on Computer Vision,
2018.

8081

