The Perfect Match: 3D Point Cloud Matching with Smoothed Densities

Zan Gojcic

Caifa Zhou

Jan D. Wegner

Andreas Wieser

ETH Zurich

{firstname.lastname@geod.baug.ethz.ch}

Abstract

We propose 3DSmoothNet, a full workﬂow to match
3D point clouds with a siamese deep learning architecture
and fully convolutional layers using a voxelized smoothed
density value (SDV) representation. The latter is com-
puted per interest point and aligned to the local refer-
ence frame (LRF) to achieve rotation invariance. Our com-
pact, learned, rotation invariant 3D point cloud descrip-
tor achieves 94.9% average recall on the 3DMatch bench-
mark data set [49], outperforming the state-of-the-art by
more than 20 percent points with only 32 output dimen-
sions. This very low output dimension allows for near real-
time correspondence search with 0.1 ms per feature point
on a standard PC. Our approach is sensor- and scene-
agnostic because of SDV, LRF and learning highly de-
scriptive features with fully convolutional layers. We show
that 3DSmoothNet trained only on RGB-D indoor scenes
of buildings achieves 79.0% average recall on laser scans
of outdoor vegetation, more than double the performance
of our closest, learning-based competitors [49, 17, 5, 4].
Code, data and pre-trained models are available online at
https://github.com/zgojcic/3DSmoothNet.

1. Introduction

3D point cloud matching is necessary to combine mul-
tiple overlapping scans of a scene (e.g., acquired using an
RGB-D sensor or a laser scanner) into a single represen-
tation for further processing like 3D reconstruction or se-
mantic segmentation. Individual parts of the scene are usu-
ally captured from different viewpoints with a relatively low
overlap. A prerequisite for further processing is thus align-
ing these individual point cloud fragments in a common co-
ordinate system, to obtain one large point cloud of the com-
plete scene.

Although some works aim to register 3D point clouds
based on geometric constraints (e.g., [27, 48, 35]), most ap-
proaches match corresponding 3D feature descriptors that
are custom-tailored for 3D point clouds and usually de-

Figure 1: 3DSmoothNet generalization ability: our de-
scriptor, trained solely on indoor scenes (top) can seam-
lessly generalize to outdoor scenes (bottom).

scribe point neighborhoods with histograms of point dis-
tributions or local surface normals (e.g., [16, 8, 28, 38]).
Since the comeback of deep learning, research on 3D local
descriptors has followed the general trend in the vision com-
munity and shifted towards learning-based approaches and
more speciﬁcally deep neural networks [49, 17, 5, 45, 4].
Although the ﬁeld has seen signiﬁcant progress in the last
three years, most learned 3D feature descriptors are either
not rotation invariant [49, 5, 45], need very high output di-
mensions to be successful [49, 4] or can hardly general-
ize to new domains [49, 17].
In this paper, we propose
3DSmoothNet, a deep learning approach for 3D point cloud
matching, which has low output dimension (16 or 32) for
very fast correspondence search, high descriptiveness (out-
performs all state-of-the-art approaches by more than 20
percent points), is rotation invariant, and does generalize
across sensor modalities and from indoor scenes of build-
ings to natural outdoor scenes.

Contributions We propose a new compact learned local
feature descriptors for 3D point cloud matching that is ef-
ﬁcient to compute and outperforms all existing methods
signiﬁcantly. A major technical novelty of our paper is
the smoothed density value (SDV) voxelization as a new
input data representation that is amenable to fully convolu-
tional layers of standard deep learning libraries. The gain
of SDV is twofold. On the one hand, it reduces the sparsity

5545

of the input voxel grid, which enables better gradient ﬂow
during backpropagation, while reducing the boundary ef-
fects, as well as smoothing out small miss-alignments due to
errors in the estimation of the local reference frame (LRF).
On the other hand, we assume that it explicitly models the
smoothing that deep networks typically learn in the ﬁrst
layers, thus saving network capacity for learning highly
descriptive features. Second, we present a Siamese net-
work architecture with fully convolutional layers that learns
a very compact, rotation invariant 3D local feature de-
scriptor. This approach generates low-dimensional, highly
descriptive features that generalize across different sensor
modalities and from indoor to outdoor scenes. Moreover,
we demonstrate that our low-dimensional feature descriptor
(only 16 or 32 output dimensions) greatly speeds up corre-
spondence search, which allows real-time applications.

2. Related Work

This section reviews advances in 3D local feature de-
scriptors, starting from the early hand-crafted feature de-
scriptors and progressing to the more recent approaches that
apply deep learning.

Hand-crafted 3D Local Descriptors Pioneer works on
hand-crafted 3D local feature descriptors were usually in-
spired by their 2D counterparts. Two basic strategies exist
depending on how rotation invariance is established. Many
approaches including SHOT [38], RoPS [11], USC [37]
and TOLDI [42] try to ﬁrst estimate a unique local refer-
ence frame (LRF), which is typically based on the eigen-
value decomposition of the sample covariance matrix of the
points in neighborhood of the interest point. This LRF is
then used to transform the local neighborhood of the interest
point to its canonical representation in which the geometric
peculiarities, e.g. orientation of the normal vectors or lo-
cal point density are analyzed. On the other hand, several
approaches [29, 28, 2] resort to a LRF-free representation
based on intrinsically invariant features (e.g., point pair fea-
tures). Despite signiﬁcant progress, hand-crafted 3D local
descriptors never reached the performance of hand-crafted
2D descriptors. In fact, they still fail to handle point cloud
resolution changes, noisy data, occlusions and clutter [10].

Learned 3D Local Descriptors The success of deep-
learning methods in image processing also inspired vari-
ous approaches for learning geometric representations of
3D data. Due to the sparse and unstructured nature of raw
point clouds, several parallel tracks regarding the represen-
tation of the input data have emerged.

One idea is projecting 3D point clouds to 2D images and
then inferring local feature descriptors by drawing from the
rich library of well-established 2D CNNs developed for im-
age interpretation. For example, [6] project 3D point clouds
to 2D depth maps and extract features using a 2D auto-

encoder. [13] use a 2D CNN to combine the rendered views
of feature points at multiple scales into a single local feature
descriptor. Another possibility are dense 3D voxel grids ei-
ther in the form of binary occupancy grids [21, 40, 25] or an
alternative encoding [49, 44]. For example, 3DMatch [49],
one of the pioneer works in learning 3D local descriptors,
uses a volumetric grid of truncated distance functions to
represent the raw point clouds in a structured manner. An-
other option is estimating the LRF (or a local reference
axis) for extracting canonical, high-dimensional, but hand-
crafted features and using a neural network solely for a di-
mensionality reduction. Even-though these methods [17, 9]
manage to learn a non-linear embedding that outperforms
the initial representations, their performance is still bounded
by the descriptiveness of the initial hand-crafted features.

PointNet [24] and PointNet++ [26] are seminal works
that introduced a new paradigm by directly working on raw
unstructured point-clouds. They have shown that a per-
mutation invariance of the network, which is important for
learning on unordered sets, can be accomplished by using
symmetric functions. Albeit, successful in segmentation
and classiﬁcation tasks, they do not manage to encapsulate
the local geometric information in a satisfactory manner,
largely because they are unable to use convolutional lay-
ers in their network design [5]. Nevertheless, PointNet of-
fers a base for PPFNet [5], which augments raw point co-
ordinates with point-pair features and incorporates global
context during learning to improve the feature represen-
tation. However, PPFNet is not fully rotation invariant.
PPF-FoldNet addresses the rotation invariance problem of
PPFNet by solely using point-pair features as input. It is
based on the architectures of the PointNet [24] and Fold-
ingNet [43] and is trained in a self-supervised manner. The
recent work of [45] is based on PointNet, too, but deviates
from the common approach of learning only the feature de-
It follows the idea of [46] in trying to fuse the
scriptor.
learning of the keypoint detector and descriptor in a single
network in a weakly-supervised way using GPS/INS tagged
3D point clouds. [45] does not achieve rotation invariance
of the descriptor and is limited to smaller point cloud sizes
due to using PointNet as a backbone.

Arguably, training a network directly from raw point
clouds fulﬁlls the end-to-end learning paradigm. On the
downside, it does signiﬁcantly hamper the use of convo-
lutional layers, which are crucial to fully capture local ge-
ometry. We thus resort to a hybrid strategy that, ﬁrst, trans-
forms point neighborhoods into LRFs, second, encodes un-
structured 3D point clouds as SDV grids amenable to con-
volutional layers and third, learns descriptive features with
a siamese CNN. This strategy does not only establish rota-
tion invariance but also allows very good performance with
small output dimensions, which massively speeds up corre-
spondence search.

5546

Figure 2: Input parameterization: (a) We extract the spherical support S of the interest point p, which is used (b) to
estimate a unique LRF. (c) Each data cube is transformed to its canonical representation and (d) voxelized using a Gaussian
smoothing kernel. (e) The normalized 3D SDV voxel grid is used as input to our siamese 3DSmoothNet architecture. Note
that (d) and (e) show 2D slices of 3D cubes.

3. Method

In a nutshell, our workﬂow is as follows (Fig. 2 & 3):
(i) given two raw point clouds, (ii) compute the LRF of
the spherical neighborhood around the randomly selected
interest points, (iii) transform the neighborhoods to their
canonical representations, (iv) voxelize them with the help
of Gaussian smoothing, (v) infer the per point local feature
descriptors using 3DSmoothNet and, for example, use them
as input to a RANSAC-based robust point cloud registration
pipeline.

More formally, consider two overlapping point cloud

sets P and Q represented in matrix form as P ∈ Rn×3
and Q ∈ Rm×3. Let (P)i =: pi represent the coordinate
vector of an individual point of point cloud P located in the
overlapping region. A bijective function maps point pi to
its corresponding (but initially unknown) point (Q)j =: qj
in the second point cloud. Under the assumption of a static
scene and rigid point clouds (and neglecting noise and dif-
fering point cloud resolutions), this bijective function can
be described with the transformation parameters of the con-
gruent transformation

qj = Rpi + t,

(1)

where R ∈ SO(3) denotes the rotation matrix and t ∈ R3
the translation vector. With point subsets Pc and Qc for
which correspondences exist, the mapping function can be
written as

Qc = KPcRT + 1 ⊗ tT ,

(2)

where K ∈ P|Q′| denotes a permutation matrix whose en-
tries kij = 1 if pi corresponds to qj and 0 otherwise and
1 is a vector of ones. In our setting both, the permutation
matrix K and the transformation parameters R and t are
unknown initially. Simultaneously solving for all is hard
as the problem is non-convex with binary entries in K [20].
However, if we ﬁnd a way to determine K, the estimation of
the transformation parameters is straightforward. This boils
down to learning a function that maps point pi to a higher
dimensional feature space in which we can determine its

corresponding point qj . Once we have established corre-
spondence, we can solve for R and t. Computing a rich
feature representation across the neighborhood of pi en-
sures robustness against noise and facilitates high descrip-
tiveness. Our main objective is a fully rotation invariant
local feature descriptor that generalizes well across a large
variety of scene layouts and point cloud matching settings.
We choose a data-driven approach for this task and learn a
compact local feature descriptor from raw point clouds.

3.1. Input parameterization

A core requirement for a generally applicable local fea-
ture descriptor is its invariance under isometry of the Eu-
clidian space. Since achieving the rotation invariance in
practice is non-trivial, several recent works [49, 5, 45]
choose to ignore it and thus do not generalize to rigidly
transformed scenes [4]. One strategy to make a feature de-
scriptor rotation invariant is regressing the canonical orien-
tation of a local 3D patch around a point as an integral part
of a deep neural network [24, 45] inspired by recent work
in 2D image processing [15, 47]. However, [5, 7] ﬁnd that
this strategy often fails for 3D point clouds. We therefore
choose a different approach and explicitly estimate LRFs by
adapting the method of [42]. An overview of our method is
shown in Fig. 2 and is described in the following.

:

Local reference frame Given a point p in point cloud
P, we select its local spherical support S ⊂ P such that
||pi − p||2 ≤ rLRF} where rLRF denotes
S = {pi
the radius of the local neighborhood used for estimating the
LRF. In contrast to [42], where only points within the dis-
tance 1
3 rLRF are used, we approximate the sample covari-
ance matrix ˜ΣS using all the points pi ∈ S. Moreover,

we replace the centroid with the interest point p to reduce
computational complexity. We compute the LRF via the
eigendecomposition of ˜ΣS :

1

|S| X

pi∈S

(pi − p)(pi − p)T

(3)

˜ΣS =

5547

Figure 3: 3DSmoothNet network architecture: We extract interest points in the overlapping region of two fragments. The
cubic patches (bounding box is color coded to the interest points), centered at the interest point and aligned with the estimate
LRF are converted to the SDV voxel grid and fed to the network. 3DSmoothNet consists of convolutional (green rectangles
with number of ﬁlters and ﬁlter size respectively), batch-normalization (orange), ReLU activation function (blue) and an
l2-normalization (magenta) layer. Both branches share all the parameters. The anchor fθ(Xa), positive fθ(Xp) and negative
fθ(Xn) arguments of the batch hard loss are color coded according to the interest points. Negative examples are sampled on
the ﬂy from all the positive examples of the mini-batch (denoted with the four voxel grids).

We select the z-axis ˆzp to be collinear to the estimated nor-
mal vector ˆnp obtained as the eigenvector corresponding to
the smallest eigenvalue of ˜ΣS . We solve for the sign ambi-
guity of the normal vector ˆzp by

ˆzp = 


ˆnp,

−ˆnp,

if Ppi∈Shˆnp, pipi ≥ 0

otherwise

(4)

The x-axis ˆxp is computed as the weighted vector sum

representation p′

formed to their canonical
(Fig. 2(c)), we use them to describe the transformed local
neighborhood of the interest points. We represent points
in a SDV voxel grid, centered on the interest point p′ and
aligned with the LRF. We write the SDV voxel grid as a

i ∈ S ′

three dimensional matrix XSDV ∈ RW ×H×D whose ele-
ments (XSDV)jkl =: xjkl represent the SDV of the cor-
responding voxel computed using the Gaussian smoothing
kernel with bandwidth h:

ˆxp =

|| Ppi∈S

1

αiβivi||2 X

pi∈S

αiβivi

(5)

xjkl =

1

njkl

where vi = ppi − hppi , ˆzpiˆzp is the projection of the
vector ppi to a plane orthogonal to ˆzp and αi and βi are
weights related to the norm and the scalar projection of the
vector ppi to the vector ˆzp computed as

αi = (rLRF − ||p − pi||2)2
βi = hppi , ˆzpi2

(6)

Intuitively, the weight αi favors points lying close to the
interest point thus making the estimation of ˆxp more ro-
bust against clutter and occlusions. βi gives more weight to
points with a large scalar projection, which are likely to con-
tribute signiﬁcant evidence particularly in planar areas [42].
Finally, the y-axis ˆyp completes the left-handed LRF and is
computed as ˆyp = ˆxp × ˆzp.

Smoothed density value (SDV) voxelization Once
points in the local neighborhood pi ∈ S have been trans-

exp −||cjkl − p′

2h2

i||2

2

(7)

njkl

1

i=1

√2πh

X
s.t. ||cjkl − p′

i||2 < 3h

i ∈ S ′ that
where njkl denotes the number of points p′
lie within the distance 3h of the voxel centroid cjkl (see
Fig. 2(d)). Further, all values of XSDV are normalized such
that they sum up to 1 in order to achieve invariance with
respect to varying point cloud densities. For ease of nota-
tion we omit the superscript SDV in XSDV in all follow-
ing equations. The proposed SDV voxel grid represen-
tation has several advantages over the traditional binary-
occupancy grid [21, 40], the truncated distance function
[49] or hand-crafted feature representations [17, 9, 5, 4].
First, we mitigate the impact of boundary effects and noise
of binary-occupancy grids and truncated distance functions
by smoothing density values over voxels. Second, com-
pared to the binary occupancy grid we reduce the sparsity
of the representation on the fragments of the test part of
3DMatch data set by more than 30 percent points (from
about 90% to about 57%), which enables better gradient

5548

Figure 4: Results on the 3DMatch data set after RANSAC: 3DSmoothNet generates reliable correspondences for pairs
with low overlap (32% (top), 48% (bottom)) and predominantly planar regions (top row) or hard cases with vegetation and
repetitive geometries (Christmas tree, windows in bottom row).

ﬂow during backpropagation. Third, the SDV representa-
tion helps our method to achieve better generalization as we
do not overﬁt exact data cues during training. Finally, in
contrast to hand-crafted feature representations, SDV voxel
grid representation provides input with a geometrically in-
formed structure, which enables us to exploit convolutional
layers that are crucial to capture the local geometric charac-
teristics of point clouds (Fig. 5).

Network architecture Our network architecture (Fig. 3)
is loosely inspired by L2Net [36], a state-of-the-art learned
local image descriptor. 3DSmoothNet consists of stacked
convolutional layers that applies strides of 2 (instead of
max-pooling) in some convolutional layers to down-sample
the input [33]. All convolutional layers, except the ﬁnal
one, are followed by batch normalization [14] and use the
ReLU activation function [22]. In our implementation, we
follow [36] and ﬁx the afﬁne parameters of the batch nor-
malization layer to 1 and 0 and we do not train them during
the training of the network. To avoid over-ﬁtting the net-
work, we add dropout regularization [34] with a 0.3 dropout
rate before the last convolutional layer. The output of the
last convolutional layer is fed to a batch normalization layer
followed by an l2 normalization to produce unit length local
feature descriptors.

Training We train 3DSmoothNet (Fig. 3) on point cloud
fragments from the 3DMatch data set [49]. This is an
RGB-D data set consisting of 62 real-world indoor scenes,
ranging from ofﬁces and hotel rooms to tabletops and
restrooms. Point clouds obtained from a pool of data
sets [41, 32, 19, 39, 3] are split into 54 scenes for train-
ing and 8 scenes for testing. Each scene is split into sev-
eral partially overlapping fragments with their ground truth
transformation parameters T .

Consider two fragments Fi and Fj , which have more
than 30% overlap. To generate training examples, we start

by randomly sampling 300 anchor points pa from the over-
lapping region of fragment Fi. After applying the ground
truth transformation parameters Tj() the positive sample pp
is then represented as the nearest-neighbor pp =: nn(pa) ∈
Tj(Fj), where nn() denotes the nearest neighbor search
in the Euclidean space based on the l2 distance. We re-
frain from pre-sampling the negative examples and instead
use the hardest-in-batch method [12] for sampling negative
samples on the ﬂy. During training we aim to minimize the
soft margin Batch Hard (BH) loss function

LBH(θ,X ) =

|X |

X

i=1

1
|X|

ln(cid:16)1 + exp(cid:2)||fθ(Xa
||fθ(Xa

− min

j=1...|X |

j6=i

i) − fθ(Xp

i )||2
j)||2(cid:3)(cid:17)

i) − fθ(Xp

(8)

The BH loss is deﬁned for a mini-batch X , where Xa
i and
Xp
i represent the SDV voxel grids of the anchor and posi-
tive input samples, respectively. The negative samples are
retrieved as the hardest non-corresponding positive sam-
ples in the mini-batch (c.f. Eq. 8). Hardest-in-batch sam-
pling ensures that negative samples are neither too easy
(i.e, non-informative) nor exceptionally hard, thus prevent-
ing the model to learn normal data associations [12].

4. Results

Implementation details Our 3DSmoothNet approach is
implemented in C++ (input parametrization) using the
PCL [30] and in Python (CNN part) using Tensorﬂow [1].
During training we extract SDV voxel grids of size W =
H = D = 0.3 m (corresponding to [49]), centered at
each interest point and aligned with the LRF. We use

rLRF = √3W to extract the spherical support S and esti-

mate the LRF. We obtain the circumscribed sphere of our

5549

Figure 5: 3DSmoothNet descriptors are geometrically
informed: Embedding in 3D space with PCA (ﬁrst three
components RGB color-coded). Planar regions lie in the
blue-green, edges and corners in the orange-pink and spher-
ical surfaces in the yellow color spectrum.

2

voxel grid and use the points transformed to the canoni-
cal frame to extract the SDV voxel grid. We split each
SDV voxel grid into 163 voxels with an edge w = W
16 and
use a Gaussian smoothing kernel with an empirically deter-
mined optimal width h = 1.75w
. All the parameters wew
slected on the validation data set. We train the network with
mini-batches of size 256 and optimize the parameters with
the ADAM optimizer [18], using an initial learning rate of
0.001 that is exponentially decayed every 5000 iterations.
Weights are initialized orthogonally [31] with 0.6 gain, and
biases are set to 0.01. We train the network for 20 epochs.
We evaluate the performance of 3DSmoothNet for cor-
respondence search on the 3DMatch data set [49] and com-
pare against the state-of-the-art.
In addition, we evaluate
its generalization capability to a different sensor modality
(laser scans) and different scenes (e.g., forests) on the Chal-
lenging data sets for point cloud registration algorithms
data set [23] denoted as ETH data set.

Comparison to state-of-the-art We adopt the commonly
used hand-crafted 3D local feature descriptors FPFH [28]
(33 dimensions) and SHOT [38] (352 dimensions) as base-
lines and run implementations provided in PCL [30] for
both approaches. We compare against the current state-
of-the-art in learned 3D feature descriptors: 3DMatch [49]
(512 dimensions), CGF [17] (32 dimensions), PPFNet [5]
(64 dimensions), and PPF-FoldNet [4] (512 dimensions).
In case of 3DMatch and CGF we use the implementations
provided by the authors in combination with the given pre-
trained weights. Because source-code of PPFNet and PPF-
FoldNet is not publicly available, we report the results pre-
sented in the original papers. For all descriptors based on
the normal vectors, we ensure a consistent orientation of
the normal vectors across the fragments. To allow for a
fair evaluation, we use exactly the same interest points (pro-
vided by the authors of the data set) for all descriptors. In
case of descriptors that are based on spherical neighbor-
hoods, we use a radius that yields a sphere with the same
volume as our voxel. All exact parameter settings, further
implementation details etc. used for these experiments are
available in supplementary material.

]

%

[

l
l
a
c
e
R

100

95

90

85

80

16 dim [0.927]

32 dim [0.947]

64 dim [0.956]

128 dim [0.957]

Kitchen

H o m e1

2

H o m e

H otel1

H otel2

H otel3

Study

M IT lab

Figure 6: Recall in relation to 3DSmoothNet output di-
mensions. Values in brackets denote average recall over all
scenes.

4.1. Evaluation on the 3DMatch data set

Setup The test part of the 3DMatch data set consists of 8
indoor scenes split into several partially overlapping frag-
ments. For each fragment, the authors provide indices of
5000 randomly sampled feature points. We use these fea-
ture points for all descriptors. The results of PPFNet and
PPF-FoldNet are based on a spherical neighborhood with
a diameter of 0.6m. Furthermore, due to its memory bot-
tleneck, PPFNet is limited to 2048 interest points per frag-
ment. We adopt the evaluation metric of [5] (see supple-
mentary material).
It is based on the theoretical analysis
of the number of iterations needed by a robust registration
pipeline, e.g. RANSAC, to ﬁnd the correct set of transfor-
mation parameters between two fragments. As done in [5],
we set the threshold τ1 = 0.1m on the l2 distance between
corresponding points in the Euclidean space and τ2 = 0.05
to threshold the inlier ratio of the correspondences at 5%.

Output dimensionality of 3DSmoothNet A general goal
is achieving the highest matching performance with the
lowest output dimensionality (i.e., ﬁlter number in the last
convolutional layer of 3DSmoothNet) to decrease run-time
and to save memory. Thus, we ﬁrst run trials to ﬁnd a good
compromise between matching performance and efﬁciency
for the 3DSmoothNet descriptors1. We ﬁnd that the perfor-
mance of 3DSmoothNet quickly starts to saturate with in-
creasing output dimensions (Fig. 6). There is only marginal
improvement (if any) when using more than 64 dimensions.
We thus decide to process all further experiments only for
16 and 32 output dimensions of 3DSmoothNet.

Comparison to state-of-the-art Results of experimen-
tal evaluation on the 3DMatch data set are summarized in
Tab. 1 (left) and two hard cases are shown in Fig. 4. Ours

1Recall that for correspondence search, the brute-force implementation
of nearest-neighbor search scales with O(DN 2), where D denotes the
dimension and N the number of data points. The time complexity can
be reduced to O(DN log N ) using tree-based methods, but still becomes
inefﬁcient if D grows large (”curse of dimensionality”).

5550

]

%

[

l
l
a
c
e
R

100

80

60

40

20

0

Ours (16)
Ours (32)
CGF
SHOT
3DMatch
FPFH

0.04

0.08

0.12

0.16

0.2

τ2

Figure 7: Recall in relation to inlier ratio threshold. Re-
call of 3DSmoothNet on the 3DMatch data set remains high
even when the inlier threshold ratio is increased.

(16) and Ours (32) achieve an average recall of 92.8%
and 94.7%, respectively, which is close to solving the
3DMatch data set. 3DSmoothNet outperforms all state-of-
the-art 3D local feature descriptors with a signiﬁcant mar-
gin on all scenes. Remarkably, Ours (16) improves aver-
age recall over all scenes by almost 20 percent points with
only 16 output dimensions compared to 512 dimensions of
PPF-FoldNet and 352 of SHOT. Furthermore, Ours (16)
and Ours (32) show a much smaller recall standard devi-
ation (STD), which indicates robustness of 3DSmoothNet
to scene changes and hints at good generalization ability.
The inlier ratio threshold τ2 = 0.05 as chosen by [5] re-
sults in ≈ 55k iterations to ﬁnd at least 3 correspondences
(with 99.9% probability) with the common RANSAC ap-
proach. Increasing the inlier ratio to τ2 = 0.2 would de-
crease RANSAC iterations signiﬁcantly to ≈ 850, which
would speed up processing massively. We thus evaluate
how gradually increasing the inlier ratio changes perfor-
mance of 3DSmoothNet in comparison to all other tested
approaches (Fig. 7). While the average recall of all other
methods drops below 30% for τ2 = 0.2, recall of Ours
(16) (blue) and Ours (32) (orange) remains high at 62%
and 72%, respectively. This indicates that any descriptor-
based point cloud registration pipeline can be made more
efﬁcient by just replacing the existing descriptor with our
3DSmoothNet.

Rotation invariance We take a similar approach as [4]
to validate rotation invariance of 3DSmoothNet by rotating
all fragments of 3DMatch data set (we name it 3DRotat-
edMatch) around all three axis and evaluating the perfor-
mance of the selected descriptors on these rotated versions.
Individual rotation angles are sampled arbitrarily between
[0, 2π] and the same indices of points for evaluation are
used as in the previous section. Results of Ours (16) and
Ours (32) remain basically unchanged (Tab 1 (right)) com-
pared to the non-rotated variant (Tab 1 (left)), which con-
ﬁrms rotation invariance of 3DSmoothNet (due to estimat-
ing LRF). Because performance of all other rotation invari-

3DMatch data set

Original

Rotated

Average STD Average STD

FPFH [28]
SHOT [38]
3DMatch [49]2
CGF [17]
PPFNet [5]
PPF-FoldNet [4]
Ours (16 dim)
Ours (32 dim)

54.3
73.3
57.3
58.2
62.3
71.8
92.8
94.7

11.8
7.7
7.8
14.2
11.5
9.9
3.4
2.7

54.8
73.3
3.6
58.5
0.3
73.1
93.0
94.9

12.1
7.6
1.7
14.0
0.5
11.1
3.2
2.5

Table 1: Results on the 3DMatch and 3DRotatedMatch data
sets. We report average recall in percent over all scenes
along with the standard deviation (STD) per method. Best
performance is shown in bold. Note that results of non-
rotation invariant methods naturally drop to zero for the ro-
tated case (right column). See detailed results per scene in
the Supplementary material.

ant descriptors [28, 38, 17, 4] remains mainly identical, too,
3DSmoothNet again outperforms all state-of-the-art meth-
ods by more than 20 percent points.

Ablation study To get a better understanding of the rea-
sons for the very good performance of 3DSmoothNet, we
analyze the contribution of individual modules with an ab-
lation study on 3DMatch and 3DRotatedMatch data sets.
Along with the original 3DSmoothNet, we consider ver-
sions without SDV (we use a simple binary occupancy
grid), without LRF and ﬁnally without both, LRF and SDV.
All networks are trained using the same parameters and for
the same number of epochs. Results of this ablation study
are summarized in Tab. 2.
It turns out that the version
without LRF performs best on 3DMatch because most frag-
ments are already oriented in the same way and the original
data set version is tailored for descriptors that are not rota-
tion invariant. Inferior performance of the full pipeline on
this data set is most likely due to a few wrongly estimated
LRF, which reduces performance on already oriented data
sets (but allows generalizing to the more realistic, rotated
cases). Unsurprisingly, 3DSmoothNet without LRF fails on
3DRotatedMatch because the network cannot learn rotation
invariance from the data. A signiﬁcant performance gain of
up to more than 9 percent points can be attributed to using
an a SDV voxel grid instead of the traditional binary occu-
pancy grid.

4.2. Generalizability across modalities and scenes

We evaluate how 3DSmoothNet generalizes to outdoor
scenes obtained using a laser scanner (Fig. 1). To this end,

2Using the precomputed feature descriptors provided by the authors.

For more results see the Supplementary material.

5551

3DMatch data set

Original

Rotated

τ2 = 0.05 τ2 = 0.2 τ2 = 0.05 τ2 = 0.2

All together
W/o SDV
W/o LRF
W/o SDV & LRF

94.7
92.5
96.3
95.6

72.7
63.5
81.6
78.6

94.9
92.5
11.6
9.7

72.8
63.6
2.7
2.1

Table 2: Ablation study of 3DSmoothNet on 3DMatch and
3DRotatedMatch data sets. We report average recall over
all overlapping fragment pairs. Best performance is shown
in bold.

Gazebo

Wood

Sum. Wint. Sum. Aut. Average

20.8
FPFH [28]
64.0
SHOT [38]
22.4
3DMatch [49]
19.2
CGF [17]
Ours ( 16 dim)
37.6
Ours ( 32 dim) 91.3 84.1 67.8 72.8

14.2
45.7
8.3
13.8
47.7

14.8
60.9
13.9
10.4
31.3

38.6
73.9
22.8
37.5
76.1

22.1
61.1
16.9
20.2
48.2
79.0

Table 3: Results on the ETHdata set. We report average
recall in percent per scene as well as across the whole data
set.

we use models Ours (16) and Ours (32) trained on 3DMatch
(RGB-D images of indoor scenes) and test on four outdoor
laser scan data sets Gazebo-Summer, Gazebo-Winter, Wood-
Summer and Wood-Autumn that are part of the ETH data
set [23]. All acquisitions contain several partially overlap-
ping scans of sparse and dense vegetation (e.g., trees and
bushes). Accurate ground-truth transformation matrices
are available through extrinsic measurements of the scan-
ner position with a total-station. We start our evaluation
by down-sampling the laser scans using a voxel grid ﬁlter
of size 0.02m. We randomly sample 5000 points in each
point cloud and follow the same evaluation procedure as
in Sec 4.1, again considering only point clouds with more
than 30% overlap. More details on sampling of the fea-
ture points and computation of the point cloud overlaps are
available in the supplementary material. Due to the lower
resolution of the point clouds, we now use a larger value of
W = 1 m for the SDV voxel grid (consequently the radius
for the descriptors based on the spherical neighborhood is
also increased). A voxel grid with an edge equal to 1.5 m is
used for 3DMatch because of memory restrictions. Results
on the ETH data set are reported in Tab 3. 3DSmoothNet
achieves best performance on average (right column), Ours
(32) with 79.0% average recall clearly outperforming Ours
(16) with 48.2% due to its larger output dimension. Ours
(32) beats runner-up (unsupervised) SHOT by more than 15
percent points whereas all state-of-the-art methods stay sig-
niﬁcantly below 30%. In fact, Ours (32) applied to outdoor

Input prep.

[ms]

0.5
4.2

3DMatch
3DSmoothNet

Inference NN search Total
[ms]

[ms]

[ms]

3.7
0.3

0.8
0.1

5.0
4.6

Table 4: Average run-time per feature-point on test frag-
ments of 3DMatch data set.

laser scans still outperforms all competitors that are trained
and tested on the 3DMatch data set (cf. Tab. 3 with Tab. 1).

4.3. Computation time

We compare average run-time of our approach per inter-
est point on 3DMatch test fragments to [49] in Tab. 4 (ran
on the same PC with Intel Xeon E5-1650, 32 GB of ram
and NVIDIA GeForce GTX1080). Note that input prepa-
ration (Input prep.) and inference of [49] are processed
on the GPU, while our approach does input preparation on
CPU in its current state. For both methods, we run near-
est neighbor correspondence search on the CPU. Naturally,
input preparation of 3DSmoothNet on the CPU takes con-
siderably longer (4.2 ms versus 0.5 ms), but still the overall
computation time is slightly shorter (4.6 ms versus 5.0 ms).
Main drivers for performance are inference (0.3 ms versus
3.7 ms) and nearest neighbor correspondence search (0.1 ms
versus 0.8 ms). This indicates that it is worth investing com-
putational resources into custom-tailored data preparation
because it signiﬁcantly speeds up all later tasks. The bigger
gap between Ours(16 dim) and Ours(32 dim), is a result of
the lower capacity and hence lower descriptiveness of the
16-dimensional descriptor, which becomes more apparent
on the harder ETH data set, but can also be seen in addi-
tional experiments in supplementary material. Supplemen-
tary material also contains additional experiments, which
show the invariance of the proposed descriptor to changes
in point cloud density.

5. Conclusions

We have presented 3DSmoothNet, a deep learning ap-
proach with fully convolutional layers for 3D point cloud
matching that outperforms all state-of-the-art by more than
20 percent points. It allows very efﬁcient correspondence
search due to low output dimensions (16 or 32), and a model
trained on indoor RGB-D scenes generalizes well to terres-
trial laser scans of outdoor vegetation. Our method is ro-
tation invariant and achieves 94.9% average recall on the
3DMatch benchmark data set, which is close to solving it.
To the best of our knowledge, this is the ﬁrst learned, uni-
versal point cloud matching method that allows transferring
trained models between modalities. It takes our ﬁeld one
step closer to the utopian vision of a single trained model
that can be used for matching any kind of point cloud re-
gardless of scene content or sensor.

5552

References

[1] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian
Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Man´e, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-
nanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5

[2] Tolga Birdal and Slobodan Ilic. Point Pair Features Based
Object Detection and Pose Estimation Revisited. In Interna-
tional Conference on 3D Vision, 2015. 2

[3] Angela Dai, Matthias Nießner, Michael Zoll¨ofer, Shahram
Izadi, and Christian Theobalt. BundleFusion: Real-time
Globally Consistent 3D Reconstruction using On-the-ﬂy
Surface Re-integration. ACM Transactions on Graphics
2017 (TOG), 2017. 5

[4] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppf-foldnet:
Unsupervised learning of rotation invariant 3d local descrip-
tors. In European conference on computer vision (ECCV),
2018. 1, 3, 4, 6, 7

[5] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppfnet:
Global context aware local features for robust 3d point
matching. In IEEE Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 1, 2, 3, 4, 6, 7

[6] Gil Elbaz, Tamar Avraham, and Anath Fischer. 3d point
cloud registration for localization using a deep neural net-
work auto-encoder. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017. 2

[7] Carlos Esteves, Christine Allen-Blanchette, Ameesh Maka-
dia, and Kostas Daniilidis. Learning SO(3) Equivariant Rep-
resentations with Spherical CNNs. In European Conference
on Computer Vision (ECCV), 2018. 3

[8] A. Flint, A. Dick, and A. van den Hangel. Thrift: Local 3D
structure recognition. In 9th Biennial Conference of the Aus-
tralian Pattern Recognition Society on Digital Image Com-
puting Techniques and Applications, pages 182–188, 2007.
1

[9] Zan Gojcic, Caifa Zhou, and Andreas Wieser. Learned com-
pact local feature descriptor for tls-based geodetic monitor-
ing of natural outdoor scenes.
In ISPRS Annals of Pho-
togrammetry, Remote Sensing & Spatial Information Sci-
ences, 2018. 2, 4

[10] Yulan Guo, Mohammed Bennamoun, Ferdous Sohel, Min
Lu, Jianwei Wan, and Ngai Ming Kwok. A Comprehen-
sive Performance Evaluation of 3D Local Feature Descrip-
tors. International Journal of Computer Vision, 116(1):66–
89, 2016. 2

[11] Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min
Lu, and Jianwei Wan. Rotational projection statistics for
3d local surface description and object recognition.
Inter-

national Journal of Computer Vision, 105(1):63–86, 2013.
2

[12] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In De-
In

fense of the Triplet Loss for Person Re-Identiﬁcation.
arXiv:1703.07737, 2017. 5

[13] Haibin Huang, Evangelos Kalogerakis, Siddhartha Chaud-
huri, Duygu Ceylan, Vladimir G Kim, and Ersin Yumer.
Learning Local Shape Descriptors from Part Correspon-
dences with Multiview Convolutional Networks.
ACM
Transactions on Graphics, 37(1):6, 2018. 2

[14] Sergey Ioffe and Christian Szegedy. Batch Normalization:
Accelerating Deep Network Training by Reducing Internal
Covariate Shift.
In International Conference on Machine
Learning (ICML), 2015. 5

[15] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and
Koray Kavukcuoglu. Spatial transformer networks. In In-
ternational Conference on Neural Information Processing
Systems-Volume 2, 2015. 3

[16] A.E. Johnson and M. Hebert. Using spin images for efﬁcient
object recognition in cluttered 3d scenes. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 21:433–449,
1999. 1

[17] Marc Khoury, Qian-Yi Zhou, and Vladlen Koltun. Learning
compact geometric features. In IEEE International Confer-
ence on Computer Vision (ICCV), 2017. 1, 2, 4, 6, 7, 8

[18] Diederik P. Kingma and Jimmy Lei Ba. Adam: a Method
for Stochastic Optimization. In International Conference on
Learning Representations 2015, 2015. 6

[19] Kevin Lai, Liefeng Bo, and Dieter Fox. Unsupervised fea-
In IEEE International

ture learning for 3d scene labeling.
Conference on Robotics and Automation (ICRA), 2014. 5

[20] Hongdong Li and Richard Hartley. The 3D-3D registration
problem revisited. In International Conference on Computer
Vision (ICCV), pages 1–8, 2007. 3

[21] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-
volutional neural network for real-time object recognition.
In IEEE International Conference on Intelligent Robots and
Systems, 2015. 2, 4

[22] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units im-
prove restricted boltzmann machines. In International Con-
ference on Machine Learning (ICML), 2010. 5

[23] Franc¸ois Pomerleau, M. Liu, Francis Colas, and Roland
Siegwart. Challenging data sets for point cloud registration
algorithms. The International Journal of Robotics Research,
31(14):1705–1711, 2012. 6, 8

[24] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation.
In IEEE Computer Vision and Pattern
Recognition (CVPR), 2017. 2, 3

[25] Charles R Qi, Hao Su, Matthias Nießner, Angela Dai,
Mengyuan Yan, and Leonidas J Guibas. Volumetric and
multi-view cnns for object classiﬁcation on 3d data.
In
IEEE conference on computer vision and pattern recognition
(CVPR), 2016. 2

[26] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In Advances in Neural Informa-
tion Processing Systems, 2017. 2

5553

object labels.
puter Vision (ICCV), 2013. 5

In IEEE International Conference on Com-

[42] Jiaqi Yang, Qian Zhang, Yang Xiao, and Zhiguo Cao.
TOLDI: An effective and robust approach for 3D local shape
description. Pattern Recognition, 65:175–187, 2017. 2, 3, 4
[43] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Fold-
ingnet: Point cloud auto-encoder via deep grid deforma-
tion. In IEEE International Conference on Computer Vision
(ICCV), 2018. 2

[44] Dmitry Yarotsky. Geometric features for voxel-based surface

recognition. CoRR, abs/1701.04249, 2017. 2

[45] Zi Jian Yew and Gim Hee Lee. 3dfeat-net: Weakly super-
vised local 3d features for point cloud registration. In Euro-
pean Conference on Computer Vision (ECCV), 2018. 1, 2,
3

[46] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal
Fua. Lift: Learned invariant feature transform. In European
Conference on Computer Vision (ECCV), 2016. 2

[47] Kwang Moo Yi, Yannick Verdie, Pascal Fua, and Vincent
Lepetit. Learning to assign orientations to feature points. In
Computer Vision and Pattern Recognition (CVPR), 2016. 3
[48] B. Zeisl, K. K¨oser, and M. Pollefeys. Automatic registration
of rgb-d scans via salient directions. In IEEE International
Conference on Computer Vision, pages 2808–2815, 2013. 1
[49] Andy Zeng, Shuran Song, Matthias Nießner, Matthew
Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3DMatch:
Learning Local Geometric Descriptors from RGB-D Recon-
structions. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2017. 1, 2, 3, 4, 5, 6, 7, 8

[27] T. Rabbani, S. Dijkman, F. van den Heuvel, and G. Vossel-
man. An integrated approach for modelling and global reg-
istration of point clouds. ISPRS Journal of Photogrammetry
and Remote Sensing, 61:355–370, 2007. 1

[28] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast
point feature histograms (FPFH) for 3D registration.
In
IEEE International Conference on Robotics and Automation
(ICRA), 2009. 1, 2, 6, 7, 8

[29] Radu Bogdan Rusu, Nico Blodow, Zoltan Csaba Marton, and
Michael Beetz. Aligning point cloud views using persistent
feature histograms. In IEEE/RSJ International Conference
on Intelligent Robots and Systems, 2008. 2

[30] Radu Bogdan Rusu and Steve Cousins. 3D is here: Point
Cloud Library (PCL). In IEEE International Conference on
Robotics and Automation (ICRA), Shanghai, China, May 9-
13 2011. 5, 6

[31] Andrew M. Saxe, James L. McClelland, and Surya Ganguli.
Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks. In arXiv:1312.6120, 2013. 6

[32] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram
Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene co-
ordinate regression forests for camera relocalization in rgb-d
images. In IEEE conference on Computer Vision and Pattern
Recognition (CVPR), 2013. 5

[33] J.T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-
miller. Striving for Simplicity: The All Convolutional Net.
In International Conference on Machine Learning (ICLR) -
workshop track, 2015. 5

[34] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958, 2014. 5

[35] Pascal Theiler, Jan D. Wegner, and Konrad Schindler. Glob-
ally consistent registration of terrestrial laser scans via graph
optimization.
ISPRS Journal of Photogrammetry and Re-
mote Sensing, 109:126–136, 2015. 1

[36] Yurun Tian, Bin Fan, and Fuchao Wu. L2-Net: Deep Learn-
ing of Discriminative Patch Descriptor in Euclidean Space.
In IEEE conference on Computer Vision and Pattern Recog-
nition (CVPR), 2017. 5

[37] Federico Tombari, Samuele Salti, and Luigi Di Stefano.
Unique shape context for 3D data description. In Proceed-
ings of the ACM workshop on 3D object retrieval, 2010. 2

[38] Federico Tombari, Samuele Salti, and Luigi Di Stefano.
Unique signatures of histograms for local surface descrip-
tion. In European conference on computer vision (ECCV),
2010. 1, 2, 6, 7, 8

[39] Julien Valentin, Angela Dai, Matthias Nießner, Pushmeet
Kohli, Philip Torr, Shahram Izadi, and Cem Keskin. Learn-
ing to Navigate the Energy Landscape.
arXiv preprint
arXiv:1603.05772, 2016. 5

[40] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao.
3D
ShapeNets: A deep representation for volumetric shapes. In
IEEE conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2015. 2, 4

[41] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.
Sun3d: A database of big spaces reconstructed using sfm and

5554

