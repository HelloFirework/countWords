Grounded Video Description

Luowei Zhou1,2, Yannis Kalantidis1, Xinlei Chen1, Jason J. Corso2, Marcus Rohrbach1

1 Facebook AI, 2 University of Michigan

github.com/facebookresearch/grounded-video-description

Abstract

Video description is one of the most challenging prob-
lems in vision and language understanding due to the large
variability both on the video and language side. Models,
hence, typically shortcut the difﬁculty in recognition and
generate plausible sentences that are based on priors but
are not necessarily grounded in the video. In this work, we
explicitly link the sentence to the evidence in the video by
annotating each noun phrase in a sentence with the cor-
responding bounding box in one of the frames of a video.
Our dataset, ActivityNet-Entities, augments the challeng-
ing ActivityNet Captions dataset with 158k bounding box
annotations, each grounding a noun phrase. This allows
training video description models with this data, and im-
portantly, evaluate how grounded or “true” such model are
to the video they describe. To generate grounded captions,
we propose a novel video description model which is able
to exploit these bounding box annotations. We demonstrate
the effectiveness of our model on our dataset, but also show
how it can be applied to image description on the Flickr30k
Entities dataset. We achieve state-of-the-art performance
on video description, video paragraph description, and im-
age description and demonstrate our generated sentences
are better grounded in the video.

1. Introduction

Image and video description models are frequently not
well grounded [14] which can increase their bias [9] and
lead to hallucination of objects [24], i.e. the model men-
tions objects which are not in the image or video e.g. be-
cause they might have appeared in similar contexts during
training. This makes models less accountable and trustwor-
thy, which is important if we hope such models will even-
tually assist people in need [2, 27]. Additionally, grounded
models can help to explain the model’s decisions to humans
and allow humans to diagnose them [20]. While researchers
have started to discover and study these problems for image
description [14, 9, 24, 20],1 they are even more pronounced

1We use description instead of captioning as captioning is often used
to refer to transcribing the speech in the video, not describing the content.

Figure 1: Word-level grounded video descriptions gener-
ated by our model on two segments from our ActivityNet-
Entities dataset. We also provide the descriptions generated
by our model without explicit bounding box supervision,
the descriptions generated by [42] and the ground-truth de-
scriptions (GT) for comparison.

for video description due to the increased difﬁculty and di-
versity, both on the visual and the language side.

Fig. 1 illustrates this problem. A video description ap-
proach (without grounding supervision) generated the sen-
tence “A man standing in a gym” which correctly mentions
“a man” but hallucinates “gym” which is not visible in the
video. Although a man is in the video it is not clear if the
model looked at the bounding box of the man to say this
word [9, 24]. For the sentence “A man [...] is playing the
piano” in Fig. 2, it is important to understand that which
“man” in the image “A man” is referring to, to determine if
a model is correctly grounded. Such understanding is cru-
cial for many applications when trying to build accountable
systems or when generating the next sentence or responding
to a follow up question of a blind person: e.g. answering “Is
he looking at me?” requires an understanding which of the
people in the image the model talked about.

The goal of our research is to build such grounded sys-
tems. As one important step in this direction, we col-

16578

A  man  is seen standing in a  room  speaking to the camera while holding a  bike .A group of  people  are in a  raft  down a  river .w/o grounding supervision: A man is standing in a gym .[42]: A man is seen speaking to the camera while holding a piece of exercise equipment.GT: A man in a room holds a bike and talks to the camera.w/o grounding supervision: A group of people are in a river.[42]: A large group of people are seen riding down a river and looking off into the distance.GT: Several people are on a raft in the water.lect ActivityNet-Entities (short as ANet-Entities) which
grounds or links noun phrases in sentences with bounding
boxes in the video frames. It is based on ActivityNet Cap-
tions [10], one of the largest benchmarks in video descrip-
tion. When annotating objects or noun phrases we specif-
ically annotate the bounding box which corresponds to the
instance referred to in the sentence rather than all instances
of the same object category, e.g. in Fig. 2, for the noun
phrase “the man” in the video description, we only anno-
tate the sitting man and not the standing man or the woman,
although they are all from the object category “person”. We
note that annotations are sparse, in the sense that we only
annotate a single frame of the video for each noun phrase.
ANet-Entities has a total number of 51.8k annotated video
segments/sentences with 157.8k labeled bounding boxes,
more details can be found in Sec. 3.

Our new dataset allows us to introduce a novel
grounding-based video description model that learns to
jointly generate words and reﬁne the grounding of the ob-
jects generated in the description. We explore how this
explicit supervision can beneﬁt the description generation
compared to unsupervised methods that might also utilize
region features but do not penalize grounding.

Our contributions are three-fold.

First, we collect
our large-scale ActivityNet-Entities dataset, which grounds
video descriptions to bounding boxes on the level of noun
phrases. Our dataset allows both, teaching models to explic-
itly rely on the corresponding evidence in the video frame
when generating words and evaluating how well models
are doing in grounding individual words or phrases they
generated. Second, we propose a grounded video descrip-
tion framework which is able to learn from the bounding
box supervision in ActivityNet-Entities and we demonstrate
its superiority over baselines and prior work in generating
grounded video descriptions. Third, we show the appli-
cability of the proposed model to image captioning, again
showing improvements in the generated captions and the
quality of grounding on the Flickr30k Entities [22] dataset.

2. Related Work

Video & Image Description. Early work on automatic
caption generation mainly includes template-based ap-
proaches [5, 12, 18], where predeﬁned templates with slots
are ﬁrst generated and then ﬁlled in with detected visual evi-
dences. Although these works tend to lead to well-grounded
methods, they are restricted by their template-based na-
ture. More recently, neural network and attention-based
methods have started to dominate major captioning bench-
marks. Visual attention usually comes in the form of tem-
poral attention [34] (or spatial-attention [32] in the image
domain), semantic attention [13, 35, 36, 41] or both [19].
The recent unprecedented success in object detection [23, 7]
has regained the community’s interests on detecting ﬁne-

grained visual clues while incorporating them into end-to-
end networks [16, 26, 1, 15]. Description methods which
are based on object detectors [16, 38, 1, 15, 5, 12] tackle
the captioning problem in two stages. They ﬁrst use off-
the-shelf or ﬁne-tuned object detectors to propose object
proposals/detections as for the visual recognition heavy-
lifting. Then, in the second stage, they either attend to the
object regions dynamically [16, 38, 1] or classify the re-
gions into labels and ﬁll into pre-deﬁned/generated sentence
templates [15, 5, 12]. However, directly generating propos-
als from off-the-shelf detectors causes the proposals to bias
towards classes in the source dataset (i.e. for object detec-
tion) v.s. contents in the target dataset (i.e. for description).
One solution is to ﬁne-tune the detector speciﬁcally for a
dataset [15] but this requires exhaustive object annotations
that are difﬁcult to obtain, especially for videos. Instead of
ﬁne-tuning a general detector, we transfer the object clas-
siﬁcation knowledge from off-the-shelf object detectors to
our model and then ﬁne-tune this representation as part of
our generation model with sparse box annotations. With
a focus on co-reference resolution and identifying people,
[26] proposes a framework that can refer to particular char-
acter instances and do visual co-reference resolution be-
tween video clips. However, their method is restricted to
identifying human characters whereas we study more gen-
eral the grounding of objects.
Attention Supervision. As ﬁne-grained grounding be-
comes a potential
incentive for next-generation vision-
language systems, to what degree it can beneﬁt remains an
open question. On one hand, for VQA [4, 39] the authors
point out that the attention model does not attend to same
regions as humans and adding attention supervision barely
helps the performance. On the other hand, adding super-
vision to feature map attention [14, 37] was found to be
beneﬁcial. We noticed in our preliminary experiments that
directly guiding the region attention with supervision [15]
does not necessary lead to improvements in automatic sen-
tence metrics. We hypothesize that this might be due to the
lack of object context information and we thus introduce a
self-attention [28] based context encoding in our attention
model, which allows information passing across all regions
in the sampled video frames.

3. ActivityNet-Entities Dataset

In order to train and test models capable of explicit
grounding-based video description, one requires both lan-
guage and grounding supervision. Although Flickr30k En-
tities [22] contains such annotations for images, no large-
scale description datasets with object localization annota-
tion exists for videos. The large-scale ActivityNet Cap-
tions dataset [10] contains dense language annotations for
about 20k videos from ActivityNet [3] but lacks ground-
ing annotations. Leveraging the language annotations from

6579

Dataset

Domain

# Vid/Img

# Sent

# Obj

# BBoxes

Flickr30k Entities [22]

MPII-MD [26]
YouCook2 [40]
ActivityNet Humans [33]
ActivityNet-Entities (ours)

Image

Video
Video
Video
Video

–train
–val
–test

32k

160k

≪1k ≪1k
15k
30k
52k
35k
8.6k
8.5k

2k
5.3k
15k
10k
2.5k
2.5k

480

4
67
1
432
432
427
421

276k

2.6k
135k
63k
158k
105k
26.5k
26.1k

Table 1: Comparison of video description datasets with
noun phrase or word-level grounding annotations. Our
ActivityNet-Entities and ActivityNet Humans [33] dataset
are both based on ActivityNet [3], but ActivityNet Humans
provides boxes only for person on a small subset of videos.
YouCook2 is restricted to cooking and only has box anno-
tations for the val and the test splits.

3.1. Annotation Process

We uniformly sampled 10 frames from each video seg-
ment and presented them to the annotators together with
the corresponding sentence. We asked the annotators to
identify all concrete NPs from the sentence describing the
video segment and then draw bounding boxes around them
in one frame of the video where the target NPs can be
clearly observed. Further instructions were provided in-
cluding guidelines for resolving co-references within a sen-
tence, i.e. boxes may correspond to multiple NPs in the sen-
tence (e.g., a single box could refer to both “the man” and
“him”) or when to use multi-instance boxes (e.g. “crowd”,
“a group of people” or “seven cats”). An annotated exam-
ple is shown in Fig. 2.
It is noteworthy that 10% of the
ﬁnal annotations refer to multi-instance boxes. We trained
annotators, and deployed a rigid quality control by daily in-
spection and feedback. All annotations were veriﬁed in a
second round. The full list of instructions provided to the
annotators, validation process, as well as screen-shots of the
annotation interface can be found in the Appendix.

3.2. Dataset Statistics and Analysis

As the test set annotations for the ActivityNet Captions
dataset are not public, we only annotate the segments in the
training (train) and validation (val) splits. This brings the
total number of annotated videos in ActivityNet-Entities to
14,281. In terms of segments, we ended up with about 52k
video segments with at least one NP annotation and 158k
NP bounding boxes in total.

Respecting the original protocol, we keep as our training
set the corresponding split from the ActivityNet Captions
dataset. We further randomly & evenly split the original
val set into our val set and our test set. We use all avail-
able bounding boxes for training our models, i.e., including
multi-instance boxes. Complete stats and comparisons with
other related datasets can be found in Tab. 1.
From Noun Phrases to Objects Labels. Although we

6580

Figure 2: An annotated example from our dataset. The
dashed box (“people”) indicates a group of objects.

the ActivityNet Captions dataset [10], we collected entity-
level bounding box annotations and created the ActivityNet-
Entities (ANet-Entities) dataset2, a rich dataset that can be
used for video description with explicit grounding. With
15k videos and more than 158k annotated bounding boxes,
ActivityNet-Entities is the largest annotated dataset of its
kind to the best of our knowledge.

When it comes to videos, region-level annotations come
with a number of unique challenges. A video contains more
information than can ﬁt in a single frame, and video descrip-
tions reﬂect that. They may reference objects that appear
in a disjoint set of frames, as well as multiple persons and
motions. To be more precise and produce ﬁner-grained an-
notations, we annotate noun phrases (NP) (deﬁned below)
rather than simple object labels. Moreover, one would ide-
ally have dense region annotations at every frame, but the
annotation cost in this case would be prohibitive for even
small datasets. Therefore in practice, video datasets are
typically sparsely annotated at the region level [6]. Favour-
ing scale over density, we choose to annotate segments as
sparsely as possible and annotate every noun phrase only in
one frame inside each segment.

Noun Phrases. Following [22], we deﬁne noun phrases as
short, non-recursive phrases that refer to a speciﬁc region in
the image, able to be enclosed within a bounding box. They
can contain a single instance or a group of instances and
may include adjectives, determiners, pronouns or preposi-
tions. For granularity, we further encourage the annotators
to split complex NPs into their simplest form (e.g. “the man
in a white shirt with a heart” can be split into three NPs:
“the man”, “a white shirt”, and “a heart”).

2ActivityNet-Entities

is

released at https://github.com/

facebookresearch/ActivityNet-Entities.

A man in a striped shirt is playing the pianoon the street whilepeoplewatch him.Figure 3: The proposed framework consists of three parts: the grounding module (a), the region attention module (b) and
the language generation module (c). Region proposals are ﬁrst represented with grounding-aware region encodings. The
language model then dynamically attends on the region encodings to generate each word. Losses are imposed on the attention
weights (attn-loss), grounding weights (grd-loss), and the region classiﬁcation probabilities (cls-loss). For clarity, the details
of the temporal attention are omitted.

chose to annotate noun phrases, in this work, we model sen-
tence generation as a word-level task. We follow the con-
vention in [15] to determine the list of object classes and
convert the NP label for box to a single-word object label.
First, we select all nouns and pronouns from the NP an-
notations using the Stanford Parser [17]. The frequency of
these words in the train and val splits are computed and a
threshold determines whether each word is an object class.
For ANet-Entities, we set the frequency threshold to be 50
which produces 432 object classes.
4. Description with Grounding Supervision

In this section we describe the proposed grounded video
description framework (see Fig. 3). The framework con-
sists of three modules: grounding, region attention and lan-
guage generation. The grounding module detects visual
clues from the video, the region attention dynamically at-
tends on the visual clues to form a high-level impression of
the visual content and feeds it to the language generation
module for decoding. We illustrate three options for incor-
porating the object-level supervision: region classiﬁcation,
object grounding (localization), and supervised attention.

4.1. Overview

We formulate the problem as a joint optimization over
the language and grounding tasks. The overall loss function
consists of four parts:

L = Lsent + λαLattn + λcLcls + λβLgrd,

(1)

where Lsent denotes the teacher-forcing language genera-
tion cross-entropy loss, commonly used for language gen-
eration tasks (details in Sec. 4.2). Lattn corresponds to the
cross entropy region attention loss which is presented in
Sec. 4.3. Lcls and Lgrd are cross-entropy losses that cor-

respond to the grounding module for region classiﬁcation
and supervised object grounding (localization), respectively
(Sec. 4.4). The three grounding-related losses are weighted
by coefﬁcients λα, λc, and λβ which we selected on the
dataset validation split.

We denote the input video (segment) as V and
the target/generated sentence description (words) as S.
We uniformly sample F frames from each video as
{v1, v2, . . . , vF } and deﬁne Nf object regions on sam-
pled frame f . Hence, we can assemble a set of regions
R = [R1, . . . , RF ] = [r1, r2, . . . , rN ] ∈ Rd×N to rep-
resent the video, where N = PF
f =1 Nf is the total num-
ber of regions. We overload the notation here and use ri
(i ∈ {1, 2, . . . , N }) to also represent region feature embed-
dings, as indicated by fc6 in Fig. 3. We represent words in
S with one-hot vectors which are further encoded to word
embeddings yt ∈ Re where t ∈ {1, 2, . . . , T }, where T
indicates the sentence length and e is the embedding size.

4.2. Language Generation Module

For language generation, we adapt the language model
from [15] for video inputs, i.e. extend it to incorporate tem-
poral information. The model consists of two LSTMs: the
ﬁrst one for encoding the global video feature and the word
embedding yt into the hidden state ht
A ∈ Rm where m is
the dimension and the second one for language generation
(see Fig. 3c). The language model dynamically attends on
videos frames or regions for visual clues to generate words.
We refer to the attention on video frames as temporal atten-
tion and the one on regions as region attention.

The temporal attention takes in a sequence of frame-wise
feature vectors and determines by the hidden state how sig-
niﬁcant each frame should contribute to generate a descrip-

6581

LanguageLSTMfc6fc7𝜷t+1KsoftmaxFC(a) Grounding Modulespatio-temporal feature cls-lossKddsself-attention attentionAttentionLSTM𝛂tgrd-lossKxN mexpandhAt-1hAtattn-lossglobal videofeaturesegment positionalencodingythAthLt-1hLttemporalattentionFCsoftmaxlang-loss(c) Language Generation Moduledall regions(b) Region Attention ModulehAthLttion word. We deploy a similar module as in [42], except
that we replace the self-attention context encoder with Bi-
directional GRU (Bi-GRU) which yields superior results.
We train with cross-entropy loss Lsent.

4.3. Region Attention Module

A),

1, αt

2, . . . , αt

α tanh(Wr ˜ri+Whht

Unlike temporal attention that works on a frame level,
the region attention [1, 15] focuses on more ﬁne-grained
details in the video, i.e., object regions [23]. We denote the
region encoding as ˜R = [˜r1, ˜r2, . . . , ˜rN ], more details are
deﬁned later in Eq. 5. At time t of the caption generation,
the attention weight over region i is formulated as:
αt := Softmax(αt), (2)
αt
i = w⊤
where Wr ∈ Rm×d, Wh ∈ Rm×m, wα ∈ Rm, and
αt = [αt
N ]. The region attention encoding is
then ˜Rαt and along with the temporal attention encoding,
fed into the language LSTM.
Supervised Attention. We want to encourage the language
model to attend on the correct region when generating a
visually-groundable word. As this effectively assists the
language model in learning to attend to the correct region,
we call this attention supervision. Denote the indicators of
positive/negative regions as γt = [γt
N ], where
γt
i = 1 when the region ri has over 0.5 IoU with the GT
box rGT and otherwise 0. We regress αt to γt and hence
the attention loss for object word st can be deﬁned as:

2, . . . , γt

1, γt

Lattn = −

N

X

i=1

i log αt
γt
i.

(3)

4.4. Grounding Module

Assume we have a set of visually-groundable object
class labels {c1, c2, . . . , cK}, short as object classes, where
K is the total number of classes. Given a set of object re-
gions from all sampled frames, the grounding module esti-
mates the class probability distribution for each region.

We deﬁne a set of object classiﬁers as Wc =
[w1, w2, . . . , wK] ∈ Rd×K and the learnable scalar bi-
ases as B = [b1, b2, . . . , bK]. So, a naive way to esti-
mate the class probabilities for all regions (embeddings)
R = [r1, r2, . . . , rN ] is through dot-product:

Ms(R) = Softmax(W ⊤

c R + B✶⊤),

(4)

where ✶ is a vector with all ones, W ⊤
c R is followed by
a ReLU and a Dropout layer, and Ms is the region-class
similarity matrix as it captures the similarity between re-
gions and object classes. For clarity, we omit the ReLU and
Dropout layer after the linear embedding layer throughout
Sec. 4 unless otherwise speciﬁed. The Softmax operator is
applied along the object class dimension of Ms to ensure
the class probabilities for each region sum up to 1.

We transfer detection knowledge from an off-the-shelf
detector that is pre-trained on a general source dataset, i.e.,

Visual Genome (VG) [11], to our object classiﬁers. We ﬁnd
the nearest neighbor for each of the K object classes from
the VG object classes according to their distances in the em-
bedding space (glove vectors [21]). We then initialize Wc
and B with the corresponding classiﬁer, i.e., the weights
and biases, from the last linear layer of the detector.

On the other hand, we represent the spatial and tempo-
ral conﬁguration of the region as a 5-D tuple, including 4
values for the normalized spatial location and 1 value for
the normalized frame index. Then, the 5-D feature is pro-
jected to a ds = 300-D location embedding for all the re-
gions Ml ∈ R300×N . Finally, we concatenate all three com-
ponents:
i) region feature, ii) region-class similarity ma-
trix, and iii) location embedding together and project into
a lower dimension space (m-D):

˜R = Wg[ R | Ms(R) | Ml ],

(5)

where [·|·] indicates a row-wise concatenation and Wg ∈
Rm×(d+K+ds) are the embedding weights. We name ˜R
the grounding-aware region encoding, corresponding to the
right portion of Fig. 3a. To further model the relations
between regions, we deploy a self-attention layer over ˜R
[28, 42]. The ﬁnal region encoding is fed into the region
attention module (see Fig. 3b).

So far the object classiﬁer discriminates classes without
the prior knowledge about the semantic context, i.e., the in-
formation the language model has captured. To incorpo-
rate semantics, we condition the class probabilities on the
sentence encoding from the Attention LSTM. A memory-
efﬁcient approach is treating attention weights αt as this
semantic prior, as formulated below:

),

M t

c R + B✶⊤ + ✶αt⊤

s(R, αt) = Softmax(W ⊤

(6)
where the region attention weights αt are determined by
Eq. 2. Note that here the Softmax operator is applied row-
wise to ensure the probabilities on regions sum up to 1.
To learn a reasonable object classiﬁer, we can deploy a re-
gion classiﬁcation task on Ms(R) or a sentence-conditioned
grounding task on M t
s(R, αt), with the word-level ground-
ing annotations from Sec. 3. Next, we describe them both.

Region Classiﬁcation. We ﬁrst deﬁne a positive region as
a region that has over 0.5 intersection over union (IoU) with
an arbitrary ground-truth (GT) box. If a region matches to
multiple GT boxes, the one with the largest IoU is the ﬁnal
matched GT box. Then we classify the positive region, say
region i to the same class label as in the GT box, say class
cj . The normalized class probability distribution is hence
Ms[:, i] and the cross-entropy loss on class cj is

Lcls = − log Ms[j, i].

(7)

The ﬁnal Lcls is the average of losses on all positive regions.
Object Grounding. Given a visually-groundable word
st+1 at time step t + 1 and the encoding of all the pre-
vious words, we aim to localize st+1 in the video as one

6582

or a few of the region proposals. Supposing st+1 corre-
sponds to class cj , we regress the conﬁdence score of re-
gions M t
N ] to indica-
tors γt as deﬁned in Sec. 4.3. The grounding loss for word
st+1 is deﬁned as:

s[j, :] = βt+1 = [βt+1

, . . . , βt+1

, βt+1

1

2

N

Lgrd = −

X

i log βt+1
γt

i

.

i=1

(8)

Note that the ﬁnal loss on Lattn or Lgrd is the average
of losses on all visually-groundable words. The difference
between the attention supervision and the grounding su-
pervision is that, in the latter task, the target object cj is
known beforehand, while the attention module is not aware
of which object to seek in the scene.

5. Experiments

Datasets. We conduct most experiments and ablation stud-
ies on the newly-collected ActivityNet-Entities dataset on
video description given the set of temporal segments (i.e.
using the ground-truth events from [10]) and video para-
graph description [30]. We also demonstrate our framework
can easily be applied to image description and evaluate it on
the Flickr30k Entities dataset [22]. Note that we did not ap-
ply our method to COCO captioning as there is no exact
match between words in COCO captions and object anno-
tations in COCO (limited to only 80). We use the same pro-
cess described in Sec. 3.2 to convert NPs to object labels.
Since Flickr30k Entities contains more captions, labels that
occur at least 100 times are taken as object labels, resulting
in 480 object classes [15].
Pre-processing. For ANet-Entities, we truncate captions
longer than 20 words and build a vocabulary on words with
at least 3 occurrences. For Flickr30k Entities, since the cap-
tions are generally shorter and it is a larger corpus, we trun-
cate captions longer than 16 words and build a vocabulary
based on words that occur at least 5 times.

5.1. Compared Methods and Metrics

Compared methods. The state-of-the-art (SotA) video de-
scription methods on ActivityNet Captions include Masked
Transformer and Bi-LSTM+TempoAttn [42]. We re-train
the models on our dataset splits with the original settings.
For a fair comparison, we use exactly the same frame-wise
feature from this work for our temporal attention module.
For video paragraph description, we compare our meth-
ods against the SotA method MFT [30] with the evalua-
tion script provided by the authors [30]. For image caption-
ing, we compare against two SotA methods, Neural Baby
Talk (NBT) [15] and BUTD [1]. For a fair comparison, we
provide the same region proposal and features for both the
baseline BUTD and our method, i.e., from Faster R-CNN
pre-trained on Visual Genome (VG). NBT is specially tai-
lored for each dataset (e.g., detector ﬁne-tuning), so we re-

tain the same feature as in the paper, i.e., from ResNet pre-
trained on ImageNet. All our experiments are performed
three times and the average scores are reported.
Metrics. To measure the object grounding and atten-
tion correctness, we ﬁrst compute the localization accuracy
(Grd. and Attn.
in the tables) over GT sentences follow-
ing [25, 40]. Given an unseen video, we feed the GT sen-
tence into the model and measure the localization accuracy
at each annotated object word. We compare the region with
the highest attention weight (αi) or grounding weight (βj )
against the GT box. An object word is correctly localized
if the IoU is over 0.5. We also study the attention accuracy
on generated sentences, denoted by F1all and F1loc in the
tables. In F1all, a region prediction is considered correct
if the object word is correctly predicted and also correctly
localized. We also compute F1loc, which only considers
correctly-predicted object words. See Appendix for details.
Due to the sparsity of the annotation, i.e., each object only
annotated in one frame, we only consider proposals in the
frame of the GT box when computing all the localization ac-
curacies. For the region classiﬁcation task, we compute the
top-1 classiﬁcation accuracy (Cls. in the tables) for positive
regions. For all metrics, we average the scores across object
classes. To evaluate the sentence quality, we use standard
language evaluation metrics, including Bleu@1, Bleu@4,
METEOR, CIDEr, and SPICE, and the ofﬁcial evaluation
script3. We additionally perform human evaluation to judge
the sentence quality.

5.2. Implementation Details

Region proposal and feature. We uniformly sample 10
frames per video segment (an event in ANet-Entities) and
extract region features. For each frame, we use a Faster
R-CNN detector [23] with ResNeXt-101 backbone [29] for
region proposal and feature extraction (fc6). The detector is
pretrained on Visual Genome [11]. More model and train-
ing details are in the Appendix.
Feature map and attention. The temporal feature map
is essentially a stack of frame-wise appearance and motion
features from [42, 31]. The spatial feature map is the conv4
layer output from a ResNet-101 [15, 8] model. Note that
an average pooling on the temporal or spatial feature map
gives the global feature. In video description, we augment
the global feature with segment positional information (i.e.,
total number of segments, segment index, start time and end
time), which is empirically important.
Hyper-parameters. Coefﬁcients λα ∈ {0.05, 0.1, 0.5},
λβ ∈ {0.05, 0.1, 0.5}, and λc ∈ {0.1, 0.5, 1} vary in
the experiments as a result of model validation. We
set λα = λβ when they are both non-zero considering
the two losses have a similar functionality. The region
encoding size d = 2048, word embedding size e = 512

3https://github.com/ranjaykrishna/densevid eval

6583

Method

λα

λβ

Unsup. (w/o SelfAttn)
Unsup.
Sup. Attn.
Sup. Grd.
Sup. Cls.
Sup. Attn.+Grd.
Sup. Attn.+Cls.
Sup. Grd. +Cls.
Sup. Attn.+Grd.+Cls.

0
0
0.05
0
0
0.5
0.05
0
0.1

0
0
0
0.5
0
0.5
0
0.05
0.1

λc

0
0
0
0
0.1
0
0.1
0.1
0.1

B@1

B@4 M

C

23.2
23.0
23.7
23.5
23.3
23.8
23.9
23.8
23.8

2.28
2.27
2.56
2.50
2.43
2.44
2.59
2.59
2.57

10.9
10.7
11.1
11.0
10.9
11.1
11.2
11.1
11.1

45.6
44.6
47.0
46.8
45.7
46.1
47.5
47.5
46.9

S

15.0
13.8
14.9
14.7
14.1
14.8
15.1
15.0
15.0

Attn.

Grd.

F1all

F1loc

14.9
2.42
34.0
31.9
2.59
35.1
34.5
27.1
35.7

21.3
19.7
37.5
43.2
25.8
40.6
41.6
45.7
44.9

3.70
0.28
6.72
6.04
0.35
6.79
7.11
4.79
7.10

12.7
1.13
22.7
21.2
1.43
23.0
24.1
17.6
23.8

Cls.

6.89
6.06
0.42
0.07
14.9
0
14.2
13.8
12.2

Table 2: Results on ANet-Entities val set. “w/o SelfAttn” indicates self-attention is not used for region feature encoding.
Notations: B@1 - Bleu@1, B@4 - Bleu@4, M - METEOR, C - CIDEr, S - SPICE. Attn. and Grd. are the object localization
accuracies for attention and grounding on GT sentences. F1all and F1loc are the object localization accuracies for attention
on generated sentences. Cls. is classiﬁcation accuracy. All accuracies are in %. Top two scores on each metric are in bold.

Method

B@1 B@4 M

C

S

Attn. Grd.

F1all

F1loc Cls.

Masked Transformer [42]
Bi-LSTM+TempoAttn [42]

Our Unsup. (w/o SelfAttn)
Our Sup. Attn.+Cls. (GVD)

22.9
22.8

23.1
23.6

2.41
2.17

2.16
2.35

10.6
10.2

10.8
11.0

46.1
42.2

44.9
45.5

13.7
11.8

14.9
14.7

–
–

–
–

–
–

–
–

–
–

16.1
34.7

22.3
43.5

3.73
7.59

11.7
25.0

6.41
14.5

(a) Results on ANet-Entities test set.

vs. Unsupervised

vs. [42]

Judgments
%

∆

Judgments
∆

%

Method

About Equal

Other is better
GVD is better

34.9

29.3
35.8

38.9

27.5
33.6

6.5

6.1

(b) Human evaluation of sentences.

Table 3: (a) Results on ANet-Entities test set. The top one score for each metric is in bold. (b) Human evaluation of sentence
quality. We present results for our supervised approach vs. our unsupervised baseline and vs. Masked Transformer [42].

and RNN encoding size m = 1024 for all methods. Other
hyper-parameters in the language module are the same as
in [15]. We use a 2-layer 6-head Transformer encoder as
the self-attention module [42].

5.3. Results on ActivityNet Entities

5.3.1 Video Event Description

Although dense video description [11] further entails local-
izing the segments to describe on the temporal axis, in this
paper we focus on the language generation part and assume
the temporal boundaries for events are given. We name this
task Video Event Description. Results on the validation and
test splits of our ActivityNet-Entities dataset are shown in
Tab. 2 and Tab. 3a, respectively. Given the selected set of re-
gion proposals, the localization upper bound on the val/test
sets is 82.5%/83.4%, respectively.

In general, methods with some form of grounding super-
vision work consistently better than the methods without.
Moreover, combining multiple losses, i.e. stronger super-
vision, leads to higher performance. On the val set, the
best variant of supervised methods (i.e., Sup. Attn.+Cls.)
ourperforms the best variant of unsupervised methods (i.e.,
Unsup. (w/o SelfAttn)) by a relative 1-13% on all the met-
rics. On the test set, the gaps are small for Bleu@1, ME-
TEOR, CIDEr, and SPICE (within ± 2%), but the super-
vised method has a 8.8% relative improvement on Bleu@4.
The results in Tab. 3a show that adding box supervision
dramatically improves the grounding accuracy from 22.3%

to 43.5%. Hence, our supervised models can better localize
the objects mentioned which can be seen as an improve-
ment in their ability to explain or justify their own descrip-
tion. The attention accuracy also improves greatly on both
GT and generated sentences, implying that the supervised
models learn to attend on more relevant objects during lan-
guage generation. However, grounding loss alone fails with
respect to classiﬁcation accuracy (see Tab. 2), and therefore
the classiﬁcation loss is required in that case. Conversely,
the classiﬁcation loss alone can implicitly learn grounding
and maintains a fair grounding accuracy.

Comparison to existing methods. We refer to our best
model (Sup. Attn.+Cls.) as GVD (Grounded Visual De-
scription) and show that it sets the new SotA on ActivityNet
Captions for the Bleu@1, METEOR and SPICE metrics,
with relative gains of 2.8%, 3.9% and 6.8%, respectively
over the previous best [42]. We observe slightly inferior
results on Bleu@4 and CIDEr (-2.8% and -1.4%, respec-
tively) but after examining the generated sentences (see Ap-
pendix) we see that [42] generates repeated words way more
often. This may increase the aforementioned evaluation
metrics, but the generated descriptions are of lower quality.
Another noteworthy observation is that the self-attention
context encoder (on top of ˜R) brings consistent improve-
ments on methods with grounding supervision, but hurts
the performance of methods without, i.e., “Unsup.”. We
hypothesize that the extra context and region interaction in-
troduced by the self-attention confuses the region attention
module and without any grounding supervision makes it fail

6584

Method

VG Box B@1

B@4 M

C

ATT-FCN* [36]
NBT* [15]
BUTD [1]

X

Our Unsup. (w/o SelfAttn) X
X
Our GVD model

64.7
69.0
69.4

69.2
69.9

19.9
27.1
27.3

26.9
27.3

18.5
21.7
21.7

22.1
22.5

–
57.5
56.6

60.1
62.3

X

X

S

–
15.6
16.0

16.1
16.5

Attn.

Grd.

F1all

F1loc

–
–
24.2

21.4
41.4

–
–
32.3

25.5
50.9

–
–
4.53

3.88
7.55

–
–
13.0

11.7
22.2

Cls.

–
–
1.84

17.9
19.2

Table 4: Results on Flickr30k Entities test set. * indicates the results are obtained from the original papers. GVD refers to
our Sup. Attn.+Grd.+Cls. model. “VG” indicates region features are from VG pre-training. The top one score is in bold.

Method

B@1

B@4

M

C

MFT [30]
Our Unsup. (w/o SelfAttn)
Our GVD

45.5
49.8
49.9

9.78
10.5
10.7

14.6
15.6
16.1

20.4
21.6
22.2

Table 5: Results of video paragraph description on test set.

to properly attend to the right region, something that leads
to a huge attention accuracy drop from 14.9% to 2.42%.
Human Evaluation. Automatic metrics for evaluating gen-
erated sentences have frequently shown to be unreliable
and not consistent with human judgments, especially for
video description when there is only a single reference [27].
Hence, we conducted a human evaluation to evaluate the
sentence quality on the test set of ActivityNet-Entities. We
randomly sampled 329 video segments and presented the
segments and descriptions to the judges. From Tab. 3b, we
observe that, while they frequently produce captions with
similar quality, our GVD works better than the unsuper-
vised baseline (with a signiﬁcant gap of 6.1%). We can also
see that our GVD approach works better than the Masked
Transformer [42] with a signiﬁcant gap of 6.5%. We be-
lieve these results are a strong indication that our approach
is not only better grounded but also generates better sen-
tences, both compared to baselines and prior work [42].

5.3.2 Video Paragraph Description

Besides measuring the quality of each individual descrip-
tion, we also evaluate the coherence among sentences
within a video as in [30]. We obtained the result ﬁle and
evaluation script from [30] and evaluated both methods on
our test split. The results are shown in Tab. 5 and show that
we outperform the SotA method of [30] by a large margin.
The results are even more surprising given that we generate
description for each event separately, without conditioning
on previously-generated sentences. We hypothesize that the
temporal attention module can effectively model the event
context through the Bi-GRU context encoder and context
beneﬁts the coherence of consecutive sentences.

5.4. Results on Flickr30k Entities

We show the overall results on image description in
Tab. 4 (test) and the results on the validation set are in

the Appendix. The method with the best validation CIDEr
score is the full model (Sup. Attn.+Grd.+Cls.), which we
further refer to as the GVD model in the table. The upper
bounds on the val/test sets are 90.0%/88.5%, respectively.
We see that the supervised method outperforms the unsu-
pervised baseline by a relative 1-3.7% over all the metrics.
Our GVD model sets new SotA for all the ﬁve metrics with
relative gains up to 10%.
In the meantime, object local-
ization and region classiﬁcation accuracies are signiﬁcantly
boosted, showing that our captions can be better visually
explained and understood.

6. Conclusion

In this work, we collected ActivityNet-Entities, a novel
dataset that allows joint study of video description and
grounding. We show how to leverage the noun phrase an-
notations to generate grounded video descriptions. We also
use our dataset to evaluate how well the generated sentences
are grounded. We believe our large-scale annotations will
also allow for more in-depth analysis which have previ-
ously only been able on images, e.g. about hallucination
[24] and bias [9] as well as studying co-reference resolution.
Besides, we showed in our comprehensive experiments on
video and image description, how the box supervision can
improve the accuracy and the explainability of the gener-
ated description by not only generating sentences but also
pointing to the corresponding regions in the video frames
or image. According to automatic metrics and human eval-
uation, on ActivityNet-Entities our model performs state-
of-the-art w.r.t. description quality, both when evaluated per
sentence or on paragraph level with a signiﬁcant increase
in grounding performance. We also adapted our model to
image description and evaluated it on the Flickr30k Enti-
ties dataset where our model outperforms existing methods,
both w.r.t. description quality and grounding accuracy.

Acknowledgement. The technical work was performed
during Luowei’s summer internship at Facebook AI Re-
search.
Luowei Zhou and Jason Corso were partly
supported by DARPA FA8750-17-2-0125 and NSF IIS
1522904 as part of their afﬁliation with University of Michi-
gan. This article solely reﬂects the opinions and conclusions
of its authors but not the DARPA or NSF.

6585

References

[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
visual question answering. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
6077–6086, 2018. 2, 5, 6, 8

[2] Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Lit-
tle, Andrew Miller, Robert C. Miller, Robin Miller, Aubrey
Tatarowicz, Brandyn White, Samual White, and Tom Yeh.
Vizwiz: Nearly real-time answers to visual questions.
In
Proceedings of the 23Nd Annual ACM Symposium on User
Interface Software and Technology, UIST ’10, pages 333–
342, New York, NY, USA, 2010. ACM. 1

[3] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. Activitynet: A large-scale video
benchmark for human activity understanding.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 961–970, 2015. 2, 3

[4] Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh,
and Dhruv Batra. Human attention in visual question an-
swering: Do humans and deep networks look at the same re-
gions? Computer Vision and Image Understanding, 163:90–
100, 2017. 2

[5] Pradipto Das, Chenliang Xu, Richard F Doell, and Jason J
Corso. A thousand frames in just a few words: Lingual
description of videos through latent topics and sparse ob-
ject stitching.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 2634–2641,
2013. 2

[6] Chunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Car-
oline Pantofaru, David A Ross, George Toderici, Yeqing Li,
Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, et al.
Ava: A video dataset of spatio-temporally localized atomic
visual actions. In Proceedings of the IEEE international con-
ference on computer vision, 2018. 3

[7] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE
International Conference on, pages 2980–2988. IEEE, 2017.
2

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 6

[9] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor
Darrell, and Anna Rohrbach. Women also snowboard: Over-
coming bias in captioning models.
In Proceedings of the
European Conference on Computer Vision, pages 771–787,
2018. 1, 8

[10] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV), pages 706–715, 2017. 2, 3, 6

[11] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense

image annotations.
sion, 123(1):32–73, 2017. 5, 6, 7

International Journal of Computer Vi-

[12] Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sag-
nik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and
Tamara L Berg. Babytalk: Understanding and generat-
ing simple image descriptions. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 35(12):2891–2903,
2013. 2

[13] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and
Tao Mei. Jointly localizing and describing events for dense
video captioning.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 7492–
7500, 2018. 2

[14] Chenxi Liu, Junhua Mao, Fei Sha, and Alan L Yuille. Atten-
tion correctness in neural image captioning. In Proceedings
of the Conference on Artiﬁcial Intelligence (AAAI), pages
4176–4182, 2017. 1, 2

[15] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
In Proceedings of the IEEE Conference
Neural baby talk.
on Computer Vision and Pattern Recognition, pages 7219–
7228, 2018. 2, 4, 5, 6, 7, 8

[16] Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan
AlRegib, and Hans Peter Graf. Attend and interact: Higher-
order object interactions for video understanding.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6790–6800, 2018. 2

[17] Christopher Manning, Mihai Surdeanu, John Bauer, Jenny
Finkel, Steven Bethard, and David McClosky. The stanford
corenlp natural language processing toolkit. In Proceedings
of 52nd annual meeting of the association for computational
linguistics: system demonstrations, pages 55–60, 2014. 4

[18] Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Men-
sch, Amit Goyal, Alex Berg, Kota Yamaguchi, Tamara Berg,
Karl Stratos, and Hal Daum´e III. Midge: Generating image
descriptions from computer vision detections. In Proceed-
ings of the 13th Conference of the European Chapter of the
Association for Computational Linguistics, pages 747–756.
Association for Computational Linguistics, 2012. 2

[19] Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video
captioning with transferred semantic attributes. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), volume 2, page 3, 2017. 2

[20] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata,
Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Mar-
cus Rohrbach. Multimodal explanations: Justifying deci-
sions and pointing to the evidence.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 1

[21] Jeffrey Pennington, Richard Socher, and Christopher Man-
In
ning. Glove: Global vectors for word representation.
Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP), pages 1532–1543,
2014. 5

[22] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-
nik. Flickr30k entities: Collecting region-to-phrase corre-
spondences for richer image-to-sentence models.
In Pro-

6586

ceedings of the IEEE international conference on computer
vision, pages 2641–2649, 2015. 2, 3, 6

International Conference on Computer Vision, ICCV, pages
22–29, 2017. 2

[36] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and
Jiebo Luo.
In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 4651–4659, 2016. 2, 8

Image captioning with semantic attention.

[37] Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo,
Sang-Hun Lee, and Gunhee Kim. Supervising neural atten-
tion models for video captioning by human gaze data.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR 2017). Honolulu, Hawaii, pages 2680–29, 2017.
2

[38] Mihai Zanﬁr, Elisabeta Marinoiu, and Cristian Sminchis-
escu. Spatio-temporal attention models for grounded video
captioning. In Asian Conference on Computer Vision, pages
104–119, 2016. 2

[39] Yundong Zhang, Juan Carlos Niebles, and Alvaro Soto. In-
terpretable visual question answering by visual grounding
from attention supervision mining.
In 2019 IEEE Winter
Conference on Applications of Computer Vision (WACV),
pages 349–357. IEEE, 2019. 2

[40] Luowei Zhou, Nathan Louis, and Jason J Corso. Weakly-
supervised video object grounding from text by loss weight-
ing and object interaction. Proceedings of the British Ma-
chine Vision Conference (BMVC), 2018. 3, 6

[41] Luowei Zhou, Chenliang Xu, Parker Koch, and Jason J
Corso. Watch what you just said: Image captioning with
text-conditional attention. In Proceedings of the on Thematic
Workshops of ACM Multimedia 2017, pages 305–313. ACM,
2017. 2

[42] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher,
and Caiming Xiong. End-to-end dense video captioning with
masked transformer. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 8739–
8748, 2018. 1, 5, 6, 7, 8

[23] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, pages 91–99, 2015. 2, 5, 6

[24] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor
Darrell, and Kate Saenko. Object hallucination in image cap-
tioning. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 4035–4045,
2018. 1, 8

[25] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor
Darrell, and Bernt Schiele. Grounding of textual phrases in
images by reconstruction. In European Conference on Com-
puter Vision, pages 817–834. Springer, 2016. 6

[26] Anna Rohrbach, Marcus Rohrbach, Siyu Tang, Seong Joon
Oh, and Bernt Schiele.
Generating descriptions with
grounded and co-referenced people. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 2, 3

[27] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket
Tandon, Chris Pal, Hugo Larochelle, Aaron Courville, and
Bernt Schiele. Movie description. International Journal of
Computer Vision (IJCV), 2017. 1, 8

[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems, pages 5998–6008, 2017. 2,
5

[29] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In Computer Vision and Pattern Recogni-
tion (CVPR), 2017 IEEE Conference on, pages 5987–5995.
IEEE, 2017. 6

[30] Yilei Xiong, Bo Dai, and Dahua Lin. Move forward and tell:
A progressive generator of video descriptions. Proceedings
of the European Conference on Computer Vision, 2018. 6, 8
[31] Yuanjun Xiong, Limin Wang, Zhe Wang, Bowen Zhang,
Hang Song, Wei Li, Dahua Lin, Yu Qiao, Luc Van Gool, and
Xiaoou Tang. Cuhk & ethz & siat submission to activitynet
challenge 2016. arXiv preprint arXiv:1608.00797, 2016. 6

[32] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In International conference on
machine learning, pages 2048–2057, 2015. 2

[33] Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku, and
Tatsuya Harada. Spatio-temporal person retrieval via natural
language queries. In Proceedings of the IEEE International
Conference on Computer Vision, pages 1453–1462, 2017. 3
[34] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas,
Christopher Pal, Hugo Larochelle, and Aaron Courville. De-
scribing videos by exploiting temporal structure. In Proceed-
ings of the IEEE international conference on computer vi-
sion, pages 4507–4515, 2015. 2

[35] Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao
In IEEE

Mei. Boosting image captioning with attributes.

6587

