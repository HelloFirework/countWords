ECC: Platform-Independent Energy-Constrained Deep Neural Network

Compression via a Bilinear Regression Model

Haichuan Yang1, Yuhao Zhu1, and Ji Liu2,1

1University of Rochester, Rochester, USA

2Kwai Seattle AI Lab, Seattle, USA

Abstract

Many DNN-enabled vision applications constantly op-
erate under severe energy constraints such as unmanned
aerial vehicles, Augmented Reality headsets, and smart-
phones. Designing DNNs that can meet a stringent energy
budget is becoming increasingly important. This paper pro-
poses ECC, a framework that compresses DNNs to meet
a given energy constraint while minimizing accuracy loss.
The key idea of ECC is to model the DNN energy consump-
tion via a novel bilinear regression function. The energy
estimate model allows us to formulate DNN compression
as a constrained optimization that minimizes the DNN loss
function over the energy constraint. The optimization prob-
lem, however, has nontrivial constraints. Therefore, exist-
ing deep learning solvers do not apply directly. We pro-
pose an optimization algorithm that combines the essence
of the Alternating Direction Method of Multipliers (ADMM)
framework with gradient-based learning algorithms. The
algorithm decomposes the original constrained optimiza-
tion into several subproblems that are solved iteratively and
efﬁciently. ECC is also portable across different hardware
platforms without requiring hardware knowledge. Experi-
ments show that ECC achieves higher accuracy under the
same or lower energy budget compared to state-of-the-art
resource-constrained DNN compression techniques.

1. Introduction

Computer vision tasks are increasingly relying on deep
neural networks (DNNs). DNNs have demonstrated supe-
rior results compared to classic methods that rely on hand-
crafted features. However, neural networks are often sev-
eral orders of magnitude more computation intensive than
conventional methods [34, 45]. As a result, DNN-based
vision algorithms incur high latency and consume exces-
sive energy, posing signiﬁcant challenges to many latency-

sensitive and energy-constrained scenarios in the real-world
such as Augmented Reality (AR), autonomous drones, and
mobile robots. For instance, running face detection con-
tinuously on a mobile AR device exhausts the battery in
less than 45 minutes [20]. Reducing the latency and energy
consumption of DNN-based vision algorithms not only im-
proves the user satisfaction of today’s vision applications,
but also fuels the next-generation vision applications that
require ever higher resolution and frame rate.

Both the computer vision and hardware architecture
communities have been actively engaged in improving the
compute-efﬁciency of DNNs, of which a prominent tech-
nique is compression (e.g., pruning). Network compres-
sion removes unimportant network weights, and thus re-
duces the amount of arithmetic operations. However, prior
work [38, 39, 41] has shown that the number of non-zero
weights in a network, or the network sparsity, does not di-
rectly correlate with execution latency and energy consump-
tion. Thus, improving the network sparsity does not neces-
sarily lead to latency and energy reduction.

Recognizing that sparsity is a poor, indirect metric for
the actual metrics such as latency and energy consumption,
lots of recent compression work has started directly opti-
mizing for network latency [39, 11] and energy consump-
tion [38], and achieve lower latency and/or energy con-
sumption compare to the indirect approaches. Although
different in algorithms and implementation details, these ef-
forts share one common idea: they try to search the sparsity
bound of each DNN layer in a way that the whole model
satisﬁes the energy/latency constraint while minimizing the
loss. In other words, they iteratively search the layer spar-
sity, layer by layer, until a given latency/energy goal is met.
We refer to them as search-based approaches.

The effectiveness of the search-based approaches rests
on how close to optimal they can ﬁnd the per-layer spar-
sity combination. Different methods differ in how they
search for the optimal sparsity combination. For instance,

11206

Energy-Constrained Compression (ECC)

Online

Pre-trained 

Network

Constrained 
Optimization

Optional

Fine-tuning

Compressed 

Network

Table 1: Comparison across different resource-constrained DNN
compression techniques.

Oﬄine

Hardware Platform 

Energy

(Blackbox)

Energy Model 
Construction

Energy 

Estimation Model

Empirical Measurements

Figure 1: ECC framework overview.

NetAdapt [39] uses a heuristic-driven search algorithm
whereas AMC [11]1 uses reinforcement learning. However,
the search-based approaches are fundamentally limited by
the search space, which could be huge for deep networks.

In this paper, we propose an alternative DNN compres-
sion algorithm that compresses all the DNN layers together
rather than compressing it layer by layer. This strategy elim-
inates many of the heuristics and ﬁne-tuning required in pre-
vious layer-wise approaches. As a result, it is able to ﬁnd
compression strategies that lead to better latency and en-
ergy reductions. Due to the lack of compression techniques
that speciﬁcally target energy consumption, this paper fo-
cuses on energy consumption as a particular direct metric
to demonstrate the effectiveness of our approach, but we ex-
pect our approach to be generally applicable to other direct
metrics as well such latency and model size.

The key to our algorithm is to use a differentiable model
that numerically estimates the energy consumption of a net-
work. Leveraging this model, we formulate DNN compres-
sion as a constrained optimization problem (constrained by
a given energy budget). We propose an efﬁcient optimiza-
tion algorithm that combines ideas from both classic con-
strained optimizations and gradient-based DNN training.
Crucially, our approach is platform-free in that it treats the
underlying hardware platform as a blackbox. Prior energy-
constrained compressions all require deep understanding
of the underlying hardware architecture [38, 37], and thus
are necessarily tied to a particular hardware platform of
choice.
In contrast, our framework directly measures the
energy consumption of the target hardware platform with-
out requiring any hardware domain knowledges, and thus is
portable across different platforms.

Leveraging the constrained optimization algorithm, we
propose ECC, a DNN compression framework that auto-
matically compresses a DNN to meet a given energy bud-
get while maximizing its accuracy. ECC has two phases:
an ofﬂine energy modeling phase and an online compres-
sion phase. Given a particular network to compress, the of-
ﬂine component proﬁles the network on a particular target
platform and constructs an energy estimation model. The
online component leverages the energy model to solve the

1The method proposed in AMC originally targets model size, FLOPs
or latency, but can be extended to target energy consumption using our
modeling method introduced in Section 3.2.

Properties/Methods

EAP [38] AMC [11] NetAdapt [39] LcP [5] ECC

Use direct metric?
Target energy?
Optimization-based?
Platform-free?

X

X

X

X

X

X

X

X

X

X

X

X

constrained optimization problem followed by an optional
ﬁne-tuning phase before generating a compressed model.
In summary, we make the following contributions:
• We propose a bilinear energy consumption model of
DNN inference that models the DNN inference energy
as a function of both its weights and sparsity settings.
The energy model is constructed based on real hard-
ware energy measurement and thus requires no domain-
knowledge of the hardware architecture. Our model
shows < ±3% error rate.

• We propose ECC, a DNN compression framework that
maximizes the accuracy while meeting a given energy
budget. ECC leverages the energy estimation model to
formulate DNN compression as a constrained optimiza-
tion problem. We present an efﬁcient optimization algo-
rithm that combines the classic ADMM framework with
recent developments in gradient-based DNN algorithms.
Although targeting energy in this paper, our framework
can be extended to optimize other metrics such as latency
and model size.

• We evaluate ECC using a wide range of computer vision
tasks on both a mobile platform Jetson TX2 and a desk-
top platform with a GTX 1080 Ti GPU. We show that
ECC achieves higher accuracy under the same energy
budget compared to state-of-the-art resource-constrained
compression methods including NetAdapt
[39] and
AMC [11].

2. Related Work

Network Compression Network compression [18] is a
key technique in reducing DNN model complexity. It lever-
ages the observation that some network weights have less
impact on the ﬁnal results and thus could be removed (zero-
ed out). Compression techniques directly reduce a DNN
model’s size, which often also leads to latency and energy
reductions. Early compression techniques focus exclusively
on reducing the model size [9, 10, 36, 21, 44, 19] while la-
tency and energy reductions are “byproducts.” It is well-
known now that model size, latency, and energy consump-
tion are not directly correlated [38, 39, 41]. Therefore, com-
pressing for one metric, such as model size, does not always
translate to optimal compression results for other metrics
such as latency and energy reduction, and vice versa.

Resource-Constrained Compression Researchers re-
cently started investigating resource-constrained compres-

11207

latency,

sion, which compresses DNNs under explicit resource con-
straints (e.g., energy,
the number of Multiply-
accumulate operations) instead of using model size as a
proxy, though the model size could also be used as a con-
straint itself. Table 1 compares four such state-of-the-art
methods, EAP, AMC, and NetAdapt, and LcP. EAP [38]
compresses a model to reduce its energy consumption while
meeting a given accuracy threshold. AMC [11] compresses
a model to meet a given resource (size, FLOPs or latency)
constraint while maximizing the accuracy. NetAdapt [39]
compresses a model to meet a given latency constraint while
maximizing the accuracy. LcP [5] compresses a model to
meet a given resource constraint (the number of parameters
or the number of multiply-accumulate operations) while
maximizing the accuracy.

Although the four techniques target different metrics and
have different procedures, the core of them is to determine
the optimal sparsity ratio of each layer in a way that the
whole model meets the respective objectives. They differ
in how they determine the layer-wise sparsity ratio. EAP,
LcP, and NetAdapt all use heuristic-driven search algo-
rithms. Speciﬁcally, EAP compresses layers in the order
in which they contribute to the total energy, and prioritizes
compressing the most energy-hungry layers; NetAdapt it-
eratively ﬁnds the per-layer sparsity by incrementally de-
creasing the latency budget; LcP assigns a score to each
convolution ﬁlter, and prunes the ﬁlters based on the score
until the resource constraint is satisﬁed. AMC uses rein-
forcement learning that determines the per-layer sparsity by
“trial and error.”

ECC is a different compression technique that, instead
of compressing DNNs layer by layer, compresses all the
network layers at the same time. It avoids heuristic searches
and achieves better results.

Platform (In)dependence Previous energy-constrained
compressions [38, 37] rely on energy modeling that is tied
to a speciﬁc hardware architecture. ECC, in contrast, con-
structs the energy model directly from real hardware mea-
surements without requiring platform knowledges, and is
thus generally applicable to different hardware platforms.
AMC and NetAdapt are also platform-free as they take em-
pirical measurements from the hardware, but they target la-
tency and model size. In particular, NetAdapt constructs a
latency model using a look-up table whereas ECC’s energy
model is differentiable, which is key to formulating DNN
compression as a constrained optimization problem that can
be solved using gradient-based algorithms.

3. Method

This section introduces the proposed ECC framework for
energy-constrained DNN compression. We ﬁrst formulate
DNN compression as a constrained optimization under the
constraint of energy (Section 3.1). We then describe how

the energy is estimated using a bilinear model (Section 3.2).
Finally, we explain our novel gradient-based algorithm that
solves the optimization problem (Section 3.3).

3.1. Problem Formulation

Our objective is to minimize the loss function ℓ under a

predeﬁned energy constraint:

min
W

ℓ(W)

E(W) ≤ Ebudget,

(1a)

(1b)

where W := {w(u)}u∈U (U is the set of all layers) stacks
the weights tensors of all the layers, and E(W) denotes the
real energy consumption of the network, which depends on
the structure of DNN weights W. Compression affects the
DNN weights W, and thus affects E(W). ℓ is the loss func-
tion speciﬁc to a given learning task. In deep learning, ℓ is
a highly non-convex function.

There are two distinct classes of compression techniques.
Unstructured, ﬁne-grained compression prunes individual
elements [9, 10]; whereas structured, coarse-grained com-
pression prunes a regular structure of a DNN such as a ﬁlter
channel. While our method is applicable to both methods,
we particularly focus on the coarse-grained method that
prunes channels in DNN layers [33, 44, 19, 22, 12, 23, 25, 7]
because channel pruning is more effective on off-the-shelf
DNN hardware platforms such as GPUs [41] whereas the
ﬁne-grained approaches require specialized hardware archi-
tectures to be effective [4, 8, 37].

With the channel pruning method, the optimization prob-
lem becomes ﬁnding the sparsity of each layer, i.e., the
number of channels that are preserved in each layer, such
that the total energy meets the given budget, that is,

ℓ(W)

min
W,s
s.t. φ(w(u)) ≤ s(u),
E(s) ≤ Ebudget,

u ∈ U

(2a)

(2b)

(2c)

where s(u) corresponds to the sparsity bound of layer u ∈
U , and s := {s(u)}u∈U stacks the (inverse) sparsities of all
the layers. The energy consumption of a DNN E can now be
expressed as a function of s2. w(u) denotes the weight ten-
h × r(u)
sor of layer u. The shape of w(u) is d(u) × c(u) × r(u)
w
for the convolution layer u with d(u) output channels, c(u)
input channels, and spatial kernel size r(u)
w . With-
out loss of generality, we treat the fully connected layer as a
special convolution layer where r(u)
w = 1. φ(w(u))
(u)
calculates the layer-wise sparsity as Pi I(kw
·,i,·,·k 6= 0)
where I(·) is the indicator function which returns 1 if the
inside condition is satisﬁed and 0 otherwise.

h = r(u)

h × r(u)

2For simplicity, we reuse the same notion E in both Equation (1b)

and Equation (2c).

11208

3.2. Bilinear Energy Consumption Model

The key step to solving Equation (2) is to identify the en-
ergy model E(s), i.e., to model the DNN energy as a func-
tion of the sparsity of each layer. This step is particularly
important in that it provides an analytical form to charac-
terize the energy consumption. Existing DNN energy mod-
els are speciﬁc to a particular hardware platform [38, 37],
which requires deep understandings of the hardware and
is not portable across different hardware architectures. In
contrast, we construct an energy model directly from hard-
ware measurements while treating the hardware platform
as a blackbox. This is similar in spirit to NetAdapt [39],
which constructs a latency model through hardware mea-
surements. However, their model is a look-up table that is
huge and not differentiable. Our goal, however, is to con-
struct a differentiable model so that the optimization prob-
lem can be solved using conventional gradient-based algo-
rithms.

Our key idea is that the energy model can be obtained via
a data driven approach. Let ˆE be a differentiable function to
approximate E:

ˆE = arg min

Es[(f (s) − E(s))2],

(3)

f ∈F

where F is the space of all the potential energy mod-
:=
els, and Es is the expectation with respect
[s1, · · · , s|U |, s|U +1|].

to s

To ﬁnd a differentiable energy model ˆE, our intuition is
that the energy consumption of a DNN layer is affected by
the number of channels in its input and output feature maps,
which in turn are equivalent to the sparsity of the current and
the next layer, respectively. Therefore, the energy consump-
tion of layer j can be captured by a function that models
the interaction between sj and sj+1, where sj denotes the
(inverse) sparsity of layer j (j ∈ [1, |U|]). Based on this in-
tuition, we approximate the total network energy consump-
tion using the following bilinear model: F := {f (s) =
a0 + P|U |
a0, a1, ..., a|U | ∈ R+}, where
s|U |+1 is deﬁned as the network output dimensionality, e.g.,
the number of classes in a classiﬁcation task. Coefﬁcients
a0, a1, ...a|U | are the variables deﬁning this space.

j=1 ajsjsj+1 :

The rationale behind using the bilinear structure is that
the total number of arithmetic operations (multiplications
and additions) during a DNN inference with layers deﬁned
by s would roughly be in a bilinear form. Although other
more complex models are possible, the bilinear model is
simple and tight, which is easy to train and also avoids over-
ﬁtting effectively. We will show in Section 4 that our model
achieves high prediction accuracy.

Sharp readers might ask what if the energy function fun-
damentally can not be modeled in the bilinear form (e.g.,
for some particular DNN architectures). In such a case, one
could use a neural network to approximate the energy func-

tion since a three-layer neural network can theoretically ap-
proximate any function [13]. Note that our constrained op-
timization formulation requires only that the energy model
is differentiable and thus is still applicable.

To obtain ˆE, we sample s from a uniform distribution and
measure the real energy consumption on the target hardware
platform to get E(s). Please refer to Section 4.1 for a com-
plete experimental setup. We then use the stochastic gradi-
ent descent (SGD) method to solve Equation (3) and obtain
ˆE. Note that this process is performed once for a given net-
work and hardware combination. It is performed ofﬂine as
shown in Figure 1 and thus has no runtime overhead.

3.3. Optimization Algorithm

The optimization problem posed by Equation (2) is a
constrained optimization whereas conventional DNN train-
ing is unconstrained. Therefore, conventional gradient-
based algorithms such as SGD and existing deep learning
solvers do not directly apply here. Another idea is to extend
the gradient-based algorithm to the projected version [32]
– a (stochastic) gradient descent step followed by a pro-
jection step to the constraint [37]. When the projection is
tractable [40] or the constraint is simple (e.g., linear) [29],
Lagrangian methods can be used to solve constraints on
the parameters or the outputs of DNNs. However, due to
the complexity of our constraint, the projection step is ex-
tremely difﬁcult.

In this paper, we apply the framework of ADMM [3],
which is known to handle constrained optimizations ef-
fectively. ADMM is originally designed to solve opti-
mization problems with linear equality constraints and con-
vex objectives, both of which do not hold in our problem.
Therefore, we propose a hybrid solver that is based on the
ADMM framework while taking advantage of the recent ad-
vancements in gradient-based deep learning algorithms and
solvers [15].

Algorithm Overview We ﬁrst convert the original

problem (2) to an equivalent minimax problem [3]:

min
W,s

max

z≥0,y≥0

L(W, s, y, z)

(4)

where y is the dual variable introduced for the con-
straint (2b), and z is the dual variable for the con-
straint (2c). L is deﬁned as the augmented Lagrangian
:= ℓ(W) + L1(W, s, y) + L2(s, z),
L(W, s, y, z)
+ +
where L1(W, s, y)
Pu y(u)(φ(w(u))−s(u)), L2(s, z) := ρ2
++
z( ˆE(s) − Ebudget).
[·]+ denotes the nonnegative clamp
max(0, ·), and ρ1, ρ2 are two predeﬁned nonnegative hyper-
parameters. Note that the choices of ρ1 and ρ2 affect only
the efﬁciency of convergence, but not the convergent point
(for convex optimization).

2 Pu[φ(w(u)) − s(u)]2
2 [ ˆE(s)−Ebudget]2

:= ρ1

To compress a DNN under a given energy constraint, we
start with a dense model denoted by W dense (which could

11209

Algorithm 1: Energy-Constrained DNN Compression.

Input: Energy budget Ebudget, learning rates α, β,

penalty parameters ρ1, ρ2.

Result: DNN weights W ∗.
Initialize W = W dense, s = {φ(w(u))}u∈U , y = 0,
z = 0;
while ˆE(s) > Ebudget or ∃u, φ(w(u)) > s(u) do

Update W by proximal Adam update: W = (7);
Update s by gradient descent: (10);
Update y, z by projected gradient ascent: (11) and
(12);

end
W ∗ = W.

be obtained by optimizing the unconstrained objective), and
solve the problem in Equation (4) to obtain a compressed
network. Inspired by the basic framework of ADMM, we
solve Equation (4) iteratively, where each iteration updates
the primal variables W, s and dual variables y, z. Algo-
rithm 1 shows the pseudo-code of our algorithm.

Speciﬁcally, each iteration ﬁrst updates the DNN
weights W to minimize ℓ while preventing W to have layer-
wise (inverse) sparsities larger than s. s is then updated to
reduce the energy estimation ˆE(s). Dual variables y and z
can be seen as penalties that are dynamically changed based
on how much W and s violate the constraints (2b) and (2c).
We now elaborate on the three key updating steps.

3.3.1 Updating Primal Variable W

We ﬁrst ﬁx the sparsity bounds s and the two dual variables
y, z to update the primal variable weight tensor W by:

arg min

W

ℓ(W) + L1(W, s, y).

(5)

The challenge here is that updating W is time-
consuming, mainly due to the complexity of calculating
arg minW ℓ(W), where ℓ is the non-convex loss function
of the network. Stochastic ADMM [43] could simplify the
complexity of the primal update by using stochastic gradi-
ent, but they consider only the convex problems with shal-
low models. Instead, we propose to improve the primal up-
date’s efﬁciency using a proxy of the loss ℓ(W) at W t:

ℓ(W t) + h ˆ∇ℓ(W t), W − W ti +

1
2α

kW − W tk2
B,

(6)

where B is a positive diagonal matrix, which is usually
used in many adaptive optimizers such as ADADELTA [42]
kWkB is deﬁned by the norm
and Adam [15];
pvec(W)⊤Bvec(W) where vec(·) is the vectorization op-
eration. Without loss of generality, we use the diagonal ma-
trix B as in Adam. ˆ∇ℓ(W t) is the stochastic gradient of ℓ at
W t, and α is the learning rate for updating W. Therefore,
Equation (5) is simpliﬁed to a proximal [27] Adam update:

Algorithm 2: Proximal Operator proxαL1 (·).
Input: Input tensors ¯W = { ¯w(u)}u∈U .
Result: Proximal operation result W = {w(u)}u∈U .
Let a
Sort a(u) in descending order, let r(u) be the
corresponding ranks of elements in a(u);
foreach Layer u ∈ U do
for i ← 1 to c(u) do

B(u) , ∀u ∈ U ;

(u)
i = k ¯w

(u)
·,i,·,·k2

(u)
i >
(u)
i −s(u)]2

if a
ρ1α([r
then

+−[r

(u)
i −1−s(u)]2

+)+2αy(u)

w

(u)
·,i,·,· = ¯w

(u)
·,i,·,·;

else

w

(u)
·,i,·,· = 0;

end

end

end

arg min

W

1
2α

kW −(W t−αB−1 ˆ∇ℓ(W t))k2

B +L1(W, s, y).

(7)
If we deﬁne proxαL1 (·) as the proximal operator of func-

tion αL1(·, s, y):
proxαL1 ( ¯W) := arg min

W

1
2

kW − ¯Wk2

B + αL1(W, s, y),

(8)
the optimal solution of problem (7) admits a closed form:
proxαL1 (W t − αB−1 ˆ∇ℓ(W t)). This update essentially
performs pruning and ﬁne-tuning simultaneously. The de-
tailed algorithm for proximal operator proxαL1 (·) is shown
in Algorithm 2.

3.3.2 Updating Primal Variable s

In this step, we update the primal variable s by:
L1(W, s, y) + L2(s, z).

arg min

(9)

s

Similar as above, instead of searching for exactly solving

this subproblem, we only apply a gradient descent step:

st+1 = st − β(∇sL1(W, st, y) + ∇sL2(st, z)),

(10)

where β is the learning rate for updating s. To avoid remov-
ing a certain layer entirely, a lower bound is set for s(u). In
our method, it is set as 1 if not explicitly mentioned.

3.3.3 Updating Dual Variables

The dual updates simply ﬁx W, s and update y, z by pro-
jected gradient ascent with learning rates ρ1, ρ2:

y(u)t+1

= [y(u)t

+ ρ1(φ(w(u)) − s(u))]+,

zt+1 = [zt + ρ2( ˆE(s) − Ebudget)]+.

(11)

(12)

11210

To stabilize the training process, we perform some addi-
tional steps when updating the dual variables. The dual
variable y controls the sparsity of each DNN layer, and
larger y(u) prunes more channels in layer u. It is not neces-
sary to penalize φ(w(u)) when φ(w(u)) ≤ s(u), so y(u)
is trimmed to meet φ(w(u)) ≥ ⌊s(u)⌋. The dual vari-
able z is used to penalize the violation of energy cost ˆE(s),
and larger z makes s prone to decrease ˆE(s). In the train-
ing process, we want ˆE(s) to be monotonically decreased.
So we project the variable z to be large enough to meet
max(∇sL1(W, s, y) + ∇sL2(s, z)) ≥ ǫ, where ǫ is a small
positive quantity and we simply set 10−3. The gradient of s
is also clamped to be nonnegative.

4. Evaluation Results

We evaluate ECC on real vision tasks deployed on two
different hardware platforms. We ﬁrst introduce our exper-
imental setup (Section 4.1), followed up by the accuracy of
the energy prediction model (Section 4.2). Finally, we com-
pare ECC with state-of-the-art methods (Section 4.3).

4.1. Experimental Setup

Vision tasks & Datasets We evaluate ECC on two im-
portant vision tasks: image classiﬁcation and semantic seg-
mentation. For image classiﬁcation, we use the complete
ImageNet dataset [31]. For semantic segmentation, we use
the recently released large-scale segmentation benchmark
Cityscapes [6] which contains pixel-level high resolution
video sequences from 50 different cities.

DNN architectures For image classiﬁcation, we use
two representative DNN architectures AlexNet [17] and
MobileNet [14]. The dense versions of the two models are
from the ofﬁcial PyTorch model zoo and the ofﬁcial Tensor-
Flow repository [1], respectively. For semantic segmenta-
tion, we choose the recently proposed ERFNet [30], which
relies on residual connections and factorization structures,
and is shown to be efﬁcient on real-time segmentation. We
use the pre-trained ERFNet released by the authors. The
collection of the three networks allows us to evaluate ECC
against different DNN layer characteristics including fully
connected, convolutional, and transposed convolutional lay-
ers.

Hardware Platforms We experiment on two different
GPU platforms. The ﬁrst one is a GTX 1080 Ti GPU. We
use the nvidia-smi utility [26] to obtain real-hardware
energy measurements. The second one is the Nvidia Jet-
son TX2 embedded device, which is widely used in mobile
vision systems and contains a mobile Pascal GPU. We re-
trieve the TX2’s GPU power using the Texas Instruments
INA 3221 voltage monitor IC through the I2C interface.
The DNN architectures as well as our ECC framework are
implemented using PyTorch [28].

Baseline We compare ECC with two most recent (as
of submission) resource-constrained compression methods
NetAdapt [39] and AMC [11]. We faithfully implement
them according to what is disclosed in the papers. Ne-
tAdapt is originally designed to compress DNNs under la-
tency constraints and AMC is designed to compress DNNs
under constraint of model size, FLOPs or latency. We adapt
them to obtain the energy-constrained versions for compar-
ison. Both methods use channel pruning for compression,
same as ECC.

Earlier channel pruning methods such as Network-
Slimming [22], Bayesian Compression [23], and several
others [7, 12, 25] are agnostic to resource constraints (e.g.,
energy) because they focus on sparsity itself. They require
a sparsity bound (or regularization weight) for each layer
to be manually set before compression. The compressed
model is generated only based on the given sparsity bounds,
regardless of the energy budget. We thus do not compare
with them here.

Hyper-parameters & implementation details

The
batch size is set to 128 for AlexNet and MobileNet and to 4
for ERFNet based on the GPU memory capacity. We use the
Adam optimizer with its default Beta (0.9, 0.999); its learn-
ing rate is set to 10−5, and the weight decay is set as 10−4.3
All the compression methods are trained with the same
number of data batches / iterations, which are about 300,000
for ImageNet and 30,000 for Cityscapes. These iterations
correspond to the “short-term ﬁne-tuning” [39] in NetAdapt
and the “4-iteration pruning & ﬁne-tuning” [11] in AMC.
The reinforcement learning episodes in AMC is set to 400
as described in [11]. For the loss function ℓ, we add a
knowledge distillation (KD) term [2] combined with the
original loss (e.g., cross-entropy loss), since KD has been
shown as effective in DNN compression tasks [24, 35, 37].
In our method, the learning rate β for the sparsity bounds s
is set to reach the energy budgets with given iteration num-
ber, and the dual learning rates ρ1, ρ2 are set as 10 on Ima-
geNet and 1 on Cityscapes.

After getting the compressed models with given energy
budgets, we ﬁne-tune each model for 100,000 iterations
with aforementioned setup. For MobileNet, we addition-
ally perform 300,000 iterations of ﬁne-tuning with decayed
learning rate (cosine decay from 3 × 10−5) to minimize
the cross-entropy loss. The ﬁne-tuning procedures train the
DNN with ﬁxed non-zero positions in their weight tensors.

4.2. Energy Prediction Model

To train the energy prediction model, we obtain the real
energy measurements under different layer-wise sparsity
bounds s. We ﬁrst randomly sample s from the uniform
distribution: s(u) ∼ unif{1, c(u)}. For each sample, we

3They are chosen in favor of best pre-trained model accuracy rather

than biasing toward any compression methods.

11211

0.97

r
o
r
r

0.78

r
o
r
r

E

 
t
s
e
T
e
v
i
t

 

l

a
e
R

0.59

0.40

0.21

0.02

0

10

0.72

0.58

0.45

0.31

0.17

0.03

shown in Figure 3, where the original bilinear model has
zero hidden layer. We ﬁnd that adding an MLP does not no-
ticeable improve the prediction accuracy on GTX 1080 Ti,
but signiﬁcantly reduce the prediction accuracy on TX2. We
thus use the plain bilinear model for the rest of the evalua-
tion.

E

 
t
s
e
T
e
v
i
t

 

l

a
e
R

1

10

2

10

3

10

0

10

1

10

2

10

3

10

#Iterations

#Iterations

4.3. DNN Compression Results

(a) MobileNet on GTX 1080 Ti.

(b) MobileNet on TX2.

Figure 2: Relative test error of energy prediction using the
proposed bilinear model.

0.03

r
o
r
r

E

 
t
s
e
T
e
v
i
t

 

l

a
e
R

0.025

0.02

0.015

0.01

0

0.06

r
o
r
r

E

 
t
s
e
T
e
v
i
t

 

l

a
e
R

0.055

0.05

0.045

0.04

0.035

1
2
#Hidden Layer

3

0.03

0

1
2
#Hidden Layer

3

(a) MobileNet on GTX 1080 Ti.

(b) MobileNet on TX2.

Figure 3: Relative test error of energy prediction using an
MLP model with different hidden layers.

then construct a corresponding DNN, and measure its en-
ergy E(s). We measure the energy by taking the average of
multiple trials to minimize offset run-to-run variation. For
each DNN architecture, we collect 10,000 (s, E(s)) pairs to
train the energy model ˆE. We randomly choose 8,000 pairs
as the training data and leave the rest as test data. To opti-
mize problem (3), we use Adam optimizer with its default
hyper-parameters, and the weight decay is set as 1.0. We set
batch size as 8,000 (full training data) and train the energy
model with 10,000 iterations. It should be noted that the
bilinear energy model is linear in terms of the learnable pa-
rameters, which means a linear regression solver could be
used. In this section, we also compare the bilinear model
with a nonlinear model, for which Adam is more suitable.

In Figure 2, we show the relative test error deﬁned as
Es∼testset[| ˆE(s) − E(s)|/E(s)] at each training iteration for
MobileNet on both hardware platforms. We ﬁnd that the
relative test errors quickly converge to around 0.03. This
indicates that our energy model is not only accurate, but is
also efﬁcient to construct. The same conclusions hold for
other networks as well, but are omitted due to space limit.

To assess whether the bilinear model is sufﬁcient, we
also experiment with a more complex prediction model by
appending a multilayer perceptron (MLP) after the bilinear
model. The widths of all the MLP hidden layers are set to
128, and we use the SELU [16] activation. We vary the
number of hidden layers from 1 through 3. The prediction
errors of this augmented energy model on MobileNet are

We now show the evaluation results on two popular vi-
sion tasks: image classiﬁcation and semantic segmentation.

4.3.1

ImageNet Classiﬁcation

MobileNet
In Figure 4, we show the validation accuracies
of compressed MobileNet under different energy budgets,
and the energy cost is shown by joule (J). We set four dif-
ferent energy budgets in descending order. The dense Mo-
bileNet model has a top-1 accuracy of 0.709. The energy
cost of the dense model is 0.2877 J on GTX 1080 Ti and
0.0487 J on Jetson TX2.

0.709

0.657

0.606

0.554

0.503

y
c
a
r
u
c
c
A

0.711

0.681

0.651

0.621

0.590

y
c
a
r
u
c
c
A

ECC
NetAdapt
AMC
Dense

ECC
NetAdapt
AMC
Dense

0.451

0.197 0.215 0.233 0.252 0.270 0.288

0.560

0.020 0.025 0.031 0.037 0.043 0.049

Energy Cost (J)

(a) GTX 1080 Ti.

Energy Cost (J)

(b) Jetson TX2.

Figure 4: Top-1 accuracy of image classiﬁcation on Mo-
bileNet@ImageNet after ﬁne-tuning.

Figure 4 shows the top-1 accuracy v.s. energy compar-
isons (after ﬁne-tuning) across the three methods. The re-
sults before ﬁne-tuning is included in the supplementary
material. ECC achieves higher accuracy than NetAdapt and
AMC. For instance, on Jetson TX2 under the same 0.0247 J
energy budget, ECC achieves 2.6% higher accuracy com-
pared to NetAdapt. Compared to the dense model, ECC
achieves 37% energy savings with < 1% accuracy loss on
Jetson TX2. AMC has similar performance with NetAdapt
when the energy budget is not too small.

The accuracy improvements of ECC over NetAdapt and
AMC are more signiﬁcant under lower energy budgets. This
suggests that under tight energy budget, searching for the
optimal per-layer sparsity combinations becomes difﬁcult,
whereas ECC, via its optimization process, is able to iden-
tify better layer sparsities than search-based approaches.

AlexNet We obtain similar conclusions on AlexNet.
The dense model has a 0.566 top-1 accuracy. The energy
cost of the dense model is 0.2339 J on GTX 1080 Ti and
0.0498 J on Jetson TX2. Figure 5 compares the three meth-
ods (after ﬁne-tuning) on two platforms respectively. The

11212

Table 2: Segmentation accuracy (averaged IoU) comparison on the Cityscapes dataset.

Methods / Energy Budget

GTX 1080 Ti

Jetson TX2

1.3843 J 1.4451 J 1.5238 J 1.6519 J 1.9708 J 0.8542 J 0.9051 J 0.9756 J 1.0724 J 1.3213 J

Dense
ECC
NetAdapt [39]
AMC [11]

-

0.6713
0.6361
0.6340

-

0.6830
0.6523
0.6374

-

0.7007
0.6865
0.6749

-

0.722

-

0.7163
0.7114
0.6992

-
-
-

0.6733
0.6567
0.6344

-

0.6914
0.6708
0.6491

-

0.7017
0.6916
0.6685

-

0.722

0.7183
0.7084
0.6976

-
-
-

0.566

0.528

y
c
a
r
u
c
c
A

0.490

0.452

0.414

0.570

0.556

y
c
a
r
u
c
c
A

0.542

0.528

0.514

ECC
NetAdapt
AMC
Dense

ECC
NetAdapt
AMC
Dense

0.376

0.058 0.093 0.128 0.163 0.199 0.234

0.500

0.018 0.024 0.030 0.037 0.043 0.050

Energy Cost (J)

(a) GTX 1080 Ti.

Energy Cost (J)

(b) Jetson TX2.

Figure 5:
AlexNet@ImageNet after ﬁne-tuning.

Top-1 accuracy image classiﬁcation on

results before ﬁne-tuning are included in the supplemen-
tary material. Before ﬁne-tuning, ECC outperforms Ne-
tAdapt, which however achieves similar or slightly better
accuracy than ECC after ﬁne-tuning. ECC consistently out-
performs AMC before and after ﬁne-tuning. Compared to
dense models, ECC achieves 28% and 37% energy savings
with < 0.6% and < 1.7% accuracy loss on GTX 1080 Ti
and Jetson TX2, respectively.

Comparing the results on AlexNet (7 layers) and Mo-
bileNet (14 layers), we ﬁnd that the advantage of ECC
is more pronounced on deeper networks. This is because
as the network becomes deeper the layer sparsity search
space grows exponentially, which makes the search-based
approaches such as NetAdapt and AMC less effective.

Sparsity Analysis Figure 6a and Figure 6b show the
normalized (inverse) sparsity (i.e. #(nonzero channels)) of
each layer in MobileNet and AlexNet respectively. Differ-
ent colors represents different energy budgets used in Fig-
ure 4 and Figure 5. We ﬁnd that the 5th layer in AlexNet is
pruned heaviest. In AlexNet, that is the ﬁrst fully connected
layer which has the most number of weights; pruning it
saves lots of energy. We also observe many “spikes” in Mo-
bileNet. Our intuition is that every two consecutive layers
can be seen as a low rank factorization of a larger layer (ig-
noring the nonlinear activation between layers). The spikes
may suggest that low rank structure could be efﬁcient in
saving energy.

4.3.2 Cityscapes Segmentation

Now we apply ECC to ERFNet [30] for semantic segmen-
tation. We use the well-established averaged Intersection-
over-Union (IoU) metric, which is deﬁned as TP/(FP+TP+

y
t
i
s
r
a
p
S
d
e
z

 

i
l

a
m
r
o
N

1.00

0.84

0.68

0.52

0.36

0.20

0.0370 J
0.0308 J
0.0247 J
0.0196 J

1.00

0.80

0.60

0.40

y
t
i
s
r
a
p
S
 
d
e
z

i
l

a
m
r
o
N

0.0462 J
0.0313 J
0.0233 J
0.0176 J

2

4

8

6
Layer Index

10

12

14

0.20

1

2

4

3
5
Layer Index

6

7

(a) MobileNet (inverse) sparsity.

(b) AlexNet (inverse) sparsity.

Figure 6: Layer (inverse) sparsity after compressing on Jet-
son TX2.

FN) where TP, FP, and FN denote true positives, false pos-
itives, and false negatives, respectively. The training proto-
col is the same as the ImageNet experiments, except that the
number of training iterations is 30,000 and the results are
ﬁne-tuned with 10,000 extra iterations. The dense model
has an IoU of 0.722 and energy cost of 1.9708 J on GTX
1080 Ti and 1.3213 J on Jetson TX2.

Table 2 compares the IoUs of the three compression tech-
niques under different energy budgets. ECC consistently
achieves the highest accuracy under the same energy bud-
get. Similar to MobileNet, ERFNet is also a deep net-
work with 51 layers, which leads to large layer sparsity
search space that makes search-based approaches ineffec-
tive. Compared to the dense model, ECC reduces energy by
16% and 19% with < 0.6% IoU loss on GTX 1080 Ti and
TX2, respectively.

5. Conclusion

Future computer vision applications will be increasingly
operating on energy-constrained platforms such as mobile
robots, AR headsets, and ubiquitous sensor nodes. To accel-
erate the penetration of mobile computer vision, this paper
proposes ECC, a framework that compresses DNN to meet a
given energy budget while maximizing accuracy. We show
that DNN compression can be formulated as a constrained
optimization problem, which can be efﬁciently solved us-
ing gradient-based algorithms without many of the heuris-
tics used in conventional DNN compressions. Although
targeting energy as a case study in the paper, our frame-
work is generally applicable to other resource constraints
such as latency and model size. We hope that our work
is a ﬁrst step, not the ﬁnal word, toward heuristics-free,
optimization-based DNN improvements.

11213

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-
ﬂow: a system for large-scale machine learning. In OSDI,
volume 16, pages 265–283, 2016.

[2] J. Ba and R. Caruana. Do deep nets really need to be deep?
In Advances in neural information processing systems, pages
2654–2662, 2014.

[3] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, et al.
Distributed optimization and statistical learning via the al-
ternating direction method of multipliers. Foundations and
Trends R(cid:13) in Machine learning, 3(1):1–122, 2011.

[4] Y.-H. Chen, J. Emer, and V. Sze. Eyeriss: A Spatial Architec-
ture for Energy-efﬁcient Dataﬂow for Convolutional Neural
Networks. In Proc. of ISCA, 2016.

[5] T.-W. Chin, C. Zhang, and D. Marculescu.

Layer-
compensated pruning for resource-constrained convolutional
neural networks. arXiv preprint arXiv:1810.00518, 2018.

[6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016.

[7] B. Dai, C. Zhu, and D. Wipf. Compressing neural networks
using the variational information bottleneck. arXiv preprint
arXiv:1802.10399, 2018.

[8] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. Horowitz, and
W. Dally. EIE: Efﬁcient Inference Engine on Compressed
Deep Neural Network. In Proc. of ISCA, 2016.

[9] S. Han, H. Mao, and W. J. Dally.

Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015.

[10] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights
and connections for efﬁcient neural network.
In Advances
in neural information processing systems, pages 1135–1143,
2015.

[11] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. Amc:
Automl for model compression and acceleration on mobile
devices.
In Proceedings of the European Conference on
Computer Vision, pages 784–800, 2018.

[12] Y. He, X. Zhang, and J. Sun. Channel pruning for accel-
erating very deep neural networks.
In Proceedings of the
IEEE International Conference on Computer Vision, vol-
ume 2, 2017.

[13] K. Hornik. Approximation capabilities of multilayer feed-

forward networks. Neural networks, 4(2):251–257, 1991.

[14] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. arXiv preprint arXiv:1704.04861, 2017.

[15] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[16] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter.
In Advances in Neural

Self-normalizing neural networks.
Information Processing Systems, pages 971–980, 2017.

[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.

[18] Y. LeCun, J. S. Denker, and S. A. Solla. Optimal brain dam-
age. In Advances in neural information processing systems,
pages 598–605, 1990.

[19] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P.
Graf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint
arXiv:1608.08710, 2016.

[20] R. LiKamWa, Z. Wang, A. Carroll, F. X. Lin, and L. Zhong.
Draining our glass: An energy and heat characterization of
google glass. In Proceedings of 5th Asia-Paciﬁc Workshop
on Systems, page 10. ACM, 2014.

[21] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.
Sparse convolutional neural networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 806–814, 2015.

[22] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.
Learning efﬁcient convolutional networks through network
slimming. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 2755–2763. IEEE, 2017.

[23] C. Louizos, K. Ullrich, and M. Welling. Bayesian compres-
sion for deep learning. In Advances in Neural Information
Processing Systems, pages 3288–3298, 2017.

[24] A. Mishra and D. Marr. Apprentice: Using knowledge dis-
tillation techniques to improve low-precision network accu-
racy. arXiv preprint arXiv:1711.05852, 2017.

[25] K. Neklyudov, D. Molchanov, A. Ashukha, and D. P. Vetrov.
Structured bayesian pruning via log-normal multiplicative
noise. In Advances in Neural Information Processing Sys-
tems, pages 6775–6784, 2017.

[26] Nvidia.

nvidia-smi.

https://developer.

download.nvidia.com/compute/DCGM/docs/
nvidia-smi-367.38.pdf.

[27] N. Parikh, S. Boyd, et al. Proximal algorithms. Foundations

and Trends R(cid:13) in Optimization, 1(3):127–239, 2014.

[28] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. In NIPS-W, 2017.

[29] D. Pathak, P. Krahenbuhl, and T. Darrell. Constrained con-
volutional neural networks for weakly supervised segmenta-
tion. In Proceedings of the IEEE International Conference
on Computer Vision, pages 1796–1804, 2015.

[30] E. Romera, J. M. Alvarez, L. M. Bergasa, and R. Arroyo.
Erfnet: Efﬁcient residual factorized convnet for real-time
semantic segmentation.
IEEE Transactions on Intelligent
Transportation Systems, 19(1):263–272, 2018.

[31] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), 115(3):211–252, 2015.

[32] O. Shamir and T. Zhang. Stochastic gradient descent for non-
smooth optimization: Convergence results and optimal av-
eraging schemes. In International Conference on Machine
Learning, pages 71–79, 2013.

11214

[33] S. Srinivas and R. V. Babu. Data-free parameter pruning
for deep neural networks. arXiv preprint arXiv:1507.06149,
2015.

[34] A. Suleiman, Y.-H. Chen, J. Emer, and V. Sze. Towards Clos-
ing the Energy Gap Between HOG and CNN Features for
Embedded Vision. In Proc. of ISCAS, 2017.

[35] M. Tschannen, A. Khanna, and A. Anandkumar. Strassen-
nets: Deep learning with a multiplication budget. arXiv
preprint arXiv:1712.03942, 2017.

[36] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
structured sparsity in deep neural networks. In Advances in
Neural Information Processing Systems, pages 2074–2082,
2016.

[37] H. Yang, Y. Zhu, and J. Liu. Energy-constrained compres-
sion for deep neural networks via weighted sparse projec-
tion and layer input masking.
In International Conference
on Learning Representations, 2019.

[38] T.-J. Yang, Y.-H. Chen, and V. Sze. Designing energy-
efﬁcient convolutional neural networks using energy-aware
pruning. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 5687–5695,
2017.

[39] T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. San-
dler, V. Sze, and H. Adam. Netadapt: Platform-aware neural
network adaptation for mobile applications. In Proceedings
of the European Conference on Computer Vision, pages 285–
300, 2018.

[40] S. Ye, T. Zhang, K. Zhang, J. Li, J. Xie, Y. Liang, S. Liu,
X. Lin, and Y. Wang. A uniﬁed framework of dnn weight
pruning and weight clustering/quantization using admm.
arXiv preprint arXiv:1811.01907, 2018.

[41] J. Yu, A. Lukefahr, D. Palframan, G. Dasika, R. Das, and
S. Mahlke. Scalpel: Customizing dnn pruning to the under-
lying hardware parallelism.
In ACM SIGARCH Computer
Architecture News, volume 45, pages 548–560. ACM, 2017.
[42] M. D. Zeiler. Adadelta: an adaptive learning rate method.

arXiv preprint arXiv:1212.5701, 2012.

[43] S. Zheng and J. T. Kwok. Fast-and-light stochastic admm.

In IJCAI, pages 2407–2613, 2016.

[44] H. Zhou, J. M. Alvarez, and F. Porikli. Less is more: Towards
compact cnns. In European Conference on Computer Vision,
pages 662–677. Springer, 2016.

[45] Y. Zhu, A. Samajdar, M. Mattina, and P. Whatmough. Eu-
phrates: Algorithm-soc co-design for low-power mobile con-
tinuous vision. Proc. of ISCA, 2018.

11215

