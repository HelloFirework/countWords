Meta-Transfer Learning for Few-Shot Learning

Qianru Sun1

,

3 ∗ Yaoyao Liu2 ∗ Tat-Seng Chua1 Bernt Schiele3

1National University of Singapore

2Tianjin University†

3Max Planck Institute for Informatics, Saarland Informatics Campus

{qsun, schiele}@mpi-inf.mpg.de

liuyaoyao@tju.edu.cn

{dcssq, dcscts}@nus.edu.sg

Abstract

Meta-learning has been proposed as a framework to ad-
dress the challenging few-shot learning setting. The key
idea is to leverage a large number of similar few-shot tasks
in order to learn how to adapt a base-learner to a new task
for which only a few labeled samples are available. As deep
neural networks (DNNs) tend to overﬁt using a few samples
only, meta-learning typically uses shallow neural networks
(SNNs), thus limiting its effectiveness. In this paper we pro-
pose a novel few-shot learning method called meta-transfer
learning (MTL) which learns to adapt a deep NN for few
shot learning tasks. Speciﬁcally, meta refers to training
multiple tasks, and transfer is achieved by learning scal-
ing and shifting functions of DNN weights for each task.
In addition, we introduce the hard task (HT) meta-batch
scheme as an effective learning curriculum for MTL. We
conduct experiments using (5-class, 1-shot) and (5-class, 5-
shot) recognition tasks on two challenging few-shot learn-
ing benchmarks: miniImageNet and Fewshot-CIFAR100.
Extensive comparisons to related works validate that our
meta-transfer learning approach trained with the proposed
HT meta-batch scheme achieves top performance. An ab-
lation study also shows that both components contribute to
fast convergence and high accuracy1.

1. Introduction

While deep learning systems have achieved great perfor-
mance when sufﬁcient amounts of labeled data are avail-
able [57, 17, 45], there has been growing interest in reduc-
ing the required amount of data. Few-shot learning tasks
have been deﬁned for this purpose. The aim is to learn
new concepts from few labeled examples, e.g. 1-shot learn-

∗Equal contribution.
†Yaoyao Liu did this work during his internship at NUS.
1Code: https://github.com/y2l/meta-transfer-learning-tensorﬂow

Figure 1. Meta-transfer learning (MTL) is our meta-learning
paradigm and hard task (HT) meta-batch is our training strategy.
The upper three rows show the differences between MTL and re-
lated methods, transfer-learning [34] and meta-learning [9]. The
bottom rows compare HT meta-batch with the conventional meta-
batch [9]. FT stands for ﬁne-tuning a classiﬁer. SS represents the
Scaling and Shifting operations in our MTL method.

ing [24]. While humans tend to be highly effective in this
context, often grasping the essential connection between
new concepts and their own knowledge and experience, it
remains challenging for machine learning approaches. E.g.,
on the CIFAR-100 dataset, a state-of-the-art method [33]
achieves only 40.1% accuracy for 1-shot learning, com-
pared to 75.7% for the all-class fully supervised case [6].

Few-shot learning methods can be roughly categorized
into two classes: data augmentation and task-based meta-
learning. Data augmentation is a classic technique to in-
crease the amount of available data and thus also use-
ful for few-shot learning [21]. Several methods propose
to learn a data generator e.g.
conditioned on Gaussian
noise [28, 43, 53]. However, the generation models often

403

Feature extractor Θ(pre-trained & frozen)featureScaling & Shifting Param Φ(meta-learner)Classifierθsoftmax loss (epi-training)Meta transferring of neuron weightsepi-testepi-training element-wise product(neuron-level)softmax loss (epi-test)Training phase     Test phaseaccuracy(epi-test)meta gradient back-prop. (once)Difficulty predictorθ’L2 lossRegularization, useful???meta gradient back-prop. (once)(only in the last epi-training epoch)meta-trainingTransfer Learning [34]Meta-Learning [9]task2model1 + FT...task1model1taskNmodelNtaskN+1modelN+1Meta-Transfer Learning(ours)...taskmodeltask1model1k tasksMeta-Batch [9]Hard Task Meta-Batch(ours)...k tasks  +  k’ hard tasksbatch ik tasksbatch i+1batch ibatch i+1meta-test......k tasks  +  k’ hard tasksonline re-sampleonline re-sample...large-scale trainingtask1model + SS1 + FT1taskNmodel + SSN + FTNtaskN+1model + SSN + FTN+1underperform when trained on few-shot data [1]. An alter-
native is to merge data from multiple tasks which, however,
is not effective due to variances of the data across tasks [53].

In contrast to data-augmentation methods, meta-learning
is a task-level learning method [2, 32, 51]. Meta-learning
aims to accumulate experience from learning multiple tasks
[9, 38, 47, 30, 13], while base-learning focuses on model-
ing the data distribution of a single task. A state-of-the-
art representative of this, namely Model-Agnostic Meta-
Learning (MAML), learns to search for the optimal initial-
ization state to fast adapt a base-learner to a new task [9].
Its task-agnostic property makes it possible to generalize to
few-shot supervised learning as well as unsupervised rein-
forcement learning [13, 10]. However, in our view, there
are two main limitations of this type of approaches lim-
iting their effectiveness:
i) these methods usually require
a large number of similar tasks for meta-training which is
costly; and ii) each task is typically modeled by a low-
complexity base learner (such as a shallow neural network)
to avoid model overﬁtting, thus being unable to use deeper
and more powerful architectures. For example, for the mini-
ImageNet dataset [52], MAML uses a shallow CNN with
only 4 CONV layers and its optimal performance was ob-
tained learning on 240k tasks.

In this paper, we propose a novel meta-learning method
called meta-transfer learning (MTL) leveraging the ad-
vantages of both transfer and meta learning (see concep-
tual comparison of related methods in Figure 1). In a nut-
shell, MTL is a novel learning method that helps deep neu-
ral nets converge faster while reducing the probability to
overﬁt when using few labeled training data only. In partic-
ular, “transfer” means that DNN weights trained on large-
scale data can be used in other tasks by two light-weight
neuron operations: Scaling and Shifting (SS), i.e. αX + β.
“Meta” means that the parameters of these operations can
be viewed as hyper-parameters trained on few-shot learn-
ing tasks [30, 25]. Large-scale trained DNN weights of-
fer a good initialization, enabling fast convergence of meta-
transfer learning with fewer tasks, e.g. only 8k tasks for
miniImageNet [52], 30 times fewer than MAML [9]. Light-
weight operations on DNN neurons have less parameters to
learn, e.g. less than 2
49 if considering neurons of size 7 × 7
49 for α and < 1
( 1
49 for β), reducing the chance of overﬁt-
ting. In addition, these operations keep those trained DNN
weights unchanged, and thus avoid the problem of “catas-
trophic forgetting” which means forgetting general patterns
when adapting to a speciﬁc task [26, 27].

The second main contribution of this paper is an effec-
tive meta-training curriculum. Curriculum learning [3] and
hard negative mining [46] both suggest that faster conver-
gence and stronger performance can be achieved by a better
arrangement of training data. Inspired by these ideas, we
design our hard task (HT) meta-batch strategy to offer a

challenging but effective learning curriculum. As shown in
the bottom rows of Figure 1, a conventional meta-batch con-
tains a number of random tasks [9], but our HT meta-batch
online re-samples harder ones according to past failure tasks
with lowest validation accuracy.

Our overall contribution is thus three-fold:

i) we pro-
pose a novel MTL method that learns to transfer large-
scale pre-trained DNN weights for solving few-shot learn-
ing tasks; ii) we propose a novel HT meta-batch learn-
ing strategy that forces meta-transfer to “grow faster and
stronger through hardship”; and iii) we conduct extensive
experiments on two few-shot learning benchmarks, namely
miniImageNet [52] and Fewshot-CIFAR100 (FC100) [33],
and achieve the state-of-the-art performance.

2. Related work

Few-shot learning Research literature on few-shot learning
exhibits great diversity. In this section, we focus on meth-
ods using the supervised meta-learning paradigm [12, 51, 9]
most relevant to ours and compared to in the experiments.
We can divide these methods into three categories. 1) Met-
ric learning methods [52, 47, 50] learn a similarity space in
which learning is efﬁcient for few-shot examples. 2) Mem-
ory network methods [30, 41, 33, 29] learn to store “ex-
perience” when learning seen tasks and then generalize
that to unseen tasks. 3) Gradient descent based methods
[9, 38, 23, 13, 59] have a speciﬁc meta-learner that learns
to adapt a speciﬁc base-learner (to few-shot examples)
through different tasks. E.g. MAML [9] uses a meta-learner
that learns to effectively initialize a base-learner for a new
learning task. Meta-learner optimization is done by gra-
dient descent using the validation loss of the base-learner.
Our method is closely related. An important difference is
that our MTL approach leverages transfer learning and ben-
eﬁts from referencing neuron knowledge in pre-trained deep
nets. Although MAML can start from a pre-trained net-
work, its element-wise ﬁne-tuning makes it hard to learn
deep nets without overﬁtting (validated in our experiments).
Transfer learning What and how to transfer are key issues
to be addressed in transfer learning, as different methods are
applied to different source-target domains and bridge differ-
ent transfer knowledge [34, 56, 54, 58]. For deep models,
a powerful transfer method is adapting a pre-trained model
for a new task, often called ﬁne-tuning (FT). Models pre-
trained on large-scale datasets have proven to generalize
better than randomly initialized ones [8]. Another popu-
lar transfer method is taking pre-trained networks as back-
bone and adding high-level functions, e.g.
for object de-
tection and recognition [18, 49, 48] and image segmenta-
tion [16, 5]. Our meta-transfer learning leverages the idea
of transferring pre-trained weights and aims to meta-learn
how to effectively transfer. In this paper, large-scale trained
DNN weights are what to transfer, and the operations of

404

Figure 2. The pipeline of our proposed few-shot learning method, including three phases: (a) DNN training on large-scale data, i.e. using
all training datapoints (Section 4.1); (b) Meta-transfer learning (MTL) that learns the parameters of Scaling and Shifting (SS), based on the
pre-trained feature extractor (Section 4.2). Learning is scheduled by the proposed HT meta-batch (Section 4.3); and (c) meta-test is done
for an unseen task which consists of a base-learner (classiﬁer) Fine-Tuning (FT) stage and a ﬁnal evaluation stage, described in the last
paragraph in Section 3. Input data are along with arrows. Modules with names in bold get updated at corresponding phases. Speciﬁcally,
SS parameters are learned by meta-training but ﬁxed during meta-test. Base-learner parameters are optimized for every task.

Scaling and Shifting indicate how to transfer. Similar op-
erations have been used to modulating the per-feature-map
distribution of activations for visual reasoning [36].

Some few-shot learning methods have been proposed to
use pre-trained weights as initialization [20, 29, 37, 44, 40].
Typically, weights are ﬁne-tuned for each task, while we
learn a meta-transfer learner through all tasks, which is dif-
ferent in terms of the underlying learning paradigm.
Curriculum learning & Hard sample mining Curriculum
learning was proposed by Bengio et al. [3] and is popular
for multi-task learning [35, 42, 55, 14]. They showed that
instead of observing samples at random it is better to orga-
nize samples in a meaningful way so that fast convergence,
effective learning and better generalization can be achieved.
Pentina et al. [35] use adaptive SVM classiﬁers to evaluate
task difﬁculty for later organization. Differently, our MTL
method does task evaluation online at the phase of episode
test, without needing any auxiliary model.

Hard sample mining was proposed by Shrivastava et
al. [46] for object detection. It treats image proposals over-
lapped with ground truth as hard negative samples. Training
on more confusing data enables the model to achieve higher
robustness and better performance [4, 15, 7]. Inspired by
this, we sample harder tasks online and make our MTL
learner “grow faster and stronger through more hardness”.
In our experiments, we show that this can be generalized to
enhance other meta-learning methods, e.g. MAML [9].

3. Preliminary

We introduce the problem setup and notations of meta-

learning, following related work [52, 38, 9, 33].
Meta-learning consists of two phases: meta-train and
meta-test. A meta-training example is a classiﬁcation task
T sampled from a distribution p(T ). T is called episode,
including a training split T (tr) to optimize the base-learner,
and a test split T (te) to optimize the meta-learner. In partic-
ular, meta-training aims to learn from a number of episodes
{T } sampled from p(T ). An unseen task Tunseen in meta-
test will start from that experience of the meta-learner and

unseen.

adapt the base-learner. The ﬁnal evaluation is done by test-
ing a set of unseen datapoints T (te)
Meta-training phase. This phase aims to learn a meta-
learner from multiple episodes.
In each episode, meta-
training has a two-stage optimization. Stage-1 is called
base-learning, where the cross-entropy loss is used to opti-
mize the parameters of the base-learner. Stage-2 contains a
feed-forward test on episode test datapoints. The test loss is
used to optimize the parameters of the meta-learner. Specif-
ically, given an episode T ∈ p(T ), the base-learner θT is
learned from episode training data T (tr) and its correspond-
ing loss LT (θT , T (tr)). After optimizing this loss, the base-
learner has parameters ˜θT . Then, the meta-learner is up-
dated using test loss LT (˜θT , T (te)). After meta-training
on all episodes, the meta-learner is optimized by test losses
{LT (˜θT , T (te))}T ∈p(T ). Therefore, the number of meta-
learner updates equals to the number of episodes.

Meta-test phase. This phase aims to test the performance
of the trained meta-learner for fast adaptation to unseen
task. Given Tunseen, the meta-learner ˜θT teaches the base-
learner θTunseen to adapt to the objective of Tunseen by
some means, e.g. through initialization [9]. Then, the test
result on T (te)
unseen is used to evaluate the meta-learning ap-
proach. If there are multiple unseen tasks {Tunseen}, the
average result on {T (te)

unseen} will be the ﬁnal evaluation.

4. Methodology

As shown in Figure 2, our method consists of three
phases. First, we train a DNN on large-scale data, e.g. on
miniImageNet (64-class, 600-shot) [52], and then ﬁx the
low-level layers as Feature Extractor (Section 4.1). Second,
in the meta-transfer learning phase, MTL learns the Scal-
ing and Shifting (SS) parameters for the Feature Extractor
neurons, enabling fast adaptation to few-shot tasks (Sec-
tion 4.2). For improved overall learning, we use our HT
meta-batch strategy (Section 4.3). The training steps are
detailed in Algorithm 1 in Section 4.4. Finally, the typical
meta-test phase is performed, as introduced in Section 3.

405

Feature extractorΘ(pre-trained & frozen)featureScaling & Shifting Param Φ(meta-learner)Classifierθsoftmax loss (epi-training)Meta transferring of neuron weightsepi-testepi-training element-wise product(neuron-level)softmax loss (epi-test)Training phase     Test phaseaccuracy(epi-test)meta gradient back-prop. (once)Difficulty predictorθ’L2 lossRegularization, useful???meta gradient back-prop. (once)(only in the last epi-training epoch)Feature Extractor Base-learnerall-classtrain samplesN HTmeta-batchesFeature ExtractorMeta-learner SSN Base-learner FTNclassifier fine-tuningunseen task(train samples)unseen task(test samples)Feature ExtractorMeta-learner SSN Base-learner FTN+1final evaluationAcc.(b)meta-transfer learning(c)meta-test(a)large-scale DNN trainingwhole training phaseFeature ExtractorMeta-learner SSN Base-learner FTN+1D{T1∼k}1∼NT (trunseen)T(teunseen)4.1. DNN training on large scale data

This phase is similar to the classic pre-training stage as,
e.g., pre-training on Imagenet for object recognition [39].
Here, we do not consider data/domain adaptation from other
datasets, and pre-train on readily available data of few-shot
learning benchmarks, allowing for fair comparison with
other few-shot learning methods. Speciﬁcally, for a partic-
ular few-shot dataset, we merge all-class data D for pre-
training. For instance, for miniImageNet [52], there are
totally 64 classes in the training split of D and each class
contains 600 samples used to pre-train a 64-class classiﬁer.
We ﬁrst randomly initialize a feature extractor Θ (e.g.
CONV layers in ResNets [17]) and a classiﬁer θ (e.g. the
last FC layer in ResNets [17]), and then optimize them by
gradient descent as follows,

[Θ; θ] =: [Θ; θ] − α∇LD(cid:0)[Θ; θ](cid:1),
where L denotes the following empirical loss,

LD(cid:0)[Θ; θ](cid:1) =

1
|D| X

(x,y)∈D

l(cid:0)f[Θ;θ](x), y(cid:1),

(1)

(2)

e.g. cross-entropy loss, and α denotes the learning rate.
In this phase, the feature extractor Θ is learned. It will be
frozen in the following meta-training and meta-test phases,
as shown in Figure 2. The learned classiﬁer θ will be dis-
carded, because subsequent few-shot tasks contain differ-
ent classiﬁcation objectives, e.g. 5-class instead of 64-class
classiﬁcation for miniImageNet [52].

4.2. Meta transfer learning (MTL)

As shown in Figure 2(b), our proposed meta-transfer
learning (MTL) method optimizes the meta operations Scal-
ing and Shifting (SS) through HT meta-batch training (Sec-
tion 4.3). Figure 3 visualizes the difference of updating
through SS and FT. SS operations, denoted as ΦS1 and ΦS2 ,
do not change the frozen neuron weights of Θ during learn-
ing, while FT updates the complete Θ.

In the following, we detail the SS operations. Given a
task T , the loss of T (tr) is used to optimize the current
base-learner (classiﬁer) θ′ by gradient descent:

θ′ ← θ − β∇θLT (tr)(cid:0)[Θ; θ], ΦS{1,2}(cid:1),

(3)

which is different to Eq. 1, as we do not update Θ. Note
that here θ is different to the one from the previous phase,
the large-scale classiﬁer θ in Eq. 1. This θ concerns only
a few of classes, e.g. 5 classes, to classify each time in a
novel few-shot setting. θ′ corresponds to a temporal clas-
siﬁer only working in the current task, initialized by the θ
optimized for the previous task (see Eq. 5).

ΦS1 is initialized by ones and ΦS1 by zeros. Then, they

are optimized by the test loss of T (te) as follows,

ΦSi =: ΦSi − γ∇ΦSi

LT (te)(cid:0)[Θ; θ′], ΦS{1,2}(cid:1).

(4)

Figure 3. (a) Parameter-level Fine-Tuning (FT) is a conventional
meta-training operation, e.g. in MAML [9]. Its update works for
all neuron parameters, W and b. (b) Our neuron-level Scaling and
Shifting (SS) operations in MTL. They reduce the number of learn-
ing parameters and avoid overﬁtting problems. In addition, they
keep large-scale trained parameters (in yellow) frozen, preventing
“catastrophic fogetting” [26, 27].

In this step, θ is updated with the same learning rate γ as in
Eq. 4,

θ =: θ − γ∇θLT (te)(cid:0)[Θ; θ′], ΦS{1,2}(cid:1).

(5)

Re-linking to Eq. 3, we note that the above θ′ comes from
the last epoch of base-learning on T (tr).

Next, we describe how we apply ΦS{1,2} to the frozen
neurons as shown in Figure 3(b). Given the trained Θ,
for its l-th layer containing K neurons, we have K pairs
of parameters, respectively as weight and bias, denoted
as {(Wi,k, bi,k)}. Note that the neuron location l, k will
be omitted for readability. Based on MTL, we learn K
pairs of scalars {ΦS{1,2} }. Assuming X is input, we apply
{ΦS{1,2} } to (W, b) as

SS(X; W, b; ΦS{1,2} ) = (W ⊙ ΦS1 )X + (b + ΦS2 ), (6)

where ⊙ denotes the element-wise multiplication.

Taking Figure 3(b) as an example of a single 3 × 3 ﬁl-
ter, after SS operations, this ﬁlter is scaled by ΦS1 then the
feature maps after convolutions are shifted by ΦS2 in addi-
tion to the original bias b. Detailed steps of SS are given in
Algorithm 2 in Section 4.4.

Figure 3(a) shows a typical parameter-level Fine-Tuning
(FT) operation, which is in the meta optimization phase of
our related work MAML [9]. It is obvious that FT updates
the complete values of W and b, and has a large number of

406

: 1x4x1x1    : Cx4x1x1  : 1x4x1x1    : Cx4x3x3: 1x4x1x1    : Cx4x3x3  : 1x4x1x1    : Cx4x1x1: 1x4x1x1epi-testepi-training Feature extractorΘ(pre-trained & frozen)featureScaling & Shifting Param Φ(meta-learner)Classifierθsoftmax loss (epi-training)base gradient back-prop. (multiple epochs)Meta transferring of neuron weightsepi-testepi-training element-wise product(neuron-level)softmax loss (epi-test)Training phase     Test phaseaccuracy(epi-test)meta gradient back-prop. (once)Difficulty predictorθ’Predicted accuracyL2 lossmeta gradient back-prop. (once)Regularization, useful???meta gradient back-prop. (once)(only in the last epi-training epoch)(b)Our Scaling S1 and Shifting S2frozenlearnable..++(a)Parameter-level Fine-Tuning (FT)C xC xC xC x    : Cx4x3x3C x: 1x4x1x1C x: Cx4x3x3W b  W ′b′ΦS1 ΦS2 Φ′S1 Φ′S2 TW    bW    bupdateTupdateparameters, and our SS reduces this number to below 2
the example of the ﬁgure.

9 in

In summary, SS can beneﬁt MTL in three aspects. 1)
It starts from a strong initialization based on a large-scale
trained DNN, yielding fast convergence for MTL. 2) It does
not change DNN weights, thereby avoiding the problem
of “catastrophic forgetting” [26, 27] when learning speciﬁc
tasks in MTL. 3) It is light-weight, reducing the chance of
overﬁtting of MTL in few-shot scenarios.

4.3. Hard task (HT) meta batch

In this section, we introduce a method to schedule hard
tasks in meta-training batches. The conventional meta-
batch is composed of randomly sampled tasks, where the
randomness implies random difﬁculties [9]. In our meta-
training pipeline, we intentionally pick up failure cases in
each task and re-compose their data to be harder tasks for
adverse re-training. We aim to force our meta-learner to
“grow up through hardness”.
Pipeline. Each task T has two splits, T (tr) and T (te), for
base-learning and test, respectively. As shown in Algo-
rithm 2 line 2-5, base-learner is optimized by the loss of
T (tr) (in multiple epochs). SS parameters are then opti-
mized by the loss of T (te) once. We can also get the recog-
nition accuracy of T (te) for M classes. Then, we choose
the lowest accuracy Accm to determine the most difﬁcult
class-m (also called failure class) in the current task.

After obtaining all failure classes (indexed by {m}) from
k tasks in current meta-batch {T1∼k}, we re-sample tasks
from their data. Speciﬁcally, we assume p(T |{m}) is
the task distribution, we sample a “harder” task T hard ∈
p(T |{m}). Two important details are given below.
Choosing hard class-m. We choose the failure class-m
from each task by ranking the class-level accuracies instead
of ﬁxing a threshold. In a dynamic online setting as ours, it
is more sensible to choose the hardest cases based on rank-
ing rather than ﬁxing a threshold ahead of time.
Two methods of hard tasking using {m}. Chosen {m},
we can re-sample tasks T hard by (1) directly using the sam-
ples of class-m in the current task T , or (2) indirectly using
the label of class-m to sample new samples of that class. In
fact, setting (2) considers to include more data variance of
class-m and it works better than setting (1) in general.

4.4. Algorithm

Algorithm 1 summarizes the training process of two
main stages: large-scale DNN training (line 1-5) and meta-
transfer learning (line 6-22). HT meta-batch re-sampling
and continuous training phases are shown in lines 16-20,
for which the failure classes are returned by Algorithm 2,
see line 14. Algorithm 2 presents the learning process on
a single task that includes episode training (lines 2-5) and
episode test, i.e. meta-level update (lines 6). In lines 7-11,

the recognition rates of all test classes are computed and
returned to Algorithm 1 (line 14) for hard task sampling.

Algorithm 1: Meta-transfer learning (MTL)

Input: Task distribution p(T ) and corresponding

dataset D, learning rates α, β and γ

Output: Feature extractor Θ, base learner θ, SS

parameters ΦS{1,2}
1 Randomly initialize Θ and θ;
2 for samples in D do
3

Evaluate LD([Θ; θ]) by Eq. 2;
Optimize Θ and θ by Eq. 1;

4

5 end
6 Initialize ΦS1 by ones, initialize ΦS2 by zeros;
7 Reset and re-initialize θ for few-shot tasks;
8 for meta-batches do

9

10

11

12

13

14

15

16

17

18

19

20

Randomly sample tasks {T } from p(T );
while not done do

Sample task Ti ∈ {T };
Optimize ΦS{1,2} and θ with Ti by
Algorithm 2;
Get the returned class-m then add it to {m};

end
Sample hard tasks {T hard} from ⊆ p(T |{m});
while not done do

j

∈ {T hard} ;

Sample task T hard
Optimize ΦS{1,2} and θ with T hard
Algorithm 2 ;

j

by

end
Empty {m}.

21 end

Algorithm 2: Detail learning steps within a task T

Input: T , learning rates β and γ, feature extractor Θ,

base learner θ, SS parameters ΦS{1,2}

Output: Updated θ and ΦS{1,2} , the worst classiﬁed

class-m in T

1 Sample T (tr) and T (te) from T ;
2 for samples in T (tr) do
3

Evaluate LT (tr) ;
Optimize θ′ by Eq. 3;

4

5 end
6 Optimize ΦS{1,2} and θ by Eq. 4 and Eq. 5;
7 while not done do

8

9

Sample class-k in T (te);
Compute Acck for T (te);

10 end
11 Return class-m with the lowest accuracy Accm.

407

5. Experiments

We evaluate the proposed MTL and HT meta-batch in
terms of few-shot recognition accuracy and model conver-
gence speed. Below we describe the datasets and detailed
settings, followed by an ablation study and a comparison to
state-of-the-art methods.

5.1. Datasets and implementation details

We conduct few-shot

learning experiments on two
benchmarks, miniImageNet [52] and Fewshot-CIFAR100
(FC100) [33]. miniImageNet is widely used in related
works [9, 38, 13, 11, 31]. FC100 is newly proposed in [33]
and is more challenging in terms of lower image resolution
and stricter training-test splits than miniImageNet.

miniImageNet was proposed by Vinyals et al. [52] for few-
shot learning evaluation. Its complexity is high due to the
use of ImageNet images, but requires less resource and in-
frastructure than running on the full ImageNet dataset [39].
In total, there are 100 classes with 600 samples of 84 × 84
color images per class. These 100 classes are divided into
64, 16, and 20 classes respectively for sampling tasks for
meta-training, meta-validation and meta-test, following re-
lated works [9, 38, 13, 11, 31].

Fewshot-CIFAR100 (FC100) is based on the popular ob-
ject classiﬁcation dataset CIFAR100 [22]. The splits were
proposed by [33] (Please check details in the supplemen-
tary).
It offers a more challenging scenario with lower
image resolution and more challenging meta-training/test
splits that are separated according to object super-classes.
It contains 100 object classes and each class has 600 sam-
ples of 32 × 32 color images. The 100 classes belong to 20
super-classes. Meta-training data are from 60 classes be-
longing to 12 super-classes. Meta-validation and meta-test
sets contain 20 classes belonging to 4 super-classes, respec-
tively. These splits accord to super-classes, thus minimize
the information overlap between training and val/test tasks.
The following settings are used on both datasets. We
train a large-scale DNN with all training datapoints (Sec-
tion 4.1) and stop this training after 10k iterations. We use
the same task sampling method as related works [9, 38].
Speciﬁcally, 1) we consider the 5-class classiﬁcation and 2)
we sample 5-class, 1-shot (5-shot or 10-shot) episodes to
contain 1 (5 or 10) samples for train episode, and 15 (uni-
form) samples for episode test. Note that in the state-of-
the-art work [33], 32 and 64 samples are respectively used
in 5-shot and 10-shot settings for episode test. In total, we
sample 8k tasks for meta-training (same for w/ and w/o HT
meta-batch), and respectively sample 600 random tasks for
meta-validation and meta-test. Please check the supplemen-
tary document (or GitHub repository) for other implemen-
tation details, e.g. learning rate and dropout rate.

Network architecture. We present the details for the Fea-

ture Extractor Θ, MTL meta-learner with Scaling ΦS1 and
Shifting ΦS2 , and MTL base-learner (classiﬁer) θ.
The architecture of Θ have two options, ResNet-12 and
4CONV, commonly used in related works [9, 52, 38, 31, 29,
33]. 4CONV consists of 4 layers with 3 × 3 convolutions
and 32 ﬁlters, followed by batch normalization (BN) [19],
a ReLU nonlinearity, and 2 × 2 max-pooling. ResNet-12
is more popular in recent works [33, 29, 11, 31]. It con-
tains 4 residual blocks and each block has 3 CONV layers
with 3 × 3 kernels. At the end of each residual block, a
2 × 2 max-pooling layer is applied. The number of ﬁlters
starts from 64 and is doubled every next block. Following 4
blocks, there is a mean-pooling layer to compress the output
feature maps to a feature embedding. The difference be-
tween using 4CONV and using ResNet-12 in our methods is
that ResNet-12 MTL sees the large-scale data training, but
4CONV MTL is learned from scratch because of its poor
performance for large-scale data training (see results in the
supplementary). Therefore, we emphasize the experiments
of using ResNet-12 MTL for its superior performance. The
architectures of ΦS1 and ΦS2 are generated according to
the architecture of Θ, as introduced in Section 4.2. That is
when using ResNet-12 in MTL, ΦS1 and ΦS2 also have 12
layers, respectively. The architecture of θ is an FC layer.
We empirically ﬁnd that a single FC layer is faster to train
and more effective for classiﬁcation than multiple layers.
(see comparisons in the supplementary).

5.2. Ablation study setting

In order to show the effectiveness of our approach, we
design some ablative settings: two baselines without meta-
learning but more classic learning, three baselines of Fine-
Tuning (FT) on smaller number of parameters (Table 1), and
two MAML variants using our deeper pre-trained model
and HT meta-batch (Table 2 and Table 3). Note that the
alternative meta-learning operation to SS is the FT used in
MAML. Some bullet names are explained as follows.
update [Θ; θ] (or θ). There is no meta-training phase. Dur-
ing test phase, each task has its whole model [Θ; θ] (or the
classiﬁer θ) updated on T (tr), and then tested on T (te).
FT [Θ4; θ] (or θ). These are straight-forward ways to deﬁne
a smaller set of meta-learner parameters than MAML. We
can freeze low-level pre-trained layers and meta-learn the
classiﬁer layer θ with (or without) high-level CONV layer
Θ4 that is the 4th residual block of ResNet-12.

5.3. Results and analysis

Table 1, Table 2 and Table 3 present the overall results on
miniImageNet and FC100 datasets. Extensive comparisons
are done with ablative methods and the state-of-the-arts.
Note that tables present the highest accuracies for which
the iterations were chosen by validation. For the miniIma-

408

(b)

62
60
58
56
54

)

%

(
 

y
c
a
r
u
c
c
a

(a)

SS [Θ; θ], HT meta-batch 
SS [Θ; θ], meta-batch

76
75
74
73
72

45
44
43
42
41

(c)

58
57
56
55
54

(d)

(e)

63.5
63
62.5
62
61.5

 0.4k    1k     5k      10k      15k

iterations

 0.4k  1k     5k       10k      15k

iterations

 0.4k  1k     5k       10k      15k   

iterations

 0.4k  1k     5k       10k      15k

iterations

 0.4k  1k     5k       10k      15k

iterations

Figure 4. (a)(b) show the results of 1-shot and 5-shot on miniImageNet; (c)(d)(e) show the results of 1-shot, 5-shot and 10-shot on FC100.

geNet, iterations for 1-shot and 5-shot are at 17k and 14k,
respectively. For the FC100, iterations are all at 1k. Fig-
ure 4 shows the performance gap between with and without
HT meta-batch in terms of accuracy and converging speed.

Result overview on miniImageNet.
In Table 2, we can
see that the proposed MTL with SS [Θ; θ], HT meta-batch
and ResNet-12(pre) achieves the best few-shot classiﬁca-
tion performance with 61.2% for (5-class, 1-shot). Be-
sides, it tackles the (5-class, 5-shot) tasks with an accuracy
of 75.5% that is comparable to the state-of-the-art results,
i.e. 76.7%, reported by TADAM [33] whose model used
72 additional FC layers in the ResNet-12 arch. In terms of
the network arch, it is obvious that models using ResNet-
12 (pre) outperforms those using 4CONV by large mar-
gins, e.g. 4CONV models have the best 1-shot result with
50.44% [50] which is 10.8% lower than our best.
Result overview on FC100. In Table 3, we give the results
of TADAM using their reported numbers in the paper [33].
We used the public code of MAML [9] to get its results for
this new dataset. Comparing these methods, we can see that
MTL consistently outperforms MAML by large margins,
i.e. around 7% in all tasks; and surpasses TADAM by a
relatively larger number of 5% for 1-shot, and with 1.5%
and 1.8% respectively for 5-shot and 10-shot tasks.
MTL vs. No meta-learning. Table 1 shows the results
of No meta-learning on the top block. Compared to these,
our approach achieves signiﬁcantly better performance even
without HT meta-batch, e.g. the largest margins are 10.2%
for 1-shot and 8.6% for 5-shot on miniImageNet. This val-
idates the effectiveness of our meta-learning method for
tackling few-shot learning problems. Between two No

miniImageNet

FC100

1 (shot)

5

1

5

10

update [Θ; θ]
update θ

FT θ
FT [Θ4; θ]
FT [Θ; θ]

SS [Θ4; θ]
SS [Θ; θ](Ours)

45.3
50.0

55.9
57.2
58.3

59.2
60.2

64.6
66.7

71.4
71.6
71.6

73.1
74.3

38.4
39.3

41.6
40.9
41.6

42.4
43.6

52.6
51.8

54.9
54.3
54.4

55.1
55.4

58.6
61.0

61.1
61.3
61.2

61.6
62.4

Table 1. Classiﬁcation accuracy (%) using ablative models, on two
datasets. “meta-batch” and “ResNet-12(pre)” are used.

meta-learning methods, we can see that updating both fea-
ture extractor Θ and classiﬁer θ is inferior to updating θ
only, e.g. around 5% reduction on miniImageNet 1-shot.
One reason is that in few-shot settings, there are too many
parameters to optimize with little data. This supports our
motivation to learn only θ during base-learning.

Performance effects of MTL components. MTL with full
components, SS [Θ; θ], HT meta-batch and ResNet-12(pre),
achieves the best performances for all few-shot settings on
both datasets, see Table 2 and Table 3. We can conclude
that our large-scale network training on deep CNN signif-
icantly boost the few-shot learning performance. This is
an important gain brought by the transfer learning idea in
our MTL approach. It is interesting to note that this gain
on FC100 is not as large as for miniImageNet: only 1.7%,
1.0% and 4.0%. The possible reason is that FC100 tasks
for meta-train and meta-test are clearly split according to
super-classes. The data domain gap is larger than that for
miniImageNet, which makes transfer more difﬁcult.

HT meta-batch and ResNet-12(pre) in our approach can
be generalized to other meta-learning models. MAML
4CONV with HT meta-batch gains averagely 1% on two
datasets. When changing 4CONV by deep ResNet-12 (pre)
it achieves signiﬁcant improvements, e.g. 10% and 9% on
miniImageNet. Compared to MAML variants, our MTL re-
sults are consistently higher, e.g. 2.5% ∼ 3.3% on FC100.
People may argue that MAML ﬁne-tuning(FT) all network
parameters is likely to overﬁt to few-shot data. In the mid-
dle block of Table 1, we show the ablation study of freezing
low-level pre-trained layers and meta-learn only the high-
level layers (e.g. the 4-th residual block of ResNet-12) by
the FT operations of MAML. These all yield inferior per-
formances than using our SS. An additional observation is
that SS* performs consistently better than FT*.

Speed of convergence of MTL. MAML [9] used 240k
tasks to achieve the best performance on miniImageNet.
Impressively, our MTL methods used only 8k tasks, see
Figure 4(a)(b) (note that each iteration contains 2 tasks).
This advantage is more obvious for FC100 on which MTL
methods need at most 2k tasks, Figure 4(c)(d)(e). We attest
this to two reasons. First, MTL starts from the pre-trained
ResNet-12. And second, SS (in MTL) needs to learn only
< 2
9 parameters of the number of FT (in MAML) when
using ResNet-12.

409

Few-shot learning method

Feature extractor

1-shot

Data augmentation

Metric learning

Memory network

Adv. ResNet, [28]
Delta-encoder, [43]

Matching Nets, [52]
ProtoNets, [47]
CompareNets, [50]

Meta Networks, [30]
SNAIL, [29]
TADAM, [33]

WRN-40 (pre)
VGG-16 (pre)

55.2
58.7

4 CONV
4 CONV
4 CONV

5 CONV
ResNet-12 (pre)⋄
ResNet-12 (pre)†

Gradient descent

MAML, [9]
Meta-LSTM, [38]
Hierarchical Bayes, [13]
Bilevel Programming, [11] ResNet-12⋄
MetaGAN, [59]
ResNet-12
ResNet-12‡
adaResNet, [31]

4 CONV
4 CONV
4 CONV

MAML, HT
MAML deep, HT

FT [Θ; θ], HT meta-batch
FT [Θ; θ], HT meta-batch

4 CONV
ResNet-12 (pre)

MTL (Ours)

SS [Θ; θ], meta-batch
SS [Θ; θ], HT meta-batch

ResNet-12 (pre)
ResNet-12 (pre)

5-shot

69.6
73.6

55.31 ± 0.73
68.20 ± 0.66
65.32 ± 0.70

–

68.88 ± 0.92
76.7 ± 0.3

63.11 ± 0.92
60.60 ± 0.71

–

64.53 ± 0.68
68.63 ± 0.67
71.94 ± 0.57

64.1 ± 0.9
73.1 ± 0.9

74.3 ± 0.9
75.5 ± 0.8

43.44 ± 0.77
49.42 ± 0.78
50.44 ± 0.82

49.21 ± 0.96
55.71 ± 0.99

58.5 ± 0.3

48.70 ± 1.75
43.56 ± 0.84
49.40 ± 1.83
50.54 ± 0.85
52.71 ± 0.64
56.88 ± 0.62

49.1 ± 1.9
59.1 ± 1.9

60.2 ± 1.8
61.2 ± 1.8

⋄Additional 2 convolutional layers ‡Additional 1 convolutional layer †Additional 72 fully connected layers

Table 2. The 5-way, 1-shot and 5-shot classiﬁcation accuracy (%) on miniImageNet dataset. “pre” means pre-trained for a single classiﬁ-
cation task using all training datapoints.

Few-shot learning method

Feature extractor

1-shot

5-shot

10-shot

Gradient descent MAML, [9]‡

4 CONV

38.1 ± 1.7

50.4 ± 1.0

56.2 ± 0.8

Memory network

TADAM, [33]

ResNet-12 (pre)†

40.1 ± 0.4

56.1 ± 0.4

61.6 ± 0.5

MAML, HT
MAML deep, HT

FT [Θ; θ], HT meta-batch
FT [Θ; θ], HT meta-batch ResNet-12 (pre)

4 CONV

39.9 ± 1.8
41.8 ± 1.9

51.7 ± 0.9
55.1 ± 0.9

57.2 ± 0.8
61.9 ± 0.8

MTL (Ours)

SS [Θ; θ], meta-batch
SS [Θ; θ], HT meta-batch

ResNet-12 (pre)
ResNet-12 (pre)

43.6 ± 1.8
45.1 ± 1.8

55.4 ± 0.9
57.6 ± 0.9

62.4 ± 0.8
63.4 ± 0.8

†Additional 72 fully connected layers ‡Our implementation using the public code of MAML.

Table 3. The 5-way with 1-shot, 5-shot and 10-shot classiﬁcation accuracy (%) on Fewshot-CIFAR100 (FC100) dataset. “pre” means
pre-trained for a single classiﬁcation task using all training datapoints.

Speed of convergence of HT meta-batch.
Figure 4
shows 1) MTL with HT meta-batch consistently achieves
higher performances than MTL with the conventional meta-
batch [9], in terms of the recognition accuracy in all set-
tings; and 2) it is impressive that MTL with HT meta-batch
achieves top performances early, after e.g. about 2k itera-
tions for 1-shot, 1k for 5-shot and 1k for 10-shot, on the
more challenging dataset – FC100.

shot cases on two challenging benchmarks – miniImageNet
and FC100. In terms of learning scheme, HT meta-batch
showed consistently good performance for all baselines and
ablative models. On the more challenging FC100 bench-
mark, it showed to be particularly helpful for boosting con-
vergence speed. This design is independent from any spe-
ciﬁc model and could be generalized well whenever the
hardness of task is easy to evaluate in online iterations.

6. Conclusions

Acknowledgments

In this paper, we show that our novel MTL trained with
HT meta-batch learning curriculum achieves the top perfor-
mance for tackling few-shot learning problems. The key op-
erations of MTL on pre-trained DNN neurons proved highly
efﬁcient for adapting learning experience to the unseen task.
The superiority was particularly achieved in the extreme 1-

This research is part of NExT research which is
supported by the National Research Foundation, Prime
Minister’s Ofﬁce, Singapore under its IRC@SG Fund-
ing Initiative.
It is also partially supported by German
Research Foundation (DFG CRC 1223), and National
Natural Science Foundation of China
(61772359).

410

References

[1] S. Bartunov and D. P. Vetrov. Few-shot generative modelling

with generative matching networks. In AISTATS, 2018. 2

[2] S. Bengio, Y. Bengio, J. Cloutier, and J. Gecsei. On the op-
timization of a synaptic learning rule. In Optimality in Ar-
tiﬁcial and Biological Neural Networks, pages 6–8. Univ. of
Texas, 1992. 2

[3] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur-

riculum learning. In ICML, 2009. 2, 3

[4] O. Can´evet and F. Fleuret. Large scale hard sample mining

with monte carlo tree search. In CVPR, 2016. 3

[5] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Deeplab: Semantic image segmentation with deep
convolutional nets, atrous convolution, and fully connected
crfs.
IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834–
848, 2018. 2

[6] D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accu-
rate deep network learning by exponential linear units (elus).
In ICLR, 2016. 1

[7] N. Dalal and B. Triggs. Histograms of oriented gradients for

human detection. In CVPR, 2005. 3

[8] D. Erhan, Y. Bengio, A. C. Courville, P. Manzagol, P. Vin-
cent, and S. Bengio. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning Research,
11:625–660, 2010. 2

[9] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-
In ICML,

learning for fast adaptation of deep networks.
2017. 1, 2, 3, 4, 5, 6, 7, 8

[10] C. Finn, K. Xu, and S. Levine. Probabilistic model-agnostic

meta-learning. In NeurIPS, 2018. 2

[11] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil.
Bilevel programming for hyperparameter optimization and
meta-learning. In ICML, 2018. 6, 8

[12] H. E. Geoffrey and P. C. David. Using fast weights to deblur

old memories. In CogSci, 1987. 2

[13] E. Grant, C. Finn, S. Levine, T. Darrell, and T. L. Grif-
ﬁths. Recasting gradient-based meta-learning as hierarchical
bayes. In ICLR, 2018. 2, 6, 8

[14] A. Graves, M. G. Bellemare, J. Menick, R. Munos, and
K. Kavukcuoglu. Automated curriculum learning for neu-
ral networks. In ICML, 2017. 3

[15] B. Harwood, V. Kumar, G. Carneiro, I. Reid, and T. Drum-
In ICCV,

mond. Smart mining for deep metric learning.
2017. 3

[16] K. He, G. Gkioxari, P. Doll´ar, and R. B. Girshick. Mask

R-CNN. In ICCV, 2017. 2

[20] R. Keshari, M. Vatsa, R. Singh, and A. Noore. Learning
structure and strength of CNN ﬁlters for small sample size
training. In CVPR, 2018. 3

[21] A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele.
Lucid data dreaming for object tracking. arXiv, 1703.09554,
2017. 1

[22] A. Krizhevsky. Learning multiple layers of features from

tiny images. University of Toronto, 2009. 6

[23] Y. Lee and S. Choi. Gradient-based meta-learning with

learned layerwise metric and subspace. In ICML, 2018. 2

[24] F. Li, R. Fergus, and P. Perona. One-shot learning of ob-
IEEE Trans. Pattern Anal. Mach. Intell.,

ject categories.
28(4):594–611, 2006. 1

[25] Z. Li, F. Zhou, F. Chen, and H. Li. Meta-sgd: Learning to

learn quickly for few shot learning. In ICML, 2018. 2

[26] D. Lopez-Paz and M. Ranzato. Gradient episodic memory

for continual learning. In NIPS, 2017. 2, 4, 5

[27] M. McCloskey and N. J. Cohen. Catastrophic interference in
connectionist networks: The sequential learning problem. In
Psychology of learning and motivation, pages 3–17, 1989. 2,
4, 5

[28] A. Mehrotra and A. Dukkipati. Generative adversarial
residual pairwise networks for one shot learning. arXiv,
1703.08033, 2017. 1, 8

[29] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. Snail:
A simple neural attentive meta-learner. In ICLR, 2018. 2, 3,
6, 8

[30] T. Munkhdalai and H. Yu. Meta networks. In ICML, 2017.

2, 8

[31] T. Munkhdalai, X. Yuan, S. Mehri, and A. Trischler. Rapid
In ICML,

adaptation with conditionally shifted neurons.
2018. 6, 8

[32] D. K. Naik and R. Mammone. Meta-neural networks that

learn by learning. In IJCNN, 1992. 2

[33] B. N. Oreshkin, P. Rodr´ıguez, and A. Lacoste. TADAM: task
dependent adaptive metric for improved few-shot learning.
In NeurIPS, 2018. 1, 2, 3, 6, 7, 8

[34] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain
IEEE Trans.

adaptation via transfer component analysis.
Neural Networks, 22(2):199–210, 2011. 1, 2

[35] A. Pentina, V. Sharmanska, and C. H. Lampert. Curriculum

learning of multiple tasks. In CVPR, 2015. 3

[36] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. C.
Courville. Film: Visual reasoning with a general condition-
ing layer. In AAAI, 2018. 3

[37] S. Qiao, C. Liu, W. Shen, and A. L. Yuille. Few-shot image
In

recognition by predicting parameters from activations.
CVPR, 2018. 3

[38] S. Ravi and H. Larochelle. Optimization as a model for few-

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

shot learning. In ICLR, 2017. 2, 3, 6, 8

for image recognition. In CVPR, 2016. 1, 4

[18] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and
K. Murphy. Speed/accuracy trade-offs for modern convolu-
tional object detectors. In CVPR, 2017. 2

[39] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision, 115(3):211–252, 2015. 4, 6

[19] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 6

[40] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu,
S. Osindero, and R. Hadsell. Meta-learning with latent em-
bedding optimization. In ICLR, 2019. 3

411

[41] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and
T. P. Lillicrap. Meta-learning with memory-augmented neu-
ral networks. In ICML, 2016. 2

[42] N. Saraﬁanos, T. Giannakopoulos, C. Nikou, and I. A. Kaka-
diaris. Curriculum learning for multi-task classiﬁcation of
visual attributes. In ICCV Workshops, 2017. 3

[43] E. Schwartz, L. Karlinsky, J. Shtok, S. Harary, M. Marder,
R. S. Feris, A. Kumar, R. Giryes, and A. M. Bronstein. Delta-
encoder: an effective sample synthesis method for few-shot
object recognition. In NeurIPS, 2018. 1, 8

[44] T. R. Scott, K. Ridgeway, and M. C. Mozer. Adapted deep
embeddings: A synthesis of methods for k-shot inductive
transfer learning. In NeurIPS, 2018. 3

[45] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
IEEE Trans. Pattern

networks for semantic segmentation.
Anal. Mach. Intell., 39(4):640–651, 2017. 1

[46] A. Shrivastava, A. Gupta, and R. B. Girshick. Training
region-based object detectors with online hard example min-
ing. In CVPR, 2016. 2, 3

[47] J. Snell, K. Swersky, and R. S. Zemel. Prototypical networks

for few-shot learning. In NIPS, 2017. 2, 8

[48] Q. Sun, L. Ma, S. Joon Oh, L. Van Gool, B. Schiele, and
M. Fritz. Natural and effective obfuscation by head inpaint-
ing. In CVPR, 2018. 2

[49] Q. Sun, B. Schiele, and M. Fritz. A domain based approach

to social relation recognition. In CVPR, 2017. 2

[50] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, and
T. M. Hospedales. Learning to compare: Relation network
for few-shot learning. In CVPR, 2018. 2, 7, 8

[51] S. Thrun and L. Pratt. Learning to learn: Introduction and
overview. In Learning to learn, pages 3–17. Springer, 1998.
2

[52] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and
D. Wierstra. Matching networks for one shot learning. In
NIPS, 2016. 2, 3, 4, 6, 8

[53] Y. Wang, R. B. Girshick, M. Hebert, and B. Hariharan. Low-

shot learning from imaginary data. In CVPR, 2018. 1, 2

[54] Y. Wei, Y. Zhang, J. Huang, and Q. Yang. Transfer learning

via learning to transfer. In ICML, 2018. 2

[55] D. Weinshall, G. Cohen, and D. Amir. Curriculum learn-
ing by transfer learning: Theory and experiments with deep
networks. In ICML, 2018. 3

[56] J. Yang, R. Yan, and A. G. Hauptmann. Adapting SVM clas-
siﬁers to data with shifted distributions. In ICDM Workshops,
2007. 2

[57] L. Yann, B. Yoshua, and H. Geoffrey. Deep learning. Nature,

521(7553):436, 2015. 1

[58] A. R. Zamir, A. Sax, W. B. Shen, L. J. Guibas, J. Malik,
and S. Savarese. Taskonomy: Disentangling task transfer
learning. In CVPR, 2018. 2

[59] R. Zhang, T. Che, Z. Grahahramani, Y. Bengio, and Y. Song.
Metagan: An adversarial approach to few-shot learning. In
NeurIPS, 2018. 2, 8

412

