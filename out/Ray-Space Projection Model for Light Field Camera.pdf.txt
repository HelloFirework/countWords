Ray-Space Projection Model for Light Field Camera∗

Qi Zhang1, Jinbo Ling1, Qing Wang1, and Jingyi Yu2

1School of Computer Science, Northwestern Polytechnical University, Xi’an 710072, P.R. China,

2ShanghaiTech University, Shanghai 200031, P.R. China

qwang@nwpu.edu.cn

Abstract

Light ﬁeld essentially represents the collection of rays in
space. The rays captured by multiple light ﬁeld cameras
form subsets of full rays in 3D space and can be trans-
formed to each other. However, most previous approach-
es model the projection from an arbitrary point in 3D s-
pace to corresponding pixel on the sensor. There are few
models on describing the ray sampling and transformation
among multiple light ﬁeld cameras. In the paper, we pro-
pose a novel ray-space projection model to transform sets of
rays captured by multiple light ﬁeld cameras in term of the
Pl¨ucker coordinates. We ﬁrst derive a 6×6 ray-space intrin-
sic matrix based on multi-projection-center (MPC) model.
A homogeneous ray-space projection matrix and a funda-
mental matrix are then proposed to establish ray-ray corre-
spondences among multiple light ﬁelds. Finally, based on
the ray-space projection matrix, a novel camera calibration
method is proposed to verify the proposed model. A linear
constraint and a ray-ray cost function are established for
linear initial solution and non-linear optimization respec-
tively. Experimental results on both synthetic and real light
ﬁeld data have veriﬁed the effectiveness and robustness of
the proposed model.

1. Introduction

Light ﬁeld cameras [18, 23] can record spatial and an-
gular information of light rays in 3D space. Based on an-
gular sampling of light rays, sophisticated post-processing
techniques [20, 27, 13, 26, 34, 35, 33] ranging from digi-
tal refocusing to depth estimation have been introduced in
decades. However, the major disadvantages of such hand-
held systems are the spatio-angular trade-off and narrow
baseline. Applications of registration [15, 31, 32] and light
ﬁeld stitching [1, 8, 24, 7] are proposed to overcome these

∗The work was supported by NSFC under Grant 61531014.

limitations. In general, the performance of these applica-
tions can be enhanced by accurate geometric information
of multiple light ﬁeld cameras. But, there are fewer work-
s on establishing a generalized model for deﬁning the ray
sampling and transformation among multiple light ﬁelds.

Existing light ﬁeld camera models [3, 30] mostly deﬁne
the projection from an arbitrary point in 3D space (passing
through micro-lens) to corresponding pixel on the sensor.
Nevertheless, light ﬁeld essentially represents the collection
of rays in space. The Pl¨ucker line coordinates explicitly pro-
vide a homogeneous parameterization for rays to effectively
formulate ray-ray correspondence, which has already veri-
ﬁed in generalized epipolar geometry [22]. Dansereau et
al. [4] describe pixel-ray correspondences in 3D space and
present a 4D intrinsic matrix. However, they only focus on
the transformation of rays in monocular light ﬁeld camera.
Meanwhile, their model has redundancy and dependency,
which results in irregular ray sampling and makes param-
eterization impossible in term of the Pl¨ucker line coordi-
nates. In order to explore the ray sampling and transforma-
tion among multiple light ﬁeld cameras, multi-projection-
center model (MPC) [30] which provides independent and
effective intrinsic parameters for the Pl¨ucker representation
is used.

In the paper, we exploit the transformation of light ﬁeld-
s captured by multiple cameras and propose a novel ray-
space projection model in term of the Pl¨ucker coordinates.
We ﬁrst propose a so-called ray-space intrinsic matrix (R-
SIM) to relate the recorded indices to corresponding physi-
cal ray. We show that the RSIM is a 6 × 6 matrix consisted
of 6-parameter intrinsics, which is analogous to tradition-
al camera matrix. Furthermore, we derive a homogeneous
ray-space projection matrix and a fundamental matrix using
the RSIM and ray-space extrinsic matrix to describe ray-ray
correspondences. Secondly, we propose a novel light ﬁeld
camera calibration method to certify the proposed model.
According to the ray-sapce projection matrix, a linear con-
straint between 3D point and the rays captured by light ﬁeld

10121

cameras is established. Subsequently, an effective linear ini-
tial solution for both intrinsic and extrinsic parameters is
computed for optimization. We also deﬁne an effective ray-
ray cost function to minimize the distance among rays in
the Pl¨ucker coordinates. Finally, we exhibit empirical per-
formances in calibrating synthetic light ﬁeld cameras and
commercial light ﬁeld cameras (Lytro and Lytro Illum [18]).
Quantitative and qualitative comparisons demonstrate the
effectiveness and robustness of the proposed ray-space pro-
jection model.

Our main contributions are:
1) The ray-space projection model and fundamental ma-

trix among multiple light ﬁeld cameras are exploited.

2) A 6 × 6 RSIM is deduced to relate the recorded pixel

to physical ray in the Pl¨ucker coordinates.

3) An effective calibration algorithm is proposed to veri-
fy the proposed model, including a linear constraint for ini-
tial solution and a novel ray-ray cost function for optimiza-
tion.

2. Related Work

Light ﬁeld model. Since the hand-held light ﬁeld cam-
era is put forwarded by Ng [21], many research groups
[4, 14, 2, 9, 28, 3, 30, 29] have extensively explored vari-
ous projection models for light ﬁeld cameras. Johannsen et
al. [14] exhibit a model for reconstructing 3D points from
the parallax in adjacent micro-lens images. Heinze et al.
[9] present a similar model with [14] and deduce a linear
initialization for the focused light ﬁeld camera.

Dansereau et al. [4] propose a 12-free-parameter light
ﬁeld camera model, corresponding the recorded pixels to
the rays outside the camera. They derive a 4D decoding ma-
trix from a conventional pinhole lenslet and thin-lens mod-
el. Nevertheless, the calibration method is initialized by a
traditional camera calibration algorithm which is not effec-
tive for light ﬁeld camera. More importantly, the 4D intrin-
sic matrix has redundancy and dependency, which results in
irregular rays sampling during the calibration and rectiﬁca-
tion. Different from the calibration based on corner features
of sub-aperture images, Bok i.e. [2, 3] utilize line features
which are directly extracted from micro-lens images of raw
data to calibrate light ﬁeld camera. They formulate a 6-
parameter geometric projection model for light ﬁeld cam-
era to estimate initial solution of intrinsic parameters. But,
this method confronts a signiﬁcant challenge to obtain line
features accurately (in practice, the checkerboard should be
shot under an unfocused status in order to make the mea-
surements detectable).

More recently, Zhang et al. [28] exhibit a 4-parameter
projective distortion model to estimate the parameters using
a parallel bi-planar board. However, these parameters are
insufﬁcient to model the light ﬁeld camera geometry. Zhang
[30] propose a 6-parameter multi-projection-center
et al.

(MPC) model for light ﬁeld cameras, including traditional
and focused light ﬁeld cameras. A 3D projective transfor-
mation is deduced to describe the relationship between geo-
metric structure and light ﬁeld camera coordinates. Then, a
light ﬁeld camera calibration method is presented to verify
the effectiveness of MPC model.
Generalized epipolar geometry. The generalized epipo-
lar geometry is the intrinsic projective geometry in the ray-
space. It only depends on essential parameters of ray and
relative pose instead of scene structure. Techniques for es-
timating camera motion and structure from multiple images
have been improved and perfected over the decades [11].
Grossberg and Nayar [6] deﬁne the image pixel as the light
from a cone around a ray and propose a generalized camera
model for calibration. For the geometric analysis of mul-
tiple images, Pless [22] simpliﬁes this calibration so that it
only includes the deﬁnition of ray that the pixel samples.
They then propose a general linear framework to describe
any cameras as an unordered collection of rays which are
obtained by sensor elements. The correspondences between
rays need to be established with the assumption that the
rays intersect at a single scene point. Then, the generalized
epipolar constraint is proposed in the Pl¨ucker coordinates.
Sturm [25] introduces a hierarchy of general camera model.
In this framework, 17 corresponding rays are sufﬁcient to
solve linearly for pose estimation. Li et al. [17] carry out
a pose estimation based on the generalized epipolar con-
straint. This also can be applied to estimate the motion of
light ﬁeld camera. Guo et al. [8] propose a ray-space mo-
tion matrix to establish ray-ray correspondences for motion
estimation.

Moreover, all above mentioned methods ﬁrst reconstruct
the 3D points, and then formulate the Pl¨ucker coordinates
based on the 3D points. In other words, the performance
of these techniques are inﬂuenced by accurate reconstruc-
[15] directly generate the Pl¨ucker
tion. Johannsen et al.
coordinates based on light ﬁeld captured by camera. A lin-
ear mathematical framework is built from the relationships
between scene geometry and light rays for motion estima-
tion. However, little attention has been paid to model the
sampling and transformation of rays captured by multiple
light ﬁeld cameras in the Pl¨ucker coordinates. In the work,
we explore the relationship between a ray in 3D space and
the corresponding ray in light ﬁeld camera and propose a
ray-space projection model for light ﬁeld camera.

3. Ray-Space Projection Model

3.1. The MPC Model

Light ﬁeld cameras, especially micro-lens array assem-
bled inside, which are innovated from traditional 2D cam-
era, record 3D world in different but similar ways. With
the shifted views, light ﬁeld camera maps 3D space to many

10122

sub-aperture images, which produces 4D light ﬁeld. The ray
in 4D light ﬁeld is parameterized in a relative two-parallel-
plane coordinates [16], where Z = 0 denotes the view plane
and Z = f for the image plane. In this parametrization, the
decoded physical ray is described by r = (s, t, x, y) in term
of speciﬁc (e.g., meter) dimension. The ray r intersects with
the view plane at projection center (s, t, 0). The pair (x, y)
is the intersection of the ray r with the image plane, but
it is relative to (s, t, f ) which is the origin of image plane.
The (x, y, f ) describes the direction of ray. Then, accord-
ing to the MPC model [30], a 3D point X = (X, Y, Z)⊤ is
mapped to the pixel (x, y) in the image plane,

λ

x
y
1

 =

f
0
0

0
f
0

0 −f s
0 −f t
1

0



X
Y
Z
1



 .

(1)

This is analogous to classical projective camera model with
projection center at (s, t, 0) and principal axis parallelling
to the Z-axis.

The light ﬁeld L(i, j, u, v) captured by a light ﬁeld cam-
era is transformed into a normalized undistorted physical
light ﬁeld L(s, t, x, y) by a homogeneous decoding matrix
D ∈ R5×5 [30],

s
t
x
y
1





=



ki
0
0
0
0

0
kj
0
0
0

0
0
ku
0
0

0
0
0
kv
0

0
0
u0
v0
1





i
j
u
v
1



,

(2)

where (ki, kj, ku, kv, u0, v0) are intrinsic parameters of a
light ﬁeld camera. (ki, kj) are scale factors for s and t axes
in the view plane and (ku, kv) for x and y axes in the image
plane respectively. In addition, (−u0/ku, −v0/kv) repre-
sent the coordinates of principal point in the sub-aperture
image.

3.2. Ray Space Intrinsic Matrix

According to the MPC model, light ﬁeld camera is as-
sumed as a pinhole camera array. In this framework, a light
ﬁeld camera is described by the set of rays sampled by a
collection of perspective cameras. In order to simplify the
discussion of geometric analysis in multiple light ﬁelds, the
pixel captured by the camera is generalized and simpliﬁed
to a ray [22, 21]. The light ﬁeld essentially represents a
set of rays. Consequently, we need a new mechanism to
describe arbitrary rays in free space. The Pl¨ucker coor-
dinates provide convenience to mathematically formulate
concise and efﬁcient correspondence equations (e.g., rota-
tion and translation). In addition, the Pl¨ucker coordinates
representation is also a homogeneous parameterization to
unambiguously represent a ray in 3D projective geometry.
We will brieﬂy review the core theory leading the equations
for ray-space projection model.

In the Pl¨ucker coordinates, the ray is mathematically rep-
resented by a pair of vectors (m, q), named moment and
direction vector respectively. Moreover, the moment vector
m = X × q, for an arbitrary point X on the ray. Further,
the physical ray r = (s, t, x, y) in 3D space contains direc-
tional information (x, y) and positional information (s, t).
Therefore, the moment vector and direction vector of r are
deﬁned as,

( m = (s, t, 0)⊤ ×(x, y, 1)⊤ = (t, −s, sy − tx)⊤

q = (x, y, 1)⊤

,

(3)

where (m⊤, q⊤)⊤ are the Pl¨ucker line coordinates.

Substituting Eq. (2) into Eq. (3), there is a transformation
caused by the intrinsic parameters (ki, kj, ku, kv, u0, v0).
Then the RSIM K ∈ R6×6 is established to describe the
relationship between the ray L = (n⊤, p⊤)⊤ captured by
light ﬁeld camera and the normalized undistorted physical
ray Lc = (m⊤, q⊤)⊤ in the Pl¨ucker coordinates, i.e.,



q (cid:21)=
(cid:20) m
|

kj
0

0
ki

0
0

−kju0 −kiv0

kikv

0
0
0

0
0
0

0
0
0
ku
0
0

0
0
0
0
kv
0

0
0
0
u0
v0
1

0
0
0

=: K

{z



(cid:20) n
p(cid:21) , (4)
}

which needs to satisfy the condition ku/kv = ki/kj . (u, v)
are pixel coordinates extracted from sub-aperture image at
the view (i, j). Then, p = (u, v, 1)⊤ represents the di-
rection of ray in the sub-aperture image coordinates. n =
(i, j, 0)⊤ ×(u, v, 1)⊤ denotes the moment of ray.

3.3. Ray Space Projection Matrix

In general, considering Xw is a point in the world coor-
dinates, the transformation between the world and camera
coordinates is described by a rotation matrix R ∈ SO(3)
and a translation vector t = (tx, ty, tz)⊤ ∈ R3, formulated
as X = RXw+t. Consequently, the Pl¨ucker transformation
can be formulated according to generalized epipolar geom-
etry [22]

Lw =(cid:20) R⊤ E⊤

03×3 R⊤ (cid:21) Lc,

where E = [t]×R is the essential matrix and [·]× refers to
the vector cross product [11]. The rays Lc = (m⊤, q⊤)⊤
and Lw = (m⊤
w )⊤ are expressed by the camera coor-
dinates and world coordinates respectively. Subsequently,
according to Eqs. (3) and (4), the homogeneous ray-space
projection matrix P can be written as,

w, q⊤

(5)

(6)

Lw =(cid:20) R⊤ E⊤
{z

(cid:20) n
p (cid:21) ,
03×3 R⊤ (cid:21)K
}

|

=: P

which implies the relationship between L = (n⊤, p⊤)⊤ in
the camera coordinates and Lw in the world coordinates.

10123

j

i

o

v

u

Scene
y

x

Zc/Zw

{

}w}w

Yc/Yw

t
o

s

Xc/Xw

where M is the measurement matrix. The detailed deriva-
tion of Eq. (8) is presented in the supplemental material.

According to the ray-space projection model of Eq. (6),
the relationship among the RSIM K, extrinsic parameters
[R|t], and the points Xw in the world coordinates is extend-
ed by Eqs. (4), (5) and (8),

{ }cc

{ }c(cid:99)c(cid:99)

,R t

K

P

'P

K(cid:99)

{ }}}

F

{ }(cid:99)(cid:99)

M (Xw)(cid:20) R⊤ E⊤

03×3 R⊤ (cid:21) K(cid:20) n

p (cid:21) = 0.

(9)

In summary, the solution space of Eq. (9) is the ray set
intersecting at a point Xw. The set of ray sampled by the
light ﬁeld camera is a subspace of the whole-solution space
of Eq. (9).

Figure 1. Ray-space projection model and ray-ray transformation
among two light ﬁeld cameras.

4.2. Initialization

3.4. Fundamental Matrix

In order to establish the ray-ray transformation among t-
wo light ﬁeld cameras, the camera coordinates of the second
light ﬁeld camera are assumed as the world coordinates, as
shown in Fig. 1. Based on the ray-space projection model
Eq. (6), the fundamental matrix F between two light ﬁeld
cameras is then derived,

L′⊤ K ′⊤(cid:20) 03×3 R⊤
|

R⊤ E⊤ (cid:21)K
{z
}

=: F

L = 0,

(7)

which represents the ray-ray correspondence {L′} ↔ {L}.
R, t denote the rotation and translation between two light
ﬁeld cameras’ coordinates. For a valid correspondence, all
rays in both light ﬁelds must come from the same scene
point, as shown in Fig. 1. The detailed derivation of Eq. (7)
is presented in the supplemental material.

In order to verify the effectiveness of ray-space projec-
tion model, we propose a light ﬁeld camera calibration al-
gorithm.

4. Light Field Camera Calibration

4.1. Constraint of Ray Space Projection Matrix

w, q⊤

In 3D projective geometry, a point Xw in the world co-
ordinates can be described as the intersection of Lw =
(m⊤
w )⊤ with the plane Z = Zw. The plane is ex-
pressed by a homogeneous vector (π⊤, d)⊤, π ∈ R3,
d ∈ R. Therefore, according to the theorem X =
(π×m − d q) /π⊤q [12], we ﬁnd the constraint between
Xw and Lw in the Pl¨ucker coordinates,

Without a loss of generality, there is an assumption that
the checkerboard is on the plane Zw = 0 in the world coor-
dinates, which leads to a simpliﬁed form of Eq. (9),

0

(cid:20) 1
|

0 −Yw

1 Xw (cid:21)
}
{z

=: Ms

⊗(cid:2) n⊤ p⊤ (cid:3) ~Hs = 0,

(10)

where ~Hs is an 18×1 matrix stretched on row from the sim-
pliﬁed homogeneous ray-space projection matrix Hs. Ms
is the simpliﬁed measurement matrix of checkerboard cor-
ners Xw. ⊗ is a direct product operator. The RSIM is ab-
breviated to a lower triangle matrix Kij and the upper tri-
angle matrix Kuv. Subsequently, Hs denotes a 3×6 matrix
only using intrinsic and extrinsic parameters,

Hs =

r⊤
1
r⊤
2
01×3

−r⊤
1 [t]×
−r⊤
2 [t]×
r⊤
3

(cid:20) Kij
03×3 Kuv (cid:21) ,

03×3

(11)

where ri is the i-th column vector of rotation matrix R.

In order to derive intrinsic parameters, we abbreviate
with [h1, h2, 01×3]⊤ the ﬁrst three and with [h3, h4, h5]⊤
the second three columns of Hs respectively. hi denotes
the row vector (hi1, hi2, hi3). Utilizing the orthogonality of
r1 and r2, we have

h1K −1
h1K −1

ij K −⊤
ij K −⊤

ij h⊤
ij h⊤

2 = 0
1 = h2K −1

ij K −⊤

ij h⊤
2 ,

where K −1

ij =




1/kj

0

0

1/ki

u0/kjku

v0/kikv

Let a symmetric matrix B denote K −1

0
0


.
ij K −⊤

ij

,

1/kikv

(12)

 .

(13)

0
1

0

(cid:20) 1
|

0

0
0 −Zw

Zw −Yw
0

=: M (Xw )

{z

Xw (cid:21)
}

(cid:20) mw
qw (cid:21) = 0.

(8)

B =

1/k2
j

0

u0/kuk2

j

10124

0

1/k2
i

v0/kvk2

i

u0/kuk2
v0/kvk2

j

i

(u2

0 + v2

0 + 1)/k2

i k2

j

Note that B has only 5 distinct non-zero elements, ex-
pressed by b = (b11, b13, b22, b23, b33)⊤.
In order to get
the solution of B, Eq. (12) is rewritten as V b = 0, i.e.,

h11h21

h2
11 − h2
21

h11h23 +h13h21

2(h11h13 −h21h23)

h12h22

h2
12 − h2
22

h12h23 +h13h22

2(h12h13 −h22h23)

h13h23

h2
13 − h2
23



⊤


b11
b13
b22
b23
b33



= 0,

(14)

where V is a 2n ×5 matrix. What is more, a general non-
zero solution b is computed, only if there are at least two
such equations (from two positions) as Eq. (14). b is deter-
mined up to an unknown scale factor.

Once b is estimated, it is easy to solve K −1

ij by Cholesky
factorization [10]. The effect of the scale factor is elimi-
nated by calculating the ratio of elements. Therefore the
intrinsic parameters except (ki, kj ) are obtained,

ku = ˆk11/ˆk33,
u0 = ˆk31/ˆk33,

kv = ˆk22/ˆk33,
v0 = ˆk32/ˆk33,

(15)

where ˆkmn is the m-th row and n-th column of the esti-
mated intrinsic matrix ˆK −1
ij . Furthermore, the rest intrinsic
parameters and extrinsic parameters of different poses can
be obtained as follows,

ij h⊤

ij h⊤

1

λ =

2(cid:16)(cid:13)(cid:13)(cid:13) ˆK −⊤
τ = 1/(cid:13)(cid:13)(cid:13) ˆK −⊤

ˆK −⊤

r1 =

α
λ

1(cid:13)(cid:13)(cid:13) +(cid:13)(cid:13)(cid:13) ˆK −⊤
5(cid:13)(cid:13)(cid:13) ,

α
λ

1 , r2 =

ij h⊤

uv h⊤

2(cid:13)(cid:13)(cid:13)(cid:17) ,

ˆK −⊤

ij h⊤

2 , r3 = r1 × r2,

t = (G⊤G)−1(G⊤g),
G = (−[r1]×, −[r2]×)⊤, g = (τ ˆK −⊤

ij h⊤

3 , τ ˆK −⊤

ij h⊤

4 )⊤,

α =(cid:26) 1

−1

tz > 0
tz < 0

,

ki =

λτ
ˆk22

, kj =

λτ
ˆk11

,

(16)

where k·k denotes L2 norm, ˆKuv is formed by the intrinsic
parameters which are obtained from Eq. (15). α is deter-
mined by tz because it must be positive (i.e. the checker-
board is put in front of light ﬁeld camera). The details of
derivation are given in the supplemental material.

4.3. Distortion Model

Due to the special imaging design of two-parallel-plane
in light ﬁeld camera, there exists a radial distortion on the
image plane and a sampling distortion on the view plane si-
multaneously. With the assumption that the angular sam-
pling is ideal without distortion, only radial distortion is

considered in the paper. The undistorted coordinates (ex,ey)

is rectiﬁed by the distorted coordinates (x, y) under the

view (s, t),

(ex = x + (k1r2
ey = y + (k1r2

xy + k2r4
xy + k2r4

xy)(x − b1) + k3s

xy)(y − b2) + k4t

,

(17)

where r2
xy = (x − b1)2 + (y − b2)2 and r = (s, t, x, y)
is transformed from the measurement l = (i, j, u, v) by
the intrinsic parameters P according to Eq. (2). Com-
pared with existing radial distortion of light ﬁeld camer-
a, we add k3 and k4 to represent the distortion affected
by the shifted view. k1, k2, b1, b2 regulate conventional ra-
dial distortion on the image plane.
In summary, we use
kd = (k1, k2, k3, k4, b1, b2) to denote distortion vector.

4.4. Nonlinear Optimization

The initial solution computed by the linear method is re-
ﬁned via nonlinear optimization. Instead of minimizing the
distance between checkerboard corners and rays [4] and the
re-projection error in traditional multiview geometry [11],
we deﬁne a ray-ray cost function to acquire the nonlinear
solution. The ray-ray cost function is the minimization of
the distance between the rays and the line on the checker-
board,

dx =

|m⊤qx + q⊤mx|

kq×qxk

, dy =

|m⊤qy + q⊤my|

kq×qyk

,

(18)

x , q⊤

x )⊤ and Ly = (m⊤

where L = (m⊤, q⊤)⊤ is the ray in the Pl¨ucker coordi-
nates. Lx = (m⊤
y )⊤ are the
lines parallel to the Xw-axis and the Yw-axis respectively.
Both lines cross the checkerboard corners Xw. The deriva-
tion of the distance between two lines is given in the supple-
mental material. Further, we formulate the following cost
function according Eq. (18),

y , q⊤

#poseXp=1
where eL′

#pointXn=1

#viewXi=1

w(P, kd, Rp, tp), Lw(Xw))k,

kd(eL′

(19)

w is expressed as the Pl¨ucker coordinates after com-
puting by Eq. (2), followed by the distortion according to
Eq. (17). L′
w denotes the line in Eq. (18). P represents
intrinsic parameters, kd is distortion vector and Rp, tp are
extrinsic parameter at each position, 1 ≤ p ≤ P .

Eq. (19) are nonlinear objective functions which can be
solved using Levenberg-Marquardt algorithm based on the
trust region reﬂective method [19]. In addition, R is param-
eterized by Rodrigues formula [5]. MATLAB’s lsqnonlin
function is utilized to implement the optimization.

5. Experiments

In this section, we evaluate the performance of our al-
gorithm on calibrating synthetic light ﬁeld cameras as well
as commercial light ﬁeld cameras. We compare the pro-
posed method in ray re-projection error and re-projection

10125

Algorithm 1 Light Field Camera Calibration.
Input: Checkerboard corners Xw,

Corresponding rays l = (i, j, u, v).

Output: Intrinsic parameter P = (ki, kj, ku, kv, u0, v0),

Distortion vector kd = (k1, k2, k3, k4, b1, b2),
Extrinsic parameters Rp, tp, (1 6 p 6 P ).

1: for p = 1 to P do
2: Hs = EstimateProjectionMatrix(Xw, l)
3: end for
4: B = EstimateMatrix(Hs)
5: (ku, kv, u0, v0) = CalculateKuv(B)
6: for p = 1 to P do
7:
8: end for
9: (ki, kj) = CalculateKij( ˆKij)

10: Optimization(P, kd,SP

p=1(Rp, tp))

⊲ Eq. (10)

⊲ Eqs. (13), (14)
⊲ Eq. (15)

⊲ Eq. (16)
⊲ Eq. (19)

(Rp, tp) = CalculateRT (Hs, ku, kv, u0, v0) ⊲ Eq. (16)

error with state-of-the-arts, including DPW by Dansereau
et al. [4], BJW by Bok et al. [3] and MPC by Zhang et al.
[30].

5.1. Simulated Data

In order to evaluate the performance of our algorithm, we
simulate a light ﬁeld camera, whose intrinsic parameters are
listed in Tab. 1. These parameters are close to the setting of
a Lytro camera so that we obtain plausible input close to
real-world scenarios. The checkerboard is a pattern with a
12×12 grid of 3.51mm cells.

Table 1. Intrinsic parameters of the simulated light ﬁeld camera.

ki

kj

ku

kv

2.4e-4

2.5e-4

2.0e-3

1.9e-3

u0
-0.32

v0
-0.33

Performance w.r.t. the noise level. In this experiment,
we employ the measurements of 3 poses and 7 × 7 views
to verify the robustness of calibration algorithm. The rota-
tion angles of 3 poses are (6◦, 28◦, −8◦), (12◦, −10◦, 15◦)
and (−5◦, 5◦, −27◦) respectively. Gaussian noise with zero
mean and a standard deviation σ is added to the project-
ed image points. We vary σ from 0.1 to 1.5 pixels with a
0.1 pixel step. For each noise level, we perform 150 inde-
pendent trials. The estimated intrinsic parameters are eval-
uated by the average of relative errors with ground truth.
As shown in Fig. 2, the errors almost linearly increase with
noise level. For σ = 0.5 pixels which is larger than nor-
mal noise in practical calibration, the errors of (ki, kj) and
(ku, kv) are less than 0.14%. Although the relative error of
(u0, v0) is 0.25%, the absolute error of (−u0/ku, −v0/kv)
is less than 0.24 pixel (In Eq. (2), u = (x − u0)/ku and
v = (y − v0)/kv, where (−u0/ku, −v0/kv) is the princi-
pal point of a sub-aperture image), which demonstrates the
robustness of the proposed method to high noise level.

Performance w.r.t.

the number of poses and views.
This experiment investigates the performance with respect

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

%

/
r
o
r
r
e
 
e
v
i
t
a
l
e
r

0
0.1

0.3

0.7

0.6

0.5

0.4

0.3

0.2

0.1

%

/
r
o
r
r
e
 
e
v
i
t
a
l
e
r

0
0.1

0.3

0.7

1.1
0.5
noise level/pixel

0.9

(a) ki and kj

0.7

1.1
0.5
noise level/pixel

0.9

0.35

0.3

0.25

0.2

0.15

0.1

0.05

%

/
r
o
r
r
e
 
e
v
i
t
a
l
e
r

0
0.1

0.3

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l
e
x
i
p
/
r
o
r
r
e
 
e
t
u
l
o
s
b
a

0
0.1

0.3

k

k

i

j

1.3

1.5

u

0

v

0

1.3

1.5

k

k

u

v

1.3

1.5

0.7

1.1
0.5
noise level/pixel

0.9

(b) ku and kv

-u

0.7

1.1
0.5
noise level/pixel

0.9

-v

1.3

u

/k

0
/k

0

v

1.5

(c) u0 and v0

(d) −u0/ku and −v0/kv

Figure 2. Performance evaluation of intrinsic parameters on the
simulated data with different levels of noise σ.

to the number of poses and views. We vary the number of
poses from 3 to 10 and the number of views from 3× 3 to
7 × 7. For each combination of pose and view, by adding
a Gaussian noise with zero mean and a standard deviation
of 0.5 pixel, 200 trails with independent checkerboard pos-
es are conducted. The rotation angles are randomly gen-
erated from −30◦ to 30◦. The average relative errors of
calibration results with increasing measurements are shown
in Fig. 3. The relative errors decrease with the number of
poses. Meanwhile, when the number of poses is ﬁxed, the
errors reduce with the number of views. In particular, when
#pose≥ 4 and #view ≥ 4 × 4, all relative errors are less
than 0.5%, which further exhibits the effectiveness of the
proposed calibration method.

5.2. Real Data

We also perform experiments on real scene light ﬁelds,
including light ﬁeld datasets (Lytro) released by DPW and
light ﬁeld datasets1 (Lytro and Illum) released by MPC.

The sub-aperture images are easy to be decoded by raw
data. We improve the preprocessing of raw data described in
[4] to obtain sub-aperture images. First, normalized cross-
correlation (NCC) of the white images is used to locate
the centers of micro-lens images and estimate the average
size of micro-lens images. In addition, due to a slight ro-
tation between micro-lens array and the image sensor, a
line ﬁtting is utilized to estimate the rotation and reﬁne the
misalignment of micro-lens array. It is easy to extract 4D
light ﬁeld from 2D raw data once the centers and size of
micro-lens images are obtained. The preprocess of raw data
begins with demosaicing after alignment of micro-lens ar-
ray. Then, the vignetting raw data is reﬁne by divided by

1http://www.npu-cvpg.org/opensource

10126

3x3 Views
4x4 Views
5x5 Views
6x6 Views
7x7 Views

Table 3. Mean re-projection errors (unit: pixel). The errors of DP-
W are calculated by their least released code. The errors of BJW
are obtained from their extended work [3].

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

%

/
r
o
r
r
e
 
e
v
i
t
a
l
e
r

0

3

4

5

1

0.8

0.6

0.4

0.2

%

/
r
o
r
r
e
 
e
v
i
t
a
l
e
r

0

3

4

5

1.4

1.2

1

0.8

0.6

0.4

0.2

%

/
r
o
r
r
e
 
e
v
i
t
a
l
e
r

0

3

4

5

3x3 Views
4x4 Views
5x5 Views
6x6 Views
7x7 Views

0.7

0.6

0.5

0.4

0.3

0.2

0.1

%

/
r
o
r
r
e
 
e
v
i
t
a
l
e
r

8

9

10

0

3

4

5

6

7

Pose Number

6

7

Pose Number

(b) kj

(a) ki

3x3 Views
4x4 Views
5x5 Views
6x6 Views
7x7 Views

1

0.8

0.6

0.4

0.2

%

/
r
o
r
r
e
 
e
v
i
t
a
l
e
r

6

7

Pose Number

8

9

10

0

3

4

5

6

7

Pose Number

(d) kv

(c) ku

3x3 Views
4x4 Views
5x5 Views
6x6 Views
7x7 Views

1

0.8

0.6

0.4

0.2

%

/
r
o
r
r
e
 
e
v
i
t
a
l
e
r

6

7

Pose Number

8

9

10

0

3

4

5

6

7

Pose Number

8

9

10

3x3 Views
4x4 Views
5x5 Views
6x6 Views
7x7 Views

8

9

10

3x3 Views
4x4 Views
5x5 Views
6x6 Views
7x7 Views

8

9

10

(e) u0

(f) v0

Figure 3. Relative errors of intrinsic parameters on simulated data
with different numbers of poses and views.

Table 2. RMS ray re-projection errors (unit: mm).

A

B

C

D

E

DPW [4]
MPC [30]
Ours

0.0835
0.0810
0.0705

0.0628
0.0572
0.0438

0.1060
0.1123
0.1199

0.1050
0.1046
0.0740

0.3630
0.5390
0.2907

white image. Finally, a resampling is utilize to extract sub-
aperture images.

We ﬁrstly conduct calibration on the datasets collected
with DPW [4]. For a fair comparison, the middle 7 × 7
sub-apertures are utilized. Tab. 2 summarizes the root mean
square (RMS) ray re-projection error. Compared with D-
PW which employs 12 intrinsic parameters, the proposed
ray-space projection model only employs a half of param-
eters but achieves smaller ray re-projection error except on
dataset C. Considering the fact that the errors exhibited in
DPW are minimized in its own optimization (i.e., ray re-
projection error), we additionally evaluate the performance
in mean re-projection error with DPW and BJW. As exhib-
ited in Tab. 3, the errors of the proposed method are obvi-
ously smaller than those of DPW and BJW, which further
veriﬁes the effectiveness of nonlinear optimization (i.e. the
cost function in Eq. (19)).

A

B

C

D

E

DPW [4]
BJW [3]
MPC [30]
Ours

0.2284
0.3736
0.2200
0.1843

0.1582
0.2589
0.1568
0.1245

0.1948

0.1674

-

0.1752
0.1678

-

0.1475
0.1069

0.3360
0.2742
0.2731
0.1383

Table 4. RMS ray re-projection errors of optimizations without and
with distortion rectiﬁcation (unit: mm). The datasets are captured
with Lytro and Illum cameras.

Illum-1

Illum-2

Lytro-1

Lytro-2

Optimized
without
Rectiﬁcation

Optimized
with
Rectiﬁcation

DPW [4]
BJW [3]
MPC [30]
Ours

DPW [4]
BJW [3]
MPC [30]
Ours

0.5909

0.4866

0.1711

0.1287

-

0.5654
0.5641

0.2461
0.3966
0.1404
0.1294

-

0.4139
0.4132

0.2497
0.3199
0.0936
0.0837

-

0.1703
0.1572

0.1459
0.4411
0.1400
0.1142

-

0.1316
0.1237

0.1228
0.2673
0.1124
0.0980

It poses a signiﬁcant challenge to obtain line feature accu-
rately which is extracted from raw data to estimate an ini-
tial solution of intrinsic parameters. The light ﬁeld data for
calibration must be out of focus to make the measurement
detectable. Therefore, as shown in Tab. 3, several datasets,
i.e. C and D by [4], can not be estimated by BJW.

In order to comprehensively compare with DPW, BJW
and MPC, we also carry out calibration on the datasets cap-
tured by MPC [30]. Tab. 4 lists the RMS ray re-projection
errors compared with DPW, BJW and MPC at two calibra-
tion stages. As exhibited in Tab. 4, the proposed method
obtains smaller ray re-projection errors on the item of op-
timization without rectiﬁcation compared with DPW and
MPC. Furthermore, it is more important we achieve smal-
l errors once the distortion is introduced in the optimiza-
tion. According to the item of optimization with rectiﬁ-
cation, the proposed method outperforms DPW, BJW and
MPC. Consequently, such optimization results substantiate
that our 6-parameter ray-space projection model is effective
to describe light ﬁeld camera.

Fig. 4 demonstrates the results of pose estimation on
datasets of MPC. In addition, we carry out light ﬁeld stitch-
ing to examine the accuracy of intrinsic and extrinsic pa-
rameters estimation. 4-th and 7-th poses are center poses
of datasets Illum-1 and Illum-2 respectively (see Fig. 4(a-
b)). They are regarded as the reference views for stitch-
ing. Fig. 5 illustrates stitching results of light ﬁelds (actu-
ally these light ﬁelds are shot at a calibration board), from
which we can see all light ﬁelds are registered and stitched
very well.

Unlike the core idea of DPW, BJW conducts the calibra-
tion on the raw data directly instead of sub-aperture images.

It is vital to accurately calibrate light ﬁeld camera and
reconstruct 3D geometry. In order to verify the effective-

10127

Table 5. Quantitative comparison of different calibration methods (unit: mm). The relative error is indicated in parentheses.

Ruler

‘C’

128.0

‘V’

97.5

‘P’

99.0

‘R’

78.5

‘2’

147.5

‘0’

102.0

‘1’

84.5

‘9’

158.0

124.3 (2.89%)
DPW [4]
BJW [3]
120.6 (5.78%)
MPC [30] 127.5 (0.39%)
127.4 (0.47%)
Ours

100.2 (2.77%)
106.8 (9.54%)
97.0 (0.51%)
97.0 (0.51%)

97.6 (1.41%)
101.5 (2.53%)
98.3 (0.71%)
98.9 (0.10%)

80.2 (2.17%)
80.7 (2.80%)
77.4 (1.40%)
77.7 (1.02%)

145.7 (1.22%)
151.9 (2.98%)
144.9 (1.76%)
145.9 (1.08%)

106.6 (4.51%)
103.4 (1.37%)
103.6 (1.57%)
103.6 (1.57%)

86.1 (1.89%)
86.1 (1.89%)
85.7 (1.42%)
85.3 (0.95%)

157.4 (0.69%)
160.6 (1.20%)
159.3 (0.82%)
158.6 (0.38%)

93

2

5

4
10

6

78

1

m
Z

/

-0.16
-0.14
-0.12
-0.1
-0.08
-0.06
-0.04
-0.02
0
0.2

0.05

0

-0.05

0.1
X/m

0

0.02

0

-0.02

-0.1

0.1

0.08

0.06

0.04
Y/m

(b) Illum-2

(a) ‘C’: 128.0mm, ‘V’: 97.5mm (b) ‘P’: 99.0mm, ‘R’: 78.5mm

6

8

5

7

4

3

9

1

2

0.2
X/m

0

-0.2

0.25

0.2

0.15

0.1
Y/m

(a) Illum-1

4

1

3

5

8

2

7

6

m
Z

/

-0.4
-0.35
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
0
0.4

-0.5

-0.4

-0.3

m
Z

/

-0.2

-0.1

0
0.4

0.2
X/m

0

-0.2

0.4

0.3

0.2

0

0.1
Y/m

-0.1

-0.2

(c) Lytro-1

m
Z

/

-0.3

-0.25

-0.2

-0.15

-0.1

-0.05

0
0.3

0.2

0.1

X/m

1

2

10

5

6

9

8

4

7

3

0

-0.1

0

-0.1

0.3

0.2

0.1
Y/m

(d) Lytro-2

Figure 4. Pose estimation results of our datasets.

(c) ‘2’: 147.5mm, ‘0’: 102.0mm (d) ‘1’: 84.5mm, ‘9’: 158.0mm

Figure 6. Measurements between speciﬁc points by rulers.

(a) Illum-1

(b) Illum-2

Figure 5. The stitching results of Illum-1 and Illum-2 datasets.

ness of geometric reconstruction of the proposed method
compared with state-of-the-art methods, we capture four re-
al scene light ﬁelds and reconstruct several speciﬁc corner
points and estimate the distances between them. As shown
in Tab. 5, the estimated distances between the reconstruct-
ed points are nearly equal to those measured lengths from
real objects by rulers (see Fig. 6). For these four measure-
ment examples, the relative errors of distance between re-
constructed points demonstrate the performance of the pro-
posed model compared with state-of-the-art methods.

6. Conclusion

In the paper, we explore the linear projection relationship
between the rays in 3D space and the corresponding rays
captured by multiple light ﬁeld cameras. We ﬁrst deduce a
6 × 6 RSIM composed of 6-parameter under the Pl¨ucker co-

ordinates. A linear ray-space projection matrix and a funda-
mental matrix are then formulated to describe the ray sam-
pling and transformation among multiple view light ﬁeld-
s. Consequently, a linear constraint between 3D point and
rays captured by light ﬁeld camera is established to actuate
an effective initial solution for both intrinsic and extrinsic
parameters. Finally, a novel ray-ray cost function is estab-
lished to nonlinearly optimize the 12-parameter model (6
for intrinsics and 6 for distortion). Extensive experiments
are conducted on synthetic and real light ﬁeld data. Quali-
tative and quantitative comparisons verify the effectiveness
and robustness of the proposed ray-space projection model.

In the future, we intend to focus on exploring the invari-
ance for multiple light ﬁeld cameras based on the ray-space
projection model. The future work also includes improving
the effectiveness of initial solution and reconstructing large-
scale complex scenes using multiple light ﬁeld cameras.

Acknowledgement

Qi Zhang is also sponsored by Innovation Foundation for
Doctor Dissertation of Northwestern Polytechnical Univer-
sity under CX201919. The authors thank Dr. Xue Wang for
helpful discussions and Dahe Ma for the decoding program
of light ﬁeld.

10128

[19] Kaj Madsen, Hans Bruun Nielsen, and Ole Tingleff. Method-
s for non-linear least squares problems, 2nd Edition. Infor-
matics and Mathematical Modelling, Technical University of
Denmark, 2004.

[20] Ren Ng. Fourier slice photography. ACM TOG, 24(3):735–

744, 2005.

[21] Ren Ng. Digital light ﬁeld photography. PhD thesis, Stan-

ford University, 2006.

[22] Robert Pless. Using many cameras as one. In IEEE CVPR,

volume 2, pages 587–593, 2003.

[23] Raytrix. 3d light ﬁeld camera technology. http://www.

raytrix.de, 2013.

[24] Zhao Ren, Qi Zhang, Hao Zhu, and Qing Wang. Extending
the FOV from disparity and color consistencies in multiview
light ﬁelds. In Proc. ICIP, pages 1157–1161, 2017.

[25] Peter Sturm. Multi-view geometry for general camera mod-

els. In IEEE CVPR, volume 1, pages 206–212, 2005.

[26] Ting-Chun Wang, Alexei A Efros, and Ravi Ramamoorthi.
Depth estimation with occlusion modeling using light-ﬁeld
cameras. IEEE T-PAMI, 38(11):2170–2181, 2016.

[27] Sven Wanner and Bastian Goldluecke. Globally consistent
depth labeling of 4d light ﬁelds. In IEEE CVPR, pages 41–
48, 2012.

[28] Chunping Zhang, Zhe Ji, and Qing Wang. Rectifying projec-

tive distortion in 4d light ﬁeld. In IEEE ICIP, 2016.

[29] Qi Zhang and Qing Wang. Common self-polar triangle of
concentric conics for light ﬁeld camera calibration. In ACCV,
2018.

[30] Qi Zhang, Chunping Zhang, Jinbo Ling, Qing Wang, and
Jingyi Yu. A generic multi-projection-center model and cal-
ibration method for light ﬁeld cameras. IEEE T-PAMI, 2018.
[31] Yingliang Zhang, Zhong Li, Wei Yang, Peihong Yu, Halting
Lin, and Jingyi Yu. The light ﬁeld 3d scanner. In IEEE ICCP,
pages 1–9, 2017.

[32] Yingliang Zhang, Peihong Yu, Wei Yang, Yuanxi Ma, and
Jingyi Yu. Ray space features for plenoptic structure-from-
motion. In IEEE ICCV, pages 4631–4639, 2017.

[33] Hao Zhu, Xiaoming Sun, Qi Zhang, Qing Wang, Antonio
Robles-Kelly, Hongdong Li, and Shaodi You. Full view op-
tical ﬂow estimation leveraged from light ﬁeld superpixel.
IEEE T-CI, 2019.

[34] Hao Zhu, Qing Wang, and Jingyi Yu. Occlusion-model guid-
IEEE J-

ed anti-occlusion depth estimation in light ﬁeld.
STSP, 11(7):965–978, 2017.

[35] Hao Zhu, Qi Zhang, and Qing Wang. 4D light ﬁeld super-
pixel and segmentation. In IEEE CVPR, pages 6709–6717,
2017.

References

[1] Clemens Birklbauer and Oliver Bimber. Panorama light-ﬁeld
imaging. In Computer Graphics Forum, volume 33, pages
43–52. Wiley Online Library, 2014.

[2] Yunsu Bok, Hae-Gon Jeon, and In So Kweon. Geometric
calibration of micro-lens-based light-ﬁeld cameras using line
features. In ECCV, pages 47–61, 2014.

[3] Yunsu Bok, Hae-Gon Jeon, and In So Kweon. Geometric
calibration of micro-lens-based light ﬁeld cameras using line
features. IEEE T-PAMI, 39(2):287–300, 2017.

[4] Donald G Dansereau, Oscar Pizarro, and Stefan B Williams.
Decoding, calibration and rectiﬁcation for lenselet-based
plenoptic cameras. In IEEE CVPR, pages 1027–1034, 2013.
[5] Olivier Faugeras. Three-dimensional computer vision: a ge-

ometric viewpoint. MIT Press, 1993.

[6] Michael D Grossberg and Shree K Nayar. A general imaging
model and a method for ﬁnding its parameters. In Computer
Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE Inter-
national Conference on, volume 2, pages 108–115. IEEE,
2001.

[7] Mangtang Guo, Hao Zhu, Guoqing Zhou, and Qing Wang.
Dense light ﬁeld reconstruction from sparse sampling using
residual network. In ACCV, 2018.

[8] Xinqing Guo, Zhan Yu, Sing Bing Kang, Haiting Lin, and
Jingyi Yu. Enhancing light ﬁelds through ray-space stitching.
IEEE T-VCG, 22(7):1852–1861, 2016.

[9] Christopher Hahne, Amar Aggoun, Shyqyri Haxha, Vladan
Velisavljevic, and Juan Carlos J´acome Fern´andez. Light ﬁeld
geometry of a standard plenoptic camera. Optics Express,
22(22):26659–26673, 2014.

[10] Richard Hartley. Self-calibration of stationary cameras. I-

JCV, 22(1):5–23, 1997.

[11] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision. Cambridge University Press,
2003.

[12] William Vallance Douglas Hodge, WVD Hodge, and Daniel
Pedoe. Methods of algebraic geometry, volume 2. Cam-
bridge University Press, 1994.

[13] Hae-Gon Jeon, Jaesik Park, Gyeongmin Choe, Jinsun Park,
Yunsu Bok, Yu-Wing Tai, and In So Kweon. Accurate depth
map estimation from a lenslet light ﬁeld camera.
In IEEE
CVPR, pages 1547–1555, 2015.

[14] Ole Johannsen, Christian Heinze, Bastian Goldluecke, and
Christian Perwaß. On the calibration of focused plenoptic
cameras. In Time-of-Flight and Depth Imaging, pages 302–
317. 2013.

[15] Ole Johannsen, Antonin Sulc, and Bastian Goldluecke. On
linear structure from motion for light ﬁeld cameras. In IEEE
ICCV, pages 720–728, 2015.

[16] Marc Levoy and Pat Hanrahan. Light ﬁeld rendering.

In

ACM SIGGRAPH, pages 31–42, 1996.

[17] Hongdong Li, Richard Hartley, and Jae-hak Kim. A lin-
ear approach to motion estimation using generalized camera
models. In IEEE CVPR, pages 1–8, 2008.

[18] Lytro. Lytro redeﬁnes photography with light ﬁeld cameras.

http://www.lytro.com, 2011.

10129

