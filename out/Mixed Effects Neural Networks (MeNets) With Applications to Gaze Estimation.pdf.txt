Mixed Effects Neural Networks (MeNets) with Applications to Gaze Estimation∗

Yunyang Xiong∗

University of Wisconsin-Madison

Hyunwoo J. Kim∗
Korea University

yxiong43@wisc.edu

hyunwoojkim@korea.ac.kr

Vikas Singh

University of Wisconsin-Madison

vsingh@biostat.wisc.edu

Abstract

There is much interest in computer vision to utilize com-
modity hardware for gaze estimation. A number of papers
have shown that algorithms based on deep convolutional
architectures are approaching accuracies where streaming
data from mass-market devices can offer good gaze track-
ing performance, although a gap still remains between what
is possible and the performance users will expect in real
deployments. We observe that one obvious avenue for im-
provement relates to a gap between some basic technical
assumptions behind most existing approaches and the sta-
tistical properties of the data used for training. Speciﬁcally,
most training datasets involve tens of users with a few hun-
dreds (or more) repeated acquisitions per user. The non
i.i.d. nature of this data suggests better estimation may
be possible if the model explicitly made use of such “re-
peated measurements” from each user as is commonly done
in classical statistical analysis using so-called mixed effects
models. The goal of this paper is to adapt these “mixed ef-
fects” ideas from statistics within a deep neural network ar-
chitecture for gaze estimation, based on eye images. Such a
formulation seeks to speciﬁcally utilize information regard-
ing the hierarchical structure of the training data — each
node in the hierarchy is a user who provides tens or hun-
dreds of repeated samples. This modiﬁcation yields an ar-
chitecture that offers state of the art performance onvarious
publicly available datasets improving results by 10-20%.

1. Introduction

Gaze serves as an important cue in understanding human
attention, emotion and social interaction. Therefore, the
ability to estimate and track gaze is important for various
ﬁelds including psychology [19], neuroscience [13, 35, 8]

∗YX and HJK are corresponding authors. Work performed by HJK

before joining Korea University.

and more recently, computer vision [45, 17, 21, 23]. While
specialized eye gaze tracking hardware from several ven-
dors have been available and used in research experiments
for some time, in the last few years, many commodity prod-
ucts (such as [1]) can also be purchased that offer good real-
time accuracy. Unfortunately, many of the high performing
devices remain quite expensive and so, there is intensive
work to come up with accurate computer-vision based gaze
estimation techniques. One of the ideas in this line of work
is appearance-based methods [48], using the appearance of
the eye images to predict human gaze direction.

Generally speaking, appearance-based 3D gaze estima-
tion can be formulated as a regression f : x ∈ Rp → y ∈
R3, where x is a set of features, e.g., image derived features
and estimates of head pose from images and y is a gaze
direction in 3D space. This problem can be approached
in various ways. For example, we may use a standard k-
NN regression estimator as in [46] or random forests for
person-independent gaze estimation in [46]. The authors
in [30] design adaptive linear regression whereas Schneider
et al.[39] used support-vector regression with a polynomial
kernel. More recently, deep neural networks have been suc-
cessfully investigated for the problem in [60, 43]. For ex-
ample, in [60], one provides an input eye image to a deep
convolutional neural network and the last layer encodes the
three-dimensional gaze vector. The parameters of the net-
work can be trained with sufﬁcient training data.

Are assumptions satisﬁed? Observe that independent
of which scheme we use for inference, gaze estimation is
a statistical ﬁtting problem and understanding some of the
basic assumptions and properties may suggest natural av-
enues of improvement. One of the basic assumptions most
regression models make is that the samples are independent
identically distributed (i.i.d.) [50]. It is meaningful to as-
sess how (and whether) the common datasets used in many
gaze estimation works satisfy (or violate) this property. (A)
The Eyediap dataset [32] includes 94 video sequences of 16

17743

subjects looking at three different targets with both static
and free head motion under two different illumination con-
ditions. (B) The UT Multiview dataset contains 160 gaze
samples of 50 subjects recorded under controlled labora-
tory settings and 3D reconstructions of eye regions are used
to generate synthetic images for arbitrary head poses. (C)
A total of 214, 000 images from 15 subjects is collected in
MPIIGaze dataset, which is considered to be a challeng-
ing dataset captured under extreme illumination conditions.
The common feature in these (and other similar) datasets
is that the data is “grouped” naturally which correspond to
the number of participants in the acquisition study — these
are repeated measurements and not i.i.d. If the statistical
assumptions are violated, we can ask: (1) Is this just a the-
oretical issue or is this relevant in practice? (2) Are there
simple ﬁxes to this problem? For any gaze dataset, we can-
not expect a researcher to collect i.i.d. data: which would
mean spending effort into bringing in a participant and col-
lect only one gaze sample.

Basic setup of this paper. The answer to question (1)
is yes (we will experimentally show later). But ﬁrst, we
address (2) because it helps setup our formulation. Notice
that the issue of non i.i.d. data is not restricted to com-
puter vision and in fact, commonly occurs in social sci-
ences, epidemiology and medicine. The de-facto recom-
mendation when dealing with multiple samples from each
participant is to utilize the so-called mixed effects models
[22, 25] which are a special case of the more general hi-
erarchical Bayesian model. The mixed effects models are
composed of two parts: the ﬁxed effects and the random ef-
fects. The ﬁxed (global) effects are common in all samples
and so the corresponding coefﬁcients are called ﬁxed. In
contrast, the random (local) effects are speciﬁc to subjects
(or groups). The random effects coefﬁcients can vary de-
pending on the subjects, assumed to be drawn from some
unknown distribution. This approach is effective in many
applications [22, 2, 16, 12, 20] and broadly used in standard
statistical analysis. This raises the possibility of whether we
can utilize the “mixed effects” idea in deep neural networks
(especially CNNs) which offer state-of-the art performance
in a range of problems in computer vision. Actually, using
mixed effects is natural whenever images come clustered in
groups/hierarchy (not necessarily individual subjects): this
is common in ﬁne-grained multi-label classiﬁcation, object
detection, medical imaging, and any longitudinal data. In
general, other than participant speciﬁc random effects, we
may even consider a separate “site” or “dataset” speciﬁc
random effect. This is relevant because recent results have
shown that even for linear regression, different datasets can-
not be easily pooled in simple ways [62, 61].

Other models for repeated measurements. We should
data are
In fact, recurrent neural net-

point out that neural networks for non i.i.d.
not unique to this paper.

works (RNNs) have been studied in language modeling
[31], speech recognition [37], image captioning generation
[55], motion capture [47], and machine translation [5, 57]
since the 1990s and recent work has substantially built upon
the early formulations. However, RNNs are designed for
sequential (ordered) data and do not directly ﬁt our prob-
lem of gaze estimation based on a single input image. More
importantly, RNNs do not explicitly exploit the group infor-
mation (e.g., these 100 repeated measurements come from
Alice), whereas mixed effects models explicitly use the sub-
ject (group) information by estimating the random effects
per subject where the samples need not sequential. This
is the main gap which is addressed by our work. The con-
tributions of this paper are: 1) We provide a mathemati-
cally sound neural network which includes the beneﬁts of
terms that model repeated measurements, arguably a bet-
ter ﬁt with the statistical properties of most available gaze
datasets. 2) Experimentally, we show that our formulation
outperforms the state of the art by signiﬁcant margins (10%-
20%) on most available datasets.

1.1. Related work

Model-based methods for gaze estimation use a pre-
deﬁned geometric eye model and can be subdivided into
feature-based methods and shape-based methods. The
feature-based methods use predeﬁned geometric eye fea-
tures such as pupil center corneal reﬂection [14], iris con-
tours [36], leverage infrared sensors [14], stereo cameras
[42] and depth camera [18]. These approaches can offer
high accuracy but their dependence on specialized hard-
ware and calibration may not be suitable for more mass-
market applications. On the other hand, shape-based meth-
ods [60, 49, 52, 3] extract shape parameters from observed
eye images such as center of pupil, boundary of limbus
and iris and seek to associate them with a geometric eye
model to infer the gaze direction. These methods are quite
successful but since summaries (optical axis, cornea radii,
pupil radii) based on accurate registration are required, it is
not clear whether they can yield high accuracy with low-
resolution images from web cameras.

In contrast to the foregoing line of work, appearance-
based methods, do not use an explicit geometric eye model
and instead utilize eye images (or non-geometric features)
as an input to directly learn the parameters of a map-
ping between eye images and gaze. While early works
[4, 56, 48, 51, 41, 39] assumed ﬁxed head poses for per-
forming gaze estimation on eye images, more recent works
[29, 6, 33, 27, 28, 54, 24] show promising results with ar-
bitrary head poses, illumination and backgrounds. [4, 56]
trained neural networks on eye images for gaze estimation.
[48] utilized the local linearity of the eye appearance mani-
fold and applied local interpolation to predict gaze. In [29],
a calibrated approach, called adaptive linear regression, was

27744

developed for gaze estimation that is robust to head move-
ment. With the recent impact of deep neural network frame-
works in computer vision,
interest in appearance-based
methods has been revived, in large part via the use of CNNs
[60, 21, 58, 59, 10]. These works leverage relatively large
scale datasets collected on participants’ laptops and mobile
devices in more general settings during daily life. These
datasets aim to achieve appearance-based head-free and
calibration-free gaze estimation in a wide range of scenarios
involving signiﬁcant variations in illumination, head pose,
background and so on. We note that distinct from the ap-
proaches above, eye image synthesis has also been studied
towards generating larger training data with bigger varia-
tions in head pose [28] and more recently, via generative ad-
versarial networks (GANs) [43, 40]. [29, 26, 34] proposed
personalized gaze estimation methods to handle variability
across subjects using calibration samples.

2. Review of Mixed effects models

Many statistical models are ﬁxed effects models that use
a single “global” model and all parameters are associated
with the the full set of samples without encoding informa-
tion on which repeated samples came from which partici-
pants. In contrast, random effects models have a set of pa-
rameters for each subject (or group) and assume that the pa-
rameters are drawn from an unknown distribution. Unlike
setting up a prior distribution as in Bayesian methods, cho-
sen based on some domain knowledge, the unknown distri-
bution in random effects models are also estimated [38].

How is a mixed effects model different? A model with
both ﬁxed effects and random effects is called a mixed ef-
fects model [22]. Mixed effects models describe relation-
ships between a response variable and some covariates in
data that are grouped based on some grouping criteria. If
Alice and Bob provide 20 samples each, the model will
know which samples are from whom.
It is possible that
the measurements from Alice have higher values and sam-
ples from Bob have a bigger variance. By associating such
“random” effects to repeated measurements with the same
“person” tag, mixed effects models ﬂexibly represent the
covariance structure of the grouping of the samples.

Recall that appearance based gaze estimation is a ﬁtting
problem x → y where x ∈ Rp is an eye image or a fea-
ture vector (e.g., head pose, deep convolutional features)
extracted from an eye image and y ∈ R3 is a gaze direc-
tion. Our goal is to use ﬁxed effects for a global model (as
is the case in most existing schemes) but also include ran-
dom effects to make use of information on which samples
are from which participant — this yields subject-speciﬁc
adjustments. We ﬁrst introduce the linear and non-linear
version of mixed effects model, covered in common text-
books. Then, with this concept in hand, we will add mixed
effects terms within deep neural networks.

2.1. A Linear mixed effects model

We begin with a ﬁxed effects model, i.e., a standard lin-
ear model, and then introduce a linear mixed effects models.
Recall that a linear regression model is given as

y = β0 + β1x1 + · · · + βpxp + ǫ, ǫ ∼ N (0, Σǫ)

(1)

where x = [x1, . . . , xp] ∈ Rp, β = [β1, . . . , βp]T ∈ Rp,
y ∈ R. We call this the “standard” linear model. When
both x and y are multivariate measurements then we call
it a general linear model. For simplicity of discussion, we
introduce models with a univariate response variable (also
called labels, dependent or target variable). Observe that
in (1), all subjects have exactly the same function to map
eye appearance to gaze directions; the noise permitted in
the model estimation also comes from a distribution identi-
cal to everyone. But most gaze estimation datasets have re-
peated multiple measurements from a subject (see Fig. 1).
These samples are not independent and each subject may
have a slightly different mapping function. To address this
issue, we may add random effects to (1) for participant. This
yields a mixed effects model

y = β0 + β1x1 + · · · + βpxp + u1

i z1 + · · · + uq

ui ∼ N (0, Σu) and ǫi ∼ N (0, Σǫi )

i zq + ǫi,
(2)

i , . . . , up

where β := [β1, . . . , βp]T are the ﬁxed effects shared over
the entire population, ui := [u1
i ]T are the random
effects of the ith subject (or group), and z = [z1, . . . , zq]
is a design vector for q random effects. In the random ef-
fects part in (2), ui allows subject-speciﬁc adjustment and
ǫi is drawn from a subject-speciﬁc unknown distribution
N (0, Σǫi ) which enable precise handling of the non-i.i.d.
nature of the data. Typically, the unknown distributions are
assumed to be a zero mean Gaussian with an unknown co-
variance structure. Estimation involves estimating the pa-
rameters of a ﬁxed effects model β, the random effects com-
ponents u1, . . . , uN as well as Σǫi for all i, where N is the
number of subjects. Since the linear mixed effects models
have multiple random effects from unknown distributions,
no closed form solution is available [53]. For estimation,
EM algorithms and MCMC sampling are used [53, 15].

Alice

Bob

Charlie

6.5

Dave

8.4

8.0

7.6

p
e
e
l
s
 
f
o
 
s
r
u
o
H

p
e
e
l
s
 
f
o
 
s
r
u
o
H

7.6

7.2

6.8

p
e
e
l
s
 
f
o
 
s
r
u
o
H

6.0

5.5

p
e
e
l
s
 
f
o
 
s
r
u
o
H

5.5

5.0

4.5

1 2 3 4 5 6 7
Day of the week

1 2 3 4 5 6 7
Day of the week

1 2 3 4 5 6 7
Day of the week

1 2 3 4 5 6 7
Day of the week

Figure 1: A simple example. Sleep measurements of Alice, Bob, Char-
lie, Dave, when pooled, do not correspond to an i.i.d. sample of the distri-
bution, rather is hierarchically structured.

37745

7746

m
r
o
N
h
c
t
a
B

3
×
3

v
n
o
C

˜C
×
˜W
×
˜H

C
×
W
×
H

U
L
e
R

˜C
×
˜W
×
˜H

˜C
×
˜W
×
˜H

m
r
o
N
h
c
t
a
B

3
×
3

v
n
o
C

˜C
×
˜W
×
˜H

+

U
L
e
R

˜C
×
˜W
×
˜H

˜C
×
˜W
×
˜H

Figure 3: ResBlock structure.

can be formulated in design matrix form as,

yi = f (Xi) + ˜f (Xi) + ǫi, i = 1, . . . , N

(6)

where yi = [y(i1), . . . , y(ini)]T is the ni × 1 vector of
responses for the ni observations for subject i, summing
over all ni’s for N subjects gives us n observations, Xi =
[x(i1), . . . , x(ini)]T is the ni × p design matrix, and ǫi =
[ǫ(i1), . . . , ǫ(ini)]T ni × 1 vector of errors, ǫi ∼ N (0, Σǫi ).
Recall that the hidden representation after the fully con-
nected layer in Fig. 2 is Γ(Xi) which is the input for the
mixed effects models i.e., f (Xi) = Γ(Xi)β and ˜f (Xi) =
Γ(Xi)ui. We further assume that ui and ǫi are indepen-
dent and normally distributed and that the between-subject
observations are independent. We can verify that the covari-
ance of observations yi for subject i is

Vi = COV(yi) = Γ(Xi)ΣuΓ(Xi)T + Σǫi .

Estimation of a nonlinear mixed effects model can be
performed by EM algorithm [53, 16] and our algorithm
mimics this strategy for our formulation, see Algorithm 1.
Our algorithm is a variational EM algorithm which involves
an iterative optimization algorithm (SGD) within an EM
procedure. In any EM procedure, data are assumed to be
incomplete and the goal is to estimate the unobserved mea-
surements and model parameters iteratively. In our prob-
lem, β, ui and ǫ are unobserved measurements and they are
estimated in the Expectation step. In the Maximization step,
with the “complete” data (observed plus estimated measure-
ments), the algorithm seeks to ﬁnd the model parameters
Σu and Σǫi simply by maximizing the likelihood. Consis-
tent with convention, below, the estimate for any variable,
say ρ, is given as ˆρ.

What does the algorithm do? Algorithm 1 starts with
initial values for ˆβ, ˆu, ˆσ2, ˆΣu. Then in the E step, we cal-
ﬁxed
,
culate the ﬁxed effects part of the response variable, y
i
that is, the response variable from which we remove the
current (i.e., estimated) value of the random effects term.
For updating the ﬁxed effects term’s contribution to the re-
sponse variable, we use SGD for ﬁtting (x(ij), yﬁxed
(ij) ) using
the convolutional neural network to obtain the ˆβ and Γ(X).
We then estimate the random effects part ˆui based on re-
moving the update ﬁxed effects term. Lastly, we update the
between-subject and within-subject variance based on the

updated estimates of the residuals at M step. The algorithm
keeps iterating by updating ﬁxed effects and random effects
component until convergence. The convergence of the al-
gorithm can be captured by computing the following loss
function at each iteration:

F (g, β, ui|X, y) =

N

)

i=1

[(yi − Γ(Xi)β − Γ(Xi)ui)T Σ−1

ǫi

(7)

(yi − Γ(Xi)β − Γ(Xi)ui) + uT

i Σ−1

u ui + log |Σǫi| + log |Σu|]

This is the negative log-likelihood function with a Gaussian
assumption on noise (ǫ) and random effects ui as in the clas-
sical mixed effects models (2). To predict the gaze estimate
for a new observation j, we can encounter two cases. First,
the subject was seen at training time and second, when the
subject was not seen at training time. For the ﬁrst case, we
use both its corresponding population-level network regres-
sion term, ˆf (xij)(·) and the predicted random effect term
ˆ˜f (·) for prediction. For a sub-
corresponding to subject i,
ject not encountered at train time, next, we describe how the
contribution from these terms can be approximated without
knowing the subject “id” at test time.

Dealing with unseen subjects. A simple solution that
works well is to concurrently learn to “predict” the random
effects term ˜f (·) based on the input eye images without
knowing the subject id at test time. Assume we have two
functions, a univariate function, h(a) and a bivariate func-
tion, l(a, b). By marginalizing over the variable b, we can
ﬁnd the best h(a) such that h(a) ≈ *b l(a, b)db. If h(a)
is learned correctly, it can act as a good proxy for l(a, b)
without access to information regarding the second vari-
able b. Since our main model estimates a subject-speciﬁc
˜f (·) we use a function h(·) to predict the random effect
terms based only on the eye images. Notice that h(·) shares
much of the same network architecture as ˜f (·), so it is not
necessary to have a separate “network”, instead h(·) can
simply be a fully connected layer at the end which pre-
dicts the random effects offset using Γ(xij) as input, i.e.,
h(·) : Γ(xij) → ˜f (xij). In fact, training h(·) does not even
need to happen concurrently with Alg. 1. Once the training
of Alg. 1 has been completed, we can ﬁx the weights of
the convolutional layers, and learn a fully connected layer
h(·) which will best predict ˜f (xij) based only on the hid-
den representations Γ() provided by the convolutional lay-
ers. At test time, for a new subject, we use the ﬁxed effects
terms from our model and add in the “offset” provided by
h(Γ(·)) using that speciﬁc participant’s eye images (rather
its hidden representations from Γ(·)).

4. Experiments

In this section, we discuss the subject-independent gaze
estimation task and validate the effectiveness of our MeNets

57747

7748

specialized device to collect groud-truth gaze directions. It
requires calibration for each session. The setup of our data
collection system is shown in Fig. 4 (left) and the details of
the data collection system are available in the appendix. We
use the webcam of a MSI laptop to record eye images with
resolution 848×480 and Tobii X-30 attached to the laptop
to record corresponding gazes. The Tobii X-30 Compact
can offer gaze direction estimation error less than 1◦ under
ideal conditions including subject-speciﬁc accurate calibra-
tion [1, 7] and gives gaze direction estimation error 2.46◦
under non-ideal conditions. We collect this data from 7 vol-
unteers: 1,711 to 7,605 images per participant.

Deciding the architecture: We implemented and exper-
imented with using various state-of-the-art deep architec-
tures to serve as the convolutional module in our formula-
tion. We evaluated gaze estimation accuracy on the MPI-
IGaze dataset using leave-one-subject-out cross-validation.
The two options we evaluated were the 18-layer ResNet
and GoogLeNet. To setup the MeNets network, we change
the last classiﬁcation layer to two fully connected layers for
gaze direction regression as shown in Fig. 2. The evalua-
tions of MeNets with these two architectures versus the cor-
responding baselines (i.e., ResNet on its own, GoogLeNet
on its own) is shown in Table 1, which also shows the per-
formance of other state of the art approaches. From the
results, our proposed MeNets outperforms the correspond-
ing GoogLeNet and ResNet networks and all other contem-
porary approaches (a 10% improvement over GazeNet+, a
paired Wilcoxon test gives p-value < 0.01). Based on these
experiments, we use MeNets with the ResNets architecture
in the remaining evaluations.

Convergence of Variational EM + SGD: Before addi-
tional accuracy plots, we present some results to evaluate
the convergence of our estimation scheme. We conduct ex-
periments under two settings: within-subjects (standard 10-
fold cross-validation) and across-subjects (14 subjects for
training and one subject for testing). Both settings show
good convergence behavior. Evaluating the log-likelihood
as a function of the number of iterations, we see that 7-8
iterations are enough (see the appendix for example).

Does the “mixed effects” terms in the MeNets model
yield improvement? We evaluate whether the ﬁxed effects
terms alone in our architecture perform close to our MeNets
model (with mixed effects terms). We used the strategy in
[60] where a random subset for both training and testing
is used and includes 1500 left + 1500 right eye samples
for each person. Since eyes are not exactly symmetrical,
we swap the right eye image horizontally and mirror the
pose and gaze direction so that both eyes can be handled
by a single regression function. Here, our MeNets model is
trained with the random effects terms and then at test time,
we use two options: use predicted random effects or not
use predicted random effects. On MPIIGaze, even when

✹✟✠

✸✟✡

✸✟✠

✷✟✡

✷✟✠

✻✥✦

✧✥✧

✧✥✦

✤✥✧

✤✥✦

▼ ✁ ✂ ▼ ✁ ✂❬❢✄☎✆✝❪ ❘ ✞✁ ✂

✒✓✔✓✕ ✒✓✔✓✕✖✗✘✙✚✛✜ ✢✓✣✔✓✕

Figure 5: Comparison of MeNet, MeNet[ﬁxed], and ResNet for gaze
estimation under within-subjects and across-subjects settings. MeNet
uses random effects for gaze estimation with ResNet architecture at test
time, while MeNet[ﬁxed] does not incorporate random effects at test time
and ResNet does not consider any individual-level differences in train-
ing/testing. (left) MeNet offers the highest accuracy with 30% gain, 2.66
degrees vs 3.9 degrees by ResNet, even without incorporating random ef-
fects at test time, MeNet[ﬁxed] outperforms ResNet.
(right) Shows that
MeNet still gives the highest accuracy for across-subjects gaze estimation,
4.34 degrees vs 6.0 degrees by ResNet.

the random effects are not used at test time (but the model
was trained with the subject-speciﬁc random effects), we
achieve better accuracy versus using the ResNets architec-
ture on its own for gaze prediction. This is because incor-
porating subject speciﬁc random effects terms improves our
estimating the ﬁxed model in Fig 5. Using the full mixed
effects, yields the best results (p-value < 0.01).

We also evaluate whether other gaze datasets demon-
strate subject-speciﬁc random effects. By using subject-
speciﬁc random effects, we see that linear mixed effects
regression performs better than linear regression for gaze
estimation on the three datasets in Table 1. Linear mixed
effects model for gaze estimation is more accurate than just
using linear effects model: 2◦ more accurate on Real-video
and UT Multiview dataset. In summary, we ﬁnd that the
beneﬁt of terms that account for repeated samples is clearly
observed in our experiments.

Performance of MeNets on Within-Dataset evaluation:
We compare our proposed method with other baseline meth-
ods and evaluate gaze prediction accuracy under leave-
one-person-out setting. The model-based method EyeTab
performs poorly on three datasets with mean error larger
than 20 degrees.
It means that appearance-based meth-
ods have an advantage over model-based methods for per-
forming gaze estimation on real images. Since kNN, Ran-
dom forests, ALR regression for gaze estimation under-
perform convolutional neural networks [54], in our exper-
iments, we only report the performance of linear model,
linear mixed effects model, support vector regression, sev-
eral CNN based methods and MeNets. We perform leave-
one-person-out gaze estimation on MPIIGaze and 3 fold
cross validation on UT Multiview dataset(consistent with
[46]). Table 1 show the mean estimation errors of within-
dataset evaluation on MPIIGaze and Real-video dataset.
Our MeNet obtains mean error 4.9 degrees on MPIIGaze
under leave-one-person-out setting, while the state-of-the-
art GazeNet+ can only offer mean error of 5.4 degrees on
MPIIGaze (p-value < 0.01). Our gaze estimation also

77749

❊
☛
☛
☞
☛
✌
❞
✍
✎
☛
✍
✍
✏
✑
★
✩
✩
✪
✩
✫
✬
✭
✮
✩
✭
✭
✯
✰
Table 1: Comparison of our model with other baselines. Row 2 shows error with leave-one-person-out setting on MPIIGaze data and Row 3 shows error
on Real-video. Row 4 shows error with 3-fold cross validation on UT Multiview. Row 5 shows error with cross-dataset evaluation with training data from
the UT Multiview dataset and test error on MPIIGaze dataset. Our MeNet achieves large consistent accuracy improvement over baselines.

MPIIGaze
RealVideo

UT Multiview
Cross-dataset

MeNet

4.90 ± 0.59
6.72 ± 1.15
5.50 ± 1.03
9.51 ± 0.75

ResNet

6.04 ± 0.64

GoogLeNet
6.15 ± 0.81

GazeNet+[59]
5.40 ± 0.67

iTracker[21] MPIIGaze[54]
6.20 ± 0.85
6.59 ± 1.07

SVR

LR

LME

8.94 ± 3.20

7.44 ± 1.16

7.06 ± 1.07

6.98 ± 1.63

7.13 ± 1.74

6.90 ± 1.34

7.65 ± 2.01

9.78 ± 2.85

12.67 ± 3.57

12.90 ± 2.71

10.14 ± 1.88

5.86 ± 1.10

5.97 ± 1.15

5.78 ± 1.04

9.84 ± 1.73

9.97 ± 1.82

9.80 ± 1.83

N/A
N/A

5.98 ± 1.21

9.11 ± 2.27

9.07 ± 2.41

6.71 ± 1.41

13.30 ± 2.12

> 15

> 15

> 15

outperforms other CNN methods on UT Multiview. We
signiﬁcantly outperform all other baseline methods, which
supports the advantage of incorporating the speciﬁc effects
from each subject for gaze estimation.

How does this work for real video data?

In order
to show the ability of our method to provide subject-
independent gaze estimation for real data, we perform gaze
direction prediction for our Real-Video dataset. Since the
resolution of the eye image in Real-Video dataset is lower
than MPIIGaze dataset (and other real-world factors), it
makes gaze prediction more challenging than on the MPI-
IGaze dataset. We compared our method with other gaze
prediction methods. We outperform all other methods con-
sistently. Moreover, when adding MPIIGaze dataset for
gaze prediction on real video data, the accuracy increases
to 6.11◦. Some examples of gaze prediction can be seen in
the appendix.

Cross-Dataset evaluation: We assess the effectiveness
of our method for cross-dataset evaluations. We selected the
UT Multiview dataset for training and perform estimation
on MPIIGaze. Table 1 summarizes the mean angular er-
rors and standard deviations of our method and other CNN
based methods on MPIIGaze. Our method still outperforms
the GazeNet+, but the improvement is not as large as in
the within-dataset evaluation setting. Since UT Multiview
dataset uses learning-by-synthesis approach for generating
more eye images, the generated images are very different
from images in MPIIGaze. In [43], by adding realism of
generated images, the reﬁned generated images offers bet-
ter gaze estimation accuracy by more than 3◦. The data shift
problem is signiﬁcant here, which domain adaptation tech-
niques should be used to deal with, but were not utilized
in this paper. Then, we add the MPIIGaze dataset to the
training data and apply leave-one-person-out gaze estima-
tion for real-video dataset, it improves the gaze estimation
accuracy by more than 1◦ which partly supports the domain
shift problem between UT Multiview and MPIIGaze.

Personalized gaze estimation: As our mixed effects
model can learn the random effects associated with a spe-
ciﬁc person, our method is amenable to personalization via
few calibration samples. In order to show our method can
adapt to individual subjects, we perform personalized gaze
estimation on MPIIGaze dataset. We pick calibration sam-
ples from MPIIGaze and use the remaining samples for
evaluation. Given 200 calibration samples for each subject,
our model achieves mean error of 3.8 degrees, comparable
with state-of-the-art personalized gaze approaches [34].

Mixed-Dataset evaluation: In order to further show gaze
estimation performance on multi-datasets, we assess the ef-
fectiveness of our method for mixed-dataset evaluations,
where we pick samples from each dataset for training and
pick other samples for testing. We pick 10 subjects from
MPIIGaze dataset and the same number of eye images were
generated from GAN [44] for training a ResNet model.
Then, another 5 subjects from MPIIGaze dataset and the
same number generated by GAN were used for testing. Al-
though the actual gaze estimation task for both datasets is
the same, the difference between the synthetic dataset and
real dataset is large. Without considering the dataset differ-
ence, the trained ResNet model offers 16.36 degrees error,
while our MeNet formulation using a random effects term
for the dataset gives 11.2 degrees error and MeNet (with-
out random effects) yields 11.7 error. It means that dataset
speciﬁc random effects improve estimation over a model
that is agnostic of this information; suggesting the random
effects at test time yields additional improvements.

5. Conclusion

Most researchers performing data analysis know that the
choice of the correct model for the data at hand can lead
to improvements in performance, and conversely a sub-
optimal model can yield poor results. For appearance based
gaze estimation, we explore how an appropriate statistical
model that leverages information regarding repeated mea-
surements from the same participant, a common feature of
most if not all existing datasets, seems like a much better
ﬁt but has not been explored in computer vision much. To
practicalize this observation within modern architectures,
we propose a formulation that estimates a mixed effects
model while leveraging the beneﬁts of powerful deep neural
networks. This conceptually simple idea leads to improve-
ments (10-20% and more in some cases) over the state of the
art on most gaze estimation datasets. Code and appendix are
available at https://github.com/vsingh-group/MeNets.
Acknowledgments. This work was supported by UW
CPCP AI117924 and NSF CAREER award RI 1252725,
and partially supported by R01 EB022883, R01 AG062336,
R01 AG040396 and UW ADRC (AG033514). We thank
Karu Sankaralingam for discussions, and Mona Jalal,
Ronak Mehta, Ligang Zheng, Brandon M. Smith, Sukanya
Venkataraman, Haoliang Sun, Xiaoming Zhang, Sathya
Narayanan Ravi and Seong Jae Hwang for helping with var-
ious aspects of the experiments.

87750

References

[1] Accuracy and precision test report x2-30 fw 1.0.1.
[2] R Harald Baayen, Douglas J Davidson, and Douglas M
Bates. Mixed-effects modeling with crossed random effects
for subjects and items. Journal of memory and language,
59(4):390–412, 2008.

[3] Tadas Baltruˇsaitis, Peter Robinson, and Louis-Philippe
Morency. Openface: an open source facial behavior analysis
toolkit. In Winter Conference on Applications of Computer
Vision, pages 1–10, 2016.

[4] Shumeet Baluja and Dean Pomerleau. Non-intrusive gaze
tracking using artiﬁcial neural networks.
In Advances in
Neural Information Processing Systems, pages 753–760,
1994.

[5] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn
encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078, 2014.

[6] Jinsoo Choi, Byungtae Ahn, Jaesik Parl, et al. Appearance-
based gaze estimation using Kinect. In International Confer-
ence on Ubiquitous Robots and Ambient Intelligence, pages
260–261, 2013.

[7] A Clemotte, M Velasco, D Torricelli, R Raya, and R Ceres.
Accuracy and precision of the tobii x2-30 eye-tracking under
non ideal conditions. Eye, 16(3):2, 2014.

[8] Kim M Dalton, Brendon M Nacewicz, Tom Johnstone, et al.
Gaze ﬁxation and the neural circuitry of face processing in
autism. Nature neuroscience, 8(4):519–526, 2005.

[9] Eugene Demidenko. Mixed models: theory and applications

with R. John Wiley & Sons, 2013.

[10] Haoping Deng and Wangjiang Zhu. Monocular free-head 3d
gaze tracking with deep learning and geometry constraints.
In Computer Vision (ICCV), 2017 IEEE International Con-
ference on, pages 3162–3171. IEEE, 2017.

[11] Ludwig Fahrmeir and Gerhard Tutz. Multivariate statisti-
cal modelling based on generalized linear models. Springer
Science & Business Media, 2013.

[12] Zixing Fang and Robert L Bailey. Nonlinear mixed effects
modeling for slash pine dominant height growth following
intensive silvicultural treatments. Forest science, 47(3):287–
300, 2001.

[13] Nathalie George, Jon Driver, and Raymond J Dolan. Seen
gaze-direction modulates fusiform activity and its coupling
with other brain areas during face processing. Neuroimage,
13(6):1102–1112, 2001.

[14] Elias Daniel Guestrin and Moshe Eizenman. General theory
of remote gaze estimation using the pupil center and corneal
reﬂections. IEEE Transactions on biomedical engineering,
53(6):1124–1133, 2006.

[15] Jarrod D Hadﬁeld et al. Mcmc methods for multi-response
generalized linear mixed models: the mcmcglmm r package.
Journal of Statistical Software, 33(2):1–22, 2010.

[16] Ahlem Hajjem, Franc¸ois Bellavance, and Denis Larocque.
Mixed-effects random forest for clustered data. Journal of
Statistical Computation and Simulation, 84(6):1313–1328,
2014.

[17] Eakta Jain, Yaser Sheikh, Ariel Shamir, and Jessica Hodgins.
Gaze-driven video re-editing. ACM Transactions on Graph-
ics, 34(2):21, 2015.

[18] Li Jianfeng and Li Shigang. Eye-model-based gaze esti-
mation by rgb-d camera. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition Work-
shops, pages 592–596, 2014.

[19] Knut KW Kampe, Chris D Frith, Raymond J Dolan, et al.
Psychology: Reward value of attractiveness and gaze. Na-
ture, 413(6856):589–589, 2001.

[20] Hyunwoo J Kim, Nagesh Adluru, Heemanshu Suri, Baba C
Vemuri, Sterling C Johnson, and Vikas Singh. Riemannian
nonlinear mixed effects models: Analyzing longitudinal de-
formations in neuroimaging.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 2540–2549, 2017.

[21] Kyle Krafka, Aditya Khosla, Petr Kellnhofer, et al. Eye

tracking for everyone. In CVPR, pages 2176–2184, 2016.

[22] Nan M Laird and James H Ware. Random-effects models for

longitudinal data. Biometrics, pages 963–974, 1982.

[23] Yong Jae Lee, Joydeep Ghosh, and Kristen Grauman. Dis-
covering important people and objects for egocentric video
summarization. In CVPR, pages 1346–1353, 2012.

[24] Yin Li, Alireza Fathi, and James M. Rehg. Learning to pre-

dict gaze in egocentric video. In ICCV, 2013.

[25] Mary J Lindstrom and Douglas M Bates. Nonlinear mixed
effects models for repeated measures data. Biometrics, pages
673–687, 1990.

[26] Gang Liu, Yu Yu, Kenneth A Funes-Mora, Jean-Marc
Odobez, and Eyeware Tech SA. A differential approach for
gaze estimation with calibration. Technical report, 2018.

[27] Feng Lu, Takahiro Okabe, Yusuke Sugano, et al. Learning
gaze biases with head motion for head pose-free gaze esti-
mation. Image and Vision Computing, 32(3):169–179, 2014.
[28] Feng Lu, Yusuke Sugano, Takahiro Okabe, et al. Head pose-
free appearance-based gaze sensing via eye image synthesis.
In International Conference on Pattern Recognition, pages
1008–1011, 2012.

[29] Feng Lu, Yusuke Sugano, Takahiro Okabe, and Yoichi Sato.
Inferring human gaze from appearance via adaptive linear
regression. In Computer Vision (ICCV), 2011 IEEE Interna-
tional Conference on, pages 153–160. IEEE, 2011.

[30] Feng Lu, Yusuke Sugano, Takahiro Okabe, and Yoichi Sato.
Adaptive linear regression for appearance-based gaze esti-
mation. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 36(10):2033–2046, 2014.

[31] Tomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cer-
nock`y, and Sanjeev Khudanpur. Recurrent neural network
based language model.
In Interspeech, volume 2, page 3,
2010.

[32] Kenneth Alberto Funes Mora, Florent Monay, and Jean-
Marc Odobez. Eyediap: A database for the development and
evaluation of gaze estimation algorithms from rgb and rgb-d
cameras. In Proceedings of the Symposium on Eye Tracking
Research and Applications, pages 255–258. ACM, 2014.

[33] Kenneth Alberto Funes Mora and Jean-Marc Odobez. Gaze
estimation from multimodal kinect data. In Computer Vision
and Pattern Recognition Workshops, pages 25–30, 2012.

97751

[34] Seonwook Park, Xucong Zhang, Andreas Bulling, and Ot-
mar Hilliges. Learning to ﬁnd eye region landmarks for re-
mote gaze estimation in unconstrained settings. In Proceed-
ings of the 2018 ACM Symposium on Eye Tracking Research
& Applications, page 21. ACM, 2018.

[35] Kevin A Pelphrey, James P Morris, and Gregory McCarthy.
Neural basis of eye gaze processing deﬁcits in autism. Brain,
128(5):1038–1048, 2005.

[36] Michael J Reale, Shaun Canavan, Lijun Yin, Kaoning Hu,
and Terry Hung. A multi-gesture interaction system using
a 3-d iris disk model for gaze estimation and an active ap-
pearance model for 3-d hand pointing. IEEE Transactions
on multimedia, 13(3):474–486, 2011.

[37] Anthony J Robinson. An application of recurrent nets to
phone probability estimation. IEEE transactions on Neural
Networks, 5(2):298–305, 1994.

[38] George K Robinson. That blup is a good thing:

the esti-
mation of random effects. Statistical science, pages 15–32,
1991.

[39] Timo Schneider, Boris Schauerte, and Rainer Stiefelhagen.
Manifold alignment for person independent appearance-
based gaze estimation. In International Conference on Pat-
tern Recognition, pages 1167–1172, 2014.

[40] Matan Sela, Pingmei Xu, Junfeng He, Vidhya Naval-
pakkam, and Dmitry Lagun. Gazegan-unpaired adversar-
ial image generation for gaze estimation. arXiv preprint
arXiv:1711.09767, 2017.

[41] Weston Sewell and Oleg Komogortsev. Real-time eye gaze
tracking with an unmodiﬁed commodity webcam employ-
ing a neural network.
In CHI’10 Extended Abstracts on
Human Factors in Computing Systems, pages 3739–3744.
ACM, 2010.

[42] Sheng-Wen Shih and Jin Liu. A novel approach to 3-
d gaze tracking using stereo cameras.
IEEE Transactions
on Systems, Man, and Cybernetics, Part B (Cybernetics),
34(1):234–245, 2004.

[43] Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel,

Josh
Susskind, Wenda Wang, and Russ Webb. Learning from sim-
ulated and unsupervised images through adversarial training.
arXiv preprint arXiv:1612.07828, 2016.

[44] Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel,

Josh
Susskind, Wenda Wang, and Russ Webb. Learning from sim-
ulated and unsupervised images through adversarial training.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), volume 3, page 6, 2017.

[45] Hyun Soo Park, Eakta Jain, and Yaser Sheikh. Predicting
primary gaze behavior using social saliency ﬁelds. In ICCV,
pages 3503–3510, 2013.

[46] Yusuke Sugano, Yasuyuki Matsushita, and Yoichi Sato.
Learning-by-synthesis for appearance-based 3d gaze estima-
tion. In CVPR, pages 1821–1828, 2014.

[47] Ilya Sutskever, Geoffrey E Hinton, and Graham W Taylor.
The recurrent temporal restricted boltzmann machine.
In
Advances in Neural Information Processing Systems, pages
1601–1608, 2009.

[48] Kar-Han Tan, David J Kriegman, and Narendra Ahuja.
In Applications of

Appearance-based eye gaze estimation.

Computer Vision, 2002.(WACV 2002). Proceedings. Sixth
IEEE Workshop on, pages 191–195. IEEE, 2002.

[49] Roberto Valenti, Nicu Sebe, and Theo Gevers. Combin-
ing head pose and eye location information for gaze estima-
tion.
IEEE Transactions on Image Processing, 21(2):802–
815, 2012.

[50] Larry Wasserman. All of statistics: a concise course in sta-
tistical inference. Springer Science & Business Media, 2013.
[51] Oliver Williams, Andrew Blake, and Roberto Cipolla. Sparse
In

and semi-supervised visual mapping with the s3gp.
CVPR, volume 1, pages 230–237, 2006.

[52] Erroll Wood and Andreas Bulling. Eyetab: Model-based
gaze estimation on unmodiﬁed tablet computers.
In Pro-
ceedings of the Symposium on Eye Tracking Research and
Applications, pages 207–210. ACM, 2014.

[53] Hulin Wu and Jin-Ting Zhang. Nonparametric regression
methods for longitudinal data analysis: mixed-effects mod-
eling approaches, volume 515. John Wiley & Sons, 2006.

[54] Jia Xu, Lopamudra Mukherjee, Yin Li, Jamieson Warner,
et al. Gaze-enabled egocentric video summarization via con-
strained submodular maximization. In CVPR, pages 2235–
2244, 2015.

[55] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In International Conference on
Machine Learning, pages 2048–2057, 2015.

[56] Li-Qun Xu, Dave Machin, and Phil Sheppard. A novel ap-
In BMVC,

proach to real-time non-intrusive gaze ﬁnding.
pages 1–10, 1998.

[57] Wojciech Zaremba,

Recurrent neural network regularization.
arXiv:1409.2329, 2014.

Ilya Sutskever, and Oriol Vinyals.
arXiv preprint

[58] Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas
Bulling. Its written all over your face: Full-face appearance-
based gaze estimation.
In Proc. IEEE International Con-
ference on Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2017.

[59] Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas
Bulling. Mpiigaze: Real-world dataset and deep appearance-
based gaze estimation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 2017.

[60] Xucong Zhang, Yusuke Sugano, Mario Fritz, et al.
In CVPR,

Appearance-based gaze estimation in the wild.
pages 4511–4520, 2015.

[61] Hao Henry Zhou, Vikas Singh, Sterling C Johnson, Grace
Wahba, Alzheimers Disease Neuroimaging Initiative, et al.
Statistical tests and identiﬁability conditions for pooling and
analyzing multisite datasets. Proceedings of the National
Academy of Sciences, 115(7):1481–1486, 2018.

[62] Hao Henry Zhou, Yilin Zhang, Vamsi K Ithapu, Sterling C
Johnson, Vikas Singh, et al. When can multi-site datasets
be pooled for regression? hypothesis tests, 2-consistency
and neuroscience applications.
In Proceedings of the 34th
International Conference on Machine Learning-Volume 70,
pages 4170–4179. JMLR. org, 2017.

107752

