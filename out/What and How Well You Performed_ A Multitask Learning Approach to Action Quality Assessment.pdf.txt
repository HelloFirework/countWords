What and How Well You Performed? A Multitask Learning Approach to

Action Quality Assessment

Paritosh Parmar

Brendan Tran Morris

University of Nevada, Las Vegas

parmap1@unlv.nevada.edu, brendan.morris@unlv.edu

Abstract

Can performance on the task of action quality assess-
ment (AQA) be improved by exploiting a description of the
action and its quality? Current AQA and skills assessment
approaches propose to learn features that serve only one
task - estimating the ﬁnal score.
In this paper, we pro-
pose to learn spatio-temporal features that explain three
related tasks - ﬁne-grained action recognition, commen-
tary generation, and estimating the AQA score. A new
multitask-AQA dataset, the largest to date, comprising of
1412 diving samples was collected to evaluate our ap-
proach (http://rtis.oit.unlv.edu/datasets.
html). We show that our MTL approach outperforms
STL approach using two different kinds of architectures:
C3D-AVG and MSCADC. The C3D-AVG-MTL approach
achieves the new state-of-the-art performance with a rank
correlation of 90.44%. Detailed experiments were per-
formed to show that MTL offers better generalization than
STL, and representations from action recognition models
are not sufﬁcient for the AQA task and instead should be
learned.

1. Introduction

score

What

should an athlete

receive on her
dive/gymvault/skating/etc?
Which med student has
the highest surgical skill level? How well can he paint or
draw? How is a patient progressing in their physical reha-
bilitation program? Answering these questions involves the
quantiﬁcation of the quality of the action – determining how
well the action was carried out, also known as action quality
assessment (AQA). Existing AQA [18, 16, 26, 13, 25] and
skills assessment [4, 10, 31, 32, 33] approaches use a
single label, known as a ﬁnal score or skill-level, to train
the system using some kind of regression or ranking loss
function. However, the performance of these systems is
limited and it seems that a single score is not sufﬁcient
to characterize a complicated action.
In AQA, the ﬁnal

Multitask 

AQA

Action Quality Score:

83.25/100

Factorized Action 

Recognition:

Position: Tuck
Armstand?: No

Rotation type: Forward
No. of somersaults: 4.5

No. of twists: 0

AQA-oriented Caption 

Generation:

let's go and go he does
forward four-and-a-half
somersault three point
seven big opening dive
from even Garcia just a
really difficult dive to
the splash a
control
little more than he
might have liked nice
tight tuck position spots
feet
the water
coming over just a little

the

Figure 1: Multitask AQA concept. Recognizing an action
instance in detail and verbally describing its good and bad
points can be helpful in the process of quantifying the qual-
ity of that action instance. We propose to learn a model that
delineates an action besides measuring its quality. To see
the videos play, please download the manuscript and view
in an Adobe Reader.

score is dependent on what was done (this determines the
difﬁculty level) and how was that done (this determines
the quality of execution). We pose the following question:
can learning to describe and commentate on the action
instances help improve the performance on the AQA task?

We hypothesize that by forcing the network to learn to
do so will help better characterize the action, and hence aid

1304

in AQA. So, rather than using just a single encompassing
quality label to train the network, we introduce a multitask
learning (MTL) approach (Fig. 1) to assess the quality of
an action. Speciﬁcally, we propose to utilize 3D CNN’s to
learn spatio-temporal representations of salient motion and
appearance; optimize those using loss functions which ac-
count for i) the action quality score, ii) factorized (detailed)
action classiﬁcation, and iii) generate a verbal commentary
of performance; and are trained end-to-end. Note that the
architectures are multitask and not multi-modal since the in-
put does not use captions or action classiﬁcation to produce
the AQA score. Besides straight forward utility for AQA
and action classiﬁcation, automatic commentary or sports
narrative generation has been viewed valuable and greatly
applicable in a recent work by Yu et al. [29].

For AQA tasks, domain experts can provide detailed
analysis of performance. In the professional sports setting,
ground truth annotations for detailed action classiﬁcation
and commentary by former athletes are readily available in
broadcast footage facilitating extraction of labels and de-
scriptive captions. As such, to evaluate our approach, we
introduce the ﬁrst multitask AQA dataset with 1412 sam-
ples of diving which is also the largest AQA dataset to date.

Experimental evaluation show that performance of both
the architectures improved as more tasks were added and
the C3D-AVG-MTL variant outperforms all existing AQA
approaches in literature. MTL was shown to outperform
STL across various training set sizes. Further experiments
explore the AQA-orientedness of the feature representations
learned by our networks and ﬁnd they outperform action-
recognition representations on unseen actions indicating
that better generalized concepts of quality were learned.

Contributions: primary novelty of this works lies in the
problem formulation – to learn spatio-temporal represen-
tations by optimizing networks end-to-end jointly for ﬁne-
grained action description and AQA scoring. Task selec-
tion is intuitive. No previous work has done this; not
just for AQA, but even action recognition and captioning
tasks. We release a novel MTL-AQA dataset which is the
largest AQA dataset so far, much more diverse, challeng-
ing, and richly annotated with factorized ﬁne-grained action
class and AQA-oriented captions. Our dataset can help re-
searchers in the ﬁeld to examine new ideas for AQA and
auxiliary tasks. We show that our MTL approach works
across different architectures. Our approach is applicable to
a wide range of problems. Our proposed models are simple,
yet intuitive, and effective in carrying out central of learning
representations in a MTL setting by optimizing networks
end-to-end. Our C3D-AVG-MTL surpasses all the existing
approaches.

2. Related Work

AQA: Pirsiavash et al.
[18] proposed the use of
DFT/DCT of body pose as features for a support vector
regressor (SVR) to map to a ﬁnal action quality score.
They introduced an action quality dataset containing two
actions: Diving and Figure Skating. However, since
their method relied solely on pose features, it neglected
important visual quality cues, like splash in the case of
Diving. Since accurate pose is especially difﬁcult in sports
scenarios where athletes undergo extremely convoluted
poses, Venkataraman et al. [25] better encoded using the
approximate entropy of the poses to improve the results.

More recently, spatio-temporal features from 3D convo-
lutional neural networks (C3D) [24] proved to be very suc-
cessful on a related task of action recognition since they
captured appearance and salient motion. Seeing this as
a desirable property that would help to take into account
visual cues, Parmar and Morris [16] proposed using C3D
features for AQA. They proposed three frameworks, C3D-
SVR, C3D-LSTM, and C3D-LSTM-SVR, which differed
in their feature aggregation and regression scheme. All
the frameworks worked better than previous models prov-
ing the efﬁcacy of C3D features for AQA. Xiang et al. [26]
proposed breaking video clips into action speciﬁc segments
and fusing segment-averaged features instead of over full
videos. By adding ﬁner segment labels to data samples per-
formance was improved. Li et al. [13] divide a sample into
9 clips and use 9 different C3D networks dedicated to differ-
ent stags of Diving. Features are concatenated and further
processed through conv and fc layers to produce a ﬁnal
AQA score using a ranking loss along with the more typ-
ical L2 loss. Xu et al. [27] tackle AQA for longer action
sequences using self-attentive and multiscale convolutional
skip LSTM.

Skills assessment: Zia et al. [33] extract spatio-temporal
interest points (STIP’s) in the frequency domain to classify
a sample into novice, intermediate or expert skills level. In-
stead of using handcrafted STIP’s Doughty et al. [4] learn
and use convolutional features with ranking loss as their
objective function to evaluate surgical, drawing, chopstick
use and dough rolling skills. In their subsequent work [5],
they use temporal attention. Li et al. [14], make use of spa-
tial attention in the assessment of hand manipulation skills.
Bertasius et al. [1] focus on measuring basketball skills but
rely only on assessment of a single basketball coach making
their dataset subjective to a particular evaluator.

All of the existing AQA and SA frameworks are single
task models and only give the ﬁnal AQA score. Our pro-
posed framework is a multitask model to recognize the ac-
tion, measures its quality and also generates captions (or

305

Dataset

Events

Height

Genders

# Samples

Events

View Variation/

Background

Labels

MIT Dive [18]

Individual

10m Platform

UNLV Dive [16]

Individual

10m Platform

Male

Male

159

370

1

1

No/Same

AQA score

No/Same

AQA score

Ours MTL-AQA

Individual,
Synchronous

3m Springboard,

10m Platform

Male,
Female

1412

16

Yes/Different

AQA score,
Action class,
Commentary

Table 1: Details of our newly introduced dataset, and its comparison with the existing AQA datasets.

Position Armstand Rotation type

# SS

# TW

Free
Tuck
Pike

No
Yes

Inward
Reverse
Backward
Forward

0 to 4.5

0 to 3.5

Table 2: Classiﬁcation of dives. Each combination of the
presented sub-ﬁelds produces a different kind of maneuver.

commentary).

Multi-modal approaches and captioning:
Images and
videos (especially sports) are often accompanied by a cap-
tion or commentary which can themselves serve as labels
yet to be exploited for AQA or skill assessment. Quattoni
et al. [19] use large quantities of unlabeled images, with
associated captions, to learn image representations. They
found that this sort of pre-training with extra information
could speed up the learning on a target task. Rather than
using captions as groundtruth labels, Sonal et al. [6] treated
captions as a “view” and use them along with images to
learn a classiﬁer using co-training. They again used com-
mentary as a “view” for action recognition with success. To
train an activity classiﬁer in an automated fashion, without
the requirement of any manual labeling, Sonal and Mooney
[7] make use of broadcast closed captions and used the sys-
tem for video retrieval. There are a few works which fo-
cus on captioning in sports settings. Yu et al. [29] address
the task of generating ﬁne-grained video descriptions for
basketball and evaluate performance using their novel met-
ric. Commentary generation in cricket has been addressed
in [20, 21], while Sukhwani addressed the problem of de-
scribing tennis videos in [23]. While these works focus on
captioning or improving captioning, we integrate a caption-
ing task with an AQA task to provide stronger supervision
as commentary is a verbal description of AQA.

3. Multitask AQA Dataset

In order to facilitate research in the area of AQA, we re-
lease a new dataset. This is the ﬁrst of a kind multitask

AQA dataset. With 1412 samples, it is the largest AQA
dataset to date. This particular dataset focuses only on Div-
ing as it has seen the most usage recently. Data was com-
piled from 16 different events unlike the single main event
(2012 Olympics Men’s 10m Platform Diving competition)
used for previous datasets [18, 16] to provide signiﬁcantly
more variation. Diving samples in the new dataset were col-
lected from various International competitions and include
the 10m Platform as well as 3m Springboard, include both
male and female athletes, individual or pairs of synchro-
nized divers, and different views. A comparison of our new
dataset with existing Diving AQA sets is provided in Table
1.

Since data was collected from televised international
events, before the athletes perform their routines, informa-
tion regarding their routine is displayed. This information
includes the difﬁculty of the dive and a description of the
dive. The AQA score is extracted from the judges’ scores
after the dive completion. The dataset uses the same dive
classiﬁcation strategy as Nibali et al. [15], where instead of
using dive number (equivalent to an action class in action
recognition) directly, we factorize a dive into its compo-
nents such as the position of the dive, the number of somer-
saults (SS), and number of twists (TW). Full details for the
dive classiﬁcation is in Table 2.

Further, during and after a diving routine, television an-
alysts provide commentary. These analysts are often retired
athletes and have deep understanding of the sport. This ver-
bal account of the athlete’s performance is recorded for the
third type of action label. The commentary was consid-
ered an important indicator for performance since it was the
only way to “watch” an event before telecast was available.
Commentators say what the athlete performed, what was
correct with the athlete’s performance, and where and how
athletes made mistakes. This provides deeper insight into
the athlete’s performance and can help an average person
better understand the sport. We used Google’s Speech-To-
Text API to convert commentary audio to text.

306

4. Multitask Approach to AQA

ground truth label

MTL is a machine learning paradigm in which a single
model caters to more than a single task. An example is to
recognize road signs, roads, and vehicles together while an
STL approach would require separate models for each ob-
ject type. MTL tasks are generally chosen such that they
are related to one another and their networks have a com-
mon body that branches into task-speciﬁc heads. The total
network loss is the sum of individual task losses. When
optimized end-to-end, the network is able to learn richer
representation in the common body section since it must be
able to serve/explain all tasks. With the use of related aux-
iliary tasks, which are complementary to the main task, the
richer representation tends to help improve performance on
the main task.

In general, not just for diving, action quality is a func-
tion of what action was carried out and how well that action
was executed. This makes the choice of auxiliary tasks nat-
ural: detailed action recognition is the answer to the ‘what’
part and commentary, being a verbal description containing
good and bad points about action execution, is an answer
to the ‘how well’ part. AQA can be thought of as ﬁnding
a function that maps input video to the AQA scores. Caru-
ana in [2] views supervision signals from auxiliary tasks
as an inductive bias (assumptions). Inductive bias can be
thought of as constraints that restrict the hypothesis/search
space when ﬁnding the AQA function. Through inductive
biases, MTL provides improved generalization as compared
STL [2].

In this work, the main task is to assess the action quality
(AQA score) and the auxiliary tasks are to recognize the
action (dive type classiﬁcation) and to generate descriptive
captions/commentary. Action recognition in turn consists
of ﬁve ﬁne-grained dive sub-recognition tasks: recognizing
position and rotation type, detecting armstand, and counting
somersaults and twists.

First, let us formalize the settings and objective func-
tions. AQA is a regression problem where, generally, the
Euclidean distance between the predicted quality score and
the ground truth is used as the objective function to be mini-
mized [16, 26, 13]. Initial experimentation found that using
L1 distance in addition to L2 yielded better results on the
AQA task

LAQA = −

1
N

N

X

i=1

(xi − yi)2 + |xi − yi|

(1)

where xi is the predicted score and yi is the ground truth
score for each of the N samples. For action recognition,
we use cross-entropy loss between the predicted labels and

LCls = −

1
N

N

ksa

X

X

X

i,jlog(xsa
ysa
i,j)

(2)

i=1

sa

j=1

where ksa is the number of categories in sub-action class sa
(as in Table 2). Negative log likelihood is used as the loss
function for the captioning task

LCap = −

1
N

N

X

X

i=1

sl

ln(xcap

ycap )

(3)

with sl is the sentence length. The overall objective func-
tion to be minimized is the summation of all the losses

LM T L = αLAQA + βLAR + γLCap.

(4)

where α, β, γ are loss the weights. Now, we will introduce
two different architectures for MTL-AQA.

MTL-AQA architectures Unlike action recognition that
may be accomplished by looking at as little evidence as just
a single frame [11], for AQA the complete action sequence
needs to be considered because the athlete can make or lose
points at any point during the whole sequence.

While spatio-temporal representations learnt using 3D
CNN’s capture appearance and salient motion patterns [24],
which makes them one of the best candidates for action
recognition [24, 8] and also for AQA [16, 26, 13], 3D
CNN’s require large memories which limits their applica-
tion to small clips. We tackle this bottleneck in two ways:

1. divide the video (96 frames) into small clips (16
frames), and then aggregate clip-level representations
to obtain video-level description (Sec. 4.1)

2. downsample the video into a small clip (Sec. 4.2)
Networks designed for multitask learning generally two
segments: common network backbone and task-speciﬁc
heads. Common network backbone learns shared repre-
sentations, which are then further processed through task-
speciﬁc heads to obtain more task-oriented features and out-
puts.

4.1. Averaging as aggregation (C3D-AVG)

The ﬁrst network we present is C3D-AVG (Fig. 2).

Network backbone: Backbone consists of C3D network
[24] upto the ﬁfth pooling layer.
Aggregation scheme: An athlete gathering (or losing)
points throughout the action can be seen as an addition op-
eration. Combining this perspective with a good rule of
thumb that when good representations are learned, linear
operations on them become meaningful, we propose to en-
force a linear combination of representations to be mean-
ingful, in order to learn good representations. Speciﬁcally,

307

Figure 2: C3D-AVG-MTL network.

we propose to use averaging as the linear combination. The
network is optimized end-to-end for all three tasks.

C3D-AVG network up to Average layer can be
considered as an encoder, which encodes input video-clips
into representations that when averaged (in feature space)
would correspond to the total AQA points gathered by the
athlete. Subsequent layers can be thought of decoders for
individual tasks.
Task-speciﬁc heads: For action-recognition and AQA
tasks, clip-level pool-5 features are averaged element-
wise to yield a video-level representation. Since captioning
is a sequence-to-sequence task, the individual clip-level
features are input to the captioning branch before averaging
(individual clip-level features worked better in practice
than averaged clip-level features for captioning).

4.2. Multiscale Context Aggregation with Dilated

Convolutions (MSCADC)

Multiscale context aggregation with dilated convolutions
(MSCADC) [28] has been shown to improve the classiﬁca-
tion of dives in the work of Nibali et al. [15]. Given its
strong performance on an auxiliary task MSCADC was se-
lected for MTL. Our MTL variant network has a backbone
and multiple heads as illustrated in Table 3.
Network backbone: The MSCADC network is based on
C3D network [24] and incorporates improvements like us-
ing Batch Normalization [9] to provide better regularization
which is needed in AQA where data is quite limited. Addi-
tionally, pooling is removed from the last two convolutional
groups of C3D and instead a dilation rate of 2 is used. This
backbone structure is shared among all the MTL tasks.
Task-speciﬁc heads: We use separate heads, one for each
task. Heads consist of a context net followed by a few addi-
tional layers. The context net is where the feature maps are
aggregated at multiple scales.

Dilated convolutions and multi-scale aggregation have
shown improvements in the tasks involving dense predic-
tions [28]. We believe that removing pooling layers and
using dilated convolutions better maintains the structure of
the diving athlete without losing resolution. This helps in
better assessment of the athlete’s pose which is critical for
AQA. For example, pose can identify when legs are aligned
or split which is useful not only for diving but also other
sports such as gymnastic vault, ﬁgure skating, skiing, snow-
boarding, etc.

Unlike the C3D-AVG network, we downsample the com-
plete action into a short sequence of only 16 frames (some-
thing like key action snapshots) as done by Nibali et al. [15].
This reduces our 96-frames videos into key action snapshots
which helps in processing the complete action sequence in
a single pass. Processing an action sequence using this net-
work can be thought of as distilling information from the
input frames and putting it into feature maps, with different
feature maps containing different kinds of pose information.
A natural beneﬁt of downsampling the sequence is that there
is a signiﬁcant reduction in the the number of network pa-
rameters and memory which can be used instead to increase
spatial resolution.

5. Experiments

Implementation: PyTorch [17] is used to implement all
the networks; common network backbones were pretrained
on the UCF101 [22] action recognition dataset. The cap-
tioning module utilized a GRU [3] cell and a dropout rate of
0.2 in the encoder and decoder. Maximum caption length
is set to 100 words. Full vocabulary size is 5779. The pa-
rameters α, β, and γ in Eq. 4 are set to 1, 1, and 0.01. All
networks used the Adam optimizer [12] and were trained for
100 epochs with initial learning rate of 1e-4. Data augmen-
tation is performed through center cropping with temporal
augmentation and random horizontal ﬂipping. The center

308

96 frames(96 * 112 * 112 * 3)sharedweights8192Clip 2 (16 frames)8192Clip 6 (16 frames)C1; C2; C3a,b; C4a,b; C5a,b8192Clip 1 (16 frames)AVERAGEFCAction ClassifierAQA Score RegressorCaption GeneratorΣPositionArmstandRotation type# SS#TWAQAloss4096Finalloss...Common network backboneTask-specific heads(Common network body)

C3(32); BN
MP(1,2,2)
C3(64); BN
MP(2,2,2)

{C3(128); BN} x2

MP(2,2,2)

{C3(256); BN} x2

{C3(d=2,256); BN} x2

Dropout(0.5)

(Task-speciﬁc heads)

(AQA Score

(Action

(Captioning

Head)
C1(12)

{Cntxt net}
MP(2,2,2)
C3(12); BN

C3(1)

recognition Head)

C1(12)

{Cntxt net}
MP(2,2,2)
C3(12); BN

(Action

AP(2,11,11)

recognition sub-heads)

Head)
C1(12)

{Cntxt net}
MP(2,2,2)
C3(12); BN
Enc. GRU
Dec. GRU

Table 3: MSCADC-MTL architecture. C3(d,ch): 3D
convolutions, ch-no. of channels, d-dilation rate. C1: 1x1x1
convolutions. BN: batch normalization. MP(kr): max
pooling operation, kr-kernel size. Cntxt net: context net
for multi-scale context aggregation. AP: average pooling
across (2x11x11) volume.

crop was found to reliably capture both the athlete and other
prominent visual cues such as splash. Batch-size was set to
three samples. Additional architecture-speciﬁc implemen-
tation details are as follows:
C3D-AVG: The model is trained end-to-end with a 112 ×
112 center crop from the 171 × 128 pixel input video. Each
dive sample was temporally normalized to a length of 96
frames.
MSCADC: Since this architecture does not contain fully-
connected layers and all videos are downsampled to 16
frames, there are fewer model parameters allowing the use
of higher resolution video input. Frames are resized to
640 × 360 pixels and 180 × 180 center cropping is used.
Evaluation metrics: AQA is assessed using Spearman’s
rank correlation, dive classiﬁcation uses accuracy, and com-
mentary uses captioning metrics of Bleu, Meteor, Rouge,
and CIDEr.

5.1. Single task vs. Multi task approach

We carry out an experiment to compare the performance
of STL against that of MTL. We have a total of 3 tasks:
AQA, detailed action recognition, and commentary genera-
tion. This experiment ﬁrst considered the STL approach to
AQA task and then measured the effect of including auxil-
iary tasks. The evaluation is summarized in Table 4. We
observe that MTL approaches perform better than STL ap-
proach for both the networks, which shows that our MTL

Tasks

AQA

+ Cls
+ Caps
+ Cls + Caps

C3D-AVG

MSCADC

89.60

89.62
88.78
90.44

84.72

85.76
85.47
86.12

Table 4: STL vs. MTL across different architectures.
Cls - classiﬁction, Caps - captioning. First row shows STL
results, while the remaining rows show MTL results.

Nibali

Ours-MTL

et al. [15]

MSCADC

C3D-AVG

Position
Amstand
Rotation type
# Somersaults
# Twists

74.79
98.30
78.75
77.34
79.89

78.47
97.45
84.70
76.20
82.72

96.32
99.72
97.45
96.88
93.20

Model

B1

B2

B3

B4

M

R

C

C3D-AVG
MSCADC

0.26
0.25

0.10
0.09

0.04
0.03

0.02
0.01

0.11
0.11

0.14
0.13

0.06
0.05

Table 5: Performance on auxiliary tasks.

approach is not limited to a network but is generalizable
across networks. Other thing to note here is that MTL per-
formance improves as we incorporate more tasks. Com-
paring both the architectures, we ﬁnd that our C3D-AVG
outperforms our MSCADC for both STL and MTL, while
MSCADC has the advantage of being fast and lower mem-
ory requirement than C3D-AVG. For qualitative results, re-
fer to Table 6 and supplementary material.

Performance on the auxiliary tasks is presented in Table
5. To the best of our knowledge there is only one work
(by Nibali et al. [15]) on detailed dive classiﬁcation. Our
C3D-AVG-MTL performed best on the classiﬁcation task as
well. We also give captioning metrics for the two networks
though there is no baseline for comparison in literature.

Secondly, we compare our models with the existing
methods in Table 7. We obtain the results for all of the
existing methods on our dataset. C3D-SVR was the best
performing method in [16] but it does not seem to ben-
eﬁt from the increased number of training samples.
In
[16], C3D-LSTM was reported to be performing worse than
C3D-SVR due to insufﬁcient amount of training data and
does outperform C3D-SVR with the expanded training data.
Our MSCADC-STL works better than most of the existing
methods, whereas our C3D-AVG-STL is better performing
than all the existing methods. Furthermore, C3D-AVG-
MTL with 90.44 correlation achieves new state-of-the-art
results.

309

True labels: 89.08; [Tuck, No, Backwards, 3.5, 0] C3D-AVG labels: 80.41; [Tuck, No, Backwards, 3.5, 0]
C3D-AVG: that’s good she will certainly keep that with a dive that we can certainly do it in that ﬁeld very impressive it is
very good but it was a good dive here playing a little bit low water which is a strong start one look at that closes vertical
position it’s gonna get seven and a half’s I think super slow-motion just throwing up a little bit too much splash but she’ll get
over 60s mid sixties probably and that will be good enough to keep her middle of the pack which is where she needs to be to

True labels: 63.07; [Free, Yes, Backwards, 2, 2.5] C3D-AVG labels: 65.79; [Free, Yes, Backwards, 2, 1.5]
C3D-AVG: well it’s okay and the entry into the water not quite 100% vertical but he’s just a little bit overcooked on the end
obviously there’s a few of the divers have you use themselves so if you’re getting your hands out there we are rocking and
rolling a little bit of a splash with technically a little bit of splash that’s not the splash means that the judges will penalize him
or only got to 17 from the two and a half somersaults before he goes into the water now that was a

True labels: 84.15; [Tuck, No, Backwards, 3.5, 0] C3D-AVG labels: 81.94; [Tuck, No, Backwards, 3.5, 0]
C3D-AVG: excellent excellent dive if you might he’s got a lot of divers here with their hands together for him a lot of them
here and take a little bit of an angle on the entry that does good through that would not quite a way over a vertical look at that
perfect angle so much better judges will like that that angle so not too many

True labels: 47.77; [Pike, No, Forwards, 2.5, 1] C3D-AVG labels: 53.04; [Pike, No, Forwards, 2.5, 1]
C3D-AVG: nice nice entry because the execution was ﬁne and then just suggesting she went surﬁng over the end of the diving
board anyway she’s a safe distance from the diving board so that’s a good dive in the prelims you can see the splash moving
away from the diving board six and a half’s sevens at best moving further away from the podium dive after dive star with a
58 and it with a 64 this

Table 6: Qualitative results. Labels are ordered as follows: AQA score; [Position, Armstand?, Rotation type, #SS, #TW].
Due to space constraints only generated captions are shown here; please refer to supplementary material for groundtruth.

Method

Sp. Corr.

# samples

1059

450

280

140

Pose+DCT [18]
C3D-SVR [16]
C3D-LSTM [16]
Ours MSCADC-STL
Ours C3D-AVG-STL

Ours MSCADC-MTL
Ours C3D-AVG-MTL

26.82
77.16
84.89
84.72
89.60

86.12
90.44

Segment-speciﬁc methods (train/test on UNLV Dive [16])

S3D (best performing in [26])
Li et al. [13]
Ours MSCADC-STL
Ours C3D-AVG-STL

Ours MSCADC-MTL
Ours C3D-AVG-MTL

86.00
80.09
79.79
83.83

80.60
88.08

Table 7: Performance comparison with the existing AQA
approaches.

Method proposed by Xiang et al. [26] requires manual
annotation to mark end points of all the segments which is
not available in the new Diving-MTL data. Xiang et al. [26]
used the UNLV-Dive dataset [16] so for a fair comparison
with [26] we train and test our models on UNLV-Dive [16].
The results are enumerated in Table 7. Our C3D-AVG-STL
does not perform as well S3D [26]. However, our C3D-
AVG-MTL outperforms the S3D model. An important thing
to note here is that UNLV-Dive dataset is quite a bit smaller
than our newly introduced MTL-AQA dataset which should

STL
MTL

89.60
90.44

77.27
83.52

69.63
72.09

64.17
68.16

Table 8: STL vs. MTL generalization. Training using
increasingly reduced no. of training samples.

limit MTL performance. However, as pointed out in Section
4, MTL provides better generalization than STL, which al-
lows C3D-AVG-MTL to learn effectively from fewer train-
ing samples.

Generalization provided by MTL:
To ascertain that
MTL is providing more generalization, we train our C3D-
AVG-STL and C3D-AVG-MTL models using fewer num-
ber of datapoints. Train set size and the corresponding
STL/MTL performances are detailed in Table 8. We see
that MTL consistently outperforms STL, and also the gap
seems to widen with fewer training samples.

5.2. AQA orientedness of the learned

representations

We trained our networks end-to-end to learn AQA-
speciﬁc feature representation rather than relying on pre-
trained action-recognition oriented features (as done in
[16]). However, we question if there is a utility in
learning AQA-speciﬁc feature representation or are action-
recognition oriented features equally good? To answer
this, we follow an evaluation scheme similar to Zhang et
al. [30], where we train linear regressors on top of all

310

c1

c2

c3

c4

c5

71.01 71.39 73.13 76.34 73.69
Baseline-1
Baseline-2
72.43 70.15 70.35 57.20 37.63
C3D-AVG-MTL 74.26 77.95 82.78 86.18 85.75

Table 9: Performance of ﬁtting linear regressors on the
activations of all the convolutional layers.

c1

c2

c3

c4

c5

Train/Test events overlapping

41.10 32.06 36.53 46.86 44.78
Baseline-1
Baseline-2
37.76 42.02 37.98 44.28 38.56
C3D-AVG-MTL 38.32 42.68 45.53 49.18 38.47

Train/Test events non-overlapping

(requires more generalization)

Baseline-1
-02.68 00.75 -03.91 -02.22 03.17
25.80
Baseline-2
-07.52 -02.44 05.07 24.09
C3D-AVG-MTL -07.75 -02.77 23.51 29.56 -03.25

Table 10: Performance of ﬁtting linear regressors on the
activations of all the convolutional layers for a novel ac-
tion class, Gymnastic vault. Top rows: Within-dataset
evaluation, bottom rows: Out-of-dataset evaluation.

the convolutional layers, and compare the performance ob-
tained for AQA and action-recognition models. In partic-
ular, we consider two action-recognition baselines: C3D
model trained on UCF-101 dataset [22] (Baseline-1), and
our model trained on our MTL-AQA dataset, but for factor-
ized action recognition task (Baseline-2).

In the primary evaluation, we compare the representa-
tions for measuring the quality of diving action. Compari-
son is detailed in Table 9. In comparison to both the base-
lines, we ﬁnd that our C3D-AVG-MTL learns better repre-
sentations at all the intermediate layers.

Further we compare the representations for measuring
the quality of an unseen action class – Gymnastic vault
[16]. This helps in estimating the generalizability of the
representations. We hypothesize that if our AQA network
has learned better representations that actually capture the
concept of quality in an action, then it should be able to
measure the quality of an unseen action better than action-
recognition speciﬁc networks. We carry out 2 different
evaluations: 1) Within-dataset evaluation and 2) Out-
of-dataset evaluation.
In Within-dataset evaluation we
randomly divide the samples into train set and test set,
whereas in Out-of-dataset evaluation, train and test sam-
ples are drawn from different athletic competitions. Out-
of-dataset evaluation is more challenging and requires fea-
ture representations to be more generalizable and not suffer
from dataset-bias. Like the previous experiment, to com-

pare learned representations, we train linear regressors on
top of all the convolutional layers. Train and test sets con-
sist of 125 and 56 samples respectively. Results from both
evaluations are presented in Table 10.

In the Within-dataset evaluation,

the representations
learned by all the models seem to be working well, although
C3D-AVG-MTL performs best. The difference in perfor-
mance becomes clearer in the Out-of-dataset evaluation. As
expected, Out-of-dataset evaluation is more challenging and
performances of all the models drop. However, the perfor-
mances of Baseline-2 and our model drop more gracefully.

6. Discussion

We introduced a multitask learning approach to AQA
and showed that MTL performs better than STL because of
better generalization which is especially important in AQA
and skill assessment since datasets are small. We showed
that the representations learned by our MTL models are
better able to capture the inherent concept of quality of ac-
tions. Our approach is scalable since the supervision re-
quired for the auxiliary tasks is readily available from the
existing video footage with minimal extra effort compared
to just AQA labeling. In addition, state-of-the-art perfor-
mance was achieved without any ﬁnetuning of hyperpa-
rameters. Our best performing and recommended model,
C3D-AVG-MTL, achieved 90.44% correlation with judged
scores which still leaves a small gap to achieve human-
experts-level performance (96% [18]).

Extension to other actions and skills assessment: Al-
though this paper is geared speciﬁcally toward multitask
diving AQA, the approach is general in nature. No de-
sign decisions were biased towards or speciﬁc to the diving
tasks. Experiments even showed that the models trained on
diving do work reasonably well for another action, gymnas-
tic vault. This encouraging result hints at the direct appli-
cation of our MTL approach on other actions and everyday
skills assessment. Commentary and action class details are
available almost all the of time in the sport footages. For
non-sport skills assessment, such as surgery, needle pass-
ing, drawing, or painting, experts could be used to gener-
ate comments and deﬁnition of sub-actions for classiﬁca-
tion. Note that existing datasets can simply be augmented
to include additional labels, instead of building new datasets
from scratch. Also, our MTL approach is complementary to
the existing AQA and skills assessment approaches.

Acknowledgements: Thank you Andy (Squadra), Mark
(Wilbourne), Josh (Rana) for helping us with the dataset
collection!

311

References

[1] Gedas Bertasius, Hyun Soo Park, X Yu Stella, and Jianbo
Shi. Am i a baller? basketball performance assessment from
ﬁrst-person videos. In Computer Vision (ICCV), 2017 IEEE
International Conference on, pages 2196–2204. IEEE, 2017.
2

[2] Rich Caruana. Multitask learning. Machine learning,

28(1):41–75, 1997. 4

[3] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn
encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078, 2014. 5

[4] Hazel Doughty, Dima Damen, and Walterio Mayol-Cuevas.
Who’s better? who’s best? pairwise deep ranking for skill
determination. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018. 1, 2

[5] Hazel Doughty, Walterio Mayol-Cuevas, and Dima Damen.
The pros and cons: Rank-aware temporal attention for
skill determination in long videos.
arXiv preprint
arXiv:1812.05538, 2018. 2

[6] Sonal Gupta, Joohyun Kim, Kristen Grauman, and Ray-
mond Mooney. Watch, listen & learn: Co-training on cap-
tioned images and videos. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases,
pages 457–472. Springer, 2008. 3

[7] Sonal Gupta and Raymond Mooney. Using closed captions
to train activity recognizers that improve video retrieval. In
Proceedings of the CVPR-09 Workshop on Visual and Con-
textual Learning from Annotated Images and Videos (VCL),
Miami, FL, June 2009. 3

[8] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can
spatiotemporal 3d cnns retrace the history of 2d cnns and
imagenet? In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 6546–
6555, 2018. 4

[9] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In International Conference on Machine Learn-
ing, pages 448–456, 2015. 5

[10] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber,
Lhassane Idoumghar, and Pierre-Alain Muller. Evaluating
surgical skills from kinematic data using convolutional neu-
ral networks. In International Conference On Medical Image
Computing and Computer Assisted Intervention (MICCAI),
2018. 1

[11] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video
classiﬁcation with convolutional neural networks.
In Pro-
ceedings of the IEEE conference on Computer Vision and
Pattern Recognition, pages 1725–1732, 2014. 4

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 5

ference on Multimedia, pages 125–134. Springer, 2018. 1, 2,
4, 7

[14] Zhenqiang Li, Yifei Huang, Minjie Cai, and Yoichi Sato.
Manipulation-skill assessment from videos with spatial at-
tention network. arXiv preprint arXiv:1901.02579, 2019. 2
[15] Aiden Nibali, Zhen He, Stuart Morgan, and Daniel Green-
wood. Extraction and classiﬁcation of diving clips from
continuous video footage. In Computer Vision and Pattern
Recognition Workshops (CVPRW), 2017 IEEE Conference
on, pages 94–104. IEEE, 2017. 3, 5, 6

[16] Paritosh Parmar and Brendan Tran Morris. Learning to score
olympic events. In Computer Vision and Pattern Recognition
Workshops (CVPRW), 2017 IEEE Conference on, pages 76–
84. IEEE, 2017. 1, 2, 3, 4, 6, 7, 8

[17] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017. 5

[18] Hamed Pirsiavash, Carl Vondrick, and Antonio Torralba. As-
sessing the quality of actions. In European Conference on
Computer Vision, pages 556–571. Springer, 2014. 1, 2, 3, 7,
8

[19] Ariadna Quattoni, Michael Collins, and Trevor Darrell.
Learning visual representations using images with cap-
tions.
In Computer Vision and Pattern Recognition, 2007.
CVPR’07. IEEE Conference on, pages 1–8. IEEE, 2007. 3

[20] Ashish Sharma, Jatin Arora, Pritam Khan, Sidhartha Satap-
athy, Sumit Agarwal, Satadal Sengupta, Sankarshan Mridha,
and Niloy Ganguly. Commbox: Utilizing sensors for real-
time cricket shot identiﬁcation and commentary genera-
tion.
In Communication Systems and Networks (COM-
SNETS), 2017 9th International Conference on, pages 427–
428. IEEE, 2017. 3

[21] Rahul Anand Sharma, K Pramod Sankar, and CV Jawahar.
Fine-grain annotation of cricket videos.
In Pattern Recog-
nition (ACPR), 2015 3rd IAPR Asian Conference on, pages
421–425. IEEE, 2015. 3

[22] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402, 2012. 5, 8

[23] Mohak Kumar Sukhwani. Understanding and Describing
Tennis Videos. PhD thesis, International Institute of Infor-
mation Technology Hyderabad, 2016. 3

[24] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In Proceedings of the IEEE inter-
national conference on computer vision, pages 4489–4497,
2015. 2, 4, 5

[25] Vinay Venkataraman, Ioannis Vlachos, and Pavan K Turaga.

Dynamical regularity for action analysis. 1, 2

[26] Xiang Xiang, Ye Tian, Austin Reiter, Gregory D Hager, and
Trac D Tran. S3d: Stacking segmental p3d for action quality
assessment. In 2018 25th IEEE International Conference on
Image Processing (ICIP), pages 928–932. IEEE, 2018. 1, 2,
4, 7

[13] Yongjun Li, Xiujuan Chai, and Xilin Chen. End-to-end
learning for action quality assessment. In Paciﬁc Rim Con-

[27] Chengming Xu, Yanwei Fu, Bing Zhang, Zitian Chen, Yu-
Gang Jiang, and Xiangyang Xue. Learning to score the ﬁg-

312

ure skating sports videos. arXiv preprint arXiv:1802.02774,
2018. 2

[28] Fisher Yu and Vladlen Koltun.

aggregation by dilated convolutions.
arXiv:1511.07122, 2015. 5

Multi-scale context
arXiv preprint

[29] Huanyu Yu, Shuo Cheng, Bingbing Ni, Minsi Wang, Jian
Zhang, and Xiaokang Yang. Fine-grained video captioning
for sports narrative. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018. 2, 3

[30] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
image colorization. In European Conference on Computer
Vision, pages 649–666. Springer, 2016. 7

[31] Aneeq Zia and Irfan Essa. Automated surgical skill assess-
ment in rmis training. International journal of computer as-
sisted radiology and surgery, 13(5):731–739, 2018. 1

[32] Aneeq Zia, Yachna Sharma, Vinay Bettadapura, Eric L Sarin,
and Irfan Essa. Video and accelerometer-based motion
analysis for automated surgical skills assessment. Interna-
tional journal of computer assisted radiology and surgery,
13(3):443–455, 2018. 1

[33] Aneeq Zia, Yachna Sharma, Vinay Bettadapura, Eric L Sarin,
Thomas Ploetz, Mark A Clements, and Irfan Essa. Auto-
mated video-based assessment of surgical skills for training
and evaluation in medical schools. International journal of
computer assisted radiology and surgery, 11(9):1623–1636,
2016. 1, 2

313

