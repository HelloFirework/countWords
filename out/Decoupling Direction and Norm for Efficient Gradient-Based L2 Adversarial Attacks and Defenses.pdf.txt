Decoupling Direction and Norm for Efﬁcient Gradient-Based L2

Adversarial Attacks and Defenses

Jérôme Rony∗ 1

Luiz G. Hafemann∗ 1
Robert Sabourin1

Luiz S. Oliveira2
Eric Granger1

Ismail Ben Ayed1

1Laboratoire d’imagerie, de vision et d’intelligence artiﬁcielle (LIVIA), ÉTS Montreal, Canada

2Department of Informatics, Federal University of Paraná, Curitiba, Brazil

jerome.rony@gmail.com

luiz.gh@mailbox.org

lesoliveira@inf.ufpr.br

{ismail.benayed, robert.sabourin, eric.granger}@etsmtl.ca

Abstract

Research on adversarial examples in computer vision
tasks has shown that small, often imperceptible changes to
an image can induce misclassiﬁcation, which has security
implications for a wide range of image processing systems.
Considering L2 norm distortions, the Carlini and Wagner
attack is presently the most effective white-box attack in the
literature. However, this method is slow since it performs
a line-search for one of the optimization terms, and often
requires thousands of iterations. In this paper, an efﬁcient
approach is proposed to generate gradient-based attacks that
induce misclassiﬁcations with low L2 norm, by decoupling
the direction and the norm of the adversarial perturbation
that is added to the image. Experiments conducted on the
MNIST, CIFAR-10 and ImageNet datasets indicate that our
attack achieves comparable results to the state-of-the-art (in
terms of L2 norm) with considerably fewer iterations (as
few as 100 iterations), which opens the possibility of using
these attacks for adversarial training. Models trained with
our attack achieve state-of-the-art robustness against white-
box gradient-based L2 attacks on the MNIST and CIFAR-10
datasets, outperforming the Madry defense when the attacks
are limited to a maximum norm.

1. Introduction

Deep neural networks have achieved state-of-the-art per-
formances on a wide variety of computer vision applications,
such as image classiﬁcation, object detection, tracking, and
activity recognition [8]. In spite of their success in address-
ing these challenging tasks, they are vulnerable to active
adversaries. Most notably, they are susceptible to adver-
sarial examples1, in which adding small perturbations to an

∗Equal contribution.
1This also affects other machine learning classiﬁers, but we restrict our
analysis to CNNs, that are most commonly used in computer vision tasks.

image, often imperceptible to a human observer, causes a
misclassiﬁcation [2, 17].

Recent research on adversarial examples developed at-
tacks that allow for evaluating the robustness of models, as
well as defenses against these attacks. Attacks have been
proposed to achieve different objectives, such as minimizing
the amount of noise that induces misclassiﬁcation [5, 17], or
being fast enough to be incorporated into the training proce-
dure [7, 18]. In particular, considering the case of obtaining
adversarial examples with lowest perturbation (measured by
its L2 norm), the state-of-the-art attack has been proposed by
Carlini and Wagner (C&W) [5]. While this attack generates
adversarial examples with low L2 noise, it also requires a
high number of iterations, which makes it impractical for
training a robust model to defend against such attacks. In
contrast, one-step attacks are fast to generate, but using them
for training does not increase model robustness on white-box
scenarios, with full knowledge of the model under attack
[18]. Developing an attack that ﬁnds adversarial examples
with low noise in few iterations would enable adversarial
training with such examples, which could potentially in-
crease model robustness against white-box attacks.

Developing attacks that minimize the norm of the adver-
sarial perturbations requires optimizing two objectives: 1)
obtaining a low L2 norm, while 2) inducing a misclassiﬁca-
tion. With the current state-of-the-art method (C&W [5]),
this is addressed by using a two-term loss function, with the
weight balancing the two competing objectives found via an
expensive line search, requiring a large number of iterations.
This makes the evaluation of a system’s robustness very slow
and it is unpractical for adversarial training.

In this paper, we propose an efﬁcient gradient-based at-
tack called Decoupled Direction and Norm2 (DDN) that
induces misclassiﬁcation with a low L2 norm. This attack
optimizes the cross-entropy loss, and instead of penalizing

2Code available at https://github.com/jeromerony/fast_

adversarial.

4322

Figure 1: Example of an adversarial image on the ImageNet
dataset. The sample x is recognized as a Curly-coated re-
triever. Adding a perturbation δ we obtain an adversarial
image that is classiﬁed as a microwave (with kδk2 = 0.7).

the norm in each iteration, projects the perturbation onto
a L2-sphere centered at the original image. The change in
norm is then based on whether the sample is adversarial or
not. Using this approach to decouple the direction and norm
of the adversarial noise leads to an attack that needs signif-
icantly fewer iterations, achieving a level of performance
comparable to state-of-the-art, while being amenable to be
used for adversarial training.

A comprehensive set of experiments was conducted using
the MNIST, CIFAR-10 and ImageNet datasets. Our attack
obtains comparable results to the state-of-the-art while re-
quiring much fewer iterations (~100 times less than C&W).
For untargeted attacks on the ImageNet dataset, our attack
achieves better performance than the C&W attack, taking
less than 10 minutes to attack 1 000 images, versus over 35
hours to run the C&W attack.

Results for adversarial training on the MNIST and CIFAR-
10 datasets indicate that DDN can achieve state-of-the-art
robustness compared to the Madry defense [12]. These
models require that attacks use a higher average L2 norm to
induce misclassiﬁcations. They also obtain a higher accuracy
when the L2 norm of the attacks is bounded. On MNIST,
if the attack norm is restricted to 1.5, the model trained
with the Madry defense achieves 67.3% accuracy, while our
model achieves 87.2% accuracy. On CIFAR-10, for attacks
restricted to a norm of 0.5, the Madry model achieves 56.1%
accuracy, compared to 67.6% in our model.

2. Related Work

In this section, we formalize the problem of adversarial
examples, the threat model, and review the main attack and
defense methods proposed in the literature.

2.1. Problem Formulation

Let x be an sample from the input space X , with la-
bel ytrue from a set of possible labels Y. Let D(x1, x2)
be a distance measure that compares two input samples
(ideally capturing their perceptual similarity). P (y|x, θ)
is a model (classiﬁer) parameterized by θ. An example
˜x ∈ X is called adversarial (for non-targeted attacks)

against the classiﬁer if arg maxj P (yj|˜x, θ) 6= ytrue and
D(x, ˜x) ≤ ǫ, for a given maximum perturbation ǫ. A tar-
geted attack with a given desired class ytarget further requires
that arg maxj P (yj|˜x, θ) = ytarget. We denote as J(x, y, θ),
the cross-entropy between the prediction of the model for an
input x and a label y. Fig. 1 illustrates a targeted attack on
the ImageNet dataset, against an Inception v3 model [16].

In this paper, attacks are considered to be generated by a
gradient-based optimization procedure, restricting our analy-
sis to differentiable classiﬁers. These attacks can be formu-
lated either to obtain a minimum distortion D(x, ˜x), or to
obtain the worst possible loss in a region D(x, ˜x) ≤ ǫ. As an
example, consider that the distance function is a norm (e.g.,
L0, L2 or L∞), and the inputs are images (where each pixel’s
value is constrained between 0 and M). In a white-box sce-
nario, the optimization procedure to obtain an non-targeted
attack with minimum distortion δ can be formulated as:

min

δ

kδk subject to arg max

P (yj|x + δ, θ) 6= ytrue

j

and 0 ≤ x + δ ≤ M

(1)

With a similar formulation for targeted attacks, by changing
the constraint to be equal to the target class.

If the objective is to obtain the worst possible loss for a
given maximum noise of norm ǫ, the problem can be formu-
lated as:

P (ytrue|x + δ, θ) subject to kδk ≤ ǫ

min

δ

and 0 ≤ x + δ ≤ M

(2)

With a similar formulation for targeted attacks, by maximiz-
ing P (ytarget|x + δ, θ).

We focus on gradient-based attacks that optimize the L2
norm of the distortion. While this distance does not perfectly
capture perceptual similarity, it is widely used in computer
vision to measure similarity between images (e.g. comparing
image compression algorithms, where Peak Signal-to-Noise
Ratio is used, which is directly related to the L2 measure).
A differentiable distance measure that captures perceptual
similarity is still an open research problem.

2.2. Threat Model

In this paper, a white-box scenario is considered, also
known as a Perfect Knowledge scenario [2]. In this sce-
nario, we consider that an attacker has perfect knowledge
of the system, including the neural network architecture and
the learned weights θ. This threat model serves to evaluate
system security under the worst case scenario. Other sce-
narios can be conceived to evaluate attacks under different
assumptions on the attacker’s knowledge, for instance, no
access to the trained model, no access to the same training
set, among others. These scenarios are referred as black-box
or Limited-Knowledge [2].

4323

2.3. Attacks

2.4. Defenses

Several attacks were proposed in the literature, either
focusing on obtaining adversarial examples with a small δ
(Eq. 1) [5, 13, 17], or on obtaining adversarial examples in
one (or few) steps for adversarial training [7, 11].

L-BFGS. Szegedy et al. [17] proposed an attack for min-
imally distorted examples (Eq. 1), by considering the follow-
ing approximation:

min

δ

C kδk2 + log P (ytrue|x + δ, θ)

subject to 0 ≤ x + δ ≤ M

(3)

where the constraint x + δ ∈ [0, M ]n was addressed by us-
ing a box-constrained optimizer (L-BFGS: Limited memory
Broyden–Fletcher–Goldfarb–Shanno), and a line-search to
ﬁnd an appropriate value of C.

FGSM. Goodfellow et al. [7] proposed the Fast Gradi-
ent Sign Method, a one-step method that could generate
adversarial examples. The original formulation was devel-
oped considering the L∞ norm, but it has also been used to
generate attacks that focus on the L2 norm as follows:

˜x = x + ǫ

∇xJ(x, y, θ)
k∇xJ(x, y, θ)k

(4)

where the constraint ˜x ∈ [0, M ]n was addressed by simply
clipping the resulting adversarial example.

DeepFool. This method considers a linear approximation
of the model, and iteratively reﬁnes an adversary example by
choosing the point that would cross the decision boundary
under this approximation. This method was developed for
untargeted attacks, and for any Lp norm [13].

C&W. Similarly to the L-BFGS method, the C&W L2 at-
tack [5] minimizes two criteria at the same time – the pertur-
bation that makes the sample adversarial (e.g., misclassiﬁed
by the model), and the L2 norm of the perturbation. Instead
of using a box-constrained optimization method, they pro-
pose changing variables using the tanh function, and instead
of optimizing the cross-entropy of the adversarial example,
they use a difference between logits. For a targeted attack
aiming to obtain class t, with Z denoting the model output
before the softmax activation (logits), it optimizes:

min

δ hk˜x − xk2

2 + Cf (˜x)i

f (˜x) = max(max
i6=t

{Z(˜x)i} − Z(˜x)t, −κ)

(5)

where

and

˜x =

1
2

(tanh(arctanh(x) + δ) + 1)

where Z(˜x)i denotes the logit corresponding to the i-th class.
By increasing the conﬁdence parameter κ, the adversarial
sample will be misclassiﬁed with higher conﬁdence. To
use this attack in the untargeted setting, the deﬁnition of f
is modiﬁed to f (˜x) = max(Z(˜x)y − maxi6=y{Z(˜x)i}, −κ)
where y is the original label.

Developing defenses against adversarial examples is an
active area of research. To some extent, there is an arms race
on developing defenses and attacks that break them. Good-
fellow et al. proposed a method called adversarial training
[7], in which the training data is augmented with FGSM
samples. This was later shown not to be robust against it-
erative white-box attacks, nor black-box single-step attacks
[18]. Papernot et al. [14] proposed a distillation procedure to
train robust networks, which was shown to be easily broken
by iterative white-box attacks [5]. Other defenses involve
obfuscated gradients [1], where models either incorporate
non-differentiable steps (such that the gradient cannot be
computed) [4, 9], or randomized elements (to induce incor-
rect estimations of the gradient) [6, 19]. These defenses
were later shown to be ineffective when attacked with Back-
ward Pass Differentiable Approximation (BPDA) [1], where
the actual model is used for forward propagation, and the
gradient in the backward-pass is approximated. The Madry
defense [12], which considers a worst-case optimization, is
the only defense that has been shown to be somewhat robust
(on the MNIST and CIFAR-10 datasets). Below we provide
more detail on the general approach of adversarial training,
and the Madry defense.

Adversarial Training. This defense considers augment-
ing the training objective with adversarial examples [7], with
the intention of improving robustness. Given a model with
loss function J(x, y, θ), training is augmented as follows:

˜J(x, y, θ) = αJ(x, y, θ) + (1 − α)J(˜x, y, θ)

(6)

where ˜x is an adversarial sample. In [7], the FGSM is used
to generate the adversarial example in a single step. Tramèr
et al. [18] extended this method, showing that generating
one-step attacks using the model under training introduced
an issue. The model can converge to a degenerate solu-
tion where its gradients produce “easy” adversarial samples,
causing the adversarial loss to have a limited inﬂuence on
the training objective. They proposed a method in which
an ensemble of models is also used to generate the adver-
sarial examples ˜x. This method displays some robustness
against black-box attacks using surrogate models, but does
not increase robustness in white-box scenarios.

Madry Defense. Madry et al. [12] proposed a saddle
point optimization problem, optimizing for the worst case:

min

θ

p(θ)

where p(θ) = E(x,y)∼D(cid:2) max

δ∈S

J(x + δ, y, θ)(cid:3)

(7)

where D is the training set, and S indicates the feasible re-
gion for the attacker (e.g. S = {δ : kδk < ǫ}). They show
that Eq. 7 can be optimized by stochastic gradient descent
– during each training iteration, it ﬁrst ﬁnds the adversarial

4324

Figure 2: Histogram of the best C found by the C&W algo-
rithm with 9 search steps on the MNIST dataset.

example that maximizes the loss around the current training
sample x (i.e., maximizing the loss over δ, which is equiva-
lent to minimizing the probability of the correct class as in
Eq. 2), and then, it minimizes the loss over θ. Experiments
in Athalye et al. [1] show that it was the only defense not
broken under white-box attacks.

3. Decoupled Direction and Norm Attack

From the problem deﬁnition, we see that ﬁnding the worst
adversary in a ﬁxed region is an easier task. In Eq. 2, both
constraints can be expressed in terms of δ, and the resulting
equation can be optimized using projected gradient descent.
Finding the closest adversarial example is harder: Eq. 1 has
a constraint on the prediction of the model, which cannot
be addressed by a simple projection. A common approach,
which is used by Szegedy et al. [17] and in the C&W [5]
attack, is to approximate the constrained problem in Eq. 1
by an unconstrained one, replacing the constraint with a
penalty. This amounts to jointly optimizing both terms, the
norm of δ and a classiﬁcation term (see Eq. 3 and 5), with
a sufﬁciently high parameter C. In the general context of
constrained optimization, such a penalty-based approach
is a well known general principle [10]. While tackling an
unconstrained problem is convenient, penalty methods have
well-known difﬁculties in practice. The main difﬁculty is
that one has to choose parameter C in an ad hoc way. For
instance, if C is too small in Eq. 5, the example will not be
adversarial; if it is too large, this term will dominate, and
result in an adversarial example with more noise. This can be
particularly problematic when optimizing with a low number
of steps (e.g. to enable its use in adversarial training). Fig. 2
plots a histogram of the values of C that were obtained by
running the C&W attack on the MNIST dataset. We can
see that the optimum C varies signiﬁcantly among different
examples, ranging from 2−11 to 25. We also see that the
distribution of the best constant C changes whether we attack
a model with or without adversarial training (adversarially
trained models often require higher C). Furthermore, penalty
methods typically result in slow convergence [10].

Given the difﬁculty of ﬁnding the appropriate constant C

Algorithm 1 Decoupled Direction and Norm Attack
Input: x: original image to be attacked
Input: y: true label (untargeted) or target label (targeted)
Input: K: number of iterations
Input: α: step size
Input: γ: factor to modify the norm in each iteration
Output: ˜x: adversarial image
1: Initialize δ0 ← 0, ˜x0 ← x, ǫ0 ← 1
2: If targeted attack: m ← −1 else m ← +1
3: for k ← 1 to K do
4:
5:

g ← m∇˜xk−1 J(˜xk−1, y, θ)
g ← α g
kgk2

⊲ Step of size α in
the direction of g

6:
7:
8:
9:
10:
11:
12:

δk ← δk−1 + g
if ˜xk−1 is adversarial then

ǫk ← (1 − γ)ǫk−1

else

ǫk ← (1 + γ)ǫk−1

end if
˜xk ← x + ǫk

δk

kδkk2

⊲ Decrease norm

⊲ Increase norm

⊲ Project δk onto an
ǫk-sphere around x
⊲ Ensure ˜xk ∈ X

˜xk ← clip(˜xk, 0, 1)

13:
14: end for
15: Return ˜xk that has lowest norm k˜xk − xk2 and is adver-

sarial

(a) ˜xk not adversarial

(b) ˜xk adversarial

Figure 3: Illustration of an untargeted attack. The shaded
area denotes the region of the input space classiﬁed as ytrue.
In (a), ˜xk is still not adversarial, and we increase the norm
ǫk+1 for the next iteration, otherwise it is reduced in (b). In
both cases, we take a step g starting from the current point
˜x, and project back to an ǫk+1-sphere centered at x.

for this optimization, we propose an algorithm that does not
impose a penalty on the L2 norm during the optimization.
Instead, the norm is constrained by projecting the adversarial
perturbation δ on an ǫ-sphere around the original image x.
Then, the L2 norm is modiﬁed through a binary decision.
If the sample xk is not adversarial at step k, the norm is
increased for step k + 1, otherwise it is decreased.

4325

−12−8−4048log2(C)050100150200#ExamplesBaselineAdvtrainedWe also note that optimizing the cross-entropy may
present two other difﬁculties. First, the function is not
bounded, which can make it dominate in the optimization
of Eq. 3. Second, when attacking trained models, often the
predicted probability of the correct class for the original
image is very close to 1, which causes the cross entropy to
start very low and increase by several orders of magnitude
during the search for an adversarial example. This affects the
norm of the gradient, making it hard to ﬁnd an appropriate
learning rate. C&W address these issues by optimizing the
difference between logits instead of the cross-entropy. In
this work, the issue of it being unbounded does not affect the
attack procedure, since the decision to update the norm is
done on the model’s prediction (not on the cross-entropy). In
order to handle the issue of large changes in gradient norm,
we normalize the gradient to have unit norm before taking a
step in its direction.

The full procedure is described in Algorithm 1 and illus-
trated in Fig. 3. We start from the original image x, and
iteratively reﬁne the noise δk. In iteration k, if the current
sample ˜xk = x + δk is still not adversarial, we consider a
larger norm ǫk+1 = (1 + γ)ǫk. Otherwise, if the sample
is adversarial, we consider a smaller ǫk+1 = (1 − γ)ǫk. In
both cases, we take a step g (step 5 of Algorithm 1) from the
point ˜xk (red arrow in Fig. 3), and project it back onto an
ǫk+1-sphere centered at x (the direction given by the dashed
blue line in Fig. 3), obtaining ˜xk+1. Lastly, ˜xk+1 is projected
onto the feasible region of the input space X . In the case
of images normalized to [0, 1], we simply clip the value of
each pixel to be inside this range (step 13 of Algorithm 1).
Besides this step, we can also consider quantizing the image
in each iteration, to ensure the attack is a valid image.

It’s worth noting that, when reaching a point where the
decision boundary is tangent to the ǫk-sphere, g will have
the same direction as δk+1. This means that δk+1 will be
projected on the direction of δk. Therefore, the norm will
oscillate between the two sides of the decision boundary in
this direction. Multiplying ǫ by 1 + γ and 1 − γ will result
in a global decrease (on two steps) of the norm by 1 − γ2,
leading to a ﬁner search of the best norm.

4. Attack Evaluation

Experiments were conducted on the MNIST, CIFAR-10
and ImageNet datasets, comparing the proposed attack to the
state-of-the-art L2 attacks proposed in the literature: Deep-
Fool [13] and C&W L2 attack [5]. We use the same model
architectures and hyperparameters for training as in [5] for
MNIST and CIFAR-10 (see the supplementary material for
details). Our base classiﬁers obtain 99.44% and 85.51%
accuracy on the test sets of MNIST and CIFAR-10, respec-
tively. For the ImageNet experiments, we use a pre-trained
Inception V3 [16], that achieves 22.51% top-1 error on the
validation set. Inception V3 takes images of size 299×299

as input, which are cropped from images of size 342×342.
For experiments with DeepFool [13], we used the imple-
mentation from Foolbox [15], with a budget of 100 iterations.
For the experiments with C&W, we ported the attack (origi-
nally implemented on TensorFlow) on PyTorch to evaluate
the models in the frameworks in which they were trained.
We use the same hyperparameters from [5]: 9 search steps
on C with an initial constant of 0.01, with 10 000 iterations
for each search step (with early stopping) - we refer to this
scenario as C&W 9×10 000 in the tables. As we are in-
terested in obtaining attacks that require few iterations, we
also report experiments in a scenario where the number of
iterations is limited to 100. We consider a scenario of run-
ning 100 steps with a ﬁxed C (1×100), and a scenario of
running 4 search steps on C, of 25 iterations each (4×25).
Since the hyperparameters proposed in [5] were tuned for a
larger number of iterations and search steps, we performed a
grid search for each dataset, using learning rates in the range
[0.01, 0.05, 0.1, 0.5, 1], and C in the range [0.001, 0.01,
0.1, 1, 10, 100, 1 000]. We report the results for C&W with
the hyperparameters that achieve best Median L2. Selected
parameters are listed in the supplementary material.

For the experiments using DDN, we ran attacks with
budgets of 100, 300 and 1 000 iterations, in all cases, using
ǫ0 = 1 and γ = 0.05. The initial step size α = 1, was
reduced with cosine annealing to 0.01 in the last iteration.
The choice of γ is based on the encoding of images. For any
correctly classiﬁed image, the smallest possible perturbation
consists in changing one pixel by 1/255 (for images encoded
in 8 bit values), corresponding to a norm of 1/255. Since
we perform quantization, the values are rounded, meaning
that the algorithm must be able to achieve a norm lower than
1.5/255 = 3/510. When using K steps, this imposes:

ǫ0(1 − γ)K <

3
510

⇒ γ > 1 −(cid:18) 3

510 ǫ0(cid:19)

1
K

(8)

Using ǫ0 = 1 and K = 100 yields γ ≃ 0.05. Therefore, if
there exists an adversarial example with smallest perturba-
tion, the algorithm may ﬁnd it in a ﬁxed number of steps.

For the results with DDN, we consider quantized images
(to 256 levels). The quantization step is included in each
iteration (see step 13 of Algorithm 1). All results reported in
the paper consider images in the [0, 1] range.

Two sets of experiments were conducted: untargeted at-
tacks and targeted attacks. As in [5], we generated attacks
on the ﬁrst 1 000 images of the test set for MNIST and
CIFAR-10, while for ImageNet we randomly chose 1 000
images from the validation set that are correctly classiﬁed.
For the untargeted attacks, we report the success rate of the
attack (percentage of samples for which an attack was found),
the mean L2 norm of the adversarial noise (for successful
attacks), and the median L2 norm over all attacks while
considering unsuccessful attacks as worst-case adversarial

4326

Attack

C&W

DeepFool

DDN

C&W

DeepFool

DDN

C&W

DeepFool

DDN

T
S
I
N
M

0
1
-
R
A
F
I
C

t
e
N
e
g
a
m

I

Budget
4×25
1×100
9×10 000
100

100
300
1 000

4×25
1×100
9×10 000
100

100
300
1 000

4×25
1×100
9×10 000
100

100
300
1 000

Success Mean L2 Median L2
1.7400
1.6405
1.4121

1.7382
1.5917
1.3961

100.0
99.4
100.0

#Grads Run-time (s)
1.7
1.7
856.8

100
100
54 007

75.4

100.0
100.0
100.0

100.0
99.8
100.0

99.7

100.0
100.0
100.0

100.0
100.0
100.0

98.5

99.6
100.0
100.0

1.9685

1.4563
1.4357
1.4240

0.1924
0.1728
0.1543

0.1796

0.1503
0.1487
0.1480

1.5812
0.9858
0.4692

0.3800

0.3831
0.3749
0.3617

2.2909

1.4506
1.4386
1.4342

0.1541
0.1620
0.1453

0.1497

0.1333
0.1322
0.1317

1.3382
0.9587
0.3980

0.2655

0.3227
0.3210
0.3188

98

100
300
1 000

60
91
36 009

25

100
300
1 000

63
48
21 309

41

100
300
1 000

-

1.5
4.5
14.9

3.0
4.6
1 793.2

-

4.7
14.2
47.6

379.3
287.1
127 755.6

-

593.6
1 779.4
5 933.6

Table 1: Performance of our DDN attack compared to C&W [5] and DeepFool [13] attacks on MNIST, CIFAR-10 and
ImageNet in the untargeted scenario.

Attack

C&W 4×25
C&W 1×100
C&W 9×10 000
DDN 100
DDN 300
DDN 1 000

Average case

Success Mean L2
2.8254
2.0940
1.9481

96.11
86.89
100.00

100.00
100.00
100.00

1.9763
1.9577
1.9511

Least Likely

Success Mean L2
5.0090
2.6062
2.5370

69.9
31.7
100.0

100.0
100.0
100.0

2.6008
2.5503
2.5348

Table 2: Comparison of the DDN attack to the C&W L2
attack on MNIST.

Attack

C&W 4×25
C&W 1×100
C&W 9×10 000
DDN 100
DDN 300
DDN 1 000

Average case

Success Mean L2
0.3247
0.3104
0.2798

99.78
99.32
100.00

100.00
100.00
100.00

0.2925
0.2887
0.2867

Least Likely

Success Mean L2
0.5060
0.4159
0.3905

98.7
95.8
100.0

100.0
100.0
100.0

0.4170
0.4090
0.4050

Table 3: Comparison of the DDN attack to the C&W L2
attack on CIFAR-10.

(distance to a uniform gray image, as in [3]). We also report
the average number (for batch execution) of gradient com-
putations and the total run-times (in seconds) on a NVIDIA

Attack

C&W 4×25
C&W 1×100
C&W 9×10 000 [5]
DDN 100
DDN 300
DDN 1 000

Average case

Success Mean L2
4.2826
1.7718
0.96

99.13
96.74
100.00

99.98
100.00
100.00

1.0260
0.9021
0.8444

Least Likely

Success Mean L2
8.7336
2.2997
2.22

80.6
66.2
100.0

99.5
100.0
100.0

1.7074
1.3634
1.2240

Table 4: Comparison of the DDN attack to the C&W L2
attack on ImageNet. For C&W 9×10 000, we report the
results from [5].

GTX 1080 Ti with 11GB of memory. We did not report
run-times for the DeepFool attack, since the implementation
from foolbox generates adversarial examples one-by-one and
is executed on CPU, leading to unrepresentative run-times.
Attacks on MNIST and CIFAR-10 have been executed in a
single batch of 1 000 samples, whereas attacks on ImageNet
have been executed in 20 batches of 50 samples.

For the targeted attacks, following the protocol from [5],
we generate attacks against all possible classes on MNIST
and CIFAR-10 (9 attacks per image), and against 100 ran-
domly chosen classes for ImageNet (10% of the number of
classes). Therefore, in each targeted attack experiment, we
run 9 000 attacks on MNIST and CIFAR-10, and 100 000
attacks on ImageNet. Results are reported for two scenarios:
1) average over all attacks; 2) average performance when

4327

choosing the least likely class (i.e. choosing the worst attack
performance over all target classes, for each image). The
reported L2 norms are, as in the untargeted scenario, the
means over successful attacks.

Table 1 reports the results of DDN compared to the C&W
L2 and DeepFool attacks on the MNIST, CIFAR-10 and Ima-
geNet datasets. For the MNIST and CIFAR-10 datasets,
results with DDN are comparable to the state-of-the-art.
DDN obtains slightly worse L2 norms on the MNIST dataset
(when compared to the C&W 9×10 000), however, our at-
tack is able to get within 5% of the norm found by C&W
in only 100 iterations compared to the 54 007 iterations re-
quired for the C&W L2 attack. When the C&W attack is
restricted to use a maximum of 100 iterations, it always
performed worse than DDN with 100 iterations. On the Ima-
geNet dataset, our attack obtains better Mean L2 norms than
both other attacks. The DDN attack needs 300 iterations
to reach 100% success rate. DeepFool obtains close results
but fails to reach 100% success rate. It is also worth noting
that DeepFool seems to performs worse against adversarially
trained models (discussed in Section 6). Supplementary ma-
terial reports curves of the perturbation size against accuracy
of the models for the three attacks.

Tables 2, 3 and 4 present the results on targeted attacks on
the MNIST, CIFAR-10 and ImageNet datasets, respectively.
For the MNIST and CIFAR-10 datasets, DDN yields similar
performance compared to the C&W attack with 9×10 000
iterations, and always perform better than the C&W attack
when it is restricted to 100 iterations (we re-iterate that the
hyperparameters for the C&W attack were tuned for each
dataset, while the hyperparameters for DDN are ﬁxed for all
experiments). On the ImageNet dataset, DDN run with 100
iterations obtains superior performance than C&W. For all
datasets, with the scenario restricted to 100 iterations, the
C&W algorithm has a noticeable drop in success rate for
ﬁnding adversarial examples to the least likely class.

5. Adversarial Training with DDN

Since the DDN attack can produce adversarial examples
in relatively few iterations, it can be used for adversarial
training. For this, we consider the following loss function:

˜J(x, y, θ) = J(˜x, y, θ)

(9)

where ˜x is an adversarial example produced by the DDN
algorithm, that is projected to an ǫ-ball around x, such that
the classiﬁer is trained with adversarial examples with a
maximum norm of ǫ. It is worth making a parallel of this
approach with the Madry defense [12] where, in each itera-
tion, the loss of the worst-case adversarial (see Eq. 2) in an
ǫ-ball around the original sample x is used for optimization.
In our proposed adversarial training procedure, we optimize
the loss of the closest adversarial example (see Eq. 1). The

Defense

Attack

Attack
Success

Mean L2 Median L2

Model

Accuracy
at ǫ ≤ 1.5

Baseline

Madry
et al.

Ours

C&W 9×10 000
DeepFool 100
DDN 1 000

All

C&W 9×10 000
DeepFool 100
DDN 1 000

All

C&W 9×10 000
DeepFool 100
DDN 1 000

All

100.0
75.4
100.0
100.0

100.0
91.6
99.6
100.0

100.0
94.3
100.0
100.0

1.3961
1.9685
1.4240
1.3778

2.0813
4.9585
1.8436
1.6917

2.5181
3.9449
2.4874

2.4497

1.4121
2.2909
1.4342
1.3946

2.1071
5.2946
1.8994
1.8307

2.6146
4.1754
2.5781
2.5538

42.1
81.8
45.2
40.8

73.0
93.1
69.9
67.3

88.0
92.7
87.6

87.2

Table 5: Evaluation of the robustness of our adversarial
training on MNIST against the Madry defense.

intuition of this defense is to push the decision boundary
away from x in each iteration. We do note that this method
does not have the theoretical guarantees of the Madry de-
fense. However, since in practice the Madry defense uses
approximations (when searching for the global maximum
of the loss around x), we argue that both methods deserve
empirical comparison.

6. Defense Evaluation

We trained models using the same architectures as [5] for
MNIST, and a Wide ResNet (WRN) 28-10 [20] for CIFAR-
10 (similar to [12] where they use a WRN 34-10). As de-
scribed in Section 5, we augment the training images with
adversarial perturbations. For each training step, we run
the DDN attack with a budget of 100 iterations, and limit
the norm of the perturbation to a maximum ǫ = 2.4 on the
MNIST experiments, and ǫ = 1 for the CIFAR-10 experi-
ments. For MNIST, we train the model for 30 epochs with a
learning rate of 0.01 and then for 20 epochs with a learning
rate of 0.001. To reduce the training time with CIFAR-10,
we ﬁrst train the model on original images for 200 epochs
using the hyperparameters from [20]. Then, we continue
training for 30 more epochs using Eq. 9, keeping the same
ﬁnal learning rate of 0.0008. Our robust MNIST model has
a test accuracy of 99.01% on the clean samples, while the
Madry model has an accuracy of 98.53%. On CIFAR-10,
our model reaches a test accuracy of 89.0% while the model
by Madry et al. obtains 87.3%.

We evaluate the adversarial robustness of the models us-
ing three untargeted attacks: Carlini 9×10 000, DeepFool
100 and DDN 1 000. For each sample, we consider the small-
est adversarial perturbation produced by the three attacks
and report it in the “All” row. Tables 5 and 6 report the
results of this evaluation with a comparison to the defense of
Madry et al. [12]3 and the baseline (without adversarial train-
ing) for CIFAR-10. For MNIST, the baseline corresponds

3Models taken from https://github.com/MadryLab

4328

Defense

Attack

Attack
Success

Mean L2 Median L2

Model

Accuracy
at ǫ ≤ 0.5

Baseline

WRN 28-10

Madry
et al.

WRN 34-10

Ours

WRN 28-10

C&W 9×10 000
DeepFool 100
DDN 1 000

All

C&W 9×10 000
DeepFool 100
DDN 1 000

All

C&W 9×10 000
DeepFool 100
DDN 1 000

All

100.0
99.3
100.0
100.0

100.0
95.6
100.0
100.0

100.0
99.7
100.0

100.0

0.1343
0.5085
0.1430
0.1282

0.6912
1.4856
0.6732
0.6601

0.8860
1.5298
0.8688

0.8597

0.1273
0.4241
0.1370
0.1222

0.6050
0.9576
0.5876
0.5804

0.8254
1.1163
0.8177

0.8151

0.2
38.3
0.1
0.1

57.1
64.7
56.9
56.1

67.9
69.9
68.0

67.6

Table 6: Evaluation of the robustness of our adversarial
training on CIFAR-10 against the Madry defense.

Figure 4: Models robustness on MNIST (left) and CIFAR-10
(right): impact on accuracy as we increase the maximum
perturbation ǫ.

to the model used in Section 4. We observe that for attacks
with unbounded norm, the attacks can successfully generate
adversarial examples almost 100% of the time. However, an
increased L2 norm is required to generate attacks against the
model trained with DDN.

Fig. 4 shows the robustness of the MNIST and CIFAR-
10 models respectively for different attacks with increasing
maximum L2 norm. These ﬁgures can be interpreted as
the expected accuracy of the systems in a scenario where
the adversary is constrained to make changes with norm
L2 ≤ ǫ. For instance on MNIST, if the attacker is limited
to a maximum norm of ǫ = 1.5, the baseline performance
decreases to 40.8%; Madry to 67.3% and our defense to
87.2%. At ǫ = 2.0, baseline performance decreases to 9.2%,
Madry to 38.6% and our defense to 74.8%. On CIFAR-10,
if the attacker is limited to a maximum norm of ǫ = 0.5, the
baseline performance decreases to 0.1%; Madry to 56.1%
and our defense to 67.6%. At ǫ = 1.0, baseline performance
decreases to 0%, Madry to 24.4% and our defense to 39.9%.
For both datasets, the model trained with DDN outperforms
the model trained with the Madry defense for all values of ǫ.
Fig. 5 shows adversarial examples produced by the DDN
1 000 attack for different models on MNIST and CIFAR-10.
On MNIST, adversarial examples for the baseline are not
meaningful (the still visually belong to the original class),

Figure 5: Adversarial examples with varied levels of noise δ
against three models: baseline, Madry defense [12] and our
defense. Text on top-left of each image indicate kδk2; text
on bottom-right indicates the predicted class4.

whereas some adversarial examples obtained for the adversar-
ially trained model (DDN) actually change classes (bottom
right: 0 changes to 6). For all models, there are still some
adversarial examples that are very close to the original im-
ages (ﬁrst column). On CIFAR-10, while the adversarially
trained models require higher norms for the attacks, most
adversarial examples still perceptually resemble the original
images. In few cases (bottom-right example for CIFAR-10),
it could cause a confusion: it can appear as changing to class
1 - a (cropped) automobile facing right.

7. Conclusion

We presented the Decoupled Direction and Norm attack,
which obtains comparable results with the state-of-the-art
for L2 norm adversarial perturbations, but in much fewer
iterations. Our attack allows for faster evaluation of the
robustness of differentiable models, and enables a novel
adversarial training, where, at each iteration, we train with
examples close to the decision boundary. Our experiments
with MNIST and CIFAR-10 show state-of-the-art robustness
against L2-based attacks in a white-box scenario.

The methods presented in this paper were used in NIPS
2018 Adversarial Vision Challenge [3], ranking ﬁrst in untar-
geted attacks, and third in targeted attacks and robust models
(both attacks and defense in a black-box scenario). These
results highlight the effectiveness of the defense mechanism,
and suggest that attacks using adversarially-trained surrogate
models can be effective in black-box scenarios, which is a
promising future direction.

Acknowledgements

We thank Marco Pedersoli and Christian Desrosiers for
their insightful feedback. This research was supported by the
Fonds de recherche du Quebec - Nature et technologies, Nat-
ural Sciences and Engineering Research Council of Canada,
and CNPq grant 206318/2014-6.

4For CIFAR-10: 1: automobile, 2: bird, 3: cat, 5: dog, 8: ship, 9: truck.

4329

01234L2normofthenoise020406080100%AccuracyBaselineMadryOurs0.00.51.01.52.02.5L2normofthenoise020406080100BaselineMadryOurs[17] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. In International Conference on Learning Repre-
sentations, 2014.

[18] F. Tramèr, A. Kurakin, N. Papernot, D. Boneh, and P. Mc-
Daniel. Ensemble Adversarial Training: Attacks and De-
fenses. In International Conference on Learning Representa-
tions, 2018.

[19] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille. Mitigating
Adversarial Effects Through Randomization. In International
Conference on Learning Representations, 2018.

[20] S. Zagoruyko and N. Komodakis. Wide residual networks. In
Proceedings of the British Machine Vision Conference, pages
87.1–87.12, 2016.

References

[1] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients
give a false sense of security: Circumventing defenses to ad-
versarial examples. In Proceedings of the 35th International
Conference on Machine Learning, volume 80, pages 274–283,
2018.

[2] B. Biggio and F. Roli. Wild patterns: Ten years after the
rise of adversarial machine learning. Pattern Recognition,
84:317–331, Dec. 2018.

[3] W. Brendel, J. Rauber, A. Kurakin, N. Papernot, B. Veliqi,
M. Salathé, S. P. Mohanty, and M. Bethge. Adversarial vision
challenge. arXiv:1808.01976, 2018.

[4] J. Buckman, A. Roy, C. Raffel, and I. Goodfellow. Thermome-
ter Encoding: One Hot Way To Resist Adversarial Examples.
In International Conference on Learning Representations,
2018.

[5] N. Carlini and D. Wagner. Towards evaluating the robustness
In IEEE Symposium on Security and

of neural networks.
Privacy (SP), pages 39–57, 2017.

[6] G. S. Dhillon, K. Azizzadenesheli, Z. C. Lipton, J. Bern-
stein, J. Kossaiﬁ, A. Khanna, and A. Anandkumar. Stochastic
Activation Pruning for Robust Adversarial Defense. In Inter-
national Conference on Learning Representations, 2018.

[7] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
Harnessing Adversarial Examples. In International Confer-
ence on Learning Representations, 2015.

[8] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai,
T. Liu, X. Wang, G. Wang, J. Cai, and T. Chen. Recent ad-
vances in convolutional neural networks. Pattern Recognition,
77:354–377, May 2018.

[9] C. Guo, M. Rana, M. Cissé, and L. van der Maaten. Coun-
tering Adversarial Images using Input Transformations. In
International Conference on Learning Representations, 2018.
[10] P. A. Jensen and J. F. a. Bard. Operations Research Models

and Methods. Wiley, 2003.

[11] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial exam-
ples in the physical world. In International Conference on
Learning Representations (workshop track), 2017.

[12] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.
Towards Deep Learning Models Resistant to Adversarial At-
tacks. International Conference on Learning Representations,
2018.

[13] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool:
a simple and accurate method to fool deep neural networks.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2574–2582, 2016.

[14] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami.
Distillation as a defense to adversarial perturbations against
deep neural networks. In IEEE Symposium on Security and
Privacy (SP), pages 582–597, 2016.

[15] J. Rauber, W. Brendel, and M. Bethge. Foolbox: A python
toolbox to benchmark the robustness of machine learning
models. arXiv:1707.04131, 2017.

[16] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2818–2826, 2016.

4330

