Attentive Region Embedding Network for Zero-shot Learning

Guo-Sen Xie1, Li Liu1, Xiaobo Jin3, Fan Zhu1, Zheng Zhang2, Jie Qin1, Yazhou Yao1, Ling Shao1

1Inception Institute of Artiﬁcial Intelligence, UAE

2University of Queensland, Australia

3Xi’an Jiaotong-Liverpool University, China

Abstract

Zero-shot learning (ZSL) aims to classify images from
unseen categories, by merely utilizing seen class images
as the training data. Existing works on ZSL mainly lever-
age the global features or learn the global regions, from
which, to construct the embeddings to the semantic space.
However, few of them study the discrimination power im-
plied in local image regions (parts), which, in some sense,
correspond to semantic attributes, have stronger discrim-
ination than attributes, and can thus assist the semantic
transfer between seen/unseen classes. In this paper, to dis-
cover (semantic) regions, we propose the attentive region
embedding network (AREN), which is tailored to advance
the ZSL task. Speciﬁcally, AREN is end-to-end trainable
and consists of two network branches, i.e., the attentive
region embedding (ARE) stream, and the attentive com-
pressed second-order embedding (ACSE) stream. ARE is
capable of discovering multiple part regions under the guid-
ance of the attention and the compatibility loss. Moreover,
a novel adaptive thresholding mechanism is proposed for
suppressing redundant (such as background) attention re-
gions. To further guarantee more stable semantic transfer
from the perspective of second-order collaboration, ACSE
is incorporated into the AREN. In the comprehensive evalu-
ations on four benchmarks, our models achieve state-of-the-
art performances under ZSL setting, and compelling results
under generalized ZSL setting.

1. Introduction

Zero-shot learning (ZSL) [29, 1, 21, 46] is proposed for
solving challenging classiﬁcation tasks, wherein, the label
spaces for the training set and test set are disjoint from each
other, and there are no training samples (zero-shot) for test
categories. Most recently, for traditional recognition sys-
tems [16, 23, 33, 32, 31, 47], two issues hinder their ad-
vancements, i.e., 1) annotating large-scale samples is both

Tiger

Zebra

Seen

Seen

Bobcat

Leopard

Unseen

Unseen

0

1

0

0

Semantic vectors
0

0

1

1

0

1

1

1

0

0

1

0

0

1

1

1

1

0

1

1

1

0

0

1

Semantic space

Ground-truth semantic 
vectors in semantic space

Attributes:

longleg

Spots

hooves

stripes

claws

big

tail

(a) Semantic vector generation

Attentive regions

Correctly classified

Zebra

Bobcat

Tiger

Leopard

Wrongly classified

 

X
Tiger
=
Tiger
X

Unseen image test

Tiger

Zebra

Bobcat

Leopard

 
Bobcat
≠
Leopard
 

Unseen image test
(c) Attentive Region Embedding

(b) Direct Embedding
Figure 1. (a) Toy example of generating semantic vectors; cross-
es indicate ground-truth semantic vectors in the semantic space.
Four animals all have the attribute “tail” (shown in red dashed
box), as such, “tail” is misleading the afterward classiﬁcation. (b)
Circles represent samples in the semantic space by direct embed-
ding (DE) images to this space. For DE, Bobcat and Leopard are
wrongly recognized as Tiger. (c) Squares are these samples in
the semantic space embedded by ARE; in this case, by preserv-
ing the discrimination from the part level, the confused unseen
images (Bobcat and Leopard) can be well distinguished and cor-
rectly recognized. Best viewed in color.

time consuming and expensive [52], and 2) new categories
are constantly emerging [50], and some of them are difﬁ-
cult (or even dangerous) to be collected, e.g., the identiﬁed
cofﬁnﬁsh in the deep-sea. In contrast, ZSL has the intrinsic
advantages of tackling the image annotation and novel class
recognition problems, which makes it a hot topic in recent
years.

To transfer semantic knowledge for images from two dis-
joint category spaces, the semantic description of each cat-
egory (for both seen/unseen classes), as the high-level side
information, is key for accomplishing ZSL. Widely-used

9384

side information includes attributes [11], word vectors [39],
sentences [36], and gaze [18], among which, attributes draw
the greatest attention, and are adopted in this paper. A gen-
eral scenario for ZSL is to ﬁnd an embedding space based
on seen images. Typically, the semantic space (e.g., the s-
pace in Fig. 1(a), which is spanned by quantized attributes,
i.e., semantic vectors) [37, 13, 20, 1, 3, 36, 44, 39, 8, 42, 35,
27, 40], the image feature space [4, 26, 45, 26], and the la-
tent intermedium space [41] usually serve as the embedding
space. In that space, to further distinguish unseen images,
nearest neighbour search is used to match the tested image
representation with that of unseen class prototypes, i.e., se-
mantic vector w.r.t. each unseen class.

Most leading ZSL methods, whether end-to-end convo-
lutional neural network (CNN) based [27, 42, 22], or deep
feature-based approaches [19, 51, 26, 53, 54, 34], empha-
size on learning the embedding between the (learned) glob-
al image (or feature) and the counterpart semantic vector.
However, all these methods are actually based on global
projection of the whole image. After acquiring the seman-
tic vectors w.r.t. seen/unseen classes, there exist two draw-
backs for these global projection methods: 1) due to the
subtle difference of the seen image (tiger) and unseen im-
ages (bobcat, leopard) in the global feature space, they are
neighbors (circles in Fig. 1(b)) in the projected semantic s-
pace, where, it is hard to distinguish them; 2) the annotated
ground-truth semantic vectors of bobcat, leopard and tiger
are extremely similar (Fig. 1(a)). It is thus hard, by feed-
ing the global features to the embedding model, to learn a
desirable projection for matching the input similar images
with their confused semantic vectors. In contrast, ARE can
ﬁt input images with their confused ground-truth seman-
tic vectors pretty well (in Fig. 1(c), squares are near their
ground-truth, i.e., crosses).

Since the high-level abstractions of some image regions
can lead to the attribute concept [10], and in order to alle-
viate the above problems, we resort to the regions (parts)
in the images. We observe that 1) besides the global im-
age representation, properly discovered regions account for
better knowledge transfer from seen to unseen class, and 2)
some regions can capture local appearance differences for
the same attribute concept, e.g., the region blocks of dif-
ferent tails are different in appearance. In this sense, part
regions are more discriminative than the corresponding at-
tribute. Therefore, projecting region representation into the
semantic space can preserve more such local differences.
In this way, tiger, bobcat, zebra, and leopard can be well
recognized from each other (Fig. 1(c), squares). In term of
discriminative feature learning, the part based feature has
long been established as a powerful one [12, 25]. Moti-
vated by the above observations, to facilitate the seman-
tic transfer between seen/unseen images in the part level,
we propose an end-to-end attentive region embedding net-

work (AREN) (Fig. 2) for ZSL. To sum up, our contribu-
tions are:

1) An attention mechanism is leveraged to automatical-
ly discover semantic/discriminative regions (parts), without
any part detection or annotation. Moreover, a novel adap-
tive thresholding mechanism is further proposed to suppress
redundant attentive regions and introduce robustness, there-
fore leading to the attentive region embedding (ARE) sub-
net. This is the ﬁrst attempt to introduce attention to ZS-
L/GZSL freely, without any part detection/annotation.

2) To capture second-order appearance differences col-
laboratively with different attentive regions, an attentive
compressed second-order embedding (ACSE) is further in-
corporated into the AREN framework. This is the ﬁrst
time second-order statistics have been explored within ZS-
L/GZSL.

3) Integrating ARE and ACSE together yields the end-to-
end AREN framework, which is trained with the guidance
of a compatibility loss with frozen classiﬁer weights (taken
from the seen class attributes).

2. Related Works

(Generalized) Zero-shot Learning. As the pioneering
work of ZSL, Lampert et al. [21] propose direct attribute
prediction (DAP) model, which ﬁrst learns the attribute
classiﬁers, and then calculates the posterior of a test class
for a given image. However, DAP neglects the association-
s between different attributes. To mitigate the unreliabili-
ty of the individually learned attribute classiﬁers, a random
forest solution [17] is advocated. As a whole, the leading
methods for ZSL are the embedding based ones equipped
with the compatibility loss, which can well associate the
images and their attributes. Speciﬁcally, Akata et al. [1]
proposed ALE, where a bilinear-style hinge loss is lever-
aged. LATEM [44] was then introduced to incorporate non-
linearity to the model. Other embedding based approach-
es include DEVICE [13], SJE [3], CMT [39], ESZSL [37],
SAE [20], and DEM [52]; for a more detailed description of
them, refer to [46]. Most of the methods mentioned above
adopt deep features and emphasize the model itself, thus
resulting in relatively inferior ZSL performances. Most re-
cently, another branch of approaches, i.e., end-to-end train-
able CNN models, have been proposed. The most repre-
sentatives of these train CNN model by 1) alleviating pre-
diction bias, i.e., QSFL [42], 2) gradually zooming global
image objects, i.e., LDF [22], and 3) automatically learning
the relations, i.e., RN [50]. However, none of them focus on
the part (region) level for enhancing the semantic transfer in
ZSL. By expanding the search label space to also consider
seen classes during testing, ZSL becomes Generalized ZS-
L (GZSL). All ZSL methods can be adopted to solve GZSL
task by obeying the data splits proposed in [46].

Attention. Attention [49], widely used and extensive-

9385

(cid:94)(cid:286)(cid:286)(cid:374)(cid:3)(cid:47)(cid:373)(cid:258)(cid:336)(cid:286)(cid:400)

(cid:17)(cid:258)(cid:272)(cid:364)(cid:271)(cid:381)(cid:374)(cid:286)

(cid:11)(cid:32) (cid:37)(cid:11) (cid:12)(cid:12)x

Z

(cid:12)(cid:42) Z

(cid:11)

(cid:44) (cid:42) Z

(cid:11)

(cid:11)

x

(cid:100)(cid:349)(cid:336)(cid:286)(cid:396)

(cid:127)(cid:286)(cid:271)(cid:396)(cid:258)

(cid:38)(cid:286)(cid:258)(cid:410)(cid:437)(cid:396)(cid:286)(cid:3)(cid:68)(cid:258)(cid:393)(cid:3)(cid:449)(cid:349)(cid:410)(cid:346)(cid:3)
(cid:127)(cid:286)(cid:396)(cid:381)(cid:882)(cid:286)(cid:367)(cid:286)(cid:373)(cid:286)(cid:374)(cid:410)(cid:400)

(cid:48)

(cid:115)(cid:286)(cid:272)(cid:410)(cid:381)(cid:396)(cid:3)(cid:68)(cid:258)(cid:454)(cid:3)(cid:87)(cid:381)(cid:381)(cid:367)

(cid:69)(cid:381)(cid:396)(cid:373)(cid:258)(cid:367)(cid:3)(cid:38)(cid:286)(cid:258)(cid:410)(cid:437)(cid:396)(cid:286)(cid:3)(cid:68)(cid:258)(cid:393)

(cid:54)

(cid:94)(cid:286)(cid:272)(cid:381)(cid:374)(cid:282)(cid:882)(cid:381)(cid:396)(cid:282)(cid:286)(cid:396)(cid:3)(cid:87)(cid:381)(cid:381)(cid:367)

(cid:4)(cid:410)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:448)(cid:286)(cid:3)(cid:38)(cid:286)(cid:258)(cid:410)(cid:437)(cid:396)(cid:286)(cid:3)(cid:68)(cid:258)(cid:393)

(cid:38) (cid:18)(cid:381)(cid:374)(cid:272)(cid:258)(cid:410)(cid:286)(cid:374)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:39)(cid:68)(cid:87)

(cid:39)(cid:367)(cid:381)(cid:271)(cid:258)(cid:367)(cid:3)(cid:68)(cid:258)(cid:454)(cid:3)(cid:87)(cid:381)(cid:381)(cid:367)

(cid:40)

(cid:94)(cid:286)(cid:373)(cid:258)(cid:374)(cid:410)(cid:349)(cid:272)(cid:3)(cid:28)(cid:373)(cid:271)(cid:286)(cid:282)(cid:282)(cid:349)(cid:374)(cid:336)

(cid:4)(cid:282)(cid:258)(cid:393)(cid:410)(cid:349)(cid:448)(cid:286)(cid:3)(cid:100)(cid:346)(cid:396)(cid:286)(cid:400)(cid:346)(cid:381)(cid:367)(cid:282)(cid:349)(cid:374)(cid:336)

(cid:28)(cid:367)(cid:286)(cid:373)(cid:286)(cid:374)(cid:410)(cid:882)(cid:449)(cid:349)(cid:400)(cid:286)(cid:3)(cid:87)(cid:396)(cid:381)(cid:282)(cid:437)(cid:272)(cid:410)

(cid:12)(cid:12)

(cid:36)(cid:53)(cid:40)(cid:11) (cid:12)
x
v

(cid:36)(cid:53)(cid:40)y

(cid:40)

(cid:38)

(cid:39)(cid:68)(cid:87)

(cid:4)(cid:90)(cid:28)

(cid:36)(cid:53)(cid:40)(cid:36)

(cid:18)(cid:381)(cid:373)(cid:393)(cid:258)(cid:410)(cid:349)(cid:271)(cid:349)(cid:367)(cid:349)(cid:410)(cid:455)(cid:3)

(cid:62)(cid:381)(cid:400)(cid:400)

1(cid:40)

2(cid:40)

(cid:17)(cid:17)(cid:17)

(cid:17)
(cid:17)
(cid:17)

(cid:17)
(cid:17)
(cid:17)

(cid:17)
(cid:17)
(cid:17)

(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)

(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)

(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)

(cid:80)(cid:68)(cid:86)(cid:78)(cid:86)

K(cid:40)

(cid:4)(cid:410)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:448)(cid:286)(cid:3)(cid:90)(cid:286)(cid:336)(cid:349)(cid:381)(cid:374)(cid:3)(cid:24)(cid:349)(cid:400)(cid:272)(cid:381)(cid:448)(cid:286)(cid:396)(cid:455)(cid:3)(cid:68)(cid:381)(cid:282)(cid:437)(cid:367)(cid:286)

(cid:94)(cid:286)(cid:373)(cid:258)(cid:374)(cid:410)(cid:349)(cid:272)(cid:3)(cid:94)(cid:393)(cid:258)(cid:272)(cid:286)

(cid:20)P

KP

(cid:36)(cid:38)(cid:54)(cid:40)(cid:11) (cid:12)
x
v

(cid:21)P

(cid:54)

(cid:48)

(cid:40)

y

(cid:36)(cid:38)(cid:54)E

(cid:4)(cid:18)(cid:94)(cid:28)

(cid:36)(cid:38)(cid:54)(cid:40)(cid:36)

(cid:18)(cid:381)(cid:373)(cid:393)(cid:258)(cid:410)(cid:349)(cid:271)(cid:349)(cid:367)(cid:349)(cid:410)(cid:455)(cid:3)

(cid:62)(cid:381)(cid:400)(cid:400)

(cid:1005)

(cid:1005)

(cid:1005)

(cid:11)(cid:32) (cid:37)(cid:11) (cid:12)(cid:12)x

Z

cpsZ

(cid:94)(cid:286)(cid:272)(cid:381)(cid:374)(cid:282)(cid:286)(cid:396)(cid:882)(cid:381)(cid:396)(cid:282)(cid:286)(cid:396)(cid:3)(cid:448)(cid:286)(cid:272)(cid:410)(cid:381)(cid:396)(cid:400)

(cid:94)(cid:286)(cid:373)(cid:258)(cid:374)(cid:410)(cid:349)(cid:272)(cid:3)(cid:94)(cid:393)(cid:258)(cid:272)(cid:286)

Figure 2. The architecture of the proposed Attentive Region Embedding Network (AREN) model, and the upper branch is ARE, meanwhile,
the bottom branch is ACSE. For ARE: the input image x is ﬁrst fed into the backbone net, thus generating the last convolutional feature
map Z which undergoes the attentive region discovery module, and the K attentive feature maps Γk, k = 1, 2, · · · , K is produced. Then,
AT is applied to them. Global max pooling, concatenation and embedding to semantic space are further conducted. For ACSE: Z is ﬁrst
compressed by 1 × 1 convolution, leading to Zcps. Zcps and attentive feature maps after AT are used to construct the second-order vectors,
then vector max pooling and ebmbedding to semantic space are leveraged. Finally, compatibility losses are used for ARE/ACSE training.

ly studied in recent years, has been successfully applied to
various ﬁelds such as visual question answering [48] and
semantic segmentation [9]. Inspired by the above achieve-
ments, an attention mechanism is incorporated into our
AREN framework, with the guidance of associating images
and their attributes, i.e., the function of the compatibility
loss.
In this way, the learned attention maps can capture
multiple semantic regions that are useful for semantic trans-
fer between seen/unseen images.

Transductive setting: In addition to seen images, utiliz-
ing the unseen images (without labels) during the training
phase yields the transductive setting for ZSL/GZSL [14]. In
this paper, our focus is inductive ZSL/GZSL, i.e., the most
general setting in the realistic scenario.

3. Methodology

i , ys

i ), i = 1, 2, · · · , Ns} is given, where xs

Notations. Suppose that the training set (seen classes)
S = {(xs
i ∈ X S
is the i-th data samples (totally Ns samples) with a corre-
sponding seen class label as ys
i ∈ Y S , and Y S is
the label set of seen classes. In ZSL, given the testing set
U = {(xu
i ), i = 1, 2, · · · , Nu} from the unseen classes,
where xu
i ∈ Y U is the i-th unseen sample and
its label, respectively; the seen and unseen label sets are dis-
joint, i.e., Y S ∩ Y U = ∅. Moreover, we denote the seman-
tic vector (class prototype) set w.r.t. seen/unseen classes as

i , yu
i ∈ X U and yu

i , here ys

i }Cs

i }Cu

i /au

i=1 and {au

i=1, herein, as

i ∈ RQ is the semantic
{as
vector corresponding to the i-th seen/unseen class. Cs/Cu is
the category number for seen/unseen classes, and Q is the
dimension of the semantic vector, and also the dimension of
the semantic space. The difference for ZSL and GZSL [46]
lies in that, for GZSL, while testing the seen/unseen image
xtest

, its predicted label set is Y = Y S ∪ Y U .

j

3.1. The Attentive Region Embedding Network

The Attentive Region Embedding Network (AREN) is
illustrated in Fig. 2, which consists of two branches, i.e.,
the Attentive Region Embedding (ARE) and the Atten-
tive Compressed Second-order Embedding (ACSE). In par-
ticular, ARE is the main body for capturing discrimina-
tive regions automatically, without any part-level annota-
tion/detection. Meanwhile, ACSE is targeted at grasping
more subtle semantic information by second-order infer-
ence. To achieve the ZSL, in both ARE and ACSE, the em-
bedding to semantic space is leveraged, which is the com-
monly utilized strategy [42, 22, 52, 27] in the end-to-end
deep network based ZSL framework.

3.1.1 Attentive Region Embedding

In Fig. 2 (upper stream), the last convolutional feature
map Z of the backbone (e.g., ResNet101) for input im-

9386

age x, is fed into the ARE, which ﬁrst undergoes the At-
tentive Region Discovery (ARD) module, followed by an
adaptive thresholding (AT) procedure. In this way, the at-
tention regions can be effectively focused and highlighted.
The AT operation can further purify the generated attention
regions by ﬁltering out the ones with low attentive strength,
thus generating some feature maps with all zero elements.
Afterward, unlike the widely-used global average pooling,
we leverage the global maximum pooling for these feature
maps and then concatenate them, which leads to the repre-
sentation vARE(x) for an image x in the region space. Specif-
ically, we formulate vARE(x) as follows:

vARE(x) = G(T (R(Z))), Z = B(x),

(1)

where B, R, T , and G are the backbone network opera-
tion, the ARD operation, the AT operation, and the GM-
P/concatenation operation, respectively. The model param-
eters in Eqn. (1) are omitted to ease reading.

Due to our AT operation, some segments of the cascaded
region vector vARE will be all zeros. As validated in the ex-
periments of subsection 4.5, vARE can achieve an improved
performance over its counterpart without an AT operation.
Finally, to accomplish the ZSL/GZSL task, vARE is embed-
ded into the semantic space. The projected representation
yARE in the semantic space for x is deﬁned as

yARE = E(vARE).

(2)

The parameters in Eqn. (1) and (2) are jointly trained
with the guidance of a cross-entropy-like compatibility
function[22, 1, 42, 3, 27] (we will revisit the compatibili-
ty loss in detail in subsection 3.2).

Attentive Region Discovery Module: To discover the
multiple attentive regions of an input image x, which serve
as the bridge for semantic transfer from the part level, we
leverage the attention mechanism to automatically learn
to focus. With the supervision of high-level semantic at-
tributes (from compatibility loss) in the topmost layer of the
net, we hope that the discovered regions can match with the
annotated semantic attributes. In this way, the yielded re-
gions are essentially communicating between seen/unseen
classes, e.g., a child has heard the description of “zebra” as
looking like a “horse” with black-white stripes; then when
she sees the picture of “zebra”, by focusing on the black-
white stripy regions, she can tell it’s a “zebra”. In this sec-
tion, we will elaborate on the ARD module, i.e., mapping
R : Z → R(Z), where, Z is the last convolutional feature
map of the backbone net. Z is a 3D tensor, and we suppose
Z ∈ RH×W ×C , where C, H, and W are the size of the
channel, height, and width, respectively. Let z(h, w, c) ∈ R
be the response value in location (h, w) of the c-th channel
from Z. We further denote the desirable region number as
K, where, desirable means that 1) the number of regions is
discriminative for distinguishing seen/unseen images; and

2) some of these regions are matched with the semantic at-
tributes, e.g., the region of leg matches the attribute “leg”.
Inspired by the application of attention models to various
ﬁelds, such as image captioning [49], we employ the atten-
tion mechanism to the ZSL ﬁeld as well, with the aim of
grasping the semantic regions and further narrowing the se-
mantic gap between seen/unseen images.

Speciﬁcally, by taking Z as input, we generate K 2-

dimensional masks Mk ∈ RH×W , (k = 1, 2, · · · , K):

Mk = MMaskGeneratek (Z),

(3)

where MMaskGeneratek (·) is a mask generation operation
which is implemented by convolution on Z followed by the
Sigmoid thresholding. Thus, the value mk(h, w) in location
(h, w) of Mk can reﬂect the strength that location (h, w) of
Z falls into the k-th region. Furthermore, suppose the k-
th attentive convolutional feature map is Γk ∈ R(Z). In
particular, Γk is obtained by

Γk = OReshape(Mk) ⊗ Z.

(4)

In Eqn. (4), OReshape(·) reshapes the input to be the same
size as that of Z, ⊗ indicates an element-wise product.

Adaptive Thresholding: After the ARD, the generat-
ed K attentive maps usually have redundancy, such as the
background noises. To purify these maps, we propose the
AT operation. AT takes these K attentive feature maps (in
Eqn. (4)) as inputs, calculates the maximum value of each
2D mask map (Mk), yielding the maximum value vector
mv ∈ RK×1 w.r.t. these K mask maps. Then, the maxi-
mum value of mv is achieved, denoted as AT max:

AT max = max
16k6K

mv(k).

(5)

AT max is the global maximum value of these K attention
mask maps in Eqn. (3). An adaptive coefﬁcient α (0 6 α 6
1) is introduced, based on which, we denote the ﬁnal thresh-
olding bound as TB = α × AT max. To this end, if the k-th
value in mv is less than TB, the corresponding attentive
feature map Γk will be set as all zero elements. Throughout
the paper, for a given ﬁxed K, there is only one parameter α
to be tuned. Experimental evaluation shows improvements
in performance by setting a proper value for α.

3.1.2 Attentive Compressed Second-order Embedding

In Fig. 2, after acquiring the last convolutional feature
map Z and the K puriﬁed attentive feature maps (from the
ARE), i.e., T (Γk), k = 1, 2, · · · , K, with some of them
having all zero-elements, we resort to second-order pool-
ing [24] to alleviate the semantic gap between seen/unseen
images. We ﬁrst compress Z by a 1×1 convolution. The re-
sulting compressed feature map Zcps has Ncps(=20 through-
out this paper) channels.
In this way, the resulting com-
pressed second-order representation will be compact and
efﬁciently trainable.

9387

For each T (Γk), the second-order pooling with Zcps
yields the k-th second-order representation Pk, which may
be equal to all zero vector due to the attentive mechanism.
Pk is formulated as:

Pk = Zcps ⊚ T (Γk),

(6)

where ⊚ is the seconder-order operation [24] between two
input matrices. A vector maximum pooling is utilized to
pool these K vectors, thus generating the ﬁnal ACSE repre-
sentation vACSE(x) for the input image x.

Similarly, vACSE(x) is embedded into the semantic space
to achieve ZSL/GZSL. The projected representation yACSE
implies the second-order statistics for better semantic trans-
fer. To the best of our knowledge, this is the ﬁrst time that
second-order representation is incorporated into ZSL.

3.2. The Compatibility Loss

In this section, we discuss the problem of embedding to
the semantic space, which is the most utilized strategy for
making ZSL extendable. In general, the ZSL task formu-
lates a mapping (prediction) function f : X S 7→ Y from
the (seen) training set, as follows:

f (x, W ) = arg max
y∈Y

F (x, y; W ).

(7)

Given the trained parameters W , the function in Eqn. (7)
is used to predict an unseen image xu. To associate the
visual and semantic information, the score function F (·),
parameterized by W , is typically formulated as the bilinear
compatibility function [1, 13, 37, 3, 22, 42, 27, 46]:

F (x, y; W ) = θ(x)W φ(y),

(8)

where θ(x) and φ(y) are the visual embedding of image x
and the semantic embedding of label y, respectively.

In the context of the proposed ARE and ACSE, vARE(x)
and vACSE(x) serve as the visual embeddings of input image
x, i.e., Eqn. (8) can be reformulated as follows:

FARE(x, y; ΘARE, WARE) = vARE(x)T
FACSE(x, y; ΘACSE, WACSE) = vACSE(x)T

WAREay∗,
WACSEay∗,

(9)

ARE and Θ

where Θ
ACSE are the whole learnable parameters
w.r.t. vARE(x) and vACSE(x) respectively, WARE and WACSE are
the embedding parameters for mapping vARE(x) and vACSE(x)
to the semantic embedding ay∗, which is the L-2 normal-
ized semantic vector w.r.t. class y.

We further denote the normalized attribute matrix w.r.t.
, and the class out-

all these Cs seen classes as A ∈ RQ×Cs
puts of image x on the ﬁnal layer of ARE and ACSE are

ACSE, WACSE) in
To learn all these parameters (Θ
Eqn. (10) in an end-to-end manner, i.e., to train the proposed
AREN (Fig. 2), the loss function is

ARE, WARE, Θ

L = λ1LARE + λ2LACSE,

(11)

where λ1 and λ2 are trade-off parameters. LARE and LACSE
are speciﬁed as

LARE =

LACSE =

1
Ns

1
Ns

Ns
X

i=1

Ns
X

i=1

L(OARE(xs

i ; ΘARE, WARE), ys

i ),

(12)

L(OACSE(xs

i ; ΘACSE, WACSE), ys

i ),

where L is some classiﬁcation loss.
In this paper, cross-
entropy (CE) loss is used. Compared with traditional CE
loss, the difference lies in that the weights of the CE loss
layer are frozen as A and are ﬁxed without updating dur-
ing the training phase. In this way, the attribute matrix A
can guide the attentive region discovery, and progressively
project the input image to the direction of its semantic rep-
resentation. To this end, we term the designed two stream
loss function L as compatibility loss.

3.3. Prediction

In the AREN framework, the tested unseen image can be
projected into the semantic space by ARE/ACSE, enabling
it to perform a separate prediction.
Prediction by ARE: A test image xu can be projected into
the semantic space, thus resulting in the ARE representation
φARE(xu) (= W T
AREvARE(xu)). To predict the class label, the
location of the maximum compatibility score can be chosen
as the predicted label:

yu∗ = arg max
c∈Y U

φARE(xu)Tau
c .

(13)

Prediction by ACSE: Similarly, suppose the ACSE repre-
ACSEvACSE(xu)). The pre-
sentation of xu is φACSE(xu) (= W T
dicted class label is:

yu∗ = arg max
c∈Y U

φACSE(xu)Tau
c .

(14)

Combining ARE and ACSE: After obtaining the ARE and
ACSE representations of xu, i.e., φARE(xu) and φACSE(xu),
we ﬁrst calculate their combined vector, and then predict
the label in the same way as Eqn. (13) / Eqn. (14):

yu∗ = arg max
c∈Y U

(γ1φARE(xu)T + γ2φACSE(xu)T)au
c .

(15)

4. Experiments

4.1. Datasets and Settings

OARE(x; ΘARE, WARE) = AT
OACSE(x; ΘACSE, WACSE) = AT

W

T

AREvARE(x),

W

T

ACSEvACSE(x),

(10)

Four widely used ZSL datasets,

i.e., CUB [43],
AWA2 [46], SUN [30], and APY [11], are employed to

9388

validate the proposed AREN. Speciﬁcally, CUB contain-
s a total of 11,788 bird images from 200 classes, each
of which has a 312D continuous semantic vector. We
use the standard split (SS) and the proposed split (PS) of
150/50 (seen/unseen) for evaluation, as done in [46]. AWA2
is an extension of AWA, whose images cannot be accessed.
As such, we adopt AWA2, which includes 37,322 images of
animals from 50 classes, among which 40/10 (seen/unseen)
splits under SS/PS settings are evaluated, an 85D semantic
vector is associated with each class. SUN is a scene image
dataset, consisting of 14,340 images from 717 categories.
SS/PS splits of 645/72 for seen/unseen classes are lever-
aged, and a 102D continuous semantic vector is constructed
for each class. APY, with a total of 15,339 images, contains
32 categories with 64D attribute, and the seen/unseen splits
are 20/12, evaluated under SS/PS settings. As in [46], af-
ter obtaining the AREN model, we conduct both ZSL and
GZSL evaluations under the PS setting, only ZSL evalua-
tion under the SS setting, for all four datasets.

4.2. Training Details and Parameters

For fair comparison with the published approaches, [46]
reproduced nearly all leading methods using the 2,048D
ResNet101 features. As such, the backbone net in Fig. 2
is taken as the ResNet101 net [16].

As with the initial pre-training on the ImageNet
dataset [38], the input image size for these four datasets
is 224×224. Therefore, the size of the last convolution-
al feature map Z for ResNet101 is 2048 × 7 × 7. For
each dataset, the AREN is trained for 100 epochs with an
initial learning rate selected from [0.0001, 0.003] (which is
robust). The parameter (λ1, λ2) is ﬁxed as (0.5, 0.5) dur-
ing the training of AREN, while, when the ARE and ACSE
are trained separately, it is taken as (1, 0) and (0, 1), re-
spectively, to ensure that only their own loss function con-
tributes to the gradient updating.
In the ARE, the num-
ber K of the part regions is experientially selected from
{k ∈ N+|4 6 k 6 12}, and the AT parameter α is se-
lected from {0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. In the ACSE, the
compressed channel number Ncps is set to 20.

When testing unseen images, as for the separate test-
ing of the ARE and ACSE, the combination coefﬁcien-
t (γ1, γ2) (in Eqn. (15)) is set to (1, 0) and (0, 1), respec-
tively. Meanwhile, for the jointly trained AREN, (γ1, γ2) is
set to (0.5, 0.5) to achieve the fused matching results.

4.3. Evaluations in ZSL setting

We compare our proposed methods against current state-
of-the-art models, on all four aforementioned ZSL datasets,
under SS/PS settings [46]. Average Class Accuracy (ACA)
is adopted as the evaluation metric. Table 1 presents the ex-
perimental results, from which, it can be concluded that i)
ARE, ACSE and AREN consistently outperform the com-

Table 1. ZSL results (ACA, in %) on evaluated four datasets. Our
methods and most of the compared methods use ResNet101 as
the backbone net for fair comparisons. SS = Standard Split, PS =
Proposed Split. The best result is marked in red, the second best
in blue, and the third best in bold.

Method

DAP [1]
IAP [1]
CONSE [28]
CMT [39]
SSE [53]
LATEM [44]
ALE [2]

† DEVISE [13]

SJE [3]
ESZSL [37]
SYNC [5]
SAE [20]
PSR [4]
SCoRe[27]∗
QFSL− [20]
‡ DEM [52]∗
LDF [22]∗
SP-AEN [8]
RN [50]
UDA [19]
TMV [14]
SMS [15]
QFSL [42]
ARE

♮

♭ ACSE
AREN

CUB

SUN

AWA2

APY

SS

PS

SS

PS

SS

PS

SS

PS

37.5
27.1
36.7
37.3
43.7
49.4
53.2
53.2
55.3
55.1
54.1
33.4
–
59.5
58.5
–
67.1
–
–
39.5
51.2
59.2
69.7
70.2
69.0
70.7

40.0
24.0
34.3
34.6
43.9
49.3
54.9
52.0
53.9
53.9
55.6
33.3
56.0
–
58.8
51.7
–
55.4
55.6
–
–
–
72.1
72.5
71.5
71.8

38.9
17.4
44.2
41.9
54.5
56.9
59.1
57.5
57.1
57.3
59.1
42.4
–
–
58.9
–
–
–
–
–
61.4
60.5
61.7
60.8
61.5
61.7

39.9
19.4
38.8
39.9
51.5
55.3
58.1
56.5
53.7
54.5
56.3
40.3
61.4
–
56.2
40.3
–
59.2
–
–
–
–
58.3
59.0
59.7
60.6

58.7
46.9
67.9
66.3
67.5
68.7
80.3
68.6
69.5
75.6
71.2
80.7
–
–
72.6
–
83.4
–
–
–
–
–
84.8
86.3
86.5
86.7

46.1
35.9
44.5
37.9
61.0
55.8
62.5
59.7
61.9
58.6
46.6
54.1
63.8
–
63.5
67.1
–
–
64.2
–
–
–
79.7
66.9
65.2
67.9

35.2
22.4
25.9
26.9
31.1
34.5
30.9
35.4
32.0
34.4
39.7
8.3
–
–
–
–
–
–
–
–
–

–
44.0
43.5
44.1

33.8
36.6
26.9
28.0
34.0
35.2
39.7
39.8
32.9
38.3
23.9
8.3
38.4
–
–
35.0
–
24.1
–
–
–

–
35.5
38.7
39.2

† : Inductive & ResNet101 feature based methods.
‡ : Inductive & End-to-end trainable CNN based methods.
♮ : Transductive.
♭ : Proposed & Inductive & ResNet101 as backbone, end-to-end trainable.
∗ : Indicates that ResNet101 is not used as backbone net.

pared counterparts by a large margin, under both SS/PS set-
tings. For example, ARE achieves 72.5% on CUB under
the PS setting, which has improved the ACA up to 17%,
compared with the recently proposed RN method whose A-
CA is only 55.6%. ii) For some datasets, the jointly trained
AREN model performs slightly worse than the separately
trained ARE and ACSE. The reasons lie in that 1) the co-
efﬁcient (λ1, λ2) of the loss function in Eqn. (11) and the
prediction coefﬁcient (γ1, γ2) in Eqn. (15) are only roughly
set, and, thus, may not lead to the best optimized model;
and 2) the separate models (ARE and ACSE) are powerful
enough, and the joint training disturbs their discrimination.
iii) Most importantly, under the inductive setting, we are on
par with and have even surpassed some of the leading trans-
ductive methods (such as QFSL).

4.4. Evaluations in GZSL Setting

To evaluate the GZSL, the searched label space for a giv-
en test image is enlarged to include both unseen (Y U ) and
the seen classes (Y S). Under the PS setting [46], the test
images come from both seen and unseen classes. To begin
with, we present the evaluation protocol for GZSL. Suppose
that the ACA for the testing samples from the unseen class-
es is ACAY U , and meanwhile, ACAY S for testing samples

9389

Table 2. GZSL results (in %) in PS setting; our methods and most of the compared methods are taking ResNet101 as the backbone net for
fair comparisons. ts = ACA on Y U , tr=ACA on Y S , and H = harmonic mean. The best number is marked in bold.

Method

DAP [1]
IAP [1]

CONSE [28]

CMT [39]
SSE [53]

†

LATEM [44]

ALE [2]

DEVISE [13]

SJE [3]

ESZSL [37]
SYNC [5]
SAE [20]
PSR [4]

DEM [52]∗
QFSL [42]

RN [50]

ARE
ACSE
AREN

ARE+CS♦
ACSE+CS♦
AREN+CS♦

‡

♭

♭

CUB

SUN

AWA2

APY

ts

tr

H

ts

tr

H

ts

tr

H

ts

tr

H

1.7
0.2
1.6
7.2
8.5
15.2
23.7
23.8
23.5
12.6
11.5
7.8
24.6
19.6
33.3
38.1
38.4
34.6
38.9
61.3
61.3
63.2

67.9
72.8
72.2
49.8
46.9
57.3
62.8
53.0
59.2
63.8
70.9
54.0
54.3
57.9
48.1
61.1
76.4
80.1
78.7
66.6
68.4
69.0

3.3
0.4
3.1
12.6
14.4
24.0
34.4
32.8
33.6
21.0
19.8
13.6
33.9
29.2
39.4
47.0
51.2
48.4
52.1
63.8
64.7
66.0

4.2
1.0
6.8
8.1
2.1
14.7
21.8
16.9
14.7
11.0
7.9
8.8
20.8
20.5
30.9

–

19.0
15.2
19.0
41.7
36.8
40.3

25.1
37.8
39.9
21.8
36.4
28.8
33.1
27.4
30.5
27.9
43.3
18.0
37.2
34.3
18.5

–

29.3
28.8
38.8
35.2
34.9
32.3

7.2
1.8
11.6
11.8
4.0
19.5
26.3
20.9
19.8
15.8
13.4
11.8
26.7
25.6
23.1

–

23.1
19.9
25.5
38.2
35.8
35.9

0.0
0.9
0.5
0.5
8.1
11.5
14.0
17.1
8.0
5.9
10.0
1.1
20.7
30.5
52.1
30.0
17.5
18.2
15.6
55.6
53.5
54.7

84.7
87.6
90.6
90.0
82.5
77.3
81.8
74.7
73.9
77.8
90.5
82.2
73.8
86.4
72.8
93.4
93.2
92.9
92.9
79.8
79.2
79.1

0.0
1.8
1.0
1.0
14.8
20.0
23.9
27.8
14.4
11.0
18.0
2.2
32.3
45.1
60.7
45.3
29.5
30.4
26.7
65.5
63.9
64.7

4.8
5.7
0.0
1.4
0.2
0.1
4.6
4.9
3.7
2.4
7.4
0.4
13.5
11.1

–
–

11.6
9.6
9.2
28.0
30.8
30.0

78.3
65.6
91.2
85.2
78.9
73.0
73.7
76.9
55.7
70.1
66.3
80.9
51.4
75.1

–
–

75.3
76.5
76.9
53.7
50.8
47.9

9.0
10.4
0.0
2.8
0.4
0.2
8.7
9.2
6.9
4.6
13.3
0.9
21.4
19.4

–
–

20.1
17.1
16.4
36.8
38.3
36.9

†: Inductive & ResNet101 feature based methods. ‡: End-to-end trainable CNN based methods. ♭: Proposed & Inductive &
ResNet101 as backbone, end-to-end trainable. ∗: Indicates that ResNet101 is not used as backbone. ♦: CS, i.e., Calibrated
Stacking [6], means reducing the prediction scores for the seen classes.

CUB:PS

CUB:SS

from the seen classes. Their Harmonic mean H can then
be calculated as H =
. To this end, the
harmonic mean H is taken as the main evaluation criterion
for our models under the GZSL setting.

YU ×ACA
YU

YU +ACA

2×ACA

ACA

YS

ACAY U (ts), ACAY S (tr), and their harmonic mean H
for the evaluated datasets are listed in Table 2. From Ta-
ble 2, we can draw the following conclusions: i) On CUB,
AWA2, and APY datasets, the proposed methods without
calibrated stacking (CS) in H are comparable to/better than
current state-of-the-art methods. ii) Our initial results typ-
ically achieve a high tr, but a low ts, which indicates that
calibrated stacking [6] is needed. As shown in the last three
rows, after the CS operation, the H mean, tr and ts become
the best in most cases. iii) the overall AREN model, w/o a
CS operation, shows a lower performance than the separate
ARE and ACSE models, for some datasets. This is likely
for the same reasons as in the ZSL model.

4.5. Ablation Study

In the following, the CUB and AWA2 datasets are taken

as examples for ablation analysis.

Coefﬁcients in loss function. For the AREN model,
there exist two parameters, i.e., λ1 and λ2, in Eqn. (11). By
varying their values from {0.1, 0.5, 1.0, 1.5, 2.0} and ﬁxing
other parameters as defaults, we run different models for
10 epochs and produce the ACA maps w.r.t. λ1 and λ2 un-
der SS/PS settings for ZSL. The changing tendency of ACA
w.r.t. (λ1, λ2), overall, is stable and consistent (Fig. 3).

AT coefﬁcient

in ARE. To observe the inﬂu-
the AT coefﬁcient α for K ﬁxed atten-
ence of
tive maps, we conduct experiments varying α from

73

)

%

(
 

A
C
A

60

0.1

)

%

(
 

A
C
A

72

60

0.5

λ

2

1.0

1.5

2.0

2.0

1.0

1.5

λ

1

0.5

0.1

0.1

0.5

λ

2

1.0

1.5

2.0

0.5

0.1

1.0
λ

1

2.0

1.5

Figure 3. The ACA-(λ1, λ2) maps on CUB under SS/PS settings.

)

%

(
 

A
C
A

)

%

(
 

A
C
A

73

72

CUB:PS

71

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

68

67

AWA2:PS

66

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

71

70

69

)

%

(
 

A
C
A

68

CUB:SS

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

)

%

(
 

A
C
A

84

83.5

83

AWA2:SS

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

Figure 4. The ACA-α curves under ZSL setting.

{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, where α =
0 indicates ARE without AT, α = 1.0 means the strongest
AT is added (only the attentive map with largest activate
value is preserved, while all other maps are set to zero-
elements), and if α > 1.0, ARE becomes un-trainable. ZSL
results under SS/PS settings are illustrated in Fig. 4. The
curves show that improvements in ACA are achieved, which

9390

CUB:SS ACSE-w
CUB:SS ACSE-w/o

10

20

50
N

cps

100

200

500

)

%

(
 

A
C
A

)

%

(
 

A
C
A

70

68

66
1

64

62

60

60

40

20

0
1

90

88

86

84

)

%

(
 

A
C
A

10

20

CUB:PS ACSE-w
CUB:PS ACSE-w/o
50
200
N

100

cps

500

)

%

(
 

A
C
A

AWA2:PS ACSE-w
AWA2:PS ACSE-w/o
100

20

50
N

cps

AWA2:SS ACSE-w
AWA2:SS ACSE-w/o
200

100

50
N

cps

1

10

200

500

1

10

20

500

Figure 5. The ACA-Ncps curves of ZSL setting w/w/o attention.

well conﬁrms the effectiveness of the AT mechanism.

Channel compression in ACSE. We present the trend
of ACA w.r.t. varying values of Ncps over a discrete val-
ue range of {1, 10, 20, 50, 100, 200, 500}. ZSL results are
shown in Fig. 5. We can see that the ACSE with smal-
l values of Ncps achieves better ACA results in nearly all
cases. In particular, we achieve an accuracy of 89.2% for
AWA2:SS, under Ncps = 50. Only see ACSE-w (ACSE
with attention) curves for comparison. Attention in ACSE.
In Fig. 2, the Zcps from the ACSE and K attentive fea-
ture maps from ARE are used collaboratively to gener-
ate a second-order vector. We further observe two cases,
i.e., 1) the current ACSE with attention (ACSE-w), and
2) the ACSE without attention (ACSE-w/o), i.e., second-
order vector is obtained by Zcps ⊚ Zcps. For fair com-
parisons, we make the dimensions of the ﬁnal vectors for
ACSE-w and ACSE-w/o (almost) the same. To reuse the
results in Fig. 5 from ACSE-w, we vary the values of Ncps
from Υ = {46, 144, 203, 320, 453, 640, 1012} for ACSE-
w/o attention, thus making the dimensions of the generated
vectors from the two cases approximately the same. From
Fig. 5, it can be concluded that ACSE-w is consistently bet-
ter than ACSE-w/o.

Global versus part features: The global model is
trained by taking the original ResNet101 with its fully con-
nected layer as the backbone, followed by the projection to
semantic space and the same compatibility loss as ours. The
ACAs of the global baseline (GB), ARE, and ACSE are list-
ed in Table 3, which shows signiﬁcant improvements have
been made by our models.

4.6. Visualization

The ARE models with PS split are used to visualize
what the learned regions look like, and unseen images from
AWA2 and CUB are considered (Fig. 7). Based on the
above ten mv values for each image, ATmax (Eqn. (5)) is
obtained, and TB is thus acquired by multiplying α with
ATmax, e.g., for “horse”, let α = 0.8, only six attention
maps (in red rectangle boxes) are reserved, the discarded
four masks are backgrounds w.r.t. the sky. To this end, the

Table 3. ZSL results (ACA, in %) of global/part features.

Method

GB
ARE
ACSE

CUB

AWA2

SS

60.2
70.2
69.0

PS

62.7
72.5
71.5

SS

81.7
86.3
86.5

PS

60.3
66.9
65.2

AT mechanism is automatically suppressing the backgroud
noises. Moreover, global objects and semantic parts are ad-
dressed by these learned masks, e.g., the 4-th mask of “mal-
lard” corresponds to “whole body”, and the 1-st mask of “k-
ingbird” fucuses on “head”. ARE/ACSE models of PS split-
s are further used to visualize the distribution of the unseen
test images on AWA2 by t-SNE visualization [55] (Fig. 6).

ARE 

ACSE

Figure 6. t-SNE visualization of unseen class images on AWA2.

horse

0.98

0.53

0.89

0.50

0.83

0.98

0.85

0.58

0.60

0.88

dolphin 0.82

0.58

0.78

0.85
(a) AWA2:PS, unseen images

0.92

0.61

0.56

0.56

0.76

0.74

kingbird 0.56

0.27

0.38

0.86

1.00

0.34

0.14

0.33

0.89

0.65

mallard 0.87

0.27

0.62

0.80

1.00
0.19
(b) CUB:PS, unseen images

0.20

0.91

1.00

0.21

Figure 7. In both (a) and (b), (attention) masks in red rectangle are
selected by AT mechanism. For each row, the ﬁrst one is the input
image, the left ones are its ten attentive feature masks, the number
below is the maximum value mv within the mask.

5. Conclusions

An attentive region embedding network (AREN) is pro-
posed for solving the challenging ZSL/GZSL task, which
consists of two branches, i.e., the upper stream attentive
region embedding (ARE) and the bottom stream (atten-
tion guided) compressed second-order embedding (ACSE).
Both ARE and ACSE are embedded into the semantic s-
pace, where ZSL/GZSL is conducted through nearest neigh-
bor matching. An adaptive thresholding (AT) is also in-
corporated into the ARE. Actually, the AT can also be ap-
plied to many other general tasks which requires an atten-
tion mechanism, such as visual question answering. Inte-
grating ARE and ACSE together leads to the AREN mod-
el, which has achieved some new state-of-the-art results for
both ZSL and GZSL, on the standard benchmarks.

9391

References

[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid.
Label-embedding for attribute-based classiﬁcation. In
CVPR, 2013. 1, 2, 4, 5, 6, 7

[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid.
Label-embedding for image classiﬁcation. In TPAMI,
2016. 6, 7

[3] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele.
Evaluation of output embeddings for ﬁne-grained im-
age classiﬁcation. In CVPR, 2015. 2, 4, 5, 6, 7

[4] Y. Annadani and S. Biswas. Preserving semantic re-
lations for zero-shot learning. In CVPR, 2018. 2, 6,
7

[5] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha.
In

Synthesized classiﬁers for zero-shot learning.
CVPR, 2016. 6, 7

[6] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An
empirical study and analysis of generalized zero-shot
learning for object recognition in the wild. In ECCV,
2016. 7

[7] Y. Long, L. Liu, L. Shao, F. Shen, G. Ding, and J. Han.
From zero-shot learning to conventional supervised
classiﬁcation: Unseen visual data synthesis. In CVPR,
2017. 2

[8] L. Chen, H. Zhang, J. Xiao, W. Liu, and S.-F.
Chang. Zero-shot visual recognition using semantics-
preserving adversarial embedding network. In CVPR,
2018. 2, 6

[9] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille.
Attention to scale: Scale-aware semantic image seg-
mentation. In CVPR, 2016. 3

[10] M. Elhoseiny, Y. Zhu, H. Zhang, and A. M. Elgam-
mal. Link the head to the” beak”: Zero shot learning
from noisy text description at part precision. In CVPR,
2017. 2

[15] Y. Guo, G. Ding, X. Jin, and J. Wang. Transductive
zero-shot recognition via shared model space learning.
In AAAI, 2016. 6

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual

learning for image recognition. In CVPR, 2016. 1, 6

[17] D. Jayaraman and K. Grauman. Zero-shot recognition

with unreliable attributes. In NeurIPS, 2014. 2

[18] N. Karessli, Z. Akata, B. Schiele, A. Bulling, et al.
Gaze embeddings for zero-shot image classiﬁcation.
In CVPR, 2017. 2

[19] E. Kodirov, T. Xiang, Z. Fu, and S. Gong. Unsuper-
vised domain adaptation for zero-shot learning. In IC-
CV, 2015. 2, 6

[20] E. Kodirov, T. Xiang, and S. Gong. Semantic autoen-
In CVPR, 2017. 2, 6,

coder for zero-shot learning.
7

[21] C. H. Lampert, H. Nickisch, and S. Harmeling. Learn-
ing to detect unseen object classes by between-class
attribute transfer. In CVPR, 2009. 1, 2

[22] Y. Li, J. Zhang, J. Zhang, and K. Huang. Discrimina-
tive learning of latent features for zero-shot recogni-
tion. In CVPR, 2018. 2, 3, 4, 5, 6

[23] G. Xie, X. Zhang, S. Yan, and C. Liu. SDE: A novel
selective, discriminative and equalizing feature repre-
sentation for visual recognition. In IJCV, 2017. 1

[24] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear
cnn models for ﬁne-grained visual recognition. In IC-
CV, 2015. 4, 5

[25] G.-S. Xie, X.-Y. Zhang, W. Yang, M. Xu, S. Yan, and
C.-L. Liu. LG-CNN: From local parts to global dis-
crimination for ﬁne-grained recognition. In PR, 2017.
2

[26] Y. Long, L. Liu, F. Shen, L. Shao, and X. Li. Zero-shot
learning using synthesised unseen visual data with d-
iffusion regularisation. In TPAMI, 2017. 2

[11] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. De-
scribing objects by their attributes. In CVPR, 2009. 2,
5

[27] P. Morgado and N. Vasconcelos. Semantically consis-
tent regularization for zero-shot recognition. In CVPR,
2017. 2, 3, 4, 5, 6

[12] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. Object detection with discriminatively
trained part-based models. In TPAMI, 2010. 2

[13] A. Frome, G. S. Corrado, J. Shlens, S. Bengio,
J. Dean, T. Mikolov, et al. Devise: A deep visual-
semantic embedding model. In NeurIPS, 2013. 2, 5,
6, 7

[28] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer,
J. Shlens, A. Frome, G. S. Corrado, and J. Dean. Zero-
shot learning by convex combination of semantic em-
beddings. In arXiv:1312.5650, 2013. 6, 7

[29] M. Palatucci, D. Pomerleau, G. E. Hinton, and T. M.
Mitchell. Zero-shot learning with semantic output
codes. In NeurIPS, 2009. 1

[14] Y. Fu, T. M. Hospedales, T. Xiang, and S. Gong.
Transductive multi-view zero-shot learning. In TPA-
MI, 2015. 3, 6

[30] G. Patterson and J. Hays. Sun attribute database: Dis-
covering, annotating, and recognizing scene attributes.
In CVPR, 2012. 5

9392

[31] Y. Yao, F. Shen, J. Zhang, L. Liu, Z. Tang, and L.
Shao. Discovering and distinguishing multiple visual
senses for web learning. In TMM, 2018. 1

[44] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein,
and B. Schiele. Latent embeddings for zero-shot clas-
siﬁcation. In CVPR, 2016. 2, 6, 7

[32] Z. Zhang, Y. Xu, L. Shao, and J. Yang. Discrimina-
tive block-diagonal representation learning for image
recognition. In TNNLS, 2018. 1

[33] Z. Zhang, L. Shao, Y. Xu, L. Liu, and J. Yang.
Marginal representation learning with graph structure
self-adaptation. In TNNLS, 2017. 1

[34] J. Qin, L. Liu, L. Shao, F. Shen, B. Ni, J. Chen, and
Y. Wang. Zero-shot action recognition with error-
correcting output codes. scene attributes.
In CVPR,
2017. 2

[35] R. Qiao, L. Liu, C. Shen, and A. van den Hengel. Less
is more: zero-shot learning from online textual docu-
ments with noise suppression. In CVPR, 2016. 2

[36] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning
deep representations of ﬁne-grained visual descrip-
tions. In CVPR, 2016. 2

[37] B. Romera-Paredes and P. Torr. An embarrassingly
simple approach to zero-shot learning. In ICML, 2015.
2, 5, 6, 7

[38] O. Russakovsky,

J. Deng, H. Su,

J. Krause,
S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, et al. Imagenet large scale visual recog-
nition challenge. In IJCV, 2015. 6

[39] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng.
In

Zero-shot learning through cross-modal transfer.
NeurIPS, 2013. 2, 6, 7

[40] J. Qin, Y. Wang, L. Liu, J. Chen, and L. Shao. Beyond
semantic attributes: Discrete latent attributes learning
for zero-shot recognition. In PRL, 2016. 2

[41] H. Jiang, R. Wang, S. Shan, and X. Chen. Learning
class prototypes via structure alignment for zero-shot
recognition. In ECCV, 2018. 2

[42] J. Song, C. Shen, Y. Yang, Y. Liu, and M. Song. Trans-
ductive unbiased embedding for zero-shot learning. In
CVPR, 2018. 2, 3, 4, 5, 6, 7

[45] Y. Xian, T. Lorenz, B. Schiele, and Z. Akata. Feature
generating networks for zero-shot learning. In CVPR,
2018. 2

[46] Y. Xian, B. Schiele, and Z. Akata. Zero-shot learning-
the good, the bad and the ugly. In CVPR, 2017. 1, 2,
3, 5, 6

[47] G.-S. Xie, X.-Y. Zhang, X. Shu, S. Yan, and C.-L. Liu.
Task-driven feature pooling for image classiﬁcation.
In ICCV, 2015. 1

[48] H. Xu and K. Saenko. Ask, attend and answer: Explor-
ing question-guided spatial attention for visual ques-
tion answering. In ECCV, 2016. 3

[49] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville,
R. Salakhudinov, R. Zemel, and Y. Bengio. Show, at-
tend and tell: Neural image caption generation with
visual attention. In ICML, 2015. 2, 4

[50] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and
T. M. Hospedales. Learning to compare: Relation net-
work for few-shot learning. In CVPR, 2018. 1, 2, 6,
7

[51] M. Ye and Y. Guo. Zero-shot classiﬁcation with
In

discriminative semantic representation learning.
CVPR, 2017. 2

[52] L. Zhang, T. Xiang, S. Gong, et al. Learning a deep
In CVPR,

embedding model for zero-shot learning.
2017. 1, 2, 3, 6, 7

[53] Z. Zhang and V. Saligrama. Zero-shot learning via
semantic similarity embedding. In ICCV, 2015. 2, 6,
7

[54] Z. Zhang and V. Saligrama. Zero-shot learning via

joint latent similarity embedding. In CVPR, 2016. 2

[55] L. V. D. Maaten, and G. Hinton. Visualizing data using

[43] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-
longie. The Caltech-UCSD Birds-200-2011 Dataset.
In Technical report, 2011. 5

8

t-SNE. In JMLR, 2008.

9393

