Group-wise Correlation Stereo Network

Xiaoyang Guo1

Kai Yang2 Wukui Yang2

Xiaogang Wang1

Hongsheng Li1

1 The Chinese University of Hong Kong
{xyguo, xgwang, hsli}@ee.cuhk.edu.hk

2 SenseTime Research

{yangkai, yangwukui}@sensetime.com

Abstract

Stereo matching estimates the disparity between a recti-
Ô¨Åed image pair, which is of great importance to depth sens-
ing, autonomous driving, and other related tasks. Previ-
ous works built cost volumes with cross-correlation or con-
catenation of left and right features across all disparity lev-
els, and then a 2D or 3D convolutional neural network is
utilized to regress the disparity maps.
In this paper, we
propose to construct the cost volume by group-wise cor-
relation. The left features and the right features are di-
vided into groups along the channel dimension, and cor-
relation maps are computed among each group to obtain
multiple matching cost proposals, which are then packed
into a cost volume. Group-wise correlation provides efÔ¨Å-
cient representations for measuring feature similarities and
will not lose too much information like full correlation. It
also preserves better performance when reducing param-
eters compared with previous methods. The 3D stacked
hourglass network proposed in previous works is improved
to boost the performance and decrease the inference com-
putational cost. Experiment results show that our method
outperforms previous methods on Scene Flow, KITTI 2012,
and KITTI 2015 datasets. The code is available at https:
//github.com/xy-guo/GwcNet

1. Introduction

Accurate depth sensing plays an important role in many
computer vision applications like odometry, robot naviga-
tion, pose estimation, and object detection [10, 28, 6, 16].
Unlike monocular depth estimation [3, 5] or active depth
sensing [24], stereo matching estimates depth by matching
pixels from rectiÔ¨Åed image pairs captured by two cameras.
Traditional stereo pipelines usually consist of all or por-
tion of the following four steps, matching cost compu-
tation, cost aggregation, disparity optimization, and post-
processing [23]. Matching cost computation provides ini-
tial similarity measures for left image patches and possi-
ble corresponding right patches, which is a crucial step of
stereo matching. Common matching costs include absolute

difference (SAD), sum of squared difference (SSD), and
normalized cross-correlation (NCC). The cost aggregation
and the optimization steps incorporate contextual matching
costs and priors to obtain more robust disparity predictions.

Learning-based methods explore different feature repre-
sentations and aggregation algorithms for matching costs.
DispNetC [19] computes a correlation volume from the left
and right image features and utilizes a CNN to directly
regress disparity maps. GC-Net [9] and PSMNet [2] con-
struct concatenation-based feature volume and incorporate
a 3D CNN to aggregate contextual features. There are also
works [1, 25] trying to aggregate evidence from multiple
hand-crafted matching cost proposals. However, the above
methods have several drawbacks. The full correlation [19]
provides an efÔ¨Åcient way for measuring feature similarities,
but it loses much information because it produces only a
single-channel correlation map for each disparity level. The
concatenation volume [9, 2] requires more parameters in the
following aggregation network to learn the similarity mea-
surement function from scratch. [1, 25] stills utilizes tradi-
tional matching costs and cannot be optimized end-to-end.

In this paper, we propose a simple yet efÔ¨Åcient opera-
tion called group-wise correlation to tackle the above draw-
backs. Multi-level unary features are extracted to form
high-dimensional feature representations fl, fr for a left-
right image pair. Then, the features are split into multiple
groups along the channel dimension, and the ith left feature
group is correlated with the corresponding ith right feature
group over all disparity levels to obtain group-wise corre-
lation maps. At last, all the correlation maps are packed to
form a 4D cost volume. The unary features can be treated
as groups of structured vectors [32], so the correlation maps
for a certain group can be seen as a matching cost proposal.
In this way, we can leverage the power of traditional corre-
lation matching cost and provide better similarity measures
for the following 3D aggregation network compared with
[9, 2]. The multiple matching cost proposals also avoid the
information loss like full correlation [19].

The 3D stacked hourglass aggregation network proposed
in PSMNet [2] is modiÔ¨Åed to further improve the per-
formance and decrease the inference computational cost.

3273

1√ó1√ó1 3D convolutions are employed in the shortcut con-
nections within each hourglass module without increasing
too much computational cost.

Our main contributions can be summarized as follows.
1) We propose group-wise correlation to construct cost vol-
umes to provide better similarity measures. 2) The stacked
3D hourglass reÔ¨Ånement network is modiÔ¨Åed to improve the
performance without increasing the inference time. 3) Our
method achieves better performance than previous methods
on Scene Flow, KITTI 2012, and KITTI 2015 datasets. 4)
Experiment results show that when limiting the computa-
tional cost of the 3D aggregation network, the performance
reduction of our proposed network is much smaller than
previous PSMNet, which makes group-wise correlation a
valuable way to be implemented in real-time stereo net-
works.

2. Related Work

2.1. Traditional methods

Generally, traditional stereo matching consists of all or
portion of the following four steps: matching cost compu-
tation, cost aggregation, disparity optimization, and some
post-processing steps [23].
In the Ô¨Årst step, the match-
ing costs of all pixels are computed for all possible dis-
parities. Common matching costs include sum of absolute
difference (SAD), sum of squared difference (SSD), nor-
malized cross-correlation (NCC), and so on. Local meth-
ods [37, 34, 20] explore different strategies to aggregate
matching costs with neighbor pixels and usually utilize the
winner-take-all (WTA) strategy to choose the disparity with
minimum matching cost. In contrast, global methods min-
imize a target function to solve the optimal disparity map,
which usually takes both matching costs and smoothness
priors into consideration, such as belief propagation [30, 13]
and graph cut [15]. Semi-global matching (SGM) [7] ap-
proximates the global optimization with dynamic program-
ming. Local and global methods can be combined to obtain
better performance and robustness.

2.2. Learning based methods

Besides hand-crafted methods, researchers also pro-
posed many learned matching costs [36, 18, 27] and cost
aggregation algorithms [1, 25]. Zbontar and Lecun [36] Ô¨Årst
proposed to compute matching costs using neural networks.
The predicted matching costs are then processed with tradi-
tional cross-based cost aggregation and semi-global match-
ing to predict the disparity map. The matching cost com-
putation was accelerated in [18] by correlating unary fea-
tures. Batsos et al. proposed CBMV [1] to combine evi-
dence from multiple basic matching costs. Schonberger et
al. [25] proposed to classify scanline matching cost candi-
dates with a random forest classiÔ¨Åer. Seki et al. proposed

SGM-Nets [26] to provide learned penalties for SGM. Kno-
belreiter et al. [14] proposed to combine CNN-predicted
correlation matching costs and CRF to integrate long-range
interactions.

Following DispNetC (Mayer et al. [19]), there are a lot
of works directly regressing disparity maps from correla-
tion cost volumes [22, 17, 29, 33]. Given the left and the
right feature maps fl and fr, the correlation cost volume is
computed for each disparity level d,

Ccorr(d, x, y) =

1
Nc

hfl(x, y), fr(x ‚àí d, y)i,

(1)

where h¬∑, ¬∑i is the inner product of two feature vectors and
Nc denotes the number of channels. CRL [22] and iRes-
Net [17] followed the idea of DispNetC with stack re-
Ô¨Ånement sub-networks to further improve the performance.
There are also works integrating additional information
such as edge features [29] and semantic features [33].

Recent works employed concatenation-based feature
volume and 3D aggregation networks for better context ag-
gregation [9, 2, 35]. Kendall et al. proposed GC-Net [9] and
was the Ô¨Årst to use 3D convolution networks to aggregate
context for cost volumes. Instead of directly giving a cost
volume, the left and the right feature fl, fr are concatenated
to form a 4D feature volume,

Cconcat(d, x, y, ¬∑) = Concat {fl(x, y), fr(x ‚àí d, y)} .

(2)

Context features are aggregated from neighbour pixels and
disparities with 3D convolution networks to predict a dis-
parity probability volume. Following GC-Net, Chang et
al. [2] proposed the pyramid stereo matching network
(PSMNet) with a spatial pyramid pooling module and
stacked 3D hourglass networks for cost volume reÔ¨Ånement.
Yu et al. [35] proposed to generate and select multiple cost
aggregation proposals. Zhong et al. [38] proposed a self-
adaptive recurrent stereo model to tackle open-world data.
LRCR [8] utilized left-right consistency check and recur-
rent model to aggregate cost volumes predicted from [27]
and reÔ¨Åned unreliable disparity predictions. There are also
other works focusing on real-time stereo matching [11] and
application friendly stereo [31].

3. Group-wise Correlation Network

We propose group-wise correlation stereo network
(GwcNet), which extends PSMNet [2] with group-wise cor-
relation cost volume and improved 3D stacked hourglass
networks.

3.1. Network architecture

The structure of the proposed group-wise correlation net-
work is shown in Figure 1. The network consists of four

3274

ùêº"

ùêº#

Concat

Shared
Weights

Concat

Concat
Volume

Group-

Corr

Volume

Combined

Volume

3D

aggregation

network

Figure 1: The pipeline of the proposed group-wise correlation network. The whole network consists of four parts, unary
feature extraction, cost volume construction, 3D convolution aggregation, and disparity prediction. The cost volume is
divided into two parts, concatenation volume (Cat) and group-wise correlation volume (Gwc). Concatenation volume is built
by concatenating the compressed left and right features. Group-wise correlation volume is described in Section 3.2.

3D conv

3D conv, stride 2

3D deconv

Output

Module 3

Shortcut 3D conv, kernel 1

Output

Module 0

Output

Module 1

Output

Module 2

Figure 2: The structure of our proposed 3D aggregation network. The network consists of a pre-hourglass module (four
convolutions at the beginning) and three stacked 3D hourglass networks. Compared with PSMNet [2], we remove the
shortcut connections between different hourglass modules and output modules, thus output modules 0,1,2 can be removed
during inference to save time. 1√ó1√ó1 3D convolutions are added to the shortcut connections within hourglass modules.

parts, unary feature extraction, cost volume construction,
3D aggregation, and disparity prediction (details in Table 1).

For feature extraction, we adopt the ResNet-like network
used in PSMNet [2] with the half dilation settings and with-
out its spatial pyramid pooling module. The last feature
maps of conv2, conv3, and conv4 are concatenated to form
320-channel unary feature maps.

The cost volume is composed of two parts, a concate-
nation volume and a group-wise correlation volume. The
concatenation volume is the same as PSMNet [2] but with
fewer channels, before which the unary features are com-
pressed into 12 channels with two convolutions. The pro-
posed group-wise correlation volume will be described in
details in Section 3.2. The two volumes are then concate-
nated as the input to the 3D aggregation network.

The 3D aggregation network is used to aggregate fea-
tures from neighboring disparities and pixels, which con-
sists of a pre-hourglass module and three stacked 3D hour-
glass networks. As shown in Figure 2, the pre-hourglass
module consists of four 3D convolutions with batch normal-
ization and ReLU. Three stacked 3D hourglass networks
are followed to reÔ¨Åne low-texture ambiguities and occlu-
sion parts by encoder-decoder structures. Compared with

3D aggregation network of [2], we have several important
modiÔ¨Åcations to improve the performance and increase the
inference speed, and details are described in Section 3.3.

The pre-hourglass and three stacked 3D hourglass net-
works are connected to output modules. Each output mod-
ule predicts a disparity map. The structure of the output
module and the loss function are described in Section 3.4.

3.2. Group wise correlation volume

The left unary features and the right unary features are
denoted by fl and fr with Nc channels and in 1/4 the size
of original images.
In previous works [19, 9, 2], the left
and right features are correlated or concatenated at different
disparity levels to form the cost volume. However, both cor-
relation volume and concatenation volume have drawbacks.
The full correlation provides an efÔ¨Åcient way for measur-
ing feature similarities, but it loses much information be-
cause it produces only a single-channel correlation map for
each disparity level. The concatenation volume contains no
information about the feature similarities, so more param-
eters are required in the following aggregation network to
learn the similarity measurement function from scratch. To
solve the above issues, we propose group-wise correlation

3275

by combining advantages of the concatenation volume and
the correlation volume.

The basic idea behind group-wise correlation is splitting
the features into groups and computing correlation maps
group by group. We denote the channels of unary features
as Nc. All the channels are evenly divided into Ng groups
along the channel dimension, and each feature group there-
fore has Nc/Ng channels. The gth feature group f g
r con-
sists of the g Nc
‚àí 1)th channels
Ng
of the original feature fl, fr. The group-wise correlation is
then computed as

+ 1, . . . , g Nc
Ng

+ ( Nc
Ng

, g Nc
Ng

l , f g

Cgwc(d, x, y, g) =

1

Nc/Ng

hf g

l (x, y), f g

r (x ‚àí d, y)i,

(3)

where h¬∑, ¬∑i is the inner product. Note that the correlation is
computed for all feature groups g and at all disparity levels
d. Then, all the correlation maps are packed into a matching
cost volume of the shape [Dmax/4, H/4, W/4, Ng], where
Dmax denotes the maximum disparity and Dmax/4 corre-
sponds to the maximum disparity for the feature. When
Ng=1, the group-wise correlation becomes full correlation.
Group-wise correlation volume Cgwc can be treated as
Ng cost volume proposals, and each proposal is com-
puted from the corresponding feature group. The following
3D aggregation network aggregates multiple candidates to
regress disparity maps. The group-wise correlation success-
fully leverages the power of traditional correlation match-
ing costs and provides rich similarity-measure features for
the 3D aggregation network, which alleviates the parame-
ter demand. We will show in Section 4.5 that we explore
to reduce the channels of the 3D aggregation network, and
the performance reduction of our proposed network is much
smaller than [2]. Our proposed group-wise correlation vol-
ume requires less 3D aggregation parameters to achieve fa-
vorable results.

To further improve the performance, the group correla-
tion cost volume can be combined with the concatenation
volume. Experiment results show that the group-wise cor-
relation volume and the concatenation volume are comple-
mentary to each other.

3.3. Improved 3D aggregation module

In PSMNet [2], a stacked hourglass architecture was pro-
posed to learn better context features. Based on the net-
work, we apply several important modiÔ¨Åcations to make it
suitable for our proposed group-wise correlation and im-
prove the inference speed. The structure of the proposed
3D aggregation is shown in Figure 2 and Table 1.

1) First, we add one more auxiliary output module (see
output module 0 in Figure 2) after the pre-hourglass mod-
ule. The extra auxiliary loss makes the network learn bet-
ter features at lower layers, which beneÔ¨Åts the Ô¨Ånal predic-
tion. 2) The residual connections between different out-

Name

Layer properties

Output size

Cost Volume

unary l/r N/A, S2
volume g
volume c
volume

group-wise cost volume
concatenation cost volume
volume g,volume c: Concat

H/4√óW/4√ó320
D/4√óH/4√óW/4√ó40
D/4√óH/4√óW/4√ó24
D/4√óH/4√óW/4√ó64

Pre-hourglass

conv1
conv2
output

[32√ó32, 3√ó3√ó3, S1] √ó2
[32√ó32, 3√ó3√ó3, S1] √ó2
conv1,conv2: Add

D/4√óH/4√óW/4√ó32
D/4√óH/4√óW/4√ó32
D/4√óH/4√óW/4√ó32

Hourglass Module 1, 2, 3

N/A
32√ó64, 3√ó3√ó3, S2
64√ó64, 3√ó3√ó3, S1
64√ó128, 3√ó3√ó3, S2
128√ó128, 3√ó3√ó3, S1

input
conv1a
conv1b
conv2a
conv2b
deconv1* 128√ó64, 3√ó3√ó3, S2, deconv
shortcut1* conv1b: 64√ó64, 1√ó1√ó1, S1
plus1
deconv0* 64√ó32, 3√ó3√ó3, S2, deconv
shortcut0* input: 32√ó32, 1√ó1√ó1, S1
output

D/4√óH/4√óW/4√ó32
D/8√óH/8√óW/8√ó64
D/8√óH/8√óW/8√ó64
D/16√óH/16√óW/16√ó128
D/16√óH/16√óW/16√ó128
D/8√óH/8√óW/8√ó64
D/8√óH/8√óW/8√ó64
deconv1,shortcut1: Add&ReLU D/8√óH/8√óW/8√ó64
D/4√óH/4√óW/4√ó32
D/4√óH/4√óW/4√ó32
deconv0,shortcut0: Add&ReLU D/4√óH/4√óW/4√ó32

Output Module 0, 1, 2, 3

input
conv1
conv2**
score
prob
disparity

D/4√óH/4√óW/4√ó32
N/A
32√ó32, 3√ó3√ó3, S1
D/4√óH/4√óW/4√ó32
32√ó1, 3√ó3√ó3, S1
D/4√óH/4√óW/4√ó1
D√óH√óW √ó1
Upsample
Softmax (at disparity dimension) D√óH√óW √ó1
Soft Argmin (Equ. 4)

H√óW √ó1

Table 1: Structure details of the modules. H, W represents
the height and the width of the input image. S1/2 denotes
the convolution stride. If not speciÔ¨Åed, each 3D convolu-
tion is with a batch normalization and ReLU. * denotes the
ReLU is not included. ** denotes convolution only.

put modules are removed, thus auxiliary output modules
(output module 0, 1, 2) can be removed during inference
to save computational cost. 3) 1√ó1√ó1 3D convolutions
are added to the shortcut connections within each hourglass
module (see dashed lines in Figure 2) to improve the perfor-
mance without increasing much computational cost. Since
the 1√ó1√ó1 3D convolution only has 1/27 multiplication op-
erations compared with 3√ó3√ó3 convolutions, it runs very
fast and the time can be neglected.

3.4. Output module and loss function

For each output module, two 3D convolutions are em-
ployed to generate a 1-channel 4D volume, and then the
volume is upsampled and converted into a probability vol-
ume with softmax function along the disparity dimension.
Detailed structures are shown in Table 1. For each pixel, we
have a Dmax-length vector which contains the probability

p for all disparity levels. Then, the disparity estimation ed is

3276

Model

Cat64-Base
Gwc1-Base
Gwc10-Base
Gwc20-Base
Gwc40-Base
Gwc80-Base
Gwc160-Base
Gwc40-Cat24-Base
PSMNet [2]
Cat64-original-hg
Cat64
Gwc40 (GwcNet-g)
Gwc40-Cat24 (GwcNet-gc)

Concat
Volume

X

X

X

X

X

X

Group
Corr

Volume

Stack
Hour-
glass

Groups

Init

√ó

Channels

Volume
Channel

X

X

X

X

X

X

X

X

X

-

1√ó320
10√ó32
20√ó16
40√ó8
80√ó4
160√ó2
40√ó8

-
-
-

40√ó8
40√ó8

64
1
10
20
40
80
160

40+24

64
64
64
40

40+24

[2]
[2]
Ours
Ours
Ours

>1px
(%)

12.78
13.32
11.82
11.84
11.68
11.69
11.58
11.26
9.46
9.47
8.41
8.18
8.03

>2px
(%)

>3px
(%)

EPE (px)

8.05
8.37
7.31
7.29
7.18
7.17
7.08
6.87
5.19
5.13
4.63
4.57
4.47

6.33
6.62
5.70
5.67
5.58
5.57
5.49
5.31
3.80
3.74
3.41
3.39
3.30

1.308
1.369
1.230
1.216
1.212
1.214
1.188
1.127
0.887
0.876
0.808
0.792
0.765

Time
(ms)

117.1
104.0
112.8
116.3
122.2
133.3
157.3
135.1
246.1
241.0
198.3
200.3
210.7

Table 2: Ablation study results of proposed networks on the Finalpass of Scene Flow datasets [19]. Cat, Gwc, Gwc-Cat
represent only concatenation volume, only group-wise correlation volume, or the both. Base denotes the network variants
without stacked hourglass networks. The time is the inference time for 480√ó640 inputs on a single Nvidia TITAN Xp GPU.
The result of PSMNet [2] is trained with published code with our batch size, evaluation settings for fair comparison.

Model

KITTI 12
EPE (px)

KITTI 12
D1-all(%)

KITTI 15
EPE (px)

KITTI 15
D1-all (%)

4. Experiment

PSMNet [2]
Cat64-original-hg
Cat64
Gwc40
Gwc40-Cat24

0.713
0.740
0.691
0.662
0.659

2.53
2.72
2.41
2.30
2.10

0.639
0.652
0.615
0.602
0.613

1.50
1.76
1.55
1.41
1.49

Table 3: Ablation study results of our networks on KITTI
2012 validation and KITTI 2015 validation sets.

In this section, we evaluate our proposed stereo models
on Scene Flow datasets [19] and the KITTI dataset [4, 21].
Datasets and implementation details are described in Sec-
tion 4.1 and Section 4.2. The effectiveness and the best set-
tings of group-wise correlation are explored in Section 4.3.
The performance improvement of the new stacked hour-
glass module is discussed in Section 4.4. We also explore
the performance of group-wise correlation when the com-
putational cost is limited in Section 4.5.

given by the soft argmin function [9],

4.1. Datasets and evaluation metrics

Dmax

‚àí1Xk=0

k ¬∑ pk,

(4)

ed =

where k and pk denote a possible disparity level and the cor-
responding probability. The predicted disparity maps from

the four output modules are denoted ased0,ed1,ed2,ed3. The

Ô¨Ånal loss is given by,

L =

i=3Xi=0

Œªi ¬∑ SmoothL1 (edi ‚àí d‚àó),

(5)

where Œªi denotes the coefÔ¨Åcients for the ith disparity pre-
diction and d‚àó represents the ground-truth disparity map.
The smooth L1 loss is computed as follows,

SmoothL1 (x) =(0.5x2,

|x| ‚àí 0.5,

if |x| < 1

otherwise

(6)

Scene Flow datasets are a dataset collection of synthetic
stereo datasets, consisting of Flyingthings3D, Driving, and
Monkaa. The datasets provide 35,454 training and 4,370
testing images of size 960√ó540 with accurate ground-truth
disparity maps. We use the Finalpass of the Scene Flow
datasets, since it contains more motion blur and defocus and
is more like real-world images than the Cleanpass. KITTI
2012 and KITTI 2015 are driving scene datasets. KITTI
2012 provides 194 training and 195 testing images pairs,
and KITTI 2015 provides 200 training and 200 testing im-
age pairs. Both datasets provide sparse LIDAR ground-
truth disparity for the training images.

For Scene Flow datasets, the evaluation metrics is usu-
ally the end-point error (EPE), which is the mean average
disparity error in pixels. For KITTI 2012, percentages of
erroneous pixels and average end-point errors for both non-
occluded (Noc) and all (All) pixels are reported. For KITTI
2015, the percentage of disparity outliers D1 is evaluated

3277

for background, foreground, and all pixels. The outliers are
deÔ¨Åned as the pixels whose disparity errors are larger than
max(3px, 0.05d‚àó), where d‚àó denotes the ground-truth dis-
parity.

4.2. Implementation details

Our network is implemented with PyTorch. We use
Adam [12] optimizer, with Œ≤1 = 0.9, Œ≤2 = 0.999. The
batch size is Ô¨Åxed to 16, and we train all the networks with
8 Nvidia TITAN Xp GPUs with 2 training samples on each
GPU. The coefÔ¨Åcients of four outputs are set as Œª0 = 0.5,
Œª1 = 0.5, Œª2 = 0.7, Œª3 = 1.0.

For Scene Flow datasets, we train the stereo networks
for 16 epochs. The learning rate is set to 0.001 and down-
scaled by 2 after epoch 10, 12, and 14. To test on Scene
Flow datasets, the full images of size 960√ó540 are input to
the network for disparity prediction. We set the maximum
disparity value as Dmax = 192 following PSMNet [2] for
Scene Flow datasets. To evaluate our networks, we remove
all the images with less than 10% valid pixels (0‚â§d<Dmax)
in the test set. For each valid image, the evaluation metrics
are computed with only valid pixels.

For KITTI 2015 and KITTI 2012, we Ô¨Åne-tune the net-
work pre-trained on Scene Flow datasets for another 300
epochs. The initial learning rate is 0.001 and is down-scaled
by 10 after epoch 200. For testing on KITTI datasets, we
Ô¨Årst pad zeros on the top and the right side of the images to
make the inputs in size 1248√ó384.

4.3. The effectiveness of Group wise correlation

In this section, we explore the effectiveness and the best
settings for the group-wise correlation. In order to prove the
effectiveness of the proposed group-wise correlation vol-
ume, we conduct several experiments on the Base model,
which removes the stacked hourglass networks and only
preserves the pre-hourglass module and the output module
0. Cat-Base, Gwc-Base, and Gwc-Cat-Base are the base
models with only concatenation volume, only group-wise
correlation volume, or both volumes.

Experiment results in Table 2 show that the performance
of the Gwc-Base network increases as the group number in-
creases. When the group number is larger than 40, the per-
formance improvement becomes minor and the end-point
error stays around 1.2px. Considering the memory usage
and the computational cost, we choose 40 groups with each
group having 8 channels as our network structure, which
corresponds to the Gwc40-Base model in Table 2.

All the Gwc-Base models except Gwc1-Base outperform
the Cat-Base model which utilizes concatenation volume,
which shows the effectiveness of the group-wise correla-
tion. The Gwc40 model reduces the end-point error by
0.1px and the 3-pixel error rate by 0.75%, and the time con-
sumption is almost the same. The performance can be fur-

Figure 3: Our model Gwc-Cat achieves much better per-
formance than Cat when the number of channels decreases.
The models with 32 base channels correspond to the Cat64
model (concatenation volume) and the Gwc40-Cat24 model
(group-wise correlation and concatenation volume). The
channels of the cost volume and all 3D convolutions de-
crease by the same factor as the base channel.

ther improved by combining group-wise correlation volume
with concatenation volume (see Gwc40-Cat24-Base model
in Table 2). The group-wise correlation could provide accu-
rate matching features, and the concatenation volume pro-
vides complementary semantic information.

4.4. Improved stacked hourglass

In this paper, we applied several modiÔ¨Åcations to the
stacked hourglass networks proposed in [2] to improve the
performance of cost volume aggregation. From Table 2 and
Table 3, we can see that the model with the proposed hour-
glass networks (Cat64) increases EPE by 7.8% on Scene
Flow datasets and 5.8% on KITTI 2015 compared with
the model Cat64-original-hg (with the hourglass module in
[2]). The inference time for 640√ó480 inputs on a single
Nvidia TITAN Xp GPU also decreases by 42.7ms, because
the auxiliary output modules can be removed during infer-
ence to save time.

4.5. Limit the computational cost of 3D network

We explore to limit the computational cost by decreasing
channels in the 3D aggregation network to verify the effec-
tiveness of the proposed group-wise group correlation. The
results are shown in Figure 3. The base number of chan-
nels are modiÔ¨Åed from the original 32 to 2, and the chan-
nels of the cost volume and all 3D convolutions are reduced
with the same factor. As the number of channels decreasing,
our models with group-wise correlation volume (Gwc-Cat)
perform much better than the models with only concatena-
tion volume (Cat). The performance gain enlarges as more
channels reduced. The reason for this is that the group-wise

3278

32(ori)16842Base#Channelof3DNetwork0.81.01.21.41.61.8EPE/pxCat(ConcatVolume)Gwc-Cat(Groupwise-corr&Concat)(a) Visualization results on the Scene Flow datasets.

(b) Visualization results on the KITTI 2012 dataset.

(c) Visualization results on the KITTI 2015 dataset.

Figure 4: Depth visualization results on the test sets of Scene Flow [19], KITTI 2012 [4] and KITTI 2015 [21] datasets. From
left to right, input left images, predicted disparity maps, and error maps.

3279

DispNetC [19]
GC-Net [9]
CRL [22]
iResNet-i2e2 [17]
PSMNet [9]
SegStereo [33]
GwcNet-g (Gwc40)

D1-bg
4.32
2.21
2.48
2.14
1.86
1.88
1.74

All (%)
D1-fg
4.41
6.16
3.59
3.45
4.62
4.07
3.93

D1-all
4.34
2.87
2.67
2.36
2.32
2.25
2.11

D1-bg
4.11
2.02
2.32
1.94
1.71
1.76
1.61

Noc (%)

D1-fg
3.72
5.58
3.12
3.20
4.31
3.70
3.49

D1-all
4.05
2.61
2.45
2.15
2.14
2.08
1.92

Time

(s)
0.06
0.9
0.47
0.22
0.41
0.6
0.32

Table 4: KITTI 2015 test set results. The dataset contains 200 images for training and 200 images for testing.

DispNetC [19]
MC-CNN-acrt [36]
GC-Net [9]
iResNet-i2 [17]
SegStereo [33]
PSMNet [9]
GwcNet-gc (Gwc40-Cat24)

>2px (%)

>3px (%)

>5px (%)

Mean Error (px)

Time

Noc
7.38
3.90
2.71
2.69
2.66
2.44
2.16

All
8.11
5.45
3.46
3.34
3.19
3.01
2.71

Noc
4.11
2.43
1.77
1.71
1.68
1.49
1.32

All
4.65
3.63
2.30
2.16
2.03
1.89
1.70

Noc
2.05
1.64
1.12
1.06
1.00
0.90
0.80

All
2.39
2.39
1.46
1.32
1.21
1.15
1.03

Noc
0.9
0.7
0.6
0.5
0.5
0.5
0.5

All
1.0
0.9
0.7
0.6
0.6
0.6
0.5

(s)
0.06
67
0.9
0.12
0.6
0.41
0.32

Table 5: KITTI 2012 test set results. The dataset contains 194 images for training and 195 images for testing.

correlation provides good matching cost representations for
the 3D aggregation network, while the aggregation network
with only concatenation volume as inputs needs to learn
the matching similarity function from scratch, which usu-
ally requires more parameters and computational cost. As a
result, the proposed group-wise correlation could be a valu-
able method to be implemented in real-time stereo networks
where the computational costs are limited.

4.6. KITTI 2012 and KITTI 2015

For KITTI stereo 2015 [21], we split the training set
into 180 training image pairs and 20 validation image pairs.
Since the results on the validation set are not stable, we Ô¨Åne-
tune the pretrained model for 3 times and choose the model
with the best validation performance. From Table 3, the per-
formance of both Gwc40-Cat24 and Gwc40 is better than
the models without group-wise correlation (Cat64, Cat64-
original-hg). We submit the Gwc40 model (without con-
catenation volume) with the lowest validation error to the
evaluation server, and the results on the test set are shown
in Table 4. Our model surpasses the PSMNet [2] by 0.21%
and SegStereo [33] by 0.14% on D1-all.

For KITTI 2012 [4], we split the training set into 180
training images and 14 validation image pairs. The results
on the validation set are shown in Table 3. We submit the
best Gwc40-Cat24 model on the validation set to the evalu-
ation server. The evaluation results on the test set are shown

in Table 5. Our method surpasses PSMNet [2] by 0.19% on
3-pixel-error and 0.1px on mean disparity error.

5. Conclusion

In this paper, we proposed GwcNet to estimate dispar-
ity maps for stereo matching, which incorporates group-
wise correlation to build up the cost volumes. The group-
wise correlation volumes provide good matching features
for the 3D aggregation network, which improves the per-
formance and reduces the parameter requirements of the
aggregation network. We showed that when the computa-
tional cost is limited, our model achieves larger gain than
previous concatenation-volume based stereo networks. We
also improved the stacked hourglass networks to further im-
prove the performance and reduce the inference time. Ex-
periments demonstrated the effectiveness of our proposed
method on the Scene Flow datasets and the KITTI dataset.

Acknowledgements

This work is supported in part by SenseTime Group
Limited, in part by the General Research Fund through
the Research Grants Council of Hong Kong under Grants
CUHK14202217, CUHK14203118, CUHK14205615,
CUHK14207814, CUHK14213616, CUHK14208417,
CUHK14239816, and in part by CUHK Direct Grant.

3280

References

[1] Konstantinos Batsos, Changjiang Cai, and Philippos Mordo-
hai. Cbmv: A coalesced bidirectional matching volume for
disparity estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2060‚Äì
2069, 2018.

[2] Jia-Ren Chang and Yong-Sheng Chen.

Pyramid stereo
matching network. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5410‚Äì
5418, 2018.

[3] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In Advances in neural information processing systems,
pages 2366‚Äì2374, 2014.

[4] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2012.

[5] Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and
Xiaogang Wang. Learning monocular depth by distilling
cross-domain stereo networks. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 484‚Äì
500, 2018.

[6] Saurabh Gupta, Ross Girshick, Pablo Arbel¬¥aez, and Jiten-
dra Malik. Learning rich features from rgb-d images for ob-
ject detection and segmentation. In European Conference on
Computer Vision, pages 345‚Äì360. Springer, 2014.

[7] Heiko Hirschmuller. Accurate and efÔ¨Åcient stereo processing
by semi-global matching and mutual information. In Com-
puter Vision and Pattern Recognition, 2005. CVPR 2005.
IEEE Computer Society Conference on, volume 2, pages
807‚Äì814. IEEE, 2005.

[8] Zequn Jie, Pengfei Wang, Yonggen Ling, Bo Zhao, Yunchao
Wei, Jiashi Feng, and Wei Liu. Left-right comparative re-
current model for stereo matching.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3838‚Äì3846, 2018.

[9] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter
Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry.
End-to-end learning of geometry and context for deep stereo
regression. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 66‚Äì75, 2017.

[10] Christian Kerl, J¬®urgen Sturm, and Daniel Cremers. Robust
odometry estimation for rgb-d cameras.
In 2013 IEEE In-
ternational Conference on Robotics and Automation, pages
3748‚Äì3754. IEEE, 2013.

[11] Sameh Khamis, Sean Fanello, Christoph Rhemann, Adarsh
Kowdle, Julien Valentin, and Shahram Izadi. Stereonet:
Guided hierarchical reÔ¨Ånement for real-time edge-aware
depth prediction.
In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pages 573‚Äì590, 2018.

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[13] Andreas Klaus, Mario Sormann, and Konrad Karner.
Segment-based stereo matching using belief propagation and
a self-adapting dissimilarity measure. In 18th International

Conference on Pattern Recognition (ICPR‚Äô06), volume 3,
pages 15‚Äì18. IEEE, 2006.

[14] Patrick Knobelreiter, Christian Reinbacher, Alexander
Shekhovtsov, and Thomas Pock. End-to-end training of hy-
brid cnn-crf models for stereo. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 2339‚Äì2348, 2017.

[15] V. Kolmogorov and R. Zabih. Computing visual correspon-
dence with occlusions using graph cuts.
In Proceedings
Eighth IEEE International Conference on Computer Vision.
ICCV 2001, volume 2, pages 508‚Äì515 vol.2, July 2001.

[16] Hongyang Li, Bo Dai, Shaoshuai Shi, Wanli Ouyang, and
Xiaogang Wang. Feature Intertwiner for Object Detection.
In ICLR, 2019.

[17] Zhengfa Liang, Yiliu Feng, Yulan Guo Hengzhu Liu Wei
Chen, and Linbo Qiao Li Zhou Jianfeng Zhang. Learning
for disparity estimation through feature constancy. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2811‚Äì2820, 2018.

[18] Wenjie Luo, Alexander G Schwing, and Raquel Urtasun. Ef-
Ô¨Åcient deep learning for stereo matching.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5695‚Äì5703, 2016.

[19] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical Ô¨Çow, and scene Ô¨Çow estimation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4040‚Äì4048, 2016.

[20] Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, and
Xiaopeng Zhang. Segment-tree based cost aggregation for
stereo matching. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 313‚Äì320,
2013.

[21] Moritz Menze and Andreas Geiger. Object scene Ô¨Çow for au-
tonomous vehicles. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 3061‚Äì
3070, 2015.

[22] Jiahao Pang, Wenxiu Sun, Jimmy SJ Ren, Chengxi Yang, and
Qiong Yan. Cascade residual learning: A two-stage convo-
lutional neural network for stereo matching. In ICCV Work-
shops, volume 7, 2017.

[23] Daniel Scharstein and Richard Szeliski. A taxonomy and
evaluation of dense two-frame stereo correspondence algo-
rithms. International journal of computer vision, 47(1-3):7‚Äì
42, 2002.

[24] Daniel Scharstein and Richard Szeliski. High-accuracy
stereo depth maps using structured light. In 2003 IEEE Com-
puter Society Conference on Computer Vision and Pattern
Recognition, 2003. Proceedings., volume 1, pages I‚ÄìI. IEEE,
2003.

[25] Johannes L Schonberger, Sudipta N Sinha, and Marc Polle-
feys. Learning to fuse proposals from multiple scanline opti-
mizations in semi-global matching.
In Proceedings of the
European Conference on Computer Vision (ECCV), pages
739‚Äì755, 2018.

[26] Akihito Seki and Marc Pollefeys. Sgm-nets: Semi-global
matching with neural networks. In Proceedings of the IEEE

3281

Conference on Computer Vision and Pattern Recognition,
pages 231‚Äì240, 2017.

[27] Amit Shaked and Lior Wolf. Improved stereo matching with
constant highway networks and reÔ¨Çective conÔ¨Ådence learn-
ing. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 4641‚Äì4650, 2017.

[28] Jamie Shotton, Ross Girshick, Andrew Fitzgibbon, Toby
Sharp, Mat Cook, Mark Finocchio, Richard Moore, Push-
meet Kohli, Antonio Criminisi, Alex Kipman, et al. EfÔ¨Å-
cient human pose estimation from single depth images. IEEE
transactions on pattern analysis and machine intelligence,
35(12):2821‚Äì2840, 2013.

[29] Xiao Song, Xu Zhao, Hanwen Hu, and Liangji Fang.
Edgestereo: A context integrated residual pyramid network
for stereo matching. In Asian Conference on Computer Vi-
sion (ACCV), 2018.

[30] Jian Sun, Nan-Ning Zheng, and Heung-Yeung Shum. Stereo
matching using belief propagation.
IEEE Transactions on
pattern analysis and machine intelligence, 25(7):787‚Äì800,
2003.

[31] Stepan Tulyakov, Anton Ivanov, and Francois Fleuret. Prac-
tical deep stereo (pds): Toward applications-friendly deep
stereo matching.
In Advances in Neural Information Pro-
cessing Systems, pages 5875‚Äì5885, 2018.

[32] Yuxin Wu and Kaiming He. Group normalization. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 3‚Äì19, 2018.

[33] Guorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong
Deng, and Jiaya Jia. Segstereo: Exploiting semantic infor-
mation for disparity estimation. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 636‚Äì
651, 2018.

[34] Qingxiong Yang. A non-local cost aggregation method for
stereo matching.
In 2012 IEEE Conference on Computer
Vision and Pattern Recognition, pages 1402‚Äì1409. IEEE,
2012.

[35] Lidong Yu, Yucheng Wang, Yuwei Wu, and Yunde Jia.
Deep stereo matching with explicit cost aggregation sub-
architecture. In Thirty-Second AAAI Conference on ArtiÔ¨Åcial
Intelligence, 2018.

[36] Jure Zbontar and Yann LeCun. Computing the stereo match-
ing cost with a convolutional neural network.
In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 1592‚Äì1599, 2015.

[37] Ke Zhang, Jiangbo Lu, and Gauthier Lafruit. Cross-based lo-
cal stereo matching using orthogonal integral images. IEEE
transactions on circuits and systems for video technology,
19(7):1073‚Äì1079, 2009.

[38] Yiran Zhong, Hongdong Li, and Yuchao Dai. Open-world
stereo video matching with deep rnn. In Proceedings of the
European Conference on Computer Vision (ECCV), pages
101‚Äì116, 2018.

3282

