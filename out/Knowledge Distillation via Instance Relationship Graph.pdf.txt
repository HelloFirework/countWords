Knowledge Distillation via Instance Relationship Graph

Yufan Liu∗a, Jiajiong Cao*b, Bing Li†a, Chunfeng Yuan†a, Weiming Hua, Yangxi Lic and Yunqiang

Duanc

aNLPR, Institute of Automation, Chinese Academy of Sciences

bAnt Financial

cNational Computer Network Emergency Response Technical Team/Coordination Center of China

Abstract

(cid:2)(cid:12)(cid:15)(cid:16)(cid:5)(cid:12)(cid:6)(cid:8)(cid:18) (cid:1)(cid:8)(cid:5)(cid:16)(cid:17)(cid:14)(cid:8)(cid:15)(cid:18)

The key challenge of knowledge distillation is to extract
general, moderate and sufﬁcient knowledge from a teacher
network to guide a student network. In this paper, a novel
Instance Relationship Graph (IRG) is proposed for knowl-
edge distillation.
It models three kinds of knowledge, in-
cluding instance features, instance relationships and fea-
ture space transformation, while the latter two kinds of
knowledge are neglected by previous methods. Firstly, the
IRG is constructed to model the distilled knowledge of
one network layer, by considering instance features and
instance relationships as vertexes and edges respectively.
Secondly, an IRG transformation is proposed to models the
feature space transformation across layers. It is more mod-
erate than directly mimicking the features at intermediate
layers. Finally, hint loss functions are designed to force a
student’s IRGs to mimic the structures of a teacher’s IRGs.
The proposed method effectively captures the knowledge
along the whole network via IRGs, and thus shows stable
convergence and strong robustness to different network ar-
chitectures. In addition, the proposed method shows supe-
rior performance over existing methods on datasets of var-
ious scales.

1. Introduction

In pursuit of high performance of Deep Neural Net-
works (DNNs), deeper and wider architectures have been
proposed at the expense of larger model size and longer in-
ference time. Examples include from AlexNet [13, 11, 2]
to ResNet [7, 27, 21] and DenseNet [10]. However, in vari-
ous practical applications, these networks can not satisfy the
requirements of real-time response and low memory cost.
Therefore, more and more efforts have been putting into

*Both authors contributed equally to this research.
†Corresponding authors: Bing Li (bli@nlpr.ia.ac.cn), Chunfeng Yuan

(cfyuan@nlpr.ia.ac.cn)

(cid:4)(cid:8)(cid:5)(cid:6)(cid:9)(cid:8)(cid:14)(cid:18)

(cid:3)(cid:16)(cid:17)(cid:7)(cid:8)(cid:12)(cid:16)(cid:18)

(cid:2)(cid:1)(cid:4)

(cid:4)

(cid:10)(cid:18)(cid:15)(cid:16)(cid:10)(cid:11)(cid:11)(cid:5)(cid:16)(cid:10)(cid:13)(cid:12)(cid:18)

(cid:3)
(cid:5)

(cid:4)

(cid:2)(cid:3)(cid:1)(cid:3)(cid:1)(cid:4)(cid:4)(cid:1)(cid:2)

(cid:1)
(cid:3)

(cid:2)

(cid:5)

(cid:4)

(cid:3)

(cid:3)

(cid:1)

(cid:1)

(cid:4)

(cid:4)
(cid:2)
(cid:2)

(a) Conventional method

(b) Our method

Figure 1: Comparison between the conventional and pro-
posed methods. (a) The conventional method uses instance
features to guide the student. Each instance is an indepen-
dent point in the feature space. (b) The proposed method
deﬁnes Instance Relationship Graph (containing instance
features, instance relationships and feature space transfor-
mation) as the distilled knowledge to guide the student.

model compression.

Knowledge distillation [9, 26, 16, 25] is one of the
most popular solutions for model compression. It utilizes
a teacher-student framework to distill knowledge, such as
predicted probabilities, from a teacher network to guide a
student network. For example, Hinton et al. [9] leveraged
the ﬁnal predicted probabilities of the teacher network to
supervise the student network. Zagoruyko et al. [26] trans-
ferred attention maps distilled from some mid-level layers
to teach the student. We refer to the softened outputs or
the intermediate-layer features of the network as instance
features, since they are obtained from samples (also called
instances) independently.

Nevertheless, there exist two limitations in conventional
knowledge distillation methods. Firstly, the existing meth-
ods independently extract instance features from the teacher
network as the distilled knowledge (as shown in Figure
1(a)). The instance relationships are never considered, but
they help reduce the intra-class variations and enlarge the
inter-class differences in the feature space. Moreover, in-
stance features based methods usually suffer from signiﬁ-

7096

cant performance drop when the teacher and student have
different network architectures. On the contrary, the in-
stance relationships are more robust to network changes.
For example, the instance features of the same sample from
two teacher networks can be totally different, while for both
teachers, samples from the same class are often closer than
those from the different classes in the feature space. Sec-
ondly, these methods only distill some speciﬁc layers’ out-
puts of the teacher, without considering the inference pro-
cedure.
It is a hard constraint for the student to directly
ﬁt all these layers’ outputs of the teacher. Thus extracting
moderate knowledge from the overall inference procedure
is necessary.

In order to resolve the above limitations, a novel graph-
based knowledge distillation method is proposed.
It dis-
tills three kinds of knowledge along the whole network.
Aside from the widely used instance features, two kinds
of new knowledge including instance relationships and fea-
ture space transformation are deﬁned. An Instance Relation
Graph (IRG) is proposed to model the knowledge. Specif-
ically, for a DNN layer, an IRG is constructed, in which
the vertex of the IRG represents the instance features, and
the edge denotes the instance relationships (as shown in
Figure 1(b)). The instance relationships provide sufﬁcient
and general information of the feature distribution and make
the distilled knowledge be able to guide a student network
with different architectures from its teacher.
In order to
avoid forcing too tight constraints, the feature space trans-
formation across layers is introduced as the third type of the
knowledge and an IRG transformation is proposed to model
this knowledge. The feature space transformation is a more
relaxed description than the densely ﬁtting on teacher’s in-
stance features at intermediate layers. By combining IRG
and IRG transformation, the proposed method models more
general, moderate and sufﬁcient knowledge than the exist-
ing methods. Finally, two loss functions are designed for
IRG and IRG transformation respectively. The hint losses
are optimized together to help boost the performance of the
student model.

Experiments on 4 different datasets are conducted, under
different teacher-student architectures. The experimental
results demonstrate that the proposed method shows stable
improvement on different teacher-network pairs, and out-
performs the state-of-the-art by more than 1x. In summary,
the main contributions of our work are three-fold:

• To the best of our knowledge, we at the ﬁrst time ex-
ploit three kinds of knowledge for knowledge distilla-
tion including instance features, instance relationships,
and feature space transformation across layers.

• An IRG and its transformation are proposed to model
all the types of the knowledge. The instance features
and instance relationships are considered as the ver-
texes and edges of the IRG respectively. The feature

space transformation is naturally expressed as the IRG
transformation from one layer to another. Therefore,
all three kinds of the knowledge of a network can be
well represented via IRGs.

• Different hint losses are introduced to supervise the
training of the student network. They help the student
learn different kinds of knowledge preserved in IRGs.
The experimental results have shown the superior of
the proposed method.

2. Related Work

There are mainly two types of methods on model com-
pression. The ﬁrst type is to remove redundant informa-
tion from complex trained models, such as network prun-
ing and model quantization. Speciﬁcally, network pruning
[14, 15, 8, 17, 22] aims to delete unimportant connections
of the trained network, while model quantization methods
[5, 18, 3, 23] represent the ﬂoat weights with fewer bits.
Though pruning and quantization methods have achieved
high compression ratio with low performance loss, they can
not change the network architectures.

Different from the ﬁrst type, a new concept called knowl-
edge distillation is introduced by Hinton et al.
[9] based
on a teacher-student framework. Knowledge distillation
method transfers knowledge from the trained teacher to the
student network. Recently, it has been applied in many
areas, such as image classiﬁcation [13], scene recogni-
tion [29] and face veriﬁcation [20].

Existing knowledge distillation methods focus on trans-
ferring instance features from the teacher to the student. For
example, Ba et al. [1] trained the student network to mimic
the teacher via regressing logits before the Softmax layer.
Zhou et al. [30] made the student share some lower lay-
ers with the teacher and train them simultaneously, but they
also used logits as the distilled knowledge. For transferring
the instance features of intermediate layers, Romero et al.
[19] proposed FitNet, which extracted the feature maps of
the intermediate layer as well as the ﬁnal output to teach
the student network. After that, Zagoruyko et al. [26] de-
ﬁned Attention Transfer (AT) based on attention maps to
improve the performance of the student network. However,
these methods independently extract the instance features
from the teacher while the instance relationships in the fea-
ture space is barely considered. Moreover, the instance fea-
tures of the intermediate layers are closely related to the
network design, which is not general for different teacher-
student pairs.

Further, most methods directly teach the student to ﬁt the
instance features of the teacher, ignoring the feature space
transformation process. To address this issue, Yim et al.
[24] presented Flow of Solution Procedure (FSP) to transfer
the inference procedure of the teacher rather than the inter-
mediate layer results. The FSP matrix is actually the inner

7097

Teacher Network

s
s
r
r
e
e
y
y
a
a

l
l
 
 

m
m
e
e
t
t
S
S

N1 blocks
N1 blocks

Nn blocks
Nn blocks

Transformation

Transformation

Student Network

LIRG-t

Transformation

Transformation

N1  blocks
N1  blocks

Nn  blocks
Nn  blocks

s
s
r
r
e
e
y
y
a
a

l
l
 
 

m
m
e
e
t
t
S
S

Supervision

Nn+ 1 blocks

2
2
-
-
K
K
k
k
c
c
o
o
B
B

l
l

1
1
-
-
K
K
k
k
c
c
o
o
B
B

l
l

K
K
k
k
c
c
o
o
B
B

l
l

Softmax

s
s
t
t
i
i
g
g
o
o

l
l

LIRG

Llogits

2
2
-
-
 
 

K
K
k
k
c
c
o
o
B
B

l
l

1
1
-
-
 
 

K
K
k
k
c
c
o
o
B
B

l
l

 
 

K
K
k
k
c
c
o
o
B
B

l
l

Nn+ 1  blocks

Softmax

s
s
t
t
i
i
g
g
o
o

l
l

s
s
l
l
e
e
b
b
a
a
L
L

LGT

n
n
o
o
i
i
t
t
c
c
i
i
d
d
e
e
r
r
P
P

n
n
o
o
i
i
t
t
c
c
i
i
d
d
e
e
r
r
P
P

Figure 2: Overall framework of the proposed method.

production of the feature channels from two layers, which
is regarded as the ﬂow for solving a problem. However, the
FSP matrix can only be computed between two layers with
the same output resolution. Besides, the computational cost
of FSP is rather high.

3. Proposed Method

In this section, the overall framework of the proposed
method is ﬁrstly introduced. Then a knowledge graph called
IRG and its transformation are constructed for represent-
ing general, moderate and sufﬁcient knowledge. Subse-
quently, the hint losses about IRG and its transformation
are formulated to utilize the mined knowledge. Finally, the
overall loss is formulated based on the previous loss func-
tions to supervise the training of the student network.

The overall framework of the proposed method is illus-
trated in Figure 2. The upper blue network is the teacher
network, while the lower orange network is the student net-
work. Except the SoftmaxLoss LGT from the ground truth,
three supervision signals are added to transfer the distilled
knowledge, including LIRG, Llogits and LIRG-t. All of
the three signals are derived from IRG, which represents
the feature space of a certain layer. Speciﬁcally, LIRG is
used to transfer the instance features and the instance rela-
tionships. Llogits represents the instance features and is a
special case of LIRG. It can be absorbed into LIRG. And
LIRG-t distills the feature space transformation knowledge.
Eventually, the three loss functions make up Multi-Type
Knowledge (MTK) loss (LM T K ), which transfers all the
three types of knowledge from the teacher to the student.

3.1. Instance Relationship Graph
Given I training instances x = {xi}I

i=1, let fl(xi) be the
instance features of xi at l-th layer, which can be the ﬁnal
softened outputs [9] or the feature maps [26]. The instance
relationships are formulated as an adjacent matrix of the in-
stance features, referring to as Al. An exmaple of IRG is
shown in Figure 3(a). Then an IRG denoted as IRGl is
constructed to represent the feature space of the l-th layer,
expressed as

(a)

(b)

Figure 3: Structure of IRG. (a) An example of IRG. (b) An
example of IRG transformation.

IRGl = (Vl, El) = ({fl(xi)}I
Al(i, j) = ||fl(xi) − fl(xj)||2

i=1, Al),
2, i, j = 1, ..., I,

(1)

where Vl is the vertex set of IRG representing the instance
features at the l-th layer, El is the edge set of IRG represent-
ing the instance relationship. Each element of the feature
relationship matrix, Al, represents an edge. And each edge
is deﬁned as the Euclidean distance between the instance
features of two linked instances as shown in Equation 1.

Based on the formulation of IRG, its transformation is
deﬁned. Let IRG-tl1l2 be the IRG transformation from the
l1-th layer to the l2-th layer. As shown in Figure 3(b), it
is natural to decompose IRG-tl1l2 into the vertex trans-
formation (or called the instance feature transformation)
T rans(Vl1 , Vl2 ) and the edge transformation (or called
the instance relationship transformation) T rans(El1 , El2 ),
namely

IRG-tl1l2 = T rans(IRGl1 , IRGl2 )

= (T rans(Vl1 , Vl2 ), T rans(El1 , El2 ))
= (Λl1,l2 , Θl1,l2 ),

(2)

Λl1,l2 (i, i) = ||fl1 (xi) − fl2 (xi)||2
2,
Θl1,l2 = ||Al1

− Al2 ||2
2,

i = 1, ..., I,

where T rans(·) is the transformation function, Λl1,l2 and
Θl1,l2 are the vertex transformation matrix and edge trans-
formation matrix, respectively. As shown in Equation 2,
each element of Λl1,l2 represents the instance feature trans-
formation of the same instance xi from one layer to an-
other. Similarly, Θl1,l2 is deﬁned as the Euclidean dis-
tance between the two relationship matrixes Al1 and Al2 .

7098

Layer l1Layer l2Trans(Vl2, Vl1)Trans(El1, El2)...Teacher(cid:1)

Student(cid:1)

Teacher(cid:1)

Student(cid:1)

Layer1(cid:1)

…
(cid:1)

Layern-1(cid:1)

Layern(cid:1)

Layer1(cid:1)

Layer1(cid:1)

…
(cid:1)

…
(cid:1)

Layerm-1(cid:1)

Layern-1(cid:1)

Layer1(cid:1)

…
(cid:1)

Layerm-1(cid:1)

Layerm(cid:1)

Layern(cid:1)

Layerm(cid:1)

type of knowledge. Further, adopting the knowledge dis-
tillation densely for intermediate layers is not a moderate
constraint for the student. Therefore, the vertex difference
is only deployed for the logits layers. Consequently, LIRG
in this work is obtained as follows:
LIRG(x) = λ1 · Llogits(x) + λ2 ·X

lM ||2
2.

||AT
L

− AS

(4)

(a) One-to-one mode for LIRG

(b) One-to-many mode for LIRG

Figure 4: Two possible deploy modes for LIRG.

Then IRG-tl1l2 contains the knowledge of the feature space
transformation from the l1-th layer to the l2-th layer.

3.2. Loss for IRG

lM ∈LM
3.3. Loss for IRG Transformation.

IRG transformation is the representation of the instance
feature space transformation, consisting of vertex transfor-
mation and edge transformation. Therefore, the loss LIRG-t
also contains two parts as shown as follows:

The loss LIRG is deﬁned as the difference between the
L be the IRG of

teacher’s IRG and the student’s. Let IRGT
the teacher network at the L-th layer. Similarly, IRGS
lM
is the lM -th layer’s IRG in the student network. The for-
mulations of the two IRGs follow Equation 1. Then, the
difference of the two IRGs is divided into the difference of
the vertexes Dist(V T
lM ) and the difference of the edges
Dist(E T
lM ). Both parts are evaluated by Euclidean dis-
tance as follows:

L , V S

L , E S

LIRG(x) = Dist(IRGT
= λ1 · Dist(V T

L , V S

L, IRGS

lM )

lM ) + λ2 · Dist(E T

L , E S

lM )

= λ1 ·

I

X

||f T

L (xi) − f S

lM (xi)||2

2

(3)

i=1
+ λ2 · ||AT
L

− AS

lM ||2
2.

Note that λ1 and λ2 are the penalty coefﬁcients balanced
the two terms. Most previous works only considering the
instance features, and they can be regarded as a special
case of the IRG-based method by setting λ2 to be zero.

To fully utilize the effectiveness of LIRG when applying
it to a speciﬁc task, there are two factors that may inﬂuence
the performance.

First, there are two possible deploy modes for LIRG as
shown in Figure 4. In particular, under one-to-one mode,
the selected layers of the student is supervised by the cor-
responding layers of the teacher network. It is obvious that
one-to-one mode performs the best when the teacher and
student shares the network structure. On the other hand,
one-to-many mode utilizes the last layer of the teacher (L)
to guide the selected layers (LM ) of the student. Since
the last layer usually learns the general distribution of the
dataset, IRG of the last layer is less correlated with the net-
work design. Since one-to-many mode extracts more gen-
eral knowledge, the formulation of LIRG in Equation 4 fol-
lows this mode.

Second, the vertex difference Dist(V T

lM ) in Equa-
tion 3 can be computed only if f T
lM (xi) have the
same feature resolution and feature channel number. How-
ever, this can not be satisﬁed under most (L, lM ) combina-
tions, which indicates the edge difference is not a general

L (xi) and f S

L , V S

LIRG-t(x)
= Dist(IRG-tT
= Dist(T rans(V T
+ Dist(T rans(E T
= ||ΛT

− ΛS

l1,l2

l1l2 , IRG-tS

l3l4 )

l1 , V T
l1 , E T

l2 ), T rans(V S
l2 ), T rans(E S
2 + ||ΘT

l3 , V S
l4 ))
l3 , E S
l4 ))
l3,l4 ||2
2,

− ΘS

l1,l2

l3,l4 ||2

(5)

l3,l4

l1,l2

l1,l2

l3,l4

and ΘS

and ΘT

where ΛT
are the vertex and edge transfor-
mation of the teacher from the l1-th layer to the l2-th layer,
while ΛS
together represent the feature space
transformation of the student. Then ||ΛT
2 and
||ΘT
2 are adopted to evaluate the vertex trans-
formation difference and the edge transformation difference
between the teacher and the student. Similar to LIRG, there
is also an important factor inﬂuencing the performance of
LIRG-t.

l3,l4 ||2

l3,l4 ||2

− ΘS

− ΛS

l1,l2

l1,l2

The edge transformation part consumes much more com-
putation resources compared with the vertex part. To be
speciﬁc, for an IRG with I vertices, time complexity of the
vertex part is O(I), while that of the edge part is O(I 2). In
addition, the distilled knowledge of the vertex transforma-
tion and the edge transformation is redundant. Therefore,
the edge part of IRG transformation loss is omitted for the
sake of effectiveness. Finally, the resulting IRG transforma-
tion loss function is formulated as follows:

LIRG-t(x) = ||ΛT

l1,l2

− ΛS

l3,l4 ||2
2.

(6)

3.4. Multi-Type Knowledge Loss

We deﬁne a MTK loss (LM T K ) to train the student net-
work.It is formulated based on the SoftmaxLoss for Ground
Truth (GT) (LGT ), loss for IRG (LIRG) and loss for IRG
transformation (LIRG-t) as follows:

LM T K(x)
= LGT (x) + LIRG(x) + λ3 · LIRG-t(x)
= LGT (x) + λ1 · Llogits(x)

+ λ2 · X

||AT
L

lM ∈LM

− AS

lM ||2

2

(7)

+ λ3 · X

||ΛT

l1,l2

− ΛS

l3,l4 ||2
2,

l1l2l3l4∈LTran

7099

91

90.5

90

Y
C
A
R
U
C
C
A

89.5

89

88.5

88

87.5

87

0 .5

16
64
Baseline

32
128
Rocket

0 .0 5

0 .0 0 5

0 .0 0 0 5

(a)

(b)

Figure 5: (a) Performance of LIRG.
LM T K .

(b) Performance of

in which λ1, λ2 and λ3 are the three penalty coefﬁcients,
while LM and LTran represent the layer set for IRG and
its transformation, respectively. Using the MTK loss, the
student network can be optimized to acquire all the three
types of knowledge from the teacher network.

4. Experiments

4.1. Ablation Analysis

In this section, experiments are conducted to verify the
effectiveness of LIRG and LIRG-t. The detailed experimen-
tal settings are as below.

4.1.1 Experiment Settings

CIFAR10 [12] is adopted as our training and test dataset
for ablation analysis. Images are ﬁrst padded to 36 × 36
and then cropped to 32 × 32 for training. ResNet20 [6]
or ShufﬂeNet-x0.5 [28] is adopted as the teacher network,
while we reduce the channels of ResNet20 by half to obtain
the student network named ResNet20-x0.5. Note that the
“Baseline” is the ResNet20-x0.5 directly trained by LGT .

4.1.2 The Effectiveness of LIRG

The hyper-parameters of LIRG are ﬁrst decided according
to the experiments. After this, two deploy modes including
one-to-one mode and one-to-many mode are compared and
analyzed.

(1) Hyper-parameter Tuning. Besides the coefﬁcient
λ2, batch size is also a crucial hyper-parameter, since the
instance relationship matrix, namely Al, is computed by a
batch of instances (see Section 3.1). Al with a larger batch
size contains more instance relationships as well as more
comprehensive knowledge. In the meantime, it may be a
harder regularization for the student. In order to achieve a
trade-off between extracting moderate knowledge and ex-
tracting sufﬁcient knowledge, experiments are conducted
under different settings as shown in Figure 5(a). It can be
seen that LIRG outperforms the baseline and Rocket for
most of the cases. According to the results, we choose batch
size as 64 and λ2 as 0.005 for LIRG for the rest of the paper.
(2) Performance Analysis of One-to-one Mode and
One-to-many Mode. Under one-to-one mode, one layer’s
IRG from the teacher is used to guide a corresponding

Table 1: Student performance under different modes of
LIRG. O2O refers to one-to-one mode, while O2M refers
to one-to-many mode.

Student/Teacher: ResNet20-x0.5(88.36) / ResNet20(91.45)

O2O 1layer
O2M 1layer

89.87
89.87

O2O 3layers
O2M 3layers

90.03
90.28

O2O 5layers
O2M 5layers

89.65
90.02

Student/Teacher: ResNet20-x0.5(88.36) / ShufﬂeNet-x0.5(91.47)

O2O 1layer
O2M 1layer

89.83
89.83

O2O 3layers
O2M 3layers

89.89
90.21

O2O 5layers
O2M 5layers

89.50
89.93

(a) Eltwise1

(b) Eltwise5

(c) Eltwise8

(d) Eltwise9

Figure 6: Feature visualizations at different layers of the
teacher network.

layer’s IRG of the student (as shown in Figure 4(a)). For
one-to-many mode, last layer’s IRG is selected as the su-
pervision for the student’s last several layers (as shown in
Figure 4(b)).

Experiments are conducted under different conﬁgura-
tions for both modes. The results are shown in Table 1.
For example,“O2M 3layer” (one-to-many mode with 3 lay-
ers) refers to the situation that teahcer’s last Eltwise layer
(Eltwise9) is selected to supervise the last 3 Eltwise layers
(Eltwise7-9) of the student, while“O2O 3layer” (one-to-one
mode with 3 layers) refers to the situation that teacher’s last
3 Eltwise layers supervise the corresponding 3 layers of the
student, respectively.

According to the results, both one-to-one model and one-
to-many mode outperform baseline by a signiﬁcant margin
while one-to-many mode continuously outperforms one-to-
one mode. Figure 6 visualizes the feature maps of the
teacher at different layers. It can be observed that deeper
layers learn more discriminative and general features, and
the last Eltwise layer with the best discrimination is the
most suitable supervision for the student network. There-
fore, one-to-many mode, which always extracts knowledge
from teacher’s Eltwise9 layer, forces all the supervised lay-
ers of the student to beneﬁt from the discriminative feature
space and thus outperforms one-to-one mode.

Furthermore, one-to-many mode is more robust to the
teacher-student pair changes. As shown in Table 1, when
the teacher network changes from ResNet20 to ShufﬂeNet-
x0.5, one-to-one mode suffers from performance drop while
the performance of one-to-many mode is rather stable. It
is because Eltwise9 learns the general distribution of the
dataset, which is less related to the network architecture. On
the contrary, the feature space of the shallower layers such
as Eltwise7 are closely related to the network architecture.
Thus when the teacher has a totally different design from
that of the student, one-to-one mode performs much worse

7100

8787.58888.58989.59090.5910.50.050.0050.0005ACCURACY163264128BaselineRocketTable 2: Model performance of different methods. Perfor-
mance gain over the best competing method is marked in
the brackets.

CIFAR10

Baseline

88.36

KD
FSP
AT

Rocket†

89.09
89.21
89.15
89.35

CIFAR100

CIFAR100

coarse

72.51

73.03
73.18
73.15
73.39

ﬁne

59.88

60.21
60.46
60.58
60.88

LIRG
LM T K

90.28 (0.93)
90.69 (1.34)

74.32 (0.93)
74.64 (1.25)

61.93 (1.05)
62.25 (1.37)

Teacher

91.45

78.40

68.42

than one-to-many mode. For the rest of the paper, “O2M
3layer” mode is always adopted for LIRG, which exceeds
the baseline by 1.92%.

4.1.3 The Effectiveness of LIRG-t

Besides the instance features and instance relationships
stored in IRG, the transformation of IRGs is also an im-
portant type of knowledge. Therefore, LIRG-t and LIRG
are combined to obtain LM T K . By comparing the perfor-
mance of LIRG and LM T K , the effectiveness of LIRG-t can
be veriﬁed.

(1) Hyper-parameter Tuning. Just as LIRG, the regu-
larization strength of LIRG-t is controlled by batch size and
λ3. The relationship between the two factors and accuracy
is shown in Figure 5(b). LIRG-t is less sensitive to batch
size as well as the penalty factor, compared with LIRG.
Therefore, it takes limited time to ﬁnd an appropriate
λ3 for LM T K . Consequently, based on the results in Figure
5(b), we choose batch size as 64 and λ3 as 0.005 for LIRG-t
for the rest of the paper.

(2) Performance Analysis of LIRG-t. LIRG-t considers
the transformations of multiple pairs of layers. In particular,
for ResNet20, three pairs of layers are utilized as supervi-
sions, each of which represents the feature space transfor-
mation under a certain feature map resolution. In this way,
the overall LIRG-t boosts the feature learning process from
the beginning of the network to its end, so as to reinforce
the model performance.

As shown in Figure 5, LM T K continuously outperforms
LIRG, which indicates the effectiveness of LIRG-t. In par-
ticular, LM T K achieves an accuracy of 90.69% on CI-
FAR10, obtaining a performance gain of 0.41% (2.33%)
over LIRG (baseline). Furthermore, as shown in Figure 8,
with the help of LIRG-t, LM T K shows more stable conver-
gence on the test loss and accuracy. It is because LIRG-t
considers the global information ﬂow of the network and is
a more moderate constraint.

4.2. Performance Comparisons

In this section, we compare the proposed method with 4
state-of-the-arts, including KD [9], FSP [24], AT [26] and

Rocket [30] (using logits as the distilled knowledge). First,
the performance of different methods is evaluated on CI-
FAR10, CIFAR100-coarse and CIFAR100-ﬁne. Secondly,
different teacher-student pairs are implemented to evalu-
ate the methods’ generalization ability on network archi-
tectures. Finally, we particularly conduct experiments on
ImageNet and a subset of CIFAR10, called CIFAR10-small
to show the superiority of the proposed method on datasets
with different scales. The detailed experiment settings are
as below.

4.2.1 Experiment Settings

CIFAR10, CIFAR100 [12], ImageNet [4] and CIFAR10-
small are used for performance evaluation. Note that 10%
of CIFAR10 are randomly sampled to obtain CIFAR10-
small. Two types of teacher networks and three types
of student networks are used for performance evalua-
tion.
Speciﬁcally, ResNet20 and ShufﬂeNet-x0.5 [28]
are the two teachers. Besides ResNet20-x0.5, ResNet20-
x0.375 and ResNet14-x0.5 are the possible student net-
works. ResNet14-x0.5 is obtained by reducing 3 residual
blocks from ResNet20-x0.5, while ResNet20-x0.375 has
0.375 time of channels of ResNet20.

4.2.2 Evaluation on CIFAR10 and CIFAR100

CIFAR10 and CIFAR100 are two typical datasets for
knowledge distillation evaluation. In this section, ResNet20
and ResNet20-x0.5 are used as the teacher network and the
student network respectively. When training the networks,
images of CIFAR10 (CIFAR100) are ﬁrst padded to 36 ×36
and then cropped to 32 × 32. Furthermore, the training-test
division strictly follows the ofﬁcial protocol.

According to the results in Table 2, the proposed method
signiﬁcantly outperforms all the competing methods. To
be speciﬁc, LIRG outperforms Rocket, the best compet-
ing method, by 0.93% to 1.05% on different datasets. By
taking both IRG and IRG transformations into considera-
tion, LM T K outperforms Rocket by a larger margin from
1.25% to 1.37%. Since the performance gain of Rocket
over the baseline is 0.77% to 1.0%, the proposed method
(1.60% to 2.43%) doubles this performance gain.

We attribute the signiﬁcant performance improvement to
LM T K ’s extraction of all the three types of knowledge. The
previous methods only consider a subset of knowledge. For
example, KD, AT and Rocket only extract instance features
from the teacher, while FSP only takes feature transforma-
tion into consideration. Thus, these competing methods are
all special cases of our method. Further, none of them uses
instance relationships as the distilled knowledge. Accord-
ing to the experiments, the instance relationships not only
extract sufﬁcient knowledge from the teacher, but also make
the knowledge distillation process more robust to the net-
work designs.
In addition, the loss functions making up

7101

(a) KD

(b) AT

(c) FSP

(d) Rocket

(e) LM T K

Figure 7: Feature visualizations of Eltwise9 layer for different methods. Each color represents a class, best viewed in color.

Table 3: Model performance of different teacher-student pairs. Note that Rocket† adopts logits as the distilled knowledge
and shares lower layers of the teacher and the student. Therefore, the model size of Rocket† is a little larger than the reported
one. The number in the brackets shows the performance increase over the best competing method.

Teacher Net.

Student Net.

Baseline

KD

FSP

AT

Rocket†

LIRG

LM T K

Teacher

Dataset

CIFAR10

CIFAR100-coarse

ResNet20 (1.06M)
ResNet20 (1.06M)
ResNet20 (1.06M)

ResNet20-x0.5 (0.28M)
ResNet14-x0.5 (0.18M)

ResNet20-x0.375 (0.16M)

ShufﬂeNet-x0.5 (0.94M)

ResNet20-x0.5 (0.28M)

ResNet20 (1.06M)
ResNet20 (1.06M)
ResNet20 (1.06M)

ResNet20-x0.5 (0.28M)
ResNet14-x0.5 (0.18M)

ResNet20-x0.375 (0.16M)

ShufﬂeNet-x0.5 (0.94M)

ResNet20-x0.5 (0.28M)

CIFAR10-ﬁne

ResNet20 (1.06M)
ResNet20 (1.06M)
ResNet20 (1.06M)

ResNet20-x0.5 (0.28M)
ResNet14-x0.5 (0.18M)

ResNet20-x0.375 (0.16M)

ShufﬂeNet-x0.5 (0.94M)

ResNet20-x0.5 (0.28M)

LM T K are carefully designed, so as to achieve a good trade-
off among generalization, sufﬁciency and moderation of the
distilled knowledge. Therefore, LIRG and LIRG-t are com-
plementary to each other, boosting the performance in har-
mony.

Figure 7 visualizes the distribution of Eltwise9 layer
of the student networks from different methods. The fea-
ture space of LM T K is signiﬁcantly more separable than
those of the other methods, especially on class bound-
aries. Moreover, different classes are well clustered with
smaller inner-class variation and larger inter-class variation.
LM T K makes usage of three kinds of knowledge from the
teacher network, which enables it to learn more compact
and discriminative representations. On the contrary, previ-
ous methods only utilizes single type of knowledge, thus the
expression ability of the student network is limited.

4.2.3 Evaluation on Different Networks

In this subsection, different teacher-student pairs are ex-
plored. The experimental results are reported in Table 3.
Under different network settings, the proposed method con-
tinuously outperforms the other methods. And we ﬁnd that
the robustness to the network designs is related to the type
of the distilled knowledge.

First, some of the methods are more sensitive to the
changes of the teacher-student pairs. For instance, FSP and
AT perform worse when the teacher and student have dif-
ferent network architectures. It is because FSP and AT ex-
tract network-related knowledge. FSP extracts feature space
transformation knowledge while AT extracts attention maps
of middle layers. The network-related knowledge is hard to

88.36
86.65
86.54
88.36

72.51
68.55
66.72
72.51

59.88
56.23
53.87
59.88

89.09
87.01
87.23
89.12

73.03
68.76
66.98
72.96

60.21
56.44
54.09
60.15

89.17
87.23
87.11
89.07

73.18
68.73
67.07
72.87

60.46
56.34
54.24
60.23

89.29
87.12
87.39
89.05

73.15
68.69
67.22
72.99

60.58
56.26
54.38
60.31

89.45
87.53
87.67
89.22

73.39
69.07
67.45
73.27

60.88
56.55
54.52
60.97

90.28 (0.93)
88.55 (1.02)
88.52 (0.85)
90.29 (1.07)

90.69 (1.34)
89.08 (1.55)
89.01 (1.34)
90.65 (1.43)

74.32 (0.93)
69.94 (0.87)
68.26 (0.81)
74.22 (0.95)

74.64 (1.25)
70.18 (1.11)
68.57 (1.12)
74.56 (1.29)

61.93 (1.05)
57.44 (0.89)
55.37 (0.85)
61.83(0.86)

62.25 (1.37)
57.68 (1.13)
55.66 (1.14)
62.06 (1.09)

91.45
91.45
91.45
91.47

78.40
78.40
78.40
78.69

68.42
68.42
68.42
68.67

transform from the teacher to a student with a different net-
work design.

Second, KD, Rocket, LIRG and LM T K are relatively
more robust to the teacher-student pair changes. For ex-
ample, though ResNet20 and ShufﬂeNet-x0.5 have totally
different architectures, all the four methods perform stably
when the teacher network changes. It is because these meth-
ods utilize the knowledge that is not closely related to the
network architecture. Speciﬁcally, KD and Rocket distill
the predicted class probabilities of the teacher, while LIRG
learns instance relationship. Since the class probabilities
and the learned instance relationships are usually stable,
KD, Rocket and LIRG are able to work robust to the net-
work changes. As for LM T K , which consists of three types
of distilled knowledge, is also robust to different networks.
When one of the type, for example, feature transformation
type, works a little worse, the other two types still perform
well. Thus the overall performance does not signiﬁcantly
decrease.

4.2.4 Evaluation on CIFAR10-small and ImageNet

To explore the effectiveness of the proposed method on
datasets with different scales, experiments are conducted on
CIFAR10-small and ImageNet. According to the results in
Table 4, both LIRG and LM T K continuously outperform
the competing methods, especially on CIFAR10-small.

CIFAR10-small. For real world applications, there are
usually limited labeled images in hand.
It is necessary
to evaluate the model performance on small-scale dataset.
Therefore, CIFAR10-small is constructed by randomly se-
lecting 10% samples from the training set CIFAR10. Then

7102

Table 4: Model performance on CIFAR10-small and ImageNet. We randomly select 10% of the training instances for 10
times and the average performance is reported.

Dataset

Teacher Net.

Student Net.

Baseline

KD

FSP

AT

Rocket†

LIRG

LM T K

Teacher

CIFAR10-small

ResNet20
ResNet20

ResNet14-x0.5

ResNet20-x0.375

ImageNet

ResNet101-v2
ResNet101-v2

ResNet18
ResNext26

55.53
57.32

70.83
74.89

59.29
62.83

71.43
75.60

60.11
63.21

71.28
75.62

59.98
63.52

71.58
75.73

62.23
64.14

71.93
76.16

64.87 (2.64)
66.96 (2.82)

66.04 (3.81)
68.16 (4.02)

72.68 (0.75)
76.87 (0.71)

73.06 (1.13)
77.18 (1.02)

91.45
91.45

78.05
78.05

(a)

(b)

Figure 8: Test loss and accuracy comparisons.

all the student networks are trained on CIFAR10-small and
the accuracy on the test set of CIFAR10 is reported in Table
4. Note that the teacher network is still trained on the full
training set of CIFAR10.

According to the results, the performance gain (the num-
bers in the brackets) is tripled compared with the original
CIFAR10 settings. We attribute it to the ability to extract
sufﬁcient knowledge of the proposed method. Previous
works extract knowledge from independent instances from
the teacher network. Thus, the amount of their knowledge
is proportional to the number of training samples N . The
knowledge is very limited when there are only a few train-
ing samples. LIRG and LM T K , by digging N instance fea-
tures and N 2 instance relationships stored in IRG, extract
much more knowledge from the teacher.

ImageNet. Experiments are conducted on ImageNet to
show the efﬁcienctiveness of the proposed method on large-
scale dataset. Since ImageNet consists millions of high-
resolution images, ResNet101-v2 is used as the teacher net-
work while ResNet18 and ResNext-26 are introduced as the
students. For training, images are ﬁrst resized to 299 × 299
and randomly cropped to 224 × 224. As shown in Figure 3,
LIRG and LM T K outperform competing methods by a sig-
niﬁcant margin. It indicates the proposed method is efﬁcient
on large-scale dataset.

4.2.5 Complexity Analysis

Since the proposed method computes IRGs and IRG trans-
formations, it takes extra training time and GPU memory.
In this section, the complexity of the algorithm is analyzed.
According to the experiments, the additional resource cost
is limited under different experimental settings. In partic-
ular, the extra time and memory are proportional to batch
size and number of feature channels. In other words, once
the batch size and feature channel are ﬁxed, the additional
training time and GPU memory is a constant. To be spe-
ciﬁc, for CIFAR10, it takes 3-4 hours to train a student with

LM T K , while the typical time of Rocket is 1.2 hours and
the typical time of baseline is around an hour. For Ima-
geNet, LM T K just takes around 4 hours more compared
with the one-week baseline process. On the other hand, the
extra GPU memory cost is around 100M for both CIFAR10
and ImageNet. Therefore, the proposed method can be eas-
ily deployed for real world applications with a little extra
training resource cost but signiﬁcant performance gain.

Though additional loss functions are introduced, the pro-
posed method takes similar or less epochs to converge, com-
pared with the best competing method. As shown in Figure
8, both LIRG and LM T K achieve lower test loss and higher
accuracy compared with Rocket, under the same training
conﬁguration. Furthermore, LM T K shows more stable con-
vergence since LIRG-t introduces the moderate knowledge,
namely feature space transformation.

5. Conclusion

We ﬁnd that knowledge can be divided into three types:
instance features, instance relationships and feature space
transformation. However, most recent works only concen-
trate on the instance features. In this paper, an Instance Re-
lationship Graph (IRG) is deﬁned to preserve all the types
of knowledge.
IRG-based knowledge distillation method
is then proposed and hint loss functions corresponding to
different types of knowledge are presented to optimize the
student network. The experiments verify that the proposed
method shows strong robustness to teacher-student architec-
ture changes. In addition, it shows superior performance on
both big-scale and small-scale datasets over existing meth-
ods.

Acknowledgement. This work is partly supported by
the National Key R&D Plan (Nos.
2017YFB1002801
and 2016QY01W0106), the Natural Science Foundation of
China (Nos. U1803119, U1736106, 61751212, 61721004,
61772225 and 61876100),
the NSFC-General Technol-
ogy Collaborative Fund for Basic Research (Grant No.
U1636218), the Key Research Program of Frontier Sci-
ences, CAS (Grant No. QYZDJ-SSW-JSC040), Beijing
Natural Science Foundation (Nos. JQ18018, L172051 and
L182058) and the CAS External Cooperation Key Project.
Bing Li is also supported by Youth Innovation Promotion
Association, CAS.

7103

[18] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In European Conference on Computer Vision,
pages 525–542. Springer, 2016.

[19] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv
preprint arXiv:1412.6550, 2014.

[20] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning
face representation by joint identiﬁcation-veriﬁcation.
In
Advances in neural information processing systems, pages
1988–1996, 2014.

[21] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, volume 4, page 12, 2017.
[22] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
structured sparsity in deep neural networks. In Advances in
Neural Information Processing Systems, pages 2074–2082,
2016.

[23] J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng. Quantized
convolutional neural networks for mobile devices.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4820–4828, 2016.

[24] J. Yim, D. Joo, J. Bae, and J. Kim. A gift from knowl-
edge distillation: Fast optimization, network minimization
and transfer learning. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), volume 2, 2017.

[25] R. Yu, A. Li, V. I. Morariu, and L. S. Davis. Visual relation-
ship detection with internal and external linguistic knowl-
edge distillation. In IEEE International Conference on Com-
puter Vision (ICCV), 2017.

[26] S. Zagoruyko and N. Komodakis.

Paying more atten-
tion to attention: Improving the performance of convolu-
tional neural networks via attention transfer. arXiv preprint
arXiv:1612.03928, 2016.

[27] S. Zagoruyko and N. Komodakis. Wide residual networks.

arXiv preprint arXiv:1605.07146, 2016.

[28] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An
extremely efﬁcient convolutional neural network for mobile
devices. 2017.

[29] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In Advances in neural information processing sys-
tems, pages 487–495, 2014.

[30] G. Zhou, Y. Fan, R. Cui, W. Bian, X. Zhu, and K. Gai. Rocket
launching: A universal and efﬁcient framework for training
well-performing light net. In AAAI Conference on Artiﬁcial
Intelligence, volume 1050, page 8, 2018.

References

[1] J. Ba and R. Caruana. Do deep nets really need to be deep?
In Advances in neural information processing systems, pages
2654–2662, 2014.

[2] P. Ballester and R. M. de Araújo. On the performance of
googlenet and alexnet applied to sketches. In AAAI, pages
1124–1128, 2016.

[3] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and
Y. Bengio. Binarized neural networks: Training deep neu-
ral networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016.

[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. Ieee, 2009.

[5] S. Han, H. Mao, and W. J. Dally.

Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015.

[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. CoRR, abs/1512.03385, 2015.

[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[8] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerat-
ing very deep neural networks. In International Conference
on Computer Vision (ICCV), volume 2, 2017.

[9] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[10] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In CVPR, vol-
ume 1, page 3, 2017.

[11] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and< 0.5 mb model size. arXiv
preprint arXiv:1602.07360, 2016.

[12] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 and cifar-
100 datasets. URl: https://www. cs. toronto. edu/kriz/cifar.
html (vi sited on Mar. 1, 2016), 2009.

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.

[14] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P.
Graf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint
arXiv:1608.08710, 2016.

[15] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.
Learning efﬁcient convolutional networks through network
slimming. In Computer Vision (ICCV), 2017 IEEE Interna-
tional Conference on, pages 2755–2763. IEEE, 2017.

[16] L. Lu, M. Guo, and S. Renals. Knowledge distillation for
small-footprint highway networks. In Acoustics, Speech and
Signal Processing (ICASSP), 2017 IEEE International Con-
ference on, pages 4820–4824. IEEE, 2017.

[17] J.-H. Luo and J. Wu. An entropy-based pruning method for

cnn compression. arXiv preprint arXiv:1706.05791, 2017.

7104

