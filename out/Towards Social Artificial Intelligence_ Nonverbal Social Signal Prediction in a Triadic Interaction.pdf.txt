Towards Social Artiﬁcial Intelligence:

Nonverbal Social Signal Prediction in A Triadic Interaction∗

Hanbyul Joo1† Tomas Simon1‡ Mina Cikara2 Yaser Sheikh1

1Carnegie Mellon University 2Harvard University

{hjoo, tsimon, yaser}@fb.com, mcikara@fas.harvard.edu

Abstract

We present a new research task and a dataset to under-
stand human social interactions via computational meth-
ods, to ultimately endow machines with the ability to encode
and decode a broad channel of social signals humans use.
This research direction is essential to make a machine that
genuinely communicates with humans, which we call Social
Artiﬁcial Intelligence. We ﬁrst formulate the “social signal
prediction” problem as a way to model the dynamics of so-
cial signals exchanged among interacting individuals in a
data-driven way. We then present a new 3D motion capture
dataset to explore this problem, where the broad spectrum
of social signals (3D body, face, and hand motions) are cap-
tured in a triadic social interaction scenario. Baseline ap-
proaches to predict speaking status, social formation, and
body gestures of interacting individuals are presented in the
deﬁned social prediction framework.

1. Introduction

Consider how humans communicate—we use language,
voice, facial expressions, and body gestures to convey our
thoughts, emotions, and intentions. Such social signals that
encode the messages of an individual are then sensed, de-
coded, and ﬁnally interpreted by communication partners.
Notably, the use of all these channels is important in so-
cial interactions, where subtle meanings are transmitted via
the combination of such signals. Endowing machines with
the similar social interaction ability that encodes and de-
codes a broad spectrum of these social signals is an es-
sential goal of Artiﬁcial Intelligence (AI) to make them
to effectively cooperate with humans. We use the term,
Social Artiﬁcial Intelligence (AI), to refer to the machine
with such ability. A way to build the Social AI would be
to encode all the rules that humans observe during social

∗Website: http://domedb.perception.cs.cmu.edu/ssp
†H. Joo is now at Facebook AI Research (FAIR)
‡T. Simon is now at Facebook Reality Labs

communication [11]. Unfortunately, nonverbal interaction
is still poorly understood despite its importance in social
communication [45, 44, 6], making it hard to formalize
rules about how to understand and use social signals. In-
terestingly, we have recently witnessed a great achievement
in Natural Language Processing showing the potential to
make machines “freely” communicate with humans using
language [70]. This success has been led by data-driven
approaches leveraging large-scale language datasets and a
powerful modeling tool, deep neural networks, to automat-
ically learn the patterns of human verbal communication.
Remarkably, these achievements have not made extensive
use of the prior knowledge about grammar and the structure
of languages that linguists have accumulated over centuries.
Motivated by this, we hypothesize that a similar approach
can be applicable in modeling nonverbal communication.

However, there exists a fundamental difﬁculty in build-
ing a data-driven nonverbal communication model: the data
is extremely rare.
In the verbal language domain, words
contain the full expressive power to record verbal signals by
the composition of a handful of discrete symbols, and there
already exist millions of articles, dialogues, and speech au-
dio on the Internet which are readily usable for data-driven
methods. However, for non-verbal signals, how to “record”
or collect these signals is uncertain.
Imagine a situation
where a group of people are communicating in our daily
life. The positions and orientations of individuals, their
body gestures, gaze, and facial expressions (among oth-
ers) are the data we are interested in. Notably, these so-
cial signals emitted from all people in the group (i.e., the
signals from senders and receivers) need to be collected si-
multaneously to study their correlation and causality. Al-
though there also exist millions of videos where our daily
activities—including social interactions—are captured on
the Internet, these raw videos cannot be directly used to
understand the rules of non-verbal interactions because we
have to extract all semantic visual cues (relative location,
face, body pose, and so on) from the raw pixels.

In this paper, we introduce a new research domain with
the long-term aim to build machines that can genuinely in-

110873

teract with humans using non-verbal signals. The key idea
of this problem is to deﬁne the social interaction as a sig-
nal prediction problem among individuals, rather than ﬁnd-
ing the mapping between the social signals and their se-
mantics (e.g., as in affective computing or emotion recogni-
tion [53, 52, 54, 17]). We formulate that humans are com-
municating by receiving a set of social signals from others
and emitting response signals as output, which are again di-
rected to others as inputs (illustrated in Fig. 1). Thus, we
hypothesize that the social behavior ability can be learned
by modeling the dynamics between these signal ﬂows. The
advantages of this approach are: (1) we can tackle the prob-
lem by investigating the objectively measurable social sig-
nal data avoiding annotating the underlying meanings of
these signals and (2) it enables us to model subtle details
of social communication by considering the original con-
tinuous and high-dimensional signal space.

Importantly, we aim to include as many channels of
signals as possible to study the correlation between var-
ious channels of social signals, including facial expres-
sions, body gestures, body proximity, and body orienta-
tions. Since collecting these signals in-the-wild is hard due
to the limits of signal measurement technology (e.g., motion
capture) as well as the large variety of interaction contexts,
we collect a large-scale dataset in a studio setup [32, 33]
where these signals are markerlessly measured for natu-
rally interacting people. Our dataset includes about 180
sequences in a speciﬁc triadic social interaction scenario,
where 120 subjects participated. Directly investigating the
dynamics of the full spectrum of social signals is chal-
lenging due to the high complexity of the motion space,
requiring a much larger scale of data. We thus simplify
the problem by focusing on predicting lower dimensional,
yet important, output signals—speaking status and social
formations—emitted by the target person, while still con-
sidering broader channels including body motion and fa-
cial expressions as input. We found that this approach still
provides an important opportunity to computationally study
various channels of interpersonal social signals. Results on
our baseline social signal prediction models, implemented
by neural networks, demonstrate the strong predictive prop-
erty among social signals and also allow us to quantitatively
compare the relative inﬂuence among them. We further dis-
cuss more challenging prediction tasks, including body ges-
ture prediction and entire visual social signal prediction.

2. Background

Behavioral Science: Due to its importance in social
communication, nonverbal cues have received signiﬁcant
attention in psychology and behavioral science. Work in
this area is often categorized into diverse subﬁelds includ-
ing Proxemics, Kinesics, Vocalics, and Haptics [48]. In our
work, we focus only on Proxemics and Kinesics, which are

Figure 1. We aim to learn the dynamics between the input signals
X that an individual receives and the output social signals Y the
individual emits. The goal of social signal prediction is to regress
a function Y=F (X) in a data-driven manner.

closely related to visual cues. Hall ﬁrst introduced the con-
cept of Proxemics to describe how humans use their per-
sonal space during communications [26], and Kendon stud-
ied the spatial formations and orientations established when
multiple people communicate in a common space (named
F-formation) [35]. Facial expression in particular has re-
ceived lots of attention by researchers since the pioneering
work of Charles Darwin [14]. Ekman studied the relation
between emotions and facial expressions, and presented the
Facial Action Coding System (FACS), a system to describe
facial expressions using combinations of atomic units (Ac-
tion Units) [18]. Since then, this system remains the de-
facto standard to annotate and measure facial expressions,
and has had a broad impact in across many ﬁelds. Com-
pared to the face, body gestures remain relatively unex-
plored, even though research has substantiated the impor-
tance of body language in communication [15, 48, 43, 3].
Despite the efforts of many researchers in diverse ﬁelds,
little progress has been made in understanding nonver-
bal communications, and the approaches proposed several
decades ago are still the most widely used methods avail-
able [48]. In particular in this ﬁeld, researchers have been
using manual behavioral coding procedures that are several
decades old; however, most people are moving away from
manual behavioral coding towards automated coding.

Social Signal Processing: There has been increasing
interest in studying nonverbal communication using com-
putational methods [66]. Analyzing facial expression has
been a core focus in the vision community [12, 16, 60].
Many other methods to automatically detect social cues
from photos and videos have also been proposed, includ-
ing F-formation detection [59], recognizing proxemics from
photos [69], detecting attention [21], recognizing emotions
by body pose[58], and detecting social saliency [51]. Affec-
tive computing ﬁelds have been growing up rapidly, where
computer vision and other sensor measurements are used
with machine learning technique to understand human’s
emotion, social behavior, and roles [53].

Forecasting human motion: Predicting or forecast-
ing human motion is an emerging area in computer vision
and machine learning. Researchers propose approaches for
predicting pedestrian’s future trajectory [36] or forecast-

210874

ing human interaction in dyadic situations [29]. More re-
cently, deep neural networks are used to predict future 3D
poses from motion capture data [47, 22, 31], but they fo-
cus on periodic motions such as walk cycles. Recent work
tries to forecast human body motion in the 2D image do-
main [67, 65]. A few approaches address trajectory predic-
tion in social situations [27, 1, 25].

Social Signal Dataset: How to measure and collect non-
verbal signal data is important to pursue a data-driven ap-
proach for our goal. However, only few datasets contain so-
cially interacting group motion [2, 42, 37, 56]. The scenes
in these datasets are often in a table setup, limiting free body
movement and capturing the upper-body only. There are
datasets that capture more freely moving multiple people
(e.g., cocktail party) [71, 13, 20], but these only contain
location and orientation measurements for the people, in-
troduced to study the social formation only. Datasets pro-
viding rich 3D body motion information captured with mo-
tion capture techniques exist, but they contain single sub-
jects’ motion only [24, 30, 62]. More recently, however, full
body motion data of interacting groups using a large num-
ber of camera system was proposed for social interaction
capture [32, 33]. This work shows a potential in collecting
a large scale social interaction data without the usual issues
caused by wearing a motion capture suit and markers.

Measuring Kinesic Signals: Detecting human bodies
and keypoints in images has advanced greatly in computer
vision. There exist publicly available 2D face keypoint de-
tectors [5], body pose detectors [9, 68, 50], and hand pose
detectors [63]. 3D motion can be obtained by markerless
motion capture in a multiview setup [23, 40, 19, 33, 34],
by RGB-D cameras [61, 4], or even by monocular cam-
eras [55, 7, 41, 72, 49, 46]. Recently, methods to capture
both body and hands have also been introduced [57, 34].

3. Social Signal Prediction

The objective of Social Signal Prediction is to predict
the behavioral cues of a target person in a social situation
by using the cues from communication partners as input
(see Fig. 1). We hypothesize that the target person’s be-
havior is correlated with the behavioral cues of other indi-
viduals. For example, the location and orientation of the
target person should be strongly affected by the position of
conversational partners (known as Proxemics [26] and F-
formation [35]), and the gaze direction, body gestures, and
facial expressions of the target person should also be “con-
ditioned” by the behaviors of the conversational partners.
In the social signal prediction task, we model this condi-
tional distribution among interacting subjects, to ultimately
teach a robot how to behave in a similar social situation
driven by the behavior of communication partners. There
exist cases where the correlation of the social signals among
subjects is strong, such as hand-shaking or greeting (wav-

ing hands or bowing). But in most of the cases, the cor-
relation is implicit—there exist no speciﬁc rules on how to
behave given other people’s behavior, which makes it hard
to manually deﬁne the rules. In our approach, we tackle this
problem in a data-driven manner, by automatically learning
the conditional distributions using a large scale multimodal
social signals corpus.

We ﬁrst conceptually formulate the social signal predic-
tion problem here, and a speciﬁc implementation focusing
on the Haggling scenario is described in the next section.
Let us denote “all types of signals” that the target person re-
ceived in a social situation at time t as X(t). Thus X(t)
includes the social signals from other individuals—body
gestures, facial expression, body position, voice tones, ver-
bal languages—and also other contextual factors such as the
space where the conversation is performed or other visible
objects which may affect the behavior of the target person
(e.g., sounds or objects in the environment may attract the
attention of the person). We divide the input signal X(t)
into two parts, the signals from the conversational partners,
Xc(t), and signals from other sources (e.g., objects, envi-
ronment, and other human subjects not interacting with the
target person), Xe(t). Thus,

X(t) = {Xc(t), Xe(t)}.

(1)

The term Xc(t) may contain the social signals from mul-
tiple people and we denote the signals from each subject
separately:

Xc(t) = {Xi

c(t)}N

i=1,

(2)

where Xi
c(t) are the signals from the i-th conversational
partner in the social interaction and N is the total number
of partners. We also denote the signals emitted by the tar-
get person at time t in the social situation as Y(t). Then,
the goal of social signal prediction is to ﬁnd a function F
which takes X as input and produces Y as output to mimic
the behavior of the target person in the social situation:

Y(t + 1) = F (X(t0 : t), Y(t0 : t)) ,

(3)

where t0 : t represents a range of time from t0 to t affecting
the current behavior of the target person. Note that we de-
ﬁne the function F to take the history of the target person’s
own signals Y(t0 : t), and the function predicts the imme-
diate future motion (or response) of the target individual.
Intuitively, this formulation models the human behavior as
a function that represents the dynamics among social sig-
nals the target person is receiving and emitting. The func-
tion can be deﬁned for a speciﬁc individual, representing the
personal behavior of the target person encoding character-
istics, like physical attributes or culture of the target. Based
on that, different individuals may behave differently. If the
function is regressed by the data from many people, then
we hypothesize that the function produces more general and

310875

common social behaviors, where the individual speciﬁc be-
haviors are averaged out.

Previous approaches can be considered as subsets of this
model. For example, conversational agents (or chatbots) us-
ing natural language only can be represented as:

Yv(t + 1) = F (Xv(t0 : t)) ,

(4)

where Yv and Xv represents only verbal signals. The
human motion forecasting studied in computer vision and
graphics [22, 31] can be considered as:

Yn(t + 1) = F (Yn(t0 : t)) ,

(5)

where Yn represents nonverbal body motion. Note that in
this task, there is no social interaction modeling, and the
prediction is only for an individual using the individual’s
own previous signals.

In our work, we focus on nonverbal social signal predic-

tion in a triadic social interaction scenario:

Y(t0 : t) = F (cid:0)X1

c(t0 : t), X2

c(t0 : t)(cid:1) ,

(6)

where we predict the social signals of the target given sig-
nals of the two other people during the same window of
time. In particular, we consider diverse input and output so-
cial signals to investigate their dynamics and correlations.

4. Triadic Interaction Dataset with Full-

spectrum Social Signal Measurements

Availability of a large-scale dataset is essential to com-
putationally investigate the nonverbal communication in a
data-driven manner. Despite existing datasets that provide
measurements for human motion and behaviors [10, 38, 71,
13, 2, 30], there is no dataset that satisﬁes the following core
requirements for understanding nonverbal human behav-
iors: (1) capturing 3D full body motion with a broad spec-
trum of nonverbal cues (including face, body, and hands);
(2) capturing signals of naturally interacting groups (more
than two people to include attention switching); and (3) col-
lecting the data at scale. The limited availability of datasets
motivates us to build a new dataset that contains social inter-
actions among hundreds of interacting groups with a broad
spectrum of 3D body motion measurements. The key prop-
erties of our dataset are as follows:

• Naturally interacting multiple people in a negotiation
game scenario, where the game is carefully designed
to induce natural and spontaneous interaction

• No behavioral restriction is instructed to participants

during the capture

• A broad spectrum of social signals, including the mo-
tion from faces, bodies, and hands, are measured us-
ing a state-of-the art markerless motion capture sys-
tem [32, 33]

Figure 2. An example of the haggling sequence. (Left) an example
scene showing two sellers and one buyer. (Right) Reconstructed
3D social signals showing the 3D body, 3D face, and 3D hand
motion. 3D normal direction from faces and bodies are also com-
puted, and speaking status of each individual, manually annotated,
is also visualized here.

• Multiple synchronized modalities,

including RGB
videos from over 500 views, depth maps from 10
RGB+D sensors, and sound from 23 microphones

• Voice signals of individuals are recorded via wireless
and wired microphones, and speaking status and tim-
ing of each subject are manually annotated

• 3D point clouds are provided by fusing the depth maps

from 10 RGB+D sensors

Our dataset provides a new opportunity to investigate
the dynamics of various interpersonal nonverbal behavioral
cues emerging in social situations. Our dataset is captured
under a university-approved IRB protocol1 and publicly re-
leased for research purposes.

4.1. The Haggling Game Protocol

To evoke natural interactions, we involved participants
in a social game named the Haggling game. We invent this
game to simulate a haggling situation among two sellers
and a buyer. The triadic interaction is chosen to include in-
teresting social behaviors such as turn taking and attention
changes, which are missing in previous dyadic interaction
datasets [56]. During the game, two sellers are promoting
their own products for selling, and a buyer makes a deci-
sion about which product he/she buys between the two. The
game lasts for a minute, and the seller who has sold his/her
product is awarded $5. To maximize the inﬂuence of each
seller’s behavior on the buyer’s decision-making, the items
assigned to sellers are similar products with slightly differ-
ent properties. Example scenes are shown in Figure 2 and
our supplementary video. See the supplementary material
for the detailed game protocol.

4.2. Measuring Social Signals and Notation

We use the Panoptic Studio to reconstruct 3D anatomical
keypoints of multiple interacting people [32, 33]. As a key
advantage, the method does not require attaching sensors or
markers on subject’s body, and no behavior restrictions nor

1IRBSTUDY2015 00000478

410876

initialization poses are needed from the subjects. As out-
put, the system produces 3D body motion B(t) and 3D face
motion F(t) for each individual at each time t2. We also
denote the global position of the body as X(t). From these
measurements, we additionally compute the body orienta-
tion θ(t) and face orientation φ(t) by ﬁnding the 3D nor-
mal direction of torso and face, respectively. We describe
the detailed representation of the measurements below. See
Fig. 2 for the visualization of measured signals.

Body Motion: We follow the body motion representation
of the work of Holden et al. [28], representing a body ges-
ture at a frame as a 73-dimensional vector, B(t) ∈ R73.
This representation is based on the skeletal structure of
CMU Mocap dataset [24] with 21 joints (63 dimensions),
along with the projection of the root joint (the center of
the hip joints) on the ﬂoor plane (3 dimensions), the rela-
tive body locations and orientations represented by the ve-
locity values of the root (3 dimensions), and footstep sig-
nals (4 dimensions). The orientations are computed only
on the x-z plane with respect to the y-axis, and the loca-
tion and orientation represent the changes from the previous
frame rather than the absolute values, following the previ-
ous work [31, 28].
In particular, the ﬁrst 63 dimensions
of B(t) represents the body motion in person-centric co-
ordinates, where the root joint is at the origin and torso is
facing the z direction. We perform a retargeting process
to convert our original 3D motion data from the Panoptic
Studio, where the skeleton deﬁnition is the same as COCO
dataset [39], to this body motion representation with a ﬁxed
body scale. Thus, in our ﬁnal motion representation, indi-
vidual speciﬁc cues such as heights or lengths of limbs are
removed and only motion cues are kept.

Face Motion: For the face motion signal, we ﬁrst ﬁt the
deformable face model of [8] and use the initial 5 dimen-
sions of the facial expression parameters, because we found
the remaining dimensions have an almost negligible im-
pact on our reconstruction quality. Note that the face ex-
pression parameters in [8] are sorted by their inﬂuence by
construction and the initial components have more impact
in expressing facial motion. To this end, face motion at
a time instance is represented by a 5-dimensional vector,
F(t) ∈ R5 (See the supplementary material for the visual-
ization of these facial expressions). Here, we also do not
include individual-speciﬁc information (the face shape pa-
rameters varying for individuals) and only motion cues are
kept.

2The system also produces 3D hand motion, but we do not use this
measurement in this paper due to the occasional failures in challenging
hand motions (e.g., when both hands are close to each other). However,
believing that these are still important cues, we release the reconstruction
results for future work.

Position and Orientation: For the global position x(t)
of each individual, we use the coordinate of the root joint of
the body, ignoring the values in y axis, and thus x(t) ∈ R2.
We use a 2D unit vector to represent body orientations
θ(t) ∈ R2 and face orientation φ(t) ∈ R2, deﬁned on the
x-z plane ignoring the values in y axis. Note that we use
unit vectors rather than angle representation, because the
angle representation has a discontinuity issue when wrap-
ping around 2φ and −2φ. In contrast to the relative location
and orientation represented in the part of body motion B(t),
these x(t), θ(t), and φ(t) represent the values in the global
coordinate, which are used to model social formation. In
summary, the status of an individual at a frame in social for-
mation prediction is represented by a 6-dimensional vector,
[x(t)⊤, θ(t)⊤, φ(t)⊤]⊤ ∈ R6.

Speaking Status: The voice data V(t) of each individual
is also recorded by wireless microphones assigned to each
individual. From the audio signal, we manually annotate
a binary speaking label S(t) ∈ {0, 1} describing whether
the target subject is speaking (labelled as 1) or not speaking
(labelled as 0) at time t.

By leveraging these various behavioral cues measured in the
Haggling scenes, we model the dynamics of these signals
in a triadic interaction. The objective of our direction is
to regress the function deﬁned in Equation 6. To further
constrain the problem we assume that the target person is
the seller positioned on the left side of the buyer3, and as
input we use the social signals of the buyer (X1) and the
other seller (X2). Based on our social signal measurements,
the input and output of the function are represented as,

Y = [x0, θ0, φ0, B0, F0, S0],
X1 = [x1, θ1, φ1, B1, F1, S1],
X2 = [x2, θ2, φ2, B2, F2, S2],

(7)

where we use the superscript 0 to denote the social signals
of the target subject (the output of social signal prediction).

5. Social Signal Prediction in Haggling Scenes

We use our Haggling scenario as an example problem
of social signal prediction to computationally model triadic
interaction. In this section, we speciﬁcally deﬁne the input
and output signals used in our modeling, and then present
three social signal predicting problems, predicting speak-
ing status, predicting social formation, and predicting body
gestures (Kinesic signals). Note that we focus on estimating
the target person’s concurrent signals by taking other indi-
viduals’ signals as input as deﬁned in Equation 6 to simplify
the problem, rather than forecasting the future signals. See
the supplementary material for the implementation details.

3To simplify the problem, particularly for the formation prediction.

510877

5.1. Predicting Speaking

We predict whether the target subject is currently speak-
ing or not, denoted by S0. This is a binary classiﬁcation
task and can be trained with a Cross Entropy loss. We ﬁrst
study the correlation between the speaking signal of the tar-
get person, S0, and the person’s own social signals, either
body motion B0 or facial motion F0, or both. We expect
this correlation is stronger than the link across individuals.
Formally, a function FB0→S0 takes the target person’s own
body motion B0(t0 : t) to predict the speaking signal:

S0(t0 : t) = FB0→S0 (cid:0)B0(t0 : t)(cid:1) ,

and similarly,

S0(t0 : t) = FF 0→S0 (cid:0)F0(t0 : t)(cid:1) ,

(8)

(9)

S0(t0 : t) = F(F 0,B0)→S0 (cid:0)F0(t0 : t), B0(t0 : t)(cid:1) ,

(10)

where FF 0→S0 takes the target person’s own face motion,
and F(F 0,B0)→S0 takes both face and body cues. We com-
pare the performance of these functions with the function
that takes the signals from a communication partner, the
other seller:

S0(t0 : t) = FB2→S0 (cid:0)B2(t0 : t)(cid:1) ,
S0(t0 : t) = FF 2→S0 (cid:0)F2(t0 : t)(cid:1) ,

S0(t0 : t) = F(F 2,B2)→S0 (cid:0)F2(t0 : t), B2(t0 : t)(cid:1) ,

(11)

(12)

(13)

where the functions use body cues, face cues, and both cues,
respectively.

This framework enables us to quantitatively investigate
the link among social signals across individuals. For exam-
ple, we may easily hypothesize that there exists a strong cor-
relation between the signals from the same individual (e.g.,
speaking and facial motion of the target person), while the
correlation between the signals across different individuals
(e.g., speaking of the target person and body motion of an-
other person) may be considered as weak. By comparing
their performances, we verify there still exists strong links
among these signals exchanged across subjects.

5.2. Predicting Social Formations

We predict the location and orientations of the target
person, denoted by Yp = [x0⊤
]⊤, given the
same channels of cues from the communication partners.
This problem is strongly related to Proxemics [26] and F-
formation [35], illustrating how humans use their space in
social communications. Formally,

, φ0⊤

, θ0⊤

Yp(t0 : t) = Fp (cid:0)X1

p(t0 : t), X2

p(t0 : t)(cid:1) ,

(14)

where Yp, X1
tion signals [xi⊤

p, and X2
, θi⊤

p contain global location and orienta-
, φi⊤
]⊤ (where i = 0, 1, or 2) for the

target subject and others. Note that we only consider the po-
sitions and orientations on the ground plane (in 2D), ignor-
p(t) ∈ R6.
ing the height of the subjects, and thus Yp(t), Xi
This prediction problem is intended to see whether the ma-
chine can learn how to build a social formation to interact
with humans [64].

5.3. Predicting Body Gestures (Kinesic Signals)

Predicting body motion in social situations (by using
other subjects’ signals) is challenging, because the corre-
lation among body signals are subtle and less explicit. To
study this, we present two baseline approaches here.

By Using Social Formation Only. The ﬁrst approach
uses only the social formation information of other subjects:

B0(t0 : t) = Fp→B0 (cid:0)X1

p(t0 : t), X2

p(t0 : t)(cid:1) .

(15)

This is an ill-posed problem with diverse possible solutions,
because the formation signals of communication partners
barely tells us about the detailed behavior of our target per-
son. Yet, we can consider several required properties of the
predicted skeleton. For example, the body location and ori-
entation need to satisfy the social formation property, and
when the target person’s location is changing the appropri-
ate leg motion needs to be predicted. Intuitively, we expect
the predicted skeleton shows a similar social amount of in-
formation, location and orientations, as in social formation
prediction, but using more complicated structure, body mo-
tion. In that sense, we can divide the function Fj0 into two
stages: predicting a social formation by Fp described in
Equation 14 and predicting 3D body motion from the pre-
dicted social trajectory Yp(t0 : t):
B0(t0 : t) = Fp→B0 (cid:0)Fp (cid:0)X1

p(t0 : t), X2

p(t0 : t)(cid:1)(cid:1)

= Fp→B0 (Yp(t0 : t)) ,

(16)

where Fp→B0 is a mapping between the target subject’s
own social trajectory to body skeleton. Since the trajec-
tory (position and orientations) is a sub-part of the body be-
havior, we expect the predicted skeleton to contain similar
signals as the social trajectory. For the function Fp→B0, we
follow a similar approach to the work of Holden et al. [28].
By using Body Motions as Input. We can use the en-
tire body signals of conversational partners as input for our
function:

B0(t0 : t) = F(B1,B2)→B0 (cid:0)B1(t0 : t), B2(t0 : t)(cid:1) . (17)

In this particular example, we expect “better” prediction
quality than the previous baseline by using other subject’s
body motions as a cue to determine the target person’s body
motion. We found that this method shows more diverse up-
per body motion, responding to the motions of other sub-
jects. To this end, we present a hybrid method combining
the upper body prediction results of this method to the root
and leg motions of the previous method.

610878

6. Results

In this section, we show experimental results for three
prediction tasks, predicting speaking status, social forma-
tions, and gestures (kinesic signals) from different input
sources. The core direction is to explore the existence of
correlations of diverse behavioral channels in genuine so-
cial communications.

6.1. Pre processing Haggling Data

Given the measurement data of the Haggling games, we
ﬁrst manually annotate the start and end time of the game,
where the start time is decided when the social formation
is built and the end time is deﬁned when the social forma-
tion is broken. We crop out the motion dataset based on
this start and end times, so that we ignore the time while
subjects enter and exit the capture space. For each haggling
game scene, we also annotate the players’ roles in the game,
buyer, left-seller, and right-seller, where the left and right
are determined in the buyer’s viewpoint. In our experiment,
we specify that the left seller is our target person and predict
the social behavior of these subjects. As described in our
method section, we re-target the motion data to a standard-
ized skeleton size to remove size variation from the body
skeletons, similar to [28]. We also synthesize footstep sig-
nals and decouple the body motion from global translation
and orientation using the method of [28]. For face motion,
we ﬁt the Facaewarehouse model [8] on the 3D keypoints
of individual’s face, and use the ﬁrst ﬁve facial expression
parameters, as described in Section 4.2. Finally, we divide
the dataset into 140 training sets and 40 test sets. However,
since there exist sequences where the reconstruction errors
are severe for some frames, we select only 79 training sets
and 28 testing sets which are manually veriﬁed to be error
free. We additionally divide all training set into slices with
120 frames with a certain interval (10 frames), and gener-
ate about 10K training samples. We also consider a ﬂipped
version by considering the “right person” as the target in the
same group, generating about 20K training samples in total
for the training. We standardize all input data so that they
have zero mean and unit variance.

6.2. Speaking Classiﬁcation

We predict whether the target person is currently speak-

ing or not by observing other channels of social signals.

Result on Intra-personal Signals. First, we investigate
the performance when the target individual’s own social sig-
nals are used as input. Three different input sources—facial
expressions, body gestures, and both of them—are used to
train neural network models respectively. In particular, we
use the same neural network architecture for this experi-
ment by keeping the input dimension and network size as
the same to make the comparison as fair as possible (See the

Input Signal
Face+Body

Face
Body

F+B, Masked Body
F+B, Masked Face

Self

Other seller’s

Random person’s

88.40%
88.93%
73.12%

82.48%
56.59%

78.42%
80.14%
70.48%

75.10 %
64.27 %

49.65%
49.64%
50.22%

–
–

Table 1. Speaking status classiﬁcation accuracy using different so-
cial signal sources as input. The last two rows test performance of
the ”Face+Body” model after zeroing out parts of the input data
without retraining.

supplementary material). To train the network with differ-
ent types of input, we mask out the unused channels in the
input with their average, computed in the training set. The
prediction accuracies from these input signals are shown
in the second column of Table 1 (labeled as “Self”). As
demonstrated in our result, the social signals from the tar-
get individual show strong correlations with the speaking.
For example, the facial cue of the target person shows the
strongest correlation (about 89% accuracy) with the target
person’s own speaking status, presumably due to the strong
correlation between the lip motion and speaking. The body
motion also shows a strong correlation with more than 73%
prediction accuracy. The result with both body and face
signals, shown in the second row labeled “Face+Body” of
Table 1, is similar to the case that only the face cue is used,
and by applying an ablation study we found that this is be-
cause the network predominantly uses the face cues over the
body cues for the prediction, as shown in the last two rows
of the Table 1. More speciﬁcally, given the trained model
of “Face+Body”, we mask out the face part or body part in
the input data (at “testing” time) and evaluate the perfor-
mances. The accuracy after removing the body part is sim-
ilar to the original performance, meaning that the trained
network is less dependent on body cues, while there exists
a much larger drop if the face part is removed.

Result on Inter-personal Signals. A more interesting
experiment is investigating the performance by using the
other seller’s social signals as input to predict the target
person’s speaking status. Similarly, three different input
sources are considered, and the results are shown in the sec-
ond column of the Table 1. The result shows that there exists
a strong link between interpersonal social signals. The other
seller’s facial motion shows strong predictive power for the
target person’s speaking status, with accuracy higher than
when using only the target person’s own body signals as in-
put, presumably due to turn-taking during social communi-
cation. See the supplementary material for further analysis.
Result on Random Signals. As a baseline, we also per-
form an experiment by using signals from a random indi-
vidual in the random sequences without any social link to
our target individual, which shows about the chance level in
Table 1.

An Ablation Study to Verify the Inﬂuence of Each

710879

Figure 3. An ablation study by comparing the prediction perfor-
mances after removing each channel of the social signal input from
the trained networks of “Face+Body”in Table 1. The performance
drops after removing each part, compared to the original perfor-
mances, are shown by colors and circle sizes. The left ﬁgure is
the result by using the target person’s own signals, and the right
ﬁgure is by using the other seller’s signals. The colorbar on the
right shows the frame drops in percentage from the original per-
formances.

Part. We perform an ablation study by comparing the pre-
diction performance after removing every single channel
in the trained network. For this test, we use the network
trained with “Face+Body” in Table 1. We mask out a cer-
tain channel (e.g., a face motion component or a body joint
part) in the input at test time and check the performance
drop from the original output. The result is shown in Fig-
ure 3, where the colors and the sizes of the circles represent
the amount of performance decrease. This result shows that
the ﬁrst component of the face motion, which is correspond-
ing to the mouth opening motion, has the strongest predic-
tive power for speaking status. As another interesting result,
the result shows that the right hand has stronger predictive
power than the left hand.

6.3. Social Formation Prediction

We predict the position and orientation of the target per-
son, the “left seller”, by using the signals of communication
partners.
In this test, we explore the prediction accuracy
by considering combinations of difference sources: using
body position, body orientation, and face orientations. Ta-
ble 2 shows the results. By using all signals, we obtain the
best performance. Intuitively, we can imagine that the target
person’s location can be estimated by triangulating the face
normal direction of the other two subjects, which presum-
ably learned from our network. The prediction performance
using only position cues shows the worst, but still a reason-
able, performance among them.

6.4. Body Gestures Prediction

The quantitative evaluation of body gesture predictions
is shown in Table 3. In the ﬁrst method of Eq. 15, the body
motion is directly regressed from the estimated social for-
mation (location and orientation) of the target person, and
the output shows reasonable leg motions following the tra-
jectory movements, but has minimum upper body motion.

Types

PosOnly
Pos+face
Pos+body

Pose+face+body

Position (cm)
29.83 (13.38)
25.23 (9.74)
26.57 (10.24)
24.59 (10.23)

Body Ori. (◦)
15.24 (7.23)
13.20 (5.17)
12.80 (4.37)
12.33 (3.71)

Face Ori. (◦)
19.02 (7.64)
17.61 (6.89)
17.51 (5.60)
17.01 (5.18)

Table 2. Social Formation Prediction Errors (Std.). Average posi-
tion error between our estimation and ground-truth are reported in
centimeters. The body and face orientation errors is in degree be-
tween the estimated facial/body normal direction and ground truth.

Types

Avg. Joint Err. (cm)

Fp→B0 (Eq. 15)

F(B1,B2)→B0 (Eq. 17)

Hybrid

Average body (baseline)

8.31
8.72
8.61
7.83

Std.
2.26
2.00
1.84
2.33

Table 3. Social Body Gesture Prediction Errors (cm). Hybrid uses
Fp→B0 (Eq. 15) for lower body and F(B1,B2)→B0 (Eq. 17) for
upper body prediction.

As an alternative method, in the method of Eq. 17, the out-
put does not take into account the global formation informa-
tion, but shows more dynamic and realistic body motions by
responding to other subjects’ motion. Finally, we combine
both methods (labelled “Hybrid”), by merging the forma-
tion and leg motion from the ﬁrst method to the upper body
motion from the second method, which generates qualita-
tively better results than others, satisfying most of the no-
ticeable social rules in the scenes (distance, orientation, leg
and root movement, and natural hand motions). However,
the quantitative errors tend to be higher. Notably, the base-
line method, always generating a ﬁxed “mean pose” com-
puted from the training set, shows the best performance.
This is because the error metric computing the 3D errors
from the ground-truth cannot fully evaluate how natural the
motion appears. See the supplementary video.

6.5. Discussion

We present a data-driven social signal prediction frame-
work, which allows us to investigate the dynamics and cor-
relations among interpersonal social signals. We formal-
ize the social signal prediction framework, and describe
sub-tasks considering various channels of input and output
social signals. To build the models, we collect the Hag-
gling dataset from hundreds of participants, and demon-
strate clear evidence that the social signals emerging in gen-
uine interactions are predictive each other. The approach
described in this paper is an important direction to endow
machines with nonverbal communication ability. There are
still several unexplored issues, including how to better eval-
uate more natural behaviors, modeling both verbal and non-
verbal signals together, and modeling more diverse social
interactions than triadic scenarios.

Acknowledgements. We thank Hyun Soo Park, Luona
Yang, and Donielle Goldinger for their help and discussions
in designing and performing the data capture.

810880

References

[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,
Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So-
cial lstm: Human trajectory prediction in crowded spaces. In
CVPR, 2016. 3

[2] X. Alameda-Pineda, J. Staiano, R. Subramanian, L. Batrinca,
E. Ricci, B. Lepri, O. Lanz, and N. Sebe. Salsa: A novel
dataset for multimodal group behavior analysis. In TPAMI,
2015. 3, 4

[3] H. Aviezer, Y. Trope, and A. Todorov. Body cues, not fa-
cial expressions, discriminate between intense positive and
negative emotions. In Science, 2012. 2

[4] Andreas Baak, Meinard M, Gaurav Bharaj, Hans-peter Sei-
del, and Christian Theobalt. A Data-Driven Approach for
Real-Time Full Body Pose Reconstruction from a Depth
Camera. In ICCV, 2011. 3

[5] Tadas Baltruˇsaitis, Peter Robinson, and Louis-Philippe
Morency. Openface: an open source facial behavior anal-
ysis toolkit. In WACV, 2016. 3

[6] R. Birdwhistell. Kinesics and context: Essays on body mo-
In University of Pennsylvania Press,

tion communication.
1970. 1

[7] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Pe-
ter V. Gehler, Javier Romero, and Michael J. Black. Keep
it SMPL: automatic estimation of 3d human pose and shape
from a single image. In ECCV, 2016. 3

[8] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun
Zhou. Facewarehouse: A 3d facial expression database for
visual computing. In TVCG, 2014. 5, 7

[9] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In CVPR, 2017. 3

[10] Jean Carletta, Simone Ashby, Sebastien Bourban, Mike
Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec,
Vasilis Karaiskos, Wessel Kraaij, Melissa Kronenthal, et al.
The ami meeting corpus: A pre-announcement. In Interna-
tional Workshop on Machine Learning for Multimodal Inter-
action, 2005. 4

[11] Justine Cassell, Catherine Pelachaud, Norman Badler, Mark
Steedman, Brett Achorn, Tripp Becket, Brett Douville, Scott
Prevost, and Matthew Stone. Animated conversation: rule-
based generation of facial expression, gesture & spoken in-
tonation for multiple conversational agents. In Annual Con-
ference on Computer Graphics and Interactive Techniques,
1994. 1

[12] Wen-Sheng Chu, Fernando De la Torre, and Jeffery F. Cohn.
Selective transfer machine for personalized facial action unit
detection. In CVPR, 2013. 2

[13] Marco Cristani, Loris Bazzani, Giulia Paggetti, Andrea Fos-
sati, Diego Tosato, Alessio Del Bue, Gloria Menegaz, and
Vittorio Murino. Social interaction discovery by statistical
analysis of f-formations. In BMVC, 2011. 3, 4

[14] C Darwin. The expression of the emotions in man and ani-

mals. John Murray, 1872. 2

[15] Beatrice De Gelder. Why bodies? twelve reasons for includ-
ing bodily expressions in affective neuroscience. In Philo-

sophical Transactions of the Royal Society of London B: Bi-
ological Sciences, 2009. 2

[16] Fernando De la Torre, Wen-Sheng Chu, Xuehan Xiong,
Francisco Vicente, Xiaoyu Ding, and Jeffrey F. Cohn.
In-
traface. In Automatic Face and Gesture Recognition, 2015.
2

[17] Paul Ekman and Wallace V Friesen. The repertoire of non-
verbal behavior: Categories, origins, usage, and coding. In
Semiotica, 1969. 2

[18] Paul Ekman and Wallace V Friesen. Facial action coding

system. Consulting Psychologists Press, 1977. 2

[19] A. Elhayek, E. Aguiar, A. Jain, J. Tompson, L. Pishchulin,
M. Andriluka, C. Bregler, B. Schiele, and C. Theobalt. Ef-
ﬁcient convnet-based marker-less motion capture in general
scenes with a low number of cameras. In CVPR, 2015. 3

[20] M Farenzena, A Tavano, L Bazzani, D Tosato, G Paggetti,
G Menegaz, V Murino, and M Cristani. Social interactions
by visual focus of attention in a three-dimensional environ-
ment. In Workshop on Pattern Recognition and Artiﬁcial In-
telligence for Human Behaviour Analysis, 2009. 3

[21] A. Fathi, J. K. Hodgins, and J. M. Rehg. Social interactions:

A ﬁrst-person perspective. In CVPR, 2012. 2

[22] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji-
tendra Malik. Recurrent network models for human dynam-
ics. In ICCV, 2015. 3, 4

[23] Juergen Gall, Carsten Stoll, Edilson De Aguiar, Christian
Theobalt, Bodo Rosenhahn, and Hans-Peter Seidel. Motion
capture using joint skeleton tracking and surface estimation.
In CVPR, 2009. 3

[24] Ralph Gross and Jianbo Shi. The cmu motion of body

(mobo) database. 2001. 3, 5

[25] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese,
and Alexandre Alahi. Social gan: Socially acceptable trajec-
tories with generative adversarial networks. In CVPR, 2018.
3

[26] Edward Twitchell Hall. The hidden dimension. Doubleday

& Co, 1966. 2, 3, 6

[27] Dirk Helbing and Peter Molnar. Social force model for

pedestrian dynamics. In Physical review E. 3

[28] Daniel Holden, Jun Saito, and Taku Komura. A deep learning
In

framework for character motion synthesis and editing.
TOG, 2016. 5, 6, 7

[29] De-An Huang and Kris M Kitani. Action-reaction: Fore-
casting the dynamics of human interaction. In ECCV, 2014.
3

[30] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3.6m: Large scale datasets and predic-
tive methods for 3D human sensing in natural environments.
In TPAMI, 2014. 3, 4

[31] Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh
Saxena. Structural-rnn: Deep learning on spatio-temporal
graphs. In CVPR, 2016. 3, 4, 5

[32] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,
Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser
Sheikh. Panoptic studio: A massively multiview system for
social motion capture. In CVPR, 2015. 2, 3, 4

910881

[33] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan,
Lin Gui, Sean Banerjee, Timothy Godisart, Bart Nabbe, Iain
Matthews, et al. Panoptic studio: A massively multiview
system for social interaction capture. In TPAMI, 2017. 2, 3,
4

[34] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap-
ture: A 3d deformation model for tracking faces, hands, and
bodies. In CVPR, 2018. 3

[35] Adam Kendon. Spatial organization in social encounters:
In Conducting interaction: Pat-
The f-formation system.
terns of behavior in focused encounters. Cambridge Univer-
sity Press, 1990. 2, 3, 6

[36] Kris M Kitani, Brian D Ziebart, James Andrew Bagnell, and

Martial Hebert. Activity forecasting. In ECCV, 2012. 2

[37] Bruno Lepri, Ramanathan Subramanian, Kyriaki Kalimeri,
Jacopo Staiano, Fabio Pianesi, and Nicu Sebe. Connect-
ing meeting behavior with extraversiona systematic study. In
IEEE Transactions on Affective Computing, 2012. 3

[38] Bruno Lepri, Ramanathan Subramanian, Kyriaki Kalimeri,
Jacopo Staiano, Fabio Pianesi, and Nicu Sebe. Connect-
ing meeting behavior with extraversiona systematic study. In
IEEE Transactions on Affective Computing, 2012. 4

[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, 2014. 5

[40] Yebin Liu, J. Gall, C. Stoll, Qionghai Dai, H.-P. Seidel, and
C. Theobalt. Markerless motion capture of multiple charac-
ters using multiview image segmentation. In TPAMI, 2013.
3

[41] Julieta Martinez, Rayat Hossain, Javier Romero, and James J
Little. A simple yet effective baseline for 3d human pose
estimation. In ICCV, 2017. 3

[42] Iain McCowan, Jean Carletta, W Kraaij, S Ashby, S Bour-
ban, M Flynn, M Guillemot, T Hain, J Kadlec, V Karaiskos,
et al. The ami meeting corpus.
In International Confer-
ence on Methods and Techniques in Behavioral Research,
volume 88, 2005. 3

[43] Hanneke K. M. Meeren, Corn C. R. J. van Heijnsbergen, and
Beatrice de Gelder. Rapid perceptual integration of facial ex-
pression and emotional body language. In National Academy
of Sciences of the United States of America, 2005. 2

[44] Albert Mehrabian. Silent messages: Implicit communication

of emotions and attitudes. Wadsworth Pub Co, 1981. 1

[45] Albert Mehrabian and Susan R Ferris. Inference of attitudes
from nonverbal communication in two channels. In Journal
of consulting psychology, 1967. 1

[46] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal
Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian
Theobalt. Monocular 3d human pose estimation in the wild
using improved cnn supervision. In 3DV, 2017. 3

[47] Volodymyr Mnih, Hugo Larochelle, and Geoffrey E Hin-
ton. Conditional restricted boltzmann machines for struc-
tured output prediction. In arXiv, 2012. 3

[49] Francesc Moreno-noguer. 3d human pose estimation from a
single image via distance matrix regression. In CVPR, 2017.
3

[50] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In ECCV, 2016.
3

[51] Hyun Soo Park, Eakta Jain, and Yaser Sheikh. 3d social
saliency from head-mounted cameras. In NeurIPS, 2012. 2

[52] Rosalind W Picard. Affective computing: challenges.

In
International Journal of Human-Computer Studies, 2003. 2

[53] Rosalind W Picard and Roalind Picard. Affective computing.

MIT press, 1997. 2

[54] Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hus-
sain. A review of affective computing: From unimodal anal-
ysis to multimodal fusion. In Information Fusion, 2017. 2

[55] Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Re-
constructing 3d human pose from 2d image landmarks. In
CVPR, 2012. 3

[56] James Rehg, Gregory Abowd, Agata Rozga, Mario Romero,
Mark Clements, Stan Sclaroff, Irfan Essa, O Ousley, Yin Li,
Chanho Kim, et al. Decoding children’s social behavior. In
CVPR, 2013. 3, 4

[57] Javier Romero, Dimitrios Tzionas, and Michael J. Black.
Embodied hands: Modeling and capturing hands and bod-
ies together. In TOG, 2017. 3

[58] Konrad Schindler, Luc Van Gool, and Beatrice de Gelder.
Recognizing emotions expressed by body pose: A biologi-
cally inspired neural model. In Neural networks, 2008. 2

[59] Francesco Setti, Chris Russell, Chiara Bassetti, and Marco
Cristani. F-formation detection: Individuating free-standing
conversational groups in images. In PloS one, 2015. 2

[60] Caifeng Shan, Shaogang Gong, and Peter W McOwan. Fa-
cial expression recognition based on local binary patterns: A
comprehensive study. In Image and Vision Computing, 2009.
2

[61] Jamie Shotton, A. Fitzgibbon, M. Cook, and Toby Sharp.
Real-time human pose recognition in parts from single depth
images. In CVPR, 2011. 3

[62] Leonid Sigal, Alexandru O Balan, and Michael J Black. Hu-
maneva: Synchronized video and motion capture dataset and
baseline algorithm for evaluation of articulated human mo-
tion. In International booktitle of computer vision, 2010. 3

[63] Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser
Sheikh. Hand keypoint detection in single images using mul-
tiview bootstrapping. In CVPR, 2017. 3

[64] Marynel V´azquez, Elizabeth J Carter, Braden McDorman,
Jodi Forlizzi, Aaron Steinfeld, and Scott E Hudson. Towards
robot autonomy in group conversations: Understanding the
effects of body orientation and gaze. In International Con-
ference on Human-Robot Interaction, 2017. 6

[65] Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn,
Xunyu Lin, and Honglak Lee. Learning to generate long-
term future via hierarchical prediction. In arXiv, 2017. 3

[48] Nina-Jo Moore, Hickson Mark III, and Don W. Stacks. Non-
verbal communication: Studies and applications. Oxford
University Press, 2013. 2

[66] Alessandro Vinciarelli, Maja Pantic, and Herv´e Bourlard.
Social signal processing: Survey of an emerging domain. In
Image and vision computing, 2009. 2

1010882

[67] Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial
Hebert. An uncertain future: Forecasting from static images
using variational autoencoders. In ECCV, 2016. 3

[68] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser

Sheikh. Convolutional pose machines. In CVPR, 2016. 3

[69] Yi Yang, Simon Baker, Anitha Kannan, and Deva Ramanan.
Recognizing proxemics in personal photos. In CVPR, 2012.
2

[70] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik
Cambria. Recent trends in deep learning based natural lan-
guage processing. In IEEE Computational intelligenCe mag-
azine, 2018. 1

[71] Gloria Zen, Bruno Lepri, Elisa Ricci, and Oswald Lanz.
Space speaks: towards socially and personality aware visual
surveillance. In ACM International Workshop on Multimodal
Pervasive Video Analysis, 2010. 3, 4

[72] Xingyi Zhou, Qixing Huang, Xiao Sun, Xiangyang Xue, and
Yichen Wei. Towards 3d human pose estimation in the wild:
a weakly-supervised approach. In ICCV, 2017. 3

1110883

