Learning to Detect Human-Object Interactions with Knowledge

Bingjie Xu1, Yongkang Wong1, Junnan Li1, Qi Zhao2, Mohan S. Kankanhalli1

1National University of Singapore

2University of Minnesota

bingjiexu@u.nus.edu, yongkang.wong@nus.edu.sg, lijunnan@u.nus.edu

qzhao@cs.umn.edu, mohan@comp.nus.edu.sg

Abstract

Training	Image	Set(cid:1)

The recent advances in instance-level detection tasks
lay a strong foundation for automated visual scenes under-
standing. However, the ability to fully comprehend a social
scene still eludes us. In this work, we focus on detecting
human-object interactions (HOIs) in images, an essential
step towards deeper scene understanding. HOI detection
aims to localize human and objects, as well as to identify
the complex interactions between them. Innate in practical
problems with large label space, HOI categories exhibit a
long-tail distribution, i.e., there exist some rare categories
with very few training samples. Given the key observation
that HOIs contain intrinsic semantic regularities despite
they are visually diverse, we tackle the challenge of long-
tail HOI categories by modeling the underlying regularities
among verbs and objects in HOIs as well as general rela-
tionships. In particular, we construct a knowledge graph
based on the ground-truth annotations of training dataset
and external source.
In contrast to direct knowledge in-
corporation, we address the necessity of dynamic image-
speciﬁc knowledge retrieval by multi-modal learning, which
leads to an enhanced semantic embedding space for HOI
comprehension. The proposed method shows improved per-
formance on V-COCO and HICO-DET benchmarks, espe-
cially when predicting the rare HOI categories.

1. Introduction

Recent years have witnessed rapid progress towards vi-
sual scene understanding, from object detection [25] to ac-
tion recognition [23]. However, understanding a scene re-
quires not only detecting individual object instances but
also recognizing the visual relationships between object
pairs [18, 22, 28]. One particularly important facet of vi-
sual relationships is human-centric interaction detection,
known as human-object interaction (HOI) detection [5, 10,
13, 15, 33]. Given an input image, it aims to localize
all humans and objects, and to identify all the triplets
hhuman, verb, objecti (see Figure 1). HOI detection under-

Verb	Embeddings(cid:1)

drink_with(cid:1)

brush_with(cid:1)

…

sip(cid:1)

<	person,	drink	with,	bowl	>(cid:1)

i

(cid:1)
g
n
d
d
e
b
m
E
	
t
n
o
J
	
l

i

Knowledge	Graph(cid:1)

a
d
o
m

-
i
t
l
u
M

In HOI detection task,

Figure 1: Conceptual
illustration of our proposed multi-
modal joint embedding learning.
the
label space is often large and intrinsically having long-tail
distribution issue where some categories have few samples
(e.g. hhuman, drink with, bowli). The proposed model learns a
semantic structure aware embedding space compared to original
word embeddings, such that it can leverage semantic similarity to
retrieve the verb(s) best describing the detected hhuman, objecti
pair. The underlying semantic regularities of verbs and objects are
modeled with graph.

pins a variety of AI tasks such as visual Q&A [30], robotic
task manipulation [3], surveillance event detection [2] and
human-centered computing [27]. However, HOI detection
is still far from settled due to a large label space of verbs
and their interactions with a wide range of object types.

Innate in many problems of practical interest, the label
space of HOIs that are compositions of humans, verbs and
objects exhibits a long-tail distribution, meaning that some
categories possess very few training examples. For exam-
ple, the number of training examples “person riding a bike”
is much more than “person riding an elephant”. This is
a fundamental problem for the standard deep learning ap-
proach, which relies on amount of data for each category to
obtain an effective discriminative pattern. Though HOIs are

2019

visually diverse, the compositional elements (human, verb,
object) contain intrinsic semantic regularities [14]. Speciﬁ-
cally, verbs and objects in the HOIs share certain character-
istics across various types of scenes. For example, the sim-
ilar shape of “bike” and “elephant”, as well as the similar
spatial conﬁguration of “on top of” have implications for
the verb “ride” and its semantic-close verb “sit on”. Mo-
tivated by this observation, we propose to learn to detect
HOIs by modeling the underlying regularities among the
verbs and object categories in visual relationships using a
graph based approach.

Existing works [19, 47] have attempted to tackle the
long-tail distribution issue in HOI recognition with multi-
modal sources. However, these works do not explicitly con-
sider the joint impact from the referent source and visual
information. In contrast, our work emphasizes on the joint
update of the verb embeddings from vision and linguistic
knowledge by introducing a multi-modal embedding mod-
ule to dynamically learn the semantic dependencies of the
concepts. The idea of joint update is illustrated in Fig-
ure 1. In the original word embedding space, “drink with”
is mapped to be close to “brush with” possibly because of
the shared “with” from word vector embeddings [32]. With
the joint update from knowledge graph and training image
set, the model is more likely to comprehend the meaning of
“drink” as its neighbors include “sip”.

In this work, we aim to answer two essential questions
in leveraging knowledge to enhance the HOI detection task
with long-tail label distribution: (1) how to model the se-
mantic regularities of knowledge for HOIs? and (2) how to
dynamically retrieve the image-speciﬁc associated knowl-
edge? Our proposed model solves them by: ﬁrst, min-
ing structure of verbs and object categories from internal
training annotations and external annotations of general vi-
sual relationships dataset [28]; second, introducing a multi-
modal verb embedding space with joint update from visual
representations and linguistic knowledge. The contributions
are summarized as follows:

• In order to address the long-tail distribution issue in
HOI detection, we construct a knowledge graph to
model the dependencies of the verbs and object cat-
egories in HOIs and other visual relationships.

• We offer a new perspective into HOI detection with
multi-modal embeddings such that the model can learn
the associated verb expression refering to its semantic
structure for each visual query.

• We achieve improved performance on two bench-
marks, especially for rare HOI categories, and conduct
extensive ablation study to identify the relative contri-
butions of the individual components. Our code will
be publicly available1.

1https://bitbucket.org/freezingmolly/hoi graph

2. Related Work

HOI Understanding. Different from general visual rela-
tionships, which focuses on two arbitrary objects in the im-
ages, HOIs are human-centric with ﬁne-grained verb-object
labels. HOI understanding starts from the concept of “af-
fordance” [11]. The ﬁne-grained understanding has been
scaled up with the advances from deep learning and sev-
eral large-scale HOI datasets [1, 5, 6, 15, 47]. Works have
been done for learning to detect HOIs with constraints from
interacting object locations [13, 15], pairwise spatial con-
ﬁguration [5] to scene context of instances [10, 33]. An-
other stream of work addresses the long-tail HOI problem
with compositional learning [38] and extra image supervi-
sion [47]. Our work is in the similar vein to address the
long-tail problem in HOI detection, but leverages semantic
regularities from linguistic knowledge. The proposed multi-
modal verb embedding explicitly considers the reciprocity
between referent knowledge and visual information.
Learning with Long-Tail Labels. The instrinsic long-
tail property of label space poses challenges in realistic
tasks [4]. In the context of triplet detection with long-tail la-
bels, considering a triplet as an unique class is a hindrance
for scalability. Therefore, compositional learning [19, 38,
44] has been employed to learn each compositional label
in the triplet that is less rare individually. Modeling with
the internal data has attempted to link the classes in head
to tail [41, 45]. Works [9, 19, 28, 35, 39, 42, 43, 46, 47]
have also attempted to exploit external sources of the same
or different modalities complementing the examined data to
boost learning from very few examples. Our work follows
compositional learning and leverages semantic regularities
from linguistic knowledge, but in the context of HOI detec-
tion that verbs and interacting objects are linked with se-
mantic structures.
Graph Neural Networks. Some approaches have been
proposed that apply deep neural networks to graph struc-
tured data. One group of approaches applies feed-forward
neural networks to every node of the graph recurrently, for
example Graph Neural Network (GNN) [37] and an im-
proved version Gated Graph Neural Network (GGNN) [24].
The second group is to generalize convolution layers to
the graphs.
In the direction of spectral approach that re-
quires spectral representations of graph structure, Graph
Convolutional Network (GCN) [20] is proposed for semi-
supervised learning to process language. In contrast, non-
spectral approaches operate convolutions directly on the
graphs [8, 16]. Graph neural networks have been ex-
ploited to model scene instance dependencies [7, 22, 33]
and knowledge structures [19, 31, 40]. Our model is similar
to the direction on modeling knowledge structures, but ex-
ploits GCN to jointly match the verb semantic embeddings
rather than verb-object categories to visual representation,
to enhance comprehension of the verbs compositionally.

2020

Human-Object	Pairwise	Representations(cid:1)

Multi-Modal	Joint	Embedding(cid:1)

Pooling	
+	Res5(cid:1)

Pooling	
+	Res5(cid:1)

b
h

b
o

Graph	Modeling						for	HOIs(cid:1)

	
	
(cid:1)

Faster		
R-CNN(cid:1)

Training	Dataset:	
all	<v,	o>	

External	Source:	
all	<o1,	p,	o2>	

spatial	feature(cid:1)X ho

p

	
(cid:1)

X
visual	feature(cid:1)

r

ho

candidate	verbs(cid:1)

t

v

	

	

	

0
1
0
1
…
0
(cid:1)

Φ ho

v

s p

FC	
(cid:1)

Lreg

L2Norm(cid:1)

L2Norm(cid:1)

L

sim

Φ

g

FC(cid:1)

X

ho

(cid:1)
…

H

v

Figure 2: Illustration of the proposed verb embedding learning, which consists of human-object visual representation module (Section 3.3),
knowledge graph modeling module (Section 3.2), and joint embedding module (Section 3.4). FC and ⊖ indicate fully-connected layer and
element-wise subtraction, respectively. Respective predictions based on human and object feature vectors are not included for clarity.

verb	and	pred(cid:1)

object(cid:1)

G	internal	Y	shared	R	external	(cid:1)

3. Method

The task of HOI detection is to detect humans and
objects in an image, as well as identify verbs of each
hhuman, objecti pair. During the training phase, training
data consists of ground-truth HOI annotations, labeled as
hhuman, verb, objecti triplets, where the human and objects
are localized as bounding boxes. Given a learned model
and a given probe image, the inference process predicts all
possible verbs based on the detected humans and objects.

3.1. Problem Formulation

Formally, given a set of human and object regions
bh, bo ∈ B proposed for an image I, a set of triplet score
Sv
h,o is assigned to each hbh, boi pair representing the prob-
ability of verbs and the pair detection. Unlike learning
of verb-object categories as a whole with complexity of
O(|V| · |C|), where |V| and |C| are respectively the numbers
of verbs and object categories, we decompose Sv
h,o to en-
able combinations of all verbs and objects for a complexity
of O(|V| + |C|):

Sv
h,o = sh · so · (sv

h · sv

o · sv

h,o)

(1)

h (sv

where sh (so) is the human (object) detection class score of
bh (bo), sv
o) is the verb prediction score from human (ob-
ject) stream, and sv
h,o is the verb prediction score from the
human-object pairwise representation. Please see Figure 3
for the detailed inference procedure.

We propose a novel method to calculate sv

h,o that incor-
porates knowledge graph to address the long-tail problem
in class distributions. Different from existing HOI detec-
tion approaches [5, 10, 13, 33], we formulate HOI detec-
tion as a verb retrieval task with a given visual query pair
hbh, boi. Formally, we maximize the likelihood of the con-
ditional distribution of the referent verb(s) v∗ ∈ V as:

v∗ = arg max

p(v|Xh, Xo, G)

(2)

v∈V

where G is the relational knowledge graph which incorpo-
rates linguistic information available from training dataset
and external source (Section 3.2). Xh and Xo are respec-
tively the visual representation for bh and bo (Section 3.3).
To tackle the formulated verb retrieval task, we project
the visual and linguistic information to a joint verb embed-
ding space such that the embeddings for matched hbh, boi
and v∗ pairs are closer while the unmatched pairs are far
away. Figure 2 illustrates our proposed verb embedding
learning. The inclusion of knowledge graph allows the joint
verb embeddings to exploit the reciprocal nature of referent
expression and visual information. Thus, considering the
joint verb embedding space as a hidden variable φ (detailed
in Section 3.4), the likelihood in Eq. 2 can be written as:

Xφ

p(v, φ|Xh, Xo, G) =Xφ

2021

p(v|φ, Xh, Xo, G)

p(φ|Xh, Xo, G)

|

Inference

{z

}

|

Joint Embedding
(3)

{z

}

b
h

, 
s
h

Human 

FC	x	2 
s
h

v

S

v = s
h,o

h ⋅ s

o ⋅ (s

h

v ⋅ s

v ⋅ s

o

v )

h,o

Faster	R-CNN 

person 

b
o

, 
s
o

Object 

FC	x	2 
s
o

v

cake 

Image 

Conv	Feature 

b
h

b
o

Pairwise 

v

s

h,o

0.53	
	
0.05	
	
	
	
	
0.01	
	
																
 

	
	
	

…

<person,	cut,			cake>	
	
<person,	hold,	cake>	
	
	
	
	
<person,	eat,			cake>	
	
																
 

…
 

…

	
	
	

Figure 3: The inference procedure of the proposed model. Given an input image, the proposed model detects HOI triplets and outputs the
triplet scores. Element-wise multiplication (⊙) is applied to human stream’s verb prediction sv
o and the
pairwise verb prediction score sv

h, object stream’s verb prediction sv

h,o.

3.2. Graph Modeling for HOIs

3.2.2 Graph Convolutional Network for HOIs

In order to boost learning of long-tail classes, we exploit
a graph-based approach to model the semantic dependen-
cies from linguistic knowledge complementing the training
visual representations.

3.2.1 Preliminary: Graph Convolutional Network

Given the word embeddings of verbs and object categories,
the goal of graph modeling is to update the node representa-
tions based on ground-truth relations. Formally, we deﬁne
the knowledge graph as G = (N , E, H), where N are the
nodes, E are undirected edges linking pairs of nodes, and H
represents the feature vectors of nodes.

To model the semantic dependencies of verbs and object
categories, we construct a knowledge graph based on Graph
Convolution Network (GCN) [20], which is originally pro-
posed for semi-supervised entity classiﬁcation. The core
idea of GCN is to transform the node features based on the
neighboring nodes deﬁned by the adjacency matrix. Math-
ematically, given a graph adjacency matrix A and node fea-
tures H ∈ H, the convolutional operations for the k-th layer
in GCN is represented as:

H k+1 = D− 1

2 ˜AD− 1

2 H kW k, where( ˜A = A + I
Dii =Pj

(4)

˜Aij

where A is normalized by the diagonal node degree matrix
D with self-connections. H k ∈ R|N |×dk is the input feature
vector and H k+1 ∈ R|N |×dk+1 is the output feature vector.
dk and dk+1 is dimension of the the input and output fea-
ture vector. W k ∈ Rdk×dk+1 is the weight matrix speciﬁc
to the k-th layer, operating on each node feature H k. The
convolutional layers are usually stacked multiple times. A
non-linear operation, such as the ReLU(·) = max(0, ·) can
be applied to the output of each convolutional layer.

When learning the semantically-meaningful node features
H, we employ the links of nodes to learn W in each
layer. Speciﬁcally, the graph structure is used to capture
the semantic dependencies amongst verb and object cate-
gories in HOIs, and general visual relationships. Formally,
nodes N model all possible verbs and object categories in
the annotations, represented by word embeddings of verbs
Hv ∈ H and objects Ho ∈ H from GloVe model [32].
Each undirected edge from E connects a valid pair of verb
and object category according to the hverb, objecti annota-
tions from training dataset, and hobject1, predicate, object2i
triplets from general visual relationships dataset [28]. The
tail verb classes are thus impacted from its neighbors on the
same object node. The intuition of incorporating general
visual relationships supplementary to HOIs, such as prepo-
sition and spatial conﬁgurations, is that they intrinsically
connect to verbs. For example, “a person on the bike” has
implications of “person riding the bike”.

The adjacency matrix A is initialized with binary val-
ues deﬁning the connections (or disconnections) of nodes.
The knowledge graph here is task-oriented with manageable
size and computational cost. Speciﬁcally, the knowledge
graph for V-COCO dataset consists of 226 vertices whereas
HICO-DET dataset has 313 vertices (both with visual re-
lationships). For both datasets, the number of undirected
edges starting from each vertice is less than 193. The node-
level update can be shared in parallel at each layer.

3.3. Visual Representations

Given a detected person bh and a detected object bo, the
learned pairwise representations should preserve their se-
mantic interactions. For example, the interaction (e.g. sit)
can be characterized by the visual appearance of hbh, boi
(e.g. human pose, object shape and size), and the relative

2022

location conﬁguration. Therefore, for either bh or bo, the
feature vector Xh or Xo is the concatenation of visual fea-
ture X r of the region from feature extraction backbone, and
spatial conﬁguration X p = [ x−x′
w′

w′ , log h

, y−y′
h′

, log w

h′ ].

Therefore, the ﬁnal loss function can be obtained as:

L = λ1Lsim(φho, φg, tsim) + λ2Lreg(sv
h, sv

+λ3Lcls(sv

p, tv)
o, tv)

(7)

Here, x and y are the coordinates of the region with size
w × h, whereas x′, y′, w′ and h′ are of the other region in
the pair. Inspired by visual translation embedding [44], we
perform subtraction operation on X r and X p of bh(bo), fol-
lowed by two respective fully-connected layers, to extract
the pairwise representations X p
ho. The ﬁnal pair-
wise representation Xho is obtained by concatenating the
visual and spatial features followed by one 512 sized fully-
connected layer as:

ho and X r

where λ1, λ2 and λ3 are weights to control the contribution
of each loss term. tv ∈ R|V| denote the labels for the visual
modality. Maximizing the joint embedding term in Eq. 3 is
equivalent to minimizing Eq. 7.

During the inference stage (see Figure 3), for each
h,o is obtained
p · softmax(cos(φho, φg)).

hbh, boi pair, the pairwise prediction score sv
from the regularized verb score sv
Thus the triplet score Sv

h,o is obtained according to Eq. 1.

Xho = F C(X p

ho ◦ X r

ho)

(5)

4. Experiments

3.4. Multi Modal Joint Embedding Learning

The proposed joint embedding learning aims to distill
information from semantic dependencies to jointly learn an
embedding for HOI detection. Speciﬁcally, the goal is to
learn the transformations of visual feature fho(Xho) → φho
and GCN feature fg(Hv) → φg, such that the learned pair-
wise embedding of hbh, boi can preserve the semantic struc-
ture of verbs. This approach guides the learning of verb em-
beddings by exploiting the semantic regularities associated
with visual modality and knowledge.

The objective of the joint embedding learning is to max-
imize the similarity between positive hφho, φgi pairs, and
minimize it between all non-matching pairs to a speciﬁed
margin, as well as preserve the discriminative ability. To
this end, we use a combination of similarity loss Lsim [36],
cross entropy regularization loss Lreg and cross entropy
loss Lcls from individual streams.

Similarity Loss. Lsim for each hφho, φgi pair is deﬁned as:

Lsim(φho, φg, tsim) =(1 − cos(φho, φg),

tsim = 1
tsim = 0
(6)
If hbh, boi and v is associated in
where α is the margin.
ground-truth, the label tsim is assigned to be 1, otherwise 0.

max(cos(φho, φg)−α, 0),

Cross Entropy Regularization Loss. Lreg is applied for
verb classiﬁcation from the pairwise verb embedding, de-
ﬁned as cross entropy loss on the predicted verb scores
p ∈ R|V|. The probabilities are obtained from a shared
sv
fully-connected layer applied on φho, followed by a sigmoid
activation to simultaneously predict verb scores. We assign
multi-class verb labels tv based on the ground-truth.

Cross Entropy Loss. Lcls is individually applied to hu-
man stream and object stream. The respective Xh and Xo
are passed through two fully-connected layers and sigmoid
classiﬁers to obtain verb prediction scores sv
o. Then,
we compute Lcls between sv
o) and the ground-truth verb
labels tv.

h and sv

h (sv

In this section, we ﬁrst describe the evaluated bench-
mark datasets (i.e. V-COCO [15] and HICO-DET [5]), the
evaluation metric, and the implementation details. We also
compare our proposed model with the state-of-the-art mod-
els, and conduct ablation studies to examine the proposed
knowledge modeling and multi-modal embeddings.

4.1. Datasets and Metrics

Dataset. In this work, we evaluate our model on two bench-
marks for HOI detection. First, the V-COCO dataset [15]
is a subset of MS-COCO [26], with 5,400 images in the
train-val (training plus validation) set and 4,946 images in
the test set. It is annotated with 26 unique verb classes, and
has bounding boxes for humans and interacting objects. In
particular, three verb classes (i.e. cut, hit, eat) are annotated
with two types of targets (i.e. instrument and direct object).
Second, the HICO-DET dataset [5] contains 38,118 im-
ages in the training set and 9,658 test images, annotated
with 600 types of interactions: 80 MS-COCO object cat-
egories and 117 unique verbs. The bounding boxes of hu-
mans and corresponding objects are also annotated.

Evaluation Metrics. We follow the standard evaluation
metric and report role mean average precision (role mAP).
mAP is computed based on both recall and precision, which
is appropriate for the detection task. The goal is to cor-
rectly detect all of the hhuman, verb, objecti triplets for an
image. A triplet is considered as a true positive if (1) the
predicted triplet label is the same as the ground-truth, and
(2) both the predicted human and object bounding boxes
have intersection-over-union (IoU) greater than 0.5 w.r.t the
ground-truth annotations.

4.2. Implementation Details

For fair comparison, we use Faster R-CNN [34] with
ResNet-50 [17] as the feature extraction backbone. The pre-
trained weight for MS-COCO [26] is from [10]. Human
and object bounding boxes are detected with ResNet-50-
FPN [25] backbone as [10]. Human and object bounding

2023

Table 1: Comparisons with the state-of-the-art approaches on HICO-DET dataset [5]. Mean average Precision (mAP) (%) for the default
setting (object unknown) is reported where higher values indicates better performance. The best scores are marked in bold.

Method

Feature Backbone

Random
Fast-RCNN [12]
HO-RCNN [5]
Shen et al. [38]
VSRL [13, 15]
InteractNet [13]
GPNN [33]
iCAN [10]

Ours

-

CaffeNet
CaffeNet
VGG-19

ResNet-50-FPN
ResNet-50-FPN

ResNet-152
ResNet-50

ResNet-50

Full ↑

1.35e-3

2.85
7.81
6.46
9.09
9.94
13.11
12.80

14.70

Rare ↑

5.72e-4

1.55
5.37
4.24
7.02
7.16
9.34
8.53

13.26

Non-Rare ↑

1.62e-3

3.23
8.54
7.12
9.71
10.77
14.23
14.07

15.13

boxes with detection conﬁdence scores above 0.8 and 0.4
respectively are kept. Through grid-search on the validation
set, the hyper-parameters are set as λ1 = 0.8, λ2 = 1, and
λ3 = 1. The margin α for cosine loss is set as 0.1. A mini-
batch consists of one positive sample, the jittering positives
and negative samples. The negative samples are obtained by
pairing all the detected humans and objects that are not an-
notated in the ground-truth labels, such that the model can
learn the pairwise patterns for the negative hhuman, objecti
pairs. We use Stochastic Gradient Descent (SGD) to train
the model for 450k iterations with a learning rate of 0.001,
a weight decay of 0.0005, and a momentum of 0.9.

Each person can perform multiple verbs on the same ob-
ject simultaneously, therefore binary sigmoid classiﬁers are
employed for multilabel verb classiﬁcation. We then min-
imize the binary cross entropy losses between the ground-
truth labels and the predicted scores. Note that in HICO-
DET, simultaneous verbs need to be manually combined
for each pair of hhuman, objecti based on IoU of bound-
ing boxes due to separate verb annotations.

To obtain the word embeddings for GCN node inputs,
we use the GloVe text model [32] trained on the Wikipedia
dataset, which leads to vectors of R1×300. For the classes
whose names contain multiple words, we empirically aver-
age all matched words embeddings. The graph consists of
two layers, both with dimension of 512. LeakyReLU with
negative slope of 0.2 [40] is used as the activation after each
layer of the graph.

4.3. Results

Baselines. We compare our method with the following
baselines: (1) Fast-RCNN [12]: predictions are obtained by
linearly combining the human and object detection scores.
(2) HO-RCNN [5]: a multi-stream model combines the
scores from appearance of human and object, as well as
spatial conﬁguration of the pair. (3) VSRL [15]: uses spa-
tial constraints for the interacting objects. We report the
reimplemented result from [13]. (4) Shen et al. [38]: pre-

Table 2: Comparisons with the state-of-the-art approaches on V-
COCO dataset [15]. mAP (role) (%) is evaluated as in the standard
evaluation metric. Higher values are better. The best scores are
marked in bold.

Method

Feature Backbone

Sce. 1 ↑

VSRL [15, 13]
InteractNet [13]
BAR-CNN [21]
GPNN [33]
iCAN [10]

Ours

ResNet-50-FPN
ResNet-50-FPN
Inception-ResNet

ResNet-152
ResNet-50

ResNet-50

31.8
40.0
41.1
44.0
45.3

45.9

dictions are from separate verb and object training.
(5)
InteractNet [13]: multi-loss of object detection, human-
object pairwise prediction, and additional human-centric
branch that learns a human action-speciﬁc density func-
tion.
(6) BAR-CNN [21]: a modiﬁed Faster R-CNN de-
tection pipeline augmented with a box attention mecha-
nism. (7) GPNN [33]: a dynamic scene graph based ap-
proach with node outputs as verb and object predictions.
(8) iCAN [10]2: a multi-stream model of human, object ap-
pearance and pairwise spatial conﬁguration with additional
attention based context.

Experiment Results. We present the overall quantitative
results on V-COCO (Table 2) and HICO-DET (Table 1).
We observe that our proposed model achieves competitive
results over the state-of-the-art approaches [13, 33, 10].
For V-COCO, we follow the original evaluation proto-
col [15]. Compared to the best performing model iCAN,
we achieve an absolute gain of +0.6. For HICO-DET, we
can also observe consistent improvements on all, rare, and
non-rare HOI splits against existing best performing meth-
ods [33, 10]. We achieve absolute gains of +1.59, +3.92,

2mAP(%) on three category sets for HICO-DET in arxiv version:

14.84, 10.45, 16.15.

2024

Figure 4: Prediction samples on V-COCO test set (ﬁrst row) and HICO-DET test set (second row). Our model detects same type of verbs
with various object categories in different scenes, as well as different types of HOIs with the same kind of object. The prediction with the
highest HOI triplet score is displayed.

and +0.9 over GPNN, respectively.

Figure 4 shows sample HOI detection results on both
datasets. We highlight the top-1 prediction result in each
image.
It shows that our model is capable of predicting
verbs interacting with various types of objects, as well as
different verbs on the objects of the same category.

4.4. Ablation Study

We analyze the contributions of various components of

our model. Table 3 shows the results on both benchmarks.

Extra Knowledge. We ﬁrst examine the inﬂuence of
exploiting knowledge in complementing the visual
in-
formation. We directly combine verb predictions with
Xh, Xo, Xho from human, object and pairwise stream, re-
spectively (full model w/o knowledge). For recognizing the
challenging rare categories, the results are in favor of the
whole version of model than w/o knowledge. In this case,
the model has to use the extra knowledge for less common
verbs and objects combinations. This result supports our
core argument - extra knowledge about the semantic depen-
dencies can be used to improve HOI predictions especially
for long-tail HOI categories. External knowledge of visual
relationships is also tested to validate its contribution on
modeling dependencies of general predicates-object struc-
tures.

Graph Modeling. We here examine the inﬂuence of mod-
eling semantic dependencies based on a graph. We directly
feed the word2vec verb features, which are originally used
for GCN node inputs, into two fully-connected layers to ob-
tain verb embeddings (denoted as w/o graph). We can ob-
serve that the performance of w/o graph is worse than the
whole model. This indicates that modeling semantic de-
pendencies of verbs-objects in relationships and leveraging
message passing capabilities of GCNs together is essential.
Embeddings for pairs of nodes with edges between them
impact each other more than the distant ones.

Table 3: Ablation study of our model on V-COCO and HICO-
DET test set. Mean average precision (mAP) (%) are reported.

Method

Ours

w/o knowledge
w/o joint embed.
w/o graph
w/o external set

V-COCO
Sce.1 ↑

Full ↑

HICO-DET
Rare ↑ None-Rare ↑

45.9

42.4
44.0
44.1
45.1

14.70

13.26

12.55
13.12
13.16
13.91

10.21
11.59
11.63
12.52

15.13

13.25
13.58
13.62
14.33

Joint Embedding. We also examine the inﬂuence from the
proposed multi-modal knowledge retrieval. We concate-
nate each node output vector of verbs in either V-COCO
or HICO-DET with Xho. An averaged vector is obtained
by averaging on all concatenated multimodal vectors, and
passed through three fully connected layers to get the verb
prediction in the pairwise stream. Final predictions are
combinations of predictions from human, object and pair-
wise streams (denoted as w/o joint embedding). The de-
crease in performance supports the effect from joint update
of the verb embeddings, which preserve the classiﬁcation
ability and the structural semantics.

Qualitative Ablations. We provide qualitative results in
Figure 5. Speciﬁcally, we compare the prediction results
between the whole model and w/o knowledge. It helps to
understand the beneﬁts of extra knowledge. Given the same
image, our full model (the ﬁrst row) is more conﬁdent to
detect the less seen HOIs such as “ﬂip skateboard” based on
semantic similarity to “jump skateboard”. However, only
given the visual information w/o knowledge, the model is
limited in predicting the concurrent HOIs conﬁdently such
as “hold/swing/wield baseball bat”.

4.5. Analysis of the Learned Embeddings

To gain further insight into the learned verb embeddings,
we explore whether the embedding space has certain en-

2025

ride-instr
sit-instr

cut-instr

work_on_computer-instr

jump-skateboard
flip-skateboard
 

blow-cake

ride-instr

cut-instr

lay-instr

sit_on-bench
cut-cake

jump-skateboard 

hold-baseball_bat

swing-baseball_bat 

wield-baseball_bat

hold-baseball_bat
wield-baseball_bat 

      Ours
(full model)

lay 

w/o 

Knowledge

lay 

Figure 5: Example of detection results from Our full model (ﬁrst row) and model w/o knowledge variant (second row). The ﬁrst three
columns from left show detections on V-COCO test set and the remaining columns are from HICO-DET test set. Predictions with HOI
triplet score > 0.2 are displayed, and “no interaction” class is not displayed for clarity. Text is annotated with the same color as the
corresponding object bound box.

wear 0.76

hanced clustering properties in Figure 6 with t-SNE visu-
alization [29]. We show the t-SNE plots of both the word
embeddings (input to GCN) and the updated verb embed-
dings of 117 verbs in HICO-DET dataset. By inspecting
the semantic afﬁnities between the embeddings in Figure 6
(a) and (b), we can observe that the original GloVe embed-
dings without the proposed joint embedding space yields
a less accurate projection of data. For examples, GloVe
projects “drink with” to be close to “brush with” possibly
due to the shared “with”. However, after the joint update
with visual samples and knowledge graph, the model is
more likely to understand the meaning of “drink with” as its
neighbors include “sip” and “eat”. The semantic dependen-
cies of verbs on an object is also learned, e.g. “jump” and
“ﬂip” the skateboard. This observation explains the contri-
bution from multi-modal verb embedding learning.

5. Conclusion

In this paper, we aimed to tackle the long-tail distri-
bution issue in the label space for human-object interac-
tion (HOI) categories, which is currently not effectively re-
solved in HOI detection task. Towards this challenge, we
dynamically retrieved the associated linguistic knowledge
by introducing a multi-modal embedding space and rela-
tional graph. This joint embedding space explicitly con-
siders the cooperative impact between pairwise visual in-
formation and associated subgraph of knowledge. We then
implemented with image-speciﬁc knowledge retrieval. We
evaluated our model on two HOI detection benchmarks, and
showed promising results. Moving forward, we can address
challenges such as understanding human behavior with im-
plications from HOI detection. Speciﬁcally, human interac-
tions may imply their intent, and possibly provide informa-
tion about the past or future thus to help describing various

brush_with
drink_with
cut_with

cut

drink_with

sip

(a) GloVe word embeddings

(b) updated semantic embeddings

jump
flip

Figure 6: Visual illustration of 117 verb embeddings in HICO-
DET dataset via t-SNE visualization [29]. Top is the GloVe
word embeddings [32] and the bottom is the semantic embeddings
learned with proposed method.

dynamic behavior series. Learning knowledge from noisy
web information to alleviate ambiguity in HOI comprehen-
sion can also be explored.

Acknowledgment

This research is supported by the National Research
Foundation, Prime Minister’s Ofﬁce, Singapore under its
Strategic Capability Research Centres Funding Initiative.

2026

References

[1] PIC: Person in context. http://picdataset.com/

challenge/index/.

[2] Amit Adam, Ehud Rivlin,

Ilan Shimshoni, and David
Reinitz. Robust real-time unusual event detection using mul-
tiple ﬁxed-location monitors. IEEE TPAMI, 30(3):555–560,
2008.

[3] Brenna Argall, Sonia Chernova, Manuela M. Veloso, and
Brett Browning. A survey of robot learning from demon-
stration. Robotics and Autonomous Systems, 57(5):469–483,
2009.

[4] Samy Bengio. Sharing representations for long tail computer

vision problems. In ICMI, page 1, 2015.

[5] Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and
Jia Deng. Learning to detect human-object interactions. In
WACV, pages 381–389, 2018.

[6] Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and
Jia Deng. HICO: A benchmark for recognizing human-
object interactions in images.
In ICCV, pages 1017–1025,
2015.

[7] Ching-Yao Chuang, Jiaman Li, Antonio Torralba, and Sanja
Fidler. Learning to act properly: Predicting and explaining
affordances from images. In CVPR, pages 975–983, 2018.

[8] David K. Duvenaud, Dougal Maclaurin, Jorge Aguilera-
Iparraguirre, Rafael G´omez-Bombarelli, Timothy Hirzel,
Al´an Aspuru-Guzik, and Ryan P. Adams. Convolutional
networks on graphs for learning molecular ﬁngerprints. In
NIPS, pages 2224–2232, 2015.

[9] Andrea Frome, Gregory S. Corrado, Jonathon Shlens, Samy
Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas
Mikolov. Devise: A deep visual-semantic embedding model.
In NIPS, pages 2121–2129, 2013.

[10] Chen Gao, Yuliang Zou, and Jia-Bin Huang.

iCAN:
Instance-centric attention network for human-object interac-
tion detection. In BMVC, page 41, 2018.

[11] James J Gibson. The ecological approach to visual percep-

tion: classic edition. Psychology Press, 2014.

[12] Ross Girshick. Fast R-CNN.

In ICCV, pages 1440–1448,

2015.

[13] Georgia Gkioxari, Ross Girshick, Piotr Doll´ar, and Kaiming
He. Detecting and recognizing human-object interactions. In
CVPR, pages 8359–8367, 2018.

[14] E Bruce Goldstein and James Brockmole. Sensation and per-

ception. Cengage Learning, 2016.

[15] Saurabh Gupta and Jitendra Malik. Visual semantic role la-

beling. arXiv preprint arXiv:1505.04474, 2015.

[16] William L. Hamilton, Zhitao Ying, and Jure Leskovec. In-
In NIPS,

ductive representation learning on large graphs.
pages 1025–1035, 2017.

[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[18] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
Judy Hoffman, Fei-Fei Li, and C. Lawrence Zitnick Ross
Girshick. Inferring and executing programs for visual rea-
soning. In ICCV, pages 2989–2998, 2017.

[19] Keizo Kato, Yin Li, and Abhinav Gupta. Compositional
learning for human object interaction. In ECCV, pages 247–
264, 2018.

[20] Thomas N. Kipf and Max Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. In ICLR, 2017.
[21] Alexander Kolesnikov, Christoph H. Lampert, and Vittorio
Ferrari. Detecting visual relationships using box attention.
arXiv preprint arXiv:1807.02136, 2018.

[22] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S.
Kankanhalli. Dual-glance model for deciphering social re-
lationships. In ICCV, pages 2669–2678, 2017.

[23] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S.
Kankanhalli. Unsupervised learning of view-invariant action
representations. In NeurIPS, pages 1262–1272, 2018.

[24] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S.
In ICLR,

Zemel. Gated graph sequence neural networks.
2015.

[25] Tsung-Yi Lin, Piotr Doll´ar, Ross B. Girshick, Kaiming He,
Bharath Hariharan, and Serge J. Belongie. Feature pyramid
networks for object detection.
In CVPR, pages 936–944,
2017.

[26] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and
C. Lawrence Zitnick. Microsoft COCO: Common objects
in context. In ECCV, pages 740–755, 2014.

[27] Zhenguang Liu, Zepeng Wang, Luming Zhang, Rajiv Ratn
Shah, Yingjie Xia, Yi Yang, and Xuelong Li. Fastshrinkage:
Perceptually-aware retargeting toward mobile platforms. In
ACM Multimedia, pages 501–509, 2017.

[28] Cewu Lu, Ranjay Krishna, Michael S. Bernstein, and Fei-
Fei Li. Visual relationship detection with language priors. In
ECCV, pages 852–869, 2016.

[29] Laurens van der Maaten and Geoffrey Hinton. Visualizing
data using t-SNE. Journal of machine learning research,
pages 2579–2605, 2008.

[30] Arun Mallya and Svetlana Lazebnik. Learning models for
actions and person-object interactions with transfer to ques-
tion answering. In ECCV, pages 414–428, 2016.

[31] Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta.
The more you know: Using knowledge graphs for image
classiﬁcation. In CVPR, pages 20–28, 2017.

[32] Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. Glove: Global vectors for word representation.
In EMNLP, pages 1532–1543, 2014.

[33] Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen,
and Song-Chun Zhu. Learning human-object interactions by
graph parsing neural networks.
In ECCV, pages 407–423,
2018.

[34] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In NIPS, pages 91–99, 2015.

[35] Fereshteh Sadeghi, Santosh Kumar Divvala, and Ali Farhadi.
VisKE: Visual knowledge extraction and question answering
by visual veriﬁcation of relation phrases.
In CVPR, pages
1456–1464, 2015.

[36] Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Mar´ın,
Ferda Oﬂi, Ingmar Weber, and Antonio Torralba. Learning

2027

cross-modal embeddings for cooking recipes and food im-
ages. In CVPR, pages 3068–3076, 2017.

[37] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Ha-
genbuchner, and Gabriele Monfardini. The graph neural
network model.
IEEE Transactions on Neural Networks,
20(1):61–80, 2009.

[38] Liyue Shen, Serena Yeung, Judy Hoffman, Greg Mori, and
Fei-Fei Li. Scaling human-object interaction recognition
through zero-shot learning.
In WACV, pages 1568–1576,
2018.

[39] Qian Wang and Ke Chen. Zero-shot visual recognition via
International Journal of

bidirectional latent embedding.
Computer Vision, 124(3):356–383, 2017.

[40] Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot
recognition via semantic embeddings and knowledge graphs.
In CVPR, pages 6857–6866, 2018.

[41] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learn-

ing to model the tail. In NIPS, pages 7032–7042, 2017.

[42] Xun Xu, Timothy M. Hospedales, and Shaogang Gong. Se-
mantic embedding space for zero-shot action recognition. In
ICIP, pages 63–67, 2015.

[43] Ruichi Yu, Ang Li, Vlad I. Morariu, and Larry S. Davis.
Visual relationship detection with internal and external lin-
guistic knowledge distillation. In ICCV, pages 1068–1076,
2017.

[44] Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-
Seng Chua. Visual translation embedding network for visual
relation detection. In CVPR, pages 3107–3115, 2017.

[45] Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, and
Yu Qiao. Range loss for deep face recognition with long-
tailed training data. In ICCV, pages 5419–5428, 2017.

[46] Bohan Zhuang, Lingqiao Liu, Chunhua Shen, and Ian D.
Reid. Towards context-aware interaction recognition for vi-
sual relationship detection. In ICCV, pages 589–598, 2017.
[47] Bohan Zhuang, Qi Wu, Chunhua Shen, Ian D. Reid, and
Anton van den Hengel. HCVRD: A benchmark for large-
scale human-centered visual relationship detection. In AAAI,
pages 7631–7638, 2018.

2028

