Explicit Spatial Encoding for Deep Local Descriptors

Arun Mukundan

Giorgos Tolias

Ondˇrej Chum

Visual Recognition Group, FEE, CTU in Prague

Abstract

We propose a kernelized deep local-patch descriptor
based on efﬁcient match kernels of neural network activa-
tions. Response of each receptive ﬁeld is encoded together
with its spatial location using explicit feature maps. Two
location parametrizations, Cartesian and polar, are used to
provide robustness to a different types of canonical patch
misalignment. Additionally, we analyze how the conven-
tional architecture, i.e. a fully connected layer attached af-
ter the convolutional part, encodes responses in a spatially
variant way. In contrary, explicit spatial encoding is used in
our descriptor, whose potential applications are not limited
to local-patches. We evaluate the descriptor on standard
benchmarks. Both versions, encoding 32× 32 or 64× 64
patches, consistently outperform all other methods on all
benchmarks. The number of parameters of the model is in-
dependent of the input patch resolution.

1. Introduction

Local feature extraction and representation is still an es-
sential part of a number computer vision applications across
many different problems. A common and well performing
procedure is a sequence of three steps:
local feature de-
tection [16, 27, 29, 6, 25], local patch rectiﬁcation into a
canonical form, and ﬁnally a descriptor construction from
the canonical patch [28, 6, 25, 14]. The desired property of
the local patch descriptors is that Euclidean distance or a dot
product between two descriptors indicates whether they are
matching, i.e. the local features are coming approximately
from the same surface of a 3D scene. The descriptor meth-
ods have shifted from hand-crafted to currently the most
successful convolutional neural network (CNN) based ap-
proaches [58, 46, 5, 30, 49, 33].

Fully convolutional neural networks takes an image or a
patch as input and produces a tensor, where a vector at each
spatial location can be seen as a detector response over its
receptive ﬁeld. In the case of variable-sized, or non-aligned
input, such as images, the response tensor is transformed
into a descriptor typically by some form of global pool-
ing [40, 19, 39], which discards geometric information. The

global pooling is analogous to bags-of-features [13, 48] or
descriptor aggregation [37, 18]. In the case of aligned in-
put of ﬁxed size, such as rectiﬁed image patches, the tensor
is vectorized and further processed. Vectorization has sim-
ilar interpretation to vectorizing spatial bins in SIFT [25].
Commonly, the vectorized tensor is processed by a single
fully-connected (FC) layer [30, 49], that can be either inter-
preted as learned afﬁne (linear and bias) transformation of
the space, e.g. whitening and dimensionality reduction, or
as spatially dependent embedding with efﬁcient match ker-
nels (EMK) [9, 11] (see Section 3.2). The key contribution
of this work is a CNN module that explicitly models the
spatial information of a rectiﬁed patch. Its applicability is
not limited to local descriptors.

Two rectiﬁed patches coming from matching local fea-
tures are far from being identical in general. The differ-
ence has two sources, namely appearance change in imag-
ing process and geometric misalignment. The former comes
from different light conditions, non-planarity of the surface,
imaging artifacts, etc. The latter is caused by the detected
feature covering slightly different area of the 3D surface, or
incorrect rectiﬁcation of the patch. These are consequences
of either the appearance changes or of insufﬁcient geomet-
ric invariance of the detector, i.e. afﬁne invariant detector
acting on projectively transformed surface.

Prior work on hand-crafted feature descriptors has shown
that it is beneﬁcial to explicitly address the geometric mis-
alignment. Some of the approaches handling this are soft
assignment of gradients to bins in SIFT and continuous spa-
tial encoding by kernel methods in different [11] or multi-
ple [32] coordinate systems.

CNNs are powerful in modeling the appearance vari-
ance, while weak in modeling the geometric displacement
(at least with a single FC layer). Recent methods pro-
pose different ways of incorporating spatial information in
a CNN [34, 24], but their application ﬁeld is different than
local descriptors.
In this work, we propose to model the
geometric misalignment by efﬁcient match kernels that ex-
plicitly encode the spatial positions of the responses. To
encode the spatial information, kernel-based explicit fea-
ture maps are used in a similar fashion to hand-crafted fea-
tures [11, 32]. This can be seen as a transition from soft

19394

binning, i.e. overlapping receptive ﬁelds, to continuous efﬁ-
cient match kernels. In contrast to models with an FC layer,
with efﬁcient match kernels the number of model param-
eters does not grow with increased resolution of the input
patch, i.e. the models for 32 × 32 patch input has the same
number of parameters as the model for 64 × 64. The appli-
cations of the proposed descriptor go beyond that of local-
patches, e.g. tasks where encoding spatial position is essen-
tial [24, 34].

The rest of the paper is organized as follows. The related
work is discussed in Section 2. Conventional deep local de-
scriptors and the proposed ones is discussed in Section 3.
Implementation details are detailed in Section 4. Finally,
we present and discuss our experiments on standard bench-
marks in Section 5.

2. Related work

In this section, we review prior work related to hand-

crafted and learned descriptors of local features.

2.1. Hand-crafted descriptors

There are numerous approaches to hand-craft local de-
scriptors. The variants are based on different types of pro-
cessing of the input patch, such as ﬁlter-bank responses [6,
10, 22, 36, 43], pixel gradients [25, 28, 50, 1], pixel intensi-
ties [45, 12, 23, 41] and ordering or ranking of pixel intensi-
ties [35, 17]. The most prominent direction has been that of
gradient histograms, an approach followed also by the most
popular hand-crafted local descriptor, namely SIFT [25].
Several improvements and extensions exist in the litera-
ture [20, 57, 21, 44, 2, 14]. The RootSIFT [2] variant efﬁ-
ciently estimates Hellinger distance and became a standard
choice in approaches and tasks.

Kernel descriptors are derived from the concept of efﬁ-
cient match kernels [9] and form a ﬂexible way to design
descriptors with the desired invariant properties. Kernel de-
scriptors have been proposed not only for local patches [11]
but also as a global image descriptor [8, 7]. The kernel
descriptor of Bursuc et al. [11] was shown to outperform
learned descriptors at that time.

2.2. Learning descriptors

Structure-from-Motion and datasets such as Photo-
Tourism [55] gave rise to learned local descriptors. The
learned part varies from their pooling regions [55, 47] and
ﬁlter banks [55] to transformations for dimensionality re-
duction [47] and embeddings [38].

Learning is also applied to kernelized descriptors as in
the supervised framework by Wang et al. [54]. The local
descriptors in their case are not used separately but directly
aggregated into a global image representation, while super-
vision comes at image level. Kernel local descriptors are

combined with supervised learning in the form of discrimi-
native projections in the work of Mukundan et al. [32]. Our
work is inspired by theirs; we use the same kernel-based
position encoding, but on top of convoltutional activations
instead of pixel attributes.

2.3. Deep learning of descriptors

The interest in local descriptor learning is lately domi-
nated by deep learning [46, 58, 15, 56, 5, 3]. All examples
in the literature use architectures that consist of a sequence
of common CNN layers, similar to the ones of generic com-
puter vision tasks, such as object recognition, but less deep
and with fewer parameters in total. They typically require a
large amount of training data in the form of local patch pairs
or triplets. Some of the contributions are about mining hard
training samples [46, 30, 26], different loss functions [5],
different architectures [49] or training jointly with the local
feature detector [56].

Two of the most recent and successful deep local de-
scriptors are L2-Net [49] and HardNet [30]. L2-Net ap-
plies the loss function to intermediate feature maps too and
the loss function integrates multiple attributes. HardNet ex-
tends L2-Net by sampling the hardest within batch sam-
ples and currently constitutes the state-of-the-art descriptor.
Their common characteristic, which is shared among all an-
cestors, is that they are using common CNN layers in their
architecture. As a consequence, spatial information of con-
volutional feature maps is not explicitly encoded, but only
processed with a standard FC layer.

3. Method

We initially present the current typical architecture for
deep local descriptors in the literature. Then, we provide
a different perspective that formulates such descriptors as
match kernels.
It allows us to point out how the encod-
ing of convolutional feature maps is performed in a transla-
tion variant way, but without explicitly encoding the spatial
information. Finally, we present our novel deep local de-
scriptor which is derived through the same match kernel
framework and improves exactly this drawback. We get
inspired by hand-crafted kernel descriptors to incorporate
explicit position encoding into deep networks for local de-
scriptors. An overview of the proposed descriptor is shown
in Figure 1,

3.1. Deep local descriptors

Conventional architectures for deep local descriptors
consist of a sequence of convolutional layers, producing
translation invariant feature maps, and a ﬁnal FC layer.
We denote the descriptor extraction process by function

ψ : RN ×N → RD, where N is the size of the input patch
and D the dimensionality of the ﬁnal descriptor. Descriptor

9395

for patch a ∈ RN ×N is given by ψ(a) ∈ RD or equivalently
ψa to simplify the notation.

We denote the convolutional part of the network, i.e.
a Fully Convolutional Network (FCN), by function φ :
RN ×N → Rn×n×d. Size n of the resulting feature map
is related to input size N and the architecture of the net-
work. Feature map φ(a), equivalently denoted by φa, is a
3D tensor of activations, which we also view as a 2D grid of
d-dimensional vectors. We call these vectors convolutional
descriptors and use φp
a to denote the vector with coordinates
p = (i, j) on the n × n grid, i.e. p ∈ [n]2 1. Each convolu-
tional descriptor corresponds to a region of the input patch
a that is equal to the receptive ﬁeld size of the feature map.
The standard practice is to vectorize 3D tensor φa and
feed it to an FC layer with parameters that consist of matrix

W ∈ RD×(n×n×d) and bias w ∈ RD. The ﬁnal descriptor

is constructed as

ψa = W vec(φa) + w,

(1)

where vec denotes tensor vectorization. A local descriptor
is typically ℓ2-normalized, which is equivalently achieved
by introducing a normalization factor γa = 1/√ψ⊤
a ψa pro-
ducing descriptor ˆψa = γaψa.

Similarity (or distance) between patches a and b is esti-
mated with inner product (or Euclidean distance) ˆψ
ˆψb.
The ℓ2-normalized descriptor is always used to compare
patches, but we often use ψa (and not ˆψa) simply to spec-
ify which descriptor variant is used. Several deep local de-
scriptors in the recent literature, namely L2Net [49], Hard-
Net [30], and GeoDesc [26] follow such an architecture and
can be formulated in the same way.

⊤
a

3.2. A match-kernel perspective

We provide an alternative, but equivalent, construction
of deep local descriptors. We consider matrix W as a con-
catenation of n2 matrices, i.e.

W =





(1,1)

(i,j)

W ⊤
...
W ⊤
...
W ⊤

(n,n)

⊤





,

(2)

where Wp ∈ RD×d. Descriptor in (1) can be now written

as

ψa = X

p∈[n]2

Wpφp

a + w

′,

(3)

where w′ = w/n2. Moreover, patch similarity becomes

ˆψ

⊤
a

ˆψb ∝ X

p,q∈[n]2

= X

p,q∈[n]2

(Wpφp

a + w

′)⊤ (Wqφq

b + w

′)

gf c(φp

a, p)⊤gf c(φq

b, q),

(4)

where gf c : Rd × [n]2 → RD is a function that encodes
a convolutional descriptor in a translation variant way, de-
pending on its position in the n × n grid. The match kernel
formulation in (4) interprets deep local descriptor similar-
ity as similarity accumulation for all pairs of positions on
the n × n grid. It reveals that matching between convolu-
tional descriptors in φa and φb is performed in a translation
variant way. The encoding function g in the case of conven-
tional deep local descriptors is

gf c(v, p) = Wpv + w

′,

(5)

where matrix Wp and w′ come from the parameters of the
FC layer. In this work, we propose a new encoding function
g, not restricted to standard CNN architecture (layers), that
explicitly encodes position p on the 2D grid.

3.3. Position encoding

Explicit feature maps [53] are used to encode the po-

sition. Let f : R → R2s+1 be a feature map, where s

is a design choice deﬁning the dimensionality of the em-
bedding. Such a feature map deﬁnes a shift invariant ker-
nel K : R × R → R with kernel signature k, so that
K(α, β) = k(α − β)

f (α)⊤f (β) = K(α, β) = k(α − β).

(6)

The kernel K (or the feature map f ) is constructed to ap-
proximate the Von Mises kernel [51].

We propose encoding function gxy : Rd × [n]2 →

RD(2s+1)2

given by

gxy(φp

a, p) = φp

a ⊗ f (xp) ⊗ f (yp),

(7)

where ⊗ is the Kronecker product and xp and yp provide the
coordinates of position p in a Cartesian coordinate system 2.
It is a joint encoding of the convolutional descriptor and the
explicit representation of its position. It is inspired by the
work of Mukundan et al. [32] who propose a hand-crafted
local descriptor that encodes pixel gradients with their posi-
tions in the patch. Similarity of two such encodings is given
by

gxy(φp

a, p)⊤gxy(φq

b, q) = φp

a

⊤φq

b·k(xp−xq)·k(yp−yq).
(8)

1[i] = {1 . . . i} and [i]2 = [i] × [i]

2For p = (i, j), xp = i and yp = j.

9396

Figure 1. Overview of extraction process for the proposed descriptor. We present the case of ψxy (10), while other variants are performed
in a similar way. m

′

mxy.

xy = n2

It is equivalent to the product of descriptor similarity and
similarity of positions on the Cartesian grid.

Following the paradigm of descriptor whitening of hand-
crafted descriptors [32, 4], we propose the ﬁnal local de-
scriptor

a = X
ψxy

p∈[n]2

wpMxygxy(φp

a, p) + mxy

(9)

= Mxy


 X

p∈[n]2

wpgxy(φp

a, p)

 + n2

mxy,

(10)

where Mxy ∈ RD×d(2s+1)2
and mxy ∈ RD are parameters
to be learned during training, while wp = exp(−ρ2
p) is a
weight giving importance according to the distance ρp from
the center of the patch. Note that in contrast to (3) the same
matrix, i.e. Mxy, is used for all convolutional descriptors.
As a result the number of required parameters is reduced
and multiplication by Mxy can be efﬁciently performed af-
ter the summation (10). In analogy to the encoding of posi-
tion in a Cartesian coordinate system, we additionally pro-
pose the encoding w.r.t. a polar coordinate system3 by

gρθ(φp

a, p) = φp

a ⊗ f (ρp) ⊗ f (θp),

(11)

3For p = (i, j), ρp = p(i − c)2 + (j − c)2 and θp = tan−1 j−c
i−c ,

where c = (n + 1)/2.

and the corresponding descriptor

a = X
ψρθ

p∈[n]2

wpMρθgρθ(φp

a, p) + mρθ.

(12)

Different parameterizations, i.e. using different coordinate
system, provide tolerance to different kinds of misalign-
ment between patches. Cartesian offers tolerance to trans-
lation misalignment, while polar offers tolerance to rota-
tion and scale misalignment. To beneﬁt from both types of
tolerance, we further use the combined encoding that uses
the two coordinate systems and is produced by concate-
nation of the previous encoding. It is deﬁned as function

gc : Rd × Rd × [n]2 → R2D(2s+1)2
gc(φp
a⊗f (xp)⊗f (yp))⊤

a, p)=(cid:16)(φp

a, ˜φp

given by

,( ˜φp

a⊗f (ρp)⊗f (θp))⊤(cid:17)⊤

where ˜φ is used to show that the two encodings do not need
to rely on the same FCN φ. Subscript c refers to the com-
bined coordinate system, but we skip xyρθ to simplify the
notation. The ﬁnal descriptor proposed in this work is

(13)

⋆ψc

a = X

p∈[n]2

wpMcgc(φp

a, ˜φp

a, p) + mc

(14)

where Mc ∈ RD×2d(2s+1)2

, and left superscript ⋆ is used
to denote that a separate FCN is used for each encoding,
correspondingly coordinate system.

9397

FCNpatchpre-computedposition encodingKroneckerproductconvolutional descriptorsjoint encoding(appearance, position)parameter matrixparametervectorlocal descriptorConvolutional part φ

Conv. layer Param. matrix shape # Parameters

1

2

3

4

5

6

Total

[ 1, 32, 3, 3 ]

[ 32, 32, 3, 3 ]

[ 32, 64, 3, 3 ]

[ 64, 64, 3, 3 ]

[ 64, 128, 3, 3 ]

[ 128, 128, 3, 3 ]

288

9,216

18,432

36,864

73,728

147,456

285,984

HardNet

N = 32

N = 64

φ

FC

285,984

285,984

1,048,576

4,194,304

Total

1,334,560

4,480,288

ψc

N={32, 64}

s=1

s=2

φ

285,984

285,984

M , m 295,040

819,328

ψxy

N={32, 64}
s=1

s=2

φ

285,984

285,984

M , m 147,584

409,728

Total

433,568

695,712

⋆ψc

N={32, 64}

s=1

s=2

285,984

φ
˜φ
M , m 295,040

285,984

285,984

285,984

819,328

Total

581,024

1,105,312

Total

867,008

1,391,296

Table 1. Number of parameters for different models. The convolutional part φ has identical architecture for all models. Cases where both
φ and ˜φ appear use a separate convolutional part for the Cartesian and the polar descriptor. These speciﬁcations correspond to d = 128,
and D = 128. The resulting n is equal to 8 and 16 for N equal to 32 and 64, respectively. We report M and m due to limited space, but
we refer to Mxy, and Mc according to the respective table, and similarly for m. Descriptor ψρθ has identical requirements as descriptor
ψxy. The parameter requirements of our descriptor remain unchanged for different patch size N .

4. Implementation details

In this section, we provide implementation details that
concern the efﬁciency of the aggregation, describe the dif-
ferent architectures and their required number of parame-
ters, and ﬁnally discuss the training procedure.

product for each p. To evaluate (15), the memory require-
ments are n2d(2s + 1)2 numbers, while to evaluate (16),
only n2(d + (2s + 1)2) numbers are allocated. Using setup
d = 128 and s = 2 in our experiments, the memory require-
ments are reduced by a factor of 20.9.

4.1. Efﬁcient aggregation

We describe the implementation details for variant ψxy
a ,
but these hold for other variants too in the same way. Vec-
that encode positions

tors wpf (xp) ⊗ f (yp) ∈ R(2s+1)2
p ∈ [n]2 are ﬁxed for the 2D grid of size n × n. Thus, we
pre-compute and store them in matrix F ∈ Rn2×(2s+1)2
We reshape 3D tensor φa into matrix Φ ∈ Rn2×d. Given

these two matrices and due to the linearity of matrix to vec-
tor multiplication we can re-write the descriptor as

.

a = X
ψxy

p∈[n]2

wpMxygxy(φp

a, p) + mxy,

= Mxy

= Mxy

p∈[n]2


 X

 X

p∈[n]2

wpgxy(φp

a, p)

 + n2

mxy,

(15)

wpφp

a ⊗ f (xp) ⊗ f (yp)

 + n2

= Mxy vec(Φ⊤F ) + n2

mxy.

mxy,

(16)

Multiplication Φ⊤F makes the computation memory efﬁ-
cient because it avoids explicit storing of the Kronecker

4.2. Architecture

We use the HardNet+ [30] architecture for the convolu-
tion part, since HardNet+ achieves state-of-the-art perfor-
mance on all benchmarks. We also use it a baseline to com-
pare with.

The statistics of the convolutional part φ are described
in Table 1 (left). Each convolutional layer is followed by
batch normalization and ReLU, while no bias is used. Ta-
ble 1 (right) provides the total number of parameters for
HardNet+ and our networks, namely, plain polar or Carte-
sian encoding with different dimensionality of the explicit
feature maps (s = 1 and s = 2 frequencies used), and the
joint encoding with a common (φ) or separate (φ and ˜φ)
convolutional part. Note that for the joint encoding with
separate convolutional parts and s = 2 frequencies, the pro-
posed network needs roughly the same number of param-
eters as HardNet+ with input patch of size 32 × 32 pixels
(N = 32). In all other settings of the proposed architec-
ture, the number of parameters is signiﬁcantly reduced. Im-
portantly, the number of parameters for larger patch sizes
(such as 64×64), that provide better performance, the num-
ber of parameters stays ﬁxed for the proposed architecture.
For Hardnet+, the number of parameters of the FC layer in-
creases by a factor of 4 for 64 × 64 input patches.

9398

4.3. Training

We would like to highlight the contribution of the ex-
plicit spatial encoding and to provide direct comparison to
the current state-of-the-art descriptor construction. To avoid
changing many things at the same time, we follow exactly
the same training procedure as HardNet+, which we brieﬂy
review below.

The network is trained with the triplet loss deﬁned as

ℓ( ˆψan, ˆψpos, ˆψneg) = [1−|| ˆψan− ˆψpos||+|| ˆψan− ˆψneg||]+,

(17)
acting on a triplet formed by an anchor, a positive (matching
to the anchor), and a negative (non-matching to the anchor)
descriptor. A batch of size 1024 patches is constructed from
512 pairs of anchor-positive descriptors. Regarding a par-
ticular pair in the batch, the positive descriptors of all other
pairs are considered as candidate negatives. Finally, the one
with the smallest Euclidean distance to the anchor within
the batch is chosen as a hard negative to form a triplet.

We use Stochastic Gradient Descent (SGD) to perform
the training. The total training set consists of 2 million
anchor-positive pairs and the training lasts 10 epochs. Data
augmentation is employed by random patch rotation, scal-
ing and ﬂipping. The learning rate is set to 10, and linearly
decays to zero withing 10 epochs. Momentum is equal to
0.9 and weight decay to 10−4. Random orthogonal initial-
ization is used for the weights of the network [42]. The
method is implemented in the PyTorch framework.

5. Experiments

We ﬁrst describe the datasets and the evaluation proto-
cols used in our experiments, and then present qualitative
results showing the impact of the training on patch simi-
larity. Finally we present the results achieved by different
variants of our descriptor and show a comparison with the
state of the art.

5.1. Datasets and protocols.

We use two publicly available patch datasets, namely
PhotoTourism (PT) [55] and HPatches (HP) [4]. We use
the former for both training and evaluation, while the latter
only for evaluation when training on PT to show the gener-
alization ability of the descriptor.

The PT dataset consists of following 3 separate sets, Lib-
erty, Notredame and Yosemite. Each consists of local fea-
tures detected with the Difference-of-Gaussians (DoG) de-
tector and veriﬁed through an SfM pipeline. Each set com-
prises about half a million 64 × 64 patches, associated with
a discrete label which is the outcome of SfM veriﬁcation.
The test set consists of 100k pairs of patches corresponding
to the same (positive) 3D point, and an equal number corre-
sponding to different (negative) 3D points. The metric used

RF of p

ψxy

ψρθ

ψc

HardNet+

Figure 2. Visualization of similarity between position p of the
n × n grid (rows) on a patch and another whole patch for differ-
ent methods (columns). Heat-maps are normalized to [0, 1] with
red corresponding to the maximum similarity. Red box is used to
depict the receptive ﬁeld (RF) of p.

to measure performance is the false positive rate at 95% of
recall (FPR@95). Models are trained on one set and tested
on the other two, and the mean of 6 scores is reported.

The HP dataset contains patches of higher diversity and
is more realistic. Evaluation is performed on three different
tasks, namely veriﬁcation, retrieval, and matching. Despite
the fact that we do not train on HP, we evaluate on all 3
train/test splits and report the average performance to allow
future comparisons. We follow the common practice and
train our descriptor on Liberty of PT to evaluate on HP.

We repeat each experiment three times, with different
random seeds to initialize the parameters, and report mean
and standard deviation of the 3 runs. We followed this pol-
icy for all variants and datasets.

Recently, larger and more diverse datasets [31, 26] have
been introduced to improve local descriptor training. These
are shown to improve the performance of state-of-the-art
descriptors even by simply replacing the training dataset.
We have not included them in our experiments but expect
the impact to be similar on our descriptor too.

5.2. Visualizing patch similarity.

We construct encodings g(v, p), before aggregation, for
our descriptors and for the conventional case and con-
struct a similarity map to analyze the impact of the po-

9399

Test

Train

HardNet+ †
HardNet+

⋆ψc,s =1

⋆ψc,s =2

# Parameters

Mean

No

Yo

Li

Yo

Li

No

Liberty

Notredame

Yosemite

1,334,560

1.51 ± 0.00

1.49± 0.00

2.51± 0.00

0.53± 0.00

0.78± 0.00

1.96± 0.00

1.84± 0.00

1,334,560

1.43 ± 0.02

1.25 ± 0.03

2.35 ± 0.03

0.48 ± 0.01

0.74 ± 0.02

2.15 ± 0.01

1.61 ± 0.10

867,008

1.53 ± 0.03

1.27 ± 0.03

2.31 ± 0.08

0.48 ± 0.02

0.82 ± 0.05

2.58 ± 0.08

1.72 ± 0.09

1,391,296

1.36 ± 0.01

1.14 ± 0.03

2.16 ± 0.10

0.42 ± 0.01

0.73 ± 0.02

2.18 ± 0.07

1.51 ± 0.12

Table 2. Performance comparison of the proposed descriptors with the state-of-the-art descriptor HardNet+ on the PhotoTourism dataset.
Performance is measured via FPR@95. We repeat each experiment/training 3 times and report mean performance and standard deviation.
Patch size is N = 32. †: Reported in the original work.

sition encoding. We present such visualization in Fig-
ure 2. We pick position p and compute similarity
a, p)⊤gf c(φq
gf c(φp
b, q),∀q ∈ [n]2, for the conventional
a, p) + mc)⊤(Mcgc(φq
case, and (Mcgc(φp
b, q) + mc),∀q ∈
[n]2 for ours in the case of the combined descriptor. We ob-
serve how all architectures, including the conventional one,
result in large similarity values near p.

The dimensionality of ψsum
is equal to d and not D in this
case. However, d = D = 128, making this descriptor di-
rectly comparable to all others.

a

Second, we train a descriptor that encodes the spatial in-
formation simply by concatenation, i.e. vectorization of φa,
which does not provide any tolerance to position misalign-
ments. It is given by

5.3. Results and comparisons.

We train and evaluate different variants of the proposed
descriptor. If not otherwise stated, we use input patches of
size equal to 32×32, which is the standard practice for deep
local descriptors. We further examine the case of 64 × 64
input patches. We always set d = 128 and D = 128. The
dimensionality of the feature maps is controlled by s which
we set equal to 1 or 2 in our experiments.

Reproducing HardNet+. Our implementation, train-
ing procedure, and training hyper-parameters are based on
HardNet+ 4. We reproduce its training and report our own
results, proving that our beneﬁt is not an outcome of imple-
mentation details. We report both the achieved performed
in the original publication and our reproduced ones in all
the comparisons.

Baselines for ablation study. We train and test the fol-
lowing two baselines to see the impact of the position en-
coding. First, we train a descriptor that encodes convolu-
tional descriptors in φa in a translation invariant way, i.e.
no position encoding at all.
It is implemented by spatial
sum pooling on φa and given by

a = X
ψsum

p∈[n]2

ψp
a.

(18)

4https://github.com/DagnyT/hardnet

ψcat

a = vec ψa

(19)

Impact of position encoding. We compare our descrip-
tor with HardNet+ on PT and show results in Table 2. Con-
ceptually it is a comparison between the conventional ar-
chitecture that uses an FC layer to “feed” the convolutional
descriptors to, and our kernel-based approach to explic-
itly encode the spatial information. Our descriptors (with
s = 2) slightly outperforms HardNet+ while it has roughly
the same number of parameters. Even the variant with fewer
parameters (s = 1) performs similarly.

A more thorough comparison, examining the impact of
the explicit spatial encoding, is performed on HP and pre-
sented in Figure 4. Firstly, we evaluate ψ SUM as part of
an ablation study. It is translation invariant that totally dis-
cards the spatial information. It does not require additional
parameters other than the ones for FCN φ. It has signiﬁ-
cantly lower performance compared to all the other descrip-
tors. We additionally tried including multiplication by ma-
trix Msum in (18) and did not notice performance improve-
ments. Descriptor ψ CAT is another case not requiring addi-
tional parameters. It is translation variant in a “rigid” way,
whose tolerance to translation misalignment is restricted to
the amount that the large receptive ﬁeld offers. Despite the
very large dimensionality, it is not a top performer. Even

9400

ψSUM

ψCAT
ψxy,s =1
ψρθ,s =1
ψxy,s =2
ψρθ,s =2
⋆ψc,s =1

HARDNET+
ψc,s =2
⋆ψc,s =2
⋆ψc,s =2 (64)

HARDNET+ (64)

84.29 ± 0.11
86.92
88.70 ± 0.02
88.37 ± 0.04
88.72 ± 0.04
88.64 ± 0.09
89.02 ± 0.13
88.61 ± 0.04
88.87 ± 0.07
89.16 ± 0.06
89.45 ± 0.04
88.68 ± 0.08

ψSUM

ψCAT
ψxy,s =1
ψρθ,s =1
ψxy,s =2
ψρθ,s =2
⋆ψc,s =1

HARDNET+
ψc,s =2
⋆ψc,s =2
⋆ψc,s =2 (64)

HARDNET+ (64)

38.18 ± 0.17

51.30
51.94 ± 0.09
51.83 ± 0.04
52.96 ± 0.05
52.58 ± 0.06
52.96 ± 0.16
53.09 ± 0.15
53.06 ± 0.07
53.73 ± 0.07
54.24 ± 0.07
53.50 ± 0.12

ψSUM

ψCAT
ψxy,s =1
ψρθ,s =1
ψxy,s =2
ψρθ,s =2
⋆ψc,s =1

HARDNET+
ψc,s =2
⋆ψc,s =2
⋆ψc,s =2 (64)

HARDNET+ (64)

61.97 ± 0.18
69.91
71.22 ± 0.08
71.39 ± 0.01
71.51 ± 0.05
71.61 ± 0.09
72.14 ± 0.1
71.26 ± 0.1
71.72 ± 0.08
72.29 ± 0.08
72.54 ± 0.02
71.49 ± 0.12

80 82 84 86 88 90

Image Veriﬁcation mAP [%]

35

40

45

50

55

Image Matching mAP [%]

60

65

70

75

Image Retrieval mAP [%]

Figure 3. Performance comparison on the HPatches benchmark. The training is performed on the Liberty set of PhotoTourism dataset for
all descriptors and with identical setup. Performance is measured via mean Average Precision (mAP). We repeat each experiment/training
3 times and report mean performance and standard deviation (with the exception of ψ CAT that due to very high dimensionality was trained
only once). All descriptors have 128 dimensions, with the exception of ψ CAT which has 8192. The methods are sorted w.r.t. the required
number of parameters (top is the least demanding, i.e. less parameters). All methods are trained and tested with patch size N = 32 unless
when (64) is reported.

RSIFT
BRIEF
ORB
SIFT
BBOOST
DC-S
DC-S2S
MKD
DDESC
TF-M
L2NET+
HARDNET+ †
HARDNET+
HARDNET+ (64)
⋆ψc,s =2
⋆ψc,s =2 (64)

57.21
57.90
59.74
64.39
66.46
70.03
77.90
78.81
79.18
81.36
86.38
88.43
88.61
88.68
89.16
89.45

BRIEF
BBOOST
ORB
DC-S
SIFT
RSIFT
DC-S2S
DDESC
TF-M
MKD
L2NET+
HARDNET+ †
HARDNET+
HARDNET+ (64)
⋆ψc,s =2
⋆ψc,s =2 (64)

10.29
14.72
15.16
25.10
25.78
27.21
27.76
28.34
32.76
37.16
45.49
52.76
53.09
53.50
53.73
54.24

BRIEF
ORB
BBOOST
SIFT
RSIFT
DC-S2S
DC-S
TF-M
MKD
DDESC
L2NET+
HARDNET+ †
HARDNET+
HARDNET+ (64)
⋆ψc,s =2
⋆ψc,s =2 (64)

26.32
29.52
34.18
43.04
43.26
47.57
48.53
52.41
53.50
53.98
63.59
69.66
71.26
71.49
72.29
72.54

0

20

40

60

80 100

Patch Veriﬁcation mAP [%]

0

20

40

60

Image Matching mAP [%]

0

20

40

60

80

Patch Retrieval mAP [%]

Figure 4. Performance comparison with the state of the art on the HPatches benchmark. The learning for learned descriptors is performed
on the Liberty set of PhotoTourism dataset. Hand-crafted descriptors are shown with striped bars. Performance is measured via mean
Average Precision (mAP). The performance of our descriptor is the mean of 3 repetitions of each experiment/training. All methods are
trained and tested with patch size N = 32 unless when (64) is reported. †: Reported in the original work.

our light-weight variant with as few as 127k additional pa-
rameters (excluding φ) recovers most of the performance
loss due to lack of spatial information, i.e. w.r.t.ψ SUM . This
result suggests that the common choice of an FC layer for
deep local descriptors might be over-parametrized. It is not
the best performing either. Our variant ⋆ψc,s =2 is consis-
tently the top performing one on all tasks.

Comparison with the state of the art. We ﬁnally
present a comparison to the state of the art on HP in Fig-
ure 4. The comparison includes a set of hand-crafted and
learned local descriptors, namely RSIFT [2], SIFT [25],
BRIEF [12], BBoost [52], ORB [41], MKD [32], Deep-
Compare [58], DDesc [46], TFeat [5], L2Net [49] and Hard-
Net [30]. The proposed descriptor achieves the best perfor-
mance with a 128D descriptor on all 3 tasks consistently.

6. Conclusions

We interpret conventional convolutional local descrip-
tors as efﬁcient match kernels and show that they learn spa-
tially variant encoding through that last FC layer. We design
a novel local descriptor that explicitly encodes the spatial
information. We use a combined position parametrization
handling different sources of geometric misalignment.
It
achieves the same performance as state-of-the-art descrip-
tors with fewer parameters and consistently outperforms
them on all standard patch benchmarks with the same num-
ber of parameters.

Acknowledgments This work was
the GA ˇCR grant 19-23165S,
project CZ.02.1.01/0.0/0.0/16 019/0000765
Center
SGS17/185/OHK3/3T/13.

by
the OP VVV funded
“Research
Informatics” and the CTU student grant

supported

for

9401

References

[1] Mitsuru Ambai and Yuichi Yoshida. Card: Compact and

real-time descriptors. In ICCV, 2011. 2

[2] Relja Arandjelovic and Andrew Zisserman. Three things ev-
eryone should know to improve object retrieval. In CVPR,
2012. 2, 8

[3] Vassileios Balntas, Edward Johns, Lilian Tang, and Krys-
tian Mikolajczyk. Pn-net: conjoined triple deep network for
learning local image descriptors. In arXiv, 2016. 2

[4] Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-
tian Mikolajczyk. Hpatches: A benchmark and evaluation of
handcrafted and learned local descriptors. In CVPR, 2017.
4, 6

[5] Vassileios Balntas, Edgar Riba, Daniel Ponsa, and Krystian
Mikolajczyk. Learning local feature descriptors with triplets
and shallow convolutional neural networks. In BMVC, 2016.
1, 2, 8

[6] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc
Van Gool. Speeded-up robust features (SURF). CVIU,
110(3):346–359, 2008. 1, 2

[7] Liefeng Bo, Kevin Lai, Xiaofeng Ren, and Dieter Fox. Ob-
In

ject recognition with hierarchical kernel descriptors.
CVPR, 2011. 2

[8] Liefeng Bo, Xiaofeng Ren, and Dieter Fox. Kernel descrip-

tors for visual recognition. In NIPS, December 2010. 2

[9] Liefeng Bo and Cristian Sminchisescu. Efﬁcient match ker-
nels between sets of features for visual recognition. In NIPS,
2009. 1, 2

[10] Matthew Brown, Richard Szeliski, and Simon Winder.
Multi-image matching using multi-scale oriented patches. In
CVPR, volume 1, pages 510–517, 2005. 2

[11] Andrei Bursuc, Giorgos Tolias, and Herv´e J´egou. Kernel
local descriptors with implicit rotation matching. In ICMR,
2015. 1, 2

[12] Micheal Calonder, Vincent Lepetit, Cristoph Strecha, and
Pascal Fua. Brief: Binary robust independent elementary
features. In ECCV, 2010. 2, 8

[13] Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and Cedric Bray. Visual categorization with
bags of keypoints. In ECCV Workshop Statistical Learning
in Computer Vision, 2004. 1

[14] Jingming Dong and Stefano Soatto. Domain-size pooling in

local descriptors: Dsp-sift. In CVPR, 2015. 1, 2

[15] Xufeng Han, Thomas Leung, Yangqing Jia, Rahul Suk-
thankar, and Alexander C Berg. Matchnet: Unifying fea-
ture and metric learning for patch-based matching. In CVPR,
2015. 2

[16] Chris Harris and Mike Stephens. A combined corner and
In Alvey vision conference, volume 15,

edge detector.
page 50, 1988. 1

[17] Marko Heikkila, Matti Pietikainen, and Cordelia Schmid.
Description of interest regions with local binary patterns.
Pattern recognition, 42(3):425–436, 2009. 2

[18] Herv´e J´egou, Florent Perronnin, Matthijs Douze, Jorge
S´anchez, Patrick P´erez, and Cordelia Schmid. Aggregating
local descriptors into compact codes. In IEEE Trans. PAMI,
September 2012. 1

[19] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.
Cross-dimensional weighting for aggregated deep convolu-
tional features. ECCVW, 2016. 1

[20] Yan Ke and Rahul Sukthankar. PCA-SIFT: a more distinctive
representation for local image descriptors. In CVPR, pages
506–513, June 2004. 2

[21] Theo Gevers Koen van de Sande and Cees Snoek. Evaluat-
ing color descriptors for object and scene recognition. IEEE
Trans. PAMI, 32(9):1582–1596, 2010. 2

[22] Iasonas Kokkinos and Alan Yuille. Scale invariance without

scale selection. In CVPR, 2008. 2

[23] Stefan Leutenegger, Margarita Chli, and Roland Y Siegwart.
Brisk: Binary robust invariant scalable keypoints. In ICCV,
2011. 2

[24] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski
Such, Eric Frank, Alex Sergeev, and Jason Yosinski. An
intriguing failing of convolutional neural networks and the
coordconv solution. In NIPS, 2018. 1, 2

[25] David G. Lowe. Distinctive image features from scale-

invariant keypoints. IJCV, 60(2):91–110, 2004. 1, 2, 8

[26] Zixin Luo, Tianwei Shen, Lei Zhou, Siyu Zhu, Runze Zhang,
Yao Yao, Tian Fang, and Long Quan. Geodesc: Learn-
ing local descriptors by integrating geometry constraints. In
ECCV, 2018. 2, 3, 6

[27] Krystian Mikolajczyk and Cordelia Schmid. Scale and afﬁne
invariant interest point detectors. IJCV, 60(1):63–86, 2004.
1

[28] Krystian Mikolajczyk and Cordelia Schmid. A perfor-
IEEE Trans. PAMI,

mance evaluation of local descriptors.
27(10):1615–1630, 2005. 1, 2

[29] Krystian Mikolajczyk, Tinne Tuytelaars, Cordelia Schmid,
Andrew Zisserman, Jiri Matas, F. Schaffalitzky, T. Kadir,
and Luc Van Gool. A comparison of afﬁne region detectors.
IJCV, 65(1/2):43–72, 2005. 1

[30] Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic,
and Jiri Matas. Working hard to know your neighbor’s mar-
gins: Local descriptor learning loss. In NIPS, 2017. 1, 2, 3,
5, 8

[31] Rahul Mitra, Nehal Doiphode, Utkarsh Gautam, Sanath
Narayan, Shuaib Ahmed, Sharat Chandran, and Arjun Jain.
A large dataset for improving patch matching.
In arXiv,
2018. 6

[32] Arun Mukundan, Giorgos Tolias, Andrei Bursuc, Herv´e
J´egou, and Ondrej Chum. Understanding and improving ker-
nel local descriptors. IJCV, 2019. 1, 2, 3, 4, 8

[33] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,
and Bohyung Han. Large-scale image retrieval with attentive
deep local features. In ICCV, 2017. 1

[34] David Novotny, Samuel Albanie, Diane Larlus, and Andrea
Vedaldi. Semi-convolutional operators for instance segmen-
tation. In ECCV, 2018. 1, 2

[35] Timo Ojala, Matti Pietikainen, and Topi Maenpaa. Multires-
olution gray-scale and rotation invariant texture classiﬁcation
with local binary patterns.
IEEE Trans. PAMI, 24(7):971–
987, 2002. 2

[36] Aude Oliva and Antonio Torralba. Modeling the shape of the
scene: a holistic representation of the spatial envelope. IJCV,
42(3):145–175, 2001. 2

9402

[55] Simon Winder and Matthew Brown. Learning local image

descriptors. In CVPR, 2007. 2, 6

[56] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal
In ECCV,

Fua. Lift: Learned invariant feature transform.
pages 467–483. Springer, 2016. 2

[57] Guoshen Yu and Jean-Michel Morel. A fully afﬁne invariant
In ICASSP, pages 1597–1600.

image comparison method.
IEEE, 2009. 2

[58] Sergey Zagoruyko and Nikos Komodakis. Learning to com-
In

pare image patches via convolutional neural networks.
CVPR, 2015. 1, 2, 8

[37] Florent Perronnin, Jorge S´anchez, and Thomas Mensink. Im-
proving the ﬁsher kernel for large-scale image classiﬁcation.
In ECCV, September 2010. 1

[38] James Philbin, Michael Isard, Josef Sivic, and Andrew Zis-
serman. Descriptor learning for efﬁcient retrieval. In ECCV,
2010. 2

[39] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. Fine-
In

tuning cnn image retrieval with no human annotation.
IEEE Trans. PAMI, 2018. 1

[40] Ali S Razavian, Josephine Sullivan, Stefan Carlsson, and At-
suto Maki. Visual instance retrieval with deep convolutional
networks.
ITE Trans. on Media Technology and Applica-
tions, 2016. 1

[41] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary
Bradski. Orb: An efﬁcient alternative to sift or surf. In ICCV,
2011. 2, 8

[42] Andrew M Saxe, James L McClelland, and Surya Ganguli.
Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks.
arXiv preprint arXiv:1312.6120,
2013. 6

[43] Cordelia Schmid and Roger Mohr. Local grayvalue invari-
ants for image retrieval. IEEE Trans. PAMI, 19(5):530–535,
1997. 2

[44] Paul Scovanner, Saad Ali, and Mubarak Shah. A 3-
dimensional sift descriptor and its application to action
recognition. In Proceedings of the 15th ACM International
Conference on Multimedia, pages 357–360, 2007. 2

[45] Eli Shechtman and Michal Irani. Matching local self-
similarities across images and videos. In CVPR, pages 1–8.
IEEE, 2007. 2

[46] Edgar Simo-Serra, Eduard Trulls, Luis Ferraz,

Iasonas
Kokkinos, Pascal Fua, and Francesc Moreno-Noguer. Dis-
criminative learning of deep convolutional feature point de-
scriptors. In ICCV, 2015. 1, 2, 8

[47] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
Learning local feature descriptors using convex optimisation.
IEEE Trans. PAMI, 36(8):1573–1585, 2014. 2

[48] Josef Sivic and Andrew Zisserman. Video Google: A text
In ICCV,

retrieval approach to object matching in videos.
2003. 1

[49] Bin Fan Yurun Tian and Fuchao Wu. L2-net: Deep learn-
ing of discriminative patch descriptor in euclidean space. In
CVPR, 2017. 1, 2, 3, 8

[50] Engin Tola, Vincent Lepetit, and Pascal Fua. Daisy: An efﬁ-
cient dense descriptor applied to wide-baseline stereo. IEEE
Trans. PAMI, 32(5):815–830, 2010. 2

[51] Giorgos Tolias, Andrei Bursuc, Teddy Furon, and Herv´e
J´egou. Rotation and translation covariant match kernels for
image retrieval. CVIU, 140:9–20, 2015. 3

[52] Tomasz Trzcinski, Mario Christoudias, Vincent Lepetit, and
Pascal Fua. Learning image descriptors with the boosting-
trick. In NIPS, 2012. 8

[53] Andrea Vedaldi and Andrew Zisserman. Efﬁcient additive

kernels via explicit feature maps. In CVPR, 2010. 3

[54] Peng Wang, Jingdong Wang, Gang Zeng, Weiwei Xu, Hong-
bin Zha, and Shipeng Li. Supervised kernel descriptors for
visual recognition. In CVPR, 2013. 2

9403

