Learning to Localize Through Compressed Binary Maps

Xinkai Wei1

2 ∗

,

Ioan Andrei Bˆarsan1

,

3 ∗ Shenlong Wang1

3 ∗

,

Julieta Martinez1 Raquel Urtasun1

3

,

1Uber Advanced Technologies Group

2University of Waterloo

3University of Toronto

{xinkai.wei,andreib,slwang,julieta,urtasun}@uber.com

Abstract

One of the main difﬁculties of scaling current localiza-
tion systems to large environments is the on-board storage
required for the maps. In this paper we propose to learn
to compress the map representation such that it is optimal
for the localization task. As a consequence, higher com-
pression rates can be achieved without loss of localization
accuracy when compared to standard coding schemes that
optimize for reconstruction, thus ignoring the end task. Our
experiments show that it is possible to learn a task-speciﬁc
compression which reduces storage requirements by two
orders of magnitude over general-purpose codecs such as
WebP without sacriﬁcing performance.

1. Introduction

One of the fundamental tasks in autonomous driving is
the ability to localize the self-driving vehicle (SDV) with
respect to a geo-referenced map, as this enables routing the
vehicle from point A to point B. Furthermore, high preci-
sion localization enables the use of high deﬁnition (HD)
maps that capture the static parts of the environment. This
map is used by most self driving teams as a component of
perception and motion planning modules.

LiDAR-based localization systems are usually employed
for precise localization of SDVs [3, 9, 12, 13, 30, 31]. They
rely on having an HD map, which contains dense point
clouds [29, 31] and/or intensity LiDAR images of the
ground [3, 12, 13, 30]. One of the main difﬁculties of scal-
ing current localization systems to large environments is the
on-board storage required for the HD maps. For instance,
storing a LiDAR intensity map as a 16-bit PNG ﬁle would
require roughly 900 GB for a city such as Los Angeles, and
over 168 TB for the entire United States.1 Storing this infor-
mation onboard the vehicle is infeasible for scalability past

∗Equal contribution
1Based on information from the US Bureau of Transportation Statistics,
assuming that it takes approximately 4 MB to store a 150 m road segment
as a 16-bit PNG single-channel image (https://www.bts.gov ).

Figure 1: End failure rate for localization under differ-
ent map compression settings. Lower is better.

a single city. Streaming the HD map data on the go makes
the system dependent on a reliable broadband connection,
which may not always be available.

In this paper we propose to learn to compress the map
representation such that it is optimal for the localization
task. As a consequence, higher compression rates can be
achieved without loss of localization accuracy and robust-
ness compared to standard coding schemes that optimize
for reconstruction, thus ignoring the end task. In particular,
we leverage a fully convolutional network to learn to bina-
rize the map features, and further compress the binarized
representation using run-length encoding on top of Huff-
man coding. Both the binarization net and the decoder are
learned end-to-end using a task-speciﬁc loss. We demon-
strate the effectiveness of this idea in the context of a state-
of-the-art LiDAR intensity-based localization system [3],
and show that it is possible to learn a task-speciﬁc compres-
sion scheme which reduces storage requirements by two
orders of magnitude over general-purpose codecs such as
WebP, without sacriﬁcing performance.

2. Related Work

Localization Using HD Maps: High-deﬁnition maps
have been widely used in the ﬁeld of robot localization due

110316

102101100101Bits Per Pixel (log scale)23456789Localization Error (cm)1.050.580.400.301.030.480.280.184.940.0112PNGWebPJPGOursFigure 2: Architecture overview of our proposed joint compression and localization network.

to their ability to enable centimeter-level accuracy in a di-
verse set of environments, while avoiding some of the com-
putational costs typically associated with a full SLAM sys-
tem. Levinson and Thrun [12] used Graph-SLAM to aggre-
gate LiDAR observations into a coherent map, which was
then used in localization. K¨ummerle et al. [11] use Multi-
Level Surface Maps [26] with LiDAR, and use the map for
localization and path planning to enable a car to park itself.
Subsequent works have improved the robustness of such ap-
proaches by augmenting the maps with probabilistic occu-
pancy information [13, 30], or by fusing LiDAR matching
results with differential GPS [27].

Lightweight Localization: Numerous alternatives to HD
maps have been explored over the years in an attempt to
overcome their limitations, such as the dependence on data
collection and ofﬂine map construction. Floros et al. [6]
develop a lightweight extension to visual odometry which
leverages OpenStreetMap to eliminate the drift typically as-
sociated with pure VO. Ma et al. [15] extend this idea by us-
ing cues from multiple modalities, such as egocar trajectory,
road type and the position of the sun to localize robustly
within a lightweight map represented as a graph. Recently,
Ort et al. [17] used a similar approach, performing road
segmentation using LiDAR to localize within a lightweight
topological map with negligible on-board map storage re-
quirements. Javanmardi et al. [9] extract probabilistic 3D
planar surfaces and 2D vector maps from dense point clouds
to create lightweight maps which are then used in localiza-
tion, achieving promising results in terms of accuracy. In
recent years, image-based localization methods [5, 18, 22]
have shown promising results reaching centimeter-level ac-
curacy in indoor environments. However, these methods are
still not sufﬁcient for centimeter-level accuracy in outdoor
scenarios exhibiting fast motion, such as those occurring in

self-driving.

Image Compression:
Image compression is a classical
subﬁeld of computer vision and signal processing. It has
seen a great deal of progress in the past few years thanks
to the advent of deep learning.
In most modern incarna-
tions, a learning-based compression method consists of an
encoder network, a quantization mechanism (e.g., binary
codes), and a decoder network which reconstructs the in-
put from the quantized codes. Recent learning-based ap-
proaches [16, 20, 25] consistently outperform classic com-
pression methods like JPEG2000 and BPG. While earlier
works on learned compression typically used a standard au-
toencoder architecture [2], thereby imposing a ﬁxed code
size for all images, Toderici et al. [24] overcome this limita-
tion by using a recurrent neural network as an encoder. Sub-
sequently, Toderici et al. [25] extend the previous results,
which were typically presented on resolutions of 64 × 64
or less due to performance considerations, showing state-
of-the-art compression rates on full-resolution images. Re-
cently, Mentzer et al. [16] proposed a pipeline which ob-
tained results on par with the state-of-the-art, while also be-
ing trainable end-to-end (encoder, quantizer, decoder).

Learning to Match: Learning-based approaches have
also been employed in matching problems, which arise
in numerous vision applications, including stereo match-
ing [14, 32], optical ﬂow [19, 28], and map-based local-
ization [3].
In their pioneering work, Zbontar and Le-
Cun [32] proposed modeling the matching cost function
used in stereo depth estimation as a convolutional neural
network and learning it from data. Luo et al. [14] ex-
tend this framework to produce calibrated probability dis-
tributions over the disparities of all pixels, while also en-
abling real-time operation through the introduction of an

10317

Online embedding moduleMatching moduleCompression moduleMap embedding moduleInput HD mapOnline LiDAR sweepPosition scoreMap featureCross-correlationDeep netReconstruction from binary codes...Online featureSpatial-transformed featuresApply rotationsDeep netBinarized map...explicit correlation layer capable of speeding up inference
by an order of magnitude compared to previous work. Sim-
ilarly, DeepFlow [28] applies learning-based matching to
the problem of optical ﬂow estimations. The authors use
a learned matcher to match sparse descriptors, which are
then fed into a variational method to estimate dense ﬂow.
EpicFlow [19] extends this framework by densifying the
sparse matches using a novel interpolation scheme before
performing variational energy minimization. The task of
map-based localization using matching has also been ap-
proached from a data-driven perspective, using neural net-
works to learn representations optimal for matching a Li-
DAR observation to a map [3].

3. End-to-End Compressed Localization

LiDAR based localization systems are usually employed
by self-driving vehicles to provide high-precision localiza-
tion estimates [3, 13, 30]. They rely on having an HD map,
which contains dense point clouds [31] and/or LiDAR in-
tensity images [13, 30]. One of the main difﬁculties of scal-
ing localization to large environments is the on-board stor-
age required for these maps. To tackle this problem, in this
paper we propose to learn to compress the map represen-
tation such that it is optimal for the localization task. As
a consequence, higher compression rates can be achieved
without loss of localization accuracy or robustness degrada-
tion. In particular, our approach learns a compressed deep
embedding of the map that can be directly stored on-board,
dramatically reducing the requirements of state-of-the-art
LiDAR intensity based localization systems.

In this section, we ﬁrst revisit the state-of-the-art Deep
Ground Intensity Lidar Localizer (Deep GILL) [3] and its
probabilistic Bayes inference. We then describe our com-
pression module and show how it can be learned end-to-end
jointly with the localizer.

3.1. Deep GILL Revisit

Real-time localization with centimeter level accuracy is
critical for most self-driving cars, as they rely on the seman-
tics captured in HD maps to drive safely. In this paper, we
follow [3]’s formulation for our localization as a recursive
Bayes inference problem. In particular, the Bayes inference
framework combines the LiDAR matching energy, the vehi-
cle dynamics, and the GPS observations with the estimates
of the previous time step to form the probability of a given
location at the current time step. Let Bel(xt) be the poste-
rior distribution of the vehicle pose at time t given all the
sensor observations until time step t, we have:

Belt(x) = Belt|t−1(x; X ) · PGPS(Gt|x) · PLiDAR(It|x; w),
(1)

where x = {tx, ty, θ} is the 3-DoF vehicle pose, and It ∈
RNt×4 is the online LiDAR sweep containing Nt points

with geometric and intensity information; Xt = vx, vθ is
the vehicle dynamics encoding linear and angular velocity;
and Gt ∈ R2 are GPS observations under Universal Trans-
verse Mercator (UTM) coordinate system.

The motion model encodes the fact that the inferred pose
should agree with the vehicle dynamics given the previous
time step location belief Belt−1(xt−1), more formally de-
ﬁned as

Belt|t−1(x|Xt) = Zxt−1

P (x|Xt, xt−1)Belt−1(xt−1).

(2)
We use a Gaussian to represent the vehicle’s conditional dis-
tribution over vehicle dynamics,

P (x|Xt, xt−1) ∝ N (xt−1 ⊕ Xt, Σ),

(3)

where xt−1 ⊕ Xt is the last timestamp’s pose composed by
the current timestamp’s velocity observation; ⊕ is the pose
composition operator, and Σ is the covariance matrix for the
velocity estimation. The GPS observation model encodes
the likelihood of the GPS as a Gaussian distribution:

PGPS ∝ N ([gx, gy]T , σ2

GPSI),

(4)

where gx and gy is the map-relative position x converted to
UTM coordinate system.

The LiDAR matching model encodes the agreement be-
tween the current online LiDAR observation and the map
indexed at the hypothesized pose x:

PLiDAR ∝ s (π (f (I; wO), x) , g(M; wM)) ,

(5)

where f (·) and g(·) are the embedding network over on-
line LiDAR sweeps and maps respectively, and π is a rigid
transform that converts the online embedding image to the
map coordinates using the hypothesized pose x; M is the
dense LiDAR intensity map representation, and s is the cor-
relation operator between the online embedding and map
embedding. The computation of this term can be written as
a feed-forward network as shown in [3].

While effective for localization, the dense intensity map
used in Deep GILL [3] requires a large amount of on-board
storage. This prevents the method from scaling to larger
operational domains. To tackle this problem, in this paper
we introduce a novel learning-to-compress module that re-
duces the storage of the map signiﬁcantly, allowing us to
potentially store maps for the full continent. Next, we de-
scribe our new compressed model.

3.2. Deep Localization with Map Compression

Unlike previous compression networks that aim at opti-
mizing the reconstruction error or perceived visual quality,
in this paper we argue that optimizing for the tasks at hand

10318

Figure 3: Our compression module. We obtain gradients for training with a straight-through estimator.

is important to further reduce the storage requirements. To-
wards this goal, we extends the architecture of [3] and in-
clude a compression module responsible for encoding the
map with binary codes through deep convolutional neural
networks. Importantly, our encoding can be learned end-to-
end with our localizer.

We refer the reader to Fig. 2 for an illustration of the
overall architecture of our joint compression and matching
network. Our overall end-to-end network includes three
components. First, our embedding module takes the map
M and the online LiDAR sweep I as input, and com-
putes a deep embedding representation of both. A compres-
sion module is then applied over the map embedding layer,
which converts the high-dimensional ﬂoat-valued deep em-
bedding map to a compact convolutional binary code repre-
sentation. This representation is used as a compact storage
of the map. A decoding module is then employed to de-
code the binary codes back to the real-valued embedding
representation. Finally, matching is conducted between the
reconstructed map embedding and the online embedding.
This gives us a score for each possible transformation. We
use softmax to build the probability PLiDAR over our local-
ization search space from the raw matching score. We now
describe the modules in more detail.

Embedding Module: The embedding should capture ro-
bust yet discriminative contextual features while preserving
pixel-accurate details for precise matching. Motivated by
this fact, we designed this module to be a fully convolu-
tional encoder-decoder network following [3]. It has an U-
Net architecture [21]. The encoder consists of four blocks,
each of which has two stride-1 3 × 3 conv layers and one
stride-2 3 × 3 conv layer that down-samples the feature map
by a factor of 2. The numbers of channel per each block
are 64, 128, 256, 512, respectively. The decoder network
has four decoder blocks, each of which takes the last de-
coder block’s output and the corresponding encoder layer
feature as input in an additive manner. Each block contains
one 3 × 3 deconv layer followed by one stride-1 3 × 3 conv.

The ﬁnal embedding map has the same spatial resolution
as the input with depth equals to embedding dimension. In
this way the decoder combines both high level contextual
information as well as low-level details.

Compression Module: We highlight
the task-speciﬁc
map compression module as the core contribution of this
paper. The purpose of this module is to convert the large-
resolution, high-precision embedding into a low-precision,
lower-resolution one, without losing critical information
for matching. We employ a fully convolutional encoder-
decoder network to achieve this goal. The neural network
encoder is a fully convolutional residual network where
each scale have two 3 × 3 standard residual blocks [8] and
a stride-2 3 × 3 conv between scales. The dimensional-
ity per scale is 8, 16, 32, 64 respectively. The decoder is
a fully convolutional network with several transposed con-
volutional layers. We use the PReLU [7] as the activation
function. The output of the encoding module is passed
through a grouped soft-max module, with a binarization
module deﬁned as

exp(fj)

exp(fk)

,

bj = (cid:26) 1

0

if pj ≥ 0.5
otherwise

(6)

pj =

Pk∈Sj

where Sj is the index group that j belongs to, with each
group representing a non-overlapping subset of the full in-
dex set {1, . . . , K}; f = [f0, . . . , fi, . . . ] is the input fea-
ture. The beneﬁt of using grouped-softmax as encoder acti-
vation along with the binarizer is twofold. First, within each
group, we have at most one non-zero entry. Thus, with the
same number of channels it has better sparsity than the sig-
moid function, increasing the compressibility of the binary
encoding. Second, compared against standard soft-max, it
increases the potential capacity since the grouping of in-
dices allows a more structured encoding. While the compo-
nent is non-differentiable, backpropagation was still feasi-
ble thanks to the use of a straight-through estimator, which
we will show in Sec. 3.3 in detail.

10319

 Compression moduleBinarized mapGroup softmaxBinarizeRun-length encodingHuffman encoding01100010...01Binary decoderInput featureDecoded featureNN encodingNN decodingStraight-through estimatorInference OnlyLearning OnlyMap storage...blocks to recover the full high resolution, high-precision
embedding of the map that we use for matching. Fig. 3
illustrates the pipeline of the full compression module.

Matching Module: Our matching module follows [3],
where a series of spatial transformer networks are utilized
to rotate the online embedding multiple times at |Θ| differ-
ent candidate angles. Within each rotation angle, transla-
tional search based on inner-product similarity is equivalent
to convolving the map embedding with the online embed-
ding as kernels. Thus, enumerating all the possible pose
candidates is equivalent to a convolution with |Θ| kernels.
Unlike standard convolutions, this convolution has a very
large kernel. Following [3], we exploit FFT-conv to ac-
celerate this matching modules by an order of magnitude
(compared to GEMM-based convolutions) on a GPU.

3.3. End to End Learning

Our full localization network is trained end-to-end, as
the compression module is active during the training loop.
Our loss function consists of two parts, namely a matching
loss that encourages that the end-task is accurate and a com-
pression loss that minimizes the encoding length. Thus, our
total loss is deﬁned as

ℓ = ℓLOC(y, yGT) + λ1ℓMDL(p) + λ2ℓSPARSE(p),

(7)

where y is the ﬁnal softmax-normalized matching score,
yGT is the one-hot representation of the ground truth (GT)
position and p is the embedding after the grouped-softmax
layer in the compression module, deﬁned in Eq. 6.

We employ cross-entropy as a matching loss. This en-
courages the matching score to be the highest at the GT
position, while lowering the score of positions elsewhere:

ℓLOC(y, yGT) = Xi

yGT,i log(yi).

The compression loss tries to minimize the encoding
length. In particular, we use entropy as a differentiable sur-
rogate of code length. Note that this surrogate has been
widely used in previous deep compression approaches [24].
According to Shannon’s source coding theorem [23], en-
tropy provides an optimal code length, which could serve
as a surrogate lower-bound for the actual encoding that we
use. Our entropy is estimated within each mini-batch as

ℓMDL(p) = ¯p log ¯p,

1

W ×H×B Pi pi is the mean soft-max probabil-

where ¯p =
ity averaged across all pixels’ softmax probability pi in one
batch example, deﬁned as in Eq. 6. In practice we ﬁnd that
this theoretical lower-bound is very close to the actual bit
per pixel rate obtained after Huffman+RLE encoding.

10320

Figure 4: Top-1 Matching Performance vs Bits Per Pixel

Method

BPP

Top-9 px

Top-1 px

Lossless (PNG)
Ours (recon, 8x)
Ours (recon, 16x)
Ours (match, 8x)
Ours (match, 16x)

4.93
0.0520
0.0140
0.0098
0.0070

77.40%
97.47%
75.83%
97.27%
96.95%
74.86%
97.73% 76.05%
97.25%
73.17%

Table 1: Ablation studies on matching performance. Op-
timizing jointly for both map reconstruction and matching
greatly reduces the memory requirements.

Thus, we only need to store onboard these highly com-
pressed binary map embeddings for localization. A two-
step lossless binary encoding scheme is adopted. Our ﬁrst
step is a Huffman encoding. The motivation is that the fre-
quency of appearance of items are not equal. The Huffman
dictionary is built by the one-hot encoding of the softmax la-
tent probability p per pixel. Thus, for a 128-softmax vector
the dictionary size is 128. Frequency is computed in a batch
manner. For instance, if the ‘class‘ 5(00000101) appears
50% we could use a shorter-length code 0 to encode it. After
that a run-length encoding (RLE) is conducted over the ﬂat-
ten Huffman code map to further reduce the size by making
use of the fact that codes appear consecutively. For instance
5555558 could be further reduced to 5681. This give us
the ﬁnal binary code that we store. Note that both Huffman
encoding and run-length encoding are lossless. We choose
Huffman+RLE due to its efﬁciency and effectiveness. Em-
pirically, we also show that this approach reaches 72.5% of
the ideal entropy lower bound. While other types of entropy
coding, such as arithmetic coding exist, they are slower and
bring marginal improvements to compression rates [1].

Combining the NN encoding and the binary encoding,
this full compressive encoding scheme gives a very large
gain in terms of storage efﬁciency as shown later in the ex-
perimental section.

The decoder module then takes the binary code as in-
put. First, it transforms the Huffman+RLE codes back to
the binary map, and then applies a series of deconvolutional

102101100101Bits Per Pixel (log scale)7072747678Top-1cm Validation Accuracy (%)1.050.580.400.300.181.891.030.680.490.390.284.940.00980.0070WebPPNGJPGOurs (8x)Ours (16x)Finally, we want the soft-max probability to be as close
to one-hot as possible to reduce the loss due to hard bina-
rization. We thus minimize each individual pixel’s entropy
as a regularization term:

ℓSPARSE(p) = Xi

pi log pi

Note that direct backpropagation is not feasible in our
case, as the binarization module deﬁned in Eq. 6 is not dif-
ferentiable. To overcome this issue, we adopt the straight-
through estimator proposed in [4]. That is, during the for-
ward pass we conduct hard binarization, while during the
backward pass we substitute this module with an identity
function. We ﬁnd that this approximation provides good
gradients for the function to be learned.

3.4. Efﬁcient Inference

In the ofﬂine map encoding stage, we use our compres-
sion network to compress the map into a binary code such
that the onboard storage requirements are minimized. Dur-
ing onboard inference, the compressed code is recovered,
the decoder is then used to create the HD embedding map,
which is used for localization.

Onboard Inference: Computing the exact probability de-
ﬁned in Eq. 1 is not feasible due to the continuous space
and the infeasible integral for the vehicle dynamics model
deﬁned in Eq. 2. Following [3], we use a histogram ﬁlter
to approximate the inference process. Towards this goal,
we discretize the search space around a local region to a
5 × 5 cm grid. The integration will only be computed within
this local trust-region, which neglects the rest of the solution
space where the belief is negligible. In this manner, both
the GPS and the dynamics term can be computed efﬁciently
over a local grid as the search space. Unlike [3], when
computing the LiDAR matching term PLiDAR(It|x; w), we
ﬁrst retrieve a local map binary code b. The LiDAR em-
bedding is computed through the feature network and then
b is passed through the decoder of the compressor to re-
cover the map embedding g(M). After that, the matching
score PLiDAR can be efﬁciently computed for all hypothe-
sized poses as a feed-forward network through FFT-conv.
After each term has been computed, the ﬁnal pose estima-
tion is a soft-argmax aggregation taking uncertainty into
consideration:

x∗

t = Px Belt(x)α · x
Px Belt(x)α

where α ≥ 1 is a temperature hyper-parameter.

(8)

4. Experimental Evaluation

Dataset: We evaluate our approach over two large-scale
driving datasets that cover highway and urban driving, re-
spectively. The highway dataset was collected in [3] and

contains over 400 sequences of highway driving with a total
of 3 000 km travelled. It contains an HD, dense, LiDAR in-
tensity map stored in lossless PNG format. The self-driving
vehicle integrates a 64-line LiDAR sweeping at 10Hz, with
GPS and IMU sensor. We follow the setting of [3] and se-
lect 282 km of driving as testing, ensuring that there is no
geographic overlap between the splits. The GT localization
is estimated by a high-precision ofﬂine Graph-SLAM.

To better evaluate the potential of the model to compress
maps with more diverse content and complicated structures,
we build a new urban driving dataset that consists of 15 554
km of driving. This dataset is collected in a metropolitan
city in North America with diverse scenes and road struc-
tures. This dataset is more challenging as it introduces more
diverse vehicle maneuvers, including sharp turns and re-
verse driving, as well as some regions with poor lane mark-
ings and map changes. The ground-truth localization is esti-
mated through an high-precision ofﬂine Graph-SLAM with
multi-sensor fusion.
Intensity maps are built by multiple
passes through a comprehensive ofﬂine pose graph opti-
mization at a resolution of 5cm per pixel.

Experimental Setup: To our knowledge, no previous
work has integrated LiDAR intensity localization with deep
compression. Therefore, we evaluate our work against base-
lines without compression on localization metrics alone,
such as those found in [3], and measure the performance
degradation when using the map compression module.

Since the intensity map is stored as an image, we com-
pare against several traditional image compression algo-
rithms such as JPEG and WebP. For each compression al-
gorithm, we compress the training and testing map im-
ages and train a standard learn-to-localize matching net-
work [3]. We also train a reconstruction-based compression
network (‘ours (recon)’) that shares the same architecture
with our compression module, with the only exception that
it is trained for reconstruction error of the feature map only
(not for matching performance). This showcases whether
the task-speciﬁc compression helps our matching task.

For our proposed method, we adopt two different set-
tings for compresssion, by changing the downsampling lev-
els we used for our binary codes. We have 8x downsam-
pling model and 16x downsampling model, where the 16x
model has an extra set of downsampling and upsampling
modules before binarization, which varies the compression
rate. All the competing algorithms have the same embed-
ding feature network as our proposed model. Additionally,
we performed experiments with reduced map resolution as
an alternative baseline for reducing map storage.

We train all competing algorithms over 343k and 230k
training samples for urban and highway datasets respec-
tively. We aggregate ﬁve online LiDAR sweeps and ras-
terize them into a birds’ eye view image at 5cm/pixel, in
ranges of (−12 m, 12 m) and (−15 m, 15 m). All networks

10321

Figure 5: Qualitative results from our highway dataset. From left to right: (1) original map, (2) its computed deep
embedding, (3) the compressed embedding, (4) online LiDAR observation, (5) its embedding, and (6) the localization result.

Method

Median error (cm)
Lat

Lon

Total ≤ 100m ≤ 500m

Failure rate (%)

Bit per pixel

Lossless (PNG)
JPG-5
JPG-10
JPG-20
JPG-50
WebP-5
WebP-10
WebP-20
WebP-50
Ours

1.55
4.32
3.42
3.77
3.29
1.65
1.60
1.86
1.62
1.61

2.05
5.48
5.46
4.99
5.60
5.75
2.26
2.85
2.75
2.26

3.09
8.41
7.54
7.51
7.59
6.53
3.48
4.10
3.76
3.47

0.00
0.00
0.00
0.00
0.00
2.04
0.00
4.08
0.00
0.00

1.09
1.09
1.09
0.00
1.09
5.43
1.09
8.70
3.26
1.09

End

2.44
1.25
5.26
1.75
5.26
13.95
2.50
14.63
3.30
1.22

4.94
0.18
0.28
0.48
1.03
0.30
0.40
0.58
1.05
0.0083

Table 2: Online localization performance on the urban dataset.

are trained on four NVIDIA 1080 Ti GPUs using PyTorch.
We use the Adam optimizer [10] with an initial learning rate
of 10−3. We observed that training the entire network end-
to-end from scratch works, but is slower to converge. To
speed up training, we ﬁrst train the non-compressed match-
ing network without our additions for the localization task,
and then insert our compression module and train end-to-
end.

Matching Performance:
In order to evaluate the perfor-
mance of the models in terms of ﬁnding the best match in a
compressed map, we report the performance of the compet-
ing algorithms under the matching setting.

We conduct matching over a 1 m2 search range, after
perturbing the initial position of the vehicle around the GT
position. We sample the translational perturbation between
0 and 1 m2, and the angular perturbation between 0 and 5◦,
both uniformly. We report top-1 px and top-9 px as our met-
rics, representing whether the prediction is in the same pixel
as the GT or within the 3×3 region centered around the GT,
respectively. We report matching accuracy as a function of
bit rate per pixel on the urban dataset in Fig. 4. Note that
the proposed algorithm at 8x setting achieves 76% top-1 px
accuracy with 0.0098 bit per pixel rate. Both are higher than
all competing algorithms. Also, it obtains similar top-9 px
accuracy on par with no compression module. Especially,

the BPP is around 20-400 times smaller than all competing
algorithms. Under the 16x setting the top 1 px accuracy is
3% lower but achieves a higher compression rate at 0.007
bits per pixel.

Ablation Studies: We conducted an ablation study over
the matching performance. We ﬁrst validate whether jointly
training the compression module with our matching task
loss helps improve the matching performance and increase
the compression rate. For this, we train a compression
module using only reconstruction loss (without the match-
ing task loss). Secondly, we report whether a lower com-
pression rate is achieved through aggressive map down-
sampling. Table 1 illustrates the results. We can see that
jointly training the compression module with the task spe-
ciﬁc loss greatly helps the performance. The 16x downsam-
pled model pushes the compression rate even further, with
a 3% percent drop on top 1px results.

Online Localization: We follow [3] and compute the me-
dian and worst case localization error on the test split as our
metrics. To be speciﬁc, we report median, p95, and p99 er-
ror in meters along the lateral and longitudinal directions.
We also report an out-of-range rate, which represents the
percentage of 1 km segments where the method reaches a
localization error of 1m.

10322

Method

Median error (cm)
Lat

Lon

Total ≤ 100m ≤ 500m

Failure rate (%)

Bit per pixel

Lossless (PNG)
WebP-50
WebP-20
WebP-10
WebP-5
Ours

3.62
3.87
4.03
4.45
4.10
3.62

4.53
4.87
5.27
7.09
6.40
4.77

7.06
7.52
8.02
9.79
8.99
7.19

0.00
0.00
0.00
0.35
0.35
0.35

0.35
0.71
1.06
9.57
9.57
0.35

End

0.72
0.71
8.87
24.37
14.69
0.71

4.97
0.58
0.36
0.26
0.20
0.007

Table 3: Online localization performance on the highway dataset.

Method

Median error (cm)
Lat

Lon

Total ≤ 100m ≤ 500m End

Failure rate (%)

Bit per pixel

Lossless (PNG)
Ours (recon, 8×)
Ours (recon, 16×)
Ours (match, 8×)
Ours (match, 16×)

1.55
1.59
1.76
1.61
1.62

2.05
2.16
2.48
2.26
2.77

3.09
3.24
3.62
3.47
3.84

0.00
0.00
0.00
0.00
1.00

1.09
1.09
0.0
1.09
2.17

2.44
1.22
2.56
1.22
4.26

4.97
0.027
0.012
0.021
0.007

Table 4: Ablation studies on the urban dataset.

Table 2 shows the online localization performance on the
urban dataset. While most of the baselines provide reason-
able results, our method is clearly better than competing al-
gorithms such as JPG-5 and WebP-5, which show high fail-
ure rates at extreme compression levels. In terms of worst
case, measured by failure rate, our method is on par with
high-quality compression such as WebP-50 and JPEG-50,
and a lossless method, while our bit rate per pixel is 100
times smaller. This is shown in in Fig. 1, where we plot the
percentage of failures after 1 km against the storage.

Table 3 depicts the online localization performance on
the highway dataset. We can see that traditional off-the-
shelf compression algorithms like WebP have a large perfor-
mance drop compared to our compression-based matching.
While the method using reconstruction loss obtains storage
roughly in the same magnitude as our approach, it suffers
a large performance drop. This indicates the importance of
matching loss term for effectively selecting portions of the
map to keep. Meanwhile, our method based on the match-
ing task loss has no performance drop at half the storage of
the pure reconstruction network, nor at more than 400 times
smaller compared to the lossless compression bitrate.

Table 4 showcases the ablation study on the urban
dataset. We compare reconstruction loss driven compres-
sion models against our matching-loss driven compression
model under various architectures. From the Tables we can
see that, in terms of online localization error, our compres-
sion model trained with task-speciﬁc driven loss is better
than the reconstruction model with smaller bitrates and a
lower failure rate.

Compression

LA County

Full US

Lossless (PNG)
WebP 1
Ours (match, 8x)

168 TB
900 GB
32 GB
5.98 TB
1.8 GB 0.33 TB

Table 5: Estimated map storage requirements using var-
ious compression methods.

compression) together with the (uncompressed) online ob-
servation embedding and the localization result. For more
results please refer to the supplementary material.

Storage Analysis: We now turn back to the approximate
storage requirements described in the introduction, and
showcase projected numbers when compressing all maps
using our proposed method in Tab. 5. Our proposed method
can compress a 5cm/px HD map of the entire Los Angeles
county to just 1.8 GB, allowing it to ﬁt in RAM on most
current smartphones. We can also ﬁt the entire USA road
network at the same resolution in just 330 GB.

5. Conclusions

HD maps impose high storage requirements, which limit
the ability of a self-driving ﬂeet to operate in large-scale en-
vironments. In this paper, we proposed to learn to compress
the map representation such that it is optimal for the local-
ization task. Our experiments on a state-of-the-art localizer
have shown that it is possible to learn a task-speciﬁc com-
pression scheme that reduces storage requirements by two
orders of magnitude compared to general-purpose codecs
such as WebP, without sacriﬁcing localization performance.

Qualitative Analysis: Fig. 5 shows examples of the deep
map embeddings computed by our system (before and after

References

[1] JPEG - Wikipedia, 2019. 5

10323

[19] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and
Cordelia Schmid. Epicﬂow: Edge-preserving interpolation
of correspondences for optical ﬂow. In CVPR, 2015. 2, 3

[20] Oren Rippel and Lubomir Bourdev. Real-time adaptive im-
age compression. arXiv preprint arXiv:1705.05823, 2017.
2

[21] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 4

[22] Torsten Sattler, Akihiko Torii, Josef Sivic, Marc Pollefeys,
Hajime Taira, Masatoshi Okutomi, and Tomas Pajdla. Are
Large-Scale 3D Models Really Necessary for Accurate Vi-
sual Localization? In CVPR, 2017. 2

[23] Claude Elwood Shannon. A mathematical theory of com-
munication. Bell system technical journal, 27(3):379–423,
1948. 5

[24] George Toderici, Sean M O’Malley, Sung Jin Hwang,
Damien Vincent, David Minnen, Shumeet Baluja, Michele
Covell, and Rahul Sukthankar. Variable rate image com-
pression with recurrent neural networks.
arXiv preprint
arXiv:1511.06085, 2015. 2, 5

[25] George Toderici, Damien Vincent, Nick Johnston, Sung Jin
Hwang, David Minnen, Joel Shor, and Michele Covell.
Full resolution image compression with recurrent neural net-
works. In CVPR, pages 5435–5443, 2017. 2

[26] Rudolph Triebel, Patrick Pfaff, and Wolfram Burgard. Multi-
level surface maps for outdoor terrain mapping and loop
closing. In Intelligent Robots and Systems, 2006 IEEE/RSJ
International Conference on, pages 2276–2282. IEEE, 2006.
2

[27] Guowei Wan, Xiaolong Yang, Renlan Cai, Hao Li, Yao
Zhou, Hao Wang, and Shiyu Song. Robust and precise vehi-
cle localization based on multi-sensor fusion in diverse city
scenes. In ICRA, 2018. 2

[28] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and
Cordelia Schmid. Deepﬂow: Large displacement optical
ﬂow with deep matching. In ICCV, 2013. 2, 3

[29] Ryan W. Wolcott and Ryan M. Eustice. Visual localization
In IROS,

within lidar maps for automated urban driving.
2014. 1

[30] Ryan W. Wolcott and Ryan M. Eustice. Fast lidar localiza-
tion using multiresolution gaussian mixture maps. In ICRA,
2015. 1, 2, 3

[31] Keisuke Yoneda, Hossein Tehrani, Takashi Ogawa, Naohisa
Hukuyama, and Seiichi Mita. Lidar scan feature for local-
ization with highly precise 3-d map. In IV, 2014. 1, 3

[32] Jure Zbontar and Yann LeCun. Computing the stereo match-
In CVPR,

ing cost with a convolutional neural network.
2015. 2

[2] Johannes Balle, Valero Laparra, and Eero P. Simoncelli.
End-to-end optimization of nonlinear transform codes for
perceptual quality. 2016 Picture Coding Symposium (PCS),
2016. 2

[3] Ioan Andrei Bˆarsan, Shenlong Wang, Andrei Pokrovsky, and
Raquel Urtasun. Learning to localize using a lidar intensity
map. In Proceedings of The 2nd Conference on Robot Learn-
ing, 2018. 1, 2, 3, 4, 5, 6, 7

[4] Yoshua Bengio, Nicholas L´eonard, and Aaron Courville.
Estimating or propagating gradients through stochastic
neurons for conditional computation.
arXiv preprint
arXiv:1308.3432, 2013. 6

[5] Eric Brachmann and Carsten Rother. Learning Less is More
- 6D Camera Localization via 3D Surface Regression. arXiv,
2017. 2

[6] Georgios Floros, Benito van der Zander, and Bastian Leibe.
OpenStreetSLAM: Global vehicle localization using Open-
StreetMaps. In ICRA, 2013. 2

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation. In ICCV, 2015. 4

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 4

[9] Ehsan Javanmardi, Mahdi Javanmardi, Yanlei Gu, and Shun-
suke Kamijo. Autonomous vehicle self-localization based on
probabilistic planar surface map and multi-channel LiDAR
in urban area. IEEE Conference on Intelligent Transporta-
tion Systems, Proceedings, ITSC, 2018-March, 2018. 1, 2

[10] Diederik P. Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 7

[11] Rainer K¨ummerle, Dirk H¨ahnel, Dmitri Dolgov, Sebastian
Thrun, and Wolfram Burgard. Autonomous driving in a
multi-level parking structure. ICRA, 2009. 2

[12] Jesse Levinson, Michael Montemerlo, and Sebastian Thrun.
Map-based precision vehicle localization in urban environ-
ments. In RSS, 2007. 1, 2

[13] Jesse Levinson and Sebastian Thrun. Robust vehicle local-
ization in urban environments using probabilistic maps. In
ICRA, 2010. 1, 2, 3

[14] Wenjie Luo, Alexander G. Schwing, and Raquel Urtasun. Ef-
ﬁcient deep learning for stereo matching. In CVPR, 2016. 2
[15] Wei-Chiu Ma, Shenlong Wang, Marcus A. Brubaker, Sanja
Fidler, and Raquel Urtasun. Find your way by observing the
sun and other semantic cues. In ICRA, 2017. 2

[16] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen,
Radu Timofte, and Luc Van Gool. Conditional probabil-
ity models for deep image compression.
arXiv preprint
arXiv:1801.04260, 2018. 2

[17] Teddy Ort, Liam Paull, and Daniela Rus. Autonomous Vehi-
cle Navigation in Rural Environments without Detailed Prior
Maps. ICRA, pages 1–8, 2018. 2

[18] Noha Radwan, Abhinav Valada, and Wolfram Burgard.
Vlocnet++: Deep multitask learning for semantic visual lo-
calization and odometry.
IEEE Robotics and Automation
Letters, 3(4):4407–4414, 2018. 2

10324

