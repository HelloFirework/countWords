Bringing a Blurry Frame Alive at High Frame-Rate with an Event Camera

Liyuan Pan1

,

2, Cedric Scheerlinck1

,

2, Xin Yu1

,

2, Richard Hartley1

,

2, Miaomiao Liu1

,

2, and Yuchao Dai3

1 Australian National University, Canberra, Australia

2 Australian Centre for Robotic Vision

3 School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China

{liyuan.pan, cedric.scheerlinck}@anu.edu.au

Abstract

Event-based cameras can measure intensity changes
(called ‘events’) with microsecond accuracy under high-
speed motion and challenging lighting conditions. With the
active pixel sensor (APS), the event camera allows simul-
taneous output of the intensity frames. However, the output
images are captured at a relatively low frame-rate and often
suffer from motion blur. A blurry image can be regarded as
the integral of a sequence of latent images, while the events
indicate the changes between the latent images. Therefore,
we are able to model the blur-generation process by as-
sociating event data to a latent image.
In this paper, we
propose a simple and effective approach, the Event-based
Double Integral (EDI) model, to reconstruct a high frame-
rate, sharp video from a single blurry frame and its event
data. The video generation is based on solving a simple
non-convex optimization problem in a single scalar vari-
able. Experimental results on both synthetic and real im-
ages demonstrate the superiority of our EDI model and op-
timization method in comparison to the state-of-the-art.

1. Introduction

Event cameras (such as the Dynamic Vision Sensor
(DVS) [17] and the Dynamic and Active-pixel Vision Sen-
sor (DAVIS) [3]) are sensors that asynchronously measure
the intensity changes at each pixel independently with mi-
crosecond temporal resolution1. The event stream encodes
the motion information by measuring the precise pixel-by-
pixel intensity changes. Event cameras are more robust
to low lighting and highly dynamic scenes than traditional
cameras since they are not affected by under/over exposure
or motion blur associated with a synchronous shutter.

Due to the inherent differences between event cameras
and standard cameras, existing computer vision algorithms
designed for standard cameras cannot be applied to event

1If nothing moves in the scene, no events are triggered.

cameras directly. Although the DAVIS [3] can provide the
simultaneous output of the intensity frames and the event
stream, there still exist major limitations with current event
cameras:

• Low frame-rate intensity images: In contrast to the
high temporal resolution of event data (≥ 3µs frame
rate), the current event cameras only output low frame-
rate intensity images (≥ 5ms time resolution).

• Inherent blurry effects: When recording highly dy-
namic scenes, motion blur is a common issue due to
the relative motion between the camera and the scene.
The output of the intensity image from the APS tends
to be blurry.

To address these above challenges, various methods have
been proposed by reconstructing high frame-rate videos.
The existing methods can be in general categorized as 1)
Event data only solutions [1, 27, 2], where the results
tend to lack the texture and consistency of natural videos,
as they fail to use the complementary information con-
tained in the low frame-rate intensity image; 2) Low frame-
rate intensity-image-only solutions [11], where an end-to-
end learning framework has been proposed to learn regres-
sion between a single blurry image and a video sequence,
whereas the rich event data are not used; and 3) Jointly ex-
ploiting event data and intensity images [29, 32, 4], building
upon the interaction between both sources of information.
However, these methods fail to address the blur issue asso-
ciated with the captured image frame. Therefore, the recon-
structed high frame-rate videos can be degraded by blur.

Although blurry frames cause undesired image degrada-
tion, they also encode the relative motion between the cam-
era and the observed scene. Taking full advantage of the
encoded motion information would beneﬁt the reconstruc-
tion of high frame-rate videos.

To this end, we propose an Event-based Double Inte-
gral (EDI) model to resolve the above problems by recon-
structing a high frame-rate video from a single image (even

6820

180

160

140

120

100

80

60

40

20

y

(a) The Blurry Image

(b) The Events

(c) Tao et al. [35]

(d) Pan et al. [22]

time (

0
s)
0.78
0.775

0

50

100

150

200

x

(e) Jin et al. [11]

(f)

Scheerlinck et al. [29]

(events only)

(g) Scheerlinck et al. [29]

(h) Ours

Figure 1. Deblurring and reconstruction results of our method compared with the state-of-the-art methods on our real blurry event dataset.
(a) The input blurry image. (b) The corresponding event data. (c) Deblurring result of Tao et al. [35]. (d) Deblurring result of Pan et
al. [22]. (e) Deblurring result of Jin et al. [11]. Jin uses video as training data to train a supervised model to perform deblur, where the
video can also be considered as similar information as the event data. (f)-(g) Reconstruction results of Scheerlinck et al. [29], (f) from only
events, (g) from combining events and frames. (h) Our reconstruction result. (Best viewed on screen).

blur) and its event sequence, where the blur effects have
been reduced in each reconstructed frame. Our EDI model
naturally relates the desired high frame-rate sharp video,
the captured intensity frame and event data. Based on the
EDI model, high frame-rate video generation is as simple
as solving a non-convex optimization problem in a single
scalar variable.

Our main contributions are summarized as follows.

1) We propose a simple and effective model, named the
Event-based Double Integral (EDI) model, to restore a
high frame-rate sharp video from a single image (even
blur) and its corresponding event data.

2) Using our formulation of EDI, we propose a stable and
general method to generate a sharp video under various
types of blur by solving a single variable non-convex op-
timization problem, especially in low lighting and com-
plex dynamic conditions.

3) The frame rate of our reconstructed video can theoreti-
cally be as high as the event rate (200 times greater than
the original frame rate in our experiments).

2. Related Work

Event cameras such as the DAVIS and DVS [3, 17]
report log intensity changes, inspired by human vision.
Although several works try to explore the advantages of
the high temporal resolution provided by event cameras
[41, 13, 26, 43, 42, 8, 15, 30], how to make the best use
of the event camera has not yet been fully investigated.

Event-based image reconstruction. Kim et al. [12] recon-
struct high-quality images from an event camera under a
strong assumption that the only movement is pure camera
rotation, and later extend their work to handle 6-degree-of-
freedom motion and depth estimation [13]. Bardow et al.
[1] aim to simultaneously recover optical ﬂow and inten-
sity images. Reinbacher et al. [27] restore intensity images
via manifold regularization. Barua et al. [2] generate image
gradients by dictionary learning and obtain a logarithmic
intensity image via Poisson reconstruction. However, the
intensity images reconstructed by the previous approaches
suffer from obvious artifacts as well as lack of texture due
to the spatial sparsity of event data.

To achieve more image detail in the reconstructed im-
ages, several methods trying to combine events with inten-
sity images have been proposed. The DAVIS [3] uses a
shared photo-sensor array to simultaneously output events
(DVS) and intensity images (APS). Scheerlinck et al. [29]
propose an asynchronous event-driven complementary ﬁl-
ter to combine APS intensity images with events, and obtain
continuous-time image intensities. Brandli et al. [4] directly
integrate events from a starting APS image frame, and each
new frame resets the integration. Shedligeri et al. [32] ﬁrst
exploit two intensity images to estimate depth. Then, they
use the event data only to reconstruct a pseudo-intensity se-
quence (using [27]) between the two intensity images and
use the pseudo-intensity sequence to estimate ego-motion
using visual odometry. Using the estimated 6-DOF pose
and depth, they directly warp the intensity image to the in-
termediate location. Liu et al. [18] assume a scene should

6821

have static background. Thus, their method needs an extra
sharp static foreground image as input and the event data
are used to align the foreground with the background.

Image deblurring. Traditional deblurring methods usually
make assumptions on the scenes (such as a static scene) or
exploit multiple images (such as stereo, or video) to solve
the deblurring problem. Signiﬁcant progress has been made
in the ﬁeld of single image deblurring. Methods using gra-
dient based regularizers, such as Gaussian scale mixture [7],
l1\l2 norm [14], edge-based patch priors [34, 39] and l0-
norm regularizer [37], have been proposed. Non-gradient-
based priors such as the color line based prior [16], and the
extreme channel (dark/bright channel) prior [22, 38] have
also been explored. Another family of image deblurring
methods tends to use multiple images [5, 10, 31, 23, 24].

Driven by the success of deep neural networks, Sun et
al. [33] propose a convolutional neural network (CNN) to
estimate locally linear blur kernels. Gong et al. [9] learn
optical ﬂow from a single blurry image through a fully-
convolutional deep neural network. The blur kernel is then
obtained from the estimated optical ﬂow to restore the sharp
image. Nah et al. [21] propose a multi-scale CNN that re-
stores latent images in an end-to-end learning manner with-
out assuming any restricted blur kernel model. Tao et al.
[35] propose a light and compact network, SRN-DeblurNet,
to deblur the image. However, deep deblurring methods
generally need a large dataset to train the model and usually
require sharp images provided as supervision. In practice,
blurry images do not always have corresponding ground-
truth sharp images.

Blurry image to sharp video. Recently, two deep learn-
ing based methods [11, 25] propose to restore a video from
a single blurry image with a ﬁxed sequence length. How-
ever, their reconstructed videos do not obey the 3D geome-
try of the scene and camera motion. Although deep-learning
based methods achieve impressive performance in various
scenarios, their success heavily depend on the consistency
between the training datasets and the testing datasets, thus
hinder the generalization ability for real-world applications.

at a ﬁxed frame-rate, event cameras trigger events when-
ever the change in intensity at a given pixel exceeds a preset
threshold. Event cameras do not suffer from the limited dy-
namic ranges typical of sensors with synchronous exposure
time, and are able to capture high-speed motion with mi-
crosecond accuracy.

Inherent in the theory of event cameras is the concept
of the latent image Lxy(t), denoting the instantaneous in-
tensity at pixel (x, y) at time t, related to the rate of pho-
ton arrival at that pixel. The latent image Lxy(t) is not di-
rectly output by the camera.
Instead, the camera outputs
a sequence of events, denoted by (x, y, t, σ), which record
changes in the intensity of the latent image. Here, (x, y)
are image coordinates, t is the time the event takes place,
and polarity σ = ±1 denotes the direction (increase or de-
crease) of the intensity change at that pixel and time. Polar-
ity is given by,

(1)

where T (·, ·) is a truncation function,

Lxy(tref)(cid:17), c(cid:19) ,

σ = T (cid:18)log(cid:16) Lxy(t)
T (d, c) =

+1, d ≥ c,
0,
−1, d ≤ −c.

d ∈ (−c, c),

Here, c is a threshold parameter determining whether an
event should be recorded or not, tref denotes the timestamp
of the previous event. When an event is triggered, Lxy(tref)
at that pixel is updated to a new intensity level.

3.2. Intensity Image Formation

In addition to the event sequence, event cameras can
provide a full-frame grey-scale intensity image, at a much
slower rate than the event sequence. The grey-scale im-
ages may suffer from motion blur due to their long exposure
time. A general model of image formation is given by,

B =

1

T Z f +T /2

f −T /2

L(t) dt,

(2)

where B is a blurry image, equal to the average value of
latent images during the exposure time [f − T /2, f + T /2].

3. Formulation

3.3. Event based Double Integral Model

In this section, we develop an EDI model of the relation-
ships between the events, the latent image and the blurry
image. Our goal is to reconstruct a high frame-rate, sharp
video from a single image and its corresponding events.
This model can tackle various blur types and work stably
in highly dynamic contexts and low lighting conditions.

3.1. Event Camera Model

Event cameras are bio-inspired sensors that asyn-
chronously report logarithmic intensity changes [3, 17].
Unlike conventional cameras that produce the full image

We aim to recover a sequence of latent intensity images
by exploiting both the blur model and the event model. We
deﬁne exy(t) as a function of continuous time t such that

exy(t) = σ δt0 (t),

whenever there is an event (x, y, t0, σ). Here, δt0 (t) is an
impulse function, with unit integral, at time t0, and the se-
quence of events is turned into a continuous time signal,
consisting of a sequence of impulses. There is such a func-
tion exy(t) for every point (x, y) in the image. Since each
pixel can be treated separately, we omit the subscripts x, y.

6822

180

145

90y

45

0

7.515

7.51

7.505

time (

s)

7.5

0

50

100

x

200

240

150

would be to ﬁrst deblur the image with an existing deblur-
ring method and then to reconstruct the video using Eq. (4)
(see Fig.6 for details). However, in this way, the event data
between intensity images is not fully exploited, thus result-
ing in inferior performance. Instead, we propose to recon-
struct the video by exploiting the inherent connection be-
tween event and blur, and present the following model.

(a) The Blurry Image

(b) The Events

As for the blurred image,

L(t)dt

B =

=

1

f −T /2

T Z f +T /2
T Z f +T /2

L(f )

f −T /2

exp(cid:16)cZ t

f

e(s)ds(cid:17) dt .

(5)

(c) E(t) =R e(t) dt

(d) 1

T R exp(c E(t))dt

In this manner, we construct the relation between the
captured blurry image B and the latent image L(f ) through
the double integral of the event. We name Eq. 5 the Event-
based Double Integral (EDI) model. Taking the logarithm
on both sides of Eq. 5 and rearranging, yields

(e) Sample Frames of Our Reconstructed Video

Figure 2. The event data and our reconstructed result, where (a)
and (b) are the input of our method. (a) The intensity image from
the event camera. (b) Events from the event camera plotted in 3D
space-time (x, y, t) (blue: positive event; red: negative event). (c)
The ﬁrst integral of several events during a small time interval.
(d) The second integral of events during the exposure time.
(e)
Samples from our reconstructed video from L(0) to L(200).

During an exposure period [f −T /2, f +T /2], we deﬁne
E(t) as the sum of events between time f and t at a given
pixel,

E(t) =Z t

f

e(s)ds,

which represents the proportional change in intensity be-
tween time f and t. Except under extreme conditions, such
as glare and no-light conditions, the latent image sequence
L(t) is expressed as,

L(t) = L(f ) exp(c E(t)) = L(f ) exp(c)E(t) .

(3)

We put a tilde on top of things to denote logarithm, e.g.,

eL(t) = log(L(t)).

eL(t) = eL(f ) + c E(t).

Given a sharp frame, we can reconstruct a high frame-
rate video from the sharp starting point L(f ) by using
Eq. (4). When the input image is blurry, a trivial solution

eL(f ) = eB − log  1

T Z f +T /2

f −T /2

exp(c E(t))dt! ,

(6)

which shows a linear relation between the blurry image, the
latent image and the integral of the events in the log space.

3.4. High Frame Rate Video Generation

The right-hand side of Eq. (6) is known, apart from per-
haps the value of the contrast threshold c, the ﬁrst term from
the grey-scale image, the second term from the event se-

quence, it is possible to compute eL(f ), and hence L(f ) by

exponentiation. Subsequently, from Eq. (4) the latent image
L(t) at any time may be computed.

To avoid accumulated errors of constructing a video from
many frames of a blurred video, it is more suitable to con-
struct each frame L(t) using the closest blurred frame.

Theoretically, we could generate a video with frame-rate
as high as the DVS’s eps (events per second). However, as
each event carries little information and is subject to noise,
several events must be processed together to yield a reason-
able image. We generate a reconstructed frame every 50-
100 events, so for our experiments, the frame-rate of the re-
constructed video is usually 200 times greater than the input
low frame-rate video. Furthermore, as indicated by Eq. (6),
the challenging blind motion deblurring problem has been
reduced to a single variable optimization problem of how to
ﬁnd the best value of the contrast threshold c. In the follow-
ing section, we use L(c, t) to present the latent sharp image
L(t) with different c.

(4)

4. Optimization

The unknown contrast threshold c represents the mini-
mum change in log intensity required to trigger an event.
By choosing an appropriate c in Eq. (5), we can generate a

6823

(a) The blurry image

(b) Tao et al. [35]

(c) By human observation

(d) By energy minimization

Figure 3. An example of our reconstruction result using different
methods to estimate c, from the real dataset [20]. (a) The blurry
image. (b) Deblurring result of [35] (c) Our result where c is cho-
(d) Our result where c is computed
sen by manual inspection.
automatically by our proposed energy minimization (9).

×10 5

1.6

1.4

1.2

1

0.8

y
g
r
e
n
E

0.6

0

0.2

0.4

0.6

c

0.8

35

30

25

20

15

10

1

)

B
d
(
 

R
N
S
P

Figure 4.
value of c. The image is clearer with higher PSNR value.

The ﬁgure plot deblurring performance against the

sequence of sharper images. To this end, we ﬁrst need to
evaluate the sharpness of the reconstructed images. Here,
we propose two different methods to estimate the unknown
variable c: manually chosen and automatically optimized.

4.1. Manually Chosen c

According to our EDI model in Eq. (5), given a value
for c, we can obtain a sharp image. Therefore, we develop
a method for deblurring by manually inspecting the visual
effect of the deblurred image. In this way, we incorporate
human perception into the reconstruction loop and the de-
blurred images should satisfy human observation. In Fig. 3,
we give an example for manually chosen and automatically
optimized results on dataset from [20].

4.2. Automatically Chosen c

To automatically ﬁnd the best c, we need to build an eval-
uation metric (energy function) that can evaluate the quality
of the deblurred image L(c, t). Speciﬁcally, we propose to

exploit different prior knowledge for sharp images and the
event data.

4.2.1 Edge Constraint for Event Data

As mentioned before, when a proper c is given, our re-
constructed image L(c, t) will contain much sharper edges
compared with the original input intensity image. Further-
more, event cameras inherently yield responses at moving
intensity boundaries, so edges in the latent image may be
located where (and when) events occur. This allows us to
ﬁnd latent image edges. An edge at time t corresponds to
an event (at the pixel in question) during some time inter-
val around t so we convolve the event sequence with an
exponentially decaying window, to obtain a denoised edge
boundary,

M(t) =Z T

0

exp(−(α|t − s|)) e(t) ds,

where α is a weight parameter for time attenuation, and is
set to 1.0. Then, we use the Sobel ﬁlter S to get a sharper
binary edge map, which is also applied to L(c, t). (See Fig.
5 and 6 for details).

Here, we use cross-correlation between S(L(c, t)) and

S(M(t)) to evaluate the sharpness of L(c, t).

φedge(c) =Xx,y

S(L(c, t))(x, y) · S(M(t))(x, y) .

(7)

4.2.2 Regularizing the Intensity Image

In our model, total variation is used to suppress noise in
the latent image while preserving edges, and penalize the
spatial ﬂuctuations[28].

φTV(c) = |∇L(c, t)|1,

(8)

where ∇ represents the gradient operators.

4.2.3 Energy Minimization

The optimal c can be estimate by solving Eq. (9),

min

c

φTV(c) + λφedge(c),

(9)

where λ is a trade-off parameter. The response of cross-
correlation reﬂect the matching rate of L(c, t) and M(t)
which makes λ < 0. This single-variable minimiza-
tion problem can be solved by the nonlinear least-squares
method [19], Scatter-search[36] or Fibonacci search [6].

In Fig. 4, we illustrate the clearness of the reconstructed
image against the value of c. Meanwhile, we also pro-
vide the PSNR of the corresponding reconstructed image.
As demonstrated in the ﬁgure, our proposed reconstruction
metric could locate/identify the best deblurred image with
peak PSNR properly.

6824

Figure 5. At left, the edge image M (f ) and below, its Sobel edge map. To the right are 3 reconstructed latent images using different
values of c, low 0.03, middle 0.11 and high 0.55. Above, the reconstructed images, below, their Sobel edge maps. The optimal value of the
threshold c is found by computing the cross-correlation of such images with the edge map at the left. (Best viewed on screen).

(a) The Blur Image

(b) Jin et al. [11]

(c) Baseline 1

(d) Baseline 2

(e) S(L(c, t)) · S(M(t))

(f) Samples of Our Reconstructed Video

Figure 6. Deblurring and reconstruction results on our real blurry event dataset. (a) Input blurry images. (b) Deblurring result of [11].
(c) Baseline 1 for our method. We ﬁrst use the state-of-the-art video-based deblurring method [11] to recover a sharp image. Then use the
sharp image as input to a state-of-the-art reconstruction method [29] to get the intensity image. (d) Baseline 2 for our method. We ﬁrst
use method [29] to reconstruct an intensity image. Then use a deblurring method [11] to recover a sharp image. (e) The cross-correlation
between S(L(c, t)) and S(M(t)). (f) Samples from our reconstructed video from L(0) to L(150). (Best viewed on screen).

5. Experiment

5.1. Experimental Setup

Synthetic dataset. In order to provide a quantitative com-
parison, we build a synthetic dataset based on the GoPro
blurry dataset [21]. It supplies ground truth videos which
are used to generate the blurry images. Similarly, we em-
ploy the ground-truth images to generate event data based
on the methodology of event camera model.
Real dataset. We evaluate our method on a public
Event-Camera dataset [20], which provides a collection of
sequences captured by the event camera for high-speed
robotics. Furthermore, we present our real blurry event

dataset 2, where each real sequence is captured with the
DAVIS[3] under different conditions, such as indoor, out-
door scenery, low lighting conditions, and different motion
patterns (e.g., camera shake, objects motion) that naturally
introduce motion blur into the APS intensity images.

Implementation details. For all our real experiments, we
use the DAVIS that shares photosensor array to simultane-
ously output events (DVS) and intensity images (APS). The
framework is implemented by using MATLAB with C++
wrappers. It takes around 1.5 second to process one image
on a single i7 core running at 3.6 GHz.

2To be released with codes

6825

Table 1. Quantitative comparisons on the Synthetic dataset [21]. This dataset provides videos can be used to generate not only blurry
images but also event data. All methods are tested under the same blurry condition, where methods [21, 11, 35, 40] use GoPro dataset
[21] to train their models. Jin [11] achieves their best performance when the image is down-sampled to 45% mentioned in their paper.

Average result of the deblurred images on dataset[21]

Pan et al. [22]

Sun et al. [33]

Gong et al. [9]

Jin et al. [11]

Tao et al. [35]

Zhang et al. [40]

Nah et al. [21]

PSNR(dB)

SSIM

23.50
0.8336

25.30
0.8511

26.05
0.8632

26.98
0.8922

30.26
0.9342

29.18
0.9306

29.08
0.9135

Baseline 1 [35] + [29]

Baseline 2 [29] + [35]

Scheerlinck et al. [29]

Jin et al. [11]

PSNR(dB)

SSIM

25.52
0.7685

26.34
0.8090

25.84
0.7904

25.62
0.8556

Average result of the reconstructed videos on dataset[21]

Ours
29.06
0.9430

Ours
28.49
0.9199

(a) The Blurry Image

(b) Jin et al. [11]

(c) Ours

(d) The Reconstructed Video of [11]

(e) The Reconstructed Video of Our Method

(f) Reinbacher et al. [27]

(g) Scheerlinck et al. [29]

Figure 7. An example of the reconstructed result on our synthetic event dataset based on the GoPro dataset [21]. [21] provides videos
to generate the blurry images and event data. (a) The blurry image. The red close-up frame is for (b)-(e), the yellow close-up frame
is for (f)-(g). (b) The deblurring result of Jin et al. [11]. (c) Our deblurring result. (d) The crop of their reconstructed images and the
frame number is ﬁxed at 7. Jin et al. [11] uses the GoPro dataset added with 20 scenes as training data and their model is supervised by
7 consecutive sharp frames. (e) The crop of our reconstructed images. (f) The crop of Reinbacher [27] reconstructed images from only
events. (g) The crop of Scheerlinck [29] reconstructed image, they use both events and the intensity image. For (e)-(g), the shown frames
are the chosen examples, where the length of the reconstructed video is based on the number of events.

5.2. Experimental Results

We compare our proposed approach with state-of-the-art
blind deblurring methods, including conventional deblur-
ring methods [22, 38], deep based dynamic scene deblur-
ring methods [21, 11, 35, 40, 33], and event-based image
reconstruction methods [27, 29]. Moreover, Jin et al. [11]
can restore a video from a single blurry image based on a
deep network, where the middle frame in the restored odd-
numbered sequence is the best.

In order to prove the effectiveness of our EDI model,
we show some baseline comparisons in Fig. 6 and Table
1. For baseline 1, we ﬁrst apply a state-of-the-art deblur-
ring method [35] to recover a sharp image, and then the
recovered image as an input is then fed to a reconstruction
method [29]. For baseline 2, we ﬁrst use the video recon-
struction method to construct a sequence of intensity im-
ages, and then apply the deblurring method to each frame.
As seen in Table 1, our approach obtains higher PSNR and

6826

180

145

y 90

45

0

0

180

145

y

90

45

)
s

(
 

e
m

i
t

0

0.5
0.49

0

50

100

150

200

x

time (

s)

240

0.29
0.3

50

100

x

150

200

240

(a) The Blurry Image

(b) The Event

(c) Pan et al. [22]

(d) Tao et al. [35]

(e) Nah et al. [21]

(f) Jin et al. [11]

(g) Reinbacher et al. [27]

(h)

Scheerlinck et al. [29]

(events only)

(i) Scheerlinck et al. [29]

(j) Ours

Figure 8. Examples of reconstruction result on our real blurry event dataset in low lighting and complex dynamic conditions (a) Input
blurry images. (b) The event information. (c) Deblurring results of [22]. (d) Deblurring results of [35]. (e) Deblurring results of [21]. (f)
Deblurring results of [11] and they use video as training data. (g) Reconstruction result of [27] from only events. (h)-(i) Reconstruction
results of [29], (h) from only events, (i) from combining events and frames. (j) Our reconstruction result. Results in (c)-(f) show that
real high dynamic settings and low light condition is still challenging in the deblurring area. Results in (g)-(h) show that while intensity
information of a scene is still retained with an event camera recording, color, and delicate texture information cannot be recovered.

SSIM in comparison to both baseline 1 and baseline 2. This
also implies that our approach better exploits the event data
to not only recover sharp images but also reconstruct high
frame-rate videos.

In Table 1 and Fig. 7, we show the quantitative and qual-
itative comparisons with the state-of-the-art image deblur-
ring approaches [33, 22, 9, 11, 35, 40, 21], and the video
reconstruction method [29] on our synthetic dataset, respec-
tively. As indicated in Table 1, our approach achieves the
best performance on SSIM and competitive result on PSNR
compared to the state-of-the-art methods, and attains signif-
icant performance improvements on high-frame video re-
construction.

We report our reconstruction results on the real dataset,
including text images and low-lighting images, in Fig. 1,
Fig. 2, Fig. 3 and Fig. 8. In comparison to existing event-
based image reconstructed methods [27, 29], our recon-
structed images are not only more realistic but also contain
richer details. More deblurring results and high-temporal
resolution videos are shown in the supplementary material.

6. Conclusion

In this paper, we propose an Event-based Double Inte-
gral (EDI) model to naturally connect intensity images and
event data captured by the event camera, which also takes
the blur generation process into account. In this way, our
model can be used to not only recover latent sharp images
but also reconstruct intermediate frames at high frame-rate.
We also propose a simple yet effective method to solve our
EDI model. Due to the simplicity of our optimization pro-
cess, our method is efﬁcient as well. Extensive experiments
show that our method can generate high-quality high frame-
rate videos efﬁciently under different conditions, such as
low lighting and complex dynamic scenes.

Acknowledgements

This research was supported in part by Australia Cen-
tre for Robotic Vision (CE140100016),
the Australian
Research Council grants (DE140100180, DE180100628)
and the Natural Science Foundation of China grants
(61871325,
61603303).

61420106007,

61671387,

6827

References

[1] Patrick Bardow, Andrew J. Davison, and Stefan Leutenegger.
Simultaneous optical ﬂow and intensity estimation from an
event camera.
In IEEE Conf. Comput. Vis. Pattern Recog.
(CVPR), pages 884–892, 2016. 1, 2

[2] Souptik Barua, Yoshitaka Miyatani, and Ashok Veeraragha-
van. Direct face detection and video reconstruction from
event cameras.
In IEEE Winter Conf. Appl. Comput. Vis.
(WACV), pages 1–9, 2016. 1, 2

[3] Christian Brandli, Raphael Berner, Minhao Yang, Shih-Chii
Liu, and Tobi Delbruck. A 240× 180 130 db 3 µs latency
global shutter spatiotemporal vision sensor. IEEE Journal of
Solid-State Circuits, 49(10):2333–2341, 2014. 1, 2, 3, 6

[4] Christian Brandli, Lorenz Muller, and Tobi Delbruck. Real-
time, high-speed video decompression using a frame- and
event-based DAVIS sensor. In IEEE Int. Symp. Circuits Syst.
(ISCAS), pages 686–689, June 2014. 1, 2

[5] Sunghyun Cho, Jue Wang, and Seungyong Lee. Video de-
blurring for hand-held cameras using patch-based synthesis.
ACM Transactions on Graphics (TOG), 31(4):64, 2012. 3

[6] Richard A Dunlap. The golden ratio and Fibonacci numbers.

World Scientiﬁc, 1997. 5

[7] Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T.
Roweis, and William T. Freeman. Removing camera shake
from a single photograph. ACM Trans. Graph., 25:787–794,
2006. 3

[8] Daniel Gehrig, Henri Rebecq, Guillermo Gallego, and Da-
vide Scaramuzza. Asynchronous, photometric feature track-
ing using events and frames.
In Eur. Conf. Comput. Vis.
(ECCV), 2018. 2

[9] Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian
Reid, Chunhua Shen, Anton van den Hengel, and Qinfeng
Shi. From motion blur to motion ﬂow: A deep learning so-
lution for removing heterogeneous motion blur.
In IEEE
Conf. Comput. Vis. Pattern Recog. (CVPR), pages 2319–
2328, 2017. 3, 7, 8

[10] Tae Hyun Kim and Kyoung Mu Lee. Generalized video de-
In IEEE Conf. Comput. Vis.

blurring for dynamic scenes.
Pattern Recog. (CVPR), pages 5426–5434, 2015. 3

[11] Meiguang Jin, Givi Meishvili, and Paolo Favaro. Learning to
extract a video sequence from a single motion-blurred image.
In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), June
2018. 1, 2, 3, 6, 7, 8

[12] Hanme Kim, Ankur Handa, Ryad Benosman, Sio-Hoi Ieng,
and Andrew J. Davison. Simultaneous mosaicing and track-
ing with an event camera.
In British Machine Vis. Conf.
(BMVC), 2014. 2

[13] Hanme Kim, Stefan Leutenegger, and Andrew J. Davison.
Real-time 3D reconstruction and 6-DoF tracking with an
event camera.
In Eur. Conf. Comput. Vis. (ECCV), pages
349–364, 2016. 2

[14] Dilip Krishnan, Terence Tay, and Rob Fergus. Blind decon-
volution using a normalized sparsity measure. In IEEE Conf.
Comput. Vis. Pattern Recog. (CVPR), pages 233–240, 2011.
3

[15] Beat Kueng, Elias Mueggler, Guillermo Gallego, and Da-
vide Scaramuzza. Low-latency visual odometry using event-
based feature tracks. In IEEE/RSJ Int. Conf. Intell. Robot.
Syst. (IROS), pages 16–23, Daejeon, Korea, Oct. 2016. 2

[16] Wei-Sheng Lai, Jian-Jiun Ding, Yen-Yu Lin, and Yung-Yu
Chuang. Blur kernel estimation using normalized color-line
prior. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR),
pages 64–72, 2015. 3

[17] Patrick Lichtsteiner, Christoph Posch, and Tobi Delbruck.
A 128×128 120 db 15 µs latency asynchronous temporal
contrast vision sensor. IEEE journal of solid-state circuits,
43(2):566–576, 2008. 1, 2, 3

[18] Han-Chao Liu, Fang-Lue Zhang, David Marshall, Luping
Shi, and Shi-Min Hu. High-speed video generation with an
event camera. The Visual Computer, 33(6-8):749–759, 2017.
2

[19] Jorge J Mor´e. The levenberg-marquardt algorithm: imple-
In Numerical analysis, pages 105–

mentation and theory.
116. Springer, 1978. 5

[20] Elias Mueggler, Henri Rebecq, Guillermo Gallego, Tobi Del-
bruck, and Davide Scaramuzza. The event-camera dataset
and simulator: Event-based data for pose estimation, visual
odometry, and slam. The International Journal of Robotics
Research, 36(2):142–149, 2017. 5, 6

[21] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring.
In IEEE Conf. Comput. Vis. Pattern Recog.
(CVPR), July 2017. 3, 6, 7, 8

[22] Jinshan Pan, Deqing Sun, Hanspeter Pﬁster, and Ming-
Hsuan Yang. Deblurring images via dark channel prior. IEEE
Trans. Pattern Anal. Mach. Intell., 2017. 2, 3, 7, 8

[23] Liyuan Pan, Yuchao Dai, Miaomiao Liu, and Fatih Porikli.
Simultaneous stereo video deblurring and scene ﬂow estima-
tion.
In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR),
July 2017. 3

[24] Liyuan Pan, Yuchao Dai, Miaomiao Liu, and Fatih Porikli.
Depth map completion by jointly exploiting blurry color im-
ages and sparse depth maps.
In IEEE Winter Conf. Appl.
Comput. Vis. (WACV), pages 1377–1386. IEEE, 2018. 3

[25] Kuldeep Purohit, Anshul Shah, and AN Rajagopalan.
arXiv preprint

Bringing alive blurred moments!
arXiv:1804.02913, 2018. 3

[26] Henri Rebecq, Timo Horstsch¨afer, Guillermo Gallego, and
Davide Scaramuzza. EVO: A geometric approach to event-
based 6-DOF parallel tracking and mapping in real-time.
IEEE Robot. Autom. Lett., 2, 2017. 2

[27] Christian Reinbacher, Gottfried Graber, and Thomas Pock.
Real-time intensity-image reconstruction for event cameras
using manifold regularisation. In British Machine Vis. Conf.
(BMVC), 2016. 1, 2, 7, 8

[28] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear
total variation based noise removal algorithms. Physica D:
nonlinear phenomena, 60(1-4):259–268, 1992. 5

[29] Cedric Scheerlinck, Nick Barnes, and Robert Mahony.
Continuous-time intensity estimation using event cameras.
In Asian Conf. Comput. Vis. (ACCV), 2018. 1, 2, 6, 7, 8

6828

[30] Cedric Scheerlinck, Nick Barnes, and Robert Mahony. Asyn-
image convolutions for event cameras.

chronous spatial
IEEE Robot. Autom. Lett., 4(2):816–822, April 2019. 2

[31] Anita Sellent, Carsten Rother, and Stefan Roth. Stereo video
deblurring. In Eur. Conf. Comput. Vis. (ECCV), pages 558–
575. Springer, 2016. 3

[32] Prasan A Shedligeri, Ketul Shah, Dhruv Kumar, and
Kaushik Mitra. Photorealistic image reconstruction from
hybrid intensity and event based sensor.
arXiv preprint
arXiv:1805.06140, 2018. 1, 2

[33] Jian Sun, Wenfei Cao, Zongben Xu, and Jean Ponce. Learn-
ing a convolutional neural network for non-uniform motion
blur removal.
In IEEE Conf. Comput. Vis. Pattern Recog.
(CVPR), pages 769–777, 2015. 3, 7, 8

[34] Libin Sun, Sunghyun Cho, Jue Wang, and James Hays.
In

Edge-based blur kernel estimation using patch priors.
IEEE Int. Conf. Comput. Photography (ICCP), 2013. 3

[35] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji-
aya Jia. Scale-recurrent network for deep image deblurring.
In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), June
2018. 2, 3, 5, 7, 8

[36] Zsolt Ugray, Leon Lasdon, John Plummer, Fred Glover,
James Kelly, and Rafael Mart´ı. Scatter search and local nlp
solvers: A multistart framework for global optimization. IN-
FORMS Journal on Computing, 19(3):328–340, 2007. 5

[37] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse
representation for natural image deblurring. In IEEE Conf.
Comput. Vis. Pattern Recog. (CVPR), pages 1107–1114,
2013. 3

[38] Yanyang Yan, Wenqi Ren, Yuanfang Guo, Rui Wang, and Xi-
aochun Cao. Image deblurring via extreme channels prior. In
IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), July 2017.
3, 7

[39] Xin Yu, Feng Xu, Shunli Zhang, and Li Zhang. Efﬁcient
patch-wise non-uniform deblurring for a single image. IEEE
Transactions on Multimedia, 16(6):1510–1524, 2014. 3

[40] Jiawei Zhang, Jinshan Pan, Jimmy Ren, Yibing Song, Lin-
chao Bao, Rynson W.H. Lau, and Ming-Hsuan Yang. Dy-
namic scene deblurring using spatially variant recurrent neu-
ral networks.
In IEEE Conf. Comput. Vis. Pattern Recog.
(CVPR), June 2018. 7, 8

[41] Yi Zhou, Guillermo Gallego, Henri Rebecq, Laurent Kneip,
Hongdong Li, and Davide Scaramuzza. Semi-dense 3d re-
construction with a stereo event camera.
arXiv preprint
arXiv:1807.07429, 2018. 2

[42] Alex Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas
Daniilidis. Ev-ﬂownet: Self-supervised optical ﬂow estima-
tion for event-based cameras.
In Proceedings of Robotics:
Science and Systems, Pittsburgh, Pennsylvania, June 2018. 2
[43] Alex Zihao Zhu, Nikolay Atanasov, and Kostas Daniilidis.
Event-based visual inertial odometry. In IEEE Conf. Com-
put. Vis. Pattern Recog. (CVPR), pages 5816–5824, 2017. 2

6829

