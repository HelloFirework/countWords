Explainability Methods for Graph Convolutional Neural Networks

Phillip E. Pope*

Soheil Kolouri*

Mohammad Rostami

HRL Laboratories, LLC

HRL Laboratories, LLC

HRL Laboratories, LLC

pepope@hrl.com

skolouri@hrl.com

mrostami@hrl.com

Charles E. Martin

Heiko Hoffmann

HRL Laboratories, LLC

HRL Laboratories, LLC

cemartin@hrl.com

hhoffmann@hrl.com

Abstract

With the growing use of graph convolutional neural net-
works (GCNNs) comes the need for explainability. In this
paper, we introduce explainability methods for GCNNs. We
develop the graph analogues of three prominent explain-
ability methods for convolutional neural networks: con-
trastive gradient-based (CG) saliency maps, Class Activa-
tion Mapping (CAM), and Excitation Backpropagation (EB)
and their variants, gradient-weighted CAM (Grad-CAM)
and contrastive EB (c-EB). We show a proof-of-concept of
these methods on classiﬁcation problems in two application
domains: visual scene graphs and molecular graphs. To
compare the methods, we identify three desirable properties
of explanations: (1) their importance to classiﬁcation, as
measured by the impact of occlusions, (2) their contrastiv-
ity with respect to different classes, and (3) their sparseness
on a graph. We call the corresponding quantitative met-
rics ﬁdelity, contrastivity, and sparsity and evaluate them
for each method. Lastly, we analyze the salient subgraphs
obtained from explanations and report frequently occurring
patterns.

1. Introduction

Recent success in computer vision is mainly due to emer-
gence of deep convolutional neural networks (CNNs) [21].
This has led to state-of-the-art performances on various
computer vision tasks including object recognition [11, 13],
object detection [27], and semantic segmentation [26]. The
end-to-end nature of learning in CNNs have turned them
into powerful data-driven tools for learning from a large cor-
pus of visual data. At the same time, this end-to-end learn-
ing strategy hinders the explainability and interpretability
of decisions made by CNNs. Recently, there has been an
increasing number of works studying the inner workings of
CNNs [38, 23, 22] and explaining the decisions made by

these networks [42, 31, 39, 40]. Zhang et al. [41] provide a
good survey on methods for explainability of CNNs.

Deep CNNs, however, are designed for grid structured
data, e.g.
images, in Euclidean spaces, as convolution is
an operation deﬁned on Euclidean space for inputs with or-
dered elements. Nonetheless, in many applications we need
to deal with data deﬁned on different structures, e.g. graphs
and manifolds, where CNNs cannot be directly used. Such
non-Euclidean spaces appear in various applications includ-
ing scene graph analysis [15], 3D-shape analysis [25], so-
cial networks [37], and chemistry [35]. Geometric deep
learning [6, 1] is a recent emerging ﬁeld to overcome limi-
tations of CNNs and broaden their application. In particu-
lar, CNNs could be generalized to be applicable on graph-
structured data by extending the convolution operation onto
graphs and in general onto non-Euclidean spaces. The ex-
tension of CNNs to non-Euclidean spaces leads to graph
convolution neural networks (GCNNs) [7, 9, 19].

In addition to superior performance of a model, we need
techniques to explain why a model predicts what it predicts.
This explanation can help to identify and localize parts of
the input data relevant to the model’s decisions in a particu-
lar task. Inspired by the explainability work on CNNs [42],
we introduce explainability methods for decisions by GC-
NNs. Explainability can be particularly helpful for graphs,
even more than for images, because non-expert humans
cannot intuitively determine the relevant context within a
graph, for example, when identifying groups of atoms (a
sub-graph structure on a molecular graph) that contribute to
a particular property of a molecule.

We adapt three common explainability methods, orig-
inally designed for CNNs, and extend them to GCNNs.
These three methods are gradient-based saliency maps [32],
Class Activation Mapping (CAM) [39], and Excitation
Backpropagation (EB) [39]. In addition, we adapt two vari-
ants: gradient-weighted CAM (Grad-CAM) [31] and con-
trastive EB. We evaluate the adapted methods on two differ-

10772

ent applications: visual scene graphs and molecular graphs.
For GCNNs, we use the proposed formulation by Kipf et al.
[18]. Our speciﬁc contributions in this work are the follow-
ing three:

• Adaptation of explainability methods for CNNs to GC-

NNs,

• Demonstration of the explainability techniques on two
graph classiﬁcation problems: visual scene graphs and
molecules, and

• Characterization of each method’s trade-offs using

metrics for ﬁdelity, contrastivity, and sparsity.

The remainder of this paper is structured as follows. In
Section 2, we discuss related work in interpretability and
In Section 3, we review the mathematical def-
GCNNs.
initions of GCNNs and explainability methods on CNNs,
and then deﬁne the analogous explainability methods on
In Section 4, we detail our experiments on vi-
GCNNs.
sual scene graphs and molecules and show example results.
Moreover, we quantitatively evaluate the performance of
these four methods with respect to three metrics, ﬁdelity,
contrastivity, and sparsity, each designed to capture certain
desirable properties of explanations. We use these metrics
to evaluate the merits of each method. Lastly, in the ex-
perimental section, we analyze the frequencies of salient
substructures identiﬁed by Grad-CAM and report the top
results for each dataset.

2. Related Work

Interpretability: A long standing limitation of general
deep neural networks has been the difﬁculty in interpreting
and explaining the classiﬁcation results. Recently, explain-
ability methods have been devised for deep networks and
speciﬁcally CNNs [32, 42, 31, 39, 40, 41]. These methods
enable one to probe a CNN and identify the important sub-
structures of the input data (as deemed by the network) for
decision regarding a task, which could be used as an ex-
planatory tool or as a tool to discover unknown underlying
substructures in the data. For example, in the area of med-
ical imaging [34], in addition to classifying images having
malignant lesions, they can be localized, as the CNN can
provide reasoning for classifying an input image.

The most straightforward approach for generating a sen-
sitivity map over the input data to discover the importance
of the underlying substructures is to calculate a gradient
map within a layer by considering the norm of the gradient
vector with respect to an input for each network weight [32].
However, gradient maps are known to be noisy and smooth-
ing these maps might be necessary [33]. More advanced
techniques include Class Activation Mapping (CAM) [42],
Gradient-weighted Class Activation Mapping (Grad-CAM)
[31], and Excitation Back-Propagation (EB) [39] improve

gradient maps by taking into account some notion of con-
text. These techniques have been shown to be effective on
CNNs and can identify highly abstract notions in images.
See Zhang et. al. [41] for a survey of explainability meth-
ods for CNNs.

Graph Convolutional Neural Networks: The mathe-
matical foundation of GCNNs is deeply rooted in the ﬁeld
of graph signal processing [3, 4] and spectral graph theory
in which signal operations like Fourier transform and con-
volutions are extended to signals living on graphs. GCNNs
emerged from the spectral graph theory, e.g., as introduced
by Bruna et al. [2] or Henaff et al. [12]. GCNNs based
on spectral graph theory enable deﬁnition of parameterized
ﬁlters akin to CNNs. They, however, are often computation-
ally expensive and therefore slow. To overcome the compu-
tational bottleneck of spectral GCNNs, various authors have
proposed to approximate smooth ﬁlters in the spectral do-
main [6, 19], for instance using Chebyshev polynomials or
a ﬁrst-order approximation of spectral graph convolutions.
In this work, we use the GCNN formulation deﬁned by Kipf
and Welling [19] due to its faster training times and higher
predictive accuracy.

GCNNs have recently found use in diverse applications.
Monti et al. [25] used GCNNs for super-pixel classiﬁca-
tion as well as for classifying research papers from their
citation network. Defferard et al. [6] used GCNNs on N-
grams for text categorization. In [36] GCNNs were used
for shape segmentation, and in [14], they were used for
skeleton-based action recognition. More recently, John-
son et al. [15] used GCNNs to analyze scene-graphs with
the application of image generation from scene graphs. In
chemistry, GCNNs were used to predict various chemical
properties of organic molecules. GCNNs provide state-of-
the-art performance on several chemical prediction tasks,
including toxicity prediction [16], solubility [7], and energy
prediction [30].
In this paper we focus on explainability
methods for GCNNs with applications on scene graph clas-
siﬁcation and molecule classiﬁcation.

3. Methods

We compare and contrast the application of popular ex-
plainability methods to Graph Convolutional Neural Net-
works (GCNNs). Furthermore, we explore the beneﬁts of a
number of enhancements to these approaches.

3.1. Explainability for CNNs

The three main groups of popular explainability methods
are contrastive gradients, Class Activation Mapping, and
Excitation Backpropagation.

Contrastive gradient-based saliency maps [32] is per-
haps the most straight-forward and well-established ap-
proach. In this approach, one simply differentiates the out-
put of the model with respect to the model input, thus cre-

10773

ating a heat-map, where the norm of the gradient over in-
put variables indicates their relative importance. The result-
ing gradient in the input space points in the direction cor-
responding to the maximum positive rate of change in the
model output. Therefore, the negative values in the gradient
are discarded to only retain the parts of input that positively
contribute to the solution:

Lc

Gradient = kReLU(cid:18) ∂yc
∂x(cid:19) k

,

(1)

where yc is the score for class c before the softmax layer,
and x is the input. While easy to compute and interpret,
saliency maps generally perform worse than newer tech-
niques (like CAM, Grad-CAM, and EB), and it was recently
argued that saliency maps tend to represent noise rather than
signal [17].

Class Activation Mapping provides an improvement over
saliency maps for convolutional neural networks, including
GCNNs, by identifying important, class-speciﬁc features at
the last convolutional layer as opposed to the input space. It
is well-known that such features tend to be more semanti-
cally meaningful (e.g., faces instead of edges). The down-
side of CAM is that it requires the layer immediately be-
fore the softmax classiﬁer (output layer) to be a convolu-
tional layer followed by a global average pooling (GAP)
layer. This precludes the use of more complex, heteroge-
neous networks, such as those that incorporate several fully
connected layers before the softmax layer.

To compute CAM, let Fk ∈ Ru×v be the kth feature
map of the convolutional layer preceding the softmax layer.
Denote the global average pool (GAP) of Fk by

ek =

1

Z Xi Xj

Fk,i,j

(2)

where Z = uv. Then, a given class score, yc, can be deﬁned
as

wc

kek,

(3)

yc =Xk

where the weights wc
k are learned based on the input-output
behavior of the network. The weight wc
k encodes the im-
portance of feature k for predicting class c. By upscaling
each feature map to the size of the input images (to undo
the effect of pooling layers) the class-speciﬁc heat-map in
the pixel-space becomes

Lc

CAM [i, j] = ReLU Xk

wc

kFk,i,j! .

(4)

The Grad-CAM method improves upon CAM by relax-
ing the architectural restriction that the penultimate layer
must be a convolutional. This is achieved by using feature

map weights αc
k that are based on back-propagated gradi-
ents. Speciﬁcally, Grad-CAM deﬁnes the weights accord-
ing to

αc

k =

1

Z Xi Xj

∂yc

∂Fk,i,j

.

(5)

Following the intuition behind Equation (4) for CAM, the
heat-map in the pixel-space according to Grad-CAM is
computed as

Lc

Grad−CAM [i, j] = ReLU Xk

αc

kFk,i,j! ,

(6)

where the ReLU function ensures that only features that
have a positive inﬂuence on the class prediction are non-
zero.

Excitation Backpropagation is an intuitively simple, but
empirically effective explanation method. In [28], it is ar-
gued and demonstrated experimentally that explainability
approaches such as EB [39], which ignore nonlinearities in
the backward-pass through the network, are able to gen-
erate heat-maps that “conserve” evidence for or against a
network predicting any particular class. Let al
i be the i’th
neuron in layer l of a neural network and a(l−1)
be a neu-
ron in layer (l − 1). Deﬁne the relative inﬂuence of neu-
ron a(l−1)
i, where
) and for W (l−1) being the synap-
yl
tic weights between layers (l − 1) and l, as a probability
distribution P (a(l−1)
) over neurons in layer (l − 1). This
probability distribution can be factored as

i = σ(Pji W l−1

i ∈ R of neuron al

on the activation yl

ji y(l−1)

j

j

j

j

P (a(l−1)

j

) =Xi

P (a(l−1)

j

|al

i)P (al

i).

(7)

Zhang et al.
P (a(l−1)
|al

j

i) as

[39] then deﬁne the conditional probability

|al

i) =(Z (l−1)

0

i

y(l−1)
j W (l−1)

ji

if W (l−1)
otherwise,

ji

≥ 0,

(8)

P (a(l−1)

j

where

Z (l−1)

i

y(l−1)
j W (l−1)

=
Xj

−1

ji 


is a normalization factor such that Pj P (a(l−1)

i) = 1.
For a given input (e.g., an image), EB generates a heat-map
in the pixel-space w.r.t. class c by starting with P (aL
i =
c) = 1 at the output layer and applying Equation (7) recur-
sively.

|al

j

These reviewed explainability methods were originally
designed for CNNs, which are deﬁned for signals on a uni-
form square grid. Here, we are interested in signals sup-
ported on non-Euclidean structures, e.g., graphs. In what

10774

'/012232×1

CC(C)NCC(O)COc1cccc2ccccc12.[Cl]

Softmax
classification

Global	Average	
pooling

! = 3

! = 2

! = 1

Input	
Graph	
! = 0

).×1

'×).

'×)-

'×),

'

'×)*+

), = 128,

)- = 256,

). = 512	

87

'

67

)*+ = 75

learn a classiﬁer that maps each molecule to its correspond-
ing label, g : (Xi, Ai) → yi.

Given that our task is to classify individual graphs (i.e.,
molecules) with potentially different number of nodes, we
use several layers of graph convolutional layers followed by
a global average pooling (GAP) layer over the graph nodes
(e.g., atoms).
In this case, all graphs will be represented
with a ﬁxed size vector. Finally, the GAP features are fed to
a classiﬁer. To enable applicability of CAM [42], we simply
used a softmax classiﬁer after the GAP layer.

3.3. Explainability for Graph Convolutional Neural

Networks

Figure 1: Our GCNN + GAP architecture together with the
visualization of the input feature and adjacency matrix for a
sample molecule from the BBBP dataset.

In this subsection, we describe the extension of CNN ex-
plainability methods to GCNNs. Let the k’th graph convo-
lutional feature map at layer l be deﬁned as:

2

2 ˜A ˜D− 1
V

{z

}

|

follows, we ﬁrst brieﬂy discuss GCNNs and then describe
the extensions of these explainability methods to GCNNs.
For intuition, we note that images may be conceptualized as
lattice-shaped graphs with pixel values as node feature. In
this sense, GCNNs generalize CNNs to accommodate arbi-
trary connectivity between nodes.

3.2. Graph Convolutional Neural Networks

Let an attributed graph with N nodes be deﬁned with
its node attributes X ∈ RN ×din and its adjacency matrix
A ∈ RN ×N (weighted or binary). In addition, let the degree

matrix for this graph be Dii =Pj Aij . Following the work

of Kipf and Welling [19], we deﬁne the graph convolutional
layer to be

F l(X, A) = σ( ˜D− 1

F (l−1)(X, A)W l) ,

(9)

˜Dii = Pj

where F l is the convolutional activations at the l′th layer,
F 0 = X, ˜A = A + IN is the adjacency matrix with added
self connections where IN ∈ RN ×N is the identity matrix,
˜Aij , W l ∈ Rdl×dl+1 are the trainable convolu-
tional weights, and σ(·) is the element-wise nonlinear acti-
vation function. Figure 1 shows the used GCNN architec-
ture in this work, where the activations in layers l = 1, 2, 3
follow Eq. (9), which is a ﬁrst-order approximation of lo-
calized spectral ﬁlters on graphs.

For molecule classiﬁcation, each molecule can be repre-
sented as an attributed graph Gi = (Xi, Ai), where the node
features Xi summarize the local chemical environment of
the atoms in the molecule, including atom-types, hybridiza-
tion types, and valence structures [35], and the adjacency
matrix encodes atomic bonds and demonstrate the connec-
tivity of the whole molecule (see Figure 1). For a given
dataset of labeled molecules D = {Gi = (Xi, Ai), yi}M
i=1
with labels yi indicating a certain chemical property, e.g.,
blood-brain-barrier penetrability or toxicity, the task is to

k(X, A) = σ(V F (l−1)(X, A)W l
F l
k)

(10)

2 (see Eq.

2 ˜A ˜D− 1

k denotes the k′th column of matrix W l, and V =
where W l
˜D− 1
In this notation, for node n,
the k’th feature at the l’th layer is denoted by F l
k,n. Then,
the GAP feature after the ﬁnal convolutional layer, L, is
calculated as

(8)).

ek =

1
N

N

Xn=1

F L

k,n(X, A)

,

(11)

and the class score is calculated as, yc = Pk wc

kek. Us-
ing this notation, we extend the explainability methods to
GCNNs as follows:

Gradient-based heat-maps over nodes n are

Lc

Gradient[n] = kReLU(cid:18) ∂yc

∂Xn(cid:19) k

CAM heat-maps are calculated as

.

(12)

Lc

CAM [n] = ReLU(Xk

wc

kF L

k,n(X, A))) .

(13)

Grad-CAM’s class speciﬁc weights for class c at layer l

and for feature k are calculated by

αl,c

k =

1
N

N

Xn=1

∂yc
∂F l

k,n

,

(14)

and the heat-map calculated from layer l is

Lc

Grad−CAM [l, n] = ReLU(Xk

αl,c
k F l

k,n(X, A)) .

(15)

Grad-CAM enables us to generate heat-maps with respect
to different layers of the network.
In addition, for our
model shown in Figure 1, Grad-CAM’s heat-map at the ﬁ-
nal convolutional layer and CAM’s heat-map are equivalent

10775

Grad−CAM [L, n] = Lc

Lc
In this work, we report results for Lc
the variant Grad-CAM Avg deﬁned by

CAM [n] (See [31] for more details).
Grad−CAM [L, n] and

Lc

Grad−CAM Avg[n] =

1
L

L

Xl=1

Lc

Grad−CAM [l, n] .

(16)

Excitation Backpropagation’s heat-map for our model
is calculated via backward passes through the softmax clas-
siﬁer, the GAP layer, and several graph convolutional lay-
ers. The equations for backward passes through the softmax
classiﬁer and the GAP layer are

p(ek) =Pc

p(F L

k,n) =

F L
k,n
N ek

ekReLU (wc
k)
k) p(c) Softmax
Pk ekReLU (wc

(17)

p(ek)

GAP ,

where p(c) = 1 for the class of interest and zero otherwise.
The backward passes through the graph convolutional lay-
ers, however, are more complicated. For notational simplic-
ity, we decompose a graph convolutional operator into







k,m

ˆF l

k,n =Pm Vn,mF l
k′,n = σ(Pk′ ˆF l

F (l+1)

k,nW l

k,k′ ) ,

(18)

where the ﬁrst equation is a local averaging of atoms (with
Vn,m ≥ 0), and the second equation is a ﬁxed perceptron
applied to each atom (analogous to one-by-one convolutions
in CNNs). The corresponding backward passes for these
two functions can be deﬁned as




p(F l

p( ˆF l

k,n) =Pm
k,n) =Pk′

Vn,mF l

k,n

Pn Vn,mF l

k,m

p( ˆF l

k,m)

ˆF l

k,nReLU (W l
ˆF l

k,nReLU (W l

k,k′ )

k,k′ )

Pk

p(F (l+1)

k′,n ) .

(19)
We generate the heat-map over the input layer by recur-
sively backpropagating through the network and averaging
the backpropagated probability heat-maps on the input:

Lc

EB[n] =

1
din

din

Xk=1

p(F 0

k,n)

.

(20)

The contrastive extension of Lc
we call this contrastive variant, c-EB.

EB follows Eq. (8) in [39];

4. Experiments

This section describes our experiments and analysis of
class-speciﬁc explanations. We experiment on two applica-
tion domains, visual scene graphs and molecules. Addition-
ally, we perform a frequency analysis of graph substructures
identiﬁed by the explainability methods in the supplemen-
tary material.

4.1. Explanations on Scene Graphs

Our goal here is to train GCNNs for scene-graph clas-
siﬁcation and use our proposed explainability methods on
these GCNNs. A scene graph is a graph structured data
where nodes are objects in the scene and edges are relation-
ships between objects. We obtain the data for our ﬁrst ex-
periment from the Visual Genome dataset [20]. The Visual
Genome dataset consists of images and scene graph pairs.
Objects and relationships are of many types and the data
is collected from free text responses obtained from crowd
sourced workers. Objects have an associated region of the
image, deﬁned by a bounding box.

We construct two binary classiﬁcations tasks on scene
graphs from the Visual Genome: country vs. urban, and
indoor vs outdoor. We model each of these words with a set
of keywords, which are used to query the Visual Genome
data for matches in any attribute of an image. The image
set for a class is union of the image sets returned for each
keyword. In addition, any intersection between classes pairs
was removed. Keywords used to deﬁne each class are as
follows:

• “country”: country, countryside, farm, rural, cow,

crops, sheep

• “urban”: urban, city, downtown

• “indoor”: indoor, room, ofﬁce, bedroom, bathroom

• “outdoor”: outdoor, nature, outside

The keywords are non-comprehensive and are not
claimed to encompass the full meaning of each word.
Rather they are synthetic constructions devised for the pur-
pose of studying explanation methods on graphs. Since
the aim of this work is to study explanations, the exact de-
tails of the class deﬁnitions are not germane to the present
work. Keywords were chosen to give approximately bal-
anced classes between classiﬁcation pairs. Class ratios for
each dataset are reported in Table 1.

For simplicity, and without loss of generality, we col-
lapse all relationships into single type, which represents a
generic relationship between two objects. We note that the
various extensions of GCNNs exist for graphs with rela-
tional edges [29] for which the explainability methods de-
veloped in this paper could be used.

There is a bounding box associated with each object (i.e.,
node) in the scene-graph. We use a pretrained InceptionV3
network and extract deep features of the underlying image
for bounding boxes for all the nodes of a scene-graph. The
ﬁnal pooling layer was used as the visual feature, where
each crop was zero padded to a ﬁxed size. The feature ex-
tracted from each bounding box is of size d = 2048. Thus
our derived scene graphs consist of relational and visual
data. The ﬁnal task is then to classify the scene-graphs into
classes urban vs. country and indoor vs. outdoor.

10776

4.2. Explanations on Molecular Graphs

We study a second application domain: identifying func-
tional groups on organic molecules for biological molecular
properties. We evaluated explanation methods on three bi-
nary classiﬁcation molecular datasets, BBBP, BACE, and
task NR-ER from TOX21 [35]. Each dataset contains bi-
nary classiﬁcations of small organic molecules as deter-
mined by experiment. The BBBP dataset contains mea-
surements on whether a molecule permeates the human
blood brain barrier and is of signiﬁcant interest to drug de-
sign. The BACE dataset contains measurements on whether
a molecule inhibits the human enzyme β-secretase. The
TOX21 dataset contains measurements of molecules for
several toxicity targets. We selected the NR-ER task from
this data, which is concerned with activation of the estrogen
receptor [24]. These datasets are imbalanced. See Table 1
for the class ratios,

In addition, we followed the recommendations in [35],
which is the original paper describing the MoleculeNet
dataset, for train/test partitioning. In particular, for BACE
and BBBP, the so called “scaffold” split is recommended
by [35], which partitions molecules according to their struc-
ture, i.e. structurally similar molecules are partitioned in the
same split. We emphasize that training the GCNNs and the
conventional dataset splits are not the contribution of our
paper and we simply follow the standard practice for these
datasets.

4.3. Training and Evaluation

We partition the datasets into 80:20 training/testing sets.
For all datasets, we used the GCNN + GAP architecture
as described in Figure 1 with the following conﬁguration:
three graph convolutional layers of size 128, 256, and 512,
respectively, followed by a GAP layer, and a softmax clas-
siﬁer. Models were trained for 25 epochs using the ADAM
optimizer with learning rate 0.001, β1 = 0.9, β2 = 0.999.
The models were implemented in Keras with Tensorﬂow
backend [5]. Evaluation metrics AUC-ROC and AUC-PR
of trained models for each dataset are reported in Table 1.

Some molecular classiﬁcations results are observed to
have higher mean testing vs.
training performance. Al-
though this is unusual, the results are consistent to those
reported in [35].

4.4. Analysis of Explanation Methods

After training models for each dataset, we apply each
explanation method on all samples and to obtain a set of
scalars over nodes, i.e. a heatmap.

We show selected results in Figures 3 and 4.

In Fig-
ure , scalar importance values are encoded as the color of
the bounding box outline (red: low, yellow: middle, green:
high). In Figure 4, scalar importance values are encoded as

Dataset

N

p

N

n

AUC-ROC
Test

Train

AUC-PR

Train

Test

Country vs.

Urban

Indoor vs.
Outdoor
BBBP
BACE
TOX21

3267

3862

0.987

0.986

0.939

0.936

8290
1560
691
793

7971
470
821
5399

0.962
0.991
0.994
0.868

0.963
0.966
0.966
0.881

0.871
0.994
0.939
0.339

0.876
0.966
0.999
0.347

Table 1: Class breakdown and evaluation metrics for each
dataset. Np and Nn denote the number of positive and neg-
ative examples respectively.

the intensity of blue disk over each atom (white: low, blue:
high).

The heat-maps are calculated for positive and nega-
tive classes and normalized for each molecule across both
classes and nodes to form a probability distribution over the
input nodes. Class speciﬁcity can be seen by comparing ex-
planations across classes within a method, i.e., when nodes
activated by one class tend to be inactivate for the other.

Next, we report three quantitative metrics that capture
desirable aspects of explanations: ﬁdelity, contrastivity, and
sparsity.

Fidelity was calculated to capture the intuition that oc-
clusion of salient features identiﬁed through explanations
should decrease classiﬁcation accuracy. More precisely, we
deﬁne ﬁdelity as the difference in accuracy obtained by oc-
cluding all nodes with saliency value greater than 0.01 (on
a scale 0 to 1). We then averaged the ﬁdelity scores across
classes for each method. We report these values in Table 2.
The Contrastive Gradient method showed highest ﬁdelity.

Contrastivity was designed to capture the intuition
that class-speciﬁc features highlighted by an explanation
method should differ between classes. More precisely, we
deﬁne contrastivity as the ratio of the Hamming distance
dH between binarized heat-maps ˆm0, ˆm1 for positive and
negative classes, normalized by the total number of atoms
identiﬁed by either method, ˆm0 ∨ ˆm1, dH ( ˆm0, ˆm1)
. We re-
port this metric in Table 2. Grad-CAM showed the highest
contrastivity.

ˆm0∨ ˆm1

Sparsity was designed to measure the localization of an
explanation. Sparse explanations are particularly useful for
studying large graphs, where manual inspection of all nodes
is infeasible. More precisely, we deﬁne this measure as one
minus the number of identiﬁed objects in either explanation
ˆm0 ∨ ˆm1, divided by the total number of nodes in the graph
|V |, 1 − ˆm0∨ ˆm1
. We report these values in Table 2. The
c-EB method showed the sparsest activations.

|V |

See the Supplementary for a descriptive Figure of con-

trastivity and sparsity.

Using these metrics, we compare the quality of each ex-
planation method and demonstrate the trade-offs of each
method. In short, we conclude three main points: (1) Grad-

10777

CAM is the most contrastive, has the second highest ﬁdelity,
but low sparsity. This method is generally suitable, but may
be problematic on large graphs. (2) c-EB is the most sparse
but has low ﬁdelity and the second highest contrastivity.
Therefore, this method is most suitable for analyzing large
graphs at the expense of low ﬁdelity. (3) Contrastive gradi-
ent (CG) has the highest ﬁdelity, but low sparsity and low
contrastivity. The lack of contrastivity makes this method
unsuitable for class-speciﬁc explanations.

4.5. Subgraph Frequency Analysis

As discussed above, explanation methods on GCNNs
identify salient structure on input graphs. The collection of
salient substructures identiﬁed in a dataset suggests an op-
portunity for further analysis. We analyze this collection for
reoccurring substructures, with the goal of yielding further
insight into the data. In short, we compute the prevalence
of each salient structure in the dataset, and compare against
its prevalence in the positively labeled dataset. We give a
brief description below, and refer the reader to the Supple-
mentary for further details.

To identify salient substructure in a graph, we ﬁrst col-
lect the set of vertices with saliency value greater than some
threshold (here, 0), which we call activated vertices. Then
we collect the connected components induced by this ver-
tex set. Repeating this for all graphs in the dataset yields
a collection of salient subgraphs. We then count the fre-
quency of each identiﬁed subgraph s in (1) the explanation
set, (2) the positively labeled data, and (3) the negatively
data, which we denote as N s
n respectively. We
used these counts to normalize the counts obtained from
the explanations and construct three ratios: Rs
,

p +N s
n
Rs
e measures
the prevalence of a subgraph in explanations. The ratios
Rs
n measure how prevalent a substructure occurs in pos-
itively and negatively labeled data respectively, and serve as
p or Rs
a baseline for the ﬁrst. Note that high Rs
n corresponds
to high class speciﬁcity for salient subgraph s.

n = N s

. The ratio Rs

e = N s

p and N s

N s
p
p +N s
n

e , N s

p, Rs

p =

, Rs

p +N s
n

N s

N s

N s

n

e

We report the top structures by the ratio Rs

e obtained
from Grad-CAM for the molecule datasets and Visual
Genome datasets in Figure 2.

In the case of molecules, we note that salient sub-
graphs have an interpretation as functional groups, i.e. a
substructures that have a functional role in determining a
toxicity. Top results in Figure 2
molecular property, e.g.
include amides, tricholoromethyl, sulfonamides, and aro-
matic structures. The validity of these results as functional
groups remains to be determined by experiment.

Other work in algorithmic identiﬁcation of functional
groups include [10] and [8]. Both are rule based ap-
[8] discusses
proaches, whereas our method is learned.
some of the limitations of algorithmic identiﬁcation of func-

Figure 2: Top two substructures for each class/property, as iden-
tiﬁed by CAM/GradCAM.

tional groups. The notion of functional group is not pre-
cisely deﬁned. [8] remarks that every medicinal chemists
likely has their own notion of functional group. We do
not claim our method is the right deﬁnition of functional
groups. However, given the success of convolution neural
networks in modeling other intuitive concepts in vision and
language, we envision potential for practical applications in
this area. Further subgraph results are shown in the Supple-
mentary.

5. Conclusions

In this work, we extended explainability methods de-
signed for CNNs to GCNNs and compared these methods
qualitatively and quantitatively. We demonstrated the ex-
tended methods on two application domains: visual scene
graphs and molecular graphs.

We characterized the explanations obtained by each
method with three metrics: ﬁdelity, contrastivity, and spar-
sity. Grad-CAM showed the highest contrastivity, followed
by c-EB. While c-EB showed the sparsest activations. Fi-
nally, CG showed the highest ﬁdelity, but lowest contrastiv-
ity and second lowest sparsity. When ﬁnding class-speciﬁc
explanations, however, contrastivity is a key requirement,
which would disqualify CG.

We conclude that Grad-CAM is most suitable among the
studied methods for explanations on graphs of moderate
size. However, we caution the reader to choose a method
with characteristics most appropriate to their application. If
sparsity of explanations of is high concern, e.g., in the case
of interpreting large graphs, then c-EB might be the better
choice.

On both scene graphs and molecules, we identiﬁed graph
substructures that were relevant for a given classiﬁcation,
e.g., identifying a scene as being indoor. Such a method
may be useful for automatically mining relevant functional
groups of molecules. Additional experiments, however, are
needed to conﬁrm the chemical validity of the identiﬁed
substructures.

This work demonstrates tools for explaining GCNNs and
mining patterns in substructures of graph inputs, includ-
ing scene graphs, molecules, social networks, and electrical
grids. In future work, we plan to investigate new explain-
ability methods for graphs, and the use of explainability to
improve the quality of visual scene graphs.

10778

BBBP

0.19

0.45± 2.19
0.22± 2.43

0.17

99.99± 0.11
6.26±7.83

BACE

0.38

0.77± 2.99
0.28±1.58

0.36

100.0± 0.0
9.36±7.67

0.24

0.17

0.38

59.93 ± 20.47

41.06±19.05

29.22±14.04

0.63 ± 7.30

0.01±0.07

0.0±0.0

Country vs. Urban

Indoor vs. Outdoor

CG

CAM/

Grad-CAM

Grad-CAM

Avg

EB

c-EB

0.40

0.02 ± 0.45

11.67 ± 16.42

0.25

99.99 ± 0.39
2.82 ± 7.84

0.26

58.41 ± 19.70

0.57 ± 7.07

0.12

29.68 ± 25.45
60.92 ± 23.28

0.07

81.46 ± 21.69
59.35 ± 23.13

0.40

0.18 ± 1.88

14.23 ± 17.45

0.25

100.00 ± 0.11

2.62 ± 8.07

0.06

68.35 ± 36.35
64.31 ± 22.48

0.04

73.41 ± 37.76
76.31 ± 25.15

TOX21

0.53

0.2 ± 2.13
0.21±2.98

0.11

99.99± 0.29
4.86±8.74

0.17

44.03±23.7
0.01±0.11

Fidelity

Contrastivity

Sparsity

Fidelity

Contrastivity

Sparsity

Fidelity

Contrastivity

Sparsity

Fidelity

Contrastivity

Sparsity

Fidelity

0.18

0.38

0.19

50.87±18.76
40.35±22.11

60.29±15.40
51.4±13.97

49.06±22.59
30.12±23.04

0.18

0.35

0.12

96.97± 5.68
40.54±21.69

97.04±5.12
53.01±13.95

97.23±9.3

Contrastivity

31.31±22.91

Sparsity

Table 2: Measures of ﬁdelity, contrastivity, and sparsity for each method. The best performing method (on average) for each
metric is highlighted in green (higher values are better).

Figure 3: Selected explanation results for each Visual Genome dataset. All sample bounding boxes are shown in the Input
column. Bounding boxes are colored according to their saliency value (red:low, green:high). Top 5 objects are shown for
each method to reduce clutter. “Class 0” is either country or outdoor. “Class 1” is either urban or indoor. Contrastivity can
be seen by comparing regions across classes for each method, e.g., the gradient method tends to highlight the same features
for both classes, whereas CAM-GradCAM tends to highlight different objects. Best viewed on a computer screen.

Figure 4: Selected explanation results for each molecule dataset. Each sample is a true positive. A darker blue color indicates
a higher relevance for a given class. Sparsity can seen, e.g. in EB, by noting that fewer atoms are highlighted than other
methods. Best viewed on a computer screen.

10779

References

[1] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Van-
dergheynst. Geometric deep learning: going beyond eu-
clidean data. IEEE Signal Processing Magazine, 34(4):18–
42, 2017. 1

[2] J. Bruna, W. Zaremba, A. Szlam, and Y. Lecun. Spectral net-
works and locally connected networks on graphs. In Interna-
tional Conference on Learning Representations (ICLR2014),
CBLS, April 2014, 2014. 2

[3] S. Chen, A. Sandryhaila, J. M. Moura, and J. Kovacevic. Sig-
nal recovery on graphs: Variation minimization. IEEE Trans.
Signal Processing, 63(17):4609–4624, 2015. 2

[4] S. Chen, R. Varma, A. Sandryhaila, and J. Kovaˇcevi´c. Dis-
crete signal processing on graphs: Sampling theory. IEEE
transactions on signal processing, 63(24):6510–6523, 2015.
2

[5] F. Chollet. keras. https://github.com/fchollet/

keras, 2015. 6

[6] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolu-
tional neural networks on graphs with fast localized spectral
ﬁltering. In Advances in Neural Information Processing Sys-
tems, pages 3844–3852, 2016. 1, 2

[7] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bom-
barell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Con-
volutional networks on graphs for learning molecular ﬁnger-
prints.
In Advances in neural information processing sys-
tems, pages 2224–2232, 2015. 1, 2

[8] P. Ertl. An algorithm to identify functional groups in organic

molecules. J Cheminform, 9(1):36, Jun 2017. 7

[9] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E.
In

Dahl. Neural message passing for quantum chemistry.
International Conference on Machine Learning, 2017. 1

[10] N. Haider. Functionality pattern matching as an efﬁcient
complementary structure/reaction search tool:
an open-
source approach. Molecules, 15(8):5079–5092, Jul 2010. 7
[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 1

[12] M. Henaff, J. Bruna, and Y. LeCun.

Deep convolu-
tional networks on graph-structured data. arXiv preprint
arXiv:1506.05163, 2015. 2

[13] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In CVPR, vol-
ume 1, page 3, 2017. 1

[14] Z. Huang, C. Wan, T. Probst, and L. Van Gool. Deep learning
on lie groups for skeleton-based action recognition. In Pro-
ceedings of the 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 1243–1252. IEEE
computer Society, 2017. 2

[15] J. Johnson, A. Gupta, and L. Fei-Fei. Image generation from
scene graphs. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018. 1, 2

[16] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley.
Molecular graph convolutions: moving beyond ﬁngerprints.
Journal of computer-aided molecular design, 30(8):595–
608, 2016. 2

[17] P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T.
Sch¨utt, S. D¨ahne, D. Erhan, and B. Kim. The (un) reliabil-
ity of saliency methods. arXiv preprint arXiv:1711.00867,
2017. 3

[18] T. N. Kipf and M. Welling. Semi-supervised classiﬁca-
arXiv preprint

tion with graph convolutional networks.
arXiv:1609.02907, 2016. 2

[19] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation
with graph convolutional networks. In Advances in neural
information processing systems, 2017. 1, 2, 4

[20] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bern-
stein, and L. Fei-Fei. Visual genome: Connecting language
and vision using crowdsourced dense image annotations.
2016. 5

[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012. 1

[22] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. E. Hopcroft.
Convergent learning: Do different neural networks learn the
same representations? In FE@ NIPS, pages 196–212, 2015.
1

[23] A. Mahendran and A. Vedaldi. Understanding deep image
representations by inverting them.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 5188–5196, 2015. 1

[24] A. Mayr, G. Klambauer, T. Unterthiner, and S. Hochreiter.
Deeptox: toxicity prediction using deep learning. Frontiers
in Environmental Science, 3:80, 2016. 6

[25] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and
M. M. Bronstein. Geometric deep learning on graphs and
manifolds using mixture model cnns. In Proc. CVPR, vol-
ume 1, page 3, 2017. 1, 2

[26] Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and
K. Kim. Image to image translation for domain adaptation.
In IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 4500–4509, 2018. 1

[27] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

arXiv preprint, 2017. 1

[28] W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K.-
R. M¨uller. Evaluating the visualization of what a deep neural
network has learned. IEEE transactions on neural networks
and learning systems, 28(11):2660–2673, 2017. 3

[29] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg,
I. Titov, and M. Welling. Modeling relational data with graph
convolutional networks. In European Semantic Web Confer-
ence, pages 593–607. Springer, 2018. 5

[30] K. Sch¨utt, P. Kindermans, H. E. S. Felix, S. Chmiela,
A. Tkatchenko, and K. M¨uller. Schnet: A continuous-ﬁlter
convolutional neural network for modeling quantum interac-
tions.
In Advances in Neural Information Processing Sys-
tems, pages 991–1001, 2017. 2

[31] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, and D. Batra. Grad-cam: Visual explanations from
deep networks via gradient-based localization.
In ICCV,
pages 618–626, 2017. 1, 2, 5

10780

[32] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside
convolutional networks: Visualising image classiﬁcation
models and saliency maps. arXiv preprint arXiv:1312.6034,
2013. 1, 2

[33] D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Watten-
berg. Smoothgrad: removing noise by adding noise. arXiv
preprint arXiv:1706.03825, 2017. 2

[34] J. Wu, B. Zhou, D. Peck, S. Hsieh, V. Dialani, L. Mackey,
and G. Patterson. Deepminer: Discovering interpretable rep-
resentations for mammogram classiﬁcation and explanation.
arXiv preprint arXiv:1805.12323, 2018. 2

[35] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Ge-
niesse, A. S. Pappu, K. Leswing, and V. Pande. Moleculenet:
a benchmark for molecular machine learning. Chemical sci-
ence, 9(2):513–530, 2018. 1, 4, 6

[36] L. Yi, H. Su, X. Guo, and L. J. Guibas. Syncspeccnn: Syn-
chronized spectral cnn for 3d shape segmentation. In CVPR,
pages 6584–6592, 2017. 2

[37] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamil-
ton, and J. Leskovec. Graph convolutional neural net-
works for web-scale recommender systems. arXiv preprint
arXiv:1806.01973, 2018. 1

[38] M. D. Zeiler and R. Fergus. Visualizing and understanding
In European conference on com-

convolutional networks.
puter vision, pages 818–833. Springer, 2014. 1

[39] J. Zhang, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff. Top-
down neural attention by excitation backprop. In European
Conference on Computer Vision, pages 543–559. Springer,
2016. 1, 2, 3, 5

[40] Q. Zhang, Y. Nian Wu, and S.-C. Zhu. Interpretable convo-
lutional neural networks. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018. 1,
2

[41] Q.-s. Zhang and S.-C. Zhu. Visual interpretability for deep
learning: a survey. Frontiers of Information Technology &
Electronic Engineering, 19(1):27–39, 2018. 1, 2

[42] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2921–2929, 2016. 1, 2, 4

10781

