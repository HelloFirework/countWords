Decoders Matter for Semantic Segmentation:

Data-Dependent Decoding Enables Flexible Feature Aggregation

Zhi Tian1

Tong He1

Chunhua Shen1 ∗ Youliang Yan2

1The University of Adelaide, Australia

2Noah’s Ark Lab, Huawei Technologies

Abstract

Recent semantic segmentation methods exploit encoder-
decoder architectures to produce the desired pixel-wise seg-
mentation prediction. The last layer of the decoders is typ-
ically a bilinear upsampling procedure to recover the ﬁnal
pixel-wise prediction. We empirically show that this over-
simple and data-independent bilinear upsampling may lead
to sub-optimal results.

16 or 1

In this work, we propose a data-dependent upsampling
(DUpsampling) to replace bilinear, which takes advantages
of the redundancy in the label space of semantic segmen-
tation and is able to recover the pixel-wise prediction from
low-resolution outputs of CNNs. The main advantage of
the new upsampling layer lies in that with a relatively
lower-resolution feature map such as 1
32 of the in-
put size, we can achieve even better segmentation accu-
racy, signiﬁcantly reducing computation complexity. This
is made possible by 1) the new upsampling layer’s much
improved reconstruction capability; and more importantly
2) the DUpsampling based decoder’s ﬂexibility in leverag-
ing almost arbitrary combinations of the CNN encoders’
features. Experiments on PASCAL VOC demonstrate that
with much less computation complexity, our decoder out-
performs the state-of-the-art decoder. Finally, without any
post-processing, the framework equipped with our proposed
decoder achieves new state-of-the-art performance on two
datasets: 88.1% mIOU on PASCAL VOC with 30% com-
putation of the previously best model; and 52.5% mIOU on
PASCAL Context.

1. Introduction

Fully convolutional networks (FCNs) [21] have achieved
tremendous success in dense pixel prediction applications
such as semantic segmentation, for which the algorithm is
asked to predict a variable for each pixel of an input im-
age and is a fundamental problem in computer vision. The

∗First two authors equally contributed to this work. C. Shen is the cor-

responding author: chunhua.shen@adelaide.edu.au

Figure 1: An example of the encoder-decoder architecture used by
DeepLabv3+. Its decoder fuses low-level features of downsample ratio
= 4 and upsamples high-level features before merging them. Finally,
bilinear upsampling is applied to restore the full-resolution prediction.
“rate” denotes the atrous rate in atrous convolution.

great achievement of FCNs results from powerful features
extracted by CNNs. Importantly, the sharing convolutional
computation mechanism makes training and inference com-
putationally very efﬁcient.

In the original FCNs, several stages of strided convo-
lutions and/or spatial pooling reduce the ﬁnal image pre-
diction typically by a factor of 32, thus losing ﬁne im-
age structure information and leading to inaccurate predic-
tions, especially at the object boundaries. DeepLab [3] ap-
plies atrous (a.k.a dilation) convolutions, achieving large re-
ceptive ﬁelds while maintaining a higher-resolution feature
map. Alternatively the encoder-decoder architecture is of-
ten used to address this problem. The encoder-decoder ar-
chitecture views the backbone CNN as an encoder, respon-
sible for encoding a raw input image into lower-resolution
feature maps (e.g., 1
r of the input image size with r = 8, 16,
or 32). Afterwards, a decoder is used to recover the pixel-
wise prediction from the lower-resolution feature maps. In
previous works [5, 17], a decoder consists of a few convo-
lutional layers and a bilinear upsampling. The light-weight

3126

Root, stride 2257x257Block1 , stride 2129x129Block2 , stride 265x65Block3 , stride 233x33Block4 , stride 1,rate 2, 33x33Conv1x1,33x33Upsampleby 4 timesConcat129x129Conv3x3,129x129Conv3x3,129x129Conv3x3,129x129Bilinear by 4 timesPrediction513x513ArgmaxInputimage513x513EncoderDecoderConvolutional DecoderUpsamplingFigure 2: The framework with our proposed decoder. The major differences from the previous framework shown in Fig. 1 are 1) all fused features
are downsampled to the lowest features resolution before merging. 2) The incapble bilinear is replaced with our proposed DUpsampling to recover the
full-resolution prediction.

convolutional decoder yields high-resolution feature maps
and bilinear upsampling is ﬁnally applied to the resulting
feature maps to obtain the desired pixel-wise prediction.
The decoder commonly fuses low-level features to capture
the ﬁne-grained information lost by convolution and pool-
ing operations in CNNs. A standard DeepLabv3+ encoder-
decoder architecture is illustrated in Fig. 1.

A drawback of the oversimple bilinear upsampling is its
limited capability in recovering the pixel-wise prediction
accurately. Bilinear upsampling does not take into account
the correlation among the prediction of each pixel since it
is data independent. As a consequence, the convolutional
decoder is required to produce relatively higher-resolution
feature maps in order to obtain good ﬁnal prediction (e.g., 1
or 1
8 of the input size). This requirement causes two issues
for semantic segmentation.

4

1) The overall strides of the encode must be reduced very
aggressively by using multiple atrous convolutions [3, 32].
The price is much heavier computation complexity and
memory footprint, hampering the training on large data and
deployment for real-time applications.

For example, in order to achieve state-of-the-art per-
formance, the recent DeepLabv3+ [5] reduces the overall
strides of its encoder by four times (from 32 to 8). Thus
inference of DeepLabv3+ is very slow.

2) The decoder is often needed to fuse features at very
low levels. For example, DeepLabv3+ fuses features of
downsample ratio = 41 in block1 as shown in Fig. 1. It
is because that the ﬁneness of the ﬁnal prediction is actu-
ally dominated by the resolution of the fused low-level fea-
tures due to the inability of bilinear. As a result, in order to

1downsample ratio denotes the ratio of the resolution of the feature

maps to that of the input image.

produce high-resolution prediction, the decoder has to fuse
the high-resolution features at a low level. This constraint
narrows down the design space of the feature aggregation
and therefore is likely to cause a suboptimal combination
of features to be aggregated in the decoder. In experiments,
we show that a better feature aggregation strategy can be
found if the feature aggregation can be designed without
the constraint imposed by the resolution of feature maps.

In order to tackle the aforementioned issues caused by
bilinear, here we propose a new data-dependent upsampling
method, termed DUpsamling, to recover the pixel-wise pre-
diction from the ﬁnal outputs of the CNNs, replacing bi-
linear upsampling used extensively in previous works. Our
proposed DUpsampling takes advantages of the redundancy
in the segmentation label space and proves to be capable of
accurately recovering the pixel-wise prediction from rela-
tively coarse CNNs outputs, alleviating the need for precise
responses from the convolutional decoder.

As a result, the encoder is not required to overly re-
duce its overall strides, dramatically reducing the computa-
tion time and memory footprint of the whole segmentation
framework. Meanwhile, due to the effectiveness of DUp-
sampling, it allows the decoder to downsample the fused
features to the lowest resolution of feature maps before
merging them. This downsampling not only reduces the
amount of computation of the decoder, but much more im-
portantly it decouples the resolution of fused features and
that of the ﬁnal prediction. This decoupling allows the de-
coder to make use of arbitrary feature aggregation and thus
a better feature aggregation can be leveraged so as to boost
the segmentation performance as much as possible.

Finally, DUpsampling can be seamlessly incorporated
into the network with a standard 1×1 convolution and thus

3127

EncoderDecoderBackbone CNNDownsampleConv1x1DUpsampleFused featuresDUpsamplingArgmaxInput image513x513x333x33xC133x33xC2513x513needs no ad-hoc coding. Our overall framework is shown in
Fig. 2.

We summarize our main contributions as follows.

• We propose a simple yet effective Data-dependent Up-
sampling (DUpsampling) method to recover the pixel-
wise segmentation prediction from the coarse outputs
of the convolutional decoder, replacing the incapable
bilinear used extensively in previous methods.

• Taking advantages of our proposed DUpsampling, we
can avoid overly reducing the overall strides of the
encoder, signiﬁcantly reducing the computation time
and memory footprint of the semantic segmentation
method by a factor of 3 or so.

• DUpsampling also allows the decoder to downsample
the fused features to the lowest resolution of feature
maps before merging them. The downsampling not
only reduces the amount of computation of the decoder
dramatically but also enlarges the design space of fea-
ture aggregation, allowing a better feature aggregation
to be exploited in the decoder.

• Together with the above contributions, we propose a
new decoder scheme, which compares favourably with
state-of-the-art decoders while using much less com-
putation complexity. With the proposed decoder, the
framework illustrated in Fig. 2 achieves new state-of-
the-art performance: mIOU of 88.1%2 on PASCAL
VOC [9] with only 30% computation of the previous
best framework of DeepLabv3+ [5]. We also set a
new mIOU record of 52.5% on the PASCAL Context
dataset [23].

2. Related Work

Efforts have been devoted to improve pixel-wise predic-
tions with FCNs. They can be roughly divided into two
groups: atrous convolution [3, 32] and encoder-decoder ar-
chitectures [17, 5, 21, 2, 24].

Atrous convolution. A straightforward approach is to
reduce the overall strides of backbone CNNs by dropping
some strided convolutions or pooling layers. However, sim-
ply reducing these strides would diminish the receptive ﬁeld
of convolution networks substantially, which proves to be
crucial to semantic segmentation [3, 25, 20]. Atrous convo-
lutions [4, 3, 5, 32] can be used to keep the receptive ﬁeld
unchanged, meanwhile not downsampling the feature map
resolution too much. The major drawback of atrous convo-
lutions is much heavier computation complexity and larger
memory requirement as the size of those atrous convolu-
tional kernels, as well as the resulted feature maps, become
much larger [12, 6].

2The results on PASCAL VOC test set can be found at http://

host.robots.ox.ac.uk:8080/anonymous/UYT221.html

Encoder-decoder architectures. Encoder-decoder ar-
chitectures are proposed to overcome the drawback of
atrous convolutions and are widely used for semantic seg-
mentation. DeconvNet [24] uses stacked deconvolutional
layers to recover the full-resolution prediction gradually.
The method has the potential to produce high-resolution
prediction but is difﬁcult to train due to many parameters
introduced by the decoder. SegNet [2] shares a similar idea
with DeconvNet but uses indices in pooling layers to guide
the recovery process, resulting in better performance. Re-
ﬁneNet [17] further fuse low-level features to improve the
performance. Recently, DeepLabv3+ [5] takes advantages
of both encoder-decoder architectures and atrous convolu-
tion, achieving best reported performance on a few datasets
to date. Although efforts have been spent on designing a
better decoder, so far almost none of them can bypass the
restriction on the resolutions of the fused features and ex-
ploit better feature aggregation.

3. Our Approach

In this section, we ﬁrstly reformulate semantic segmen-
tation with our proposed DUpsampling and then present
the adaptive-temperature softmax function which makes the
training with DUsampling much easier. Finally, we show
how the framework can be largely improved with the fusion
of downsampled low-level features.

3.1. Beyond Bilinear: Data dependent Upsampling

In this section, we ﬁrstly consider the simplest decoder,
˜H× ˜W × ˜C

which is only composed of upsampling. Let F ∈ R
denote the ﬁnal outputs of the encoder CNNs and Y ∈
{0, 1, 2, ..., C}H×W be the ground truth label map, where
C and ˜C denotes the number of classes of segmentation
and the number of channels of the ﬁnal outputs, respec-
tively. Y is commonly encoded with one-hot encoding, i.e.,
Y ∈ {0, 1}H×W ×C . Note that F is typically of a factor
of 16 or 32 in spatial size of the ground-truth Y. In other
words,
32 . Since semantic segmentation
requires per-pixel prediction, F needs to be upsampled to
the spatial size of Y before computing the training loss.

˜W
W = 1

16 or 1

˜H
H =

Typically in semantic segmentation [4, 5, 21, 33, 13], the

training loss function is formulated as:

L(F, Y) = Loss(softmax(bilinear(F)), Y)).

(1)

Here Loss is often the cross-entropy loss, and bilinear is
used to upsample F to the spatial size of Y. We argue that
bilinear unsampling may not be the optimal choice here. As
we show in the experiments (Sec. 4.1.1), bilinear is oversim-
ple and has an inferior upper bound in terms of reconstruct-
ing (best possible reconstruction quality). In order to com-
pensate the loss caused by bilinear, the employed deep net-
work is consequently required to output higher-resolution

3128

Figure 3: The proposed DUpsampling. In the ﬁgure, DUpsampling is used to upsample the CNNs outputs F by twice. R denotes the resulting maps. W,
computed with the method described in Sec. 3.1, is the inverse projection matrix of DUpsampling. In practice, the upsampling ratio is typically 16 or 32.

feature maps, which are input to the bilinear operator. As
mentioned above, the solution is to apply atrous convolu-
tions, with the price of high computation complexity. For
example, reducing the overall strides from 16 to 8 incurs
more than 3 times computation.

An important observation is that the semantic segmenta-
tion label Y of an image is not i.i.d. and there contains struc-
ture information so that Y can be compressed considerably,
with almost no loss. Therefore, unlike previous methods,
which upsample F to the spatial size of Y, we instead at-
˜H× ˜W × ˜C and then com-
tempt to compress Y into ˜Y ∈ R
pute the training loss between F and ˜Y. Note that F and ˜Y
are of the same size.

In order to compress Y into ˜Y, we seek a transform un-
der some metric to minimize the reconstruction error be-
tween Y and ˜Y. Speciﬁcally, let r indicate the ratio of H
to ˜H, which is usually 16 or 32. Next, Y is divided into an
r × W
H
r grid of sub-windows of size r × r (if H or W is not
dividable by r, a padding is applied). For each sub-window
S ∈ {0, 1}r×r×C , we reshape S into a vector v ∈ {0, 1}N ,
with N = r × r × C. Finally, we compress the vector v to
˜C and then vertically and
a lower-dimensional vector x ∈ R
horizontally stack all x’s to form ˜Y.

Although a variety of ways can be used to achieve the
compression, we ﬁnd that simply using linear projecting,
˜C×N works well in
i.e., multiplying vvv by a matrix P ∈ R
this case. Formally, we have,

x = Pv; ˜v = Wx,

(2)

˜C×N is used to compress v into x. W ∈
where P ∈ R
RN × ˜C is the inverse projection matrix (a.k.a. reconstruc-
tion matrix) and used to reconstruct x back to v. ˜v is the
reconstructed v. We have omitted the offset term here. In
practice prior to the compression, v is centered by subtract-
ing its mean over the training set.

The matrices P and W can be found by minimizing the
reconstruction error between v and ˜v over the training set.
Formally,

P∗, W∗ = arg min

P,W

||v − ˜v||2

X

v

= arg min

P,W

X

v

||v − WPv||2.

(3)

This objective can be iteratively optimized with standard
stochastic gradient descent (SGD). With an orthogonality
constraint, we can simply use principal component analysis
(PCA) [29] to achieve a closed-form solution for the objec-
tive.

Using ˜Y as the target, we may pre-train the network with
a regression loss by observing that the compressed labels ˜Y
is real-valued

L(F, Y) = ||F − ˜Y||2.

(4)

Thus any regression loss, ℓ2 being a typical example as in
Eq. (4), can be employed here. Alternatively, a more direct
approach is to compute the loss in the space of Y. There-
fore, instead of compressing Y into ˜Y, we up-sample F
with the learned reconstruction matrix W and then com-
pute the pixel classiﬁcation loss between the decompressed
F and Y:

L(F, Y) = Loss(softmax(DUpsample(F)), Y).

(5)

With linear reconstruction, DUpsample(F) applies linear
˜C in the tensor
upsampling of Wfff to each feature fff ∈ R
F. Comparing with Eq. (1), we have replaced the bilin-
ear upsampling with a data-dependent upsampling, learned
from the ground-truth labels. This upsampling procedure is
essentially the same as applying a 1×1 convolution along
the spatial dimensions, with convolutional kernels stored in
W. The decompression is illustrated in Fig. 3.

Note that, besides the linear upsampling presented
above, we have also conducted experiments using a non-
linear auto-encoder for upsampling. Training of the auto-
encoder is also to minimize the reconstruction loss, and
is more general than the linear case. Empirically, we ob-
serve that the ﬁnal semantic prediction accuracy is almost
the same as using the much simpler linear reconstruction.
Therefore we focus on using the linear reconstruction in the
sequel.

Discussion with Depth-to-Space and Sub-pixel. The
simplest linear form of DUpsample can be viewed as an
improved version of Depth-to-Space in [28] or Sub-pixel in
[26] with pre-computed upsampling ﬁlters. Depth-to-Space
and Sub-pixel are typically used to upsample the inputs by

3129

2H x 2W x N/4H x W x CFRWRearrange1 x CC x N1 x N2 x 2 x N/4DUpsamplinga modest upsample ratio (e.g., ≤ 4), in order to avoid incur-
ring too many trainable parameters resulting in difﬁculties
in optimization. In contrast, as the upsampling ﬁlters in our
method are pre-computed, the upsample ratio of DUpsam-
ling can be very large (e.g., 16 or 32) if needed.

3.2. Incorporating DUpsampling with Adaptive 

temperature Softmax

So far, we have shown that DUpsampling can be used to
replace the incapable bilinear upsampling in semantic seg-
mentation. The next step is to incorporate the DUpsampling
into the encoder-decoder framework, resulting in an end-to-
end trainable system. While DUpsampling can be realized
with a 1×1 convolution operation, incorporating directly it
into the framework encounters difﬁculties in optimization.
Probably due to the W is computed with one-hot en-
coded Y, we ﬁnd that the combination of vanilla softmax
and DUpsampling has difﬁculty in producing sharp enough
activation. As a result, the cross-entroy loss is stuck during
the training process (as shown in experiment 4.1.4), which
makes the training process slow to converge.

In order to tackle the issue, we instead employ the soft-
max function with temperature [14], which adds a temper-
ature T into vanilla softmax function to sharpen/soften the
activation of softmax.

softmax(zi) =

exp(zi/T )

Pj exp(zj/T )

.

(6)

We ﬁnd that T can be learned automatically using the
standard back-propagation algorithm, eliminating the need
for tuning. We show in experiments that this adaptive-
temperature softmax makes training converge much faster
without introducing extra hyper-parameters.

3.3. Flexible Aggregation of Convolutional Features

The extremely deep CNNs [12, 6, 15] lead to the success
in computer vision. However, the depth also causes the loss
of ﬁne-grained information essential to semantic segmenta-
tion. It has been shown by a number of works [17, 5] that
combining the low-level convolutional features can improve
the segmentation performance signiﬁcantly.

Let F be the eventual CNNs feature maps used to pro-
duce the ﬁnal pixel-wise prediction by bilinear or afore-
mentioned DUpsampling. Fi and Flast represent the fea-
ture maps at level i of the backbone and last convolutional
feature maps of the backbone, respectively. For simplicity
we focus on fusing one level of low-level features, but it is
straightforward to extend it to multi-level fusion, which per-
haps boosts the performance further. The feature aggrega-
tion in previous decoders shown in Fig. 1 can be formulated
as,

F = f (concat(Fi, upsample(Flast))),

(7)

where f denotes a CNN and upsample is usually bilinear.
concat is a concatenation operator along the channel. As
described above, this arrangement comes with two prob-
lems. 1) f is applied after upsampling. Since f is a CNN,
whose amount of computation depends on the spatial size
of inputs, this arrangement would render the decoder inef-
ﬁcient computationally. Moreover, the computational over-
head prevents the decoder from exploiting features at a very
low level. 2) The resolution of fused low-level features Fi is
equivalent to that of F, which is typically around 1
4 resolu-
tion of the ﬁnal prediction due to the incapable bilinear used
to produce the ﬁnal pixel-wise prediction. In order to obtain
high-resolution prediction, the decoder can only choose the
feature aggregation with high-resolution low-level features.
In contrast, in our proposed framework, the responsibil-
ity to restore the full-resolution prediction has been largely
shifted to DUpsampling. Therefore, we can safely down-
sample any level of used low-level features to the resolution
of last feature maps Flast (the lowest resolution of feature
maps) and then fuse these features to produce ﬁnal predic-
tion, as shown in Fig. 2. Formally, Eq. (7) is changed to,

F = f (concat(downsample(Fi), Flast)),

(8)

where downsample is bilinear in our case. This rearrange-
ment not only keeps the features always to be computed
efﬁciently at the lowest resolution, but also decouples the
resolution of low-level features Fi and that of the ﬁnal seg-
mentation prediction, allowing any level of features to be
fused. In experiments, we show the ﬂexible feature fusion
enables us to exploit a better feature fusion to boost the seg-
mentation performance as much as possible.

Only when cooperating with the aforementioned DUp-
sampling, the scheme of downsampling low-level features
can work. Otherwise, the performance is bounded by the
upper bound of the incapable upsampling method of the
decoder. This is the reason why previous methods are re-
quired to upsample the low-resolution high-level feature
maps back to the spatial size of fused low-level feature
maps.

4. Experiments

The proposed models are evaluated on the PASCAL
VOC 2012 semantic segmentation [9], PASCAL Context
[23] and Cityscapes [7] benchmarks. We measure the per-
formance in terms of pixel intersection-over-union averaged
across the present classes (i.e., mIOU).

PASCAL VOC is the dataset widely used for seman-
tic segmentation. It consists of 21 classes including back-
ground. The splits of PASCAL VOC are 1, 464, 1, 449 and
1, 456 for training, validation and test, respectively. The ab-
lation study of our work is conducted over its val set. Also,
we report our performance over test set to compare with
other state-of-the-art methods.

3130

PASCAL Context is much larger than PASCAL VOC,
including 4, 998 images for training and 5, 105 images for
validation. Following previous works [17, 23], we choose
the most frequent 59 classes plus one background class (i.e.,
60 classes in total) in our experiments. There is not a test
server available and therefore we follow previous works
[17, 33, 3, 21, 36] to report our result on val set.

Cityscapes is a large-scale benchmark for semantic ur-
ban scene parsing. It contains 2, 975 images for training,
500 images for validation and 1, 525 images for testing. Ad-
ditionally, it also provides about 20, 000 weakly annotated
images.

Implementation details.

For all ablation experi-
ments on PASCAL VOC, we opt for ResNet-50 [12] and
Xception-65 [6] as our backbone networks, both of which
are modiﬁed as in [5]. Following [20, 4, 5], we use “poly”
as our learning rate policy for all experiments. The initial
learning rate is set as 0.007 and total iteration is 30k for ab-
lation experiments on PASCAL VOC. For all ResNet-based
experiments, weight decay is set to 0.0001. The batch size
is set to 48, but the batch normalization [16] statistics are
computed with a batch of 12 images. For all Xception-based
experiments, weight decay is 0.00004. We use a batch size
of 32 but compute the batch normalization statistics within
a batch of 16 images. We follow the practice [5, 4, 34]
to use the weights pre-trained on ImageNet [8] to initialize
backbone networks. All weights of newly added layers are
initialized with Gaussian distribution of variance 0.01 and
mean 0. T in adaptive-temperature softmax is initialized
to 1. ˜C is set as 64 for ResNet-50 based experiments and
128 for Xception-65 based experiments. Finally, following
previous works [4, 3, 5], we augment the training data by
randomly scaling the images from 0.5 to 2.0 and left-right
ﬂipping them.

4.1. Ablation Study

Our work focuses on the decoder part of the segmenta-
tion architecture. Therefore, for all ablation experiments,
we use the same encoder, as shown in Fig. 1. The en-
coder yields the ﬁnal feature maps with the 1
32 size
of the original image. The decoder aims to decode the low-
resolution feature maps into the prediction with the same
resolution as the original image.
In this section, we will
investigate different decoder schemes, and demonstrate our
proposed decoder’s advantages. We make use of ofﬁcial
train set instead of SBD [11] since it provides more consis-
tent annotations.

16 or 1

4.1.1 DUpsampling vs. Bilinear

First of all, we design experiments to show that the upper
bound of bilinear is much lower than that of DUpsampling,
which results in limited performance of bilinear. Speciﬁ-

Method

output stride mIOU (%) mIOU* (%)

bilinear
DUpsampling
bilinear
DUpsampling

32
32
16
16

70.77
72.09
72.15
73.15

94.80
99.90
98.40
99.95

Table 1: mIOU over the PASCAL VOC val set of DUpsampling vs. bilin-
ear upsampling. “output stride” indicates the ratio of input image spatial
resolution to ﬁnal output resolution. mIOU* denotes the upper bound.

Used low-level features mIOU (%)

FLOPS

N/A
conv1 3
b1u2c3
b3u6c3
b1u2c3 + b3u6c3
conv1 3 + b3u6c3

73.15
72.70
74.03
73.43
73.82
74.20

0.80B
1.13B
1.15B
1.23B
1.58B
1.56B

Table 2: mIOU over PASCAL VOC val set when using differ-
bxuycz denotes low-level features named
ent fusion of features.
block x/unit y/conv z in ResNet. “FLOPS” denotes the amount of com-
putation of the decoder including feature aggregation, convolutional de-
coder and the ﬁnal upsampling.

cally, we design a light-weight CNN including ﬁve convolu-
tional layers with kernel size being 3 and stride of 2, which
is fed with ground truth labels instead of raw images. Next,
DUpsampling or bilinear is added on top of that to recover
the pixel-wise prediction. This is similar to the decoder part
in the encoder-decoder architecture.

By training the two networks, with DUpsampling or
bilinear as decoder respectively, the ability to restore the
pixel-wise prediction can be quantitatively measured via
their performance over the val set, which can be viewed as
the upper bound of both methods. We use the training pro-
tocol described in implementation details to train the two
networks, except that the total iterations and initial learning
rate are set as 100k and 0.07, respectively. “output stride”
indicates the ratio of input image spatial resolution to the ﬁ-
nal CNN feature maps resolution. As shown in Table 1, the
upper bound performance of DUpsampling is well above
that of bilinear both when output stride being 32 and 16.

Given the superior upper bound performance of DUp-
sampling, we further carry out experiments with raw input
images. In the experiments, we employ ResNet-50 as the
backbone network. Unsurprisingly, by merely replacing the
bilinear with DUpsampling, the mIOU on PASCAL VOC
val set is improved by 1.3 points and 1 point, when the
output stride is 32 and 16 respectively, as shown in Table
1. The improvement is signiﬁcant because mIOU is strict.
Interestingly, the DUpsampling of output stride being 32
achieves similar performance to the bilinear case of output
stride being 16. This shows that the proposed DUpsampling
may eliminate the need for expensive computationally high-
resolution feature maps from the CNNs.

3131

Decoder

Low-level features / ratio

mIOU (%)

FLOPS

Decoder

low-level features / ratio mIOU (%)

FLOPS

ResNet-50

Vanilla
Proposed
Vanilla
Proposed

b1u2c3 / 4
b1u2c3 / 4
conv1 3 / 2 + b3u6c3 / 16
conv1 3 / 2 + b3u6c3 / 16

Xception-65

efb2u1c2 / 4
Vanilla
Proposed
efb2u1c2 / 4
mfb1u16c3 / 16
Vanilla
Proposed mfb1u16c3 / 16

73.26
74.03

-

74.20

78.70
79.09
78.74
79.67

5.53B
1.15B
22.34B
1.56B

5.53B
1.93B
0.41B
1.98B

Table 3: mIOU over the PASCAL VOC val set when using different
fusion strategies of features. bxuycz denotes low-level features named
block x/unit y/conv z in ResNet or Xception. “ef” and “mf” respectively
indicate “entry ﬂow” and “middle ﬂow” in Xception. “-” means out-of-
memory. “ratio” denotes the ratio of the resolution of feature maps to the
resolution of the input image (i.e., downsample ratio). “FLOPS” denotes
the amount of computation of the decoders.

4.1.2 Flexible aggregation of convolutional features

Due to the ﬂexibility of our proposed decoder, we can em-
ploy any combination of features to improve segmentation
performance, regardless of the resolution of fused features.
For ResNet-50, we experiment with many different com-
binations of features, as shown in Table 2. The best one
is the combination of conv1 3 + b3u6u3, achieving mIOU
74.20% over val set. Additionally, as shown in Table 2, the
amount of computation changes little when features at dif-
ferent levels are fused, which allows us to choose the best
feature fusion without considering the price of computation
incurred by the resolution of fused features.

In order to understand how the fusion works, we visu-
alize the segmentation results with and without low-level
features in Fig. 4. Intuitively, the one fusing low-level fea-
tures yields more consistent segmentation, which suggests
the downsampled low-level features are still able to reﬁne
the segmentation prediction substantially.

4.1.3 Comparison with the vanilla bilinear decoder

We further compare our proposed decoder scheme with
the vanilla bilinear decoder shown in Fig. 1, which fuses
low-level features b1u2c3 (downsample ratio = 4). As

Figure 4: The prediction results with low-level features and without low-
level features. ResNet-50 is used as the backbone.

Vanilla
Proposed mfb1u16c3 / 16

efb2u1c2 / 4

79.36
79.06

43.65B
25.14B

Table 4: mIOU on the Cityscapes val set. Our proposed decoder with
much less computation complexity achieves a similar performance as the
vanilla decoder.

shown in Table 3, it achieves mIOU 73.26% on val set with
ResNet-50 as the backbone. By replacing vanilla decoder
with our proposed decoder in Fig. 2, the performance is
improved to 74.03%. Because of the same low-level fea-
tures used, the improvement should be due to the capable
DUpsampling instead of bilinear used to restore the full-
resolution prediction. Furthermore, we explore a better fea-
ture fusion conv1 3 + b3u6c3 for proposed deocder and im-
prove the overall performance slightly to 74.20%. When the
vanilla decoder uses the fusion of features, it incurs much
heavier computation computation complexity and runs out
of our GPUs memory due to the high resolution of conv1 3,
which prevents the vanilla decoder from exploiting the low-
level features.

We also experiment our proposed decoder with
Xception-65 as the backbone. Similarly, with the same low-
level features efb2u1c3 (downsample ratio = 4), our pro-
posed decoder improves the performance from 78.70% to
79.09%, as shown in Table 3. When using a better low-level
features mfb1u16c3 (downsample ratio = 16), the vanilla
decoder just improves the performance negligibly by 0.04%
because its performance is constrained by the incapable bi-
linear upsampling used to restore the full-resolution predic-
tion. In contrast, our proposed decoder can still beneﬁt a
lot from the better feature fusion due to the use of much
powerful DUpsampling. As shown in Table 3, with the bet-
ter feature fusion, the performance of our proposed decoder
is improved to 79.67%. Moreover, since we downsample
low-level features before fusing, our proposed decoder re-
quires much fewer FLOPS than the vanilla decoder of the
best performance, as shown in Table 3.

Finally, we compare our proposed decoder with the
vanilla bilinear decoder on the Cityscapes val set. Follow-
ing [5], Xception-71 is used as our backbone and the num-
ber of iterations is increased to 90k with a initial learning
rate being 0.01. As shown in Table 4, under the same train-
ing and testing settings, our proposed decoder achieves a
comparable performance with the vanilla one while using
much less computation.

4.1.4

Impact of adaptive-temperature softmax

As mentioned before,
the adaptive-temperature softmax
eases the training of the proposed DUpsampling method.
When training the framework with vanilla softmax with T
being 1, it achieves 69.81% over val set, which is signiﬁ-

3132

Imagew/o low-level featuresw/ low-level featuresGround truthmentation details. Each round is initialized with the last
round model and the base learning rate is reduced accord-
ingly (i.e. 0.007 for COCO, 0.001 for SBD and 0.0001 for
trainval). We use 500k iterations when training over COCO
and 30k iterations for the last two rounds. Additionally, fol-
lowing previous works [4, 5], we make use of multi-scale
testing and left-right ﬂipping when inferring over test set.

As shown in Table 5, our framework sets the new
record on PASCAL VOC and improve the previous method
DeepLabv3+ with the same backbone by 0.3%, which is
signiﬁcant due to the benchmark has been very compet-
itive. Meanwhile, since our proposed decoder can elimi-
nate the need for high-resolution feature maps, we employ
output stride being 16 instead of 8 in DeepLabv3+ when
inferring over test set. As a result, our whole framework
only takes 30% computation of DeepLabv3+ (897.94B vs.
3055.35B in Multiply-Adds) to achieve the state-of-the-art
performance. The performance of our proposed framework
on PASCAL Context val set is shown in Table 6. With
Xception-71 as backbone, our framework sets the new state-
of-the-art on this dataset without pre-training on COCO.

5. Conclusion

We have proposed a ﬂexible and light-weight decoder
scheme for semantic image segmentation. This novel de-
coder employs our proposed DUpsampling to produce the
pixel-wise prediction, which eliminates the need for compu-
tationally inefﬁcient high-resolution feature maps from the
underlying CNNs and decouples the resolution of the fused
low-level features and that of the ﬁnal prediction. This de-
coupling expands the design space of feature aggregation of
the decoder, allowing almost arbitrary features aggregation
to be exploited to boost the segmentation performance as
much as possible. Meanwhile, our proposed decoder avoids
upsampling low-resolution high-level feature maps back to
the spatial size of high-resolution low-level feature maps,
reducing the computation of decoder remarkably. Exper-
iments demonstrate that our proposed decoder has advan-
tages of both effectiveness and efﬁciency over the vanilla
decoder extensively used in previous semantic segmenta-
tion methods. Finally, the framework with the proposed
decoder attains the state-of-the-art performance while re-
quiring much less computation than previous state-of-the-
art methods.

Acknowledgments The authors would like to thank
Huawei Technologies for the donation of GPU cloud com-
puting resources.

References

[1] Anurag Arnab, Sadeep Jayasumana, Shuai Zheng, and
Philip HS Torr. Higher order conditional random ﬁelds in
deep neural networks. In Proc. Eur. Conf. Comp. Vis., pages
524–540. Springer, 2016.

3133

Figure 5: Training losses for vanilla softmax and adaptive-temperature
softmax.

Method mIOU (%)

PSPNet [35]
DeepLabv3 [4]
EncNet [33]
DFN [31]
IDW-CNN [27]
CASIA IVA SDN [10]
DIS [22]
DeepLabv3+ [5] (Xception-65)
Our proposed (Xception-65)

85.4
85.7
85.9
86.2
86.3
86.6
86.8
87.8
88.1

Table 5: State-of-the-art methods on PASCAL VOC test set.

Method mIOU (%)

FCN-8s [21]
CRF-RNN [36]
HO CRF [1]
Piecewise [18]
VeryDeep [30]
DeepLabv2 [3]
ReﬁneNet [17]
EncNet [33]
Our proposed (Xception-65)
Our proposed (Xception-71)

37.8
39.3
41.3
43.3
44.5
45.7
47.3
51.7
51.4
52.5

Table 6: State-of-the-art methods on PASCAL Context val set.

cantly lower than 73.15% of the counterpart with adaptive-
temperature softmax. We further plot training losses for
vanilla softmax and adaptive-temperature softmax in Fig.
5, which shows the advantage of this adaptive-temperature
softmax.

4.2. Comparison with state of the art Methods

Finally, we compare the framework of our proposed
decoder with state-of-the-art methods. To compete with
these state-of-the-art methods, we choose Xception-65 as
the backbone network and the best feature aggregation in
the ablation study for our decoder.

Following previous methods, SBD [11] and COCO [19]
are used to train the model as well. Speciﬁcally, the model is
successively trained over COCO, SBD and PASCAL VOC
trainval set, with the training protocol described in imple-

050001000015000200002500030000Iteration0.00.51.01.52.02.5LossVanilla softmaxAdaptive-temperature softmax[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
SegNet: A deep convolutional encoder-decoder architecture
for image segmentation. IEEE Trans. Pattern Anal. Mach.
Intell., (12):2481–2495, 2017.

[17] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian D
Reid. ReﬁneNet: Multi-path reﬁnement networks for high-
resolution semantic segmentation.
In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., volume 1, page 5, 2017.

[3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs.
IEEE Trans. Pattern Anal.
Mach. Intell., 40(4):834–848, 2018.

[4] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for seman-
tic image segmentation. arXiv preprint arXiv:1706.05587,
2017.

[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-
rian Schroff, and Hartwig Adam. Encoder-decoder with
atrous separable convolution for semantic image segmenta-
tion. Proc. Eur. Conf. Comp. Vis., 2018.

[6] Franc¸ois Chollet. Xception: Deep learning with depthwise
separable convolutions. Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., pages 1610–02357, 2017.

[7] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding.
In Proc.
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.

[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei.
Imagenet: A large-scale hierarchical im-
age database. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,
pages 248–255. Ieee, 2009.

[9] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. Int. J. Comp. Vis., 88(2):303–338,
2010.

[10] Jun Fu, Jing Liu, Yuhang Wang, and Hanqing Lu. Stacked
deconvolutional network for semantic segmentation. arXiv
preprint arXiv:1708.04943, 2017.

[11] Bharath Hariharan, Pablo Arbel´aez, Lubomir Bourdev,
Subhransu Maji, and Jitendra Malik. Semantic contours from
inverse detectors. 2011.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proc. IEEE
Conf. Comp. Vis. Patt. Recogn., pages 770–778, 2016.

[13] Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming
Sun, and Youliang Yan. Knowledge adaptation for efﬁcient
semantic segmentation. arXiv preprint arXiv:1903.04688,
2019.

[14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
arXiv preprint

ing the knowledge in a neural network.
arXiv:1503.02531, 2015.

[15] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Weinberger. Densely connected convolutional networks.
In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.

[16] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. arXiv preprint arXiv:1502.03167, 2015.

[18] Guosheng Lin, Chunhua Shen, Anton Van Den Hengel, and
Ian Reid. Efﬁcient piecewise training of deep structured
models for semantic segmentation.
In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., pages 3194–3203, 2016.

[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
Proc. Eur. Conf. Comp. Vis., pages 740–755. Springer, 2014.
[20] Wei Liu, Andrew Rabinovich, and Alexander C Berg.
arXiv preprint

Parsenet: Looking wider to see better.
arXiv:1506.04579, 2015.

[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., pages 3431–3440,
2015.

[22] Ping Luo, Guangrun Wang, Liang Lin, and Xiaogang Wang.
Deep dual learning for semantic image segmentation.
In
Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 21–26,
2017.

[23] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and
Alan Yuille. The role of context for object detection and se-
mantic segmentation in the wild. In Proc. IEEE Conf. Comp.
Vis. Patt. Recogn., pages 891–898, 2014.

[24] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmentation.
In Proc. IEEE Int. Conf. Comp. Vis., pages 1520–1528, 2015.
[25] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and
Jian Sun. Large kernel mattersimprove semantic segmenta-
tion by global convolutional network. In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., pages 1743–1751. IEEE, 2017.

[26] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network.
In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 1874–
1883, 2016.

[27] Guangrun Wang, Ping Luo, Liang Lin, and Xiaogang Wang.
Learning object interactions and descriptions for semantic
image segmentation. In Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., pages 5859–5867, 2017.

[28] Zbigniew Wojna, Vittorio Ferrari, Sergio Guadarrama,
Nathan Silberman, Liang-Chieh Chen, Alireza Fathi, and
Jasper Uijlings. The devil is in the decoder. arXiv preprint
arXiv:1707.05847, 2017.

[29] Svante Wold, Kim Esbensen, and Paul Geladi. Principal
component analysis. Chemometrics and Intelligent Labora-
tory Systems, 2(1-3):37–52, 1987.

[30] Zifeng Wu, Chunhua Shen, and Anton van den Hengel.
Bridging category-level and instance-level semantic image
segmentation. arXiv preprint arXiv:1605.06885, 2016.

[31] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,
Gang Yu, and Nong Sang. Learning a discriminative fea-

3134

ture network for semantic segmentation. In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., 2018.

[32] Fisher Yu and Vladlen Koltun.

aggregation by dilated convolutions.
arXiv:1511.07122, 2015.

Multi-scale context
arXiv preprint

[33] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang,
Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Con-
text encoding for semantic segmentation.
In Proc. IEEE
Conf. Comp. Vis. Patt. Recogn., 2018.

[34] Pingping Zhang, Wei Liu, Hongyu Wang, Yinjie Lei, and
Huchuan Lu. Deep gated attention networks for large-
scale street-level scene segmentation. Pattern Recognition,
88:702–714, 2019.

[35] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network.
In
Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 2881–
2890, 2017.

[36] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang
Huang, and Philip HS Torr. Conditional random ﬁelds as
recurrent neural networks. In Proc. IEEE Int. Conf. Comp.
Vis., pages 1529–1537, 2015.

3135

