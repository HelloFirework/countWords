BeautyGlow: On-Demand Makeup Transfer Framework with Reversible

Generative Network

Hung-Jen Chen, Ka-Ming Hui, Szu-Yu Wang, Li-Wu Tsao, Hong-Han Shuai, Wen-Huang Cheng

National Chiao Tung University, Taiwan

{hjc.eed07g,ming2242.cm06g,joey840404.eed03,leo159753.me05,hhshuai,whcheng}@nctu.edu.tw

Abstract

As makeup has been widely-adopted for beautiﬁcation,
ﬁnding suitable makeup by virtual makeup applications be-
comes popular. Therefore, a recent line of studies proposes
to transfer the makeup from a given reference makeup image
to the source non-makeup one. However, it is still challeng-
ing due to the massive number of makeup combinations. To
facilitate on-demand makeup transfer, in this work, we pro-
pose BeautyGlow that decompose the latent vectors of face
images derived from the Glow model into makeup and non-
makeup latent vectors. Since there is no paired dataset, we
formulate a new loss function to guide the decomposition.
Afterward, the non-makeup latent vector of a source image
and makeup latent vector of a reference image and are ef-
fectively combined and revert back to the image domain to
derive the results. Experimental results show that the trans-
fer quality of BeautyGlow is comparable to the state-of-the-
art methods, while the unique ability to manipulate latent
vectors allows BeautyGlow to realize on-demand makeup
transfer.

1. Introduction

As the slogan says, “Life isn’t perfect, but your makeup
can be.” Nowadays, makeup provides a convenient way to
improve the facial appearance, e.g., powder foundations for
hiding the skin imperfections, blushes for creating chubby
cheeks. Although makeup is ubiquitous in daily life, it is
not easy to ﬁnd the best-suited makeup because 1) makeup
trials are time-consuming and inconvenient, and 2) there
are thousands and thousands of cosmetic products, varying
from brands, functions, colors, and way-to-use, which leads
to a more intractable number of the makeup style combina-
tions. To address the ﬁrst issue, virtual makeup applications
that provide beautiﬁcation function come up to facilitate the
makeup trials, such as YoucamMakeup1 and Meitu2. How-

1https://www.perfectcorp.com/app/ymk
2http://makeup.meitu.com/en/

Figure 1: The makeup features such as eyeshadows and lip
gloss are extracted from reference makeup images and are
transferred to source non-makeup images. The lightness of
the makeup can be tuned by adjusting the magniﬁcation in
the latent space.

ever, how to ﬁnd suitable makeup for each user is still an
open problem.

One of the possible solutions is to let users select the
makeup styles from the reference photos of celebrities or
friends and transfer the makeup to users’ faces. For exam-
ple, Guo et al. [10] propose to decompose the images into
three layers, i.e., face structure layer, skin detail layer, and
color layer, and transfer the information from each layer of
reference images to the corresponding layer of the target
images. However, the predeﬁned layers and transfer func-
tion are not data-driven and thus are inclined to generate
artifacts in many cases. To directly learn from the data, a
dataset of photo triplets (user face, reference makeup, and
transferred results) is desirable for learning. Nevertheless,
most of the existing datasets only provide the images with
makeup and non-makeup pairs. To avoid the need for such
triplet datasets, [23] proposes to use CNN for identifying
different makeup functions and extracting the correspond-
ing features. Afterward, different loss functions are deﬁned
for different parts of makeup to make the after-makeup im-
ages natural, e.g., foundations are transferred by regulariz-
ing the inner product of the feature maps for smoothing the

10042

skin’s texture. Although previous work generates good re-
sults on makeup transfer, the domain knowledge is required
to design different functions to generate different makeup.

Recent years, Generative Adversarial Networks (GANs)
are widely used for generating high-resolution realistic im-
ages. For style transfer on whole images (e.g., painting
style), [30] proposes CycleGAN to incorporate the cycle
consistency loss to train the mapping function between two
domains by adopting two generators and discriminators.
However, CycleGAN can only generate a general makeup
style rather than a speciﬁc makeup style. Based on Cyl-
ceGAN, Chang et al. [1] introduce an asymmetric function
and train a makeup removal and transfer network together to
preserve the face identity. Moreover, [19] utilizes the GAN
framework and further proposes pixel-level histogram loss
to maximize the similarity of makeup style, while percep-
tual loss and cycle consistency loss are designed to preserve
identity. Nonetheless, there is no encoder in GAN-based
methods and thus cannot adjust the makeup extent by inter-
polating the latent space. Adjusting the makeup from light
to heavy is important for ﬁnding the best-suited makeup
since users can obtain many candidates from one reference
images.

A recent line of studies synthesizes images by ﬂow-
based generative models, of which the encoder function is
reversible and supports different applications by manipulat-
ing the latent space and reserving it back to real image space
[4, 15]. Due to the ability of manipulating latent space, in
this paper, we propose an unsupervised on-demand makeup
transfer approach, namely, BeautyGlow, based on the Glow
architecture [15]. Speciﬁcally, Glow provides a general
framework to learn a generative network with invertible
functions that encode input images into a meaningful la-
tent space and enable modiﬁcation of existent data points.
Although Glow allows manipulating the latent vectors, it
is challenging to transfer the local makeup details of the
reference image to the target image since the makeup and
face are mixed as the latent vector. One of the possible
approaches is to ﬁnd the average latent vector of makeup
images and average latent vector of non-makeup images,
and then use the difference as the direction of manipulat-
ing. However, this approach contains two major issues: 1) it
can only ﬁnd the general makeup but not the user-speciﬁed
makeup, and 2) it requires a lot of images from the same
person or same makeup to ﬁnd the correct average latent
vector.

To address this issue, BeautyGlow ﬁrst deﬁnes a trans-
formation matrix that decomposes the latent vectors into la-
tent vector of makeup features and latent vector of facial
identity features. However, due to the lack of paired data,
we further formulate a new loss function containing per-
ceptual loss, makeup loss, intra-domain loss, inter-domain
loss, and cycle consistency loss, to guide the decomposi-

tion. Compared with other methods based on GANs, Beau-
tyGlow does not need to train two large networks, i.e., gen-
erator and discriminator, which makes it more stable. Most
importantly, the invertible meaningful latent space with the
transformation matrix facilitated the on-demand makeup
transfer, that is, users can freely adjusting the makeup from
light to heavy and BeautyGlow spontaneously synthesizes
the results (less than 1 second).3 The contributions of this
paper are summarized as follows.

• Inspired by Glow, we propose BeautyGlow that can
transfer the makeup from reference image to target
image. The meaningful latent space facilitates on-
demand makeup adjustment. To the best of our knowl-
edge,
this is the ﬁrst Glow-based makeup transfer
framework.

• New transformation matrix and loss function are for-
mulated to guide the model training. It is worth noting
that the proposed framework can be easily extended to
other applications that require decomposing the latent
image vector into two latent vectors, e.g., rain removal,
fog removal.

• Experimental results on quantitative and qualitative
comparison manifest that the proposed BeautyGlow is
comparable to the state-of-the-art methods, while the
manipulation on latent vectors can generate realistic
images from light makeup to heavy makeup.

2. Related works

2.1. Makeup Studies

Traditionally, image processing techniques are applied
for makeup transfer. For example, image analogy [11] in-
troduces a framework that requires a pair of well-aligned
before-makeup and after-makeup photos of the same person
for makeup transfer. Guo et al. decompose facial details by
decomposing reference images into three layers and trans-
fer information from each layer to the corresponding layer
of the other image [10]. Moreover, [28] proposes to detect
the face landmarks and transfer the makeup by adjusting
the landmark with skin color GMM-based segmentation.
In addition to process the makeup, [17] manipulates the
intrinsic image layers with a physically-based reﬂectance
model to simulate the makeup with different lighting condi-
tions in a photo. Compared with these work, the proposed
BeautyGlow can perform makeup transfer and makeup re-
moval simultaneously with on-demand makeup adjustment
by manipulating the latent space without the need of post-
processing.

3A demo is available at https://beautyglow.github.io/.

10043

2.2. Style Transfer

To mix the content and style together, most works [7, 29]
use different layers in CNN to represent styles and content
and swap the style for style transfer. The style transfer can
be categorized into three classes. 1) Per-Style-Per-Model
(PSPM) [14, 25, 26, 8, 21, 20]. PSPM requires training dif-
ferent models for different styles. As the number of styles
becomes large (e.g., makeup), it is difﬁcult and inconve-
nient to use PSPM for style transfer. 2) Multiple-Style-Per-
Model (MSPM) [5, 2, 6]. MSPM only requires a trained
model to transfer multiple styles by slightly adjusting the in-
stance normalization layers. 3) Arbitrary-Style-Per-Model
(ASPM) [13, 9, 12]. ASPM focuses on the zero-shot style
transfer, which can transfer the new styles (unseen in the
training data) by ﬁnding the transformation function via a
style prediction network.
It is worth noting that the pro-
posed BeautyGlow belongs to ASPM. However, compared
with style transfer, the requirement of the details for makeup
transfer is much higher than that for style transfer.

2.3. GAN for Style/Makeup Transfer

Due to the good performance of generating realistic
images by adversarial network, GAN is widely-used for
image-to-image translation. For example, Taigman et al.
propose a method to transfer a realistic photo to comic char-
acters by GAN and VAE. Moreover, [18, 24, 16, 22] aim
to transfer style, attributes or speciﬁc details by exploiting
GANs. CycleGAN further introduces two coupled genera-
tor and a cycle consistency loss to transfer images between
two different domains [30], which avoids the requirement
of input-output pairs. Although CycleGAN can be directly
exploited to makeup transfer, i.e., one generator transfers a
general makeup to non-makeup faces, and the other gener-
ator removes makeup from makeup faces, it cannot transfer
a user-speciﬁed style. To fulﬁll the need to transfer a spe-
ciﬁc makeup to a non-makeup face, [1] alters the coupled
generator in CycleGAN by using two images (source and
target) for the makeup generator. The generator generates a
makeup mask referenced from the target image and covers
it on the source image to complete the makeup transfer. On
the other hand, BeautyGAN adopts similar idea with dual
input and output for makeup transfer and removal and en-
hance the correctness of instance-level makeup transfer by
matching the color histogram in different segments of the
face [19]. However, GAN-based methods contain no en-
coder to construct the latent space from the data and thus
can not realize on-demand makeup transfer by simply inter-
polating the latent vectors.

2.4. Latent Space Adaption

The likelihood-based approaches, such as Variational
AutoEncoders (VAE) and Glow, generate realistic images
with a latent space, while manipulating the latent space

model can facilitate developing new image applications,
e.g., image editing [31], avatar synthesis [27]. Speciﬁcally,
VAE optimizes the likelihood by maximizing the variational
lower bound and learns a latent space that represents all the
data points. On the other hand, ﬂow-based generative mod-
els [3, 4] construct a bijective reversible transfer function
F from the image space to the latent space so that modify-
ing latent vectors can be mapped back to real images with
F −1. Glow further improves the model architecture by a
new permutation layer which is learned after updating the
model rather than the ﬁxed permutation layer [15].

3. BeautyGlow

3.1. Glow

We ﬁrst present the Glow framework [15] as the prelim-
inary. Glow introduces an invertible function f compris-
ing a sequence of transformation matrices for mapping im-
ages into a latent space, where f is required to fulﬁll the
following properties. (1) The determinant of the Jacobian
matrix of the transformation matrix should be calculated ef-
ﬁciently. (2) From the sampled datapoint z in latent space,
mapping it back to the data x by using the inverse trans-
formation x = f −1(z) should also be obtained easily. To
accomplish these goals, Glow utilizes the additive layer and
afﬁne coupling layer introduced in [4]. A coupling layer
represents a bijection function, which can update part of the
input vector or latent vectors efﬁciently. Also, squeezing
layer is adapted to implement the multi-scale architecture,
which divides the image into sub-images of shape 2 × 2 × c,
then reshapes them into 1 ×1 × 4c for squeezing image in-
formation into channels. On top of that, before propagating
the output of the current level to the next level, half of the
dimensions of the output are factored out and dumped into
latent space to reduce the computational cost and the num-
ber of parameters. Furthermore, Glow replaces the ﬁxed
permutation in [4] with a new layer, Invertible 1 × 1 conv,
to avoid some components of the input vector is left un-
changed in the coupling layer. Based on this model, we
are able to transform input images into meaningful vectors
in latent space and manipulate the vectors for on-demand
makeup transfer.

3.2. Formulation

Based on the latent space derived from Glow, the goal is
to extract the makeup features from the reference makeup
image and apply it to the source non-makeup image. Specif-
ically, considering two domains, the non-makeup images
domain denoted as X ⊂ Rh×w×c where h, w, and c rep-
resent the height of input images, the width of input im-
ages, and the number of RGB channels, respectively. The
makeup images domain denoted as Y ⊂ Rh×w×c, which
are encoded into the latent space denoted asZ ⊂ Rc×h×w

10044

Figure 2: The framework of the BeautyGlow. Glow transforms all images in makeup domain I Y and all images in the non-
makeup domainI X as inputs: they are encoded into latent space Z , denoted as LX and LY respectively. a vector obtained
from LX as source non-makeup, LX
r , are decomposed into
two latent features (F X
s ), (F Y
s is generated by adding F X
and M Y
s ).

s , and a vector obtained from LY as reference makeup image, LY
r , M Y

r ) respectively through a transformation matrix W . LY
s , M Y

s can also be decomposed into reconstruction latent features, (F Y

together and LY

s , M X

r

s

through Glow[15].

s = LX

Given two images as inputs: a source image I X

s ∈ X and
a reference image I Y
r ∈ Y are encoded into two latent vec-
tors, LX
s ∈ Z and LY
r ∈ Z respectively with Glow. After
that, a transformation matrix W is expected to extract facial
features denoted as F X
s · W and extract makeup fea-
tures denoted as M Y
r · (I − W ), where I is identity
matrix. Here, we adopt the additive formulation, i.e., adding
F X
s ∈ Z ,
since 1) additive formulation can be computed on-the-ﬂy,
and 2) the interpolation in additive formulation is intuitive.
The after-makeup vector LY
is then decoded back to image
domain as the after-makeup image I Y
s ∈ Y . The model
architecture of BeautyGlow is shown in Figure 2.

to form after-makeup latent vector LY

s and M Y

r = LY

r

s

However, how to learn the transformation matrix W that
perfectly decomposes the latent vectors into latent facial
features and latent makeup features remains challenging.

3.3. Objective

To tackle this challenge, we introduce several losses to
guide the decomposition. 1) The Perceptual loss is pro-
posed to extract the facial features. 2) Intra-domain loss
and inter-domain loss are designed for ensuring the after-
makeup image and de-makeup image being at the makeup
and non-makeup domain respectively. 3) Makeup loss is
to extract the makeup features. 4) Cycle consistency loss

maintains the face and makeup information as input fea-
tures. The details of loss functions and model architecture
are presented as follows, while the effects of different losses
are evaluated in Section 4.

Perceptual Loss. We ﬁrst introduce the perceptual loss
Lp to teach W how to extract facial features. Since the
original source image is assumed to be non-makeup, we de-
ﬁne the perceptual loss as follows.

Lp = kF X

s − LX

s k2,

(1)

which constrains the distance between the facial latent fea-
ture F X
and the latent features of original source image
s
LX
s .

Makeup Loss. Since the latent vector of the reference im-
age includes makeup features, which means the W should
be able to discriminate face features and makeup features.
However, there is no image representing makeup styles, and
thus the makeup features cannot be derived by transferring
the image through Glow. Therefore, it is challenging to train
how to describe a makeup style with several latent features.
Here, we ﬁrst tackle this problem by assuming that the la-
tent features of a human face image are composed of fa-
cial features and makeup features. When the facial features

10045

are removed, the rest is makeup features. Meanwhile, as
shown in Glow[15], attributes could be extracted through
manipulating the latent features for images. Therefore, we
ﬁrst calculate the average latent vector of all images with
makeup, denoted as ¯LY , and the average latent vector of all
images without makeup, denoted as ¯LX . Since the differ-
ence (¯LY − ¯LX ) represents the direction from non-makeup
latent vector to makeup latent vector, which should be sim-
ilar to M Y

r , we formulate the makeup loss Lm as follows.

Lm = kM Y

r − ( ¯LY − ¯LX )k2.

(2)

Intra-Domain Loss. The makeup loss only forces M Y
r to
be similar with the direction from non-makeup latent vector
to makeup latent vector, while the centroids of non-makeup
latent vectors and makeup latent vectors are not fully ex-
ploited. The facial latent vectors of reference images, F Y
r ,
are supposed to be close to non-makeup domain rather than
makeup domain. Moreover, the after-makeup latent vectors
(LY
s ) are supposed to be close to the makeup domain instead
of the non-makeup domain. Therefore, we use the average
latent vector of all non-makeup images, ¯LX , to represent
the centroid of non-makeup domain, and the average latent
vector of all makeup images, ¯LY to represent the centroid
of makeup domain. The intra-domain loss is then deﬁned
as

Lintra = kF Y

r − ¯LX k2 + kLY

s − ¯LY k2.

(3)

Inter-Domain Loss.
In addition to the intra-domain loss,
we further introduce inter-domain loss to ensure that F Y
is away from the centroid of makeup domain to clearly de-
compose the facial latent vectors and makeup latent features
effectively. Meanwhile, LY
is also supposed to be away
from the centroid of non-makeup domain. As such, we for-
mulate the inter-domain loss as follows. Instead of the L2-
norm, we calculate the similarity between two latent vectors
A and B as

r

s

Sim(A, B ) =

sum(A N B)

|A||B|

,

(4)

where N denotes element-wise multiplication and | · | de-
note the L2-norm of the matrix. The loss function is ex-
pressed as

Linter = (1 + Sim(F Y

r , ¯LY )) + (1 + Sim(LY

s , ¯LX )) (5)

Cycle Consistency Loss.
In order to maintain the facial
and makeup information, two cycle consistency losses are
also designed in the latent space. Speciﬁcally, if we multi-
ply the after-makeup latent vector, i.e., LY
s , with transfor-
mation matrix W , it supposed to be close to the facial latent

vectors of the source image, i.e., F Y
s . On the other hand, if
we multiply LY
s with (I − W ), it is supposed to be close
as makeup latent features of reference latent features M Y
r .
Therefore, the loss function is formulated as

Lcyc = kLY

s W − F X

s k2 + kLY

s (I − W ) − M Y

r k2

(6)

Total loss.
as follows.

In sum, the overall loss function L is deﬁned

L = Lp + λcycLcyc + λmLm + λiaLintra + λieLinter

(7)
where λcyc, λm, λia, and λie are the weights to control the
relative importance of each term. The transformation matrix
W can be trained via the objective function in Equation 7
with gradient descent based method, which will be detailed
later.

4. Experimental Results

In this section, we ﬁrst describe the implementation de-
tails and training parameter setting. Afterward, we brieﬂy
introduce the baselines, which are the state-of-the-art meth-
ods in makeup transfer. We conduct both qualitative and
quantitative experiments to compare with baselines. A
qualitative analysis on different losses of BeautyGlow is
also presented. Finally, we show the advantages of ﬂow-
based method by presenting the makeup results from light
to heavy, which is simply derived by interpolating the latent
vectors and inverting back to image domain.

4.1. Implementation Details

Training Pipeline. We evaluate our method on MT
dataset [19] consisting of about 4000 female images (1000
non-makeup images and 3000 makeup images). MT dataset
includes different races, poses, expression and makeup
styles varying from subtle to heavy. Therefore, we ﬁrst fol-
low the architecture of Glow [15] with pre-trained weights.
Glow was originally trained on 5 machines with each 8
GPUs. Due to the limitation of the number of GPUs
and GPU memories, we resize the training image size to
128x128. It takes about 3 days to train a Glow model with
MT dataset.

Moreover, due to the resolution constraint, we ﬁrst ap-
ply face parsing,
i.e., separate different face parts and
keep makeup details. Images are separated (with/without
makeup) into left eye, right eye, lips, and the rest since eyes
and lips are signiﬁcantly changed by makeup. Please note
that we only train single W which is shared by different
parts of the faces. In post-processing, we use Poisson blend-
ing to integrate the generated makeup facial parts and the
source image.

10046

Reference

Source

Liao et al. [21]

Cycle-GAN [30]

Chang et al. [1]

Ours

Reference

Source

Liao et al. [21]

Cycle-GAN [30]

Li et al. [19]

Ours

Figure 3: Makeup Transfer Results

Training Details. Given the dataset of unpaired makeup
and non-makeup images, we ﬁrst use Glow to transform the
images into the latent space and calculate the centroids of
makeup and non-makeup latent domains. Afterward, we
train the transformation matrix W via the objective function
in Equation 7 with Adam optimizer, which is a classic ex-
tension of stochastic gradient descent procedure to update
model weights iteratively based on the training data. The
learning rate is set as 0.001 with the batch size as 100. The
size of the transformation matrix W is 128 × 128, while the
control parameters of perceptual loss, makeup loss, intra-
domain loss, inter-domain loss, and cycle consistency loss
are set as (λp = 0.01, λcyc = 0.001, λm = 0.1, λintra =
0.1, λinter = 1000), respectively.

4.2. Baselines

BeautyGlow is compared with several methods includ-
ing traditional method, style transfer and image domain
adoption via qualitative and quantitative experiments as
listed below.
Deep Image Analogy [21] extracts features by CNN and
adapts the notion of image analogy to the deep feature
space for obtaining semantically-meaningful dense corre-
spondences. Here, we add the WLS ﬁlter to keep details for
photo-to-photo transfer.
CycleGAN [30] is an unsupervised image-to-image trans-
lation work, which does not require paired images. There-
fore, makeup images and non-makeup images are regarded
as two different domains for training.
PairedCycleGAN [1] is a paired image-to-image makeup
transfer method which aims to transfer a speciﬁc reference
makeup to a source image.

BeautyGAN [19] is an instance-level makeup transfer
method for makeup transfer and removal via GAN. The re-
sults are enhanced by matching the color histogram in dif-
ferent segments of the face.

4.3. Qualitative Comparison

In Figure 3, we demonstrate a qualitative comparison of
the baselines and our results. The results produced by Im-
age Analogy show that eyeshadows and eyeliners disappear
and only subtle color changes stay around the eyes. This is
because it applies whole image transformation, parts aside
from makeup including skin tone and hair might also be
changed to an unnatural color. Compared with Image Anal-
ogy, image domain adoption methods can create a more
realistic look. CycleGAN [30] only synthesizes a general
makeup on the non-makeup face. The makeup style or
amount cannot be adjusted since the images are transferred
in only two general domain, makeup and non-makeup. The
general makeup, though realistic, cannot be used for spe-
ciﬁc makeup trials. Moreover, PairedCycleGAN [1] can
transfer makeup realistically and correctly in pairs. How-
ever, the disadvantages of PairedCycleGAN is that makeup
are generated by a ﬁxed adversarial network so that users
cannot adjust the makeup to be lighter or heavier. There-
fore, when transferring some heavy makeup, the results may
look unpleasant and sometimes unrealistic, but users can
only ﬁnd new reference images with lighter makeup, which
is time-consuming and even unfeasible.

Comparing to the baselines, our method successfully de-
composes the makeup latent vectors and non-makeup latent
vectors, so it does not change the look of the image severely
like style transfer. Meanwhile, we can generate a realistic

10047

Reference

Source

w.o. Lm

w.o. Lintra

Ours

Reference

Source

w.o. Lm

w.o. Lintra

Ours

Figure 4: Analysis of Different Loss Terms of BeautyGlow.

Table 1: User Study Results

preference comparison
Ours / Liao et al. [21]
Ours / Li et al. [19]
Ours / Chang et al. [1]

our result

baseline

60%
55%
45%

40%
45%
55%

and accurate makeup for a non-makeup face comparable to
PairedCycleGAN. In Section 4.6, we show the power of the
ability to ﬁne-tune the amount of makeup and customize it
for each face to get the perfect result.

4.4. Quantitative Comparison

For quantitative evaluation of BeautyGlow, a user study
is conducted with 50 volunteers (34 males and 16 females)
aged from 18 years old to 35 years old. To simplify the pro-
cess of comparison, a standard A/B test is used. Therefore,
each user is asked to compare the results of BeautyGlow
with Image Analogy [21], BeautyGlow with PairedCycle-
GAN [1], and BeautyGlow with BeautyGAN [19]. For each
user, we randomly choose ﬁfteen pairs of source and ref-
erence images and generate the results by different meth-
ods. The positions of different methods for the A/B test is
also randomly selected to avoid the bias. Table 1 shows
the results of the A/B test. For the ease of comparison,
we normalized the votes and get the preference percent-
age. The results show that BeautyGlow signiﬁcantly outper-
forms Image Analogy while is comparable to BeautyGAN
and PairedCycleGAN. Note that we observe some unusual
situations that user preference decreases while transferring
heavy makeup. Some exaggerated makeup is not suitable
for the source image, forcing users to choose a rather subtle

makeup face. As a result, with the latent space manipula-
tion in our model, we can ﬁne-tune the makeup amount for
a more preferred look.

4.5. Loss Analysis

We introduce ﬁve losses for training the transformation
function. In Figure 4, we show the results of two important
loss terms by training the transformation function W with-
out the loss term. When removing Lm, the transformation
matrix learns makeup features without average makeup fea-
tures. Hence, the makeup style is over-enhanced, leading to
unnatural after-makeup images, especially in the regions of
eyes. On the other hand, without Lintra, the after-makeup
image contains no makeup components because there is no
loss term constraining the resulting image to be close to the
centroid of the makeup images. In addition, facial features
are supposed to be at the group of non-makeup images, so
the facial features extracted from makeup images would in-
clude makeup features.

We show the results of face components like eyes and
mouth for different pairs of a reference image and a source
image. In the ﬁrst two rows, without using makeup loss,
the makeup style is over-emphasized and the color of other
regions is changed dramatically. When the intra-loss term is
removed, the result looks like the same as the source image.
Moreover, in the second row, the color of the results without
makeup loss is obviously brighter than the source and the
reference image.

4.6. Latent Space Manipulation

The highlighted property of BeautyGlow is the ability
to manipulate the latent vectors for generating the sane
makeup style with different lightness. To show the after-

10048

Reference

Source

0.8

1.1

1.4

1.7

Reference

Source

0.8

1.1

1.4

1.7

Figure 5: The obviousness of the makeup

makeup face with different makeup lightness, we manipu-
late the weight of the makeup features in the latent space. In
Figure 5, we synthesize 3 different lightness of two makeup
styles on different faces with the magniﬁcation of 0.7, 1.0,
1.5.
In the ﬁrst row, as the magniﬁcation increases, the
eyeshadows and lip color become redder and redder.
In
the second row, the eyeshadows also become darker and
the lip color becomes redder. It is worth noting that even
though the makeup is magniﬁed, the face is still not dis-
torted, which means the transformation matrix completely
decomposes the latent vector into the facial features and
makeup features for the manipulations.

5. Conclusions and Future Work

As quoted from Est´ee Lauder, “Glow is the essence of
beauty,” which is also true for makeup transfer. In this pa-
per, we propose BeautyGlow to decompose the latent vec-
tors derived from the Glow model into makeup and non-
makeup latent vectors. Since there is no paired dataset,
we formulate a new loss function containing perceptual
loss, makeup loss, inter-domain loss, intra-domain loss,
and cycle consistency loss, to guide the optimization of
the transformation matrix. Experimental results show that
the makeup transfer quality of BeautyGlow is comparable
to the state-of-the-art methods, while the unique ability to
manipulate latent vectors allows BeautyGlow to realize on-
demand makeup transfer. In the future, we plan to apply
the proposed general framework to other image synthesis
applications.

6. Acknowledgements

This work was supported in part by the Ministry of Sci-
ence and Technology of Taiwan under grants MOST-107-
2218-E-009-062, MOST-107-2218-E-002-054, MOST-
107-2221-E-182-025-MY2, MOST-105-2628-E-009-008-
MY3, MOST-106-2221-E-009-154-MY2, and the Grant
Number MOST 108-2634-F-009-006- through Pervasive
Artiﬁcial Intelligence Research (PAIR) Labs.

References

[1] Huiwen Chang, Jingwan Lu, Fisher Yu, and Adam Finkel-
stein. Pairedcyclegan: Asymmetric style transfer for apply-
ing and removing makeup.
In 2018 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 2,
3, 6, 7

[2] Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang
Hua. Stylebank: An explicit representation for neural image
style transfer. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017. 3

[3] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:

Non-linear independent components estimation. 2014. 3

[4] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.

Density estimation using real nvp. 2017. 2, 3

[5] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
A learned representation for artistic style. 2017 International
Conference for Learning Representations (ICLR), 2017. 3

[6] Michael Elad and Peyman Milanfar. Style transfer via texture
synthesis. IEEE Transactions on Image Processing, 2017. 3
[7] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A
neural algorithm of artistic style. 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015. 3
[8] Leon A Gatys, Alexander S Ecker, Matthias Bethge, Aaron
Hertzmann, and Eli Shechtman. Controlling perceptual fac-

10049

In 2017 IEEE Conference on
tors in neural style transfer.
Computer Vision and Pattern Recognition (CVPR), 2017. 3
[9] Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent
Dumoulin, and Jonathon Shlens. Exploring the structure of a
real-time, arbitrary neural artistic stylization network. 2017
British Machine Vision Conference (BMVC), 2017. 3

[10] Dong Guo and Terence Sim. Digital face makeup by ex-
ample. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2009. 1, 2

[11] Aaron Hertzmann, Charles E. Jacobs, Nuria Oliver, Brian
In SIG-

Image analogies.

Curless, and David H. Salesin.
GRAPH ’01, 2001. 2

[12] Wei-Lin Hsiao and Kristen Grauman. Learning the latent
look: Unsupervised discovery of a style-coherent embedding
from fashion images.
In 2017 IEEE International Confer-
ence on Computer Vision (ICCV), 2017. 3

[13] Xun Huang and Serge J Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. In 2017
IEEE International Conference on Computer Vision (ICCV),
2017. 3

[14] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep-
tual losses for real-time style transfer and super-resolution.
In 2016 European Conference on Computer Vision (ECCV),
2016. 3

[15] Diederik P Kingma and Prafulla Dhariwal. Glow: Gener-
ative ﬂow with invertible 1x1 convolutions. arXiv preprint
arXiv:1807.03039, 2018. 2, 3, 4, 5

[16] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh
Singh, and Ming-Hsuan Yang. Diverse image-to-image
translation via disentangled representations. 2018 The Eu-
ropean Conference on Computer Vision (ECCV), 2018. 3

[17] Chen Li, Kun Zhou, and Stephen Lin. Simulating makeup
through physics-based manipulation of intrinsic image lay-
ers. In 2015 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2015. 2

[18] Minjun Li, Haozhi Huang, Lin Ma, Wei Liu, Tong Zhang,
and Yugang Jiang. Unsupervised image-to-image translation
with stacked cycle-consistent adversarial networks. 2018 Eu-
ropean Conference on Computer Vision (ECCV), 2018. 3

[19] Tingting Li, Ruihe Qian, Chao Dong, Si Liu, Qiong Yan,
Wenwu Zhu, and Liang Lin. Beautygan: Instance-level fa-
cial makeup transfer with deep generative adversarial net-
work. In 2018 ACM Multimedia Conference on Multimedia
Conference (ACMMM), 2018. 2, 3, 5, 6, 7

[20] Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, and
Jan Kautz. A closed-form solution to photorealistic image
stylization. 2018 European Conference on Computer Vision
(ECCV), 2018. 3

[21] Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing
Kang. Visual attribute transfer through deep image analogy.
2017 ACM Transactions on Graphics (TOG), 2017. 3, 6, 7

[22] Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, and Tie-
Yan Liu. Conditional image-to-image translation. In 2018
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 3

[23] Si Liu, Xinyu Ou, Ruihe Qian, Wei Wang, and Xiaochun
Cao. Makeup like a superstar: Deep localized makeup trans-

fer network. 2016 International Joint Conference on Artiﬁ-
cial Intelligence (IJCAI), 2016. 1

[24] Am´elie Royer, Konstantinos Bousmalis, Stephan Gouws,
Fred Bertsch, Inbar Moressi, Forrester Cole, and Kevin Mur-
phy. Xgan: Unsupervised image-to-image translation for
many-to-many mappings. 2018 International Conference for
Learning Representations (ICLR), 2017. 3

[25] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Vic-
tor S Lempitsky. Texture networks: Feed-forward synthesis
of textures and stylized images. In 2016 International Con-
ference on Machine Learning (ICML), 2016. 3

[26] Dmitry Ulyanov, Andrea Vedaldi, and Victor S Lempitsky.
Improved texture networks: Maximizing quality and diver-
sity in feed-forward stylization and texture synthesis. In 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 3

[27] Lior Wolf, Yaniv Taigman, and Adam Polyak. Unsupervised
In 2017 IEEE Interna-

creation of parameterized avatars.
tional Conference on Computer Vision (ICCV), 2017. 3

[28] Lin Xu, Yangzhou Du, and Yimin Zhang. An automatic
framework for example-based virtual makeup. In 2013 IEEE
International Conference on Image Processing (ICIP), 2013.
2

[29] Yexun Zhang, Ya Zhang, and Wenbin Cai. Separating style
and content for generalized style transfer.
In 2018 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 3

[30] J Zhu, T Park, P Isola, and AA Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works, 2017. 2, 3, 6

[31] Jun-Yan Zhu, Eli Shechtman Philipp Kr¨ahenb¨uhl, and
Alexei A. Efros. Generative visual manipulation on the natu-
ral image manifold. In 2016 European Conference on Com-
puter Vision (ECCV), 2016. 3

10050

