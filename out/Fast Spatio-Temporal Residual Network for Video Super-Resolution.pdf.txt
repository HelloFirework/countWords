Fast Spatio-Temporal Residual Network for Video Super-Resolution

Sheng Li1, Fengxiang He2, Bo Du∗1, Lefei Zhang∗1, Yonghao Xu3, and Dacheng Tao2

1School of Computer Science, Wuhan University, China

2UBTECH Sydney AI Centre, SCS, FEIT, the University of Sydney, Australia

3The State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing,

{shli, remoteking, zhanglefei}@whu.edu.cn {fengxiang.he, dacheng.tao}@sydney.edu.au

yonghaoxu@ieee.org

Wuhan University, China

Abstract

Recently, deep learning based video super-resolution
(SR) methods have achieved promising performance. To si-
multaneously exploit the spatial and temporal information
of videos, employing 3-dimensional (3D) convolutions is a
natural approach. However, straight utilizing 3D convolu-
tions may lead to an excessively high computational com-
plexity which restricts the depth of video SR models and
thus undermine the performance. In this paper, we present
a novel fast spatio-temporal residual network (FSTRN) to
adopt 3D convolutions for the video SR task in order to en-
hance the performance while maintaining a low computa-
tional load. Speciﬁcally, we propose a fast spatio-temporal
residual block (FRB) that divide each 3D ﬁlter to the prod-
uct of two 3D ﬁlters, which have considerably lower di-
mensions. Furthermore, we design a cross-space residual
learning that directly links the low-resolution space and
the high-resolution space, which can greatly relieve the
computational burden on the feature fusion and up-scaling
parts. Extensive evaluations and comparisons on bench-
mark datasets validate the strengths of the proposed ap-
proach and demonstrate that the proposed network signif-
icantly outperforms the current state-of-the-art methods.

1. Introduction

Super-resolution (SR) addresses the problem of estimat-
ing a high-resolution (HR) image or video from its low-
resolution (LR) counterpart. SR is wildly used in various
computer vision tasks, such as satellite imaging [4] and
surveillance imaging [17]. Recently, deep learning based
methods have been a promising approach to solve SR prob-

∗Corresponding author.

k x k

Conv2d

ReLU

k x k

Conv2d

ReLU

k x k x k

Conv3d

PReLU

1 x k x k

Conv3d

k x 1 x 1

Conv3d

Addition

Addition

Addition

(a) EDSR

(b) 3D residual block

(c) Proposed FRB

Figure 1: Comparison of (a) residual block in EDSR[27],
(b) single C3D residual block, and (c) the proposed FRB.

lem [5, 20, 27, 29, 30, 45]. A straight idea for video SR
is to perform single image SR frame by frame. However,
it ignores the temporal correlations among frames, the out-
put HR videos usually lack the temporal consistency, which
may emerge as spurious ﬂickering artifacts [33].

Most existing methods for the video SR task utilize the
temporal fusion techniques to extract the temporal infor-
mation in the data, such as motion compensation [3, 39],
which usually need manually deigned structure and much
more computational consumption. To automatically and
simultaneously exploit the spatial and temporal informa-
tion, it is natural to employ 3-dimensional (3D) ﬁlters to

10522

Figure 2: Visually observations on the orginal frames and the SR results on the Dancing video at ×4 SR, it is noticeable that
the proposed FSTRN approach not only achieves the highest PSNR and SSIM values, but also restores the ﬁnest texture with
the fewest artifacts.

replace 2-dimensional (2D) ﬁlters. However, the additional
dimension would bring much more parameters and lead to
an excessively heavy computational complexity. This phe-
nomenon severely restricts the depths of the neural network
adopted in the video SR methods and thus undermine the
performance [15].

Since there are considerable similarities between the
input LR videos and the desired HR videos, the resid-
ual connection is widely involved in various SR networks
[20, 25, 27], fully demonstrating the residual connection
advantages. However, the residual identity mapping for SR
task are beyond sufﬁcient usage, it is either applied on HR
space [20, 37], largely increasing the computational com-
plexity of the network, or applied on the LR space to fully
retain the information from the original LR inputs [47], im-
posing heavy burdens on the feature fusion and upscaling
stage at the ﬁnal part of networks.

To address these problems, we propose fast spatio-
temporal residual network (FSTRN) (Fig. 3) for video SR.
It’s difﬁcult and impractical to build a very deep spatio-
temporal network directly using original 3D convolution
(C3D) due to high computational complexity and memory
limitations. So we propose fast spatio-temporal residual
block (FRB) (Fig. 1c) as the building module for FSTRN,
which consists of skip connection and spatio-temporal fac-
torized C3Ds. The FRB can greatly reduce computational
complexity, giving the network the ability to learn spatio-
temporal features simultaneously while guaranteeing com-
putational efﬁciency. Also, global residual learning (GRL)
are introduced to utilize the similarities between the input
LR videos and the desired HR videos. On the one hand,

we adopt to use LR space residual learning (LRL) in order
to boost the feature extraction performance. On the other
hand, we further propose a cross-space residual connection
(CRL) to link the LR space and HR space directly. Through
CRL, LR videos are employed as an “anchor” to retain the
spatial information in the output HR videos.

Theoretical analyses of the proposed method provide a

generalization bound O(1/√n) with no explicitly depen-

dence on the network size (n is the sample size), which
guarantees the feasibility of our algorithm on unseen data.
Thorough empirical studies on benchmark datasets evalu-
ation validate the superiority of the proposed FSTRN over
existing algorithms.

In summary, the main contributions of this paper are

threefold:

• We propose a novel framework fast spatio-temporal
residual network (FSTRN) for high-quality video SR.
The network can exploit spatial and temporal informa-
tion simultaneously. By this way, we retain the tempo-
ral consistency and ease the problem of spurious ﬂick-
ering artifacts.

• We propose a novel fast spatio-temporal residual block
(FRB), which divides each 3D ﬁlter to the product of
two 3D ﬁlters which have signiﬁcantly lower dimen-
sions. By this way, we signiﬁcantly reduce the com-
puting load while enhance the performance through
deeper neural network architectures.

• We propose to employ global residual learning (GRL)
which consist of LR space residual learning (LRL) and

10523

HR/PSNR/SSIMBicubic/26.62/0.82SRCNN/27.89/0.85SRGAN/27.09/0.82RDN/27.06/0.81BRCN/28.05/0.86VESPCN/27.55/0.84FSTRN/28.59/0.88cross-space residual learning (CRL) to utilize the con-
siderable similarity between the input LR videos and
the output HR videos, which signiﬁcantly improve the
performance.

2. Related work

2.1. Single image SR with CNNs

In recent years, convolutional neural networks (CNNs)
have achieved signiﬁcant success in many computer vision
tasks [13, 23, 24, 34, 36], including the super-resolution
(SR) problem. Dong et al. pioneered a three layer deep
fully convolutional network known as the super-resolution
convolutional neural network (SRCNN) to learn the non-
linear mapping between LR and HR images in the end-to-
end manner [5, 6]. Since then, many research has been
presented, which are usually based on deeper network and
more advanced techniques.

As the network deepens, residual connections have been
a promising approach to relieve the optimization difﬁculty
for deep neural networks [13]. Combining residual learn-
ing, Kim et al. propose a very deep convolutional network
[20] and a deeply-recursive convolutional network (DRCN)
[21]. These two models signiﬁcantly boost the perfor-
mance, which demonstrate the potentials of the residual
learning in the SR task. Tai et al. present a deep recur-
sive residual network (DRRN) with recursive blocks and
a deep densely connected network with memory blocks
[37], which further demonstrates the superior performance
of residual learning.

All the above methods work on interpolated upscaled in-
put images. However, directly feeding interpolated images
into neural networks can result in a signiﬁcantly high com-
putational complexity. To address this problem, an efﬁcient
sub-pixel convolutional layer [33] and transposed convolu-
tional layer [7] are proposed in order to upscale the feature
maps to a ﬁne resolution at the end of the network.

Other methods employing residual connections include
EDSR [27], SRResNet [25], SRDenseNet [42] to RDN
[47]. However, residual connections are limited within the
LR space. These residuals can enhance the performance of
feature extraction but would put a excessively heavy load
on the up-scaling and fusion parts of the network.

2.2. Video SR with CNNs

Based on image SR methods and further to grasp the
temporal consistency, most existing methods employ a
sliding frames window [3, 18, 19, 26, 39]. To handle
spatio-temporal information simultaneously, existing meth-
ods usually utilize temporal fusion techniques, such as
motion compensation [3, 19, 26, 39], bidirectional recur-
rent convolutional networks (BRCN) [14], long short-term

memory networks (LSTM) [10]. Sajjadi et al. use a dif-
ferent way by using a frame-recurrent approach where the
previous estimated SR frames are also redirected into the
network, which encourages more temporally consistent re-
sults [32].

A more natural approach to learn spatio-temporal in-
formation is to employ 3D convolutions (C3D), which has
shown superior performances in video learning [16, 43, 44].
Caballero et al. [3] mentioned the slow fusion can also be
seen as C3D. In addition, Huang et al. [15] improved BRCN
using C3D, allowing the model to ﬂexibly obtain access to
varying temporal contexts in a natural way, but the network
is still shallow. In this work, we aimed to build a deep end-
to-end video SR network with C3D and maintain high efﬁ-
ciency of computational complexity.

3. Fast spatio-temporal residual network

3.1. Network structure

In this section, we describe the structure details of the
proposed fast spatio-temporal residual network (FSTRN).
As shown in Fig.
3, FSTRN mainly consists of four
parts: LR video shallow feature extraction net (LFENet),
fast spatio-temporal residual blocks (FRBs), LR feature fu-
sion and up-sampling SR net (LSRNet), and global residual
learning (GRL) part composing by LR space residual learn-
ing (LRL) and cross-space residual learning (CRL).

LFENet simply uses a C3D layer to extract features
from the LR videos. Let’s denote the input and output of
the FSTRN as ILR and ISR and the target output IHR, the
LFENet can be represented as:

F L

0 = HLF E (ILR) ,

(3.1)

0

where F L
HLF E (·) denotes C3D operation in the LFENet. F L

is the output of extracted feature-maps, and
0 is
then used for later LR space global residual learning and
also used as input to FRBs for further feature extraction.

FRBs are used to extract spatio-temporal features on the
LFENet output. Assuming that D of FRBs are used, the ﬁrst
FRB performs on the LFENet output, and the subsequent
FRB further extract features on the previous FRB output, so
the output F L

d of the d-th FRB can be expressed as:

F L

d = HF RB,d(cid:0)F L
d−1(cid:1)
= HF RB,d(cid:0)HF RB,d−1(cid:0)···(cid:0)HF RB,1(cid:0)F L

(3.2)
where HF RB,d denotes the operations of the d-th FRB,
more details about the FRB will be shown in Section 3.2.

0 (cid:1)(cid:1)···(cid:1)(cid:1) ,

Along with the FRBs, LR space residual learning (LRL)
is conducted to further improve feature learning in LR
space. LRL makes fully use of feature from the preceding
layers and can be obtained by

F L

LRL = HLRL(cid:0)F L

D, F L

0 (cid:1) ,

(3.3)

10524

Cross-space Residual Connection

LR Residual Connection

SR Mapping

R
L

𝐼(cid:3013)(cid:3019)

v
n
o
C

𝐹(cid:2868)(cid:3013)

U
L
e
R
P

B
v
n
R
o
F
C

n
o
C

v 𝐹(cid:2869)(cid:3013)

U
L
e
R
P

B
v
n
R
o
F
C

v
n
o
C

𝐹(cid:3031)(cid:3013)

U
L
e
R
P

B
v
n
R
o
F
C

v
n
o
C

𝐹(cid:3005)(cid:3013)

U
L
e
R
P

u
o
p
o
r
D

t𝐹(cid:3013)(cid:3019)(cid:3013)(cid:3013)

v
n
o
C

e
l
a
c
s
p
U

v
n
o
C

𝐹(cid:3020)(cid:3019)(cid:3013) 𝐹(cid:3020)(cid:3019)(cid:3009)𝐼(cid:3009)(cid:3019)

R
H

Figure 3: The architecture of our proposed fast spatio-temporal residual network (FSTRN).

where F L
LRL is the output feature-maps of LRL by utilizing
a composite function HLRL. More details will be presented
in Section 3.3.

LSRNet is applied to obtain super-resolved video in HR
space after the efﬁcient feature extraction of LRL. Speciﬁ-
cally, we use a C3D for feature fusion followed by a decon-
volution [8] for upscaling and again a C3D for feature-map
channels tuning in the LSRNet. The output F L
SR can be for-
mulated as:

(3.4)

F L

SR = HLSR(cid:0)F L

LRL(cid:1) ,

where HLSR (·) denotes the operations of LSRNet.
At last, the network output is composed of the F L
SR from
the LSRNet and an additional LR to HR space global resid-
ual, forming a cross-space residual learning (CRL) in HR
space. The detail of the CRL is also given in Section 3.3.
So denote a SR mapping of input from LR space to HR
space be F H

SR, the output of FSTRN can be obtained as

ISR = HF ST RN (ILR) = F L

SR + F H

SR,

(3.5)

where HF ST RN represents the function of the proposed
FSTRN method.

accompanied by more computations. To solve this, we pro-
pose a novel fast spatio-temporal residual block (FRB) by
factorizing the C3D on the above single 3D residual block
into two step spatio-temporal C3Ds, i.e., we replace the in-
ﬂated k×k×k cubic ﬁlter with a 1×k×k ﬁlter followed by
a k × 1 × 1 ﬁlter, which has been proven to perform better,
in both training and test loss [44, 46], as shown in Figure
1c. Also, we change the rectiﬁed linear unit (ReLU) [9] to
its variant PReLU, in which the slopes of the negative part
are learned from the data rather than predeﬁned [12]. So the
FRB can be formulated as:

(3.6)

F L
d = F L

d−1 + Wd,t(cid:0)Wd,s(cid:0)σ(cid:0)F L

d−1(cid:1)(cid:1)(cid:1) ,

where σ denoted the PReLU [12] activation function. Wd,s
and Wd,t correspond to weights of the spatial convolution
and the temporal convolution in FRB, respectively, where
the bias term is not shown. In this way, the computational
cost can be greatly reduced, which will be shown in Sec-
tion 5.2. Consequently, we can build a larger, C3D-based
model to directly video SR under limited computing re-
sources with better performance.

3.2. Fast spatio temporal residual blocks

3.3. Global residual learning

Now we present details about the proposed fast spatio-

temporal residual block (FRB), which is shown in Fig. 1.

Residual blocks have been proven to show excellent per-
formances in computer vision, especially in the low-level to
high-level tasks [20, 25]. Lim et al. [27] proposed a mod-
iﬁed residual block by removing the batch normalization
layers from the residual block in SRResNet, as shown in
Figure 1a, which showed a great improvement in single-
image SR tasks. To apply residual blocks to multi-frame
SR, we simply reserve only one convolutional layer, but in-
ﬂate the 2D ﬁlter to 3D, which is similar to [16]. As shown
in Figure 1b, the k × k square ﬁlter is expanded into a
k × k × k cubic ﬁlter, endowing the residual block with
an additional temporal dimension.
After the inﬂation, the ensuing problems are obvious, in
that it takes much more parameters than 2D convolution,

In this section, we describe the proposed global residual
learning (GRL) on both LR and HR space. For SR tasks,
input and output are highly correlated, so the residual con-
nection between the input and output is wildly employed.
However, previous works either perform residual learning
on ampliﬁed inputs, which would lead to high computa-
tional costs, or perform residual connection directly on the
input-output LR space, followed by upscaling layers for fea-
ture fusion and upsamping, which puts a lot of pressure on
these layers.

To address these problems, we come up with global
residual learning (GRL) on both LR and HR space, which
mainly consists of two parts: LR space residual learning
(LRL) and cross-space residual learning (CRL).

LR space residual learning (LRL) is introduced along
with the FRBs in LR space. We apply a residual connection

10525

with a followed parametric rectiﬁed linear unit (PReLU)
[12] for it. Considering the high similarities between in-
put frames, we also introduced a dropout [35] layer to en-
hance the generalization ability of the network. So the out-
put F L

LRL of LRL can be obtained by:

F L

LRL = HLRL(cid:0)F L

D, F L

0 (cid:1) = σL(cid:0)F L

D + F L

0 (cid:1) ,

where σL denoted the combination function of PReLU ac-
tivation and dropout layer.

(3.7)

Cross-space residual learning (CRL) uses a simple SR
mapping to directly map the LR video to HR space, and then
adds to the LSRNet result F L
SR, forming a global residual
learning in HR space. Speciﬁcally, CRL introduces a in-
terpolated LR to the output, which can greatly alleviate the
burden on the LSRNet, helping improve the SR results. The
LR mapping to HR space can be represented as:

F H

SR = HCRL (ILR) ,

(3.8)

where F H
SR is a super-resolved input mapping on HR space.
HCRL denotes the operations of the mapping function. The
mapping function is selected to be as simple as possible so
as not to introduce too much additional computational cost,
including bilinear, nearest, bicubic, area, and deconvolution
based interpolations.

The effectiveness of GRL and the selection of SR map-

ping method is demonstrated in Section 5.3.

3.4. Network learning

In training, we use l1 loss function for training. To deal
with the l1 norm, we use the Charbonnier penalty function
ρ (x) = √x2 + ε2 for the approximation.

Let θ be the parameters of network to be optimized, ISR
be the network outputs. Then the objective function is de-
ﬁned as:

L (ISR, IHR; θ) =

1
N

N

Xn=1

ρ (I n

HR − I n

SR)

(3.9)

where N is the batch size of each training. Here we em-
pirically set ε = 1e − 3. Note that although the network
produces the same frames as the input, we focus on the re-
construction of the center frame from the input frames in
this work. As a result, our loss function is mainly related to
the center frame of the input frames.

(covering bound) of the hypothesis space H induced by
FSTRN. This covering bound constrain the complexity of
FSTRN. Then we obtain an O(cid:16)q 1
n(cid:17) upper bound for the

generalization error (generalization bound) of FSTRN. This
generalization bound gives a theoretical guarantee to our
proposed algorithms.

As Fig. 1c shows, FRB is obtained by adding an identity
mapping to a chain-like neural network with one PReLU
and two convolutional layers. Bartlett et al. proves that most
standard nonlinearities are Lipschitz-continuous (including
PReLU) [1]. Suppose the afﬁne transformations introduced
by the two convolutional operators can be respectively ex-
pressed by weight matrices Ai
2. Expect all FRBs,
from the input end of the stem to the output end, there are
1 convolutional layer, 1 PReLU, 1 upscale, and 1 convo-
lutional layer (we don’t consider dropout here). They can
be respectively expressed by weight matrix A1, nonlinear-
ity σ1, weight matrix A2, and weight matrix A3. As Fig.
3 shows, LR residual learning is an identity mapping and
HR residual learning can be expressed by a weight matrix
AHR. We can further obtain an upper bound for the hypoth-
esis space induced by FSTRN as follows.

1 and Ai

1 and kAi

1kσ ≤ si
2kσ ≤ si
2, which are satisﬁed that kAi

Theorem 1 (Covering bound for FSTRN). For the i-th FRB
(i = 1, . . . , D), suppose the Lipschitz constant of the PReLU
is ρi, and the spectral norm of the weight matrices are
bounded: kAi
2. Also, suppose
there are two reference matrices M i
2 respectively
for Ai
1 and Ai
ikσ ≤ bi
i,
i = 1, 2. Similarly, suppose the spectral norm of weight
matrices A1, A2, A3, and AHR are respectively upper
bounded by s1, s2, s3, and sHR. Also, there are 4 corre-
sponding reference matrices Mi, i ∈ {1, 2, 3, HR} such
that kAi − Mik ≤ bi. Meanwhile, suppose the Lipschitz
constant of nonlinearity σ1 is ρ1. Then, the ε-covering num-
ber satisﬁes that

i − M i

1 and M i

N (H) ≤

1kXk2
b2
2 ¯α
ε2

D

Xd=1
log(cid:0)2W 2(cid:1) +
log(cid:0)2W 2(cid:1)"(cid:18) b2
ε2(cid:19)2
log(cid:0)2W 2(cid:1) ,

2

2

b2
+ (∗)
ε2
b2
HRkXk2

+

2

ε2

NF RB(d)
+(cid:18) s2b3

ε3 (cid:19)2#

4. Theoretical analysis

where

In learning theory, we usually use generalization error to
express the generalization capability of an algorithm, which
is deﬁned as the difference between the expected risk R and
the empirical risk ˆR of the algorithm. In this section, we
study the generalization ability of FSTRN. Speciﬁcally, we
ﬁrst give an upper bound for the covering number N (H)

10526

2 d

εd

NF RB(d) =(cid:18)kXk2s1ρd
(cid:19)
(cid:0)1 + sd
2(cid:1)

h(cid:0)bd
1(cid:1)

2

2

1si

Yi=1h(cid:0)ρisi
+(cid:0)bd
1(cid:1)

2sd

2

2(cid:1)
2i ,

(4.1)

+ 1i

(4.2)

(∗) = (kXk2s1ρ1)2

εd =

ε − sHR − 1

¯α

ε2 =

ε − sHR − 1

¯α

ρ1(1 + s2) + sHR + 1,

D

d

Yd=1h(cid:0)ρdsd
Yi=1(cid:2)ρi(1 + si
( D
Yi=1(cid:2)ρi(1 + si

2

1sd

2(cid:1)

1)(1 + si

1)(1 + si

(4.4)

(4.3)

+ 1i ,
2) + 1(cid:3) ,
2) + 1(cid:3) + 1)

(4.5)

1)(1 + sj

ρ1(1 + s2), (4.6)

and

¯α =


D

Yj=1hρj(1 + sj

2) + 1i


A detailed proof is omitted here and given in the ap-
pendix based on [2, 11]. Finally, we can obtain the fol-
lowing theorem. For the brevity, we denote the right-hand
side (RHS) of eq. (4.1) as R
ε .

Theorem 2 (Generalization Bound for FSTRN). For any
real δ ∈ (0, 1), with probability at least 1− δ, the following
inequality holds for any hypothesis Fθ:

R(Fθ)
≤ ˆR(Fθ) +

8

3
2

N

+

36
N

√R log N + 3r log(2/δ)

2N

.

(4.7)

Theorem 2 can be obtained from Theorem 1. A de-
tailed proof is given in the appendix. Eq. (4.7) gives an

O(cid:16)1/√N(cid:17) generalization bound for our proposed algo-

rithm FSTRN. Another strength of our result is that all fac-
tors involved do not explicitly rely on the size of our neural
network, which could be extremely large. This strength can
prevent the proposed result from meaninglessness. Overall,
this result theoretically guarantees the feasibility and gener-
alization ability of our method.

5. Experiments

In this section, we ﬁrst analyze the contributions of the
network and then present the experimental results obtained
to demonstrate the effectiveness of the proposed model on
benchmark datasets quantitatively and qualitatively.

5.1. Settings

Datasets and metrics. For a fair comparison with exist-
ing works, we used 25 YUV format benchmark video se-
quences as our training sets, which have been previously
used in [14, 15, 28, 31, 38]. We tested the proposed model
on the benchmark challenging videos same as [14] with the
same settings, including the Dancing, Flag, Fan, Treadmill

and Turbine videos, which contain complex motions with
severe motion blur and aliasing. Following [5, 41], SR was
only applied on the luminance channel (the Y channel in
YCbCr color space), and performances were evaluated with
the peak signal-to-noise ratio (PSNR) and structural simi-
larity (SSIM) on the luminance channel.

Training settings. Data augmentation was performed on
the 25 YUV video sequences dataset. Following [14, 15], to
enlarge the training set, we trained the model in a volume-
based way by cropping multiple overlapping volumes from
the training videos. During the cropping, we took a large
spatial size as 144 × 144 and the temporal step as 5, and
the spatial and temporal strides were set as 32 and 10, re-
spectively. Furthermore, inspired by [40], the ﬂipped and
transposed versions of the training volumes were consid-
ered. Speciﬁcally, we rotated the original images by 90◦
and ﬂipped them horizontally and vertically. As a result,
we could generate 13020 volumes from the original video
dataset. After this, both of the training and testing LR inputs
generating processes are divided into two stages: smoothing
each original frame by a Gaussian ﬁlter with a standard de-
viation of 2, and downsampling the preceding frames using
the bicubic method. In addition, to maintain the number of
output frames equal to original video in the test stage, frame
padding was applied at the test videos head and tail.

In these experiments, we focused on video SR of upscale
factor 4, which is usually considered the most challenging
and universal case in video SR. The number of FRBs and
the dropout rate were empirically set to be 5 and 0.3. The
Adam optimizer [22] was used to minimize the loss function
with standard back-propagation. We started with a step size
of 1e − 4 and then reduced it by a factor of 10 when the
training loss stopped going down. The batch size was set
depending on the GPU memory size.

Blocks

C3DRB

FRB

Reduce ratio

#Params
#FLOPs
∼ 111K ∼ 566M
∼ 252M
∼ 49K
55.86% 55.48%

Table 1: #Params and #FLOPs comparisons of one residual
block using single C3D (Fig. 1b) and one FRB (Fig. 1c).

5.2. Study of FRB

In this section, we investigate the effect of the proposed
FRB on efﬁciency. We analyze the computational efﬁciency
of the FRB compared to the residual block built directly us-
ing C3D (C3DRB). Supposing we have all input and output
feature-map size of 64, each input consists 5 frames with
the size 32 × 32, then a detail params and ﬂoating-point
operations (FLOPs) comparison of the proposed FRB and

10527

Methods

Bicubic

SRCNN[5]
SRGAN[25]

RDN[47]
BRCN[14]
VESPCN[3]
FSTRN(ours)

Dancing

Treadmill

Flag

Fan

Turbine

Average

PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM

26.78 / 0.83
27.91 / 0.87
27.11 / 0.84
27.51 / 0.82
28.08 / 0.88
27.89 / 0.86
28.66 / 0.89

21.58 / 0.65
22.61 / 0.73
22.40 / 0.72
22.69 / 0.72
22.67 / 0.74
22.46 / 0.74
23.06 / 0.76

26.97 / 0.78
28.71 / 0.83
28.19 / 0.83
28.62 / 0.82
28.86 / 0.84
29.01 / 0.85
29.81 / 0.88

33.42 / 0.93
34.25 / 0.94
33.48 / 0.93
34.46 / 0.93
34.15 / 0.94
34.40 / 0.94
34.79 / 0.95

26.06 / 0.76
27.84 / 0.81
27.38 / 0.81
28.10 / 0.82
27.63 / 0.82
28.19 / 0.83
28.57 / 0.84

27.80 / 0.80
29.20 / 0.84
28.65 / 0.84
29.30 / 0.84
29.16 / 0.85
29.40 / 0.85
29.95 / 0.87

Table 2: Comparison of the PSNR and SSIM results for the test video sequences by Bicubic, SRCNN[5], SRGAN[25],
RDN[47], BRCN[14], VESPCN[3], and our FSTRN with scale factor 4.

the C3DRB are summarized in Table 1. It’s obvious to see
that the FRB can greatly reduce parameters and calculations
by more than half amount. In this way, the computational
cost can be greatly reduced, so we can build a larger, C3D-
based model to directly video SR under limited computing
resources with better performance.

5.3. Ablation investigations

We conducted ablation investigation to analyze the con-
tributions of FRBs and GRL with different degradation
models in this section. Fig. 4a shows the convergence
curves of the degradation models, including: 1) the baseline
obtained without FRB, CRL and LRL (FSTRN F0C0L0);
2) baseline integrated with FRBs (FSTRN F1C0L0); 3)
baseline with FRBs and LRL (FSTRN F1C0L1); 4)
baseline with all components of FRBs, CRL and LRL
(FSTRN F1C1L1), which is our FSTRN. The number D
of FRBs was set to 5, and CRL uses bilinear interpolation.
The baseline converges slowly and performs relatively
poor (green curve), and the additional FRBs greatly im-
prove the performance (blue curve), which can be due to
the efﬁcient inter-frame features capture capabilities. As
expected, LRL further improved network performance (ma-
genta curve). Finally, the addition of CRL was applied (red
curve), constituted GRL on both LR and HR space. It can
be clearly seen that the network performed faster conver-
gence speed and better performance, which demonstrated
the effectiveness and superior ability of FRB and GRL.

Furthermore, to show how different interpolation meth-
ods in CRL affect the network performance, we investi-
gated different interpolation method for CRL. Speciﬁcally,
we explored bilinear, nearest, bicubic, area and deconvolu-
tion based interpolations. As shown in Fig. 4b, different
interpolation method except deconvolution behaved almost
the same, reason for this is because the deconvolution needs
a process to learn the upsampling ﬁlers, while other meth-
ods do not need. All the different interpolation method con-
verged to almost the same performance, indicated that the
performance improvement of FSTRN is attributed to the in-

30

29

)

B
d
(
 

R
N
S
P

28

27

26

25

0

Ablation Investigation of FRB, CRL and LRL

FSTRN_F0C0L0
FSTRN_F1C0L0
FSTRN_F1C0L1
FSTRN_F1C1L1

150

200

50

100

Epoch

(a)

30

29

)

B
d
(
 

R
N
S
P

28

27

26

25

0

Ablation Investigation of different CRL

BILINEAR
NEAREST
BICUBIC
AREA
DECONV

150

200

50

100

Epoch

(b)

Figure 4: Convergence analysis on different degradation
models (a) and different interpolation method for CRL (b).
The curves for each combination are based on the PSNR on
test video with scaling factor ×4 in 200 epochs.

troduction of GRL, and has little to do with speciﬁc inter-
polation method in CRL.

5.4. Comparisons with state of the art

We compared the proposed method with different single-
image SR methods and state-of-the-art multi-frame SR
methods, both quantitatively and qualitatively, including
Bicubic interpolation, SRCNN [5, 6], SRGAN [25], RDN
[47], BRCN [14, 15] and VESPCN [3]. The number D of
FRBs was set to 5 in following comparisons and the upscale
method of CRL was set to bilinear interpolation.

The quantitative results of all the methods are summa-
rized in Table 2, where the evaluation measures are the
PSNR and SSIM indices. Speciﬁcally, compared with the
state-of-the-art SR methods, the proposed FSTRN shows
signiﬁcant improvement, surpassing them 0.55 dB and 0.2
on average PSNR and SSIM respectively.

In addition to the quantitative evaluation, we present
some qualitative results in terms of single-frame (in Figure
2) and multi-frame (in Figure 5) SR comparisons, showing
visual comparisons between the original frames and the ×4
SR results. It is easy to see that the proposed FSTRN re-
covers the ﬁnest details and produces most pleasing results,
both visually and with regard to the PSNR/SSIM indices.

10528

(a) Original

(b) SRCNN

(c) RDN

(d) BRCN

(e) VESPCN

(f) FSTRN

Figure 5: Comparison between original frames (1st ∼ 5th frames, from the top row to bottom) of the Flag video and the
SR results obtained by SRCNN, RDN, BRCN, VESPCN and FSTRN, respectively. Our results show sharper outputs with
smoother inter-frame transitions compared to other works.

Our results show sharper outputs and even in grid process-
ing, which is recognized as the most difﬁcult to deal in SR,
the FSTRN can handle it very well, showing promising per-
formance.

6. Conclusion

In this paper, we present a novel fast spatio-temporal
residual network (FSTRN) for video SR problem. We also
design a new fast spatio-temporal residual block (FRB) to
extract spatio-temporal features simultaneously while as-
suring high computational efﬁciency. Besides the residuals
used on the LR space to enhance the feature extraction per-
formance, we further propose a cross-space residual learn-
ing to exploit the similarities between the low-resolution
(LR) input and the high-resolution (HR) output. Theoreti-
cal analysis provides guarantee on the generalization ability,
and empirical results validate the strengths of the proposed
approach and demonstrate that the proposed network signif-
icantly outperforms the current state-of-the-art SR methods.

7. Acknowledgements

This work was supported in part by the National Natu-
ral Science Foundation of China under Grants 61822113,
41871243, 41431175, 61771349,
the National Key R
& D Program of China under Grant 2018YFA0605501,
Australian Research Council Projects FL-170100117, DP-
180103424, IH-180100002 and the Natural Science Foun-
dation of Hubei Province under 2018CFA050.

References

[1] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky.
Spectrally-normalized margin bounds for neural networks.
In NIPS, pages 6240–6249, 2017.

[2] Peter L Bartlett and Shahar Mendelson. Rademacher and
gaussian complexities: Risk bounds and structural results.
JMLR, 3(Nov):463–482, 2002.

[3] Jose Caballero, Christian Ledig, Andrew P. Aitken, Alejan-
dro Acosta, Johannes Totz, Zehan Wang, and Wenzhe Shi.
Real-time video super-resolution with spatio-temporal net-

10529

works and motion compensation.
2857, 2017.

In CVPR, pages 2848–

[4] Liujuan Cao, Rongrong Ji, Cheng Wang, and Jonathan Li.
Towards domain adaptive vehicle detection in satellite im-
age by supervised super-resolution transfer. In AAAI, pages
1138–1144, 2016.

[5] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Learning a deep convolutional network for image
In David J. Fleet, Tom´as Pajdla, Bernt
super-resolution.
Schiele, and Tinne Tuytelaars, editors, ECCV, volume 8692
of Lecture Notes in Computer Science, pages 184–199.
Springer, 2014.

[6] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(2):295–307, Feb 2016.

[7] Chao Dong, Chen Change Loy, and Xiaoou Tang. Acceler-
ating the super-resolution convolutional neural network. In
ECCV, pages 391–407, 2016.

[8] Vincent Dumoulin and Francesco Visin. A guide to convo-
lution arithmetic for deep learning. CoRR, abs/1603.07285,
2016.

[9] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep
sparse rectiﬁer neural networks.
In Geoffrey J. Gor-
don, David B. Dunson, and Miroslav Dud´ık, editors, AIS-
TATS, volume 15 of JMLR Proceedings, pages 315–323.
JMLR.org, 2011.

[10] Jun Guo and Hongyang Chao. Building an end-to-end
spatial-temporal convolutional network for video super-
resolution. In Satinder P. Singh and Shaul Markovitch, edi-
tors, AAAI, pages 4053–4060. AAAI Press, 2017.

[11] Fengxiang He, Tongliang Liu, and Dacheng Tao. Why resnet
works? residuals generalize. CoRR, abs/1904.01367, 2019.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation.
In ICCV, pages 1026–
1034. IEEE Computer Society, 2015.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[14] Yan Huang, Wei Wang, and Liang Wang. Bidirectional
recurrent convolutional networks for multi-frame super-
resolution. In Corinna Cortes, Neil D. Lawrence, Daniel D.
Lee, Masashi Sugiyama, and Roman Garnett, editors, NIPS,
pages 235–243, 2015.

[15] Yan Huang, Wei Wang, and Liang Wang. Video super-
resolution via bidirectional recurrent convolutional net-
works. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 40(4):1015–1028, April 2018.

[16] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-
tional neural networks for human action recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
35(1):221–231, Jan 2013.

[18] Younghyun Jo, Seoung Wug Oh, Jaeyeon Kang, and Seon
Joo Kim. Deep video super-resolution network using dy-
namic upsampling ﬁlters without explicit motion compensa-
tion. In CVPR, pages 3224–3232, 2018.

[19] Armin Kappeler, Seunghwan Yoo, Qiqin Dai, and Agge-
los K. Katsaggelos. Video super-resolution with convolu-
tional neural networks.
IEEE Transactions on Computa-
tional Imaging, 2(2):109–122, June 2016.

[20] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In CVPR, pages 1646–1654, 2016.

[21] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-
recursive convolutional network for image super-resolution.
In CVPR, pages 1637–1645, 2016.

[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. CoRR, abs/1412.6980, 2014.

[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In Peter L. Bartlett, Fernando C. N. Pereira, Christo-
pher J. C. Burges, L´eon Bottou, and Kilian Q. Weinberger,
editors, NIPS, pages 1106–1114, 2012.

[24] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, Nov
1998.

[25] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P. Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a
generative adversarial network.
In CVPR, pages 105–114,
2017.

[26] Renjie Liao, Xin Tao, Ruiyu Li, Ziyang Ma, and Jiaya Jia.
Video super-resolution via deep draft-ensemble learning. In
ICCV, pages 531–539, 2015.

[27] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for single
image super-resolution. In CVPRW, pages 1132–1140. IEEE
Computer Society, 2017.

[28] Ce Liu and Deqing Sun. On bayesian adaptive video super
resolution. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 36(2):346–360, Feb 2014.

[29] Ding Liu, Zhaowen Wang, Yuchen Fan, Xianming Liu,
Zhangyang Wang, Shiyu Chang, Xinchao Wang, and
Thomas S. Huang. Learning temporal dynamics for video
super-resolution: A deep learning approach. IEEE Transac-
tions on Image Processing, 27(7):3432–3445, July 2018.

[30] Ding Liu, Zhaowen Wang, Bihan Wen, Jianchao Yang, Wei
Han, and Thomas S. Huang. Robust single image super-
resolution via deep networks with sparse prior. IEEE Trans-
actions on Image Processing, 25(7):3194–3207, July 2016.

[31] Matan Protter, Michael Elad, Hiroyuki Takeda, and Pey-
man Milanfar. Generalizing the nonlocal-means to super-
resolution reconstruction. IEEE Transactions on Image Pro-
cessing, 18(1):36–51, Jan 2009.

[17] Jiening Jiao, Wei-Shi Zheng, Ancong Wu, Xiatian Zhu,
Deep low-resolution person re-

and Shaogang Gong.
identiﬁcation. In AAAI, 2018.

[32] Mehdi SM Sajjadi, Raviteja Vemulapalli, and Matthew
Brown. Frame-recurrent video super-resolution. In CVPR,
pages 6626–6634, 2018.

10530

[33] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,
Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In
CVPR, pages 1874–1883, 2016.

[34] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[35] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overﬁtting. Journal of Ma-
chine Learning Research, 15(1):1929–1958, 2014.

[36] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, pages 1–9, 2015.

[37] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem-
net: A persistent memory network for image restoration. In
ICCV, pages 4549–4557, 2017.

[38] Hiroyuki Takeda, Peyman Milanfar, Matan Protter, and
Michael Elad. Super-resolution without explicit subpixel
motion estimation. IEEE Transactions on Image Processing,
18(9):1958–1975, Sept 2009.

[39] Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, and Jiaya
Jia. Detail-revealing deep video super-resolution. In ICCV,
pages 4482–4490, 2017.

[40] Radu Timofte, Rasmus Rothe, and Luc Van Gool. Seven
ways to improve example-based single image super resolu-
tion. In CVPR, pages 1865–1873, 2016.

[41] Radu Timofte, Vincent De Smet, and Luc J. Van Gool.
Anchored neighborhood regression for fast example-based
super-resolution.
In ICCV, pages 1920–1927. IEEE Com-
puter Society, 2013.

[42] Tong Tong, Gen Li, Xiejie Liu, and Qinquan Gao.

super-resolution using dense skip connections.
pages 4809–4817, 2017.

Image
In ICCV,

[43] Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torre-
sani, and Manohar Paluri. Learning spatiotemporal features
with 3d convolutional networks. In ICCV, pages 4489–4497,
2015.

[44] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. CoRR, abs/1711.11248,
2017.

[45] Zhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, and
Thomas S. Huang.
Deep networks for image super-
resolution with sparse prior. In ICCV, pages 370–378, 2015.
[46] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and
Kevin Murphy. Rethinking spatiotemporal feature learning
for video understanding. CoRR, abs/1712.04851, 2017.

[47] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image super-resolution.
In CVPR, 2018.

10531

