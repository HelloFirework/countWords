Deep Generative and Discriminative Embeddings for Novelty Detection

Where’s Wally Now?

Philippe Burlina, Neil Joshi, and I-Jeng Wang

Johns Hopkins University Applied Physics Laboratory

{philippe.burlina, neil.joshi,i-jeng.wang}@jhuapl.edu

Abstract

We develop a framework for novelty detection (ND)
methods relying on deep embeddings, either discriminative
or generative, and also propose a novel framework for as-
sessing their performance. While much progress was made
recently in these approaches, it has been accompanied by
certain limitations: most methods were tested on relatively
simple problems (low resolution images / small number of
classes) or involved non-public data; comparative perfor-
mance has often proven inconclusive because of lacking
statistical signiﬁcance; and evaluation has generally been
done on non-canonical problem sets of differing complexity,
making apples-to-apples comparative performance evalua-
tion difﬁcult. This has led to a relative confusing state of af-
fairs. We address these challenges via the following contri-
butions: We make a proposal for a novel framework to mea-
sure the performance of novelty detection methods using a
trade-space demonstrating performance (measured by RO-
CAUC) as a function of problem complexity. We also make
several proposals to formally characterize problem com-
plexity. We conduct experiments with problems of higher
complexity (higher image resolution / number of classes).
To this end we design several canonical datasets built from
CIFAR-10 and ImageNet (IN-125) which we make available
to perform future benchmarks for novelty detection as well
as other related tasks including semantic zero/adaptive shot
and unsupervised learning. Finally, we demonstrate, as one
of the methods in our ND framework, a generative novelty
detection method whose performance exceeds that of all re-
cent best-in-class generative ND methods.

1. Motivation, prior work, and contributions

Motivation Novelty detection (ND) methods

[7, 13–
15, 18, 31, 33, 36, 38] have applications in a wide array of
use cases including in semi/unsupervised learning, lifelong
learning and zero-shot learning [29]. Application examples
include: (1) robotic applications with lifelong and open set

learning abilities, where the ability to perform novelty de-
tection allows a robot to trigger human-machine dialogue
to seek information on novel objects it encounters; (2) per-
forming medical diagnostics for rare diseases (e.g., my-
opathies) – for which prior observations are sparse or un-
available – and using novelty detection to pre-screen pa-
tients and refer them to clinicians; and (3), zero-shot se-
mantic learning leveraging novelty detection to improve
zero-shot classiﬁcation performance by turning a general-
ized / uninformed zero-shot problem into an easier informed
problem (where it is known if the object comes or not
from a novel class) for which higher performance can be
achieved [29, 38].

Prior work Past work in ND goes far and wide, and
started with methods primarily grounded on classical ma-
chine learning (for recent surveys see [14, 31, 33], and more
domain speciﬁc surveys [2, 4, 25]), including approaches
such as [7,13,15]. For instance, [7] used image features and
Support Vector Data Description (SVDD) for ND applied to
hyperspectral imaging. [13] used convolutional sparse mod-
els to characterize novelty. Novel multiscale density estima-
tors working for high dimensional data were developed and
applied to ND in dynamic data in [15]. By contrast, repre-
sentation learning via deep learning (DL) has offered novel
ways to implement ND [18, 25, 36], with methods falling
into two main categories: generative and discriminative.

Recent ND via DL Novelty detection using discrimina-
tive deep learning approaches were developed ﬁrst based
on deep embeddings computed by processing the image
through deep discriminative networks, including deep con-
volutional neural nets (DCNNs), deep belief networks
(DBNs), or recursive neural nets (RNNs). In [18] discrimi-
native embeddings via DBNs and DCNNs were used along
with one class SVM (1CSVM) to detect novelty. Most
recently, generative approaches, principally via generative
adversarial networks (GANs) or variational autoencoders
(VAEs) have been embraced for representation learning for
novelty detection [1, 3, 8, 16, 20, 21, 24, 26, 28, 32, 36, 41].
Most approaches focus on GANs’ ability to offer varied
means for embedding (e.g., in latent space, using the en-

111507

Figure 1. ND framework We propose a novelty detection (ND) framework along with a principled approach for evaluating ND methods
which computes performance as (ROCAUC)=f(problem complexity). Our ND framework performs novelty detection in two main steps,
by a) embedding the image either via generative or discriminative networks, then b) computing a novelty score. Speciﬁcally: the left
block shows our use of two types of discriminative embeddings (here via VGG16) using either GAP features XGAP 512 computed from the
output of the convolutional layers (marked as ConvNet) or the concatenation of all features from the ConvNet layers as well as the fully
connected layers (marked as MLP), producing an embedding vector denoted as XM U LT I . An alternative in our framework consists of
using a generative embedding: using the latent space vector of a GAN (here we use Ganomaly [3]), denoted as XGAN . Embedded feature
vectors are then used to compute a novelty score via one of several approaches: either one class SVM (1CSVM), local outlier factor (LOF),
elliptic envelope (EE) or isolation forest (IF). The Ganomaly architecture uses as a discriminator a series of encoder (DE1) decoder (DD)
and encoder (DE2). In our framework we directly leverage the latent vector XGAN output of DE1 as embedding for ND.

Discriminative embedding

Generative embedding

ConvNet MLP

XGAP512

Input image

concatenate

XMULTI

either

Novelty score 

(LOF, OCSVM, EE, 

or IF

G

DE1

DD

DE2

D

XGAN

Novelty 0/1?

coder or decoder network for embeddings, or using the dis-
criminator’s output, or other more complex ways) where
novelty scores can then be computed. Looking at the core
contributions: AnoGAN in [36] used DCGAN [34], and
compared several GAN embedding and novelty measure
approaches that included inverting the mapping from im-
age to latent space, and measuring reconstruction error. It
demonstrated improvements in area under the curve (AUC)
when compared to simply using the GAN discriminator
for novelty detection.
It did so in experiments perform-
ing pixel-based anomaly detection on OCT images, using
49 OCT images and 64 × 64 sliding windows for pixel
ND. A related method, ADGAN, was applied to whole im-
age anomaly detection [16] and tested on images includ-
ing MNIST (28 × 28 images) and CIFAR-10 (32 × 32)),
showing some improvement over past GAN-based meth-
ods. [41] proposed a related approach using a more efﬁ-
cient GAN implementation and a modiﬁed loss function,
and tested it on network intrusion data and a simple image
dataset. Recently, [3] developed an approach using GANs
where the discriminator network structure was composed
of an encoder/decoder/encoder path, providing two latent
embedded spaces, and the novelty score was measured via
the reconstruction error between two latent spaces. This re-
sulted in best in class performance when compared to all
aforementioned methods on CIFAR-10 and MNIST.

Challenges Given the explosion of methods relying on
deep embeddings for ND, one would hope to bring some or-
der and address challenges in interpreting what constitutes
actual progress in performance and what led to performance
improvement. Some challenges stem from the difﬁculty in

making apples-to-apples comparisons due to the lack of re-
peated testing protocols. For instance, even when the same
dataset is used, say MNIST, choosing different schemes for
partitioning inlier and outlier classes results in large differ-
ences in difﬁculty for the resulting novelty detection prob-
lems. Quantitatively measuring ND problem complexity
has never before been addressed, and is itself a complex
challenge. Finally, most past studies have used rather sim-
ple problems with small number of classes and low resolu-
tion images, making it hard to predict how these methods
would generalize in more complex, in-the-wild situations.

Contributions of this work

We address ND, deﬁned as the problem of training on
inlier data not corrupted by outliers, and making inferences
on new observations to detect outliers. To address the ND
challenge we make the following contributions: using a
simple taxonomy of methods, we develop a framework for
ND methods including both discriminative and generative
embeddings, coupled with various approaches for measur-
ing novelty. We make a proposal for a principled method
for evaluating the performance trade space of ND methods,
that expresses computed performance (AUC) as a function
of measured problem complexity. We propose and discuss
several methods for quantitatively assessing problem com-
plexity based on semantic, information theoretic, and Bayes
error based approaches. We propose, release, and test on
canonical datasets and protocols for ND assessment, based
on CIFAR-10 and ImageNet with higher image resolution
and number of classes. Finally, we demonstrate that one
of our ND generative methods exceeds performance when
compared to all prior generative methods reported thus far.

211508

2. Methods

We describe next the main approaches used herein in
our ND framework using generative or discriminative deep
embeddings. The family of algorithms we consider uses
the following pipeline: a) computation of deep embedding,
via discriminative methods, or using GANs for generative
methods (Section 2.1) leading to an embedded vector X of
an input image I; b) PCA dimensionality reduction and nor-
malization of X, and then followed by c) measuring novelty
scores (Section 2.2). As novelty detection is trained on a set
of inlier classes only, and testing is carried out on a set of
images from inliers and outlier (yet unseen) classes, step (c)
broadly consists of using training exemplars to character-
ize some notion of “distance” computed from a test image
to training inlier exemplars, which is then translated into a
novelty score. This framework and its various subcompo-
nents are illustrated in Figure 1.

2.1. Discriminative and generative embeddings for

ND

Discriminative embeddings
In this work we start by
computing deep embedding on the image using a pre-
trained DCNN, here using VGG-16 [35, 37]. The structure
of VGG-16 is recalled here only for convenience: it is ﬁrst
composed of a series of convolutional blocks:

C 1

(2,64) → C 2

(2,128) → C 3

(3,256) → C 4

(3,512) → C 5

(3,512) → F

where a block named C i
(nl,nd) denotes the ith block com-
posed of nl successive convolutional layers of size 3 × 3
with depth nd, each of which is followed by rectiﬁed linear
units (ReLU) activation, and where each such block is fol-
lowed by a pooling layer. This is then followed by ﬂattening
F and then fed to three successive fully connected layers on
a vector of width 4096:

F C 1

4096 → F C 2

4096 → F C 3

4096

Our embeddings consist of computing both GAP (global av-
erage pooling) features , denoted XGAP 512, and multi-layer
(XM U LT I ) features. The GAP features are computed out of
the output of C 5
(3,512) and we apply the average operator to
each of the 512 feature planes resulting in a 512-long fea-
ture vector. XGAP 512 feature embedding forms a represen-
tation of input images that can be interpreted to contain low
and mid level semantic information. In addition we also use
as alternate embedding the concatenated feature outputs out
of all layers (convolutional and fully-connected), denoted
as XM U LT I , of total dimension 9664. Feature computation
is followed by dimensionality reduction using PCA (with
dimension equal to 120). Since this approach uses a pre-
trained network, note that in experiments reported later, care
is taken that no class used for pre-training the DCNN in Im-
ageNet is used also as an outlier class to ensure that outlier

classes truly are unseen by the model. To take a stronger
stand we also pose the same restrictions on inlier classes.

Generative embeddings Generative adversarial net-
works (GANs) are a deep generative approach which learns
to generate novel images from a training dataset. GANs
are composed of two networks that work with adversar-
ial losses. One network performs image generation, using
for example up-convolutions, that map a randomly sampled
vector from latent space to image space, thereby generat-
ing synthetic images. Synthetic images thus created are
then fed to a discriminative network (along with real im-
ages), and this network is trained to classify generated vs
real images. The discriminative network may use a cross-
entropy loss function or Jensen-Shannon divergence which
it minimizes, while the generative network – tries to fool the
discriminative network, and to maximize this loss function.
At convergence, the discriminator has learned to discrimi-
nate between real and fake images, while the generator has
leaned to generate realistic looking images that are essen-
tially sampled from the original training image distribution.
One possibility for a GAN embedding used for novelty
detection consists of using the latent space. Computing it
could be done by a network that performs an inversion of
the generative mapping (an encoder network) that takes an
image as input and generates a latent vector as output. As an
alternative method, this latent vector can be fed back again
through the GAN’s generator network, in essence creating
a reconstruction of the original input image. One can then
send this reconstruction through the GAN’s discriminator
network to perform novelty detection.

An alternate method to obtain the latent vector is via the
method in [3] which relied on a discriminator that used a de-
coder/encoder/decoder structure. Because of this structure,
by training the discriminator, one essentially obtains an en-
coder that yields a latent space mapping “for free” (with-
out inversion needed). Unlike [3], we use this approach in
our framework as a means of producing a generative latent
space embedding for novelty detection. We call this vector
embedding XGAN . In our approach, we use XGAN via the
ND scoring methods described in the next section.

Finally, we compare our ND generative approach to three
methods: 1) the original Ganomaly ND scheme in [3],
which relies on computing the latent vector reconstruction
error between the output of the ﬁrst and second encoder in
the discriminator (See Figure 1). 2) Alternatively, the image
reconstruction error can be used as a score of novelty, as is
considered in [36, 41]. [41] in particular employed a nov-
elty score based on a modiﬁcation of the AnoGAN in [36].
We use it here also for performance comparisons and de-
note it as ND-EGAN [41] (for “efﬁcient” GAN). 3) An-
other principle for GAN-based ND is based on the hypoth-
esis that the discriminator may be used to detect anomalies.

311509

We use a variation of this method exploiting the discrimi-
nator of Wasserstein GAN (WGAN) [5] which we denote
“ND-DGAN’.

resulting ND performance, should depend on how close the
inlier and outlier are in distribution or semantics. Here we
have different choices for closeness:

2.2. Novelty scores

Novelty detection is done by computing novelty scored
on the aforementioned XGAP 512, XM U LT I , and XGAN
embedding vectors. We use four approaches which essen-
tially measure the departure of a test vector X compared to
a set of inlier samples, used for training. These are brieﬂy
reviewed below (for more details see the appendix):

LOF The ﬁrst approach uses local outlier factor (or
“LOF”) [9]. LOF computes a novelty score based on as-
sessing the local density of points around a test point when
compared to the density measured for each of this point’s
neighbours. Intuitively, if a test point’s density is lesser than
its neighbours’ densities, the point may be an outlier.

1CSVM The second approach here uses a one class sup-
port vector machine (or “1CSVM”) [7, 39]. 1CSVM is a
large-margin non-parametric classiﬁer that essentially pro-
ceeds by bounding a set of training inlier exemplars via the
smallest enclosing hypersphere.
In loose terms (see ap-
pendix for precise details) the novelty score is then a dis-
tance from the test vector to the weighted centroid of the
support vectors on this hypersphere.

IF Isolation forest (later termed “IF”) is another novelty
detection method used here [27]. It is akin to random forests
in that it consists of building random CART tree structures
on the training data and exploiting the fact that outliers typ-
ically stand isolated in a branch close to the root of the tree.
Random features are selected and splits are computed on
these features. The path from the root to a feature vector
averaged over a set of random trees is taken as isolation
score and therefore (short meaning isolated) used as nov-
elty score.

EE Finally, we also use an ND score that takes the el-
liptic envelope (later termed “EE”) obtained by assuming a
Gaussian distribution ﬁt to the inlier training exemplars.

In subsequent experiments, for nomenclature: we denote
the complete method applying a ND method “XXX” on fea-
ture type “YY” by “XXX/YY” (e.g., EE applied to GAP512
is denoted “EE/XGAP 512”).

2.3. Characterizing ND problem complexity

For comparing the different deep learning novelty detec-
tion approaches here, we use a set of canonical test prob-
lems mixing inlier and outlier classes: we thereby obtain
a wide range of problems of varying complexity. We en-
deavor to characterize this complexity via some quantita-
tive measure, to allow for an apples-to-apples comparison
of problems of similar complexity. Our ﬁnal goal is to as-
sess how well the proposed ND methods perform for differ-
ent complexity regimes. Intuitively, the complexity, and the

One possible approach to complexity is to assess how
semantically related the outlier classes are to inlier classes.
This could be achieved by looking at proximity in a hier-
archical class representation (e.g. ImageNet WordNet rep-
resentation). However, one challenge with this approach is
that semantic closeness would depend on the speciﬁc hier-
archical representation utilized.

As an alternative, we opt for complexity assessment by
characterizing closeness in distribution, here to be solved
by evaluating distances of probability density of embedded
vectors lying in high dimensional spaces. However, this
endeavor is itself still an open research problem. Two ap-
proaches could be considered:

KLCA: KL-divergence based Complexity Assess-
ment A ﬁrst possibility is to characterize the distance of
distributions (inlier and outlier) via information theoretic
measures, to characterize ND complexity, e.g. using cross-
entropy, earth moving distance or KL divergence. The com-
putation of these measures in general, and KL divergence in
particular, in high dimensions, is still an open problem for
which recently some approaches have been proposed. We
used in particular the method in [40] which leverages efﬁ-
cient K nearest neighbour (KNN) to compute KL. We call
this method KLCA (for KL-based complexity assessment).
BERCA: Bayes error rate complexity assessment We
propose a second approach for characterizing complexity
which consists of assessing the complexity metric as the
Bayes error rate of the two class problem associated with
the one class problem partition. We denote this via the
acronym “BERCA”. Following this route, we consider the
two class classiﬁcation problem Bayes decision rule, i.e. the
rule that minimizes the probability of error P ∗. We compute
the empirical estimate of this error via KNN. Denoting by
kknn the numbers of neighbours used in KNN and nknn the
numbers of samples used, we recall [17] that the knn error
rate P is such that P ∗ ≤ P ≤ P ∗ + ǫ, where ǫ → 0 when
kknn, nknn → ∞. Henceforth, we use the Bayes error rate
computed from the empirical KNN error rate estimate as
the problem complexity metric. We use numerical values of
kknn = 10 and nknn = 1000.

Standardizing the performance evaluation
Finally, equipped with the complexity measures above,
we propose that for apples-to-apples comparisons, ND al-
gorithmic performance be characterized in a trade space
that expresses computed performance, measured in terms
of ROCAUC, as a function of complexity of the ND prob-
lem where this performance was evaluated (i.e. AU C =
f (complexity)). This is to be displayed as box plots since
the complexity of problems will vary, as will also the result-
ing performance range for a given complexity bin.

411510

Figure 2. Examples of images for one of the problems run in IN-
125. In this example 9 inlier classes are randomly selected from
the IN-125 dataset (including bikes, bridges, etc..), and one outlier
class (airplane) is also selected. We show examples of correct and
incorrect novelty detection. For this experiment AUC=0.896

case, we perform experiments on SIMO and MISO prob-
lems deﬁned above for nc = 10 classes. In each problem
case, we run 10 experiments, circularly rotating the single
inlier (SIMO) or single outlier (MISO) every time. We then
evaluate the AUC performance averaged across all 10 ex-
periments (shown in Table 1).

Figure 3. Examples of images for one of the problems run in IN-
125. In this example 9 inlier classes are randomly selected from
the IN-125 dataset (including bunnies, carrots, cows, etc..), and
one outlier class (Kangaroo) is also selected. We show examples
of correct and incorrect novelty detection. Most incorrect classiﬁ-
cations result from confusing factors such as multiple objects and
humans, or bunnies/kangaroo similarity in appearance. For this
experiment AUC=0.878

3. Standardizing ND problem structure and

canonical datasets

We also propose to standardize the data used for evalu-
ating ND around two main datasets with low and high reso-
lution images and two main protocols for choosing inlier vs
outlier class partitioning, described next.

MISO and SIMO problems In general, ND methods
operate on the assumption that they are trained on inliers
only, while testing is carried out on a set of inliers and
outlier-class items, an assumption which is deemed to be
semi-supervised. In addition, partitions of inliers and out-
liers sets can include either single or multiple classes. With
that in mind, we consider two types of problems, and we
adopt the following nomenclature: Single (class) Inlier,
Multiple (class) Outlier problems (termed “SIMO” hence-
forth), and Multiple (class) Inlier, Single (class) Outlier
problems (called “MISO”).

Recalling our ﬁrst use case of an exploring robot as an il-
lustrative example, and assuming the existence of nc classes
in the universe, SIMO problems can be thought of as one-
vs-all problems, in which the robotic agent bootstraps its
knowledge of the universe with a single starter inlier class,
and we perform experiments that evaluates on that same in-
lier class, plus n − 1 other outlier classes that the agent may
encounter, while doing exploration. MISO problems can be
seen as leave-one-out problems, where the ND method is
trained on nc − 1 inlier classes, and tested on that same set
of inlier classes, plus the remaining outlier class.

CIFAR-10 As a baseline, we evaluate ND methods on
the CIFAR-10 dataset, which contains 10 common classes.
This dataset has a large number of images per class (6000),
but coarse image resolution (32x32 RGB images). In this

IN-125 Since prior work on ND has consisted mostly of
testing on simple datasets (MNIST and CIFAR-10), in this
work, we construct a reference dataset composed of 125 Im-
ageNet classes (termed “IN-125”) on which we additionally
evaluate the ND methods described above. Note that, by de-
sign, none of the 125 classes used here in IN-125 belongs
to the original 1000 ImageNet competition classes that are
used for training ConvNets like VGG16, so as to avoid
issues that our ND problems may include outlier classes
that in fact were used for pre-training weights of VGG-16
(or any other network used for discriminative embedding),
which would invalidate the assumptions these classes have
not been seen previously.

IN-125 increases complexity along two challenges when
compared to ND experiments conducted on prior ND stud-
ies in that (a) the images’ resolutions are higher and (b) the
number of exemplars per class is about one order of magni-
tude lower compared to datasets like CIFAR-10 or MNIST.
We release the speciﬁcation [11] of this dataset for future
comparisons, which consists of the set of problems, each
of which we detail by providing the list of classes used as
inliers and outliers.

511511

4. Experiments and performance characteriza-

tion

Figure 4. AUC=f(c) (AUC as a function of problem complexity)
for 125 problems in IN-125 (1CSVM applied to GAP features)

jhu150_LOO_LIMIT_10 - 1CSVM*

We run experiments on the datasets detailed above. The
results of applying our ND framework to two IN-125 prob-
lems is exempliﬁed in Figures 2 and 3. We demonstrate the
use of the performance evaluation framework we designed
whereby we express performance, measured in terms of
AUC, as a function of complexity for each of the approaches
to ND described in this study. We also use the set of se-
lected problems described in the previous section. For each
problem we show the distribution of AUC=f(complexity).

We ﬁrst performed complexity assessment using KLCA.
Results suggested that it is not an effective empirical com-
plexity measure in that KL divergence did not always cor-
relate or predict the performance of ND algorithms. This is
likely due to the issue of attempting to compute KL for dis-
tributions that don’t have overlapping support everywhere,
a problem which is exacerbated by the use of a limited num-
ber of points in high dimensional spaces.

We next evaluated performance as a function of BERCA
and plot in Figs 4-7, for GAP512 features, whisker plots
of the AUC as a function of the BERCA complexity, for
each of the main ND scoring methods (1CSVM, LOF,
IF and EE). The plots demonstrate the effectiveness of
this approach in comparing methods.
It can be observed
that in general performance decreases as BERCA com-
plexity increases for all methods, with graceful degrada-
tion, as should be expected. One exception to this is for
LOF/XGAP 512 (Fig 7) which shows an average AUC that
is less sensitive to increasing problem complexity (for the
range of problems tested here). In aggregate, it is also no-
table that the resulting AUC performance is promising con-
sidering the problems’ complexity.

Note that as a result of the large number of nc = 125
classes in this case, we experimented on the MISO case of
IN-125 only, with the following variation. We ran 125 ex-
periments as expected, cycling the single outlier every time.
However, for our inliers, we randomly chose 10 classes
from the remaining nc − 1 = 124 classes. By restricting
the number of inliers in this way, we could perform a more
accurate comparison to our CIFAR-10 results, without in-
troducing unnecessary variables.

Next, we summarize AUC for all the methods in our
framework on the different datasets in Table 1 showing
average AUC over the problem considered. We also in-
clude results we obtain using best of breed generative meth-
ods including Ganomaly, DGAN and EGAN. We segregate
results into discriminative and generative approaches be-
cause of their different nature and assumptions made (see
more on this in the discussion). The best results are bold-
faced. We can see clearly again the inﬂuence of complex-
ity of the problem on the resulting method performance,

0.9

0.8

0.7

0.6

C
U
A

0.5

0.4

0.3

0.00 - 0.06

0.06 - 0.10

0.10 - 0.14

0.14 - 0.18

0.18 - 0.21

Complexity

0.21 - 0.25

0.25 - 0.29

0.29 - 0.32

0.32 - 1.00

Figure 5. AUC=f(c) (AUC as a function of problem complexity)
for 125 problems in IN-125 (Elliptic Envelope applied to GAP
features)

jhu150_LOO_LIMIT_10 - Elliptic Envelope

0.9

0.8

0.7

0.6

C
U
A

0.5

0.4

0.3

0.00 - 0.06

0.06 - 0.10

0.10 - 0.14

0.14 - 0.18

0.18 - 0.21

Complexity

0.21 - 0.25

0.25 - 0.29

0.29 - 0.32

0.32 - 1.00

Figure 6. AUC=f(c) (AUC as a function of problem complexity)
for 125 problems in IN-125 (Isolation Forest applied to GAP fea-
tures)

jhu150_LOO_LIMIT_10 - Isolation Forest

0.8

0.7

0.6

C
U
A

0.5

0.4

0.3

0.00 - 0.06

0.06 - 0.10

0.10 - 0.14

0.14 - 0.18

0.18 - 0.21

Complexity

0.21 - 0.25

0.25 - 0.29

0.29 - 0.32

0.32 - 1.00

which was precisely characterized in earlier plots by us-
ing metric values of complexity via BERCA. In aggre-
gate it can be seen that methods such as LOF/XGAP 512
and LOF/XM U LT I perform best on CIFAR-10 (MISO),
and EE/XGAP 512 and EE/XM U LT I do so on the sim-
pler set of problems in CIFAR-10 (SIMO), and that on
1CSVM/XGAP 512 and EE/XGAP 512 for the more complex

611512

Figure 7. AUC=f(c) (AUC as a function of problem complexity)
for 125 problems in IN-125 (Local Outlier Factor applied to GAP
features)

jhu150_LOO_LIMIT_10 - Local Outlier Factor

0.7

0.6

0.5

C
U
A

0.4

0.3

0.00 - 0.06

0.06 - 0.10

0.10 - 0.14

0.14 - 0.18

0.18 - 0.21

Complexity

0.21 - 0.25

0.25 - 0.29

0.29 - 0.32

0.32 - 1.00

IN-125.

For generative methods, the results show that the combi-
nation of 1CSVM and using XGAN features improves upon
the best in class methods previously reported for the class
of generative ND methods [3].

5. Discussion

Analysis of results In general all deep embedding meth-
ods outperformed GAN-based methods. But these should
not be taken as equal:
this is because discriminative em-
beddings use pre-trained networks and as such exploit ad-
ditional prior information in addition to to the inlier train-
ing samples they used for training (which is the only in-
formation used by generative methods). This is why these
methods should be considered separately. When consider-
ing only generative ND methods, one of the methods in our
framework, combining XGAN and 1CSVM, resulted in best
overall performance in that class of methods and compared
to prior work. In aggregate it is also encouraging to see that
generative methods’ performance, despite using less infor-
mation, often comes close to that of discriminative methods.
How to best compare DL-based ND methods going
forward? Recent ND studies using DL methods, even
when they used the same datasets (e.g. MNIST), often did
not adopt the same conventions for what classes were used
as inliers or outliers, and different combinations and parti-
tioning entailed different resulting problem complexity for
the speciﬁc ND problems being addressed. There is there-
fore a pressing need for more comparable protocols when
reporting ND performance methods as their performance
signiﬁcantly depend on the problem complexity. Because
of this we see value in having future research studies use
means of reporting performance of ND methods similar or
inspired by what was adopted here: taking into account a
quantitative complexity measure such as the one we pro-
posed herein in BERCA, so as to allow apples-to-apples
comparisons between studies. Showing trade spaces involv-

ing AUC=f(complexity) would also allow one to predict the
operational performance of a method for new problems and
datasets. Additionally, this approach has value for analyz-
ing trends of results, as was shown in the whisker plots
above.
Future work As an alternate approach to ND problem gen-
eration, it is possible to achieve different levels of com-
plexity by selecting unknown instances with different lev-
els of similarity to known classes based on class hierarchy
or semantic relationship. Our proposed metric in essence
takes that concept and introduces a more general quanti-
tative measure that is tied more directly to classiﬁcation
performance. It would be valuable to investigate how our
complexity measure is related to the complexity driven
by subjective semantic similarity used in information re-
trieval. Our dataset can certainly facilitate such a study.
Future/alternative ways to characterize complexity can be
investigated: these could consist of using information the-
oretic measures such as KL-divergence with multivariate
Gaussian assumptions or be inspired by metrics such as
Bayesian theory of surprise [6]. Future work can also ex-
tend the use of ND for improved zero-shot learning perfor-
mance [12, 30]

Recent work using GANs for embeddings and nov-
elty detection has suggested possible beneﬁts. However
these studies were often conducted on restricted datasets,
or datasets with small number of classes, large number of
images per class, and/or low image resolution. This study
shows that using GANs as representation confers beneﬁts
while using no prior information (other than inlier train-
ing data) even for more complex datasets. We believe
that newer GAN formulations such as BigGAN [10] Pro-
GAN [22] and StyleGAN [23] can play a role to further
this work by allowing larger resolution images which in turn
may entail latent spaces that would possibly facilitate better
ND.

6. Conclusion

We present a framework for novelty detection based on
deep embeddings, both discriminative and generative. We
propose a new way to fairly characterize novelty detection
performance using problem complexity which allows for
apples-to-apples comparisons. One of the proposed gener-
ative methods in our framework demonstrates best of breed
performance among recently proposed generative novelty
detection methods.

Acknowledgements

We thank Jemima Albayda and Mauro Maggioni (JHU)
for thoughtful inputs and discussions. The support of JHU
APL internal research and development funding is grate-
fully acknowledged.

711513

Table 1. Average AUC performance of various methods in our framework using generative or discriminative embeddings, and comparison
with recent methods of record. In parenthesis: error margins for 95% conﬁdence intervals.

Novelty detection methods via discriminative embeddings.

Method (Novelty measure/Embedding vector) CIFAR-10 (MISO) CIFAR-10 (SIMO)
1CSVM/XGAP 512
1CSVM/XM U LT I
EE/XGAP 512
EE/XM U LT I
IF/XGAP 512
IF/XM U LT I
LOF/XGAP 512
LOF/XM U LT I

0.6853 (0.0907)
0.7322 (0.0500)
0.7196 (0.0931)
0.7165 (0.0636)
0.6706 (0.0817)
0.6750 (0.0634)
0.6324 (0.0792)
0.6783 (0.0566)

0.5771 (0.1180)
0.5932 (0.0688)
0.5408 (0.1130)
0.5527 (0.0889)
0.5378 (0.1168)
0.5373 (0.0561)
0.6224 (0.0581)
0.6030 (0.0399)

Novelty detection methods via generative embeddings.

Method (Novelty measure/Embedding vector) CIFAR-10 (MISO) CIFAR-10 (SIMO)
1CSVM/XGAN
ND-DGAN [5, 36]
ND-EGAN [36, 41]
GANOMALY [3]

0.6279 (0.1266)
0.4495 (0.1125)
0.4183 (0.0950)
0.6228 (0.1092)

0.5627 (0.1458)
0.4898 (0.0305)
0.4655 (0.1250)
0.5321 (0.1292)

IN-125 (MISO)
0.6241 (0.1291)
0.5759 (0.1292)
0.6280 (0.1458)
0.5696 (0.1389)
0.5437 (0.1238)
0.5315 (0.1259)
0.5037 (0.1112)
0.5249 (0.1243)

IN-125 (MISO)
0.5792 (0.0719)
0.4789 (0.0300)
0.4822 (0.0741)
0.5708 (0.0775)

Appendix A: Additional Technical Details

We provide here some more details on the novelty de-
tection algorithms: LOF The LOF of a point (here a fea-
ture vector X) is based on comparing the density of points
around X against the density of X’s neighbours [9]. Deﬁn-
ing ﬁrst the k-distance dk(X) of X to that of its k-th nearest
neighbour, and noting Lk(X) the set of points (the so-called
“MinPts”) within dk(X), one deﬁnes the “reachability dis-
tance” of X from any origin point O and for a given k as:
Rk(X, O) = max(d(X, O), dk(O)). To characterize den-
sity, one deﬁnes the local reachability density lrd(X) by
taking the inverse of the average reachability distance of all
points O ∈ Lk(X).

To compare densities, the LOF (X) is deﬁned as the av-
erage of the the lrd(.) of all points in Lk(X) divided by
lrd(X). This ratio considers the average local densities of
the neighbours of X compared to the local density of X.

Intuitively if this value is higher than one the point X is
less dense (less reachable by its neighbours than the neigh-
bours of X are by their own neighbours), and is therefore
an outlier. Likewise, the opposite of the LOF can be used as
a score to detect novelty (values of this increasing as a point
becomes more an inlier).

1CSVM In 1CSVM, inliers points x are modeled as ly-
ing inside a hypersphere with center denoted a and radius
R which is found by minimizing the error function:

F (R, a) = R2 with kxi − ak2 ≤ R2, ∀i

(1)

To allow for training datasets corrupted with outliers one in-
troduces slack variables ξi ≥ 0 to allow for some violations

kxi − ak2 ≤ R2 + ξi, ξi ≥ 0 ∀i

(2)

and modiﬁes the minimization problem to include penalties
on ξi magnitude F(R,a) = R2 + C Pi ξi, with C a weight-
ing for slack variables. Using Lagrange multipliers αi ≥ 0
and γi ≥ 0, this problem can be formulated as one of min-
imizing L with regard to R, a, xi, and maximizing L with
respect to αi and γi:

L(R, a, αi, γi, ξi) = R2 + C X

ξi − X

γiξi

i

i

αi{R2 + ξi − kxi − ak2}

− X

i

It can be shown [7, 19, 39] that this reduces to minimizing:

L = X

αi(xi · xi) − X

αiαj(xi · xj)

(3)

i

i,j

with constraints in Eq. (2). The linear dot product is com-
monly replaced with a nonlinear kernel K(x, y) (e.g. RBF)
to allow for nonlinear decision boundaries to emerge. Min-
imizing L produces a set of weights αi for the correspond-
ing samples xi, and the center a and radius R of the hy-
persphere. By invoking the complementary slackness con-
straints training exemplars with nonzero weights emerge as
support vectors of the data. To test out if a a test exemplar
y lies within the hypersphere, one can use as score the dis-
tance of a sample to the center of the hypersphere [7,19,39]:

d(y) =

1
R2 [K(y, y)−2 X

i

αiK(y, xi)+X

αiαjK(xi, xj)]

i,j

(4)

811514

References

[1] R. Abay, S. Gehly, S. Balage, M. Brown, and R. Boyce. Ma-
neuver detection of space objects using generative adversar-
ial networks. 2018. 1

[2] M. Ahmed, A. N. Mahmood, and M. R. Islam. A survey of
anomaly detection techniques in ﬁnancial domain. Future
Generation Computer Systems, 55:278–288, 2016. 1

[3] S. Akcay, A. Atapour-Abarghouei, and T. P. Breckon.
Ganomaly: Semi-supervised anomaly detection via adver-
sarial training. arXiv preprint arXiv:1805.06725, 2018. 1, 2,
3, 7, 8

[4] L. Akoglu, H. Tong, and D. Koutra. Graph based anomaly
detection and description: a survey. Data mining and knowl-
edge discovery, 29(3):626–688, 2015. 1

[5] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan.

arXiv preprint arXiv:1701.07875, 2017. 4, 8

[6] P. Baldi and L. Itti. Of bits and wows: A bayesian theory
of surprise with applications to attention. Neural Networks,
23(5):649–666, 2010. 7

[7] A. Banerjee, P. Burlina, and C. Diehl. A support vec-
tor method for anomaly detection in hyperspectral imagery.
IEEE Transactions on Geoscience and Remote Sensing,
44(8):2282–2291, 2006. 1, 4, 8

[8] P. Bergmann, S. L¨owe, M. Fauser, D. Sattlegger, and C. Ste-
ger.
Improving unsupervised defect segmentation by ap-
plying structural similarity to autoencoders. arXiv preprint
arXiv:1807.02011, 2018. 1

[9] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. Lof:
In ACM sigmod

identifying density-based local outliers.
record, volume 29, pages 93–104. ACM, 2000. 4, 8

[10] A. Brock, J. Donahue, and K. Simonyan. Large scale gan
arXiv

training for high ﬁdelity natural image synthesis.
preprint arXiv:1809.11096, 2018. 7

[11] P. Burlina, N. Joshi, and I.-J. Wang. Problem speciﬁcation
for in-125, url = https://github.com/neil454/
in-125, urldate = 2019-03-19. 5

[12] P. M. Burlina, A. C. Schmidt, and I.-J. Wang. Zero shot deep
learning from semantic attributes. In 2015 IEEE 14th Inter-
national Conference on Machine Learning and Applications
(ICMLA), pages 871–876. IEEE, 2015. 7

[13] D. Carrera, G. Boracchi, A. Foi, and B. Wohlberg. Detect-
ing anomalous structures by convolutional sparse models. In
Neural Networks (IJCNN), 2015 International Joint Confer-
ence on, pages 1–8. IEEE, 2015. 1

[14] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detec-
tion: A survey. ACM computing surveys (CSUR), 41(3):15,
2009. 1

[15] G. Chen, M. Iwen, S. Chin, and M. Maggioni. A fast multi-
scale framework for data in high-dimensions: Measure esti-
mation, anomaly detection, and compressive measurements.
In Visual Communications and Image Processing (VCIP),
2012 IEEE, pages 1–6. IEEE, 2012. 1

[16] L. Deecke, R. Vandermeulen, L. Ruff, S. Mandt, and
M. Kloft. Anomaly detection with generative adversarial net-
works. 2018. 1, 2

[17] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern classiﬁca-

tion. Wiley, New York, 1973. 4

[18] S. M. Erfani, S. Rajasegarar, S. Karunasekera, and C. Leckie.
High-dimensional and large-scale anomaly detection using a
linear one-class svm with deep learning. Pattern Recogni-
tion, 58:121–134, 2016. 1

[19] D. E. Freund, N. Bressler, and P. Burlina. Automated detec-
tion of drusen in the macula. In Biomedical Imaging: From
Nano to Macro, 2009. ISBI’09. IEEE International Sympo-
sium on, pages 61–64. IEEE, 2009. 8

[20] K. Gray, D. Smolyak, S. Badirli, and G. Mohler. Coupled
igmm-gans for deep multimodal anomaly detection in human
mobility data. arXiv preprint arXiv:1809.02728, 2018. 1

[21] N. Jain, L. Manikonda, A. O. Hernandez, S. Sengupta,
and S. Kambhampati.
Imagining an engineer: On gan-
based data augmentation perpetuating biases. arXiv preprint
arXiv:1811.03751, 2018. 1

[22] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
arXiv preprint arXiv:1710.10196, 2017. 7

[23] T. Karras, S. Laine, and T. Aila. A style-based genera-
tor architecture for generative adversarial networks. arXiv
preprint arXiv:1812.04948, 2018. 7

[24] M. Kimura and T. Yanagihara. Semi-supervised anomaly
detection using gans for visual inspection in noisy training
data. arXiv preprint arXiv:1807.01136, 2018. 1

[25] D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, and K. J. Kim.
A survey of deep learning-based network anomaly detection.
Cluster Computing, pages 1–13, 2017. 1

[26] Y. Lai, J. Hu, Y. Tsai, and W. Chiu. Industrial anomaly detec-
tion and one-class classiﬁcation using generative adversar-
ial networks. In 2018 IEEE/ASME International Conference
on Advanced Intelligent Mechatronics (AIM), pages 1444–
1449. IEEE, 2018. 1

[27] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation forest. In
Data Mining, 2008. ICDM’08. Eighth IEEE International
Conference on, pages 413–422. IEEE, 2008. 4

[28] Y. Liu, Z. Li, C. Zhou, Y. Jiang, J. Sun, M. Wang, and X. He.
Generative adversarial active learning for unsupervised out-
lier detection. arXiv preprint arXiv:1809.10816, 2018. 1

[29] J. Markowitz, A. C. Schmidt, P. M. Burlina, and I.-J. Wang.
Hierarchical zero-shot classiﬁcation with convolutional neu-
ral network features and semantic attribute learning. In Inter-
national Conference on Machine Vision Applications (MVA),
2017. IAPR, 2017. 1

[30] J. Markowitz, A. C. Schmidt, P. M. Burlina, and I.-J. Wang.
Hierarchical zero-shot classiﬁcation with convolutional neu-
ral network features and semantic attribute learning. In 2017
Fifteenth IAPR International Conference on Machine Vision
Applications (MVA), pages 194–197. IEEE, 2017. 7

[31] S. Matteoli, M. Diani, and J. Theiler. An overview of back-
ground modeling for detection of targets and anomalies in
hyperspectral remotely sensed imagery.
IEEE Journal of
Selected Topics in Applied Earth Observations and Remote
Sensing, 7(6):2317–2336, 2014. 1

[32] M. Naphade, M.-C. Chang, A. Sharma, D. C. Anastasiu,
V. Jagarlamudi, P. Chakraborty, T. Huang, S. Wang, M.-Y.
Liu, R. Chellappa, et al. The 2018 nvidia ai city challenge.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, pages 53–60, 2018. 1

911515

[33] M. A. Pimentel, D. A. Clifton, L. Clifton, and L. Tarassenko.
A review of novelty detection. Signal Processing, 99:215–
249, 2014. 1

[34] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 2

[35] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson.
Cnn features off-the-shelf: an astounding baseline for recog-
nition. In Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2014 IEEE Conference on, pages 512–519.
IEEE, 2014. 3

[36] T. Schlegl, P. Seeb¨ock, S. M. Waldstein, U. Schmidt-Erfurth,
and G. Langs. Unsupervised anomaly detection with gen-
erative adversarial networks to guide marker discovery. In
International Conference on Information Processing in Med-
ical Imaging, pages 146–157. Springer, 2017. 1, 2, 3, 8

[37] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 3

[38] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot
learning through cross-modal transfer. In Advances in neural
information processing systems, pages 935–943, 2013. 1

[39] D. M. Tax and R. P. Duin. Support vector data description.

Machine learning, 54(1):45–66, 2004. 4, 8

[40] Q. Wang, S. R. Kulkarni, and S. Verd´u. Divergence estima-
tion for multidimensional densities via k-nearest-neighbor
distances.
IEEE Transactions on Information Theory,
55(5):2392–2405, 2009. 4

[41] H. Zenati, C. S. Foo, B. Lecouat, G. Manek, and V. R. Chan-
drasekhar. Efﬁcient gan-based anomaly detection. arXiv
preprint arXiv:1802.06222, 2018. 1, 2, 3, 8

1011516

