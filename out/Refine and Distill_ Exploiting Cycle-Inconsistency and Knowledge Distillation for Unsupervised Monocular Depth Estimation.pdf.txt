Reﬁne and Distill: Exploiting Cycle-Inconsistency and Knowledge

Distillation for Unsupervised Monocular Depth Estimation

Andrea Pilzer1, St´ephane Lathuili`ere1, Nicu Sebe1

,

2, and Elisa Ricci1

3

,

1DISI, University of Trento, via Sommarive 14, Povo (TN), Italy

2Huawei Technologies Ireland, Dublin, Ireland

3Technologies of Vision, Fondazione Bruno Kessler, via Sommarive 18, Povo (TN), Italy

{andrea.pilzer, stephane.lathuiliere, niculae.sebe, e.ricci}@unitn.it

Abstract

Nowadays, the majority of state of the art monocular
depth estimation techniques are based on supervised deep
learning models. However, collecting RGB images with as-
sociated depth maps is a very time consuming procedure.
Therefore, recent works have proposed deep architectures
for addressing the monocular depth prediction task as a
reconstruction problem, thus avoiding the need of collect-
ing ground-truth depth. Following these works, we propose
a novel self-supervised deep model for estimating depth
maps. Our framework exploits two main strategies: reﬁne-
ment via cycle-inconsistency and distillation. Speciﬁcally,
ﬁrst a student network is trained to predict a disparity map
such as to recover from a frame in a camera view the asso-
ciated image in the opposite view. Then, a backward cycle
network is applied to the generated image to re-synthesize
back the input image, estimating the opposite disparity. A
third network exploits the inconsistency between the orig-
inal and the reconstructed input frame in order to output
a reﬁned depth map. Finally, knowledge distillation is ex-
ploited, such as to transfer information from the reﬁnement
network to the student. Our extensive experimental evalu-
ation demonstrate the effectiveness of the proposed frame-
work which outperforms state of the art unsupervised meth-
ods on the KITTI benchmark.

1. Introduction

In the last few years, deep learning-based approaches for
depth estimation [4, 20, 24, 37, 12, 42, 27, 31] have at-
tracted a growing interest, motivated, on the one hand, by
their ability to predict very accurate depth maps and, on
the other hand, by the importance of recovering depth in-
formation in several applications, such as robot navigation,
autonomous driving, virtual reality and 3D reconstruction.
Exploiting the availability of very large annotated

Figure 1. Outline of the proposed approach: from the right view
image, we predict the left image from which we re-synthesize the
right image. The inconsistencies are used by the inconsistency-
module to improve the depth estimation. The reﬁned depth maps
are used to improve the Student Network via knowledge distilla-
tion.

datasets, Convolutional Neural Networks
(ConvNets)
trained in a supervised setting are now state-of-the-art in
many computer vision tasks such as object detection [10],
instance segmentation [30], human pose estimation [29].
However, a major weakness of these approaches is the
need of collecting large-scale labeled datasets. In the case
of depth estimation, acquiring data is especially costly.
For instance, in the scenario of depth estimation for au-
tonomous driving, it implies driving a car equipped with
a laser LiDaR scanner for hours under diverse lighting and
weather conditions. Self-supervised depth estimation, also
referred to as unsupervised, recently emerged as an inter-
esting paradigm and an effective alternative to supervised
methods [26, 7, 28, 12, 33]. Roughly speaking, in the self-
supervised setting, stereo image pairs are considered as in-
put and a deep predictor is learned in order to estimate the
associated disparity maps. Speciﬁcally, the predicted dis-
parity is employed to synthesize, from a frame in a camera
view (e.g. from the left camera), the opposite view through
warping. The deep network is trained via gradient descent
by minimizing the discrepancy between the original and

19768

the reconstructed image. Importantly, even if stereo images
pairs are required for training, depth can be recovered from
a single image at test time.

In this paper, we follow this research thread and pro-
pose a novel self-supervised deep architecture for monocu-
lar depth estimation. The proposed approach, illustrated in
Fig 1, consists of a ﬁrst sub-network, referred to as the stu-
dent network, which receives as input an image from a cam-
era view and predicts a disparity map such as to recover the
opposite view. On top of this network, we propose several
contributions. First, from the generated image, we propose
to re-synthesize the input image by estimating the opposite
disparity. The resulting network forms a cycle. Second, a
third network exploits the cycle inconsistency between the
original and the reconstructed input images in order to re-
ﬁne the estimated depth maps. Our intuition is that incon-
sistency maps provide rich information which can be further
exploited, as they indicate where the ﬁrst two networks fail
to predict disparity pixels. Finally, we propose to use the
principle of distillation in order to transfer knowledge from
the whole network, seen as a teacher, to the student net-
work.
Interestingly, our framework produce two outputs,
corresponding to the depth maps estimated respectively by
the student and the teacher networks. This is extremely rel-
evant in practical applications, as the student network can
be exploited in case of low computation power or real-time
constraints.

Our extensive experiments on two large publicly avail-
able datasets, i.e. the KITTI [8] and the Cityscapes [2]
datasets, demonstrate the effectiveness of the proposed
framework. Notably, by combining the proposed cycle
structure with our inconsistency-aware reﬁnement, our un-
supervised framework outperforms previous usupervised
approaches, while obtaining comparable results with the
state-of-the-art supervised methods on the KITTI dataset.

2. Related Work

In the last decade, deep learning models have greatly im-
proved the performance of depth estimation methods. The
vast majority of methods focus on a supervised setting and
the problem of predicting depth maps is cast as a pixel-
level regression problem [4, 24, 43, 21, 37, 39, 5]. The ﬁrst
ConvNet approach for monocular depth prediction was pro-
posed in Eigen et al. [4], where the beneﬁt of considering
both local and global information was demonstrated. More
recent works improved the performance of deep models by
exploiting probabilistic graphical models implemented as
neural networks [24, 35, 38, 37]. For instance, Wang et
al. [35] proposed integrating hierarchical Conditional Ran-
dom Fields (CRFs) into a ConvNet for joint depth estima-
tion and semantic segmentation. Xu et al. [38, 37] exploited
CRFs within a deep architecture in order to fuse information
at multiple scales. However, supervised approaches rely on

expensive ground-truth annotations and, consequently, lack
ﬂexibility for deployment in novel environments.

Recently, several works proposed to tackle the depth es-
timation problem within an unsupervised learning frame-
work [19, 27, 33, 41, 31]. For instance, Garg et al. [7] at-
tempted to learn depth maps in an indirect way. They used
a ConvNet to predict the right-to-left disparity map from
the left image and then reconstructed the right image ac-
cording to the predicted disparity. They also introduced
a network architecture operating based on a coarse-to-ﬁne
principle, i.e. they employed an encoder-decoder network
where the decoder ﬁrst estimates a low resolution disparity
map and then reﬁnes it in order to obtain a map at higher
resolution.
Improving upon [7], Godard et al. [12] pro-
posed to use a single generative network to estimate both
the left-to-right and the right-to-left disparity maps. Con-
sistency between the two disparities was exploited in form
of a loss in order to better constrain the model. Other recent
works demonstrated that temporal information and, in par-
ticular, considering multiple consecutive frames contribute
to improve depth estimation [34, 40, 11, 42]. In particular,
Zhou et al. [42] exploited temporal information to jointly
learn the depth and the camera ego-motion from monocular
sequences. Similarly, in [11], a deep network was designed
in order to estimate both the depth and the camera pose from
three consecutive frames. In this paper we focus on improv-
ing frame-level unsupervised depth estimation and we do
not exploit any additional information such as supervision
from related tasks (e.g. ego-motion estimation) or temporal
consistency. In this respect, our work can be regarded as
complementary to [42, 11].

The idea of exploiting cycle-consistency for depth es-
timation was recently investigated in [31]. Speciﬁcally,
Pilzer et al. [31] introduced a deep architecture for stereo
depth estimation which is organized in form of a cycle: two
sub-networks, corresponding to the two half-cycles, esti-
mate respectively the left-to-right and right-to-left dispari-
ties. They also showed that cycle consistency, together with
an adversarial loss, can greatly improve the quality of the
predicted depth maps. The main difference with our pro-
posal is that the architecture in [31] is designed for stereo
depth estimation whereas we focus on the monocular set-
ting. Moreover, contrary to [31], our architecture exploits
cycle inconsistency both at training and at test time. Si-
multaneously, Tosi et al. [32] proposed disparity reﬁne-
ment and Yang et al. [39] proposed to compute the error
maps between the original input images and their cycle-
reconstructed versions and considered them as an additional
input to a second network which produces reﬁned depth es-
timates. Opposite to our approach, the deep model in [39]
is trained using supervision derived by Stereo Direct Sparse
Odometry [36]. Furthermore, to construct the cycle, we ex-
ploit a backward network and introduce a distillation loss.

9769

Figure 2. The proposed approach is composed of two modules. A ﬁrst network Gs predicts the right-to-left disparity map dl from the
right image and synthesizes the left image as described in Sec. 3.4. In the second module, a generator network Gb predicts the left-to-right
disparity map dr in order to re-synthesize the right image. The model obtained in this way forms a cycle. The cycle inconsistency is used
by a third network to predict the ﬁnal disparity map. We use a set of losses (orange dot arrows) detailed in Sec. 3.4

.

Recently, knowledge distillation attracted a lot of atten-
tion [15]. This methodology consists in compressing a large
deep network (usually referred to as the teacher) into a
much smaller model (student) operating on the same modal-
ity. The student network is trained such that its outputs
match those of the teacher. Knowledge distillation has been
exploited for many computer vision tasks such as domain
adaptation [14], object detection [1], learning from noisy
labels [23] or facial analysis [25]. However, to the best of
our knowledge, this work is the ﬁrst attempt to exploit distil-
lation for depth estimation. We claim that distillation is es-
pecially relevant for depth estimation since, in practical ap-
plications such as autonomous driving, real-time constraints
may impose limitations in term of network size. Note that,
we employ an unusual distillation scenario in which the stu-
dent network is a sub-network of the teacher.

3. Proposed Method

3.1. Overview

The aim of this work is to estimate the depth of a scene
from a single image. However, at training time, we consider
that we dispose of pairs of images {Il, Ir} of size H × W ,
derived from a stereo pair and corresponding to the same
time instant. Here, Il denotes the left camera view and Ir
is the right camera view. Given Ir, we are interested in
predicting a correspondence map dl ∈ RH×W , namely the
right-to-left disparity, in which each pixel value represents
the offset of the corresponding pixel between the right and
the left images. Finally, assuming that the images are recti-
ﬁed, the depth at a pixel location (x, y) of the left image can
be recovered from the predicted disparity with dl = f.b
d(x,y) ,

where b is the distance between the two cameras and f is
the camera focal length.

An overview of the proposed framework is shown in
Fig. 2. A ﬁrst network Gs predicts the right-to-left disparity
map dl from the right image Ir, and synthesizes the left im-
age by warping Ir according to dl. Roughly speaking, the
network Gs is trained to minimize the discrepancy between
the real and the reconstructed left image (Sec. 3.4).

We employ a second generator network Gb that takes as
input the synthesized left image and predicts a left-to-right
disparity map dr that is used to re-synthesize the right im-
age. The model obtained in this way forms a cycle. This
cycle design has three advantages. First, at training time,
by sharing weights between Gs and Gb, the networks learn
to predict disparity maps from the images of the training
set (in the forward half-cycle Gs) but also from the syn-
thesized images (in the backward half-cycle Gb).
In that
sense, the use of the cycle can be seen as a sort of data
augmentation. Second, in order to re-synthesize correctly
the right image, the second network Gb requires a correct
input left image. Thus, Gb imposes a global constraint
on the estimated disparity dl oppositely to standard pixel-
wise discrepancy losses, such as L1 or L2 that act only lo-
cally. Third, by comparing the input right image Ir and
the output right image ˆIr synthesized after applying our cy-
cle framework, we can measure the cycle inconsistency. At
a given location of the input image, if we observe no in-
consistency, Gs and Gb must have predicted correctly the
disparity maps. Conversely, in case of inconsistency, Gs or
Gb (or both) must have predicted incorrectly the disparity
maps. Note that inconsistencies may also appear on objects
regions that are visible in only one of the two views. In-

9770

terestingly, these regions are usually located on the object
edges. Therefore, looking at cycle inconsistency also pro-
vides information about object edges that can help to pre-
dict better depth maps. Importantly, this inconsistency can
be measured both at training and testing times, even if at
testing time, we dispose only of the right image.

The main contribution of this work consists in exploit-
ing the cycle inconsistency by training a third network in
order to improve the prediction performance and output a
reﬁned depth map d′
In addition, since employing our
l.
inconsistency-aware network leads to more accurate depth
predictions, we propose to use the disparity maps predicted
by Gi in order to improve Gs training via a knowledge dis-
tillation approach.

Note that, another possible cycle approach, as proposed
in [39], would consist in using a single network to pre-
dict the two disparity maps. The two disparities can be
used to obtain the synthesized left image and then the re-
synthesized right image. Nevertheless, this approach has a
major disadvantage with respect to our approach, i.e., since
only the warping operator in employed between the two
synthesized images, and consequently the receptive ﬁeld of
ˆIr in Il is very small. In particular, when implementing the
warping operator via bilinear sampling, the receptive ﬁeld
of the warping operator in only 2 × 2. Therefore, the right
image reconstruction loss can act on the reconstructed left
image only locally. Conversely, our backward network Gb
imposes a global consistency on dl thanks to its large re-
ceptive ﬁeld.

l. While the estimated depth d′

The outputs of our method correspond to the estimated
depth maps dl and d′
l corre-
sponding to the teacher model is typically more accurate, in
some applications, e.g. in resource-constrained settings, it
could be convenient to exploit only a small student network.
In the following, we describe the design of our cycled
network. Then, we introduce our novel inconsistency-aware
network. Finally, we present the optimization objective in-
cluding our proposed distillation approach.

3.2. Unsupervised Monocular Cycled Network

In this work, we adopt a setting in which the model
is trained without the need of ground truth depth maps.
This approach is often referred to as unsupervised or self-
supervised depth estimation. Roughly speaking, it consists
in training a network to predict a disparity map that can be
used to generate the left image from the right image. For-
mally speaking, we employ a ﬁrst network Gs that takes as
input the right image Ir and predicts the right-to-left dis-
parity dl. Following [12], we adopt a U-Net architecture
for Gs. We employ a warping function fw(·) that synthe-
sizes the left view image by sampling from Ir according to
dl:

ˆIl = fw(dl, Ir).

(1)

Importantly, fw(·) is implemented using the bilinear sam-
pler from the spatial transformer network [16] resulting in
a fully differentiable model. Consequently, the network can
be trained via gradient descent by minimizing the discrep-
ancy between ˆIl and Il (see Sec. 3.4 for details about net-
work training).

Inspired by [31], we employ a second network Gb in or-

der to re-synthesize the right image according to:

where:

ˆIr = fw(dr, ˆIl).

dr = Gb(ˆIl)

(2)

(3)

The Gb and Gs networks share their encoder parameters.
Note that, differently from the stereo depth model proposed
in [31], our second half-cycle network takes only the syn-
thesized left image as input. This crucial difference allows
the use of this cycle in the monocular setting at testing time.
Concerning the decoder networks, we adopt an architecture
composed of a sequence of up-convolution layers in which
the disparity is estimated and gradually reﬁned from low to
full resolutions similarly to [12]. We obtain the estimated
left and the right disparity maps at each scale dn
l and dn
r ,
n ∈ {0, 1, 2, 3}, with sizes [H/2n, W/2n]. More precisely,
n of size
dn
r is computed from the decoder feature map ξr
[H/2n, W/2n] via a convolutional layer. Then, dn
r is con-
n obtaining a tensor that is input to an up-
catenated with ξr
convolution layer in order to estimate the disparity at the
next resolution dn−1

.

r

3.3. Inconsistency Aware Network

We deﬁne the inconsistency tensor as the difference be-
tween the input image Ir and the image ˆIr predicted by the
backward network Gb:

Ir = Ir − ˆIr

(4)

The proposed inconsistency-aware network Gi takes as in-
put the concatenation of Ir, Ir and dl. We employ a net-
work architecture similar to the half-cycle monocular net-
work described in Sec. 3.2. However, we propose to pro-
vide to the encoder network the disparity maps dn
l , n ∈
{1, 2, 3} estimated by Gs at each scale. More precisely, we
concatenate along the channel axis each disparity dn
l with
network features of corresponding dimensionality.

The inconsistency-aware network Gi estimates the right-
) and we recon-

l = Gi(Ir, Ir, dl, d

{1,2,3}
l

to-left disparity d′
struct the left view image ˆIl
tion fw:

′

by applying the warping func-

ˆI′
l = fw(d′

l, Ir)

(5)

Similarly to Gs and Gb, Gi estimates low resolution dispar-
n, n ∈ {1, 2, 3} that are gradually reﬁned from
ity maps d′
low to full resolutions.

l

9771

3.4. Network Training and Knowledge Self 

Distillation

In this section, we detail the losses employed to train the

proposed network in an end-to-end fashion.
Reconstruction. First, we employ a reconstruction and
stucture similarity loss for each network. Following [12],
we adopt the L1 loss to measure the discrepancy between
the synthesized and the real images and the structure simi-
larity loss LSSIM to measure the discrepancy between the
synthesized and the real images structure. By summing the
losses of the three networks Gs, Gb and Gi, we obtain:

l converges to dl. On the contrary, the goal is to help

that d′
dl to become as accurate as d′
l.

For the second, we impose that the decoder features
ξ′n
r , n ∈ 0, 1, 2 of the teacher are similar to the features
ξn
r of the student. The self-distillation loss is given by:

Ldist = ||ξn

r − S(ξ′n

r )||2

(9)

The total training loss is given by:

Ltot = Lrec + λdistLdist

(10)

L(0)

rec = λs[αLSSIM (ˆIl, Il) + (1 − α)||ˆIl − Il||1]
+λb[αLSSIM (ˆIr, Ir) + (1 − α)||ˆIr − Ir||1]
+λt[αLSSIM (ˆI′
l − Il||1]

l, Il) + (1 − α)||ˆI′

4. Experiments

(6)

We evaluate our proposed approach on two publicly
available datasets and compare its performance with state
of the art methods.

where λs, λb and λt are adjustment parameters and α =
0.85. Similarly, we also compute a reconstruction loss L(n)
rec
for the low resolution disparity maps. Following [11], we
n to H × W and
upsample the low resolution dn
use the warping operator fw to re-synthesize full resolution
images that are compared with the real images according to
the L1 loss. The total reconstruction loss is:

r and d′

l , dn

l

Lrec =

4

X

n=0

L(n)
rec

(7)

Self-Distillation. Finally, we propose to introduce a knowl-
edge distillation loss. As detailed in the experimental sec-
tion (Sec 4), the inconsistency-aware network outperforms
by a signiﬁcant margin the simple half-cycle network Gs.
This boost is at the cost of a higher computation complexity.
The idea of the proposed self-distillation loss consists in dis-
tilling knowledge from inconsistency-aware network to the
half-cycle network Gs. Thus, we improve the performance
of Gs without adding any computation complexity at test-
ing time. To do so, we evaluate disparity and feature distil-
lation. For the ﬁrst, we impose that the network Gd predicts
disparity maps similar to the output of inconsistency-aware
network. It can be seen as a distillation approach where Gs
plays the role of the student and the whole network (com-
posed of Gs, Gb and Gi) is the teacher. However, in our
particular case, the student network is a sub-network of the
teacher. From this perspective, we name this approach self-
distillation. The self-distillation loss is given by:

Ldist = ||dl − S(d′

l)||1

(8)

where S denotes the stop-gradient operation.
In particu-
lar, the stop-gradient operation equals the identity function
when computing the forward pass of the back-propagation
algorithm but it has a null gradient when computing the
backward pass. The purpose of the stop-gradient is to avoid

4.1. Experimental Setup

Datasets. We perform experiments on two large stereo
images datasets, i.e. KITTI [9] and Cityscapes [3]. Both
datasets are recorded from driving vehicles. Concerning the
KITTI dataset, we employ the training and test split of Eigen
et al. [4]. This split is composed of 22,600 training image
pairs, and 697 test pairs. We consider data-augmentation
with online random ﬂipping of the images during training as
in [12]. For Cityscapes, images were collected with higher
resolution. To train our model we combine images from the
densely and coarse annotated splits to obtain 22,973 image-
pairs as in [31]. The test split is composed of 1,525 image-
pairs of the densely annotated split. The evaluation is per-
formed using the pre-computed disparity maps.
Evaluation Metrics. The quantitative evaluation is per-
formed according to several standard metrics used in pre-
vious works [4, 12, 35]. Let P be the total number of pixels
in the test set and ˆdi, di the estimated depth and ground truth
depth values for pixel i. We compute the following metrics:

mean

• Mean relative error (abs rel): 1
• Squared relative error (sq rel): 1
• Root
q 1
• Mean
q 1

error
i=1 k log ˆdi − log di k2

i=1( ˆdi − di)2,

P PP
P PP

squared

log 10

i=1

P PP
P PP

i=1
error

k ˆdi−dik

di

,

k ˆdi−dik2

,

di

(rmse):

(rmse

log):

• Accuracy with threshold τ , i.e.the percentage of ˆdi
) < ατ . We employ α =

such that δ = max( di
ˆdi
1.25 and τ ∈ [1, 2, 3] following [4].

ˆdi
di

,

4.2. Baselines for Ablation.

To perform the ablation study presented in Sec.4.3, we

consider the following baselines:

9772

• half-cycle: our basic building block, uses the forward
branch that takes Ir as input and generates dl to re-
construct the other stereo view ˆIl. Neither cycle-
consistency nor self-distillation are used in this model.
• cycle: a backward network is added to the half-cycle
model in order to reconstruct ˆIr from the estimated ˆIl.
Note that the backward network is used only at training
time. At test time, the output is the same as for the
half-cycle model.

• teacher, we stack the inconsistency-aware network af-

ter the cycle as described in Sec 3.3.

• student: the output of the inconsistency-aware network
is distilled in order to reﬁne the ﬁrst half-cycle. At test
time, the output and the computation complexity are
the same as in the half-cycle model.

In Tables 1, 2 and 3 we indicate with HC, C, T and S, the
half-cycle, cycle, teacher and student respectively; feat and
disp denote self-distillations of features and disparities.
Training Procedure. The whole network is trained follow-
ing an iterative procedure. First, we start by training the
forward half-cycle network for 10 epochs. In a second step,
we train the backward network decoder for 5 epochs with-
out updating the ﬁrst half-cycle network. The whole cy-
cle is then jointly trained for further 10 epochs. Then, the
inconsistency-aware module is pretrained for 5 epochs. Fi-
nally, the whole network is jointly ﬁne-tuned for 10 epochs.
Parameters. The model is implemented with the deep
learning library TensorFlow. Similarly to [12], the input
images are down-sampled to a resolution of 512 × 256 from
the original sizes which are 1226×370 for the KITTI dataset
and for CityScapes. In all our experiments we use a batch
size equal to 8 stereo image pairs and the Adam optimizer
with learning rate set to 10−5 following the recommenda-
tions of [22].

The half-cycle and cycle networks are trained with the
following loss parameters λs = 1, λb = 0.1 and λt = 0.
When training the teacher network we use λs = 0, λb = 0
and λt = 1. We weight the distillation loss Ldist with
λdist = 0.005 and λdist = 0.1 respectively, if feature distil-
lation or disparity distillation is applied. The joint training
of the full network is done with learning rate lr = 10−5,
loss parameters λs = 1, λb = 0.1, λt = 1 and λdist equal
to 0.005 in the case feature distillation and 0.1 in the case
of disparity distillation, respectively.

4.3. Results

Ablation Study. To demonstrate the validity of the pro-
posed contributions we ﬁrst conduct an ablation study on
the KITTI dataset [9] and the CityScapes dataset [3]. Re-
sults are shown in Table 1 and Table 3, respectively.

We split the ablation in two parts where we employ two
different reconstruction loss variants. For the ﬁrst part, as
in [12], we use a multi-scale reconstruction loss where the

Method

HC
C

T feat
T disp

S feat
S disp

T feat
T disp

S feat
S disp

Abs Rel

Sq Rel RMSE RM SElog

δ < 1.25

δ < 1.252

δ < 1.253

lower is better

higher is better

0.1487
0.1451

0.1220
0.1234

0.1438
0.1438

0.1017
0.0983

0.1474
0.1424

1.2942
1.2943

1.0433
1.0509

1.2806
1.2551

0.8930
0.8306

1.2416
1.2306

5.800
5.850

5.321
5.283

5.834
5.771

4.768
4.656

5.849
5.785

0.246
0.242

0.229
0.228

0.241
0.238

[11] L1 loss

0.206
0.202

0.241
0.239

0.805
0.796

0.834
0.834

0.797
0.797

0.878
0.882

0.788
0.795

0.925
0.924

0.933
0.934

0.926
0.927

0.946
0.948

0.923
0.924

0.965
0.967

0.968
0.968

0.968
0.969

0.972
0.973

0.968
0.968

Table 1. Ablation study on KITTI dataset using the training and
testing split proposed by Eigen et al. [4]. The upper part shows
the results with the multiscale reconstruction L1 loss in [12], the
bottom part with the L1 loss proposed in [11].

Method

1-CN C
1-CN S disp
Ours S disp

1-CN T disp
Ours T disp

Abs Rel

Sq Rel RMSE RM SElog

δ < 1.25

δ < 1.252

δ < 1.253

lower is better

1.3326
1.2622
1.2551

1.3609
1.0509

5.837
5.868
5.771

5.952
5.283

0.1533
0.1503
0.1438

0.1478
0.1234

higher is better

0.240
0.243
0.238

0.243
0.228

0.785
0.783
0.797

0.793
0.834

0.919
0.918
0.927

0.921
0.934

0.967
0.967
0.969

0.966
0.968

Table 2. Ablation study where our two-network cycle is replaced
by the single-network cycle from Yang et al. [39] (referred as to
1-CN).

Method

HC
C

T feat
T disp

S feat
S disp

T feat
T disp

S feat
S disp

Abs Rel

Sq Rel RMSE RM SElog

δ < 1.25

δ < 1.252

δ < 1.253

lower is better

higher is better

0.4676
0.4523

0.4087
0.3988

0.4494
0.4467

0.3878
0.3846

0.4455
0.4305

7.3992
6.2604

5.8777
5.8752

6.2599
5.9012

5.8190
6.2007

6.2748
5.9552

5.741
5.381

4.394
4.293

5.343
5.297

4.123
4.476

5.366
5.281

0.493
0.557

0.334
0.316

0.421
0.473

[11] L1 loss

0.397
0.318

0.468
0.519

0.735
0.736

0.846
0.848

0.739
0.736

0.861
0.864

0.739
0.740

0.890
0.888

0.940
0.941

0.891
0.890

0.945
0.945

0.891
0.891

0.945
0.946

0.967
0.968

0.947
0.946

0.969
0.969

0.946
0.946

Table 3. Ablation study on the Cityscapes dataset. The upper
part shows the results with the multiscale reconstruction L1 loss
in [12], the bottom part with the L1 loss proposed in [11].

smaller scale reconstruction is compared with a downsam-
pled version of the stereo image. In contrast with that, for
the second part, we employ a more effective reconstruc-
tion loss, upsampling to input scale all the disparities before
warping as described in Sec. 3.4.

In Table 1 it is interesting to note that our intuition of
self-constraining the monocular student network with cy-
cled design improves, without requiring additional losses,
in several of the metrics compared to the simple forward
branch. This comes at the cost of doubling the forward
propagation time at training but not at testing time. More-
over, the monocular cycled structure has the big advantage
of automatically computing the inconsistency of the recon-
struction both at training and testing time. Therefore, stack-
ing a network aware of the inconsistencies and previous es-
timations, the teacher network, improves the performance.
We observe that our proposed inconsistency-aware network
brings an important improvement consistent over all the
metrics, e.g. 14% and 18% in Abs Rel and Sq Rel, respec-

9773

RGB Image

Eigen et al. [4]

Garg et al. [7]

Godard et al. [12]

Pilzer et al. [31]

Ours

GT Depth Map

Figure 3. Qualitative comparison of different state-of-the-art models with our teacher network on the KITTI testing split proposed by [4].
The sparse KITTI ground truth depth maps are interpolated with bilinear interpolation for better visualization.

Method

Sup Video

Abs Rel

Sq Rel RMSE RM SElog

δ < 1.25

δ < 1.252

δ < 1.253

lower is better

higher is better

Eigen et al. [4]
Xu et al. [37]
Jiang et al.[17]
Gan et al. [6]
Guo et al. [13]

Yang et al. [39]
Zou et al.[44]
Godard et al.[11]

Zhou et al. [42]
Garg et al. [7]
Kundu et al. [18], 50m
Godard et al. [12]
Pilzer et al. [31]

Ours Student
Ours Teacher

Y
Y
Y
Y
Y

Y
N
N

N
N
N
N
N

N
N

N
N
N
N
N

Y
Y
Y

N
N
N
N
N

N
N

0.190
0.132
0.131
0.098
0.097

0.097
0.150
0.115

0.208
0.169
0.203
0.148
0.152

1.515
0.911
0.937
0.666
0.653

0.734
1.124
1.010

1.768
1.08
1.734
1.344
1.388

0.1424
0.0983

1.2306
0.8306

7.156

-

5.032
3.933
4.170

4.442
5.507
5.164

6.856
5.104
6.251
5.927
6.016

5.785
4.656

0.270
0.162
0.203
0.173
0.170

0.187
0.223
0.212

0.283
0.273
0.284
0.247
0.247

0.239
0.202

0.692
0.804
0.827
0.890
0.889

0.888
0.806
0.858

0.678
0.740
0.687
0.803
0.789

0.795
0.882

0.899
0.945
0.946
0.964
0.967

0.958
0.933
0.946

0.885
0.904
0.899
0.922
0.918

0.924
0.948

0.967
0.981
0.981
0.985
0.986

0.980
0.973
0.97

0.957
0.962
0.958
0.964
0.965

0.968
0.973

Table 4. Comparison with the state of the art. Training and testing are performed on the KITTI [9] dataset. Supervised and semi-supervised
methods are marked with Y in the supervision (Sup.) column, unsupervised methods with N. Methods using a frame sequence in input
and, thus, exploiting temporal information either at training or testing time, are marked with Y in the Video column. Numbers are obtained
on Eigen [4] test split with Garg [7] image cropping. Depth predictions are capped at the common threshold of 80 meters, if capped at 50
meters we specify it. Best scores among static unsupervised methods are in bold. Best scores among other method categories are in italic.

tively, comparing cycle and teacher.

Student-teacher distillation leads to a consistent im-
provement over all metrics, demonstrating that self-
distillation improves the student, while keeping the perfor-
mance of teacher constant. Regarding the two distillation
strategies, we found that network with disparity distillation
converges faster than that with the feature distillation. This
is not unexpected, given the much more compact size of the
disparity compared to the several channels of the features.

For demonstrating the validity of the design of our cy-
cle network, we perform an ablation study where our two-

network cycle structure is replaced by the single-network
cycle proposed by Yang et al. [39]. In this experiment, we
use our proposed inconsistency-aware module to exploit the
inconsistency estimated by the single network cycle in [39].
Contrary to [39], we trained the models without supervi-
sion in order to compare the two different approaches in
the unsupervised setting. We use the L1 loss from [12]
for fair comparison. Results are reported in Table 2. We
observe that the inconsistency estimates obtained with the
single-network cycle of [39] are associated with worse per-
formance with respect to those of our method.

9774

RGB Image

half-cycle

cycle

student

teacher

GT Depth Map

Figure 4. Qualitative comparison of different baseline models of the proposed approach on the Cityscapes testing dataset.

We also performed an ablation study on the Cityscapes
dataset in Table 3, following the evaluation procedure pro-
posed in [31]. The results conﬁrm the trends observed on
KITTI. The cycle network improves over the half-cycle in
ﬁve metrics out of seven. The teacher, effectively exploit-
ing inconsistencies, is associated with an improvement on
all error metrics (ranging from 7% to 20%). Distillation fur-
ther provides a boost in performance of about 1.5% to 5%.
In the second part of the ablation study, the teacher further
improves its estimations gaining over 20% over the initial
cycle setting. More interesting is the gain in performance of
the student that improves from 2% to 5%.

In Fig. 4, we present qualitative results for Cityscapes.
half-cycle and cycle images are smooth and do not present
artifacts. The teacher provides more accurate depth maps
with sharper edges for small objects and better background
estimations (e.g. third row, people in the back). After dis-
tillation also the student inherits this ability and we observe
more detailed predictions compared to the original cycle.

4.4. Comparison with State of the Art

In Table 4 we compare with several state-of-the-
art works, considering both supervised learning-based (
Eigen et al. [4], Xu et al. [37], Jiang et al. [17], Gan et
al. [6], Guo et al. [13], Yang et al. [39]) and unsupervised
learning-based (Zhou et al. [42], Garg et al. [7], Kundu et
al. [18], Godard et al. [12], Pilzer et al. [31], Godard et
al. [11] and Zou et al.[44]) methods.

The teacher network reaches state-of-the-art perfor-
mance for the frame-level unsupervised setting, even im-
proving over the state-of-the-art method that use depth su-
pervision as [37], and is competitive with those using depth
and video clues [6, 13, 39]. Note that Yang et al. [39] con-
sider a similar setting to ours proposing to use errors to
reﬁne the depth estimation with a stacked network. Our

method has several advantages though: it is unsupervised,
it does not consider multiple video frames and it avoids
the use of several losses whose hyper-parameters are hard
to tune. Furthermore, as demonstrated by our experiments
in Table 2, our approach adopts a more effective network
structure for computing cycle inconsistencies. The student
network, after distillation, improves on unsupervised ap-
proaches with similar network capacity like [7, 12, 31] and
it is only outperformed by previous unsupervised methods
that exploit additional information during training like [11].
Qualitative results in Figure 3 show that our model pre-
dicts more accurately challenging areas, i.e. sky, trees in
background and shadowed areas difﬁcult to interpret, com-
pared to competitive unsupervised models [7, 12, 31]. Note
that small details are better reconstructed by [12] but, over-
all, our estimations look smoother and have fewer large er-
rors, as the train windshield in row seven.

5. Conclusions

We proposed a monocular depth estimation network
which computes the inconsistencies between input and
cycle-reconstructed images and exploit them to generate
state-of-the-art depth predictions through a reﬁnement net-
work. We proved that distillation is an effective paradigm
for depth estimation and improve the student network per-
formance by transferring information from the reﬁnement
network.
In future work we plan to further improve the
distillation process by accounting for teacher and student
conﬁdence in the estimates. In this way we expect to bet-
ter guide the learning process and correct more effectively
prediction inconsistencies.

6. Acknowledgement

We want to thank the NVIDIA Corporation for the dona-

tion of the GPUs used in this project.

9775

References

[1] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Man-
mohan Chandraker. Learning efﬁcient object detection mod-
els with knowledge distillation. In NIPS, 2017.

[2] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016.

[3] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016.

[4] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In NIPS, 2014.

[5] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation. In CVPR, 2018.

[6] Yukang Gan, Xiangyu Xu, Wenxiu Sun, and Liang Lin.
Monocular depth estimation with afﬁnity, vertical pooling,
and label enhancement. In ECCV, 2018.

[7] Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, and Ian
Reid. Unsupervised cnn for single view depth estimation:
Geometry to the rescue. In ECCV, 2016.

[8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
IJRR,

Urtasun. Vision meets robotics: The kitti dataset.
2013.

[9] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In CVPR, 2012.

[10] Ross Girshick. Fast r-cnn. In ICCV, 2015.

[11] Cl´ement Godard, Oisin Mac Aodha, and Gabriel Brostow.
Digging into self-supervised monocular depth estimation.
arXiv preprint arXiv:1806.01260, 2018.

[12] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. In CVPR, 2017.

[13] Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and
Xiaogang Wang. Learning monocular depth by distilling
cross-domain stereo networks. In ECCV, September 2018.

[14] Saurabh Gupta, Judy Hoffman, and Jitendra Malik. Cross

modal distillation for supervision transfer. In CVPR, 2016.

[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the
knowledge in a neural network. NIPS Workshop on Deep
Learning, 2015.

[16] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and
Koray Kavukcuoglu. Spatial transformer networks.
In C.
Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R.
Garnett, editors, NIPS. 2015.

[17] Huaizu

Jiang,

Gustav

Larsson, Michael Maire
Greg Shakhnarovich, and Erik Learned-Miller.
Self-
supervised relative depth learning for urban scene under-
standing. In ECCV, 2018.

[18] Jogendra Nath Kundu, Phani Krishna Uppala, Anuj Pahuja,
and R Venkatesh Babu. Adadepth: Unsupervised content
congruent adaptation for depth estimation. In CVPR, 2018.

[19] Yevhen Kuznietsov, J¨org St¨uckler, and Bastian Leibe. Semi-
supervised deep learning for monocular depth map predic-
tion. CVPR, 2017.

[20] Lubor Ladicky, Jianbo Shi, and Marc Pollefeys. Pulling

things out of perspective. In CVPR, 2014.

[21] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In 3DV, 2016.

[22] St´ephane Lathuilire, Pablo Mesejo, Xavier Alameda-Pineda,
and Radu Horaud. A comprehensive analysis of deep regres-
sion. To appear in TPAMI, 2019.

[23] Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao,
Jiebo Luo, and Li-Jia Li. Learning from noisy labels with
distillation. In ICCV, 2017.

[24] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.
Learning depth from single monocular images using deep
convolutional neural ﬁelds. TPAMI, 2016.

[25] Ping Luo, Zhenyao Zhu, Ziwei Liu, Xiaogang Wang, Xiaoou
Tang, et al. Face model compression by distilling knowledge
from neurons. In AAAI, 2016.

[26] Wenjie Luo, Alexander G Schwing, and Raquel Urtasun. Ef-

ﬁcient deep learning for stereo matching. In CVPR, 2016.

[27] Reza Mahjourian, Martin Wicke, and Anelia Angelova. Un-
supervised learning of depth and ego-motion from monocu-
lar video using 3d geometric constraints. In CVPR, 2018.

[28] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical ﬂow, and scene ﬂow estimation. In CVPR, 2016.

[29] Xuecheng Nie, Jiashi Feng, Junliang Xing, and Shuicheng
Yan. Pose partition networks for multi-person pose estima-
tion. In ECCV, 2018.

[30] David Novotny, Samuel Albanie, Diane Larlus, and Andrea
Vedaldi. Semi-convolutional operators for instance segmen-
tation. In ECCV, 2018.

[31] Andrea Pilzer, Dan Xu, Mihai Puscas, Elisa Ricci, and Nicu
Sebe. Unsupervised adversarial depth estimation using cy-
cled generative networks. In 3DV, 2018.

[32] Fabio Tosi, Filippo Aleotti, Matteo Poggi, and Stefano Mat-
toccia. Learning monocular depth estimation by infusing tra-
ditional stereo knowledge. In CVPR, 2019.

[33] Chaoyang Wang, Jose Miguel Buenaposada, Rui Zhu, and
Simon Lucey. Learning depth from monocular videos using
direct methods. In CVPR, 2018.

[34] Chaoyang Wang, Jos´e Miguel Buenaposada, Rui Zhu, and
Simon Lucey. Learning depth from monocular videos using
direct methods. In CVPR, 2018.

[35] Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian
Price, and Alan Yuille. Towards uniﬁed depth and seman-
tic prediction from a single image. In CVPR, 2015.

[36] Rui Wang, Martin Schw¨orer, and Daniel Cremers. Stereo
dso: Large-scale direct sparse visual odometry with stereo
cameras. In ICCV, 2017.

9776

[37] Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, and
Nicu Sebe. Monocular depth estimation using multi-scale
continuous crfs as sequential deep networks. TPAMI, 2018.
[38] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and
Elisa Ricci. Structured attention guided convolutional neural
ﬁelds for monocular depth estimation. In CVPR, 2018.

[39] Nan Yang, Rui Wang, Jorg Stuckler, and Daniel Cremers.
Deep virtual stereo odometry: Leveraging deep depth predic-
tion for monocular direct sparse odometry. In ECCV, 2018.
[40] Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, and Ram
Nevatia. Every pixel counts: Unsupervised geometry learn-
ing with holistic 3d motion understanding. arXiv preprint
arXiv:1806.10556, 2018.

[41] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasek-
era, Kejie Li, Harsh Agarwal, and Ian Reid. Unsuper-
vised learning of monocular depth estimation and visual
odometry with deep feature reconstruction. arXiv preprint
arXiv:1803.03893, 2018.

[42] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In CVPR, 2017.

[43] Wei Zhuo, Mathieu Salzmann, Xuming He, and Miaomiao
Liu. Indoor scene structure analysis for single image depth
estimation. In CVPR, 2015.

[44] Yuliang Zou, Zelun Luo, and Jia-Bin Huang. Df-net: Un-
supervised joint learning of depth and ﬂow using cross-task
consistency. In ECCV, 2018.

9777

