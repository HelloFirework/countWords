Learning Pyramid-Context Encoder Network for High-Quality Image Inpainting

Yanhong Zeng1
2, Baining Guo3
1School of Data and Computer Science, Sun Yat-sen University, Guangzhou, P.R. China

2 ∗, Jianlong Fu3, Hongyang Chao1

,

,

2The Key Laboratory of Machine Intelligence and Advanced Computing (Sun Yat-sen University),

Ministry of Education, Guangzhou, P.R. China

3Microsoft Research, Beijing, P.R. China

zengyh7@mail2.sysu.edu.cn, {jianf,bainguo}@microsoft.com, isschhy@mail.sysu.edu.cn

Figure 1: High-quality image inpainting results generated by the proposed Pyramid-context ENcoder Network (PEN-Net).
In each pair, the left is a damaged image masked in white, and the right is the result of image inpainting. PEN-Net shows
excellent performance on a variety of images, including facades, natural scene, face and texture. [Best viewed in color]

Abstract

High-quality image inpainting requires ﬁlling missing
regions in a damaged image with plausible content. Exist-
ing works either ﬁll the regions by copying image patches or
generating semantically-coherent patches from region con-
text, while neglect the fact that both visual and semantic
plausibility are highly-demanded.
In this paper, we pro-
pose a Pyramid-context ENcoder Network (PEN-Net) for
image inpainting by deep generative models. The PEN-Net
is built upon a U-Net structure, which can restore an image
by encoding contextual semantics from full resolution input,
and decoding the learned semantic features back into im-
ages. Speciﬁcally, we propose a pyramid-context encoder,
which progressively learns region afﬁnity by attention from

∗This work was performed when the ﬁrst author was visiting Microsoft

Research as a research intern.

a high-level semantic feature map and transfers the learned
attention to the previous low-level feature map. As the miss-
ing content can be ﬁlled by attention transfer from deep to
shallow in a pyramid fashion, both visual and semantic co-
herence for image inpainting can be ensured. We further
propose a multi-scale decoder with deeply-supervised pyra-
mid losses and an adversarial loss. Such a design not only
results in fast convergence in training, but more realistic re-
sults in testing. Extensive experiments on various datasets
show the superior performance of the proposed network.

1. Introduction

Image inpainting aims at ﬁlling missing pixels in a dam-
aged image given a corresponding mask [2]. This task has
drawn great attention and become a valuable and active re-
search topic for decades [5, 12, 17], because high-quality

1486

image inpainting can beneﬁt a broad range of applications,
such as old photo restoration, object removal, and so on.

High-quality image inpainting usually requires syn-
thesizing not only visually-realistic but semantically-
reasonable content for missing regions [3, 5, 28, 29, 31].
Existing approaches can be roughly divided into two
groups. As shown in Table 1, the ﬁrst group inspired by tex-
ture synthesis techniques attempts to ﬁll regions at image-
level [1, 5, 22]. Speciﬁcally, such approaches usually sam-
ple and paste full image resolution patches from source im-
ages into missing regions, which allows synthesizing re-
sults with details. However, as the lack of high-level under-
standing of an image, such approaches often fail in generat-
ing semantically-reasonable results. To solve this problem,
the second group of approaches proposes to encode the se-
mantic context of an image into a latent feature space by
deep neural networks and then generate semantic-coherent
patches by generative models [13, 17, 31]. However, it re-
mains challenging to generate visually-realistic results from
a compact latent feature, as full image resolution details can
be usually smoothed by stacked convolutions and poolings.

To ensure that both visual and semantic coherence can
be satisﬁed, we propose to ﬁll regions at both image and
feature levels. First, we adopt a U-Net [19] structure as our
backbone, which can encode the context from low-level pix-
els to high-level semantic features and decode the features
back into an image. Speciﬁcally, we propose a Pyramid-
context ENcoder Network (PEN-Net) with three tailored
key components, i.e., a pyramid-context encoder, a multi-
scale decoder, and an adversarial training loss, to boost the
capacity of U-Net in image inpainting. Second, once the
compact latent features have been encoded from images,
the pyramid-context encoder ﬁlls regions from high-level
semantic features to low-level features (with richer details)
in a pyramid pathway before decoding. To this end, we pro-
pose an Attention Transfer Network (ATN) to learn region
afﬁnity between patches inside/outside missing regions in
a high-level feature map, and then transfer (i.e., weighted
copy by afﬁnity) relevant features from outside into in-
side regions of previous feature map with higher resolution.
Third, the proposed multi-scale decoder takes as input the
reconstructed features from ATNs through skip connections
and the latent features for ﬁnal decoding. The PEN-Net
is optimized by minimizing deeply-supervised pyramid L1
losses and an adversarial loss.

To the best of our knowledge, the proposed PEN-Net
is the ﬁrst work that is able to ﬁll missing regions at both
image-level and feature-level for image inpainting. we
highlight our contributions as follows:

• Cross-layer attention transfer. We propose a novel
network, ATN, to learn region afﬁnity from high-level
feature maps (e.g., the compact latent features in the
encoder). The resultant afﬁnity map can guide feature

Category

Method

Semantic Details

image
level

feature
level

Ours

PatchMatch[1],
Region ﬁlling[5]
GL[9],
PConv[13],
GntIpt[31]

(cid:2)

(cid:2)

(cid:2)

(cid:2)

Table 1: Two groups of typical approaches for image in-
painting. PatchMatch [1] and Region ﬁlling [5] ensure that
patches with more details can be used for ﬁlling, while
GL [9], Pconv [13] and GntIpt [31] can generate semantic-
coherent results. Compared with those methods, our ap-
proach can satisfy both semantic and visual requirements.

transfer in adjacent low-level layers in an encoder.

• Pyramid ﬁlling. Our model can ﬁll holes multiple
times (depends on the depth of the encoder) by repeat-
ing using ATNs from deep to shallow, which can re-
store an image with more ﬁne-grained details.

2. Related Work

Image inpainting by patch-based methods.

Patch-
based methods were ﬁrst proposed for texture synthe-
sis [6, 7]. They were then applied in image inpainting to
ﬁll missing regions at image level [24]. They usually sam-
ple and paste similar patches from database or undamaged
surroundings into missing regions based on distance met-
rics between patches (e.g. Euclidean distance, SIFT dis-
tance [15], etc.). Bertalmio et al. proposed to combine
patch-based texture synthesis techniques with diffusion-
based propagation under image decomposition [3]. A num-
ber of approaches try to improve performance by providing
better ﬁlling order or optimal patches [5, 22, 27]. Patch-
Match was proposed for quickly ﬁnding similar matches
between image patches [1]. Patch-based methods for im-
age inpainting are able to generate sharp results similar
with context. However, it’s hard to generate semantically-
reasonable results by patch-based methods, due to the lack
of high level understanding of images.

Image inpainting by deep generative models. Deep
generative models for image inpainting usually encode an
image into a latent feature, ﬁll missing regions at the
feature-level, and decode the feature back into an image.
Promising results have been achieved by deep generative
models recently. Based on deep feature learning and adver-
sarial training, Context Encoder, one of the ﬁrst deep gener-
ative models, is able to give reasonable results for semantic
hole-ﬁlling [17]. Guidance loss was introduced to make the
feature maps generated in decoder as close as possible to the
feature maps of ground-truth generated in encoder [28]. Di-
lated convolutions [30] were introduced to increase recep-

1487

(a) Pyramid-context Encoder

high-level semantics
L(cid:73)

(b) Multi-scale Decoder

deconv

1L(cid:73)(cid:16)

l(cid:73)

1(cid:73)

ATN

deep

skip

connections

1L(cid:92)(cid:16)

l(cid:92)

1L(cid:77)(cid:16)
l(cid:77)

RGB

pyramid 
L1 loss

1(cid:92)

ATN

shallow

1(cid:77)

low-level pixels

image + mask

l(cid:92)
high-level 
feature map 

extract 
patches

(c) Attention Transfer Network (ATN)

dilated conv.

match

attention score
1l(cid:73) (cid:16)

low-level 
feature map

fill

filled feature

3x3
rate=1
3x3
rate=2
3x3
rate=4
3x3
rate=8

D

adversarial loss

(d) Discriminator (D)

real or fake

real or fake

1l(cid:92) (cid:16)

reconstructed 

feature

ground 
truth

output 

input

Figure 2: The Pyramid-context Encoder Network (PEN-Net) is proposed to boost the capability of U-Net in image inpainting
with three tailored components, i.e., a pyramid-context encoder (a), a multi-scale decoder (b), and an adversarial training
loss (d). First, once the compact latent feature has been encoded, the pyramid-context encoder further improves the encoding
effectiveness by ﬁlling regions from high-level feature maps to low-level feature maps (with richer details) through the
proposed Attention Transfer Network (ATN) (c). Second, the multi-scale decoder takes as input the reconstructed features
from ATNs through skip connections and the latent features for decoding. Finally, the decoder decodes the features back into
an image. The whole network is optimized by minimizing pyramid L1 losses and an adversarial loss. [Best viewed in color.]

tive ﬁeld in completion network by Iizuka et al. [9]. Special
convolution operations such as PConv [13] and ShCNN [18]
were designed for eliminating the effects caused by the
placeholder values in masked regions in an image. Contex-
tual attention layer [31] and Patch-swap layer [21] were pro-
posed for ﬁlling missing pixels with similar patches from
undamaged regions at high-level feature maps. Inspired by
image stylization, MNPS proposed to optimize texture de-
tails by using a pre-trained classiﬁcation network during in-
ference [29]. Isola et al. try to solve image inpainting by
a general image translation framework [10]. Leveraging
high-level semantic feature learning, deep generative mod-
els are able to generate semantically-coherent results for the
missing regions. However, it remains challenging to gener-
ate visually-realistic results from a compact latent feature.

3. Pyramid-context Encoder Network

The Pyramid-context Encoder Network (PEN-Net) con-
sists of three parts (as shown in Figure 2), i.e., a pyramid-
context encoder (a), a multi-scale decoder (b) and a discrim-
inator (d). The PEN-Net is built upon a U-Net structure,
which can encode a damaged image with mask from full
input resolution pixels into a compact latent features and
decode the features back into an image.

As the compact latent features encode the semantics of
the context, the pyramid-context encoder can further im-
proves the encoding effectiveness by ﬁlling missing re-
gions from the compact latent feature to low-level fea-
tures (with higher resolution and richer details).
It ﬁlls
holes by repeating using the proposed Attention Transfer
Network (ATN) (c) multiple times (according to the depth
of the encoder) before decoding. Speciﬁcally, an ATN
learns region afﬁnity between patches inside/outside miss-
ing regions from high-level semantic features, and the
learned attention is transferred to ﬁll regions (i.e., weighted
copy from the context by afﬁnity) in its previous feature
map with higher resolution. Multi-scale information is fur-
ther aggregated to reﬁne the ﬁlled features by four groups of
dilated convolutions with different rates in an ATN. Finally,
the multi-scale decoder takes as input the reconstructed fea-
tures from ATNs through skip connections and the latent
features for decoding.
In addition to an adversarial loss,
pyramid L1 losses are used to progressively reﬁne the pre-
diction output by the decoder at all scales.

We describe details of the pyramid-context encoder and
the ATN in Section 3.1. The multi-scale decoder and pyra-
mid L1 losses are introduced in Section 3.2 followed by ad-
versarial training loss described in Section 3.3.

1488

3.1. Pyramid-context encoder

ﬁlled with context weighted by the attention score:

Pyramid-context encoder In order to improve the ef-
fectiveness of encoding, the pyramid-context encoder is
proposed for ﬁlling missing regions before decoding. Once
a compact latent feature is learned, the pyramid-context en-
coder ﬁlls regions from high-level semantic features to low-
level features (with higher resolution) by repeating using the
proposed ATNs in a pyramid fashion. Under the assump-
tion that pixels with similar semantics should have similar
details, an ATN is applied at each level to learn region afﬁn-
ity from high-level semantic features, thus the learned re-
gion afﬁnity can further guide feature transfer inside/outside
missing regions in an adjacent layer with higher resolution.
Given a pyramid-context encoder of L layers, we denote
the feature maps from deep to shallow as φL, φL−1, ..., φ1
as shown in (a) of Figure 2. The features constructed by
ATNs in each layer from deep to shallow are denoted as:

ψL−1 = f (φL−1, φL),
ψL−2 = f (φL−2, ψL−1),

· · · ,

(1)

ψ1 = f (φ1, ψ2) = f (φ1, f (φ2, ...f (φL−1, φL))),

where we denote the operation of the ATN as f . By such
a cross-layer attention transfer and pyramid ﬁlling mecha-
nism, both visual and semantic coherence for the missing
regions can be ensured. The details of f (i.e., ATN) are
introduced as below.

Attention Transfer Network We follow state-of-the-
art approaches to ﬁll missing regions by using attention
[21, 28, 31]. The attention is usually obtained by region
afﬁnity between patches (usually 3×3) inside/outside miss-
ing regions, thus relevant features outside can be transferred
(i.e., weighted copy from the context by afﬁnity) into inside
regions. As shown in (c) of Figure 2, the ATN ﬁrst learns
region afﬁnity from a high-level feature map, ψl. It extracts
patches from ψl and calculate the cosine similarity between
patches inside and outside missing regions:

sl
i,j = (cid:2)

,

pl
i
(cid:2)(cid:2)pl
i(cid:2)(cid:2)2

pl
j
(cid:2)(cid:2)pl
j(cid:2)(cid:2)2

(cid:3),

(2)

where pl
i is the i-th patch extracted from ψl outside mask,
pl
j is the j-th patch extracted from ψl inside the mask. Then
softmax is applied on the similarities to obtain the attention
score for each patch:

αl

j,i =

exp(sl
i,j)
i=1 exp(sl

(cid:3)N

i,j)

.

(3)

After obtaining the attention score from a high-level feature
map, the holes in its adjacent low-level feature map can be

pl−1
j =

N

(cid:4)

i=1

j,i pl−1
αl

i

,

(4)

i

where pl−1
is the i-th patch extracted from φl−1 outside
masked regions, and pl−1
is the j-th patch to be ﬁlled in
missing regions. After calculating all patches, we can ﬁ-
nally obtain a ﬁlled feature ψl−1 by attention transfer from
ψl. In particular, all these operations can be formulated into
convolution operations for end-to-end training [31].

j

We propose to further reﬁne the ﬁlled features in an ATN
as shown in (c) of Figure 2. Speciﬁcally, multi-scale contex-
tual information can be aggregated by four groups of dilated
convolutions with different rates. Such a design ensures
structure coherence with context in the ﬁnal reconstructed
features, which improves the inpainting results in testing.

3.2. Multi-scale decoder

Multi-scale decoder The proposed multi-scale decoder
takes as input the reconstructed features from ATNs through
skip connections and the latent features from the encoder.
We denote the feature maps generated by the multi-scale
decoder as ϕL−1, ϕL−2, ..., ϕ1 from deep to shallow, which
are obtained as follows:

ϕL−1 = g(ψL−1 ⊕ g(φL)),
ϕL−2 = g(ψL−2 ⊕ ϕL−1),

· · · ,

ϕ1 = g(ψ1 ⊕ ϕ2),

(5)

where g denotes transposed convolution operation, ⊕ de-
notes feature concatenation, and ψl is the reconstructed fea-
ture from an ATN in the l-th layer of the encoder.

On one hand, the reconstructed features generated by
ATNs encode more low-level information for missing re-
gions. Such a design enables the decoder to generate visu-
ally realistic results with ﬁne-grained details. On the other
hand, the features obtained from the compact latent features
by convolutions are able to synthesize novel objects in miss-
ing regions, even when the objects cannot be found outside
missing regions. Combining those two kinds of features,
the decoder is able to synthesize novel objects with high
coherence in semantics and textures with the context of the
image. For example, the proposed decoder is able to syn-
thesize eyes in human face images with both eyes masked.
Pyramid L1 losses We also propose deeply-supervised
pyramid L1 losses to progressively reﬁne the predictions for
missing regions at each scale. Speciﬁcally, each pyramid
loss is a normalized L1 distance between a prediction of
speciﬁc scale and the corresponding ground truth:

Lpd =

L−1

(cid:4)

l=1

(cid:2)(cid:2)xl − h(ϕl)(cid:2)(cid:2)1 ,

(6)

1489

(a) Input

(b) PatchMatch

(c) GL

(d) CA

(e) PConv

(f) Ours

Figure 3: Qualitative comparisons with baselines on four datasets with different characteristics. In each row, the ﬁrst image
is the input with a large mask in the center (i.e., 128 × 128), and the left images from left to right are the results generated by
PatchMatch [1], GL [9], CA [31], PConv [13] and our model respectively. [Best viewed with zoom-in.]

where h denotes a 1 × 1 convolution which decodes ϕl into
an RGB image with the same size, and xl is the ground truth
scaled to the same size as ϕl. The overall objective function
incorporating pyramid L1 losses and an adversarial loss is
described in the next section.

3.3. Adversarial training loss

As image inpainting is an ill-posed problem that there
are many possible results for the missing regions, we use
adversarial training to select the most realistic one. Adver-
sarial training usually involves a generator (G) and a dis-
criminator (D), which aims at achieving a Nash equilibrium,
so that fake data generated by the generator cannot be dis-
tinguished from real data by the discriminator. As shown
in (d) of Figure 2, the pyramid-context encoder and the
multi-scale decoder form a generator, and we adopt Patch-
GAN [10] as our discriminator. Spectral normalization is
used in the discriminator to stabilize the training [16].

We ﬁrst deﬁne the ﬁnal prediction from the generator as:

z = G(x ⊙ (1 − M ), M ) ⊙ M + x ⊙ (1 − M ),

(7)

where x is the ground truth, ⊙ is an element-wise multipli-
cation, M is the mask where 1 labels missing regions and 0
labels context. The hinge version of the adversarial loss for
the discriminator can be denoted as:

LD = Ex∼pdata [max(0, 1 − D(x))]
+ Ez∼pz [max(0, 1 + D(z))] ,

(8)

where D(x) and D(z) are the logits output from D. The
adversarial loss for the generator can be denoted as:

LG = −Ez∼pz [D(z)] .

(9)

The whole PEN-Net is optimized by minimizing an adver-
sarial loss and pyramid L1 losses deﬁned in Section 3.2. We
deﬁne the overall objective function as:

L = λGLG + λpdLpd.

(10)

1490

4. Experiments

We evaluate the proposed network with baselines from
both quantitative and qualitative aspects. Details of experi-
mental settings are introduced in Section 4.1, and the exper-
iments results are described in Section 4.2, followed by the
analysis of the effectiveness of our model in Section 4.3.

4.1. Experimental settings

Datasets We conduct experiments on four datasets with

different characteristics as below (details in Table 2):

– Facade [25]: a collection of highly-structured facades

from different cities around the world.

– DTD [4], an evolving dataset of 47 kinds of describ-

able textures collected in the wild.

– CELEBA-HQ [11], a high-quality version of the hu-

man face dataset from CELEBA [14].

– Places2 [32], a dataset that contains images of 365

scenes collected from the natural world.

Baselines We compare with the following baselines for

their state-of-the-art performance:

– PatchMatch (PM) [1]: a typical patch-based approach,

which copies similar patches from the surroundings.

– GL [9]: a generative model, which leverages both
global and local discriminators for image completion.
– CA [31]: a two-stage inpainting model, which lever-

ages contextual attention at high-level features.

– PConv [13]: a generative model, which proposes a spe-

cial convolution layer for ﬁlling irregular holes.

Implementation details We use random blocks for
training, following the experimental settings used by base-
lines [9, 31] for fair comparisons. All images are resized to
256 × 256 for training and testing. When extracting hole
and non-hole patches in each level, we use nearest neighbor
down-sampling to evolve the holes. Our full model runs at
0.19 seconds per frame on a GPU TITAN V for images of
size 256 × 256. All the results reported are output directly
from the trained models without using any post-processing.
The code will be made publicly available. 1

4.2. Results

Quantitative comparisons As Places2 contains natural-
world images, which is considered as the most challeng-
ing dataset [9, 31] (compared with Facade/DTD/CELEBA-
HQ), we conduct quantitative comparisons on Places2. All
images are randomly masked with 128 × 128 squares for
testing. We use L1 loss, multi-scale structural similarity
(MS-SSIM) [26], Inception Score (IS) [20] and Fr´echet In-
ception Distance (FID) [8] as evaluation metrics. The re-
sults listed in Table 3 show the comparable performance of
the proposed approach against baselines.

1https://github.com/researchmm/PEN-Net-for-Inpainting

Dataset

Facade [25]

DTD [4]

CELEBA-HQ [11]

#Train

506
4,512
28,000

Places2 [32]

1,803,460

#Test

100
1,128
2,000
36,500

#Total

606
5,560
30,000

1,839,960

Table 2: Training and test splits of four datasets.

Method

L1 Loss† MS-SSIM¶

IS¶

PatchMatch [1]
GL [8]
PConv [12]
CA [29]
PEN-Net (ours)

12.90
9.27
8.92
9.91
9.94

60.00%
73.40%
74.67%
73.02%
78.09%

43.03
42.05
47.00
44.81
50.51

FID†

20.36
19.18
18.39
18.34
15.19

Table 3: Quantitative comparisons on Places2 with L1 Loss,
MS-SSIM, IS and FID. † Lower is better. ¶ Higher is better.

L1 loss can roughly reﬂect the ability of models to re-
construct the original image content. MS-SSIM extracts
and evaluates the similarity of structural information from
paired images in multi-scale to provide a good approxi-
mation to human visual perception. However, there are a
great deal of solutions different from original content for the
missing regions, while L1 loss and MS-SSIM are limited to
comparing with the original image content. Under the as-
sumption that a damaged scene image should maintain the
same attributes after image inpainting, an inpainting result
should be conﬁdently identiﬁed as a speciﬁc category by the
pre-trained classiﬁcation network. To this end, we also use
the inception score as one of the evaluation metrics:

I = exp( E

z∼pz

[(DKL(p(y|z))(cid:6)p(y))]),

(11)

where z is inpainting results deﬁned in Section 3.2, and y is
the label predicted by pre-trained classiﬁcation models. We
use the pre-trained classiﬁcation network released by Zhou
et al. [32]. Besides, FID has driven an increasing attention
and becomes a commonly-used numeric metric in the ﬁeld
of image generation. We also include FID to measure the
Wasserstein-2 distance between real and fake images using
a pre-trained Inception-V3 model [23].

Qualitative comparisons In order to take both visual
and semantic coherence into account, we conduct qualita-
tive comparisons on the test set of four datasets with differ-
ent characteristics, which are highly-structured with ﬁne-
grained textures. We masked the test images with center
128 × 128 squares, and our model shows superior perfor-
mance against the state-of-the-art. As shown in Figure 3,
the typical patch-based method, PatchMatch, is able to gen-
erate clear textures but with distorted structures inconsis-
tent with surrounding areas, while deep generative models
including GL, CA and PConv tend to generate blurry tex-
tures in the ﬁnal results. With the help of cross-layer at-

1491

(a) Input

(b) PatchMatch

(c) GL

(d) CA

(e) PConv

(f) Ours

Figure 4: Qualitative comparisons for image inpainting with irregular masks on Facade. [Best viewed with zoom-in.]

(a) Original 

(b) Input

(c) Ours

(a) Original 

(b) Input

(c) Ours

Figure 5: Example results generated by the proposed net-
work on Places2. [Best viewed with zoom-in.]

Figure 6: Example results generated by the proposed net-
work on CELEBA-HQ. [Best viewed with zoom-in.]

tention transfer and pyramid ﬁlling mechanisms, our model
is able to generate semantically-reasonable and visually-
realistic results with clear textures and consistent structures
with context. We also verify the ability of the proposed net-
work to ﬁll missing regions given irregular masks. Specif-
ically, we use the images of Facade and masks released by
Liu et al. [13] for testing. As shown in Figure 4, the base-
lines tend to create color discrepancies and distorted struc-
tures, while our model outperforms the-state-of-the-art with
consistent colors and structures. More example results gen-
erated by our model on images of natural scene and human
face can be found in Figure 5 and Figure 6.

User study In addition to quantitative and qualitative
comparisons, we also perform two settings of user study,
i.e., paired images and single image user study. The vol-
unteers are all image experts with image processing back-
ground. They are not informed of mask information.

In the ﬁrst setting, over 20 volunteers are invited to eval-
uate the performance of the models on Facade. Each time,
a pair of images generated from different models are shown
to the volunteers in an anonymous way. The volunteers are
asked to choose the more natural one from those two im-
ages. We collected 613 valid votes in total, and the statistics
of the results are shown in Table 4. The statistics show that
our model is ranked better in most of time (82.10%) over
other models. We also found that people prefer clear results
generated by PatchMatch (PM), CA and ours.

In the second setting, we randomly distribute the valida-
tion set of DTD into four groups. Images in three groups
are masked with 32 × 32, 64 × 64 or 128 × 128 squares, and
the last group is unmasked. Over 25 volunteers are invited
to evaluate the naturalness of inpainting results generated
by our model with different mask size. Each time, an image
sampled from real data or our inpainting results is shown to

1492

Method
Percentage

PM

GL

CA

PConv

Ours

40.15% 34.25% 70.30% 23.70% 82.10%

Table 4: Statistics of paired images user study. The value
indicates the percentage of being ranked as better.

Mask size
Percentage

0 (real)
92.66% 82.23% 52.63% 32.70%

128

64

32

Table 5: Statistics of single image user study. The value
indicates the percentage of being considered as real.

Method

L1 loss† ms-ssim¶

IS¶

patch-swap [21]
single ATN (ours)
PEN-Net (ours)

12.13
9.85
9.94

64.00% 29.26
71.61% 37.02
78.09% 50.51

FID†

36.85
26.38
15.19

Table 6: Ablation comparison of cross-layer attention trans-
fer network (ATN) and pyramid ﬁlling mechanism over
Places2. † Lower is better. ¶ Higher is better.

the volunteers to guess whether the image is a real image
from the dataset. We collected 1,425 valid votes in total,
and the statistics are shown in Table 5. We found that, the
inpainting results from the group with 32 × 32 masks can
be considered as real in 82.23% of the time. Even in the
challenging 128 × 128 case, we received 32.70% votes.

4.3. Analysis

We analyze the effectiveness of different components
of the proposed network by visualizing the learned feature
maps or ablation study as follows.

Effectiveness of the pyramid L1 loss Pyramid L1
losses is proposed to progressively reﬁne the predictions at
each scale. We conduct experiments on images of human
faces and visualize the images decoded at each scale. As
shown in Figure 7, the pyramid loss is helpful to decode the
compact latent feature into an image layer by layer.

Effectiveness of the ATN In order to verify the effec-
tiveness of the attention transfer network (ATN), we visu-
alize the learned feature maps on a same U-Net backbone
with different attention mechanisms. As shown in Figure 8,
the vanilla U-Net encoder (without using attention) encodes
little information inside missing regions, and it fails in gen-
erating plausible results. Without the guidance (i.e., atten-
tion map) from deeper layers, CA [31] (the commonly-used
attention method) failed in ﬁlling coherent patches inside
the missing regions in shallow layers. With the proposed
cross-layer ATN, our model is able to ﬁll regions with co-
herent patches. In addition to comparing with CA in Figure
8, we further compare with patch-swap layer [21] (the lat-
est attention method) on a same U-Net backbone in Table 6.
We can observe that, both cross-layer attention transfer net-

(a) 

(b) 

(c) 

(d) 

(e)

Figure 7: Images generated by the decoder at each scale.
(a) is the input. (b) is the ﬁnal prediction generated by our
model. (c), (d) and (e) are prediction output by the decoder
at multiple scales (all resized to 256 × 256 for visualiza-
tion). [Best viewed with zoom-in.]

t

-

e
N
U
a

 

l
l
i

n
a
V

A
C

s
r
u
O

(a) Input

(b) Output

(c) learned feature maps of different layers

Figure 8: Visualization of the feature maps learned by the
encoder. (a) is the input. (b) is the ﬁnal prediction gener-
ated by the models. (c) are visualized feature maps from
different layers. [Best viewed with zoom-in.]

work and pyramid ﬁlling mechanism bring improvements
of performance on a U-Net backbone.

5. Conclusion

In this paper, we propose a Pyramid-context Encoder
Network (PEN-Net) to generate semantically-reasonable
and visually-realistic results for image inpainting. Specif-
ically,
the proposed network boosts both the encoding
and decoding effectiveness of a vanilla U-Net by using a
pyramid-context encoder and a multi-scale decoder. We
highlight two key differences of the attention transfer net-
work used in the encoder, cross-layer attention transfer and
pyramid ﬁlling from high-level semantic features to low-
level features with more details. As a future work, we plan
to reﬁne the proposed network for higher resolution images.
Acknowledgments This work is partially supported
by NSF of China under Grant 61672548, U1611461,
61173081, and the Guangzhou Science and Technology
Program, China, under Grant 201510010165.

1493

[18] Jimmy SJ Ren, Li Xu, Qiong Yan, and Wenxiu Sun. Shepard
convolutional neural networks. In NeurIPS, pages 901–909,
2015. 3

[19] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, pages 234–241, 2015. 2

[20] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In NeurIPS, pages 2234–2242, 2016. 6

[21] Yuhang Song, Chao Yang, Zhe Lin, Xiaofeng Liu, Qin
Huang, Hao Li, and CC Jay. Contextual-based image in-
painting: Infer, match, and translate. In ECCV, pages 3–19,
2018. 3, 4, 8

[22] Jian Sun, Lu Yuan, Jiaya Jia, and Heung-Yeung Shum. Image
completion with structure propagation. In TOG, volume 24,
pages 861–868, 2005. 2

[23] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, pages 2818–2826,
2016. 6

[24] Alexandru Telea. An image inpainting technique based
Journal of graphics tools,

on the fast marching method.
9(1):23–34, 2004. 2

[25] Radim Tyleˇcek and Radim ˇS´ara. Spatial pattern templates
for recognition of objects with regular structure. In GCPR,
pages 364–374, 2013. 6

[26] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multi-
scale structural similarity for image quality assessment. In
ACSSC, volume 2, pages 1398–1402. IEEE, 2003. 6

[27] Yonatan Wexler, Eli Shechtman, and Michal Irani. Space-

time completion of video. TPAMI, (3):463–476, 2007. 2

[28] Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, and
Shiguang Shan. Shift-net: Image inpainting via deep feature
rearrangement. In ECCV, pages 1–17, 2018. 2, 4

[29] Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang,
and Hao Li. High-resolution image inpainting using multi-
scale neural patch synthesis.
In CVPR, pages 6721–6729,
2017. 2, 3

[30] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-

tion by dilated convolutions. In ICLR, 2016. 2

[31] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Generative image inpainting with contex-
tual attention. In CVPR, pages 5505–5514, 2018. 2, 3, 4, 5,
6, 8

[32] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. TPAMI, 40(6):1452–1464, 2018. 6

References

[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and
Dan B Goldman.
Patchmatch: A randomized corre-
spondence algorithm for structural image editing. TOG,
28(3):24:1–24:11, 2009. 2, 5, 6

[2] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and
Coloma Ballester. Image inpainting. In SIGGRAPH, pages
417–424, 2000. 1

[3] Marcelo Bertalmio, Luminita Vese, Guillermo Sapiro, and
Stanley Osher. Simultaneous structure and texture image in-
painting. TIP, 12(8):882–889, 2003. 2

[4] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
Mohamed, and Andrea Vedaldi. Describing textures in the
wild. In CVPR, pages 3606–3613, 2014. 6

[5] Antonio Criminisi, Patrick P´erez, and Kentaro Toyama. Re-
gion ﬁlling and object removal by exemplar-based image in-
painting. TIP, 13(9):1200–1212, 2004. 1, 2

[6] Alexei A Efros and William T Freeman. Image quilting for
In SIGGRAPH, pages 341–

texture synthesis and transfer.
346. ACM, 2001. 2

[7] Alexei A Efros and Thomas K Leung. Texture synthesis by
non-parametric sampling. In ICCV, pages 1033–1038. IEEE,
1999. 2

[8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NeurIPS, pages 6626–6637, 2017. 6

[9] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.
Globally and locally consistent image completion. TOG,
36(4):107, 2017. 2, 3, 5, 6

[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, pages 1125–1134, 2017. 3, 5

[11] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. In ICLR, 2018. 6

[12] Anat Levin, Assaf Zomet, Shmuel Peleg, and Yair Weiss.
Seamless image stitching in the gradient domain. In ECCV,
pages 377–389, 2004. 1

[13] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,
Andrew Tao, and Bryan Catanzaro. Image inpainting for ir-
regular holes using partial convolutions.
In ECCV, pages
85–100, 2018. 2, 3, 5, 6, 7

[14] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
In ICCV, pages

Deep learning face attributes in the wild.
3730–3738, 2015. 6

[15] David G Lowe. Object recognition from local scale-invariant

features. In ICCV, pages 1150–1157. IEEE, 1999. 2

[16] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks. In ICLR, 2018. 5

[17] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In CVPR, pages 2536–2544, 2016.
1, 2

1494

