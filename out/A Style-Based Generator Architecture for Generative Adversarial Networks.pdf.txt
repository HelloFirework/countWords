A Style-Based Generator Architecture for Generative Adversarial Networks

Tero Karras
NVIDIA

Samuli Laine

NVIDIA

Timo Aila
NVIDIA

tkarras@nvidia.com

slaine@nvidia.com

taila@nvidia.com

Abstract

We propose an alternative generator architecture for gen-
erative adversarial networks, borrowing from style transfer
literature. The new architecture leads to an automatically
learned, unsupervised separation of high-level attributes
(e.g., pose and identity when trained on human faces) and
stochastic variation in the generated images (e.g., freckles,
hair), and it enables intuitive, scale-specific control of the
synthesis. The new generator improves the state-of-the-art
in terms of traditional distribution quality metrics, leads to
demonstrably better interpolation properties, and also bet-
ter disentangles the latent factors of variation. To quantify
interpolation quality and disentanglement, we propose two
new, automated methods that are applicable to any genera-
tor architecture. Finally, we introduce a new, highly varied
and high-quality dataset of human faces.

1. Introduction

The resolution and quality of images produced by gener-
ative methods — especially generative adversarial networks
(GAN) [21] — have seen rapid improvement recently [28,
41, 4]. Yet the generators continue to operate as black boxes,
and despite recent efforts [2], the understanding of various
aspects of the image synthesis process, e.g., the origin of
stochastic features, is still lacking. The properties of the la-
tent space are also poorly understood, and the commonly
demonstrated latent space interpolations [12, 48, 34] provide
no quantitative way to compare different generators against
each other.

Motivated by style transfer literature [26], we re-design
the generator architecture in a way that exposes novel ways
to control the image synthesis process. Our generator starts
from a learned constant input and adjusts the “style” of
the image at each convolution layer based on the latent
code, therefore directly controlling the strength of image
features at different scales. Combined with noise injected
directly into the network, this architectural change leads to
automatic, unsupervised separation of high-level attributes

(e.g., pose, identity) from stochastic variation (e.g., freckles,
hair) in the generated images, and enables intuitive scale-
specific mixing and interpolation operations. We do not
modify the discriminator or the loss function in any way, and
our work is thus orthogonal to the ongoing discussion about
GAN loss functions, regularization, and hyper-parameters
[23, 41, 4, 37, 40, 33].

Our generator embeds the input latent code into an inter-
mediate latent space, which has a profound effect on how
the factors of variation are represented in the network. The
input latent space must follow the probability density of the
training data, and we argue that this leads to some degree
of unavoidable entanglement. Our intermediate latent space
is free from that restriction and is therefore allowed to be
disentangled. As previous methods for estimating the de-
gree of latent space disentanglement are not directly ap-
plicable in our case, we propose two new automated met-
rics — perceptual path length and linear separability — for
quantifying these aspects of the generator. Using these met-
rics, we show that compared to a traditional generator archi-
tecture, our generator admits a more linear, less entangled
representation of different factors of variation.

Finally, we present a new dataset of human faces (Flickr-
Faces-HQ, FFHQ) that offers much higher quality and
covers considerably wider variation than existing high-
resolution datasets (Appendix A). We have made this dataset
publicly available, along with our source code and pre-
trained networks.1 The accompanying video can be found
under the same link.

2. Style-based generator

Traditionally the latent code is provided to the gener-
ator through an input layer, i.e., the first layer of a feed-
forward network (Figure 1a). We depart from this design
by omitting the input layer altogether and starting from a
learned constant instead (Figure 1b, right). Given a la-
tent code z in the input latent space Z, a non-linear map-
ping network f : Z → W first produces w ∈ W (Fig-
ure 1b, left). For simplicity, we set the dimensionality of

1https://github.com/NVlabs/stylegan

4401

Latent

Latent

Normalize

Normalize

Fully-connected

Mapping
network

FC

FC

FC

FC

FC

FC

FC

FC

PixelNorm

Conv 3×3

PixelNorm

Upsample

Conv 3×3

PixelNorm

Conv 3×3

PixelNorm

4×4

8×8

Synthesis network

Const 4×4×512

A

A

A

A

style

AdaIN

Conv 3×3

style

AdaIN

4×4

Upsample

Conv 3×3

style

AdaIN

Conv 3×3

style

AdaIN

8×8

B

B

B

B

(a) Traditional

(b) Style-based generator

Figure 1. While a traditional generator [28] feeds the latent code
though the input layer only, we first map the input to an interme-
diate latent space W, which then controls the generator through
adaptive instance normalization (AdaIN) at each convolution layer.
Gaussian noise is added after each convolution, before evaluating
the nonlinearity. Here “A” stands for a learned affine transform, and
“B” applies learned per-channel scaling factors to the noise input.
The mapping network f consists of 8 layers and the synthesis net-
work g consists of 18 layers — two for each resolution (42−10242).
The output of the last layer is converted to RGB using a separate
1 × 1 convolution, similar to Karras et al. [28]. Our generator has
a total of 26.2M trainable parameters, compared to 23.1M in the
traditional generator.

both spaces to 512, and the mapping f is implemented using
an 8-layer MLP, a decision we will analyze in Section 4.1.
Learned affine transformations then specialize w to styles
y = (ys, yb) that control adaptive instance normalization
(AdaIN) [26, 16, 20, 15] operations after each convolution
layer of the synthesis network g. The AdaIN operation is
defined as

AdaIN(xi, y) = ys,i

xi − µ(xi)

σ(xi)

+ yb,i,

(1)

where each feature map xi is normalized separately, and
then scaled and biased using the corresponding scalar com-
ponents from style y. Thus the dimensionality of y is twice
the number of feature maps on that layer.

Comparing our approach to style transfer, we compute
the spatially invariant style y from vector w instead of an
example image. We choose to reuse the word “style” for
y because similar network architectures are already used
for feedforward style transfer [26], unsupervised image-to-
image translation [27], and domain mixtures [22]. Com-
pared to more general feature transforms [35, 53], AdaIN is
particularly well suited for our purposes due to its efficiency
and compact representation.

Noise

Method

CelebA-HQ

a Baseline Progressive GAN [28]
b + Tuning (incl. bilinear up/down)
c + Add mapping and styles
d + Remove traditional input
e + Add noise inputs
f + Mixing regularization

7.79
6.11
5.34
5.07
5.06
5.17

FFHQ
8.04
5.25
4.85
4.88
4.42
4.40

Table 1. Fréchet inception distance (FID) for various generator de-
signs (lower is better). In this paper we calculate the FIDs using
50,000 images drawn randomly from the training set, and report
the lowest distance encountered over the course of training.

Finally, we provide our generator with a direct means to
generate stochastic detail by introducing explicit noise in-
puts. These are single-channel images consisting of uncor-
related Gaussian noise, and we feed a dedicated noise im-
age to each layer of the synthesis network. The noise image
is broadcasted to all feature maps using learned per-feature
scaling factors and then added to the output of the corre-
sponding convolution, as illustrated in Figure 1b. The im-
plications of adding the noise inputs are discussed in Sec-
tions 3.2 and 3.3.

2.1. Quality of generated images

Before studying the properties of our generator, we
demonstrate experimentally that the redesign does not com-
promise image quality but, in fact, improves it considerably.
Table 1 gives Fréchet inception distances (FID) [24] for vari-
ous generator architectures in CelebA-HQ [28] and our new
FFHQ dataset (Appendix A). Results for other datasets are
given in the supplement. Our baseline configuration (a)
is the Progressive GAN setup of Karras et al. [28], from
which we inherit the networks and all hyperparameters ex-
cept where stated otherwise. We first switch to an improved
baseline (b) by using bilinear up/downsampling operations
[58], longer training, and tuned hyperparameters. A de-
tailed description of training setups and hyperparameters is
included in the supplement. We then improve this new base-
line further by adding the mapping network and AdaIN op-
erations (c), and make a surprising observation that the net-
work no longer benefits from feeding the latent code into the
first convolution layer. We therefore simplify the architec-
ture by removing the traditional input layer and starting the
image synthesis from a learned 4 × 4 × 512 constant tensor
(d). We find it quite remarkable that the synthesis network
is able to produce meaningful results even though it receives
input only through the styles that control the AdaIN opera-
tions.

Finally, we introduce the noise inputs (e) that improve
the results further, as well as novel mixing regularization (f)
that decorrelates neighboring styles and enables more fine-
grained control over the generated imagery (Section 3.1).

We evaluate our methods using two different loss func-
tions: for CelebA-HQ we rely on WGAN-GP [23], while

4402

2.2. Prior art

Much of the work on GAN architectures has focused on
improving the discriminator by, e.g., using multiple discrim-
inators [17, 43, 10], multiresolution discrimination [55, 51],
or self-attention [57]. The work on generator side has mostly
focused on the exact distribution in the input latent space [4]
or shaping the input latent space via Gaussian mixture mod-
els [3], clustering [44], or encouraging convexity [48].

Recent conditional generators feed the class identifier
through a separate embedding network to a large number
of layers in the generator [42], while the latent is still pro-
vided though the input layer. A few authors have considered
feeding parts of the latent code to multiple generator layers
[8, 4]. In parallel work, Chen et al. [5] “self modulate” the
generator using AdaINs, similarly to our work, but do not
consider an intermediate latent space or noise inputs.

3. Properties of the style-based generator

Our generator architecture makes it possible to control
the image synthesis via scale-specific modifications to the
styles. We can view the mapping network and affine trans-
formations as a way to draw samples for each style from a
learned distribution, and the synthesis network as a way to
generate a novel image based on a collection of styles. The
effects of each style are localized in the network, i.e., modi-
fying a specific subset of the styles can be expected to affect
only certain aspects of the image.

To see the reason for this localization, let us consider how
the AdaIN operation (Eq. 1) first normalizes each channel
to zero mean and unit variance, and only then applies scales
and biases based on the style. The new per-channel statistics,
as dictated by the style, modify the relative importance of
features for the subsequent convolution operation, but they
do not depend on the original statistics because of the nor-
malization. Thus each style controls only one convolution
before being overridden by the next AdaIN operation.

3.1. Style mixing

To further encourage the styles to localize, we employ
mixing regularization, where a given percentage of images
are generated using two random latent codes instead of one
during training. When generating such an image, we sim-
ply switch from one latent code to another — an operation
we refer to as style mixing — at a randomly selected point
in the synthesis network. To be specific, we run two latent
codes z1, z2 through the mapping network, and have the cor-
responding w1, w2 control the styles so that w1 applies be-
fore the crossover point and w2 after it. This regularization
technique prevents the network from assuming that adjacent
styles are correlated.

Table 2 shows how enabling mixing regularization during
training improves the localization considerably, indicated by

4403

Figure 2. Uncurated set of images produced by our style-based gen-
erator (config f) with the FFHQ dataset. Here we used a variation
of the truncation trick [38, 4, 31] with ψ = 0.7 for resolutions
42 − 322. Please see the accompanying video for more results.

FFHQ uses WGAN-GP for configuration a and non-
saturating loss [21] with R1 regularization [40, 47, 13] for
configurations b–f. We found these choices to give the best
results. Our contributions do not modify the loss function.
We observe that the style-based generator (e) improves
FIDs quite significantly over the traditional generator (b), al-
most 20%, corroborating the large-scale ImageNet measure-
ments made in parallel work [5, 4]. Figure 2 shows an uncu-
rated set of novel images generated from the FFHQ dataset
using our generator. As confirmed by the FIDs, the aver-
age quality is high, and even accessories such as eyeglasses
and hats get successfully synthesized. For this figure, we
avoided sampling from the extreme regions of W using the
so-called truncation trick [38, 4, 31] — Appendix B details
how the trick can be performed in W instead of Z. Note
that our generator allows applying the truncation selectively
to low resolutions only, so that high-resolution details are
not affected.

All FIDs in this paper are computed without the trun-
cation trick, and we only use it for illustrative purposes in
Figure 2 and the video. All images are generated in 10242
resolution.

B
e
c
r
u
o
S

Source A

B
e
c
r
u
o
s
m
o
r
f

s
e
l
y
t
s

e
s
r
a
o
C

B
e
c
r
u
o
s

m
o
r
f

s
e
l
y
t
s

e
l
d
d
i
M

B
m
o
r
f

e
n
i
F

Figure 3. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated by
copying a specified subset of styles from source B and taking the rest from source A. Copying the styles corresponding to coarse spatial
resolutions (42 – 82) brings high-level aspects such as pose, general hair style, face shape, and eyeglasses from source B, while all colors
(eyes, hair, lighting) and finer facial features resemble A. If we instead copy the styles of middle resolutions (162 – 322) from B, we inherit
smaller scale facial features, hair style, eyes open/closed from B, while the pose, general face shape, and eyeglasses from A are preserved.
Finally, copying the fine styles (642 – 10242) from B brings mainly the color scheme and microstructure.

4404

Mixing
regularization

e 0%
50%
f 90%
100%

1
4.42
4.41
4.40
4.83

Number of latents during testing

2
8.22
6.10
5.11
5.17

3

4

12.88
8.71
6.88
6.63

17.41
11.61
9.03
8.40

Table 2. FIDs in FFHQ for networks trained by enabling the mixing
regularization for different percentage of training examples. Here
we stress test the trained networks by randomizing 1 . . . 4 latents
and the crossover points between them. Mixing regularization im-
proves the tolerance to these adverse operations significantly. La-
bels e and f refer to the configurations in Table 1.

(a)

(b)

(a) Generated image

(b) Stochastic variation (c) Standard deviation
(a) Two generated
Figure 4. Examples of stochastic variation.
images.
(b) Zoom-in with different realizations of input noise.
While the overall appearance is almost identical, individual hairs
are placed very differently. (c) Standard deviation of each pixel
over 100 different realizations, highlighting which parts of the im-
ages are affected by the noise. The main areas are the hair, silhou-
ettes, and parts of background, but there is also interesting stochas-
tic variation in the eye reflections. Global aspects such as identity
and pose are unaffected by stochastic variation.

improved FIDs in scenarios where multiple latents are mixed
at test time. Figure 3 presents examples of images synthe-
sized by mixing two latent codes at various scales. We can
see that each subset of styles controls meaningful high-level
attributes of the image.

3.2. Stochastic variation

There are many aspects in human portraits that can be
regarded as stochastic, such as the exact placement of hairs,
stubble, freckles, or skin pores. Any of these can be random-
ized without affecting our perception of the image as long as
they follow the correct distribution.

Let us consider how a traditional generator implements
stochastic variation. Given that the only input to the net-
work is through the input layer, the network needs to invent
a way to generate spatially-varying pseudorandom numbers
from earlier activations whenever they are needed. This con-

(c)

(d)

Figure 5. Effect of noise inputs at different layers of our generator.
(a) Noise is applied to all layers. (b) No noise. (c) Noise in fine lay-
ers only (642 – 10242). (d) Noise in coarse layers only (42 – 322).
We can see that the artificial omission of noise leads to featureless
“painterly” look. Coarse noise causes large-scale curling of hair
and appearance of larger background features, while the fine noise
brings out the finer curls of hair, finer background detail, and skin
pores.

sumes network capacity and hiding the periodicity of gen-
erated signal is difficult — and not always successful, as ev-
idenced by commonly seen repetitive patterns in generated
images. Our architecture sidesteps these issues altogether by
adding per-pixel noise after each convolution.

Figure 4 shows stochastic realizations of the same un-
derlying image, produced using our generator with different
noise realizations. We can see that the noise affects only
the stochastic aspects, leaving the overall composition and
high-level aspects such as identity intact. Figure 5 further
illustrates the effect of applying stochastic variation to dif-
ferent subsets of layers. Since these effects are best seen
in animation, please consult the accompanying video for a
demonstration of how changing the noise input of one layer
leads to stochastic variation at a matching scale.

We find it interesting that the effect of noise appears
tightly localized in the network. We hypothesize that at any
point in the generator, there is pressure to introduce new con-
tent as soon as possible, and the easiest way for our network
to create stochastic variation is to rely on the noise provided.
A fresh set of noise is available for every layer, and thus there
is no incentive to generate the stochastic effects from earlier
activations, leading to a localized effect.

4405

(a) Distribution of

(b) Mapping from

(c) Mapping from

Z to features

W to features

features in training set
Figure 6. Illustrative example with two factors of variation (im-
age features, e.g., masculinity and hair length). (a) An example
training set where some combination (e.g., long haired males) is
missing. (b) This forces the mapping from Z to image features to
become curved so that the forbidden combination disappears in Z
to prevent the sampling of invalid combinations. (c) The learned
mapping from Z to W is able to “undo” much of the warping.

3.3. Separation of global effects from stochasticity
The previous sections as well as the accompanying video
demonstrate that while changes to the style have global ef-
fects (changing pose, identity, etc.), the noise affects only in-
consequential stochastic variation (differently combed hair,
beard, etc.). This observation is in line with style transfer lit-
erature, where it has been established that spatially invariant
statistics (Gram matrix, channel-wise mean, variance, etc.)
reliably encode the style of an image [19, 36] while spatially
varying features encode a specific instance.

In our style-based generator, the style affects the entire
image because complete feature maps are scaled and biased
with the same values. Therefore, global effects such as pose,
lighting, or background style can be controlled coherently.
Meanwhile, the noise is added independently to each pixel
and is thus ideally suited for controlling stochastic variation.
If the network tried to control, e.g., pose using the noise,
that would lead to spatially inconsistent decisions that would
then be penalized by the discriminator. Thus the network
learns to use the global and local channels appropriately,
without explicit guidance.

4. Disentanglement studies

There are various definitions for disentanglement [50, 46,
1, 6, 18], but a common goal is a latent space that consists of
linear subspaces, each of which controls one factor of vari-
ation. However, the sampling probability of each combina-
tion of factors in Z needs to match the corresponding den-
sity in the training data. As illustrated in Figure 6, this pre-
cludes the factors from being fully disentangled with typical
datasets and input latent distributions.2

A major benefit of our generator architecture is that the
intermediate latent space W does not have to support sam-

2The few artificial datasets designed for disentanglement studies (e.g.,
[39, 18]) tabulate all combinations of predetermined factors of variation
with uniform frequency, thus hiding the problem.

pling according to any fixed distribution; its sampling den-
sity is induced by the learned piecewise continuous map-
ping f (z). This mapping can be adapted to “unwarp” W so
that the factors of variation become more linear. We posit
that there is pressure for the generator to do so, as it should
be easier to generate realistic images based on a disentan-
gled representation than based on an entangled representa-
tion. As such, we expect the training to yield a less entan-
gled W in an unsupervised setting, i.e., when the factors of
variation are not known in advance [9, 32, 45, 7, 25, 30, 6].
Unfortunately the metrics recently proposed for quantify-
ing disentanglement [25, 30, 6, 18] require an encoder net-
work that maps input images to latent codes. These metrics
are ill-suited for our purposes since our baseline GAN lacks
such an encoder. While it is possible to add an extra network
for this purpose [7, 11, 14], we want to avoid investing effort
into a component that is not a part of the actual solution. To
this end, we describe two new ways of quantifying disen-
tanglement, neither of which requires an encoder or known
factors of variation, and are therefore computable for any
image dataset and generator.

4.1. Perceptual path length

As noted by Laine [34], interpolation of latent-space vec-
tors may yield surprisingly non-linear changes in the image.
For example, features that are absent in either endpoint may
appear in the middle of a linear interpolation path. This is a
sign that the latent space is entangled and the factors of vari-
ation are not properly separated. To quantify this effect, we
can measure how drastic changes the image undergoes as we
perform interpolation in the latent space. Intuitively, a less
curved latent space should result in perceptually smoother
transition than a highly curved latent space.

As a basis for our metric, we use a perceptually-based
pairwise image distance [59] that is calculated as a weighted
difference between two VGG16 [54] embeddings, where the
weights are fit so that the metric agrees with human percep-
tual similarity judgments. If we subdivide a latent space in-
terpolation path into linear segments, we can define the total
perceptual length of this segmented path as the sum of per-
ceptual differences over each segment, as reported by the im-
age distance metric. A natural definition for the perceptual
path length would be the limit of this sum under infinitely
fine subdivision, but in practice we approximate it using a
small subdivision epsilon ǫ = 10−4. The average perceptual
path length in latent space Z, over all possible endpoints, is
therefore

lZ = Eh 1

ǫ2

d(cid:0)G(slerp(z1, z2; t)),
G(slerp(z1, z2; t + ǫ))(cid:1)i,

(2)

where z1, z2 ∼ P (z), t ∼ U (0, 1), G is the generator (i.e.,
g ◦ f for style-based networks), and d(·, ·) evaluates the per-

4406

Method

b Traditional generator Z
d Style-based generator W
e + Add noise inputs W
W

+ Mixing 50%
f + Mixing 90%

W

full
412.0
446.2
200.5
231.5
234.0

Path length

end
415.3
376.6
160.6
182.1
195.9

Separa-
bility
10.78
3.61
3.54
3.51
3.79

Table 3. Perceptual path lengths and separability scores for various
generator architectures in FFHQ (lower is better). We perform the
measurements in Z for the traditional network, and in W for style-
based ones. Making the network resistant to style mixing appears
to distort the intermediate latent space W somewhat. We hypothe-
size that mixing makes it more difficult for W to efficiently encode
factors of variation that span multiple scales.

ceptual distance between the resulting images. Here slerp
denotes spherical interpolation [52], which is the most ap-
propriate way of interpolating in our normalized input latent
space [56]. To concentrate on the facial features instead of
background, we crop the generated images to contain only
the face prior to evaluating the pairwise image metric. As
the metric d is quadratic [59], we divide by ǫ2. We compute
the expectation by taking 100,000 samples.

Computing the average perceptual path length in W is

carried out in a similar fashion:

lW = Eh 1

ǫ2

d(cid:0)g(lerp(f (z1), f (z2); t)),
g(lerp(f (z1), f (z2); t + ǫ))(cid:1)i,

(3)

where the only difference is that interpolation happens in
W space. Because vectors in W are not normalized in any
fashion, we use linear interpolation (lerp).

Table 3 shows that this full-path length is substantially
shorter for our style-based generator with noise inputs, in-
dicating that W is perceptually more linear than Z. Yet,
this measurement is in fact slightly biased in favor of the in-
put latent space Z. If W is indeed a disentangled and “flat-
tened” mapping of Z, it may contain regions that are not on
the input manifold — and are thus badly reconstructed by the
generator — even between points that are mapped from the
input manifold, whereas the input latent space Z has no such
regions by definition. It is therefore to be expected that if we
restrict our measure to path endpoints, i.e., t ∈ {0, 1}, we
should obtain a smaller lW while lZ is not affected. This is
indeed what we observe in Table 3.

Table 4 shows how path lengths are affected by the map-
ping network. We see that both traditional and style-based
generators benefit from having a mapping network, and ad-
ditional depth generally improves the perceptual path length
as well as FIDs. It is interesting that while lW improves in
the traditional generator, lZ becomes considerably worse, il-
lustrating our claim that the input latent space can indeed be
arbitrarily entangled in GANs.

Method

b Traditional 0 Z
Traditional 8 Z
Traditional 8 W
Style-based 0 Z
Style-based 1 W
Style-based 2 W
f Style-based 8 W

FID

5.25
4.87
4.87
5.06
4.60
4.43
4.40

full
412.0
896.2
324.5
283.5
219.9
217.8
234.0

Path length

end
415.3
902.0
212.2
285.5
209.4
199.9
195.9

Separa-
bility
10.78
170.29
6.52
9.88
6.81
6.25
3.79

Table 4. The effect of a mapping network in FFHQ. The number
in method name indicates the depth of the mapping network. We
see that FID, separability, and path length all benefit from having
a mapping network, and this holds for both style-based and tra-
ditional generator architectures. Furthermore, a deeper mapping
network generally performs better than a shallow one.

4.2. Linear separability

If a latent space is sufficiently disentangled, it should
be possible to find direction vectors that consistently corre-
spond to individual factors of variation. We propose another
metric that quantifies this effect by measuring how well the
latent-space points can be separated into two distinct sets via
a linear hyperplane, so that each set corresponds to a specific
binary attribute of the image.

In order to label the generated images, we train auxiliary
classification networks for a number of binary attributes,
e.g., to distinguish male and female faces. In our tests, the
classifiers had the same architecture as the discriminator
we use (i.e., same as in [28]), and were trained using the
CelebA-HQ dataset that retains the 40 attributes available
in the original CelebA dataset. To measure the separability
of one attribute, we generate 200,000 images with z ∼ P (z)
and classify them using the auxiliary classification network.
We then sort the samples according to classifier confidence
and remove the least confident half, yielding 100,000 labeled
latent-space vectors.

For each attribute, we fit a linear SVM to predict the la-
bel based on the latent-space point — z for traditional and w
for style-based — and classify the points by this plane. We
then compute the conditional entropy H(Y |X) where X are
the classes predicted by the SVM and Y are the classes de-
termined by the pre-trained classifier. This tells how much
additional information is required to determine the true class
of a sample, given that we know on which side of the hyper-
plane it lies. A low value suggests consistent latent space
directions for the corresponding factor(s) of variation.
score

as
exp(Pi H(Yi|Xi)), where i enumerates the 40 attributes.
Similar to the inception score [49],
the exponentiation
brings the values from logarithmic to linear domain so that
they are easier to compare.

separability

calculate

final

We

the

Tables 3 and 4 show that W is consistently better sep-
arable than Z, suggesting a less entangled representation.

4407

Figure 7. The FFHQ dataset offers a lot of variety in terms of age, ethnicity, viewpoint, lighting, and image background.

Furthermore, increasing the depth of the mapping network
improves both image quality and separability in W, which
is in line with the hypothesis that the synthesis network in-
herently favors a disentangled input representation. Inter-
estingly, adding a mapping network in front of a traditional
generator results in severe loss of separability in Z but im-
proves the situation in the intermediate latent space W, and
the FID improves as well. This shows that even the tradi-
tional generator architecture performs better when we intro-
duce an intermediate latent space that does not have to follow
the distribution of the training data.

5. Conclusion

Based on both our results and parallel work by Chen et
al. [5], it is becoming clear that the traditional GAN gen-
erator architecture is in every way inferior to a style-based
design. This is true in terms of established quality metrics,
and we further believe that our investigations to the sepa-
ration of high-level attributes and stochastic effects, as well
as the linearity of the intermediate latent space will prove
fruitful in improving the understanding and controllability
of GAN synthesis.

We note that our average path length metric could easily
be used as a regularizer during training, and perhaps some
variant of the linear separability metric could act as one, too.
In general, we expect that methods for directly shaping the
intermediate latent space during training will provide inter-
esting avenues for future work.

6. Acknowledgements

We thank Jaakko Lehtinen, David Luebke, and Tuomas
Kynkäänniemi for in-depth discussions and helpful com-
ments; Janne Hellsten, Tero Kuosmanen, and Pekka Jänis
for compute infrastructure and help with the code release.

A. The FFHQ dataset

We have collected a new dataset of human faces, Flickr-
Faces-HQ (FFHQ), consisting of 70,000 high-quality im-
ages at 10242 resolution (Figure 7). The dataset includes
vastly more variation than CelebA-HQ [28] in terms of age,
ethnicity and image background, and also has much bet-
ter coverage of accessories such as eyeglasses, sunglasses,
hats, etc. The images were crawled from Flickr (thus in-

ψ = 1

ψ = 0.7

ψ = 0.5

ψ = 0

ψ = −0.5 ψ = −1

Figure 8. The effect of truncation trick as a function of style scale
ψ. When we fade ψ → 0, all faces converge to the “mean” face
of FFHQ. This face is similar for all trained networks, and the in-
terpolation towards it never seems to cause artifacts. By applying
negative scaling to styles, we get the corresponding opposite or
“anti-face”. It is interesting that various high-level attributes of-
ten flip between the opposites, including viewpoint, glasses, age,
coloring, hair length, and often gender.

heriting all the biases of that website) and automatically
aligned [29] and cropped. Only images under permissive
licenses were collected. Various automatic filters were used
to prune the set, and finally Mechanical Turk allowed us
to remove the occasional statues, paintings, or photos of
photos. We have made the dataset publicly available at
https://github.com/NVlabs/ffhq-dataset

B. Truncation trick in W

If we consider the distribution of training data, it is clear
that areas of low density are poorly represented and thus
likely to be difficult for the generator to learn. This is a sig-
nificant open problem in all generative modeling techniques.
However, it is known that drawing latent vectors from a trun-
cated [38, 4] or otherwise shrunk [31] sampling space tends
to improve average image quality, although some amount of
variation is lost.

We can follow a similar strategy. To begin, we compute
z∼P (z)[f (z)]. In case of
the center of mass of W as ¯w = E
FFHQ this point represents a sort of an average face (Fig-
ure 8, ψ = 0). We can then scale the deviation of a given
w from the center as w′ = ¯w + ψ(w − ¯w), where ψ < 1.
While Brock et al. [4] observe that only a subset of networks
is amenable to such truncation even when orthogonal reg-
ularization is used, truncation in W space seems to work
reliably even without changes to the loss function.

4408

References
[1] A. Achille and S. Soatto. On the emergence of invari-
ance and disentangling in deep representations. CoRR,
abs/1706.01350, 2017. 6

[2] D. Bau, J. Zhu, H. Strobelt, B. Zhou, J. B. Tenenbaum, W. T.
Freeman, and A. Torralba. GAN dissection: Visualizing
and understanding generative adversarial networks. In Proc.
ICLR, 2019. 1

[3] M. Ben-Yosef and D. Weinshall. Gaussian mixture genera-
tive adversarial networks for diverse datasets, and the unsu-
pervised clustering of images. CoRR, abs/1808.10356, 2018.
3

[4] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN
training for high fidelity natural image synthesis. CoRR,
abs/1809.11096, 2018. 1, 3, 8

[5] T. Chen, M. Lucic, N. Houlsby, and S. Gelly. On self
CoRR,

modulation for generative adversarial networks.
abs/1810.01365, 2018. 3, 8

[6] T. Q. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud. Isolat-
ing sources of disentanglement in variational autoencoders.
CoRR, abs/1802.04942, 2018. 6

[7] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel. InfoGAN: interpretable representation learn-
ing by information maximizing generative adversarial nets.
CoRR, abs/1606.03657, 2016. 6

[8] E. L. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep
generative image models using a Laplacian pyramid of ad-
versarial networks. CoRR, abs/1506.05751, 2015. 3

[9] G. Desjardins, A. Courville, and Y. Bengio. Disentan-
gling factors of variation via generative entangling. CoRR,
abs/1210.5474, 2012. 6

[10] T. Doan, J. Monteiro, I. Albuquerque, B. Mazoure, A. Du-
rand, J. Pineau, and R. D. Hjelm. Online adaptative curricu-
lum learning for GANs. CoRR, abs/1808.00020, 2018. 3

[11] J. Donahue, P. Krähenbühl, and T. Darrell. Adversarial fea-

ture learning. CoRR, abs/1605.09782, 2016. 6

[12] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to
generate chairs with convolutional neural networks. CoRR,
abs/1411.5928, 2014. 1

[13] H. Drucker and Y. L. Cun. Improving generalization perfor-
mance using double backpropagation. IEEE Transactions on
Neural Networks, 3(6):991–997, 1992. 3

[14] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky,
O. Mastropietro, and A. Courville. Adversarially learned in-
ference. In Proc. ICLR, 2017. 6

[15] V. Dumoulin, E. Perez, N. Schucher, F. Strub, H. d. Vries,
A. Courville, and Y. Bengio.
Feature-wise transforma-
tions. Distill, 2018. https://distill.pub/2018/feature-wise-
transformations. 2

[16] V. Dumoulin, J. Shlens, and M. Kudlur. A learned represen-

tation for artistic style. CoRR, abs/1610.07629, 2016. 2

[17] I. P. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-

adversarial networks. CoRR, abs/1611.01673, 2016. 3

[18] C. Eastwood and C. K. I. Williams. A framework for the
In

quantitative evaluation of disentangled representations.
Proc. ICLR, 2018. 6

[19] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In Proc. CVPR, 2016.
6

[20] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens.
Exploring the structure of a real-time, arbitrary neural artistic
stylization network. CoRR, abs/1705.06830, 2017. 2

[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio. Generative
Adversarial Networks. In NIPS, 2014. 1, 3

[22] W.-S. Z. Guang-Yuan Hao, Hong-Xing Yu. MIXGAN: learn-
ing concepts from different domains for mixture generation.
CoRR, abs/1807.01659, 2018. 2

[23] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.
Courville. Improved training of Wasserstein GANs. CoRR,
abs/1704.00028, 2017. 1, 2

[24] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter. GANs trained by a two time-scale update rule
converge to a local Nash equilibrium. In Proc. NIPS, pages
6626–6637, 2017. 2

[25] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,
M. Botvinick, S. Mohamed, and A. Lerchner.
beta-vae:
Learning basic visual concepts with a constrained variational
framework. In Proc. ICLR, 2017. 6

[26] X. Huang and S. J. Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. CoRR,
abs/1703.06868, 2017. 1, 2

[27] X. Huang, M. Liu, S. J. Belongie, and J. Kautz. Mul-
timodal unsupervised image-to-image translation. CoRR,
abs/1804.04732, 2018. 2

[28] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of GANs for improved quality, stability, and varia-
tion. CoRR, abs/1710.10196, 2017. 1, 2, 7, 8

[29] V. Kazemi and J. Sullivan. One millisecond face alignment
with an ensemble of regression trees. In Proc. CVPR, 2014.
8

[30] H. Kim and A. Mnih. Disentangling by factorising. In Proc.

ICML, 2018. 6

[31] D. P. Kingma and P. Dhariwal. Glow: Generative flow with
invertible 1x1 convolutions. CoRR, abs/1807.03039, 2018.
3, 8

[32] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. In ICLR, 2014. 6

[33] K. Kurach, M. Lucic, X. Zhai, M. Michalski, and S. Gelly.
The gan landscape: Losses, architectures, regularization, and
normalization. CoRR, abs/1807.04720, 2018. 1

[34] S. Laine. Feature-based metrics for exploring the latent space

of generative models. ICLR workshop poster, 2018. 1, 6

[35] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.
Universal style transfer via feature transforms. In Proc. NIPS,
2017. 2

[36] Y. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural style

transfer. CoRR, abs/1701.01036, 2017. 6

[37] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bous-
quet. Are GANs created equal? a large-scale study. CoRR,
abs/1711.10337, 2017. 1

[38] M. Marchesi. Megapixel size image creation using generative

adversarial networks. CoRR, abs/1706.00082, 2017. 3, 8

4409

[59] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang.
The unreasonable effectiveness of deep features as a percep-
tual metric. In Proc. CVPR, 2018. 6, 7

[39] L. Matthey,

I. Higgins, D. Hassabis, and A. Lerch-
testing sprites dataset.

dsprites: Disentanglement

ner.
https://github.com/deepmind/dsprites-dataset/, 2017. 6

[40] L. Mescheder, A. Geiger, and S. Nowozin. Which train-
CoRR,

ing methods for GANs do actually converge?
abs/1801.04406, 2018. 1, 3

[41] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spec-
tral normalization for generative adversarial networks. CoRR,
abs/1802.05957, 2018. 1

[42] T. Miyato and M. Koyama. cGANs with projection discrim-

inator. CoRR, abs/1802.05637, 2018. 3

[43] G. Mordido, H. Yang, and C. Meinel. Dropout-gan: Learn-
ing from a dynamic ensemble of discriminators. CoRR,
abs/1807.11346, 2018. 3

[44] S. Mukherjee, H. Asnani, E. Lin, and S. Kannan. Cluster-
GAN : Latent space clustering in generative adversarial net-
works. CoRR, abs/1809.03627, 2018. 3

[45] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic
backpropagation and approximate inference in deep genera-
tive models. In Proc. ICML, 2014. 6

[46] K. Ridgeway. A survey of inductive biases for factorial

representation-learning. CoRR, abs/1612.05299, 2016. 6

[47] A. S. Ross and F. Doshi-Velez. Improving the adversarial ro-
bustness and interpretability of deep neural networks by reg-
ularizing their input gradients. CoRR, abs/1711.09404, 2017.
3

[48] T. Sainburg, M. Thielk, B. Theilman, B. Migliori, and T. Gen-
tner. Generative adversarial interpolative autoencoding: ad-
versarial training on latent space interpolations encourage
convex latent distributions. CoRR, abs/1807.06650, 2018. 1,
3

[49] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung,
A. Radford, and X. Chen. Improved techniques for training
GANs. In NIPS, 2016. 7

[50] J. Schmidhuber. Learning factorial codes by predictability
minimization. Neural Computation, 4(6):863–879, 1992. 6
Improved
training with curriculum gans. CoRR, abs/1807.09295, 2018.
3

[51] R. Sharma, S. Barratt, S. Ermon, and V. Pande.

[52] K. Shoemake. Animating rotation with quaternion curves. In

Proc. SIGGRAPH ’85, 1985. 7

[53] A. Siarohin, E. Sangineto, and N. Sebe. Whitening and col-
oring transform for GANs. CoRR, abs/1806.00420, 2018. 2
Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 6

[54] K. Simonyan and A. Zisserman.

[55] T. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro.
High-resolution image synthesis and semantic manipulation
with conditional GANs. CoRR, abs/1711.11585, 2017. 3

[56] T. White. Sampling generative networks: Notes on a few

effective techniques. CoRR, abs/1609.04468, 2016. 7

[57] H. Zhang,

I. Goodfellow, D. Metaxas, and A. Odena.
CoRR,

Self-attention generative adversarial networks.
abs/1805.08318, 2018. 3

[58] R. Zhang. Making convolutional networks shift-invariant

again, 2019. 2

4410

