Building Efﬁcient Deep Neural Networks with Unitary Group Convolutions

Ritchie Zhao

Yuwei Hu

Jordan Dotzel

Christopher De Sa

Zhiru Zhang

Cornell University

Ithaca, NY 14853, USA

{rz252, yh457, jad443}@cornell.edu, cdesa@cs.cornell.edu, zhiruz@cornell.edu

Abstract

We propose unitary group convolutions (UGConvs), a
building block for CNNs which compose a group convo-
lution with unitary transforms in feature space to learn a
richer set of representations than group convolution alone.
UGConvs generalize two disparate ideas in CNN architec-
ture, channel shufﬂing (i.e. ShufﬂeNet [29]) and block-
circulant networks (i.e. CirCNN [6]), and provide unifying
insights that lead to a deeper understanding of each tech-
nique. We experimentally demonstrate that dense unitary
transforms can outperform channel shufﬂing in DNN ac-
curacy. On the other hand, different dense transforms ex-
hibit comparable accuracy performance. Based on these
observations we propose HadaNet, a UGConv network us-
ing Hadamard transforms. HadaNets achieve similar ac-
curacy to circulant networks with lower computation com-
plexity, and better accuracy than ShufﬂeNets with the same
number of parameters and ﬂoating-point multiplies.

1. Introduction

Deep convolutional neural networks (CNNs) have
proven extremely successful at large-scale computer vision
problems. Research over the past few years has made steady
progress on improving CNN accuracy [26]. Concurrently,
efforts have been made to reduce the number of parame-
ters and ﬂoating-point multiplies (fpmuls) in CNNs. One
major trend in this research space is the increasing sparsity
of layer connections. Early networks such as AlexNet [13]
and VGG [19] exclusively utilize dense mappings, i.e. con-
volutional (conv) or fully-connected (FC) layers that form
a weight connection between every input and every out-
put feature. More advanced architectures such as Xcep-
tion [2] and MobileNets [8] make use of depthwise separa-
ble convolutions, which consist of a sparse spatial mapping
(depthwise convolution) and a dense cross-channel map-
ping (pointwise convolution). Even more recently, Shuf-

ﬂeNet [29] replaces the pointwise convolutions with sparse
group convolutions, and additionally proposes a channel
shufﬂe to allow information to ﬂow between groups. These
changes to layer structure look to remove weight connec-
tions while retaining accuracy performance.

A different line of efﬁcient CNNs research looks to train
networks with circulant or block-circulant 1 weights [1, 20,
6, 22]. An n × n circulant matrix contains only n unique
elements. Moreover, every circulant matrix C can be di-
agonalized by the normalized discrete Fourier matrix F as
follows:

C = F∗DF

(1)

giving rise to an asymptotically faster algorithm for matrix
multiplication via the fast Fourier transform (FFT). By ex-
ploiting these properties of circulant weights, these works
can also reduce CNN complexity and model size.

In this paper, we propose the concept of unitary group
convolution (UGConv), deﬁned as a building block for
neural networks that combines a weight layer (most com-
monly a group convolution) with unitary transforms in fea-
ture space. We show that group convs with channel shuf-
ﬂe (ShufﬂeNet) and block-circulant networks (CirCNN) are
speciﬁc instances of UGConvs. By unifying two different
lines of work in CNN literature, we gain a deeper under-
standing into the basic underlying idea — that group con-
volutions exhibit improved learning ability when performed
in a transformed feature basis. Through a series of exper-
iments, we then investigate how different transforms and
UGConv structures affect the learning performance. Specif-
ically, our contributions are as follows:

1. We propose the concept of unitary group convolu-
tions. We show that ShufﬂeNets and circulant net-
works, techniques from two disparate lines of research,
are in fact both instances of UGConv networks. This
lets us unify the conceptual insights of both works.

1In this paper, block-circulant, block-diagonal, etc. refers to matrices
consisting of square sub-matrices which are circulant, diagonal, etc. This
is different from the canonical deﬁnition of a block-diagonal matrix.

11303

2. We evaluate how different unitary transforms affect
learning performance. Our experiments show that
when the weight layer is highly sparse (i.e. the num-
ber of groups is large), dense transforms outperform
simple permutations.

3. We propose HadaNets, UGConv networks using the
easy-to-compute Hadamard transform. HadaNets ob-
tain similar accuracy as circulant networks at a lower
computation complexity, and outperform ShufﬂeNets
with identical parameter and fpmul counts.

2. Related Work

2.1. Depthwise Separable and Group Convolutions

In a traditional convolutional layer, each 3D ﬁlter must
learn both spatial and cross-channel correlations. A depth-
wise separable convolution decouples this into two steps:
a depthwise convolution which only performs spatial ﬁlter-
ing, and a pointwise convolution which only learns cross-
channel mappings. The idea originated in Sifre 2014 [18]
and was subsequently popularized by networks like Xcep-
tion [2] and MobileNets [8]. These works showed that
depthwise separable convolutions can outperform tradi-
tional convolutions using fewer parameters and fpmuls.

A group convolution divides the input and output fea-
tures into mutually independent groups and performs a con-
volution in each one. Depthwise convs are speciﬁc cases of
group convs with group size 1. Group convolutions were
part of the original AlexNet, but only to facilitate training
on multiple GPUs [13]; they gained popularity as a build-
ing block of efﬁcient CNNs as part of ResNeXt [25] and
ShufﬂeNet [29]. The latter proposed channel shufﬂing to
promote cross-channel information ﬂow, surpassing Mo-
bileNets in accuracy and parameter efﬁciency.

Interleaved group convolutions [28, 24, 21] examines in-
terleaving group convs and channels shufﬂes, and showed
how a speciﬁc combination of width and sparsity (i.e. num-
ber of groups) can maximize accuracy. Deep Roots [10]
uses group convolutions with increasing group size deeper
into the network to improve numerous existing models. Dis-
tinct from these works, we study the composition of group
convs with dense unitary transforms.

2.2. Circulant and Block Circulant Networks

An n-by-n circulant matrix requires only O(n) storage
space and O(n log n) operations for the matrix-vector prod-
uct (see Equation (1)). Circulant weights can reduce the
model size and computational complexity of CNNs in a
deterministic manner. Cheng et al.
in 2015 applies this
to achieve 18x parameter reduction on AlexNet with only
0.7% Top-1 accuracy loss [1]. Other authors proposed vari-
ations of circulant structure. Moczulski et al.’s ACDC used

cosine transforms to avoid complex values that arise with
DFTs and added a second channel-wise ﬁlter [16]. Sind-
hwani et al. studied the superset of generalized Toeplitz-like
matrices [20]. These works exclusively worked on struc-
tured FC layers.

More recently, Wang et al. [6, 23] proposed to use block-
circulant matrices and applied them to both FC and convolu-
tional layers. Block-circulant structure elegantly addresses
the long-standing issue of non-square weight matrices. The
same authors also leveraged the butterﬂy structure of the
DFT to construct efﬁcient accelerators for circulant nets in
dedicated hardware [6, 22]. A more recent follow-up in this
line of work proposed to use permuted block-diagonal ma-
trices in specialized hardware [4].

2.3. Random Projections and Hadamard Networks

Our study on random orthogonal and Hadamard trans-
forms is partly inspired by the Fastfood transform [14] and
its application to CNNs [27]. This work is a well-known
example of using random embeddings and Hadamard trans-
forms in machine learning.

A recent work from Devici et al. [5] used Hadamard-
transformed images as CNN inputs. Their work differs sig-
niﬁcantly from ours; they applied a single 2D Hadamard on
the input image to extract frequency features while we use
Hadamard throughout the network for channel mixing.

3. Unitary Group Convolutions

The basic idea of a UGConv is a group convolution sand-
wiched between two unitary transforms in feature space.
Let X be an M -channel input tensor to a conv/FC layer.
Each channel is a 2D feature map (for a dense layer the di-
mensions are 1 × 1). Let x(i) denote the i’th channel in X.
Similarly, let Y be the N -channel output tensor, and let W
be the weight tensor consisting of M × N ﬁlters. We can
now deﬁne an ordinary conv layer below:

y(j) =

M

X

i=1

x(i) ∗ W(ij),

1 ≤ j ≤ N

Figure 1(a) illustrates such a conv or dense layer. Note that
although the ﬁgure looks like matrix multiplication, each
square represents a 2D weight ﬁlter or feature map.

A group convolution is simply a collection of G disjoint
convolutions (G is the number of groups). Each conv takes
M/G input channels and produces N/G output channels.

˜y(g,j) =

M/G

X

i=1

˜x(g,i) ∗ ˜W(g,ij),

1 ≤ j ≤ N/G

(2)

Here g denotes the group (1 ≤ g ≤ G), and we re-index
x and y with two indices (group, channel in group). Fig-
ure 1(b) illustrates how non-zero weights in a group conv

11304

x1

x2

x3

x4

=

y1

y2

y3

y4

(e)

y1

y2

y3

y4

Regular Conv

11

21 31

41

12

22

32

42

13 23

33

43

y(j)=෍iW(ij)∗x(i)

x1

y1

*

x2

x3

=

y2

y3

14 24

34

Block-Diagonal 

Conv

11,1

21,1

44

x4

y4

y(j,d)=෍iW(ij,d)∗x(i,d)

x1,1

y1,1

11,2

21,2

12,1

22,1

*

x1,2

x2,1

=

y1,2

y2,1

(c)

Group Conv

(a)

1,11

1,21

1,12

1,22

y(g,j)=෍iW(g,ij)∗x(g,i)

x1,1

y1,1

*

=

x1,2

x2,1

x2,2

y1,2

y2,1

y2,2

2,11

2,21

1,12 1,22

Block-Circulant 

Conv

1

2

5

2

1

6

3

4

7

4

3

8

*

(b)

(d)

12,2

22,2

F*

x2,2

y2,2

6

5

8

7

Block-Circulant Conv (Decomposed via 𝐶=𝐹∗𝐷𝐹)

x1

F

෠1
෠5

෠3
෠7

෠2
෠6

෠4
෠8

F*

*

=

x2

x3

x4

F

Figure 1. Relation between group convs and circulant weights – each square represents a 2D feature/ﬁlter, which can be 1 × 1 for an FC
layer. (a) regular conv layer; (b) group conv with 2 groups; (c) same group conv reordered to show the block-diagonal weight structure; (d)
block-circulant conv layer; (e) same block-circulant conv decomposed into block-DFTs and a block-diagonal conv layer.

× N

form M
G blocks along the main diagonal. A group conv
G
reduces parameter size and fpmuls by a factor of G relative
to an ordinary conv. However, this is achieved by remov-
ing all weight connections between groups and negatively
impacts learning behavior.

A UGConv recovers this lost learning ability by sand-
wiching the group conv between two cross-channel unitary
transforms P and Q (Figure 2(a)). More formally, we can
deﬁne a UGConv is:

˜Xk = PXk

∀k

˜y(g,j) =

M/G

X

i=1

˜x(g,i) ∗ ˜W(g,ij),

1 ≤ j ≤ N/G

(3)

Yl = Q ˜Yl

∀l

For a tensor X containing M channels, Xk is deﬁned as the
M -length vector formed by taking the k’th element/pixel
from each channel. P ∈ CM ×M and Q ∈ CN ×N are

unitary matrix transforms applied element-wise over the in-
put and output channels. We use tilde (˜x, ˜y, ˜W) to indi-
cate tensors in the transformed feature space. Note that: (1)
P and Q can be identity transforms, and thus UGConv in-
cludes group convolutions; (2) unitary transforms preserve
inner products, thus they should not diminish gradient mag-
nitudes in the network; (3) UGConv can also be applied to
FC layers (using 1×1 feature maps and a 1×1 group conv).

One key point to make is the equivalence between a
group conv and a convolution with block-diagonal weights
(i.e. weights which consist of sub-blocks of square diagonal
matrices). Figure 1(c) shows a block-diagonal conv, which
visually already looks identical to the group conv in Fig-
ure 1(b). More formally, divide X and Y into size D × 1
sub-blocks, and W into D × D sub-blocks which are diag-
onal. Let i index the input sub-blocks (0 ≤ i ≤ M/D − 1),
j index the output sub-blocks (0 ≤ j ≤ N/D − 1), and d
index the channels within each sub-block (1 ≤ d ≤ D). We

11305

Unitary Transform P

Group Convolution

Identity

1x1 Group Conv

Channel Shuffle

3x3 Depthwise

Conv

Block Hadamard

1x1 Group Conv

Block Hadamard

3x3 Depthwise

Conv

Unitary Transform Q

1x1 Group Conv

1x1 Group Conv

(a)

Sum

(b)

Sum

(c)

Figure 2. CNN block architectures – (a) a general block for unitary group convolutions; (b) a ShufﬂeNet block reproduced from the
original paper [29]; (c) our proposed HadaNet variation. Note that both ShufﬂeNet and HadaNet blocks contain the UGConv pattern.

can express the block-diagonal conv as follows:

3.2. UGConv and Circulant Networks

y(j ∗D+d) =

M/D−1

X

i=0

x(i∗D+d) ∗ W(i∗D+d j ∗D+d)

Only D convs need to be performed for each D × D sub-
block because they are diagonal. Similar to Equation 2,
we can simplify notation by re-labeling using a tuple (sub-
block, channel in sub-block). This removes the multiplies
by D and allows i and j to start from 1. Then:

y(j,d) =

M/D

X

i=1

x(i,d) ∗ W(ij,d),

1 ≤ j ≤ N/D

(4)

It is easy to see that Equation 4 matches Equation 2.

3.1. UGConv and ShufﬂeNet

ShufﬂeNet is a variant of the MobileNets architecture
in which the pointwise convolutions (which take up 93.4%
of the multiply-accumulate operations [29]) are converted
into group convolutions. However, when multiple group
convs are stacked together, the lack of connections between
groups over many layers prevents the learning of cross-
group correlations. To address this, ShufﬂeNet shufﬂes the
output channels groups in a ﬁxed, round-robin manner —
for each group, the ﬁrst channel is shufﬂed into group 1, the
second channel into group 2, etc. This shufﬂe can be ex-
pressed as a permutation in feature space, and ShufﬂeNets
are thus an example of of UGConvNets where P is identity
and Q is a ﬁxed permutation matrix.

ShufﬂeNet shows experimentally that it is beneﬁcial
to shufﬂe information across groups when stacking group
convs. However, shufﬂing channels is not the only way to
accomplish such information mixing.

Circulant and block-circulant neural networks [6, 23]
utilize layers that impose a block-circulant structure on
the 2D weight
their weight tensors. For an FC layer,
matrix is made to be circulant. For a conv layer,
the
circulant structure is applied over the input and out-
put channels axes. That is to say, given a 4D convo-
lutional weight tensor with shape (height, width,
in channels, out channels), each 2D slice of this
tensor [i,j,:,:] becomes circulant.

Figure 1(d) shows a block-circulant layer where each
2 × 2 sub-block of the weight tensor is circulant. By Equa-
tion (1), each D × D circulant matrix can be decomposed
into a D-length DFT, a diagonal matrix, and a correspond-
ing IDFT. In Figure 1(e), each D ×D sub-block is diagonal-
ized in this fashion. We use tilde to indicate weight values in
the DFT-transformed space. The resulting weight structure
is block-diagonal, and the weight layer sits between two
block-DFT transforms. We know from the previous sec-
tion that block-diagonal weights correspond to group con-
volutions. Therefore, a block-circulant layer is just a group
convolution in a transformed feature space. This of course
falls within the deﬁnition of a UGConv, with P and Q being
block-DFT/IDFT transforms. Note that these DFTs are ap-
plied along the channels, and so circulant networks are not
examining the spatial frequency components of the image.

We make a few additional notes about block-circulant
layers. First, the size of the circulant blocks D is equal
to the number of groups in the equivalent group conv (not
the group size). Thus each D-length DFT touches a single
channel in every group, fully mixing information between
groups. Second, though our example uses a ”square” weight
tensor (i.e. M = N ), non-square block-circulant tensors

11306

can be diagonalized as well. As long as both M and N
are divisible by D, the ’rectangular’ weight tensor can be
divided into D × D blocks. In this case, P ∈ CM×M is not
the inverse of Q ∈ CN×N, but each sub-block along the
diagonal of P is the inverse of the corresponding sub-block
in Q. We say that P is the block-inverse of Q.

Because P and Q are block-inverses, if we directly stack
multiple such blocks many of the transforms will cancel
out. However, practical DNNs include batch norm and/or
nonlinearities between linear layers. The block-DFTs (and
orthogonal transforms in general) do not commute with
channel-wise or pointwise operations, which prevents triv-
ial cancellation. However, note that channel shufﬂes do
commute and cancel out in this manner.

3.3. Discussion of UGConvs

We have provided two speciﬁc examples from literature
(ShufﬂeNet [29] and CirCNN [6]) which combine a struc-
tured sparse weight layer (group convolution) with unitary
transforms. The transforms help to improve cross-channel
representation learning without adding additional parame-
ters. However, the two techniques have important differ-
ences. ShufﬂeNet’s permutations are very lightweight as
they require no arithmetic operations. However, permu-
tations do not affect the sparsity of weight layer. On the
other hand, CirCNN composes block-DFTs with a group
conv to create an effective weight structure (i.e. circulant
weights) which is dense. Moreover, it does so while still
having less asymptotic computational complexity than un-
structured dense weights.

We hypothesize that the representation learning capabil-
ity of a UGConv layer is a function of both the sparsity
of the weights as well as that of the transform. An un-
structured dense weight layer offers the best learning ca-
pability; grouping introduces sparsity and degrades cross-
channel learning performance, some of which can be recov-
ered via transforms. Because dense transforms create dense
weight structures (i.e. circulant weights), we believe they
enable learning a richer set of representations compared to
sparse transforms (i.e. channel shufﬂing). When the weight
sparsity is low (i.e. number of groups is small), the differ-
ence between the two may be negligible in terms of network
accuracy. However, we expect dense transforms to outper-
form shufﬂing when using many groups.

Another difference is that ShufﬂeNet applies channel
shufﬂe on only one side of the weight layer, while CirCNN
effectively applies transformations on both sides. We use
the terms 1-sided and 2-sided UGConvs to refer to these
two cases, and test both in our experiments.

3.4. The Hadamard Transform

One drawback of dense transforms such as DFT is that
they require more computational overhead as compared to

Table 1. Hadamard vs. Discrete Fourier transforms – The en-
tries of the DFT matrix are the complex roots of unity. The entries
of the Hadamard matrix are +1 or −1. The last column shows
the structure of P∗DP where D is a diagonal matrix and P is the
transform; differences are bolded.
Fourier

Hadamard





Transform

P

Structure
of P ∗ DP

FP Muls

FP Adds

1

1

1

1

ω

ω

1 ω

1 ω

2

3

ω

ω

2

4

6

1

3

6

9

ω

ω

ω

















a b
c
d a b
c
b

d
c
d a b
d a
c
n log n
n log n





1

1

1

1

1 −1
1

1 −1
1 −1 −1
1

b

1 −1 −1

c

a
d
b a d c
c
b
d c b a

d a







0

n log n

shufﬂing. Even using the ’fast’ algorithm, each n × n DFT
requires O(n log n) ﬂoating-point multiplies and adds. Fur-
thermore, the fact that the DFT uses complex numbers may
further complicate software/hardware implementations. Fi-
nally, the DFT is taken over the channels where there is no
spatial structure — the transform exists purely to mix infor-
mation across channels and not to perform domain-speciﬁc
analysis. Given this, we would like to ﬁnd a more efﬁcient
alternative.

The Hadamard transform [17] is deﬁned as a matrix con-
taining only +1/−1 elements and whose rows and columns
are mutually orthogonal. Table 1 shows a 4 × 4 Hadamard
matrix. Because all coefﬁcients have magnitude 1, the
transform can be computed without multiplies, i.e. using
adds/subtracts only. This is extremely important as ﬂoating-
point multiplies are typically the computational bottleneck
for DNN computation on both GPUs and specialized hard-
ware. In addition, the Hadamard transform can be gener-
ated recursively like the Fourier transform, meaning that a
fast Hadamard transform (FHT) exists similar to the FFT
to compute a n-length Hadamard transform in O(n log n)
adds/subtracts [17]. The recursive nature of FHT also en-
ables Hadamard kernels to be implemented without explic-
itly storing the matrix itself; instead the matrix can be gener-
ated on the ﬂy (similar to existing implementations of FFT
kernels). This means neither FHT nor FFT requires storing
additional parameters.

Hadamard is more efﬁcient that DFT, but does it achieve
the same learning performance? There is some high-level
intuition that this would be the case: Table 1 compares the
weight structure imposed by P∗DP when P is DFT and
Hadamard. DFT results in a circulant matrix; Hadamard re-
sults in a nearly identical weight matrix with only a few dif-
ferent elements. We hypothesize that there will be no accu-

11307

Table 2. Test error on a toy MNIST network – a ’G’ in the layer width columns indicates a group layer. In the transform columns, P and
Q denote 1-sided pre-conv and post-conv transforms, respectively; PQ denotes a 2-sided transform. All values are averaged over 5 runs
and 90% conﬁdence bounds for each value are at most ±5%.

Layer Width

L2

Conv3x3

L3

FC

L4

FC

Transform

None

Rand Ortho

Rand Perm

P

Q

PQ

P

Q

20

20

20,G

20,G

20,G 10

6%

4% 4% 4% 5%

6%

20,G 10,G 27% 10% 8%

4% 27% 26% 25%

20,G 10

25% 10% 10% 10% 27% 20% 21%

20,G 10,G 60% 23% 17% 20% 57% 55% 57%

PQ

5%

racy impact in replacing circulant weights with Hadamard-
diagonalizable weights in neural nets.

We further speculate that dense unitary transforms in
general, including DFT and Hadamard, achieve comparable
learning performance. This is again because the ordering of
channels in DNNs is essentially random (i.e.
the channel
order encodes no useful information), meaning there are
no patterns that can be exploited by one particular cross-
channel transform and not others. The transforms in UG-
Conv exist solely to connect different channel groups, and
any dense transform will work as well as another. To test
this hypothesis, we experiment with randomly generated or-
thogonal transforms in addition to DFT and Hadamard.

4. Experimental Validation

We ﬁrst present ablation studies on a toy MNIST net-
work followed by deeper CIFAR-10 models. These experi-
ments build up insights on UGConv. We then demonstrate
the utility of Hadamard using grouped ResNets and a Shuf-
ﬂeNet model from literature trained on ImageNet.

4.1. Dense Transforms vs. Shufﬂe

Our ﬁrst experiment uses a toy MNIST network. This al-
lows us to isolate the UGConv block and to compare dense
orthogonal transforms versus permutations in a simple set-
ting. We stress that the goal here is not to build a realistic
classiﬁer. The layer architecture is denoted below, where
each layer is described as (number of channels)(layer type):

10Conv3x3 − 20Conv3x3 − 20FC − 10FC

We perform 2 × 2 max pooling before each 3 × 3 conv layer,
and a global average pool before the ﬁrst FC layer. Each
layer is followed by batch normalization and ReLU.

We convert the ﬁrst FC layer of the network (20FC1,
shown in bold) into a UGConv block (i.e.
it becomes a
grouped FC with transforms). The group number is equal
to the number of channels to maximize sparsity. From this
base architecture we derive three variations: (1) convert the
preceding Conv3x3 layer into group conv; (2) convert the

following FC layer into group FC; (3) convert both sur-
rounding layers into group layers. These test the perfor-
mance of transforms in the context of stacked group layers.
Two types of transform are evaluated: randomly generated
dense orthogonal and random permutation transforms. We
test with both 1-sided (using one of P or Q and setting the
other to identity) and 2-sided UGConvs (P = Q−1). All
results are averaged over ﬁve runs, and we regenerate the
random transformation matrices between runs.

Table 2 shows our results. Due to the small size of the
network, the 90% conﬁdence bound for these values can
be as large as ±5%. Nevertheless, differences between
transforms are clearly demonstrated. When L3 is the only
grouped layer in the network (row 1), transforms have little
to no effect. However, when two or more group layers are
stacked together, the dense orthogonal transforms achieve
improved accuracy. Permutations did not improve accuracy
in any experiment. This is a clear (albeit artiﬁcial) demon-
stration that when the number of groups is very large, dense
transforms outperform permutations in learning ability.

Another interesting observation is that there is little dif-
ference between 1-sided and 2-sided transforms, regardless
of whether the UGConv block is stacked before or after an-
other group layer. For example, in Table 2 row 3, a dense
orthogonal transform improves accuracy even when it is
placed after both group layers.
It may be surprising that
a transform affects layers preceding it. But keep in mind
that the transform also affects gradients on the backwards
pass, allowing the same weights to ’see’ more downstream
activations during backpropagation. Alternatively, we can
view the UGConv layer as a learnable structured weight
layer (see Section 3.2) — within this perspective, the weight
structure is a function of transforms both before or after.

4.2. Evaluation of Different Transforms

We have shown that dense orthogonal transforms can im-
prove over shufﬂes in small DNNs with large group sizes.
To validate our results on more realistic architectures, we
perform experiments on CIFAR-10 [12] using ResNet [7].
We use UGConvs to replace the two 3 × 3 convolutions in

11308

Table 3. Test error for UGConvs on CIFAR-10 – The ﬁrst three columns show the number of groups used in the three stages (S1-S3).
The Base column shows the test error with no transforms, and the other columns show improvement in test error over this baseline. Some
entries are blank due to insufﬁcient time to complete the experiments.

# of Groups

Base

1-sided Transforms

2-sided Transforms

Params

S1

S2

S3

Shufﬂe Hada Ortho

Shufﬂe*

Fourier Hada Ortho

ResNet-20

ResNet-56

ShufﬂeNet-29

ShufﬂeNet-56

Mean

4

8

4

8

4

8

4

8

4

8

8

16

8

16

8

16

8

16

8

16

16

32

16

32

16

32

16

32

16

32

19.5% 3.3% 4.0% 4.0%

23.8% 2.9% 4.3% 3.9%

16.0% 4.0% 4.4% 4.2%

20.6% 5.4% 6.1% 6.4%

18.3% 2.7% 2.4% 3.1%

22.1% 0.6% 3.4% 3.6%

16.2% 3.6% 3.5% 3.4%

19.7% 4.3% 4.4% 4.9%

17.5% 3.4% 3.6% 3.7%

21.5% 3.3% 4.6% 4.7%

3.1%

4.1%

4.0%

5.8%

3.8%

3.8%

3.9%

5.2%

3.7%

4.7%

4.1% 4.2% 3.8%

5.4% 5.4% 5.3%

4.7% 4.5% 4.6%

7.1% 7.2% 6.8%

4.9% 4.5% 4.2%

5.1%

4.6%

5.0% 5.3%

4.5% 4.7%

6.0% 6.0% 6.0%

4.6% 4.4% 4.3%

5.9% 5.9% 5.9%

25K

14K

76K

41K

23K

17K

41K

29K

each ResNet block, and to replace the 1 × 1 projection lay-
ers. ResNets are divided into three stages (S1, S2, S3), with
later stages having more channels. We use more groups in
later stages, keeping the ratio of channels to groups con-
stant. Two models are tested: ResNet-20 (3 block per stage)
and ResNet-56 (9 blocks per stage). We also experiment
with the same high-level architecture but using the build-
ing block from ShufﬂeNet [29]. This block which con-
tains two 1 × 1 convs and a 3 × 3 depthwise conv (see
Figure 2(b)). Following ShufﬂeNet we apply transforma-
tions around the ﬁrst 1 × 1 group conv only and make no
changes to the second group conv. Again, two models are
tested: ShufﬂeNet-29 (3 block per stage) and ShufﬂeNet-
56 (6 blocks per stage).

We use layer widths and training hyperparameters from
[7] and make use of standard data augmentations: padding
8 pixels on each side and randomly cropping back to origi-
nal size, combined with a random horizontal ﬂip [7, 9, 15].
Each network is trained for 200 epochs, and we report the
mean test error over the last 5 epochs.

We test

the following transforms:

identity (None),
ShufﬂeNet permutation (Shufﬂe), block-Hadamard (Hada),
block-DFT (Fourier), and block-random-orthogonal (Or-
tho). The block transforms follow the same structure de-
scribed in Section 3.2. For each transform, both 1-sided
(letting Q be the transform and P identity) and 2-sided (P
and Q are block-inverses) versions are tested where rea-
sonable. The 1-sided DFT is left out because it introduced
complex numbers into the network. For the 2-sided chan-
nel shufﬂe (Shufﬂe*), we set P = Q to essentially perform
additional shufﬂing; this is done since using block-inverse
shufﬂes will lead to trivial cancellation. All results are dis-
played in Table 3 — the error rate with no transforms is
given ﬁrst followed by the accuracy improvement achieved

with each UGConv setup. Our base error rates are high for
CIFAR-10 because group convolutions signiﬁcantly com-
press the network

A key result here is that dense orthogonal transforms per-
form similarly in accuracy. Fourier, Hada, and Ortho obtain
results which are within a spread of 0.4% in both 1-sided
and 2-sided settings. On the other hand, the shufﬂe trans-
forms (1 and 2-sided) clearly perform worse for the larger
group sizes. This conﬁrms our hypothesis that Hadamard
is comparable to DFT in learning performance while being
much easier to compute. It also provides evidence that all
dense UGConvs achieve comparable learning performance.
Another observation is that 2-sided transforms signiﬁ-
cantly outperform their 1-sided variants, which is different
from the MNIST data. We currently do not have an explana-
tion for this effect. One speculation was that 2-sided trans-
forms perform better when the number of input and output
channels did not match. However, further testing with the
small MNIST network showed that this was not the case.

Finally, note that the accuracy trends remained the same
whether the transforms were applied to 3 ×3 group convs in
ResNet or 1 ×1 group convs in ShufﬂeNet. This is evidence
that spatial and cross-channel dependencies are effectively
decoupled in convolutional layers, and that the size of the
ﬁlter does not signiﬁcantly affect channel-space transforms.

4.3. Hadamard Networks on ImageNet

The data from previous sections point to two regimes:
at low weight sparsity (i.e. small group numbers) a simple
shufﬂe is sufﬁcient to maximize accuracy. At large group
numbers, however, dense transforms outperform shufﬂes.
This section evaluates the 2-sided block-Hadamard trans-
form against shufﬂe on ImageNet. Hadamard was chosen
as it is far more efﬁcient than other dense unitary transforms

11309

Table 4. Top-1 classiﬁcation error on ImageNet – we include data on both the original ShufﬂeNet (with our own code) and our pre-
activation variation. Our baseline ShufﬂeNet implementation is close to the literature results (52.7%). For each model we show the number
of parameters and fpmuls, as well as the overhead in additions from the Hadamard transform.

Shufﬂe Hada

Delta

Params FPmuls Hada Adds

ResNet-18 g8

46.4% 44.6% (-1.8%)

ResNet-18 g16

55.8% 52.3% (-3.5%)

1.9M

1.2M

ShufﬂeNet-x0.25 g8

53.6% 52.6% (-1.0%)

0.46M

330M

226M

17M

7.8M

10.4M

0.95M

(see Section 3.4, and ShufﬂeNet was used for comparison as
it is highly related work and a strong baseline. We refer to
networks using Hadamard UGConvs as HadaNets. Figure 2
compares the residual blocks of ShufﬂeNet and HadaNet.

Due to hardware constraints, we chose small models
with fairly large group size — this is the setting where dense
transforms should perform the best compared to shufﬂe.
We evaluate ResNet-18 following the ImageNet architec-
ture from [7] and using group sizes 8 and 16 throughout
the network. We also test with the ShufﬂeNet-x0.25 g8,
which is the smallest ShufﬂeNet variant from [29]. This
network has 50 layers and also uses 8 groups. Each net-
work was trained with the hyperparameters and learning
rate schedule described in their respective papers. We com-
pare 1-sided shufﬂe to 2-sided block-Hadamard (note that
ShufﬂeNet from literature already contains the 1-sided shuf-
ﬂe). All results are displayed in Table 4. Our reproduction
of ShufﬂeNet-x0.25-g8 achieved a Top-1 error of 53.6%,
which is close to the 52.7% reported in Table 2 of [29].

The results demonstrate that the Hadamard transform
can indeed outperform shufﬂing in terms of accuracy on
large scale datasets. ResNet-18 with group convs is a non-
standard model, but it serves to show that the trends ob-
served in CIFAR-10 ResNets carry over to ImageNet. On
the other hand, ShufﬂeNet is a well-optimized baseline
which obtains good accuracy performance on a very tight
parameter and fpmul budget. In addition, despite very lit-
tle hyperparameter tuning, HadaNet was able to improve
slightly over ShufﬂeNet.

4.4. Practicality of HadaNet

HadaNet slightly outperforms ShufﬂeNet on accuracy,
but requires extra ﬂoating-point adds. An N -channel group
conv with B groups requires N 2/B fpmuls for the weight
layer and 2N log B adds for the two block-Hadamard trans-
forms. Compared to multiplies, additions are already much
cheaper in hardware. The last column of Table 4 shows
the number of additions needed for each network if the
fast Hadamard transform is used. The relative overhead of
HadaNet is fairly small: the extra adds amount to only 2-5%
of existing multiply-accumulates in those networks.

However, the overhead of the Hadamard transform de-
pends on a well-optimized implementation. The reason we
did not show runtime on GPU is that an O(n log n) fast

Hadamard kernel operating along the channels is not cur-
rently available — as a result our own HadaNet implemen-
tation is fairly slow.

On the other hand, we believe the Hadamard transform
might be useful for specialized DNN accelerators imple-
mented with FPGAs [3] or ASICs [11]. Top computer hard-
ware conferences already contain works demonstrating the
use of circulant matrices for DNN compression in dedicated
hardware [6, 22, 4]. These works show that DFTs can be
very efﬁciently implemented in a dedicated module due to
its recursive nature. We choose Hadamard because it also
has the same recursive properties, meaning it should be even
simpler in hardware due to lower computational complex-
ity. All-in-all, this paper reveals that in high weight spar-
sity regimes, dense transforms outperform simple shufﬂing.
HadaNet is more efﬁcient than the existing state-of-the-art
dense transform (i.e. DFT transforms) while achieving sim-
ilar accuracy performance in DNNs.

5. Conclusions and Future Work

We introduce the concept of unitary group convolutions,
a composition of group convolutions with unitary trans-
forms in feature space. We use the UGConv framework
to unify two disparate ideas in CNN literature, ShufﬂeNets
and block-circulant networks, and provide valuable insights
into both techniques. UGConvs with dense unitary trans-
forms demonstrate superior ability to learn cross-channel
mappings versus ordinary and shufﬂed group convolutions.
Based on these these observations we propose HadaNet,
a variant of ShufﬂeNet that improves accuracy on the Im-
ageNet dataset without incurring additional parameters or
ﬂoating-point multiplies.

One future work is to replace the Hadamard transform
with a trained 0, +1, −1 transform; training may allow the
transform to adapt to the weights, and introducing zeros en-
ables sparse compute reduction.

Acknowledgments

This work was supported in part by the Semiconductor Re-
search Corporation (SRC) and DARPA. We would like to
thank Prof. Yanzhi Wang (Northeastern University), Prof.
Bo Yuan (Rutgers University), and their students for pro-
viding technical details and code regarding CirCNN [6].

11310

References

[1] Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok
Choudhary, and Shi-Fu Chang. An Exploration of Parameter
Redundancy in Deep Networks with Circulant Projections.
Int’l Conf. on Computer Vision (ICCV), pages 2857–2865,
2015.

[2] Franc¸ois Chollet. Xception: Deep learning with Depthwise
Separable Convolutions. Conf. on Computer Vision and Pat-
tern Recognition (CVPR), Jun 2016.

[3] Eric Chung, Jeremy Fowers, Kalin Ovtcharov, Michael Pa-
pamichael, Adrian Caulﬁeld, Todd Massengill, Ming Liu,
Daniel Lo, Shlomi Alkalay, Michael Haselman, Maleen
Abeydeera, Logan Adams, Hari Angepat, Christian Boehn,
Derek Chiou, Oren Firestein, Alessandro Forin, Kang Su
Gatlin, Mahdi Ghandi, Stephen Heil, Kyle Holohan, Ah-
mad El Husseini, Tamas Juhasz, Kara Kagi, Ratna K.
Kovvuri, Sitaram Lanka, Friedel van Megen, Dima Mukhor-
tov, Prerak Patel, Brandon Perez, Amanda Grace Rapsang,
Steven K. Reinhardt, Bita Darvish Rouhani, Adam Sapek,
Raja Seera, Sangeetha Shekar, Balaji Sridharan, Gabriel
Weisz, Lisa Woods, Phillip Yi Xiao, Dan Zhang, Ritchie
Zhao, , and Doug Burger. Serving DNNs in Real Time
at Datacenter Scale with Project Brainwave . IEEE Micro,
38(2):8–20, 2018.

[4] Chunhua Deng, Siyu Liao, Yi Xie, Keshab K. Parhi, Xue-
hai Qian, and Bo Yuan. PermDNN: Efﬁcient Compressed
Deep Neural Network Architecture with Permuted Diagonal
Matrices . Int’l Symp. on Microarchitecture (MICRO), Oct
2019.

[5] T. Ceren Deveci, Serdar Cakir, and A. Enis Cetin. En-
ergy Efﬁcient Hadamard Neural Networks. arXiv preprint,
arXiv:1805.05421, May 2018.

[6] Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning
Liu, Youwei Zhuo, Chao Wang, Xuehai Qian, Yu Bai,
Geng Yuan, Xiaolong Ma, Yipeng Zhang, Jian Tang, Qinru
Qiu, Xue Lin, and Bo Yuan. CirCNN: Accelerating and
Compressing Deep Neural Networks using Block-Circulant
Weight Matrices.
Int’l Symp. on Microarchitecture (MI-
CRO), pages 395–408, 2017.

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep Residual Learning for Image Recognition. arXiv e-
print, arXiv:1512.0338, Dec 2015.

[8] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient Convo-
lutional Neural Networks for Nobile Vision Applications.
arXiv e-print, arXiv:1704.04861, 2017.

[9] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q
Weinberger. Deep Networks with Stochastic Depth. Euro-
pean Conference on Computer Vision (ECCV), pages 646–
661, 2016.

[10] Yani Ioannou, Duncan Robertson, Roberto Cipolla, and An-
tonio Criminisi. Deep Roots: Improving CNN Efﬁciency
with Hierarchical Filter Groups. Conf. on Computer Vision
and Pattern Recognition (CVPR), Jun 2017.

[11] Norman P Jouppi, Cliff Young, Nishant Patil, David Patter-
son, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh

Bhatia, Nan Boden, Al Borchers, et al. In-Datacenter Perfor-
mance Analysis of a Tensor Processing Unit. Int’l Symp. on
Computer Architecture (ISCA), pages 1–12, 2017.

[12] Alex Krizhevsky and Geoffrey Hinton. Learning Multiple

Layers of Features from Tiny Images. Tech report, 2009.

[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-
agenet Classiﬁcation with Deep Convolutional Neural Net-
works. Advances in Neural Information Processing Systems
(NIPS), pages 1097–1105, 2012.

[14] Quoc Le, Tam´as Sarl´os, and Alex Smola. Fastfood —Ap-
Int’l

proximating Kernel Expansions in Loglinear Time.
Conf. on Machine Learning (ICML), 85, 2013.

[15] Min Lin, Qiang Chen, and Shuicheng Yan. Network in Net-

work. arXiv e-print, arXiv:1312.4400, 2013.

[16] Marcin Moczulski, Misha Denil, Jeremy Appleyard, and
Nando de Freitas. ACDC: A Structured Efﬁcient Linear
Layer.
Int’l Conf. on Learning Representations (ICLR),
2016.

[17] William K Pratt, Julius Kane, and Harry C Andrews.
Hadamard Transform Image Coding. Proceedings of the
IEEE, 57(1):58–68, 1969.

[18] Laurent Sifre. Rigid-Motion Scattering for Image Classiﬁca-

tion. Ph.D. thesis, 2014.

[19] Karen Simonyan and Anderw Zisserman. Very Deep Convo-
lutional Networks for Large-Scale Image Recognition. arXiv
e-print, arXiv:1409.15568, Apr 2015.

[20] Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Struc-
tured Transforms for Small-Footprint Deep Learning. Ad-
vances in Neural Information Processing Systems (NIPS),
pages 3088–3096, 2015.

[21] Ke Sun, Mingjie Li, Dong Liu, and Jingdong Wang. IGCV3:
Interleaved Low-Rank Group Convolutions for Efﬁcient
Deep Neural Networks. arXiv e-print, arXiv:1806.00178,
Jun 2018.

[22] Shuo Wang, Zhe Li, Caiwen Ding, Bo Yuan, Qinru Qiu,
Yanzhi Wang, and Yun Liang. C-LSTM: Enabling Efﬁ-
cient LSTM using Structured Compression Techniques on
FPGAs.
to appear in International Symposium on Field-
Programmable Gate Arrays (FPGA), 2018.

[23] Yanzhi Wang, Caiwen Ding, Zhe Li, Geng Yuan, Siyu Liao,
Xiaolong Ma, Bo Yuan, Xuehai Qian, Jian Tang, Qinru Qiu,
and Xue Lin. Towards Ultra-High Performance and En-
ergy Efﬁciency of Deep Learning Systems: An Algorithm-
Hardware Co-Optimization Framework. AAAI Conf ’ on Ar-
tiﬁcial Intelligence (AAAI), Feb 2018.

[24] Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai,
Richang Hong, and Guo-Jun Qi. IGCV2: Interleaved Struc-
tured Sparse Convolutional Neural Networks. arXiv e-print,
arXiv:1804.06202, Apr 2018.

[25] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated Residual Transformations for Deep
Neural Networks. Conf. on Computer Vision and Pattern
Recognition (CVPR), Jun 2017.

[26] Xiaowei Xu, Yukun Ding, Sharon Xiaobo Hu, Michael
Niemier, Jason Cong, Yu Hu, and Yiyu Shi. Scaling for
Edge Inference of Deep Neural Networks. Nature Electron-
ics, 1(4):216, 2018.

11311

[27] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de
Freitas, Alex Smola, Le Song, and Ziyu Wang. Deep Fried
Convnets.
Int’l Conf. on Computer Vision (ICCV), pages
1476–1483, 2015.

[28] Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. In-
terleaved Group Convolutions. Conf. on Computer Vision
and Pattern Recognition (CVPR), Jun 2017.

[29] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian
Sun. ShufﬂeNet: An Extremely Efﬁcient Convolutional
Neural Network for Mobile Devices.
arXiv e-print,
arXiv:1707.01083, Aug 2017.

11312

