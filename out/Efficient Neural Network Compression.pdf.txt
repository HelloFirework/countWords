Efﬁcient Neural Network Compression

Hyeji Kim, Muhammad Umar Karim Khan, and Chong-Min Kyung

Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea

{hyejikim89, umar, kyung}@kaist.ac.kr

Abstract

Network compression reduces the computational com-
plexity and memory consumption of deep neural networks
by reducing the number of parameters. In SVD-based net-
work compression the right rank needs to be decided for
every layer of the network. In this paper we propose an ef-
ﬁcient method for obtaining the rank conﬁguration of the
whole network. Unlike previous methods which consider
each layer separately, our method considers the whole net-
work to choose the right rank conﬁguration. We propose
novel accuracy metrics to represent the accuracy and com-
plexity relationship for a given neural network. We use
these metrics in a non-iterative fashion to obtain the right
rank conﬁguration which satisﬁes the constraints on FLOPs
and memory while maintaining sufﬁcient accuracy. Exper-
iments show that our method provides better compromise
between accuracy and computational complexity/memory
consumption while performing compression at much higher
speed. For VGG-16 our network can reduce the FLOPs by
25% and improve accuracy by 0.7% compared to the base-
line, while requiring only 3 minutes on a CPU to search
for the right rank conﬁguration. Previously, similar re-
sults were achieved in 4 hours with 8 GPUs. The proposed
method can be used for lossless compression of a neural
network as well. The better accuracy and complexity com-
promise, as well as the extremely fast speed of our method
makes it suitable for neural network compression.

1. Introduction

Deep convolutional neural networks have been consis-
tently showing outstanding performance in a variety of ap-
plications, however, this performance comes at a high com-
putational cost compared to past methods. The millions
of parameters of a typical neural network require immense
computational power and memory for storage. Thus, model
compression is required to reduce the number of parame-
ters of the network. Our aim in this paper is to develop
a method that can optimize trained neural networks for re-
duction in computational power and memory usage while

(cid:20)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

r1

I1

I1

(cid:68)
(cid:66)
(cid:20)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:21)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

r2

O1

I2

O1    I2

(cid:69)
(cid:66)
(cid:20)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:68)
(cid:66)
(cid:21)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

O2

I3

O3

(cid:22)
(cid:38)
(cid:41)

(cid:69)
(cid:66)
(cid:21)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

O2     I3

(cid:68)
(cid:66)
(cid:22)
(cid:38)
(cid:41)

(cid:69)r3
(cid:66)
(cid:22)
(cid:38)
(cid:41)

O3

Figure 1. Convolutional and fully-connected layers and their de-
composed counterparts. Each layer is split into two with low-rank
decomposition.

providing competitive accuracy.

Generally, ﬁlter pruning techniques have been used for
network compression. The aim of such approaches is to
develop pruning policies, which can satisfy the target con-
straints on FLOPs and memory. Layer wise search-based
heuristic methods [11, 18, 23], reinforcement learning [1, 4,
10], and genetic and evolutionary algorithms [22, 25] have
been used to deﬁne the pruning policy. A greedy selec-
tion method based on a heuristic metric has been proposed
in [5, 21] to prune multiple ﬁlters of the network together.

Another approach towards network compression is using
kernel decomposition over each ﬁlter in the network. Con-
volutional and fully connected layers can be represented as
matrix multiplications, and kernel decomposition can be ap-
plied to these matrices [2, 7, 12, 15, 28]. Kernel decompo-
sition with singular value decomposition (SVD) automati-
cally assigns importance (the singular values) to the decom-
posed kernels. This automatic sorting makes ﬁlter pruning
easier, as the decomposed kernels with the lower parameters
are the ﬁrst to be pruned. Simply put, low-rank approxima-
tion of a layer decomposes it into two matrix multiplications
for network compression as shown in Fig. 1.

With kernel decomposition schemes, the problem boils
down to the choice of the optimal compression ratio for each
layer of the network. We need to ﬁnd the right rank con-
ﬁguration (i.e. compression ratios) for the whole network
that satisﬁes constraints on speed, memory and accuracy.
This is different from past methods which use a solver to

12569

y
g
r
e
n
e
-
A
C
P
 
d
e
z
i
l
a
m
r
o
N

1

0.8

0.6

0.4

0.2

0

y
c
a
r
u
c
c
A
 
d
e
z
i
l
a
m
r
o
N

1

0.8

0.6

0.4

0.2

0

Conv1
Conv2
Conv3
Conv4
Conv5

Conv1
Conv2
Conv3
Conv4
Conv5

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

Normalized Rank

(a) yp,l

Normalized Rank

(b) ym,l

Figure 2. Interpolated function of layer-wise accuracy metric in
(b) ym,l
AlexNet.
based on normalized accuracy using partial training dataset.

(a) yp,l based on normalized PCA-energy.

minimize the approximation error at the ﬁxed rank [19, 20]
and a training technique to better compensate for accuracy
loss [3,27]. An iterative search-based approach was adopted
in [9, 14, 28] to obtain the right rank conﬁguration. In [28],
the authors deﬁne an PCA energy-based accuracy feature
and use it to select a layer to be compressed in every it-
eration. The ﬁnal rank of each layer is the result of itera-
tive layer-wise network compression. Reinforcement learn-
ing was used in [9] to ﬁnd the rank of each layer indepen-
dently. Unlike [28] and [9], which optimize each layer sepa-
rately, [14] searched for the right rank conﬁguration for the
whole network. Although, [14] shows better performance
compared to [28] and [9], it still takes signiﬁcant amount of
time due to its iterative search for the right rank conﬁgura-
tion.

In this paper, we propose Efﬁcient Neural network Com-
pression (ENC) to obtain the optimal rank conﬁguration
for kernel decomposition. The proposed method is non-
iterative; therefore, it performs compression much faster
compared to numerous recent methods. Speciﬁcally, we
propose three methods: ENC-Map, ENC-Model and ENC-
Inf. ENC-Map uses a mapping function to obtain the right
rank conﬁguration from the given constraint on complexity.
ENC-Model uses a metric representative of the accuracy
of the whole network to ﬁnd the right rank conﬁguration.
ENC-Inf uses both the accuracy model and inference on a
validation dataset to arrive at the right rank conﬁguration.
The code for our method is available online.1

The rest of the paper is structured as follows. Section
2 and 3 describe the accuracy metrics. Section 4 discusses
ENC-Map. Search-based methods, ENC-Model and ENC-
Inf, are given in Section 5. Experiments are discussed in
Section 6 and Section 7 concludes the paper.

2. Layer-wise Accuracy Metrics

The error of the neural network can be divided over the
constituent layers of the network. In other words, each layer

1https://github.com/Hyeji-Kim/ENC

contributes to the error or accuracy of the neural network. In
this section, we describe a couple of metrics, which repre-
sents the accuracy contributed by a layer as a function of the
rank of that layer. This layer-wise accuracy metrics can be
used to predict the contribution of an individual layer to the
overall accuracy of the network. The ﬁrst metric is based
on heuristics while the second involves computing the ac-
curacy over a validation dataset.

Prl

PCA energy-based Metric. After singular value de-
composition (SVD), the number of principal components
retained directly affects the complexity as well as the accu-
racy. The un-normalized PCA energy is given by σ′
l(rl) =
d=1 σl(d), where rl is the rank of the l-th layer in the
neural network and σl(d) is the d-th singular value after
performing SVD on the parameters on l-th layer.
In de-
tail, σ′
l(rl) is the sum of the ﬁrst rl diagonal entries of the
diagonal matrix after decomposition. It is obvious that the
accuracy decreases with the decrease in un-normalized PCA
energy. The PCA energy of the l-th layer with a rank rl is
obtained by performing min-max normalization to the un-
normalized PCA energy, i.e.,

yp,l(rl) =

σ′
l(rl) − σ′
l(rmax
σ′

) − σ′

l(1)

l

l(1)

.

(1)

Here rmax

l

is the maximum or initial rank.

Measurement-based Metric. The second metric for
layer-wise accuracy that we use in this paper is based on
evaluation of the neural network on a validation dataset. For
the l-th layer, the accuracy model is obtained by changing
rl while keeping the rest of the network unchanged. Empir-
ical models are developed for each layer. Note that the pos-
sible number of ranks for a layer occupies a large linear dis-
crete space, making it impractical to evaluate the accuracy
over the validation dataset. Therefore, we use VBMF [24]
to sample ranks over which the accuracy is estimated, and
the ranks are sampled to make the measured accuracy in-
crease. Then, we follow it up by Piecewise Cubic Hermite
Interpolating Polynomial (PCHIP) algorithm. We denote
the measurement-based layer-wise accuracy metric of the
l-th layer by ym,l(rl).

In Fig. 2, we show the result of interpolation with both
the PCA energy-based and measurement-based approaches.
Both the metrics show different proﬁles. This is because the
PCA energy is not a representation of accuracy in itself but
monotonic with the overall accuracy.

3. Accuracy Metric

In this section, we describe how we can use the layer-
wise metrics to represent the accuracy of the complete neu-
ral network. We estimate the joint distribution of the output
of the neural network and the conﬁguration of its layers.
The conﬁguration of the layers of a neural network is de-
ﬁned only by the choice of the rank for each layer.

12570

Our aim is to maximize the accuracy of the network with
a given set of constraints. Let us deﬁne the conﬁguration of
a neural network through the rank conﬁguration, which is a
set of ranks of each layer, i.e.,

R = {r1, r2, ..., rL},

(2)

weighted the PCA energy only with the network complexity
to reduce the inﬂuence of the Ap(R) at lower complexities
and the Am(R) at higher complexities. Mathematically,

Ac(R) = (cid:26)Ap(R) ×

C(R)

Corig(cid:27) + Am(R),

(7)

where rl is the rank of the l-th layer. The overall accuracy
of the network can be represented by the joint distribution
of accuracy of individual layers. Precisely,

where C(R) is the complexity of the rank conﬁguration R
and Corig is the total complexity of the network. The com-
plexity C(R) is deﬁned by

P(A; R) = P(a1, a2, ..., aL; R),

(3)

where al is the accuracy provided by the l-th layer. Practi-
cally, the accuracy contribution of a layer also depends on
the ranks of other layers. However, the accuracy model can
be simpliﬁed as the function of layer-wise accuracy metric
contribution of a layer by applying the layer-wise accuracy
metrics in Sec. 2, yp,l(rl) and ym,l(rl), which depend on
the rank of the layer only. Considering the independence,
we can model the overall accuracy metric as

P(A; R) =

L

Yl=1

P(al; rl).

(4)

This representation has been inspired by [28] where the
product of layer-wise accuracy metrics is used to estimate
the overall accuracy.

To estimate the accuracy, we deﬁne three types of overall
accuracy metric; measurement-based Am(R), PCA energy-
based Ap(R), and the combination of two metrics, Ac(R).
The layer-wise metric based on the measured accuracy can
be directly used to replace P(al; rl), i.e.,

Am(R) =

L

Yl=1

ym,l(rl).

(5)

Although the normalized PCA energy of (1) is not deﬁned
from the accuracy, it was shown in [28] that the PCA energy
of a layer is proportional to the accuracy of the network,
allowing us to use the PCA energy as the accuracy metric.
Thus, we deﬁne the PCA energy-based accuracy metric as

Ap(R) =

L

Yl=1

yp,l(rl).

(6)

In our experiments, we have noticed that the PCA energy
metric does not accurately represent the accuracy at very
low complexity. Also, as the network is redundant, the
measurement-based metric can not sufﬁciently represent the
effect of complexity reduction against accuracy at higher
complexities. Therefore, we deﬁne a combined metric. This
metric takes into consideration both the PCA energy and the
measurement based layer-wise accuracy metrics. We have

C(R) =

L

Xl=1

Cl(rl) =

L

Xl=1

clrl,

(8)

where Cl(rl) is the complexity of the l-th layer. The com-
plexity coefﬁcient cl for spatial [12] and channel [7] decom-
position is cl = WlHlDl(Il + Ol) and cl = WlHl(IlD2
l +
Ol), respectively. Here Wl and Hl are the width and height
of output feature map, Dl is the size of ﬁlter window, Il and
Ol are the number of input channels and ﬁlters.

As will be seen shortly, we are not interested in the exact
value of the estimated accuracy but rather the relative value
of accuracy metric obtained from the various rank conﬁgu-
rations. In other words, we use the accuracy metric to ex-
tract some partial rank conﬁgurations with the largest value
of accuracy metric.

4. ENC-Map: Rank Conﬁguration with

Accuracy-Complexity Mapping

In this section, we present a simple method to choose
the rank conﬁguration for a neural network by mapping
complexity against accuracy. The mapping is performed
through the rank conﬁgurations, as both complexity and
accuracy are functions of rank conﬁgurations. At ﬁrst,
we intuitively though that all layers having same accuracy
penalty would be better compression strategy than having
same compression ratio (i.e. uniform). Therefore, we only
consider the rank conﬁgurations for which layer-wise met-
rics are equal for every layer. Mathematically,

Re = R | yi,l(rl) = yi,k(rk),

(9)

where l, k ∈ 1, 2, ..., L and i ∈ {p, m} . Next the com-
plexity of Re, C(Re), is computed using (8). The accuracy
metric and complexity are plotted against each other over
Re, which provides a mapping between complexity and ac-
curacy metric. The mapping is mathematically given as

fC−A : R → R.

(10)

Also note that the mappings from complexity and accuracy
metric to the rank conﬁguration and vice versa do exist as
well, i.e.,

fC−R : R → RL.

(11)

12571

Total Complexity

(cid:2159)

(cid:2200)(cid:2779)
rank

+(cid:2012)(cid:3046)
Ct
-(cid:2012)(cid:3046)

(cid:2200)(cid:2779)
rank

Rmin

Rmax

(cid:2200)(cid:2778)
rank

(cid:2195)(cid:2183)(cid:2206)

(cid:2200)(cid:2779)

(cid:2195)(cid:2191)(cid:2196)

(cid:2200)(cid:2779)

(cid:1872)(cid:2870)

(cid:1872)(cid:2869)

Rmax

Rmin

(cid:2195)(cid:2191)(cid:2196)

(cid:2200)(cid:2778)

(cid:2195)(cid:2183)(cid:2206)

(cid:2200)(cid:2778)

(cid:2200)(cid:2778)
rank

Figure 3. Extraction of candidate rank conﬁgurations (as an exam-
ple of two-layered CNN). The effective space is deﬁned by bound-
aries of rank conﬁguration, Rmax and Rmin, and step size tl,
where the complexity is around target complexity Ct with space
margin ±δs. The candidate rank conﬁgurations are lying on Ct
with the complexity margin ±δm denoted as star points.

Using this mapping, the respective rank conﬁguration is cal-
culated from the inverse function of layer-wise accuracy
metric, where a = yk,l(rl) is converted to rl = y−1
k,l (a)
and k ∈ {p, m}. Note that the inverse exists as yk,l(rl)
is an increasing function as shown in Fig. 2. Here, a is the
achievable layer-wise accuracy metric obtained from fC−A,
while satisfying constraints on complexity. This method is
called ENC-Map as we use a simple mapping to obtain the
right rank conﬁguration.

5. ENC-Model/Inf: Rank Conﬁguration in

Combinatorial Space

The method described in the previous section strongly
depends on the equal layer-wise metrics. Therefore, we ex-
tend the rank conﬁguration to the combinatorial problem
to choose the optimal rank conﬁguration in the non-equal
layer-wise metrics.

The combinatorial space is deﬁned by the Cartesian
product of the vector space of rank for each layer. In this
space, we extract the partial rank conﬁgurations that sat-
isfy the target complexity called candidate rank conﬁgura-
tions as illustrated in Fig. 3. To quickly extract the candi-
date rank conﬁgurations and ﬁnd the optimal rank conﬁgu-
ration in non-iterative manner, we perform two steps. First,
we limit the range of the ranks with ENC-Map. Then, we
extract the candidate rank conﬁgurations by hierarchically
grouping the layers to reduce the number of effective layers
for sub-space generation.

5.1. Limiting the Search Space

To limit the search space, we use the simple method de-
scribed in the previous section. We obtain the upper and
lower bounds on the search space near the target complex-
ity by using the mapping in (11) by

Rmax = fC−R(Ct + δs),

(12)

Rmax
Rmax
Rmin
Rmin
Ro
Rtarget

 

k
n
a
R
d
e
z
i
l
a
m
r
o
N

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

2

3

4

5

6

7

8

9

10 11 12 13

Layer Index

Figure 4. Distribution of normalized rank conﬁgurations for VGG-
16 (Ct=25%, δs=10%). A ﬁnal rank conﬁguration Ro is selected
within the boundaries, Rmax(C=35%) and Rmin(C=15%).

Rmin = fC−R(Ct − δs).

(13)

Here Ct is the complexity constraint and δs is the space
margin. The ﬁnal rank conﬁguration, Ro, is deﬁned within
the space boundaries, Rmax and Rmin, as shown in Fig. 4.

5.2. Min offsetting the Search Space

In the limited space by the Rmax and Rmin, we are only

interested in the candidate rank conﬁgurations R as

R = [ R | Ct − δm < C(R) < Ct + δm,

(14)

for R ∈ [Rmin, Rmax]. R includes the rank conﬁgurations
for which the complexity is roughly equal to Ct. Note that
we slightly expand the search space by using the param-
eter δm. To simplify the computations, we shift the space
boundaries from [Rmin, Rmax] to [0, Rmax − Rmin]. Then,
we generate the differential search space and extract the dif-
ferential candidate rank conﬁgurations ˆR deﬁned by
ˆR = [ R | ∆Ct − δm < C(R) < ∆Ct + δm,

(15)

for ∆Ct = C(Rmax) − Ct and R ∈ [0, Rmax − Rmin].
Note that R = Rmax − ˆR.

5.3. Hierarchical Extraction of ˆR

It is not feasible to generate the complete combinatorial
search space for deep neural network, since the space com-
plexity is exponential to the number of layers. Hence, we hi-
erarchically generate the sub-spaces by grouping some lay-
ers in a top-down manner as illustrated in Fig. 5 and extract
the candidate rank conﬁgurations.

As denoted in (8), the complexity is the weighted sum
of the complexity coefﬁcient and rank of each layer. From
the min-offset space, layers having same complexity coefﬁ-
cient ci can simply be grouped from {ri, ri+1, ri+2, ...} to
r′
i. The groped rank r′

i is deﬁned by

r′

i = [0 : min({ti}) : X{max(ri)}],

(16)

12572

(cid:20)
(cid:89)
(cid:81)
(cid:82)
(cid:38)
r1

(cid:23)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:21)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:22)
(cid:89)
(cid:81)
(cid:82)
(cid:38)
(cid:3)
r'2

(cid:24)
(cid:89)
(cid:81)
(cid:82)
(cid:38)
r5

(cid:25)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:26)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:27)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:28)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:19)
(cid:20)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:20)
(cid:20)
(cid:89)
(cid:81)
(cid:82)
(cid:38)

(cid:3)r'6

(cid:21)
(cid:20)
(cid:89)
(cid:81)
(cid:82)
(cid:38)
r12

(cid:3)(cid:4084)(cid:1005)

r'2,1

r'6,1

r'6,2

(cid:3)(cid:4084)(cid:1006)

(cid:3)(cid:4084)(cid:1007)

r2

r3

r4

(cid:3)(cid:4084)(cid:1008)

r6

r7

r8

r9

r10

(cid:3)(cid:4084)(cid:1009)

r11

Figure 5. Hierarchical space generation (as an example of 12-
layered network). There are 3 hierarchy level and ﬁve sub-spaces
(X1, ..., X5). The maximum space complexity is O(n5) of X1.

where i is the index of ﬁrst layer in a group. Here, the range
of rank in min-offset space is represented with the vector
space and the ti means the step size of vector space.

As an example, a 12-layered CNN is illustrated in Fig. 5.
The maximum number of layers in a bottom group is set to
3, so that there are 3-hierarchical levels for layer grouping.
The maximum space complexity is reduced from O(n12) to
O(n5) in X1. At the top-level space, X1 is composed of
5 vector spaces, and we only retain the rank conﬁgurations
that satisfy Ct in X1. The sub-space X2 is deﬁned by two
vector spaces, which are also the grouped variables from
the bottom layers in X4 and X5. To simplify the extrac-
tion strategy, in our experiments, we choose the partial sets
in each bottom sub-spaces (X3, X4, X5), by measuring the
accuracy metric of each rank conﬁguration.

5.4. Choice of Rank Conﬁguration

Apart from the simple method described in Sec. 4, we
use two methods, ENC-Model and ENC-Inf, for obtaining
the optimal rank conﬁguration for a given neural network.
For both the methods, we prepare a subset R using (14)
including the candidate rank conﬁgurations.

In ENC-Model, we choose a rank conﬁguration Ro that
maximizes A(R) which can be one of Am(R), Ap(R), and
Ac(R) denoted in (5, 6, 7). I.e.,

Ro = arg max

A(R)|R ∈ R.

(17)

R

In ENC-Inf, we choose N rank conﬁgurations, which
provide the largest values of A(R). These N rank con-
ﬁgurations are stored in RA and then evaluated over the
validation dataset to choose a best rank conﬁguration.

The complete process is given in Algorithm 1. The ENC-
Map provides a quick and dirty solution, whereas ENC-
Model and ENC-Inf approaches are relatively slower. How-
ever, even our slowest method easily outperforms state-of-
the-art approaches in speed.

The proposed method can be used to reduce both the
FLOPs and the memory consumption of a neural network.
The constraint Ct can represent both the number of FLOPs
or the number of parameters of the neural network. Also,

Algorithm 1 : Optimal Rank Conﬁguration

INPUTS: h ← A neural network, Ct ← Target complex-
ity, method ← ENC-Map, ENC-Model, ENC-Inf
OUTPUT: Ro ← Rank conﬁguration
Parameters: δs ← Parameter for limiting the search
space, δm ← Parameter for marginal error of Ct, N ←
Number of rank conﬁgurations for evaluation
//Layer-wise metrics
Compute yp,l , ym,l for all l ∈ 1...L
Compute fC−A, fC−R
if method == ENC-Map then

Ro = fC−R(Ct)

else

//Candidate rank-set generation
Rmax = fC−R(Ct + δs)
Rmin = fC−R(Ct − δs)
R = {∪R | Ct − δm < C(R) < Ct + δm}

for R ∈ [Rmin, Rmax]

//Accuracy estimate for the
// whole neural network
Compute A of R
if method == ENC-Model then

Ro = arg maxR A(R) s.t. R ∈ [Rmin, Rmax]

else if method == ENC-Inf then

Compute RA ∈ RN ×L
Evaluate accuracy of each row of RA over

the validation dataset

Select Ro from RA with best evaluation

end if

end if

our proposed methods can provide rank conﬁgurations un-
der both complexity and accuracy constraints. Till now,
the discussion followed complexity constrained systems. In
other words, given a complexity constraint our aim is to ob-
tain a rank conﬁguration that maximizes the accuracy. How-
ever, shifting to accuracy constrained systems is straight-
forward. Given an accuracy constraint, we use the inverse
mapping of fC−A, fA−C , to obtain the complexity corre-
sponding to a given accuracy. The obtained complexity is
input to Algorithm 1 to obtain the rank conﬁguration.

6. Experimental Results

In this section, we present the experimental evaluation
of the proposed methods. We ﬁrst present a comparison of
the proposed methods. We discuss the scenarios that dictate
the choice of any of the proposed methods. Afterwards,
we compare the proposed methods against some recently
proposed neural-network optimizing methods.

For our experiments, we have optimized AlexNet [17],
VGG-16 [26] and ResNet-56 [8] networks. For AlexNet
and VGG-16, we have used the ImageNet [6] dataset. For

12573

(cid:36)(cid:79)(cid:72)(cid:91)(cid:49)(cid:72)(cid:87)

(cid:36)(cid:79)(cid:72)(cid:91)(cid:49)(cid:72)(cid:87)

(cid:57)(cid:42)(cid:42)(cid:16)(cid:20)(cid:25)

(cid:57)(cid:42)(cid:42)(cid:16)(cid:20)(cid:25)

(cid:53)(cid:72)(cid:86)(cid:49)(cid:72)(cid:87)(cid:16)(cid:24)(cid:25)

(cid:53)(cid:72)(cid:86)(cid:49)(cid:72)(cid:87)(cid:16)(cid:24)(cid:25)

(cid:56)(cid:81)(cid:76)(cid:73)(cid:82)(cid:85)(cid:80)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)
(cid:40)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)
(cid:36)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:36)(cid:16)(cid:36)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:44)(cid:81)(cid:73)(cid:11)(cid:49)(cid:20)(cid:19)(cid:19)(cid:12)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:44)(cid:81)(cid:73) (cid:11)
(cid:20)(cid:19)(cid:19)(cid:12)

(cid:12)
(cid:12)

(cid:23)(cid:19)

(cid:22)(cid:24)

(cid:22)(cid:19)

(cid:21)(cid:24)

(cid:21)(cid:19)

(cid:20)(cid:24)

(cid:20)(cid:19)

(cid:24)

(cid:19)

(cid:3)
(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)
(cid:168)

(cid:3)
(cid:3)
(cid:20)

(cid:64)

(cid:8)

(cid:62)
(cid:3)

(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)
(cid:168)

(cid:3)
(cid:3)

(cid:20)
(cid:16)
(cid:83)
(cid:82)
(cid:55)

(cid:23)(cid:19)

(cid:22)(cid:24)

(cid:22)(cid:19)

(cid:21)(cid:24)

(cid:21)(cid:19)

(cid:20)(cid:24)

(cid:20)(cid:19)

(cid:24)

(cid:19)

(cid:25)(cid:19)

(cid:64)

(cid:56)(cid:81)(cid:76)(cid:73)(cid:82)(cid:85)(cid:80)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)fp,l(cid:12)
(cid:40)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:24)(cid:19)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)fm,l(cid:12)
(cid:36)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:8)
(cid:23)(cid:19)
(cid:62)
(cid:3)
(cid:36)(cid:16)(cid:36)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)
(cid:92)
(cid:70)
(cid:68)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:44)(cid:81)(cid:73)(cid:11)(cid:49)(cid:20)(cid:19)(cid:19)(cid:12)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:44)(cid:81)(cid:73) (cid:11)N=(cid:20)(cid:19)(cid:19)(cid:12)
(cid:85)
(cid:22)(cid:19)
(cid:88)
(cid:70)
(cid:70)
(cid:36)
(cid:168)

(cid:21)(cid:19)

(cid:3)
(cid:3)
(cid:20)
(cid:16)
(cid:83)
(cid:82)
(cid:55)

(cid:20)(cid:19)

(cid:64)

(cid:8)

(cid:62)
(cid:3)
(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)
(cid:168)

(cid:3)
(cid:3)
(cid:20)
-
p
o
T

60

50

40

30

20

10

0
(cid:19)(cid:17)(cid:22)
-10

(cid:25)(cid:19)

(cid:24)(cid:19)

(cid:23)(cid:19)

(cid:22)(cid:19)

(cid:21)(cid:19)

(cid:20)(cid:19)

(cid:64)

(cid:8)

(cid:62)
(cid:3)

(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)
(cid:168)

(cid:3)
(cid:3)

(cid:20)
(cid:16)
(cid:83)
(cid:82)
(cid:55)

(cid:25)(cid:19)

(cid:64)

(cid:12)
(cid:12)

(cid:56)(cid:81)(cid:76)(cid:73)(cid:82)(cid:85)(cid:80)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)
(cid:40)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)
(cid:36)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)
(cid:36)(cid:16)(cid:36)(cid:40)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:44)(cid:81)(cid:73) (cid:11)
(cid:36)(cid:16)(cid:36)(cid:40)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)(cid:19)(cid:19)
(cid:20)(cid:19)(cid:19)(cid:12)

Uniform
ENC-Map (fp,l)
E-O(1)
ENC-Map (fm,l)
A-O(1)
ENC-Model 
A-AEC-Top1
ENC-Inf (N=100)
A-AEC-Top100

(cid:56)(cid:81)(cid:76)(cid:73)(cid:82)(cid:85)(cid:80)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)
(cid:40)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:8)
(cid:62)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)
(cid:36)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:3)
(cid:92)
(cid:70)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)
(cid:68)
(cid:36)(cid:16)(cid:36)(cid:40)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)
(cid:85)
(cid:88)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:44)(cid:81)(cid:73) (cid:11)
(cid:70)
(cid:36)(cid:16)(cid:36)(cid:40)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)(cid:19)(cid:19)
(cid:20)(cid:19)(cid:19)(cid:12)
(cid:22)(cid:19)
(cid:70)
(cid:36)
(cid:168)

(cid:12)
(cid:24)(cid:19)
(cid:12)
(cid:23)(cid:19)

(cid:3)
(cid:3)
(cid:20)
(cid:16)
(cid:83)
(cid:82)
(cid:55)

(cid:21)(cid:19)

(cid:20)(cid:19)

(cid:19)

(cid:56)(cid:81)(cid:76)(cid:73)(cid:82)(cid:85)(cid:80)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)fm,l(cid:12)
(cid:36)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)fp,l(cid:12)
(cid:40)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)
(cid:40)(cid:16)(cid:40)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:44)(cid:81)(cid:73) (cid:11)N=(cid:20)(cid:19)(cid:19)(cid:12)
(cid:40)(cid:16)(cid:40)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)(cid:19)(cid:19)

(cid:3)
(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)
(cid:168)

(cid:3)
(cid:3)
(cid:20)

(cid:64)

(cid:8)

(cid:62)
(cid:3)

(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)
(cid:168)

(cid:3)
(cid:3)

(cid:20)
(cid:16)
(cid:83)
(cid:82)
(cid:55)

(cid:25)(cid:19)

(cid:24)(cid:19)

(cid:23)(cid:19)

(cid:22)(cid:19)

(cid:21)(cid:19)

(cid:20)(cid:19)

(cid:19)

(cid:56)(cid:81)(cid:76)(cid:73)(cid:82)(cid:85)(cid:80)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)
(cid:36)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:68)(cid:83)(cid:3)(cid:11)
(cid:40)(cid:16)(cid:50)(cid:11)(cid:20)(cid:12)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)
(cid:40)(cid:16)(cid:40)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)
(cid:40)(cid:49)(cid:38)(cid:16)(cid:44)(cid:81)(cid:73) (cid:11)
(cid:40)(cid:16)(cid:40)(cid:38)(cid:16)(cid:55)(cid:82)(cid:83)(cid:20)(cid:19)(cid:19)

(cid:12)
(cid:12)

(cid:20)(cid:19)(cid:19)(cid:12)

(cid:19)(cid:17)(cid:21)

0.4

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:26)

0.5

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:27)

0.6

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:28)

0.7

(cid:19)(cid:17)(cid:25)

0.8

(cid:19)(cid:17)(cid:26)

0.9

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:28)

(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:22)
(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:24)
(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:28)

(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:28)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74) (cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

FLOPs Saving Ratio

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74) (cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74) (cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74) (cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

(cid:19)
(cid:19)(cid:17)(cid:28)

(cid:19)(cid:17)(cid:21)

(cid:16)(cid:20)(cid:19)

(cid:19)(cid:17)(cid:23)

0.2

(cid:19)
(cid:19)(cid:17)(cid:24)
0.3
(cid:16)(cid:20)(cid:19)

(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:22)
(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:24)
(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:28)

(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:27)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

(cid:20)(cid:20)

(a) AlexNet

(b) VGG-16

(c) ResNet-56

Figure 6. Performance comparison of overall accuracy metrics. Baseline top-1 accuracy is 56.6% for AlexNet, 70.6% for VGG-16, and
93.1% for ResNet-56. The results are without ﬁne-tuning. Smaller ∆ Acc. is better.

ResNet-56, we have used the Cifar-10 [16] dataset. We have
performed the same pre-processing (image cropping and re-
sizing) as described in the original papers [8, 17, 26]. Also,
we have used 4% and 10% of the ImageNet and Cifar-10
training datasets as the validation datasets.

The parameters δs and δm were set to 10% of the origi-
nal total complexity and 0.5% of the target complexity, re-
spectively. For ENC-Model and ENC-Inf, we use the accu-
racy metric Am(R) for AlexNet, Ac(R) for VGG-16, and
Ap(R) for ResNet-56 as A(R). Also, note that we have per-
formed channel decomposition to the ﬁrst layer whenever
required and spatial decomposition to the rest of the layers
using truncated SVD. For VGG-16, the ﬁrst layer was de-
composed to half the original rank following [14] and for
ResNet-56, the ﬁrst layer was not compressed at all.

The experiments were conducted on a system with four
Nvidia GTX 1080ti GPUs and an Intel Zeon E5-2620 CPU.
The experiments conducted on the CPU used a single core
and code optimization was not performed. The Caffe [13]
library was used for development.

require 5 seconds and 5 minutes for ResNet-56. Note that
once these metrics are evaluated, they can be used for differ-
ent complexity constraints. For every new neural network
that needs to be optimized, these metrics need to evaluated
beforehand.

6.2. Comparison of the Fast, Search without Infer 

ence, and Search with Inference Methods

In this section, we compare the performance of ENC-
In
Map, ENC-Model and ENC-Inf against each other.
Fig. 6, it is seen that at lower compression (or relatively
higher complexity requirements), the performance of all the
three methods is almost the same. It means that the ENC-
Map is the best solution at lower compression. At higher
compression, the optimal rank conﬁguration is more sensi-
tive to the complexity and accuracy metric. Therefore, the
performance of ENC-Map is lower than ENC-Model and
ENC-Inf, and ENC-Inf using validation accuracy shows rel-
atively the best performance. The time taken by each model
is given in Table 1.

6.1. Comparison of Layer wise accuracy metrics

6.3. Performance Comparison with Other Methods

This section presents the comparison of the two layer-
wise accuracy metrics based on the results of the ENC-Map
as shown in Fig. 6. We note that the PCA energy-based met-
ric shows poor performance with the shallower AlexNet.
This is expected as the PCA energy is not based on actual
inference and expected to show poorer performance com-
pared to the measurement-based layer-wise accuracy met-
ric. However, with the deeper VGG-16 and ResNet-56, the
performance of PCA energy-based and the measurement-
based layer-wise accuracy metric is almost same. The rea-
son is that the complexity difference from rank reduction on
each layer is less as the layer is deeper, so that the accuracy
difference is also smaller. The measurement-based metric
can not completely represent the overall accuracy.

The PCA energy-based and measurement-based metrics

The overall results are summarized in Table 1. Here we
use the decrease in accuracy (i.e. ∆Acc.), which is the dif-
ference of accuracy of the original and optimized neural
networks, as a metric to evaluate different methods. Lower
decrease in accuracy is desired. For a complexity metric,
we use the percentage of (1−compression ratio) as FLOPs.
Also, we mention the results with uniform rank reduction,
which applies the same rank reduction ratio to every layer
of the neural network.

AlexNet with ImageNet.

In [15], the authors show
the compression results for FLOPs and parameters.
It is
not possible to use ENC-Map under both these constraints
as it provides mapping only from one of these constraints
to the accuracy. However, ENC-Model and ENC-Inf can
be used by populating R by rank conﬁgurations that sat-

12574

Model

Target

Complexity

Searching

Policy

AlexNet

FLOPs 37.5%

Y-D et al. [15]

(56.6% / 79.9%)

Parameters 18.4%

ENC-Inf

@ImageNet

[15]

ENC-Model

w/o FT

- / -
- / -
- / -

w/ FT

- / 1.6

Search
Time

-

-0.1 / -0.2
-0.1 / -0.2

3m @4GPUs
1m @CPU

Top-1/Top-5 ∆Acc.[%]

VGG-16

Uniform

29.9 / 23.6

0.8 / 0.5

Heuristic [9]

ADC [9]

- / 11.7
- / 9.2

- / -
- / -

(70.6% / 89.9%)

FLOPs 25%

ARS [14]

13.0 / 9.1

-0.3 / -0.1

@ImageNet

ResNet-56
(93.1% / -)
@Cifar-10

FLOPs 50%

ENC-Inf

ENC-Model
ENC-Map

Uniform

AMC [10]∗
ENC-Inf

ENC-Model
ENC-Map

10.7 / 7.3
11.3 / 7.9
14.5 / 10.2

-0.6 / -0.2
-0.7 / -0.2
-0.2 / -0.1

12.7 / -
2.7∗ / -
2.9 / -
3.5 / -
3.3 / -

0.2 / -
0.9∗ / -
0.1 / -
0.1 / -
0.1 / -

-
-

4h @8GPUs
7h @4GPUs
(in our impl.)
5m @4GPUs

3m @CPU
4s @CPU

-

1h @GPU

3m @4GPUs

1m @CPU
3s @CPU

Table 1. Performance comparison of network compression techniques. The proposed methods, uniform, [9], heuristic in [9], and [14] use
the SVD based spatial decomposition. [15] uses the Tucker decomposition based channel decomposition. [10] uses the channel pruning.
Baseline accuracy of [10]∗ is 92.8%. Smaller ∆Acc. is better.

45.0
44.5
44.0
43.5
43.0
42.5
42.0

]

%

[
 
r
o
r
r
E
1
-
p
o
T

 

LRA+FR[27]
LRD+KT[19]
ENC-Inf

24.5
23.5
22.5
21.5
20.5
19.5

]

%

[
 
r
o
r
r
E
5
-
p
o
T

 

Y-D[15]
LCNN[3]
ENC-Model

0.2

0.4

0.6

0.8

1.0

1.0

0.2 0.25 0.3 0.35 0.4

FLOPs Saving Ratio 

FLOPs Saving Ratio

(a)

(b)

Figure 7. Performance comparison for AlexNet. (a) compression
of only convolutional layers. (b) compression of whole layers in-
cluding fully-connected layers. Smaller error is better.

isfy both the FLOPs and parameters constraints. We have
used N =50 here and optimize both convolutional and fully-
Initial learning rate of 10−3 was used,
connected layers.
which was reduced by a factor of 2 after every 2 epochs till
the 16-th. Fine-tuning takes around 9 hours. As denoted
in Table 1, our method shows 1.8% higher top-5 accuracy
than [15] at the same complexity. To ﬁnd the rank conﬁgu-
ration, our algorithm only takes only a minute with a single
core CPU with ENC-Model and 3 minutes on 4 GPUs with
ENC-Inf. Fig. 7 indicates that our method outperforms the
state-of-the-art. The FLOPs saving ratios of reffed works in
Fig. 7 are calculated in our experiment. We achieve 43.33%
and 42.40% top-1 error at 30.7% and 95.0% FLOPs for
Fig. 7(a). Also, we attain the 20.4% top-5 error at 30%
FLOPs for Fig. 7(b). Compared to [19, 27] using the ac-
curacy compensation techniques such as knowledge trans-
fer [19] and force regularization [27], our method uses only
ﬁne-tuning which is the most simple and effective strategy.

VGG-16

(FLOPs 20%)

Top-5
∆Acc.

Asym.3D [28]
Y-D et al. [15]

CP-3C [11]

1.0%
0.5%
0.3%

Search Range

Layer-by-layer

ENC-Inf

0.0% Whole-network

Table 2. Comparison over target complexity of 20% FLOPs with
VGG-16. Baseline top-5 accuracy is 89.9%.

VGG-16 with ImageNet. We optimize the convo-
lutional layers under a FLOPs constraint. The complex-
ity constraint is set to 25% and 20% FLOPs to compare
with [9, 14] and [11, 15, 28], respectively. For ENC-Inf, we
set N =40. The layers 7-8, 9-10, and 11-13 are grouped
each other for top-level sub-spaces. The compressed net-
work was ﬁne-tuned with an initial learning rate of 10−5,
which was decreased by a factor of 10 at the fourth epoch.
Fine-tuning takes about 1 day. With 25% FLOPs reduc-
tion, ENC-Inf takes only 5 minutes at 4 GPUs, and it shows
the 2.3% and 0.3% higher top-1 accuracy without and with
ﬁne-tuning, respectively, compared to [14]. As denoted in
Table 1, our ENC-Map is extremely fast.
It can ﬁnd the
result in only 4 seconds with a single core CPU, while the
previous research takes 4 hours at 8 GPUs [9] and 7 hours
at 4 GPUs [14]. Results with 20% FLOPs reduction are
given in Table 2. It is seen that our method outperforms the
layer-by-layer strategies.

12575

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:27)

(cid:20)(cid:17)(cid:19)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

Model

AlexNet
VGG-16
(cid:19)(cid:17)(cid:25)
ResNet-56

(cid:19)(cid:17)(cid:23)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

Searching

(cid:57)(cid:42)(cid:42)(cid:16)(cid:20)(cid:25)
Policy
(cid:53)(cid:72)(cid:86)(cid:49)(cid:72)(cid:87)(cid:16)(cid:24)(cid:25)
(cid:36)(cid:79)(cid:72)(cid:91)(cid:49)(cid:72)(cid:87)
ENC-Inf
(cid:36)(cid:79)(cid:72)(cid:91)(cid:49)(cid:72)(cid:87)(cid:16)(cid:70)(cid:82)(cid:81)(cid:89)(cid:17)
ENC-Model
ENC-Map

(cid:19)(cid:17)(cid:27)

(cid:20)(cid:17)(cid:19)

FLOPs

31%
24%
55%

Top-1
∆Acc.

0.0%
-0.4%
-0.1%

Table 3. Lossless compression with full ﬁne-tuning.

Searching

Fine-tuning

Policy

Epochs

FLOPs

ADC [9]

ENC-Model

ENC-Model

0
0

0.1
0.2
1

64%
57%

41%
39%
33%

Top-1
∆Acc.

0.0%
-0.1%

-0.1%
-0.1%
0.0%

Table 4. Fast lossless compression with brief ﬁne-tuning for VGG-
16. Baseline top-1 accuracy is 70.6%. Smaller ∆Acc. is better.

ResNet-56 with Cifar10. The search space is deﬁned
in 3-level hierarchy. The layers 2-19, 21-37, and 39-55
are placed in separated groups excluding ﬁrst convolutional
layer and fully-connected layer. We set the the maximum
number of bottom layers in a group as four. The second-
level spaces are deﬁned by the number of bottom layers
such as Fig. 5. For ENC-Inf, we set N =20. The compressed
ResNet-56 is ﬁne-tuned with an initial learning rate of 10−3,
which is reduced by a factor of 10 at the 16th, 24th and 32nd
epoch. Fine-tuning takes around 1 hour. ENC-Map and
ENC-Model achieve a 0.1% accuracy loss (i.e. 93.1% top-1
accuracy) with 50% FLOPs reduction after ﬁne-tuning. The
times taken by ENC-Map and ENC-Inf are 3 seconds with
single core CPU and 5 minutes with 4 GPUs, respectively.
It is much more efﬁcient compared to the learning based
state-of-the-art, which takes 1 hour on a single GPU [10].

6.4. Lossless Compression

The idea of lossless compression is to set the target accu-
racy to that of the original neural network and then optimize
the network. However, since we use ﬁne-tuning after opti-
mization, we can set the target accuracy to slightly lower
than that of the original neural network. The remaining ac-
curacy is recovered by ﬁne-tuning. To set the reduced ac-
curacy constraint, we use the accuracy estimation method
in [14]. Then, the complexity constraint (i.e. FLOPs) is
calculated from the mapping function between the accu-
racy and complexity constraints. As summarized in Table 3,
our compression methods achieve a signiﬁcant reduction in
FLOPs without accuracy loss.

The combination of our fast compression method and
brief ﬁne-tuning can further reduce the FLOPs without ac-
curacy loss. Table 4 shows the result of lossless compres-

(cid:80)
(cid:85)
(cid:82)
(cid:49)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:21)

(cid:92)
(cid:70)
(cid:81)
(cid:72)
(cid:87)
(cid:68)
(cid:47)
(cid:3)
(cid:71)
(cid:72)
(cid:93)
(cid:76)
(cid:79)
(cid:68)
(cid:80)
(cid:85)
(cid:82)
(cid:49)

(cid:20)(cid:17)(cid:21)

(cid:20)(cid:17)(cid:19)

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:21)

(cid:57)(cid:42)(cid:42)(cid:16)(cid:20)(cid:25)
(cid:53)(cid:72)(cid:86)(cid:49)(cid:72)(cid:87)(cid:16)(cid:24)(cid:25)

(cid:80)
(cid:85)
(cid:82)
(cid:49)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:21)

(cid:36)(cid:79)(cid:72)(cid:91)(cid:49)(cid:72)(cid:87)
(cid:36)(cid:79)(cid:72)(cid:91)(cid:49)(cid:72)(cid:87)(cid:16)(cid:70)(cid:82)(cid:81)(cid:89)(cid:17)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:27)

(cid:20)(cid:17)(cid:19)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:27)

(cid:20)(cid:17)(cid:19)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

(cid:36)(cid:71)(cid:71)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:79)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:3)(cid:71)(cid:88)(cid:72)(cid:3)(cid:87)(cid:82)

(cid:76)(cid:81)(cid:70)(cid:85)(cid:72)(cid:68)(cid:86)(cid:72)(cid:71)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:86)(cid:3)(cid:76)(cid:86)(cid:3)(cid:81)(cid:72)(cid:74)(cid:79)(cid:76)(cid:74)(cid:76)(cid:69)(cid:79)(cid:72)

(cid:57)(cid:42)(cid:42)(cid:16)(cid:20)(cid:25)
(cid:53)(cid:72)(cid:86)(cid:49)(cid:72)(cid:87)(cid:16)(cid:24)(cid:25)

(cid:92)
(cid:70)
(cid:81)
(cid:72)
(cid:87)
(cid:68)
(cid:47)
(cid:3)
(cid:71)
(cid:72)
(cid:93)
(cid:76)
(cid:79)
(cid:68)
(cid:80)
(cid:85)
(cid:82)
(cid:49)

(cid:20)(cid:17)(cid:21)

(cid:20)(cid:17)(cid:19)

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:21)

(cid:36)(cid:79)(cid:72)(cid:91)(cid:49)(cid:72)(cid:87)
(cid:36)(cid:79)(cid:72)(cid:91)(cid:49)(cid:72)(cid:87)(cid:16)(cid:70)(cid:82)(cid:81)(cid:89)(cid:17)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:27)

(cid:20)(cid:17)(cid:19)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:27)

(cid:20)(cid:17)(cid:19)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

(cid:41)(cid:47)(cid:50)(cid:51)(cid:86)(cid:3)(cid:54)(cid:68)(cid:89)(cid:76)(cid:81)(cid:74)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82)

Figure 8. Normalized CPU latency of optimized models. The
latency of baseline CNN is 10.59s in VGG-16, 142.2ms in ResNet-
56, and 825.0ms in AlexNet for single image classiﬁcation.

sion with ﬁne-tuning under 1 epoch. The accuracy thresh-
olds considering ﬁne-tuning are calculated for 0.1, 0.2, and
1 epochs using the method in [14]. We reduced the FLOPs
by 41% with VGG-16 without any accuracy loss. The pro-
cess took only 0.1 epoch or 22 minutes with a single GPU.
Also, our method with 1 epoch ﬁne-tuning takes 3.7 hours
with a single GPU, and it provides better compression (33%
FLOPs) compared to the 4 hour search (64% FLOPs) in [9].

6.5. Results on an Embedded Board

We evaluate the latency of the compressed network mod-
els for inference on the ODROID-XU4 board with Samsung
Exynos5422 mobile processor. Fig. 8 shows that the FLOPs
saving ratio in our paper is directly related to the real la-
tency for single image inference. We use ENC-Model for
optimizing the convolutional layers of AlexNet, VGG-16,
and ResNet-56 in Fig. 8. We note that the latency of con-
volutional layers is 72% and 62% of total latency for the
baseline ResNet-56 and AlexNet, respectively, and the bias
latency in Fig. 8 is due to the other operations including
batch normalization and fully-connected layers. The results
of AlexNet-conv indicate that the theoretical FLOPs reduc-
tion of convolutional layers corresponds to the latency im-
provement of those layers.

7. Conclusion

In this paper, we propose the efﬁcient neural network
compression methods. Our methods are based on low-rank
kernel decomposition. We propose a holistic, model-based
approach to obtain the rank conﬁguration that satisﬁes
the given set of constraints. Our method can compress
the neural network while providing competitive accuracy.
Moreover, the time taken by our method for compression
is in seconds or minutes, whereas previously proposed
methods take hours to achieve similar results.

Acknowledgement This work was supported by MSIT as
GFP / (CISS-2013M3A6A6073718).

12576

References

[1] Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, and
Kris M Kitani. N2n learning: Network to network compres-
sion via policy gradient reinforcement learning. Apr 2018.

[2] Marcella Astrid and Seung-Ik Lee. Cp-decomposition with
tensor power method for convolutional neural networks com-
pression.
In Big Data and Smart Computing (BigComp),
2017 IEEE International Conference on, pages 115–118.
IEEE, 2017.

[3] Hessam Bagherinezhad, Mohammad Rastegari, and Ali
Farhadi. Lcnn: Lookup-based convolutional neural network.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 7120–7129, 2017.

[4] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun
Wang. Reinforcement learning for architecture search by
network transformation. arXiv preprint arXiv:1707.04873,
2017.

[5] Ting-Wu Chin, Cha Zhang, and Diana Marculescu. Layer-
compensated pruning for resource-constrained convolutional
neural networks. arXiv preprint arXiv:1810.00518, 2018.

[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei.
Imagenet: A large-scale hierarchical im-
age database. In Computer Vision and Pattern Recognition,
2009. CVPR 2009. IEEE Conference on, pages 248–255.
IEEE, 2009.

[7] Misha Denil, Babak Shakibi, Laurent Dinh, Nando de Fre-
In Ad-
itas, et al. Predicting parameters in deep learning.
vances in Neural Information Processing Systems (NIPS),
pages 2148–2156, 2013.

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[9] Yihui He and Song Han. Adc: Automated deep compression
and acceleration with reinforcement learning. arXiv preprint
arXiv:1802.03494, 2018.

[10] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
Song Han. Amc: Automl for model compression and ac-
celeration on mobile devices.
In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 784–
800, 2018.

[11] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
for accelerating very deep neural networks. In International
Conference on Computer Vision (ICCV), volume 2, 2017.

[12] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
Speeding up convolutional neural networks with low rank
expansions. arXiv preprint arXiv:1405.3866, 2014.

[13] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional architecture for fast
feature embedding. In Proceedings of the 22nd ACM inter-
national conference on Multimedia, pages 675–678. ACM,
2014.

[14] Hyeji Kim and Chong-Min Kyung. Automatic rank selection
for high-speed convolutional neural network. arXiv preprint
arXiv:1806.10821, 2018.

[15] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim
Choi, Lu Yang, and Dongjun Shin. Compression of deep
convolutional neural networks for fast and low power mobile
applications. In Proceedings of the International Conference
on Learning Representations (ICLR), May 2016.

[16] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Cite-
seer, 2009.

[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, pages 1097–1105, 2012.

[18] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. arXiv
preprint arXiv:1608.08710, 2016.

[19] Shaohui Lin, Rongrong Ji, Chao Chen, Dacheng Tao, and
Jiebo Luo. Holistic cnn compression via low-rank decompo-
sition with knowledge transfer. IEEE transactions on pattern
analysis and machine intelligence, 2018.

[20] Shaohui Lin, Rongrong Ji, Xiaowei Guo, Xuelong Li, et al.
Towards convolutional neural networks compression via
global error reconstruction.
In IJCAI, pages 1753–1759,
2016.

[21] Shaohui Lin, Rongrong Ji, Yuchao Li, Yongjian Wu, Feiyue
Huang, and Baochang Zhang. Accelerating convolutional
networks via global & dynamic ﬁlter pruning.
In IJCAI,
pages 2425–2432, 2018.

[22] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha
Fernando, and Koray Kavukcuoglu. Hierarchical repre-
sentations for efﬁcient architecture search. arXiv preprint
arXiv:1711.00436, 2017.

[23] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ﬁlter
level pruning method for deep neural network compression.
arXiv preprint arXiv:1707.06342, 2017.

[24] Shinichi Nakajima, Masashi Sugiyama, S Derin Babacan,
and Ryota Tomioka. Global analytic solution of fully-
observed variational bayesian matrix factorization. Journal
of Machine Learning Research, 14(Jan):1–37, 2013.

[25] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V
Le. Regularized evolution for image classiﬁer architecture
search. arXiv preprint arXiv:1802.01548, 2018.

[26] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In Pro-
ceedings of the International Conference on Learning Rep-
resentations (ICLR), May 2015.

[27] Wei Wen, Cong Xu, Chunpeng Wu, Yandan Wang, Yiran
Chen, and Hai Li. Coordinating ﬁlters for faster deep neural
networks. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 658–666, 2017.

[28] Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
Accelerating very deep convolutional networks for classiﬁ-
cation and detection. IEEE transactions on pattern analysis
and machine intelligence, 38(10):1943–1955, 2016.

12577

