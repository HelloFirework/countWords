Large-scale weakly-supervised pre-training for video action recognition

Deepti Ghadiyaram, Du Tran, Dhruv Mahajan

Facebook AI

{deeptigp, trandu, dhruvm}@fb.com

Abstract

Current fully-supervised video datasets consist of only
a few hundred thousand videos and fewer than a thou-
sand domain-speciﬁc labels. This hinders the progress to-
wards advanced video architectures. This paper presents
an in-depth study of using large volumes of web videos for
pre-training video models for the task of action recogni-
tion. Our primary empirical ﬁnding is that pre-training
at a very large scale (over 65 million videos), despite on
noisy social-media videos and hashtags, substantially im-
proves the state-of-the-art on three challenging public ac-
tion recognition datasets. Further, we examine three ques-
tions in the construction of weakly-supervised video ac-
tion datasets. First, given that actions involve interactions
with objects, how should one construct a verb-object pre-
training label space to beneﬁt transfer learning the most?
Second, frame-based models perform quite well on action
recognition; is pre-training for good image features sufﬁ-
cient or is pre-training for spatio-temporal features valu-
able for optimal transfer learning? Finally, actions are gen-
erally less well-localized in long videos vs. short videos;
since action labels are provided at a video level, how should
one choose video clips for best performance, given some
ﬁxed budget of number or minutes of videos?

1. Introduction

It is well-established [20, 32] that pre-training on large
datasets followed by ﬁne-tuning on target datasets boosts
performance, especially when target datasets are small
[3, 10, 26, 49]. Given the well-known complexities in con-
structing large-scale fully-supervised video datasets, it is in-
tuitive that large-scale weakly-supervised pre-training is vi-
tal for video tasks.

Recent studies [36, 46, 62] have clearly demonstrated
that pre-training on hundreds of millions (billions) of noisy
web images signiﬁcantly boosts the state-of-the-art in ob-
ject classiﬁcation. While one would certainly hope that suc-
cesses would carry over from images [36, 46, 62] to videos,
action recognition from videos presents certain unique chal-
lenges that are absent from the image tasks.

First, while web images primarily face the challenge of
label noise (i.e., missing or incorrect object labels), for
videos in the wild, the challenges are two-fold: label noise
and temporal noise due to the lack of localization of action
labels. In real-world videos, a given action typically occu-
pies only a very small portion of a video. In stark contrast, a
typical web image is a particular moment in time, carefully
selected by its creator for maximal relevance and salience.

Second, in prior work on images, labels were restricted
to scenes and objects (i.e., nouns). However, action labels
(eg: “catching a ﬁsh”) are more complex, typically involv-
ing at least one verb-object pair. Further, even at large
scale, many valid verb-object pairs may be observed rarely
or never at all; for example, “catching a bagel” is an entirely
plausible action, but rarely observed. Therefore, it is natu-
ral to inquire: is it more useful to pre-train on labels chosen
from marginal distributions of nouns and verbs, do we need
to pre-train on the observed portion of the joint distribution
of (verb, noun) labels, or do we need to focus entirely on the
target dataset’s labels? How many such labels are sufﬁcient
for effective pre-training and how diverse should they be?

Third, the temporal dimension raises several interest-
ing questions. By analogy to images, short videos should
be better temporally-localized than longer videos; we in-
vestigate this assumption and also ask how localization af-
fects pre-training. In addition, longer videos contain more
frames, but short videos presumably contain more relevant
frames; what is the best choice of video lengths when con-
structing a pre-training dataset?

Finally, we question whether pre-training on videos (vs
images) is even necessary. Both frame-based models and
image-based pre-training methods like inﬂation [11] have
been successful in action recognition.
Is pre-training on
video clips actually worth the increased compute, or, are
strong image features sufﬁcient?

In this work, we address all these questions in great
detail. Our key aim is to improve the learned video fea-
ture representations by focusing exclusively on training
data, which is complementary to model-architecture de-
sign. Speciﬁcally, we leverage over 65 million public, user-
generated videos from a social media website and use the

112046

associated hashtags as labels for pre-training. The label
noise and temporal noise makes our training framework
weakly-supervised. Unlike all existing fully-supervised
video datasets [35, 41, 42, 43, 60, 64] which required ex-
pensive annotation, our training data is truly extensible to
billion-scale without incurring any annotation costs. We
effectively tackle the aforementioned challenges with label
space and temporal noise, and demonstrate signiﬁcant per-
formance gains on various target tasks. Overall, we sum-
marize our ﬁndings:

• Large-scale weak-supervision is extremely beneﬁcial:
We show that large-scale video data, despite not providing
strong supervision, tremendously helps models of varied
capacities in learning better features. Our experiments
clearly demonstrate that content diversity and scale outdo
label and temporal noise.

• Impact of data volume and model capacity: We re-
port interesting ﬁndings on the effect of pre-training data
size, data sampling strategies, model capacities, etc. For
instance, we ﬁnd that increasing the training data (Sec.
4.1.1) improves performance while increasing model ca-
pacity exhibits interesting behavior (Sec. 4.1.2) .

• What is a suitable pre-training video label space? We
systematically construct pre-training label sets that vary
in cardinality and type (e.g., verbs, nouns, etc.), and study
their effects of target tasks (Sec. 4.2). One key ﬁnding is
that as in [46], pre-training labels that overlap the most
with the target labels improve performance.

• Short vs.

long videos for pre-training? We study the
effect of pre-training on short vs. long videos (Sec. 4.3.1)
and show that (a) for a ﬁxed video length budget (e.g.,
400K minutes of total training video duration), it is ben-
eﬁcial to choose a large number of short videos as they
supply succinct localized actions compared to fewer long
videos, (b) for a ﬁxed video budget (e.g., 5M ), choosing
longer videos are beneﬁcial as they offer diverse content.
• Do we need pre-training on video data? We investi-
gate the true value of pre-training using video data. We
show that it is necessary to use videos as opposed to video
frames or images followed by inﬂation [11] to achieve
better performance when operating at scale (Sec. 4.3.2).
• State-of-the-art results: We achieve a top-1 accuracy of
81.3% on Kinetics, a 3.6% boost over the previous state-
of-the-art [65] (Sec. 4.4). While the gains in [65] were
achieved via architectural innovation, increased compute,
etc., our boost is purely from pre-training a simple ar-
chitecture (R(2+1)D [14]) at scale. On EPIC Kitchens
action recognition challenge [16], we achieve an accu-
racy of 25.6% on unseen (S2) test data, an improvement
of 4.6% over the top entry in the leadership board at the
time of submission. On Something-something [28], we
achieve an accuracy of 51.6%, a 2.1% improvement over
the previous state-of-the-art [70].

2. Related Work

Learning from weak supervision: Given the known chal-
lenges in gathering exhaustive annotations for various im-
age and video tasks, leveraging object labels and other meta
information to supply weak supervision [7, 15, 19, 22, 30,
31, 39, 40, 50, 52, 53, 54, 55, 56, 59] is a widely-adopted
approach. Orthogonal to this strategy, our work investigates
transfer learning beneﬁts when networks are pre-trained on
weakly-supervised data, i.e., data afﬂicted with label and
temporal noise. While novel changes to architectures have
been proposed [57, 61] to combat label noise, our exper-
iments demonstrate that large-scale training of an existing
video architecture [14] makes it noise-resilient.
Dataset sources: Many prior approaches use Internet as
a natural source of content [6, 12, 13, 18, 24, 25, 34, 37,
44, 46, 51, 58, 67] and the associated search queries, hash-
tags, or user-comments as labels to construct datasets. Most
large-scale video datasets [2, 4, 38] were constructed by
ﬁrst curating a label taxonomy, analyzing the text metadata
surrounding YouTube or Flickr videos, followed by some
manual cleaning of the non-visual labels. [8, 21, 23, 47]
analyzed movie scripts for automatic annotation of human
actions for recognizing and localizing actions and actors.
Our proposed work uses web-data and the associated text to
supply weak supervision during pre-training.
Pre-training on large datasets: Since datasets for com-
plex tasks such as object detection, segmentation and ac-
tion recognition in videos are in smaller order of magnitude
compared to ImageNet [17], pre-training on larger, auxil-
iary data followed by ﬁne-tuning on target tasks [11, 20, 26,
36, 45, 46, 62, 14, 65] is very popular.
Indeed, inﬂation
[11] was proposed to exclusively leverage ImageNet instan-
tiation by way of converting 2D ﬁlters to 3D, given that
training 3D models is computationally expensive. In this
work, we show that pre-training on video clips performs
signiﬁcantly better than pre-training on image/video frames
followed by inﬂation (Sec. 4.3.2).

3. Weak-supervision of video models

We harness millions of public videos from a social me-
dia website and use the associated hashtags as labels to
supply weak supervisory signals while training video mod-
els. We construct and experiment with a variety of weakly-
supervised datasets, which we describe next.

3.1. Source datasets

To construct pre-training video datasets, we use several
seed action label sets and gather videos that are related
to these labels. Speciﬁcally, for a given seed action label
“catching a ﬁsh,” we ﬁrst construct all possible meaningful
phrases (i.e., relevant hashtags) from it by taking the orig-
inal and stemmed versions of every word in the seed label

12047

then download public videos

and concatenating them in all possible permutations. As
an example, relevant hashtags(“catching a f ish”) =
{#catchingaf ish, #catchf ish, #f ishcatching, ...}.
We
tagged
the hashtags from the set of
with at
relevant hashtags(“catching a f ish”) and associate
them with the initial seed label. We use the seed labels as
the ﬁnal labels for videos during pre-training.

least one of

that

are

3.1.1 Properties of the source datasets

Seed labels: As we describe in Sec. 4.2, we study the ef-
fect of pre-training on different types of labels by consid-
ering four different seed label sets. The resulting source
datasets are summarized in Table 1. We use a notation
IG − source − size throughout this paper, where source
indicates the seed label set used and size indicates the
number of videos1. Our primary training set uses 400
action labels from Kinetics [69] as seed labels, resulting
in IG − Kinetics dataset comprising 359 labels2. We
also consider (a) the 1428 hashtags that match the 1000
synsets from ImageNet-1K [17], thereby constructing an
IG − Noun dataset3, (b) the 438 verbs from Kinetics and
VerbNet [63], thus an IG − Verb dataset, and (c) all pos-
sible concatenations of the 438 verbs and the 1428 nouns
from the above two seed label sets. We identify over
10, 653 such meaningful combinations4, thus constructing
an IG − Verb + Noun dataset. More details on dataset con-
struction are provided in the supplementary material. We
want to reiterate that no manual annotation was involved
in constructing these datasets implying that there is a large
amount of noise in the data.
Long tail distribution: Distribution of videos in all our
source datasets is heavily long-tailed. To mitigate its effect
on model training, we adopt the square root sampling ap-
proach [48] when constructing pre-training data in our ex-
periments as it proved to be most beneﬁcial for images [46].
Diversity: Unlike all benchmark datasets which contain
short videos of localized actions, video lengths in our source
datasets range from 1 to 60 seconds. Thus, the labeled
action can occur anywhere in a video, resulting in a large
amount of temporal noise, an aspect we study in Sec. 4.3.

Given that we cannot make our datasets or the exact
hashtags used public (as was the case with [62, 46]), we ac-
knowledge that it is not possible for other research groups
to reproduce our results at this time. Despite this limitation,
we hope that the reader ﬁnds value in our wide-range of

1We use IG − source as notation whenever we refer to pre-training

data source alone throughout this paper.

2For the remaining 41 labels, we could not ﬁnd sufﬁcient videos (i.e.,

at least 50 per label) using the approach detailed in Sec. 3.1.

3We get 1428(> 1000) total hashtags because multiple hashtags may

map to the same synset.

4Note that this is far fewer than 438 ∗ 1428 =~600k, as we discarded

those concatenations which are not associated with at least 50 videos.

Pre-training dataset
IG-Kinetics
IG-Noun
IG-Verb
IG-Verb+Noun

Total #Videos

#Labels

65M

19M

19M

19M

359

1428

438

10653

Table 1. Statistics of the weakly-supervised datasets constructed for pre-training.

ﬁndings on various aspects of large-scale video pre-training
that we describe in Sec. 4.

3.2. Target datasets

Next, we describe the target datasets used in experiments.
Kinetics [64]: Kinetics is a multi-class dataset with ~246K
training videos (400 human action labels). We report per-
formance on the 20K validation videos.
EPIC-Kitchens [16]: EPIC-Kitchens is a multi-class ego-
centric dataset with ~28K training videos associated with
352 noun and 125 verb classes. For our ablation studies, fol-
lowing [5] we construct a validation set of unseen kitchen
environments. We evaluate our best pre-trained model on
validation, standard seen (S1: 8047 videos), and unseen
(S2: 2929 videos) kitchens test datasets.
Something-Something-v1 [28] is a multi-class dataset of
~86K training videos and 174 ﬁne-grained actions. We re-
port results on the 11, 522 validation set.

Video deduplication: We devise a pipeline to deduplicate
videos in the source datasets that may overlap with any
from the target dataset. To err on the side of caution, we
adopt an aggressive low-precision high-recall strategy and
remove any potential duplicates (eg: we removed ~29K
videos from the IG-Kinetics dataset). Details are provided
in the supplementary material.

3.3. Pre training Setup

Models: R(2+1)D-d [14]5 is the fundamental architec-
ture used for pre-training, where d denotes model depth
= {18, 34, 101, 152}. As in [29], we construct models of
depth > 34 by replacing simple temporal blocks with bot-
tleneck blocks for computational feasibility. We direct the
reader to the supplementary material for details.

Loss Function: Our pre-training datasets are multi-label
since multiple hashtags may be associated with any given
video. The authors of [36, 46] have observed that per-
label sigmoid outputs with logistic loss do not work well
for noisy labels. Hence, we follow a simple strategy of
randomly assigning one of the associated hashtags to each
video thereby formulating a multi-class problem, and use
softmax activations with cross-entropy loss.

Training Details: Video frames are down-sampled to a
resolution of 128 × 171 and each video clip is generated by
cropping a random patch of size 112 × 112 from a frame.
Video clips of either 8 or 32 frames are used in our exper-
iments, and temporal jittering is also applied to the input.

5Source code: https://github.com/dutran/R2Plus1D

12048

Synchronous stochastic gradient descent (SGD) is used to
train our models on 128 GPUs across 16 machines using
caffe2 [9]. When 32 frames per input video clip are consid-
ered, each GPU processes 6 videos at a time (due to memory
constraints), while 16 videos are processed at a time when 8
frames per video clip are considered. Batch normalization
(BN) is applied to all convolutional layers and the statistics
[33] are computed on each GPU. All pre-training experi-
ments process 490M videos in total. Learning rate is set
following the linear scaling procedure proposed in [27] with
a warmup. An initial learning rate of 0.192 is used which
is divided by 2 at equal steps such that the total number of
learning rate reductions is 13 over the course of training.

4. Experiments

In this section, we study various aspects of large-scale
weakly-supervised video pretraining. We ﬁrst describe our
evaluation setup and then report our extensive analysis on
three aspects: (a) effect of scale, e.g., model capacity and
pre-training data size, (b) design of the pre-training label
space, and (c) temporal properties of videos. We also pre-
train on benchmark datasets such as Sports-1M [38], Kinet-
ics [64] as competitive baselines.

Evaluation Setup: As in [46], we consider two scenarios:

• Full-ﬁnetuning (full-ft) approach involves bootstrap-
ping with a pre-trained model’s weights and training end-
to-end on a target dataset. We do a grid search for best the
hyper-parameters (learning rate etc.) on validation data
constructed by randomly holding out (10%) of training
data. The hyper-parameters used for each experiment and
target dataset are in the supplementary material. Full-ft
approach has the disadvantage that it can potentially mask
the absolute effect of pre-training for large target datasets.
• Fully-connected (fc-only) approach involves extracting
features from the ﬁnal fc layer of a pre-trained model and
training a logistic regressor on each target dataset. This
approach evaluates the strength of the learned features
without changing the network parameters.

For multi-class target datasets our loss function is a L2-
regularized logistic regressor and we report accuracy. For
multi-label datasets, we use a per-label sigmoid output fol-
lowed by logistic loss and report mAP. During testing, cen-
ter crops of 10 clips uniformly sampled from each test video
are considered, and the average of these 10 clip predictions
are used to obtain the ﬁnal video-level prediction.

4.1. Effect of large scale

4.1.1 Amount of pre-training data

To understand this question, we pre-train on different
amounts of training data by constructing different data sub-
sets - IG-Kinetics- {500K, 1M, 5M, 10M, 19M, 65M }.
R(2+1)D-34 models are independently trained on these data

subsets on the exact same labels, with an input of 8-frames
per video and evaluated on Kinetics (Fig. 1 (a)) and EPIC-
Kitchens (Fig. 1 (b)).

As in [46, 62], we observe that performance improves
log-linearly with training data size indicating that more pre-
training data leads to better feature representations. For
Kinetics, with full-ft approach, pre-training using 65M
videos gives a signiﬁcant boost of 7.8% compared to train-
ing from scratch (74.8% vs. 67.0%). With increase in
training data, performance gains are even more impressive
when using fc-only approach, which achieves an accuracy
of 73.0% with 65M training videos, thus closely match-
ing the accuracy from full-ft approach (74.8%). On EPIC-
Kitchens, using IG-Kinetics-65M yields an improvement of
3.8% compared to using Kinetics for pre-training (16.1%
vs. 12.3%). Compared with Kinetics, on EPIC-Kitchens,
there is a larger gap in the performance between full-ft and
fc-only approaches. This may be due to a signiﬁcant do-
main difference in the pre-training and target label space.

These plots indicate that despite the dual challenge of
label and temporal noise, pre-training using millions of web
videos exhibit excellent transfer learning performance.

Data Sampling: Web data typically follows a Zipﬁan (long
tail) distribution. When using only a subset of such data for
pre-training, a natural question to ask is, if there are better
ways to choose a data subset beyond random sampling. We
design one such approach where we retain all videos from
tail classes and only sub-sample head classes. We refer to
this scheme as tail-preserving sampling.

Figure 1 (c) compares random and tail-preserving sam-
pling strategies for Kinetics and reports performance ob-
tained via fc-only approach. We observe that the tail-
preserving strategy does consistently better and in fact,
the performance saturates around 10M − 19M data
points. Hence, for all future experiments, we adopted tail-
preserving sampling strategy when needed.

4.1.2 Effect of model capacity

Table 2 reports the capacity of different video models and
their effect on transfer learning performance. Speciﬁcally,
we use IG-Kinetics-65M to pre-train 4 different R(2+1)D-
d models, where d = {18, 34, 101, 152} with input clip-
length 32. On Kinetics, we observe that increasing model
capacity improves the overall performance by 3.9%.
In
comparison, when training from scratch, the accuracy im-
proves only by 2.7%. Interestingly, on EPIC-Kitchens, pre-
training either using IG-Kinetics-65M or Kinetics (referred
to as baseline) yield similar gains with the increase in model
capacity. Unlike in [46] where the transfer learning perfor-
mance was observed to be bottlenecked by capacity, we see
a saturation in performance when going from d = 101 to
d = 1526. Given that R(2+1)D-152 has higher GFLOPS

6For EPIC-Kitchens, we even observe a performance drop.

12049

80

70

60

50

40

)

%
n

 

i
(
 

y
c
a
r
u
c
c
a
1
-
p
o
t

 

105

(a) Target: Kinetics

(b) Target: Epic-Kitchens

(c) Target: Kinetics; fc-only

67.0

66.3

68.2

74.8

73.0

71.3 72.3 73.4

68.3

65.0

60.1

44.7

38.0

106

Number of training videos

107

fc-only

full-ft

baseline

15

10

5

0

)

 

%
n
i
(
 
P
A
m

12.3

16.1

14.7 14.8

12.8

10.5

9.0

3.3

2.5

106

5.0 4.6

4.1

6.3

fc-only

full-ft

baseline

108

108

−5

105

Number of training videos

107

)

%
n

 

i
(
 
y
c
a
r
u
c
c
a

 

1
-
p
o
T

75

70

65

60

55

50

45

40

35

105

71.2 71.4

72.9

73.0

73.0

61.1

68.3

65.0

55.2

60.1

44.7

38.0

106

random
tail-preserving

108

107

Number of training videos

Figure 1. Illustrating the effect of increasing the number of pre-training videos. For Kinetics, we train a R(2+1)D-34 model from scratch as baseline, while for EPIC-Kitchens,
we pre-train R(2+1)D-34 on Kinetics as baseline (indicated in orange). Random sampling was used for experiments reported in (a) and (b). X-axis is in log-scale.

Models

GFLOPS

# params

R(2+1)D-18
R(2+1)D-34
R(2+1)D-101
R(2+1)D-152

83
152
176
252

33M
64M
86M
118M

Kinetics

full-ft
76.0
78.2
79.1
79.9

baseline
69.3
69.6
71.7
72.0

Epic-Kitchens
full-ft
20.8
22.4
24.9
23.7

baseline
14.8
15.2
17.1
17.8

Table 2. Performance when pre-trained models of varied capacities are fully-
ﬁnetuned on Kinetics (top-1 accuracy) and Epic-Kitchens (mAP). For EPIC-
Kitchens, as a baseline, we use a model pre-trained on Kinetics.

compared to the largest image model in [46], we believe
that our model may be bottlenecked by the amount of pre-
training data. Thus, using more than 65M training videos
may further boost the accuracy. Additionally, inability to
do long-range temporal reasoning beyond 32 frames (due
to memory constraints) may also be leading to this behav-
ior. These questions are interesting to explore in the future.

4.2. Exploring the pre training label space

Web videos and the associated (noisy) hashtags are avail-
able in abundance; hence it is natural to question: what con-
stitutes a valuable pre-training label space for achieving su-
perior transfer learning performance and how to construct
one? Since hashtags are generally composed of nouns,
verbs, or their combinations, and vary greatly in their fre-
quency of occurrence, it is important to understand the
trade-offs of different pre-training label properties (eg: car-
dinality and type) on transfer learning. In this section, we
study these aspects in great detail.

EPIC-Kitchens the most while IG-Verb signiﬁcantly helps
the verb prediction task (at least 1.2% in both cases, Fig.
2(b) and (c)). We found an overlap of 62% between IG-
Verb and the verb labels and 42% between IG-Noun and
the noun labels in EPIC-Kitchens. Pre-training on Sports-
1M performs poorly across all target tasks, presumably due
to its domain-speciﬁc labels.

Given that actions in EPIC-Kitchens are deﬁned as verb-
noun pairs, it is reasonable to expect that IG-Verb+Noun
is the most well-suited pre-training label space for EPIC-
Kitchens-actions task. Interestingly, we found that this was
not the case (Fig. 2 (d)). To investigate this further, we
plot the cumulative distributions of the number of videos
per label for all four pre-training datasets in Fig. 3. We
observe that though IG-Verb+Noun captures all plausible
verb-noun combinations leading to a very large label space,
it is also heavily skewed (and hence sparse) compared to
other datasets. This skewness in the IG-Verb+Noun label
space is perhaps offsetting its richness and diversity as well
as the extent of its overlap with the EPIC-Kitchens action
labels. Thus, for achieving maximum performance gains,
it may be more effective to choose those pre-training labels
that most overlap with the target label space while mak-
ing sure that label distribution does not become too skewed.
Understanding and exploiting the right trade-offs between
these two factors is an interesting future research direction.

4.2.1 Effect of the nature of pre-training labels

4.2.2 Effect of the number of pre-training labels

To study the type of pre-training labels that would help
target tasks the most, as mentioned in Sec. 3.1, we sys-
tematically construct label sets that are verbs, nouns, and
their combinations. Speciﬁcally, we use IG-Kinetics-19M,
IG-Verb-19M, IG-Noun-19M, and IG-Verb+Noun-19M as
our pre-training datasets. We use R(2+1)D-34 with clip-
length of 32 for training. From Fig. 2, we may observe
that for each target dataset, the source dataset whose la-
bels overlap the most with it yield maximum performance.
For instance, for Kinetics we see an improvement of at
least 5.5%, when we use IG-Kinetics-19M for pre-training,
compared to other pre-training datasets (Fig. 2(a)). Pre-
training on IG-Noun beneﬁts the noun prediction task of

In Sec. 4.1.1, we study how varying the number of pre-
training videos for a ﬁxed source label space effects the
transfer learning performance.
In this section, we inves-
tigate the reverse scenario, i.e., vary the number of pre-
training labels while keeping the number of videos ﬁxed.
We consider IG-Verb+Noun as our candidate pre-training
dataset due to a large number (10, 653) of labels. We ran-
domly7 sub-sample different number of labels from the full
label set all the way to 675 labels and ﬁx the number of
videos in each resulting dataset to be 1M . We did not have

7Random sampling also makes sure that we remove uniformly from
head and tail classes and long-tail issue with IG-Verb+Noun does not affect
the observations.

12050

(a) Kinetics

75.1

67.9

69.6 68.8

69.6

53.1

y
c
a
r
u
c
c
A
1
-
p
o
T

 

80

75

70

65

60

55

50

45

40

(b) Epic-Kitchens-Noun

18.7

17.5

16.9 16.8

16.5

11.6

P
A
m

20

18

16

14

12

10

(c) Epic-Kitchens-Verb

38.5

36.6 37.0

35.8

34.1

29.4

P
A
m

40

38

36

34

32

30

28

P
A
m

10

9

8

7

6

5

4

3

(d) Epic-Kitchens-Action

8.8

7.6

7.6

6.7

IG-Noun

IG-Verb

7.1

IG-Verb+Noun

IG-Kinetics
Sports-1M

Kinetics

4.2

Figure 2. (a) Top-1 accuracy on Kinetics and (b)-(d) mAP on the three Epic-Kitchens tasks after fc-only ﬁnetuning, when different source label sets are used (indicated in the
legend). The results indicate that target tasks beneﬁt the most when their labels overlap with the source hashtags. Best viewed in color.

1.0

s
o
e
d
i
v

0.8

 
f
o

 
r
e
b
m
u
n
 
f
o
n
o
i
t
c
a
r
F

 

0.6

0.4

0.2

IG-Noun (1428 labels)

IG-Verb (438 labels)

IG-Verb+Noun (10653 labels)

IG-Kinetics (359 labels)

0.0

0.0

0.2

0.4

0.6

Fraction of labels

0.8

Figure 3. Cumulative distribution of the number of videos per label for the 4 pre-
training datasets discussed in Sec. 4.2.1. The x-axis is normalized by the total number
of labels for each dataset.

y
c
a
r
u
c
c
A
1
-
p
o
T

 

75

70

65

60

55

50

45

40

35

(a) Label space: IG-Verb+Noun

67.4

68.3

68.5

68.3

68.5

52.4

52.2

50.2

43.5

51.5

fc-only

full-ft

210

211

212

213

Number of pre-training labels

80

70

60

50

40

30

20

10

y
c
a
r
u
c
c
A
1
-
p
o
T

 

(b) Label space: IG-Kinetics

64.8

66.6

67.7

41.0

27.8

70.0

71.0

60.8

52.4

fc-only

full-ft

15.4

24

25
Number of pre-training labels

26

27

28

Figure 4. Top-1 accuracy on Kinetics when pre-training on different number of
labels. Note that the source datasets used in panels (a) and (b) are different, hence the
results are not comparable. X-axis is log scale.

enough training videos, i.e., at least 1M for fewer than 675
labels. Label sampling is done such that the smaller label
space is a subset of the larger one. R(2+1)D-34 is used for
pre-training with a clip-length of 8.

Figure 4 (a) shows performance on Kinetics. We may
observe that using full-ft, there is an improvement of ~1%
until 1350 labels, following which the performance satu-
rates. For fc-only approach, the improvement in accuracy is
~9% before it saturates at 2700 labels. This suggests that the
relatively fewer action labels in Kinetics (400) may not re-
quire a highly diverse and extensive pre-training label space
such as IG-Verb+Noun. However, a large image label space
( 17K hashtags) was proven [46] to be effective for highly
diverse target image tasks (e.g., ImageNet-5k). Hence, we
believe that to reap the full beneﬁts of a large pre-training
video label space, there is a need for more diverse bench-
mark video datasets with large label space.

Next, to understand the effect when the number of pre-
training labels are fewer than the target labels (i.e, <400
for Kinetics), we consider IG-Kinetics as our pre-training
dataset and vary the number of labels from 20 to 360. Pre-
training data size is again ﬁxed to 1M . From Fig. 4 (b), we
may observe a log-linear behavior as we vary the number of
labels. There is a signiﬁcant drop in the performance when
using fewer labels even in the full-ft evaluation setting. This
indicates that pre-training on a small label space that is a

subset of the target label space hampers performance.

In summary, while using fewer pre-training labels hurts
performance (Fig. 4 (b)), increasing the diversity through a
simple approach of combining verbs and nouns (Fig. 4 (a))
does not improve performance either. Thus, this analysis
highlights the challenges in label space engineering, espe-
cially for video tasks.

4.3. Exploring the temporal dimension of video

We now explore the temporal aspects of videos over long
and short time scales. As mentioned in Sec. 3.1, our dataset
inherently has large amounts of temporal noise as video
lengths vary from 1 – 60 seconds and no manual clean-
ing was undertaken. While short videos are better local-
ized, longer videos can potentially contain more diverse
content. First, we attempt to understand this trade-off be-
tween temporal noise and visual diversity. Second, we ad-
dress a more fundamental question of whether video clip-
based pre-training is needed at all or is frame-based pre-
training followed by inﬂation [11] is sufﬁcient. The latter
has an advantage of being very fast and more scalable.

4.3.1 Effect of temporal noise

To study this, we construct 3 datasets from IG-Kinetics:
(i) short-N: N videos of lengths between 1 – 5 seconds.
(ii) long-N: N videos of lengths between 55 – 60 seconds.
(iii) long-center-N: N videos (4 second long) con-
structed from the center portions of videos from long-N.
We ensure that the temporal dimension is the only fac-
tor that varies by keeping the label space and distribution
(videos per label) ﬁxed across these 3 datasets. Tempo-
ral jittering is performed for all these datasets during pre-
training. Also, note that the exact same number of videos
are seen while training on all the datasets. We now consider
the following two scenarios.
Fixed number of videos budget (F1): A natural question
that arises is: given a ﬁxed budget of videos, what tem-
poral property should guide our selection of pre-training
videos? To answer this, we ﬁx the total number of unique
videos to 5M and consider short-5M, long-5M, and
long-center-5M datasets. Note that both short-5M
and long-center-5M have similar per-video duration
(i.e., 4 seconds on average), but long-center-5M has
greater temporal noise, since short videos are presum-
ably more temporally localized than any given portion of

12051

long-5M

long-500K

short-5M

long-center-5M

Input dataset

Pre-training

F1
F2

60.6

-

-

50.6

57.4

51.4

ImageNet

Table 3. Video top-1 accuracy when R(2+1)D-34 is pre-trained on 4 different short
and long video datasets, followed by fc-only ﬁnetuning on Kinetics.

IG-Kinetics-19M-Images
IG-Kinetics-250M-Images

Input

Image
Image
Image

longer videos. Between short-5M and long-5M, while
short-5M has better temporal localization, long-5M
may have greater content diversity From Table 3, we may
observe that short-5M performs signiﬁcantly better than
long-center-5M suggesting that short videos do pro-
vide better temporal localization. Also, long-5M performs
better than short-5M by 3.2% indicating that more di-
verse content in longer videos can mask the effect of tem-
poral noise. Thus, for a ﬁxed total number of videos, longer
videos may beneﬁt transfer learning than short videos.

Fixed video time budget (F2): If storage or bandwidth
is a concern,
it is more practical to ﬁx the total dura-
tion of videos, instead of the total number. Given this
ﬁxed budget of video hours, should we choose short or
long videos? To answer this, we consider short-5M,
long-center-5M and long-500K datasets, all with
similar total video hours. From Table 3, we observe that
short-5M signiﬁcantly outperforms long-500K. This
indicates that diversity and/or temporal localization intro-
duced by using more short videos is more beneﬁcial than
the diversity within fewer long videos. Thus, for a ﬁxed
video duration budget, choosing more short videos yields
better results. long-center-5M and long-500K per-
form similarly, indicating that on average, a ﬁxed central
crop from a long video contains similar information to a
random crop from a long video. short-5M outperforms
long-center-5M, consistent with the claim that short
videos do indeed have better temporal localization.

4.3.2 Frame- vs. clip-based pre-training:
Although we have shown substantial gains when using clip-
based R(2+1)D models for large-scale weakly supervised
pre-training, it is computationally more intensive than 2D
(image) models. Moreover, techniques such as inﬂation
[11] efﬁciently leverage pre-trained image models by con-
verting 2D ﬁlters to 3D and achieve top-performance on
benchmark datasets. Given these, we want to understand
the key value in pre-training directly on weakly-supervised
video clips vs. images.

Towards this end, we ﬁrst construct an image variant
of the IG-Kinetics dataset (sufﬁxed by −Images in Table
4), following the procedure described in Sec. 3.1. We pre-
train an 18 layer 2D deep residual model (R2D) [29] from
scratch on different types of 2D data (image/single video
frames). We then inﬂate [11] this model to R3D8 [14] and
perform full-ﬁnetuning with a clip-length of 8 on Kinetics.

8We chose to inﬂate to R3D because it was not immediately obvious
how to inﬂate a 2D model to R(2+1)D given that it factorizes 3D convo-
lution to 2D spatial and 1D temporal [14].

Pre-train

model

R2D-18
R2D-18
R2D-18
R2D-18

R3D-18
R3D-18

FT

model

R3D-18
R3D-18
R3D-18
R3D-18

R3D-18
R3D-18

Top-1

66.5
67.0
67.0
67.5

65.6
71.7

IG-Kinetics-19M

Video frame

Kinetics

IG-Kinetics-19M

Video clip
Video clip

Table 4. Understanding the beneﬁt of using images vs. videos for pre-training.

From the inﬂation-based models in Table 4, we may ob-
serve that, pre-training on ImageNet achieves an improve-
ment of 0.9% compared to training R3D from scratch,
while pre-training on IG-Kinetics-19M-Images yields a
modest boost of 0.5% over ImageNet. Training on ran-
dom video frames from IG-Kinetics-19M gives a further
improvement of 0.5% over weakly-supervised image pre-
training and an overall boost of 1.0% over ImageNet. To
make sure that this marginal improvement is not because of
pre-training on only 19M weakly-supervised noisy images,
we pre-train using IG-Kinetics-250M-Images but ﬁnd no
further improvements. Finally, pre-training R3D directly
using video clips achieves an accuracy of 71.7%, a signif-
icant jump of 4.2% over the best inﬂated model (67.5%).
This clearly indicates that effectively modeling the temporal
structure of videos in a very large-scale pre-training setup is
extremely beneﬁcial.

4.4. Comparisons with state of the art

In this section, we compare R(2+1)D-34 and R(2+1)D-
152 models pre-trained on IG-Kinetics-65M with several
state-of-the-art approaches on 3 different target datasets.
For the results reported in this section alone, we follow [11]
to perform fully-convolutional prediction for a closely-fair
comparison with other approaches. Speciﬁcally, the fully-
connected layer in R(2+1)D is transformed into a 1 × 1 × 1
convolutional layer (while retaining learned weights), to al-
low fully-convolutional evaluation. Each test video is scaled
to 128 × 171, then cropped to 128 × 128 (a full center crop).
We also report results from using another frame scaling ap-
proach (indicated as SE in Tables 5 - 7), where each (train /
test) video’s shortest edge is scaled to 128, while maintain-
ing its original aspect ratio, followed by a full center crop..
We note that each approach being compared varies
greatly in terms of model architectures, pre-training datasets
(ImageNet vs. Sports-1M), amount and type of input data
(RGB vs ﬂow vs audio, etc.), input clip size, input frame
size, evaluation strategy, and so on. We also note that many
prior state-of-the-art models use complex, optimized net-
work architectures compared to ours. Despite these dif-
ferences, our approach of pre-training on tens of millions
of videos outperforms all existing methods by a substantial
margin of 3.6% when fully-ﬁnetuned on Kinetics (Table 5).
In Table 6, we report the performance on the valida-
tion [5], seen (S1), and unseen (S2) test datasets that are part

12052

Method; pre-training

top-1

top-5

Input type

Method; pre-training

Verbs

Nouns

Actions

I3D-Two-Stream [11]; ImageNet

R(2+1)D-Two-Stream [14]; Sports-1M

3-stream SATT [69]; ImageNet

NL I3D [65]; ImageNet

R(2+1)D-34; Sports-1M

Ours R(2+1)D-34; IG-Kinetics

Ours R(2+1)D-34; IG-Kinetics; SE

Ours R(2+1)D-152; IG-Kinetics

Ours R(2+1)D-152; IG-Kinetics; SE

75.7
75.4
77.7

77.7

71.7
79.1
79.6
80.5
81.3

92.0
91.9
93.2

93.3

90.5
93.9
94.2
94.6
95.1

RGB + ﬂow
RGB + ﬂow
RGB + ﬂow +

audio
RGB

RGB
RGB
RGB
RGB
RGB

Table 5. Comparison with the state-of-the-art on Kinetics. SE: short edge scaling.

of the EPIC-Kitchens Action Recognition Challenge [1].
Since the training dataset of EPIC-Kitchens consists of only
~20K videos, for stronger baselines, we pre-train separate
R(2+1)D-34 models on Kinetics and Sports-1M and ﬁne-
tune on EPIC-Kitchens. We also report the top-performing
method from the challenge website [1] at the time of this
manuscript submission. From Table 6, we may observe
that, on unseen kitchens (S2), R(2+1)D-152 pre-trained on
IG-Kinetics-65M improves the top-1 accuracy on verbs and
nouns by 8.9% and 9.1% compared to R(2+1)D-34 pre-
trained on Kinetics; and a 7.3% boost on actions compared
to R(2+1)D-34 pre-trained on Sports-1M. Similar substan-
tial gains hold for seen (S1) and validation datasets. We note
that we process only 32 RGB frames of the input video (no
optical ﬂow), at a much lower resolution (128 × 128) com-
pared to the state-of-the-art, which is an ensemble model.

Finally, we report the performance (Table 7) on the val-
idation data of Something-V1 [28], a challenging dataset
with ﬁne-grained classes. Using only RGB as input, pre-
training with IG-Kinetics-65M achieves a top-1 accuracy of
51.6%, an improvement of 2.1% over state-of-the-art [70]9
(49.5%). Compared to other approaches that use only RGB
as input [68], our approach yields a boost of 3.4%.

5. Discussion

In this work, we explored the feasibility of large-scale,
noisy, weakly-supervised pre-training with tens of million
of videos. Despite the presence of signiﬁcant noise in la-
bel space and temporal localization, our pre-trained mod-
els learn very strong feature representations. These models
are able to signiﬁcantly improve the state-of-the-art action
recognition results on the popular Kinetics [64], a recently
introduced EPIC-Kitchens [16], and Something-something
[28] datasets. All of our large-scale pre-trained models
show signiﬁcant gains over Kinetics and Sports-1M, the
de facto pre-training datasets in the literature. Our abla-
tion studies address many important questions related to
scale, label space, and temporal dimension, while also rais-
ing other interesting questions.

Our study of label spaces found that sampling from
the joint distribution of verb-noun pairs performs relatively
poorly; this is presumably due to the skewed distribution

9This number is achieved using RGB+ﬂow and an ensemble of models.

top-1

top-5

top-1

top-5

top-1

top-5

Test Unseen (S2)

Leaderboard [1]

54.5

81.2

30.4

55.7

21.0

39.4

R(2+1)D [14]

d=34; Kinetics

d=34; Sports-1M

Ours: d=34; IG-Kin.

Ours: d=34; IG-Kin. ; SE

Ours: d=152; IG-Kin.

Ours: d=152; IG-Kin.; SE

48.4
47.2
55.5
56.0
55.3
57.3

77.2
77.4
80.9
80.6
80.3
81.1

Test Seen (S1)

26.6
28.7
33.6
32.4
34.7
35.7

50.4
50.0
56.7
55.6
58.2
58.7

16.8
18.3
23.7
23.6
25.4
25.6

31.2
31.6
39.1
39.5
40.7
42.7

Leaderboard [1]

66.1

91.3

47.9

72.8

36.7

58.6

R(2+1)D [14]

d=34; Kinetics
d-34; Sports-1M

Ours: d=34; IG-Kin.

Ours: d=34; IG-Kin. ; SE

Ours: d=152; IG-Kin.

Ours: d=152; IG-Kin.; SE

59.1
59.6
63.3
63.2
63.8
65.2

87.4
87.2
87.5
87.6
87.7
87.4

Validation

38.0
43.7
46.3
45.4
45.3
45.1

62.7
67.0
69.6
68.7
68.3
67.8

26.8
31.0
34.4
33.4
34.1
34.5

46.1
50.3
54.2
52.4
53.5
53.8

Baradel et. al. [5]; -

40.9

-

-

-

-

-

R(2+1)D [14]

d=34; Kinetics

d=34; Sports-1M

Ours: d=34; IG-Kin.

Ours: d=34; IG-Kin. ; SE

Ours: d=152; IG-Kin.

Ours: d=152; IG-Kin.; SE

46.8
50.0
56.6
55.5
56.6
58.4

79.2
79.8
83.5
83.3
83.8
84.1

25.6
24.8
32.7
34.8
34.5
36.9

47.5
46.2
55.5
57.2
58.5
60.3

15.3
16.0
22.5
22.8
23.5
26.1

29.4
30.3
39.2
39.8
40.6
42.7

Table 6. Comparison with the state-of-the-art approaches on Epic-Kitchens dataset.
IG-Kin. refers to IG-Kinetics. SE: short edge scaling.

Method; pre-training

top-1

top-5

Input type

NL I3D + Joint GCN [66]

S3D-G [68]

ECOEnLite [70]
ECOEnLite [70]

R(2+1)D-34; Kinetics

R(2+1)D-34; Sports-1M

Ours: R(2+1)D-34; IG-Kin.

Ours: R(2+1)D-34; IG-Kin.; SE

Ours: R(2+1)D-152; IG-Kin.

Ours: R(2+1)D-152; IG-Kin.; SE

46.1
48.2
46.4
49.5

45.2
45.7
49.7
49.9
51.0
51.6

76.8
78.7

-
-

74.1
74.5
77.5
77.5
79.0
78.8

RGB
RGB
RGB

RGB + ﬂow

RGB
RGB
RGB
RGB
RGB
RGB

Table 7. Comparison with the state-of-the-art on Something-V1 [28].
refers to IG-Kinetics. SE: short edge scaling.

IG-Kin.

and suggests that solving low-shot learning at scale is an
important area of investment. Data augmentation can also
help here: social media offers a nearly unlimited supply
of unlabeled video, and non-parametric approaches like K-
nearest neighbors could be employed to enhance sparse la-
bels. Optimizing label space granularity in the face of data
sparsity is another worth-while direction; this may require
ﬁnding algorithmic ways to construct hashtag taxonomies
for videos. Future research should also invest in creating
new public benchmarks with larger and richer label spaces.
Label spaces of current target datasets are small; they do not
reﬂect the value of large-scale pre-training.

Finally, our analysis of temporal localization raises more
questions. Our experiments clearly show the competing
beneﬁts of both good temporal localization in short videos,
and greater diversity in long videos. Understanding this
trade-off more rigorously may lead to intelligent data con-
struction strategies that can leverage the best of both worlds.

12053

References

[1] EPIC-Kitchens Action Recognition Challenge.

[Online]
Available https://competitions.codalab.org/
competitions/20115#results. 8

[2] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m: A
large-scale video classiﬁcation benchmark. arXiv preprint
arXiv:1609.08675, 2016. 2

[3] M. Andriluka, U.

Iqbal, A. Milan, E.

Insafutdinov,
L. Pishchulin, J. Gall, and B. Schiele. Posetrack: A bench-
mark for human pose estimation and tracking.
In CVPR,
2018. 1

[4] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K.
Ni, D. Poland, D. Borth, and L. Li. The new data and new
challenges in multimedia research. CoRR, abs/1503.01817,
2015. 2

[5] F. Baradel, N. Neverova, C. Wolf, J. Mille, and G. Mori.
arXiv preprint

Object level visual reasoning in videos.
arXiv:1806.06157, 2018. 3, 7, 8

[6] A. Bergamo and L. Torresani. Exploiting weakly-labeled
web images to improve object classiﬁcation: a domain adap-
tation approach. In NIPS, 2010. 2

[7] H. Bilen and A. Vedaldi. Weakly supervised deep detection

networks. In CVPR, 2016. 2

[8] P. Bojanowski, F. Bach, I. Laptev, J. Ponce, C. Schmid, and
In ICCV,

J. Sivic. Finding actors and actions in movies.
2013. 2

[9] Caffe2 Team. Caffe2 : A new lightweight, modular, and scal-
able deep learning framework. [Online] Available https:
//caffe2.ai/. 4

[10] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-
person 2d pose estimation using part afﬁnity ﬁelds. arXiv
preprint arXiv:1611.08050, 2016. 1

[11] J. Carreira and A. Zisserman. Quo vadis, action recognition?
a new model and the kinetics dataset. CVPR, 2017. 1, 2, 6,
7, 8

[12] X. Chen and A. Gupta. Webly supervised learning of convo-

lutional networks. In ICCV, 2015. 2

[13] X. Chen, A. Shrivastava, and A. Gupta. Neil: Extracting

visual knowledge from web data. In ICCV, 2013. 2

[14] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M.
Paluri. A closer look at spatiotemporal convolutions for ac-
tion recognition. CVPR, 2018. 2, 3, 7, 8

[15] J. Dai, K. He, and J. Sun. Boxsup: Exploiting bounding
boxes to supervise convolutional networks for semantic seg-
mentation. In ICCV, 2015. 2

[16] D. Damen, H. Doughty, G. M. Farinella, S. Fidler,
A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett,
W. Price, and M. Wray. Scaling egocentric vision: The epic-
kitchens dataset. In ECCV, 2018. 2, 3, 8

[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
Imagenet: A large-scale hierarchical image database. CVPR,
2009. 2, 3

[18] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning ev-
erything about anything: Webly-supervised visual concept
learning. In CVPR, 2014. 2

[19] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. Efros. What
makes paris look like paris? ACM Transactions on Graphics,
2012. 2

[20] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional ac-
tivation feature for generic visual recognition. ICML, 2014.
1, 2

[21] O. Duchenne, I. Laptev, J. Sivic, F. Bach, and J. Ponce. Auto-
matic annotation of human actions in video. In CVPR. IEEE,
2009. 2

[22] T. Durand, T. Mordan, N. Thome, and M. Cord. Wild-
cat: Weakly supervised learning of deep convnets for image
classiﬁcation, pointwise localization and segmentation.
In
CVPR, 2017. 2

[23] M. Everingham, J. Sivic, and A. Zisserman. Hello! my name
is buffy–automatic naming of characters in tv video. 2006. 2
[24] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-
ture tells a story: Generating sentences from images.
In
ECCV, 2010. 2

[25] R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning
object categories from internet image searches. Proceedings
of the IEEE, 2010. 2

[26] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 1, 2

[27] P. Goyal,

P. Doll´ar, R. Girshick,

P. Noordhuis,
L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He.
Accurate, large minibatch sgd: training imagenet in 1 hour.
arXiv preprint arXiv:1706.02677, 2017. 4

[28] R. Goyal, S. E. Kahou, V. Michalski, J. Materzynska,
S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos,
M. Mueller-Freitag, et al. The “something something” video
database for learning and evaluating visual common sense.
In ICCV, 2017. 2, 3, 8

[29] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, pages 770–778, 2016. 3, 7
[30] S. Hong, J. Oh, H. Lee, and B. Han. Learning transferrable
knowledge for semantic segmentation with deep convolu-
tional neural network. In CVPR, 2016. 2

[31] S. Hong, D. Yeo, S. Kwak, H. Lee, and B. Han. Weakly su-
pervised semantic segmentation using web-crawled videos.
arXiv preprint arXiv:1701.00352, 2017. 2

[32] M. Huh, P. Agrawal, and A. A. Efros. What makes
arXiv preprint

imagenet good for transfer learning?
arXiv:1608.08614, 2016. 1

[33] S. C. Ioffe, S. Batch normalization: Accelerating deep net-
ICML,

work training by reducing internal covariate shift.
2015. 4

[34] H. Izadinia, B. C. Russell, A. Farhadi, M. D. Hoffman, and
A. Hertzmann. Deep classiﬁers from image tags in the wild.
In Workshop on Community-Organized Multimodal Mining:
Opportunities for Novel Solutions, 2015. 2

[35] Y. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes.
2014.
http://crcv.ucf.edu/THUMOS14. 2

12054

[36] A. Joulin, L. van der Maaten, A. Jabri, and N. Vasilache.
Learning visual features from large weakly supervised data.
ECCV, 2016. 1, 2, 3

[56] A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Fer-
rari. Learning object class detectors from weakly annotated
video. In CVPR, 2012. 2

[57] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and
A. Rabinovich. Training deep neural networks on noisy la-
bels with bootstrapping. arXiv preprint arXiv:1412.6596,
2014. 2

[58] F. Schroff, A. Criminisi, and A. Zisserman. Harvesting im-

age databases from the web. PAMI, 2011. 2

[59] Z. Shi, P. Siva, and T. Xiang. Transfer learning by rank-
ing for weakly supervised object annotation. arXiv preprint
arXiv:1705.00873, 2017. 2

[60] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset
of 101 human actions classes from videos in the wild. CRCV-
TR-12-01, 2012. 2

[61] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fer-
gus. Training convolutional networks with noisy labels.
arXiv preprint arXiv:1406.2080, 2014. 2

[62] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisit-
ing unreasonable effectiveness of data in deep learning era.
ICCV, 2017. 1, 2, 3, 4

[63] VerbNet. VerbNet : A Computational Lexical Resource for
Verbs. [Online] Available https://verbs.colorado.
edu/verbnet/. 3

[64] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S.
Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M.
Suleyman, and A. Zisserman. The kinetics human action
video dataset. arXiv:1705.06950, 2017. 2, 3, 4, 8

[65] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural

networks. CVPR, 2018. 2, 8

[66] X. Wang and A. Gupta. Videos as space-time region graphs.

arXiv preprint arXiv:1806.01810, 2018. 8

[67] X.-J. Wang, L. Zhang, X. Li, and W.-Y. Ma. Annotating

images by mining image search results. PAMI, 2008. 2

[68] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy. Rethink-
ing spatiotemporal feature learning for video understanding.
arXiv preprint arXiv:1712.04851, 2017. 8

[69] Y. Bian, C. Gan, X. Liu, F. Li, X. Long, Y. Li, H. Qi, J.
Zhou, S. Wen, and Y. Lin. Revisiting the effectiveness of
off-the-shelf temporal modeling approaches for large-scale
video classiﬁcation. arXiv:1708.03805, 2017, 2017. 3, 8

[70] M. Zolfaghari, K. S. Singh, and T. Brox. ECO: efﬁcient con-
volutional network for online video understanding. arXiv
preprint arXiv:1804.09066, 2018. 2, 8

[37] A. Joulin, L. van der Maaten, A. Jabri, and N. Vasilache.
Learning visual features from large weakly supervised data.
In ECCV, 2016. 2

[38] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. CVPR, 2014. 2, 4

[39] A. Kolesnikov and C. H. Lampert. Seed, expand and con-
strain: Three principles for weakly-supervised image seg-
mentation. In ECCV, 2016. 2

[40] O. Koller, H. Ney, and R. Bowden. Deep hand: How to train
a cnn on 1 million hand images when your data is continuous
and weakly labelled. In CVPR, 2016. 2

[41] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
HMDB: a large video database for human motion recogni-
tion. ICCV, 2011. 2

[42] I. Laptev and T. Lindeberg.

Space-time interest points.

ICCV, 2003. 2

[43] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld.
Learning realistic human actions from movies. CVPR, 2008.
2

[44] A. Li, A. Jabri, A. Joulin, and L. van der Maaten. Learning

visual n-grams from web data. In ICCV, 2017. 2

[45] Z. Li and D. Hoiem. Learning without forgetting. PAMI,

2017. 2

[46] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri,
Y. Li, A. Bharambe, and L. van der Maaten. Exploring the
limits of weakly supervised pretraining. ECCV, 2018. 1, 2,
3, 4, 5, 6

[47] M. Marszalek, I. Laptev, and C. Schmid. Actions in context.

In CVPR, 2009. 2

[48] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS, 2013. 3

[49] J. Y.-H. Ng, J. Choi, J. Neumann, and L. S. Davis. Action-
ﬂownet: Learning motion representation for action recogni-
tion. In WACV, 2018. 1

[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object lo-
calization for free?-weakly-supervised learning with convo-
lutional neural networks. In CVPR, 2015. 2

[51] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describ-
ing images using 1 million captioned photographs. In NIPS,
2011. 2

[52] D. Pathak, P. Krahenbuhl, and T. Darrell. Constrained con-
volutional neural networks for weakly supervised segmenta-
tion. In ICCV, 2015. 2

[53] D. Pathak, E. Shelhamer, J. Long, and T. Darrell. Fully
convolutional multi-class multiple instance learning. arXiv
preprint arXiv:1412.7144, 2014. 2

[54] J. Peyre, I. Laptev, C. Schmid, and J. Sivic. Weakly-

supervised learning of visual relations. In ICCV, 2017. 2

[55] P. O. Pinheiro and R. Collobert. From image-level to pixel-
level labeling with convolutional networks. In CVPR, 2015.
2

12055

