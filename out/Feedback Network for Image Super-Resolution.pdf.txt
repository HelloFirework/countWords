Feedback Network for Image Super-Resolution

Zhen Li1

Jinglei Yang2

Zheng Liu3 Xiaomin Yang1∗ Gwanggil Jeon4 Wei Wu1∗

1Sichuan University, 2University of California, Santa Barbara, 3University of British Columbia,

4Incheon National University

Abstract

Recent advances in image super-resolution (SR) ex-
plored the power of deep learning to achieve a better re-
construction performance. However, the feedback mecha-
nism, which commonly exists in human visual system, has
not been fully exploited in existing deep learning based
image SR methods.
In this paper, we propose an image
super-resolution feedback network (SRFBN) to reﬁne low-
level representations with high-level information. Speciﬁ-
cally, we use hidden states in a recurrent neural network
(RNN) with constraints to achieve such feedback manner.
A feedback block is designed to handle the feedback con-
nections and to generate powerful high-level representa-
tions. The proposed SRFBN comes with a strong early re-
construction ability and can create the ﬁnal high-resolution
image step by step.
In addition, we introduce a curricu-
lum learning strategy to make the network well suitable
for more complicated tasks, where the low-resolution im-
ages are corrupted by multiple types of degradation. Ex-
tensive experimental results demonstrate the superiority of
the proposed SRFBN in comparison with the state-of-the-
art methods. Code is avaliable at https://github.
com/Paper99/SRFBN_CVPR19.

1. Introduction

Image super-resolution (SR) is a low-level computer vi-
sion task, which aims to reconstruct a high-resolution (HR)
image from its low-resolution (LR) counterpart.
It is in-
herently ill-posed since multiple HR images may result
in an identical LR image. To address this problem, nu-
merous image SR methods have been proposed, includ-
ing interpolation-based methods[45], reconstruction-based
methods[42], and learning-based methods [33, 26, 34, 15,
29, 6, 18].

Since Dong et al. [6] ﬁrstly introduced a shallow Convo-
lutional Neural Network (CNN) to implement image SR,
deep learning based methods have attracted extensive at-

∗Corresponds to: {arielyang,wuwei}@scu.edu.cn

Output

t

F

out

Feedback

block

F


1t
out

F

in

Input

(a)

(b)

Figure 1. The illustrations of the feedback mechanism in the pro-
posed network. Blue arrows represent the feedback connections.
(a) Feedback via the hidden state at one iteration. The feedback
block (FB) receives the information of the input Fin and hidden
state from last iteration F t−1
to the next iteration and output. (b) The principle of our feedback
scheme.

out , and then passes its hidden state F t

out

tention in recent years due to their superior reconstruction
performance. The beneﬁts of deep learning based methods
mainly come from its two key factors, i.e., depth and skip
connections (residual or dense) [18, 36, 31, 11, 47, 46, 37].
The ﬁrst one provides a powerful capability to represent and
establish a more complex LR-HR mapping, while preserv-
ing more contextual information with larger receptive ﬁelds.
The second factor can efﬁciently alleviate the gradient van-
ishing/exploding problems caused by simply stacking more
layers to deepen networks.

As the depth of networks grows, the number of parame-
ters increases. A large-capacity network will occupy huge

3867

Unfold in timeLRSRLRLRLR1t2t3tMore clearly1SR2SR3SRstorage resources and suffer from the overﬁtting problem.
To reduce network parameters, the recurrent structure is
often employed. Recent studies [22, 10] have shown that
many networks with recurrent structure (e.g. DRCN [19]
and DRRN [31]) can be extrapolated as a single-state Re-
current Neural Network (RNN). Similar to most conven-
tional deep learning based methods, these networks with
recurrent structure can share the information in a feedfor-
ward manner. However, the feedforward manner makes it
impossible for previous layers to access useful information
from the following layers, even though skip connections are
employed.

In cognition theory, feedback connections which link
the cortical visual areas can transmit response signals from
higher-order areas to lower-order areas [17, 9]. Motivated
by this phenomenon, recent studies [30, 40] have applied
the feedback mechanism to network architectures. The
feedback mechanism in these architectures works in a top-
down manner, carrying high-level information back to pre-
vious layers and reﬁning low-level encoded information.

In this paper, we propose a novel network for image SR,
namely the Super-Resolution Feedback Network (SRFBN),
in order to reﬁne low-level information using high-level one
through feedback connections. The proposed SRFBN is
essentially an RNN with a feedback block (FB), which is
speciﬁcally designed for image SR tasks. The FB is con-
structed by multiple sets of up- and down-sampling layers
with dense skip connections to generate powerful high-level
representations. Inspired by [40], we use the output of the
FB, i.e., a hidden state in an unfolded RNN, to achieve the
feedback manner (see Fig. 1(a)). The hidden state at each
iteration ﬂows into the next iteration to modulate the input.
To ensure the hidden state contains the information of the
HR image, we connect the loss to each iteration during the
training process. The principle of our feedback scheme is
that the information of a coarse SR image can facilitate an
LR image to reconstruct a better SR image (see Fig. 1(b)).
Furthermore, we design a curriculum for the case, in which
the LR image is generated by a complex degradation model.
For each LR image, its target HR images for consecutive
iterations are arranged from easy to hard based on the re-
covery difﬁculty. Such curriculum learning strategy well
assists our proposed SRFBN in handling complex degrada-
tion models. Experimental results demonstrate the superi-
ority of our proposed SRFBN against other state-of-the-art
methods.

In summary, our main contributions are as follows:

• Proposing an image super-resolution feedback net-
work (SRFBN), which employs a feedback mecha-
nism. High-level information is provided in top-down
feedback ﬂows through feedback connections. Mean-
while, such recurrent structure with feedback connec-
tions provides strong early reconstruction ability, and

requires only few parameters.

• Proposing a feedback block (FB), which not only ef-
ﬁciently handles feedback information ﬂows, but also
enriches high-level representations via up- and down-
sampling layers, and dense skip connections.

• Proposing a curriculum-based training strategy for the
proposed SRFBN, in which HR images with increas-
ing reconstruction difﬁculty are fed into the network as
targets for consecutive iterations. This strategy enables
the network to learn complex degradation models step
by step, while the same strategy is impossible to settle
for those methods with only one-step prediction.

2. Related Work

2.1. Deep learning based image super resolution

Deep learning has shown its superior performance in var-
ious computer vision tasks including image SR. Dong et
al. [7] ﬁrstly introduced a three-layer CNN in image SR to
learn a complex LR-HR mapping. Kim et al. [18] increased
the depth of CNN to 20 layers for more contextual infor-
mation usage in LR images. In [18], a skip connection was
employed to overcome the difﬁculty of optimization when
the network became deeper. Recent studies have adopted
different kind of skip connections to achieve remarkable im-
provement in image SR. SRResNet[21] and EDSR[23] ap-
plied residual skip connections from [13]. SRDenseNet[36]
applied dense skip connections from [14]. Zhang et al. [47]
combined local/global residual and dense skip connections
in their RDN. Since the skip connections in these net-
work architectures use or combine hierarchical features in
a bottom-up way, the low-level features can only receive
the information from previous layers, lacking enough con-
textual information due to the limitation of small receptive
ﬁelds. These low-level features are reused in the follow-
ing layers, and thus further restrict the reconstruction abil-
ity of the network. To ﬁx this issue, we propose a super-
resolution feedback network (SRFBN), in which high-level
information ﬂows through feedback connections in a top-
down manner to correct low-level features using more con-
textual information.

Meanwhile, with the help of skip connections, neural
networks go deeper and hold more parameters. Such large-
capacity networks occupy huge amount of storage resources
and suffer from overﬁtting. To effectively reduce network
parameters and gain better generalization power, the recur-
rent structure was employed[19, 31, 32]. Particularly, the
recurrent structure plays an important role to realize the
feedback process in the proposed SRFBN (see Fig. 1(b)).

2.2. Feedback mechanism

The feedback mechanism allows the network to carry a
notion of output to correct previous states. Recently, the

3868

Upsample

SR

3x3 Conv

Deconv

FB 

3x3 Conv

3x3 Conv

LR

Upsample

Unfold

1SR
I

1
SR

I

1
Re s

3x3 Conv

Deconv

F

1
out

FB 

F

1
in

3x3 Conv

3x3 Conv

I

LR

LR

F

1
out

Upsample

F 

1t
out

tSR
I

t

SR

I

t

Re s

3x3 Conv

Deconv

t

F

out

FB 

F

t

in

Upsample

F 

1T
out

t

F

out

TSR
I

T

SR

I

T

Re s

3x3 Conv

Deconv

T

F

out

FB 

F

T

in

3x3 Conv

3x3 Conv

I

LR

LR

3x3 Conv

3x3 Conv

I

LR

LR

Figure 2. The architecture of our proposed super-resolution feedback network (SRFBN). Blue arrows represent feedback connections.
Green arrows represent global residual skip connections.

feedback mechanism has been adopted by many network
architectures for various vision tasks[5, 4, 40, 11, 10, 28].

For image SR, a few studies also showed efforts to intro-
duce the feedback mechanism. Based on back-projection,
Haris et al. [11] designed up- and down-projection units to
achieve iterative error feedback. Han et al. [10] applied a
delayed feedback mechanism which transmits the informa-
tion between two recurrent states in a dual-state RNN. How-
ever, the ﬂow of information from the LR image to the ﬁnal
SR image is still feedforward in their network architectures
unlike ours.

The most relevant work to ours is [40], which transfers
the hidden state with high-level information to the informa-
tion of an input image to realize feedback in an convolu-
tional recurrent neural network. However, it aims at solving
high-level vision tasks, e.g. classiﬁcation. To ﬁt a feedback
mechanism in image SR, we elaborately design a feedback
block (FB) as the basic module in our SRFBN, instead of
using ConvLSTM as in [40]. The information in our FB ef-
ﬁciently ﬂows across hierarchical layers through dense skip
connections. Experimental results indicate our FB has supe-
rior reconstruction performance than ConvLSTM1 and thus
is more suitable for image SR tasks.

2.3. Curriculum learning

Curriculum learning[2], which gradually increases the
difﬁculty of the learned target, is well known as an ef-
ﬁcient strategy to improve the training procedure. Early
work of curriculum learning mainly focuses on a single
task. Pentina et al. [27] extended curriculum learning to
multiple tasks in a sequential manner. Gao et al. [8] utilized
curriculum learning to solve the ﬁxation problem in image
restoration. Since their network is limited to a one-time pre-
diction, they enforce a curriculum through feeding different
training data in terms of the complexity of tasks as epoch in-
creases during the training process. In the context of image

1Further analysis can be found in our supplementary material.

SR, Wang et al. [38] designed a curriculum for the pyramid
structure, which gradually blends a new level of the pyra-
mid in previously trained networks to upscale an LR image
to a bigger size.

While previous works focus on a single degradation pro-
cess, we enforce a curriculum to the case, where the LR
image is corrupted by multiple types of degradation. The
curriculum containing easy-to-hard decisions can be settled
for one query to gradually restore the corrupted LR image.

3. Feedback Network for Image SR

Two requirements are contained in a feedback system:
(1) iterativeness and (2) rerouting the output of the system
to correct the input in each loop. Such iterative cause-and-
effect process helps to achieve the principle of our feedback
scheme for image SR: high-level information can guide an
LR image to recover a better SR image (see Fig. 1(b)). In
the proposed network, there are three indispensable parts to
enforce our feedback scheme: (1) tying the loss at each it-
eration (to force the network to reconstruct an SR image at
each iteration and thus allow the hidden state to carry a no-
tion of high-level information), (2) using recurrent structure
(to achieve iterative process) and (3) providing an LR input
at each iteration (to ensure the availability of low-level in-
formation, which is needed to be reﬁned). Any absence of
these three parts will fail the network to drive the feedback
ﬂow.

3.1. Network structure

As shown in Fig. 2, our proposed SRFBN can be un-
folded to T iterations, in which each iteration t is tempo-
rally ordered from 1 to T . In order to make the hidden state
in SRFBN carry a notion of output, we tie the loss for every
iteration. The description of the loss function can be found
in Sec. 3.3. The sub-network placed in each iteration t con-
tains three parts: an LR feature extraction block (LRFB), a
feedback block (FB) and a reconstruction block (RB). The

3869

weights of each block are shared across time. The global
residual skip connection at each iteration t delivers an up-
sampled image to bypass the sub-network. Therefore, the
purpose of the sub-network at each iteration t is to recover
a residual image I t
Res while input a low-resolution image
ILR. We denote Conv(s, n) and Deconv(s, n) as a con-
volutional layer and a deconvolutional layer respectively,
where s is the size of the ﬁlter and n is the number of ﬁlters.
of
Conv(3, 4m) and Conv(3, m). m denotes the base
number of ﬁlters. We provide an LR input ILR for the LR
feature extraction block, from which we obtain the shallow
features F t

in containing the information of an LR image:

The LR feature

extraction

consists

block

F t

in = fLRF B(ILR),

(1)

where fLRF B denotes the operations of the LR feature ex-
traction block. F t
in are then used as the input to the FB. In
addition, F 1

in are regarded as the initial hidden state F 0

out.

The FB at the t-th iteration receives the hidden state from
through a feedback connection and
out represents the output of the FB.

previous iteration F t−1
out
in. F t
shallow features F t
The mathematical formulation of the FB is:

out = fF B(F t−1
F t

out , F t

in),

(2)

where fF B denotes the operations of the FB and actually
represents the feedback process as shown in Fig. 1(a). More
details of the FB can be found in Sec. 3.2.

The reconstruction block uses Deconv(k, m) to upscale
out to HR ones and Conv(3, cout) to generate
Res. The mathematical formulation of the

LR features F t
a residual image I t
reconstruction block is:

I t
Res = fRB(F t

out),

(3)

where fRB denotes the operations of the reconstruction
block.

The output image I t

SR at the t-th iteration can be ob-

tained by:

I t
SR = I t

Res + fU P (ILR),

(4)

where fU P denotes the operation of an upsample kernel.
The choice of the upsample kernel is arbitrary. We use a
bilinear upsample kernel here. After T iterations, we will
get totally T SR images (I 1

SR, ..., I T

SR, I 2

SR).

3.2. Feedback block

As shown in Fig. 3, the FB at the t-th iteration re-
ceives the feedback information F t−1
to correct low-level
out
representations F t
in, and then passes more powerful high-
level representations F t
out as its output to the next iteration
and the reconstruction block. The FB contains G projec-
tion groups sequentially with dense skip connections among
them. Each projection group, which can project HR features

Figure 3. Feedback block (FB).

to LR ones, mainly includes an upsample operation and a
downsample operation.

At the beginning of the FB, F t

out are concate-
nated and compressed by Conv(1, m) to reﬁne input fea-
tures F t
out , producing the re-
ﬁned input features Lt
0:

in by feedback information F t−1

in and F t−1

Lt

0 = C0([F t−1

out , F t

in]),

(5)

out , F t

in] refers to the concatenation of F t−1

where C0 refers to the initial compression operation and
[F t−1
in.
Let H t
g be the HR and LR feature maps given by the
g-th projection group in the FB at the t-th iteration. H t
g can
be obtained by:

out and F t

g and Lt

H t

g = C ↑

g ([Lt

0, Lt

1, ..., Lt

g−1]),

(6)

refers

where C ↑
to the upsample operation using
g
Deconv(k, m) at the g-th projection group. Correspond-
ingly, Lt

g can be obtained by

Lt

g = C ↓

g ([H t

1, H t

2, ..., H t

g]),

(7)

where C ↓
refers to the downsample operation using
g
Conv(k, m) at the g-th projection group. Except for the
ﬁrst projection group, we add Conv(1, m) before C ↑
g and
C ↓

g for parameter and computation efﬁciency.
In order to exploit useful information from each projec-
tion group and map the size of input LR features F t+1
in at the
next iteration, we conduct the feature fusion (green arrows
in Fig. 3) for LR features generated by projection groups to
generate the output of FB:

F t
out = CF F ([Lt

1, Lt

2, ..., Lt

G]),

(8)

where CF F represents the function of Conv(1, m).

3.3. Curriculum learning strategy

(I 1

HR, I 2

HR, ..., I T

T target HR images (I 1

We choose L1 loss to optimize our proposed net-
HR) are
work.
placed to ﬁt in the multiple output in our proposed net-
HR) are identical for the single
work.
degradation model.
For complex degradation models,
(I 1
HR) are ordered based on the difﬁculty of
tasks for T iterations to enforce a curriculum. The loss func-
tion in the network can be formulated as:

HR, ..., I T

HR, ..., I T

HR, I 2

HR, I 2

L(Θ) =

1
T

T

X

t=1

W t (cid:13)(cid:13)I t

HR

− I t

SR(cid:13)(cid:13)1

,

(9)

3870

ConvDeconv1x1 ConvConv1x1 ConvDeconv1x1 ConvConv1x1 ConvDeconv1x1 Conv1x1 Conv1toutFtoutFtinF0tL1tLtgLTGLtGH1tHtgHwhere Θ denotes to the parameters of our network. W t is
a constant factor which demonstrates the worth of the out-
put at the t-th iterations. As [40] do, we set the value to 1
for each iteration, which represents each output has equal
contribution. Details about settings of target HR images for
complex degradation models will be revealed in Sec. 4.4.

3.4. Implementation details

We use PReLU[12] as the activation function following
all convolutional and deconvolutional layers except the last
layer in each sub-network. Same as [11], we set various k in
Conv(k, m) and Deconv(k, m) for different scale factors
to perform up- and down-sampling operations. For ×2 scale
factor, we set k in Conv(k, m) and Deconv(k, m) as 6 with
two striding and two padding. Then, for ×3 scale factor, we
set k = 7 with three striding and two padding. Finally, for
×4 scale factor, we set k = 8 with four striding and two
padding. We take the SR image I T
SR at the last iteration
as our ﬁnal SR result unless we speciﬁcally analysis every
output image at each iteration. Our network can process
both gray and color images, so cout can be 1 or 3 naturally.

4. Experimental Results

4.1. Settings

Datasets and metrics. We use DIV2K[1] and Flickr2K
as our training data. To make full use of data, we adopt
data augmentation as [23] do. We evaluate SR results under
PSNR and SSIM[39] metrics on ﬁve standard benchmark
datasets: Set5[3], Set14[41], B100[24], Urban100[15], and
Manga109[25]. To keep consistency with previous works,
quantitative results are only evaluated on luminance (Y)
channel.

Degradation models. In order to make fair comparison
with existing models, we regard bicubic downsampling as
our standard degradation model (denoted as BI) for gener-
ating LR images from ground truth HR images. To ver-
ify the effectiveness of our curriculum learning strategy, we
further conduct two experiments involving two other multi-
degradation models as [47] do in Sec. 4.4 and 4.5.3. We
deﬁne BD as a degradation model which applies Gaussian
blur followed by downsampling to HR images. In our ex-
periments, we use 7x7 sized Gaussian kernel with standard
deviation 1.6 for blurring. Apart from the BD degradation
model, DN degradation model is deﬁned as bicubic down-
sampling followed by adding Gaussian noise, with noise
level of 30.

Scale factor

×2

×3

×4

Input patch size

60 × 60

50 × 50

40 × 40

Table 1. The settings of input patch size.

Training settings. We train all networks with the batch-

size of 16. To fully exploit contextual information from LR
images, we feed RGB image patches with different patch
size based on the upscaling factor. The settings of input
patch size are listed in Tab. 1. The network parameters are
initialized using the method in [12]. Adam[20] is employed
to optimize the parameters of the network with initial learn-
ing rate 0.0001. The learning rate multiplies by 0.5 for ev-
ery 200 epochs. We implement our networks with Pytorch
framework and train them on NVIDIA 1080Ti GPUs.

43;0703.0,3,8841%

43;0703.0,3,8841







 


/
 
#

$
!









 


/
 
#

$
!



%
%
%
%
'$#






54.

(a)









%
%
%
%
'$#






54.

(b)

Figure 4. Convergence analysis of T and G on Set5 with scaling
factor ×4.

4.2. Study of T and G

In this subsection, we explore the inﬂuence of the num-
ber of iterations (denoted as T) and the number of pro-
jection groups in the feedback block (denoted as G). The
base number of ﬁlters m is set to 32 in subsequent exper-
iments. We ﬁrst investigate the inﬂuence of T by ﬁxing G
to 6. It can be observed from Fig. 4(a) that with the help
of feedback connection(s), the reconstruction performance
is signiﬁcantly improved compared with the network with-
out feedback connections (T=1). Besides, as T continues to
increase, the reconstruction quality keeps rising. In other
words, our feedback block surely beneﬁts the information
ﬂow across time. We then study the inﬂuence of G by ﬁx-
ing T to 4. Fig. 4(b) shows that larger G leads to higher ac-
curacy due to stronger representative ability of deeper net-
works.
In conclusion, choosing larger T or G both con-
tribute to better results. It is worth noticing that small T and
G still outperform VDSR[18]. In the following discussions,
we use SRFBN-L (T=4, G=6) for analysis.

No. Prediction

1st

2nd

3rd

4th

SRFBN-L-FF

SRFBN-L

30.69
31.85

31.74
32.06

32.00
32.11

32.09
32.11

Table 2. The impact of feedback on Set5 with scale factor ×4.

4.3. Feedback vs. feedforward

To investigate the nature of the feedback mechanism in
our network, we compare the feedback network with feed-
forward one in this subsection.

3871

We ﬁrst demonstrate the superiority of the feedback
mechanism over its feedforward counterpart. By simply
disconnecting the loss to all iterations except the last one,
the network is thus impossible to reroute a notion of out-
put to low-level representations and is then degenerated to
a feedforward one (however still retains its recurrent prop-
erty), denoted as SRFBN-L-FF. SRFBN-L and SRFBN-L-
FF both have four iterations, producing four intermediate
output. We then compare the PSNR values of all intermedi-
ate SR images from both networks. The results are shown in
Tab. 2. SRFBN-L outperforms SRFBN-L-FF at every iter-
ation, from which we conclude that the feedback network is
capable of producing high quality early predictions in con-
trast to feedforward network. The experiment also indicates
that our proposed SRFBN does beneﬁt from the feedback
mechanism, instead of only rely on the power of the recur-
rent structure. Except for the above discussions about the
necessity of early losses, we also conduct two more abala-
tive experiments to verify other parts (discussed in Sec. 3)
which form our feedback system. By turning off weights
sharing across iterations, the PSNR value in the proposed
network is decreased from 32.11dB to 31.82dB on Set5 with
scale factor ×4. By disconnecting the LR input at each iter-
ation except the ﬁrst iteration, the PSNR value is decreased
by 0.17dB.

t = 1

t = 2

t = 3

t = 4

image, which further leads to a more accurate residual im-
age. To some extent, this illustration reﬂects the reason why
the feedback network has more powerful early reconstruc-
tion ability than the feedforward one. The second obser-
vation is that the feedback network learns different repre-
sentations in contrast to feedforward one when handling the
same task. In the feedforward network, feature maps vary
signiﬁcantly from the ﬁrst iteration (t=1) to the last itera-
tion (t=4): the edges and contours are outlined at early iter-
ations and then the smooth areas of the original image are
suppressed at latter iterations. The distinct patterns demon-
strate that the feedforward network forms a hierarchy of in-
formation through layers, while the feedback network is al-
lowed to devote most of its efforts to take a self-correcting
process, since it can obtain well-developed feature repre-
sentations at the initial iteration. This further indicates that
F t
out containing high-level information at the t-th iteration
in the feedback network will urge previous layers at subse-
quent iterations to generate better representations.

Model

from scratch

from pretrained

w/o CL with CL w/o CL with CL

BD
DN

29.78
26.92

29.96
26.93

29.98
26.96

30.03
26.98

Table 3. The investigation of curriculum learning (CL) on BD and
DN degradation models with scale factor ×4. The average PSNR
values are evaluated on Set5.

t = 1

t = 2

t = 3

t = 4









$# 
$#

$# 
$#


!


!

d
r
a
w
r
o
f
d
e
e
F

k
c
a
b
d
e
e
F

Figure 5. Average feature maps of feedforward and feedback net-
Figure 5. Average feature maps of feedforward and feedback net-
works.

To dig deeper into the difference between feedback and
feedforward networks, we visualize the average feature map
of every iteration in SRFBN-L and SRFBN-L-FF, illus-
trated in Fig. 5. Each average feature map is the mean
of F t
out in channel dimension, which roughly represents
the output of the feedback block at the t-th iteration. Our
network with global residual skip connections aims at re-
covering the residual image. In other words, the tasks of
our network are to suppress the smooth area of the orig-
inal input image[16] and to predict high-frequency com-
ponents (i.e. edges and contours). From Fig. 5, we have
two observations. First, compared with the feedforward
network at early iterations, feature maps acquired from the
feedback network contain more negative values, showing a
stronger effect of suppressing the smooth area of the input





$#
$
$#
$
0209
0209
##
##
!
$
'$#
'$#

!
$



 


/
 



#

$
!

 


/
 


#

$
!





$#

$#


























:2-0741!,7,209078  



:2-0741!,7,209078  





$#

$#













Figure 6. Performance and number of parameters. Results are
evaluated on Set5 with scale factor ×4. Red points represent our
proposed networks.

4.4. Study of curriculum learning

As mentioned in Sec. 4.1, we now present our results for
two experiments on two different degradation models, i.e.
BD and DN, to show the effectiveness of our curriculum
learning strategy.

We formulate the curriculum based on the recovery difﬁ-
culty. For example, to guide the network to learn recovering
a BD operator corrupted image step by step, we provide a
Gaussian blurred HR image as (intermediate) ground truth

3872

so that the network only needs to learn the inversion of a sin-
gle downsampling operator at early iterations. Original HR
image is provided at latter iterations as a senior challenge.
Speciﬁcally, we empirically provide blurred HR images at
ﬁrst two iterations and original HR images at remaining two
iterations for experiments with the BD degradation model.
For experiments with the DN degradation model, we instead
use noisy HR images at ﬁrst two iterations and HR images
without noise at last two iterations.

We also examine the compatibility of this strategy with
two common training processes, i.e. training from scratch
and ﬁne-tuning on a network pretrained on the BI degrada-
tion model. The results shown in Tab. 3 infer that the cur-
riculum learning strategy well assists our proposed SRFBN
in handling BD and DN degradation models under both
circumstances. We also observe that ﬁne-tuning on a net-
work pretrained on the BI degradation model leads to higher
PSNR values than training from scratch.

HR

Bicubic

VDSR

DRRN

 img_092  from Urban100

MemNet

EDSR

D-DBPN

SRFBN(Ours)

HR

Bicubic

VDSR

DRRN

 BokuHaSitatakaKun  from Manga109

MemNet

EDSR

D-DBPN

SRFBN(Ours)

Figure 7. Visual results of BI degradation model with scale factor
×4.

4.5. Comparison with the state of the arts

The SRFBN with a larger base number of ﬁlters (m=64),
which is derived from the SRFBN-L, is implemented for
comparison. A self-ensemble method[35] is also used to
further improve the performance of the SRFBN (denoted as
SRFBN+). A lightweight network SRFBN-S (T=4, G=3,
m=32) is provided to compare with the state-of-the-art
methods, which are carried only few parameters.

4.5.1 Network parameters

The state-of-the-art methods considered in this experiment
include SRCNN[7], VDSR[18], DRRN[31], MemNet[36],
EDSR[23], DBPN-S[11] and D-DBPN[11]. The com-
parison results are given in Fig. 6 in terms of the net-
work parameters and the reconstruction effects (PSNR).

The SRFBN-S can achieve the best SR results among the
networks with parameters fewer than 1000K. This demon-
strates our method can well balance the number of pa-
rameters and the reconstruction performance. Meanwhile,
in comparison with the networks with a large number of
parameters, such as D-DBPN and EDSR, our proposed
SRFBN and SRFBN+ can achieve competitive results,
while only needs the 35% and 8% parameters of D-DBPN
and EDSR, respectively. Thus, our network is lightweight
and more efﬁcient in comparison with other state-of-the-art
methods.

4.5.2 Results with BI degradation model

For BI degradation model, we compare the SRFBN and
SRFBN+ with seven state-of-the-art
image SR meth-
ods: SRCNN[7], VDSR[18], DRRN[31], SRDenseNet[36],
MemNet[36], EDSR[23], D-DBPN[11]. The quantitative
results in Tab. 4 are re-evaluated from the corresponding
public codes. Obviously, our proposed SRFBN can out-
perform almost all comparative methods. Compared with
our method, EDSR utilizes much more number of ﬁlters
(256 vs. 64), and D-DBPN employs more training im-
ages (DIV2K+Flickr2K+ImageNet vs. DIV2K+Flickr2K).
However, our SRFBN can earn competitive results in con-
trast to them.
In addition, it also can be seen that our
SRFBN+ outperforms almost all comparative methods.

We show SR results with scale factor ×4 in Fig. 7. In
general, the proposed SRFBN can yield more convincing
results. For the SR results of the ‘BokuHaSitatakaKun’ im-
age from Manga109, DRRN and MemNet even split the ‘M’
letter. VDSR, EDSR and D-DBPN fail to recover the clear
image. The proposed SRFBN produces a clear image which
is very close to the ground truth. Besides, for the ‘img 092’
from Urban100, the texture direction of the SR images from
all comparative methods is wrong. However, our proposed
SRFBN makes full use of the high-level information to take
a self-correcting process, thus a more faithful SR image can
be obtained.

4.5.3 Results with BD and DN degradation models

As aforementioned, the proposed SRFBN is trained us-
ing curriculum learning strategy for BD and DN degrada-
tion models, and ﬁne-tuned based on BI degradation model
using DIV2K. The proposed SRFBN and SRFBN+ are
compared with SRCNN[7], VDSR[18], IRCNN G[43], IR-
CNN C[43], SRMD(NF)[44], and RDN[47]. Because of
degradation mismatch, SRCNN and VDSR are re-trained
for BD and DN degradation models. As shown in Tab. 5,
The proposed SRFBN and SRFBN+ achieve the best on al-
most all quantative results over other state-of-the-art meth-
ods.

3873

Dataset

Scale

Bicubic

Set5

Set14

B100

Urban100

Manga109

×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

33.66/0.9299
30.39/0.8682
28.42/0.8104

30.24/0.8688
27.55/0.7742
26.00/0.7027

29.56/0.8431
27.21/0.7385
25.96/0.6675

26.88/0.8403
24.46/0.7349
23.14/0.6577

30.30/0.9339
26.95/0.8556
24.89/0.7866

SRCNN

[7]

36.66/0.9542
32.75/0.9090
30.48/0.8628

32.45/0.9067
29.30/0.8215
27.50/0.7513

31.36/0.8879
28.41/0.7863
26.90/0.7101

29.50/0.8946
26.24/0.7989
24.52/0.7221

35.60/0.9663
30.48/0.9117
27.58/0.8555

VDSR

[18]

DRRN

[31]

37.53/0.9590
33.67/0.9210
31.35/0.8830

33.05/0.9130
29.78/0.8320
28.02/0.7680

31.90/0.8960
28.83/0.7990
27.29/0.7260

30.77/0.9140
27.14/0.8290
25.18/0.7540

37.22/0.9750
32.01/0.9340
28.83/0.8870

37.74/0.9591
34.03/0.9244
31.68/0.8888

33.23/0.9136
29.96/0.8349
28.21/0.7721

32.05/0.8973
28.95/0.8004
27.38/0.7284

31.23/0.9188
27.53/0.8378
25.44/0.7638

37.60/0.9736
32.42/0.9359
29.18/0.8914

MemNet

[32]

37.78/0.9597
34.09/0.9248
31.74/0.8893

33.28/0.9142
30.00/0.8350
28.26/0.7723

32.08/0.8978
28.96/0.8001
27.40/0.7281

31.31/0.9195
27.56/0.8376
25.50//0.7630

37.72/0.9740
32.51/0.9369
29.42/0.8942

SRFBN-S

(Ours)

37.78/0.9597
34.20/0.9255
31.98/0.8923

33.35/0.9156
30.10/0.8372
28.45/0.7779

32.00/0.8970
28.96/0.8010
27.44/0.7313

31.41/0.9207
27.66/0.8415
25.71/0.7719

38.06/0.9757
33.02/0.9404
29.91/0.9008

EDSR
[23]

38.11/0.9602
34.65/0.9280
32.46/0.8968

33.92/0.9195
30.52/0.8462
28.80/0.7876

32.32/0.9013
29.25/0.8093
27.71/0.7420

32.93/0.9351
28.80/0.8653
26.64/0.8033

39.10/0.9773
34.17/0.9476
31.02/0.9148

D-DBPN

[11]

38.09/0.9600

-/-

32.47/0.8980

33.85/0.9190

-/-

28.82/0.7860

32.27/0.9000

-/-

27.72/0.7400

32.55/0.9324

-/-

26.38/0.7946

38.89/0.9775

-/-

30.91/0.9137

SRFBN
(Ours)

38.11/0.9609
34.70/0.9292
32.47/0.8983

33.82/0.9196
30.51/0.8461
28.81/0.7868

32.29/0.9010
29.24/0.8084
27.72/0.7409

32.62/0.9328
28.73/0.8641
26.60/0.8015

39.08/0.9779
34.18/0.9481
31.15/0.9160

SRFBN+

(Ours)

38.18/0.9611
34.77/0.9297
32.56/0.8992

33.90/0.9203
30.61/0.8473
28.87/0.7881

32.34/0.9015
29.29/0.8093
27.77/0.7419

32.80/0.9341
28.89/0.8664
26.73/0.8043

39.28/0.9784
34.44/0.9494
31.40/0.9182

Table 4. Average PSNR/SSIM values for scale factors ×2, ×3 and ×4 with BI degradation model. The best performance is shown in red
and the second best performance is shown in blue.

Dataset

Model

Bicubic

SRCNN

[7]

VDSR

[18]

IRCNN G

IRCNN C

SRMD(NF)

[43]

[43]

[44]

RDN
[47]

SRFBN
(Ours)

SRFBN+

(Ours)

Set5

Set14

B100

Urban100

Manga109

BD
DN

BD
DN

BD
DN

BD
DN

BD
DN

28.34/0.8161
24.14/0.5445

31.63/0.8888
27.16/0.7672

33.30/0.9159
27.72/0.7872

33.38/0.9182
24.85/0.7205

29.55/0.8246
26.18/0.7430

34.09/0.9242
27.74/0.8026

34.57/0.9280
28.46/0.8151

34.66/0.9283
28.53/0.8182

34.77/0.9290
28.59/0.8198

26.12/0.7106
23.14/0.4828

28.52/0.7924
25.49/0.6580

29.67/0.8269
25.92/0.6786

29.73/0.8292
23.84/0.6091

27.33/0.7135
24.68/0.6300

30.11/0.8364
26.13/0.6974

30.53/0.8447
26.60/0.7101

30.48/0.8439
26.60/0.7144

30.64/0.8458
26.67/0.7159

26.02/0.6733
22.94/0.4461

27.76/0.7526
25.11/0.6151

28.63/0.7903
25.52/0.6345

28.65/0.7922
23.89/0.5688

26.46/0.6572
24.52/0.5850

28.98/0.8009
25.64/0.6495

29.23/0.8079
25.93/0.6573

29.21/0.8069
25.95/0.6625

29.28/0.8080
25.99/0.6636

23.20/0.6661
21.63/0.4701

25.31/0.7612
23.32/0.6500

26.75/0.8145
23.83/0.6797

26.77/0.8154
21.96/0.6018

24.89/0.7172
22.63/0.6205

27.50/0.8370
24.28/0.7092

28.46/0.8581
24.92/0.7362

28.48/0.8581
24.99/0.7424

28.68/0.8613
25.10/0.7458

25.03/0.7987
23.08/0.5448

28.79/0.8851
25.78/0.7889

31.66/0.9260
26.41/0.8130

31.15/0.9245
23.18/0.7466

28.68/0.8574
24.74/0.7701

32.97/0.9391
26.72/0.8424

33.97/0.9465
28.00/0.8590

34.07/0.9466
28.02/0.8618

34.43/0.9483
28.17/0.8643

Table 5. Average PSNR/SSIM values for scale factor ×3 with BD and DN degradation models. The best performance is shown in red and
the second best performance is shown in blue.

more accurate details in SR images. From above compar-
isions, we further indicate the robustness and effectiveness
of SRFBN in handling BD and DN degradation models.

HR

Bicubic

SRCNN

VDSR

5. Conclusion

 butterfly from Set5

IRCNN_G

SRMDNF

RDN

SRFBN (Ours)

HR

Bicubic

SRCNN

VDSR

 img_044 from Urban100

IRCNN_C

SRMD

RDN

SRFBN (Ours)

Figure 8. Visual results of BD and DN degradation models with
scale factor ×3. The ﬁrst set of images shows the results obtained
from BD degradation model. The second set of images shows the
results from DN degradation model.

In Fig. 8, we also show two sets of visual results with
BD and DN degradation models from the standard bench-
mark datasets. Compared with other methods, the pro-
posed SRFBN could alleviate the distortions and generate

In this paper, we propose a novel network for image
SR called super-resolution feedback network (SRFBN) to
faithfully reconstruct a SR image by enhancing low-level
representations with high-level ones. The feedback block
(FB) in the network can effectively handle the feedback
information ﬂow as well as the feature reuse. In addition,
a curriculum learning strategy is proposed to enable the
network to well suitable for more complicated tasks, where
the low-resolution images are corrupted by complex degra-
dation models. The comprehensive experimental results
have demonstrated that the proposed SRFBN could deliver
the comparative or better performance in comparison with
the state-of-the-art methods by using very fewer parameters.

Acknowledgement. The research in our paper is spon-
sored by National Natural Science Foundation of China
(No.61701327 and No.61711540303), Science Founda-
tion of Sichuan Science and Technology Department
(No.2018GZ0178).

3874

References

[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge
In

on single image super-resolution: Dataset and study.
CVPRW, 2017.

[2] Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Ja-

son Weston. Curriculum learning. In ICML, 2009.

[3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and
Marie-Line Alberi-Morel. Low-complexity single-image
super-resolution based on nonnegative neighbor embedding.
In BMVC, 2012.

[4] Chunshui Cao, Xianming Liu, Yi Yang, Yinan Yu, Jiang
Wang, Zilei Wang, Yongzhen Huang, Liang Wang, Chang
Huang, Wei Xu, Deva Ramanan, and Thomas S. Huang.
Look and think twice: Capturing top-down visual atten-
tion with feedback convolutional neural networks. In ICCV,
2015.

[5] Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Ji-
tendra Malik. Human pose estimation with iterative error
feedback. In CVPR, 2015.

[6] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Learning a deep convolutional network for image
super-resolution. In ECCV, 2014.

[7] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. TPAMI, 2016.

[8] Ruohan Gao and Kristen Grauman. On-demand learning for

deep image restoration. In ICCV, 2017.

[9] Charles D Gilbert and Mariano Sigman. Brain states: top-

down inﬂuences in sensory processing. Neuron, 2007.

[10] Wei Han, Shiyu Chang, Ding Liu, Mo Yu, Michael Witbrock,
and Thomas S. Huang. Image super-resolution via dual-state
recurrent networks. In CVPR, 2018.

[11] Muhammad Haris, Gregory Shakhnarovich, and Norimichi
Ukita. Deep back-projection networks for super-resolution.
In CVPR, 2018.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation. In ICCV, 2015.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[14] Gao Huang, Zhuang Liu, Van Der Maaten Laurens, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In CVPR, 2016.

[15] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single
image super-resolution from transformed self-exemplars. In
CVPR, 2015.

[16] Zheng Hui, Xiumei Wang, and Xinbo Gao. Fast and accu-
rate single image super-resolution via information distilla-
tion network. In CVPR, 2018.

[17] J. M. Hup´e, A. C. James, B. R. Payne, S. G. Lomber, P Gi-
rard, and J Bullier. Cortical feedback improves discrimina-
tion between ﬁgure and background by v1, v2 and v3 neu-
rons. Nature, 1998.

[18] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In CVPR, 2016.

[19] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-
recursive convolutional network for image super-resolution.
In CVPR, 2016.

[20] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2014.

[21] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P. Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a
generative adversarial network. In CVPR, 2017.

[22] Qianli Liao and Tomaso Poggio. Bridging the gaps between
residual learning, recurrent neural networks and visual cor-
tex. arXiv preprint arXiv:1604.03640, 2016.

[23] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for single
image super-resolution. In CVPRW, 2017.

[24] David R. Martin, Charless C. Fowlkes, Doron Tal, and Jiten-
dra Malik. A database of human segmented natural images
and its application to evaluating segmentation algorithms and
measuring ecological statistics. In ICCV, 2001.

[25] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto,
Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa.
Sketch-based manga retrieval using manga109 dataset. Mul-
timedia Tools and Applications, 2017.

[26] Tomer Peleg and Michael Elad. A statistical prediction
model based on sparse representations for single image
super-resolution. TIP, 2014.

[27] Anastasia Pentina, Viktoriia Sharmanska, and Christoph H.
Lampert. Curriculum learning of multiple tasks. In CVPR,
2014.

[28] Deepak Babu Sam and R. Venkatesh Babu. Top-down feed-
In

back for crowd counting convolutional neural network.
AAAI, 2018.

[29] Samuel Schulter, Christian Leistner, and Horst Bischof. Fast
and accurate image upscaling with super-resolution forests.
In CVPR, 2015.

[30] Marijn F Stollenga, Jonathan Masci, Faustino Gomez, and
J¨urgen Schmidhuber. Deep networks with internal selective
attention through feedback connections. In NIPS. 2014.

[31] Ying Tai, Jian Yang, and Xiaoming Liu.

resolution via deep recursive residual network.
2017.

Image super-
In CVPR,

[32] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem-
net: A persistent memory network for image restoration. In
ICCV, 2017.

[33] Radu Timofte, Vincent De Smet, and Luc Van Gool.
Anchored neighborhood regression for fast example-based
super-resolution. In ICCV, 2013.

[34] Radu Timofte, Vincent De Smet, and Luc Van Gool. A+:
Adjusted anchored neighborhood regression for fast super-
resolution. In ACCV, 2015.

[35] Radu Timofte, Rasmus Rothe, and Luc Van Gool. Seven
ways to improve example-based single image super resolu-
tion. In CVPR, 2016.

[36] Tong Tong, Gen Li, Xiejie Liu, and Qinquan Gao.

Image
In ICCV,

super-resolution using dense skip connections.
2017.

3875

[37] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-
hanced super-resolution generative adversarial networks. In
ECCVW, 2018.

[38] Yifan Wang, Federico Perazzi, Brian Mcwilliams, Alexan-
der Sorkinehornung, Olga Sorkinehornung, and Christopher
Schroers. A fully progressive approach to single-image
super-resolution. In CVPRW, 2018.

[39] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.
Simoncelli. Image quality assessment: from error visibility
to structural similarity. TIP, 2004.

[40] Amir R. Zamir, Te-Lin Wu, Lin Sun, William B. Shen,
Bertram E. Shi, Jitendra Malik, and Silvio Savarese. Feed-
back networks. In CVPR, 2017.

[41] Roman Zeyde, Michael Elad, and Matan Protter. On single
image scale-up using sparse-representations. In Curves and
Surfaces, 2010.

[42] Kaibing Zhang, Xinbo Gao, Dacheng Tao, Xuelong Li, et al.
Single image super-resolution with non-local means and
steering kernel regression. TIP, 2012.

[43] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.
Learning deep CNN denoiser prior for image restoration. In
CVPR, 2017.

[44] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a
single convolutional super-resolution network for multiple
degradations. In CVPR, 2017.

[45] Lei Zhang and Xiaolin Wu. An edge-guided image interpo-
lation algorithm via directional ﬁltering and data fusion. TIP,
2006.

[46] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng
Zhong, and Yun Fu. Image super-resolution using very deep
residual channel attention networks. In ECCV, 2018.

[47] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image super-resolution.
In CVPR, 2018.

3876

