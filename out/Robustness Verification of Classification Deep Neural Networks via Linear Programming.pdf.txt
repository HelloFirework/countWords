Robustness Veriﬁcation of Classiﬁcation Deep Neural Networks via Linear

Programming

Wang Lin1

2

,

,

3 Zhengfeng Yang2 ∗ Xin Chen3∗ Qingye Zhao3 Xiangkun Li2 Zhiming Liu4 Jifeng He2
1 School of Information Science and Technology, Zhejiang Sci-Tech University
2 Shanghai Key Lab of Trustworthy Computing, East China Normal University

3 State Key Laboratory for Novel Software Technology, Nanjing University

4 Center for Research and Innovation in Software Engineering, Southwest University

linwang@zstu.edu.cn, zfyang@sei.ecnu.edu.cn, chenxin@nju.edu.cn

qingyezhao@foxmail.com, xkli@stu.ecnu.cn, zhimingliu88@swu.edu.cn, jifeng@sei.ecnu.edu.cn

Abstract

There is a pressing need to verify robustness of classiﬁ-
cation deep neural networks (CDNNs) as they are embed-
ded in many safety-critical applications. Existing robust-
ness veriﬁcation approaches rely on computing the over-
approximation of the output set, and can hardly scale up to
practical CDNNs, as the result of error accumulation ac-
companied with approximation. In this paper, we develop a
novel method for robustness veriﬁcation of CDNNs with sig-
moid activation functions. It converts the robustness veriﬁ-
cation problem into an equivalent problem of inspecting the
most suspected point in the input region which constitutes
a nonlinear optimization problem. To make it amenable, by
relaxing the nonlinear constraints into the linear inclusion-
s, it is further reﬁned as a linear programming problem. We
conduct comparison experiments on a few CDNNs trained
for classifying images in some state-of-the-art benchmark-
s, showing our advantages of precision and scalability that
enable effective veriﬁcation of practical CDNNs.

1. Introduction

Classiﬁcation deep neural networks (CDNNs) are a spe-
cial kind of deep neural networks that take in complicat-
ed, high-dimensional input, transform it through multiple
layers of neurons and ﬁnally identify it as a speciﬁc out-
put label or class. Acting as classiﬁers, they have been

∗Corresponding authors. This work was supported in part by the Na-
tional Key Research and Development Program under Grant 2017YF-
B1001801, in part by the National Natural Science Foundation of Chi-
na under Grant 61772203, 61602348, 61690204, 61632015, 61672435,
61732019, 61751210 and 61572441, in part by the Shanghai Natural Sci-
ence Foundation under Grant 17ZR1408300, in part by the Collaborative
Innovation Center of Novel Software Technology and Industrialization,
and in part by Southwest University under Grant SU116007.

adopted in a variety of applications, such as adaptive con-
trol [31, 34], pattern recognition [28], image understand-
ing [8, 18], cyber-security [29], speech and audio analysis
[14, 15] and self-driving [26]. It is a growing trend that em-
bedding CDNNs in safety-critical systems to make judge-
ments like human experts [19].

Unfortunately, it has been reported by many researchers
[3, 32] that CDNNs are unrobust with respect to perturba-
tions. For example, applying minimal changes to the input
image, being imperceptible to the human eye, can make the
well-trained CDNN misclassify it.

The unrobust issue obviously raises potential safety is-
sues for safety critical systems using neural networks and
calls for effective veriﬁcation techniques that can check the
robustness of CDNNs [11, 17]. We state that a CDNN is ro-
bust when, given every possible input value within a speci-
ﬁed region, its output is guaranteed to have the same classi-
ﬁcation label, which corresponds to the principal robust re-
quirement of classiﬁcation: minimal perturbations (restrict-
ed by a threshold) should not affect the classiﬁcation result.

Given a CDNN, its robustness can be veriﬁed by estimat-
ing the range of the output set with respect to a given input
region. Some methods are proposed to compute the out-
put range of CDNNs of particular types [2, 6, 12, 22]. The
method combining local gradient descent search with global
mixed integer linear programming can handle the CDNNs
with ReLU activation functions [9], which is further ex-
tended to treat a special class of perturbations [33]. The
ployhedra manipulating based method is able to compute
the exact output region of CDNNs with ReLU activation
functions and an union of ployhedra as its input set [35].

In general, the computation of output set is very difﬁcult
since the theories to handle general activation functions are
undecidable. The methods that transform the veriﬁcation
problem of CDNNs into programming receive most atten-

111418

tions in recent years. The method extending the Simplex
method to deal with the constraints imposed by ReLU nodes
is suggested to verify the ReLU equipped DNNs [17]. The
approach integrating SAT solving with linear programming
and linear approximation can handle CDNNs with piece-
wise linear activation functions [10].

The piece-wise linear feature of ReLU make the veriﬁ-
cation of ReLU equipped CDNNs much easier [5, 25]. For
those nonlinear activation functions, such as sigmoid, ap-
proximation based methods are studied. The abstraction-
reﬁnement method introduces piece-wise linear functions to
approximate sigmoid activation functions, encodes the net-
work as a set of linear constraints and then solves them by
the solver HYSAT [24]. The discretization based method
uses layer-by-layer analysis and SMT techniques to verify
CDNNs for image classiﬁcation [16]. A simulation-based
approach formulates the output range estimation problem
into the maximal sensitivity, which can be computed via
solving a chain of convex optimization problems [36].

As the error arising from approximation propagates a-
long the network layer by layer, those approximation-based
approaches are prone to fail when encountering practical
CDNNs. How to provide adequate precision and scalability
to satisfy the requirement of verifying practical CDNNs is
the key challenge that veriﬁcation methods must face with.
This paper proposes a novel method to robustness ver-
iﬁcation of CDNNs equipped with the activation function:
sigmoid. Rather than computing the over-approximation of
the output set with respect to the given input region, it trans-
fers the robustness veriﬁcation problem into the problem of
ﬁnding the point in the input region that is easier to be as-
signed with a different label than any other points in the
region and checks whether or not the point will be misclas-
siﬁed. To be speciﬁc, we build an equivalent optimization
problem where the relation between the neurons in the net-
works is encoded as constraints and the objective function
aims to ﬁnd the minimal difference between the output val-
ues with respect to the desired label and the other labels. As
the desired label should have the maximal value, the opti-
mum of the optimization problem is positive if and only if
the robustness property holds.

The optimization problem is further relaxed by replac-
ing the nonlinear sigmoid function in constraints with its
linear inclusions, so that linear programming (LP) solvers
can work. Precisely, the sigmoid function is contained in
a closed polyhedron produced by combining bound deriva-
tion with linear inclusion. The derived optimization prob-
lem constitutes a sufﬁcient condition for veriﬁcation as the
feasible set of its constraints is a superset of the original
one.

The proposed method has two advantages over existing
ones: 1. It veriﬁes robustness by inspecting whether or not
the worst point in the input region will be classiﬁed dif-

ferently, and thus provides more precise estimation on ro-
bustness of CDNNs. 2. It can safely scale up to realistic
sized networks which beyond the reach of existing ones. It
is supported by the comparison experiments conducted on
CDNNs generated from many state-of art benchmarks.

The main contributions of the paper are summarized as
follows: 1. We propose a novel method for verifying ro-
bustness of CDNNs which converts the robustness veriﬁca-
tion problem into an equivalent optimization problem. 2.
We suggest a new computational approach, by approximat-
ing the activation function sigmoid with a tightly enclosed
region, and then leveraging LP solvers to solve it. 3. We
conduct a comparison experiment on a set of benchmark-
s consisting of many state-of-the-art CDNNs, which shows
our advantages on precision and scalability that enable ver-
iﬁcation of practical CDNNs.

The paper is organized as follows. Section 2 introduces
some notions for CDNNs, and then describes the robust-
ness veriﬁcation problem.
In Section 3, the problem of
robustness veriﬁcation is transformed to a nonlinear opti-
mization problem and an LP based computational method
is addressed to deal with it. We show the experiments on
benchmarks, conﬁrming the effectiveness of our algorithm
in Section 4 before concluding in Section 5.

2. Preliminaries

Notations Let N and R be the set of natural numbers
and the ﬁeld of real number, respectively; ̥n denotes the
set consisting of the positive integer numbers not greater
than n, i.e., ̥n = {1, 2, . . . , n}, where n is a positive inte-
ger; I denotes the open interval (0, 1), and I s denotes the
Cartesian product I s = I ×· · ·×I with the positive integer
number s.

2.1. Deep Neural Networks

Deep neural networks (DNNs) consist of an input lay-
er, an output layer, and multiple hidden layers in between.
Neurons (so-called nodes), in a DNN are arranged in dis-
joint layers, with each neuron in one layer connected to the
next layer, but no connection between neurons in the same
layer. Furthermore, the output of each neuron in the hid-
den layer is assigned by a linear combination of the neuron
outputs of the previous layer, and then applying a non-linear
activation function. Formally, DNNs are deﬁned as follows.

Deﬁnition 1 (Deep Neural Network) A deep neural net-
work N is a tuple ⟨L, X, W, B, Φ⟩, where

• L = {L[0] . . . , L[n]} is a set of layers, where layer
L[0] is the input layer, L[n] is the output layer, and
L[1], . . . , L[n−1] are the hidden layers. Each layer
L[k], 0 ≤ k ≤ n is associated with an sk-dimensional
vector space Ψk ⊆ Rsk , in which each dimension cor-
responds to a neuron.

211419

• X = {x[0], . . . , x[n]}, where x[k] is the vector corre-
sponding to the values of the neurons in the layer L[k]
for 0 ≤ k ≤ n.

• W = {W [1], . . . , W [n]} is the set of weight matri-
ces. Each non-input layer L[k] with 1 ≤ k ≤ n has
a weight matrix W [k] ∈ Rsk×sk−1 , and neurons in
L[k] are connected to neurons from the preceding layer
L[k−1] by the weight matrix W [k].

• B = {b[1], . . . , b[n]} is the set of bias vectors. For
each non-input layer L[k] with 1 ≤ k ≤ n, the bias
vector b[k] ∈ Rsk is used to assigned bias values to
the neurons in L[k].

• Φ = {ϕ[1], . . . , ϕ[n]} is a set of activation function-
s ϕ[k] : Ψk−1 → Ψk, one for each non-input layer
L[k] with 1 ≤ k ≤ n. Furthermore, the value vector
x[k] for the neurons in L[k] are determined by the val-
ue vector of x[k−1] with the weight matrix W [k], the
bias vector b[k] and the activation function ϕ[k], i.e.,
x[k] = ϕ[k](W [k] x[k−1] + b[k]) with the activation
function ϕ[k] being applied element-wise.

Given a DNN N : ⟨L, X, W, B, Φ⟩, it is seen that the
output of the k − 1-th layer is the input of the k-th layer.
From the mapping point of view, each non-input layer L[k],
1 ≤ k ≤ n, deﬁnes a function fk : Rsk−1 → Rsk , with

fk(x) = ϕ[k](W [k]x + b[k]).

(1)

Therefore, the behavior of N is deﬁned by the composition
function f : Rs0 → Rsn , which is deﬁned as
f (x) = fn(fn−1(· · · (f1(x)))).

(2)

For the purpose of this paper we are assuming that all the
weights matrices W and bias vectors B in N have ﬁxed val-
ues. For an overview of weight and bias selection and a re-
view of previous work, see [13]. Observing the composition
of the given DNN, the difﬁculty in proving its properties is
caused by the presence of activation functions. The activa-
tion function is generally a nonlinear function connecting
the neuron values of the preceding layer to the ones of the
following layer. There are some typical activation function-
s, such as the sigmoid, rectiﬁed linear unit (ReLU), and tanh
functions[13]. In this paper, we only focus on DNNs with
the widely used sigmoid activation function

ϕ(x) =

1

1 + e−x

.

(3)

to vectors x as ϕ(x): (ϕ(x1), · · · , ϕ(xn))T

We extend the deﬁnition of ϕ(x) to apply component-wise
. Due to the
property of the sigmoid function with ϕ(x) : R → I, the
input and the output of the function fk for the given input
x[0] ∈ Rs0 , can be restricted to

According to the Universal Approximation Theo-
rem [27], it guarantees that, in principle, a DNN N is able
to approximate any nonlinear real-valued function. Despite
the impressive ability of approximating nonlinear function-
s, much complexities represent in predicting the output be-
haviors of N due to the nonlinearity and non-convexity of
DNNs.

2.2. Classiﬁcation Deep Neural Networks and Ro-

bustness Property

In this subsection, we will introduce the notion of clas-
siﬁcation deep neural networks (CDNNs), and describe
the robustness veriﬁcation problem of CDNNs. CDNN is
one of the primary applications in deep neural network-
s.
In this type of networks, a CDNN will act as a clas-
siﬁer, which is used to specify which of labels some in-
put belongs to. Given a CDNN N : ⟨L, X, W, B, Φ⟩ with
an input x[0], the activation for x[k] in layer k is x[k] =
fk(fk−1(· · · (f1(x[0])))), the activation for x[n] in the out-
put layer can be propagated through the layers shown in (2),
that is, x[n] = f (x[0]) ∈ I sn .

The classiﬁcation decision is asked to produce a function
ϱ : Rs0 → ̥sn based on the output value x[n], assigning to
x[0] to label ℓ ∈ ̥sn by selecting the index corresponding
to the largest element in x[n]. Therefore, the function ϱ is
the composition of arg max and f , i.e.,

ϱ(x[0]) = arg max(f (x[0])) = arg max
1≤ℓ≤sn

(x[n]).

(4)

To make the input x[0] be classiﬁed, the largest element of
the output x[n] must be unique. In other words, ℓ = ϱ(x[0])
implies that x

for all ˜ℓ ̸= ℓ.

[n]
ℓ > x

[n]
˜ℓ

Given a CDNN N , we say that two inputs x[0] and y[0]
have the same label if ϱ(x[0]) = ϱ(y[0]). This property
for having the same label can also be generalized from the
discrete inputs to the input region. More speciﬁcally, we say
that the input region Θ has the same label if two arbitrary
inputs x[0] and y[0] chosen from Θ, have the same label,
namely,

ϱ(x[0]) = ϱ(y[0]),

∀x[0], y[0] ∈ Θ.

Similar to the description in [16], this property that the giv-
en input region having the same label is called as robust-
ness. In this situation, robustness speciﬁcation of the given
CDNN N is described over the speciﬁed input region Θ.
We begin with a common deﬁnition for robustness.

Deﬁnition 2 (Robustness) Given a classiﬁcation deep
neural network N with an input region Θ, the robustness
property holds if and only if all inputs within the input re-
gion Θ have the same label, i.e.,

x[k] = fk(x[k−1]) ∈ I sk ,

1 ≤ k ≤ n.

∀x[0], y[0] ∈ Θ =⇒ ϱ(x[0]) = ϱ(y[0]).

(5)

311420

Namely, if there exists an index ℓ, 1 ≤ ℓ ≤ sn, such that

ϱ(x[0]) = ℓ,

∀ x[0] ∈ Θ

(6)

is satisﬁed, we say that N with respect to Θ is robust.

Given a CDNN N , the problem of robustness veriﬁcation is
to decide whether N with respect to the input region Θ is
robust. According to Deﬁnition 2, the target for robustness
veriﬁcation is to determine whether there exists an uniﬁed
label (index) ℓ such that any output x[n] produced by the
input selected from Θ satisﬁes

x

[n]
ℓ > x

[n]
˜ℓ

for all ˜ℓ ̸= ℓ.

(7)

On the contrary, we say that N is unrobust if there exist two
inputs x[0, y[0] ∈ Θ such that

arg max(f (x[0])) ̸= arg max(f (y[0]))

is satisﬁed. Hereafter we will always consider the input re-
gion Θ is a Cartesian product, that is, Θ = Ω1 × · · · × Ωs0
where Ωi = [ai, bi] is a closed interval bounded by ai, bi ∈
R for all 1 ≤ i ≤ s0.

3. Robustness Veriﬁcation of Classiﬁcation

Deep Neural Networks

Instead of computing the over-approximation of the out-
put set, in this work we propose a direct method for attack-
ing the robustness veriﬁcation problem. Given a CDNN
with the input region, we establish a nonlinear optimiza-
tion problem equivalent to the original robustness veriﬁca-
tion problem. Namely, a sufﬁcient and necessary condition
for robustness veriﬁcation is to inspect the most suspicious
point in the input region is robust or not. (see Section 3.1).
To alleviate the computational intractability for the practi-
cal CDNNs, a surrogate is given to relax the derived non-
linear optimization as a linear programming problem (see
Section 3.2). Moreover, the positivity of the optimum for
the relaxed LP problem sufﬁces to guarantee the robustness
property of the given network (see Section 3.3).

3.1. From Robustness Veriﬁcation to Nonlinear Op-

timization

Given a CDNN N : ⟨L, X, W, B, Φ⟩ with an input re-
gion Θ, recall that the robustness satisfaction is to veri-
fy whether the existence of the identical label ℓ such that
each output holds the condition (7). As a result, ℓ is eas-
ily obtained by checking an arbitrary output, i.e., ℓ =
arg max f (x[0]), where x[0] is a random input selected from
Θ. Depending on the label ℓ, robustness veriﬁcation by
checking the condition (7) is equivalent to verify

min{x

[n]
ℓ − x

[n]
˜ℓ

, ˜ℓ ̸= ℓ} > 0

(8)

is satisﬁed for each output x[n]. In other words, N with Θ
is robust if and only if

min
x[n]

{ min{x

[n]
ℓ − x

[n]
˜ℓ

, ˜ℓ ̸= ℓ} } > 0.

(9)

From (9), one can construct the following nonlinear op-
timization problem whose objective is a linear piece-wise
function:

p∗ = minx[k],z[k] { x[n]
s.t. ai ≤ x[0]

i ≤ bi

ℓ − x[n]
˜ℓ

, ˜ℓ ̸= ℓ }

z[k] = W [k] x[k−1] + b[k]
x[k] = ϕ(z[k])

i = 1, · · · , s0,
k = 1, · · · , n,
k = 1, · · · , n.

(10)




The following theorem shows the equivalent transformation
between robustness veriﬁcation and nonlinear optimization
solving.

Theorem 1 Let N be a classiﬁcation deep neural network,
and Θ be the given input region. Suppose y[0] is selected
randomly from Θ, and ℓ is the label classiﬁed by N with
y[0], i.e., ℓ = ϱ(y[0]). Suppose p∗ is the optimum of the
optimization problem (10), established by N , Θ and ℓ. Then
N with respect to Θ is robust if and only if p∗ > 0.

Proof 1 Observe that the constraints of the optimization
problem (10) are the equivalent expression of N : x[n] =
f (x[0]). Thus, taking the equivalence between (7) and (9)
(cid:3)
yields the desired result.

In fact, the optimization problem (10) tries to ﬁnd the
point that are most likely to be unrobust in the input region.
If the worst point is robust, all the other points should be
robust too.

3.2. Linear Encoding for Deep Neural Networks

The subsection considers how to encode the behavior in
CDNNs in terms of linear constraints so that highly scal-
able LP solvers can contribute to verify CDNNs of practical
size. The encoding is based on the input-output behavior
of every neuron in the network, and the main challenge is
to handle the non-linearities, which are arising from the ac-
tivation functions. To facilitate such linear relaxation, we
start by computing the lower and upper bounds of all neu-
rons via layer-by-layer estimation, precedes linear inclusion
for activation functions within the yielded bounds.
Bound Derivation with Interval Analysis Without loss of
generality, we consider a single layer L[k]. As shown in
Section 2, it is known that the values of neurons, x[k] in
L[k] can be computed by

x[k] = ϕ(W [k]x[k−1] + b[k]),

(11)

where ϕ is applied element-wise. By introducing the aux-
iliary variables z[k], the evaluation (11) can be rewritten as
the composition of a linear mapping

z[k] = W [k] x[k−1] + b[k],

(12)

411421

and a nonlinear mapping

x[k] = ϕ(z[k]).

(13)

Let D[0] = Θ be the interval vector representing the
bounds of the input neurons, and let D[k], 1 ≤ k ≤ n be
the interval vector representing the bounds of the k-th layer
neurons. The bounds of all neurons can be obtained by per-
forming the procedure of the layer-by-layer analysis. More
speciﬁcally, suppose D[k−1] is yielded from the previous
layer, using the interval computation to (12) yields the range
[k]
j , denoted by [αk,j, βk,j] for each j = 1, · · · , sk. S-
of z
ince ϕ is a monotonically increasing function, the i-th ele-
ment of x[k] can be bounded by ϕ(αk,i) ≤ x[k]
i ≤ ϕ(βk,i).
As a result, we have x[k] ⊆ D[k], where D[k] is the follow-
ing interval vector

D[k] = [ϕ(αk,1), ϕ(βk,1)] × · · · × [ϕ(αk,sk ), ϕ(βk,sk )].

Linear Inclusion Let us explain how to compute linear in-
clusion for the activation function y = ϕ(x) within the do-
main [a, b]. Let us ﬁrst choose the midpoint ξ of the interval
[a, b], and the Taylor expansion for ϕ(x) yields a linear ap-
proximation at the point ξ,

ϕ(x) = p(x) + r(x),

where p(x) = ϕ(ξ)+ ϕ′(ξ)(x− ξ) is a linear approximation
of ϕ(x) at the point ξ, and r(x) is the error function. By ex-
ploring interval evaluation [23], we can obtain the inclusion,
denoted by [r, r], representing the range of r(x) evaluated
at the domain [a, b].
To summarize,

the procedure called LI, combining
bound derivation with linear approximation, is applicable
to linear inclusion for the activation function ϕ(x), namely,

LI: ϕ(a) ≤ ϕ(x) ≤ ϕ(b), p(x) + r ≤ ϕ(x) ≤ p(x) + r. (14)

As mentioned above, suppose the range of z[k]
j

is
[αk,j , βk,j] produced from the previous layer. For each
[k]
1 ≤ j ≤ sk calling LI procedure to x
j ), yields
the associated linear inequalities:

[k]
j = ϕ(z

p[k]
j (z[k]

j ) + rk,j ≤ x[k]

ϕ(αk,j) ≤ x[k]
j ≤ p[k]

j ≤ ϕ(βk,j),
j (z[k]

j ) + rk,j .

(15)

(16)

By calling LI procedure component-wise to the vector x[k],
the nonlinear constraint (13) can be relaxed to the following
set of linear constraints

Ck : 


x[k] ∈ D[k],
x[k] − p[k](z[k]) − r[k] ≥ 0,
x[k] − p[k](z[k]) − r[k] ≤ 0,

(17)

where p[k](z[k]) = ϕ(ξ[k]) − ϕ′(ξ[k])(z[k] − ξ[k]) and
ξ[k] is the midpoint vector for the range of z[k], r[k] =
(rk,1, · · · , rk,sk )T , and r[k] = (rk,1, · · · , rk,sk )T .

Using (12) and (17), we provide a way for linear encod-
ing the input-output behavior of layer L[k]. Thus, the set
of linear constraints encoding the network N can be de-
k=1 Ck. In this situation, it follows from
the above linear encoding that the equivalent optimization
problem (10) for robustness veriﬁcation can be relaxed into
the following optimization problem:

ﬁned as: C = ∪n

r = minx[k],z[k]{ x[n]
p∗
s.t. x[k] ∈ D[k],

ℓ − x[n]
˜ℓ

, ˜ℓ ̸= ℓ }

k = 0, · · · , n
z[k] = W [k] x[k−1] + b[k], k = 1, · · · , n,
x[k] − p[k] (z[k]) − rk ≥ 0, k = 1, · · · , n
x[k] − p[k] (z[k]) − rk ≤ 0, k = 1, · · · , n.

(18)




Remark 1 In comparison with the optimization problem
(10), the feasible set of (18) is the superset of the one of (10),
which results in the optimum of (18) is the lower bound of
the optimum of (10), i.e., p∗

r ≤ p∗.

3.3. Transform to Linear Programming

The objective function of the optimization problem (18)
is piecewise-linear. To make it amenable to LP solvers, it
is transformed into an equivalent optimization problem by
introducing an auxiliary variable t:

p∗
r = max t
s.t. x

[n]
˜ℓ

≥ t,

˜ℓ ̸= ℓ,

[n]
ℓ − x
x[i] ∈ D[k],
k = 0, · · · , n
z[k] = W [k] x[k−1] + b[k], k = 1, · · · , n
x[k] − p[k] (z[k]) − rk ≥ 0, k = 1, · · · , n
x[k] − p[k] (z[k]) − rk ≤ 0, k = 1, · · · , n

(19)




[n]
˜ℓ

r and ˜p∗

Suppose p∗

}, and x

problem (18) and (19) respectively. Since p∗

r are optimums of the optimization
r = min{x[n]
ℓ −
r ≥ ˜p∗
x
r.
On the other hand, ˜p∗
r is the maximum value satisfying the
[n]
set of constraints {x
ℓ − x
r ≥ p∗
r.
Combining the two facts, it follows that p∗

r, for all ˜ℓ ̸= ℓ, we have p∗

≥ t, ˜ℓ ̸= ℓ}, thus ˜p∗

[n]
ℓ − x

≥ ˜p∗

[n]
˜ℓ

[n]
˜ℓ

r = ˜p∗
r.

The above transformation between the optimization
problems derives a sufﬁcient condition for robustness ver-
iﬁcation of CDNNs.

Theorem 2 Given a CDNN N : ⟨L, X, W, B, Φ⟩ with the
input region Θ. Suppose (19) is the linear programming
problem established as above, and p∗
r is an optimum of (19).
If p∗

r > 0, then N is robust.

Proof 2 For the given N with the input region Θ, one may
establish the optimization problem (10). Due to the equiv-
alence between (18) and (19), and in combination with Re-
mark 1, p∗
r is the lower bound of the optimum of (10). Tak-
ing p∗
r > 0 into account, the optimum of (10) is positive.
Finally, it follows from Theorem 1 that N is robust.
(cid:3)

511422

Theorem 2 guarantees when p∗

r > 0, N is robust with Θ.
Consider the case p∗
r <= 0, and the corresponding worst
point is ˜x[0], it is required to verify ˜x[0] using N , denoted
as ϱ(˜x[0]). If ϱ(˜x[0]) ̸= ℓ, that is ˜x[0] is misclassiﬁed, ˜x[0]
provides a counterexample to show that N is not robust. But
if ϱ(˜x[0]) = ℓ, the robustness of N remains undetermined
as we cannot predict the other bad points are robust or not.
To verify the robustness property of the given network
N , we have established a relaxed linear programming prob-
lem (19), which can be solved by using conventional algo-
rithms such as the interior-point method [4]. Furthermore,
the positivity of the optimum of (19) sufﬁces to verify the
robustness of N . Detailed procedures are summarized in
Algorithm 1.

Algorithm 1: RobustVeriﬁer (Robustness veriﬁcation
for a CDNN)

Input: A CDNN N ; An input region Θ.
Output: bRobust; bUnrobust.

1 bRobust ← FALSE; bUnrobust ← FALSE;
2 Determine the label ℓ:

3

Choose a random element x[0] from Θ, i.e.,

x[0] ∈ Θ;

ℓ ← ϱ(x[0]);

4
5 D[0] ← Θ;
6 C ← {};
7 for k = 1 : n do

8

9

10

D[k] is computed from D[k−1] by bound
derivation;
Ck ← LinearEncoding(N , D[k]);
C ← C ∪ Ck;

11 Construct the linear programming problem as in (19);
12 Call an LP solver to obtain the optimum p∗

r and its

optimizer: (˜x[k], ˜z[k]);

13 if p∗

r > 0 then
bRobust ← TRUE;

14

15 else

16

17

if ϱ(˜x[0]) ̸= ℓ then

bUnrobust ← TRUE;

RobustVeriﬁer takes as inputs a CDNN N with an input
region Θ. In line 1, two Boolean ﬂags are deﬁned, i.e., bRo-
bust and bUnrobust. The procedure ﬁrst determines the la-
bel ℓ of the N by executing an element chosen from Θ, and
then initializes the set of the linear constraints (line 5 and
6). Line 7 to 10 obtain the linear constraints approximating
the activation functions. Line 11 constructs the correspond-
ing linear programming, and its solution can be found by
calling an LP solver (line 12).

RobustVeriﬁer returns one of the following results:

if

bRobust is TRUE then N with Θ is robust (line 14), and
if bUnrobust is TRUE then N with Θ is unrobust since t-
wo inputs x[0] and ˜x[0] have the different label. Otherwise,
RobustVeriﬁer fails to determine whether N is robust.

Remark 2 Suppose D[n] is the range of the output layer Ln
obtained by bound derivation (line 8), denoted by D[n] =
[l1, u1] × · · · [lsn , usn ]. Once lℓ > u˜ℓ for each ˜ℓ ̸= ℓ, it is to
say that N with the input region Θ has the same label ℓ. In
this case, Robustveriﬁer ends and bRobust is set to TRUE.
Therefore, this simple method for computing the lower and
upper bounds of the output layer, called as RobustLU, can
also be used to verify the robustness property of N with Θ.

4. Evaluation

In the evaluation, we use image classiﬁcation networks
generated from three popular image datasets designed for
image classiﬁcation, as target CDNNs for robustness veri-
ﬁcation. Those image classiﬁcation networks include net-
works trained for classifying 10 classes of hand-written im-
ages of digits 0-9 from the database MNIST [20] , 43 class-
es of German trafﬁc signs from the database (GTSRB) [30]
, and 101 classes of images from the Caltech 101 dataset
[21] . All the image classiﬁcation networks are built from
the neural networks library Keras [7] with a deep learning
package Tensorﬂow [1] as its backend.

Details of the three datasets are listed below:

• Modiﬁed National Institute of Standards and Technol-
ogy database. The MNIST database is a large collec-
tion of hand-written images designed for training vari-
ous image processing systems in which each image is
of size 28 × 28 and in black and white. It has 60,000
training input images, belonging to one of 10 labels,
i.e. from the digit 0 to the digit 9.

• German Trafﬁc Signs Recognition Benchmark. The
GTSRB is a large image set of trafﬁc signs devised for
the single-image, multi-class classiﬁcation problem. It
consists of over 50,000 images of trafﬁc signs, belong-
ing to 43 different classes. In GTSRB, each image is
of size 32 × 32 and has three channels (Red, Green and
Blue).

• Caltech-101 Dataset. The Caltech-101 dataset consist-
s of images belonging to 101 different classes where
the number of images in each class varies from 40 to
800. Most images are of the resolution 300 × 200 pixel
and have three channels.

In [24], the tool NEVER is proposed to verify a special
type of neural networks with sigmoid activation functions,
called Multi-Layer Perceptrons (MLPs). A MLP is a degen-
eration of CDNN as it restricts activation functions only to

611423

be located on the output neurons. We generalized the algo-
rithm to treat CDNNs, called Generalized-NEVER (GNEV-
ER).

We have implemented the proposed algorithms Ro-
bustVeriﬁer, RobustLU (see Remark 2) and Generalized-
NEVER as MATLAB programs. The LP problems gener-
ated from robust veriﬁcation are settled by the LP solver
linprog. The following experimental results were obtained
by running them on an Ubuntu 18.04 LTS computer with
a 3.2GHz Intel(R) Xeon Gold 6146 processor and 128 GB
RAM.

4.1. Performance

In the section, we evaluate the performance of the three

tools by varied perturbations and network structures.

The CDNN trained from the MNIST dataset is used for
performance evaluation whose input layer has 784 neurons
and output layer consists of 10 neurons. For each image in
MNIST, a set of pixels with a range of possible perturbation
of these pixels is speciﬁed, which forms an input region Θ
for the input layer.

Given a CDNN N and an input region Θ, robustness ver-
iﬁcation requires to check whether the robustness property
holds with respect to Θ, i.e., all inputs within the input re-
gion Θ are assigned to the same label. Figure 1 shows a
conﬁguration of input region: a set of perturbations (with
disturbance radius ϵ = 0.5) to a block of size 5 × 5 on the
top left corner of each image.

Figure 1. The MNIST CDNN is robust w.r.t. perturbed images.

Table 1 lists the comparison results of the three tools Ro-
bustVeriﬁer, RobustLU and GNEVER with varied perturba-
tions. Here, the input region is a block of size 5 × 5 on
the top left corner of the image; ϵ denotes the disturbance
radius, that is, ϵ = 0.2 means perturbations varying with-
in the range [−0.2, 0.2] are imposed on the original image;
r1, r2 and r3 record the recognition rates of the tools Ro-
bustVeriﬁer, RobustLU and GNEVER, respectively. Here,
the recognition rate is deﬁned as dividing the number of ro-
bust images plus adversarial examples reported by a speciﬁc
tool by the total number of images under veriﬁcation.

It can be shown from Table 1 that under all circum-
stances, the tool RobustVeriﬁer outperforms the other two
tools, while the tool RobustLU provides better performance
than the tool GNEVER. For example, consider the case of
ϵ = 0.2, the correct recognition rates of three tools are

Table 1. Performance on varied perturbations
r3
ϵ

r1

r2

0.10
0.15
0.20
0.25
0.30

95.94%
92.16%
87.90%
79.43%
70.56%

92.56%
84.57%
83.88%
67.74%
65.32%

90.39%
82.00%
64.65%
65.78%
45.74%

87.90%, 83.88% and 64.65%, of the total 10,000 tests, re-
spectively. And when ϵ grows up to 0.3, the correct recog-
nition rates are reduced to 70.56%, 65.32% and 45.74%,
respectively. The performance result complies with our
discussion of approximation precision in descending order.
For the same tool, seeing the results in the same column, the
performance of veriﬁcation tools decreases when the pertur-
bations become bigger, since the increasement of perturba-
tions speeds up the accumulation of approximation errors.

Table 2 lists the comparison results on varied network
structures. Here, the input region is deﬁned as a 5 × 5 block
on the top left corner and the disturbance radius ϵ is ﬁxed to
0.20; n denotes the number of layers in CDNNs and in each
hidden layer the number sk of neurons varies with 40 ≤
sk ≤ 100.

Table 2. Performance on varied network structure

n
5
6
7
8
9
10

r1

87.90%
72.33%
61.33%
50.58%
45.90%
34.65%

r2

83.88%
51.84%
43.11%
29.02%
33.28%
14.19%

r3

64.65%
50.24%
29.07%
27.79%
31.63%
12.40%

Table 2 shows that the tool RobustVeriﬁer performs best
in all cases and RobustLU performs better than GNEVER.
And with the increasement of hidden layers, inspecting the
results line by line, the performance of veriﬁcation tools
decreases. It is the approximation error accumulated layer
by layer mainly responsible for performance decreasement.
The tight approximation technique adopted helps our tool
to be less affected.

4.2. Precision vs performance

In this subsection, we evaluate the three tools with larger
CDNNs and investigate how does the precision affect the
performance of verifying practical CDNNs.

For the CDNN generated from GTSRB, we specify the
input region as a 3 × 3 block on the center of each image
with the disturbance radius ϵ = 0.05 on one of the RGB
(Red Green Blue) channels.

For this conﬁguration of input region,

the tool Ro-
bustVeriﬁer can verify that the perturbed images fall into the
same class as their original ones while the other two cannot.

711424

Figure 2. The GTSRB CDNN is robust w.r.t. perturbed images.

Figure 3. Adversarial examples for the GTSRB CDNN.

Figure 2 shows 4 pairs of original and perturbed images of
the conﬁguration that has been veriﬁed to be robust. Then
we try to verify larger input region by enlarging the distur-
bance radius or the size of perturbation block. Unfortunate-
ly, neither of the two tools can verify the perturbed images
to be robust under those conﬁgurations. For the input re-
gion, a 3 × 3 block put on the central with the disturbance
radius ϵ = 0.1 on one of the RGB channels, our tool can
return many adversarial examples corresponding to a mis-
classiﬁcation of perturbed images. Figure 3 presents some
adversarial examples, of which the input region is set with
disturbance radius ϵ = 0.1 on one channel to the central
block of size 3 × 3.

Compared with that of the MNIST database, the CDNN
of GTSRB has to identify more classes, 43 vs 10. Thus,
it is more sensitive to perturbations inborn. A veriﬁcation
method unable to provide enough precision does not work
when encountering CDNNs of this kind.

We also conduct experiments on the CDNN yielded from
the Caltech 101 dataset. The trained CDNNs have no less
than 180,000 neurons, while the input region is set as a
block of size 40 × 40 on the center of image with the dis-
turbance radius ϵ = 0.5. Several pairs of original and per-
turbed images are shown in Figure 4.

Figure 5 lists the result of performance vs network struc-
ture. Here, the X axis denotes the number of layers of
CDNNs while the Y axis records the rate of successful ver-
iﬁcation. For each network structure, ten different conﬁg-
urations of the number of neurons are chosen for experi-
ments where the number of neurons of each hidden layer is

Figure 4. The trained CDNN is robust w.r.t. perturbed images.

generated randomly from the range [40,150]. The average
successful veriﬁcation rates of ten experiments are shown
in the ﬁgure.

For the tool GEVER, its performance varies between
23% and 81%. It drops sharply with the increase of hidden
layers as the error introduced by its approximation propa-
gates layer by layer very quickly. For all experiments, our
tool RobustVeriﬁer can give exact results to more than 82%
disturbed images. Note that, the veriﬁcation results depend
not only on the speciﬁc veriﬁcation method but also on the
quality of CDNNs being veriﬁed. The CDNNs of 7 or 8
layers seems not to be as good as the others. In this exper-
iment, our tool RobustVeriﬁer performs best and shows its
ability of treating real CDNNs.

Figure 5. Performance vs network structure on Caltech-101.

5. Conclusion

We have presented a novel method for attacking ro-
bustness veriﬁcation of classiﬁcation deep neural networks
with sigmoid activation functions. To make it amenable,
we started with converting the veriﬁcation problem into
an equivalent nonlinear optimization problem, proceeded
by solving a linear programming problem yielded from
the linear relaxation for nonlinear activation functions.
Our LP based approach has been implemented inside the
tool RobustVeriﬁer and validated on several state-of-the-art
CDNNs for realistic images. The performance results high-
light the potential of the approach in providing formal guar-
antees about the robustness of CDNNs of signiﬁcant size.

811425

References

[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur,
Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gor-
don Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasude-
van, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang
Zheng. Tensorﬂow: A system for large-scale machine learn-
ing. In Proceedings of 12th USENIX Symposium on Operat-
ing Systems Design and Implementation (OSDI), pages 265–
283, 2016.

[2] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos,
Dimitrios Vytiniotis, Aditya V. Nori, and Antonio Crimin-
isi. Measuring neural net robustness with constraints.
In
Proceedings of the 29 Annual Conference on Neural Infor-
mation Processing Systems (NIPS), pages 2613–2621, 2016.
[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nel-
son, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and
Fabio Roli. Evasion attacks against machine learning at
test time.
In Proceedings of the European Conference on
Machine Learning and Knowledge Discovery in Databases,
pages 387–402, 2013.

[4] Stephen Boyd and Lieven Vandenberghe. Convex Optimiza-

tion. Cambridge University Press, 2004.

[5] Rudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet
Kohli, and M. Pawan Kumar.
Piecewise linear neural
network veriﬁcation: A comparative study. CoRR, ab-
s/1711.00455, 2017.

[6] Nicholas Carlini and David A. Wagner. Towards evaluat-
ing the robustness of neural networks.
In Proceedings of
the IEEE Symposium on Security and Privacy, pages 39–57,
2017.

[7] Franc¸ois Chollet. Keras. https://keras.io., 2015.
[8] Dan C. Ciresan, Ueli Meier, and J¨urgen Schmidhuber. Multi-
column deep neural networks for image classiﬁcation.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 3642–3649, 2012.

[9] Souradeep Dutta, Susmit Jha, Sriram Sanakaranarayanan,
and Ashish Tiwari. Output range analysis for deep feedfor-
ward neural networks. In Proceedings of the 10th Interna-
tional Symposium on NASA Formal Methods (NFM), pages
121–138, 2018.

[10] R¨udiger Ehlers. Formal veriﬁcation of piece-wise linear
feed-forward neural networks.
In Proceedings of the 15th
International Symposium on Automated Technology for Ver-
iﬁcation and Analysis (ATVA), pages 269–286, 2017.

[11] Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analy-
sis of classiﬁers’ robustness to adversarial perturbations. Ma-
chine Learning, 107(3):481–508, 2018.

[12] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep s-
parse rectiﬁer neural networks. In Proceedings of the 14th In-
ternational Conference on Artiﬁcial Intelligence and Statis-
tics (AISTATS), pages 315–323, 2011.

[13] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep

Learning. MIT Press, 2016.

chine. In Proceedings of the 15th Annual Conference of the
International Speech Communication Association (INTER-
SPEECH), pages 223–227, 2014.

[15] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r. Mohamed, N.
Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath,
and B. Kingsbury. Deep neural networks for acoustic model-
ing in speech recognition: The shared views of four research
groups.
IEEE Signal Processing Magazine, 29(6):82–97,
2012.

[16] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min
Wu. Safety veriﬁcation of deep neural networks.
In Pro-
ceedings of the 29th International Conference on Computer
Aided Veriﬁcation (CAV), pages 3–29, 2017.

[17] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and
Mykel J. Kochenderfer. Reluplex: An efﬁcient SMT solver
for verifying deep neural networks.
In Proceedings of the
29th International Conference on Computer Aided Veriﬁca-
tion (CAV), pages 97–117, 2017.

[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works.
In Proceedings of the 26th Annual Conference on
Neural Information Processing Systems (NIPS), pages 1106–
1114, 2012.

[19] Zeshan Kurd and Tim Kelly. Establishing safety criteria for
artiﬁcial neural networks. In Proceedings of the 7th Interna-
tional Conference on Knowledge-Based and Intelligent In-
formation and Engineering Systems (KES), pages 163–169,
2003.

[20] Yann LeCun, Corinna Cortes,

J.C.
The mnist database of handwritten digits.

Burges.
http://yann.lecun.com/exdb/mnist/, 1998.

and Christopher

[21] Fei-Fei Li, Robert Fergus, and Pietro Perona. Learning gen-
erative visual models from few training examples: An incre-
mental bayesian approach tested on 101 object categories.
Computer Vision and Image Understanding, 106(1):59–70,
2007.

[22] Awni Y. Hannun Maas, Andrew L. and Andrew Y. Ng. Rec-
tiﬁer nonlinearities improve neural network acoustic models.
In Proceedings of the 30th International Conference on Ma-
chine Learning (ICML), 2013.

[23] Ramon Edgar Moore, R. B. Kearfott, and M. J. Cloud. In-
troduction to Interval Analysis. Cambridge University Press,
2009.

[24] Luca Pulina and Armando Tacchella. An abstraction-
reﬁnement approach to veriﬁcation of artiﬁcial neural net-
works.
In Proceedings of the 22nd International Confer-
ence on Computer Aided Veriﬁcation (CAV), pages 243–257,
2010.

[25] Luca Pulina and Armando Tacchella. Challenging SMT
solvers to verify neural networks. AI Communications,
25(2):117–135, 2012.

[26] Sebastian Ramos, Stefan K. Gehrig, Peter Pinggera, Uwe
Franke, and Carsten Rother. Detecting unexpected obsta-
cles for self-driving cars: Fusing deep learning and geomet-
ric modeling. In Proceedings of the IEEE Intelligent Vehicles
Symposium (IV), pages 1025–1032, 2017.

[14] Kun Han, Dong Yu, and Ivan Tashev. Speech emotion recog-
nition using deep neural network and extreme learning ma-

[27] Anton Maximilian Sch¨afer and Hans Georg Zimmermann.
Recurrent neural networks are universal approximators. In

911426

Proceedings of the 16th International Conference on Artiﬁ-
cial Neural Networks (ICANN), pages 632–640, 2006.

[28] J¨urgen Schmidhuber. Deep learning in neural networks: An

overview. Neural Networks, 61:85–117, 2015.

[29] Eui Chul Richard Shin, Dawn Song, and Reza Moazzezi.
Recognizing functions in binaries with neural networks. In
Proceedings of the 24th USENIX Security Symposium, pages
611–626, 2015.

[30] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man
vs. computer: Benchmarking machine learning algorithms
for trafﬁc sign recognition. Neural Networks, 32:323–332,
2012.

[31] Changyin Sun, Wei He, Weiliang Ge, and Cheng Chang.
Adaptive neural network control of biped robots.
IEEE
Transactions on Systems, Man, and Cybernetics: Systems,
47(2):315–326, 2017.

[32] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus.
Intriguing properties of neural networks. In Proceedings of
the International Conference on Learning Representations
(ICLR 2014), 2014.

[33] Vincent Tjeng and Russ Tedrake. Verifying neural networks
with mixed integer programming. CoRR, abs/1711.07356,
2017.

[34] Tong Wang, Huijun Gao, and Jianbin Qiu. A combined adap-
tive neural network and nonlinear model predictive control
for multirate networked industrial process control.
IEEE
Transactions on Neural Networks and Learning Systems,
27(2):416–425, 2016.

[35] Weiming Xiang, Hoang-Dung Tran, and Taylor T. Johnson.
Reachable set computation and safety veriﬁcation for neu-
ral networks with relu activations. CoRR, abs/1712.08163,
2017.

[36] Weiming Xiang, Hoang-Dung Tran, , and Taylor T. Johnson.
Output reachable set estimation and veriﬁcation for multilay-
er neural networks. IEEE Transactions on Neural Networks
and Learning Systems, 99:1–7, 2018.

1011427

