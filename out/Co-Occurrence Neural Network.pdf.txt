Co-Occurrence Neural Network

Ira Shevlev

Shai Avidan

School of Electrical Engineering

School of Electrical Engineering

Tel-Aviv University

Tel-Aviv, Israel

Tel-Aviv University

Tel-Aviv, Israel

irenably@post.tau.ac.il

avidan@eng.tau.ac.il

Abstract

Convolutional Neural Networks (CNNs) became a very
popular tool for image analysis. Convolutions are fast to
compute and easy to store, but they also have some limita-
tions. First, they are shift-invariant and, as a result, they
do not adapt to different regions of the image. Second, they
have a ﬁxed spatial layout, so small geometric deformations
in the layout of a patch will completely change the ﬁlter re-
sponse. For these reasons, we need multiple ﬁlters to handle
the different parts and variations in the input.

We augment the standard convolutional tools used in
CNNs with a new ﬁlter that addresses both issues raised
above. Our ﬁlter combines two terms, a spatial ﬁlter and a
term that is based on the co-occurrence statistics of input
values in the neighborhood. The proposed ﬁlter is differen-
tiable and can therefore be packaged as a layer in CNN and
trained using back-propagation.

We show how to train the ﬁlter as part of the network
and report results on several data sets. In particular, we
replace a convolutional layer with hundreds of thousands of
parameters with a Co-occurrence Layer consisting of only a
few hundred parameters with minimal impact on accuracy.

1. Introduction

A common solution to many computer vision problems
is based on Convolutional Neural Networks (CNNs). CNNs
gained popularity, in part, because they offer a ﬂexible ar-
chitecture that can be adapted to many different tasks. At
their core, CNNs are based on simple primitives that include
convolutions, non-linearity and pooling.

A convolutional layer is the most informative layer, be-
cause it stores the parameters of the network. But convolu-
tions (i.e., ﬁlters) are shift invariant and one needs a large
number of ﬁlters to deal with different regions of the input.
Moreover, ﬁlters depend on the spatial layout of the input
and are not suitable to deal directly with the distribution of

the input values. As a result, a small geometric deformation
in the input patch will be considered as a different pattern
that requires additional ﬁlters to deal with it.

We propose a new ﬁlter that addresses these issues. The
ﬁlter is based on the Co-occurrence Filter (CoF) [10]. CoF
combines a spatial ﬁlter and a component that is based on
the co-occurrence of pixel values. The co-occurrence com-
ponent will mix (i.e., smooth) pixel values that co-occur fre-
quently in the image plane, while pixel values that do not
co-occur frequently will not mix. This makes CoF a bound-
ary preserving ﬁlter that can smooth textured regions while
preserving the boundaries between them. A unique property
of the co-occurrence term is that it depends on pixel values
and not on the spatial layout of pixels in the image plane.

We take this idea one step further.

Instead of collect-
ing co-occurrence statistics, as is done in CoF, we learn
weights based on co-occurrence statistics that optimize the
objective function of the network. So if values in the input
co-occur often in the training set, we learn the best weight
to take advantage of this fact and minimize the loss func-
tion of the network. This deep co-occurrence matrix differs
from a standard co-occurrence matrix in two ways. First,
the weights of a co-occurrence matrix are positive, because
they describe distributions. In contrast, the weights of the
deep co-occurrence matrix can be negative. Second, the co-
occurrence matrix is symmetric by deﬁnition. This is not
necessarily the case for the deep co-occurrence matrix.

The new ﬁlter is embedded in a Co-occurrence Layer
(CoL) that can replace standard convolutional layers. CoL
offers a number of useful properties. To begin with, CoL
uses a small number of parameters to generate a large num-
ber of ﬁlters, depending on the values in a given neighbor-
hood. In addition, the new layer is differentiable and can
be trained using back-propagation without any pre- or post-
processing. On top of that, CoL can handle different pat-
terns of the input, as well as distributions (i.e., histograms)
or re-arrangement of values, because of its co-occurrence
term. We show how to train CoL using back-propagation
and evaluate its performance on a number of data sets.

14797

2. Background

Almost as soon as neural networks re-gained popularity
there has been a surge of interest in ways to speed them up
and reduce their memory footprint.

Some early work exploited the linear structure of convo-
lutions for efﬁcient evaluations. This includes, for example,
works by Denton et al. [4] and Jaderberg et al. [8]. The
key insight of these works is to approximate the 4D tensor
that deﬁnes the network using various decomposition tech-
niques.

In Han et al.

[7] the authors propose a three stage
pipeline to compress a network: Pruning, trained quantiza-
tion and Huffman coding. This leads to impressive reduc-
tion in the network size by about ×35 of ×49 depending on
the network. But the authors use standard ﬁlters to achieve
the signiﬁcant savings and do not offer new ﬁlters similar to
the one proposed here.

Using new ﬁlters in Neural Networks can be found in the
work of Wang et al. on non-local neural network [17]. In
their work, the response at a pixel location is a weighted
sum of the features at all locations in its neighborhood.
While this approach helps improve the performance of the
network, it does not offer a new ﬁlter as the one described in
this proposal. Another work is that of Cohen et al. [3]. They
propose a steerable CNN approach where the ﬁlter response
at each pixel location is a linear combination of some base
ﬁlters. The emphasis of the work is that transformed ver-
sions of the same image region should be treated the same
by the network. Instead of taking an axiomatic approach,
Weiler et al. [18] propose to learn ﬁlters that are rotation
equivariant. Excellent results on the rotated MNIST dataset
and the ISBI 2012 2D EM segmentation challenge are re-
ported.

Jampani et al. [9] embed bilateral ﬁlters within a neural
network. They treat bilateral ﬁlters as ﬁlters in higher di-
mensional space and propose to learn the weights of these
ﬁlters as part of the network training. Recently, SPLATNet
[15] used this idea of sparse bilateral convolutional layers
for direct processing of point clouds with impressive results.
Battaglia et al. [1] proposed an interactive network to
learn interaction between objects. This is related to co-
occurrences with a couple of important differences. First,
we operate at the pixel level and not at the object level.
Second, and more importantly, they use existing building
blocks to capture the interaction between objects. We, on
the other hand, propose a ﬁlter that directly captures co-
occurrence statistics.

There has been a considerable amount of work at the in-
tersection of edge-preserving ﬁlters and CNNs. However,
most of this effort is focused on learning the parameters of
the edge-preserving ﬁlter. They are not designed to intro-
duce the edge-preserving ﬁlter into the network.

For example, Wu et al. [19] propose a very fast deep

version of the guided image ﬁlter where the parameters of
the ﬁlter are learned by the network. They use standard
building blocks in their network.

Gharbi et al. [6] use pairs of input/output images to train
a convolutional neural network to predict the coefﬁcients
of a locally-afﬁne model in bilateral space. This lets them
perform real-time image enhancement.

Our work is also inspired by the Network in Network
approach [12] where the goal is to replace the linear con-
volution with a micro neural network. However, they use
standard building blocks to build the micro neural network.
Cohen et al. [2] described an architecture that is driven
by two operators: generalization of inner product and a
long-mean-exp function. The proposed structure does not
deal with statistical context of the input. Moreover, SimNet
requires pre-process on the input data while CoL can be just
deﬁned as a part of the network without additional effort.

We, as opposed to previous work, propose a novel build-

ing block to be used in neural networks.

3. Method

A linear ﬁlter is of the form:

Jp = X

wqIq

q∈Np

(1)

where I is the input image, J is the output image, the sub-
script Iq denotes pixel q in image I, Np denotes the neigh-
borhood of pixel p and wq is the weight assigned to pixel
q.

In Convolutional Neural Networks (CNNs) the weights
wq of each layer are adjusted so as to optimize the objec-
tive function of the network.
In this section we deﬁne a
Co-occurrence Layer (CoL) that is based on a variation of
the co-occurrence ﬁlter and show how to use it as part of a
neural network.

3.1. The Co occurrence ﬁlter

The Co-occurrence ﬁlter (CoF) [10] extends the Bilateral
Filter (BF) [16] by replacing the range Gaussian Gr with a
co-occurrence matrix M . Speciﬁcally, a BF is deﬁned as:

Jp =

1
Z X

q∈Np

Gs(p, q)Gr(Ip, Iq)Iq

(2)

where Z is a constant designed to ensure the weights sum
to 1. The CoF is deﬁned as:

Jp =

1
Z X

q∈Np

Gs(p, q)M (Ip, Iq)Iq

(3)

where the co-occurrence matrix M is given by:

M (a, b) =

C(a, b)
h(a)h(b)

(4)

4798

Figure 1. Applying CoL: The CoL is based on Equation 7.
In
the input image for a center pixel p and a neighboring pixel q, we
use deep co-occurrence matrix entry L([Ip], [Iq]) to create a ﬁlter
based on co-occurrence statistics. Multiplying this ﬁlter (element
wise) with the spatial ﬁlter w leads to the ﬁlter that is actually
applied at pixel location p. This way different ﬁlters are used at
different regions of the image.

and

C(a, b) = X

exp −

p,q

d(p, q)2

2σ2

[Ip = a][Iq = b]

(5)

counts the number of times pairs of pixel values co-occur
within a window, weighted by their distance. This count is
normalized by h(a), h(b) which are the histogram values for
pixel values a and b, respectively. The σ is a user speciﬁed
parameter and [·] is the indicator function.

Intuitively, the CoF mixes pixel values that co-occur fre-
quently, while preserving pixel values that do not co-occur
frequently.

In the case of a gray scale image, the size of the ma-
trix M is conveniently set to 256 × 256, but if the input
is a color image then the co-occurrence matrix becomes a
2563 × 2563 matrix. This is clearly too large a matrix to
work with and the solution proposed in [10] was to quantize
the RGB values into, say, 256 color clusters using k-means.
The co-occurrence ﬁlter can then be written as:

Jp =

1
Z X

p,q

Gs(p, q)M ([Ip], [Iq])Iq

(6)

where we denote by [Iq] the index of the cluster to which Iq
was assigned.

3.2. The Co occurrence layer

The co-occurrence matrix M captures the joint proba-
bility of observing activation values a and b together. This
does not have a direct link to the actual objective function

Figure 2. Training CoL: In the forward pass we use equation 7 to
propagate pixel values from the input layer I, using current w and
L. In the backward pass we update the weights of w and L using
equations 13, 14 and propagating the total loss using equation 12.

of the network. To this end, we use a deep co-occurrence
matrix L that is trained to learn weights that are based on co-
occurring activation values. The entries of L are trained to
optimize the objective function of the network. With slight
abuse of notation, we will deﬁne a Co-occurrence Layer
(CoL) function as:

Jp = CoL(I, w, L([Ip], [Iq])) = X

wqL([Ip], [Iq])Iq

q∈Np

(7)

where the Np denotes the neighborhood of activation p.

In words, the value of activation Jp in the output layer is
a weighted sum of activation values Iq from the input layer.
The weights are deﬁned both by the spatial ﬁlter wq and a
matrix L([Ip], [Iq]). The implementation details of CoL are
illustrated in Figure 1. We quantize the pair of the input
activations Ip, Iq and fetch the relevant entry from the deep
co-occurrence matrix L. Then we multiply the fetched ﬁlter
with the spatial ﬁlter w in order to calculate a total ﬁlter for
each point. The ﬁnal step is simply applying the total ﬁlter
to the input layer at that particular location.

Because the input activation values can take any real
value, we quantize them uniformly into k bins. Speciﬁcally,
we normalize the values of every channel to be in the range
[0, 1] and then for each x ∈ [0, 1], we deﬁne the index of x
as [x] = round(kx).

The difference between the co-occurrence matrix M
used for CoF and the matrix L used for CoL is the core of
this work. The co-occurrence matrix M counts the number
of times pixel values co-occur in the input image. The ma-
trix L, on the other hand, learns weights that are designed
to optimize the performance of the network.

This difference is also evident in the way CoL and CoF
are calculated and applied.
In CoF, there is a collection
stage in which we scan the image and collect co-occurrence
statistics. Once the co-occurrence matrix M is collected, we
apply it to the image using equation 6. The situation is com-
pletely different with CoL. In the CoL case we start with

4799

=Fetched co-occurrence filterSpatial filterTotal filterInput imagepqDeep co-occurrence matrixForward passBackward pass    ;              ; shift-invariant ﬁlters.

Figure 3 illustrates the properties of CoL that were de-
tailed above. We trained a network on CIFAR-101 with CoL
(with ﬁlter sizes L[5 × 5] and w[3 × 3]) as a ﬁrst layer, that
works directly on raw pixel values. As can be seen, the ﬁl-
ter looks different at different locations in the image. Since
the spatial ﬁlter is shift invariant the differences in the ﬁl-
ters are because of the co-occurrence term. For example,
the pixel in the sky of the image has a ﬁlter that is very sim-
ilar to w. The reason for this is the values of neighboring
pixels are close to the value of the central pixel and quan-
tized to the same bin, so the fetched co-occurrence ﬁlter is
constant. The sample on the horse also captures a fairly uni-
form region, only this time the corresponding entry in L has
a negative value and, as a result, the w ﬁlter is ﬂipped. This
example demonstrates that indeed our algorithm can learn
negative weights. Near the rear leg of the horse the pixel
values are much more diverse, leading to a completely dif-
ferent ﬁlter. Finally, as shown in Figure 3, observe that L is
not symmetric.

3.3. Backpropagation

Equation 7 is used in the forward pass of the net-
work. We now deﬁne its derivatives as required for back-
propagation.

The general formulation of the co-occurrence layer is a
function of the input layers, convolution, and co-occurrence
ﬁlter. Speciﬁcally, let En
p denote the error function of the
n-th layer for pixel p, and let J n
p denote the output of the
n-th CoL at pixel p, then:

p = CoL(I n−1, wn, Ln([I n−1
J n

p

], [I n−1

q

]))

(8)

is the forward pass equation. Applying the chain rule we
have:

∂En
p
∂I n−1

k

=

∂En
p
∂J n
p

∂CoL(I n−1, wn, Ln([I n−1

p

], [I n−1

q

]))

∂I n−1

k

(9)

∂En
p
∂wn
k

=

∂En
p
∂J n
p

∂CoL(I n−1, wn, Ln([I n−1

p

], [I n−1

q

]))

∂wn
k

∂En
p

∂Ln(a, b)

=

∂En
p
∂J n
p

∂CoL(I n−1, wn, Ln([I n−1

p

∂Ln(a, b)

(10)
])

], [I n−1

q

By substituting the CoL deﬁnition, the derivatives are:

(11)

∂En
p
∂I n−1

k

=

∂En
p
∂J n
p

· wn
k

· Ln([I n−1

p

], [I n−1

k

])

(12)

∂En
p
∂wn
k

=

∂En
p
∂J n
p

· Ln([I n−1

p

], [I n−1

k

]) · I n−1

k

(13)

1Taken from https://github.com/seansoleyman/cifar10-resnet.

4800

Figure 3. Applying the CoL ﬁlter: We apply the matrix L and
ﬁlter w to the image using equation 7. For a center pixel p and
a neighboring pixel q, we use matrix entry L([Ip], [Iq]) and the
weight in the ﬁlter w at the corresponding spatial location to obtain
the weight of pixel q. This leads to different ﬁlters at different
regions of the image. In the sky region, the ﬁlter remains roughly
the same. On the horse, the ﬁlter ﬂips sign. Near the rear leg of
the horse we have a completely different ﬁlter.

a randomly generated matrix L. The forward pass of the
training algorithm applies it to the input using equation 7,
while the back-prop pass updates the weights of the matrix
L (see Figure 2). We give the details of the back-prop stage
in section 3.3.

Because of the different ways M and L are calculated,
they have different properties. The entries of M must be
positive because they form a probability distribution func-
tion. The entries of L can, and often do, have negative val-
ues. Also, by deﬁnition, M is symmetric but L is not nec-
essarily so.

The neighborhood Np = [Nx0 , Ny0 , Nz0 ] of CoL can be
2D or 3D depending on Nz0 . If Nz0 = 1 then the spatial
support is 2D which means that each input channel is pro-
cessed separately. If Nz0 > 1 the support is a 3D cube that
spans several input channels at once.

· Ny0
·Ny0

For k bins and a neighborhood of size Np =
[Nx0 , Ny0 , Nz0 ] the total number of parameters in CoL is
k · k + Nx0
· Nz0 . Such a CoL has the potential to
create a k(Nx0
·Nz0 ) ﬁlters, depending on input content.
For example, for k = 5 bins and a ﬁlter of size 3 × 3 × 3
there are a total of 527 different ﬁlters that can be generated
with as few as 52 parameters, which is the number of pa-
rameters needed to store L and w. This, we believe, is one
of the reasons that a single CoL equation can replace a typi-
cal convolution layer with tens or even hundreds of regular,

Learnedco-occurrence matrixLearned spatial filterimage∂En
p
∂Ln

(a,b)

=

∂En
p
∂J n
p

· X

wn
q

· I n−1

q

· δ(a, b)

(14)

q∈Np

Equation 12 expresses the back-propagated error function.
Equations 13 and 14 show the derivatives of the spatial ﬁl-
ter and the co-occurrence matrix, respectively. The delta
function

δ(a, b) = δ(a = [I n−1

p

], b = [I n−1

q

])

(15)

in equation 14 veriﬁes if the input values are quantized to
the relevant indices of the deep co-occurrence matrix L.

3.4. A Toy Example

We illustrate the power of CoL on a toy example and
compare its performance with two other popular layers:
fully-connected and convolutional. We generated a syn-
thetic set of 6000 training images and 1000 test images
of size 10 × 10 and 4 pixel values that were sampled i.i.d
from two different distributions (i.e., histograms). The pixel
values of the ﬁrst distribution came from the histogram
[0.4, 0.1, 0.1, 0.4], and the pixel values of the second came
from the histogram [0.1, 0.4, 0.4, 0.1]. The mean pixel value
of both histograms is the same. (See Figure 4(a)). We eval-
uated a number of network architectures:

1. conv(1 × 1 × 9) → avg(9 × 9) → fc(36 × 2)
2. conv(3 × 3 × 2) → avg(7 × 7) → fc(32 × 2)
3. conv(3 × 3 × 9) → avg(9 × 9) → fc(36 × 2)
4. fc(100 × 36) → fc(36 × 2)
5. CoL(4 × 4) → avg(5 × 5) → fc(36 × 2)

All networks use a stride step of 1. The CoL in network 5
only uses a co-occurrence term of size L[4 × 4] and a spa-
tial ﬁlter that is set to one. In the ﬁrst three architectures,
the size of convolutions and average pooling was designed
to reduce the image. In all cases, the last fully connected
layer is trained to output an estimated distribution of the
two classes as the output. See details in Figure 4(b). The
average pooling layer is used to decrease the power of the
ﬁnal fully connected layer and still preserve enough infor-
mation about the input. The CoL network (item (4) above)
consists of 16 parameters and the spatial ﬁlter is constant 1
of size (10 × 10).

Figure 4(b) shows the different instances of networks and
Figure 4(c) shows the results of this experiment. As can be
seen, the CoL-based network, which uses far less parame-
ters compare to a standard convolutional layer , achieves a
far lower loss, and does so in a lower number of iterations.
This shows that CoL can capture complex visual patterns in
a very compact form.

This experiments highlights a number of properties of
the CoL. First, it does not depend on the spatial layout of
pixel values and can handle distributions of pixel values.

Figure 4. Toy example: (a) Two examples of images where the
pixels are sampled i.i.d from two different 4-bin histograms. (b)
Three toy network architectures with different initial layers: (i) the
ﬁrst layer is Conv with different sizes of ﬁlter (ii) the ﬁrst layer is
CoL (iii) the network contains a fully-connected layer. (c) Graphs
of the loss function of the different networks. As can be seen, the
CoL architecture converges faster and to a much lower loss. See
more details about the different architectures in the text 3.4.

Second, the number of parameters of the co-occurrence ma-
trix does not depend on the spatial support, and a [4×4] deep
co-occurrence matrix can be used with a ﬁlter with a spatial
support of [10 × 10].

4. Experiments and Results

The code was implemented in TensorFlow and tested on
a GPU TITAN X (Pascal) with 12G memory. We tested the
proposed layer on two datasets: CIFAR-10 [11] that is used
for image classiﬁcation, and the ADE20k dataset [20] that
is used for semantic pixel labeling. In each case, we used
an existing, publicly available code.

Implementation details: We use a single CoL ﬁlter for
all the input data. That is, all input channels contribute to
a single co-occurrence term. We found that this gives the
best trade-off in terms of computer time and memory re-
quirements. We do believe that further research is required
on this topic. To speed up computation, we follow the ap-
proach used by Durand and Dorsey [5] to speed up the bi-
lateral ﬁlter. Speciﬁcally, we loop over the bin values, and
for each bin perform a regular shift-invariant convolution,
which is fast to compute. As a result, our algorithm is k
times slower than a regular CNN both in training and test-
ing (where k is the number of bins we use). We use k = 5
in all the experiments reported here. The initializer that was

4801

Avg_poolInputOutput(ii)Avg_poolOutput(i)Output(iii)InputInputCoLFCConvFCFCFC(b)(a)(c)# stacks

# blocks

layer

size

ﬁrst convolutional layer

stack(1)

stack(2)

stack(3)

block(1)

block(2)

block(1)

block(2)

block(1)

block(2)

conv1
conv2
conv1
conv2
conv1
conv2
conv1
conv2
conv1
conv2
conv1
conv2

[3 × 3 × 3 × 32]
[3 × 3 × 32 × 32]
[3 × 3 × 32 × 32]
[3 × 3 × 32 × 32]
[3 × 3 × 32 × 32]
[3 × 3 × 32 × 64]
[3 × 3 × 64 × 64]
[3 × 3 × 64 × 64]
[3 × 3 × 64 × 64]
[3 × 3 × 64 × 128]
[3 × 3 × 128 × 128]
[3 × 3 × 128 × 128]
[3 × 3 × 128 × 128]

Table 1. the original ResNet that used in CIFAR10 experiments.
The table describe all the convolutional layers with sizes: the ﬁrst
convolutional layer changes the number of channels other has the
same size per stack.

used in all our experiments is a common truncated normal
distribution with a standard deviation of 0.1.

Experiments on CIFAR-10: For the CIFAR-10 dataset,
we used the ResNet architecture2. The architecture con-
sists of three stacks, each consisting of two blocks. Every
block contains batch-normalization - convolution - batch-
normalization - convolution. The ﬁrst convolution maps the
input channels to a different number of output channels and,
as was explained in section 3, CoL requires that the number
of input and output channels be equal. See Table 1.

The results are reported in Figure 5. We compared the
performance of networks where we removed all instances
of conv2 from a stack (see Table 1) completely or replaced
them with CoL. Since there are three stacks we have tried
all possible combinations of stacks. The graph shows the
test error as a function of compression ratio and emphasizes
the gap in test error between different types of blocks. The
original block, which is marked with ”X” in Figure 5, has
the lowest test error, but also the largest number of param-
eters (its compression ratio is 1 and every other network is
compared to it). As can be seen, CoL does not help per-
formance at the early stages of the network and, in some
cases, slightly hurts performance. However, using CoL in
the third stack consistently reduces the error by about 1%,
while cutting the total number of parameters by almost a
half.

Ablation test on CoL components: We conducted an
ablation test to measure the contribution of the spatial term
and the co-occurrence term to the performance of CoL.
Speciﬁcally, we replaced both conv2 layers of the third
stack of the ResNet architecture. Then, we evaluated two

2Taken from https://github.com/seansoleyman/cifar10-resnet.

Figure 5. CIFAR10 test results: The x-axis denotes the compres-
sion ratio of the number parameters and the y-axis denotes the test
error. Each point corresponds to a different conﬁguration of the
network with and without CoL. ”w/o CoL” means dropping the
conv2 layer in that stack and ”w/ CoL” means replacing the conv2
layer with a CoL. The text indicates which stacks were modiﬁed
(i.e., ”stacks(2,3)” means that we modiﬁed the second and third
stacks). The result of the original network is marked by ’X’. As
can be seen, the modifying of the last stack (i.e., stack number
3) leads to a decrease of between 0.5% to 1% in the error, while
cutting the number of parameters by approximately half.

variants: in the ﬁrst, CoL contains only the L[5 × 5] term,
and in the second, CoL only uses w[3 × 3 × 3]. The test er-
ror in both cases was 8.53. Only when we trained with both
the spatial and co-occurrence terms did the test error drop
to 8.13, which is similar to the error of the original network.
Pruning experiment: In another experiment, we com-
pared CoL with two pruning techniques to see how much
can a convolutional layer be compressed using existing
techniques. Speciﬁcally, we evaluated two methods. The
ﬁrst is a magnitude-based method: prune weights with mag-
nitude less than a chosen threshold. The second method cal-
culates an SVD decomposition of the weights, set to zero a
ﬁxed number of eigenvalues and reconstructs the weights
with the truncated eigenvalues. This experiment was con-
ducted on the third stack of the network. In both methods
we pruned the network to be of size as similar as possible to
the size of the network with CoL in the third stack. Results
are reported in Table 2. As can be seen, both magnitude and
SVD-based pruning give inferior results to CoL.

Distribution of ﬁlters: CoL can encode a large number
of different ﬁlters in a compact representation. We suspect
that at the deeper layers of a network the number of different
ﬁlters captured by CoL is higher than in the early layers, and
this might lead to increased representation power that leads
to improved performance. To quantify this, we collected
the ﬁlters used by CoL, clustered them into prototypes us-

4802

0.40.50.60.70.80.91.0compression ratio8.08.59.09.510.010.511.0test error(%)stacks(1)stacks(2)stacks(1,2)stacks(3)stacks(1,3)stacks(2,3)stacks(1,2,3)w/o CoLw/ CoLoriginalMethod

Test error(%)

Original network
Magnitude-based pruning
SVD-based pruning
CoL (ours)

8.11
9.57
8.75
8.13

Table 2. Pruning evaluation: We compare two pruning tech-
niques to CoL. As can be seen, both magnitude and SVD-based
pruning give inferior results to CoL. Compression ratio for all
methods is about 55%.

Figure 6. Distribution of ﬁlters by prototype (i.e., clusters), in the
CIFAR-10 experiment, for the ﬁrst and third stacks. As can be
seen, the ﬁrst prototype of the ﬁrst stack accounts for almost 70%
of the ﬁlters generated by CoL. In contrast, the ﬁrst prototype of
the third stack captures only 50%. See text for more details.

ing k-means, and calculated a histogram of ﬁlters. That is,
we count, for each prototype (i.e., cluster center) ﬁlter, how
many similar ﬁlters were generated by the CoL layer.

Figure 6 shows the distribution of ﬁlter prototypes for the
CIFAR-10 experiments. In particular, we collected statis-
tics from the ﬁrst and third stacks. As can be seen, the ﬁrst
prototype of the ﬁrst stack accounts for almost 70% of the
ﬁlters generated by CoL. In contrast, the ﬁrst prototype of
the third stack accounts for only 50%. This shows a corre-
lation between the distribution of ﬁlters generated by CoL
and its performance. The more uniform the distribution, the
higher the performance of CoL.

Experiments on ADE20k: We tested CoL on the MIT
Scene Parsing Benchmark that is based on the ADE20k
dataset [20]. This dataset contains 22k scene-centric im-
ages with 150 semantic categories. The training set consists
of 20k images and we report results on the validation set that
consists of 2k images. Our network is largely based on the
Fully Connected Network (FCN) of [13]3. The input to the

Accuracy Total IoU Score

1st conv(7 × 7) (orig.)
1st conv(3 × 3)
1st conv(3 × 3)/w CoL
without 2nd conv
CoL as 2nd layer

62.69
62.02
64.41
62.09
63.27

0.1362
0.1374
0.1618
0.1218
0.1332

0.3815
0.3788
0.4029
0.3713
0.3829

Table 3. ADE20k result: Results (on validation set) of semantic
segmentation network with and without CoL. Reducing the ﬁlter
size from the original size of (7 × 7) to (3 × 3) cuts the number
of parameters of this layer by half but hurts performance slightly.
Adding a single CoL layer increases performance by about 1%. A
similar behaviour is observed for the second layer. See details of
different architectures in Table 4.

network is the result of a preprocessing stage that takes each
image from the dataset and passes it through VGG19 [14].

We evaluated a number of different architectures, as
shown in Table 4. In the ﬁrst experiment the ﬁrst convoluti-
nal layer of size [7 × 7 × 512 × 4096] is replaced by a layer
with a smaller spatial support of size [3 × 3 × 512 × 4096],
followed by a CoL of size L[5 × 5] and w[7 × 7 × 13]. As
shown in Table 3 we have reduced the number of parame-
ters by a factor of 9
49 while increasing accuracy by 2.4%.
In the second experiment we replaced the second convolu-
tion with CoL. In this case replacing a convolutional layer
of size [1 × 1 × 4096 × 4096] with a CoL of size L[5 × 5]
and w[7 × 7 × 13] increased accuracy by 1%.

5. Conclusions

We proposed a new ﬁlter that augments regular ﬁlters
with a term that is based on co-occurrence statistics. The re-
sulting ﬁlter is differentiable and can be trained using back-
propagation.

The ﬁlter is not shift-invariant and as such can adapt to
different regions of the input image.
In addition, the co-
occurrence term lets the ﬁlter handle local geometric de-
formations in the image plane. The ﬁlter is deﬁned by a
small number of parameters yet can generate many differ-
ent regular ﬁlters, based on input content. This property lets
us replace many regular ﬁlters with a single co-occurrence
based ﬁlter.

We deﬁned a new layer, termed Co-Occurrence Layer
(CoL), that encapsulates this ﬁlter and have shown that it
can be used in different places of a network. Finally, exper-
iments on two data sets demonstrate the advantages of our
method.

Acknowledgments: Parts of this research were sup-

ported by ISF grant 1917/15.

3We

use

the

TensorFlow

implementation

found

at

https://github.com/shekkizh/FCN.tensorﬂow.

4803

012345678910prototype0.00.10.20.30.40.50.60.7fractionCIFAR-10 CoL input first stackCIFAR-10 CoL input third stackLayer Type
Conv1
Conv2
Conv3

Layer Type
Conv1
Conv2
Conv3

Layer Type
Conv1
CoL
Conv2
Conv3

Layer Type
Conv1
Conv3

Layer Type
Conv1
CoL
Conv3

Original network

Filter Shape
7 × 7 × 4096
1 × 1 × 4096
1 × 1 × 150
Modiﬁed Conv1

Filter Shape
3 × 3 × 4096
1 × 1 × 4096
1 × 1 × 150

Modiﬁed Conv1 + CoL

Filter Shape
3 × 3 × 4096

L[5 × 5], w[7 × 7 × 13]

1 × 1 × 4096
1 × 1 × 150
W/O Conv2

Filter Shape
3 × 3 × 4096
1 × 1 × 150

Input Size
7 × 7 × 512
7 × 7 × 4096
7 × 7 × 150

Input Size
7 × 7 × 512
7 × 7 × 4096
7 × 7 × 150

Input Size
7 × 7 × 512
7 × 7 × 4096
7 × 7 × 4096
7 × 7 × 150

Input Size
7 × 7 × 512
7 × 7 × 4096

active Techniques, SIGGRAPH ’02, pages 257–266. ACM,
2002.

[6] M. Gharbi, J. Chen, J. T. Barron, S. W. Hasinoff, and F. Du-
rand. Deep bilateral learning for real-time image enhance-
ment. ACM Trans. Graph., 36(4):118:1–118:12, 2017.

[7] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural network with pruning, trained quanti-
zation and huffman coding. CoRR, abs/1510.00149, 2015.

[8] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up
convolutional neural networks with low rank expansions. In
British Machine Vision Conference, BMVC 2014, Notting-
ham, UK, September 1-5, 2014, 2014.

[9] V. Jampani, M. Kiefel, and P. V. Gehler. Learning sparse high
dimensional ﬁlters: Image ﬁltering, dense crfs and bilateral
neural networks. In The IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), June 2016.

[10] R. J. Jevnisek and S. Avidan. Co-occurrence ﬁlter. In 2017
IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017,
pages 3816–3824, 2017.

[11] A. Krizhevsky. Learning multiple layers of features from

tiny images. 2009.

[12] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR,

CoL instead of Conv2

abs/1312.4400, 2013.

Filter Shape
3 × 3 × 4096

L[5 × 5], w[7 × 7 × 3]

1 × 1 × 150

Input Size
7 × 7 × 512
7 × 7 × 4096
7 × 7 × 4096

Table 4. FCN: The table describes the ﬁve different architectures
evaluated on ADE20k. See results of different architectures in Ta-
ble 3.

References

[1] P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende, and
k. kavukcuoglu. Interaction networks for learning about ob-
jects, relations and physics.
In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances
in Neural Information Processing Systems 29, pages 4502–
4510. Curran Associates, Inc., 2016.

[2] N. Cohen, O. Sharir, and A. Shashua. Deep simnets. In 2016
IEEE Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016,
pages 4782–4791, 2016.

[3] T. S. Cohen and M. Welling.

Steerable cnns. CoRR,

abs/1612.08498, 2016.

[4] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-
gus. Exploiting linear structure within convolutional net-
works for efﬁcient evaluation. In Advances in Neural Infor-
mation Processing Systems 27: Annual Conference on Neu-
ral Information Processing Systems 2014, December 8-13
2014, Montreal, Quebec, Canada, pages 1269–1277, 2014.

[5] F. Durand and J. Dorsey. Fast bilateral ﬁltering for the dis-
play of high-dynamic-range images. In Proceedings of the
29th Annual Conference on Computer Graphics and Inter-

[13] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
IEEE Trans. Pattern

networks for semantic segmentation.
Anal. Mach. Intell., 39(4):640–651, Apr. 2017.

[14] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[15] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H.
Yang, and J. Kautz. Splatnet: Sparse lattice networks for
point cloud processing. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018.

[16] C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and
color images. In Proceedings of the Sixth International Con-
ference on Computer Vision, ICCV ’98, pages 839–, Wash-
ington, DC, USA, 1998. IEEE Computer Society.

[17] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural
networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018.

[18] M. Weiler, F. A. Hamprecht, and M. Storath. Learning steer-
able ﬁlters for rotation equivariant cnns. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2018.

[19] H. Wu, S. Zheng, J. Zhang, and K. Huang. Fast end-to-end
trainable guided ﬁlter. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018.

[20] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Tor-
In Proceed-
ralba. Scene parsing through ade20k dataset.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017.

4804

