Pluralistic Image Completion

Chuanxia Zheng

Tat-Jen Cham

Jianfei Cai

School of Computer Science and Engineering
Nanyang Technological University, Singapore
{chuanxia001,astjcham,asjfcai}@ntu.edu.sg

Figure 1. Example completion results of our method on images of a face, a building, and natural scenery with various masks (missing
regions shown in white). For each group, the masked input image is shown left, followed by sampled results from our model without any
post-processing. The results are diverse and plausible. (Zoom in to see the details.)

Abstract

1. Introduction

Most image completion methods produce only one result
for each masked input, although there may be many reason-
able possibilities. In this paper, we present an approach for
pluralistic image completion – the task of generating mul-
tiple and diverse plausible solutions for image completion.
A major challenge faced by learning-based approaches is
that usually only one ground truth training instance per la-
bel. As such, sampling from conditional VAEs still leads
to minimal diversity. To overcome this, we propose a novel
and probabilistically principled framework with two paral-
lel paths. One is a reconstructive path that utilizes the only
one given ground truth to get prior distribution of missing
parts and rebuild the original image from this distribution.
The other is a generative path for which the conditional
prior is coupled to the distribution obtained in the recon-
structive path. Both are supported by GANs. We also in-
troduce a new short+long term attention layer that exploits
distant relations among decoder and encoder features, im-
proving appearance consistency. When tested on datasets
with buildings (Paris), faces (CelebA-HQ), and natural im-
ages (ImageNet), our method not only generated higher-
quality completion results, but also with multiple and di-
verse plausible outputs.

Image completion is a highly subjective process. Sup-
posing you were shown the various images with missing
regions in ﬁg. 1, what would you imagine to be occupying
these holes? Bertalmio et al. [4] related how expert con-
servators would inpaint damaged art by: 1) imagining the
semantic content to be ﬁlled based on the overall scene; 2)
ensuring structural continuity between the masked and un-
masked regions; and 3) ﬁlling in visually realistic content
for missing regions. Nonetheless, each expert will indepen-
dently end up creating substantially different details, even if
they may universally agree on high-level semantics, such as
general placement of eyes on a damaged portrait.

Based on this observation, our main goal is thus to gen-
erate multiple and diverse plausible results when presented
with a masked image — in this paper we refer to this task
as pluralistic image completion (depicted in ﬁg. 1). This
is as opposed to approaches that attempt to generate only a
single “guess” for missing parts.

Early image completion works [4, 7, 5, 8, 3, 13] fo-
cus only on steps 2 and 3 above, by assuming that gaps
should be ﬁlled with similar content to that of the back-
ground. Although these approaches produced high-quality
texture-consistent images, they cannot capture global se-
mantics and hallucinate new content for large holes. More
recently, some learning-based image completion methods
[29, 14, 39, 40, 42, 24, 38] were proposed that infer seman-

43211438

tic content (as in step 1). These works treated completion
as a conditional generation problem, where the input-to-
output mapping is one-to-many. However, these prior works
are limited to generate only one “optimal” result, and do
not have the capacity to generate a variety of semantically
meaningful results.

To obtain a diverse set of results, some methods utilize
conditional variational auto-encoders (CVAE) [34, 37, 2,
10], a conditional extension of VAE [19], which explicitly
code a distribution that can be sampled. However, speciﬁ-
cally for an image completion scenario, the standard single-
path formulation usually leads to grossly underestimating
variances. This is because when the condition label is it-
self a masked image, the number of instances in the training
data that match each label is typically only one. Hence the
estimated conditional distributions tend to have very limited
variation since they were trained to reconstruct the single
ground truth. This is further elaborated on in section 3.1.

An important insight we will use is that partial images,
as a superset of full images, may also be considered as gen-
erated from a latent space with smooth prior distributions.
This provides a mechanism for alleviating the problem of
having scarce samples per conditional partial image. To do
so, we introduce a new image completion network with two
parallel but linked training pipelines. The ﬁrst pipeline is a
VAE-based reconstructive path that not only utilizes the full
instance ground truth (i.e. both the visible partial image, as
well as its complement — the hidden partial image), but
also imposes smooth priors for the latent space of comple-
ment regions. The second pipeline is a generative path that
predicts the latent prior distribution for the missing regions
conditioned on the visible pixels, from which can be sam-
pled to generate diverse results. The training process for
the latter path does not attempt to steer the output towards
reconstructing the instance-speciﬁc hidden pixels at all, in-
stead allowing the reasonableness of results be driven by
an auxiliary discriminator network [11]. This leads to sub-
stantially great variability in content generation. We also
introduce an enhanced short+long term attention layer that
signiﬁcantly increases the quality of our results.

We compared our method with existing state-of-the-art
approaches on multiple datasets. Not only can higher-
quality completion results be generated using our approach,
it also presents multiple diverse solutions.

The main contributions of this work are:

1. A probabilistically principled framework for image
completion that is able to maintain much higher sam-
ple diversity as compared to existing methods;

2. A new network structure with two parallel training
paths, which trades off between reconstructing the
original training data (with loss of diversity) and main-
taining the variance of the conditional distribution;

3. A novel self-attention layer that exploits short+long
term context information to ensure appearance consis-
tency in the image domain, in a manner superior to
purely using GANs; and

4. We demonstrate that our method is able to complete
the same mask with multiple plausible results that have
substantial diversity, such as those shown in ﬁgure 1.

2. Related Work

Existing work on image completion either uses informa-
tion from within the input image [4, 5, 3], or information
from a large image dataset [12, 29, 42]. Most approaches
will generate only one result per masked image.
Intra-Image Completion Traditional intra-image comple-
tion, such as diffusion-based methods [4, 1, 22] and patch-
based methods [5, 7, 8, 3], assume image holes share similar
content to visible regions; thus they would directly match,
copy and realign the background patches to complete the
holes. These methods perform well for background comple-
tion, e.g. for object removal, but cannot hallucinate unique
content not present in the input images.
Inter-Image Completion To generate semantically new
content, inter-image completion borrows information from
a large dataset. Hays and Efros [12] presented an image
completion method using millions of images, in which the
image most similar to the masked input is retrieved, and cor-
responding regions are transferred. However, this requires
a high contextual match, which is not always available. Re-
cently, learning-based approaches were proposed.
Initial
works [20, 30] focused on small and thin holes. Context en-
coders (CE) [29] handled 64×64-sized holes using GANs
[11]. This was followed by several CNN-based methods,
which included combining global and local discriminators
as adversarial loss [14], identifying closest features in the
latent space of masked images [40], utilizing semantic la-
bels to guide the completion network [36], introducing ad-
ditional face parsing loss for face completion [23], and de-
signing particular convolutions to address irregular holes
[24, 41]. A common drawback of these methods is that they
often create distorted structures and blurry textures incon-
sistent with the visible regions, especially for large holes.
Combined Intra- and Inter-Image Completion To over-
come the above problems, Yang et al. [39] proposed
multi-scale neural patch synthesis, which generates high-
frequency details by copying patches from mid-layer fea-
tures. However, this optimization is computational costly.
More recently, several works [42, 38, 35] exploited spatial
attention [16, 46] to get high-frequency details. Yu et al.
[42] presented a contextual attention layer to copy similar
features from visible regions to the holes. Yan et al. [38]
and Song et al. [35] proposed PatchMatch-like ideas on fea-
ture domain. However, these methods identify similar fea-

1439

tures by comparing features of holes and features of visible
regions, which is somewhat contradictory as feature trans-
fer is unnecessary when two features are very similar, but
when needed the features are too different to be matched
easily. Furthermore, distant information is not used for new
content that differs from visible regions. Our model will
solve this problem by extending self-attention [43] to har-
ness abundant context.
Image Generation Image generation has progressed sig-
niﬁcantly using methods such as VAE [19] and GANs [11].
These have been applied to conditional image generation
tasks, such as image translation [15], synthetic to realis-
tic [44], future prediction [27], and 3D models [28]. Per-
haps most relevant are conditional VAEs (CVAE) [34, 37]
and CVAE-GAN [2], but these were not specially targeted
for image completion. CVAE-based methods are most use-
ful when the conditional labels are few and discrete, and
there are sufﬁcient training instances per label. Some re-
cent work utilizing these in image translation can produce
diverse output [47, 21], but in such situations the condition-
to-sample mappings are more local (e.g. pixel-to-pixel), and
only change the visual appearance. This is untrue for image
completion, where the conditional label is itself the masked
image, with only one training instance of the original holes.
In [6], different outputs were obtained for face completion
by specifying facial attributes (e.g. smile), but this method
is very domain speciﬁc, requiring targeted attributes.

3. Approach

Suppose we have an image, originally Ig, but degraded
by a number of missing pixels to become Im (the masked
partial image) comprising the observed / visible pixels. We
also deﬁne Ic as its complement partial image comprising
the original missing pixels. Classical image completion
methods attempt to reconstruct the original unmasked im-
age Ig in a deterministic fashion from Im (see ﬁg. 2 “Deter-
ministic”). This results in only a single solution. In contrast,
our goal is to sample from p(Ic|Im).

3.1. Probabilistic Framework

In order to have a distribution to sample from, a cur-
rent approach is to employ the CVAE [34] which estimates
a parametric distribution over a latent space, from which
sampling is possible (see ﬁg. 2 “CVAE”). This involves a
variational lower bound of the conditional log-likelihood of
observing the training instances:

log p(Ic|Im) ≥ − KL(qψ(zc|Ic, Im)||pφ(zc|Im))
qψ(zc|Ic,Im)[log pθ(Ic|zc, Im)]

+ E

(1)

where zc is the latent vector, qψ(·|·) the posterior im-
portance sampling function, pφ(·|·) the conditional prior,

Reconstructed
Target

Output

Decoder

Concat/Add

Sample

Inference

Encoder

Input

Deterministic

CVAE

Instance Blind

Ours

Figure 2. Completion strategies given masked input. (Determinis-
tic) structure directly predicts the ground truth instance. (CVAE)
adds in random sampling to diversify the output. (Instance Blind)
only matches the visible parts, but training is unstable. (Ours) uses
a generative path during testing, but is guided by a parallel recon-
structive path during training. Yellow path is used for training.

pθ(·|·) the likelihood, with ψ, φ and θ being the deep net-
work parameters of their corresponding functions. This
lower bound is maximized w.r.t. all parameters.

c ), where z∗

For our purposes, the chief difﬁculty of using CVAE [34]
directly is that the high DoF networks of qψ(·|·) and pφ(·|·)
are not easily separable in (1) with the KL distance eas-
ily driven towards zero, and is approximately equivalent
pφ(zc|Im)[log pθ(Ic|zc, Im)] (the “GSNN”
to maximizing E
variant in [34]). This consequently learns a delta-like prior
of pφ(zc|Im) → δ(zc − z∗
c is the maximum
latent likelihood point of pθ(Ic|·, Im). While this low vari-
ance prior may be useful in estimating a single solution,
sampling from it will lead to negligible diversity in image
completion results (as seen in ﬁg. 9). When the CVAE vari-
ant of [37], which has a ﬁxed latent prior, is used instead, the
network learns to ignore the latent sampling and directly es-
timates Ic from Im, also resulting in a single solution. This
is due to the image completion scenario when there is only
one training instance per condition label, which is a partial
image Im. Details are in the supplemental section ??.

A possible way to diversify the output is to simply not
incentivize the output to reconstruct the instance-speciﬁc Ig
during training, only needing it to ﬁt in with the training set
distribution as deemed by an learned adversarial discrimi-
nator (see ﬁg. 2 “Instance Blind”). However, this approach
is unstable, especially for large and complex scenes [35].
Latent Priors of Holes In our approach, we require that
missing partial images, as a superset of full images, to also
arise from a latent space distribution, with a smooth prior
of p(zc). The variational lower bound is:

log p(Ic) ≥ − KL(qψ(zc|Ic)||p(zc))

+ E

qψ(zc|Ic)[log pθ(Ic|zc)]

(2)

1440

Figure 3. Overview of our architecture with two parallel pipelines. The reconstructive pipeline (yellow line) combines information from
Im and Ic, which is used only for training. The generative pipeline (blue line) infers the conditional distribution of hidden regions, that
can be sampled during testing. Both representation and generation networks share identical weights.

where in [19] the prior is set as p(zc) = N (0, I). However,
we can be more discerning when it comes to partial images
since they have different numbers of pixels. A missing par-
tial image Ic with more pixels (larger holes) should have
greater latent prior variance than a missing partial image
Ic with fewer pixels (smaller holes). Hence we generalize
the prior p(zc) = Nm(0, σ2(n)I) to adapt to the number of
pixels n.

Prior-Conditional Coupling Next, we combine the la-
tent priors into the conditional lower bound of (1). This
can be done by assuming zc is much more closely related to
Ic than to Im, so qψ(zc|Ic, Im)≈qψ(zc|Ic). Updating (1):

log p(Ic|Im) ≥ − KL(qψ(zc|Ic)||pφ(zc|Im))

+ E

qψ(zc|Ic)[log pθ(Ic|zc, Im)]

(3)

However, unlike in (1), notice that qψ(zc|Ic) is no longer
freely learned during training, but is tied to its presence in
(2). Intuitively, the learning of qψ(zc|Ic) is regularized by
the prior p(zc) in (2), while the learning of the conditional
prior pφ(zc|Im) is in turn regularized by qψ(zc|Ic) in (3).

Reconstruction vs Creative Generation One issue with
(3) is that the sampling is taken from qψ(zc|Ic) during train-
ing, but is not available during testing, whereupon sampling
must come from pφ(zc|Im) which may not be adequately
learned for this role. In order to mitigate this problem, we
modify (3) to have a blend of formulations with and without
importance sampling. So, with simpliﬁed notation:

log p(Ic|Im) ≥λ (cid:8)Eqψ [log pr

+ (1 − λ) Epφ [log pg

θ(Ic|zc, Im)] − KL(qψ||pφ)(cid:9)
(4)

θ(Ic|zc, Im)]

where 0 ≤ λ ≤ 1 is implicitly set by training loss co-
efﬁcients in section 3.3. When sampling from the impor-
tance function qψ(·|Ic), the full training instance is available
and we formulate the likelihood pr
θ(Ic|zc, Im) to be focused
on reconstructing Ic. Conversely, when sampling from the
learned conditional prior pφ(·|Im) which does not contain
Ic, we facilitate creative generation by having the likeli-
hood model pg
θ(zc, Im) be independent of
the original instance of Ic. Instead it only encourages gen-
erated samples to ﬁt in with the overall training distribution.
Our overall training objective may then be expressed as
jointly maximizing the lower bounds in (2) and (4), with
the likelihood in (2) uniﬁed to that in (4) as pθ(Ic|zc) ∼=
pr
θ(Ic|zc, Im). See the supplemental section ??.

θ(Ic|zc, Im) ∼= ℓg

3.2. Dual Pipeline Network Structure

This formulation is implemented as our dual pipeline
framework, shown in ﬁg. 3. It consists of two paths: the
upper reconstructive path uses information from the whole
image, i.e. Ig={Ic, Im}, while the lower generative path
only uses information from visible regions Im. Both repre-
sentation and generation networks share identical weights.
Speciﬁcally:

• For the upper reconstructive path, the complement par-
tial image Ic is used to infer the importance function
qψ(·|Ic)=Nψ(·) during training. The sampled latent
vector zc thus contains information of the missing re-
gions, while the conditional feature fm encodes the in-
formation of the visible regions. Since there is suf-
ﬁcient information, the loss function in this path is
geared towards reconstructing the original image Ig.

• For the lower generative path, which is also the test

1441

++samplesampleInf1Inf2Representation NetworkInference NetworkGeneration NetworkDiscriminator NetworkReconstructiveGenerativeResidual EncoderResidual DecoderShort+Long Term Attention Inf11 Residual Block7 Residual BlocksInf2Residual Encoder StartTraining and TestingTrainingDiscriminator 1Discriminator 2Distribution Link path, the latent distribution of the holes Ic is inferred
based only on the visible Im. This would be signif-
icantly less accurate than the inference in the upper
path. Thus the reconstruction loss is only targeted at
the visible regions Im (via fm).

• In addition, we also utilize adversarial learning net-
works on both paths, which ideally ensure that the full
synthesized data ﬁt in with the training set distribution,
and empirically leads to higher quality images.

3.3. Training Loss

Various terms in (2) and (4) may be more conventionally
expressed as loss functions. Jointly maximizing the lower
bounds is then minimizing a total loss L, which consists of
three groups of component losses:

ad + Lg

KL + Lg

app + Lg

L = αKL(Lr

app) + αad(Lr

KL) + αapp(Lr

ad)
(5)
where the LKL group regularizes consistency between pairs
of distributions in terms of KL divergences, the Lapp group
encourages appearance matching ﬁdelity, and while the Lad
group forces sampled images to ﬁt in with the training set
distribution. Each of the groups has a separate term for the
reconstructive and generative paths.

Distributive Regularization The typical interpretation of
the KL divergence term in a VAE is that it regularizes the
learned importance sampling function qψ(·|Ic) to a ﬁxed la-
tent prior p(zc). Deﬁning as Gaussians, we get:

Lr,(i)

KL = −KL(qψ(z|I (i)

c )||Nm(0, σ2(n)I))

(6)

For the generative path, the appropriate interpretation
is reversed: the learned conditional prior pφ(·|Im), also a
Gaussian, is regularized to qψ(·|Ic).

Lg,(i)

KL = −KL(qψ(z|I (i)

c ))||pφ(z|I (i)

m )))

(7)

Note that the conditional prior only uses Im, while the im-
portance function has access to the hidden Ic.

likelihood

Appearance Matching Loss The
term
pr
θ(Ic|zc, Im) may be interpreted as probabilistically en-
couraging appearance matching to the hidden Ic. However,
our framework also auto-encodes the visible Im deter-
ministically, and the loss function needs to cater for this
reconstruction. As such, the per-instance loss here is:

Lr,(i)

app = ||I (i)

rec − I (i)

g ||1

(8)

rec =G(zc, fm) and I (i)

where I (i)
are the reconstructed and
original full images respectively. In contrast, for the gener-
ative path we ignore instance-speciﬁc appearance matching
for Ic, and only focus on reconstructing Im (via fm):

g

Lg,(i)

app = ||M ∗ (I (i)

gen − I (i)

g )||1

(9)

1442

1x1 Con v

Softmax(QTQ)

Enco der Feature 
Maps C2xHxW

Con textu al Flow

Qu ery Feature 
Maps C1/4xHxW

Attention Maps

Deco der Featu re 

Maps C1xHxW

Self-Attention

Figure 4. Our short+long term attention layer. The attention map
is directly computed on the decoder features. After obtaining the
self-attention scores, we use these to compute self-attention on de-
coder features, as well as contextual ﬂow on encoder features.

where I (i)
gen=G(˜zc, fm) is the generated image from the ˜zc
sample, and M is the binary mask selecting visible pixels.

Adversarial Loss The formulation of pr
θ(Ic|zc, Im) and
the instance-blind pg
θ(Ic|˜zc, Im) also incorporates the use
of adversarially learned discriminators D1 and D2 to judge
whether the generated images ﬁt into the training set distri-
bution. Inspired by [2], we use a mean feature match loss in
the reconstructive path for the generator,

Lr,(i)
ad = ||fD1 (I (i)

rec ) − fD1 (I (i)

g )||2

(10)

where fD1 (·) is the feature output of the ﬁnal layer of D1.
This encourages the original and reconstructed features in
the discriminator to be close together. Conversely, the ad-
versarial loss in the generative path for the generator is:

Lg,(i)
ad = [D2(I (i)

gen) − 1]2

(11)

This is based on the generator loss in LSGAN [26], which
performs better than the original GAN loss [11] in our sce-
nario. The discriminator loss for both D1 and D2 is also
based on LSGAN.

3.4. Short+Long Term Attention

Extending beyond the Self-Attention GAN [43], we pro-
pose not only to use the self-attention map within a de-
coder layer to harness distant spatial context, but also to
further capture feature-feature context between encoder and
decoder layers. Our key novel insight is: doing so would al-
low the network a choice of attending to the ﬁner-grained
features in the encoder or the more semantically generative
features in the decoder, depending on circumstances.

Our proposed structure is shown in ﬁg. 4. We ﬁrst calcu-
late the self-attention map from the features fd of a decoder
middle layer, using the attention score of:

βj,i =

exp(sij)
i=1 exp(sij)

PN

, where sij = Q(fdi)T Q(fdj),

(12)

N is the number of pixels, Q(fd)=Wqfd, and Wq is a 1x1
convolution ﬁlter. This leads to the short-term intra-layer
attention feature (self-attention in ﬁg. 4) and the output yd:

cdj =

N

X

i=1

βj,ifdi ,

yd = γdcd + fd

(13)

where, following [43], we use a scale parameter γd to bal-
ance the weights between cd and fd. The initial value of γd
is set to zero. In addition, for attending to features fe from
an encoder layer, we have a long-term inter-layer attention
feature (contextual ﬂow in ﬁg. 4) and the output ye:

cej =

N

X

i=1

βj,ifei ,

ye = γe(1 − M)ce + Mfe

(14)

As before, a scale parameter γe is used to combine the en-
coder feature fe and the attention feature ce. However, un-
like the decoder feature fd which has information for gen-
erating a full image, the encoder feature fe only represents
visible parts Im. Hence, a binary mask M (holes=0) is used.
Finally, both the short and long term attention features are
aggregated and fed into further decoder layers.

4. Experimental Results

We evaluated our proposed model on four datasets in-
cluding Paris [9], CelebA-HQ [25, 17], Places2 [45], and
ImageNet [31] using the original training and test splits for
those datasets. Since our model can generate multiple out-
puts, we sampled 50 images for each masked image, and
chose the top 10 results based on the discriminator scores.
We trained our models for both regular and irregular holes.
For brevity, we refer to our method as PICNet. We provide
PyTorch implementations and interactive demo.

4.1. Implementation Details

Our generator and discriminator networks are inspired
by SA-GAN [43], but with several important modiﬁcations,
including the short+long term attention layer. Furthermore,
inspired by the growing-GAN [17], multi-scale output is ap-
plied to make the training faster.

The image completion network, implemented in Pytorch
v0.4.0, contains 6M trainable parameters. During opti-
mization, the weights of different losses are set to αKL =
αrec=20, αad=1. We used Orthogonal Initialization [33]
and the Adam solver [18]. All networks were trained from
scratch, with a ﬁxed learning rate of λ=10-4. Details are in
the supplemental section ??.

to get diverse but reasonable solutions for one masked im-
age. The original image is only one solution of many, and
comparisons should not be made based on just this image.

However, just for the sake of obtaining quantitative mea-
sures, we will assume that one of our top 10 samples
(ranked by the discriminator) will be close to the original
ground truth, and select the single sample with the best bal-
ance of quantitative measures for comparison. The com-
parison is conducted on ImageNet 20, 000 test images, with
quantitative measures of mean ℓ1 loss, peak signal-to-noise
ration (PSNR), total variation (TV), and Inception Score
(IS) [32]. We used a 128 × 128 mask in the center.

Method
GL [14]
CA [42]

PICNet-regular

ℓ1 loss
15.32
13.57
12.91

PSNR TV loss
13.97
19.36
19.55
19.22
20.10
12.18

IS

24.31
28.80
24.90

Table 1. Quantitative comparison with state-of-the-art. For center
masks, our model was trained on regular holes.

Qualitative Comparisons First, we show the results in
ﬁg. 5 on the Paris dataset [9]. For fair comparison among
learning-based methods, we only compared with those
trained on this dataset. PatchMatch [3] worked by copy-
ing similar patches from visible regions and obtained good
results on this dataset with repetitive structures. Context En-
coder (CE) [29] generated reasonable structures with blurry
textures. Shift-Net [38] made improvements by feature
copying. Compared to these, our model not only generated
more natural images, but also with multiple solutions, e.g.
different numbers of windows and varying door sizes.

Next, we evaluated our methods on CelebA-HQ face
dataset, with ﬁg. 6 showing examples with large regular
holes to highlight the diversity of our output. Context Atten-
tion (CA) [42] generated reasonable completion for many
cases, but for each masked input they were only able to gen-
erate a single result; furthermore, on some occasions, the
single solution may be poor. Our model produced various
plausible results by sampling from the latent space condi-
tional prior.

Finally, we report the performance on the more challeng-
ing ImageNet dataset by comparing to the previous Patch-
Match [3], CE [29], GL [14] and CA [42]. Different from
the CE and GL models that were trained on the 100k sub-
set of training images of ImageNet, our model is directly
trained on original ImageNet training dataset with all im-
ages resized to 256 × 256. Visual results on a variety of ob-
jects from the validation set are shown in ﬁg. 7. Our model
was able to infer the content quite effectively.

4.2. Comparison with Existing Work

4.3. Ablation Study

Quantitative Comparisons Quantitative evaluation is
hard for the pluralistic image completion task, as our goal is

Our PICNet vs CVAE vs “Instance Blind” vs Bicycle-
GAN We investigated the inﬂuence of using our two-path

1443

(a) Input

(b) PM

(c) CE

(d) Shift-Net

(e) PICNet-regular

Figure 5. Comparison of our model with PatchMatch(PM) [3], Context Encoder(CE) [29] and Shift-Net [38] on images taken from the
Paris [9] test set for center region completion. Best viewed by zooming in.

(a) Input

(b) CA

(c) PICNet-random

Figure 6. Comparison of our model with Contextual Attention(CA) [42] on CelebA-HQ. Best viewed by zooming in.

(a) Input

(b) PM

(c) CE

(d) GL

(e) CA

(f) PICNet-regular

Figure 7. Qualitative results and comparisons with the PM, CE, Global and Local(GL) [14] and CA on the ImageNet validation set.

1444

(a) Input

(b) BicycleGAN

(c) PICNet

Figure 8. Comparison of our Pluralistic model with BicycleGAN.

Method
CVAE

Instance Blind

BicycleGAN [46]
PICNet-Pluralistic

Diversity (LPIPS)
Iout(m)
Iout
0.014
0.004
0.049
0.015
0.027
0.060
0.088
0.029

Table 2. Quantitative comparisons of diversity.

Figure 9. Comparison of training with different strategies: ours
(top), CVAE (middle), instance-blind (bottom).

training structure in comparison to other variants such as
the CVAE [34] and “instance blind” structures in ﬁg. 2.
We trained the three models using common parameters. As
shown in ﬁg. 9, for the CVAE, even after sampling from
the latent prior distribution, the outputs were almost iden-
tical, as the conditional prior learned is narrowly centered
at the maximum latent likelihood solution. As for “instance
blind”, if reconstruction loss was used only on visible pix-
els, the training may become unstable. If we used recon-
struction loss on the full generated image, there is also lit-
tle variation as the framework has likely learned to ignore
the sampling and predicted a deterministic outcome purely
from Im.

We also trained and tested BicycleGAN [47] for center
masks. As is obvious in ﬁg. 8, BicycleGAN is not directly
suitable, leading to poor results or minimal variation.
Diversity Measure We computed diversity scores using
the LPIPS metric reported in [47]. The average score is cal-
culated between 50K pairs generated from a sampling of
1K center-masked images. Iout and Iout(m) are the full
output and mask-region output, respectively. While [47]
obtained relatively higher diversity scores (still lower than
ours), most of their generated images look unnatural (ﬁg. 8).
Short+Long Term Attention vs Contextual Attention
We visualized our attention maps as in [43]. To compare to
the contextual attention (CA) layer [42], we retrained CA on
the Paris dataset via the authors’ code, and used their pub-
licly released face model. The CA attention maps are pre-
sented in their color-directional format. As shown in ﬁg. 10,
our short+long term attention layer borrowed features from
different positions with varying attention weights, rather

Figure 10. Visualization of attention map using different attention
modules: ours (top), contextual attention (bottom). We highlight
the most-attended regions for the query position (red point).

than directly copying similar features from just one visi-
ble position. For the building scene, CA’s results were of
similar high quality to ours, due to the repeated structures
present. However for a face with a large mask, CA was un-
able to borrow features for the hidden content (e.g. mouth,
eyes) from visible regions, with poor output. Our attention
map is able to utilize both decoder features (which do not
have masked parts) and encoder features as appropriate.

5. Conclusion

We proposed a novel dual pipeline training architecture
for pluralistic image completion. Unlike existing meth-
ods, our framework can generate multiple diverse solutions
with plausible content for a single masked input. The ex-
perimental results demonstrate this prior-conditional lower
bound coupling is signiﬁcant for conditional image genera-
tion. We also introduced an enhanced short+long term at-
tention layer which improves realism. Experiments on a
variety of datasets showed that our multiple solutions were
diverse and of high-quality, especially for large holes.

Acknowledgements This research is supported by the
BeingTogether Centre, a collaboration between Nanyang
Technological University (NTU) Singapore and University
of North Carolina (UNC) at Chapel Hill. The BeingTo-
gether Centre is supported by the National Research Foun-
dation, Prime Ministers Ofﬁce, Singapore under its Inter-
national Research Centres in Singapore Funding Initiative.
This research was also conducted in collaboration with
Singapore Telecommunications Limited and partially sup-
ported by the Singapore Government through the Industry
Alignment Fund - Industry Collaboration Projects Grant.

1445

References

[1] Coloma Ballester, Marcelo Bertalmio, Vicent Caselles,
Guillermo Sapiro, and Joan Verdera. Filling-in by joint inter-
polation of vector ﬁelds and gray levels. IEEE transactions
on image processing, 10(8):1200–1211, 2001.

[2] Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang
Hua. Cvae-gan: Fine-grained image generation through
asymmetric training.
In 2017 IEEE International Confer-
ence on Computer Vision (ICCV), pages 2764–2773. IEEE,
2017.

[3] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and
Dan B Goldman. Patchmatch: A randomized correspon-
dence algorithm for structural image editing. ACM Trans-
actions on Graphics (ToG), 28:24, 2009.

[4] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and
Coloma Ballester.
In Proceedings of
the 27th annual conference on Computer graphics and in-
teractive techniques, pages 417–424. ACM Press/Addison-
Wesley Publishing Co., 2000.

Image inpainting.

[5] Marcelo Bertalmio, Luminita Vese, Guillermo Sapiro, and
Simultaneous structure and texture im-
IEEE transactions on image processing,

Stanley Osher.
age inpainting.
12(8):882–889, 2003.

[6] Zeyuan Chen, Shaoliang Nie, Tianfu Wu, and Christopher G
Healey. High resolution face completion with multiple con-
trollable attributes via fully end-to-end progressive genera-
tive adversarial networks. arXiv preprint arXiv:1801.07632,
2018.

[7] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Ob-
ject removal by exemplar-based inpainting.
In Computer
Vision and Pattern Recognition, 2003. Proceedings. 2003
IEEE Computer Society Conference on, volume 2, pages II–
II. IEEE, 2003.

[8] Antonio Criminisi, Patrick P´erez, and Kentaro Toyama.
Region ﬁlling and object removal by exemplar-based im-
age inpainting.
IEEE Transactions on image processing,
13(9):1200–1212, 2004.

[9] Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic,
and Alexei Efros. What makes paris look like paris? ACM
Transactions on Graphics, 31(4), 2012.

[10] S. M. Ali Eslami, Danilo Jimenez Rezende, Frederic Besse,
Fabio Viola, Ari S. Morcos, Marta Garnelo, Avraham Ruder-
man, Andrei A. Rusu, Ivo Danihelka, Karol Gregor, David P.
Reichert, Lars Buesing, Theophane Weber, Oriol Vinyals,
Dan Rosenbaum, Neil Rabinowitz, Helen King, Chloe
Hillier, Matt Botvinick, Daan Wierstra, Koray Kavukcuoglu,
and Demis Hassabis. Neural scene representation and ren-
dering. Science, 360(6394):1204–1210, 2018.

[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014.

[12] James Hays and Alexei A Efros. Scene completion using
millions of photographs. In ACM Transactions on Graphics
(TOG), volume 26, page 4. ACM, 2007.

[13] Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Jo-
hannes Kopf. Image completion using planar structure guid-
ance. ACM Transactions on graphics (TOG), 33(4):129,
2014.

[14] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.
Globally and locally consistent image completion. ACM
Transactions on Graphics (TOG), 36(4):107, 2017.

[15] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adversar-
ial networks. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 5967–5976. IEEE,
2017.

[16] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Advances in neural infor-
mation processing systems, pages 2017–2025, 2015.

[17] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196, 2017.

[18] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[19] Diederik P Kingma and Max Welling. Auto-encoding varia-

tional bayes. arXiv preprint arXiv:1312.6114, 2013.

[20] Rolf K¨ohler, Christian Schuler, Bernhard Sch¨olkopf, and
Stefan Harmeling. Mask-speciﬁc inpainting with deep neu-
ral networks. In German Conference on Pattern Recognition,
pages 523–534. Springer, 2014.

[21] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh
Singh, and Ming-Hsuan Yang. Diverse image-to-image
translation via disentangled representations.
In European
Conference on Computer Vision (ECCV), 2018.

[22] Anat Levin, Assaf Zomet, and Yair Weiss. Learning how to
inpaint from global image statistics. In null, page 305. IEEE,
2003.

[23] Yijun Li, Sifei Liu, Jimei Yang, and Ming-Hsuan Yang.
Generative face completion.
In Computer Vision and Pat-
tern Recognition (CVPR), 2017 IEEE Conference on, pages
5892–5900. IEEE, 2017.

[24] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang,
Image inpainting for
Andrew Tao, and Bryan Catanzaro.
In Proceedings
irregular holes using partial convolutions.
of the European Conference on Computer Vision (ECCV),
September 2018.

[25] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild.
In Proceedings
of the IEEE International Conference on Computer Vision,
pages 3730–3738, 2015.

[26] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares genera-
tive adversarial networks. In Computer Vision (ICCV), 2017
IEEE International Conference on, pages 2813–2821. IEEE,
2017.

[27] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep
multi-scale video prediction beyond mean square error.
arXiv preprint arXiv:1511.05440, 2015.

[28] Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan,
and Alexander C Berg. Transformation-grounded image

1446

In 2017
generation network for novel 3d view synthesis.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 702–711. IEEE, 2017.

[42] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Generative image inpainting with contex-
tual attention. arXiv preprint arXiv:1801.07892, 2018.

[43] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-
tus Odena. Self-attention generative adversarial networks.
arXiv preprint arXiv:1805.08318, 2018.

[44] Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. T2net:
Synthetic-to-realistic translation for solving single-image
depth estimation tasks. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 767–783, 2018.
[45] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE transactions on pattern analysis
and machine intelligence, 40(6):1452–1464, 2018.

[46] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Ma-
lik, and Alexei A Efros. View synthesis by appearance ﬂow.
In European conference on computer vision, pages 286–301.
Springer, 2016.

[47] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation.
In Advances
in Neural Information Processing Systems, pages 465–476,
2017.

[29] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2536–2544, 2016.

[30] Jimmy SJ Ren, Li Xu, Qiong Yan, and Wenxiu Sun. Shep-
In Advances in Neural

ard convolutional neural networks.
Information Processing Systems, pages 901–909, 2015.

[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211–252, 2015.

[32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In Advances in Neural Information Pro-
cessing Systems, pages 2234–2242, 2016.

[33] Andrew M Saxe, James L McClelland, and Surya Ganguli.
Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks.
arXiv preprint arXiv:1312.6120,
2013.

[34] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning
structured output representation using deep conditional gen-
erative models. In Advances in Neural Information Process-
ing Systems, pages 3483–3491, 2015.

[35] Yuhang Song, Chao Yang, Zhe Lin, Xiaofeng Liu, Qin
Huang, Hao Li, and CC Jay. Contextual-based image in-
painting: Infer, match, and translate. In Proceedings of the
European Conference on Computer Vision (ECCV), pages 3–
19, 2018.

[36] Yuhang Song, Chao Yang, Yeji Shen, Peng Wang, Qin
Huang, and C-C Jay Kuo. Spg-net: Segmentation prediction
and guidance network for image inpainting. arXiv preprint
arXiv:1805.03356, 2018.

[37] Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial
Hebert. An uncertain future: Forecasting from static images
using variational autoencoders. In European Conference on
Computer Vision (ECCV), 2016.

[38] Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, and
Shiguang Shan. Shift-net: Image inpainting via deep feature
rearrangement. In The European Conference on Computer
Vision (ECCV), September 2018.

[39] Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang,
and Hao Li. High-resolution image inpainting using multi-
scale neural patch synthesis.
In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), vol-
ume 1, page 3, 2017.

[40] Raymond A Yeh, Chen Chen, Teck Yian Lim, Alexander G
Schwing, Mark Hasegawa-Johnson, and Minh N Do. Seman-
tic image inpainting with deep generative models. In Com-
puter Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on, pages 6882–6890. IEEE, 2017.

[41] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Free-form image inpainting with gated
convolution. arXiv preprint arXiv:1806.03589, 2018.

1447

