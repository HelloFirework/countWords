Semi-supervised Transfer Learning for Image Rain Removal

Wei Wei1,2, Deyu Meng1 ∗, Qian Zhao1, Zongben Xu1, Ying Wu2

1School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China

2Department of Electrical and Computer Engineering, Northwestern University, IL, USA

Abstract

Single image rain removal is a typical inverse problem in
computer vision. The deep learning technique has been ver-
iﬁed to be effective for this task and achieved state-of-the-
art performance. However, previous deep learning methods
need to pre-collect a large set of image pairs with/without
synthesized rain for training, which tends to make the neu-
ral network be biased toward learning the speciﬁc patterns
of the synthesized rain, while be less able to generalize to
real test samples whose rain types differ from those in the
training data. To this issue, this paper ﬁrstly proposes a
semi-supervised learning paradigm toward this task. Dif-
ferent from traditional deep learning methods which only
use supervised image pairs with/without synthesized rain,
we further put real rainy images, without need of their clean
ones, into the network training process. This is realized by
elaborately formulating the residual between an input rainy
image and its expected network output (clear image without
rain) as a speciﬁc parametrized rain streaks distribution.
The network is therefore trained to adapt real unsupervised
diverse rain types through transferring from the supervised
synthesized rain, and thus both the short-of-training-sample
and bias-to-supervised-sample issues can be evidently alle-
viated. Experiments on synthetic and real data verify the
superiority of our model compared to the state-of-the-arts.

1. Introduction

Rain streaks and rain drops often occlude or blur the key
information of the images captured outdoors. Thus the rain
removal task for an image or a video is useful and necessary,
which can be served as an important pre-processing step for
outdoor visual system. An effective rain removal technique
can often help an image/video better deliver more accurate
detection or recognition results [17].

Current rain removal tasks can be mainly divided into
two categories: video rain removal (VRR) and single im-
age rain removal (SIRR). Compared to VRR, which could

Figure 1: The comparison of the synthesized rain and real
rain. (a) is a clean image; (b), (c) are two synthesized rainy
image samples. (d), (e), (f) are real world rainy images.

utilize the temporal correlation among consecutive frames,
SIRR is generally much more difﬁcult and challenging
without the aid of much prior knowledge capable of be-
ing extracted from a single image. Since being ﬁrstly pro-
posed by Kang et al. [17], the SIRR problem has been at-
tracting much attention. Recently, deep learning methods
[9, 10, 29, 32, 31, 23, 12, 11, 8] have been empirically sub-
stantiated to achieve state-of-the-art performance for SIRR
by training an appropriate, carefully designed network to
detect and remove the rain streaks simultaneously.

Albeit achieving good performance on this task, current
deep learning approach still exists some limitations on the
methodology. First, for training data, since it is hard to ob-
tain clean/rainy image pairs from real rainy scenarios, pre-
vious methods use synthesized data as an alternative, and
mainly adopt the strategy of adding the “fake” rain streaks
synthesized by the Photoshop software1 on the clean im-
ages. Two samples of such synthesized rainy images are
shown in Figure 1.(b) and (c) (the corresponding clean im-
age is shown in Figure 1.(a)). Albeit being varied by the rain
streak direction and density, the synthesized rainy images
still cannot include sufﬁciently wider range of rain streak
patterns in real rainy images. For instance, in Figure 1.(d),
the rain streaks have multiple directions in a single frame in-
ﬂuenced by the wind; in Figure 1.(e), the rain streaks have

∗Deyu Meng is the corresponding author.

1https://www.photoshopessentials.com/photo-effects/rain/

3877

multi-layers because of their different distances to the cam-
era; in Figure 1.(f), the rain streaks produce the effect of
aggregation which is similar to fog or mist. Therefore there
exists obvious bias between synthetic training data and real
testing data in this task, naturally leading to an issue that the
network trained on the synthetic training data possibly not
capable of being ﬁnely generalized to the real test data.

Meanwhile, one of the main problems for deep learning
methods lies on the preliminary conditions that they gener-
ally need sufﬁciently large number of supervised samples
(ideal cases are natural images with/without real rain for
our task), which are generally time-consuming and cumber-
some to collect, in order to train a derain network. However,
one generally can easily attain large amount of practical un-
supervised samples, i.e., real rainy images, while without
their corresponding clean ones. How to rationally feed these
cheap samples into the network training is not only mean-
ingful and necessary for the investigated task, but also pos-
sibly inevitable in the next generation of deep learning to
fully prompt its capability on unsupervised data for general
image restoration tasks.

Due to the inconsistence in the distribution of training
data and test data, this task can be naturally viewed as a
typical domain adaption problem. How to transfer from
learning the synthesized rain patterns (training, supervised)
to learning real rain patterns (testing, unsupervised) is cru-
cial. To alleviate the aforementioned issue of previous su-
pervised deep learning methods for the SIRR task, instead
of from the perspective of manually collecting more appro-
priate supervised dataset (real rainy images and their cor-
responding clean ones) to better suit this task, we propose
a novel semi-supervised method attempting to effectively
feed unsupervised real rainy images into the network train-
ing as well, ultimately expecting to transfer from synthe-
sized rain domain to real rain domain. Different from pre-
vious supervised deep learning methods by only using syn-
thesized image pairs as network inputs, our method is ca-
pable of fully utilizing unsupervised practical rainy images
during training in a mathematically sound manner. Specif-
ically, our model allows both the supervised synthetic data
and unsupervised real data being fed into the network si-
multaneously, and the network parameters can be optimized
by the combination of least square residuals (for supervised
samples) of network output images of supervised inputs and
their ground truth labels, and negative log-likelihood (NLL)
losses of a speciﬁc parametrized rain distribution (for unsu-
pervised samples) measured by the difference of network
output images of unsupervised inputs and their original
rainy ones. In this manner, both supervised synthetic and
unsupervised real samples can be rationally employed in
our method for network training.

In summary,

the main contributions of the proposed

method are:

• To our knowledge, this is the ﬁrst work that takes no-
tice of domain adaption issue for SIRR task. We are
the ﬁrst to propose a semi-supervised transfer learn-
ing framework for this task. Different from the previ-
ous deep learning SIRR methods, our model can fully
take use of the unsupervised real rainy images, which
can be easily collected in practice, without need of
the corresponding clean ones. Such unsupervised sam-
ples not only help evidently reduce the time and labor
costs of pre-collecting image pairs with/without real
rain for network parameters updating, but also allevi-
ate the over-ﬁtting issue of the deep network on limited
rain types covered by only supervised training samples
through compensating those unsupervised ones con-
taining more general and practical rain characteristics.
• We provide a general methodology for simultaneously
utilizing supervised and unsupervised knowledge for
image restoration tasks. For supervised one, the tradi-
tional least square loss between network output images
and their clean ones can be directly employed. For
the unsupervised one, we can rationally formulate the
residual between the expected output clean images and
their original noisy ones through a likelihood term im-
posed on a parameterized distribution designed based
on the domain understanding for residuals (e.g., rain in
our study).

• We design an Expectation Maximization algorithm to-
gether with a gradient descent strategy to solve the pro-
posed model. The rain distribution parameters and net-
work parameters can be optimized by sequence in each
epoch. Experiments implemented on synthesized rainy
images and especially real ones show that our model
is capable of transferring from learning synthesized to
real rain patterns, thus substantiating the superiority of
the proposed method compared to the state-of-the-arts.

The rest of this paper is organized as follows. In Section
2 we detailedly review the previous derain methods along
a history line. In Section 3 we present our model as well
as the optimization algorithms. We show the experimental
results in Section 4 and make conclusion in Section 5 .

2. Related work

2.1. Single image rain removal methods

The problem of SIRR was ﬁrstly proposed by Kang et al.
[17]. They detected the rain from the high frequency part of
an image based on morphological component analysis and
dictionary learning. Chen et al.’s [5] also operated on the
high frequency part of the rainy image but they employed
a hybrid feature set, including histogram of oriented gra-
dients, depth of ﬁeld, and Eigen color, in order to distin-
guish the rain portions from the image and enhance the tex-
ture/edge information. After that, Luo et al. [26] introduced

3878

screen blend model and used discriminative sparse coding
for rain layer separation, and the model is solved by greedy
pursuit algorithm. Li et al.’s [24] incorporated patch-based
Gaussian mixture model to deliver the prior information of
image background and rain layer, and trained the model pa-
rameters under pre-collected clean and rainy images. Sim-
ilarly, Zhang et al. [30] learned a set of generic sparsity-
based and low-rank representation-based convolutional ﬁl-
ters to represent background and rain streaks, respectively.
Gu et al. [14] combined analysis sparse representation to
represent image large-scale structures and synthesis sparse
representation to represent image ﬁne-scale textures, in-
cluding the directional prior and the non-negativeness prior
in their JCAS model. More recently, Zhu et al. [33] pro-
posed a joint optimization process that alternates between
removing rain-streak details from background layer and re-
moving non-streak details from rain layer. Their model is
aided by the rain priors, which are narrow directions and
self-similarity of rain patches, and the background prior,
which is centralized sparse representation. Chang et al. [4]
transformed a rainy image into a domain where the line pat-
tern appearance has extremely distinct low-rank structure,
and proposed a model with compositional directional total
variational and low-rank priors, to deal with the rain streaks
as line pattern noise and camera noise at the same time.

While these model-based methods are mathematically
sound, they mostly suffer from slow speed when testing be-
cause they need to solve an optimization problem. Deep
learning has an advantage on test speed and has been sub-
stantiated to be effective in many computer vision tasks
[21, 20, 15], so does in SIRR. Fu et al. ﬁrstly introduced
deep learning technique for this task in [9]. They trained a
convolutional neural network (CNN) with three hidden lay-
ers on the high frequency domain of the image. Later, they
further ameliorated the CNN by introducing deeper hidden
layers, batch normalization and negative residual mapping
structure, and achieved better effect [10]. To better deal
with the scenario of heavy rain images (where individual
streaks are hardly seen, and thus visually similar to mist or
[29] exploited a contextualized dilated
fog). Yang et al.
network with a binary map. In their model, a continuous
process of rain streak detection, estimation and removal are
predicted in a sequential order. Zhang et al. [32] applied the
mechanism of GAN and introduced a perceptual loss func-
tion for the consideration of rain removal problem. After-
wards, they developed a density aware multi-stream dense
network for joint rain density estimation and de-raining
[31].
In summary, these methods learn from synthesized
rain data and test their learned network in real scenes.

2.2. Video rain removal methods

Since the extra inter-frame information is extremely help-
ful, these methods showed relatively better reconstruction
effect than SIRR methods. Early video derain methods
[13, 2, 3, 18] designed many useful techniques to detect po-
tential rain streaks based on their physical characteristics
and removed these detected rain by image restoration algo-
rithms. In recently years, low-rankness [6, 27], total vari-
ation [16], stochastic distribution priors [28], convolutional
sparse coding [22], neural networks [25] have been applied
to the task and achieved satisfying results.

Since the SIRR problem is more difﬁcult in real world
with less information provided other than a rainy image, to
design an effective SIRR regime is also more challenging
beyond VRR ones.

3. Semi-supervised model for SIRR

We show the framework of our model which includes the
training data (both supervised and unsupervised samples)
and the network loss in Figure 2. As introduced aforemen-
tioned, our model is capable of feeding not only supervised
synthesized rainy images but also unsupervised real rainy
images into the network training process, in order to trans-
fer from learning synthesized rain patterns to learning real
rain patterns.

3.1. Model formulation

As shown in Figure 1.(d,e,f), the real rain usually shows
relatively more complex patterns and representations com-
pared to synthesized rain. However, due to the techni-
cal defects, these data’s “labels” (i.e., the corresponding
clean images) are generally unavailable. Although we can
hardly exactly extract the rain layer, as well as the clean
background, from a real rainy image, we instead can de-
sign a parametrized distribution to ﬁnely approximate its
stochastic conﬁgurations. Since the rain generally contains
multi-modal structures due to their occurrence on positions
with different distances to the cameras, we can ﬁnely ap-
proximately express the rain as a Gaussian mixture model
(GMM). That is,

R ∼

K

X

k=1

πkN (R|µk, Σk),

(1)

where πk, µk, Σk denote the mixture coefﬁcients, Gaussian
distribution means and variances. Mixture models can be
universal approximations to any continuous functions if the
parameters are learned appropriately, and thus it is suitable
to be utilized to describe the rain streaks to-be-extracted
from the input rainy image. Thus the negative log likeli-
hood function imposed on these unsupervised samples can
be written as:

For literature comprehensiveness, we simply list several
representative state-of-the-art video rain removal methods.

Lunsupervised(R; Π, Σ) = −

N

X

n=1

log

K

X

k=1

πkN (Rn|0, Σk),

(2)

3879

where xi, i = 1, ...N represents the samples of the synthe-
sized rainy image.

Moreover, since GMM can be adapted to any continuous
distribution, in order to let it better ﬁt the real rain samples,
we add a constraint that the discrepancy between synthe-
sized rain data domain and real rain data domain is not too
far by minimizing a Kullback−Leibler divergence between
a Gaussian Gsyn learned from the synthesized rain and the
aforementioned mixture of Gaussians GM Mreal learned
from the real rain during training, with a small controlling
parameter, as shown in the middle-right of Figure 2. This is
to indicate that our model is expecting to transfer from syn-
thesized rain to real rain, other than to arbitrary domains.
Since this KL divergence is not analytically tractable, we
use the minimum of KL divergence between Gsyn and each
component of GM Mreal as an empirical and simple substi-
tute, to ensure that at least one component of GMM learned
from the real samples is similar to rain. That is,

DKL(Gsyn||GM Mreal) ≃ min

k

DKL(Gsyn||GM M k

real), (4)

where GM M k
GM Mreal.

real

indicates

the kth component of

To further remove the potential remained rain streaks in
the output image, we add a Total Variation regularizer term
to slightly smooth the image. Note that together with the
aforementioned likelihood term on rain, a complete MAP
model (likelihood + regularizer) is formulated on the to-be-
estimated network outputs of the unsupervised real rain im-
ages. It facilitates a right direction for gradient descent to
network training on these unsupervised data even without
speciﬁc explicit guidance of corresponding clean images.

By combining Eq.(2), (3), (4) and TV term, the entire

objective function to train the network is formulated as:

L(w, Π, Σ) =

N1

X

i=1

||fw(xi) − yi||2

F + α

N2

X

n=1

||fw(˜x)n||T V

+βDKL(Gx||GM M ˜x)−λ

N2

X

n=1

log

K

X

k=1

πkN (˜xn −fw(˜x)n|0, Σk),

(5)
where xi, yi, i = 1, ...N1 represent corresponding rainy in-
put and ground truth label sample pairs of the synthesized
supervised data, and ˜xn, n = 1, ...N2 represent the rainy
input of the real unsupervised data without ground truth la-
bels. Through the last term of Eq. (5), the unsupervised
data can be fed into the same network with which imposed
on the supervised data, and the term ˜xn − fw(˜x)n is the
supposed rain extracted from the input rainy image, which
is equivalent to Rn as deﬁned in Eq. (2). α, β and λ are the
trade-off parameters. Note that when α, β and λ equal to 0,
our model degenerates to the conventional supervised deep
learning model [10].

By using such objective setting, the network can be
trained not only by the well annotated supervised data, but

3880

Figure 2: The ﬂow chart of the proposed method. Those
surrounded with concrete yellow square frames are the
given inputs for network training. The arrows represent the
forward process of network training. The upper panel shows
the supervised learning term, which is to minimize the dif-
ference of network output and the corresponding clean im-
age using a least square loss. The lower panel shows the
unsupervised learning term, which is to minimize an MAP
model with a GMM likelihood term imposed on rain distri-
bution and a TV regularization term on background. The
network structure and parameters are shared in both parts.
A K-L regularizer between the distribution of two types of
residuals (extracted rain) is further added to control the de-
gree of freedom.

where Π = π1, ...πK , Σ = Σ1, ...ΣK , K is the number
of mixture components, and N is the number of samples.
Note that the means of Gaussian distributions are manually
set to be zero, and this doesn’t affect the results in our ex-
periments.

By utilizing the above encoding manner, we can also
construct an objective function for unsupervised rainy im-
ages, which can be further used to ﬁne-tune the network pa-
rameters through back-propagating its gradients to the net-
work layers.

Meanwhile, we follow the network structure and nega-
tive residual mapping skill of DerainNet [10] (a deep con-
volutional neural network) to formulate the loss function on
supervised samples. The network which is denoted by fw(·)
(here w represents the network parameters) is supposed to
remove the rain streaks of input image and output a rain-free
one. The classical loss function of CNN is to minimize the
least square loss between the expected derain output fw(xi)
and the ground truth label yi, as shown in the upper panel
of Figure 2. That is, the loss function imposed on the super-
vised samples is with the following least square form:

Lsupervised =

N

X

i=1

||fw(xi) − yi||2
F ,

(3)

also purely unsupervised inputs by fully encoding the prior
information underlying rain streak distributions. As com-
pared with the traditional deep learning techniques imple-
mented on only supervised samples, the better generaliza-
tion effect of the network is expected due to the fact that it
facilitates a rational transferring effect from the supervised
samples to unsupervised types of rain.

3.2. The EM algorithm

Since the loss function in Eq. (5) is intractable, we use
the Expectation Maximization algorithm [7] to iteratively
solve the model. In E step, the posterior distribution which
represents the responsibility of certain mixture component
is calculated. In M step, the mixture distribution and the
convolutional neural network parameters are updated..
E step : Introduce a latent variable znk where znk ∈ {0, 1}
and PK
k=1 znk = 1, indicating the assignment of noise term
(˜xn−fw(˜x)n) to a certain component of the mixture model.
According to the Bayes’ theorem, the posterior responsibil-
ity of component k for generating the noise is given by:

γnk =

πkN (˜xn − fw(˜x)n|0, Σk)

Pk πkN (˜xn − fw(˜x)n|0, Σk)

.

(6)

M step : After the E step, the loss function in Eq. (5) is
unfolded into a differential one with respect to GMM pa-
rameters, shown as:

min
w,Π,Σ

N2

K

λ

X

X

γnk(

n=1

k=1

(˜xn −fw(˜x)n)2

2Σk

+

1
2

log |Σk|−log πk)

N1

+

X

i=1

||fw(xi)−yi||2

F +α

N2

X

n=1

||fw(˜xn)||T V +βDKL(Gx||GM M ˜x).

(7)
The closed-form solution of mixture coefﬁcients and Gaus-
sian covariance parameters are [7]:

N

X

n=1

γnk,

πk =

Nk
N

,

(8)

γnk(˜xn − f (˜x)n)2, k = 1, ...K.

(9)

Nk =

1
Nk

N

X

n=1

Σk =

Then we can employ the gradient methods to optimize the
objective function as deﬁned in Eq.
(7) and the gradient
so calculated can thus be easily back propagated to the net-
work to gradually ameliorate its parameters w. We readily
utilize Adam [19], the off-the-shelf ﬁrst order gradient op-
timization algorithm, for network parameter training on the
objective function (7) imposed on both synthesized super-
vised and real unsupervised training samples.

3.3. Discussions on domain transfer learning

The main difference of the proposed method from the
other supervised deep learning SIRR methods is the in-
volvement of the real world rainy images whose ground

truth rain-free images or ground-truth rain images) are un-
available during training. One main motivation for this in-
vestigation is that the manually synthesized rain shapes usu-
ally differ from real ones collected in practice. According
to several SIRR methods in the framework of deep learning
[9, 10, 29, 31], clean images are used to synthesize rainy
images by Photoshop software. Although each clean image
is supposed to synthesize several different type of rainy im-
age, as shown in upper panel of Figure 1, the difference of
scale, illuminance and distance to the camera of the real rain
streaks and usually accompanied fog or mist visual effect
are hardly sufﬁciently considered, thus yielding nonnegligi-
ble gap between the synthesized rainy images for training
and the real rainy images for testing.

In our method, the involvement of the unsupervised real
rainy data alleviates this problem. As shown in Figure 3,
we use the same synthesized rainy data with [10] as the
supervised training data. To empirically show the domain
transfer capability and verify the superiority of our model
on this point, we use a different way to synthesize rainy
images introduced in [28], and separate them as unsuper-
vised input set and validation set. Therefore the supervised
training rain and validation rain lie in distinct domains. We
found that our model shows better capability to overcome
the gap and transfer from the training data domain to valida-
tion data domain. Although our semi-supervised model not
extremely ﬁnely ﬁt the effect of the training data when the
unsupervised term in our loss function Eq. (5) plays more
important role (as shown in Column 1 of Figure 3, green
and blue lines are our semi-supervised model, with differ-
ent unsupervised term parameters), Column 2 of Figure 3
reﬂects that our model has better effect on the target domain
(solid line represents supervised data domain while dotted
line represents target domain). Moreover, with the training
dataset booming, the baseline supervised CNN (red line in
Figure 3) tends more and more to achieve speciﬁc patterns
of the training data (i.e., the performance of training data
improve), thus less being generalized to the validation data
(i.e., the performance of testing data does not improve cor-
respondingly, even slightly worsen) if they lie in separate
domain, as shown in Column 3 of Figure 3. However, the
involvement of the unsupervised term in our loss function
can effectively alleviate this issues, as shown in Column 4
and 5 of Figure 3, which is critical in real rain removal task.

4. Experimental results

In this section, we evaluate our methods both on synthe-
sized rainy data and real world rainy data. The compared
methods include the discriminative sparse coding based
method (DSC) [26], layer priors based method (LP) [24],
CNN method [10], joint bi-layer optimization (JBO) [33],
multi-task deep learning method (JORDER) [29] and multi-
stream dense net (DID-MDN) [31]. These methods include

3881

Figure 3: The PSNR trend graph of supervised training data and validation data during training process. In all subgraphs,
the solid line represents trend of supervised training data and the dotted line represents the trend of validation data. Note that
they in distinct domain because of the different rain-synthesized way. The red, green, blue lines represent the unsupervised
term controlling parameter λ in Eq. (5) equals to 0 (equivalent to supervised learning), 0.2 and 1, respectively. The three
rows use ﬁve hundred, ﬁve thousand and ten thousand image patches as the training data from top to bottom.

conventional unsupervised model-driven methods and more
recent supervised data-driven deep learning methods. Our
method to some extent can be viewed as an intrinsic combi-
nation of both methodologies.

4.1. Implementation details

For supervised training data, we use one million 64×64
synthesized rainy/clean image patch pairs which are the
same with the baseline CNN method [10]. For unsuper-
vised training data, we collect the real world rainy images
from the dataset provided by [29, 28, 32] and Google image
search. We randomly cropped one million 64×64 image
patches from these images to constitute the unsupervised
samples. Batch size is 20. The initial learning rate is 10−3,
decaying by multiplying 0.1 after every 5 epochs. We train
15 epochs in total. The training is implemented using Ten-
sorﬂow [1].

We design the number of GMM components as 3. For
the trade-off parameter λ, we simply set it as 0.5 throughout
all our experiments. The parameter α which controls the TV
smoothing term is set as a small value 10−5. The parameter
β which controls the KL divergence term is set to be 10−9.
The network structures and related parameters are directly
inherited from the baseline method [10].

4.2. Experiments on synthetic images

In this subsection, we evaluate the rain removal effect of
our method with synthetic data by both visual quality and
performance metric. We use the skill of [28] to synthesize
the rainy image as test data. Considering the complexity

and multiformity of the rain streaks, we compare our meth-
ods with others under two different scenarios: sparse rain
streaks and dense rain streaks. In each scenario we use ten
test images. Figure 4 shows an example of synthetic data
with sparse rain streaks. The added rain streaks are sparse
but with multiple lengths and layers, in consideration of the
different distance to the camera. As shown in Figure 4, the
DSC method [26] and JBO method [33] fail to remove the
main component of the rain streaks. The LP method [24]
tends to blur the visual effect of the image and over-smooth
the texture and edge information. The two deep learning
methods CNN [10] and JORDER [29] have better rain re-
moval effects, but rain streaks sill clearly exist in their re-
sults. Comparatively, our method could better remove the
sparse rain streaks and keep the background information.

We also design the experiments with dense rain streaks
scenario. In real world, the dense rain streaks have the ef-
fect of aggregation, blurring the image similar to fog or mist
when the rain is heavy. In Figure 5, the added rain is heavy,
with not only the long rain streaks, but also the brought blur-
ring effect damaging the image visual quality. As shown in
Figure 5, the results of DSC [26], JORDER [29] and JBO
[33] still have obvious rain streaks, while LP [24] still over-
smoothes the image. Compared with the baseline CNN
method [10], our method has better restoration results.

Since the ground truth is known for the synthetic experi-
ments, we use the most extensive performance metric Peak
Signal-to-Noise Ratio (PSNR) for a quantitative evaluation.
As is evident in Table 1, our method attains the best PSNR
in both two groups of data with different scenarios, in agree-

3882

Table 1: Mean PSNR comparison of two groups of data on synthesized rainy images.

Dataset
Dense
Sparse

Input DSC[26] LP[24]
19.27
17.95
24.14
25.67

19.00
25.05

JORDER[29] CNN[10]

18.75
24.22

19.90
26.88

JBO[33] DID-MDN[31] Ours
21.60
26.98

18.87
25.24

18.60
25.66

(a) Input

(b) Ground truth

(a) Input

(b) Ground truth

(c) DSC [26]

(d) LP [24]

(c) DSC [26]

(d) LP [24]

(e) CNN [10]

(f) JORDER [29]

(e) CNN [10]

(f) JORDER [29]

(g) JBO [33]

(h) Ours

(g) JBO [33]

(h) Ours

Figure 4: Synthesized rain removal results under the sparse
rain streaks scenario.

Figure 5: Synthesized rain removal results under the dense
rain streaks scenario.

ment with the visual effect in Figures 4 and 5.

4.3. Experiments on real images

The most direct way to evaluate a SIRR method is to see
its visual effect of restoration results on the real world rainy
images. We use the testing data selected from the Google
search. To better represent the diversity of the real rain sce-
narios, we intentionally select images with different types
of rain streaks as shown in Figure 6.

To conﬁrm the necessity of investigating transfer learn-
ing for this task, we list the complete synthesized rain types
[9] in our supervised training data in Figure 7. The bias
of rain between Figures 6 and 7 is obvious and the transfer
ability of our model can thus be substantiated. The visual

effect of derained images verify that our method can remove
more rain streaks and better keep the visual quality. Com-
pared to other competing methods, our method can remove
more amount of the rain streaks while still better keep the
structure of image undamaged.

5. Conclusion

In this paper, we have attempted to solve the SIRR
problem in a semi-supervised transfer learning manner.
We train a CNN on both synthesized supervised and
real unsupervised rainy images.
In this manner, our
method especially alleviates the hard-to-collect-training-
sample and overﬁtting-to-training-sample issues existed in
conventional deep learning methods designed for this task.

3883

]

Figure 6: Real rain streaks removal experiments under different scenarios. From left to right are input image, results of
DSC[26], LP [24], CNN [10], DID-MDN[31] and ours. Demarcated areas in each image are ampliﬁed at a 3 time larger
scale.

Figure 7: List of fourteen synthesized rain data types in our supervised data. The left image is the original one without
rain streaks, and the right 14 ones are those superimposed with different rain types. The rain details can be more evidently
observed by zooming in the images on a computer screen.

The experiments implemented on synthesized and real im-
ages substantiate the effectiveness of the proposed method.

We admit that our model is still not almighty for all
rainy image which could be extremely complicated to han-
dle. The involvement of more elaborate priors on rain and
background layers in training the network could be the fu-
ture direction to further improve the performance for this
task. Also this semi-supervised transfer learning method-
ology could be considered into other inverse problems as
well. We wish to apply the human prior knowledge into the
learning process of neural network framework, more sufﬁ-
ciently realizing the combination of data-based and model-

based methods. The ultimate goal is to take advantage of
both supervised data-based deep learning methods, which
could shorten the testing time to fulﬁll the online require-
ment, and model-based method, to put the network training
into a more explainable direction.

Acknowledge

This research was supported by National Key R&D
Program of China (2018YFB1004300), China NSFC
projects (61661166011, 11690011,61603292, 61721002,
U1811461), National Science Foundation grant
IIS-
1619078, IIS-1815561, and the Army Research Oﬁce ARO
W911NF-16-1-0138.

3884

References

[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow:
a system for large-scale machine learning.
In OSDI, vol-
ume 16, pages 265–283, 2016. 6

[2] Peter C Barnum, Srinivasa Narasimhan, and Takeo Kanade.
Analysis of rain and snow in frequency space. International
Journal of Computer Vision, 86(2-3):256, 2010. 3

[3] J´er´emie Bossu, Nicolas Hauti`ere, and Jean-Philippe Tarel.
Rain or snow detection in image sequences through use of a
histogram of orientation of streaks. International Journal of
Computer Vision, 93(3):348–367, 2011. 3

[4] Yi Chang, Luxin Yan, and Sheng Zhong. Transformed low-
rank model for line pattern noise removal. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1726–1734, 2017. 3

[5] Duan-Yu Chen, Chien-Cheng Chen, and Li-Wei Kang. Vi-
sual depth guided color image rain streaks removal using
sparse coding. IEEE Transactions on Circuits and Systems
for Video Technology, 24(8):1430–1455, 2014. 2

[6] Yi-Lei Chen and Chiou-Ting Hsu. A generalized low-
rank appearance model for spatio-temporally correlated rain
streaks.
In Computer Vision (ICCV), 2013 IEEE Interna-
tional Conference on, pages 1968–1975. IEEE, 2013. 3

[7] Arthur P Dempster, Nan M Laird, and Donald B Rubin.
Maximum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society. Series B
(methodological), pages 1–38, 1977. 5

[8] Zhiwen Fan, Huafeng Wu, Xueyang Fu, Yue Huang, and
Xinghao Ding. Residual-guide network for single image de-
raining. In 2018 ACM Multimedia Conference on Multime-
dia Conference, pages 1751–1759. ACM, 2018. 1

[9] Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao,
and John Paisley. Clearing the skies: A deep network archi-
tecture for single-image rain removal. IEEE Transactions on
Image Processing, 26(6):2944–2956, 2017. 1, 3, 5, 7

[10] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao
Ding, and John Paisley. Removing rain from single images
via a deep detail network. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2017. 1, 3, 4,
5, 6, 7, 8

[11] Xueyang Fu, Borong Liang, Yue Huang, Xinghao Ding, and
John Paisley. Lightweight pyramid networks for image de-
raining. arXiv preprint arXiv:1805.06173, 2018. 1

[12] Xueyang Fu, Qi Qi, Yue Huang, Xinghao Ding, Feng Wu,
and John Paisley. A deep tree-structured fusion model for
single image deraining. arXiv preprint arXiv:1811.08632,
2018. 1

[13] Kshitiz Garg and Shree K Nayar. Detection and removal of
rain from videos. In Computer Vision and Pattern Recogni-
tion, 2004. CVPR 2004. Proceedings of the 2004 IEEE Com-
puter Society Conference on. IEEE, 2004. 3

[14] Shuhang Gu, Deyu Meng, Wangmeng Zuo, and Lei Zhang.
Joint convolutional analysis and synthesis sparse represen-
tation for single image layer separation. In 2017 IEEE In-

ternational Conference on Computer Vision (ICCV), pages
1717–1725. IEEE, 2017. 3

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016. 3

[16] Tai-Xiang Jiang, Ting-Zhu Huang, Xi-Le Zhao, Liang-Jian
Deng, and Yao Wang. A novel tensor-based video rain
streaks removal approach via utilizing discriminatively in-
trinsic priors. In Proceedings of the Conference on Computer
Vision and Pattern Recognition, 2017. 3

[17] Li-Wei Kang, Chia-Wen Lin, and Yu-Hsiang Fu. Auto-
matic single-image-based rain streaks removal via image
decomposition.
IEEE Transactions on Image Processing,
21(4):1742–1755, 2012. 1, 2

[18] Jin-Hwan Kim, Jae-Young Sim, and Chang-Su Kim. Video
deraining and desnowing using temporal correlation and
low-rank matrix completion. IEEE Transactions on Image
Processing, 24(9):2658–2670, 2015. 3

[19] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 5

[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In Advances in Neural Information Processing Sys-
tems, pages 1097–1105, 2012. 3

[21] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
3

[22] Minghan Li, Qi Xie, Qian Zhao, Wei Wei, Shuhang Gu, Jing
Tao, and Deyu Meng. Video rain streak removal by mul-
tiscale convolutional sparse coding.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 6644–6653, 2018. 3

[23] Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, and Hongbin
Zha. Recurrent squeeze-and-excitation context aggregation
net for single image deraining. In European Conference on
Computer Vision, pages 262–277. Springer, 2018. 1

[24] Yu Li, Robby T Tan, Xiaojie Guo, Jiangbo Lu, and Michael S
Brown. Rain streak removal using layer priors. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2736–2744, 2016. 3, 5, 6, 7, 8

[25] Jiaying Liu, Wenhan Yang, Shuai Yang, and Zongming Guo.
D3r-net: Dynamic routing residue recurrent network for
video rain removal. IEEE Transactions on Image Process-
ing, 2018. 3

[26] Yu Luo, Yong Xu, and Hui Ji. Removing rain from a sin-
gle image via discriminative sparse coding. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 3397–3405, 2015. 2, 5, 6, 7, 8

[27] Weihong Ren, Jiandong Tian, Zhi Han, Antoni Chan, and
Yandong Tang. Video desnowing and deraining based on ma-
trix decomposition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4210–
4219, 2017. 3

3885

[28] W. Wei, L. Yi, Q. Xie, Q. Zhao, D. Meng, and Z. Xu. Should
we encode rain streaks in video as deterministic or stochas-
tic? In 2017 IEEE International Conference on Computer
Vision (ICCV), volume 00, pages 2535–2544, Oct. 2018. 3,
5, 6

[29] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zong-
ming Guo, and Shuicheng Yan. Deep joint rain detection
and removal from a single image.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 1357–1366, 2017. 1, 3, 5, 6, 7

[30] He Zhang and Vishal M Patel. Convolutional sparse and low-
rank coding-based rain streak removal.
In Applications of
Computer Vision (WACV), 2017 IEEE Winter Conference on,
pages 1259–1267. IEEE, 2017. 3

[31] He Zhang and Vishal M Patel. Density-aware single image
In CVPR,

de-raining using a multi-stream dense network.
2018. 1, 3, 5, 7, 8

[32] He Zhang, Vishwanath Sindagi, and Vishal M Patel.

Im-
age de-raining using a conditional generative adversarial net-
work. arXiv preprint arXiv:1701.05957, 2017. 1, 3, 6

[33] Lei Zhu, Chi-Wing Fu, Dani Lischinski, and Pheng-Ann
Joint bi-layer optimization for single-image rain
Heng.
streak removal.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2526–
2534, 2017. 3, 5, 6, 7

3886

