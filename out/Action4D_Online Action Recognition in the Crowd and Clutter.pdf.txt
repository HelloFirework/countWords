Action4D: Online Action Recognition in the Crowd and Clutter

Quanzeng You

Hao Jiang

Microsoft Cloud & AI

One Microsoft Way, Redmond, WA 98052

{quyou, jiang.hao}@microsoft.com

Abstract

(cid:53)(cid:72)(cid:68)(cid:71)(cid:76)(cid:81)(cid:74)
(cid:54)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)
(cid:51)(cid:79)(cid:68)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)

(cid:53)(cid:72)(cid:68)(cid:71)(cid:76)(cid:81)(cid:74)
(cid:54)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)
(cid:51)(cid:79)(cid:68)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)

Recognizing every person’s action in a crowded and clut-
tered environment is a challenging task in computer vision.
We propose to tackle this challenging problem using a holis-
tic 4D “scan” of a cluttered scene to include every detail
about the people and environment. This leads to a new
problem, i.e., recognizing multiple people’s actions in the
cluttered 4D representation. At the ﬁrst step, we propose a
new method to track people in 4D, which can reliably detect
and follow each person in real time. Then, we build a new
deep neural network, the Action4DNet, to recognize the ac-
tion of each tracked person. Such a model gives reliable and
accurate results in the real-world settings. We also design
an adaptive 3D convolution layer and a novel discrimina-
tive temporal feature learning objective to further improve
the performance of our model. Our method is invariant to
camera view angles, resistant to clutter and able to han-
dle crowd. The experimental results show that the proposed
method is fast, reliable and accurate. Our method paves
the way to action recognition in the real-world applications
and is ready to be deployed to enable smart homes, smart
factories and smart stores.

1. Introduction

Action recognition is a key task in computer vision.
Even though human vision is good at recognizing subtle
actions, computer vision algorithm still cannot achieve the
same robustness and accuracy. The difﬁculty is largely
caused by the variations of the visual inputs. The input
video may be crowded and cluttered. People may have dif-
ferent clothing, different body shapes and are highly artic-
ulated. They may perform the same action in slightly dif-
ferent ways. The camera viewing angles can be drastically
different so that the same action in the training videos may
look quite different from those in the testing videos.

To tackle the above challenges, in this paper, we propose
a novel 4D method for robust action recognition. The input
of our method is a 4D volume of the dynamic environment

(cid:53)(cid:72)(cid:68)(cid:71)(cid:76)(cid:81)(cid:74)
(cid:54)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)
(cid:51)(cid:79)(cid:68)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)

(cid:53)(cid:72)(cid:68)(cid:71)(cid:76)(cid:81)(cid:74)
(cid:54)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)
(cid:51)(cid:79)(cid:68)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)

(cid:11)(cid:68)(cid:12)(cid:53)(cid:72)(cid:68)(cid:71)(cid:76)(cid:81)(cid:74)

(cid:11)(cid:69)(cid:12)(cid:3)(cid:54)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)

(cid:11)(cid:70)(cid:12)(cid:3)(cid:51)(cid:79)(cid:68)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)

Figure 1. Examples of the inference results of our action recog-
nition system on a crowded and cluttered environment with four
Kinect V2 cameras. The top two rows show the projected 3D
bounding boxes on each camera view given by our multiple-people
tracker. The last row shows the volumes generated from the four
calibrated depth cameras for each subject with different actions.

constructed from multiple calibrated RGBD cameras. Fig-
ure 1 illustrates our scheme. The proposed method tracks
each individual person using the 4D representation and rec-
ognizes their actions.
It is view invariant, able to handle
crowd and clutter, and scalable to applications in a huge
space with hundreds of cameras.

Recognizing multiple people’s actions in cluttered 4D
volume is a new and challenging problem. To the best of
our knowledge, our method gives the ﬁrst solution to this
problem. In particular, we propose a novel Action4DNet to
recognize the action of each subject in a cluttered environ-
ment using online 4D modeling. Our work has the follow-
ing contributions:

11857

• We tackle the new problem of recognizing multiple

people’s actions in cluttered 4D volume data.

• We propose a new people detection and tracking

method using the 4D volume data in real time.

• We propose a new deep neural network, Action4DNet,
for action recognition. We design an adaptive convolu-
tional layer to deal with the noise introduced from mul-
tiple camera sensors. We also propose a new discrim-
inative loss for better temporal feature learning in se-
quential action recognition. To the best of our knowl-
edge, our approach is the ﬁrst attempt to apply deep
neural networks to cluttered “holistic” 4D volume data
for online frame-wise action recognition.

• We collect and label a new 4D dataset in our experi-
mentation. There is no existing 4D action recognition
dataset that includes multiple people and clutter. We
will publish the dataset.

• Our proposed method is resistant to crowd and clut-
ter, and it can be directly used in complex real-world
applications.

1.1. Related works

In previous studies, most action recognition methods
work on single view 2D videos. Accumulated foreground
shape [1] has been used to recognize the actions in the Kid-
In [28], shape context is used to model
sRoom project.
the whole body conﬁguration in action recognition. Apart
from RGB color, motion is also a useful feature for action
recognition [9]. Other popular handcrafted features for ac-
tion recognition include spatial-temporal features [14] and
spatial-temporal volumes [4]. Based on these features, ac-
tion detection and recognition can be formulated as a match-
ing problem. By careful design, we do not even need to di-
rectly extract the features; the space and time matching can
be efﬁciently solved using low rank analysis [23].

In recent years, deep learning has been widely used in
action recognition and detection using RGB videos [12, 30,
24, 8, 7]. These deep learning methods use multiple streams
such as color, motion, body part heat map and ﬁnd actions in
the spatial-temporal 3D volume. Single view depth images
have also been used in action recognition [26]. However,
training classiﬁers for action recognition using 2D RGB or
depth videos is a challenging task. It requires training data
to include all kinds of variations about camera settings, peo-
ple clothing, object appearances, and backgrounds.

Most of the current 3D action recognition methods de-
pend on Kinect 3D skeleton extraction [17, 21, 22], which
can relieve the view dependency issue in 2D action recog-
nition. Unfortunately, Kinect skeleton estimation becomes

unreliable in cluttered environments. In addition, 3D skele-
tons alone are insufﬁcient for action recognition. For in-
stance, disambiguating actions such as playing with phone
and reading a book is tricky without knowing the objects
in people’s hands. 3D people volume from visual hull
[15] has also been extensively used in action recognition
[5, 29, 11]. Traditional visual hull methods usually need
special blue/green or static background and background
subtraction to single out people from the background. This
greatly limits its usability in real-world applications.

In contrast, our method works directly on cluttered 4D
volume data. The volume representation includes not only
people but also the objects they are interacting with. With-
out the dependency on people segmentation, our method
can be robustly applied to action recognition in crowded and
cluttered environments.

2. Method

Our task is to recognize individuals’ actions in a clut-
tered and crowded environment. Our method starts with
the construction of 3D volume representation of the whole
scene at each time instant. Then, we propose a new people
detection and tracking method using sequential 3D volume
data of the whole scene. In such a way, we can crop each
person-centered 3D volume at each time instant. These as-
sociated 3D volume sequences by our 4D tracker are used as
input to build our Action4DNet. The details are discussed
in following sections.

2.1. People detection and tracking

Detecting each subject in the scene is a necessary step
before we can recognize the action of each individual. For
action recognition, we also need to observe every subject in
a duration. We thus need to track each person in the scene.
Tracking also helps remove false people detections and re-
cover the missing ones. Most of the previous multiple peo-
ple tracking methods usually use background subtraction to
remove the background clutter. Unfortunately, background
subtraction or ﬁgure/ground separation is hard for uncon-
strained dynamic environment. Our 4D tracker does not
need ﬁgure/ground separation and is able to work on the
noisy 4D data directly.

Given a set of calibrated RGBD images, we build the 3D
point cloud of the whole scene. The volumes are built on top
of the 3D point cloud. We set the occupancy of a voxel O(i)
to one if there is a point in it. These voxels are on the scene
surface of the environment.
It is also possible to ﬁll the
internal voxels of each object. However, our experiments
suggest that action recognition does not beneﬁt much from
such a denser representation. We thus only use the surface
volumes in this work.

11858

(cid:51)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:3)
(cid:57)(cid:82)(cid:79)(cid:88)(cid:80)(cid:72)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:22)(cid:39)

(cid:23)(cid:11)(cid:22)(cid:91)(cid:22)(cid:91)(cid:22)(cid:12)

(cid:48)(cid:68)(cid:91)(cid:16)(cid:51)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:22)(cid:39)

(cid:21)(cid:91)(cid:21)(cid:91)(cid:21)

(cid:27)(cid:11)(cid:22)(cid:91)(cid:22)(cid:91)(cid:22)(cid:12)

(cid:25)(cid:20)(cid:91)(cid:25)(cid:20)(cid:91)(cid:27)(cid:24)

(cid:23)(cid:91)(cid:25)(cid:20)(cid:91)(cid:25)(cid:20)(cid:91)(cid:27)(cid:24)

(cid:23)(cid:91)(cid:22)(cid:19)(cid:91)(cid:22)(cid:19)(cid:91)(cid:23)(cid:20)

(cid:27)(cid:91)(cid:22)(cid:19)(cid:91)(cid:22)(cid:19)(cid:91)(cid:23)(cid:20)

(cid:48)(cid:68)(cid:91)(cid:16)(cid:51)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:22)(cid:39)

(cid:48)(cid:68)(cid:91)(cid:16)(cid:51)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)

(cid:41)(cid:79)(cid:68)(cid:87)

(cid:41)(cid:38)

(cid:41)(cid:38)

(cid:21)(cid:91)(cid:21)(cid:91)(cid:21)

(cid:27)(cid:11)(cid:22)(cid:91)(cid:22)(cid:91)(cid:22)(cid:12)

(cid:21)(cid:91)(cid:21)(cid:91)(cid:21)

(cid:27)(cid:91)(cid:20)(cid:24)(cid:91)(cid:20)(cid:24)(cid:91)(cid:21)(cid:19)

(cid:27)(cid:91)(cid:20)(cid:24)(cid:91)(cid:20)(cid:24)(cid:91)(cid:21)(cid:19)

(cid:27)(cid:91)(cid:26)(cid:91)(cid:26)(cid:91)(cid:20)(cid:19)

(cid:22)(cid:28)(cid:21)(cid:19)

(cid:20)(cid:19)(cid:21)(cid:23)

(cid:21)

(a)

t-1

t

t+1

t+n

...

...

...

...

...

...

...

...

: Trajectory
: Trajectory

: Prediction
: Prediction

: Candidate
: Candidate

(b)

Figure 2. (a): People classiﬁcation CNN. (b): We ﬁnd disjoint
paths on the tracking graph.

2.1.1 People candidate proposal

We use a light-weight people candidate proposal scheme
let f (x, y, z) be the volume data and assume
as follows:
z = 0 is the ground plane. The top-down envelop image is
g(x, y) = maxz(z (f (x, y, z))), where (ξ) is an indicator
function which equals 1 if ξ > 0, otherwise 0. Based on the
observation that each potential object corresponds to at least
one local maximum on g, we use a simple Gaussian ﬁlter to
extract the candidates. The local maxima are found on the
Gaussian ﬁltered top-down envelope using non-maximum
suppression. Each candidate volume is a cuboid around a
local maximum with a given width and height. Currently,
we set the height of the cropped volumes to be the height of
the whole scene volume.

We train a 3D CNN to classify each candidate volume to
be people or non-people. The CNN structure of our people
classiﬁer is shown in Figure 2 (a), which consists of a se-
quence of 3D convolution layers, ReLUs and pooling layers
(ReLUs are not shown), followed by a multi-layer percep-
tron (MLP). The 3D people classiﬁer gives the probability
of each candidate 3D bounding box containing a person.
Even with just a few thousand frames of training data, the
people detector can achieve high accuracy to support the
following data association for people tracking.

2.1.2 Data association

With the extracted candidates, people tracking can be for-
mulated as a path following problem. We try to link the
detected trajectories to the detections in the current frame t
and the next n frames. Here n is a small number, e.g., three.
The tracking graph is shown in Figure 2 (b). There are
three kinds of nodes in the graph: the rectangle nodes rep-
resent the trajectories already formed, the oval nodes repre-
sent candidates, and the pentagon nodes are the prediction
nodes. The number of prediction nodes equals the num-
ber of candidate nodes plus the number of the prediction
nodes at previous time instant. The edges indicate possi-
ble matches between nodes. The edge weights are deter-
mined by the difference of the probabilities from our 3D

(cid:11)(cid:68)(cid:12)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:85)(cid:72)(cid:86)(cid:88)(cid:79)(cid:87)(cid:86)

(cid:11)(cid:69)(cid:12)(cid:3)(cid:55)(cid:90)(cid:82)(cid:3)(cid:86)(cid:76)(cid:71)(cid:72)(cid:16)(cid:89)(cid:76)(cid:72)(cid:90)(cid:3)(cid:53)(cid:42)(cid:37)(cid:3)(cid:76)(cid:80)(cid:68)(cid:74)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:82)(cid:88)(cid:85)(cid:3)(cid:79)(cid:68)(cid:69)

Figure 3. Sample tracking results of the proposed method. (a) Vi-
sualization of our tracking results. On the left of the screen is the
top-down view with each numbered red circle representing one
person. On the right is the real-time 3D point cloud. (b) We use
two other RGB cameras to capture the corresponding side views
of our lab.

people classiﬁer, the Euclidean distance, the occupancy vol-
ume difference, and the color histogram difference between
neighboring nodes. The trajectory node also has a weight
inversely proportional to the trajectory length. To track ob-
jects in the scene, we ﬁnd the extension of each trajectory
from time t − 1 to t + n, so that these paths pass each tra-
jectory node and all the paths are node disjoint.

This optimization problem can be reduced to a min-cost
ﬂow problem and it can be solved efﬁciently using a poly-
nomial algorithm [19]. Each trajectory is only extended to
the neighboring nodes within a radius dL, which is deter-
mined by the max speed of a person and the frame rate of
the tracking algorithm.

After the optimization, we extend each existing trajec-
tory by one-unit length. We remove trajectories with low
people score, which is computed as the weighted sum of the
current people probability and the previous people score.
And, we include new trajectory for each candidate node at
time t that is not on any path. The new set of trajectories are
used to form a new graph for the next time instant.

Our people detection and tracking algorithm is robust
against clutter and crowd. Figure 3 shows sample results
from our 4D tracking over a few thousand frames. The
tracker is able to handle cases such as putting a box above
the head as shown in Figure 3 (b).

2.2. Action recognition

The above tracker gives us accurate 3D location of each
subject at each time instant, which can be used to crop out
3D volumes for action recognition. Figure 4 shows the
cropped volume representations, where persons are at the
center. Even with the cluttered background, the volume rep-
resentation clearly shows the action of a person. As a mat-
ter of fact, the background objects are desirable for action
recognition because of their context information.

We process the 4D volume (sequence of 3D volumes)
data to infer the action at each time instant. There are many
other clues that can be used to infer the action of a person,
e.g., the body poses, the movement of body parts, and the
objects the subject is handling. For instance, if we see a
chair underneath a person, we can infer that the person is

11859

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

Figure 4. We recognize actions using volumes (color represents
the height of each voxel). The action can be easily identiﬁed even
from a static snapshot. The actions are (a) bending, (b) drink-
ing, (c) lifting, (d) pushing/pulling, (e) squatting, (f) yawning, (g)
calling, (h) eating, (i) opening drawer, (j) reading, (k) waving, (l)
clapping, (m) kicking, (n) pointing, (o) sitting, and (p) browsing
cell phone. These real-time generated volumes are fed into our
Action4DNet for action recognition.

sitting. Potentially, the position or the speed of each person
can also be used to infer speciﬁc actions. However, in this
paper we depend on the volume data alone for building our
4D action recognition model.

We construct deep convolutional neural network, Ac-
tion4DNet, for accurate action recognition. The input 4D
volumes go through a sequence of 3D convolution layers
combined with 3D pooling layers to produce action fea-
tures. Meanwhile, we also propose to use an auxiliary at-
tention net, which will be discussed in more details in the
following subsections. These features at each time instant
are fed into a Recurrent Neural Network (RNN) to aggre-
gate the temporal information for ﬁnal action classiﬁcation.
In the following, we present the network structures in

more details.

to automatically learn the most relevant local sub-volume
features, and global max-pooling [16] is used for global fea-
tures. Both features are the inputs to the Recurrent Neural
Network (we use LSTM) for action classiﬁcation.

Let V ∈ RF ×L×W ×H be the output from the last 3D
convolution layer, where F is the number of ﬁlters, L, W
and H are the size of the 3D output.
In particular, each
location in the 3D output can be represented as vijk ∈ RF
for 1 ≤ i ≤ L, 1 ≤ j ≤ W and 1 ≤ k ≤ H. The attention
weights for all vijk are computed as

βijk = hT

t−1U vijk

α = softmax(β),

(1)

(2)

where α ∈ RL×W ×H is the attention weights, U ∈ RD×F
is the weight matrix to be learned and ht−1 ∈ RD is the
previous hidden state of size D from the Recurrent Neural
Network. In such a way, the network is expected to auto-
matically discover the relevance of different sub-volumes
for different actions.

Next, the local feature v is computed as the weighted

sum of all the sub-volume features vijk

v = (cid:2)

αijkvijk.

i,j,k

(3)

In addition, we employ a 3D convolution layer followed
by a global pooling layer to obtain the global feature g (see
Figure 5). Next, both the global feature g and the local at-
tention feature v are supplied to the LSTM cell to capture
the temporal dependencies. The action classiﬁcation model,
which is a multi-layer perceptron (MLP), takes the hidden
state from the LSTM cell as input to recognize different ac-
tions at each time instant.

2.2.1 Attention Action4DNet

2.2.2 Adaptive convolutional layer

xt

(cid:51)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:3)(cid:57)(cid:82)(cid:79)(cid:88)(cid:80)(cid:72)

(cid:25)(cid:20)(cid:91)(cid:25)(cid:20)(cid:91)(cid:27)(cid:24)

ht-1

(cid:22)(cid:39)(cid:3)(cid:38)(cid:49)(cid:49)(cid:86)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:22)(cid:39)

(cid:48)(cid:68)(cid:91)(cid:16)(cid:51)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:22)(cid:39)

(cid:48)(cid:68)(cid:91)(cid:16)(cid:51)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:22)(cid:39)

(cid:48)(cid:68)(cid:91)(cid:16)(cid:51)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:22)(cid:39)

(cid:48)(cid:68)(cid:91)(cid:16)(cid:51)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)

(cid:21)(cid:91)(cid:21)(cid:91)(cid:21)
(cid:25)(cid:23)(cid:59)(cid:22)(cid:19)(cid:91)(cid:22)(cid:19)(cid:91)(cid:23)(cid:21)

(cid:21)(cid:91)(cid:21)(cid:91)(cid:21)

(cid:21)(cid:91)(cid:21)(cid:91)(cid:21)

(cid:20)(cid:21)(cid:27)(cid:91)(cid:20)(cid:24)(cid:91)(cid:20)(cid:24)(cid:91)(cid:21)(cid:20)

(cid:21)(cid:24)(cid:25)(cid:91)(cid:26)(cid:91)(cid:26)(cid:91)(cid:20)(cid:19)

(cid:21)(cid:91)(cid:21)(cid:91)(cid:21)
(cid:21)(cid:24)(cid:25)(cid:59)(cid:22)(cid:91)(cid:22)(cid:91)(cid:24)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:22)(cid:39)

(cid:21)(cid:24)(cid:25)

(cid:42)(cid:79)(cid:82)(cid:69)(cid:68)(cid:79)(cid:3)(cid:48)(cid:68)(cid:91)(cid:16)

(cid:51)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)

(cid:21)(cid:24)(cid:25)(cid:59)(cid:22)(cid:91)(cid:22)(cid:91)(cid:24)

(cid:22)(cid:91)(cid:22)(cid:91)(cid:24)

(cid:47)(cid:54)(cid:55)(cid:48)

(cid:36)(cid:55)(cid:55)

(cid:20)(cid:91)(cid:22)(cid:91)(cid:22)(cid:91)(cid:24)

(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)

(cid:24)(cid:20)(cid:21)

(cid:24)(cid:20)(cid:21)

(cid:21)(cid:24)(cid:25)

(cid:20)(cid:26)

(cid:21)(cid:24)(cid:25)

(cid:38)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)

Figure 5. Our proposed attention Action4DNet.

Figure 5 shows the proposed neural network architecture
for action recognition using the volumetric data. It starts
with several 3D convolution layers followed by 3D max-
pooling layers. Then, attention model [2, 18] is employed

In previous section, we describe our attention Action4DNet
using standard 3D convolutional neural networks. In real-
world environments, camera sensors and calibration errors
can introduce noises to the generated volumes. In our vol-
ume representations, these noises could lead to different ac-
tivation outputs for a regular convolutional layer. It is attrac-
tive if the model itself can adapt to the noise. We propose
the adaptive convolutional layer, which is designed with an
extra adaptive activation mechanism.

Again, let V ∈ RF ×L×W ×H be the output from the last
3D convolution layer. Then, we attach another convolution
layer with two 1 × 1 × 1 kernels followed by a softmax
operator. The output is denoted as Z ∈ R2×L×W ×H , which
serves as the adaptive probability for each location in V . We
produce the new outputs at location (i, j, k) as

V ′[:, i, j, k] = Z1ijk ∗ V [:, i, j, k],

(4)

11860

where ∗ is the product between a scalar Z1ijk and a vec-
tor V [:, i, j, k] ∈ RF . This layer can be inserted into any
regular 3D convolutional layers. We call them adaptive con-
volutional layers.

2.2.3 Discriminative temporal feature learning

RNNs are designed for capturing temporal dependencies.
State-of-the-art action recognition models also apply RNNs
to action recognition [25] in order to understand and incor-
porate temporal information in videos. However, the tem-
poral transitions are more difﬁcult to capture in continuous
domains, such as videos than in discrete domains, such as
natural languages. Recently, optical ﬂow has been widely
employed to assist the model for better temporal feature
learning. The computation of optical ﬂow requires more
computing power. In addition, it is more difﬁcult in our 4D
scenario, due to the noise in our large volume data.

Instead, we expect the model can learn to distinguish the
stepwise states by only looking at the sequential data. To
achieve this goal, we propose a margin-ranking loss, which
tries to differentiate the temporal features in a given train-
ing sequence. Let H = {h1, h2, . . . , hn} be the hid-
den states from the recurrent neural network (see Figure 5).
We add additional semantic layer with weight W to map
h′

i = W hi and deﬁne the loss as

L(H ′) =

n−2

(cid:2)

i=1

max(0, c + score(h′

i, h′

n)

(5)

− score(h′

n−1, h′

n))

1, h′

2, . . . , h′

where H ′ = {h′
n} is the set of the mapped
hidden states, c is a constant and score(·, ·) computes the
similarity between its two inputs. In this work, we adopt the
cosine similarity function.

The above loss attempts to guarantee that within any
given training sequence, the last frame has a larger simi-
larity with the second to the last frame than all other previ-
ous frames. In general, this constraint is true in videos as
well as in our 4D case. In particular, for training sequences
where all the frames have the same label, this loss func-
tion differentiates the states at different input frames. At
the same time, the cross-entropy loss for action recognition
classiﬁer tries to maintain correct prediction on these dif-
ferentiated states. Our experiments suggest that this mecha-
nism leads to better action recognition performance for our
Action4DNet model.

3. Experimental results

In this section, we evaluate the proposed 4D approach for
action recognition and compare our approach against differ-
ent competing methods.

3.1. Ground truth experimentation setup

To evaluate the performance of our method, we collect
a 4D action recognition dataset. We set up three different
environments (Env1, Env2 and Evn3) with different num-
ber of Kinect V2 cameras to capture the RGBD images
and then we generate 4D volume representation of the dy-
namic scene. The three environments are located at differ-
ent rooms with different backgrounds. We label the videos
in a per-frame fashion: each video frame has an action la-
bel. We also evaluate all action recognition models using
the per-frame accuracy. The statistics of our dataset are
summarized in Table 1.

Envs

# of cameras

# of subjects

# of volumes

Env1
Env2
Env3

4
8
7

15
12
9

90K
64K
34K

Table 1. Our dataset from three different environments.

The scene includes not only people but also objects such
as sofa, tables, chairs, boxes, drawers, cups, and books.
There are over 20 different subjects in the dataset. They
have different body shapes, gender and heights. The dataset
includes 16 actions in the everyday life: drinking, clap-
ping, reading book, calling, playing with phone, bending,
squatting, waving hands, sitting, pointing, lifting, opening
drawer, pull/pushing, eating, yawning, and kicking. Each
action can be done in a standing or a sitting pose. Here,
action “sitting” means sitting without doing anything.

We compare our proposed method against different base-

line methods. The baselines include:

• ShapeContext256 and ShapeContext512: 3D Shape
context is a 3D version of the shape context [3] de-
scriptor. The 3D shape context has the height axis and
the angle axis uniformly partitioned, and the radial axis
logarithmically partitioned. We test two versions of the
3D shape context: ShapeContext256 has 256 bins and
ShapeContext512 has 512 bins. We build a deep net-
work whose input is the 3D shape context descriptors.
The network uses an LSTM network to aggregate the
temporal information.

• Moment: Moment is another popular shape descriptor.
We use the raw moments up to order 4. Similar to the
above shape context approach, the moment descriptor
is fed into a CNN for action recognition.

• Skeletons: OpenPose [6] is one of the state-of-the-art
stick ﬁgure detectors on RGB images. We normalize
the positions of the joints of each subject using the
neck point and then concatenate the xy coordinates
into a feature vector. We train a deep network using
similar approach to the above shape context method.

11861

• Color+Depth: In this method, we ﬁnd the bounding
boxes of each person based on our tracking result. We
crop the color and depth images of each person in the
video from all the cameras. We train a deep neural
network using the cropped color and depth images and
their action labels. To be fair, we do not use motion in
all the methods in this paper.

• PointNet: PointNet [20] is one of state-of-the-art deep
learning methods for object recognition and semantic
segmentation on 3D point clouds. We extend the Point-
Net model to include an LSTM layer so that it can han-
dle sequential data for action recognition. The network
can be trained end-to-end using the point clouds from
multiple RGBD images.

• I3D and NL-I3D: Inﬂated 3D ConvNet [7] (I3D)
achieves the state-of-the-art action recognition on
RGB videos. We also compare with non-local I3D [27]
(NL-I3D), which introduces non-local operations for
better long-range dependencies modeling.

• SparseConvNet SparseConvNet [10] deﬁnes subman-
ifold convolution, which keeps track of “active” site to
reduce computational overhead. We train SparseCon-
vNet using the 3D volumes along with an LSTM head
to recognize actions in 3D streams.

All the models are implemented using PyTorch. For I3D
and NL-I3D, we use the pre-trained models on Kinetics
dataset [13] and ﬁne-tune them on our dataset. All other
baselines and our proposed models are trained from scratch.
We use the same training, testing and validating splits when
evaluating different methods. To make the models agnostic
to the number of cameras, we train Skeleton, I3D, NL-I3D
on single camera frames. During testing, we aggregate the
predictions from different cameras to obtain the ﬁnal ac-
tion recognition results. Color+Depth also depends on the
number of cameras. However, when trained on single cam-
era frames, its performance is much worse. We only report
its performance on test one and two (using all four camera
frames for training and testing) under environment one.

In our experimentation, we extract the 4D volume repre-
sentation for each person based on our 4D people tracker.
Given each person’s location, we extract a volume cen-
tered at that location. This volume is set large enough to
cover a person with different poses. In particular, we exper-
iment with different voxel sizes. Table 2 shows the results
of two models. Conv3D + ATT is the model in Figure 5
and Conv3D is with similar architecture without the atten-
tion branch. The proposed adaptive convolution layer and
the discriminative loss have not been applied in this exper-
iment. The results in Table 2 shows that we can achieve
better performance with smaller voxel size. In the follow-

(cid:19)(cid:17)(cid:27)(cid:24)

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:26)(cid:24)

(cid:19)(cid:17)(cid:26)

(cid:93)

(cid:19)(cid:17)(cid:25)(cid:24)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:24)(cid:24)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:24)

(cid:19)(cid:17)(cid:23)

(cid:16)(cid:19)(cid:17)(cid:20)(cid:24)

(cid:92)

(cid:16)(cid:19)(cid:17)(cid:21)

(cid:16)(cid:19)(cid:17)(cid:21)(cid:24)

(cid:16)(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:26)(cid:24)

(cid:19)(cid:17)(cid:26)

(cid:93)

(cid:19)(cid:17)(cid:25)(cid:24)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:24)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:25)

(cid:19)(cid:17)(cid:23)(cid:23)

(cid:19)(cid:17)(cid:23)(cid:21)

(cid:19)(cid:17)(cid:23)(cid:27)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:22)(cid:27)

(cid:91)

(cid:16)(cid:19)(cid:17)(cid:20)(cid:24)

(cid:92)

(cid:16)(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:26)(cid:24)

(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:25)(cid:24)

(cid:93)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:24)(cid:24)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:24)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:23)(cid:23)

(cid:19)(cid:17)(cid:23)(cid:21)

(cid:16)(cid:19)(cid:17)(cid:20)(cid:24)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:22)(cid:27)

(cid:19)(cid:17)(cid:22)(cid:25)

(cid:19)(cid:17)(cid:22)(cid:23)

(cid:91)

(cid:92)

(cid:19)(cid:17)(cid:25)(cid:24)

(cid:19)(cid:17)(cid:25)

(cid:93)

(cid:19)(cid:17)(cid:24)(cid:24)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:24)

(cid:16)(cid:19)(cid:17)(cid:21)

(cid:16)(cid:19)(cid:17)(cid:21)(cid:24)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:24)

(cid:91)

(cid:16)(cid:19)(cid:17)(cid:20)

(cid:16)(cid:19)(cid:17)(cid:20)(cid:24)

(cid:92)

(cid:16)(cid:19)(cid:17)(cid:21)

(cid:16)(cid:19)(cid:17)(cid:21)(cid:24)

(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:24)

(cid:91)

(cid:19)(cid:17)(cid:23)

(cid:19)(cid:17)(cid:22)(cid:24)

(cid:19)(cid:17)(cid:27)(cid:24)

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:26)(cid:24)

(cid:93)

(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:25)(cid:24)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:24)(cid:24)

(cid:19)(cid:17)(cid:26)(cid:25)

(cid:19)(cid:17)(cid:26)(cid:23)

(cid:93)

(cid:19)(cid:17)(cid:26)(cid:21)

(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:25)(cid:27)

(cid:19)(cid:17)(cid:25)(cid:25)

(cid:19)(cid:17)(cid:25)(cid:23)

(cid:16)(cid:19)(cid:17)(cid:20)(cid:24)

(cid:16)(cid:19)(cid:17)(cid:21)

(cid:16)(cid:19)(cid:17)(cid:21)(cid:24)

(cid:92)

(cid:16)(cid:19)(cid:17)(cid:22)

(cid:16)(cid:19)(cid:17)(cid:20)(cid:24)

(cid:16)(cid:19)(cid:17)(cid:21)

(cid:92)

(cid:16)(cid:19)(cid:17)(cid:21)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:24)

(cid:19)(cid:17)(cid:22)(cid:24)

(cid:19)(cid:17)(cid:23)

(cid:91)

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:26)(cid:24)

(cid:93)

(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:25)(cid:24)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:24)(cid:24)

(cid:19)(cid:17)(cid:27)(cid:24)

(cid:19)(cid:17)(cid:27)

(cid:19)(cid:17)(cid:26)(cid:24)

(cid:93)

(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:25)(cid:24)

(cid:19)(cid:17)(cid:25)

(cid:19)(cid:17)(cid:24)(cid:23)

(cid:19)(cid:17)(cid:24)(cid:21)

(cid:16)(cid:19)(cid:17)(cid:20)

(cid:16)(cid:19)(cid:17)(cid:22)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:23)

(cid:19)(cid:17)(cid:23)(cid:21)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:27)

(cid:19)(cid:17)(cid:23)(cid:25)

(cid:91)

(cid:16)(cid:19)(cid:17)(cid:20)(cid:24)

(cid:92)

(cid:16)(cid:19)(cid:17)(cid:21)

(cid:16)(cid:19)(cid:17)(cid:21)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:24)

(cid:19)(cid:17)(cid:24)(cid:24)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:25)

(cid:91)

(cid:19)(cid:17)(cid:26)

(cid:19)(cid:17)(cid:25)(cid:24)

(cid:16)(cid:19)(cid:17)(cid:20)(cid:24)

(cid:92)

(cid:16)(cid:19)(cid:17)(cid:21)

(cid:16)(cid:19)(cid:17)(cid:21)(cid:24)

(cid:19)(cid:17)(cid:24)

(cid:19)(cid:17)(cid:23)(cid:24)

(cid:19)(cid:17)(cid:24)(cid:24)

(cid:91)

(cid:19)(cid:17)(cid:25)(cid:24)

(cid:19)(cid:17)(cid:25)

Figure 6. Kinect V2 skeleton estimation is prone to errors if there
is clutter in the scene. Row one: Depth images. Row two: Nor-
malized skeletons from Kinect V2.

ing experiments, we will use the 25mm voxels to evaluate
the proposed models.

Model

Conv3D
Conv3D + ATT
Conv3D
Conv3D + ATT

Voxel Size Volume Size
31 × 31 × 43
31 × 31 × 43
63 × 63 × 85
63 × 63 × 85

50mm
50mm
25mm
25mm

Acc

71.1
74.8
80.5
82.5

Table 2. Accuracies (Acc) of two Conv3D models on similar vol-
ume coverages with different voxel sizes on the data of Env1.

Apart from the target subject, background clutter and
other subjects in the scene are also included in the cropped
volume as shown in Figure 4. Potential approaches, such
as semantic segmentation and 3D skeleton estimation, can
be used to separate a person from the clutter. However, the
results can be unreliable in a “cluttered” environment.

For instance, Figure 6 shows that the skeleton estima-
tion from Kinect V2 becomes increasingly unreliable as the
background clutter increases. When people interact with
large objects and their body parts are occluded by these ob-
jects, the skeleton estimation fails.

In this paper, we therefore do not depend on the semantic
segmentation and 3D skeleton estimation. Instead, we use
the full 4D volume data that contains every bit of informa-
tion for action recognition. In the following, we show the
experimental results on the ground truth data.

3.2. Ground truth experimentation

We conduct the following three tests on our dataset. (1)
Test one: we use 14 single-subject videos performed by dif-
ferent subjects from Env1. The training is on ten videos and
testing is on three videos. One video is used for validation.
It has a total of 68K frames in the training videos, 6K frames
in the validating videos and 10K in the testing videos. (2)
Test two: we take the trained models from test one and
evaluate them on 4 multiple-subject videos also collected
from Env1, which include 3, 3, 3, and 2 people respec-
tively. It has a total of 6K frames for all the multiple-subject
(3) Test three: we also conduct a cross-
testing videos.
environment test to further study the robustness of differ-
ent approaches. We train all models on data from Env1
and Env2. We test all models on data collected from Env3,

11862

which has some non-overlap subjects with Env1 and Env2
as well as single and multiple person videos. One video
from Env3 is used as validating video for model selection.
We report two accuracy numbers for each test. Acc is
stricter: we deem action recognition is correct if and only
if the action prediction result matches the ground truth la-
bel of the corresponding video frame. One issue about this
criterion is that at the action boundaries, accurate labeling
is hard. For transient actions, the small offset of labeling
may cause the mismatch between detection results and the
ground truth. To remedy this issue, we deﬁne another accu-
racy, revised accuracy (RAcc). For RAcc, an action classi-
ﬁcation is correct if and only if the predicted action label is
the same as the ground truth label of a frame within the win-
dow of plus/minus three relative to the current video frame.

Person 1

Person 2

Person 3

Average

Models

Acc RAcc Acc RAcc Acc RAcc Acc RAcc
56.9
60.5
ShpCtx256
58.9
55.2
ShpCtx512
48.6
Moments
37.4
63.9
Color+Depth 53.6
71.0
66.7
Skeleton
PointNet
58.9
63.5
77.4
69.5
SparseNet
84.8
77.5
I3D
NL- I3D
73.7
84.0

54.7
53.4
40.1
56.6
64.9
58.7
70.3
76.7
75.5

61.6
62.6
47.1
60.1
62.8
63.8
76.4
82.4
82.9

55.7
56.8
38.4
52.7
56.8
57.9
69.7
74.2
74.2

56.5
53.1
54
71.7
79.1
79.1
79.9
87.8
88.1

51.5
47.6
44.9
64.1
72.1
76.5
71.9
78.8
78.8

63.1
60.5
44.9
60.5
72.2
63.7
76.1
84.6
81.3

Action4DNet
(w/o ATT)
Action4DNet 84.0

83.6

90.2

79.2

86.8

79.1

85.9

80.6

87.5

89.6

84.7

91.9

83.6

90.0

84.1

90.4

Table 3. Evaluation of the proposed models and several baselines
on test one. We show percentages of both the accuracies (Acc) and
the revised accuracies (RAcc) of all the evaluated models.

Table 3 shows the accuracies of different competing
methods in ground truth test one. In this test, our proposed
Action4DNet trained with adaptive convolutional layer and
the proposed discriminative loss achieves the highest av-
erage revised accuracy (RAcc) 90.0%. We also train the
model without using the attention model, which obtains
worse performance than Action4DNet. However, it still
performs better than all baselines. Our method’s accuracy
improves by more than 30% over the competing methods
such as ShapeContext, Moment, Color+Depth, Skeleton
and PointNet and by about 6% over recent deep learning
based approaches on RGB videos including I3D, NL-I3D
and SparseNet. We also achieve the highest accuracies in
each individual test.

These results are not a surprise. The handcrafted features
such as shape context and moments are not as strong as the
learned features from deep learning especially when there
is strong background clutter. The PointNet gives low accu-
racies in this experiment. This is likely due to the strong
clutter and because PointNet has to sample the point clouds
to ﬁt into the GPU memory. The Color+Depth and Skeleton
approaches perform better than other handcrafted feature

methods, but they give much worse results than our pro-
posed method. I3D and NL-I3D show better performance
over other approaches. However, both methods are also
dependent on the camera views: if the camera settings are
different, we have to retrain the model. In contrast, our pro-
posed method can be used in different camera settings with-
out retraining. The input to SparseNet is view-independent
as well. However, it shows worse performance than our
model. The following test two and test three conﬁrms the
generability of our approach.

Models

Acc RAcc

ShapeContxt
ShapeContxt16
Moments
Color+Depth
Skeleton
PointNet
SparseNet
I3D
NL-I3D
Action4DNet (w/o ATT)
Action4DNet

37.5
34.2
36.2
46.1
53.8
58.9
60.4
58.1
56.4
79.9
86.3

43.6
39.1
44.5
56.6
62.0
64.6
68.3
66.7
64.2
87.1
93.3

Table 4. Evaluation of the proposed models and several baselines
in ground truth test two, which involves multiple people.

We use the same model trained in test one to evaluate
all the multiple people videos in test two. As shown in Ta-
ble 4, our method still achieves the highest accuracy among
all the methods. PointNet and SparseNet show less perfor-
mance degradations due to their color agnostic inputs. All
other competing approaches show worse accuracies due to
multiple people mutual occlusions and background clutter.
As a matter of fact, our Action4DNet even shows better per-
formance than in test one. This could be attributed to the
short duration and thus less variation of actions in each of
the testing video in test two.

Models

Acc RAcc

Skeleton
PointNet
SparseNet
I3D
NL-I3D
Action4DNet (w/o ATT)
Action4DNet

45.0
49.4
68.2
58.1
61.3
74.8
81.4

49.4
53.8
73.3
65.3
68.3
80.5
87.0

Table 5. Results on cross-environment testing. We train all the
models on data from Env1 and Env2 and test them on Env3.

Due to space limit, we only include the performance of
deep learning approaches for test three. The results are
shown in Table 5. Again, the proposed model shows bet-

11863

l
e
b
a
l
 
e
u
r
T

None
Drink
Clap
Read
Phone call
Play phone
Bend down
Squat
Wave
Sit
Point
Lift/hold box
Open drawer
Pull/Push sth
Eat
Yawn
Kick

0.920.000.010.010.000.000.000.000.010.030.000.000.000.010.010.000.00
0.010.940.000.000.050.000.000.000.000.000.000.000.000.000.000.000.00
0.000.000.970.010.000.020.000.000.000.000.000.000.000.000.000.000.00
0.000.000.000.990.000.000.000.000.000.000.000.000.000.000.000.000.00
0.040.010.000.000.740.000.000.000.000.200.000.000.000.000.000.000.00
0.040.000.090.030.000.690.000.000.000.130.010.000.000.000.000.000.00
0.050.000.000.000.000.000.950.000.000.000.000.000.000.000.000.000.00
0.000.000.000.000.000.000.001.000.000.000.000.000.000.000.000.000.00
0.060.000.000.000.000.000.000.000.940.000.010.000.000.000.000.000.00
0.040.000.000.030.060.000.000.000.000.880.000.000.000.000.000.000.00
0.170.000.000.000.000.000.000.000.070.010.730.000.000.000.000.010.00
0.000.000.000.000.000.000.000.000.000.000.001.000.000.000.000.000.00
0.030.000.000.000.000.000.000.000.000.000.000.000.970.000.000.000.00
0.040.000.000.000.000.000.000.000.000.000.000.000.000.960.000.000.00
0.000.000.030.010.000.000.000.000.000.000.000.000.000.000.960.000.00
0.020.000.010.000.000.000.000.000.160.030.000.000.000.000.000.780.00
0.120.000.000.020.000.000.000.000.000.000.000.070.000.000.000.000.79
D rin k
N o n e

C la p R ea d

E at
Y a w n

S q u at

W a v e

K ic k

Sit

P h o n e call
Pla y p h o n e

B e n d d o w n

P ull/P ush sth
Lift/h old b o x
P oint
O p e n dra w er

Predicted label
(a) Test one

None
Drink
Clap
Read
Phone call
Play phone
Bend down
Squat
Wave
Sit
Point
Lift/hold box
Open drawer
Pull/Push sth
Eat
Yawn
Kick

0.960.000.000.000.010.000.000.000.010.000.000.000.000.010.000.000.01
0.100.880.000.000.000.000.000.000.020.000.000.000.000.000.000.000.00
0.000.001.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00
0.010.000.000.940.000.010.000.000.000.010.000.010.000.000.010.000.00
0.060.000.000.000.940.000.000.000.000.000.000.000.000.000.000.000.00
0.030.000.000.000.000.970.000.000.000.000.000.000.000.000.000.000.00
0.440.000.000.000.000.000.380.000.000.000.000.000.020.150.000.000.00
0.000.000.000.000.000.000.001.000.000.000.000.000.000.000.000.000.00
0.020.000.000.000.000.000.000.000.900.000.080.000.000.000.000.000.00
0.040.000.000.000.010.000.000.000.010.930.000.010.010.010.000.000.00
0.000.000.000.000.000.000.000.000.030.000.970.000.000.000.000.000.00
0.040.000.000.000.000.000.000.000.000.000.000.910.000.050.000.000.00
0.430.000.000.000.000.000.000.000.000.000.000.150.420.000.000.000.00
0.010.000.000.000.000.000.000.000.000.000.000.010.000.980.000.000.00
0.020.000.010.010.000.000.000.000.000.000.000.000.000.000.970.000.00
0.000.000.000.000.000.000.000.000.130.000.000.010.000.000.000.860.00
0.050.000.000.000.000.000.000.000.000.000.000.000.030.000.000.000.92
D rin k
N o n e

C la p R ea d

E at
Y a w n

S q u at

W a v e

K ic k

Sit

P h o n e call
Pla y p h o n e

B e n d d o w n

P ull/P ush sth
Lift/h old b o x
P oint
O p e n dra w er

Predicted label
(b) Test two

None
Drink
Clap
Read
Phone call
Play phone
Bend down
Squat
Wave
Sit
Point
Lift/hold box
Open drawer
Pull/Push sth
Eat
Yawn
Kick

0.920.000.010.000.020.010.000.000.000.000.000.000.000.020.000.000.00
0.070.820.000.000.070.000.000.000.000.000.010.000.000.000.000.030.00
0.050.000.890.000.000.050.000.000.000.000.000.000.000.000.000.000.00
0.110.000.030.830.000.030.000.000.000.000.000.000.000.000.000.000.00
0.050.000.010.000.930.000.000.000.000.000.000.000.000.000.000.000.00
0.150.000.060.000.000.760.000.000.000.000.000.000.000.000.000.000.00
0.440.000.000.000.000.000.530.000.000.000.000.030.000.000.000.000.00
0.070.000.000.000.000.000.020.870.000.000.000.040.000.000.000.000.00
0.110.000.000.000.000.000.000.000.840.000.050.000.000.000.000.000.00
0.020.000.000.000.000.000.000.000.000.960.010.000.000.000.000.000.00
0.070.000.000.000.000.000.000.000.030.000.890.000.000.000.000.010.00
0.010.000.000.000.000.000.000.000.000.000.000.990.000.000.000.000.00
0.090.000.000.000.000.000.000.000.000.000.000.030.850.030.000.000.00
0.240.000.000.000.000.000.000.000.000.000.000.010.010.750.000.000.00
0.130.020.010.000.020.000.000.000.000.000.000.000.000.000.830.000.00
0.210.010.010.000.000.000.000.000.010.000.000.010.000.000.000.750.00
0.080.000.000.000.020.000.000.000.000.000.000.020.000.000.000.000.89
D rin k
N o n e

C la p R ea d

E at
Y a w n

S q u at

W a v e

K ic k

Sit

P h o n e call
Pla y p h o n e

B e n d d o w n

P ull/P ush sth
Lift/h old b o x
P oint
O p e n dra w er

Predicted label
(c) Test three

Figure 7. Confusion matrix for ground truth test of our Action4DNet model.

ter performance against all baselines. In addition, the ac-
curacies of our model are comparable with our results in
Table 3, which is tested in the same environment. This sug-
gests that our approach is general and robust against back-
ground changes. Other approaches, including I3D and NL-
I3D are severely impacted by the different backgrounds and
lightings in different environments.

Table 3, Table 4 and Table 5 show that our proposed
method consistently gives much better results than all the
competing methods. The high accuracy also beneﬁts from
our reliable 4D people tracker, which obtains 100% track-
ing rate for all the testing and training videos. Our method
is also fast, with a single GTX1080 TI, our method is able
to track 10 people and infer their actions at 15 frames per
second (FPS) on volumes with 50mm × 50mm × 50mm
voxel size. On the 25mm × 25mm × 25mm voxels, it is
possible to recognize actions at 25 FPS on a single person.
Figure 7 shows the confusion matrices of our Ac-
tion4DNet on the three different tests. It is interesting to see
that there are many missing detections in test two and test
three. Especially, for the bend down action, both test two
and test three have over 40% missing recognitions. This
is potentially due to the large variations of this action and
the inconsistent labeling standards used by different ground
truth labelers. Meanwhile, our method also confuses some
actions as seen in Figure 7. This is mostly due to the noisy
data from the Kinect sensor. Using better depth cameras and
better time synchronization, our action recognition results
can be further improved. Moreover, we can further include
other voxel attributes such as color and use multi-resolution
volume data to achieve more robust results.

3.3. Ablation study

We evaluate the impact of the proposed adaptive convo-
lution layer and the discriminative temporal feature learn-
ing on action recognition performance. Table 6 lists the re-
sults. We show the accuracies of Action4DNet on test one,

Models

Test one Test two Test three

Action4DNet-A-D
Action4DNet-D
Action4DNet-A
Action4DNet

82.5
81.6
82.6
84.1

82.5
85.4
85.3
86.3

76.4
80.6
81.3
81.4

Table 6. Ablation study on the proposed adaptive convolution and
the discriminative loss. We use “-A” and “-D” to represent the
model variations without adaptive convolution and without dis-
criminative loss respectively.

test two and test three respectively. The proposed adap-
tive convolution layer introduces more parameters. Thus, it
may not help the model without the discriminative temporal
feature learning as shown in test one. However, the results
in test two and test three suggest that the adaptive convo-
lution layer indeed improves the generability of the model
on different settings. The discriminative loss improves the
performance over baseline Action4DNet-A-D in all three
tests. Overall, the results indicate that the two proposed
mechanisms are effective in learning better action recogni-
tion models from 4D volumes.

4. Conclusion

We propose a novel online 4D action recognition
method, the Action4DNet, which is able to generate 4D vol-
umes of the environment, track each person in the volume
and infer the actions of each subject. Our method is able
to handle multiple people and strong clutter. In particular,
the proposed adaptive convolution layer and the discrimina-
tive temporal feature learning objective further improve the
performance of our model. Our experimental results under
different settings conﬁrm that our method gives better per-
formance over different competing methods. The proposed
method can be deployed to enable different applications to
enhance how people interact with the environment.

11864

References

[1] Kidsromm.

http://vismod.media.mit.edu/

vismod/demos/kidsroom/kidsroom.html. 2

[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473, 2014. 4

[3] Serge Belongie, Jitendra Malik, and Jan Puzicha. Shape
matching and object recognition using shape contexts. IEEE
Transactions on Pattern Analysis & Machine Intelligence,
24(4):509–522, 2002. 5

[4] M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri.
Actions as space-time shapes. In Tenth IEEE International
Conference on Computer Vision (ICCV’05), volume 2, pages
1395–1402 Vol. 2, 2005. 2

[5] Cristian Canton-Ferrer, Josep R Casas, and Montse Pardas.
Human model and motion based 3d action recognition in
multiple view scenarios. In 2006 14th European Signal Pro-
cessing Conference, pages 1–5. IEEE, 2006. 2

[6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 7291–7299, 2017. 5

[7] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In Com-
puter Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on, pages 4724–4733. IEEE, 2017. 2, 6

[8] Ali Diba, Vivek Sharma, and Luc Van Gool. Deep temporal
linear encoding networks. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), July 2017. 2

[9] Alexei A Efros, Alexander C Berg, Greg Mori, and Jitendra
Malik. Recognizing action at a distance. In ICCV 2003, page
726. IEEE, 2003. 2

[10] Benjamin Graham, Martin Engelcke, and Laurens van der
Maaten. 3d semantic segmentation with submanifold sparse
convolutional networks. Proceedings of the IEEE Computer
Vision and Pattern Recognition CVPR, Salt Lake City, UT,
USA, pages 18–22, 2018. 6

[11] Michael B. Holte, Cuong Tran, Mohan M. Trivedi, and
Thomas B. Moeslund. Human action recognition using mul-
tiple views: A comparative perspective on recent develop-
ments. In Proceedings of the 2011 Joint ACM Workshop on
Human Gesture and Behavior Understanding, pages 47–52,
2011. 2

[12] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari,
and Cordelia Schmid. Action tubelet detector for spatio-
temporal action localization.
In IEEE International Con-
ference on Computer Vision, ICCV 2017, pages 4415–4423,
2017. 2

[13] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-
man action video dataset. arXiv preprint arXiv:1705.06950,
2017. 6

[14] Ivan Laptev and Tony Lindeberg. Space-time interest points.
In Proceedings of the Ninth IEEE International Conference
on Computer Vision, pages 432–, 2003. 2

[15] A. Laurentini. The visual hull concept for silhouette-based
image understanding. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 16(2):150–162, 1994. 2

[16] Min Lin, Qiang Chen, and Shuicheng Yan. Network in net-

work. arXiv preprint arXiv:1312.4400, 2013. 4

[17] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C
Kot. Global context-aware attention lstm networks for 3d
action recognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1647–
1656, 2017. 2

[18] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Re-
current models of visual attention.
In Advances in neural
information processing systems, pages 2204–2212, 2014. 4
[19] Christos H Papadimitriou and Kenneth Steiglitz. Combinato-
rial optimization: algorithms and complexity. Courier Cor-
poration, 1998. 3

[20] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 652–660,
2017. 6

[21] Hossein Rahmani and Mohammed Bennamoun. Learning
action recognition model from depth and skeleton videos. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 5832–5841, 2017. 2

[22] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.
Ntu rgb+d: A large scale dataset for 3d human activity analy-
sis. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016. 2

[23] E. Shechtman and M. Irani. Space-time behavior based cor-
relation.
In IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition (CVPR’05), volume 1,
pages 405–412 vol. 1, 2005. 2

[24] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip H. S.
Torr, and Fabio Cuzzolin. Online real-time multiple spa-
tiotemporal action localisation and prediction. In The IEEE
International Conference on Computer Vision (ICCV), Oct
2017. 2

[25] Hongsong Wang and Liang Wang. Modeling temporal
dynamics and spatial conﬁgurations of actions using two-
stream recurrent neural networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), July
2017. 5

[26] Pichao Wang, Wanqing Li, Zhimin Gao, Yuyao Zhang,
Chang Tang, and Philip Ogunbona. Scene ﬂow to action
map: A new representation for rgb-d based action recog-
nition with convolutional neural networks.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017. 2

[27] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2018. 6

[28] Yang Wang, Hao Jiang, Mark S Drew, Ze-Nian Li, and Greg
Mori. Unsupervised discovery of action classes.
In Com-
puter Vision and Pattern Recognition, 2006 IEEE Computer
Society Conference on, volume 2, pages 1654–1661. IEEE,
2006. 2

11865

[29] Daniel Weinland, Remi Ronfard, and Edmond Boyer. Free
viewpoint action recognition using motion history volumes.
Computer vision and image understanding, 104(2-3):249–
257, 2006. 2

[30] Mohammadreza Zolfaghari, Gabriel L. Oliveira, Nima
Sedaghat, and Thomas Brox. Chained multi-stream networks
exploiting pose, motion, and appearance for action classiﬁ-
cation and detection. In The IEEE International Conference
on Computer Vision (ICCV), Oct 2017. 2

11866

