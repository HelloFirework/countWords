NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences

Chen Zhao1, Zhiguo Cao1, Chi Li1, Xin Li2, and Jiaqi Yang ∗1

1School of Artiﬁcial Intelligence and Automation, Huazhong University of Science and Technology

2Lane Department of Computer Science and Electrical Engineering, West Virginia University

1{hust zhao, zgcao, li chi, jqyang} @hust.edu.cn

2Xin.Li @mail.wvu.edu

Abstract

Feature correspondence selection is pivotal to many
feature-matching based tasks in computer vision. Search-
ing for spatially k-nearest neighbors is a common strategy
for extracting local information in many previous works.
However, there is no guarantee that the spatially k-nearest
neighbors of correspondences are consistent because the
spatial distribution of false correspondences is often irreg-
ular. To address this issue, we present a compatibility-
speciﬁc mining method to search for consistent neighbors.
Moreover, in order to extract and aggregate more reliable
features from neighbors, we propose a hierarchical network
named NM-Net with a series of convolution layers taking
the generated graph as input, which is insensitive to the
order of correspondences. Our experimental results have
shown the proposed method achieves the state-of-the-art
performance on four datasets with various inlier ratios and
varying numbers of feature consistencies.

1. Introduction

Searching for good feature correspondences (a.k.a.
matches) is a fundamental step in computer vision tasks -
e.g., structure from motion [32], simultaneous location and
mapping [2], panoramic stitching [4], and stereo matching
[14]. Finding consistent feature correspondences between
two images relies on two key steps [22, 3, 23] - i.e., feature
matching and correspondence selection. Speciﬁcally, initial
correspondences can be obtained by matching local key-
point features such as SIFT [22]. Due to various reasons
(e.g., key-point localization errors, limited distinctiveness
of local descriptors, and illumination/viewpoint changes),
mismatches are often inevitable. To address this issue, cor-
respondence selection can be employed as a postprocessing

∗Corresponding author

(a) Image segmentation

(b) Point cloud segmentation

(c) Correspondence selection

Figure 1. Comparison of (c) correspondence selection (can be
viewed as a binary classiﬁcation task for each correspondence)
with standard segmentation problems including (a) image segmen-
tation and (b) point cloud segmentation. Most spatially adjacent
pixels and points in (a) and (b) are semantically consistent (belong-
ing to the same class), yet the spatial distribution of mismatches in
(c) is irregular, resulting in many outliers (red dots) contaminated
in the local regions around inliers (green crosses). For visualiza-
tion, image correspondences (4D) are projected to a 2D space via
tsne [24].

step to ensure correct matches and improve the accuracy [3].
This paper focuses on a learning-based approach toward se-
lecting correct matches from an initial set of feature corre-
spondences [36].

Feature correspondence selection is challenging due to
the scarcity of available information as well as the limita-
tion of local feature representations. Spatial positions of
matched features are discrete and irregular (note that RGB
or texture information is not available any more). To effec-
tively mine the consistency from raw positions, spatially lo-
cal information is often employed in previous hand-crafted

215

algorithms [20, 3, 23]. Indeed, spatially local information
has played an important role in image segmentation [21]
and point cloud segmentation [26]. As shown in Fig. 1 (a)
and (b), most feature points located in adjacent regions are
semantically consistent (belonging to the same class). How-
ever, spatially local information is unreliable for correspon-
dence selection due to irregular distribution of mismatches.
As shown in Fig. 1 (c), around the vicinity of correct cor-
respondences (marked by green crosses), a large number
of mismatches (denoted by red dots) can be found. To
overcome this difﬁculty, we present a compatibility-speciﬁc
neighbor mining algorithm to search for top-k consistent
neighbors of each correspondence. Correspondences are
determined to be compatible if they meet the same under-
lying constraint [9, 1, 5]. When compared with spatially k-
nearest neighbor (knn) search, the proposed neighbor min-
ing approach is more reliable because potential inliers ex-
hibit guaranteed consistency with each other [17].

Besides neighbor mining, another important issue is to
ﬁnd a proper representation for correspondence selection.
Representations learned by various convolution neural net-
works (CNN) have become the standard in many computer
vision tasks [16, 31, 13]. Correspondence selection can
also be regarded as a binary classiﬁcation problem for each
match - i.e., correct (inlier) vs. false (outlier). Nevertheless,
it is often impractical to directly use a CNN to extract fea-
tures from unordered and irregular correspondences. The
ﬁrst learning-based method for correspondence selection
using multi-layer perceptron is proposed recently in [36],
but unfortunately it has ignored useful local information
such as those obtained by compatibility-speciﬁc neighbor
mining (shown to be advantageous in our work). To ﬁll
this gap, we propose a hierarchical deep learning network
called NM-Net (neighbors mining network), where features
are successively extracted and aggregated. Compatibility-
speciﬁc local information is utilized from two aspects: 1) a
graph is generated for each correspondence where the nodes
denote compatible neighbors found by our neighbor mining
approach; 2) features are extracted and aggregated by a set
of convolution layers that take the generated graph as input.
In a nutshell, the contributions of this paper are as fol-

lows:

• We suggest that compatibility-speciﬁc neighbors are
more reliable (have stronger local consistency) for fea-
ture correspondences than spatial neighbors.

• We propose a deep classiﬁcation network called NM-
Net1 that fully mines compatibility-speciﬁc locality
for correspondence selection, with hierarchically ex-
tracted and aggregated local correspondences. Our

1The

code will
sailor-z/NM-Net

be

available

at https://github.com/

network is also insensitive to the order of correspon-
dences.

• Our method achieves the state-of-the-art performance
on comprehensive evaluation benchmarks including
correspondences with various proportions of inliers
and varying numbers of feature consistencies.

2. Related Work

Parametric methods. Generation-veriﬁcation is arguably
the most popular formulation of parametric methods such
as RANSAC [9] and its variations (e.g., PROSAC [6], LO-
RANSAC [7], and USAC [27]). The consistency (compati-
bility) of correspondences is searched under a global con-
straint. Speciﬁcally, the generation and veriﬁcation pro-
cedures are alternatively used to estimate a global trans-
formation - e.g., homography matrix or essential matrix.
Correspondences consistent with the transformation are se-
lected as inliers. Parametric methods have two fundamental
weaknesses: 1) the accuracy of estimated global transfor-
mation severely degrades when initial inlier ratio is low [18]
because the sampled correspondences may include no in-
lier; 2) the assumed global transformation is unsuitable for
the case of multi-consistency matching [37] and non-rigid
matching [23].
Non-parametric methods. Leveraging local information
for correspondence selection is a popular strategy in non-
parametric methods. For example, a locality preserving
matching algorithm is presented in [23], which assumes
that local geometric structures in the vicinity of inliers are
invariant under rigid/non-rigid transformations, where the
spatially knn search is utilized to represent variations of lo-
cal structures. The spatially local information is exploited
in a statistical manner in [3]. The similarity of local re-
gions between two images is measured by the number of
correspondences; all correspondences located in the regions
are considered inliers if the number is larger than a pre-
deﬁned threshold. Additionally, local compatible informa-
tion has been explored by other non-parametric approaches
- e.g.,
[1] measured the compatibility of each two cor-
respondences as a payoff in a game-theoretic framework,
where the probability of correct correspondences is itera-
tively calculated by ESS’s algorithm [34];
[17] estimated
an afﬁnity matrix to represent the compatibility of corre-
spondences and proposed a spectral technique to select in-
liers. Although these algorithms involve the compatibil-
ity information among correspondences, they do not sufﬁ-
ciently mine local information from compatible correspon-
dences. By contrast, we use compatibility-speciﬁc neigh-
bors to integrate local information to each correspondence
via a data-driven manner.
Learning-based methods. Deep learning has achieved
great success in recent years - e.g., image classiﬁcation [16,

216

31, 13], object detection [11, 10, 29], and image segmenta-
tion [21]. However, directly employing a standard CNN is
infeasible to correspondence selection because correspon-
dence representations are irregular and unordered. In [36],
a deep learning framework based on multi-layer perceptron
is employed to ﬁnd inliers but without any local feature ex-
traction or aggregation. Considering that point-cloud data
has the similar characteristics with correspondences, Point-
Net [25] and PointNet++ [26] can be referred for correspon-
dence selection, which have been developed for point cloud
classiﬁcation and segmentation recently. Nonetheless, each
point is individually processed in PointNet without any lo-
cal information involved; spatially nearest information is
exploited by PointNet++ in a grouping layer even though
such spatially local information could be unreliable for cor-
respondences. Different from these learning-based meth-
ods for irregular data, our approach covers both locality se-
lection and locality integration concerns via a compatibility
metric and a hierarchical manner, respectively.

3. Motivation

Finding consistency (compatibility) among matches and
selecting good correspondences is a chicken-and-egg prob-
lem: ﬁnding the consistency (for an assumed global trans-
formation as an example) would require a set of inliers (i.e.,
the knowledge about good correspondences); but meantime,
the selection of inliers also relies on the results of ﬁnding re-
liable consistency. To get around this circular problem, we
propose to utilize local information of correspondences as
an surrogate representation for feature consistency.

Local information has been the cornerstone in many
learning-based methods for image/point cloud classiﬁca-
tion and segmentation[31, 13, 21, 25], where local context
features in convolution kernels are commonly extracted.
Since correspondence selection can be considered as a bi-
nary classiﬁcation problem for each correspondence (i.e.,
inlier vs. outlier), it appears plausible to mine reliable lo-
cal information for establishing good correspondences. As
aforementioned, hand-crafted methods have employed spa-
tially local information to select correct matches. However,
unlike the cases of images and point clouds, directly us-
ing spatially local information for feature correspondence
selection is not a good idea as shown in Fig. 2 (a) - the spa-
tially selected k-nearest neighbors of an inlier are incom-
patible with two outliers. By contrast, the neighbors picked
by a compatibility metric (which will be formally deﬁned
in the next section) are consistent as shown in Fig. 2 (b),
but their positions are not necessarily to be spatially adja-
cent to the query correspondence. Since the matched key-
points of an inlier indicate the same 3D position from dif-
ferent viewpoints in the real world, the consistency among
inliers is readily guaranteed. Such observation motivates us
to develop a compatibility-speciﬁc method to mine consis-

(a) Spatially k-nearest neighbors

(b) Compatibility-speciﬁc k-nearest neighbors

Figure 2. Visual illustration of (a) spatially k-nearest neighbors
and (b) compatibility-speciﬁc k-nearest neighbors of feature cor-
respondences. The blue lines represent initial feature correspon-
dences between two images, the yellow line denotes a sampled in-
lier, and the green lines and red lines respectively imply the neigh-
bors of the sampled inlier being inliers and outliers. Two outliers
are included in (a) the spatially k-nearest neighbors, which are
considered as inconsistent portions; (b) the compatibility-speciﬁc
k-nearest neighbors are consistent without outliers, but their posi-
tions are not necessarily to be spatially close.

(a) < 20%

(b) 20 − 35%

(c) 35 − 50%

(d) > 50%

Figure 3. Comparison between the spatially knn (SP knn) and the
compatibility-speciﬁc knn (CS knn). Hundreds of image pairs
are sampled from our experimental datasets (Sect. 5) and divided
into 4 parts according to inlier ratios of the initial correspondence
sets, - i.e., (a) lower than 20%, (b) from 20% to 35%, (c) from 35%
to 50%, and (d) higher than 50%. The assessment metric is the
average inlier ratio of the neighbors of correct correspondences.

tent neighbors.

The superiority of compatibility-speciﬁc neighbor min-
ing can be further justiﬁed by the statistics collected from
experimental datasets (Sect. 5) as shown in Fig. 3. The per-
centages of inliers in compatibility-speciﬁc nearest neigh-
bors are remarkably higher than those results in spatially

217

nearest neighbors in all cases, with the gap being more
dramatic as k increases. These empiric ﬁndings strongly
suggest that neighbors chosen by the compatibility-speciﬁc
method are more reliable.

4. Method

In addition to neighbor mining, correspondence selection
requires an appropriate representation of the information
provided by reliable neighbors. Built upon the success of
deep learning in many visual recognition tasks [13, 29, 21],
a learning-based method was developed for correspondence
selection in [36]. A key strategy behind [36] is to use multi-
layer perceptron that individually processes unordered cor-
respondences. Unfortunately, this recent work fails to inte-
grate local information for each correspondence; a key new
insight of our work is to demonstrate the beneﬁts of exploit-
ing locality for correspondence selection by our proposed
NM-Net. Our framework employs ResNet [13] as the back-
bone and compatibility-speciﬁc neighbor mining algorithm
as the grouping module. The 4D raw correspondences are
taken as input and the classiﬁcation of each correspondence
(i.e., inlier or outlier) is output.

4.1. Problem Statement

′

′

′

′

′

= {p

Given a pair of images (I, I
= {k

), two sets of discrete key-
points (K = {k}, K
}) are detected and local
patterns around those keypoints are described as (P =
}). The initial correspondence set C = {c} is
{p}, P
generated by brute-force matching between (K, K
) based
on descriptor similarities. The correspondence selection
boils down to a binary classiﬁcation problem focusing on
the classiﬁcation of each c as an inlier cinlier ∈ Cinlier or
an outlier coutlier ∈ Coutlier.

′

4.2. Mining Neighbors

Compatibility-speciﬁc neighbor mining plays a crucial
role in our network for the following two reasons. First,
it explores the local space of each correspondence and ex-
tracts local information by our proposed compatibility met-
ric. Second, it integrates unordered correspondences into a
graph in which nodes correspond to the mined neighbors so
that convolutions can be performed for further feature ex-
traction and aggregation. As mentioned in Sect. 3, given a
pair of correspondences (ci, cj), quantifying the compati-
bility score denoted by s(ci, cj) is non-trivial due to lack-
ing of label information. One promising attack is that the
j)
variations of local structures around (ki, k
are similar if ci and cj are compatible [1]. Based on this
important observation, we propose to compute s(ci, cj) by
exploiting the variations as follows.

i) and (kj, k

′

′

First, the Hessian-afﬁne detector [15] is used to detect
keypoints; it provides the local afﬁne information around

keypoints that is required by the introduced compatibil-
ity metric. Local afﬁne information is critical for search-
ing for consistent correspondences when images undergo
viewpoint or scale changes [5]. Second, a transformation
characterizing the variation between local structures around
(ki, k

i) of ci can be calculated by

′

Hi = T

′

iT−1

i

,

(1)

i) describe the posi-
where a pair of 3 × 3 matrices (Ti, T
tions and local structures of the matched keypoints. Then
we can calculate Ti by (the same for T

′

i)

′

1(cid:21) ,
Ti = (cid:20)Ai ki

0

(2)

where Ai is a 2 × 2 matrix representing the local afﬁne in-
formation extracted by the Hessian-afﬁne detector and ki
is the position of the keypoint. Third, intuitively, ci and cj
are compatible if the corresponding transformations Hi and
Hj are consistent; in other words, local structure variations
estimated by a consistent pair of transformations should be
similar. Consequently, we have adopted reprojected errors
that represent local structure variations to measure the dis-
similarity of (Hi, Hj) by

,

(3)

1(cid:21))(cid:12)(cid:12)(cid:12)(cid:12)
1(cid:21)) − ρ(Hi ·(cid:20)ki

ej(ci) = (cid:12)(cid:12)(cid:12)(cid:12)
ρ(Hj ·(cid:20)ki
c(cid:3)T
where ρ((cid:2)a b

) = (cid:2)a/c

. Note that the com-
patibility of (ci, cj) is negatively correlated with the sum of
reprojected errors (ej(ci)+ei(cj)). As a strategy of normal-
izing sij to the range of (0, 1], we propose to use a Gaussian
kernel - i.e.,

b/c(cid:3)T

s(ci, cj) = e−λ(ej (ci)+ei(cj )),

(4)

where λ is a hyper-parameter. Note that λ will not affect the
ranking of s(ci, cj); so the search of compatible neighbors
for each correspondence is insensitive to λ. For any given
ci, a graph Gi is generated by ﬁrst selecting the neighbors

of ci, - i.e.,ncj
with top-k s(ci, cj
its neighbors.

io (j = 1, 2, ..., k), as those correspondences

i ), and then sequentially linking ci with all

4.3. Network Architecture

The network architecture as shown in Fig.4 includes two
key modules - i.e., grouping and ResNet blocks. Our net-
work design is partially inspired by hierarchically extracted
features to fully leverage correspondence-level local infor-
mation (e.g., PointNet++ [26]). The details of NM-Net are
given as follows.
Feature extraction and aggregation. In NM-Net, features
are extracted and aggregated along three lines. First, we

218

(a) Network architecture

(b) Feature aggregation

Figure 4. NM-Net architecture. NM-Net is (a) a classiﬁcation
network for feature correspondences. A grouping module is de-
signed to ﬁrst mine reliable neighbors via the compatibility metric
and then convert them into a graph for each correspondence to
achieve ordered representations. In the bottom-left dashed box of
(a), the black dot denotes a query correspondence and those blue
dots are its compatibility-speciﬁc neighbors. (b) illustrates the hi-
erarchical aggregation of the features extracted from neighbors by
a series of ResNet blocks.

use a grouping module to extract local information for each
correspondence, where compatibility-speciﬁc knn search is
adopted. The unordered raw correspondences are converted
to N graphs Gi(i = 1, 2, ..., N ), where the nodes are sorted
by s(ci, cj
i ), resulting in a set of regular organizations that
are invariant to the order of correspondences. Second, fea-
tures are hierarchically extracted and aggregated by a set of
convolutions as

f n
j =

d

Xj=1

ωj f n−1

j

,

(5)

where f n
is the feature of the j-th node in Gi at the n-th
j
layer, d is the size of convolution kernel, and ωj denotes
the learned weight. The N × k feature map is successively
aggregated into N × 1, with the feature dimension being
increased from 32 to 256 as shown in Fig. 4 (b). In con-
trast to [36], our convolutions take regular graphs as input
instead of isolated correspondences, with reliably captured
local features. Third, for global feature extraction, Instance
Normalization [33] is used to normalize the feature map in
each ResNet block, which has been proven more effective
than average-pooling and max-pooling in [36].
Loss function. A simple yet effective cross-entropy loss
function is used to calculate the deviation between the out-

puts and corresponding labels - i.e.,

L(ω, C) =

1
N

N

Xi=1

αiH (yi, S (g (ci, ω))),

(6)

where g is the output of NM-Net, S indicates the logistic
function, yi denotes the ground-truth label of ci, H rep-
resents a binary cross entropy function, and αi is a self-
adaptive weight to balance positive and negative samples.
The regression loss in [36] is not used because the ground-
truth global transformation may be unavailable in some ap-
plications such as multi-consistency matching and non-rigid
matching. As we will show next, our simpler form of loss
function can achieve even better performance.

5. Experiments

This section includes extensive experimental evaluations
on four standard datasets covering a variety of contexts -
i.e., narrow and wide baseline matching, matching for re-
construction (i.e., structure from motion), and matching
with multiple consistencies. We also present comprehen-
sive comparisons with several state-of-the-art methods in-
cluding both hand-crafted approaches (i.e., RANSAC [9],
GTM [1], and LPM [23]) and learning-based approaches
(i.e., PointNet [25], PointNet++ [26], and LGC-Net [36]).

5.1. Experimental Setup

Optimization and architecture details. The conﬁguration
of NM-Net (Fig. 4 (a)) is C(32, 1, 4)-GP-R(32, 1, 3)-R(32,
1, 3)-R(64, 1, 3)-R(64, 1, 3)-R(128, 1, 3)-R(128, 1, 3)-
R(256, 1, 3)-R(256, 1, 3)-C(256, 1, 1)-C(1, 1, 1), where
C(n, h, w) denotes a convolution layer with n output chan-
nels and a h×w convolution kernel, GP indicates the group-
ing module, and R(n, h, w) represents a ResNet block that
includes two convolution layers C(n, h, w). Every convo-
lution layer is followed by Instance Normalization, Batch
Normalization, and ReLU activation, except for the last one.
NM-Net is trained by Adam [8] with a learning rate being
10−3 and batch size being 16. For LGC-Net, we use the
code released by the authors to train the model. For Point-
Net and PointNet++, we adopt the ResNet backbone follow-
ing the accomplishment in [36]. To verify the effectiveness
of compatibility-speciﬁc neighbor mining method, another
version of NM-Net is also implemented (named NM-Net-
sp), where compatibility-speciﬁc knn search is replaced by
spatially knn search. Parameters including the number of
neighbors k and λ (Eq. 4) are set to 8 and 10−3, respectively.
Benchmark datasets. Four datasets are employed in our
experiments - i.e., NARROW, WIDE, COLMAP [30], and
MULTI [37] (Table 1). The ﬁrst two datasets are col-
lected by us using a drone in four scenes, and we respec-
tively keep a sample interval of 10 and 20 frames to attain
narrow and wide baseline matching data. For NARROW,

219

Input correspondences1×4 conv, 321×N×432×N×1Grouping32×N×kResNet blockShared(...)256×N×11×1 conv, 11×N×1instance-normbatch-normconvxreluCompatibleResNet blockSharedFeature aggregation(...)Neighbors 32 × k256 × 1Dataset

# Image pairs

# Training

# Validation

# Testing

Inlier ratio (%)

NARROW

WIDE

COLMAP [30]

MULTI [37]

24070
11426
18850

45

16849
7998
13195

-

3610
1713
2827

-

3610
1713
2827

-

40.827
32.771
7.496
40.828

Challenges

VP changes
VP changes

VP changes, rotation
Dynamic scenarios

Table 1. Properties of the experimental datasets. VP means viewpoint and the inlier ratio indicates the average proportion of inliers in
initial correspondence sets computed over a whole dataset.

Method

Precision (%)

Recall (%)

F-measure (%) MSE MAE Median Max Min (10−2)

RANSAC [9]

GTM [1]
LPM [23]

PointNet [25]

PointNet++ [26]

LGC-Net [36]

NM-Net-sp

NM-Net

86.923
88.707
72.667
79.003
83.677
95.238
96.946
97.169

60.397
52.949
68.504
86.163
85.045
98.405
97.659
97.870

69.194
65.653
70.173
82.102
84.112
96.611
97.283
97.501

2.017
2.042
2.087
2.293
2.248
2.096
2.482
2.436

2.622
2.728
2.879
2.787
2.773
2.255
2.664
2.608

2.809
2.886
3.107
3.503
3.328
3.021
3.687
3.630

4.978
4.968
4.869
4.728
5.128
5.006
5.038
5.021

0.755
2.467
25.453
1.180
3.899
0.558
0.245
0.390

Method

Precision (%)

Recall (%)

F-measure (%) MSE MAE Median Max Min (10−2)

RANSAC [9]

GTM [1]
LPM [23]

PointNet [25]

PointNet++ [26]

LGC-Net [36]

NM-Net-sp

NM-Net

80.740
79.989
62.940
64.730
73.926
88.139
91.742
92.332

51.198
47.711
64.487
77.287
81.856
97.138
94.039
94.251

60.350
58.881
62.828
70.068
77.245
91.264
92.749
93.145

2.052
2.040
2.038
2.282
2.180
2.059
2.513
2.488

2.711
2.784
2.921
2.863
2.771
2.230
2.731
2.718

3.125
3.068
3.080
3.458
3.255
2.995
3.751
3.781

5.040
5.041
4.926
4.905
5.013
5.061
5.110
5.113

1.347
1.046
19.782
5.528
3.020
1.226
0.650
0.553

Method

Precision (%)

Recall (%)

F-measure (%) MSE MAE Median Max Min (10−2)

RANSAC [9]

GTM [1]
LPM [23]

PointNet [25]

PointNet++ [26]

LGC-Net [36]

NM-Net-sp

NM-Net

25.156
22.931
15.879
13.596
18.659
26.383
29.296
31.003

14.477
19.913
34.293
41.765
41.953
71.132
59.710
58.499

17.464
19.075
19.595
19.710
24.301
33.949
37.503
38.887

1.984
2.004
2.019
2.051
2.060
1.981
1.983
1.953

2.985
3.073
3.037
2.864
2.902
2.554
2.446
2.402

3.169
3.245
3.113
3.193
3.200
3.071
3.047
3.027

5.044
5.102
4.693
4.878
4.877
4.717
5.125
4.989

1.383
4.804
17.298
14.138
4.150
1.265
2.250
1.514

Table 2. Evaluation results. The three tables from top to bottom are results on NARROW, WIDE, and COLMAP datasets, respectively.
Precision, recall, and F-measure are colored for highlighting because they explicitly measure the performance of correspondence selection.
NM-Net-sp indicates a variant of our NM-Net with spatial neighbors. The best result in each column is rendered in bold.

WIDE, and COLMAP, the ground-truth camera parameters
are obtained by VisualSFM [35] and the ground-truth labels
of correspondences are calculated by comparing the corre-
sponding epipolar distances [12] with a threshold (10−4).
MULTI is a tiny dataset consisting of 45 image pairs with
available ground-truth labels for each correspondence. We
use this dataset to test the generalization in cases of multi-
consistency matching.

Evaluation criteria. To measure the correspondence se-
lection performance, we employ precision (P), recall (R),

and F-measure (F) as in [19, 3, 23]. Moreover, consider-
ing that accurate estimation of the global transformation is
required in some image alignment and 3D reconstruction
tasks [32, 2, 4], the deviation between the essential matrix
E estimated by selected correspondences and ground-truth
Egt is measured by MSE, MAE, median, max, and min as
in [28]. Since P, R, and F explicitly reﬂect the performance
of correspondence selection, we will focus on analyzing re-
sults from the perspective of these metrics.

220

(a) LGC-Net

(b) NM-Net

Figure 5. Visual results on COLMAP dataset. Green and red
lines represent inliers and outliers in the selected correspondence
set by LGC-Net and NM-Net, respectively.

5.2. Single Consistency

Finding a single consistency corresponds to a global
transformation (e.g., essential matrix) in static scenes
is a popular application [32, 2] for feature correspon-
dences. Our experimental results on NARROW, WIDE, and
COLMAP datasets which contain a single consistency in
each image pair are presented in Table 2.

As reported in Table 2 (a), (b), and (c), NM-Net sig-
niﬁcantly outperforms hand-crafted algorithms and other
learning-based approaches in terms of F-measure. First,
when compared with hand-crafted algorithms such as
RANSAC, GTM (using the same binary item in Eq. 4),
and LPM (employing spatially k-nearest information), NM-
Net outperforms them by about 20 percentages on all three
datasets. Second, NM-Net remarkably surpasses a rep-
resentative set of learning-based approaches. For Point-
Net and LGC-Net, global features are extracted by aver-
age pooling and Context Normalization, respectively. For
PointNet++, local information is added by spatially knn
search for each correspondence. NM-Net also extracts both
global features and local features but mines neighbors rely-
ing on the proposed compatibility metric of Eq. (4). The
superiority of our framework can be easily veriﬁed. Third,
NM-Net performs better than NM-Net-sp on all datasets;
the gap becomes more dramatic on COLMAP which is a
more challenging dataset with an extremely low initial inlier
ratio (7.496%).
It implies that our compatibility-speciﬁc
knn search is more robust to high outlier ratios than stan-
dard spatially knn search. Some representative visual com-
parison results are presented in Fig. 5, in which NM-Net
is compared against current state-of-the-art deep learning
framework LGC-Net. More visual results can be found in
the supplementary material.

5.3. Multiple Consistencies

Multi-consistency feature matching in the situation of
dynamic scenarios remains an open research problem as
mentioned in [37]. In contrast to a global transformation
for static scenes, several local transformations correspond-
ing to multiple consistencies are included into the initial

Method

P (%)

R (%)

F (%)

PointNet [25]

PointNet++ [26]

LGC-Net [36]

NM-Net

48.223
64.661
61.736
51.898

5.829
7.871
36.849
33.653

8.717
13.327
41.849
35.605

Table 3. Generalization on MULTI dataset. Tested models are
pretrained on NARROW dataset. Metrics are precision (P), recall
(R), and F-measure (F).

(a) LGC-Net

(b) NM-Net

Figure 6. Visual results on MULTI dataset. Different colors rep-
resent different feature consistencies.

correspondence set. Because MULTI only contains 45 im-
age pairs, models pretrained on NARROW that includes a
similar inlier ratio (Table 1), are adopted to test the general-
ization from single consistency to multiple consistencies.

Table 3 and Fig. 6 show quantitative and qualitative re-
sults on MULTI dataset, respectively. Although LGC-Net
achieves higher F-measure than NM-Net (still the second
best), NM-Net is able to pick out all kinds of consisten-
cies; while LGC-Net ﬁnds out only one kind of consistency.
Note that LGC-Net is trained under the supervision of a hy-
brid loss that includes a regression loss corresponding to a
global transformation, which explains why LGC-Net is less
effective for multi-consistency matching with several local
transformations. By contrast, NM-Net is trained with a clas-
siﬁcation loss that is insensitive to multiple consistencies.

5.4. Method Analysis

Parameter k. The number of neighbors k for each cor-
respondence is a core parameter in NM-Net, which deter-
mines the receptive ﬁeld of local features in each graph
G. Consequently, several versions of NM-Net with differ-
ent numbers of neighbors are studied on NARROW, WIDE,
and COLMAP. As shown in Fig. 7, NM-Net experiences
performance degradation when k is either too small (4) or
too large (32). When k = 4, the search space for each
correspondence is too small to extract enough available lo-
cal features; when k = 32, the consistency of neighbors in
Ginlier for an inlier cinlier will decline and some outliers
will be included undesirably as nuisance. Based on above
analysis, we have used k = 8 as the default size of neigh-
bors in NM-Net.
Learning effectiveness validation. Since the neighbors

221

(a) NARROW

(b) WIDE

(c) COLMAP

Figure 7. Analysis of parameter k. We train our NM-Net with different values of neighborhood size k ({4, 8, 16, 32}) while keep other
settings identical, and examine the performance variation on NARROW, WIDE, and COLMAP datasets.

Figure 8. Learning effectiveness validation. The hand-crafted
method judges the correctness of a correspondence by comparing
the sum of compatibility scores of the neighbors with a threshold.
This threshold is varied from 5 to 7.9 to alleviate the effect of
threshold setting.

i

i

searched by the compatibility metric are more consistent for
inliers than for outliers, compatibility scores of correspon-
dences in Ginlier should be higher than those in Goutlier
in theory. A potential issue arises: can correspondences
be directly classiﬁed based on the scores? To address this
question, a hand-crafted approach is designed to calculate
the sum of scores in each graph Gi as ssum
. Then ci will
be determined as an inlier if ssum
is higher than a prede-
ﬁned threshold. A comparison between the hand-crafted
approach and NM-Net on COLMAP dataset is included in
Fig. 8. Clearly, NM-Net achieves a far higher F-measure
than the hand-crafted one with all thresholds. Due to un-
certainties such as viewpoint changes and camera rotation,
the distribution of correspondences is distinctly different
among various image pairs. Utilizing raw scores to distin-
guish inliers and outliers is therefore unreliable. In contrast
to the hand-crafted approach, the scores are leveraged as in-
dexes to search for compatible neighbors in our proposed
method. The local features hidden in these neighbors can
be fully explored by a powerful deep learning network.
Compatibility metric analysis.
Since compatibility-
speciﬁc knn search is a key procedure in NM-Net to gener-
ate the initial graph G, the consistency of elements in G di-
rectly affects the effectiveness of raw information. To shed
more light to this important matter, an analysis of the com-
patibility metric (Eq. 4) is conducted as follows.

The inlier ratios of the neighbors searched by Eq. 4 on
NARROW, WIDE, and COLMAP datasets are shown in
Fig. 9. The compatibility metric is deemed to be reason-

(a) Neighbors of inliers

(b) Neighbors of outliers

Figure 9. Compatibility metric analysis. The inlier ratios of
neighbors recognized via our compatibility metric (Eq. 4) of (a)
inliers and (b) outliers are calculated on NARROW, WIDE, and
COLMAP datasets to examine if this metric can provide distin-
guishable local information.

able for the following reasons. First, the neighbors of in-
liers are signiﬁcantly more consistent than the ones of out-
liers, with the inlier ratios being 3 times higher. Second,
our approach achieves approximate 100% inlier ratios on
both NARROW and WIDE datasets as shown in Fig. 9 (a),
where almost all searched neighbors of inliers are consistent
(i.e., inliers). However, the inlier ratios of the neighbors of
correct matches drop considerably on the more challenging
COLMAP dataset. This result suggests that there is still a
large room for further improvement from the perspective of
robustness of the compatibility metric. We leave this to our
future study.

6. Conclusion

We have presented a hierarchical classiﬁcation network
named NM-Net to select correct matches from initial cor-
respondences, which fully mines compatibility-speciﬁc lo-
cality for each correspondence. Experiments demonstrate
that NM-Net behaves favorably to the state-of-the-art (both
hand-crafted and learning-based) approaches. The current
shortcoming of our approach is the demand for key-point
detectors with local afﬁne information to compute the com-
patibility score. We expect developing a more advanced
compatibility metric without such constraint in our future
works.
Acknowledgments. This work was supported in part by
the National Natural Science Foundation of China under
Grant 61876211 and by the 111 Project on Computational
Intelligence and Intelligent Control under Grant B18024.

222

References

[1] Andrea Albarelli, Emanuele Rodol`a, and Andrea Torsello.
Robust game-theoretic inlier selection for bundle adjustment.
In International Symposium on 3D Data Processing, Visual-
ization and Transmission, 2010.

[2] Selim Benhimane and Ezio Malis. Real-time image-based
tracking of planes using efﬁcient second-order minimiza-
tion. In Proceedings of IEEE/RSJ International Conference
on Intelligent Robots and Systems, volume 1, pages 943–948,
2004.

[3] JiaWang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit
Yeung, Tan Dat Nguyen, and Ming-Ming Cheng. Gms:
Grid-based motion statistics for fast, ultra-robust feature cor-
respondence.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2017.

[4] Matthew Brown and David G Lowe. Automatic panoramic
image stitching using invariant features. International Jour-
nal of Computer Vision, 74(1):59–73, 2007.

[5] H. Y. Chen, Y. Y. Lin, and B. Y. Chen. Co-segmentation
guided hough transform for robust feature matching. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
37(12):2388, 2015.

[6] Ondrej Chum and Jiri Matas. Matching with prosac-
progressive sample consensus. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 220–226, 2005.

[7] Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally opti-

mized ransac. Pattern Recognition, pages 236–243, 2003.

[8] D.P.Kingma and J.Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014, 2014.

[9] Martin A Fischler and Robert C Bolles. Random sample
consensus: A paradigm for model ﬁtting with applications to
image analysis and automated cartography. Communications
of the ACM, 24(6):381–395, 1981.

[10] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 1440–1448,
2015.

[11] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
580–587, 2014.

[12] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision. Cambridge university press,
2003.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[14] Heiko Hirschmuller. Stereo processing by semiglobal match-
ing and mutual information. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 30(2):328–341, 2008.

[15] Mikolajczyk K and Schmid C. Scale & afﬁne invariant in-
terest point detectors. International Journal of Computer Vi-
sion, 60(1):63–86, 2004.

[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In International Conference on Neural Information
Processing Systems, pages 1097–1105, 2012.

[17] Marius Leordeanu and Martial Hebert. A spectral technique
for correspondence problems using pairwise constraints. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1482–1489, 2005.

[18] Xiangru Li and Zhanyi Hu. Rejecting mismatches by cor-
International Journal of Computer

respondence function.
Vision, 89(1):1–17, 2010.

[19] Wen-Yan Daniel Lin, Ming-Ming Cheng, Jiangbo Lu, Hong-
sheng Yang, Minh N Do, and Philip Torr. Bilateral functions
for global motion modeling. In Proceedings of the European
Conference on Computer Vision, pages 341–356. Springer,
2014.

[20] Hairong Liu and Shuicheng Yan. Common visual pattern
discovery via spatially coherent correspondences.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1609–1616, 2010.

[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3431–3440, 2015.

[22] David G Lowe. Distinctive image features from scale-
invariant keypoints. International Journal of Computer Vi-
sion, 60(2):91–110, 2004.

[23] Jiayi Ma, Ji Zhao, Hanqi Guo, Junjun Jiang, Huabing Zhou,
and Yuan Gao. Locality preserving matching. In Proceed-
ings of the International Joint Conference on Artiﬁcial Intel-
ligence, pages 4492–4498, 2017.

[24] Laurens van der Maaten and Geoffrey Hinton. Visualiz-
ing data using t-sne. Journal of machine learning research,
9(Nov):2579–2605, 2008.

[25] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, volume 1, page 4,
2017.

[26] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In International Conference on
Neural Information Processing Systems, pages 5099–5108,
2017.

[27] R Raguram, O Chum, M Pollefeys, J Matas, and J. M.
Frahm. Usac: A universal framework for random sample
consensus. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 35(8):2022–2038, 2013.

[28] Ren´e Ranftl and Vladlen Koltun. Deep fundamental matrix
estimation. In Proceedings of the European Conference on
Computer Vision, pages 284–299, 2018.

[29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In International Conference on Neural
Information Processing Systems, pages 91–99, 2015.

[30] Johannes Lutz Sch¨onberger and Jan-Michael Frahm.
Structure-from-motion revisited. In Proceedings of the IEEE

223

Conference on Computer Vision and Pattern Recognition,
2016.

[31] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[32] Noah Snavely, Steven M Seitz, and Richard Szeliski. Model-
ing the world from internet photo collections. International
Journal of Computer Vision, 80(2):189–210, 2008.

[33] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Im-
proved texture networks: Maximizing quality and diversity
in feed-forward stylization and texture synthesis.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4105–4113, 2017.

[34] J¨orgen W Weibull. Evolutionary game theory. MIT press,

1997.

[35] Changchang Wu. Towards linear-time incremental structure
In International Conference on 3D Vision,

from motion.
pages 127–134, 2013.

[36] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit,
Mathieu Salzmann, and Pascal Fua. Learning to ﬁnd good
correspondences. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018.

[37] Chen Zhao, Jiaqi Yang, Yang Xiao, and Zhiguo Cao.
Scalable multi-consistency feature matching with non-
cooperative games. In Proceedings of the IEEE International
Conference on Image Processing, pages 1258–1262. IEEE,
2018.

224

