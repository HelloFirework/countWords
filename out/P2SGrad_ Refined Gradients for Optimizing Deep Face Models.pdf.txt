P2SGrad: Reﬁned Gradients for Optimizing Deep Face Models

Xiao Zhang1 Rui Zhao2

Junjie Yan2 Mengya Gao2 Yu Qiao3 Xiaogang Wang1 Hongsheng Li1

1CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong

2SenseTime Research

3SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences

zhangx9411@gmail.com hsli@ee.cuhk.edu.hk

Abstract

Cosine-based softmax losses [20, 29, 27, 3] signiﬁ-
cantly improve the performance of deep face recognition
networks. However, these losses always include sensitive
hyper-parameters which can make training process unsta-
ble, and it is very tricky to set suitable hyper parameters
for a speciﬁc dataset. This paper addresses this challenge
by directly designing the gradients for training in an adap-
tive manner. We ﬁrst investigate and unify previous co-
sine softmax losses from the perspective of gradients. This
uniﬁed view inspires us to propose a novel gradient called
P2SGrad (Probability-to-Similarity Gradient), which lever-
ages a cosine similarity instead of classiﬁcation probabil-
ity to control the gradients for updating neural network pa-
rameters. P2SGrad is adaptive and hyper-parameter free,
which makes training process more efﬁcient and faster. We
evaluate our P2SGrad on three face recognition bench-
marks, LFW [7], MegaFace [8], and IJB-C [16]. The re-
sults show that P2SGrad is stable in training, robust to
noise, and achieves state-of-the-art performance on all the
three benchmarks.

1. Introduction

Over the last few years, deep convolutional neural net-
works have signiﬁcantly boosted the face recognition accu-
racy. State-of-the-art approaches are based on deep neu-
ral networks and adopt the following pipeline:
training a
classiﬁcation model with different types of softmax losses
and use the trained model as a feature extractor to test un-
seen samples. Then the cosine similarities between testing
faces’ features, are exploited to determine whether these
features belong to the same identity. Unlike other vision
tasks, such as object detection, where training and testing
have the same objectives and evaluation procedures, con-
ventional face recognition systems were trained with soft-
max losses but tested with cosine similarities.
In other
words, there is a gap between the softmax probability in
training and inner product similarity in testing.

This problem is not well addressed in the classical soft-
max cross-entropy loss function (softmax loss for short

in the remaining part), which mainly considers probabil-
ity distributions of training classes and ignores the test-
ing setup.
In order to bridge this gap, cosine softmax
losses [28, 13, 14] and their angular margin based vari-
ants [29, 27, 3] directly use cosine distances instead of in-
ner products as the input raw classiﬁcation scores, namely
logits. Specially, the angular margin based variants aim to
learn the decision boundaries with a margin between dif-
ferent classes. These methods improve the face recognition
performance in the challenging setup.

In spite of their successes, cosine-based softmax loss is
only a trade-off: the supervision signals for training are still
classiﬁcation probabilities, which are never evaluated dur-
ing testing. Considering the fact that the similarity between
two testing face images is only related to themselves while
the classiﬁcation probabilities are related to all the identi-
ties, cosine softmax losses are not the ideal training mea-
sures in face recognition.

This paper aims to address these problems from a differ-
ent perspective. Deep neural networks are generally trained
with Stochastic Gradient Descent (SGD) algorithms where
gradients play an essential role in this process. In addition
to the loss function, we focus on the gradients of cosine
softmax loss functions. This new perspective not only al-
lows us to analyze the relations and problems of previous
methods, but also inspires us to develop a novel form of
adaptive gradients, P2SGrad, which mitigates the problem
of training-testing mismatch and further improves the face
recognition performance in practice.

To be more speciﬁc, P2SGrad optimizes deep models by
well-designed gradients. Compared with the conventional
gradients in cosine-based softmax losses, P2SGrad uses co-
sine distances to replace the probabilities in the original gra-
dients. P2SGrad decouples gradients from hyperparameters
and the number of classes, and matches testing targets.

This paper mainly contributes in the following aspects:

1. We analyze the recent cosine softmax losses and their
angular-margin based variants from the perspective of
gradients, and propose a general formulation to unify
different cosine softmax cross-entropy losses;

2. With this uniﬁed model, we propose an adaptive

9906

Training

Updating

Training Facial Images

Deep Face Model

Feature

Class. Prob. One-hot Ground Truth

Softmax

		𝑃#

	𝑃%

	𝑃&
…
	𝑃$

With

0

1

0
…
0

Cross  Entropy

𝐿 $(

Feature Extractor

Feature Pairs

Cosine

Similarity

> 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑

Same 
Person

Different 
Person

< 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑

Testing Face Pairs

Testing

Figure 1. Pipeline of current face recognition system. In this general pipeline, deep face models trained on classiﬁcation tasks are treated
as feature extractors. Best viewed in color.

hyperparameter-free gradient method - P2SGrad for
training deep face recognition networks. This method
reserves advantages of using cosine distances in train-
ing but replaces classiﬁcation probabilities with cosine
similarities in the backward propagation;

3. We conduct extensive experiments on large-scale face
datasets. Experimental results show that P2SGrad out-
performs state-of-the-art methods on the same setup
and clearly improves the stability of the training pro-
cess.

2. Related Works

The accuracy improvements of face recognition [9, 6,
18, 25] enjoy the large-scale training data, and the im-
provements of neural network structures. Modern face
datasets contain a huge number of identities, such as
LFW [7], PubFig [10], CASIA-WebFace [32], MS1M [4]
and MegaFace [17, 8], which enable the effective training
of very deep neural networks. A number of recent studies
demonstrated that well-designed network architectures lead
to better performance, such as DeepFace [26], DeepID2,
3 [22, 23] and FaceNet [21].

In face recognition, feature representation normaliza-
tion, which restricts features to lie on a ﬁxed-radius hyper-
sphere, is a common operation to enhance models’ ﬁnal per-
formance. COCO loss [13, 14] and NormFace [28] stud-
ied the effect of normalization through mathematical analy-
sis and proposed two strategies through reformulating soft-
max loss and metric learning. Coincidentally, L2-softmax
[20] also proposed a similar method. These methods obtain
the same formulation of cosine softmax loss from different
views.

Optimizing auxiliary metric loss function is also a pop-
ular choice for boosting performance. In the early years,
most face recognition approaches utilized metric loss func-
tions, such as triplet loss [30] and contrastive loss [2], which
use Euclidean margin to measure distance between features.
Taking advantages of these works, center loss [31] and

range loss [33] were proposed to reduce intra-class varia-
tions through minimizing distance within target classes [1].
Simply using Euclidean distance or Euclidean margin
is insufﬁcient to maximize the classiﬁcation performance.
To circumvent this difﬁculty, angular margin based softmax
loss functions were proposed and became popular in face
recognition. Angular constraints were added to traditional
softmax loss function to improve feature discriminativeness
in L-softmax [12] and A-softmax [11], where A-softmax
applied weight normalization but L-softmax [12] did not.
CosFace [29], AM-softmax [27] and ArcFace [3] also em-
braced the idea of angular margins and employed simpler as
well as more intuitive loss functions compared with afore-
mentioned methods. Normalization is applied to both fea-
tures and weights in these methods.

3. Limitations of cosine softmax losses

In this section we will discuss limitations caused by the
mismatch between training and testing of face recognition
models. We ﬁrst provide a brief review of the workﬂow of
cosine softmax losses. Then we will reveal the limitations
of existing loss functions in face recognition from the per-
spective of forward and backward calculation respectively.

3.1. Gradients of cosine softmax losses

In face recognition tasks,

the cosine softmax cross-
entropy loss has an elegant two-part formulation, softmax
function and cross-entropy loss.

We discuss softmax function at ﬁrst. Assuming that the
vector ~xi denotes the feature representation of a face image,
the input of the softmax function is the logit fi,j , i.e.,

fi,j = s ·

h~xi, ~Wji

k~xik2k ~Wjk2

= s · hˆxi, ˆWji = s · cos θi,j ,

(1)

where s is a hyperparameter and fi,j is the classiﬁcation
score (logit) that ~xi is assigned to class j, and Wj is the
weight vector of class j. ˆxi and ˆWj are normalized vec-
tors of xi and Wj respectively. θi,j is the angle between

9907

feature xi and class weight Wj . The logits fi,j are then
input into the softmax function to obtain the probability
efi,j
Pi,j = Softmax(fi,j) =
k=1 efi,k , where C is the number
of classes and the output Pi,j can be interpreted as the prob-
ability of ~xi being assigned to a certain class j. If j = yi,
then Pi,yi is the class probability of ~xi being assigned to its
corresponding class yi.

PC

Then we discuss the cross-entropy loss associated with
the softmax function, which measures the divergence be-
tween the predicted probability Pi,yi and ground truth dis-
tributions as

𝑊$%

𝜕 cos 𝜃0,$%

𝜕𝑊$%

𝒙𝒊

LCE(~xi) = − log Pi,yi = − log

,

(2)

𝜕 cos 𝜃0 ,&	

𝜕𝑊&

efi,yi
k=1 efi,k

PC

where LCE(~xi) is the loss of input feature ~xi. The larger
probability Pi,yi is, the smaller loss LCE(~xi) is.

In order to decrease the loss LCE(~xi), the model needs
to enlarge Pi,yi and thus enlarge fi,yi . Then θi,yi becomes
smaller.
In summary, cosine softmax loss function maps
θi,yi to the probability Pi,yi and calculates the cross-entropy
loss to supervise the training.

In the backward propagation process, classiﬁcation prob-
abilities Pi,j play key roles for optimization. The gradient
of ~xi and ~Wj in cosine softmax losses are calculated as

(Pi,j − ✶(yi = j)∇f (cos θi,j) ·

∂ cos θi,j

∂~xi

,

=

C

Xj=1

∂LCE(~xi)

∂~xi

∂LCE(~xi)

∂ ~Wj

= (Pi,j − ✶(yi = j)∇f (cos θi,j) ·

∂ cos θi,j

∂ ~Wj

,

(3)
where the indicator function ✶(j = yi) returns 1 when j =
yi and 0 otherwise. ∂ cos θi,j
can be computed
respectively as:

and ∂ cos θi,j

∂ ~Wj

∂~xi

∂ cos θi,j

∂~xi

∂ cos θi,j

∂ ~Wj

=

=

1

k~xik2

1

k ~Wjk2

( ˆWj − cos θi,j · ˆxi),

(ˆxi − cos θi,j · ˆWj),

(4)

∂ ~Wj

where ˆWj and ˆxi are unit vectors of ~Wj and ~xi, respec-
tively. ∂ cos θi,j
is visualized as the red arrow in Fig. 2. This
gradient vector is updating directions of class weights ~Wj .
Intuitively, we expect the updating of ~Wj makes ~Wyi close
to ~xi, and makes ~Wj for j 6= yi away from ~xi. Gradient
∂ cos θi,j
is vertical to ~Wj and points toward ~xi. Thus it is

∂ ~Wj

the fastest and optiaml direction for updating ~Wj .
Then we consider the gradient ∇f (cos θi,j).

In con-
ventional cosine softmax losses [20, 28, 13], classiﬁcation
score f (cos θi,j) = s · cos θi,j and thus ∇f (cos θi,j) = s.
In angular margin-based cosine softmax losses [27, 29, 3],

𝑊&, 𝒋 ≠ 𝒚𝒊

∂ cos θi,j

∂ ~Wj

Figure 2. Gradient direction of
. Note this gradient is the
updating direction of ~Wj . The red pointed line shows that the
gradient of ~Wj is vertical to ~Wj itself and in the plane spanned by
~xi and ~Wj . This can be seen as the fastest direction for updating
~Wyi to be close to ~xi and for updating ~Wj , j 6= yi to be far away

from ~xi. Best viewed in color.

however, the gradient of fmargin(cos θi,yi ) for j = yi de-
pends on where the margin parameter m is. For exam-
ple, in CosFace [29] f (cos θi,yi ) = s · (cos θi,yi − m),
thus ∇f (cos θi,yi ) = s and in ArcFace [3] f (cos θi,yi ) =
s · cos (θi,yi + m), thus ∇f (cos θi,yi ) = s · sin (θi,yi +m)
. In
general, gradient ∇f (cos θi,j) is always a scalar related to
parameters s, m and cos θi,j .

sin θi,yi

Based on the aforementioned discussions, we reconsider
, the ﬁrst

gradients of class weights ~Wj in Eq. (3). In ∂LCE
∂ ~Wj
part (Pi,j − ✶(yi = j) · ∇f (cos θi,j) is a scalar, which de-
cides the length of gradient, while the second part ∂ cos θi,j
is a vector which decides the direction of gradient. Since
the directions of gradients for various cosine softmax losses
remain the same, the essential difference of these cosine
softmax losses is the different lengths of gradients, which
signiﬁcantly affect the optimization of model. In the follow-
ing sections, we will discuss the suboptimal gradient length
caused by forward and backward process respectively.

∂ ~Wj

3.2. Limitations in probability calculation

In this section we discuss the limitations of the forward
calculation of cosine softmax losses in deep face networks
and focus on the classiﬁcation probability Pi,j obtained in
the forward calculation.

We ﬁrst revisit the relation between Pi,j and θi,j . The
classiﬁcation probability Pi,j in Eq. (3) is a part of gradi-
ent length. Hence Pi,j signiﬁcantly affects the length of
gradient. Probability Pi,j and logit fi,j are positively corre-
lated. For all cosine softmax losses, logits fi,j measure θi,j
between feature ~xi and class weight ~Wj . A larger θi,j pro-

9908

π
2

7π
16

j
,
i
θ

3π
8

5π
16

π
4

Average θi,yi
Average θi,j, j 6= yi

Iteration

Figure 3. The change of average θi,j of each mini-batch when
training on WebFace dataset. (Red) average angles in each mini-
batch for non-corresponding classes, θi,j for j 6= yi.
(Brown)
average angles in each mini-batch for corresponding classes, θi,yi .

duces lower classiﬁcation probability Pi,j while a smaller
θi,j produces higher Pi,j . It means that θi,j affects gradient
length by its corresponding probability Pi,j . The equation
sets up a mapping relation between θi,j and Pi,j and makes
θi,j affects optimization. Above analysis is also the reason
why cosine softmax losses are effective on face recognition
tasks.

Since θi,yi is the direct measurement of the generaliza-
tion but it can only indirectly affect gradient by correspond-
ing Pi,yi , setting a reasonable mapping relation between
θi,yi and Pi,yi is crucial. However, there are two tricky
problems in current cosine softmax losses: (1) classiﬁcation
probability Pi,yi is sensitive to hyperparameter settings; (2)
the calculation of Pi,yi is dependent on class number, which
is not related to face recognition tasks. We will discuss these
problems below.

Pi,yi

is sensitive to hyperparameters.

The most
common hyperparameters in conventional cosine softmax
losses [20, 28, 13] and margin variants [3] are the scale pa-
rameter s and the angular margin parameter m. We will
analyze the sensitivity of probability Pi,yi to hyperparam-
eter s and m. For a more accurate analysis, we ﬁrst look
at the actual range of θi,j . Fig. 3 exhibits how the average
θi,j changes in training. Mathematically, θi,j could be any
value in [0, π]. In practice, however, the maximum θi,j is
around π
2 . The blue curve reveals that θi,j for j 6= yi do
not change signiﬁcantly during training. The brown curve
reveals that θi,yi is gradually reduced. Therefore we can
reasonably assume that θi,j ≈ π
2 for j 6= yi and the range
of θi,yi is [0, π

2 ]. Then Pi,yi can be rewritten as

Pi,yi =

≈

=

efi,yi
k=1 efi,k

efi,yi

PC
efi,yi +Pk6=yi

efi,yi

efi,yi +Pk6=yi

=

es·cos π/2

es·cos θi,k

efi,yi

efi,yi + (C − 1)

,

(5)
where fi,yi is logit that ~xi is assigned to its corresponding
class yi, and C is class number.

Theoretically, we can give the correspondence between
probability Pi,yi and angle θi,yi under different hyperpa-
rameter settings. In state-of-the-art angular margin based

i
y
,
i

P
y
t
i
l
i
b
a
b
o
r
P

1.0

0.8

0.6

0.4

0.2

0.0

0

s = 64, m = 0.2

s = 64, m = 0

s = 30, m = 0.5

s = 8, m = 0

s = 8, m = 0.5
cos θi,yi

π
16

π
8

3π
16

π
4

5π
16

3π
8

7π
16

π
2

Figure 4. Probability Pi,yi curves w.r.t. the angle θi,yi with differ-
ent hyperparameter settings.

θi,yi ∈ (0, π
2 )

j
,
i
θ

π
2

7π
16

3π
8

5π
16

π
4

3π
16

π
8

π
16

i
y
,
i

P
y
t
i
l
i
b
a
b
o
r
P

1.0

0.8

0.6

0.4

0.2

0.0

Average θi,yi
Average Pi,yi

Figure 5. The change of probability Pi,yi and angle θi,yi as the
iteration number increases with the hyperparameter setting s = 35
and m = 0.2. Best viewed in color.

Num. of Iteration

losses [3], logit fi,yi = s · cos (θi,yi + m). Fig. 4 reveals
that different settings of s and m can signiﬁcantly affect the
relation between θi,yi and Pi,yi . Apparently, both the green
curve and the purple curve are examples of unreasonable re-
lations. The former is so lenient that even a very larger θi,yi
can produce a high Pi,yi ≈ 1. The later is so strict that even
a very small θi,yi can just produce a low Pi,yi . In short, for
a speciﬁc degree of θi,yi , the difference of probability Pi,yi
under different settings is very large. This observation indi-
cates that probability Pi,yi is sensitive to parameters s and
m.

To further conﬁrm this conclusion, we take an example
of correspondences between Pi,yi and θi,yi in real training.
In Fig. 5, the red curve represents the change of Pi,yi and the
blue curve represents the change of θi,yi during the training
process. As we discussed above, Pi,yi ≈ 1 can produce
very short gradients so that has little affection in updating.
This setting is not ideal because Pi,yi increases to 1 rapidly
but θi,yi is still large. Therefore classiﬁcation probability
Pi,yi largely depends on the setting of hyperparameter.

Pi,yi contains class number. In closed-set classiﬁcation
problems, probabilities Pi,j become smaller as the growth
of class number C because each class is assigned more or
less probability (but not 0). This is reasonable in classiﬁca-
tion tasks. However, this is not suitable for face recognition,
which is an open-set problem. Since θi,yi is the direct mea-
surement of generalization of ~xi while Pi,yi is the indirect
measurement, we expect that they have a consistent seman-
tic meaning. But Pi,yi is related to class nubmer C while

9909

i
y
,
i

P
y
t
i
l
i
b
a
b
o
r
P

1.0

0.8

0.6

0.4

0.2

0.0

0

Class Number C = 10

Class Number C = 100

Class Number C = 1, 000

Class Number C = 10, 000

Class Number C = 100, 000
cos θi,yi

π
16

π
8

3π
16

π
4

5π
16

3π
8

7π
16

π
2

θi,yi ∈ (0, π
2 )

Figure 6. Pi,yi with different class numbers. The hyperparameter
setting is ﬁxed to s = 15 and m = 0.5 for fair comparison. Best
viewed in color.

θi,yi is not, which causes the mismatch between them.

As shown in Fig. 6, we can summarize that the class

number C is an important factor for Pi,yi .

From the above discussion, we reveal that limitations
exist in the forward calculation of cosine softmax losses.
Both hyperparameters and the class number, which are un-
related to face recognition tasks, can determine the proba-
bility Pi,yi , and thus affect the gradient length in Eq. (3).

3.3. Limitation in backward calculation of cosine

softmax losses

In this section, we discuss the limitations in the back-
ward calculation of the cosine softmax function, especially
the angular-margin based softmax losses [3].

We revisit gradient ∇f (cos θi,j) in Eq. (3). Besides
Pi,yi , the part of ∇f (cos θi,j) also affects the length of
gradient. Larger ∇f (cos θi,j) produce longer gradients
while smaller ones produce shorter gradients. So we expect
θi,yi and values of ∇f (cos θi,j) to be positively correlated:
small θi,yi for small ∇f (cos θi,j) and large θi,yi for larger
∇f (cos θi,j).

The logit fi,yi

is different in various cosine softmax
losses, and thus the speciﬁc form of ∇f (cos θi,j) is dif-
ferent. Generally, we focus on simple cosine softmax
losses [20, 28, 13] and state-of-the-art angular margin based
loss [3]. Their ∇f (cos θi,j) are visualized in Fig. 7, which
shows that, under the factor of ∇f (cos θi,j), the lengths of
gradients in conventional softmax cosine losses [20, 28, 13]
are constant. However in angular margin-based losses [3],
the lengths of gradients and θi,yi are negatively correlated,
which is completely contrary to our expectations. More-
over, the correspondence between length of gradients in an-
gular margin-based loss [3] and θi,yi becomes tricky: when
θi,yi gradually reduced, Pi,yi tends to shorten length of gra-
dients but ∇f (cos θi,j) tends to elongate the length. There-
fore the geometric meaning of the gradient length becomes
unexplained in angular margin-based cosine softmax loss.

3.4. Summary

In the above discussion, we ﬁrst reveal that various
cosine softmax losses have the same updating direction.
Hence the main difference between the variants is their gra-
dient lengths. For the length of gradient, there are two
the probability Pi,yi in
scalars that determine its value:
the forward process and the gradient ∇f (cos θi,j). For
Pi,yi , we ﬁnd that it can easily lose its semantic mean-
ing with different hyperparameter settings and class num-
bers. For ∇f (cos θi,j), its value depends on the deﬁnition
of f (cos θi,yi ).

In summary, from the perspective of gradient, the widely
used cosine softmax losses [20, 28, 13] and their angular
margin variants [3] cannot produce optimal gradient lengths
with well-explained geometric meanings.

4. P2SGrad: Change Probability to Similarity

in Gradient

In this section, we propose a new method, namely
P2SGrad, that determines the gradient length only by θi,j
in training face recognition models. Formally, the gradient
length produced by P2SGrad is hyperparameter-free and not
related to the number of class C nor to a ad-hoc deﬁnition
of logit fi,yi . P2SGrad does not need a speciﬁed formula-
tion of loss function because gradients is well-designed to
optimize deep models.

Since the main difference of state-of-the-art cosine soft-
max losses is the gradient length, reforming a reasonable
gradient length is an intuitive thought. In order to decouple
the length factor and direction factor of the gradients, we
rewrite Eq. (3) as

C

∇LCE(~xi) =

L(Pi,j, f (cos θi,j)) · D( ~Wj, ~xi),

(6)

Xj=1

∇LCE( ~Wj) = L(Pi,j, f (cos θi,j)) · D(~xi, ~Wj),

where the direction factors D( ~Wj, ~xi) and D(~xi, ~Wj) are
deﬁned as

D( ~Wj, ~xi) =

D(~xi, ~Wj) =

1

k~xik2

1

k ~Wjk2

( ˆWj − cos θi,j · ˆxi),

(ˆxi − cos θi,j · ˆWj),

(7)

where ˆWj and ˆxi are unit vectors of ~Wj and ~xi, respectively.
cos θi,j is the cosine distances between feature ~xi and class
weights ~Wj . The direction factors will not be changed be-
cause they are the fastest changing directions, which are
speciﬁed before. The length factor |L(Pi,j, f (cos θi,j))| is
deﬁned as

|L(Pi,j, f (cos θi,j))| =((1 − Pi,yi )|∇f (cos θi,yi )|

Pi,j · |∇f (cos θi,j)|

j = yi,
j 6= yi.

(8)

9910

𝑊"#

𝑥⃗&

𝜃&,"#

𝑊"#

𝑥⃗&

𝜃&,"#

Simple Cosine Softmax Loss

Angular Margin-based Loss

Figure 7. How ∇f (cos θi,j) affects the length of gradients. (Left) the correspondence between θi,yi and ∇f (cos θi,j). The red curve
means ∇f (cos θi,j) is constant in conventional cosine softmax losses [20, 28, 13] while the blue curve means small a θi,yi can produce a
very large ∇f (cos θi,j). (Right) each point refers to a feature ~xi and the vertical vector is weight ~Wyi . The θi,yi is angle between each ~xi
and ~Wyi . The color from light to dark corresponds to the value of ∇f (cos θi,j) from small to large. Hence for the factor of ∇f (cos θi,j),

the dark points produce longer gradients than the light points. Best viewed in color.

The length factor |L(Pi,j, f (cos θi,j))| depends on the prob-
ability Pi,j and ∇f (cos θi,j) and is what we aim to reform.
Since we expect that the new length is hyperparameter-
free, the cosine logit f (cos θi,j) will not have hyperparam-
eters like s or m. Thus a constant ∇f (cos θi,j) should be
an ideal choice.

For the probability Pi,j , because it is hard to set a rea-
sonable mapping function between θi,j and Pi,j , we can di-
rectly use cos θi,j as a good alternative of Pi,j in the gradi-
ent length term. Firstly, they have the same theoretical range
of [0, 1] where θi,j ∈ [0, π
2 ]. Secondly, unlike Pi,j which is
adversely inﬂuenced by hyperparameter and the number of
class, cos θi,j does not contain any of these. It means that
we do not need to select speciﬁed parameters settings for
ideal correspondence between θi,yi and Pi,yi . Moreover,
compared with Pi,j , cos θi,j is a more natural supervision
because cosine similarities are used in the testing phase of
open-set face recognition systems while probabilities only
apply for close-set classiﬁcation tasks. Therefore, our re-
formed gradient length factor ˜L(cos θi,j) can be deﬁned as:

˜L(cos θi,j) = cos θi,j − ✶(j = yi),

(9)

where ˜L(cos θi,j) is a function of cos θi,j . The reformed
gradients ˜GP2SGrad could then be deﬁned as

C

˜GP2SGrad(~xi) =

˜L(cos θi,j) · D( ~Wj, ~xi),

(10)

Xj=1

˜GP2SGrad( ~Wj) = ˜L(cos θi,j) · D(~xi, ~Wj),

where ✶ is the indicator function. The full formulation can

be rewrite as

˜GP2SGrad(~xi) =

C

Xj=1

(cos θi,j − ✶(j = yi)) ·

∂ cos θi,j

∂~xi

,

˜GP2SGrad( ~Wj) = (cos θi,j − ✶(j = yi)) ·

∂ cos θi,j

∂ ~Wj

,

(11)
Although the analysis process is slightly complicated,
the formulation of P2SGrad is not only succinct but reason-
able. When j = yi, the proposed gradient length and θi,j
are positively correlated, when j 6= yi, they are negatively
correlated. More importantly, gradient length in P2SGrad
only depends on θi,j and thus ﬁts the testing metric of face
recognition systems.

5. Experiments

In this section, we conduct a series of experiments to
evaluate the proposed P2SGrad. We ﬁrst verify advantages
of P2SGrad in some exploratory experiments by testing
the model’s performance on LFW [7]. Then we evaluate
P2SGrad on MegaFace [8] Challenge and IJBC 1:1 veriﬁ-
cation [16] with the same training conﬁguration.

5.1. Exploratory Experiments

Preprocessing and training setting. We use CASIA-
WebFace [32] as training data and ResNet-50 as the neural
network architecture. Here WebFace [32] dataset is cleaned
and contains about 450k facial images. RSA [15] is ap-
plied to images to extract facial areas and then aligns the
faces similarity transformation. All images are resized to
144 × 144. Also, we conduct pixel value normalization
by subtracting 127.5 and then dividing by 128. For all ex-
ploratory experiments, the size of a mini-batch is 512 in
every iteration.

9911

Avg. θi,yi of l2-softmax.

Grad. Length of l2-softmax.

1.0
π
2

7π
16

3π
8

5π
16
0.8

π
4

3π
16

π
8

π
0.6
16

i
y
,
i
θ

.
g
v
A

0k

30k

60k

Iteration.

90k

Avg. θi,yi of ArcFace.

Grad. Length of ArcFace.

i
y
,
i
θ

.
g
v
A

π
2

0.4
7π
16

3π
8

5π
16

π
4
0.2

3π
16

π
8

π
16

0.0

0k
0.0

h
t
g
n
e
L

.

d
a
r
G

h
t
g
n
e
L

.

d
a
r
G

1.2

1.0

0.8

0.6

0.4

0.2

0.0

1.2

1.0

0.8

0.6

0.4

0.2

0.0

i
y
,
i
θ

.
g
v
A

i
y
,
i
θ

.
g
v
A

Avg. θi,yi of CosFace.

Grad. Length of CosFace.

π
2

7π
16

3π
8

5π
16

π
4

3π
16

π
8

π
16

0k

30k

60k

Iteration.

90k

Avg. θi,yi of P2SGrad.

Grad. Length of P2SGrad.

π
2

7π
16

3π
8

5π
16

π
4

3π
16

π
8

π
16

h
t
g
n
e
L

.

d
a
r
G

h
t
g
n
e
L

.

d
a
r
G

1.2

1.0

0.8

0.6

0.4

0.2

0.0

1.2

1.0

0.8

0.6

0.4

0.2

0.0

30k

0.2

60k

90k
0.4

0k

0.6

30k

Iteration.

0.8

60k

Iteration.

90k

1.0

Figure 8. Curves of θi,yi and gradient lengths w.r.t. iteration. Gradient lengths in existing cosine-based softmax losses (top-left, top-right,
bottom-left) rapidly decrease to nearly 0 while gradient length produced by P2SGrad (bottom-right) can match θi,yi between xi and its
ground truth class yi. Best viewed in color.

Init. LR

10−1
10−2
10−3
10−4

NormFace CosFace ArcFace

Method

×
√
√
√

×
×
√
√

×
×
√
√

P2SGrad

√
√
√
√

Table 1. The sensitiveness of initial learning rates. This table
shows whether our P2SGrad and these cosine-based softmax loss
are trainable under different initial learning rates.

The change of gradient length and θi,yi w.r.t.

iter-
ation. Since P2SGrad aims to set up a reasonable map-
ping from θi,yi to the length of gradients, it is necessary
to visualize such mapping. In order to demonstrate the ad-
vancement of P2SGrad, we plot mapping curves of several
cosine-based softmax losses in Fig. 8. This ﬁgure clearly
shows that P2SGrad produces more optimal gradient length
according to the change of θi,yi .

Robustness of initial learning rates. An important
problem of margin-based loss is that they are difﬁcult to
train with large learning rates. The implementation of L-
softmax [12] and A-softmax [11] use extra hyperparameters
to adjust the margin so that the models are trainable. Thus a
small initial learning rate is important for properly training
angular-margin-based softmax losses. In contrast, accord-
ing to Table. 1, our proposed P2SGrad is stable with large
learning rates.

Convergence rate. The convergence rate is important
for evaluating an optimization method. We evaluated the
trained model’s performance on Labeled Faces in the Wild
(LFW) dataset of several cosine-based softmax losses and
our P2SGrad method at different training periods. LFW
dataset is an academic test set for unrestricted face veriﬁ-
cation. Its testing protocol contains about 13, 000 images
of about 1, 680 identities. There are 3, 000 positive matches

θi,yi of l2-softmax
θi,yi of CosFace
θi,yi of ArcFace
θi,yi of P2SGrad

π
2

7π
16

3π
8

5π
16

π
4

3π
16

π
8

π
16

i
y
,
i
θ

.
g
v
A

0k

30k

60k

Num. of Iteration

90k

Figure 9. The change of average θi,yi w.r.t. iteration number. θi,yi
represents the angle between xi and the weight vector of its ground
truth class yi. Curves by the proposed P2SGrad, l2-softmax loss
[20], CosFace [29] and ArcFace [3] are shown.

Method

l2-softmax [20]
CosFace [29]
ArcFace [3]
P2SGrad

Num. of Iteration

30k
81.50
83.63
85.32
91.25

60k
91.27
93.58
94.77
97.38

90k
97.92
99.05
99.47
99.82

Table 2. Convergence rates of P2SGrad and compared losses. With
the same number of iterations, P2SGrad leads to the best perfor-
mance.

and the same number of negative matches. Table. 2 shows
the results with the same training conﬁguration while Fig. 9
shows the decrease of average θi,yi in P2SGrad is more
quickly than other losses. These results reveal that our pro-
posed P2SGrad can optimize neural network much faster.

9912

Method

Size of MegaFace Distractor

101

102

103

104

105

106

l2-softmax [20]
CosFace [29]
ArcFace [3]
P2SGrad

99.73% 99.49% 99.03% 97.85% 95.56% 92.05%
99.82% 99.68% 99.46% 98.57% 97.58% 95.50%
99.78% 99.65% 99.48% 98.87% 98.03% 96.88%
99.86% 99.70% 99.52% 98.92% 98.35% 97.25%

Table 3. Recognition accuracy on MegaFace. Inception-ResNet [24] models trained with different compared softmax loss and the same
cleaned WebFace [32] and MS1M [4] training data.

Method

VggFace [18]

Crystal Loss [19]
l2-softmax [20]
CosFace [29]
ArcFace [3]
P2SGrad

10−5

10−4

10−3

True Acceptance Rate @ False Acceptance Rate
10−2
10−6

10−1
95.64% 87.13% 74.79% 59.75% 43.69% 32.20%
99.06% 97.66% 95.63% 92.29% 87.35% 81.15% 71.37%
98.40% 96.45% 92.78% 86.33% 77.25% 62.61% 26.67%
99.01% 97.55% 95.37% 91.82% 86.94% 76.25% 61.72%
99.07% 97.75% 95.55% 92.13% 87.28% 82.15% 72.28%
99.03% 97.79% 95.58% 92.25% 87.84% 82.44% 73.16%

10−7

-

Table 4. TARs by different compared softmax losses on the IJB-C 1:1 veriﬁcation task. The same training data (WebFace [32] and MS1M
[4]) and Inception-ResNet [24] networks are used. Results of VggFace [18] and Crystal Loss [19] are from [19].

5.2. Evaluation on MegaFace

Preprocessing and training setting. Besides the men-
tioned WebFace [32] dataset, we add another public training
dataset, MS1M [4], which contains about 2.35M cleaned
and aligned images. Here we use Inception-ResNet [5, 24]
with a batch size of 512 for training.

Evaluation results. MegaFace 1 million Challenge [8]
is a public identiﬁcation benchmark to test the perfor-
mance of facial identiﬁcation algorithms. The distractor
in MegaFace contains about 1, 000, 000 images. Here we
follow the cleaned testing protocol in [3]. The results
of P2SGrad on MegaFace dataset are shown in Table 3.
P2SGrad exceeds other compared cosine-based losses on
MegaFace 1 million challenge with every size of distractor.

carefully designed gradients. Extensive experiments vali-
date the robustness and fast convergence of the proposed
method. Moreover, experimental results show that P2SGrad
achieves superior performance over state-of-the-art meth-
ods on several challenging face recognition benchmarks.

Acknowledgements. This work is supported in part by
SenseTime Group Limited, in part by the General Research
Fund through the Research Grants Council of Hong
Kong under Grants CUHK14202217, CUHK14203118,
CUHK14205615, CUHK14207814, CUHK14213616,
CUHK14208417, CUHK14239816,
in part by CUHK
Direct Grant, and in part by National Natural Science
Foundation of China (61472410) and the Joint Lab of
CAS-HK.

5.3. Evaluation on IJBC 1:1 veriﬁcation

References

Preprocessing and training setting. Same as 5.2.
Evaluation results. The IJB-C dataset [16] contains
about 3, 500 identities with a total of 31, 334 still facial
images and 117, 542 unconstrained video frames. The en-
tire IJB-C testing protocols are designed to test detection,
identiﬁcation, veriﬁcation and clustering of faces.
In the
1:1 veriﬁcation protocol, there are 19, 557 positive matches
and 15, 638, 932 negative matches. Therefore we test Ture
Acceptance Rates at very strict False Acceptance Rates.
Table. 4 exhibits that P2SGrad surpasses all other cosine-
based losses.

6. Conclusion

we comprehensively discussed the limitation of the for-
ward and backward processes in training deep model for
face recognition. To deal with the limitations, we pro-
posed a simple but effective gradient method, P2SGrad,
which is hyperparameter free and leads to better optimiza-
tion results. Unlike previous methods which focused on loss
functions, we improve the deep network training by using

[1] Peter N. Belhumeur, Jo˜ao P Hespanha, and David J. Krieg-
man. Eigenfaces vs. ﬁsherfaces: Recognition using class
speciﬁc linear projection.
IEEE Transactions on pattern
analysis and machine intelligence, 19(7):711–720, 1997.

[2] S Chopra, R Hadsell, and Y Lecun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In Computer Vision and Pattern Recognition, 2005. CVPR
2005. IEEE Computer Society Conference on, pages 539–
546 vol. 1, 2005.

[3] Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arcface:
Additive angular margin loss for deep face recognition. arXiv
preprint arXiv:1801.07698, 2018.

[4] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and
Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for
large-scale face recognition.
In European Conference on
Computer Vision, pages 87–102. Springer, 2016.

[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[6] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. arXiv preprint arXiv:1709.01507, 2017.

9913

[22] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang.
Deep learning face representation by joint identiﬁcation-
veriﬁcation. In Advances in neural information processing
systems, pages 1988–1996, 2014.

[23] Yi Sun, Ding Liang, Xiaogang Wang, and Xiaoou Tang.
Deepid3: Face recognition with very deep neural networks.
arXiv preprint arXiv:1502.00873, 2015.

[24] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning.
In AAAI, vol-
ume 4, page 12, 2017.

[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, Andrew Rabinovich, et al. Going deeper with
convolutions. Cvpr, 2015.

[26] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior
Wolf. Deepface: Closing the gap to human-level perfor-
mance in face veriﬁcation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
1701–1708, 2014.

[27] Feng Wang, Weiyang Liu, Haijun Liu, and Jian Cheng. Ad-
ditive margin softmax for face veriﬁcation. arXiv preprint
arXiv:1801.05599, 2018.

[28] Feng Wang, Xiang Xiang, Jian Cheng, and Alan L Yuille.
Normface: l 2 hypersphere embedding for face veriﬁcation.
arXiv preprint arXiv:1704.06369, 2017.

[29] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li,
Dihong Gong, Jingchao Zhou, and Wei Liu. Cosface: Large
margin cosine loss for deep face recognition. arXiv preprint
arXiv:1801.09414, 2018.

[30] Kilian Q Weinberger and Lawrence K Saul. Distance met-
ric learning for large margin nearest neighbor classiﬁcation.
Journal of Machine Learning Research, 10(Feb):207–244,
2009.

[31] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A
discriminative feature learning approach for deep face recog-
nition. In European Conference on Computer Vision, pages
499–515. Springer, 2016.

[32] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learn-
arXiv preprint

ing face representation from scratch.
arXiv:1411.7923, 2014.

[33] Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, and
Yu Qiao. Range loss for deep face recognition with long-
tailed training data. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5409–
5418, 2017.

[7] Gary B Huang, Manu Ramesh, Tamara Berg, and Erik
Learned-Miller. Labeled faces in the wild: A database for
studying face recognition in unconstrained environments.
Technical report, Technical Report 07-49, University of Mas-
sachusetts, Amherst, 2007.

[8] Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel
Miller, and Evan Brossard. The megaface benchmark: 1
million faces for recognition at scale. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4873–4882, 2016.

[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, pages 1097–1105, 2012.

[10] Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, and
Shree K Nayar. Attribute and simile classiﬁers for face veri-
ﬁcation. In Computer Vision, 2009 IEEE 12th International
Conference on, pages 365–372. IEEE, 2009.

[11] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha
Raj, and Le Song. Sphereface: Deep hypersphere embedding
for face recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), volume 1, 2017.

[12] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang.
Large-margin softmax loss for convolutional neural net-
works. In ICML, pages 507–516, 2016.

[13] Yu Liu, Hongyang Li, and Xiaogang Wang. Learning deep
features via congenerous cosine loss for person recognition.
arXiv preprint arXiv:1702.06890, 2017.

[14] Yu Liu, Hongyang Li, and Xiaogang Wang. Rethinking fea-
ture discrimination and polymerization for large-scale recog-
nition. arXiv preprint arXiv:1710.00870, 2017.

[15] Yu Liu, Hongyang Li, Junjie Yan, Fangyin Wei, Xiaogang
Wang, and Xiaoou Tang. Recurrent scale approximation for
object detection in cnn. In IEEE International Conference
on Computer Vision, 2017.

[16] Brianna Maze, Jocelyn Adams, James A Duncan, Nathan
Kalka, Tim Miller, Charles Otto, Anil K Jain, W Tyler
Niggel, Janet Anderson, Jordan Cheney, et al.
Iarpa janus
benchmark–c: Face dataset and protocol. In 11th IAPR In-
ternational Conference on Biometrics, 2018.

[17] Aaron Nech and Ira Kemelmacher-Shlizerman. Level play-
In 2017 IEEE
ing ﬁeld for million scale face recognition.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3406–3415. IEEE, 2017.

[18] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al.

Deep face recognition. In BMVC, volume 1, page 6, 2015.

[19] Rajeev Ranjan, Ankan Bansal, Hongyu Xu, Swami Sankara-
narayanan, Jun-Cheng Chen, Carlos D Castillo, and Rama
Chellappa. Crystal loss and quality pooling for uncon-
strained face veriﬁcation and recognition. arXiv preprint
arXiv:1804.01159, 2018.

[20] Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-
constrained softmax loss for discriminative face veriﬁcation.
arXiv preprint arXiv:1703.09507, 2017.

[21] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniﬁed embedding for face recognition and clus-
tering. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 815–823, 2015.

9914

