Latent Filter Scaling for Multimodal Unsupervised Image-to-Image Translation

King Abdullah University for Science and Technology (KAUST)

KAUST

KAUST

Yazeed Alharbi

Neil Smith

Peter Wonka

yazeed.alharbi@kaust.edu.sa

Figure 1: An illustration of latent ﬁlter scaling. Input noise codes are mapped to the ﬁlters of the network, instead of injection
into the input image.

Abstract

1. Introduction

In multimodal unsupervised image-to-image translation
tasks, the goal is to translate an image from the source
domain to many images in the target domain. We present
a simple method that produces higher quality images than
current state-of-the-art while maintaining the same amount
of multimodal diversity. Previous methods follow the un-
conditional approach of trying to map the latent code di-
rectly to a full-size image. This leads to complicated net-
work architectures with several introduced hyperparame-
ters to tune. By treating the latent code as a modiﬁer
of the convolutional ﬁlters, we produce multimodal output
while maintaining the traditional Generative Adversarial
Network (GAN) loss and without additional hyperparam-
eters. The only tuning required by our method controls the
tradeoff between variability and quality of generated im-
ages. Furthermore, we achieve disentanglement between
source domain content and target domain style for free as
a by-product of our formulation. We perform qualitative
and quantitative experiments showing the advantages of
our method compared with the state-of-the art on multiple
benchmark image-to-image translation datasets.

Recently, GANs have emerged as a promising research
ﬁeld [9, 3, 2, 11, 19]. The use of adversarial training has
shown to be effective in many computer vision tasks. Gen-
erative networks can be trained with reasonable success to
produce realistic images of humans and objects [6, 21, 7].

In the unconditional generation task, the aim is to map
a randomly generated low-dimensional code to a realistic
image. After training, the entries of the input code should
control sources of variation in the output image. There is an
effort in the community to guarantee certain desirable qual-
ities about this mapping. For example, InfoGAN [6] aims
to enhance the interpretability of changing the latent code
through disentanglement. Other methods aim for a mapping
where the interpolation between two latent codes results
in an image that is semantically interpolated between the
images resulting from the two codes. Additionally, some
methods aim for an application-speciﬁc disentangled map-
ping, where changing certain parts of the input code will
correspond to changes in certain parts of the output images
[20, 17]. In the case of pedestrian image generation, the dis-
entanglement might allow the user to change the pedestrian
appearance independently of the background.

In the literature, image-to-image translation is handled
similarly to unconditional generation, where the latent vec-

1458

In order to generate
tor is directly mapped to an image.
many images in the target domain for each input image, pre-
vious methods [27, 12, 15] make use of an input latent code
in addition to autoencoding, adding multiple terms to the
loss. The intuitive idea behind our contribution is that in
image-to-image translation tasks we often start with an im-
age and the aim is to use the latent vector to control local
changes in the input image. Therefore, we opt for treating
the latent vector as a modiﬁer of the network’s ﬁlters, in-
stead of taking the unconditional approach of treating the
latent vector as encoded data.

Contributions: Our contribution is a method for multi-
modal unsupervised image-to-image translation that is sim-
ple, competitive with the state-of-the-art, and provides dis-
entanglement of the latent codes from the input images.
This is achieved without modifying standard GAN loss and
with minimal overhead. Our method achieves state-of-the-
art quality and diversity on multiple image-to-image trans-
lation datasets, while maintaining the simplicity and gen-
erality of standard GAN loss and architecture. To the best
of our knowledge, our method is the ﬁrst method that does
not require autoencoding or reconstruction losses for latent
codes or images. Our method essentially prevents mode
collapse in image-to-image GANs while enabling a larger
degree of freedom in the quality-variability tradeoff. In con-
trast, previous methods include several losses in addition to
the GAN loss, each with a new hyperparameter to tune.

Notation: In this paper, GANs that take an image from
one domain and produce an image in another domain will be
referred to as image-to-image translation GANs. If paired
data are used, the GAN will be referred to as supervised. It
will be referred to as unsupervised if the images from the
two domains are not paired. Finally, image-to-image trans-
lation GANs that produce a single image will be referred to
as deterministic or unimodal, while multimodal ones make
use of an input latent vector in addition to the input image
to produce many outputs.

2. Related Work

For paired data, conditional adversarial networks [13,
23] show reasonable results that are applicable to almost
any dataset. However, requiring paired data is a major limi-
tation.

Many works in literature search for ways to produce sim-
ilar results without using pair information [12, 15, 1]. Zhu
et al. [26] demonstrate that unimodal unpaired image-to-
image translation tasks are under-constrained, since there
might be many possible translations in the output space.
CycleGAN [26] is one of the most successful approaches
to handling this issue. Instead of training one network that
maps source domain images to target domain images, they
propose adding another network to map domain images to
source images forming a cycle. They argue that imposing

this cycle-consistency loss produces better image quality.

While there are acceptable results for unconditional gen-
eration GANs and unimodal image-to-image translation
GANs, the multimodal case is an open area of research. The
success of GANs in generating diverse output images from
scratch does not naturally extend to image-to-image trans-
lation problems. Diversity in the conditional case, where
an image is given and the goal is to translate it into another
class of images, is proving to be more challenging. Un-
like the unconditional case, where the latent vector can be
simply mapped to a full size image, the conditional case
requires using both the latent vector and the input image.
Simple concatenation of the latent vector with the image of-
ten leads to deterministic output, where the network learns
to ignore the latent code. This is the problem that most
conditional image-to-image translation papers tackle: pre-
serving original image structure, and preserving inﬂuence
and variability of the latent vector. In other words, given
an input image and a latent vector, the aim is to change the
appearance using the latent vector, such that different latent
vectors produce different target domain images.

Earlier proposals for multimodal output are largely lim-
ited in capacity and application[8, 4, 5].
In the case of
PixelNN [4], multi-modality is achieved by providing some
form of the output image to the GAN. For example, a low
resolution image or a normal map must be fed to the GAN,
and given different low resolution images different outputs
will be produced. On the other hand, MADGAN [8] em-
ploys multiple generators with the obvious limitation of
having a constant and discrete number of generated target
domain modes. Cascaded Reﬁnement Networks [5] have
the same limitation of generating a constant discrete num-
ber of outputs at test time.

Previous methods [27, 12, 15, 1] follow the uncondi-
tional approach for achieving conditional multi-modality.
In the unconditional case, the latent code is often inter-
preted as a compressed version of the output image. After
training an unconditional image-to-image translation GAN,
it is often found that entries in the input noise vector cor-
respond to semantic labels of the output images. For ex-
ample, on MNIST, changing one entry in the input vector
might change thickness or slope of a digit. In a sense, un-
conditional image generation is an inverse problem where
the goal is to enforce the correspondence between latent
code entries and semantic variations in the resulting image.
Following this approach for image-to-image GANs, previ-
ous methods are often forced to compress the input image
until concatenation with the low-dimensional code is mean-
ingful. One of the more successful ways to achieve this
is the disentanglement approach where the compression is
regularized such that each image can be described by two
components: the latent code which is domain speciﬁc, and
a content code which is shared between source and target

1459

domain [12, 15].

One of the earlier methods for multimodal image-to-
image translation is BicycleGAN [27]. BicycleGAN com-
bines two existing loss cycles: one to encourage latent code
diversity and one to encourage faithfulness to ground truth
images. One limitation of BicycleGAN is requiring paired
images. This raises a new problem of producing multimodal
output in an unsupervised fashion without requiring pair in-
formation. Recent methods [12, 15] handle the problem by
disentangling the input image into content and style. Con-
tent is assumed to be shared between the input and output
distributions, while style is domain-speciﬁc. Then, to trans-
late an image, the content code is computed and is concate-
nated with a style code sampled from target distribution.
While augmented CycleGAN [1] does not disentangle input
images, they still follow the common approach of injecting
the latent code somewhere in the network, and adding loss
terms to ensure the latent code is meaningful and diverse.

While disentanglement can be desirable in some graph-
ics problems where there is need to manipulate certain parts
of the image independently, it introduces several hyperpa-
rameters to tune. We claim that it is not necessary to pro-
duce multimodal output. Contrary to the state-of-the-art
approaches, we propose handling the conditional image-to-
image problem in an entirely different approach from the
unconditional case.

3. Methodology

3.1. Problem setup

Our goal is to perform multimodal unsupervised image-
to-image translation. Given an image x from source do-
main X, we want to translate it to many images yi in domain
Y. To produce multimodal output, we accept a latent code
z ∼ N (0k x 1, I k x k) that is expected to describe the ways
in which our output should differ. So our task is to ﬁnd a
function G such that: G: (x, zi) → yi, where x ∈ X, yi ∈ Y.

3.2. The latent scaling approach

The core argument of our work presented here is that the
latent code in conditional image generation should be inter-
preted differently than in the unconditional case. Tradition-
ally, the latent code is concatenated with the input image di-
rectly or in feature space after compression through autoen-
coding. This follows the interpretation of the latent code as
encoded data to be converted to a full-sized image. Instead,
we propose interpreting the latent code entries as modula-
tors of local changes in the input image. Speciﬁcally, the
latent code is not considered as encoded data, but as a mod-
iﬁer of the convolutional operations of the network. A sim-
ple analogy is that previous methods generate diverse new
images by appending the input image with different chan-
nels. On the other hand, our method generates diverse new

images by using different brushes. We map the latent code
to ﬁlters, such that the latent code modulates the strength of
applying each ﬁlter. This is easiest to explain in the case
of having a latent vector of length 3 operating only on the
last 3 channels of the network. In that case, the latent vector
will modulate only the color of the output image. How-
ever, in the hidden layers of the generator network, scal-
ing the ﬁlters will correspond to modulating the resulting
feature maps. For example, scaling an edge-detecting ﬁl-
ter might result in stronger edges in the ﬁnal image. Given
a k-dimensional latent code and an input image, we push
the latent code through a fully-connected network to pro-
duce a scalar per ﬁlter. Then, the image is pushed through
the convolutional network where each ﬁlter is scaled by the
mapped latent code. By not treating the latent code as data
to be concatenated with the input image, we provide a sim-
pler solution of the problem. Our approach allows a simple
image-to-image GAN to achieve comparable quality with
state-of-the-art while producing more diverse target domain
images. Furthermore, without modifying standard GAN
loss, we achieve disentanglement between source domain
image content and target domain image style. This occurs
as a result of our formulation where the latent code corre-
sponds to local changes in the input image.

3.3. Simplifying hyperparameters for multimodal 

ity

One major beneﬁt of our method is the way it preserves
the simple GAN loss. Since previous methods treat the la-
tent code as compressed data, a simple concatenation of the
latent code to the input image often leads to unimodal be-
havior where the latent code is ignored. This might be a
result of augmenting a high-dimensional image with a low-
dimensional vector. It is clear that the traditional usage of
latent codes in unconditional GANs does not naturally ex-
tend to image-to-image GANs, as it requires adding mul-
tiple losses for reconstruction of images and latent codes.
This is evident in the losses of BicycleGAN[27] and MU-
NIT [12].

Bicycle GAN loss:

min
G,E

max

D

LVAE

GAN(G, D, E) + λLVAE

1

(G, E)

(1)

+LGAN(G, D)
+λlatentLlatent

1

(G, E) + λKLLKL(E)

Where λ is a hyperparameter that controls the weight of
the L1 VAE loss, λlatent controls the weight of the recon-
struction loss of the latent code, and λKL controls the weight
of encouraging the encoded distribution to be similar to a
random Gaussian.

1460

Figure 2: The results of applying the same latent codes to
different input images. Disentanglement occurs as a result
of our change of viewpoint and without explicit guidance.

Figure 3: The results of interpolating between latent codes
and generating the corresponding images.

Latent scaling loss:

min

G

max

D

= LGAN(G, D)

(3)

MUNIT loss:

min

E1,E2,G1,G2

max
D1,D2

L(E1, E2, G1, G2, D1, D2) = Lx1

GAN

We choose to use the least-squares GAN loss (LSGAN)

as described in [18]:

+Lx2
+λc(Lc1

GAN + λx(Lx1
recon + Lc2

recon + Lx2
recon)
recon) + λs(Ls1

recon + Ls2

recon)

(2)

min
D

min

G

LGAN(D) =E

y(cid:2)(D(y) − 1)2(cid:3)

LGAN(G) =E

x,z(cid:2)D(G(x, z))2(cid:3)

+ E
x,z(cid:2)(D(G(x, z)) − 1)2(cid:3)

(4)

Where λx controls the weight of image reconstruction
loss, λc controls the weight of disentangled content code
reconstruction loss, and λs controls the weight of disentan-
gled style code reconstruction loss for both source and tar-
get domains.

By treating the latent code as a modiﬁer of the network’s
ﬁlters, we can use the traditional GAN loss without any ad-
ditional encoding or decoding, while preventing mode col-
lapse for image-to-image translation. Our network learns
to map an input Gaussian latent code to a scalar per ﬁlter,
consequently learning to map different scalings of the net-
work’s ﬁlters to different output images. Throughout train-
ing, the latent code is never injected into the input image
nor the learned feature maps. Instead, the latent code en-
tries are multiplied by the feature maps. Consequently, we
avoid the need for reconstruction losses and the need for
losses based on encoded latent code diversity, and we use
only the standard GAN loss.

We apply label smoothing such that the desired value for
discriminator real samples and for generator samples is 0.9
instead of 1.

3.4. Mapping a low dimensional latent vector to

scalars

There is a design choice in how to map the latent vector
to the actual scalars that affect feature maps. One option
is to sample a Gaussian with the same dimensionality as
the network’s ﬁlters. This leads to highly undesirable ef-
fects. As noted in [27], having a latent vector with high
dimensionality makes sampling more difﬁcult and leads to
the network modeling less meaningful sources of variation.
Since the total number of ﬁlters in state-of-the-art image-to-
image networks is high (over 2000 ﬁlters), we use a fully-
connected network to map a low-dimensional latent vector
to the actual number of ﬁlters.

The mapping process leads to desirable effects. First, it
allows us to maintain the same latent code sampling pro-

1461

3.6. Controlling the tradeoff between variability

and quality

The only tuning required by our method is tuning the
mapping between input latent codes and the ﬁnal scalars
per ﬁlter. In our experiments, the mapping choice can be
tuned easily. We ﬁnd that applying the hyperbolic tangent
function (tanh) as the activation function leads to conserva-
tive variation with the beneﬁt of high quality images. Us-
ing leaky rectiﬁed linear unit (LRELU) or a linear fully-
connected network leads to more variability at the cost
of occasionally generating unrealistic images. This can
be attributed to the magnitude of the mapped scalars be-
ing bounded when using tanh and unbounded when using
LRELU or no activation.

We ﬁnd that using bias in the fully-connected network
leads to more conservative results. In this case, we believe
that the average target domain image is contained in the
bias, while the variation is achieved by the multiplied in-
put code values. Eliminating bias leads to more variation in
the produced images but again at the cost of quality.

Tuning the mapping between latent codes and scalars per
ﬁlter allows user control over variety and quality of pro-
duced images. While there’s dependence on the speciﬁc
dataset, we ﬁnd that using tanh in the ﬁnal activation layer
with bias leads to high quality low variety images. On the
other hand, using an afﬁne layer (linear activation without
bias) leads to high variety low quality images. We show
some results of modifying the scalar mapping in Figure 4.

Figure 4: A demonstration of user control over the vari-
ability/quality tradeoff. An afﬁne activation leads to more
stylistic variety at the cost of lower quality images. A tanh
activation leads to very realistic images but with lower vari-
ability.

cedure as previous methods in the ﬁeld. Since our method
multiplies the ﬁlters by the latent code, we ﬁnd that it helps
training to learn how to scale the input latent code. Second,
it allows the network to learn to make ﬁlters work in tan-
dem. Finally, control over the ﬁnal mapped scalars allows
for controlling the tradeoff between quality and variability
in output images.

3.5. Disentanglement through mapped scalars

3.7. Compatibility with existing deep learning li 

After training, we examine the learned mapping between
input noise which is sampled from a Gaussian distribution.
We ﬁnd that the latent codes, without explicit guidance, are
style codes that are independent of the input image. Mean-
ing that providing the same input code with multiple source
domain images will produce target domain images of the
same style. This indicates that our network is learning
target domain style independently of the input image, and
not correlating speciﬁc inputs with speciﬁc styles. Thus,
our method obtains disentanglement between the source do-
main image and the target domain style essentially for free.
Disentanglement occurs as a result of the network mapping
the latent code to operations on the input images. Just as
painting a shoe with a certain color or with certain specu-
larity should easily transfer to other shoes. We show the
result of using the same latent codes with different images
in Figure 2.

Additionally, we demonstrate how interpolating between
two latent codes leads to semantically interpolated images,
as shown in Figure 3. The interpolation is not only in color,
but also in features such as specularity and the presence of
shoe string holes.

braries

In many deep learning packages, it is easier to handle
feature maps per batch than ﬁlters. This is because ﬁlters
are per network, while feature maps are per input image.
Therefore, we choose to multiply the latent vector entries
by the feature maps instead of the ﬁlters to offer compati-
bility with most deep learning libraries. By the associative
property of convolution, this is equivalent to the scalar mul-
tiplication with the ﬁlters:

(c ∗ f ) ⊛ I = c ∗ (f ⊛ I)

where c is a scalar, f is a k x k ﬁlter, I is a m x n image, * is
the scalar multiplication operation, and ⊛ is convolution.

3.8. Implementation details

We follow the network architecture described in [26]
with a few differences as seen in Figure 1. First, our net-
work contains only one generator that takes an image from
the source domain and produces an image in the target do-
main, and one discriminator (instead of two generators and
discriminators cycling between source and target domains).
Second, we accept a latent code as an input to the network.

1462

Figure 5: A comparison between shoes generated by our
method, BicycleGAN, and MUNIT. AMT users preferred
BicycleGAN to both, but preferred ours to MUNIT.

Finally, we add a trainable fully-connected network to map
the input latent code to scalars which are multiplied by each
feature map of the network. The scalars are multiplied di-
rectly by the output of the convolution and before applying
RELU or instance normalization.

3.9. Similarity to previous methods

The idea of modifying the scale of a feature map was
explored previously but with a different approach and for
a different task. Adaptive Instance Normalization (AdaIN)
[10] was proposed for style transfer, where the authors pro-
posed transferring feature map statistics in order to transfer
style from one image to another. A major difference be-
tween our method and AdaIN is that we learn feature map
scales, where AdaIN simply computes scales from the in-
put style image. Furthermore, we apply the scaling to fea-
ture maps prior to any normalization which is equivalent to
ﬁlter scaling. StyleGAN is a more similar concurrent work
where Karras et al. [14] explore ﬁlter scaling and show great
results for unconditional face image generation.

4. Evaluation

We perform qualitative and quantitative experiments us-
ing benchmark image-to-image translation datasets. We
show results on the following datasets: winter to summer
[16], edges to shoes [24], and labels to facades [22].

For our qualitative experiments, we compare our results
with BicycleGAN [27], and MUNIT [12]. These are two
state-of-the-art methods. BicycleGAN uses pair informa-
tion, while MUNIT, as well as our method, do not use pair
information. We use two metrics: quality, measured by
Amazon Mechanical Turk (AMT) user preference, and di-
versity, measured by Learned Perceptual Image Patch Sim-
ilarity (LPIPS) [25].

Figure 6: A comparison between the variety of generated
windows in our method and that of BicycleGAN. Note how
BicycleGAN is primarily changing color, while our method
changes style as well.

4.1. Quantitative results

We follow the same experimental settings as [27] and
[12]. To measure quality, we present AMT users with two
generated images: one by our algorithm, and the other by
a different algorithm, and ask them to pick the image they
prefer. We use the likelihood of images generated by an al-
gorithm to be picked by users as the quality measure. Simi-
larly to Bicycle GAN and MUNIT, we adopt the LPIPS [25]
diversity score as a quantitative measure of variability in the
GAN output conditioned on the same image. We measure
the LPIPS score using 1900 pairs, where each pair is two
different generated images conditioned on the same input
image.

Quality Diversity

BicycleGAN 57.2%

Ours

—

MUNIT

45.1%

0.104

0.109

0.109

Table 1: Comparison of quality and diversity between ours
and state-of-the-art methods. Quality is measured by the
percentage of images where another algorithm was pre-
ferred to ours. Diversity is measured by perceptual distance

1463

Figure 7: Our results on the winter to summer dataset. Learnt sources of semantic variation include time of day, presence of
clouds, and amount of foliage.

As shown in Table 1, our method results in the same va-
riety (as measured by the pairwise LPIPS distance) as the
best previously reported results but with higher AMT user
rating of quality on the edges to shoes dataset. We conﬁrm
results by [12] showing the superior quality of BicycleGAN
which uses pair information. BicycleGAN quality is pre-
ferred to ours about 57% of the time. This is comparable to
the results obtained by MUNIT, showing that BicycleGAN
is preferred to MUNIT 56% of the time.
In terms of di-
versity BicycleGAN scores lower than both our method and
MUNIT. However, we ﬁnd that our method was preferred to
MUNIT about 55% of the time. The quality of images gen-
erated by our algorithm can be seen in 5. While our method
uses only the standard GAN loss without additional hyper-
parameters, we obtain higher quality than MUNIT and more

stylistic variability than BicycleGAN.

4.2. Qualitative results

We present images generated by our method to conﬁrm
the quantitative results in terms of variety and quality. Fig-
ure 7 showcases our results on the winter to summer im-
ages. Since our method leads to style disentanglement, we
produce synchronized results using the same code for each
style. Our method learns several sources of semantic vari-
ability including time of day, condition of the sky, as well
as amount of foliage.

As can be seen in Figure 6, our method leads to more
semantic variability which can be observed in the windows
of generated facades. While BicycleGAN produces high-
quality facade images, the variance in the window appear-

1464

ance is limited mainly to color. Our method on the other
hand exhibits multiple realistic kinds of windows. This sup-
ports the higher diversity score of our method on the edges
to shoes dataset. We believe that the use of pair information
often restricts BicycleGAN to produce images that are too
close to the ground truth. This explains why unsupervised
methods such as our method and MUNIT produce images
that exhibit more variety.

4.3. Discussion

Quantitative and qualitative results show that our
method, while substantially simpler than both BicycleGAN
and MUNIT, improves upon the best reported quality and
diversity of upaired multimodal image-to-image GANs. We
outperform MUNIT in terms of quality on the edges2shoes
dataset while obtaining a higher diversity score than Bicy-
cleGAN.

Examining images generated by our method shows a
high semantic variability. This includes generating differ-
ent windows or architectural styles on the labels2facades
dataset, and generating different times of day on the sum-
mer2winter dataset as seen in Figures 6 and 7. Generated
images shown in Figure 5 conﬁrm our quantitative results.
We ﬁnd that, in general, BicycleGAN produces the highest
quality images, while our method produces higher quality
than MUNIT.

Additionally, after training using our method we show
that latent codes are independent of the input images. As a
result, latent codes are transferable from one output image
to another, such that using the same latent code with dif-
ferent input images yields output images of the same style.
This follows from our motivation where we map latent code
entries to operations. Furthermore, interpolation between
latent codes produces images that are interpolated in a se-
mantic sense.

Our network, while simple and easy to tune, outperforms
state-of-the-art methods. We believe that our success can be
explained by referring to our change of viewpoint. Previous
methods either extend unconditional GANs to accept im-
ages as additional input, or extent image-to-image GANs to
accept a latent vector as additional input. As a result, au-
toencoding losses are needed to guarantee that input images
and latent codes are not ignored. Therefore, a large portion
of the training procedures is spent on tasks that are not nec-
essary for multimodality, and only serves to preserve con-
tent after encoding or decoding. Our method, on the other
hand, directly maps the latent code to the convolutional op-
erations of the network. By design, the input image and
the latent code will not be ignored. In addition, the latent
code directly affects the convolutional operations. Thus, we
avoid complicating the network architecture and we avoid
solving auxiliary tasks. We believe that the simplicity and
effectiveness of our method will lead to a wide adoption in

the future in any image-to-image translation task.

5. Limitations and future work

The main limitation of our method is ﬁnding a good map-
ping between randomly generated latent codes and scalars
per ﬁlter. We believe that more work can be done in ﬁnding
an optimal mapping. While almost any setting can lead to
diverse and high-quality images, there are settings that are
noticeably better than others.

Another limitation is related to disentanglement. Meth-
ods that use reconstruction losses to learn how to disentan-
gle input images (such as MUNIT) have an advantage in
style transfer. This is because the style from the input im-
age can be applied to another input images. In our method
the disentanglement occurs in the target domain. In other
words, our method cannot extract style from an input image
and apply it to another input image. The disentanglement in
our method, however, can take a noise code that generated
a certain output image and apply it to any input image to
transfer the style. Thus, it is easy to transfer the style of a
generated image to any input image, but to transfer the style
of an input image to another input image the user must gen-
erate many images and use the latent code of the generated
image most similar to the input image.

6. Conclusion

We present a method for multimodal unsupervised
image-to-image translation. Our method is based on the
idea that latent codes should be interpreted as modiﬁers
of operations, and not as encoded data,
in the case of
conditional image generation. This formulation produces
disentangled codes without autoencoding loss and with-
out adding to the standard GAN loss. Our results show
improvement on the state-of-the-art both qualitatively and
quantitatively in terms of quality and diversity while using
a drastically simpler network architecture. The simplicity of
the architecture means easier implementation for users. In
addition to simplicity, our method is general and can be ap-
plied to existing image-to-image translation methods such
as CycleGAN [26].

Acknowledgement The project was funded in part by
the KAUST Ofﬁce of Sponsored Research (OSR) under
Award No. URF/1/3426-01-01.

References

[1] Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip
Bachman, and Aaron Courville. Augmented cyclegan:
Learning many-to-many mappings from unpaired data. arXiv
preprint arXiv:1802.10151, 2018.

[2] Martin Arjovsky and L´eon Bottou. Towards principled
methods for training generative adversarial networks. arXiv
preprint arXiv:1701.04862, 2017.

1465

[18] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares genera-
tive adversarial networks. In Computer Vision (ICCV), 2017
IEEE International Conference on, pages 2813–2821. IEEE,
2017.

[19] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-
Dickstein. Unrolled generative adversarial networks. arXiv
preprint arXiv:1611.02163, 2016.

[20] Franziska Mueller, Florian Bernard, Oleksandr Sotny-
chenko, Dushyant Mehta, Srinath Sridhar, Dan Casas, and
Christian Theobalt. Ganerated hands for real-time 3d hand
tracking from monocular rgb.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 49–59, 2018.

[21] Alec Radford, Luke Metz, and Soumith Chintala. Un-
supervised representation learning with deep convolu-
tional generative adversarial networks.
arXiv preprint
arXiv:1511.06434, 2015.

[22] Radim Tyleˇcek and Radim ˇS´ara. Spatial pattern templates
In Proc.

for recognition of objects with regular structure.
GCPR, Saarbrucken, Germany, 2013.

[23] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image
synthesis and semantic manipulation with conditional gans.
arXiv preprint arXiv:1711.11585, 2017.

[24] Aron Yu and Kristen Grauman. Fine-grained visual compar-
isons with local learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
192–199, 2014.

[25] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. CoRR, abs/1801.03924,
2018.

[26] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. arXiv preprint, 2017.

[27] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation.
In Advances
in Neural Information Processing Systems, pages 465–476,
2017.

[3] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein generative adversarial networks.
In Interna-
tional Conference on Machine Learning, pages 214–223,
2017.

[4] Aayush Bansal, Yaser Sheikh, and Deva Ramanan. Pix-
arXiv preprint

elnn: Example-based image synthesis.
arXiv:1708.05349, 2017.

[5] Qifeng Chen and Vladlen Koltun. Photographic image syn-
thesis with cascaded reﬁnement networks.
In IEEE Inter-
national Conference on Computer Vision (ICCV), volume 1,
page 3, 2017.

[6] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Infogan: Interpretable repre-
sentation learning by information maximizing generative ad-
versarial nets. In Advances in neural information processing
systems, pages 2172–2180, 2016.

[7] Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep
generative image models using a laplacian pyramid of adver-
sarial networks. In Advances in neural information process-
ing systems, pages 1486–1494, 2015.

[8] Arnab Ghosh, Viveka Kulharia, Vinay Namboodiri,
Philip HS Torr, and Puneet K Dokania. Multi-agent
diverse generative adversarial networks.
arXiv preprint
arXiv:1704.02906, 1(4), 2017.

[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014.

[10] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 1501–1510, 2017.

[11] Xun Huang, Yixuan Li, Omid Poursaeed, John E Hopcroft,
and Serge J Belongie. Stacked generative adversarial net-
works. In CVPR, volume 2, page 3, 2017.

[12] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
Multimodal unsupervised image-to-image translation. arXiv
preprint arXiv:1804.04732, 2018.

[13] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. arXiv preprint, 2017.

[14] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
arXiv preprint arXiv:1812.04948, 2018.

[15] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh
Singh, and Ming-Hsuan Yang. Diverse image-to-image
translation via disentangled representations. arXiv preprint
arXiv:1808.00948, 2018.

[16] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised
image-to-image translation networks. In Advances in Neural
Information Processing Systems, pages 700–708, 2017.

[17] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc
Van Gool, Bernt Schiele, and Mario Fritz. Disentangled
person image generation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
99–108, 2018.

1466

