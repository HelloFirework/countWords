An Alternative Deep Feature Approach to Line Level Keyword Spotting

George Retsinas1

,

2, Georgios Louloudis1, Nikolaos Stamatopoulos1, Giorgos Sﬁkas1

,

3, Basilis Gatos1

1 Institute of Informatics and Telecommunications, NCSR “Demokritos”, Greece

2 School of Electrical and Computer Engineering, National Technical University of Athens, Greece

3 Department of Computer Science and Engineering, University of Ioannina, Greece

{georgeretsi,louloud,nstam,sfikas,bgat}@iit.demokritos.gr

Abstract

Keyword spotting (KWS) is deﬁned as the problem of
detecting all instances of a given word, provided by the
user either as a query word image (Query-by-Example,
QbE) or a query word string (Query-by-String, QbS) in a
body of digitized documents. Keyword detection is typi-
cally preceded by a preprocessing step where the text is seg-
mented into text lines (line-level KWS). Methods following
this paradigm are monopolized by test-time computation-
ally expensive handwritten text recognition (HTR)-based
approaches; furthermore, they typically cannot handle im-
age queries (QbE). In this work, we propose a time and
storage-efﬁcient, deep feature-based approach that enables
both the image and textual search options. Three distinct
components, all modeled as neural networks, are combined:
normalization, feature extraction and representation of im-
age and textual input into a common space. These com-
ponents, even if designed on word level image representa-
tions, collaborate in order to achieve an efﬁcient line level
keyword spotting system. The experimental results indicate
that the proposed system is on par with state-of-the-art KWS
methods.

1. Introduction and Related work

A signiﬁcant, as well as ever-increasing amount of digi-
tized handwritten documents exists today all over the world,
creating an urgent need for efﬁcient and automatic docu-
ment collection indexing. Keyword spotting (KWS) refers
to the task of automatically retrieving instances of a given
keyword query within a document image. KWS has been
proposed as an alternative to full text recognition in cases
where recognition is deemed to be very difﬁcult or expected
to give poor results [5].

Depending on how the query word is speciﬁed by the
user, either using an example word image or just a text

string, we can distinguish Query-by-Example (QbE) from
Query-by-String (QbS) keyword spotting methods, respec-
tively. A different taxonomy of KWS systems is related
with the existence of segmentation of the document collec-
tion resulting to the distinction into segmentation-based and
segmentation-free systems. Segmentation-free approaches
aim to locate the query instances on the entire document
without the involvement of any segmentation procedure
whereas segmentation-based approaches assume that a seg-
mentation step into either line (line-based) or word (word-
based) level has preceded. The proposed method is consid-
ered as segmentation-based (more speciﬁcally line-based)
since it is assumed that the segmented lines are provided.
The main innovation introduced is that, for the ﬁrst time,
both QbE and QbS search options on line level are allowed
under the same KWS system. The proposed system, which
relies on the success of Pyramidal Histogram of Charac-
ters (PHOC) embeddings [1], is based on the combination
of three components, namely the Character Width Normal-
ization, the Feature Extraction and the Common Space En-
coder. It should be noted that the main component of each
subtask is a neural network. Experiments conducted on the
well-known data set IAM [9] prove the effectiveness of the
proposed system.

Concerning word-based KWS methods,

the seminal
work of Almaz´an et al. [1], which introduced the PHOC
representation, has inﬂuenced many recently proposed
methods. In a nutshell, the PHOC representation encodes
whether a speciﬁc character appears in a particular spatial
region of a text string. The main idea of this method re-
lies on the ability to embed both word images as well as
text strings into a common vectorial subspace. The use of
a common representation for both word images and text
strings makes feasible a search using either a word im-
age (QbE) or a word string (QbS). The abovementioned
PHOC representation is learned using Support Vector Ma-
chines (SVMs) while Kernel Common Subspace Regres-
sion (KCSR) is used in order to ﬁnd projections that max-

12658

imize the correlation of the image space and the text space
in a common latent subspace.

A number of recent methods have been inspired by the
work of Almaz´an et al., further extending or adapting the
base model [11],[12],[7],[8],[16]. All these word-based ap-
proaches for keyword spotting make use of Convolutional
Neural Networks (CNNs) instead of SVMs and KCSR for
the attribute and common subspace learning. Sudholt and
Fink [11, 12] were the ﬁrst to use CNNs in order to learn
the PHOC representation given the raw word image con-
tent. The method is able to outperform Almaz´an’s method
under both QbE and QbS scenarios. In Krishnan et al. [7], a
word image representation is ﬁrst learned using a CNN and
subsequently used to learn a common subspace with KCSR
as in [1]. Finally, in Wilkinson et al. [16] a triplet CNN
is employed, accepting pairs of positive word matches to-
gether with a negative word match. Moreover, a new word
embedding is proposed, dubbed DCT of Words (DCTofW).

[3].

Methods categorized to the line-based approach typi-
cally subscribe to the QbS paradigm. Line-based keyword
It is
spotting was ﬁrst introduced by Fischer et al.
a supervised method which is based on character Hidden
Markov Models (HMMs). The main advantage of the pro-
posed method is its ability to work on line images making a
word segmentation step unnecessary. Furthermore, the sys-
tem is making use of character models which can be trained
very easily since line level transcription and segmentation
of document images can be produced very easily, while en-
abling zero-shot learning at the same time. The authors ex-
tended this work in [2], where they incorporated charac-
ter language models (unigrams and bigrams) at the decod-
ing step leading to a better performance compared to their
previous method. A major drawback of the methods pre-
sented by Fischer et al. is the large computational cost of the
keyword-speciﬁc HMM Viterbi decoding process needed to
obtain the conﬁdence scores of each word to be spotted.
Toselli and Vidal [14] presented a novel approach for the
computation of conﬁdence scores, directly from character
lattices produced during a single Viterbi decoding process
using only the ﬁller model. Experiments showed that same
spotting results were obtained using the proposed method,
while requiring between one and two orders of magnitude
less query computing time.

The best-performing approach for line-based keyword
spotting was reported by Frinken et al. [4]. In this work, the
authors make use of a recurrent neural network (BLSTM
network) using a modiﬁcation of the CTC token passing al-
gorithm which was previously used for Handwritten Text
Recognition (HTR). Experiments conducted on the IAM
dataset showed that the method outperformed the classical
HMM approach. Finally, Toselli et al. [15] presented a line-
based keyword spotting method on the basis of frame-level
word posterior probabilities. KWS conﬁdence scores are

obtained by word graphs produced by a full-ﬂedged statisti-
cal HTR system, based on stochastic optical character mod-
els, as well as probabilistic lexicon and language models.
Their system achieved very promising results requiring less
time for indexing when compared to the system of Frinken
et al., which is still the best in terms of performance.

The rest of this paper is organized as follows. We present
an overview of the proposed system and outline its main
contributions in Section 2. Sections 3, 4 and 5 describe
in detail the main components of the proposed system,
i.e. Character Width Normalization, Feature Extraction and
Common Space Encoder. In Section 6, the proposed line
matching procedure is presented, while experimental results
on KWS trials are reported in Section 7 highlighting the ef-
fectiveness of the proposed system. Finally, conclusions are
drawn in Section 8.

2. System Overview and Contribution

In this work, we present a novel keyword spotting
method that is based on deep feature extraction and fast
query to text line matching. The key idea of the proposed
method is that the features extracted from the convolutional
output of a PHOC estimation network, trained on word im-
ages, should be similar in the case when the input is a line
image or even a document image. We should stress that the
convolutional layers are not constrained by the size of the
input image. As a result, the keyword search procedure can
be addressed as a feature matching procedure, 1-d search
when the input is a line image or 2-d when the input is a
document image (segmentation-free approach). Based on
the success of line segmentation methods [6], we assume
line images as input since the matching procedure is signif-
icant simpliﬁed (1-d search).

Our processing pipeline consists of three components,
all modeled as neural networks: (a) the Character Width
Normalization. With this component we can normalize in-
puts, rendering horizontal zones of deep features consum-
merate to one another, and enabling fast query matching,
since instances of the same word have similar width at the
normalized line images. (b) the Feature extraction. This
component is responsible for producing discriminative fea-
tures for the query as well as the text lines of the document
collection. Deep features are deﬁned as the activations of
an intermediate layer, produced when feeding an image to
the network. Such features have proven to be robust on
training-test content disparity; for example they have been
successfully applied to cross-language KWS tasks without
the need for model reﬁnement [10]. (c) the Common Space
Encoder. This component enables the QbS option, i.e. us-
ing a string as a query instead of a word image.

After having processed the query and the text lines, a
step which is preformed ofﬂine, the query can be compared
against the set of text lines on the ﬂy using a fast matching

12659

Figure 1. The proposed KWS system overview. Word image and string attribute inputs are encoded into a common space, suitable for
performing an efﬁcient line matching approach on the sequence of convolutional features representing the line image. xq is the ﬁnal
feature vector and nq is the number of line segments extracted from the query.

procedure. Matching of a speciﬁc query with a single text
line can be accomplished very efﬁciently due to two factors:
(a) matched features are extracted by max-pooling over a
set number of horizontal zones (b) the size of the pooling
ﬁlter is ﬁxed given the word length of the query, due to the
Character Width Normalization component. Matched to-
kens are then sorted and returned to the user. The proposed
model processing pipeline together with the query to text
line matching can be reviewed in Fig. 1.

One interesting aspect of this work is that, all the afore-
mentioned components focus on the simplest word-level
image representation and collaborate in order to efﬁciently
address the complex problem of line-level KWS.

3. Character Width Normalization

The existence of different writing styles as well as dif-
ferent scales leads to considerable image width differences,
even for the same word. This variance, closely related to
scale, complicates the line matching procedure, since we
cannot pre-deﬁne the space that a speciﬁc word, the query
word, occupies. Therefore, in order to build an efﬁcient
KWS system, we propose an image normalization scheme,
where different instances of the same word have the same
width. Key step towards such a normalization is the estima-
tion of the average character width which is performed by a
convolution neural network since it can be easily correlated
with visual features (e.g. curvature of strokes).

The normalization step should be able to estimate the
character width irrespective of the size of the input image
(word or line). One simple and efﬁcient way to perform

the width estimation on both categories, word and line, is
to extract a set of ﬁxed-size patches from the images. The
patches are randomly sampled from the image while their
number is proportional to the size of the input image. The
width estimation network fw(), expects a ﬁxed-sized image
patch as input and provides an estimation of the character
width for the speciﬁc patch. As the task at hand is a re-
gression problem, the network fw is trained using the mean
squared error (MSE) loss function.

Even though the problem was clearly stated, the required
labels for training the CNN, i.e. the character widths corre-
sponding to image patches, are not known in advance. We
follow a simple procedure for the assignment of the average
character width (label) to each image patch. Given a word
image and its transcription, the expected average character
width (label) of every patch generated by this word image
is the value w/nc, where w is the image width and nc is the
number of characters of the word’s transcription. Albeit the
generated regression labels are not accurate, they sufﬁce for
this regression task since a few pixels difference will not af-
fect the ﬁnal result. In fact, the trained network provides an
average loss of 3 pixels at test set, which is acceptable for
the purpose of the normalization task.

Given an image I, a set of patches Pi, i = 1, ..., np
is generated and each patch has its own width estimation
wi = fw(Pi).
It is possible that the system produces
erroneous character width estimations, especially for the
case of patches not containing representative information
(e.g. patch which mostly contains blank space). This is-
sue is addressed by the assignment of the median value

12660

Line Feature Extraction3-d feature map:Line Feature Representation:Convolutional LayersFully Connected LayersPHOC EstimatorMax PoolingWidth Estimatorestimated character widthPre-defined character widthQbE:QbS:(𝑥𝑞, 𝑛𝑞)QbEFeature ExtractionQbSFeature ExtractionsecretMatchingLine Scoreofflinewc = median({wi}) (more likely to correspond to the av-
erage character width) as the estimation for the whole im-
age.

After estimating the average character width, we normal-
ize the initial image to a ﬁxed average character width wref
(deﬁned by the user). This normalization corresponds to a
simple image resizing, for which the width is rescaled by a
factor wref /wc. The normalization process is depicted at
Figure 1.

4. Feature Extraction

A neural network, trained to learn a semantic prediction
task can be used to easily extract so-called deep features
[17]. We use deep features to describe both text lines and
queries since such features are known to outperform tradi-
tional feature extraction methods on various vision tasks.
These features are extracted as activations of a neural net-
work with a feed-forward architecture, tuned to a semantic
attribute prediction task.

In particular, the task is predicting word-level attributes
that correspond to the existence or absence of a unigram or
bigram at a speciﬁc section of the word. To this end, we
adapt on the PHOCNet architecture inspired by the VGG
architecture [11, 12]. The input is a word image of arbitrary
size, processed by a convolutional backbone that leads to a
fully-connected head topped by sigmoid outputs. Each sig-
moid output predicts a word semantic attribute, e.g. whether
the letter ’b’ exists on the ﬁrst half of the word, or whether
the bigram ’st’ exists on the second third of the word, and
so on.

The proposed PHOC estimator network bears two dif-
ferences w.r.t. to the PHOCNet architecture. The ﬁrst novel
point is the application of adaptive pooling on a speciﬁc re-
gion of interest (ROI), which corresponds to a part of the
initial input image. We perform training using word images
that contain information from neighboring words, i.e. not
a tight crop of the word image as the word segmentation
ground truth indicates (see Figure 2). This training strategy
tunes the neural network in such a way that it can efﬁciently
generalize to inputs corresponding to entire line images. In
addition, it enables the creation of more robust ﬁlters, not
affected by the existence of neighboring image elements.
To this end, a bounding box with the region of interest is
also deﬁned for the pooling operation as it is shown in Fig-
ure 2. Hence, the training input consists of pairs of images
and bounding boxes.

The second novel point

is related with the pooling
scheme used to create a ﬁxed-size bridge with the fully-
connected head. We employ a horizontal zoning-based
scheme, dividing the text-line or word input into ﬁxed-size
horizontal segments. This scheme follows the rationale of
zoning techniques that have been widely used for keyword
spotting [5]. The use of pyramidal pooling is also avoided,

contrary to [11, 12]. We further simplify the above men-
tioned pooling scheme by completely ignoring the segmen-
tation scheme over the convolutional map and instead, apply
max-pooling over the entire map. This procedure will result
to the creation of a feature vector whose size will be equal
to the depth of the last convolutional layer. Furthermore,
the application of a max-pooling operation over the entire
convolutional map drastically reduces the size of the gener-
ated feature vector and therefore the number of parameters
of the upcoming fully connected layer. The signiﬁcance of
this observation in the context of the proposed system, is
that each word is represented using a collection of differ-
ent ﬁlter responses instead of a sequence of feature vectors
(zoning) or a set of sequences (pyramidal). The creation
of a single feature vector simpliﬁes the forthcoming match-
ing procedure which can be accomplished using a nearest
neighbor algorithm. The proposed pooling strategy hence
results to the creation of a simpliﬁed and compact feature
descriptor, suitable for fast matching.

Figure 2. The PHOC estimation CNN consists of three distinct
parts: 1) Convolutional 2) ROI Pooling and 3) Fully connected.
Training is performed on an extended image containing a word
instance along with the ROI bounding box of the word.

Given that fc is the function consisting of the convolu-
tional layers, fp is the pooling operation over the ROI and
ff c is the fully connected part which results to the PHOC
description, the PHOC estimation process can be formu-
lated as follows:

x = fp(fc(I), b),

y = ff c(x)

(1)

where I is the input image, b is the ROI bounding box, x ∈
Rnd is the extracted deep feature vector and y ∈ Rnphoc is
the estimated PHOC. Note that nd is an important hyper-
parameter of the PHOC estimation network, since it deﬁnes
the dimensionality of the extracted deep features.

5. Common Space Encoder

The PHOC estimation network, after discarding the fully
connected layers, provides discriminative features, ideal for
a line-based approach. Nevertheless, the discarded layers
create the connection between the convolutional features
and the PHOC representation which is mandatory for the

12661

Convolutional partFully Connected partInput imageROI PoolingROIConvolutional FeaturesFixed-sized feature vectorPyramidal poolingROIQbS scenario. Therefore, an inverse function is required
for the QbS scenario, i.e. a function that accepts a PHOC
embedding (input) and produces a feature vector which is
similar to the feature vector generated by the PHOC’s esti-
mator convolutional part (output).

One problem that may arise with this function is related
with the fact that each input (PHOC representation) corre-
sponds to a variety of outputs (word images of the same
word written by different writers) and therefore the prob-
lem is ill-posed, since the function has to describe an one-
to-many relation. This problem can be effectively solved by
choosing to encode the PHOC embedding y as well as the
convolutional feature vector x into a common space of ﬁxed
dimensionality ne.

The encoding process is performed by two separate en-
coding functions, namely ep (encodes the PHOC embed-
dings) and ef (encodes the convolutional features), which
are modeled by neural networks consisting of fully con-
nected layers. Therefore, given a pair (x ∈ Rnd ,y∈ Rnphoc )
the resulting ep(y) ∈ Rne and ef (x) ∈ Rne must be identi-
cal in the encoding space.

A critical aspect concerning the training of the encod-
ing functions is the creation of an appropriate loss function.
The obvious goal is to minimize the distance between ep(y)
and ef (x), but such an approach may lead to trivial solu-
tions (e.g. projection to the origin point). To address this
problem, the training is performed using pairs of different
words, i.e. (x1, y1) and (x2, y2), and the goal is the min-
imization of the encoding distance within the same word
while preserving an appropriate distance between different
words.

Given a distance d(·, ·), the inner distance i and cross

distance c are deﬁned as:

i1 = d(ef (x1), ep(y1)),
c1 = d(ef (x1), ef (x2)),
c3 = d(ep(y1), ef (x2)),

i2 = d(ef (x2), ep(y2))
c2 = d(ef (x1), ep(y2))
c4 = d(ep(y1), ep(y2))

i = (i1 + i2)/2

c = (c1 + c2 + c3 + c4)/4

(2)

(3)

The obvious choice is to minimize the inner distance, while
at the same time maximize the cross distance, i.e. the loss
would be L = i − c. However, our goal is to preserve
a cross distance at least as big as the reference distance
m = d(y1, y2) in order to preserve the correlation of the
initial PHOC representations. We call this reference dis-
tance as margin m. The margin term is crucial since the data
distribution on the new space (with the help of the margin
term) is expected to be similar to the data distribution on
the PHOC space. In fact, the cross distance should be al-
ways greater than the inner distance by the aforementioned
margin (c > i + m) in order to ensure separable classes of

words. Therefore, the loss function is formulated as:

L = i + max(0, i − c + m)

(4)

Implementation details: 1) The ep consists of 4 fully con-
nected layers with ReLU non-linearities, while ef is a linear
transformation. 2) The dimension of the encoding space is
set to ne = 128. 3) The cosine distance is selected as the
distance function d. 4) The pairs (x, y) are selected such as
their PHOC representations are not far apart, i.e. d(y1, y2)
is less than a threshold t. The latter assists convergence,
since unconstrained selection of pairs may generate dissim-
ilar pairs which are easy to separate.

6. Line Image Features and Matching

Following the deﬁnitions of the subsystems and their re-
spective networks, this section contains a detailed descrip-
tion on how the aforementioned systems are cooperating in
order to perform line based KWS.

We can distinguish two main steps: 1) Extract compact
features from a line image and store them. This step is per-
formed off-line in a KWS application. 2) Given a query (ei-
ther image or string), extract appropriate features and com-
pute a matching score between the speciﬁc keyword and the
line (see Fig. 1).

6.1. Line Features Extraction

The extraction of line features consists of three steps pre-

sented in Figure 1, as part of the overall system:

1. Resize the input line image (hi × wi) with respect to
a pre-deﬁned average character width wref using the
Character Width Normalization component (see Sec-
tion 3). The size of the resized image is h × w.

2. Apply the convolutional part of the PHOC estimator
network (fc, see section 4) to the resized image pro-
duced in step 1. The output corresponds to a 3-d fea-
ture map, for which the height and width is propor-
tional to the height and width of the resized line image
(h/4 × w/4 - the two max pooling layers of kernel and
stride 2 of fc downsample the image by 4) while its
depth corresponds to the number of ﬁlters of the ﬁnal
convolution layer (nd = 512).

3. Organize the aforementioned 3-d map (h/4 × w/4 ×
nd) of visual features into a sequence of feature vec-
tors (nl × nd). The organization of the 3-d map into
a sequence of feature vectors makes possible the efﬁ-
cient matching between a query word and a line im-
age. The 3-d map is reduced by making use of local
max-pooling performed on non-overlapping segments
of size h/4 × wstep, where wstep = wref /4nquant.
As a result, each line image is described by a short

12662

sequence of feature vectors reducing the storage cost
as well as the matching time. This step is depicted at
Figure 3.

Figure 3. Extraction of line features from the 3-d convolutional
feature map. Each line is represented by a sequence of nl feature
vectors of size nd. Each feature vector occupies wref /nquant
width in the input line image.

6.2. QbE Feature Extraction

Given a query image Iq, we apply the following steps:

1. Estimate the average character width of the image and
use it to resize the image accordingly (the resized im-
age is denoted as I ′

q and its size is hq × wq).

2. Calculate the number of line segments nq by divid-
ing the resized image’s width by the step width wstep
which is deﬁned on the line features extraction section.

3. Apply the convolutional part of the PHOC estimator
network (fc, see section 4). This leads to the creation
of a 3-d feature map whose size is hq/4 × wq/4 × nd.
Finally, extract a single feature vector by performing
max-pooling for each channel (i.e.
taking the maxi-
mum value over hq/4 × wq/4 values). It should be
noted that during this step no ROI is deﬁned.

4. Project the produced feature vector into the common
encoding space using the encoding function ef (see
Section 5).

→ The ﬁnal feature vector xq is generated as: xq =

ef (fp(fc(I ′

q)))

6.3. QbS Feature Extraction

Given a query string sq, we apply the following steps:

1. Calculate the number of line segments nq, by multiply-
ing the number of query’s characters with the character
quantization parameter nquant (each character should
occupy nquant line segments).

2. Compute the PHOC embedding of the query string sq.

3. Encode the PHOC embedding into the common encod-
ing space using the corresponding neural network ep
(see Section 5).

→ The ﬁnal feature vector xq is generated as: xq =

ep(P HOC(sq)).

6.4. Line Matching

It is assumed that line features are represented as a se-
quence of vectors {xi}, xi ∈ Rnd , i = 1, . . . , nl. The
query (either image or string) is represented as a single fea-
ture vector xq ∈ Rne . A necessary parameter which should
be known beforehand in order to perform the matching is
the number of line segments nq corresponding to the query
width, i.e. how many line segments make up the query. The
number nq is calculated during the query feature extraction
step (see Sections 6.2 and 6.3).

For each line, a set of scores is calculated using the co-
sine distance. In more detail, the scoring is performed using
the cosine distance between the query feature vector and a
feature vector created using a max-pooling procedure over
consecutive nq line features, i.e.:

¯xi[k] = max

j=0,...,nq −1

xi+j[k], k = 1, . . . , nd − nq + 1 (5)

d[i] = 1 − cos(ef (¯xi), xq),

¯xi ∈ Rnd , xq ∈ Rne

(6)

The merge of consecutive nq line features into a single fea-
ture vector, using a sliding window approach of max opera-
tions, is visualized at Figure 4. Since a set of feature vectors
xi can be extracted from a line image (the total number is
equal to nl − nq + 1), the ﬁnal score is the minimum among
all scores calculated using the set of feature vectors xi:

line score =

min

i=1,...,nl−nq +1

d[i]

(7)

For the matching step it is imperative for the two feature
vectors to have the same dimensions. To this end, each fea-
ture vector xi is projected to the common encoding space
using the network ef . This task has been already performed
for the query feature vector xq during the feature extraction
step. The simplicity of the matching procedure described
above, is a considerable advantage of the proposed method,
since it reduces the retrieval time.

7. Experimental Results

7.1. Experimental Setup

The performance of the proposed system was evaluated
on the well-known IAM dataset1. It consists of 1539 hand-

12663

𝑤𝑠𝑡𝑒𝑝𝑥𝑖1𝑥𝑖𝑗𝑥𝑖𝑛𝑑……maxj-thmaph/4w/4𝑛𝑑Approaches QbE-PHOC QbE-cfeat QbS-PHOC

TPP
Zoning
Entire

83.98
83.66
83.27

82.15
82.24
81.73

93.01
92.47
91.11

Table 1. MAP (%) performance evaluation of different pooling
strategies on the word-segmented IAM dataset.

ists in the aforementioned query lists and therefore, for our
experiments, we adopt the same characters set (68 unique
characters) and create 5-level PHOC embeddings with as
many unigrams (1020 dimensional vectors).

The performance of the KWS methods is recorded in
terms of the Mean Average Precision (MAP) since it is a
retrieval problem. The retrieval list consists of the lines in
the test set sorted according to their matching score with the
query. A line that contains the requested query is considered
as a hit.

7.2. Pooling strategies on PHOC estimation

One major difference of the proposed system with re-
spect to PHOCNet is the usage of a PHOC estimation net-
work with only one segment at the adaptive pooling layer,
between the convolutional and the fully connected part. We
experimented on the impact of the pooling operation to the
system performance by evaluating three different strategies
at the adaptive pooling layer: 1) Temporal Pyramidal Pool-
ing - TPP (initial PHOCNet architecture [12]) 2) Zoning
into 5 segments (without the pyramidal scheme) 3) Using
max-pooling on the entire convolutional output (1 segment).
These strategies are evaluated on the segmented words
as in [12] and the query list is extracted similarly to [1]. It
should be stressed that the results are not directly compa-
rable with the state-of-the-art word-level KWS techniques
due to the fact that the characters set used in this work is
signiﬁcantly different (see Section 7.1). The results are pre-
sented at Table 1, where we distinguish the QbE scenario,
both on PHOC level (QbE-PHOC) and on deep feature level
(QbE-cfeat) and the QbS scenario on PHOC level (QbS-
PHOC). As the experimental results indicate, the pooling
strategy does not play signiﬁcant role since the performance
of the system is similar regardless the pooling scheme or
the feature level. As a result, using max-pooling on the en-
tire convolutional output simpliﬁes the subsequent match-
ing procedure while barely affecting the performance of the
overall system.

7.3. Encoder Performance

A major contribution of this work is that it enables both
QbS and QbE scenarios on deep features level, using an
encoding neural network which projects convolutional fea-
tures as well as PHOC embeddings into a common encoding

12664

Figure 4. Line features transformation according to query size nq.
This step is essential in order to have comparable query and line
features.

written document images of modern English written by 657
different writers and it has been partitioned into writer-
independent training (6161 lines), validation (920 lines) and
test sets (929 lines).

In order to be comparable with the results reported in the
bibliography, we follow the two most widely used setups
of the IAM dataset. IAMDB1: 882 queries selected as in
[3] using all non-stop words that appear at least once in the
training set as well as the test set. IAMDB2: All non-stop
words among the 4000 most frequent words that also occur
in the training set are selected as queries as in [4], resulting
in 3421 queries in total.

In addition, for measuring the performance of the pro-
posed system we consider two possible scenarios as in [3].
In the ﬁrst scenario (local), a local threshold is used for
each keyword separately. Concerning the second scenario
(global), a global threshold is used that is independent of
the keyword. For a vocabulary of common keywords, local
thresholds can be optimized at training stage. On the other
hand, for arbitrary out-of-vocabulary keywords, a global
threshold has to be applied.

Another critical aspect for KWS techniques is the deﬁ-
nition of the character set. Word-based approaches assume
only lowercase and numeric characters [1]. On the con-
trary, line-based approaches [4] assume a wider variety of
possible characters, including capital letters as well as some
special characters (e.g. ‘/’, ‘-’ e.t.c). This variety also ex-

1http://www.fki.inf.unibe.ch/databases/

iam-handwriting-database

∙∙∙𝑛𝑙∙∙∙𝑛𝑞max∙∙∙𝑛𝑑𝑛𝑙−𝑛𝑞+1𝑛𝑑methods

local

global

QbS with ROI
QbS without ROI

QbE with ROI
QbE without ROI

88.73
87.13

84.25
83.01

83.15
79.14

73.16
71.58

Table 2. Impact of the ROI variation on the KWS system perfor-
mance. MAP (%) results are reported at the line-level IAMDB1
setup (nquant = 3).

space. Therefore, we can evaluate both QbE and QbS on the
new encoding space. For this experiment we used the pool-
ing strategy over the entire convolutional map. The word
level KWS with encoder component results to: 81.36% and
89.43% MAP for QbE and QbS respectively. These results
are similar to the KWS scenario without the encoder even
though the feature dimension is compressed (from 1020 of
PHOC and 512 of convolutional features to 128 of the en-
coded features), while feature-based QbS is enabled.

7.4. Line level KWS

Having conﬁrmed the effectiveness of the systems com-
ponents we proceed to the evaluation of the whole proposed
KWS system using the IAMDB1 and IAMDB2 setups. It is
worth to note that, even if QbE scenario is not considered
on line-level by existing methods, we evaluated it using the
query list of IAMDB1 setup since the corresponding query
images can be detected at the training set. If there are more
than one instance for a query string the query image is se-
lected randomly (the reported QbE results are the average
MAP of 10 runs). However, the IAMDB2 setup cannot be
used for this case since query strings do not necessarily exist
on the training set.

First, we evaluate the impact of the ROI variation of
feature extraction network as shown in Table 2 using the
IAMDB1 setup. The results indicate that the usage of a
training scheme with expanded word images, as done in the
ROI variation, assist the network to generate features that
are more similar to the extracted line features.

The experimental results of the proposed method along
with several state-of-the-art methods, for both IAMDB1
and IAMDB2 setups, are reported at Table 3. The pro-
posed system, concerning the QbS scenario, signiﬁcantly
outperforms the majority of the line-based KWS methods
reported in the literature and it achieves comparable per-
formance compared to the best system [4]. Morever, the
experimental results indicate the validity and the effective-
ness of the proposed method, which enables both QbS and
QbE scenarios, since the QbE scenario achieves compara-
ble results with the QbS scenario (especially for the local
evaluation scenario). The gap in performance between QbS
and QbE cases can be attributed to the fact that the QbE ap-

IAMDB1

IAMDB2

methods

local

global

global

Fisher et al. [3]
Fisher et al. [2]
Toselli et al. [13]
Toselli et al. [15]
Frinken et al. [4]
proposed QbS

68.92

-
-
-
-

47.75
55.05

-
-
-

88.73

83.15

-

36.00
61.03
72.00
76.00
75.31

proposed QbE

84.25

73.16

-

Table 3. MAP (%) evaluation on the line-level IAMDB1 and
IAMDB2 setups (nquant = 3).

proach may generates a feature vector that lies on the mar-
gin of the word cluster (deﬁned by the feature vectors of all
the instances of the same word), compared to QbS features
that are unique per word and by default are in the center of
the word cluster.

Another important observation is that even though we
cannot straightforwardly evaluate the Character Width Nor-
malization component, since we do not have accurate la-
bels, we can conclude that it performs sufﬁciently well since
the full pipeline provides noteworthy results.

Finally, we should stress that the proposed method can
be used for real-time applications due to its storage and time
efﬁciency. For clarity, we report some indicative storage
and time requirements on the IAM dataset: the line image
features, which computed ofﬂine, require 260KB storage
(without any quantization), while it takes around 0.28msec
to compare a query to a line.

8. Conclusions

In this work we have proposed a keyword spotting sys-
tem that is capable of performing QbE and QbS KWS un-
der a uniﬁed framework. The proposed system is built on
the synergy of three distinct neural networks, respectively
trained to normalize, encode image content into features,
and construct a common text and image space. Even though
the training is performed on word level, the comparative ex-
perimental results indicate that the proposed system is on
par with state-of-the-art HTR-based QbS KWS methods,
while it also enables successfully the QbE alternative. As
future work, we envisage the generalization of the proposed
framework for the segmentation-free scenario, i.e. not de-
pending on the existence of document image segmentation
at any level, but instead applying the method to the entire
(unsegmented) document image.

Acknowledgments: This work has been supported by the
EU project READ (Horizon-2020 programme, grant Ref.
674943).

12665

[14] A. H. Toselli and E. Vidal. Fast hmm-ﬁller approach for
key word spotting in handwritten documents. In 12th Inter-
national Conference on Document Analysis and Recognition
(ICDAR), pages 501–505. IEEE, 2013. 2

[15] A. H. Toselli, E. Vidal, V. Romero, and V. Frinken. Hmm
word graph based keyword spotting in handwritten document
images. Information Sciences, 370:497–518, 2016. 2, 8

[16] T. Wilkinson and A. Brun. Semantic and verbatim word spot-
ting using deep neural networks. In 15th International Con-
ference on Frontiers in Handwriting Recognition (ICFHR),
pages 307–312. IEEE, 2016. 2

[17] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. arXiv preprint, 2018. 4

References

[1] J. Almaz´an, A. Gordo, A. Forn´es, and E. Valveny. Word
spotting and recognition with embedded attributes.
IEEE
transactions on pattern analysis and machine intelligence,
36(12):2552–2566, 2014. 1, 2, 7

[2] A. Fischer, V. Frinken, H. Bunke, and C. Suen.

Improv-
ing hmmm-based keyword spotting with character language
models.
In 12th International Conference on Document
Analysis and Recognition (ICDAR), pages 506–510, 2013.
2, 8

[3] A. Fischer, A. Keller, V. Frinken, and H. Bunke. Lexicon-
free handwritten word spotting using character hmms. Pat-
tern Recognition Letters, 33(7):934–942, 2012. 2, 7, 8

[4] V. Frinken, A. Fischer, R. Manmatha, and H. Bunke. A novel
word spotting method based on recurrent neural networks.
IEEE transactions on pattern analysis and machine intelli-
gence, 34(2):211–224, 2012. 2, 7, 8

[5] Angelos P. Giotis, Giorgos Sﬁkas, Basilis Gatos, and
Christophoros Nikou. A survey of document image word
spotting techniques. Pattern Recognition, 68:310 – 332,
2017. 1, 4

[6] T. Gruuening, G. Leifert, T. Strauss, and R. Labahn. A robust
and binarization-free approach for text line detection in his-
torical documents. In 2017 14th IAPR International Confer-
ence on Document Analysis and Recognition (ICDAR), vol-
ume 01, pages 236–241, Nov 2017. 2

[7] P. Krishnan, K. Dutta, and C. V. Jawahar. Deep feature em-
bedding for accurate recognition and retrieval of handwritten
text. In 15th International Conference on Frontiers in Hand-
writing Recognition (ICFHR), pages 289–294. IEEE, 2016.
2

[8] Praveen Krishnan and C. V. Jawahar. Hwnet v2: An efﬁ-
cient word image representation for handwritten documents.
CoRR, abs/1802.06194, 2018. 2

[9] U.-V. Marti and H. Bunke. The iam-database: an english
sentence database for ofﬂine handwriting recognition.
In-
ternational Journal on Document Analysis and Recognition,
5(1):39–46, Nov 2002. 1

[10] George Retsinas, Giorgos Sﬁkas, and Basilis Gatos. Trans-
In Multidisci-
ferable deep features for keyword spotting.
plinary Digital Publishing Institute Proceedings, volume 2,
page 89, 2018. 2

[11] S. Sudholt and G. A. Fink. Phocnet: A deep convolutional
neural network for word spotting in handwritten documents.
In 15th International Conference on Frontiers in Handwrit-
ing Recognition (ICFHR), pages 277–282. IEEE, 2016. 2,
4

[12] S. Sudholt and G. A. Fink. Evaluating word string embed-
dings and loss functions for cnn-based word spotting. In 14th
International Conference on Document Analysis and Recog-
nition (ICDAR), pages 493–498. IEEE, 2017. 2, 4, 7

[13] Alejandro H´ector Toselli, Joan Puigcerver, and Enrique Vi-
dal. Two methods to improve conﬁdence scores for lexicon-
free word spotting in handwritten text.
In 15th Interna-
tional Conference on Frontiers in Handwriting Recognition
(ICFHR), pages 349–354. IEEE, 2016. 8

12666

