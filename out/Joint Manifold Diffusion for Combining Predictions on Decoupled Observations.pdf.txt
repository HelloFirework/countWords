Joint Manifold Diffusion for Combining Predictions on Decoupled Observations

Kwang In Kim

UNIST

Hyung Jin Chang

University of Birmingham

Abstract

We present a new predictor combination algorithm that
improves a given task predictor based on potentially relevant
reference predictors. Existing approaches are limited in
that, to discover the underlying task dependence, they either
require known parametric forms of all predictors or access
to a single ﬁxed dataset on which all predictors are jointly
evaluated. To overcome these limitations, we design a new
non-parametric task dependence estimation procedure that
automatically aligns evaluations of heterogeneous predictors
across disjoint feature sets. Our algorithm is instantiated
as a robust manifold diffusion process that jointly reﬁnes
the estimated predictor alignments and the corresponding
task dependence. We apply this algorithm to the relative
attributes ranking problem and demonstrate that it not only
broadens the application range of predictor combination
approaches but also outperforms existing methods even when
applied to classical predictor combination settings.

1. Introduction

When the performance of an estimated predictor is not
adequate for the task at hand, e.g. due to limited training
examples, we might beneﬁt from the knowledge gained from
related tasks. Multi-task learning (MTL) [1, 12, 16, 19]
explores this possibility by solving multiple problems
simultaneously, and so capturing and beneﬁting from the
potential task dependence. The success of MTL in many
visual learning problems has demonstrated such task depen-
dence [1, 12, 23, 19, 10]. In most existing MTL algorithms,
task dependence is modeled through the latent structures on
the parameter spaces of the corresponding task predictors.
For instance, Evgeniou and Pontil’s algorithm [6] penalizes
pair-wise parameter deviations of task predictors. Since
it is unlikely that all tasks exhibit known task dependence
structure, MTL algorithms attempt to automatically discover
the underlying dependence and identify outliers, by e.g.
enforcing sparsity and/or introducing low-rank constraints
on the aggregated task parameter matrices [1, 8] or by
explicitly performing clustering of tasks [25, 18].

A major limitation of these traditional MTL approaches
is that they require all task predictors to share the same
predictive model or even the same parameter space, making

them difﬁcult to apply to heterogeneous predictors, e.g. com-
bining deep neural networks and support vector machines.
However, the best predictor forms often depend on the in-
dividual tasks of interest. Further, existing MTL approaches
are designed to train multiple predictors simultaneously,
and so they cannot be directly applied to train a new task
predictor given previously-trained reference predictors, e.g.
combining pre-trained or pre-compiled predictor libraries
without access to the corresponding task training data.

test

Recently, Kim et al. [10] proposed a non-parametric pre-
dictor combination approach where the predictor evaluations
made at sampled data points are improved by combining
them with reference predictions at
time without
requiring simultaneous training. This enables us to combine
predictors with different or even unknown parametric forms.
However, the application scope of this approach is limited
in its own way, as it requires a large set of data points on
which all predictors are jointly evaluated.
In practical
applications, different predictions can be constructed based
on the respective feature representations tailored for speciﬁc
tasks of interest, and often these features are available by
themselves as separate databases without having explicit
references to the corresponding source data (e.g. images).

In this paper, we propose a new algorithm to avoid the
limitations of previous predictor combination approaches,
thereby broadening the application spectrum of the non-
parametric predictor combination approach [10]. Building
on their test-time combination approach, our algorithm
improves a task predictor based on a set of reference
predictors. However, unlike their approach, we do not
require that all predictors are available for evaluation on
a single ﬁxed set. Our algorithm takes as input decoupled
predictor evaluations and automatically aligns these
predictions to discover the underlying task dependence. As
the initial estimates of the alignments and the corresponding
task dependence might be noisy, we denoise them jointly via
a manifold diffusion process. The new algorithm combines
the beneﬁts of classical parametric MTL approaches and
recent test-time combination algorithms, and facilitates
combination applications where multiple heterogeneous
predictors are constructed from disjoint feature sets. We
apply our algorithm to the relative attributes ranking prob-
lem, and extend the application over previous approaches.
Furthermore, evaluated on seven challenging datasets,
our approach demonstrates that even when applied in the

17549

restricted settings of traditional approaches, it signiﬁcantly
improves both accuracy and time efﬁciency.

Relative attributes ranking: Relative attributes rank-
ing [17, 11] refers to the problem of inferring a linear
ordering of database images based on the strengths of
attribute present in each entry. This problem differs from
binary attribute classiﬁcation where the goal is to predict
the presence or absence of an attribute. Instead, relative
attributes ranking focuses on attributes where such clear
binary classiﬁcations cannot be obtained, e.g. a shoe A
can be ‘more formal’ than B but it could still appear ‘less
formal’ than C. This problem is also different from classical
data-retrieval type ranking applications where the goal is
to identify database entries that match a given query.

This goal can be achieved by learning a rank function
f based on user-provided rank labels: Given a set of data
points X ={x1,...,xn}⊂X , rank learning aims to construct
a function f :X → R that agrees with the observed pair-wise
rank labels R ={(i(1),j(1)),...,(i(l),j(l))}⊂ X×X where
(i, j) ∈ R implies that the rank of xi is higher than xj :
f (xi) > f (xj). For instance, Parikh and Grauman’s original
Relative Attributes algorithm learns a rank support vector
machine (RankSVM) [17] while Yang et al. extended it into
Deep Relative Attributes [24] using neural networks.

2. Joint manifold diffusion for test-time predic-

tor combination

Our algorithm improves a given task predictor based
on a set of reference predictors. As it is unknown a priori
which reference predictors are relevant, our algorithm
automatically identiﬁes and exploits the relevant references.
Existing approaches are limited in that they either require
known and shared parametric forms for all task predictors
(e.g. in parametric MTLs) or evaluating multiple predictors
on a single ﬁxed dataset (in test-time predictor combination
approach [10]). We bypass these limitations and allow the
combination of multiple heterogeneous predictors by 1) a
new non-parametric measure of task dependence (Sec. 2.1)
and 2) a robust joint diffusion process that constructs bridge
variables coupling the predictors of disjoint data instances
(Sec. 2.2).

Problem deﬁnition: Suppose that we are given a rank
predictor function f constructed as an estimate of the
unknown ground-truth ranker (or task). Our goal is to
reﬁne f based on a set of m reference predictors {gk}m
k=1.
As there is no guarantee that the reference predictors are
relevant to the ground-truth task or its estimate f , our
algorithm automatically identiﬁes any relevant references.

Adopting Kim et al.’s predictor combination frame-
work [10], we regard f as a noisy observation of the ground-
truth. Our algorithm denoises f by embedding {f,gk}m
k=1
into a predictor manifold M and performing manifold
denoising induced by the diffusion process therein. The do-
k=1 do not have

mains of the predictor f and references {gk}m

to be identical. Instead, we assume that they are connected

the corresponding data representation per task can be deﬁned

via an underlying data space eX equipped with a probability
distribution P . An example of eX is the space of images while
via the respective feature extractors ek : eX →X k on which

the predictors are deﬁned: f ∈ C∞(X ) and gk ∈ C∞(X k).1
Therefore, we regard a predictor gk being deﬁned on
its own feature domain X k or by combining it with the

corresponding feature extractor, as a function on the shared

data domain eX : ˜gk := g·ek ∈ C∞(eX ). As discussed shortly,

this decomposition of feature representations and predictors
facilitates applications where the main predictor f is
combined with multiple heterogeneous reference predictors.

2.1. Denoising over predictor manifold

probability distribution P , our predictor manifold M is
given as an equivalence class of square-integrable functions

Assuming that the input space eX is provided with a
L2(eX ,P ): Each function ˜g∈ L2(eX ,P ) is projected onto M

by centering and scale-normalization:

ProjM [˜g] :=

(1)

˜g−R ˜gdP

k˜g−R ˜gdPkL2( eX ,P )

.

This manifold construction facilitates scale and shift-
invariant comparisons of ranking functions:
In ranking
applications, e.g. scaling of a ranker g(·) by a constant c,
cg(·) should not alter the nature of rankings it induces. Simi-
larly, a constant offset g(·)+c of a ranker g(·) should lead to
the same ranking results. For problems where the absolute
scales are important, e.g. regression, inverse normalization
can be performed after denoising. For brevity of notation,
we omit the projection symbol ProjM and use ˜g to denote
an element of M . The Riemannian2 metric on this Hilbert
sphere M can be induced from the ambient L2 metric:

h˜gk,˜gliL2( eX ,P ) =Z ˜gk ˜gldP,

(2)

which uniquely identiﬁes a Laplace-Beltrami operator
inducing a diffusion process on M .

if the parametric forms of the predictors { ˜f , ˜gk}m

It might be possible to evaluate the metric directly (Eq. 2)
k=1 are
known. When their parametric forms are unknown or for
general non-parametric predictors, we instead approximate

the metric h˜gk, ˜gliL2( eX ,P ) based on their evaluations on a

sample eX ={˜x1,...,˜xn}⊂ eX :

(f )⊤gk,

hf ,gki =

1
n

f := ˜f| eX , gk := ˜gk| eX .

(3)

Manifold
evaluations
cess

denoising: Using
(Eq. 3),

sample-based metric
the manifold denoising pro-
a

to iteratively solve

can be described as

1Here, C∞ is the space of smooth (inﬁnitely differentiable) functions.
2For L2( eX ,P ), we adopt the natural identiﬁcation of functions that

deviates on a set of measure zero.

7550

diffusion equation on a graph formed by matrix

it can also be rewritten as a generalized Rayleigh quotient

G = [f ⊤,(g1)⊤,...,(gm)⊤]⊤⊂ R(m+1)×n [22, 9, 10]:

∂G
∂t

=−δ∆G

(4)

with a diffusion coefﬁcient δ > 0 and the graph Laplacian
∆ constructed from G:

2 W D− 1
2 ,

∆ = I−D− 1
σ2 (cid:19),
[W ]kl = exp(cid:18)−hgk,gli2

(5)

where σ2 is a scale hyperparameter, and the diagonal matrix

D contains the row sums of W ([D]kk =Pl Wkl). When

each graph node gk corresponds to an i.i.d. Gaussian-noise
contaminated observation of an underlying clean manifold
point, this process tends to contract G towards M [9] and
therefore, as the diffusion proceeds, G(t) tends to recover
a smooth noise-free version of M .

To simulate the diffusion process, we discretize Eq. 4 in

time and obtain an implicit Euler update rule:

G(t+1)−G(t) =−δ∆(t)G(t+1).

(6)

Note the time-dependence of ∆ as it is constructed from the
variable G being evolved (Eq. 5).

2.2. Joint manifold diffusion

2.2.1

f -diffusion: Reﬁning the predictor f

As our goal is to reﬁne the main predictor f given references,
we hold the reference variables {gk}m
k=1 in G ﬁxed and only
update f during the diffusion (Eq. 6). In this case, the up-
dated solution f (t+1) at time t+1 (the ﬁrst row of G(t+1))
can be obtained as the maximizer p∗ of a score functional:3

O(p) =hp,f (t)i2

M +δ

mXk=1

W1khp,gki2
M ,

(7)

where we explicitly incorporate the normalization conditions
(scaling and centering) such that the solution stays on the
predictor manifold M :

ha,biM =

(Ca)⊤Cb
kCakkCbk

(8)

with C = I − 1

n 11⊤ and 1 = [1, ... , 1]⊤. The score O is
a smooth function of p, and it can be maximized using
any smooth optimization method. However, by deﬁning a
symmetric matrix Q = SS⊤ with
√δW11Cg1

√δW1mCgm

S =(cid:20) Cf (t)

kf (t)k

,

,...,

kCg1k

kCgmk

(cid:21),

(9)

3The implicit Euler step in Eq. 6 corresponds to a linear system whose
solution can be obtained by minimizing the corresponding quadratic energy
function; See [9] for details.

O(p) =

p⊤Qp
p⊤Cp

.

(10)

This reveals that the optimal solution p∗ can be obtained as
the eigenvector corresponding to the maximum eigenvalue
of the generalized eigenvalue equation Qp = λCp. For
general symmetric matrices Q and C, the computation for
ﬁnding this eigenvector is cubic complexity: O(n3) for n
data points, which quickly becomes infeasible as n grows. A
more efﬁcient approach can be taken by noting that for prac-
tical applications, the number of reference predictors m will
be much smaller than n and the matrix Q is constructed as a
weighted combination of outer products of centered vectors

(Eq. 9). Therefore, all eigenvectors {ek} corresponding to
non-zero eigenvalues of Q are also centered, i.e., ek = Cek
implying that they also constitute the eigenvectors of the
centering matrix C. This renders the generalized eigenvalue
problem at hand into a regular eigenvalue problem Qp = λp.
Finally, the maximum eigenvector of Q is obtained as the
maximum left-singular vector of S and hence the complexity
of this step reduces to O(m2n). As we maximize the
squared metric in Eq. 7, the optimizer p∗ of O can be
inversely correlated to the original rank predictions f (0).
Therefore, the ﬁnal updated solution f (t+1) is obtained by
multiplying the solution p∗ with sgn[−1hp∗,f (0)i].

f -diffusion step is motivated by
Discussion: Our
adaptively-weighted correction of f via robust local aver-
aging of the references {gk}. A key application challenge

is that we do not know which references, if any, are relevant.
Thus, our algorithm must automatically identify them. This
can be naturally addressed based on adaptive control of
the combination weights {W1k} exercised via the diffusion
process. Our algorithm controls the metric similarity
between the main predictor and the references weighted by
{[W ]1k}, which are increasing functions of the similarities
themselves (Eqs. 7-8). These weights provide the means
to disregard irrelevant references. The uniformity of the
weights is controlled by the hyperparameter σ2 (Eq. 5):
For large σ2, all references contribute equally, which might
include outliers. For small σ2
w, the single most relevant
reference inﬂuences the solution, which might neglect other
less relevant but still beneﬁcial references.

2.2.2 B-diffusion:

Combining predictions

from

decoupled observations

A major limitation of our initial predictor combination al-
gorithm is that it relies on a large number of predictor eval-
uations sampled from the joint distribution P (f,g1,...,gm),
i.e. the sample predictions {f ,gk}m
k=1 are obtained by jointly
evaluating the corresponding predictors { ˜f , ˜gk}m
k=1 on a
shared sample set eX ⊂ eX . However, in practical applications,

each predictor can be coupled with a feature representation

7551

tailored for an individual task of interest. Furthermore, often
these features are available by themselves, without explicit

k=1 are all coupled,

fore, even though the data generation processes of multiple
k=1 are governed by a single proba-

references to the corresponding source images in eX . There-
bility distribution P (eX ) on eX , it is unrealistic to assume that
i.e. for all i ={1,...,n} and k ={1,...,m}, there exists ˜xi∈ eX

feature domains {X ,X k}m
the available sample instances {X,X k}m
such that xk
i = ek(˜xi)∈ X k. Also, the number of available
sample instances may vary across tasks leading to predictor
k=1. In this case, direct

vectors that differ in sizes {f ,gk}m
evaluations of the metric h·,·iM in Eq. 7 is not possible.
Motivated by recent work on centered kernel align-
ment [20, 4], we construct bridge variables {Bk}m
k=1 that
align each reference variable gk to the main predictor
variable f . To motivate the construction, ﬁrst we note that the
metric evaluation hf ,giM of prediction vectors f and g cor-
responds to a measure of the alignment of the corresponding
centered gram matrices Gf = ﬀ ⊤ and Gg = gg⊤:

.

(11)

hf ,giM =

tr[Gf CGgC]

ptr[Gf CGf C]ptr[GgCGgC]

For typical kernel alignment applications e.g. in kernel
learning [4] and clustering [15], a gram (kernel) matrix G
contains pair-wise evaluations of a positive deﬁnite kernel
k(·,·). In Eq. 11, our kernel evaluates the product of two
scalar inputs (k(a,b) = ab).
When the two gram matrices Gf and Gg are constructed
from disjoint sample sets, and therefore, element-wise data
coupling is not provided, a bridge matrix Bgf of positive
entries can be constructed to align Gg with respect to Gf :

hf ,giBgf =

tr[Gf CBgf GgB⊤

ptr[Gf CGf C]qtr[Bgf GgB⊤

gf C]
gf CBgf GgB⊤

gf C]

.

(12)

The elements of each row in Bgf total to one and therefore,
each entry in the aligned gram matrix Bgf GgB⊤
gf is obtained
as a probabilistic (convex) combination of a Gg-column.

If both gram matrices Gf and Gg are full rank as in
existing kernel alignment applications, such a bridge matrix
can be straightforwardly constructed by maximizing the
alignment score hf ,giBgf (possibly, with additional regular-
izers, e.g. non-negativity and sparsity [20]). Unfortunately,
this approach is not applicable in our case as the number
of variables in Bgf is much higher than the effective degrees
of freedom of the observed gram matrices (of rank 1): Our
preliminary experiments indicated that naïvely applying this
strategy trivially leads to the maximum alignment (value
of 1), even for a random gram matrix Gg.

Instead, we cast the bridge matrix learning as a con-
tinuous relaxation of bipartite graph matching: Suppose

that f ∈ Rn(f ) and gk ∈ Rn(k) are obtained as evalu-
ations of f and gk on the respective feature instances

X ={x1,...,xn(f )}, and X k ={xk
n(k)} and for each
set, the ﬁrst n′ data instances are paired, i.e. there exists

1,...,xk

˜xi ∈ eX such that ([f ]i,[gk]i) = (f (˜e(˜xi)),gk(˜ek(˜xi))) for

i = 1,...,n′. Using these coupling labels, Bkf is initialized as

[Bkf (0)]ij =(1

0

if i = j and i≤ n′
otherwise.

(13)

which then evolves by diffusion propagating the labels to

the entire bipartite graph G = (X, X k). To facilitate this
process, we construct a pair of graph Laplacians ∆f and ∆k
based on the similarities of the respective feature domains
and the predictor evaluations: For the main predictor f , the
Laplacian ∆f is deﬁned as

2 W xf D− 1

∆f = I−D− 1
ij = exp(cid:18)−kxi−xjk2
W x

σ2
x

2 , W xf = W x

ij◦W f
ij,

(cid:19),W f

ij = exp(cid:18) ([f ]i−[f ]j)2

σ2
f

(14)

(cid:19),

with A◦ B being the Hadamard product of A and B. The
graph Laplacian ∆k is similarly constructed. Note that
∆f and ∆k are anisotropic as they use the corresponding
predictor evaluations f and gk in calculating the respective
diffusivities (W xf and W xkgk
; Eq. 14). Given the initial
solution Bkf (0), the diffusion process on the bipartite graph
G is speciﬁed via these two Laplacians: The solution of
the corresponding implicit Euler method is obtained as the
minimizer of an energy

E(V ) =kV −Bkf (0)k2

F

+δBtr[V ⊤∆f V ]+δBtr[V ∆kV ⊤]

(15)

whose optimum V ∗ can be obtained as the solution of a
Sylvester equation:

δB∆f V +δBV ∆k = Bkf (0).

(16)

This analytical approach generates a dense matrix Bkf ,
and therefore, it cannot be applied to large-scale problems
(n > 10,000). For these problems, we adopt the explicit Euler
method and alternate V -updates based on two Laplacians:

Bkf (t+1) = Bkf (t)−δB∆f Bkf (t)
Bkf (t+1) = Bkf (t)−δBBkf (t)∆k

(17a)

(17b)

explicitly controlling the sparsity of Bkf (t): At each
iteration, each row of Bkf (t) is sparsiﬁed by keeping only
the largest K values and assigning zero to the rest of the
elements. Given the initial label of {0, 1} in Bkf (0), the
diffused variables Bkf stay bounded in [0, 1]. At each
iteration, we normalize each row of Bkf (t) such that its
element values sum to 1.

2.2.3

Joint diffusion

Our ﬁnal algorithm consists of two diffusion processes:
f -diffusion updates the predictor variables f while B-
diffusion updates the bridge variables. These diffusions are

7552

Algorithm 1: Predictor combination using joint mani-
fold diffusion.

Input: Initial main predictor f and reference predictors

k=1; weight matrix W x and reference graph

k=1 (Eq. 14); hyperparameters σ2

{gk}m
Laplacians {∆k}m
(Eq. 5), δ (Eq. 7), T1, and T2;

Output: Reﬁned predictions f .

t = 0;
Build graph Laplacian ∆xf using W x and f (0) (Eq. 14);
for t1 = 1,...,T1 do

for t2 = 1,...,T2 do

Update f (t) based on the score function O (Eq. 7)
and metric h·,·iBgf (Eq. 12).
t = t+1;

end
for t2 = 1,...,T2 do

Update {Bkf (t)}m
Normalize rows of {Bkf (t)}m
t = t+1;

k=1;

k=1 based on Eqs. 15-17b;

end
Update ∆xf using W x and f (t);

end

respectively governed by two classes of graph Laplacians
∆ (Eq. 5) and {∆f ,∆k}m
k=1 (Eq. 14), and as both ∆(t) and
∆f (t) depend on f (t), the two diffusion processes interact
nonlinearly. We propose to interweave the two processes:
First, we initialize B by performing the B-diffusion. Then,
the two steps of f -diffusion and B-diffusion alternate
until the termination condition is satisﬁed. Algorithm 1
summarizes the proposed joint diffusion process.

2.2.4 Hyperparameters

f (similarly, σ2

k=1) requires tuning the scale parameters σ2

Unlike the implicit Euler method (Eq. 15), the explicit
Bkf update rule (Eqs. 17a and 17b) is not stable uniformly
over all values of δB. Hence, we ﬁx δB at a small value
10−5. Building the graph Laplacian ∆f (similarly for
{∆k}m
x and
σ2
f , and the number of nearest neighbors (NN) N in X.
We determine σ2
x as twice the mean distance within the
local N -neighborhood following Hein and Maier [9]. The
NN parameter N , the sparsity parameter K, and f -scale
parameter σ2
k) are globally tuned to maximize
the maximum coupling score hf ,gkiBkf across all reference
{gk}m
k=1 (Eq. 11). They are determined during the ﬁrst
iteration and are held ﬁxed throughout the diffusion process.
The step-size parameter δ (Eq. 7) and the scale parameter
σ2 (Eq. 5) for f -diffusion is decided based on the ranking
accuracy (deﬁned as the ratio of correctly ranked pairs
with respect to all pair-wise comparisons) on the validation
sets: While our algorithm is unsupervised, we automatically
tune the hyperparameters using small validation sets to
facilitate fair comparisons with other algorithms (see Sec. 3
for details).
In practice, the hyperparameters would be
adjusted by the user trying different parameter combinations.

Figure 1. Accuracy of our algorithm on OSR dataset (attribute 3)
with respect to varying hyperparameters σ2 and δ.

Figure 1 shows that indeed, this sampling approach is
feasible as the accuracy surface varies smoothly with respect
to these hyperparameters.

For joint diffusion, we set an upper bound T2 on the
number of steps in each f - and B-diffusion process, and
terminate the iterations immediately when the validation ac-
curacy (for f -diffusion) or alignment score (for B-diffusion)
does not increase. These two processes alternate until the
joint iteration number meets the upper bound T1, or the
f -validation accuracy does not improve. Our algorithm
converges fairly quickly, typically within 10 iterations. We
set T1,T2 = 20 (see Algorithm 1).

3. Experiments

3.1. Design evaluation on a synthetic dataset

{˜tk}12

To gain an insight into the effectiveness of our bridge
estimation approach, we constructed a toy dataset with a
known task metric structure. First, we generated 12 different
tasks by explicitly building their ground-truth predictors
k=1: Each member is constructed as a linear function on

the parameter vectors of 12 predictors, the last four are
randomly generated (with each element sampled from the
uniform distribution on [−1,1]) while the ﬁrst 8 parameter
vectors form two groups of 4 linearly depending predictors:

the 100-dimensional input space: ˜tk(x) = x⊤ewk. Among
fW 1 = [ew1, ... ,ew4] is obtained by multiplying a pair of

randomly generated vectors of sizes 100 × 1 and 1 × 4,
respectively. The parameters of the second group (tasks 5-8)
are generated similarly. The corresponding coupled noisy
observations Hc ={hk
k=1 are obtained by evaluating these
ground-truths on an input dataset of n=1,000 data points

c }m

mean Gaussian with standard deviation 0.2) to the result.

eX ={ex1,...,exn} and adding a mild level of noise (i.i.d. zero-
k=1 each sub-sampled from eX (n(k)≈ n/2):

k=1 are
constructed based on task-speciﬁc feature sets {X k : X k =
{xk

Similarly, decoupled observations Hd = {hk
1,...,xk

To simulate different feature extraction operations, we
applied principal component analysis with the feature dimen-
sions varying randomly across tasks (under the condition

n(k)}}m

d}m

that 95% of the total variance is retained): X k ⊂ ek| eX with
ek being the k-th principal component feature extractor. Fi-

7553

Figure 2. Example estimation of the task metric h·,·iM (Eq. 8) from
decoupled predictions {gk}12
k=1. By design, tasks 1-4 and tasks 5-8
respectively form groups of strongly correlated tasks. (left) pair-
wise metric evaluations from the ground-truth predictions; (center)
metric estimated based on decoupled predictions using the initial
bridge estimate; (right) ﬁnal metric evaluations constructed via
joint diffusion.

nally, the noisy predictions Hd are obtained by constructing
the least-squares parameter approximations of Hc:

wk = argmin

w

n(k)Xi=1

(w⊤xk

i −[hk

c ]i)2,

(18)

evaluating the resulting predictors {gk : gk(x) = x⊤wk}12
k=1
respectively on {X k}12
k=1, and adding Gaussian noise to
the results. Across different feature matrices {X k}12
k=1, the
source of feature instances in the ﬁrst 30 rows are shared,
providing coupling labels.

For each task k, we used hk as the main predictor f
and the rest as the references constituting a total of 12
predictor combination problems. Figure 2 shows the results
of the bridge estimation process. (Left) shows the metric
evaluated from the coupled predictions Hc: the k-row of the
displayed matrix shows the metric evaluations of hk
c (as the
main predictor) with respect to the remaining predictors (as
references). This matrix can be regarded as the ground-truths
for bridge estimation process. (Center) shows the metric
evaluated on the decoupled predictors Hd using the initially
estimated bridge variables Bkf (0) (Eq. 15). Given the mild
level of task noise (as shown in Fig. 2(left)), the initial
metric evaluations on decoupled observations already well-
recovered the underlying task dependence. Finally, (Right)
shows the metric evaluated on the predictions denoised
via the joint diffusion process. Our algorithm successfully
suppressed noise and reﬁned the underlying metric structure.

3.2. Evaluation on real datasets

We evaluate our joint manifold diffusion algorithm on
seven datasets and compare its performance with four base-
line algorithms. Each entry in these datasets is assigned with
multiple ground-truth attributes and therefore, predicting
the relative strengths of these attributes constitutes multiple
predictor combination problems: For each target attribute,
our algorithm reﬁnes the corresponding predictor based on
the remaining predictors as references.

3.2.1 Baseline methods

A) Ind: The ﬁrst baseline algorithm (Ind) evaluates and
selects the best predictor per dataset, per attribute from
among deep neural networks (DNNs[24]), and linear and
nonlinear rank support vector machines (RankSVMs[17])
based on validation accuracy. For all experiments, the
baseline algorithms were trained based on pair-wise
rank labels extracted from 200 training data points. For
given training inputs X = {x1,...,xn} and pair-wise rank
labels {(i(1), j(1)), ... , (i(l), j(l))}, the linear RankSVM
(f (x) = w⊤x) minimizes the regularized rank energy:

E S(f ) =

lXk=1

L([xi(k),xj(k)],f )+λSkwk2,

(19)

where the margin-based rank loss L is deﬁned as
L([xi,xj],f ) = (max(1−(f (xi)−f (xj)),0))2.

(20)

The regularization hyperparameter λS ≥ 0 is tuned based

on the accuracy on a separate validation set of the same
size as the training set. For non-linear RankSVMs, we use

a Gaussian kernel k(x,x′) = exp(cid:0)−kx−x′k2/σ2

scale hyperparameter σ2
S > 0. In this case, the parameter
norm kwk2 in Eq. 19 is replaced by the RKHS norm
corresponding to k: kwk2
k.

S(cid:1) with a

B) TPC: The second baseline uses Kim et al.’s test-time
predictor combination approach (TPC) [10]. This algorithm
was originally developed for regression but adapting it to
ranking using rank loss L is straightforward. Both TPC
and our algorithm require the initial main rank predictor
k=1 as inputs, which we

f (0) and reference predictors {gk}m
obtain from Ind.
C) MTL1: The last two baselines (MTL1 and MTL2)
implement adaptations of two existing multi-task learning
algorithms. MTL1 is based on Evgeniou and Pontil’s ap-
proach of penalizing the pair-wise parameter deviations [6].
Adapted to test time combination setting, MTL1 minimizes4

LMTL1 (f ) =

L([xi(k),xj(k)],f )

lXk=1
+λSkwk2 +λ2

mXk=1

Wkkw−wkk2

(21)

similarly as

where the weight parameters {Wk}m
Wk = exp(−kw − wkk2/σ2
λS, λ2, and σ2

k=1 are deﬁned
task graph Laplacian (Eq. 5):
w). The hyperparameters
w are tuned based on a validation set.5

in our

4Many other existing MTL approaches, e.g. parameter matrix decompo-
sition approaches [8] and low-rank matrix learning algorithms [1], strictly
require simultaneous training, making them difﬁcult to apply in the test-time
combination setting of improving a predictor given ﬁxed references.

5Evgeniou and Pontil’s original algorithm assumes that all tasks are re-
lated and therefore uses uniform weights, i.e. Wk = 1/m. Our preliminary
experiments demonstrated that the non-uniform version (Eq. 21) always
achieves higher accuracy indicating that not all tasks are equally relevant.

7554

Similarly to RankSVM, MTL1 can also construct non-linear
predictors using Gaussian kernels (with hyperparameter σ2
S).
D) MTL2 adapts Pentina et al.’s curriculum learning
approach [19], which penalizes the deviation of the main
predictor parameter w from a single best reference predictor
wk. Pentina et al.’s original algorithm uses a bound on the
generalization accuracy to select the reference predictor,
which is not directly applicable to our rank learning problem.
Instead, validation accuracy is used to select the reference.
For all datasets, we ran ten experiments with different
training and validation set conﬁgurations and report the
average results.

3.2.2 Datasets

A) Public Figure Face (PubFig) dataset contains 800
images from 8 random identities [17]. Our goal is to
estimate a linear ordering of database images based on the
relative strengths of each of 11 different facial attributes
(Masculine-looking, White, Young, Smiling, Chubby,
Visible-forehead, Bushy-eyebrows Narrow-eyes, Pointy-nose,
Big-lips, and Round-face).

B) Outdoor Scene Recognition (OSR) dataset provides
2,688 images of 8 scene categories and 6 attributes [17].

We use a combination of GIST features and color
histograms for PubFig and GIST features for OSR. The
attribute rank labels are constructed from the category labels
as provided by the authors of [17]. For each attribute, we
improve the corresponding predictor using the predictors
of the remaining attributes as references.

C) Shoes dataset contains 14,658 images of 10 categories
and 10 attributes [11]. We use a combination of GIST
features and color histograms provided by the authors
of [11]. Our goal is to estimate the attribute rankings
similarly to PubFig and OSR settings. However, here
the datasets for the main and reference predictors are
disjoint and we explicitly estimate the bride variables using
additional 200 paired instances. As in this case TPC is not
applicable, we compare with MTL1 and MTL2.

D) Cal7 dataset contains 1,474 images of 7 categories (Face,
Motorbikes, Dolla-Bill, Garﬁeld, Snoopy, Stop-Sign, and
Windor-Chair) as a subset of Caltech-101 dataset [7]. The
dataset provides ﬁve different feature representations per
image: wavelet, Gabor, CENTRIST, HOG, GIST, and LBP
features [14]. The goal is to estimate a linear database order-
ing according to the category of each entry. For each single
feature, we conﬁgured a corresponding main prediction task
and constructed reference predictors using the remaining
features. For each experiment, two disjoint feature sets for
the main and reference predictors, respectively are prepared
(roughly, half of the dataset was allocated for the main
and the rest were allocated for references) representing the
scenario where multiple predictions are generated based on
heterogeneous, decoupled feature observations. To estimate
the bridge variables, we use 200 coupled data instances as

a sample from the joint distribution P (f,g1,...,gm). As the
predictor variables are decoupled across tasks, TPC is not
applicable. Further, since the respective feature spaces and
the corresponding predictors are heterogeneous, (adaptations
of) classical parametric MTL approaches cannot be directly
applied. Therefore, we compare our algorithm with only
independent baselines (Ind).

E) NUS-WIDE-Object (NUS) dataset contains 30,000
images of 31 categories [3]. We use color histogram, color
moments, color correlation, edge distribution and wavelet
features as provided by the authors of [3] and [14].

F) Handwritten digits (HW) dataset provides 6 different
feature representations of 2,000 handwritten digits, each
represented by Fourier coefﬁcients, proﬁle correlations,
Karhunen-Loève coefﬁcients, pixel averages in 2 × 3
windows, Zernike moment and morphological features [2].
Experimental settings for NUS and HW are identical to

Cal7. We use 200 paired data to learn bridge variables.

G) Animals With Attributes (AWA) dataset contains
30,475 images of 50 animal categories. We use the SURF,
SIFT and PHOG histograms and the features extracted
by pre-trained DeCAF [5] and VGG19 [21] networks as
provided by the authors of [13]. The experimental setting is
similar to those of Cal7-HW except that, here we explicitly
pair all data points across tasks enabling the application
of TPC. This toy setting constitutes the ideal case where
all reference predictors are inherently relevant in reﬁning
the main predictor and it enables us to verify the correct
operation of TPC and our approach.

3.2.3 Results

Figure 3 summarizes the results. While not all target at-
tributes show marked improvements, TPC and our algorithm
consistently improve upon or are on par with Ind. Comparing
TPC and ours, the performances are almost identical on OSR.
For PubFig, the two algorithms demonstrated the comple-
mentary strengths across different target attributes, while our
algorithm achieves higher average accuracy. The correspond-
ing results on AWA are notably different: While TPC already
achieves better results than the baseline Ind, our algorithm
further improves accuracy by a large margin. In addition,
by virtue of the fast Eigen-decomposition-based approach
(Eq. 10) the runtime of our algorithm is around 20× shorter
than TPC: For AWA with 30,475 images, our algorithm took
around 0.2 seconds for the entire combination process. As
TPC requires fully coupled predictor evaluations, it cannot
be applied to Cal7, NUS, and HW datasets, in which our
algorithm continues to outperform Ind. For these datasets,
our algorithm demonstrates even better performance than
the best individual task predictors, which demonstrates the
utility of combining predictors across multiple features.

The two multi-task learning adaptations MTL1 and MTL2
to the test-time combination setting also showed measurable
performance improvement over Ind.
In particular, they

7555

Figure 3. Average accuracy of different ranking algorithms (over 10 different training and test set conﬁgurations. Ind: best baseline
independent predictors; MTL1 and MTL2: adaptations of existing MLT algorithms ([6] and [19], respectively); TPC: Kim et al.’s test-time
predictor combination algorithm [10]. The length of each error bar corresponds to twice the standard deviation.

achieved the highest average accuracy on target attributes 2,
4, and 5 of the OSR dataset. On the other hand, for PubFig
and Shoes, our algorithm constantly outperformed these
algorithms demonstrating complementary strengths. As
both MTL1 and MTL2 require the parametric forms of all
predictors to be shared across different tasks, it is not straight-
forward to apply these algorithms when different tasks use
heterogeneous features (Cal7, NUS, and HW datasets).

Our manifold structure (Eq. 1) and metric therein (Eqs. 2–
3) are directly aligned with the case when the predictor
outputs are one-dimensional (e.g. ranking and regression
problems). When the output space is multi-dimensional
(e.g. multi-class classiﬁcation), our metric structure needs
changing to align predictions of different dimensions.
We expect that this can be done by calculating canonical
correlations between the input pairs, but it would involve
non-trivial modiﬁcations.

4. Conclusion

In this paper, we have presented a new algorithm improv-
ing a given task predictor by combining multiple reference
predictors, each constructed from the respective tasks.
Conventional approaches require either all task predictor’s
known and shared parametric forms or multiple predictors’
evaluation on a single ﬁxed dataset. We address these
limitations by formulating the problem as a non-parametric
task dependence estimation and by a robust joint diffusion
process that automatically couples the predictors of disjoint
data instances. This not only facilitates a new (decoupled,
parameter-free) predictor combination application but also
signiﬁcantly improves the accuracy and run-time over
existing algorithms when applied to challenging relative
attributes ranking datasets.

Identifying data coupling across heterogeneous domains is
a challenging problem. This problem arises in the predictor
combination setting where different predictors are evaluated
on data instances sampled from multiple heterogeneous do-
mains. We attempted to address this challenge by estimating
soft couplings via a joint diffusion process propagating a
small set of coupled data points. An alternative possibility
that we have not explored in this work is to consider recent
label-free set pairing approaches, e.g. instantiated using
cyclic GANs [26]. This type of approach is not immediately
applicable to our setting as they do not generate explicit
pairings and, therefore, would require modifying the entire
task dependence measure and the corresponding denoising
process. Future work should explore this possibility.

7556

859095123456Accuracy (%)AttributeOSR607080901001234567891011Accuracy (%)AttributePubFig63738312345678910Accuracy (%)AttributeShoesIndMTL1MTL2TPCOurs4858687812345Accuracy (%)FeatureAWA6368737812345Accuracy (%)FeatureNUS7075808590123456Accuracy (%)FeatureHW80859095100123456Accuracy (%)FeatureCal7References

[1] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task

feature learning. Machine Learning, 73(3), 2008. 1, 6

[2] C. L. Blake and C.

J. Merz.
learning databases,

of machine
//archive.ics.uci.edu/ml. 7

UCI

1998.

repository
https:

[3] T. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng. NUS-
WIDE: a real-world web image database from National Uni-
versity of Singapore. In ACM CIVR, pages 48:1–48:9, 2009. 7
[4] C. Cortes, M. Mohri, and A. Rostamizadeh. Algorithms
for learning kernels based on centered alignment. JMLR,
13:795–828, 2012. 4

[5] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. DeCAF: a deep convolutional
activation feature for generic visual recognition. In ICML,
pages 647–655, 2014. 7

[6] T. Evgeniou and M. Pontil. Regularized multi–task learning.

In KDD, pages 109–117, 2004. 1, 6, 8

[7] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples: An incremental
bayesian approach tested on 101 object categories. Computer
Vision and Image Understanding, 106(1):59–70, 2007. 7

[8] P. Gong, J. Ye, and C. Zhang. Robust multi-task feature

learning. In KDD, pages 895–903, 2012. 1, 6

[9] M. Hein and M. Maier. Manifold denoising. In NIPS, pages

561–568, 2007. 3, 5

[10] K. I. Kim, J. Tompkin, and C. Richardt. Predictor combination

at test time. In ICCV, pages 3553–3561, 2017. 1, 2, 3, 6, 8

[11] A. Kovashka, D. Parikh, and K. Grauman. Whittlesearch:
In CVPR,

Image search with relative attribute feedback.
pages 2973–2980, 2012. 2, 7

[12] A. Kumar and H. Daumé III. Learning task grouping and
overlap in multi-task learning. In ICML, pages 1383–1390,
2012. 1

[13] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning
to detect unseen object classes by between-class attribute
transfer. In CVPR, pages 951–958, 2009. 7

[14] Y. Li, F. Nie, H. Huang, and J. Huang. Large-scale multi-view
spectral clustering via bipartite graph. In Proc. AAAI, pages
2750–2756, 2015. 7

[15] Y. Lu, L. Wang, J. Lu, J. Yang, and C. Shen. Multiple
kernel clustering based on centered kernel alignment. Pattern
Recogn., 47:3656–3664, 2014. 4

[16] Y. Luo, D. Tao, B. Geng, C. Xu, and S. J. Maybank. Manifold
regularized multitask learning for semi-supervised multilabel
image classiﬁcation. IEEE TIP, 22(2):523–536, 2013. 1

[17] D. Parikh and K. Grauman. Relative attributes. In ICCV,

pages 503–510, 2011. 2, 6, 7

[18] A. Passos, P. Rai, J. Wainer, and H. Daumé III. Flexible
modeling of latent task structures in multitask learning. In
ICML, pages 1103–1110, 2012. 1

[19] A. Pentina, V. Sharmanska, and C. H. Lampert. Curriculum
learning of multiple tasks. In CVPR, pages 5492–5500, 2015.
1, 7, 8

[20] I. Redko and Y. Bennani. Kernel alignment for unsupervised

transfer learning,. In arXiv:1610.06434v1, 2016. 4

[21] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, page
arXiv:1409.1556, 2015. 7

[22] B. Wang and Z. Tu. Sparse subspace denoising for image

manifolds. In CVPR, pages 468–475, 2013. 3

[23] Y. Yan, E. Ricci, R. Subramanian, G. Liu, and N. Sebe.
Multitask linear discriminant analysis for view invariant
action recognition. IEEE TIP, 23(12):5599–5611, 2014. 1

[24] X. Yang, T. Zhang, C. Xu, S. Yan, M. S. Hossain, and
IEEE T-MM,

A. Ghoneim. Deep relative attributes.
18(9):1832–1842, 2016. 2, 6

[25] L. W. Zhong and J. T. Kwok. Convex multitask learning with

ﬂexible task clusters. In ICML, pages 49–56, 2012. 1

[26] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired
image-to-image translation using cycle-consistent adversarial
networks. In ICCV, pages 2223–2232, 2017. 8

7557

