Distilled Person Re-identiﬁcation: Towards a More Scalable System

Ancong Wu1 , Wei-Shi Zheng2,3 ∗ , Xiaowei Guo5 , and Jian-Huang Lai2,4

1

School of Electronics and Information Technology, Sun Yat-sen University, China

2School of Data and Computer Science, Sun Yat-sen University, China

3Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China

4Guangdong Province Key Laboratory of Information Security, China

5YouTu Lab, Tencent

wuancong@gmail.com, wszheng@ieee.org, scorpioguo@tencent.com, stsljh@mail.sysu.edu.cn

Abstract

Unlabelled data and little 
labelled data of new scene

Massive labelled data of other scenes (unavailable)
Scene 1 Data
Scene M Data

Scene 2 Data

Person re-identiﬁcation (Re-ID), for matching pedestri-
ans across non-overlapping camera views, has made great
progress in supervised learning with abundant labelled da-
ta. However, the scalability problem is the bottleneck for
applications in large-scale systems. We consider the scala-
bility problem of Re-ID from three aspects: (1) low labelling
cost by reducing label amount, (2) low extension cost by
reusing existing knowledge and (3) low testing computation
cost by using lightweight models. The requirements ren-
der scalable Re-ID a challenging problem. To solve these
problems in a uniﬁed system, we propose a Multi-teacher
Adaptive Similarity Distillation Framework, which requires
only a few labelled identities of target domain to transfer
knowledge from multiple teacher models to a user-speciﬁed
lightweight student model without accessing source domain
data. We propose the Log-Euclidean Similarity Distillation
Loss for Re-ID and further integrate the Adaptive Knowl-
edge Aggregator to select effective teacher models to trans-
fer target-adaptive knowledge. Extensive evaluations show
that our method can extend with high scalability and the
performance is comparable to the state-of-the-art unsuper-
vised and semi-supervised Re-ID methods.

1. Introduction

With the development of surveillance systems, person
re-identiﬁcation (Re-ID) has drawn much attention in re-
cent years. Most researches focus on supervised learning
[25, 67, 31, 28, 1, 47] and have made great progress. How-
ever, system extension is still a signiﬁcant obstacle for ap-
plying Re-ID in large-scale surveillance systems because
of the scalability problem, which is still under-explored.
Some previous works attempt to improve scalability from
different aspects, such as unsupervised and transfer learn-
ing [42, 24, 62, 52, 51, 11] for reducing label amount and

∗Corresponding author

…

Student Model (cid:1834)(cid:3020)

(user-specified and 

lightweight)

New scene model learning

Scene 1 
Teacher 

Model (cid:1834)(cid:3021)(cid:2869)

Scene 2 
Teacher 

Model (cid:1834)(cid:3021)(cid:2870)

Scene M 
Teacher 

Model (cid:1834)(cid:3021)(cid:3014)

Teacher model pool (available)

Knowledge distillation

Figure 1. A scalable adaptation Re-ID system. We propose to store
the knowledge in teacher models trained in existing scenes. When
extending to a new scene, we can ﬂexibly transfer the knowledge
from teacher models to a user-speciﬁed lightweight student model
by using unlabelled data and little labelled data of the new scene,
without using source domain data which may not be accessible.

fast retrieval [56, 73] for large-scale applications. To build
a more scalable Re-ID system, it is expected that these scal-
ability problems can be addressed in a uniﬁed system.

We consider the requirements and challenges of the scal-

ability problem of Re-ID mainly from three aspects:
(1) Low labelling cost. Existing supervised Re-ID methods
require abundant labelled identities, which is unrealistic for
a large-scale system. A scalable Re-ID system should be
able to learn from unlabelled data and limited labelled data,
which provide very limited information for learning.
(2) Low extension cost. When extending to a new scene,
most existing Re-ID methods apply transfer learning, which
requires auxiliary source domain data for pretraining or
joint training. In some cases, source domain data of other
scenes may not be accessible because of privacy problem or
transfer problem. Even if source domain data is available,
joint training increases computation costs. Moreover, pre-
trained models may not be applicable due to different user-
speciﬁed requirements of model architecture and capacity.
Thus, a scalable Re-ID system should be able to extend to a
new scene ﬂexibly with low cost. Knowledge transfer with-
out accessing source domain data is a challenge.
(3) Low testing computation cost. Existing state-of-the-art

1187

methods for Re-ID are based on large neural network mod-
els, e.g. ResNet-50 [20], which cannot meet the require-
ment of the camera hardware development trend of front-
end processing on the chips. Thus, a scalable Re-ID system
should be able to learn lightweight models.

To solve the above problems, we propose a scalable
adaptation Re-ID system based on knowledge reuse as
shown in Fig. 1. The system can extend with unlabelled
data and only a few labelled identities in the target domain,
without using source domain data. Instead of storing raw
data, we store the knowledge of Re-ID for an existing scene
i in a teacher model HT i. When extending to a new scene,
knowledge in a teacher model pool {HT i}M
i=1 is aggregated
and transferred to a new user-speciﬁed lightweight student
model HS for the target domain. In this system, knowledge
transfer and knowledge aggregation are two key problems
and we address them as follows.

Knowledge transfer in our system is challenging, be-
cause limited labelled target data and absence of source data
lead to little information for learning and knowledge need
be compressed in a lightweight model. To solve these prob-
lems, we need to imitate and distill the knowledge in the
teacher models, which is knowledge distillation [21]. For
Re-ID, we exploit knowledge embedded in similarity and
propose the Log-Euclidean Similarity Distillation Loss for
imitating the sample pairwise similarity matrix of the teach-
er. Most knowledge distillation methods [21, 37, 61] are de-
signed for closed-set classiﬁcation and they convey knowl-
edge by soft labels. They are not suitable for Re-ID, be-
cause Re-ID is an open-set identiﬁcation problem, in which
the identities are non-overlapping in training and testing.

Furthermore, to effectively aggregate knowledge from
multiple teachers, we propose the Adaptive Knowledge Ag-
gregator for adjusting the contributions of multiple teach-
ers in the distillation loss dynamically, in order to selec-
t effective teachers and aggregate effective knowledge for
the target domain. Only a few labelled identities (e.g. 10)
are needed as validation data for computing empirical risk
to guide knowledge aggregation. We further integrate the
Log-Euclidean Similarity Distillation Loss and the Adap-
tive Knowledge Aggregator into a Multi-teacher Adaptive
Similarity Distillation Framework as shown in Fig. 2.

In summary, the contributions of this paper are: (1) We
propose the Log-Euclidean Similarity Distillation Loss for
knowledge distillation for Re-ID, which is an open-set iden-
tiﬁcation problem. (2) We propose the Adaptive Knowledge
Aggregator for aggregating effective knowledge from mul-
tiple teacher models for learning a lightweight student mod-
el. (3) We further integrate them in a Multi-teacher Adap-
tive Similarity Distillation Framework for scalable person
re-identiﬁcation, which can simultaneously reduce labelling
cost, extension cost and testing computation cost.

2. Related Work

Supervised Person Re-identiﬁcation.
Person Re-
identiﬁcation has witnessed a fast growing development in
recent years, from feature design [19, 15, 32, 31, 35, 69, 58]
to distance metric learning [54, 19, 43, 25, 67, 36, 41, 30,
58, 33, 38, 31, 9, 68, 60, 63, 69, 29, 52, 6] and end-to-end
deep learning [28, 1, 55, 57, 49, 22, 64, 65, 70, 47]. Most
existing works rely on abundant labelled data. Although
high performance can be achieved by deep models, heavy
labelling cost hinders the scalability of these methods.

Scalable Person Re-identiﬁcation. Recently, scalable
person re-identiﬁcation has drawn more attention for re-
ducing costs of system extension. Unsupervised learning
[42, 24, 62, 51, 14, 53, 13, 7, 27, 71], transfer learning
[72, 53, 13, 11], small sample learning [63, 2] and active
learning [34, 50, 45] are for reducing labelling cost by min-
imizing the requirement of labelled data or selecting specif-
ic data for labelling. Fast adaptation [5, 39] is for reducing
computation cost in training. Fast retrieval by lightweight
model [56] and binary representation [8, 73] are for reduc-
ing computation cost in testing. These methods only ad-
dress one of the problems of labelling cost, extension cost
or testing computation cost, while our method address these
problems simultaneously in a uniﬁed framework.

Knowledge Transfer/Distillation.
Knowledge trans-
fer/distillation [21] is for transferring knowledge from a
large teacher model to a smaller student model by imita-
tion. A vast majority of knowledge distillation methods
such as [21, 37, 61, 16] are designed for closed-set classi-
ﬁcation problems by using soft labels of the teacher model
to guide learning the student model. However, person re-
identiﬁcation is an open-set identiﬁcation problem, in which
the identities in training and testing are non-overlapping,
and thus the soft-label-based distillation methods are not so
suitable for Re-ID. Some methods also consider using in-
formation other than soft labels as knowledge. Fitnets [44]
and FSP [59] exploit feature maps and PKT [40] exploits
the probability distribution of data, which are not directly
related to measuring similarity for matching in Re-ID. To
more effectively represent and convey knowledge, we distill
the knowledge embedded in sample similarity by imitating
the teacher similarity matrix. As for distilling from mul-
tiple teacher models as in our proposed method, [16, 61]
exploit the ensemble of multiple teachers, but they are for
closed-set classiﬁcation and the contributions of different
teachers cannot be adaptively adjusted as in our method.
Semi-supervised teacher-student frameworks [48, 17] are
for closed-set classiﬁcation and cannot solve our problem.

Hypothesis transfer learning (HTL) [26] studies learn-
ing from source models without source data. Existing HTL
methods [3, 12] are for closed-set domain adaptation, which
cannot solve the open-set identiﬁcation problem for Re-ID.

1188

3. Similarity Knowledge Distillation

As designed in the scalable adaptation Re-ID system
(Fig. 1) in Section 1, to learn a model for the target do-
main with only a few labelled data, we can transfer knowl-
edge from other domains. To transfer knowledge without
accessing source domain data, we regard the model as a stu-
dent, which learns to imitate the teacher models by knowl-
edge distillation. In most knowledge distillation methods
[21, 37, 16], soft labels are utilized. However, it is not so
suitable to convey knowledge by identity soft labels for Re-
ID, because Re-ID is an open-set identiﬁcation problem in
which there is no overlap of identity in training and testing.
To overcome this problem, we exploit the knowledge
embedded in similarity and make the student model imitate
the pairwise similarities of the teacher model. For N un-
labelled image samples {Ii}N
i=1, let A denote the pairwise
similarity matrix, where the element ai,j in the i-th row and
the j-th column of A is the similarity between samples Ii
and Ij determined by model H. Let HS denote the student
model to be learned and HT denote the teacher model that is
ﬁxed. The pairwise similarity matrices of the student model
HS and the teacher model HT are denoted by AS and AT ,
respectively. To transfer knowledge from teacher to studen-
t, we minimize the distance between the student similarity
matrix AS and the teacher similarity matrix AT as follow:

by Eq. (2) is between 0 and 1.
(2) AS is a symmetric positive semi-deﬁnite matrix. In E-
q. (3), X⊤
XS is symmetric positive semi-deﬁnite. In our
S
case of mini-batch learning, the feature dimension d is larg-
er than the batch size N . Generally, XS ∈ Rd×N satisﬁes
rank(XS) = N and has full rank, so that AS ∈ RN ×N is
a symmetric positive deﬁnite (SPD) matrix in this case.

Teacher Similarity Matrix AT . The teacher similarity
matrix AT , as the target of student similarity matrix AS,
should satisfy the properties of student similarity matrix.

Generally, when using neural network as teacher model,
the teacher similarity matrix AT can be computed as the
student model in Eq. (3) by using the feature matrix XT =
[xT ,1, xT ,2, ..., xT ,N ] extracted by the teacher model HT .

In other cases, the teacher similarity matrix AT may not
satisfy the two properties of the student similarity matrix.
For example, when pairwise veriﬁcation neural network is
used as teacher model without constraint, the similarity ma-
trix may not be SPD. Some simple transformations can be
applied to make the teacher similarity matrix valid. First, if
the range of similarities in AT is not [0, 1], it can be mapped
to [0, 1] by normalization. Second, if AT is not symmet-
ric positive deﬁnite, we can project it onto the cone of all
positive semi-deﬁnite matrices as in [54]. More details are
provided in the supplementary due to space limitation.

min dist(AS, AT ),

(1)

3.2. Log Euclidean Similarity Distillation

where dist(·) is a distance metric for similarity matrices.
Note that AT is ﬁxed as the target for learning AS.

3.1. Construction of Similarity Matrices

Student Similarity Matrix AS. To obtain the pairwise
similarity matrix AS for student model HS, we use cosine
similarity, which is commonly used in neural networks for
Re-ID [47]. For a sample Ii, the student model HS extract-
s a d-dimensional feature vector HS(Ii; ΘS), where ΘS
is the parameter of HS. Let xS,i ∈ Rd denote the non-
negative normalized unit feature vector formulated by

xS,i = ReLU(HS(Ii; ΘS))/ kReLU(HS(Ii; ΘS))k ,

(2)

where ReLU(x) = max(0, x) is an activation function ap-
plied after the feature layer of HS.

Let XS = [xS,1, xS,2, ..., xS,N ] ∈ Rd×N denote the fea-
i=1. The student similarity ma-

ture matrix for samples {Ii}N
trix AS is computed by

AS = X

⊤

S XS,

(3)

where aS,i,j = x⊤
S,i
tween samples Ii and Ij .

xS,j in AS is the cosine similarity be-

Properties of Student Similarity Matrix. We derive the
properties of the student similarity matrix AS as follows:
(1) The range of similarities in AS is [0, 1]. The cosine sim-
ilarity between non-negative unit feature vectors extracted

After constructing the similarity matrices for student and
teacher, we aim to distill the knowledge by minimizing the
distance between the similarity matrices in Eq. (1). As ana-
lyzed above, the student and teacher similarity matrices are
symmetric positive deﬁnite (SPD) matrices, which are in-
trinsically lying on a Riemannian manifold [4] instead of a
vector space. Hence, when measuring the distance between
AS and AT , we take this property into consideration and
measure the distance in a log-Euclidean Riemannian frame-
work [4] instead of using Euclidean metric as follow:

dist(AS, AT ) = klog(AS) − log(AT )kF ,

(4)

where log(A) is the matrix logarithm of A. For any SPD
matrix A, the logarithm of it is

log(A) = Udiag(log(λ1), log(λ2), ..., log(λN ))U

⊤,

(5)

where U is the orthonormal matrix of eigenvectors and λi
is the eigenvalue, which are obtained from the eigendecom-
position A = Udiag(λ1, λ2, ..., λN )U⊤.

We distill the knowledge embedded in the similarity
from teacher to student by minimizing the Log-Euclidean
distance as follow:

min
ΘS

LT (XS) = (cid:13)(cid:13)(cid:13)

log(X

⊤

S XS) − log(AT )(cid:13)(cid:13)(cid:13)

2

F

,

(6)

where ΘS is the parameter of student model HS, XS is the
feature matrix extracted by HS and processed by Eq. (2),

1189

Target Domain Data
Unlabelled Data

Validation Data
(very few labelled IDs)
ID: 1
ID: 10

ID: 2

…

Similarity Knowledge Distillation

Student 

Features

Similarity
matrices

Update Student Model (cid:1834)(cid:3020)
Model (cid:1834)(cid:3020)
(cid:1775)(cid:3020)
Model (cid:1834)(cid:3021)(cid:2869)
Model (cid:1834)(cid:3021)(cid:2870)
Model (cid:1834)(cid:3021)(cid:3014)

(cid:1824)(cid:3020)
(cid:1824)(cid:3021)(cid:2869)
(cid:1824)(cid:3021)(cid:2870)
(cid:1824)(cid:3021)(cid:3014)

(cid:1775)(cid:3021)(cid:2869)
(cid:1775)(cid:3021)(cid:2870)
(cid:1775)(cid:3021)(cid:3014)

Teacher 

Teacher 

Teacher 

…

…

…

Distillation

Losses

(cid:1864)(cid:3021)(cid:2869)
(cid:1864)(cid:3021)(cid:2870)
(cid:1864)(cid:3021)(cid:3014)

…

Adaptive Knowledge Aggregation

Gradients w.r.t. features (cid:1824)(cid:3020)
Loss (cid:1864)(cid:3023)(cid:3006)(cid:3019) (Eq.(8))

Empirical Risk 

Validation 

Simulated 
Feature 
Update

Update Teacher weights

Adaptive 

Aggregated 
Distillation 

Loss (cid:1864)(cid:3021)(cid:3002) (Eq.(7))

(cid:2009)(cid:2869)
(cid:2009)(cid:2870)
(cid:2009)(cid:3014)

…

Figure 2. Overview of the Multi-teacher Adaptive Similarity Distillation Framework. Unlabelled data and a few labelled identities of target
domain are required. The teacher models are ﬁxed feature extractors for existing scenes in the system. The student model is a new user-
speciﬁed lightweight model to be trained. The teacher weight αi is for controlling the contribution of Log-Euclidean Similarity Distillation
Loss LT i of teacher HT i and it is learned by minimizing validation empirical risk loss LV ER. The student model is trained by unlabelled
data with the Adaptive Aggregated Distillation Loss LT A. (← denotes forward propagation and L99 denotes backward propagation.)

X⊤
XS is AS as computed in Eq. (3) and AT is ﬁxed sim-
S
ilarity matrix provided by teacher model HT . We call LT
the Log-Euclidean Similarity Distillation Loss for HT . The
effectiveness of the Log-Euclidean metric is further validat-
ed in our experiments as compared to the Euclidean metric.

4. Learning to learn from Multiple Teachers

We have illustrated learning from a single teacher mod-
el by similarity knowledge distillation. In this section, we
study learning from multiple teacher models. Since not all
teachers can provide effective and complementary knowl-
edge due to variations between cameras (e.g. illumination
and background), we need learning to adjust the contribu-
tions of multiple teachers, i.e.
learning to learn. We pro-
pose the Adaptive Knowledge Aggregator to aggregate ef-
fective knowledge from multiple teachers for learning a s-
tudent model, which requires only a few labelled identities
for reducing labelling cost. It is integrated with similarity
knowledge distillation to form the Multi-teacher Adaptive
Similarity Distillation Framework as shown in Fig. 2.

4.1. Multi teacher Adaptive Aggregated Distillation

To learn a student model HS from multiple teacher mod-
els in a teacher model pool {HT i}M
i=1 simultaneously, the
Log-Euclidean Similarity Distillation Loss LT in Eq. (6)
for a single teacher HT is generalized as follow:

LT A(XS; {αi}M

i=1) =

min
ΘS

M

X

i=1

αiLT i(XS),

(7)

where LT i is the Log-Euclidean Similarity Distillation Loss
for teacher model HT i, αi is the teacher weight for control-
ling the contribution of LT i. The teacher weight αi should
satisfy PM
i=1 αi = 1 and αi ≥ 0. XS is the feature matrix
extracted by student model HS parameterized by ΘS.

The constrain PM

of αi. It can be simply satisﬁed by αi = | ˜αi|/PM
where ˜αi is an unconstrained real number parameter.

i=1 αi = 1 is for normalizing the scale
j=1 | ˜αj|,

For unsupervised learning without prior knowledge, αi
can be set equally as 1/M . However, in practice, there
may be some ineffective teacher models that provide wrong
knowledge. Hence, instead of regarding αi as ﬁxed hyper-
parameter which needs tuning, we aim to learn αi dynam-
ically to make the loss LT A adaptive to the target domain.
We call LT A the Adaptive Aggregated Distillation Loss.

4.2. Adaptive Knowledge Aggregation

To learn the teacher weights {αi}M

i=1, guiding informa-
tion is required. Generally, for validating whether a Re-ID
system is working normally, it is necessary and feasible for
human operator to label a small amount of identities (e.g. ≤
10). Although the small amount of data is far from enough
for training an effective model from scratch due to overﬁt-
ting, we can compute the validation empirical risk on it to
provide guiding information for aggregating knowledge.

i }N

i , yi)}Nv

Validation Empirical Risk. For the target domain, we have
a large amount of unlabelled data DU = {IU
i=1. Addi-
tionally, we label data of a few identities (not in DU ) to
form a small validation set DL = {(IL
i=1, where
yi = 1, 2..., Cv is the label (Cv = 10 in our case). To
indicate whether the student is learning correct knowledge
from teachers, the empirical risk on validation data DL can
be computed. Let xU
S,i denote the features of unla-
belled sample IU
i , respectively. Let
PL = {(i, j)|yi = yj} denote the set of index pair (i, j)
of positive sample pair (IL
j ) in the validation set DL.
As there is no overlap identity in DL and DU , (IL
k ) is
a negative sample pair. We apply a Softmax cross entropy

k and labelled sample IL

S,k and xL

i , IL

i , IU

1190

loss for computing the validation empirical risk by

i

exp(xL⊤
S,i
S,j ) + PN
xL

xL

S,j )

LV ER(XU

S , XL

S ) = X

−log

xU

(i,j)∈PL

exp(xL⊤
S,i

k=1 exp(xL⊤

S,k)
(8)
We call LV ER the validation empirical risk loss. When
the similarity of positive pair xL⊤
S,j becomes larger and
S,i
the similarity of negative pair xL⊤
xU
S,k becomes smaller,
S,i
the loss LV ER becomes smaller. Thus, it can indicate the
empirical risk effectively.

xL

S,i

.

Adaptive Knowledge Aggregator.
To learn teacher
weights {αi}M
i=1 that can minimize the validation empirical
risk LV ER, we propose the Adaptive Knowledge Aggrega-
tor for optimizing {αi}M

i=1 by gradient descent.

S and XL

In the learning process of student model HS, at each
step, we have the feature matrices XU
S for un-
labelled and labelled data, respectively. Feature learning
is guided by gradient descent of the Adaptive Aggregat-
ed Distillation Loss LT A. To evaluate whether the cur-
rent teacher weights {αi}M
i=1 are effective, it is expected
that the features updated by LT A parameterized by {αi}M
i=1
can decrease the validation empirical risk loss LV ER to the
most extent. We simulate one step update of the features
XU
i=1) and
LT A(XL

S by using gradients of LT A(XU

i=1) with respect to XU

S ; {αi}M
S by

S and XL

S and XL

S ; {αi}M

XU ′

S = XU

S − β

XL′

S = XL

S − β

∂LT A(XU

∂LT A(XL

S ; {αi}M
∂XU
S
S ; {αi}M
∂XL
S

i=1)

i=1)

,

,

(9)

where XU ′
β is the step size of the simulated updating.

S and XL′

S are the simulated updated features and

S , XL′

the updated features XU ′
S

Then, we compute the validation empirical

S ) of the updated features XU ′

risk
LV ER(XU ′
S and XL′
S .
Note that,
S are re-
lated to the teacher weights αi because the gradients
∂LT A(XU
contain αi. Thus,
to minimize the validation empirical risk loss LV ER, the

S ;{αi}M
∂XU
S

S ;{αi}M
∂XL
S

and XL′

∂LT A(XL

and

i=1)

i=1)

∂LV ER(XU ′
∂αi

S ,XL′
S )

gradient
weights αi can be learned by gradient descent as follow:

can be computed and the teacher

α′

i = αi − γα

∂LV ER(XU ′
∂αi

S , XL′
S )

,

(10)

where α′
learning rate γα.

i is the updated value of teacher weight αi by using

the

risk during training,

With the objective of minimizing the validation
learning target
i=1) parameterized by teacher weights
i=1 can adaptively select effective teacher models by

empirical
LT A(XS; {αi}M
{αi}M
weighting to provide better guidance for student model.
Optimization. Before training, the teacher weights α1,
α2,..., αM are initialized as 1/M . There are mainly three
steps in training: (1) Feature extraction and similarity ma-
trix construction as illustrated in Section 3.1; (2) Updating

0.4

0.3

0.2

0.1

MSMT17
DukeMTMC
CUHK03
ViPER

 
t

i

h
g
e
w

 
r
e
h
c
a
e
T

0

3.7

3.6

3.5

3.4

3.3

2

4

6

8

10

12

14

16

w/ Aggregator
w/o Aggregator

2

4

6

8
Epochs

10

12

14

16

R
E
V

L

k
s
R

i

 
l

a
c
i
r
p
m
E
n
o

 

i
t

i

a
d
a
V

l

Figure 3. Change of teacher weights αi (the upper ﬁgure) and
change of validation empirical risk loss LV ER with and with-
out using Adaptive Knowledge Aggregator (the lower ﬁgure) in
the training process of Market-1501. The teacher weights can
select more effective teacher models trained on larger dataset-
s (MSMT17, DukeMTMC). The validation empirical risk loss
LV ER can be further minimized with the learned teacher weights.

teacher weights {αi}M
i=1 by Adaptive Knowledge Aggrega-
tor; (3) Updating student model HS by distillation loss LT A
with updated teacher weights {αi}M
i=1. The three steps are
repeated for training student model HS. This process is also
shown in Algorithm 1 in the supplementary.

Visual Understanding. To better understand the effect of
the adaptive teacher weights αi, change of αi and change
of the validation empirical risk loss LV ER during training
on Market-1501 are shown in Fig. 3. The experiment de-
tails are illustrated later in Section 5.
It can be observed
that, with the Adaptive Knowledge Aggregator, the learned
teacher weights are larger for effective teachers (trained on
large datasets MSMT17, DukeMTMC) and smaller for the
ineffective teacher model (trained on small dataset ViPER).
Compared with the case without using aggregator (i.e. using
equal teacher weights), adaptive teacher weights can further
minimize the validation empirical risk loss LV ER.

5. Experiments

We conducted extensive experiments on two large per-
son re-identiﬁcation benchmark datasets Market-1501 [66]
and DukeMTMC [70]. Our Multi-teacher Adaptive Similar-
ity Distillation Framework was evaluated and compared to
knowledge distillation, unsupervised, semi-supervised and
small sample learning methods. Further evaluations show
the effectiveness of the Log-Euclidean Similarity Distilla-
tion Loss and the Adaptive Knowledge Aggregator.

Experiment Settings and Datasets. We used Market-
1501 [66] and DukeMTMC [70] as target datasets. Source
datasets were for training teacher models. To increase the
diversity of effectiveness of teacher models for the target
scene to simulate the practical situation, datasets of dif-
ferent scales collected in different scenes were used. We
trained 5 teacher models T 1, T 2, T 3, T 4, T 5 with labelled
data in the training sets of MSMT17 [53], CUHK03 [28],
ViPER [18], DukeMTMC [70] and Market-1501 [66], re-

1191

Table 1. Basic information of datasets.

Teacher

Dataset

MSMT17 [53]
CUHK03 [28]

Images
T 1
126,441
T 2
28,192
T 3
1,264
T 4
DukeMTMC [70] 36,411
T 5 Market-1501 [66] 32,668

VIPeR [18]

Identities Cameras

Scene

4,101
1,467
632
1,812
1,501

15
2
2
8
6

outdoor, indoor

indoor
outdoor
outdoor
outdoor

spectively. Once a teacher model was trained, source data
and extra training were not needed. Basic information of the
datasets is in Table 1. Note that, when forming the teacher
model pool, teacher model of the target dataset was exclud-
ed. For Market-1501, the teacher model pool with M =
4 teachers was {T 1, T 2, T 3, T 4} (MSMT17, CUHK03,
ViPER, DukeMTMC); while for DukeMTMC, the teach-
er model pool with M = 4 teachers was {T 1, T 2, T 3, T 5}
(MSMT17, CUHK03, ViPER, Market-1501).

The standard splits of training and testing IDs of Market-
1501 and DukeMTMC were adopted as in [66] and [70].
Our experiments were conducted under two settings:
(1) Unsupervised setting: all data in training set was unla-
belled. (2) Semi-supervised setting: Concerning labelling
cost, Cv = 10 identities in training set were randomly se-
lected to be labelled, and the remaining data was unlabelled.

Performance Metrics.
In testing, similarities between
query and gallery samples were determined by the student
model. The performance metrics cumulative matching char-
acteristic (CMC) and mean Average Precision (mAP) were
applied following the standard evaluation protocol in [66]
and [70]. Note that, scalability is also important in our e-
valuations, including training time, model size indicated by
parameter number (#Para) and testing computation cost of
the model indicated by ﬂoating-point operations (FLOPs).

Implementation Details. For teacher models of source
scenes, an advanced Re-ID model PCB [47] was adopted.
For student model of the target scene, a lightweight mod-
el MobileNetV2 [46] was adopted and a convolution layer
was applied to reduce the last feature map channel number
to 256. It was initialized by ImageNet pretraining, without
training on any Re-ID dataset. The input images were re-
sized to 384 × 128 and feature maps of the last convolution
layer were extracted as feature vectors. In each batch, for
computing validation empirical risk, we sampled two im-
ages for each identity from labelled data to guarantee posi-
tive pairs. More details are provided in the supplementary.

5.1. Comparison under Unsupervised Setting

Compared Methods. For unsupervised setting, we did not
use labelled data for our method and set ﬁxed equal teacher
weights αi as 1/M in the Adaptive Aggregated Distillation
Loss LT A in Eq. (7) without using the Adaptive Knowl-
edge Aggregator. We compared with unsupervised Re-
ID methods including unsupervised features LOMO [31],
BOW [66] and unsupervised learning models UMDL [42],
PTGAN [53], PUL [14], CAMEL [62], SPGAN [13], TJ-
AIDL [51] and HHL [71]. Among them, the advanced deep

Table 2. Performance under unsupervised setting. “Ours (unsuper-
vised)” denotes the unsupervised version of our method. “Back-
bones” denotes model architecture. “#Para” denotes the number
of parameters. “FLOPs” denotes ﬂoating-point operations (testing
computation cost). “Train” denotes training time. “R-1” denotes
rank-1 accuracy (%). “mAP” denotes mean average precision (%).
DukeMTMC

Methods

Backbones

#Para FLOPs Market-1501
(M)

(G)

-
-
-

-
-
-

LOMO [31]
BOW [66]
UMDL [42]
PTGAN [53]
6.8
PUL [14]
25.6
CAMEL [62]
0.9
SPGAN [13]
25.6
TJ-AIDL [51]
4.2
HHL [71]
25.6
Fukuda [16]
MobileNetV2 3.4
Ours (unsupervised) MobileNetV2 3.4

GoogleNet
ResNet-50
ResNet-56
ResNet-50
MobileNet
ResNet-50

-
-
-

1.5
4.1
6.2
4.1
0.6
4.1
0.3
0.3

-

-

-
-
-
-
-

-
-
-
-
-

12.3 4.8
17.1 8.3
18.5 7.3
27.4
30.0 16.4

R-1 mAP Train R-1 mAP Train
27.2 8.0
35.8 14.8
34.5 12.4
38.6
45.5 20.5
54.5 26.3 13.7h 42.2 21.0 13.7h
57.7 26.7
58.2 26.5
62.2 31.4 21.0h 46.9 27.2 21.0h
45.1 23.0 1.1h 27.6 18.9 1.4h
61.5 33.5 1.1h 48.4 29.4 1.4h

46.4 26.2
44.3 23.0

-
-

-
-

models require source data for transfer learning or pretrain-
ing, while our method only requires teacher models. More-
over, we compared with a multi-teacher knowledge distilla-
tion method Fukuda et al. [16]. For evaluating scalability,
training time was tested on a TITAN X GPU. In practice,
since the model for target domain is a new user-speciﬁed
model without learning from Re-ID data, the time of pre-
training on source data was included in training time. The
results as well as the parameter number (#Para) and testing
computation costs (FLOPs) are reported in Table 2.

Results and Analysis. Our method outperformed the com-
pared unsupervised Re-ID and multi-teacher distillation
methods, except that the rank-1 accuracy on Market-1501
is slightly lower than HHL [71]. Although our method
does not require source data, the Log-Euclidean Similari-
ty Distillation Loss can effectively transfer knowledge of
source domains to the student model. The complementarity
of knowledge of multiple teacher models can increase the
generalization ability of the student model. The distillation
method Fukuda et al. [16] also exploited multiple teacher-
s, but it is based on soft labels for closed-set classiﬁcation,
which is not effective for the open-set Re-ID problem.

Scalability of the methods is compared as below. Train-
ing data required by our method contains only target data
and is smaller than the other methods that require source
data for pretraining or joint training. The computation cost
indicated by FLOPs of our backbone model MobileNetV2
is much lower than the others. With smaller training set and
lighter backbone model, the training time is much short-
er than the methods with comparable performance such as
CAMEL [62] and HHL [71]. Thus, our framework is more
scalable than the compared methods.

5.2. Comparison under Semi supervised Setting

Compared Methods. For semi-supervised setting, 10 la-
belled IDs were available,
thus the full version of our
method (“Ours (semi)”) can be applied. For comparison, we
chose two competitive recent advanced unsupervised Re-ID
methods CAMEL [62] and HHL [71] to extend to semi-

1192

Methods

Table 3. Performance (%) under semi-supervised setting with 10
labelled identities. Semi-supervised and small sample learning
methods for Re-ID were compared (same notations as Table 2).
DukeMTMC

#Para FLOPs Market-1501
(M)
25.6
DNS [63]
ResNet-50
0.9
CAMEL (semi) ResNet-56
HHL (semi)
25.6
ResNet-50
CAMEL (semi) MobileNetV2 3.4
HHL (semi)
MobileNetV2 3.4
Ours (semi)
MobileNetV2 3.4

R-1 mAP Train R-1 mAP Train
11.8 5.3 3.2h 10.0 4.6 3.2h
54.4 26.2 13.7h 42.1 21.1 13.7h
63.9 34.4 21.0h 46.7 26.5 21.0h
13.2 5.0 1.4h 13.6 5.6 1.4h
56.7 27.7 21.0h 42.9 23.9 21.0h
63.7 35.4 1.3h 57.4 36.7 1.7h

(G)
4.1
6.2
4.1
0.3
0.3
0.3

Backbones

supervised version (“CAMEL (semi)” and “HHL (semi)”)
by using the positive and negative sample pairs obtained
from these labelled samples. We also tested using Mo-
bileNetV2 trained on MSMT17 (source dataset of the best
teacher model) for CAMEL (semi) and using MobileNetV2
as backbone for HHL (semi). A small sample learning Re-
ID method DNS [63] was also compared, for which we used
a PCB [47] model trained on MSMT17 as the best teacher
model T 1. Training time was tested as the unsupervised
setting in Section 5.1. The results are reported in Table 3.

Results and Analysis. Among the compared methods, the
performance of our method is the best, except that rank-
1 accuracy is slightly lower than HHL (semi) on Market-
1501. Compared with the unsupervised results in Table 2,
CAMEL (semi) and HHL (semi) beneﬁted little from the 10
extra labelled identities. The small sample learning method
DNS failed due to overﬁtting. Compared to our unsuper-
vised version in Table 2, our method beneﬁted more from
the labelled identities especially on DukeMTMC (explained
in Section 5.3). Although 10 labelled identities can pro-
vide little information for directly learning a model, they
are sufﬁcient for our method to compute the validation em-
pirical risk to select better teacher models. Moreover, our
method can learn MobileNetV2 more effectively than HHL
and CAMEL. Scalability analysis is similar to Section 5.1.

5.3. Further Evaluations

In this section, we further evaluate and analyze the com-

ponents and capabilities of our method.
Evaluation of Knowledge Distillation. Knowledge distil-
lation is the key technique in our Multi-teacher Adaptive
Similarity Distillation Framework. To fairly compare our
Log-Euclidean Similarity Distillation Loss LT in Eq. (6)
with existing knowledge distillation losses, we evaluated
learning from a single teacher model T 1 (MSMT17). To
evaluate the effectiveness of the Log-Euclidean metric in
LT , we also tested using Euclidean metic, which is denot-
ed by “LT w/o log”. We compared with a representative
soft-label-based distillation method Hinton et al. [21] and
a recent advanced probability-distribution-based distillation
method PKT [40]. The results are reported in Table 4.

The performance of our loss LT is the best and very close
to the teacher model, the upper bound for distillation meth-
ods. The performance of Hinton et al. [21] is lower than
other methods, because it is designed for closed-set classi-

Table 4. Performance (%) of distilling a single teacher model T 1.
Our loss LT without logarithm “LT w/o log” and other knowledge
distillation methods were compared. Please see text for details.

Methods

Teacher T 1 (MSMT17)
Hinton et al. [21]
PKT [40]
LT w/o log
LT (Eq. (6))

Market-1501
R-1
51.5
41.2
46.1
47.7
49.7

mAP
24.9
20.0
22.3
23.0
24.6

DukeMTMC
R-1
47.6
32.5
44.7
45.0
47.6

mAP
30.6
20.8
28.6
29.8
31.1

ﬁcation by using soft labels, which is not suitable for the
open-set Re-ID problem. PKT [40] used probability distri-
bution for knowledge distillation, which is not so effective
as using similarity in our method for Re-ID. Comparisons
between “LT ” and “LT w/o log” show the effectiveness of
the Log-Euclidean metric, which considers the symmetric
positive deﬁnite (SPD) property of similarity matrices.

Effect of the Learned Teacher Weights αi. Another key
component in our method is the Adaptive Knowledge Ag-
gregator for learning teacher weights αi to adjust the dis-
tillation loss LT A in Eq. (7). We conducted some exper-
iments as follows: (1) Evaluating the performance of al-
l teachers individually.
(2) Ensembles of all teachers by
distance fusion with weights learned by RankSVM [43] on
the validation set and with equal weights. Joint training a
PCB model [47] with all source data. (3) Using multi-task
weighting methods Uncertainty [23] and GradNorm [10] to
learn teacher weights in our framework. (4) Training our
framework without learning teacher weights αi (“Ours (un-
supervised)”) and training our framework with subsets of
teachers of top-k αi. The backbone of our method was Mo-
bileNetV2 and the others were ResNet-50. The results and
the teacher weights αi after training are reported in Table 5.
teachers T 1
- Individual Teacher.
(MSMT17), T 2 (CUHK03) and T 4 (DukeMTMC) are ef-
fective and comparable. For DukeMTMC, only teacher T 1
(MSMT17) is effective, since DukeMTMC is more chal-
lenging with more camera views than Market-1501. For
both datasets, T 3 (ViPER) is the worst because its training
set is small. T 3 provides weak knowledge as interference to
test the robustness of the system. Our method outperformed
the best teacher by about 10% and is more lightweight.
- w/ and w/o Learning αi. Teacher weights αi learned by
Adaptive Knowledge Aggregator can indicate the effective-
ness of the teachers. The weights for the worst teacher T 3
are nearly zero. When comparing “Ours (semi)” with “Ours
(unsupervised)”, for Market-1501, the selection by teacher
weights brings limited improvement; whilst for DukeMTM-
C, the improvement is much more signiﬁcant, because dis-
tance fusion with equal weights is already better than in-
dividual teachers for Market-1501 but it is not effective for
DukeMTMC. Ensemble by distance fusion increases testing
computation costs and is not as scalable as our method.

For Market-1501,

Furthermore, we ranked αi in descending order to select
a subset of teachers for training. The teachers with large αi

1193

Table 5. Performance (%) of evaluating the Adaptive Knowledge
“αi” is the teacher weight
Aggregator for selecting teachers.
learned by the Adaptive Knowledge Aggregator. Ensembles of
teachers, joint training and task weighting were compared.

Market-1501 (teacher model pool {T 1, T 2, T 3, T 4})

Models
Teacher T 1 (MSMT17)
Teacher T 2 (CUHK03)
Teacher T 3 (ViPER)
Teacher T 4 (DukeMTMC)
RankSVM weighted fusion (all teachers)
Distance fusion (all teachers)
Joint training (all source data)
Uncertainty [23]
GradNorm [10]
Ours (unsupervised)
Ours (T i of top 1 αi {T 4})
Ours (T i of top 2 αi {T 4, T 1})
Ours (T i of top 3 αi {T 4, T 1, T 2})
Ours (semi)

#Para
25.6
25.6
25.6
25.6
102.4
102.4
25.6
3.4
3.4
3.4
3.4
3.4
3.4
3.4

FLOPs

4.1
4.1
4.1
4.1
16.4
16.4
4.1
0.3
0.3
0.3
0.3
0.3
0.3
0.3

αi
0.398
0.145
0.000
0.458

-
-
-
-
-
-
-
-
-
-

R-1
51.5
51.7
28.5
49.4
57.6
56.5
62.8
61.3
60.5
61.5
48.1
63.0
63.5
63.7

DukeMTMC (teacher model pool {T 1, T 2, T 3, T 5})

Models
Teacher T 1 (MSMT17)
Teacher T 2 (CUHK03)
Teacher T 3 (ViPER)
Teacher T 5 (Market-1501)
RankSVM weighted fusion (all teachers)
Distance fusion (all teachers)
Joint training (all source data)
Uncertainty [23]
GradNorm [10]
Ours (unsupervised)
Ours (T i of top 1 αi {T 1})
Ours (T i of top 2 αi {T 1, T 5})
Ours (T i of top 3 αi {T 1, T 5, T 2})
Ours (semi)

#Para
25.6
25.6
25.6
25.6
102.4
102.4
25.6
3.4
3.4
3.4
3.4
3.4
3.4
3.4

FLOPs

4.1
4.1
4.1
4.1
16.4
16.4
4.1
0.3
0.3
0.3
0.3
0.3
0.3
0.3

αi
0.581
0.071
0.029
0.320

-
-
-
-
-
-
-
-
-
-

R-1
47.6
25.3
19.7
30.8
41.8
39.7
53.6
51.0
44.8
48.4
47.6
57.9
57.5
57.4

mAP
24.9
26.2
12.0
24.2
31.6
30.7
35.6
33.3
32.8
33.5
23.8
34.6
35.5
35.4

mAP
30.6
14.8
10.7
18.6
28.3
26.8
36.1
30.6
26.5
29.4
31.1
36.7
36.7
36.7

Train
3.4h
1.4h
0.1h
1.7h

-
-

6.6h
1.1h
1.1h
1.1h
1.1h
1.3h
1.3h
1.3h

Train
3.4h
1.4h
0.1h
1.3h

-
-

6.2h
1.4h
1.4h
1.4h
1.4h
1.7h
1.7h
1.7h

(> 1/4) can bring signiﬁcant improvement while those with
small αi (< 1/4) cannot bring improvement because they
are weak or cannot provide complementary knowledge.
- Comparison with Ensemble and Task Weighting. Our
method outperformed ensemble,
joint training and task
weighting [23, 10]. Moreover, our testing computation cost
was much lower than ensemble and our training time was
shorter than joint training. These show the advantages of
our similarity knowledge distillation and Adaptive Knowl-
edge Aggregator for aggregating knowledge of teachers.

The Number of Validation IDs. We tested using different
numbers of validation identities from 0 to 50. As shown in
Table 6, our method can achieve comparable performance
with 5 to 50 identities. Since the validation identities are
only for learning teacher weights αi and not for training
the student model parameters, there is no overﬁtting prob-
lem even with only 1 labelled identity. Comparing using 1
ID with using 0 ID, the performance dropped signiﬁcantly
especially on DukeMTMC, which indicates the importance
of validation empirical risk. Thus, our Adaptive Knowledge
Aggregator is robust with only a few labelled identities.

Different Student Model Architectures. To show the ﬂex-
ibility of our method, we used MobileNetV2 [46], ResNet-
18 and ResNet-50 [20] as backbones for the student model,
which are with different architectures and capacities. The
results in Table 7 show that, our method achieved compara-
ble performance for all three models. Thus, knowledge can
be effectively distilled to models of different architectures.

Finetuning with Our Method as Initialization. When
more labelled data is given, the MobileNetV2 [46] studen-
t model learned by our method can be used as initializa-
tion for ﬁnetuning. We ﬁnetuned it on 20% labelled identi-

Table 6. Performance (%) of using different numbers of labelled
identities in validation set.
0

Validation set IDs

1

5

Market

DukeMTMC

R-1
mAP
R-1
mAP

61.5
33.5
48.4
29.4

62.6
34.2
55.9
35.0

63.2
34.6
57.6
36.6

10
63.7
35.4
57.4
36.7

20
63.9
35.4
57.5
36.8

50
64.2
35.8
57.9
37.1

Table 7. Performance (%) of different backbones of student model.

Backbones

ResNet-50
ResNet-18

MobileNetV2

Market-1501
R-1
63.5
63.1
63.7

mAP
35.2
34.9
35.4

DukeMTMC
R-1
56.1
56.7
57.4

mAP
35.4
36.4
36.7

Table 8. Performance (%) of ﬁnetuning MobileNetV2 [46] with
our method as initialization on a small subset of 20% IDs.
DukeMTMC
mAP
R-1
45.4
64.7
49.9
29.7
46.0
66.7

Market-1501
mAP
R-1
55.5
78.3
60.1
35.6
57.7
77.1

ImageNet
ImageNet

100%
20%
20%

Labelled IDs

Initialization

Ours

ties and compared with the models ﬁnetuned with 20% and
100% labelled identities initialized by ImageNet pretrain-
ing. The results are shown in Table 8. The model initialized
by our method ﬁnetuned on 20% IDs can achieve compa-
rable performance with the model initialized by ImageNet
pretraining ﬁnetuned on 100% IDs, while the performance
of ﬁnetuning on 20% IDs with ImageNet pretraining was
much lower. With prior knowledge of Re-ID, our method
can generalize better with fewer target labelled samples.

6. Conclusion

In this paper, we aim to address the scalability of per-
son re-identiﬁcation from three aspects, including labelling
cost, extension cost and testing computation cost. We
propose a Multi-teacher Adaptive Similarity Distillation
Framework, which can ﬂexibly train a new user-speciﬁed
lightweight model, with only a few labelled identities and
without source data. The framework stores knowledge of
Re-ID in a teacher model pool. When extending to a new
scene, knowledge can be adaptively aggregated and distilled
to a lightweight student model. For knowledge distillation
for Re-ID, an open-set identiﬁcation problem, we propose
the Log-Euclidean Similarity Distillation Loss to imitate
the sample pairwise similarity matrix of the teacher mod-
el. To effectively learn from multiple teachers, we propose
the Adaptive Knowledge Aggregator to adjust the contri-
bution of each teacher model by minimizing the validation
empirical risk computed on a few labelled identities. Ex-
tensive evaluations show that our method is more scalable
and can achieve performance comparable to state-of-the-art
unsupervised and semi-supervised Re-ID methods.

Acknowledgement

This work was supported partially by the National Key
Research and Development Program of China (2016YF-
B1001002), NSFC(61522115, 61573387, U1611461),
Guangdong Province Science and Technology Innovation
Leading Talents (2016TX03X157), and the Royal Society
Newton Advanced Fellowship (NA150459).

1194

References

[1] Ejaz Ahmed, Michael

and Tim K. Marks.
An improved deep learning architecture for person re-
identiﬁcation. In CVPR, 2015. 1, 2

Jones,

[2] TM Feroz Ali and Subhasis Chaudhuri. Maximum margin
metric learning over discriminative nullspace for person re-
identiﬁcation. In CVPR, 2018. 2

[3] Xiang Li Ao, Shuang and Charles X. Ling. Fast generalized
distillation for semi-supervised domain adaptation. In AAAI,
2017. 2

[4] Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas
Ayache. Fast and simple calculus on tensors in the log-
euclidean framework. In MICCAI, 2005. 3

[5] Song Bai, Xiang Bai, and Qi Tian. Scalable person re-
identiﬁcation on supervised smoothed manifold. In CVPR,
2017. 2

[6] Slawomir Bak and Peter Carr. One-shot metric learning for

person re-identiﬁcation. In CVPR, 2017. 2

[7] Slawomir Bak, Peter Carr, and Jean-Francois Lalonde. Do-
main adaptation through synthesis for unsupervised person
re-identiﬁcation. In ECCV, 2018. 2

[8] Jiaxin Chen, Yunhong Wang, Jie Qin, Li Liu, and Ling Shao.
Fast person re-identiﬁcation via cross-camera semantic bina-
ry transformation. In CVPR, 2017. 2

[9] Ying-Cong Chen, Wei-Shi Zheng, Jian-Huang Lai, and Pong
Yuen. An asymmetric distance model for cross-view feature
mapping in person re-identiﬁcation. IEEE TCSVT, 2015. 2

[10] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and An-
drew Rabinovich. Gradnorm: Gradient normalization for
adaptive loss balancing in deep multitask networks. In ICM-
L, 2018. 7, 8

[11] De Cheng, Yihong Gong, Zhihui Li, Dingwen Zhang, Wei-
wei Shi, and Xingjun Zhang. Cross-scenario transfer metric
learning for person re-identiﬁcation. PR, 2018. 1, 2

[12] Boris Chidlovskii, Stephane Clinchant, and Gabriela Csurka.
Domain adaptation in the absence of source domain data. In
SIGKDD, 2016. 2

[13] Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi
Yang, and Jianbin Jiao. Image-image domain adaptation with
preserved self-similarity and domain-dissimilarity for person
reidentiﬁcation. In CVPR, 2018. 2, 6

[14] Hehe Fan, Liang Zheng, Chenggang Yan, and Yi Yang.
Unsupervised person re-identiﬁcation: Clustering and ﬁne-
tuning. ACM Transactions on Multimedia Computing, Com-
munications, and Applications (TOMM), 2018. 2, 6

[15] Michela Farenzena, Loris Bazzani, Alessandro Perina, Vitto-
rio Murino, and Marco Cristani. Person re-identiﬁcation by
symmetry-driven accumulation of local features. In CVPR,
2010. 2

[16] Takashi Fukuda, Masayuki Suzuki, Gakuto Kurata, Samuel
Thomas, Jia Cui, and Bhuvana Ramabhadran. Efﬁcien-
t knowledge distillation from an ensemble of teachers. Proc.
Interspeech, 2017. 2, 3, 6

[18] Douglas Gray, Shane Brennan, and Hai Tao. Evaluating ap-
pearance models for recognition, reacquisition, and tracking.
In PETS, 2007. 5, 6

[19] Douglas Gray and Hai Tao. Viewpoint invariant pedestrian
recognition with an ensemble of localized features. In ECCV.
2008. 2

[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 2, 8

[21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the
knowledge in a neural network. In NeurIP workshop, 2015.
2, 3, 7

[22] Lin Ji, Liangliang Ren, Jiwen Lu, Jianjiang Feng, and
Zhou Jie. Consistent-aware deep learning for person re-
identiﬁcation in a camera network. In CVPR, 2017. 2

[23] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geome-
try and semantics. In CVPR, 2018. 7, 8

[24] Elyor Kodirov, Tao Xiang, Zhenyong Fu, and Shaogang
Gong. Person re-identiﬁcation by unsupervised ℓ1 graph
learning. In ECCV, 2016. 1, 2

[25] Martin K¨ostinger, Martin Hirzer, Paul Wohlhart, Peter M
Roth, and Horst Bischof. Large scale metric learning from
equivalence constraints. In CVPR, 2012. 1, 2

[26] Ilja Kuzborskij and Francesco Orabona. Stability and hy-

pothesis transfer learning. In ICML, 2013. 2

[27] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsupervised
person re-identiﬁcation by deep learning tracklet association.
In ECCV, 2018. 2

[28] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-
reid: Deep ﬁlter pairing neural network for person re-
identiﬁcation. In CVPR, 2014. 1, 2, 5, 6

[29] Xiang Li, Wei-Shi Zheng, Xiaojuan Wang, Tao Xiang, and
Shaogang Gong. Multi-scale learning for low-resolution per-
son re-identiﬁcation. In ICCV, 2015. 2

[30] Zhen Li, Shiyu Chang, Feng Liang, Thomas S Huang, Lian-
gliang Cao, and John R Smith. Learning locally-adaptive
decision functions for person veriﬁcation. In CVPR, 2013. 2

[31] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z Li. Per-
son re-identiﬁcation by local maximal occurrence represen-
tation and metric learning. In CVPR, 2015. 1, 2, 6

[32] Chunxiao Liu, Shaogang Gong, Chen Change Loy, and X-
inggang Lin. Person re-identiﬁcation: what features are im-
portant? In ECCV Workshop, 2012. 2

[33] Lianyang Ma, Xiaokang Yang, and Dacheng Tao. Person
re-identiﬁcation over camera networks using multi-task dis-
tance metric learning. IEEE TIP, 2014. 2

[34] Niki Martinel, Abir Das, Christian Micheloni, and Amit K
Roy-Chowdhury. Temporal model adaptation for person re-
identiﬁcation. In ECCV, 2016. 2

[35] T. Matsukawa, T. Okabe, E. Suzuki, and Y. Sato. Hierarchi-
cal gaussian descriptor for person re-identiﬁcation. In CVPR,
2016. 2

[17] Chen Gong, Xiaojun Chang, Meng Fang, and Jian Yang.
Teaching semi-supervised classiﬁer via generalized distilla-
tion. In IJCAI, 2018. 2

[36] Alexis Mignon and Fr´ed´eric Jurie. Pcca: A new approach for
distance learning from sparse pairwise constraints. In CVPR,
2012. 2

1195

[37] Asit Mishra and Debbie Marr. Apprentice: Using knowledge
distillation techniques to improve low-precision network ac-
curacy. arXiv preprint arXiv:1711.05852, 2017. 2, 3

[38] Sakrapee Paisitkriangkrai, Chunhua Shen,

and Anton
van den Hengel. Learning to rank in person re-identiﬁcation
with metric ensembles. In CVPR, 2015. 2

[39] Rameswar Panda, Amran Bhuiyan, Vittorio Murino, and
Amit K Roy-Chowdhury.
Unsupervised adaptive re-
identiﬁcation in open world dynamic camera networks. In
CVPR, 2017. 2

[40] Nikolaos Passalis and Anastasios Tefas. Learning deep rep-
resentations with probabilistic knowledge transfer. In ECCV,
2018. 2, 7

[41] Sateesh Pedagadi, James Orwell, Sergio Velastin, and
Boghos Boghossian. Local ﬁsher discriminant analysis for
pedestrian re-identiﬁcation. In CVPR, 2013. 2

[42] Peixi Peng, Tao Xiang, Yaowei Wang, Massimiliano Pon-
til, Shaogang Gong, Tiejun Huang, and Yonghong Tian.
Unsupervised cross-dataset transfer learning for person re-
identiﬁcation. In CVPR, 2016. 1, 2, 6

[43] Bryan Prosser, Wei-Shi Zheng, Shaogang Gong, Tao Xiang,
and Q Mary. Person re-identiﬁcation by support vector rank-
ing. In BMVC, 2010. 2, 7

[44] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:1412.6550,
2014. 2

[45] Sourya Roy, Sujoy Paul, Neal E Young, and Amit K Roy-
Chowdhury. Exploiting transitivity for learning person re-
identiﬁcation models on a budget. In CVPR, 2018. 2

[46] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In CVPR, 2018. 6, 8

[47] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin
Wang. Beyond part models: Person retrieval with reﬁned
part pooling. In ECCV, 2018. 1, 2, 3, 6, 7

[48] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In NeurIP, 2017. 2

[49] Rahul Rama Varior, Mrinal Haloi, and Gang Wang. Gated
siamese convolutional neural network architecture for human
re-identiﬁcation. In ECCV, 2016. 2

[50] Hanxiao Wang, Shaogang Gong, Xiatian Zhu, and Tao Xi-
ang. Human-in-the-loop person re-identiﬁcation. In ECCV,
2016. 2

[51] Jingya Wang, Xiatian Zhu, Shaogang Gong, and Wei Li.
Transferable joint attribute-identity deep learning for unsu-
pervised person re-identiﬁcation. In CVPR, 2018. 1, 2, 6

[52] Xiaojuan Wang, Wei-Shi Zheng, Xiang Li, and Jianguo
Zhang. Cross-scenario transfer person reidentiﬁcation. IEEE
TCSVT, 2016. 1, 2

[53] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identiﬁcation. In CVPR, 2018. 2, 5, 6

[54] Kilian Q Weinberger, John Blitzer, and Lawrence K Saul.
Distance metric learning for large margin nearest neighbor
classiﬁcation. In NeurIP, 2005. 2, 3

[55] Shangxuan Wu, Ying-Cong Chen, Xiang Li, Ancong Wu,
Jinjie You, and Wei-Shi Zheng. An enhanced deep feature
representation for person re-identiﬁcation. In WACV, 2016.
2

[56] Weiji Wu, Ancong Wu, and Wei-Shi Zheng. Light person

re-identiﬁcation by multi-cue tiny net. In ICIP, 2018. 1, 2

[57] Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang
Wang. Learning deep feature representations with domain
guided dropout for person re-identiﬁcation. In CVPR, 2016.
2

[58] Fei Xiong, Mengran Gou, Octavia Camps, and Mario Sz-
naier. Person re-identiﬁcation using kernel-based metric
learning methods. In ECCV, 2014. 2

[59] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A
gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. In CVPR, 2017. 2

[60] Jinjie You, Ancong Wu, Xiang Li, and Wei-Shi Zheng. Top-
push video-based person re-identiﬁcation. In CVPR, 2016.
2

[61] Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning

from multiple teacher networks. In SIGKDD, 2017. 2

[62] Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Cross-
view asymmetric metric learning for unsupervised person re-
identiﬁcation. In ICCV, 2017. 1, 2, 6

[63] Li Zhang, Tao Xiang, and Shaogang Gong. Learning a dis-
criminative null space for person re-identiﬁcation. In CVPR,
2016. 2, 7

[64] Haiyu Zhao, Maoqing Tian, Shuyang Sun, Shao Jing, Junjie
Yan, Yi Shuai, Xiaogang Wang, and Xiaoou Tang. Spindle
net: Person re-identiﬁcation with human body region guided
feature decomposition and fusion. In CVPR, 2017. 2

[65] Liming Zhao, Li Xi, Yueting Zhuang, and Jingdong Wang.
Deeply-learned part-aligned representations for person re-
identiﬁcation. In ICCV, 2017. 2

[66] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiﬁcation:
A benchmark. In ICCV, 2015. 5, 6

[67] Wei-Shi Zheng, Shaogang Gong, and Tao Xiang. Reidentiﬁ-
cation by relative distance comparison. IEEE TPAMI, 2013.
1, 2

[68] Wei-Shi Zheng, Shaogang Gong, and Tao Xiang. Towards
open-world person re-identiﬁcation by one-shot group-based
veriﬁcation. IEEE TPAMI, 2016. 2

[69] Wei-Shi Zheng, Xiang Li, Tao Xiang, Shengcai Liao,
Partial person re-

Jianhuang Lai, and Shaogang Gong.
identiﬁcation. In ICCV, 2015. 2

[70] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identiﬁcation
baseline in vitro. In ICCV, 2017. 2, 5, 6

[71] Zhun Zhong, Liang Zheng, Shaozi Li, and Yi Yang. Gener-
alizing a person retrieval model hetero-and homogeneously.
In ECCV, 2018. 2, 6

[72] Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li,
Camera style adaptation for person re-

and Yi Yang.
identiﬁcation. In CVPR, 2018. 2

[73] Xiatian Zhu, Botong Wu, Dongcheng Huang, and Wei-Shi
Zheng. Fast open-world person re-identiﬁcation. IEEE TIP,
2017. 1, 2

1196

