Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance

for Cross-View Image Translation

Hao Tang1

,

2* Dan Xu3* Nicu Sebe1

,

4 Yanzhi Wang5 Jason J. Corso6 Yan Yan2

1DISI, University of Trento, Trento, Italy
2Texas State University, San Marcos, USA
3University of Oxford, Oxford, UK 4Huawei Technologies Ireland, Dublin, Ireland
5Northeastern University, Boston, USA 6University of Michigan, Ann Arbor, USA

Abstract

Cross-view image translation is challenging because it
involves images with drastically different views and severe
deformation. In this paper, we propose a novel approach
named Multi-Channel Attention SelectionGAN (Selection-
GAN) that makes it possible to generate images of nat-
ural scenes in arbitrary viewpoints, based on an image
of the scene and a novel semantic map. The proposed
SelectionGAN explicitly utilizes the semantic information
and consists of two stages.
In the ﬁrst stage, the condi-
tion image and the target semantic map are fed into a cy-
cled semantic-guided generation network to produce initial
coarse results. In the second stage, we reﬁne the initial re-
sults by using a multi-channel attention selection mecha-
nism. Moreover, uncertainty maps automatically learned
from attentions are used to guide the pixel loss for bet-
ter network optimization. Extensive experiments on Day-
ton [41], CVUSA [43] and Ego2Top [1] datasets show that
our model is able to generate signiﬁcantly better results
than the state-of-the-art methods. The source code, data
and trained models are available at https://github.
com/Ha0Tang/SelectionGAN .

1. Introduction

Cross-view image translation is a task that aims at syn-
thesizing new images from one viewpoint to another. It has
been gaining a lot interest especially from computer vision
and virtual reality communities, and has been widely inves-
tigated in recent years [40, 20, 54, 34, 47, 15, 31, 52, 45].
Earlier works studied this problem using encoder-decoder
Convolutional Neural Networks (CNNs) by involving view-
point codes in the bottle-neck representations for city scene
synthesis [52] and 3D object translation [45]. There also
exist some works exploring Generative Adversarial Net-

*Equal contribution.

Figure 1: Examples of our cross-view translation results on
two public benchmarks i.e. Dayton [41] and CVUSA [43],
and on our self-created large-scale benchmark based on
Ego2Top [1].
works (GAN) for similar tasks [31]. However, these ex-
isting works consider an application scenario in which the
objects and the scenes have a large degree of overlapping in
appearances and views.

Different from previous works, in this paper, we focus on
a more challenging setting in which ﬁelds of views have lit-
tle or even no overlap, leading to signiﬁcantly distinct struc-
tures and appearance distributions for the input source and
the output target views, as illustrated in Fig. 1. To tackle this
challenging problem, Regmi and Borji [34] recently pro-
posed a conditional GAN model which jointly learns the
generation in both the image domain and the corresponding
semantic domain, and the semantic predictions are further
utilized to supervise the image generation. Although this
approach performed an interesting exploration, we observe
unsatisfactory aspects mainly in the generated scene struc-
ture and details, which are due to different reasons. First,
since it is always costly to obtain manually annotated se-
mantic labels, the label maps are usually produced from
pretrained semantic models from other large-scale segmen-
tation datasets, leading to insufﬁciently accurate predictions
for all the pixels, and thus misguiding the image genera-
tion. Second, we argue that the translation with a single

12417

Uncertainty

Maps

Uncertainty

Maps

Sg

Ia

C

Ig

Gi

`1<

I

0

g

Gs

Fi

Fs

`2<

S

0

g

C

Stage I

`3

00

I

g

Ig

Gs

`4<

00

S

g

Final 
Output

Multi-Channel 

Attention 
Selection 
Module 

Ga

Stage II

Figure 2: Overview of the proposed SelectionGAN. Stage I presents a cycled semantic-guided generation sub-network which
accepts images from one view and conditional semantic maps and simultaneously synthesizes images and semantic maps
in another view. Stage II takes the coarse predictions and the learned deep semantic features from stage I, and performs a
ﬁne-grained generation using the proposed multi-channel attention selection module.

phase generation network is not able to capture the com-
plex scene structural relationships between the two views.
Third, a three-channel generation space may not be suitable
enough for learning a good mapping for this complex syn-
thesis problem. Given these problems, could we enlarge the
generation space and learn an automatic selection mecha-
nism to synthesize more ﬁne-grained generation results?

Based on these observations, in this paper, we propose a
novel Multi-Channel Attention Selection Generative Adver-
sarial Network (SelectionGAN), which contains two gener-
ation stages. The overall framework of the proposed Selec-
tionGAN is shown in Fig. 2. In this ﬁrst stage, we learn a
cycled image-semantic generation sub-network, which ac-
cepts a pair consisting of an image and the target seman-
tic map, and generates images for the other view, which
further fed into a semantic generation network to recon-
struct the input semantic maps. This cycled generation adds
more strong supervision between the image and semantic
domains, facilitating the optimization of the network.

The coarse outputs from the ﬁrst generation network,
including the input image, together with the deep feature
maps from the last layer, are input into the second stage
networks. Several intermediate outputs are produced, and
simultaneously we learn a set of multi-channel attention
maps with the same number as the intermediate generations.
These attention maps are used to spatially select from the
intermediate generations, and are combined to synthesize
a ﬁnal output. Finally, to overcome the inaccurate seman-
tic label issue, the multi-channel attention maps are further
used to generate uncertainty maps to guide the reconstruc-
tion loss. Through extensive experimental evaluations, we
demonstrate that SelectionGAN produces remarkably bet-
ter results than the baselines such as Pix2pix [16], Zhai et
al. [47], X-Fork [34] and X-Seq [34]. Moreover, we estab-
lish state-of-the-art results on three different datasets for the

arbitrary cross-view image synthesis task.

Overall, the contributions of this paper are as follows:

• A novel multi-channel attention selection GAN frame-
work (SelectionGAN) for the cross-view image transla-
tion task is presented. It explores cascaded semantic guid-
ance with a coarse-to-ﬁne inference, and aims at produc-
ing a more detailed synthesis from richer and more di-
verse multiple intermediate generations.

• A novel multi-channel attention selection module is pro-
posed, which is utilized to attentively select interested in-
termediate generations and is able to signiﬁcantly boost
the quality of the ﬁnal output. The multi-channel atten-
tion module also effectively learns uncertainty maps to
guide the pixel loss for more robust optimization.

• Extensive experiments clearly demonstrate the effective-
ness of the proposed SelectionGAN, and show state-of-
the-art results on two public benchmarks, i.e. Dayton [41]
and CVUSA [43]. Meanwhile, we also create a larger-
scale cross-view synthesis benchmark using the data from
Ego2Top [1], and present results of multiple baseline
models for the research community.

2. Related Work
Generative Adversarial Networks (GANs) [11] have
shown the capability of generating better high-quality im-
ages [42, 18, 12], compared to existing methods such as
Restricted Boltzmann Machines [13, 35] and Deep Boltz-
mann Machines [14]. A vanilla GAN model [11] has two
important components, i.e. a generator G and a discrimina-
tor D. The goal of G is to generate photo-realistic images
from a noise vector, while D is trying to distinguish be-
tween a real image and the image generated by G. Although
it is successfully used in generating images of high visual ﬁ-
delity [18, 48, 32], there are still some challenges, i.e. how
to generate images in a controlled setting. To generate
domain-speciﬁc images, Conditional GAN (CGAN) [27]

2418

l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
a
C
o
a
G
8
v
4
A
0
f
B
u
x
E
k
3
G
E
w
h
h
i
/
z
q
w
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
A
F
/
E
U
w
T
w
g
W
c
L
s
p
D
c
Z
M
z
u
z
z
M
w
K
I
e
Q
f
v
H
h
Q
x
K
v
/
4
8
2
/
c
Z
L
s
Q
R
M
L
G
o
q
q
b
r
q
7
o
l
R
w
Y
3
3
/
2
y
u
s
r
W
9
s
b
h
W
3
S
z
u
7
e
/
s
H
5
c
O
j
p
l
G
Z
Z
t
h
g
S
i
j
d
j
q
h
B
w
S
U
2
L
L
c
C
2
6
l
G
m
k
Q
C
W
9
H
o
Z
u
a
3
n
l
A
b
r
u
S
D
H
a
c
Y
J
n
Q
g
e
c
w
Z
t
U
5
q
d
l
G
I
X
t
A
r
V
/
y
q
P
w
d
Z
J
U
F
O
K
p
C
j
3
i
t
/
d
f
u
K
Z
Q
l
K
y
w
Q
1
p
h
P
4
q
Q
0
n
V
F
v
O
B
E
5
L
3
c
x
g
S
t
m
I
D
r
D
j
q
K
Q
J
m
n
A
y
v
3
Z
K
z
p
z
S
J
7
H
S
r
q
Q
l
c
/
X
3
x
I
Q
m
x
o
y
T
y
H
U
m
1
A
7
N
s
j
c
T
/
/
M
6
m
Y
2
v
w
w
m
X
a
W
Z
R
s
s
W
i
O
B
P
E
K
j
J
7
n
f
S
5
R
m
b
F
2
B
H
K
N
H
e
3
E
j
a
k
m
j
L
r
A
i
q
5
E
I
L
l
l
1
d
J
8
6
I
a
+
N
X
g
/
r
J
S
u
8
v
j
K
M
I
J
n
M
I
5
B
H
A
F
N
b
i
F
O
j
S
A
w
S
M
8
w
y
u
8
e
c
p
7
8
d
6
9
j
0
V
r
w
c
t
n
j
u
E
P
v
M
8
f
N
4
S
O
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
a
C
o
a
G
8
v
4
A
0
f
B
u
x
E
k
3
G
E
w
h
h
i
/
z
q
w
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
A
F
/
E
U
w
T
w
g
W
c
L
s
p
D
c
Z
M
z
u
z
z
M
w
K
I
e
Q
f
v
H
h
Q
x
K
v
/
4
8
2
/
c
Z
L
s
Q
R
M
L
G
o
q
q
b
r
q
7
o
l
R
w
Y
3
3
/
2
y
u
s
r
W
9
s
b
h
W
3
S
z
u
7
e
/
s
H
5
c
O
j
p
l
G
Z
Z
t
h
g
S
i
j
d
j
q
h
B
w
S
U
2
L
L
c
C
2
6
l
G
m
k
Q
C
W
9
H
o
Z
u
a
3
n
l
A
b
r
u
S
D
H
a
c
Y
J
n
Q
g
e
c
w
Z
t
U
5
q
d
l
G
I
X
t
A
r
V
/
y
q
P
w
d
Z
J
U
F
O
K
p
C
j
3
i
t
/
d
f
u
K
Z
Q
l
K
y
w
Q
1
p
h
P
4
q
Q
0
n
V
F
v
O
B
E
5
L
3
c
x
g
S
t
m
I
D
r
D
j
q
K
Q
J
m
n
A
y
v
3
Z
K
z
p
z
S
J
7
H
S
r
q
Q
l
c
/
X
3
x
I
Q
m
x
o
y
T
y
H
U
m
1
A
7
N
s
j
c
T
/
/
M
6
m
Y
2
v
w
w
m
X
a
W
Z
R
s
s
W
i
O
B
P
E
K
j
J
7
n
f
S
5
R
m
b
F
2
B
H
K
N
H
e
3
E
j
a
k
m
j
L
r
A
i
q
5
E
I
L
l
l
1
d
J
8
6
I
a
+
N
X
g
/
r
J
S
u
8
v
j
K
M
I
J
n
M
I
5
B
H
A
F
N
b
i
F
O
j
S
A
w
S
M
8
w
y
u
8
e
c
p
7
8
d
6
9
j
0
V
r
w
c
t
n
j
u
E
P
v
M
8
f
N
4
S
O
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
a
C
o
a
G
8
v
4
A
0
f
B
u
x
E
k
3
G
E
w
h
h
i
/
z
q
w
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
A
F
/
E
U
w
T
w
g
W
c
L
s
p
D
c
Z
M
z
u
z
z
M
w
K
I
e
Q
f
v
H
h
Q
x
K
v
/
4
8
2
/
c
Z
L
s
Q
R
M
L
G
o
q
q
b
r
q
7
o
l
R
w
Y
3
3
/
2
y
u
s
r
W
9
s
b
h
W
3
S
z
u
7
e
/
s
H
5
c
O
j
p
l
G
Z
Z
t
h
g
S
i
j
d
j
q
h
B
w
S
U
2
L
L
c
C
2
6
l
G
m
k
Q
C
W
9
H
o
Z
u
a
3
n
l
A
b
r
u
S
D
H
a
c
Y
J
n
Q
g
e
c
w
Z
t
U
5
q
d
l
G
I
X
t
A
r
V
/
y
q
P
w
d
Z
J
U
F
O
K
p
C
j
3
i
t
/
d
f
u
K
Z
Q
l
K
y
w
Q
1
p
h
P
4
q
Q
0
n
V
F
v
O
B
E
5
L
3
c
x
g
S
t
m
I
D
r
D
j
q
K
Q
J
m
n
A
y
v
3
Z
K
z
p
z
S
J
7
H
S
r
q
Q
l
c
/
X
3
x
I
Q
m
x
o
y
T
y
H
U
m
1
A
7
N
s
j
c
T
/
/
M
6
m
Y
2
v
w
w
m
X
a
W
Z
R
s
s
W
i
O
B
P
E
K
j
J
7
n
f
S
5
R
m
b
F
2
B
H
K
N
H
e
3
E
j
a
k
m
j
L
r
A
i
q
5
E
I
L
l
l
1
d
J
8
6
I
a
+
N
X
g
/
r
J
S
u
8
v
j
K
M
I
J
n
M
I
5
B
H
A
F
N
b
i
F
O
j
S
A
w
S
M
8
w
y
u
8
e
c
p
7
8
d
6
9
j
0
V
r
w
c
t
n
j
u
E
P
v
M
8
f
N
4
S
O
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
a
C
o
a
G
8
v
4
A
0
f
B
u
x
E
k
3
G
E
w
h
h
i
/
z
q
w
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
A
F
/
E
U
w
T
w
g
W
c
L
s
p
D
c
Z
M
z
u
z
z
M
w
K
I
e
Q
f
v
H
h
Q
x
K
v
/
4
8
2
/
c
Z
L
s
Q
R
M
L
G
o
q
q
b
r
q
7
o
l
R
w
Y
3
3
/
2
y
u
s
r
W
9
s
b
h
W
3
S
z
u
7
e
/
s
H
5
c
O
j
p
l
G
Z
Z
t
h
g
S
i
j
d
j
q
h
B
w
S
U
2
L
L
c
C
2
6
l
G
m
k
Q
C
W
9
H
o
Z
u
a
3
n
l
A
b
r
u
S
D
H
a
c
Y
J
n
Q
g
e
c
w
Z
t
U
5
q
d
l
G
I
X
t
A
r
V
/
y
q
P
w
d
Z
J
U
F
O
K
p
C
j
3
i
t
/
d
f
u
K
Z
Q
l
K
y
w
Q
1
p
h
P
4
q
Q
0
n
V
F
v
O
B
E
5
L
3
c
x
g
S
t
m
I
D
r
D
j
q
K
Q
J
m
n
A
y
v
3
Z
K
z
p
z
S
J
7
H
S
r
q
Q
l
c
/
X
3
x
I
Q
m
x
o
y
T
y
H
U
m
1
A
7
N
s
j
c
T
/
/
M
6
m
Y
2
v
w
w
m
X
a
W
Z
R
s
s
W
i
O
B
P
E
K
j
J
7
n
f
S
5
R
m
b
F
2
B
H
K
N
H
e
3
E
j
a
k
m
j
L
r
A
i
q
5
E
I
L
l
l
1
d
J
8
6
I
a
+
N
X
g
/
r
J
S
u
8
v
j
K
M
I
J
n
M
I
5
B
H
A
F
N
b
i
F
O
j
S
A
w
S
M
8
w
y
u
8
e
c
p
7
8
d
6
9
j
0
V
r
w
c
t
n
j
u
E
P
v
M
8
f
N
4
S
O
5
w
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
N
F
f
T
2
+
T
f
L
2
v
M
r
h
u
H
Y
v
e
d
0
1
G
z
S
w
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
z
1
G
e
M
r
6
t
H
L
Y
B
A
8
h
d
0
g
6
D
H
g
R
T
x
F
M
A
9
I
l
j
A
7
6
U
3
G
z
M
4
s
M
7
N
C
C
P
k
H
L
x
4
U
8
e
r
/
e
P
N
v
n
C
R
7
0
M
S
C
h
q
K
q
m
+
6
u
K
B
X
c
W
N
/
/
9
t
b
W
N
z
a
3
t
g
s
7
x
d
2
9
/
Y
P
D
0
t
F
x
0
6
h
M
M
2
w
w
J
Z
R
u
R
9
S
g
4
B
I
b
l
l
u
B
7
V
Q
j
T
S
K
B
r
W
h
0
M
/
N
b
T
6
g
N
V
/
L
B
j
l
M
M
E
z
q
Q
P
O
a
M
W
i
c
1
u
y
h
E
r
9
o
r
l
f
2
K
P
w
d
Z
J
U
F
O
y
p
C
j
3
i
t
9
d
f
u
K
Z
Q
l
K
y
w
Q
1
p
h
P
4
q
Q
0
n
V
F
v
O
B
E
6
L
3
c
x
g
S
t
m
I
D
r
D
j
q
K
Q
J
m
n
A
y
v
3
Z
K
z
p
3
S
J
7
H
S
r
q
Q
l
c
/
X
3
x
I
Q
m
x
o
y
T
y
H
U
m
1
A
7
N
s
j
c
T
/
/
M
6
m
Y
2
v
w
w
m
X
a
W
Z
R
s
s
W
i
O
B
P
E
K
j
J
7
n
f
S
5
R
m
b
F
2
B
H
K
N
H
e
3
E
j
a
k
m
j
L
r
A
i
q
6
E
I
L
l
l
1
d
J
s
1
o
J
/
E
p
w
f
1
m
u
3
e
V
x
F
O
A
U
z
u
A
C
A
r
i
C
G
t
x
C
H
R
r
A
4
B
G
e
4
R
X
e
P
O
W
9
e
O
/
e
x
6
J
1
z
c
t
n
T
u
A
P
v
M
8
f
O
Q
i
O
6
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
N
F
f
T
2
+
T
f
L
2
v
M
r
h
u
H
Y
v
e
d
0
1
G
z
S
w
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
z
1
G
e
M
r
6
t
H
L
Y
B
A
8
h
d
0
g
6
D
H
g
R
T
x
F
M
A
9
I
l
j
A
7
6
U
3
G
z
M
4
s
M
7
N
C
C
P
k
H
L
x
4
U
8
e
r
/
e
P
N
v
n
C
R
7
0
M
S
C
h
q
K
q
m
+
6
u
K
B
X
c
W
N
/
/
9
t
b
W
N
z
a
3
t
g
s
7
x
d
2
9
/
Y
P
D
0
t
F
x
0
6
h
M
M
2
w
w
J
Z
R
u
R
9
S
g
4
B
I
b
l
l
u
B
7
V
Q
j
T
S
K
B
r
W
h
0
M
/
N
b
T
6
g
N
V
/
L
B
j
l
M
M
E
z
q
Q
P
O
a
M
W
i
c
1
u
y
h
E
r
9
o
r
l
f
2
K
P
w
d
Z
J
U
F
O
y
p
C
j
3
i
t
9
d
f
u
K
Z
Q
l
K
y
w
Q
1
p
h
P
4
q
Q
0
n
V
F
v
O
B
E
6
L
3
c
x
g
S
t
m
I
D
r
D
j
q
K
Q
J
m
n
A
y
v
3
Z
K
z
p
3
S
J
7
H
S
r
q
Q
l
c
/
X
3
x
I
Q
m
x
o
y
T
y
H
U
m
1
A
7
N
s
j
c
T
/
/
M
6
m
Y
2
v
w
w
m
X
a
W
Z
R
s
s
W
i
O
B
P
E
K
j
J
7
n
f
S
5
R
m
b
F
2
B
H
K
N
H
e
3
E
j
a
k
m
j
L
r
A
i
q
6
E
I
L
l
l
1
d
J
s
1
o
J
/
E
p
w
f
1
m
u
3
e
V
x
F
O
A
U
z
u
A
C
A
r
i
C
G
t
x
C
H
R
r
A
4
B
G
e
4
R
X
e
P
O
W
9
e
O
/
e
x
6
J
1
z
c
t
n
T
u
A
P
v
M
8
f
O
Q
i
O
6
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
N
F
f
T
2
+
T
f
L
2
v
M
r
h
u
H
Y
v
e
d
0
1
G
z
S
w
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
z
1
G
e
M
r
6
t
H
L
Y
B
A
8
h
d
0
g
6
D
H
g
R
T
x
F
M
A
9
I
l
j
A
7
6
U
3
G
z
M
4
s
M
7
N
C
C
P
k
H
L
x
4
U
8
e
r
/
e
P
N
v
n
C
R
7
0
M
S
C
h
q
K
q
m
+
6
u
K
B
X
c
W
N
/
/
9
t
b
W
N
z
a
3
t
g
s
7
x
d
2
9
/
Y
P
D
0
t
F
x
0
6
h
M
M
2
w
w
J
Z
R
u
R
9
S
g
4
B
I
b
l
l
u
B
7
V
Q
j
T
S
K
B
r
W
h
0
M
/
N
b
T
6
g
N
V
/
L
B
j
l
M
M
E
z
q
Q
P
O
a
M
W
i
c
1
u
y
h
E
r
9
o
r
l
f
2
K
P
w
d
Z
J
U
F
O
y
p
C
j
3
i
t
9
d
f
u
K
Z
Q
l
K
y
w
Q
1
p
h
P
4
q
Q
0
n
V
F
v
O
B
E
6
L
3
c
x
g
S
t
m
I
D
r
D
j
q
K
Q
J
m
n
A
y
v
3
Z
K
z
p
3
S
J
7
H
S
r
q
Q
l
c
/
X
3
x
I
Q
m
x
o
y
T
y
H
U
m
1
A
7
N
s
j
c
T
/
/
M
6
m
Y
2
v
w
w
m
X
a
W
Z
R
s
s
W
i
O
B
P
E
K
j
J
7
n
f
S
5
R
m
b
F
2
B
H
K
N
H
e
3
E
j
a
k
m
j
L
r
A
i
q
6
E
I
L
l
l
1
d
J
s
1
o
J
/
E
p
w
f
1
m
u
3
e
V
x
F
O
A
U
z
u
A
C
A
r
i
C
G
t
x
C
H
R
r
A
4
B
G
e
4
R
X
e
P
O
W
9
e
O
/
e
x
6
J
1
z
c
t
n
T
u
A
P
v
M
8
f
O
Q
i
O
6
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
N
F
f
T
2
+
T
f
L
2
v
M
r
h
u
H
Y
v
e
d
0
1
G
z
S
w
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
z
1
G
e
M
r
6
t
H
L
Y
B
A
8
h
d
0
g
6
D
H
g
R
T
x
F
M
A
9
I
l
j
A
7
6
U
3
G
z
M
4
s
M
7
N
C
C
P
k
H
L
x
4
U
8
e
r
/
e
P
N
v
n
C
R
7
0
M
S
C
h
q
K
q
m
+
6
u
K
B
X
c
W
N
/
/
9
t
b
W
N
z
a
3
t
g
s
7
x
d
2
9
/
Y
P
D
0
t
F
x
0
6
h
M
M
2
w
w
J
Z
R
u
R
9
S
g
4
B
I
b
l
l
u
B
7
V
Q
j
T
S
K
B
r
W
h
0
M
/
N
b
T
6
g
N
V
/
L
B
j
l
M
M
E
z
q
Q
P
O
a
M
W
i
c
1
u
y
h
E
r
9
o
r
l
f
2
K
P
w
d
Z
J
U
F
O
y
p
C
j
3
i
t
9
d
f
u
K
Z
Q
l
K
y
w
Q
1
p
h
P
4
q
Q
0
n
V
F
v
O
B
E
6
L
3
c
x
g
S
t
m
I
D
r
D
j
q
K
Q
J
m
n
A
y
v
3
Z
K
z
p
3
S
J
7
H
S
r
q
Q
l
c
/
X
3
x
I
Q
m
x
o
y
T
y
H
U
m
1
A
7
N
s
j
c
T
/
/
M
6
m
Y
2
v
w
w
m
X
a
W
Z
R
s
s
W
i
O
B
P
E
K
j
J
7
n
f
S
5
R
m
b
F
2
B
H
K
N
H
e
3
E
j
a
k
m
j
L
r
A
i
q
6
E
I
L
l
l
1
d
J
s
1
o
J
/
E
p
w
f
1
m
u
3
e
V
x
F
O
A
U
z
u
A
C
A
r
i
C
G
t
x
C
H
R
r
A
4
B
G
e
4
R
X
e
P
O
W
9
e
O
/
e
x
6
J
1
z
c
t
n
T
u
A
P
v
M
8
f
O
Q
i
O
6
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
M
X
M
1
D
E
C
N
A
f
g
W
a
7
M
4
d
N
S
a
A
E
t
G
Y
f
4
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
z
1
G
e
M
r
6
t
H
L
Y
B
A
8
h
V
0
V
9
B
j
w
I
p
4
i
m
A
c
k
S
5
i
d
9
C
Z
j
Z
m
e
W
m
V
k
h
h
P
y
D
F
w
+
K
e
P
V
/
v
P
k
3
T
p
I
9
a
G
J
B
Q
1
H
V
T
X
d
X
l
A
p
u
r
O
9
/
e
y
u
r
a
+
s
b
m
4
W
t
4
v
b
O
7
t
5
+
6
e
C
w
Y
V
S
m
G
d
a
Z
E
k
q
3
I
m
p
Q
c
I
l
1
y
6
3
A
V
q
q
R
J
p
H
A
Z
j
S
8
m
f
r
N
J
9
S
G
K
/
l
g
R
y
m
G
C
e
1
L
H
n
N
G
r
Z
M
a
H
R
S
i
e
9
E
t
l
f
2
K
P
w
N
Z
J
k
F
O
y
p
C
j
1
i
1
9
d
X
q
K
Z
Q
l
K
y
w
Q
1
p
h
3
4
q
Q
3
H
V
F
v
O
B
E
6
K
n
c
x
g
S
t
m
Q
9
r
H
t
q
K
Q
J
m
n
A
8
u
3
Z
C
T
p
3
S
I
7
H
S
r
q
Q
l
M
/
X
3
x
J
g
m
x
o
y
S
y
H
U
m
1
A
7
M
o
j
c
V
/
/
P
a
m
Y
2
v
w
z
G
X
a
W
Z
R
s
v
m
i
O
B
P
E
K
j
J
9
n
f
S
4
R
m
b
F
y
B
H
K
N
H
e
3
E
j
a
g
m
j
L
r
A
i
q
6
E
I
L
F
l
5
d
J
4
7
w
S
+
J
X
g
/
r
J
c
v
c
v
j
K
M
A
x
n
M
A
Z
B
H
A
F
V
b
i
F
G
t
S
B
w
S
M
8
w
y
u
8
e
c
p
7
8
d
6
9
j
3
n
r
i
p
f
P
H
M
E
f
e
J
8
/
O
o
y
O
6
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
M
X
M
1
D
E
C
N
A
f
g
W
a
7
M
4
d
N
S
a
A
E
t
G
Y
f
4
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
z
1
G
e
M
r
6
t
H
L
Y
B
A
8
h
V
0
V
9
B
j
w
I
p
4
i
m
A
c
k
S
5
i
d
9
C
Z
j
Z
m
e
W
m
V
k
h
h
P
y
D
F
w
+
K
e
P
V
/
v
P
k
3
T
p
I
9
a
G
J
B
Q
1
H
V
T
X
d
X
l
A
p
u
r
O
9
/
e
y
u
r
a
+
s
b
m
4
W
t
4
v
b
O
7
t
5
+
6
e
C
w
Y
V
S
m
G
d
a
Z
E
k
q
3
I
m
p
Q
c
I
l
1
y
6
3
A
V
q
q
R
J
p
H
A
Z
j
S
8
m
f
r
N
J
9
S
G
K
/
l
g
R
y
m
G
C
e
1
L
H
n
N
G
r
Z
M
a
H
R
S
i
e
9
E
t
l
f
2
K
P
w
N
Z
J
k
F
O
y
p
C
j
1
i
1
9
d
X
q
K
Z
Q
l
K
y
w
Q
1
p
h
3
4
q
Q
3
H
V
F
v
O
B
E
6
K
n
c
x
g
S
t
m
Q
9
r
H
t
q
K
Q
J
m
n
A
8
u
3
Z
C
T
p
3
S
I
7
H
S
r
q
Q
l
M
/
X
3
x
J
g
m
x
o
y
S
y
H
U
m
1
A
7
M
o
j
c
V
/
/
P
a
m
Y
2
v
w
z
G
X
a
W
Z
R
s
v
m
i
O
B
P
E
K
j
J
9
n
f
S
4
R
m
b
F
y
B
H
K
N
H
e
3
E
j
a
g
m
j
L
r
A
i
q
6
E
I
L
F
l
5
d
J
4
7
w
S
+
J
X
g
/
r
J
c
v
c
v
j
K
M
A
x
n
M
A
Z
B
H
A
F
V
b
i
F
G
t
S
B
w
S
M
8
w
y
u
8
e
c
p
7
8
d
6
9
j
3
n
r
i
p
f
P
H
M
E
f
e
J
8
/
O
o
y
O
6
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
M
X
M
1
D
E
C
N
A
f
g
W
a
7
M
4
d
N
S
a
A
E
t
G
Y
f
4
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
z
1
G
e
M
r
6
t
H
L
Y
B
A
8
h
V
0
V
9
B
j
w
I
p
4
i
m
A
c
k
S
5
i
d
9
C
Z
j
Z
m
e
W
m
V
k
h
h
P
y
D
F
w
+
K
e
P
V
/
v
P
k
3
T
p
I
9
a
G
J
B
Q
1
H
V
T
X
d
X
l
A
p
u
r
O
9
/
e
y
u
r
a
+
s
b
m
4
W
t
4
v
b
O
7
t
5
+
6
e
C
w
Y
V
S
m
G
d
a
Z
E
k
q
3
I
m
p
Q
c
I
l
1
y
6
3
A
V
q
q
R
J
p
H
A
Z
j
S
8
m
f
r
N
J
9
S
G
K
/
l
g
R
y
m
G
C
e
1
L
H
n
N
G
r
Z
M
a
H
R
S
i
e
9
E
t
l
f
2
K
P
w
N
Z
J
k
F
O
y
p
C
j
1
i
1
9
d
X
q
K
Z
Q
l
K
y
w
Q
1
p
h
3
4
q
Q
3
H
V
F
v
O
B
E
6
K
n
c
x
g
S
t
m
Q
9
r
H
t
q
K
Q
J
m
n
A
8
u
3
Z
C
T
p
3
S
I
7
H
S
r
q
Q
l
M
/
X
3
x
J
g
m
x
o
y
S
y
H
U
m
1
A
7
M
o
j
c
V
/
/
P
a
m
Y
2
v
w
z
G
X
a
W
Z
R
s
v
m
i
O
B
P
E
K
j
J
9
n
f
S
4
R
m
b
F
y
B
H
K
N
H
e
3
E
j
a
g
m
j
L
r
A
i
q
6
E
I
L
F
l
5
d
J
4
7
w
S
+
J
X
g
/
r
J
c
v
c
v
j
K
M
A
x
n
M
A
Z
B
H
A
F
V
b
i
F
G
t
S
B
w
S
M
8
w
y
u
8
e
c
p
7
8
d
6
9
j
3
n
r
i
p
f
P
H
M
E
f
e
J
8
/
O
o
y
O
6
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
M
X
M
1
D
E
C
N
A
f
g
W
a
7
M
4
d
N
S
a
A
E
t
G
Y
f
4
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
z
1
G
e
M
r
6
t
H
L
Y
B
A
8
h
V
0
V
9
B
j
w
I
p
4
i
m
A
c
k
S
5
i
d
9
C
Z
j
Z
m
e
W
m
V
k
h
h
P
y
D
F
w
+
K
e
P
V
/
v
P
k
3
T
p
I
9
a
G
J
B
Q
1
H
V
T
X
d
X
l
A
p
u
r
O
9
/
e
y
u
r
a
+
s
b
m
4
W
t
4
v
b
O
7
t
5
+
6
e
C
w
Y
V
S
m
G
d
a
Z
E
k
q
3
I
m
p
Q
c
I
l
1
y
6
3
A
V
q
q
R
J
p
H
A
Z
j
S
8
m
f
r
N
J
9
S
G
K
/
l
g
R
y
m
G
C
e
1
L
H
n
N
G
r
Z
M
a
H
R
S
i
e
9
E
t
l
f
2
K
P
w
N
Z
J
k
F
O
y
p
C
j
1
i
1
9
d
X
q
K
Z
Q
l
K
y
w
Q
1
p
h
3
4
q
Q
3
H
V
F
v
O
B
E
6
K
n
c
x
g
S
t
m
Q
9
r
H
t
q
K
Q
J
m
n
A
8
u
3
Z
C
T
p
3
S
I
7
H
S
r
q
Q
l
M
/
X
3
x
J
g
m
x
o
y
S
y
H
U
m
1
A
7
M
o
j
c
V
/
/
P
a
m
Y
2
v
w
z
G
X
a
W
Z
R
s
v
m
i
O
B
P
E
K
j
J
9
n
f
S
4
R
m
b
F
y
B
H
K
N
H
e
3
E
j
a
g
m
j
L
r
A
i
q
6
E
I
L
F
l
5
d
J
4
7
w
S
+
J
X
g
/
r
J
c
v
c
v
j
K
M
A
x
n
M
A
Z
B
H
A
F
V
b
i
F
G
t
S
B
w
S
M
8
w
y
u
8
e
c
p
7
8
d
6
9
j
3
n
r
i
p
f
P
H
M
E
f
e
J
8
/
O
o
y
O
6
Q
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
e
e
N
j
p
z
o
F
g
r
V
H
P
N
G
L
i
L
f
n
T
l
z
J
Y
8
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
A
F
/
E
U
w
T
w
g
W
c
L
s
p
D
c
Z
M
z
u
z
z
M
w
K
I
e
Q
f
v
H
h
Q
x
K
v
/
4
8
2
/
c
Z
L
s
Q
R
M
L
G
o
q
q
b
r
q
7
o
l
R
w
Y
3
3
/
2
y
u
s
r
W
9
s
b
h
W
3
S
z
u
7
e
/
s
H
5
c
O
j
p
l
G
Z
Z
t
h
g
S
i
j
d
j
q
h
B
w
S
U
2
L
L
c
C
2
6
l
G
m
k
Q
C
W
9
H
o
Z
u
a
3
n
l
A
b
r
u
S
D
H
a
c
Y
J
n
Q
g
e
c
w
Z
t
U
5
q
d
l
G
I
3
m
W
v
X
P
G
r
/
h
x
k
l
Q
Q
5
q
U
C
O
e
q
/
8
1
e
0
r
l
i
U
o
L
R
P
U
m
E
7
g
p
z
a
c
U
G
0
5
E
z
g
t
d
T
O
D
K
W
U
j
O
s
C
O
o
5
I
m
a
M
L
J
/
N
o
p
O
X
N
K
n
8
R
K
u
5
K
W
z
N
X
f
E
x
O
a
G
D
N
O
I
t
e
Z
U
D
s
0
y
9
5
M
/
M
/
r
Z
D
a
+
D
i
d
c
p
p
l
F
y
R
a
L
4
k
w
Q
q
8
j
s
d
d
L
n
G
p
k
V
Y
0
c
o
0
9
z
d
S
t
i
Q
a
s
q
s
C
6
j
k
Q
g
i
W
X
1
4
l
z
Y
t
q
4
F
e
D
+
8
t
K
7
S
6
P
o
w
g
n
c
A
r
n
E
M
A
V
1
O
A
W
6
t
A
A
B
o
/
w
D
K
/
w
5
i
n
v
x
X
v
3
P
h
a
t
B
S
+
f
O
Y
Y
/
8
D
5
/
A
D
w
Q
j
u
o
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
e
e
N
j
p
z
o
F
g
r
V
H
P
N
G
L
i
L
f
n
T
l
z
J
Y
8
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
A
F
/
E
U
w
T
w
g
W
c
L
s
p
D
c
Z
M
z
u
z
z
M
w
K
I
e
Q
f
v
H
h
Q
x
K
v
/
4
8
2
/
c
Z
L
s
Q
R
M
L
G
o
q
q
b
r
q
7
o
l
R
w
Y
3
3
/
2
y
u
s
r
W
9
s
b
h
W
3
S
z
u
7
e
/
s
H
5
c
O
j
p
l
G
Z
Z
t
h
g
S
i
j
d
j
q
h
B
w
S
U
2
L
L
c
C
2
6
l
G
m
k
Q
C
W
9
H
o
Z
u
a
3
n
l
A
b
r
u
S
D
H
a
c
Y
J
n
Q
g
e
c
w
Z
t
U
5
q
d
l
G
I
3
m
W
v
X
P
G
r
/
h
x
k
l
Q
Q
5
q
U
C
O
e
q
/
8
1
e
0
r
l
i
U
o
L
R
P
U
m
E
7
g
p
z
a
c
U
G
0
5
E
z
g
t
d
T
O
D
K
W
U
j
O
s
C
O
o
5
I
m
a
M
L
J
/
N
o
p
O
X
N
K
n
8
R
K
u
5
K
W
z
N
X
f
E
x
O
a
G
D
N
O
I
t
e
Z
U
D
s
0
y
9
5
M
/
M
/
r
Z
D
a
+
D
i
d
c
p
p
l
F
y
R
a
L
4
k
w
Q
q
8
j
s
d
d
L
n
G
p
k
V
Y
0
c
o
0
9
z
d
S
t
i
Q
a
s
q
s
C
6
j
k
Q
g
i
W
X
1
4
l
z
Y
t
q
4
F
e
D
+
8
t
K
7
S
6
P
o
w
g
n
c
A
r
n
E
M
A
V
1
O
A
W
6
t
A
A
B
o
/
w
D
K
/
w
5
i
n
v
x
X
v
3
P
h
a
t
B
S
+
f
O
Y
Y
/
8
D
5
/
A
D
w
Q
j
u
o
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
e
e
N
j
p
z
o
F
g
r
V
H
P
N
G
L
i
L
f
n
T
l
z
J
Y
8
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
A
F
/
E
U
w
T
w
g
W
c
L
s
p
D
c
Z
M
z
u
z
z
M
w
K
I
e
Q
f
v
H
h
Q
x
K
v
/
4
8
2
/
c
Z
L
s
Q
R
M
L
G
o
q
q
b
r
q
7
o
l
R
w
Y
3
3
/
2
y
u
s
r
W
9
s
b
h
W
3
S
z
u
7
e
/
s
H
5
c
O
j
p
l
G
Z
Z
t
h
g
S
i
j
d
j
q
h
B
w
S
U
2
L
L
c
C
2
6
l
G
m
k
Q
C
W
9
H
o
Z
u
a
3
n
l
A
b
r
u
S
D
H
a
c
Y
J
n
Q
g
e
c
w
Z
t
U
5
q
d
l
G
I
3
m
W
v
X
P
G
r
/
h
x
k
l
Q
Q
5
q
U
C
O
e
q
/
8
1
e
0
r
l
i
U
o
L
R
P
U
m
E
7
g
p
z
a
c
U
G
0
5
E
z
g
t
d
T
O
D
K
W
U
j
O
s
C
O
o
5
I
m
a
M
L
J
/
N
o
p
O
X
N
K
n
8
R
K
u
5
K
W
z
N
X
f
E
x
O
a
G
D
N
O
I
t
e
Z
U
D
s
0
y
9
5
M
/
M
/
r
Z
D
a
+
D
i
d
c
p
p
l
F
y
R
a
L
4
k
w
Q
q
8
j
s
d
d
L
n
G
p
k
V
Y
0
c
o
0
9
z
d
S
t
i
Q
a
s
q
s
C
6
j
k
Q
g
i
W
X
1
4
l
z
Y
t
q
4
F
e
D
+
8
t
K
7
S
6
P
o
w
g
n
c
A
r
n
E
M
A
V
1
O
A
W
6
t
A
A
B
o
/
w
D
K
/
w
5
i
n
v
x
X
v
3
P
h
a
t
B
S
+
f
O
Y
Y
/
8
D
5
/
A
D
w
Q
j
u
o
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
e
e
N
j
p
z
o
F
g
r
V
H
P
N
G
L
i
L
f
n
T
l
z
J
Y
8
=
"
>
A
A
A
B
7
X
i
c
b
V
D
L
S
g
N
B
E
O
y
N
r
x
h
f
U
Y
9
e
B
o
P
g
K
e
y
K
o
M
e
A
F
/
E
U
w
T
w
g
W
c
L
s
p
D
c
Z
M
z
u
z
z
M
w
K
I
e
Q
f
v
H
h
Q
x
K
v
/
4
8
2
/
c
Z
L
s
Q
R
M
L
G
o
q
q
b
r
q
7
o
l
R
w
Y
3
3
/
2
y
u
s
r
W
9
s
b
h
W
3
S
z
u
7
e
/
s
H
5
c
O
j
p
l
G
Z
Z
t
h
g
S
i
j
d
j
q
h
B
w
S
U
2
L
L
c
C
2
6
l
G
m
k
Q
C
W
9
H
o
Z
u
a
3
n
l
A
b
r
u
S
D
H
a
c
Y
J
n
Q
g
e
c
w
Z
t
U
5
q
d
l
G
I
3
m
W
v
X
P
G
r
/
h
x
k
l
Q
Q
5
q
U
C
O
e
q
/
8
1
e
0
r
l
i
U
o
L
R
P
U
m
E
7
g
p
z
a
c
U
G
0
5
E
z
g
t
d
T
O
D
K
W
U
j
O
s
C
O
o
5
I
m
a
M
L
J
/
N
o
p
O
X
N
K
n
8
R
K
u
5
K
W
z
N
X
f
E
x
O
a
G
D
N
O
I
t
e
Z
U
D
s
0
y
9
5
M
/
M
/
r
Z
D
a
+
D
i
d
c
p
p
l
F
y
R
a
L
4
k
w
Q
q
8
j
s
d
d
L
n
G
p
k
V
Y
0
c
o
0
9
z
d
S
t
i
Q
a
s
q
s
C
6
j
k
Q
g
i
W
X
1
4
l
z
Y
t
q
4
F
e
D
+
8
t
K
7
S
6
P
o
w
g
n
c
A
r
n
E
M
A
V
1
O
A
W
6
t
A
A
B
o
/
w
D
K
/
w
5
i
n
v
x
X
v
3
P
h
a
t
B
S
+
f
O
Y
Y
/
8
D
5
/
A
D
w
Q
j
u
o
=
<
/
l
a
t
e
x
i
t
>
has been proposed. CGAN usually combines a vanilla GAN
and some external information, such as class labels or tags
[29, 30, 4, 39, 36], text descriptions [33, 49], human pose
[8, 37, 28, 22] and reference images [25, 16].
Image-to-Image Translation frameworks adopt
input-
output data to learn a parametric mapping between inputs
and outputs. For example, Isola et al. [16] propose Pix2pix,
which is a supervised model and uses a CGAN to learn a
translation function from input to output image domains.
[53] introduce CycleGAN, which targets un-
Zhu et al.
paired image translation using the cycle-consistency loss.
To further improve the generation performance, the atten-
tion mechanism has been recently investigated in image
translation, such as [3, 44, 38, 24, 26]. However, to the
best of our knowledge, our model is the ﬁrst attempt to in-
corporate a multi-channel attention selection module within
a GAN framework for image-to-image translation task.
Learning Viewpoint Transformations. Most existing
works on viewpoint transformation have been conducted
to synthesize novel views of the same object, such as cars,
chairs and tables [9, 40, 5]. Another group of works explore
the cross-view scene image generation, such as [46, 52].
However, these works focus on the scenario in which the
objects and the scenes have a large degree of overlapping
in both appearances and views. Recently, several works
started investigating image translation problems with dras-
tically different views and generating a novel scene from a
given arbitrary one. This is a more challenging task since
different views have little or no overlap. To tackle this prob-
lem, Zhai et al. [47] try to generate panoramic ground-level
images from aerial images of the same location by using a
convolutional neural network. Krishna and Ali [34] propose
a X-Fork and a X-Seq GAN-based structure to address the
aerial to street view image translation task using an extra
semantic segmentation map. However, these methods are
not able to generate satisfactory results due to the drastic
difference between source and target views and their model
design. To overcome these issues, we aim at a more ef-
fective network design, and propose a novel multi-channel
attention selection GAN, which allows to automatically se-
lect from multiple diverse and rich intermediate generations
and thus signiﬁcantly improves the generation quality.

3. Multi-Channel Attention Selection GAN

In this section we present the details of the proposed
multi-channel attention selection GAN. An illustration of
the overall network structure is depicted in Fig. 2. In the
ﬁrst stage, we present a cascade semantic-guided genera-
tion sub-network, which utilizes the images from one view
and conditional semantic maps from another view as inputs,
and reconstruct images in another view. These images are
further input into a semantic generator to recover the in-
put semantic map forming a generation cycle. In the sec-

ond stage, the coarse synthesis and the deep features from
the ﬁrst stage are combined, and then are passed to the
proposed multi-channel attention selection module, which
aims at producing more ﬁne-grained synthesis from a larger
generation space and also at generating uncertainty maps to
guide multiple optimization losses.

′

′

3.1. Cascade Semantic guided Generation
Semantic-guided Generation. Cross-view synthesis is a
challenging task, especially when the two views have lit-
tle overlapping as in our study case, which apparently leads
to ambiguity issues in the generation process. To alleviate
this problem, we use semantic maps as conditional guid-
ance. Since it is always costly to obtain annotated semantic
maps, following [34] we generate the maps using segmenta-
tion deep models pretrained from large-scale scene parsing
datasets such as Cityscapes [6]. However, [34] uses seman-
tic maps only in the reconstruction loss to guide the gen-
eration of semantics, which actually provides a weak guid-
ance. Different from theirs, we apply the semantic maps
not only in the output loss but also as part of the network’s
input. Speciﬁcally, as shown in Fig. 2, we concatenate
the input image Ia from the source view and the seman-
tic map Sg from a target view, and input them into the im-
age generator Gi and synthesize the target view image I
g
g=Gi(Ia, Sg). In this way, the ground-truth semantic
as I
maps provide stronger supervision to guide the cross-view
translation in the deep network.
Semantic-guided Cycle. Regmi and Borji [34] observed
that the simultaneous generation of both the images and
the semantic maps improves the generation performance.
Along the same line, we propose a cycled semantic gener-
ation network to beneﬁt more the semantic information in
learning. The conditional semantic map Sg together with
the input image Ia are input into the image generator Gi,
and produce the synthesized image I
g is further
fed into the semantic generator Gs which reconstructs a
new semantic map S
g. We can formalize the process as
g)=Gs(Gi(Ia, Sg)). Then the optimization ob-
g as close as possible to Sg, which nat-
Gi→
g≈Sg. The two generators are explicitly connected
I
g
by the ground-truth semantic maps, which in this way pro-
vide extra constraints on the generators to learn better the
semantic structure consistency.
Cascade Generation. Due to the complexity of the task,
after the ﬁrst stage, we observe that the image generator
Gi outputs a coarse synthesis, which yields blurred scene
details and high pixel-level dis-similarity with the target-
view images. This inspires us to explore a coarse-to-ﬁne
generation strategy in order to boost the synthesis perfor-
mance based on the coarse predictions. Cascade models

urally forms a semantic generation cycle, i.e.

jective is to make S

g. Then I

g=Gs(I

[Ia, Sg]

Gs→ S

S

′

′

′

′

′

′

′

′

2419

Fc

Fc

Fc

C

V
N
O
C

L
O
O
P

Fc

0

F

c

N Intermediate Generations

. . .

. . .

. . .

Final Output

C

V
N
O
C

V
N
O
C

 
e
s
W

i

-
l
e
n
n
a
h
C

x
a
m

t
f
o
S

Multi-Scale Spatial  Pooling

Multi-Channel Attention Selection

N-Channel Attention Maps

Uncertainty Maps

Figure 3: Illustration of the proposed multi-channel attention selection module. The multi-scale spatial pooling pools features
in different receptive ﬁelds in order to have better generation of scene details; the multi-channel attention selection aims at
automatically select from a set of intermediate diverse generations in a larger generation space to improve the generation
quality. The symbols ⊕, ⊗, c(cid:13) and ↑(cid:13) denote element-wise addition, element-wise multiplication, concatenation, and up-
sampling operation, respectively.

have been used in several other computer vision tasks such
as object detection [2] and semantic segmentation [7], and
have shown great effectiveness. In this paper, we introduce
the cascade strategy to deal with the complex cross-view
translation problem. In both stages we have a basic cycled
semantic guided generation sub-network, while in the sec-
ond stage, we propose a novel multi-channel attention se-
lection module to better utilize the coarse outputs from the
ﬁrst stage and produce ﬁne-grained ﬁnal outputs. We ob-
served signiﬁcant improvement by using the proposed cas-
cade strategy, illustrated in the experimental part.

3.2. Multi Channel Attention Selection

An overview of the proposed multi-channel attention se-
lection module Ga is shown in Fig. 3. The module consists
of a multi-scale spatial pooling and a multi-channel atten-
tion selection component.
Multi-Scale Spatial Pooling. Since there exists a large ob-
ject/scene deformation between the source view and the tar-
get view, a single-scale feature may not be able to capture all
the necessary spatial information for a ﬁne-grained genera-
tion. Thus we propose a multi-scale spatial pooling scheme,
which uses a set of different kernel size and stride to per-
form a global average pooling on the same input features.
By so doing, we obtain multi-scale features with different
receptive ﬁelds to perceive a different spatial context. More
speciﬁcally, given the coarse inputs and the deep semantic
features produced from the stage I, we ﬁrst concatenate all
of them as new features denoted as Fc for the stage II as:

Fc = concat(Ia, I

′

g, Fi, Fs)

(1)

where concat(·) is a function for channel-wise concatena-
tion operation; Fi and Fs are features from the last con-

volution layers of the generators Gi and Gs, respectively.
We apply a set of M spatial scales {si}M
i=1 in pooling, re-
sulting in pooled features with different spatial resolution.
Different from the pooling scheme used in [50] which di-
rectly combines all the features after pooling, we ﬁrst se-
lect each pooled feature via an element-wise multiplication
with the input feature. Since in our task the input features
are from different sources, highly correlated features would
preserve more useful information for the generation. Let
us denote pl ups(·) as pooling at a scale s followed by an
up-sampling operation to rescale the pooled feature at the
same resolution, and ⊗ as element-wise multiplication, we
can formalize the whole process as follows:
Fc ← concat(cid:0)Fc ⊗ pl up1(Fc), . . . , Fc ⊗ pl upM (Fc))
(2)
Then the features Fc are fed into a convolutional layer,
which produces new multi-scale features F
c for the use in
the multi-channel selection module.
Multi-Channel Attention Selection.
In previous cross-
view image synthesis works, the image is generated only
in a three-channel RGB space. We argue that this is not
enough for the complex translation problem we are dealing
with, and thus we explore using a larger generation space to
have a richer synthesis via constructing multiple interme-
diate generations. Accordingly, we design a multi-channel
attention mechanism to automatically perform spatial and
temporal selection from the generations to synthesize a ﬁne-
grained ﬁnal output.

′

Given the multi-scale feature volume F

c∈Rh×w×c,
where h and w are width and height of the features, and c
is the number of channels, we consider two directions. One
is for the generation of multiple intermediate image syn-

′

2420

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
k
I
y
9
1
5
+
o
w
A
9
1
1
K
8
f
H
W
I
e
Q
V
7
L
g
4
U
=
"
>
A
A
A
B
7
X
i
c
b
V
B
N
S
8
N
A
E
J
3
U
r
1
q
/
q
h
6
9
L
B
b
B
U
0
l
E
0
G
P
B
i
8
c
K
/
Y
I
2
l
M
1
m
0
6
7
d
Z
M
P
u
R
C
i
h
/
8
G
L
B
0
W
8
+
n
+
8
+
W
/
c
t
j
l
o
6
4
O
B
x
3
s
z
z
M
w
L
U
i
k
M
u
u
6
3
U
9
r
Y
3
N
r
e
K
e
9
W
9
v
Y
P
D
o
+
q
x
y
c
d
o
z
L
N
e
J
s
p
q
X
Q
v
o
I
Z
L
k
f
A
2
C
p
S
8
l
2
p
O
4
0
D
y
b
j
C
5
m
/
v
d
J
6
6
N
U
E
k
L
p
y
n
3
Y
z
p
K
R
C
Q
Y
R
S
t
1
B
j
J
U
a
I
b
V
m
l
t
3
F
y
D
r
x
C
t
I
D
Q
o
0
h
9
W
v
Q
a
h
Y
F
v
M
E
m
a
T
G
9
D
0
3
R
T
+
n
G
g
W
T
f
F
Y
Z
Z
I
a
n
l
E
3
o
i
P
c
t
T
W
j
M
j
Z
8
v
r
p
2
R
C
6
u
E
J
F
L
a
V
o
J
k
o
f
6
e
y
G
l
s
z
D
Q
O
b
G
d
M
c
W
x
W
v
b
n
4
n
9
f
P
M
L
r
1
c
5
G
k
G
f
K
E
L
R
d
F
m
S
S
o
y
P
x
1
E
g
r
N
G
c
q
p
J
Z
R
p
Y
W
8
l
b
E
w
1
Z
W
g
D
q
t
g
Q
v
N
W
X
1
0
n
n
q
u
6
5
d
e
/
h
u
t
Z
o
F
X
G
U
4
Q
z
O
4
R
I
8
u
I
E
G
3
E
M
T
2
s
D
g
E
Z
7
h
F
d
4
c
5
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
A
f
O
5
w
/
B
i
Y
9
K
<
/
l
a
t
e
x
i
t
>
G}N

thesis, and the other is for the generation of multi-channel
attention maps. To produce N different intermediate gener-
ations IG={I i
i=1, a convolution operation is performed
with N convolutional ﬁlters {W i
i=1 followed by a
tanh(·) non-linear activation operation. For the generation
of corresponding N attention maps, the other group of ﬁl-
ters {W i
i=1 is applied. Then the intermediate gener-
ations and the attention maps are calculated as follows:

G, bi

A, bi

G}N

A}N

I i
G = tanh(F

I i
A = Softmax(F

′

cW i
cW i

G + bi
A + bi

′

G),

A),

for i = 1, . . . , N

for i = 1, . . . , N

(3)

where Softmax(·) is a channel-wise softmax function used
for the normalization. Finally, the learned attention maps
are utilized to perform channel-wise selection from each in-
termediate generation as follows:

I

′′

g = (I 1

A ⊗ I 1

G) ⊕ · · · ⊕ (I N

A ⊗ I N
G )

(4)

′′

′′

′′

g =Gs(I

where I
g represents the ﬁnal synthesized generation se-
lected from the multiple diverse results, and the symbol ⊕
denotes the element-wise addition. We also generate a ﬁ-
nal semantic map in the second stage as in the ﬁrst stage,
g ). Due to the same purpose of the two se-
i.e. S
mantic generators, we use a single Gs twice by sharing the
parameters in both stages to reduce the network capacity.
Uncertainty-guided Pixel Loss. As we discussed in the in-
troduction, the semantic maps obtained from the pretrained
model are not accurate for all the pixels, which leads to
a wrong guidance during training. To tackle this issue,
we propose the generated attention maps to learn uncer-
tainty maps to control the optimization loss. The uncer-
tainty learning has been investigated in [19] for multi-task
learning, and here we introduce it for solving the noisy se-
mantic label problem. Assume that we have K different
loss maps which need a guidance. The multiple generated
attention maps are ﬁrst concatenated and passed to a convo-
lution layer with K ﬁlters {W i
i=1 to produce a set of K
uncertainty maps. The reason of using the attention maps to
generate uncertainty maps is that the attention maps directly
affect the ﬁnal generation leading to a close connection with
the loss. Let Li
p denote a pixel-level loss map and Ui denote
the i-th uncertainty map, we have:

u}K

Ui = σ(cid:0)W i
Li
p ←

Li
p
Ui

u(concat(I 1

+ log Ui,

A, . . . , I N

A ) + bi
u(cid:1)
for i = 1, . . . , K

(5)

where σ(·) is a Sigmoid function for pixel-level normaliza-
tion. The uncertainty map is automatically learned and acts
as a weighting scheme to control the optimization loss.
Parameter-Sharing Discriminator. We extend the vanilla
discriminator in [16] to a parameter-sharing structure. In
the ﬁrst stage, this structure takes the real image Ia and the
generated image I
g or the ground-truth image Ig as input.

′

The discriminator D learns to tell whether a pair of images
from different domains is associated with each other or not.
In the second stage, it accepts the real image Ia and the gen-
erated image I
g or the real image Ig as input. This pairwise
input encourages D to discriminate the diversity of image
structure and capture the local-aware information.

′′

3.3. Overall Optimization Objective
Adversarial Loss. In the ﬁrst stage, the adversarial loss of
g] from
D for distinguishing synthesized image pairs [Ia, I
real image pairs [Ia, Ig] is formulated as follows,

′

LcGAN (Ia, I

′

g) =EIa,Ig [log D(Ia, Ig)] +
g hlog(1 − D(Ia, I

Ia,I ′

E

(6)

′

g))i .

In the second stage, the adversarial loss of D for distin-
guishing synthesized image pairs [Ia, I
g ] from real image
pairs [Ia, Ig] is formulated as follows:

′′

LcGAN (Ia, I

′′

g )=EIa,Ig [log D(Ia, Ig)] +
g hlog(1 − D(Ia, I

Ia,I ′′

E

(7)

′′

g ))i .

Both losses aim to preserve the local structure information
and produce visually pleasing synthesized images. Thus,
the adversarial loss of the proposed SelectionGAN is the
sum of Eq. (6) and (7),

LcGAN = LcGAN (Ia, I

′

g) + λLcGAN (Ia, I

′′

g ).

(8)

Overall Loss. The total optimization loss is a weighted sum
of the above losses. Generators Gi, Gs, attention selection
network Ga and discriminator D are trained in an end-to-
end fashion optimizing the following min-max function,

min

{Gi,Gs,Ga}

max
{D}

L =

4

X

i=1

λiLi

p + LcGAN + λtvLtv.

′′

(9)
p uses the L1 reconstruction to separately calcu-

where Li
late the pixel loss between the generated images I

g
g and the corresponding real images. Ltv is the total
and S
variation regularization [17] on the ﬁnal synthesized image
I
g . λi and λtv are the trade-off parameters to control the
relative importance of different objectives. The training is
performed by solving the min-max optimization problem.

g, S

g, I

′′

′′

′

′

3.4. Implementation Details
Network Architecture. For a fair comparison, we employ
U-Net [16] as our generator architectures Gi and Gs. U-
Net is a network with skip connections between a down-
sampling encoder and an up-sampling decoder. Such ar-
chitecture comprehensively retains contextual and textural
information, which is crucial for removing artifacts and
padding textures. Since our focus is on the cross-view im-
age generation task, Gi is more important than Gs. Thus
we use a deeper network for Gi and a shallow network for

2421

Figure 4: Results generated by different methods in 256×256 resolution in a2g and g2a directions on Dayton dataset.

Table 1: SSIM, PSNR, Sharpness Difference (SD) and KL score (KL) of different methods. For these metrics except KL
score, higher is better. (*) These results are reported in [34].

Direction

Method

Dayton (64×64)

Dayton (256×256)

CVUSA

⇆

a2g

g2a

Zhai et al. [47]

Pix2pix [16]
X-Fork [34]
X-Seq [34]

SelectionGAN (Ours)

Pix2pix [16]
X-Fork [34]
X-Seq [34]

SelectionGAN (Ours)

SSIM

PSNR

-

0.4808*
0.4921*
0.5171*
0.6865
0.3675*
0.3682*
0.3663*
0.5118

-

19.4919*
19.6273*
20.1049*
24.6143
20.5135*
20.6933*
20.4239*
23.2657

SD

-

16.4489*
16.4928*
16.6836*
18.2374
14.7813*
14.7984*
14.7657*
16.2894

KL

-

6.29 ± 0.80*
3.42 ± 0.72*
6.22 ± 0.87*
1.70 ± 0.45
6.39 ± 0.90*
4.45 ± 0.84*
7.20 ± 0.92*
2.25 ± 0.56

SSIM

PSNR

-

0.4180*
0.4963*
0.5031*
0.5938
0.2693*
0.2763*
0.2725*
0.3284

-

17.6291*
19.8928*
20.2803*
23.8874
20.2177*
20.5978*
20.2925*
21.8066

SD

-

19.2821*
19.4533*
19.5258*
20.0174
16.9477*
16.9962*
16.9285*
17.3817

KL

-

38.26 ± 1.88*
6.00 ± 1.28*
5.93 ± 1.32*
2.74 ± 0.86
7.88 ± 1.24*
6.92 ± 1.15*
7.07 ± 1.19*
3.55 ± 0.87

SSIM

0.4147*
0.3923*
0.4356*
0.4231*
0.5323

PSNR

17.4886*
17.6578*
19.0509*
18.8067*
23.1466

SD

16.6184*
18.5239*
18.6706*
18.4378*
19.6100

KL

27.43 ± 1.63*
59.81 ± 2.12*
11.71 ± 1.55*
15.52 ± 1.73*

2.96 ± 0.97

-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

Gs. Speciﬁcally, the ﬁlters in ﬁrst convolutional layer of Gi
and Gs are 64 and 4, respectively. For the network Ga, the
kernel size of convolutions for generating the intermediate
images and attention maps are 3×3 and 1×1, respectively.
We adopt PatchGAN [16] for the discriminator D.
Training Details. Following [34], we use ReﬁneNet [23]
and [51] to generate segmentation maps on Dayton and
Ego2Top datasets as training data, respectively. We fol-
low the optimization method in [11] to optimize the pro-
posed SelectionGAN, i.e. one gradient descent step on dis-
criminator and generators alternately. We ﬁrst train Gi, Gs,
Ga with D ﬁxed, and then train D with Gi, Gs, Ga ﬁxed.
The proposed SelectionGAN is trained and optimized in an
end-to-end fashion. We employ Adam [21] with momen-
tum terms β1=0.5 and β2=0.999 as our solver. The initial
learning rate for Adam is 0.0002. The network initialization
strategy is Xavier [10], weights are initialized from a Gaus-
sian distribution with standard deviation 0.2 and mean 0.

4. Experiments

4.1. Experimental Setting
Datasets. We perform the experiments on three different
(i) For the Dayton dataset [41], following the
datasets:
same setting of [34], we select 76,048 images and cre-
ate a train/test split of 55,000/21,048 pairs. The images
in the original dataset have 354×354 resolution. We re-
size them to 256×256; (ii) The CVUSA dataset [43] con-
sists of 35,532/8,884 image pairs in train/test split. Fol-
lowing [47, 34], the aerial images are center-cropped to
224×224 and resized to 256×256. For the ground level

images and corresponding segmentation maps, we take the
ﬁrst quarter of both and resize them to 256×256; (iii) The
Ego2Top dataset [1] is more challenging and contains dif-
ferent indoor and outdoor conditions. Each case contains
one top-view video and several egocentric videos captured
by the people visible in the top-view camera. This dataset
has more than 230,000 frames. For training data, we ran-
domly select 386,357 pairs and each pair is composed of
two images of the same scene but different viewpoints. We
randomly select 25,600 pairs for evaluation.

Parameter Settings. For a fair comparison, we adopt the
same training setup as in [16, 34]. All images are scaled to
256×256, and we enabled image ﬂipping and random crops
for data augmentation. Similar to [34], the low resolution
(64×64) experiments on Dayton dataset are carried out for
100 epochs with batch size of 16, whereas the high resolu-
tion (256×256) experiments for this dataset are trained for
35 epochs with batch size of 4. For the CVUSA dataset, we
follow the same setup as in [47, 34], and train our network
for 30 epochs with batch size of 4. For the Ego2Top dataset,
all models are trained with 10 epochs using batch size 8. In
our experiment, we set λtv=1e−6, λ1=100, λ2=1, λ3=200
and λ4=2 in Eq. (9), and λ=4 in Eq. (8). The number of
attention channels N in Eq. (5) is set to 10. The proposed
SelectionGAN is implemented in PyTorch. We perform our
experiments on Nvidia GeForce GTX 1080 Ti GPU with
11GB memory to accelerate both training and inference.

Evaluation Protocol. Similar to [34], we employ Incep-
tion Score, top-k prediction accuracy and KL score for the
quantitative analysis. These metrics evaluate the generated

2422

Table 2: Accuracies of different methods. For this metric, higher is better. (*) These results are reported in [34].

Dir.

⇆

a2g

g2a

Method

Dayton (64×64)

Dayton (256×256)

CVUSA

Top-1

Top-5

Top-1

Top-5

Top-1

Top-5

Accuracy (%)

Accuracy (%)

Accuracy (%)

Accuracy (%)

Accuracy (%)

Accuracy (%)

Zhai et al. [47]

Pix2pix [16]
X-Fork [34]
X-Seq [34]

SelectionGAN (Ours)

Pix2pix [16]
X-Fork [34]
X-Seq [34]

SelectionGAN (Ours)

-

7.90*
16.63*
4.83*
45.37
1.65*
4.00*
1.55*
14.12

-

15.33*
34.73*
5.56*
79.00
2.24*
16.41*
2.99*
51.81

-

27.61*
46.35*
19.55*
83.48
7.49*
15.42*
6.27*
39.45

-

39.07*
70.01*
24.96*
97.74
12.68*
35.82*
8.96*
74.70

-

6.80*
30.00*
30.16*
42.11
10.23*
10.54*
12.30*
20.66

-

9.15*
48.68*
49.85*
68.12
16.02*
15.29*
19.62*
33.70

-

23.55*
61.57*
62.59*
77.74
30.90*
30.76*
35.95*
51.01

-

27.00*
78.84*
80.70*
92.89
40.49*
37.32*
45.94*
63.03

13.97*
7.33*
20.58*
15.98*
41.52

14.03*
9.25*
31.24*
24.14*
65.51

42.09*
25.81*
50.51*
42.91*
74.32

52.29*
32.67*
63.66*
54.41*
89.66

-
-
-
-

-
-
-
-

-
-
-
-

-

-
-

Table 3: Inception Score of different methods. For this metric, higher is better. (*) These results are reported in [34].

Dir.

⇆

a2g

g2a

Method

Dayton (64×64)

Dayton (256×256)

Zhai et al. [47]

Pix2pix [16]
X-Fork [34]
X-Seq [34]

SelectionGAN (Ours)

Real Data

Pix2pix [16]
X-Fork [34]
X-Seq [34]

SelectionGAN (Ours)

Real Data

all

classes

-

1.8029*
1.9600*
1.8503*
2.1606
2.3534
1.7970*
1.8557*
1.7854*
2.1571
2.3015

Top-1
class

-

1.5014*
1.5908*
1.4850*
1.7213
1.8135
1.3029*
1.3162*
1.3189*
1.4441
1.5056

Top-5
classes

-

1.9300*
2.0348*
1.9623*
2.1323
2.3250
1.6101*
1.6521*
1.6219*
2.0828
2.2095

all

classes

-

2.8515*
3.0720*
2.7384*
3.0613
3.8319
3.5676*
3.1342*
3.5849*
3.2446
3.7196

Top-1
class

-

1.9342*
2.2402*
2.1304*
2.2707
2.5753
2.0325*
1.8656*
2.0489*
2.1331
2.3626

Top-5
classes

-

2.9083*
3.0932*
2.7674*
3.1336
3.9222
2.8141*
2.5599*
2.8414*
3.4091
3.8998

all

classes
1.8434*
3.2771*
3.4432*
3.8151*
3.8074
4.8741

-
-
-
-
-

CVUSA

Top-1
class

1.5171*
2.2219*
2.5447*
2.6738*
2.7181
3.2959

-
-
-
-
-

Top-5
classes
1.8666*
3.4312*
3.5567*
4.0077*
3.9197
4.9943

-
-
-
-
-

Table 4: Ablations study of the proposed SelectionGAN.

Baseline

A

B

C

D
E
F
G
H

Setup
Gi→ I
Gi→ I

′

g
′

g

Ia

Sg

[Ia, Sg]

Gi→ I

′

g

[Ia, Sg]

Gi→ I

′

g

Gs→ S

′

g

D + Uncertainty-Guided Pixel Loss

E + Multi-Channel Attention Selection

F + Total Variation Regularization
G + Multi-Scale Spatial Pooling

SSIM

PSNR

SD

0.4555

19.6574

18.8870

0.5223

22.4961

19.2648

0.5374

22.8345

19.2075

0.5438
0.5522
0.5989
0.6047
0.6167

22.9773
23.0317
23.7562
23.7956
23.9310

19.4568
19.5127
20.0000
20.0830
20.1214

images from a high-level feature space. We also employ
pixel-level similarity metrics to evaluate our method, i.e.
Structural-Similarity (SSIM), Peak Signal-to-Noise Ratio
(PSNR) and Sharpness Difference (SD).

4.2. Experimental Results

Baseline Models. We conduct ablation study in a2g (aerial-
to-ground) direction on Dayton dataset. To reduce the
training time, we randomly select 1/3 samples from the
whole 55,000/21,048 samples i.e. around 18,334 samples
for training and 7,017 samples for testing. The proposed
SelectionGAN considers eight baselines (A, B, C, D, E, F,
G, H) as shown in Table 4. Baseline A uses a Pix2pix struc-
ture [16] and generates I
g using a single image Ia. Base-
line B uses the same Pix2pix model and generates I
g us-
ing the corresponding semantic map Sg. Baseline C also
uses the Pix2pix structure, and inputs the combination of a
conditional image Ia and the target semantic map Sg to the
generator Gi. Baseline D uses the proposed cycled seman-
tic generation upon Baseline C. Baseline E represents the

′

′

Figure 5: Qualitative results of coarse-to-ﬁne generation on
CVUSA dataset.

pixel loss guided by the learned uncertainty maps. Baseline
F employs the proposed multi-channel attention selection
module to generate multiple intermediate generations, and
to make the neural network attentively select which part is
more important for generating a scene image with a new
viewpoint. Baseline G adds the total variation regulariza-
tion on the ﬁnal result I
g . Baseline H employs the proposed
multi-scale spatial pooling module to reﬁne the features Fc
from stage I. All the baseline models are trained and tested
on the same data using the conﬁguration.
Ablation Analysis. The results of ablation study are shown
in Table 4. We observe that Baseline B is better than
baseline A since Sg contains more structural information

′′

2423

Table 5: Quantitative results on Ego2Top dataset. For all metrics except KL score, higher is better.

Method

SSIM

PSNR

SD

Inception Score

Accuracy

KL Score

all classes

Top-1 class

Top-5 classes

Top-1

Top-5

Pix2pix [16]
X-Fork [34]
X-Seq [34]

SelectionGAN (Ours)

Real Data

0.2213
0.2740
0.2738
0.6024

-

15.7197
16.3709
16.3788
26.6565

16.5949
17.3509
17.2624
19.7755

-

-

2.5418
4.6447
4.5094
5.6200
6.4523

1.6797
2.1386
2.0276
2.5328
2.8507

2.4947
3.8417
3.6756
4.7648
5.4662

1.22
5.91
4.78
28.31

-

1.57
10.22
8.96
54.56

-

5.33
20.98
17.04
62.97

-

6.86
30.29
24.40
76.30

-

120.46 ± 1.94
22.12 ± 1.65
25.19 ± 1.73
3.05 ± 0.91

-

Figure 6: Results generated by different methods in
256×256 resolution in a2g direction on CVUSA dataset.

than Ia. By comparison Baseline A with Baseline C, the
semantic-guided generation improves SSIM, PSNR and SD
by 8.19, 3.1771 and 0.3205, respectively, which conﬁrms
the importance of the conditional semantic information; By
using the proposed cycled semantic generation, Baseline D
further improves over C, meaning that the proposed seman-
tic cycle structure indeed utilizes the semantic information
in a more effective way, conﬁrming our design motivation;
Baseline E outperforms D showing the importance of us-
ing the uncertainty maps to guide the pixel loss map which
contains an inaccurate reconstruction loss due to the wrong
semantic labels produced from the pretrained segmentation
model; Baseline F signiﬁcantly outperforms E with around
4.67 points gain on the SSIM metric, clearly demonstrating
the effectiveness of the proposed multi-channel attention se-
lection scheme; We can also observe from Table 4 that, by
adding the proposed multi-scale spatial pool scheme and
the TV regularization, the overall performance is further
boosted. Finally, we demonstrate the advantage of the pro-
posed two-stage strategy over the one-stage method. Sev-
eral examples are shown in Fig. 5.
It is obvious that the
coarse-to-ﬁne generation model is able to generate sharper
results and contains more details than the one-stage model.

State-of-the-art Comparisons. We compare our Selec-
tionGAN with four recently proposed state-of-the-art meth-
ods, which are Pix2pix [16], Zhai et al. [47], X-Fork [34]
and X-Seq [34]. The comparison results are shown in Ta-
bles 1, 2, 3, and 5. We can observe the signiﬁcant improve-
ment of SelectionGAN in these tables. SelectionGAN con-
sistently outperforms Pix2pix, Zhai et al., X-Fork and X-
Seq on all the metrics except for Inception Score. In some
cases in Table 3 we achieve a slightly lower performance as
compared with X-Seq. However, we generate much more
photo-realistic results than X-Seq as shown in Fig. 4 and 6.

Qualitative Evaluation. The qualitative results in higher

Figure 7: Arbitrary cross-view image translation on
Ego2Top dataset.

resolution on Dayton and CVUSA datasets are shown in
Fig. 4 and 6. It can be seen that our method generates more
clear details on objects/scenes such as road, tress, clouds,
car than the other comparison methods in the generated
ground level images. For the generated aerial images, we
can observe that grass, trees and house roofs are well ren-
dered compared to others. Moreover, the results generated
by our method are closer to the ground truths in layout and
structure, such as the results in a2g direction in Fig. 4 and 6.
Arbitrary Cross-View Image Translation. Since Dayton
and CVUSA datasets only contain two views in one scene,
i.e. aerial and ground views. We further use the Ego2Top
dataset to conduct the arbitrary cross-view image transla-
tion experiments. The quantitative and qualitative results
are shown in Table 5 and Fig. 7, respectively. Given an im-
age and some novel semantic maps, SelectionGAN is able
to generate the same scene but with different viewpoints.

5. Conclusion

We propose the Multi-Channel Attention Selection GAN
(SelectionGAN) to address a novel image synthesizing task
by conditioning on a reference image and a target semantic
map. In particular, we adopt a cascade strategy to divide the
generation procedure into two stages. Stage I aims to cap-
ture the semantic structure of the scene and Stage II focus
on more appearance details via the proposed multi-channel
attention selection module. We also propose an uncertainty
map-guided pixel loss to solve the inaccurate semantic la-
bels issue for better optimization. Extensive experimental
results on three public datasets demonstrate that our method
obtains much better results than the state-of-the-art.
Acknowledgements: This research was partially supported
by National
Institute of Standards and Technology Grant
60NANB17D191 (YY, JC), Army Research Ofﬁce W911NF-15-
1-0354 (JC) and gift donation from Cisco Inc (YY).

2424

References

[1] Shervin Ardeshir and Ali Borji. Ego2top: Matching viewers

in egocentric and top-view videos. In ECCV, 2016. 1, 2, 6

[2] Dong Chen, Shaoqing Ren, Yichen Wei, Xudong Cao, and
In

Jian Sun. Joint cascade face detection and alignment.
ECCV, 2014. 4

[3] Xinyuan Chen, Chang Xu, Xiaokang Yang, and Dacheng
Tao. Attention-gan for object transﬁguration in wild images.
In ECCV, 2018. 3

[4] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image
translation. In CVPR, 2018. 3

[5] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese. 3d-r2n2: A uniﬁed approach for
single and multi-view 3d object reconstruction.
In ECCV,
2016. 3

[6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016. 3

[7] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-
In

mantic segmentation via multi-task network cascades.
CVPR, 2016. 4

[8] Haoye Dong, Xiaodan Liang, Ke Gong, Hanjiang Lai, Jia
Zhu, and Jian Yin. Soft-gated warping-gan for pose-guided
person image synthesis. In NeurIPS, 2018. 3

[9] Alexey Dosovitskiy,

Jost Tobias Springenberg, Maxim
Tatarchenko, and Thomas Brox. Learning to generate chairs,
tables and cars with convolutional networks. IEEE TPAMI,
39(4):692–705, 2017. 3

[10] Xavier Glorot and Yoshua Bengio. Understanding the dif-
In

ﬁculty of training deep feedforward neural networks.
ICAIS, 2010. 6

[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
2, 6

[12] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Improved training of

Dumoulin, and Aaron Courville.
wasserstein gans. In NIPS, 2017. 2

[13] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A
fast learning algorithm for deep belief nets. MIT Press Neu-
ral computation, 18(7):1527–1554, 2006. 2

[14] Geoffrey E Hinton and Ruslan R Salakhutdinov. A better
way to pretrain deep boltzmann machines. In NIPS, 2012. 2
[15] Rui Huang, Shu Zhang, Tianyu Li, Ran He, et al. Beyond
face rotation: Global and local perception gan for photoreal-
istic and identity preserving frontal view synthesis. In ICCV,
2017. 1

[16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, 2017. 2, 3, 5, 6, 7, 8

[18] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. In ICLR, 2018. 2

[19] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geome-
try and semantics. In CVPR, 2018. 5

[20] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee,
and Jiwon Kim. Learning to discover cross-domain relations
with generative adversarial networks. In ICML, 2017. 1

[21] Diederik Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015. 6

[22] Mohamed Ilyes Lakhal, Oswald Lanz, and Andrea Caval-
laro. Pose guided human image synthesis by view disentan-
glement and enhanced weighting loss. In ECCV, 2018. 3

[23] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian D
Reid. Reﬁnenet: Multi-path reﬁnement networks for high-
resolution semantic segmentation. In CVPR, 2017. 6

[24] Shuang Ma, Jianlong Fu, Chang Wen Chen, and Tao Mei.
Da-gan: Instance-level image translation by deep attention
generative adversarial networks. In CVPR, 2018. 3

[25] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep
multi-scale video prediction beyond mean square error.
ICLR, 2016. 3

[26] Youssef Alami Mejjati, Christian Richardt, James Tompkin,
Darren Cosker, and Kwang In Kim. Unsupervised attention-
guided image-to-image translation. In NeurIPS, 2018. 3

[27] Mehdi Mirza and Simon Osindero. Conditional generative

adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 2

[28] Natalia Neverova, Riza Alp Guler, and Iasonas Kokkinos.

Dense pose transfer. In ECCV, 2018. 3

[29] Augustus Odena. Semi-supervised learning with generative

adversarial networks. In ICML Workshop, 2016. 3

[30] Augustus Odena, Christopher Olah, and Jonathon Shlens.
Conditional image synthesis with auxiliary classiﬁer gans.
In ICML, 2017. 3

[31] Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan,
and Alexander C Berg. Transformation-grounded image
generation network for novel 3d view synthesis. In CVPR,
2017. 1

[32] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gener-
ative adversarial networks. In ICLR, 2016. 2

[33] Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka,
Bernt Schiele, and Honglak Lee. Learning what and where
to draw. In NIPS, 2016. 3

[34] Krishna Regmi and Ali Borji. Cross-view image synthesis

using conditional gans. In CVPR, 2018. 1, 2, 3, 6, 7, 8

[35] David E Rumelhart and James L McClelland. Parallel dis-
tributed processing: explorations in the microstructure of
cognition. volume 1. foundations. 1986. 2

[36] Hao Tang, Xinya Chen, Wei Wang, Dan Xu, Jason J. Corso,
Nicu Sebe, and Yan Yan. Attribute-guided sketch generation.
In FG, 2019. 3

[17] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
ECCV, 2016. 5

[37] Hao Tang, Wei Wang, Dan Xu, Yan Yan, and Nicu Sebe.
Gesturegan for hand gesture-to-gesture translation in the
wild. In ACM MM, 2018. 3

2425

[38] Hao Tang, Dan Xu, Nicu Sebe, and Yan Yan. Attention-
guided generative adversarial networks for unsupervised
image-to-image translation. In IJCNN, 2019. 3

[39] Hao Tang, Dan Xu, Wei Wang, Yan Yan, and Nicu Sebe.
Dual generator generative adversarial networks for multi-
domain image-to-image translation. In ACCV, 2018. 3

[40] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.
Multi-view 3d models from single images with a convolu-
tional network. In ECCV, 2016. 1, 3

[41] Nam N Vo and James Hays. Localizing and orienting street

views using overhead imagery. In ECCV, 2016. 1, 2, 6

[42] Xiaolong Wang and Abhinav Gupta. Generative image mod-
In

eling using style and structure adversarial networks.
ECCV, 2016. 2

[43] Scott Workman, Richard Souvenir, and Nathan Jacobs.
Wide-area image geolocalization with aerial reference im-
agery. In ICCV, 2015. 1, 2, 6

[44] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In CVPR, 2018. 3

[45] Jimei Yang, Scott E Reed, Ming-Hsuan Yang, and Honglak
Lee. Weakly-supervised disentangling with recurrent trans-
formations for 3d view synthesis. In NIPS, 2015. 1

[46] Xiaochuan Yin, Henglai Wei, Xiangwei Wang, Qijun Chen,
et al. Novel view synthesis for large-scale scene using adver-
sarial loss. arXiv preprint arXiv:1802.07064, 2018. 3

[47] Menghua Zhai, Zachary Bessinger, Scott Workman, and
Nathan Jacobs. Predicting ground-level scene layout from
aerial imagery. In CVPR, 2017. 1, 2, 3, 6, 7, 8

[48] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-
tus Odena. Self-attention generative adversarial networks.
arXiv preprint arXiv:1805.08318, 2018. 2

[49] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan:
Text to photo-realistic image synthesis with stacked genera-
tive adversarial networks. In ICCV, 2017. 3

[50] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
In

Wang, and Jiaya Jia. Pyramid scene parsing network.
CVPR, 2017. 4

[51] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Scene parsing through

Barriuso, and Antonio Torralba.
ade20k dataset. In CVPR, 2017. 6

[52] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Ma-
lik, and Alexei A Efros. View synthesis by appearance ﬂow.
In ECCV, 2016. 1, 3

[53] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017. 3

[54] Xinge Zhu, Zhichao Yin, Jianping Shi, Hongsheng Li, and
Dahua Lin. Generative adversarial frontal view to bird view
synthesis. In 3DV, 2018. 1

2426

