Context-Aware Crowd Counting

Weizhe Liu Mathieu Salzmann

Pascal Fua

Computer Vision Laboratory, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)

{weizhe.liu, mathieu.salzmann, pascal.fua}@epfl.ch

Abstract

State-of-the-art methods for counting people in crowded
scenes rely on deep networks to estimate crowd density.
They typically use the same ﬁlters over the whole image or
over large image patches. Only then do they estimate local
scale to compensate for perspective distortion. This is typ-
ically achieved by training an auxiliary classiﬁer to select,
for predeﬁned image patches, the best kernel size among a
limited set of choices. As such, these methods are not end-
to-end trainable and restricted in the scope of context they
can leverage.

In this paper, we introduce an end-to-end trainable deep
architecture that combines features obtained using multiple
receptive ﬁeld sizes and learns the importance of each such
feature at each image location.
In other words, our ap-
proach adaptively encodes the scale of the contextual infor-
mation required to accurately predict crowd density. This
yields an algorithm that outperforms state-of-the-art crowd
counting methods, especially when perspective effects are
strong.

1. Introduction

Crowd counting is important for applications such as
video surveillance and trafﬁc control. In recent years, the
emphasis has been on developing counting-by-density al-
gorithms that rely on regressors trained to estimate the
people density per unit area so that the total number can
be obtained by integration, without explicit detection be-
ing required. The regressors can be based on Random
Forests [18], Gaussian Processes [7], or more recently Deep
Nets [41, 42, 26, 31, 40, 36, 32, 24, 19, 30, 33, 22, 15, 28, 5],
with most state-of-the-art approaches now relying on the
latter.

Standard convolutions are at the heart of these deep-
learning-based approaches. By using the same ﬁlters and
pooling operations over the whole image, these implicitly
rely on the same receptive ﬁeld everywhere. However,
due to perspective distortion, one should instead change
the receptive ﬁeld size across the image. In the past, this

has been addressed by combining either density maps ex-
tracted from image patches at different resolutions [26] or
feature maps obtained with convolutional ﬁlters of differ-
ent sizes [42, 5]. However, by indiscriminately fusing in-
formation at all scales, these methods ignore the fact that
scale varies continuously across the image. While this was
addressed in [31, 30] by training classiﬁers to predict the
size of the receptive ﬁeld to use locally, the resulting meth-
ods are not end-to-end trainable; cannot account for rapid
scale changes because they assign a single scale to relatively
large patches; and can only exploit a small range of recep-
tive ﬁelds for the networks to remain of a manageable size.
In this paper, we introduce a deep architecture that ex-
plicitly extracts features over multiple receptive ﬁeld sizes
and learns the importance of each such feature at every
image location, thus accounting for potentially rapid scale
changes. In other words, our approach adaptively encodes
the scale of the contextual information necessary to predict
crowd density. This is in contrast to crowd-counting ap-
proaches that also use contextual information to account for
scaling effects as in [32], but only in the loss function as
opposed to computing true multi-scale features as we do.
We will show that it works better on uncalibrated images.
When calibration data is available, we will also show that
it can be leveraged to infer suitable local scales even better
and further increase performance.

Our contribution is therefore an approach that incor-
porates multi-scale contextual information directly into an
end-to-end trainable crowd counting pipeline, and learns
to exploit the right context at each image location. As
shown by our experiments, we consistently outperform the
state of the art on all standard crowd counting benchmarks,
such as ShanghaiTech, WorldExpo’10, UCF CC 50 and
UCF QNRF, as well as on our own Venice dataset1, which
features strong perspective distortion.

2. Related Work

Early crowd counting methods [39, 38, 20] tended to
rely on counting-by-detection, that is, explicitly detecting

1https://sites.google.com/view/weizheliu/home/

projects/context-aware-crowd-counting

5099

individual heads or bodies and then counting them. Unfor-
tunately, in very crowded scenes, occlusions make detec-
tion difﬁcult, and these approaches have been largely dis-
placed by counting-by-density-estimation ones, which rely
on training a regressor to estimate people density in vari-
ous parts of the image and then integrating. This trend be-
gan in [7, 18, 10], using either Gaussian Process or Ran-
dom Forests regressors. Even though approaches relying
on low-level features [9, 6, 4, 27, 7, 14] can yield good re-
sults, they have now mostly been superseded by CNN-based
methods [42, 31, 5], a survey of which can be found in [36].
The same can be said about methods that count objects in-
stead of people [1, 2, 8].

The people density we want to measure is the number
of people per unit area on the ground. However, the deep
nets operate in the image plane and, as a result, the den-
sity estimate can be severely affected by the local scale of a
pixel, that is, the ratio between image area and correspond-
ing ground area. This problem has long been recognized.
For example, the algorithms of [41, 17] use geometric in-
formation to adapt the network to different scene geome-
tries. Because this information is not always readily avail-
able, other works have focused on handling the scale im-
plicitly within the model. In [36], this was done by learning
to predict pre-deﬁned density levels. These levels, how-
ever, need to be provided by a human annotator at train-
ing time. By contrast, the algorithms of [26, 32] use im-
age patches extracted at multiple scales as input to a multi-
stream network. They then either fuse the features for ﬁnal
density prediction [26] without accounting for continuous
scale changes or introduce an ad hoc term in the training
loss function [32] to enforce prediction consistency across
scales. This, however, does not encode contextual informa-
tion into the features produced by the network and therefore
has limited impact. While [42, 5] aim to learn multi-scale
features, by using different receptive ﬁelds, they combine
all of these features to predict the density.

In other words, while the previous methods account for
scale, they ignore the fact that the suitable scale varies
smoothly over the image and should be handled adaptively.
This was addressed in [16] by weighting different density
maps generated from input images at various scales. How-
ever, the density map at each scale only depends on features
extracted at this particular scale, and thus may already be
corrupted by the lack of adaptive-scale reasoning. Here,
we argue that one should rather extract features at multiple
scales and learn how to adaptively combine them. While
this, in essence, was also the motivation of [31, 30], which
train an extra classiﬁer to assign the best receptive ﬁeld for
each image patch, these methods remain limited in several
important ways. First, they rely on classiﬁers, which re-
quires pre-training the network before training the classiﬁer,
and thus is not end-to-end trainable. Second, they typically

assign a single scale to an entire image patch that can still
be large and thus do not account for rapid scale changes.
Last, but not least, the range of receptive ﬁeld sizes they
rely on remains limited in part because using much larger
ones would require using much deeper architectures, which
may not be easy to train given the kind of networks being
used.

By contrast, in this paper, we introduce an end-to-end
trainable architecture that adaptively fuses multi-scale fea-
tures, without explicitly requiring deﬁning patches, but
rather by learning how to weigh these features for each in-
dividual pixel, thus allowing us to accommodate rapid scale
changes. By leveraging multi-scale pooling operations, our
framework can cover an arbitrarily large range of receptive
ﬁelds, thus enabling us to account for much larger context
than with the multiple receptive ﬁelds used by the above-
mentioned methods. In Section 4, we will demonstrate that
it delivers superior performance.

3. Approach

As discussed above, we aim to exploit context, that is,
the large-scale consistencies that often appear in images.
However, properly assessing what the scope and extent of
this context should be in images that have undergone per-
spective distortion is a challenge. To meet it, we intro-
duce a new deep net architecture that adaptively encodes
multi-level contextual information into the features it pro-
duces. We then show how to use these scale-aware features
to regress to a ﬁnal density map, both when the cameras are
not calibrated and when they are.

3.1. Scale Aware Contextual Features

We formulate crowd counting as regressing a people den-
sity map from an image. Given a set of N training images
{Ii}1≤i≤N with corresponding ground-truth density maps
{Dgt
i }, our goal is to learn a non-linear mapping F param-
eterized by θ that maps an input image Ii to an estimated
density map Dest
(Ii) = F(Ii, θ) that is as similar as possi-
ble to Dgt
i

in L2 norm terms.

i

Following common practice [25, 29, 23], our starting
point is a network comprising the ﬁrst ten layers of a pre-
trained VGG-16 network [34]. Given an image I, it outputs
features of the form

fv = Fvgg(I) ,

(1)

which we take as base features to build our scale-aware
ones.

As discussed in Section 2, the limitation of Fvgg is that
it encodes the same receptive ﬁeld over the entire image. To
remedy this, we compute scale-aware features by perform-
ing Spatial Pyramid Pooling [11] to extract multi-scale con-
text information from the VGG features of Eq. 1. Speciﬁ-
cally, as illustrated at the bottom of Fig. 1, we compute these

5100

front-end network

input image

feature maps

Average 
pooling

Conv

Conv

Conv

Conv

Upsam

ple

Upsam

ple

Upsam

ple

Upsam

ple

-

--

-

-

Conv

Conv

Conv

Conv

×

×

×

×

weighted feature maps

back-end decoder

density map

concatenation

-

Input image

VGG-16 network

VGG features fv<

Average pooling

.
.
.

.
.
.

.
.
.

…

…

…

…

…

…

…

.
.
.

…

…

…

…

…

…

…

.
.
.

1 × 1

Conv

.
.
.

.
.
.

.
.
.

bilinear interpolation

scale features sj

contrast features cj

k(j) × k(j)

 blocks

k(j) × k(j)

 blocks

Figure 1: Context-Aware Network. (Top) RGB images are fed to a font-end network that comprises the ﬁrst 10 layers of the VGG-16
network. The resulting local features are grouped in blocks of different sizes by average pooling followed by a 1×1 convolutional layer.
They are then up-sampled back to the original feature size to form the contrast features. Contrast features are further used to learn the
weights for the scale-aware features that are then fed to a back-end network to produce the ﬁnal density map. (Bottom) As shown in this
expanded version of the ﬁrst part of the network, the contrast features are the difference between local features and context features.

scale-aware features as

sj = Ubi(Fj(Pave(fv, j), θj)) ,

(2)

where, for each scale j, Pave(·, j) averages the VGG fea-
tures into k(j) × k(j) blocks; Fj is a convolutional network
with kernel size 1 to combine the context features across
channels without changing their dimensions. We do this
because SPP keeps each feature channel independent, thus
limiting the representation power. We veriﬁed that with-
out this the performance drops. This is in contrast to earlier
arthitectures that convolve to reduce the dimension [37, 43];
and Ubi represents bilinear interpolation to up-sample the
array of contextual features to be of the same size as fv. In
practice, we use S = 4 different scales, with corresponding
block sizes k(j) ∈ {1, 2, 3, 6} since it shows better perfor-
mance compared with other settings.

The simplest way to use our scale-aware features would
be to concatenate all of them to the original VGG features
fv. This, however, would not account for the fact that scale
varies across the image. To model this, we propose to learn
to predict weight maps that set the relative inﬂuence of each
scale-aware feature at each spatial location. To this end, we
ﬁrst deﬁne contrast features as

cj = sj − fv .

(3)

They capture the differences between the features at a spe-
ciﬁc location and those in the neighborhood, which often

is an important visual cue that denotes saliency. Note that,
for human beings, saliency matters. For example, in the
image of Fig. 2, the eye is naturally drawn to the woman
at the center in part because edges in the rest of the image
all point in her direction and that edges at her location do
not. In our context, these contrast features provide us with
important information to understand the local scale of each
image region. We therefore exploit them as input to aux-
iliary networks with weights θj
sa that compute the weights
ωj assigned to each one of the S different scales we use.
Each such network outputs a scale-speciﬁc weight map of
the form

ωj = F j

sa(cj, θj

sa) .

(4)

F j
sa is a 1×1 convolutional layer followed by a sigmoid
function to avoid division by zero. We then employ these
weights to compute our ﬁnal contextual features as

fI ="fv|PS
PS

j=1 ωj ⊙ sj

j=1 ωj

# ,

(5)

where [·|·] denotes the channel-wise concatenation opera-
tion, and ⊙ is the element-wise product between a weight
map and a feature map.

Altogether, as illustrated in Fig. 1, the network F(I, θ)
extracts the contextual features fI as discussed above, which
are then passed to a decoder consisting of several dilated
convolutions that produces the density map. The speciﬁc ar-
chitecture of the network is described in Table 1. As shown

5101

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
R
E
K
h
q
o
+
1
6
7
D
W
3
X
o
j
I
a
X
c
e
R
b
G
l
f
E
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
q
2
F
J
p
T
N
d
t
M
u
3
W
z
C
7
k
Q
o
o
X
/
D
i
w
d
F
v
P
p
n
v
P
l
v
3
L
Q
5
a
O
s
L
C
w
/
v
z
D
C
z
b
5
h
K
Y
d
B
1
v
5
3
K
2
v
r
G
5
l
Z
1
u
7
a
z
u
7
d
/
U
D
8
8
6
p
o
k
0
4
x
3
W
C
I
T
3
Q
u
p
4
V
I
o
3
k
G
B
k
v
d
S
z
W
k
c
S
v
4
Y
T
m
6
L
+
u
M
T
1
0
Y
k
6
g
G
n
K
Q
9
i
O
l
I
i
E
o
y
i
t
X
y
P
+
C
h
i
b
o
h
H
a
o
N
6
w
2
2
6
c
5
F
V
8
E
p
o
Q
K
n
2
o
P
7
l
D
x
O
W
x
V
w
h
k
9
S
Y
v
u
e
m
G
O
R
U
o
2
C
S
z
2
p
+
Z
n
h
K
2
Y
S
O
e
N
+
i
o
n
Z
R
k
M
9
v
n
p
E
z
6
w
x
J
l
G
j
7
F
J
K
5
+
3
s
i
p
7
E
x
0
z
i
0
n
T
H
F
s
V
m
u
F
e
Z
/
t
X
6
G
0
X
W
Q
C
5
V
m
y
B
V
b
L
I
o
y
S
T
A
h
R
Q
B
k
K
D
R
n
K
K
c
W
K
N
P
C
3
k
r
Y
m
G
r
K
0
M
Z
U
h
O
A
t
f
3
k
V
u
h
d
N
z
/
L
9
Z
a
N
1
U
8
Z
R
h
R
M
4
h
X
P
w
4
A
p
a
c
A
d
t
6
A
C
D
F
J
7
h
F
d
6
c
z
H
l
x
3
p
2
P
R
W
v
F
K
W
e
O
4
Y
+
c
z
x
/
X
Y
p
A
8
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
R
E
K
h
q
o
+
1
6
7
D
W
3
X
o
j
I
a
X
c
e
R
b
G
l
f
E
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
q
2
F
J
p
T
N
d
t
M
u
3
W
z
C
7
k
Q
o
o
X
/
D
i
w
d
F
v
P
p
n
v
P
l
v
3
L
Q
5
a
O
s
L
C
w
/
v
z
D
C
z
b
5
h
K
Y
d
B
1
v
5
3
K
2
v
r
G
5
l
Z
1
u
7
a
z
u
7
d
/
U
D
8
8
6
p
o
k
0
4
x
3
W
C
I
T
3
Q
u
p
4
V
I
o
3
k
G
B
k
v
d
S
z
W
k
c
S
v
4
Y
T
m
6
L
+
u
M
T
1
0
Y
k
6
g
G
n
K
Q
9
i
O
l
I
i
E
o
y
i
t
X
y
P
+
C
h
i
b
o
h
H
a
o
N
6
w
2
2
6
c
5
F
V
8
E
p
o
Q
K
n
2
o
P
7
l
D
x
O
W
x
V
w
h
k
9
S
Y
v
u
e
m
G
O
R
U
o
2
C
S
z
2
p
+
Z
n
h
K
2
Y
S
O
e
N
+
i
o
n
Z
R
k
M
9
v
n
p
E
z
6
w
x
J
l
G
j
7
F
J
K
5
+
3
s
i
p
7
E
x
0
z
i
0
n
T
H
F
s
V
m
u
F
e
Z
/
t
X
6
G
0
X
W
Q
C
5
V
m
y
B
V
b
L
I
o
y
S
T
A
h
R
Q
B
k
K
D
R
n
K
K
c
W
K
N
P
C
3
k
r
Y
m
G
r
K
0
M
Z
U
h
O
A
t
f
3
k
V
u
h
d
N
z
/
L
9
Z
a
N
1
U
8
Z
R
h
R
M
4
h
X
P
w
4
A
p
a
c
A
d
t
6
A
C
D
F
J
7
h
F
d
6
c
z
H
l
x
3
p
2
P
R
W
v
F
K
W
e
O
4
Y
+
c
z
x
/
X
Y
p
A
8
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
R
E
K
h
q
o
+
1
6
7
D
W
3
X
o
j
I
a
X
c
e
R
b
G
l
f
E
=
"
>
A
A
A
B
8
3
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
q
2
F
J
p
T
N
d
t
M
u
3
W
z
C
7
k
Q
o
o
X
/
D
i
w
d
F
v
P
p
n
v
P
l
v
3
L
Q
5
a
O
s
L
C
w
/
v
z
D
C
z
b
5
h
K
Y
d
B
1
v
5
3
K
2
v
r
G
5
l
Z
1
u
7
a
z
u
7
d
/
U
D
8
8
6
p
o
k
0
4
x
3
W
C
I
T
3
Q
u
p
4
V
I
o
3
k
G
B
k
v
d
S
z
W
k
c
S
v
4
Y
T
m
6
L
+
u
M
T
1
0
Y
k
6
g
G
n
K
Q
9
i
O
l
I
i
E
o
y
i
t
X
y
P
+
C
h
i
b
o
h
H
a
o
N
6
w
2
2
6
c
5
F
V
8
E
p
o
Q
K
n
2
o
P
7
l
D
x
O
W
x
V
w
h
k
9
S
Y
v
u
e
m
G
O
R
U
o
2
C
S
z
2
p
+
Z
n
h
K
2
Y
S
O
e
N
+
i
o
n
Z
R
k
M
9
v
n
p
E
z
6
w
x
J
l
G
j
7
F
J
K
5
+
3
s
i
p
7
E
x
0
z
i
0
n
T
H
F
s
V
m
u
F
e
Z
/
t
X
6
G
0
X
W
Q
C
5
V
m
y
B
V
b
L
I
o
y
S
T
A
h
R
Q
B
k
K
D
R
n
K
K
c
W
K
N
P
C
3
k
r
Y
m
G
r
K
0
M
Z
U
h
O
A
t
f
3
k
V
u
h
d
N
z
/
L
9
Z
a
N
1
U
8
Z
R
h
R
M
4
h
X
P
w
4
A
p
a
c
A
d
t
6
A
C
D
F
J
7
h
F
d
6
c
z
H
l
x
3
p
2
P
R
W
v
F
K
W
e
O
4
Y
+
c
z
x
/
X
Y
p
A
8
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
V
D
j
2
I
0
h
+
T
r
y
K
3
v
4
9
y
5
k
f
X
5
m
j
U
v
E
=
"
>
A
A
A
B
7
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
m
k
L
b
S
i
b
7
a
R
d
u
t
m
E
3
U
2
h
h
P
4
G
L
x
4
U
8
e
o
P
8
u
a
/
c
d
v
m
o
K
0
v
L
D
y
8
M
8
P
O
v
G
E
q
u
D
a
u
+
+
2
U
N
j
a
3
t
n
f
K
u
5
W
9
/
Y
P
D
o
+
r
x
S
U
s
n
m
W
L
o
s
0
Q
k
q
h
N
S
j
Y
J
L
9
A
0
3
A
j
u
p
Q
h
q
H
A
t
v
h
+
H
5
e
b
0
9
Q
a
Z
7
I
J
z
N
N
M
Y
j
p
U
P
K
I
M
2
q
s
5
U
f
9
f
D
L
r
V
2
t
u
3
V
2
I
r
I
N
X
Q
A
0
K
N
f
v
V
r
9
4
g
Y
V
m
M
0
j
B
B
t
e
5
6
b
m
q
C
n
C
r
D
m
c
B
Z
p
Z
d
p
T
C
k
b
0
y
F
2
L
U
o
a
o
w
7
y
x
b
I
z
c
m
G
d
A
Y
k
S
Z
Z
8
0
Z
O
H
+
n
s
h
p
r
P
U
0
D
m
1
n
T
M
1
I
r
9
b
m
5
n
+
1
b
m
a
i
2
y
D
n
M
s
0
M
S
r
b
8
K
M
o
E
M
Q
m
Z
X
0
4
G
X
C
E
z
Y
m
q
B
M
s
X
t
r
o
S
N
q
K
L
M
2
H
w
q
N
g
R
v
9
e
R
1
a
F
3
V
P
c
u
P
1
7
X
G
X
R
F
H
G
c
7
g
H
C
7
B
g
x
t
o
w
A
M
0
w
Q
c
G
H
J
7
h
F
d
4
c
6
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
E
f
O
5
w
8
d
o
I
7
f
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
V
D
j
2
I
0
h
+
T
r
y
K
3
v
4
9
y
5
k
f
X
5
m
j
U
v
E
=
"
>
A
A
A
B
7
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
m
k
L
b
S
i
b
7
a
R
d
u
t
m
E
3
U
2
h
h
P
4
G
L
x
4
U
8
e
o
P
8
u
a
/
c
d
v
m
o
K
0
v
L
D
y
8
M
8
P
O
v
G
E
q
u
D
a
u
+
+
2
U
N
j
a
3
t
n
f
K
u
5
W
9
/
Y
P
D
o
+
r
x
S
U
s
n
m
W
L
o
s
0
Q
k
q
h
N
S
j
Y
J
L
9
A
0
3
A
j
u
p
Q
h
q
H
A
t
v
h
+
H
5
e
b
0
9
Q
a
Z
7
I
J
z
N
N
M
Y
j
p
U
P
K
I
M
2
q
s
5
U
f
9
f
D
L
r
V
2
t
u
3
V
2
I
r
I
N
X
Q
A
0
K
N
f
v
V
r
9
4
g
Y
V
m
M
0
j
B
B
t
e
5
6
b
m
q
C
n
C
r
D
m
c
B
Z
p
Z
d
p
T
C
k
b
0
y
F
2
L
U
o
a
o
w
7
y
x
b
I
z
c
m
G
d
A
Y
k
S
Z
Z
8
0
Z
O
H
+
n
s
h
p
r
P
U
0
D
m
1
n
T
M
1
I
r
9
b
m
5
n
+
1
b
m
a
i
2
y
D
n
M
s
0
M
S
r
b
8
K
M
o
E
M
Q
m
Z
X
0
4
G
X
C
E
z
Y
m
q
B
M
s
X
t
r
o
S
N
q
K
L
M
2
H
w
q
N
g
R
v
9
e
R
1
a
F
3
V
P
c
u
P
1
7
X
G
X
R
F
H
G
c
7
g
H
C
7
B
g
x
t
o
w
A
M
0
w
Q
c
G
H
J
7
h
F
d
4
c
6
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
E
f
O
5
w
8
d
o
I
7
f
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
V
D
j
2
I
0
h
+
T
r
y
K
3
v
4
9
y
5
k
f
X
5
m
j
U
v
E
=
"
>
A
A
A
B
7
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
m
k
L
b
S
i
b
7
a
R
d
u
t
m
E
3
U
2
h
h
P
4
G
L
x
4
U
8
e
o
P
8
u
a
/
c
d
v
m
o
K
0
v
L
D
y
8
M
8
P
O
v
G
E
q
u
D
a
u
+
+
2
U
N
j
a
3
t
n
f
K
u
5
W
9
/
Y
P
D
o
+
r
x
S
U
s
n
m
W
L
o
s
0
Q
k
q
h
N
S
j
Y
J
L
9
A
0
3
A
j
u
p
Q
h
q
H
A
t
v
h
+
H
5
e
b
0
9
Q
a
Z
7
I
J
z
N
N
M
Y
j
p
U
P
K
I
M
2
q
s
5
U
f
9
f
D
L
r
V
2
t
u
3
V
2
I
r
I
N
X
Q
A
0
K
N
f
v
V
r
9
4
g
Y
V
m
M
0
j
B
B
t
e
5
6
b
m
q
C
n
C
r
D
m
c
B
Z
p
Z
d
p
T
C
k
b
0
y
F
2
L
U
o
a
o
w
7
y
x
b
I
z
c
m
G
d
A
Y
k
S
Z
Z
8
0
Z
O
H
+
n
s
h
p
r
P
U
0
D
m
1
n
T
M
1
I
r
9
b
m
5
n
+
1
b
m
a
i
2
y
D
n
M
s
0
M
S
r
b
8
K
M
o
E
M
Q
m
Z
X
0
4
G
X
C
E
z
Y
m
q
B
M
s
X
t
r
o
S
N
q
K
L
M
2
H
w
q
N
g
R
v
9
e
R
1
a
F
3
V
P
c
u
P
1
7
X
G
X
R
F
H
G
c
7
g
H
C
7
B
g
x
t
o
w
A
M
0
w
Q
c
G
H
J
7
h
F
d
4
c
6
b
w
4
7
8
7
H
s
r
X
k
F
D
O
n
8
E
f
O
5
w
8
d
o
I
7
f
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
p
7
O
/
o
3
O
p
k
c
U
0
d
I
Y
a
W
J
p
l
F
T
h
B
R
s
8
=
"
>
A
A
A
B
7
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
m
k
L
b
S
i
b
7
a
Z
d
u
9
m
E
3
Y
l
Q
Q
n
+
D
F
w
+
K
e
P
U
H
e
f
P
f
u
G
1
z
0
N
Y
X
F
h
7
e
m
W
F
n
3
j
C
V
w
q
D
r
f
j
u
l
t
f
W
N
z
a
3
y
d
m
V
n
d
2
/
/
o
H
p
4
1
D
J
J
p
h
n
3
W
S
I
T
3
Q
m
p
4
V
I
o
7
q
N
A
y
T
u
p
5
j
Q
O
J
W
+
H
4
9
t
Z
v
f
3
E
t
R
G
J
e
s
B
J
y
o
O
Y
D
p
W
I
B
K
N
o
L
d
/
0
8
8
d
p
v
1
p
z
6
+
5
c
Z
B
W
8
A
m
p
Q
q
N
m
v
f
v
U
G
C
c
t
i
r
p
B
J
a
k
z
X
c
1
M
M
c
q
p
R
M
M
m
n
l
V
5
m
e
E
r
Z
m
A
5
5
1
6
K
i
M
T
d
B
P
l
9
2
S
s
6
s
M
y
B
R
o
u
1
T
S
O
b
u
7
4
m
c
x
s
Z
M
4
t
B
2
x
h
R
H
Z
r
k
2
M
/
+
r
d
T
O
M
r
o
N
c
q
D
R
D
r
t
j
i
o
y
i
T
B
B
M
y
u
5
w
M
h
O
Y
M
5
c
Q
C
Z
V
r
Y
X
Q
k
b
U
U
0
Z
2
n
w
q
N
g
R
v
+
e
R
V
a
F
3
U
P
c
v
3
l
7
X
G
T
R
F
H
G
U
7
g
F
M
7
B
g
y
t
o
w
B
0
0
w
Q
c
G
A
p
7
h
F
d
4
c
5
b
w
4
7
8
7
H
o
r
X
k
F
D
P
H
8
E
f
O
5
w
8
f
T
I
7
g
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
p
7
O
/
o
3
O
p
k
c
U
0
d
I
Y
a
W
J
p
l
F
T
h
B
R
s
8
=
"
>
A
A
A
B
7
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
m
k
L
b
S
i
b
7
a
Z
d
u
9
m
E
3
Y
l
Q
Q
n
+
D
F
w
+
K
e
P
U
H
e
f
P
f
u
G
1
z
0
N
Y
X
F
h
7
e
m
W
F
n
3
j
C
V
w
q
D
r
f
j
u
l
t
f
W
N
z
a
3
y
d
m
V
n
d
2
/
/
o
H
p
4
1
D
J
J
p
h
n
3
W
S
I
T
3
Q
m
p
4
V
I
o
7
q
N
A
y
T
u
p
5
j
Q
O
J
W
+
H
4
9
t
Z
v
f
3
E
t
R
G
J
e
s
B
J
y
o
O
Y
D
p
W
I
B
K
N
o
L
d
/
0
8
8
d
p
v
1
p
z
6
+
5
c
Z
B
W
8
A
m
p
Q
q
N
m
v
f
v
U
G
C
c
t
i
r
p
B
J
a
k
z
X
c
1
M
M
c
q
p
R
M
M
m
n
l
V
5
m
e
E
r
Z
m
A
5
5
1
6
K
i
M
T
d
B
P
l
9
2
S
s
6
s
M
y
B
R
o
u
1
T
S
O
b
u
7
4
m
c
x
s
Z
M
4
t
B
2
x
h
R
H
Z
r
k
2
M
/
+
r
d
T
O
M
r
o
N
c
q
D
R
D
r
t
j
i
o
y
i
T
B
B
M
y
u
5
w
M
h
O
Y
M
5
c
Q
C
Z
V
r
Y
X
Q
k
b
U
U
0
Z
2
n
w
q
N
g
R
v
+
e
R
V
a
F
3
U
P
c
v
3
l
7
X
G
T
R
F
H
G
U
7
g
F
M
7
B
g
y
t
o
w
B
0
0
w
Q
c
G
A
p
7
h
F
d
4
c
5
b
w
4
7
8
7
H
o
r
X
k
F
D
P
H
8
E
f
O
5
w
8
f
T
I
7
g
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
p
7
O
/
o
3
O
p
k
c
U
0
d
I
Y
a
W
J
p
l
F
T
h
B
R
s
8
=
"
>
A
A
A
B
7
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
m
k
L
b
S
i
b
7
a
Z
d
u
9
m
E
3
Y
l
Q
Q
n
+
D
F
w
+
K
e
P
U
H
e
f
P
f
u
G
1
z
0
N
Y
X
F
h
7
e
m
W
F
n
3
j
C
V
w
q
D
r
f
j
u
l
t
f
W
N
z
a
3
y
d
m
V
n
d
2
/
/
o
H
p
4
1
D
J
J
p
h
n
3
W
S
I
T
3
Q
m
p
4
V
I
o
7
q
N
A
y
T
u
p
5
j
Q
O
J
W
+
H
4
9
t
Z
v
f
3
E
t
R
G
J
e
s
B
J
y
o
O
Y
D
p
W
I
B
K
N
o
L
d
/
0
8
8
d
p
v
1
p
z
6
+
5
c
Z
B
W
8
A
m
p
Q
q
N
m
v
f
v
U
G
C
c
t
i
r
p
B
J
a
k
z
X
c
1
M
M
c
q
p
R
M
M
m
n
l
V
5
m
e
E
r
Z
m
A
5
5
1
6
K
i
M
T
d
B
P
l
9
2
S
s
6
s
M
y
B
R
o
u
1
T
S
O
b
u
7
4
m
c
x
s
Z
M
4
t
B
2
x
h
R
H
Z
r
k
2
M
/
+
r
d
T
O
M
r
o
N
c
q
D
R
D
r
t
j
i
o
y
i
T
B
B
M
y
u
5
w
M
h
O
Y
M
5
c
Q
C
Z
V
r
Y
X
Q
k
b
U
U
0
Z
2
n
w
q
N
g
R
v
+
e
R
V
a
F
3
U
P
c
v
3
l
7
X
G
T
R
F
H
G
U
7
g
F
M
7
B
g
y
t
o
w
B
0
0
w
Q
c
G
A
p
7
h
F
d
4
c
5
b
w
4
7
8
7
H
o
r
X
k
F
D
P
H
8
E
f
O
5
w
8
f
T
I
7
g
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
x
m
r
6
J
R
T
D
C
F
p
h
u
z
I
h
K
U
m
J
b
Y
V
Y
A
L
0
=
"
>
A
A
A
B
7
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
m
k
L
b
S
i
b
7
a
Z
d
u
9
m
E
3
Y
l
Q
Q
n
+
D
F
w
+
K
e
P
U
H
e
f
P
f
u
G
1
z
0
N
Y
X
F
h
7
e
m
W
F
n
3
j
C
V
w
q
D
r
f
j
u
l
t
f
W
N
z
a
3
y
d
m
V
n
d
2
/
/
o
H
p
4
1
D
J
J
p
h
n
3
W
S
I
T
3
Q
m
p
4
V
I
o
7
q
N
A
y
T
u
p
5
j
Q
O
J
W
+
H
4
9
t
Z
v
f
3
E
t
R
G
J
e
s
B
J
y
o
O
Y
D
p
W
I
B
K
N
o
L
Z
/
1
8
8
d
p
v
1
p
z
6
+
5
c
Z
B
W
8
A
m
p
Q
q
N
m
v
f
v
U
G
C
c
t
i
r
p
B
J
a
k
z
X
c
1
M
M
c
q
p
R
M
M
m
n
l
V
5
m
e
E
r
Z
m
A
5
5
1
6
K
i
M
T
d
B
P
l
9
2
S
s
6
s
M
y
B
R
o
u
1
T
S
O
b
u
7
4
m
c
x
s
Z
M
4
t
B
2
x
h
R
H
Z
r
k
2
M
/
+
r
d
T
O
M
r
o
N
c
q
D
R
D
r
t
j
i
o
y
i
T
B
B
M
y
u
5
w
M
h
O
Y
M
5
c
Q
C
Z
V
r
Y
X
Q
k
b
U
U
0
Z
2
n
w
q
N
g
R
v
+
e
R
V
a
F
3
U
P
c
v
3
l
7
X
G
T
R
F
H
G
U
7
g
F
M
7
B
g
y
t
o
w
B
0
0
w
Q
c
G
A
p
7
h
F
d
4
c
5
b
w
4
7
8
7
H
o
r
X
k
F
D
P
H
8
E
f
O
5
w
8
G
z
I
7
Q
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
x
m
r
6
J
R
T
D
C
F
p
h
u
z
I
h
K
U
m
J
b
Y
V
Y
A
L
0
=
"
>
A
A
A
B
7
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
m
k
L
b
S
i
b
7
a
Z
d
u
9
m
E
3
Y
l
Q
Q
n
+
D
F
w
+
K
e
P
U
H
e
f
P
f
u
G
1
z
0
N
Y
X
F
h
7
e
m
W
F
n
3
j
C
V
w
q
D
r
f
j
u
l
t
f
W
N
z
a
3
y
d
m
V
n
d
2
/
/
o
H
p
4
1
D
J
J
p
h
n
3
W
S
I
T
3
Q
m
p
4
V
I
o
7
q
N
A
y
T
u
p
5
j
Q
O
J
W
+
H
4
9
t
Z
v
f
3
E
t
R
G
J
e
s
B
J
y
o
O
Y
D
p
W
I
B
K
N
o
L
Z
/
1
8
8
d
p
v
1
p
z
6
+
5
c
Z
B
W
8
A
m
p
Q
q
N
m
v
f
v
U
G
C
c
t
i
r
p
B
J
a
k
z
X
c
1
M
M
c
q
p
R
M
M
m
n
l
V
5
m
e
E
r
Z
m
A
5
5
1
6
K
i
M
T
d
B
P
l
9
2
S
s
6
s
M
y
B
R
o
u
1
T
S
O
b
u
7
4
m
c
x
s
Z
M
4
t
B
2
x
h
R
H
Z
r
k
2
M
/
+
r
d
T
O
M
r
o
N
c
q
D
R
D
r
t
j
i
o
y
i
T
B
B
M
y
u
5
w
M
h
O
Y
M
5
c
Q
C
Z
V
r
Y
X
Q
k
b
U
U
0
Z
2
n
w
q
N
g
R
v
+
e
R
V
a
F
3
U
P
c
v
3
l
7
X
G
T
R
F
H
G
U
7
g
F
M
7
B
g
y
t
o
w
B
0
0
w
Q
c
G
A
p
7
h
F
d
4
c
5
b
w
4
7
8
7
H
o
r
X
k
F
D
P
H
8
E
f
O
5
w
8
G
z
I
7
Q
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
x
m
r
6
J
R
T
D
C
F
p
h
u
z
I
h
K
U
m
J
b
Y
V
Y
A
L
0
=
"
>
A
A
A
B
7
H
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
p
H
L
4
t
F
8
F
Q
S
E
f
R
Y
9
O
K
x
g
m
k
L
b
S
i
b
7
a
Z
d
u
9
m
E
3
Y
l
Q
Q
n
+
D
F
w
+
K
e
P
U
H
e
f
P
f
u
G
1
z
0
N
Y
X
F
h
7
e
m
W
F
n
3
j
C
V
w
q
D
r
f
j
u
l
t
f
W
N
z
a
3
y
d
m
V
n
d
2
/
/
o
H
p
4
1
D
J
J
p
h
n
3
W
S
I
T
3
Q
m
p
4
V
I
o
7
q
N
A
y
T
u
p
5
j
Q
O
J
W
+
H
4
9
t
Z
v
f
3
E
t
R
G
J
e
s
B
J
y
o
O
Y
D
p
W
I
B
K
N
o
L
Z
/
1
8
8
d
p
v
1
p
z
6
+
5
c
Z
B
W
8
A
m
p
Q
q
N
m
v
f
v
U
G
C
c
t
i
r
p
B
J
a
k
z
X
c
1
M
M
c
q
p
R
M
M
m
n
l
V
5
m
e
E
r
Z
m
A
5
5
1
6
K
i
M
T
d
B
P
l
9
2
S
s
6
s
M
y
B
R
o
u
1
T
S
O
b
u
7
4
m
c
x
s
Z
M
4
t
B
2
x
h
R
H
Z
r
k
2
M
/
+
r
d
T
O
M
r
o
N
c
q
D
R
D
r
t
j
i
o
y
i
T
B
B
M
y
u
5
w
M
h
O
Y
M
5
c
Q
C
Z
V
r
Y
X
Q
k
b
U
U
0
Z
2
n
w
q
N
g
R
v
+
e
R
V
a
F
3
U
P
c
v
3
l
7
X
G
T
R
F
H
G
U
7
g
F
M
7
B
g
y
t
o
w
B
0
0
w
Q
c
G
A
p
7
h
F
d
4
c
5
b
w
4
7
8
7
H
o
r
X
k
F
D
P
H
8
E
f
O
5
w
8
G
z
I
7
Q
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Q
7
k
R
r
t
K
T
k
R
5
k
7
d
3
i
A
x
P
V
v
w
5
T
B
u
A
=
"
>
A
A
A
B
+
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
h
H
L
4
t
F
q
J
e
S
i
K
D
H
o
h
e
P
F
W
w
t
t
K
F
s
t
p
t
2
7
e
a
D
3
U
m
h
h
P
4
T
L
x
4
U
8
e
o
/
8
e
a
/
c
d
P
m
o
K
0
v
L
D
y
8
M
8
P
M
v
n
4
i
h
U
b
H
+
b
Z
K
a
+
s
b
m
1
v
l
7
c
r
O
7
t
7
+
g
X
1
4
1
N
Z
x
q
h
h
v
s
V
j
G
q
u
N
T
z
a
W
I
e
A
s
F
S
t
5
J
F
K
e
h
L
/
m
j
P
7
7
N
6
4
8
T
r
r
S
I
o
w
e
c
J
t
w
L
6
T
A
S
g
W
A
U
j
d
W
3
7
X
H
t
6
Z
z
0
U
I
R
c
k
5
z
7
d
t
W
p
O
3
O
R
V
X
A
L
q
E
K
h
Z
t
/
+
6
g
1
i
l
o
Y
8
Q
i
a
p
1
l
3
X
S
d
D
L
q
E
L
B
J
J
9
V
e
q
n
m
C
W
V
j
O
u
R
d
g
x
E
1
m
7
x
s
f
v
m
M
n
B
l
n
Q
I
J
Y
m
R
c
h
m
b
u
/
J
z
I
a
a
j
0
N
f
d
M
Z
U
h
z
p
5
V
p
u
/
l
f
r
p
h
h
c
e
5
m
I
k
h
R
5
x
B
a
L
g
l
Q
S
j
E
k
e
A
x
k
I
x
R
n
K
q
Q
H
K
l
D
C
3
E
j
a
i
i
j
I
0
Y
V
V
M
C
O
7
y
l
1
e
h
f
V
F
3
D
d
9
f
V
h
s
3
R
R
x
l
O
I
F
T
q
I
E
L
V
9
C
A
O
2
h
C
C
x
h
M
4
B
l
e
4
c
3
K
r
B
f
r
3
f
p
Y
t
J
a
s
Y
u
Y
Y
/
s
j
6
/
A
G
c
R
Z
J
V
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Q
7
k
R
r
t
K
T
k
R
5
k
7
d
3
i
A
x
P
V
v
w
5
T
B
u
A
=
"
>
A
A
A
B
+
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
h
H
L
4
t
F
q
J
e
S
i
K
D
H
o
h
e
P
F
W
w
t
t
K
F
s
t
p
t
2
7
e
a
D
3
U
m
h
h
P
4
T
L
x
4
U
8
e
o
/
8
e
a
/
c
d
P
m
o
K
0
v
L
D
y
8
M
8
P
M
v
n
4
i
h
U
b
H
+
b
Z
K
a
+
s
b
m
1
v
l
7
c
r
O
7
t
7
+
g
X
1
4
1
N
Z
x
q
h
h
v
s
V
j
G
q
u
N
T
z
a
W
I
e
A
s
F
S
t
5
J
F
K
e
h
L
/
m
j
P
7
7
N
6
4
8
T
r
r
S
I
o
w
e
c
J
t
w
L
6
T
A
S
g
W
A
U
j
d
W
3
7
X
H
t
6
Z
z
0
U
I
R
c
k
5
z
7
d
t
W
p
O
3
O
R
V
X
A
L
q
E
K
h
Z
t
/
+
6
g
1
i
l
o
Y
8
Q
i
a
p
1
l
3
X
S
d
D
L
q
E
L
B
J
J
9
V
e
q
n
m
C
W
V
j
O
u
R
d
g
x
E
1
m
7
x
s
f
v
m
M
n
B
l
n
Q
I
J
Y
m
R
c
h
m
b
u
/
J
z
I
a
a
j
0
N
f
d
M
Z
U
h
z
p
5
V
p
u
/
l
f
r
p
h
h
c
e
5
m
I
k
h
R
5
x
B
a
L
g
l
Q
S
j
E
k
e
A
x
k
I
x
R
n
K
q
Q
H
K
l
D
C
3
E
j
a
i
i
j
I
0
Y
V
V
M
C
O
7
y
l
1
e
h
f
V
F
3
D
d
9
f
V
h
s
3
R
R
x
l
O
I
F
T
q
I
E
L
V
9
C
A
O
2
h
C
C
x
h
M
4
B
l
e
4
c
3
K
r
B
f
r
3
f
p
Y
t
J
a
s
Y
u
Y
Y
/
s
j
6
/
A
G
c
R
Z
J
V
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Q
7
k
R
r
t
K
T
k
R
5
k
7
d
3
i
A
x
P
V
v
w
5
T
B
u
A
=
"
>
A
A
A
B
+
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
h
H
L
4
t
F
q
J
e
S
i
K
D
H
o
h
e
P
F
W
w
t
t
K
F
s
t
p
t
2
7
e
a
D
3
U
m
h
h
P
4
T
L
x
4
U
8
e
o
/
8
e
a
/
c
d
P
m
o
K
0
v
L
D
y
8
M
8
P
M
v
n
4
i
h
U
b
H
+
b
Z
K
a
+
s
b
m
1
v
l
7
c
r
O
7
t
7
+
g
X
1
4
1
N
Z
x
q
h
h
v
s
V
j
G
q
u
N
T
z
a
W
I
e
A
s
F
S
t
5
J
F
K
e
h
L
/
m
j
P
7
7
N
6
4
8
T
r
r
S
I
o
w
e
c
J
t
w
L
6
T
A
S
g
W
A
U
j
d
W
3
7
X
H
t
6
Z
z
0
U
I
R
c
k
5
z
7
d
t
W
p
O
3
O
R
V
X
A
L
q
E
K
h
Z
t
/
+
6
g
1
i
l
o
Y
8
Q
i
a
p
1
l
3
X
S
d
D
L
q
E
L
B
J
J
9
V
e
q
n
m
C
W
V
j
O
u
R
d
g
x
E
1
m
7
x
s
f
v
m
M
n
B
l
n
Q
I
J
Y
m
R
c
h
m
b
u
/
J
z
I
a
a
j
0
N
f
d
M
Z
U
h
z
p
5
V
p
u
/
l
f
r
p
h
h
c
e
5
m
I
k
h
R
5
x
B
a
L
g
l
Q
S
j
E
k
e
A
x
k
I
x
R
n
K
q
Q
H
K
l
D
C
3
E
j
a
i
i
j
I
0
Y
V
V
M
C
O
7
y
l
1
e
h
f
V
F
3
D
d
9
f
V
h
s
3
R
R
x
l
O
I
F
T
q
I
E
L
V
9
C
A
O
2
h
C
C
x
h
M
4
B
l
e
4
c
3
K
r
B
f
r
3
f
p
Y
t
J
a
s
Y
u
Y
Y
/
s
j
6
/
A
G
c
R
Z
J
V
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Q
7
k
R
r
t
K
T
k
R
5
k
7
d
3
i
A
x
P
V
v
w
5
T
B
u
A
=
"
>
A
A
A
B
+
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
h
H
L
4
t
F
q
J
e
S
i
K
D
H
o
h
e
P
F
W
w
t
t
K
F
s
t
p
t
2
7
e
a
D
3
U
m
h
h
P
4
T
L
x
4
U
8
e
o
/
8
e
a
/
c
d
P
m
o
K
0
v
L
D
y
8
M
8
P
M
v
n
4
i
h
U
b
H
+
b
Z
K
a
+
s
b
m
1
v
l
7
c
r
O
7
t
7
+
g
X
1
4
1
N
Z
x
q
h
h
v
s
V
j
G
q
u
N
T
z
a
W
I
e
A
s
F
S
t
5
J
F
K
e
h
L
/
m
j
P
7
7
N
6
4
8
T
r
r
S
I
o
w
e
c
J
t
w
L
6
T
A
S
g
W
A
U
j
d
W
3
7
X
H
t
6
Z
z
0
U
I
R
c
k
5
z
7
d
t
W
p
O
3
O
R
V
X
A
L
q
E
K
h
Z
t
/
+
6
g
1
i
l
o
Y
8
Q
i
a
p
1
l
3
X
S
d
D
L
q
E
L
B
J
J
9
V
e
q
n
m
C
W
V
j
O
u
R
d
g
x
E
1
m
7
x
s
f
v
m
M
n
B
l
n
Q
I
J
Y
m
R
c
h
m
b
u
/
J
z
I
a
a
j
0
N
f
d
M
Z
U
h
z
p
5
V
p
u
/
l
f
r
p
h
h
c
e
5
m
I
k
h
R
5
x
B
a
L
g
l
Q
S
j
E
k
e
A
x
k
I
x
R
n
K
q
Q
H
K
l
D
C
3
E
j
a
i
i
j
I
0
Y
V
V
M
C
O
7
y
l
1
e
h
f
V
F
3
D
d
9
f
V
h
s
3
R
R
x
l
O
I
F
T
q
I
E
L
V
9
C
A
O
2
h
C
C
x
h
M
4
B
l
e
4
c
3
K
r
B
f
r
3
f
p
Y
t
J
a
s
Y
u
Y
Y
/
s
j
6
/
A
G
c
R
Z
J
V
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Q
7
k
R
r
t
K
T
k
R
5
k
7
d
3
i
A
x
P
V
v
w
5
T
B
u
A
=
"
>
A
A
A
B
+
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
h
H
L
4
t
F
q
J
e
S
i
K
D
H
o
h
e
P
F
W
w
t
t
K
F
s
t
p
t
2
7
e
a
D
3
U
m
h
h
P
4
T
L
x
4
U
8
e
o
/
8
e
a
/
c
d
P
m
o
K
0
v
L
D
y
8
M
8
P
M
v
n
4
i
h
U
b
H
+
b
Z
K
a
+
s
b
m
1
v
l
7
c
r
O
7
t
7
+
g
X
1
4
1
N
Z
x
q
h
h
v
s
V
j
G
q
u
N
T
z
a
W
I
e
A
s
F
S
t
5
J
F
K
e
h
L
/
m
j
P
7
7
N
6
4
8
T
r
r
S
I
o
w
e
c
J
t
w
L
6
T
A
S
g
W
A
U
j
d
W
3
7
X
H
t
6
Z
z
0
U
I
R
c
k
5
z
7
d
t
W
p
O
3
O
R
V
X
A
L
q
E
K
h
Z
t
/
+
6
g
1
i
l
o
Y
8
Q
i
a
p
1
l
3
X
S
d
D
L
q
E
L
B
J
J
9
V
e
q
n
m
C
W
V
j
O
u
R
d
g
x
E
1
m
7
x
s
f
v
m
M
n
B
l
n
Q
I
J
Y
m
R
c
h
m
b
u
/
J
z
I
a
a
j
0
N
f
d
M
Z
U
h
z
p
5
V
p
u
/
l
f
r
p
h
h
c
e
5
m
I
k
h
R
5
x
B
a
L
g
l
Q
S
j
E
k
e
A
x
k
I
x
R
n
K
q
Q
H
K
l
D
C
3
E
j
a
i
i
j
I
0
Y
V
V
M
C
O
7
y
l
1
e
h
f
V
F
3
D
d
9
f
V
h
s
3
R
R
x
l
O
I
F
T
q
I
E
L
V
9
C
A
O
2
h
C
C
x
h
M
4
B
l
e
4
c
3
K
r
B
f
r
3
f
p
Y
t
J
a
s
Y
u
Y
Y
/
s
j
6
/
A
G
c
R
Z
J
V
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
Q
7
k
R
r
t
K
T
k
R
5
k
7
d
3
i
A
x
P
V
v
w
5
T
B
u
A
=
"
>
A
A
A
B
+
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
n
9
a
v
W
r
6
h
H
L
4
t
F
q
J
e
S
i
K
D
H
o
h
e
P
F
W
w
t
t
K
F
s
t
p
t
2
7
e
a
D
3
U
m
h
h
P
4
T
L
x
4
U
8
e
o
/
8
e
a
/
c
d
P
m
o
K
0
v
L
D
y
8
M
8
P
M
v
n
4
i
h
U
b
H
+
b
Z
K
a
+
s
b
m
1
v
l
7
c
r
O
7
t
7
+
g
X
1
4
1
N
Z
x
q
h
h
v
s
V
j
G
q
u
N
T
z
a
W
I
e
A
s
F
S
t
5
J
F
K
e
h
L
/
m
j
P
7
7
N
6
4
8
T
r
r
S
I
o
w
e
c
J
t
w
L
6
T
A
S
g
W
A
U
j
d
W
3
7
X
H
t
6
Z
z
0
U
I
R
c
k
5
z
7
d
t
W
p
O
3
O
R
V
X
A
L
q
E
K
h
Z
t
/
+
6
g
1
i
l
o
Y
8
Q
i
a
p
1
l
3
X
S
d
D
L
q
E
L
B
J
J
9
V
e
q
n
m
C
W
V
j
O
u
R
d
g
x
E
1
m
7
x
s
f
v
m
M
n
B
l
n
Q
I
J
Y
m
R
c
h
m
b
u
/
J
z
I
a
a
j
0
N
f
d
M
Z
U
h
z
p
5
V
p
u
/
l
f
r
p
h
h
c
e
5
m
I
k
h
R
5
x
B
a
L
g
l
Q
S
j
E
k
e
A
x
k
I
x
R
n
K
q
Q
H
K
l
D
C
3
E
j
a
i
i
j
I
0
Y
V
V
M
C
O
7
y
l
1
e
h
f
V
F
3
D
d
9
f
V
h
s
3
R
R
x
l
O
I
F
T
q
I
E
L
V
9
C
A
O
2
h
C
C
x
h
M
4
B
l
e
4
c
3
K
r
B
f
r
3
f
p
Y
t
J
a
s
Y
u
Y
Y
/
s
j
6
/
A
G
c
R
Z
J
V
<
/
l
a
t
e
x
i
t
>
layer
1 - 2

3 - 4

5 - 7

8 - 10

front-end(Fvgg)
3×3×64 conv-1
2 × 2 max pooling
3×3×128 conv-1
2 × 2 max pooling
3×3×256 conv-1
2 × 2 max pooling
3×3×512 conv-1

layer

1
2
3
4
5
6
7

back-end decoder
3×3×512 conv-2
3×3×512 conv-2
3×3×512 conv-2
3×3×256 conv-2
3×3×128 conv-2
3×3×64 conv-2
1×1×1 conv-1

Table 1: Network architecture of proposed model Convolu-
tional layers are represented as “(kernel size) × (kernel size) ×
(number of ﬁlters) conv-(dilation rate)”.

Figure 2: Context and saliency. People’s gaze tends be drawn to
the person in the center, probably because most the image edges
point in that direction.

computed as

ωj = F j

gc([cj|fg] , θj

gc) .

(7)

by our experiments, this network already outperforms the
state of the art on all benchmark datasets, without explicitly
using information about camera geometry. As discussed be-
low, however, these results can be further improved when
such information is available.

3.2. Geometry Guided Context Learning

Because of perspective distortion, the contextual scope
suitable for each region varies across the image plane.
Hence, scene geometry is highly related to contextual in-
formation and could be used to guide the network to better
adjust to the scene context it needs.

We therefore extend the previous approach to exploiting
geometry information when it is available. To this end, we
represent the scene geometry of image Ii with a perspective
map Mi, which encodes the number of pixels per meter in
the image plane. Note that this perspective map has the
same spatial resolution as the input image. We therefore use
it as input to a truncated VGG-16 network. In other words,
the base features of Eq. 1 are then replaced by features of
the form

fg = F ′

vgg(Mi, θg) ,

(6)

where F ′
vgg is a modiﬁed VGG-16 network with a single
input channel. To initialize the weights corresponding to
this channel, we average those of the original three RGB
channels. Note that we also normalize the perspective map
Mi to lie within the same range as the RGB images. Even
though this initialization does not bring any obvious differ-
ence in the ﬁnal counting accuracy, it makes the network
converge much faster.

To further propagate the geometry information to later
stages of our network, we exploit the modiﬁed VGG fea-
tures described above, which inherently contain geometry
information, as an additional input to the auxiliary network
of Eq. 4. Speciﬁcally, the weight map for each scale is then

These weight maps are then used as in Eq. 5. Fig. 3 depicts
the corresponding architecture.

3.3. Training Details and Loss Function

Whether with or without geometry information, our net-

works are trained using the L2 loss deﬁned as

L(θ) =

1
2B

B

Xi=1

kDgt

i − Dest

i k2
2 ,

(8)

where B is the batch size. To obtain the ground-truth den-
sity maps Dgt
i , we rely on the same strategy as previous
work [19, 31, 42, 30]. Speciﬁcally, to each image Ii, we
associate a set of ci 2D points P gt
i }1≤j≤ci that de-
note the position of each human head in the scene. The
corresponding ground-truth density map Dgt
is obtained by
i
convolving an image containing ones at these locations and
zeroes elsewhere with a Gaussian kernel N gt(p|µ, σ2) [21].
We write

i = {P j

∀p ∈ Ii, Dgt

i (p|Ii) =

ci

Xj=1

N gt(p|µ = P j

i , σ2) ,

(9)

where µ and σ represent the mean and standard deviation of
the normal distribution. To produce the comparative results
we will show in Section 4, we use the same σ as the methods
we compare against.

To minimize the loss of Eq. 8, we use Stochastic Gradi-
ent Descent (SGD) with batch size 1 for various size dataset
and Adam with batch size 32 for ﬁxed size dataset. Further-
more, during training, we randomly crop image patches of
1
4 the size of the original image at different locations. These
patches are further mirrored to double the training set.

4. Experiments

In this section, we evaluate the proposed approach.
We ﬁrst introduce the evaluation metrics and benchmark

5102

perspective map

front-end network

feature maps

Conv

Upsample

-

Conv

Conv

Upsample

Conv

Upsample

-

-

Conv

Conv

Average 
pooling

input image

front-end network

feature maps

Conv

Upsample

-

Conv

×

×

×

×

weighted feature maps

back-end decoder

density map

Figure 3: Expanded Context-Aware Network. To account for camera registration information when available, we add a branch to the
architecture of Fig. 1. It takes as input a perspective map that encodes local scale. Its output is concatenated to the original contrast features
and the resulting scale-aware features are used to estimate people density.

concatenation

datasets we use in our experiments. We then compare our
approach to state-of-the-art methods, and ﬁnally perform a
detailed ablation study.

4.1. Evaluation Metrics

Previous works in crowd density estimation use the mean
absolute error (M AE) and the root mean squared error
(RM SE) as evaluation metrics [42, 41, 26, 31, 40, 36].
They are deﬁned as

M AE =

1
N

N

Xi=1

|zi− ˆzi| and RM SE =vuut

1
N

N

Xi=1

(zi − ˆzi)2 ,

where N is the number of test images, zi denotes the true
number of people inside the ROI of the ith image and ˆzi
the estimated number of people. In the benchmark datasets
discussed below, the ROI is the whole image except when
explicitly stated otherwise. Note that number of people can
be recovered by integrating over the pixels of the predicted

density maps as ˆzi =Pp∈Ii

Dest

i

(p|Ii).

4.2. Benchmark Datasets and Ground truth Data

We use ﬁve different datasets to compare our approach to
recent ones. The ﬁrst four were released along with recent
papers and have already been used for comparison purposes
since. We created the ﬁfth one ourselves and will make it
publicly available as well.

ShanghaiTech [42].
It comprises 1,198 annotated images
with 330,165 people in them. It is divided in part A with
482 images and part B with 716.
In part A, 300 images
form the training set and, in part B, 400. The remainder are
used for testing purposes. For a fair comparison with ear-
lier work [42, 32, 19, 33], we created the ground-truth den-
sity maps in the same manner as they did. Speciﬁcally, for
Part A, we used the geometry-adaptive kernels introduced

(a) Input image

(b) Ground truth

(c) Our prediction

Figure 4: Crowd density estimation on ShanghaiTech. First
row: Image from Part A. Second row: Image from Part B. Our
model adjusts to rapid scale changes and delivers density maps
that are close to the ground truth.

in [42], and for part B, ﬁxed kernels. In Fig. 4, we show one
image from each part, along with the ground-truth density
maps and those estimated by our algorithm.

UCF-QNRF [15].
It comprises 1,535 jpeg images with
1,251,642 people in them. The training set is made of 1,201
of these images. Unlike in ShanghaiTech, there are dra-
matic variations both in crowd density and image resolu-
tion. The ground-truth density maps were generated by
adaptive Gaussian kernels as in [15].

UCF CC 50 [14].
It contains only 50 images with a peo-
ple count varying from 94 to 4,543, which makes it chal-
lenging for a deep-learning approach. For a fair comparison
again, the ground-truth density maps were generated using
ﬁxed kernels and we follow the same 5-fold cross-validation
protocol as in [14]: We partition the images into 5 10-image
groups. In turn, we then pick four groups for training and
the remaining one for testing. This gives us 5 sets of results
and we report their average.

5103

Model
Zhang et al. [41]
MCNN [42]
Switch-CNN [31]
CP-CNN [36]
ACSCP [32]
Liu et al. [24]
D-ConvNet [33]
IG-CNN [30]
ic-CNN[28]
CSRNet [19]
SANet [5]
OURS-CAN

Part A

Part B

M AE RM SE M AE RM SE
181.8
110.2
90.4
73.6
75.7
73.6
73.5
72.5
68.5
68.2
67.0
62.3

277.7
173.2
135.0
106.4
102.7
112.0
112.3
118.2
116.2
115.0
104.5
100.0

32.0
26.4
21.6
20.1
17.2
13.7
18.7
13.6
10.7
10.6
8.4
7.8

49.8
41.3
33.4
30.1
27.4
21.4
26.0
21.1
16.0
16.0
13.6
12.2

Table 2: Comparative results on the ShanghaiTech dataset.

WorldExpo’10 [41].
It comprises 1,132 annotated video
sequences collected from 103 different scenes. There are
3,980 annotated frames, with 3,380 of them used for train-
ing purposes. Each scene contains a Region Of Interest
(ROI) in which people are counted. The bottom row of
Fig. 5 depicts three of these images and the associated cam-
era calibration data. We generate the ground-truth den-
sity maps as in our baselines [31, 19, 5]. As in previous
work [41, 42, 31, 30, 19, 5, 21, 36, 32, 28, 33] on this
dataset, we report the MAE of each scene, as well as the
average over all scenes.

Venice. The four datasets discussed above have the ad-
vantage of being publicly available but do not contain pre-
cise calibration information.
In practice, however, it can
be readily obtained using either standard photogrammetry
techniques or onboard sensors, for example when using a
drone to acquire the images. To test this kind of scenario,
we used a cellphone to ﬁlm additional sequences of the
Piazza San Marco in Venice, as seen from various view-
points on the second ﬂoor of the basilica, as shown in the
top two rows of Fig. 5. We then used the white lines on
the ground to compute camera models. As shown in the
bottom two rows of Fig. 5, this yields a more accurate cal-
ibration than in WorldExpo’10. The resulting dataset con-
tains 4 different sequences and in total 167 annotated frames
with ﬁxed 1,280 × 720 resolution. 80 images from a single
long sequence are taken as training data, and we use the im-
ages from the remaining 3 sequences for testing purposes.
The ground-truth density maps were generated using ﬁxed
Gaussian kernels as in part B of the ShanghaiTech dataset.

4.3. Comparing against Recent Techniques

In Tables 2, 3, 4, and 5, we compare our results to those
of the method that returns the best results for each one of
the 4 public datasets, as currently reported in the literature.

Model
Idrees et al. [14]
MCNN [42]
Encoder-Decoder [3]
CMTL [35]
Switch-CNN [31]
Resnet101 [12]
Densenet201 [13]
Idrees et al. [15]
OURS-CAN

M AE RM SE

315
277
270
252
228
190
163
132
107

508
426
478
514
445
277
226
191
183

Table 3: Comparative results on the UCF QNRF dataset.

Model
Idrees et al.[14]
Zhang et al. [41]
MCNN [42]
Switch-CNN [31]
CP-CNN [36]
ACSCP [32]
Liu et al. [24]
D-ConvNet [33]
IG-CNN [30]
ic-CNN[28]
CSRNet [19]
SANet [5]
OURS-CAN

M AE RM SE
419.5
467.0
377.6
318.1
295.8
291.0
337.6
288.4
291.4
260.9
266.1
258.4
212.2

541.6
498.5
509.1
439.2
320.9
404.6
434.3
404.7
349.4
365.5
397.5
334.9
243.7

Table 4: Comparative results on the UCF CC 50 dataset.

Scene1 Scene2 Scene3 Scene4 Scene5 Average

Model
9.8
Zhang et al. [41]
MCNN [42]
3.4
Switch-CNN [31] 4.4
CP-CNN [36]
2.9
2.8
ACSCP [32]
2.6
IG-CNN [30]
17.0
ic-CNN[28]
1.9
D-ConvNet [33]
2.9
CSRNet [19]
2.6
SANet [5]
2.0
DecideNet [21]
OURS-CAN
2.9
OURS-ECAN
2.4

14.1
20.6
15.7
14.7
14.05
16.1
12.3
12.1
11.5
13.2
13.14
12.0
9.4

14.3
12.9
10.0
10.5
9.6

10.15

9.2
20.7
8.6
9.0
8.9
10.0
8.8

22.2
13.0
11.0
10.4
8.1
20.2
8.1
8.3
16.6
13.3
17.4
7.9
11.2

3.7
8.1
5.9
5.8
2.9
7.6
4.7
2.6
3.4
3.0
4.75
4.3
4.0

12.9
11.6
9.4
8.9
7.5
11.3
10.3
9.1
8.6
8.2
9.23
7.4
7.2

Table 5: Comparative results in MAE terms on the World-
Expo’10 dataset.

They are those of [5], [15], [5], and [32], respectively. In
each case, we reprint the results as given in these papers and
add those of OURS-CAN, that is, our method as described
in Section 3.1. On the ﬁrst three datasets, we consistently
and clearly outperform all other methods. On the World-
Expo’10 dataset, we also outperform them on average, but

5104

Figure 5: Calibration in Venice and WorldExpo’10. (Top row) Images of Piazza San Marco taken from different viewpoints. (Middle
row) We used the regular ground patterns to accurately register the cameras in each frame. The red ellipse overlaid in red is the projection
of a 1m radius circle from the ground plane to the image plane. (Bottom row) The same 1m radius circle overlaid on three WorldExpo’10
images. As can be seen in the bottom right image, the ellipse surface corresponds to an area that could be ﬁlled by many more people
that could realistically ﬁt in a 1m radius circle. By contrast, the ellipse deformations are more consistent and accurate for Venice, which
denotes a better registration.

not in every scene. More speciﬁcally, in Scenes 2 and 4 that
are crowded, we do very well. By contrast, the crowds are
far less dense in Scenes 1 and 5. This makes context less
informative and our approach still performs honorably but
looses its edge compared to the others. Interestingly, as can
be seen in Table 5, in such uncrowded scenes, a detection-
based method such as DecideNet [21] becomes competitive
whereas it isn’t in the more crowded ones. In Fig. 6, we
use a Venice image to show how well our approach does
compared to the others in the crowded parts of the scene.

The ﬁrst three datasets do not have any associated cam-
era calibration data, whereas WorldExpo’10 comes with a
rough estimation of the image plane to ground plane ho-
mography and Venice with an accurate one. We therefore
used these homographies to run OURS-ECAN, our method
as described in Section 3.2. We report the results in Ta-
bles 5 and 6. Unsurprisingly, OURS-ECAN clearly further
improves on OURS-CAN when the calibration data is accu-
rate as for Venice and even when it is less so as for World-
Expo, but by a smaller margin.

Original image Region of interest

Ground truth

MCNN [42]

Switch-CNN [31]

CSRNet [19]

OURS-CAN

OURS-ECAN

Figure 6: Density estimation in Venice. Original image, ROI,
ground truth density map within the ROI, and density maps es-
timated both by the baselines and our method. Note how much
more similar the density map produced by OURS-ECAN is to the
ground truth than the others, especially in the upper corner of the
ROI, where people density is high.

4.4. Ablation Study

Finally, we perform an ablation study to conﬁrm the ben-
eﬁts of encoding multiple level contextual information and
of introducing contrast features.

5105

1

2

1

2

1

2

1

2

1

2

(a)

(b)

(c)

(d)

(e)

Figure 7: Image-plane density vs ground-plane density. (a) The two purple boxes highlight patches in which the crowd density per
square meter is similar in the top image. (b) Ground-truth image density obtained by averaging the head annotations in the image plane
as is done in all the approaches discussed in this paper, including ours. The bottom two patches are expanded versions of the same two
purple boxes. The density appears much larger in one than in the other due to perspective distortion that increases the image density further
away from the camera. (c) The density estimation returned by OURS-ECAN. (d) The ground-truth density normalized for image-scale
variations so that it can be interpreted as a density per square meter. (e) The OURS-ECAN density similarly normalized. Note that the
estimated densities in the two small windows now fall in the same range of values, which is correct.

Model
MCNN [42]
Switch-CNN [31]
CSRNet[19]
OURS-CAN
OURS-ECAN

M AE RM SE
145.4
52.8
35.8
23.5
20.5

147.3
59.5
50.0
38.9
29.9

Table 6: Comparative results on the Venice dataset.

Model
VGG-SIMPLE
VGG-CONCAT
VGG-NCONT

M AE RM SE
68.0
63.4
63.1

113.4
108.7
106.4

OURS-CAN

62.3

100.0

Table 7: Ablation study on the ShanghaiTech part A dataset.

Concatenating and Weighting VGG Features. We
compare our complete model without geometry, OURS-
CAN, against two simpliﬁed versions of it. The ﬁrst one,
VGG-SIMPLE, directly uses VGG-16 base features fv as
input to the decoder subnetwork. In other words, it does not
adapt for scale. The second one, VGG-CONCAT, concate-
nates all scale-aware features {sj}1≤j≤S to the base fea-
tures instead of computing their weighted linear combina-
tion, and then passes the resulting features to the decoder.

We compare these three methods on the ShanghaiTech
Part A, which has often been used for such ablation stud-
ies [36, 5, 19]. As can be seen in Table 7, concatenating
the VGG features as in VGG-CONCAT yields a signiﬁ-
cant boost, and weighing them as in OURS-CAN a further
one.

Contrast Features. To demonstrate the importance of
using contrast features to learn the network weights, we
compare OURS-CAN against VGG-NCONT that uses the
scale features sj instead of the contrast ones to learn the

weight maps. As can be seen in Table 7, this also results in
a substantial performance loss.

5. Conclusion and Future Perspectives

In this paper, we have shown that encoding multi-scale
context adaptively, along with providing an explicit model
of perspective distortion effects as input to a deep net, sub-
stantially increases crowd counting performance. In partic-
ular, it yields much better density estimates in high-density
regions.

This is of particular interest for crowd counting from mo-
bile cameras, such as those carried by drones.
In future
work, we will therefore augment the image data with the
information provided by the drone’s inertial measurement
unit to compute perspective distortions on the ﬂy and allow
monitoring from the moving drone.

We will also expand our approach to process consecutive
images simultaneously and enforce temporal consistency,
which among other things implies correcting ground-truth
densities to also account for perspective distortions and be
able to properly reason in the terms of ground-plane den-
sities instead of image-plane densities, which none of the
approaches discussed in this paper do. We did not do it ei-
ther so that our results could be properly compared to the
state of the art. However, as shown in Fig. 7, the price to
pay is that the estimated densities, because they are close
to this image-based ground truth, need to be corrected for
perspective distortion before they can be treated as ground-
plane densities. An obvious improvement would therefore
be to directly regress to ground densities.

Acknowledgments This work was supported in part by

the Swiss Federal Ofﬁce for Defense Procurement.

5106

References

[1] Carlos Arteta, Victor Lempitsky, J.Alison Noble, and An-
drew Zisserman. Interactive Object Counting. In European
Conference on Computer Vision, 2014. 2

[2] Carlos Arteta, Victor Lempitsky, and Andrew Zisserman.
Counting in the Wild. In European Conference on Computer
Vision, 2016. 2

[3] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
Segnet: A Deep Convolutional Encoder-Decoder Architec-
ture for Image Segmentation. arXiv Preprint, 2015. 6

[4] Gabriel J. Brostow and Roberto Cipolla. Unsupervised
Bayesian Detection of Independent Motion in Crowds.
In
Conference on Computer Vision and Pattern Recognition,
pages 594–601, 2006. 2

[5] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale
Aggregation Network for Accurate and Efﬁcient Crowd
Counting.
In European Conference on Computer Vision,
2018. 1, 2, 6, 8

[6] Antoni B. Chan, Zhang-Sheng John Liang, and Nuno Vas-
concelos. Privacy Preserving Crowd Monitoring: Counting
People Without People Models or Tracking. In Conference
on Computer Vision and Pattern Recognition, 2008. 2

[7] Antoni B. Chan and Nuno Vasconcelos. Bayesian Poisson
In International Confer-

Regression for Crowd Counting.
ence on Computer Vision, pages 545–551, 2009. 1, 2

[8] Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ram-
prasaath R. Selvaju, Dhruv Batra, and Devi Parikh. Count-
ing Everyday Objects in Everyday Scenes. In Conference on
Computer Vision and Pattern Recognition, 2017. 2

[9] Ke Chen, Chen Change Loy, Shaogang Gong, and Tao Xi-
In

ang. Feature Mining for Localised Crowd Counting.
British Machine Vision Conference, page 3, 2012. 2

[10] Luca Fiaschi, Rahul Nair, Ullrich Koethe, and Fred A. Ham-
precht. Learning to Count with Regression Forest and Struc-
tured Labels. In International Conference on Pattern Recog-
nition, pages 2685–2688, 2012. 2

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Spatial Pyramid Pooling in Deep Convolutional Networks
for Visual Recognition.
In European Conference on Com-
puter Vision, 2014. 2

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep Residual Learning for Image Recognition.
In Con-
ference on Computer Vision and Pattern Recognition, pages
770–778, 2016. 6

[13] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q. Weinberger. Densely Connected Convolutional Net-
works.
In Conference on Computer Vision and Pattern
Recognition, 2017. 6

[14] Haroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak
Shah. Multi-Source Multi-Scale Counting in Extremely
Dense Crowd Images.
In Conference on Computer Vision
and Pattern Recognition, pages 2547–2554, 2013. 2, 5, 6

[15] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong
Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak
Shah. Composition Loss for Counting, Density Map Estima-
tion and Localization in Dense Crowds. In European Con-
ference on Computer Vision, 2018. 1, 5, 6

[16] Di Kang and Antoni B. Chan. Crowd Counting by Adap-
tively Fusing Predictions from an Image Pyramid. In British
Machine Vision Conference, 2018. 2

[17] Di Kang, Debarun Dhar, and Antoni B. Chan. Incorporating
Side Information by Adaptive Convolution. In Advances in
Neural Information Processing Systems, 2017. 2

[18] Victor Lempitsky and Andrew Zisserman. Learning to Count
Objects in Images. In Advances in Neural Information Pro-
cessing Systems, 2010. 1, 2

[19] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet:
Dilated Convolutional Neural Networks for Understanding
the Highly Congested Scenes. In Conference on Computer
Vision and Pattern Recognition, 2018. 1, 4, 5, 6, 7, 8

[20] Zhe Lin and Larry S. Davis. Shape-Based Human Detection
and Segmentation via Hierarchical Part-Template Matching.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 32(4):604–618, 2010. 1

[21] Jiang Liu, Chenqiang Gao, Deyu Meng, and Alexander G.
Hauptmann1. Decidenet: Counting Varying Density Crowds
through Attention Guided Detection and Density Estimation.
In Conference on Computer Vision and Pattern Recognition,
2018. 4, 6, 7

[22] Linbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, and
Liang Lin. Crowd Counting Using Deep Recurrent Spatial-
Aware Network. In International Joint Conference on Artiﬁ-
cial Intelligence, 2018. 1

[23] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C.
Berg. SSD: Single Shot Multibox Detector.
In European
Conference on Computer Vision, 2016. 2

[24] Xialei Liu, Joost van de Weijer, and Andrew D. Bagdanov.
Leveraging Unlabeled Data for Crowd Counting by Learn-
ing to Rank. In Conference on Computer Vision and Pattern
Recognition, 2018. 1, 6

[25] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
Convolutional Networks for Semantic Segmentation.
In
Conference on Computer Vision and Pattern Recognition,
2015. 2

[26] Daniel Onoro-Rubio and Roberto J. L´opez-Sastre. Towards
Perspective-Free Object Counting with Deep Learning.
In
European Conference on Computer Vision, pages 615–629,
2016. 1, 2, 5

[27] Vincent Rabaud and Serge Belongie. Counting Crowded
Moving Objects. In Conference on Computer Vision and Pat-
tern Recognition, pages 705–711, 2006. 2
[28] Viresh Ranjan, Hieu Le, and Minh Hoai.

Iterative Crowd
In European Conference on Computer Vision,

Counting.
2018. 1, 6

[29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards Real-Time Object Detection with
Region Proposal Networks. In Advances in Neural Informa-
tion Processing Systems, 2015. 2

[30] Deepak Babu Sam, Neeraj N. Sajjan, R. Venkatesh Babu,
and Mukundhan Srinivasan. Divide and Grow: Capturing
Huge Diversity in Crowd Images with Incrementally Grow-
ing CNN.
In Conference on Computer Vision and Pattern
Recognition, 2018. 1, 2, 4, 6

5107

[31] Deepak Babu Sam, Shiv Surya, and R. Venkatesh Babu.
Switching Convolutional Neural Network for Crowd Count-
ing. In Conference on Computer Vision and Pattern Recog-
nition, page 6, 2017. 1, 2, 4, 5, 6, 7, 8

[32] Zan Shen, Yi Xu, Bingbing Ni, Minsi Wang, Jianguo Hu,
and Xiaokang Yang. Crowd Counting via Adversarial Cross-
Scale Consistency Pursuit. In Conference on Computer Vi-
sion and Pattern Recognition, 2018. 1, 2, 5, 6

[33] Zenglin Shi, Le Zhang, Yun Liu, and Xiaofeng Cao. Crowd
Counting with Deep Negative Correlation Learning. In Con-
ference on Computer Vision and Pattern Recognition, 2018.
1, 5, 6

[34] Karen Simonyan and Andrew Zisserman. Very Deep Convo-
lutional Networks for Large-Scale Image Recognition. In In-
ternational Conference on Learning Representations, 2015.
2

[35] Vishwanath A. Sindagi and Vishal M. Patel. CNN-based
Cascaded Multi-task Learning of High-level Prior and Den-
sity Estimation for Crowd Counting. In International Con-
ference on Advanced Video and Signal Based Surveillance,
2017. 6

[36] Vishwanath A. Sindagi and Vishal M. Patel. Generating
High-Quality Crowd Density Maps Using Contextual Pyra-
mid CNNs. In International Conference on Computer Vision,
pages 1879–1888, 2017. 1, 2, 5, 6, 8

[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going Deeper with
Convolutions. In Conference on Computer Vision and Pat-
tern Recognition, pages 1–9, June 2015. 3

[38] Xin Wang, Bin Wang, and Liming Zhang. Airport Detection
in Remote Sensing Images Based on Visual Attention.
In
International Conference on Neural Information Processing,
2011. 1

[39] Bo Wu and Ram Nevatia. Detection of Multiple, Partially
Occluded Humans in a Single Image by Bayesian Combina-
tion of Edgelet Part Detectors. In International Conference
on Computer Vision, 2005. 1

[40] Feng Xiong, Xinjian Shi, and Dit-Yan Yeung. Spatiotempo-
ral Modeling for Crowd Counting in Videos. In International
Conference on Computer Vision, pages 5161–5169, 2017. 1,
5

[41] Cong Zhang, Hongsheng Li, Xiaogang Wang, and Xiaokang
Yang. Cross-Scene Crowd Counting via Deep Convolutional
Neural Networks.
In Conference on Computer Vision and
Pattern Recognition, pages 833–841, 2015. 1, 2, 5, 6

[42] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao,
and Yi Ma.
Single-Image Crowd Counting via Multi-
Column Convolutional Neural Network. In Conference on
Computer Vision and Pattern Recognition, pages 589–597,
2016. 1, 2, 4, 5, 6, 7, 8

[43] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid Scene Parsing Network. In
Conference on Computer Vision and Pattern Recognition,
2017. 3

5108

