Neural Rerendering in the Wild

Moustafa Meshry∗1, Dan B Goldman2, Sameh Khamis2, Hugues Hoppe2, Rohit Pandey2,

Noah Snavely2, Ricardo Martin-Brualla2

1University of Maryland, 2Google Inc.

Abstract

We explore total scene capture — recording, modeling,
and rerendering a scene under varying appearance such
as season and time of day. Starting from internet photos
of a tourist landmark, we apply traditional 3D reconstruc-
tion to register the photos and approximate the scene as a
point cloud. For each photo, we render the scene points
into a deep framebuffer, and train a neural network to learn
the mapping of these initial renderings to the actual pho-
tos. This rerendering network also takes as input a latent
appearance vector and a semantic mask indicating the lo-
cation of transient objects like pedestrians. The model is
evaluated on several datasets of publicly available images
spanning a broad range of illumination conditions. We cre-
ate short videos demonstrating realistic manipulation of the
image viewpoint, appearance, and semantic labeling. We
also compare results with prior work on scene reconstruc-
tion from internet photos.

1. Introduction

Imagine spending a day sightseeing in Rome in a fully
realistic interactive experience without ever stepping on a
plane. One could visit the Pantheon in the morning, enjoy
the sunset overlooking the Colosseum, and ﬁght through the
crowds to admire the Trevi Fountain at night time. Realiz-
ing this goal involves capturing the complete appearance
space of a scene, i.e., recording a scene under all possible
lighting conditions and transient states in which the scene
might be observed—be it crowded, rainy, snowy, sunrise,
spotlit, etc.—and then being able to summon up any view-
point of the scene under any such condition. We call this
ambitious vision total scene capture. It is extremely chal-
lenging due to the sheer diversity of appearance—scenes
can look dramatically different under night illumination,
during special events, or in extreme weather.

∗Work performed during an internship at Google.

(a) Input deep buffer

(b) Output renderings

Figure 1: Our neural rerendering technique uses a large-scale in-
ternet photo collection to reconstruct a proxy 3D model and trains
a neural rerendering network that takes as input a deferred-shading
deep buffer (consisting of depth, color and semantic labeling) gen-
erated from the proxy 3D model (left), and outputs realistic ren-
derings of the scene under multiple appearances (right).

In this paper, we focus on capturing tourist landmarks
around the world using publicly available community pho-
tos as the sole input, i.e., photos in the wild. Recent ad-
vances in 3D reconstruction can generate impressive 3D
models from such photo collections [1, 39, 41], but the ren-
derings produced from the resulting point clouds or meshes
lack the realism and diversity of real-world images. Alter-
natively, one could use webcam footage to record a scene
at regular intervals but without viewpoint diversity, or use
specialized acquisition (e.g., Google Street View, aerial, or
satellite images) to snapshot the environment over a short
time window but without appearance diversity. In contrast,
community photos offer an abundant (but challenging) sam-
pling of appearances of a scene over many years.

Our approach to total scene capture has two main com-
ponents: (1) creating a factored representation of the input
images, which separates viewpoint, appearance conditions,
and transient objects such as pedestrians, and (2) rendering
realistic images from this factored representation. Unlike
recent approaches that extract implicit disentangled repre-
sentations of viewpoint and content [31, 34, 43], we employ
state-of-the-art reconstruction methods to create an explicit
intermediate 3D representation, in the form of a dense but
noisy point cloud, and use this 3D representation as a “scaf-
folding” to predict images.

16878

An explicit 3D representation lets us cast the rendering
problem as a multimodal image translation [15, 25, 53]. The
input is a deferred-shading framebuffer [35] in which each
rendered pixel stores albedo, depth, and other attributes, and
the outputs are realistic views under different appearances.
We train the model by generating paired datasets, using the
recovered viewpoint parameters of each input image to ren-
der a deep buffer of the scene from the same view, i.e., with
pixelwise alignment. Our model effectively learns to take an
approximate initial scene rendering and rerender a realistic
image. This is similar to recent neural rerendering frame-
works [20, 28, 44] but using uncontrolled internet images
rather than carefully captured footage.

We explore a novel strategy to train the multimodal im-
age translation model. Rather than jointly estimating an em-
bedding space for the appearance together with the render-
ing network [15, 25, 53], our system performs staged train-
ing of both. First, an appearance encoding network is pre-
trained using a proxy style-based loss [9], an efﬁcient way
to capture the style of an image. Then, the rerendering net-
work is trained with ﬁxed appearance embeddings from the
pretrained encoder. Finally, both the appearance encoding
and rerendering networks are jointly ﬁnetuned. This sim-
ple yet effective strategy lets us train simpler networks on
large datasets. We demonstrate experimentally how a model
trained in this fashion better captures scene appearance.

Our system is a ﬁrst step towards addressing total scene
capture and focuses primarily on the static parts of scenes.
Transient objects (e.g., pedestrians and cars) are handled by
conditioning the rerendering network on the expected se-
mantic labeling of the output image, so that the network can
learn to ignore these objects rather than trying to halluci-
nate their locations. This semantic labeling is also effective
at discarding small or thin scene features (e.g., lampposts)
whose geometry cannot be robustly reconstructed, yet are
easily identiﬁed using image segmentation methods. Con-
ditioning our network on a semantic mask also enables the
rendering of scenes free of people if desired. Code will be
available at https://bit.ly/2UzYlWj.
In summary, our contributions include:
• A ﬁrst step towards total scene capture, i.e., recording
and rerendering a scene under any appearance from in-
the-wild photo collections.

• A factorization of input images into viewpoint, appear-
ance, and semantic labeling, conditioned on an approx-
imate 3D scene proxy, from which we can rerender re-
alistic views under varying appearance.

• A more effective method to learn the appearance latent
space by pretraining the appearance embedding net-
work using a proxy loss.

• Compelling results including view and appearance in-
terpolation on ﬁve large datasets, and direct compar-
isons to previous methods [39].

2. Related work

Scene reconstruction Traditional methods for scene re-
construction ﬁrst generate a sparse reconstruction using
large-scale structure-from-motion [1], then perform Multi-
View Stereo (MVS) [7, 38] or variational optimization [13]
to reconstruct dense scene models. However, most such
techniques assume a single appearance, or else simply re-
cover an average appearance of the scene. We build upon
these techniques, using dense point clouds recovered from
MVS as proxy geometry for neural rerendering.

In image-based rendering [4, 10], input images are used
to generate new viewpoints by warping input pixels into the
outputs using proxy geometry. Recently, Hedman et al. [12]
introduce a neural network to compute blending weights for
view-dependent texture mapping that reduces artifacts in
poorly reconstructed regions. However, image-based ren-
dering generally assumes the captured scene has static ap-
pearance, so it is not well-suited to our problem setup in
which the appearance varies across images.

Neural scene rendering [6] applies deep neural networks
to learn a latent scene representation that allows generation
of novel views, but is limited to simple synthetic geometry.

Appearance modeling A given scene can have dramati-
cally different appearances at different times of day, in dif-
ferent weather conditions, and can also change over the
years. Garg et al. [8] observe that for a given viewpoint,
the dimensionality of scene appearance as captured by in-
ternet photos is relatively low, with the exception of out-
liers like transient objects. One can recover illumination
models for a photo collection by estimating albedo using
cloudy images [39], retrieving the sun’s location through
timestamps and geolocation [11], estimating coherent albe-
dos across the collection [22], or assuming a ﬁxed view-
point [42]. However, these methods assume simple light-
ing models that do not apply to nighttime scene appearance.
Radenovic et al. [33] recover independent day and night re-
constructions, but do not enable smooth appearance inter-
polations between the two.

Laffont et al. [23] assign transient attributes like “fall” or
“sunny” to each image, and learn a database of patches that
allows for editing such attributes. Other works require di-
rect supervision from lighting models estimated using 360-
degree images [14], or ground truth object geometry [48].
In contrast, we use a data-driven implicit representation of
appearance that is learned from the input image distribution
and does not require direct supervision.

Deep image synthesis The seminal work of pix2pix [16]
trains a deep neural network to translate an image from one
domain, such as a semantic labeling, into another domain,
such as a realistic image, using paired training data. Image-
to-image (I2I) translation has since been applied to many
tasks [5, 24, 32, 49, 47, 50]. Several works propose im-

6879

provements to stabilize training and allow for high-quality
image synthesis [18, 46, 47]. Others extend the I2I frame-
work to unpaired settings where images from two domains
are not in correspondence [21, 26, 52], multimodal outputs
where an input image can map to multiple images [46, 53],
or unpaired datasets with multimodal outputs where an im-
age in one domain is converted to another domain while
preserving the content [2, 15, 25].

Image translation techniques can be used to rerender
scenes in a more realistic domain, to enable facial expres-
sion synthesis [20], to ﬁx artifacts in captured 3D perfor-
mances [28], or to add viewpoint-dependent effects [44].
In our paper, we demonstrate an approach for training a
neural rerendering framework in the wild, i.e., with uncon-
trolled data instead of captures under constant lighting con-
ditions. We cast this as a multimodal image synthesis prob-
lem, where a given viewpoint can be rendered under multi-
ple appearances using a latent appearance vector, and with
editable semantics by conditioning the output on the desired
semantic labeling of the output.

3. Total scene capture

We deﬁne the problem of total scene capture as creat-
ing a generative model for all images of a given scene. We
would like such a model to:

– encode the 3D structure of the scene, enabling render-

ing from an arbitrary viewpoint,

– capture all possible appearances of the scene, e.g., all
lighting and weather conditions, and allow rendering
the scene under any of them, and

– understand the location and appearance of transient ob-
jects in the scene, e.g., pedestrians and cars, and allow
for reproducing or omitting them.

Although these goals are ambitious, we show that one can
create such a generative model given sufﬁcient images of a
scene, such as those obtained for popular tourist landmarks.
We ﬁrst describe a neural rerendering framework that we
adapt from previous work in controlled capture settings [28]
to the more challenging setting of unstructured photo col-
lections (Section 3.1). We extend this model to enable ap-
pearance capture and multimodal generation of renderings
under different appearances (Section 3.2). We further ex-
tend the model to handle transient objects in the training
data by conditioning its inputs on a semantic labeling of the
ground truth images (Section 3.3).

3.1. Neural rerendering framework

We adapt recent neural rerendering frameworks [20, 28]
to work with unstructured photo collections. Given a large
internet photo collection {Ii} of a scene, we ﬁrst generate
a proxy 3D reconstruction using COLMAP [36, 37, 38],
which applies Structure-from-Motion (SfM) and Multi-
View Stereo (MVS) to create a dense colored point cloud.

Figure 2: Output frames of a standard image translation net-
work [16] trained for neural rerendering in a small dataset of 250
photos of San Marco. The network overﬁts the dataset and learns
to hallucinate lampposts close to their approximate location in the
scene (green), and virtual tourists (yellow), as well as memorizing
a per-viewpoint appearance matching the speciﬁc input photos.

An alternative to a point cloud is to generate a textured
mesh [19, 45]. Although meshes generate more complete
renderings, they tend to also contain pieces of misregistered
ﬂoating geometry which can occlude large regions of the
scene [39]. As we show later, our neural rerendering frame-
work can produce highly realistic images given only point-
based renderings as input.

Given the proxy 3D reconstruction, we generate an
aligned dataset of rendered images and real images by ren-
dering the 3D point cloud from the viewpoint vi of each in-
put image Ii, where vi consists of camera intrinsics and ex-
trinsics recovered via SfM. We generate a deferred-shading
deep buffer Bi for each image [35], which may contain per-
pixel albedo, normal, depth and any other derivative infor-
mation. In our case, we only use albedo and depth and ren-
der the point cloud by using point splatting with a z-buffer
with a radius of 1 pixel.

However, the image-to-image translation paradigm used
in [20, 28] is not appropriate for our use case, as it assumes
a one-to-one mapping between inputs and outputs. A scene
observed from a particular viewpoint can look very different
depending on weather, lighting conditions, color balance,
post processing ﬁlters, etc. In addition, a one-to-one map-
ping fails to explain transient objects in the scene, such as
pedestrians or cars, whose location and individual appear-
ance is impossible to predict from the static scene geometry
alone. Interestingly, if one trains a sufﬁciently large neural
network on this simple task on a dataset, the network learns
to (1) associate viewpoint with appearance via memoriza-
tion and (2) hallucinate the location of transient objects, as
shown in Figure 2.

3.2. Appearance modeling

To capture the one-to-many relationship between input
viewpoints (represented by their deep buffers Bi) and out-
put images Ii under different appearances, we cast the
rerendering task as multimodal image translation [53]. In
such a formulation, the goal is to learn a latent appearance
vector za
that captures variations in the output domain Ii
i
that cannot be inferred from the input domain Bi. We com-
pute the latent appearance vector as za
i = Ea(Ii, Bi) where

6880

is computed through a cross-cycle with a second image
(I2, B2), i.e. ˆza
1 )), B2). We also apply
a GAN loss on the intermediate appearance transfer output
R(B2, za

1 = Ea(R(B2, za

1 ) as in [15, 25].

Staged appearance training The key to our staged train-
ing approach is the appearance pretraining stage, where
we pretrain the appearance encoder Ea independently on
a proxy task. We then train the rendering network R while
ﬁxing the weights of Ea, allowing R to ﬁnd the correlations
between output images and the embedding produced by the
proxy task. Finally, we ﬁne-tune both Ea and R jointly.

This staged approach simpliﬁes and stabilizes the train-
ing of R, enabling training of a simpler network with fewer
regularization terms. In particular, we remove the cycle and
cross-cycle consistency losses, the latent vector reconstruc-
tion loss, and the KL-divergence loss, leaving only a direct
reconstruction loss and a GAN loss. We show experimen-
tally in Section 4 that this approach results in better appear-
ance capture and rerenderings than the baseline model.

Appearance pretraining To pretrain the appearance en-
coder Ea, we choose a proxy task that optimizes an embed-
ding of the input images into the appearance latent space
using a suitable distance metric between input images. This
training encourages embeddings such that if two images
are close under the distance metric, then their appearance
embeddings should also be close in the appearance latent
space. Ideally the distance metric we choose should ignore
the content or viewpoint of Ii and Bi, as our goal is to en-
code a latent space that is independent of viewpoint. Ex-
perimentally we ﬁnd that the style loss employed in neural
style-transfer work [9] has such a property; it largely ig-
nores content and focuses on more abstract properties.

To train the embedding, we use a triplet loss, where for
each image Ii, we ﬁnd the set of k closest and furthest
neighbor images given by the style loss, from which we
can sample a positive sample Ip and negative sample In,
respectively. The loss is then:
max(cid:16)kgj
L(Ii, Ip, In) = X

nk2 + α, 0(cid:17)

pk2 − kgj

i −gj

i −gj

j

where gj
i is the Gram matrix of activations at the jth layer of
a VGG network of image Ii, and α is a separation margin.

3.3. Semantic conditioning

To account for transient objects in the scene, we condi-
tion the rerendering network on a semantic labeling Si of
image Ii that depicts the location of transient objects such
as pedestrians. Speciﬁcally, we concatenate the semantic
labeling Si to the deep buffer Bi wherever the deep buffer
was previously used. This discourages the network from
encoding variations caused by the location of transient ob-
jects in the appearance vector, or associating such transient
objects with speciﬁc viewpoints, as shown in Figure 2.

6881

Figure 3: An aligned dataset is created using Structure from Mo-
tion (SfM) and Multi-View Stereo (MVS). Our staged approach
pre-trains the appearance encoder E a using a triplet loss (left).
Then the rerenderer R is trained using standard reconstruction and
GAN losses (right), and ﬁnally ﬁne-tuned together with E a. Photo
Credits Rafael Jimenez (Creative Commons).

Ea is an appearance encoder that takes as input both the out-
put image Ii and the deep buffer Bi. We argue that having
the appearance encoder Ea observe the input Bi allows it to
learn more complex appearance models by correlating the
lighting in Ii with scene geometry in Bi. Finally, a reren-
dering network R generates a scene rendering conditioned
on both viewpoint Bi and the latent appearance vector za.
Figure 3 shows an overview of the overall process.

To train the appearance encoder Ea and rendering net-
work R, we ﬁrst adopted elements from recent methods in
multimodal synthesis [15, 25, 53] to ﬁnd a combination that
is most effective in our scenario. However, this combination
still has shortcomings as it is unable to model infrequent
appearances well. For instance, it does not reliably capture
night appearances for scenes in our datasets. We hypoth-
esize that the appearance encoder (which is jointly trained
with the rendering network) is not expressive enough to cap-
ture the large variability in the data.

To improve the model expressiveness, our approach is
to stabilize the joint training of R and Ea by pretraining
the appearance network Ea independently on a proxy task.
We then employ a staged training approach in which the
rendering network R is ﬁrst trained using ﬁxed appearance
embeddings, and ﬁnally we jointly ﬁne-tune both networks.
This staged training regime allows for a simpler model that
captures more complex appearances.

We present our baseline approach, which adapts state-
of-the-art multimodal synthesis techniques, and then our
staged training strategy, which pretrains the appearance en-
coder on a proxy task.

Baseline Our baseline uses BicycleGAN [53] with two
main adaptations. First, our appearance encoder also takes
as input the buffer Bi, as described above. Second, we
add a cross-cycle consistency loss similar to [15, 25] to en-
courage appearance transfer across viewpoints. Let za
1 =
Ea(I1, B1) be the captured appearance of an input im-
age I1. We apply a reconstruction loss between image I1
and cross-cycle reconstruction ˆI1 = R(B1, ˆza
1 ), where ˆza

1

Input

Segmentation

I2I

+Sem

+Sem+BaseApp +Sem+StagedApp Ground Truth

Figure 4: Example visual results of our ablative study in Table 1. From left to right, input color render, segmentation mask from the
corresponding ground truth images, result using an image-to-image baseline (I2I), with semantic conditioning (+Sem), and with semantic
conditioning and a baseline appearance modeling based on [53] (+Sem+BaseApp), with semantic conditioning and staged appearance
training (+Sem+StagedApp). Photo Credits: Flickr users Gary Campbell-Hall, Steve Collis, and Tahbepet (Creative Commons).

A separate beneﬁt of semantic labeling is that it allows
the rerendering network to reason about static objects in the
scene not captured in the 3D reconstruction, such as lamp-
posts in San Marco Square. This prevents the network from
haphazardly introducing such objects, and instead lets them
appear where they are detected in the semantic labeling,
which is a signiﬁcantly simpler task. In addition, by adding
the segmentation labeling to the deep buffer, we allow the
appearance encoder to reason about semantic categories like
sky or ground when computing the appearance latent vector.

We compute “ground truth” semantic segmentations
images Ii using DeepLab [3] trained on
on the input
ADE20K [51]. ADE20K contains 150 classes, which we
map to a 3-channel color image. We ﬁnd that the quality of
the semantic labeling is poor on the landmarks themselves,
as they contain unique buildings and features, but is reason-
able on transient objects.

Using semantic conditioning, the rerendering network
takes as input a semantic labeling of the scene.
In order
to rerender virtual camera paths, we need to synthesize se-
mantic labelings for each frame in the virtual camera path.
To do so, we train a separate semantic labeling network that
takes as input the deep buffer Bi, instead of the output im-
age Ii, and estimates a “plausible” semantic labeling ˆSi for
that viewpoint given the rendered deep buffer Bi. For sim-
plicity, we train a network with the same architecture as the
rendering network (minus the injected appearance vector)
on samples (Bi, Si) from the aligned dataset, and we mod-
ify the semantic labelings of the ground truth images Si and
mask out the loss on pixels labeled as transient as deﬁned
by a curated list of transient object categories in ADE20K.

4. Evaluation

Here we provide an extensive evaluation of our system.
Please also refer to the supplementary video to best appreci-
ate the quality of the results, available in the project website:
https://bit.ly/2UzYlWj.

Implementation details Our rerendering network is a
symmetric encoder-decoder with skip connections, where
the generator is adopted from [18] without using progres-
sive growing. We use a multiscale-patchGAN discrimina-
tor [46] with 3 scales and employ a LSGAN [27] loss. As
a reconstruction loss, we use the perceptual loss [17] evalu-
ated at convi,2 for i ∈ [1, 5] of VGG [40]. The appearance
encoder architecture is adopted from [25], and we use a la-
tent appearance vector za ∈ R8. We train on 8 GPUs for
∼ 40 epochs using 256 × 256 crops of input images, but we
show compelling results on up to 600×900 at test time. The
generator runtime for the staged training network is 330 ms
for a 512x512 frame on a TitanV without fp16 optimiza-
tions. Architecture and training details can be found in the
supplementary material.

Datasets We evaluate our method on ﬁve datasets recon-
structed with COLMAP [36] from public images, summa-
rized in Table 1. A separate model is trained for each
dataset. We create aligned datasets by rendering the recon-
structed point clouds with a minimum dimension of 600
pixels, and throw away sparse renderings (>85% empty
pixels), and small images (<450 pixels across). We ran-
domly select a validation set of 100 images per dataset.

Ablative study We perform an ablative study of our sys-
tem and compare the proposed methods in Figure 4. The

6882

Dataset

#Images #Points VGG

I2I
L1

+Sem

+Sem+BaseApp

+Sem+StagedApp

PSNR VGG

L1

PSNR VGG

L1

PSNR VGG

L1

PSNR

Sacre Coeur
Trevi
Pantheon
Dubrovnik
San Marco

1165
3006
4972
5891
7711

33M 70.78 39.98
35M 86.52 42.95
9M
68.28 39.77
33M 78.42 40.60
80.18 44.04
7M

14.36
14.14
14.50
14.21
13.97

66.17 34.78
81.82 36.46
67.47 36.27
78.58 39.88
78.36 39.34

15.62
15.57
15.13
14.51
14.58

60.06 21.58
79.10 28.12
64.06 28.85
76.61 34.57
70.35 26.24

18.98
17.37
16.76
15.38
17.87

61.23 25.22
75.55 25.00
60.66 23.77
71.65 27.48
68.96 23.11

17.81
18.19
17.95
17.01
18.32

Table 1: Dataset statistics (number of registered images and size of reconstructed point cloud) and average error on the validation set using
VGG/perceptual loss (lower is better), L1 loss (lower is better), and PSNR (higher is better), for four methods: an image-to-image baseline
(I2I), with semantic conditioning (+Sem), with semantic conditioning and a baseline appearance modeling based on [53] (+Sem+BaseApp),
and with semantic conditioning and staged appearance training (+Sem+StagedApp).

.

e
n
i
l
e
s
a
B

d
e
g
a
t

S

e
n
i
l
e
s
a
B

d
e
g
a
t

S

Figure 5: Examples of appearance interpolation for a ﬁxed viewpoint. The left- and rightmost appearances are captured from real images,
and the intermediate frames are generated by linearly interpolating the appearances in the latent space. Notice how the baseline method is
unable to capture complex scenes, like the sunset and night scene, and its interpolations are rather linear, as can be appreciated in the street
lamps (top). The staged training method performs better, but generates twilight artifacts in the sky when interpolating between day and
night appearances (bottom).

results of the image-to-image translation baseline method
contain additional blurry artifacts near the ground because
it hallucinates the locations of pedestrians. Using semantic
conditioning, the results improve slightly in those regions.
Finally, encoding the appearance of the input photo allows
the network to match the appearance. The staged training
recovers a closer appearance in San Marco and Pantheon
datasets (two bottom rows). However, in Sacre Coeur (top
row), the smallest dataset, the baseline appearance model
is able to better capture the general appearance of the im-
age, although the staged training model reproduces the di-
rectionality of the lighting with more ﬁdelity.

Reconstruction metrics We report image reconstruction
errors in the validation set using several metrics: perceptual
loss [17], L1 loss, and PSNR. We use the ground truth se-
mantic mask from the source image, and we extract the ap-
pearance latent vector using the appearance encoder. Staged
training of the appearance fares better than the baseline for
all but the smallest dataset (Sacre Coeur), where the staged
training overﬁts to the training data and is unable to gener-
alize. The baseline method assumes a prior distribution of

the latent space and is less prone to overﬁtting at the cost of
poorer modeling of appearance.

Appearance interpolation The rerendering network al-
lows for interpolating the appearance of two images by in-
terpolating their latent appearance vectors. Figure 5 depicts
two examples, showing that the staged training approach
is able to generate more complex appearance changes, al-
though its generated interpolations lack realism when tran-
sitioning between day and night. In the following, we only
show results for the staged training model.

Appearance transfer Figure 6 demonstrates how our full
model can transfer the appearance of a given photo to oth-
ers. It shows realistic renderings of the Trevi fountain from
ﬁve different viewpoints under four different appearances
obtained from other photos. Note the sunny highlights
and the spotlit night illumination appearance of the stat-
ues. However, these details can ﬂicker when synthesizing
a smooth camera path or smoothly interpolating the appear-
ance in the latent space, as seen in the supplementary video.

6883

Figure 6: We capture the appearance of the original images in the left column, and rerender several viewpoints under them. The last column
is a detail of the previous one. The top row shows the renderings part of the input to the rerenderer, that exhibit artifacts like incomplete
features in the statue, and an inconsistent mix of day and night appearances. Note the hallucinated twilight scene in the sky using the last
appearance. Image credits: Flickr users William Warby, Neil Rickards, Rafael Jimenez, acme401 (Creative Commons).

Photo

Frame 0

Frame 20

Frame 40

Frame 60

Frame 80

Frame 100

Photo

Figure 7: Frames from a synthesized camera path that smoothly transitions from the photo on the left to the photo on the right by smoothly
interpolating both viewpoint and the latent appearance vectors. Please see the supplementary video. Photo Credits: Allie Caulﬁeld, Tahbepet,
Till Westermayer, Elliott Brown (Creative Commons).

Image interpolation Figure 7 shows sets of two images
and frames of smooth image interpolations between them,
where both viewpoint and appearance transition smoothly
between them. Note how the illumination of the scene can
transition smoothly from night to day. The quality of the
results is best appreciated in the supplementary video.

Semantic consistency Figure 8 shows the output of the
staged training model with ground truth and predicted seg-
mentation masks. Using the predicted masks, the network
produces similar results on the building and renders a scene
free of people. Note however how the network depicts
pedestrians as black, ghostly ﬁgures when they appear in
the segmentation mask.

(a) w/ GT segmentation

(b) w/ predicted segmentations

Figure 8: Example semantic labelings and output renders when
using the “ground truth” segmentation mask computed from the
corresponding real image (from the validation set) and the pre-
dicted one from the associated deep buffer. Note the artifacts on
the bottom right where the ground is misclassiﬁed as building.

6884

(a) Neural artifacts

(b) Sparse reconstructions

(c) Segmentations artifacts

Figure 10: Limitations of the current system.

Segmentation Our model relies heavily on the segmenta-
tion mask to synthesize parts of the image not modeled in
the proxy geometry, like the ground or sky regions. Thus
our results are very sensitive to errors in the segmentation
network, like in the sky region in Figure 10c or an appear-
ing “ghost pole” artifact in San Marco (frame 40 of bottom
row in Figure 7, best seen in video). Jointly training the
neural rerenderer together with the segmentation network
could reduce such artifacts.

Neural artifacts Neural networks are known to produce
screendoor patterns [30] and other intriguing artifacts [29].
We observe such artifacts in repeated structures, like the
patterns on the ﬂoor of San Marco, which in our renderings
are misaligned as if hand-painted. Similarly, the inscription
above the Trevi fountain is reproduced with a distorted font
(see Figure 10a).

Incomplete reconstructions Sometimes an image con-
tains partially reconstructed parts of the 3D model, creat-
ing large holes in the rendered Bi. This forces the network
to hallucinate the incomplete regions, generally leading to
blurry outputs (see Figure 10b).

Temporal artifacts When smoothly varying the view-
point, sometimes the appearance of the scene can ﬂicker
considerably, especially under complex appearance, such
as when the sun hits the Trevi Fountain, creating complex
highlights and cast shadows. Please see the supplementary
video for an example.

In summary, we present a ﬁrst attempt at solving the total
scene capture problem. Using unstructured internet photos,
we can train a neural rerendering network that is able to
produce highly realistic scenes under different illumination
conditions. We propose a novel staged training approach
that better captures the appearance of the scene as seen in
internet photos. Finally, we evaluate our system on ﬁve
challenging datasets and against state-of-the-art 3D recon-
struction methods.

6885

(a) [39]

(b) ours

(c) original images

Figure 9: Comparison of [39] and our approach. Rows 1 & 3:
original photos. Rows 2 & 4: detailed crops. Image credits: Graeme
Churchard, Sarah-Rose (Creative Commons).

Comparison to 3D reconstruction methods We evalu-
ated our technique against the one of Shan et al. [39] on the
Colosseum, which contains 3K images, 10M color vertices
and 48M triangles and was generated from Flickr, Google
Street View, and aerial images. Their 3D representation
is a dense vertex-colored mesh, where the albedo and ver-
tex normals are jointly recovered together with a simple 8-
dimensional lighting model (diffuse, plus directional light-
ing) for each image in the photo collection.

Figure 9 compares both methods and the original ground
truth image. Their method suffers from ﬂoating white ge-
ometry on the top edge of the Colosseum, and has less de-
tail, although it recovers the lighting better than our method,
thanks to its explicit lighting reasoning. Note that both mod-
els are accessing the test image to compute lighting coefﬁ-
cients and appearance latent vectors, with dimension 8 in
both cases, and that we use the predicted segmentation la-
belings from Bi.

We ran a randomized user study on 20 random sets of
output images that do not contain close-ups of people or
cars, and were not in our training set. For each viewpoint,
200 participants chose “which image looks most real?” be-
tween an output of their system and ours (without seeing
the original). Respondents preferred images generated by
our system a 69.9% of the time, with our technique being
preferred on all but one of the images. We show the 20 ran-
dom sets of the user study in the supplementary material.

5. Discussion

Our system’s limitations are signiﬁcantly different from

those of traditional 3D reconstruction pipelines:

References

[1] S. Agarwal, N. Snavely,

R. Szeliski. Building Rome in a day.
2

I. Simon, S. M. Seitz, and
In ICCV, 2009. 1,

[2] A. Almahairi, S. Rajeshwar, A. Sordoni, P. Bachman, and
A. Courville. Augmented CycleGAN: Learning many-to-
many mappings from unpaired data. In ICML, 2018. 3

[3] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. DeepLab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE Trans. PAMI, 2018. 5

[4] P. E. Debevec, C. J. Taylor, and J. Malik. Modeling and ren-
dering architecture from photographs: A hybrid geometry-
and image-based approach. In Proc. SIGGRAPH, 1996. 2

[5] H. Dong, S. Yu, C. Wu, and Y. Guo. Semantic image synthe-

sis via adversarial learning. In ICCV, 2017. 2

[6] S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Mor-
cos, M. Garnelo, A. Ruderman, A. A. Rusu, I. Danihelka,
K. Gregor, et al. Neural scene representation and rendering.
Science, 2018. 2

[7] Y. Furukawa and J. Ponce. Accurate, dense, and robust mul-

tiview stereopsis. IEEE Trans. PAMI, 2010. 2

[8] R. Garg, H. Du, S. M. Seitz, and N. Snavely. The dimension-

ality of scene appearance. In ICCV, 2009. 2

[9] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer

using convolutional neural networks. In CVPR, 2016. 2, 4

[10] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen.

The lumigraph. In Proc. SIGGRAPH, 1996. 2

[11] D. Hauagge, S. Wehrwein, P. Upchurch, K. Bala, and
N. Snavely. Reasoning about photo collections using models
of outdoor illumination. In BMVC, 2014. 2

[12] P. Hedman, J. Philip, T. Price, J.-M. Frahm, G. Drettakis, and
G. Brostow. Deep blending for free-viewpoint image-based
rendering. In Proc. SIGGRAPH, 2018. 2

[20] H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Niessner,
P. P´erez, C. Richardt, M. Zollh¨ofer, and C. Theobalt. Deep
video portraits. In Proc. SIGGRAPH, 2018. 2, 3

[21] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to
discover cross-domain relations with generative adversarial
networks. In ICML, 2017. 3

[22] P.-Y. Laffont, A. Bousseau, S. Paris, F. Durand, and G. Dret-
takis. Coherent intrinsic images from photo collections. In
Proc. SIGGRAPH Asia, 2012. 2

[23] P.-Y. Laffont, Z. Ren, X. Tao, C. Qian, and J. Hays. Transient
attributes for high-level understanding and editing of outdoor
scenes. In Proc. SIGGRAPH, 2014. 2

[24] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In CVPR, 2017. 2

[25] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. K. Singh, and M.-H.
Yang. Diverse image-to-image translation via disentangled
representations. In ECCV, 2018. 2, 3, 4, 5

[26] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-

image translation networks. In NeurIPS, 2017. 3

[27] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smol-
ley. Least squares generative adversarial networks. In ICCV,
2017. 5

[28] R. Martin-Brualla, R. Pandey, S. Yang, P. Pidlypenskyi,
J. Taylor, J. Valentin, S. Khamis, P. Davidson, A. Tkach,
P. Lincoln, A. Kowdle, C. Rhemann, D. B. Goldman, C. Ke-
skin, S. Seitz, S. Izadi, and S. Fanello. LookinGood: Enhanc-
ing performance capture with real-time neural re-rendering.
In Proc. SIGGRAPH Asia, 2018. 2, 3

[29] A. Mordvintsev, C. Olah, and M. Tyka. Inceptionism: Go-
ing deeper into neural networks. Google Research Blog. Re-
trieved June, 2015. 8

[30] A. Odena, V. Dumoulin, and C. Olah. Deconvolution and

checkerboard artifacts. Distill, 2016. 8

[13] V. H. Hiep, R. Keriven, P. Labatut, and J.-P. Pons. Towards
In CVPR,

high-resolution large-scale multi-view stereo.
2009. 2

[31] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C.
Berg. Transformation-grounded image generation network
for novel 3D view synthesis. In CVPR, 2017. 1

[14] Y. Hold-Geoffroy, K. Sunkavalli, S. Hadap, E. Gambaretto,
and J.-F. Lalonde. Deep outdoor illumination estimation. In
CVPR, 2017. 2

[32] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.
Efros. Context encoders: Feature learning by inpainting. In
CVPR, 2016. 2

[15] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal
unsupervised image-to-image translation. In ECCV, 2018. 2,
3, 4

[16] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 2, 3

[17] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In ECCV, 2016.
5, 6

[18] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of GANs for improved quality, stability, and varia-
tion. In ICLR, 2018. 3, 5

[33] F. Radenovi´c, J. L. Sch¨onberger, D. Ji, J.-M. Frahm,
O. Chum, and J. Matas. From dusk till dawn: Modeling
in the dark. In CVPR, 2016. 2

[34] H. Rhodin, M. Salzmann, and P. Fua.

Unsupervised
geometry-aware representation learning for 3D human pose
estimation. In ECCV, 2018. 1

[35] T. Saito and T. Takahashi. Comprehensible rendering of 3-D

shapes. In Proc. SIGGRAPH, 1990. 2, 3

[36] J. L. Sch¨onberger. Colmap. http://colmap.github.

io, 2016. 3, 5

[37] J. L. Sch¨onberger and J.-M. Frahm. Structure-from-motion

revisited. In CVPR, 2016. 3

[19] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface
reconstruction. In Proc. Eurographics Symposium on Geom-
etry Processing, 2006. 3

[38] J. L. Sch¨onberger, E. Zheng, M. Pollefeys, and J.-M. Frahm.
Pixelwise view selection for unstructured multi-view stereo.
In ECCV, 2016. 2, 3

6886

[39] Q. Shan, R. Adams, B. Curless, Y. Furukawa, and S. M.
Seitz. The Visual Turing Test for scene reconstruction. In
Proc. 3DV, 2013. 1, 2, 3, 8

[40] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. CoRR, 2014. 5
[41] N. Snavely, S. M. Seitz, and R. Szeliski. Photo Tourism:
In Proc. SIGGRAPH,

Exploring photo collections in 3D.
2006. 1

[42] K. Sunkavalli, W. Matusik, H. Pﬁster, and S. Rusinkiewicz.

Factored time-lapse video. In Proc. SIGGRAPH, 2007. 2

[43] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3D
models from single images with a convolutional network. In
ECCV, 2016. 1

[44] J. Thies, M. Zollh¨ofer, C. Theobalt, M. Stamminger, and
M. Nießner. IGNOR: Image-guided neural object rendering.
arXiv 2018, 2018. 2, 3

[45] M. Waechter, N. Moehrle, and M. Goesele. Let there be
In

color! Large-scale texturing of 3D reconstructions.
ECCV, 2014. 3

[46] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and semantic
manipulation with conditional gans. In CVPR, 2018. 3, 5

[47] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, N. Yakovenko, A. Tao,
In

J. Kautz, and B. Catanzaro. Video-to-video synthesis.
NeurIPS, 2018. 2, 3

[48] T. Y. Wang, T. Ritschel, and N. J. Mitra. Joint material and
illumination estimation from photo sets in the wild. In Proc.
3DV, 2018. 2

[49] X. Wang and A. Gupta. Generative image modeling using
style and structure adversarial networks. In ECCV, 2016. 2
[50] Z. Zhang, Y. Song, and H. Qi. Age progression/regression

by conditional adversarial autoencoder. In CVPR, 2017. 2

[51] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Tor-
ralba. Scene parsing through ADE20K dataset. In CVPR,
2017. 5

[52] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In ICCV, 2017. 3

[53] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros,
O. Wang, and E. Shechtman. Toward multimodal image-
to-image translation. In NeurIPS, 2017. 2, 3, 4, 5, 6

6887

