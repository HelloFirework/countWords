Visual Query Answering by Entity-Attribute Graph Matching and Reasoning

Peixi Xiong1∗

, Huayi Zhan1∗

, Xin Wang2∗

,

Baivab Sinha3

, Ying Wu1

1Northwestern University,

2 Southwest Jiaotong University,

3Sichuan Changhong Electric Co. Ltd

1 {peixixiong2018, huayi.zhan, yingwu}@u.northwestern.edu,

2 xinwang@swjtu.cn, 3 baivabsinha@changhong.com

Abstract

Visual Query Answering (VQA) is of great signiﬁcance
in offering people convenience: one can raise a question
for details of objects, or high-level understanding about the
scene, over an image. This paper proposes a novel method
to address the VQA problem. In contrast to prior works,
our method that targets single scene VQA, replies on graph-
based techniques and involves reasoning. In a nutshell, our
approach is centered on three graphs. The ﬁrst graph, re-
ferred to as inference graph GI , is constructed via learn-
ing over labeled data. The other two graphs, referred to as
query graph Q and entity-attribute graph GEA, are gener-
ated from natural language query Qnl and image Img, that
are issued from users, respectively. As GEA often does not
take sufﬁcient information to answer Q, we develop tech-
niques to infer missing information of GEA with GI . Based
on GEA and Q, we provide techniques to ﬁnd matches of
Q in GEA, as the answer of Qnl in Img. Unlike com-
monly used VQA methods that are based on end-to-end neu-
ral networks, our graph-based method shows well-designed
reasoning capability, and thus is highly interpretable. We
also create a dataset on soccer match (Soccer-VQA)1 with
rich annotations. The experimental results show that our
approach outperforms the state-of-the-art method and has
high potential for future investigation.

1. Introduction

In recent years, visual query answering (VQA) has re-
ceived signiﬁcant attention [20, 24, 8] as it involves multi-
disciplinary research, e.g. natural language understanding,
visual information retrieving and multi-modal reasoning.
The task of VQA is to ﬁnd an answer to a query Qnl based

∗Authors contributed equally
1The Soccer-VQA dataset has been released publicly at

http://120.25.121.173/research/vqa

Figure 1: The image is about soccer match, where each per-
son object is associated with attributes: id, uniform color,
status (Standing, Moving, Expansion), direction (Backing,
Facing, N/A), as well as location, and the soccer object is
attributed with location.

on the content of an image. There are a variety of applica-
tions of VQA, e.g. surveillance video understanding, visual
commentator robot, etc. Solving VQA problems usually re-
quires high level reasoning from the content of an image.

Ever since the VQA problem was ﬁrst brought up
by [19], end-to-end neural network has become the dom-
inant approach in the community. The basic idea is to
extract representations of image and text by convolutional
neural network and recurrent neural network respectively,
and to combine these two representations to form a joint
embedding which is then fed into a classiﬁer to infer the an-
swer [36]. The NN-based approach tries to learn the corre-
lations of text query with input image (and implicitly with
the expected answer) in a joint semantic space. A major
improvement to the basic method is to add attention mech-
anism [38, 31, 37, 12, 18].

Despite the dominance in VQA literature, neural net-
work approach has a few weaknesses, which greatly hinders
its further development. First of all, deep neural network
works as “black boxes”, hence it is very hard to identify
the causal relations between model design and system per-

8357

formance. Secondly, but more importantly, there is no evi-
dence to support the hypothesis that neural network has the
capability of reasoning in solving VQA problems. On the
contrary, recent work [10] has shown that it is possible to
do quite well on many VQA problems by simply memoriz-
ing statistics about query / answer pairs. To overcome these
weaknesses, a more feasible method with viable reasoning
capability is highly needed. Consider the problem of nat-
ural language query answering (NLQ), which is analogous
to VQA problem but without image input, the state-of-the-
art approach to NLQ problem prefers to apply graph-based
techniques, that represents underlying answers and queries
as knowledge graph and query graph, respectively, and ﬁnd
answers with graph pattern matching, rather than relying on
conventional neural network based methods. The beneﬁt of
the approach lies in that structured representations contain
richer information than unstructured ones, and hence is ca-
pable to ﬁnd reliable results. Indeed, the similar technique
can also be applied for VQA problem.

Example 1: Figure 1 depicts an image about a soccer
match, where two teams are distinguished by red and green
uniforms, and each object is associated with a set of at-
tributes. A typical query may ask “How many players are
there in the image?”. Though simple, it is nontrivial to an-
swer the query, as we not only need to identify all the person
objects, but also have to infer the hidden attribute “role” of
each person, i.e. reasoning whether the person is a player,
or a goalkeeper, or a referee.

To answer the query, one can represent the image with
graph structure by identifying objects along with their at-
tributes, and constructing a graph GEA, denoted by entity-
attribute graph, using objects that are identiﬁed.

The beneﬁts of graph representation are twofold:

(1)
as GEA may not contain sufﬁcient information to answer
query, e.g. value of attribute “role ” may not be identiﬁed
via visual method, we are allowed to develop techniques
to reason missing information that is crucial for the query;
and (2) query answering can be evaluated via graph pattern
matching due to structured representation of the query. (cid:2)

This example suggests that we leverage graph-based
method to resolve the VQA problem. While to do this, sev-
eral questions have to be settled. (1) How to represent image
and query with graphs? (2) How to infer crucial informa-
tion when GEA constructed from image is insufﬁcient? (3)
How to ﬁnd answers from graphs with GEA?

The contributions of our paper include following aspects:
(1) We produced a data set of 7900 images on soccer
match. For each image in the data set, we make a detailed
annotation on objects to describe their attributes, e.g. color,
role, status, location, etc. To the best of our knowledge, this
is the ﬁrst data set about soccer match in VQA literature.

(2) We propose approaches to answering visual ques-
tions with graph-based techniques. More speciﬁcally, we
ﬁrst construct an entity-attribute graph from a given image;
we then train a classiﬁer to infer missing information that
are crucial for answering queries; we ﬁnally provide meth-
ods to answer queries with graph pattern matching.

2. Related Work

We categorize related work into following three parts.

Current VQA approaches are
Visual query answering.
[38] introduces a
mainly based on deep neural works.
spatial attention mechanism similar to the model for im-
age captioning. Instead of computing the attention vector
iteratively, [31] obtains a global spatial attention weights
vector which is then used to generate a new image em-
bedding. [37] proposed to model the visual attention as a
multivariate distribution over a grid-structured conditional
random ﬁeld on image regions, thus multiple regions can
be selected at the same time. This attention mechanism
is called structured multivariate attention in [37]. There
has been many other improvements to the standard deep
learning method, e.g. [7] utilized Multimodal Compact Bi-
linear (MCB) pooling to efﬁciently and expressively com-
bine multimodal features. Another interesting idea is the
implementation of Neural Module Networks [2, 11], which
decomposes queries into their linguistic substructures, and
uses these structures to dynamically instantiate module net-
works. [27] proposed to build graph over scene objects and
question words. The visual graph is similar to ours, but the
query graph differs. Note that the method [27] proposed is
still a neural network based method as the structured repre-
sentations are fed into a recurrent network to form the ﬁnal
embedding and the answer is again inferred by a classiﬁer.

Visual Objects Processing. Visual object detection as well
as relationship identiﬁcation are the preliminary tasks for
not only VQA but also image captioning [17, 32, 27]. Other
works, e.g. [33], produce high-level attributes for input im-
ages, based on which further processing can be conducted.
These prior works show that detecting all visual objects,
their attributes and relationships is very vital for resolving
VQA problem.

Graph-based query answering. Query answering has been
extensively studied for graph data. In a nutshell, this work
includes two aspects: query understanding, and query eval-
uation. We next review previous work on two aspects.
languages are very
(1) Queries expressed with natural
user-friendly, but nontrivial
Typically,
they need to be structured before issuing over e.g. search
engine, knowledge graph, since structured queries are more
expressive. There exist a host of works that based on query
logs, human interaction and neural network, respectively.
[23] leverages query logs to train a classiﬁer, based on

to understand.

8358

In this section, we introduce our dataset as well as typical

Scene

type

hidden

[35] propose an
which structured queries are generated.
approach to generate the structured queries through talking
between the data (i.e. the knowledge graph) and the user.
[34] introduced how to generate a core inferential chain
from a query with convolutional neural networks. As we
only cope with a set of ﬁxed queries, hence, we defer the
topic of query understanding to another paper, and focus
primarily on the query evaluation.
(2) To evaluate queries on graphs, a typical method is
graph pattern matching. There has been a host of work on
graph pattern matching, e.g. techniques for ﬁnding exact
matches [4, 29], inexact matches [39, 28], and evaluating
SPARQL queries on RDF data [30]. Our work differs from
the prior work in the following: (1) we integrate arithmeti-
cal and set operations in the query graph, and (2) we develop
technique to infer missing values for query answering.

3. New Dataset

domain speciﬁc questions.

3.1. Innovations

Traditional VQA datasets, e.g. [3, 10, 25, 9, 15] are of
large scale. Though workforce and resource intensive, these
datasets are inappropriate for rule learning and reasoning
due to characteristics of overbroad domain and insufﬁcient
scene meaning. Some other datasets, e.g. [13, 19, 1], nar-
row the domain for better reasoning. However, images in
these datasets are very elementary, with simple relationship
among objects in the image, as a consequence, they are not
very helpful to ﬁnd interesting rules after reasoning. Com-
pared with theirs, ours has following two main innovations:
(1) our dataset is not only domain speciﬁc, but also includes
images that are pretty content-rich, these together enables
us to do reasoning very well; (2) with rules inferred, com-
plex questions, that implicate reasoning, arithmetic operat-
ing, etc., can be answered with high accuracy.

3.2. Images

Scale. A set of 7900 frames were collected from 2016 FIFA
World Cup videos, among which, 5900 frames are chosen
as training set, 1000 frames for validation and remaining for
testing. To ensure validity of testing, we discarded similar
frames from the same sequence.

Annotation. Annotation of our dataset consists of four
main parts, based on the object type: person, ﬁeld, soc-
cer and scene (Table 1). Here, we localize an object by a
bounding box and record the minimum and maximum val-
ues of four corners. To distinguish each person, we annotate
the role he plays, the relative direction between him and the
goal, his action and his uniform color, etc. To better locate
objects, we record whether this image is about the left, right
or middle part of the ﬁeld, along with corresponding four

keypoints. To better evaluate the high-level meaning of the
image, we also record the scene type of it.

Object

Attribute

Type

Descriptions

id

role

obvious

An index for each person in the ﬁeld.

hidden

e.g. player, goalkeeper, referee

Person

uniform

obvious

location

obvious

The uniform color of this person.
e.g. red, blue

The coordinates of the bounding box.
e.g. (xmin, ymin, xmax, ymax)

direction

obvious

The direction between this person and the
goal. e.g. backing, facing, n/a

status

obvious

The current action of this person.
e.g. standing, moving, expansion

defending

hidden

Whether this person is defending others.
e.g. yes, no

part

obvious Which part of the ﬁeld is this image about.

Field

keypoint

obvious

Soccer

location

obvious

e.g. left, right, middle

Record locations
four corners of
penalty area; Or, the lengths of center cir-
cle’s major and minor axis and its center.

for

The coordinates of the bounding box.
e.g. (xmin, ymin, xmax, ymax)

e.g. normal scene, free kick, kick off, cor-
ner kick, penalty kick

Table 1: Visual objects and their attributes.

3.3. Questions

Our questions, which are of 7 types,

involve count-
ing, detection, role identiﬁcation and understanding of the
scene. To better evaluate performance of the model, we cat-
egorized the questions into three levels, easy, medium and
hard (Table 2). They are decided by the number of vision
tasks needed during the process, and the level of knowledge
graph usage for reasoning. For the answer part, we asked
5 people to manually answer the questions, so the answers
may vary in format.

Question

Is there any referee in the image?

Id
Qnl1 Who is holding the soccer?
Qnl2 What is the uniform color of the referee?
Qnl3
Qnl4 Which team does the goalkeeper belong to?
Qnl5 Who is the defending team?
Qnl6 Which part of the ﬁeld are the players being now?
Qnl7

How many players are there in the image?

Difﬁculty
Easy
Easy
Easy
Medium
Medium
Hard
Hard

Table 2: A set of questions

Evaluation Criteria. The accuracy is calculated by check-
ing if the predicted answer is the same as any of human-
provided answers. In our experiments, to eliminate errors
that are caused by machines’ indistinguishability on vari-
ance of the ground truth answers, we asked 20 people with
different gender and age to manually check if the question
is correctly answered.

4. Our Approach

In this section, we introduce our approach with details.

8359

(cid:53)(cid:9)(cid:33)(cid:18)(cid:15)(cid:18)(cid:9)(cid:21)(cid:18)(cid:7)(cid:25)(cid:15)(cid:5)(cid:26)(cid:27)
(cid:28)(cid:20)(cid:9)(cid:3)(cid:14)(cid:15)(cid:4)(cid:21)(cid:14)(cid:2)(cid:20)(cid:9)(cid:7)(cid:11)(cid:53)(cid:25)(cid:28)(cid:12)

(cid:11)(cid:20)(cid:33)(cid:33)(cid:6)(cid:2)(cid:9)(cid:18)(cid:12)

(cid:16)(cid:5)(cid:24)(cid:18)(cid:6)(cid:18)(cid:31)(cid:7)(cid:14)(cid:15)(cid:5)(cid:2)(cid:9)(cid:2)(cid:9)(cid:17)(cid:7)(cid:31)(cid:5)(cid:14)(cid:5)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:5)(cid:6)(cid:10)(cid:3)(cid:2)(cid:3)(cid:7)(cid:11)(cid:1)(cid:8)(cid:12)

Image

Natural language 

query QNL

(cid:13)(cid:5)(cid:14)(cid:4)(cid:15)(cid:5)(cid:6)(cid:7)(cid:16)(cid:5)(cid:9)(cid:17)(cid:4)(cid:5)(cid:17)(cid:18)
(cid:19)(cid:15)(cid:20)(cid:21)(cid:18)(cid:3)(cid:3)(cid:2)(cid:9)(cid:17)(cid:7)(cid:11)(cid:13)(cid:16)(cid:19)(cid:12)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)
(cid:21)(cid:20)(cid:9)(cid:14)(cid:18)(cid:9)(cid:14)(cid:3)

(cid:47)(cid:4)(cid:18)(cid:15)(cid:10)
(cid:21)(cid:20)(cid:9)(cid:14)(cid:18)(cid:9)(cid:14)(cid:3)

(cid:53)(cid:9)(cid:33)(cid:18)(cid:15)(cid:18)(cid:9)(cid:21)(cid:18)(cid:7)

(cid:17)(cid:15)(cid:5)(cid:26)(cid:27)

(cid:22)(cid:9)(cid:14)(cid:2)(cid:14)(cid:10)(cid:23)(cid:8)(cid:14)(cid:14)(cid:15)(cid:2)(cid:24)(cid:4)(cid:14)(cid:18)(cid:7)(cid:25)(cid:15)(cid:5)(cid:26)(cid:27)
(cid:28)(cid:20)(cid:9)(cid:3)(cid:14)(cid:15)(cid:4)(cid:21)(cid:14)(cid:2)(cid:20)(cid:9)(cid:7)(cid:11)(cid:1)(cid:25)(cid:28)(cid:12)

(cid:47)(cid:4)(cid:18)(cid:15)(cid:10)(cid:7)(cid:25)(cid:15)(cid:5)(cid:26)(cid:27)

(cid:28)(cid:20)(cid:9)(cid:3)(cid:14)(cid:15)(cid:4)(cid:21)(cid:14)(cid:2)(cid:20)(cid:9)(cid:7)(cid:11)(cid:47)(cid:25)(cid:28)(cid:12)

(cid:29)(cid:2)(cid:3)(cid:3)(cid:2)(cid:9)(cid:17)(cid:7)(cid:52)(cid:5)(cid:6)
(cid:53)(cid:9)(cid:33)(cid:18)(cid:15)(cid:18)(cid:9)(cid:21)(cid:18)(cid:7)(cid:11)(cid:1)

(cid:53)(cid:9)(cid:21)(cid:20)(cid:34)(cid:26)(cid:6)(cid:18)(cid:14)(cid:18)(cid:7)(cid:22)(cid:8)(cid:25)

(cid:47)(cid:4)(cid:18)(cid:15)(cid:10)(cid:7)(cid:17)(cid:15)(cid:5)(cid:26)(cid:27)

(cid:24)(cid:5)(cid:21)(cid:36)(cid:2)(cid:9)(cid:17)
(cid:31)(cid:2)(cid:15)(cid:18)(cid:21)(cid:14)(cid:2)(cid:20)(cid:9)

(cid:46)(cid:46)(cid:46)

(cid:18)(cid:44)(cid:26)(cid:5)(cid:9)(cid:3)(cid:2)(cid:20)(cid:9)
(cid:3)(cid:14)(cid:5)(cid:14)(cid:4)(cid:3)

(cid:15)(cid:20)(cid:6)(cid:18)

(cid:19)(cid:18)(cid:15)(cid:3)(cid:20)(cid:9)

(cid:2)(cid:31)
(cid:4)(cid:9)(cid:2)(cid:33)(cid:20)(cid:15)(cid:34)(cid:35)(cid:21)(cid:20)(cid:6)(cid:20)(cid:15)

(cid:32)

(cid:24)(cid:6)(cid:5)(cid:21)(cid:36)

(cid:24)(cid:5)(cid:21)(cid:36)(cid:2)(cid:9)(cid:17)
(cid:31)(cid:2)(cid:15)(cid:18)(cid:21)(cid:14)(cid:2)(cid:20)(cid:9)

(cid:30)

(cid:31)(cid:2)(cid:3)(cid:14)(cid:5)(cid:9)(cid:21)(cid:18)

(cid:2)(cid:31)

(cid:18)(cid:44)(cid:26)(cid:5)(cid:9)(cid:3)(cid:2)(cid:20)(cid:9)

(cid:3)(cid:14)(cid:5)(cid:14)(cid:4)(cid:3)

(cid:19)(cid:18)(cid:15)(cid:3)(cid:20)(cid:9)

(cid:15)(cid:20)(cid:6)(cid:18)

(cid:30)

(cid:6)(cid:20)(cid:21)(cid:5)(cid:14)(cid:2)(cid:20)(cid:9)

(cid:46)(cid:46)(cid:46)

(cid:6)(cid:20)(cid:21)(cid:5)(cid:14)(cid:2)(cid:20)(cid:9)

(cid:38)
(cid:11)(cid:37)(cid:37)(cid:38)(cid:39)(cid:37)(cid:38)(cid:32)(cid:39)(cid:37)(cid:40)(cid:41)(cid:39)(cid:42)(cid:43)(cid:43)(cid:12)

(cid:4)(cid:9)(cid:2)(cid:33)(cid:20)(cid:15)(cid:34)(cid:35)(cid:21)(cid:20)(cid:6)(cid:20)(cid:15)

(cid:11)(cid:42)(cid:43)(cid:37)(cid:39)(cid:41)(cid:43)(cid:43)(cid:39)(cid:42)(cid:45)(cid:45)(cid:39)(cid:41)(cid:40)(cid:38)(cid:12)

(cid:17)(cid:15)(cid:18)(cid:18)(cid:9)

(cid:22)(cid:9)(cid:14)(cid:2)(cid:14)(cid:10)(cid:23)(cid:8)(cid:14)(cid:14)(cid:15)(cid:2)(cid:24)(cid:4)(cid:14)(cid:18)(cid:7)(cid:25)(cid:15)(cid:5)(cid:26)(cid:27)

(cid:25)(cid:15)(cid:5)(cid:26)(cid:27)(cid:7)(cid:19)(cid:5)(cid:14)(cid:14)(cid:18)(cid:15)(cid:9)(cid:7)(cid:29)(cid:5)(cid:14)(cid:21)(cid:27)(cid:2)(cid:9)(cid:17)(cid:7)

(cid:11)(cid:25)(cid:29)(cid:12)

(cid:11)(cid:24)(cid:12)(cid:7)(cid:22)(cid:9)(cid:14)(cid:2)(cid:14)(cid:10)(cid:23)(cid:5)(cid:14)(cid:14)(cid:15)(cid:2)(cid:24)(cid:4)(cid:14)(cid:18)(cid:7)(cid:17)(cid:15)(cid:5)(cid:26)(cid:27)

(cid:47)(cid:4)(cid:18)(cid:15)(cid:10)(cid:48)(cid:7)(cid:49)(cid:20)(cid:50)(cid:7)(cid:34)(cid:5)(cid:9)(cid:10)(cid:7)(cid:26)(cid:6)(cid:5)(cid:10)(cid:18)(cid:15)(cid:3)(cid:7)(cid:5)(cid:15)(cid:18)(cid:7)(cid:14)(cid:27)(cid:18)(cid:15)(cid:18)(cid:7)(cid:2)(cid:9)(cid:7)(cid:14)(cid:27)(cid:18)(cid:7)(cid:2)(cid:34)(cid:5)(cid:17)(cid:18)(cid:30)(cid:7)

num(cid:11)(cid:19)(cid:18)(cid:15)(cid:3)(cid:20)(cid:9)(cid:12)

(cid:11)(cid:30)(cid:12)

(cid:15)(cid:20)(cid:6)(cid:18)
(cid:26)(cid:6)(cid:5)(cid:10)(cid:18)(cid:15)

(cid:11)(cid:5)(cid:12)(cid:7)(cid:51)(cid:52)(cid:18)(cid:15)(cid:52)(cid:2)(cid:18)(cid:50)(cid:7)(cid:20)(cid:33)(cid:7)(cid:14)(cid:27)(cid:18)(cid:7)(cid:5)(cid:26)(cid:26)(cid:15)(cid:20)(cid:5)(cid:21)(cid:27)

(cid:11)(cid:21)(cid:12)(cid:7)(cid:47)(cid:4)(cid:18)(cid:15)(cid:10)(cid:7)(cid:5)(cid:9)(cid:31)(cid:7)(cid:2)(cid:14)(cid:3)(cid:7)(cid:17)(cid:15)(cid:5)(cid:26)(cid:27)(cid:7)(cid:15)(cid:18)(cid:26)(cid:15)(cid:18)(cid:3)(cid:18)(cid:9)(cid:14)(cid:5)(cid:14)(cid:2)(cid:20)(cid:9)

Figure 2: Overview of our approach, Entity Attribute Graph and Queries

4.1. Representation

Below, we ﬁrst review a few concepts.

4.1.1 Entity-Attribute Graph

We start with notions of entities, attributes, relations and
entity-attribute graphs.

Entities, Attributes & Relations. Entities are typically
deﬁned as objects or concepts that exist in the real world,
e.g. people, soccer etc. An entity often carries multiple at-
tributes, that describe characteristics of the entity, e.g. uni-
form color, person role. Among entities, there may exist
various relationships, e.g. friendship, showing the correla-
tion of entity pairs.

Entity-Attribute Graphs. Assume a set E of entities, a set
D of values, a set P of predicates indicating attributes of
entities and a set Θ of types. Each entity e in E has a unique
ID and a type in Θ.

An entity-attribute graph, denoted as EAG, is a set of
triples t = (s, p, o), where subject s is an entity in E, p is
a predicate in P, and object o is either an entity in E or
a value d in D. It can be represented as a directed edge-
labeled graph GEA = (V, E), such that (a) V is the set of
nodes consisting of s and o for each triple t = (s, p, o); and
(b) there is an edge in E from s to o labeled by p for each
triple t = (s, p, o).

We consider two types of equality:

(a) node identity on E: e1 ⇔ e2 if entities e1 and e2 have
the same ID, i.e. they refer to the same entity; and
(b) value equality on D: d1 = d2 if they are the same value.
In GEA, e1 and e2 are represented as the same node if

e1 ⇔ e2; similarly for values d1 and d2 if d1 = d2.

Example 2: Figure 2 (b) shows a sample EAG, where each
rounded (resp. square) node represents an entity (resp. at-
tribute), each directed edge labeled by p from an entity node
ve to a value node va denotes that ve has a p attribute with
value va, and each object pair is connected with bidirec-
tional arrow due to mutual relationship, e.g. distance.
(cid:2)

Image Representation. An image can be represented as an
EAG with detected objects and obvious attributes. This can
be achieved via a few visual tasks. While EAG generated
directly after image processing is often incomplete, i.e. it
may miss some crucial information to answer queries. We
hence refer to entity-attribute graphs with incomplete infor-
mation as incomplete entity-attribute graphs, and associate
nodes with white rectangles, to indicate the missing value of
an entity or attribute in EAG. Figure 2(b) is an incomplete
entity-attribute graph, in which square nodes representing
person roles are associated with white rectangle.

As queries issued with natural languages are often trans-
lated into graph structures for the purpose of evaluation, to
answer structured queries, it would be beneﬁcial to con-
struct an EAG from an image so that existing techniques
can be directly applied for query answering.

4.1.2 Query Representation

It is recognized that querying graph data with keywords
from Qnl may not well capture users query intention [23].
Instead, a structured query with “query focus” is favored. In
light of this, we next introduce the notion of query graphs.

Query Graphs. A query graph Q(uo) is a set of triples
(sQ, pQ, oQ), where sQ is either a variable z or a function
f (z) taking z as parameter, oQ is one of a value d or z or
f (z), and pQ is a predicate in P. Here function f (z) is
deﬁned by users, and variable z has one of three forms: (a)
entity variable y, to map to an entity, (b) value variable y∗,
to map to a value, and (c) wildcard y, to map to an entity.
Here sQ can be either y or y, while oQ can be y, y∗ or y.
Entity variables and wildcard carry a type, denoting the type
of entities they represent.

A query graph can also be represented as a graph such
that two variables are represented as the same node if they
have the same name of y, y∗ or y; similarly for functions
f (z) and values d. We assume w.l.o.g.
that Q(x) is con-
nected, i.e. there exists an undirected path between uo and
each node in Q(uo). In particular, uo is a designated node in

8360

Q(uo), denoting the query focus and labeled by “?”. Take
Fig. 2(c) as example. It depicts a query graph that is gen-
erated from query “How many players are there in the im-
age?”. Note that the “query focus” uo carries a function
num () that calculates the total number of person entities
with role “player”.

Remark. In this paper, we do not cope with arbitrary Qnl,
and only handle a set of ﬁxed queries (Table 2). In light
of this, we do not provide techniques to structure Qnl. We
refer interested readers to references, e.g. [23, 35, 34], for
more details about the task.

4.1.3 Graph Pattern Matching

updated EAG for query answering. The inference graph GI
is used to infer missing values of an incomplete EAG. and
constructed by module IGC over training data. As is query-
independent, GI is constructed ofﬂine, which warrants
the efﬁciency of our approach. As the other part of input,
natural language query Qnl needs to be structured for query
evaluation. To this end, QN L is ﬁrst parsed via our NLP
module, and then structured by module QGC. After Q(uo)
and GEA are generated, our approach employs module GM
for matching computation, and returns ﬁnal result.

As some modules employ existing techniques,

to
emphasize our novelty, we will elaborate modules VA and
VGA in Section 4.3, modules IGC and VI in Section 4.4,
and module GM in Section 4.5 with more details.

We introduce the notion of valuation, followed by graph
pattern matching problem (GPM).

4.3. EAG Generation from Images

Valuation. A valuation of Q(uo) in a set S of triples is
a mapping ν from Q(uo) to S that preserves values in D
and predicates in P, and maps variables y and y to en-
tities of the same type. More speciﬁcally, for each triple
(sQ, pQ, oQ) in Q(uo), there exists (s, p, o) in S, written
as (sQ, pQ, oQ) (cid:4)→ν (s, p, o) or simply (sQ, pQ, oQ) (cid:4)→
(s, p, o), where
(a) ν(sQ) = s, p = pQ, ν(oQ) = o;
(b) o is an entity if oQ is a variable y or y; it is a value if
oQ is y∗, and o = d if oQ is a value d; and
(c) entities s and sQ have the same type; similarly for enti-
ties o and oQ if oQ is y or y.

We say that ν is a bijection if ν is one-to-one and onto.

Graph Pattern Matching. [4]. Consider an EAG GEA =
(V, E) and a query graph Q(uo)=(VQ, EQ, uo). We say that
GEA matches Q(uo) at e if there exist a set S of triples in
GEA and a valuation ν of Q(uo) in S such that ν(x) = e,
and ν is a bijection between Q(uo) and S. We refer to S as
a match of Q(uo) in GEA at e under ν. Intuitively, ν is an
isomorphism from Q(uo) to S when Q(uo) and S are de-
picted as graphs. That is, we adopt subgraph isomorphism
for the semantics of graph pattern matching.

4.2. VQA Modeling

We propose a comprehensive approach as modeling of

the VQA problem.

Figure 2(a) presents the overview of our approach. As
can be seen, our approach revolves around three graphs:
entity-attribute graph, query graph and inference graph.
The generation of entity-attribute graph GEA follows
three steps. Module VA conducts the ﬁrst step, i.e. image
processing, and outputs all the detected objects along with
their attributes. Using visual contents produced in step one,
module VGA constructs an incomplete EAG.
In the last
step, module VI takes inference graph and incomplete EAG
as inputs, infer missing information with GI , and outputs an

We next introduce how an EAG is constructed by illus-

trating functions of modules VA and VGA.

4.3.1 Visual Processing

Inspired by [32, 16, 5], module VA conducts a few visual
tasks to detect the objects and ﬁgure out their attributes. In-
ﬂuenced by queries given in Table 2, for each image img,
module VA only recognizes four types of objects, i.e.person,
ﬁeld, soccer and scene, as shown in Table 1.

(a) Standing

(b) Moving

(c) Expansion

Figure 3: Person status.

Many obvious attributes of person object can be obtained
by simple vision tasks. For instance, attributes “location”,
“direction”, and “status” can be ﬁgured out by object de-
tection, followed by skeleton detection in the object regions
and appropriate classifying for skeleton patterns. As shown
in Fig. 3, we categorize three types of person “status”, i.e.
standing, moving and expansion, where the last one is dis-
tinguished from the ﬁrst two by the pattern of object’s knees
and the space he occupied.

Figure 4: Image registration to standard ﬁeld.

8361

Obvious attributes of ﬁeld object can be detected as fol-
lows. Attribute “part” is distinguished via simple image
classiﬁer. Attribute “keypoint” can be identiﬁed by edge
and circle detection. With “keypoint”, we register the im-
age into our standard ﬁeld (Figure 4), then all local coordi-
nates (locations of person and soccer) in the image can be
transformed into a global coordinates of the bird’s-eye view
standard soccer ﬁeld.

After processing, VA module outputs a set of identiﬁed

objects and their obvious attributes for EAG construction.

4.3.2 EAG Construction

Module VGA is responsible for EAG construction. Given
output of module VA over image, VGA conducts the follow-
ing: (1) constructing an empty entity-attribute graph GEA;
(2) treating objects and attribute values as subject and ob-
ject, respectively, and creating nodes corresponding to each
object and attribute value in GEA; (3) connecting node ve
to node va with edge labeled by p, to indicate that entity
e has an attribute p with value a (nodes ve and va corre-
spond to e and a, respectively), for each entity and its obvi-
ous attribute; and (4) linking node pair (ve1 ,ve2 ), with bidi-
rectional edge labeled with distance between entity e1 and
e2. Note that, VGA also connects entity node ve to a value
node vb taking blank value with edge labeled by p′, if p′ is
a hidden attribute, and the value of attribute p′ can not be
identiﬁed by module VA.

4.4. EAG-based Reasoning

An incomplete EAG is often not able to provide query an-
swers due to missing values of some hidden attributes. This
motivates us to develop methods to infer values of hidden
attributes. Below, we present modules IGC and IM, which
are responsible for inference graph construction and miss-
ing value inference, respectively.

In our model, the inference graph is constructed using
the Bayesian network. Essentially, Bayesian network is a
kind of directed acyclic graph model, of which the parame-
ters can be explicitly represented by the nodes (i.e., random
variables). Additionally, the parameters can be endowed
with distributions (i.e., priors). Using Bayesian network as
inference graph leads to the resulting structure being very
concise.

4.4.1

Inference Graph

As mentioned above, the inference graph is constructed us-
ing Bayesian network. A typical Bayesian network con-
sists of decision and utility nodes [21]. We follow the de-
scriptive notations used in [14] to facilitate our problem.
Deﬁned by D = {x(i)}N
i=1 the set of N instances, each
instance x(i) = [x(i)
n ] is the observation over n
random variables: x1 ∼ X1, · · · , xn ∼ Xn. Under this
assumption, a Bayesian network can be formally described

1 , · · · , x(i)

Figure 5: The pipeline of inference graph used for inferring
the role of a person object.

by B =< G, ΘG >, where G is a directed acyclic graph and
ΘG the set of parameters that can maximize the likelihood
[6, 22]. The i-th node in G corresponds to a random variable
Xi, and an edge between two connected nodes indicates the
direct dependency. The symbol of ΘG is a parametric set
that uses to quantify the dependencies within G. Speciﬁ-
cally, the parameters set of the i-th node associated with an
observation xi in ΘG can be denoted by θxi |Πi(x), where
Πi(x) is a function which takes x as input, and outputs the
values of attributes whose child is i. Note here that xi is a
possible value of Xi. For notational simplicity, the notation
of θxi |Πi(x) is fully equal to θXi=xi |Πi(x).

With the notations above, the unique joint probability
distribution of a Bayesian network (i.e., the inference graph
Gi) is given by

PB(x) =

n

(cid:2)

i=1

θxi|Πi(x)

(1)

In our ﬁrst problem, the purpose of Bayesian network is
to infer the corresponding role that can be further regarded
as an additional variable, e.g. Y (similar handling for the
second one). The notation of Y is also a random variable
associated with our target value with the values y ∈ Y. In
order to take Y into consideration, we rearrange the data
D into another form: D = {(yi, x(i))}N
i=1. Accordingly,
Eq. (1) is reformulated to the following form

PB(y|x) =

PB(y, x)
PB(x)

=

θy|Πi(x) (cid:3)n

i=1 θxi|y,Πi(x)

(cid:4)y′∈Y θy′|Πi(x) (cid:3)n

i=1 θxi|y′,Πi(x)

(2)

4.4.2 Learning the Inference Graph

To preserve the signiﬁcance of posterior estimator PB(y|x),
Na¨ıve Bayes takes the class variables as the root, and all
attributes are conditional independent when conditioned on
the class [22]. This assumption leads to the following form

PB(y|x) ∝ θy

n

(cid:2)

i=1

θxi|y

(3)

8362

(cid:5)(cid:22)(cid:15)(cid:23)(cid:24)(cid:15)(cid:23)(cid:22)(cid:14)(cid:23)(cid:24)(cid:14)(cid:9)

(cid:3)(cid:2)(cid:21)(cid:12)(cid:25)(cid:17)(cid:2)(cid:8)

(cid:6)(cid:4)(cid:1)(cid:7)(cid:2)(cid:8)

(cid:1)(cid:2)(cid:3)(cid:4)

(cid:11)(cid:3)(cid:12)(cid:13)(cid:4)(cid:1)

min(cid:5)(cid:27)(cid:17)(cid:7)(cid:25)(cid:12)(cid:8)(cid:21)(cid:4)(cid:9)
(cid:3)(cid:2)(cid:21)(cid:12)(cid:25)(cid:17)(cid:2)(cid:8)

(cid:26)(cid:2)(cid:21)(cid:21)(cid:4)(cid:1)

(cid:5)(cid:22)(cid:15)(cid:23)(cid:24)(cid:15)(cid:23)(cid:22)(cid:14)(cid:23)(cid:24)(cid:14)(cid:9)

(cid:16)(cid:8)(cid:17)(cid:18)(cid:2)(cid:1)(cid:19)(cid:20)(cid:21)(cid:2)(cid:3)(cid:2)(cid:1)

Q(cid:8)(cid:3)(cid:29)

(cid:5)(cid:10)(cid:9)

(cid:1)(cid:2)(cid:3)(cid:4)

(cid:6)(cid:4)(cid:1)(cid:7)(cid:2)(cid:8)
(cid:16)(cid:8)(cid:17)(cid:18)(cid:2)(cid:1)(cid:19)(cid:20)(cid:21)(cid:2)(cid:3)(cid:2)(cid:1)

(cid:31)(cid:2)(cid:12)(cid:3)(cid:32)(cid:4)(cid:4)(cid:11)(cid:4)(cid:1)

(cid:5)(cid:10)(cid:9)

(cid:6)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)

(cid:11)(cid:10)

(cid:8)(cid:6)

(cid:7)

(cid:9)(cid:10)(cid:7)(cid:11)(cid:12)(cid:11)(cid:13)(cid:7)
(cid:8)(cid:6)

(cid:34)(cid:17)(cid:4)(cid:3)(cid:27)

(cid:11)(cid:12)(cid:1)(cid:25)

(cid:5)(cid:10)(cid:9)

(cid:14)(cid:12)(cid:15)(cid:16)(cid:2)(cid:6)(cid:1)(cid:3)(cid:17)(cid:3)(cid:6)(cid:18)(cid:3)(cid:16)(cid:19)(cid:17)(cid:12)(cid:9)(cid:20)(cid:16)(cid:8)(cid:1)(cid:16)(cid:17)(cid:8)(cid:4)(cid:3)

(cid:14)(cid:21)(cid:15)(cid:16)(cid:2)(cid:6)(cid:1)(cid:3)(cid:17)(cid:3)(cid:6)(cid:18)(cid:3)(cid:16)(cid:19)(cid:17)(cid:12)(cid:9)(cid:20)(cid:16)(cid:8)(cid:1)(cid:16)(cid:11)(cid:3)(cid:12)(cid:22)(cid:16)(cid:7)(cid:11)(cid:12)(cid:11)(cid:13)(cid:7)(cid:16)

Q(cid:8)(cid:3)(cid:15)

Q(cid:8)(cid:3)(cid:14)

Q(cid:8)(cid:3)(cid:28)

(cid:1)(cid:2)(cid:3)(cid:4)

(cid:6)(cid:4)(cid:1)(cid:7)(cid:2)(cid:8)
(cid:16)(cid:8)(cid:17)(cid:18)(cid:2)(cid:1)(cid:19)(cid:20)(cid:21)(cid:2)(cid:3)(cid:2)(cid:1)

(cid:1)(cid:4)(cid:18)(cid:4)(cid:1)(cid:4)(cid:4)

(cid:5)(cid:10)(cid:9)

(cid:6)(cid:4)(cid:1)(cid:7)(cid:2)(cid:8)(cid:5)(cid:10)(cid:9)

(cid:1)(cid:2)(cid:3)(cid:4)

(cid:1)(cid:4)(cid:18)(cid:4)(cid:1)(cid:4)(cid:4)

Q(cid:8)(cid:3)(cid:30)

Q(cid:8)(cid:3)(cid:33)

num(cid:5)(cid:6)(cid:4)(cid:1)(cid:7)(cid:2)(cid:8)(cid:9)

(cid:5)(cid:10)(cid:9)

(cid:1)(cid:2)(cid:3)(cid:4)

(cid:11)(cid:3)(cid:12)(cid:13)(cid:4)(cid:1)

Figure 6: Query graphs

As can be seen here, Na¨ıve Bayes simpliﬁes the structure
of Bayesian network. In our proposed model, the structure
of Na¨ıve Bayes is used to infer the role of detected person.
To graphically and demonstratively infer the role of
detected person, Figure 5 summarizes the pipeline of
inference graph GI , which are composed of two col-
laborative parts:
state extraction (observation) and role
probability inference. To be speciﬁc, orientation, action,
color uniqueness of uniform, as well as ﬁeld type are ﬁrstly
employed to describe the state of an unknown candidate,
which are then fed into the inference graph to produce the
probability of each role. And the ﬁnal role is decided based
on the maximum probability.

After inference, one can either use a complete EAG to
answer queries, or directly apply inference graph to ﬁnd an-
swers to certain queries (see Section 5 for an example).

4.5. EAG-based Matching

As introduced earlier, given a natural language query
Qnl, one needs to translate it into a query graph for eval-
uation. In light of this, we manually construct a set of query
graphs, shown in Figure 6 as the correspondence of the set
of questions given in Table 2. It is worth noting that query
graph of Qnl5 is not provided as the query does not need
matching computation.

One may notice that some of pattern graphs are asso-
ciated with functions on nodes or edges. The reason is that
when transforming the questions into query graphs, we need
to deﬁne some auxiliary functions to ﬁnd correct answers.

Speciﬁcally, (1) we deﬁne the “min ()” function to mea-
sure the minimum distance, for the question “Who is hold-
ing the soccer?”. The argument to min () is an array whose
i-th element is the distance between the i-th player in the
image and the soccer. Here the distance is Euclidean dis-
tance. (2) The “num ()” function is deﬁned for the question
“How many players are there in the image?”. Its indepen-
dent variable is all the person objects whose role attribute is
“player” in the image, and the function value is the number
of the independent variable.

Given an EAG that is generated from an image, we can
answer queries as following. We ﬁrst ignore functions de-
ﬁned on a query graph, and apply typical graph pattern

Figure 7: Inference Graphs

matching algorithm, e.g. VF2 [4] to ﬁnd matches. Over the
set of matches of query graph, we operate arithmetic or set
operations deﬁned by functions, and obtain ﬁnal answers.

5. Experiments

In this section, we conducted two sets of experiments to
evaluate (1) the effectiveness of our inference module, and
(2) the accuracy of our approach.

5.1. Effectiveness of Inference

To measure the performance of VI module, we deﬁne the

inference accuracy following the F-measure [26]:

Acc(A = “v”) =

2 · (recall(A = “v”) · precision(A = “v”))
(recall(A = “v”) + precision(A = “v”))

,

where recall(A = “v”) = #true value inferred
#true value instance , and precision(A =
“v”) = #true value inferred
#inferred instance . Here #true value inferred is the
the instances, whose attribute A is in-
number of all
ferred correctly as “v”, #true value instance is the num-
ber of all the instances with attribute A of value “v”, and
#inferred instance indicates the total number of instances
whose attribute A is inferred as “v”.

p(X|i = G)

p(X|i = R)

p(X|i = P )

direction=“F”
direction=“B”
direction=“N”

status=“E”
status=“M”
status=“S”
status=“N”

u color=“M”
u color=“U”

ﬁeld=“L”
ﬁeld=“M”
ﬁeld=“R”

3.79
82.53
13.68

47.59
16.21
34.02
2.18

4.02
95.98

51.38
4.71
43.91

18.24

4.4

77.36

0.47
69.99
27.36
2.18

20.89
79.11

16.76
70.85
12.39

14.71
8.06
77.23

4.46
78.82
14.3
2.42

99.36
0.64

15.01
72.86
12.13

Table 3: Conditional probability (%)

Accuracy of Role. Based on queries and image character-
istics, we used four variables, i.e.direction, status, ﬁeld and
unique color (abbr. u color) to compute conditional proba-
bilities. Figure 7(a) and Table 3 show inference graph and
conditional probabilities, respectively. Note that the domain
of variables direction, status and ﬁeld are given in Table 1,
while variable u color can have one of two values, to indi-
cate whether a person object has the unique uniform color
(=“U”) or not (=“M”).

8363

Using the conditional probabilities, VI infers role of
each person object. The inference accuracy is shown in
Table 4. One can ﬁnd that the inference accuracies for
different roles are above 85%, among which the accuracy
even reaches 99% for role player.

role=“G”
role=“R”
role=“P ”

precision

94.4
87.4
98.8

recall
85.5
82.8
99.3

Acc
89.8
85
99

Table 4: Inference accuracy of role (%). Here “G”, “R”
and “P ” indicate goalkeeper, referee and player, respec-
tively.

Accuracy of Team Status. Team status tells us whether
a team is attacking or defending, it is closely related to
question Qnl5 .
In practice, a defending team often has
more players with “defending” status, and moreover, most
of players are back to the goal. Based on this observation,
we designed three variables, they are p status, p direction
and t possession, that represents players’ status, players’ di-
rection and possession of the soccer, respectively. The do-
mains of three variables are all {true, f alse}, where true
indicates that the team has more players with expansion sta-
tus (resp. has more players back to the goal, has a player
closest to the soccer), and f alse otherwise.

Along the same line as computation of inference accu-
racy for role, we ﬁgure out inference accuracy for team
status. Due to space constraint, we do not report condi-
tional probabilities, but show inference accuracy and infer-
ence graph in Table 5 and Figure 7(b), respectively. As is
shown, the inference accuracy reaches 81.3% when infer-
ring whether a team is a defending team. Note that, in con-
trast to other questions, one can directly answer Qnl5 via
inference, no matching computation is needed.

precision

Team Status=“D”
Team Status=“A”

89.8
79.1

recall
74.2
92.1

Acc
81.3
85.1

Table 5: Inference accuracy of team status (%). Here “D”
and “A” indicate defending and attacking, respectively.

5.2. Overall Performance

We compared the following state-of-the-art methods:
LSTM+CNN [3] and HieCoAttenVQA [18] with ours. As
shown in Table 7 and 6, our approach is typically effective
for medium and hard questions: (1) for medium questions,
the average accuracy of our approach is 16.6% and 15.7%
higher than that of LSTM+CNN and HieCoAttenVQA, re-
spectively; and for hard questions, our approach substan-
tially outperforms LSTM+CNN and HieCoAttenVQA, with
average accuracy 3.59 and 3.56 times higher, than that of
LSTM+CNN and HieCoAttenVQA, respectively. The ad-

vantage of our approach grows even larger for hard prob-
lems. (2) LSTM+CNN and HieCoAttenVQA work slightly
better than our approach on simple questions, since they can
easily learn correlations between images and questions, thus
provide higher accuracy than ours. (3) Our method works
best among three methods, as for all questions, the average
accuracy of our approach is 38.1% and 31.9% higher than
that of LSTM+CNN and HieCoAttenVQA, respectively.

Easy Medium Hard
15.18
LSTM+CNN
63.28
15.26
HieCoAttenVQA 68.26
69.09
69.63
Ours

47.67
48.04
55.58

Average
46.40
49.11
64.76

Table 6: Average accuracy comparison (%)

LSTM+CNN HieCoAttenVQA

Qnl1
Qnl2
Qnl3
Qnl4
Qnl5
Qnl6
Qnl7

44.23
71.31
74.58
40.48
49.19
20.56
11.08

43.62
77.66
83.78
39.29
49.90
18.70
12.63

Ours
71.58
64.88
70.8
47.46
63.7
88.7
50.55

Table 7: Accuracy comparison per query (%)

6. Conclusion

We propose a framework for understanding images re-
garding soccer matches and answering domain speciﬁc
queries issued with natural languages. In contrast to pre-
vious works which learn correlation between images and
answers, our method is able to do reasoning with inference
graph GI , and answer queries using structured query graph
Q and entity-attribute graph GEA. Our idea on ﬁnding an-
swers with graphs largely broaden the view in dealing with
reasoning problems. Besides the approach, we also propose
a new dataset, which is the ﬁrst real-life dataset about soccer
match in VQA literature. Experimental results show that our
approach obtains better performance in accuracy, compared
with the state of the art algorithms, furthermore, it signiﬁ-
cantly outperforms its counterparts for hard questions.

The study of graph-based VQA problem is still in its
infancy. One issue is how to integrate external data, e.g.
knowledge graph, for complicated reasoning tasks. Another
issue concerns improvement of inference scheme, such that
more hidden attributes can be inferred. The third topic is to
design an interactive scheme for inference and visual tasks,
thereby achieving better performances.

Acknowledgement

This work was supported in part by National Science
Foundation grant IIS-1619078, IIS-1815561, and the Army
Research Ofﬁce ARO W911NF-16-1-0138.

8364

References

[1] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Deep
compositional question answering with neural module net-
works. CoRR, abs/1511.02799, 2015.

[2] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neu-
ral module networks. 2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), Jun 2016.

[3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zit-
nick, and D. Parikh. VQA: Visual Question Answering. In
International Conference on Computer Vision (ICCV), 2015.
[4] L. P. Cordella, P. Foggia, C. Sansone, and M. Vento. A (sub)
graph isomorphism algorithm for matching large graphs.
TPAMI, 26(10):1367–1372, 2004.

[5] B. Dai, Y. Zhang, and D. Lin. Detecting visual relationships
with deep relational networks. In CVPR, pages 3298–3308.
IEEE Computer Society, 2017.

[6] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian net-
work classiﬁers. Machine learning, 29(2-3):131–163, 1997.
[7] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling
for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016.

[8] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.
Are you talking to a machine? dataset and methods for mul-
tilingual image question. In Advances in neural information
processing systems, pages 2296–2304, 2015.

[9] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.
Are you talking to a machine? dataset and methods for mul-
tilingual image question answering. In NIPS, 2015.

[10] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and
D. Parikh. Making the v in vqa matter: Elevating the role
of image understanding in visual question answering. 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), Jul 2017.

[11] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko.
Learning to reason: End-to-end module networks for visual
question answering. CoRR, abs/1704.05526, 3, 2017.

[12] I. Ilievski, S. Yan, and J. Feng. A focused dynamic
CoRR,

attention model for visual question answering.
abs/1604.01485, 2016.

[13] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L.
Zitnick, and R. Girshick. Clevr: A diagnostic dataset for
compositional language and elementary visual reasoning. In
CVPR, 2017.

[14] D. Koller, N. Friedman, and F. Bach. Probabilistic graphical

models: principles and techniques. MIT press, 2009.

[15] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bern-
stein, and L. Fei-Fei. Visual genome: Connecting language
and vision using crowdsourced dense image annotations. Int.
J. Comput. Vision, 123(1):32–73, May 2017.

[16] Y. Li, W. Ouyang, B. Zhou, K. Wang, and X. Wang. Scene
graph generation from objects, phrases and region captions.
In ICCV, pages 1270–1279. IEEE Computer Society, 2017.
[17] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual re-
lationship detection with language priors. In European Con-
ference on Computer Vision, pages 852–869. Springer, 2016.

[18] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical
question-image co-attention for visual question answering,
2016.

[19] M. Malinowski and M. Fritz. A multi-world approach to
question answering about real-world scenes based on uncer-
tain input. In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances in Neu-
ral Information Processing Systems 27, pages 1682–1690.
Curran Associates, Inc., 2014.

[20] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neu-
rons: A neural-based approach to answering questions about
images. In Proceedings of the IEEE international conference
on computer vision, pages 1–9, 2015.

[21] K. Murphy et al. The bayes net toolbox for matlab. Comput-

ing science and statistics, 33(2):1024–1034, 2001.

[22] F. Petitjean, W. Buntine, G. I. Webb, and N. Zaidi. Accurate
parameter estimation for bayesian network classiﬁers using
hierarchical dirichlet processes. Machine Learning, 107(8-
10):1303–1331, 2018.

[23] J. Pound, A. K. Hudek, I. F. Ilyas, and G. E. Weddell. In-
In

terpreting keyword queries over web knowledge bases.
CIKM, pages 305–314, 2012.

[24] M. Ren, R. Kiros, and R. Zemel. Image question answering:
A visual semantic embedding model and a new dataset. Proc.
Advances in Neural Inf. Process. Syst, 1(2):5, 2015.

[25] M. Ren, R. Kiros, and R. S. Zemel. Exploring models and

data for image question answering. In NIPS, 2015.

[26] Y. Sasaki. The truth of the f-measure. Teach Tutor Mater, 01

2007.

[27] D. Teney, L. Liu, and A. van den Hengel.

Graph-
structured representations for visual question answering.
arXiv preprint, 2017.

[28] Y. Tian and J. M. Patel. TALE: A tool for approximate large

graph matching. In ICDE, pages 963–972, 2008.

[29] J. R. Ullmann. An algorithm for subgraph isomorphism.

JACM, 23(1):31–42, 1976.

[30] A. Wagner, D. T. Tran, G. Ladwig, A. Harth, and R. Studer.
Top-k linked data query processing. In ESWC, pages 56–71,
2012.

[31] H. Xu and K. Saenko. Ask, attend and answer: Exploring
question-guided spatial attention for visual question answer-
ing.
In European Conference on Computer Vision, pages
451–466. Springer, 2016.

[32] T. Yao, Y. Pan, Y. Li, and T. Mei. Exploring visual relation-
ship for image captioning. arXiv preprint arXiv:1809.07041,
2018.

[33] T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei. Boosting image
captioning with attributes. In IEEE International Conference
on Computer Vision, ICCV, pages 22–29, 2017.

[34] W. Yih, M. Chang, X. He, and J. Gao. Semantic parsing
via staged query graph generation: Question answering with
knowledge base. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics, pages
1321–1331, 2015.

[35] W. Zheng, H. Cheng, L. Zou, J. X. Yu, and K. Zhao. Natural
language question/answering: Let users talk with the knowl-
edge graph. In CIKM, pages 217–226, 2017.

8365

[36] B. Zhou, Y. Tian, S. Sukhbaatar, A. Szlam, and R. Fer-
gus. Simple baseline for visual question answering. CoRR,
abs/1512.02167, 2015.

[37] C. Zhu, Y. Zhao, S. Huang, K. Tu, and Y. Ma. Structured
attentions for visual question answering. In Proc. IEEE Int.
Conf. Comp. Vis, volume 3, 2017.

[38] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7w:
Grounded question answering in images.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4995–5004, 2016.

[39] L. Zou, L. Chen, and M. T. ¨Ozsu. Distancejoin: Pattern
match query in a large graph database. PVLDB, 2(1):886–
897, 2009.

8366

