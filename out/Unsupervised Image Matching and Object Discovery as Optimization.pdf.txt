Unsupervised Image Matching and Object Discovery as Optimization

Huy V. Vo1,2,3, Francis Bach1,2, Minsu Cho4, Kai Han5, Yann LeCun6, Patrick P´erez3 and Jean Ponce1,2

1D´epartement d’informatique de l’ENS, ENS, CNRS, PSL University, Paris, France

2INRIA, Paris, France

3Valeo.ai

4POSTECH 5University of Oxford 6New York University

Figure 1: The proposed optimization-based method automatically discovers links between images that depict similar objects. This ﬁgure
shows two image clusters that emerge as a by-product of this approach on the VOC 6x2 object recognition dataset that mixes 6 classes
under two viewpoints. See text for details.

Abstract

Learning with complete or partial supervision is power-
ful but relies on ever-growing human annotation efforts. As
a way to mitigate this serious problem, as well as to serve
speciﬁc applications, unsupervised learning has emerged as
an important ﬁeld of research. In computer vision, unsu-
pervised learning comes in various guises. We focus here
on the unsupervised discovery and matching of object cate-
gories among images in a collection, following the work of
Cho et al. [12]. We show that the original approach can be
reformulated and solved as a proper optimization problem.
Experiments on several benchmarks establish the merit of
our approach.

1. Introduction

can launch the corresponding massive annotation efforts for
speciﬁc projects that may involve millions images [40].

But handling Internet-scale image (or video) repositories
or the continuous learning scenarios associated with per-
sonal assistants or autonomous cars demands approaches
less hungry for manual annotation. Several alternatives
are possible, including weakly supervised approaches that
rely on readily available meta-data [2, 9] or image-level la-
bels [14, 23, 24, 25, 39, 45] instead of more complex anno-
tations such as bounding boxes [17, 38] or object masks [20]
as supervisory signal; semi supervised methods [6, 26] that
exploit a relatively small number of fully annotated pic-
tures, together with a larger set of unlabelled images; and
self supervised algorithms that take advantage of the in-
ternal regularities of image parts [15, 37] or video subse-
quences [1, 34, 48] to construct image models that can be
further ﬁne-tuned in fully supervised settings.

Remarkable progress has been achieved in visual tasks
such as image categorization, object detection, or semantic
segmentation, typically using fully supervised algorithms
and vast amount of manually annotated data (e.g., [17, 20,
21, 27, 29, 38, 40]). With the advent of crowd-sourcing,
large corporations and, to a lesser extent, academic units

We address here the even more challenging problem of
discovering both the structure of image collections – that
is, which images depict similar objects (or textures, scenes,
actions, etc.), and the objects in question, in a fully un-
supervised setting [8, 11, 16, 30, 39, 41, 43]. Although
weakly, semi, and self supervised methods may provide a

8287

more practical foundation for large-scale visual recogni-
tion, the fully unsupervised construction of image models
is a fundamental scientiﬁc problem in computer vision, and
it should be studied. In addition, any reasonable solution to
this problem will facilitate subsequent human labelling (by
presenting discovered groups to the operator) and scaling
through automatic label propagation, help interactive query-
based visual search by linking ahead of time fragments of
potential interest, and provide a way to learn visual models
for subsequent recognition.

1.1. The implicit structure of image collections

Any collection of images, say, those found on the Inter-
net, or more modestly, in a dataset such as Pascal VOC’07,
admits a natural graph representation, where nodes are the
pictures themselves, and edges link pairs of images with
similar visual content.
In supervised image categoriza-
tion (e.g., [27, 29]) or object detection (e.g., [17, 20, 38])
tasks, both the graph structure and the visual content are
clearly deﬁned: Annotators typically sort the images into
bags, each one intended to represent some “object”, “scene”
or, say, “action” class (“horse”, “forest”, “playing tennis”,
etc.). Two nodes are linked by an edge when they are associ-
ated with the same bag, and each class is empirically deﬁned
by the images (or some manually-deﬁned rectangular re-
gions within) in the corresponding connected component of
the graph. In weakly supervised cosegmentation [23, 25, 39]
or colocalization [14, 24, 45] tasks, on the other hand, the
graph is fully connected, and all images are supposed to
contain instances of the (few) same object categories, say,
“horse”, “grass”, “sky”, “background”. Manual interven-
tion is reduced to selecting which images to put into a single
bag, and the visual content, in the form of regions deﬁned
by pixel-level symbolic labels or bounding boxes associated
with one of the predeﬁned categories, is discovered using a
clustering algorithm.1

We address in this paper the much more difﬁcult problem
of fully unsupervised image matching and object discovery,
where both the graph structure and a model of visual con-
tent in the form of object bounding boxes must be extracted
from the native data without any manual intervention. This
problem has been addressed in various forms, e.g., cluster-
ing [16]2, image matching [39] or topic discovery [41, 43]
(see also [8, 11], where “pseudo-object” labels are learned
in an unsupervised manner). In this presentation, we build
directly on the work of Cho et al. [12] (see [28] for related

1In both the cases of supervised image categorization/object detec-
tion and weakly supervised cosegmentation/colocalization, once the graph
structure and the visual content have been identiﬁed at training time, these
can be used to learn a model of the different object classes and add nodes,
edges, and possibly additional bounding boxes at test time.

2Note that plain unsupervised clustering, whether classic, spectral, dis-
criminative or deep [4, 22, 32, 36], focuses on data partitioning and not on
the discovery of subsets of matching items within a cluttered collection.

work): Given an image and its neighbors, assumed to con-
tain the same object, a robust matching technique exploits
both appearance and geometric consistency constraints to
assign conﬁdence and saliency (“stand-out”) scores to re-
gion proposals in this image. The overall discovery algo-
rithm alternates between localization steps where the neigh-
bors are ﬁxed and the regions with top saliency scores are
selected as potential objects, and retrieval steps where the
conﬁdence of the regions within potential objects are used
to ﬁnd the nearest neighbors of each image. After a ﬁxed
number of steps, the region with top saliency in each im-
age is declared to be the object it contains. Empirically,
this method has been shown in [12] to give good results.
However, it does not formulate image matching and object
discovery as a proper optimization problem, and there is no
guarantee that successive iterations will improve some ob-
jective measure of performance. The aim of this paper is to
remedy this situation.

2. Proposed approach

2.1. Problem statement

Let us consider a set of n images, each containing pi
rectangular region proposals, with i in {1 . . . n}. We as-
sume that the images are equipped with some implicit graph
structure, where there is a link between two images when
the second image contains at least one object from a cate-
gory depicted in the ﬁrst one, and our aim is to discover this
structure, that is, ﬁnd the links and the corresponding ob-
jects. To model this problem, let us deﬁne an indicator vari-
able xk
i , whose value is 1 when region number k of image
i corresponds to a “foreground object” (visible in large part
and from a category that occurs multiple times in the image
collection), and 0 otherwise. We collect all the variables
i associated with image i into an element xi of {0, 1}pi ,
xk
and concatenate all the variables xi into an element x of
{0, 1}Pn
i=1 pi . Likewise, let us deﬁne an indicator variable
eij , whose value is 1 if image j contains an object also oc-
curring in image i, with 1 ≤ i, j ≤ n and j 6= i, and 0
otherwise, collect all the variables eij associated with im-
age i into an element ei of {0, 1}n, and concatenate all the
variables ei into an n × n matrix e with rows eT
i . Note that
we can use e to deﬁne a neighborhood for each image in the
set: Image j is a neighbor of the image i if eij = 1. By
deﬁnition, e deﬁnes an undirected graph if e is symmetric
and a directed one otherwise. Let us also denote by Skl
ij the
similarity between regions k and l of images i and j, and by
Sij the pi × pj matrix with entries Skl
ij .

We propose to maximize with respect to x and e the ob-

jective function

n

S(x, e) =

Xi,j=1

eij X1≤k≤pi

j6=i

1≤l≤pj

Skl
ij xk

i xl

j =

xT
i [eijSij]xj. (1)

n

Xi,j=1

j6=i

8288

Intuitively maximizing S(x, e) encourages building edges
between images i and j that contain regions k and l with a
strong similarity Skl
ij . Of course we would like to impose
certain constraints on the x and e variables. The following
cardinality constraints are rather natural:
• An image should not contain more than a prededined
number of objects, say ν,

∀ i ∈ 1 . . . n, xi · 1pi ≤ ν,

(2)

where 1pi is the element of Rpi with all entries equal to one.
• An image should not match more than a predeﬁned num-
ber of other images, say τ ,

∀ i ∈ 1 . . . n, ei · 1n ≤ τ.

(3)

Assumptions. We will suppose from now on that Sij is el-
ementwise nonnegative, but not necessarily symmetric (the
similarity model we explore in Section 3 is asymmetrical).
Likewise, we will assume that the matrix e has a zero diag-
onal but is not necessarily symmetric.

Under these assumptions,

the cubic pseudo-Boolean
function S is supermodular [10]. Without constraints, this
type of functions can be maximized in polynomial time us-
ing a max-ﬂow algorithm [7] (in the case of S(x, e), which
does not involve linear and quadratic terms, the solution is
of course trivial without constraints, and amounts to setting
all xk
i and eij with i 6= j to 1). When the cardinality con-
straints (2-3) are added, this is not the case anymore, and
we have to resort to a gradient ascent algorithm as explained
next.

2.2. Relaxing the problem

Let us ﬁrst note that, for binary variables xk

i , xl

j and eij ,

conditions [44] hold, and we have the following equivalent
primal and dual versions of our problem

(cid:26) max(x,e)∈D inf λ,µ≥0 K(x, e; λ, µ),

minλ,µ≥0 sup(x,e)∈D K(x, e; λ, µ),

(6)

where the domain D is the Cartesian product of [0, 1]Pi pi
and the space of n × n matrices with entries in [0, 1] and a
zero diagonal. With slight abuse we denote it D = [0, 1]N ,

with N = Pi pi + n(n − 1).

2.3. Solving the dual problem

We propose to solve the dual problem with a subgradient
descent approach. Starting from some initial values for λ0
and µ0, we use the update rule

(cid:26) λt+1

i = [λt
µt+1
i = [µt

i + α(xt
i + β(et

i · 1pi − ν)]+,
i · 1n − τ )]+,

(7)

where [·]+ denotes positive part, k ≥ 0, α and β are ﬁxed
step sizes, xt
i · 1n − τ are respectively the
negative of the subgradients of the Lagrangian with respect
to λi and µi in λt

i · 1pi − ν and et

i and µt

i, and

(xt, et) ∈ argmax(x,e)∈[0,1]N K(x, e; λt, µt).

(8)

As shown in Appendix, for ﬁxed values of λ and µ, our
Lagrangian is a supermodular pseudo-Boolean function of
binary variables sets x and e. This allows us to take advan-
tage of the following direct corollary of [3, Prop. 3.7].

Proposition 2.1. Let f denote some supermodular pseudo-
Boolean function of n variables. We have

max

x∈{0,1}n

f (x) = max

x∈[0,1]n

f (x),

(9)

we have

S(x, e) =

n

Xi,j=1

j6=i

X1≤k≤pi

1≤l≤pj

Skl
ij min(eij, xk

i , xl

j),

and the set of maximizers of f (x) in [0, 1]n is the convex
hull of the set of maximizers of f on {0, 1}n.

(4)

In particular, we can take

with Skl
ij ≥ 0. Relaxing our problem so all variables are
allowed to take values in [0, 1], our objective becomes a sum
of concave functions, and thus is itself a concave function,
deﬁned over the convex set (hyperrectangle) [0, 1]N , where
N is the total number of variables. This is the standard tight
concave continuous relaxation of supermodular functions.
The Lagrangian associated with our relaxed problem is

n

K(x, e; λ, µ) = S(x, e)−

[λi(xi·1pi−ν)+µi(ei·1n−τ )],

Xi=1

(5)
where λ = (λ1, . . . , λn)T and µ = (µ1, . . . , µn)T are pos-
itive Lagrange multipliers. The function S(x, e) is concave
and the primal problem is strictly feasible; hence Slater’s

(xt, et) ∈ argmax(x,e)∈{0,1}N K(x, e; λt, µt).

(10)

As shown in [7, 10], the corresponding supermodular cubic
pseudo-Boolean function optimization problem is equiva-
lent to a maximum stable set problem in a bipartite con-
ﬂict graph, which can itself be reduced to a maximum-ﬂow
problem. See Appendix for details.

Note that the size of the min-cut/max-ﬂow problems that
have to be solved is conditioned by the number of nonzero
ij entries, which is upper-bounded by n2p2 when the ma-
Skl
trices Sij are dense (denoting p = max{pi}). This is pro-
hibitively high given that, in practice, p is between 1000 and
4000. To make the computations manageable, we set all but
between 100 and 1000 (depending on the dataset’s size) of
the largest entries in Sij to zero in our implementation.

8289

2.4. Solving the primal problem

Once the dual problem is solved, as argued by Nedi´c &
Ozdaglar [35] and Bach [3], an approximate solution of the
primal problem can be found as a running average of the
primal sequence (xt, et) generated as a by-product of the
sub-gradient method:

ˆx =

1
T

T −1

Xt=0

xt,

ˆe =

1
T

T −1

Xt=0

et

(11)

after some number T of iterations. Note the scalars ˆxk
i and
ˆeij lie in [0, 1] but do not necessarily verify the constraints
(2) and (3). Theoretical guarantees on these values can be
found under additional assumptions in [3, 35].

2.5. Rounding the solution and greedy ascent

Note that

Pj6=i(eijSij + ejiST

two problems remain to be solved: The
solution (ˆx, ˆe) found now belongs to [0, 1]N instead of
{0, 1}N , and it may not satisfy the original constraints.
Note, however, that because of the form of the function
S, given some i in {1, . . . , n} and ﬁxed values for e and
all xj with j 6= i, the maximum value of S given the
constraints is obtained by setting to 1 exactly the ν entries
of xi corresponding to the ν largest entries of the vector
ji)xj . Likewise, for some ﬁxed value
of x, the maximum value of S is reached by setting to 1, for
all i in {1, . . . , n}, exactly the τ entries of ei corresponding
to the τ largest scalars xT
i Sijxj for j 6= i in {1 . . . n}.
This suggests the following approach to rounding up the
solution, where the variables xi are updated sequentially
in an order speciﬁed by some random permutation σ of
{1, . . . , n}, before the variables ei are updated in parallel.
Given the permutation σ, the algorithm below turns the
running average (ˆx, ˆe) of the primal sequence into a dis-
crete solution (x, e) that satisﬁes the conditions (2) and (3):

Initialize x = ˆx, e = ˆe.
For i = 1 to n do

Compute the indices k1 to kν of the ν largest
elements of the vector

Pn
j6=σ(i)(eσ(i)jSσ(i)j + ejσ(i)ST

jσ(i))xj .

xσ(i) ← 0.
For t = 1 to ν do xkt

σ(i) ← 1.

For i = 1 to n do

i Sijxj .

Compute the indices j1 to jτ of the τ largest scalars
xT
ei ← 0.
For t = 1 to τ do eijt ← 1.

Return x, e.

Note that there is no preferred order for the image in-
dices. This actually suggests repeating this procedure with
different random permutations until the variables x and e do
not change anymore or some limit on the number of itera-
tions is reached. This iterative procedure can be seen as a

greedy ascent procedure over the discrete variables of inter-
est. Note that by construction the terms in the left and right
sides of (2) and (3) are equal at the optimum.

2.6. Ensemble post processing

The parameter ν can be seen from two different view-
points: (1) as the maximum number of objects that may be
depicted in an image, or (2) as an upper bound on the total
number of object region candidates that are under consid-
eration in a picture. Both viewpoints are equally valid but,
following Cho et al. [12], we focus in the rest of this pre-
sentation on the second one, and present in this section a
simple heuristic for selecting one ﬁnal object region among
these candidates. Concretely, since using random permuta-
tions during greedy ascent provides a different solution for
each run of our method, we propose to apply an ensemble
method to stabilize the results and boost performance in this
selection process, itself viewed as a post-processing stage
separate from the optimization part.

Let us suppose that after L independent executions of the
greedy ascent step, we obtain L solutions (x(l), e(l)), 1 ≤
l ≤ L. We start by combining these solutions into a single
discrete pair (¯x, ¯e) where ¯x and ¯e satisfy

• ¯xk

i = 1 if ∃ l, 1 ≤ l ≤ L such that xk

i (l) = 1,

• ¯eij = 1 if ∃ l, 1 ≤ l ≤ L such that eij(l) = 1.

This way of combining the individual solutions can be seen
as a max pooling procedure. We have also tried average
pooling but found it less effective. Note that after this in-
termediate step, an image might violate any of the two con-
straints (2-3). This is not a problem in this postprocessing
stage of our method. Indeed, we next show how to use ¯x
and ¯e to select a single object proposal for each image.

We choose a single proposal for each image out of those
retained in ¯x (proposals (i, k) s.t. ¯xk
i = 1). To this end,
we rank the proposals in image i according to a score uk
i
deﬁned for each proposal (i, k) as

uk
i = ¯xk

i Xj∈N (i,k)

Skl
ij ,

max
l|¯xl
j =1

(12)

j =1 Skl

where N (i, k) is composed of the τ images represented by
the 1s in ¯ei which have the largest similarity to (i, k) as
measured by maxl|¯xl
ij . Finally, we choose the pro-
posal in image i with maximum score uk
i as the ﬁnal ob-
ject region. Note that the graph of images corresponding to
these ﬁnal object regions can be retrieved by computing e
that maximizes the objective function given the value of x
deﬁned by these regions as in the greedy ascent. Also, the
method above can be generalized to more than one proposal
per image using the deﬁned ranking.

8290

3. Similarity model

Let us now get back to the deﬁnition of the similarity
function Sij . As advocated by Cho et al. [12], a rectan-
gular region which is a tight ﬁt for a compact object (the
foreground) should better model this object than a larger re-
gion, since it contains less background, or than a smaller
region (a part) since it contains more foreground. Cho et
al. [12] only implement the ﬁrst constraint, in the form of
a stand-out score. We discuss in this section how to imple-
ment these ideas in the optimization context of this work.

3.1. Similarity score

Following [12], the similarity score between proposal k

of image i and proposal l of image j can be deﬁned as

denote the actual rectangular image region associated with
proposal k in image i, and let A(r) denote the area of some
rectangle r. A plausible deﬁnition for P k
i

is

P k
i = {l : A(rk

i ∩ rl

i) > ρA(rl

i)},

(16)

for some reasonable value of ρ, e.g., 0.5. Likewise, a plau-
sible deﬁnition for Bk

i is

Bk

i ∩ rl

i) > δA(rk

i = {l : A(rk

i )},
(17)
for reasonable values of δ and γ, e.g., 0.8 and 2. Follow-
ing [12], we deﬁne the stand-out score of a match (k, l) as

i ) and A(rl

i) > γA(rk

Skl
ij = skl

ij − vkl

ij , where vkl

ij =

max
(k′,l′)∈Bk

i ×Bl

j

sk′l′
ij

.

(18)

skl
ij = akl

ij Xo∈O

g(rk

i , rl

j, o) X1≤k′≤pi

1≤l′≤pj

g(rk′

i , rl′

j , o)ak′l′

ij

,

(13)

With this deﬁnition, Skl
tation, we threshold these scores so they are nonnegative.

ij may be negative. In our implemen-

i and rl

where akl
ij is a similarity term based on appearance alone,
using the WHO descriptor (whiten HOG) [13, 19] in our
case, rk
j denote the image rectangles associated with
the two proposals, o is a discretized offset (translation plus
two scale factors) taking values in O, and g(r, s, o) mea-
sures the geometric compatibility between o and the rect-
angles r and s. Intuitively, skl
ij scales the appearance-only
score akl
ij by a geometric-consistency term akin to a gener-
alized Hough transform [5], see [12] for details.

Note that we can rewrite Eq. (13) as

skl
ij = bkl

ij · cij,

(14)

ij

k′,l′=1 bk′l′

j, o), and cij = Pp

where bkl
is the vector of dimension |O| with entries
ij
akl
ij g(rk
i , rl
. The pipj vectors
bkl
ij and the vector cij can be precomputed with time and
storage cost of O(p2|O|). Each term skl
ij can then be com-
puted in O(|O|) time, and the matrix Sij can thus be com-
puted with a total time and space complexity of O(p2|O|).
ij deﬁned by Eq. (13) depends on
the number of region proposals per images, which may in-
troduce a bias for edges between images that contain many
region proposals. It may thus be desirable to normalize this
score by deﬁning it instead as

Note that the score skl

skl
ij =

1

pipj

bkl
ij · cij.

3.2. Stand out score

Let us identify the region proposals contained in some
image i with their index k, and deﬁne P k
i as the set of re-
gions that are parts of that region (that is, they are included,
with some tolerance, within k). Let us also deﬁne Bk
i as the
set of regions that form the background for k (that is, k is
included, with some tolerance, within these regions). Let rk
i

When Bk

i and Bl
when the regions rk
tation of vkl
simple heuristic that greatly speeds up calculations.

j are large, which is generally the case
i and rl
j are small, a brute-force compu-
ij may be very slow. We propose below instead a

Let Qij denote the set formed by the q matches (k, l)
with highest scores skl
ij , sorted in increasing order, which
can be computed in O(p2 log p). The stand-out scores
can be computed efﬁciently by the following procedure:

ij to 0.

Initialize all vkl
For each match (k′, l′) in Qij do
For each match (k, l) in P k′
For k = 1 to pi and l = 1 to pj do

i × P l′

j do vkl

ij = sk′l′

ij

.

If skl

ij > 0 and vkl

ij = 0 then vkl

ij =

max
(k′,l′)∈Bk

i ×Bl

j

sk′l′
ij

.

this step, O(P(k′,l′)∈Qij

The idea is that relatively few high-conﬁdence matches
(k′, l′) in Qij can be used to efﬁciently compute many
stand-out scores. There is a trade-off between the cost of
j |), and the number of
variables vkl
i ×
P l′
j |). In practice, we have found that taking q = 10, 000
is a good compromise, with only about 5% of the stand-out
scores being computed in a brute-force manner, and a sig-
niﬁcant speed-up factor of over 10.

ij it assigns a value to, O(| ∪(k′,l′)∈Qij P k′

| |P l′

|P k′
i

(15)

4. Experiments and results

Datasets, proposals and metric. For our experiments we
use the same datasets (ObjectDiscovery [OD], VOC 6x2
and VOC all) and region proposals (obtained by the ran-
domized Prim’s algorithm [RP] [33]) as Cho et al. [12]. OD
consists of pictures of three object classes (airplane, horse
and car) with outliers not containing any object instance.
There are 100 images per category, with 18, 7 and 11 out-
liers respectively (containing no object instance). VOC all

8291

Method

Cho et al.

Cho et al., our version

w/o EM

w EM

w/o CO

w CO

w/o CO

w CO

w/o NS
w NS
w/o NS
w NS

w/o NS
w NS
w/o NS
w NS

OD

84.2
84.2

81.9 ± 0.9
83.1 ± 0.8
82.9 ± 0.8
84.4 ± 0.8
84.4 ± 0.0
85.6 ± 0.3
83.8 ± 0.2
85.8 ± 0.6

VOC 6x2

67.7
67.6

65.9 ± 1.0
67.2 ± 1.0
66.6 ± 0.7
68.1 ± 0.9
68.8 ± 0.4
68.7 ± 0.5
67.4 ± 0.4
69.4 ± 0.3

Table 1: Performance of different conﬁgurations of our algorithm
compared to the results of Cho et al. on Object Discovery and
VOC 6x2 datasets in the separate setting.

is a subset of the PASCAL VOC2007 train+val dataset ob-
tained by eliminating all images containing only objects
marked as difﬁcult or truncated. Finally, VOC 6x2 is a sub-
set of VOC all containing only images of 6 classes – aero-
plane, bicycle, boat, bus, horse – and motorbike from two
different views, left and right.

For evaluation, we use the standard CorLoc measure,
the percentage of images correctly localized. It is a proxy
metric in the case of unsupervised discovery. An image
is “correctly localized” when the intersection over union
(IoU ) between one of the ground-truth regions and the pre-
dicted one is greater than 0.5. Following [12], we evaluate
our algorithm in “separate” and “mixed” settings.
In the
former case, the class-wise performance is averaged over
classes.
In the latter, a single performance is computed
over all classes jointly. In our experiments, we use ν = 5,
τ = 10 and standout matrices with 1000 non-zero entries
unless mentioned otherwise.

Separate setting. We ﬁrstly evaluate different settings
of our algorithm on the two smaller datasets, OD and
VOC 6x2. The performance is governed by three design
choices: (1) using the normalized stand-out score (NS) or
its unnormalized version, (2) using continuous optimization
(CO) or variables x and e with all entries equal to one to
initialize the greedy ascent procedure, and (3) using the en-
semble method (EM) or not. In total, we thus have eight
conﬁgurations to test.

The results are shown in Table 1. We have found a small
bug in the publicly available code of Cho et al. [12], and re-
port both the results from [12] and those we obtained after
correction. We observe that the normalized standout score
always gives comparable or better results than its unnormal-
ized counterpart, while the ensemble method also improves
both the score and the stability (lower variance) of our solu-
tion. Combining the normalized standout score, the ensem-
ble method, and the continuous optimization initialization
to greedy ascent yields the best performance. Our best re-
sults outperform [12] by small but statistically signiﬁcant
margins: 1.6% for OD and 1.8% for VOC 6x2. Finally,
to assess the merit of the continuous optimization, we have

Method

Cho et al.

Cho et al., our execution

w/o CO

w CO

w/o EM
w EM

w/o EM
w EM

Li et al. [31]
Wei et al. [49]

VOC all

36.6
37.6

36.4 ± 0.3
39.0 ± 0.2
37.8 ± 0.3
39.2 ± 0.2

40.0
46.9

Table 2: Performance on VOC all in separate setting with differ-
ent conﬁgurations.

measured its duality gap on OD and VOC 6x2: it ranges
from 1.5% to 8.7% of the energy, with an average of 5.2%
and 3.9% on the two datasets respectively.

We now evaluate our algorithm on VOC all. As the com-
plexity of solving the max ﬂow problem grows very fast
with the number of images, for conﬁgurations with contin-
uous optimization, we reduce the number of non-zero en-
tries in each standout matrix such that the total number of
nodes in the graph is around 2 × 107. These standout matri-
ces are then used in rounding the continuous solution, but in
the greedy ascent procedure we switch to standout matrices
with 1000 non-zero entries. For conﬁgurations without the
continuous optimization, we always use the standout matri-
ces with 1000 non-zero entries. Also, to reduce the mem-
ory footprint of our method, we preﬁlter the set of potential
neighbors of each image for the class person that contains
1023 pictures. Pre-ﬁltering is done by marking 100 nearest
neighbors of each image in terms of Euclidean distance be-
tween GIST [46] descriptors as potential neighbors. In the
separate setting, we only apply the pre-ﬁltering on the class
person which has 1023 images. The other classes are sufﬁ-
ciently small for not resorting to the preﬁltering procedure.
Table 2 shows the CorLoc values obtained by our method
with different conﬁgurations compared to Cho et al. It can
be seen that the ensemble postprocessing and the continu-
ous optimization are also helpful on this dataset. We obtain
the best result with the conﬁguration that includes both of
them, which is 1.6% better than Cho et al. However, our
performance is still inferior to state of the art in image colo-
calization [31, 49] which employ deep features from con-
volutional neural networks trained for image classiﬁcation
and explicitly exploits the single-class assumption.

Mixed setting. We now compare in Table 3 the perfor-
mance of our algorithm to Cho et al. in the mixed setting
(none of the other methods is applicable to this case). It can
be seen that our algorithm without the continuous optimiza-
tion has the best performance among those in considera-
tion. Compared to Cho et al., it gives a CorLoc 0.8% better
on OD dataset, 4.3% better on VOC 6x2 and 2.3% better
on VOC all. The decrease in performance of our method
when using the continuous optimization is likely due to the
fact that we use standout matrices with only 200 non-zero
entries on OD, 100 non-zero entries on VOC 6x2 and 100

8292

Method

Cho et al.

Cho et al., our execution

OD

-

82.2

VOC 6x2

VOC all

-

55.9

37.6
37.5

w/o CO
w CO

83.0 ± 0.4
80.8 ± 0.5

60.2 ± 0.4
59.3 ± 0.4

39.8 ± 0.2
38.5 ± 0.2

Table 3: Performance on the datasets in mixed setting.

Method

w/o CO

w CO

w/o CO

w CO

w/o CO

w CO

VOC 6x2
w/o EM 63.5 ± 1.2
w EM 67.7 ± 0.8
w/o EM 65.8 ± 0.8
w EM 68.1 ± 0.7
w/o EM 67.2 ± 1.0
w EM 68.7 ± 0.5
w/o EM 68.1 ± 0.9
w EM 69.4 ± 0.3
w/o EM 68.6 ± 1.0
w EM 69.1 ± 0.3
w/o EM 68.9 ± 0.7
w EM 70.0 ± 0.3

ν = 1

ν = 5

ν = 10

Table 4: Performance of different conﬁgurations of our algorithm
with ν = 1, ν = 5 and ν = 10.

non-zero entries on VOC all (due to the limit on the number
of nodes of the bipartite graphs) in the conﬁguration with
the continuous optimization while we use standout matrices
with 1000 non-zero entries in the conﬁguration without the
continuous optimization.

Sensitivity to ν. We compare the performance of our
method when using different values of ν on the VOC 6x2
dataset.3 Table 4 shows the CorLoc obtained by different
conﬁgurations of our algorithm, all with normalized stand-
out. The performance consistently increases with the value
of ν on this dataset. In all other experiments however, we
set ν = 5 to ease comparisons to [12].

Using deep features. Since activations from deep neural
networks trained for image classiﬁcation (deep features) are
known to be better image representations than handcrafted
features in various tasks, we have also experimented with
such descriptors. We have replaced WHO [19] by activa-
tions from different layers in VGG16 [42], when computing
the appearance similarity between regions. In this case, the
similarity between two regions is simply the scalar prod-
uct of the corresponding deep features (normalized or not).
As a preliminary experiment to evaluate the effectiveness of
deep features, we have run our algorithm without the contin-
uous optimization with the standout score computed using
layers conv4 3, conv5 3 and fc6 in VGG16. Table 5 shows
the results of these experiments. Surprisingly, most of the
deep features tested give worse results than WHO. This may
be due to the fact that our matching task is more akin to im-
age retrieval than classiﬁcation, for which deep features are
typically trained. Among those tested, only a variant of the
features extracted from the layer conv5 3 of VGG16 gives
an improvement (about 2%) compared to the result obtained

3Note that we have also tried the interpretation of ν as the maximum

number of objects per image, without satisfying results so far.

by using WHO.

Features

WHO

conv4 3

conv5 3

warping +

unnormalized

center cropping

normalized

ROI pooling [18]

unnormalized

normalized

warping +

unnormalized

center cropping

normalized

ROI pooling [18]

unnormalized

normalized

fc6

warping +

unnormalized

center cropping

normalized

Average
68.8 ± 0.5
64.2 ± 0.2
57.1 ± 0.6
63.1 ± 0.2
63.4 ± 0.4
64.9 ± 0.2
64.1 ± 0.4
70.7 ± 0.2
68.2 ± 0.3
61.3 ± 0.2
61.0 ± 0.4

Table 5: Performance of our algorithm with deep features on
VOC 6x2 in the separate setting.

Unsupervised initial proposals.
It should be noted that,
although our algorithm like that of Cho et al. [12] is totally
unsupervised once given the region proposals, the random-
ized Prim’s algorithm itself is supervised [33]. To study
the effect of this built-in supervision, we have also tested
the unsupervised selective search algorithm [47] for choos-
ing region proposals. We have conducted experiments on
VOC 6x2 dataset with the three different settings of selec-
tive search (fast, medium and quality). As one might expect,
the fast mode gives the smallest number of proposals and of
positive ones (proposals whose IoU with one ground truth
box is greater than 0.5); the quality mode outputs the largest
set of proposals and of positive ones, the medium mode lies
in-between. To compare with [12], we also run their public
software with each mode of selective search.

Proposal algorithm

Cho et al.

Ours

selective search

fast

medium
quality

randomized Prim’s

23.3
20.6
32.6
67.6

41.4 ± 0.5
48.4 ± 0.5
62.8 ± 0.6
69.4 ± 0.4

Table 6: Object discovery on VOC 6x2 with selective search and
randomized Prim’s as region proposal algorithms.

The results are shown in Table 6.

It can be seen that
the performance of both Cho et al.’s method and ours drop
signiﬁcantly when using selective search. This may be due
to the fact that the percentage of positive proposals found by
selective search is much smaller than that of RP. However,
we see that with the quality mode of selective search, our
method gives results quite close to those of RP, whereas the
method in [12] fails badly. This suggests that our method is
more robust.

Visualization.
In order to gain insight into the structures
discovered by our approach, we derive from its output a
graph of image regions and visualize its main connected
components. The nodes of this graph are the image regions
that have been ﬁnally retained. Two regions (i, k) and (j, l)
are connected if the images containing them are neighbors
in the discovered undirected image graph (eij or eji = 1)

8293

and improving our continuous optimization approach so as
to handle large datasets in a mixed setting, perhaps through
some form of variable clustering.

Appendix: Maximization of supermodular cu-
bic pseudo-Boolean functions

An immediate corollary of [7, Lemma 1] is that a cu-
bic pseudo-Boolean function with nonegative trinary coef-
ﬁcients and no binary terms is supermodular. For ﬁxed λ
and µ, this is obviously the case for the Lagrangian K in
(5).

In addition, the unary terms in K are nonpositive, and
the Langragian can thus be rewritten, up to some constant
additive term, in the form

f (x1, . . . , xn) = Xi∈U

ci ¯xi + X(i,j,k)∈T

cijkxixjxk,

(19)

where ¯xi = 1−xi (the complement of xi), U ⊂ {1, . . . , n},
T ⊂ {1, . . . , n}2, and all coefﬁcients ci and cijk are pos-
itive. We specialize in the rest of this section the general
maximization method of [7] to functions of this form.

The conﬂict graph [7, 10] G(f ) associated with such a
function f has as a set of nodes X(f ) = V ∪ W , where
the elements of V correspond to linear terms, those of W
correspond to cubic terms, and an edge links to nodes when
one of the corresponding terms contains a variable, and the
other one its complement. By construction G(f ) is a bi-
partite graph, with edges joining only elements of V to ele-
ments of W .

As shown in [7] maximizing f amounts to ﬁnding a max-
imum weight stable set in G(f ), where the nodes of V
are assigned weights ci and the nodes of W are assigned
weights cijk, which in turn reduces to computing a maxi-
mum ﬂow between nodes s and t in the network deducted
from G(f ) by (1) adding a source node and edges with up-
per capacity bound ci between s and the corresponding ele-
ments of V ; (2) adding a sink node t and edges with upper
capacity bound cijk between the corresponding elements of
W and t; (3) assigning to all edges (from V to W ) in G(f )
an upper capacity bound of +∞.

Let [A, ¯A] denote the minimum cut obtained by comput-
ing the maximum ﬂow in this graph, where s is an element
of A and t is an element of ¯A = X(f ) \ A. The maximum
weight stable set is then S = (A ∩ V ) ∪ ( ¯A ∩ W ). The
monomials ¯xi and xixjxk associated with elements of S
are set to 1, from which the values of all variables are easily
deduced.

Acknowledgments. This work was supported in part by
the Inria/NYU collaboration agreement, the Louis Vuit-
ton/ENS chair on artiﬁcial intellgence and the EPSRC Pro-
gramme Grant Seebibyte EP/M013774/1. We also thank
Simon Lacoste-Julien for his valuable comments and sug-
gestions.

8294

Figure 2: Visualization of VOC 6x2 in the mixed setting. The
ﬁgure shows the third component in the graph of regions, corre-
sponding roughly to class motorbike. The two ﬁrst components
are shown in Fig.1.

and the standout score between them, Skl
certain threshold.

ij , is greater than a

Choosing the threshold to get a sufﬁcient number of large
enough components for visualization purpose has proven
difﬁcult. We used instead an iterative procedure: the graph
is ﬁrst constructed with a high threshold to produce a small
number of connected components of reasonable size, which
are removed from the graph. On the remaining graph, a
new, suitable threshold is found to get new components of
sufﬁcient size. This is repeated until a target number of
components is reached.

When applied to our results in the mixed setting on
VOC 6x2 dataset, this visualization procedure yields clus-
ters that roughly match object categories. In Figure 1, we
show sub-sampled graphs (for visualization purpose) of the
two ﬁrst components, which roughly correspond to classes
bicycle and aeroplane. The third component is shown in
Figure 2. Although containing also images of other classes,
it is by far dominated by motorbike images. The visual-
ization suggests that our model does extract meaningful se-
mantic structures from the image collections and regions
they contain.

5. Conclusion

We have presented an optimization-based approach to
fully unsupervised image matching and object discovery
and demonstrated its promise on several standard bench-
marks. In its current form, our algorithm is limited to rel-
atively small datasets. We are exploring several paths for
scaling up its performance, including better mechanisms
based on deep features and the PHM algorithm for pre-
ﬁltering image neighbors and selecting regions proposals.
Future work will also be dedicated to developing effective
ensemble methods for discovering multiple objects in im-
ages, further investigating a symmetric version of the pro-
posed approach using an undirected graph, understanding
why deep features do not give better results in our context,

References

[1] P. Agrawal, J. Carreira, and J. Malik. Learning to see

by moving. In ICCV, 2015.

[2] J.-B. Alayrac, P. Bojanowski, N. Agrawal, I. Laptev,
J. Sivic, and S. Lacoste-Julien. Learning from nar-
rated instruction videos.
IEEE Trans. Pattern Anal.
and Machine Intell., 40(9):2194–2208, 2018.

[3] F. Bach. Learning with submodular functions: A con-
vex optimization perspective. Foundations and Trends
in Machine Learning, 6(2-3):145–373, 2013.

[4] F. Bach and Z. Harchaoui. DIFFRAC : a discrimina-
In Proc.

tive and ﬂexible framework for clustering.
Neural Info. Proc. Systems, 2007.

[5] D. Ballard. Generalizing the Hough transform to de-

tect arbitrary shapes. Pattern Recognition, 1981.

[6] M. Belkin, I. Matveeva, and P. Niyogi. Regulariza-
tion and semi-supervised learning on large graphs. In
COLT, 2004.

[7] A. Billionnet and M. Minoux. Maximizing a super-
modular pseudoboolean function: A polynomial algo-
rithm for supermodular cubic functions. Discrete Ap-
plied Mathematics, 12:1–11, 1985.

[8] P. Bojanowski and A. Joulin. Unsupervised learning

by predicting noise. In ICML, 2017.

[9] P. Bojanowski, R. Lajugie, E. Grave, F. Bach,
I. Laptev, J. Ponce, and C. Schmid. Weakly-
supervised alignment of video with text.
In ICCV,
2015.

[10] E. Boros and P. Hammer. Pseudo-Boolean optimiza-
tion. Discrete Applied Mathematics, 123(1-3):155–
225, 2002.

[11] M. Caron, P. Bojanowski, A. Joulin, and M. Douze.
Deep clustering for unsupervised learning of visual
features. In ECCV, 2018.

[12] M. Cho, S. Kwak, C. Schmid, and J. Ponce. Unsu-
pervised object discovery and localization in the wild:
Part-based matching with bottom-up region proposals.
In CVPR, 2015.

[13] N. Dalal and B. Triggs. Histograms of oriented gradi-

ents for human detection. In CVPR, 2005.

[17] P. Felzenszwalb, R. Girshick, D. McAllester, and
D. Ramanan. Object detection with discriminatively
trained part-based models. IEEE Trans. Pattern Anal.
and Machine Intell., 32(9):1627–1645, 2010.

[18] R. Girshick. Fast R-CNN. In ICCV, 2015.

[19] B. Hariharan, J. Malik, and D. Ramanan. Discrimina-
tive decorrelation for clustering and classiﬁcation. In
ECCV, 2012.

[20] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask

R-CNN. In ICCV, 2017.

[21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual

learning for image recognition. In CVPR, 2016.

[22] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe.
Deep clustering: Discriminative embeddings for seg-
mentation and separation. In ICASSP, 2016.

[23] A. Joulin, F. Bach, and J. Ponce. Discriminative clus-

tering for image co-segmentation. In CVPR, 2010.

[24] A. Joulin, K. Tang, and L. Fei-Fei. Efﬁcient image and
video co-localization with Frank-Wolfe algorithm. In
ECCV, 2014.

[25] G. Kim and E. Xing. Distributed cosegmentation via
submodular optimization on anisotropic diffusion. In
ICCV, 2011.

[26] D. P. Kingma, S. Mohamed, D. J. Rezende, and
M. Welling. Semi-supervised learning with deep gen-
erative models. In Proc. Neural Info. Proc. Systems,
2014.

[27] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Im-
agenet classiﬁcation with deep convolutional neural
networks. In NIPS, 2012.

[28] S. Kwak, M. Cho, I. Laptev, J. Ponce, and C. Schmid.
Unsupervised object discovery and tracking in video
collections. In ICCV, 2015.

[29] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags
of features: spatial pyramid matching for recognizing
natural scene categories. In CVPR, 2006.

[30] Y. J. Lee and K. Grauman. Object-graphs for context-

aware category discovery. In CVPR, 2010.

[31] Y. Li, L. Liu, C. Shen, and A. Hengel.

Image co-
localization by mimicking a good detector’s conﬁ-
dence score distribution. In ECCV, 2016.

[14] T. Deselaers, B. Alexe, and V. Ferrari. Localizing ob-
jects while learning their appearance. In ECCV, 2010.

[32] S. Lloyd. Least squares quantization in PCM. IEEE

Trans. on information theory, 28(2):129–137, 1982.

[15] C. Doersch, A. Gupta, and A. Efros. Unsupervised
visual representation learning by context prediction.
In ICCV, 2015.

[33] S. Manen, M. Guillaumin, and L. Van Gool. Prime
object proposals with randomized Prim’s algorithm. In
ICCV, 2013.

[16] A. Faktor and M. Irani. Clustering by composition–
unsupervised discovery of image categories. In ECCV,
2012.

[34] M. Matthieu, C. Couprie, and Y. LeCun. Deep multi-
scale video prediction beyond mean square error. In
ICLR, 2016.

8295

[35] A. Nedi´c and A. Ozdaglar. Approximate primal solu-
tions and rate analysis for dual subgradient methods.
SIAM Journal on Optimization, 19(4), 2009.

[42] K. Simonyan and A. Zisserman. Very deep convo-
lutional networks for large-scale image recognition.
CoRR, abs/1409.1556, 2014.

[36] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral
clustering: Analysis and an algorithm. In NIPS, 2002.

[37] M. Noroozi and P. Favaro. Unsupervised learning of
In

visual representations by solving jigsaw puzzles.
ECCV, 2106.

[38] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-
CNN: Towards real-time object detection with region
proposal networks. In NIPS, 2015.

[43] J. Sivic, B. C. Russell, A. Zisserman, W. T. Freeman,
and A. A. Efros. Unsupervised discovery of visual
object class hierarchies. In CVPR, 2008.

[44] M. Slater. Lagrange multipliers revisited. Cowles

Commission Discussion Paper No. 403, 1950.

[45] K. Tang, A. Joulin, and L.-j. Li. Co-localization in

real-world images. In CVPR, 2014.

[39] M. Rubinstein and A. Joulin. Unsupervised Joint Ob-
ject Discovery and Segmentation in Internet Images.
In CVPR, 2013.

[46] A. Torralba, R. Fergus, and Y. Weiss. Small codes
and large image databases for recognition. In CVPR,
2008.

[40] O. Russakovsky,

J. Deng, H. Su,

J. Krause,
S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. Berg, and L. Fei-Fei. ImageNet large
scale visual recognition challenge.
Int. J. Computer
Vision, 115(3):211–252, 2015.

[41] B. Russell, W. Freeman, A. Efros, J. Sivic, and A. Zis-
serman. Using multiple segmentations to discover ob-
jects and their extent in image collections. In CVPR,
2006.

[47] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers,
and A. W. M. Smeulders. Selective search for object
recognition. IJCV, 2013.

[48] X. Wang and A. Gupta. Unsupervised learning of vi-

sual representations using videos. In ICCV, 2015.

[49] X. Wei, C. Zhang, Y. Li, C. Xie, J. Wu, C. Shen, and
Z. Zhou. Deep descriptor transforming for image co-
localization. In IJCAI, 2017.

8296

