Reliable and Efﬁcient Image Cropping: A Grid Anchor based Approach

Hui Zeng1

Lida Li1
1The Hong Kong Polytechnic University

Zisheng Cao2

Lei Zhang1,3 ∗

2DJI Co.,Ltd

3DAMO Academy, Alibaba Group

{cshzeng, cslli}@comp.polyu.edu.hk, zisheng.cao@dji.com, cslzhang@comp.polyu.edu.hk

Abstract

Image cropping aims to improve the composition as well
as aesthetic quality of an image by removing extraneous
content from it. Existing image cropping databases pro-
vide only one or several human-annotated bounding boxes
as the groundtruth, which cannot reﬂect the non-uniqueness
and ﬂexibility of image cropping in practice. The employed
evaluation metrics such as intersection-over-union cannot
reliably reﬂect the real performance of cropping models, ei-
ther. This work revisits the problem of image cropping, and
presents a grid anchor based formulation by considering
the special properties and requirements (e.g., local redun-
dancy, content preservation, aspect ratio) of image crop-
ping. Our formulation reduces the searching space of can-
didate crops from millions to less than one hundred. Con-
sequently, a grid anchor based cropping benchmark is con-
structed, where all crops of each image are annotated and
more reliable evaluation metrics are deﬁned. We also de-
sign an effective and lightweight network module, which si-
multaneously considers the region of interest and region of
discard for more accurate image cropping. Our model can
stably output visually pleasing crops for images of different
scenes and run at a speed of 125 FPS.

1. Introduction

Cropping is an important and widely used operation to
improve the aesthetic quality of captured images. It aim-
s to remove the extraneous contents of an image, change
its aspect ratio and consequently improve its composition
[37]. Since cropping is a high-frequency need in photog-
raphy but a tedious job when a large number of images are
to be cropped, automatic image cropping has been attracting
much interest in both academia and industry in past decades
[4, 8, 20, 39, 13, 12, 1, 3, 5, 34, 2, 22].

Early researches on image cropping mostly focused on
cropping the major subject or important region of an im-
age for small displays [4, 9] or generating image thumbnails

∗Corresponding author. This work is supported by HK RGC General

Research Fund (PolyU 152135/16E).

Figure 1. The property of non-uniqueness of image cropping. Giv-
en a source image, many good crops (labeled with “√”) can be ob-
tained under different aspect ratios (e.g., 1:1, 4:3, 16:9). Even un-
der the same aspect ratio, there are still multiple acceptable crops.
Regarding the three crops with 16:9 aspect ratio, by taking the
middle one as the groundtruth, the bottom one (a bad crop, labeled
with “×”) will have obviously larger IoU (intersection-over-union)
than the top one but with worse aesthetic quality. This shows that
IoU is not a reliable metric to evaluate cropping quality.

[33, 27]. Attention scores or saliency values were the prin-
cipal concerns of these methods [30, 32]. With little con-
sideration of the overall image composition, the attention-
based methods may lead to visually unpleasing outputs [39].
Moreover, user study was employed as the major criteria to
subjectively evaluate cropping performance, making it very
difﬁcult to objectively compare different methods.

Recently, several benchmark databases have been re-
leased for image cropping [39, 13, 5]. On these databas-
es, one or several bounding boxes were annotated by ex-
perienced human subjects as “groundtruth” crops for each
image. Two objective metrics, namely intersection-over-
union (IoU) and boundary displacement error (BDE) [14],
were deﬁned to evaluate the performance of image crop-
ping models on these databases. These public benchmarks
enable many researchers to develop and test their cropping
models, signiﬁcantly facilitating the research on automatic
image cropping [39, 11, 34, 5, 6, 10, 15, 22, 36].

Though many efforts have been made, there exists sever-
al intractable challenges caused by the special properties of
image cropping. As illustrated in Fig. 1, image cropping
is naturally a subjective and ﬂexible task without unique

15949

Table 1. IoU scores of recent representative works on two bench-
marks in comparison with two simplest baselines. Baseline N sim-
ply calculates the IoU between the groundtruth and source image
without cropping. Baseline C crops the central part whose width
and height are 0.9 time of the source image.

Method

Yan et al. [39]
Chen et al. [5]
Chen et al. [6]
Wang et al. [34]

Li et al. [22]

Baseline N
Baseline C

Set 1
0.7487
0.6683
0.7640
0.8130
0.8019

0.8237
0.7843

ICDB[39]

Set 2
0.7288
0.6618
0.7529
0.8060
0.7961

0.8299
0.7599

Set 3
0.7322
0.6483
0.7333
0.8160
0.7902

0.8079
0.7636

FCDB[5]

–

0.6020
0.6802

–

0.6633

0.6379
0.6647

solution. Good crops can vary signiﬁcantly under differ-
ent requirements of aspect ratio and/or resolution. Even
under certain aspect ratio or resolution constraint, accept-
able crops can also vary. Such a high degree of freedom
makes the existing cropping databases, which have only one
or several annotations, difﬁcult to learn reliable and robust
cropping models.

The commonly employed IoU or BDE metric is unreli-
able to evaluate the performance of image cropping models
either. Referring to the three crops with 16:9 aspect ratio
in Fig. 1, by taking the middle one as the groundtruth, the
bottom one, which is a bad crop, will have obviously larger
IoU than the top one, which is a good crop. Such a problem
can be more clearly observed from Table 1. By using IoU to
evaluate the performance of recent works [39, 34, 5, 6, 22]
on the benchmarks ICDB [39] and FCDB [5], most of them
have even worse performance than the two simplest base-
lines: no cropping (i.e., take the source image as cropping
output, denoted by Baseline N) or central crop (i.e., crop
the central part whose width and height are 0.9 time of the
source image, denoted by Baseline C).

The special properties of image cropping make it a chal-
lenging task to train an effective and efﬁcient cropping mod-
el. On one hand, since the annotation of image cropping
(which requires good knowledge and experience in photog-
raphy) is very expensive [5], existing cropping databases
[39, 13, 5] provide only one or several annotated crops for
about 1,000 source images. On the other hand, the search-
ing space of image cropping is very huge, with millions of
candidate crops for each image. Clearly, the amount of an-
notated data in current databases is insufﬁcient to train a
robust cropping model.

In this work, we reconsider the problem of image crop-
ping and propose a new approach, namely grid anchor based
image cropping, to address this challenging task in a reliable
and efﬁcient manner. Our contributions are threefold.

1). We propose a grid anchor based formulation for image
cropping by considering the special properties and re-
quirements of this problem. Our formulation reduces
the number of candidate crops from millions to less

than one hundred, providing a very efﬁcient solution
for image cropping.

2). Based on our formulation, we construct a new im-
age cropping database with exhaustive annotations for
each source image. With 106,860 annotated candidate
crops, our database provides a good platform to learn
robust image cropping models. More reliable metrics
are also deﬁned to evaluate the performance of learned
cropping models.

3). We design an efﬁcient and effective module for image
cropping under the convolutional neural network (CN-
N) architecture. The learned cropping model runs at a
speed of 125 FPS and obtains promising performance
under various requirements.

2. Related work

The existing image cropping methods can be divided into

three categories according to their major drives.

Attention-driven methods. Earlier methods are mostly
attention-driven, aiming to identify the major subject or the
most informative region of an image. Most of them [4, 33,
32, 27] resort to a saliency detection algorithm (e.g. [19])
to get an attention map of an image, and search a cropping
window with the highest attention value. Some methods
also employ face detection [42] or gaze interaction [30] to
ﬁnd the important region of an image.

Aesthetic-driven methods. The aesthetic-driven meth-
ods improve the attention-based methods by emphasizing
the overall aesthetic quality of images. These methods
[42, 29, 7, 23, 39, 41, 13, 40] usually design a set of hand-
crafted features to characterize the image aesthetic proper-
ties or composition rules. Some methods further deign qual-
ity measures [42, 23] to evaluate the quality of candidate
crops, while some resort to training an aesthetic discrimi-
nator such as SVM [29, 7]. The release of two cropping
databases [39, 13] facilitates the training of discriminative
cropping models. However, the handcrafted features are not
strong enough to accurately predict image aesthetics [11].

Data-driven methods. Most recent methods are data-
driven, which train an end-to-end CNN model for image
cropping. However, limited by the insufﬁcient number of
annotated training samples, many methods in this category
[5, 34, 35, 11, 10, 15, 22] adopt a general aesthetic clas-
siﬁer trained from image aesthetic databases such as AVA
[28] and CUHKPQ [25] to help cropping. However, a gen-
eral aesthetic classiﬁer trained on full images may not be
able to reliably evaluate the crops within one image [6, 36].
An alternative strategy is to use pairwise learning to con-
struct more training data [6, 36] . But annotation of ranking
pairs is also very expensive because of the subjective nature
of image cropping. Recently, Wei et al. [36] constructed a
large scale comparative photo composition (CPC) database

5950

Figure 2. The local redundancy of image cropping. Small local
changes (e.g., shifting and/or scaling) on the cropping window of
an acceptable crop (the bottom-right one) are very likely to output
acceptable crops too.

Figure 3. Illustration of the grid anchor based formulation of image
cropping. M and N are the numbers of bins for grid partition,
while m and n deﬁne the adopted range of anchors for content
preservation.

using an efﬁcient two-stage annotation protocol, which pro-
vides a good training set for pairwise learning. Unfortu-
nately, pairwise learning cannot provide adequate evalua-
tion metrics for image cropping.

3. Grid anchor based image cropping

As illustrated in Fig. 1, image cropping has a high de-
gree of freedom. There is not a unique optimal crop for a
given image. We consider two practical requirements of a
good image cropping system. Firstly, a reliable cropping
system should be able to return acceptable results for dif-
ferent settings (e.g., aspect ratio and resolution) rather than
one single output. Secondly, the cropping system should be
lightweight and efﬁcient to run on resource limited devices.
With these considerations, we propose a grid anchor based
formulation for practical image cropping, and construct a
new benchmark under this formulation.

3.1. Grid anchor based formulation

Given an image with resolution H × W , a candidate
crop can be deﬁned using its top-left corner (x1, y1) and
bottom-right corner (x2, y2), where 1 ≤ x1 < x2 ≤ H and
1 ≤ y1 < y2 ≤ W . It is easy to calculate that the num-
ber of candidate crops is H(H−1)W (W −1)
, which is a huge
number even for an image of size 100 × 100. Fortunate-
ly, by exploiting the following properties and requirements
of image cropping, the searching space can be signiﬁcant-
ly reduced, making automatic image cropping a tractable
problem.

4

Local redundancy: Image cropping is naturally a prob-
lem with local redundancy. As illustrated in Fig. 2, a set of
similar and acceptable crops can be obtained in the neigh-
borhood of a good crop by shifting and/or scaling the crop-

ping widow. Intuitively, we can remove the redundant can-
didate crops by deﬁning crops on image grid anchors rather
than dense pixels. The proposed grid anchor based formu-
lation is illustrated in Fig. 3. We construct an image grid
with M × N bins on the original image, and deﬁne the
corners (x1, y1) and (x2, y2) of one crop on the grid cen-
ters, which serve as the anchors to generate a representative
crop in the neighborhood. Such a formulation largely re-
duces the number of candidate crops from H(H−1)W (W −1)
to M (M −1)N (N −1)

, which can be several orders smaller.

4

4

Content preservation: Generally, a good crop should p-
reserve the major content of the source image [13]. There-
fore, the cropping window should not be too small in order
to avoid discarding too much the image content. To this
end, we constrain the anchor points (x1, y1) and (x2, y2)
of a crop into two regions with m × n bins on the top-left
and bottom-right corners of the source image, respectively,
as illustrated in Fig. 3. This further reduces the number of
crops from M (M −1)N (N −1)

to m2n2.

4

The smallest possible crop (highlighted in red solid lines
in Fig. 3) generated by the proposed scheme covers about
(M −2m+1)(N −2n+1)
grids of the source image, which may
still be too small to preserve enough image content. We thus
further constrain the area of potential crops to be no smaller
than a certain proportion of the whole area of source image:

M N

Scrop ≥ λSImage,

(1)

where Scrop and SImage represent the areas of crop and
original image, and λ ∈ [ (M −2m+1)(N −2n+1)

, 1).

M N

Aspect ratio: Because of the standard resolution of imag-
ing sensors and displays, most people have been accus-
tomed to the popular aspect ratios such as 16:9, 4:3 and
1:1. Candidate crops which have very different aspect ra-
tios may be inconvenient to display and can make people

5951

feel uncomfortable. We thus require the aspect ratio of ac-
ceptable candidate crops satisfy the following condition:

α1 ≤

Wcrop
Hcrop

≤ α2,

(2)

where Wcrop and Hcrop are the width and height of a crop.
α1 and α2 deﬁne the range of aspect ratio and we set them
to 0.5 and 2 to cover most common aspect ratios.

With Eq. 1 and Eq. 2, the ﬁnal number of candidate

crops in each image is less than m2n2.

3.2. Grid anchor based cropping database

4

Our proposed grid anchor based formulation reduces the
number of candidate crops from H(H−1)W (W −1)
to less
than m2n2. This enables us to annotate all the candidate
crops for each image. To make the annotation cost as low
as possible, we ﬁrst made a small scale subjective study to
ﬁnd the smallest {M, N, m, n} that ensure at least 3 accept-
able crops for each image. We collected 100 natural images
and invited ﬁve volunteers to participate in this study. We
set M = N ∈ {16, 14, 12, 10} and m = n ∈ {5, 4, 3} to
reduce possible combinations. λ in Eq.1 was set to 0.5. Af-
ter the tests, we found that M = N = 12 and m = n = 4
can lead to a good balance between cropping quality and
annotation cost. Finally, the number of candidate crops is
successfully reduced to no more than 90 for each image.
Note that the setting of these parameters mainly aims to re-
duce annotation cost for training.
In the testing stage, it
is straightforward to use ﬁner image grid to generate more
candidate crops.

With the above settings, we constructed a Grid Anchor
based Image Cropping Database (GAICD). We ﬁrst crawled
∼50,000 images from the Flickr website. Considering that
many images uploaded to Flickr already have good com-
position, we manually selected 1,000 images whose com-
position can be obviously improved, as well as 236 im-
ages with proper composition to ensure the generality of the
GAICD. The selected images cover a variety of scenes and
lighting conditions. For each image, our annotation toolbox
(please refer to the supplementary ﬁle for details) automati-
cally generates all the candidate crops in ordered aspect ra-
tio. There are 106,860 candidate crops of the 1,236 images
in total. The annotators were required to rate the candidates
at ﬁve scores (from 1 to 5) which represent “bad,” “poor,”
“fair,” “good,” and “excellent”.

A total of 19 annotators passed our test on photography
composition and participated into the annotation. They are
either experienced photographers from photography com-
munities or students from the art department of two uni-
versities. Each crop was annotated by seven different sub-
jects. The mean opinion score (MOS) was calculated for
each candidate crop as its groundtruth quality score. We
found that for 94.25% candidate crops in our database, the

Figure 4. One example source image and several of its annotated
crops in our GAICD. The MOS is marked under each crop.

standard deviations of their rating scores are smaller than 1,
which conﬁrms the annotation consistency under our grid
anchor based formulation. More statistical analyses of our
GAICD are presented in the supplementary ﬁle. Fig. 4
shows one source image and several of its annotated crops
(with MOS scores) in the GAICD.

3.3. Evaluation metrics

The dense annotations of our GAICD enable us to de-
ﬁne more reliable metrics to evaluate cropping performance
than IoU or BDE used in previous databases [39, 13, 5]. We
deﬁne two metrics on GAICD. The ﬁrst one is average S-
pearman’s rank-order correlation coefﬁcient (SRCC). The
SRCC has been widely used to evaluate the rank correlation
between the MOS and model’s predictions in image quality
and aesthetic assessment [21, 26]. Denote by gi the vector
of MOS of all crops for image i, and by pi the predicted
scores of these crops by a model. The SRCC is deﬁned as:

SRCC(gi, pi) = cov(rgi , rpi )/(std(rgi )std(rpi )),

(3)

where rgi and rpi record the ranking order of scores in gi
and pi, and cov(·) and std(·) are the operators of covariance
and standard deviation. The average SRCC is deﬁned as:

SRCC =

1
T X

T

i=1

SRCC(gi, pi),

(4)

where T is the number of testing images.

Considering the fact that users may care more about
whether the returned crops are acceptable or not than the
accurate ranking order of all crops, we deﬁne a new metric,
which we call “return K of top-N accuracy” (AccK/N ), for
practical cropping applications. Denote by Si(N ) the set of
crops whose MOS rank the top-N for image i, and denote
by {ci1, ci2, ..., ciK} the set of K best crops returned by a
cropping model. The AccK/N aims to check how many of
the K returned crops fall into set Si(N ):

AccK/N =

1
T K X

T

i=1 X

K

j=1

T rue(cij ∈ Si(N )),

(5)

where T rue(∗) = 1 if * is true, otherwise T rue(∗) = 0.
In our experiments, we set N to either 5 or 10, and evaluate
K = 1, 2, 3, 4 for both N = 5 and N = 10. We further
average AccK/N over K for each N , leading to two average
accuracy metrics:

AccN =

1
4 X

4

K=1

AccK/N .

(6)

5952

Figure 5. The proposed CNN architecture for image cropping model learning.

4. Cropping model learning

Limited by insufﬁcient training data, most previous crop-
ping methods focused on how to leverage additional aes-
thetic databases [34, 6, 10] or how to construct more train-
ing pairs [5, 36], paying limited attention to how to design
a suitable network for image cropping itself. They usually
adopt the standard CNN architecture widely used in objec-
t detection. Our GAICD provides a better platform with
much more annotated samples for model training. By con-
sidering the special properties of image cropping, we de-
sign an effective and lightweight module for cropping mod-
el learning. The overall architecture is shown in Fig. 5,
which consists of one general feature extraction module and
one image cropping module.

Feature extraction: As in many previous works [34, 11,
5, 6, 10, 15, 22, 36], we truncate one pre-trained CNN mod-
el (e.g., VGG16 [31] or ResNet50 [17]) as the feature ex-
traction module. The spatial arrangement of context and
objects in an image plays a key role in image composition.
For example, the “rule of thirds”, which is the most com-
monly used composition rule, suggests to place importan-
t compositional elements at certain locations of an image
[38]. Therefore, the feature extraction module needs to pre-
serve sufﬁcient spatial resolution for evaluating image com-
position in the following cropping module. Truncating at
shallower layers can preserve higher spatial resolution but
the output feature map may not have enough receptive ﬁeld
to describe large objects in images. We conducted extensive
experiments to decide the most cost-effective layer to trun-
cate two standard CNN models for image cropping. More
details can be found in Sec. 5.2.1.

Modeling both the RoI and RoD: One signiﬁcant dif-
ference between image cropping and object detection is that
object detection only focuses on the region of interest (RoI),
while cropping also needs to consider the discarded infor-
mation (hereafter we call it region of discard (RoD)). On
one hand, removing distracting information can signiﬁcant-
ly improve the composition. On the other hand, cropping
out important region can dramatically change or even de-
stroy an image. Taking the second last crop in Fig. 4 as
an example, although it may have acceptable composition

but its visual quality is much lower than the source image
because the beautiful sunset glow is cropped out. The dis-
carded information is unavailable to the cropping model if
only the RoI is considered, while modeling the RoD can
effectively solve this problem.

Referring to Fig. 5, let F denote the whole feature map
output by the feature extraction module, and the feature
maps in RoI and RoD are denoted by FRoI and FRoD, re-
spectively. We ﬁrst employ the RoIAlign [16] to transform
FRoI into F A
RoI which has ﬁxed spatial resolution s × s.
The FRoD is constructed by removing FRoI from F , name-
ly, setting the values of FRoI to zeros in F . Then the Ro-
DAlign (using the same bilinear interpolation as RoIAlign)
is performed on FRoD, leading to F A
RoD which has the same
spatial resolution as F A
RoD are concatenat-
ed along the channel dimension as one aligned feature map
which contains the information in both RoI and RoD. The
combined feature map is fed into two fully connected layers
for ﬁnal MOS prediction.

RoI and F A

RoI . F A

Reducing the channel dimension: Another difference
between image cropping and object detection is that the for-
mer does not need to accurately recognize the category of
different objects, which allows us to signiﬁcantly reduce the
channel dimension of the feature map. In practice, we ﬁnd
that the channel dimension of the feature map (output by the
VGG16 model) can be reduced from 512 to 8 using 1 × 1
convolution without sacriﬁcing much the performance. The
low channel dimension makes our image cropping module
very efﬁcient and lightweight. More details can be found in
Sec. 5.2.1.

Loss function: Denote by eij = gij − pij , where gij
and pij are the groundtruth MOS and predicted score of the
j-th crop for image i. The Huber loss [18] is employed as
the loss function to learn our cropping model because of its
robustness to outliers:

Lij =




1
2
δ|eij| −

e2
ij , when |eij| ≤ δ,

δ2, otherwise,

1
2

(7)

where δ is ﬁxed at 1 throughout our experiments.

5953

Table 2. Image cropping performance by using different feature
extraction modules. The truncating layer (tlayer), stride (str), re-
ceptive ﬁeld (rf) and parameter size (par (Mbit)) of the feature ex-
traction module are shown for each case.

model

vgg16

resnet50

tlayer
c4 1
c4 3
c5 1
c5 3
pool5
c3 2
c3 4
c4 3
c4 6
c5 1

str
8
8
16
16
32
8
8
16
16
32

rf
60
92
132
192
212
67
99
195
291
355

par
11.1
29.1
38.1
56.1
56.1
3.4
5.6
19.9
32.7
55.8

SRCC Acc5 Acc10
58.3
61.8
65.5
65.6
61.9
50.8
52.9
60.8
61.2
58.3

0.695
0.715
0.735
0.737
0.702
0.620
0.647
0.709
0.712
0.692

40.1
42.5
46.6
47.0
43.6
33.1
35.1
41.8
42.1
40.6

Table 3. Ablation experiments on the RoI and RoD.

module

RoD
RoI

RoI+RoD

SRCC Acc5 Acc10
43.4
62.9
65.5

0.597
0.706
0.735

29.8
44.8
46.6

Table 4. Image cropping performance by using different spatial
resolution (s × s) and channel dimension (cdim). The number
of ﬁlters (nﬁlter) is ﬁxed as 512 in the FC layers. The VGG16
model (truncated at conv5 1) is employed as the feature extraction
module for all cases. The parameter size (par (Mbit)) of the image
cropping module (including two FC layers with s×s×(2∗cdim)×
512 and 1 × 1 × 512 × 512 kernels) is reported for each case.

s × s
3×3
5×5
7×7
9×9

11×11

9×9
9×9
9×9
9×9
9×9
9×9

cdim nﬁlter
512
512
512
512
512
512
512
512
512
512
512

8
8
8
8
8
32
16
8
4
2
1

par
1.28
1.78
2.53
3.53
4.78
11.13
6.06
3.53
2.27
1.63
1.32

SRCC Acc5 Acc10
58.9
61.5
63.1
65.5
65.6
65.3
65.8
65.5
65.1
64.1
62.6

0.689
0.711
0.725
0.735
0.736
0.733
0.736
0.735
0.731
0.719
0.706

42.4
44.6
45.4
46.6
46.8
46.4
46.8
46.6
45.9
45.1
43.8

5. Experiments

5.1. Implementation details

We randomly selected 200 images from our GAICD as
the testing set and used the remaining 1,036 images (con-
taining 89,519 annotated crops in total) for training and val-
idation. In the training stage, our model takes one image
and 64 randomly selected crops of it as a batch to input. In
the testing stage, the trained model evaluates all the gener-
ated crops of one image and outputs a predicted MOS for
each crop. To improve the training and testing efﬁciency,
the short side of input images is resized to 256. The stan-
dard ADAM optimizer with the default parameters was em-
ployed to train our model for 40 epoches. Learning rate was
ﬁxed at 1e−4 throughout our experiments. We randomly ad-
justed the contrast and saturation of the source images for
data augmentation in the training stage. The MOS were nor-
malized by removing the mean and dividing by the standard
deviation across the training set.

5.2. Ablation study of our cropping model

5.2.1 Feature extraction module

We ﬁrst conduct a set of experiments to determine the ap-
propriate feature extraction module on two pre-trained mod-
els (VGG16 [31] and ResNet50 [17]). For each model, we
truncated at ﬁve different layers, which cover various strides
and receptive ﬁelds, and evaluated their effects on cropping
performance. The image cropping module (including both
the RoI and RoD) was ﬁxed for all cases. The truncating
layer, stride, receptive ﬁeld, parameter size and cropping
performance for each module are reported in Table 2. To
save space, we do not report each single accuracy index in
the ablation study.

We can make three observations from Table 2. First,
for both the VGG16 and ResNet50 models, a too small re-
ceptive ﬁeld in the feature extraction module will lead to
unsatisﬁed performance. Increasing the receptive ﬁeld can
signiﬁcantly improve the cropping accuracy at the cost of
deeper architecture and more parameters. The performance
plateaus when the receptive ﬁeld is increased to more than
half of the image size.
It is worth noting that the above
observations on stride and receptive ﬁeld are based on cer-
tain input image size (short side equals to 256 in our ex-
periments), which may provide good reference for other in-
put size. Second, a too large stride (e.g., 32) deteriorates
the performance, either. This is because downsampling too
much the feature map will lose important spatial informa-
tion for image cropping. Speciﬁcally, for the input image
of resolution 256 × 256, downsampling with stride 32 will
result in feature maps of size 8 × 8, and consequently the
feature map of a candidate crop may only have a spatial res-
olution of 4 × 4, which is insufﬁcient to generate accurate
crops. Finally, the VGG16 models generally outperforms
the ResNet50 models. This may be because the ResNet50
models can be overﬁtted on our database. We thus choose
the VGG16 model (truncated at conv5 1 layer) as the fea-
ture extraction module in the following experiments.

5.2.2

Image cropping module

We then evaluate the proposed image cropping module, in-
cluding the effects of parameter size, RoI and RoD.

Parameter size: There are two key parameters in the
image cropping module: spatial resolution (s × s) of the
aligned feature map and channel dimension (cdim) after
dimension reduction. Table 4 reports the cropping perfor-
mance of using different s × s and cdim. The number of
ﬁlters was ﬁxed at 512 for the FC layers. We ﬁrst found
that a smaller s (e.g. 3 or 5) would result in obviously

5954

release the source code or executable program. We thus
compare our method, namely Grid Anchor based Image
Cropping (GAIC), with the following baseline and recently
developed state-of-the-art methods whose source codes are
available.

Baseline L: The baseline L does not need any training.
It simply outputs the largest crop among all eligible candi-
dates. The result is similar to the “baseline N” mentioned
in Table 1, i.e., the source image without cropping.

VFN [6]: The View Finding Network (VFN) is trained in
a pair-wise ranking manner using professional photograph-
s crawled from the Flickr. High-quality photos were ﬁrst
manually selected, and a set of crops were then generated
from each image. The ranking pairs were constructed by al-
ways assuming that the source image has better quality than
the generated crops.

VEN and VPN [36]: Compared with VFN, the View
Evaluation Network (VEN) employs more reliable ranking
pairs to train the model. Speciﬁcally, the authors annotated
more than 1 million ranking pairs using a two-stage anno-
tation strategy. A more efﬁcient View Proposal Network
(VPN) was proposed in the same work, and it was trained
using the predictions of VEN. The VPN is based on the de-
tection model SSD [24], and it outputs a prediction vector
for 895 predeﬁned boxes.

A2-RL [22]: The A2RL is trained in an iterative opti-
mization manner. The model adjusts the cropping window
and calculates a reward (based on predicted aesthetic score)
for each step. The iteration stops when the accumulated re-
ward satisﬁes some termination criteria.

5.3.2 Qualitative comparison

To demonstrate the advantages of our cropping method over
previous ones, we ﬁrst conduct qualitative comparison of
different methods on four typical scenes: single object,
multi-objects, building and landscape. Note that these im-
ages are out of any existing cropping databases. In the ﬁrst
set of comparison, we compare all methods under the set-
ting of returning only one best crop. Each model uses its de-
fault candidate crops generated by its source code except for
VFN, which does not provide such code and uses the same
candidates as our method. The results are shown in Fig. 6.
We can make several interesting observations. Both VFN
and A2-RL fail to robustly remove distracting elements in
images. VFN cuts some important content, while A2-RL
simply returns the source image in many cases. VEN and
our GAIC model can stably output visually pleasing crops.
The major differences lie in that VEN prefers more close-
up crops while our GAIC tends to preserve as much useful
information as possible.

A ﬂexible cropping system should be able to output ac-
ceptable results under different requirements in practice,
e.g., different aspect ratios. In Fig. 7, we show the cropping

5955

Figure 6. Qualitative comparison of returned top-1 crop by differ-
ent methods.

worse performance. This again proves the importance of
sufﬁcient spatial information for image cropping. s = 9
seems to be an appropriate choice since further increasing
the value does not bring obvious improvements. The chan-
nel dimension of feature maps can be signiﬁcantly reduced
for the problem of image cropping. As can be seen from
Table 4, the performance is still reasonable even if we re-
duce the channel dimension to 1 (note that VGG16 output
512 channels of feature maps). The low channel dimension
makes the proposed image cropping module efﬁcient and
lightweight. In the following experiments, we chose 8 as
the reduced channel dimension which has a good trade-off
between cost and efﬁcacy. Under this setting, the whole im-
age cropping module has only 3.53 Mbits parameters.

RoI and RoD: We make an ablation study on the role
of RoI and RoD. The results of using only RoI, only RoD
and both of them are reported in Table 3. As can be seen,
modeling only the RoD results in very poor accuracy, mod-
eling only the RoI performs much better, while modeling
simultaneously the RoI and RoD achieves the best cropping
accuracy in all cases. This corroborates our analysis that
image cropping needs to consider both the RoI and RoD.

5.3. Comparison to other methods

As discussed in the introduction section, the limitations
of existing image cropping databases and evaluation metrics
make the learning and evaluation of reliable cropping mod-
els difﬁcult. Nonetheless, we still evaluated our model on
the previous databases [39, 5], and the results can be found
in the supplementary ﬁle. Here we report the experimental
results on the proposed GAICD.

5.3.1 Comparison methods

Though a number of image cropping methods have been de-
veloped [34, 11, 5, 6, 10, 15, 22, 36], many of them do not

Figure 7. Qualitative comparison of returning crops with different aspect ratios by different methods.

Table 5. Quantitative comparison between different methods on the GAICD. “–” means that result is not available.

Method

Baseline L
A2-RL [22]

VPN[36]
VFN[6]
VEN[36]

GAIC (ours)

SRCC Acc1/5 Acc2/5 Acc3/5 Acc4/5 Acc5 Acc1/10 Acc2/10 Acc3/10 Acc4/10 Acc10

–
–
–

0.450
0.621
0.735

24.5
23.0
40.0
27.0
40.5
53.5

–
–
–

30.0
37.5
47.0

–
–
–

26.0
38.5
44.5

–
–
–

17.5
36.5
41.5

–
–
–

25.1
38.1
46.6

41.0
38.5
49.5
39.0
54.0
71.5

–
–
–

40.5
51.5
66.0

–
–
–

39.0
50.5
66.5

–
–
–

31.5
47.0
58.0

–
–
–

37.5
50.8
65.5

FPS

–
4
75
0.5
0.2
125

results by the competing methods under three most com-
monly used aspect ratios: 16:9, 4:3 and 1:1. The A2-RL is
not included because it does not support this test. Again,
our model outputs the most visually pleasing crop in most
cases. More results can be found in supplementary ﬁle.

5.3.3 Quantitative comparison

We then perform quantitative comparisons by using the
metrics deﬁned in Section 3.3. Among the competitors,
VFN, VEN and our GAIC support predicting scores for al-
l the candidate crops provided by our database, thus they
can be quantitatively evaluated by all the deﬁned evalua-
tion metrics. VPN uses its own pre-deﬁned cropping boxes
which are different from our database, and Baseline L and
A2-RL output only one single crop. Therefore, we can only
calculate Acc1/5 and Acc1/10 for them. We approximate the
output boxes by VPN and A2-RL to the nearest anchor box
in our database when calculating the quantitative indexes.

The results of all competing methods are shown in Ta-
ble 5. We can see that both A2-RL and VFN only obtain
comparable performance to Baseline L. This is mainly be-
cause A2-RL is supervised by a general aesthetic classiﬁer
in training, and the ranking pairs used in VFN are not very
reliable. By using more reliable ranking pairs, VEN obtains
much better performance than VFN. VPN performs slight-
ly worse than VEN as expected because it is supervised by
the predictions of VEN. Our method outperforms VEN by
a large margin, which owes to the richer cropping informa-
tion leveraged by our annotation approach compared to the
pair-wise ranking annotations used by VEN, as well as the

more effective cropping module training of our model.

5.3.4 Running speed

A practical image cropping model should also have fast
speed for real-time implementation. In the last column of
Table 5, we compare the running speed in terms of frame-
per-second (FPS) for all competing methods. All models
are run on the same PC with i7-6800K CPU, 64G RAM and
one GTX 1080Ti GPU. As can be seen, our GAIC model
runs at 125 FPS, which is much faster than all the competi-
tors. It is worth mentioning that both GAIC and VPN are
based on VGG16 architecture, but GAIC has much less pa-
rameters than VPN (40 Mbits vs. 290 Mbits). The other
methods are much slower because A2-RL needs to iterate
the cropping window while VFN and VEN need to individ-
ually process each crop.

6. Conclusion

We analyzed the limitations of existing formulation and
databases on image cropping. Consequently, we proposed
a more reliable and efﬁcient formulation for practical im-
age cropping, namely grid anchor based image cropping
(GAIC). A new benchmark was constructed, which contain-
s 1,236 source images and 106,860 annotated crops, as well
as two types of reliable evaluation metrics. We further pro-
posed a lightweight and effective cropping module under
the CNN architecture. Our GAIC can robustly output visu-
ally pleasing crops under different aspect ratios and it runs
at a speed of 125FPS, much faster than other methods.

5956

[24] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and
A. C. Berg. SSD: Single shot multibox detector. In ECCV, pages
21–37. Springer, 2016. 7

[25] W. Luo, X. Wang, and X. Tang. Content-based photo quality assess-

ment. In ICCV, pages 2206–2213, 2011. 2

[26] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and L. Zhang.
Waterloo exploration database: New challenges for image quali-
ty assessment models.
IEEE Transactions on Image Processing,
26(2):1004–1016, 2017. 4

[27] L. Marchesotti, C. Cifarelli, and G. Csurka. A framework for visual
saliency detection with applications to image thumbnailing. In ICCV,
pages 2232–2239, 2009. 1, 2

[28] N. Murray, L. Marchesotti, and F. Perronnin. AVA: A large-scale
database for aesthetic visual analysis. In CVPR, pages 2408–2415,
2012. 2

[29] M. Nishiyama, T. Okabe, Y. Sato, and I. Sato. Sensation-based photo

cropping. In ACM Multimedia, pages 669–672, 2009. 2

[30] A. Santella, M. Agrawala, D. DeCarlo, D. Salesin, and M. Cohen.
Gaze-based interaction for semi-automatic photo cropping. In ACM
SIGCHI, pages 771–780, 2006. 1, 2

[31] K. Simonyan and A. Zisserman. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556,
2014. 5, 6

[32] F. Stentiford. Attention based auto image cropping. In ICVS Work-

shop on Computation Attention & Applications, 2007. 1, 2

[33] B. Suh, H. Ling, B. B. Bederson, and D. W. Jacobs. Automatic
In ACM symposium on

thumbnail cropping and its effectiveness.
User interface software and technology, pages 95–104, 2003. 1, 2

[34] W. Wang and J. Shen. Deep cropping via attention box prediction

and aesthetics assessment. In ICCV, 2017. 1, 2, 5, 7

[35] W. Wang, J. Shen, and H. Ling. A deep network solution for attention
and aesthetics aware photo cropping. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2018. 2

[36] Z. Wei, J. Zhang, X. Shen, Z. Lin, R. Mech, M. Hoai, and D. Sama-
ras. Good view hunting: Learning photo composition from dense
view pairs. In CVPR, pages 5437–5446, 2018. 1, 2, 5, 7, 8

[37] Wikipedia contributors. Cropping (image) — Wikipedia, the free
https://en.wikipedia.org/w/index.

encyclopedia.
php?title=Cropping_(image)&oldid=847382681,
2018. [Online; accessed 10-July-2018]. 1

[38] Wikipedia contributors. Rule of thirds — Wikipedia, the free en-
cyclopedia. https://en.wikipedia.org/w/index.php?
title=Rule_of_thirds&oldid=852178012, 2018.
[On-
line; accessed 31-July-2018]. 5

[39] J. Yan, S. Lin, S. Bing Kang, and X. Tang. Learning the change for
automatic image cropping. In CVPR, pages 971–978, 2013. 1, 2, 4,
7

[40] L. Zhang, M. Song, Y. Yang, Q. Zhao, C. Zhao, and N. Sebe. Weak-
IEEE Transactions on Multimedia,

ly supervised photo cropping.
16(1):94–107, 2014. 2

[41] L. Zhang, M. Song, Q. Zhao, X. Liu, J. Bu, and C. Chen. Probabilis-
tic graphlet transfer for photo cropping. IEEE Transactions on Image
Processing, 22(2):802–815, 2013. 2

[42] M. Zhang, L. Zhang, Y. Sun, L. Feng, and W. Ma. Auto cropping for

digital photographs. In ICME, 2005. 2

References

[1] N. Bhatt and T. Cherna. Multifunctional environment for image crop-

ping, Oct. 13 2015. US Patent 9,158,455. 1

[2] C. S. B. Chedeau. Image cropping according to points of interest,

Mar. 28 2017. US Patent 9,607,235. 1

[3] J. Chen, G. Bai, S. Liang, and Z. Li. Automatic image cropping: A
computational complexity study. In CVPR, pages 507–515, 2016. 1
[4] L.-Q. Chen, X. Xie, X. Fan, W.-Y. Ma, H.-J. Zhang, and H.-Q. Zhou.
A visual attention model for adapting images on small displays. Mul-
timedia systems, 9(4):353–364, 2003. 1, 2

[5] Y.-L. Chen, T.-W. Huang, K.-H. Chang, Y.-C. Tsai, H.-T. Chen, and
B.-Y. Chen. Quantitative analysis of automatic image cropping algo-
rithms: A dataset and comparative study. In WACV, pages 226–234,
2017. 1, 2, 4, 5, 7

[6] Y.-L. Chen, J. Klopp, M. Sun, S.-Y. Chien, and K.-L. Ma. Learn-
ing to compose with professional photographs on the web. In ACM
Multimedia, pages 37–45, 2017. 1, 2, 5, 7, 8

[7] B. Cheng, B. Ni, S. Yan, and Q. Tian. Learning to photograph. In

ACM Multimedia, pages 291–300, 2010. 2

[8] A. Chor, J. Schwartz, P. Hellyar, T. Kasperkiewicz, and D. Par-
lin. System for automatic image cropping based on image saliency,
Apr. 6 2006. US Patent App. 10/956,628. 1

[9] G. Ciocca, C. Cusano, F. Gasparini, and R. Schettini. Self-adaptive
image cropping for small displays. IEEE Transactions on Consumer
Electronics, 53(4), 2007. 1

[10] Y. Deng, C. C. Loy, and X. Tang. Aesthetic-driven image enhance-
arXiv preprint arXiv:1707.05251,

ment by adversarial learning.
2017. 1, 2, 5, 7

[11] Y. Deng, C. C. Loy, and X. Tang. Image aesthetic assessment: An
experimental survey. IEEE Signal Processing Magazine, 34(4):80–
106, 2017. 1, 2, 5, 7

[12] E. O. Downing, O. M. Koenders, and B. T. Grover. Automated image
cropping to include particular subjects, Apr. 28 2015. US Patent
9,020,298. 1

[13] C. Fang, Z. Lin, R. Mech, and X. Shen. Automatic image cropping
using visual composition, boundary simplicity and content preserva-
tion models. In ACM Multimedia, pages 1105–1108, 2014. 1, 2, 3,
4

[14] J. Freixenet, X. Mu˜noz, D. Raba, J. Mart´ı, and X. Cuf´ı. Yet another
survey on image segmentation: Region and boundary information
integration. In ECCV, pages 408–422, 2002. 1

[15] G. Guo, H. Wang, C. Shen, Y. Yan, and H.-Y. M. Liao. Automatic
image cropping for visual aesthetic enhancement using deep neural
networks and cascaded regression. arXiv preprint arXiv:1712.09048,
2017. 1, 2, 5, 7

[16] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask R-CNN. In

ICCV, pages 2980–2988. IEEE, 2017. 5

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for

image recognition. In CVPR, pages 770–778, 2016. 5, 6

[18] P. J. Huber et al. Robust estimation of a location parameter. The

annals of mathematical statistics, 35(1):73–101, 1964. 5

[19] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visu-
al attention for rapid scene analysis. IEEE Transactions on pattern
analysis and machine intelligence, 20(11):1254–1259, 1998. 2

[20] N. Jogo. Image cropping and synthesizing method, and imaging ap-

paratus, Apr. 24 2007. US Patent 7,209,149. 1

[21] S. Kong, X. Shen, Z. Lin, R. Mech, and C. Fowlkes. Photo aesthetics
In ECCV,

ranking network with attributes and content adaptation.
pages 662–679. Springer, 2016. 4

[22] D. Li, H. Wu, J. Zhang, and K. Huang. A2-RL: Aesthetics aware
reinforcement learning for image cropping. In CVPR, pages 8193–
8201, 2018. 1, 2, 5, 7, 8

[23] L. Liu, R. Chen, L. Wolf, and D. Cohen-Or. Optimizing photo com-
position. In Computer Graphics Forum, volume 29, pages 469–478,
2010. 2

5957

