Transferable Interactiveness Knowledge for

Human-Object Interaction Detection

Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu∗

Shanghai Jiao Tong University

{yonglu li, ssluvble, otaku huang, liangxu, maze1234556}@sjtu.edu.cn

fhaoshu@gmail.com, wangyanfeng@sjtu.edu.cn, lucewu@sjtu.edu.cn

Abstract

Human-Object Interaction (HOI) Detection is an impor-
tant problem to understand how humans interact with ob-
In this paper, we explore Interactiveness Knowl-
jects.
edge which indicates whether human and object inter-
act with each other or not. We found that interactive-
ness knowledge can be learned across HOI datasets, re-
gardless of HOI category settings. Our core idea is
to exploit an Interactiveness Network to learn the gen-
eral interactiveness knowledge from multiple HOI datasets
and perform Non-Interaction Suppression before HOI clas-
siﬁcation in inference. On account of the generaliza-
tion of interactiveness, interactiveness network is a trans-
ferable knowledge learner and can be cooperated with
any HOI detection models to achieve desirable results.
We extensively evaluate the proposed method on HICO-
DET and V-COCO datasets. Our framework outperforms
state-of-the-art HOI detection results by a great mar-
gin, verifying its efﬁcacy and ﬂexibility. Code is avail-
able at https://github.com/DirtyHarryLYL/
Transferable-Interactiveness-Network.

1. Introduction

Human-Object Interaction (HOI) detection retrieves hu-
man and object locations and infers the interaction classes
from still image. As a sub-task of visual relationship [24,
17], HOI is strongly related to the human body and ob-
ject understanding [33, 36, 39, 11, 26, 21, 38]. It is crucial
for behavior understanding and can facilitate activity under-
standing [9, 28], imitation learning [3], etc. Recently, im-
pressive progress has been made by utilizing Deep Neural
Networks (DNNs) in this area [34, 19, 32, 31].

∗Cewu Lu is the corresponding author, he is also a member of Depart-
ment of Computer Science and Engineering, Shanghai Jiao Tong Univer-
sity, MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao
Tong University, and SJTU-SenseTime AI lab.

(a)
(b)

HOI Detection 

Model

Interactiveness

HOIs

Interactiveness Prior Learning

Interactiveness

Network

HOI Classifier

HOIs

Multiple HOI Datasets

Figure 1. Interactiveness Knowledge Learning. (a) HOI datasets
contain implicit interactiveness knowledge. We can learn it better
by performing explicit interactiveness discrimination, and utilize
it to improve the HOI detection performance. (b) Interactiveness
knowledge is beyond the HOI categories and can be learned across
datasets, which can bring greater performance improvement.

Generally, human and objects need to be detected ﬁrst.
Given an image and its detections, human and objects are
often paired exhaustively [19, 31, 32]. HOI detection task
aims to classify these pairs as different HOI categories. Pre-
vious one-stage methods [34, 19, 31, 13, 32] directly clas-
sify a pair as speciﬁc HOIs. These methods actually predict
interactiveness implicitly at the same time, where interac-
tiveness indicates whether a human-object pair is interac-
tive. For example, when a pair is classiﬁed as HOI “eat
apple”, we can implicitly predict that it is interactive.

Though interactiveness is an essential element for HOI
detection, we neglected to study how to utilize it and im-
prove its learning.
In comparison to HOI categories, in-
teractiveness conveys more basic information. Such at-
tribute makes it easier for interactiveness to transfer across
datasets.Based on this inspiration, we propose a Interac-
tiveness Knowledge learning method as seen in Figure 1.
With our framework, interactiveness can be learned across
datasets and applied to any speciﬁc dataset. By utilizing
interactiveness, we take two stages to identify HOIs: we
ﬁrst discriminate a human-object pair as interactive or not

3585

and then classify it as speciﬁc HOIs. Compared to previous
one-stage method [34, 19, 31, 13, 32], we take advantage of
powerful interactiveness knowledge that incorporates more
information from other datasets. Thus our method can de-
crease the false positives signiﬁcantly. Additionally, after
the interactiveness ﬁltering in the ﬁrst stage, we do not need
to handle a large number of non-interactive pairs which are
overwhelmingly more than interactive ones.

In this paper, we proposed a novel two-stage method to
classify pairs hierarchically as shown in Figure 2. We in-
troduce an interactiveness network which can be combined
with any HOI detection model. We set a hierarchical logical
strategy: by utilizing binary interactiveness labels, interac-
tiveness network will bring in a strong supervised constraint
which reﬁnes the framework in training and learns the in-
teractiveness from multiple datasets. In testing, interactive-
ness network performs Non-Interaction Suppression (NIS)
ﬁrst. Then the HOI detection model will classify the re-
maining pairs as speciﬁc HOIs, where non-interactive pairs
have been decreased signiﬁcantly. Moreover, if the model
classiﬁes a pair as speciﬁc HOIs, it should ﬁgure out that
the pair is interactive simultaneously. Such two-stage pre-
diction will alleviate the learning difﬁculty and bring in hi-
erarchical predictions. For special attention, interactiveness
offers extra information to help HOI classiﬁcation and is
independent of HOI category settings. That means it can
be transferred across datasets and utilized to enhance HOI
models designed for different HOI settings.

We perform extensive experiments on HICO-DET [34],
V-COCO [13] datasets. Our method cooperated with trans-
ferred interactiveness outperforms the state-of-the-art meth-
ods by 2.38, 3.06, and 2.17 mAP on three Default category
sets on HICO-DET, 4.0 and 3.4 mAP on V-COCO.

2. Related Works

Visual Relationship Detection. Visual relationship detec-
tion [6, 17, 24, 16] aims to detect the objects and classify
their relationships simultaneously. In [17], Lu et al. pro-
posed a relationship dataset VRD and an approach com-
bined with language priors. Predicates within relationship
triplet hsubject, predicate, objecti include actions, verbs,
spatial and preposition vocabularies. Such vocabulary set-
ting and severe long-tail issue within the dataset make this
task quite difﬁcult. Large-scale dataset Visual Genome [24]
is then proposed to promote studies in this problem. Recent
works [23, 25, 40, 30] put attention on more effective and
efﬁcient visual feature extraction and try to exploit semantic
information to reﬁne the relationship detection.
Human-Object Interaction Detection. Human-Object
Interaction [1, 4, 2] is essential to understand human-
centric interaction with objects. Recently several large-
scale datasets, such as V-COCO [13], HICO-DET [34],
HCVRD [18], were proposed for the exploration of HOI

Exhaustive Pairing

Dense HOI Graph

Non-Interaction 

Suppression

Human-Object Pair

Non-Interactive

Interactive

HOI 1 … HOI n

Sparse HOI Graph

Human Node
Object   Node
Predicate Edge

HOI Detection 

Model

HOIs

…

(a) One-Stage Inference

HOI Detection 

Model

HOIs

(b) Two-Stage Inference

Figure 2. HOIs within an image can be represented as a HOI graph.
Human and object can be seen as nodes, whilst the interactions are
represented as edges. Exhaustive pairing of all nodes would im-
port overmuch non-interactive edges and do damage to detection
performance. Our Non-Interaction Suppression can effectively re-
duce non-interactive pairs. Thus the dense graph would be con-
verted to a sparse graph and then be classiﬁed.

detection. Different from HOI recognition [35, 5, 12, 8, 15]
which is an image level classiﬁcation problem, HOI de-
tection needs to detect interactive human-object pairs and
classify their interactions at instance level. With the assis-
tance of DNNs and large-scale datasets, recently methods
have made signiﬁcant progress. Chao et al. [34] proposed
a multi-stream model combining visual features, spatial lo-
cations to help tackle this problem. To address the long tail
issue, Shen et al. [37] studied zero-shot learning problem
and predicted the verb and object separately. In [19], an ac-
tion speciﬁc density map estimation method is introduced
to locate objects interacted with human. In [32], Qi et al.
proposed GPNN incorporating DNN and graphical model,
which uses message parsing to iteratively update states and
classiﬁes all possible pairs/edges. Gao et al. [31] exploited
an instance centric attention module to enhance the infor-
mation from the interest region and facilitate the HOI clas-
siﬁcation. Generally, these methods inference in one-stage
and may suffer from severe non-interactive pair domination
problem. To address this issue, we utilize interactiveness
to explicitly discriminate non-interactive pairs and suppress
them before HOI classiﬁcation.

3. Preliminary

HOI

representation can be described as a graph
model [32, 23] as seen in Figure 2. Instances and relations
are expressed as nodes and edges respectively. With ex-
haustive pairing [19, 31], HOI graph G = (V, E) is dense
connected, where V includes human node Vh and object
node Vo. Let vh ∈ Vh and vo ∈ Vo denote the human
and object nodes. Thus edges e ∈ E are expressed as
e = (vh, vo) ∈ Vh × Vo. With n nodes, exhaustive par-
ing will generate a mass of edges. We aim to assign HOI
(including no HOI) labels on those edges. Considering that
a vast majority of non-interactive edges existing in E should
be discarded, our goal is to seek a sparse G ∗ with corrected

3586

CNN Layers

Pooling

Pose Map

CNN Block HP

Pooling

Concat

X

CNN Block OP

Pooling

FCs 1024

Object Detection Scores

P

LIS Function

Interactive

or

Non-Interactive

FCs 1024

NIS

Spatial Maps

CNN Layers

Pooling

Object Detection

Feature Extraction (R)

Human Stream
Object   Stream
Spatial-Pose Stream
Spatial Stream

Human Feature

Object Feature

CNN Block HC

Pooling

CNN Block OC

Pooling

FCs 1024

FCs 1024

C

HOIs

HOIs

HOIs

Figure 3. Overview of our framework. Interactiveness network P can cooperate with any HOI models (referred as C). P employs human,
object and spatial-pose streams to extract features from human and object appearance, spatial locations and human pose information. The
outputs of three streams are concatenated and inputted to the interactiveness discriminator. When cooperated with multi-stream C such
as [34, 31] (human, object, and spatial streams), H P and OP in P can share weights (dotted lines) with H C and OC in C during joint
training. In this work, these four blocks are all residual blocks [14]. LIS and NIS will be detailed in Section 4.3 and Section 4.5.

HOI labeling on its edges.

4. Our Method

4.1. Overview

As aforementioned, we introduce Interactiveness
Knowledge to advance HOI detection performance. That
is, explicitly discriminate the non-interactive pairs and sup-
press them before HOI classiﬁcation. From the semantic
point of view, interactiveness provides more general in-
formation than conventional HOI categories. Since any
human-object pair can be assigned binary interactiveness la-
bels according to the HOI annotations, i.e. “interactive” or
“non-interactive”, interactiveness knowledge can be learned
from multiple datasets with different HOI category settings
and transferred to any speciﬁc datasets.

To exploit this cue, we proposed interactiveness network
(interactiveness predictor, referred as P) which utilizes in-
teractiveness to reduce false positives caused by overmuch
non-interactive pair candidates. Some conventional mod-
ules are also included, namely, Representation Network
R (feature extractor) and Classiﬁcation Network C (HOI
classiﬁer). R is responsible for feature extraction from
detected instances. C utilizes node and edge features to
perform HOI classiﬁcation. Figure 3 is an overview of
our framework which follows the hierarchical classiﬁcation
paradigm. Speciﬁcally, we ﬁrst train P and C jointly to
learn the interactiveness and HOIs knowledge. Under usual
circumstances, the ratio of non-interactive edges is domi-
nant within inputs. Hence P will bring a strong supervised
signal to reﬁne the framework. In testing, P is utilized in
two stages. First, P evaluates the interactiveness of edges
by exploiting the learned interactiveness knowledge, so we

can convert the dense HOI graph to a sparse one. Second,
combined with interactiveness score from P, C will process
the sparse graph and classify the remaining edges.

In addition, on account of the generalization ability of in-
teractiveness knowledge, it can be transferred with P across
datasets (Section 4.4). Details of the framework architec-
ture are illustrated in Section 4.2 and 4.3. The process of
training and testing will be detailed in Section 4.4.

4.2. Representation and Classiﬁcation Networks

Human and Object Detection. In HOI detection, human
and object need to be detected ﬁrst. In this work, we fol-
low the setting of [31] and employ the Detectron [29] with
ResNet-50-FPN [20] to prepare bounding boxes and detec-
tion scores. Before post-processing, detection results will
be ﬁltered by the detection score thresholds ﬁrst.
Representation Network.
In previous methods [34, 19,
31], R is often modiﬁed from object detector such as Fast
R-CNN [10] or Faster R-CNN [11]. We also exploited
a Faster R-CNN [11] with ResNet-50 [14] based R here.
During training and testing, R is frozen and acts as a feature
extractor. Given the detected bounding boxes, we produce
human and object features by cropping ROI pooling feature
maps according to box coordinates.
HOI Classiﬁcation Network. As for C, multi-stream ar-
chitecture and late fusion strategy are frequently used and
approved effective [34, 31]. Follow [34, 31], for our classi-
ﬁcation network C, we utilize a human stream and an ob-
ject stream to extract human, object and context features.
Within each stream, a residual block [14] (denoted as H C ,
OC , seen in Figure 3) with pooling layer and fully con-
nected layers (FCs) are adopted. Moreover, an extra spa-

3587

Pose Map

Human Map Object Map

Low-grade Instance Suppressive Function

Person 2

Person 1

Person 1

Person 2

Person 1

Person 2

Person 1

Person 2

Figure 4. Inputs of the spatial-pose stream. Three kinds of maps
are included: pose map, human map and object map. Person 2
in two images both have interaction “feed” with giraffes. But two
pairs of Person 1 and giraffe are all non-interactive. Their poses
and locations are helpful for the interactiveness discrimination.

tial stream [34] is adopted to encode the spatial locations
of instances. Its input is a two-channel tensor consisting of
a human map and an object map, shown in Figure 4. Hu-
man and object maps are all 64x64 and obtained from the
human-object union box. In the human channel, the value
is 1 in the human bounding box and 0 in other areas. The
object channel is similar which has value 1 in the object
bounding box and 0 elsewhere. Following the late fusion
strategy, each stream will ﬁrst perform HOI classiﬁcation,
then three prediction scores will be fused by element-wise
sum in the same proportion to produce the ﬁnal result of C.

4.3. Interactiveness Network

Interactiveness needs to be learned by extracting and
combining essential information. The visual appearance of
human and object are obviously required. Besides, interac-
tive and non-interactive pairs also have other distinguishing
features, e.g. spatial location and human pose information.
For example, in the upper image of Figure 4, Person 1 and
the giraffe far from him are not interactive. Their spatial
maps [34] can provide pieces of evidence to help with clas-
siﬁcation. Furthermore, pose information is also helpful. In
the lower image, although two people are both close to the
giraffe, only Person 2 and the giraffe are interactive. The
arm of Person 2 is uplift and touching the giraffe. Whilst
Person 1 is back on to the giraffe, and his pose is quite dif-
ferent from the typical pose of “feed”.

Based on these reasons, the combination of visual ap-
pearance, spatial location and human pose information is
key to interactiveness discrimination. Hence P needs to en-
code these key elements together to learn the interactiveness
knowledge. A natural choice is the multi-stream architec-
ture as presented: human, object and spatial-pose streams.
Human and Object stream. For human and object appear-

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t
h
g
i
e

W

0.001

0.1

0.2

0.3

0

0

0.991

0.7

0.8

0.9

1

0.4

0.5

0.6

Object Detection Score

Figure 5. The illustration of P(·) within Low-grade Suppressive
Function. Its input is object detection score. High-grade detected
objects will be emphasized and distinguished with low-grade ones.
In addition, P(0) = 5.15E − 05 and P(1) = 9.99E − 01.

ance, we extract ROI pooling features from representation
network R, then input them into residual blocks H P and
OP, respectively. The architecture of H P and OP are same
as H C and OC (Figure 3). Through subsequent global av-
erage pooling and FCs, the output features of two streams
are denoted as fh and fo, respectively.
Spatial-Pose Stream. Different from [34], our spatial-pose
stream input includes a special 64x64 pose map. Given the
union box of each human and his/her paired object, we em-
ploy pose estimation [22, 27] to estimate his/her 17 key-
points (in COCO format [7]). Then, we link the keypoints
with lines of different gray value ranging from 0.15 to 0.95
to represent different body parts, which implicitly encodes
the pose features. Whilst the other area is set as 0. Finally,
we reshape the union box to 64x64 to construct the pose
map. We concatenate the pose map with human and object
maps which are the same as those in the spatial stream of
C. This forms the input for our spatial-pose stream. Next,
we exploit two convolutional layers with max pooling and
two 1024 sized FCs to extract the feature fsp of three maps.
Last, the output will be concatenated with the outputs of hu-
man and object streams for interactiveness discrimination.
Given a HOI graph G with all possible edges, P will
evaluate the interactiveness of pair (vh, vo) based on learned
knowledge, and gives conﬁdence:

sP
(h,o) = fP(fh, fo, fsp) ∗ L(sh, so),

(1)

where L(sh, so) is a novel weight function named Low-
grade Instance Suppressive Function (LIS). It takes the hu-
man and object detection scores sh, so as inputs:

L(sh, so) = P(sh) ∗ P(so),

(2)

3588

where

P(x) =

T

1 + e(k−wx)

,

Dataset 

X

R

(3)

P
Weights
Sharing
C

HOIs

Interactive

Non-Interactive

Dataset 

X

R

Interactive

Non-Interactive

P

NIS

P(·) is a part of the logistic function, the value of T, k
and w will be determined by data-driven manner. Fig-
ure 5 depicts the curve of P(·) whose domain deﬁnition is
(0, 1). Bounding boxes will have low weight till their score
is higher than a threshold. Previous works [31, 19] often
directly multiply detection scores by the ﬁnal classiﬁcation
score. But they cannot notably emphasize the differentia-
tion between high quality and inaccurate detection results.
LIS has the ability to enhance the differentiation between
high and low grade object detections as shown in Figure 5.
Weights Sharing Strategy. An additional beneﬁt of our
interactiveness network is that, if cooperated with multi-
stream HOI detection model C, P can share the weights
of convolutional blocks with the ones in C. As shown in
Figure 3, blocks H P and OP can share weights with H C
and OC in the joint training. This weights sharing strategy
can guarantee information sharing and better optimization
of P and C in the multi-task training.

4.4. Interactiveness Knowledge Transfer Training

With R, P and C, our framework has two modes of uti-
lization: hierarchical joint training in Default Mode, and in-
teractiveness transfer training in Transfer Learning Mode.
Hierarchical Joint Training. In Default Mode, we intro-
duce our hierarchical joint training scheme, as illustrated
in Figure 6 (a). By adding a supervisor P, our frame-
work works in an unconventional training mode. To be
speciﬁc, the framework is trained with hierarchical classiﬁ-
cation tasks, i.e. explicit interactiveness discrimination and
HOI classiﬁcation. The objective function of the framework
can be expressed as:

L = LC + LP,

(4)

where LC denotes the HOI classiﬁcation cross entropy loss,
while LP is the binary classiﬁcation cross entropy loss.

Different from one-stage methods, additional interac-
tiveness discrimination enforces the model to learn inter-
activeness knowledge, which can bring more powerful su-
pervised constraints. Namely, when a pair is predicted as
speciﬁc HOIs such as “cut cake”, P must give the predic-
tion “interactive” simultaneously. Experiment results (Sec-
tion 5.4) prove that interactiveness knowledge learning can
effectively reﬁne the training and improve the performance.
The framework in Default Mode is called “RPDCD” in the
following, where “D” indicates “Default”.
Interactiveness Knowledge Transfer Training. Noting
that P only needs binary labels which are beyond the HOI
classes, so interactiveness is transferable and reusable. In
Transfer Learning Mode, P can be used as a transferable

C

HOIs

Two-Stage Inference

Hierarchical Joint Training

(a)
(b)

Multiple 
Datasets

Dataset 

X

R

R

P

C

Interactive

Non-Interactive

HOIs

Dataset 

X

R

Interactive

P

C

Non-Interactive

NIS

HOIs

Interactiveness Prior Transfer Training

Two-Stage Inference

Figure 6. The schemes for training and testing.
(a) In Default
Mode, P and C are ﬁrst trained jointly with weights sharing on
the same dataset. (b) In Transfer Learning Mode, P can learn in-
teractiveness knowledge across datasets and cooperates with mul-
tiple Cs trained on different datasets. In testing, our framework
infers in two stages, i.e. P performs interactiveness discrimination
at ﬁrst, then C classiﬁes the remaining edges/pairs.

knowledge learner to learn interactiveness from multiple
datasets and be applied to each of them respectively, as
illustrated in Figure 6 (b). On the contrary, C must be
trained on a single dataset once a time considering the va-
riety of HOI category settings in different datasets. There-
fore, knowledge of the speciﬁc HOIs is difﬁcult to transfer.
We will compare and evaluate the transferability of interac-
tiveness knowledge and HOI knowledge in Section 5.

For better representation of the transferability and perfor-
mance enhancement of interactiveness, we set several trans-
fer learning modes, referred as “RPT nCD”, where “T” in-
dicates “Transfer”, and “n” means P learns interactiveness
knowledge from “n” datasets: 1) RPT 1CD: train P on 1
dataset and apply P to another dataset. 2) RPT 2CD: train
P on 2 datasets and apply P to them respectively.

To compare the transferability of interactiveness knowl-
edge and HOIs knowledge, we set a transfer learning mode
“RCT ” for C: 3) RCT : train C (without P) on one dataset
and apply it to another dataset. For example, we ﬁrst train
and test C on HICO-DET (referred as “RCD”). Second,
we replace the last FC layer of C with a FC layer that ﬁts
the number of V-COCO HOIs, then ﬁnetune C for 1 epoch
on V-COCO train set. Last, we test this new C on V-COCO
test set. Details of the above modes can be found in Table 1.

4.5. Testing with Non Interaction Suppression

After the interactiveness learning, we further utilize P to
suppress the non-interactive pair candidates in testing, i.e.
Non-Interaction Suppression (NIS). The inference process
is based on tree structure as shown in Figure 2. Detected
instances in test set will be paired exhaustively, so a dense
graph G of human and objects is generated. First, we em-
ploy P to compute the interactiveness score of all edges.
Next, we suppress the edges that meet NIS conditions, i.e.
interaction score sP

(h,o) smaller than a certain threshold α.

3589

Through NIS, we can convert G to G

denotes
the approximate sparse HOI graph. The HOI classiﬁcation
score vector S C

(h,o) of (vh, vo) from C is:

where G

′

′

Test Set

Method

P-Train Set

C-Train Set

HICO-DET

HICO-DET

RPDCD
HICO-DET
RPT 1CD
HICO-DET
RPT 2CD HICO-DET, V-COCO HICO-DET
HICO-DET

V-COCO

RCD
RCT

-
-

S C

(h,o) = FC[Γ

′

′

; G

(vh, vo)],

(5)

HICO-DET

′

where Γ
pair (vh, vo) can be obtained by:

are input features. The ﬁnal HOI score vector of a

S(h,o) = S C

(h,o) ∗ sP

(h,o).

(6)

V-COCO

V-COCO

RPDCD
RPT 1CD
RPT 2CD HICO-DET, V-COCO

HICO-DET

V-COCO

RCD
RCT

-
-

Table 1. Mode settings in experiments.

V-COCO
V-COCO
V-COCO
V-COCO
V-COCO

HICO-DET

Here we multiply interactiveness score sP
output of C.

(h,o) from P by the

5. Experiments

In this section, we ﬁrst introduce datasets and metrics
adopted and then give implementation details of our frame-
work. Next, we report our HOI detection results quantita-
tively and qualitatively compared with state-of-the-art ap-
proaches. Finally, we conduct ablation studies to validate
the validity of the components in our framework.

5.1. Datasets and Metrics

Datasets. We adopt two HOI datasets HICO-DET [34] and
V-COCO [13]. HICO-DET [34] includes 47,776 images
(38,118 in train set and 9658 in test set), 600 HOI categories
on 80 object categories (same with [7]) and 117 verbs, and
provides more than 150k annotated human-object pairs. V-
COCO [13] provides 10,346 images (2,533 for training,
2,867 for validating and 4,946 for testing) and 16,199 per-
son instances. Each person has annotations for 29 action
categories (ﬁve of them have no paired object). The objects
are divided into two types: “object” and “instrument”.
Metrics. We follow the settings adopted in [34], i.e. a pre-
diction is a true positive only when the human and object
bounding boxes both have IoUs larger than 0.5 with ref-
erence to ground truth, and the HOI classiﬁcation result is
accurate. The role mean average precision [13] is used to
measure the performance.

5.2. Implementation Details

′

We employ a Faster R-CNN [11] with ResNet-50 [14]
as R and keep it frozen. C consists of three streams similar
to [34, 31], extracting features Γ
from instance appearance,
spatial location as well as context. Within human and object
streams, a residual block [14] with global average pooling
and four 1024 sized FCs are used. Relatively, the spatial
stream is composed of two convolutional layers with max
pooling, and two 1024 sized FCs. Following [34, 31], we
use the late fusion strategy in C. P also consists of three
streams (seen in Figure 3). A residual block [14] with global
average pooling, and two 1024 sized FCs are adopted in hu-
man and object streams. Residual blocks within these two

streams will share weights with those in C. Spatial-Pose
stream consists of two convolutional layers with max pool-
ing and two 1024 sized FCs. The outputs of three streams
are concatenated and passed through two 1024 sized FCs to
perform interactiveness discrimination.

For a fair comparison, we adopt the object detection re-
sults and COCO [7] pre-trained weights from [31] which
are provided by authors. Since NIS and LIS can suppress
non-interactive pairs, we set detection conﬁdence thresh-
olds lower than [31], i.e. 0.6 for human and 0.4 for object.
The image-centric training strategy [11] is also applied. In
other words, pair candidates from one image make up the
mini-batch. We adopt SGD and set an initial learning rate
as 1e-4, weight decay as 1e-4, momentum as 0.9. In train-
ing, the ratio of positive and negative samples is 1:3. We
jointly train the framework for 25 epochs. In LIS mentioned
in Equation 3, we set T = 8.4, k = 12.0, w = 10.0. In test-
ing, the interactiveness threshold α in NIS is set as 0.1. All
experiments are conducted on a single Nvidia Titan X GPU.

5.3. Results and Comparisons

From Table 2, we can ﬁnd that

We compare our method with ﬁve state-of-the-art HOI
detection methods [34, 37, 19, 32, 31] on HICO-DET, and
four methods [13, 19, 32, 31] on V-COCO. The HOI de-
tection result is evaluated with mean average precision. For
HICO-DET, we follow the settings in [34]: Full (600 HOIs),
Rare (138 HOIs), Non-Rare (462 HOIs) in Default and
Known Object mode. For V-COCO, we evaluate AProle (24
actions with roles). More details can be found in [34, 13].
Default Mode.
the
RPDCD has already outperformed compared methods.
We respectively achieve 17.03 and 19.17 mAP on Default
and Know Object Full sets on HICO-DET. In particular, we
boost the performance of 2.97 and 4.18 mAP on Rare sets.
To illustrate, as the generalization ability of interactive-
ness is beyond HOI category settings, information scarcity
and learning difﬁculty of rare categories is alleviated. So
the performance difference between rare and non-rare cat-
egories is accordingly reduced. Results on V-COCO are
shown in Table 3. RPDCD also achieves superior perfor-
mance and outperforms state-of-the-art method [31] (late
and early fusion model), yielding 47.8 mAP, which quan-

3590

Default
Rare Non-Rare

Known Object

Full

Rare Non-Rare

herd-sheep walk-sheep

sit_at-dining_table
eat_at-dining_table

carry-backpack
wear-backpack

Method

Shen et al. [37]
HO-RCNN [34]
InteractNet [19]
GPNN [32]
iCAN [31]
RCD
RPDCD
RCT
RPT 1CD
RPT 2CD

Full

6.46
7.81
9.94
13.11
14.84
13.75
17.03

10.61
16.91
17.22

4.24
5.37
7.16
9.34
10.45
10.23
13.42

7.78
13.32
13.51

7.12
8.54
10.77
14.23
16.15
15.45
18.11

11.45
17.99
18.32

-

-

-

10.41

8.94

10.85

-
-

16.26
15.34
19.17

12.47
19.05
19.38

-
-

11.33
10.98
15.51

8.87
15.22
15.38

-
-

17.73
17.02
20.26

13.54
20.19
20.57

Table 2. Results comparison on HICO-DET [34]. D indicates the
default mode, and T means the transfer learning model.

titatively validates the efﬁcacy of the interactiveness. No-
tably, RCD shows limited performance when compared
with other models containing P. This reveals the perfor-
mance enhancement ability of interactiveness network P.
Transfer Learning Mode. By leveraging transferred in-
teractiveness knowledge, RPT 2CD presents great perfor-
mance improvement and achieves the most state-of-the-art
performance. On HICO-DET, RPT 2CD surpasses [31]
by 2.38, 3.06, and 2.17 mAP on three Default category
sets. Meanwhile, it also outperforms [31] by 4.0 and 3.4
mAP on V-COCO. This indicates the good transferabil-
ity and effectiveness of interactiveness. Since HICO-DET
train set (38K) is much bigger than V-COCO train set
(2.5K), improvement is also larger when transferring is per-
formed from HICO-DET to V-COCO. As we can see, mode
RPT 1CD achieves obvious improvement on V-COCO, but
it shows a relatively smaller improvement on HICO-DET
when compared with mode RPDCD.

We also evaluate the transferability of HOIs knowledge.
In comparison with RCD, RCT shows a signiﬁcant per-
formance decrease of 3.14 and 4.7 mAP on two datasets,
as shown in Table 2 and 3. It proves that interactiveness is
more suitable and easier to transfer than HOIs knowledge.
Non-Interaction Reduction. The non-interactive pairs re-
duction effect after employing NIS are shown in Table
In default mode RPDCD, NIS shows obvious effec-
4.
tiveness. With interactiveness transferred from multiple
datasets, RPT 2CD achieves better suppressive effect and
discards 70.94% and 73.62% non-interactive pairs respec-
tively on two datasets, thus bringing more performance
gain. Meanwhile, RPT 1CD also performs well and sup-
presses a certain amount of non-interactive pair candidates.
This suggests the good transferability of interactiveness.
Visualized Results. Representative predictions are shown
in Figure 7. We can ﬁnd that our model is capable of de-
tecting various kinds of complicated HOIs such as multiple
interactions within one pair, one person performing multi-
ple interactions with different objects, one object interacted
with multiple persons, multiple persons performing differ-
ent interactions with multiple objects.

Figure 8 shows the visualized effects of NIS. We can
see that NIS effectively distinguish the non-interactive pairs

hold-cellphone

type_on-cellphone

hold,jump,ride,sit_on,straddle

work_on_computer-instr

lay-instr

bicycle

read-instr

hold-instr

hit-obj

bicycle

hit-instr
hold-obj

sit-instr

cut-obj

cut-instr
hold-obj

Figure 7. Visualization of sample HOI detections. Subjects and
objects are represented with blue and red bounding boxes. While
interactions are marked by green lines linking the box centers.

Method

Gupta et al. [13]
InteractNet [19]
GPNN [32]
iCAN w/ late(early) [31]
RCD
RPDCD
RCT
RPT 1CD
RPT 2CD

AProle

31.8
40.0
44.0

44.7 (45.3)

43.2
47.8

38.5
48.3
48.7

Table 3. Results comparison on V-COCO [13]. D indicates the
default mode, and T means the transfer learning model.

and suppress them in extremely difﬁcult scenarios, such as
a person performing a confusing action and the tennis ball,
a crowd of people with ties. In the bottom-left corner we
show an even harder sample. When the subject and ob-
ject are the left hand and right hand, C predicts wrong HOI
“type on keyboard”. C may mistake the left hand for the
keyboard because they are too close. However, P accu-
rately ﬁgures out that two hands are non-interactive. These
results prove that the one-stage method would yield many
false positives without interactiveness and NIS.

5.4. Ablation Studies

In mode RPDCD, we analyze the signiﬁcance of Low-
grade Instance Suppressive, Non-Interaction Suppression
and the three streams within P (seen in Table 5).
Non-Interaction Suppression NIS plays a key role to re-
duce the non-interactive pairs. We evaluate its impact by
removing NIS during testing. In other words, we directly
use the S(h,o) from Equation 6 as the ﬁnal predictions with-
out NIS. Consequently, the model shows an obvious perfor-
mance degradation, which proves the importance of NIS.
Low-grade Instance Suppressive LIS suppress the low-
grade object detections and reward the high-grade ones. By
removing L(sh, so) in Equation 1, we observe a degradation
in Table 5. This suggests that LIS is capable of distinguish-
ing the low-grade detections and improves the performance

3591

ski-instr

skateboard-instr

hit-obj

work_on-instr

ski-instr

wear-tie

hit-obj

cut-obj

type_on-keyboard

sit_on-chair

hold-apple

hold-handbag

Figure 8. Visualized effects of NIS. Green lines mean accurate HOIs, while purple lines mean non-interactive pairs which are suppressed.
Without NIS, C would generate false positive predictions for these non-interactive pairs in one-stage inference, which are shown by the
purple texts below the images. Even some extremely hard scenarios can be discovered and suppressed, such as mis-groupings between
person and object close to each other, person and object in clutter scene.

Test Set

Method

Reduction

HICO-DET

V-COCO

RPDCD
RPT 1CD
RPT 2CD
RPDCD
RPT 1CD
RPT 2CD

-65.96%
-62.24%
-70.94%
-65.98%
-59.51%
-73.62%

Table 4. Non-interactive pairs reduction after performing NIS.

brought by P in the hierarchical joint training.
Three Streams. By keeping one stream in P each time,
we evaluate their contributions as shown in Table 5. We
can ﬁnd that spatial-pose stream is the largest contributor,
but we still need appearance features from the other two
streams to achieve better performance.

Method

Default Full KO Full

HICO-DET

V-COCO
AProle

6. Conclusion

RPDCD
w/o NIS
w/o LIS
w/o NIS & LIS
H Stream Only
O Stream Only
S-P Stream Only

17.03
15.86
16.35
15.45
14.91
15.28
15.73

19.17
17.35
18.83
17.31
16.21
16.89
17.46

47.8
46.2
47.4
45.8
44.5
45.2
46.0

Table 5. Results of ablation studies. Human, object, spatial-pose
stream are representated as H, O and S-P stream.

without using more costly superior object detector.
NIS & LIS Without NIS and LIS both, our method only
takes effect in the joint training of P and C. As we can
see in Table 5, performance degrades greatly but still out-
performs other methods, which indicates the enhancement

In this paper, we propose a novel method to learn and
utilize the implicit interactiveness knowledge, which is gen-
eral and beyond HOI categories. Thus, it can be transferred
across datasets. With interactiveness knowledge, we exploit
an interactiveness network to perform Non-interaction Sup-
pression before HOI classiﬁcation in inference. Extensive
experiment results show the efﬁcacy of interactiveness. By
combining our method with existing detection models, we
achieve state-of-the-art results on HOI detection.

Acknowledgement: This work is supported in part by the
National Key R&D Program of China, No.2017YFA0700800,
National Natural Science Foundation of China under Grants
61772332.

3592

References

[1] Yang Wang, Hao Jiang, Mark S Drew, Ze-Nian Li, and Greg
Mori. Unsupervised discovery of action classes. In CVPR,
2006. 2

[2] N Ikizler, R. G Cinbis, S Pehlivan, and P Duygulu. Recog-

nizing actions from still images. In ICPR, 2008. 2

[3] Brenna D Argall, Sonia Chernova, Manuela Veloso, and
Brett Browning. A survey of robot learning from demon-
stration. Robotics and autonomous systems, 57(5), 2009. 1

[4] Weilong Yang, Yang Wang, and Greg Mori. Recognizing
human actions from still images with latent poses. In CVPR,
2010. 2

[5] Vincent Delaitre, Ivan Laptev, and Josef Sivic. Recognizing
human actions in still images: a study of bag-of-features and
part-based representations. In BMVC, 2010. 2

[6] M. A. Sadeghi and A. Farhadi. Recognition using visual

phrases. In CVPR, 2012. 2

[7] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, 2014. 4, 6

[8] Chao-Yeh Chen and Kristen Grauman. Predicting the lo-
cation of interactees in novel human-object interactions. In
ACCV, 2014. 2

[9] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. Activitynet: A large-scale video
benchmark for human activity understanding.
In CVPR,
2015. 1

[10] Ross Girshick. Fast r-cnn. In ICCV, 2015. 3
[11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015. 1, 3, 6

[12] Yu Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and
Jia Deng. Hico: A benchmark for recognizing human-object
interactions in images. In ICCV, 2015. 2

[13] Saurabh Gupta and Jitendra Malik. Visual semantic role la-

beling. arXiv preprint arXiv:1505.04474, 2015. 1, 2, 6, 7

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 3, 6

[15] Arun Mallya and Svetlana Lazebnik. Learning models for
actions and person-object interactions with transfer to ques-
tion answering. In ECCV, 2016. 2

[16] Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. Situation
recognition: Visual semantic role labeling for image under-
standing. In CVPR, 2016. 2

[17] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-
Fei. Visual relationship detection with language priors. In
ECCV, 2016. 1, 2

[18] Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and An-
ton van den Hengel. Care about you: towards large-scale
human-centric visual relationship detection. arXiv preprint
arXiv:1705.09892, 2017. 2

[20] Tsung-Yi Lin, Piotr Doll´ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, 2017. 3

[21] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-

shick. Mask r-cnn. In ICCV, 2017. 1

[22] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
In ICCV,

RMPE: Regional multi-person pose estimation.
2017. 4

[23] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.
In

Scene graph generation by iterative message passing.
CVPR, 2017. 2

[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. IJCV, 123(1):32–73, 2017. 1, 2

[25] Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-
Seng Chua. Visual translation embedding network for visual
relation detection. In CVPR, 2017. 2

[26] Cewu Lu, Hao Su, Yonglu Li, Yongyi Lu, Li Yi, Chi-Keung
Tang, and Leonidas J Guibas. Beyond holistic object recog-
nition: Enriching image understanding with part states. In
CVPR, 2018. 1

[27] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu
Fang, and Cewu Lu. Crowdpose: Efﬁcient crowded scenes
pose estimation and a new benchmark.
arXiv preprint
arXiv:1812.00324, 2018. 4

[28] Bo Pang, Kaiwen Zha, Hanwen Cao, Chen Shi, and Cewu
Lu. Deep rnn framework for visual sequential applications.
arXiv preprint arXiv:1811.09961, 2018. 1

[29] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr
Doll´ar, and Kaiming He. Detectron. https://github.
com/facebookresearch/detectron, 2018. 3

[30] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi
Parikh. Graph r-cnn for scene graph generation. In ECCV,
2018. 2

[31] Chen Gao, Yuliang Zou, and Jia-Bin Huang. ican: Instance-
centric attention network for human-object interaction detec-
tion. arXiv preprint arXiv:1808.10437, 2018. 1, 2, 3, 5, 6,
7

[32] Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen,
and Song-Chun Zhu. Learning human-object interactions by
graph parsing neural networks. In ECCV, 2018. 1, 2, 6, 7

[33] Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, and
Song-Chun Zhu. Learning pose grammar to encode human
body conﬁguration for 3d pose estimation. In AAAI, 2018. 1
[34] Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and
Jia Deng. Learning to detect human-object interactions. In
WACV, 2018. 1, 2, 3, 4, 6, 7

[35] Hao-Shu Fang, Jinkun Cao, Yu-Wing Tai, and Cewu Lu.
Pairwise body-part attention for recognizing human-object
interactions. In ECCV, 2018. 2

[36] Yuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, and
Cewu Lu. Pose ﬂow: Efﬁcient online pose tracking. arXiv
preprint arXiv:1802.00977, 2018. 1

[19] Georgia Gkioxari, Ross Girshick, Piotr Doll´ar, and Kaiming
He. Detecting and recognizing human-object interactions.
arXiv preprint arXiv:1704.07333, 2017. 1, 2, 3, 5, 6, 7

[37] Liyue Shen, Serena Yeung, Judy Hoffman, Greg Mori, and
Fei Fei Li. Scaling human-object interaction recognition
through zero-shot learning. In WACV, 2018. 2, 6, 7

3593

[38] Wenqiang Xu, Yonglu Li, and Cewu Lu. Srda: Generat-
ing instance segmentation annotation via scanning, reason-
ing and domain adaptation. In ECCV, 2018. 1

[39] Hao-Shu Fang, Guansong Lu, Xiaolin Fang, Jianwen Xie,
Yu-Wing Tai, and Cewu Lu. Weakly and semi supervised
human body part parsing via pose-guided knowledge trans-
fer. CVPR, 2018. 1

[40] Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang
Wang, Jing Shao, and Chen Change Loy. Zoom-net: Mining
deep feature interactions for visual relationship recognition.
arXiv preprint arXiv:1807.04979, 2018. 2

3594

