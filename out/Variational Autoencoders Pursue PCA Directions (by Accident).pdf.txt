Variational Autoencoders Pursue PCA Directions (by Accident)

Michal Rol´ınek∗, Dominik Zietlow∗ and Georg Martius

Max-Planck-Institute for Intelligent Systems, T¨ubingen, Germany

{mrolinek, dzietlow, gmartius}@tue.mpg.de

Abstract

The Variational Autoencoder (VAE) is a powerful archi-
tecture capable of representation learning and generative
modeling. When it comes to learning interpretable (disen-
tangled) representations, VAE and its variants show unpar-
alleled performance. However, the reasons for this are un-
clear, since a very particular alignment of the latent embed-
ding is needed but the design of the VAE does not encourage
it in any explicit way. We address this matter and offer the
following explanation: the diagonal approximation in the
encoder together with the inherent stochasticity force local
orthogonality of the decoder. The local behavior of promot-
ing both reconstruction and orthogonality matches closely
how the PCA embedding is chosen. Alongside providing an
intuitive understanding, we justify the statement with full
theoretical analysis as well as with experiments.

1. Introduction

The Variational Autoencoder (VAE) [24, 36] is one of
the foundational architectures in modern-day deep learn-
ing. It serves both as a generative model as well as a repre-
sentation learning technique. The generative model is pre-
dominantely exploited in computer vision [25, 15, 22, 16]
with notable exceptions such as generating combinatorial
graphs [26]. As for representation learning, there is a vari-
ety of applications, ranging over image interpolation [19],
one-shot generalization [35], language models [43], speech
transformation [3], and more. Aside from direct applica-
tions, VAEs embody the success of variational methods in
deep learning and have inspired a wide range of ongoing
research [23, 44].

Recently, unsupervised learning of interpretable latent
representations has received a lot of attention. Interpretabil-
ity of the latent code is an intuitively clear concept. For
instance, when representing faces one latent variable would
solely correspond to the gender of the person, another to
skin tone, yet another to hair color and so forth. Once such

∗These authors contributed equally to this work.

Figure 1. Latent traversal over a single latent coordinate on an ex-
emplary image from the CelebA dataset [28] for a trained β-VAE.
The latent coordinate clearly isolates the azimuth angle. Provided
by courtesy of the authors of [17].

a representation is found it allows for interpretable latent
code manipulation, which is desirable in a variety of ap-
plications; recently, for example, in reinforcement learning
[39, 18, 11, 41, 34].

The term disentanglement [10, 2, 29] offers a more for-
mal approach. A representation is considered disentangled
if each latent component encodes precisely one “aspect” (a
generative factor) of the data. Under the current disentan-
glement metrics [17, 21, 6, 29], VAE-based architectures
(β-VAE [17], TCVAE [6], FactorVAE [21]) dominate the
benchmarks, leaving behind other approaches such as Info-
GAN [7] and DCIGN [25]. Exemplarily, a latent traversal
for a β-VAE is shown in Fig. 1 in which precisely one gen-
erative factor is isloated (face azimuth).

The success of VAE-based architectures on disentangle-
ment tasks comes with a certain surprise. One surprising
aspect is that VAEs have been challenged on both of its own
design functionalities, as generative models [14, 12] and as
log-likelihood optimizers [30, 33]. Yet, no such claims are
made in terms of disentanglement. Another surprise stems
from the fact that disentanglement requires the following
feature: the representative low-dimensional manifold must
be aligned well with the coordinate axes. However, the de-
sign of the VAE does not suggest any such mechanism. On
the contrary, the idealized log-likelihood objective is, for
example, invariant to rotational changes in the alignment.

Such observations have planted a suspicion that the in-
ner workings of the VAE are not sufﬁciently understood.
Several recent works approached this issue [5, 40, 8, 1, 12,
31, 9]. However, a mechanistic explanation for the VAE’s
unexpected ability to disentangle is still missing.

In this paper, we isolate an internal mechanism of the
VAE (also β-VAE) responsible for choosing a particular la-
tent representation and its alignment. We give theoretical

112406

analysis covering also the nonlinear case and explain the
discovered dynamics intuitively. We show that this mecha-
nism promotes local orthogonality of the embedding trans-
formation and clarify how this orthogonality corresponds to
good disentanglement. Further, we uncover strong resem-
blance between this mechanism and the classical Principle
Components Analysis (PCA) algorithm. We conﬁrm our
theoretical ﬁndings in experiments.

Our theoretical approach is particular in the following
ways: (a) we base the analysis on the implemented loss
function in contrast to the typically considered idealized
loss, and (b) we identify a speciﬁc regime, prevalent in prac-
tice, and utilize it for a vital simpliﬁcation. This simpliﬁca-
tion is the crucial step in enabling formalization.

The results, other than being signiﬁcant on their own,
also provide a solid explanation of “why β-VAEs disentan-
gle”.

2. Background

Let us begin with reviewing the basics of VAE, PCA, and
of the Singular Value Decomposition (SVD), along with a
more detailed overview of disentanglement.

2.1. Variational Autoencoders

Let {xi}N

i=1 be a dataset consisting of N i.i.d. sam-
ples xi ∈ X = Rn of a random variable x. An autoen-
coder framework operates with two mappings, the encoder
Encϕ : X → Z and the decoder Decθ : Z → X, where
Z = Rd is called the latent space. In case of the VAE, both
mappings are probabilistic and a ﬁxed prior distribution
p(z) over Z is assumed. Since the distribution of x is also
ﬁxed (actual data distribution q(x)), the mappings Encϕ
and Decθ induce joint distributions q(x, z) = qϕ(z|x)q(x)
and p(x, z) = pθ(x|z)p(z), respectively (omitting the de-
pendencies on parameters θ and ϕ). The idealized VAE ob-
jective is then the marginalized log-likelihood

N

Xi=1

log p(xi).

(1)

This objective is, however, not tractable and is approxi-
mated by the evidence lower bound (ELBO) [24]. For a
ﬁxed xi the log-likelihood log p(xi) is lower bounded by

E

z∼q(z|xi)

log p(xi | z) − DKL(q(z | xi) k p(z)) ,

(2)

where the ﬁrst term corresponds to the reconstruction loss
and the second to the KL divergence between the latent rep-

resentation q(z | xi) and the prior distribution p(z). A
variant, the β-VAE [17], introduces a weighting β on the
KL term for regulating the trade-off between reconstruction
(ﬁrst term) and the proximity to the prior. Our analysis will
automatically cover this case as well.

Finally, the prior p(z) is set to N (0,I) and the encoder

is assumed to have the form

Encϕ(x) ∼ qϕ(z|x) = N (cid:0)µϕ(x), diag σ2

ϕ(x)(cid:1) ,

where µϕ and σϕ are deterministic mappings depending
on parameters ϕ. Note particularly, that the covariance
matrix is enforced to be diagonal. This turns out to be
highly signiﬁcant for the main result of this work. The KL-
divergence in (2) can be computed in closed form as

(3)

LKL =

1
2

d

Xj=1(cid:0)µ2

j (xi) + σ2

j (xi) − log σ2

j (xi) − 1(cid:1) .

(4)

In practical implementations, the reconstruction term from
(2) is approximated with either a square loss or a cross-
entropy loss.

2.2. Disentanglement

In the context of learning interpretable representations
[2, 17, 5, 40, 38] it is useful to assume that the data orig-
inates from a process with some generating factors. For
instance, for images of faces this could be face azimuth,
skin brightness, hair length, and so on. Disentangled repre-
sentations can then be deﬁned as ones in which individual
latent variables are sensitive to changes in individual gen-
erating factors, while being relatively insensitive to other
changes [2]. Although quantifying disentanglement is non-
trivial, several metrics have been proposed [21, 17, 6].

Note also, that disentanglement is impossible without
ﬁrst learning a sufﬁciently expressive latent representation
capable of good reconstruction.

In an unsupervised setting, the generating factors are of
course unknown and the learning has to resort to statisti-
cal properties. Linear dimensionality reduction techniques
demonstrate the two basic statistical approaches. Principle
Components Analysis (PCA) greedily isolates sources of
variance in the data, while Independent Component Anal-
ysis (ICA) recovers a factorized representation, see [37] for
a recent review.

One important point to make is that disentanglement is
sensitive to rotations of the latent embedding. Following
the example above, let us denote by a, s, and h, continuous
values corresponding to face azimuth, skin brightness, and
hair length. Then, if we change the ideal latent representa-
tion as follows

a
s
h





 7→ 
0.75a + 0.25s + 0.61h
0.25a + 0.75s − 0.61h

−0.61a + 0.61s + 0.50h


 ,

(5)

we obtain a representation that is equally expressive in
terms of reconstruction (in fact we only multiplied with a
3D rotation matrix) but individual latent variables entirely
lost their interpretable meaning.

12407

2.3. PCA and Latent Representations

Let us examine more closely how PCA chooses the

alignment of the latent embedding and why it matters.

It is well known [4] that for a linear autoencoder with

encoder Y ′ ∈ Rd×n, decoder Y ∈ Rn×d, and square error

as reconstruction loss, the objective

Following the PCA example, we formalize which linear
mappings have the desired “axes-preserving” property.

Proposition 1 (Axes-preserving linear mappings). Assume

M ∈ Rn×d with d < n has d distinct nonzero singular

values. Then the following statements are equivalent:

min

Y,Y ′ Xxi∈X

kxi − Y Y ′xik2

(6)

(a) The columns of M are (pairwise) orthogonal.
(b) In every SVD of M as M = U ΣV ⊤, |V | is a permu-

tation matrix.

is minimized by the PCA decomposition. Speciﬁcally, by
d , for Pd = Id×nP ∈ Rd×n,
setting Y ′ = Pd, and Y = P ⊤
where P ∈ Rn×n is an orthogonal matrix formed by the n

normalized eigenvectors (ordered by the magnitudes of the
corresponding eigenvalues) of the sample covariance matrix

of X and Id×n ∈ Rd×n is a trivial projection matrix.

However, there are many minimizers of (6) that do not
induce the same latent representation. In fact, it sufﬁces to
append Y ′ with some invertible transformations (e.g. rota-
tions and scaling) and preﬁx Y with their inverses. This ge-
ometrical intuition is well captured using the singular value
decomposition (SVD), see also Figure 2.

Theorem 1 (SVD rephrased, [13]). Let M : Rn → Rd be a

linear transformation (matrix). Then there exist

• U : Rn → Rn, an orthogonal transformation (matrix)

of the input space,

• Σ : Rn → Rd a “scale-and-embed” transformation

(induced by a diagonal matrix),

• V : Rd → Rd, an orthogonal transformation (matrix)

of the output space

such that M = V ΣU ⊤.

Remark 1. For the sake of brevity, we will refer to orthogo-
nal transformations (with slight abuse of terminology) sim-
ply as rotations.

Example 1 (Other minimizers of the PCA objective). De-
ﬁne Y and Y ′ with their SVDs as Y = P ⊤ΣQ and its pseu-
doinverse Y ′ = Y † = Q⊤Σ†P and see that
Y Y ′ = P ⊤ΣQQ⊤Σ†P = P ⊤Id×nIn×dP = P ⊤

d Pd (7)

so they are indeed also minimizers of the objective (6) irre-
spective of our choice of Q and Σ.

It is also straightforward to check that the only choices
of Q, which respect the coordinate axes given by PCA, are
for |Q| to be a permutation matrix.

The take-away message (valid also in the non-linear

case) from this example is:

Different rotations of the same latent space are equally

suitable for reconstruction.

We strongly suggest developing a geometrical under-
standing for both cases (a) and (b) via Figure 2. For an
intuitive understanding of the formal requirement of distinct
eigenvalues, we refer to Supp. C.2.

Take into consideration that once the encoder preserves
the principle directions of the data, this already ensures an
axis-aligned embedding. The same is true also if the de-
coder is axes-preserving, provided the reconstruction of the
autoencoder is accurate.

2.4. Related work

Due to high activity surrounding VAEs, additional care is
needed when it comes to evaluating novelty. To the best of
our knowledge, two recent works address related questions
and require special attention.

The authors of [5] also aim to explain good performance
of (β–)VAE in disentanglement tasks. A compelling in-
tuitive picture of the underlying dynamics is drawn and
supporting empirical evidence is given.
In particular, the
authors hypothesize that “β–VAE ﬁnds latent components
which make different contributions to the log-likelihood
term of the cost function [reconstruction loss]”, while sus-
pecting that the diagonal posterior approximation is respon-
sible for this behavior. Our theoretical analysis conﬁrms
both conjectures (see Section 4).

Concurrent work [40] develops ISA-VAE; another VAE-
based architecture suited for disentanglement. Some parts
of the motivation overlap with the content of our work.
First, rotationally nonsymmetric priors are introduced for
reasons similar to the content of Section 3.1. And second,
both orthogonalization and alignment with PCA directions
are empirically observed for VAEs applied to toy tasks.

3. Results

3.1. The problem with log likelihood

The message from Example 1 and from the discussion
about disentanglement is clear: latent space rotation mat-
ters. Let us look how the idealized objectives (1) and (2)
handle this.

For a ﬁxed rotation matrix U we will be comparing
a baseline encoder-decoder pair (Encϕ, Decθ) with a pair

12408

Figure 2. Geometric interpretation of the singular value decomposition (SVD). Sequential illustration of the effects of applying the corre-
sponding SVD matrices of the encoder transformation V Σ†U ⊤ (left to right) and decoder U ΣV ⊤ (right to left). We notice that steps (i)
and (ii) of the encoder preserve the principle directions of the data. Step (iii), however, causes misalignment. In that regard, good encoders
are the ones for which step (iii) is trivial. The same argument works for the decoder (in reverse order). This condition is equivalent (for
non-degenerate transformations) to U ΣV ⊤ having orthogonal columns (See Proposition 1, where this is phrased for the decoder).

(Encϕ,U , Decθ ,U ) deﬁned as

3.2. Reformulating VAE loss

Encϕ,U (x) = U Encϕ(x),
Decθ ,U (z) = Decθ(U ⊤z).

(8)

(9)

The shortcomings of idealized losses are summarized in

the following propositions.

Proposition 2 (Log-likelihood rotation invariance). Let ϕ,
θ be any choice of parameters for encoder-decoder pair
(Encϕ,U , Decθ ,U ). Then, if the prior p(z) is rotationally
symmetric, the value of the log-likelihood objective (1) does
not depend on the choice of U .

Note that the standard prior N (0,I) is rotationally sym-
metric. This deﬁciency is not salvaged by the ELBO ap-
proximation.

Proposition 3 (ELBO rotation invariance). Let ϕ, θ
be any choice of parameters for encoder-decoder pair
(Encϕ,U , Decθ ,U ). Then, if the prior p(z) is rotationally
symmetric, the value of the ELBO objective (2) does not de-
pend on the choice of U .

We do not claim novelty of these propositions, however
we are not aware of their formalization in the literature. The
proofs can be found in Supplementary Material (Suppl. A).
An important point now follows:

Log-likelihood based methods (with rotationally
symmetric priors) cannot claim to be designed to

produce disentangled representations.

However, enforcing a diagonal posterior of the VAE en-
coder (3) disrupts the rotational symmetry and conse-
quently the resulting objective (4) escapes the invariance
arguments. Moreover, as we are about to see, this diag-
onalization comes with beneﬁcial effects regarding disen-
tanglement. We assume this diagonalization was primarily
introduced for different reasons (tractability, computational
convenience), hence the “by accident” part of the title.

The fact that VAEs were not meant to promote orthog-
onality reﬂects in some technical challenges. For one, we
cannot follow a usual workﬂow of a theoretical argument;
set up an idealized objective and ﬁnd suitable approxima-
tions which allow for stochastic gradient descent (a top-
down approach). We need to do the exact opposite, start
with the implemented loss function and ﬁnd the right sim-
pliﬁcations that allow isolating the effects in question while
preserving the original training dynamics (a bottom-up ap-
proach). This is the main content of this section.

First, we formalize the typical situation in which VAE
architectures “shut down” (ﬁll with pure noise) a subset of
latent variables and put high precision on the others.

Deﬁnition 1. We say that parameters ϕ, θ induce a polar-
ized regime if the latent coordinates {1, 2, . . . , d} can be
partitioned as Va ∪ Vp (sets of active and passive variables)
such that

(a) µ2

(b) σ2

j (x) ≪ 1 and σ2
j (x) ≪ 1 for j ∈ Va,

j (x) ≈ 1 for j ∈ Vp,

(c) The decoder ignores the passive latent components,

i.e.

∂ Decθ(z)

∂zj

= 0 ∀j ∈ Vp.

The polarized regime simpliﬁes the loss LKL from (4);
part (a) ensures zero loss for passive variables and part (b)
implies that σ2
j (x)). All in all, the per-
sample-loss reduces to

j (x) ≪ − log(σ2

L≈KL(xi) =

1

2 Xj∈Va(cid:0)µ2

j (xi) − log(σ2

j (xi)) − 1(cid:1) .

(10)

We will assume the VAE operates in the polarized
regime.
In Section 5.2, we show on multiple tasks and
datasets that the two objectives align very early in the train-
ing. This behavior is well-known to practitioners.

12409

Also, we approximate the reconstruction term in (2), as

it is most common, with a square loss

Lrec(xi) = Ek Decθ(Encϕ(xi)) − xik2

(11)

where the expectation is over the stochasticity of the en-
coder. All in all, the loss we will analyze has the form

Although we aim to ﬁx the deterministic loss (13), we
do not need to freeze the mean encoder and the decoder
entirely. Following Example 1, for each Ji and its SVD
Ji = UiΣiV ⊤
, we are free to modify Vi as long we corre-
i
spondingly (locally) modify the mean encoder.

Then we state the optimization problem as follows:

Lrec(xi) + L≈KL(xi).

(12)

Xxi∈X

Moreover, the reconstruction loss can be further decom-
posed into two parts; deterministic and stochastic. The for-
mer is deﬁned by

Lrec(xi) = k Decθ(µ(xi)) − xik2

(13)

and captures the square loss of the mean encoder. Whereas
the stochastic loss

ˆLrec(xi) = Ek Decθ(µ(xi)) − Decθ(Encϕ(xi))k2

(14)

is purely induced by the noise injected in the encoder.

Proposition 4. If the stochastic estimate Decθ(Encϕ(xi))
is unbiased around Decθ(µ(xi)), then

Lrec(xi) = Lrec(xi) + ˆLrec(xi).

(15)

This decomposition resembles

the classical bias-

variance decomposition of the square error [20].

3.3. The main result

Now, we ﬁnally give theoretical evidence for the central

claim of the paper:

Optimizing the stochastic part of the reconstruction

loss promotes local orthogonality of the decoder.

On that account, we set up an optimization problem
which allows us to optimize the stochastic loss (14) inde-
pendently of the other two. This will isolate its effects on
the training dynamics.

In order to make statements about local orthogonality,
we introduce for each xi the Jacobian (linear approxima-
tion) Ji of the decoder at point µ(xi), i.e.

Ji =

∂ Decθ(µ(xi))

∂µ(xi)

.

Since, according to (3),
Encϕ(xi) = µ(xi) + ε(xi) with

the encoder can be written as

ε(xi) ∼ N (cid:0)0, diag σ2(xi)(cid:1) ,

we can approximate the stochastic loss (14) with

E

ε(xi)(cid:13)(cid:13)Decθ(µ(xi)) −(cid:0)Decθ(µ(xi)) + Jiε(xi)(cid:1)(cid:13)(cid:13)

= E

ε(xi)kJiε(xi)k2,

(16)

2

(17)

min
Vi,σi

j >0 Xxi∈X
s. t. Xxi∈X

log E

ε(xi)kJiε(xi)k2

L≈KL(xi) = C,

(18)

(19)

where ε(xi) are sampled as in (16).
A few remarks are now in place.
• This optimization is not over network parameters,
j (only con-

rather directly over the values of all Vi, σi
strained by (19)).

• Both the objective and the constraint concern global

losses, not per sample losses.

• Indeed, none of Vi, σi

VAE objective (12).

j interfere with the rest of the

The presence of the (monotone) log function has one main
advantage; we can describe all global minima of (18) in
closed form. This is captured in the following theorem, the
technical heart of this work.

Theorem 2 (Main result). The following holds for opti-
mization problem (18, 19):

(a) Every local minimum is a global minimum.

(b) In every global minimum, the columns of every Ji are

orthogonal.

The full proof as well as an explicit description of the
minima is given in Suppl. A.1. However, an outline of the
main steps is given in the next section on the example of a
linear decoder.

The presence of the log term in (18) admittedly makes
our argument indirect. There are, however, a couple of
points to make. First, as was mentioned earlier, encourag-
ing orthogonality was not a design feature of the VAE. In
this sense, it is unsurprising that our results are also mildly
indirect.

Also, and more importantly, the global optimality of
Theorem 2 also implies that, locally, orthogonality is en-
couraged even for the pure (without logarithm) stochastic
loss.

Corollary 1. For ﬁxed xi ∈ X consider a subproblem of

(18) deﬁned as

min
Vi,σi

j >0

s. t.

E

ε(xi)kJiε(xi)k2
L≈KL(xi) = Ci.

(20)

(21)

12410

Also then, the result on the structure of local (global) min-
ima holds:

(a) Every local minimum is a global minimum.

(b) In every global minimum, the columns of every Ji are

orthogonal.

All in all, Theorem 2 justiﬁes the central message of the
paper stated at the beginning of this section. The analogy
with PCA is now also clearer. Locally, VAEs optimize a
tradeoff between reconstruction and orthogonality.

This result is unaffected by the potential β term in Equa-
tion (2), although an appropriate β might be required to en-
sure the polarized regime.

4. Proof outline

In this section, we sketch the key steps in the proof of
Theorem 2 and, more notably, the intuition behind them.
The full proof can be found in Suppl. A.1.

We will restrict ourselves to a simpliﬁed setting. Con-
sider a linear decoder M with SVD M = U ΣV T , which
removes the necessity of local linearization. This reduces
the objective (18) from a “global” problem over all exam-
ples xi to an objective where we have the same subproblem
for each xi.

As in optimization problem (18, 19), we resort to ﬁxing

the mean encoder (imagine a well performing one).

In the next paragraphs, we separately perform the opti-
mization over the parameters σ and the optimization over
the matrix V .

4.1. Weighting precision

For this part, we ﬁx the decoder matrix M and optimize
d). The simpliﬁed objective is

over values σ2 = (σ2

1, . . . , σ2

min

σ

s. t.

E

ε∼N (0,diag(σ2))kM εk2
Xj

− log σ2

j = C,

(22)

(23)

encoder is ﬁxed.

where the kµk2 terms from (10) disappear since the mean
The values − log(σj) can now be thought of as preci-
sions allowed for different latent coordinates. The log func-
tions even suggests thinking of the number of signiﬁcant
digits. Problem (22) then asks to distribute the “total pre-
cision budget“ so that the deviation from decoding “uncor-
rupted” values is minimal.

We will now solve this problem on an example linear

decoder M1 : R2 → R3 given by
y(cid:19) 7→ 
4x + y
−3x + y

5x − y

M1 : (cid:18)x


 .

Already here we see, that the latent variable x seems more
inﬂuential for the reconstruction. We would expect that x
receives higher precision than y.

Now, for ε = (εx, εy), we compute

kM1εk2 = k4εx + εyk2 + k−3εx + εyk2 + k5εx − εyk2

and after taking the expectation, we can use the fact that ε
has zero mean and write

EkM1εk2 =

var(4εx + εy) + var(−3εx + εy) + var(5εx − εy).
Finally, we use that for uncorrelated random variables A
and B we have var(A + cB) = var A + c2 var B. After
rearranging we obtain

EkM1εk2 = σ2

x(42 + (−3)2 + 52)+σ2

y(12 + 12 + (−1)2)

= 50σ2

x + 3σ2
y,

where σ = (σ2
squared norms of the column vectors of M1.

y). Note that the coefﬁcients are the

x, σ2

This turns the optimization problem (22) into a simple
exercise, particularly after realizing that (23) ﬁxes the value
of the product σxσy. Indeed, we can even set a2 = 50σx
and b2 = 3σy in the trivial inequality a2 + b2 ≥ 2ab and
√50 · 3 · e−C ≈ 24.5e−C,

EkM1εk2 = 50σ2

y ≥ 2 ·

x + 3σ2

ﬁnd that

(25)

with equality achieved when σ2
plies that the precision − log σ2
siderably higher than for y, just as expected.

x/σ2
y = 3/50. This also im-
x on variable x will be con-

Two remarks regarding the general case follow.
• The full version of inequality (25) relies on the con-
cavity of the log function; in particular, on (a version
of) Jensen’s inequality.

• The minimum value of the objective depends on the
product of the column norms. This also carries over to
the unsimpliﬁed setting.

4.2. Isolating sources of variance

Now that we can ﬁnd optimal values of precision, the fo-
cus changes on optimally rotating the latent space. In order
to understand how such rotations inﬂuence the minimum
of objective (22), let us consider the following example in

which we again resort to decoder matrix M2 : R2 → R3.
Imagine, the encoder alters the latent representation by
a 45◦ rotation. Then we can adjust the decoder M1 by ﬁrst
undoing this rotation. In particular, we set M2 = M1R⊤
45◦ ,
where Rθ is a 2D rotation matrix, rotating by angle θ. We
have

M2 : (cid:18)x′

y′(cid:19) 7→ 


1
2

√2(3x′ + 5y′)
√2(−2x′ − y′)
√2(3x′ + 2y′)




(24)

12411

Figure 3. 2D illustration of orthogonality in M V ⊤. The vec-
tors w1, w2 are the columns of M V ⊤. Minimizing the product
kw1kkw2k while maintaining the volume kw1kkw2k cos(α) re-
sults in w1 ⊥ w2.

and performing analogous optimization as before gives

EkM2εk2 =

61
2

σ2
x +

45
2

σ2

y ≥ 2r 61 · 45

4

e−C ≈ 52.4e−C.

(26)

We see that the minimal value of the objective is more
than twice as high, a substantial difference. On a high level,
the reason M1 was a better choice of a decoder is that the
variables x and y had very different impact on the recon-
struction. This allowed to save some precision on variable
y, as it had smaller effect, and use it on x, where it is more
beneﬁcial.

For a higher number of latent variables, one way to
achieve a “maximum stretch” among the impacts of latent
variables, is to pick them greedily, always picking the next
one so that its impact is maximized. This is, at heart, the
greedy algorithm for PCA.

Let us consider a slightly more technical statement. We
saw in (25) and (26) that after ﬁnding optimal values of σ
the remaining objective is the product of the column norms
of matrix M . Let us denote such quantity by colΠ(M ) =

Qj kM·jk. Then for a ﬁxed matrix M , we optimize

colΠ(M V ⊤)

min

V

(27)

over orthogonal matrices V .

This problem can be interpreted geometrically. The col-
umn vectors of M V ⊤ are the images of base vectors ej .
Consequently, the product gives an upper bound on the vol-
ume (the image of the unit cube)

Yj

kM V ⊤ejk ≥ Vol({M V ⊤x : x ∈ [0, 1]d}) .

(28)

However, as orthogonal matrices V are isometries, they do
not change this volume. Also, the bound (28) is tight pre-
cisely when the vectors M V ⊤ej are orthogonal. Hence,
the only way to optimize colΠ(M V ⊤) is by tightening the
bound, that is by ﬁnding V for which the column vectors
of M V ⊤ are orthogonal, see Figure 3 for an illustration.
In this regards, it is important that M performs a different
scaling along each of the axis (using Σ), which allows for
changing the angles among the vectors M V ⊤ej (cf. Figure
2).

Table 1. Percentage of training time where ∆KL < 3 % (Eq. (30))
continuously until the end. Reported for β-VAE with low (dataset
dependent) and high (10) latent dimension.
β-VAE (dep.)

β-VAE (10)

dSprites
fMNIST
MNIST
Synth. Lin.
Synth. Non-Lin.

97.8 %
99.8 %
99.8 %
99.8 %
99.9 %

90.6 %
97.7 %
99.5 %
96.7 %
98.5 %

5. Experiments

We performed several experiments with different archi-
tectures and datasets to validate our results empirically. We
show the prevalence of the polarized regime, the strong or-
thogonal effects of the (β-)VAE, as well as the links to dis-
entanglement.

5.1. Setup

Architectures. We evaluate the classical VAE, β-VAE, a
plain autoencoder, and β-VAEΣ, where the latter removes
the critical diagonal approximation (3) and produces a full
covariance matrix Σ(xi) for every sample. The resulting
KL term of the loss is changed accordingly (see Suppl. B.3
for details).

Datasets. We evaluate on the well-known datasets dSprites
[32], MNIST [27] and FashionMNIST [42], as well as on
two synthetic ones. For both synthetic tasks the input data
X is generated by embedding a unit square V = [0, 1]2
into a higher dimension. The latent representation is then
expected to be disentangled with respect to axes of V . In
one case (Synth. Lin.) we used a linear transformation

flin : R2 → R3 and in the other one a non-linear (Synth.
embedding fnon−lin : R2 → R6. The exact

Non-Lin.)
choice of transformations can be found in Suppl. B. Fur-
ther information regarding network structures and training
parameters is also provided in Suppl. B.4.

Disentanglement metric. For quantifying the disentangle-
ment of a representation, the so called Mutual Information
Gap (MIG) was introduced in [6]. As MIG is not well de-
ﬁned for continuous variables, we use an adjusted deﬁnition
comprising both continuous and discrete variables, simply
referred to as Disentanglement score. Details are described
in Suppl. B.1. Just as in the case of MIG, the Disentangle-
ment score is a number between 0 and 1, where higher value
means stronger disentanglement.

Orthogonality metric. For measuring the practical effects
of Theorem 2, we introduce a measure of non-orthogonality.
As argued in Proposition 1 and Figure 2, for a good de-
coder M and its SVD M = U ΣV ⊤, the matrix V should be
trivial (a signed permutation matrix). We measure the non-
triviality with the Distance to Orthogonality (DtO) deﬁned
as follows. For each xi, i = 1, . . . , N , employing again the

12412

ProductVolTable 2. Results for the distance to orthogonality DtO of the decoder (Equation 29) and disentanglement score for different architectures
and datasets. Lower DtO values are better and higher Disent. values are better. Random decoders provide a simple baseline for the numbers.

dSprites

Synth. Lin.

Disent. ↑
DtO ↓
Disent. ↑
DtO ↓
Synth. Non-Lin. Disent. ↑
DtO ↓
DtO ↓
MNIST
DtO ↓
fMNIST

β-VAE

VAE

AE

0.33 ± 0.15
0.76 ± 0.08
0.99 ± 0.01
0.00 ± 0.00
0.73 ± 0.16
0.18 ± 0.02

–
–

0.21 ± 0.10
1.08 ± 0.15

–
–
–
–

1.59 ± 0.08
1.36 ± 0.05

0.09 ± 0.04
1.62 ± 0.03
0.71 ± 0.19
0.33 ± 0.18
0.59 ± 0.30
0.54 ± 0.13
1.83 ± 0.05
1.87 ± 0.03

β-VAEΣ
0.12 ± 0.06
1.73 ± 0.14
0.71 ± 0.31
0.34 ± 0.35
0.42 ± 0.24
0.55 ± 0.02
1.93 ± 0.08
2.02 ± 0.08

Random Decoder

1.86 ± 0.11
0.79 ± 0.21
0.89 ± 0.16
2.11 ± 0.11
2.11 ± 0.11

Jacobian Ji of the decoder at xi and its SVD Ji = UiΣiV ⊤
i
and deﬁne

5.3. Orthogonality and Disentanglement

DtO =

1
N

N

Xi=1

kVi − P (Vi)kF ,

(29)

where k · kF is the Frobenius norm and P (Vi) is a signed
permutation matrix that is closest to V (in L1 sense). Find-
ing the nearest permutation matrix is solved to optimality
via mixed-integer linear programming (see Suppl. B.2).

5.2. Polarized regime

In Section 3.2, we assumed VAEs operate in a polarized
regime and approximated LKL, the KL term of the imple-
mented objective (4), with L≈KL (10). In Table 1 we show
that the polarized regime is indeed dominating the training
in all examples after a short initial phase. We report the
fraction of the training time in which the relative error

Now, we provide evidence for Theorem 2 by investigat-
ing the DtO (29) for a variety of architectures and datasets,
see Table 2. The results clearly support the claim that the
VAE based architectures indeed strive for local orthogo-
nality. By generalizing the β-VAE architecture, such that
the approximate posterior is any multivariate Gaussian (β-
VAEΣ), the objective becomes rotationally symmetric (just
as the idealized objective). As such, no speciﬁc alignment
is prioritized. The simple autoencoders also do not favor
particular orientations of the latent space.

Another important observation is the clear correlation
between DtO and the disentanglement score. We show this
in Figure 4 where different restarts of the same β-VAE ar-
chitecture on the dSprites dataset are displayed. We used the
state-of-the-art value β = 4 [17]. Additional experiments
are reported in Suppl. C.

∆KL = |LKL − L≈KL|

LKL

(30)

6. Discussion

stays below 3 % continuously until the end (evaluated ev-
ery 500 batches). Active variables can be selected by

pvar (µj (xi)) > 0.5.

Figure 4. Alignment of the latent representation (low DtO, (29))
results in better disentanglement (higher score). Each datapoint
corresponds to an independent run with 10, 30, or 50 epochs.

We isolated the mechanism of VAE that leads to local or-
thogonalization and, in effect, to performing local PCA. Ad-
ditionally, we demonstrated the functionality of this mecha-
nism in intuitive terms, in formal terms, and also in experi-
ments. We also explained why this behavior is desirable for
enforcing disentangled representations.

Our insights show that VAEs make use of the differences
in variance to form the representation in the latent space
– collapsing to PCA in the linear case. This does not di-
rectly encourage factorized latent representations. With this
in mind, it makes perfect sense that recent improvements of
(β-)VAE [6, 21, 40] incorporate additional terms promoting
precisely independence.

It is also unsatisfying that VAEs promote orthogonality
somewhat indirectly. It would seem that designing archi-
tectures allowing explicit control over this feature would be
beneﬁcial.

12413

0.10.20.30.40.5Disentanglement Score0.70.80.91.01.11.2DtO103050References

[1] Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon,
Rif A. Saurous, and Kevin Murphy. Fixing a broken ELBO.
In Proc. 35th Intl. Conference on Machine Learning (ICML),
volume 80, pages 159–168. PMLR, 2018. 1

[2] Y. Bengio, A. Courville, and P. Vincent. Representation
learning: A review and new perspectives. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 35(8):1798–
1828, Aug 2013. 1, 2

[3] Merlijn Blaauw and Jordi Bonada. Modeling and trans-
forming speech using variational autoencoders. In INTER-
SPEECH, pages 1770–1774, 2016. 1

[4] H. Bourlard and Y. Kamp. Auto-association by multilayer
perceptrons and singular value decomposition. Manuscript
M217, Philips Research Laboratory, Brussels, Belgium,
1987. 3

[5] Christopher P. Burgess,

Irina Higgins, Arka Pal, Lo¨ıc
Matthey, Nick Watters, Guillaume Desjardins, and Alexan-
der Lerchner. Understanding disentangling in β-vae. ArXiv
e-prints, abs/1804.03599, 2018. 1, 2, 3

[6] Tian Qi Chen, Xuechen Li, Roger B. Grosse, and David K.
Duvenaud.
Isolating sources of disentanglement in varia-
tional autoencoders. ArXiv e-prints, abs/1802.04942, 2018.
1, 2, 7, 8

[7] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel.
InfoGAN: Interpretable Representation
Learning by Information Maximizing Generative Adversar-
ial Nets. ArXiv e-prints, June 2016. 1

[8] B. Dai, Y. Wang, J. Aston, G. Hua, and D. Wipf. Hid-
den talents of the variational autoencoder. ArXiv e-prints,
abs/1706.05148, 2018. 1

[9] Bin Dai and David Wipf. Diagnosing and enhancing vae

models. ArXiv e-prints, abs/1903.05789, 2019. 1

[10] Guillaume Desjardins, Aaron Courville, and Yoshua Bengio.
Disentangling factors of variation via generative entangling.
ArXiv e-prints, abs/1210.5474, 2012. 1

[11] A. Ghadirzadeh, A. Maki, D. Kragic, and M. Bjrkman.
Deep predictive policy training using reinforcement learn-
ing.
In 2017 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS), pages 2351–2358, Sept
2017. 1

[12] Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari,
Michael Black, and Bernhard Sch¨olkopf. From variational to
deterministic autoencoders. ArXiv e-prints, abs/1903.12436,
2019. 1

[13] G. H. Golub and W. Kahan. Calculating the singular values
and pseudo-inverse of a matrix. Journal of the Society for
Industrial and Applied Mathematics: Series B, Numerical
Analysis, 2(2):205–224, 1965. 3

[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Z. Ghahra-
mani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger, editors, Advances in Neural Information Pro-
cessing Systems 27, pages 2672–2680. Curran Associates,
Inc., 2014. 1

[15] K. Gregor, F. Besse, D. Jimenez Rezende, I. Danihelka, and
D. Wierstra. Towards conceptual compression. ArXiv e-
prints, abs/1604.08772, 2016. 1

[16] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende,
and Daan Wierstra. Draw: A recurrent neural network for
image generation. In Francis Bach and David Blei, editors,
Proc. ICML, volume 37, pages 1462–1471. PMLR, 2015. 1
[17] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,
Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
Alexander Lerchner. Beta-VAE: Learning basic visual con-
cepts with a constrained variational framework. ICLR, 2017.
1, 2, 8

[18] I. Higgins, A. Pal, A. A. Rusu, L. Matthey, C. P Burgess,
A. Pritzel, M. Botvinick, C. Blundell, and A. Lerchner.
DARLA: Improving zero-shot
transfer in reinforcement
learning. ArXiv e-prints, July 2017. 1

[19] X. Hou, L. Shen, K. Sun, and G. Qiu. Deep feature consistent
variational autoencoder. In 2017 IEEE Winter Conference on
Applications of Computer Vision (WACV), pages 1133–1141,
March 2017. 1

[20] Gareth James, Daniela Witten, Trevor Hastie, and Robert
Tibshirani. An Introduction to Statistical Learning: With Ap-
plications in R. Springer Publishing Company, Incorporated,
2014. 5

[21] Hyunjik Kim and Andriy Mnih. Disentangling by fac-
torising.
In Jennifer Dy and Andreas Krause, editors,
Proc. ICML, volume 80, pages 2649–2658. PMLR, 2018. 1,
2, 8

[22] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi
Chen, Ilya Sutskever, and Max Welling.
Improved varia-
tional inference with inverse autoregressive ﬂow. In D. D.
Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
editors, Advances in Neural Information Processing Systems
29, pages 4743–4751. Curran Associates, Inc., 2016. 1

[23] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi
Chen, Ilya Sutskever, and Max Welling.
Improved varia-
tional inference with inverse autoregressive ﬂow. In D. D.
Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
editors, Advances in Neural Information Processing Systems
29, pages 4743–4751. Curran Associates, Inc., 2016. 1

[24] D. P Kingma and M. Welling. Auto-Encoding Variational

Bayes. ICLR, 2014. 1, 2

[25] Tejas D Kulkarni, William F. Whitney, Pushmeet Kohli,
and Josh Tenenbaum. Deep convolutional inverse graph-
ics network. In C. Cortes, N. D. Lawrence, D. D. Lee, M.
Sugiyama, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems 28, pages 2539–2547. Curran
Associates, Inc., 2015. 1

[26] Matt J Kusner, Brooks Paige, and Jos´e Miguel Hern´andez-
Lobato. Grammar variational autoencoder. ArXiv e-prints,
abs/1703.01925, 2017. 1

[27] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, November 1998. 7

[28] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), 2015.
1

12414

text modeling using dilated convolutions. In Doina Precup
and Yee Whye Teh, editors, Proc. ICML, volume 70, pages
3881–3890. PMLR, 2017. 1

[44] Cheng Zhang, Judith B¨utepage, Hedvig Kjellstr¨om, and
Stephan Mandt. Advances in variational inference. ArXiv
e-prints, abs/1711.05597, 2017. 1

[29] Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain
Gelly, Bernhard Sch¨olkopf, and Olivier Bachem. Chal-
lenging common assumptions in the unsupervised learn-
ing of disentangled representations.
ArXiv e-prints,
abs/1811.12359, 2018. 1

[30] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian
Goodfellow, and Brendan Frey. Adversarial autoencoders.
arXiv e-print, abs/1511.05644, 2015. 1

[31] Emile Mathieu, Tom Rainforth, N. Siddharth, and Yee
Whye Teh. Disentangling disentanglement in variational
auto-encoders. ArXiv e-prints, abs/1812.02833, 2018. 1

[32] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander
Lerchner. dsprites: Disentanglement testing sprites dataset.
https://github.com/deepmind/dsprites-dataset/, 2017. 7

[33] L. Mescheder, S. Nowozin, and A. Geiger. Adversarial vari-
ational bayes: Unifying variational autoencoders and gener-
ative adversarial networks.
In Proceedings of the 34th In-
ternational Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research. PMLR, Aug.
2017. 1

[34] A. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, and S. Levine.
Visual Reinforcement Learning with Imagined Goals. ArXiv
e-prints, abs/1807.04742, July 2018. 1

[35] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka,
Karol Gregor, and Daan Wierstra. One-shot generalization
in deep generative models. ArXiv e-prints, abs/1603.05106,
2016. 1

[36] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wier-
stra. Stochastic backpropagation and approximate inference
in deep generative models. ICML, 2014. 1

[37] Karl Ridgeway. A survey of inductive biases for facto-
rial representation-learning. ArXiv e-prints, abs/1612.05299,
2016. 2

[38] Jrgen Schmidhuber. Learning factorial codes by predictabil-
ity minimization. Neural Computation, 4(6):863–879, 1992.
2

[39] Iulian Vlad Serban, Chinnadhurai Sankar, Mathieu Germain,
Saizheng Zhang, Zhouhan Lin, Sandeep Subramanian, Tae-
sup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary
Ke, Sai Mudumba, Alexandre de Br´ebisson, Jose Sotelo,
Dendi Suhubdy, Vincent Michalski, Alexandre Nguyen,
Joelle Pineau, and Yoshua Bengio. A deep reinforcement
learning chatbot. ArXiv e-prints, abs/1709.02349, 2017. 1

[40] Jan St¨uhmer, Richard Turner, and Sebastian Nowozin. ISA-
VAE: Independent subspace analysis with variational au-
toencoders.
In Submitted to International Conference on
Learning Representations, 2019. 1, 2, 3, 8

[41] H. van Hoof, N. Chen, M. Karl, P. van der Smagt, and
J. Peters. Stable reinforcement learning with autoencoders
for tactile and visual data. In 2016 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pages
3928–3934, Oct 2016. 1

[42] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-
MNIST: a novel image dataset for benchmarking machine
learning algorithms. ArXiv e-prints, abs/1708.07747, 2017.
7

[43] Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Tay-
lor Berg-Kirkpatrick. Improved variational autoencoders for

12415

