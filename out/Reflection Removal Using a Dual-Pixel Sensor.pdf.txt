Reﬂection Removal Using a Dual-Pixel Sensor

Abhijith Punnappurath

York University

Michael S. Brown
York University

pabhijith@eecs.yorku.ca

mbrown@eecs.yorku.ca

Abstract

Reﬂection removal is the challenging problem of remov-
ing unwanted reﬂections that occur when imaging a scene
that is behind a pane of glass.
In this paper, we show
that most cameras have an overlooked mechanism that can
greatly simplify this task. Speciﬁcally, modern DLSR and
smartphone cameras use dual pixel (DP) sensors that have
two photodiodes per pixel to provide two sub-aperture views
of the scene from a single captured image. “Defocus-
disparity” cues, which are natural by-products of the DP
sensor encoded within these two sub-aperture views, can be
used to distinguish between image gradients belonging to
the in-focus background and those caused by reﬂection in-
terference. This gradient information can then be incorpo-
rated into an optimization framework to recover the back-
ground layer with higher accuracy than currently possible
from the single captured image. As part of this work, we
provide the ﬁrst image dataset for reﬂection removal con-
sisting of the sub-aperture views from the DP sensor.

1. Introduction

This paper addresses the problem of removing reﬂec-
tion interference that occurs when imaging a scene behind
a pane of glass. The novelty of our work lies in our use of
the information available from dual pixel (DP) sensors that
are found on most smartphone and DSLR cameras. Tra-
ditional image sensors have a single photodiode per pixel
site. DP sensors have two photodiodes that effectively split
the pixel in half. The DP sensor design furnishes, from a
single captured image, two views of the scene where rays
passing through the left side of the lens are captured by the
right half-pixels (right sub-aperture view) and those pass-
ing through the right side of the lens are captured by the left
half-pixels (left sub-aperture view).

The DP sensor is effectively a rudimentary two-sample
light-ﬁeld camera. Within this context, scene points that are
in-focus will have no difference between their positions in
the left and right sub-aperture views. However, out-of-focus
scene points will be blurred in opposite directions in the two

Captured input image 

Left (L) sub‐aperture view

Right (R) sub‐aperture view

L 

R 

L 

R 

Estimated background 

gradient map 
based on L/R 

defocus‐disparity cues

L/R disparity for background

L/R disparity for reflection

Ground truth

background layer

Our estimated
background layer

Our estimated
reflection layer

Figure 1. An example sketching our basic idea. The captured im-
age and its two sub-aperture views are shown. In the zoomed-in
boxes, the upper half corresponds to the left view, and the lower
half to the right. In the box on the right showing an out-of-focus
reﬂection region, a horizontal shift can be observed between the
two white dots (best viewed electronically and zoomed), while no
disparity exists in the left box of an in-focus background region.
This disparity (illustrated in the plots) allows us to compute a mask
for image gradients belonging to the background region that can
be used to extract the background layer.

sub-aperture views, resulting in very small but detectable
shifts. These shifts, which we refer to as defocus-disparity
cues, are related to the amount of out-of-focus blur incurred
by the scene point with respect to the camera lens’s depth of
ﬁeld. These defocus-disparity cues, which are natural by-
products of the DP sensor, allow us to robustly determine
which gradients in the captured composite image belong to
the in-focus background layer. Fig. 1 shows an example.

11556

Contribution We introduce a new reﬂection removal
method that exploits the two sub-aperture views available
on a DP sensor. We explain the relationship between the
defocus-disparity cues in the two sub-aperture views with
respect to the background layer and the objects reﬂected
by the glass. Working from this backdrop, we propose a
method that uses these defocus-disparity cues to detect gra-
dients corresponding to the in-focus background and incor-
porate them into an optimization framework to recover the
background layer. Our experimental results demonstrate
the advantages of this additional information over current
methods. More importantly, our results are obtained with-
out hardware modiﬁcations or training – we simply use the
data that was already available, yet ignored. As part of this
work, we introduce a new dataset for reﬂection removal that
provides access to the two sub-aperture views.

1.1. Related work

We ﬁrst provide a brief overview of the original function-
ality of DP sensors, and their extended capabilities. We also
discuss single- and multi-image reﬂection removal methods
as well as methods using light-ﬁeld cameras as the DP sen-
sor can be considered a two-sample per pixel light-ﬁeld.
DP sensor Dual pixel sensors were developed to provide a
fast method for autofocus [13, 28], the idea here being that
by examining the image disparity between the two views,
a change in lens position can be calculated to minimize the
amount of out-of-focus blur, thus focusing the image. How-
ever, the DP data can be used for tasks beyond autofocus.
Recent work by Wadhwa et al. [30] showed how the two
DP views can be used to extract dense depth maps for the
purpose of synthesizing shallow depth-of-ﬁeld images.
Reﬂection removal
Single image Most single-image methods exploit the statis-
tics of natural images to make the reﬂection removal prob-
lem less ill-posed. Long-tail distribution of gradients [18],
sparsity of corners and edges [20], the ghosting effect [27],
difference in smoothness between the background and re-
ﬂection layers [22], and depth of ﬁeld conﬁdence maps [33]
are some of the priors that have been employed.

More recently, deep learning techniques have also been
applied to this task [9, 40, 32, 39]. Fan et al. [9] ﬁrst learn
an intermediate edge map to guide background recovery,
whereas Wan et al. [32] combine the two stages of gradient
and image inference into a uniﬁed framework. While Zhang
et al. [40] seek to use both low- and high-level image infor-
mation, Yang et al. [39] estimate both the background and
the reﬂection layers in cascade. Although much progress
has been made in single-image reﬂection removal, there is
still a large margin for improvement due to the highly ill-
posed nature of the problem.
Multiple images Capturing multiple images of the scene in
a pre-deﬁned manner can make the reﬂection removal prob-

lem more tractable. The vast majority of multi-image meth-
ods are based on motion cues [12, 5, 10, 11, 21, 29, 37].
These methods take advantage of the difference in motion
between the two layers given images of the same scene
taken from different viewpoints. Prior works have mod-
eled the motion of the two layers as pure translation [5],
afﬁne [10], or a full homography [11]. Recent approaches
[12, 21, 29, 37] have replaced these parametric models with
dense per-pixel motion vectors. Methods that require spe-
cialized hardware or non-conventional capture settings have
also been proposed – using a polarizer [26, 24, 15, 8], vary-
ing focus [25], capturing a ﬂash no-ﬂash pair [2, 3] and so
forth. Although these multi-image approaches produce bet-
ter results due to the availability of additional information,
they place the burden on the photographer to acquire special
hardware or skills, and thereby vastly limit their applicabil-
ity to lay users.
Light-ﬁeld cameras While layer separation is ill-posed
with conventional imaging, the task becomes tractable with
light ﬁeld imaging as demonstrated by recent works [34,
14, 6, 23]. Wang et al. [34] built their own portable cam-
era array to obtain an image stack for reﬂection removal.
Johannsen et al.
[14] propose a variational approach for
layer separation assuming user assistance in gradient label-
ing. Chandramouli et al. [6] advocate a deep learning ap-
proach to recover the scene depth, as well as the two layers.
Ni et al. [23] use focus manipulation to remove the reﬂec-
tions. The drawback of these methods is the need for spe-
cialized light ﬁeld cameras. Our method, in contrast, works
by using information available on DP sensors of most cur-
rent commodity cameras.

2. Image formation model with DP sensor

A DP sensor splits a single pixel in half using an arrange-
ment of a microlens sitting atop two photodiodes. See Fig.
2(a). The two halves of the dual pixel – the two photodiodes
– can detect light and record signals independently. When
the two signals are summed together, the pixel intensity that
is produced will match the value from a normal single diode
sensor. The split-pixel arrangement has the effect that light
rays from the left half of the camera lens’s aperture will be
recorded by the right half of the dual pixel, and vice versa.
A scene point that is out of focus will experience a dis-
parity or shift between the left and right views due to the
circle of confusion that is induced. It is precisely this shift
that is exploited by DP auto-focus systems. By examin-
ing the signed average disparity value within a region of
interest, the auto-focus algorithm can determine not only in
which direction to move the lens in order to bring that re-
gion into focus (and thereby minimize disparity) but also by
how much.

Within this backdrop, we examine the image formation
model for a DP sensor imaging a scene through a trans-

1557

Glass

Camera

Dual pixel

sensor array

DOF

Main lens

7
6
5

4
3

2

1
0

No disparity

y
t
i
s
n
e
t
n

I

2

2

1

y
t
i
s
n
e
t
n

I

Blur size

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

Position on sensor

(b) DP data background

Position on sensor

(d) DP data reflection

(f) DP left view

4

y
t
i
s
n
e
t
n

I

2

1

y
t
i
s
n
e
t
n

I

One

dual pixel

0 1 2 3 4 5 6 7

Position on sensor

unit

(c) Image data background

0 1 2 3 4 5 6 7

Position on sensor

(e) Image data reflection

(g) DP right view

Microlens

Right photodiode
Left photodiode

(a)

Background object

Reflected object

Reflection

(f+)

(g+)

(h+)

(h) Observed image

Figure 2. Image formation model for a dual-pixel camera capturing a scene behind glass.
(a) An in-focus background scene point is
recorded at pixel site 1, while an out-of-focus reﬂection scene point creates a defocus blur spread across pixels sites 2 to 6. Light from
opposite halves of the lens is collected by the left and right half-pixels. There is no disparity for the background scene point (b), whereas
a disparity proportional to the blur size is induced by the reﬂection (d). The sum of the left and right half-pixels represents the observed
image intensity at that pixel site (c), (e). The DP view and the observed image are a superposition of the background object and the reﬂected
object (f-h). The shift in the reﬂection between the two views is evident from the position of the tip of the triangle (f+,g+).

parent glass. A dense DP sensor array effectively yields
views through the left and right halves of the lens from a
single capture. Depending on the sensor’s orientation, this
can also be the upper and lower halves of the lens; without
loss of generality, we consider them to be the left and right
views in the rest of the paper.

We make the following two assumptions. First, we as-
sume the background layer has predominately stronger im-
age intensity than the reﬂection layer. This assumption is
made by all reﬂection removal algorithms. Second, we as-
sume the background scene content lies within the depth of
ﬁeld (DOF) of the camera, while the objects in the scene be-
ing reﬂected on the glass are at a different depth and there-
fore outside the DOF. The second assumption is also com-
mon [22, 33, 38, 7, 9, 39], and as noted by Wan et al. [31], it
is quite reasonable to presume that the background and the
objects in front of the glass have different distances from
the camera.
In such a scenario, the observed image is a
superposition of the in-focus background and a de-focused
reﬂection.

Based on these assumptions, we illustrate the image for-
mation model in Fig. 2(a) for a DP camera imaging a scene
through glass. A point on the in-focus background object
emits light that travels through the camera’s lens and is fo-
cused on the sensor at a single pixel (labeled as 1). Observe
from the ﬁgure that rays that pass through the right half of
the main lens aperture hit the microlens at an angle such that
they are directed into the left half-pixels. The same applies
to the left half of the aperture and the right half-pixels. For
an in-focus scene point, there is no disparity (Fig. 2(b)).
The sum of the left and right values is stored as the image
intensity at that pixel (Fig. 2(c)).

Next, consider the triangular object in front of the glass
that constitutes the reﬂection layer. Light from a point on
this object focuses in front of the sensor, and produces a
ﬁve-pixel wide (labeled 2 to 6) defocus-blurred image on
the sensor. The left and right views created by the split-
pixels have a disparity that is proportional to the blur size
(Fig. 2(d)). The blurred reﬂection image is obtained by
summing up the left and right signals (Fig. 2(e)). The com-
posite DP data that is the sum of the in-focus background
(with no disparity) and the out-of-focus reﬂection (with a
non-zero disparity) as observed from the left and right views
over the entire imaging sensor is shown in Figs. 2(f,g). No-
tice the shift between views as highlighted by the zoomed-in
regions (f+,g+). The ﬁnal image output by the camera that
is the sum of left and right DP views is also shown in Fig.
2(h), and its zoomed-in region in (h+).

If b represents the background layer and f denotes the la-
tent sharp reﬂection layer, both in lexicographically ordered
vector form, the composite left gLV and right gRV DP views
can be expressed mathematically as

gLV =

b
2

+ WLVf , gRV =

b
2

+ WRVf ,

(1)

where WLV and WRV are the matrices that multiply the un-
derlying sharp reﬂection layer f to produce its defocused
and shifted versions of half intensity in the left and right
views, respectively. The observed image g can be expressed
as g = gLV + gRV = b + r, where r equals the blurred re-
ﬂection layer and is given by r = (WLV + WRV)f .

1558

Figure 3. Input images and our estimated weighted gradient maps
of the background.

3. Proposed method

Working from our previous section, we describe our re-
ﬂection removal method that leverages the a priori knowl-
edge that (i) the background layer is sharp and has zero dis-
parity, and (ii) the reﬂection layer is defocus-blurred and has
a non-zero disparity between the left and right DP views.

3.1. Defocus disparity cues

Levin et al. [18] demonstrated that labeling the gradients
of the input image can serve as a powerful mechanism for
reﬂection removal. However, the labeling was done manu-
ally by the user. Inspired by the success of [18], we propose
to use the defocus-disparity cues between the left and right
DP views to automatically identify which gradients belong
to the background layer.

Let the gradients of the left and right DP views obtained
by applying the ﬁrst-order horizontal and vertical derivative
ﬁlters be represented as hLV and hRV. To compute disparity,
we select a patch of size N ×N pixels in hLV and perform
a horizontal search over a range of −t to t pixels in hRV.
A 1D search sufﬁces because the split-pixels produce an al-
most pure horizontally rectiﬁed disparity in the sensor’s ref-
erence frame. The search interval 2t + 1 can be restricted to
a few pixels because the baseline between DP views is very
narrow (approximately equal to aperture diameter [30]). We
compute the sum of squared differences (SSD) for each in-
teger shift. Following [30], we ﬁnd the minimum of these
2t+1 points and ﬁt a quadratic 1
2 a1x2 +a2x+a3 to the SSD
value using the minimum and its two surrounding points.
At a given pixel i, the location of the quadratic’s minimum
si = −a2
serves as our sub-pixel minimum. We also com-
a1
pute a conﬁdence value at each pixel i as [4]:

βi = exp  log|a1i|
σa1 −

a3i
σ2

a3!.

(2)

We construct our weighted gradient map of the background
using the conﬁdence values βi as

ci =(ρβi

0

if |si| < ǫ and βi > 1,
otherwise.

(3)

and σa3 = 256 for all examples presented in this paper.
Note that blurred reﬂection gradients are weak [22, 38, 7],
and very few can be reliably labeled. In our experiments, we
did not observe any improvement in the results by adding
labeled reﬂection gradients to our cost function, and there-
fore, we do not include them in our gradient map.

Although our disparity estimation technique is similar
in spirit to [30], our use of gradients instead of image in-
tensities is a notable departure from their approach. Fur-
thermore, they employ several heuristics (e.g., repeated tex-
ture, lack of texture, outlier motion) to compute conﬁdence,
whereas our conﬁdence estimates are based directly on the
quadratic ﬁt.

3.2. In focus and defocus image distributions

The difference in sharpness between the background and
reﬂection layers provides yet another valuable cue for layer
separation. This idea was explored in [22]. The defocused
reﬂection layer has fewer large gradients than the in-focus
background. Following [22], we model the blurred reﬂec-
tion layer’s gradient distribution using a Gaussian function
with a narrow spread as

PR(l) =

1

√2πσ2

2

e− l

2σ2 ,

(4)

where l represents the gradient value, and σ denotes the
standard deviation of the Gaussian.

It is well known that the gradients of natural images have
a heavy-tailed distribution, and that this distribution can be
modeled using a hyper-Laplacian function [16, 17]. There-
fore, we express the probability distribution of the gradients
of the in-focus background layer as

PB(l) = e−α|l|p

,

(5)

where α is a positive scalar, and we set p = 2

3 [16].

The work by [22] also applies different distributions on
the gradients of the two layers. However, they model even
the background’s gradient distribution using a Gaussian.
They then force the distribution to have a tail by applying
the max operator and preventing the gradients from getting
close to zero. In comparison, our use of the hyper-Laplacian
distribution more naturally encourages large gradients in
the background. Furthermore, [22] relies purely on rela-
tive smoothness, and so their method fails in cases where
there is not a clear difference in sharpness between the two
layers (see example 1 of Fig. 4). Our proposed method as-
similates additional information about the gradients using
disparity as a cue, and yields stable performance even when
the reﬂection layer is only slightly out-of-focus.

3.3. Cost function

Two examples of our estimated background gradient maps
are shown in Fig. 3. We ﬁx ρ = 5, N = 11, t = 5, σa1 = 5,

Our cost function uses a probabilistic model to seek the
most likely explanation of the superimposed image using

1559

Algorithm 1 Reﬂection removal using a dual-pixel sensor
Input image g, the left gLV and right gRV DP
Input: :

views, relative weight λ, maximum iterations Q.

Output: : Background b, and blurred reﬂection r.

1: Compute C using gLV and gRV (see Section 3.1)
2: D∗ = CD
3: q = 0
4: b = (DT D + λD∗
5: do

T D∗)−1(λD∗

T D∗g)

6:

ei =(cid:16)max(cid:0)|(Db)i|, 0.001(cid:1)(cid:17)(p−2)

7: E = diag(ei)
8:

b = (DT ED + λD∗
q + +

T D∗)−1(λD∗

T D∗g)

9:
10: while q < Q
11: r = g − b

the probabilities of the background and reﬂection layers.
Speciﬁcally, we maximize the joint probability P (b, r). As-
suming that the background and the reﬂection are indepen-
dent [19], the joint probability can be expressed as the prod-
uct of the probabilities of each of the two layers – that is,
P (b, r) = P (b)P (r). Following [36], we deﬁne our distri-
bution over both background and reﬂection layers using the
histogram of derivative ﬁlters as

P (z) ≈Yi,k

P(cid:0)(Dkz)i(cid:1),

z = either b or r,

(6)

where we assume that the horizontal and vertical derivative
ﬁlters Dk ∈ {Dx, Dy, Dxx, Dxy, Dyy} are independent
over space and orientation.
Maximizing P (b, r) is equal to minimizing its negative
log, and from equations (4)(5)(6), we obtain the following
cost function:

Equation (9) can be solved using iterative reweighed least
squares, and the steps are outlined in Algorithm 1. In all
our experiments, the optimization converges quickly within
a few iterations. Note that our cost function is based purely
on gradients. Therefore, we ﬁnally rescale the recovered
background and reﬂection images based on the input im-
age’s intensity range.

We would like to add that we chose not to include any
explicit image ﬁdelity terms based on the image formation
model in equation (1) inside our cost function. The defo-
cus blurring operation encoded by the matrices WLV and
WRV can be space-varying depending on the depth of the
reﬂected object. A per-pixel-varying defocus kernel is hard
to reliably estimate from the composite image. Moreover,
the blur size is a function of aperture (see equation 3 of
[30]). Observe that our cost function based on gradients
is not aperture-speciﬁc, does not entail complex per-pixel
depth estimation, and is straightforward to optimize.

4. Experiments

There are no publicly available datasets for reﬂection re-
moval that provide dual pixel data. Therefore, to evaluate
the performance of our proposed algorithm, we capture our
own dataset using a dual pixel camera. Although DP tech-
nology exists on most modern cameras, the vast majority of
these devices do not provide users access to DP data. This
is primarily because DP autofocus occurs at the very early
stages of the camera pipeline and the current raw readout
hardware combines the information to mimic a single read-
out for each pixel. As far as we are aware, there is no direct
Camera2 API [1] call to read off the DP measurements even
from the Google Pixel 2 phone used in [30]. As a result, we
use the Canon EOS 5D Mark IV DSLR camera, one of the
few commercially available cameras that provides access to
sensor’s DP data, to capture our dataset.

arg min

b,r (Xi,k (cid:18)|(Dkb)i|p + λ(cid:0)(Dkr)i(cid:1)2(cid:19)),

where we integrate the relative weight between the two
2ασ2 into a single parameter λ,
terms and the multiplier
which controls the amount of defocus blur in the reﬂection
layer. This can be rewritten as

1

arg min

b,r n||Db||p

2o,
p + λ||Dr||2

(8)

where the matrix D consists of the ﬁve Dks vertically
stacked. Expressing in terms of a single layer b, and in-
corporating the conﬁdence values C = diag(ci) from equa-
tion (3) to enforce agreement with the labeled gradients, we
obtain

arg min

b

n||Db||p

2o.
p + λ||CD(g − b)||2

(9)

(7)

4.1. Data capture

Our dataset is divided into two categories – controlled
indoor scenes with ground truth and scenes captured “in the
wild”. Following the data capture methodology adopted
by the recent single-image reﬂection removal benchmark
dataset of [31], we use different postcards as background
and reﬂection (see Fig. 4) for the controlled dataset. We
select postcards with texture ranging from medium to high
for both background and reﬂection, and combine them pair-
wise in a manner that our dataset has a wide diversity of
complex overlapping textures. In particular, we select six
postcards for background and ﬁve postcards for reﬂection,
for a total of 30 different scenes.

The defocus blur size and the disparity are functions of
the aperture. To evaluate our algorithm’s robustness to de-
gree of defocus blur and extent of disparity, we also vary the
aperture value. Speciﬁcally, we select ﬁve different aperture

1560

Figure 4. Examples from our controlled dataset.

sizes {F13, F10, F8, F5.6, F4}. In the supplementary ma-
terial, we provide animations that switch between the two
DP views to better reveal how the defocus blur and dispar-
ity change with aperture. For each of the 30 scenes, we
capture images using these ﬁve different apertures, giving
us a total of 150 images for the controlled dataset. In or-
der to make the controlled scenes even more challenging,
we place a light source close to the postcard in front of the
glass to boost the interference from the reﬂection [31]. The
ground truth background layer is captured with the portable
glass pane removed.

While a controlled setup allows for a quantitative eval-
uation of our proposed method as well as competing algo-
rithms, these scenes do not necessarily reﬂect the complex-
ities encountered in images captured in an unconstrained
manner. Therefore, we augment our dataset with images

captured in the wild (see Fig. 5 for some examples). For
the in-the-wild category, we found it difﬁcult to capture the
ground truth (due to motion in the scene, the glass pane be-
ing ﬁxed, etc.), and so we analyse results only qualitatively.

4.2. Comparisons

We compare our results with six contemporary reﬂection
removal algorithms – four single-image algorithms, LB14
[22], WS16 [33], ZN18 [40], and YG18 [39], and two
motion-based multi-image algorithms, LB13 [21], GC14
[11]. The codes for all six methods have been made publicly
available by the authors. For the single-image algorithms,
we use the default parameters mentioned in their paper or
provided in the original code, and feed the captured image
as input. We chose the conventional methods of LB14 [22]
and WS16 [33] for comparison because they operate un-

1561

BackgroundBackgroundBackgroundReflectionReflectionReflectionGround truthInputGround truthInputInputGround truthF13F8F4LB13GC14LB14WS16ZN18YG18OursFigure 5. Examples from our in-the-wild dataset.

der the same assumptions that we do; the background is
sharp and the reﬂection is defocused. YG18 [39] and ZN18
[40] are the two most recent deep learning methods for
single-image reﬂection removal with state-of-the-art perfor-
mance. Since the two sub-aperture views are available to us
from the DP sensor, and these are essentially two differ-
ent viewpoints of the scene, we also compare against the
multi-image methods of LB13 [21] and GC14 [11] which
exploit motion cues for layer separation. For a fair com-
parison against these methods, we restricted their search
space to pure translation instead of a full homography. We
provide the left and right DP views as input to the multi-
image methods because the change in viewpoint is highest
between these two images. In our experiments, including
the input image along with the DP views did not improve
their performance. Code for the light-ﬁeld camera-based
methods discussed in Section 1.1 is not publicly available.

4.3. Error metrics

We quantitatively compare the results of our proposed al-
gorithm as well as competing techniques on the controlled
dataset. We evaluate performance using several metrics:
(i) peak signal to noise ratio (PSNR) and (ii) structural
similarity index (SSIM) [35] are the two most commonly
employed. Following [31], we also use (iii) local mean
squared error as a similarity measure (sLMSE), (iv) normal-
ized cross correlation (NCC), and (v) structure index (SI).
Please refer to [31] for more details of metrics (iii) to (v).

4.4. Results on controlled scenes

The performance of LB13 [21], GC14 [11], LB14 [22],
WS16 [33], ZN18 [40], YG18 [39], and our proposed
method on the 150 images in the controlled category of
our dataset for the ﬁve error metrics is recorded in Table
1.
It can be observed that we outperform competing ap-
proaches by a sound margin on all metrics. Fig. 4 shows

1562

BackgroundBackgroundBackgroundReflectionReflectionReflectionInputInputInputLB14WS16ZN18YG18OursOur estimatedgradient mapOur estimated gradient mapOur estimated gradient mapGC14LB13Method

PSNR SSIM sLMSE NCC
(dB)

LB13 [21]
GC14 [11]
LB14 [22]
WS16 [33]
ZN18 [40]
YG18 [39]

Ours

16.12
16.02
14.20
16.62
15.57
16.49
19.45

0.689
0.798
0.842
0.836
0.797
0.832
0.883

0.870
0.888
0.797
0.884
0.867
0.871
0.946

0.966
0.945
0.981
0.975
0.979
0.978
0.982

SI

0.758
0.496
0.840
0.837
0.818
0.847
0.870

Table 1. Quantitative results on our controlled dataset.

three representative examples from our controlled set with
three different aperture settings. We noticed that the multi-
image methods LB13 [21] and GC14 [11] do not perform
well in general because both methods rely on large changes
in viewpoint, whereas the baseline between the DP views is
very narrow. The ﬁrst row of Fig. 4 shows an example cap-
tured at F13 aperture value. Although the background does
not have a lot of texture, the reﬂection is sharp due to the
narrow aperture, and ZN18 [40] and YG18 [39] have traces
of reﬂection in the top right of the image.
It can be ob-
served from the zoomed-in regions in the second row that
LB14 [22] and WS16 [33] both also have residual reﬂec-
tion. In comparison, our method recovers both background
and reﬂection (shown in the third row) more accurately.

Another example with a highly textured background as
well as reﬂection captured at the F8 aperture is shown next.
Competing techniques erroneously remove either too little
(red box LB14 [22]) or too much (green box YG18 [39])
detail from the background, or miscalculate the overall con-
tribution of the reﬂection layer. Our output more closely
matches the ground truth when compared to other algo-
rithms. The third example shot at the F4 aperture is more
challenging because although the reﬂection is blurred, it
covers a signiﬁcant portion of the heavily textured back-
ground. All methods suffer from a loss of detail in this case.
However, our method still produces a fairly good separation
of the background and reﬂection layers.

4.5. Results on in the wild scenes

Fig.

5 shows three examples from our in-the-wild
dataset. Since there is no ground truth, we provide zoomed-
in regions corresponding to background (green) and reﬂec-
tion (red) for a visual comparison of various algorithms.
Our estimated background gradient map is also shown. It
can be observed that our method performs consistently well
as opposed to competing techniques. More results are pro-
vided in the supplementary material.

We ﬁx the parameters λ = 100 and Q = 3 for all ex-
periments in this paper. On a 3.10 GHz processor with 32
GB RAM, our MATLAB algorithm takes approximately 2

(a)

(b)

(c)

(d)

Figure 6. (a) Input image, (b) our estimated background, (c) our
estimated reﬂection, and (d) depth map of the reﬂection layer.

minutes to process an 800 × 800 image.
An interesting extension of our work is the ability to re-
cover a coarse depth map of the reﬂected scene. An ex-
ample is demonstrated in Fig. 6. By subtracting out the
estimated background from the left and right views, we can
obtain the reﬂected scene as observed from the left and right
views (see equation 1). These two images can then be used
to extract a depth map of the reﬂected scene following the
disparity estimation technique of Wadhwa et al. [30].

5. Discussion and summary

We have proposed a method to perform reﬂection re-
moval by exploiting the data available on a DP sensor.
We used the defocus-disparity cues present in the two sub-
aperture views to simplify the task of determining which
image gradients belong to the background layer. This well-
labeled gradient map allows our optimization scheme to
recover the background layer more accurately than other
methods that do not use this additional information. The
best part of our approach is that it does not require hard-
ware modiﬁcations or training – instead it uses data already
available within each camera shot. The only downside is
most camera APIs currently do not provide access to this
useful data. We hope this work will inspire manufacturers
to provide access. In the meantime, we offer a new dataset
for reﬂection removal that provides the two DP sub-aperture
views.

We do note that our defocus-disparity cues are based
on the assumption that the reﬂection layer is out of focus.
Thus, one limitation of our approach is that we cannot fully
distinguish between the gradients of the two layers if the
background and the scene being reﬂected are at nearly equal
distances from the glass – that is, both layers are in sharp fo-
cus, and the disparity is too small to be detected. One idea
for future work is to use focus bracketing to combine mul-
tiple DP images for improved layer recovery.

Acknowledgment

This study was funded in part by the Canada First Re-
search Excellence Fund for the Vision: Science to Applica-
tions (VISTA) programme and an NSERC Discovery Grant.

1563

References

[1] Google,

Inc.:

Camera2 API Package Summary.

http://developer.android.com/reference/
android/hardware/camera2/. Accessed: 2016-07-
16.

[2] A. Agrawal, R. Raskar, and R. Chellappa. Edge suppres-
sion by gradient ﬁeld transformation using cross-projection
tensors. In CVPR, 2006.

[3] Amit Agrawal, Ramesh Raskar, Shree K. Nayar, and
Yuanzhen Li. Removing photography artifacts using gradient
projection and ﬂash-exposure sampling. ACM Transactions
on Graphics, 24:828–835, 2005.

[4] Robert Anderson, David Gallup, Jonathan T Barron, Janne
Kontkanen, Noah Snavely, Carlos Hern´andez, Sameer Agar-
wal, and Steven M Seitz. Jump: Virtual reality video. SIG-
GRAPH Asia, 2016.

[5] E. Be’ery and A. Yeredor. Blind separation of superim-
posed shifted images using parameterized joint diagonaliza-
tion.
IEEE Transactions on Image Processing, 17(3):340–
353, 2008.

[6] Paramanand Chandramouli, Mehdi Noroozi, and Paolo
Favaro. ConvNet-based depth estimation, reﬂection separa-
tion and deblurring of plenoptic images. In ACCV, 2016.

[7] Y. Chung, S. Chang, J. Wang, and S. Chen.

Interference

reﬂection separation from a single image. In WACV, 2009.

[8] Y. Diamant and Y. Y. Schechner. Overcoming visual rever-

berations. In CVPR, 2008.

[9] Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, and
David Wipf. A generic deep architecture for single image
reﬂection removal and image smoothing. In ICCV, 2017.

[10] K. Gai, Z. Shi, and C. Zhang. Blind separation of su-
perimposed moving images using image statistics.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
34(1):19–32, 2012.

[11] X. Guo, X. Cao, and Y. Ma. Robust separation of reﬂection

from multiple images. In CVPR, 2014.

[12] B. Han and J. Sim. Glass reﬂection removal using co-
saliency-based image alignment and low-rank matrix com-
pletion in gradient domain.
IEEE Transactions on Image
Processing, 27(10):4873–4888, 2018.

[13] Jinbeum Jang, Yoonjong Yoo, Jongheon Kim, and Joonki
Paik. Sensor-based auto-focusing system using multi-scale
feature extraction and phase correlation matching. Sensors,
15(3):5747–5762, 2015.

[14] Ole Johannsen, Antonin Sulc, and Bastian Goldluecke. Vari-
ational separation of light ﬁeld layers. In Vision, Modeling
and Visualization. The Eurographics Association, 2015.

[15] N. Kong, Y. Tai, and J. S. Shin. A physically-based approach
to reﬂection separation: From physical modeling to con-
strained optimization. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 36(2):209–221, 2014.

[16] Dilip Krishnan and Rob Fergus. Fast image deconvolution

using hyper-laplacian priors. In NIPS, 2009.

[17] Anat Levin, Rob Fergus, Fr´edo Durand, and William T. Free-
man.
Image and depth from a conventional camera with
a coded aperture. ACM Transactions on Graphics, 26(3),
2007.

[18] A. Levin and Y. Weiss. User assisted separation of re-
ﬂections from a single image using a sparsity prior.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
29(9):1647–1654, 2007.

[19] Anat Levin, Assaf Zomet, and Yair Weiss. Learning to per-
ceive transparency from the statistics of natural scenes. In
NIPS, 2002.

[20] A. Levin, A. Zomet, and Y. Weiss. Separating reﬂections

from a single image using local features. In CVPR, 2004.

[21] Y. Li and M. S. Brown. Exploiting reﬂection change for au-

tomatic reﬂection removal. In ICCV, 2013.

[22] Y. Li and M. S. Brown. Single image layer separation using

relative smoothness. In CVPR, 2014.

[23] Y. Ni, J. Chen, and L. Chau. Reﬂection removal on single
light ﬁeld capture using focus manipulation. IEEE Transac-
tions on Computational Imaging, 4:562–572, 2018.

[24] Bernard Sarel and Michal Irani. Separating transparent lay-

ers through layer information exchange. In ECCV, 2004.

[25] Yoav Y. Schechner, Nahum Kiryati, and Ronen Basri. Sepa-
ration of transparent layers using focus. International Jour-
nal of Computer Vision, 39(1):25–39, 2000.

[26] Y. Y. Schechner, J. Shamir, and N. Kiryati. Polarization-
based decorrelation of transparent layers: The inclination an-
gle of an invisible surface. In ICCV, 1999.

[27] YiChang Shih, D. Krishnan, F. Durand, and W. T. Freeman.

Reﬂection removal using ghosting cues. In CVPR, 2015.

[28] Przemysław ´Sliwi´nski and Paweł Wachel. A simple model
for on-sensor phase-detection autofocusing algorithm. Jour-
nal of Computer and Communications, 1(06):11, 2013.

[29] Chao Sun, Shuaicheng Liu, Taotao Yang, Bing Zeng,
Zhengning Wang, and Guanghui Liu. Automatic reﬂection
removal using gradient intensity and motion cues. In ACM
Multimedia, 2016.

[30] Neal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E.
Feldman, Nori Kanazawa, Robert Carroll, Yair Movshovitz-
Attias, Jonathan T. Barron, Yael Pritch, and Marc Levoy.
Synthetic depth-of-ﬁeld with a single-camera mobile phone.
ACM Transactions on Graphics, 37(4):64:1–64:13, 2018.

[31] R. Wan, B. Shi, L. Duan, A. Tan, and A. C. Kot. Benchmark-
In ICCV,

ing single-image reﬂection removal algorithms.
2017.

[32] Renjie Wan, Boxin Shi, Ling-Yu Duan, Ah-Hwee Tan, and
Alex C. Kot. CRRN: multi-scale guided concurrent reﬂec-
tion removal network. In CVPR, 2018.

[33] R. Wan, B. Shi, T. A. Hwee, and A. C. Kot. Depth of ﬁeld

guided reﬂection removal. In ICIP, 2016.

[34] Qiaosong Wang, Haiting Lin, Yi Ma, Sing Bing Kang, and
Jingyi Yu. Automatic layer separation using light ﬁeld imag-
ing. arXiv, 2015.

[35] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.
Simoncelli. Image quality assessment: From error visibility
to structural similarity. IEEE Transactions on image process-
ing, 13(4):600–612, 2004.

[36] Y. Weiss. Deriving intrinsic images from image sequences.

In ICCV, 2001.

[37] Tianfan Xue, Michael Rubinstein, Ce Liu, and William T.
Freeman. A computational approach for obstruction-free

1564

photography. ACM Transactions on Graphics, 34(4):79:1–
79:11, 2015.

[38] Q. Yan, Y. Xu, X. Yang, and T. Nguyen. Separation of weak
reﬂection from a single superimposed image. IEEE Signal
Processing Letters, 21(10):1173–1176, 2014.

[39] Jie Yang, Dong Gong, Lingqiao Liu, and Qinfeng Shi. See-
ing deeply and bidirectionally: A deep learning approach for
single image reﬂection removal. In ECCV, 2018.

[40] Xuaner Zhang, Ren Ng, and Qifeng Chen. Single image
reﬂection separation with perceptual losses. In CVPR, 2018.

1565

