Translate-to-Recognize Networks for RGB-D Scene Recognition

Dapeng Du

Limin Wang∗ Huiling Wang Kai Zhao Gangshan Wu

State Key Laboratory for Novel Software Technology, Nanjing University, China

Abstract

Cross-modal transfer is helpful to enhance modality-
speciﬁc discriminative power for scene recognition. To
this end, this paper presents a uniﬁed framework to inte-
grate the tasks of cross-modal translation and modality-
speciﬁc recognition, termed as Translate-to-Recognize Net-
work (TRecgNet). Speciﬁcally, both translation and recog-
nition tasks share the same encoder network, which allows
to explicitly regularize the training of recognition task with
the help of translation, and thus improve its ﬁnal general-
ization ability. For translation task, we place a decoder
module on top of the encoder network and it is optimized
with a new layer-wise semantic loss, while for recogni-
tion task, we use a linear classiﬁer based on the feature
embedding from encoder and its training is guided by the
standard cross-entropy loss. In addition, our TRecgNet al-
lows to exploit large numbers of unlabeled RGB-D data to
train the translation task and thus improve the representa-
tion power of encoder network. Empirically, we verify that
this new semi-supervised setting is able to further enhance
the performance of recognition network. We perform exper-
iments on two RGB-D scene recognition benchmarks: NYU
Depth v2 and SUN RGB-D, demonstrating that TRecgNet
achieves superior performance to the existing state-of-the-
art methods, especially for recognition solely based on a
single modality.

1. Introduction

Recently computer vision community has concentrated
on applying convolutional neural networks (CNN) [15] to
various vision tasks [10, 22, 8, 28, 30, 29]. Meanwhile, the
rapid development of cost-affordable depth sensors (e.g.,
Microsoft Kinect and Intel Realsense) have triggered more
attractions to revisit computer vision problems using RGB-
D data, such as object detection [7, 34], image segmenta-
tion [19], activity recognition [36, 31].

In this paper, we focus on enhancing the modality-
speciﬁc network’s representative power of RGB-D scene
recognition, the goal of which is to accurately classify the

∗Corresponding author.

given scene images with aligned color and depth informa-
tion.
It is a highly challenging classiﬁcation task on un-
derstanding RGB-D scene data on one hand for its essen-
tially diverse combination of cluttered objects, bounding in
different semantic representations for indoors scenes, and
importantly on the other hand for the data scarcity prob-
lem. The amount of the largest RGB-D dataset [25] is still
order-of-magnitude insufﬁcient to provide enough labeled
RGB-D data. Since pre-trained RGB CNN models are eas-
ily adapted for new RGB data, recent works have focused
on learning effective depth features. A few methods [27, 40]
directly use the pre-trained RGB CNN weights to ﬁne tune
a depth CNN, but only limited improvements are given.
[7] directly transferred semantic supervision from labeled
RGB images to unlabeled depth images, which has limits
on transform direction.

This paper tackles major challenges mentioned above
from two aspects: 1) We propose to enhance the discrim-
inative power for both RGB and depth’s single-modal net-
works by a cross-modal translation procedure, and 2) we
enhance training data sampling with the generated images
of high semantic relevance for the classiﬁcation task. The
basic idea is that the modality translation enhances the de-
scription power of the encoding network as it forces the
RGB/depth data to infer information towards its comple-
mentary modality. RGB→ depth translation network could
improve the representation ability of RGB scene network,
by learning generating depth data which encodes better
on geometric and appearance invariant cues, while depth
scene network get beneﬁted learning color and texture cues
through depth→ RGB translation. Meanwhile, this transla-
tion process produces new cross-modal data of high quality
for the other modality data’s classiﬁcation task.

Speciﬁcally, we propose to couple an arbitrary modality-
speciﬁc scene Recognition network with a modal-
ity Translation network trained in a multi-task manner,
termed as TRecgNet; both branches share the same Encod-
ing Network (E Net), as shown in Figure 1. The effective-
ness of TRecgNet that boost modality-speciﬁc description
power essentially lies on the effect of modality translation,
i.e., how the RGB-D data could effectively learn the seman-
tic similarity from the paired data to boost the jointly learn-

11836

Figure 1. TRecgNet. Cubes are feature maps with dimensions and size represented as #features@height∗width. The pipeline consists
of two parallel streams: 1) Recognition Branch is for recognizing scene images, in which E → C Nets are updated using supervised
classiﬁcation loss. 2) Translation Branch aims at constructing paired complementary-modal data of input through E → D Nets. The
translation procedure is constrained by semantic supervision S Net. We jointly train the two branches in an end-to-end manner. In the test
phase, we only use the Recognition Branch.

ing task. We do not simply use the pixel-level Euclidean
distance loss as the supervision like many reconstruction
works did [35]. We argue that this difference is unreliable,
especially for RGB → depth translation, since the ground
truth images of depth data usually exist many outliers be-
cause of the equipment’s limitations and operation errors.
Also, low level pixel-wise similarity fails to provide any se-
mantic relevance. Instead, we perform the translation pro-
cess using pre-trained semantic model, which is inspired by
the style transfer related works [3, 13, 2]. It has been shown
that CNNs trained with sufﬁcient labeled data on speciﬁc vi-
sion tasks, such as object classiﬁcation, has already learned
to extract semantic content representations. This general-
ized ability is not limited to speciﬁc datasets or tasks. In
these works, authors use the perceptual constraint from one
speciﬁc layer, usually conv4 x of VGG model, to keep the
”rough” content of the image when transforming it to an-
other style. In contrast, we propose to leverage perceptual
loss from multiple layers supervising the translation pro-
cess, for a simple yet effective intuition that higher layer
tends to preserve the semantic content while lower one does
better in capturing the detailed information, which could
provide enough and effective cues for cross-modal similar-
ity learning.
We test

the TRecgNet on the two benchmarks of
the RGB-D Indoor scene recognition task, SUN RGB-D
dataset and NYU Depth dataset v2. Our TRecgNet could
obtain an evident improvement on both modality-speciﬁc
and RGB-D settings. The main contributions of this paper
are two-fold:

• A TRecgNet is proposed to transfer complementary

cues through a label-free modality translation process,
which improves modality-speciﬁc classiﬁcation task
in an end-to-end manner and achieves state-of-the-art
performance on RGB-D indoor scene benchmarks.

• TRecgNet can generate more photo-realistic and se-
mantically related data to enhance training data sam-
pling which alleviates the data scarcity problem and
effectively improves the classiﬁcation performance.

2. Related Work

RGB-D Scene Recognition. Earlier works relied on
handcrafted features to capture the characteristic properties
of the scene. Banica et al. [1] used second-order pooling
to quantize local features for segmentation and scene clas-
siﬁcation. Gupta et al. [5] proposed to quantize segmenta-
tion outputs by detecting contours from depth images as lo-
cal features for scene classiﬁcation. Recently, multi-layered
networks such as CNNs is able to learn useful representa-
tions from large amounts of data. In general, they learned
modality-speciﬁc features from RGB and depth images sep-
arately, and then performed the fusion. Wang et al. [27]
extracted and combine deep discriminative features from
different modalities in a component aware fusion manner.
Gupta et al. [7] transferred the RGB model to depth net us-
ing unlabeled paired data according to their mid-level rep-
resentations. However, these methods only consider trans-
ferring color cues to depth net and ignore that depth cues
could also beneﬁt the RGB net. More recently, the recogni-
tion power of depth net has been comprehensively studied.
In [26], Song et al. argued that learning depth features from
pre-trained RGB CNN models are biased and seek to learn

11837

SourceModalityA256@28x28128@56x5664@112x112512@14x14128@56x56TargetModalityBGenerated ModalityB1startWeights fixedBranch1 : Classification BranchBranch2: Translation BranchClass LabelsClassification Net (C)23@224x224SimilarityEncoding Net (E) Decoding Net(D) 256@28x2864@112x112layer1layer2layer33@224x224Conv1x1->Feature PlusResidual UpsampleLayerUpsample + Conv1x1ResNetBasicBlockLayerLegendLayer-wise Content LossClassification Loss12Semantic Content Net (S)layer4depth features from scratch using weakly supervised depth
patches. However, since depth data still suffers from data
scarcity problem, the structure of the depth nets fail to go
deep which limits its extendibility .

Paired Image-to-Image Translation. Paired image-to-
image translation problems could be formulated as pixel
level mapping functions, however, each pixel is translated
independently from others in this manner [37]. Recently,
GANs [4, 21] have achieved impressive results in image
generation. Wang et al. [32] factored the generation into
two steps: surface normal generation and content gener-
ation. They used a pixel-wise surface normal constraints
as additional supervision. In [18], the authors proposed to
use GANs to learn the mapping functions between paired
images. These GAN constraints help generate more var-
iegated images from data distribution learning, however,
GAN based model is hard to train and generated images
tend to lack the semantic consistency of source images that
are hard to be leveraged in subsequent semantics related
work, such as classiﬁcation or segmentation tasks. More
recently, in many style transfer works [3, 13, 2], they use
perceptual loss from speciﬁc layer of pre-trained RGB mod-
els, to maintain the structural content during the translation.
However, most of them only use content constraint from one
speciﬁc layer, usually conv4 x of VGG model, ignoring that
pre-trained models could perform different levels of seman-
tic supervisions which are good enough for image transla-
tion. In this paper, we propose to leverage perceptual loss
from multiple layers to constrain the translation process of
RGB-D data. Unlike pure image generation task, we focus
on how this translation process ﬁnally beneﬁts the classiﬁ-
cation task and the performance of data augmentation using
generated images.

3. Method

This section details the description of our Translate-to-
Recognize Network (TRecgNet). The proposed framework
is illustrated in Figure 1. Assume that in RGB-D setting
we would like to train a classiﬁcation network using one
modality data. Let (MA, MB) be the paired RGB-D im-
ages of speciﬁc classes from set L = {1, 2, ..., Nc}, where
Nc is the total number of scene classes. Our object is to
learn an embedding E : MA → Rd with a translation
mapping T : Rd → MB and a class prediction function
C : Rd → L. The core problem is how the translation pro-
cess could make the modality-speciﬁc Encoder Network (E
Net) learn effective complementary-modal cues to beneﬁt
the classiﬁcation task.

3.1. The TRecgNet Architecture

TRecgNet consists of four parts including Encoder Net-
work (E Net), Classiﬁcation Net (C Net), Decoder Net (D
Net) and Semantic Content Network (S Net). Figure 1

Figure 2. Architecture of the residual upsample layer. The input
feature maps get upsampled with scale 2 followed by one basic
residual block.

shows the exemplar architecture of RGB TRecgNet built on
ResNet-18, which uses pre-trained ResNet18 as the E and
C Nets. D decodes the feature maps from E to reconstruct
the complementary data. Depth TRecgNet uses the same
structure only by exchanging the position of source and tar-
get modalities. We make three measures to enhance the
translation process. First, we empirically remove the ﬁrst
max-pooling operation of ResNet. In the whole structure,
feature maps only shrink by convolution operation with
stride and less information would get lost which is very im-
portant for image translation process. Second, we introduce
residual upsample layer in the D Net. A residual upsample
layer upsamples feature maps with a bilinear interpolation
operation accompanied with one residual block which mim-
ics the basic residual block of ResNet. Figure 2 shows the
architecture of the residual upsample layer. Third, we prop-
agate the context information from three stages of E to the
corresponding outputs of D similar with [23]. We use the
plus operation instead of concatenation which could reduce
the number of parameters in the D Net.

3.2. Layer wise Content Supervised Translation

The translation network aims to improve the presentation
ability of E Net learning characteristics of complementary
data, the procedure of which is supervised by a semantic
RGB CNN model, which uses the combination of percep-
tual constrains from lower layers to higher ones, measuring
the layer-wise similarity of the generated and the paired
data.

We use the ResNet model pre-trained on ImageNet [38]
as the supervision content network S, for the consideration
of accordance with the architecture of E Net. More details
could be referred to Section 3.1. We denote the image rep-
resentations of S Net as Φ = {φl
M , l ∈ [1, 2, 3, 4]}, where
φi
M is the ith layer representation for input data M. It maps
an input image from modality M to a feature vector in Rd.
Speciﬁcally, we deﬁne the L1 loss between two feature vec-
tors for translation supervision. Suppose we are training the
classiﬁcation task for MA. Generated images y′
i and MB
are fed into the S Net, and we can get layer-wise presenta-
tions from y′
i and MB. We constrain them from every each
layer (layer1-layer4 in ResNet) be the same by L1 loss:

11838

Conv1X1,BatchNormInterpolate(scale=2)Conv1X1,BatchNormConv3X3,BatchNormReLUReLUbasicresidualblockresidualupsamplelayerLcontent(yi, y′

i, l) =

L

X

l=1

3.3. Training Strategy

kφl

yi − φl

T (xi)k1

(1)

In this section, we introduce our optimization procedure
in detail. To jointly learn the embedding and the trans-
lation pair, we optimize E, C, D networks in a multi-task
manner. Speciﬁcally, given a pair of RGB-D images, let
eMA = E({xi}) be the embeddings from E Net computed
on MA and dMB = D(E({xi}) be the generated modality
B data decoded from D. We simultaneously update 1) the
E → D → S to minimize the distance of layer-wise vectors
to constrain their semantic similarity, and 2) the E → C us-
ing a cross entropy loss function for classiﬁcation task. The
total loss is updated with a linear combination:

Ltotal = αLcontent + βLCcls

(2)

where Lcontent is Equation (1) described in Section 3.2,
Lcls is the cross-entropy loss for classiﬁcation task, with
their coefﬁcients α, β set as 10, 1 from the best trials.

As the datasets we use in Section 4 are character-
ized by unbalanced images for categories, we use rescal-
ing weights given to each category for cross-entropy loss-
Rescaling weights aims to assign different weights to differ-
ent classes to handle the issue of imbalance training. Specif-
ically, we use the following rescaling strategy:

Lweighted cls =

1
N X

i

−w(yi) log

f (xi)yi
Pj f (xi)j

.

The weight w(y) is computed as:

w(y) =

N (y) − N(c min) + δ
N(c max) − N(c min)

,

(3)

(4)

where Ny is the number of images of class y. c min and
c max represent the class with the least and the most num-
ber of training images. The δ is set as 0.01.

In the test phase, we only use the recognition branch for

recognition prediction, as illustrated in Figure 1.

Imbalanced translations between two modalities.
There are several factors demonstrating that the translations
from RGB-to-depth and depth-to-RGB are imbalanced. For
example, translation from RGB to depth images is more
relatively natural procedure while it would become an ill-
posed problem from the reversed direction. What’s more,
the ground truth of depth data comes with much more value
errors due to the characteristics of collecting equipments
and process. Therefore, we sample a random noise vec-
tor from N(0,1) concatenated to the input feature to D in

Figure 3. Fusion Network. We only use Encoding Nets from RGB
and Depth TRecgNets for fusion. The weights of two E Nets are
ﬁxed, and only weights of the classiﬁer are updated.

the Depth TRecgNet (translation from depth to RGB). We
found it useful to stable the training of Depth TRecgNet.
The generated RGB images also get an interpretable con-
trol from the sampled noise. The dimensionality of noise is
set as 128 in our experiments.

Initialization with unlabeled RGB-D data. As men-
tioned in Section 1, the sizes of most existing labeled RGB-
D datasets are in small orders of magnitude compared with
RGB datasets. However, there are a large number of unla-
beled RGB-D pairs, for example, from RGB-D video se-
quences. A signiﬁcant advantage of our method is that
we are capable of initializing the TRecgNet with these
unlabeled data.
In other words, the modality translation
process is a label-free procedure, by which the TRecgNet
could learn rich representations from unlabeled RGB-D
data boosting the further task. Related experiments are de-
tailed in Section. 4.

3.4. Fusion

After we get two trained TRecgNets for RGB and depth
data, we compute two E Nets and concatenate modality-
speciﬁc features from them. The embedding is operated on
global average pooling (GAP) [18] to reduce the number of
parameters followed by three fully connected layers. The
whole structure is illustrated in Figure 3. We ﬁx the Encod-
ing Networks and directly train the classiﬁer in an end-to-
end manner. We ﬁnd this would be superior to that directly
combines the two prediction results and show the effective-
ness of modality-speciﬁc networks more straightforwardly.

4. Experiments

In this section, we ﬁrst introduce the evaluation datasets
and the implementation details of our proposed approach.
Then we discuss the ablation study of TRecgNet on effec-
tiveness and layer contribution of S. We also compare the
quality of generated images with other approaches. Finally,
we evaluate the performance of our approach with state-of-
the-art methods. We quantitatively report the average ac-
curacy over all scene categories following the conventional
evaluation scheme.

4.1. Datasets

SUN RGB-D Dataset is currently the largest RGB-D
dataset. It contains RGB-D images from NYU depth v2,

11839

RGB Encoding NetDepth Encoding Net1024FCFC1024concatenationGlobal Average poolingFCClasses numWeights fixedData

RGB

Depth

Model

ResNet18

TRecg

TRecg Aug
ResNet18
ResNet18

TRecg
TRecg
TRecg
TRecg

Init

Places
Places
Places
random
Places
random
Places

random/unlabeled
Places/unlabeled

TRecg Aug

Places

Acc (%)

47.4
49.8
50.6
38.1
44.5
42.2
46.8
44.2
47.6
47.9

Table 1. Ablation study of TRecg-ResNet18 on recognition per-
formance. The results are reported on the test set of SUN RGB-D
(Top-1’s mean accuracy %). “Aug” means using generated data in
training.

Berkeley B3DO [12], and SUN3D [33] and is compromised
of 3,784 Microsoft Kinect v2 images, 3,389 Asus Xtion im-
ages, 2,003 Microsoft Kinect v1 images and 1,159 Intel Re-
alSense images. Following the standard experimental set-
tings stated in [25], we only work with 19 major scene cate-
gories which contain more than 80 images. As per standard
splits, there are in total 4,845 images for training and 4,659
for testing.

NYU Depth Dataset V2 (NYUD2) is a relatively small
dataset; only a few of its 27 indoor categories are well pre-
sented. Following the standard split in [24], the categories
are grouped into ten including nine most common cate-
gories and an other category representing the rest. Also,
we use 795 / 654 images for training / testing following the
standard split.

4.2. Implementation Details

The proposed approach is implemented in popular deep
learning framework, Pytorch [20], on an NVIDIA TITAN
Xp GPU. We train the network using Adam stochastic op-
timization [14] to learn network parameters, with the batch
size set to 40. The RGB-D images are resized to 256 × 256
and randomly cropped to 224×224. We train the TRecgNet
in 70 epochs, and the learning rate is initialized as 2 × 10−4
at the ﬁrst 20 epochs and linearly decades in the rest of
50.
In the test phase, we use a center crop operation on
test images. We employ geocentric HHA (Horizontal dis-
parity, Height above ground and Angle with gravity) [6]
to encode depth images, which has been shown better per-
formance to capture the scenes structural and geometrical
properties of depth data for kinds of vision tasks.
In the
subsequent experiments, we separately train two kinds of
TRecgNets for evaluation. The basic TRecgNet is trained
without using generated data while the TrecgNet Aug refers
to training a TRecgNet with generated data from the previ-
ous corresponding basic TRecgNet. (RGB TRecgNet Aug

Figure 4. Effect of content model using different layer as super-
vision on classiﬁcation for (A) RGB TRecgNet and (B) Depth
TRecgNet. Tested on SUN RGB-D dataset.

Figure 5. Examples of TRecgNet translating RGB images to depth
ones by semantic supervision from different layers. The combina-
tion of layer-wise content supervision gives the best photo-realist
translation. The input images are all from the test set of SUN
RGB-D dataset.

leverages the basic depth TRecgNet as the generated data
sampler, and vice versa). Speciﬁcally, in the training phase
of TrecgNet Aug, we randomly use the generated data, the
number of which is controlled as 30% of batch size, to
achieve the best performance.

4.3. Results on SUN RGB D dataset

Study on the effectiveness of TRecgNet. We begin our
experiments by studying the effectiveness of TRecgNet for
RGB-D recognition task. We tend to prove that 1) learn-
ing essential similarity from modality translation branch
could effectively beneﬁt classiﬁcation task, and 2) Unla-
beled RGB-D data helps in training TRecgNet for recogni-
tion task, and 3) generated data improves training process.

11840

Mean Accuracy (%)RGB TRecgNetMean Accuracy (%)Number of EpochsDepth TRecgNet(B) Depth TrecgNet(A) RGB TrecgNetNumber of Epochs(A) Layer1(B) Layer2(C) Layer3(D) Layer4(E) Layer1-4Figure 6. Examples of generated data by our TrecgNet from test set of SUN RGB-D dataset. (B) are depth images translated from original
RGB data (A), and (D) are generated RGB images using original depth ones (C).

Loss
Mean Acc (depth)
Mean Acc (rgb)

Pixel
13.1%
7.3%

Pixel+GAN

17.7%
20.6%

Ours
30.3%
18.4%

Table 2. We compare the quality of generated depth images by
training a vanilla ResNet18 pre-trained on Places dataset. The
training images are all generated using training data of SUN RGB-
D dataset by three methods. The results are reported on the test set
of SUN RGB-D (Top-1’s mean accuracy %). Images generated by
our TRecgNet achieve the best result.

Since patterns of color and depth information vary greatly
on visual appearance, geometry and surface, we use the fol-
lowing baselines: for the classiﬁcation of RGB modality,
we ﬁne tune pre-trained ResNet18 on RGB images; for the
depth modality, we train the network both from scratch and
from the pre-trained model.

We test our TRceg Net with following settings: a) the
same initialization schedule with baseline, b) the Depth
TRecgNet is pre-trained using 5k unlabeled RGB-D video
sequences from the NYUD2 dataset. c) Two TRecgNets ex-
tract the translation branches as the data augmentation sup-
plier to retrain the TRecgNet; 30% of original training data
is randomly replaced with generated data. The experimental
results are summarized in Table 1. We observe that for each
modality, our TRecgNet outperforms the baseline with big
margins. RGB TRecgNet outperforms the baseline by 2.4%
while Depth TRecgNet by 3.7% and 2.3% with randomly
initialization and pre-trained weights from Places dataset,
separately.

It

is worth noting that unlabeled RGB-D data pre-
training for TRecgNet makes a further boosting of 2% and
0.8%, when training from scratch and using pre-trained
weights, respectively. Without pre-trained Places weights
but only 5k unlabeled data, the result is very comparable to
that of vanilla ResNet pre-trained on Places dataset could
achieve, only with quite less training data. This indicates

that we could ﬂexibly design a E Net achieving acceptable
results without pre-training it using large-scale datasets like
ImageNet or Places dataset.

We also ﬁnd promotions when using generated images
as training data, with 1.1% for the Aug-Depth and 0.8%
for Aug-RGB TRecgNets. We show some generated data
examples in Figure 6. It’s interesting to ﬁnd that the gener-
ated depth images tend to be brighter and have better con-
tours than the original ones, due to the learned RGB con-
text information in the translation process. As for generated
RGB data, since the original depth data exists with non-
negligible measurement error, generated RGB data would
be inevitably of low quality on details, however, we turned
out to ﬁnd it be alleviated by adding the random noise in the
training of Depth TRecgNet.

Study on layer contribution of Semantic Content Net-
work. The effectiveness of transferring complementary
cues relies much on the semantic content model S Net.
Therefore, we take an interest in how the S Net affects the
translation and recognition tasks using the different layer
as supervision. We separately use layer 1 ∼ layer 4 as
well as their combination as the translation constraint to test
the scene recognition performance of TRecgNet, compared
with directly ﬁne tuning pre-trained ResNet18 model; all
the experiments are conducted on SUN RGB-D dataset. In
Figure 4, the accuracy is plotted against the number of train-
ing epochs. The following observations can be made: 1)
TRecgNet achieves markable improvement to varying de-
gree by using the different layer as supervision. 2) The
training procedure becomes more steady when using layer-
wise content constraint and achieves the best performance.
The result demonstrates the effectiveness of TRecgNet us-
ing S as the supervision for cross-modal translation. Fig-
ure 5 gives an example of RGB TrecgNet translating images
by semantic supervisions from different layers. Similarity
learning from lower layers’ supervision focus on keeping
textures while higher layers’ constraints try to capture more

11841

(A) RGB(B)generateddepth(C)depthGT(A) RGB(B)generateddepth(C)depthGT(D)GeneratedRGB(D)GeneratedRGBBaseline

Proposed

SOTA

Method
ResNet18
ResNet18
TRecgNet
TRecgNet Aug
TRecgNet
TRecgNet Aug
Multimodal fusion[40]
Modality&component fusion [27]
RGB-D-CNN+wSVM [26]
DF2Net metric learning [17]

RGB Init
ImageNet

Places

ImageNet
ImageNet

Places
Places
Places
Places
Places
Places

Places

ImageNet
ImageNet

Depth Init
ImageNet

Accuracy (%)
RGB
46.6
47.4
48.7
49.2
49.8
50.6
40.4
40.4
Fast R-CNN 44.6
46.3

Depth Fusion
44.5
44.8
46.9
47.9
46.8
47.9
36.5
36.5
42.7
39.2

50.1
50.8
55.5
56.1
56.1
56.7
41.5
48.1
53.8
54.6

Places
Places
Places
Places

Places

Table 3. Comparison with state-of-the-art methods on the test set of the SUN RGB-D dataset. The performance is measured by Top-1’s
mean accuracy over classes. “Aug” means using generated data in training.

Baseline

Proposed

SOTA

Method
ResNet18
TRecgNet
TRecgNet
TRecgNet Aug
Modality & component fusion [27]
RGB-D-CNN+wSVM [26]
DF2Net metric learning [17]

RGB Init

Depth Init

Places
Places

Places
Places

Accuracy (%)
RGB Depth Fusion
59.8
60.2
SUN RGB-D SUN RGB-D 63.8
SUN RGB-D SUN RGB-D 64.8
53.5
Fast R-CNN 53.4
61.1

52.3
55.2
56.7
57.7
51.5
56.4
54.8

63.8
65.5
66.5
69.2
63.9
67.5
65.4

Places
Places
Places

Places

Places

Table 4. Comparison with state-of-the-art methods on the test set of the NYUD2 dataset. The performance is measured by Top-1’s mean
accuracy over classes. “Aug” means using generated data in training.

semantic cues ignoring luminance or texture. In contrast,
layer-wise content supervision combines multi-layer char-
acteristics which could not only get better performance on
classiﬁcation task but generate more photo-realistic images.

Study on quality of generated images. In this study,
we evaluate the quality of generated data from TRecgNet.
Since depth data suffers a lot from scarcity and value er-
ror problems, we focus on depth data generation. We com-
pare TRecgNet with two image generation approaches. The
ﬁrst one is to generate depth data only using L1 loss on
pixel-level intensity, which is very common in images re-
construction works [16] as stated before. We also test im-
ages generated by pixel-to-pixel GAN supervision [11]. We
qualitatively compare some generated examples in Figure 7.
Images from pixel intensity supervision tend to be blurred
stifﬂy imitating the training data while our TRecgNet pro-
duce more natural images, especially for depth data, even
for cases that the ground truth is of signiﬁcant errors, see the
bottom row. Interestingly, RGB images generated based on
GAN shows an impressive effect on color diversity. We also
quantitatively evaluate the quality of generated images by
ﬁne tuning pre-trained ResNet18 only using generated im-
ages by different methods. Table 2 shows the results. Depth
images from our TRecgNet outperform other methods by a

big margin, indicating its effectiveness for depth data aug-
mentation while GAN based methods achieve a better result
on RGB images.

Comparison with state-of-the-art methods. We report
TRecgNet on SUN RGB-D test set compared with state-
of-the-art methods, as shown in Table 3. Most RGB-D
scene recognition methods build upon models pre-trained
on Places dataset [39]. Apart from that, we also report the
results using ImageNet pre-trained weights. Most of these
methods rely on ﬁne tuning Places-CNN. Song et al. [26]
work on learning more effective depth representations from
supervised depth patches via SSP [9] and makes compre-
hensive investigations on fusion strategies, such as multi-
scale and aided from object detection. DF 2Net adopts
triplet loss based metric learning [17] to learn discrimi-
native and correlative features for modality-speciﬁc repre-
sentation and fusion learning. Our TRecgNets outperform
other state-of-the-art methods on both kinds of modalities
and their fusion by an evident margin. It is worth noting
that our method succeeds in learning modality-speciﬁc fea-
tures from cross-modal transfer learning, and we do not rely
much on any sophisticated and speciﬁcally designed fusion
strategy.

11842

Figure 7. Generated examples by different methods. RGB data translated to depth one is shown in the ﬁrst two rows while images in the
last two rows give a reversed example. (A), (E) are ground truth of RGB and depth data. (B), (C) and (D) are generated images separately
from pixel-level L1 loss supervision, pixel-to-pixel GAN, and our TRecgNet.

4.4. Results on NYUD2 dataset

We also evaluate TRecgNet on the NYUD2 test set and
compare with other representative works. NYUD2 is a rel-
atively small RGB-D dataset, thus we only evaluate the
TRecgNet using pre-trained weights of Places dataset or
SUN RGB-D dataset. In particular, we study the general-
ization ability of learned TRecgNet representations on SUN
RGB-D. We transfer the learned TRecgNets from SUN
RGB-D and ﬁne tune on data from the NYUD2 dataset. We
report the results in Table 4. We ﬁnd that for RGB modality,
our RGB TRecgNet only yields a slightly better result. We
argue that it is mainly because the size of NYUD2 dataset is
too small. Negative effects from errors of depth maps badly
affects the translation from the RGB to the depth. However,
transferring pre-trained weights from SUN RGB-D datasets
encourages the TRecgNet to overcome this problem, en-
hance modality-speciﬁc representation power and improve
its ﬁnal recognition performance. When adding the gener-
ated data, we observe further promotions for both modal-
ities. Experiments in NYUD2 dataset reveal TRecgNet’s
requirement on the scale of training data to some degree.
Translation using small dataset is difﬁcult in beneﬁting the
multi-training task, especially for modalities that exist mea-
suring errors that can’t be ignored. Figure 4 also shows
some hints for problem that when training TRecgNet in the

ﬁrst several epochs, it tends to behave more unstably than
that of ﬁne tuning the backbone network directly and get
suboptimal results.

5. Conclusion and Future Work

In this paper, we have presented an effective Translate-
to-Recognize Network (TRecgNet)
to learn modality-
speciﬁc RGB-D representation for RGB-D scene recogni-
tion task. TRecgNet enables a CNN classiﬁcation network
to learn more discriminative feature by a translation process
learning essential similarity with cross-modal data. Train-
ing TRecgNet allows using unlabeled RGB-D data as ini-
tialization which makes up for the data scarcity problem.
As experiments demonstrated on SUN RGB-D and NYUD2
datasets, we both achieve the state-of-the-art results validat-
ing the effectiveness of the proposed method. In the future,
we plan to try instantiate more as well as deeper CNN mod-
els such as ResNet50 and VGG Network. We will also try
to handle the big error problem of depth data.

Acknowledgments

This work is supported by the National Science Founda-
tion of China under Grant No.61321491, and Collaborative
Innovation Center of Novel Software Technology and In-
dustrialization.

11843

(A) sourcemodality(B) pixelsimilarity(C) pix2pixGAN(D) ours(E) targetmodality(GT)References

[1] Dan Banica and Cristian Sminchisescu. Second-order con-
strained parametric proposals and sequential search-based
structured prediction for semantic segmentation in rgb-d im-
ages. In CVPR, pages 3517–3526, 2015.

[2] Yang Chen, Yu-Kun Lai, and Yong-Jin Liu. Cartoongan:
Generative adversarial networks for photo cartoonization. In
CVPR, pages 9465–9474, 2018.

[3] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-
In

age style transfer using convolutional neural networks.
CVPR, pages 2414–2423. IEEE, 2016.

[4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014.

[5] Saurabh Gupta, Pablo Arbel´aez, Ross Girshick, and Jiten-
dra Malik. Indoor scene understanding with rgb-d images:
Bottom-up segmentation, object detection and semantic seg-
mentation. IJCV, 112(2):133–149, 2015.

[6] Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, and Jiten-
dra Malik. Learning rich features from rgb-d images for ob-
ject detection and segmentation. In ECCV, pages 345–360.
Springer, 2014.

[7] Saurabh Gupta, Judy Hoffman, and Jitendra Malik. Cross
modal distillation for supervision transfer. In CVPR, pages
2827–2836, 2016.

[8] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross B.
Girshick. Mask R-CNN. In ICCV, pages 2980–2988, 2017.

[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Spatial pyramid pooling in deep convolutional networks for
visual recognition.
In ECCV, pages 346–361. Springer,
2014.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[11] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, pages 5967–5976. IEEE, 2017.

[12] Allison Janoch, Sergey Karayev, Yangqing Jia, Jonathan T
Barron, Mario Fritz, Kate Saenko, and Trevor Darrell. A
category-level 3d object dataset: Putting the kinect to work.
In Consumer Depth Cameras for Computer Vision, pages
141–165. Springer, 2013.

[13] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
ECCV, pages 694–711. Springer, 2016.

[14] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, pages 1097–1105, 2012.

[16] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks.
In 3D Vision

(3DV), 2016 Fourth International Conference on, pages 239–
248. IEEE, 2016.

[17] Yabei Li, Junge Zhang, Yanhua Cheng, Kaiqi Huang, and
Tieniu Tan. Df2net: Discriminative feature learning and fu-
sion network for rgb-d indoor scene classiﬁcation. In AAAI,
2018.

[18] Min Lin, Qiang Chen, and Shuicheng Yan. Network in net-

work. arXiv preprint arXiv:1312.4400, 2013.

[19] John McCormac, Ankur Handa, Stefan Leutenegger, and
Andrew J Davison. Scenenet rgb-d: Can 5m synthetic im-
ages beat generic imagenet pre-training on indoor segmenta-
tion. In ICCV, volume 4, 2017.

[20] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.

[21] Alec Radford, Luke Metz, and Soumith Chintala. Un-
supervised representation learning with deep convolu-
tional generative adversarial networks.
In arXiv preprint
arXiv:1511.06434, 2015.

[22] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: towards real-time object detection with re-
gion proposal networks. In NIPS, pages 91–99, 2015.

[23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention, pages 234–241.
Springer, 2015.

[24] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Indoor segmentation and support inference from

Fergus.
rgbd images. In ECCV, pages 746–760. Springer, 2012.

[25] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
Sun rgb-d: A rgb-d scene understanding benchmark suite. In
CVPR, pages 567–576. IEEE, 2015.

[26] Xinhang Song, Shuqiang Jiang, Luis Herranz, and Cheng-
peng Chen. Learning effective rgb-d representations for
scene recognition. IEEE Transactions on Image Processing,
2018.

[27] Anran Wang, Jianfei Cai, Jiwen Lu, and Tat-Jen Cham.
Modality and component aware feature fusion for rgb-d
scene classiﬁcation.
In CVPR, pages 5995–6004. IEEE,
2016.

[28] Limin Wang, Sheng Guo, Weilin Huang, Yuanjun Xiong,
and Yu Qiao. Knowledge guided disambiguation for large-
scale scene classiﬁcation with multi-resolution cnns. IEEE
Trans. Image Processing, 26(4):2055–2068, 2017.

[29] Limin Wang, Wei Li, Wen Li, and Luc Van Gool.
Appearance-and-relation networks for video classiﬁcation.
In CVPR, pages 1430–1439, 2018.

[30] Limin Wang, Zhe Wang, Yu Qiao, and Luc Van Gool.
Transferring deep object and scene representations for event
recognition in still images.
International Journal of Com-
puter Vision, 126(2-4):390–409, 2018.

[31] Pichao Wang, Wanqing Li, Zhimin Gao, Yuyao Zhang,
Chang Tang, and Philip Ogunbona. Scene ﬂow to action
map: A new representation for rgb-d based action recogni-
tion with convolutional neural networks. In CVPR, 2017.

11844

[32] Xiaolong Wang and Abhinav Gupta. Generative image mod-
In

eling using style and structure adversarial networks.
ECCV, pages 318–335. Springer, 2016.

[33] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.
Sun3d: A database of big spaces reconstructed using sfm
and object labels. In ICCV, pages 1625–1632. IEEE, 2013.

[34] Xiangyang Xu, Yuncheng Li, Gangshan Wu, and Jiebo Luo.
Multi-modal deep feature learning for rgb-d object detection.
Pattern Recognition, 72:300–313, 2017.

[35] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera,
Kejie Li, Harsh Agarwal, and Ian Reid. Unsupervised learn-
ing of monocular depth estimation and visual odometry with
deep feature reconstruction. In CVPR, pages 340–349, 2018.
[36] Jing Zhang, Wanqing Li, Philip O Ogunbona, Pichao Wang,
and Chang Tang. Rgb-d-based action recognition datasets:
A survey. Pattern Recognition, 60:86–105, 2016.

[37] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
In European conference on computer

image colorization.
vision, pages 649–666. Springer, 2016.

[38] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. PAMI, 2017.

[39] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-
ralba, and Aude Oliva. Learning deep features for scene
recognition using places database.
In Advances in neural
information processing systems, pages 487–495, 2014.

[40] Hongyuan Zhu, Jean-Baptiste Weibel, and Shijian Lu. Dis-
criminative multi-modal feature fusion for rgbd indoor scene
recognition. In CVPR, pages 2969–2976. IEEE, 2016.

11845

