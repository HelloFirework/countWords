SDC – Stacked Dilated Convolution:

A Uniﬁed Descriptor Network for Dense Matching Tasks

Ren´e Schuster1

Christian Unger2
1DFKI - German Research Center for Artiﬁcial Intelligence

Oliver Wasenm¨uller1

Didier Stricker1
2BMW Group

firstname.lastname@{bmw,dfki}.de

Abstract

Dense pixel matching is important for many computer vi-
sion tasks such as disparity and ﬂow estimation. We present
a robust, uniﬁed descriptor network that considers a large
context region with high spatial variance. Our network has
a very large receptive ﬁeld and avoids striding layers to
maintain spatial resolution. These properties are achieved
by creating a novel neural network layer that consists of
multiple, parallel, stacked dilated convolutions (SDC). Sev-
eral of these layers are combined to form our SDC descrip-
tor network.
In our experiments, we show that our SDC
features outperform state-of-the-art feature descriptors in
terms of accuracy and robustness. In addition, we demon-
strate the superior performance of SDC in state-of-the-art
stereo matching, optical ﬂow and scene ﬂow algorithms on
several famous public benchmarks.

]
2
1
[
S
A
L
E

]
9
1
[

M
P
C

]
3
3
[
F
F
S

1. Introduction

Applications for driver assistance, robot navigation, au-
tonomous vehicles, and others require a detailed and ac-
curate perception of the environment. Many of these high
level computer vision tasks are based on ﬁnding pixel-wise
correspondences across different images (e.g. optical ﬂow
or stereo, see Figure 1). Robust dense matching of pixel po-
sitions under unconstrained conditions typically is a very
challenging task for several reasons. Perspective defor-
mations, changing lighting conditions, sensor noise, occlu-
sions, and other effects can change the appearance of corre-
sponding image points drastically. Thus, heuristic descrip-
tors (e.g. SIFT [24] or CENSUS [42]) can produce very dis-
similar descriptors for corresponding image points. A key
factor to overcome these issues is the size of context infor-
mation that is considered by a descriptor. However, increas-
ing the patch size introduces spatial invariance for state-
of-the-art descriptors which results in less accurate match-
ing. Recently, deep neural networks were shown to produce
more robust and expressive features. These networks rely

Original Results

Improved with SDC

Figure 1: Our new SDC feature descriptor improves pixel-
wise matching in terms of accuracy and density in state-of-
the-art algorithms. From top to bottom: Disparity map for
ELAS [12] on ETH3D [30], optical ﬂow for CPM [19] on
Sintel [6], and scene ﬂow (disparity and optical ﬂow com-
ponents) for SFF [33] on KITTI [26].

on best practice design decisions from other domains which
results in the use of pooling or other striding layers. Such ar-
chitectures typically achieve a medium sized receptive ﬁeld
only and reduce the spatial resolution of the resulting fea-
ture descriptor. Both properties lower the accuracy of the
matching task.

In this paper, we present a deep neural network with a
large receptive ﬁeld that utilizes a novel architecture block
to compute highly robust, accurate, dense, and discrimina-
tive descriptors for images. To this end, we stack paral-
lel dilated convolutions (SDC). Our design follows two key
observations. First, image patches with low entropy lead
to poor descriptors and thus to incorrect matching. This

2556

fact strengthens the common belief that a robust descriptor
should have a large receptive ﬁeld to incorporate context
knowledge for pixels under difﬁcult visual conditions. Sec-
ondly, accurate matching requires a high spatial precision
that is lost when applying striding layers which produce
coarse, high-level features for deeper layers of the feature
network. Our novel architecture block provides a large re-
ceptive ﬁeld with only few trainable parameters while main-
taining full spatial resolution. Overall, our contribution con-
sists of the following:
• By stacking multiple, parallel dilated convolutions
(SDC), we create a novel neural network block which is
beneﬁcial for any dense, pixel-wise prediction task that
requires high spatial accuracy.

• The combination of these blocks to a fully convolutional
architecture with a large receptive ﬁeld that can be used
for feature description.

• Vast sets of experiments to justify our design decisions,
to compare to other descriptors, and to demonstrate the
accuracy and robustness for scene ﬂow, optical ﬂow, and
stereo matching on the well known public data sets KITTI
[26], MPI Sintel [6], Middlebury [3, 29], HD1K [21], and
ETH3D [30] with our uniﬁed network.

2. Related Work

A feature descriptor is a vector that represents the char-
acteristics of the associated object in a compact, distinctive
manner. It is not to be confused with an interest point (or
key point, sometimes feature point) which identiﬁes loca-
tions where a feature descriptor would be rather unique. In
the context of dense matching, feature descriptors on pixel-
level are required. Since single pixels carry only very little
information, a region around each pixel is considered for
the description.

Conventional descriptors are often based on image gra-
dients to make them invariant to changes in lighting. A very
common descriptor – SIFT [24] – computes histograms of
gradients in regular grids around the center pixel. Using
a multi-scale search and the major orientation of the gra-
dients makes SIFT robust to changes in scale and rotation.
However, SIFT was not designed to describe all pixels of an
image in a dense manner. The full description is rather slow
and sensitive to deformations, occlusions, and motions. Ro-
bustness is also a problem for faster hand-crafted feature
extractors like SURF [5] and DAISY [38]. Binary descrip-
tors (e.g. BRIEF [7], ORB [28], or CENSUS [42]) are even
faster since they are more compact. At the same time, they
are less expressive and less distinctive.

To improve robustness, many have applied deep learning
for feature extraction on patch-level recently. In [15, 43],
features are learned jointly with a decision metric to distin-
guish corresponding and non-matching image patches with
a siamese architecture [9]. For the same reason as L2Net

[37], we do not include a decision network because we
want universal features that can be used within any pipeline.
The architecture of L2Net [37] avoids pooling layers but
requires strided convolution to achieve a medium sized re-
ceptive ﬁeld of 32 pixels. Additionally, they have exper-
imented with a two-stream design where the input of the
second branch is the up-scaled central part of the origi-
nal patch similar as in [43].
In contrast, our architecture
exploits multi-scale information inherently as described in
Section 3.1.

Deep features for the optical ﬂow task were proposed
by [2, 11]. PatchBatch introduced batch normalization for
patch description for the ﬁrst time, and [2] utilized a new
thresholded hinge loss. Both architectures consist of several
convolutions and pooling layers to obtain considerably large
receptive ﬁelds. As motivated earlier, our design can easily
increase the size of the receptive ﬁeld without losing the
spatial accuracy as it happens during pooling.

For stereo matching, previous work used very light-
weight architectures with small receptive ﬁelds in favor of
speed [44, 25]. For the limited search in stereo matching,
the expressiveness of these networks might be sufﬁcient. In
contrast, our universal descriptor network for different tasks
and domains uses much more context information.

Another concept that is heavily used in our work is atrous
or dilated convolution [41]. It is a generalization of regu-
lar convolution where the kernels are widened by inserting
zeros (cf. Figure 3). This effectively increases the ker-
nel’s perceptive ﬁeld without adding more parameters or
losing spatial details. These advantages were mostly ex-
ploited in state-of-the-art semantic segmentation networks
[14, 17, 39, 41] by cascading several dilated convolution
layers with different dilation factors. Other architectures
use dilated convolutions for context aggregation in an end-
to-end network after constructing coarse high-level features
[13]. Our novel concept stacks multiple dilated convolu-
tions (SDC) in parallel and combines each output by con-
catenation to form a single SDC block. This is similar to the
Atrous Spatial Pyramid Pooling (ASPP) in [8] with two ma-
jor differences. Firstly, we do not sum the parallel results,
but stack them. Secondly, our parallel block is not used for
feature pooling in a deeper stage of the network, but for
feature computation in the ﬁrst and only stage of our net-
work. That is also why our dilation rates are much smaller
in comparison. Another similar combination of dilated con-
volutions was recently presented in [40]. They stack a block
which is similar to ASPP on top of the DeepLab [8] model
which boosted performance of object localization signiﬁ-
cantly. However, [8, 40] both exploit parallel dilated convo-
lution for semantic context pooling, while we, for the ﬁrst
time, use convolution with different dilation rates to com-
pute multi-scale feature descriptors.

2557

Figure 2: Our SDC feature network. It consists of 5 SDC blocks with varying number of output channels. The ﬁnal feature
vectors are normalized to unit range pixel-wise.

created a block of stacked dilated convolutions (SDC) in
parallel of which the outputs are concatenated. This way,
each subsequent layer has full access to previous features
of different dilation rates.

3.1. SDC Layer

As others before [8, 22], we argue that convolution with
dilation rate r and stride r is equal to convolution with dila-
tion rate 1 (no dilation) of sub-sampled input by factor r (no
smoothing). Dilated convolution without striding thus pro-
duces a sub-scale response at full spatial resolution. This
key observation is heavily used by our SDC layer design
where we stack the output of convolutions with different di-
lation rates to produce a multi-scale response (see Figure 3).
Whereas others apply pooling over multiple scales, we feed
the entire multi-scale information to the next layers.

We note that convolution with parallel dilated kernels
is similar to convolution with a single larger, sparse ker-
nel (merging the dilated kernels). However, expressiveness
is lost where the different dilated kernels overlap (see Fig-
ure 3). Further, only very few deep learning frameworks
support sparse convolution in an efﬁcient way. Nonethe-
less, an experimental comparison between both designs is
provided in the supplementary material.

3.2. SDC Network

Following the interpretation of dilated convolution of the
previous section, we conclude to stack several SDC layers
to compute, aggregate, and pass information for multiple
scales from end to end. This naturally results in an expo-
nentially growing receptive ﬁeld but avoids gridding effects
because every convolution is fed with the results of every
previous convolution of all dilation rates.

The complete network is illustrated in Figure 2. We
use 5 SDC layers. Each SDC layer applies four parallel
convolutions with 5 × 5 kernels, the same number of out-
put dimensions, and dilation rates of 1, 2, 3, and 4. Expo-
nential Linear Unit (ELU) [10] is used for all activations.
We do not use batch normalization because we train with
a small batch size (cf. Section 3.3). The SDC layers have
64, 64, 128, 256, and 128 output channels respectively. The
ﬁnal feature vector of the last layer is normalized to unit
range. Experiments to justify the decision for this design are

2558

Figure 3: The architecture of a single SDC layer. Our con-
tribution is the combination of parallel convolutions with
different dilation rates. The outputs are stacked along the
feature dimension to produce a multi-scale response.

3. Feature Network

Historically, a large receptive ﬁeld in convolutional neu-
ral networks (CNNs) is primarily obtained by using strid-
ing layers. These are typically pooling layers and more
recently, pooling is replaced by strided convolution [35].
Striding layers also improve run time by reducing the size of
intermediate representations and introduce some translation
invariance. For tasks like image classiﬁcation, these bene-
ﬁts come at no cost since only a single prediction per image
is required. For tasks which require a dense per-pixel pre-
diction, strided layers have the disadvantage of reducing the
spatial resolution. This makes pixel-wise prediction overly
smooth and less accurate.

The obvious way to obtain a large receptive ﬁeld with-
out striding is to use larger kernels. The drawbacks of this
approach are a drastic increase in run-time and number of
parameters which makes such networks slow and prone to
overﬁtting. This problem can be surpassed by dilated con-
volution because although the kernels are large, they are
sparse (in a regular way). Yet, a sequence of dilated con-
volutions can introduce gridding effects (different output
nodes use disjoint subsets of input nodes) if dilation rates
are not selected properly [39]. As a consequence, we have

SDC Layer 1Input: M×N×3Convolutions: 4Sizes: [5, 5, 5, 5]Kernels: [16, 16, 16, 16]Dilations: [1, 2, 3, 4]Output: M×N×64Color ImageFeatureMapSDC Layer 2Input: M×N×64Convolutions: 4Sizes: [5, 5, 5, 5]Kernels: [32, 32, 32, 32]Dilations: [1, 2, 3, 4]Output: M×N×128SDC Layer 3Input: M×N×128Convolutions: 4Sizes: [5, 5, 5, 5]Kernels: [32, 32, 32, 32]Dilations: [1, 2, 3, 4]Output: M×N×128SDC Layer 5Input: M×N×256Convolutions: 4Sizes: [5, 5, 5, 5]Kernels: [32, 32, 32, 32]Dilations: [1, 2, 3, 4]Output: M×N×128SDC Layer 4Input: M×N×128Convolutions: 4Sizes: [5, 5, 5, 5]Kernels: [64, 64, 64, 64]Dilations: [1, 2, 3, 4]Output: M×N×256ConvolutionDilation rate: 1ConvolutionDilation rate: 2ConvolutionDilation rate: 3ConvolutionDilation rate: 4SDC LayerParallel convolutions: 4Kernel sizes: [5, 5, 5, 5]Dilation rates: [1, 2, 3, 4]Receptive field: 17presented in the supplementary material. Our setup yields a
receptive ﬁeld of 81 pixels.

Because we do not use any striding, dense image features
can be computed in a single forward pass without patch ex-
traction. This makes our design much faster than previous
deep descriptors [2, 11, 37] during inference.

Our design provides another advantage that can be used
within SDC layers: The same kernels are useful for differ-
ent scales (especially low level vision ﬁlters). Thus, it is
reasonable to share weights between the parallel convolu-
tions within one SDC block. The only requirement is that
the parallel convolutions are of the same shape. By shar-
ing weights, the amount of parameters gets divided by the
number of parallel convolutions (factor 4 in our case). This
allows to construct very light-weight feature networks with
a comparatively large receptive ﬁeld. To demonstrate that,
we drive network size to an extreme. In our experiments
in Section 4, we train a network with only about 5 % of
the parameters of our original design that we call Tiny. The
Tiny network has only 4 SDC blocks, each with only 3 par-
allel dilated convolutions of 3 × 3 kernels and dilation rates
1, 2, and 3 which share their weights, yielding a receptive
ﬁeld of 25 pixels.

3.3. Training Details

Our goal is a universal feature descriptor. Thus, we train
a uniﬁed feature network on multi-domain data. We use im-
ages of the training splits of the following data sets: Scene
ﬂow quadtuplets of KITTI 2015 [26], optical ﬂow and
stereo pairs from MPI Sintel [6], Middlebury stereo data
version 3 [29], Middlebury Optical Flow data [3], HD1K
Benchmark Suite for optical ﬂow [21], and the two-view
stereo data from ETH3D [30]. This is the union of data sets
which are used in the Robust Vision Challenge1 for optical
ﬂow and stereo. We further split 20 % and 10 % from the
KITTI training set for validation during training and eval-
uation of our experiments in Section 4 respectively. Since
image sizes, sequence count and lengths vary strongly be-
tween data sets, we sample image pairs non-uniformly from
each set and then select 100 patches from the reference im-
age. For each reference patch, we use the ground truth dis-
placement of non-occluded image regions and sample the
corresponding patch from the second view. Additionally,
we sample a third patch from the second view by altering
the ground truth displacement with a random offset to ob-
tain a negative correspondence. All details about the patch
sampling along with examples for the sampled triplets can
be found in the supplementary material.

We use a triplet training approach [17] where we feed the
reference patch, the matching patch and the non-matching
patch to three of our SDC networks with shared weights.
For training stability, we normalize the input by subtracting

1www.robustvision.net

Figure 4: Visualization of the triplet training. For each
patch triplet, we compute the loss based on the dis-
tance of the feature descriptors for corresponding and non-
corresponding patches.

the mean and dividing by the standard deviation of all train-
ing images. As objective function, we choose the thresh-
olded hinge embedding loss of [2] deﬁned in Equation 1.

L (r, p, n) = max(cid:16)0, kf (r) − f (p)k2

2 − τ(cid:17)

+ max(cid:16)0, m + τ − kf (r) − f (n)k2

2(cid:17) ,

(1)

where {r, p, n} is the patch triplet, f is the feature trans-
formation of the network, τ is the threshold, and m is the
margin between matching and non-matching features. We
have also experimented with the SoftMax-Triplet loss from
[17] and the SoftPN loss from [4]. Both showed similar
performance while being much less stable in training. An
overview of the training strategy is given in Figure 4.

We choose ADAM [20] as optimizer and train with a
batch size of 32 with an initial learning rate of 0.01 that we
exponentially decrease continuously by a power of 0.7 ev-
ery 100 k iterations. We train for 1 million iterations where
convergence saturates, or until overﬁtting which we rarely
observe in any of our experiments. Overﬁtting is avoided
by the random sampling strategy of image pairs and patch
triplets which provides many diverse combinations. Pho-
tometric data augmentation could not further improve the
training process. Instead, we note a small decrease in per-
formance. To speed up training, we crop the input patches
and intermediate feature representations to the maximum
required size for the respective dilation rate. The com-
plete training of our network takes about 3 days on a single
GeForce GTX 1080.

4. Experiments

We conduct a series of diverse experiments to validate
the superior performance of our approach compared to other
feature descriptors in Section 4.1. After demonstrating that
SDC features outperform heuristic descriptors as well as
other neural networks in image patch comparison, we will
test our SDC features with different algorithms for differ-
ent matching tasks on a large number of diverse data sets

2559

SDC NetworkPositiveMatchReferencePatchNegativeMatchPositiveDistanceNegativeDistanceLosssharedsharedSDC NetworkSDC NetworkTable 1: Comparison of the accuracy for representative
state-of-the-art descriptors and our SDC design. For learn-
ing approaches, we further provide information about re-
ceptive ﬁeld size (RF) in pixels, number of parameters
(Size) and accumulated sub-sampling factor due to striding.

Network

Accuracy

RF

Size

Factor

SDC (Ours)

LargeNet
L2Net [37]
Tiny (Ours)

PatchBatch [11]

DilNet

2Stream [43]
FFCNN [2]

BRIEF [7]
DAISY [38]

SIFT [24]

97.2 %
96.8 %
96.7 %
96.0 %
95.7 %
95.5 %
92.3 %
90.6 %

93.7 %
92.1 %
89.0 %

81
81
32
25
51
96
64
56

–
–
–

1.95 M
22.5 M
1.34 M
0.12 M
0.92 M
5.43 M
2.41 M
4.89 M

–
–
–

1
1
4
1
8
1
2
4

–
–
–

in Section 4.2. For all experiments, we use a single uniﬁed
descriptor network. Unlike others [2, 36], we do not re-train
or ﬁne-tune our network on each individual data set.

4.1. Accuracy, Robustness, ROC

In this section, we compare our SDC descriptor network
to other state-of-the-art descriptors. Representative classi-
cal, heuristic descriptors are SIFT [24], DAISY [38], and
BRIEF [7]. Furthermore, we train the following architec-
tures of previous work that contain striding layers:
• 2Stream: The central-surround network from [43].
• PatchBatch: The architecture of [11] which utilizes batch

normalization.

• L2Net: The basic variant of [37] with only a single stream
and without batch normalization, which we found to per-
form the best among all variants of this network.

• FFCNN: The FlowFieldsCNN architecture [2], which
showed great improvements over classical descriptors for
optical ﬂow estimation.

In addition, we design and evaluate two alternative architec-
tures that avoid striding layers.
• DilNet: An example of dilated convolution in a sequence:

Conv(7,64,1,1)–Conv(7,64,1,2)–Conv(7,128,1,3)–
Conv(7,128,1,4)–Conv(7,128,1,3)–Conv(7,256,1,2)–
Conv(7,128,1,1).

• LargeNet: An example for single, large convolutions
Conv(17,64,1,1)–Conv(17,64,1,1)–

without dilation:
Conv(17,128,1,1)–Conv(17,256,1,1)–Conv(17,128,1,1).

The four numbers of each convolution layer Conv(k,n,s,d)
describe square kernel size k, number of kernels n, stride
s, and dilation rate d. Note that DilNet and LargeNet try to
mimic the shape of our SDC network. More details about
each network are given in Table 1.

(a) ROC curves.

(b) Robustness curves.

Figure 5: In the comparison of ROC and robustness curves,
our SDC design outperforms state-of-the-art feature net-
works and heuristic descriptors.

ﬁne accuracy as percentage of correctly distinguished patch
triplets, i.e. the positive feature distance is smaller than the
negative one. Towards that end, we have sampled 2000
patch triplets from our test images (cf. Section 3.3 and
the supplementary material). The results are given in Ta-
ble 1. Our design outperforms all other feature descriptors
in terms of accuracy. Our receptive ﬁeld (RF) is compara-
tively large, while the network size is comparatively small
and we also avoid sub-sampling. Our Tiny version is ex-
tremely compact without much loss of accuracy. The sur-
prisingly good result of L2Net [37] is worth mentioning,
indicating that strided convolution should be preferred over
pooling. Also, some of the learning approaches perform
worse than the classical descriptors.

First, we evaluate the accuracy of all descriptors. We de-

We have also computed the Receiver-Operating-

2560

0.00.10.20.30.40.5False-Positive-Rate0.50.60.70.80.91.0True-Positive-RateSDC (ours)LargeNetL2NetTiny (ours)PatchBatchDilNet2StreamFFCNNBRIEFDAISYSIFT100101102Distance to correct match0.650.700.750.800.850.900.951.00RobustnessSDC (ours)LargeNetL2NetTiny (ours)PatchBatchDilNet2StreamFFCNNBRIEFDAISYSIFTCharacteristics (ROC) for all descriptors based on the same
test triplets. Therefore, we split each triplet into two pairs, a
positive and a negative one. True-Positive-Rates over False-
Postive-Rates for varying classiﬁcation thresholds are given
in Figure 5a. Again, our SDC features achieve top perfor-
mance with a large margin over heuristic descriptors and
most neural networks.

However, matching is not really a classiﬁcation task. The
distance of corresponding descriptors does not matter, as
long as it is smaller than these of non-matching descriptors.
To take this into account, we have set up a ﬁnal experiment
to show the matching robustness of the descriptors as intro-
duced by [2]. We have tested each positive corresponding
patch pair of our test data against all other correspondences
within a certain distance to the correct match. The results
are shown in Figure 5b. Naturally, the robustness is higher
for larger distances to the correct patch. This experiment
validates the effectiveness of our design once again. SDC
achieves the highest robustness throughout the whole range
of displacements. Our top performance is then followed by
a dense cluster of other deep descriptors including our Tiny
variant. Note the performance of all networks which are ex-
plicitly designed to avoid sub-sampling (no strides greater
than 1), especially for small offsets.

4.2. Cross Task and Cross Domain Matching

For the second part of our experiments, we apply our
feature descriptor in actual matching tasks.
In total, we
test 5 algorithms for 3 dense matching tasks with overall 6
data sets. For stereo matching, we evaluate ELAS [12] and
SGM [16] on KITTI [26], Middlebury [29], and ETH3D
[30]. CPM [19] and FlowFields++ [32] are selected to rep-
resent optical ﬂow matching algorithms and are evaluated
on KITTI [26], Middlebury [3], HD1K [21], and MPI Sin-
tel [6]. Finally, we test SceneFlowFields (SFF) [33] on
KITTI [26]. Where possible, we evaluate the non-occluded
areas (noc) and the full image (all) separately, because vi-
sual matching is only possible in visible regions. On KITTI,
these regions are further split into static background (bg)
and dynamic foreground (fg). For the Middlebury stereo
data, we evaluate all levels of resolution: Full (F), half
(H), and quarter resolution (Q). For Sintel, we consider the
more realistic ﬁnal rendering pass only. We have computed
baseline results for the common error metrics average end-
point error (EPE) and the percentage of outliers with an
EPE greater than 3 pixels (>3px) for all data sets. We then
change the feature descriptor of every algorithm to our SDC
features and repeat the experiment. It is important to note,
that we change nothing but the descriptor. For the sake of
comparability, we do not ﬁne tune any algorithm, though
we expect ﬁne-tuning to improve the results in general.

Stereo Matching. ELAS [12] uses ﬁrst order image gra-
dients for feature description. We use the default parameter

Table 2: Evaluation of stereo matching algorithms. We
compare ELAS [12] and SGM [16] with the default descrip-
tors and our SDC features on KITTI [26], Middlebury [29],
and ETH3D [30].

ELAS [12]

SGM [16]

Data set

Original

EPE >3px

SDC (ours)
EPE

Original

EPE >3px

SDC (ours)
EPE

bg
fg
all

bg
fg
all

F
H
Q

F
H
Q

c
o
I n
T
T
I
K

l
l
a

c
o
n

l
l
a

y
r
u
b
e
l
d
d
i
M

D
3
H
T
E

noc
occ
all

>3px

6.56
12.21
7.39

7.22
14.34
8.29

26.33
16.85
11.62

29.87
21.02
15.91

6.03
17.68
6.50

1.30
1.88
1.38

1.34
2.02
1.45

20.42
4.44
2.03

22.47
6.03
2.91

0.98
2.14
1.02

4.30
8.25
4.88

4.86
11.28
5.83

22.24
12.03
10.12

26.22
16.97
15.19

2.17
12.99
2.61

1.08
1.41
1.13

1.12
1.62
1.19

20.08
3.42
1.91

22.16
5.08
2.86

0.60
1.64
0.64

>3px

4.32
6.46
4.36

4.65
7.25
5.03

43.92
15.93
10.43

47.28
19.71
14.77

2.83
6.40
3.62

1.02
1.15
1.04

1.11
1.54
1.18

44.45
6.12
1.81

47.56
7.64
2.74

0.65
1.36
0.81

3.44
7.70
4.06

3.61
8.61
4.34

45.52
13.37
8.80

48.09
16.73
12.26

3.11
4.81
3.49

0.98
1.40
1.04

1.00
1.68
1.10

52.41
6.98
1.75

53.50
8.26
2.46

0.75
1.11
0.83

set called MIDDLEBURY which includes interpolation af-
ter consistency check. In addition, we obtain an open source
implementation of SGM2 which uses the symmetric CEN-
SUS transform [34] of 9 × 7 patches as a descriptor.

Results for both algorithms on all stereo data sets are
given in Table 2. Green color indicates where our features
outperform the baseline; decrease in accuracy is marked in
red. In case of ELAS [12], the impact of SDC features is
advantageous in all cases, and even signiﬁcant most of the
time. SGM [16] shows a couple of negative test cases. First
of all, the full resolution (F) images of Middlebury [29]
which produce bad results for both descriptors on both data
sets, since the default parameters of ELAS [12] and SGM
[16] are not adjusted to the maximum possible disparity of
that resoltuion. This might also apply to the half resolution
images (H) to some extend. As a consequence, this data
should not be considered in the comparison. Then there
is the foreground regions of KITTI [26], where our deep
features perform slightly worse than CENSUS. This might
be, because foreground regions are underrepresented in the
data set, and thus in the randomly sampled training patches.
Lastly, the non-occluded areas of ETH3D [30] show min-
imally higher errors for our features. However, the large
receptive ﬁeld of SDC features can compensate for that in
occluded regions to improve the overall results.
In sum-
mary, SDC features improve dense stereo matching for both
algorithms on all data sets.

Optical Flow Correspondences. CPM [19] computes
sparse matches in non-overlapping 3 × 3 blocks that can
be used for interpolation with EPICFlow [27] or RICFlow
[18]. The original feature descriptor is SIFT [24]. We
evaluate the generated matches of this algorithm in Ta-

2www.github.com/gishi523/semi-global-matching

2561

Table 3: Optical ﬂow evalation with FlowFields++ [32]. We compare SIFT [24] to our SDC features on KITTI [26], Sintel
[6], Middlebury [3], and HD1K [21]. Results for dense matching, after consistency check, and after interpolation are shown.

Matching

Filtered

Interpolated

Data set

SIFT [24]

>3px

EPE

SDC (ours)
>3px

EPE >3px

SIFT [24]

SDC (ours)

SIFT [24]

EPE Density >3px

EPE Density >3px

EPE >3px

SDC (ours)
EPE

c
o
I n
T
T
I
K

l
l
a

bg
fg
all

bg
fg
all

l noc
occ
all

e
t
n
i
S

23.22
27.61
23.98

35.96
29.17
34.93

16.21
83.18
21.88

12.07
14.47
12.48

53.20
21.81
48.45

9.31

120.78
18.75

Middlebury

5.47

1.21

HD1K

15.52

12.99

15.25
16.90
15.53

29.19
18.64
27.59

10.15
78.85
15.97

3.79

7.76

6.56
4.45
6.19

39.42
24.15
37.11

5.17
89.72
12.33

0.76

7.48

8.04
10.31
8.39

9.02
10.32
9.22

4.15
42.59
5.15

2.24

5.64

1.89
2.11
1.92

3.09
2.11
2.94

1.00
10.10
1.23

0.51

1.16

–
–

73.3 %

–
–

63.3 %

–
–

75.7 %

93.1 %

82.6 %

6.91
9.10
7.27

8.30
9.10
8.43

3.61
43.81
4.82

2.26

4.10

2.00
2.01
2.00

3.62
2.01
3.36

0.96
10.71
1.25

0.49

1.02

–
–

86.1 %

9.56
6.13
8.97

–
–

19.13
6.46
74.7 % 17.21

–
–

84.4 %

96.9 %

94.4 %

6.35
45.04
9.62

1.69

4.34

3.08
1.97
2.89

9.45
2.14
8.34

2.34
22.26
4.03

0.28

0.96

8.52
8.99
8.60

17.19
9.19
15.98

6.10
46.80
9.55

1.79

4.62

2.97
2.57
2.90

9.13
2.71
8.16

2.18
21.33
3.80

0.30

1.31

e
g
a
m

I

l
a
n
i
g
i
r

O

C
D
S
h
t
i

w

KITTI [26]

Sintel [6]

Middlebury [3]

HD1K [21]

Figure 6: Exemplary visual comparison of ﬁltered optical ﬂow from FF++ [32] on four different data sets. The second row
shows results for the original method, while the bottom row shows results after changing the feature descriptor to SDC. Note
that all parameters are the same for both experiments. Quantitative evaluation on full data sets is provided in Table 3.

Table 4: Evaluation of optical ﬂow matching with CPM
[19]. We compare SIFT [24] and our SDC features on
KITTI [26], Sintel [6], Middlebury [3], and HD1K [21].

Data set

SIFT [24]

SDC (ours)

>3px

EPE

Density >3px

EPE

Density

c
o
I n
T
T
I
K

l
l
a

bg
fg
all

bg
fg
all

e
t
n
i

l noc
occ
all

S

Middlebury

HD1K

10.69
12.67
11.26

11.71
12.67
11.87
4.33
45.03
5.30

4.11

5.85

2.17
2.40
2.21

3.28
2.40
3.13
1.06
10.49
1.28

0.79

1.29

–
–

7.88 %

–
–

6.79 %

–
–

8.93 %

8.37
9.96
8.64

9.48
9.96
9.56
4.64
49.52
5.90

10.10 % 2.57

9.80 %

4.46

2.30
2.14
2.30

3.79
2.14
3.51
1.18
12.56
1.50

0.66

1.17

–
–

9.83 %

–
–

8.50 %

–
–

9.52 %

10.49 %

10.54 %

ble 4. FlowFields++ (FF++) [32] performs dense match-
ing, followed by a consistency check and interpolation with
RICFlow [18]. We compare the results between the orig-

inally used SIFT features [24] and our SDC features after
each of these 3 steps in Table 3. For the ﬁltered results after
the consistency check, we also give the density as percent-
age of covered ground truth pixels. Visual examples are
given in Figure 6.

In some cases, both algorithms show a slight increase
in endpoint error for the complete KITTI data (all) when
used with our SDC features. This is most likely due to the
fact, that the KITTI noc data excludes the out-of-bounds
motions only, not the real occlusions. A higher endpoint er-
ror in the occluded areas is actually an advantage, because
it makes outlier ﬁltering during consistency check easier. In
fact, EPE and outliers are better for KITTI-all-fg for FF++
after ﬁltering (see Table 3). Also, it is important to note that
the ﬁltered matches with SDC are much denser for both al-
gorithms (cf. Figure 6). Dense, well distributed matches
make interpolation easier. This way, our feature descriptor
supports the whole pipeline. Again, we did not change any-
thing but the descriptor, not even the distance function that

2562

Table 5: Results for scene ﬂow estimation. SceneFlowFields [33] with SIFTFlow [23] features and our SDC features are
compared on the KITTI Scene Flow Benchmark [26]. The densities after ﬁltering increase from 43.6 % to 67.0 % in noc and
from 36.4 % to 56.0 % in all regions when using SDC features.

Matching

Filtered

Interpolated

Ego-motion Reﬁnement

Data

SIFTFlow

SDC (ours)

SIFTFlow

SDC (ours)

SIFTFlow

>3px

EPE >3px

EPE >3px EPE >3px EPE >3px

SDC (ours)

SDC (ours)
EPE >3px EPE >3px EPE >3px EPE

SIFTFlow

1
D

2
D

l

F

F
S

c
o
n

l
l
a

c
o
n

l
l
a

c
o
n

l
l
a

c
o
n

l
l
a

bg
fg
all

bg
fg
all

bg
fg
all

bg
fg
all

bg
fg
all

bg
fg
all

bg
fg
all

bg
fg
all

9.84
16.23
10.91

11.62
20.64
12.99

17.49
16.65
17.35

31.38
20.80
29.61

22.95
25.40
23.36

36.68
29.70
35.47

29.99
34.97
30.82

42.68
40.04
42.28

2.00
2.64
2.11

4.93
15.64
6.55

2.82
2.88
2.83

8.47
5.20
7.97

9.07
7.06
8.74

45.26
17.64
41.08

–
–
–

–
–
–

4.69
9.72
5.53

6.53
14.51
7.59

10.49
11.41
10.65

25.54
15.91
24.09

13.25
14.42
13.44

28.36
17.60
26.73

17.65
22.03
17.65

31.19
28.09
31.19

1.10
1.89
1.23

5.63
10.08
6.31

1.80
1.86
1.81

8.71
5.54
8.23

5.10
4.34
4.97

38.11
8.20
33.58

–
–
–

–
–
–

2.29
2.76
2.37

2.30
2.76
2.38

2.74
2.75
2.74

2.83
2.75
2.82

2.34
2.14
2.31

2.43
2.14
2.38

4.51
5.00
4.59

4.61
5.00
4.67

0.85
0.80
0.84

0.85
0.80
0.84

0.92
0.88
0.91

0.94
0.88
0.93

0.85
1.05
0.88

0.97
1.05
0.99

–
–
–

–
–
–

1.39
2.81
1.60

1.40
2.82
1.61

1.94
2.88
2.08

2.11
2.88
2.23

2.55
3.00
2.61

2.71
3.00
2.76

3.83
5.39
3.83

3.73
5.40
3.98

0.68
0.78
0.69

0.68
0.78
0.69

0.80
0.88
0.81

0.84
0.88
0.85

0.95
1.19
0.98

1.11
1.19
1.12

–
–
–

–
–
–

4.94
7.85
5.43

5.33
7.78
5.71

12.05
9.91
11.69

18.11
9.85
16.86

17.77
11.48
16.72

26.84
12.52
24.68

20.38
16.25
19.69

26.84
17.36
27.43

1.04
1.33
1.09

1.13
1.33
1.16

2.24
1.69
2.15

3.30
1.68
3.06

6.65
2.73
5.99

13.42
2.83
11.82

–
–
–

–
–
–

4.26
7.59
4.82

4.58
8.20
5.13

7.64
8.48
7.78

12.97
10.70
12.63

10.18
7.52
9.73

18.31
8.88
16.89

12.52
13.72
12.72

20.46
16.42
19.85

0.93
1.17
0.97

1.02
1.26
1.06

1.35
1.38
1.36

2.20
1.51
2.09

2.52
1.92
2.42

7.44
2.12
6.64

–
–
–

–
–
–

–
–
–

–
–
–

6.89
10.33
7.47

8.80
10.24
9.02

9.42
13.05
10.03

13.04
13.97
13.18

11.24
17.48
12.28

14.68
18.49
15.26

–
–
–

–
–
–

1.47
1.62
1.49

1.82
1.61
1.79

2.27
3.42
2.46

4.71
3.47
4.52

–
–
–

–
–
–

–
–
–

–
–
–

6.12
8.61
6.54

8.74
10.82
9.05

8.10
9.07
8.26

11.73
10.31
11.51

9.46
15.03
10.39

12.97
17.62
13.67

–
–
–

–
–
–

1.17
1.43
1.21

1.61
1.57
1.60

2.02
2.52
2.11

5.00
2.68
4.65

–
–
–

–
–
–

is used to compare the feature descriptors. CPM [19] for
example uses the sum of absolute difference as feature dis-
tance, while our network was trained using the L2 distance.
Overall, our SDC features reduce the outliers during optical
ﬂow matching by up to 50 %.

Matching-based Scene Flow Algorithms. SceneFlow-
Fields (SFF) [33] is the stereo extension of FlowFields
[1, 31] to estimate 3D motion. The pipeline is compara-
ble to FF++ except for one additional reﬁnement step that
is used in SFF where the authors estimate the ego-motion
to adjust the sceneﬂow of the static scene. We evaluate all
intermediate results and present them in Table 5.

Similar to our experiments on stereo and optical ﬂow, our
SDC features improve scene ﬂow matching signiﬁcantly
which results in almost half the percentage of outliers and
endpoint errors. This effect can be maintained throughout
the whole pipeline for almost all image regions. As be-
fore, outlier ﬁltering at the foreground regions (fg) of KITTI
seems to be more difﬁcult with SDC features which could
probably be solved by adjusting the consistency threshold.
The minor decrease in correctness of the ﬁltered matches
might again be acceptable when considering that SDC fea-
tures increase the ﬁltered density from 43.6 % to 67.0 %
and from 36.4 % to 56.0 % in non-occluded (noc) and all
image regions (cf. Figure 1). Our SDC features improve

scene ﬂow matching over all image regions (including un-
matchable, occluded areas) by more than 10 percent points
which corresponds to 25 % less outliers after matching.

5. Conclusion

Based on the observation that dilated convolution is re-
lated to sub-scale ﬁltering, we have designed a novel layer
by stacking multiple parallel dilated convolutions (SDC).
These SDC layers have been combined to a new architec-
ture that can be used for image feature description. For all
experiments, we have used only a single uniﬁed network
for all data sets and algorithms. Our SDC features have out-
performed heuristic image descriptors like SIFT and other
descriptor networks from previous works in terms of accu-
racy and robustness.
In a second set of experiments, we
have applied our SDC feature network for different match-
ing tasks on many diverse data sets and have shown that our
deep descriptor improves matching for stereo, optical ﬂow,
and scene ﬂow drastically yielding a better ﬁnal result in the
majority of cases.

Acknowledgments

This work was partially funded by the BMW Group and
partly by the Federal Ministry of Education and Research
Germany in the project VIDETE (01IW18002).

2563

References

[1] Christian Bailer, Bertram Taetz, and Didier Stricker. Flow
Fields: Dense correspondence ﬁelds for highly accurate large
displacement optical ﬂow estimation. In International Con-
ference on Computer Vision (ICCV), 2015. 8

[2] Christian Bailer, Kiran Varanasi, and Didier Stricker. CNN-
based patch matching for optical ﬂow with thresholded hinge
embedding loss. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2017. 2, 4, 5, 6

[3] Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth,
Michael J Black, and Richard Szeliski. A database and eval-
uation methodology for optical ﬂow. International Journal
of Computer Vision (IJCV), 2011. 2, 4, 6, 7

[4] Vassileios Balntas, Edward Johns, Lilian Tang, and Krys-
tian Mikolajczyk.
PN-Net: Conjoined triple deep net-
work for learning local image descriptors. arXiv preprint
arXiv:1601.05030, 2016. 4

[5] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. SURF:
In European Conference on

Speeded up robust features.
Computer Vision (ECCV), 2006. 2

[6] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation.
In European Conference on Computer Vision (ECCV), 2012.
1, 2, 4, 6, 7

[7] Michael Calonder, Vincent Lepetit, Christoph Strecha, and
Pascal Fua. BRIEF: Binary robust independent elemen-
tary features. In European Conference on Computer Vision
(ECCV), 2010. 2, 5

[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. DeepLab: Semantic im-
age segmentation with deep convolutional nets, atrous con-
volution, and fully connected CRFs. Transactions on Pattern
Analysis and Machine Intelligence (PAMI), 2018. 2, 3

[9] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning
a similarity metric discriminatively with application to face
veriﬁcation. In Conference on Computer Vision and Pattern
Recognition (CVPR), 2005. 2

[10] Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochre-
iter. Fast and accurate deep network learning by exponential
linear units (ELUs). arXiv preprint arXiv:1511.07289, 2015.
3

and metric learning for patch-based matching. In Conference
on Computer Vision and Pattern Recognition (CVPR), 2015.
2

[16] Heiko Hirschm¨uller. Stereo processing by semiglobal match-
ing and mutual information. Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 2008. 6

[17] Elad Hoffer and Nir Ailon. Deep metric learning using triplet
In International Workshop on Similarity-Based

network.
Pattern Recognition (SIMBAD), 2015. 2, 4

[18] Yinlin Hu, Yunsong Li, and Rui Song. Robust interpola-
tion of correspondences for large displacement optical ﬂow.
In Conference on Computer Vision and Pattern Recognition
(CVPR), 2017. 6, 7

[19] Yinlin Hu, Rui Song, and Yunsong Li. Efﬁcient coarse-
to-ﬁne patchmatch for large displacement optical ﬂow.
In
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016. 1, 6, 7, 8

[20] Diederik P Kingma and Jimmy Ba. Adam: A method for
In International Conference for

stochastic optimization.
Learning Representations (ICLR, 2015. 4

[21] Daniel Kondermann, Rahul Nair, Katrin Honauer, Karsten
Krispin, Jonas Andrulis, Alexander Brock, Burkhard Gusse-
feld, Mohsen Rahimimoghaddam, Sabine Hofmann, Claus
Brenner, et al. The HCI benchmark suite: Stereo and ﬂow
ground truth with uncertainties for urban autonomous driv-
ing. In Conference on Computer Vision and Pattern Recog-
nition (CVPR) Workshops, 2016. 2, 4, 6, 7

[22] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet:
Dilated convolutional neural networks for understanding the
highly congested scenes. In Conference on Computer Vision
and Pattern Recognition (CVPR), 2018. 3

[23] Ce Liu, Jenny Yuen, and Antonio Torralba. SIFT Flow:
Dense correspondence across scenes and its applications.
Transactions on Pattern Analysis and Machine Intelligence
(PAMI), 2011. 8

[24] David G Lowe. Object recognition from local scale-invariant
In International Conference on Computer Vision

features.
(ICCV), 1999. 1, 2, 5, 6, 7

[25] Wenjie Luo, Alexander G Schwing, and Raquel Urtasun. Ef-
ﬁcient deep learning for stereo matching. In Conference on
Computer Vision and Pattern Recognition (CVPR), 2016. 2

[11] David Gadot and Lior Wolf. Patchbatch: A batch augmented
loss for optical ﬂow. In Conference on Computer Vision and
Pattern Recognition (CVPR), 2016. 2, 4, 5

[26] Moritz Menze and Andreas Geiger. Object scene ﬂow for
In Conference on Computer Vision

autonomous vehicles.
and Pattern Recognition (CVPR), 2015. 1, 2, 4, 6, 7, 8

[12] Andreas Geiger, Martin Roser, and Raquel Urtasun. Efﬁ-
In Asian Conference on

cient large-scale stereo matching.
Computer Vision (ACCV). 2010. 1, 6

[13] Fatma G¨uney and Andreas Geiger. Deep discrete ﬂow. In

Asian Conference on Computer Vision (ACCV), 2016. 2

[14] Ryuhei Hamaguchi, Aito Fujita, Keisuke Nemoto, Tomoyuki
Imaizumi, and Shuhei Hikosaka. Effective use of dilated
convolutions for segmenting small object instances in remote
sensing imagery.
In Winter Conference on Applications of
Computer Vision (WACV), 2018. 2

[15] Xufeng Han, Thomas Leung, Yangqing Jia, Rahul Suk-
thankar, and Alexander C Berg. MatchNet: Unifying feature

[27] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and
Cordelia Schmid. EpicFlow: Edge-preserving interpolation
of correspondences for optical ﬂow. In Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2015. 6

[28] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary
Bradski. ORB: An efﬁcient alternative to SIFT or SURF. In
International Conference on Computer Vision (ICCV), 2011.
2

[29] Daniel Scharstein and Richard Szeliski. A taxonomy and
evaluation of dense two-frame stereo correspondence algo-
rithms.
International Journal of Computer Vision (IJCV),
2002. 2, 4, 6

2564

Conference on Computer Vision and Pattern Recognition
(CVPR), 2015. 2, 5

[44] Jure Zbontar and Yann LeCun. Computing the stereo match-
ing cost with a convolutional neural network. In Conference
on Computer Vision and Pattern Recognition (CVPR), 2015.
2

[30] Thomas Sch¨ops, Johannes L Sch¨onberger, Silvano Galliani,
Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An-
dreas Geiger. A multi-view stereo benchmark with high-
resolution images and multi-camera videos. In Conference
on Computer Vision and Pattern Recognition (CVPR), 2017.
1, 2, 4, 6

[31] Ren´e Schuster, Christian Bailer, Oliver Wasenm¨uller, and
Didier Stricker. Combining stereo disparity and optical ﬂow
for basic scene ﬂow. In Commercial Vehicle Technology Sym-
posium (CVT), 2018. 8

[32] Ren´e Schuster, Christian Bailer, Oliver Wasenm¨uller, and
Didier Stricker. FlowFields++: Accurate optical ﬂow corre-
spondences meet robust interpolation. In International Con-
ference on Image Processing (ICIP), 2018. 6, 7

[33] Ren´e Schuster, Oliver Wasenm¨uller, Georg Kuschk, Chris-
tian Bailer, and Didier Stricker. SceneFlowFields: Dense
interpolation of sparse scene ﬂow correspondences. In Win-
ter Conference on Applications of Computer Vision (WACV),
2018. 1, 6, 8

[34] Robert Spangenberg, Tobias Langner, and Ra´ul Rojas.
Weighted semi-global matching and center-symmetric cen-
sus transform for robust driver assistance. In International
Conference on Computer Analysis of Images and Patterns
(CAIP), 2013. 6

[35] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas
Brox, and Martin Riedmiller. Striving for simplicity: The
all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
3

[36] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
PWC-Net: CNNs for optical ﬂow using pyramid, warping,
and cost volume.
In Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 5

[37] Yurun Tian, Bin Fan, Fuchao Wu, et al. L2-Net: Deep learn-
ing of discriminative patch descriptor in euclidean space.
In Conference on Computer Vision and Pattern Recognition
(CVPR), 2017. 2, 4, 5

[38] Engin Tola, Vincent Lepetit, and Pascal Fua. DAISY: An
efﬁcient dense descriptor applied to wide-baseline stereo.
Transactions on Pattern Analysis and Machine Intelligence
(PAMI), 2010. 2, 5

[39] Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua
Huang, Xiaodi Hou, and Garrison Cottrell. Understanding
convolution for semantic segmentation.
In Winter Confer-
ence on Applications of Computer Vision (WACV), 2018. 2,
3

[40] Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi
Feng, and Thomas S Huang. Revisiting dilated convolution:
A simple approach for weakly-and semi-supervised seman-
tic segmentation.
In Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 2

[41] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-
tion by dilated convolutions. In International Conference on
Learning Representations (ICLR), 2016. 2

[42] Ramin Zabih and John Woodﬁll. Non-parametric local trans-
In European

forms for computing visual correspondence.
Conference on Computer Vision (ECCV), 1994. 1, 2

[43] Sergey Zagoruyko and Nikos Komodakis. Learning to com-
In

pare image patches via convolutional neural networks.

2565

