Tightness-aware Evaluation Protocol for Scene Text Detection

Yuliang Liu, Lianwen Jin∗, Zecheng Xie, Canjie Luo, Shuaitao Zhang, Lele Xie

College of Electronic Information Engineering, South China University of Technology

liu.yuliang@mail.scut.edu.cn; lianwen.jin@gmail.com

Abstract

Evaluation protocols play key role in the developmen-
tal progress of text detection methods. There are strict re-
quirements to ensure that the evaluation methods are fair,
objective and reasonable. However, existing metrics exhib-
it some obvious drawbacks: 1) They are not goal-oriented;
2) they cannot recognize the tightness of detection methods;
3) existing one-to-many and many-to-one solutions involve
inherent loopholes and deﬁciencies. Therefore, this pa-
per proposes a novel evaluation protocol called Tightness-
aware Intersect-over-Union (TIoU) metric that could quan-
tify completeness of ground truth, compactness of detection,
and tightness of matching degree. Speciﬁcally, instead of
merely using the IoU value, two common detection behav-
iors are properly considered; meanwhile, directly using the
score of TIoU to recognize the tightness. In addition, we
further propose a straightforward method to address the an-
notation granularity issue, which can fairly evaluate word
and text-line detections simultaneously. By adopting the
detection results from published methods and general ob-
ject detection frameworks, comprehensive experiments on
ICDAR 2013 and ICDAR 2015 datasets are conducted to
compare recent metrics and the proposed TIoU metric. The
comparison demonstrated some promising new prospects,
e.g., determining the methods and frameworks for which the
detection is tighter and more beneﬁcial to recognize. Our
method is extremely simple; however, the novelty is none
other than the proposed metric can utilize simplest but rea-
sonable improvements to lead to many interesting and in-
sightful prospects and solving most the issues of the pre-
vious metrics. The code is publicly available at https:
//github.com/Yuliang-Liu/TIoU-metric.

1. Introduction

Recent metrics for evaluating text detection have been
adopted from the object detection Pascal VOC metric [4].
However, unlike object detection, text detection tasks re-
quire the bounding box to be tighter because the primary
goal of detection is to recognize the text. Simply adopting

(a) Cutting.

(b) Pure.

(c) Outlier-GTs.

(d) Cutting & Outlier-GTs.

Figure 1. Unreasonable cases obtained using recent evaluation
metrics. (a), (b), (c), and (d) all have the same IoU of 0.66 against
the GT. Red: GT. Blue: detection.

the same IoU metric for text detection leads to the following
issues:

• As shown in Fig. 1 (a), detection over a ﬁxed IoU
threshold with the ground truth (GT) may not com-
pletely recall the text (some characters are missed);
however, previous metrics consider that the GT has
been entirely recalled.

• As shown in Figs. 1 (b), (c), and (d), detection over
a ﬁxed IoU threshold with the GT may still contain
background noise; however, previous metrics consider
such detection to have 100% precision.

• As shown in Fig. 1, previous metrics consider detec-
tions (a), (b), (c), and (d) to be equivalent perfect de-
tections because they all have the same IoU value that
is higher than a threshold. However, considering that
the primary goal of detection is to recognize the text,
these detections are not equivalent: 1) In (a), there is
no way to recognize the characters outside the detec-
tion bounding box; 2) in (c), it is very difﬁcult for a
recognizer to distinguish which is the target GT; 3) the
issues pertaining to both (a) and (c) can simultaneous-
ly occur for (d); 4) as for (b), it is easy for a normal
text recognizer to recognize the content correctly.

• Previous metrics severely rely on an IoU threshold.

43219612

However, if a relatively high IoU threshold is set, some
satisfactory bounding boxes may be discarded (e.g., if
0.7 is set as the threshold, the detection in Fig. 1 (b)
will be misjudged); if a low IoU threshold is set, sev-
eral inexact bounding boxes would be included.

The essential reasons that cause such inequalities are: 1)
The detections in Figs. 1 (a) and (d) cut the GT region; 2)
the detections in Figs. 1 (c) and (d) both contain outlier-
GT; and 3) recent metrics use binary results (0 or 100%) to
represent the ﬁnal recall or precision score.

To solve these issues, an intuitive solution is to penalize
the exceptional detections. In the proposed Tightness-aware
Intersection-over-Union (TIoU) metric, we use the occupa-
tion ratio of detection to GT and occupation ratio of outlier-
GT to detection as penalty factors, respectively. In addition,
we directly use the score of the TIoU as the value of recall
and precision, and thus, the compact degree among differ-
ent methods can be distinguished. The TIoU metric targets
the following three characteristics, which are also the main
contributions of this work:

• Completeness. Using the TIoU metric would force
methods to pay more attention to recalling every part
of the GT, i.e., ensuring the completeness of GT.

• Compactness. Because the detections of outlier-GT
will be punished by TIoU, the compactness of the de-
tection would receive more attention.

• Tightness-aware. TIoU can distinguish the tightness
among different detection methods, i.e., a 0.9 IoU de-
tection would be much better than a 0.5 IoU detection
in our metric.

2. Related work

Evaluation methods [18, 28, 4] have been an important
research topic for several decades. For scene text detec-
tion, there are four mainstream evaluation methods that are
largely identical but have minor differences: ICDAR 2003
(IC03) [16], ICDAR 2013 (IC13) [9, 28], ICDAR 2015
(IC15) [8], and AP-based methods [4, 26].

2.1. ICDAR 2003 Evaluating Method

The early IC03 metric [16] is based on the notions used
by the information retrieval community to calculate preci-
sion and recall, which are as follows:

Recall(G, D) =

P|G|

i=1 BestM atchG(Gi)

|G|

,

(1)

P recision(G, D) =

P|D|

j=1 BestM atchD(Dj)

|D|

,

(2)

where, BestM atchG and BestM atchD indicate the result
of the closest match between detection and ground truth
rectangles, as deﬁned below [28]:

BestM atchG(Gi) = max

j=1...|D|

2 · Area(Gi ∩ Dj)

Area(Gi) + Area(Dj)

,

(3)

BestM atchD(Dj) = max

i=1...|G|

2 · Area(Dj ∩ Gi)

Area(Dj) + Area(Gi)

.

(4)
The matching mechanism involves ﬁnding the perfect-
matching pairs to detect the matching value, i.e., if a rectan-
gle is perfectly matched, the match value is unity, else the
value is less than 1. The disadvantages of the IC03 met-
ric are as follows: 1) Multiple detections can be repeatedly
matched to the same GT; 2) as the authors reported in [16]
themselves, this evaluation scheme only considers one-to-
one (OO) matches, and thus one-to-many (OM) and many-
to-one (MO) matches [28] are considered to have zero recall
and precision. However, practically, word-level and text-
line detections can both be conducive to recognition.

2.2. ICDAR2013 Evaluation Method

IC13 [9, 28] has three evaluation metrics: IC13, DetEval,
and IoU. The IoU metric is subsequently adopted following
the IC15 [8] metric, which is discussed in the next subsec-
tion.

2.2.1

ICDAR2013, DetEval

Unlike Eqs. 3 and 4, the criteria of these two evaluations are
based on mutual overlap rates between detection ({Dj}j )
and ground truth ({Gi}i):

A(Gi ∩ Dj)

A(Dj)

> tp,

A(Gi ∩ Dj)

A(Gi)

> tr,

(5)

(6)

where, tp and tr are the thresholds of precision and recall,
respectively. The metrics evaluate the methods using three
steps including OO, OM, and MO. Unlike the IC03 metric,
OO guarantees that each GT can be at most matched once,
and multiple matching detections are considered false posi-
tives.

OM indicates that a GT is matched by a set of detection
results, and it should satisfy two requirements: a) Sufﬁcient
detections covering the GT; b) each contributing detection is
covered enough by the GT. If these conditions are satisﬁed,
the precision value of each detection box and recall value of
the GT are both set to 0.8 [9].

MO indicates that a detection is matched by a set of GT.
The following two requirements must be satisﬁed: a) The
detection must contain sufﬁciently overlapping GT; b) each

43229613

GT must be recalled with sufﬁcient area. If these conditions
are satisﬁed, the recall value of each GT and the precision
value of this detection are both set to 1.

Although these two matching methods can overcome the
problem of inconsistency of the annotation granularity to
some extent, such metrics still involve some unsatisfactory
circumstances (examples are shown in Fig. 2):

• For MO matching, a rough detection can recall all GTs,
as shown in the left image of the ﬁrst row of Fig. 2.
Such rough detection may not be proper recognized,
which is relatively unfair for those methods that can
granularly detect the text.

• For OM matching, many segmented detections are
considered as correct results, which is unfair for sin-
gularly perfect detection. Some examples are shown
in the right image of the ﬁrst row of Fig. 2.
It is
worth mentioning that although 0.8 is set to penalize
over-segmentation, loophole still exists. For example,
if the overall precision is less than 0.8, a method that
separates a perfect detection into numerous small over-
segmented OM detections (e.g., 20) can make the pre-
cision close to 0.8. An example is given below:

origin precision =

0 + 1 + 0 + 0

4

20

= 0.25

(7)

f ake precision =

0 +

z
0.8 + ... + 0.8 +0 + 0

}|

{

23

= 0.7

(8)

• For the IC13 metric, the matching order is OO, OM
and MO. Because each GT and detection can only be
matched once, the OO matching pairs would not be
calculated during OM or MO matching, which would
affect the detection results. As shown in the bottom
image of Fig. 2, the long detections do not satisfy MO
matching because the bluish detections have already
OO matched with some GT in advance. Thus, the long
detections are regarded as false positives and the rest
of the GTs are considered to be not recalled.

• For DetEval evaluation, OM and MO are evaluated in
advance before OO, which may lead to unsatisfactory
results. As shown in the middle image of Fig. 2, be-
cause OM is validated in advance, the two detection
bounding boxes have already been used. Thus, the two
jacinth GTs are considered not to be recalled.

2.3. ICDAR 2015 IoU Metric

The IC15 metric [8] follows the same metric as Pascal
VOC [4]. Under this metric, detections are assigned to

Figure 2. The ﬁrst row shows examples of many-to-one and one-
to-many matching. The second and third rows show some failure
matching cases (using IC13 metric) because of the matching order.
Red: GT bounding box. Blue: detections. Jacinth: GT bounding
boxes that can not be recalled because of one-to-many matching.
Bluish: detection box that causes failure of many-to-one matching.

ground truth objects and judged to be true or false posi-
tives by measuring the bounding box overlap. To be con-
sidered a correct detection, the value of Intersection-over-
Union (IoU, deﬁned in Eq. 9) must exceed 0.5.

A(Gj ∩ Di)
A(Gj ∪ Di)

> 0.5.

(9)

Detections are assigned to true positives only when they
satisfy the overlap criterion and they have the top ranking
conﬁdence to the target GT. Ground truth objects with no
matching detections are false negatives. Although this met-
ric has received the most attention, it still has many draw-
backs, which have been pointed out in the introduction sec-
tion.

2.4. AP based evaluation method

To avoid ﬁnetuning the output detection conﬁdence,
dataset such as RCTW-17 [26] has adopted interpolated av-
erage precision as the main detection evaluation metric: For
a given task and class, the precision-recall curve is com-
puted based on the method’s ranked output. Basically, this
metric relies on the IoU metric to calculate the precision and
recall in advance.

3. Methodology

3.1.

Tightness aware

Intersection over Union

(TIoU) Metric

The primary goal of text detection evaluation metrics is
to quantify the performance of different methods. Because
the major function of detecting text regions is to recognize
text, it is a strong requirement for detecting bounding boxes
to preserve the completeness of text information and avoid
interference with other text instances. However, previous
evaluation metrics mentioned in Section 2 do not consid-
er the impact of cutting GT regions and outlier-GT. Mean-
while, it is not easy to distinguish the tightness of detec-

43239614

Ground Truth

Ground Truth

Detection #1

Detection #2

Detection #3

Detection #4

Detection #1

Detection #2

Detection #3

Detection #4

Rec.

province

Rec. tarp

Rec.

IoU

TIoU-R

Ct

NED

dowing

0.612

0.375

0.386

0.625

Rec.

IoU

TIoU-R

Ct

NED

province

0.891

0.866

0.028

0

Rec.

IoU

TIoU-R

Ct

NED

ovinc

0.652

0.424

0.348

0.375

Rec.

IoU

TIoU-R

Ct

NED

ovince

0.585

0.342

0.415

0.25

Rec.

IoU

TIoU-R

Ct

TIoU-P

Ot

NED

rtarp

0.628

0.513

0.184

0.459

0.269

0.25

Rec.

IoU

TIoU-R

Ct

TIoU-P

Ot

NED

tarp

0.776

0.736

0.052

0.629

0.190

0

Rec.

IoU

TIoU-R

Ct

TIoU-P

Ot

NED

than

0.516

0.516

0.0

0.266

0.483

0.75

Rec.

IoU

TIoU-R

Ct

TIoU-P

Ot

NED

batf

0.507

0.491

0.032

0.262

0.484

0.75

(a) Cutting effect.

(b) Outlier effect.

Figure 3. Qualitative visualization of TIoU metric. Blue: Detection. Bold red: Target GT region. Light red: Other GT regions. Rec.:
Recognition results by CRNN [24]. NED: Normalized edit distance. Previous metrics evaluate all detection results and target GTs as 100%
precision and recall, respectively, while in TIoU metric, all matching pairs are penalized by different degrees. Ct is deﬁned in Eq. 10. Ot
is deﬁned in Eq. 13.

tions. Hence, the merits of the detecting methods cannot be
thoroughly embodied. To address these problems, our so-
lution is derived from three basic annotating concepts: (a)
Annotation does not cut the text instance; (b) annotation
contains less background noise, especially outlier text in-
stances; (c) annotations may not be perfect matching the
text instance, they should be as much perfect as possible.

3.1.1 TIoU-Recall

Intuitively, one GT rectangle Gi cut by a detection bounding
box Dj may result in incorrect recognition. Although tra-
ditional IoU metrics can measure the tightness between Gi
and Dj , it can not goal-oriented evaluate the cases shown in
Fig. 1 (a) and (b) (Detections in (a) and (b) have the same
value of IoU (0.66) against the ground truth, while the for-
mer one do not recall a few characters of the GT). To solve
this issue, the cutting behavior can be penalized by the pro-
portion of intersection in GT.

Firstly, we deﬁne the not-recalled area of Gi as Ct:

Ct = A(Gi) − A(Dj ∩ Gi), Ct ∈ [0, A(Gi)],

(10)

where A(∗) means the area of the region. Then, the propor-
tion of intersection in Gi is given by:

f (Ct) = 1 − x, x =

Ct

A(Gi)

.

(11)

Therefore, the ﬁnal TIoU-Recall is deﬁned as follows:

T IoURecall =

A(Gi ∩ Dj) ∗ f (Ct)

A(Gi ∪ Dj)

.

(12)

Equation 12 is a simple but effective solution for manag-
ing the cutting behavior, e.g., the TIoU-Recall in Fig. 1 (a)
and (b) are 0.424 and 0.66, respectively, which implies that
missing characters account for a 35.8% decline in recall.

3.1.2 TIoU-Precision

On the other hand, one detection covers several GTs may
also affect the recognition results because it is intricate for
recognition methods to distinguish which text is target GT,
as shown in Fig. 1 (c). The proposed solution is to pe-
nalize such type of detections to make detection compact
for avoiding outlier-GTs. Nevertheless, if the outlier-GTs
are inside the target GT region, even the perfect detec-
tion bounding box cannot avoid containing these outlier-
s. Therefore, only the outlier-GT region that is inside the
detection bounding box but outside the target GT region
would be penalized. The area (Ot) of the union of all el-
igible outlier-GTs is calculated using equation 13:

Otij =A((G1 ∩ Dj − G1 ∩ Dj ∩ Gi)∪

... ∪ (Gi−1 ∩ Dj − Gi−1 ∩ Dj ∩ Gi)∪
(Gi+1 ∩ Dj − Gi+1 ∩ Dj ∩ Gi) ∪ ... ∪
(Gn ∩ Dj − Gn ∩ Dj ∩ Gi)),
Otij ∈ [0, A(Dj − Dj ∩ Gi)].

(13)

Note that for each Gn(n 6= i) that does not intersect with
Dj , it can be simply ignored, which can improve comput-
ing efﬁciency. Then, the proportion of intersection in Dj is
given by:

f (Ot) = 1 − x, x =

Ot

A(Dj)

.

(14)

Using equation 14, we can deﬁne the TIoU-Precision in the
same way as TIoU-Recall, as shown in equation 15:

T IoUP recision =

A(Dj ∩ Gi) ∗ f (Ot)

A(Dj ∪ Gi)

.

(15)

For intuitively understanding the performance of TIoU
metrics, we show many examples in Fig. 3, which will be
further discussed in Section 4.2.

43249615

3.2. Tightness aware Metric

To calculate the ﬁnal score, the harmonic mean of recall

and precision is usually adopted as the primary metric:

Hmean = 2

Recall · P recision
Recall + P recision

,

where recall and precision are calculated by

Recallori =

P M atchgti

N umgt

,

(16)

(17)

P recisionori =

P M atchdtj

N umdt

.

(18)

IC13 and IC15 both use a binary [0, 100%] results to deter-
mine M atchgti and M atchdtj .

Binary results can not quantify the tightness of the meth-
ods, e.g., two detections D1 and D2 would be regarded
the same event if the IoU of D1 is 51% and that of D2 is
100%. This is unreasonable because a good evaluation pro-
tocol should be able to reﬂect the discrepancy of detecting
tightness.

To this end, we follow and improve IC03 method that
directly uses the matching value to represent the score of
each item, i.e., binary [0, 100%] results are replaced by a
continuous [0-100%] index. However, unlike IC03, TIoU
guarantees that all GT can be matched only once. In ad-
dition, we use a more reasonable TIoU value as the score,
instead of using Eq. 3 and 4. Meanwhile, if the highest IoU
is less than 0.5, the score is set to 0. Therefore, equations 17
and 18 become

RecallSIoU =

P IoUi
N umgt

,

P recisionSIoU =

P IoUj
N umdt

.

(19)

(20)

Equations 19 and 20 are referred to as the Score
Intersection-over-Union (SIoU) metric, which is now
tightness-aware for evaluating detection methods. To fur-
ther consider cutting and outlier intervention, we can use
the score of TIoU-Recall and TIoU-Precision to calculate
the results, as shown in equations 21 and 22:

RecallT IoU =

P recisionT IoU =

P T IoUrecall

N umgt

P T IoUprecision

N umdt

(21)

(22)

3.3. The Solution of One to many and Many to one

Metrics.

Based on our observation, the annotation inconsistency
exists on nearly all benchmark datasets. Currently, there is

no strict objective annotation protocol, and thus, such am-
biguous inconsistency is hard to avoid. Detection methods
cannot be fairly evaluated if the GT annotation is incon-
sistent. Taking the dataset of ICDAR 2015 challenge 4 as
an example: This dataset adopts word-level annotation, but
sometimes different words are stacked together even with-
out obvious interval. Some examples are shown in Fig. 4.
Mandatory labeling the text by words is practically demand-
ing for detection methods.

Figure 4. Examples of annotation inconsistency of GT images in
the ICDAR 2015 challenge 4 dataset. Some words without ap-
parent interval are separately annotated while some are annotated
with single bounding box.

The motivation of the OM and MO metrics is to solve the
inconsistency of annotation granularity because both word-
level and text-line detections may appear simultaneously,
and it is hard to judge which one is better for recognition.

Section 2 has already pointed out the weaknesses of the
existing OM and MO methods. This section presents a very
straightforward solution, i.e., the metric should be used with
both word-level and text-line annotations being provided.
However, to avoid redundant calculation of the same GT,
the evaluation requires thorough consideration. Taking the
IC15 dataset as an example, the evaluating steps can be out-
lined as follows (algorithm procedures can be found on ap-
pendix):

• Creating text-line annotations based on word-level an-
notations. Each text-line annotation contains at least t-
wo word annotations. All the “don’t cared” regions are
ignored in the former. Because IC15 test set contains
only 2077 GT boxes, the entire creation procedure is
very fast.

• The auxiliary text-line annotations are evaluated in ad-
vance with the same metric as that used for evaluating
word-level annotation. If a detection is matched to a
text-line GT, we can calculate the TIoU-precision ac-
cording to Eq. 12. In the subsequent word-level evalu-
ation stage, the matched detection is ignored. Then, we
use word-level annotation and Eq. 6 to decide whether

43259616

the word-level GTs inside this text-line GT are sufﬁ-
ciently recalled.
If a word-level GT is recalled, the
text-line TIoU-Recall is given by:

T IoU ∗

Recall =

A(Gj ∩ Di) ∗ f (Ct)

A(Gj)

.

(23)

In the next word-level evaluating stage, this GT would
be considered as “don’t cared”. If the GT is not suf-
ﬁciently recalled, the TIoU-Recall of this GT will be
calculated in word-level stage.

(a) MTS.

(b) FEN.

(c) R2CNN.

Figure 5. According to Table 1, the performances of these three
methods are comparable under previous metrics but different while
using TIoU metric. This is because TIoU can perceive the com-
pleteness of the target GT, the compactness of the detection, and
the tightness of the matching. We can also ﬁnd that a detection that
achieves 0.9 or higher TIoU value is nearly perfect, which could be
directly served as a new manual annotation. MTS: MaskTextSpot-
ter.

(a) East.

(b) PixelLink.

(c) RRD.

Figure 6. The visualization results are corresponding to Table 2.
The performances of PixelLink and RRD are much better than East
under previous metrics. However, in TIoU metric, the results are
comparable, and East can even outperform RRD, which is mainly
because the detections of East are tighter than other two methods.

4. Experiments

This section quantitatively and qualitatively tested the
proposed TIoU metric on two most popular scene text de-
tection datasets - IC13 [9] and IC15 [8].

4.1. Quantitative evaluation

Quantitative experiments have been conducted to eval-
uate the differences between the previous metrics and the
proposed TIoU metric. To evaluate the results, several state-
of-the-art methods were adopted. All the methods were s-
elected based on three criteria: 1) The methods have been
frequently referred to in the literature; 2) the authors are
willing to provide their detection results for our evaluation;
3) the authors or other researchers have published the source
code or test model, thus making the result easy to reproduce,
such as PixelLink [2]. Some results of IC13 directly used
the model of IC15 provided by the authors (because they
did not provide IC13 model).

In addition, we further compared the metrics by several
popular general object detection frameworks on the IC13
dataset. All the training data for these general detection
frameworks are strictly the same, including 1715 samples
from FORU [27] and 229 samples from the ofﬁcial IC13
training set.

To further demonstrate the effectiveness of the TIoU
metric, we adopted two recognition methods (CRNN [24]
and RARE [25]) to evaluate end-to-end results. The end-
to-end metric assesses the localization in the same way as
detection task, and then evaluate the recall, precision, and
Hmean based on transcription perfect match [8].

The results are shown in Table 1 and Table 2, respective-

ly. From Table 1, several new prospects can be observed:

• The values of TIoU show a similar tendency as end-
to-end evaluation results. For example, Faster R-
CNN outperforms SSD by 5% in the original IoU met-
ric (0.676 vs. 0.626); however, in TIoU metric, the
latter surpasses the former by almost 5%, which is cor-
responding to End-to-end results: For Fe1 and Fe2, the
latter exceeds the former by 7.8% and 6%, respective-
ly. To make it clearer, we further draw a line chart
which can be more intuitive to visualize the correla-
tions, as shown in Figure 7. From the ﬁgure, we can
ﬁnd the results of the Fe1, Fe2, and FT IoU are ex-
tremely similar whereas the results of the FIoU are
clearly different from the other three results. On the
other hand, the correlation coefﬁcients r(FT IoU ; Fe1)
and r(FT IoU ; Fe2) are higher than r(FIoU ; Fe1) and
r(FIoU ; Fe2), with 0.5% and 2.6%, respectively.

• TIoU is tightness-aware. FEN, R2CNN, and Mask-
TextSpotter (MTS) are comparable under previous
metrics; however, in TIoU metrics,
the results are
much different, with 0.751, 0.704, and 0.769, respec-
tively for these three methods. This is mainly because
the TIoU is tightness-aware: Examples are shown in
Fig. 5, MTS is the tightest method, and next is FEN
and R2CNN, which are corresponding to the TIoU
ranks.

43269617

Table 1. Comparison of evaluation methods on ICDAR 2013 for general detection frameworks and previous state-of-the-art methods. det:
DetEval. i: IoU. e1: End-to-end recognition results by using CRNN [24]. e2: End-to-end recognition results by using RARE [25]. t:
TIoU.

Methods

Rdet

Faster R-CNN (VGG16) [22]

SSD (300x300) [14]

YOLO-v2 (320x320) [20]

YOLO-v3 (320x320) [21]

YOLO-v3 (512x512) [21]

Mask R-CNN [5]

R-FCN (resNet-50) [1]

Faster R-CNN-FPN [13]

RetinaNet (resNet-50-FPN) [13]

East [32]

SegLink [23]

PixelLink [2]

TextBox [11]

SWT-MSER [3, 19]

FEN [30]

R2CNN [7]

MaskTextSpotter [17]

WordSup [6]

AF-RPN [31]

0.410

0.476

0.431

0.648

0.694

0.767

0.603

0.674

0.452

0.707

0.6

0.633

0.731

0.371

0.899

0.905

0.886

0.871

0.896

Pdet

0.549

0.88

0.772

0.823

0.867

0.793

0.796

0.882

0.901

0.816

0.739

0.679

0.896

0.258

0.947

0.943

0.95

0.928

0.945

Fdet

Ri

Pi

Fi

0.469

0.618

0.553

0.725

0.771

0.780

0.686

0.764

0.602

0.758

0.662

0.655

0.805

0.305

0.923

0.923

0.917

0.899

0.92

0.615

0.752

0.676

0.484

0.886

0.626

0.481

0.877

0.621

0.68

0.874

0.765

0.721

0.895

0.799

0.718

0.715

0.716

0.656

0.869

0.748

0.686

0.875

0.769

0.46

0.906

0.611

0.731

0.835

0.779

0.572

0.666

0.615

0.621

0.618

0.619

0.741

0.892

0.809

0.17

0.181

0.175

0.885

0.934

0.909

0.875

0.908

0.891

0.873

0.935

0.903

0.702

0.821

0.757

0.854

0.902

0.877

Re1

0.396

0.398

0.372

0.519

0.566

0.544

0.527

0.578

0.409

0.588

0.485

0.539

0.594

0.083

0.719

0.745

0.751

0.611

0.731

Pe1

0.432

Fe1

0.413

0.639

0.491

0.548

0.443

0.611

0.561

0.65

0.605

Re2

0.406

0.391

0.526

0.523

0.585

Pe2

0.442

Fe2

0.423

Rt

Pt

Ft

0.377

0.554

0.448

0.629

0.483

0.377

0.727

0.496

0.571

0.547

0.339

0.682

0.453

0.516

0.566

0.502

0.696

0.583

0.672

0.625

0.549

0.73

0.627

0.494

0.518

0.58

0.525

0.551

0.527

0.545

0.536

0.627

0.573

0.678

0.624

0.744

0.528

0.543

0.597

0.385

0.647

0.59

0.488

0.712

0.579

0.699

0.644

0.551

0.737

0.631

0.7

0.497

0.375

0.77

0.504

0.595

0.591

0.6

0.607

0.603

0.567

0.684

0.620

0.497

0.491

0.481

0.508

0.643

0.618

0.075

0.079

0.716

0.717

0.732

0.738

0.752

0.752

0.648

0.629

0.72

0.725

0.495

0.549

0.614

0.317

0.759

0.762

0.766

0.624

0.756

0.507

0.501

0.387

0.471

0.425

0.489

0.517

0.432

0.442

0.437

0.664

0.638

0.564

0.712

0.629

0.243

0.275

0.122

0.136

0.129

0.757

0.758

0.721

0.783

0.751

0.749

0.756

0.687

0.721

0.704

0.766

0.766

0.733

0.809

0.769

0.662

0.642

0.533

0.626

0.575

0.744

0.75

0.665

0.711

0.687

Table 2. Comparison of metrics on the ICDAR 2015 challenge 4. Word&Text-Line Annotations use our new solution to address OM and
MO issues. i: IoU. s: SIoU. t: TIoU.

Methods

Original Word-level-Only Annotations

Word&Text-Line Annotations

Ri

Pi

Fi

Rs

Ps

Fs

Rt

Pt

Ft

Ri

Pi

Fi

Rt

Pt

Ft

SegLink [23]

0.728

0.802

0.764

0.54

0.594

0.566

0.467

0.581

0.517

0.747

0.836

0.789

0.505

0.598

0.548

East [32]

0.772

0.846

0.808

0.593

0.65

0.62

0.528

0.635

0.576

0.785

0.864

0.823

0.567

0.64

0.601

RRD [12]

0.778

0.868

0.821

0.594

0.663

0.627

0.515

0.652

0.575

0.783

0.879

0.829

0.53

0.653

0.585

PixelLink [2]

0.817

0.829

0.823

0.616

0.626

0.621

0.552

0.618

0.583

0.829

0.851

0.84

0.585

0.627

0.605

TextBox++ [10]

0.808

0.891

0.847

0.619

0.683

0.649

0.537

0.672

0.597

0.812

0.9

0.854

0.549

0.67

0.603

DMPNet [15]

0.765

0.757

0.761

0.564

0.558

0.561

0.479

0.546

WordSup [6]

0.773

0.805

0.789

0.568

0.591

0.579

0.49

0.577

0.51

0.53

0.781

0.779

0.78

0.512

0.554

0.532

0.785

0.831

0.807

0.522

0.588

0.553

R2CNN [7]

0.828

0.887

0.855

0.641

0.687

0.663

0.559

0.676

0.612

0.831

0.901

0.865

0.577

0.676

0.622

AF-RPN [31]

0.832

0.891

0.861

0.645

0.69

0.667

0.577

0.677

0.623

0.844

0.912

0.877

0.607

0.681

0.642

MaskTextSpotter [17]

0.795

0.89

0.84

0.6

0.671

0.633

0.527

0.658

0.585

0.803

0.906

0.851

0.549

0.662

0.6

(a) Line chart.

(b) Correlation coefﬁcients.

Figure 7. (a) X-axis represents the detection methods listed in Ta-
ble 1, and Y-axis represents the values of the F-measures. (Zoom
in for better visualization)

• Text detection is still a challenging task. Compar-
ing the TIoU metric and previous metrics among all
detection methods, most of the results consistently de-
crease by more than 10%, which indicates there are
still insufﬁciencies of text detection methods. In ad-
dition, even the best performance of TIoU Hmean is
under 0.8, which further shows that designing a robust

text detection method is not an easy task.

Experiments on the IC15 dataset also demonstrate the
effectiveness of the proposed metrics, as shown in Table 2:

• TIoU is also tightness-aware on multi-oriented
dataset. For example, PixelLink and RRD outperfor-
m East in previous IoU metric Fi; however, the gap
reduces using SIoU Fs, which indicates that the de-
tections of East are tighter; in TIoU metric Ft, East
can further surpass RRD. This is because TIoU metric
can reﬂect the tightness of different detection meth-
ods. As some detection results shown in Fig. 6, al-
though the rotated detecting rectangles of RRD satis-
fy the requirement of previous IoU metric, they may
cut the GT regions to some extent, which is the main
reason that cause the decrease of TIoU. This example
also implies that tighter quadrilateral detection meth-
ods may reasonably be more beneﬁt from TIoU metric

43279618

than rectangle-based methods.

• Objectively evaluated Text-line detections. Because
there are many ambiguous annotation granularities on
the IC15 dataset (shown in Fig. 4), we have added
the text-line level annotations of this dataset, and us-
ing our new solution (details in Section 3.3) for OM
and MO matching. The results show that the results
of all methods can be increased by different degrees in
both the IoU and the TIoU metrics. This is mainly be-
cause some ﬁne text line detections are fairly evaluated
instead of treating them as false positives. For exam-
ple, before using our new Joint-word&text-line eval-
uation metric, TextBox++ outperformed PixelLink for
a large margin (2.4%). However, the former is worse
than the latter by 0.2% with joint text-line annotations.
The variation can be explained by visualization results
shown in Fig. 8. The quantitative and qualitative re-
sults both demonstrate that our solution can reveal the
potential novelty of segmentation-based methods.

(a) Original GT.

(b) PixelLink.

(c) TextBox++.

Figure 8. Examples of ambiguous detections by different meth-
ods. If using original metrics, detections of PixelLink in this ﬁg-
ure are false positives, which is the main reason that PixelLink
(0.823) is worse than TextBox++ (0.847) (refer to Tab. 2). While
using our joint word&text-line solution, such text-line detections
can be fairly evaluated, and PixelLink (0.605) can even outperform
TextBox++ (0.603) in TIoU metric (refer to Tab. 2).

4.2. Qualitative evaluation

Fig. 3 shows some examples of the qualitatively evalu-
ated results. If previous IoU metrics are used, all detections
and target GTs of Fig. 3 are considered to have 100% pre-
cision and recall because the their IoUs are all higher than
0.5. Clearly, this is unreasonable because some detection-
s signiﬁcantly reduce the recognition performance, such as
Detection #1 in Fig. 3 (a) and Detection #3 in Fig. 3 (b).

Using the TIoU metric can assist in avoid these phe-
nomenons. As shown in Fig. 3, the TIoU recall and preci-
sion are associated to recognition to some extent. For exam-
ple, NED of detection #1 in (a) is 0.625, which means only
37.5% of the text can be correctly recognized because of
the cutting effect. In this case, the TIoU-recall is also equal
to 37.5%. This coincidence doesn’t imply there is a speciﬁc
relationship between the TIoU and recognition performance
because NED can vary depending on the recognition meth-
ods. However, in Fig. 3, it is intuitive that high NED cases
usually have a relative strong penalizes of TIoU value. Note
that, for some high IoU (but not perfect detections) such as
Detection #2 in Fig. 3 (a), the recall only decreases slight-
ly (from 0.891 to 0.866), which still meets our subjective
judgment.

5. Conclusion

In this paper, we presented a goal-oriented TIoU metric
to address many drawbacks of previous metrics. The TIoU
metric is simple but intuitive, which moderately consider
two common detection behaviors that may signiﬁcantly in-
ﬂuence the recognition. It uses the TIoU score as a measure
of recall and precision to perceive the tightness of detection
methods. Quantitative experiments on ICDAR 2013 and IC-
DAR 2015 datasets showed that the proposed metric has a
similar tendency of the end-to-end detection and recogni-
tion results among general object detection frameworks and
previous state-of-the-art text detection methods. Qualitative
experiments further demonstrated the reduction from previ-
ous IoU to TIoU is mainly because TIoU can perceive and
quantify the tightness of detections.

In addition, because the previous one-to-many and
many-to-one metrics have many drawbacks, we proposed
a straightforward solution to solve these issues. The result-
s on the ICDAR 2015 dataset show that all methods can
be improved with different degrees using our new solution.
This is mainly because the proposed method can reasonably
evaluate the text-line detections instead of roughly regard-
ing them as false positives.

In future, we will try to use TIoU metric to guide train-
ing because its characteristics may be beneﬁted to provide
a strong supervision. In addition, it can also be used to help
incremental or semi-supervised learning because TIoU can
judge whether a detection is suitable to serve as a new GT
annotation.

Acknowledgements

This research is supported in part by GD-NSF (no.
2017A030312006), the National Key Research and Devel-
opment Program of China (No. 2016YFB1001405), NS-
FC (Grant No.: 61673182, 61771199), and GDSTP (Grant
No.:2017A010101027), GZSTP(no. 201704020134).

43289619

References

[1] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection
via region-based fully convolutional networks. In Advances
in neural information processing systems, pages 379–387,
2016.

[2] D. Deng, H. Liu, X. Li, and D. Cai. Pixellink: Detecting

scene text via instance segmentation. AAAI 2018, 2018.

[3] B. Epshtein, E. Ofek, and Y. Wexler. Detecting text in natu-
ral scenes with stroke width transform. In Computer Vision
and Pattern Recognition (CVPR), 2010 IEEE Conference on,
pages 2963–2970. IEEE, 2010.

[4] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams,
J. Winn, and A. Zisserman. The pascal visual object classes
challenge: A retrospective. International journal of comput-
er vision, 111(1):98–136, 2015.

[5] K. He, G. Gkioxari, P. Dollr, and R. Girshick. Mask r-cnn.
Proceedings of the IEEE International Conference on Com-
puter Vision, 2017.

[6] H. Hu, C. Zhang, Y. Luo, Y. Wang, J. Han, and E. Ding.
Wordsup: Exploiting word annotations for character based
text detection. Proceedings of the IEEE International Con-
ference on Computer Vision, 2017.

[7] Y. Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu,
and Z. Luo. R2cnn: Rotational region cnn for orientation ro-
bust scene text detection. arXiv preprint arXiv:1706.09579,
2017.

[8] D. Karatzas and e. a. Gomez-Bigorda, Lluis.

Icdar 2015
competition on robust reading.
In Document Analysis and
Recognition (ICDAR), 2015 13th International Conference
on, pages 1156–1160. IEEE, 2015.

[9] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. I. Big-
orda, S. R. Mestre, J. Mas, D. F. Mota, J. A. Almazn, and
L. P. D. L. Heras. Icdar 2013 robust reading competition. In
International Conference on Document Analysis and Recog-
nition, pages 1484–1493, 2013.

[10] M. Liao, B. Shi, and X. Bai. Textboxes++: A single-shot
IEEE Transactions on Image

oriented scene text detector.
Processing, 27(8):3676–3690, 2018.

[11] M. Liao, B. Shi, X. Bai, X. Wang, and W. Liu. Textboxes: A
fast text detector with a single deep neural network. In AAAI,
pages 4161–4167, 2017.

[12] M. Liao, Z. Zhu, B. Shi, G.-s. Xia, and X. Bai. Rotation-
sensitive regression for oriented scene text detection. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 5909–5918, 2018.

[13] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar. Focal
loss for dense object detection. IEEE transactions on pattern
analysis and machine intelligence, 2018.

[14] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-
Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector.
In European conference on computer vision, pages 21–37.
Springer, 2016.

[15] Y. Liu and L. Jin. Deep matching prior network: Toward
tighter multi-oriented text detection. Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition, 2017.

[16] S. M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, and
R. Young. Icdar 2003 robust reading competitions. In null,
page 682. IEEE, 2003.

[17] P. Lyu, M. Liao, C. Yao, W. Wu, and X. Bai. Mask textspot-
ter: An end-to-end trainable neural network for spotting tex-
t with arbitrary shapes. arXiv preprint arXiv:1807.02242,
2018.

[18] V. Y. Mariano, J. Min, J.-H. Park, R. Kasturi, D. Mihalcik,
H. Li, D. Doermann, and T. Drayer. Performance evaluation
of object detection algorithms. In Pattern Recognition, 2002.
Proceedings. 16th International Conference on, volume 3,
pages 965–969. IEEE, 2002.

[19] J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust wide-
baseline stereo from maximally stable extremal regions. Im-
age and vision computing, 22(10):761–767, 2004.

[20] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

arXiv preprint arXiv:1612.08242, 2016.

[21] J. Redmon and A. Farhadi. Yolov3: An incremental improve-

ment. arXiv preprint arXiv:1804.02767, 2018.

[22] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in neural information processing systems, pages
91–99, 2015.

[23] B. Shi, X. Bai, and S. Belongie. Detecting oriented text
in natural images by linking segments. Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2017.

[24] B. Shi, X. Bai, and C. Yao. An end-to-end trainable neural
network for image-based sequence recognition and its appli-
cation to scene text recognition. IEEE transactions on pat-
tern analysis and machine intelligence, 39(11):2298–2304,
2017.

[25] B. Shi, X. Wang, P. Lyu, C. Yao, and X. Bai. Robust scene
text recognition with automatic rectiﬁcation.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4168–4176, 2016.

[26] B. Shi, Yao, M. Liao, Y. M., X. P., L. Cui, L. S. Serge Be-
longie, and B. X. Icdar2017 competition on reading chinese
text in the wild (rctw-17). arXiv preprint arXiv:1708.09585,
2017.

[27] e. a. Shuye, Zhang. “scut-foru dataset” [online].

In http-

s://pan.baidu.com/s/1kVRIpd9, 2016.

[28] C. Wolf and J.-M. Jolion. Object count/area graphs for the
evaluation of object detection and segmentation algorithms.
International Journal of Document Analysis and Recognition
(IJDAR), 8(4):280–296, 2006.

[29] C. Yao, X. Bai, W. Liu, and Y. Ma. Detecting texts of arbi-
trary orientations in natural images. In Computer Vision and
Pattern Recognition, pages 1083–1090, 2012.

[30] S. Zhang, Y. Liu, L. Jin, and C. Luo. Feature enhancement

network: A reﬁned scene text detector. AAAI 2018, 2017.

[31] Z. Zhong, L. Sun, and Q. Huo. An anchor-free region pro-
posal network for faster r-cnn based text detection approach-
es. arXiv preprint arXiv:1804.09003, 2018.

[32] X. Zhou, C. Yao, H. Wen, Y. Wang, S. Zhou, W. He, and
J. Liang. East: An efﬁcient and accurate scene text detec-
tor. Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2017.

43299620

