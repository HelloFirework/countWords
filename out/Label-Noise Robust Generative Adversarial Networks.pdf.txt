Label-Noise Robust Generative Adversarial Networks

Takuhiro Kaneko1 Yoshitaka Ushiku1

Tatsuya Harada1

2

,

1The University of Tokyo

2RIKEN

Real (unobservable)

Real (observable)

Generated

Generated

(a) Clean labeled data

(b) Noisy labeled data
Flipped with probability 0.5

(c) cGAN trained with (b)

(d) rcGAN trained with (b)

[Baseline]

[Proposed]

Figure 1. Examples of label-noise robust conditional image generation. Each column shows samples belonging to the same class. In
(c) and (d), each row contains samples generated with a ﬁxed z and a varied yg. Our goal is, given noisy labeled data (b), to learn a
conditional generative distribution that corresponds with clean labeled data (a). When naive cGAN (c) is trained with (b), it fails to learn
the disentangled representations, disturbed by noisy labeled data. In contrast, proposed rcGAN (d) succeeds in learning the representations
disentangled on the basis of clean labels, which are close to (a), even when we can only access the noisy labeled data (b) during training.

Abstract

1. Introduction

Generative adversarial networks (GANs) are a frame-
work that learns a generative distribution through adver-
sarial training. Recently, their class conditional exten-
sions (e.g., conditional GAN (cGAN) and auxiliary classi-
ﬁer GAN (AC-GAN)) have attracted much attention owing
to their ability to learn the disentangled representations and
to improve the training stability. However, their training re-
quires the availability of large-scale accurate class-labeled
data, which are often laborious or impractical to collect in
a real-world scenario. To remedy this, we propose a novel
family of GANs called label-noise robust GANs (rGANs),
which, by incorporating a noise transition model, can learn
a clean label conditional generative distribution even when
training labels are noisy.
In particular, we propose two
variants: rAC-GAN, which is a bridging model between
AC-GAN and the label-noise robust classiﬁcation model,
and rcGAN, which is an extension of cGAN and solves this
problem with no reliance on any classiﬁer. In addition to
providing the theoretical background, we demonstrate the
effectiveness of our models through extensive experiments
using diverse GAN conﬁgurations, various noise settings,
and multiple evaluation metrics (in which we tested 402
conditions in total).

In computer vision and machine learning, generative
modeling has been actively studied to generate or repro-
duce samples indistinguishable from real data. Recently,
deep generative models have emerged as a powerful frame-
work for addressing this problem. Among them, generative
adversarial networks (GANs) [14], which learn a genera-
tive distribution through adversarial training, have become
a prominent one owing to their ability to learn any data dis-
tribution without explicit density estimation. This mitigates
oversmoothing resulting from data distribution approxima-
tion, and GANs have succeeded in producing high-ﬁdelity
data for various tasks [24, 38, 69, 7, 19, 29, 53, 25, 66, 77,
18, 31, 10, 61, 60, 76, 8, 22].

Along with this success, various extensions of GANs
have been proposed. Among them, class conditional ex-
tensions (e.g., conditional GAN (cGAN) [37, 39] and aux-
iliary classiﬁer GAN (AC-GAN) [41]) have attracted much
attention mainly for two reasons. (1) By incorporating class
labels as supervision, they can learn the representations that
are disentangled between the class labels and the others.
This allows them to selectively generate images conditioned
on the class labels [37, 41, 22, 73, 23, 10]. Recently, this
usefulness has also been demonstrated in class-speciﬁc data
augmentation [12, 75]. (2) The added supervision simpli-

2467

ﬁes the learned target from an overall distribution to the
conditional distribution. This helps stabilize the GAN train-
ing, which is typically unstable, and improves image qual-
ity [41, 39, 69, 7].

In contrast to these powerful properties, a possible limi-
tation is that typical models rely on the availability of large-
scale accurate class-labeled data and their performance de-
pends on their accuracy. Indeed, as shown in Figure 1(c),
when conventional cGAN is applied to noisy labeled data
(where half labels are randomly ﬂipped, as shown in Fig-
ure 1(b)), its performance is signiﬁcantly degraded, inﬂu-
enced by the noisy labels. When datasets are constructed in
real-world scenarios (e.g., crawled from websites or anno-
tated via crowdsourcing), they tend to contain many misla-
beled data (e.g., in Clothing1M [63], the overall annotation
accuracy is only 61.54%). Therefore, this limitation would
restrict application.

Motivated by these backgrounds, we address the fol-
lowing problem: “How can we learn a clean label con-
ditional distribution even when training labels are noisy?”
To solve this problem, we propose a novel family of GANs
called label-noise robust GANs (rGANs) that incorporate
a noise transition model representing a transition proba-
bility between the clean and noisy labels.
In particular,
we propose two variants: rAC-GAN, which is a bridging
model between AC-GAN [41] and the label-noise robust
classiﬁcation model, and rcGAN, which is an extension of
cGAN [37, 39] and solves this problem with no reliance on
any classiﬁer. As examples, we show generated image sam-
ples using rcGAN in Figure 1(d). As shown in this ﬁgure,
our rcGAN is able to generate images conditioned on clean
labels even where conventional cGAN suffers from severe
degradation.

Another important issue regarding learning deep neural
networks (DNNs) using noisy labeled data is the memo-
rization effect. In image classiﬁcation, a recent study [67]
empirically demonstrated that DNNs can ﬁt even noisy (or
random) labels. Another study [5] experimentally showed
that there are qualitative differences between DNNs trained
on clean and noisy labeled data. To the best of our knowl-
edge, no previous studies have sufﬁciently examined such
an effect for conditional deep generative models. Motivated
by these facts, in addition to providing a theoretical back-
ground on rAC-GAN and rcGAN, we conducted extensive
experiments to examine the gap between theory and prac-
tice. In particular, we evaluated our models using diverse
GAN conﬁgurations from standard to state-of-the-art in var-
ious label-noise settings including synthetic and real-world
noise. We also tested our methods in the case when a noise
transition model is known and in the case when it is not.
Furthermore, we introduce an improved technique to stabi-
lize training in a severely noisy setting (e.g., that in which
90% of the labels are corrupted) and show the effectiveness.

Overall, our contributions are summarized as follows:
• We tackle a novel problem called label-noise robust
conditional image generation, in which the goal is to
learn a clean label conditional generative distribution
even when training labels are noisy.

• To solve this problem, we propose a new family of
GANs called rGANs that incorporate a noise transi-
tion model into conditional extensions of GANs.
In
particular, we propose two variants, i.e., rcGAN and
rAC-GAN, for the two representative class conditional
GANs, i.e., cGAN and AC-GAN.

• In addition to providing a theoretical background, we
examine the gap between theory and practice through
extensive experiments (in which we tested 402 condi-
tions in total). Details and more results are available at
https://takuhirok.github.io/rGAN/.

2. Related work

Deep generative models. Generative modeling has been a
fundamental problem and has been actively studied in com-
puter vision and machine learning. Recently, deep genera-
tive models have emerged as a powerful framework. Among
them, three popular approaches are GANs [14], variational
autoencoders (VAEs) [27, 50], and autoregressive models
(ARs) [57]. All these models have pros and cons. One well-
known problem with GANs is training instability; however,
the recent studies have been making a great stride in solv-
ing this problem [11, 43, 51, 74, 3, 4, 35, 15, 24, 62, 38, 36,
69, 7]. In this paper, we focus on GANs because they have
ﬂexibility to the data representation, allowing for incorpo-
rating a noise transition model. However, with regard to
VAEs and ARs, conditional extensions [26, 34, 65, 58, 47]
have been proposed, and incorporating our ideas into them
is a possible direction of future work.

Conditional extensions of GANs. As discussed in Sec-
tion 1, conditional extensions of GANs have been actively
studied to learn the representations that are disentangled be-
tween the conditional information and the others or to sta-
bilize training and boost image quality. Other than class or
attribute labels [37, 41, 22, 73, 23, 10], texts [45, 71, 70, 64],
object locations [44], images [11, 19, 29, 61], or videos [60]
are used as conditional information, and the effectiveness of
conditional extensions of GANs has also been veriﬁed for
them. In this paper, we focus on the situation in which noise
exists in the label domain because obtaining robustness in
such a domain has been a fundamental and important prob-
lem in image classiﬁcation and has been actively studied,
as discussed in the next paragraph. However, also in other
domains (e.g., texts or images), it is highly likely that noise
may exist when data are collected in real-world scenarios
(e.g., crawled from websites or annotated via crowdsourc-
ing). We believe that our ﬁndings would help the research
also in these domains.

2468

Label-noise robust models. Learning with noisy labels has
been keenly studied since addressed in the learning theory
community [1, 40]. Lately, this problem has also been stud-
ied in image classiﬁcation with DNNs. For instance, to ob-
tain label-noise robustness, one approach replaces a typical
cross-entropy loss with a noise-tolerant loss [2, 72]. An-
other approach cleans up labels or selects clean labels out
of noisy labels using neural network predictions or gradi-
ent directions [46, 55, 33, 20, 49, 16]. The other approach
incorporates a noise transition model [54, 21, 42, 13], simi-
larly to ours. These studies show promising results in both
theory and practice and our study is based on their ﬁndings.
The main difference from them is that their goal is to ob-
tain label-noise robustness in image classiﬁcation, but our
goal is to obtain such robustness in conditional image gen-
eration. We remark that our developed rAC-GAN internally
uses a classiﬁer; thus, it can be viewed as a bridging model
between noise robust image classiﬁcation and conditional
image generation. Note that we also developed rcGAN,
which is a classiﬁer-free model, motivated by the recent
studies [41, 39] that indicate that AC-GAN tends to lose di-
versity through a side effect of generating recognizable (i.e.,
classiﬁable) images. Another related topic is pixel-noise ro-
bust image generation [6, 30]. The difference from them is
that they focused on the noise inserted in a pixel domain,
but we focus on the noise in a label domain.

3. Notation and problem statement

We begin by deﬁning notation and the problem state-
ment. Throughout, we use superscript r to denote the real
distribution and g the generative distribution. Let x ∈ X
be the target data (e.g., images) and y ∈ Y the correspond-
ing class label. Here, X is the data space X ⊆ Rd, where
d is the dimension of the data, and Y is the label space
Y = {1, . . . , c}, where c is the number of classes. We as-
sume that y is noisy (and we denote such noisy label by ˜y)
and there exists a corresponding clean label ˆy that we can-
not observe during training. In particular, we assume class-
dependent noise in which each clean label ˆy = i is cor-
rupted to a noisy label ˜y = j with a probability p(˜y = j|ˆy =
i) = Ti,j , independently of x, where we deﬁne a noise tran-
sition matrix as T = (Ti,j) ∈ [0, 1]c×c (Pi Ti,j = 1). Note
that this assumption is commonly used in label-noise robust
image classiﬁcation (e.g., [2, 72, 54, 21, 42, 13]).

Our

task is, when given noisy labeled samples
(xr, ˜yr) ∼ ˜pr(x, ˜y), to construct a label-noise robust con-
ditional generator such that ˆpg(x, ˆy) = ˆpr(x, ˆy), which can
generate x conditioned on clean ˆy rather than conditioned
on noisy ˜y. This task is challenging for typical conditional
generative models, such as AC-GAN [41] (Figure 2(b))
and cGAN [37, 39] (Figure 2(d)), because they attempt to
construct a generator conditioned on the observable labels;
i.e., in this case, they attempt to construct a noisy-label-

dependent generator that generates x conditioned on noisy ˜y
rather than conditioned on clean ˆy. Our main idea for solv-
ing this problem is to incorporate a noise transition model,
i.e., p(˜y|ˆy), into these models (viewed as orange rectangles
in Figures 2(c) and (e)). In particular, we develop two vari-
ants: rAC-GAN and rcGAN. We describe their details in
Sections 4 and 5, respectively.

4. Label-noise robust AC-GAN: rAC-GAN

4.1. Background: AC GAN

AC-GAN [41] is one of representative conditional exten-
sions of GANs [14]. AC-GAN learns a conditional gen-
erator G that transforms noise z and label yg into data
xg = G(z, yg) with two networks. One is a discrimi-
nator D that assigns probability p = D(x) for samples
x ∼ pr(x) and assigns 1 − p for samples x ∼ pg(x).
The other is an auxiliary classiﬁer C(y|x) that represents
a probability distribution over class labels given x. These
networks are optimized by using two losses, namely, an ad-
versarial loss and an auxiliary classiﬁer loss.

Adversarial loss. An adversarial loss is deﬁned as

LGAN = E
+ E

xr∼pr(x)[log D(x
z∼p(z),yg∼p(y)[log(1 − D(G(z, yg)))],

r)]

(1)

where D attempts to ﬁnd the best decision boundary be-
tween real and generated data by maximizing this loss, and
G attempts to generate data indistinguishable by D by min-
imizing this loss.

Auxiliary classiﬁer loss. An auxiliary classiﬁer loss is used
to make the generated data belong to the target class. To
achieve this, ﬁrst C is optimized using a classiﬁcation loss
of real data:

Lr

AC = E(xr ,yr)∼pr(x,y)[− log C(y = yr|x

r)],

(2)

where C learns to classify real data to the corresponding
class by minimizing this loss. Then, G is optimized by us-
ing a classiﬁcation loss of generated data:

Lg

AC = E

z∼p(z),yg∼p(y)[− log C(y = yg|G(z, yg))], (3)

where G attempts to generate data belonging to the corre-
sponding class by minimizing this loss.

Full objective. In practice, shared networks between D and
C are commonly used [41, 15]. In this setting, the full ob-
jective is written as

LD/C = − LGAN + λr
ACLg

LG = LGAN + λg

ACLr
AC,

AC,

(4)

(5)

AC and λg

where λr
AC are the trade-off parameters between
the adversarial loss and the auxiliary classiﬁer loss for the
real and generated data, respectively. D/C and G are opti-
mized by minimizing LD/C and LG, respectively.

2469

G

D/C

D/C

D

D

(b, d) (c, e)

(a) Conditional generator

(b) AC-GAN discriminator

(c) rAC-GAN discriminator

(d) cGAN discriminator

(e) rcGAN discriminator

[Baseline]

[Proposed]

[Baseline]

[Proposed]

Figure 2. Comparison of naive and label-noise robust GANs. We denote the generator, discriminator, and auxiliary classiﬁer by G, D, and
C, respectively. Among all models, conditional generators (a) are similar. In our rAC-GAN (c) and rcGAN (e), we incorporate a noise
transition model (viewed as an orange rectangle) into AC-GAN (b) and cGAN (d), respectively.

4.2. rAC GAN

By the above deﬁnition, when yr is noisy (i.e., ˜yr is
given) and C ﬁts such noisy labels,1 AC-GAN learns the
noisy label conditional generator G(z, ˜yg). In contrast, our
goal is to construct the clean label conditional generator
G(z, ˆyg). To achieve this goal, we incorporate a noise tran-
sition model (i.e., p(˜y|ˆy); viewed as an orange rectangle in
Figure 2(c)) into the auxiliary classiﬁer. In particular, we
reformulate the auxiliary classiﬁer loss as

rAC = E(xr ,˜yr)∼ ˜pr(x,˜y)[− log ˜C(˜y = ˜yr|x
Lr

r)]

= E(xr ,˜yr)∼ ˜pr(x,˜y)
[− log X

ˆyr

p(˜y = ˜yr|ˆy = ˆyr) ˆC(ˆy = ˆyr|x

r)]

= E(xr ,˜yr)∼ ˜pr(x,˜y)[− log X

Tˆyr ,˜yr ˆC(ˆy = ˆyr|x

r)],

(6)

ˆyr

where we denote the noisy label classiﬁer by ˜C and the
clean label classiﬁer by ˆC (and we explain the reason why
we call it clean in Theorem 1). Between the ﬁrst and second
lines, we assume that the noise transition is independent of
x, as mentioned in Section 3. Note that this formulation
(called the forward correction) is often used in label-noise
robust classiﬁcation models [54, 21, 42, 13] and rAC-GAN
can be viewed as a bridging model between GANs and
them. In naive AC-GAN, ˜C is optimized for Lr
AC, whereas
in our rAC-GAN, ˆC is optimized for Lr
rAC. Similarly, G is
optimized using ˆC rather than using ˜C:

Lg

rAC = E

z∼p(z),ˆyg∼p(ˆy)[− log ˆC(ˆy = ˆyg|G(z, ˆyg))].

(7)

Theoretical background.
In the above, we use a cross-
entropy loss, which is a kind of proper composite loss [48].
In this case, Theorem 2 in [42] shows that minimizing the

1Zhang et al. [67] discuss generalization and memorization of DNNs
and empirically demonstrated that DNNs are capable of ﬁtting even noisy
(or random) labels. Although other studies empirically demonstrated that
some techniques (e.g., dropout [5], mixup [68], and high learning rate [55])
are useful for preventing DNNs from memorizing noisy labels, their theo-
retical support still remains as an open issue. In this paper, we conducted
experiments on various GAN conﬁgurations to investigate such effect in
our task. See Section 7.1 for details.

forward corrected loss (i.e., Equation 6) is equal to mini-
mizing the original loss under the clean distribution. More
precisely, the following theorem holds.

Theorem 1. When T is nonsingular,

argmin

ˆC

= argmin

ˆC

E(xr ,˜yr)∼ ˜pr(x,˜y)[− log X

Tˆyr ,˜yr ˆC(ˆy = ˆyr|x

r)]

ˆyr

E(xr ,ˆyr)∼ ˆpr(x,ˆy)[− log ˆC(ˆy = ˆyr|x

r)].

(8)

For a detailed proof, refer to Theorem 2 in [42]. This
supports the idea that, by minimizing Lr
rAC for noisy la-
beled samples, we can obtain ˆC that classiﬁes x as its cor-
responding clean label ˆy. In rAC-GAN, G is optimized for
this clean classiﬁer ˆC; hence, in G’s input space, ˆyg is en-
couraged to represent clean labels.

5. Label-noise robust cGAN: rcGAN

5.1. Background: cGAN

cGAN [37, 39] is another representative conditional ex-
tension of GANs [14].
In cGAN, a conditional genera-
tor G(z, yg) and a conditional discriminator D(x, y) are
jointly trained using a conditional adversarial loss.

Conditional adversarial loss. A conditional adversarial
loss is deﬁned as

LcGAN = E(xr ,yr)∼pr(x,y)[log D(x

r, yr)]

+ E

z∼p(z),yg∼p(y)[log(1 − D(G(z, yg), yg))],

(9)

where D attempts to ﬁnd the best decision boundary be-
tween real and generated data conditioned on y by maxi-
mizing this loss. In contrast, G attempts to generate data
indistinguishable by D with a constraint on yg by minimiz-
ing this loss.
In an optimal condition [14], cGAN learns
G(z, y) such that pg(x, y) = pr(x, y).

5.2. rcGAN

By the above deﬁnition, when yr is noisy (i.e., ˜yr is
given), cGAN learns the noisy label conditional generator
G(z, ˜yg). In contrast, our goal is to construct the clean la-
bel conditional generator G(z, ˆyg). To achieve this goal, we

2470

insert a noise transition model (viewed as an orange rectan-
gle in Figure 2(e)) before ˆyg is given to D. In particular, we
sample ˜yg from ˜yg ∼ p(˜y|ˆyg) and redeﬁne Equation 9 as

6. Advanced techniques for practice

6.1. Noise transition probability estimation

LrcGAN = E(xr ,˜yr)∼ ˜pr(x,˜y)[log D(x
+ E

z∼p(z),ˆyg∼p(ˆy),˜yg∼p(˜y|ˆyg)[log(1 − D(G(z, ˆyg), ˜yg))],
(10)

r, ˜yr)]

where D attempts to ﬁnd the best decision boundary be-
tween real and generated data conditioned on noisy labels ˜y,
by maximizing this loss. In contrast, G attempts to generate
data indistinguishable by D with a constraint on clean la-
bels ˆyg (and we explain the rationale behind calling it clean
in Theorem 2), by minimizing this loss.

Theoretical background. In an optimal condition, the fol-
lowing theorem holds.

Theorem 2. When T is nonsingular (i.e., T has a unique
inverse), G is optimal if and only if ˆpg(x, ˆy) = ˆpr(x, ˆy).

Proof. For G ﬁxed, rcGAN is the same as cGAN where y
is replaced by ˜y. Therefore, by extending Proposition 1 and
Theorem 1 in [14] (GAN optimal solution) to a conditional
setting, the optimal discriminator D for ﬁxed G is

D(x, ˜y) =

˜pr(x, ˜y)

˜pr(x, ˜y) + ˜pg(x, ˜y)

.

(11)

Then G is optimal if and only if

˜pg(x, ˜y) = ˜pr(x, ˜y).

(12)

As mentioned in Section 3, we assume that label corruption
occurs with p(˜y|ˆy), i.e., independently of x. In this case,

In the above, we assume that T is known, but this
assumption may be too strict for real-world applications.
However, fortunately, previous studies [54, 21, 42, 13] have
been eagerly tackling this problem and several methods for
estimating T ′ (where we denote the estimated T by T ′) have
been proposed. Among them, we tested a robust two-stage
training algorithm [42] in the experiments and analyzed the
effects of estimated T ′. We show the results in Section 7.2.

6.2. Improved technique for severely noisy data

Thorough extensive experiments, we ﬁnd that some
GAN conﬁgurations suffer from performance degradation
in a severely noisy setting (e.g., in which 90% of the labels
are corrupted). In this type of environment, each label is
ﬂipped with a high probability. This disturbs G form associ-
ating an image with a label. To strengthen their connection,
we incorporate mutual information regularization [9]:

LMI = E

z∼p(z),ˆyg∼p(ˆy)[− log Q(ˆy = ˆyg|G(z, ˆyg))], (17)

where Q(ˆy|x) is an auxiliary distribution approximating a
true posterior p(ˆy|x). We optimize G and Q by minimizing
this loss with trade-off parameters λg
MI, respec-
tively. This formulation is similar to Equation 7, but the
difference is whether G is optimized for ˆC (optimized us-
ing real images and noisy labels) or for Q (optimized using
generated images and clean labels). We demonstrate the ef-
fectiveness of this technique in Section 7.3.

MI and λq

7. Experiments

˜p(x, ˜y) = ˜p(˜y|x)p(x) = X

p(˜y|ˆy)ˆp(ˆy|x)p(x)

7.1. Comprehensive study

ˆy

= X

p(˜y|ˆy)ˆp(x, ˆy) = X

Tˆy,˜y ˆp(x, ˆy).

(13)

ˆy

ˆy

Substituting Equation 13 into Equation 12 gives

X

Tˆy,˜y ˆpg(x, ˆy) = X

Tˆy,˜y ˆpr(x, ˆy).

(14)

ˆy

ˆy

By considering the matrix form,

T ⊤ ˆP g = T ⊤ ˆP r,

(15)

where ˆP g = [ˆpg(x, ˆy = 1), . . . , ˆpg(x, ˆy = c)]⊤ and ˆP r =
[ˆpr(x, ˆy = 1), . . . , ˆpr(x, ˆy = c)]⊤. When T has an inverse,

T ⊤ ˆP g = T ⊤ ˆP r ⇔ ˆP g = (T ⊤)−1T ⊤ ˆP r = ˆP r.

(16)

As the corresponding elements in ˆP g and ˆP r are equal,
ˆpg(x, ˆy) = ˆpr(x, ˆy).

In Sections 4 and 5, we showed that our approach is the-
oretically grounded. However, generally, in DNNs, there is
still a gap between theory and practice. In particular, the
label-noise effect in DNNs just recently began to be dis-
cussed in image classiﬁcation [67, 5], and it is demonstrated
that such a gap exists. However, in conditional image gener-
ation, such an effect has not been sufﬁciently examined. To
advance this research, we ﬁrst conducted a comprehensive
study, i.e., compared the performance of conventional AC-
GAN and cGAN and proposed rAC-GAN and rcGAN using
diverse GAN conﬁgurations in various label-noise settings
with multiple evaluation metrics.2 Due to the space limi-
tation, we brieﬂy review the experimental setup and only
provide the important results in this paper. See the supple-
mentary materials at https://takuhirok.github.
io/rGAN/ for details and more results.

Dataset. We veriﬁed the effectiveness of our method on
two benchmark datasets: CIFAR-10 and CIFAR-100 [28],

This supports the idea that, in an optimal condition, rc-

GAN learns G(z, ˆy) such that ˆpg(x, ˆy) = ˆpr(x, ˆy).

2Through Sections 7.1–7.3, we tested 392 conditions in total. For each

condition, we trained two models with different initializations.

2471

e
n

i
l

e
s
a
B

d
e
s
o
p
o
r
P

AC-CT-GAN

cSN-GAN

AC-CT-GAN

cSN-GAN

rAC-CT-GAN

rcSN-GAN

(a) CIFAR-10 (symmetric noise with μ = 0.7)

rAC-CT-GAN

rcSN-GAN

(b) CIFAR-10 (asymmetric noise with μ = 0.7)

Figure 3. Generated image samples on CIFAR-10. Each column shows samples belonging to the same class. Each row contains samples
generated with a ﬁxed z and a varied yg. In symmetric noise (a), cSN-GAN is primarily inﬂuenced by noisy labels and fails to learn the
disentangled representations. In asymmetric noise (b), it is expected that fourth and sixth columns will include cat and dog, respectively.
However, in AC-CT-GAN and cSN-GAN, these columns contain the inverse. As evidence, we list the accuracy in the fourth column for
cat/dog classes in Table 1. These scores indicate that the proposed models are robust but the baselines are weak for the ﬂipped classes.

which are commonly used in both image generation and
label-noise robust image classiﬁcation. Both datasets con-
tain 60k 32 × 32 natural images, which are divided into
50k training and 10k test images. CIFAR-10 and CIFAR-
100 have 10 and 100 classes, respectively. We assumed
two label-noise settings that are popularly used in label-
noise robust image classiﬁcation: (1) Symmetric (class-
independent) noise [59]: For all classes, ground truth labels
are replaced with uniform random classes with probability
µ. (2) Asymmetric (class-dependent) noise [42]: Ground
truth labels are ﬂipped with probability µ by mimicking
real mistakes between similar classes. Following [42], for
CIFAR-10, ground truth labels are replaced with truck →
automobile, bird → airplane, deer → horse, and cat ↔ dog,
and for CIFAR-100, ground truth labels are ﬂipped into the
next class circularly within the same superclasses. In both
settings, we selected µ from {0.1, 0.3, 0.5, 0.7, 0.9}.

GAN conﬁgurations. A recent study [32] shows the sensi-
tivity of GANs to hyperparameters. However, when clean
labeled data are not available, it is impractical to tune the
hyperparameters for each label-noise setting. Hence, in-
stead of searching for the best model with hyperparameter
tuning, we tested various GAN conﬁgurations using default
parameters that are typically used in clean label settings
and examined the label-noise effect. We chose four mod-
els to cover standard, widely accepted, and state-of-the-art
models: DCGAN [43], WGAN-GP [15], CT-GAN [62],
and SNGAN [38]. We implemented AC-GAN, rAC-GAN,
cGAN, and rcGAN based on them. For cGAN and rcGAN,
we used the concat discriminator [37] for DCGAN and the
projection discriminator [39] for the others.

Evaluation metrics. As discussed in previous studies [56,
32, 52], evaluation and comparison of GANs can be chal-
lenging partially because of the lack of an explicit likeli-
hood measure. Considering this fact, we used four metrics
for a comprehensive analysis: (1) the Fréchet Inception dis-
tance (FID), (2) Intra FID, (3) the GAN-test, and (4) the
GAN-train. The FID [17] measures the distance between
pr and pg in Inception embeddings. We used it to assess the

cat/dog

AC-CT-GAN rAC-CT-GAN cSN-GAN rcSN-GAN
75.9/13.0

84.8/10.3

35.6/55.9

13.4/83.9

Table 1. Accuracy in the fourth column in Figure 3(b) (ground
truth: cat) for the ﬂipped classes (cat ↔ dog)

quality of an overall generative distribution. Intra FID [39]
calculates the FID for each class. We used it to assess the
quality of a conditional generative distribution.3 The GAN-
test [52] is the accuracy of a classiﬁer trained on real images
and evaluated on generated images. This metric approxi-
mates the precision (image quality) of GANs. The GAN-
train [52] is the accuracy of a classiﬁer trained on generated
images and evaluated on real images in a test. This metric
approximates the recall (diversity) of GANs.

Results. We present the quantitative results for each condi-
tion in Figure 4 and provide a comparative summary be-
tween the proposed models (i.e., rAC-GAN and rcGAN)
and the baselines (i.e., AC-GAN and cGAN) across all con-
ditions in Figure 5. We show the generated image samples
on CIFAR-10 with µ = 0.7 in Figure 3. Regarding the FID
(i.e., evaluating the quality of the overall generative distri-
bution), the baselines and the proposed models are compa-
rable in most cases, but when we use CT-GAN and SN-
GAN (i.e., state-of-the-art models) in symmetric noise, the
proposed models tend to outperform the baselines (32/40
conditions). This indicates that the label ambiguity caused
by symmetric noise could disturb the learning of GANs if
they have the high data-ﬁtting ability. However, this degra-
dation can be mitigated by using the proposed methods.

Regarding the other metrics (i.e., evaluating the quality
of the conditional generative distribution), rAC-GAN and
rcGAN tend to outperform AC-GAN and cGAN, respec-
tively, across all the conditions. The one exception is rAC-
WGAN-GP on CIFAR-10 with symmetric noise, but we
ﬁnd that it can be improved using the technique introduced
in Section 6.2. We demonstrate this in Section 7.3. Among
the four models, CT-GAN and SN-GAN work relatively
well for rAC-GAN and rcGAN, respectively. This tendency

3We used Intra FID only for CIFAR-10 because, in CIFAR-100, the

number of clean labeled data for each class (500) is too few.

2472

DCGAN

WGAN-GP

CT-GAN

SN-GAN

DCGAN

WGAN-GP

CT-GAN

SN-GAN

AC-GAN

rAC-GAN

cGAN

rcGAN

(a) CIFAR-10 (symmetric noise)

(b) CIFAR-10 (asymmetric noise)

←

D
F

I

←

→

→

I

D
F
 
a
r
t
n
I

t
s
e
t
-

N
A
G

i

n
a
r
t
-

N
A
G

←

D
F

I

→

→

t
s
e
t
-

N
A
G

i

n
a
r
t
-

N
A
G

Noise rate µ

Noise rate µ
Noise rate µ
(c) CIFAR-100 (symmetric noise)

Noise rate µ

Noise rate µ

Noise rate µ
(d) CIFAR-100 (asymmetric noise)

Noise rate µ

Noise rate µ

Figure 4. Quantitative results on CIFAR-10 and CIFAR-100. ↓ indicates the smaller the value, the better the performance. ↑ indicates the
larger the value, the better the performance. Note that the scale is adjusted on each graph for ease of viewing.

Smaller is better

Larger is better

Larger is better

d
e
s
o
p
o
r
P

d
e
s
o
p
o
r
P

d
e
s
o
p
o
r
P

oritizes learning simple (i.e., clean) labels. In contrast, in
asymmetric noise, the label corruption pattern is restrictive;
as a result, AC-GAN easily ﬁts noisy labels.

rAC-GAN vs. AC-GAN
rcGAN vs. cGAN

rAC-GAN vs. AC-GAN
rcGAN vs. cGAN

rAC-GAN vs. AC-GAN
rcGAN vs. cGAN

7.2. Effects of estimated T ′

Baseline
(a) FID

Baseline

(b) GAN-test

Baseline

(c) GAN-train

Figure 5. Comparison between the proposed models and the base-
lines across all the conditions in Figure 4.

AC-GAN

rAC-GAN

cGAN

rcGAN

Symmetric
Asymmetric

-0.846 ± 0.084
-0.976 ± 0.008

-0.786 ± 0.163
-0.476 ± 0.119

-0.989 ± 0.013
-0.985 ± 0.029

-0.818 ± 0.142
-0.427 ± 0.274

Table 2. Pearson correlation coefﬁcient between the noise rate and
GAN-train. The scores are averaged over all GAN conﬁgurations.

is also observed in clean label settings (i.e., µ = 0). This
indicates that the performance of rAC-GAN and rcGAN is
closely related to the advance in the baseline GANs.

To analyze the dependency on the noise rate, we calcu-
lated the Pearson correlation coefﬁcient between GAN-train
and the noise rate. We list these in Table 2. These results
indicate that cGAN has the highest dependency on the noise
rate, while AC-GAN shows robustness for symmetric noise
but weakness for asymmetric noise. This would be related
to the memorization effect in a DNN classiﬁer. cGAN is
a classiﬁer-free model; therefore, it learns the distribution
conditioned on the labels regardless of whether they are
noisy or clean. In contrast, AC-GAN internally uses a clas-
siﬁer that prioritizes learning simple (i.e., clean) labels [5].
In symmetric noise, the corruption variety is large, making
it difﬁcult to memorize labels. As a result, AC-GAN pri-

In Section 7.1, we report the results using known T . As
a more practical setting, we also evaluate our method with
T ′ estimated by a robust two-stage training algorithm [42].
We used CT-GAN for rAC-GAN and SN-GAN for rcGAN,
which worked relatively well in both noisy and clean set-
tings in Section 7.1. We list the scores in Table 3.
In
CIFAR-10, even using T ′, rAC-CT-GAN and rcSN-GAN
tend to outperform conventional AC-CT-GAN and cSN-
GAN, respectively, and show robustness to label noise. In
CIFAR-100, when the noise rate is low, rAC-CT-GAN and
rcSN-GAN work moderately well; however, in highly noisy
settings, their performance is degraded. Note that such a
tendency has also been observed in noisy label image clas-
siﬁcation with T ′ [42], in which the authors argue that the
high-rate mixture and limited number of images per class
(500) make it difﬁcult to estimate the correct T . Further
improvement remains as an open issue.

7.3. Evaluation of improved technique

As shown in Figure 4, rAC-GAN and rcGAN show ro-
bustness for label noise in almost all cases, but we ﬁnd that
they are still weak to severely noisy settings (i.e., symmet-
ric noise with µ = 0.9) even though using known T . To

2473

Model

rAC-CT-GAN

with T ′

rcSN-GAN

with T ′

Metric

FID ↓

Intra FID ↓
GAN-test ↑
GAN-train ↑

FID ↓

Intra FID ↓
GAN-test ↑
GAN-train ↑

CIFAR-10 (symmetric noise)

0.1
10.9
28.7
95.3
78.7
10.7
25.5
85.3
80.7

0.3
11.4
31.0
93.2
75.9
11.9
29.4
79.0
78.1

0.5
11.3
30.1
92.0
76.9
12.4
29.4
84.8
77.4

0.7
11.5
31.7
87.7
73.7
12.1
29.7
82.8
75.6

0.9
13.0
38.9
70.4
63.4
15.0
87.4
15.9
15.0

CIFAR-10 (asymmetric noise)
0.1
0.9
11.0
10.8
36.8
28.5
76.6
94.9
67.3
79.8
11.3
10.8
33.9
25.7
71.2
86.6
65.7
80.5

0.7
10.4
35.0
78.5
69.1
10.9
32.6
74.9
69.3

0.5
10.2
31.2
85.2
74.0
11.0
28.7
84.0
75.7

0.3
10.2
27.4
92.9
79.5
10.8
26.0
87.2
79.0

CIFAR-100 (symmetric noise)
0.1
0.9
18.5
19.7

0.3
19.3

0.5
17.7

0.7
17.3

CIFAR-100 (asymmetric noise)
0.1
0.9
19.0
19.4

0.3
19.3

0.5
19.7

0.7
18.8

–

76.6
21.2
14.3

–

53.4
40.1

–

67.1
21.4
16.6

–

36.6
32.8

–

68.1
23.3
17.5

–

37.7
31.3

–
1.0
1.0
20.0

–
1.0
1.0

–
2.5
2.3
19.8

–
1.7
1.8

–

74.1
19.1
13.8

–

65.0
41.7

–

68.9
19.9
14.1

–

63.0
39.3

–

28.7
10.7
14.7

–

32.4
20.1

–
7.2
5.5
14.7

–
7.8
6.1

–
2.2
3.9
13.9

–
3.8
3.9

Table 3. Quantitative results using the estimated T ′. The second row indicates a noise rate. Bold and italic fonts indicate that the score is
better or worse by more than 3 points over or under the baseline models (i.e., AC-CT-GAN or cSN-GAN), respectively.

Model

Metric

FID ↓

Intra FID ↓
Improved
rAC-GAN GAN-test ↑
GAN-train ↑

Improved

rcGAN

FID ↓

Intra FID ↓
GAN-test ↑
GAN-train ↑

CIFAR-10 (symmetric noise)
A
D

B

C

27.9
55.7
65.1
59.9
30.4
76.9
27.3
31.9

14.7
34.6
77.7
70.8
16.9
39.6
65.7
60.7

12.4
33.4
78.2
69.1
14.2
52.9
38.9
36.7

13.5
36.9
63.5
59.7
14.9
48.2
48.8
47.3

CIFAR-100 (symmetric noise)

A

B

C

D

33.1

20.4

17.2

18.4

–

26.2
17.1
50.2

–
4.5
6.0

–

22.5
16.3
25.8

–

12.0
10.3

–

21.5
14.8
18.0

–
9.5
7.5

–

15.4
11.7
18.7

–
6.1
4.4

Table 4. Quantitative results using the improved technique. In the
second row, A, B, C, and D indicate DCGAN, WGAN-GP, CT-
GAN, and SN-GAN, respectively. We evaluated in severely noisy
settings (i.e., symmetric noise with µ = 0.9). Bold and italic fonts
indicate that the score is better or worse by more than 3 points over
or under naive models (i.e., rAC-GAN or rcGAN), respectively.

Metric

FID ↓

GAN-train ↑

Clean

Noisy

AC
6.8
56.6

c

12.0
53.9

AC
4.4
49.5

rAC
4.6
51.7

c
9.4
48.6

rc
9.4
49.8

AC
4.8
52.8

Mixed
c

rAC
4.7
57.0

10.5
51.7

rc
9.7
55.0

Table 5. Quantitative results on Clothing1M. AC, rAC, c, and rc
denote AC-CT-GAN, rAC-CT-GAN, cSN-GAN, and rcSN-GAN,
respectively. Bold font indicates better scores in each block.

improve the performance, we developed an improved tech-
nique (Section 6.2). In this section, we validate its effect.
We list the scores in Table 4. We ﬁnd that the improved de-
gree depends on the GAN conﬁgurations, but, on the whole,
the performance is improved by the proposed technique. In
particular, we ﬁnd that the improved technique is most ef-
fective for rAC-WGAN-GP, in which all the scores doubled
compared to those of naive rAC-WGAN-GP.

7.4. Evaluation on real world noise

Finally, we tested on Clothing1M [63] to analyze the ef-
fectiveness on real-world noise.4 Clothing1M contains 1M
clothing images in 14 classes. The data are collected from
several online shopping websites and include many misla-
beled samples. This dataset also contains 50k, 14k, and
10k of clean data for training, validation, and testing, re-
spectively. Following the previous studies [63, 42], we ap-
proximated T using the partial (25k) training data that have
both clean and noisy labels. We tested on three settings:
(1) 50k clean data, (2) 1M noisy data, and (3) mixed data
that consists of clean data (bootstrapped to 500k) and 1M
noisy data, which are used in [63] to boost the performance
of image classiﬁcation. We used AC-CT-GAN/rAC-CT-
GAN and cSN-GAN/rcSN-GAN. We resized images from
256 × 256 to 64 × 64 to shorten the training time.

4We tested 10 conditions in total. For each condition, we trained three

models with different initializations.

Results. We list the scores in Table 5.5 The comparison
of FID values indicates that the scores depend on the num-
ber of data (noisy, mixed > clean) rather than the differ-
ence between the baseline and proposed models. This sug-
gests that, in this type noise setting, the scale of the dataset
should be made large, even though labels are noisy, to cap-
ture an overall distribution. In contrast, the comparison of
the GAN-train between the clean and noisy data settings in-
dicates the importance of label accuracy. In the noisy data
setting, the scores improve using rAC-GAN or rcGAN but
they are still worse than those using AC-GAN and cGAN in
the clean data setting. The balanced models are rAC-GAN
and rcGAN in the mixed data setting. They are comparable
to the models in the noisy data setting in terms of the FID
and outperform the models in the clean data setting in terms
of the GAN-train. Recently, data augmentation [12, 75]
has been studied intensively as an application of conditional
generative models. We expect the above ﬁndings to provide
an important direction in this space.

8. Conclusion

Recently, conditional extensions of GANs have shown
promise in image generation; however, the limitation here
is that they need large-scale accurate class-labeled data to
be available. To remedy this, we developed a new family
of GANs called rGANs that incorporate a noise transition
model into conditional extensions of GANs. In particular,
we introduced two variants: rAC-GAN, which is a bridg-
ing model between GANs and the noise-robust classiﬁca-
tion models, and rcGAN, which is an extension of cGAN
and solves this problem with no reliance on any classiﬁer. In
addition to providing a theoretical background, we demon-
strate the effectiveness and limitations of the proposed mod-
els through extensive experiments in various settings. In the
future, we hope that our ﬁndings facilitate the construction
of a conditional generative model in real-world scenarios in
which only noisy labeled data are available.

Acknowledgement. We thank Hiroharu Kato, Yusuke Mukuta, and
Mikihiro Tanaka for helpful discussions. This work was supported by
JSPS KAKENHI Grant Number JP17H06100, partially supported by
JST CREST Grant Number JPMJCR1403, Japan, and partially supported
by the Ministry of Education, Culture, Sports, Science and Technology
(MEXT) as “Seminal Issue on Post-K Computer.”

5We did not use Intra FID because the number of clean labeled data
for each class is few. We did not use the GAN-test because this dataset is
challenging, and a learned classiﬁer tends to be deceived by noisy data.

2474

References

[1] D. Angluin and P. Laird. Learning from noisy examples.

Machine Learning, 2(4):343–370, 1988. 3

[2] P. S. Aritra Ghosh, Himanshu Kumar. Robust loss functions
under label noise for deep neural networks. In AAAI, 2017.
3

[3] M. Arjovsky and L. Bottou. Towards principled methods for

training generative adversarial networks. In ICLR, 2017. 2

[4] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gener-

ative adversarial networks. In ICML, 2017. 2

[5] D. Arpit, S. Jastrz˛ebski, N. Ballas, D. Krueger, E. Bengio,
M. S. Kanwal, T. Maharaj, A. Fischer, A. Courville, Y. Ben-
gio, and S. Lacoste-Julien. A closer look at memorization in
deep networks. In ICML, 2017. 2, 4, 5, 7

[6] A. Bora, E. Price, and A. G. Dimakis. AmbientGAN: Gen-
erative models from lossy measurements. In ICLR, 2018. 3

[7] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN
arXiv

training for high ﬁdelity natural image synthesis.
preprint arXiv:1809.11096, 2018. 1, 2

[8] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Neural photo
In ICLR,

editing with introspective adversarial networks.
2017. 1

[9] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel. InfoGAN: Interpretable representation learn-
ing by information maximizing generative adversarial nets.
In NIPS, 2016. 5

[10] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo.
StarGAN: Uniﬁed generative adversarial networks for multi-
domain image-to-image translation. In CVPR, 2018. 1, 2

[11] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep gen-
erative image models using a Laplacian pyramid of adversar-
ial networks. In NIPS, 2015. 2

[12] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, and
H. Greenspan. Synthetic data augmentation using GAN for
improved liver lesion classiﬁcation. In ISBI, 2018. 1, 8

[13] J. Goldberger and E. Ben-Reuven. Training deep neural-
networks using a noise adaptation layer. In ICLR, 2017. 3,
4, 5

[14] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, 2014. 1, 2, 3, 4, 5

[19] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 1, 2

[20] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. Men-
torNet: Learning data-driven curriculum for very deep neural
networks on corrupted labels. In ICML, 2018. 3

[21] I. Jindal, M. Nokleby, and X. Chen. Learning deep networks
In ICDM,

from noisy labels with dropout regularization.
2016. 3, 4, 5

[22] T. Kaneko, K. Hiramatsu, and K. Kashino. Generative at-
tribute controller with conditional ﬁltered generative adver-
sarial networks. In CVPR, 2017. 1, 2

[23] T. Kaneko, K. Hiramatsu, and K. Kashino. Generative ad-
versarial image synthesis with decision tree latent controller.
In CVPR, 2018. 1, 2

[24] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of GANs for improved quality, stability, and varia-
tion. In ICLR, 2018. 1, 2

[25] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to
discover cross-domain relations with generative adversarial
networks. In ICML, 2017. 1

[26] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling.
In

Semi-supervised learning with deep generative models.
NIPS, 2014. 2

[27] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. In ICLR, 2014. 2

[28] A. Krizhevsky. Learning multiple layers of features from

tiny images. Technical report, 2009. 5

[29] C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunning-
ham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and
W. Shi. Photo-realistic single image super-resolution using a
generative adversarial network. In CVPR, 2017. 1, 2

[30] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Kar-
ras, M. Aittala, and T. Aila. Noise2Noise: Learning image
restoration without clean data. In ICML, 2018. 3

[31] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-

image translation networks. In NIPS, 2017. 1

[32] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bous-
quet. Are GANs created equal? A large-scale study. arXiv
preprint arXiv:1711.10337, 2017. 6

[33] E. Malach and S. Shalev-Shwartz. Decoupling "when to up-

date" from "how to update". In NIPS, 2017. 3

[15] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
A. Courville. Improved training of Wasserstein GANs. In
NIPS, 2017. 2, 3, 6

[34] E. Mansimov, E. Parisotto, J. L. Ba, and R. Salakhutdinov.
In ICLR,

Generating images from captions with attention.
2016. 2

[16] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang,
and M. Sugiyama. Co-teaching: Robust training of deep
neural networks with extremely noisy labels. arXiv preprint
arXiv:1804.06872, 2018. 3

[17] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler,
G. Klambauer, and S. Hochreiter. GANs trained by a two
time-scale update rule converge to a Nash equilibrium.
In
NIPS, 2017. 6

[35] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smol-
ley. Least squares generative adversarial networks. In ICCV,
2017. 2

[36] L. Mescheder, A. Geiger, and S. Nowozin. Which training
methods for GANs do actually converge? In ICML, 2018. 2

[37] M. Mirza and S. Osindero. Conditional generative adversar-
ial nets. arXiv preprint arXiv:1411.1784, 2014. 1, 2, 3, 4,
6

[18] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Globally and
locally consistent image completion. ACM Trans. on Graph.,
36(4):107:1–107:14, 2017. 1

[38] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral
normalization for generative adversarial networks. In ICLR,
2018. 1, 2, 6

2475

[39] T. Miyato and M. Koyama. cGANs with projection discrim-

inator. In ICLR, 2018. 1, 2, 3, 4, 6

[40] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari.

Learning with noisy labels. In NIPS, 2013. 3

[41] A. Odena, C. Olah, and J. Shlens. Conditional image syn-
thesis with auxiliary classiﬁer GANs. In ICML, 2017. 1, 2,
3

[42] G. Patrini, A. Rozza, A. K. Menon, R. Nock, and L. Qu.
Making deep neural networks robust to label noise: A loss
correction approach. In CVPR, 2017. 3, 4, 5, 6, 7, 8

[43] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. In ICLR, 2016. 2, 6

[44] S. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and
H. Lee. Learning what and where to draw. In NIPS, 2016. 2
[45] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and
H. Lee. Generative adversarial text to image synthesis. In
ICML, 2016. 2

[46] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and
A. Rabinovich. Training deep neural networks on noisy la-
bels with bootstrapping. In ICLR, 2015. 3

[47] S. Reed, A. van den Oord, N. Kalchbrenner, V. Bapst,
M. Botvinick, and N. de Freitas. Generating interpretable
images with controllable structure. In ICLR Workshop, 2017.
2

[48] M. D. Reid and R. C. Williamson. Composite binary losses.

JMLR, 11:2387–2422, 2010. 4

[49] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to
reweight examples for robust deep learning. In ICML, 2018.
3

[50] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic
backpropagation and approximate inference in deep genera-
tive models. In ICML, 2014. 2

[51] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Chen. Improved techniques for training GANs.
In NIPS, 2016. 2

[52] K. Shmelkov, C. Schmid, and K. Alahari. How good is my

GAN? In ECCV, 2018. 6

[53] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,
and R. Webb. Learning from simulated and unsupervised
images through adversarial training. In CVPR, 2017. 1

[54] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fer-
gus. Training convolutional networks with noisy labels. In
ICLR Workshop, 2015. 3, 4, 5

[55] D. Tanaka, D. Ikami, T. Yamasaki, and K. Aizawa. Joint
In

optimization framework for learning with noisy labels.
CVPR, 2018. 3, 4

[56] L. Theis, A. van den Oord, and M. Bethge. A note on the

evaluation of generative models. In ICLR, 2016. 6

[57] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu.

Pixel recurrent neural networks. In ICML, 2016. 2

[58] A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt,
A. Graves, and K. Kavukcuoglu. Conditional image genera-
tion with pixelCNN decoders. In NIPS, 2016. 2

[59] B. van Rooyen, A. Menon, and R. C. Williamson. Learn-
ing with symmetric label noise: The importance of being
unhinged. In NIPS, 2015. 6

[60] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, G. Liu, A. Tao, J. Kautz,
and B. Catanzaro. Video-to-video synthesis. arXiv preprint
arXiv:1808.06601, 2018. 1, 2

[61] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and semantic
manipulation with conditional GANs. In CVPR, 2018. 1, 2

[62] X. Wei, B. Gong, Z. Liu, W. Lu, and L. Wang. Improving
the improved training of Wasserstein GANs: A consistency
term and its dual effect. In ICLR, 2018. 2, 6

[63] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning
from massive noisy labeled data for image classiﬁcation. In
CVPR, 2015. 2, 8

[64] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. G. X. Huang, and
X. He. AttnGAN: Fine-grained text to image generation with
attentional generative adversarial networks. In CVPR, 2018.
2

[65] X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image: Con-
ditional image generation from visual attributes. In ECCV,
2016. 2

[66] Z. Yi, H. Zhang, P. Tan, and M. Gong. DualGAN: Unsuper-
vised dual learning for image-to-image translation. In ICCV,
2017. 1

[67] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals.
Understanding deep learning requires rethinking generaliza-
tion. In ICLR, 2017. 2, 4, 5

[68] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz.
mixup: Beyond empirical risk minimization. In ICLR, 2018.
4

[69] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-
attention generative adversarial networks. arXiv preprint
arXiv:1805.08318, 2018. 1, 2

[70] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang,
and D. Metaxas. StackGAN++: Realistic image synthesis
with stacked generative adversarial networks. arXiv preprint
arXiv:1710.10916, 2017. 2

[71] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and
D. N. Metaxas. StackGAN: Text to photo-realistic image
synthesis with stacked generative adversarial networks.
In
ICCV, 2017. 2

[72] Z. Zhang and M. R. Sabuncu. Generalized cross entropy loss
for training deep neural networks with noisy labels. arXiv
preprint arXiv:1805.07836, 2018. 3

[73] Z. Zhang, Y. Song, and H. Qi. Age progression/regression
by conditional adversarial autoencoder. In CVPR, 2017. 1, 2
[74] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative

adversarial network. In ICLR, 2017. 2

[75] Z. Zhong, L. Zheng, Z. Zheng, S. Li, and Y. Yang. Camera
style adaptation for person re-identiﬁcation. In CVPR, 2018.
1, 8

[76] J.-Y. Zhu, P. Krähenbühl, E. Shechtman, and A. A. Efros.
Generative visual manipulation on the natural image mani-
fold. In ECCV, 2016. 1

[77] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In ICCV, 2017. 1

2476

