Taking a Deeper Look at the Inverse Compositional Algorithm

Zhaoyang Lv1

2

,

Frank Dellaert1

James M. Rehg1 Andreas Geiger2

1Georgia Institute of Technology, Atlanta, United States

2Autonomous Vision Group, MPI-IS and University of T¨ubingen, Germany

{zhaoyang.lv, rehg}@gatech.edu

frank.dellaert@cc.gatech.edu

andreas.geiger@tue.mpg.de

Abstract

denote the warped template and image, respectively1, the
Lucas-Kanade objective can be stated as follows

In this paper, we provide a modern synthesis of the clas-
sic inverse compositional algorithm for dense image align-
ment. We ﬁrst discuss the assumptions made by this well-
established technique, and subsequently propose to relax
these assumptions by incorporating data-driven priors into
this model. More speciﬁcally, we unroll a robust version of
the inverse compositional algorithm and replace multiple
components of this algorithm using more expressive models
whose parameters we train in an end-to-end fashion from
data. Our experiments on several challenging 3D rigid mo-
tion estimation tasks demonstrate the advantages of com-
bining optimization with learning-based techniques, out-
performing the classic inverse compositional algorithm as
well as data-driven image-to-pose regression approaches.

1. Introduction

Since the seminal work by Lucas and Kanade [32], dense
image alignment has become an ubiquitous tool in computer
vision with many applications including stereo reconstruc-
tion [41], tracking [5, 42, 50], image registration [8, 29, 43],
super-resolution [22] and SLAM [13, 16, 17].
In this pa-
per, we provide a learning-based perspective on the Inverse
Compositional algorithm, an efﬁcient variant of the origi-
nal Lucas-Kanade image registration technique. In partic-
ular, we lift some of the restrictive assumptions by param-
eterizing several components of the algorithm using neural
networks and training the entire optimization process end-
to-end. In order to put contributions into context, we will
now brieﬂy review the Lucas-Kanade algorithm, the Inverse
Compositional algorithm, as well as the robust M-Estimator
which form the basis for our model. More details can be
found in the comprehensive reviews of Baker et al. [2, 3].

(2)

(3)

2

2

min

ξ

kI(ξ) − T(0)k2
2

(1)

where I(ξ) denotes image I transformed using warp param-
eters ξ and T(0) = T denotes the original template.

Minimizing (1) is a non-linear optimization task as the
image I depends non-linearly on the warp parameters ξ.
The Lucas-Kanade algorithm therefore iteratively solves for
the warp parameters ξk+1 = ξk ◦ ∆ξ. At every iteration k,
the warp increment ∆ξ is obtained by linearizing

kI(ξk + ∆ξ) − T(0)k2

2

min
∆ξ

using ﬁrst-order Taylor expansion

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

min
∆ξ

I(ξk) +

∂I(ξk)

∂ξ

∆ξ − T(0)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Note that the “steepest descent image” ∂I(ξk)/∂ξ needs to
be recomputed at every iteration as it depends on ξk.

Inverse Compositional Algorithm: The inverse compo-
sitional (IC) algorithm [3] avoids this by applying the warp
increments ∆ξ to the template instead of the image

kI(ξk) − T(∆ξ)k2

2

min
∆ξ

(4)

using the warp parameter update ξk+1 = ξk ◦ (∆ξ)−1. In
the corresponding linearized equation

min
∆ξ

I(ξk) − T(0) −

∂T(0)

∂ξ

2

2

(5)

∆ξ(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

∂T(0)/∂ξ does not depend on ξk and can thus be pre-
computed, resulting in a more efﬁcient algorithm.

Lucas-Kanade Algorithm: The Lucas-Kanade algorithm
minimizes the photometric error between a template and an
image. Letting T : Ξ → RW ×H and I : Ξ → RW ×H

1The warping function Wξ : R2 → R2 might represent translation,
afﬁne 2D motion or (if depth is available) rigid or non-rigid 3D motion. To
avoid clutter in the notation, we do not make Wξ explicit in our equations.

14581

Robust M-Estimation: To handle outliers or ambiguities
(e.g., multiple motions), robust estimation [19, 54] can be
used. The robust version of the IC algorithm [2] has the
following objective function

min
∆ξ

rk(∆ξ)T W rk(∆ξ)

(6)

where rk(∆ξ) = I(ξk) − T(∆ξ) is the residual between
image I and template T at the k’th iteration, and W is a
diagonal weight matrix that depends on the residual2 and is
chosen based on the desired robust loss function ρ [54].

Optimization: The minimizer of (6) after linearization is
obtained as the Gauss-Newton update step [6]

(JT WJ)∆ξ = JT W rk(0)

(7)

where J = ∂T(0)/∂ξ is the Jacobian of the template T(0)
with respect to the warp parameters ξ. As the approximate
Hessian JT WJ easily becomes ill-conditioned, a damping
term is added in practice. This results in the popular Leven-
berg–Marquardt (trust-region) update equation [35]:

∆ξ = (JT WJ + λ diag(JT WJ))

−1

JT W rk(0)

(8)

For different values of λ, the parameter update ∆ξ varies
between the Gauss-Newton direction and gradient descent.
In practice, λ is chosen based on simple heuristics.

Limitations: Despite its widespread utility, the IC method
suffers from a number of important limitations. First, it as-
sumes that the linearized residual leads to an update which
iteratively reaches a good local optimum. However, this as-
sumption is invalid in the presence of high-frequency textu-
ral information or noise in I or T. Second, choosing a good
robust loss function ρ is difﬁcult as the true data/residual
distribution is often unknown. Moreover, Equation (6) does
not capture correlations or higher-order statistics in the in-
puts I and T as the residuals operate directly on the pixel
values and the weight matrix W is diagonal. Finally, damp-
ing heuristics do not fully exploit the information available
during optimization and thus lead to suboptimal solutions.

Contributions:
In this paper, we propose to combine the
best of both (optimization and learning-based) worlds by
unrolling the robust IC algorithm into a more general pa-
rameterized feed-forward model which is trained end-to-
end from data. In contrast to generic neural network esti-
mators, this allows our algorithm to incorporate knowledge
about the structure of the problem (e.g., family of warp-
ing functions, 3D geometry) as well as the advantages of
a robust iterative estimation framework. At the same time,
our approach relaxes the restrictive assumptions made in the
original IC formulation [3] by incorporating trainable mod-
ules and learning the entire model end-to-end.

More speciﬁcally, we make the following contributions:

(A) We propose a Two-View Feature Encoder which re-
places I, T with feature representations Iθ, Tθ that
jointly encode information about the input image I and
the template T. This allows our model to exploit spa-
tial as well as temporal correlations in the data.

(B) We propose a Convolutional M-Estimator that re-
places W in (6) with a learned weight matrix Wθ
which encodes information about I, T and rk in a way
such that the unrolled optimization algorithm ignores
irrelevant or ambiguous information as well as outliers.

(C) We propose a Trust Region Network which replaces
the damping matrix λ diag(JT WJ) in (8) with a
learned damping matrix diag(λθ) whose diagonal en-
tries λθ are estimated from “residual volumes” which
comprise residuals of a Levenberg-Marquardt update
when applying a range of hypothetical λ values.

We demonstrate the advantages of combining the classical
IC method with deep learning on the task of 3D rigid mo-
tion estimation using several challenging RGB-D datasets.
We also provide an extensive ablation study about the rel-
evance of each model component that we propose. Results
on traditional afﬁne 2D image alignment tasks are provided
in the supplementary material. Our implementation is pub-
licly accessible.3

2. Related Work

We are not the ﬁrst to inject deep learning into an opti-
mization pipeline. In this section, we ﬁrst review classical
methods, followed by direct pose regression techniques and
related work on learning-based optimization.

Classical Methods: Direct methods [18, 21] that align im-
ages using the sum-of-square error objective (1) are prone to
outliers and varying illuminations. Classical approaches ad-
dress this problem by exploiting more robust objective func-
tions [32, 37], heuristically chosen patch- [47] or gradient-
based [29] features, and photometric calibration as a pre-
processing step [16]. The most common approach is to use
robust estimation (6) as in [7]. However, the selection of a
good robust function ρ is challenging and traditional formu-
lations assume that the same function applies to all pixels,
ignoring correlations in the inputs. Moreover, the inversion
of the linearized system (7) may still be ill-conditioned [1].
To overcome this problem, soft constraints in the form of
damping terms (8) can be added to the objective [2,3]. How-
ever, this may bias the system to sub-optimal solutions.

This paper addresses these problems by relaxing the
main assumptions of the Inverse Compositional (IC) algo-
rithm [2, 3] using data-driven learning. More speciﬁcally,
we propose to learn the feature representation (A), robust

2We omit this dependency to avoid clutter in the notation.

3https://github.com/lvzhaoyang/DeeperInverseCompositionalAlgorithm

24582

estimator (B) and damping (C) jointly to replace the tradi-
tional heuristic rules of classical algorithms.

Direct Pose Regression: A notably different approach to
classical optimization techniques is to directly learn the en-
tire mapping from the input to the warping parameters ξ
from large amounts of data, spanning from early work using
linear hyperplane approximation [23] to recent work using
deep neural networks [9, 49, 51, 53, 58]. Prominent exam-
ples include single image to camera pose regression [25,26],
image-based 3D object pose estimation [34,46] and relative
pose prediction from two views [9, 49]. However, learning
a direct mapping requires high-capacity models and large
amounts of training data. Furthermore, obtaining pixel-
accurate registrations remains difﬁcult and the learned rep-
resentations do not generalize well to new domains.

To improve accuracy, recent methods adopt cascaded
networks [20,52] and iterative feedback [24]. Lin et al. [31]
combines the multi-step iterative spatial transformer net-
work (STN) with the classical IC algorithm [2, 3] for align-
ing 2D images. Variants of this approach have recently been
applied to various 3D tasks: Li et al. [30] proposes to itera-
tively align a 3D CAD model to an image. Zhou et al. [56]
jointly train for depth, pose and optical ﬂow.

Different from [31] and its variants which approximate
the pseudo-inverse of the Jacobian implicitly using stacked
convolutional layers, we exploit the structure of the opti-
mization problem and explicitly solve the original robust
objective (6) with learned modules using few parameters.

Learning-based Optimization: Recently, several meth-
ods have exploited the differentiable nature of iterative op-
timization algorithms by unrolling for a ﬁxed number of it-
erations. Each iteration is treated as a layer in a neural net-
work [12, 28, 36, 39, 40, 55]. In this section, we focus on
the most related work which also tackles the least-squares
optimization problem [13, 38, 48, 51]. We remark that most
of these techniques can be considered special cases of our
more general deep IC framework.

Wang et al. [51] address the 2D image tracking prob-
lem by learning an input representation using a two-stream
Siamese network for the IC setting. In contrast to us, they
exploit only spatial but not temporal correlations in the in-
puts (A), leverage a formulation which is not robust (B) and
do not exploit trust-region optimization (C).

Clark et al. [13] propose to jointly learn depth and pose
estimation by minimizing photometric error using the for-
mulation in (4). In contrast to us, they do not learn feature
representations (A) and neither employ a robust formulation
(B) nor trust-region optimization (C).

Ranftl et al. [38] propose to learn robust weights for
sparse feature correspondences and apply their model to
fundamental matrix estimation. As they do not target direct
image alignment, their problem setup is different from ours.

Besides, they neither learn input features (A) nor leverage
trust-region optimization (C). Instead, they solve their opti-
mization problem using singular value decomposition.

Concurrent to our work, Tang and Tan [48] propose
a photometric Bundle Adjustment network by learning to
align feature spaces for monocular reconstruction of a static
scene. Different from us, they did not exploit temporal cor-
relation in the inputs (A) and do not employ a robust for-
mulation (B). While they propose to learn the damping pa-
rameters (C), in contrast to our trust-region volume formu-
lation, they regress the damping parameters from the global
average pooled residuals.

3. Method

This section describes our model. A high-level overview
over the proposed unrolled inverse-compositional algorithm
is given in Fig. 1. Using the same notation as in Section 1,
our goal is to minimize the error by warping the image to-
wards the template, similar to (6):

min

ξ

r(ξ)T Wθ r(ξ)

(9)

The difference to (6) is that in our formulation, the weight
matrix Wθ as well as the template Tθ(ξ) and the image
Iθ(ξ) (and thus also the residual r(ξ) = Iθ(ξ) − Tθ(0))
depend on the parameters of a learned model θ.

We exploit the inverse compositional algorithm to solve
the non-linear optimization problem in (9), i.e., we linearize
the objective and iteratively update the warp parameters ξ:

ξk+1 = ξk ◦ (∆ξ)−1
∆ξ = (JT WθJ + diag(λθ))
rk = Iθ(ξk) − Tθ(0)
J = ∂Tθ(0)/∂ξ

−1

JT Wθ rk

(10)

(11)

(12)

(13)

starting from ξ0 = 0.

The most notable change from (8) is that the image fea-
tures (Iθ, Tθ), the weight matrix (Wθ) and the damping
factors λθ are predicted by learned functions which have
been collectively parameterized by θ = {θI, θW, θλ}. Note
that also the residual rk as well as the Jacobian J implicitly
depend on the parameters θ, though we have omitted this
dependence here for notational clarity.

We will now provide details about these mappings.

(A) Two-View Feature Encoder: We use a fully convo-
lutional neural network φθ to extract feature maps from the
image I and the template T:

Iθ = φθ([I, T])
Tθ = φθ([T, I])

(14)

(15)

Here, the operator [·, ·] indicates concatenation along the
feature dimension.
Instead of I and T we then feed Iθ

34583

[", $]

[$, "]

Feature Pyramid

(A) Two-View 
Feature Encoder

(A) Two-View 
Feature Encoder

(A) Two-View 
Feature Encoder

(A) Two-View 
Feature Encoder

"'

∑

"(

∑

"(

∑

"'

∑

K	Inverse-
Composition

K	Inverse-
Composition

K	Inverse-
Composition

K	Inverse-
Composition

$'

$'

$'

$'

Pre-computation

'

Inverse-composition
at ;th iteration

Residual maps from damping proposals:

("# (*)

(,

%0

−

"#

$#

(B) Convolutional 

M-estimator

−

%2

-(.) → Δ1(.) → %2+1

(.)

, . ∈ {1 … N}

(C) Trust Region 

Network

,:

,2+1 = ,2 ∘ Δ, −>

Δ,

-? 

Figure 1: High-level Overview of our Deep Inverse Compositional (IC) Algorithm. We stack [I, T] and [T, I] as inputs
to our (A) Two-View Feature Encoder pyramid which extracts 1-channel feature maps Tθ and Iθ at multiple scales using
channel-wise summation. We then perform K IC steps at each scale using Tθ and Iθ as input. At the beginning of each scale,
we pre-compute W using our (B) Convolutional M-estimator. For each of the K IC iterations, we compute the warped
(i)
image I(ξk) and rk. Subsequently, we sample N damping proposals λ(i) and compute the proposed residual maps r
k+1. Our
(C) Trust Region Network takes these residual maps as input and predicts λθ for the trust region update step.

and Tθ to the residuals in (12) and to the Jacobian in (13).
Note that we use the notation Iθ(ξk) in (12) to denote that
the feature map Iθ is warped by a warping function that is
parameterized via ξ. More formally, this can be stated as
Iθ(ξ) = Iθ(Wξ(x)), where x ∈ R2 denote pixel locations
and Wξ : R2 → R2 is a warping function that maps a pixel
location to another pixel location. For instance, Wξ may
represent the space of 2D translations or afﬁne transforma-
tions. In our experiments, we will focus on challenging 3D
rigid body motions, using RGB-D inputs and representing
ξ as an element of the Euclidean group ξ ∈ se(3).

Note that compared to directly using the image I and T
as input, our features capture high-order spatial correlations
in the data, depending on the receptive ﬁeld size of the con-
volutional network. Moreover, they also capture temporal
information as they operate on both I and T as input.

(B) Convolutional M-Estimator: We parameterize the
weight matrix Wθ as a diagonal matrix whose elements are

determined by a fully convolutional network ψθ that oper-
ates on the feature maps and the residual:

Wθ = diag(ψθ(Iθ(ξk), Tθ(0), rk))

(16)

as is illustrated in Fig. 1. Note that this enables our al-
gorithm to reason about relevant image information while
capturing spatial-temporal correlations in the inputs which
is not possible with classical M-Estimators. Furthermore,
we do not restrict the implicit robust function ρ to a partic-
ular error model, but instead condition ρ itself on the input.
This allows for learning more expressive noise models.

(C) Trust Region Network: For estimating the damping
λθ we use a fully-connected network as illustrated in Fig. 1.
We ﬁrst sample a set of scalar damping proposals λi on
a logarithmic scale and compute the resulting Levenberg-
Marquardt update step as

∆ξi = (JT WJ + λi diag(JT WJ))

−1

JT W rk(0) (17)

44584

We stack the resulting N residual maps

r

(i)

k+1 = Iθ(ξk ◦ (∆ξi)−1) − Tθ(0)

(18)

into a single feature map, ﬂatten it, and pass it to a fully
connected neural network νθ that outputs the damping pa-
rameters λθ:

λθ = νθ (cid:16)JT WJ,hJT Wr

(1)
k+1, . . . , JT Wr

(N )

k+1i(cid:17) (19)

The intuition behind our trust region networks is that the
residuals predicted using the Levenberg-Marquardt update
comprise valuable information about the damping parame-
ter itself. This is empirically conﬁrmed by our experiments.

Coarse-to-Fine Estimation: To handle large motions, it is
common practice to apply direct methods in a coarse-to-ﬁne
fashion. We apply our algorithm at four pyramid levels with
three iterations each. Our entire model including coarse-to-
ﬁne estimation is illustrated in Fig. 1. We extract features at
all four scales using a single convolutional neural network
with spatial average pooling between pyramid levels. We
start with ξ = 0 at the coarsest pyramid level, perform 3
iterations of our deep IC algorithm, and proceed with the
next level until we reach the original image resolution.

Training and Inference: For training and inference, we
unroll the iterative algorithm in equations (10)-(13). We ob-
tain the gradients of the resulting computation graph using
auto-differentiation. Details about the network architectures
which we use can be found in the supplementary material.

4. Experiments

We perform our experiments on the challenging task
of 3D rigid body motion estimation from RGB-D inputs4.
Apart from the simple scenario of purely static scenes where
only the camera is moving, we also consider scenes where
both the camera as well as objects are in motion, hence re-
sulting in strong ambiguities. We cast this as a supervised
learning problem: using the ground truth motion, we train
the models to resolve these ambiguities by learning to focus
either on the foreground or the background.

Warping Function: Given pixel x ∈ R2, camera intrin-
sics K and depth D(x), we deﬁne the warping Wξ(x) in-
duced by rigid body transform Tξ with ξ ∈ se(3) as

Wξ(x) = K Tξ D(x)K−1 x

(20)

Using the warped coordinates, compute Iθ(ξ) via bilinear
sampling from Iθ and set the warped feature value to zero
for all occluded areas (estimated via z-buffering).

Training Objective: To balance the inﬂuences of trans-
lation and rotation we follow [30] and exploit the 3D End-
Point-Error (EPE) as loss function. Let p = D(x)K−1x
denote the 3D point corresponding to pixel x in image I
and let P denote the set of all such 3D points. We minimize
the following loss function

L =

1
|P| X

l∈L

X

p∈P

kTgt p − T(ξl) pk2

2

(21)

where L denotes the set of coarse-to-ﬁne pyramid levels (we
apply our loss at the ﬁnal iteration of every pyramid level)
and Tgt is the ground truth transformation.

Implementation: We use the Sobel operator to compute
the gradients in T and analytically derive J. We calculate
the matrix inverse on the CPU since we observed that invert-
ing a small dense matrices H ∈ R6×6 is signiﬁcantly faster
on the CPU than on the GPU. In all our experiments we
use N = 10 damping proposals for our Trust Region Net-
work, sampled uniformly in logscale between [10−5, 105].
We use four coarse-to-ﬁne pyramid levels with three itera-
tions each. We implemented our model and the baselines in
PyTorch. All experiments start with a ﬁxed learning rate of
0.0005 using ADAM [27]. We train a total of 30 epochs,
reducing the learning rate at epoch [5,10,15].

4.1. Datasets

We systematically train and evaluate our method on four

datasets which we now brieﬂy describe.

MovingObjects3D: For the purpose of systematically eval-
uating highly varying object motions, we downloaded six
categories of 3D models from ShapeNet [10]. For each ob-
ject category, we rendered 200 video sequences with 100
frames in each sequence using Blender. We use data ren-
dered from the categories ’boat’ and ’motorbike’ as test set
and data from categories ’aeroplane’, ’bicycle’, ’bus’, ’car’
as training set. From the training set we use the ﬁrst 95%
of the videos for training and the remaining 5% for valida-
tion. In total, we obtain 75K images for training, 5K images
for validation, and 25K for testing. We further subsample
the sequences using sampling intervals {1, 2, 4} in order to
obtain small, medium and large motion subsets.

For each rendered sequence, we randomly select one 3D
object model within the chosen category and stage it in a
static 3D cuboid room with random wall textures and four
point light sources. We randomly choose the camera view-
point, point light source position and object trajectory, see
supplementary material for details. This ensures diversity in
object motions, textures and illumination. The videos also
contain frames where the object is only partially visible. We
exclude all frames where the entire object is not visible.

4Additional experiments on classical afﬁne 2D motion estimation tasks

can be found in the supplementary material.

BundleFusion: To evaluate camera motion estimation in
a static environment, we use the eight publicly released

54585

scenes from BundleFusion5 [14] which provide fully syn-
chronized RGB-D sequences. We hold out ’copyroom’ and
’ofﬁce2’ scenes for test and split the remaining scenes into
training (ﬁrst 95% of each trajectory) and validation (last
5%). We use the released camera trajectories as ground
truth. We subsampled frames at intervals {2, 4, 8} to in-
crease motion magnitudes and hence the level of difﬁculty.

DynamicBundleFusion: To further evaluate camera mo-
tion estimation under heavy occlusion and motion ambigu-
ity, we use the DynamicBundleFusion dataset [33] which
augments the scenes from BundleFusion with non-rigidly
moving human subjects as distractors. We use the same
training, validation and test split as above. We train and
evaluate frames subsampled at intervals {1, 2, 5} due to the
increased difﬁculty of this task.

TUM RGB-D SLAM: We evaluate our camera motion es-
timates on the TUM RGB-D SLAM dataset [45]. We hold
out ’fr1/360’, ’fr1/desk’, ’fr2/desk’ and ’fr2/pioneer 360’
for testing and split the remaining trajectories into training
(ﬁrst 95% of each trajectory) and validation (last 5%). We
randomly sample frame intervals from {1,2,4,8}. All im-
ages are resized to 160×120 pixels and depth values outside
the range [0.5m, 5.0m] are considered invalid.

4.2. Baselines

We implemented the following baselines.

ICP: We use classical Point-to-Plane ICP [11] and Point-
to-Point-ICP [4] implemented in Open3D [57]. To examine
the effect of ambiguity between the foreground and back-
ground in the object motion estimation task, we also evalu-
ate a version for which we provide the ground truth instance
segmentation to both methods. Note that this is an upper
bound to the performance achievable by ICP methods. We
thus call these the Oracle ICP methods.

RGB-D Visual Odometry: We compare to the RGB-D
visual odometry method [44] implemented in Open3D [57]
on TUM RGBD SLAM datasets for visual odometry.

Direct Pose Regression: We compare three different vari-
ants that directly predict the mapping f : I, T → ξ. All
three networks use Conv1-6 encoder layers from FlowNet-
Simple [15] as two-view regression backbones. We use spa-
tial average pooling after the last feature layer followed by a
fully-connected layer to regress ξ. All three CNN baselines
are trained using the loss in (21).
• PoseCNN: A feed-forward CNN that directly predicts ξ.
• IC-PoseCNN: A PoseCNN with iterative reﬁnement us-
ing the IC algorithm, similar to [31] and [30]. We noticed
that training becomes unstable and performance saturates

5http://graphics.stanford.edu/projects/bundlefusion/

when increasing the number of iterations. For all our ex-
periments, we thus used three iterations.

• Cascaded-PoseCNN: A cascaded network with three it-
erations, similar to IC-PoseCNN but with independent
weights for each iteration.

Learning-based Optimization: We implemented the fol-
lowing related algorithms within our deep IC framework.
For all methods, we use the same number of iterations,
training loss and learning rate as used for our method.
• DeepLK-6DoF: We implemented a variant of DeepLK
[50] which predicts the 3D transformation ξ ∈ se(3) in-
stead of translation and scale prediction in their original
2D task. We use Gauss-Newton as the default optimiza-
tion for this approach and no Convolutional M-Estimator.
A comparison of this approach with our method when
using only the two-view feature network (A) shows the
beneﬁts of our two-view feature encoder.

• IC-FC-LS-Net: We also implemented LS-Net [13]
within our IC framework with the following differences
to the original paper. First, we do not estimate or re-
ﬁne depth. Second, we do not use a separate network
to provide an initial pose estimation. Third, we replace
their LSTM layers with three fully connected (FC) layers
which take the ﬂattened JT WJ and JT Wrk as input.

Ablation Study: We use (A), (B), (C) to refer to our contri-
butions in Sec.1. We set W to the identity matrix when the
Convolutional M-Estimator (B) is not used and use Gauss-
Newton optimization in the absence of the Trust Region
Network (C). We consider the following conﬁgurations:
• Ours (A)+(B)+(C): Our proposed method with shared
weights. We perform coarse-to-ﬁne iterations on four
pyramid levels with three IC iterations at each level. We
use shared weights for all iterations in (B) and (C).

• Ours (A)+(B)+(C) (No WS): A version of our method
without shared weights. All settings are the same as
above except that the network for (B) and (C) have inde-
pendent weight parameters at each coarse-to-ﬁne scale.

• Ours (A)+(B)+(C) (K iterations/scale): The same net-
work as the default setting with shared weights, except
that we change the inner iteration number K.

• No Learning: Vanilla coarse-to-ﬁne IC alignment mini-

mizing photometric error (4) without learned modules.

4.3. Results and Discussion

Table 1 and Table 2 summarize our main results. For
each dataset, we evaluate the method separately for three
different motion magnitudes {Small, Medium, Large}. In
Table 1, {Small, Medium, Large} correspond to frames
sampled from the original videos at intervals {1, 2, 4}. In
Table 2, [Small, Medium, Large] correspond to frame in-
tervals {2, 4, 8} on BundleFusion and {1, 2, 5} on Dynam-
icBundleFusion. We show the following metrics/statistics:

64586

Model Descriptions

Point-Plane ICP [11]
Point-Point ICP [4]
Oracle Point-Plane ICP [11]
Oracle Point-Point ICP [4]

PoseCNN
IC-PoseCNN
Cascaded-PoseCNN

No learning
IC-FC-LS-Net, adapted from [13]
DeepLK-6DoF, adapted from [50]
Ours: (A)
Ours: (A)+(B)
Ours: (A)+(B)+(C)
Ours: (A)+(B)+(C) (No WS)
Ours: (A)+(B)+(C) (K = 1)
Ours: (A)+(B)+(C) (K = 5)

P
C

I

R
P
D

m

i
t
p
O
g
n
i
n
r
a
e
L

3D EPE (cm) ↓ on Validation/Test

Θ (Deg) ↓ / t (cm) ↓ / (t < 5 (cm) & Θ < 5◦) ↑ on Test

Small

Medium

Large

Small

Medium

Large

4.88/4.28
5.02/4.38
3.91/3.31
4.34/3.99

5.18/4.60
5.14/4.56
5.24/4.68

11.66/11.26

4.96/4.62
4.41/3.75
4.35/3.66
4.33/3.26
3.58/2.91
3.62/2.89
4.12/3.37
3.60/2.92

10.13/8.74
10.33/9.06
10.68/9.63
11.83/10.29

10.43/9.20
10.40/9.13
10.43/9.21

21.85/22.95
10.49/9.21
9.05/7.54
8.80/7.23
8.84/7.30
7.30/5.94
7.54/6.08
8.64/7.08
7.49/6.09

20.24/17.43
20.43/17.68
22.53/19.98
21.27/26.13

20.08/17.74
19.80/17.31
20.32/17.30

37.01/38.88
20.31/17.34
18.46/15.33
18.28/15.06
18.14/15.04
15.48/12.96
16.00/12.98
17.67/14.92
16.06/13.01

4.32/10.54/66.23%
4.04/10.51/70.0%
2.74/9.75/78.6%
3.81/10.11/75.0%

15.40/40.45/11.2%
8.29/20.90/33.4%
7.89/20.15/33.8%
15.33/40.52/11.1%
8.31/19.72/40.3% 16.64/41.40/16.4%
9.30/26.41/39.1%
19.53/62.30/14.1%

3.91/10.51/69.8%
3.93/10.49/70.1%
3.90/10.50/70.0%

7.89/21.10/34.7%
7.90/21.01/34.8%
7.90/21.11/34.6%

15.34/40.54/11.0%
15.31/40.51/11.1%
15.32/40.60/10.8%

4.29/15.90/66.8%
3.94/10.49/70.0%
4.03/10.25/68.1%
4.09/10.19/68.6%
4.02/10.11/69.2%
3.74/9.73/74.5%
3.69/9.73/74.9%
3.85/9.95/71.2%
3.68/9.77/74.4%

8.33/31.80/32.2%
7.90/21.20/34.5%
7.96/20.34/33.6%
8.00/20.28/32.9%
7.96/20.26/33.4%
7.41/19.60/38.2%
7.37/19.74/38.4%
7.80/20.13/35.5%
7.46/19.69/37.8%

15.78/55.70/10.44%
15.33/40.63/11.2%
15.43/39.56/10.6%
15.37/39.49/10.9%
15.35/39.56/10.9%
14.71/38.39/12.9%
14.65/38.69/12.8%
15.21/39.30/11.6%
14.76/38.65/12.4%

Table 1: Quantitative Evaluation on MovingObjects3D. We evaluate the average 3D EPE, angular error in Θ (Euler angles),
translation error t and success ratios t < 5 & Θ < 5◦ for three different motion magnitudes {Small, Medium, Large} which
correspond to frames sampled from the original videos using frame intervals {1, 2, 4}.

Model Descriptions

3D EPE (cm) ↓ Validation/Test

on BundleFusion [14]

3D EPE (cm) ↓ Validation/Test
on DynamicBundleFusion [33]

Model
Size (K)

Inference
Time (ms)

P
C

I

R
P
D

m

i
t
p
O
g
n
i
n
r
a
e
L

Point-Plane ICP [11]
Point-Point ICP [4]

PoseCNN
IC-PoseCNN
Cascaded-PoseCNN

No learning
IC-FC-LS-Net, adapted from [13]
DeepLK-6DoF, adapted from [50]
Ours: (A)
Ours: (A)+(B)
Ours: (A)+(B)+(C)
Ours: (A)+(B)+(C) (No WS)

Small

Medium

Large

Small

Medium

Large

2.81/2.01
3.62/2.48

4.76/3.41
4.32/3.26
4.46/3.41

4.52/3.35
4.04/3.03
4.09/2.99
3.59/2.65
2.42/1.75
2.27/1.48
2.14/1.52

6.85/4.52
8.17/5.72

9.78/6.85
8.98/6.52
9.38/6.81

8.64/6.30
9.06/6.85
8.14/5.84
7.68/5.46
5.39/3.47
5.11/3.09
5.15/3.10

16.20/11.11
17.23/12.58

17.90/13.31
16.30/12.81
16.50/13.02

17.06/12.51
17.15/13.32
16.81/12.27
16.56/11.92
12.59/8.40
12.16/7.84
12.26/7.81

1.26/0.97
1.42/1.08

2.20/1.65
2.21/1.66
2.20/1.60

4.89/3.39
2.33/1.80
2.15/1.72
2.01/1.65
2.22/1.70
1.09/0.74
0.93/0.61

2.64/2.09
3.40/2.42

4.22/3.19
4.24/3.18
4.13/3.15

5.64/4.69
4.43/3.45
3.78/3.12
3.68/2.96
3.61/2.97
2.15/1.54
1.86/1.32

10.38/4.89
13.09/7.43

13.90/8.24
12.99/8.05
12.97/8.15

13.88/8.58
13.84/8.35
12.73/7.22
12.68/7.11
12.57/6.88
9.78/4.64
8.88/3.82

-
-

19544
19544
58632

-

674
596
597
622
662
883

310
402

5.7
14.1
14.1

1.5
1.7
2.2
2.2
2.4
7.6
7.6

Table 2: Quantitative Evaluation on BundleFusion and DynamicBundleFusion. In BundleFusion [14], the motion mag-
nitudes {Small, Medium, Large} correspond to frame intervals {2, 4, 8}. In DynamicBundleFusion [33], the motion magni-
tudes {Small, Medium, Large} correspond to frame intervals {1, 2, 5} (we reduce the intervals due to the increased difﬁculty).

mRPE: θ (Deg) ↓ / t (cm) ↓

KF 1

KF 2

KF 4

KF 8

RGBD VO [44]
Ours: (A)
Ours: (A)+(B)
Ours: (A)+(B)+(C)

0.55/1.03
0.53/1.17
0.51/1.14
0.45/0.69

1.39/2.81
0.97/2.63
0.87/2.44
0.63/1.14

3.99/5.95
2.87/6.89
2.60/6.56
1.10/2.09

9.20/13.83
7.63/12.16
7.30/11.21
3.76/5.88

Table 3: Results on TUM RGB-D Dataset [45]. This table
shows the mean relative pose error (mRPE) on our test split
of the TUM RGB-D Dataset [45]. KF denotes the size of
the key frame intervals. Please refer to the supplementary
materials for a detailed evaluation of individual trajectories.

• 3D End-Point-Error (3D EPE): This metric is deﬁned
in (21). We only evaluate errors on the rigidly moving
objects, i.e., the moving objects in MovingObjects3D and
the rigid background mask in DynamicBundleFusion.

• Object rotation and translation: We evaluate 3D rota-

tion using the norm of Euler angles Θ, translation t in cm
and the success ratio (t < 5 (cm) & Θ < 5◦), in Table 1.
• Relative Pose Error: We follow the TUM RGBD metric

[45] using relative axis angle θ and translation t in cm.

• Model Weight: The number of learnable parameters.
• Inference speed: The forward execution time for an

image-pair of size 160 × 120, using GTX 1080 Ti.

Comparison to baseline methods: Compared to all base-
line methods (Table 1 row 1-10 and Table 2 row 1-8), our
full model ((A)+(B)+(C)) achieves the overall best per-
formance across different motion magnitudes and datasets
while maintaining fast inference speed. Compared to ICP
methods (ICP rows in Table 1 and Table 2) and classical
method (No Learning in Table 1 and Table 2), our method
achieves better performance without instance information
at runtime. Compared to RGB-D visual odometry [44],
our method works particularly well in unseen scenarios and

74587

T

I

I(ξGT)

I(ξ⋆)

Figure 2: Qualitative Results on MovingObjects3D. Visualization of the warped image I(ξ) using the ground truth object
motion ξGT (third row) and the object motion ξ⋆ estimated using our method (last row) on the MovingObjects3D validation
(left two) and test sets (right three). In I, we plot the instance boundary in red and the instance boundary of T in green
as comparison. Note the difﬁculty of the task (truncation, independent background object) and the high quality of our
alignments. Black regions in the warped image are due to truncation or occlusion.

in the presence of large motions (Table 3). Besides, note
that our model can achieve better performance with a sig-
niﬁcantly smaller number of weight parameters compared
to direct image-to-pose regression (DRP rows in Table 1
and Table 2). Fig. 2 shows a qualitative comparison of
our method on the MovingObject3D dataset by visualizing
I(ξ). Note that our method achieves excellent two-view im-
age alignments, despite large motion, heavy occlusion and
varying illumination of the scene.

Ablation Discussion: Across all ablation variants and
datasets, our model achieves the best performance by com-
bining all three proposed modules ((A)+(B)+(C)). This
demonstrates that all components are relevant for robust
learning-based optimization. Note that the inﬂuence of the
proposed modules may vary according to the properties of
the speciﬁc task and dataset. For example, in the presence
of noisy depth estimates from real data, learning a robust M-
estimator (B) (Table 2 row 10 in BundleFusion) provide sig-
niﬁcant improvements. In the presence of heavy occlusions
and motion ambiguities, learning the trust region step (C)
helps to ﬁnd better local optima which results in large im-
provements in our experiments, observed when estimating
object motion (Table 1 row 13) and when estimating cam-

era motion in dynamic scenes (Table 2 row 11 in Dynam-
icBundleFusion). We ﬁnd our trust region network (C) to be
highly effective for learning accurate relative motion in the
presence of large variations in motion magnitude(Table 3).

5. Conclusion

We have taken a deeper look at the inverse compositional
algorithm by rephrasing it as a neural network with three
trainable submodules which allows for relaxing some of the
most signiﬁcant restrictions of the original formulation. Ex-
periments on the challenging task of relative rigid motion
estimation from two RGB-D frames demonstrate that our
method achieves more accurate results compared to both
classical IC algorithms as well as data hungry direct image-
to-pose regression techniques. Although our data-driven IC
method can better handle challenges in large object mo-
tions, heavy occlusions and varying illuminations, solving
those challenges in the wild remains a subject for future re-
search. To extend the current method to real-world envi-
ronments with complex entangled motions, possible future
directions include exploring multiple motion hypotheses,
multi-view constraints and various motion models which
will capture a broader family of computer vision tasks.

84588

References

[1] P. Anandan. A computational framework and an algorithm
for the measurement of visual motion. International Journal
of Computer Vision (IJCV), 2(3):283–310, 1989. 2

[2] S. Baker, R. Gross, I. Matthews, and T. Ishikawa. Lucas-
kanade 20 years on: A unifying framework: Part 2. Technical
report, Carnegie Mellon University, 2003. 1, 2, 3

[3] S. Baker and I. Matthews. Lucas-kanade 20 years on: A uni-
fying framework: Part 1. Technical report, Carnegie Mellon
University, Pittsburgh, PA, July 2002. 1, 2, 3

[4] P. J. Besl and N. D. McKay. A method for registration
IEEE Trans. Pattern Anal. Machine Intell.,

of 3-d shapes.
14(2):239–256, Feb. 1992. 6, 7

[5] A. Bibi, M. Mueller, and B. Ghanem. Target response adap-
tation for correlation ﬁlter tracking. In Proc. of the European
Conf. on Computer Vision (ECCV), 2016. 1

[6] A. Bj¨orck. Numerical Methods for Least Squares Problems.

Siam Philadelphia, 1996. 2

[7] M. J. Black and P. Anandan. The robust estimation of
multiple motions: Parametric and piecewise-smooth ﬂow
ﬁelds. Computer Vision and Image Understanding (CVIU),
63(1):75–104, 1996. 2

[8] Brown, Matthew, Lowe, and David. Automatic panoramic
image stitching using invariant features. International Jour-
nal of Computer Vision (IJCV), 74(1):59–73, August 2007.
1

[9] A. Byravan and D. Fox. SE3-Nets: Learning rigid body mo-
tion using deep neural networks. In Proc. IEEE International
Conf. on Robotics and Automation (ICRA), pages 173–180.
IEEE, 2017. 3

[10] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich
3D Model Repository. Technical Report arXiv:1512.03012
[cs.GR], Stanford University — Princeton University —
Toyota Technological Institute at Chicago, 2015. 5

[11] Y. Chen and G. Medioni. Object modelling by registration
Image and Vision Computing,

of multiple range images.
10(3):145–155, 1992. 6, 7

[12] I. Cherabier, J. Sch¨onberger, M. Oswald, M. Pollefeys, and
A. Geiger. Learning priors for semantic 3d reconstruction.
In Proc. of the European Conf. on Computer Vision (ECCV),
2018. 3

[13] R. Clark, M. Bloesch, J. Czarnowski, S. Leutenegger, and
A. J. Davison. Learning to solve nonlinear least squares for
monocular stereo.
In European Conf. on Computer Vision
(ECCV), Sept. 2018. 1, 3, 6, 7

[14] A. Dai, M. Nießner, M. Zoll¨ofer, S. Izadi, and C. Theobalt.
BundleFusion: real-time globally consistent 3D reconstruc-
tion using on-the-ﬂy surface re-integration. ACM Transac-
tions on Graphics 2017 (TOG), 2017. 6, 7

[15] A. Dosovitskiy, P. Fischer, E. Ilg, P. Haeusser, C. Hazirbas,
V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet:
Learning optical ﬂow with convolutional networks. In Proc.
of the IEEE International Conf. on Computer Vision (ICCV),
2015. 6

[16] J. Engel, V. Koltun, and D. Cremers. Direct sparse odome-
try. IEEE Trans. Pattern Anal. Machine Intell., 40:611–625,
2018. 1, 2

[17] J. Engel, T. Sch¨ops, and D. Cremers. LSD-SLAM: Large-
scale direct monocular SLAM. In European Conf. on Com-
puter Vision (ECCV), September 2014. 1

[18] B. K. Horn and E. Weldon. Direct methods for recovering

motion. Intl. J. of Computer Vision, 2(1):51–76, 1988. 2

[19] P. J. Huber. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 03 1964. 2
[20] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet 2.0: evolution of optical ﬂow estimation
with deep networks. In IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR), 2017. 3

[21] M. Irani and P. Anandan. About direct methods. In Proc.
of the IEEE International Conf. on Computer Vision (ICCV)
Workshops, 2000. 2

[22] M. Irani and S. Peleg. Improving resolution by image reg-
istration. Graphical Model and Image Processing (CVGIP),
53(3):231–239, 1991. 1

[23] F. Jurie and M. Dhome. Hyperplane approximation for tem-
plate matching. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 24(7):996–1000, 2002. 3

[24] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-
to-end recovery of human shape and pose.
In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
2018. 3

[25] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu-
tional network for real-time 6-dof camera relocalization. In
Proc. of the IEEE International Conf. on Computer Vision
(ICCV), 2015. 3

[26] A. Kendall, H. Martirosyan, S. Dasgupta, and P. Henry. End-
to-end learning of geometry and context for deep stereo re-
gression. In Proc. of the IEEE International Conf. on Com-
puter Vision (ICCV), 2017. 3

[27] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In Proc. of the International Conf. on Learning
Representations (ICLR), 2015. 5

[28] E. Kobler, T. Klatzer, K. Hammernik, and T. Pock. Varia-
tional networks: Connecting variational methods and deep
learning.
In Proc. of the German Conference on Pattern
Recognition (GCPR), 2017. 3

[29] A. Levin, A. Zomet, S. Peleg, and Y. Weiss. Seamless image
stitching in the gradient domain. In Proc. of the European
Conf. on Computer Vision (ECCV), 2004. 1, 2

[30] Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox. Deepim: Deep
In Proc. of the

iterative matching for 6d pose estimation.
European Conf. on Computer Vision (ECCV), 2018. 3, 5, 6

[31] C.-H. Lin and S. Lucey. Inverse compositional spatial trans-
former networks. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2017. 3, 6

[32] B. D. Lucas and T. Kanade. An iterative image registration
technique with an application to stereo vision. In Proc. of the
International Joint Conf. on Artiﬁcial Intelligence (IJCAI),
1981. 1, 2

[33] Z. Lv, K. Kim, A. Troccoli, D. Sun, J. Rehg, and J. Kautz.
Learning rigidity in dynamic scenes with a moving camera

94589

network for learning monocular stereo. In Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2017.
3

[50] C. Wang, H. K. Galoogahi, C.-H. Lin, and S. Lucey. Deep-lk
for efﬁcient adaptive object tracking. In IEEE Intl. Conf. on
Robotics and Automation (ICRA), 2018. 1, 6, 7

[51] C. Wang, J. Miguel Buenaposada, R. Zhu, and S. Lucey.
Learning depth from monocular videos using direct meth-
ods. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), June 2018. 3

[52] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con-
volutional pose machines. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), pages 4724–4732, 2016. 3
[53] Z. Yin and J. Shi. GeoNet: Unsupervised Learning of Dense
Depth, Optical Flow and Camera Pose. In Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2018.
3

[54] Z. Zhang, P. Robotique, and P. Robotvis. Parameter estima-
tion techniques: a tutorial with application to conic ﬁtting.
Image and Vision Computing (IVC), 15:59–76, 1997. 2

[55] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. H. S. Torr. Conditional ran-
dom ﬁelds as recurrent neural networks. In Proc. of the IEEE
International Conf. on Computer Vision (ICCV), 2015. 3

[56] H. Zhou, B. Ummenhofer, and T. Brox. Deeptam: Deep
In Proc. of the European Conf. on

tracking and mapping.
Computer Vision (ECCV), 2018. 3

[57] Q.-Y. Zhou, J. Park, and V. Koltun. Open3D: A modern li-

brary for 3D data processing. arXiv:1801.09847, 2018. 6

[58] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsu-
pervised learning of depth and ego-motion from video.
In
Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 3

for 3d motion ﬁeld estimation. In European Conf. on Com-
puter Vision (ECCV), 2018. 6, 7

[34] F. Manhardt, W. Kehl, N. Navab, and F. Tombari. Deep
model-based 6d pose reﬁnement in rgb. In Proc. of the Eu-
ropean Conf. on Computer Vision (ECCV), September 2018.
3

[35] D. W. Marquardt. An algorithm for least-squares estimation
of nonlinear parameters. Journal of the Society for Industrial
and Applied Mathematics (SIAM), 11(2):431–441, 1963. 2

[36] T. Meinhardt, M. M¨oller, C. Hazirbas, and D. Cremers.
Learning proximal operators: Using denoising networks for
regularizing inverse imaging problems. In Proc. of the IEEE
International Conf. on Computer Vision (ICCV), 2017. 3

[37] R. A. Newcombe, S. Lovegrove, and A. J. Davison. DTAM:
dense tracking and mapping in real-time.
In Proc. of the
IEEE International Conf. on Computer Vision (ICCV), 2011.
2

[38] R. Ranftl and V. Koltun. Deep fundamental matrix estima-
In Proc. of the European Conf. on Computer Vision

tion.
(ECCV), 2018. 3

[39] R. Ranftl and T. Pock. A deep variational model for image
segmentation. In Proc. of the German Conference on Pattern
Recognition (GCPR), 2014. 3

[40] G. Riegler, M. R¨uther, and H. Bischof. Atgv-net: Accurate
In Proc. of the European Conf. on

depth super-resolution.
Computer Vision (ECCV), 2016. 3

[41] D. Scharstein and R. Szeliski. A taxonomy and evaluation
of dense two-frame stereo correspondence algorithms. Inter-
national Journal of Computer Vision (IJCV), 47:7–42, 2002.
1

[42] J. Shi and C. Tomasi. Good features to track. In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
1994. 1

[43] H.-Y. Shum and R. Szeliski. Systems and experiment paper:
Construction of panoramic image mosaics with global and
local alignment. International Journal of Computer Vision
(IJCV), 36(2):101–130, February 2000. 1

[44] F. Steinbr¨ucker, J. Sturm, and D. Cremers. Real-time vi-
sual odometry from dense rgb-d images.
In Computer Vi-
sion Workshops (ICCV Workshops), 2011 IEEE Interna-
tional Conference on, pages 719–722. IEEE, 2011. 6, 7

[45] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers. A benchmark for the evaluation of rgb-d slam systems.
In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems
(IROS), Oct. 2012. 6, 7

[46] M. Sundermeyer, Z.-C. Marton, M. Durner, M. Brucker, and
R. Triebel. Implicit 3d orientation learning for 6d object de-
tection from rgb images. In Proc. of the European Conf. on
Computer Vision (ECCV), September 2018. 3

[47] R. Szeliski.

Image alignment and stitching: A tutorial.
Foundations and Trends in Computer Graphics and Vision,
2(1):1–104, 2007. 2

[48] C. Tang and P. Tan. Ba-net: Dense bundle adjustment net-
In Intl. Conf. on Learning Representations (ICLR),

work.
2019. 3

[49] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg,
A. Dosovitskiy, and T. Brox. Demon: Depth and motion

104590

