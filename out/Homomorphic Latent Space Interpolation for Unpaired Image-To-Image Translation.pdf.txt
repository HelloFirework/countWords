Homomorphic Latent Space Interpolation for Unpaired Image-to-image

Translation

Ying-Cong Chen1 Xiaogang Xu1

1The Chinese University of Hong Kong

Zhuotao Tian1

Jiaya Jia1
2Tencent Youtu Lab

2

,

{ycchen,xgxu,zttian,leojia}@cse.cuhk.edu.hk

Abstract

Generative adversarial networks have achieved great
success in unpaired image-to-image translation. Cycle con-
sistency allows modeling the relationship between two dis-
tinct domains without paired data. In this paper, we propose
an alternative framework, as an extension of latent space
interpolation, to consider the intermediate region between
two domains during translation. It is based on the fact that
in a ﬂat and smooth latent space, there exist many paths
that connect two sample points. Properly selecting paths
makes it possible to change only certain image attributes,
which is useful for generating intermediate images between
the two domains. We also show that this framework can be
applied to multi-domain and multi-modal translation. Ex-
tensive experiments manifest its generality and applicability
to various tasks.

1. Introduction

Unpaired image-to-image translation and latent space in-
terpolation were developed separately and serve different
applications. Unpaired image-to-image translation [28, 9,
14, 4] aims to map images from one domain to another,
e.g, translating a collection of neutral faces to smiling ones.
Since no pair information is available, the connection of dif-
ferent domains is usually built upon the cycle-consistency
constraint [28], which largely promotes the capacity of gen-
erative models and leads to many impressive results.

When the purpose is to generate a sequence of images
between the input two domains, intermediate states should
be considered, which is however beyond the capability of
the cycle-consistency constraint. We show an example in
Fig. 1 – directly using StarGAN [4] does not generate a nat-
ural sequence (or expression ﬂow) to gradually close mouth.
There exists a quick change between (c) and (d).

On the other hand,

latent
space interpolation [11, 21, 22] focuses on intermediate
states based on an assumption that deep neural networks

to generate smooth ﬂow,

N
A
G
r
a
t

S

s
r
u
O

(a)

(b)

(c)

(d)

(e)

Figure 1. Rendering intermediate states between (a) “open-mouth”
domain and (e) “close-mouth” domain. The ﬁrst-row results
are generated by StarGAN [4]. Rendering intermediate states is
achieved by altering the input domain label continuously. (c) and
(d) show that abrupt change of expression exists. Our results in the
second row model intermediate regions and show smooth transla-
tion effect.

can model natural images as ﬂat and smooth distributions.
Speciﬁcally, if x and y are sampled from two respective do-
mains X and Y, moving from x toward y in the latent space
continuously produces realistic images from domain X to
Y. Albeit this nice property, this method cannot directly
serve image-to-image translation because it does not dis-
tinguish among different attribute factors, and thus makes
complicated expression transition tangled with identity or
background changes. Also, the interpolation path ends at y
instead of a translated version of x.

In this paper, we address latent space interpolation in un-
paired image-to-image translation. This solution inherently
allows modeling intermediate regions between different do-
mains, with additional important and appealing capacity of
multi-domain and multi-modal translation. Since in a ﬂat
and smooth latent space, many paths exist to connect two
samples, interpolating along different paths leads to diverse
intermediate results [24]. Our idea is to choose the path
that only corresponds to a certain attribute component to
make transition natural to human perception. Here the term
attribute deﬁnes image domains. For example, smiling at-
tribute divides facial images to smiling and non-smiling do-
mains. Fig. 2 provides an example where translating be-

2408

Figure 2. Illustration of latent space interpolation along different paths. Paths 1 and 2 connect (a) “non-smiling male” and (b) “ smiling
female”. They change facial attributes in different orders – i.e., path 1 changes the smiling expression ﬁrst while path 2 interpolates
gender. They naturally serve the multi-domain image-to-image translation task where path 1(i) and 2(i) form translation between
smiling and non-smiling domains, and male and female domains respectively. Path 3(i) synthesizes a smile different from path 1(i). Thus,
using different target-domain samples, our method can produce output required for each domain, termed as multi-modal image-to-image
translation. Image sequence of the last row illustrates the continuous change of path 3(i).

tween Male and Female (or Smiling and Non-smiling) can
be achieved by interpolating along path 1(i) (or path 2(i))
respectively. Besides multi-domain and continuous transla-
tion capacity, as shown in paths 1(i) and 3(i), this model can
also deal with multi-modal translation.

With this principle, the key to our method is a control-
lable interpolator, whose output is controlled by a vector
v. Each element of v corresponds to a mixing indicator for
each attribute. We take path 3(i) of Fig. 2 for example. A
proper v only deals with the smiling attribute between (a)
and (c), while keeping other attributes untouched.

Although promising, this strategy requires conquering a
few difﬁculties. First, interpolation is only allowed in a
smooth and ﬂat space. VAE [13] imposes Gaussian prior
on the latent feature space so that interpolation is allowed.
However, it could generate blurry results, as a Gaussian
prior may be insufﬁcient to model complicated natural im-
ages. Our solution is to directly minimize the Wasserstein
distance between the interpolated and real samples of the
latent space. This makes interpolated sample distribution
as close as possible to the real ones. We also introduce a
knowledge guidance loss that leverages a well-trained net-
work to regularize the latent space, which further improves
interpolation quality. Finally, a homomorphic loss is intro-
duced to train the controllable interpolator. Our total con-
tribution is manifold.

• We propose an interpolation-based framework for un-
paired image-to-image translation, which is feasible
for multi-domain, multi-modal and continuous trans-
lation tasks.

• We propose a few important strategies to train our
model, leading to an interpolatable latent space and a
controllable interpolator.

• Extensive experiments show that our model can gener-
ate high-quality results and is ﬂexible to serve various
applications.

2. Related Work

Latent Space Interpolation Latent space interpolation is
widely used to visualize the manifold structure in a ﬂat fea-
ture space [10, 1, 21, 22, 2]. Intuitively, semantical inter-
polation in the latent space indicates that the space cap-
tures certain high-level information, which is beneﬁcial for
both recognition [1] and generation tasks [10]. However, a
vanilla interpolation between two images may not be that
useful for creation, since all attributes would change to-
gether along the interpolation path, and users lose control
of individual ones. One remedy is to interpolate along at-
tribute vectors rather than between samples [23, 13, 10, 3].
For certain target attributes, average of positive and negative
samples are computed, and the attribute vector is deﬁned as

2409

Path 1Path 2Path 3(a)(b)(c)(i) Non-smiling →Smiling (ii) Male →Female(i) Male →Female(ii) Non-smiling →Smiling (iii) Change other factors(iii) Change other factors(iii) Change other factors(i) Non-smiling →Smiling (ii) Male →Femalethe difference of them. This cancels out the inﬂuence of
non-target attributes and allows users to edit only the target
one. Nevertheless, it ignores the fact that many attributes
are intrinsically multi-modal. As illustrated in Fig. 2(b) and
(c), smiling can be quite different. Interpolation with a uni-
versal smiling attribute vector can only generate the average
smile. In contrast, our model can produce multi-modal re-
sults with different examples.

Image-to-image

Unpaired
Translation Unpaired
image-to-image translation [28, 4, 9, 15] aims to map
images of one domain to another. CycleGAN [28], Disco-
GAN [9] and DualGAN [26] are three pioneering methods,
which introduce the cycle-consistency constraint to build
the connection. There are however a few remaining issues.
The domain scalability issue refers to the incapability of
handling more than two domains, which is addressed by
StarGAN [4] and ModularGAN [27]. The multi-modality
issue refers to incapability to produce multiple results,
which is addressed by MUNIT [7] and DRIT [14]. The
discreteness issue refers to the inability to continuously
control the transformation strength between two domains,
which is addressed by GANimation [17]. We note GANi-
mation [17] requires continuous label annotation, which is
costly and is limited in the ﬁeld of facial expression.

Instead of relying on the cycle consistency constraint,
our model seeks another way to tackle the unpaired image-
to-image translation problem. Our model can be deemed
as a general alternative that tackles the domain scalability,
multi-modality and discreteness issues simultaneously.

3. Proposed Method

Without the loss of generality, we take the face attribute
translation task as an example to introduce our method.
Other tasks are also supported and are presented in the
supplementary material. We deﬁne the dataset as D =
{(I1, y1), (I2, y2) · · · (IN , yN )} of N samples, where Ii ∈
RH×W ×3 and yi = [y1
i ] are the i-th face image
and its corresponding attributes respectively. The subscript
and superscript index samples and attributes respectively.

i , · · · , yd

i , y2

We further introduce the concept of grouped attribute.
For example, we can group angry, happy, sad, contemp-
tuous, disguised, fear and surprise – these attributes are
provided in RaFD [12] dataset as binary attribute labels –
to form the group expression attribute. Thus, the plain at-
tributes yi can be rearranged to zi = {z1
i },
i ∈ Rci×1 denotes the k-th grouped attribute of the
where zk
i-th sample. This makes it more intuitive to use our model.
An instance is that paths 1(i), 2(ii) and 3(i) of Fig. 2, with
the expression attribute, consider the 8 expressions rather
than only smiling.

i , · · · , zc

i , z2

In the model level, we have an encoder E, an interpola-
tor I and a decoder D. The encoder E maps images Ii and

Ij to feature Fi = E(Ii) and Fj = E(Ij), so that the in-
terpolated feature I(Fi, Fj) is indistinguishable from real
samples. The interpolator I produces interpolated results
of two samples. The decoder D maps the latent features
back to the image space. In the following, we elaborate on
the design of each part.

3.1. Learning Encoder and Decoder

It is well known that natural images usually lie on a non-
convex manifold, making interpolation usually difﬁcult. We
train an encoder to unfold the image manifold to a ﬂattened
latent space, such that the interpolated samples are in real-
image space. This is achieved by applying GAN to make
interpolated feature similar to that of real samples.

Speciﬁcally, we leverage WGAN-GP [5] to train our
model. A critic D is trained to maximize the Wasserstein
distance between real samples and interpolated ones, and
the encoder E and interpolator I are trained to minimize
the distance between them. It is formulated as

LGAND = EPI [D( ˆF )] − EPr [D(F )] + λgpLgp,

LGANE,I = EPr [D(F )] − EPI [D( ˆF )],

(1)

(2)

min

D

min
E,I

where F = E(I) is the feature extracted by the encoder, ˆF
is the interpolated feature generated by ˆF = I(Fi, Fj), Pr
and PI are the distributions of real and interpolated sam-
ples respectively, and Lgp is the gradient penalty term de-
ﬁned in [5]. Here the interpolator I works with encoder E
cooperatively to generate reasonable images. More details
of I are provided in later sections.

Note that simply using Eqs. (1) and (2) may cause the
encoder to map all images to a small feature space where
interpolation becomes easy. To an extreme, if the encoder
maps all images to a single point, the interpolated and real
samples yield Wasserstein distance 0. But this trivial solu-
tion carries no information about the images. To avoid it, we
additionally incorporate a decoder D to invert features back
to images. The decoder is trained with perceptual loss [8] as
Eq. (3). The reconstruction term for the encoder is deﬁned
as Eq. (4).

min

D

LD = E(||Φ3(D(F )) − Φ3(I)||2),

(3)

min

E

Lrecon = E(||Φ3(D(E(I))) − Φ3(I)||2),

(4)

where Φ3(I) is the RELU3 1 feature of the VGG network.

Semantic Knowledge Guidance Previous work has ob-
served that a pretrained VGG network [20] can be utilized
for latent space interpolation [3, 23, 1]. We leverage this
property to guide the training of our encoder.
Inspired
by [18, 6], we treat a pretrained VGG network as a teacher,

2410

and use its intermediate layer to guide the training of our
encoder, formulated as

min
E,P

LKG = EPr ||P [E(I)] − Φ5(I)||2,

(5)

where P is a 1×1 convolutional layer that adapts the feature
space deﬁned by E(I) to the space of Φ5(I). Φ5 denotes
the ReLU5 1 layer of the VGG network [20]. As the VGG
network is trained with millions of images, Φ5(I) contains
rich semantic information and provides extra guidance for
the encoder. Generally, this term works as regularization
and helps the encoder converge to a good result.

By combining Eqs. (2), (4) and (5), the ﬁnal objective

function of the encoder E is

LE = λGANE LGANE,I + λreconLrecon + λKGLKG, (6)

where λGANE , λrecon and λKG are scalars to balance
terms. We set them as 1s in our experiments.

3.2. Learning Interpolator

With a well-learned encoder that maps images to a ﬂat

space, interpolation can be done linearly as

I(Fi, Fj) = Fi + α(Fj − Fi),

(7)

where Fi and Fj are two real samples, and α ∈ [0, 1] is a
parameter that controls the level of mixing of two samples.
The second term α(Fj −Fi) can also be viewed as a shifting
vector that points from Fi towards Fj .

Note that Eq. (7) only deﬁnes one possible path that con-
nects samples i and j. Other interpolation methods like
Slerp [19] can also connect them and produces different
intermediate results. Nevertheless, all these handcrafted
methods do not allow adjusting how attributes are mixed.
So they are not usable for our task. To accommodate image-
to-image translation, we extend I(Fi, Fj) to a more ﬂexi-
ble I v(Fi, Fj), where v ∈ [0, 1]c×1 is a control vector.
Each dimension of v sets the interpolation strength of each
grouped attribute between two samples. More speciﬁcally,
the linear interpolation deﬁned in Eq. (7) is extended to a
piecewise one of

I v(Fi, Fj) = Fi +

c

X

k=1

vkT k(Fj − Fi),

(8)

where vk is the kth dimension of v, and T k(·) is a learnable
mapping function represented by CNN.

Minimizing Homomorphic Gap
It is expected that
T k(Fj − Fi) and vk correspond to the interpolation direc-
tion and strength of the kth grouped attribute zk respec-
tively. As vk varies from 0 to 1, the kth grouped attribute
changes from sample i to j accordingly. If all possible val-
ues of z form an attribute space, interpolation in the latent

feature space should correspond to interpolation in the at-
tribute space. Let A(·) be a function that maps latent fea-
ture to an attribute vector, i.e., A(Fi) = zi, we deﬁne the
relation between the latent space and the attribute space as

A(I v(Fi, Fj)) = I ′

v(A(Fi), A(Fj)), ∀v ∈ [0, 1]c×1 (9)

j − zk

i + vk(zk

v(zi, zj)k = zk

v(zi, zj) = [I ′

where I ′
v(zi, zj) can be viewed as an interpolation func-
tion deﬁned in the attribute space. Further, I ′
v(zi, zj) is
deﬁned as I ′
v(zi, zj)1 · · · , I ′
v(zi, zj)c],
where I ′
i ). So the left hand
side of Eq. (9) denotes the attribute values of interpolated
samples I v(Fi, Fj), and the right hand side contains the
corresponding attribute values of the two samples. As both
sides are conditioned on the same control vector v, they are
expected to be equal. In this regard, Eq. (9) describes an
ideal case that the interpolation operations I v and I ′
v share
the same structure in the latent feature and attribute space.
This property is analogous to homomorphism in algebra. In
practice, there inevitably exists a gap between two sides in
Eq. (9), which we call the homomorphic gap.

With Eq. (9) introduced, our objective turns to minimiz-
ing the homomorphic gap. Recall that A(·) maps latent fea-
ture to attribute values, which is not deﬁned for interpolated
features. We choose to train a network A′(·) to approximate
A(·) and replace A(I v(Fi, Fj)) with A′(I v(Fi, Fj)) in
Eq. (9). Then we reduce the homomorphic gap by minimiz-
ing the cross-entropy of I ′
v(zi, zj) and A′(I v(Fi, Fj)), as
shown in Eq. (10). We call it the Homomorphic loss:

min
I v

LI hom = E[−I ′

v(zi, zj) log(A′(I v(Fi, Fj)))].

(10)
Also, v is deﬁned everywhere in the c-dimensional unit hy-
percube. During training, we assign uniformly random val-
ues to v to cover the whole feasible set.

Rigorous Training According to Eq. (8), optimizing Eq.
(10) needs to optimize T k(·), where k = 1, · · · , c.
In
experiments, when complicated attributes exist, the corre-
sponding T k(·) tends to be lazy – that is, it may update Fi
slightly to fool the attribute classiﬁcation network A′(·). To
alleviate this problem, we turn A′(·) to a rigorous classiﬁer:
instead of mapping Fi to zi, A′(·) is trained to map the in-
terpolated feature Fi + Pc
k=1 vkT k(Fj − Fi) to attribute
zi, expressed as

min
A′

LA′ = E[−zi log(A′(I v(Fi, Fj)))].

(11)

(10) and (11), we note that I v(·) and A′(·)
From Eqs.
are mutually dependent. Therefore, they are iteratively up-
In this way, A′(·) keeps checking
dated during training.
unchanged parts, making it harder for T k(·) to fool.

Handling Residual Components When v = 1 where
1 = [1, 1, · · · , 1] ∈ Rc×1, I v(Fi, Fj) is expected to reach

2411

sample j. However, this is not guaranteed with solely the
homomorphic loss, because the provided attributes may not
explain everything. Therefore, we extend Eq. (8) to

I v(Fi, Fj) = Fi +

c+1

X

k=1

vkT k(Fj − Fi),

(12)

where the additional mapping function T c+1(Fj −Fi) mod-
els the residual components that are not explained by the
given attributes. Accordingly, we extend the c-dimension
control vector v to c + 1 dimensions, where the last dimen-
sion is the edit strength of the residual mapping function.
Now we can safely impose the terminal of the interpolation
curve as Fj , which is formulated as

LI t = ||I v(Fi, Fj) − Fj||2, where v = 1.

(13)

To summarize this part, the overall loss function of I v is

LI = λGANI LGANE,I + λI hom LI hom + λI t LI t , (14)

where LGANE,I , LI hom and LI t are deﬁned in Eqs. (2),
(10) and (13) respectively. λGANI , λI hom and λI hom are
set to 1 in our experiments.

The training procedure is outlined in Algorithm 1. More
training details are contained in the supplementary material.

Algorithm 1 Training Our Model
Input: Ii and zi, where i = 1, 2, · · · , N
Output: encoder E, interpolator I v and decoder D

while not converged do

sample v from c-dimensional uniform distribution;
t ← 0;
while t < 5 do

update the critic D based on Eq. (1);
update the decoder D based on Eq. (3);
update the P in Eq. (5);
update the attribute classiﬁer A′ based on Eq. (11);

end while
update the encoder E based on Eq. (6);
update the interpolator I based on Eq. (14).

end while

3.3. Applications

We describe how our model can be applied to multi-
domain, multi-modal and continuous translation as follows.

Multi-domain Translation For each target domain t, we
preselect a sample It. Given a query sample Iq, domain
translation is conducted as

Iout = D(I vt (E(Iq), E(It))),

(15)

where vt is the vector corresponding to the target domain.

Dim

Attribute

Age

Expression
Hair Color
Hair Style

Labels
Young

Mouth Slightly Open, Smiling

Black Hair, Blond Hair Brown Hair, Gray Hair

Receding Hairline, Bangs

Gender Trait

Male, No Beard, Mustache, Goatee, Sideburns

1
2
3
4
5

Table 1. Grouped Attributes of CelebA [16]. The 1st-3rd columns:
dimension index in the control vector v, name of grouped at-
tributes, corresponding attribute labels.

Dim

Attribute

Labels

1

2
3

Expression

happy, angry, contemptuous, sad,
disgusted, neutral, fearful, surprised

Gaze
Others

look left, look front, look right
is Caucasian, is male, is kid

Table 2. Grouped Attributes of RaFD [12].

Multi-Modal Translation By using different exemplars in
Eq. (15), we can generate results like MUNIT [7].

Continuous Translation By changing vt in Eq.
(15)
smoothly, our model allows changing attributes continu-
ously. This controls the edit strength or generates animation
along the translation process.

4. Experiments

Datasets Our experiments are conducted on CelebA [16]
and RaFD [12]. CelebA contains 200K celebrity images,
each with 40 attribute labels. We deﬁne grouped attributes
based on these labels as shown in Table 1. Separation of
training and testing sets follows that of [16]. RaFD [12] is
a smaller dataset that contains 67 identities, each displays
8 emotional expressions, 3 eye locations, and 3 other at-
tributes about the identities. Similarly, we group these la-
bels into 3 higher-level attributes as shown in Table 2. In
our experiments, we use 65 identities for training and the
other two for testing. All images are center cropped, resized
to 128 × 128.

4.1. Analysis

Pivotal Parts in Training It is noted that the knowledge
guidance loss LKG and the homomorphic loss LI hom with
rigorous training play a key role in our model. Without ei-
ther of them, the training may converge poorly, leading to
unsatisfactory results. To illustrate this, we disable each
part and compare results with our ﬁnal one in Fig. 3. The
homomorphic loss Eq. (10) allows controlling the interpo-
lated attribute with control vector v. As shown in Fig. 3(f),
without this term, the generated image cannot transfer the
target attribute from the reference image.

When we disable the rigorous training, the interpolator
may produce small change to just deceive the discriminator,
leading to very mild update of results. This is shown in Fig.
3(d). Compared with our ﬁnal model in Fig. 3(c), the effect

2412

e
l
a
m
e
F

e
h
c
a
t
s
u
M

h
t
u
o
M

(a) Ref

(b) Original

(c) Final

(d) no RT

(e) no KG (f) no Hom

Figure 3. Effectiveness of rigorous training (RT), knowledge guid-
ance (KG) and homomorphic loss (Hom). Each row edits one at-
tribute.
(a) and (b) are the input reference and original images
respectively. (c) is our ﬁnal result. (d-f) are the results without
using one component each.

(a) Ref

(c) Gender

(b) Original

(e) Hair Color
Figure 4. Illustration of the role of control vector and exemplar. (a)
and (b) are the reference and original images respectively. (c)-(e)
are the results conditioned by different v. Rows 1 and 2 are of
different reference images, and thus the results vary accordingly.

(d) Smile

is not desirable. The knowledge guidance loss utilizes a
well-trained network as a teacher to guide training of the
encoder. As the teacher network is trained on many images,
it effectively extracts semantic features and seldom suffer
from overﬁtting. As shown in Fig. 3(e), without this term,
the encoder does not learn a smooth and ﬂat latent space.
This makes the generated image look unrealistic.

Pivotal Parts in Testing The control vector v and the ref-
erence exemplars are also important to apply our model to
image-to-image translation tasks. The control vector de-
termines which attribute to alter, while the exemplars de-
termine how attribute translation is instantiated. By jointly
using both of them, we ﬂexibly control the interpolation re-
sults. This is illustrated in Fig. 4. Each row shows how the
result changes with the same exemplar and yet a different
control vector. Each column shows how it changes with the
same control vector and yet a different exemplar. As shown
in Fig. 4(c)-(e), by setting v to one-hot vectors presenting
the gender, expression, and hair color respectively, we suc-
cessfully and effectively vary corresponding attributes. The

T
N
A
G
E
L
E

T
I
N
U
M

s
r
u
O

e
c
n
e
r
e
f
e
R

(a)

(b)

(c)

(d)

(e)

Figure 5. Multi-modal image-to-image translation on bang at-
tribute. (a) is the input image. (b-d) present four different output
(the 1st-3rd rows) and the corresponding exemplars (the 4th row).

exemplars also affect the ﬁnal results. For example, results
in the 1st and 2nd rows of Fig. 4 have quite different gender,
expression, and hair color change.

4.2. Comparison with Other Methods

One of the largest advantages of our model is the abil-
ity to handle multi-modal, multi-domain, and continuous
image-to-image translation. In this section, we provide both
qualitative and quantitative comparison with other methods.

4.2.1 Qualitative Evaluation

Multi-Modal Translation Using different exemplars, our
model can produce multiple outputs for image-to-image
translation. Fig. 5 compares our approach with two multi-
modal translation methods, i.e., MUNIT [7] and ELEGANT
[25]. For ELEGANT [25], the assumption of attribute dis-
entangled to different latent codes, again, could be hard to
achieve. As shown in Fig. 5(d), the image does not change
much. Compared with our method, MUNIT [7] does not
leverage information of multiple domains. When skin, hair
color and background are wrongly updated, as shown in Fig.
5, the result quality decreases.

Multi-Domain Translation Our model deals with multi-
domain image-to-image translation with Eq. (15). Figs. 6
and 7 compare our results with two related methods, i.e.,
StarGAN [4] and ELEGANT [25]. StarGAN [4] takes do-
main labels as input to generator, and produces target do-
main results. ELEGANT [25] divides the latent code into
different parts. Each part encodes information of one at-
tribute. Visually, our model accomplishes more natural

2413

T
N
A
G
E
L
E

N
A
G
r
a
t

S

s
r
u
O

T
N
A
G
E
L
E

N
A
G
r
a
t
S

s
r
u
O

t
e
l
e
c
a
F

s
r
u
O

(a) Original

(b) Angry

(c) Sad

(d) Happy

(e) Contemptuous

(f) Disgusted

(g) Fear

(h) Surprise

(i) Look Left

(j) Look Right

Figure 6. Multi-domain image-to-image translation on RaFD [12].

Original

Young

Mustache

Not Smile

Close Mouth

Black Hair

Bangs

Hairline

Female

Figure 7. Multi-domain image-to-image translation on CelebA [16].

Figure 8. Illustration of attribute interpolation and extrapolation. (a) is the result of interpolation. (b) further increases the edit strength to
perform exaggeration.

(a) Interpolation

(b) Extrapolation

– and with signiﬁcant changes – results than ELEGANT
[25] and StarGAN [4]. ELEGANT [25] assumes each at-
tribute can be well disentangled into different parts of the
latent code. This is not easily achieved because several at-
tributes are intrinsically correlated. As a result, the training
is not stable, causing sometimes noisy results. StarGAN [4]
works well, and yet still occasionally produces strong edit,

leading to visual artifacts.

Continuous Translation With the well learned latent
space, our model allows synthesizing images across differ-
ent domains. This has already been shown in Figs. 1 and
2. We also note that a good latent space should uncover the
structure of natural image manifold [2]. To an extreme, it
should even gain the capacity of extrapolation. This allows

2414

ELEGANT [25]

StarGAN [4]
Facelet [3]

Ours

Young Male
27%
20%
24%
28%
30%
25%
41%
18%

Smiling Black Hair Bangs Mustache Hairline Mouth Open

22%
23%
35%
43%

36%
47%
24%
49%

48%
42%
25%
43%

41%
47%
49%
48%

23%
21%
10%
33%

35%
34%
16%
45%

Total
31%
33%
27%
40%

Table 3. Turing Test on CelebA dataset. Each entry reports the percentage of taking the edited image as real. Higher is better.

Ours > StarGAN
Ours > Facelet

Ours > ELEGANT

Young Male
64%
51%
72%
76%
89%
88%

Smiling Bangs Black Hair Mustache Hairline Mouth Open

83%
67%
50%

50%
57%
59%

72%
83%
65%

49%
49%
51%

46%
90%
76%

74%
80%
71%

Total
61%
72%
69%

Table 4. A/B Test on CelebA dataset. Each entry reports the percentage that our results are preferred. Larger than 50% indicates that our
method is statically more preferred by the subjects.

exaggerating the difference between two domains. Fig. 8
compares the interpolation/extrapolation capacity between
our model and Facelet [3].

Facelet [3] is a feature interpolation approach whose
latent feature is deﬁned by a pretrained VGG network.
Similar to ours, it requires only discrete attribute labels
and has the capability to translate between different do-
mains smoothly. However, when applying very strong edit
strength, the result quality could drop.
In contrast, our
model works consistently well in both situations of inter-
polation and extrapolation. This indicates that the encoder
trained by Eq. (6) actually unfolds the natural image man-
ifold, leading to a ﬂat and smooth latent space that allows
interpolation and even extrapolation.

4.2.2 User Study

We also conduct user study on the Amazon Mechanical
Turk platform to compare our performance with others.
Turing Test and A/B Test are conducted.

Turing Test Each time subjects are presented with an arbi-
trary real image and the other that is edited by one method.
Both images are normalized to 128 × 128. Subjects are re-
quested to pick the real one. Table 3 shows the percentage
that an edited image is regarded as real. Note that differ-
ent attributes are counted separately, each includes 2,500
comparisons. Higher value means that human is harder to
distinguish between the real image and the edited one. The
ﬁnal statistics show that our model has 40% chance to fool
human eyes, which outperforms StarGAN [4] (33%), EL-
EGANT [25] (31%) and Facelet [3] (27%). We also note
for Male attribute, people are easier to identify the edited
image. The reason might be that our model only changes
gender traits on faces, while the hairstyle or clothes are
also highly correlated with gender. Therefore, subjects can
recognize the edited image based on the incompatibility of
faces and other cues.

A/B Test A/B Test refers to the pair-wise comparison of

Label ⇒ Facade

Facade ⇒ Label

Figure 9. A failure case. Our method does not perfectly handle the
situation when two domains are essentially different.

our model and another baseline model. Each time subjects
are given an original image and two edited ones (our method
vs. another), and are asked to pick one with higher edit qual-
ity. All three images are scaled to 128 × 128 and placed in
one row. Similar to the Turing Test, different attributes are
separately counted, and each one includes 2,500 compar-
isons. Table 4 presents the percentage that images gener-
ated by our method are chosen. Overall, our method out-
performs StarGAN [4], ELEGANT [25], and Facelet [3] by
61%, 72% and 69% respectively.

4.3. Limitations

Our model relies on the assumption that images of dif-
ferent domains can be embedded in a smooth and ﬂat space.
This is hardly achieved when these domains are very differ-
ent. Fig. 9 illustrates a case that performs translation be-
tween facade images and semantic labels. Our model does
not perform well in this case, since it is very difﬁcult to ﬁnd
intermediate regions in between.

5. Concluding Remarks

We have proposed a framework for unpaired image-to-
image translation focusing on generating natural and gradu-
ally changing intermediate results. Our method is based on
latent space interpolation, which intrinsically allows con-
tinuous translation. In addition, by learning a controllable
interpolator, we ﬂexibly select the interpolation path, which
alters the target attribute while keeping others almost in-
tact. We have also shown that our method can serve multi-
domain and multi-modal image-to-image translation.

2415

References

[1] Y. Bengio, G. Mesnil, Y. Dauphin, and S. Rifai. Better mix-

ing via deep representations. In ICML, 2013. 2, 3

[2] D. Berthelot, C. Raffel, A. Roy, and I. Goodfellow. Under-
standing and improving interpolation in autoencoders via an
adversarial regularizer. arXiv, 2018. 2, 7

[3] Y.-C. Chen, H. Lin, M. Shu, R. Li, X. Tao, Y. Ye, X. Shen,
In

and J. Jia. Facelet-bank for fast portrait manipulation.
CVPR, 2018. 2, 3, 8

[4] Y. Choi, M. Choi, and M. Kim. Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image
translation. In CVPR, 2018. 1, 3, 6, 7, 8

[5] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.
Courville. Improved training of wasserstein gans. In NIPS,
2017. 3

[21] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf.

Wasserstein auto-encoders. arXiv, 2017. 1, 2

[22] D. Ulyanov, A. Vedaldi, and V. Lempitsky. It takes (only)
In AAAI,

two: Adversarial generator-encoder networks.
2018. 1, 2

[23] P. Upchurch, J. R. Gardner, G. Pleiss, R. Pless, N. Snavely,
K. Bala, and K. Q. Weinberger. Deep feature interpolation
for image content changes. In CVPR, 2017. 2, 3

[24] T. White. Sampling generative networks. arXiv, 2016. 1
[25] T. Xiao, J. Hong, and J. Ma. Elegant: Exchanging latent
encodings with gan for transferring multiple face attributes.
In ECCV, 2018. 6, 7, 8

[26] Z. Yi, H. R. Zhang, P. Tan, and M. Gong. Dualgan: Unsuper-
vised dual learning for image-to-image translation. In ICCV,
2017. 3

[27] B. Zhao, B. Chang, Z. Jie, and L. Sigal. Modular generative

[6] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge

adversarial networks. In ECCV, 2018. 3

[28] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In ICCV, 2017. 1, 3

in a neural network. arXiv, 2015. 3

[7] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal
unsupervised image-to-image translation. In ECCV, 2018. 3,
5, 6

[8] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In ECCV, 2016.
3

[9] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to
discover cross-domain relations with generative adversarial
networks. In ICML, 2017. 1, 3

[10] D. P. Kingma and P. Dhariwal. Glow: Generative ﬂow with

invertible 1x1 convolutions. arXiv, 2018. 2

[11] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. In ICLR, 2014. 1

[12] O. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus, S. T.
Hawk, and A. Van Knippenberg. Presentation and validation
of the radboud faces database. Cognition and emotion, 2010.
3, 5, 7

[13] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and
O. Winther. Autoencoding beyond pixels using a learned
similarity metric. arXiv, 2015. 2

[14] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. Singh, and M.-H.
Yang. Diverse image-to-image translation via disentangled
representations. In ECCV, 2018. 1, 3

[15] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-

image translation networks. In NIPS, 2017. 3

[16] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face

attributes in the wild. In ICCV, 2015. 5, 7

[17] A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu, and
F. Moreno-Noguer. Ganimation: Anatomically-aware facial
animation from a single image. In ECCV, 2018. 3

[18] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y. Bengio. Fitnets: Hints for thin deep nets. ICLR, 2014.
3

[19] K. Shoemake. Animating rotation with quaternion curves. In

SIGGRAPH, 1985. 4

[20] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv, 2014. 3,
4

2416

