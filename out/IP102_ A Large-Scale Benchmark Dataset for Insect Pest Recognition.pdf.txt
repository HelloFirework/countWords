IP102: A Large-Scale Benchmark Dataset for Insect Pest Recognition

Xiaoping Wu1, Chi Zhan1, Yu-Kun Lai2, Ming-Ming Cheng1, Jufeng Yang1 ∗

1College of Computer Science, Nankai University, Tianjin, China

2School of Computer Science and Informatics, Cardiff University, Cardiff, UK

{xpwu95, chizhan nt}@163.com, LaiY4@cardiff.ac.uk, {cmm, yangjufeng}@nankai.edu.cn

Abstract

Insect pests are one of the main factors affecting agri-
cultural product yield. Accurate recognition of insect pests
facilitates timely preventive measures to avoid economic
losses. However, the existing datasets for the visual clas-
siﬁcation task mainly focus on common objects, e.g., ﬂow-
ers and dogs. This limits the application of powerful deep
learning technology on speciﬁc domains like the agricul-
tural ﬁeld. In this paper, we collect a large-scale dataset
named IP102 for insect pest recognition. Speciﬁcally, it
contains more than 75, 000 images belonging to 102 cat-
egories, which exhibit a natural long-tailed distribution. In
addition, we annotate about 19, 000 images with bounding
boxes for object detection. The IP102 has a hierarchical
taxonomy and the insect pests which mainly affect one spe-
ciﬁc agricultural product are grouped into the same upper-
level category. Furthermore, we perform several baseline
experiments on the IP102 dataset, including handcrafted
and deep feature based classiﬁcation methods. Experimen-
tal results show that this dataset has the challenges of inter-
and intra- class variance and data imbalance. We believe
our IP102 will facilitate future research on practical insect
pest control, ﬁne-grained visual classiﬁcation, and imbal-
anced learning ﬁelds. We make the dataset and pre-trained
models publicly available at https://github.com/
xpwu95/IP102.

1. Introduction

Insect pests are known to be a major cause of damage
to the commercially important agricultural crops [8]. Cate-
gorization of insect pests plays a crucial role in agricultural
pest forecasting, which is vital for food security and stable
agricultural economy [10]. Due to the vast number of pest
species and the subtle differences among species, insect pest
recognition heavily relies on the professional knowledge of
agricultural experts [1], meaning it is expensive and time-

∗Corresponding author

(cid:53)(cid:76)(cid:70)(cid:72)(cid:3)(cid:79)(cid:72)(cid:68)(cid:73)(cid:3)(cid:70)(cid:68)(cid:87)(cid:72)(cid:85)(cid:83)(cid:76)(cid:79)(cid:79)(cid:68)(cid:85)

(cid:53)(cid:76)(cid:70)(cid:72)(cid:3)(cid:79)(cid:72)(cid:68)(cid:73)(cid:3)(cid:85)(cid:82)(cid:79)(cid:79)(cid:72)(cid:85)

(cid:51)(cid:68)(cid:71)(cid:71)(cid:92)(cid:3)(cid:86)(cid:87)(cid:72)(cid:80)(cid:3)(cid:80)(cid:68)(cid:74)(cid:74)(cid:82)(cid:87)

(cid:38)(cid:76)(cid:70)(cid:68)(cid:71)(cid:72)(cid:79)(cid:79)(cid:76)(cid:71)(cid:68)(cid:72)

(cid:59)(cid:92)(cid:79)(cid:82)(cid:87)(cid:85)(cid:72)(cid:70)(cid:75)(cid:88)(cid:86)

(cid:36)(cid:83)(cid:82)(cid:79)(cid:92)(cid:74)(cid:88)(cid:86)(cid:3)(cid:79)(cid:88)(cid:70)(cid:82)(cid:85)(cid:88)(cid:80)

(cid:55)(cid:68)(cid:85)(cid:81)(cid:76)(cid:86)(cid:75)(cid:72)(cid:71)(cid:3)(cid:83)(cid:79)(cid:68)(cid:81)(cid:87)(cid:3)(cid:69)(cid:88)(cid:74)

(cid:39)(cid:68)(cid:70)(cid:88)(cid:86)(cid:3)(cid:71)(cid:82)(cid:85)(cid:86)(cid:68)(cid:79)(cid:76)(cid:86)

(cid:47)(cid:92)(cid:70)(cid:82)(cid:85)(cid:80)(cid:68)(cid:3)(cid:71)(cid:72)(cid:79)(cid:76)(cid:70)(cid:68)(cid:87)(cid:88)(cid:79)(cid:68)

(cid:51)(cid:85)(cid:82)(cid:71)(cid:72)(cid:81)(cid:76)(cid:68)(cid:3)(cid:79)(cid:76)(cid:87)(cid:88)(cid:85)(cid:68)

(cid:38)(cid:82)(cid:85)(cid:81)(cid:3)(cid:69)(cid:82)(cid:85)(cid:72)(cid:85)

(cid:36)(cid:80)(cid:83)(cid:72)(cid:79)(cid:82)(cid:83)(cid:75)(cid:68)(cid:74)(cid:68)

Figure 1. Example images of the IP102 dataset. Each image be-
longs to a different species of insect pests.

consuming. With the development of machine learning and
computer vision techniques, automated insect pest recogni-
tion attracts increasing research attention.

Most of the previous works on insect pest recognition
can be described by a traditional machine learning classiﬁ-
cation framework, which is composed of two modules: (1)
feature representation of the insect pest images: a series of
handcrafted features including GIST [30], SIFT [25], and
SURF [3] etc. are adopted to represent the whole image.
(2) machine learning classiﬁers including the support vec-
tor machine [4] and the k-nearest neighbor (KNN) classiﬁer.
These feature-based methods rely on the careful choice of
features. If incomplete or erroneous features are extracted
from insect pest images, the subsequent classiﬁer may fail
to distinguish similar pest species.

Recently, deep learning enables robust feature learn-
ing and achieves state-of-the-art performance on a vari-
ety of image classiﬁcation tasks.
It is well known that
the ImageNet Large Scale Visual Recognition Challenge
(ILSVRC) [6] marks the beginning of the rapid develop-
ment of deep learning, demonstrating that large-scale image
datasets play a key role in driving deep learning progress.

8787

However, so far, deep learning methods on insect pest
recognition are restricted to small datasets, which only con-
tain very few samples or pest species. Meanwhile, most of
the existing insect pest images in public datasets are col-
lected in controlled lab environments, which cannot well
satisfy the requirement of insect pest recognition in the real
ﬁeld environment. Moreover, insect pest recognition has its
own characteristics different from the existing object or an-
imal classiﬁcation work [41, 16, 27]. Speciﬁcally, different
insect pest species may have high appearance similarity and
the same species may be in different forms including egg,
larva, pupa and adult, i.e., signiﬁcant intra-class difference
and large inter-species similarity.

To advance the insect pest recognition research in com-
puter vision, we introduce the IP102, a new large-scale
insect pest dataset in this work. First, we collect more
than 300,000 images using common image search engines,
which are weakly labeled by the queries. Next, each image
is checked by volunteers to make sure it is relevant to insect
pests. Agricultural experts then further check and annotate
the images with the category label or bounding boxes. The
detailed dataset building process is introduced in the follow-
ing section. Finally, our IP102 dataset covers 102 species of
common crop insect pests with over 75, 000 images. Com-
pared with the currently available pest datasets in the lit-
erature, the IP102 has a much larger scale, which beneﬁts
methods based on deep learning. Our dataset also involves
several other features. First, images belonging to the same
category may capture different growth forms of the same
type of insect pests. Such diversity is unique to the pest
datasets but ignored by previous datasets. Besides, the class
imbalance is a property of insect pests as some species are
much more likely to be observed. Our dataset satisﬁes the
features of imbalanced data distribution, just like in the real
world. Fig. 1 shows some examples of the pest dataset.

In order to validate the application value of our proposed
dataset, we also report extensive performance for state-of-
the-art classiﬁcation and object detection algorithms. The
results indicate that the dataset is challenging and creates
new opportunities for research.

Our contributions are summarized as follows:

• To our knowledge, we build the largest scale dataset
for insect pest recognition, including insect pest clas-
siﬁcation and detection. The whole dataset is made
available to the research community.

• We conduct extensive experiments on the dataset using
CNNs and handcrafted features and establish the per-
formance as the baseline for future research. We also
test several state-of-the-art detection models on the de-
tection split of the IP102. We hope this can advance the
research on insect pest recognition.

2. Related Work

In this section, we introduce the related work of insect

pest recognition methods and review the existing datasets.

2.1. Insect Pest Recognition

Early insect pest recognition is helpful for pest control
and improving the quality and yield of agricultural prod-
ucts [35]. In recent years, many computer-aided insect pest
recognition systems [32, 2] are presented in the vision com-
munity. We group them into two types: the handcrafted and
deep feature based methods.

Handcrafted features such as SIFT [25], HOG [5] etc.
perform well on the low-level feature representations (e.g.,
color, edge, and texture).
In the early years, handcrafted
feature based methods are the primary solutions for insect
pest recognition. Samanta et al. [35] utilize correlation-
based feature selection and artiﬁcial neural networks to di-
agnose 8 tea insect pests based on a dataset containing 609
samples. In [28, 32], an SVM classiﬁer is applied to identify
whiteﬂies, aphids, and thrips in leaf images. These methods
tend to extract several typical handcrafted features to repre-
sent the insect pest and then evaluate on the small datasets
with few categories. However, there are a large number of
insect pest categories in real life. It is inefﬁcient and time-
consuming to design feature extractors for recognizing di-
verse insect pests. In addition, handcrafted features lack the
representation ability for high-level semantic information.

Recently, deep learning technology widely attracts the
attention of researchers [18, 34, 24]. Deep convolu-
tional neural networks (CNNs) such as GoogleNet [39] and
ResNet [13] show excellent performance in the image clas-
siﬁcation task. There are also several works [23, 2] that suc-
cessfully apply CNNs to solve the problem of insect pest
recognition. Liu et al. [23] classify paddy ﬁeld pests via
training a deep CNN, and their dataset comprises approxi-
mately 5, 000 training samples for 12 classes. Alfarisy et
al. [2] also use the CaffeNet [14] for paddy pest classi-
ﬁcation.
In addition, [7] achieves comparable results to
the deep CNN (i.e., VGGNet [36]) based on bio-inspired
methods. Yet the evaluated dataset is small containing
just 563 samples. Overall, these deep feature based works
lack enough samples for optimizing the massive hyper-
parameters of CNNs. In order to promote further scientiﬁc
research and practical applications, we should address the
issues of limited categories and samples. Hence we collect
the large-scale IP102 dataset, which contains 102 categories
of insect pests with 75, 222 samples.

2.2. Related Datasets

Some of the small datasets related to insect pest recogni-
tion are released, such as [35, 42, 7]. Most of them typically
contain fewer than 1, 000 samples. For example, [40] col-
lects a dataset only consists of 200 samples in 20 classes

8788

for paddy ﬁeld insect pest classiﬁcation. Subsequently, sev-
eral larger datasets are presented. Xie et al. [44] present a
dataset which contains 1, 440 samples and 24 common in-
sect pests of ﬁeld crops. Yet on average it only has 60 sam-
ples per class, which is also hard to train a CNN. To tackle
this problem, [23, 43, 2] propose some datasets which con-
tain more than 4, 500 samples in total and 100 samples for
each class. However, only the dataset of [43] is available
so far. Besides, the background, object pose of the same
class of pest images in this dataset [43] are highly similar,
making it difﬁcult to cope with the complexities of real-
life scenes. Table 2 illustrates the details of these related
datasets. In contrast, our purposed IP102 covers 102 com-
mon insect pest species in practical applications and is built
in the wild. Besides, the IP102 dataset has 75, 222 images
and an average size of 737 samples per class.

3. Our Insect Pest Dataset

3.1. Data Collection & Annotation

We collect and annotate the IP102 dataset with following
four stages: 1) taxonomic system establishment, 2) image
collection, 3) preliminary data ﬁltering, and 4) professional
data annotation.

3.1.1 Taxonomic System Establishment

We establish a hierarchical taxonomic system for the IP102
dataset. We invite several agricultural experts and discuss
the common categories of insect pests which exist in daily
life. There are 102 classes ﬁnally obtained and they present
a hierarchical structure as shown in Fig 4. Each insect pest
is assigned an upper-level class (denoted as super-class in
the following) based on the crop that suffers from the pest.
In other words, each insect pest is a subordinate class (de-
noted as sub-class in the following) of a certain super-class.
For example, the pest of paddy stem maggot spoils the crop
of rice, and the rice belongs to the ﬁeld crop. Hence, in
the taxonomic system of the IP102, the sub-class of paddy
stem maggot has the super-class of rice and ﬁeld crop. The
detailed structure of the IP102 dataset is introduced in the
following dataset structure subsection 3.3.

3.1.2

Image Collection

We utilize the Internet as the primary source to collect im-
ages, which is widely used to build datasets such as the Im-
ageNet [6] and the Microsoft COCO [21]. The ﬁrst collec-
tion step relies on common image search engines, including
Google, Flickr, and Bing etc. We use the English name
and corresponding synonyms of each sub-class as the query
keywords. Only top-2, 000 results are kept for each key-
word. Then we search from several professional agriculture
and insect science websites. In addition to the image form,

(a1)

(a2)

(a3)

(a4)

(b1)

(b2)

(b3)

(b4)

Figure 2. Different forms of insect pest images. The red dashed
boxes denote different forms of pests, containing (a1) egg, (a2)
larva, (a3) pupa, and (a4) adult, which belong to the same sub-
class. The images surrounded by blue dashed box are dropped
because there are no or more than one insect pest category.

we also collect video clips which contain the content of in-
sect pests. From the video clips, we capture images at 5
frames per second. As a consequence, we collect more than
300, 000 candidate images for the IP102 dataset.

3.1.3 Preliminary Data Filtering

We organize 6 volunteers to manually ﬁlter the candidate
images. Before data ﬁltering, they receive three parts of
training content, i.e., 1) the common sense of insect pests
from agricultural experts, 2) the taxonomic system of the
IP102, and 3) different forms of insect pests. For exam-
ple, Fig. 2 shows four forms of insect pests, containing egg,
larva, pupa, and adult. Even they are at different stages of
the life cycle, yet all of them can cause varying degrees of
damage to agricultural products. At the process of prelimi-
nary data ﬁltering, volunteers delete the images which con-
tain none or more than one insect pest category as illustrated
in Fig 2. Then, we convert the format of ﬁltered images to
JPEG and delete the images which are repeated or damaged.
Finally, we have about 120, 000 images with weak labels of
query keywords. The label of super-class is assigned ac-
cording to the taxonomic system of the IP102 dataset.

3.1.4 Professional Data Annotation

Data annotation by agricultural experts is the most impor-
tant procedure. In the taxonomic system of the IP102, there
are 8 kinds of crops damaged by insect pests. For each crop,
we invite a corresponding agricultural expert who studies it
primarily. Therefore, in total, we invite 8 agricultural ex-
perts to annotate the images ﬁltered in the previous proce-
dure. We build a Question/Answer (Q/A) system for conve-
nient annotation. For the image shown on the interface of
the Q/A system, experts need to answer which category the
image belongs. The professional data annotation comprises

8789

5000

4000

3000

2000

r
e
b
m
u
N
 
s
e
c
n
a
t
s
n
I

(a)

5000

4000

3000

2000

r
e
b
m
u
N
 
s
e
c
n
a
t
s
n
I

(b)

5000

4000

3000

2000

r
e
b
m
u
N
 
s
e
c
n
a
t
s
n
I

(c)

1000

0

17

3
1

9
1

5
2

1
3

7
3

3
4

9
4

5
5

1
6

7
6

3
7

9
7

5
8

1
9

Class Index

1000

0

(cid:23)

7
9

(cid:21)
0
1

(cid:19)
(cid:20)

(cid:19)
(cid:21)

(cid:25)
(cid:21)

5
(cid:22)

(cid:22)
(cid:23)

(cid:19)
(cid:24)

(cid:19)
(cid:25)

(cid:22)
(cid:26)

(cid:25)(cid:22)
(cid:27)

(cid:21)
(cid:22)

(cid:24)
(cid:24)

(cid:26)
(cid:25)

(cid:27)
(cid:26)

(cid:27)
(cid:27)

7
9

Class Index

1000

(cid:21)
0
1

0

(cid:24)
1

(cid:24)
(cid:23)

(cid:19)
(cid:25)

(cid:25)
(cid:20)

(cid:19)
(cid:22)

(cid:28)
3

(cid:23)
(cid:27)

(cid:25)
(cid:25)

(cid:20)
4

(cid:25)
(cid:27)

1
(cid:20)

(cid:23)
(cid:26)

Class Index

(cid:20)
(cid:19)
(cid:20)

(cid:21)
(cid:25)

(cid:27)
(cid:26)

(cid:25)
9

(cid:20)
(cid:24)

(cid:28)
(cid:28)

Figure 3. Sample number distribution of the IP102 dataset in different levels. The red calibration tails split 2 super-classes in the sub-ﬁgure
(b) and 8 super-classes in the sub-ﬁgure (c), respectively.

(cid:68)(cid:68)

(cid:48)(cid:68)(cid:81)(cid:74)(cid:82)

(cid:53)(cid:76)(cid:70)(cid:72)

(cid:68)(cid:68)(cid:68)(cid:68)(cid:68)

(cid:44)(cid:51)(cid:20)(cid:19)(cid:21)

(cid:38)(cid:76)(cid:87)(cid:85)(cid:88)

(cid:40)(cid:38)

(cid:57)(cid:76)(cid:87)(cid:76)(cid:86)

(cid:36)(cid:79)(cid:73)(cid:68)(cid:79)(cid:73)(cid:68)

(cid:38)(cid:82)(cid:85)(cid:81)

(cid:58)(cid:75)(cid:72)(cid:68)(cid:87)

(cid:41)(cid:38)

(cid:37)(cid:72)(cid:72)(cid:87)

Figure 4. Taxonomy of the IP102 dataset. The ‘FC’ and ‘EC’ de-
note the ﬁeld and economic crops, respectively. On the sub-class
level, only 35 classes are shown. The full list of each sub-class
can be found in the released IP102 dataset.

Table 1. Training/validation/testing (denoted as Train/Val/Test) set
split and imbalance ratio (IR) of the IP102 dataset on different
class levels. The ‘Class’ indicates the sub-class number of the
corresponding super-class. The ‘FC’ and ‘EC’ denote the ﬁeld
and economic crops, respectively.

Super-Class

Class

Train

Test

IR

Rice

Corn

Wheat

Beet

Alfalfa

Vitis

Citrus

Mango

C
F

C
E

2 FC

0
1
P
I

EC

14

13

9

8

13

16

19

10

57

45

Val

843

5,043

8,404

1,399

2,048

2,649

340

441

6,230

1,037

10,525

1,752

4,356

5,840

725

971

2,531

4,212

1,030

1,330

3,123

5,274

2,192

2,927

24,602

4,098

12,341

20,721

3,448

10,393

6.4

27.9

5.2

15.4

10.7

74.8

17.6

61.7

39.4

80.8

IP102

102

45,095

7,508

22,619

80.8

of independent and synergistic annotations. At the phase
of independent annotation. Each agricultural expert is re-
sponsible for annotating only one kind of crop super-class.
For example, for the expert who studies the rice primarily,
he needs to annotate these images with the super-class of
In this case, the expert has 15 options for category
rice.
selection in the Q/A system. These options consist of 14 in-
sect pest classes which mainly damage the rice crop and an
“other class” option. The “other class” means that the im-
age does not belong to the 14 insect pest classes of concern
or contains none or more than one insect pest class. The
next phase is the synergistic annotation. There are ﬁxed 103
(i.e., 102 insect pest classes plus 1 “other class”) category
options in the Q/A system for each expert. Besides, these
8 experts synergistically annotate the “other class” images
from the last independent annotation phase. For an image,
each expert needs to annotate it, i.e., choose one of the 103
options. The ﬁnal annotation results follow a strict criterion:

one image belongs to a category only when it is agreed by
more than 5 experts, otherwise it will be deleted.

The detection of pest locations in images is also very im-
portant. It can help agricultural experts or users better ﬁnd
the speciﬁc location of pests (especially those that are not
obvious in the image). In addition, the real-world scenario
makes it complex to recognize the insect pests. A cluttered
background can misguide the classiﬁer when the target pest
is not salient, and the existence of multiple samples of pests
in the image demands respective recognition. The pest con-
trol measures in the scene need accurate pest location and
category of each pest. Therefore, effective pest insect de-
tection can alleviate the complexity of realistic scenario by
sample-aware recognition with spatial information. It can
also boost classiﬁcation performance by removing irrele-
vant background features. Considering the difﬁculty and
cost of labeling the bounding box, we randomly select part
of images from each class to form a subset for the object de-

8790

Table 2. Comparison with existing datasets related to insect pests.
The ‘Class’ denotes the class number. The ‘Avail’ indicates if the
dataset is available. The ‘Y’ and ‘N’ denote ‘yes’ and ‘no’, respec-
tively. The ‘Avg’ denotes average numbers of samples per class.

Dataset

Year Class Avail Sample Avg

Samanta et al. [35]

Wang et al. [42]

2012

2012

Venugoban et al. [40]

2014

Xie et al. [44]

Liu et al. [23]

Xie et al. [43]

Deng et al. [7]

Alfarisy et al. [2]

2015

2016

2018

2018

2018

8

9

20

24

12

40

10

13

IP102

2019

102

N

Y

N

Y

N

Y

Y

N

Y

609

225

200

1,440

5,136

4,500

563

76

25

10

60

428

113

56

4,511

347

75,222

737

tection task. The experts label the bounding boxes of insect
pests following the format of Pascal VOC [9].

3.2. Dataset Split

The IP102 dataset contains 75, 222 images and 102
classes of insect pests, yet the smallest category only has 71
samples. For more reliable test results on the IP102, there
should be enough samples of each category on the testing
set. Hence we follow a roughly 6 : 1 : 3 split. The train-
ing, validation, and testing sets are split at sub-class level.
Speciﬁcally, the IP102 is split into 45, 095 training, 7, 508
validation, and 22, 619 testing images for classiﬁcation task.
Detailed splits at different levels are shown in Table 1. Cor-
responding image lists for each set are released in the IP102
dataset. For the task of object detection, there are totally
18, 983 annotated images. We split those images containing
bounding box annotations into 15, 178 and 3, 798 images as
training and testing sets, respectively.

3.3. Dataset Structure

The IP102 dataset has a hierarchical structure and Fig. 4
shows its detailed taxonomy. Each sub-class is assigned
with a super-class according to the crop that the insect
pest class mainly damages. For example, the sub-class of
tetranychus cinnbarinus (TC) has the super-class of citrus.
The 8 crops (e.g., rice, corn, and wheat) are further grouped
into two super-classes (i.e., ﬁeld crop and economic crop).
For example, the citrus belongs to the super-class of eco-
nomic crop. In addition, Table 1 shows the number distri-
butions of sub-classes in different super-class levels.

pared to the largest datasets [23, 43, 2], our dataset contains
over 14 times more samples. With respect to the class di-
versity, the largest and least datasets only have 40 and 8
classes, respectively. However, there are a large number
of insect pests in real life and our IP102 comprises of 102
classes. Considering the average number of samples per
class, the IP102 has at least 309 more images than those
compared datasets. In addition to the statistic distinction,
only half of the datasets is available and only [43] has a rel-
atively large scale. Due to these limitations, most existing
datasets (e.g., [40, 44, 7]) related to insect pests are hard to
be applied to practical applications.

3.5. Diversity and Difﬁculty

Insect pests at different stages of life cycle can damage
agricultural products in different degrees. So we retain im-
ages containing all of these during data collection and an-
notation. Figs. 2(a1-a4) show different forms of pests in the
IP102, containing egg, larva, pupa, and adult. For the classi-
ﬁcation model, classifying them to the same category is dif-
ﬁcult because it is hard to extract discriminant features. In
addition to the biological diversity, the imbalanced data dis-
tribution also cannot be ignored. As illustrated in Fig. 3, the
three sub-ﬁgures demonstrate the imbalanced distribution
of the proposed dataset in different levels, where (a), (b),
and (c) show the instance number distributions of the 102
sub-classes, 2 super-classes, and 8 super-classes, respec-
tively. Speciﬁcally, based on the hierarchical label system
of the IP102 dataset, the 102 sub-classes are divided into 8
super-classes according to the crop that the insect pest class
mainly damages, e.g., rice and corn, and 2 super-classes ac-
cording to the type of damaged crops, i.e., ﬁeld crop and
economic crop. The imbalanced distribution across differ-
ent levels brings challenges to the imbalanced learning ﬁeld
and the use of hierarchical labels. Table 1 also shows that
the dataset has high imbalance ratio (IR) (i.e., higher than 9
IR [12]) at most super-class level of the IP102. Imbalanced
data can lead to the classiﬁcation model learning a biased
result to those classes with relative more training samples.

4. Experimental Evaluation

The choice of features usually plays a signiﬁcant role in
image recognition. To comprehensively evaluate the IP102
dataset, we ﬁrst evaluate the classiﬁcation performance uti-
lizing handcrafted and deep features, respectively. Subse-
quently, we evaluate several object detection frameworks on
the subset of IP102.

4.1. Experiment Settings

3.4. Comparison with Other Datasets

In Table 2, we compare the IP102 with several existing
datasets related to the task of insect pest recognition. Com-

The SVM classiﬁer is trained with the one-vs-rest
scheme by employing LIBLINEAR [11]. The near neigh-
bor number of the KNN classiﬁer is set to 5. When train-
ing the deep networks, we ﬁne-tune all layers via a Mini-

8791

Table 3. Classiﬁcation performance of the SVM and KNN classiﬁers under different evaluation metrics on the IP102 dataset. The
representations are divided into handcrafted and deep features, respectively.

# Methods

SVM

KNN

e CH
r
u
t
a
e
F
d
e
t
f
a
r
c
d
n
a
H

Gabor [29]

GIST [30]

SIFT [25]

SURF [3]

LCH [38]

e Alexnet [17]

r
u
t
a
e
F
p
e
e
D

GoogleNet [39]

VGGNet [36]

ResNet [13]

Pre

9.7

8.5

12.2

25.1

28.2

7.2

41.5

45.8

43.4

43.6

Rec

3.2

3.9

3.8

6.3

7.3

5.0

16.4

25.8

37.6

39.1

F1

2.5

3.6

3.8

6.8

8.3

4.7

21.0

30.4

39.1

40.6

GM MAUC

Acc

Pre

Rec

F1

GM MAUC

Acc

0.3

0.5

0.6

1.0

1.5

0.9

9.3

16.0

28.3

31.0

12.0

12.1

12.1

19.9

21.2

11.1

32.5

41.9

48.1

48.7

12.9

14.2

13.1

18.1

19.5

13.1

28.3

40.5

48.7

49.5

18.2

22.0

19.1

19.4

21.3

21.6

36.7

36.8

41.9

43.7

14.2

14.9

15.1

10.3

11.5

14.7

32.4

31.7

37.8

39.1

15.0

16.5

15.4

12.1

13.4

16.1

33.5

33.0

39.0

40.5

8.3

9.1

9.2

5.6

7.1

8.3

23.9

23.3

29.8

30.7

16.8

20.0

19.2

15.9

17.5

19.0

41.0

41.6

47.6

48.2

15.8

19.2

18.2

13.1

14.7

16.8

40.7

40.7

47.1

49.4

batch Stochastic Gradient Descent optimizer with the mini-
batch size of 64. The learning rate is initialized as 0.01
and drops by a factor of 0.1 every 40 epochs. The weight
decay and momentum parameters are set to 0.0005 and
0.9, respectively. To avoid overﬁtting, we also employ the
dropout [37], set to 0.3. We keep the basic architectures of
these deep models unchanged, and only change the last fully
connected layer from 1, 000 to the class number we aim to
classify. The size of input images is ﬁxed to 224 × 224.
The deep feature based experiments are implemented using
PyTorch [31] and performed on an NVIDIA Titan X GPU
with 12 GB onboard memory.

4.2. Evaluation Metrics

The IP102 has an imbalanced class distribution. We em-
ploy several comprehensively metrics for the classiﬁcation
task, including precision, recall, F-measure, G-mean, and
MAUC. The precision (denoted as Pre) describes the ability
of the classiﬁer not to label a negative sample as positive.
The recall (denoted as Rec) indicates the ability to ﬁnd all
the positive samples for one speciﬁc class. The F1 combines
the precision and recall as a trade-off. The G-mean (denoted
as GM) evaluates class-wise sensitivity and indicates the
balanced classiﬁcation performances on the majority and
minority classes. The micro average scheme MAUC [15] is
deﬁned as the area under the curve metric. As for the task
of object detection, we utilize the Average Precision (AP)
(IoU=[.50:.05:.95]), AP.50 (IoU=.50), and AP.75 (IoU=.75)
as performance evaluation metrics. The IoU is deﬁned as
the intersection over the union between detected box and
ground-truth. The larger the threshold of IoU, the greater
the difﬁculty of detection.

Table 4. Classiﬁcation performance of different deep models. The
‘st’ denotes training from scratch.

Method

F1

GM Acc

F1st

GMst Accst

AlexNet [17]

34.1

GoogleNet [39]

32.7

VGGNet [36]

ResNet [13]

38.7

40.1

27.0

21.3

30.9

31.5

41.8

43.5

48.2

49.4

29.1

27.0

33.3

29.6

22.2

11.3

25.5

22.2

35.3

40.2

41.4

35.7

4.3. Classiﬁcation Results on Handcrafted Features

We extract several handcrafted texture and color fea-
tures from the IP102 dataset, including Color Histogram
(CH), LCH [38], Gabor [29], GIST [30], SIFT [25], and
SURF [3]. Then, we utilize the SVM and KNN classiﬁers
to build the baseline methods on handcrafted features.

Table 3 shows the classiﬁcation performance of hand-
crafted features. We can see that color (CH) features per-
form poorly on most evaluation metrics compared to tex-
ture (Gabor [29]) features. This indicates that texture fea-
tures play a more important role when insect pests appear
in the wild. As shown in Fig. 1, large area of monotonous
background color makes it difﬁcult to discriminate insect
pests by color features. The best handcrafted feature barely
achieves about 19.5% accuracy with SURF [3] features and
SVM classiﬁer. The main reason is that these handcrafted
features can neither catch the comprehensive information
relating to insect pests nor eliminate the noise in pests im-
ages in the real environment. Furthermore, plenty of dif-
ferent insect pests share similar appearance but traditional
handcrafted features are not enough to capture subtle dif-
ferences. The large accuracy gap between the IP102 and

8792

previous small-scale dataset [19, 44] also demonstrates that
the IP102 exhibits high recognition difﬁculty.

4.4. Classiﬁcation Results on Deep Features

Deep features are proved to be effective in image recog-
nition.
In this section, we evaluate the performance of
state-of-the-art deep convolutional networks on the IP102
dataset, including AlexNet [17], GoogleNet [39], VGGNet-
16 (VGGNet) [36], and ResNet-50 (ResNet) [13].

All the networks are pre-trained on the ImageNet [6] and
then ﬁne-tuned on the IP102 dataset. We extract deep fea-
tures from the CNNs by removing the last layer in model ar-
chitectures. Subsequently, we utilize these deep features to
train the SVM and KNN classiﬁers. Table 3 shows the clas-
siﬁcation performance of deep features. The ResNet per-
forms best compared to the other three models on most of
the metrics. So it can make a better feature representation
of the IP102, even if its feature dimension (2, 048) is less
than VGGNet (4, 096).
In addition, deep feature outper-
forms handcrafted feature based methods in general. This
demonstrates the feature learning ability of deep models.
Then, we can further see that the KNN performs better over-
all versus the SVM classiﬁer. Especially with the AlexNet
features, the KNN results outperform the SVM on most of
It has 40.7% accuracy with the KNN while
the metrics.
only has 28.3% accuracy with the SVM classiﬁer. More-
over, the SVM achieves the poor performance of 16.4 recall
and 9.3% G-mean. This illustrates that the deep features
from AlexNet have low sensitivity.

Table 4 shows the softmax classiﬁcation performance
of deep models on different evaluation metrics. Note that
ResNet achieves the best results on all metrics. Yet the big
gap between 49.4% accuracy and 31.5% G-mean indicates
the high imbalance of our IP102 dataset. The classiﬁcation
models bias to those classes with a large number of samples.
In addition, the highest accuracy of 49.4% demonstrates the
challenges of IP102. We also train the deep models from
scratch, i.e., without pre-training on the ImageNet. The re-
sults are much worse compared to ﬁne-tuning pre-trained
models, due to the fact that these deep models have a huge
number of hyper-parameters and can easily overﬁt on the
classes with fewer training samples.

4.5. Detection Results

We evaluate several state-of-the-art object detection
methods on the IP102 dataset. Two stage based methods
including Faster R-CNN (FRCN) [34] and FPN [20] (uti-
lizing FRCN as the backbone detection framework). They
detect objects through ﬁrst sliding the window on a fea-
ture map to scan potential objects and then classifying them
and regressing corresponding box coordinates. One stage
based methods including SSD300 [22], ReﬁneDet [45], and
YOLOv3 [33] directly regress the category and position

Table 5. Classiﬁcation performance with different hierarchical
labels. Each row shows the results of the sub-classes of corre-
sponding crop.

Super-Class

Pre

Rec

F1

GM MAUC Acc

Rice

Corn

Wheat

Beet

Alfalfa

Vitis

Citrus

Mango

C
F

C
E

31.5

55.1

37.5

51.6

42.1

78.2

69.6

75.8

30.0

54.4

34.5

49.5

41.2

76.3

68.5

74.7

30.4

28.3

54.6

50.3

35.5

29.3

50.4

45.3

41.4

38.1

77.1

74.9

68.8

65.2

75.1

72.3

32.3

61.9

52.1

62.0

46.2

86.8

76.6

89.0

32.1

62.2

53.0

62.2

46.4

86.7

76.6

89.0

Table 6. Average precision performance of object detection meth-
ods under different IoU thresholds.

Method

Backbone

AP

AP.50

AP.75

FRCNN [34]

VGG-16

FPN [20]

ResNet-50

SSD300 [22]

VGG-16

ReﬁneDet [45] VGG-16

21.05

28.10

21.49

22.84

YOLOv3 [33]

DarkNet-53

25.67

47.87

54.93

47.21

49.01

50.64

15.23

23.30

16.57

16.82

21.79

for each object. Detection performance in Table 6 shows
the superiority of region proposal based two-stage detec-
tors (FPN) over the uniﬁed ones (SSD300, ReﬁneDet, and
YOLOv3). We observe that combining the feature maps
from multiple layers (FPN and YOLOv3) in deep networks
is efﬁcient for the multi-scale adaption of object sizes.

4.6. Further Analysis

In Table 5, we further evaluate the performance of deep
models on each super-class.
In the hierarchical structure
of our proposed IP102 dataset, each sub-class is assigned a
super-class. Each super-class is a subset of IP102, which
covers a portion of the 102 insect pests. For example, for
the super-class “Rice”, our target is to classify one of the
subset of IP102 into 14 categories. The detailed class dis-
tribution on super-class is shown in Table 1. We choose
ResNet [13] as the basic CNN model, which performs best
on the IP102 in the last subsection. We also report the
classiﬁcation results on the metrics for imbalanced learn-
ing evaluation, since the sample number distribution of the
IP102 at the super-class level is still imbalanced as shown
in Fig. 3. Observed from Table 5, the model performance
varies among the 8 super-classes. Moreover, the gap be-
tween the best performance “Mango” and the worst perfor-
mance “Rice” is 56.9% accuracy. The classiﬁcation results

8793

100

)

%
(
y
c
a
r
u
c
c
A

90

80

70

60

50

40

30

20

10

0

1(cid:3)

80

60

40

20

0

-20

-40

-60

-80

(cid:69)(cid:12)

100

50

0

-50

-100

(cid:70)(cid:12)

(cid:68)(cid:12)

7(cid:3) 13(cid:3) 19(cid:3) 25(cid:3) 31(cid:3) 37(cid:3) 43(cid:3) 49(cid:3) 55(cid:3) 61(cid:3) 67(cid:3) 73(cid:3) 79(cid:3) 85(cid:3) 91(cid:3) 97(cid:3) 10(cid:21)

Class Index

-100

-80

-60

-40

-20

0

20

40

60

80

100

-100

-80

-60

-40

-20

0

20

40

60

80

100

Figure 5. (a) The top-1 accuracy of ResNet on each sub-class of the IP102. (b) and (c) Visualizations of 2D t-SNE [26] feature embeddings
on the IP102. (b) ResNet ﬁne-tuned from the IP102 with ImageNet pre-training. (c) ResNet trained from scratch on the IP102.

GT: 9(cid:21)

Rank1: 9(cid:21)(72.5%)

GT: 9(cid:24)

Rank1: 9(cid:24) (89.2%)

GT: (cid:28)(cid:28)

Rank1: (cid:28)(cid:28) (96.7%)

GT: 10(cid:20)

Rank1: 10(cid:20) (82.2%)

(cid:20)(cid:23)(cid:3)(cid:19)(cid:17)(cid:28)(cid:28)(cid:24)

(cid:20)(cid:23)(cid:3)(cid:19)(cid:17)(cid:28)(cid:28)(cid:24)

(cid:20)(cid:24)(cid:3)(cid:19)(cid:17)(cid:28)(cid:27)(cid:23)

(cid:23)(cid:27)(cid:3)(cid:19)(cid:17)(cid:28)(cid:26)(cid:27)

(cid:21)(cid:20)(cid:3)(cid:19)(cid:17)(cid:25)(cid:24)(cid:24)

(cid:21)(cid:23)(cid:3)(cid:19)(cid:17)(cid:28)(cid:22)(cid:21)

(cid:28)(cid:23)(cid:3)(cid:19)(cid:17)(cid:26)(cid:25)(cid:21)

(cid:28)(cid:24)(cid:3)(cid:19)(cid:17)(cid:26)(cid:21)(cid:26)

(cid:21)(cid:23)(cid:3)(cid:19)(cid:17)(cid:26)(cid:20)(cid:25)

(cid:21)(cid:23)(cid:3)(cid:19)(cid:17)(cid:26)(cid:21)(cid:20)

(cid:28)(cid:23)(cid:3)(cid:19)(cid:17)(cid:27)(cid:27)(cid:21)

(cid:28)(cid:23)(cid:3)(cid:19)(cid:17)(cid:28)(cid:27)(cid:26)

(cid:23)(cid:24)(cid:3)(cid:19)(cid:17)(cid:26)(cid:20)(cid:25)

GT: 0

Rank1: 1 (14.7%)

GT: 9

Rank1: 3 (27.0%)

GT: 3

Rank1: 4 (17.6%)

GT: 3

Rank1: 22 (15.7%)

Figure 6. Samples of ResNet classiﬁcation results on the “Mango”
(top) and “Rice” (bottom) super-classes. The images in the
top row are correctly classiﬁed and those in the bottom row are
wrongly classiﬁed.

Figure 7. Sample detection results on the IP102 dataset. The top
row shows the images which are correctly detected. The bottom
row shows some failure cases, such as the right two images which
are correctly detected but wrongly classiﬁed.

on these two super-classes are illustrated in Fig. 6. We can
see that the insect pests from “Mango” have discriminative
characteristics in respect of shape, color, background etc.
As for “Rice”, the images are easily misclassiﬁed due to
three aspects. First, the colors between object and back-
ground are similar. The pests are hard to be distinguished
with massive background information. Second, the intra-
class variation is large, as illustrated in Fig. 2. These pests
typically affect crops to varying degrees throughout their
life cycle, and they are hard to be correctly classiﬁed espe-
cially in the larval period. Third, the pests between classes
are often similar, e.g., Asiatic rice borer and yellow rice
borer. Consequently, as illustrated in Fig. 7, the difﬁculty
of insect pest recognition also brings challenges to the de-
tection task. Even the target is accurately detected, yet it
may be misclassiﬁed.

Moreover, in Fig. 5(a), we show the classiﬁcation accu-
racy results of ResNet [13] on each sub-class of IP102. In
addition, Fig. 5(b) and Fig. 5(c) visualize the feature em-
bedding of IP102 by t-SNE [26]. We can see that, with the
ImageNet [17] pre-trained model, ResNet represents better
to discriminate different insect pests in the feature space.

5. Conclusion

In this work, we collect a large-scale dataset, named
IP102, for insect pest recognition, including over 75, 000
images of 102 species. Compared with previous datasets,
the IP102 conforms to several characteristics of insect pest
distribution in the real environments (e.g., diversity and
class imbalance). Meanwhile, we also evaluate some state-
of-the-art recognition methods on our dataset. The results
demonstrate that current handcrafted feature methods and
deep feature methods cannot yet handle the pest recognition
well. We hope this work will help advance future research
on several fundamental problems as well as common object
classiﬁcation and detection tasks, such as the ﬁne-grained
visual classiﬁcation and imbalanced learning etc.

Acknowledgment

This work was supported by the NSFC (No. 61876094,
61620106008, 61572264), Natural Science Foundation of
Tianjin, China (No. 18JCYBJC15400, 18ZXZNGX00110,
17JCJQJC43700), the National Youth Talent Support Pro-
gram, and the Open Project Program of the National Labo-
ratory of Pattern Recognition (NLPR).

8794

References

[1] H Al Hiary, S Bani Ahmad, M Reyalat, M Braik, and Z Al-
rahamneh. Fast and accurate detection and dlassiﬁcation of
plant diseases. International Journal of Computer Applica-
tions, 17(1):31–38, 2011.

[2] Ahmad Arib Alfarisy, Quan Chen, and Minyi Guo. Deep
learning based classiﬁcation for paddy pests & diseases
recognition. In ICMAI, 2018.

[3] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. SURF:

Speeded up robust features. In ECCV, 2006.

[4] Corinna Cortes and Vladimir Vapnik. Support-vector net-

works. Machine Learning, 20(3):273–297, 1995.

[5] Navneet Dalal and Bill Triggs. Histograms of oriented gra-

dients for human detection. In CVPR, 2005.

[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009.

[7] Limiao Deng, Yanjiang Wang, Zhongzhi Han, and Renshi
Yu. Research on insect pest image detection and recogni-
tion based on bio-inspired methods. Biosystems Engineer-
ing, 169:139–148, 2018.

[8] Juan J Estruch, Nadine B Carozzi, Nalini Desai, Nicholas B
Duck, Gregory W Warren, and Michael G Koziel. Trans-
genic plants: An emerging approach to pest control. Nature
Biotechnology, 15(2):137, 1997.

[9] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (VOC) challenge. International Journal of Computer
Vision, 88(2):303–338, 2010.

[10] Fina Faithpraise, Philip Birch, Rupert Young, J Obu, Bassey
Faithpraise, and Chris Chatwin. Automatic plant pest detec-
tion and recognition using k-means clustering algorithm and
correspondence ﬁlters.
International Journal of Advanced
Biotechnology and Research, 4(2):189–199, 2013.

[11] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. LIBLINEAR: A library for large
linear classiﬁcation. Journal of Machine Learning Research,
9(Aug):1871–1874, 2008.

[12] Alberto Fern´andez, Salvador Garc´ıa, Mar´ıa Jos´e del Jesus,
and Francisco Herrera. A study of the behaviour of lin-
guistic fuzzy rule based classiﬁcation systems in the frame-
work of imbalanced data-sets. Fuzzy Sets and Systems,
159(18):2378–2398, 2008.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[14] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,
and Trevor Darrell. Caffe: Convolutional architecture for fast
feature embedding. In ACM MM, 2014.

[15] Qi Kang, Lei Shi, MengChu Zhou, XueSong Wang, QiDi
Wu, and Zhi Wei. A distance-based weighted undersam-
pling scheme for support vector machines and its application
to imbalanced classiﬁcation. IEEE Transactions on Neural
Networks and Learning Systems, 29(9):4152–4165, 2018.

[16] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3D object representations for ﬁne-grained categorization. In
ICCV Workshop, 2013.

[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012.

[18] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep

learning. Nature, 521(7553):436, 2015.

[19] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[20] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In CVPR, 2017.

[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
ECCV, 2014.

[22] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. SSD: Single shot multibox detector. In ECCV, 2016.

[23] Ziyi Liu, Junfeng Gao, Guoguo Yang, Huan Zhang, and
Yong He. Localization and classiﬁcation of paddy ﬁeld pests
using a saliency map and deep convolutional neural network.
Scientiﬁc Reports, 6:20410, 2016.

[24] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015.

[25] David G Lowe. Distinctive image features from scale-
invariant keypoints. International Journal of Computer Vi-
sion, 60(2):91–110, 2004.

[26] Laurens van der Maaten and Geoffrey Hinton. Visualizing
data using t-SNE. Journal of Machine Learning Research,
9(Nov):2579–2605, 2008.

[27] Subhransu Maji, Esa Rahtu,

Juho Kannala, Matthew
Blaschko, and Andrea Vedaldi. Fine-grained visual classi-
ﬁcation of aircraft. arXiv preprint arXiv:1306.5151, 2013.

[28] M Manoja and J Rajalakshmi. Early detection of pest on
leaves using support vector machine. International Journal
of Electrical and Electronics Research, 2(4):187–194, 2014.

[29] Rajiv Mehrotra, Kameswara Rao Namuduri, and Nagarajan
Ranganathan. Gabor ﬁlter-based edge detection. Pattern
Recognition, 25(12):1479–1494, 1992.

[30] Aude Oliva and Antonio Torralba. Modeling the shape of
the scene: A holistic representation of the spatial envelope.
International Journal of Computer Vision, 42(3):145–175,
2001.

[31] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in PyTorch. In NIPS Workshop, 2017.

[32] R Uma Rani and P Amsini. Pest identiﬁcation in leaf im-
ages using SVM classiﬁer. International Journal of Compu-
tational Intelligence and Informatics, 6(1):30–41, 2016.

[33] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental

improvement. arXiv preprint arXiv:1804.02767, 2018.

8795

[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In NIPS, 2015.

[35] RK Samanta and Indrajit Ghosh. Tea insect pests classiﬁca-
tion based on artiﬁcial neural networks. International Jour-
nal of Computer Engineering Science, 2(6):336, 2012.

[36] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[37] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: A simple
way to prevent neural networks from overﬁtting. The Jour-
nal of Machine Learning Research, 15(1):1929–1958, 2014.
[38] Michael J Swain and Dana H Ballard. Color indexing. Inter-

national Journal of Computer Vision, 7(1):11–32, 1991.

[39] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, 2015.

[40] Kanesh Venugoban and Amirthalingam Ramanan.

Image
classiﬁcation of paddy ﬁeld insect pests using gradient-based
features.
International Journal of Machine Learning and
Computing, 4(1):1–5, 2014.

[41] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical Re-
port CNS-TR-2011-001, California Institute of Technology,
2011.

[42] Jiangning Wang, Congtian Lin, Liqiang Ji, and Aiping
Liang. A new automatic identiﬁcation system of insect im-
ages at the order level. Knowledge-Based Systems, 33:102–
110, 2012.

[43] Chengjun Xie, Rujing Wang, Jie Zhang, Peng Chen, Wei
Dong, Rui Li, Tianjiao Chen, and Hongbo Chen. Multi-level
learning features for automatic classiﬁcation of ﬁeld crop
pests. Computers and Electronics in Agriculture, 152:233–
241, 2018.

[44] Chengjun Xie, Jie Zhang, Rui Li, Jinyan Li, Peilin Hong,
Junfeng Xia, and Peng Chen. Automatic classiﬁcation for
ﬁeld crop insects via multiple-task sparse representation and
multiple-kernel learning. Computers and Electronics in
Agriculture, 119:123–132, 2015.

[45] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and
Stan Z Li. Single-shot reﬁnement neural network for object
detection. In CVPR, 2018.

8796

