A Generative Appearance Model for End-to-end Video Object Segmentation

Joakim Johnander1

3

,

Martin Danelljan1

2

,

Emil Brissman1

4

,

Fahad Shahbaz Khan1

5

,

Michael Felsberg1

1 CVL, Link¨oping University, Sweden

2 CVL, ETH Z¨urich, Switzerland

3 Zenuity, Sweden

4 Saab, Sweden

5 IIAI, UAE

Abstract

One of the fundamental challenges in video object seg-
mentation is to ﬁnd an effective representation of the tar-
get and background appearance. The best performing ap-
proaches resort to extensive ﬁne-tuning of a convolutional
neural network for this purpose. Besides being prohibitively
expensive, this strategy cannot be truly trained end-to-end
since the online ﬁne-tuning procedure is not integrated into
the ofﬂine training of the network.

To address these issues, we propose a network architec-
ture that learns a powerful representation of the target and
background appearance in a single forward pass. The in-
troduced appearance module learns a probabilistic gener-
ative model of target and background feature distributions.
Given a new image, it predicts the posterior class probabil-
ities, providing a highly discriminative cue, which is pro-
cessed in later network modules. Both the learning and
prediction stages of our appearance module are fully dif-
ferentiable, enabling true end-to-end training of the entire
segmentation pipeline. Comprehensive experiments demon-
strate the effectiveness of the proposed approach on three
video object segmentation benchmarks. We close the gap to
approaches based on online ﬁne-tuning on DAVIS17, while
operating at 15 FPS on a single GPU. Furthermore, our
method outperforms all previously published approaches on
the large-scale YouTube-VOS dataset.

1. Introduction

Video object segmentation (VOS) is the task of tracking
and segmenting one or multiple target objects in a video
sequence.
In this work, we consider the semi-supervised
setting, where the ground-truth segmentation is only given
in the ﬁrst frame. The task is generic, i.e., the targets are
arbitrary and no further assumptions regarding the object
classes are made. The VOS problem is challenging from
several aspects. The target may undergo signiﬁcant appear-
ance changes and may be subject to fast motion or occlu-
sion. Moreover, the scene may contain distractor objects

Image

RGMP [31]

A-GAME (Ours)

Figure 1. Comparison between our proposed approach and the re-
cently proposed RGMP [31].
In RGMP, the input features are
concatenated with the initial mask and feature map. In contrast,
we explicitly capture the target and background appearance, in-
cluding distractor objects, by generative modelling. While RGMP
severely struggles, the proposed approach successfully identiﬁes
and accurately segments all annotated targets. As in RGMP, we do
not invoke computationally intensive ﬁne-tuning in the ﬁrst frame,
but instead aim to learn the appearance model in a single forward
pass. The ﬁgure is best viewed in colour.

that are visually or semantically similar to the target.

To tackle the aforementioned challenges, the standard
strategy is to invoke extensive iterative optimization in the
ﬁrst frame [1, 2, 21, 30], given the initial image-mask pair.
However, this strategy comes at an immense computational
cost, rendering real-time operation infeasible. Furthermore,
these methods do not train the segmentation pipeline end-
to-end, since the online ﬁne-tuning step is excluded from
the ofﬂine learning stage. In response to the these issues, we
explore the problem of ﬁnding a feedforward network archi-
tecture for VOS that completely avoids online optimization.
Recent works have posed video object segmentation as a
feedforward mask-reﬁnement process [23,31,34], where the
previous mask prediction is adapted to ﬁt the target in the
current frame using a convolutional neural network. How-

8953

ever, since no explicit modelling of the target appearance is
performed, such approaches inherently fail if the target is
occluded or out of view. This problem has been approached
by incorporating simple appearance models based on e.g.,
concatenation of the feature map from the ﬁrst frame [31],
or utilization of a set of foreground and background feature
vectors [4, 13]. However, these appearance models are ei-
ther too simplistic, achieving unsatisfactory discriminative
power, or cannot be fully trained end-to-end due to the re-
liance of non-differentiable components.

In this work, we propose a novel neural network archi-
tecture for video object segmentation that integrates a pow-
erful appearance model of the scene. In contrast to previous
methods, our network internally learns a generative proba-
bilistic model of the foreground and background feature dis-
tributions. For this purpose, we employ a class-conditional
mixture of Gaussians, which is inferred through a single
forward pass. Our appearance model outputs the poste-
rior class probabilities, thus providing a powerful cue con-
taining discriminative information about the image content.
This completely removes the need for online ﬁne-tuning, as
target-speciﬁc appearance information is captured in a sin-
gle forward pass. We demonstrate our approach in ﬁg. 1.

The proposed generative appearance model is seamlessly
integrated as a module in our video object segmentation net-
work. Our complete architecture is composed of a back-
bone feature extractor, the generative appearance module,
a mask propagation branch, a fusion component, and a ﬁ-
nal upsampling and prediction module. For our genera-
tive appearance module, both the model inference and the
prediction stages are fully differentiable. This ensures that
the entire segmentation pipeline can be trained end-to-end,
which is not the case for methods invoking online ﬁne-
tuning [1, 2, 12, 21, 23, 30] or K-Nearest-Neighbor predic-
tion [4, 13]. Finally, our appearance module is lightweight,
enabling efﬁcient online inference.

We perform extensive experiments on 3 datasets, includ-
ing the recent large-scale YouTubeVOS dataset [32]. We
obtain a ﬁnal score of 66.0% on YouTube-VOS, outper-
forming all previously published methods. Further, our ap-
proach achieves the best mean IoU of 67.2% on Davis17
among all causal video object segmentation methods. We
perform a comprehensive analysis of our method in terms
of an ablation study. Our analysis clearly underlines the ef-
fectiveness of the proposed generative appearance module
and the importance of full end-to-end learning.

2. Related Work

In this work we address the problem of video object seg-
mentation where an initial segmentation mask is provided,
deﬁning the target in the ﬁrst frame. In recent years interest
in this problem has surged and a wide variety of approaches
have been proposed. Caelles et al. [2] proposed to use a

convolutional neural network pre-trained for the semantic
segmentation task, and ﬁne-tune this in the ﬁrst frame to
segment out foreground and background. The approach was
extended in a number of works: continuous training during
the sequence [30]; adding instance-level semantic informa-
tion [21]; incorporating motion information via optical ﬂow
[1, 6, 12]; performing temporal propagation via a Markov
random ﬁeld [1]; location-speciﬁc embeddings [8]; sophis-
tic data augmentation [16]; or a combination of these [20].
While these approaches obtain satisfactory results in many
scenarios, they have one critical drawback in common: they
learn the target appearance in the initial frame via exten-
sive training of deep neural networks with stochastic gra-
dient descent. This leads to a signiﬁcant time-delay before
these methods can start tracking, and an average computa-
tion time that renders real-time processing infeasible.

Despite reduced accuracy, several approaches avoid in-
voking expensive ﬁne-tuning procedures in the ﬁrst frame.
Some methods rely on optical ﬂow coupled with reﬁne-
ment [15, 29]. Li et al. proposed DyeNet [18], which
combines optical ﬂow with an object proposal network,
interleaving bidirectional mask-propagation and target re-
identiﬁcation. DyeNet provides outstanding performance,
but it is not causal and relies on future video frames to make
predictions. Jampani et al. [14] explicitly try to avoid opti-
cal ﬂow and propose an approach based on bilateral ﬁlters.
Cheng et al. [5] track different parts of the target with visual
object tracking techniques, and reﬁne the ﬁnal solution with
a convolutional neural network. Xu et al. [32] instead train
a convolutional LSTM [11] to track and segment the target.

More closely related to our work, Perazzi et al. [23] pose
video object segmentation as a mask reﬁnement problem.
Based on an input image, the mask predicted from the pre-
vious frame is reﬁned with a neural network. The network
is recurrent in time, with a particularly deep recurrent con-
nection, an entire VGG16 [28].
In the work by Yang et
al. [34], the mask was reduced to a rough spatial prior on
the target location, and this together with a channel-wise at-
tention mechanism provided improved performance. Wug
et al. [31] extend [23] and concatenate the initial frame
feature map and mask with the current feature map and
previous mask, and train a standard convolutional neural
network to match and segment in a fully recurrent fash-
ion. Also more explicit matching mechanisms have been
proposed, where the input features are matched with a set
of features with known class membership [4, 13] using K-
Nearest-Neighbour (KNN). While these methods model the
target appearance, the non-parametric nature of KNN re-
quires the entire training set to be stored. Additionally, the
process of ﬁnding the K nearest neighbours is not differen-
tiable. In contrast to existing work, our approach learns a
compact appearance model of the scene in a single differ-
entiable forward pass.

8954

Figure 2. Full architecture of the proposed approach, illustrating both model initialization and frame processing. Model Initialization: A
feature map is extracted from the initial frame, which is then fed together with the mask to the mask propagation module. This pair is
furthermore used to initialize the appearance model. Frame processing: A feature map is extracted from the current frame and fed to both
the appearance and mask-propagation modules whose outputs are combined, generating a coarse mask-encoding. Our upsampling module
then reﬁnes the mask-encoding by also considering low-level information contained in the shallow features. The predictor then generates
a ﬁnal segmentation, based on this encoding. Moreover, the mask-encoding and appearance model parameters are fed back via a recurrent
connection. During training, we use two cross-entropy losses applied to the coarse and ﬁne segmentations, respectively.

3. Method

3.2. Generative Appearance Module

The aim of this work is to develop a network architecture
for video object segmentation with the capability of learn-
ing accurate models of the target and background appear-
ance through a single forward pass. That is, the network
must learn in a one-shot manner to discriminate between
target and background pixels, without invoking stochastic
gradient descent. We tackle this problem by integrating a
generative model of the foreground and background appear-
ance. This model directly aids the segmentation process by
providing discriminative posterior class probabilities. The
learning and inference is computationally efﬁcient and end-
to-end differentiable, enabling a seamless integration of our
generative component into a neural network.

3.1. Overview

Our approach is divided into ﬁve components that jointly
address the video object segmentation task and are trained
jointly end-to-end. The model is illustrated in ﬁg. 2. Given
an input image, features are ﬁrst extracted with a backbone
network. These are then passed to the appearance- and
mask-propagation modules. The outputs of these two mod-
ules are combined in the fusion module, comprising two
convolutional layers and outputting a coarse mask encod-
ing. The encoding is handed to a predictor that generates
a coarse segmentation mask. This prediction is used to up-
date the appearance module and further used as input to the
mask-propagation layer in the next frame to provide a rough
spatial prior. The mask encoding output by the fusion com-
ponent is also passed through an upsampling module, in
which the coarse encoding is combined with successively
more shallow features in order to produce a ﬁnal reﬁned
segmentation.

The task of our appearance module is to learn a gener-
ative model of the video content in a deep feature space.
Our generative model is conditioned on the class variable,
indicating target or background. Given a new frame, the ap-
pearance module returns the posterior class probabilities at
each image location. This output forms an extremely strong
cue for foreground/background discrimination, as the pro-
posed module explicitly models their respective appearance
in a probabilistic manner.
Model learning: Formally, let the set of features extracted
from the image be denoted as {xp}p. The feature xp at
each spatial location p is a D-dimensional vector of real
numbers. We model these observed feature vectors as i.i.d.
samples drawn from the underlying distribution

p(xp) =

K

X

k=1

p(zp = k)p(xp|zp = k) .

(1)

Each class-conditional density is a multi-variate Gaussian
with mean µk and covariance matrix Σk,

p(xp|zp = k) = N (xp|µk, Σk) .

(2)

The discrete random variable zp in (1) assigns the obser-
vation xp to a speciﬁc component zp = k. We use a uni-
form prior p(zp = k) = 1/K for this variable, where K
is the number of components. Each component exclusively
models the feature vectors of either the foreground or back-
ground. As further detailed below, we use four Gaussians,
where the components k ∈ {0, 2} model background and
k ∈ {1, 3} model foreground features.

In the ﬁrst frame, our generative mixture model is in-
ferred from the extracted features and the initial target mask.
In subsequent frames, we update the model using the net-
work predictions as soft class labels. In general, to update

8955

the mixture model in a frame i we require a set of features
xi
p together with a set of soft component assignment vari-
ables αi
p,k ∈ [0, 1]. These variables can be thought of as
soft labels, describing the level of assignment of the vector
xi
p to component k. In the ﬁrst frame i = 0, the feature
vectors would be strictly assigned to either foreground or
background α0

p,k ∈ {0, 1}, using the initial target mask.

Given the variables αi

p,k, we compute the model param-

eter updates as,

xi
p

,

p,k

i

k = Pp αi
Pp αi
k = Pp αi

p,k

˜µ

˜Σi

p − ˜µ

p,k diag{(xi
Pp αi

p,k

(3a)

i

k)2 + rk}

.

(3b)

For efﬁciency, we limit the covariance matrix to be diago-
nal, where diag{v} is a diagonal matrix with entries corre-
sponding to the input vector v. To avoid singularities, the
covariance is regularized with a vector rk, which is a train-
able parameter in our network. In the ﬁrst frame, the mix-
ture model parameters in (2) are directly achieved from (3),
i.e. µ
k. In subsequent frames, these
parameters are updated with new information (3) using a
learning rate λ,

k and Σ0

k = ˜Σ0

k = ˜µ

0

0

ﬁnal prediction of our segmentation network according to

p,0 = 1 − ˜yp(I i, θi−1, Φ)
αi
p,1 = ˜yp(I i, θi−1, Φ) .
αi

(5)

Here, ˜yp(I i, θi−1, Φ) ∈ [0, 1] is the probability of the target
class, given the input image I i, neural network parameters
Φ, and current mixture model parameter estimates θi−1.

A drawback of using a single Gaussian component per
class is that only uni-modal distributions can be accurately
represented. However, the background appearance is typi-
cally multi-modal, especially in the presence of background
objects that are similar to the target, often termed distrac-
tors. To obtain satisfactory discrimination between fore-
ground and background, it is therefore critical to capture
the feature distribution of such distractors. We therefore
add Gaussian components in our model that are dedicated
to the task of modeling hard examples. These compo-
nents are explicitly learned to counter the errors of the two
base components.
Ideally, we would wish the base com-
ponents alone to correctly predict the assignment variables,
i.e. p(zi
p,k, k = 0, 1. The additional
components are trained on data where this does not hold by
considering incorrectly classiﬁed background (k = 2) and
foreground (k = 3) respectively. Their corresponding as-
signment variables are computed as,

p = k|xi

k) = αi

k, Σi

p, µ

i

µ

Σi

i
k = (1 − λ)µ
k = (1 − λ)Σi−1

i−1
k + λ ˜µ
k + λ ˜Σi

i
k ,
k .

αi
p,2 = max(0, αi
αi
p,3 = max(0, αi

p,0 − p(zi
p,1 − p(zi

p = 0|xi
p = 1|xi

p, µ
p, µ

(4)

0, Σi
i
i
1, Σi

0))

1)) .

(6)

Assignment variables: Next, we describe the computa-
tion of the assignment variables αi
p,k. Note that (3) re-
sembles the M-step in the Expectation Maximization (EM)
algorithm for a mixture of Gaussians.
In EM, the vari-
ables zi
p are treated as latent and (3) is derived by maxi-
mizing the expected complete-data log-likelihood. In that
case the assignment variables are computed in the E-step as
αi
k }k
are the previous estimates of the parameters. However, the
setting is different in our case. The discrete assignment vari-
ables zi
p are fully observed in the ﬁrst frame. Moreover, in
the subsequent frames, the network reﬁnes the posteriors
p, θi−1), providing even better assignment esti-
p(zi
mates. We therefore exploit these factors in the estimation
of the assignment variables αi

p, θi−1), where θi−1 = {µ

p,k = p(zi

p = k|xi

p = k|xi

, Σi−1

i−1
k

kp.

Our model consists of one base component for back-
ground k = 0 and foreground k = 1, respectively. Given
the ground truth binary target mask yp in the ﬁrst frame,
where yp = 1 for foreground and yp = 0 otherwise, we set
p,1 = yp. That is, the feature vectors xi
α0
p
are strictly assigned to the foreground and background base
components according to the initial mask.
In subsequent
frames, where the ground-truth is not available, we use the

p,0 = 1 − yp and α0

i

p, µ

k, Σi

p = k|xi

The posteriors p(zi
k) are evaluated using
only the base components. Given (6), we ﬁnally update the
parameters of the components k = 2, 3 using (3) and (4).
Module output: Given the mixture model parameters
computed in the previous frame, θi−1, our model can pre-
dict the component posteriors,

p(zi

p = k|xi

p, θi−1) =

p(zi

p = k)p(xi

p|zi
p = k)p(xi

p = k)
p|zi

p = k)

Pk p(zi

. (7)

Note that each component k belongs to either foreground or
background, and that the outputs (7) thus provide a discrim-
inative mask encoding. In practice, we found it beneﬁcial
to feed the log-probabilities log(p(zi
p = k))
into the conv layers in the fusion module. By canceling out
constant factors, the outputs are calculated as,

p = k)p(xi

p|zi

ln |Σi−1

k

| + (xi

p − µ

si
pk = −

i−1
k

)T(Σi−1
2

k

)−1(xi

p − µ

i−1
k

)

.

(8)
The component posteriors (7) can be reconstructed from
si
pk by a simple soft-max operation. The output (8) should
therefore be interpreted as component scores, encoding
foreground and background assignment. The entire appear-
ance modelling procedure is summarized in Algorithm 1.

8956

3.3. Object Segmentation Architecture

As our backbone feature extractor, we use ResNet101
[10] with dilated convolutions [3] to reduce the stride of the
deepest layer from 32 to 16. It is pretrained on ImageNet
and all layers up to the last block, layer4, are frozen. The
mask-propagation module is based on the concept proposed
in [31]. The module constructs a mask encoding based on
the mask predicted in the previous frame, a feature map
predicted in the current frame, and a feature map extracted
from the initial frame together with the given ground-truth
mask. The entire module consists of three convolutional
layers, where the middle layer is a dilation pyramid [3].

The outputs of the mask propagation and appearance
modules are concatenated and fed into the fusion module,
comprising two convolution layers. The result is processed
by the upsampling module from which a predicted soft seg-
mentation ˆyp is obtained. The output of the fusion module is
also fed into a predictor that produces a coarse segmentation
˜yp, which is utilized by the mask propagation and appear-
ance modules (using (5)) in the next timestep. By separating
the feature extractor and upsampling path from the recurrent
module we get a shorter path between variables of different
time steps. We experienced the coarse mask to be a sufﬁ-
cient representation of the previous target segmentation. As
a special case, during sequences with multiple objects, we
run our approach once per object and combine the result-
ing soft segmentations with softmax-aggregation [31]. The
aggregated soft segmentations then replaces the coarse seg-
mentations ˜yp in the recurrent connection.

The output of the fusion module provides coarse mask-
encoding that is used to locate and segment the target. There
have been considerable efforts in semantic segmentation
and instance segmentation litterature to reﬁne ﬁnal seg-
mentations. We adopt an upsampling path similar to [25],
where the coarse representation is successively combined
with successively shallower features.

3.4. Network Training

We train the proposed neural network end-to-end in a re-
current fashion. Based on a video and a single ground-truth
segmentation, the network predicts segmentation masks for
each frame in the video. We train on three datasets:
DAVIS2017 [26]: The DAVIS2017 training set comprises
60 videos containing one or several annotated objects to
track. Each video is between 25 and 100 frames long, each
of which is labeled with a ground-truth segmentation.
YouTube-VOS [32]: The YouTube-VOS training set con-
sists of 3471 videos with one or several target objects. Each
video is 20 to 180 frames long, where every ﬁfth frame is
labelled. We use only the labelled frames during training.
SynthVOS: In order to cover a wide varity of classes we
follow [23, 31] and utilize objects from the salient object
segmentation dataset MSRA10k [7]. It contains 104 images

i

k, Σi

k, and the input feature map xi

Algorithm 1: The appearance module inference and
update. Inference: Based on the appearance model pa-
rameters, µ
p, a soft
segmentation is constructed for the background, fore-
ground, and the two residual components. Update:
The appearance model parameters are updated based
on the coarse segmentation ˜yi
p.

1 Inference(xi

p, µ

i

k, Σi

k):

2

for k = 0, 1, 2, 3: compute si
return si
3
4 Update(xi

pk
p, ˜yi

k, Σi

i

pk from (8)

k based on (3)

p, µ

k):
for k = 0, 1: compute αi
p,k from (5)
k, ˜Σi
i
for k = 0, 1: compute ˜µ
for k = 0, 1: compute si
pk based on (8)
for k = 0, 1: compute
p(zi
i
0, Σi
for k = 2, 3: compute αi
p,k from (6)
k, ˜Σi
i
for k = 2, 3: compute ˜µ
for k = 0, 1, 2, 3: update µ
return µ

0) = Softmax(si

k and Σi

p = k|xi

p, µ

k and Σi

k

i

i

k based on (3)

k from (4)

p0, si

p1)

5

6

7

8

9

10

11

12

where a single object is segmented. We paste 1 to 5 such ob-
jects onto an image from VOC2012 [9]. A synthetic video
is obtained by moving the objects across the image.

One training sample consists of a video snippet of n
frames and a given ground-truth for the ﬁrst frame.
Im-
ages are normalized with ImageNet [27] mean and standard
deviation. We let our model predict segmentation masks
in each frame and apply a cross-entropy loss. We also place
an auxillary loss on the coarse segmentations ˜yp. The losses
are summed and minimized with Adam in two stages:
Initial training: First we train for 80 epochs using all three
datasets on half resolution images (240 × 432). The batch-
size is set to 4 video snippets, using 8 frames in each snip-
pet. We use a learning rate of 10−4, exponential learning
rate decay of 0.95 per epoch, and a weight decay of 10−5.
Finetuning: We then ﬁnetune for 100 epochs on the
DAVIS2017 and YouTube-VOS training sets, using full im-
age resolution. During this step we sample sequences from
both datasets with equal probability. The batchsize is low-
ered to 2 snippets, to accomodate longer sequences of 14
frames. We use a learning rate of 10−5, exponential learn-
ing rate decay of 0.985 per epoch, and a weight decay
of 10−6. The training is stopped early by observing the
performance on a held-out set of 300 sequences from the
YouTube-VOS training set.

4. Experiments

We ﬁrst conduct an ablation study of the proposed
approach on the Youtube-VOS benchmark [32]. Then,

8957

we compare with the state-of-the-art on three video ob-
ject segmentation datasets [24, 26, 32]. Our approach,
called A-GAME,
is implemented in PyTorch [22] and
trained on a single Nvidia V100 GPU. Our code and
trained networks are available at https://github.
com/joakimjohnander/agame-vos.

4.1. Ablation Study

We perform an extensive ablative analysis of our ap-
proach on the large-scale YouTube-VOS dataset. We use
the ofﬁcial validation set, comprising 474 videos labelled
with one or multiple objects. Ground-truth masks are with-
held, and results are obtained through an online evaluation
server. Performance is measured in terms of the mean Jac-
card index J [26], i.e. intersection-over-union (IoU), and
the mean contour accuracy F . The two measures are sep-
arately calculated for seen and unseen classes, resulting in
four performance measures. The overall performance (G) is
the average of all four measures.

In our ablative experiments, we analyze six key modi-
ﬁcations of our approach, as explained below. Results are
shown in table 1. For each version, we retrain the entire
network from scratch using the exact same procedure.
Appearance module: We ﬁrst analyze the impact of the
proposed appearance module (see section 3.2) by remov-
ing it from the network (No appearance module in table 1).
This leads to a major reduction in overall performance, from
66.0% to 50.0%. The results clearly demonstrate that the
introduced appearance module is an essential component in
our video object segmentation approach. Further insights
are obtained by studying the performance on seen and un-
seen classes in table 1. Note that removing the appearance
module causes a 9.1% decrease for classes that are seen dur-
ing training, and a remarkable 20.6% decrease for unseen
classes. Thus, our generative appearance model component
is crucial for the generalization to arbitrary objects that are
unseen during training. This is explained by the target spe-
ciﬁc and class-agnostic nature of our appearance module.
Mask-propagation module:
Secondly, we investigate
the importance of the mask-propagation module (see sec-
tion 3.3). Refraining from propagating the mask predicted

Version
A-GAME
No appearance module
No mask-prop module
Unimodal appearance
No update
Appearance SoftMax
No end-to-end

G

66.0
50.0
64.0
64.4
64.9
55.8
58.8

J seen (%) J unseen (%)

66.9
57.8
65.5
65.8
66.0
59.3
62.5

61.2
40.6
59.5
58.8
59.8
50.7
53.1

Table 1. Ablation study on YouTube-VOS. We report the overall
performance G along with segmentation accuracy J on classes
seen and unseen during training. See text for further details.

Image

Final segmentation

Appearance

Figure 3. Visualization of the appearance module on ﬁve videos
from YouTube-VOS. The ﬁnal segmentation of our approach is
shown (middle) together with output of the appearance module
(right). The appearance module accurately locates the target (red)
with the foreground representation while accentuating potential
distractors (green) with the secondary mixture component.

in the previous frame (No mask-prop module in table 1) re-
sults in a 2.0% reduction in performance. While this reduc-
tion is signiﬁcant, the importance of the mask-propagation
module is small compared to that of the appearance module.
Gaussian mixture components: As described in sec-
tion 3.2, we employ two Gaussian mixture components to
model the foreground and background, respectively. In ad-
dition to the base mixture component, a secondary Gaus-
sian mixture component is added to capture hard examples
that are not accurately modelled by a unimodal distribution.
We investigate the impact of this additional mixture compo-
nents by removing them from our model. The resulting ver-
sion (Unimodal appearance in table 1) thus only employs a
single base mixture component for each class. The resulting
performance drop of 1.6% indicates the importance of mod-
eling hard examples in the presence of distractor objects.

The impact of the multi-modal generative model is also
analyzed qualitatively in ﬁg. 3. The mixture component
dedicated to hard negative image regions is able to model
other objects in the vicinity of the target (row 1 and 2) and
accurately captures other objects of the same class (row 3-
5). Note that both the appearance module’s output and the
ﬁnal segmentations are soft, and only for the purpose of vi-
sualization we show the arguments of the maxima.
Model update: We investigate the impact of updating the
generative model in each frame using (4). The version No
update (table 1) only uses the initial frame to compute the
mixture model parameters (3), and no update (4) is per-

8958

Method
S2S [33]
OSVOS [2]
OnAVOS [30]
MSK [23]
OSMN [34]
S2S [33]
RGMP [31]
RGMP† [31]
A-GAME
A-GAME†

O-Ft

G overall (%) J seen (%) J unseen (%)

X

X

X

X
×
×
×
×
×
×

64.4
58.8
55.2
53.1
51.2
57.6
53.8
50.5
66.0
66.1

71.0
59.8
60.1
59.9
60.0
66.7
59.5
54.1
66.9
67.8

55.5
54.2
46.6
45.0
40.6
48.2
45.2
41.7
61.2
60.8

Table 2. State-of-the-art comparison on the YouTubeVOS bench-
mark. Our approach obtains the best overall performance (G) de-
spite not performing any online ﬁne-tuning (O-Ft). Further, our
approach provides a large gain in performance for categories un-
seen during training (J unseen), compared to existing methods.
Entries marked by † were trained with only YouTube-VOS data.

formed during training and inference. Updating the genera-
tive model to capture changes in the target and background
appearance leads to a 1.1% improvement in performance.
Appearance module output: As previously described,
our appearance module outputs the log-probability scores
(8). To validate this choice, we also compare with out-
putting the posterior probabilities (Appearance SoftMax in
table 1), obtained by adding a SoftMax layer after com-
puting the scores (8), between the appearance and fusion
modules. This leads to a signiﬁcant degradation in per-
formance (−10.2%). These results are in line with con-
ventional techniques in segmentation [19] and classiﬁca-
tion [17], where activations in the network are not converted
to probabilities until the ﬁnal output layer.
End-to-end learning:
Finally, we analyze the impact
of end-to-end differentiation and training in our approach.
Speciﬁcally, we investigate the importance of end-to-end
differentiability in the learning stage of the appearance
module. The comparison is performed by not backpropa-
gating through the model inference computation (3) during
the training of the network. Note that, the rest of the frame-
work remains unchanged. The resulting method (No end-to-
end in table 1) obtains poor results, with a total degradation
of 7.2% in overall performance. This highlights the impor-
tance of permitting true end-to-end learning.

4.2. State of the Art Comparison

We compare our approach with the state-of-the-art on
three video object segmentation benchmarks: YouTube-
VOS [32], DAVIS2017 [26], and DAVIS2016 [24].
YouTube-VOS:
This recently introduced large-scale
dataset contains 474 sequences with 91 classes, 26 of which
are not included in the YouTube-VOS training set. We use
the ofﬁcial validation set, as in section 4.1. We compare
our approach with all, to our best knowledge, published re-
sults [32]. Additionally, we evaluate the RGMP method,
using the code provided by the authors. The results are
shown in table 2. For each approach, we indicate if the
method employs online ﬁne-tuning (O-Ft) and if it is causal,

Method
CINM [1]
OSVOS-S [21]
OnAVOS [30]
OSVOS [2]
DyeNet [18]
RGMP [31]
VM [13]
FAVOS [5]
OSMN [34]
A-GAME

X

X

X

X

X

X

70.6
68.0
65.4
60.3
69.1
66.7

O-Ft Causal J &F mean (%) F (%) J (%)
67.2
64.7
61.6
56.6
67.3
64.8
56.5
54.6
52.5
67.2

74.0
71.3
69.1
63.9
71.0
68.6

X
×
×
×
×
×
×

58.2
54.8
70.0

61.8
57.1
72.7

X
×
X

X

X

X

X

-

-

Table 3. State-of-the-art comparison on the DAVIS2017 valida-
tion set. For each method we report whether it employs online
ﬁne-tuning (O-Ft), is causal, and the ﬁnal performance J (%).
Our approach obtains superior results compared to state-of-the-art
methods without online ﬁne-tuning. Further, our approach closes
the performance gap to existing methods employing online ﬁne-
tuning.

Method
OnAVOS [30]
OSVOS-S [21]
MGCRN [12]
CINM [1]
LSE [8]
OSVOS [2]
MSK [23]
SFL [6]
DyeNet [18]
FAVOS [5]
RGMP [31]
VM [13]
MGCRN [12]
PML [4]
OSMN [34]
CTN [15]
VPN [14]
MSK [23]
A-GAME

O-Ft Causal

X

X

X

X

X

X

X

X
×
×
×
×
×
×
×
×
×
×
×

X

X

X

X

X

X

X

X
×
X

X

X

X

X

X

X

X

X

X

13s
4.5s
0.73s
>30s

84.9
87.5
85.7
85.0
80.1
80.6
75.4
76.0

85.5
86.6
85.1
84.2
81.5
80.2
77.6
75.4

Speed J &F mean (%) F (%) J (%)
86.1
85.6
84.4
83.4
82.9
79.8
79.7
74.8
84.7
82.4
81.5
81.0
76.4
75.5
74.0
73.5
70.2
69.9
82.0

9s
12s
7.9s
0.42s
1.80s
0.13s
0.32s
0.36s
0.28s
0.14s
1.30s
0.63s
0.15s
0.07s

76.5
81.2
73.5
71.4
67.9

76.6
79.3
72.9
69.3
65.5

81.0
81.8

79.5
82.0

82.1

82.2

-

-

-

-

-

-

Table 4. State-of-the-art comparison on DAVIS2016 validation set,
which is a subset of DAVIS2017. For each method we report
whether it employs online ﬁne-tuning (O-Ft), is causal, the com-
putation time (if available), and the ﬁnal performance J (%). Our
approach obtains competitive results compared to causal methods
without online ﬁne-tuning.

i.e. if the segmentation output depends on future frames in
the video. Here we let X and × indicate yes and no, re-
spectively. Among previous approaches performing exten-
sive online ﬁne-tuning in the ﬁrst frame, OSVOS and On-
AVOS achieve ﬁnal scores of 58.8% and 55.2%. For the
S2S method, we compare with two versions: one with and
one without online ﬁne-tuning, obtaining 64.4% and 57.6%,
respectively. Our approach obtains a ﬁnal score of 66.0%,
signiﬁcantly outperforming state-of-the-art without invok-
ing any online ﬁne-tuning. Furthermore, our method per-
forms notably well on the unseen category, which only con-
siders objects that are not seen during training. Again, this
demonstrates the effectiveness of our class-agnostic appear-
ance module, which generalizes to arbitrary target objects.
DAVIS2017: The dataset comprises 30 videos with one
or multiple target objects. The results are shown in ta-
ble 3. Among existing methods, DyeNet is the only ap-

8959

Image

Ground Truth

RGMP [31]

CINM [1]

FAVOS [5]

A-GAME

Figure 4. Qualitative comparison between our approach and 3 state-of-the-art approaches. Our approach is able to accurately segment all
targets, demonstrating robustness to occlusions and successfully discriminating between different objects. This is largely thanks to the
powerful appearance model in our architecture.

proach that is non-causal, since it processes the entire video
in a bidirectional manner. It is therefore not applicable to
real-time or online systems. The RGMP method, achiev-
ing a score of 64.8%, relies on mask propagation and an
appearance model constructed by simply concatenating im-
age features from the ﬁrst frame. VideoMatch (VM) stores
foreground and background feature vectors that are then
matched with feature vectors in the test image. This method
obtains a ﬁnal result of 56.5%. The proposed method, em-
ploying an end-to-end differentiable generative probabilis-
tic appearance model, achieves a score of 67.2%. Our ap-
proach outperforms all causal methods not invoking online
ﬁne-tuning, and is even on par with the best non-causal and
online ﬁne-tuning-based techniques.
DAVIS2016: For completeness, we also evaluate our ap-
proach on DAVIS2016. It is a subset of DAVIS2017, con-
taining 20 videos labeled with a single object. The small
size and number of objects in DAVIS2016 limits the diver-
sity. It has therefore become highly saturated over the years.
In table 4 we show the ﬁnal result of each method, along
with computational time reported by the respective authors.
Our approach obtains a competitive performance of 82.0%
compared to state-of-the-art. Unlike our method, the top
performing approaches on DAVIS2016, such as OSVOS,
OnAVOS, and FAVOS do not generalize well to the larger
and more diverse YouTube-VOS and DAVIS2017 datasets.

4.3. Qualitative Evaluation

We qualitatively compare our approach with three state-
of-the-art approaches (RGMP [31], CINM [1], FAVOS [5])

on three videos from DAVIS2017. The results are shown in
ﬁg. 4. RGMP tends to lose parts of objects, and struggles
with discrimination between different objects. While CINM
can produce detailed segmentation masks (row 5), it suffers
from several failure modes (row 2, 4, 6). FAVOS struggles
with discriminating targets (row 2, 6) and fails to capture
details (row 6) or precise boundaries (row 4). The proposed
approach succeeds to accurately segment both targets in all
scenarios while being one or several orders of magnitude
faster compared to FAVOS and CINM, respectively.

5. Conclusion

We propose to address the VOS problem by learning
the appearance of the target in an efﬁcient and differen-
tiable manner, avoiding the drawbacks of existing matching
or online-ﬁnetuning based approaches. The target appear-
ance is modelled as a mixture of Gaussians in an embed-
ding space, and we show that both learning and inference
of this model can be expressed in closed form. This permits
the implementation of the appearance model as a compo-
nent in a neural network that is trained on end-to-end. We
thoroughly analyze the proposed approach and demonstrate
its effectiveness on three benchmarks, resulting in state-of-
the-art performance.

Acknowledgments: This work was partially supported by
the Wallenberg AI, Autonomous Systems and Software Pro-
gram (WASP); SFF (SymbiCloud); and Swedish Research
Council (ELLIIT and grant 2018-04673).

8960

References

[1] L. Bao, B. Wu, and W. Liu. Cnn in mrf: Video ob-
ject segmentation via inference in a cnn-based higher-order
spatio-temporal mrf. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 5977–
5986, 2018. 1, 2, 7, 8

[2] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix´e,
D. Cremers, and L. Van Gool. One-shot video object seg-
mentation. In CVPR 2017. IEEE, 2017. 1, 2, 7

[3] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE transactions on pattern analysis and ma-
chine intelligence, 40(4):834–848, 2018. 5

[4] Y. Chen, J. Pont-Tuset, A. Montes, and L. Van Gool. Blaz-
ingly fast video object segmentation with pixel-wise metric
learning. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1189–1198,
2018. 2, 7

[5] J. Cheng, Y.-H. Tsai, W.-C. Hung, S. Wang, and M.-H. Yang.
Fast and accurate online video object segmentation via track-
ing parts. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018. 2, 7, 8

[6] J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang. Segﬂow:
Joint learning for video object segmentation and optical ﬂow.
In Computer Vision (ICCV), 2017 IEEE International Con-
ference on, pages 686–695. IEEE, 2017. 2, 7

[7] M. Cheng. Msra10k database, 2015. 5

[8] H. Ci, C. Wang, and Y. Wang. Video object segmentation
by learning location-sensitive embeddings. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 501–516, 2018. 2, 7

[9] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams,
J. Winn, and A. Zisserman. The pascal visual object classes
challenge: A retrospective.
International journal of com-
puter vision, 111(1):98–136, 2015. 5

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 5

[11] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997. 2

[12] P. Hu, G. Wang, X. Kong, J. Kuen, and Y.-P. Tan. Motion-
guided cascaded reﬁnement network for video object seg-
mentation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1400–1409,
2018. 2, 7

[13] Y.-T. Hu, J.-B. Huang, and A. G. Schwing. Videomatch:
Matching based video object segmentation. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 54–70, 2018. 2, 7

[14] V. Jampani, R. Gadde, and P. V. Gehler. Video propagation

networks. In Proc. CVPR, volume 6, page 7, 2017. 2, 7

[16] A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele.
Lucid data dreaming for object tracking. In The 2017 DAVIS
Challenge on Video Object Segmentation - CVPR Work-
shops, 2017. 2

[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 7

Imagenet
In

[18] X. Li and C. Change Loy. Video object segmentation with
joint re-identiﬁcation and attention-aware mask propagation.
In The European Conference on Computer Vision (ECCV),
September 2018. 2, 7

[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3431–3440, 2015. 7

[20] J. Luiten, P. Voigtlaender, and B. Leibe. Premvos: Proposal-
generation, reﬁnement and merging for the davis challenge
on video object segmentation 2018, 2018. 2

[21] K. Maninis, S. Caelles, Y. Chen, J. Pont-Tuset, L. Leal-Taixe,
D. Cremers, and L. Van Gool. Video object segmentation
without temporal information. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2018. 1, 2, 7

[22] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017. 6

[23] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and
A. Sorkine-Hornung. Learning video object segmentation
from static images. In Computer Vision and Pattern Recog-
nition, volume 2, 2017. 1, 2, 5, 7

[24] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,
M. Gross, and A. Sorkine-Hornung. A benchmark dataset
and evaluation methodology for video object segmentation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 724–732, 2016. 6, 7

[25] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar. Learn-
In European Conference on

ing to reﬁne object segments.
Computer Vision, pages 75–91. Springer, 2016. 5

[26] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel´aez, A. Sorkine-
Hornung, and L. Van Gool. The 2017 davis challenge on
video object segmentation. arXiv:1704.00675, 2017. 5, 6, 7
[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, pages 1–42, April 2015. 5

[28] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
2

[29] Y.-H. Tsai, M.-H. Yang, and M. J. Black. Video segmenta-
tion via object ﬂow. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 3899–
3908, 2016. 2

[30] P. Voigtlaender and B. Leibe. Online adaptation of convo-
lutional neural networks for video object segmentation. In
British Machine Vision Conference 2017, BMVC 2017, Lon-
don, UK, September 4-7, 2017, 2017. 1, 2, 7

[15] W.-D. Jang and C.-S. Kim. Online video object segmenta-
tion via convolutional trident network. In CVPR, volume 1,
page 7, 2017. 2, 7

[31] S. Wug Oh, J.-Y. Lee, K. Sunkavalli, and S. Joo Kim. Fast
video object segmentation by reference-guided mask propa-
gation. In Proceedings of the IEEE Conference on Computer

8961

Vision and Pattern Recognition, pages 7376–7385, 2018. 1,
2, 5, 7, 8

[32] N. Xu, L. Yang, Y. Fan, J. Yang, D. Yue, Y. Liang, B. L.
Price, S. Cohen, and T. S. Huang. Youtube-vos: Sequence-
to-sequence video object segmentation. In Computer Vision -
ECCV 2018 - 15th European Conference, Munich, Germany,
September 8-14, 2018, Proceedings, Part V, pages 603–619,
2018. 2, 5, 6, 7

[33] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. S.
Huang. Youtube-vos: A large-scale video object segmenta-
tion benchmark. CoRR, abs/1809.03327, 2018. 7

[34] L. Yang, Y. Wang, X. Xiong, J. Yang, and A. K. Katsaggelos.
Efﬁcient video object segmentation via network modulation.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018. 1, 2, 7

8962

