Visual Tracking via Adaptive Spatially-Regularized Correlation Filters

Kenan Dai1, Dong Wang1∗, Huchuan Lu1

,

2, Chong Sun1

,

3, Jianhua Li1

1 School of Information and Communication Engineering, Dalian University of Technology, China

2Peng Cheng Laboratory 3 Tencent Youtu Lab

dkn2014@mail.dlut.edu.cn, {wdice,lhchuan}@dlut.edu.cn, waynecool@mail.dlut.edu.cn, jianhual@dlut.edu.cn

Abstract

In this work, we propose a novel adaptive spatially-
regularized correlation ﬁlters (ASRCF) model to simulta-
neously optimize the ﬁlter coefﬁcients and the spatial reg-
ularization weight. First, this adaptive spatial regulariza-
tion scheme could learn an effective spatial weight for a
speciﬁc object and its appearance variations, and therefore
result in more reliable ﬁlter coefﬁcients during the track-
ing process. Second, our ASRCF model can be effective-
ly optimized based on the alternating direction method of
multipliers, where each subproblem has the closed-from so-
lution. Third, our tracker applies two kinds of CF models
to estimate the location and scale respectively. The loca-
tion CF model exploits ensembles of shallow and deep fea-
tures to determine the optimal position accurately. The s-
cale CF model works on multi-scale shallow features to es-
timate the optimal scale efﬁciently. Extensive experiments
on ﬁve recent benchmarks show that our tracker perform-
s favorably against many state-of-the-art algorithms, with
real-time performance of 28fps.

1. Introduction

Visual tracking [36, 25, 24] is a fundamental computer
vision problem and has many realistic applications includ-
ing video surveillance, behavior analysis, to name a few.
Although many efforts have been done, it is still a tough
task to design a robust and efﬁcient tracker due to the difﬁ-
culties from both foreground and background variations.

Recently, tracking algorithms based on correlation ﬁlters
(CF) have achieved top-ranked performance and drawn in-
creasing attentions. Usually, the CF-based trackers [18, 12,
11, 15, 8] exploit large numbers of cyclically shifted sam-
ples for learning, and convert the correlation operations in
the spatial domain to the element-wise multiplications in the
frequency domain, thereby reducing the computation com-
plexity and improving the tracking speed signiﬁcantly.

∗Corresponding Author: Dr. Wang

(a) SRDCF [11]

(b) ASRCF

Figure 1. The visualization of different spatial regularizations for
the (a) SRDCF [11] and (b) ASRCF methods. For SRDCF, the
spatial regularization has a negative Gaussian shape, which is al-
most equal for different objects and ﬁxed during the tracking pro-
cess. By contrast, our ASRCF method attempts to learn an adap-
tive spatial regularization, which is ﬂexible for different objects in
different time. As shown in (b), the ASRCF model has learned an
effective spatial regularization that provides a higher penalty on
the noise part and a lower penalty on the reliable part.

However, there exist two major imperfections of the ear-
lier CF-based methods. First, the circulant shifted sampling
process always suffers from periodic repetitions on bound-
ary positions and makes the CF model be trained with a por-
tion of unreal samples. This dilemma has been alleviated to
some extent with additional pre-deﬁned spatial constraints
on ﬁlter coefﬁcients [11, 15]. But these constraints are usu-
ally ﬁxed for different objects and not changed during the
tracking process, which cannot fully exploit the diversity
information of different objects in different time. Second,
the object localization and scale estimation are usually con-
ducted on the same feature space, which requires extract-
ing multi-scale feature maps during the tracking process.
This strategy signiﬁcantly increases the computational load
and decreases the tracking speeds when the tracker exploits
some powerful and complicated features (such as features
extracted from deep networks). That is why the top-ranked
CF trackers often runs very slow (e.g., DeepSRDCF [10],
C-COT [13], DRT [32] and RPCF [33]).

In this work, we develop a robust and efﬁcient CF-based
tracker with two major efforts: adaptive spatial regulariza-
tion and efﬁcient scale estimation. The contributions of this

4670

work can be summarized as follows.

First,

this work proposes a novel adaptive spatially-
regularized correlation ﬁlters (ASRCF) model, which could
effectively estimate an object-aware spatial regularization
(see Figure 1) and obtain more reliable ﬁlter coefﬁcients
during the tracking process. Our ASRCF is a general CF
model and the well-known KCF, SRDCF and BACF algo-
rithms are all its special cases.

Second, our ASRCF model can be effectively optimized
via the alternating direction method of multipliers (ADM-
M), where each subproblem has the analytic solution.

Third, our tracker effectively and efﬁciently estimates
both location and scale with two CF models: one exploits
complicated features for accurate localization; and the other
exploits shallow features for fast scale estimation.

Overall, our tracker achieves very remarkable perfor-
mance with a real-time speed on the OTB2015, TC128,
VOT2016, VOT2017 and LaSOT benchmarks.

2. Related Work

The trackers based on correlation ﬁlters (CF) have
achieved great success in recent years. We brieﬂy intro-
duce some relevant ones to highlight our motivations. The
MOSSE [3] method is the earliest CF-based tracker, which
uses only grayscale samples to train the ﬁlter. The CSK [17]
tracker introduces kernel trick into the CF formula. By ex-
ploiting circulant shifted samples, the ﬁlter coefﬁcients can
be efﬁciently optimized in the frequency domain. Based
on CSK [17], the KCF [18] method exploits multi-channel
HOG [7] features to enhance the feature representation a-
bility and improves the tracking performance signiﬁcant-
ly. Similarly, the color naming features are introduced to
achieve robust tracking in color videos [12]. The DSST [9],
SAMF [26] and IBCCF [23] trackers address the scale adap-
tation problem using multi-scale searching strategies.

The traditional CF methods rely on a periodic assump-
tion of the training and detection samples, which pro-
duces unexpected boundary effects and makes the tracker
be trained and applied on a portion of unreal samples. To
address this issue, Danelljan et al. [11] introduce a spatial
regularization term in the CF formulae to penalize the ﬁl-
ter coefﬁcients near the boundary regions. Galoogahi et
al. [15] directly multiply the ﬁlter with a binary matrix to
generate real positive and negative samples for model train-
ing. The aforementioned two spatial constraints are widely
used in subsequent research works [8, 13, 22, 32]. These
spatial constraints are usually ﬁxed for different objects and
not changed during the tracking process; thus, they cannot
fully exploit the diversity information of different objects in
different frames. In this wok, we propose a novel adaptive
spatial regularization term to make the tracker learn more
reliable ﬁlter coefﬁcients during the tracking process.

Recently, many researchers have attempted to combine
the CF model with deep visual features, making the CF-

based trackers achieve state-of-the-art performance [29, 8,
13, 22, 32]. Ma et al. [29] exploit three layers of CNN
features pre-trained on the classiﬁcation to generate feature
maps for training CF models. Danelljan et al. [13] use the
continuous convolution ﬁlters for combinations of feature
maps with different spatial resolutions. However, these CF-
based trackers no longer have the speed advantage due to
the complicated deep features. Particularly, their scale es-
timation strategies require extracting multi-scale deep fea-
tures, which is extremely expensive and makes the tracker
very slow. In this work, we exploit two kinds of CF models
to estimate the location and scale separately. The accurate
object localization is obtained based on one CF model only
with single scale robust deep features; while the efﬁcient s-
cale estimation is conducted with the other CF model with
multi-scale shallow features.

3. Adaptive Spatially-Regularized Correlation

Filters (ASRCF)

3.1. Objective Function of Our ASRCF Model
Original Correlation Filters (CF): The original multi-
channel CF model in the spatial domain aims to minimize
the following objective function [18]:

E(H) =

y −

K(cid:3)k=1

1

2(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)

xk ∗ hk(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)

2

2

+

λ
2

K(cid:3)k=1

(cid:3)hk(cid:3)2
2,

(1)

where xk ∈ RT×1 and hk ∈ RT×1 denote the k-th channel
of the vectorized image and ﬁlter respectively, and K is the
total channel number. The vector y ∈ RT×1 is the desired
response (i.e., the Gaussian-shaped ground truth), ∗ denotes
the spatial correlation operator and λ is a regularization con-
stant. H = [h1, h2, ..., hK] is the matrix representing the
ﬁlters from all K channels.

The original CF model suffers from periodic repetition-
s on boundary positions caused by circulant shifted sam-
ples, which inevitably degrades the tracking performance.
To solve this problem, several spatial constraints have been
introduced to alleviate the unexpected boundary effect-
s. The representative methods include spatially regular-
ized discriminative correlation ﬁlters (SRDCF) [11] and
background-aware correlation ﬁlters (BACF) [15]. Their
basic ideas are presented as follows.

SRDCF: The SRDCF method [11] introduces a spatial reg-
ularization to penalize the ﬁlter coefﬁcients with respect to
their spatial locations and modiﬁes the objective function as

E(H) =

y −

K(cid:3)k=1

1

2(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)

xk ∗ hk(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)

2

2

+

λ
2

K(cid:3)k=1

2, (2)

(cid:3)(cid:4)w ⊙ hk(cid:3)2

where (cid:4)w is a negative Gaussian-shaped spatial weight vec-

tor to make the learned ﬁlters have a high response around
the center of the tracked object.

4671

BACF: The BACF method [15] proposes a background-
aware CF and introduces the following objective function:

E(H) =

y −

K(cid:3)k=1

1

2(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)

xk ∗(cid:5)P⊤hk(cid:6)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)

2

2

+

λ
2

K(cid:3)k=1

(cid:3)hk(cid:3)2
2,

(3)
where P ∈ RT×T is a diagonal binary matrix to make the
correlation operator directly apply on the true foreground
and background samples.

The constraints on equations (2) and (3) are ﬁxed dur-
ing the tracking process and identical for different objects,
which cannot well reﬂect the characteristics and appearance
variations of a speciﬁc object. Thus, it is reasonable to intro-
duce an adaptive spatial regularization into the CF model.

Our Objective Function: Motivated by the discussion-
s above, we propose a novel adaptive spatially-regularized
correlation ﬁlters (ASRCF) method to learn effective multi-
channel CFs. Our objective function is deﬁned as follows:

E(H, w) =

1

2(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)

+

xk ∗ (P⊤hk)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)

K(cid:3)k=1
y −
K(cid:3)k=1
(cid:3)w ⊙ hk(cid:3)2

2 +

λ1
2

2

2

.

λ2
2 (cid:3)w − wr(cid:3)2

2

(4)
In equation (4), the ﬁrst term is the ridge regression ter-
m that convolves the training data X = [x1, x2, ..., xK]
with the ﬁlter H = [h1, h2, ..., hK] to ﬁt the Gaussian-
distributed ground truth y. The second term is a regular-
ization term introducing an adaptive spatial regularization
on the ﬁlter H, where the spatial weight w requires to be
optimized. The third term attempts to make the adaptive s-
patial weight w be similar to a reference weight wr. This
constraint introduces a priori information on w and avoids
model degradation1. λ1 and λ2 are the regularization pa-
rameters of the second and third terms, respectively.

We note that the proposed ASRCF is a general CF model
and the well-known KCF, SRDCF and BACF algorithms are
all special cases of our model (shown in Table 1).

Table 1. The generalization ability of our ASRCF model.

Method

P

w

KCF

P = I w = 1, λ2 = 0

SRDCF P = I w = (cid:4)w, λ2 = 0

w = 1, λ2 = 0

BACF

-

3.2. Optimization of Our ASRCF Model

Inspired by previous works [11, 15], correlation ﬁlter-
s are usually learned in the frequency domain for efﬁcient

1 If there is no third term, the solution of w will be degraded, i.e., w = 0.

4672

,

training and testing. Thus, we express the objective function
(4) in the frequency domain (using Parseval’s theorem), and
convert it into the equality constrained optimization form:

2

2

+ λ1
2

2
2 + λ2
2

2(cid:2)(cid:2)(cid:2)(cid:2)(cid:7)y −
K(cid:8)k=1(cid:7)x k ⊙(cid:7)g k(cid:2)(cid:2)(cid:2)(cid:2)
E(H, (cid:7)G , w) = 1
K(cid:8)k=1(cid:3)w ⊙ hk(cid:3)2
K(cid:8)k=1(cid:3)w − wr(cid:3)2
s.t., (cid:7)gk = √T FP⊤hk, k = 1, ..., K
where (cid:7)G = [(cid:7)g1,(cid:7)g2, ...,(cid:7)gK] ((cid:7)gk = √T FP⊤hk, k =

1, ..., K) is an auxiliary variable matrix.
In equation (5),
the symbol ˆ denotes the discrete Fourier transform form
of a given signal, and F is the orthonormal T × T matrix
of complex basis vectors to map any T dimensional vec-

torized signal into the Fourier domain (such as(cid:7)a = √T Fa,

a ∈ RT×1). The model in equation (5) is bi-convex, and can

be minimized to obtain a local optimal solution using the al-
ternating direction method of multipliers (ADMM) [4]. The
augmented Lagrangian form of equation (5) can be formu-
lated as

(5)

L(H, (cid:7)G , w, (cid:7)V)
= E(H, (cid:7)G , w) +

+ µ
2

K(cid:8)k=1(cid:2)(cid:2)(cid:2)(cid:7)g k − √T FP⊤hk(cid:2)(cid:2)(cid:2)

2

K(cid:8)k=1(cid:7)v ⊤k ((cid:7)g k − √T FP⊤hk)

2

,

(6)

where V = [v1, v2, ..., vK] ∈ RT×K is the Lagrange mul-
tiplier, and (cid:7)V = [(cid:7)v1,(cid:7)v2, ...,(cid:7)vK] ∈ RT×K is the corre-
sponding Fourier transform. By introducing sk = 1
µ vk
(k = 1, 2, ..., K), the optimization of equation (6) is equiv-
alent to solving equation (7) .

L(H, (cid:7)G, w,(cid:7)S )= 1

+ λ1
2

2

K(cid:8)k=1(cid:7)x k ⊙(cid:7)g k(cid:2)(cid:2)(cid:2)(cid:2)
2(cid:2)(cid:2)(cid:2)(cid:2)(cid:7)y −
K(cid:8)k=1(cid:3)w ⊙ hk(cid:3)2
K(cid:8)k=1(cid:2)(cid:2)(cid:2)(cid:7)g k − √T FP⊤hk +(cid:7)sk(cid:2)(cid:2)(cid:2)

2
2 + λ2

+ µ
2

2

2

2 (cid:3)w − wr(cid:3)2

2

,

(7)

solving the following subproblems:

Then, the ADMM algorithm is adopted by alternately

where(cid:7)S = [(cid:7)s1,(cid:7)s2, ...,(cid:7)sK] ∈ RT×K .
Subproblem H: If (cid:7)G, w and (cid:7)S are given, the optimal H∗

can be obtained as

⎧⎨
⎩

h∗k = arg min
hk

2 +

2 (cid:2)(cid:2)(cid:2)(cid:7)g k − √T FP⊤hk +(cid:7)sk(cid:2)(cid:2)(cid:2)

μT P (sk + gk)

2

2

µ

λ1

2 (cid:3)w ⊙ hk(cid:3)2
=(cid:15)λ1W⊤W + μT P⊤P(cid:16)−1
μT p ⊙ (sk + gk)
λ1(w ⊙ w) + μT p

=

⎫⎬
⎭

, (8)

where W = diag (w) ∈ RT×T represents the diagonal
matrix and p = [P11, P22, ..., PT T ]⊤ is the column vector
composed by the diagonal elements of the cropping matrix
P (For P, we also have P⊤P = P). Equation (8) shows
that the solution of hk merely requires the element-wise
multiplication and the inverse fast Fourier transform (i.e.,
sk = 1√T
tion complexities of solving hk and all H are O (T log T )
and O (KT log T ) respectively.

F⊤(cid:7)gk). Thus, the computa-

F⊤(cid:7)sk and gk = 1√T

Subproblem ˆG: If other variables in equation (7) are ﬁxed,
the optimal ˆG∗ can be estimated by solving the optimiza-
tion problem (9).

(cid:7)G∗ = arg min

(cid:2)G

⎧⎪⎪⎪⎨
⎪⎪⎪⎩

1

2

+

K(cid:8)k=1(cid:7)x k ⊙(cid:7)g k(cid:2)(cid:2)(cid:2)(cid:2)

2(cid:2)(cid:2)(cid:2)(cid:2)(cid:7)y −
K(cid:8)k=1(cid:2)(cid:2)(cid:2)(cid:7)g k − √T FP⊤hk +(cid:7)sk(cid:2)(cid:2)(cid:2)

µ
2

2

2

2

.

⎫⎪⎪⎪⎬
⎪⎪⎪⎭

(9)
However, it is difﬁcult to optimize the problem (9) due to
its high computation complexity. Thus, we consider pro-
cessing on all channels of each pixel, and reformulate the
optimization problem (9) as

2

1

+

Vj ( (cid:2)G )

⎧⎪⎨
⎪⎩

V∗j ((cid:7)G ) = arg min

2(cid:2)(cid:2)(cid:2)(cid:7)y j − Vj((cid:7)X )⊤Vj((cid:7)G )(cid:2)(cid:2)(cid:2)
K(cid:8)k=1(cid:2)(cid:2)(cid:2)Vj((cid:7)G ) + Vj((cid:18)M )(cid:2)(cid:2)(cid:2)
Vj( ˆM ) = Vj(ˆS ) − Vj(√T FP⊤H),

⎫⎪⎬
⎪⎭
where Vj(ˆg ) ∈ RK×1 denotes the values of all channels of
ﬁlter ˆg on pixel j. Then, the analytical solution of equation
(10) can be obtained as

(10)
(11)

µ
2

2

2

2

,

⊤

⊤

V∗j ((cid:7)G ) = 1

Vj ((cid:2)X )(cid:20)

µT (cid:19)I − Vj ((cid:2)X )Vj ((cid:2)X )
(cid:21)(cid:7)yjVj((cid:7)X )+μVj(√T FP⊤H) − μVj((cid:7)S )(cid:22) .

µT +Vj ((cid:2)X )

= A−1− A−1uv⊤A−1

v are two column vectors and uv⊤ is a rank-one matrix).

(12)
The derivation of equation (12) uses the Sherman Morrsion
1+v⊤A−1 u (here u and

formula:(cid:5)A + uv⊤(cid:6)−1
Solving w: If H, (cid:7)G and(cid:7)S are ﬁxed, the closed-form solu-
2(cid:24)
2 (cid:3)w − wr(cid:3)2

tion regrading w can be determined as

w∗ = arg min

2 + λ2

K(cid:8)k=1(cid:3)Nkw(cid:3)2

=(λ1

N⊤k Nk+λ2I)−1λ2wr

,

2

w (cid:23) λ1
K(cid:8)k=1

λ2wr

=

λ1

K(cid:3)

k=1

hk⊙hk+λ2 1

(13)
where Nk = diag(hk) ∈ RT×T . In practice, we utilize an
additional ADMM solver to obtain the weight w∗ for better

convergence. Some representative examples of the learned
weights are shown in Figure 2. From this ﬁgure, we can see
that the adaptive spatial regularization learning works well
in introducing large penalties on some unreliable regions,
thereby encouraging the learned ﬁlters to focus more on the
reliable regions of the tracked object in the next iteration.

Figure 2. The visualization of the adaptive spatial regularization.
For each pixel, a larger value of the adaptive spatial regularization
will give a greater learning penalty of the ﬁlter at this pixel. Better
viewed in color and zoom in for details.

Lagrangian Multiplier Update: We update Lagrangian
multipliers as

(cid:7)Si+1 = (cid:7)Si + (cid:7)Gi+1 − (cid:7)Hi+1,

(14)

where (cid:7)Si denotes the Fourier transform of the Lagrangian

in the previous state, ˆG(i+1) and ˆH(i+1) are the current
solutions to the two subproblems above at iteration i + 1.
The regularization constant μ is commonly set as μ(i+1) =
min(μmax, βμ(i)) [4].

Thus, the optimization process can be conducted by it-
eratively applying the four steps above, including (1) solv-

grangian multipliers. After convergence, the optimal ﬁlter

ing H, (2) solving (cid:7)G, (3) solving w and (4) updating La-
parameter H∗ (with its Fourier transform (cid:7)G∗) and spatial

regularization weight w∗ can be obtained.

4. Object Localization and Scale Estimation
4.1. Object Localization

For tracking, the location of the tracked object can be

determined in the Fourier domain as

K(cid:3)k=1(cid:7)xk ⊙(cid:7)gk,

(cid:7)r =

(15)

where r and(cid:7)r denote the response map and its Fourier trans-

form. In this work, we adopt ensembles of deep and shallow
features for object localization (see implementation details
in Section 5). After obtaining the response map, the optimal
location can be obtained based on the maximum response.

4673

4.2. Model Update

Similar to other CF-based trackers [18, 11, 15], we train
our ﬁlters with an online adaptive template scheme. The
adaptive method of the template model is as follow:

(cid:7)Xnew
model = (1 − η)(cid:7)Xold

model + η(cid:7)X∗,

(16)

model represents the newly updated template mod-

where ˆX(new)
el, ˆX(old)
model is the old template model and (cid:7)X∗ denotes the
current observation (η is the online learning rate). In the
meanwhile, we update the reference weight as wr ← w∗.
Similar to [11], the reference weight wr is initialized with a
negative Gaussian shape in the ﬁrst frame. We note that the
aforementioned update schemes make our model effective-
ly adapt to the appearance variations of the tracked object
and introduce a more reasonable priori for adaptive spatial
regularization during the tracking process.

4.3. Scale Estimation

For scale estimation,

the previous CF-based tracker-
s [18, 11, 15] usually apply the learned ﬁlter on multiple res-
olutions of the searching area to estimate scale changes, and
then select the optimal scale with the maximum response.
This manner leads to two imperfections for the CF-based
model with deep features: (1) it is very time-consuming to
extract multi-scale deep visual features; and (2) it is difﬁcult
to estimate the accurate scale based on deep CNN features
since the pooling layers make feature descriptions loss some
detailed information.

In this work, we attempt to learn two CF models (one
location CF is for object localization and the other scale CF
is for scale estimation). The location CF model for objec-
t localization is trained on ensembles of deep and shallow
features. Although the extraction process of this CF model
is time-consuming, it merely requires to be extracted on one
scale search region during the tracking process. The scale
CF model for scale estimation is trained on efﬁcient shallow
features (HOG features in this work). During the tracking
process, we apply this CF model on ﬁve scale search regions
and obtain their related response maps. Then, the best scale
is determined based on the scale corresponding to the max-
imum score of ﬁve response maps. The effectiveness of our
designed scale estimation scheme is veriﬁed in Section 5.2.
In every frame, the overall framework (Figure 4.3) ﬁrst
estimates the position using the location CF model with
complicated features, and then applies the scale CF mod-
el to reﬁne the scale based on ﬁve scale HOG feature maps.

5. Experiments

is

Our

tracker

implemented based on the MAT-
LAB2017a platform with the MatConvNet toolbox, and
runs on a PC machine with an Intel i7 8700 CPU, 32GB
RAM and a single NVIDIA GTX 1080Ti GPU, 11G mem-

Update 

CF1

Five Scale 

HOG feature 

maps

CF1 for 
CF1 for

estimating

scale

Best
Scale

ADMM1

One Scale 

Fusion 

feature map

CF2 for 
predicting
position

Best

position

ADMM2

Update 

CF2

Figure 3. The tracking framework of location and scale CF models.

ory. The tracking speed of our tracker is 28fps approxi-
mately, which makes our tracker meet the real-time require-
ment. For localization, we exploit an ensemble of deep
(Norm1 from VGG-M, Conv4-3 from VGG-16) and hand-
crafted (HOG) features for object representation. Besides,
we merely use ﬁve-scale HOG features for scale estima-
tion. The regularization parameters λ1 and λ2 are empir-
ically chosen as λ1= 0.2 and λ2= 0.001, respectively. We
set the learning rates of our ASRCF model as η = 0.0186,
and use three-step iterations for the ADMM optimization
process. The penalty factor μ of ADMM is initially set to
1 and then updated by μ(i+1) = min(μmax, βμ(i)), where
β = 10 and μmax = 104. Our project is available on the
website: http://github.com/Daikenan/ASRCF.

In this section, we demonstrate the effectiveness of our
tracker on the OTB2015 [35], TC128 [27] VOT2016 [19],
VOT2017 [20] and LaSOT [14] datasets.

5.1. Quantitative Evaluation

OTB2015 Dataset. The OTB2015 [35] dataset is one of the
most popular tracking benchmarks which consists of 100
challenging image sequences with 11 different attributes,
such as illumination variation (IV), scale variation (SV),
occlusion (OCC), deformation (DEF), motion blur (MB),
fast motion (FM), in-plane Rotation (IPR), out-of-plane ro-
tation (OPR), out-of-view (OV), background clutters (BC)
and low resolution (LR). The one pass evaluation (OPE) is
employed to evaluate different trackers based on two crite-
ria: distance precision and overlap success.

We compare our tracker against recent state-of-the-art
trackers including ECO [8], MDNet [30], LSART [31], C-
COT [13], DaSiamRPN [39], SiamRPN [21], DeepSRD-
CF [10], ACT [5], BACF [15], StructSiam [38], CF2 [29],
SRDCF [11], SiamFC [2], Staple [1], CFNet [1] and
KCF [18]. Figure 4 reports both precision and success plots
of different trackers in terms of the OPE rule. Overall, the
proposed tracking algorithm achieves almost the best result
with an AUC score of 0.692 and a distance precision rate
of 0.922.
In Table 2, we summarize both accuracies and
speeds of top-5 trackers on OTB2015. Among these top-

4674

ranked methods, our tracker achieves almost the best accu-
racy and the fastest speed (the only tracker with real-time
performance).

e

t

a
r
 
s
s
e
c
c
u
S

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Success plots of OPE

Ours [0.692]
ECO [0.687]
MDNet [0.678]
LSART [0.672]
C-COT [0.667]
DaSiamRPN [0.658]
SiamRPN [0.637]
DeepSRDCF [0.635]
ACT [0.625]
BACF [0.621]
Struct_siam [0.621]
CF2 [0.603]
SRDCF [0.598]
SiamFC [0.582]
Staple [0.578]
CFNet [0.568]
KCF [0.477]

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Overlap threshold

i

i

n
o
s
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Precision plots of OPE

LSART [0.923]
Ours [0.922]
MDNet [0.909]
ECO [0.909]
C-COT [0.896]
DaSiamRPN [0.881]
ACT [0.859]
Struct_siam [0.851]
DeepSRDCF [0.851]
SiamRPN [0.851]
CF2 [0.845]
BACF [0.824]
SRDCF [0.789]
Staple [0.784]
SiamFC [0.771]
CFNet [0.748]
KCF [0.696]

10

20

30

40

50

Location error threshold

Figure 4. Precision and success plots on OTB2015 [35]. The leg-
end contains the average distance precision score at 20 pixels and
the area-under-the-curve (AUC) score for each tracker.

Table 2. Accuracy and speed comparisons of top-5 trackers on the
OTB2015 dataset. The best two results are shown in red and blue
fonts, respectively.

C-COT

LSART MDNet

ECO

Ours

Success
Precision
GPU/CPU

FPS

0.667
0.896
CPU
0.7

0.672
0.923
GPU
1.3

0.678
0.909
GPU
1.7

0.687
0.909
GPU
17.9

0.692
0.922
GPU
28.0

Figure 6 illustrates overlap success plots of differen-
t trackers with 6 attributes (such as background clutter, de-
formation, occlusion, scale variation and so on). We can
see that our tracker achieves almost the best performance
in these attributes. First, our tracker performs well under
background clutter, deformation and occlusion conditions,
and obtains 1.6%, 1.6% and 0.8% gain respectively than the
second best tracker (ECO [8]). This is mainly owed to the
proposed adaptive spatial regularization, which makes the
learned ﬁlter focus on the reliable features of the tracked
object and alleviate the effects of unexpected noises within
the object region. In addition, our tracker works well in han-
dling scale variation based on the designed scale estimation
scheme using multi-scale shallow features.
TC128 Dataset. We perform comparisons on the
TC128 [27] dataset, which consists of 128 challenging color
sequences. We compare our tracker with 8 state-of-the-art
trackers including ECO [8], C-COT [13], SRDCF [11], S-
RDCFdecon, DeepSRDCF [10], MCCT [34], BACF [15],
MCPF [37] and 32 more default trackers in TC128. The re-
sults of top 15 trackers are reported in Figure 7, from which
we can see that the proposed tracker performs the best in
terms of both precision and success criterion.
VOT2016 Dataset. We also perform comparisons on the
VOT2016 dataset [19] which contains 60 challenging se-
quences. During the test phase, the tracker will be reset if
there is no overlap between prediction and groundtruth. The
expected average overlap (EAO) considering both bounding
box overlap (accuracy) and reset times (robustness) serves
as the major evaluation metric on VOT2016. In Table 3(a),

e
t
a
r
 
s
s
e
c
c
u
S

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Success plots of OPE

Ours [0.603]
ECO [0.597]
MCCT [0.586]
C-COT [0.566]
ECO-HC [0.555]
MCPF [0.544]
DeepSRDCF [0.536]
SRDCFdecon [0.534]
SRDCF [0.509]
BACF [0.495]
MEEM [0.458]
Struck [0.441]
KCF [0.384]
ASLA [0.380]
Frag [0.374]

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

Precision plots of OPE

Ours [0.825]
ECO [0.800]
MCCT [0.799]
MCPF [0.776]
C-COT [0.774]
ECO-HC [0.740]
DeepSRDCF [0.740]
SRDCFdecon [0.729]
SRDCF [0.696]
BACF [0.660]
MEEM [0.641]
Struck [0.614]
KCF [0.551]
ASLA [0.516]
LOT [0.490]

i

i

n
o
s
c
e
r
P

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0

5

10

Overlap threshold

20

15

35
Location error threshold

25

30

40

45

50

Figure 7. Performance evaluation on the TC128 dataset in terms
of success and precision plots.

we compare our method with top-10 trackers including C-
COT, TCNN, SSAT, MLDF, Staple, DDC, EBT, SRBT,
STAPLE+ and DNT. Table 3 shows that our tracker achieves
the best in terms of EAO and R scores, furthermore, our
tracker is much faster than the second best tracker (C-COT).
VOT2017 Dataset. The VOT2017 [20] dataset contains 60
challenging sequences (replacing some simple sequences
with more difﬁcult ones in VOT2016) and has more ac-
curate groundtruth. The evaluation criteria in VOT2017
is same as that in VOT2016.
In Table 3(b), we compare
our method with top-10 trackers in the VOT2017 [20] of-
ﬁcial report. The compared trackers include LSART [31],
ECO [8], CFCF [16], GNet, MCCT [34], C-COT [13], C-
SRDCF [28], SiamDCF, MCPF [37] and CRT [6]. Table
3(b) shows that our tracker achieves the best performance
in terms of EAO while maintaining very competitive A and
R scores. As presented before, our tracker is much faster
than the second best tracker (LSART).
LaSOT Dataset. The LaSOT [14] dataset is a recent large-
scale dataset with 1,400 sequences and more than 3.5M
frames in total (the average frame length is more than 2,500
frames). We also follow the one-pass evaluation to compare
different trackers based on three criteria (precision, normal-
ized precision and success). The success and precision plots
are reported to compare the proposed trackers with 34 track-
ers reported in [14]. We refer the readers to [14] for more
detailed descriptions. Figure 8 shows that our tracker also
achieves very competitive results, especially better than all
CF-based methods (e.g., ECO [8] and STRCF [22]).

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e

t

a
r
 
s
s
e
c
c
u
S

Success plots of OPE on LaSOT

[0.413] MDNet
[0.412] VITAL
[0.359] Ours
[0.358] SiamFC
[0.356] StructSiam
[0.353] DSiam
[0.340] ECO
[0.339] SINT
[0.315] STRCF
[0.311] ECO_HC
[0.296] CFNet
[0.285] TRACA
[0.280] MEEM
[0.277] BACF
[0.272] HCFT
[0.271] SRDCF
[0.269] PTAV
[0.266] Staple
[0.263] CSRDCF
[0.262] Staple_CA
[0.258] SAMF
[0.246] LCT
[0.234] Struck
[0.233] DSST
[0.232] fDSST
[0.228] TLD
[0.214] SCT4
[0.211] ASLA
[0.211] KCF
[0.186] CN
[0.178] CT
[0.172] CSK
[0.168] L1APG
[0.163] MIL
[0.151] STC

Precision plots of OPE on LaSOT

0.6

0.5

0.4

0.3

0.2

0.1

[0.374] MDNet
[0.372] VITAL
[0.341] SiamFC
[0.340] StructSiam
[0.337] Ours
[0.329] DSiam
[0.299] SINT
[0.298] ECO
[0.292] STRCF
[0.272] ECO_HC
[0.265] CFNet
[0.250] HCFT
[0.243] PTAV
[0.239] BACF
[0.237] TRACA
[0.231] Staple
[0.231] CSRDCF
[0.231] Staple_CA
[0.227] SRDCF
[0.224] MEEM
[0.214] SAMF
[0.196] DSST
[0.193] LCT
[0.192] fDSST
[0.186] SCT4
[0.185] Struck
[0.184] KCF
[0.184] TLD
[0.170] ASLA
[0.158] CN
[0.154] L1APG
[0.142] STC
[0.136] CSK
[0.131] IVT
[0.102] CT

i

i

n
o
s
c
e
r
P

0

0

0.1

0.2

0.3

0.4

0.6
Overlap threshold

0.5

0.7

0.8

0.9

1

0

0

5

10

15

20

25

30

35

40

45

50

Location error threshold

Figure 8. Performance evaluation on the LaSOT dataset in terms
of success and precision plots.

4675

Ours

BACF [15]

C-COT [13]

ECO [8]

CF2 [29]

Figure 5. Qualitative evaluation of our tracker and related algorithms on the Bird1, Biker, Freeman4, Human3 and Singer2 sequences.

Success plots of OPE - background clutter (32)

1

e

t

a
r
 
s
s
e
c
c
u
S

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Ours [0.693]
ECO [0.677]
MDNet [0.671]
C-COT [0.657]
LSART [0.652]
DaSiamRPN [0.639]
ACT [0.629]
DeepSRDCF [0.627]
BACF [0.621]
CF2 [0.601]
SiamRPN [0.598]
Struct_siam [0.585]
SRDCF [0.580]
Staple [0.560]
CFNet [0.550]
SiamFC [0.524]
KCF [0.493]

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

e

t

a
r
 
s
s
e
c
c
u
S

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Success plots of OPE - deformation (44)

Ours [0.660]
MDNet [0.649]
DaSiamRPN [0.645]
ECO [0.644]
LSART [0.640]
SiamRPN [0.622]
C-COT [0.607]
BACF [0.582]
ACT [0.578]
Struct_siam [0.571]
DeepSRDCF [0.566]
CF2 [0.563]
Staple [0.550]
SRDCF [0.544]
SiamFC [0.506]
CFNet [0.473]
KCF [0.436]

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Overlap threshold

Overlap threshold

Success plots of OPE - scale variation (65)

Success plots of OPE - out-of-plane rotation (62)

1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e

t

a
r
 
s
s
e
c
c
u
S

ECO [0.674]
Ours [0.663]
MDNet [0.661]
C-COT [0.656]
LSART [0.653]
DaSiamRPN [0.639]
SiamRPN [0.625]
DeepSRDCF [0.609]
ACT [0.605]
Struct_siam [0.605]
BACF [0.579]
SRDCF [0.565]
SiamFC [0.557]
CF2 [0.554]
CFNet [0.539]
Staple [0.525]
KCF [0.399]

e

t

a
r
 
s
s
e
c
c
u
S

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Ours [0.678]
ECO [0.677]
MDNet [0.664]
LSART [0.653]
DaSiamRPN [0.646]
C-COT [0.643]
SiamRPN [0.633]
DeepSRDCF [0.607]
ACT [0.607]
Struct_siam [0.595]
BACF [0.586]
CF2 [0.572]
SiamFC [0.557]
SRDCF [0.550]
CFNet [0.541]
Staple [0.533]
KCF [0.455]

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

e

t

a
r
 
s
s
e
c
c
u
S

e

t

a
r
 
s
s
e
c
c
u
S

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Success plots of OPE - occlusion (49)

Ours [0.678]
ECO [0.670]
C-COT [0.656]
LSART [0.652]
MDNet [0.646]
DaSiamRPN [0.611]
Struct_siam [0.602]
DeepSRDCF [0.601]
SiamRPN [0.592]
ACT [0.585]
BACF [0.576]
CF2 [0.571]
SRDCF [0.559]
SiamFC [0.543]
Staple [0.542]
CFNet [0.516]
KCF [0.443]

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Overlap threshold

Success plots of OPE - out of view (14)

Ours [0.666]
ECO [0.642]
LSART [0.632]
MDNet [0.627]
C-COT [0.624]
Struct_siam [0.555]
DeepSRDCF [0.553]
BACF [0.552]
SiamRPN [0.550]
DaSiamRPN [0.537]
ACT [0.536]
SiamFC [0.506]
CF2 [0.488]
Staple [0.476]
SRDCF [0.460]
CFNet [0.414]
KCF [0.393]

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Overlap threshold

Overlap threshold

Overlap threshold

Figure 6. Evaluation of different trackers with 6 attributes on the OTB-2015 [35] dataset. The legend contains the average distance precision
score at 20 pixels and the area-under-the-curve score for each tracker.

4676

Table 3. Performance evaluation on the VOT2016 [19] and VOT2017 [20] datasets. In this table, we compare our method with top-10
trackers in the VOT2016 [19] and VOT2017 [20] ofﬁcial reports. The results are presented in terms of expected average overlap (EAO),
accuracy rank (A) and robustness rank (R). The best three results are shown in red, blue and green colors, respectively.

DNT
0.278
0.515
0.329

STAPLE+

0.286
0.557
0.368

SRBT
0.290
0.496
0.350

EBT
0.291
0.465
0.252

DDC
0.293
0.541
0.345

Staple MLDF
0.295
0.311
0.490
0.544
0.233
0.378

SSAT
0.321
0.577
0.291

TCNN
0.325
0.554
0.268

C-COT

0.331
0.539
0.238

Ours
0.391
0.563
0.187

(a) VOT 2016

MCPF
0.248
0.510
0.427

SiamDCF

CSRDCF

C-COT

0.249
0.500
0.473

0.256
0.491
0.356

0.267
0.494
0.318

MCCT
0.270
0.525
0.323

GNet
0.274
0.502
0.276

ECO
0.280
0.483
0.276

CFCF
0.286
0.509
0.281

CFWCR

0.303
0.484
0.267

LSART Ours
0.328
0.494
0.234

0.323
0.493
0.218

(b) VOT 2017

EAO

A
R

EAO

A
R

5.2. Ablation Studies

Effectiveness of Different Components. We conduct the
ablation studies to verify the effectiveness of key compo-
nents in our tracker, and report the comparison results in
Figure 9(a). The basic notions are as follows. (1) ‘Baseline’
denotes the method that does not exploit the adaptive spatial
regularization and the designed scale estimation scheme.
(2) ‘Baseline+AR’ means the baseline method with adding
the adaptive spatial regularization. (3) ‘Baseline+MSS’ s-
tands for the baseline method replacing multi-scale estima-
tion on the original feature space with multi-scale estima-
tion on shallow features. (4) ‘Baseline+AR+MSS’ is our
ﬁnal tracker that combines the baseline method with both
adaptive spatial regularization and multi-scale estimation on
shallow features. From Figure 9(a), we can see that both
adaptive spatial regularization and designed scale estima-
tion scheme contribute to the substantial improvement over
the baseline method. Besides, our ﬁnal tracker improves the
baseline method by 7.1% and 6.5% in terms of success and
precision criterion, respectively.

e
t
a
r
 
s
s
e
c
c
u
S

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Success plots of OPE

Baseline+AR+MSS [0.692]
Baseline+AR [0.665]
Baseline+MSS [0.659]
Baseline [0.620]

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

e
t
a
r
 
s
s
e
c
c
u
S

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Success plots of OPE

Ours(ASRCF) [0.692]
Ours(BACF) [0.663]
Ours(SRDCF) [0.639]
Ours(KCF) [0.555]

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Overlap threshold

Overlap threshold

(a)

(b)

Figure 9. Ablation analysis using the OTB-2015 [35] Dataset.

Different Variants of Our ASRCF Model.
In Table 1,
we point out the well-known KCF [18], SRDCF [11] and
BACF [15] methods are all special cases of our ASRCF
model. To show the effectiveness of ASRCF, we compare
it with those special cases with the same feature extraction

and scale estimation scheme2. From Figure 9(b), we can see
that the SRDCF and BACF methods performs better than
the original KCF algorithm, which can be attributed to the
adopted spatial constraints on ﬁlter coefﬁcients. Our ASR-
CF model provides an adaptive spatial regularization, which
makes the tracker achieve the best results.

6. Conclusions

In this work, we attempt to introduce an adaptive spatial
regularization into the objective function of correlation ﬁl-
ters (denoted as ASRCF). Compared with previous works
using ﬁxed spatial constraints, this regularization could be
effectively learned with respect to a speciﬁc object being
tracked and updated to consider the appearance variations
during the tracking process. Our ASRCF model is effective-
ly optimized using the ADMM algorithm, which can learn
the reliable ﬁlter coefﬁcients and therefore make our tracker
robust. To speed up our tracker, we exploit two CF mod-
els to estimate the location and scale separately. One CF
model with complicated features is responsible for accurate
localization. The other CF model with multi-scale shallow
features is aimed to accelerate scale estimation. Extensive
experimental results show that our ASRCF tracker performs
signiﬁcantly better than many state-of-the-art tracking algo-
rithms, with a real-time speed of 28fps.

Acknowledgement. This paper is supported in part by Na-
tional Natural Science Foundation of China Nos. 61872056,
61771088, 61751212, 61725202 and 61829102, and in part
by the Fundamental Research Funds for the Central Uni-
versities under Grant No. DUT18JC30. This work is also
sponsored by CCF-Tencent Open Research Fund.

2 In the original source codes, the adopted features and scale estimation man-
ners of KCF, SRDCF, BACF and our methods are different; thus, direct
comparisons among them are not fair. Here, we adopt the same feature
extraction and scale estimation scheme for fair evaluation.

4677

References

[1] Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej
Miksik, and Philip H. S. Torr. Staple: Complementary learn-
ers for real-time tracking. In CVPR, 2016.

[2] Luca Bertinetto, Jack Valmadre, Jo˜ao F Henriques, Andrea
Vedaldi, and Philip HS Torr. Fully-convolutional siamese
networks for object tracking. In ECCVW, 2016.

[3] David S. Bolme, J. Ross Beveridge, Bruce A. Draper, and
Yui Man Lui. Visual object tracking using adaptive correla-
tion ﬁlters. In CVPR, 2010.

[4] Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and
Jonathan Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers.
Foundations and Trends in Machine Learning, 3(1):1–122,
2011.

[5] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and
Huchuan Lu. Real-time ‘Actor-Critic’ tracking. In ECCV,
2018.

[6] Kai Chen and Wenbing Tao. Convolutional regression for
IEEE Transcations on Image Processing,

visual tracking.
27(7):3611–3620, 2018.

[7] Navneet Dalal and Bill Triggs. Histograms of oriented gra-

dients for human detection. In CVPR, 2005.

[8] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Michael Felsberg. ECO: efﬁcient convolution operators for
tracking. In CVPR, 2017.

[9] Martin Danelljan, Gustav H¨ager, Fahad Shahbaz Khan, and
Michael Felsberg. Accurate scale estimation for robust visu-
al tracking. In BMVC, 2014.

[10] Martin Danelljan, Gustav H¨ager, Fahad Shahbaz Khan, and
Michael Felsberg. Convolutional features for correlation ﬁl-
ter based visual tracking. In ICCVW, 2015.

[11] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and
Michael Felsberg. Learning spatially regularized correlation
ﬁlters for visual tracking. In ICCV, 2015.

[12] Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg,
and Joost van de Weijer. Adaptive color attributes for real-
time visual tracking. In CVPR, 2014.

[13] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,
and Michael Felsberg. Beyond correlation ﬁlters: Learning
continuous convolution operators for visual tracking. In EC-
CV, 2016.

[14] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia
Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.
LaSOT: A high-quality benchmark for large-scale single ob-
ject tracking. CoRR, abs/1809.07845, 2018.

[15] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.
Learning background-aware correlation ﬁlters for visual
tracking. In ICCV, 2017.

[16] Erhan Gundogdu and A. Aydin Alatan. Good features to
IEEE Transcations on Image

correlate for visual tracking.
Processing, 27(5):2526–2540, 2018.

[18] Joao F. Henriques, Caseiro Rui, Pedro Martins, and Jorge
Batista. High-speed tracking with kernelized correlation ﬁl-
ters.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 37(3):583–596, 2015.

[19] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg,
Roman P. Pﬂugfelder, Luka Cehovin, Tom´as Voj´ır, Gustav
H¨ager, Alan Lukezic, and Gustavo Fern´andez. The visual ob-
ject tracking VOT2016 challenge results. In ECCVW, 2016.
[20] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg,
Roman P. Pﬂugfelder, Luka Cehovin Zajc, Tomas Vojir, and
Gustav H¨ager. The visual object tracking VOT2017 chal-
lenge results. In ICCVW, 2017.

[21] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.
High performance visual tracking with siamese region pro-
posal network. In CVPR, 2018.

[22] Feng Li, Cheng Tian, Wangmeng Zuo, Lei Zhang, and Ming-
Hsuan Yang. Learning spatial-temporal regularized correla-
tion ﬁlters for visual tracking. In CVPR, 2018.

[23] Feng Li, Yingjie Yao, Peihua Li, David Zhang, Wangmeng
Zuo, and Ming-Hsuan Yang. Integrating boundary and center
correlation ﬁlters for visual tracking with aspect ratio varia-
tion. In ICCVW, 2017.

[24] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep
visual tracking: Review and experimental comparison. Pat-
tern Recognition, 76:323–338, 2018.

[25] Xi Li, Weiming Hu, Chunhua Shen, Zhongfei Zhang, Antho-
ny R. Dick, and Anton van den Hengel. A survey of appear-
ance models in visual object tracking. ACM Transactions on
Intelligent Systems and Technology, 4(4):58:1–58:48, 2013.
[26] Yang Li and Jianke Zhu. A scale adaptive kernel correlation

ﬁlter tracker with feature integration. In ECCVW, 2014.

[27] Pengpeng Liang, Erik Blasch, and Haibin Ling.

En-
coding color information for visual tracking: Algorithms
and benchmark.
IEEE Transcations on Image Processing,
24(12):5630–5644, 2015.

[28] Alan Lukezic, Tomas Vojir, Luka Cehovin Zajc, Jiri Matas,
and Matej Kristan. Discriminative correlation ﬁlter with
channel and spatial reliability. In CVPR, 2017.

[29] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan
Yang. Hierarchical convolutional features for visual tracking.
In ICCV, 2015.

[30] Hyeonseob Nam and Bohyung Han. Learning multi-domain
convolutional neural networks for visual tracking. In CVPR,
2016.

[31] Chong Sun, Huchuan Lu, and Ming-Hsuan Yang. Learning
spatial-aware regressions for visual tracking. In CVPR, 2018.
[32] Chong Sun, Dong Wang, Huchuan Lu, and Ming-Hsuan
Yang. Correlation tracking via joint discrimination and re-
liability learning. In CVPR, 2018.

[33] Yuxuan Sun, Chong Sun, Dong Wang, You He, and Huchuan
In

Lu. Roi pooled correlation ﬁlters for visual tracking.
CVPR, 2019.

[34] Ning Wang, Wengang Zhou, Qi Tian, Richang Hong, Meng
Wang, and Houqiang Li. Multi-cue correlation ﬁlters for ro-
bust visual tracking. In CVPR, 2018.

[17] Jo˜ao F. Henriques, Rui Caseiro, Pedro Martins, and Jorge P.
Batista. Exploiting the circulant structure of tracking-by-
detection with kernels. In ECCV, 2012.

[35] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-
ing benchmark. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 37(9):1834–1848, 2015.

4678

[36] Alper Yilmaz, Omar Javed, and Mubarak Shah. Object track-

ing: A survey. ACM Computing Surveys, 38(4):13, 2006.

[37] Tianzhu Zhang, Changsheng Xu, and Ming-Hsuan Yang.
Multi-task correlation particle ﬁlter for robust object track-
ing. In CVPR, 2017.

[38] Yunhua Zhang, Lijun Wang, Jinqing Qi, Dong Wang,
Mengyang Feng, and Huchuan Lu. Structured siamese net-
work for real-time visual tracking. In ECCV, 2018.

[39] Zheng Zhu, Qiang Wang, Li Bo, Wei Wu, Junjie Yan, and
Weiming Hu. Distractor-aware siamese networks for visual
object tracking. In ECCV, 2018.

4679

