ContextDesc: Local Descriptor Augmentation with Cross-Modality Context

Zixin Luo1
Yao Yao1

∗

,

Tianwei Shen1

Shiwei Li1

Lei Zhou1
,†
Tian Fang3

,*

Jiahui Zhang2
Long Quan1

1Hong Kong University of Science and Technology

2Tsinghua University

3Shenzhen Zhuke Innovation Technology (Altizure)

{zluoag,tshenaa,lzhouai,yyaoag,slibc,quan}@cse.ust.hk

jiahui-z15@mails.tsinghua.edu.cn

fangtian@altizure.com

Abstract

Most existing studies on learning local features focus
on the patch-based descriptions of individual keypoints,
whereas neglecting the spatial relations established from
their keypoint locations. In this paper, we go beyond the lo-
cal detail representation by introducing context awareness
to augment off-the-shelf local feature descriptors. Speciﬁ-
cally, we propose a uniﬁed learning framework that lever-
ages and aggregates the cross-modality contextual infor-
mation, including (i) visual context from high-level image
representation, and (ii) geometric context from 2D keypoint
distribution. Moreover, we propose an effective N-pair
loss that eschews the empirical hyper-parameter search
and improves the convergence. The proposed augmenta-
tion scheme is lightweight compared with the raw local fea-
ture description, meanwhile improves remarkably on sev-
eral large-scale benchmarks with diversiﬁed scenes, which
demonstrates both strong practicality and generalization
ability in geometric matching applications. [code release]

1. Introduction

Designing powerful local feature descriptor is a funda-
mental problem in applications such as panorama stitch-
ing [21], wide-baseline matching [24, 54, 55], image re-
trieval [27] and structure-from-motion (SfM) [57, 39, 52,
56]. Despite the recent notable achievements, the perfor-
mance of state-of-the-art learned descriptors is observed to
be somewhat saturated on standard benchmarks. As shown
in Fig. 1a, due to repetitive patterns, the matching algorithm
often ﬁnds false matches as nearest neighbors that are vi-
sually indistinguishable from groundtruth, unless validated
by geometry. Essentially, such visual ambiguity may not be
easily resolved given only local information. In this spirit,

*Interns at Shenzhen Zhuke Innovation Technology (Altizure).
†Corresponding author.

(a)

(b)

Figure 1: (a) Saturated results on standard benchmark [2]
by a recent method [23]. The search of nearest neigh-
bors (NN) returns false matches though visually similar to
groundtruth (GT), indicating the limitation of relying on
only local visual information. (b) 2D keypoints distribute
structurally, on which we human beings are capable of es-
tablishing coarse matches even without color information.

we seek to enhance the local feature description with extra
prior knowledge, which we refer to as introducing context
awareness to augment local feature descriptors.

As a common practice, a multi-scale-like architecture
can help to capture visual context of different levels, which
is referred to as multi-scale domain aggregation by DSP-
SIFT [8] and adopted by recent learned descriptors [50,
19, 43]. Beside of the challenge on selecting proper do-
main sizes, a na¨ıve multi-scale implementation may cost
excessive computation such as doubled inference time and
doubled feature dimensionality [50, 19, 43]. Seeking for
more reasonable accuracy-efﬁciency trade-offs, we instead
resort to well-studied high-level image representation, e.g.,
the regional representation used by image retrieval stud-

2527

Ref.NNGTRef.NNGTies [33, 38] which essentially incorporates rich image con-
text. Thereby, we strive to effectively combine the local
feature description and off-the-shelf visual understandings
so as to go beyond the local detail representation.

In addition, it would be interesting to exploit context in
other modality. In particular, as shown in Fig. 1b, since key-
point is principally designed to be repeatable in the same
underlying scene, its distribution thus reveals comprehen-
sive scene structure that allows we human beings to estab-
lish coarse matches even without color information, which
further enables us to explore geometric context formed by
the spatial relations of keypoints to help to alleviate the vi-
sual ambiguity of local descriptions.

Thus far, we have discussed two context candidates, re-
ferred to as visual context and geometric context that in-
corporate high-level visual representation over the image
and geometric cues from 2D keypoint distribution, respec-
tively. Instead of learning a completely new descriptor, in
the present work, we target to ﬂexibly leverage the above
context awareness to augment off-the-shelf local descrip-
tors without altering their dimensionality, in which process
we consider the key challenges threefold:

• A proper integration of geometric local feature and se-
mantic high-level representation. As keypoint description
requires sub-pixel accuracy, the integration is not sup-
posed to obscure the raw representation of local details.

• The instability of 2D keypoint distribution. Due to image
appearance changes, keypoint distribution often suffers
from substantial variations of sparsity, non-uniformity or
perspective, which raises difﬁculties on acquiring strong
invariance property of the feature encoder.

• An effective learning scheme. Input signals and features
in different modalities are supposed to be efﬁciently pro-
cessed and aggregated in a uniﬁed framework.

Finally, regarding practicability, the augmentation is not
supposed to introduce excessive computational cost, as the
local feature description is often regarded as part of prepro-
cessing in practical pipelines.

Although contextual information has been widely ex-
plored in semantic-based tasks, the challenges faced by lo-
cal feature learning are substantially different, posing many
non-trivial technical and systematic issues to overcome. In
this paper, we propose a uniﬁed augmentation scheme that
effectively leverages and aggregates cross-modality con-
text, of which the contributions are summarized threefold:
1) a novel visual context encoder that integrates high-level
visual understandings from regional image representation,
a technique often used by image retrieval [33, 38]. 2) A
novel geometric context encoder that consumes unordered
points and exploits geometric cues from 2D keypoint distri-
bution, while being robust to complex variations. 3) A novel
N-pair loss that requires no manual hyper-parameter search

and has better convergence properties. To our best knowl-
edge, it is the ﬁrst work that emphasizes the importance of
context awareness, and in particular addresses the usability
of spatial relations of keypoints in local feature learning.

The proposed augmentation is extensively evaluated
and achieves state-of-the-art results on several large-scale
benchmarks,
including patch-level homography dataset,
image-level wild outdoor/indoor scenes and application-
level 3D reconstruction image sets, while being lightweight
compared with raw local description, demonstrating both
strong generalization ability and practicability.

2. Related Work

Learned local descriptors.
Initially, local descriptors are
jointly learned with a new comparison metric [9, 50],
which is later simpliﬁed as direct comparison in Euclidean
space [40, 48, 3, 19, 1]. More recently, efforts are spent on
efﬁcient training data sampling [43, 25, 11], effective regu-
larizations [43, 53], and geometric shape estimation of input
patches [26, 7]. However, most of above methods take indi-
vidual image patches as input, whereas in the present work,
we aim to take advantage of contextual cues beyond the lo-
cal detail and incorporate features in multiple modalities.

Context awareness. Although widely introduced in com-
puter vision tasks, context awareness has received little at-
tention in learning 2D local descriptors.
In terms of vi-
sual context, the central-surround (CS) structure [50, 19,
43] leverages multi-scale information by additionally feed-
ing the central part of patches to boost the performance,
whereas sacriﬁcing computational efﬁciency due to the dou-
bled extraction time and feature dimensionality. To incor-
porate semantics, one previous practice [18] designs a new
comparison metric and describes features from histogram
of semantic labels.
In contrast to geometric matching, a
family of studies has focused on ﬁnding semantic corre-
spondences [45, 34] across different objects of the same
category. Beside of visual information, a recent study [49]
explores to encode motion context for identifying outliers
from keypoint matches, i.e., 4-d coordinate pairs, while we
aim to exploit geometric context from single image with-
out any reference. Overall, encoding proper context is non-
trivial and still unclear in 2D local feature learning.

Point feature learning.
In the present work, one of our
goals is to explore geometric features from keypoint dis-
tribution, we thus resort to PointNet [31] and its vari-
ants [32, 5, 49] to consume unordered points. Although
great success has been witnessed in learning tasks on 3D
points, there are only few studies exploiting the potential
outcome of 2D keypoint sets. In essence, keypoint structure
is not intuitively meaningful and robust, as being highly de-
pendent on the performance of interest point detectors and
strongly affected by image variations. However, in descrip-

2528

Figure 2: The proposed augmentation framework consumes a single image as input, from which 2D keypoints, local and
regional features are extracted and encoded as geometric and visual context to improve the raw local feature description.

tor learning, we consider the keypoint location as an impor-
tant cue that bridges each individual local feature that has
potentials to alleviate the local visual ambiguity.

Loss formulation. Recent
local descriptors are often
evolved with advanced variants of N-pair losses. Initially,
L2-Net [43] adopts a log-likelihood formulation, which is
later extended by HardNet [25] with a subtractive hinge
loss. Furthermore, GeoDesc [23] applies an adaptive mar-
gin to improve the convergence in terms of different hard
negative mining strategies, while AffNet [7] approaches the
same issue by ﬁxing the distance to hardest negative sample
during training. Meanwhile, on the other hand, DOAP [11]
extends the N-pair loss to a list-wise ranking loss, while [17]
points out and studies the scale effects in N-pair losses while
introducing additional manual tuning of hyper-parameters.
Principally, a good loss is supposed to encourage similar
patches to be close while dissimilar ones to be distant in the
descriptor space. In this spirit, we aim to further resolve the
scale effects in [17] in an self-adaptive manner, without the
need of complex heuristics or manual tuning.

3. Local Descriptor Augmentation

Overview. As illustrated in Fig. 2, the proposed framework
consists of two main modules: preparation (left) and aug-
mentation (right). The preparation module provides input
signals in different modalities (raw local feature, high-level
visual feature and keypoint location), which are then fed to
the augmentation module and aggregated into compact fea-
ture descriptions. At test time, the augmentation needs to
be performed once per image, resulting in K feature vec-
tors for K corresponding keypoints.

3.1. Preparation

Patch sampler. This module takes images and their key-
points as input, producing 32 × 32 gray-scale patches. Akin

to [48, 23], image patches are sampled by a spatial trans-
former [16], whose parameters are derived from keypoint
attributes (coordinates, orientation and scale) from the SIFT
detector. As a result, the sampled patch has the same sup-
port region size with the SIFT descriptor.

Local feature extractor. This module takes image patches
as input, producing 128-d feature descriptions as output.
We borrow the lightweight 7-layer convolutional networks
as used in several recent works [43, 25, 23].

Regional feature extractor. In contrast to aggregating fea-
tures of different domain sizes [50, 19, 43], in the present
work, we ﬁx the sampling scale of patches, and exploit con-
textual cues by inspiration of well-studied regional repre-
sentation in image retrieval tasks [44, 33, 28]. Without the
loss of generality, we reuse features from an off-the-shelf
deep image retrieval model of ResNet-50 [12]. As in [44],
feature maps are extracted from the last bottleneck block,
across which each response is regarded as a regional fea-
ture vector effectively corresponding to a particular region
in the image. As a result, we derive regional features of
32 × W
H
32 × 2048, where H and W denote the original im-
age height and width. The aggregation of regional and local
features will be later discussed in Sec. 3.3.

3.2. Geometric context encoder

This module takes K unordered points as input, and out-
puts 128-d corresponding feature vectors. Each input point
is represented as 2D keypoint coordinate, and can be asso-
ciated with other attributes.

2D point processing. At ﬁrst glance, 2D keypoints are in-
appropriate to serve as robust contextual cues, as its pres-
ence is heavily dependent on image appearance and thus
affected by various image variations. As a result, keypoint
distribution depicting the same scene may suffer from sig-
niﬁcant density or structure variations, as examples shown
in Fig. 1b. Hence, acquiring strong invariance property is

2529

PreparationAugmentationAggregationVisual context encoder×4×32×32×128Geometric context encoderRegional feature extractor××332×32×2048×2ResNet-50Patch samplerKeypointsImage sampleSampling gridRegional featuresInterpolation×128CMatchabilitypredictorResidual Unit 0w/ CN(…)Residual Unit 3w/ CN×1×3Perceptron×128MLPCLocal feature extractorMLP×128×640×512Quad LossN-pair loss×2048×128CChannel-wise concatenationElement-wise summationMLPMulti-layer perceptronCNContextNormalizationtanh2-NormalizationMLP w/ CNthe key challenge when designing the context encoder.

Initially, we attempt to approach the goal by Point-
Net [31] and its variants [32, 5]. Although having shown
great success on processing 3D point clouds, those preva-
lent PointNet methods fails to achieve consistent improve-
ment in terms of 2D points processing (Sec. 4.4.1). Instead,
we resort to [49], in which context normalization (CN) is
equipped in PointNet and consumes putative matches (4-d
coordinate pairs) for outlier rejection in image matching. In
this work, we aim to further explore the usability of CN for
modeling 2D point distribution in single image.

l

l

l)

i−µ
σ

i = (o

, where ol

Formally, CN is a non-parametric operation that sim-
ply normalizes feature maps according to their distribution,
written as ˆol
i is the output of i-th point
in layer l, and µl, σl are mean and standard deviation of
the output in layer l. To equip the operation, we borrow
the residual architecture in [49], where each residual unit is
built with perceptrons followed by context and batch nor-
malization, as illustrated in Fig. 3a.

However, the above design leads to a non-negative out-
put from the residual branch that may impact the represen-
tational ability as investigated in [13] and witnessed in our
experiments (Sec. 4.4.1). Following the teachings of [13],
we re-arrange the operations in each residual unit with pre-
activation, which is compatible with CN as presented in
Fig. 3b. We then construct four such units for the encoder,
as shown in Fig. 2. We will show that this simple revision
plays an important role to ease the optimization.

n
o
r
t

p
e
c
r
e
P

m
r
o
N
x
e

t

n
o
C

U
L
e
R
+
N
B

 

 

‧‧‧

+

m
r
o
N

 
t
x
e

t

n
o
C

U
L
e
R
+
N
B

 

 

n
o
r
t

p
e
c
r
e
P

‧‧‧

+

(a) Original design

(b) Re-arrange w/ pre-activation

Figure 3: Different designs of residual unit with context
normalization, where re-arranging with pre-activation im-
proves by a considerable margin than its counterpart.

Intuitively, the non-parametric CN sufﬁces to model the
keypoint distribution in our task, while high-level abstrac-
tions (e.g., in PointNet++ [32]) may not be necessary.

Matchability predictor.
In 3D point cloud processing,
low-level color and normal [31] information or complex
geometric attributes [5] are often incorporated to enhance
the representation. Similarly, associating 2D coordinate in-
put with other meaningful attributes would be promising
to boost the performance. However, due to the substantial
variations, e.g. perspective change, it is non-trivial to deﬁne
appropriate intermediate attributes on 2D points.

Although this issue has been merely discussed, we draw
inspiration from [10], which poses a problem named match-

ability prediction that targets to decide whether a keypoint
descriptor is matchable before the matching stage. In prac-
tice, the matchability serves as learned attenuation to diver-
sify the keypoints, so that the feature encoder can implicitly
focus on the points that are more robust, i.e., matchable, in
order to improve the invariance property.

In the present work, we approach the matchability pre-
diction with deep learning techniques instead of a random
forest in [10], and constrain the prediction to be consis-
tent between images. Inspired by learning-based keypoint
detection methods [35, 51], we resort to an unsupervised
learning scheme that aims to appropriately rank points by
their matchability. Formally, given K correspondences
(pn
2 ), n ∈ [1, K] from an image pair, we ﬁrst extract
1 , f n
their local features (f n
2 ), then construct feature quadru-
2, f j
ples as (f i
2 ), satisfying i, j ∈ [1, K], i 6= j and
holding that:

1, f j

1 , pn

1 , f i

H(f i

1) > H(f j

1 ) & H(f i

2) > H(f j
2 )

H(f i

1) < H(f j

1 ) & H(f i

2) < H(f j
2 )

or

,

(1)




where H(:) absorbs the raw local feature into a single real-
valued matchability, implemented as standard multi-layer
perceptrons (MLPs). Here, Cond. 1 aims to preserve a rank-
ing of each keypoint, hence improves the repeatability of
prediction. The condition can be re-written as:

R(f i
(H(f i

1 , f i

2, f j
1, f j
1) − H(f j

2 ) =
1 ))(H(f i

2) − H(f j

2 )) > 0,

(2)

the ﬁnal objective can be obtained with a hinge loss:

Lquad =

1

K(K − 1) X

i,j,i6=j

max(0, 1 − R(f i

1, f j

1 , f i

2, f j

2 )).

(3)
In the proposed framework, the matchability is learned
as an auxiliary task, which is then activated by tanh and
associated with keypoint coordinates as the network input,
as in Fig. 2. Beside of Eq. 3, the gradient from ﬁnal aug-
mented features will ﬂow through the matchability predic-
tor, allowing a joint optimization of the entire encoder. The
visualization of predicted matchability is shown in Fig. 4.

Figure 4: Visualization of matchability responding to the
entire image (best viewed in color).

2530

1.00.50.0-0.5-1.03.3. Visual context encoder

32 × W

This module consumes regional features of H

32 ×
2048 in Sec. 3.1, K local features and their location, and
produces K augmented features. To integrate visual infor-
mation in different levels, a valid option as in [5] is to con-
catenate the global representation of entire image on raw
local features. In our framework, the global feature can be
derived by applying Maximum Activations of Convolutions
(MAC) aggregation [33], which simply max-pools over all
dimensions of regional features. However, such compact
representation is shown to obscure the raw local description,
due to the lack of spatial distinctions (Sec. 4.4.1). Hence,
we stick to the regional representation, where the key issue
is to handle the regional features and keypoints of different
numbers ( H

32 × W

32 and K).

To achieve the goal, we associate regional features to a
32 × W
regular sampling grid on the image, then interpolate H
32
grid points at coordinates of the K keypoints. For interpo-
lation, we use inverse distance weighted average based on k
nearest neighbors (in default we use k = 3), formulated as:

f (ˆpi) = Pk

j=1 w(pj)f (pj)
Pk

j=1 w(pj)

, and w(pj) =

1

d(ˆpi, pj)

, (4)

32 × W

where f (:) is the regional feature located at a certain grid
point. ˆpi, i ∈ [1, N ] and pj , j ∈ [1, H
32 ] indicate the
interpolated and original grid point. Next, the dimensional-
ity is reduced by applying point-wise MLPs, where we also
insert CN after each perceptron in order to capture global
context. Finally, raw local features are concatenated and
further mapped by MLPs, forming the ﬁnal 128-d features.
The above process is illustrated in Fig. 2.

3.4. Feature aggregation with raw local feature

To aggregate the above two types of contextual features,
similar to the CS structure, one option is to concatenate
them together and forms features of, in our case, 384-d
(128 × 3). However, the increased dimensionality will in-
troduce excessive computational cost in the matching stage
of O(n2) complexity. Instead, as shown in Tab. 2, we pro-
pose to combine different feature streams into a single vec-
tor by element-wise summation and L2-normalization, i.e.,
without altering the feature dimensionality. Beside of the
simplicity, such strategy allows ﬂexible use of the proposed
augmentation. For example, in situations where regional
features are not available, one may aggregate with only ge-
ometric context without the need of retraining the model.

3.5. N pair loss with softmax temperature

N-pair losses have been primarily used by recent works.
Empirically, the subtractive hinge loss [25, 23, 7] has re-
ported better performance, of which the main idea is to push

similar samples away from dissimilar ones to a certain mar-
gin in the descriptor space. However, setting the appropriate
margin is tricky, which does not always assure convergence
as observed in [23, 7]. More generally, the criteria of mak-
ing a good loss is studied in [17], from which guidelines are
provided on tuning loss parameters on a particular dataset.
In this spirit, we aim to further ease the pain of parameter
search in [17], and obtain an adaptive loss that allows fast
convergence regardless of the learning difﬁculty.

We use the log-likelihood form of N-pair loss [43] as
a base, which originally does not involve any tunable pa-
rameter. Formally, given L2-normalized feature descriptors
F1 = [f 1
2 ]T ∈ RN ×128,
the distance matrix D = [dij]N ×N can be obtained by
D = p2(1 − F1FT
2 ). By applying both row-wise (r) and

1 ]T , F2 = [f 1

column-wise (c) softmax, we derive the ﬁnal loss as:

2 ...f N

1 ...f N

1 f 2

2 f 2

LN -pair = −

1
2

(X

i

log sr

ii + X

i

log sc

ii),

(5)

where [sij]N ×N = softmax(2 − D).

Noted that since input features are L2-normalized, the re-
sulting dij is bounded by [0, 2], which causes convergence
issues due to the scale sensitivity of softmax function [15].
Similarly, we introduce a single trainable parameter α, re-
ferred to as softmax temperature, to amend the inability of
re-scaling the input. The loss now becomes:

[sij]N ×N = softmax(α(2 − D)),

(6)

where α is initialized to 1 and regularized with the same
weight decay in the network, hence does not require any
manual tuning or complex heuristics. In the experiments in
Sec. 4.4.2, we show this simple technique improves dras-
tically than its original form [43], whose performance we
suspect is hindered due to the above-mentioned scale sensi-
tivity. In the proposed framework, we compute the N-pair
loss on augmented features, and obtain the total loss:

Ltotal = LN -pair + λLquad,

(7)

where we choose λ = 1 in the experiment.

4. Experiments

4.1. Implementation

Training details. Although the framework is end-to-end
trainable, we ﬁx the local and regional feature extractors in
Sec. 3.1 during the training, in order to clearly demonstrate
the efﬁcacy of the proposed augmentation scheme. We train
the networks using SGD with a base learning rate of 0.05,
weight decay of 0.0001 and momentum at 0.9. The learn-
ing rate exponentially decays by 0.1 for every 100k steps.
The batch size is set to 2, and each time 1024 keypoints are

2531

randomly sampled including random numbers of matchable
and noisy keypoints (see Appendix A.1). Input patches are
standardized to have zero mean and unit norm, while in-
put keypoint coordinates are normalized to [−1, 1] regard-
ing the image size.

Training dataset. Although UBC Phototour [4] is used as a
common practice, this dataset consists of only three scenes
with limited diversity of keypoint distribution. In order to
achieve better generalization ability, we resort to large-scale
photo-tourism [46, 33] and aerial datasets (GL3D) [38] as
in [48, 23], and generate groundtruth matches from SfM.
We manually exclude the data that is used in the evaluation.

Data augmentation. We randomly perturb input patches by
afﬁne transformations including rotation (90°), anisotropic
scaling and translation w.r.t. the detection scale. For key-
point augmentation, we perturb the coordinate with random
homography transformation as in [6] (see Appendix A.1).

4.2. Evaluation datasets

Homography dataset. HPatches [2] is a large-scale patch
dataset for evaluating local features regarding illumination
and viewpoint changes. As groundtruth homographies and
raw images are provided, HPatches can also be used to eval-
uate image matching performance, which we accordingly
refer to as HPSequences as in [20], consisting of 116 se-
quences and 580 image pairs.

Wild dataset. Similar to settings in [49], we also eval-
uate on outdoor YFCC100M [42] (1000 pairs) and in-
door SUN3D [47] (539 pairs) datasets. Compared with
HPSequences, the two datasets additionally introduce varia-
tions such as self-occlusions, and in particular, repetitive or
feature-poor patterns in indoor scenes, which is generally
considered challenging for sparse matching.

SfM dataset. Following [37], we evaluate on SfM dataset
such as well-known Fountain and Herzjesu [41], or land-
mark collections [46]. We integrate the proposed frame-
work into SfM pipeline, i.e., COLMAP [36], and use the
keypoints provided in [37] to compute the local features.

4.3. Evaluation protocols

Patch level. For HPatches [2], we follow its evaluation pro-
tocols and use mean average precision (mAP) for three sub-
tasks, including patch veriﬁcation, matching, and retrieval.

Image level. For HPSequences, we use Recall = # Correct
Matches / # Correspondences deﬁned in [14], to quantify
the image matching performance, where # Correct matches
are matches found by nearest neighbor searching and ver-
iﬁed by groundtruth geometry, e.g., homography, while #
Correspondences are matches that should have been iden-
tiﬁed by the given keypoint locations. Following [14], a
match point is determined to be correct if it is within 2.5

pixels from the wrapped keypoint in the reference image.
We use a standard SIFT detector to localize the keypoints,
of which the number is randomly sampled to 2048. For
YFCC100M [42] and SUN3D [47], we follow the same set-
ting in [49] and report the median number of inlier matches
after RANSAC for each dataset.

Reconstruction level. For clarity, we report metrics in [37]
that quantify the completeness of SfM, including the num-
ber of registered images (# Registered), sparse points (#
Sparse Points) and image observations (# Observations).

4.4. Ablation study

4.4.1 Design of context encoder

In this section, we evaluate two splits of HPSequences [2]:
illumination (i) and viewpoint (v), regarding different image
transformations. We report Recall as deﬁned in Sec. 4.3.
If not speciﬁed, we use GeoDesc [23] as a baseline model
(baseline (GeoDesc)) to extract raw local features, whose
parameters are ﬁxed during the training of augmentation.

Visual context. We compare four designs, including i) CS
(256-d): the central-surround (CS) structure [50, 19, 43] as
described in Sec. 2, which concatenates local features from
different domain sizes.
ii) w/ global feature: the integra-
tion with global features [5], which is originally designed
for improving 3D local descriptors. iii) w/ regional feature:
the proposed integration with interpolated regional features,
and its variant iv) w/ regional feature + CN: with context
normalization to incorporate global visual information.

As shown in Tab. 1 (left columns), the CS structure [50,
19, 43] delivers only marginal improvements despite the
doubled dimensionality. Meanwhile, though being effective
in 3D descriptor learning, the integration with global fea-
tures [5] instead harms the performance, which we ascribe
to the limited representation ability of a single global fea-
ture. Finally, the proposed integration with interpolated re-
gional features shows clear improvements, as it better han-
dles both spatial and visual distinctiveness. Moreover, to
strengthen global context awareness, we show that the per-
formance can be further boosted by equipping context nor-
malization when encoding regional features.

Geometric context. We study ﬁve options: i) PointNet-like
architecture, i.e., segmentation networks in [31] without the
ﬁnal classiﬁer. ii) Pre-activated context normalization (CN)
networks in Sec. 3.2 with 2D xy input, and its variants iii)
with additional raw local feature input or iv) with match-
ability. We also compare the use of pre-activation of the
residual unit in context normalization networks.

As presented in Tab. 1 (middle columns), though widely
used in processing 3D points, PointNet [31] does not per-
form well in our task, while the similar phenomenon is also
observed in [49] when processing 2D correspondences. Be-
sides, it is noticed that input with raw local feature does

2532

Visual context encoder
Strategy

Recall i/v

Geometric context encoder

Comparison with other methods

Network architecture

Recall i/v

Method

Recall i/v

59.46
baseline (GeoDesc [23])
CS (256-d) [50, 19, 43]
59.83
w/ global feature [5]
59.11
63.64
w/ regional feature
w/ regional feature + CN 63.98

baseline (GeoDesc [23])
PointNet [31]

71.24
71.27
71.02 w/ CN (pre.) + xy
73.37 w/ CN (pre.) + xy + raw local feature
73.63 w/ CN (orig.) + xy + matchability
w/ CN (pre.) + xy + matchability

59.46
59.61
61.67
60.91
59.94
62.82

SIFT [22]
L2-Net [43]

71.24
70.96
72.63 HardNet [25]
72.99 GeoDesc [23]
71.25 ContextDesc
73.40 ContextDesc+

47.36
47.58
57.63
59.46
66.55
67.14

53.06
53.96
63.36
71.24
75.52
76.42

Table 1: Comparisons on HPSequences [2] of different designs of visual and geometric context encoder, and the performance
of entire augmentation scheme. ‘i/v’ denotes two evaluations on illumination and viewpoint sequences, respectively.

not help to boost the performance, which we attribute to
the weak relevance between local features as extracted from
different orientations and levels of scale space pyramid. In-
stead, the incorporation with matchability is notably beneﬁ-
cial, as matchability is more comprehensive as a high-level
abstraction of local feature. Finally, the pre-activation is
clearly a preferable alternative than its original design.

Integration with cross-modality context. Finally, we
evaluate the full augmentation with both visual and geo-
metric context (ContextDesc). As shown in Tab. 1 (right
columns), the simple summation aggregation in Sec. 3.4
effectively takes advantage of both context, delivering re-
markable improvements over the state-of the-art.

4.4.2 Efﬁcacy of softmax temperature in N-pair loss

To demonstrate the validity of proposed loss in Sec. 3.5, we
train only the local base model without any context aware-
ness, and compare different losses including: i) the plain
N-pair loss in [43] without scale temperature, and ii) the
scale-aware loss in [17] with its original parameters.

GeoDesc [23] w/ loss in [43] w/ loss in [17] Ours

HPatches, mAP [%]

Veriﬁcation
Matching
Retrieval

Seq. i
Seq. v

91.1
59.1
74.9

59.5
71.2

78.3
23.9
46.8

HPSequences, Recall

32.2
48.5

81.2
40.5
64.0

50.0
64.8

90.2
59.2
76.0

59.7
72.6

Table 2: Evaluation results on 1) HPatches [2] of three com-
plementary tasks: patch veriﬁcation, matching and retrieval.
2) HPSequences of two sequence splits.

As shown in Tab. 2, the proposed loss improves the
overall performance over the previous best-performing
GeoDesc [23] under similar
training settings, while
GeoDesc requires additional geometric supervision. Be-
sides, the proposed loss clearly shows better convergence
compared with losses in [43] and [17]. Although we sus-
pect that the loss in [17] may perform better with careful
parameter searching, the proposed loss is advantageous due
to its self-adaptivity without the need of complex heuristics
or manual tuning.

Moreover, once replace GeoDesc with the above model
as a base in the augmentation scheme, the ﬁnal performance
can be further improved by a signiﬁcant margin, denoted as
ContextDesc+ in Tab. 1 (right columns), which again ad-
dresses the advance of improved base model. We will use
this model to complete the following experiments.

4.5. Generalization

Wild dataset.
The evaluation results on two chal-
lenging datasets (outdoor YFCC100M [42] and indoor
SUN3D [47]) are presented in Tab. 3. The proposed cross-
modality context augmentation delivers ∼35% and ∼125%
improvements over the previous state of the art, which ef-
fectively demonstrates the strong generalization ability of
the learned augmented features in practical scenes.

SIFT [22] L2-Net [43] HardNet [25] GeoDesc [23] Ours

median number of inlier matches

indoor
outdoor

138
168

153
173

239
219

271
214

365
482

Table 3: Evaluation results on wild datasets:
SUN3D [47] and outdoor YFCC100M [42] datasets.

indoor

SfM dataset. We further demonstrate the improvement in
complex SfM pipeline. As shown in Tab. 4, the integra-
tion of augmented feature generalizes well among differ-
ent scenes even in large-scale SfM tasks, meanwhile con-
sistently boosts the completeness of sparse reconstruction.
Some matching results are presented in Fig. 5, and more
visualizations can be found in the appendix.

# Images

# Registered

# Sparse Points

# Observations

Fountain

SIFT [22]

GeoDesc [23]

Herzjesu

South Building

Roman Forum

Alamo

Ours
SIFT

GeoDesc

Ours
SIFT

GeoDesc

Ours
SIFT

GeoDesc

Ours
SIFT

GeoDesc

Ours

11

8

128

2,364

2,915

11
11
11
8
8
8

128
128
128
1,407
1,566
1,571
743
893
921

10,004
16,687
16,965
4,916
8,720
9,429
62,780
170,306
174,359
242,192
770,363
848,319
120,713
353,329
424,348

44K
83K
84K
19K
38K
40K
353K
887K
893K
1,805K
5,051K
5,484K
1,384K
3,159K
3,488K

Table 4: Evaluation results on SfM dataset [37].

2533

Figure 5: Matching results after RANSAC in different challenging scenarios. From top to bottom: SIFT, GeoDesc and ours.
The augmented feature helps to ﬁnd more inlier matches, and further allows a more accurate recovery of camera geometry.

4.6. Discussions on practicability

Invariance property. We again use Recall and evaluate
on Heinly benchmark [14] to quantify the invariance prop-
erty. As shown in Tab 5, the proposed method improves re-
markably over the previous best-performing descriptor, ex-
cept for some minor underperformance regarding Rotation
change when images are rotated up to 180°, which may be
caused by the inability of being fully rotation-invariant es-
pecially for the regional feature extractor.

SIFT [22] GeoDesc [23] Ours

Recall

JPEG
Blur
Exposure
Day-Night
Scale
Rotation
Scale-Rotation
Planar

60.7
41.0
78.2
29.2
81.2
82.4
29.6
48.2

66.1
47.7
86.4
39.6
85.8
87.6
33.7
59.1

78.6
57.8
88.2
43.3
88.1
86.3
38.0
61.7

Table 5: Evaluation results regrading different transforma-
tions on Heinly benchmark [14].

Computational cost. Towards practicability, we only use
shallow MLPs or non-parametric context normalization in
the augmentation framework, which thus introduces only
insigniﬁcant computation overhead. As reported in Tab. 6,
suppose that regional features are readily extracted, e.g.,
from a retrieval model deployed in SfM pipeline for accel-
erating image matching, the full augmentation then requires
only ∼5% time cost compared with the raw local feature de-
scription. Virtually, the proposed framework allows ﬂexible
integration and reuse of other visual components to achieve
system-level efﬁciency, such as saliency or segmentation
masks, and thus has large rooms for future improvements.

End-to-end training. For ablation purposes, the parame-
ters of base local and regional models are previously ﬁxed
in the training, and we here provide further studies about
the efﬁcacy of an end-to-end training scheme.

In the ﬁrst setting, we freeze only the regional model and
train from scratch with Eq. 7 on the augmented feature. As

Preparation

local feat.

regional feat.

geo. context

Augmentation
vis. context multi-context

Time (ms)
FLOPs (B)
Params (M)

351
802.9

2.4

49

123.4
24.5

5
1.7
<0.1

14
13.9
3.1

18
15.7
3.2

Table 6: The computational cost of proposed framework,
evaluated on 10k keypoints from an 896 × 896 image. The
inference time is estimated on an NVIDIA GTX 1080 GPU.

a result, the performance is notably improved from 67.14
to 67.53, and 76.42 to 77.20 for i/v sequences of HPSe-
quences, compared with ContextDesc+ in Tab. 1.

In the second setting, we further end-to-end train with
the regional model, which is additionally optimized by a
standard cross-entropy classiﬁcation loss as in [28] for sim-
plicity (see Appendix A.1 for details). Although several loss
balancing strategies have been experimented, we did not ob-
serve a consistent improvement for ﬁnal matching perfor-
mance, which we ascribe to the substantial challenge pos-
ing by multi-task learning. Thus, we currently recommend
a separate training for the regional model, and look forward
to an improved solution in the future.

5. Conclusion

In contrast to current trends, we have addressed the im-
portance of introducing context awareness to augment lo-
cal feature descriptors. The proposed framework takes key-
point location, raw local and high-level regional feature as
input, from which two types of context are encoded: geo-
metric and visual context, while the training adopts a novel
N-pair loss that is self-adaptive and parameter-tuning free.
We have conducted extensive evaluations on diversiﬁed and
large-scale datasets, and demonstrate remarkable improve-
ments over the state of the art, meanwhile showing the
strong generalization and practicability in real applications.

Acknowledgment. This work is supported by Hong Kong
RGC GRF 16203518, T22-603/15N, ITC PSKL12EG02.
We thank the support of Google Cloud Platform.

2534

SIFTGeoDescOursReferences

[1] V. Balntas, E. Johns, L. Tang, and K. Mikolajczyk. Pn-net:
Conjoined triple deep network for learning local image de-
scriptors. In arXiv, 2016. 2

[2] V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk.
Hpatches: A benchmark and evaluation of handcrafted and
learned local descriptors. In CVPR, 2017. 1, 6, 7

[3] V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk. Learning
local feature descriptors with triplets and shallow convolu-
tional neural networks. In BMVC, 2016. 2

[4] M. Brown and D. G. Lowe. Automatic panoramic image

stitching using invariant features. In IJCV, 2007. 6

[5] H. Deng, T. Birdal, and S. Ilic. Ppfnet: Global context aware
local features for robust 3d point matching. In CVPR, 2018.
2, 4, 5, 6, 7

[6] D. DeTone, T. Malisiewicz, and A. Rabinovich. Deep image

homography estimation. In arXiv, 2016. 6

[7] J. M. Dmytro Mishkin, Filip Radenovic. Repeatability is not
enough: learning discriminative afﬁne regions via discrim-
inability. In ECCV, 2018. 2, 3, 5

[8] J. Dong and S. Soatto. Domain-size pooling in local descrip-

tors: Dsp-sift. In CVPR, 2015. 1

[9] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg.
Matchnet: Unifying feature and metric learning for patch-
based matching. In CVPR, 2015. 2

[10] W. Hartmann, M. Havlena, and K. Schindler. Predicting

matchability. In CVPR, 2014. 4

[11] K. He, Y. Lu, and S. Sclaroff. Local descriptors optimized

for average precision. In CVPR, 2018. 2, 3

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 3

[13] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in

deep residual networks. In ECCV, 2016. 4

[14] J. Heinly, E. Dunn, and J.-M. Frahm. Comparative evaluation

of binary features. In ECCV. 2012. 6, 8

[15] E. Hoffer, I. Hubara, and D. Soudry. Fix your classiﬁer: the
In ICLR,

marginal value of training the last weight layer.
2018. 5

[16] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial

transformer networks. In NIPS, 2015. 3

[17] M. Keller, Z. Chen, F. Maffra, P. Schmuck, and M. Chli.
Learning deep descriptors with scale-aware triplet networks.
In CVPR, 2018. 3, 5, 7

[18] N. Kobyshev, H. Riemenschneider, and L. Van Gool. Match-
In

ing features correctly through semantic understanding.
3DV, 2014. 2

[19] B. Kumar, G. Carneiro, I. Reid, et al. Learning local image
descriptors with deep siamese and triplet convolutional net-
works by minimising global loss functions. In CVPR, 2016.
1, 2, 3, 6, 7

[20] K. Lenc and A. Vedaldi. Large scale evaluation of local im-
In BMVC,

age feature detectors on homography datasets.
2018. 6

[21] S. Li, L. Yuan, J. Sun, and L. Quan. Dual-feature warping-

based motion model estimation. In CVPR, 2015. 1

[22] D. G. Lowe. Distinctive image features from scale-invariant

keypoints. 2004. 7, 8

[23] Z. Luo, T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang,
and L. Quan. Geodesc: Learning local descriptors by inte-
grating geometry constraints. In ECCV, 2018. 1, 3, 5, 6, 7,
8

[24] J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust wide-
baseline stereo from maximally stable extremal regions. In
Image and vision computing, 2004. 1

[25] A. Mishchuk, D. Mishkin, F. Radenovic, and J. Matas. Work-
ing hard to know your neighbor’s margins: Local descriptor
learning loss. In NIPS, 2017. 2, 3, 5, 7

[26] K. Moo Yi, Y. Verdie, P. Fua, and V. Lepetit. Learning to

assign orientations to feature points. In CVPR, 2016. 2

[27] D. Nister and H. Stewenius. Scalable recognition with a vo-

cabulary tree. In CVPR, 2006. 1

[28] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han. Large-
In

scale image retrieval with attentive deep local features.
ICCV, 2017. 3, 8

[29] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisser-
man. Object retrieval with large vocabularies and fast spatial
matching. In CVPR, 2007.

[30] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman.
Lost in quantization: Improving particular object retrieval in
large scale image databases. In CVPR, 2008.

[31] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
In CVPR, 2017. 2, 4, 6, 7

[32] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
In NIPS, 2017. 2, 4

[33] F. Radenovi´c, G. Tolias, and O. Chum. Cnn image retrieval
learns from bow: Unsupervised ﬁne-tuning with hard exam-
ples. In ECCV, 2016. 2, 3, 5, 6

[34] I. Rocco, R. Arandjelovic, and J. Sivic. End-to-end weakly-

supervised semantic alignment. In CVPR, 2018. 2

[35] N. Savinov, A. Seki, L. Ladicky, T. Sattler, and M. Pollefeys.
Quad-networks: unsupervised learning to rank for interest
point detection. In CVPR, 2017. 4

[36] J. L. Sch¨onberger and J.-M. Frahm. Structure-from-motion

revisited. In CVPR, 2016. 6

[37] J. L. Sch¨onberger, H. Hardmeier, T. Sattler, and M. Polle-
feys. Comparative evaluation of hand-crafted and learned
local features. In CVPR, 2017. 6, 7

[38] T. Shen, Z. Luo, L. Zhou, R. Zhang, S. Zhu, T. Fang, and
L. Quan. Matchable image retrieval by learning from surface
reconstruction. In ACCV, 2018. 2, 6

[39] T. Shen, S. Zhu, T. Fang, R. Zhang, and L. Quan. Graph-
In

based consistent matching for structure-from-motion.
ECCV, 2016. 1

[40] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and
F. Moreno-Noguer. Discriminative learning of deep convo-
lutional feature point descriptors. In CVPR, 2015. 2

[41] C. Strecha, W. Von Hansen, L. Van Gool, P. Fua, and
U. Thoennessen. On benchmarking camera calibration and
multi-view stereo for high resolution imagery.
In CVPR,
2008. 6

2535

[42] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,
D. Poland, D. Borth, and L.-J. Li. Yfcc100m: The new data
in multimedia research. In CACM, 2016. 6, 7

[43] Y. Tian, B. Fan, F. Wu, et al. L2-net: Deep learning of dis-
In CVPR,

criminative patch descriptor in euclidean space.
2017. 1, 2, 3, 5, 6, 7

[44] G. Tolias, R. Sicre, and H. J´egou. Particular object retrieval
with integral max-pooling of cnn activations. In ICLR, 2016.
3

[45] N. Ufer and B. Ommer. Deep semantic feature matching. In

CVPR, 2017. 2

[46] K. Wilson and N. Snavely. Robust global translations with

1dsfm. In ECCV, 2014. 6

[47] J. Xiao, A. Owens, and A. Torralba. Sun3d: A database
of big spaces reconstructed using sfm and object labels. In
ICCV, 2013. 6, 7

[48] K. M. Yi, E. Trulls, V. Lepetit, and P. Fua. Lift: Learned

invariant feature transform. In ECCV, 2016. 2, 3, 6

[49] K. M. Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and
P. Fua. Learning to ﬁnd good correspondences. In CVPR,
2018. 2, 4, 6

[50] S. Zagoruyko and N. Komodakis. Learning to compare im-
In CVPR,

age patches via convolutional neural networks.
2015. 1, 2, 3, 6, 7

[51] L. Zhang and S. Rusinkiewicz. Learning to detect features in

texture images. In CVPR, 2018. 4

[52] R. Zhang, S. Zhu, T. Fang, and L. Quan. Distributed very
large scale bundle adjustment by global camera consensus.
In ICCV, 2017. 1

[53] X. Zhang, X. Y. Felix, S. Kumar, and S.-F. Chang. Learning

spread-out local feature descriptors. In ICCV, 2017. 2

[54] L. Zhou, S. Zhu, Z. Luo, T. Shen, R. Zhang, M. Zhen,
T. Fang, and L. Quan. Learning and matching multi-view
descriptors for registration of point clouds. In ECCV, 2018.
1

[55] L. Zhou, S. Zhu, T. Shen, J. Wang, T. Fang, and L. Quan.
Progressive large scale-invariant image matching in scale
space. In ICCV, 2017. 1

[56] S. Zhu, T. Fang, J. Xiao, and L. Quan. Local readjustment

for high-resolution 3d reconstruction. In CVPR, 2014. 1

[57] S. Zhu, R. Zhang, L. Zhou, T. Shen, T. Fang, P. Tan, and
L. Quan. Very large-scale global sfm by distributed motion
averaging. In CVPR, 2018. 1

2536

