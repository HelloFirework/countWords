Zoom-In-to-Check: Boosting Video Interpolation via

Instance-level Discrimination

Liangzhe Yuan1 ∗ Yibo Chen1∗ Hantian Liu1

Tao Kong1

2

,

Jianbo Shi1

1University of Pennsylvania

2Tsinghua University

{lzyuan,yibochen,lhantian,jshi}@seas.upenn.edu, taokongcn@gmail.com

Abstract

We propose a light-weight video frame interpolation al-
gorithm. Our key innovation is an instance-level supervi-
sion that allows information to be learned from the high-
resolution version of similar objects. Our experiment shows
that the proposed method can generate state-of-the-art re-
sults across different datasets, with fractional computation
resources (time and memory) of competing methods.

Given two image frames, a cascade network creates
an intermediate frame with 1) a ﬂow-warping module that
computes coarse bi-directional optical ﬂow and creates an
interpolated image via ﬂow-based warping, followed by 2)
an image synthesis module to make ﬁne-scale corrections.
In the learning stage, object detection proposals are gen-
erated on the interpolated image. Lower resolution objects
are zoomed into, and the learning algorithms using an ad-
versarial loss trained on high-resolution objects to guide
the system towards the instance-level reﬁnement corrects
details of object shape and boundaries.

1. Introduction

High ﬁdelity video frame interpolation has usages in
novel-view rendering, video compression, and frame rate
conversion. Existing methods focus on achieving overall
high-quality interpolation averaged over all regions of im-
ages. The lack of explicitly object instances modeling be-
came the bottleneck for algorithm’s improvement.

Flow-based image synthesis algorithms [1, 10, 19, 22]
generate realistic colors and patterns by explicitly copying
pixels from given frames. For challenging scenes with oc-
clusion, complex deformation or fast motion, ﬂow-based
interpolation suffers due to inaccuracy in optical ﬂow es-
timation algorithms. To compensate for optical ﬂow error,
[19, 22] added an additional network to reﬁne the interpo-
lation results, at a cost of much higher computational cost.

∗indicates equal contribution

Supplementary video: https://youtu.be/q-_wIRq26DY.

Figure 1: Object detection proposals allow region-of-
interest (RoI) Zoom-In-to-Check. An adversarial discrim-
inator is trained using high-resolution objects across the
entire video against the current interpolated image region.
From top to bottom: synthesized image by image synthe-
sis loss only; by whole image adversarial discriminator; by
proposed instance-level discriminator; and the ground truth.

A kernel-based interpolation approach achieves the same
per-pixel mapping goal without requiring a precise per-pixel
ﬂow estimation. The size of blending kernels in a such

112183

method directly restricts the motion that the network is able
to capture. To capture larger motion, big kernels (51 × 51)
are used in [20], which results in heavy memory and com-
putation resource usage.

To hallucinate pixels on dis-occluded objects [15] or
sharpen motion blurred objects [13] often a Generative Ad-
versarial Network (GAN) is used. However, such generative
models are susceptible to mode collapse, resulting in over-
ﬁtting issues: when an object is blurry, it favors removing
the object altogether.

We propose a lightweight video synthesis framework
that takes advantage of a newly proposed instance-level ad-
versarial training. Our system consists of a two-stage inter-
polation network: a cascade design with a ﬂow-based mod-
ule followed by a kernel-based module. The design substan-
tially alleviates the computational resource at the inference
stage, as it requires neither large scale network needed to
estimate accuracy optical ﬂow, nor large size kernel needed
to preserve clean boundary and capture large motion.

From our experiments, we found image-level supervi-
sion has a tendency to remove object details, particularly
when the optical ﬂow is fuzzy. To alleviate this issue, we
propose an instance-level discriminator to focus our sys-
tem on the ﬁne details of individual objects. However, if
the ‘ground-truth’ reference images also lack details due to
lower resolution or motion blur, there is no sufﬁcient feed-
back to the network on how to correct its mistakes. Our
key observation is that in the video we often have similar
objects that appear at high-resolution with greater details.
This allows the algorithm to learn not just from the current
reference frame, but also from semantically similar objects
at higher-resolution.

This design allows our network to leverage instance-
level attention in learning and thus performs better in chal-
lenging scenes. To the best of our knowledge, we are the
ﬁrst to present an instance-level adversarial learning frame-
work that effectively exploits the network’s capacity and
achieves an accuracy-speed trade-off for video interpolation
task. Using 78% computational time and 21% model pa-
rameters of SepConv [21], we achieve state-of-the-art inter-
polation quality.

2. Related Work

Optical ﬂow estimation is a basic building block for
video frame interpolation [10, 19, 29, 31]. In fact, the image
interpolation quality has been used to evaluate the accuracy
of optical ﬂow estimation [1]. With rapidly improving qual-
ity of optical ﬂow estimation, state-of-the-art optical ﬂow
methods [3, 8, 26] can serve as a strong baseline for video
interpolation. The drawbacks for ﬂow-based video interpo-
lation include 1) producing artifacts around object boundary
due to lack of occlusion reasoning, 2) training optical ﬂow
estimators requires task-speciﬁc datasets, and 3) the overall

algorithm is not end-to-end trainable.

One line of research focused on integrating optical ﬂow
into an end-to-end trainable video interpolation framework.
Liu et al.[31] developed a network to extract per-pixel 3D
optical ﬂow vector across space and time in the input video.
The intermediate image is generated by trilinear interpola-
tion across the input video volume. The method obtains
high-quality results in frame interpolation and their unsu-
pervised ﬂow estimation results are comparable to the state-
of-the-art. However, [31] tends to fail when the scene con-
tains repetitive patterns. The work by Jiang et al. [10] ad-
dressed the issue of occlusion by estimating bidirectional
ﬂow together with visibility mask, followed by a ﬂow re-
ﬁnement network. Niklaus et al. [19] addressed the issue of
inaccuracy of optical ﬂow by retaining pixel-wise contex-
tual information extracted from ResNet18 [7], and employ
a synthesis network with a GridNet [4] architecture to gen-
erate the interpolated frame.

Moving away from optical ﬂow based methods, [17, 21,
20] eliminated the need of per-pixel explicit motion estima-
[17] propagate predicted phase infor-
tion. Meyer et al.
mation across oriented multi-scale pyramid levels to cope
with large motions. Niklaus et al. [20] estimate a spatially-
adaptive convolution kernel for pixel synthesis for interop-
eration of two input frames. Although this method enables
high-quality video frame interpolation, it is difﬁcult to es-
timate all the kernels at once and the interpolation process
is very memory intensive. In [21], the authors improve the
efﬁciency by approximating a 2D kernel with a pair of 1D
kernels. This work relieves the intensive memory require-
ment but the fundamental limitation still exists, where the
capability of capturing large motion and ﬂexibility in frame
resolution is still limited by the kernel size, which is pro-
hibitively expensive to increase.

A related but harder task is video frame extrapolation.
This task contains a similar challenge of motion estimation
and object completion on dis-occluded regions. Earlier ap-
proaches use variational models that can represent the in-
herent uncertainty in prediction. Mathieu et al.
[16] de-
veloped a multi-scale conditional GAN architecture to im-
prove the prediction. These methods suffer from blurriness
and contain artifacts for large motion. Vondrick et al. [27]
train a two-stream adversarial network that untangles fore-
ground from background to predict into the future. Lee et
al.[14] propose a stochastic video prediction model based
on VAE-GAN for object sythnesis and completion. Several
recent works seek to learn a transformation from past pixels
to the future directly. [28] untangles the memory of the past
from the prediction of the future by learning to predict sam-
pling kernels. [22] combines ﬂow-based and kernel-based
approaches to learn a model to predict a motion vector and
a kernel simultaneously for each pixel.

12184

Figure 2: An overview of our model. The ﬂow estima-
tion module (left) takes two frames: I1 and I2 as input. It
predicts the bidirectional optical ﬂows f1→t and f2→t for
coarse motion estimation, and a blending mask b for occlu-
sion reasoning. The image synthesis module (right) takes
images I1, I2, corresponding features F1, F2, estimated op-
tical ﬂows f1→t, f2→t and blending mask b to synthesize
target frame ˜It. Instance-level adversarial discrimination is
further added on ˜It to preserve sharper image details.

3. Method

3.1. Coarse Optical Flow Estimation

To compensate large displacement motion, we ﬁrst esti-
mate coarse optical ﬂow to generate an initial interpolated
frame ˆIt given two consecutive video frames I1 and I2. We
use a U-Net like network to estimate bidirectional optical
ﬂows f1→t and f2→t, which can be used to warp I1 and
I2 respectively to designated time ˆIt. In the meantime, our
network also predicts a per-pixel weighting mask b to blend
two synthesized images into one. The blending mask b here
can be seen as a conﬁdence mask and it’s designed to deal
with occlusion. Inspired by [19], we employ a pre-trained
feature extractor to extract high level features from both I1
and I2, denoted as F1 and F2 respectively. Note that em-
pirically the ﬂow-based methods present a satisfying per-
formance on most of the regions but often fail to cope with
ﬁne-grain details and complex motions. Thus our ﬂow esti-
mation module only serves as an initial step for video inter-
polation task.

3.2. Image Synthesis Module

We perform both pixel-level and semantic feature-level
warping as shown in Fig.2. In detail, we feed images I1,
I2, corresponding deep feature maps F1, F2, ﬂows f2→t,
f1→t and mask b into later module for further reﬁnement. In
the image synthesis module, we use estimated bi-directional
ﬂow f2→t, f1→t and blending mask b to warp both images
and features into time t by bi-linear interpolation [9].

ˆIt = b ⊙ g(I1, f1→t) + (1 − b) ⊙ g(I2, f2→t)
ˆFt = b ⊙ g(F1, f1→t) + (1 − b) ⊙ g(F2, f2→t)

(1)

(2)

Figure 3: Image level adversarial learning v.s. proposed
instance level adversarial learning. We crop RoIs from
high resolution images and resize them into constant size
patches, which are used to train our low resolution im-
ages. This forces the system to focus on reﬁning details
and boundaries of instances.

where g(I, f ) is the bi-linear warping function that takes a
warping map f to warp a tensor I to ˆI and ⊙ is an element-
wise multiplication operator. Then we concatenate warped
features ˆFt and image ˆIt and feed it into the image synthe-
sis layers. Different from [19], in which the author used a
giant GridNet [4] to reﬁne the image, we simply use three
convolutional layers with kernel size 9 to approximate a
large receptive ﬁeld. We will show that this approxima-
tion is enough to get good performance with our proposed
instance-level adversarial loss.

3.3. Instance level Discriminator

Flow-guided warped image ˆIt generated from the two
previous stages has two problems: (a) as the optical ﬂow is
trained on the whole image, it often results in twisted and
blurry boundaries, as shown in Fig.1; b) optical ﬂow es-
timation fails to dis-occlude objects in the images, which
are common cases the interpolation algorithm needs to deal
with. To address the issues, we use the adversarial learning
[5] to empower the model on synthesizing instances and re-
covering structural patterns. In the experiments, we explore
two algorithm variations for video interpolation: (a) directly
discriminating on the whole image, and (b) zoom-in on ob-
ject instance area, as shown in Fig.3.

Direct adversarial learning on the whole image makes
the generated ˜It looks more realistic compared with the real
image It. However, since the majority of the image is usu-
ally the background, the image-level supervision provides a
uniform gradient across the whole image, such that the se-
mantic details are ignored and the optimization of the fore-
ground is diminished.

The instance-centered learning forces the model to pay
more attention to instances, especially on small-scale ob-
jects. Given an image It, we use region proposal method [6]
to generate several regions of interest (RoI). If we have ac-
cess to the high-resolution images during training, we crop

12185

order gradient difference between interpolated image and
the groundtruth to be consistent, which further improves the
reconstruction quality [16]. The above photometric losses
are computed as

Lph =ρ(˜I − Igt)

+ρ(

∂˜I
∂x

−

∂Igt
∂x

) + ρ(

∂˜I
∂y

−

∂Igt
∂y

)

ρ(x) =(x2 + ǫ2)α

(4)

(5)

where ρ(·) is the robust ℓ1 norm also known as Charbonnier
norm.

The second term of the interpolation loss is perceptual
loss [11]. It quantiﬁes the network higher-level feature re-
construction quality and thus makes more visually plausible
image interpolation results. Our experiments show that the
perceptual loss enables the network to learn to reconstruct
crispy image boundary. The perceptual loss is deﬁned as

Lpe = |Φ(˜I) − Φ(Igt)|1

(6)

in which the Φ(·) is the feature extraction function and in
our work, we use the latent features from VGG-16 [23]. We
apply photometric loss and perceptual loss on both the ini-
tial interpolated image ˆI and the synthesized image ˜I. We
also constrain the ﬁrst-order gradient of bi-directional opti-
cal ﬂow f1→t, f2→t and the corresponding blending mask b
to be locally smooth, resulting in smoothness loss Ls.

The above loss functions applied on full images mainly
guide our network for the coarse level interpolation and we
group them as the interpolation loss,

Lsynth = λ0Lph + λ1Lpe + λ2Ls

(7)

Adversarial Loss. In order to deal with complex scenar-
ios and enlarge model capacity, we utilize another network
D(·) to discriminate synthesized images. The adversarial
loss consists of two parts, namely the generator loss and the
discriminator loss. Let the ( ˜Pi, Pi) refer to a pair of syn-
thesized and groundtruth RoIs, where i = 1, · · · , N . The
discriminator will examine each one of them and the adver-
sarial losses are formulated as:

Ld =

1
N

N

X

i=1

E[min(0, −1 − D( ˜Pi))]

+E[min(0, −1 + D(Pi))]

1
N

N

X

i=1

E[D( ˜Pi)]; Ladv = λ3Ld + λ4Lg

(8)

Figure 4: Trained with the proposed instance-level adver-
sarial loss, our model generates the best results with mini-
mal number of parameters and least run-time on interpolat-
ing FHD resolution images, comparing to other methods.

the corresponding RoIs from high-resolution images and
use them to guide the synthesis of low-resolution results.
We perform RoIAlign as described in [6] to pool the RoIs
from It and ˆIt into patches with ﬁxed size of h × w. The
RoIAlign can achieve two effects: a) through bi-linear in-
terpolation, the gradient can be backpropagated to the exact
pixel location and previous modules, thus the total network
can be updated end-to-end; b) reshaping operation naturally
realizes zoom-in effect, balancing network’s focus on close
and far away, large and small objects. The reshaped RoIs of
different objects are illustrated in Fig.3.

There are two ways to choose how many RoIs per image
used for training: we can either choose a ﬁxed number of
RoIs with highest response from region proposal network,
or use RoIs whose score are above a certain threshold of
non-maximum suppression during region proposing. In our
experiments, we found the ﬁnal interpolation quality is not
sensitive to the number of RoIs per image used for train-
ing. Using 10-30 region proposals per image during training
leads to ±0.002 STD of SSIM and ±0.08 STD of IE/PSNR
in testing. We empirically choose 16 RoIs per image in the
training stage.

A discriminator with spectral normalization [18] is em-
ployed to examine only on the speciﬁc RoIs instead of on
the whole image. The details of adversarial loss Ladv are
described in the next section.

3.4. Training Objectives

We use two losses to train the network: a global interpo-

lation loss Lint and an instance adversarial loss Ladv.

L = Lint + Ladv

(3)

Lg = −

Interpolation Loss. For the global interpolation loss, we
ﬁrst minimize the robust ℓ1 norm [25] on the per-pixel color
difference, which is used in recent self-supervised optical
ﬂow estimation work [30]. We further constrain the ﬁrst-

3.5. Training Details

The network is trained on a mixer of UCF101 [24] and
CityScapes [2] dataset. We randomly pick four triplets in

12186

Figure 5: Example of a trimap mask using groundtruth seg-
mentation (white) with 12(a)/24(b) pixels dilation (gray).
Evaluation is done on gray and white area.

Figure 6: Evaluation metrics on trimaps with various widths
on CityScapes dataset. DVF [31] is excluded in the plot for
better visualization.

Figure 7: Higher SSIM score from Oursroigan indicates we
preserve more perceptual structures than SepConv [21].

every video clip of UCF101 and one triplet in every se-
quence of CityScapes training set, which gives us around
26k triplets in all.
In practice, as our proposed training
pipeline is self-contained and does not need labels, any
collection of video clips are sufﬁcient to train our net-
work. We keep UCF101 original image size and down-
sample CityScapes images to 256 × 512. Note that we
use the high-resolution version of images in CityScapes
dataset to supervise adversarial learning. Forming high-
resolution and low-resolution training pair is the key to our
learning algorithm. During training, we randomly crop a
256 × 256 region of triplets as input. We also randomly
ﬂip images for data augmentation. The size of output from
RoIAlign is set to be 64 × 64. An Adam optimizer [12]
with β1 = 0.9 and 2 = 0.999 is used with initial learn-
ing rate 1e-4, which is decayed exponentially by a factor of
0.1 for every 10 epochs and clipped at 1e-8 during training.
Also, we added decayed random noise to ‘real’ images and

UCF101 [24]

IE

SSIM PSNR

DVF [31]

SepConv [21]

SuperSloMo [10]

Oursbaseline

Oursgan

Oursroigan

11.54
11.28
10.87
11.23
11.66
10.92

0.869
0.875
0.885
0.876
0.870
0.882

29.70
30.29
30.48
30.08
29.85
30.23

CityScapes [2]

IE

SSIM PSNR

DVF [31]

SepConv [21]

SuperSloMo [10]

Oursbaseline

Oursgan

Oursroigan

17.49
7.85
−
9.38
9.04
8.03

0.722
0.923

−

0.890
0.902
0.925

23.88
30.92

−

29.31
29.93
30.77

Table 1: Quantitative evaluation of different methods on
CityScapes1 and UCF1012, including Interpolation Error
(IE) [1], Peak-Signal-To-Noise (PSNR), and Structural-
Similarity-Image-Metric (SSIM). Lower IE and higher
SSIM and PSNR indicate better quality.

scheduled to train the discriminator more to smooth the ad-
versarial learning. The weights for different losses are set
as (λ0, λ1, λ2, λ3, λ4) = (1, 1, 0.01, 0.1, 0.01).

4. Experiments

To evaluate our method, we quantitatively and qualita-
tively compare it with several state-of-art video frame inter-
polation methods. Namely, Deep Voxal Flow (DVF) [31]
is a ﬂow warping method for video interpolation; Sepera-
ble adaptive Convolutions (SepConv) [21] is a kernel based
method with the adaptive separable convolutions; Super-
SloMo [10] uses a cascaded optical ﬂow estimator to in-
terpolate video frames. We refer Oursroigan as our net-
work trained with proposed instance-level adversarial loss,
Oursgan as the model trained with the adversarial loss on
overall image, and Oursbaseline as the model trained with-
out any adversarial loss.

We compare the algorithm results on two different
datasets, CityScapes [2] and UCF101 [24]. CityScapes con-
tains different objects, e.g. cars, people, trafﬁc lights, etc.,
with various size and distance, which is good to differenti-
ate algorithms’ interpolation abilities on small objects and
partial occlusions. UCF101 contains people activities, e.g.
boating, making-up, boxing, etc., which is good to show
results on fast motion and complex deformation.

1SuperSloMo [10] is not open-sourced so we don’t have their results

on CityScapes dataset.

2We re-run the evaluation on the synthesis images provided by [10].

12187

Ground Truth

Oursroigan

DVF [31]

SepConv [21]

Figure 8: Qualitative results from different methods on CityScapes dataset. Best viewed in color.

4.1. Ablation Studies

Baseline. Considering our baseline is methodologically
similar to [19, 31], comparing metrics within our methods
(Oursbaseline, Oursroigan) would serve as an ablation study to
show the effectiveness of the proposed instance-level dis-
crimination. Results in both Tab.1 and Fig.6 show Oursroigan
consistently outperforms or on-par with Oursbaseline across
datasets. This shows from one aspect that the proposed
instance-level discrimination can improve algorithm perfor-
mance.

Adversarial Training. We verify the advantages of us-
ing adversarial learning to improve video interpolation per-
formance. From experiment on both datasets, training with
adversarial loss gives us sharper boundaries in images. In
Fig.1, we show an example of the effectiveness of adver-
sarial loss. From zoomed-in ﬁgures, we can see adversar-
ial loss helps preserve edges and shapes. This can be at-
tributed to the adversarial loss better facilitating image syn-
thesis module’s learning and potentially correcting the inac-
curate optical ﬂow estimation.

Interestingly, we ﬁnd training model with image level ad-
versarial loss would lead to a local-minimal solution some-
times. This phenomenon is especially noted when testing
on CityScapes dataset. Since the image-level adversarial
training does not explicitly constrain the instances, the net-
work tends to erase the uncertain objects in the scene and
recover the background. This is because the data distribu-

tion is dominated by rigid objects and background such that
training with image level adversarial loss leads to a biased
learning result. In the next part, we discuss the proposed
instance-level discrimination which would potentially ﬁx
this issue.

Instance-level Discriminator. We further verify the ad-
vantages of introducing an attention mechanism in adver-
sarial training, which greatly improves the video interpo-
lation performance as a result. From experiments, it is
shown that training with instance-level discrimination gives
us sharper boundaries on small, thin objects and image de-
tails. With the adversarial loss, both rigid moving object
and non-rigid human body shape are preserved better than
baseline method with, as we can see in Fig.1.

In Table 1, we show Oursroigan method outperforms
Oursbaseline and Oursgan method on all three standards in
CityScapes dataset.
In UCF101 dataset, as the RoI size
in the image is quite close to the entire image size, the
instance-level discriminator model and full-image-level dis-
criminator model perform considerably similar. Qualita-
tively, results in Fig.9 still show better interpolation results
on instances by our methods, due to the instance-level ad-
versarial training. Noticeably, we also measured all three
metrics using the trimaps of ‘human’, ‘vehicles’ groups
with various dilation widths in CityScapes dataset to quanti-
tatively illustrate the proposed instance-level discrimination
improves synthesized instance quality. Trimaps are gener-
ated using the groundtruth segmentation masks, as shown in

12188

Ground Truth

GT Enlarged

Oursroigan

DVF [31]

SepConv [21]

SuperSloMo [10]

Figure 9: Qualitative results from different methods on UCF101 dataset. Best viewed in color.

Fig.5. In Fig.6, we show our method achieves the best per-
formance on object instances. When dilation width is less
than 12 pixels, Oursroigan performs the best. As the trimap
width grows to over 12 pixels, more background pixels are
included such that Oursroigan performs slightly worse than
SepConv [21] on IE and PSNR. Introducing the region pro-
posals and zooming into them force the network to focus on
details and to utilize the ﬁne-grained information for learn-
ing ﬁlters. By formulating video interpolation problem as
perturbing semantic objects in image space, the pixel-level
motion estimation can be better grouped and updated.

Training with High Resolution Patches. We also study
the effects of training with different image resolution. Due
to data augmentation and the concerns of training speed, re-
searchers used to down-sample high-resolution images or
crop part of images for training. However, high-resolution
images often preserve ﬁne-grained information and it can
potentially improve algorithms performance. In our model,
we train our proposed model with instance-level discrimi-
nator on real image patches from high-resolution images.
More speciﬁcally, based on the region proposals we crop the
‘fake’ RoIs from synthesized images and the corresponding

‘real’ patches from its high-resolution counterpart, forming
low-resolution high-resolution pairs. The high-resolution
patches ultimately force the generator to super-resolve and
synthesize the details on low-resolution images. From Table
1, we show that using high-resolution patches to train the
network with region based adversarial training boosts per-
formances beyond both the baseline model and the model
using full image adversarial training. Fig.6 also shows train-
ing with high-resolution image patches consistently im-
proves interpolation qualities on instances.

4.2. Quantitative Evaluation

We achieve the highest SSIM across datasets consis-
tently, both on the foreground and full images. We compare
our approach with state-of-the-art video interpolation meth-
ods, including Separable adaptive Convolution (SepConv)
[21], and Deep Voxel Flow (DVF) [31] on both UCF101
and CityScapes dataset. As shown in Table 1, our method
achieves the best SSIM score on CityScapes dataset. Ta-
ble 1 also demonstrates the quantitative results on UCF101
dataset, where we also compare with the SuperSloMo [10].
We re-run the evaluation on the images provided by [31]

12189

and [10], and images generated from [21]. All metrics are
computed under the motion masks provided by [31], which
highlights the capabilities to cope with regions of motion
and occlusion. Our method achieves the highest SSIM score
among the lightweight models and performs comparably
to the heavy model, SuperSloMo [10]. We also show the
proposed model achieves the highest SSIM on instances in
Fig.6. As the SSIM metric measures perceptual and struc-
tural similarity, it serves as a strong cue that the proposed
method can render the most realistic scene and structural
details as shown in Fig.7.

4.3. Qualitative Results

We also present qualitative comparisons with other
In Fig.8, we present the comparison on differ-
methods.
ent street scenes under various lighting condition.
It is
obvious that DVF[31] generates the most artifacts such as
distortion of the whole scene, unrealistic deformation of
cars and buildings, misalignment of white lines and etc..
SepConv[21] is capable of dealing with motion within their
kernel size, but it consistently results in severe blur and ar-
tifacts near the image boundary, as shown in all of our ex-
amples. Our proposed approach is particularly good at re-
covering ﬁne-grained details, for example, the trafﬁc sign in
the ﬁrst example. Also, it ﬁlls up the occluded regions in a
natural and realistic way, such as the white lines on the road
in the fourth example. Fig.9 shows a qualitative comparison
on UCF101. It is hard for DVF[31] to handle the occlusion
as shown in the second example, although it was trained on
UCF101. SepConv[21] is observed to have frequent dupli-
cate artifacts, such as splits of horse legs and vault pole.
SuperSloMo[10] performs well in most scenes but some-
times fails in the reﬁnement of details in small scale such
as the chin of the boxing player, and legs of running horses.
Our proposed method enables the reconstruction of the ﬁne-
grained details and thus is capable of interpolating the chal-
lenging scenes.

4.4. Discussion

Our network achieves the state-of-art video interpola-
tion results using minimal model parameters and running
fastest at inference time, illustrated in Fig.4. During train-
ing, we zoom into instances and train our model by discrim-
inating on the rescaled RoIs. The scaling on instances due
to the physical distance and the zooming-in helps the net-
work learn more structural and general ﬁlters that not only
recover crispy boundary on objects but also structural pat-
terns in the background, e.g. pole, trafﬁc sign, etc. even
though they are not explicitly trained by the discriminator.
Also, inspired by the super-resolution literature, we expect
our model learn to super-resolve and render semantic de-
tails by training with high-resolution patches. At inference
time, only the ﬂow estimation module and image synthesis

Figure 10: Failure cases on CityScapes dataset. Left Col-
umn: The network tends to erase objects and recover back-
ground to overﬁt to the training objectives. Right Column:
Our model may fail in cluttered scenes.

module are needed, resulting in fast inference time. As a
result, it costs our network 0.36s to run on a 1024 × 2048
image.

Our network still has several limitations. For large non-
rigid body movements, the interpolated objects are slightly
distorted. As the right column in Fig.10 shows, cluttered
scenes will lead to failure case. Large overlapping instances
with mutual occlusion makes the system hard to dis-occlude
individual objects. Adversarial learning is also likely to
overﬁt to some data points in the training data. For ex-
ample, when the motion estimation is blurry, the synthe-
sis module tends to remove the uncertainty and choose to
safely reconstruct the background, as shown in Fig.10 left
column. Finally, larger models with better optical ﬂow es-
timation would generate better results than ours on a clean
and texture-rich area (ground).

5. Conclusions

We demonstrate a lightweight video interpolation frame-
work that can retain instance level object details. We use
a ﬂow estimation module to synthesize the intermediate
frame followed by a light-weight image synthesis module
to correct detailed shape errors. The network is trained by
a region based discriminator which utilizes high-resolution
image patches to supervise low-resolution RoIs, constrain-
ing instances in images to look realistic. Due to the mod-
ularity, our proposed adversarial training strategy can be
universally used as a training block to improve algorithm
performance. In the future, we hope to improve the model
design to compensate some drawbacks in our model, e.g.
employing deformable convolutions to tackle large motions
and complex deformations. We also want to further expand
our work to video prediction task.

6. Acknowledgement

We gratefully appreciate support through Honda Re-

search Institute Curious Minded Machine program.

12190

References

[1] S. Baker, D. Scharstein, J. P. Lewis, S. Roth, M. J. Black, and
R. Szeliski. A database and evaluation methodology for opti-
cal ﬂow. International Journal of Computer Vision, 92(1):1–
31, Mar 2011. 1, 2, 5

[2] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In Proc. of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016. 4, 5

[3] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In Proceedings of the IEEE International Conference on
Computer Vision, pages 2758–2766, 2015. 2

[4] D. Fourure, R. Emonet, E. Fromont, D. Muselet,
A. Tr´emeau, and C. Wolf. Residual conv-deconv grid net-
work for semantic segmentation.
In Proceedings of the
British Machine Vision Conference, 2017, 2017. 2, 3

[5] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets.
In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, edi-
tors, Advances in Neural Information Processing Systems 27,
pages 2672–2680. Curran Associates, Inc., 2014. 3

[6] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.
In Computer Vision (ICCV), 2017 IEEE International Con-
ference on, pages 2980–2988. IEEE, 2017. 3, 4

[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. arXiv preprint arXiv:1512.03385,
2015. 2

[8] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. In IEEE conference on computer vision
and pattern recognition (CVPR), volume 2, page 6, 2017. 2
[9] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in neural information

transformer networks.
processing systems, pages 2017–2025, 2015. 3

[10] H. Jiang, D. Sun, V. Jampani, M. Yang, E. G. Learned-Miller,
and J. Kautz. Super slomo: High quality estimation of mul-
tiple intermediate frames for video interpolation. CoRR,
abs/1712.00080, 2017. 1, 2, 5, 7, 8

[11] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution.
In European
Conference on Computer Vision, pages 694–711. Springer,
2016. 4

[12] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 5

[13] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and
O. Winther. Autoencoding beyond pixels using a learned
similarity metric. arXiv preprint arXiv:1512.09300, 2015. 2
[14] A. X. Lee, R. Zhang, F. Ebert, P. Abbeel, C. Finn, and
S. Levine. Stochastic adversarial video prediction. arXiv
preprint arXiv:1804.01523, 2018. 2

[15] X. Liang, L. Lee, W. Dai, and E. P. Xing. Dual motion
GAN for future-ﬂow embedded video prediction. CoRR,
abs/1708.00284, 2017. 2

[16] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-
scale video prediction beyond mean square error. CoRR,
abs/1511.05440, 2015. 2, 4

[17] S. Meyer, A. Djelouah, B. McWilliams, A. Sorkine-
Hornung, M. H. Gross, and C. Schroers. Phasenet for video
frame interpolation. CoRR, abs/1804.00884, 2018. 2

[18] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral
normalization for generative adversarial networks. In Inter-
national Conference on Learning Representations, 2018. 4

[19] S. Niklaus and F. Liu. Context-aware synthesis for video
frame interpolation. In IEEE Conference on Computer Vision
and Pattern Recognition, 2018. 1, 2, 3, 6

[20] S. Niklaus, L. Mai, and F. Liu. Video frame interpolation
via adaptive convolution. In IEEE Conference on Computer
Vision and Pattern Recognition, 2017. 2

[21] S. Niklaus, L. Mai, and F. Liu. Video frame interpolation
In IEEE International

via adaptive separable convolution.
Conference on Computer Vision, 2017. 2, 5, 6, 7, 8

[22] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker, D. Tarjan,
A. Tao, and B. Catanzaro. Sdc-net: Video prediction using
spatially-displaced convolution.
In The European Confer-
ence on Computer Vision (ECCV), September 2018. 1, 2

[23] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 4

[24] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset
of 101 human actions classes from videos in the wild. CoRR,
abs/1212.0402, 2012. 4, 5

[25] D. Sun, S. Roth, and M. J. Black. A quantitative analysis
of current practices in optical ﬂow estimation and the princi-
ples behind them. International Journal of Computer Vision,
106(2):115–137, 2014. 4

[26] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. Pwc-net: Cnns
for optical ﬂow using pyramid, warping, and cost volume.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 8934–8943, 2018. 2

[27] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating
videos with scene dynamics. CoRR, abs/1609.02612, 2016.
2

[28] C. Vondrick and A. Torralba. Generating the future with ad-
versarial transformers. In 2017 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 2992–
3000, July 2017. 2

[29] T.-C. Wang, J.-Y. Zhu, N. K. Kalantari, A. A. Efros, and
R. Ramamoorthi. Light ﬁeld video capture using a learning-
based hybrid imaging system. ACM Transactions on Graph-
ics (Proceedings of SIGGRAPH 2017), 36(4), 2017. 2

[30] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis. Ev-
ﬂownet: Self-supervised optical ﬂow estimation for event-
based cameras. arXiv preprint arXiv:1802.06898, 2018. 4

[31] X. T. Y. L. Ziwei Liu, Raymond Yeh and A. Agarwala. Video
frame synthesis using deep voxel ﬂow. In Proceedings of In-
ternational Conference on Computer Vision (ICCV), October
2017. 2, 5, 6, 7, 8

12191

