VITAMIN-E: VIsual Tracking And MappINg

with Extremely Dense Feature Points

Masashi Yokozuka, Shuji Oishi, Thompson Simon, Atsuhiko Banno

Robot Innovation Research Center,

National Institute of Advanced Industrial Science and Technology (AIST), Japan

{yokotsuka-masashi, shuji.oishi, simon.thompson, atsuhiko.banno}@aist.go.jp

Abstract

In this paper, we propose a novel indirect monocular
SLAM algorithm called “VITAMIN-E,” which is highly ac-
curate and robust as a result of tracking extremely dense
feature points. Typical indirect methods have difﬁculty in
reconstructing dense geometry because of their careful fea-
ture point selection for accurate matching. Unlike conven-
tional methods, the proposed method processes an enor-
mous number of feature points by tracking the local ex-
trema of curvature informed by dominant ﬂow estimation.
Because this may lead to high computational cost during
bundle adjustment, we propose a novel optimization tech-
nique, the ”subspace Gauss–Newton method”, that signiﬁ-
cantly improves the computational efﬁciency of bundle ad-
justment by partially updating the variables. We concur-
rently generate meshes from the reconstructed points and
merge them for an entire 3D model. The experimental
results on the SLAM benchmark dataset EuRoC demon-
strated that the proposed method outperformed state-of-the-
art SLAM methods, such as DSO, ORB-SLAM, and LSD-
SLAM, both in terms of accuracy and robustness in trajec-
tory estimation. The proposed method simultaneously gen-
erated signiﬁcantly detailed 3D geometry from the dense
feature points in real time using only a CPU.

1. Introduction

Simultaneous localization and mapping (SLAM) is a key
technology for applications such as autonomous systems
and augmented reality. Whereas LiDAR-based SLAM[34,
11, 25] is well established and widely used in autonomous
vehicles, visual SLAM with a monocular camera does not
provide sufﬁcient accuracy and robustness, particularly re-
garding dense map reconstruction, to replace LiDAR-based
SLAM. Although some visual SLAM algorithms that use
stereo cameras[7, 32], RGB-D cameras[12, 14, 33], and in-
ertial sensors[20, 18, 2, 28, 19] have achieved high perfor-

Figure 1. Dense geometry reconstruction with VITAMIN-E on
EuRoC V101. (https://youtu.be/yfKccCmmMsM)

mance, these methods are based on pure monocular SLAM;
hence, improving monocular SLAM is important.

Monocular SLAM methods can be classiﬁed into two

types: direct methods and indirect methods.

Direct methods: Direct methods estimate camera poses
and reconstruct the scene by minimizing the photometric
error deﬁned as a sum of the intensity difference between
each pixel in the latest image and the reprojection of the
color / monochrome 3D map. Direct methods, such as LSD-
SLAM[5], SVO[9], and DSO[6], process almost all pixels
in incoming images. They do not require exact pixel corre-
spondences among multiple views unlike indirect methods,
which leads to denser map reconstruction. However, direct
methods are susceptible to image noise, luminance ﬂuctu-
ation, and lens aberration because they directly use pixel
intensities. To overcome this drawback, Bergmann et al.[1]
proposed a normalization method against luminance ﬂuctu-
ation and a calibration method for lens aberration. As an-
other approach, Zhang et al.[35] proposed an auto-exposure
method that is suitable for direct methods.

Indirect methods: Indirect methods minimize the ge-
ometric error between observed 2D feature points and re-

19641

Input image

(b) Localization and Mapping.

(c) Meshing & Denoising

(a) Feature tracking

(d) Reconstruction by Mesh Merging

Figure 2. Overview of the proposed monocular SLAM.

*Colored by normal.

projections of the corresponding 3D points. As a re-
sult of the use of feature descriptors,
indirect methods
such as PTAM[15] and ORB-SLAM[21] are robust against
brightness changes and image noise. Additionally, indirect
methods explicitly establish feature point correspondences;
hence, outliers are easily removed using RANSAC[8] or
M-estimation[27]. This characteristic, however, can be a
drawback: Indirect methods carefully select stable feature
points, thus the reconstructed 3D map tends to be sparse and
does not provide detailed geometry. Densiﬁcation methods,
such as PMVS[10] and the extension L-PMVS[26], might
be useful for obtaining dense geometry; however, they are
ofﬂine methods and not applicable in real time.

In this paper, we propose the novel VIsual Track-
ing And MappINg with Extremely dense feature points,
“VITAMIN-E,” which is highly precise, robust, and dense
because of the tracking of a large number of feature points.
Indirect methods are inherently robust against noise, illumi-
nation change, and outliers as a result of the use of feature
descriptors. Retaining this advantage, we reconstruct de-
tailed 3D maps by establishing dense point correspondence.
The contributions of this study are as follows: We ﬁrst in-
troduce a new dense feature point tracking algorithm based
on dominant ﬂow estimation and curvature extrema tracing.
This allows VITAMIN-E to process an enormous number of
feature points; however, the need to maintain them simulta-
neously might lead to a high computational cost. There-
fore, we also introduce a novel optimization technique,
called subspace Gauss–Newton method, for bundle adjust-
ment. The optimization technique signiﬁcantly improves

the efﬁciency of bundle adjustment by partially updating the
variables. Moreover, VITAMIN-E generates meshes from
the reconstructed feature points and integrates them using
a truncated signed distance function (TSDF)[22, 30, 24].
Compared with not only conventional indirect methods but
also state-of-the-art direct methods, VITAMIN-E provides
highly detailed 3D geometry as shown in Figure 1 in real
time using only a CPU

2. Dense Feature Point Tracking

2.1. Feature Point Tracking

Indirect methods that use image descriptors can be un-
stable because of incorrect feature point correspondences.
They build feature point correspondences between multi-
ple views by matching the descriptors. Extracting consis-
tent descriptors over multiple frames, however, becomes
difﬁcult because descriptors vary as the monocular camera
changes its pose. Methods such as the KLT tracker[29] that
continuously track feature points while updating the feature
descriptors might be useful for overcoming the problem.
However, because the tracked positions drift as a result of a
minute change of feature descriptors, the correspondences
over multiple views tend to be incorrect. These problems
originate with the use of feature descriptors.

Rather than associating feature points based on descrip-
tors, VITAMIN-E tracks the local extrema of curvature in
incoming images. In the proposed method, feature points
denote the extrema of curvature on image intensities. Let
f (x, y) be an image, then curvature κ of image f is as fol-

9642

lows:

2.3. Curvature Extrema Tracking

κ = f 2

y fxx − 2fxfyfxy + f 2

x fyy,

(1)

where fx represents the partial derivative of f with respect
to x, which can be obtained using a Sobel operator or sim-
ilar technique. VITAMIN-E builds point correspondences
over multiple images by tracking the local maximum point
of curvature κ(x, y, t), which is the extension of κ to time
domain t. Figure 2(a) shows an example scene from which
a large number of extrema of curvature κ are extracted.
Whereas conventional indirect methods rely only on fea-
ture points with a large curvature to obtain stable correspon-
dences, the proposed method tracks all detected extrema to
reconstruct detailed geometry.

2.2. Dominant Flow Estimation

After detecting the extrema of curvature, the proposed
method estimates a dominant ﬂow that represents the av-
erage of optical ﬂow over the images, which provides a
good initial value to extrema tracking and makes it signif-
icantly stable, as explained later. Speciﬁcally, we deter-
mine the corresponding feature pairs between current and
previous images using the BRIEF[4] feature. Because we
only have to identify coarse feature pairs over consecu-
tive frames at this moment, feature matching is performed
on low-resolution images, subsampled to 1/6 of the former
size.

Then, we ﬁt the afﬁne transformation model y = Ax+b
to the feature pairs. x and y denote the position of a fea-
ture point in the previous and current frame, respectively,
and A and b represent a matrix of 2 × 2 and a 2D transla-
tion, respectively. A and b are obtained by minimizing cost
function E using the Gauss–Newton method:

VITAMIN-E tracks feature points by tracing the extrema
of image curvature by making use of the dominant ﬂow. Be-
cause it depends only on extrema instead of feature descrip-
tors used in conventional indirect methods, VITAMIN-E is
free from the variation of feature descriptors caused by im-
age noise or illumination changes, which makes VITAMIN-
E highly robust.

According to the dominant ﬂow represented by A and b,
we ﬁrst predict a current position ¯xt1 of tracking point xt0 :

¯xt1 = Axt0 + b.

(4)

Next, prediction ¯xt1 is corrected to xt1 by maximizing eval-
uation function F :

(5)

F = κ (xt1 , t1) + λw(∥xt1 − ¯xt1 ∥2) ,

where κ stores the curvature in each pixel, and w(x) =
1 − ρ(x) and λ denote an evaluation function and weight
for the prediction, respectively. The maximization is per-
formed using the hill climbing method, with ¯xt1 as the ini-
tial position. Speciﬁcally, maximum point xt1 is obtained
by iterating the hill climbing method in eight neighboring
pixels at each step until it reaches the local maximum value
of F . Function w prevents the maximization process from
falling into wrong extrema, thereby playing a regularization
role.

Note that extrema tracking can easily fall into local so-
lutions because there are many extrema in image curvature
and it is almost impossible to distinguish them without any
descriptors. However, the prediction according to the dom-
inant ﬂow boosts the accuracy of extrema tracking and en-
ables it to approach the optimal solution.

E =

N

∑i

ρ (∥yi − (Axi + b) ∥2) ,

(2)

3. Bundle Adjustment for Dense Tracking

where N and ρ denote the total number of corresponding
points and a kernel function for M-estimation, respectively.
The following Geman–McClure kernel with scale parame-
ter σ is used in VITAMIN-E :

ρ(x) =

x2

x2 + σ2 .

(3)

As a result of M-estimation, the dominant ﬂow repre-
sented by A and b can be estimated stably, even for low-
resolution images, and it allows us to roughly predict the
position of feature points in the next frame. Note that
VITAMIN-E does not rely on conventional feature match-
ing in its core but only for prior information for dense ex-
trema tracking, as described in the next section. Whereas
conventional feature matching has difﬁculty in making all
feature points couple correctly between consecutive frames,
afﬁne transformation is easily obtained when at least three
correspondences are given.

3.1. Bundle Adjustment

Bundle adjustment iteratively adjusts the reconstructed
map by minimizing reprojection errors. Given the i-th 3D
point pi, the j-th camera’s rotation Rj and translation tj ,
and the 2D position uij of pi observed in the j-th camera
frame, the objective function is formulated as follows:

E =

N

M

∑i

∑j

ρ(∥uij − φ(RT

j (pi − tj)) ∥2) ,

(6)

where N and M are the numbers of feature points and
camera poses respectively, φ denotes the 3D-2D projec-
tion function, and ρ is a kernel function for M-estimation.
Speciﬁcally, optimal camera variables cj = (Rj , tj ) and
pi are obtained by applying the Gauss–Newton method to
Equation 6, which results in iteratively solving the follow-
ing equations:

Hδx = −g, x = x + δx,

(7)

9643

where x = (c1, · · · , cM , p1, · · · , pN )T , and H and g rep-
resent the Hessian matrix and gradient around x of E, re-
spectively. H and g can be represented by the camera vari-
able cj block and the feature point variable pi block as fol-
lows:

cp Hpp] ,
H = [Hcc Hcp

H T

g = [gc
gp] .

(8)

Hessian matrix H in bundle adjustment has a unique struc-
ture: Hcc and Hpp are sparse matrices with only diagonal
elements in block units, whereas Hcp is a dense matrix. Ef-
ﬁcient solutions that focus on this unique structure are the
keys to developing highly precise and robust visual SLAM.
State-of-the-art monocular SLAM methods, such as
ORB-SLAM[21] and DSO[6], solve Equation 7 by decom-
posing it using the Schur complement matrix instead of di-
rectly solving it:

(Hcc − HcpH −1

pp H T

cp) δxc = −gc + HcpH −1

Hppδxp = −gp − H T

cpδxc,

pp gp,

(9)

(10)

where xc = (c1, · · · , cM )T and xp = (p1, · · · , pN )T .
The decomposition allows us to solve bundle adjustment
faster. The number of camera variables M is remarkably
smaller than that of feature point variables, and the inverse
matrix of Hpp can be easily calculated because it has only
diagonal components; thus, the Schur complement matrix

pp H T

(Hcc − HcpH −1

tiny compared with the original H, and the inverse matrix
is rapidly computable.

cp) whose size is M ×M is signiﬁcantly

pp H T

cp as a new H and gc − HcpH −1

Equation 9 is also called marginalization. When regard-
ing Hcc − HcpH −1
pp gp as
a new g, the decomposition is equivalent to eliminating all
feature point variables p from cost function E. State-of-the-
art SLAMs make themselves efﬁcient using the marginal-
ization technique to prevent the increase in computational
cost caused by a large number of variables.

However, in the case of maintaining thousands of fea-
ture points in every frame, as in the dense extrema tracking
in VITAMIN-E, the size of matrix H fundamentally can-
not be made sufﬁciently small because variable elimination
is applicable only to old variables unrelated to the current
frame for stability. Moreover, the size of the Schur comple-
ment matrix is proportional to the number of feature points;
thus, the calculation cost of bundle adjustment over tens of
thousands points, where the size of H is 100,000 × 100,000
or more, becomes too high to run bundle adjustment in real
time.

3.2. Subspace Gauss–Newton Method

updating all of them at once, as in Equations 9 and 10, by
decomposing these equations further as follows:

Hcici δxci = −(gci +

i−1

∑l=1

Hclci δxcl +

M

∑r=i+1

Hcicr δxcr +

N

∑j=1

Hcipj δxpj),

(11)

Hpj pj δxpj = −(gpj +

j−1

∑l=1

Hplpj δxpl +

N

∑r=j+1

Hpipr δxpr +

M

∑i=1

H T

cipj

δxci).

(12)

Equation 11 updates δxci of a camera variable, and Equa-
tion 12 δxpj of a feature point variable. Hcici , Hcipj ,
and Hpj pj are matrices of 6 × 6, 6 × 3, and 3 × 3, re-
spectively. The subspace Gauss–Newton method iteratively
solves Equations 11 and 12 until δxci and δxpj converge,
respectively.

These formulae are extensions of the Gauss–Seidel
method, which is an iterative method to solve a linear sys-
tem of equations, and equivalent to solving the Gauss–
Newton method by ﬁxing all variables except the variables
to be optimized. The advantage of the proposed subspace
Gauss–Newton method is that it does not require a large
inverse matrix, unlike Equation 9, but instead, only an in-
verse matrix of 6 × 6 at most. Additionally, as in ORB-
SLAM[21] and DSO[6], further speedup is possible by ap-
propriately performing variable elimination that sets most
elements of Hcc and Hpp to zero. Because the proposed
optimization method limits the search space in the Gauss–
Newton method to its subspace, we call it the “subspace
Gauss–Newton method.” 1

4. Dense Reconstruction

A large number of accurate 3D points are generated in
real time with VITAMIN-E as a result of the dense extrema
tracking and subspace Gauss–Newton method described in
previous sections. This leads not only to point cloud gener-
ation but also allows further dense geometry reconstruction
that cannot be achieved by conventional indirect methods.

Meshing and Noise Removal: We ﬁrst project the 3D
points onto an image and apply Delaunay triangulation to
generate triangular meshes. Then, We use NLTGV mini-
mization proposed by Greene et al[23] to remove noise on
the meshes. NLTGV minimization allows us to smooth the
meshes, thereby retaining local surface structures, unlike

To deal with the explosion in the size of H, we propose
a novel optimization technique called the “subspace Gauss–
Newton method.” It partially updates variables rather than

1 Alternating optimization, such as our method, has been used in some
contexts[36][31]. See the supplementary information for details of the
novel aspect of our method.

9644

typical mesh denoising methods such as Laplacian smooth-
ing. Figure 2(c) shows example results of Delaunay trian-
gulation and smoothing with NLTGV minimization.

Mesh Integration in TSDF: Finally, we integrate the
meshes in a TSDF to reconstruct the entire geometry of the
scene. The TSDF represents an object shape by discretizing
the space into grids that store the distance from the object
surface, and can merge multiple triangular meshes by stor-
ing the average value of distances from the meshes to each
grid.

5. Experimental Results

5.1. Setup

We

evaluated the performance of

the proposed
VITAMIN-E on the visual SLAM benchmark EuRoC[3]. 2
The dataset was created using ﬂying drones equipped with
a stereo camera and an IMU in an indoor environment,
and provided ground truth of trajectories obtained by a
Leica MS 50 laser tracker and Vicon motion capture. Eu-
RoC is also well-known for data variations, with different
difﬁculties ranked by the movement speed and lighting con-
ditions. In this experiment, we compared results obtained
using VITAMIN-E and other monocular SLAM methods,
DSO[6], ORB-SLAM[21], and LSD-SLAM[5], using
only the left images of the stereo camera in the EuRoC
dataset. Note that because VITAMIN-E and DSO[6] do
not include loop closing and relocalization, these functions
in ORB-SLAM[21] and LSD-SLAM[5] were disabled to
evaluate performance fairly. Similar evaluations can be
found in the papers on DSO[6] and SVO[9].

VITAMIN-E ran on a Core i7-7820 HQ without any
GPUs, threading each process for real time processing. Ini-
tialization was performed using essential matrix decompo-
sition. Bundle adjustment is signiﬁcantly sensitive to the
initial value of the variable. In VITAMIN-E, we initialized
the camera variables using P3P[17, 13] with RANSAC[8]
and feature point variables using triangulation. Note that
the proposed bundle adjustment ran so fast that we applied
it to every frame rather than each key frame as in ORB-
SLAM and DSO. To manage the TSDF, OpenChisel[16]
was used in our implementation because of its ability to
handle a TSDF on a CPU.

5.2. Evaluation Criteria

The evaluation on EuRoC was performed according to

the following criteria:

Localization success rate: We deﬁned the localization
success rate as Ns
N , where Ns and N denote the number of
images that were successfully tracked and the images in the
entire sequence, respectively. If the success rate was less

2 See the supplementary information for experimental results on other

datasets.

than 90%, we regarded the trial as a failure. When local-
ization failed even once, the methods could not estimate the
camera position later because loop closing and relocaliza-
tion were disabled in this experiment. Therefore, robustness
greatly contributed to the success rate.

Localization accuracy: The localization accuracy was
computed by scaling the estimated trajectories so that the
RMSE from ground truth trajectories was minimized be-
cause scale is not available in monocular SLAM. Note that
we did not evaluate the accuracy when the success rate was
less than 90% because the RMSE of very short trajectories
tends to have an unfairly high accuracy.

Number of initialization retries: Initialization plays an
important role in monocular visual SLAM and signiﬁcantly
affects the success rate. Because different methods have dif-
ferent initialization processes, the number of initialization
retries in each method is not directly comparable, but can
be a reference regarding whether the method has a weak-
ness in initialization in certain cases.

5.3. Results and Discussion

Table 1 shows the experimental results. The results were
obtained by applying each method to the image sequences
in EuRoC ﬁve times, and Table 1 shows the average val-
ues and the standard deviations of the aforementioned cri-
teria. Bold font is used to emphasize the highest accuracy
in each sequence. Regarding LSD-SLAM, the results of ini-
tialization retries are not included in the table because LSD-
SLAM did not have a re-initialization function and failed to
initialize in any sequences. We thus manually identiﬁed the
frame for which the initialization worked well in each trial,
so the number of retries of LSD-SLAM was excluded from
the evaluation.

The EuRoC image sequences MH01, MH02, MH04,
MH05, V101, V201, and V202 are relatively easy cases
for visual SLAM because the camera motion is relatively
slow and the illumination does not change frequently. By
contrast, the camera moves fast in MH03, V102, V103, and
V203, and additionally the lighting conditions dynamically
change in V102, V103, and V203. Furthermore, in V203,
the exposure time is so long that we can see severe mo-
tion blur in the image sequence, particularly in an extremely
dark environment.

Even for the challenging environments, VITAMIN-E
never lost localization and outperformed other SLAM meth-
ods, DSO, ORB-SLAM, and LSD-SLAM, both in terms of
accuracy and robustness, as shown in Table 1. Particularly,
in the sequences that contain fast camera motion, such as
V102 and V103, the proposed method was superior to the
existing methods. The reconstruction results are shown in
Figures 3 and 4. Despite the proposed VITAMIN-E suc-
cessfully generating dense and accurate geometry compared
with its competitors, it performed equally fast on a CPU, as

9645

Table 1. Experimental results : localization success or failure [✓ or ×], localization accuracy [cm], localization success rate [%], and
number of initialization retries.

Sequence name
(no. of images)

Our method

DSO[6]

MH01 easy

✓ 12.9 ± 0.5

cm ✓

6.0 ± 0.8

cm ✓

(3682)

100.0 ± 0.0 %

100.0 ± 0.0 %

0 ± 0

0 ± 0

MH02 easy

✓

8.8 ± 0.5

cm ✓

4.2 ± 0.2

cm ✓

(3040)

100.0 ± 0.0 %

100.0 ± 0.0 %

MH03 medium ✓ 10.6 ± 1.3

cm ✓ 21.1 ± 0.9

cm ×

(2700)

100.0 ± 0.0 %

100.0 ± 0.0 %

0 ± 0

0 ± 0

ORB-SLAM[21]
w/o loop closure

LSD-SLAM[5]
w/o loop closure

5.2 ± 1.1
97.7 ± 1.6
19 ± 11
4.1 ± 0.4
92.4 ± 1.1

56 ± 6

cm ×
%

cm ×
%

(44.9 ± 7.2)
cm
28.9 ± 23.6 %

−

(58.3 ± 6.9)

cm
73.0 ± 1.5 %

−

(4.5 ± 0.4)
48.9 ± 0.8 %

cm × (266.2 ± 61.3)

cm
28.4 ± 20.7 %

0 ± 0

0 ± 0

0 ± 0

−

MH04 difﬁcult ✓ 19.3 ± 1.6

cm ✓ 20.3 ± 1.0

cm ✓

(2033)

100.0 ± 0.0 %

95.7 ± 0.0 %

MH05 difﬁcult ✓ 14.7 ± 1.1

cm ✓ 10.2 ± 0.6

cm ✓

(2273)

100.0 ± 0.0 %

95.5 ± 0.0 %

0 ± 0

5 ± 0

33.6 ± 9.4
95.2 ± 0.8 %

cm × (136.4 ± 114.3)

cm
27.2 ± 7.0 %

6 ± 1

14.9 ± 4.6
90.0 ± 4.0 %

cm ×

−

(27.4 ± 16.4)

cm
22.7 ± 0.5 %

−

cm ×
%

(20.0 ± 22.8)

cm
11.6 ± 11.2 %

V101 easy

(2911)

0 ± 0

2 ± 0

✓

9.7 ± 0.2

cm ✓ 13.4 ± 5.8

cm ✓

100.0 ± 0.0 %

100.0 ± 0.0 %

0 ± 0

0 ± 0

18 ± 5

8.8 ± 0.1
96.6 ± 0.0

1 ± 0

V102 medium

✓

9.3 ± 0.6

cm ✓ 53.0 ± 5.5

cm × (14.5 ± 11.7)

cm ×

(1710)

100.0 ± 0.0 %

100.0 ± 0.0 %

52.0 ± 3.3 %

V103 difﬁcult

✓ 11.3 ± 0.5

cm ✓ 85.0 ± 36.4

cm × (37.2 ± 20.7)

cm ×

(2149)

100.0 ± 0.0 %

100.0 ± 0.0 %

65.5 ± 8.8 %

0 ± 0

0 ± 0

17 ± 4

V201 easy

(2280)

0 ± 0

0 ± 0

✓

7.5 ± 0.4

cm ✓

7.6 ± 0.5

cm ✓

100.0 ± 0.0 %

100.0 ± 0.0 %

0 ± 0

0 ± 0

56 ± 26
6.0 ± 0.1
95.2 ± 0.0

0 ± 0

V202 medium

✓

8.6 ± 0.7

cm ✓ 11.8 ± 1.4

cm ✓

(2348)

100.0 ± 0.0 %

100.0 ± 0.0 %

0 ± 0

0 ± 0

12.3 ± 2.7
99.5 ± 1.2 %

cm ×

0 ± 0

V203 difﬁcult

✓ 140.0 ± 5.2

cm ✓ 147.5 ± 6.6

cm × (104.3 ± 64.0)

cm ×

(1922)

100.0 ± 0.0 %

100.0 ± 0.0 %

0 ± 0

0 ± 0

16.8 ± 15.9 %
233 ± 123

cm × (131.3 ± 20.4)
%

cm
74.1 ± 8.9 %

−

(67.0 ± 14.0)

cm
15.2 ± 0.1 %

−

(29.3 ± 2.0)

cm
11.0 ± 0.1 %

−

−

(42.1 ± 9.2)

cm
11.3 ± 0.2 %

−

(17.7 ± 1.6)

cm
11.9 ± 0.2 %

−

Table 2. Average tracking time per frame in each method.

Our method

36 msec/frame

DSO

53 msec/frame

ORB-SLAM

25 msec/frame

LSD-SLAM

30 msec/frame

Table 3. Average computation time for each process of VITAMIN-E. Whereas the front-end processes ran in parallel for each frame, the
back-end processes for generating the 3D mesh model were performed at a certain interval.

Feature tracking Localization & mapping Meshing & denoising

TSDF updating & marching cubes

TSDF updating & marching cubes

Front-end

Back-end

(low-resolution; voxel size ≃ 15 cm)

(high-resolution; voxel size ≃ 2.5 cm)

36 msec/frame

25 msec/frame

45 msec/frame

175 msec/time

4000 msec/time

shown in Tables 2 and 3. Note that the smaller the size of
the voxel in a TSDF for a detailed 3D model, the higher the
computational cost.

The high accuracy and robustness of VITAMIN-E de-
rives from tracking a large number of feature points and
performing bundle adjustment for every frame. Sharing re-
projection errors among variables is important for accurate
monocular SLAM, and the proposed method efﬁciently dif-
fuses errors to an enormous number of variables via fast
bundle adjustment. Simultaneously, it prevents localization
failure caused by losing sight of some feature points by han-

dling a large number of feature points.

6. Conclusion

In this paper, we proposed a novel visual SLAM method
that reconstructs dense geometry with a monocular cam-
era. To process a large number of feature points, we pro-
posed curvature extrema tracking using the dominant ﬂow
between consecutive frames. The subspace Gauss–Newton
method was also introduced to maintain an enormous num-
ber of variables by partially updating them to avoid a large

9646

Figure 3. Reconstruction results : (a) dense extrema tracking in real time, (b) reconstructed 3D points, (c) mesh models, and (d) normal
maps generated with the proposed dense geometry reconstruction in different sized TSDF voxels, and reconstructed 3D points in the same
scenes with (e) ORB-SLAM and (f) DSO.

inverse matrix calculation in bundle adjustment. More-
over, supported by the accurate and dense point clouds,
we achieved highly dense geometry reconstruction with
NLTGV minimization and TSDF integration.

should be improved when loop closing is introduced in
VITAMIN-E, and fusing IMU data would also be effective
to stably estimate the camera pose for challenging environ-
ments such as EuRoC V203.

VITAMIN-E is executable on a CPU in real time, and it
outperformed state-of-the-art SLAM methods, DSO, ORB-
SLAM, and LSD-SLAM, both in terms of accuracy and ro-
bustness on the EuRoC benchmark dataset. Performance

Acknowledgements: This work was supported in part by JSPS KAK-

ENHI, Japan Grant Numbers 16K16084 and 18K18072, and a project com-

missioned by the New Energy and Industrial Technology Development Or-

ganization (NEDO).

9647

Absolute translation error on MH03.

50 msec/frame

Our Method

DSO

ORB-SLAM

LSD-SLAM

1340 frame

1350 frame
Fast motion

1360 frame

0

250

500

750

1000

1250

1350

1500

Frame Index

1750

2000
2010

Absolute translation error on V102.

2250

2500

2750

2000 frame

2010 frame

2020 frame

Nearly pure rotation

50 msec/frame

Our Method

DSO

ORB-SLAM

LSD-SLAM

90 frame

100 frame

110 frame

Drastic depth change

0

200

400

600

100

800

910

1000

Frame Index

1200

1400

1600

1800

900 frame

910 frame

920 frame

Absolute translation error on V103.

Our Method

DSO

ORB-SLAM

LSD-SLAM

Nearly pure rotation

50 msec/frame

1327 frame

1337 frame

1347 frame

Fast motion &  Rapid illumination change

]

m

[
r
o
r
r
E

]

m

[
r
o
r
r
E

]

m

[
r
o
r
r
E

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0

250

500

750

1000

1250

1500

1750

Frame Index

1337

1880

2000

2250

1870 frame

1880 frame

1890 frame

Nearly pure rotation

Figure 4. Chronological RMSEs of estimated trajectories on EuRoC V102, V103, and MH03, and the reconstructed point clouds in each
scene: VITAMIN-E successfully estimated the camera trajectories despite a drastic depth change, nearly pure camera rotation, rapid camera
motion, and severe lighting conditions, whereas the competitors suffered from them and resulted in large trajectory errors or completely
getting lost.

9648

References

[1] Paul Bergmann, Rui Wang, and Daniel Cremers. Online pho-
tometric calibration of auto exposure video for realtime vi-
sual odometry and SLAM. IEEE Robotics and Automation
Letters, 3, 2018.

[2] Michael Blosch, Sammy Omari, Marco Hutter, and Roland
Siegwart. A multi-state constraint Kalman ﬁlter for vision-
aided inertial navigation. In Proc. of International Confer-
ence on Intelligent Robots (IROS), 2015.

[3] Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas
Schneider, Joern Rehder, Sammy Omari, Markus W Achte-
lik, and Roland Siegwart. The EuRoC micro aerial vehi-
cle datasets.
International Journal of Robotics Research,
35(10):1157–1163, 2016.

[4] Michael Calonder, Vincent Lepetit, Christoph Strecha, and
Pascal Fua. BRIEF: Binary robust independent elementary
features. In Proc. of European Conference on Computer Vi-
sion (ECCV), 2010.

[5] Jakob Engel and Daniel Cremers. LSD-SLAM: Large-scale
direct monocular SLAM. In Proc. of European Conference
on Computer Vision (ECCV), 2014.

[6] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct
sparse odometry. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2018.

[7] Jakob Engel, J¨org St¨uckler, and Daniel Cremers. Large-scale
In Proc. of International

direct slam with stereo cameras.
Conference on Intelligent Robots (IROS), 2015.

[8] Martin A. Fischler and Robert C. Bolles. Random sample
consensus: A paradigm for model ﬁtting with applications to
image analysis and automated cartography. Communications
of the ACM, 24(6):381–395, 1981.

[9] Christian Forster, Zichao Zhang, Michael Gassner, Manuel
Werlberger, and Davide Scaramuzza.
SVO: Semidirect
visual odometry for monocular and multicamera systems.
IEEE Transactions on Robotics, 33(2):249–265, 2017.

[10] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and
robust multi-view stereopsis. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 32(8):1362–1376, 2010.

[11] Wolfgang Hess, Damon Kohler, Holger Rapp, and Daniel
Andor. Real-time loop closure in 2D LIDAR SLAM.
In
Proc. of International Conference on Robotics and Automa-
tion (ICRA), 2016.

[12] Albert S. Huang, Abraham Bachrach, Peter Henry, Michael
Krainin, Daniel Maturana, Dieter Fox, and Nicholas Roy.
Visual odometry and mapping for autonomous ﬂight using
an RGB-D camera. In Proc. of International Symposium on
Robotics Research (ISRR), 2011.

[13] Tong Ke and Stergios Roumeliotis. An efﬁcient algebraic
solution to the perspective-three-point problem. In Proc. of
International Conference on Computer Vision and Pattern
Recognition (CVPR), 2017.

[14] Christian Kerl, J¨urgen Sturm, and Daniel Cremers. Robust
odometry estimation for RGB-D cameras.
In Proc. of In-
ternational Conference on Robotics and Automation (ICRA),
2013.

[15] Georg Klein and David Murray. Parallel tracking and map-
ping for smallar workspaces. In Proc. of International Sym-
posium on Mixed and Augmented Reality (ISMAR), 2007.

[16] Matthew Klingensmith, Ivan Dryanovski, Siddhartha Srini-
vasa, and Jizhong Xiao. Chisel: Real time large scale 3D re-
construction onboard a mobile device using spatially hashed
signed distance ﬁelds. Robotics: Science and Systems, 2015.
[17] Laurent Kneip, Davide Scaramuzza, and Roland Siegwart. A
novel parametrization of the perspective-three-point problem
for a direct computation of absolute camera position and ori-
entation. In Proc. of International Conference on Computer
Vision and Pattern Recognition (CVPR), 2011.

[18] Stefan Leutenegger, Simon Lynen, Michael Bosse, Roland
Siegwart, and Paul Furgale. Keyframe-based visual-inertial
odometry using nonlinear optimization. International Jour-
nal of Robotics Research, 34(3):314–334, 2015.

[19] Simon Lynen, Markus Wilhelm Achtelik, Stephan Weiss,
and Margarita Chli. A robust and modular multi-sensor fu-
sion approach applied to mav navigation. In Proc. of Inter-
national Conference on Intelligent Robots (IROS), 2013.

[20] Anastasios I. Mourikis and Stergios I. Roumeliotis. A multi-
state constraint Kalman ﬁlter for vision-aided inertial naviga-
tion. In Proc. of International Conference on Robotics and
Automation (ICRA), 2007.

[21] Raul Mur-Artal and Juan D. Tard´os. ORB-SLAM2: an
open-source SLAM system for monocular, stereo and RGB-
D cameras.
IEEE Transactions on Robotics, 33(5):1255–
1262, 2017.

[22] Richard A. Newcombe, Shahram Izadi, Otmar Hilliges,
David Molyneaux, David Kim, Andrew J. Davison, Push-
meet Kohi, Jamie Shotton, Steve Hodges, and Andrew
Fitzgibbon. Kinectfusion: Real-time dense surface mapping
and tracking. In Proc. of International Symposium on Mixed
and Augmented Reality (ISMAR), 2011.

[23] W. Nicholas Greene and Nicholas Roy.

Flame: Fast
lightweight mesh estimation using variational smoothing on
delaunay graphs.
In Proc. of International Conference on
Computer Vision (ICCV), 2017.

[24] Helen Oleynikova, Zachary Taylor, Marius Fehr, Roland
Siegwart, and Juan Nieto. Voxblox: Incremental 3D eu-
clidean signed distance ﬁelds for on-board mav planning.
In Proc. of International Conference on Intelligent Robots
(IROS), 2017.

[25] Chanoh Park, Peyman Moghadam, Sooshin Kim, Alberto
Elfes, Clinton Fookes, and Sridha Sridharan. Elastic lidar
fusion: Dense map-centric continuous-time slam. In Proc.
of International Conference on Robotics and Automation
(ICRA), 2018.

[26] Alex Poms, Chenglei Wu, Shoou-I Yu, and Yaser Sheikh.
Learning patch reconstructability for accelerating multi-view
stereo.
In Proc. of International Conference on Computer
Vision and Pattern Recognition (CVPR), 2018.

[27] William H. Press, Saul A. Teukolsky, William T. Vetterling,
and Brian P. Flannery. Numerical Recipes in C. Cambridge
University Press, Cambridge, USA, second edition, 1992.

[28] Tong Qin, Peiliang Li, and Shaojie Shen. Vins-mono: A
robust and versatile monocular visual-inertial state estimator.
IEEE Transactions on Robotics, 34(4):1004–1020, 2018.

9649

[29] Jianbo Shi and Carlo Tomasi. Good features to track.

In
Proc. of International Conference on Computer Vision and
Pattern Recognition (CVPR), 1994.

[30] Frank Steinbr¨ucker, J¨urgen Sturm, and Daniel Cremers. Vol-
umetric 3D mapping in real-time on a cpu. In Proc. of In-
ternational Conference on Robotics and Automation (ICRA),
2014.

[31] Bill Triggs, Philip McLauchlan, Richard Hartley, and An-
drew Fitzgibbon. Bundle adjustment - a modern synthesis.
In Proc. of the Int. WS on Vision Algorithms: Theory and
Practice, 2000.

[32] R. Wang, M. Schw¨orer, and D. Cremers. Stereo DSO: Large-
scale direct sparse visual odometry with stereo cameras. In
International Conference on Computer Vision (ICCV), 2017.
[33] Ji Zhang, Michael Kaess, and Sanjiv Singh. Real-time depth
In Proc. of International

enhanced monocular odometry.
Conference on Intelligent Robots (IROS), 2014.

[34] Ji Zhang and Sanjiv Singh. LOAM: Lidar odometry and
mapping in real-time. Robotics: Science and Systems, 2014.
[35] Zichao Zhang, Christian Forster, and Davide Scaramuzza.
Active exposure control for robust visual odometry in HDR
environments.
In Proc. of International Conference on
Robotics and Automation (ICRA), 2017.

[36] Qian-Yi Zhou and Vladlen Koltun. Color map optimization
for 3d reconstruction with consumer depth cameras. ACM
Trans. Graph., 33(4):155:1–155:10, 2014.

9650

