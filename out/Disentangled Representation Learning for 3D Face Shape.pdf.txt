Disentangled Representation Learning for 3D Face Shape

Zi-Hang Jiang, Qianyi Wu, Keyu Chen, Juyong Zhang∗

University of Science and Technology of China

{jzh0103, wqy9619, cky95}@mail.ustc.edu.cn juyong@ustc.edu.cn

Abstract

In this paper, we present a novel strategy to design dis-
entangled 3D face shape representation. Speciﬁcally, a
given 3D face shape is decomposed into identity part and
expression part, which are both encoded in a nonlinear
way. To solve this problem, we propose an attribute de-
composition framework for 3D face mesh. To better repre-
sent face shapes which are usually nonlinear deformed be-
tween each other, the face shapes are represented by a ver-
tex based deformation representation rather than Euclidean
coordinates. The experimental results demonstrate that our
method has better performance than existing methods on
decomposing the identity and expression parts. Moreover,
more natural expression transfer results can be achieved
with our method than existing methods.

1. Introduction

A 3D face model is comprised of several components
like identity, expression, appearance, pose, etc., and the
3D face shape is determined by identity and expression at-
tributes [19]. Decoupling 3D face shape into these two com-
ponents is an important problem in computer vision as it
could beneﬁt many applications like face component trans-
fer [42, 36], face animation [12, 35], avatar animation [21],
etc. The aim of this paper is to develop an attribute decom-
position model for 3D face shape such that a given face
shape can be well represented by its identity and expression
part.

Some existing 3D face parametric models already rep-
resent face shapes by the identity and expression param-
eters. Blanz and Vetter proposed 3D Morphable Model
(3DMM) [4] to model face shapes. The most popular form
of 3DMM is a linear combination of identity and expres-
sion basis [2, 43]. FaceWareHouse [13] adopts the bilinear
model and constructs face shapes from a tensor with iden-
tity and expression weights. Recently, FLAME [25] utilizes
articulated model along attributes like the jaw, neck et al. to

∗corresponding author

achieve the state-of-the-art result. A common characteris-
tic of these linear and bilinear models is that each attribute
lies in individual linear space and their combination from
each attribute is also linear. Linear statistical models have
limitations like limited expression ability and disentangle-
ment. This limitation comes from the linear formulation
itself [38, 31]. However, facial variations are nonlinear in
the real world, e.g., the variations in different facial expres-
sions. Although some recent works [7, 6, 5, 27, 23] are
proposed to improve statistical models, they still construct
the 3D face shape by linearly combining the basis.

Inspired by rapid advances of deep learning techniques,
learning-based approaches have been proposed to embed
3D face shape into nonlinear parameter spaces, and the rep-
resentation ability of these methods gets greatly improved,
e.g., being able to represent geometry details [3], or recon-
structing whole face shapes using very few parameters [31].
However, all of these methods encode the entire face shape
into one vector in the latent space, and thus cannot distin-
guish the identity and expression separately. On the other
hand, many applications like animation [11], face retarget-
ing [37, 35], and more challenging task like 3D face recog-
nition [30, 26] need to decompose 3D face shape into iden-
tity and expression component.

In this paper, we aim to build a disentangled parametric
space for 3D face shape with powerful representation abil-
ity. Some classical linear methods [4, 13] have already de-
composed expression and identity attributes, while they are
limited by the representation ability of linear models. Al-
though deep learning based method is regarded as a poten-
tial enhancement way, how to design the learning method
is not straightforward e.g. the neural network structure and
the 3D face shape representation features for deep learning.
Besides, another challenging issue is that how to make use
of the identity and expression labels in the existing datasets
like FaceWareHouse [13] for the network training.

To restate the problem, assuming that the identity and
expression are separately encoded as vector zid and zexp,
the linear model like 3DMM decodes the shape via a linear
transformation in the form ¯S + Aidzid + Aexpzexp, where ¯S
is mean shape, Aid and Aexp are the identity and expression

111957

PCA basis. Considering its non-linear nature, we propose
to recover the shape via a nonlinear decoder in the form
F (Did(zid), Dexp(zexp)), where Did(·), Dexp(·) and F (·)
are nonlinear mapping functions learned by the deep neu-
ral network. For this learning task, we develop a general
framework based on spectral graph convolution [17], which
allows inputting vertex based feature on the mesh and de-
couples 3D face shape into separated attribute components.
Considering that different face shapes are mainly caused by
deformations, we propose to represent the input face shape
of the neural network with vertex based deformation rather
than Euclidean coordinates. The vertex based deformation
representation for 3D shape is proposed in [20, 34, 41],
which captures local deformation gradient and is deﬁned on
vertices. In our experiments, vertex based deformation rep-
resentation can greatly improve the representation ability,
and make the shape deformation more natural. In summary,
the main contributions of this paper include the following
aspects:

• We propose to learn a disentangled latent space for 3D
face shape that enables semantic edit in identity and
expression domains.

• We propose a novel framework for the disentangling
task deﬁned on 3D face mesh. Vertex-based deforma-
tion representation is adopted in our framework, and
it achieves better performance than Euclidean coordi-
nates.

• Experimental results demonstrate that our method can
achieve much better results in disentangling identity
and expression. Therefore, applications like expres-
sion transfer based on our method can get more satis-
fying results.

2. Related Work

Linear 3D Face Shape Models Since the similar work of
3DMM [4], linear parametric models are widely used to
represent the 3D face shapes. Vlasic et al. [40] propose
a multi-linear model to decouple attributes into different
modes and Cao et al. [13] adopt a bilinear model to repre-
sent 3D face shape via identity and expression parameters.
Recently, other methods were proposed for further improve-
ment. E.g, by using a large scale dataset to improve 3DMM
ability [5], or using an articulated model to better capture
middle-end of face [25].
Nonlinear 3D Face Models Recently, some works pro-
pose to embed the 3D face shapes by the nonlinear para-
metric model with the powerfulness of deep learning based
method. Liu et al. [26] propose a multilayer perceptron to
learn a residual model for 3D face shape. Tran [38] put for-
ward an encoder-decoder structure for 3D face shape, which
is a part of the nonlinear form of 3DMM. Bagautdinov et

al. [3] propose a compositional Variational Autoencoder
structure for representing geometry details in different lev-
els. Tewari et al. [3] generate 3D face by self-supervised
approach. Anurag et al. [31] propose a graph-based convo-
lutional autoencoder for 3D face shape. These works adopt
deep neural network to learn a new parametric latent space
for 3D face shape, while none of them consider the problem
of face attribute decoupling.
Deep Learning for 3D Shapes Analysis Deep learning
based method for 3D shapes analysis attracts more and more
attentions in recent years [9]. Masci et al. [28] ﬁrst propose
mesh convolutional operations for local patches in geodesic
polar coordinates. Sinha et al. [33] use geometry image to
represent Euclidean parametrization of a 3D object. Monti
and Boscaini et al. [29] introduce d-dimensional pseudo-
coordinates that deﬁne a local system around each point
with weight functions in the spatial domain. Tan et al. [34]
apply spatial graph convolution to extract localized defor-
mation components of mesh. Bruna et al. [10] ﬁrst propose
spectral graph convolution by exploiting the connection be-
tween graph Laplacian and the Fourier basis. Defferrard et
al. [17] further improve the computation speed of spectral
graph convolution by truncated Chebyshev polynomials. In
our framework, we adapt fast spectral graph convolution
operator for shape attribute extraction. To the best of our
knowledge, this is the ﬁrst deep learning based method for
the disentangling task deﬁned on 3D mesh data.

3. Disentangled 3D Face Representation

3.1. Overview

Given a collection of 3D face meshes, we aim to ob-
tain a compact representation of identity and expression. A
common observation in expression analysis [14] is that hu-
man expressions lie in a high-dimension manifold, and an
illustration is shown in Fig. 1 where expression manifold

Figure 1. 3D face shape space illustration. As observed in [14],
the human expression should lie in a manifold. Based on that,
we illustrate each 3D face lie in its expression manifold. Those
expression manifolds of different identities should be similar [14,
18].

11958

of each individual is rendered in yellow. As the expres-
sion manifolds of different individuals are similar [18], an
expression of one person could be translated to the same
expression on the mean face. On the other hand, each indi-
vidual has its neutral expression, which is set as the origin
point in each manifold and used to represent his/her identity
attribute. Likewise, the same expression on mean face rep-
resents her/his expression attribute. These two meshes are
denoted as identity mesh and expression mesh respectively.
Based on this observation, our disentangled 3D face rep-
resentation includes two parts: decomposition and fusion
networks. Decomposition network disentangles attributes
by decoupling the input face mesh into identity mesh and
expression mesh. And the fusion network recovers the orig-
inal face mesh from identity mesh and expression mesh.

We deﬁne a facial mesh as graph structure with a set
of vertices V and edges, M = (V, A) with |V| = n .
A ∈ {0, 1}n×n represents the adjacency matrix, where
Aij = 1 denotes an edge connection between vertex vi and
vj , and Aij = 0 otherwise. In our framework, the facial
meshes in the training data set contain the same connectiv-
ity, and each vertex is associated with a feature vector Rd.
The graph feature of mesh M is denoted as G ∈ R|V|×d.
In our proposed method, a 3D face mesh M is paired with
two meshes, identity mesh Mid and expression mesh Mexp.
The triplet (M, Mid, Mexp) will be used for training our
networks.

Spectral Graph Convolution Like convolution (correla-
tion) operator for regular 2D image, we adopt a graph con-
volution operator, spectral graph convolution, for extract-
ing useful vertex feature on mesh. We ﬁrst provide some
background about this convolution, and more details can be
found in [10, 17, 22].

2 AD− 1

a diagonal matrix with Di,i = Pn

As we deﬁne our mesh M = (V, A) in graph structure,
the normalized Laplacian matrix can be deﬁned as L =
I − D− 1
2 , where D is the degree matrix, speciﬁcally,
j=1 Ai,j and I stands
for identity matrix. Spectral graph convolution deﬁned on
graph Fourier transform domain, which is eigenvectors U
of laplacian matrix L: L = U ΛU T . The convolution on
Fourier space is deﬁned as x ∗ y = U ((U T x) ⊗ (U T y)),
where ⊗ is the element-wise Hadamard product. It follows
that a signal x is ﬁlter by gθ as y = gθ(L)x. An efﬁcient
way in computation of spectral convolution is parametrized
gθ as a Chebyshev polynomial of order K, like input x ∈
Rn×Fin :

Fin

K−1

i,jTk( ˜L)xi,
θk

(1)

yj =

Xi=1

Xk=0

where yj is the j-th feature of y ∈ Rn×Fout , ˜L =
2L/λmax − In is a scaled Laplacian matrix, λmax is the
maximum eigenvalue, Tk is the Chebyshev polynomial

of order k and can be compute recursively as Tk(x) =
2xTk−1(x) − Tk−2(x) with T0 = 1 and T1 = x. Each
convolution layer has Fin × Fout vector of Chebyshev co-
efﬁcients, θi,j ∈ Rk, as trainable parameters.

Deformation Representation In existing 3D face shape
representation works [4, 13, 25, 31], Euclidean coordinate
in R3 is the most common used vertex feature. With spectral
graph convolution, we can use other features deﬁned on the
vertex. As pointed out in [24], spectral graph convolution is
a special form of Laplacian smoothing. Since the main dif-
ference among different facial meshes is mainly caused by
non-rigid deformations, we prefer a vertex feature related to
local deformation rather than the widely used Euclidean co-
ordinate. In this work, we adopt a recent deformation rep-
resentation (DR) [20, 41] to model 3D mesh. We choose
neutral expression of mean face as reference mesh, and oth-
ers are treated as deformed meshes. We brieﬂy introduce the
details on how to compute DR feature for a given deformed
mesh.

Let us denote the position of the ith vertex vi on the ref-
erence mesh as pi , and the position of vi on the deformed
mesh as p′
i. The deformation gradient in the 1-ring neigh-
borhood of vi from the reference model to the deformed
model is deﬁned as the afﬁne transformation matrix Ti that
minimizes the following energy:

E(Ti) = Xj∈Ni

cijk(p′

i − p′

j) − Ti(pi − pj)k2

(2)

where Ni is the 1-ring neighborhood of vertex vi and cij
is the cotangent weight depending only on the reference
model to cope with irregular tessellation [8]. By polar de-
composition Ti = RiSi, Ti can be decomposed into a ro-
tation part Ri and a scaling/shear part Si, where rotation
can be represent as rotating around the axis ωi by angle θi.
We collect non-trivial entries in the rotation and scale/shear
components, and obtain the deformation representation of
ith vertex in deformed mesh as a R9 vector. The DR feature
of a mesh can treat as a graph feature G ∈ R|V|×9 when
d = 9.

3.2. Decomposition Networks

The input of decomposition networks is deformation rep-
resentation feature G of 3D face mesh, and our goal is to
disentangle it into identity and expression attributes. It is
equivalent to map the input mesh M to the other two triplet
elements (Mid, Mexp).

Decomposition part includes two parallel networks with
the same structure, one for extracting expression mesh
Mexp and the other for extracting identity mesh Mid. Tak-
ing the identity branch as an example, the input will go
through several spectral graph convolution layers for mesh
feature extraction, with a bottleneck architecture of fully

11959

Figure 2. Framework overview. Our network includes two parts, the decomposition part and the fusion part. There are two branches in the
decomposition part, one for expression extraction and the other one for identity extraction. Fusion module targets for recovering original
mesh from the output of the decomposition part.

connected layers as an encoder-decoder structure. This
structure is applied to obtain latent identity representation.

The output should be close to DR feature of Mid.
The same structure and principle are applied on expression
branch to obtain expression mesh Mexp. We use the bottle-
neck layer in encoder-decoder part for each branch as a new
compact parametric space for the corresponding attribute.
These two branches accomplish attribute disentanglement
task as shown in Fig.2.

We denote Gid as the deformation representation of iden-
tity mesh Mid, so does Gexp for expression mesh Mexp. In
order to control the distribution in latent space, we use vari-
ational strategy when training each branch. Let Did and
Dexp be the decoder for identity and expression extraction,
and zid, zexp be the latent representation of each branch, the
loss terms are deﬁned as:

Lid = kGid − Did(zid)k1
Lid kld = KL(N (0, 1)kQ(zid|Gid))
Lexp = kGexp − Dexp(zexp)k1
Lexp kld = KL(N (0, 1)kQ(zexp|Gexp)),

(3)

where Lid and Lid kld are identity reconstruction loss and
KullbackLeibler (KL) divergence loss, so do Lexp and
Lexp kld for expression attribute. The KL loss enforces a
unit Gaussian prior N (0, 1) with zero mean on the distribu-
tion of latent vectors Q(z).

3.3. Fusion Network

As a representation, it is essential to rebuild the origi-
nal input from the decomposed identity and expression at-
tributes. Therefore, we naturally propose a fusion module
to merge identity and expression meshes pair (Mid, Mexp)
for reconstruction. And this module further guarantees that
our decomposition is, in a sense, lossless. Since the mesh
triplets are isomorphic, we can get a new graph by concate-
nating vertex features from identity and expression mesh.
The new graph has the same edge set and vertex set with
the original input, except for the concatenated 2d-dimension
feature on each vertex. The fusion module targets to convert
this new graph with vertex feature in R2d to an isomorphic
graph with vertex feature in Rd (original input). We also
apply spectral graph convolution with activation layers to
achieve this target.

Now, let Gcat = [ ˆGid, ˆGexp] be the concatenated new
graph feature and Gori be the feature of the original mesh
M. Here ˆGid, ˆGexp are outputs of the identity/expression
branch respectively. The loss function for the fusion mod-
ule is:

Lrec = kF (Gcat) − Gorik1,

(4)

where F represents the fusion network.

3.4. Training Process

We ﬁrst pretrain the decomposition network and fusion
network sequentially. Then we train the entire network in
an end-to-end strategy. During the end-to-end training step,

11960

FusionModuleIdentityBranchExpressionBranchϵ~𝑁(0,1)⨂⨁ϵ~𝑁(0,1)⨂⨁𝐷𝑒𝑥𝑝𝐷𝑖𝑑𝐸𝑖𝑑𝐸𝑒𝑥𝑝IdentityBranchϵ~𝑁(0,1)⨂⨁𝐷𝑖𝑑𝐸𝑖𝑑ExpressionBranchϵ~𝑁(0,1)⨂⨁𝐷𝑒𝑥𝑝𝐸𝑒𝑥𝑝መ𝒢𝒢መ𝒢𝑒𝑥𝑝መ𝒢𝑖𝑑ExpressionMeshIdentityMesh𝐹𝑧𝑒𝑥𝑝𝑧𝑖𝑑MeanNeutralMeshwe add disentangling loss in the following form:

Ldis = kDexp(Eexp( ˆGid)) − ¯Gk1 + kDid(Eid( ˆGexp)) − ¯Gk1,
(5)

where ¯G is the feature of mean neutral face, as shown in
Fig. 2. The disentangling loss guarantees the identity part
containing no expression information, and the expression
part does not contain any identity information. In summary,
the full loss function is deﬁned as follow:

Ltotal = Lrec + Ldis + Lid + Lexp+
αid kldLid kld + αexp kldLexp kld.

(6)

Data Augmentation We train our model with FaceWare-
House [13] dataset, which includes 150 identities and 47 ex-
pressions for each identity. In our experiment, as the quan-
tity of identities is very small, there exists an over-ﬁtting
problem in the training process of identity decomposition
branch. We develop a novel data augmentation method to
overcome such over-ﬁtting problem. Given m identity sam-
ples in the training set, we generate new 3D face meshes
via interpolations among m samples. The deformation rep-
resentation(DR) features of these identity samples are de-
noted as (DR1, DR2, . . . , DRm). We generate new DR
features and reconstruct the 3D face meshes from these
new DR features. We create an uniform distribution vec-
tor, (r, θ1, . . . , θm−1) in polar coordinates system, where r
follows uniform distribution U(0.5, 1.2), and others follow
uniform distribution U(0, π/2). We convert the above polar
coordinates into Cartesian coordinates (a1, . . . , am), and
i=1 aiDRi.
These m features are a bootstrap sample from the training
dataset. This data augmentation method can create various
3D faces with only several samples from the training set
and can solve the over-ﬁtting problem. In our experiment,
we set m = 5 and generate 10000 new 3D face meshes (see
supplementary for some examples) for training.

interpolate the sampled m DR features by Pm

4. Experiment

In this section, we will ﬁrst introduce our implementa-
tion1 details in 4.1. details in 4.1. Then we will introduce
several metrics used for measuring reconstruction and de-
composition accuracy in 4.2. Finally, we will show our
experiments on two different datasets in Sec 4.3 and 4.4,
including ablation study and comparison with baselines.

4.1. Implementation Details

At ﬁrst, we introduce data preparation procedure of gen-
erating the ground-truth identity and expression mesh. Tak-
ing FaceWareHouse for example, the neutral expression of a
subject represents his/her identity mesh. As for expression

mesh, we compute the average shape of the same expres-
sion belonging to 140 subjects and deﬁne the output 47 ex-
pressions as the ground-truth meshes on mean face. These
operations can also be applied to other 3D face shape data
sets.

Our algorithm is implemented in Keras [15] with Tensor-
ﬂow [1] backend. All the training and testing experiments
were tested on a PC with NVIDIA TiTan XP and CUDA
8.0.

We train our networks for 50 epochs per step with a
learning rate of 1e-4, and a learning rate decay of 0.6 ev-
ery 10 epochs. The hyper-parameters αid kld, αexp kld are
set as 1e-5.

4.2. Evaluation Metric

The main target of our method is to decompose a given
3D face shape into identity and expression parts as accurate
as possible and achieve high 3D shape reconstruction ac-
curacy at the same time. Therefore, evaluation criteria are
designed based on these two aspects.

4.2.1 Reconstruction Measurement

i=1 kvi − v′

|V|P|V|

We adopt two kinds of metrics to evaluate the 3D shape
reconstruction accuracy.
Average vertex distance The average vertex distance Eavd
between reconstructed mesh M′ and original mesh M is
deﬁned as: Eavd(M, M′) = 1
Perceptual Error As Eavd can not reﬂect perceptual dis-
tance [16, 39]. In [39], spatial-temporal edge difference
was proposed to measure perceptual distance by the local
error of dynamic mesh independent of its absolute position.
In this work, we adopt the spatial edge difference error Esed
to measure the perceptual error. Let eij be the edge con-
nects vi and vj of original mesh M, and edge e′
ij is the
corresponding edge in reconstructed mesh M′, the relative
edge difference is deﬁned as: ed(eij, e′

ik2.

ij k

ij) = |

keij k−ke′
keij k

|

The weighted average of relative edge difference around
ij )

a vertex vi is computed as: ¯ed(vi) =
where lij is the edge length of edge eij . Therefore the local
deviation around a vertex vi can be expressed by

lij ed(eij ,e′

Pj∈Ni

Pj∈Ni

lij

σ(vi) =sPj∈Ni

lij(ed(eij, e′

ij) − ¯ed(vi))2
lij

.

(7)

Pj∈Ni

We compute the average local deviation over all the vertices
and get the spatial edge difference error:

Esed =

1
|V|

|V|

Xi=1

σ(vi).

(8)

1Avalible at https://github.com/zihangJiang/DR-Learning-for-3D-Face

And smaller value of Esed means better perceptual result.

11961

Method

Eavd

Esed

Eid

Eexp

Mean Error Median Mean Error Median Mean Error Median Mean Error Median
0.484
0.630

0.472
0.328

0.527
0.711

0.477
0.329

Bilinear [13]
FLAME [25]
MeshAE [31]
Ours w/o DR & Fusion
Ours w/o DR
Ours w/o Fusion
Ours

0.993
0.882
0.825
0.981
0.939
0.661
0.472

0.998
0.905
0.811
1.292
0.836
0.579
0.381

0.0243
0.0144
0.0151
0.177
0.447
0.00283
0.00333

0.0183
0.0074
0.0777
0.0938
0.388
0.0000
0.0000

-

0.395
0.446
0.183
0.121

-

0.380
0.463
0.178
0.121

-

0.170
0.0992
0.0582
0.0388

-

0.160
0.0750
0.0494
0.0267

Table 1. Quantitative results on Facewarehouse. All number were in millimeters. DR: deformation representation; Fusion: fusion module.

4.2.2 Decomposition Measurement

To measure the disentangled representation for 3D face
shape, we propose a metric for reconstructed identity mesh
from the models with the same identity and different expres-
sions, and expression mesh from the models with different
identities and the same expression.

Taking identity part for example, we denote {Mi} as
the test set containing a series of expressions of an iden-
tical person. A good decomposition method is supposed
to decompose {Mi} into several similar identity features
and various expression features. Moreover, the meshes re-
constructed from those identity features are supposed to be
similar with each other, hence the standard deviation of re-
constructed identity meshes {Mi
id} is suitable to be used
to evaluate the decomposed ability of the disentangled rep-
resentation. And it is the same to other test set {N j} con-
sisted of identical expressions and different identities. So
the decomposition metric is deﬁned as follow:

Eid = σ({Mi
Eexp = σ({N j

id})
exp}),

(9)

id} and {N j

where {Mi
exp} are reconstructed identity and
expression meshes of test sets {Mi} and {N j}, while σ is
the standard deviation operator. This metric adopts vertex
distance.

4.3. Experiments on FaceWareHouse [13]

FaceWareHouse is a widely used 3D face shape dataset
developed by Cao et al., which includes 47 expressions
along 150 different identities. It is easy to obtain the train-
ing triplets from Facewarehouse dataset. We conduct ab-
lation study of our framework and compare our method
with the bilinear model which is widely referred with this
dataset. In all the experiments of this part, we choose the
ﬁrst 140 identities with their expression face shapes to build
the training set, and the left 10 identities for testing.

where Cr is the reduced core tensor containing the top-left
corner of the original tensor produced by HO-SVD decom-
position, αid and αexp are the row vectors of identity and
expression weights. And 50 and 25 are recommended as
the proper reduced dimensions of identity and expression
subspaces [13].

exp.

id, αi

For a given 3D face shape, αid and αexp can be
optimized by applying Alternating Least Squares (ALS)
method to the tensor contraction. We denote {Mi} like we
used in 4.2.2 and optimize (αi
exp) for each Mi. The
identity mesh is reconstructed with identity parameters αi
id
and neutral expression parameters, and the expression mesh
is reconstructed with mean face identity and expression pa-
rameter αi
FLAME Li et al. [25] propose FLAME model by represent-
ing 3D face shape including identity, expression, head rota-
tion, and yaw motion with linear blendskinning and achieve
state of the art result. For comparison, we train FLAME
with identity model and expression model.
MeshAE Anurag [31] proposed a spectral graph convolu-
tional mesh autoencoders (MeshAE) structure for 3D face
shape embedding. We also evaluate the model’s reconstruc-
tion ability on FaceWareHouse dataset as it encode whole
shape 3D face without disentangling identity and expres-
sion.

For a fair comparison,

the dimensions of our latent
spaces (identity zid and expression zexp) are separately set
as 50 and 25, the same with the bilinear model and FLAME.
And the size of latent space for Mesh AutoEncoder (Me-
shAE) is set as 75. Quantitative results are given in Tab
1. Our framework gets much better result in each evalua-
tion. We also show qualitative visual result of our results
on identity and expression decomposition in Fig 3. The vi-
sual result and numerical result demonstrate that our dis-
entangled learning not only achieves better reconstruction
accuracy but also neatly decouples expression and identity
attributes.

4.3.1 Baseline Comparison

4.3.2 Ablation Study

Bilinear model Cao et al. [13] proposed 2-mode tensor
product formulation for 3D face shape representations as:

M = Cr ×2 αid ×3 αexp

(10)

In our framework, we have two novel designs including 3D
face shape representation and fusion network, which greatly
improve the representation ability of our method. To inves-

11962

Average error Mean Error Median Error

FLAME [25]
Ours

2.001
1.643

1.615
1.536

Table 2. Extrapolation results on COMA dataset. All results are in
millimeters.

considering identity and expression attribute. We evaluate
the ability of extrapolation over expression by training our
model with COMA dataset. However, different from Face-
WareHouse dataset, the shape models in COMA dataset are
not speciﬁed with expression labels. We manually select 12
models with representative expressions for all the 12 iden-
tities. For each shape model in the remaining, the resid-
ual DR feature between the original model and its identity
model is used for supervision during the training process.

To measure the generalization of our model, we perform
12 cross validation for one expression. For our method, we
set our latent vector size as 8, with 4 for identity and 4 for
expression. And we compare our method with FLAME,
which is the state-of-the-art 3D face model representation
with decomposed attributes. For comparison, FLAME is
trained for expression model and obtained with 8 compo-
nents for identity and expression respectively.

We compare our method with FLAME on expression ex-
trapolation experiment, and report the average vertex dis-
tance on all the 12 cross validation experiments in Tab. 2. It
can be observed that our method gets better generalization
result compared with the state-of-the-art FLAME method
on extrapolation experiment. All the 12 expressions ex-
trapolation cross validation experiments are given in sup-
plementary.

4.5. Discussion on Larger Dataset

There is a long-standing problem in conducting learn-
ing method in 3D vision topic, which is lack of 3D data.
Recently, more and more methods proposed solution to
tackle this problem, e.g. combine multiple dataset by non-
rigid registration. In our framework, we adopt a novel data
augmentation strategy by interpolation/extrapolation of DR
feature. We also design an experiment on a large-scale
dataset. We create a larger dataset by convert Bospho-
rus [32] to mesh by nonrigid registration and combine
with FaceWareHouse (FWH). We evaluate our method on
three different training datasets: original FaceWareHouse,
combination of FWH and Bosphorus, and DR-augmented
FWH. Tab. 3 shows the comparison results. Our augmen-
tation strategy leads to the best scores on all aspects, which
demonstrates that it greatly improves the model’s stability
and robustness. We hope our data augmentation strategy
can beneﬁt 3D vision community.

11963

Figure 3. Results of identity and expression decomposition. The
original and extracted identity and expression components are
given from top to bottom. We show samples from two subjects.

tigate the effectiveness of these two designs, Tab. 1 presents
the variants of our learning method, where w/o is the abbre-
viation of without. In the following, we compare our well-
designed framework with other implementation strategies.
We adopt a novel vertex based deformation representa-
tion [20] for 3D face shape. Another straightforward way
is to directly use the Euclidean coordinates as the method
in [31]. The results of without using DR is reported in
Tab. 1.

Another novel design in our pipeline is the fusion net-
work. A natural replacement for fusion module is to rep-
resent 3D face as a composite model like 3DMM [4, 26]:
G = ¯G + Did(zid) + Dexp(zexp). where ¯G is the feature
of mean face. The result that without using fusion is shown
in Tab. 1. We also report errors without using both designs.
It can be observed from the ablation study, both DR and
fusion network greatly improve the performance. DR sig-
niﬁcantly improved our model’s performance in the aver-
age vertex distance error evaluation. And the fusion mod-
ule helps to disentangle the expression more naturally i.e.
achieves smaller error in Eexp. Our proposed framework
get a slightly higher error in Esed when adding the fusion
module. While considering for all evaluation metrics, our
method still achieves more satisfying result than other com-
parative tests.

4.4. Experiment on COMA Dataset [31]

Very recently, Anurag et al. released the COMA dataset
which includes 20,466 3D face models. This dataset is cap-
tured at 60fps with a multi-camera active stereo system,
which contains 12 identities performing 12 different expres-
sions. COMA dataset was used to build a nonlinear 3D face
representation [31], while it encodes and decodes the whole
3D face shape into one vector in the latent space without

Dataset
Original FWH
Combination
DR-augmented

Eavd

18.3/18.0
16.9/16.6

4.7/3.8

Esed

0.05/0.03
0.06/0.03
0.03/0.00

Eid

1.4/1.4
1.6/1.6
1.2/1.2

Eexp
0.5/0.3
0.5/0.4
0.4/0.3

Table 3. More quantitative results. Table gives our results on
different datasets: original FWH, combination of Bosphorus and
FWH (Combination) and our DR-augmented FWH. All number in
0.1 millimeters.

Figure 4. Exploring interpolation results on latent space. Based on
our method, we can obtain identity and expression code for two
3D face model M0 and M1, and we interpolate latent identity
and expression vectors individually, in stride of 0.25.

 

 

 

 

 

 

Source     Target                       Blinear                       GT                           Ours

0 mm

>8 mm

Figure 5. Expression transfer application. Comparing to the bi-
linear model, our method achieves more natural and stable visual
results.

5. Application

Based on our proposed disentangled representation for
3D face shape, we can apply our model in many applica-
tions like expression transfer and face recognition. In the
following part, we ﬁrst show that our method can achieve
better performances than traditional method on expression
transfer, and then we show the shape exploration results
in the trained identity and expression latent space of our
model.

5.1. Expression Transfer

A standard solution for expression transfer [40, 11, 36]
is to transfer the expression weights from source to target
face. We randomly select two identities from the test data
set of FaceWareHouse to compare the expression transfer
results of the bilinear model and our method. For the bi-
linear model, we ﬁrst solve the identity and expression pa-
rameters for the reference model and then transfer the ex-
pression parameter from the source to the target face. In
our method, we directly apply the latent expression code of
source face to the target face. Some results are shown in
Fig. 5. The corresponding expressions on the target ob-
ject in FaceWareHouse dataset are treated as the ground
truth. It can be easily observed that our method can achieve
more natural and accurate performances, and our results are
closer to the ground truth in quantitative error evaluations.

5.2. Latent space interpolation

Our disentangled representation includes two latent
codes for identity and expression. With the learned latent
spaces, we can interpolate models by gradually changing
identities and expressions. The interpolating operation is
applied on the latent code, and the models are recovered
from the generated code with the trained decoder. In this
experiment, We interpolate latent code by step of 0.25 in
identity and expression separately, and thus we can observe
that the interpolation results are meaningful and reasonable
as shown in Fig. 4,

6. Conclusion

We have proposed a disentangled representation learning
method for 3D face shape. A given 3D face shape can be ac-
curately decomposed into identity part and expression part.
To effectively solve this problem, a well-designed frame-
work is proposed to train decomposition networks and fu-
sion network. To better represent the non-rigid deforma-
tion space, the input face shape is represented as vertex
based deformation representation rather than Euclidean co-
ordinates. We have demonstrated the effectiveness of the
proposed method via ablation study and extensive quantita-
tive and qualitative experiments. Applications like expres-
sion transfer based on our disentangled representation have
shown more natural and accurate results compared with tra-
ditional method.
Acknowledgement We thank Kun Zhou et al. and Arman
Savran et al. for allowing us to use their 3D face datasets.
The authors are supported by National Key R&D Program
of China (No. 2016YFC0800501), National Natural Sci-
ence Foundation of China (No. 61672481), and Youth In-
novation Promotion Association CAS (No. 2018495). This
project is funded by Huawei company.

11964

Expressioncodeℳ𝟎ℳ𝟏References

[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow:
a system for large-scale machine learning.
In OSDI, vol-
ume 16, pages 265–283, 2016. 5

[2] Brian Amberg, Reinhard Knothe, and Thomas Vetter. Ex-
pression invariant 3d face recognition with a morphable
model.
In Automatic Face & Gesture Recognition, 2008.
FG’08. 8th IEEE International Conference on, pages 1–6.
IEEE, 2008. 1

[3] Timur Bagautdinov, Chenglei Wu, Jason Saragih, Pascal
Fua, and Yaser Sheikh. Modeling facial geometry using com-
positional vaes. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 1, 2

[4] Volker Blanz and Thomas Vetter. A morphable model for
the synthesis of 3d faces. In Proceedings of the 26th Annual
Conference on Computer Graphics and Interactive Tech-
niques, SIGGRAPH 1999, Los Angeles, CA, USA, August
8-13, 1999, pages 187–194, 1999. 1, 2, 3, 7

[5] James Booth,

Epameinondas Antonakos,

Stylianos
Ploumpis, George Trigeorgis, Yannis Panagakis, Stefanos
Zafeiriou, et al.
3d face morphable models in-the-wild.
In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 1, 2

[6] James Booth, Anastasios Roussos, Allan Ponniah, David
Dunaway, and Stefanos Zafeiriou. Large scale 3d morphable
models.
International Journal of Computer Vision, 126(2-
4):233–254, 2018. 1

[7] James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan
Ponniah, and David Dunaway. A 3d morphable model learnt
from 10,000 faces. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 5543–5552, 2016. 1

[8] Mario Botsch and Olga Sorkine. On linear variational sur-
face deformation methods. IEEE Transactions on Visualiza-
tion and Computer Graphics, 14(1):213–230, 2008. 3

[9] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur
Szlam, and Pierre Vandergheynst. Geometric deep learning:
going beyond euclidean data. IEEE Signal Processing Mag-
azine, 34(4):18–42, 2017. 2

[10] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Le-
cun. Spectral networks and locally connected networks on
graphs. In International Conference on Learning Represen-
tations, 2014. 2, 3

[11] Chen Cao, Qiming Hou, and Kun Zhou. Displaced dynamic
expression regression for real-time facial tracking and ani-
mation. ACM Transactions on graphics (TOG), 33(4):43,
2014. 1, 8

[12] Chen Cao, Yanlin Weng, Stephen Lin, and Kun Zhou. 3d
shape regression for real-time facial animation. ACM Trans-
actions on Graphics (TOG), 32(4):41, 2013. 1

[13] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun
Zhou. Facewarehouse: A 3D facial expression database for
visual computing. IEEE Transactions on Visualization and
Computer Graphics, 20(3):413–425, 2014. 1, 2, 3, 5, 6

[14] Ya Chang, Changbo Hu, Rogerio Feris, and Matthew Turk.
Manifold based analysis of facial expression. Image and Vi-
sion Computing, 24(6):605–614, 2006. 2

[15] Franc¸ois Chollet et al. Keras. https://github.com/

fchollet/keras, 2015. 5

[16] Massimiliano Corsini, Mohamed-Chaker Larabi, Guillaume
Lavou´e, Oldˇrich Petˇr´ık, Libor V´aˇsa, and Kai Wang. Per-
ceptual metrics for static and dynamic triangle meshes. In
Computer Graphics Forum, volume 32, pages 101–125. Wi-
ley Online Library, 2013. 5

[17] Micha¨el Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. Convolutional neural networks on graphs with
fast localized spectral ﬁltering.
In Advances in Neural In-
formation Processing Systems, pages 3844–3852, 2016. 2,
3

[18] Paul Ekman. Facial action coding system (facs). A human

face, 2002. 2, 3

[19] Katie Fisher, John R Towler, and Martin Eimer. Facial
identity and facial expression are initially integrated at vi-
sual perceptual stages of face processing. Neuropsychologia,
80:115–125, 2016. 1

[20] Lin Gao, Yu-Kun Lai, Jie Yang, Ling-Xiao Zhang, Leif
Kobbelt, and Shihong Xia. Sparse data driven mesh defor-
mation. arXiv preprint arXiv:1709.01250, 2017. 2, 3, 7

[21] Alexandru Eugen Ichim, Soﬁen Bouaziz, and Mark Pauly.
Dynamic 3d avatar creation from hand-held video input.
ACM Transactions on Graphics (TOG), 34(4):45, 2015. 1

[22] Thomas N. Kipf and Max Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. In International
Conference on Learning Representations (ICLR), 2017. 3

[23] Paul Koppen, Zhen-Hua Feng, Josef Kittler, Muhammad
Awais, William Christmas, Xiao-Jun Wu, and He-Feng Yin.
Gaussian mixture 3d morphable face model. Pattern Recog-
nition, 74:617–628, 2018. 1

[24] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights
into graph convolutional networks for semi-supervised learn-
ing. arXiv preprint arXiv:1801.07606, 2018. 3

[25] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and
Javier Romero. Learning a model of facial shape and ex-
pression from 4D scans. ACM Transactions on Graphics,
(Proc. SIGGRAPH Asia), 36(6), 2017. 1, 2, 3, 6, 7

[26] Feng Liu, Ronghang Zhu, Dan Zeng, Qijun Zhao, and Xi-
aoming Liu. Disentangling features in 3d face shapes for
joint face reconstruction and recognition. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2018. 1, 2, 7

[27] Marcel L¨uthi, Thomas Gerig, Christoph Jud, and Thomas
Vetter.
IEEE
transactions on pattern analysis and machine intelligence,
40(8):1860–1873, 2018. 1

Gaussian process morphable models.

[28] Jonathan Masci, Davide Boscaini, Michael Bronstein, and
Pierre Vandergheynst. Geodesic convolutional neural net-
works on riemannian manifolds.
In Proceedings of the
IEEE international conference on computer vision work-
shops, pages 37–45, 2015. 2

[29] Federico Monti, Davide Boscaini,

Jonathan Masci,
Emanuele Rodola, Jan Svoboda, and Michael M Bronstein.

11965

Geometric deep learning on graphs and manifolds using
mixture model cnns.
In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017. 2

recognition in the wild. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 787–796, 2015.
1

[30] Theodoros Papatheodorou and Daniel Rueckert. 3d face

recognition. In Face Recognition. InTech, 2007. 1

[31] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and
Michael J. Black. Generating 3D faces using convolutional
mesh autoencoders. In European Conference on Computer
Vision (ECCV), volume Lecture Notes in Computer Science,
vol 11207, pages ł725–741. Springer, Cham, Sept. 2018. 1,
2, 3, 6, 7

[32] Arman Savran, Nes¸e Aly¨uz, Hamdi Dibeklio˘glu, Oya
C¸ eliktutan, Berk G¨okberk, B¨ulent Sankur, and Lale Akarun.
Bosphorus database for 3d face analysis. In European Work-
shop on Biometrics and Identity Management, pages 47–56.
Springer, 2008. 7

[33] Ayan Sinha, Jing Bai, and Karthik Ramani. Deep learning 3d
shape surfaces using geometry images. In European Confer-
ence on Computer Vision, pages 223–240. Springer, 2016.
2

[34] Qingyang Tan, Lin Gao, Yu-Kun Lai, Jie Yang, and Shi-
hong Xia. Mesh-based autoencoders for localized defor-
mation component analysis.
In Proceedings of the Thirty-
Second AAAI Conference on Artiﬁcial Intelligence, pages
2452–2459, 2018. 2

[35] Justus Thies, Michael Zollh¨ofer, Matthias Nießner, Levi Val-
gaerts, Marc Stamminger, and Christian Theobalt. Real-
time expression transfer for facial reenactment. ACM Trans.
Graph., 34(6):183:1–183:14, 2015. 1

[36] Justus Thies, Michael Zollhofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nießner. Face2face: Real-time
face capture and reenactment of rgb videos. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 2387–2395, 2016. 1, 8

[37] Justus Thies, Michael Zollh¨ofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nießner. Facevr: Real-time
gaze-aware facial reenactment in virtual reality. ACM Trans.
Graph., 37(2):25:1–25:15, 2018. 1

[38] Luan Tran and Xiaoming Liu. Nonlinear 3d face morphable
model. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 1, 2

[39] Libor V´asa and V´aclav Skala. A perception correlated com-
parison method for dynamic meshes. IEEE Transactions on
Visualization and Computer Graphics, 17:220–230, 2011. 5
[40] Daniel Vlasic, Matthew Brand, Hanspeter Pﬁster, and Jovan
Popovi´c. Face transfer with multilinear models. ACM trans-
actions on graphics (TOG), 24(3):426–433, 2005. 2, 8

[41] Qianyi Wu, Juyong Zhang, Yu-Kun Lai, Jianmin Zheng, and
Jianfei Cai. Alive caricature from 2d to 3d. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2018. 2, 3

[42] Fei Yang, Jue Wang, Eli Shechtman, Lubomir Bourdev, and
Dimitri Metaxas. Expression ﬂow for 3d-aware face com-
ponent transfer. ACM Transactions on Graphics (TOG),
30(4):60, 2011. 1

[43] Xiangyu Zhu, Zhen Lei, Junjie Yan, Dong Yi, and Stan Z
Li. High-ﬁdelity pose and expression normalization for face

11966

