CRAVES: Controlling Robotic Arm with a Vision-based Economic System

Yiming Zuo1 ∗, Weichao Qiu2 ∗, Lingxi Xie2,4, Fangwei Zhong3, Yizhou Wang3,5,6, Alan L. Yuille2

1Tsinghua University
4Noah’s Ark Lab, Huawei Inc.

2Johns Hopkins University

3Peking University
5Peng Cheng Laboratory 6DeepWise AI Lab

{zuoyiming17,qiuwch,198808xc,zfw1226,alan.l.yuille}@gmail.com,

yizhou.wang@pku.edu.cn

Abstract

Training a robotic arm to accomplish real-world tasks
has been attracting increasing attention in both academia
and industry. This work discusses the role of computer vi-
sion algorithms in this ﬁeld. We focus on low-cost arms on
which no sensors are equipped and thus all decisions are
made upon visual recognition, e.g., real-time 3D pose es-
timation. This requires annotating a lot of training data,
which is not only time-consuming but also laborious.

In this paper, we present an alternative solution, which
uses a 3D model to create a large number of synthetic data,
trains a vision model in this virtual domain, and applies it
to real-world images after domain adaptation. To this end,
we design a semi-supervised approach, which fully lever-
ages the geometric constraints among keypoints. We apply
an iterative algorithm for optimization. Without any anno-
tations on real images, our algorithm generalizes well and
produces satisfying results on 3D pose estimation, which is
evaluated on two real-world datasets. We also construct
a vision-based control system for task accomplishment, for
which we train a reinforcement learning agent in a virtual
environment and apply it to the real-world. Moreover, our
approach, with merely a 3D model being required, has the
potential to generalize to other types of multi-rigid-body dy-
namic systems.

1. Introduction

Precise and agile robotic arms have been widely used
in the assembly industry for decades, but the adaptation
of robots to domestic use is still a challenging topic. This
task can be made much easier if vision input are provided
and well utilized by the robots. A typical example lies
in autonomous driving [8].
In the area of robotics, re-
searchers have paid more and more attentions to vision-
based robots and collected large-scale datasets, e.g., for ob-
ject grasping [19][29] and block stacking [12]. However,

∗This work was done when the ﬁrst author was an intern at the Johns

Hopkins University. The ﬁrst two authors contributed equally.

the high cost of conﬁguring a robotic system largely limits
researchers from accessing these interesting topics.

This work aims at equipping a robotic system with com-
puter vision algorithms, e.g., predicting its real-time status
using an external camera, so that researchers can control
them with a high ﬂexibility, e.g., mimicking the behavior of
human operators. In particular, we build our platform upon
a low-cost robotic arm named OWI-535 which can be pur-
chased from Amazon1 for less than $40. The downside is
that this arm has no sensors and thus it totally relies on vi-
sion inputs2 – on the other hand, we can expect vision inputs
to provide complementary information in sensor-equipped
robotic systems. We chose this arm for two reasons.
(i)
Accessibility:
the cheap price reduces experimental bud-
gets and makes our results easy to be reproduced by lab re-
(ii) Popularity: users
searchers (poor vision people :( ).
around the world uploaded videos to YouTube recording
how this arm was manually controlled to complete vari-
ous tasks, e.g., picking up tools, stacking up dices, etc.
These videos were captured under substantial environmen-
tal changes including viewpoint, lighting condition, occlu-
sion and blur. This raises real-world challenges which are
very different from research done in a lab environment.

Hence, the major technical challenge is to train a vision
algorithm to estimate the 3D pose of the robotic arm.
Mathematically, given an input image x, a vision model
M : p = h(x; θ) is used to predict p, the real-time 3D
pose of the arm, where θ denotes the learnable parameters,
e.g., in the context of deep learning [17], network weights.
Training such a vision model often requires a considerable
amount of labeled data. One option is to collect a large
number of images under different environments and anno-
tate them using crowd-sourcing, but we note a signiﬁcant
limitation as these efforts, which take hundreds of hours, are
often not transferable from one robotic system to another.
In this paper, we suggest an alternative solution which bor-

1https://www.amazon.com/dp/B0017OFRCY/
2Even when the initialized status of the arm is provided and each action
is recorded, we cannot accurately compute its real-time status because each
order is executed with large variation – even the battery level can affect.

14214

Figure 1. An overview of our system (best viewed in color). The goal is to accomplish tasks using camera as the only sensor. The vision
module detects 2D keypoints of the arm and then computes its 3D pose. The control module uses the 3D pose estimation to determine the
next move and feeds it to the motors. In our setup, scene understanding is achieved by directly providing the 3D location of targets.

rows a 3D model and synthesizes an arbitrary amount of
labeled data in the virtual world with almost no cost, and
later adapts the vision model trained on these virtual data to
real-world scenarios.

This falls into the research area of domain adaptation [4].
Speciﬁcally, the goal is to train M on a virtual distribution
xV ∼ P V and then generalize it to the real distribution
xR ∼ P R. We achieve this goal by making full use of a
strong property, that the spatial relationship between key-
points, e.g., the length of each bone, is ﬁxed and known.
This is to say, although the target distribution P R is differ-
ent from P V and data in P R remain unlabeled, the predicted
keypoints should strictly obey some geometric constraints
τ . To formulate this, we decompose M into two compo-
nents, namely M1 : y = f (x; θ) for keypoint detection and
M2 : p = g(y; τ ) for 3D pose estimation, respectively.
Here, M2 is parameter-free and thus cannot be optimized,
so we train M1 on P V and hope to adapt it to P R, and y be-
comes a hidden variable. We apply an iterative algorithm to

infer p⋆ = arg maxpR Pr(y; θ | x) · Pr(p; τ | y) dy, and

the optimal y⋆ determined by p⋆ serves as the guessed la-
bel, which is used to ﬁne-tune M1. Eventually, prediction is
achieved without any annotations in the target domain.

We design two benchmarks to evaluate our system. The
ﬁrst one measures pose estimation accuracy, for which we
manually annotate two image datasets captured in our lab
and crawled from YouTube, respectively. Our algorithm,
trained on labeled virtual data and ﬁne-tuned with unlabeled
lab data, achieves a mean angular error of 4.81◦, averaged
over 4 joints. This lays the foundation of the second bench-
mark in which we create an environment for the arm to ac-

complish a real-world task, e.g., touching a speciﬁed point.
Both quantitative (in distance error and success rates) and
qualitative (demos are provided in the supplementary ma-
terial) results are reported. Equipped with reinforcement
learning, our vision-based algorithm achieves comparable
accuracy with human operators. All our data and code have
been released at our website, https://craves.ai.

In summary, the contribution of this paper is three-fold.
First, we design a complete framework to achieve satisfying
accuracy in task accomplishment with a low-cost, sensor-
free robotic arm. Second, we propose a vision algorithm
involving training in virtual environment and domain adap-
tation, and verify its effectiveness in a typical multi-rigid-
body system. Third, we develop a platform with two real-
world datasets and a virtual environment so as to facilitate
future research in this ﬁeld.

2. Related Work

• Vision-based Robotic Control

Vision-based robotic control is attracting more and more
attentions. Compared with conventional system relying
on speciﬁc sensors, e.g. IMU and rotary encoder, vision
has the ﬂexibility to adapt to complex and novel tasks.
Recent progress of computer vision makes vision-based
robotic control more feasible. Besides using vision algo-
rithms as a perception module, researchers are also ex-
ploring training an end-to-end control system purely from
vision [13][18][23]. To this end, researchers collected
large datasets for various tasks, including grasping [19][29],
block stacking [12], autonomous driving [8][43], etc.

On the other hand, training a system for real-world con-

24215

ControllerScene Understanding(Detection, Segmentation)Image (cid:1876)3D pose2D KeypointsNew Image (cid:1876)+1Arm MotorsKeypoint Detection3D ReconstructionSyntheticTraining Images(cid:1877)=(cid:1858)(cid:4666)(cid:1876);(cid:4667)=(cid:1859)(cid:4666)(cid:1877);(cid:4667)Figure 2. Here shows the 4 joints and 17 keypoints of OWI-535
used in our experiment. Each joint is assigned a speciﬁc name.
Color of keypoint correspond to the part to which it belongs.

trol tasks is always time-consuming, and high-accuracy
sensor-based robots are expensive , both of which have pre-
vented a lot of vision researchers from entering this re-
search ﬁeld. For the ﬁrst issue, people turned to use sim-
ulators such as MuJoCo [37] and Gazebo [16] so as to ac-
celerate training processes, e.g., with reinforcement learn-
ing, and applied to real robots, e.g., PR2 [39], Jaco [32]
and KUKA IIWA [2](ranging from $50, 000 to $200, 000).
For the second issue, although low-cost objects (e.g., toy
cars [15]) have been used to simulate real-world scenarios,
low-cost robotic arms were rarely used, mainly due to the
limitation caused by the imprecise motors and/or sensors,
so that conventional control algorithms are difﬁcult to be
applied. For instance, Lynxmotion Arm is an inexpensive
($300) robotic arm used for training reinforcement learn-
ing algorithms [31][5]. The control of this arm was done
using a hybrid of camera and servo-motor, which provides
joint angle. This paper uses an even cheaper ($40) and more
popular robotic arm named OWI-535, which merely relies
on vision inputs from an external camera. To the best of our
knowledge, this arm has never been used for automatic task
accomplishment, because lacking of sensors.

• Computer Vision with Synthetic Data

Synthetic data have been widely applied to many com-
puter vision problems in which annotations are difﬁ-
track-
cult
ing [6][24][44], human parsing [41], VQA [14], 6-D pose
estimation [34][35], semantic segmentation [10], etc.

to obtain, such as optical ﬂow [3], object

Domain adaptation is an important stage to transfer
models trained on synthetic data to real scenarios. There
are three major approaches, namely, domain randomiza-
tion [36][38], adversarial training [9][25][33][40] and joint
supervision [20]. A more comprehensive survey on domain
adaptation is available in [4]. As an alternative solution,
researchers introduced intermediate representation (e.g., se-
mantic segmentation) to bridge the domain gap [11]. In this
work, we focus on semi-supervised learning with the as-
sistance of domain randomization. The former method is
mainly based on 3D priors obtained from modeling the ge-
ometry of the target object [7][28]. Previously, researchers

applied parameterized 3D models to reﬁne the parsing re-
sults of humans [1][27] or animals [47], or ﬁne-tune the
model itself [28]. The geometry of a robotic system often
has a lower degree of freedom, which enables strong shape
constraints to be used for both purposes, i.e., prediction re-
ﬁnement and model ﬁne-tuning.

3. Approach

3.1. System Overview

We aim at designing a vision-based system to control a
sensor-free robotic arm to accomplish real-world tasks. Our
system, as illustrated in Figure 1, consists of three major
components working in a loop. The ﬁrst component, data
acquisition, samples synthetic images x from a virtual envi-
ronment for training and real images from an external cam-
era for real-time control. The second component is pose
estimation, an algorithm p = h(x; θ) which produces the
3D pose (joint angles) of the robotic arm (see Figure 2 for
the deﬁnition of four joints). The third component is a con-
troller, which takes p as input, determines an action for the
robotic arm to take, and therefore triggers a new loop.

Note that data acquisition (Section 3.2) may happen in
both virtual and real environments – our idea is to collect
cheap training data in the virtual domain, train a vision
model and tune it into one that works well in real world. The
core of this paper is pose estimation (Section 3.3), which is
itself an important problem in computer vision, and we in-
vestigate it from the perspective of domain transfer. While
studying motion control (Section 3.4) is also interesting yet
challenging, it goes out of the scope of this paper, so we
setup a relatively simple environment and apply an rein-
forcement learning algorithm.

3.2. Data Acquisition

The OWI-535 robotic arm has 5 motors named rotation,
base, elbow, wrist and gripper. Among them, the status of
the gripper is not necessary for motion planing and thus it
is simply ignored in this paper. The range of motion for the
ﬁrst 4 motors are 270◦, 180◦, 300◦ and 120◦, respectively.
In order to collect training data with low costs, we turn
to the virtual world. We download a CAD model of the arm
with exactly the same appearance as the real one which was
constructed using Unreal Engine 4 (UE4)3. Using Maya,
we implement its motion system which was not equipped
in the original model. The angle limitation as well as the
collision boundary of each joint is also manually conﬁg-
ured. The CAD model of OWI-535 has 170,648 vertices in
total, among which, we manually annotate 17 visually dis-
tinguishable vertices as our keypoints, as shown in Figure 2.

3https://3dwarehouse.sketchup.com/model/u21290a3e-f8ef-

46da-985c-9aa56b0dee53/Maplin-OWI-ROBOTIC-ARM

34216

This number is larger than the degree-of-freedom of the sys-
tem (6 camera parameters and 4 joint angles), meanwhile
reasonably small so that all keypoints can be annotated on
real images for evaluation. The images and annotations are
collected from UE4 via UnrealCV [30].

We create real-world dataset from two sources for bench-
mark and replication purpose. The ﬁrst part of data are
collected from an arm in our own lab, and we maximally
guarantee the variability in its pose, viewpoint and back-
ground. The second part of data are crawled from YouTube,
on which many users uploaded videos of playing with this
arm. Both subsets raise new challenges which are not cov-
ered by virtual data, such as motion blur and occlusion, to
our vision algorithm, with the second subset being more
challenging as the camera intrinsic parameters are unknown
and the arm may be modded for various purposes. We man-
ually annotate the 17 keypoints on these images, which typ-
ically takes one minute for a single frame.

More details of these datasets are covered in Section 4.

3.3. Transferable 3D Pose Estimation

We ﬁrst deﬁne some terminologies used in this paper.
Let p deﬁne all parameters that determine the arm’s position
in the image. In our implementation, p has 10 dimensions:
6 camera extrinsic parameters (location, rotation) and 4 an-
gles of motors. y ∈ R17×2 and z ∈ R17×3 are the locations
of keypoints in 2D and 3D, respectively. Both y and z are
deterministic functions of p.

The goal is to design a function p = h(x; θ) which re-
ceives an image x and outputs the pose vector p which de-
ﬁnes the pose of the object. θ denotes the learnable param-
eters, e.g., network weights in the context of deep learning.
The keypoints follow the same geometry constraints in both
virtual and real domains. In order to fully utilize these con-
straints, we decompose h(·) into two components, namely,
2D keypoint detection M1 : y = f (x; θ) and 3D pose es-
timation M2 : (p, z) = g(y; τ ). Here, τ is a ﬁxed set of
equations corresponding to the geometric constraints, e.g.,
the length between two joints. This is to say, M1 is trained
to optimize θ while M2 is a parameter-free algorithm which
involves ﬁtting a few ﬁxed arithmetic equations.

To alleviate the expense of data annotation, we apply a
setting known as semi-supervised learning [46] which con-
tains two parts of training data. First, a labeled set of train-
ing data D1 = {(xn, yn)}N
n=1 is collected from the virtual
environment. This process is performed automatically with
little cost, and also easily transplanted to other robotic sys-
tems with a 3D model available. Second, an unlabeled set
of image data D2 = {˜xm}M
m=1 is provided, while the corre-
sponding label ˜ym for each ˜xm remains unknown. We use
P V and P R to denote the virtual and real image distribu-
tions, i.e., xn ∼ P V and ˜xm ∼ P R, respectively. Since P V
and P R can be different in many aspects, we cannot expect

a model trained on D1 to generalize sufﬁciently well on D2.
The key is to bridge the gap between P V and P R. One
existing solution works in an explicit manner, which trains a
mapping r(·), so that when we sample ˜xm from P R, r(˜xm)
maximally mimics the distribution of P V. This is achieved
by unpaired image-to-image translation [45], which was
veriﬁed effective in some vision tasks [9]. However, in
our problem, an additional cue emerges, claiming that the
source and target images have the same label distribution,
i.e., both scenarios aim at estimating the pose of exactly the
same object, so we can make use of this cue to achieve do-
main adaptation in an implicit manner. In practice, we do
semi-supervised training by providing the system with un-
labeled data. Our approach exhibits superior transfer ability
in this speciﬁc task, while we preserve the possibility of
combining both manners towards higher accuracy.

To this end, we reformulate M1 and M2 in a probabilistic
style. M1 produces a distribution F(x; θ) ∋ y, and simi-
larly, M2 outputs G(y; τ ) ∋ (p, z). Here, the goal is to
maximize the marginal likelihood of (p, z) while y remains
a latent variable:

(p⋆, z⋆) = arg max

(p,z)Z Pr(y; θ | x) · Pr(p, z; τ | y) dy.

(1)
There is another option, which directly computes y⋆ =
arg maxy Pr(y; θ | x) and then infers p⋆ and z⋆ from y⋆.
We do not take it because we trust Pr(p, z; τ | y) more than
Pr(y; θ | x), since the former is formulated by strict geo-
metric constraints. Eqn (1) can be solved using an iterative
algorithm, starting with a model F(x; θ) pre-trained in the
virtual dataset.

In the ﬁrst step, we ﬁx θ and infer F(x; θ). This is done
by cropping the input image to 256 × 256 and feeding it to a
stacked hourglass network [26] with 2 stacks. The network
produces K = 17 heatmaps, each of which, sized 64 × 64,
corresponds to a keypoint. These heatmaps are taken as
input data of G(y; τ ) which estimates p and z as well as
y. This is done by making use of geometric constraints
τ , which appears as a few linear equations with ﬁxed pa-
rameters, e.g., the length of each bone of the arm. This is a
probabilistic model and we apply an iterative algorithm (see
Section 3.5) to ﬁnd an approximate solution y′, p′ and z′.
Note that y′ is not necessarily the maximum in F(x; θ).

In the second step, we take the optimal y′ to update θ.
As F(x; θ) is a deep network, this is often achieved by gra-
dient back-propagation. We incorporate this iterative algo-
rithm with stochastic gradient descent. In each basic unit
known as an epoch, each step is executed only once. Al-
though convergence is most often not achieved, we con-
tinue with the next epoch, which brings more informative
supervision. Compared with solving Eqn (1) directly, this
strategy improves the efﬁciency in the training stage, i.e., a
smaller number of iterations is required. Figure 3 shows an

44217

Pre-train

2D 

Prediction

3D 

Reconstruction

Unlabeled Real Images

CNN

Initial Keypoint 

Prediction

Fine-tune

Synthetic Images

Refined Keypoint 

Prediction

Figure 3. The pipeline of transferable 3D pose estimation (best
viewed in color). The initial prediction may contain both accurate
(green) and inaccurate (cyan) keypoints, and even outliers (red).
By introducing a 3D-prior constraint, we obtain a reﬁned keypoint
prediction, which is used to ﬁne-tune the neural network.

illustration of our transferable pose estimation pipeline.

3.4. Motion Control

In order to control the arm to complete tasks, we need a
motion control module which takes the estimated 3D pose
as input and outputs an action to achieve the goal. The mo-
tion control policy at = π(st, gt) is learned via a deep re-
inforcement learning algorithm. st is the state about the
environment at time t, e.g. the arm pose p. gt represents
the goal, e.g. target location. at is the control signal for
each joint in our system. The policy is learned in our virtual
environment, and optimized by Deep Deterministic Policy
Gradient (DDPG) [21]. Our experiment shows that using
arm pose as input, the policy learned in the virtual environ-
ment can be directly applied to the real world.

3.5. Implementation Details

• Training Data Variability

Our approach involves two parts of training data, namely,
a virtual subset to pre-train 2D keypoint detection, and an
unlabeled real subset for ﬁne-tuning.
In both parts, we
change the background contents of each training image so
as to facilitate data variability and thus alleviate over-ﬁtting.
In the virtual domain, background can be freely con-
trolled by the graphical renderer. In this scenario, we place
the arm on a board, under a sky sphere, and the background
of the board and the sphere are both randomly sampled from
the MS-COCO dataset [22]. In the real domain, however,
background parsing is non-trivial yet can be inaccurate. To
prevent this difﬁculty, we create a special subset for ﬁne-
tuning, in which all images are captured in a clean envi-
ronment, e.g., in front of a white board, which makes it
easy to segment the arm with a pixel-wise color-based ﬁlter,
and then place it onto a random image from the MS-COCO
dataset. We observe consistent accuracy gain brought by
these simple techniques.

• Joint Keypoint Detection and Pose Estimation

We use an approximate algorithm to ﬁnd the y′ (as well
as p′ and z′) that maximizes Pr(y; θ | x) · Pr(p, z; τ | y)
in Eqn (1), because an accurate optimization is mathemati-
cally intractable. We ﬁrst compute y′ = arg maxy F(x; θ)
which maximizes Pr(y; θ | x). This is performed on the
heatmap of each 2D keypoint individually, which produces
not only the most probable y′
k but also a score ck indicating
its conﬁdence. We ﬁrst ﬁlter out all keypoints with a thresh-
old ξ, i.e., all keypoints with ck < ξ are considered un-
known (and thus completely determined by geometric prior)
in the following 3D reconstruction module. This is to max-
imally prevent the impact of outliers. In practice, we use
ξ = 0.3 and our algorithm is not sensitive to this parameter.
Next, we recover the 3D pose using these 2D keypoints,
i.e., maximizing Pr(p, z; τ | y = y′). Under the assump-
tion of perspective projection, that each keypoint yk ∈ R2
is the 2D projection of a 3D coordinate zk ∈ R3, which can
be written in a linear equation:

[y|1]⊤ · ˆS = K · [R|T] · [z|1]⊤

.

(2)

Here, y ∈ RK×2 and z ∈ RK×3 are 2D and 3D coordi-
nate matrices, respectively, and 1 ∈ RK×1 is an all-one
vector. K ∈ R3×3 is the camera intrinsic matrix, which is
constant for a speciﬁc camera. S ∈ RK , R ∈ R3×3 and
T ∈ R3×1 denote the scaling vector, rotation matrix and
translation vector, respectively, all of which are determined
by p. For each keypoint k, zk is determined by the mo-
0 · Wk, where z0 ∈ RK×3 is a
tor transformation zk = zk
constant matrix indicating the coordinates of all keypoints
when motor angles are 0, and Wk ∈ R3×3 is the motor
transformation matrix for the kth keypoint, which is also
determined by p. ˆS = diag(S) is the scaling matrix. Due
to the inaccuracy in prediction (y can be inaccurate in ei-
ther prediction or manual annotation) and formulation (e.g.,
perspective projection does not model camera distortion),
Eqn (2) may not hold perfectly. In practice, we assume the
recovered 3D coordinates to follow an isotropic Gaussian
distribution, and so maximizing its likelihood gives the fol-
lowing log-likelihood loss:

L(p, z | y) = (cid:13)(cid:13)(cid:13)

[y|1]⊤ · ˆS − K · [R|T] · [z|1]⊤(cid:13)(cid:13)(cid:13)

4. Experiments

2

2

.

(3)

4.1. Dataset and Settings

We generated 5,000 synthetic images with randomized
camera parameters,
lighting conditions, arm poses and
background augmentation (see Section 3.5). Among them,
4,500 are used for training and the remaining 500 for vali-
dation. This dataset, later referred to as the virtual dataset,
is used to verify the 2D keypoint detection model, e.g., a
stacked hourglass network, works well.

54218

Model

Virtual Lab YouTube YouTube-vis

99.95 95.66
Synthetic
CycleGAN 99.86 97.84
99.89 96.14
CyCADA 99.84 98.09
Our Method 99.63 99.55

ADDA

80.05
75.26
79.19
73.47
87.01

81.61
76.98
80.04
74.37
88.89

Table 1. 2D keypoint detection accuracy (PCK@0.2, %) on three
datasets. Models are tested on YouTube dataset when considering
all keypoints and considering only the visible ones.

ping. On top of this model, we consider several approaches
to achieve domain transfer. One is to train an explicit model
which transfers virtual data to fake real data on which we
train a new model. In practice, we apply a popular genera-
tive model named CycleGAN [45]. We trained the Cycle-
GAN network with synthetic image as the source domain
and lab image as the target domain for 100 epochs. Other
domain adaptation methods, i.e., ADDA [40] and its follow-
up work CyCADA [9] are also applied and compared with
our approach described in Section 3.3. We mix the syn-
thetic and real images with a ratio of 6 : 4 and use the same
hyper-parameters as for the baseline. Background clutters
are added to the lab images in an online manner to facilitate
variability (see Section 3.5).

Results are summarized in Table 1. The baseline model
works almost perfectly on virtual data, which reports a
PCK@0.2 accuracy of 99.95%. However, this number
drops signiﬁcantly to 95.66% in lab data, and even dramat-
ically to 80.05% in YouTube data, demonstrating the exis-
tence of domain gaps. These gaps are largely shrunk after
domain adaptation algorithms are applied. Training with
images generated by CycleGAN, we found that the model
the lab dataset by
works better in its target domain, i.e.
a margin of 2.18%. However, this model failed to gen-
eralize to YouTube dataset, as the accuracy is even lower
than the baseline model. The cases are similar for ADDA
and CyCADA, which gains 0.48% and 2.43% improvement
on the lab dataset respectively, but both of the approaches
do not generalize well to the YouTube dataset. Our ap-
proach, on the other hand, achieves much higher accuracy,
with a PCK@0.2 score of 99.55% in the lab data, and
87.01% in the YouTube, boosting the baseline performance
by 6.96%. In the subset of visible YouTube keypoints, the
improvement is even higher (7.28%). In addition, the re-
ﬁned model only produces a slightly worse PCK@0.2 ac-
curacy (99.63% vs. 99.95%) on virtual data, implying that
balance is achieved between “ﬁtting on virtual data” and
“transferring to real data”.

The results reveal that performance of explicit domain
adaptation manners, i.e. CycleGAN, ADDA and CyCADA
can be limited in several aspect. For instance, compared
with our 3D geometric based domain adaptation method, al-
though the models trained with these explicit domain adap-

64219

Figure 4. Example images of three datasets used in this paper.
From top to bottom: synthetic images (top two rows), lab images
and YouTube images. Please zoom in to see details.

In the real environments, we collected and manually an-
notated two sets of data. The ﬁrst one is named the lab
dataset, which contains more than 20,000 frames captured
by a 720P Webcam. We manually chose 428 key frames
and annotated them. For this purpose, we rendered the 3D
model of the arm in the virtual environment and adjusted it
to the same pose of the real arm so that arms in these two
images exactly overlap with each other – in this way we
obtained the ground-truth arm pose, as well as the camera
intrinsic and extrinsic (obtained by a checkerboard placed
alongside the robotic arm at the beginning of each video)
parameters. We deliberately put distractors, e.g. colorful
boxes, dices and balls, to make the dataset more difﬁcult
and thus evaluate the generalization ability of our models.
The frames used for ﬁne-tuning and for testing come from
different videos.

The second part of real-world image data, the YouTube
dataset, is crawled from YouTube, which contains 109
videos with the OWI-535 arm. This is a largely diversiﬁed
collection, in which the arm may even be modded, i.e., the
geometric constraints may not hold perfectly. We sampled
275 frames and manually annotated the visibility as well
as position for each 2D keypoint. Note that, without cam-
era parameters, we cannot annotate the accurate pose of the
arm. This dataset is never included in training, but used to
observe the behavior of domain adaptation.

Sample images of three datasets are shown in Figure 4.

4.2. Pose Estimation

4.2.1 Detecting 2D Keypoints

We ﬁrst evaluate 2D keypoint detection, and make use of a
popular metric named PCK@0.2 [42] to evaluate the accu-
racy. For this purpose, we train a 2-stack hourglass network
from scratch for 30 epochs in the virtual dataset. Stan-
dard data augmentation techniques are applied, including
random translation, rotation, scaling, color shifting and ﬂip-

Motor

Camera

Model

Lab

YouTube YouTube-vis

Model

Rotation Base Elbow Wrist Average Rotation Location

Synthetic
Reﬁned

7.41
4.40

6.20 7.15
3.29 5.35

7.74
6.03

7.13
4.81

6.29
5.29

7.58
6.73

Table 2. 3D pose estimation errors (degrees) and camera parameter
prediction errors (degrees and centimeters) in the lab dataset.

Figure 5. Qualitative results from our YouTube dataset. The chal-
lenges include occlusion, user modiﬁcation, lighting, etc. We
show synthetic images generated using the camera parameters and
pose estimated from the single input image. Both success cases
(left ﬁve columns) and failure cases (rightmost column) are shown.

tation methods ﬁt to the target domain, it has a poor per-
formance on unseen data. Moreover, we fail to train a Cy-
cleGAN model with YouTube dataset as the target domain,
because the distribution of data in YouTube is too diverse
and such transformation is hard to learn.

4.2.2 Estimating the 3D Pose

We ﬁrst test the performance of 3D pose estimation on the
virtual dataset. We use the model trained only on synthetic
images since it has the best 2D prediction accuracy on syn-
thetic data. The experiment was conducted on 500 synthetic
images. The angular error for four joints are 2.67◦, 2.80◦,
2.76◦, 3.31◦, with an average of 2.89◦. The error of camera
parameters is 2.25◦ for rotation and 2.59cm for location.

We also test the 3D pose estimation performance of our
model on real images, which is the basis for completing
tasks. The quantitative 3D pose estimation result is only re-
ported for the lab dataset, since getting 3D annotation for
YouTube data is difﬁcult. We estimate the camera intrinsic
parameters by using camera calibration with checkerboard.
Results are shown in Table 2. The results reveal that our re-
ﬁned model outperforms the synthetic only model by 2.32◦
on average angular error. The qualitative result on YouTube
dataset are shown in Figure 5. Since the camera intrinsic pa-
rameters are unknown for YouTube videos, we use the weak
perspective model during reconstruction. Heavy occlusion,
user modiﬁcation and extreme lighting make the 3D pose
estimation hard in some cases. We select typical samples
for success and failure cases.

95.66
Synthetic Only
BG ✗
3D ✗
94.52
BG ✓ 3D ✗
98.72
3D ✓ 97.31
BG ✗
BG ✓ 3D ✓ 99.55

80.05
80.24
84.04
86.52
87.01

81.61
82.22
87.11
88.27
88.89

Table 3. 2D keypoint detection accuracy (PCK@0.2, %) under ab-
lation study. ‘3D’ stands for joint keypoint detection and 3D pose
estimation, and ‘BG’ for random background augmentation.

# Images

Lab

YouTube YouTube-vis

2,500
5,000
10,000
20,000

92.38
95.66
96.13
97.25

75.24
80.05
80.71
81.11

77.33
81.61
81.71
81.83

Table 4. 2D keypoint detection accuracy (PCK@0.2, %) with re-
spect to the number of training images.

dom background augmentation and joint keypoint detection
and pose estimation. To evaluate the contribution of these
two strategies to the improvement on accuracy, we did an
ablation study. Results are shown in Table 3.

We compare the performance of 5 models: 1) baseline
model, trained on 4,500 synthetic images; 2) - 5) mod-
els trained with/without joint estimation and with/without
background augmentation. Note that if a model is trained
without joint estimation, we directly take the argmax on
the predicted heatmap as annotations for training. We see
that both strategies contribute to the overall performance
improvement. Our model performs best when combining
both strategies.

4.2.4 Ablation Study: Number of Training Images

With the help of domain randomization, we can generate an
unlimited number of synthetic images with high abundance
in their appearance. On the other hand, the performance
of deep models tends to saturate as the number of training
data increases. Therefore, it is necessary to balance between
performance and data-efﬁciency.

Results are shown in Table 4. As the number of training
images increases from 2,500 to 5,000, the accuracy signiﬁ-
cantly increases (by 3.28% and 4.81% on lab and YouTube
dataset, respectively). When the number of training images
continue to increase to 20,000, the accuracy only increases
by a small margin (1.49% and 0.22% on lab and YouTube
dataset, respectively). Therefore, 5,000 synthetic images is
a nice balance between accuracy and efﬁciency.

4.3. Controlling the Arm with Vision

4.2.3 Ablation Study: Domain Adaptation Options

As described in Section 3.5, when training the reﬁned
model, two strategies are applied on the real images: ran-

We implement a complete control system purely based
on vision, as described in Section 3.4.
It takes a video
stream as input, estimates the arm pose, then plans the mo-
tion and sends control signal to the motors.

74220

Agent

Input
Type

Distance
Error (cm)

Success Average
Time (s)

Rate

Human

Direct

Human Camera
Camera

Ours

0.65

2.67
2.66

100.0%

66.7%
59.3%

29.8

38.8
21.2

Table 5. Quantitative result for completing the reaching task. Our
system achieves comparable performance with human when the
same input signal is given.

This system is veriﬁed with a task, reaching a target
point. The goal is controlling the arm to make the arm tip
reach right above a speciﬁed point on the table without colli-
sion. Each attempt is considered successful if the horizontal
distance between arm tip and the target is within 3cm. The
system is tested at 6 different camera views. For each view,
the arm needs to reach 9 target points. The target points
and camera views are selected to cover a variety of cases.
We also place distractors to challenge our vision module. A
snapshot of our experiment setup can be seen in Figure 6.

We report human performance on the same task. Human
is asked to watch the video stream from a screen and control
the arm with a game pad. This setup ensures human and
our system accepts the same vision input in comparison. In
addition, we allow human to directly look at the arm and
move freely to observe the arm when doing the task. The
performance for both setups are reported.

Our control system can achieve comparable performance
with human in this task. The result is reported in Table 5.
Human can perform much better if directly looking at the
arm. This is because human can constantly move his head
to pick the best view for current state. Potentially, we could
use action vision, or multi-camera system, to mimic this
ability and further improve the system, which are interesting
future work but beyond the scope of this work. It is worth
noticing our system can run real-time and ﬁnish the task
faster than human.

Built on this control module, we show that our system
can move a stack of dices into a box separately. Please
see https://youtu.be/8hZjdqDrYas for video demonstra-
tion. To simplify deployment, we directly feed the ground-
truth locations of the dices and the box into the system and
apply the same controller as the reaching task. The success-
ful rate of this task is not high, because it requires highly
accurate pose control in both horizontal and vertical direc-
tions. We also provide some interesting failure cases in the
video. Failure cases are caused by several reasons, e.g., self
occlusion or rarely seen conﬁgurations. Also, the controller
sometimes fails, e.g., if the arm is too far from the camera,
a long-distance movement only causes minor visual differ-
ence. At the current point, this demo reveals the potential of
our vision-based control system, as well as advocates more
advanced vision algorithms to be designed to improve its
performance.

Figure 6. A snapshot of the real-world experiment setup. Loca-
tions of the goals are printed on the reference board and are used
as reference when measuring the error. We scatter background ob-
jects randomly during testing.

5. Conclusions

In this paper, we built a system, which is purely based on
vision inputs, to control a low-cost, sensor-free robotic arm
to accomplish tasks. We used a semi-supervised algorithm
which integrates labeled synthetic as well as unlabeled real
data to train the pose estimation module. Geometric con-
straints of multi-rigid-body system (the robotic arm in this
case) was utilized for domain adaptation. Our approach,
with merely a 3D model being required, has the potential to
be applied to other multi-rigid-body systems.

To facilitate reproducible research, we created a virtual
environment to generate synthetic data, and also collected
two real-world datasets from our lab and YouTube videos,
respectively, all of which can be used as benchmarks to
evaluate 2D keypoint detection and/or 3D pose estimation
algorithms.
In addition, the low cost of our system en-
ables vision researchers to study robotic tasks, e.g., rein-
forcement learning, imitation learning, active vision, etc.,
without large economic expenses. This system also has the
potential to be used for high-school and college educational
purposes.

Beyond our work, many interesting future directions can
be explored. For example, we can train perception and con-
troller modules in a joint, end-to-end manner [18][13], or
incorporate other vision components, such as object detec-
tion and 6D pose estimation, to enhance the ability of the
arm so that more complex tasks can be accomplished.

Acknowledgements

This work is supported by IARPA via DOI/IBC con-
tract No. D17PC00342, NSFC-61625201, 61527804, Qual-
comm University Research Grant and CCF-Tencent Open
Fund. The authors would like to thank Vincent Yan, Kaiyue
Wu, Andrew Hundt, Prof. Yair Amir and Prof. Gregory
Hager for helpful discussions.

84221

USB CameraReference BoardOWI-ArmBackground ObjectsReferences

[1] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
Gehler, Javier Romero, and Michael J Black. Keep it smpl:
Automatic estimation of 3d human pose and shape from a
single image. In European Conference on Computer Vision,
2016. 3

[2] Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei
Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs,
Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using sim-
ulation and domain adaptation to improve efﬁciency of deep
robotic grasping.
In International Conference on Robotics
and Automation, 2018. 3

[3] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and
Michael J Black. A naturalistic open source movie for op-
tical ﬂow evaluation. In European Conference on Computer
Vision, 2012. 3

[4] Gabriela Csurka. Domain adaptation for visual applications:
A comprehensive survey. arXiv preprint arXiv:1702.05374,
2017. 2, 3

[5] Marc Peter Deisenroth.

Learning to control a low-
cost manipulator using data-efﬁcient reinforcement learning.
Robotics: Science and Systems, 2012. 3

[6] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora
Vig. Virtual worlds as proxy for multi-object tracking anal-
ysis. In Conference on Computer Vision and Pattern Recog-
nition, 2016. 3

[7] Yuan Gao and Alan L Yuille. Exploiting symmetry and/or
manhattan properties for 3d object structure estimation from
single and multiple images. In Conference on Computer Vi-
sion and Pattern Recognition, 2017. 3

[8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. The Inter-
national Journal of Robotics Research, 32(11):1231–1237,
2013. 1, 2

[9] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei A. Efros, and Trevor Dar-
rell. Cycada: Cycle-consistent adversarial domain adap-
tation.
In International Conference on Machine Learning,
2018. 3, 4, 6

[10] Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell.
Fcns in the wild: Pixel-level adversarial and constraint-based
adaptation. arXiv preprint arXiv:1612.02649, 2016. 3

[11] Zhang-Wei Hong, Yu-Ming Chen, Hsuan-Kung Yang, Shih-
Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, Brian Hsi-Lin
Ho, Chih-Chieh Tu, Tsu-Ching Hsiao, Hsin-Wei Hsiao, Sih-
Pin Lai, Yueh-Chuan Chang, and Chun-Yi Lee. Virtual-to-
real: Learning to control in visual semantic segmentation.
In International Joint Conference on Artiﬁcial Intelligence,
2018. 3

[12] Andrew Hundt, Varun Jain, Chris Paxton, and Gregory D
Hager. Training frankenstein’s creature to stack: Hypertree
architecture search. arXiv preprint arXiv:1810.11714, 2018.
1, 2

[14] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr:
A diagnostic dataset for compositional language and elemen-
tary visual reasoning. In Conference on Computer Vision and
Pattern Recognition, 2017. 3

[15] Gregory Kahn, Adam Villaﬂor, Vitchyr Pong, Pieter
Abbeel, and Sergey Levine. Uncertainty-aware reinforce-
ment
arXiv preprint
arXiv:1702.01182, 2017. 3

learning for collision avoidance.

[16] Nathan P Koenig and Andrew Howard. Design and use
paradigms for gazebo, an open-source multi-robot simula-
tor.
In International Conference on Intelligent Robots and
Systems, 2004. 3

[17] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep

learning. Nature, 521(7553):436, 2015. 1

[18] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter
Abbeel. End-to-end training of deep visuomotor policies.
The Journal of Machine Learning Research, 17(1):1334–
1373, 2016. 2, 8

[19] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz,
and Deirdre Quillen. Learning hand-eye coordination for
robotic grasping with deep learning and large-scale data col-
lection. The International Journal of Robotics Research,
37(4-5):421–436, 2018. 1, 2

[20] Chi Li, M. Zeeshan Zia, Quoc-Huy Tran, Xiang Yu, Gre-
gory D. Hager, and Manmohan Chandraker. Deep supervi-
sion with shape concepts for occlusion-aware 3d object pars-
ing. In Conference on Computer Vision and Pattern Recog-
nition, 2017. 3

[21] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
Daan Wierstra. Continuous control with deep reinforcement
learning.
In International Conference for Learning Repre-
sentations, 2016. 5

[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, 2014. 5

[23] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong
Zhang, and Yizhou Wang. End-to-end active object track-
ing via reinforcement learning. In International Conference
on Machine Learning, pages 3286–3295, 2018. 2

[24] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong
Zhang, and Yizhou Wang. End-to-end active object track-
ing and its real-world deployment via reinforcement learn-
ing.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2019. 3

[25] Faisal Mahmood, Richard Chen, and Nicholas J Durr. Un-
supervised reverse domain adaptation for synthetic medical
images via adversarial training. IEEE Transactions on Med-
ical Imaging, 2018. 3

[26] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In European Con-
ference on Computer Vision, 2016. 4

[13] Eric Jang, Sudheendra Vijaynarasimhan, Peter Pastor, Julian
Ibarz, and Sergey Levine. End-to-end learning of semantic
grasping. arXiv preprint arXiv:1707.01932, 2017. 2, 8

[27] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-
ter Gehler, and Bernt Schiele. Neural body ﬁtting: Unifying
deep learning and model based human pose and shape es-

94222

[40] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Dar-
rell. Adversarial discriminative domain adaptation. In Con-
ference on Computer Vision and Pattern Recognition, 2017.
3, 6

[41] G¨ul Varol, Javier Romero, Xavier Martin, Naureen Mah-
mood, Michael J Black, Ivan Laptev, and Cordelia Schmid.
Learning from synthetic humans.
In Conference on Com-
puter Vision and Pattern Recognition, 2017. 3

[42] Yi Yang and Deva Ramanan. Articulated human detection
with ﬂexible mixtures of parts. IEEE transactions on pattern
analysis and machine intelligence, 35(12):2878–2890, 2013.
6

[43] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike
Liao, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A
diverse driving video database with scalable annotation tool-
ing. arXiv preprint arXiv:1805.04687, 2018. 2

[44] Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and
Yizhou Wang. AD-VAT: An asymmetric dueling mechanism
for learning visual active tracking. In International Confer-
ence on Learning Representations, 2019. 3

[45] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In International Conference
on Computer Vision, 2017. 4, 6

[46] Xiaojin Zhu. Semi-supervised learning literature survey.
Computer Science, University of Wisconsin-Madison, 2006.
4

[47] Silvia Zufﬁ, Angjoo Kanazawa, and Michael J Black. Li-
ons and tigers and bears: Capturing non-rigid, 3d, articulated
shape from images. In Conference on Computer Vision and
Pattern Recognition, 2018. 3

timation. In International Conference on 3D Vision, 2018.
3

[28] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas
Daniilidis. Learning to estimate 3d human pose and shape
from a single color image. In Conference on Computer Vi-
sion and Pattern Recognition, 2018. 3

[29] Lerrel Pinto and Abhinav Gupta.

Supersizing self-
supervision: Learning to grasp from 50k tries and 700 robot
hours. In International Conference on Robotics and Automa-
tion, 2016. 1, 2

[30] Weichao Qiu, Fangwei Zhong, Yi Zhang, Siyuan Qiao, Zi-
hao Xiao, Tae Soo Kim, Yizhou Wang, and Alan Yuille. Un-
realcv: Virtual worlds for computer vision. In Proceedings
of the 25th ACM International Conference on Multimedia,
2017. 4

[31] Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau
B¨ol¨oni, and Sergey Levine. Vision-based multi-task manipu-
lation for inexpensive robots using end-to-end learning from
demonstration. In International Conference on Robotics and
Automation, 2018. 3

[32] Andrei A Rusu, Matej Vecerik, Thomas Roth¨orl, Nicolas
Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot
learning from pixels with progressive nets. arXiv preprint
arXiv:1610.04286, 2016. 3

[33] Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Joshua
Susskind, Wenda Wang, and Russell Webb.
Learning
from simulated and unsupervised images through adversar-
ial training. In Conference on Computer Vision and Pattern
Recognition, 2017. 3

[34] Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas.
Render for cnn: Viewpoint estimation in images using cnns
trained with rendered 3d model views. In International Con-
ference on Computer Vision, 2015. 3

[35] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian
Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d
orientation learning for 6d object detection from rgb images.
In European Conference on Computer Vision, 2018. 3

[36] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Woj-
ciech Zaremba, and Pieter Abbeel. Domain randomization
for transferring deep neural networks from simulation to the
real world. In International Conference on Intelligent Robots
and Systems, 2017. 3

[37] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A
In International

physics engine for model-based control.
Conference on Intelligent Robots and Systems, 2012. 3

[38] Jonathan Tremblay, Aayush Prakash, David Acuna, Mark
Brophy, Varun Jampani, Cem Anil, Thang To, Eric Camer-
acci, Shaad Boochoon, and Stan Birchﬁeld. Training deep
networks with synthetic data: Bridging the reality gap by
domain randomization. arXiv preprint arXiv:1804.06516,
2018. 3

[39] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn,
Pieter Abbeel, Sergey Levine, Kate Saenko, and Trevor
Darrell. Adapting deep visuomotor representations with
weak pairwise constraints. arXiv preprint arXiv:1511.07111,
2015. 3

104223

