Shifting More Attention to Video Salient Object Detection

Deng-Ping Fan1

Wenguan Wang2

Ming-Ming Cheng1 ∗

Jianbing Shen2

3

,

1 TKLNDST, CS, Nankai University

2 Inception Institute of Artiﬁcial Intelligence

3 Beijing Institute of Technology

http://mmcheng.net/DAVSOD/

Abstract

The last decade has witnessed a growing interest in
video salient object detection (VSOD). However, the re-
search community long-term lacked a well-established
VSOD dataset representative of real dynamic scenes with
high-quality annotations. To address this issue, we elabo-
rately collected a visual-attention-consistent Densely Anno-
tated VSOD (DAVSOD) dataset, which contains 226 videos
with 23,938 frames that cover diverse realistic-scenes, ob-
jects, instances and motions. With corresponding real hu-
man eye-ﬁxation data, we obtain precise ground-truths.
This is the ﬁrst work that explicitly emphasizes the chal-
lenge of saliency shift, i.e., the video salient object(s) may
dynamically change. To further contribute the community a
complete benchmark, we systematically assess 17 represen-
tative VSOD algorithms over seven existing VSOD datasets
and our DAVSOD with totally ∼84K frames (largest-scale).
Utilizing three famous metrics, we then present a compre-
hensive and insightful performance analysis. Furthermore,
we propose a baseline model. It is equipped with a saliency-
shift-aware convLSTM, which can efﬁciently capture video
saliency dynamics through learning human attention-shift
behavior. Extensive experiments1 open up promising future
directions for model development and comparison.

1. Introduction

Salient object detection (SOD) targets at extracting the
most attention-grabbing objects from still images [17] or
dynamic videos. This task originates from the cognitive
studies of human visual attention behavior, i.e., the aston-
ishing ability of the human visual system (HVS) to quick-
ly orient attention to the most informative parts of visual
scenes. Previous studies [6, 45] quantitatively conﬁrmed
that there exists a strong correlation between such explic-
it, object-level saliency judgment (object-saliency) and the
implicit visual attention allocation behavior (visual atten-
tion mechanism).

∗M.M. Cheng (cmm@nankai.edu.cn) is the corresponding author.
1Dataset and code are available at: http://dpfan.net/DAVSOD/

Figure 1: Annotation examples of our DAVSOD dataset. The
rich annotations, including saliency shift, object-/instance-level
ground-truths (GT), salient object numbers, scene/object cate-
gories, and camera/object motions, provide a solid foundation for
VSOD task and beneﬁt a wide range of potential applications.

Video salient object detection (VSOD) is thus signiﬁ-
cantly essential for understanding the underlying mecha-
nism behind HVS during free-viewing in general and in-
strumental to a wide range of real-world applications, e.g.,
video segmentation [74, 83], video captioning [57], video
compression [27, 29], autonomous driving [91], robotic in-
teraction [82], weakly supervised attention [95]. Besides its
academic value and practical signiﬁcance, VSOD presents
great difﬁculties due to the challenges carried by video da-
ta (diverse motion patterns, occlusions, blur, large object-
deformations, etc.) and the inherent complexity of human
visual attention behavior (i.e., selective attention allocation,
attention shift [5, 37, 60]) during dynamic scenes. Thus it
invoked dramatically increasing research interest over the
past few years [7, 25, 31, 36, 38, 39, 61] (Table 2).

However, in striking contrast with the ﬂourishing de-
velopment of VSOD modeling, the effort on a standard
representative VSOD benchmark still lags behind serious-
ly. Although several datasets [35, 40, 43, 52, 56, 59, 75]
are proposed for VSOD, they suffered from the following
shortages. First, during dynamic-viewing, the allocation
of attentional resources is not only selective but also dy-
namically varied among different parts of inputs, with the
changing of video content. Nevertheless, previous datasets
are annotated via static frames, without a dynamic human
eye-ﬁxation guided annotation methodology, and thus do
not reveal real human attention behavior during dynamic-

8554

Attention ShiftSaliency Shift3#Salient Obj.:42Video Cate.: AnimalObject Cate.: LionCamera Mo.: SlowObject Mo.: SlowFrameFixationInstance-levelVSOD GTObject-levelVSOD GTFigure 2: Sample video sequences from our DAVSOD dataset, with instance-level GT and ﬁxations overlaid.

viewing. Second, they are typically limited in their scala-
bility, coverage, diversity and difﬁculty. Thus, these limita-
tions of existing datasets inhibit the further development of
this branch.

This paper presents two contributions. First, we collect
a large-scale DAVSOD (Densely Annotated Video Salient
Object Detection) dataset speciﬁcally designed for VSOD.

Year
Dataset
2013
SegV2 [40]
2014
FBMS [56]
2015
MCL [35]
2015
ViSal [75]
2016
DAVIS [59]
2017
UVSD [52]
VOS [43]
2018
DAVSOD 2019

720
463
193

#AF. DL AS FP EF IL
1,065 X

#Vi.
14
59
9
17
50
18
200
226 23,938 X X X X X

3,455 X
3,262 X
7,467

X

• It contains 226 video sequences, which were strict-
ly annotated according to real human ﬁxation record-
s (Fig. 2). More importantly, two essential dynam-
ic human attention characteristics, i.e., selective at-
tention and attention shift are both considered.
In
DAVSOD, the salient object(s) may change at differ-
ent time (Fig. 1), which is more realistic and requires a
complete video content understanding. Above efforts
result in a visual-attention-consistent VSOD dataset.

• Besides, the videos were carefully selected to cov-
er diverse scene/object categories, motion patterns,
and densely annotated with per-frame pixel-accurate
ground-truths (GT).

• Another discriminative feature of DAVSOD is the
availability of both object- and instance-level annota-
tions, beneﬁting broader potential research direction-
s, such as instance-level VSOD, video salient object
subitizing, saliency-aware video captioning, etc.

Second, with the established DAVSOD dataset and previ-
ous 7 VSOD datasets [35,40,43,52,56,59,75], we present a
comprehensive evaluation of 17 state-of-the-art models [8,
11,35,41,44,52,53,62,67,68,70,74–76,81,87,92], making
it the most complete VSOD benchmark. Additionally, we
also propose a baseline model, named SSAV (Saliency-Shift
Aware VSOD). It learns to predict video saliency by using
a saliency-shift-aware convLSTM module, which explicit-
ly models human visual attention-shift behavior in dynamic
scenes. The promising results on above benchmark clearly
demonstrate its effectiveness.

Our two contributions represent a complete benchmark
suite with the necessary tools for a complementary evalua-
tion, bring a more insightful glimpse into the task of VSOD
and boost more research efforts towards this direction.

Table 1: Statistics of previous VSOD datasets and the pro-
posed DAVSOD dataset, showing DAVSOD provides much richer
annotations. #Vi.: number of videos. #AF.: number of annotated
frames. DL: whether provide densely (per-frame) labeling. AS:
whether consider attention shift. FP: whether annotate salient ob-
jects according to eye ﬁxation records. EF: whether offer the eye
ﬁxation records for annotated salient object(s). IL: whether pro-
vide instance-level annotation.

2. Related Work

VSOD Datasets. Over the past few years, several datasets
(Table 1) have been created or introduced into VSOD.
Speciﬁcally, SegV2 [40] and FBMS [56] are two early
adopted datasets. Since they are designed for their spe-
ciﬁc purposes, they are not very suitable for VSOD task.
Another dataset MCL [35] only has 9 simple video ex-
amples. ViSal [75] is the ﬁrst specially designed VSOD
dataset, while only containing 17 video sequences with ob-
vious objects. More recently, Wang et al. [76] introduced
DAVIS [59], a famous video segmentation dataset with 50
challenging scenes, for VSOD. Although above datasets
advanced the ﬁeld of VSOD to various degrees, they are
severely limited to small scales (only dozens of videos). In
addition, those datasets do not consider real human atten-
tion during dynamic scenes instead arbitrarily and manual-
ly identify the salient objects by only a few annotators. The
annotation is performed over each frame individually, failed
in accounting temporal characteristics in complex dynam-
ic scenes. A recent larger scale VOS [43] dataset partially
remedied above limitations. But its diversity and generality
are quite limited as it contains many simple indoor, stable-
camera scenarios.

Overall, our DAVSOD signiﬁcantly discriminate from
above datasets: i) Through in-depth analyzing real human
dynamic attention behavior, we observe visual attention-
shift phenomenon, and thus, for the ﬁrst time, emphasize
the shift of salient objects in dynamic scenes and provide

8555

No.

Model

Year Pub.

#Training

Training Set

Basic

Type OF SP

S-measure

PCT

Code

SIVM [62]
1
DCSM [36]
2
RDCM [47]
3
SPVM [53]
4
CDVM [20]
5
TIMP [92]
6
STUW [21]
7
EBSG [55]
8
SAGM [74]
9
10
ETPM [64]
11 RWRV [35]
12 GFVM [75]
13 MB+M [87]
14 MSTM [70]
SGSP [52]
15
SFLR [8]
16
STBP [81]
17
VSOP [28]
18
DSR3 [38]
19
20
VQCU [3]
21 CSGM [77]
STUM [2]
22
SAVM [78]
23
bMRF [7]
24
LESR [93]
25
26
TVPI [61]
SDVM [4]
27
SCOM [11]
28
STCR [39]
29
DLVS [76]
30
SCNN [68]
31
32
FGRN [41]
33
SCOV [33]
34 MBNM [44]
PDBM [67]
35
36 UVOS [31]
37 SSAV (Ours)

2010 ECCV
2011 TCSVT
2013 TCSVT
2014 TCSVT
2014 TCSVT
2014 CVPR
2014 TIP
2015 CVPR
2015 CVPR
2015 CVPR
2015 TIP
2015 TIP
2015 ICCV
2016 CVPR
2017 TCSVT
2017 TIP
2017 TIP
2017 TYCB
2017 BMVC 44 (6+8+30) clips
2018 TMM
2018 TCSVT
2018 TIP
2018 TPAMI
2018 TMM
2018 TMM
2018 TIP
2018 TIP
∼10K frame pairs
2018 TIP
44 (6+8+30) clips
2018 TIP
∼18K frame pairs
2018 TIP
2018 TCSVT ∼11K frame pairs
2018 CVPR ∼10K frame pairs
2018 ECCV
2018 ECCV ∼13K frame pairs Voc12 + Coco [49] + DV
2018 ECCV ∼18K frame pairs
2018 ECCV
2019 CVPR ∼13K frame pairs DAVSOD val + DO +DV

MK+S2+FS
S2+FS+DV

MK+DO+S2+FS

MK+DO+DV

10C+S2+DV

10C+S2+DV

MK

CRF, statistic

SORM distance

gabor, region contrast

SP, histogram

compressed domain

time-mapping

uncertainty weighting

gestalt principle
geodesic distance
eye tracking prior

random walk
gradient ﬂow

minimum barrier distance
minimum spanning tree

histogram, graph

low-rank coherency
background priors
object proposals

RCL [48]

spectral, graph structure
joint video co-saliency

M

M
N/A

C++
N/A

0.481∼0.606

0.539∼0.667

45.4* M&C++

X 0.470∼0.724

72.4* M&C++
0.023*

9.8*
56.1* M&C++
1.73*
69.2* M&C++
50.7*

T
T
T X
T
T
T X
T X
T X
T X X 0.615∼0.749
T X
0.330∼0.595
18.3*
T
T X X 0.613∼0.757
53.7* M&C++
0.552∼0.726
0.02* M&C++
T
0.540∼ 0.657
0.02* M&C++
T
T X X 0.557∼0.706
51.7* M&C++
T X X 0.470∼0.724 119.4* M&C++
X 0.533∼0.752 49.49* M&C++
T
T X X
M&C++
D
Py&Ca
X
T
T X X

0.78*
3.86* M&C++

N/A
M

M

local spatiotemporal neighborhood cues T

geodesic distance

MRF

localized estimation, spatiotemporal

geodesic distance, CRF

spatiotemporal decomposition

DCL [42]

CRF

FCN [54]

VGGNet [66]

LSTM

BOW [22], proposal, FCIS [46]

motion based, DeepLab [9]

DC [85]

standard edge detector
SSLSTM, PDC [67]

X

T X X 0.615∼0.749
T X X
T X X
X
T
T
D X X 0.555∼0.832
D
D X X 0.682∼0.881
D X X 0.674∼0.794
D X
0.693∼0.861
T X X
D X
D
D X X
D

0.637∼0.898
0.698∼0.907

0.724∼0.941

N.A.

45.4* M&C++
N/A
2.63*
5.93*
N/A
2.78* M&C
N/A
N/A
N/A

38.8

0.47
38.5
0.09
3.44
2.63
0.05

Py&Ca

N/A

Py&Ca

N/A
N/A

Py&Ca

N/A

0.05

Py&Ca

Table 2: Summarizing of 36 previous representative VSOD methods and the proposed SSAV model. Training Set: 10C = 10-
Clips [24]. S2 = SegV2 [40]. DV = DAVIS [59]. DO = DUT-OMRON [84]. MK = MSRA10K [12]. MB = MSRA-B [51]. FS = FBMS [56].
Voc12= PASCAL VOC2012 [16]. Basic: CRF = Conditional Random Field. SP = superpixel. SORM = self-ordinal resemblance measure.
MRF = Markov Random Field. Type: T = Traditional. D = Deep learning. OF: Whether use optical ﬂow. SP: Whether use superpixel
over-segmentation. S-measure [18]: The range of scores over the 8 datasets in Table 4. PCT: Per-frames Computation Time (second).
Since [3, 7, 11, 33, 44, 47, 68, 93] did not release implementations, corresponding PCTs are borrowed from their papers or provided by
authors. Code: M = Matlab. Py = Python. Ca= Caffe. N/A = Not Available in the literature. “*” indicates CPU time.

the unique annotations of visual-attention-consistent prop-
erty. ii) Its diversity, large-scale dense annotation, as well
as comprehensive object-/instance-level salient object an-
notations, rich attribute annotations (e.g., object numbers,
motion patterns, scene/object categories), altogether make
a solid and unique foundation for VSOD.

VSOD Models. Early VSOD models [8, 26, 28, 35, 52, 53,
62, 63, 74, 75] are built upon hand-crafted features (color,
motion, etc.), and largely rely on classic heuristics in im-
age salient object detection area (e.g., center-surround con-
trast [12], background prior [79]) and cognitive theories of
visual attention (e.g., feature integration theory [69], guid-
ed search [80]). They also explored the way of integrat-
ing spatial and temporal saliency features through different
computational mechanisms, such as gradient ﬂow ﬁeld [75],
geodesic distance [74], restarted random walk [35], and
spectral graph structure [3]. Traditional VSOD models are
bound to signiﬁcant feature engineering and limited expres-
sion ability of hand-features. See Table 2 for more details.

More recently, deep learning based VSOD models [31,

38, 39, 41, 67, 68, 76] have gained more attention inspired
by the success of applying deep neural networks on im-
age saliency detection [13–15, 32, 50, 71, 72, 86, 88–90, 94].
More speciﬁcally, the work of Wang et al. [76] represents
an early attempt that trains a fully convolutional neural net-
work for VSOD. Another concurrent work [38] uses a 3D
ﬁlter to incorporate both spatial and temporal information
in a spatiotemporal CRF framework. Later, spatiotemporal
deep feature [39], RNN [41], pyramid dilated convLSTM
[67] are proposed for better capturing spatial and temporal
saliency characteristics. These deep VSOD models gener-
ally achieved better performance due to the strong learning
ability of neural network. However, these models ignored
the saliency shift phenomenon which is quite important for
understanding the human visual attention mechanism.
In
contrast, our SSAV model utilizes the saliency shift cue ex-
plicitly, yielding a competitive VSOD model.

In this work, we systematically benchmark 17 state-of-
the-art VSOD models on seven previous datasetsand the
proposed DAVSOD dataset, which represents the largest

8556

(a)

(b)

(c)

(d)

s
e
c
n
a
t
s
n
i
#

s
e
m
a
r
f
#

y
c
n
e
u
q
e
r
f

Annotated salient object instances in each video

video

video

Image frames in each video

Object/Instance size

(e)

ratio

Figure 3: Statistics of the proposed DAVSOD dataset. (a) Scene/object categories. (b, c) Distribution of annotated instances and image
frames, respectively. (d) Ratio distribution of the objects/instances. (e) Mutual dependencies among scene categories in (a).

performance evaluation in VSOD area so far. With our ex-
tensively quantitative results, we present deep insights into
VSOD and point out some promising research directions.

3. Proposed Dataset

Some example frames can be found in Fig. 1 and
Fig. 2. See our website for details. We will show details
of DAVSOD from the following 4 key aspects.

3.1. Stimuli Collection

The stimuli of DAVSOD come from DHF1K [73], which
is the current largest-scale dynamic eye-tracking dataset.
There are several advantages of using DHF1K create our
dataset. DHF1K2 is collected from Youtube and covers di-
verse realistic-scenes, different object appearances and mo-
tion patterns, various object categories, and large span of
major challenges in dynamic scenarios, providing us a solid
basis to build a large-scale and representative benchmark.
More essentially, the companied visual ﬁxation record al-
lows us to produce reasonable and biologically-inspired
object-level saliency annotations. We manually trim the
videos into shot clips (Fig. 3(c)) and remove dark-screen
transitions.
In this way, we ﬁnally reach a large-scale
dataset, containing 226 video sequences with totally 23, 938
frames and 798 seconds duration.

3.2. Data Annotation
Saliency Shift Annotation. Human attention behavior is
more complex during realistic, dynamic scenes [37,60], i.e.,
selective attention allocation and overt attention shift (due
to abrupt onsets, new dynamic events, etc.) may both hap-
pening. With the eye-tracking record of DHF1K, we also
observe stimulus-driven attention-shifts [23] are ubiquitous,
as shown in Fig. 1. However, none of the previous work in
the VSOD area explicitly emphasizes such essential visu-
al attention behavior. In DAVSOD, we annotate the salient

2Download: https://github.com/wenguanwang/DHF1K

objects according to real human ﬁxations, and the temporal
location at which attention shift occurs, for the ﬁrst time,
emphasizing the challenge of saliency shift3 in this ﬁeld.
Scene/Object Category Labeling. Consistent with [73],
each video is manually labeled with a category (i.e., Ani-
mal, Vehicle, Artifact, Human Activity). Human Activity has
four sub-classes: Sports, Daily-, Social-, and Art-Activity.
For object class, following MSCOCO [49], only “thing”
categories instead of “stuff” are included. Then we built
a list of about 70 most frequently present scenes/objects. In
Fig. 3(a)&(e), we show the scene/object categories and their
mutual dependencies, respectively. Five annotators were
asked to annotate the object labels.
Instance-/Object-Level Salient Object Annotation.
Twenty human annotators, who were pre-trained with ten
video examples, are instructed to select up to ﬁve objects
per-frame according to the corresponding ﬁxation records
and carefully annotate them (by tracing boundaries instead
of rough polygons). They are also asked to differentiate
instances and annotate them individually,
resulting in
totally 23,938 object-level ground-truth masks and 39,498
instance-level salient object annotations.

3.3. Dataset Features and Statistics

To offer deeper insights into the proposed DAVSOD, we

discuss its several important characteristics.
Sufﬁcient Salient Object Diversity. The salient objects in
DAVSOD span a large set of classes (Fig. 3 (a)) such as an-
imals (e.g., lion, bird), vehicles (e.g., car, bicycle), artifacts
(e.g., box, building), and humans in various activities (e.g.,
dancer, rider), enabling a comprehensive understanding of
object-level saliency in dynamic scenes.

3 Notion of saliency shift. The saliency shift is not just represented
as a binary signal, w.r.t., whether it happens in a certain frame. Since we
focus on an object-level task, we change the saliency values of different
objects according to the shift of human attention.

8557

takephotodancechimpanzeebirdsheepSalient ObjectsVehicleAnimalHumanArtifactsocialartdailysport35%15%7%18%8%14%17%57%0501001502000200400600050100150200instanceobject00.10.20.30.40.50.600.10.20.30.40.50.60.70.80.90100200300400DAVSOD

Camera Mo.

Object Mo.

# Object Instances

slow fast

stable

slow fast

1

2

3 ≥ 4

# videos

102

124

117

72

37

134 125 46

33

Table 3: Statistics regarding camera/object motions and
salient object instance numbers in DAVSOD dataset.

Amount of Salient Object Instances. Existing datasets
fall in short of limited numbers of salient object instances
(Table 1). However, previous studies [34] showed human
could accurately enumerate up to ﬁve objects at a glance
without counting.
In Table 3, DAVSOD is therefore de-
signed to contain more salient objects (≤ 5 salient object
instances per-frame, avg.: 1.65). The distribution of anno-
tated instances in each video can be found in Fig. 3(b).
Size of Salient Objects. The size of object-level salien-
t object is deﬁned as the proportion of foreground objec-
In Fig. 3(d), the ratio distribution
t pixels to the image.
in DAVSOD are 0.29% ∼ 91.3% (avg.: 11.5%), yielding a
broader range.
Varied Camera Motion Patterns. DAVSOD contains di-
verse camera motions (summarized in Table 3). Algorithms
trained on such data could potentially handle realistic dy-
namic scenes better and thus are more practical.
Diverse Object Motion Patterns. DAVSOD inherits the
advantage of DHF1K that covers diverse (Table 3) realistic
dynamic scenes (e.g., object motion from stable to fast). It
is crucial to avoid over-ﬁtting and benchmark algorithms
objectively and precisely.
Center Bias. To depict the degree of center bias, we
compute the average saliency map over all frames for
each dataset. The center bias of DAVSOD and existing
datasets [35, 40, 43, 52, 56, 59, 75] are presented in Fig. 4.

3.4. Dataset Splits

Existing datasets do not maintain a preserved test set,
easily leading to model over-ﬁtting. Thus, our videos are s-
plit into separate training, validation and test sets in the ratio
of 4:2:4. Following random selection, we arrive at a unique
split containing 90 training and 46 validation videos with
released annotations, and 90 test videos with preserved an-
notations for benchmarking. The test set is further divided
into 35 easy, 30 normal, and 25 difﬁcult subsets according
to the degree of difﬁculty of the VSOD task.

4. Proposed Approach
4.1. Saliency Shift Aware VSOD Model
Overview of Model. The proposed SSAV model has two
essential components: pyramid dilated convolution (PD-
C) [67], and saliency-shift-aware convLSTM (SSLSTM).
The former is for robust static saliency representation learn-
ing. The latter one extends traditional convLSTM [65]
with saliency-shift-aware attention (SSAA) mechanism. It
takes the static feature sequence from PDC module as input

SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]

FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]

ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]

MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]

DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]

UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]

VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]

DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD

Figure 4: Center bias of DAVSOD and existing VSOD datasets.

and produces corresponding VSOD results with considering
temporal dynamics and saliency-shift simultaneously.
Pyramid Dilated Convolution (PDC) Module. Recent ad-
vance [10,67] in semantic segmentation and VSOD showed
that stacking a set of parallel dilated convolution layer with
sampling rates can bring better performance, due to the ex-
ploit of multi-scale information and the preservation of s-
patial details. We use the PDC module [67] as our static
feature extractor. Formally, let Q ∈ RW×H×C denote a 3D
feature tensor of an input frame I ∈ Rw×h×3. A dilated conv
layer Dd with the dilated rate d > 1 can be applied to Q
to obtain an output feature P ∈ RW×H×C ′
, which maintains
original spatial resolution while considering a larger recep-
tive ﬁeld (with sampling step d). The PDC is achieved by
arranging a set of K dilated conv layers {Ddk }K
k=1 with d-
ifferent dilated rates {dk}K

k=1 in parallel:

X = [Q, P1, . . . , Pk, . . . , PK ],

(1)
where X ∈ RW×H×(C+KC ′) and Pk = Ddk(Q). [., .] indicates
the concatenation operation. The PDC-enhanced feature X
is a more robust representation (by leveraging multi-scale
information) and preserves original information Q (through
residual connection).
Saliency-Shift-Aware convLSTM (SSLSTM). We pro-
pose a saliency-shift-aware convLSTM, which equips con-
vLSTM [65] with a saliency-shift-aware attention mecha-
nism. It is a powerful recurrent model that not only captures
temporal dynamics but also discriminates salient objects
from the background as well as encodes attention-shift in-
formation. More speciﬁcally, through the PDC module, we
obtain the static representations {Xt}T
t=1 of an input video
with T frames. At time step t, given Xt, the saliency-shift-
aware convLSTM outputs the corresponding salient object
mask St ∈ [0, 1]W×H :

Hidden state: Ht = convLSTM(Xt, Ht−1),

Saliency-shift-aware attention: At = F A({X1, · · · , Xt}),

Attention-enhanced feature: Gm,t = At ⊙ Hm,t,
St = σ(wS ⊗ Gt),

Salient object prediction:

(2)

where H ∈ RW×H×M indicates the 3D-tensor hidden s-
tate. The attention map A ∈ [0, 1]W×H is computed from
a saliency-shift-aware attention network F A, which takes
previous frames into account. Gt ∈ RW×H×M indicates the
attention-enhanced feature in time t. Gm,t ∈ RW×H indi-
cates the 2D feature slice of Gt in the m-th channel (m ∈
[1, M ]). ⊙ is element-wise multiplication. wS ∈ R1×1×M ,
a 1 × 1 conv kernel, is adopted as a salient object readout

8558

Figure 5: Overall architecture of the proposed SSAV model. SSAV consists of two components: pyramid dilated convolution (PDC)
module and saliency-shift-aware convLSTM (SSLSTM) module. The former is for efﬁcient static saliency learning, and the latter captures
temporal dynamics and saliency-shift simultaneously. See § 4 for details.

function, ⊗ indicates conv operation and σ is the sigmoid
activation function.

The key component of above module is the saliency-
shift-aware attention networkF A. Clearly, it acts as a neural
attention mechanism since it is utilized to weight the output
feature H of the convLSTM. Besides, it is desired to be ef-
fective enough to model the human attention-shift behavior.
Considering such task is also different, a small convLSTM
is introduced to build F A, generating a convLSTM in con-
vLSTM structure:
Saliency-shift-aware attention: At = F A({X1, · · · , Xt}),
t = convLSTMA(Xt, HA

Attention feature extraction: HA

t−1),

(3)

Attention mapping: At = σ(wA ⊗ HA

t ),

note that the ﬁrst equation is formulated by the last two
equations. Where wA ∈ R1×1×M indicates a 1 × 1 con-
v kernel that maps the attention feature HA as a signif-
icance matrix and sigmoid σ maps the signiﬁcance value
into [0, 1]. Then the attention At is employed to enhance
the salient object segmentation feature H in Eq. 2. Due
to the apply of convLSTMA, our attention module gain-
s strong learning ability, which provides a solid founda-
tion for learning attention-shift in both explicit and implicit
manners. Let {It∈ Rw×h×3}T
t=1denote a training video with
T frames, {Ft∈ [0, 1]W×H }T
t=1human eye-tracking annota-
tion sequence and {Mt ∈ {0, 1}W×H }T
t=1 video salient ob-
ject ground-truth, we adopt a loss deﬁned over the output
{At ∈ {0, 1}W×H}T
t=1 of the attention model and the ﬁnal
video salient object estimation {St ∈ {0, 1}W×H }T

t=1:
t=1 (cid:16)ℓ(It) · LAtt(At, Ft) + LVSOD(St, Mt)(cid:17),

L = XT

(4)

where LAtt and LVSOD are both cross entropy loss. ℓ(·) ∈
{0,1} indicates whether the attention annotation is avail-
able (since most current VSOD datasets lack eye-ﬁxation

record, see Table 1). When the corresponding attention an-
notation is missing, the error cannot be propagated back.
More importantly, when ℓ(·) = 0, the saliency-shift-aware
attention model F A in Eq. 3 is trained implicitly, which can
be viewed as a typical neural attention mechanism. When
the ground-truth attention is available (ℓ(·) = 1), F A is
trained in an explicit way. With the convLSTM structure,
F A is powerful enough to accurately shift the attention of
our VSOD model to the important objects (see Fig. 6).

4.2. Implementation Details

The base CNN network of PDC model is borrowed from
the conv blocks from ResNet-50 [30] and the conv strides
of the last two blocks are changed to 1. All the input frame
images are resized into 473×473 spatial resolution, and Q ∈
R60×60×2048. Following [67], we set K = 4, C = 512 and
dk = 2k (k ∈ {1, · · ·, 4}). For the convLSTM in Eq. 2, we
use a 3 × 3 × 32 conv kernel. The convLSTMA in Eq. 3
utilizes a 3×3×16 conv kernel. For training protocol, we
follow the same settings in [67] (exclude MSRA-10k [12]
dataset). In addition, we further exploit the validation set of
DAVSOD to train the saliency-shift-aware attention module
explicitly.

5. Benchmark Evaluation Results
5.1. Experimental Settings

Evaluation Metrics. To quantitatively assess the model
performance, we adopt 2 popular evaluation metrics: Mean
Absolute Error (MAE) M [58], F-measure F [1], and the
recent released structure measure S-measure S [18].
Benchmark Models. We benchmark 17 models in total (11
traditional methods, 6 deep learning based models). These
models were selected based on the two criteria: i) having
released implementations, and ii) being representative.
Benchmark Protocols.

To provide a comprehensive

8559

Xt-1CSt-1   CQtP1P2P3P4HtAt    St  CSt+1GtGt-1  It+1ItIt-1dilated convC concatenationelement-wise multiplication  XtLVSOD LVSOD Mt+1Mt-1Ft+1FtFt-1Gt+1Ht+1Ht-1At+1At-1Xt+1LVSOD LAtt LAtt LAtt d442816162816284ResNet50+PDCSSLSTM LossMtSaliency shiftHtAHt-1AHt+1AMetric

a
S
i
V

2
V
g
e
S

-
S
I
V
A
D

l max F ↑
S ↑
M ↓
T max F ↑
-
S
M
S ↑
B
M ↓
F
T max F ↑
S ↑
M ↓
max F ↑
S ↑
M ↓
D max F ↑
S
S ↑
V
U
M ↓
L max F ↑
C
S ↑
M
M ↓
T max F ↑
S ↑
M ↓
T max F ↑
D
O
S ↑
S
V
M ↓
A
D

-
S
O
V

-

2010-2015

2016-2017

2018

SIVM TIMP SPVM RWRV MB+M SAGM GFVM MSTM STBP SGSP SFLR SCOM SCNN DLVS FGRN MBNM PDBM SSAV†
[62]

[41]†

[11]†

[44]†

[76]†

[67]†

[68]†

[74]

[35]

[92]

[70]

[52]

[81]

[75]

[53]

[87]

[8]

.522
.606
.197

.426
.545
.236

.450
.557
.212

.581
.605
.251

.293
.481
.260

.420
.548
.185

.439
.558
.217

.298
.486
.288

.479
.612
.170

.456
.576
.192

.488
.593
.172

.573
.644
.116

.338
.537
.178

.598
.642
.113

.401
.575
.215

.395
.563
.195

.700
.724
.133

.330
.515
.209

.390
.592
.146

.618
.668
.108

.404
.581
.146

.595
.665
.105

.351
.511
.223

.358
.538
.202

.440
.595
.188

.336
.521
.242

.345
.556
.199

.438
.583
.162

.281
.536
.180

.446
.577
.167

.422
.552
.211

.283
.504
.245

.692
.726
.129

.487
.609
.206

.470
.597
.177

.554
.618
.146

.339
.563
.169

.261
.539
.178

.562
.661
.158

.342
.538
.228

.688
.749
.105

.564
.659
.161

.515
.676
.103

.634
.719
.081

.414
.629
.111

.422
.615
.136

.482
.619
.172

.370
.565
.184

.683
.757
.107

.571
.651
.160

.569
.687
.103

.592
.699
.091

.426
.628
.106

.406
.613
.132

.506
.615
.162

.334
.553
.167

.673
.749
.095

.500
.613
.177

.429
.583
.165

.526
.643
.114

.336
.551
.145

.313
.540
.171

.567
.657
.144

.344
.532
.211

.622
.629
.163

.595
.627
.152

.544
.677
.096

.640
.735
.061

.403
.614
.105

.607
.700
.078

.526
.576
.163

.410
.568
.160

.677
.706
.165

.630
.661
.172

.655
.692
.138

.673
.681
.124

.544
.601
.165

.645
.679
.100

.426
.557
.236

.426
.577
.207

.779
.814
.062

.660
.699
.117

.727
.790
.056

.745
.804
.037

.562
.713
.059

.669
.734
.054

.546
.624
.145

.478
.624
.132

.831
.762
.122

.797
.794
.079

.783
.832
.048

.764
.815
.030

.420
.555
.206

.422
.569
.204

.690
.712
.162

.464
.599
.220

.831
.847
.071

.762
.794
.095

.714
.783
.064

**
**
**

.550
.712
.075

.628
.730
.054

.609
.704
.109

.532
.674
.128

.852
.881
.048

.759
.794
.091

.708
.794
.061

**
**
**

.564
.721
.060

.551
.682
.060

.675
.760
.099

.521
.657
.129

.848
.861
.045

.767
.809
.088

.783
.838
.043

**
**
**

.630
.745
.042

.625
.709
.044

.669
.715
.097

.573
.693
.098

.883
.898
.020

.816
.857
.047

.861
.887
.031

.716
.809
.026

.550
.698
.079

.698
.755
.119

.670
.742
.099

.520
.637
.159

.888
.907
.032

.821
.851
.064

.855
.882
.028

.800
.864
.024

.863
.901
.018

.798
.856
.021

.742
.818
.078

.572
.698
.116

.939
.943
.020

.865
.879
.040

.861
.893
.028

.801
.851
.023

.801
.861
.025

.774
.819
.027

.742
.819
.073

.603
.724
.092

Table 4: Benchmarking results of 17 state-of-the-art VSOD models on 7 datasets: SegV2 [40], FBMS [56], ViSal [75], MCL [35],
DAVIS [59], UVSD [52], VOS [43] and the proposed DAVSOD (35 easy test set). Note that TIMP was only tested on 9 short sequences of
VOS because it cannot handle long videos. “**” indicates the model has been trained on this dataset. “-T” indicates the results on the test
set of this dataset. “†” indicates deep learning model. Darker color indicates better performance. The best scores are marked in bold.

benchmark, we evaluate 17 representative methods on ex-
isting 7 datasets and the proposed DAVSOD dataset. The
test sets of FBMS [56] (30 clips), DAVIS [59] (20 clips),
DAVSOD (35 easy clips) datasets, and the whole ViSal [75]
(17 clips), MCL [35] (9 clips), SegV2 [40] (13 clips), UVS-
D [52] (18 clips) datasets are used for testing. For VOS [43]
dataset, we randomly select 40 sequences as test set. There
are total 182 videos with 848,340 (47,130×18) frames.

5.2. Performance Comparison and Data Analysis

In this section, we provide some interesting ﬁndings

which would beneﬁt the further research.
Performance of Traditional Models. Based on the differ-
ent metrics in Table 4, we conclude that: “SFLR [8], S-
GSP [52], and STBP [81] are the top 3 non-deep learning
models for VSOD.” Both SFLR and SGSP explicitly con-
sider the optical ﬂow strategy to extract the motion features.
However, the computational cost is usually expensive (see
Table 2). One noteworthy ﬁnding is that all these models u-
tilize the superpixel technology to integrate spatiotemporal
features on region level.
Performance of Deep Models. The top 3 models in this
benchmark (i.e., SSAV, PDBM [67], MBNM [44]) are al-
l based on deep learning technique, which demonstrates
the strong learning power of neural networks. For ViSal

dataset (the ﬁrst speciﬁcally-designed dataset for VSOD),
their average performance (e.g., max E-measure [19], max
F-measure, or S-measure) is even higher than 0.9.

Traditional vs Deep VSOD Models.
In Table 4, almost
all of the deep models outperform traditional algorithms,
as more powerful saliency representations can be extract-
ed from networks. Another interesting ﬁnding is the clas-
sic leading method (SFLR [8]) performs better than some
deep models (e.g., SCOM [11]) on MCL, UVSD, ViSal, and
DAVSOD datasets. It indicates that investigating more ef-
fective deep learning architectures with the exploit of hu-
man prior knowledge for VSOD is a promising direction.

Dataset Analysis. We mark the scores with gray color in
Table 4. Darker colors mean better performance for speciﬁc
metrics (e.g., max F , S, and M). We ﬁnd that ViSal and
UVSD datasets are relatively easy, since the top 2 models:
SSAV and PDBM [67] gained very high performance (e.g.,
S > 0.9). However, for more challenging datasets like
DAVSOD, the performance of VOSD models decrease dra-
matically (S < 0.73). It reveals that both the overall and
individual performance of VOSD models leave abundant
room for future research.

Runtime Analysis. Table 2 reports the computation time of
previous VSOD methods and the proposed SSAV approach

8560

(1)

(2)

(3)

(4)

(5)

(a) Frame

(b) Fixation

(c) GT

(d) SSAV

(e) MBNM [44] (f) FGRN [41]

(g) PDBM [67]

(h) SFLR [8]

(i) SAGM [74]

Figure 6: Visual comparisons with top 3 deep (MBNM [44], FGRN [41], PDBM [67]) models and 2 traditional classical (SFLR [8],
SAGM [74]) models on the proposed DAVSOD dataset. Our SSAV model captures the saliency shift phenomenon successfully.

(in PCT column). For the models with released codes, the
timings are tested on the same platform: Intel Xeon(R) E5-
2676v3 @2.4GHz×24 and GTX TITAN X. The rest of the
timings are borrowed from their papers. Note that the pro-
posed model does not apply any pre-/post-processing (e.g.,
CRF), thus the processing speed only takes about 0.05s.

5.3. Ablation Study

Implicit vs Explicit Saliency-Shift-Aware Attention
Mechanism. To study the inﬂuence of different training
strategies of the proposed SSAA module, we derive 2 base-
lines: explicit and implicit, refer to the proposed SSAV
model trained explicitly or implicitly. We obtain the im-
plicit baseline by only using VSOD annotations (exclude
DAVSOD). We observe that SSAV with explicit attention is
better than the one with implicit attention, according to the
statistics in Table 5. It demonstrates that utilizing ﬁxation
data can help our model to better capture saliency shift and
thus further boost ﬁnal VSOD performance.
Effectiveness of Saliency-Shift-Aware convLSTM. To s-
tudy the effectiveness of SSLSTM (§ 4), we provide another
baseline: w/o SSLSTM, which excludes SSLSTM module
from the proposed SSAV model. From Table 5, we observe
a performance decrease (e.g., S : 0.724 → 0.667), which
conﬁrms that the proposed SSLSTM module is effective to
learn both selective attention allocation and attention shift
cues from the challenging data.
Comparison with State-of-the-Arts. In Table 4, we com-
pare the proposed SSAV model with current 17 state-of-the-
art VSOD algorithms. The proposed baseline method per-
forms better against other competitors over most existing
datasets. More speciﬁcally, our model obtains signiﬁcant
performance improvements on ViSal and FBMS datasets. It
also obtains comparable performance on VOS, SegV2 and
DAVIS datasets.

5.4. Analysis for the saliency shift challenge

For the proposed challenging DAVSOD dataset,

the
SSAV model also gains the best performance. We attribute

Type

S ↑ max F ↑ M ↓
0.092
0.724
0.684
0.103
0.132
SSLSTM w/o SSLSTM 0.667

Baseline
explicit
implicit

0.603
0.593
0.541

SSAA

Table 5: Ablation studies of the SSAV on DAVSOD dataset.

the promising performance to the introduce of SSLSTM,
which efﬁciently captures saliency allocations in dynam-
ic scenes and guides our model to accurately attend to
those visually important regions. Fig. 6 shows that the pro-
posed SSAV approach obtains more visually favorable re-
sults than other top competitors. Our SSAV model captures
the saliency shift successfully (from frame-1 to frame-5:
cat → [cat, box] → cat → box → [cat, box]). However, the
other top-performance VSOD models either do not high-
light the whole salient objects (e.g., SFLR, SAGM) or on-
ly capture the moving cat (e.g., MBNM). We envision our
SSAV model would open up promising future directions for
model development.

6. Conclusion

We have presented a comprehensive study on VSOD by
creating a new visual-attention-consistent DAVSOD dataset,
building up the largest-scale benchmark, and proposing a
SSAV baseline model. Compared with other competing tra-
ditional or deep learning models, the proposed SSAV model
achieves superior performance and produces more visually
favorable results. Extensive experiments veriﬁed that even
considering top performing models, VSOD remain seems
far from being solved. The above contributions and in-depth
analyses would beneﬁt the develop of this area and be help-
ful to stimulate broader potential research, e.g., saliency-
aware video captioning, video salient object subitizing and
instance-level VSOD.

Acknowledgements. This
supported by NSFC
(61620106008, 61572264), the national youth talent support program,
the Fundamental Research Funds for the Central Universities (Nankai
University, NO. 63191501) and Tianjin Natural Science Foundation
(17JCJQJC43700, 18ZXZNGX00110).

research was

8561

References

[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada,
and Sabine Susstrunk. Frequency-tuned salient region detec-
tion. In IEEE CVPR, pages 1597–1604, 2009. 6

[2] Tariq Alshawi, Zhiling Long, and Ghassan AlRegib. Unsu-
pervised uncertainty estimation using spatiotemporal cues in
video saliency detection. IEEE TIP, pages 2818–2827, 2018.
3

[3] C¸ a˘glar Aytekin, Horst Possegger, Thomas Mauthner, Serkan
Kiranyaz, Horst Bischof, and Moncef Gabbouj. Spatiotem-
poral saliency estimation by spectral foreground detection.
IEEE TMM, 20(1):82–95, 2018. 3

[4] Saumik Bhattacharya, K Subramanian Venkatesh, and
Sumana Gupta. Visual saliency detection using spatiotem-
poral decomposition. IEEE TIP, 27(4):1665–1675, 2018. 3
[5] Marc D. Binder, Nobutaka Hirokawa, and Uwe Windhorst,
editors. Gaze Shift, pages 1676–1676. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 2009. 1

[6] Ali Borji, Dicky N Sihite, and Laurent Itti. What stands out
in a scene? A study of human explicit saliency judgment.
Vision Research, 91:62–77, 2013. 1

[7] Chenglizhao Chen, Shuai Li, Hong Qin, Zhenkuan Pan, and
Guowei Yang. Bi-level feature learning for video saliency
detection. IEEE TMM, 2018. 1, 3

[8] Chenglizhao Chen, Shuai Li, Yongguang Wang, Hong Qin,
and Aimin Hao. Video saliency detection via spatial-
temporal fusion and low-rank coherency diffusion.
IEEE
TIP, 26(7):3156–3170, 2017. 2, 3, 7, 8

[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE TPAMI, 40(4):834–848,
2018. 3

[10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE TPAMI, 40(4):834–848,
2018. 5

[11] Yuhuan Chen, Wenbin Zou, Yi Tang, Xia Li, Chen Xu, and
Nikos Komodakis. Scom: Spatiotemporal constrained opti-
mization for salient object detection. IEEE TIP, 27(7):3345–
3357, 2018. 2, 3, 7

[12] Ming-Ming Cheng, Niloy J. Mitra, Xiaolei Huang, Philip
H. S. Torr, and Shi-Min Hu. Global contrast based salient
region detection. IEEE TPAMI, 37(3):569–582, 2015. 3, 6

[13] Runmin Cong, Jianjun Lei, Huazhu Fu, Ming-Ming Cheng,
Weisi Lin, and Qingming Huang. Review of visual saliency
detection with comprehensive information.
IEEE TCSVT,
PP(99):1–19, 2018. 3

[14] Runmin Cong, Jianjun Lei, Huazhu Fu, Qingming Huang,
Xiaochun Cao, and Chunping Hou. Co-saliency detection for
rgbd images based on multi-constraint feature matching and
cross label propagation. IEEE TIP, 27(2):568–579, 2018. 3
[15] Runmin Cong, Jianjun Lei, Huazhu Fu, Weisi Lin, Qing-
ming Huang, Xiaochun Cao, and Chunping Hou. An iter-
ative co-saliency framework for rgbd images. IEEE TYCB,
49(1):233–246, 2019. 3

[16] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes challenge: A retrospective. IJCV,
111(1):98–136, 2015. 3

[17] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang-
Hua Gao, Qibin Hou, and Ali Borji. Salient objects in clut-
ter: Bringing salient object detection to the foreground. In
ECCV. Springer, 2018. 1

[18] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and
Ali Borji. Structure-measure: A New Way to Evaluate Fore-
ground Maps. In IEEE ICCV, pages 4548–4557, 2017. 3,
6

[19] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-
Ming Cheng, and Ali Borji. Enhanced-alignment Measure
for Binary Foreground Map Evaluation.
In IJCAI, pages
698–704, 2018. 7

[20] Yuming Fang, Weisi Lin, Zhenzhong Chen, Chia-Ming T-
sai, and Chia-Wen Lin. A video saliency detection model in
compressed domain. IEEE TCSVT, 24(1):27–38, 2014. 3

[21] Yuming Fang, Zhou Wang, Weisi Lin, and Zhijun Fang.
Video saliency incorporating spatiotemporal cues and uncer-
tainty weighting. IEEE TIP, 23(9):3910–3921, 2014. 3

[22] Li Fei-Fei and Pietro Perona. A bayesian hierarchical model
for learning natural scene categories. In IEEE CVPR, pages
524–531, 2005. 3

[23] Steven L Franconeri, Daniel J Simons, and Justin A Junge.
Searching for stimulus-driven shifts of attention. Psycho-
nomic Bulletin & Review, 11(5):876–881, 2004. 4

[24] Ken Fukuchi, Kouji Miyazato, Akisato Kimura, Shigeru
Takagi, and Junji Yamato. Saliency-based video segmen-
tation with graph cuts and sequentially updated priors.
In
ICME, pages 638–641, 2009. 3

[25] Siavash Gorji and James J Clark. Going From Image to
Video Saliency: Augmenting Image Salience With Dynamic
Attentional Push. In IEEE CVPR, pages 7501–7511, 2018.
1

[26] Chenlei Guo, Qi Ma, and Liming Zhang. Spatio-temporal
saliency detection using phase spectrum of quaternion fouri-
er transform. In IEEE CVPR, pages 1–8, 2008. 3

[27] Chenlei Guo and Liming Zhang. A novel multiresolution
spatiotemporal saliency detection model and its applications
in image and video compression. IEEE TIP, 19(1):185–198,
2010. 1

[28] Fang Guo, Wenguan Wang, Jianbing Shen, Ling Shao, Jian
Yang, Dacheng Tao, and Yuan Yan Tang. Video saliency
detection using object proposals. IEEE Transactions on Cy-
bernetics, 2017. 3

[29] Hadi Hadizadeh and Ivan V Bajic. Saliency-aware video

compression. IEEE TIP, 23(1):19–33, 2014. 1

[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In IEEE

Deep residual learning for image recognition.
CVPR, pages 770–778, 2016. 6

[31] Yuan-Ting Hu, Jia-Bin Huang, and Alexander G. Schwing.
Unsupervised video object segmentation using motion
saliency-guided spatio-temporal propagation.
In ECCV.
Springer, 2018. 1, 3

8562

[32] Md Amirul Islam, Mahmoud Kalash, and Neil DB Bruce.
Revisiting salient object detection: Simultaneous detection,
ranking, and subitizing of multiple salient objects. In IEEE
CVPR, pages 7142–7150, 2018. 3

[49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, pages 740–755. Springer, 2014. 3, 4

[33] Yeong Jun Koh, Young-Yoon Lee, and Chang-Su Kim. Se-
quential clique optimization for video object segmentation.
In ECCV. Springer, 2018. 3

[50] Nian Liu, Junwei Han, and Ming-Hsuan Yang. PiCANet:
Learning pixel-wise contextual attention for saliency detec-
tion. In IEEE CVPR, pages 3089–3098, 2018. 3

[34] Edna L Kaufman, Miles W Lord, Thomas Whelan Reese,
and John Volkmann. The discrimination of visual number.
The American Journal of Psychology, 62(4):498–525, 1949.
5

[35] Hansang Kim, Youngbae Kim, Jae-Young Sim, and Chang-
Su Kim. Spatiotemporal saliency detection for video se-
quences based on random walk with restart.
IEEE TIP,
24(8):2552–2564, 2015. 1, 2, 3, 5, 7

[36] Wonjun Kim, Chanho Jung, and Changick Kim. Spatiotem-
poral saliency detection and its applications in static and dy-
namic scenes. IEEE TCSVT, 21(4):446–456, 2011. 1, 3

[37] Christof Koch and Shimon Ullman. Shifts in selective vi-
In

sual attention: Towards the underlying neural circuitry.
Matters of Intelligence, pages 115–141. 1987. 1, 4

[38] Trung-Nghia Le and Akihiro Sugimoto. Deeply supervised
In

3d recurrent fcn for salient object detection in videos.
BMVC, 2017. 1, 3

[39] Trung-Nghia Le and Akihiro Sugimoto. Video salient objec-
IEEE TIP,

t detection using spatiotemporal deep features.
27(10):5002–5015, 2018. 1, 3

[40] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and
James M Rehg. Video segmentation by tracking many ﬁgure-
ground segments. In IEEE ICCV, pages 2192–2199, 2013.
1, 2, 3, 5, 7

[41] Guanbin Li, Yuan Xie, Tianhao Wei, Keze Wang, and Liang
Lin. Flow guided recurrent neural encoder for video salient
object detection. In IEEE CVPR, pages 3243–3252, 2018. 2,
3, 7, 8

[42] Guanbin Li and Yizhou Yu. Deep contrast learning for salient

object detection. In IEEE CVPR, pages 478–487, 2016. 3

[43] Jia Li, Changqun Xia, and Xiaowu Chen. A benchmark
dataset and saliency-guided stacked autoencoders for video-
based salient object detection.
IEEE TIP, 27(1):349–364,
2018. 1, 2, 5, 7

[44] Siyang Li, Bryan Seybold, Alexey Vorobyov, Xuejing Lei,
and C.-C. Jay Kuo. Unsupervised video object segmentation
with motion-based bilateral networks.
In ECCV. Springer,
2018. 2, 3, 7, 8

[51] Tie Liu, Jian Sun, Nan-Ning Zheng, Xiaoou Tang, and
Heung-Yeung Shum. Learning to Detect A Salient Object.
In IEEE CVPR, pages 1–8, 2007. 3

[52] Zhi Liu, Junhao Li, Linwei Ye, Guangling Sun, and Li-
quan Shen. Saliency detection for unconstrained videos us-
ing superpixel-level graph and spatiotemporal propagation.
IEEE TCSVT, 27(12):2527–2542, 2017. 1, 2, 3, 5, 7

[53] Zhi Liu, Xiang Zhang, Shuhua Luo, and Olivier Le Meur.
IEEE

Superpixel-based spatiotemporal saliency detection.
TCSVT, 24(9):1522–1540, 2014. 2, 3, 7

[54] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In IEEE
CVPR, pages 3431–3440, 2015. 3

[55] Thomas Mauthner, Horst Possegger, Georg Waltner, and
Horst Bischof. Encoding based saliency detection for videos
and images. In IEEE CVPR, pages 2494–2502, 2015. 3

[56] Peter Ochs, Jitendra Malik, and Thomas Brox. Segmentation
of moving objects by long term video analysis. IEEE TPAMI,
36(6):1187–1200, 2014. 1, 2, 3, 5, 7

[57] Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video
In CVPR,

captioning with transferred semantic attributes.
pages 6504–6512, 2017. 1

[58] Federico Perazzi, Philipp Kr¨ahenb¨uhl, Yael Pritch, and
Alexander Hornung. Saliency ﬁlters: Contrast based ﬁltering
for salient region detection. In CVPR, pages 733–740, 2012.
6

[59] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc
Van Gool, Markus Gross, and Alexander Sorkine-Hornung.
A benchmark dataset and evaluation methodology for video
object segmentation. In IEEE CVPR, pages 724–732, 2016.
1, 2, 3, 5, 7

[60] Matthew S Peterson, Arthur F Kramer, and David E Irwin.
Covert shifts of attention precede involuntary eye move-
ments. Perception & Psychophysics, 66(3):398–405, 2004.
1, 4

[61] Wenliang Qiu, Xinbo Gao, and Bing Han. Eye ﬁxation as-
sisted video saliency detection via total variation-based pair-
wise interaction. IEEE TIP, pages 4724–4739, 2018. 1, 3

[45] Yin Li, Xiaodi Hou, Christof Koch, James M Rehg, and
Alan L Yuille. The secrets of salient object segmentation.
In IEEE CVPR, pages 280–287, 2014. 1

[62] Esa Rahtu, Juho Kannala, Mikko Salo, and Janne Heikkil¨a.
Segmenting salient objects from images and videos. In EC-
CV, pages 366–379. Springer, 2010. 2, 3, 7

[46] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.
Fully convolutional instance-aware semantic segmentation.
In IEEE CVPR, pages 2359–2367, 2017. 3

[63] Hae Jong Seo and Peyman Milanfar. Static and space-time
visual saliency detection by self-resemblance. Journal of Vi-
sion, 9(12):15–15, 2009. 3

[47] Yong Li, Bin Sheng, Lizhuang Ma, Wen Wu, and Zhifeng
Xie. Temporally coherent video saliency using regional dy-
namic contrast. IEEE TCSVT, 23(12):2067–2076, 2013. 3

[48] Ming Liang and Xiaolin Hu. Recurrent convolutional neural
network for object recognition. In IEEE CVPR, pages 3367–
3375, 2015. 3

[64] Karthikeyan Shanmuga Vadivel, Thuyen Ngo, Miguel Eck-
stein, and BS Manjunath. Eye tracking assisted extraction of
attentionally important objects from videos. In CVPR, pages
3241–3250, 2015. 3

[65] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
Wai-Kin Wong, and Wang-Chun Woo. Convolutional LST-

8563

[82] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and
Thomas S Huang. Deep interactive object selection. In IEEE
CVPR, pages 373–381, 2016. 1

[83] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, D-
ingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and
Thomas Huang. Youtube-vos: Sequence-to-sequence video
object segmentation. In ECCV, pages 585–601, 2018. 1

[84] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and
Ming-Hsuan Yang. Saliency detection via graph-based man-
ifold ranking. In IEEE CVPR, pages 3166–3173, 2013. 3

[85] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-

tion by dilated convolutions. In ICLR, 2016. 3

[86] Yu Zeng, Huchuan Lu, Lihe Zhang, Mengyang Feng, and
Ali Borji. Learning to promote saliency detectors. In IEEE
CVPR, pages 1644–1653, 2018. 3

[87] Jianming Zhang, Stan Sclaroff, Zhe Lin, Xiaohui Shen, Bri-
an Price, and Radomir Mech. Minimum barrier salient object
detection at 80 fps. In IEEE ICCV, pages 1404–1412, 2015.
2, 3, 7

[88] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi,
and Richard Hartley. Deep unsupervised saliency detection:
A multiple noisy labeling perspective. In IEEE CVPR, pages
9029–9038, 2018. 3

[89] Lu Zhang, Ju Dai, Huchuan Lu, You He, and Gang Wang. A
bi-directional message passing model for salient object de-
tection. In IEEE CVPR, pages 1741–1750, 2018. 3

[90] Xiaoning Zhang, Tiantian Wang, Jinqing Qi, Huchuan Lu,
and Gang Wang. Progressive attention guided recurrent net-
work for salient object detection. In IEEE CVPR, pages 714–
722, 2018. 3

[91] Ziyu Zhang, Sanja Fidler, and Raquel Urtasun.

Instance-
level segmentation for autonomous driving with deep dense-
ly connected mrfs.
In IEEE CVPR, pages 669–677, 2016.
1

[92] Feng Zhou, Sing Bing Kang, and Michael F Cohen. Time-
mapping using space-time saliency. In IEEE CVPR, pages
3358–3365, 2014. 2, 3, 7

[93] Xiaofei Zhou, Zhi Liu, Chen Gong, and Wei Liu. Improv-
ing video saliency detection via localized estimation and s-
patiotemporal reﬁnement.
IEEE TMM, pages 2993–3007,
2018. 3

[94] Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, Xi Peng,
and Ahmed Elgammal. A generative adversarial approach
for zero-shot learning from noisy texts. In CVPR, 2018. 3

[95] Yizhe Zhu, Jianwen Xie, Zhiqiang Tang, Xi Peng, and
Ahmed Elgammal. Learning where to look: Semantic-
guided multi-attention localization for zero-shot learning.
arXiv preprint arXiv:1903.00502, 2019. 1

M network: A machine learning approach for precipitation
nowcasting. In NIPS, 2015. 5

[66] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR,
2015. 3

[67] Hongmei Song, Wenguan Wang, Sanyuan Zhao, Jianbing
Sheng, and Kin-Man Lam. Pyramid dilated deeper convL-
STM for video salient object detection. In ECCV. Springer,
2018. 2, 3, 5, 6, 7, 8

[68] Yi Tang, Wenbin Zou, Zhi Jin, Yuhuan Chen, Yang Hua, and
Xia Li. Weakly supervised salient object detection with spa-
tiotemporal cascade neural networks. IEEE TCSVT, 2018. 2,
3, 7

[69] Anne M Treisman and Garry Gelade. A feature-integration
theory of attention. Cognitive Psychology, 12(1):97–136,
1980. 3

[70] Wei-Chih Tu, Shengfeng He, Qingxiong Yang, and Shao-Yi
Chien. Real-time salient object detection with a minimum
spanning tree. In IEEE CVPR, pages 2334–2342, 2016. 2, 3,
7

[71] Tiantian Wang, Lihe Zhang, Shuo Wang, Huchuan Lu, Gang
Yang, Xiang Ruan, and Ali Borji. Detect globally, reﬁne
locally: A novel approach to saliency detection.
In IEEE
CVPR, pages 3127–3135, 2018. 3

[72] Wenguan Wang, Jianbing Shen, Xingping Dong, and Al-
i Borji. Salient object detection driven by ﬁxation prediction.
In IEEE CVPR, pages 1711–1720, 2018. 3

[73] Wenguan Wang, Jianbing Shen, Fang Guo, Ming-Ming
Cheng, and Ali Borji. Revisiting video saliency: A large-
scale benchmark and a new model. In IEEE CVPR, pages
4894–4903, 2018. 4

[74] Wenguan Wang, Jianbing Shen, and Fatih Porikli. Saliency-
aware geodesic video object segmentation. In IEEE CVPR,
pages 3395–3402, 2015. 1, 2, 3, 7, 8

[75] Wenguan Wang, Jianbing Shen, and Ling Shao. Consisten-
t video saliency using local gradient ﬂow optimization and
global reﬁnement. IEEE TIP, 24(11):4185–4196, 2015. 1, 2,
3, 5, 7

[76] Wenguan Wang, Jianbing Shen, and Ling Shao. Video salient
object detection via fully convolutional networks. IEEE TIP,
27(1):38–49, 2018. 2, 3, 7

[77] Wenguan Wang, Jianbing Shen, Hanqiu Sun, and Ling Shao.
IEEE TCSVT,

Video co-saliency guided co-segmentation.
28(8):1727–1736, 2018. 3

[78] Wenguan Wang, Jianbing Shen, Ruigang Yang, and Fatih
IEEE

Porikli. Saliency-aware video object segmentation.
TPAMI, pages 20–33, 2018. 3

[79] Yichen Wei, Fang Wen, Wangjiang Zhu, and Jian Sun.
Geodesic saliency using background priors. In ECCV, pages
29–42. Springer, 2012. 3

[80] Jeremy M Wolfe, Kyle R Cave, and Susan L Franzel. Guided
search: An alternative to the feature integration model for
visual search. Journal of Experimental Psychology: Human
Perception and Performance, 15(3):419, 1989. 3

[81] Tao Xi, Wei Zhao, Han Wang, and Weisi Lin. Salient object
detection with spatiotemporal background priors for video.
IEEE TIP, 26(7):3425–3436, 2017. 2, 3, 7

8564

