Dichromatic Model Based Temporal Color Constancy for AC Light Sources

Jun-Sang Yoo, Jong-Ok Kim

School of Electrical Engineering, Korea University

Seoul, Korea

look2017@korea.ac.kr, jokim@korea.ac.kr

Abstract

Existing dichromatic color constancy approach com-
monly requires a number of spatial pixels which have high
specularity.
In this paper, we propose a novel approach
to estimate the illuminant chromaticity of AC light source
using high-speed camera. We found that the temporal ob-
servations of an image pixel at a ﬁxed location distribute
on an identical dichromatic plane. Instead of spatial pixels
with high specularity, multiple temporal samples of a pixel
are exploited to determine AC pixels for dichromatic plane
estimation, whose pixel intensity is sinusoidally varying
well. A dichromatic plane is calculated per each AC pixel,
and illuminant chromaticity is determined by the intersec-
tion of dichromatic planes. From multiple dichromatic
planes, an optimal illuminant is estimated with a novel MAP
framework. It is shown that the proposed method outper-
forms both existing dichromatic based methods and tempo-
ral color constancy methods, irrespective of the amount of
specularity.

1. Introduction

Color constancy is human’s inherent ability to adapt to
various changes of lighting condition [30]. Thanks to our
brain’s memory ability, human can easily discern the orig-
inal color of an object irrespective of illuminant condition
[26, 29]. In contrast, for machine vision, a computational
color constancy technique is necessarily required to recover
the original color of an object because it does not have any
prior knowledge about neither reﬂectance of an object nor
illuminant color. Thus, color constancy plays a vital role
in machine vision, which contributes to image quality en-
hancement. There have been many color constancy algo-
rithms to improve color visual quality [1, 3, 7, 9, 11, 17, 34],
and most of these methods are typically classiﬁed into to
4 major categories: statistics-based, physics-based, gamut-
based, and learning-based methods.

Statistics-based methods have been most actively re-
searched. Due to their simple assumption of Lambertian re-

Figure 1. Summary of the proposed method. The fast variations
of AC light source is captured with high-speed camera. An illu-
minant is estimated from multiple dichromatic planes, which are
obtained from temporal observations at a pixel.

ﬂectance and low computational cost with quite high accu-
racy [6, 19, 39], it is even commonly used in digital camera
[43]. However, there is a critical drawback of these methods
that a scene must include various colors on target surfaces to
satisfy their statistical assumption [10, 38, 39]. On the other
hand, physics-based methods are more complex approach
to color constancy than statistics-based ones in that spec-
ularity (surface reﬂected light) is additionally considered
[36]. Their fundamental concept is based on the dichro-
matic reﬂection model, which represents the physical rela-
tionship between illuminant and object surface [13, 38]. In
contrast with statistics-based methods, they work well with
monotonous surface color [11, 14, 35]. However, since the
number of parameters in the dichromatic reﬂection model
is increased by adding the specular component, they be-
come a severe ill-posed problem, and thus commonly re-
quire additional assumption such as ﬁxed parameter (com-
monly diffuse weight) and sufﬁcient specularity. Gamut-
based methods [12, 16, 18] have also attracted many atten-
tions, but they require proper training data which are ap-
propriate for a target illuminant. Recently, there have been
proposed learning-based methods to use convolutional neu-

12329

ral network. They learn the plenty of ﬁlter parameters of
each layer given massive image-illuminant color pairs, and
draw an optimal solution [2,4,8,21,33]. However, their per-
formances highly depend on training data set, and still have
difﬁculty in solving a fundamental ill-posed problem clearly
such as discrimination between object reﬂectance and illu-
minant [4].

Thanks to the recent development of high-speed cam-
era, it has been just equipped with consumer devices such
as smartphone. It is expected that it will be popularly used
for consumer as well as industry in future because it makes
possible to capture minute variations of a scene, which is
imperceptible for human eyes [5, 20]. In this paper, we pro-
pose a novel approach to exploit high-speed capture capa-
bility for color constancy. Alternative current (AC) electric
power is varying sinusoidally with time. For example, it
ﬂickers 120 times per second for 60 Hz AC power. Light
sources powered by this alternating current make their in-
tensities ﬂuctuate with an double AC frequency [44]. This
ﬂickering of AC light source can be captured by high-speed
camera, and exploited as a powerful visual cue [37]. Pre-
vious dichromatic based single image methods use distinct
spatial pixels to obtain a dichromatic line or a plane. Their
performance signiﬁcantly depends on the selection of spa-
tial pixels (so called specular pixels), which ideally have
identical diffuse and specular chromaticities. However, it
is difﬁcult to extract specular region from an image. Es-
pecially they work poorly for non-specular images, which
commonly have low signal-to-noise ratio (SNR), leading to
low model estimation accuracy [38, 46]. Existing dichro-
matic based temporal methods [31, 48] exploit the RGB in-
tensity differences of adjacent frames to estimate illuminant
color. These method also require high-specularity because
a pixel with low intensity is prone to have many temporal
noises.

In this paper, we propose a novel temporal color con-
stancy method. With multiple high-speed observations at
a pixel on temporal domain, it attempts to ﬁnd an optimal
solution for ill-posed dichromatic equation. The sinusoidal
variation of AC light intensity enables us to obtain multiple
distinct reﬂectances at the same location of a scene during
a short time interval. In contrast with previous dichromatic
model based methods, the proposed method does not re-
quire a high-specularity region, and does not assume that
the diffuse weight of the dichromatic model is ﬁxed. These
assumptions have been a critical limitation for practical use
so far. We analyzed the temporal variations of dichromatic
model parameters, and actually observed that both diffuse
and specular weights are dynamic under time-varying AC
light. This means that under AC light sources, the dichro-
matic model should be described by an original plane rather
than a projected line. Inspired by this observation, we ﬁrst
determine a set of AC pixels which are the pixels with si-

nusoidally varying intensity. The AC pixel is much more
common than specular one, and it can be easily determined
due to its periodic intensity property. Note that dichromatic
based illuminant estimation is very sensitively affected by
the specularity of pixel samples in existing methods. Pixel
intensities on temporal domain are modeled by a sinusoidal
curve using Gauss-Newton method, and a number of closely
ﬁt pixels are selected as an AC pixel. A dichromatic plane
is estimated in a least-square way for each AC pixel. From
those AC pixels, we obtain dichromatic planes, and a candi-
date illuminant is obtained by calculating the intersection of
a pair of dichromatic planes. This candidate illuminant esti-
mation is performed for all dichromatic plane pairs. Finally,
an optimal illuminant is estimated by the proposed maxi-
mum a posterioi (MAP) framework. This MAP estimation
is formulated by incorporating the physical and statistical
properties of illuminants, which are actually directional ac-
curacy (likelihood) and Planckian locus distance (prior con-
straint). Through a MAP formulation with both properties,
physical and statistical advantages are optimally combined
to produce an accurate estimation of illuminant.

The contributions of the paper are summarized as fol-

lows:
• Under AC illuminants, high-speed camera can capture the
fast variations of illuminant intensity. We analyzed the vari-
ations of image pixel on temporal domain, and exploit them
to accurately estimate a dichromatic plane, which makes it
easy to estimate an optimal illuminant of a scene.
• Due to sinusoidally time-varying property of a pixel, we
can easily select a AC pixel which contributes to estimate
an accurate dichromatic plane. Also, it is easy to denoise it
by a sinusoidal modeling. This AC pixel is easy to ﬁnd and
is more common than specular one.
• We propose a new MAP estimation framework to de-
termine an optimal illuminant from multiple dichromatic
planes. It incorporates the physical property and statistical
prior of illuminants.

2. Dichromatic Based Illuminant Estimation

In dichromatic reﬂection model, reﬂected light from an
inhomogeneous object is composed of diffuse and specu-
lar reﬂections, since refractive index is different between
surfaces and bodies (cause diffuse reﬂection), and between
surfaces and the air (cause specular reﬂection) [36]. Thus, it
is composed of both diffuse and specular components, and
represented by the weighted sum of chromaticity and illu-
minant as given by

Ic = mdΛc + msΓc,

c ∈ r, g, b

(1)

where Λc and Γc are diffuse and illuminant (specular) chro-
maticitiy, respectively. In (1), md and ms are diffuse and

12330

y
t
i
s
n
e

t

n

i
 

e
v
i
t

l

a
e
R

10

0

-10

0

Diffuse weight
Specular weight

5

0

t

i

h
g
e
w

l

 
r
a
u
c
e
p
S

15

-5
-10

5
10
Frame index

(a)

0

Diffuse weight

10

(b)

Figure 2. Variations of diffuse and specular weights. (a) Relative
temporal variations and (b) their relationship. As shown in (b),
md and ms have a non-linear relationship, thus dichromatic model
should span a plane rather than a line.

specular weights, respectively, which are deﬁned as:

md = wdX Bi, ms = wsX Gi

(2)

where wd and ws are parameters to indicate the geometric
dependence of the reﬂectance, and they are actually deter-
mined by the angle between surface normal and incident

light direction. Both P Bi and P Gi are related to the in-
tensity of incident light. Also, P Bi and P Gi reﬂect dif-

fuse albedo and Fresnel reﬂectance, respectively [23, 42].

In color constancy, dichromatic model is used to separate

illuminant chromaticity, Γc, from reﬂected light.

Spatial Image Color Constancy Existing dichromatic
model based illuminant estimation methods can be classi-
ﬁed into line-based [14, 25, 38, 46] and plane-based [13, 35,
40, 41] methods. In these methods, they commonly gather
a group of distinct spatial pixels which are assumed to have
identical Λ and Γ, and estimate multiple lines or planes for
an illuminant estimation. If md (Λc − Γc) is constant on a
uniform surface, a dichromatic plane (which corresponds
to (1)) can be projected into a line.
In inverse intensity
chromaticity (IIC) space [38], RGB chromaticity (deﬁned
as σc = Ic
) can be represented by linear relationship be-
P Ii
tween inverse intensity of each channel and specular chro-
maticity, as expressed by

σc = pl

+ Γc

(3)

1

P Ic

where pl = md (Λc − Γc).
Inspired by IIC, Woo et al.
[46] proposed the inverse intensity red chromaticity (IIRC)
space. They show that the target specular chromaticity (cor-
responding to Γc in (3)) is on the line drawn by selected
specular pixels in IIRC space. These line-based methods
can reduce the complexity of dichromatic model by project-
ing 2D plane into a 1D line. However, they commonly re-
quire so many specular pixels on uniform surfaces because
md is assumed to be constant.
In other words, if illumi-
nant intensity varies or pixels from the surfaces with dis-

tinct geometry are selected together, they work poorly be-
cause these spatial pixels have different md. If pixels which
have different md are selected, the IIC model in (3) draws
a curved line rather than a straight one, and the curvedness
of the dichromatic line hinders accurate illuminant estima-
tion. Note that the direction of a dichromatic line domi-
nantly determines illuminant color, and even a marginal er-
ror in the line direction estimation can result in large dis-
crepancy to a ground truth illuminant. Furthermore, the di-
rection of a dichromatic line is susceptible to noises of sam-
ple data which can severely twisted it. Dichromatic plane
approaches have a similar problem with linear model be-
cause plane should be estimated from distinct spatial pixels
which are assumed to have identical diffuse and specular
chromaticities. It is difﬁcult to select those pixels, and it is
more complex and more sensitive to noise due to increased
dimension. Thus, it has not been actively researched re-
cently.

Temporal Image Color Constancy There have been pro-
posed few dichromatic based temporal color constancy
methods for image sequences [31, 48]. Their key con-
cept is that md in (1) does not change between adjacent
frames, based on the observation that incident illuminant in-
tensity and surface geometry are usually kept unchanged on
temporal domain (represented as md (t) = md (t + ∆t)).
Thus, by calculating the difference of pixel intensities of
two neighboring frames, specular chromaticity can be esti-
mated under the assumption that specular and diffuse chro-
maticity do not vary with time. This is given by

Γc =

Ic (t + ∆t) − Ic (t)
ms (t + ∆t) − ms (t)

=

∆Ic (t)
∆ms (t)

(4)

Even though ∆ms (t) is still unknown,

the specular
chromaticity Γc can be easily calculated by normalizing
∆Ic (t). This approach is simple and has merit of fast im-
plementation, but works poorly under less-specular because
pixels in less-specular regions have relatively low SNR.
Since ∆Ic (t) is commonly very small, noise can be a se-
rious obstacle, especially for noise-prone shooting environ-
ments such as low light and high frame rate (or short expo-
sure time), which results in low SNR.

The diffuse and specular weights in (1) are actually vary-
ing under typical indoor environments. To conﬁrm this
variations, we capture the white color checker with 150
fps. Then, diffuse and specular chromaticities are calcu-
lated from the color checker image sequence, based on the
prior knowledge that the white color checker is achromatic.
Dichromatic model, (1) can be re-written by the form of
matrix-vector product as:

IR
IG
IB




 = 



ΛR ΓR
ΛG ΓG
ΛB ΓB


(cid:18)md
ms(cid:19)

(5)

12331

Figure 3. Summary of the proposed method. We analyzed the minute temporal intensity variations captured by high-speed image, and
exploit to obtain accurate dichromatic plane for illuminant estimation.

By applying the pseudo inverse to (5), the variations of
md and ms can be easily estimated. As shown in Fig.
2, both md and ms are dynamic because they reﬂect the
time-varying intensity of the incident light. If geometry of
image pixel also varies with time by moving light source
or camera, they ﬂuctuate more dynamically. As far as we
know, these properties have been neglected so far, not be-
ing dealt with seriously. Based on this observation, the
proposed method considers md and ms as variable, and at-
tempts to estimate dichromatic plane by actively exploiting
time-varying AC light source.

are sinusoidally varying with time and contain much less
noise. And then, dichromatic plane is estimated per AC
pixel, based on its temporal intensity variations. Finally, a
set of candidate illuminant vectors are extracted by calcu-
lating the intersection of each dichromatic plane pairs. The
optimal illuminant is estimated using MAP estimation.

Note that the proposed method can be identically applied
to dynamic video using motion estimation as [31,48]. How-
ever, it is not dealt with in our paper because estimation
accuracy of dynamic video depends on the performance of
motion estimation.

3. The Proposed Method

Assuming static video, specular and diffuse chromatic-
ities are kept unchanged for entire video (unless the illu-
minant color is changed) in dichromatic model, (1). On
the other hand, the weights md and ms are varying with
time due to the varying intensity of AC incident light. Thus,
on temporal domain, RGB intensity vectors of a ﬁxed pixel
location are exist on identical dichromatic plane (which is
spanned by Λ and Γ), and their temporal locations are de-
termined by md and ms. We observe these temporal loca-
tion variations of a AC pixel, and exploit it to estimate the
accurate dichromatic plane. Multiple dichromatic planes
are estimated from a number of AC pixels, and the illumi-
nant color can be estimated from their intersections. How-
ever, it is challenging to estimate dichromatic plane accu-
rately due to inherent low light noise of high-speed image
caused by short exposure time. Thus, as shown in Fig.
3, we ﬁrst determine a set of AC pixels, whose intensities

3.1. Selection of AC Pixel

In this subsection, we determine a number of AC pix-
els from input high-speed frames to estimate a dichromatic
plane. An AC pixel is deﬁned by a pixel whose intensity
varies sinusoidally with time just like AC light variations.
If multiple observations of a pixel on temporal domain are
ﬁt into a sinusoidal curve, it becomes an AC pixel. A high-
speed image is inherently prone to noise very well due to
its short exposure time. By exploiting the periodic property
of an AC pixel, we can easily remove the effect of temporal
noise. This noise-free AC pixel plays a vital role to estimate
a dichromatic plane accurately.

The mean intensity of three RGB channels, which is ex-
pressed as Im = (IR + IG + IB) /3, can be modeled as
sinusoidal curve with additional offset. It is represented as

Im (t) ≈ f (t, β) = Am sin (4πfact/fcam + φ) + off (6)

where Am is the maximum variation of AC light, φ is phase,
fac is the frequency of AC current (e.g., typically 50 or 60

12332

High-speed images AC variation analysis/modeling Dichromatic plane estimation  per AC pixel Selected AC pixels Temporal analysis Dichromatic plane estimation 1t+tLight source estimation Illuminant estimation  & color correction Color correction Input image Corrected image Figure 4. A sinusoidal model ﬁt of an AC pixel. Note that iteration
converges with t < 0.03 seconds.

which is given by β = (cid:0)Am φ off(cid:1)T

Hz), fcam is capture frame rate, and off is a DC offset. β is
a collection of parameters in (6), and is a parameter vector
. Since f (t, β) is a
nonlinear function of t, β can be iteratively estimated using
Gauss-Newton method. Gauss-Newton method is used to
ﬁnd a minimum of a non-linear function, and has an advan-
tage in computation complexity because it does not require
second or higher order derivatives [45].

In our work, we aim to minimize the squared error be-
tween Im (t) and f (t, β), which is represented by r (β).
Thus, we select a β which minimize the squared error
r (β)T

r (β) as

ˆβ = argmin

r (β)T

r (β) = argmin

β

β

tN

Xt=t1

(Im (t) − f (t, β))2

(7)
where r is an error vector between Im (t) and f (t, β) from
t=t1 to tN . It is given by

r (β) = 


Im (t1) − f (t1, β)

...

Im (tN ) − f (tN , β)




(8)

The parameter vector β is iterativey updated by minimizing
rTr as shown in right-bottom graph in Fig. 4. In our diverse
experiments, β converges as fast as within 60 iterations.

Finally, the temporal error of a pixel is calculated to de-

termine a AC pixel for dichromatic plane estimation.

Figure 5. Examples of plane estimation. For temporal observa-
tions of the three test pixels (denoted by green circle) in the top-
left image, their dichromatic planes (green plane in the plot) are
estimated. Blue line denotes a RGB direction of each temporal
sample.
Irrespective of the extent of specularity, a dichromatic
plane is estimated well in that it contains most of blue lines.

where ˆβ (1) denote the ﬁrst element of ˆβ.

Note that the model ﬁt error in (9) should be normalized
by the estimated sinusoidal amplitude for fair comparison
irrespective of pixel intensity. These AC pixels can be de-
noised easily and accurately with a sinusoidal model, and it
can contribute to estimate more accurate dichromatic plane.
It is enough to guarantee that multiple observations of an
AC pixel are from the same surface (or reﬂectance). Thus,
it does not require any specularity on a pixel. Note that AC
pixels are automatically determined from an input image by
choosing a pixel with low ﬁtting error of (9).

3.2. Estimation of Dichromatic Plane

In previous subsection, a number of AC pixels are deter-
mined, and each AC pixel has N different observations at
distinct time. A dichromatic plane is estimated from these
N temporal observations for a given AC pixel (see Fig. 5).
For k-th AC pixel, its N observed RGB pixels are on the
same plane, Pk, based on the dichromatic model.
If the
RGB values of an AC pixel are denoted as x, y, z, the
dichromatic plane Pk is expressed by

Pk : akx + bky + ck = z

(10)

where vk = (ak bk ck)T
dichromatic plane Pk.

denotes the normal vector of the

To estimate the dichromatic plane, normal vector vk

should be ﬁrst determined optimally as follows:

ET =

r(cid:16) ˆβ(cid:17)T

r(cid:16) ˆβ(cid:17)

ˆβ (1) tN

(9)

ˆvk = argmin

vk

tN

Xt=t1

(zt − (akxt + bkyt + ck))2

(11)

12333

Figure 6. An test images extracted from videos in our laboratory
setting.

8

6

4

2

)
°
(
r
o
r
r
e
 
r
a
u
g
n
A

l

50 100

)
°
(
r
o
r
r
e
 
r
a
u
g
n
A

l

400

3

2.5

2

10

5
25
# of temporal observations

20

15

200

300

Frame rate

(a)

(b)

Figure 7. Angular error accuracy of the proposed method with (a)
various FPS and (b) number of temporal observations (150 fps).

where ˆvk is an optimal solution to be best-ﬁt.
square solution, is given by

Its least-

ˆv = (cid:0)ATA(cid:1)−1

ATb

(12)

where A and b are composed of temporal observations of
RGB intensities, and are given by

A =

xt1
xt2

xtN




yt1
yt2
...
ytN

1
1

1




b =

zt1
zt2
...
ztN







(13)

Then, the k-th plane error is calculated as

EP,k =

1

ImtN

(Aˆvk − b)T (Aˆvk − b)

(14)

Note that in (14), least-square error is divided by mean
intensity Im, to neglect the intensity level of a pixel. We
discard the planes which have high EP , because they are
likely to have many noises.

3.3. Illuminant Estimation

Using the plane error measured of (14), Np accurate
dichromatic planes are selected among multiple candidates.
Note that a single dichromatic plane is derived per each AC
pixel. Theoretically, all dichromatic planes should share a
common intersection. However, all dichromatic planes may
not meet at a ﬁxed point actually due to noise and the ac-
curacy of dichromatic model. Thus, we ﬁrst calculate an
intersection for a pair of dichromatic planes, and this is for
all combinations, Np C2. In other words, Np C2 number of
candidate illuminants is estimated. In this subsection, we
propose a Bayesian framework to determine an optimal il-
luminant from Np C2 intersections.

To obtain an optimal illuminant from dichromatic planes,
we adopts a MAP. Given all estimated planes, P, the opti-
mal illuminant should maximize the posteriori probability
p (Γ|P), and posteriori probability can be decomposed into
the product of likelihood and prior probability as

ˆΓ = argmax

p (Γ|P) = argmax

Γ

Γ

Np

Yk=1

p (Pk|Γ) p (Γ) (15)

Since all dichromatic planes estimated for an input scene
should share a common illuminant vector, Γ, we can obtain
the illuminant by calculating the intersection of dichromatic
planes. In our work, an optimal illuminant is estimated from
NL (which is equal to Np C2) plausible candidate illumi-
nants. Then, (15) can be re-written as

ˆΓ =

argmax

Γi∈{Γ1,Γ2,··· ,ΓNL }

Np

Yk=1

p (Pk|Γi) p (Γi)

(16)

Taking logarithm to the right side of (16), it is rewritten by:

ˆΓ =

argmax

Γi∈{Γ1,Γ2,··· ,ΓNL }

Np

Xk=1

ln p (Pk|Γi) + Np ln p (Γi)

(17)
ˆΓ is estimated by maximizing the posterior probability of
the illuminant Γ given multiple observations of dichro-
matic planes. Inspired by the previous combinational meth-
ods which exploit both physical and statistical properties
[34, 35], the proposed framework includes both properties.
For likelihood probability, it should reﬂect the relation-
ship between candidate illuminant and the plane. Given Γi,
the accuracy of k-th plane Pk can be measured by the an-
gle between Γi and the normal vector of a chromatic plane,
vk. Ideally, both directions should be orthogonal, and their
angular error is converted into the probability as follows

p (Pk|Γi) = exp(cid:18)−

1

Ep,k

cos(cid:18) ˆvk · Γi

kˆvkkkΓik(cid:19)(cid:19)

(18)

where the weight is controlled by plane error Ep,k in (14).
Prior information in the MAP framework typically in-
cludes the ideal property of estimate. Planckian locus is
adopted as a prior of illuminant. The possible illuminants
in the real world are commonly well-represented by black
body radiators. In chromaticity space, black body radiators
make a locus according to the varying temperature, which
is called Planckian locus [22, 28]. A candidate illuminant
is orthogonally projected into Planckian locus on the CIE
1960 uv chromaticity space, and its Euclidean distance is
converted into the prior probability as follows

p (Γi) = exp(cid:18)−

duv (Γi)

λb (cid:19)

(19)

12334

Table 1. Angular error comparisons with various single image methods for specular video (150 fps).

Statistics-based

Gamut-based

Physics-based

Method

Gray world
Max-RGB [24]
Shades of gray [15]
1st order grey edge [43]
2nd order grey edge [43]
Grey pixels [47]
Pixel gamut [16]
1st order gradient gamut [18]
2nd order gradient gamut [18]
IIC [38]
CLS [25]
ICC [46]
Proposed

Mean Median
4.87
16.42
5.11
9.51
14.27
7.99
8.19
6.39
7.84
7.22
9.01
3.78
2.47

4.50
15.14
3.39
9.24
14.94
6.98
8.06
5.82
8.18
6.91
8.55
2.15
1.93

Trimean Best-25% Worst-25%

5.46
17.40
6.92
11.35
16.11
9.03
9.00
7.11
8.53
7.87
10.12
6.61
3.12

2.22
9.65
2.09
2.42
5.15
4.58
3.84
3.18
3.30
3.73
3.48
0.79
0.50

8.26
24.45
10.87
17.88
23.91
13.20
13.31
10.63
11.72
11.39
15.39
10.12
5.12

white of a color checker. Angular error e is used to evaluate
the quantitative performance and is given by

e = arccos


Γg · ˆΓ

kΓgk(cid:13)(cid:13)(cid:13)

ˆΓ(cid:13)(cid:13)(cid:13)

where Γg is a ground truth chromaticity.




(20)

Figure 8. Visual results of the proposed method with various light
sources.

where λb is a smoothing parameter, and duv (Γi) is Eu-
clidean projection distance between Γi and Planckian locus
on CIE 1960 uv chromaticity space.

We aim to ﬁnd a optimal illuminant which minimizes the
MAP estimation of (17) from candidate illuminants. It is
worth noting that λb controls the importance between like-
lihood and prior.

4. Experimental Results

To evaluate the proposed method, we produced 80 high-
speed raw videos, which were captured using Sentech STC-
MCS43U3V high-speed vision camera. Exposure time is
set to half the number of frames per second, which is usu-
ally used (e.g. 1/150 sec for 75 fps). Each raw frame is nor-
malized and demosaiced to apply color constancy. To con-
vince that the proposed method works well irrespective of
the amount of specularity, the test video sequences are cate-
gorized into specular (55% of total) and non-specular (45%
of total) by the extent of specularity in videos. They are
composed of various natural objects such as plastic, textile,
metal, rubber, stones, and fruit (see Fig. 6). We regularly se-
lect 60 AC pixels among all to estimate a dichromatic plane
(Np=60). The ground truth chromaticity of illuminant is
calculated by averaging the chromaticities of the reference

Performance with Frame Rate To capture the fast AC
variations of illuminant, the capture frame rate should ex-
ceed the frequency of illuminant. To avoid aliasing, it
should be twice the Nyquist frequency for the case of un-
known fac [27, 32]. However, it can be conﬁgured below
the Nyquist frequency under the assumption of known fac.
Fig. 7 (a) shows the angular error accuracy with various fps.
When the frame rate is lower then 100 fps, the angular error
of the proposed method is rather high because the variation
of illuminant intensity becomes indistinguishable by insuf-
ﬁcient number of samples. When the frame rate is over 100,
the performance of the proposed method increased rapidly.
We can see that the angular error performance is saturated
when the frame rate is over 200 fps, which is still below the
Nyquist frequency (240 Hz).

Fig.

7 (b) shows the angular error of the proposed
method with the number of temporal observations. With the
number of temporal observation, the accuracy of the pro-
posed method increases until saturated. Note that the 15
temporal observations are made for 0.1 sec with 150 fps.

Comparisons with Single Image Methods To represent
the video performance of single image methods, we sam-
pled 10 frames from video regularly. Then, their angular
errors are averaged to represent a video performance. Since
an uniform AC light source is considered, our experimental
setup is wrapped up with matte cloth to avoid the effect of
external ambient light. The number of temporal observa-
tions is 100, which is less than 1 sec. We compare the pro-
posed method to several state-of-the-art methods including

12335

Table 2. Angular error comparisons with various single image methods for non-specular video (150 fps).

Statistics-based

Gamut-based

Physics-based

Method

Gray world
Max-RGB [24]
Shades of gray [15]
1st order grey edge [43]
2nd order grey edge [43]
Grey pixels [47]
Pixel gamut [16]
1st order gradient gamut [18]
2nd order gradient gamut [18]
IIC [38]
CLS [25]
ICC [46]
Proposed

Mean Median
4.20
11.76
3.82
4.07
5.43
6.32
6.75
11.74
12.37
9.03
9.01
4.16
2.58

3.24
12.27
2.92
2.69
3.86
5.79
5.67
12.58
12.76
8.22
6.20
3.68
2.37

Trimean Best-25% Worst-25%

4.99
12.86
4.60
5.68
7.35
6.75
8.16
12.46
12.72
9.95
11.26
5.40
3.00

1.61
5.22
1.47
1.34
2.17
3.76
1.89
6.59
8.41
4.47
2.60
1.06
0.94

8.16
18.51
7.25
9.03
11.80
9.51
13.19
15.85
15.63
14.47
18.43
8.18
4.60

statistics-based, gamut-based, and physics-based methods.
We classify our test videos into specular and non-specular
videos, and evaluate each separately. Table 1 and 2 compare
the proposed method with existing color constancy methods
for a single image. The proposed method outperforms all
existing methods. Especially, the performance of the pro-
posed method is outstanding with worst-25% angular error,
which is almost half of the second best methods. Compared
to existing methods, the proposed method works well with
specular as well as non-specular image.

Comparisons with Temporal Method The proposed
method is compared with existing temporal method, which
is dichromatic model in (4) (introduced in [31,48]). To con-
duct fair comparisons, 60 identical pixels are used, and prin-
cipal component analysis is exploited to estimate illuminant
in temporal method of (4). Table 3 summarizes the angular
error performance of the proposed method. The proposed
method shows the lower angular error, which is attributed
to noise-robust characteristics of the proposed method.

Evaluation with various light sources We evaluate the
proposed method with various combinations of
light
sources (as shown in Fig. 8) and the public DELIGHT [37].
Our ﬁtting model in (6) are applied to both our experiment
setting and DELIGHT. For evaluation with DELIGHT, its
bulb response values are resampled to 60 Hz (originally 50
Hz). Note that the amplitude of sinusoidal variations are
normalized to 1 for fair comparison. Table 4 shows that our
model in (6) is well-ﬁt to both data, independent of bulb
types. Visual results of the proposed method are evaluated
as shown in Fig. 8. With various environments under mixed
AC light or ambient light, the proposed method works well.

5. Conclusions

We proposed a novel temporal color constancy method
for AC light sources. Under AC light source, the intensities

Table 3. Angular error comparisons with temporal method for
high-speed video (150 fps).

Specular

Non-specular

Method

Mean Median Mean Median

Temporal
Proposed

5.42
2.47

5.02
1.93

6.55
2.58

3.94
2.37

Table 4. Mean squared error of our ﬁtting method with DELIGHT
dataset (top) and our experiment setting (bottom).

Data

DELIGHT

Ours

Incan

0.00001
0.00026

Fluor

0.00013
0.00047

LED

0.00045
0.00021

of a pixel (named by AC pixel) is time-varying with a fre-
quency identical to AC electric power. This periodic time-
varying property helps a dichromatic plane to be estimated
accurately. A variety of experiments show that the pro-
posed method can estimate an illuminant accurately, leading
to better color constancy. Also, the proposed method out-
performs existing dichromatic based methods and tempo-
ral color constancy methods, irrespective of the amount of
specularity. In this paper, only a static scene is considered
for experiments in order to avoid the effect of object mo-
tion. It would be possible to compensate motion by align-
ment between neighboring frames. It may take some time
to acquire temporal pixel samples for dichromatic plane es-
timation, and this may be constrained by fast object motion.
Higher speed capability can decrease the time length of AC
pixels, resulting in less sensitive to object motion. It will be
further investigated thoroughly.

6. Acknowledgment

This work is supported by Samsung Electronics,
and the National Research Foundation of Korea (NRF)
grant funded by the Korea government (MSIT) (No.
2019R1A2C1005834).

12336

References

[1] Kobus Barnard, Vlad Cardei, and Brian Funt. A comparison
of computational color constancy algorithms. i: Methodol-
ogy and experiments with synthesized data. IEEE transac-
tions on Image Processing, 11(9):972–984, 2002.

[2] Jonathan T Barron. Convolutional color constancy. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 379–387, 2015.

[3] Jonathan T. Barron and Yun-Ta Tsai. Fast fourier color con-
In The IEEE Conference on Computer Vision and

stancy.
Pattern Recognition (CVPR), July 2017.

[4] Simone Bianco, Claudio Cusano, and Raimondo Schettini.
Single and multiple illuminant estimation using convolu-
tional neural networks.
IEEE Transactions on Image Pro-
cessing, 26(9):4347–4362, 2017.

[5] Jacopo Bonato, Luigi M Gratton, Pasquale Onorato, and
Stefano Oss. Using high speed smartphone cameras and
video analysis techniques to teach mechanical wave physics.
Physics Education, 52(4):045017, 2017.

[6] David H Brainard and William T Freeman. Bayesian color

constancy. JOSA A, 14(7):1393–1411, 1997.

[7] Gershon Buchsbaum. A spatial processor model for ob-
Journal of the Franklin institute,

ject colour perception.
310(1):1–26, 1980.

[8] Ayan Chakrabarti. Color constancy by learning to predict
chromaticity from luminance. In Advances in Neural Infor-
mation Processing Systems, pages 163–171, 2015.

[9] Ayan Chakrabarti, Keigo Hirakawa, and Todd Zickler. Color
constancy with spatio-spectral statistics. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 34(8):1509–
1519, 2012.

[10] Marc Ebner. Color constancy based on local space aver-
age color. Machine Vision and Applications, 20(5):283–301,
2009.

[11] Graham D Finlayson and Steven D Hordley. Color constancy

at a pixel. JOSA A, 18(2):253–264, 2001.

[12] Graham D Finlayson, Paul M Hubel, and Steven Hordley.
Color by correlation.
In Color and Imaging Conference,
volume 1997, pages 6–11. Society for Imaging Science and
Technology, 1997.

[13] Graham D Finlayson and Gerald Schaefer. Convex and non-
convex illuminant constraints for dichromatic colour con-
stancy. In Computer Vision and Pattern Recognition, 2001.
CVPR 2001. Proceedings of the 2001 IEEE Computer Soci-
ety Conference on, volume 1, pages I–I. IEEE, 2001.

[14] Graham D Finlayson and Gerald Schaefer.

Solving for
colour constancy using a constrained dichromatic reﬂec-
tion model.
International Journal of Computer Vision,
42(3):127–144, 2001.

[15] Graham D Finlayson and Elisabetta Trezzi. Shades of gray
and colour constancy.
In Color and Imaging Conference,
volume 2004, pages 37–41. Society for Imaging Science and
Technology, 2004.

[16] David A Forsyth. A novel algorithm for color constancy.
International Journal of Computer Vision, 5(1):5–35, 1990.
[17] Arjan Gijsenij and Theo Gevers. Color constancy using natu-
ral image statistics and scene semantics. IEEE Transactions

on Pattern Analysis and Machine Intelligence, 33(4):687–
698, 2011.

[18] Arjan Gijsenij, Theo Gevers, and Joost Van De Weijer. Gen-
eralized gamut mapping using image derivative structures for
color constancy. International Journal of Computer Vision,
86(2-3):127–139, 2010.

[19] Arjan Gijsenij, Theo Gevers, Joost Van De Weijer, et al.
Computational color constancy: Survey and experiments.
IEEE Transactions on Image Processing, 20(9):2475–2489,
2011.

[20] David John Grifﬁths. Developmemt of High Speed High Dy-
namic Range Videography. PhD thesis, Virginia Tech, 2017.
[21] Yuanming Hu, Baoyuan Wang, and Stephen Lin. Fc4: Fully
convolutional color constancy with conﬁdence-weighted
pooling. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.

[22] Deane B Judd, David L MacAdam, G¨unter Wyszecki, HW
Budde, HR Condit, ST Henderson, and JL Simonds. Spec-
tral distribution of typical daylight as a function of correlated
color temperature. Josa, 54(8):1031–1040, 1964.

[23] Johann Heinrich Lambert. Photometria sive de mensura et

gradibus luminis, colorum et umbrae. Klett, 1760.

[24] Edwin H Land. The retinex theory of color vision. Scientiﬁc

american, 237(6):108–129, 1977.

[25] Thomas M Lehmann and Christoph Palm. Color line search
JOSA A,

for illuminant estimation in real-world scenes.
18(11):2679–2691, 2001.

[26] Laurence T Maloney and Brian A Wandell. Color con-
stancy: a method for recovering surface spectral reﬂectance.
In Readings in Computer Vision, pages 293–297. Elsevier,
1987.

[27] Mrinal Mandal and Amir Asif. Continuous and Discrete
Time Signals and Systems with CD-ROM. Cambridge Uni-
versity Press, 2007.

[28] Baptiste Mazin, Julie Delon, and Yann Gousseau. Estimation
of illuminants from projections on the planckian locus. IEEE
Transactions on Image Processing, 24(6):1944–1955, 2015.
[29] John J McCann, Suzanne P McKee, and Thomas H Tay-
lor. Quantitative studies in retinex theory a comparison be-
tween theoretical predictions and observer responses to the
color mondrian experiments. Vision research, 16(5):445–
IN3, 1976.

[30] Christa Neumeyer. On perceived colors. Behavioral and

Brain Sciences, 15(1):49–49, 1992.

[31] Veronique Prinet, Dani Lischinski, and Michael Werman. Il-
luminant chromaticity from image sequences. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 3320–3327, 2013.

[32] John G Proakis. Digital signal processing: principles algo-

rithms and applications. Pearson Education India, 2001.

[33] Yanlin Qian, Ke Chen, Jarno Nikkanen, Joni-Kristian Kama-
rainen, and Jiri Matas. Recurrent colour constancy. In Proc.
IEEE Int. Conf. on Computer Vision (ICCV), Venice, Italy,
22–29 October 2017, 2017.

[34] Yanlin Qian, Ke Chen, Jarno Nikkanen, Joni-Kristian
Dichromatic gray pixel
arXiv preprint

K¨am¨ar¨ainen, and Jiri Matas.
for camera-agnostic color constancy.
arXiv:1803.08326, 2018.

12337

[35] Gerald Schaefer. Robust dichromatic colour constancy. In
International Conference Image Analysis and Recognition,
pages 257–264. Springer, 2004.

[36] Steven A Shafer. Using color to separate reﬂection compo-
nents. Color Research & Application, 10(4):210–218, 1985.
[37] Mark Sheinin, Yoav Y Schechner, and Kiriakos N Kutulakos.
Computational imaging on the electric grid.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 6437–6446, 2017.

[38] Robby T Tan, Katsushi Ikeuchi, and Ko Nishino. Color con-
stancy through inverse-intensity chromaticity space. In Dig-
itally Archiving Cultural Objects, pages 323–351. Springer,
2008.

[39] Shoji Tominaga and Brian A Wandell. Natural scene-
illuminant estimation using the sensor correlation. Proceed-
ings of the IEEE, 90(1):42–56, 2002.

[40] Javier Toro. Dichromatic illumination estimation without
pre-segmentation. Pattern Recognition Letters, 29(7):871–
877, 2008.

[41] Javier Toro and Brian Funt. A multilinear constraint on
dichromatic planes for illumination estimation. IEEE Trans-
actions on Image Processing, 16(1):92–97, 2007.

[42] Joost Van De Weijer and Shida Beigpour. The dichromatic
reﬂection model-future research directions and applications.
[43] Joost Van De Weijer, Theo Gevers, and Arjan Gijsenij. Edge-
based color constancy. IEEE Transactions on image process-
ing, 16(9):2207–2214, 2007.

[44] Michael Vollmer and Klaus-Peter M¨ollmann. Flickering

lamps. European Journal of Physics, 36(3):035027, 2015.

[45] P ˚A Wedin. On the gauss-newton method for the nonlinear
least-squares problems. Institute for Applied Mathematics,
Stockolm, Sweden, Working Paper, 24, 1974.

[46] Sung-Min Woo, Sang-Ho Lee, Jun-Sang Yoo, and Jong-Ok
Kim. Improving color constancy in an ambient light environ-
ment using the phong reﬂection model. IEEE Transactions
on Image Processing, 27(4):1862–1877, 2018.

[47] Kai-Fu Yang, Shao-Bing Gao, and Yong-Jie Li. Efﬁcient
illuminant estimation for color constancy using grey pixels.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2254–2263, 2015.

[48] Qingxiong Yang, Shengnan Wang, Narendra Ahuja, and
Ruigang Yang. A uniform framework for estimating illumi-
nation chromaticity, correspondence, and specular reﬂection.
IEEE Transactions on Image Processing, 20(1):53–63, 2011.

12338

