Improving Transferability of Adversarial Examples with Input Diversity

Cihang Xie1

Zhishuai Zhang1

Yuyin Zhou1

Song Bai2

Jianyu Wang3

Zhou Ren4

Alan Yuille1

1Johns Hopkins University

2University of Oxford

3Baidu Research

4Wormpex AI Research

Abstract

Though CNNs have achieved the state-of-the-art perfor-
mance on various vision tasks, they are vulnerable to adver-
sarial examples — crafted by adding human-imperceptible
perturbations to clean images. However, most of the ex-
isting adversarial attacks only achieve relatively low suc-
cess rates under the challenging black-box setting, where
the attackers have no knowledge of the model structure and
parameters. To this end, we propose to improve the trans-
ferability of adversarial examples by creating diverse in-
put patterns. Instead of only using the original images to
generate adversarial examples, our method applies random
transformations to the input images at each iteration. Ex-
tensive experiments on ImageNet show that the proposed at-
tack method can generate adversarial examples that trans-
fer much better to different networks than existing base-
lines. By evaluating our method against top defense so-
lutions and ofﬁcial baselines from NIPS 2017 adversarial
competition, the enhanced attack reaches an average suc-
cess rate of 73.0%, which outperforms the top-1 attack sub-
mission in the NIPS competition by a large margin of 6.6%.
We hope that our proposed attack strategy can serve as a
strong benchmark baseline for evaluating the robustness of
networks to adversaries and the effectiveness of different de-
fense methods in the future. Code is available at https:
//github.com/cihangxie/DI-2-FGSM .

1. Introduction

Recent success of Convolutional Neural Networks
(CNNs) leads to a dramatic performance improvement on
various vision tasks, including image classiﬁcation [15, 32,
13], object detection [10, 28, 40] and semantic segmen-
tation [22, 5]. However, CNNs are extremely vulnerable
to small perturbations to the input images, i.e., human-
imperceptible additive perturbations can result in failure
predictions of CNNs. These intentionally crafted images
are known as adversarial examples [36]. Learning how to
generate adversarial examples can help us investigate the

n
a
e
C

l

M
S
G
F

M
S
G
F
-
I

M
S
G
F
-
2
I
D

Inception-v3

Inception-v4

Inception-Resnet-v2

Resnet-v2-152

walking stick

walking stick

walking stick

yellow lady-slipper

armadillo

three-toed sloth

mantis

rapeseed

capuchin

green lizard

little blue heron

mantis

pot

green lizard

vine snake

leopard

jaguar

cheetah

snow leopard

diamondback rattlesnake

Egyptian cat

running shoe

screwdriver

snow leopard

nipple

Egyptian cat

snow leopard

running shoe

cheetah

leopard

American alligator

walking stick

Komodo dragon

cat bear

leopard

bullfrog

pot

red fox

cat bear

armadillo

American alligator

walking stick

water snake

pot

terrapin

European gallinule

mud turtle

bullfrog

leopard

jaguar

Egyptian cat

tiger cat

snow leopard

green lizard

vine snake

lynx

leopard

tiger cat

jaguar

cheetah

walking stick

vine snake

pot

mantis

picket fence

pot

red fox

American alligator

cat bear

proboscis monkey

walking stick

vine snake

pot

mantis

green mamba

leopard

jaguar

cheetah

snow leopard

black bear

Figure 1. The comparison of success rates using three different
attacks. The ground-truth “walking stick” is marked as pink in the
top-5 conﬁdence distribution plots. The adversarial examples are
crafted on Inception-v3 with the maximum perturbation ǫ = 15.
From the ﬁrst row to the the third row, we plot the top-5 conﬁdence
distributions of clean images, FGSM and I-FGSM, respectively.
The fourth row shows the result of the proposed Diverse Inputs
Iterative Fast Gradient Sign Method (DI2-FGSM), which attacks
the white-box model and all black-box models successfully.

robustness of different models [1] and understand the insuf-
ﬁciency of current training algorithms [11, 17, 37].

Several methods [11, 36, 16] have been proposed re-
In general, these at-
cently to ﬁnd adversarial examples.
tacks can be categorized into two types according to the
number of steps of gradient computation, i.e., single-step
attacks [11] and iterative attacks [36, 16]. Generally, itera-
tive attacks can achieve higher success rates than single-step
attacks in the white-box setting, where the attackers have
a perfect knowledge of the network structure and weights.
However, if these adversarial examples are tested on a dif-
ferent network (either in terms of network structure, weights
or both), i.e., the black-box setting, single-step attacks per-
form better. This trade-off is due to the fact that iterative
attacks tend to overﬁt the speciﬁc network parameters (i.e.,
have high white-box success rates) and thus making gener-
ated adversarial examples rarely transfer to other networks
(i.e., have low black-box success rates), while single-step
attacks usually underﬁt to the network parameters (i.e., have

2730

low white-box success rates) thus producing adversarial ex-
amples with slightly better transferability. Observing the
phenomenon, one interesting question is whether we can
generate adversarial examples with high success rates un-
der both white-box and black-box settings.

In this work, we propose to improve the transferability of
adversarial examples by creating diverse input patterns. Our
work is inspired by the data augmentation [15, 32, 13] strat-
egy, which has been proven effective to prevent networks
from overﬁtting by applying a set of label-preserving trans-
formations (e.g., resizing, cropping and rotating) to training
images. Meanwhile, [38, 12] showed that image transfor-
mations can defend against adversarial examples under cer-
tain situations, which indicates adversarial examples can-
not generalize well under different transformations. These
transformed adversarial examples are known as hard exam-
ples [30, 31] for attackers, which can then be served as good
samples to produce more transferable adversarial examples.
We incorporate the proposed input diversity strategy
with iterative attacks, e.g., I-FGSM [17] and MI-FGSM [9].
At each iteration, unlike the traditional methods which max-
imize the loss function directly w.r.t. the original inputs, we
apply random and differentiable transformations (e.g., ran-
dom resizing, random padding) to the input images with
probability p and maximize the loss function w.r.t.
these
transformed inputs. Note that these randomized operations
were previously used to defend against adversarial exam-
ples [38], while here we incorporate them into the attack
process to create hard and diverse input patterns. Fig. 1
shows an adversarial example generated by our method and
compares the success rates to other attack methods under
both white-box and black-box settings.

We test the proposed input diversity on several network
under both white-box and black-box settings, and single-
model and multi-model settings. Compared with traditional
iterative attacks, the results on ImageNet (see Sec. 4.2)
show that our method gets signiﬁcantly higher success rates
for black-box models and maintains similar success rates
for white-box models. By evaluating our attack method
w.r.t. the top defense solutions and ofﬁcial baselines from
NIPS 2017 adversarial competition [18], this enhanced at-
tack reaches an average success rate of 73.0%, which out-
performs the top-1 attack submission in the NIPS competi-
tion by a large margin of 6.6%. We hope that our proposed
attack strategy can serve as a benchmark for evaluating the
robustness of networks to adversaries and the effectiveness
of different defense methods in future.

2. Related Work

2.1. Generating Adversarial Examples

Traditional machine learning algorithms are known to
be vulnerable to adversarial examples [7, 14, 3]. Recently,

Szegedy et al. [36] pointed out that CNNs are also fragile
to adversarial examples, and proposed a box-constrained L-
BFGS method to ﬁnd adversarial examples reliably. Due to
the expensive computation in [36], Goodfellow et al. [11]
proposed the fast gradient sign method to generate adver-
sarial examples efﬁciently by performing a single gradient
step. This method was extended by Kurakin et al. [16] to an
iterative version, and showed that the generated adversarial
examples can exist in the physical world. Dong et al. [9]
proposed a broad class of momentum-based iterative algo-
rithms to boost the transferability of adversarial examples.
The transferability can also be improved by attacking an
ensemble of networks simultaneously [21]. Besides image
classiﬁcation, adversarial examples also exist in object de-
tection [39], semantic segmentation [39, 6], speech recog-
nition [6], deep reinforcement learning [20], etc.. Unlike
adversarial examples which can be recognized by human,
Nguyen et al. [25] generated fooling images that are dif-
ferent from natural images and difﬁcult for human to recog-
nize, but CNNs classify these images with high conﬁdences.
Our proposed input diversity is also related to EOT [2].
These two works differ in several aspects: (1) we mainly fo-
cus on the challenging black-box setting while [2] focuses
on the white-box setting; (2) our work aims at alleviating
overﬁtting in adversarial attacks, while [2] aims at making
adversarial examples robust to transformations, without any
discussion of overﬁtting; and (3) we do not apply expecta-
tion step in each attack iteration, while “expectation” is the
core idea in [2].

2.2. Defending Against Adversarial Examples

Conversely, many methods have been proposed recently
to defend against adversarial examples. [11, 17] proposed to
inject adversarial examples into the training data to increase
the network robustness. Tram`er et al. [37] pointed out that
such adversarially trained models still remain vulnerable to
adversarial examples, and proposed ensemble adversarial
training, which augments training data with perturbations
transferred from other models, in order to improve the net-
work robustness further. [38, 12] utilized randomized im-
age transformations to inputs at inference time to mitigate
adversarial effects. Dhillon et al. [8] pruned a random sub-
set of activations according to their magnitude to enhance
network robustness. Prakash et al. [27] proposed a frame-
work which combines pixel deﬂection with soft wavelet de-
noising to defend against adversarial examples. [24, 33, 29]
leveraged generative models to purify adversarial images by
moving them back towards the distribution of clean images.

3. Methodology

Let X denote an image, and ytrue denote the correspond-
ing ground-truth label. We use θ to denote the network pa-
rameters, and L(X, ytrue; θ) to denote the loss. To generate

2731

the adversarial example, the goal is to maximize the loss
L(X + r, ytrue; θ) for the image X, under the constraint that
the generated adversarial example X adv = X + r should
look visually similar to the original image X and the corre-
sponding predicted label yadv 6= ytrue. In this work, we use
l∞-norm to measure the perceptibility of adversarial pertur-
bations, i.e., ||r||∞ ≤ ǫ. The loss function is deﬁned as

L(X, ytrue; θ) = −✶ytrue · log (softmax(l(X; θ))) ,

(1)

where ✶ytrue is the one-hot encoding of the ground-truth
ytrue and l(X; θ) is the logits output. Note that all the
baseline attacks have been implemented in the cleverhans
library [26], which can be used directly for our experiments.

3.1. Family of Fast Gradient Sign Methods

In this section, we give an overview of the family of fast

gradient sign methods.

Fast Gradient Sign Method (FGSM). FGSM [11] is the
ﬁrst member in this attack family, which ﬁnds the adver-
sarial perturbations in the direction of the loss gradient
∇X L(X, ytrue; θ). The update equation is

X adv = X + ǫ · sign(∇X L(X, ytrue; θ)).

(2)

Iterative Fast Gradient Sign Method (I-FGSM). Ku-
rakin et al. [17] extended FGSM to an iterative version,
which can be expressed as

X adv
X adv

0 = X
n+1 = Clipǫ

X(cid:8)X adv

n + α · sign(∇X L(X adv

(3)

n , ytrue; θ))(cid:9) ,

where Clipǫ
X indicates the resulting image are clipped
within the ǫ-ball of the original image X, n is the iteration
number and α is the step size.

Momentum Iterative Fast Gradient Sign Method (MI-
FGSM). MI-FGSM [9] proposed to integrate the momen-
tum term into the attack process to stabilize update direc-
tions and escape from poor local maxima. The updating
procedure is similar to I-FGSM, with the replacement of
Eq. (3) by:

gn+1 = µ · gn +

∇X L(X adv
||∇X L(X adv

n , ytrue; θ)
n , ytrue; θ)||1

(4)

X adv

n+1 = Clipǫ

X(cid:8)X adv

n + α · sign(gn+1)(cid:9) ,

where µ is the decay factor of the momentum term and gn
is the accumulated gradient at iteration n.

3.2. Motivation

general,

Let ˆθ denote the unknown network parameters.
should
In
have high success
rates on both white-box mod-
els, i.e., L(X adv, ytrue; θ) > L(X, ytrue; θ), and black-box

adversarial

example

strong

a

models, i.e., L(X adv, ytrue; ˆθ) > L(X, ytrue; ˆθ). On one
hand, the traditional single-step attacks, e.g., FGSM, tend
to underﬁt to the speciﬁc network parameters θ due to
inaccurate linear appropriation of the loss L(X, ytrue; θ),
thus cannot reach high success rates on white-box models.
On the other hand, the traditional iterative attacks, e.g., I-
FGSM, greedily perturb the images in the direction of the
sign of the loss gradient ∇X L(X, ytrue; θ) at each iteration,
and thus easily fall into the poor local maxima and overﬁt
to the speciﬁc network parameters θ. These overﬁtted
adversarial examples rarely transfer to black-box models.
In order to generate adversarial examples with strong
transferability, we need to ﬁnd a better way to optimize the
loss L(X, ytrue; θ) to alleviate this overﬁtting phenomenon.
Data augmentation [15, 32, 13] is shown as an effective
way to prevent networks from overﬁtting during the train-
ing process. Meanwhile, [38, 12] showed that adversarial
examples are no longer malicious if simple image trans-
formations are applied, which indicates these transformed
adversarial images can serve as good samples for better op-
timization. Those facts inspire us to apply random and dif-
ferentiable transformations to the inputs for the sake of the
transferability of adversarial examples.

3.3. Diverse Input Patterns

Based on the analysis above, we aim at generating more
transferable adversarial examples via diverse input patterns.

DI2-FGSM. First, we propose the Diverse Inputs Iterative
Fast Gradient Sign Method (DI2-FGSM), which applies im-
age transformations T (·) to the inputs with the probability p
at each iteration of I-FGSM [17] to alleviate the overﬁtting
phenomenon.

In this paper, we consider random resizing, which resizes
the input images to a random size, and random padding,
which pads zeros around the input images in a random man-
ner [38], as the instantiation of the image transformations
T (·)1. The transformation probability p controls the trade-
off between success rates on white-box models and success
rates on black-box models, which can be observed from
Fig. 4. If p = 0, DI2-FGSM degrades to I-FGSM and leads
to overﬁtting. If p = 1, i.e., only transformed inputs are
used for the attack, the generated adversarial examples tend
to have much higher success rates on black-box models but
lower success rates on white-box models, since the original
inputs are not seen by the attackers.

In general, the updating procedure of DI2-FGSM is sim-

ilar to I-FGSM, with the replacement of Eq. (3) by

X adv

n+1 = Clip

ǫ

X {X

adv

n + α · sign(cid:16)∇X L(T (X

adv

n ; p), ytrue; θ)(cid:17)},
(5)

1We have also experimented with other image transformations, e.g.,
rotation or ﬂipping, to create diverse input patterns, and found random re-
sizing & padding yields adversarial examples with the best transferability.

2732

FGSM

I-FGSM

MI-FGSM

N=1

μ=0

where lk(X; θk) is the logits output of the k-th model with
the parameters θk, wk is the ensemble weight with wk ≥ 0

p=0

p=0

K

DI2-FGSM

μ=0

M-DI2-FGSM

and

wk = 1.

Pk=1

4. Experiment

Figure 2. Relationships between different attacks. By setting
setting values of the transformation probability p, the decay factor
µ and the total iteration number N , we can relate these different
attacks in the family of Fast Gradient Sign Methods.

where the stochastic transformation function T (X adv

n ; p) is

T (X adv

n ; p) =(T (X adv

X adv

n

n ) with probability p

with probability 1 − p

. (6)

M-DI2-FGSM.
Intuitively, momentum and diverse inputs
are two completely different ways to alleviate the overﬁt-
ting phenomenon. We can combine them naturally to form
a much stronger attack, i.e., Momentum Diverse Inputs Iter-
ative Fast Gradient Sign Method (M-DI2-FGSM). The over-
all updating procedure of M-DI2-FGSM is similar to MI-
FGSM, with the only replacement of Eq. (4) by

gn+1 = µ · gn +

∇X L(T (X adv
||∇X L(T (X adv

n ; p), ytrue; θ)
n ; p), ytrue; θ)||1

.

(7)

3.4. Relationships between Different Attacks

The attacks mentioned above all belong to the family of
Fast Gradient Sign Methods, and they can be related via dif-
ferent parameter settings as shown in Fig. 2. To summarize,

• If the transformation probability p = 0, M-DI2-FGSM
degrades to MI-FGSM, and DI2-FGSM degrades to I-
FGSM.

• If the decay factor µ = 0, M-DI2-FGSM degrades to

DI2-FGSM, and MI-FGSM degrades to I-FGSM.

• If the total iteration number N = 1, I-FGSM degrades

to FGSM.

3.5. Attacking an Ensemble of Networks

Liu et al. [21] suggested that attacking an ensemble
of multiple networks simultaneously can generate much
stronger adversarial examples. The motivation is that if
an adversarial image remains adversarial for multiple net-
works, then it is more likely to transfer to other networks
as well. Therefore, we can use this strategy to improve the
transferability even further.

We follow the ensemble strategy proposed in [9], which
fuse the logit activations together to attack multiple net-
works simultaneously. Speciﬁcally, to attack an ensemble
of K models, the logits are fused by:

l(X; θ1, ..., θK) =

K

Xk=1

wklk(X; θk)

(8)

4.1. Experiment Setup

Dataset.
It is less meaningful to attack the images that are
already classiﬁed wrongly. Therefore, we randomly choose
5000 images from the ImageNet validation set that are clas-
siﬁed correctly by all the networks which we test on, to form
our test dataset. All these images are resized to 299×299×3
beforehand.

Networks. We consider
four normally trained net-
works, i.e., Inception-v3 (Inc-v3) [35], Inception-v4 (Inc-
v4) [34], Resnet-v2-152 (Res-152) [13] and Inception-
Resnet-v2 (IncRes-v2) [34], and three adversarially trained
networks [37],
i.e., ens3-adv-Inception-v3 (Inc-v3ens3),
ens4-adv-Inception-v3 (Inc-v3ens4) and ens-adv-Inception-
ResNet-v2 (IncRes-v2ens). All networks are publicly avail-
able2,3.

Implementation details. For the parameters of different
attackers, we follow the default settings in [16] with the
step size α = 1 and the total iteration number N =
min(ǫ + 4, 1.25ǫ). We set the maximum perturbation of
each pixel to be ǫ = 15, which is still imperceptible for hu-
man observers [23]. For the momentum term, decay factor
µ is set to be 1 as in [9]. For the stochastic transformation
function T (X; p), the probability p is set to be 0.5, i.e., at-
tackers put equal attentions on the original inputs and the
transformed inputs. For transformation operations T (·), the
input X is ﬁrst randomly resized to a rnd × rnd × 3 im-
age, with rnd ∈ [299, 330), and then padded to the size
330 × 330 × 3 in a random manner.

4.2. Attacking a Single Network

We ﬁrst perform adversarial attacks on a single network.
We craft adversarial examples only on normally trained net-
works, and test them on all seven networks. The success
rates are shown in Table 1, where the diagonal blocks in-
dicate white-box attacks and off-diagonal blocks indicate
black-box attacks. We list the networks that we attack on in
rows, and networks that we test on in columns.

From Table 1, a ﬁrst glance shows that M-DI2-FGSM
outperforms all other baseline attacks by a large margin on
all black-box models, and maintains high success rates on
all white-box models. For example, if adversarial examples

2https://github.com/tensorflow/models/tree/

master/research/slim

3https://github.com/tensorflow/models/tree/

master/research/adv_imagenet_models

2733

Model

Attack

Inc-v3

Inc-v4

IncRes-v2 Res-152

Inc-v3ens3

Inc-v3ens4

IncRes-v2ens

Inc-v3

Inc-v4

IncRes-v2

Res-152

FGSM
I-FGSM
DI2-FGSM (Ours)
MI-FGSM
M-DI2-FGSM (Ours)

FGSM
I-FGSM
DI2-FGSM (Ours)
MI-FGSM
M-DI2-FGSM (Ours)

FGSM
I-FGSM
DI2-FGSM (Ours)
MI-FGSM
M-DI2-FGSM (Ours)

FGSM
I-FGSM
DI2-FGSM (Ours)
MI-FGSM
M-DI2-FGSM (Ours)

64.6% 23.5%
99.9% 14.8%
99.9% 35.5%
99.9% 36.6%
99.9% 63.9%

26.4% 49.6%
22.0% 99.9%
43.3% 99.7%
51.1% 99.9%
72.4% 99.5%

24.3% 19.3%
22.2% 17.7%
46.5% 40.5%
53.5% 45.9%
71.2% 67.4%

34.4% 28.5%
20.8% 17.2%
53.8% 49.0%
50.1% 44.1%
78.9% 76.5%

21.7%
11.6%
27.8%
34.5%
59.4%

19.7%
13.2%
28.9%
39.4%
62.2%

39.6%
97.9%
95.8%
98.4%
96.1%

27.1%
14.9%
44.8%
42.2%
74.8%

21.7%
8.9%
21.4%
27.5%
47.9%

20.4%
10.9%
23.1%
33.7%
52.1%

19.4%
12.6%
28.6%
37.8%
57.4%

75.2%
99.1%
99.2%
99.0%
99.2%

8.0%
3.3%
5.5%
8.9%
14.3%

8.4%
3.2%
5.9%
11.2%
17.6%

8.5%
4.6%
8.2%
15.3%
25.1%

12.4%
5.4%
13.0%
18.2%
35.2%

7.5%
2.9%
5.2%
8.4%
14.0%

7.7%
3.0%
5.5%
10.7%
15.6%

7.3%
3.7%
6.6%
13.0%
20.7%

11.0%
4.6%
11.1%
15.2%
29.4%

3.6%
1.5%
2.8%
4.7%
7.0%

4.1%
1.7%
3.2%
5.3%
8.8%

4.8%
2.5%
4.8%
8.8%
14.9%

6.0%
2.8%
6.9%
9.0%
19.0%

Table 1. The success rates on seven networks where we attack a single network. The diagonal blocks indicate white-box attacks, while
the off-diagonal blocks indicate black-box attacks which are much more challenging. Experiment results demonstrate that our proposed
input diversity strategy substantially improve the transferability of generated adversarial examples.

n
a
e
l
c

l

a
i
r
a
s
r
e
v
d
a

Figure 3. Visualization of randomly selected clean images and
their corresponding adversarial examples. All these adversarial
examples are generated on Inception-v3 using our proposed DI2-
FGSM with the maximum perturbation of each pixel ǫ = 15.

are crafted on IncRes-v2, M-DI2-FGSM has success rates
of 67.4% on Inc-v4 (normally trained black-box model)
and 25.1% on Inc-v3ens3 (adversarially trained black-box
model), while strong baselines like MI-FGSM only obtains
the corresponding success rates of 45.9% and 15.3%, re-
spectively. This convincingly demonstrates the effective-
ness of the combination of input diversity and momentum
for improving the transferability of adversarial examples.

We then compare the success rates of I-FGSM and
DI2-FGSM to see the effectiveness of diverse input pat-
terns solely. By generating adversarial examples with in-
put diversity, DI2-FGSM signiﬁcantly improves the suc-
cess rates of I-FGSM on challenging black-box models, re-
gardless whether this model is adversarially trained, and
maintains high success rates on white-box models. For
example, if adversarial examples are crafted on Res-152,
DI2-FGSM has success rates of 99.2% on Res-152 (white-

box model), 53.8% on Inc-v3 (normally trained black-
box model) and 11.1% on Inc-v3ens4 (adversarially trained
black-box model), while I-FGSM only obtains the corre-
sponding success rates of 99.1%, 20.8% and 4.6%, respec-
tively. Compared with FGSM, DI2-FGSM also reaches
much higher success rates on the normally trained black-
box models, and comparable performance on the adversari-
ally trained black-box models. Besides, we visualize 5 ran-
domly selected pairs of such generated adversarial images
and their clean counterparts in Figure 3. These visualization
results show that these generated adversarial perturbations
are human imperceptible.

It should be mentioned that the proposed input diver-
sity is not merely applicable to fast gradient sign meth-
ods. To demonstrate the generalization, we also incorpo-
rate C&W attack [4] with input diversity. The experiment is
conducted on 1000 correctly classiﬁed images. For the pa-
rameters of C&W, the maximal iteration is 250, the learning
rate is 0.01 and the conﬁdence is 10. As Table 2 suggests,
our method D-C&W obtains a signiﬁcant performance im-
provement over C&W on black-box models.

4.3. Attacking an Ensemble of Networks

Though the results in Table 1 show that momentum and
input diversity can signiﬁcantly improve the transferability
of adversarial examples, they are still relatively weak at at-
tacking an adversarially trained network under the black-
box setting, e.g., the highest black-box success rate on
IncRes-v2ens is only 19.0%. Therefore, we follow the strat-

2734

Model

Attack

Inc-v3

Inc-v4

IncRes-v2 Res-152

Inc-v3ens3

Inc-v3ens4

IncRes-v2ens

Inc-v3

Inc-v4

IncRes-v2

Res-152

C&W
D-C&W (Ours)
C&W
D-C&W (Ours)
C&W
D-C&W (Ours)
C&W
D-C&W (Ours)

100.0% 5.7%
100.0% 16.8%
100.0%
15.1%
29.3% 100.0%
15.8%
11.2%
33.9% 25.6%
11.4%
6.9%
33.0% 27.7%

5.3%
13.0%
9.2%
20.1%
99.9%
100.0%

6.1%
24.4%

3.0%
5.1%
5.8%
11.2%
4.4%
7.8%
7.1%
15.4%
6.3%
8.6%
11.2%
19.4%
100.0%
4.4%
100.0% 13.1%

2.5%
3.9%
3.5%
5.3%
3.6%
7.3%
4.1%
9.3%

1.1%
2.1%
1.9%
3.1%
3.4%
4.0%
2.3%
5.7%

Table 2. The success rates on seven networks where we attack a single network using C&W attack. Experiment results demonstrate
that the proposed input diversity strategy can enhance C&W attack for generating more transferable adversarial examples.

Model

Attack

-Inc-v3

-Inc-v4

-IncRes-v2

-Res-152

-Inc-v3ens3

-Inc-v3ens4

-IncRes-v2ens

Ensemble

Hold-out

I-FGSM
DI2-FGSM (Ours)
MI-FGSM
M-DI2-FGSM (Ours)

I-FGSM
DI2-FGSM (Ours)
MI-FGSM
M-DI2-FGSM (Ours)

96.6% 96.9%
88.9%
89.6%
96.9% 96.9%
90.1%
91.1%

36.4%
43.7%
67.9%
69.9%
71.4%
65.9%
80.7% 80.6%

98.7%
93.2%
98.8%
94.0%

33.3%
64.1%
64.6%
80.7%

96.2%
87.7%
96.8%
89.3%

25.4%
51.7%
55.6%
70.9%

97.0%
91.7%
96.8%
92.8%

12.9%
36.3%
22.8%
44.6%

97.3%
91.7%
97.0%
92.7%

15.1%
35.0%
26.1%
44.5%

94.3%
93.2%
94.6%
94.9%

8.8%
30.4%
15.8%
39.4%

Table 3. The success rates of ensemble attacks. Adversarial examples are generated on an ensemble of six networks, and tested on the
ensembled network (white-box setting) and the hold-out network (black-box setting). The sign “-” indicates the hold-out network. We
observe that the proposed M-DI2-FGSM signiﬁcantly outperforms all other attacks on all black-box models.

egy in [21] to attack multiple networks simultaneously in
order to further improve transferability. We consider all
seven networks here. Adversarial examples are generated
on an ensemble of six networks, and tested on the ensem-
bled network and the hold-out network, using I-FGSM, DI2-
FGSM, MI-FGSM and M-DI2-FGSM, respectively. FGSM
is ignored here due to its low success rates on white-box
models. All ensembled models are assigned with equal
weight, i.e., wk = 1/6.

The results are summarized in Table 3, where the top row
shows the success rates on the ensembled network (white-
box setting), and the bottom row shows the success rates on
the hold-out network (black-box setting). Under the chal-
lenging black-box setting, we observe that M-DI2-FGSM
always generates adversarial examples with better transfer-
ability than other methods on all networks. For example,
by keeping Inc-v3ens3 as a hold-out model, M-DI2-FGSM
can fool Inc-v3ens3 with an success rate of 44.6%, while I-
FGSM, DI2-FGSM and MI-FGSM only have success rates
of 12.9%, 36.3% and 22.8%, respectively. Besides, com-
pared with MI-FGSM, we observe that using diverse input
patterns alone, i.e., DI2-FGSM, can reach a much higher
success rate if the hold-out model is an adversarially trained
network, and a comparable success rate if the hold-out
model is a normally trained network.

Under the white-box setting, we see that DI2-FGSM and
M-DI2-FGSM reach slightly lower (but still very high) suc-
cess rates on ensemble models compared with I-FGSM and
MI-FGSM. This is due to the fact that attacking multiple
networks simultaneously is much harder than attacking a

single model. However, the white-box success rates can be
improved if we assign the transformation probability p with
a smaller value, increase the number of total iteration N or
use a smaller step size α (see Sec. 4.4).

4.4. Ablation Studies

In this section, we conduct a series of ablation experi-
ments to study the impact of different parameters. We only
consider attacking an ensemble of networks here, since it
is much stronger than attacking a single network and can
provide a more accurate evaluation of the network robust-
ness. The max perturbation of each pixel ǫ is set to 15 for
all experiments.

Transformation probability p. We ﬁrst study the inﬂu-
ence of the transformation probability p on the success
rates under both white-box and black-box settings. We
set the step size α = 1 and the total iteration number
N = min(ǫ + 4, 1.25ǫ). The transformation probability p
varies from 0 to 1. Recall the relationships shown in Fig. 2,
M-DI2-FGSM (or DI2-FGSM) degrades to MI-FGSM (or
I-FGSM) if p = 0.

We show the success rates on various networks in Fig. 4.
We observe that both DI2-FGSM and M-DI2-FGSM achieve
a higher black-box success rates but lower white-box suc-
cess rates as p increase. Moreover, for all attacks, if p is
small, i.e., only a small amount of transformed inputs are
utilized, black-box success rates can increase signiﬁcantly,
while white-box success rates only drop a little. This phe-
nomenon reveals the importance of adding transformed in-
puts into the attack process.

2735

1

0.8

0.6

0.4

0.2

e
t
a
R
 
s
s
e
c
c
u
S

0

0

0.2

1

0.8

0.6

0.4

0.2

e
t
a
R
 
s
s
e
c
c
u
S

Inc-v3
Inc-v4
IncRes-v2
Res-152
Inc-v3-ens3
Inc-v3-ens4
IncRes-v2-ens

-Inc-v3
-Inc-v4
-IncRes-v2
-Res-152
-Inc-v3-ens3
-Inc-v3-ens4
-IncRes-v2-ens

Inc-v3
Inc-v4
IncRes-v2
Res-152
Inc-v3-ens3
Inc-v3-ens4
IncRes-v2-ens

0.8

1

0

0

0.2

0.4

0.6
Probability
(a)

0.4

0.6
Probability
(b)

1

0.8

0.6

0.4

0.2

e
t
a
R
 
s
s
e
c
c
u
S

-Inc-v3
-Inc-v4
-IncRes-v2
-Res-152
-Inc-v3-ens3
-Inc-v3-ens4
-IncRes-v2-ens

0.8

1

0
1/30

1/25

1

0.8

0.6

0.4

0.2

e
t
a
R
 
s
s
e
c
c
u
S

Inc-v3
Inc-v4
IncRes-v2
Res-152
Inc-v3-ens3
Inc-v3-ens4
IncRes-v2-ens

-Inc-v3
-Inc-v4
-IncRes-v2
-Res-152
-Inc-v3-ens3
-Inc-v3-ens4
-IncRes-v2-ens

1/10

1/5

0
1/30

1/25

1/15

1/20
Step Size
(a)

-Inc-v3
-Inc-v4
-IncRes-v2
-Res-152
-Inc-v3-ens3
-Inc-v3-ens4
-IncRes-v2-ens

1/10

1/5

Inc-v3
Inc-v4
IncRes-v2
Res-152
Inc-v3-ens3
Inc-v3-ens4
IncRes-v2-ens

1/15

1/20
Step Size
(b)

Figure 4. The success rates of DI2-FGSM (a) and M-DI2-FGSM
(b) when varying the transformation probability p. “Ensem-
ble” (white-box setting) is with dashed lines and “Hold-out”
(black-box setting) is with solid lines.

Figure 6. The success rates of DI2-FGSM (a) and M-DI2-FGSM
(b) when varying the step size α. “Ensemble” (white-box setting)
is with dashed lines and “Hold-out” (black-box setting) is with
solid lines.

1

0.8

0.6

0.4

0.2

e
t
a
R
 
s
s
e
c
c
u
S

1

0.8

0.6

0.4

0.2

e
t
a
R
 
s
s
e
c
c
u
S

Inc-v3
Inc-v4
IncRes-v2
Res-152
Inc-v3-ens3
Inc-v3-ens4
IncRes-v2-ens

-Inc-v3
-Inc-v4
-IncRes-v2
-Res-152
-Inc-v3-ens3
-Inc-v3-ens4
-IncRes-v2-ens

Inc-v3
Inc-v4
IncRes-v2
Res-152
Inc-v3-ens3
Inc-v3-ens4
IncRes-v2-ens

-Inc-v3
-Inc-v4
-IncRes-v2
-Res-152
-Inc-v3-ens3
-Inc-v3-ens4
-IncRes-v2-ens

0
15

19
Iteration Number

23

27

31

0
15

19
Iteration Number

23

27

31

(a)

(b)

Figure 5. The success rates of DI2-FGSM (a) and M-DI2-FGSM
(b) when varying the total iteration number N . “Ensemble”
(white-box setting) is with dashed lines and “Hold-out” (black-
box setting) is with solid lines.

The trends shown in Fig. 4 also provide useful sugges-
tions of constructing strong adversarial attacks in practice.
For example, if you know the black-box model is a new
network that totally different from any existing networks,
you can set p = 1 to reach the maximum transferability. If
the black-box model is a mixture of new networks and ex-
isting networks, you can choose a moderate value of p to
maximize the black-box success rates under a pre-deﬁned
white-box success rates, e.g., white-box success rates must
greater or equal than 90%.

Total iteration number N . We then study the inﬂuence of
the total iteration number N on the success rates under both
white-box and black-box settings. We set the transforma-
tion probability p = 0.5 and the step size α = 1. The total

iteration number N varies from 15 to 31, and the results are
plotted in Fig. 5. For DI2-FGSM, we see that the black-box
success rates and white-box success rates always increase
as the total iteration number N increase. Similar trends can
also be observed for M-DI2-FGSM except for the black-box
success rates on adversarially trained models, i.e., perform-
ing more iterations cannot bring extra transferability on ad-
versarially trained models. Moreover, we observe that the
success rates gap between M-DI2-FGSM and DI2-FGSM is
diminished as N increases.

Step size α. We ﬁnally study the inﬂuence of the step size
α on the success rates under both white-box and black-box
settings. We set the transformation probability p = 0.5.
In order to reach the maximum perturbation ǫ even for a
small step size α, we set the total iteration number be pro-
portional to the step size, i.e., N = ǫ/α. The results are
plotted in Fig. 6. We observe that the white-box success
rates of both DI2-FGSM and M-DI2-FGSM can be boosted
if a smaller step size is provided. Under the black-box set-
ting, the success rates of DI2-FGSM is insensitive to the
step size, while the success rates of M-DI2-FGSM can still
be improved with smaller step size.

4.5. NIPS 2017 Adversarial Competition

In order to verify the effectiveness of our proposed at-
tack methods in practice, we here reproduce the top defense
entries and ofﬁcial baselines from NIPS 2017 adversarial
competition [18] for testing transferability. Due to the re-
source limitation, we only consider the top-3 defense en-
tries, i.e., TsAIL [19], iyswim [38] and Anil Thomas4, as well
3 ofﬁcial baselines, i.e., Inc-v3adv, IncRes-v2ens and Inc-v3.

4https://github.com/anlthms/nips-2017/tree/

master/mmd

2736

Attack

TsAIL

iyswim Anil Thomas

Inc-v3adv

IncRes-v2ens

Inc-v3 Average

I-FGSM
DI2-FGSM (Ours)
MI-FGSM
MI-FGSM*
M-DI2-FGSM (Ours)

14.0% 35.6%
22.7% 58.4%
14.9% 45.7%
13.6% 43.2%
20.0% 69.8%

30.9%
48.0%
46.6%
43.9%
64.4%

98.2%
91.5%
97.3%
94.4%
93.3%

96.4%
90.7%
95.4%
93.0%
92.4%

99.0% 62.4%
68.1%
97.3%
66.4%
98.7%
64.2%
97.3%
73.0%
97.9%

Table 4. The success rates on top defense solutions and ofﬁcial baselines from NIPS 2017 adversarial competition [18]. * indicates
the ofﬁcial results reported in the competition. Our proposed M-DI2-FGSM reaches an average success rate of 73.0%, which outperforms
the top-1 attack submission in the NIPS competition by a large margin of 6.6%.

We note that the No.1 solution and the No.3 solution apply
signiﬁcantly different image transformations (compared to
random resizing & padding used in our attack method) for
defending against adversarial examples. For example, the
No.1 solution, TsAIL, applies an image denoising network
for removing adversarial perturbations, and the No.3 solu-
tion, Anil Thomas, includes a series of image transforma-
tions, e.g., JPEG compression, rotation, shifting and zoom-
ing, in the defense pipeline. The test dataset contains 5000
images which are all of the size 299 × 299 × 3, and their
corresponding labels are the same as the ImageNet labels.

Generating adversarial examples. When generating ad-
versarial examples, we follow the procedure in [18]: (1)
split the dataset equally into 50 batches; (2) for each batch,
the maximum perturbation ǫ is randomly chosen from the
set { 4
255 }; and (3) generate adversarial exam-
ples for each batch under the corresponding ǫ constraint.

255 , 16

255 , 12

255 , 8

Attacker settings. For the settings of attackers, we fol-
low [9] by attacking an ensemble eight diferent models, i.e.,
Inc-v3, Inc-v4, IncRes-v2, Res-152, Inc-v3ens3, Inc-v3ens4,
IncRes-v2ens and Inc-v3adv [17]. The ensemble weights
are set as 1/7.25 equally for the ﬁrst seven models and
0.25/7.25 for Inc-v3adv. The total iteration number N is
10 and the decay factor µ is 1. This conﬁguration for MI-
FGSM won the 1-st place in the NIPS 2017 adversarial at-
tack competition. For DI2-FGSM and M-DI2-FGSM, we
choose p = 0.4 according to the trends shown in Fig. 4.

Results. The results are summarized in Table 4. We also
report the ofﬁcial results of MI-FGSM (named MI-FGSM*)
as a reference to validate our implementation. The per-
formance difference between MI-FGSM and MI-FGSM*
is due to the randomness of the max perturbation magni-
tude introduced in the attack process. Compared with MI-
FGSM, DI2-FGSM have higher success rates on top de-
fense solutions while slightly lower success rates on base-
line models, which results in these two attack methods hav-
ing similar average success rates. By integrating both di-
verse inputs and momentum term, this enhanced attack,
M-DI2-FGSM, reaches an average success rate of 73.0%,
which is far better than other methods. For example, the
top-1 attack submission, MI-FGSM, in the NIPS competi-
tion only gets an average success rate of 66.4%. We believe

this superior transferability can also be observed on other
defense submissions which we do not evaluate on.

4.6. Discussion

We provide a brief discussion of why the proposed di-
verse input patterns can help to generate adversarial exam-
ples with better transferability. One hypothesis is that the
decision boundaries of different networks share similar in-
herent structures due to the same training dataset, e.g., Im-
ageNet. For example, as shown in Fig 1, different networks
make similar mistakes in the presence of adversarial exam-
ples. By incorporating diverse patterns at each attack iter-
ation, the optimization produces adversarial examples that
are more robust to small transformations. These adversarial
examples are malicious in a certain region at the network
decision boundary, thus increasing the chance to fool other
networks, i.e., they achieve better black-box success rate
than existing methods.
In the future, we plan to validate
this hypothesis theoretically or empirically.

5. Conclusions

In this paper, we propose to improve the transferability
of adversarial examples with input diversity. Speciﬁcally,
our method applies random transformations to the input im-
ages at each iteration in the attack process. Compared with
traditional iterative attacks, the results on ImageNet show
that our proposed attack method gets signiﬁcantly higher
success rates for black-box models, and maintains simi-
lar success rates for white-box models. We improve the
transferability further by integrating momentum term and
attacking multiple networks simultaneously. By evaluating
this enhanced attack against the top defense submissions
and ofﬁcial baselines from NIPS 2017 adversarial compe-
tition [18], we show that this enhanced attack reaches an
average success rate of 73.0%, which outperforms the top-1
attack submission in the NIPS competition by a large mar-
gin of 6.6%. We hope that our proposed attack strategy
can serve as a benchmark for evaluating the robustness of
networks to adversaries and the effectiveness of different
defense methods in future. Code is publicly available at
https://github.com/cihangxie/DI-2-FGSM.

Acknowledgement: This work was supported by a gift grant from YiTu

and ONR N00014-12-1-0883.

2737

References

[1] A. Arnab, O. Miksik, and P. H. Torr. On the robustness of
semantic segmentation models to adversarial attacks. arXiv
preprint arXiv:1711.09856, 2017. 1

[2] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesiz-
ing robust adversarial examples. In International Conference
on Machine Learning, pages 284–293, 2018. 2

[3] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c,
P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against
machine learning at test time. In Joint European conference
on machine learning and knowledge discovery in databases,
pages 387–402, 2013. 2

[4] N. Carlini and D. Wagner. Towards evaluating the robustness
In IEEE Symposium on Security and

of neural networks.
Privacy, 2017. 5

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 2017. 1

[6] M. Cisse, Y. Adi, N. Neverova, and J. Keshet. Houdini:
Fooling deep structured prediction models. arXiv preprint
arXiv:1707.05373, 2017. 2

[7] N. Dalvi, P. Domingos, S. Sanghai, D. Verma, et al. Adver-
sarial classiﬁcation. In ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, 2004. 2

[8] G. S. Dhillon, K. Azizzadenesheli, J. D. Bernstein, J. Kos-
saiﬁ, A. Khanna, Z. C. Lipton, and A. Anandkumar. Stochas-
tic activation pruning for robust adversarial defense. In In-
ternational Conference on Learning Representations, 2018.
2

[9] Y. Dong, F. Liao, T. Pang, H. Su, X. Hu, J. Li, and J. Zhu.
Boosting adversarial attacks with momentum. arXiv preprint
arXiv:1710.06081, 2017. 2, 3, 4, 8

[10] R. Girshick. Fast r-cnn. In International Conference on Com-

puter Vision, 2015. 1

[11] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
In International Confer-

harnessing adversarial examples.
ence on Learning Representations, 2015. 1, 2, 3

[12] C. Guo, M. Rana, M. Ciss´e, and L. van der Maaten. Coun-
tering adversarial images using input transformations. In In-
ternational Conference on Learning Representations, 2018.
2, 3

[13] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. In European Conference on Com-
puter Vision, 2016. 1, 2, 3, 4

[14] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and
J. Tygar. Adversarial machine learning. In ACM workshop
on Security and artiﬁcial intelligence, 2011. 2

[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems, 2012.
1, 2, 3

[16] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial ex-
amples in the physical world. In International Conference
on Learning Representations Workshop, 2017. 1, 2, 4

[17] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial
In International Conference on

machine learning at scale.
Learning Representations, 2017. 1, 2, 3, 8

[18] A. Kurakin, I. Goodfellow, S. Bengio, Y. Dong, F. Liao,
M. Liang, T. Pang, J. Zhu, X. Hu, C. Xie, et al. Ad-
versarial attacks and defences competition. arXiv preprint
arXiv:1804.00097, 2018. 2, 7, 8

[19] F. Liao, M. Liang, Y. Dong, T. Pang, X. Hu, and J. Zhu.
Defense against adversarial attacks using high-level repre-
sentation guided denoiser. In Computer Vision and Pattern
Recognition, 2018. 7

[20] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu,
and M. Sun. Tactics of adversarial attack on deep reinforce-
ment learning agents. In International Joint Conference on
Artiﬁcial Intelligence, 2017. 2

[21] Y. Liu, X. Chen, C. Liu, and D. Song. Delving into transfer-
able adversarial examples and black-box attacks. In Interna-
tional Conference on Learning Representations, 2017. 2, 4,
6

[22] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
In Computer Vision

networks for semantic segmentation.
and Pattern Recognition, 2015. 1

[23] Y. Luo, X. Boix, G. Roig, T. Poggio, and Q. Zhao. Foveation-
arXiv

based mechanisms alleviate adversarial examples.
preprint arXiv:1511.06292, 2015. 4

[24] D. Meng and H. Chen.

Magnet:
defense against adversarial examples.
arXiv:1705.09064, 2017. 2

a two-pronged
arXiv preprint

[25] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks
are easily fooled: High conﬁdence predictions for unrecog-
nizable images.
In Computer Vision and Pattern Recogni-
tion, 2015. 2

[26] N. Papernot, F. Faghri, N. Carlini, I. Goodfellow, R. Fein-
man, A. Kurakin, C. Xie, Y. Sharma, T. Brown, A. Roy,
A. Matyasko, V. Behzadan, K. Hambardzumyan, Z. Zhang,
Y.-L. Juang, Z. Li, R. Sheatsley, A. Garg, J. Uesato,
W. Gierke, Y. Dong, D. Berthelot, P. Hendricks, J. Rauber,
and R. Long.
cleverhans v2.1.0: an adversarial machine
learning library. arXiv preprint arXiv:1610.00768, 2018. 3

[27] A. Prakash, N. Moran, S. Garber, A. DiLillo, and J. Storer.
Deﬂecting adversarial attacks with pixel deﬂection. arXiv
preprint arXiv:1801.08926, 2018. 2

[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in Neural Information Processing Systems, 2015.
1

[29] P. Samangouei, M. Kabkab, and R. Chellappa. Defense-
GAN: Protecting classiﬁers against adversarial attacks using
generative models. In International Conference on Learning
Representations, 2018. 2

[30] A. Shrivastava, A. Gupta, and R. Girshick. Training region-
based object detectors with online hard example mining. In
Computer Vision and Pattern Recognition, 2016. 2

[31] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and
F. Moreno-Noguer. Discriminative learning of deep convolu-
tional feature point descriptors. In International Conference
on Computer Vision, 2015. 2

2738

[32] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations, 2015. 1, 2, 3

[33] Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman.
Pixeldefend: Leveraging generative models to understand
and defend against adversarial examples.
arXiv preprint
arXiv:1710.10766, 2017. 2

[34] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, 2017. 4

[35] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
Computer Vision and Pattern Recognition, 2016. 4

[36] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. In International Conference on Learning Repre-
sentations, 2014. 1, 2

[37] F. Tram`er, A. Kurakin, N. Papernot, D. Boneh, and P. Mc-
Daniel. Ensemble adversarial training: Attacks and defenses.
arXiv preprint arXiv:1705.07204, 2017. 1, 2, 4

[38] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille. Mitigating
adversarial effects through randomization. In International
Conference on Learning Representations, 2018. 2, 3, 7

[39] C. Xie, J. Wang, Z. Zhang, Y. Zhou, L. Xie, and A. Yuille.
Adversarial Examples for Semantic Segmentation and Ob-
ject Detection. In International Conference on Computer Vi-
sion, 2017. 2

[40] Z. Zhang, S. Qiao, C. Xie, W. Shen, B. Wang, and A. L.
Yuille. Single-shot object detection with enriched semantics.
arXiv preprint arXiv:1712.00433, 2017. 1

2739

