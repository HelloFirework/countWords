Attention-guided Uniﬁed Network for Panoptic Segmentation

Yanwei Li1,2, Xinze Chen3, Zheng Zhu1,2, Lingxi Xie4,5, Guan Huang3,

Dalong Du3, Xingang Wang1

1Institute of Automation, CAS 2University of Chinese Academy of Sciences

3Horizon Robotics, Inc.

4Johns Hopkins University 5Noah’s Ark Lab, Huawei Inc.

{liyanwei2017,zhuzheng2014,xingang.wang}@ia.ac.cn

{xinze.chen,guan.huang,dalong.du}@horizon.ai

198808xc@gmail.com

Abstract

This paper studies panoptic segmentation, a recently
proposed task which segments foreground (FG) objects at
the instance level as well as background (BG) contents at
the semantic level. Existing methods mostly dealt with these
two problems separately, but in this paper, we reveal the un-
derlying relationship between them, in particular, FG ob-
jects provide complementary cues to assist BG understand-
ing. Our approach, named the Attention-guided Uniﬁed
Network (AUNet), is a uniﬁed framework with two branches
for FG and BG segmentation simultaneously. Two sources
of attentions are added to the BG branch, namely, RPN and
FG segmentation mask to provide object-level and pixel-
level attentions, respectively. Our approach is generalized
to different backbones with consistent accuracy gain in both
FG and BG segmentation, and also sets new state-of-the-
arts both in the MS-COCO (46.5% PQ) and Cityscapes
(59.0% PQ) benchmarks.1

1. Introduction

Scene understanding is a fundamental yet challenging
task in computer vision, which has a great impact on
other applications such as autonomous driving and robotics.
Classic tasks for scene understanding mainly include ob-
ject detection,
instance segmentation and semantic seg-
mentation. This paper considers a recently proposed task
named panoptic segmentation [23], which aims at ﬁnding
all foreground (FG) objects (named things, mainly includ-
ing countable targets such as people, animals, tools, etc.) at
the instance level, meanwhile parsing the background (BG)
contents (named stuff, mainly including amorphous regions
of similar texture and/or material such as grass, sky, road,
etc.) at the semantic level. The benchmark algorithm [23]
and MS-COCO panoptic challenge winners [1] dealt with

1This work was done in Horizon Robotics.

(a) Input Image

(b) Panoptic Segmentation

(c) Foreground: things

(d) Background: stuff

Figure 1. Given an image 1(a), the goal of panoptic segmenta-
tion 1(b) is to ﬁnd FG things at the instance level 1(c) and BG
stuff at the semantic level 1(d). The things of the same class share
the same color family but appear in different intensities. All these
results are produced by the proposed approach.

this task by directly combining FG instance segmentation
models [15] and BG scene parsing [45] algorithms, which
ignores the underlying relationship and fails to borrow rich
contextual cues between things and stuff.

In this paper, we present a conceptually simple and uni-
ﬁed framework for panoptic segmentation. To facilitate in-
formation ﬂow between FG things and BG stuff, we com-
bine conventional instance segmentation and semantic seg-
mentation networks, leading to a uniﬁed network with two
branches. This strategy brings an immediate improvement
in segmentation accuracy as well as higher efﬁciency in
computation (because the network backbone can be shared).
This implies that panoptic segmentation beneﬁts from com-
plementary information provided by FG objects and BG
contents, which lays the foundation of our approach.

7026

Going one step further, we explore the possibility of in-
tegrating higher-level visual cues (i.e., beyond the features
extracted from the end of the backbone) towards the more
accurate segmentation. This is achieved via two attention-
based modules working at the object level and the pixel
level, respectively. For the ﬁrst module, we refer to the
regional proposals, each of which indicates a possible FG
thing, and adjusts the probability of the corresponding re-
gion to be considered as FG things and BG stuff. For the
second module, we take out the FG segmentation mask, and
use it to reﬁne the boundary between FG things and BG
stuff. In the context of deep networks, these two modules,
named the Proposal Attention Module (PAM) and Mask At-
tention Module (MAM), respectively, are implemented as
additional connections across FG and BG branches. Within
MAM, a new layer named RoIUpsample is designed to de-
ﬁne an accurate mapping function between pixels in the
ﬁxed-shape FG mask and the corresponding feature map. In
practice, all additional connections go from the FG branch
to the BG branch, mainly due to the observation that FG
segmentation is often more accurate2. Furthermore, BG
stuff, while being reﬁned by FG things, also gives feedback
via gradients. Consequently, both FG and BG segmentation
accuracies are considerably improved.

The overall approach, named Attention-guided Uniﬁed
Network (AUNet), can be easily instantiated to various net-
work backbones, and optimized in an end-to-end manner.
We evaluate AUNet in two popular segmentation bench-
marks, namely, the MS-COCO [28] and Cityscapes [8]
datasets, and claim the state-of-the-art performance in
terms of PQ, a standard metric integrating accuracies of
both things and stuff [23]. In addition, the beneﬁts brought
by joint optimization and two attention-based modules are
veriﬁed through an extensive ablation study 4.2.

The major contribution of this research is to present a
simple and uniﬁed framework for both FG and BG seg-
mentation, which reaches the top performance in MS-
COCO [28] and Cityscapes [8] datasets. Furthermore, this
work also investigate the complementary information deliv-
ered by FG objects and BG contents. While panoptic seg-
mentation serves as a natural scenario of studying this topic,
its application lies in a wider range of visual tasks. Our so-
lution, AUNet, is a preliminary exploration in this ﬁeld, yet
we look forward to more efforts along this direction.

The remainder of this paper is organized as follows. Sec-
tion 2 brieﬂy reviews related work. Section 3 elaborates the
proposed AUNet, including two attention-based modules.
After experiments are shown in Section 4, we conclude this
work in Section 5.

2We ﬁnd the pixel accuracy of things is much higher (6.7% absolute
gap) than that of stuff, when considering instance with the same semantic
as one category, e.g., all individuals are evaluated as person in testing. We
evaluate them on the same MS-COCO semantic evaluation metric.

2. Related Work

Traditional deep learning based scene understanding re-
searches often focused on foreground or background tar-
gets [15, 45]. Recently, the rapid progress in object detec-
tion [13, 14, 34] and instance segmentation [9, 15, 25, 31]
made it possible to achieve object localization and segmen-
tation at a ﬁner level. Meanwhile, the development of se-
mantic segmentation [5, 6, 33, 45] boosted the performance
of scene parsing. Despite their effectiveness, the separation
of these tasks caused the lack of contextual cues in instance
segmentation as well as the confusion brought by individu-
als in semantic segmentation. To bridge this gap, recently,
researchers proposed a new task named panoptic segmen-
tation [23], which aims at accomplishing both tasks (FG
instance and BG semantic segmentation) simultaneously.

Panoptic Segmentation: In [23], the author gave a bench-
mark of panopic segmentation by combining instance and
semantic segmentation models. Later, a weakly-supervised
method [24] was proposed on top of initialized semantic
results, and an end-to-end approach [11] was designed to
combine both FG and BG cues. However, their performance
is far from the benchmark [23]. Different from them, our
proposed AUNet achieves the top performance in an end-
to-end framework. Furthermore, we also establish the bond
between proposal-based instance and FCN based semantic
segmentation. Most recently works include [22, 29, 40].

Instance Segmentation:
Instance segmentation aims at
discriminating different instances of the same object. There
are mainly two streams of methods to solve this task,
namely, proposal-based methods and segmentation-based
methods. Proposal-based methods, with the help of accu-
rate regional proposals, often achieved higher performance.
Recent examples include MNC [9], FCIS [25], Mask R-
CNN [15] and PANet [31]. Moreover, segmentation-based
methods aggregated pixel-level cues to compose instances
combined with semantic segmentation [2, 26, 32] or depth
ordering [44] results.

Semantic Segmentation: With the development of so-
called encoding-decoding networks such as FCN [33], rapid
progress has been made in semantic segmentation [5, 6, 45].
In segmentation, capturing contextual information plays a
vital role, for which various approaches were proposed in-
cluding ASPP used in DeepLab [5, 6] for multi-scale con-
texts, DenseASPP [41] for global contexts, and PSPNet [45]
which collected contextual priors. There were also efforts
to use attention modules for spatial feature selection, such
as [12, 42, 43], which will be detailed discussed next.

Attention-based Modules: Attention-based modules have
been widely applied in visual tasks, including image pro-
cessing, video understanding, and object tracking [7, 19, 37,
46, 47]. In particular, SENet [19] formulated channel-wise
relationships via an attention-and-gating mechanism, non-

7027

RoIAlign

x4

Foreground branch

14x14

14x14

28x28

28x28

RoIUpsample

 
 

 
 

 
 

 
 

Shared Feature
Shared Feature

 
 
 

 
 
 

 
 
 

 
 
 

RPN 
branch

Pi

Shared Backbone

 
 

 
 

 
 

 
 

Input

RoIs

RPN

7x7

1024 1024

class

box

class
class

box
box

Si

S''

i

 
 

 
 

 
 

 
 

PAM

 
 

 
 

 
 

 
 

Background branch

Proi

Spam

Smam

MAM

Foreground 

things

Panoptic Output

Background 

stuff

Figure 2. The proposed network structure. We adopt FPN as our backbone and share features with three parallel branches, namely
foreground branch, background branch, and RPN branch. In the training stage, the network is optimized in an end-to-end manner. In
the inference stage, panoptic results are generated by things and stuff results following the method described in Section 3.4. “⊕” denotes
element-wise sum and the green “⊗” represents Proposal Attention Module (PAM) or Mask Attention Module (MAM) according to its
position. PAM and MAM model the complementary relation between two branches. Details of PAM and MAM are shown in Figure 3 and
Figure 5. The red and green arrows represent upsample and attention operations, respectively.

local network [37] bridged self-attention for machine trans-
lation [36] to video classiﬁcation using non-local ﬁlters. In
the scope of scene understanding, [42] and [43] aggregated
global contextual information as well as class-dependent
features by channel-attention operations. More recently,
self-attention and channel attention were adopted by [12]
to model long-range contexts in the spatial and channel di-
mensions, respectively. In this work, we establish the rela-
tionship between foreground things and background stuff in
panoptic segmentation with a series of coarse-to-ﬁne atten-
tion blocks.

3. Attention-guided Uniﬁed Network

3.1. Problem and Baselines

Panoptic segmentation task aims at understanding every-
thing visible in one view, which means each pixel of an im-
age must be assigned a semantic label and an instance ID.
To address this issue, the existing top algorithms [1, 23] di-
rectly combined the instance and semantic results from sep-
arate models, such as Mask R-CNN [15] and PSPNet [45].
We formulate the problem of panoptic segmentation as
recognizing and segmenting all FG things and understand-
ing all BG stuff. In this way, we solve the problem from two
aspects, namely foreground branch and background branch
in a uniﬁed network (Figure 2). In detail, given an input
image X, our goal is to generate FG things result YTh and
BG stuff result YSt simultaneously. Thus, the panoptic re-
sult YPa can be generated from YTh and YSt directly us-
ing the fusion method in Section 3.4. The performance of

panoptic results is evaluated by panoptic quality (PQ) [23]
as described in Section 4.1. For this purpose, we ﬁrstly in-
troduce our uniﬁed framework for panoptic segmentation in
this section. Then, key elements in our designed attention-
guided modules are elaborated, including proposal attention
module (PAM) and mask attention module (MAM). Finally,
we give our implementation details.

In this work, we view the method, in which things and
stuff are generated from separate models, as our baseline.
Speciﬁcally, the baseline method gives the result of things
YTh and stuff YSt from separate models MTh and MSt re-
spectively. And the FG model MTh and BG model MSt are
given the similar backbones (e.g., FPN [27]) for the follow-
ing uniﬁed framework.

3.2. Uniﬁed Framework

In order to bridge the gap between FG things with BG
stuff, we propose the Attention-guided Uniﬁed Network
(AUNet). Comparing with the baseline approach, the pro-
posed AUNet fuses two models (MTh and MSt) together
by sharing the same backbone and generates YTh and YSt
from parallel branches. As clearly illustrated in Figure 2,
the AUNet is conceptually simple: FPN is adopted as the
backbone to extract discriminative features from different
scales and shared by all the branches.

Different from traditional approaches, which directly
combine results from MTh and MSt, the proposed AUNet
optimizes them using a joint loss function L (deﬁned in Sec-
tion 3.4) and facilitates both tasks in a uniﬁed framework.
In detail, we adopt a proposal-based instance segmentation

7028

module to generate ﬁner masks M in foreground branch.
And for background branch, light heads are designed to
aggregate scene information from shared multi-scale fea-
tures.
In this way, the shared backbone is supervised by
FG things and BG stuff simultaneously, which promotes the
connection between two branches in feature space. In order
to build up the bond between FG objects and BG contents
more explicitly, two sources of attention modules are added.
We consider the coarse attention operation between the i-th
scale BG feature map with the corresponding RPN feature
map, denoted by Si and Pi respectively. The attention mod-
ule can be formulated as Si ⊗ Pi , where “⊗” denotes atten-
tion operations, as illustrated in Figure 2. Furthermore, the
ﬁner relationship is established by the attention between the
processed feature map Spam and the generated FG segmen-
tation mask Proi, which can be formulated as Spam ⊗ Proi.
Details will be investigated in the following section.

3.3. Attention guided Modules

Considering the complementary relationship between
FG things and BG stuff, we introduce features from fore-
ground branch to background branch for more contextual
cues. From another perspective, the attention operation
connecting two branches also establishes a bond between
proposal-based method and FCN-based method segmenta-
tion. To this end, two spatial attention modules are pro-
posed, namely proposal attention module (PAM) and mask
attention module (MAM).

3.3.1 Proposal Attention Module

In classic two-stage detection frameworks, region proposal
network (RPN) [34] is introduced to give predicted binary
class labels (foreground and background) and bounding-
box coordinates. This means RPN features contain rich
background information which can only be obtained from

CrxW''xH''

CrxW''xH''

RPN branch

Pi

Conv 1x1

ReLU

Mi

Conv 1x1

Conv 3x3

CsxW''xH''

Background branch

Si

M'i

1-Sigmoid

1xW''xH''

Pi

CsxW''xH''

P
A
G

 

1
x
1
v
n
o
C

N
G

1
x
1
x
s
C

i

d
o
m
g
S

i

CsxW''xH''

Ni

S''i

Proposal Attention

Background Reweight

Conv 3x3

S'i

Conv 3x3

Figure 3. The designed proposal attention module (PAM) for com-
plementary relationship establishment. We adopt this block in
each scale of shared features, i.e., W ′′ and H ′′ changes in each
scale. Here, “⊗” denotes spatial element-wise multiplication and
“⊕” denotes element-wise sum. The green arrows represent oper-
ations in PAM. GAP and GN indicate Global Average Pooling and
Group Normalization [38], respectively.

stuff annotations in background branch. Therefore, we pro-
pose a new approach to establish the complementary rela-
tionship between FG elements and BG contents, called Pro-
posal Attention Module (PAM). As shown in Figure 3, we
utilize contextual cues from RPN branch for attention oper-
ation. Here, we give a detailed formulation for this process.
Given an input feature map Pi ∈ RCr
from the
i-th scale RPN branch, the FG weighted map Mi before
sigmoid activation can be formulated as:

×W ′′

×H ′′

Mi = f (σ(f (Pi, wi,1)), wi,2)

(1)

×H ′′

where f (·, ·) denotes a convolution function, σ represents
the ReLU activation function, Mi ∈ R1×W ′′
means
the generated FG weighted map, both wi,1 ∈ RC ′
×1×1
and wi,2 ∈ R1×C ′

×1×1 indicate convolutional parameters.
To emphasize the background contents, we formulate the
i as 1 − sigmoid(Mi). Then, the
can be

attention weighted map M ′
i-th scale activated feature map S ′
presented as:

i ∈ RCs

×W ′′

×H ′′

×Cr

r

r

S ′

i,j = Si,j ⊗ M ′

i ⊕ Si,j

(2)

where ⊗ and ⊕ denotes element-wise multiplication and
sum respectively, Si,j means the j-th layer of semantic fea-
ture map Si ∈ RCs

×W ′′

×H ′′

.

Motivated by [19], a simple background reweight func-
tion is designed to downweight useless background layers
after attention operation. We believe it could be improved,
but it is beyond the scope of this work. The reweighted fea-
ture map S ′′

can be generated as:

×W ′′

×H ′′

i ∈ RCs

Ni = sigmoid(GN(f (G(S ′

i), wi,3)))

S ′ ′

i,k = S ′

i,k ⊗ Ni

(3)

(4)

where G and GN denotes global average pooling and group
×1×1 means reweighting
norm [38] respectively, Ni ∈ RCs
×1×1 represents convolutional pa-
operator, wi,3 ∈ RCs
rameter, and S ′

i,k indicates the k-th pixel channel in S ′
i.

×Cs

Based on the above formulation of PAM, we highlight
the background regions in the shared feature maps via at-
tention operation and background reweight function. It also
facilitates the learning of things in turn by enhancing the
weights of activated foreground regions during backpropa-
gation (see Section 4.2).

3.3.2 Mask Attention Module

With the introduction of contextual cues by PAM, back-
ground branch is encouraged to focus more on the regions of
stuff. However, the predicted coarse areas from RPN branch
lack enough cues for precise BG representations. Unlike
RPN features, the m×m ﬁxed-shape masks generated from
foreground branch encode ﬁner FG layouts. Thus, we pro-
pose Mask Attention Module (MAM) to further model the

7029

relationship, as illustrated in Figure 5. Consequently, the
1 × W ′ × H ′ shape FG segmentation mask is needed for
similar attention operations as before. Now, the problem is:
how to reproduce the W ′ × H ′ shape FG feature map from
m × m masks?
RoIUpsample:
In order to solve the size mismatching
problem, we propose a new differentiable layer called
RoIUpsample. Speciﬁcally, RoIUpsample is designed sim-
ilar to the inverse process of RoIAlign [15], as clearly illus-
trated in Figure 4. In the RoIUpsample layer, the m × m
mask (m equals to 14 or 28 in Mask R-CNN) is ﬁrstly
reshaped to the same size of RoIs (generated from RPN).
Then we utilize the designed inverse bilinear interpolation
to compute values of the output features at four regularly
sampled locations (same with RoIAlign) in each mask bin,
and then sum up the ﬁnal results as the generated mask fea-
ture map. To meet the requirement of bilinear interpola-
tion [21], in which near points are given more contributions,
an operation for inverse bilinear interpolation is formulated:

valuex

R(p1,1) = (1−xp)(1−yp)
×valuey
R(p1,2) = (1−xp)yp
R(p2,1) = xp(1−yp)
R(p2,2) =

×valuey

×valuey

valuex

valuex

xpyp

valuex

×valuey

R(pg)

R(pg)

R(pg)
R(pg)

(5)




where R(pj,k) denotes the result of point pj,k after inverse
bilinear interpolation, R(pg) here equals to one quarter of
the corresponding value in the input mask, and normalized
weights valuex, valuey are deﬁned as:

valuex = x2

p + (1 − xp)2 , valuey = y2

p + (1 − yp)2 (6)

in which xp and yp indicate the distance between grid point
pg and generated p1,1 in two axes respectively, as presented
in Figure 4(b). Note that with the Equation 5 and 6, the m×
m mask can also be reverted from the generated W ′ × H ′
feature map with the forward bilinear interpolation.

Then, the generated feature map is assigned to four dif-
ferent scales according to the size of RoIs, which is similar
with that in FPN [27]. Consequently, the generated FG fea-
ture map is achieved for the following operations.
Attention Operation: Different from traditional instance
segmentation tasks, the predicted FG masks are utilized to
give background branch more contextual guidance in pixel-
level. We ﬁrstly aggregate them together to the Cm × W ′ ×
H ′ feature map using RoIUpsample, as presented in Fig-
ure 5. Then, the ﬁner 1 × W ′ × H ′ activated BG regions
can be produced, similar with that in PAM. With the intro-
duction of attention, the FG masks is also supervised by se-
mantic loss function, which enables a further improvement
in scene understanding (both for things and stuff), as dis-
cussed in Section 4.2. A similar background reweight func-
tion is adopted to aggregate useful highlighted background

Grid points of bilinear 

interpolation

Variable size RoI

p1,1
p1,1

p1,2
p1,2

yp
yp
yp
yp

pg
pg

xp
xp
xp
xp

p2,1
p2,1

p2,2
p2,2

Grid points of inverse 
bilinear interpolation

Variable size RoI

RoIAlign 
output

Fixed dimensional

representation

Conv feature map

(a) RoIAlign process

RoIUpsample 

output

mxm mask

Generated mask feature map

(b) RoIUpsample process

Figure 4. Comparison between RoIAlign [15] and our proposed
RoIUpsample. The designed RoIUpsample, which can be viewed
as an inverse operation of RoIAlign, reverts the feature map from
FG masks according to their accurate spatial locations. Here, we
show an example of RoIAlign output and RoIUpsample input with
m = 2 for an intuitive illustration.

RoIUpsample Feature
RoIUpsample Feature

CmxW'xH'

RoIUpsample

mxm mask

 
 

 
 

 
 

 
 

Proi

Conv 1x1

ReLU

Cs'xW'xH'

Conv 1x1

Cs'xW'xH'

1-Sigmoid

1xW'xH'

P
A
G

 

1
x
1
v
n
o
C

N
G

Background branch

1
x
1
x
s

'

C

i

d
o
m
g
S

i

Spam

Mask Attention

Background Reweight

Cs'xW'xH'

Smam

Figure 5. The proposed mask attention module (MAM) for a ﬁner
relationship modelling. Here, “⊗” denotes spatial element-wise
multiplication and “⊕” denotes element-wise sum. The red and
green arrows represent upsample and operations in MAM respec-
tively. GAP and GN are identical with that in PAM.

features. Consequently, we model the complementary rela-
tionship between FG things and BG stuff with the proposed
PAM and MAM.

3.4. Implementation Details

In this section, we give more implementation details on

the training and inference stage of our proposed AUNet.

Training: As well elaborated in Section 3.2, all of our pro-
posed methods are trained in a uniﬁed framework. The
whole network is optimized via a joint loss function L dur-
ing training stage:

L = λ1LRPN + λ2LRCNN + λ3LMask + λ4LSeg

(7)

7030

where LRPN, LRCNN, LMask , and LSeg denotes the loss
function of RPN, RCNN, instance segmentation, and se-
mantic segmentation, respectively. Speciﬁcally, hyperpa-
rameters are designed to balance training processes, where
λ1 to λ4 are set to {1, 1, 1, 0.3} for MS-COCO and {1,
0.75, 1, 1} for Cityscapes.

In details, we adopt ResNet-FPN [17, 27] as our back-
bone. And the hyperparameters in the foreground branch
are set following Mask R-CNN [15]. The backbone is pre-
trained on ImageNet [35], and the remaining parameters are
initialized following [16]. As standard practice [10, 17, 27],
8 GPUs are used to train all the models. Each mini-batch
has 2 images per GPU for ResNet-50 and ResNet-101 based
networks and 1 image per GPU for the others. The net-
works are optimized for several epochs (18 for MS-COCO
and 100 for Cityscapes) using mini-batch stochastic gradi-
ent descent (SGD) with a weight decay of 4e-5 and a mo-
mentum of 0.9. Batch Normalization [20] in the backbone
is ﬁxed and Group Normalization [38] is added to all of
the branches in our ﬁnal results. For MS-COCO [28], the
learning rate is initialized with 0.02 for the ﬁrst 13 epochs
and divided by 10 at 15-th and 18-th epoch respectively. In-
put images are horizontally ﬂipped and reshaped to the scale
with a 600 pixels short edge during training. Multi-scale
testing is adopted for ﬁnal results 4.3. For Cityscapes [8],
the learning rate is initialized with 0.01 and divided by 10 at
68-th and 88-th epoch respectively. We construct each mini-
batch for training from 16 random 512×1024 image crops
(2 crops per GPU) after randomly ﬂipping and scaling each
image by 0.5 to 2.0×. Multi-scale testing is dropped in 4.3.

Inference: The panoptic results are produced in inference
stage by fusing the results of FG things and BG stuff in a
similar way with that in [23]. In this stage, the overlaps of
things are ﬁrst resolved in a NMS-like procedure which pre-
dicts the segments with higher conﬁdence scores. And the
relationships among categories are also considered during
this procedure. For example, ties should not be overlapped
by person in the ﬁnal result. Then, the non-overlapping in-
stance segments are combined with stuff results by assign-
ing instance label ﬁrst in favor of the things.

4. Experiments

In this section, our approach is evaluated on Microsoft
COCO [28] and Cityscapes [8] datasets. We ﬁrst give de-
scription of the datasets as well as the evaluation metrics.
Then we evaluate our method and give detailed analyses.
Comparison with the state-of-the-art methods in panoptic
segmentation are presented at last.

4.1. Dataset and Metrics

Dataset: Due to the novelty of panoptic task itself, there
are few datasets with detailed panoptic annotations as well

as public evaluation metrics. Microsoft COCO [28] is the
most suitable and challenging one for the new panoptic seg-
mentation task, for the detailed annotations and high data
complexity. It consists of 115k images for training and 5k
images for validation, as well as 20k images for test-dev
and 20k images for test-challenge. MS-COCO panoptic
annotations includes 80 thing categories and 53 stuff cat-
egories. We train our models on train set with no extra data
and reports results on val set and test-dev set for compari-
son. Cityscapes [8] dataset is adopted to further illustrate
the effectiveness of the proposed method. In detail, it con-
tains 2975 images for training, 500 images for validation
and 1525 images for testing with ﬁne annotations. It has
another 20k coarse annotations for training, which are not
used in our experiment. We report our results on val set
with 19 semantic label and 8 annotated instance categories.

Evaluation Metrics: We adopt the evaluation metrics in-
troduced by [23], which computes panoptic quality (PQ)
metric for evaluation. PQ can be explained as the multi-
plication of a segmentation quality (SQ) and a recognition
quality (RQ) term:

PQ =

P(p,g)∈T P IoU (p, g)

|T P |
{z

}
|
segmentation quality(SQ)

×

|T P |

2 |F P | + 1

|T P | + 1
|

{z

2 |F N |
}

recognition quality(RQ)

(8)

where IoU(p, g) means the intersection-over-union be-
tween predicted object p and ground truth g, true positives
(T P ) denotes matched pairs of segments (IoU(p, g) > 0.5),
false positives (F P ) represents unmatched predicted seg-
ments, and false negatives (F N ) means unmatched ground
truth segments. PQ, SQ, and RQ of both thing and stuff are
also reported in our results.

4.2. Component wise Analysis and Diagnosis

In this section, we will decompose our approach step-
by-step to reveal the effect of each component. All ex-
periments in this section are trained and evaluated on MS-
COCO dataset in a single model with no extra data. Here,
we adopt ResNet-50-FPN as our backbone. For fair com-
parison, we strictly follow the merging method in [23] with
no trick or multi-scale data augmentation in training and
inference stage when doing component-wise analyses. As
presented in Table 1, our proposed AUNet achieve an ab-
solute improvement of 2.4% in PQ when compared with
separate training method.

4.2.1 Uniﬁed Framework

As elaborated in Section 3.2, our proposed uniﬁed frame-
work deals with FG things and BG stuff in parallel branches.
As shown in Table 1, the uniﬁed framework boosts up the
performance both in PQSt and PQTh, which brings 1.1%
absolute improvements in PQ. This can be attributed to the

7031

Table 1. Comparison among different settings of panoptic quality
(%) on the MS-COCO dataset. “rewt” means using background
reweight function in PAM and MAM. PQTh and PQSt indicates
PQ for things and stuff respectively.

Method

PAM MAM rewt

PQ PQTh

PQSt

AP mIoU

sep
e2e
PAM
PAMr
MAM
MAMr
AUNet

✗

✗

✓

✓

✗

✗

✓

✗

✗

✗

✗

✓

✓

✓

✗

✗

✗

✓

✗

✓

✓

37.2
38.3
39
39.4
38.9
39.2
39.6

47.1
47.9
48.5
48.9
48.6
48.6
49.1

22.8
23.9
24.5
25.2
24.2
24.9
25.2

33.4
33.7
34.2
34.4
34.3
34.3
34.7

44.5
44.8
45.1
45.3
45.2
45.3
45.1

e
g
a
m

I

t
u
p
n
I

)
e
l
a
c
s
h
t
4
(

M
A
P
n
i

M
A
M
n
i

k
s
a

M
 
d
e
t
a
v
i
t
c
A

k
s
a

M
 
d
e
t
a
v
i
t
c
A

shared backbone and joint optimization, with which the net-
work is supervised to focus on more discriminative features
for both things and stuff. With the shared backbone, the mis-
classiﬁcation in stuff are effectively reduced and the things
are given more details.

4.2.2 Proposal Attention Module

The proposed PAM builds the complementary relationship
between things and stuff from different scales. By this
way, the binary-classiﬁed RPN branch is optimized under
the supervision of semantic labels. With the bond between
stuff and things established, the network performs consis-
tent gain in PQSt and PQTh, as presented in Table 1. The
background reweight function proves its effectiveness in
PQSt. This can be resulted from the global contextual fea-
tures introduced by global average pooling in Equation 3,
which means it chooses to aggregate highlighted BG fea-
tures under the guidance of global context. As shown in
Figure 6, the activated feature map M ′
4 emphasize the back-
ground areas with context cues. It is worth noting that we
have tried other fusion methods for FG and BG feature fu-
sion, such as concatenation and direct summary after fea-
ture transformation. But these strategies have minor contri-
butions, which means the attention is more appropriate for
relationship establishment.

4.2.3 Mask Attention Module

While the PAM establishes the bond between FG objects
and BG contents, the MAM gives background ﬁner repre-
sentations, as elaborated in Section 3.3.2 and Figure 6. As
that in PAM, MAM also achieves better performance over
the raw method in both PQSt and PQTh. However, the con-
tribution of MAM is slightly lower than PAM. We guess
this is caused by the lack of contextual cues in the gener-
ated FG segmentation mask.3 In fact, we also evaluate the
performance when adopting different resolution masks for
RoIUpsample, namely the 14 × 14 mask and the 28 × 28

Figure 6. Heatmaps of the activated BG areas in PAM (the 4th
scale, M ′
4) and MAM. The red regions are assigned more weights
while the blue regions less weights in the background branch. All
the input images are sampled from the MS-COCO val set.

one. The result shows the high resolution mask features
bring a further gain (0.1% absolute improvement in PQ)
over the smaller one. This is reasonable, because RoIUp-
sample layer generates ﬁner layouts if given higher resolu-
tion masks. With the help of background reweight function,
MAMr achieves 39.2% in PQ.

4.3. Comparison to State of the arts

We compare our proposed network with other state-
test-dev and

of-the-art methods on MS-COCO [28]
Cityscapes [8] val set.

MS-COCO: As shown in Table 2, the proposed AUNet
achieves the leading PQ performance 46.5% in MS-COCO
dataset without bells-and-whistles.
In details, winners of
COCO2018 panoptic challenge [1] adopt numerous addi-
tional network enhancements during training and inference
stage, e.g., abundant extra data (110k external annotated
MS-COCO images), multi-scale training, model ensemble.
Moreover, considering the network enhancements adopted
by the winner teams, cascade R-CNN [4] is adopted for
things and extra blocks or label bank [18] are added for
stuff as well. Different from them, the proposed AUNet
achieves the top performance in a uniﬁed framework with
no extra data or additional network enhancements for both
things and stuff. To be more speciﬁc, only one single model
based on the ResNeXt-152-FPN4 is adopted in the AUNet.
Filtering out the improvement bring by model ensem-
ble, we compare the AUNet with “PKU 360” team who
adopted a similar backbone but with additional skills. The
result shows that our algorithm perform better than them
especially in PQSt, for about 4.9% absolute improvements.
Furthermore, the AUNet overpasses the former end-to-end
method, namely JSIS-Net [11], with a 19.3% absolute gap,
which proves the effectiveness of the proposed method. In
Table 2, it is clear that the AUNet have a great balance be-

3We adopt zero padding for vacant areas in RoIUpsample layer, result-
ing in blank BG context. This needs to be investigated in the future works.

4We use the 64×4d variant of ResNeXt [39] with deformable conv [10]

and non-local blocks [37].

7032

Table 2. Panoptic quality (%) on MS-COCO 2018 test-dev. “extra data” here denotes using extra dataset for training, “e2e” represents using
a uniﬁed framework for things and stuff prediction, and “enhanceTh” and “enhanceSt” indicates using additional enhancement techniques
in network heads for things and stuff respectively. PQTh and PQSt means PQ result for things and stuff respectively. We report our single
model results with no extra data or network enhancement.

Method

backbone

extra data

e2e

enhanceTh

enhanceSt

PQ

SQ

RQ PQTh

SQTh RQTh

PQSt

SQSt RQSt

Megvii (Face++)
Caribbean
PKU 360

ensemble model
ensemble model

ResNeXt-152-FPN

JSIS-Net [11]

ResNet-50

Ours
Ours
Ours

ResNet-101-FPN
ResNet-152-FPN
ResNeXt-152-FPN

✓

✗

✗

✗

✗

✗

✗

✗

✗

✗

✓

✓

✓

✓

✓

✓

✓

✗

✗

✗

✗

✓

✓

✓

✗

✗

✗

✗

53.2
46.8
46.3

83.2
80.5
79.6

62.9
57.1
56.1

62.2
54.3
58.6

27.2

71.9

35.9

29.6

45.2
45.5
46.5

80.6
80.8
81.0

54.7
55.0
56.1

54.4
54.7
55.8

85.5
81.8
83.7

71.6

83.3
83.4
83.7

72.5
65.9
69.6

39.4

64.8
65.2
66.3

39.5
35.5
27.6

79.7
78.5
73.6

23.4

72.3

31.3
31.6
32.5

76.6
76.9
77.0

48.5
43.8
35.6

30.6

39.4
39.7
40.7

Table 3. Panoptic quality (%) on the Cityscapes val set. PQTh
and PQSt denotes PQ result for things and stuff respectively. We
compare our results with the bottom-up methods (the ﬁrst row).
Oursequ indicates all things are considered as one category in the
background branch during training.

Method

DWT [3]
SGN [30]
Li et. al. [24]

backbone

PQ PQTh

PQSt

AP mIoU

VGG16
VGG16

-
-

-
-

-
-

ResNet-101

53.8

42.5

62.1

Mask R-CNN [15]

ResNet-50

-

-

Oursequ
Ours
Ours

55.0
ResNet-50-FPN
ResNet-50-FPN
56.4
ResNet-101-FPN 59.0

51.2
52.7
54.8

-

57.8
59.0
62.1

21.2
29.2
28.6

31.5

32.2
33.6
34.4

-
-
-

-

-

73.6
75.6

tween things and stuff, even when comparing with the chal-
lenge winners (no extra data). This is due to the introduc-
tion of uniﬁed framework and attention-guided modules for
complementary relationship establishment, as well elabo-
rated in Section 4.2. Figure 7 gives intuitive presentations
of the top performance using our proposed AUNet.

Cityscapes: We compare our proposed method with the
leading bottom-up methods and Mask R-CNN in Table 3.
Firstly, we adopt the same training strategy with that in MS-
COCO, which means all things are considered as one cate-
gory in background branch, denoted as Oursequ. However,
the strategy is inferior to that when using all 19 semantic
labels, as illustrated in Table 3. Additionally, the MAM,
which is proved to decrease the PQ in Cityscapes, is dis-
abled in the ﬁnal results. We guess the decline is caused by
the inconsistency with prior information 2, which means the
relatively worse things prediction may give wrong cues to
stuff. Overall, the proposed method surpass previous state-
of-the-art [24], with a 5.2% absolute gap.

5. Conclusions

This paper presents AUNet, a uniﬁed framework for
panoptic segmentation. The key difference from prior ap-
proaches lies in that we unify FG (instance-level) and BG
(semantic-level) segmentation into one model, so that the
FG branch, often being better optimized, can assist the BG

(a) Input image

(b) Ground truth

(c) Our results

Figure 7. Example results of AUNet on the MS-COCO val set.
Our performance on things 7(c) is even better than human anno-
tations 7(b). The things of the same class share the same color
family but appear in different intensities.

branch via two sources of attention (i.e., proposal attention
module and mask attention module), which offer object-
level and pixel-level guidance, respectively. In experiments,
we observe consistent accuracy gain in MS-COCO, based
on which new state-of-the-arts are achieved.

Our research delivers an important message:

in visual
tasks, it is often beneﬁcial to partition targets into a few
subclasses according to their properties, so that complemen-
tary information can be propagated across subclasses to as-
sist scene understanding. Panoptic segmentation, being a
new task, offers a natural partition between FG things and
BG stuff, yet more possibilities remain unexplored and to
be studied in the future.

Acknowledgement

We would like to thank Jiagang Zhu and Yiming Hu for
valuable discussions. This work was supported by the Na-
tional Key Research and Development Program of China
No. 2018YFD0400902 and National Natural Science Foun-
dation of China under Grant 61573349.

7033

References

[1] COCO: Panoptic Leaderboard. http://cocodataset.

org/#panoptic-leaderboard. 1, 3, 7

[2] Anurag Arnab and Philip HS Torr.

segmentation with a dynamically instantiated network.
CVPR, 2017. 2

Pixelwise instance
In

[3] Min Bai and Raquel Urtasun. Deep watershed transform for

instance segmentation. In CVPR, 2017. 8

[4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving

into high quality object detection. In CVPR, 2018. 7

[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. PAMI, 2018. 2

[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for semantic
image segmentation. arXiv:1706.05587, 2017. 2

[7] Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and
Alan L Yuille. Attention to scale: Scale-aware semantic im-
age segmentation. In CVPR, 2016. 2

[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016. 2, 6, 7

[9] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-
In

mantic segmentation via multi-task network cascades.
CVPR, 2016. 2

[10] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. In ICCV, 2017. 6, 7

[11] Daan de Geus, Panagiotis Meletis, and Gijs Dubbelman.
Panoptic segmentation with a joint semantic and instance
segmentation network. arXiv:1809.02110, 2018. 2, 7, 8

[12] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Han-
qing Lu. Dual attention network for scene segmentation.
arXiv:1809.02983, 2018. 2, 3

[13] Ross Girshick. Fast r-cnn. In ICCV, 2015. 2
[14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In CVPR, 2014. 2

[15] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-

shick. Mask r-cnn. In ICCV, 2017. 1, 2, 3, 5, 6, 8

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation. In ICCV, 2015. 6

[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 6

[18] Hexiang Hu, Zhiwei Deng, Guang-Tong Zhou, Fei Sha, and
Greg Mori. Labelbank: Revisiting global perspectives for
semantic segmentation. arXiv:1703.09891, 2017. 7

[21] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.

Spatial transformer networks. In NIPS, 2015. 5

[22] Alexander Kirillov, Ross Girshick, Kaiming He, and
Panoptic feature pyramid networks.

Piotr Doll´ar.
arXiv:1901.02446, 2019. 2

[23] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Panoptic segmentation.

and Piotr Doll´ar.

Rother,
arXiv:1801.00868, 2018. 1, 2, 3, 6

[24] Qizhu Li, Anurag Arnab, and Philip HS Torr. Weakly-and
semi-supervised panoptic segmentation. In ECCV, 2018. 2,
8

[25] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.
Fully convolutional instance-aware semantic segmentation.
arXiv:1611.07709, 2016. 2

[26] Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Jianchao Yang,
Liang Lin, and Shuicheng Yan.
Proposal-free network
for instance-level object segmentation. arXiv:1509.02636,
2015. 2

[27] Tsung-Yi Lin, Piotr Doll´ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, 2017. 3, 5, 6

[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, 2014. 2, 6, 7

[29] Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu
Liu, Gang Yu, and Wei Jiang. An end-to-end network for
panoptic segmentation. arXiv:1903.05027, 2019. 2

[30] Shu Liu, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. Sgn:
Sequential grouping networks for instance segmentation. In
ICCV, 2017. 8

[31] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
In

Path aggregation network for instance segmentation.
CVPR, 2018. 2

[32] Yiding Liu, Siyu Yang, Bin Li, Wengang Zhou, Jizheng Xu,
Houqiang Li, and Yan Lu. Afﬁnity derivation and graph
merge for instance segmentation. In ECCV, 2018. 2

[33] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015. 2

[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015. 2, 4

[35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. IJCV, 2015. 6

[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NIPS, 2017. 3

[37] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In CVPR, 2018. 2, 3,
7

[19] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

[38] Yuxin Wu and Kaiming He. Group normalization. In ECCV,

works. In CVPR, 2018. 2, 4

2018. 4, 6

[20] Sergey Ioffe and Christian Szegedy. Batch normalization:
accelerating deep network training by reducing internal co-
variate shift. In ICML, 2015. 6

[39] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In CVPR, 2017. 7

7034

[40] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min
Bai, Ersin Yumer, and Raquel Urtasun. Upsnet: A uniﬁed
panoptic segmentation network. arXiv:1901.03784, 2019. 2
[41] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan
Yang. Denseaspp for semantic segmentation in street scenes.
In CVPR, 2018. 2

[42] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,
Gang Yu, and Nong Sang. Learning a discriminative fea-
ture network for semantic segmentation. arXiv:1804.09337,
2018. 2, 3

[43] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang,
Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Con-
text encoding for semantic segmentation. In CVPR, 2018. 2,
3

[44] Ziyu Zhang, Sanja Fidler, and Raquel Urtasun.

Instance-
level segmentation for autonomous driving with deep
densely connected mrfs. In CVPR, 2016. 2

[45] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
In

Wang, and Jiaya Jia. Pyramid scene parsing network.
CVPR, 2017. 1, 2, 3

[46] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and
Weiming Hu. Distractor-aware siamese networks for visual
object tracking. In ECCV, 2018. 2

[47] Zheng Zhu, Wei Wu, Wei Zou, and Junjie Yan. End-to-end
ﬂow correlation tracking with spatial-temporal attention. In
CVPR, 2018. 2

7035

