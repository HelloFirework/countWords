Timeception for Complex Action Recognition

Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders

QUVA Lab, University of Amsterdam

{nhussein,egavves,a.w.m.smeulders}@uva.nl

Abstract

This paper focuses on the temporal aspect for recogniz-
ing human activities in videos; an important visual cue that
has long been undervalued. We revisit the conventional def-
inition of activity and restrict it to ‚ÄúComplex Action‚Äù: a
set of one-actions with a weak temporal pattern that serves
a speciÔ¨Åc purpose. Related works use spatiotemporal 3D
convolutions with Ô¨Åxed kernel size, too rigid to capture the
varieties in temporal extents of complex actions, and too
short for long-range temporal modeling.
In contrast, we
use multi-scale temporal convolutions, and we reduce the
complexity of 3D convolutions. The outcome is Timeception
convolution layers, which reasons about minute-long tem-
poral patterns, a factor of 8 longer than best related works.
As a result, Timeception achieves impressive accuracy in
recognizing the human activities of Charades, Breakfast Ac-
tions, and MultiTHUMOS. Further, we demonstrate that
Timeception learns long-range temporal dependencies and
tolerate temporal extents of complex actions.

1. Introduction

In ordinary life, activities of daily living pop up fre-
quently. Our conversations include actions like ‚Äúcooking
a meal‚Äù or ‚Äúcleaning the house‚Äù much more frequently than
actions like ‚Äújumping‚Äù or ‚Äúcutting a cucumber‚Äù. The lat-
ter, which we call one-actions, exhibit one visual pattern,
possibly repetitive. They are usually short in time, homoge-
neous in motion and coherent in form. In contrast, cooking
a meal or cleaning the house are very different actions. We
refer to them as complex actions, characterized by: i. They
are typically composed of several one-actions, see Ô¨Ågure 1.
ii. These one-actions, contained in a complex action, ex-
hibit large variations in their temporal duration and tem-
poral order.
iii. As a consequence of the composition, a
complex action takes much longer to unfold. And, by the
in-homogeneity in composition, the complex action needs
to be sampled in full, not to miss crucial parts.

In the recent literature, the main focus is the recogni-
tion of short-range actions like in HMDB, UCF and Kinet-
ics [1, 2, 3]. Few attention has been paid to the recogni-

Figure 1: Properties of complex action ‚ÄúCooking a Meal‚Äù:
composition: consists of several one-actions (Cook, ...),
order: weak temporal order of one-actions (Get √é Wash),
extent: one-actions vary in their temporal extents.

tion of long-range and complex actions, as in Charades and
EventNet [4, 5], which we study here. The Ô¨Årst challenge is
minute-long temporal modeling while maintaining attention
to seconds-long details. Statistical temporal pooling, as ap-
plied in [6, 7] falls short of learning temporal order. Neural
temporal modeling [8, 9] and spatio-temporal convolutions
of various types [10, 11, 12] successfully learns temporal
order of 8 [13] or 128 timesteps [14]. But the computational
cost is far beyond scaling up to 1000 timesteps needed for
complex actions. The second challenge is tolerating varia-
tions in temporal extent and temporal order of one-actions.
Related methods [11, 15] learn spatio-temporal convolu-
tions with Ô¨Åxed-size kernels, which would be too rigid for
complex actions. To address these challenges, we present
Timeception, a novel convolutional layer dedicated only for
temporal modeling.
It learns long-range temporal depen-
dencies with attention to short-range details. Plus, it toler-
ates the differences in temporal extent of one-actions com-
prising the complex action. As a result, we demonstrate
success in recognizing the long and complex actions, and
achieving state-of-the-art-results in Charades [4], Breakfast
Actions [16] and MultiTHUMOS [17].

The novelties of of this paper are: i. We introduce a con-
volutional temporal layer effectively and efÔ¨Åciently learn
minute-long action ranges of 1024 timesteps, a factor of 8
longer than best related work. ii. We introduce multi-scale
temporal kernels to account for large variations in duration
of action components. iii. We use temporal-only convolu-

1254

ùë£1ùë£2ùë£3tions, which are better suited for complex actions than spa-
tiotemporal counterparts.

2. Related Work

Temporal Modeling. The stark difference between video
and image classiÔ¨Åcation is the temporal dimension, which
necessitates temporal modeling. A widely used approach is
statistical pooling: max and average pooling [18, 19], at-
tention pooling [6], rank pooling [20], dynamic images [21]
and context gating [7], to name a few. Beyond statistical
[22] uses Fisher
pooling, vector aggregation is also used.
Vector [23] to aggregate spatio-temporal features over time,
while [9, 24, 25] extend VLAD [26] to use local convolu-
tion features extracted from video frames. The downside
of statistical pooling and vector aggregation is completely
neglecting temporal patterns ‚Äì an important visual cue.

Other strands of work use neural methods for temporal
modeling. LSTMs are used to model the sequence in action
videos [8]. While TA-DenseNet[27] extents DenseNet [28]
to exploit the temporal dimension. To our knownedge, no
substantial improvements have been reported recently.
Short-range Action Recognition. Few works [29] learn
deep appearance features by frame-level classiÔ¨Åcation of
actions, using 2D CNNs. Others complement deep appear-
ance features with shallow motion features, as IDT [30].
Also, auxiliary image representations is fused with RGB
signals:
[31] uses OpticalFlow channels, while [32] use
Dynamic Images. 3D CNNs are the natural evolution of
their 2D counterparts. C3D [11, 10] proposes 3D CNNs to
capture spatio-temporal patterns of 8 frames in a sequence.
In the same vein, I3D [13] inÔ¨Çates the kernels of ImageNet-
pretrained 2D CNN to jump-start the training of 3D CNNs.
While effective in short-range video sequences of few sec-
onds, 3D convolutions are too computationally expensive to
address minute-long videos, which is our focus.
Long-range Action Recognition. To learn long-range tem-
poral patterns, [33] uses CRF on top of CNN feature maps
to model human activities. To learn video-wide represen-
tations, TRN [34] learns relations between several video
segments. TSN [35, 36] learns temporal structure in long
videos. LTC [37] considers different temporal resolutions
as a substitute to bigger temporal windows. Inspired by self-
attention [38], non-local networks [14] proposes a 3D CNN
with a long temporal footprint of 128 timesteps.

All aforementioned methods succeed in modeling tem-
poral footprint of 128 timesteps (‚àº4-5 sec) at max. In this
work, we address complex actions with long-range tempo-
ral dependencies of up to 1024 timesteps, jointly.
Convolution Decomposition. CNNs succeed in learning
spatial [39, 29] and spatiotemporal [11, 10, 40, 37, 41] ac-
tion concepts, but existing convolutions grow heavy in com-
putation, specially at the higher layers where the number of
channels can grow as much as 2k [42]. To control the com-

putational complexity, several works propose the decompo-
sition of 2D and 3D convolutions. Xception [43] argues
that separable 2D convolutions are as effective as typical
2D convolutions. Similarly, S3D [15, 12] considers separa-
ble 2+1D convolutions to reduce the complexity of typical
3D convolutions. ResNet [42] reduces the channel dimen-
sion using 1√ó1 2D convolution before applying the costly
3√ó3 2D spatial convolution. ShufÔ¨ÇeNet [44] models cross-
channel correlation by channel shufÔ¨Çing instead of 1√ó1 2D
convolution. ResNeXt [45] proposes grouped convolutions,
while Inception [46, 47] replaces the Ô¨Åxed-size 2D spatial
kernels into multi-scale 2D spatial kernels of different sizes.
In this work, we propose the decomposition of spa-
tiotemporal convolutions into depthwise-separable tempo-
ral convolutions, which we show to be better suited for long-
range temporal modeling that 2+1D convolutions. More-
over, to account for the differences in temporal extents, we
propose temporal convolutions with multi-scale kernels.

3. Method

3.1. Motivation

Modern 3D CNNs learn spatiotemporal kernels over
three orthogonal subspaces of video information: the tem-
poral (T ), the spatial (S) and the semantic channel subspace
(C). One spatiotemporal kernel w ‚àà RT √óL√óL√óC learns
a latent concept by simultaneously convolving these three
subspaces [11, 13], where T is the number of timesteps, C
is the number of channels, and L is the size of spatial win-
dow. Though, there is no fundamental reason why these
subspaces must be convolved simultaneously. Instead, as
showcased in [15], one can model these subspaces sep-
arately, w ‚àù ws √ó wt, by decomposing w into spatial
ws ‚àà R1√óL√óL√óC and temporal wt ‚àà RT √ó1√ó1√óC kernels.
Strictly speaking, while replacing w with a cascade Àúw =
ws √ó wt is often referred to as ‚Äúdecomposition‚Äù, this opera-
tion is not tensor decomposition ‚Äì there is no strict require-
ment that, at optimality, we have w‚àó ‚â° Àúw‚àó. Instead, as the
cascade Àúw is, by deÔ¨Ånition, computationally more efÔ¨Åcient
than the full kernel w, the only practical requirement is that
the resulting cascade Àúw yields equally good or better accu-
racies for the task at hand. In light of this realization, while
the aforementioned decomposition along the spatial and
temporal axes is intuitive and empirically successful [15],
it is not the only possibility. Therefore, Any other decom-
position is permissible, namely: Àúw = wŒ± √ó wŒ≤ √ó wŒ≥ √ó ...,
as long as some basic principles are maintained for the Ô¨Ånal
cascade Àúw. Generalizing on recent decomposed architec-
tures [12, 43], we identify from the literature three intuitive
design principles for the spatiotemporal CNNs:
i. Subspace Modularity. In the context of deep network
cascades, a decomposition should be modular, such that be-
tween subspaces, it retains the nature of the respective sub-

255

Figure 2: The core component of our method is Timeception layer, left. Simply, it takes as an input the features X; corre-
sponding to T timesteps from the previous layer in the network. Then, it splits them into N groups, and temporally convolves
each group using temporal convolution module, right. It is a novel building block comprising multi-scale temporal-only con-
volutions to tolerate a variety of temporal extents in a complex action. Timeception makes use of grouped convolutions and
channel shufÔ¨Çing to learn cross-channel correlations efÔ¨Åciently than 1√ó1 spatial convolutions.

spaces across subsequent layers. Namely, after a cascade of
spatial and a temporal convolutions, it must be possible that
yet another cascade (of spatial and temporal convolutions)
is possible and meaningful.
ii. Subspace Balance. A decomposition should make sure
that a balance is retained between the subspaces and their
parameterization in different layers. Namely, increasing
the number of parameters for modeling a speciÔ¨Åc subspace
should come at the expense of reducing the number of pa-
rameters of another subspace. A typical example is conven-
tional 2D CNN, in which the spatial subspace (S) is reduced
while the semantic channel subspace (C) is expanded.
iii. Subspace EfÔ¨Åciency. When designing the decomposi-
tion for a speciÔ¨Åc task, we should make sure that the bulk
of the available parameter budget is dedicated to subspaces
that are directly relevant to the task at hand. For instance,
for long-range temporal modeling, a logical choice is a de-
composition that increases the convolutional parameters for
the temporal subspace (T ).

Motivated by the aforementioned design principles, we
propose a new temporal convolution layer for encoding
long-range patterns in complex actions, named Timecep-
tion, see Ô¨Ågure 2. First, we discuss the Timeception layer.
Then we describe how to stack Timeception layers on top
of existing 2D or 3D CNNs.

3.2. Timeception Layer

For modeling complex actions in long videos, our tem-
poral modeling layer faces two objectives. First, we would
like to learn the possible long-range temporal dependencies
between one-actions throughout the entire video, and for a
frame sequence of up to 1000 timesteps. Second, we would
like to tolerate the variations in the temporal extents of one-
actions throughout the video.

Next, we present the Timeception layer, designed with
these two objectives in mind. Timeception is a layer that

sits on top of either previous Timeception layers, or a CNN.
The CNN can be either purely spatial; processing frames
independently, like ResNet [42], or short-range spatiotem-
poral; processing nearby bursts of frames, like I3D [13].

Long-range Temporal Dependencies. There exist two de-
sign consequences for modeling long-range temporal de-
pendencies between one-actions throughout the video. The
Ô¨Årst consequence is that our temporal network must be com-
posed of deeper stacks of temporal layers. Via successive
layers, thereafter, complex and abstract spatiotemporal pat-
terns can emerge, even when they reside at temporally very
distant locations in the video. Given that we need deeper
temporal stacks and we have a speciÔ¨Åc parameter budget
for the complete model, the second consequence is that the
temporal layers must be as cost-effective as possible.

Revisiting the cost-effectiveness of spatiotemporal mod-
els, existing architectures rely either on joint spatiotemporal
kernels [13] with parameter complexity O(T ¬∑ L2 ¬∑ C) or de-
composed spatial and temporal kernels [15, 12] with param-
eter complexity O((L2 + T ) ¬∑ C). To make the Timeception
layer temporally cost-effective, according to the third de-
sign principle of subspace importance, we opt for trading
spatial and semantic complexity for longer temporal win-
dows. SpeciÔ¨Åcally, we propose depthwise-separable tem-
poral convolution with kernel wT C
t ‚àà RT√ó1√ó1√ó1. Hereafter,
we refer to this convolution as temporal-only. What is more,
unlike [13, 15, 12], we propose to focus only on temporal
modeling and drop the spatial kernel ws ‚àà R1√óL√óL√óC alto-
gether. Hence, the Timeception layer relies completely on
the preceding CNN for the detection of any spatial pattern.

The simpliÔ¨Åed temporal-only kernel has some interest-
ing properties. Each kernel acts on only one channel. As
the kernels do not extend to the channel subspace, they
are encouraged to learn generic and abstract, rather than
semantically-speciÔ¨Åc, temporal combinations. For instance,
the kernels learn to detect the temporal pattern of one latent

256

GroupTemp ConvTemp ConvT√óL√óL√óC‚Ä¢ ‚Ä¢ ‚Ä¢N GroupsConcat+ ShuffleMax 1Dk=2, s=2T√óL√óL√óC/NT√óL√óL√óC/NT√óL√óL√óCT/2√óL√óL√óC(a) TimeceptionLayerConv 1Dk=3, s=1Max 1Dk=2, s=1Conv 1Dk=7, s=1Conv 1Dk = 5, s = 1Conv 2Dk=1x1, s=1ConcatConv 2Dk=1x1, s=1Conv 2Dk=1x1, s=1Conv 2Dk =1x1, s=1T√óL√óL√óC/NT√óL√óL√óC/NT√óL√óL√óC/NT√óL√óL√óC/(M.N)T√óL√óL√ó5C/(M.N)Conv 2Dk =1x1, s=1(b) Temporal Conv Moduleconcept represented by one channel. Last, as the parameter
complexity of a single Timeception layer is approximately
O(T + logL), it is computationally feasible to train a deep
model to encode temporal patterns of up to 1024 timesteps.
This amounts to about 40 seconds of video sequences.

Unfortunately, by stacking temporal-only convolutions
one after the other, we violate the Ô¨Årst design principle of
subspace modularity. The reason is that the semantic sub-
space in long-range spatiotemporal patterns is ignored. To
this end, we propose to use channel grouping operation [45]
before the temporal-only convolutions and channel shuf-
Ô¨Çing operation [44] after the temporal-only convolutions.
The purpose of channel grouping is reducing the complexity
of cross-channel correlations, by modeling it separately for
each group. Clearly, as each group contains a random sub-
set of channels, not all possible correlations are accounted
for. This is mitigated by channel shufÔ¨Çing and channel con-
catenation, which makes sure that the channels are grouped
altogether albeit in a different order. As such, the next Time-
ception layer will group a different subset of channels. To-
gether, channel grouping and channel shufÔ¨Çing is more cost-
effective operation to learn cross-channel correlations than
1√ó1 2D convolutions [43].
Tolerating Variant Temporal Extents. The second objec-
tive for the Timeception layer is to tolerate the differences
in temporal extents of complex actions. While in the previ-
ous description we assume a Ô¨Åxed length for the temporal-
only kernels, one-actions in a complex video may vary in
length. To this end, we propose to replace Ô¨Åxed-size tem-
poral kernels with multi-scale temporal kernels. There are
two possible ways to implement multi-scale kernels, see Ô¨Åg-
ure 3. The Ô¨Årst way, inspired by Inception [46] for images,
is to adopt K kernels, each of a different size k. The second
way, inspired by [48], is to employ dilated convolutions.

The temporal convolution module, see Ô¨Ågure 2b, takes
as an input the features of one group Xn ‚àà RT√óL√óL√ó[C/N ].
Then it applies Ô¨Åve temporal operations in total. The Ô¨Årst
three operations are temporal convolutions with kernel sizes
k = {3, 5, 7}, each maintaining the number of channels at
C/N . The forth operation is a temporal max-pooling with
stride s = 1 and kernel size k = 2. Its purpose is to max-
out activations over local temporal window (k = 2), instead
of convolving them. The Ô¨Åfth operation is simply a dimen-
sion reduction for the input feature Xn, using a 1√ó1 spatial
convolution. To maintain a manageable number of dimen-
sions for the output, the input to the Ô¨Årst fours operations
are shrinked by a factor of M using a 1√ó1 spatial convolu-
tion . After the channel reduction, all Ô¨Åve outputs are con-
catenated across channel dimension, resulting in an output
Yn ‚àà RT √óL√óL√ó(5C/M N ).
Summary of Timeception. A Timeception layer, see
Ô¨Ågure 2a, expects an input feature X ‚àà RT √óL√óL√óC
from the previous layer in the network. The features X

Figure 3: To tolerate temporal extents, we use multi-scale
temporal kernels, with two options: i. different kernel sizes
k ‚àà {1, 3, 5, 7} and Ô¨Åxed dilation rate d = 1, ii. different
dilation rates d ‚àà {1, 2, 3} and Ô¨Åxed kernel size k = 3.

across the channel dimension are then split into N chan-
nel groups. Each group Xn ‚àà RT √óL√ó√óL√ó[C/N ] is con-
volved with the temporal convolution module, resulting in
Yn ‚àà RT √óL√ó√óL√ó[5C/M N ]. This module expands the num-
ber of channels per group by a factor of 5/M . After that,
the features of all groups Y = {Yn | n ‚àà [1, ..., N ]}
are concatenated across the channel axis and then randomly
shufÔ¨Çed. Last, to adhere to the second design principle of
subspace balance, the Timeception layer concludes with a
temporal max pooling of kernel size k = 2 and stride s = 2.
The reason is that while the channel subspace expands by a
factor of 5/M after each Timeception layer, the temporal
subspace shrinks by a factor of 2.

3.3. The Final Model

The Ô¨Ånal model consists of four Timeception layers
stacked on top of the last convolution layer of a CNN, used
as backbone. We explore two backbone choices: a spatial
2D CNN and a short-range spatiotemporal 3D CNN.
2D CNN. The Ô¨Årst baseline uses ResNet-152 [42] as back-
bone. It takes as an input 128 video frames, and processes
them, up to the last spatial convolution layer res5c. Thus,
the corresponding output for the input frames is the feature
X ‚àà R128√ó7√ó7√ó2048. Then, we proceed with four suc-
cessive layers of Timeception, with BatchNorm and ReLU.
Each has channel expansion factor of 5/M = 5/4 =
1.25, M = 4 and temporal reduction factor of 2. Thus, the
resulting feature is Y ‚àà R8√ó7√ó7√ó5000. To further reduce
the spatial dimension, we follow the convention of CNNs
by using spatial average pooling, which results in the fea-
ture Y‚Ä≤ ‚àà R8√ó5000. And to Ô¨Ånally reduce the temporal di-
mension, we use depthwise-separable temporal convolution
with kernel size k ‚àà R8√ó1√ó1√ó1 with no zero-padding. The
resulted feature Z ‚àà R5000 is classiÔ¨Åed with a two-layer
MLP, with BatchNorm and ReLU.
3D CNN. The second baseline uses I3D [13] as backbone.
It takes as an input 128 video segments (each has 8 succes-
sive frames), and independently processes these segments,
up to the last spatiotemporal convolution layer mixed-5c.
Thus, the corresponding output for the input segments is the
feature X ‚àà R128√ó7√ó7√ó1024. The rest of this baseline is no
different than the previous one. The beneÔ¨Åt of using I3D is

257

ùëëùëòùë£1ùë£ùëõùë£2ùëò=1,ùëë=1ùëò=3,¬†ùëë=1ùëò=5,ùëë=1ùëò=7,¬†ùëë=1ùëò=1,ùëë=1ùëò=3,¬†ùëë=1ùëò=3,ùëë=2ùëò=3,¬†ùëë=3that the Timeception layers learn long-range temporal com-
binations of short-range spatiotemporal patterns.
Implementation. When training the model on a speciÔ¨Åc
dataset, Ô¨Årst we pretrain the backbone CNN on this dataset.
We use uniformly sampled frames for the 2D backbone and
uniformly sampled video segments (each has 8 successive
frames) for the 3D backbone. After pre-training, we plug-in
Timeception and MLP layers on top of the last convolution
layer of the backbone and Ô¨Åne-tune the model on the same
dataset. At this stage, only Timeception layers are trained,
while the backbone CNN is frozen. The model is trained
with batch-size 32 for 100 epoch. It is optimized with SGD
with 0.1, 0.9 and 1e‚àí5 as learning rate, momentum and
weight decay, respectively. Our public implementation [49]
uses TensorFlow [50] and Keras [51].

4. Experiments

4.1. Datasets

The scope of this paper is complex actions with their
three properties: composition, temporal extent and tempo-
ral order ‚Äìsee Ô¨Ågure 1. Thus, we choose to conduct our
experiments on Charades [4], Breakfast Actions [16] and
MultiTHUMOS [17]. Other infamous datasets for action
recognition do not meet the properties of complex actions.
Charades is multi-label, action classiÔ¨Åcation, video dataset
with 157 classes.
It contains 8k, 1.2k and 2k videos for
training, validation and test splits, respectively (67 hrs for
training split). On average, each complex action (i.e. each
video) is 30 seconds and contains 6 one-actions. Thus, Cha-
rades meets the criteria of complex actions. We use mean
Average Precision (mAP) for evaluation. As labels of test
set are held out, we report results on the validation set, sim-
ilar to all related works [33, 9, 33, 14, 52].
Breakfast Actions is a dataset for unscripted cooking-
oriented human activities. It contains 1712 videos in total,
1357 for training and 335 for test. The average length of
videos is 2.3 minutes. It is a video classiÔ¨Åcation task of 12
categories of breakfast activities, where each video repre-
sents only one activity. Besides, each video has temporal
annotation of one-actions composing its activity. In total,
there are 48 classes of one-actions. In our experiments, we
only use the activity annotation, and we do not use the tem-
poral annotation of the one-actions.
MultiTHUMOS is a dataset
for human activities in
untrimmed videos, with the primary focus on temporal lo-
calization. It contains 65 action classes and 400 videos (30
hrs). Each video can be thought of a complex action, which
comprises 11 one-actions on average. MultiTHUMOS ex-
tends the original THUMOS-14 [53] by providing multi-
label annotation for the videos in validation and test splits.
Having multiple and dense labels for the video frames en-
able temporal models to beneÔ¨Åt from the temporal relations

between one-actions across the video. Similar to Charades,
mAP is used for evaluation.

4.2. Tolerating Temporal Extents

In this experiment, we evaluate the capacity of the multi-
scale kernels to tolerate the differences in temporal extents
of actions. The experiment is carried out on Charades.

Figure 4: We split a video of 128 timesteps into segments of
equal length (left, before alteration), and alter their temporal
extents by expansion and shrinking (right, after alteration).
We use 4 types of alterations:(a) very-coarse, (b) coarse, (c)
Ô¨Åne, and (d) very-Ô¨Åne. Numbers in boxes are timesteps.

Original v.s. Altered Temporal Extents First, we train
two baselines, one with multi-scale temporal kernels (as in
Timeception) and the other with Ô¨Åxed-size kernels. The
training is done on the original temporal extent of training
videos. Then, at test time only, we alter the temporal ex-
tents of test videos. SpeciÔ¨Åcally, we split each test video
into segments. Then, we temporally expand or shrink these
segments. Expansion is done by repeating frames, while
shrinking is done by dropping frames. We use 4 types of
alterations with varying granularity to test the model in dif-
ferent scenarios: (a) very-coarse, (b) coarse, (c) Ô¨Åne, and
(d) very-Ô¨Åne, see in Ô¨Ågure 4.

Altered Extent

Percentage Drop ‚Üì in mAP

I3D

ResNet

Fixed ‚Üì Multi ‚Üì Fixed ‚Üì Multi ‚Üì

(a) very-coarse
(b) coarse
(c) Ô¨Åne
(d) very-Ô¨Åne

2.09
2.92
1.74
2.18

1.75
2.44
1.12
1.71

1.52
3.26
1.59
1.38

1.08
2.15
1.13
1.20

Table 1: Timeception, with multi-scale kernel, tolerates the
altered temporal extents better than Ô¨Åxed-size kernels. We
report the percentage drop in mAP (lower is better) when
testing on original v.s. altered videos of Charades. I3D and
ResNet are backbone CNNs.

The results of this controlled experiment are shown in
table 1. We observe that Timeception is more effective than
Ô¨Åxed-size kernels in handling unexpected variations in tem-
poral extents. The same observations is conÔ¨Årmed using
either I3D or ResNet as backbone architecture.
Fixed-size vs. Multi-scale Temporal Kernels This ex-
periment points out the merit of using multi-scale tem-

258

43232poral kernels. For this, we compare Ô¨Åxed-size tempo-
ral convolutions against multi-scale temporal-only convo-
lutions, either with different kernel sizes k or dilation rates
d. And we train 3 baseline models with different conÔ¨Åg-
urations of k, d:
i. Fixed kernel size and Ô¨Åxed dilation
rate d = 1, k = 3. This is the typical conÔ¨Åguration used
ii. Different kernel sizes
in 3D CNNs [11, 13, 14, 15].
k ‚àà {1, 3, 5, 7} and Ô¨Åxed dilation rate d = 1.
iii. Fixed
kernel size k = 3 and different dilation rates d ‚àà {1, 2, 3}.
The result of this experiment are shown in table 2. We
observe that using multi-scale kernels is better suited for
modeling complex actions than Ô¨Åxed-size kernels. The
same observation holds for both I3D and ResNet as back-
bones. Also, we observe little to no change in performance
when using different dilation rates d instead of different ker-
nel sizes k.

Kernel

Kernel

Dilation

mAP (%)

Type

Size (k)

Rate (d)

ResNet

I3D

Multi-scale

Fixed-size

1,3,5,7

3

3

1

1,2,3

30.82
30.37

33.76
33.89

1

29.30

31.87

Table 2: Timeception, using multi-scale kernels (i.e. differ-
ent kernel sizes (k) or dilation rates (d), outperforms Ô¨Åxed-
size kernels on Charades. I3D/ResNet are backbone.

4.3. Long range Temporal Dependencies

In this experiment, we demonstrate the capacity of mul-
tiple Timeception layers to learn long-range temporal de-
pendencies for complex actions. We train several baseline
models equipped with Timeception layers. These baselines
use different number of input timesteps. We experiment on
Charades, with both ResNet and I3D as backbone.
ResNet is used, with a different number of timesteps as in-
puts: T ‚àà {32, 64, 128}, followed by Timeception layers.
ResNet processes one frame at a time. Hence, in one feed-
forward pass, the number of timesteps consumed by Time-
ception layers is equal to that consumed by ResNet.
I3D is considered, with a different number of timesteps as
inputs: T ‚àà {256, 512, 1024}, followed by Timeception
layers. I3D processes 8 frames into one super-frame at a
time. Thus, Timeception layers model T ‚Ä≤ ‚àà {32, 64, 128}
super-frames, Practically however, as each super-frame is
related to a segment of 8 frames, both I3D+Timeception
process in total T ‚àà {256, 512, 1024} frames.

We report results in table 3 and we make two observa-
tions. First, stacking Timeception layers leads to an im-
proved accuracy when using both ResNet and I3D as back-
bone. As the only change between these models is the
number of Timeception layers, we deduce that the Time-
ception layers have succeeded in learning temporal abstrac-

Baseline

CNN Steps TC Steps Params mAP (%)

+ 3 TC
ResNet + 3 TC
+ 4 TC

+ 3 TC
I3D + 3 TC
+ 4 TC

32
64
128

256
512
1024

32
64
128

32
64
128

3.82
3.82
5.58

1.95
1.95
2.83

30.37
31.25
31.82

33.89
35.46
37.19

Table 3: Timeception layers allow for deep and efÔ¨Åcient
temporal models, able to learn the temporal abstractions
needed to learn complex actions. Columns are: Baseline:
backbone CNN + how many Timception layers (TC) on top
of it, CNN Steps: input timesteps to the CNN, TC Steps: in-
put timesteps to the Ô¨Årst Timeception layer, Params: num-
ber of parameters used by Timeception layers, in millions.

tions. Second, despite stacking more and more Timecep-
tion layers, the number of parameters is controlled.
In-
terestingly, using 4 Timeception layers on I3D processing
1024 timesteps requires half the parameters needed for a
ResNet processing 128 timesteps. The reason is the num-
ber of channels from ResNet is twice as much as from I3D
(2048 v.s. 1024). We conclude that Timeception layers al-
low for deep and efÔ¨Åcient models, able to learn long-range
temporal abstractions, which is crucial for complex actions.

Learned Weights of Timeception. Figure 5 visualizes the
learned weights by our model. SpeciÔ¨Åcally, three Timecep-
tion layers trained on top of I3D backbone. The Ô¨Ågure de-
picts the weights of multi-scale temporal convolutions with
different kernel sizes k ‚àà {3, 5, 7}. For simplicity, only
the Ô¨Årst 30 kernels from each kernel-size, are shown. We
make two remarks for these learned weights. First, at layer
1, we notice that long kernels (k = 7) captures Ô¨Åne-grained
temporal dependencies, because of the rapid transition of
kernel weights. But at layer 3, these long kernels tend to
focus on coarse-grained temporal correlations, because of
the smooth transition between kernel weights. The same
behavior prevails for the short (k = 3) and medium (k = 5)
kernels. Second, at layer 3, we observe that long-range and
short-range temporal patterns are learned by short kernels
(k = 3) and long kernels (k = 7), respectively. The con-
clusion is that for complex actions, both video-wide and lo-
cal temporal reasoning, even at the top layer, is crucial for
recognition.

4.4. Effectiveness of Timeception

To demonstrate the effectiveness of Timeception, we
compare it against related temporal convolution layers: i.
separable temporal convolution [12], that models both T , C
simultaneously. ii. grouped separable temporal convolution
to model T , followed by 1√ó1 2D convolution to model C.

259

Figure 5: The learned weights by temporal convolutions of three Timeception layers. Each uses multi-scale convolutions
with varying kernel sizes k ‚àà {3, 5, 7}. In bottom layer (1), we notice that long kernels (k = 7) captures Ô¨Åne-grained
temporal dependencies. But at the top layer (3), the long kernels tend to focus on coarse-grained temporal correlation. The
same behavior prevails for the shot (k = 3) and medium (k = 5) kernels.

iii. grouped separable temporal convolution to model T ,
followed by channel shufÔ¨Çing to model C. Interestingly in
Ô¨Ågure 6 top, Timeception is very efÔ¨Åcient in maintaining a
reasonable increase in number of parameters as the network
goes deeper. Also, Ô¨Ågure 6 bottom shows how Timeception
improves mAP on Charades, scales up temporal capacity of
backbone CNNs while maintaining the overall model size.

Figure 6: Top: the cost of adding new Timeception layers
is marginal, compared to related temporal layers. Bottom:
Timeception improves performance, scales up temporal ca-
pacity of backbone CNNs while maintaining the model size.

4.5. Experiments on Benchmarks

Charades is used to evaluate our model, and to compare
against related works. In this experiment, our baseline net-
works use 4 Timeception layers. The number of convolu-
tional groups is 8 for I3D and 16 for ResNet, be it 2D or
3D. The results in table 4 shows that Timeception mono-
tonically improves the performance of the backbone CNN.

The absolute gain on top of ResNet and I3D is 8.8% and
4.3%, respectively.

Ours

Method

Modality mAP (%)

RGB + Flow 18.6
[33] Two-stream
[33] Two-stream + LSTM RGB + Flow 17.8
[9] ActionVLAD
21.0
RGB + iDT
RGB + Flow 22.4
[33] Temporal Fields
[34] Temporal Relations
25.2

RGB

[54] ResNet-152

ResNet-152 + TC

[13] I3D

I3D + TC

‚úì

‚úì

RGB
RGB

RGB
RGB

[14] 3D ResNet-101
[14] 3D ResNet-101 + NL
[52] 3D ResNet-50 + GCN RGB + RP
[52] 3D ResNet-101 + GCN RGB + RP

RGB
RGB

‚úì

3D ResNet-101 + TC

RGB

22.8
31.6

32.9
37.2

35.5
37.5
37.5
39.7
41.1

Table 4: Timeception (TC) outperforms related works using
the same backbone CNN. It achieves the absolute gain of
8.8% and 4.3% over ResNet and I3D, respectively. More
over, using the full capacity of Timeception improves 1.4%
over best related work.

Beyond the overall mAP, how beneÔ¨Åcial is Timeception?
And in what cases exactly does it help? To answer this ques-
tion, we make two comparisons to assess the relative per-
formance of Timeception. We experiment two scenarios: i.
short-range (32 timesteps) v.s. long-range (128 timesteps),
ii. Ô¨Åxed-scale v.s. multi-scale kernels. The results are
shown in Ô¨Ågures 7, 8, and we make two observations.

First, when comparing the relative performance of multi-
scale vs. Ô¨Åxed-size Timeception, see Ô¨Ågure 7, we observe
that multi-scale Timeception excels in complex actions with
dynamic temporal patterns. As an example, ‚Äútake clothes +

260

123456Temporal Layers050100Parameters (Millions)separablegrouped separablegrouped separable + shufflingTimeception25303540Performance mAP %101102103Timesteps40M30M20MR2DR2D + TCR3DR3D + TCR3D + NLR3D + GCNI3DI3D + TCMethod

I3D
I3D + TC

3D ResNet-50
3D ResNet-50 + TC

Activities (Acc. %) Actions (mAP %)

64.31
69.30

66.73
71.25

47.71
56.36

53.27
59.64

Table 5: Timeception outperform baselines in recognizing
the long-range activities of Breakfast dataset.

MultiTHUMOS is used as a third dataset to experiment
our model. This helps in investigating the generality on dif-
ferent datasets. Related works use this dataset for tempo-
ral localization of one-actions in each video of complex ac-
tion. Differently, we use this dataset to serve our objective:
multi-label classiÔ¨Åcation of complex actions, i.e. the entire
video. As such, the evaluation method used is mAP [55]. To
assess the performance of our model, we compare against
I3D as a baseline. As shown in the results in table 6, Time-
ception, equipped with multi-scale kernels outperforms that
with Ô¨Åxed-size kernel.

Method

Kernel
Size k

Dilation
Rate d

mAP (%)

I3D
I3D + Timeception
I3D + Timeception
I3D + Timeception

‚Äì
3
3

1,3,5,7

‚Äì
1

1,2,3

1

72.43
72.83
74.52
74.79

Table 6: Timeception, with multi-scale temporal kernels,
helps baseline models to capture the long-range dependen-
cies between one-actions in videos of MultiTHUMOS.

5. Conclusion

Complex actions such as ‚Äúcooking a meal‚Äù or ‚Äúcleaning
the house‚Äù can only be recognized when processed fully.
This is in contrast to one-actions, that can be recognized
from a small burst of frames. This paper presents Timecep-
tion, a novel temporal convolution layer for complex action
recognition. Thanks to using efÔ¨Åcient temporal-only convo-
lutions, Timeception can scale up to minute-long temporal
modeling. In addition, thanks to multi-scale temporal con-
volutions, Timeception can tolerate the changes in temporal
extents of complex actions. Interestingly, when visualizing
the temporal weights we observe that earlier timeception
layers learn fast temporal changes, whereas later timecep-
tion layers focus on more global temporal transitions. Eval-
uating on popular benchmarks, the proposed Timeception
improves the state-of-the-art notably.

Acknowledgment

We thank Xiaolong Wang [14] for sharing the code.

261

Figure 7: Multi-scale Timeception outperforms the Ô¨Åxed-
kernel when complex actions are dynamic, in green. But
when complex actions with rigid temporal patters, Ô¨Åxed-
size performs better than multi-scale, in orange.

tidy clothes + put clothes‚Äù, one actor may take longer than
others to tidy clothes. In contrast, Ô¨Åxed-size Timeception
excels in the cases where the complex action is more rigor-
ous in the temporal pattern, e.g. ‚Äúopen window + close win-
dow‚Äù. Second, when comparing the relative performance of
short-range (32 timesteps) v.s. long-range (1024 timesteps)
Timeception, see Ô¨Ågure 8, the later excels in complex ac-
tions than requires the entire video to unfold, e.g. ‚ÄúÔ¨Åx door
+ close door‚Äù. However, short-range Timeception would do
better in one-actions, like ‚Äúopen box + close box‚Äù or ‚Äúturn
on light + turn of light‚Äù.

Figure 8: Long-range Timeception outperforms the short-
range version when complex actions need the entire video
to unfold, in green. However, we see one-actions that short-
range Timeception can easily capture, in orange.

Breakfast Actions is used as a second dataset to experiment
our model. The average length of a video in this datataset
is 2.3 sec. For this experiment, we use 3 layers of Timecep-
tion. And as for the backbone, we use I3D and 3D ResNet-
50. None of the backbones is Ô¨Åne-tuned on this dataset, only
Timeception layers are trained. To make one video consum-
able by our baseline, from each video we uniformly sample
64 video snippet, each of 8 sucessive frames. That makes
the total timesteps modeled by the baseline is 512. Finally,
we report result in table 5.

0.10.00.1mAP Differencesit on table, hold paper, work on paperwash cup, glass, bottle, dishopen fridge, get something, close fridgetaking shoes, Walking through doorwaytake clothes, tidy clothes, put clothestake blanket, tidy blanket, put blanketpour cup/glass/bottlehold pillow, throw pillowsit on chair, take cup/glass/bottleopening a window, close window0.10.00.1mAP Differencefix door, close doorwash table, put on tablesomeone run, someone stand/sitopen cabinet, take food, close cabinetopen laptop, put laptop, watch laptoptidy broom, tidy floorhold box, take boxOpeen box, close boxopen laptop, watch laptopturn on light, turn off lightReferences

[1] Hildegard Kuehne, Hueihan Jhuang, Est¬¥ƒ±baliz Garrote,
Tomaso Poggio, and Thomas Serre. Hmdb: a large video
database for human motion recognition. In ICCV, 2011. 1

[2] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. In CRCV-TR, 2012. 1

[3] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, et al. The kinetics
human action video dataset. In arXiv, 2017. 1

[4] Gunnar A Sigurdsson, G¬¥ul Varol, Xiaolong Wang, Ali
Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in
homes: Crowdsourcing data collection for activity under-
standing. In ECCV, 2016. 1, 5

[5] Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, and
Shih-Fu Chang. Eventnet: A large scale structured concept
library for complex event detection in video. In ACM MM,
2015. 1

[6] Rohit Girdhar and Deva Ramanan. Attentional pooling for

action recognition. In NIPS, 2017. 1, 2

[7] Antoine Miech, Ivan Laptev, and Josef Sivic. Learnable
pooling with context gating for video classiÔ¨Åcation. In arXiv,
2017. 1, 2

[8] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko,
and Trevor Darrell. Long-term recurrent convolutional net-
works for visual recognition and description. In CVPR, 2015.
1, 2

[9] Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic,
and Bryan Russell. Actionvlad: Learning spatio-temporal
aggregation for action classiÔ¨Åcation. In CVPR, 2017. 1, 2, 5,
7

[10] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convo-
In

lutional neural networks for human action recognition.
TPAMI, 2013. 1, 2

[11] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In ICCV, 2015. 1, 2, 6

[12] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. In CVPR, 2018. 1, 2, 3,
6

[13] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In CVPR,
2017. 1, 2, 3, 4, 6, 7

[14] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In CVPR, 2018. 1, 2, 5,
6, 7, 8

[15] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and
Kevin Murphy. Rethinking spatiotemporal feature learning
for video understanding. In ECCV, 2018. 1, 2, 3, 6

[16] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language
of actions: Recovering the syntax and semantics of goal-
directed human activities. In CVPR, 2014. 1, 5

[17] Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo An-
driluka, Greg Mori, and Li Fei-Fei. Every moment counts:

Dense detailed labeling of actions in complex videos.
IJCV, 2018. 1, 5

In

[18] Noureldien Hussein, Efstratios Gavves, and Arnold WM
Smeulders. UniÔ¨Åed embedding and metric learning for zero-
exemplar event detection. In CVPR, 2017. 2

[19] Amirhossein Habibian, Thomas Mensink, and Cees GM
Snoek. Video2vec embeddings recognize events when ex-
amples are scarce. In TPAMI, 2017. 2

[20] Basura Fernando, Efstratios Gavves, Jos¬¥e Oramas, Amir
Ghodrati, and Tinne Tuytelaars. Rank pooling for action
recognition. In TPAMI, 2017. 2

[21] Hakan Bilen, Basura Fernando, Efstratios Gavves, Andrea
Vedaldi, and Stephen Gould. Dynamic image networks for
action recognition. In CVPR, 2016. 2

[22] Dan Oneata, Jakob Verbeek, and Cordelia Schmid. Action
and event recognition with Ô¨Åsher vectors on a compact fea-
ture set. In ICCV, 2013. 2

[23] Jorge S¬¥anchez, Florent Perronnin, Thomas Mensink, and
Jakob Verbeek. Image classiÔ¨Åcation with the Ô¨Åsher vector:
Theory and practice. In IJCV, 2013. 2

[24] Ionut Cosmin Duta, Bogdan Ionescu, Kiyoharu Aizawa, and
Nicu Sebe. Spatio-temporal vector of locally max pooled
features for action recognition in videos. In PCVPR, 2017. 2
[25] Ionut C Duta, Bogdan Ionescu, Kiyoharu Aizawa, and Nicu
Spatio-temporal vlad encoding for human action

Sebe.
recognition in videos. In ICMM, 2017. 2

[26] Relja Arandjelovic and Andrew Zisserman. All about vlad.

In CVPR, 2013. 2

[27] Amir Ghodrati, Efstratios Gavves, and Cees GM Snoek.
Video time: Properties, encoders and evaluation. In BMVC,
2018. 2

[28] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens
van der Maaten. Densely connected convolutional networks.
In CVPR, 2017. 2

[29] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video
classiÔ¨Åcation with convolutional neural networks. In CVPR,
2014. 2

[30] Christoph Feichtenhofer, Axel Pinz, and AP Zisserman.
Convolutional two-stream network fusion for video action
recognition. In CVPR, 2016. 2

[31] Karen Simonyan and Andrew Zisserman. Two-stream con-
In

volutional networks for action recognition in videos.
NIPS, 2014. 2

[32] Hakan Bilen, Basura Fernando, Efstratios Gavves, and An-
drea Vedaldi. Action recognition with dynamic image net-
works. In TPAMI, 2017. 2

[33] Gunnar A Sigurdsson, Santosh Divvala, Ali Farhadi, and Ab-
hinav Gupta. Asynchronous temporal Ô¨Åelds for action recog-
nition. In CVPR, 2017. 2, 5, 7

[34] Bolei Zhou, Alex Andonian, and Antonio Torralba. Tempo-

ral relational reasoning in videos. In ECCV, 2018. 2, 7

[35] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment
networks: Towards good practices for deep action recogni-
tion. In ECCV, 2016. 2

[36] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment

262

networks for action recognition in videos. In TPAMI, 2018.
2

[37] Gul Varol, Ivan Laptev, and Cordelia Schmid. Long-term
In TPAMI,

temporal convolutions for action recognition.
2017. 2

[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NIPS, 2017. 2

[39] Limin Wang, Yuanjun Xiong, Zhe Wang, and Yu Qiao. To-
wards good practices for very deep two-stream convnets. In
arXiv, 2015. 2

[40] Christoph Feichtenhofer, Axel Pinz, Richard P Wildes, and
Andrew Zisserman. What have we learned from deep repre-
sentations for action recognition? In CVPR, 2018. 2

[41] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can
spatiotemporal 3d cnns retrace the history of 2d cnns and
imagenet? In CVPR, 2018. 2

[42] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 2, 3, 4

[43] Franc¬∏ois Chollet. Xception: Deep learning with depthwise

separable convolutions. In CVPR, 2017. 2, 4

[44] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
ShufÔ¨Çenet: An extremely efÔ¨Åcient convolutional neural net-
work for mobile devices. In CVPR, 2018. 2, 4

[45] Saining Xie, Ross Girshick, Piotr Doll¬¥ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In CVPR, 2017. 2, 4

[46] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, Andrew Rabinovich, et al. Going deeper with
convolutions. In CVPR, 2015. 2, 4

[47] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, 2016. 2

[48] A¬®aron Van Den Oord, Sander Dieleman, Heiga Zen, Karen
Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,
Andrew W Senior, and Koray Kavukcuoglu. Wavenet: A
generative model for raw audio. In SSW, 2016. 4

[49] Implementation

of

timeception.

github.com/

noureldien/timeception, 2017. 5

[50] Mart¬¥ƒ±n Abadi et al. TensorÔ¨Çow. tensorflow.org, 2015.

5

[51] Franc¬∏ois Chollet et al. Keras. keras.io, 2015. 5
[52] Xiaolong Wang and Abhinav Gupta. Videos as space-time

region graphs. In ECCV, 2018. 5, 7

[53] Haroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban,
Ivan Laptev, Rahul Sukthankar, and Mubarak Shah. The thu-
mos challenge on action recognition for videos ‚Äúin the wild‚Äù.
In CVIU, 2017. 5

[54] Charades

algorithms.

github.com/gsig/

charades-algorithms, 2017. 7

[55] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Ma-
chine learning in Python. JMLR, 2011. 8

263

