Multimodal Explanations by Predicting Counterfactuality in Videos

Atsushi Kanehira1, Kentaro Takemoto2, Sho Inayoshi2, and Tatsuya Harada2,3

1Preferred Networks, 2The University of Tokyo, 3RIKEN

Abstract

This study addresses generating counterfactual explana-
tions with multimodal information. Our goal is not only to
classify a video into a speciﬁc category, but also to pro-
vide explanations on why it is not categorized to a spe-
ciﬁc class with combinations of visual-linguistic informa-
tion. Requirements that the expected output should satisfy
are referred to as counterfactuality in this paper: (1) Com-
patibility of visual-linguistic explanations, and (2) Positive-
ness/negativeness for the speciﬁc positive/negative class.
Exploiting a spatio-temporal region (tube) and an attribute
as visual and linguistic explanations respectively, the ex-
planation model is trained to predict the counterfactuality
for possible combinations of multimodal information in a
post-hoc manner. The optimization problem, which appears
during training/inference, can be efﬁciently solved by in-
serting a novel neural network layer, namely the maximum
subpath layer. We demonstrated the effectiveness of this
method by comparison with a baseline of the action recog-
nition datasets extended for this task. Moreover, we provide
information-theoretical insight into the proposed method.

1. Introduction

The visual cognitive ability of machines has signiﬁcantly
improved mostly due to the recent development of deep
learning techniques. Owing to its high complexity, the deci-
sion process is inherently a black-box, and therefore, much
research has focused on making the machine explain the
reason behind its decision to verify its trustability.

The present study particularly focuses on building a sys-
tem that not only classiﬁes a video, but also explains why
a given sample is not predicted to one class but another, in
spite of almost all existing research pursuing the reason for
the positive class. Concretely, we intend to generate the ex-
planation in the form “X is classiﬁed to A not B because C

*$%++(,(*%’()&

!"#$%&""’(’)*#+,#-,%)#.&%/%+

!"# 0,12#3/45

6)$&/")#

!"#$%&%’()&

/"’12#5,%) ’1#

&1*

(%’55’12

’1#

&1*7

Figure 1: Our model not only classiﬁes a video to a category
(Pole vault), but also generates explanations why the video
is not classiﬁed to another class (Long jump). It outputs sev-
eral pairs of attribute (e.g., using pole) and spatio-temporal
region (e.g., red box) as an explanation.

and D exists in X.”1 This type of explanation is referred to as
counterfactual explanation [29] in this paper. It may aid us
in understanding why the model prediction is different from
what we think, or when discrimination is difﬁcult between
two speciﬁc classes. The explanation is valuable especially
for long videos because it provides information efﬁciently
on the content of what humans cannot see immediately.

We ﬁrst need to discuss the desired output. This work
treats an explanation as the one satisfying two conditions as
follows:
(A) The output should be interpretable for humans,
(B) The output should have ﬁdelity to the explained target.
Related to (A), one natural way of obtaining inter-
pretable output would be to assign an importance to each el-
ement in the input space and visualize it. Although this can
help us to perceive the important region for the model pre-
diction, the interpretation leading to it cannot be uniquely
determined. We enhance the interpretability of the expla-
nation by leveraging not only parts of visual input but also
linguistics, which is compatible with the visual information
similar to the previous work [2]. More speciﬁcally, dealing
with a spatio-temporal region of the target video and (the
existence of) an attribute as elements, we concatenate them

This work is done at the University of Tokyo.

1rephrased by “X would be classiﬁed as B not A if C and D not in X.”

8594

to generate explanations. An example is shown in the Fig. 1.
To realize (B) while satisfying (A), the expected output
of the visual-linguistic explanation should have the follow-
ing two properties:
(1) Visual explanation is the region which retains high
positiveness/negativeness on model prediction for spe-
ciﬁc positive/negative classes,

(2) Linguistic explanation is compatible with the visual

counterpart.

The score to measure how the requirements above are ful-
ﬁlled is hereafter referred to as the counterfactuality score,
or simply counterfactuality.

The above-listed requirements cannot be achieved by
naively exploiting output of the existing method, which con-
siders only positiveness for explanation, such as in [2],
where the authors attempt to generate visual-linguistic ex-
planations of positive class. This is mainly because posi-
tiveness/negativeness need to be considered simultaneously
for speciﬁc positive/negative classes in the same region.

To build a system that generates explanations satisfying
both (1) and (2), we propose a novel framework for gener-
ating counterfactual explanations based on predicting coun-
terfactuality. The outline of the framework is depicted in
two steps: (a) Train a classiﬁcation model that is the target
of the explanation, (b) Train an auxiliary explanation model
in a post-hoc manner by utilizing output and mid-level fea-
tures of the target classiﬁer after freezing its weights to pre-
vent output change. An explanation model predicts counter-
factuality scores for all negative classes. It is trained by ex-
ploiting the fact that supervised information “X is classiﬁed
to category A.” can be translated into “X is not classiﬁed to
any category B except A.”

The proposed explanation model holds a trainable clas-
siﬁer that predicts simultaneous existence of the pair of
class and attribute. Counterfactuality for a speciﬁc visual-
linguistic explanation (or region-attribute) can be simply
calculated by subtracting classiﬁer outputs corresponding
to positive/negative classes of interest. When the system
outputs the explanation, several pairs of [attribute, region]
are selected, whose counterfactuality is large for input pos-
itive/negative classes.

Maximization (or minimization) of the prediction score
with regard to the region is required during the train-
ing/inference process, which is computationally intractable
in a naive computation. Under the natural assumption that
candidate regions are tube-shaped, the maximum (or min-
imum) value and corresponding region path can be efﬁ-
ciently computed by dynamic programming. We construct
the algorithm such that it can be implemented as a layer of
a neural network with only standard functions (e.g., max
pooling, relu), pre-implemented in most deep learning li-
braries [19, 1, 7, 14, 25] by changing the computation or-
der, which enables combining it easily with Convoluional

Neural Networks (CNNs) on GPUs.

The proposed simple and intuitive framework for pre-
dicting counterfactuality is justiﬁed as the maximization of
the lower bound of conditional mutual information, as dis-
cussed later, providing an information-theoretical point of
view toward it.

Moreover, we assigned additional annotations for exist-
ing action-recognition datasets to enable quantitative eval-
uation for this task, and evaluated our method utilizing the
created dataset.

The contributions of this work are as follows:
• Introduce a novel task, which is generating counterfac-

tual explanations with spatio-temporal region,

• Propose a novel method based on simultaneous esti-
mations of visual-linguistic compatibility and discrim-
inativeness of two speciﬁc classes,

• Propose a novel neural network layer for efﬁciently
solving the dynamic programming problem which ap-
pears during training/inference procedures,

• Derive a connection between the proposed method and
conditional mutual information maximization, provid-
ing better understanding of the proposed model from
the information-theoretical viewpoint,

• Propose a metric as well as extending datasets for this

task, enabling quantitative evaluation,

• Demonstrate the effectiveness of the proposed ap-

proach by experiment.

2. Related Work

We divide existing research on visual explanation into

two categories, i.e., justiﬁcation, or introspection.

Methods for justiﬁcation are expected to explain as hu-
mans do, by projecting input to correct reason obtained
from the outside [20]. Methods exploiting textual [13] or
multimodal [18, 2, 16] supervision belong to this category.
True explanations, as by humans, are expected to be gener-
ated regardless of the type of model, and evaluation is per-
formed by comparison with ground-truth supervision.

In the latter category, the main goal is to know where the
model actually “looks” in the input by propagating the pre-
diction to the input space [22, 4, 30, 21, 31, 9, 32], or by
learning instance-wise importance of elements [6, 8] with
an auxiliary model. As opposed to the methods of the for-
mer category, it is important to show the region where the
model focuses for prediction rather than whether the predic-
tion is true for humans. The evaluation is often performed
by the investigating before/after output of the model when
the element considered to be important is changed.

Although almost all previous research pursues the reason
for positiveness, we attempt to provide the reason for nega-
tiveness as well. While our work is categorized to the latter,
it also has an aspect of the former; The important region

8595

for the model prediction is outputted, while exploiting the
linguistic attribute for enhancing interpretability for human.
[2] conducted research similar to the present study, stat-
ing an application for grounding visual explanation to coun-
terfactual explanation, where the textual explanations (with-
out the region) are generated by comparing the output of
generated explanations for target sample and the nearest
sample to it. Because their work is in the former category
where negativeness cannot be well-deﬁned, no quantitative
evaluation was provided. The main differences between our
work and this previous study are:

• We set generating counterfactual explanations as the
main goal, and propose a method which utilizes multi-
modal information speciﬁcally for this task,

• Our work belongs to the latter category where neg-
ativeness can be well-deﬁned by model output, and
therefore, quantitative evaluation is possible,

• We provide quantitative evaluation with the metric.

3. Method

We describe the details of our proposed method in this
section. The main goal of this work is to build a system that
not only classiﬁes a video, but also explains why a given
sample is not predicted to one class but another. As stated
earlier, we utilize the combination of the spatio-temporal re-
gion (tube) and the attribute as the element of explanations.
The expected output explanation should have following two
properties:
(1) Visual explanation is the region which retains high
positiveness/negativeness on the model prediction for
speciﬁc positive/negative classes.

(2) Linguistic explanation is compatible to the visual

counterpart.

First, we formulate the task addressed in subsection 3.1,
and describe the outline of the framework as well as the
actual training/inference process of the explanation model
from subsection 3.2 to 3.4. In the subsequent subsection 3.5
and 3.6, we elucidate the method and its implementation
for efﬁciently solving the optimization problem in the train-
ing/inference step. Theoretical background of our method
will be discussed in subsection 3.7.

3.1. Task formulation

spatio-temporal coordinate, scale, and shape of the element
of visual region, respectively. R is a possible set of R. In
this work, we particularly limit R to the set containing all
possible tubes. In other words, R contains at most one ele-
ment corresponding to the time step t, and all the elements
are spatially and temporally continuous.

We build an explainable model, having the following
two functions: (a) Classify the video x to a speciﬁc class
cpos ∈ C, (b) Explain the reason for speciﬁc negative class
cneg ∈ C\cpos by the combination of attribute s (linguistic)
and spatio-temporal tube R (visual). Our model predicts
several pairs of (s, R) for speciﬁc class pair cpos, cneg, and
simply puts them together as ﬁnal output.

3.2. System pipeline

We outline the pipeline of the proposed method in Fig. 2.
Our model holds two modules, namely, the classiﬁcation
module and the explanation module. The outline of the
framework follows two steps:
(a) Train a classiﬁcation
model, which is the target of the explanation, (b) Train an
auxiliary explanation model in a post-hoc manner as in ex-
isting research (e.g.,[13]) by utilizing output and mid-level
activation of the target classiﬁer after freezing its weights to
prevent change in output. Speciﬁcally, we explicitly repre-
sent feature extraction parts in the pre-trained classiﬁcation
network as

p(c|x) = f (h(x)), h(x) ∈ RW ×H×T ×d,

(1)

where W, H, T indicate width, height, and the number
of frames in the mid-level feature, respectively. The d-
dimensional feature vector corresponding to each physical
coordinate is denoted by h(R)[i, j, t] ∈ Rd.

We introduce an auxiliary model g, which is responsible
for the explanation. It predicts conterfactuality, which mea-
sures (1) the positiveness/negativeness of the region R on
the model prediction p(c|x) for a speciﬁc pair of cpos, cneg,
and (2) the compatibility of linguistic explanation s to the
visual counterpart R. By ﬁxing the parameter of the feature
extraction part h(·), we obtain

ycpos,cneg,s,R = g(h(x)) ∈ [0, 1](|C|−1)×|S|×|R|

(2)

Notations used throughout this paper and formulation of

the task we deal with are described in this subsection.

Let x ∈ RW ′×H ′×T ′×3 be the input video where
W ′, H ′, T ′ are width, height, and the number of frames of
the video, respectively. We denote the class and the attribute
by c ∈ C and s ∈ S , respectively. We assume that attributes
are assigned to each sample used for training, and that
the assigned set of attributes is represented as S(x) ⊂ S.
R ⊂ R denotes the spatio-temporal region used for visual
explanation, and its element [i, j, t, scale, shape] ∈ R is the

which holds a counterfactuality score corresponding to one
combination of cpos, cneg, s, R in each dimension. Posi-
tive class cpos is sampled from p(c|x) during training, and
cpos = arg maxc p(c|x) is applied in the inference step.
Any remaining class cneg ∈ C\cpos is regarded as negative.
We consider the element of R in the space of h(x). In
other words, the coordinate (i, j, t) of R corresponds to that
of h(x). The shape of R is ﬁxed to [W/W ′, H/H ′] for the
sake of simplicity. The extension to multiple scales and the
aspect ratio will be discussed in subsection 3.6.

8596

# "#$%%&’&"$(&)*+,)-.#/

mcs

∆mc

pos

c

neg

s

!

"

!

"

’())*+",(-,’./#$’
%0#/("0,#$"*/+, R

cpos ∼ p(c|x)

#

)

·

(
f

)
x
|
c
(
p

"
"
!

h(x)

)

·

(
ˆg

x

|C|

mc

pos

s

!

&
%.2(3$"(

|S|

mcs

(i, j, t) ∈ R

#

!

&

1

−

|C|

%

$
!
"
$
#
"
"
!

1
−

|
C
|

|R |

!

|R |

!

*

1
/
)
"
’

*

1
−

|
C
|

|S|

R

∆mc

pos

c

neg

s

|S|

|S|

∆m

cpos ,cneg ,s[R]

yc

pos ,cneg ,s,R

$ /01#$*$(&)*+,)-.#/ g(·)

2(+.%*/+,

34*%0,-!#!)$%$#’5

2(+.%*/+,

34*%0/(%,-!#!)$%$#’5

’6’%$),/(%-(%

’6’%$),

*+%$#)$1*!%$,’%!%$

Figure 2: Pipeline of the proposed method. Our model holds two modules, the classiﬁcation and explanation module. The
outline of the framework follows two steps: (a) Train a classiﬁcation model to be explained, (b) Train an auxiliary explanation
model in a post-hoc manner by utilizing output and mid-level features of the target classiﬁer after freezing its weights.

3.3. Predicting counterfactuality

3.4. Training and inference

Our explanation model predicts counterfactuality, that
is, (1) how much the region R retains high positive-
ness/negativeness on the p(c|x) for cpos, cneg, and (2) how
much s is compatible to R. The counterfactuality score is
predicted in the following steps.

A target sample x is inputted to obtain the mid-level
representation h(x) and the conditional probability p(c|x).
The explanation model holds classiﬁers ˆgcs for each pair of
(c, s). These classiﬁers are applied to each element feature
of h(x), and predict simultaneous existence of (c, s) as

mcs = ˆgcs(h(x)) ∈ RW ×H×T ,

(3)

is obtained for each pair of (c, s). For simplicity, we uti-
lize a linear function that preserves geometrical informati-
ton, that is, the convolutional layer as classiﬁer ˆgcs.

To measure how likely the region element is consid-
ered to be cpos, not cneg, with linguistic explanation s,
we element-wise subtract the value of (3) for all cneg ∈
C\{cpos} and s as

∆mcpos,cneg,s = mcposs − mcnegs

(4)

To obtain the score for the region R, we deﬁne the pro-
cedure of aggregating scalar values through R in the 3 di-
mensional tensor ∆m as

We illustrate the loss function optimized in the training

step and the procedure in the inference step.

Loss function: The supervised information “A sample
is classiﬁed to category cpos.” can be translated to “A sam-
ple is not classiﬁed to any category cneg ∈ C\cpos.” By
utilizing this, the model is trained to enlarge the counter-
factuality score corresponding to class pairs cpos, cneg and
attributes s ∈ S(x). The output obtained after sigmoid ac-
tivation in (6) can be interpreted as a probability, and its
negative log likelihood is minimized. Because computing
output ycpos,cneg,s,R for all pairs of cneg ∈ C\cpos, s ∈ S
and R ∈ R is not feasible, only R maximizing the loss is
utilized for each pair of cneg, s while training. Formally, for
a given x and cpos, the loss

ℓ(x, cpos) =

−log ˆycpos,cneg,s

1

|S(x)| Xs∈S(x) Xcneg∈C\cpos

where ˆycpos,cneg,s = min

ycpos,cneg,s,R (7)

R

As stated in the next subsection 3.5,
is calculated.
ˆycpos,cneg,s can be efﬁciently computed by dynamic pro-
gramming under the condition where R, a possible set of
R, is limited to the set of all spatio-temporal tubes.

The overall loss function to be optimized is obtained by

∆mcpos,cneg,s[i, j, t].

(5)

taking the expectation of (7) over x and cpos as

∆mcpos,cneg,s[R] = X(i,j,t)∈R

Please note ∆mcpos,cneg,s[R] ∈ R. By applying a sigmoid
activation function to the output, we obtain counterfactual-
ity ycpos,cneg,s,R ∈ [0, 1](|C|−1)×|S| as

ycpos,cneg,s,R = σ(∆mcpos,cneg,s[R])

(6)

where σ(a) =

1

1+exp(−a) .

L = E

p(x)p(cpos|x) [ℓ(x, cpos)] .

(8)

p(x) indicates the true sample distribution, and p(cpos|x)
is the pre-trained network in (1). Empirically, the expec-
tation over x is calculated by summing up all training N
samples, and that over cpos is achieved by sampling from
the conditional distribution p(cpos|x) given x.

8597

!

!"#

!"$

!"%

!"&

"""

!"’

-.)/"

(&012

"#","30

()--4+#"

,)5","30

"""

!

#

$
"
!

#

)
)
(
’
&
%

!

#

$
"
!

#

)
)
(
’
&
%

!

#

$
"
!

#

)
)
(
’
&
%

!

#

$
"
!

#

)
)
(
’
&
%

!

!

#

&
+
)
*

#

#

)
)
(
’
&
,

Figure 3: The illustration of maximum subpath pooling.
Finding the subpath in the 3d tensor whose summation is
maximum can be implemented by sequentially applying the
elementwise sum, relu, and 2d max pooling in the time di-
rection, following global 2d max pooling.

Inference: During inference, provided with positive and
negative class cpos, cneg as well as input x, pairs of the at-
tribute s and the region R are outputted whose score is the
largest. Formally,

s⋆, R⋆ = arg max

s,R

ycpos,cneg,s,R

(9)

is calculated as the element of explanation. For computing
k multiple outputs, we compute maxR ycpos,cneg,s,R for all
s and pick k pairs whose scores are the largest. Minimiza-
tion for R is also efﬁciently calculated as is the case in the
training step.

3.5. Maximum subpath pooling

We describe in detail the maximization (minimization)
problem for R appearing in (7) and (9) in this subsection.
We limit R, a possible set of R, to the set containing all
possible tubes. A tube can be expressed as a path in the 3d
tensor, starting at one spatial coordinate (i, j) in time t, and
move to the neighbor {(i + l, j + m) | − k ≤ l, m ≤ k}
in time t + 1 where k controls how much movement of the
spatial coordinate (i, j) is allowed when the time changes
from t to t + 1. The path can start and end at any time.
The R consists of coordinates (i, j, t) satisfying the path
condition.

With this limitation, the maximization problem with re-
gard to R can be cast as an extension of ﬁnding a sub-
array whose summation is maximum, and it can be efﬁ-
ciently solved by the algorithm proposed in [28] (shown
in the supplementary material), which is an extension of the
Kadane’s algorithm [5]. Although [28] utilized it only for
the inference, we need to train the parameters, especially
by back-propagation on GPUs to combine CNNs. To re-
alize this, we construct the algorithm such that it can be
implemented as a layer of a neural network with only stan-
dard functions pre-implemented in most deep learning li-
braries [19, 1, 7, 14, 25]. Interestingly, as shown in Fig. 3,

w

d

a1
2 x 2

a2
2 x 3

a3
3 x 2

a4
3 x 3

1/4

1/2

1/4

1/2

1/2

1/2

!

1/2

1

1/2

1

1

1

1/4

1/2

1/4

1/4

1/2

1/4

1/2

1/2

1/2

1

1

1

1/2

1/2

1/2

1

1

1

1

1

1

1

1

1

Figure 4: The spatial weights multiplied with the parame-
ters of the convolutional layer. Each element of the weight
has a value proportional to the overlap to the outputted
shape (in red). These values are normalized such that the
summation equals 1.

the same result can be achieved by sequentially applying
relu, 2d maxpooling, and element-wise summation in the
time direction followed by global 2d maxpooling. The ker-
nel size of maxpooling is a hyper-parameter corresponding
to k mentioned above. We ﬁxed it to 3 × 3, which means
that k = 1. The computational cost of this algorithm is
O(W HT ), which can be solved by a single forward path,
since the iteration for W and H can be parallelized on GPU
without signiﬁcant overhead. In the case of minimization
of the objective, the same algorithm can be applied just by
inverting sign of input and output.

To acquire the path R⋆ whose summation is maximum,
we simply need to calculate the partial derivative of the
maximum value with regards to each input. Because

∂∆m

cpos ,cneg ,s[R⋆]

∂∆m

cpos ,cneg ,s

[i, j, t] =(1 ((i, j, t) ∈ R⋆)

0 (otherwise)

we can obtain the path corresponding to the maximum by
extracting the element whose derivative is equal to 1. Im-
plementation is likewise easy for the library, which has the
function of automatic differentiation.

This procedure can be interpreted as a kind of pooling.
To observe this, we denote the aggregated feature after ap-
plying sum pooling to mid-level local features throughout
the region R⋆ by pool(h(x), R⋆), and redeﬁne ∆w =
wcposs − wcnegs, where wcs is the parameter of the con-
volutional layer ˆgcs. The summation of the score inside the
sigmoid function in (9) can be written as

max

R

∆mcpos,cneg,s[R] = X(i,j,t)∈R⋆

∆w⊤h(x)[i, j, t]

h(x)[i, j, t] = ∆w⊤pool(h(x), R⋆)

= ∆w⊤ X(i,j,t)∈R⋆

We refer to the sequence of this process as the maximum
subpath pooling layer.

3.6. Multiple scales and aspect ratio

So far, we only considered the situation where the scale
and aspect ratio of R is ﬁxed to [W/W ′, H/H ′], 1 : 1. We
modify the algorithm to treat different scales and shapes of
the region. As described in 3.5, the input of the optimiza-
tion algorithm ∆mcpos,c¬,s (deﬁned in (4)) is a 3d tensor,

8598

dataset

video

class

attribute

Olympic

UCF101-24

783
3204

16
24

39
40

bbox

949
6128

Table 1: statistics of dataset used in the experiment

which corresponds to each physical coordinate in the re-
gion. We expand the input from 3d to 5d by expanding mc,s
(deﬁned in (3)), taking scales and shapes into consideration.
To obtain the 5d tensor, we prepare multiple parameters w
of the convolutional layer ˆgcs (3) for each scale and shape.
(For simplicity, the subscripts c, s of w will be omitted be-
low). partial To treat different scales, we extract mid-level
representations from different layers of the target classiﬁer
to construct h(x). After convolution is applied separately,
they are appended for the scale-dimension.

When considering different shapes at each scale, we pre-
pare different parameters by multiplying the signiﬁcance
corresponding to the shape of a region. Formally, wi =
w ⊙ ai is computed, where ai has the same window size
as w and consists of the importance weight for each posi-
tion of parameter, which is determined by the overlap ratio
between the region and each local element. They are nor-
malized to satisfy |ai| = 1. Different wi are applied sepa-
rately and output is appended to the shape-dimension of the
tensor. Concretely, we compute the scores corresponding
to four kinds of shapes (2 × 2, 2 × 3, 3 × 2, 3 × 3) from
3 × 3 convolution for each scale as in Fig. 4. The optimiza-
tion problem can be solved by applying the same algorithm
described in subsection 3.5 to the obtained 5d tensor.

3.7. Theoretical background

To demonstrate that linguistic explanation s for region
R is strongly dependent on the output of the target classi-
ﬁer c by minimizing the loss function proposed above, we
reveal the relationship between the loss function and condi-
tional mutual information. Conditional mutual information
of the distribution parameterized by our model is denoted
by MI(c, s|x, R) and can be bounded as

MI(c, s|x, R) = E

≥ E

p(c,s,x,R)(cid:20)log

ˆp(c, s|x, R)

ˆp(c|x, R)ˆp(s|x, R)(cid:21)

p(c,s,x,R) [log ˆp(c|s, x, R)]

(10)

(10) is derived from H(c|x, R) ≥ 0 and KL[p|q] ≥ 0 for
any distribution p, q where H(·) and KL[·|·] indicate the en-
tropy and KL divergence, respectively. In our case, we pa-
rameterize the joint distribution as

ˆp(c, s|x, R) =

exp(mcs[R])

Pc′∈C,s′∈S exp(mc′s′ [R])

(11)

(10) is further bounded as

(10) ≥E

≥E

=E

R

p(c,s,x)(cid:20)min
p(cpos,s,x)

log

exp(mcs[R])

Pc′s′ exp(mc′s′ [R])(cid:21)

σ(∆mcpos,cneg,s[R])

min

R

 Xcneg∈C\cpos
 Xcneg∈C\cpos

p(cpos,s,x)

ˆycpos,cneg,s


(12)

 (13)

(14)

σ(·) is the sigmoid function and (12) is derived from the
fact Ea[f(a)] ≥ min f(a). On the bound (13), we utilize the

relationship (1 +Pi ai) ≤Qi(1 + ai) [3].

Finally, by decomposing as p(s, c|x) = p(s|c, x)p(c|x)
and setting p(c|x) as the target classiﬁer and p(s|c, x) =
[s ∈ S(x)]/|S(x)| following the inversion of sign, (8) is
obtained. The minimization of the loss function can be jus-
tiﬁed as the maximization of the lower bound of conditional
mutual information. It may be beneﬁcial to investigate the
relationship with other methods proposed for the explana-
tion task, such as [6], based on mutual information maxi-
mization.

4. Experiment

We describe experiments to demonstrate the effective-
ness of proposed method, in particular the explanation mod-
ule, which is the main proposal for this task. After the de-
tails of experimental settings including datasets and met-
rics used for quantitative evaluation are described in subsec-
tion 4.1, the obtained results are discussed in subsection 4.2.

4.1. Setting

Given an input video x, and a pair of positive/negative
the explanation module outputs several
i=1. We separately eval-

class cpos, cneg,
pairs of attribute/region {(si, Ri)}k
uate each pair of output.

Dataset: Two existing video datasets for action recogni-
tion: Olympic Sports [17] and UCF101-24 categories [15]
were used in the experiments. The Olympic Sports dataset
consists of 16 categories for sports action. The UCF101-24
categories is a subset of the UCF101 dataset [24] extract-
ing 24 out of 101 general action classes. We utilized the
original train/test split provided by the datasets. We addi-
tionally assigned these datasets with Amazon Mechanical
Tutk (AMT) to make the evaluation possible as follows: (a)
Assign a set of attributes to all videos in the dataset, (b) As-
sign a bounding box of assigned attributes for samples in
the test split. Statistics of the dataset are shown in Table. 1,
and a few examples of the annotations are provided in the
supplementary material.

8599

method

Olympic UCF101-24

baseline
propose

0.76
0.89

0.68
0.88

Table 2: The ratio of the probability p(cpos|x) for the posi-
tive class cpos decreasing after the region is masked out.

Olympic Sports

UCF101-24

method

baeline
propose

top1

0.02
0.13

top3

0.07
0.38

top5

0.12
0.59

top1

0.02
0.14

top3

0.07
0.41

top5

0.13
0.65

Table 3: The concept accuracy on the Olympic Sports
dataset and the UCF101-24 dataset.

To quantify this, we exploit the bounding boxes assigned
for each attribute in the test set, and compute the accuracy as
follows. IoU (intersection over union) is calculated between
given R and all bounding boxes R′, which corresponds to
attribute s′. We measure the accuracy by selecting the at-
tribute s′ with the largest IoU score, and checking its con-
sistency with s, which is the counterpart of R. This metric
is referred to as concept accuracy in the following parts.

Detailed settings In the classiﬁcation module, the output
of convolutional layers was used as h(·). Fc layers follow-
ing convolutional layers were considered as f (·).

Speciﬁcally, we dealt with C3D-resnet [27, 11] as the
target classiﬁer in the experiments, which is based on the
spatio-temporal convolution [26] with residual architec-
ture [12]. Our network for classiﬁcation consists of nine
convolutional layers and one fully connected layer accept-
ing a 112 × 112 × 16 size input. Relu activation was applied
to all layers, except the ﬁnal fc layer. We selected the out-
puts of the last and the 2nd to last convolutional layers to
construct h(·). Moreover, we replaced 3d max pooling to
2d max pooling to guarantee T = T ′ for all activations.
The target classiﬁer was trained with SGD, where learning
rate, momentum, weight decay, and batch size, were set to
0.1, 0.9, 1e-3, and 64 respectively. To train the model in
Olympic Sports, we pre-trained it with the UCF101-24.

For the training of the explanation module, all the set-
tings (e.g., learning rate) were set to the same as in the
training of classiﬁcation module, except that the batch size
was 30. We decomposed the weight of the convolutional
layer wcs corresponding to the pair of (c, s) to wcs =
wc ⊙ ws + wc + ws to reduce the number of parameters
where ws and wc are the parameter shared by the same at-
tribute and class respectively.

4.2. Identiﬁability of negative class

To assess whether the obtained region R is truly an ex-
planation of the reason for a speciﬁc negative class, we
compared the negative class accuracy with a baseline. Be-
cause there is no previous work for this task, we employed
a simple baseline. For the UCF101-24 dataset, we exploited
the bounding box for the action detection provided in [23],

8600

Figure 5: The negative class accuracy on the Olympic
Sports dataset (above) and the UCF101-24 dataset (below).
The y-axis depicts the mean accuracy and the x-axis denotes
the number of negative classes used for averaging, whose
prediction value is maximum.

Metric: As stated earlier, the method for this task is ex-

pected to satisfy the following two requirements:
(1) Visual explanation is the region which retains high
positiveness/negativeness on the model prediction for
speciﬁc positive/negative classes,

(2) Linguistic explanation is compatible to the visual

counterpart,

and methods are evaluated based on them. To make the
quantitative evaluation possible, we propose two metrics,
both of which are based on the accuracy.

As for (1), we need to evaluate whether the obtained re-
gion is truly an explanation of the reason for “the target
classiﬁer predicts a sample to not cneg but cpos”. More
speciﬁcally, we would like to conﬁrm whether the region
explains the speciﬁc negative class cneg, not other negatives
ˆcneg ∈ C\{cpos, cneg}. To evaluate this quantitatively, we
investigate how the output of the target classiﬁer changes
corresponding to cneg when region R is removed from the
input. A mask z = {0, 1}W ′×H ′×T ′×3 is prepared, which
takes the value of 0 if the corresponding pixel’s location
is contained to R restored on the input space, otherwise it
takes the value of 1. Applying again the masked sample to
the target classiﬁer, the difference from the original output
f (h(x ⊙ z)) − f (h(x)) is calculated for all negative classes
where ⊙ denotes the Hadamard product. We calculate the
accuracy, i.e., we pick the largest values out of the obtained
difference scores, and examine if cneg exists within this set.
We refer to this metric as negative class accuracy.

As for (2), we assess how the region R makes the con-
cept s identiﬁable by humans for each output pair (s, R).

12345678910# of negative classes0.00.10.20.30.40.50.60.70.8accuracyOlympic Sports12345678910# of negative classes0.00.10.20.30.40.50.60.70.8accuracyUCF101-24propose-top1baseline-top1propose-top3baseline-top3propose-top5baseline-top5Olympic Sports

UCF101-24

method

baeline
propose

fc1

0.45
0.62

fc2

0.44
0.64

fc3

0.46
0.59

fc1

0.45
0.64

fc2

0.43
0.62

fc3

0.44
0.61

Table 4: The top3 negative class accuracy on the Olympic
Sports dataset and the UCF101-24 dataset averaged over 3
negative classes whose prediction probability is the largest,
by changing the number of fully-connected layers.

which provides an upper limit of the performance for the ac-
tion detection task. As bounding boxes are not provided for
the Olympic Sports dataset, we cropped the center (32×32)
from all frames. This forms a simple but strong baseline be-
cause the instance containing category information usually
appears in the center of the frame in this dataset.

The results of the negative class accuracy for the
Olympic Sports and the UCF101-24 datasets are shown in
Fig. 5. The accuracy averaged over negative classes hav-
ing largest p(c|x) is calculated and the x-axis of the ﬁg-
ures depicts the number of used negative classes. Our
method performed consistently better than the baseline on
both datasets, demonstrating the generalization of identi-
ﬁability of negativeness in unseen samples. The gap be-
tween the accuracy of our method and that of baseline is
decreased when negative classes having small p(c|x) are
included. We conjecture the reason is that such a easy neg-
ative class, which is highly dissimilar to the positive class,
does not have common patterns to identify them. For exam-
ple, for positive class ‘pole vault’, the negative class ‘high
jump’ is considered to be more similar than ‘bowling’ for
the classiﬁer, and detecting the negativeness for such a hard
negative ‘pole vault’ is relatively easy (e.g.
the region of
‘pole’), although detecting it is difﬁcult for the easy neg-
ative class. We believe the low-accuracy for such an easy
negative class does not have signiﬁcant impact because in
real applications, we may be interested in the reason for
the negativeness of the hard negative class, which is difﬁ-
cult to discriminate. In addition, we also report the ratio of
the probability p(cpos|x) for the positive class cpos decreas-
ing after the region is masked out in Table. 2. From these
results, we claim that our method can ﬁnd both better of
negativeness/positiveness on the negative/positive classes.

4.3. Identiﬁability of concept

To evaluate whether the obtained region makes the con-
cept (linguistic explanation) identiﬁable by human, we mea-
sured concept accuracy described above for the case where
category prediction is correct. The same baseline was ap-
plied for region selection as in the previous subsection 4.2,
and the attribute is randomly selected. Results are shown
in Table 3. In both datasets, our method is consistently bet-
ter than the baseline. Finding the region by which a spe-
ciﬁc concept can be identiﬁed is the signiﬁcantly challeng-
ing task, where methods need to identify small objects or

!"#$%&’()’*+ *",-./&0/0#’* )$1/20$-3"45-6",’"*-’0-7$8,’1/&-9#

3’:’*+ *",-.:/,$)"/84’*+ )$1/20$-;"0,28$-’0-.’,,’*+

Figure 6: Example output from our system for the samples
of the UCF101-24 dataset.

atomic-actions [10]. Although it is conceivable that there is
still room for improvement of this metric, we believe that
our simple method can serve as a base for future work re-
garding this novel task.

4.4. Inﬂuence of the complexity of classiﬁer

To investigate the inﬂuence of the complexity of the clas-
siﬁer module on the generalization ability of the explanation
module, we measured the negative class accuracy by chang-
ing the number of fc-layers of f (c|x) in 1 ∼ 3. The other
settings remain the same as those in subsection 4.2. The
top3 accuracy averaged on 3 negative classes is shown in
Table. 4 (Other results are shown in supplementary mate-
rial).
In both datasets, the gap between the baseline and
the proposed method is consistent regardless of the number
of fc-layers, demonstrating the robustness of the proposed
method to the complexity of classiﬁers to be explained.

4.5. Output examples

We show a few output examples in Fig. 6 for the sam-
ple videos of the UCF101-24 dataset. As observed in the
ﬁgures, our model is considered to appropriately localize
the area compatible with the linguistic explanation. Other
examples are shown in the supplementary material.

5. Conclusion

In this work, we particularly focused on building a model
that not only categorizes a sample, but also generates an ex-
planation with linguistic and region. To this end, we pro-
posed a novel algorithm to predict counterfactuality, while
identifying the important region for the linguistic explana-
tion. Furthermore, we demonstrated the effectiveness of the
approach on two existing datasets extended in this work.

6. Acknowledgement

This work was partially supported by JST CREST Grant
Number JPMJCR1403, Japan, and partially supported by
the Ministry of Education, Culture, Sports, Science and
Technology (MEXT) as ”Seminal Issue on Post-K Com-
puter.” Authors would like to thank Kosuke Arase, Mikihiro
Tanaka, Yusuke Mukuta for helpful discussions.

8601

[18] D. H. Park, L. A. Hendricks, Z. Akata, A. Rohrbach,
B. Schiele, T. Darrell, and M. Rohrbach. Multimodal expla-
nations: Justifying decisions and pointing to the evidence. In
CVPR, 2018. 2

[19] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017. 2, 5

[20] A. S. Ross, M. C. Hughes, and F. Doshi-Velez. Right for the
right reasons: Training differentiable models by constraining
their explanations. 2017. 2

[21] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, and D. Batra. Grad-cam: Visual explanations from
deep networks via gradient-based localization.
In ICCV,
2017. 2

[22] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside
convolutional networks: Visualising image classiﬁcation
models and saliency maps. arXiv preprint arXiv:1312.6034,
2013. 2

[23] G. Singh, S. Saha, M. Sapienza, P. Torr, and F. Cuzzolin.
Online real time multiple spatiotemporal action localisation
and prediction. 2017. 7

[24] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012. 6

[25] S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a
next-generation open source framework for deep learning.
In Workshop on Machine Learning Systems (LearningSys) in
NIPS, 2015. 2, 5

[26] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, 2015. 7

[27] D. Tran, J. Ray, Z. Shou, S.-F. Chang, and M. Paluri. Con-
vnet architecture search for spatiotemporal feature learning.
arXiv preprint arXiv:1708.05038, 2017. 7

[28] D. Tran, J. Yuan, and D. Forsyth. Video event detection:
From subvolume localization to spatio-temporal path search.
2014. 5

[29] S. Wachter, B. Mittelstadt, and C. Russell. Counterfactual
explanations without opening the black box: Automated de-
cisions and the gdpr. 2017. 1

[30] J. Zhang, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff. Top-
In ECCV,

down neural attention by excitation backprop.
2016. 2

[31] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
In CVPR, 2016. 2

[32] B. Zhou, Y. Sun, D. Bau, and A. Torralba. Interpretable basis

decomposition for visual explanation. In ECCV, 2018. 2

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 2, 5

[2] L. Anne Hendricks, R. Hu, T. Darrell, and Z. Akata. Ground-

ing visual explanations. In ECCV, 2018. 1, 2, 3

[3] M. T. R. AUEB. One-vs-each approximation to softmax for

scalable estimation of probabilities. In NIPS, 2016. 6

[4] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R.
M¨uller, and W. Samek. On pixel-wise explanations for non-
linear classiﬁer decisions by layer-wise relevance propaga-
tion. PloS one, 10(7):e0130140, 2015. 2

[5] J. Bentley. Programming pearls: algorithm design tech-
niques. Communications of the ACM, 27(9):865–873, 1984.
5

[6] J. Chen, L. Song, M. J. Wainwright, and M. I. Jordan.
Learning to explain: An information-theoretic perspective on
model interpretation. In ICML, 2018. 2, 6

[7] F. Chollet. keras. https://github.com/fchollet/

keras, 2015. 2, 5

[8] P. Dabkowski and Y. Gal. Real time image saliency for black

box classiﬁers. In NIPS, 2017. 2

[9] R. C. Fong and A. Vedaldi.

Interpretable explanations of

black boxes by meaningful perturbation. In ICCV, 2017. 2

[10] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li,
S. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar,
C. Schmid, and J. Malik. Ava: A video dataset of spatio-
temporally localized atomic visual actions. In CVPR, 2018.
8

[11] K. Hara, H. Kataoka, and Y. Satoh. Can spatiotemporal 3d
cnns retrace the history of 2d cnns and imagenet? In CVPR,
2018. 7

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 7

[13] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue,
B. Schiele, and T. Darrell. Generating visual explanations.
In ECCV, 2016. 2, 3

[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014. 2, 5

[15] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014. 6

[16] A. Kanehira and T. Harada. Learning to explain with com-

plemental examples. In CVPR, 2019. 2

[17] J. C. Niebles, C.-W. Chen, and L. Fei-Fei. Modeling tempo-
ral structure of decomposable motion segments for activity
classiﬁcation. In ECCV, 2010. 6

8602

