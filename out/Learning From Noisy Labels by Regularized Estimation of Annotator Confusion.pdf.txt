Learning From Noisy Labels By

Regularized Estimation Of Annotator Confusion

Ryutaro Tanno1 ∗

Ardavan Saeedi2

Swami Sankaranarayanan2

Daniel C. Alexander1

Nathan Silberman2

1University College London, UK
2

{r.tanno, d.alexander}@ucl.ac.uk

1

2Butterﬂy Network, New York, USA

{asaeedi,swamiviv,nsilberman}@butterflynetinc.com

Abstract

The predictive performance of supervised learning algo-
rithms depends on the quality of labels. In a typical label
collection process, multiple annotators provide subjective
noisy estimates of the “truth” under the inﬂuence of their
varying skill-levels and biases. Blindly treating these noisy
labels as the ground truth limits the accuracy of learning al-
gorithms in the presence of strong disagreement. This prob-
lem is critical for applications in domains such as medical
imaging where both the annotation cost and inter-observer
variability are high. In this work, we present a method for
simultaneously learning the individual annotator model and
the underlying true label distribution, using only noisy ob-
servations. Each annotator is modeled by a confusion ma-
trix that is jointly estimated along with the classiﬁer pre-
dictions. We propose to add a regularization term to the
loss function that encourages convergence to the true anno-
tator confusion matrix. We provide a theoretical argument
as to how the regularization is essential to our approach
both for the case of single annotator and multiple anno-
tators. Despite the simplicity of the idea, experiments on
image classiﬁcation tasks with both simulated and real la-
bels show that our method either outperforms or performs
on par with the state-of-the-art methods and is capable of
estimating the skills of annotators even with a single label
available per image.

1. Introduction

In many practical applications, supervised learning algo-
rithms are trained on noisy labels obtained from multiple
annotators of varying skill levels and biases. When there is
a substantial amount of disagreement in the labels, conven-
tional training algorithms that treat such labels as the “truth”
lead to models with limited predictive performance. To
mitigate such variation, practitioners typically abide by the
principle of “wisdom of crowds” [1] and aggregate labels by
computing the majority vote. However, this approach has
limited efﬁcacy in applications where the number of anno-

∗A part of the work done during internship at Butterﬂy Network.

tations is modest or the tasks are ambiguous. For example,
vision applications in medical image analysis [2] require an-
notations from clinical experts, which incur high costs and
often suffer from high inter-reader variability [3, 4, 5, 6].

However, if the exact process by which each annotator
generates the labels was known, we could correct the an-
notations accordingly and thus train our model on a cleaner
set of data. Furthermore, this additional knowledge of the
annotators’ skills can be utilized to decide on which exam-
ples to be labeled by which annotators [7, 8, 9]. Therefore,
methods that can accurately model the label noise of anno-
tators are useful for improving not only the accuracy of the
trained model, but also the quality of labels in the future.

Previous work proposed various methods for jointly esti-
mating the skills of the annotators and the ground truth (GT)
labels. We categorize these methods into two groups: (1)
two-stage approach and (2) simultaneous approach. Meth-
ods in the ﬁrst category perform label aggregation and train-
ing of a supervised learning model in two separate steps.

abilistic model of annotators. The observable variables are

The noisy labels eY are ﬁrst aggregated by building a prob-
the noisy labels eY, and the latent variables/parameters to be

estimated are the annotator skills and GT labels Y. Then, a
machine learning model is trained on the pairs of aggregated
labels Y and input examples X (e.g.
images) to perform
the task of interest. The initial attempt was made in [10]
in the early 1970s and more recently, numerous lines of re-
search [11, 6, 12, 13, 14] proposed extensions of this work
e.g. by estimating the difﬁculty of each example. However,
in all these cases, information about the raw inputs X is
completely neglected in the generative model of noisy la-
bels used in the aggregation step, and this highly limits the
quality of estimated true labels in practice.

The simultaneous approaches [15, 16, 17, 18] address
this issue by integrating the prediction of the supervised
learning model (i.e. distribution p(Y|X)) into the proba-
bilistic model of noisy labels, and have been shown to im-
prove the predictive performance. These methods employ
variants of the expectation-maximization (EM) algorithm
during training, and require a reasonable number of labels

11244

for each example. However, in most real world applica-
tions, it is practically prohibitive to collect a large number
of labels per example, and this requirement limits their ap-
plications. A notable exception is the Model Boostrapped
EM (MBEM) algorithm presented in [19] that is capable of
learning even with little label redundancy.

In this paper, we propose a more effective alternative to
these EM-based approaches for jointly modeling the anno-
tator skills and GT label distribution. Our method separates
the annotation noise from true labels by (1) ensuring high
ﬁdelity with the data by minimizing the cross entropy loss
and (2) encouraging the estimated annotators to be maxi-
mally unreliable by minimizing the trace of the estimated
confusion matrices. Our method is also simpler to imple-
ment, only requiring an addition of a regularization term to
the cross-entropy loss. Furthermore, we provide a theoret-
ical result that such regularization is capable of recovering
the annotation noise as long as the average confusion matrix
(CM) over annotators is diagonally dominant.

Experiments on image classiﬁcation tasks with both sim-
ulated and real noisy labels demonstrate that our method,
despite being much simpler, leads to better or compara-
ble performance with MBEM [19] and generalized EM
[15, 20], and is capable of recovering CMs even when there
is only one label available per example. We simulated a
diverse range of annotator types on MNIST and CIFAR10
data sets while we used a ultrasound dataset for cardiac view
classiﬁcation to test the efﬁcacy in a real-world application.
We also show importance of modeling individual annotators
by comparing against various modern noise-robust methods
[21, 22, 23, 24], when the inter-annotator variability is high.

Other Related Works. More broadly, our work is related
to methods for robust learning in the presence of label noise.
There is a large body of literature that do not explicitly
model individual annotators unlike our method.

The effects of label noise are well studied in common
classiﬁers such as SVMs and logistic regression, and ro-
bust variants have been proposed [25, 26, 27]. More re-
cently, various attempts have been made to train deep neu-
ral networks under label noise. Reed et al. [21] developed
a robust loss to model “prediction consistency”, which was
later extended by [28]. In [29] and [22], label noise was
parametrized in the form of a transition matrix and incor-
porated into neural networks for binary and multi-way clas-
siﬁcation. A more effective alternative for estimating such
transition matrix was proposed in [30], and a method for
capturing image dependency of label noise was shown in
[31]. We will later compare our model to several of these
methods to test the value of modelling individual annotators
in gaining robustness to label noise.

Multiple lines of work have shown that a small portion
of clean labels improves robustness. [32] proposed to learn
from clean labels to correct the labels of noisy examples.

[33] proposed a method for learning to weigh examples dur-
ing each training iteration by using the validation loss on
clean labels as the meta-objective.
[34] employs a simi-
lar approach, but trains a separate network that proposes
weighting. However, curating a set of clean labels of sufﬁ-
cient size is expensive for many applications, and this work
focuses on the scenario of learning from purely noisy labels.

2. Methods

i

i

i

}i=1,...,N .

, ..., ˜y(R)

i }r=1,...,R

We assume that a set of images {xi}N

i=1 are assigned
with noisy labels {˜y(r)
i=1,...,N from multiple annotators
where ˜y(r)
denotes the label from annotator r given to ex-
ample xi, but no ground truth (GT) labels {yi}i=1,...,N
are available.
In this work, we present a new pro-
cedure for multiclass classiﬁcation problem that can si-
multaneously estimate the annotator noise and GT label
distribution p(y|x) from such noisy set of data D =
{xi, ˜y(1)
The method only requires
adding a regularization term, that is the average accuracy
of all annotator models, to the cross-entropy loss function.
Intuitively, the method biases ours models of each annota-
tor to be as inaccurate as possible while having the model
still explain the data. We will show that this is capable of
decoupling the annotation noise from the true label distribu-
tion, as long as the average labels of the real annotators are
“sufﬁciently” correct (which we formalize in Sec. 2.3). For
simplicity, we ﬁrst describe the method in the dense label
scenario in which each image has labels from all annota-
tors, and then extend to scenarios with missing labels where
only a subset of annotators label each image. As we shall
see later, the method works even when each image is only
labelled by a single annotator.

2.1. Noisy Observation Model

We ﬁrst describe our probabilistic model of the observed
noisy labels from multiple annotators.
In particular, we
make two key assumptions: (1) annotators are statistically
independent, (2) annotation noise is independent of the in-
put image. By assumption (1), the probability of observing
noisy labels {˜y(1), ..., ˜y(R)} on image x can be written as:

p(˜y(1), ..., ˜y(R)|x) =

Zy∈Y

RYr=1

p(˜y(r)|y, x) · p(y|x)dy (1)

where p(y|x) denotes the true label distribution of the im-
age, and p(˜y(r)|y, x) describes the noise model by which
annotator r corrupts the ground truth label y. For clas-
siﬁcation problems, the label y takes a discrete value in
Y = {1, ..., L}. From assumption (2), the probability that
annotator r corrupts the GT label y = i to ˜y(r) = j is
independent of the image x i.e. p(˜y(r) = j|y = i, x) =
p(˜y(r) = j|y = i) =: a(r)
ji . Here we refer to the associ-

11245

Figure 1: General schematic of the model (eq. 2) in the presence of 4 annotators. Given input image x, the classiﬁer parametrised by
θ generates an estimate of the ground truth class probabilities, pθ(x). Then, the class probabilities of respective annotators p(r)(x) :=
A(r)pθ(x) for r ∈ {1, 2, 3, 4} are computed. The model parameters {θ, A(1), A(2), A(3), A(4)} are optimized to minimize the sum of
four cross-entropy losses between each estimated annotator distribution p(r)(x) and the noisy labels ˜y(r) observed from each annotator.
The probability that each annotator provides accurate labels can be estimated by taking the average diagonal elements of the associated
confusion matrix (CM), which we refer to as the “skill level” of the annotator.

ated L × L transition matrix A(r) = (a(r)
ji ) as the confusion
matrix (CM) of annotator r. The joint probability over the
noisy labels is simpliﬁed to:

p(˜y(1), ..., ˜y(R)|x) =

RYr=1

LXy=1

a(r)
˜y(r),y · p(y|x)

(2)

Fig. 1 provides a schematic of our overall architecture,
which models the different constituents in the above joint
probability distribution.
In particular, the model consists
of two components: the base classiﬁer which estimates the
ground truth class probability vector ˆpθ(x) whose ith ele-
ment approximates p(y = i|x), and the set of the CM es-
timators { ˆA(r)}R
r=1. Each
product ˆp(r)(x) := ˆA(r) ˆpθ(x) represents the estimated
class probability vector of the corresponding annotator. At
inference time, we use the most conﬁdent class in ˆpθ(x)
as the ﬁnal classiﬁcation output. Next, we describe our op-
timization algorithm for jointly learning the parameters of
the base classiﬁer, θ and the CMs, { ˆA(r)}R

r=1 which approximate {A(r)}R

r=1.

2.2. Joint Estimation of Confusion and True labels

i }N

Given training inputs X = {xi}N

i=1 and noisy labels
i=1 for r = 1, ..., R, we optimize the param-
eters {θ, ˆA(r)} by minimizing the negative log-likelihood

eY(r) = {˜y(r)
(NLL), −log p(eY(1), ..., eY(R)|X). From eq. 2, this op-

timization objective equates to the sum of cross-entropy
losses between the observed labels and the estimated an-
notator label distributions:

−log p(eY(1), ..., eY(R)|X) =

NXi=1

RXr=1

CE(A(r)ˆpθ(xi), ˜y(r)

i

).

(3)
Minimizing above encourages each annotator-speciﬁc pre-
diction ˆp(r)(x) := ˆA
ˆpθ(x) to be as close as possible to
the noisy label distribution of the corresponding annotator

(r)

p(r)(x). However, this loss function alone is not capable
of separating the annotation noise from the true label dis-
tribution; there are inﬁnite combinations of { ˆA(r)}R
r=1 and
classiﬁcation model ˆpθ such that ˆp(r) perfectly matches the
annotator’s label distribution p(r) for any input x.

(r)

P. Minimizing the cross-entropy loss
(r)

To formalize this problem, we denote the CM of the es-
timated true label distribution1 ˆpθ by P. The CM of the
estimated annotator’s label distribution ˆp(r) is then given
by the product ˆA
(eq. 3) encourages ˆA
P to converge to the true CM of the
corresponding annotator A(r) i.e. ˆA
P → A(r). However,
there are inﬁnitely many solutions pairs ( ˆA
, P) that sat-
isfy the equality ˆA
P = A(r). This means that we need to
regularize the optimization to encourage convergence to the
desired solutions i.e. ˆA

→ A(r) and P → I.

(r)

(r)

(r)

(r)

To combat this problem, we propose to add the trace of
the estimated CMs to the loss in eq. 3. Extending to the
“missing labels” regime in which only a subset of annota-
tors label each example, we derive the combined loss:

NXi=1

RXr=1

✶(˜y(r)

i ∈ S(xi))·CE( ˆA(r)ˆpθ(xi), ˜y(r)

i

)+λ

RXr=1

(r)

tr( ˆA

)

(4)
where S(x) denotes the set of all labels available for im-
age x, and tr(A) denotes the trace of matrix A. We
simply perform gradient descent on this loss to learn
{θ, ˆA(1), ..., ˆA(R)}.

Numerous previous work have considered the same
observation model, but proposed various optimization
schemes. The original work [15, 20] employed the gen-
eralized EM algorithm to estimate {θ, ˆA(1), ..., ˆA(R)}, and
more recent work [17, 18] employed variants of hard-EM
to optimize the same model. Khetan et al.,[19] proposed a

1Pji = Rx∈X p(argmaxk[ˆpθ(x)]k = j|y = i)p(x)dx

11246

Figure 2: A diverse set of 4 simulated annotators on CIFAR-10. The top row shows the ground truths while the bottom row are the
estimation from our method, trained with only one label per image.

method called model-bootstrapped EM (MBEM) in which
the predictions of the base neural network classiﬁer are used
in the M-step update of CMs to learn from singly labelled
data, which was not viable with the prior work. However,
in all of the above EM-based methods, each M-step for the
parameters of NN is not available in closed form and thus
performed via gradient descent. This means that every M-
step requires a training of the CNN classiﬁer, rendering each
iteration of EM expensive. A naive solution to this is to per-
form only few iterations of gradient descent in each E-step,
however, this could limit the performance if sufﬁcient con-
vergence is not achieved. Our approach directly maximizes
the likelihood with the trace regularizer and does not suffer
from these issues. In Sec. 4, we show empirically this ap-
proach leads to an improvement both in terms of accuracy
and convergence rate over the previous methods on noisy
labels with high inter-annotator variability.

2.3. Motivation for Trace Regularization

(r)

Here we intend to motivate the addition of the trace reg-
ularizer in eq. 4. In the last section, we saw that minimizing
cross-entropy loss alone encourages ˆA
P → A(r). There-
fore, if we could devise a regularizer which, when mini-
mized, uniquely ensures the convergence ˆA
→ A(r), then
this would make P tend to the identity matrix, implying that
the base model fully captures the true label distribution i.e.
argmaxk[ ˆp(x)θ]k = y ∀x. We describe below the trace reg-
ularizer is indeed a such regularizer when both ˆA(r) and
A(r) satisfy some conditions. We ﬁrst show this result as-
suming that there is a single annotator, and then extend to
the scenario with multiple annotators.

(r)

Lemma 1 (Single Annotator). Let P be the CM of the es-
timated true labels ˆpθ and ˆA be the estimated CM of the
annotator. If the model matches the noisy label distribution
of the annotator i.e. ˆAP = A, and both ˆA and A are diago-
nally dominant (aii > aij , ˆaii > ˆaij ) for all i 6= j, then ˆA
with the minimal trace uniquely coincides with the true A.

Proof. We show that each diagonal element in the true CM
A forms a lower bound to the corresponding element in its
estimation.

aii =Xj

ˆaijpji ≤Xj

ˆaiipji = ˆaii(Xj

pji) = ˆaii

(5)

It therefore follows that tr(A) ≤
for all i ∈ {1, ..., L}.
tr( ˆA). We now show that the equality ˆA = A is uniquely
achieved when the trace is the smallest i.e. tr(A) = tr( ˆA) ⇒
A = ˆA. From (5), if the trace of A and ˆA are the same,
we see that their diagonal elements also match i.e. aii =
ˆaii∀i ∈ {1, ..., L}. Now, the non-negativity of all elements

in CMs P and ˆA, and the equality aii = Pj ˆaijpji imply

that pji = ✶[i = j] i.e. P is the identity matrix.

We note that the above result was also mentioned in [22]
in a more general context of label noise modelling (that ne-
glects annotator information). Here we further augment
their proof by showing the uniqueness of solutions (i.e.
tr(A) = tr( ˆA) ⇒ A = ˆA).
In addition, the trace regu-
larization was never used in practice in [22] — for imple-
mentation reason, the Frobenius norm was used in all their
experiments. We now extend this to the multiple annota-
tor regime. We will show later that minimizing the mean
trace of all annotators indeed enhances the estimation qual-
ity of both CM and true label distributions, particularly in
the presence of high annotator disagreement.

(r)

(r)

If ˆA

:= R−1PR

Theorem 1 (Multiple Annotators). Let ˆA
be the
P = A(r) for
estimated CM of annotator r.
r = 1, ..., R, and the average true and estimated CMs
(r)
A∗
are
argmin ˆA
In other words, when the trace of the mean CM is mini-
mized, the estimation of respective annotator’s CMs match
the true values.

)i and such solutions are unique.

r=1 A(r) and ˆA
dominant,
(R)htr(ˆA

:= R−1PR

then A(1), ..., A(R)

diagonally

,...,ˆA

r=1

ˆA

=

(1)

∗

∗

11247

∗

∗

∗

) with equality if and only if A∗ = ˆA

Proof. As the average CMs A∗ and ˆA
are diagonally domi-
nant and we have A∗ = ˆA
P, Lemma 1 yields that tr(A∗) ≤
tr( ˆA
. Therefore,
when the trace of the average CM of annotators is mini-
mized i.e. tr( ˆA
) = tr(A∗), the estimated CM of the true
label distribution P reduces to identity, giving ˆA
= A(r)
for all r ∈ {1, ..., R}.

(r)

∗

∗

(r)

The above result shows that if each estimated annota-
tor’s distribution ˆA
ˆpθ(x) is very close to the true noisy
distribution p(r)(x) (which is encouraged by minimizing
the cross-entropy loss), and on average for each class c, the
number of correctly labelled examples exceeds the number
of examples of every other class c′ that are mislabelled as c
(the mean CM is diagonally dominant), then minimizing its
trace will drive the estimates of CMs towards the true val-
ues. To encourage { ˆA(1), ..., ˆA(R)} to be also diagonally
dominant, we initialize them with identity matrices. Intu-
itively, the combination of the trace term and cross-entropy
separates the true distribution from the annotation noise by
ﬁnding the maximal amount of confusion which can explain
the noisy observations well.

3. Experiments

We now aim to verify the proposed method on various
image recognition tasks. Particularly, we demonstrate (1)
advantage of our simpler optimization scheme compared to
EM-based approaches (Sec. 3.2), (2) importance of mod-
eling multiple annotators (Sec. 3.3) and (3) the applica-
bility of the model in a challenging real world application
(Sec. 3.2). We address the ﬁrst two questions by testing the
proposed method on MNIST and CIFAR-10 datasets with
a diverse set of simulated annotators. To answer the ﬁnal
question, we evaluate our approach on the task of cardiac
view classiﬁcation using ultrasound images where the la-
bels are noisy and sparse, and are acquired from multiple
annotators of varying levels of expertise.

3.1. Set Up

We focus on a regime in which models have only access
to noisy labels from multiple annotators. For MNIST and
CIFAR-10 data sets, we simulate noisy labels from a range
of annotators with different skill levels and biases.

MNIST Experiments. We consider two different models
of annotator types: (i) pairwise-ﬂipper: each annotator is
correct with probability p or ﬂips the label of each class to
another label (the ﬂipping target is chosen uniformly at ran-
dom for each class), (ii) hammer-spammer: each annotator
is always correct with probability p or otherwise chooses
labels uniformly at random [19]. For each annotator type
and skill level p, we create a group of 5 annotators by gen-
erating CMs from the associated distribution (illustration of

CMs are given in the supplementary material). Given the
GT labels, we generate noisy labels as deﬁned by the CM
per annotator. These noisy labels are used during training.

CIFAR-10 Experiments. We consider a diverse group of
4 annotators with different patterns of CMs as shown in
Fig. 2: (i) is a “hammer-spammer” as deﬁned above, (ii)
tends to mix up semantically similar categories of images
e.g. cats and dogs, and automobiles and trucks, (iii) is likely
to confuse “neighbouring” classes and (iv) is an adversarial
annotator who has a wrong association of class names to
object categories. On average, labels generated by these an-
notators are correct only 45% of the time.

In synthetic experiments, we assume that equal number
of labels are generated by each annotator on average. We
also note that all models are trained on noisy labels and
do not have access to the ground truth. Unless otherwise
stated, we hold out 10% of training images as a validation
set, on which the best performing model is selected. We
also perform no data augmentation during training. Full de-
tails of training and model architectures are provided in the
supplementary material. In Sec. 3.2 and Sec. 3.3 below, we
compare our model against two separate sets of baselines to
address different questions.

Figure 3: Comparison between our method, generalized EM,
MBEM trained on noisy labels on MNIST from “pairwise ﬂip-
pers” for a range of mean skill level p. (a), (b) show classiﬁcation
accuracy in two cases, one where all annotators label each exam-
ple and the other where only one label is available per example.
(c), (d) quantify the CM recovery error as the annotator-wise aver-
age of the normalized Frobenius norm between each ground truth
CM and its estimate. The shaded areas represent the cases where
the average CM over the annotators are not diagonally dominant.

11248

Figure 4: Visualization of the mean CM estimates when the di-
agonal dominance (D.D.) holds (mean skill level, p = 0.3) and
does not hold (p = 0.25). In all cases, only one label is provided
per image. The numbers are rounded to nearest integers. Here
the respective models are trained on the noisy labels from 5 “pair-
wise ﬂippers”. Note that when each image receives only 1 label,
the generalised EM [15] completely fails to recover the CM due
to the failure of M-step for updating the confusion matrices (see
Algorithm. 2 in the supplementary material).

3.2. Comparing with EM based Approaches

This section examines the ability of our method in learn-
ing the CMs of annotators and the GT label distribution
on MNIST and CIFAR-10.
In particular, we compare
against two prior methods: (1) generalized EM [20], the
ﬁrst method for end-to-end training of the CM model in
the presence of multiple annotators, and (2) Model Boot-
strapped EM (MBEM) [19],
the present state-of-the-art
method. We analyze the performance in two cases, one in
which all labels from 5 annotators are available for each
image (“dense labels”), and another where only one ran-
domly selected annotator labels each example (“1 label per
image”). We quantify the error of CM estimation by the
average Frobenius norm between each CM and its estimate
over the annotators, and this metric is normalized to be in
the range [0, 1] by dividing by the number of classes L i.e.

R−1L−1PrPi,j ||a(r)

ij − ˆa(r)

ij ||2.

Performance Comparison. Fig. 3 compares the classiﬁ-
cation accuracy and the error of CM estimation on MNIST
for a range of mean skill-levels p where labels are generated
by a group of 5 “pairwise-ﬂippers”. The “oracle” model is
the idealistic scenario where CMs of the annotators are a
priori known to the model while “annotators” indicate the
average labeling accuracy of each annotator group.

Fig. 3 shows a strong correlation between the classiﬁca-
tion accuracy and the error of CM estimation. We observe
our model displays consistently better or comparable per-
formance in terms of both classiﬁcation accuracy and esti-
mation of CMs with dense labels (Fig. 3(a) and (c)). When
each example receives only one label from one of the anno-
tators, we observe the same trend as long as the mean CMs
are diagonally dominant (Fig. 3(b,d)). We also observe that
when the diagonal dominance holds, all three methods per-

Method

Accuracy

CM error

Our method
Our method (no trace norm)
MBEM [19]
generalized EM [15]

81.23 ± 0.21
80.29 ± 0.65
73.33 ± 0.46
70.49 ± 0.23

0.72 ± 0.01
1.37 ± 0.12
2.53 ± 0.24
6.13 ± 0.28

Single CM [22]
Weighted Doctor Net [24]
Soft-bootstrap [21]
Vanilla CNN [21]

68.82 ± 2.27
60.11 ± 1.80
54.73 ± 1.33
52.33 ± 0.31

-
-
-
-

(a) Dense labels

Method

Accuracy

CM error

Our method
Our method (no trace norm)
MBEM [19]
generalized EM [15]

77.65 ± 0.31
76.31 ± 0.49
55.97 ± 1.23
53.38 ± 0.71

1.22 ± 0.01
1.46 ± 0.27
4.58 ± 0.64
4.47 ± 0.64

Single CM [22]
Weighted Doctor Net [24]
Soft-bootstrap [21]
Vanilla CNN [21]

59.91 ± 0.98
57.98 ± 0.14
42.91 ± 1.08
36.04 ± 1.04

-
-
-
-

(b) 1 label per image

Table 1: Mean classiﬁcation accuracy and CM estimation errors
(×10−2) on CIFAR-10 with dense labels. Average annotator ac-
curacy is 45%. Standard deviations are computed based on 3 runs
with varied weight initialization.

form better than the annotators. On the other hand, when the
diagonal dominance does not hold (see the grey regions), all
models undergo a steep drop in classiﬁcation accuracy due
to the inability to estimate CMs accurately as reﬂected in
Fig. 3(c,d), which is consistent with Theorem. 1. Fig. 4 also
visualizes the average of the estimated CMs at this break
point. We also note that with only one label per image,
the generalized EM algorithm [15, 20] is not capable of re-
covering CMs at all and predict identity matrices (Fig. 4),
which renders the model equivalent to a vanilla classiﬁer
directly trained on noisy labels. A similar set of results in
the “spammer-hammer” case are also available in the sup-
plementary materials.

On CIFAR-10 dataset, Tab. 1 shows that our method out-
performs MBEM and the generalized EM in terms of both
classiﬁcation accuracy and CM estimation by a large mar-
gin.
In addition, the standard deviations of these metrics
are generally smaller for our method than for the baselines.
Fig. 2 illustrates that our method can estimate CMs of the
4 very different annotators even when each image receives
only one label. Interestingly, Tab. 1 shows that even remov-
ing the trace norm can achieve reasonably high classiﬁca-
tion accuracy and low CM estimation error. We believe this
is because of the unexplained robustness of a deep CNN to
label noise. Nevertheless, adding the trace norm improves
the performance, and we also observe on MNIST that such
improvement is pronounced in the presence of larger noise
(see supplementary materials).

11249

Sensitivity to Hyper-parameters. We next study the ro-
bustness of our method against the generalized EM and
MBEM to the speciﬁcation of hyper-parameters. We used
the group of ﬁve pairwise-ﬂippers with the mean skill level
p = 0.35 to generate noisy labels on MNIST data set. For
our model, we compare the effects of the scaling λ of the
trace-norm in eq. 4 on the trajectory of classiﬁcation accu-
racy on the validation set and the quality of CM estimation.
For the baselines, we experiment by varying the number of
EM steps (denoted by T ) and the number of stochastic gra-
dient descent for each E-step (denoted by G) while ﬁxing
the total number of training iterations at 100, 000. We ob-
served our model presents robustness to different values of
λ as long as the trace-norm loss is not larger than the cross-
entropy loss (where the estimated CMs will start to diffuse
too much), and Fig. 5 shows the stability of the validation
curves for λ ∈ {0.1, 0.01, 0.001}. Both the MBEM and
generalized EM show evident dependence on the values of
T and G and by and large display slower convergence than
our method. We also observe that if too few gradient de-
scents are performed (G = 1000) during each E-step, the
model converges to a lower accuracy in both classiﬁcation
and CM estimation.

Figure 5: Curves of validation accuracy during training of
our method, generalized EM and MBEM for a range of hyper-
parameters. For our method, the scaling of the trace regularizer is
varied in [0.001, 0.01, 0.1]. while, for EM and MBEM, we vary
the number of EM steps (T ), and the number of gradient descent
steps per E-step (G) while ﬁxing the total number of training iter-
ations at 100, 000.

3.3. Value of Modelling Individual Annotators

Now, we compare the performance of our method
against the prior work that aim to improve robustness to
noisy labels without explicitly modelling the individual an-
notators. The ﬁrst baseline is the vanilla classiﬁer trained
on the majority vote labels. We also compare against the
noise robust approaches proposed in [21] and [22]. Reed et
al. [21] adds to the cross-entropy loss a label consistency
term based on the negative entropy of the softmax outputs,
and we used the default hyper-parameter β = 0.95 for com-
parison. Sukhbaatar et al. [22] explicitly accounts for the
label noise with a single CM, but does not model individual

Figure 6: Classiﬁcation accuracy on MNIST of different noise-
robust models as a function of the mean annotator skill level p in
two cases. Here, for each mean skill-level p, a group of 5 “pair-
wise ﬂippers” is formed and used to generate labels.
(a). each
example receives labels from all the annotators. (b). each example
is labelled by only 1 randomly selected annotator.

annotators. We add the trace-norm of the same scaling used
in our method (λ = 0.01) to the loss function for training.
We also include Weighted Doctor Net architecture (WDN)
[24] in the comparison, a recent method that models the an-
notators individually and then learns averaging weights for
combining them. It should be noted that this model consid-
ers a different observation model of the labels and does not
explicitly model the true label distribution. When we have
access to multiple labels per example, with the exception of
WDN, we aggregated the labels by computing the majority
vote and trained all models. This is because we observed a
consistent improvement on validation accuracy (thus poses
a tougher challenge against our method) and this would be a
more realistic utilization of such data set. For both MNIST
and CIFAR-10 experiments, we test on the same set of sim-
ulated labels as used in Sec. 3.2.

Fig. 6 shows better or comparable classiﬁcation accuracy
than all the baselines when the diagonal dominance of the
mean CM holds. In particular, our methods show signiﬁcant
improvement when the mean skill level of the annotators are
relatively low (e.g. p = 0.3 and 0.35). The results are pro-
nounced in the case with only one label available per image
for which the baseline methods undergo a steep drop in ac-
curacy (see Fig. 6(b)). Results in the “spammer-hammer”
case are available in the supplementary material. Similarly
on CIFAR-10 data set, Tab. 1 shows that our method im-
proves the classiﬁcation accuracy upon the baselines. Such
improvement is pronounced in the case of sparse labels. On
the other hand, a vanilla CNN with only L2 weight decay
overﬁts to the training data very quickly in the presence of
such high noise.

11250

(a) Different classes of cardiac views

Method

Ours

w/o trace norm

MBEM [19]
WDN [24]

Single-CM [22]

Soft-bootstrap [21]

Vanilla CNN

Accuracy

CM error

75.57 ± 0.16
70.99 ± 3.31
73.91 ± 0.11
59.15 ± 1.60
74.38 ± 0.29
72.99 ± 0.17
70.95 ± 0.44

11.48 ± 0.48
15.22 ± 0.94
12.18 ± 0.29

-
-
-
-

(d) Performance comparison

(b) Skill estimation

(c) Learned CMs

Figure 7: Results on the cardiac view classiﬁcation dataset: (a) illustrates examples of different cardiac view images.
(b) plots the
estimated skill level of each annotator (average of the diagonal elements of its estimated CM) against the ground truth (c) compares the
estimated CMs of the two least skilled and two most skilled annotators according the GT labels (d) summarizes the classiﬁcation accuracy
and error of CM estimation for different methods.

3.4. Experiments on Cardiac View Classiﬁcation

4. Discussion and Conclusion

Lastly, we illustrate the results of our approach for a real
data set with sparse and noisy labels from the medical do-
main. This data set consists of images of the cardiac region
in different views, acquired using a hand-held ultrasound
probe. The task is to classify a given ultrasound image into
one of six different view classes (see Fig. 7(a)). The process
of obtaining a cardiac view label is crucial for guiding the
user to the correct locations of measurements, and affects
the quality of the downstream cardiac tasks.

A committee of sonographers (with varying levels of ex-
perience) were tasked with providing the cardiac view la-
bels to a large volume of ultra-sound images, and each ex-
ample is only labelled by a subset of them. To acquire
ground truth in this setting, we chose those samples where
the three most experienced sonographers agreed on a given
label. The resulting data set consists of noisy labels pro-
vided by the remaining less experienced 6 sonographers for
a total of 240, 000 training images and 22, 000 validation
images. In addition, we also acquired labels from two non-
expert users and included in the training data.

We estimated the skill-level of each annotator by com-
puting the average value of the diagonal elements in the cor-
responding learned CM, and Fig. 7(b) shows that the group
of experts can be separated from the two non-experts with
varying levels of experinces (one is less competent than the
other). Fig. 7(c) shows that confusion between A3C and
A5C, even common among experts, can be detected (see the
result for ‘Expert 1’) while clearly capturing the patterns of
mistakes for the non-experts. In addition, Fig. 7(d) shows
that our model outperforms MBEM [19] again in classiﬁ-
cation accuracy and the quality of CM estimation. Lastly,
the higher classiﬁcation accuracy of our model with respect
to the other baseline models illustrates again that modelling
individual annotators improves robustness to label noise.

We introduced a new theoretically grounded algorithm
for simultaneously recovering the label noise of multiple an-
notators and the ground truth label distribution. Our method
enjoys implementation simplicity, requiring only adding a
regularization term to the loss function. Experiments on
both synthetic and real data sets have shown superior per-
formance over the common EM-based methods in terms of
both classiﬁcation accuracy and the quality of confusion
matrix estimation. Comparison against the other modern
noise-robust methods demonstrates that the modelling indi-
vidual annotators improves robustness to label noise. Fur-
thermore, the method is capable of estimating annotation
noise even when there is a single label per image.

Our work was primarily motivated by medical imaging
applications for which the number of classes are mostly lim-
ited to below 10. However, future work shall consider im-
posing structures on the confusion matrices to broaden up
the applicability to massively multi-class scenarios e.g. in-
troducing taxonomy based sparsity [18] and low-rank ap-
proximation. We also assumed that there is only one ground
truth for each input; this no longer holds true when the input
images are truly ambiguous—recent advances in modelling
multi-modality of label distributions [35, 36] potentially fa-
cilitate relaxation of such assumption. Another limiting as-
sumption is the image independence of the annotator’s label
noise. The majority of disagreement between annotators
arise in the difﬁcult cases.
Integrating such input depen-
dence of label noise [16, 37] is also a valuable next step.

Acknowledgments
We would like to thank Alon Daks, Israel Malkin and Pouya
Samangouei at Butterﬂy Network for their feedback, and
Dr. Linda Moy, MD of NYU Langone Medical Center for
providing references on inter-reader variability in radiology.
RT is supported by Microsoft Research Scholarship.

11251

References

[1] James Surowiecki. The wisdom of crowds. Anchor, 2005.

[2] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Ar-
naud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen
Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Gin-
neken, and Clara I S´anchez. A survey on deep learning in
medical image analysis. Medical image analysis, 42:60–88,
2017.

[3] Takeyuki Watadani, Fumikazu Sakai, Takeshi Johkoh,
Satoshi Noma, Masanori Akira, Kiminori Fujimoto, Alexan-
der A Bankier, Kyung Soo Lee, Nestor L M¨uller, Jae-Woo
Song, et al.
Interobserver variability in the ct assessment
of honeycombing in the lungs. Radiology, 266(3):936–944,
2013.

[4] Andrew B Rosenkrantz, Ruth P Lim, Mershad Haghighi,
Molly B Somberg, James S Babb, and Samir S Taneja. Com-
parison of interreader reproducibility of the prostate imag-
ing reporting and data system and likert scales for evalua-
tion of multiparametric prostate mri. American Journal of
Roentgenology, 201(4):W612–W618, 2013.

[5] Elizabeth Lazarus, Martha B Mainiero, Barbara Schepps,
Susan L Koelliker, and Linda S Livingston. Bi-rads lexi-
con for us and mammography: interobserver variability and
positive predictive value. Radiology, 239(2):385–391, 2006.

[6] Simon K Warﬁeld, Kelly H Zou, and William M Wells. Si-
multaneous truth and performance level estimation (staple):
an algorithm for the validation of image segmentation. IEEE
transactions on medical imaging, 23(7):903–921, 2004.

[7] Peter Welinder and Pietro Perona. Online crowdsourcing:
rating annotators and obtaining cost-effective labels. In Com-
puter Vision and Pattern Recognition Workshops (CVPRW),
2010 IEEE Computer Society Conference on, pages 25–32.
IEEE, 2010.

[8] Chengjiang Long, Gang Hua, and Ashish Kapoor. Active
visual recognition with expertise estimation in crowdsourc-
ing. In Proceedings of the IEEE International Conference on
Computer Vision, pages 3000–3007, 2013.

[9] Chengjiang Long and Gang Hua. Multi-class multi-
annotator active learning with robust gaussian process for vi-
sual recognition. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2839–2847, 2015.

[10] Alexander Philip Dawid and Allan M Skene. Maximum like-
lihood estimation of observer error-rates using the em algo-
rithm. Applied statistics, pages 20–28, 1979.

[11] Padhraic Smyth, Usama M Fayyad, Michael C Burl, Pietro
Perona, and Pierre Baldi. Inferring ground truth from sub-
jective labelling of venus images. In Advances in neural in-
formation processing systems, pages 1085–1092, 1995.

[12] Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R
Movellan, and Paul L Ruvolo. Whose vote should count
more: Optimal integration of labels from labelers of un-
known expertise.
In Advances in neural information pro-
cessing systems, pages 2035–2043, 2009.

[13] Peter Welinder, Steve Branson, Pietro Perona, and Serge J
Belongie. The multidimensional wisdom of crowds.
In
Advances in neural information processing systems, pages
2424–2432, 2010.

[14] Filipe Rodrigues, Francisco Pereira, and Bernardete Ribeiro.
Learning from multiple annotators: distinguishing good
from random labelers.
Pattern Recognition Letters,
34(12):1428–1436, 2013.

[15] Vikas C Raykar, Shipeng Yu, Linda H Zhao, Anna Jerebko,
Charles Florin, Gerardo Hermosillo Valadez, Luca Bogoni,
and Linda Moy. Supervised learning from multiple experts:
whom to trust when everyone lies a bit. In Proceedings of the
26th Annual international conference on machine learning,
pages 889–896. ACM, 2009.

[16] Yan Yan, R´omer Rosales, Glenn Fung, Mark Schmidt, Ger-
ardo Hermosillo, Luca Bogoni, Linda Moy, and Jennifer
Dy. Modeling annotator expertise: Learning when every-
body knows a bit of something. In AISTATs, pages 932–939,
2010.

[17] Steve Branson, Grant Van Horn, and Pietro Perona. Lean
crowdsourcing: Combining humans and machines in an on-
line system. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 7474–7483,
2017.

[18] Grant Van Horn, Steve Branson, Scott Loarie, Serge Be-
longie, Cornell Tech, and Pietro Perona. Lean multiclass
crowdsourcing.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2714–
2723, 2018.

[19] Ashish Khetan, Zachary C Lipton, and Anima Anandkumar.
In International

Learning from noisy singly-labeled data.
Conference on Learning Representations, 2018.

[20] Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Her-
mosillo Valadez, Charles Florin, Luca Bogoni, and Linda
Moy. Learning from crowds. Journal of Machine Learning
Research, 11(Apr):1297–1322, 2010.

[21] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian
Szegedy, Dumitru Erhan, and Andrew Rabinovich. Train-
ing deep neural networks on noisy labels with bootstrapping.
arXiv preprint arXiv:1412.6596, 2014.

[22] Sainbayar Sukhbaatar,

Joan Bruna, Manohar Paluri,
Lubomir Bourdev, and Rob Fergus. Training convolutional
networks with noisy labels. arXiv preprint arXiv:1406.2080,
2014.

[23] Jacob Goldberger and Ehud Ben-Reuven. Training deep
neural-networks using a noise adaptation layer. In Interna-
tional Conference on Learning Representations, 2017.

[24] Melody Y Guan, Varun Gulshan, Andrew M Dai, and Geof-
frey E Hinton. Who said what: Modeling individual labelers
improves classiﬁcation. AAAI, 2018.

[25] Benoˆıt Fr´enay and Michel Verleysen. Classiﬁcation in the
presence of label noise: a survey. IEEE transactions on neu-
ral networks and learning systems, 25(5):845–869, 2014.

11252

[26] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Raviku-
mar, and Ambuj Tewari. Learning with noisy labels.
In
Advances in neural information processing systems, pages
1196–1204, 2013.

[27] Jakramate Bootkrajang and Ata Kab´an. Label-noise robust
logistic regression and its applications.
In Joint European
conference on machine learning and knowledge discovery in
databases, pages 143–158. Springer, 2012.

[28] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiy-
oharu Aizawa.
Joint optimization framework for learning
with noisy labels. In IEEE Conference on Computer Vision
and Pattern Recognition, 2018.

[29] Volodymyr Mnih and Geoffrey E Hinton. Learning to label
aerial images from noisy data. In International conference
on machine learning, pages 567–574, 2012.

[30] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon,
Richard Nock, and Lizhen Qu. Making deep neural networks
robust to label noise: A loss correction approach. In Proc.
IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pages
2233–2241, 2017.

[31] Jacob Goldberger and Ehud Ben-Reuven. Training deep
In ICLR,

neural-networks using a noise adaptation layer.
2017.

[32] Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhi-
nav Gupta, and Serge J Belongie. Learning from noisy large-
scale datasets with minimal supervision.
In CVPR, pages
6575–6583, 2017.

[33] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urta-
sun. Learning to reweight examples for robust deep learning.
In International Conference on Machine Learning, 2018.

[34] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and
Li Fei-Fei. Mentornet: Learning data-driven curriculum for
very deep neural networks on corrupted labels. In Interna-
tional Conference on Machine Learning, pages 2309–2318,
2018.

[35] Ardavan Saeedi, Matthew D Hoffman, Stephen J DiVerdi,
Asma Ghandeharioun, Matthew J Johnson, and Ryan P
Adams. Multimodal prediction and personalization of
photo edits with deep generative models. arXiv preprint
arXiv:1704.04997, 2017.

[36] Simon AA Kohl, Bernardino Romera-Paredes, Clemens
Meyer, Jeffrey De Fauw, Joseph R Ledsam, Klaus H Maier-
Hein, SM Eslami, Danilo Jimenez Rezende, and Olaf Ron-
neberger. A probabilistic u-net for segmentation of ambigu-
ous images. arXiv preprint arXiv:1806.05034, 2018.

[37] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang
Wang. Learning from massive noisy labeled data for im-
age classiﬁcation.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2691–
2699, 2015.

11253

