Generating 3D Adversarial Point Clouds

Chong Xiang

Charles R. Qi

Bo Li

Shanghai Jiao Tong University

Facebook AI Research

University of Illinois at Urbana-Champaign

Shanghai, China

California, USA

Illinois, USA

xiangchong97@gmail.com

charlesq34@gmail.com

lxbosky@gmail.com

Abstract

Deep neural networks are known to be vulnerable to ad-
versarial examples which are carefully crafted instances to
cause the models to make wrong predictions. While adver-
sarial examples for 2D images and CNNs have been exten-
sively studied, less attention has been paid to 3D data such
as point clouds. Given many safety-critical 3D applications
such as autonomous driving, it is important to study how
adversarial point clouds could affect current deep 3D mod-
els. In this work, we propose several novel algorithms to
craft adversarial point clouds against PointNet, a widely
used deep neural network for point cloud processing. Our
algorithms work in two ways: adversarial point perturba-
tion and adversarial point generation. For point perturba-
tion, we shift existing points negligibly. For point genera-
tion, we generate either a set of independent and scattered
points or a small number (1-3) of point clusters with mean-
ingful shapes such as balls and airplanes which could be
hidden in the human psyche. In addition, we formulate six
perturbation measurement metrics tailored to the attacks in
point clouds and conduct extensive experiments to evaluate
the proposed algorithms on the ModelNet40 3D shape clas-
siﬁcation dataset. Overall, our attack algorithms achieve a
success rate higher than 99% for all targeted attacks 1.

1. Introduction

Despite of the great success in various learning tasks,
deep neural networks (DNNs) have been found vulnerable
to adversarial examples. The adversary is able to add im-
perceivable perturbation to the original data and mislead
DNNs with high conﬁdence. Many algorithms have been
proposed to generate adversarial examples for data such as
2D images [25, 9, 17, 15, 3], natural languages [10, 33], and
audios [4, 5]. Several recent works [1, 7] have proposed ad-
versarial examples in the 3D space, but they simply project

1Untargeted attacks are easier to achieve with the proposed methods,

so in this paper we only focus on targeted attacks.

Figure 1: Attack pipeline. Our algorithms create adversar-
ial examples by either adversarial point perturbation (left)
or adversarial point generation (right). The bottle is mis-
classiﬁed after our attacks.

3D objects to 2D images as data pre-processing. However,
no existing work has explored the vulnerability of actual 3D
models. In this paper, we study the robustness of 3D mod-
els which directly deal with 3D objects. Speciﬁcally, we
choose to represent 3D objects with point clouds, which are
the raw data from most 3D sensors such as depth cameras
and Lidars. Therefore, we attack 3D models by generating
3D adversarial point clouds.

As to the attacking target, we focus on the commonly
used PointNet model [19]. We choose PointNet because the
model and its variants have been widely and successfully
adopted in many applications such as 3D object detection
for autonomous driving [18, 34, 30], semantic segmentation
for indoor scene understanding [12, 19], and AI-assisted
shape design [24]. Furthermore, the model has been shown
to be robust to various input point perturbations and cor-
ruptions [19]. The demonstrated more robustness than 3D
CNNs makes it a challenging and solid benchmark model
for our evaluations. Although we focus on attacking Point-

19136

Net, we expect our attacking algorithms and evaluation met-
rics extensible to more 3D models.

As the input to PointNet, a point cloud is a 3D geometric
data structure that has the advantages of simple represen-
tation and low storage requirement. However, it is chal-
lenging to generate adversarial point clouds given its spe-
cial properties. The point cloud’s irregular format has made
existing attack algorithms designed for 2D images unsuit-
able: 1) In raw point clouds with XY Z, there are no “pixel
values” positioned in a regular structure that can be slightly
modiﬁed; 2) The search space for generating new adversar-
ial points is very large, as points can be added to arbitrary
positions; 3) The commonly used Lp norm measurement
in 2D images to bound perturbations does not ﬁt for point
cloud data with irregularity and varying cardinality.

To the best of our knowledge, we are the ﬁrst to extend
adversarial attack research to the irregular point cloud data,
by addressing aforementioned challenges. We propose sev-
eral novel attack methods for mainly two types of adversar-
ial attacks on point clouds: adversarial point perturbation
and adversarial point generation which are unnoticeable to
human or hidden in the human psyche. The attack pipeline
is illustrated in Figure 1.

For adversarial point perturbation, we propose to shift
existing points negligibly. We optimize the perturbation
vector under the commonly used Lp norm constraint. Our
experiments show that we are able to craft unnoticeable ad-
versarial point clouds with 100% success rate given an ac-
ceptable perturbation budget.

For adversarial point generation, We propose to synthe-
size and place a set of independent points or a limited num-
ber of point clusters close to the original object. In particu-
lar, we search for “vulnerable” regions of objects and opti-
mize the positions of points and the shapes of the clusters.
In total, we have three kinds of generated points, namely
independent points, adversarial clusters (points in generic
shapes such as balls), and adversarial objects (points in ob-
ject shapes such as mini airplanes), shown in the right three
columns in Figure 1. We constrain the generation by their
sizes, their distances to the object surface as well as how
many shapes we place.

Our attacks achieve 100% success rate for scattered ad-
versarial points, 99.3% for adding three adversarial shapes,
98.2% for two, and 78.8% for one.

Furthermore, in Section 6.4 we discuss the transferrabil-
ity of our 3D adversarial point clouds as well as the pos-
sibility to combine PointNet with CNNs to defense attacks
in images. Sample code and data is available at https:
//github.com/xiangchong1/3d-adv-pc to sup-
port further research.

To summarize, the contributions of this paper can be

summarized as follows:

• We are the ﬁrst to generate 3D adversarial examples

against 3D learning models and provide baseline eval-
uations for future research. Speciﬁcally, we choose
representative point cloud data and PointNet model for
our evaluations.

• We demonstrate the unique challenges in dealing with
irregular data structures such as point clouds and pro-
pose novel algorithms for both adversarial point per-
turbation and adversarial point generation.

• We propose six different perturbation metrics tailored
to different attack tasks and perform extensive experi-
ments to show our attack algorithms can achieve a suc-
cess rate higher than 99% for all targeted attacks.

• We provide robustness analysis for 3D point cloud
models and show that analyzing properties of differ-
ent 3D models sheds light on potential defenses for 2D
instances.

2. Related Work

Point Clouds and PointNet. Point clouds are consisted of
unordered points with varying cardinality, which makes it
hard to be consumed by neural networks. Qi et al. [19] ad-
dressed this problem by proposing a new network called
PointNet, which is now widely used for deep point cloud
processing. PointNet and its variants [20, 26] exploit a
single symmetric function, max pooling, to reduce the un-
ordered and varying length input to a ﬁxed-length global
feature vector and thus enables end-to-end learning. [19]
also tried to demonstrate the robustness of the proposed
PointNet and introduced the concept of critical points and
upper bounds. They showed that points sets laying between
critical points and upper bounds yield the same global fea-
tures and thus PointNet is robust to missing points and ran-
dom perturbation. However, they did not study the robust-
ness of PointNet against adversarial manipulations, which
is the main focus of this paper.
Adversarial Examples. Szegedy et al. [25] ﬁrst pointed
out that machine learning models such as neural networks
were vulnerable to carefully crafted adversarial perturba-
tion. An adversarial example which appears similar to its
original data can easily fool the neural networks with high
conﬁdence. Such vulnerability of machine learning mod-
els has raised great concerns in the community and many
works have been proposed to improve the attack perfor-
mance [9, 17, 15, 3, 16, 29, 28] and search for possible
defense [2, 14, 31, 21, 22, 32]. The state-of-the-art attack
algorithm, optimization based attack [3], deﬁnes an objec-
tive loss function which measures both attack effectiveness
and perturbation magnitude, and uses optimization to ﬁnd a
near-optimal adversarial solution. However, the algorithm
only deals with 2D data. Several recent works [11, 1, 7]
also study the adversarial examples in the physical world.
However, these works only project physical objects to 2D

9137

images and do not study models which directly deal with
3D objects. To the best of our knowledge, we are the ﬁrst
to generate adversarial examples for 3D machine learning
models.

3. Problem Formulation

Point Cloud Data. A point cloud is a set of points which
are sampled from object surfaces. A data record x ∈ Rn×3
corresponds to a point set of size n, where each point is
represented by a 3-tuple (x, y, z) coordinate. One most im-
portant characteristic of point cloud data is its irregularity
(a point cloud is not deﬁned on a regular grid structure),
which makes it hard to adapt existing attacking algorithms
from 2D images. Moreover, we are able to add points at
any positions in the 3D space while we cannot add pixels
in 2D images. However, such lack of constrain results in
an extremely large search space for generative adversarial
examples. New attack algorithms should be proposed to ad-
dress the above problems.

Targeted Adversarial Attacks.
In this paper, we only fo-
cus on targeted attacks against 3D point cloud classiﬁcation
models. It is ﬂexible to extend our algorithms to other tasks
like attacking segmentation models.

The goal of targeted attacks is to mislead a 3D deep
model (e.g., PointNet) to classify an adversarial example as
a selected target class. Formally, for a classiﬁcation model
F : X → Y, which maps an input x ∈ X ⊂ Rn×3 to its
corresponding class label y ∈ Y ⊂ Z, an adversary has a
malicious target class t′ ∈ Y. Based on a perturbation met-
ric D : Rn×3 × Rn′×3 → R, the goal of the attack is to ﬁnd
a legitimate input x′ ⊂ Rn′×3 which:

min D(x, x′),

s.t. F(x′) = t′

(1)

Note that, for point cloud data, n does not necessarily equal
to n′.

As mentioned in [3], directly solving this problem is dif-
ﬁcult. Therefore, we reformulate the problem as gradient-
based optimization algorithms:

min f (x′) + λ ∗ D(x, x′)

(2)

Here f (x′) = (max
i6=t′

(Z(x′)i) − Z(x′)t′ )+ is the adver-

sarial loss function whose output measures the possibility
of a successful attack, where Z(x)i is the ith element of
the logits (the input of softmax layer) and (r)+ represents
max(r, 0). By optimizing over Equation 2, we aim to search
for adversarial examples with least 3D perturbation.

Attacking Types.
In this paper, we consider two differ-
ent types of attacks in point clouds 2: adversarial point per-

2To guarantee the points can still cover the object surface, we do not

allow an adversary to remove points.

turbation and adversarial point generation. In perturbation
attacks, we modify existing points by shifting their XY Z
positions with adversarial jitters such that a point xi ∈ R3
in the point cloud x becomes x′
i = xi + δi, for i = 1, ..., n
where δi ∈ R3 is the perturbation to the i-th point.
In
generation attacks, we generate a set of adversarial points
z = {zi|i = 1, ..., k} (or z ∈ Rk×3 as an array represen-
tation of it) where each zi ∈ R3 is a new point in addition
to the existing point cloud x. Then the union of the orig-
inal points and adversarial points are input to the model:
x′ = x ∪ z, or in the array representation x′ ∈ R(n+k)×3
through array concatenation (thus n′ = n + k). This man-
ner of attacking is very new and vastly different from at-
tacks in images, because we cannot generate new pixels in
a ﬁx-sized image.

4. Adversarial Point Perturbation

In this section, we focus on the ﬁrst and the simpler type
of point cloud attack: adversarial point perturbation. Since
for perturbation we have correspondences between the orig-
inal points and the perturbed ones, we can simply use Lp
norm to measure the distance between the two clouds.
Lp Norm. The Lp norm is a commonly used metric for ad-
versarial perturbation of ﬁxed-shape data. For the original
point sets S and corresponding adversarial set S ′, the Lp
norm of the perturbation is deﬁned as:

DLp (S, S ′) = (X

(si − s′

i)p)

1

p

(3)

i

where si is the ith point coordinate in set S, and s′

i is its

corresponding point in set S ′.

We can directly use Equation 2 to generate the adversar-
i=1, by optimization with the L2 norm

ial perturbations {δi}n
distance to bound the perturbation.

5. Adversarial Point Generation

Besides perturbing existing points, another general type
of attacking strategy is to generate new adversarial points
to mislead the 3D model. Among the ways to generate new
points, a simple approach is to add arbitrary number of in-
dependent points (Section 5.1), ideally close to the object
surface so that they are unnoticeable 3.

On the other hand, we consider a more challenging at-
tack task where the adversary is only able to add a limited
number (1-3) of adversarial shapes (Section 5.2 and Sec-
tion 5.3), as either generic primitive shapes such as balls or
meaningful shapes such as small airplane models. The task
is challenging since points can only be added within small
regions of the 3D space and the points of original object
remain unchanged. The goal of this attack is to generate

3How to realize this attack in real world is still a question though.

9138

adversarial point clusters that are plausible so they cloud be
hidden in the human psyche.

In the following subsections, we will introduce metrics
and our attacking algorithms to generate adversarial indi-
vidual points, as well as two kinds of adversarial shapes:
adversarial clusters and adversarial objects.

5.1. Generating Adversarial Independent Points

In this section, we focus on the attack of generating (un-
noticeable) independent points. Note that when adding new
points to the original point clouds, we have to deal with
data dimensionality changes. We ﬁrst introduce metrics that
measure the deviation of adversarial points to the original
one and then desrcribe our attack algorithm.

5.1.1 Perturbation Metrics

Hausdorff Distance. Hausdorff distance is often used to
measure how far two subsets of a metric space are from
each other. Formally, for an original point set S and its
adversarial counterpart S ′, we deﬁne Hausdorff distance as:

DH(S, S ′) = max
y∈S ′

min
x∈S

kx − yk2
2

(4)

Intuitively, Hausdorff distance ﬁnds the nearest original
point for each adversarial point and outputs the maximum
square distance among all such nearest point pairs. We do
not include the term max
2 since we do not
x∈S

kx − yk2

min
y∈S ′

modify the original object S.
Chamfer Measurement.4 Chamfer measurement [8] is a
similar perturbation metric as Hausdorff distance. The dif-
ference is Chamfer Measurement takes the average, rather
than the maximum, of the distances of all nearest point
pairs. The formal deﬁnition is as follows:

DC(S, S ′) =

1

kS ′k0

X

y∈S ′

min
x∈S

kx − yk2
2

(5)

Number of Points Added. We also want to measure the
number of points added in our attack, by counting points
whose distances from the object surface is above a certain
threshold. Formally, for an original point set S, the gener-
ated point set S ′, and a threshold value Tthre, the number of
points added is deﬁned as:

Count(S, S ′) = X

y∈S ′

✶[min
x∈S

kx − yk2 > Tthre]

(6)

where ✶[·] is the indicator function whose value is 1 when
the statement is true and 0 otherwise. Note that the number
of points added is not optimized as the perturbation metric

4We name it as “Chamfer measurement” since this perturbation metric
does not satisfy triangle inequality, which means it does not satisfy the
deﬁnition of distance.

D in Equation 2 due to its incompatibility with gradient-
based optimization algorithms, but is reported as an addi-
tional performance metric.

5.1.2 Attacking Algorithm

Directly adding points to the unconstrained 3D space is in-
feasible due to the large search space. Therefore we propose
an initialize-and-shift method to ﬁnd appropriate position
for each added point:

1. Initialize a number of points to the same coordinates

of existing points as initial points.

2. Shift initial points via optimizing Equation 2 and out-

put their ﬁnal positions.

During the optimization process, some initial points are
shifted from their initial positions and “added” to the origi-
nal objects as adversarial points. The others that are barely
shifted do not change the shape of the object, and thus can
be discarded as points-not-added.

To make the optimization more efﬁcient, we propose to
initialize points to the positions of “critical points” of the
target. Critical points are like key points or salient points
in a 3D point cloud. In PointNet speciﬁcally, they can be
computed by taking the points that remain active after the
max pooling [19], which means they are at important posi-
tions that determine the object category. Adversarial points
around these critical positions are more likely to change the
ﬁnal prediction.

We use Hausdorff and Chamfer measurements as the per-
turbation metrics D for this attack because they are more ca-
pable of measuring how unnoticeable the adversarial point
clouds of different dimensionality are.

5.2. Generating Adversarial Clusters

For adversarial clusters, we aim to minimize the radius
of the generated cluster so that they look like a ball attached
to the original object and will not arouse suspicion. In addi-
tion, we also encourage the cluster to be close to the object
surface. To satisfy these two requirements, we introduce the
perturbation metrics used as follows.

5.2.1 Perturbation Metrics

Farthest Distance. If the farthest pair-wise point distance
in a point set is controlled within a certain threshold, the
points in this set are able to form a shaped cluster. Formally,
we deﬁne farthest distance of a point set S as:

Dfar(S) = max
x,y∈S

kx − yk2

(7)

Chamfer Measurement. Besides encouraging point clus-
ter to form within a small radius, we may also want to push

9139

the added clusters towards the surface of the object. There-
fore, we also include the Chamfer Measurement (deﬁned
in Equation 5) as our perturbation metric and optimization
objective.
Number of Clusters Added. Similar to the number of
points added in the unnoticeable adversarial point cloud
generation, the number of clusters added also serves as
an additional metric for attack performance, which is hard
bounded to 1-3 in our experiments.

5.2.2 Attacking Algorithm

Before going into the details of generation algorithms, we
need to reformulate Equation 2 as follow:

min f (x′) + λ · (X

Dfar(Si) + µ · DC(S0, Si))

(8)

i

where i ∈ {1, 2, . . . , m}, S0 is the original object, Si is
the ith adversarial point cluster, m is number of adversarial
clusters, and µ is the weight used to balance the importance
between Farthest Distance loss and Chamfer Measurement
loss. Here we abuse the notation a little to use D to denote
both mappings Rn×3 × Rn′×3 → R and Rn×3 → R.

Generating adversarial clusters is a special case of
adding adversarial point clouds, so we can adopt
the
initialize-and-shift method used. However, unlike indepen-
dent point generation, we have to constrain the added points
clustered to be within small regions. As points are likely to
get stuck in their initialized vicinity due to the ubiquity of
local-minima, we need a more efﬁcient initialization meth-
ods for adversarial clusters generation.

We try to leverage the idea of “vulnerable regions” for
initialization. For formatted data like 2D images, it is com-
mon to impose a L1 constraint to encourage the sparsity
of the perturbation vector. The region with large perturba-
tion under a proper L1 constraint is believed to be impor-
tant for model decisions and thus vulnerable to adversarial
attacks. However, the L1 constraint is not well deﬁned on
point clouds thus inapplicable here. Instead, we take advan-
tage of “critical points” again to effectively ﬁnd potentially
vulnerable regions for initialization. Critical points, as a
subset of the original set, collectively determine the global
features of the object shape but could also be vulnerable re-
gions to attacks.

Given a victim object and a target class t′, the attack pro-

cess is as follows:

1. Obtain the critical points of the objects in target class.

2. Use the clustering algorithm DBSCAN [6] to cluster

the selected critical points.

3. Choose points in the k largest clusters as the initial
points, where k is a self-chosen parameter as well as
a metric for attack performance evaluation.

4. Optimize over Equation 8 using gradient-based algo-

rithms and ﬁnd optimal cluster positions and shapes.

Note that DBSCAN groups points that are closely packed
(or points with local density passing a threshold), while
marking the other points lying in low-density regions as out-
liers [6]. Thus, we are able to ﬁlter out outlier points and get
compact clusters via it.

Besides tuning DBSCAN, it is essential to determine the
number of objects in target class we use, as well as the
number of critical points selected from each target object.
Choosing only one target object restricts the space distribu-
tions of the critical points. However, using too many target
objects result in density scattered critical points unhelpful
to identify a sparse set of vulnerable regions. The reasons
for tuning the number of critical points selected are simi-
lar, we want a moderate number of them. However, such
parameter tuning does not need to too ﬁne grained as the
attack pipeline is still dominated by the optimization over
Equation 8.

5.3. Generating Adversarial Objects

For this attack, we start from some meaningful objects
like small airplanes, slightly modify them, and place them
in the appropriate adversarial positions. People may not
become suspicious because the adversarial objects are like
other benign objects nearby.

5.3.1 Perturbation Metrics

Lp Norm. Since we want to only slightly modify the mean-
ingful objects and make the generated shapes similar to the
real-world ones, we adopt the Lp, speciﬁcally L2, as our
ﬁrst metric.
Chamfer Measurement. Similar to the adversarial clus-
ters, we want to encourage the generate shape to be close
the original object.
Number of Clusters Added. Number of clusters added,
bounded to 1-3, is also used to evaluate the attack perfor-
mance.

5.3.2 Attacking Algorithm

We also need to rewrite the objective function to ﬁt the at-
tack setting:

min f (x′) + λ · (X

DL2 (Si0, Si) + µ · DC(S0, Si)) (9)

i

where i ∈ {1, 2, . . . , m}, S0 is the original object, Si is the
ith adversarial point cluster, Si0 is the ith real-world clus-
ters, m is number of adversarial clusters, and µ is the weight
used to balance the importance between L2 loss and Haus-
dorff Distance loss. To mount this attack, we need to ﬁnd
the vulnerable regions ﬁrst and then to initialize the perturb

9140

the added real-world point clusters. The attack pipeline is
as follows:

1. Obtain the critical points of the objects in target class.

2. Use the clustering algorithm DBSCAN [6] to cluster

the selected critical points.

3. Identify the k largest clusters and calculate the position
of cluster centers, where k is a self-chosen parameter
as well as a metric for attack performance evaluation.

4. Choose meaningful objects and initialize them to make
their centers overlap with the calculated positions in
the previous step.

5. Optimize over Equation 9 using gradient-based algo-

rithms and ﬁnd optimal cluster positions and shapes.

Note that we also have the freedom to choose the ori-
entation of the modiﬁed clusters. Since adversarial clusters
with different orientations would not arouse suspicion, we
do not impose an constraint on the magnitude of rotation.

6. Experiment Results

In this section, we implement the proposed algorithms
for different attack tasks and conduct extensively evaluation
on attack performance based on various metrics.

6.1. Dataset and 3D Models

We use the aligned benchmark ModelNet40 [27, 23]
dataset for our experiments. The ModelNet40 dataset con-
tains 12,311 CAD models from 40 most common object cat-
egories in the world. 9,843 objects are used for training and
the other 2,468 for testing. As done by Qi et al. [19], we uni-
formly sample 1,024 points from the surface of each object,
and re-scale them into a unit ball. We use the same PointNet
structure as proposed in [19] and train the model with all
ModelNet40 training data to obtain our victim model. The
ModelNet40 dataset is very imbalanced. For our attacks,
we randomly select 25 test examples from each 10 largest
classes, namely airplane, bed, bookshelf, bottle, chair, mon-
itor, sofa, table, toilet and vase, to generated adversarial
point clouds for. For each victim data record, we generate
adversarial examples targeted on the rest 9 classes. There-
fore, we have 2,250 (victim,target) attach pairs for our ex-
periments.

6.2. Adversarial Point Perturbation Evaluation

We evaluate the attack performance for adversarial point
perturbation in this subsection. We use L2 distance as the
perturbation constraint D in Equation 2 and minimize the
objective loss to ﬁnd the optimal perturbation. To obtain
good attack performance, it is essential to choose an appro-
priate value for the weight λ, which controls the balance
between minimizing adversarial loss and perturbation mag-
nitude. If the λ is too small, the perturbation constraint is

Figure 2: Visualization for adversarial point perturbation.

not strong enough and the perturbation would become too
obvious. On the other hand, a λ that is too large would re-
sult in minimizing perturbation magnitude only and fail to
attack. For all of our attacks, we perform 10-step binary
search for the near-optimal λ. During the search, we record
the smallest perturbation D(x, x′) and its corresponding ad-
versarial example x′, and ﬁnally output the most unnotice-
able adversarial example.

we report the experiment results for three cases: best
case for the most easily attacked (victim,target) class pair,
average case for all attacking class pairs, and worst case for
the most difﬁcult pair. The success rate and mean perturba-
tion loss for point shifting attacks are reported in the ﬁrst
two columns of Table 1. We can see we successfully attack
all victims examples into all target classes. The perturbation
loss for this attack is relatively small, considering the per-
turbation vector contains 1,024 elements. We also provide
visualization in the ﬁrst row of Figure 2. We choose class
“bottle” as our visualization victim because adversarial per-
turbation would become more obvious for a simple shape
like a bottle. More visualization for other objects can be
found in the supplementary. From the visualization, we can
see the perturbation (the adversarial point cloud) is nearly
indistinguishable.

6.3. Adversarial Point Generation Evaluation

In this subsection, we evaluate the attack performance
of three different ways of adversarial point generation: in-
dependent points, adversarial clusters, and adversarial ob-
jects.
Adversarial Independent Points. For generating adver-
sarial independent points, we take Hausdorff and Chamfer
measurements as perturbation metrics D and optimize over
Equation 2. We use two different distances separately and
compare performance of these two constraints. To calcu-
late the number of points added, we get the critical points
of the newly generated adversarial point clouds, set Tthre

9141

Case

Best

Average

Worst

Shifting Points (L2 Norm)
success rate
mean loss

Adding Points (DH)

mean loss

#points added

success rate

0.0874
0.3032
0.4674

100%
100%
100%

0.0003
0.0105
0.0210

93
88
99

100%
100%
100%

mean loss
3.1 × 10−5
2.7 × 10−4
7.2 × 10−4

Adding Points (DC)

#points added

success rate

58
51
49

100%
100%
100%

Table 1: Attack performance evaluation for adversarial point perturbation and adversarial independent point generation

Attack

Dfar / DL2

adversarial clusters
adversarial objects

0.5401
0.5539

#Shape 1

DC

0.1374
0.1776

success rate

Dfar / DL2

78.8%
54.6%

0.3118
0.0838

#Shape 2

DC

0.1839
0.1332

success rate

Dfar / DL2

98.2%
93.8%

0.1818
0.0212

#Shape 3

DC

0.1744
0.0855

success rate

99.3%
97.3%

Table 2: Attack performance evaluation for adversarial clusters and adversarial objects (average case).

Figure 3: Visualization for adding 3 adversarial clusters/objects.

to 0.01, and count over points moved further than it. The
experiment results are shown in Table 1. First, both Haus-
dorff and Chamfer constraints result in attack success rate of
100%, proving the effectiveness of proposed initialize-and-
shift algorithm. Secondly, we can see great difference be-
tween the mean distance losses and number of points added.
Since Hausdorff distance only controls the largest distance
of all nearest point pairs while Chamfer measurement cal-
culates the average distance, the loss value of Hausdorff is
much larger than that of Chamfer and so does the number of
points added. A more detailed analysis on the different at-
tack performance of Hausdorff and Chamfer measurement
is included in the supplementary.

The visualization of the adversarial bottle by adding
points is shown in the second and third rows of Figure 2.
From the ﬁgure, we can easily observe the different char-
acteristics of different constraints (Hausdorff constraint re-
sults in more added points while Chamfer constraint leads
to more obvious outliers). Considering the different prop-
erties of the two constraints, one can combine these two
constraints and adjust the weights for the two perturbation
metrics according to the speciﬁc attack goals.

Adversarial Clusters. To get initial clusters, we randomly

select 8 different objects from the target class test set, and
obtain 32 most important critical points for each selected
object based on the number of global feature channels it
contributes to. We then use DBSCAN algorithm to cluster
these 32 × 8 critical points. After that, we retain k clusters
of largest size and discard other small clusters and outliers.
For each cluster, we conduct subsampling or padding to ob-
tain 32 initial points. We vary k from 1 to 3 to see how
the number of adversarial clusters affects the attack perfor-
mance (success rate and distance loss). In Equation 8, the
parameter λ is chosen via 5-step binary search while µ is
preﬁxed according to the adversary’s preference on smaller
or closer clusters. In our experiment, we set µ to 0.1. Due
to the lack of space, we only report the quantitative results
for average case in Table 2. The comprehensive results for
three cases are included in the supplementary.

The table shows that, as the number of adversarial clus-
ters increases, the attack success rate is signiﬁcantly im-
proved and we are able to attack 99.3% examples when
adding 3 adversarial clusters. Moreover, a larger number
of added clusters also helps reduce the perturbation loss for
each cluster. When we only add one cluster, the farthest
distance of the cluster for average case is 0.5401, which is

9142

Add (DH)

Add (DC)

3 Clusters

3 Objects

6.4. More Analysis on 3D Model Robustness

PointNet++ [20]

DGCNN [26]

Augmented PointNet
Different initialization

Shift

3.9%
1.9%
3.0%
5.5%

8.9%
6.8%
4.0%
7.4%

3.6%
7.4%
5.7%
7.8%

8.9%
16.9%
46.5%
48.3%

9.6%
16.5%
34.5%
39.2%

Table 3: Attack success rates of untargeted transfer at-
tacks against PointNet++, DGCNN, augmented PointNet,
and PointNet with a different weight initialization.

ǫ

0

0.1

0.2

0.3

0.4

0.5

LeNet (original)
LeNet (binarized)

PointNet

99.2% 70.7% 35.5% 18.7% 12.0% 9.0%
98.9% 97.2% 93.3% 86.2% 75.4% 28.0%
99.0% 98.6% 98.1% 96.8% 94.1% 63.8%

Table 4: Test accuracy of adversarial examples on MNIST.

quite large considering the whole object ﬁts in a unit ball.
However, the farthest distance drops dramatically to 0.1818
when we are adding 3 clusters. Thus, it is reasonable to ex-
pect better attack performance if the adversary is able to add
more than 3 clusters. Visualization for adding 3 adversarial
clusters can be found in the ﬁrst row of Figure 3. Several
small clusters are clearly shown for most attack pairs.
Adversarial Objects. For this attack, we use the same set-
ting as the adversarial cluster to identify the vulnerable re-
gions. We randomly select an object from the “airplane”
class and initialize it to the centers of different vulnerable
regions. The airplane is re-scaled to three-tenths of its orig-
inal size, and 64 points are uniformly sampled from the sur-
face. 5 After the initialization, we optimize according to
Equation 9 to perform the attack. Similarly, the parameter
λ is determined by binary search while the µ is pre-set to
0.2 in our experiment.

The results for different perturbation metrics are pro-
vided in the second row of Table 2. We can this attack is
more challenging than that of adversarial clusters since the
shapes of the objects are almost predeﬁned. However, the
shapes similar to real-world objects made this attack less
suspicious. Moreover, we still achieve a reasonably high
success rate of 97.3% when adding three adversarial clus-
ters, and we can achieve better performance if more adver-
sarial objects are allowed to be added. We also provide vi-
sualization in the second row of Figure 3. We can see the
several small airplanes near the bottle are already capable
to fool the PointNet model. Comparison between two rows
of visualization in Figure 3 shows that when the clusters
are close to the object, adversarial clusters have better vi-
sualization performance while adversarial objects are less
suspicious when the clusters are far from the surface. This
further justiﬁes our attempt to introduce two kinds of mean-
ingful shapes.

5We choose an airplane to simulate the scenario where the adversary
could manipulate several micro-UAVs to suspend around the victim object.

In this subsection, we provide robustness analyses for

PointNet-like models.

Transferablity of Adversarial Point Clouds. We feed
our crafted adversarial point clouds to PointNet++ [20],
DGCNN [26], a PointNet trained with data augmentation,
and a PointNet trained with a different weight initialization
and ﬁnd that theses 3D adversarial examples actually hardly
transfer as targeted attacks. Furthermore, we calculate the
success rate for untargeted attack and the results are shown
Table 3. We can see the transferability for untargeted at-
tack is also limited compared with 2D adversarial examples.
Since our proposed attack methods are general and can be
applied to attack other 3D models, the low transferability
may be related with special properties of 3D models them-
selves. This intrinsic property makes it possible to design
black-box defense against such adversarial instances.

Defense on MNIST [13] with PointNet. Motivated by
aforementioned robustness analysis, we take a step forward
to use PointNet structure for defense on MNIST dataset. We
binarize the grey-scale images and sample 256 points from
each MNIST digit. We craft adversarial examples by attack-
ing LeNet [13] with FGSM [9]. The test accuracy of bina-
rized images (LeNet) and sampled point clouds (PointNet)
with different values of attack parameter ǫ are reported in
Table 4. We can see the PointNet model achieve relatively
high test accuracy and show promising defense properties
against adversarial examples.

In summary, our analysis shows that PointNet structure
is more robust than traditional CNNs. We believe part of the
robustness comes from the learning of global features via
max pooling. Though the intriguing properties is not fully
understood yet, we believe a further study on this would mo-
tivate a defense direction to include PointNet-like structure
to improve the robustness of traditional nerual networks.

7. Conclusion

Arguable as the ﬁrst to study the vulnerability of 3D
learning models, in this paper, we have proposed several
attacking algorithms to generate adversarial point clouds to
fool the widely used PointNet model, including adversar-
ial point perturbation and adversarial point generation. We
also propose six different perturbation metrics and exten-
sively evaluate the performance of the proposed attack al-
gorithms. Our extensive experiment results show that the
proposed algorithms are able to ﬁnd 3D adversarial point
clouds with an attack success rate higher than 99% given an
acceptable perturbation budget. We hope this work is able
to provide a baseline as well as a guideline for future 3D
adversarial example research.

9143

References

[1] A. Athalye and I. Sutskever. Synthesizing robust adversarial

examples. arXiv preprint arXiv:1707.07397, 2017. 1, 2

[2] N. Carlini and D. Wagner. Defensive distillation is not robust
to adversarial examples. arXiv preprint arXiv:1607.04311,
2016. 2

[3] N. Carlini and D. Wagner. Towards evaluating the robustness
of neural networks. In Security and Privacy (SP), 2017 IEEE
Symposium on, pages 39–57. IEEE, 2017. 1, 2, 3

[4] N. Carlini and D. Wagner.

ples: Targeted attacks on speech-to-text.
arXiv:1801.01944, 2018. 1

Audio adversarial exam-
arXiv preprint

[5] N. Das, M. Shanbhogue, S.-T. Chen, L. Chen, M. E.
Kounavis, and D. H. Chau. Adagio: Interactive experimen-
tation with adversarial attack and defense for audio. arXiv
preprint arXiv:1805.11852, 2018. 1

[6] M. Ester, H.-P. Kriegel, J. Sander, X. Xu, et al. A density-
based algorithm for discovering clusters in large spatial
databases with noise. In Proceedings of the Second Interna-
tional Conference on Knowledge Discovery and Data Min-
ing, KDD’96, pages 226–231, 1996. 5, 6

[7] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li,
A. Prakash, A. Rahmati, and D. Song. Robust physical-
world attacks on deep learning models.
arXiv preprint
arXiv:1707.08945, 1, 2017. 1, 2

[8] H. Fan, H. Su, and L. J. Guibas. A point set generation net-
In

work for 3d object reconstruction from a single image.
CVPR, volume 2, page 6, 2017. 4

[9] I. Goodfellow, J. Shlens, and C. Szegedy.

ing and harnessing adversarial examples.
arXiv:1412.6572, 2014. 1, 2, 8

Explain-
arXiv preprint

[10] R. Jia and P. Liang.

uating reading comprehension systems.
arXiv:1707.07328, 2017. 1

Adversarial examples for eval-
arXiv preprint

[11] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial exam-
ples in the physical world. arXiv preprint arXiv:1607.02533,
2016. 2

[12] L. Landrieu and M. Simonovsky. Large-scale point cloud se-
mantic segmentation with superpoint graphs. arXiv preprint
arXiv:1711.09869, 2017. 1

[13] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 8

[14] D. Meng and H. Chen. Magnet: a two-pronged defense
against adversarial examples.
In Proceedings of the 2017
ACM SIGSAC Conference on Computer and Communica-
tions Security, pages 135–147. ACM, 2017. 2

[15] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deep-
fool: a simple and accurate method to fool deep neural net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2574–2582, 2016. 1,
2

[16] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik,
and A. Swami. Practical black-box attacks against machine
learning.
In Proceedings of the 2017 ACM on Asia Con-
ference on Computer and Communications Security, pages
506–519. ACM, 2017. 2

[17] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik,
and A. Swami. The limitations of deep learning in adversar-
ial settings. In Security and Privacy (EuroS&P), 2016 IEEE
European Symposium on, pages 372–387. IEEE, 2016. 1, 2
[18] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas. Frustum
pointnets for 3d object detection from rgb-d data. Proc. Com-
puter Vision and Pattern Recognition (CVPR), IEEE, 2018.
1

[19] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
arXiv preprint arXiv:1612.00593, 2016. 1, 2, 4, 6

[20] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
arXiv preprint arXiv:1706.02413, 2017. 2, 8

[21] R. Ranjan, S. Sankaranarayanan, C. D. Castillo, and R. Chel-
Improving network robustness against adversar-
arXiv preprint

lappa.
ial attacks with compact convolution.
arXiv:1712.00699, 2017. 2

[22] P. Samangouei, M. Kabkab, and R. Chellappa. Defense-gan:
Protecting classiﬁers against adversarial attacks using gener-
ative models. arXiv preprint arXiv:1805.06605, 2018. 2

[23] N. Sedaghat, M. Zolfaghari, E. Amiri, and T. Brox.
Orientation-boosted voxel nets for 3d object recognition. In
British Machine Vision Conference (BMVC), 2017. 6

[24] M. Sung, H. Su, V. G. Kim, S. Chaudhuri, and L. Guibas.
Complementme: weakly-supervised component suggestions
for 3d modeling. ACM Transactions on Graphics (TOG),
36(6):226, 2017. 1

[25] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199, 2013. 1, 2

[26] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and
J. M. Solomon. Dynamic graph cnn for learning on point
clouds. arXiv preprint arXiv:1801.07829, 2018. 2, 8

[27] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1912–1920, 2015. 6

[28] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song.
Generating adversarial examples with adversarial networks.
arXiv preprint arXiv:1801.02610, 2018. 2

[29] C. Xiao, J.-Y. Zhu, B. Li, W. He, M. Liu, and D. Song.
Spatially transformed adversarial examples. arXiv preprint
arXiv:1801.02612, 2018. 2

[30] D. Xu, D. Anguelov, and A. Jain. Pointfusion: Deep sen-
sor fusion for 3d bounding box estimation. Proc. Computer
Vision and Pattern Recognition (CVPR), IEEE, 2018. 1

[31] W. Xu, D. Evans, and Y. Qi. Feature squeezing: Detecting
adversarial examples in deep neural networks. arXiv preprint
arXiv:1704.01155, 2017. 2

[32] Z. Yan, Y. Guo, and C. Zhang. Deepdefense: Training deep
neural networks with improved robustness. arXiv preprint
arXiv:1803.00404, 2018. 2

[33] Z. Zhao, D. Dua, and S. Singh. Generating natural adversar-

ial examples. arXiv preprint arXiv:1710.11342, 2017. 1

[34] Y. Zhou and O. Tuzel. Voxelnet: End-to-end learning for
point cloud based 3d object detection. Proc. Computer Vision
and Pattern Recognition (CVPR), IEEE, 2018. 1

9144

