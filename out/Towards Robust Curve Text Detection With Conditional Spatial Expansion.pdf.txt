Towards Robust Curve Text Detection with Conditional Spatial Expansion

Zichuan Liu1, Guosheng Lin1, Sheng Yang1, Fayao Liu2, Weisi Lin1 and Wang Ling Goh1

1Nanyang Technological University, Singapore

2University of Adelaide, Australia

{zliu016, syang014}@e.ntu.edu.sg, {gslin, wslin, ewlgoh}@ntu.edu.sg, fayaoliu@gmail.com

Abstract

It

is challenging to detect curve texts due to their
irregular shapes and varying sizes. In this paper, we ﬁrst
investigate the deﬁciency of the existing curve detection
methods and then propose a novel Conditional Spatial
Expansion (CSE) mechanism to improve the performance
of curve text detection.
Instead of regarding the curve
text detection as a polygon regression or a segmentation
problem, we treat it as a region expansion process. Our
CSE starts with a seed arbitrarily initialized within a
text region and progressively merges neighborhood regions
based on the extracted local
features by a CNN and
contextual information of merged regions. The CSE is
highly parameterized and can be seamlessly integrated into
existing object detection frameworks. Enhanced by the
data-dependent CSE mechanism, our curve text detection
system provides robust instance-level text region extraction
with minimal post-processing. The analysis experiment
shows that our CSE can handle texts with various shapes,
sizes, and orientations, and can effectively suppress the
false-positives coming from text-like textures or unexpected
texts included in the same RoI. Compared with the existing
curve text detection algorithms, our method is more robust
and enjoys a simpler processing ﬂow. It also creates a new
state-of-art performance on curve text benchmarks with F-
score of up to 78.4%.

1. Introduction

In recent years, great progress has been made in text
detection. The performance has been enhanced by the
advanced object detection and segmentation frameworks
based on Neural Networks. Although detecting words or
text lines with different sizes and orientations has been well
tackled by recently proposed methods [28, 24, 34, 32, 23],
detecting curve texts remains a challenging problem.

The main challenges of curve text detection come from
irregular shapes and highly varying orientations.
The
traditional bounding box representation does not scale well

Figure 1. Problems of existing curve text detection methods:
Two region proposals shown in (a) and (d) inevitably include
unexpected texts since they are closed to each other. Thus,
it causes failures for Mask RCNN based methods and polygon
regression methods demonstrated in (b) and (e). Our CSE
method demonstrated in (c) and (f) shows strong robustness to this
situation and brings signiﬁcant performance gain.

in the curve scenario since one box may cover multiple
text objects. Therefore, the recently proposed curve text
detection algorithms [39, 27, 40] follow a two-stage detect-
and-reﬁne approach to generate elaborated polygons or
boundaries. In these methods, a CNN based text detector
is applied to locate the regions containing texts, and
then a segmentation or polygon regression algorithm is
performed on these regions to produce a tight polygon or
boundary. Both methods highly depend on the accurate
region proposal provided by the text detector. They prefer
a proposed region with only one targeted object included
which reduces the ambiguity of the sampled features.
Although an oriented box regression is preferable, it often
fails in the curve text scenario [29, 14]. The recently
proposed curve text detection methods turn to predict
horizontal rectangles in the ﬁrst stage, which is inevitably
affected non-targeted texts in the sampled regions.

Speciﬁcally, both segmentation and regression based

7269

(a) Proposed Box(b) Mask RCNN(c) Our CSE (d) Proposed Box(e) Polygon Regression(f) Our CSE methods can be disrupted by the unexpected texts included
in the same box region. The segmentation based methods
can fail to distinguish the targeted text from the others
and misclassify the unexpected texts or text-like patterns as
positive, shown in Fig. 1 (b). The regression based methods
can produce incorrect boundaries by indistinguishably
considering all texts as one object. Moreover, as shown in
Fig. 1 (e), the regression results (produced by predicting
an offset of a proposed box region) are highly coupled with
previous stage box proposals. A poor box proposal greatly
affects the ﬁnal polygon which degrades the performance.

To tackle the problem mentioned above, we propose
a novel Conditional Spatial Expansion (CSE) mechanism,
which acts as a second-stage component applied in the
widely adopted two-stage detection workﬂow. Our method
is derived from conditional modeling of dependency
between an interior point (called a seed) and the rest parts
of a text instance. The CSE can be seen as a conditional
prediction process which retrieves an instance level text
region by seeding and expanding. Starting with an arbitrary
interior point (seed) of a text region, CSE selectively
expands its territory based on location observation of
the image patches and the context inferred from merged
regions. Compared with the segmentation based method,
our CSE is extremely discriminative especially when texts
1 (c).
are close to each other as demonstrated in Fig.
It provides a controllable approach to extract an expected
text region with minimum efforts of post-processing. On
the other hand, our CSE is highly ﬂexible since a seed
can be speciﬁed at any location within the targeted text
region. Compared with the polygon regression methods, the
seeding-and-expanding paradigm has less coupling with the
previous text detector. Base on a coarse region proposal, our
CSE is directly applied on the spatial features produced by
the backbone, which preserves all the spatial information
and will not be affected by the imperfect region proposals,
shown in Fig. 1 (f). The experiments show that our method
outperforms the existing curve text detection methods on
public benchmarks. The contributions of this work are
summarized as follows:

• The curve text detection is formulated as a conditional
region expansion problem, which initializes a seed
within a text region and then progressively retrieves
targeted object by region expansion;

• The spatial dependency between the seed and the
rest part of an object is modeled by a parameterized
Conditional Spatial Expansion mechanism, which
allows us to selectively extract a text region indicated
by a seed with high area precision;

• Our CSE acts as a second-stage text extractor which
can be seamlessly integrated into existing object
detection workﬂows;

• The arbitrariness of seed’s location and high spatial
selectivity of our method reduce coupling with the
previous detector and thus provide ﬂexible and robust
boundary prediction;

• Our method outperforms the existing curve text
detection methods on public curve text datasets with
F-measurement of 80.2% on Total-Text [5] and 78.4%
on CTW-1500 [39].

2. Related Works

2.1. Quadrilateral Text Detection

In the quadrilateral text detection,

the ground-truths
are constrained to a rectangle or a quadrilateral. Base
on the types of targets to be retrieved,
text detection
methods can be categorized as detection based methods and
segmentation based methods.

The detection beased method follows

the object
detection frameworks [9, 31, 20, 30, 21, 10] driven by
Convolutional Neural Networks (CNNs) [17]. TextBoxes
[18] adopts SSD as a base detector and handles variation
of aspect ratios of text instance by elaborated reference
box design. As the variants of the faster RCNN [30],
the Rotation Region Proposal Network (RRPN) [29] and
Rotational Region CNN (R2CNN) [14] are designed to
detect arbitrarily oriented texts in a two-stage manner. In
addition, EAST [43] and DeepReg [11] are proposed to
directly regress the geometry of a text instance.

The segmentation based methods are mostly designed
to extract long text lines in an image. They interpret text
detection as a semantic segmentation problem which has
been well addressed by the Fully Convolutional Neural
Networks (FCNs) [25, 7, 8, 41]. Zhang et al. [42] combines
FCN and MSER [13] to recognize text blocks and then
extract corresponding characters. Yao et al. [38] applies
FCN to predict multiple properties of texts, such as text
regions and orientations, to extract the target text regions.
To distinguish adjacent
the component
segmentation method [24, 23, 35, 6, 36] is proposed, where
a text region is broken into several components, which
will be combined into different instances by data-driven
clustering, inter-node communication or post-processing.

instances,

text

2.2. Curve Text Detection

Although the methods reviewed above have succeeded
in the quadrilateral text detection, most of them cannot
scale well
in the case of arbitrary text shape. New
representations and detection framework are proposed for
this task.
[39] propose Transverse and
Longitudinal Offset Connection (TLOC) method based on
Faster RCNN and Recurrent Neural Networks (RNNs)
to directly regress the polygon shape of text regions.

Liu et al.

7270

Mask Text-Spotter [27] regards the curve text detection as
an instance segmentation problem and applies the Mask-
RCNN to produce boundaries of text instances. TextSnake
[26] adopts FCN as the base detector and extract text
instance by detecting and assembling local components.

Most of the existing curve text detection methods are
potentially modeling the posterior probability between
observed image patches and the ground-true foreground
labels.
Instead, our method captures the dependency
between arbitrary image patches and the rest of the text
regions. The proposed modeling is naturally robust to
ambiguity caused by multiple text instances included in one
RoI. Moreover, our CSE considers more local details and
thus can produce more elaborated text boundaries.

3. Method

3.1. Overview

Our method retrieves an instance level text region by
seeding and then expanding. A seed uniquely indicates
an object and can be arbitrarily initialized at the interior
of an object region. Seeding is to select a location within
an object from which the corresponding object region is
extracted by expanding. As will be illustrated in Sect.
3.4, a seed is initialized by an object detector with a
predicted box center. Starting with a seed, the expanding
is conducted by selectively merging adjacent sub-regions
to form a targeted object region. As shown in Fig. 2, the
sub-regions are abstracted as feature points or nodes, which
are sampled from the input image at discrete locations.
They are organized as a grid and locally assigned an
expanding indicator y ∈ R5 to represent the merging
direction to neighborhood nodes. Five entries of y denote
the probabilities of all possible merging directions (to −
bottom, to − right, to − lef t, to − top and none). A
node will be merged into an existing object region indicated
by a seed if its major merging direction is pointing to its
neighborhoods that already belong to the object region. An
instance level object boundary can be easily produced by
mapping all the positive nodes back to the original image
and extracting the contour.

3.2. Modeling

The seeding-and-expanding paradigm provides a ﬂexible
and controllable way to extract object
regions with
minimal post-processing. It also reduces the performance
coupling with the ﬁrst-stage detector by allowing arbitrary
initialization of a seed node. However, with different
seed locations, the dynamics of the region expansion are
different.
the state of expanding
indicator varies with the seed’s location and also depends on
the expanding indicators of its neighborhoods. Therefore,
obtaining the expanding indicators should be regarded as a

For a speciﬁc node,

Figure 2. Our CSE inside-out explores the expanding indicator of
every node and merges nodes with merging direction pointing to
nodes that are already in the object region.

conditional prediction problem. Centered at a seed shown
in Fig. 2, we divide the region into sections (deﬁned as
nodes with the same minimum number of steps to a seed)
using a set of contours. We assume that the expanding
indicators of nodes Pk within the k-th section are mutually
independent and their states depend only on the current
spatial feature Xk and the states of nodes within previous
sections Pk−1, Pk−2, · · · , P0. The optimal estimator of Yk
can be represented by

ˆYk = argmax

Yk

Pr(Yk|Yk−1, · · · , Y0; Xk, · · · , X0),

(1)

which maximizes the posterior probability of Yk when
observing spatial features X(·) := {x(p)|p ∈ P(·)} and
:= {y(p)|p ∈ P(·)}.
indicators of previous nodes Y(·)
This conditional modeling allows the region expansion to be
adaptable with an arbitrarily initialized seed location. Also,
it effectively differentiates expected object from the others
by considering the context derived from a seed. Moreover,
independence assumption among node in the same section
results in a dendritic Conditional Spatial Expansion process
with high-level parallelism.

3.3. Conditional Spatial Expansion

To estimate conditional probability illustrated in Eq.
1, we develop a highly parameterized Condition Spatial
Expansion (CSE) mechanism. Given a seed inside an
object region, we construct its neighborhood feature points
by expanding a S × S grid and then sampling the
features produced by the backbone network using bilinear
interpolation, as shown in Fig. 3 (a). Starting with the
seed node, our CSE explores every single node inside-out
and computes corresponding y and the transition vectors
ho = [hb
o]T to its neighborhoods based on the
current sampled feature x ∈ Rdx , local state c ∈ Rd

o, hr

o, ht

o, hl

7271

P0P1P2P3Seed-associated Feature PointMerging DirectionSeedExpanding DirectionA node with Expand IndicatorMerged RegionA SectionFigure 3. Given a seed located at the interior of a text region, we expand a grid with S × S points and sample the feature produced by
backbone at these locations using bilinear interpolation. The CSE computation starts with the seed and spreads to the adjacent feature
nodes. Each node takes the outputs and the hidden states of previous nodes as input and produces new hidden state and output.

i, ht

i , hl

i , hr

and transition vectors hb
i ∈ Rd coming from
the adjacent feature points. The transition vectors encode
the position sensitive information which helps the CSE
to be aware of the relative location of the current node
to the seed. Depending on the relative position to the
seed, the inputs and outputs for the nodes are illustrated
in Fig. 3 (c). For a node in Pk, our CSE only takes the
c and h as inputs from Pk−1 and output new h to Pk+1.
This constructs an inference process originated from the
seed which propagates the contextual information among
the grid in a dendritic manner. The computation of the
nodes in the same section is independent and thus can be
fully parallelized on GPUs. The computation complexity
is linear to the side of the grid, which is computationally
efﬁcient.

Inside a speciﬁc node, the computation is illustrated by a
computing graph shown in Fig 4. Without loss of generality,
we denote all possible inputs from neighborhood nodes by
ci ∈ R4d, hi ∈ R5d, and yi ∈ R20, which are represented
by

are available according to the relative position to the seed
node 2, and the rest will be set to zeros. Particularly, hc
i is
deﬁned to uniquely indicate the seed node. Other than hc
i of
a seed which is learned by backpropagation, we explicitly
set the hc

i of other node to zeros.

From the current observed feature x, transition vectors
hi and the predicted expanding indicators of neighborhoods

yi, we compute a candidate local state ec by
ec = tanh(Wc × s + bc),
s = [x, yi, hi]T ,

(5)

(6)

where × represents the matrix multiplication, Wc ∈
Rd×(dx+5d+20) and bc ∈ Rd denote weights and bias of
linear transform before a tanh activation. We apply the
gating mechanism [12] to combine the local state cb, cr, cl
and ct from the neighborhoods with the current candidate

state ec to obtain the local state of current node c, which is

formulated as

ci = [cb
hi = [hc
yi = [yb

i , cr
i , hb
i , yr

i , cl
i, ct
i]T ,
i , hl
i , hr
i , yl
i, yt

i, ht
i ]T ,

i]T ,

(2)

(3)

(4)

i ∈ Rd, h(·)

where c(·)
i ∈ R5 denotes the
local states, transition vectors and the expanding indicators
of neighborhood nodes 1. Here, only parts of ci, hi and yi

i ∈ Rd and y(·)

c = δ(cb · gb

c + cr · gr

c + cl · gl

c + ct · gt

c + ec · gec),

(7)

where δ denotes the layer normalization operator [4], ·
is the element-wise multiplication, and gb, gr, gl, gt, gc
represent the outputs of gating function which can be further
illustrated by

gc = [gb

ci , gr

ci , gl

ci , gt

ci ]T = σ(Wgci × s + bgci ),

gec = σ(Wgec × s + bgec ).

(8)

(9)

1The superscripts indicate the relative position to the current node, b-

bottom, r-right, l-left, t-top and c-current.

2The non-zero inputs are indicated in Fig. 3 (c)

7272

Seeding and SamplingCondition Spatial ExpansionExtracted FeaturesSeeding LocationText RegionsPoints Associated with SeedNodes not Associated with SeedComputing Graph of CSEExpected Text RegionBilinearInterpolation13245678(a)(b)21346578(c)Input DataOutput DataFigure 4. Computing Graph inside a Node.

, bgci

Here, Wgci
and Wgec , bgec are deﬁned as the weight
matrix and bias to map s into corresponding gating vectors
gci and gec. Since the local state c is essentially the weighted
sum of state values of the previous nodes, the values of
c increases exponentially with k in our two-dimensional
scenario. This signiﬁcantly harms the numerical stability
in both training and testing phases.
the layer
normalization technique is essential for the CSE to ensure
the convergence of training and prevent overﬂow in testing.
Finally, the expanding indicator y and the output transition
vectors ho are derived from the local state c, which are
illustrated as follows

Thus,

go = σ(Wgo × s + bgo ),
ho = [hb
y = sof tmax(Wy × c + by),

o, hr

o, ht

o, hl

o]T = tanh(c) · go + bo,

(10)

(11)

(12)

where Wgo and bg0 represent the weight matrix and bias
used to produce the corresponding gating signal, and Wy
and by transform c into logits before feeding to softmax
activation.

3.4. Seeding

In our CSE method, a seed is assumed to be located
within an object region. This prerequisite can be easily
guaranteed by using the outputs of an off-the-shelf object
detector. Here, we adopt detected box centers and shapes
by Faster RCNN [31] to decide seed locations and the
shapes, and uniformly sample S × S features using bi-
linear interpolation from a region indicated by a bounding
box. In fact, our CSE method only requires a seed to be
located within an object region with sampling grid coarsely
covering the targeted object. Moreover, given a sampling
grid, any node within the object region can be speciﬁed as
a seed. As shown in Sect. 4.2, randomly initializing seed
location and corresponding grid size does not signiﬁcantly
affect the performance. Therefore, a weaker detector, which

Figure 5. Ground-truth Labeling: All arrows represent
the
candidate merging directions for nodes, and the corresponding
scores predicted by CSE are marked. The candidate directions
with highest scores are label as positive directions, which are
represented by solid arrows.

is easy to optimize, could be sufﬁcient for CSE to produce
satisfactory results.

3.5. Optimization

Labeling In the training phase, the ground-true merging
directions are labeled using the strategy illustrated in Fig.
5. For each grid in CSE, we ﬁrst label the nodes within
the target ground-true object region as positive and the
rest as negative. For every positive node, we search its
neighborhood positive nodes at the previous section and
label the corresponding merging directions as candidate
directions3. Among the candidate directions of the same
node, we only label the one with the highest score as the
ﬁnal positive merging direction. For the seed node, we
always label its ys as positive.

Loss Function We apply cross-entropy loss to each node

to optimize our CSE model, which can be represented by

Lcse =

1

N X

p∈P

− ln(y∗(p)),

(13)

where N = S × S represents the number of nodes in a
grid, P denotes a set of all nodes, and y∗ is the value of
the positive merging direction. Our optimization strategy
computes the loss according to the current CSE prediction.
Intuitively, it intends to boost the positive candidates which
are already strong, which reduces the ambiguity in labeling
and speed-up the convergence.

3At most two merging directions will be labeled as positive.

7273

concatSSLayer NormSigmoidTanhSoftmaxPoint ProductMat MulAdd0.70.20.50.10.60.10.70.80.30.5SeedPositive NodePositive DirectionCandidate DirectionNegative NodeP0P1P2P3Ground-true Region0.50.10.34. Experiment

4.1. Experiment Details

The experiment is conducted on Tensorﬂow 1.5.0 [3].
We adopt Faster RCNN driven by ResNet-34 to initialize
seed locations and corresponding grids in all experiments.
The CSE is implemented and optimized in C++ and
accelerated by CUDA. Following the existing training
strategies for scene text detection [43, 27, 21], we pretrain
our model on a combined dataset. The pretraining dataset
consists of over 10k images from full set of ICDAR-17 MLT
[1] and the training sets of MSRA-TD500 [37], Total-Text
[5] and CTW-1500 [39]. After the pretraining, we ﬁne-tune
and evaluate our method on two curve text datasets Total-
Text (with 1255 training images and 300 testing images)
and CTW-1500 (with 1000 training images and 500 testing
images). The model is trained on the combined dataset for
50k iteration and ﬁne-tuned on the datasets to be evaluated.
We adopt the Adam optimizer [16] to train the network. In
the pretraining phase, the learning rate is ﬁxed to 0.01 for
the ﬁrst 30k iterations and scaled down to 0.002 for the rest
iterations. In the ﬁne-tuning, the initial learning rate is set
to 0.001 and decays exponentially 0.9 every 5000 iterations.
All the experiment is conducted on Dell Alienware with
Intel i7 processor, 64GB memory and two NVIDIA GTX
1080 Ti GPUs. The batch size is set to 1 for each of two
GPUs in training and only one GPU is used for evaluation.

4.2. Flexibility and Robustness

In this

experiment, we validate

the ﬂexibility
and robustness of our CSE method qualitatively and
quantitatively.
In the qualitative experiment, we generate
a set of sampling grids (in yellow) with different locations
and sizes by randomly manipulating the ground-true boxes
as shown in the ﬁrst row of Fig. 6. We apply the CSE to
the corresponding RoIs shown in the second row of Fig.
6, and visualize the extracted text regions by heat maps
in the third row of Fig. 6. Fig. 6 (a) shows the ﬂexibility
of our method. Our CSE method can effectively retrieve
the text region with different seed’s locations. Even for
a text object with large curvature and slim shape, our
method can capture all the related sub-regions with high
area precision. On the other hand, for a proposed region
with many unexpected texts included or even dominated
by another text instances (demonstrated in Fig. 6 (b) and
(c)), our method only extracts associated object regions
indicated by the seed. It indicates that our CSE is robust to
the ambiguity caused unexpected objects and can produce
satisfactory results even for a poor sampling grid generated
by the previous object detector.

In addition to visually investigating our CSE, we
quantitatively verify its ﬂexibility and robustness by
rescaling the size of a proposed sampling grid and

relocating the seed in a gird. The grid rescaling resizes
the height and width of a proposed region proposal by a
factor of δs ≥ 1.0. The seed relocation is applied to a
sampling grid to change its seed to a new node which is
still within the targeted object region but have δc deviation
δc is
in Euclidean space from the original seed node.
normalized by the square root of the original grid area.
We study the effects of rescaling and relocation separately
by proﬁling the precisions, recalls, and F-scores on both
Total-Text and CTW-1500, and the results are shown in
Fig. 7. The performance variation respective to rescaling
factor δs on two datasets is proﬁled in Fig. 7 (a) and (b).
The performance is maintained at around 80% on Total-
Text and 78% on CTW-1500 for δs ranging from 1.0 to
It slightly drops when δs is larger than 1.5 and the
1.5.
F-scores remain above 77% and 73% respectively. As
for seed relocation, the according performance variations
on Total-Text and CTW-1500 are illustrated in Fig. 7 (c)
and (d). The performance is not greatly affected by the
seed relocation. The F-scores remains 80% and 78% and
decrease by only 3% and 4% when δc is changed from 0.0 to
1.0. In conclusion, our CSE is robust to randomly initialized
seed locations and distorted sampling grids.

4.3. Comparing with Mask RCNN

The baseline Mask RCNN method is implemented
based on the method proposed in [27]. We remove the
text recognition branch and only keep the detection and
segmentation branches. For a fair comparison, two methods
share the same text detector which is based on Faster RCNN
architecture. The quantity results are reported in Tab 4.3.
Our method is overall better than the baseline method. The
F-scores of the baseline method on both datasets are 67.5%
to 67.8%, respectively. In comparison, our CSE performs
much better than the Mask RCNN based method by over
10% with F-score of 80.2% on Total-Text and 77.4% on
CTW-1500.

To explore the cause of performance gain, we visually
compare the output conﬁdence map produced by two
methods on CTW-1500, which is shown in Fig. 8. Fig. 8 (a)
demonstrates the failed examples produced by the baseline
method. In these cases, the segmentation is distorted by the
adjacent text instance. Parts of the unexpected text instances
included in a box cause high activation in a conﬁdence
map and corrupt the boundary prediction.
In contrast,
our CSE is extremely robust in this scenario, since the
contextual information captured by CSE helps to eliminate
the ambiguity caused by unexpected objects. Moreover,
the condition modeling allows our CSE to retrieve long
curve text lines with high precision, which is ﬂexible and
promising in real applications.

7274

Figure 6. Robustness and Flexibility Analysis: In the ﬁrst row of each case, the sampling regions are represented by the bounding boxes in
yellow and the seed locations are labeled by the cross dash lines. The second row shows the zoom-in of RoIs. The corresponding heatmaps
of associate regions are shown in the third row. As shown in (a), our method is very ﬂexible in the seed’s locations. (b) and (c) prove the
robustness of our method to extract text from a density text region.

Figure 7. Performance v.s. rescaling factor δs on Total-Text (a) and
CTW-1500 (b); Performance v.s. relocation factor δc on Total-Text
(c) and CTW-1500 (d).

Datasets

Model

P

MRCNN 69.2
81.4
CSE

Total-Text

CTW1500

R

65.8
79.1

F

67.5
80.2

P

65.1
78.7

R

70.8
76.1

F

67.8
77.4

Table 1. Performance Comparison between Mask RCNN based
method and our CSE method.

4.4. Comparing with Polygon Regression

In this experiment, we compare our method with another
baseline based on polygon regression proposed in [39]. The
baseline is implemented based on the publicly available
source code provided in [39], and is pretrained and ﬁne-
tuned with our training strategy. Similar to the previous
experiment, the backbone and the RPN are shared among
two methods, and the rest parts are implemented based on
respective workﬂows. The results are shown in Tab 4.4. Our

Figure 8. (a) Examples output by baseline method; (b) Examples
output by our CSE method. Compared with the baseline Mask
RCNN method, our method shows impressing robustness to the
ambiguity caused by adjacent texts and impressing selectivity to
the targeted instance.

method outperforms the baseline method by 5% in terms of
F-score on Total-Text, with the precision of 80.9%, recall
of 80.3% and F-score of 80.6%. On CTW-1500, our CSE
achieves F-score of 77.6 %, which is 4.4% better than the
baseline.

We also investigate the causes of performance gain by
visualizing the detection results of both methods. As
demonstrated in Fig. 9, with the same RoI proposals shown
in Fig. 9 (a), the polygon regression could be corrupted
by the other text object which is occasionally included.
As can be seen in Fig. 9 (b), the baseline model may
consider all the texts included in a proposed region as
a single object and regress the corresponding boundary.
Although this problem can be mitigated by training a more
accurate text detector to reduce the unexpected texts, the
proposed bounding boxes inevitably cover additional texts
due to text’s highly varying shapes and orientations. As

7275

(a)(b)(c)1.001.251.501.752.00s5060708090(a)recallprecisionF-score1.001.251.501.752.00s5060708090100(b)recallprecisionF-score0.000.250.500.751.00c5060708090100(c)recallprecisionF-score0.000.250.500.751.00c5060708090(d)recallprecisionF-score(b)(a)(a)Datasets

Methods

Poly-Reg
CSE

Total-Text

CTW1500

P

73.8
80.9

R

77.4
80.3

F

75.6
80.6

P

77.1
79.2

R

69.7
76.0

F

73.2
77.6

Table 2. Performance Comparison between Polygon Regression
based method and our CSE method.

Methods

SegLink [32]

EAST [43]

Mask TextSpotter [27]

TextSnake et al. [26]

CSE

P

30.3

50.0

69.0

82.7

81.4

R

23.8

36.2

55.0

74.5

79.1

F

time (s)

26.7

42.0

61.3

78.4

80.2

-

-

-

-

0.42

Table 3. Detection Performance on Total-Text.

Methods

SegLink [32]

EAST [43]

DMPNet [22]

CTD [39]

CTD+TLOC [39]

TextSnake et al. [26]

CSE

P

42.3

78.7

69.9

74.3

77.4

67.9

81.1

R

40.0

49.1

56.0

65.2

69.8

85.3

76.0

F

time (s)

40.8

60.4

62.2

69.5

73.4

75.6

78.4

-

-

-

-

-

-

0.38

Table 4. Detection Performance on CTW-1500.

Figure 9. (a) The images with RoIs; (b) The text boundaries output
by baseline method; (c) The text boundaries produced by our CSE
method. The baseline method is easily affected by the unexpected
texts included in the same boxes, while our method shows strong
robustness to this situation.

shown in Fig. 9 (c), the conditional expansion mechanism
only merges the sub-regions that are similar to the region
indicated by the seed. By exploring the spatial dependency
as well as the local information, our CSE method is much
more robust than the polygon regression method and can
produce more elaborated boundaries.

4.5. Peer Comparison

We compare our method with the recently proposed
methods for curve text detection on curve text benchmarks,
Total-Text and CTW-1500.
The results are shown in
Tab.4.5 and Tab.4.5. Our method creates a new state-
of-art performance with the precision of 81.4%, recall of
79.1% and F-score of 80.2% on Total-Text. On CTW-1500
containing both curve texts and long text lines, our method
also achieves the state-of-art performance with the precision
of 81.1%, recall of 76.0% and F-score of 78.4%. The
inference time is 0.42 ms per image and 0.38 ms per image
on Total-Text and CTW-1500 respectively. The detection
results are demonstrated in Fig. 10.
It shows that our
method can effectively handle curve texts with irregular
shapes, highly varying sizes and arbitrary orientations.

Figure 10. Detection results on Total-Text and CTW-1500.

5. Conclusion

In this work, we analyze the deﬁciency of the existing
curve text detection methods and improve the performance
by developing a novel parameterized Conditional Spatial
Expansion (CSE) mechanism. Our method shows strong
robustness to the ambiguity caused by close texts with
arbitrary shapes and orientations.
is ﬂexible and
can extract text regions in a controllable manner. Our
CSE method outperforms the existing curve text detection
methods.

It

Acknowledgement

G. Lin’s participation was partly supported by the
National Research Foundation Singapore under its AI
Singapore Programme [AISG-RP-2018-003] and a MOE
Tier-1 research grant [RG126/17 (S)].

7276

(a)(b)(c)References

[1] Icdar 2017 robust reading competition. http://u-pat.

org/ICDAR2017/index.php. 6

[2] Resnet-34.

https://www.kaggle.com/pytorch/

resnet34.

[3] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow:
Large-scale machine learning on heterogeneous distributed
systems. arXiv preprint arXiv:1603.04467, 2016. 6

[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
arXiv preprint arXiv:1607.06450,

Layer normalization.
2016. 4

[5] Chee Kheng Ch’ng and Chee Seng Chan.

Total-text:
A comprehensive dataset for scene text detection and
recognition.
In Document Analysis and Recognition
(ICDAR), 2017 14th IAPR International Conference on,
volume 1, pages 935–942. IEEE, 2017. 2, 6

[6] Dan Deng, Haifeng Liu, Xuelong Li, and Deng Cai.
Pixellink: Detecting scene text via instance segmentation.
In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018. 2

[7] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and
Gang Wang. Context contrasted feature and gated multi-
scale aggregation for scene segmentation.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018. 2

[8] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu,
and Gang Wang. Semantic correlation promoted shape-
variant context for segmentation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
2019. 2

[9] Ross Girshick. Fast r-cnn.

In Proceedings of the IEEE
international conference on computer vision, pages 1440–
1448, 2015. 2

[10] Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu
Qiao, and Changming Sun. An end-to-end textspotter with
explicit alignment and attention. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 5020–5029, 2018. 2

[11] Wenhao He, Xu-Yao Zhang, Fei Yin, and Cheng-Lin
Liu. Deep direct regression for multi-oriented scene text
detection. arXiv preprint arXiv:1703.08289, 2017. 2

[12] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term

memory. Neural computation, 9(8):1735–1780, 1997. 4

[13] Weilin Huang, Yu Qiao, and Xiaoou Tang. Robust scene
text detection with convolution neural network induced mser
trees. In European Conference on Computer Vision, pages
497–511. Springer, 2014. 2

[14] Yingying Jiang, Xiangyu Zhu, Xiaobing Wang, Shuli Yang,
Wei Li, Hua Wang, Pei Fu, and Zhenbo Luo. R2cnn:
rotational region cnn for orientation robust scene text
detection. arXiv preprint arXiv:1706.09579, 2017. 1, 2

[15] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos
Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu
Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan

Icdar 2015 competition
Chandrasekhar, Shijian Lu, et al.
on robust reading.
In Document Analysis and Recognition
(ICDAR), 2015 13th International Conference on, pages
1156–1160. IEEE, 2015.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 6

[17] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324,
1998. 2

[18] Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang,
and Wenyu Liu. Textboxes: A fast text detector with a single
deep neural network. In AAAI, pages 4161–4167, 2017. 2

[19] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In CVPR, volume 1, page 4,
2017.

[20] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector.
In European
conference on computer vision, pages 21–37. Springer, 2016.
2

[21] Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and
Junjie Yan. Fots: Fast oriented text spotting with a uniﬁed
network. arXiv preprint arXiv:1801.01671, 2018. 2, 6

[22] Yuliang Liu and Lianwen Jin. Deep matching prior network:

Toward tighter multi-oriented text detection. 8

[23] Z. Liu, G. Lin, W. L. Goh, F. Liu, C. Shen, and X. Yang.
Correlation Propagation Networks for Scene Text Detection.
ArXiv e-prints, Sept. 2018. 1, 2

[24] Zichuan Liu, Guosheng Lin, Sheng Yang, Jiashi Feng, Weisi
Lin, and Wang Ling Goh. Learning markov clustering
networks for scene text detection. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.
1, 2

[25] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation.
In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 3431–3440, 2015. 2

[26] S. Long, J. Ruan, W. Zhang, X. He, W. Wu, and C. Yao.
TextSnake: A Flexible Representation for Detecting Text of
Arbitrary Shapes. ArXiv e-prints, July 2018. 3, 8

[27] P. Lyu, M. Liao, C. Yao, W. Wu, and X. Bai. Mask
TextSpotter: An End-to-End Trainable Neural Network for
Spotting Text with Arbitrary Shapes. ArXiv e-prints, July
2018. 1, 3, 6, 8

[28] Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan,
and Xiang Bai. Multi-oriented scene text detection via
corner localization and region segmentation. arXiv preprint
arXiv:1802.08948, 2018. 1

[29] Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong
Wang, Yingbin Zheng, and Xiangyang Xue. Arbitrary-
oriented scene text detection via rotation proposals.
IEEE
Transactions on Multimedia, 2018. 1, 2

[30] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object

7277

In Proceedings of

detection.
the IEEE Conference on
Computer Vision and Pattern Recognition, pages 779–788,
2016. 2

[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks.
In Advances in neural information
processing systems, pages 91–99, 2015. 2, 5

[32] Baoguang Shi, Xiang Bai, and Serge Belongie. Detecting
oriented text in natural images by linking segments. arXiv
preprint arXiv:1703.06520, 2017. 1, 8

[33] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.
Training region-based object detectors with online hard
example mining. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 761–769,
2016.

[34] Zhi Tian, Weilin Huang, Tong He, Pan He, and Yu Qiao.
text
In European Conference on Computer

Detecting text
proposal network.
Vision, pages 56–72. Springer, 2016. 1

image with connectionist

in natural

[35] Yue Wu and Prem Natarajan. Self-organized text detection
with minimal post-processing via border learning. In Proc.
ICCV, 2017. 2

[36] Chuhui Xue, Shijian Lu, and Fangneng Zhan. Accurate
scene text detection through border semantics awareness and
bootstrapping. In European Conference on Computer Vision
(ECCV), 2018. 2

[37] Cong Yao, Xiang Bai, Wenyu Liu, Yi Ma, and Zhuowen Tu.
Detecting texts of arbitrary orientations in natural images.
In Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on, pages 1083–1090. IEEE, 2012. 6

[38] Cong Yao, Xiang Bai, Nong Sang, Xinyu Zhou, Shuchang
Zhou, and Zhimin Cao. Scene text detection via holistic,
multi-channel prediction. arXiv preprint arXiv:1606.09002,
2016. 2

[39] Liu Yuliang, Jin Lianwen, Zhang Shuaitao, and Zhang
Sheng. Detecting curve text in the wild: New dataset and
new solution. arXiv preprint arXiv:1712.02170, 2017. 1, 2,
6, 7, 8

[40] Fangneng Zhan and Shijian Lu. Esir: End-to-end scene text
recognition via iterative image rectiﬁcation.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2019. 1

[41] Fangneng Zhan, Shijian Lu, and Chuhui Xue. Verisimilar
image synthesis for accurate detection and recognition of
texts in scenes. In European Conference on Computer Vision
(ECCV), 2018. 2

[42] Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao,
Wenyu Liu, and Xiang Bai. Multi-oriented text detection
with fully convolutional networks.
In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4159–4167, 2016. 2

[43] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang
Zhou, Weiran He,
an
efﬁcient and accurate scene text detector. arXiv preprint
arXiv:1704.03155, 2017. 2, 6, 8

and Jiajun Liang.

East:

7278

