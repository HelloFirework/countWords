ClusterNet: Deep Hierarchical Cluster Network with Rigorously

Rotation-Invariant Representation for Point Cloud Analysis

Chao Chen1

Guanbin Li1 ∗

Ruijia Xu1

Tianshui Chen1,2

Meng Wang3

Liang Lin1,2

1Sun Yat-sen University

2DarkMatter AI Research

3Hefei University of Technology

chench227@mail2.sysu.edu.cn, liguanbin@mail.sysu.edu.cn, xurj3@mail2.sysu.edu.cn

tianshuichen@gmail.com, wangmeng@hfut.edu.cn, linliang@ieee.org

Abstract

Current neural networks for 3D object recognition are
vulnerable to 3D rotation. Existing works mostly rely on
massive amounts of rotation-augmented data to alleviate
the problem, which lacks solid guarantee of the 3D rotation
invariance. In this paper, we address the issue by introduc-
ing a novel point cloud representation that can be mathe-
matically proved rigorously rotation-invariant, i.e., identi-
cal point clouds in different orientations are uniﬁed as a
unique and consistent representation. Moreover, the pro-
posed representation is conditional information-lossless,
because it retains all necessary information of point cloud
except for orientation information.
In addition, the pro-
posed representation is complementary with existing net-
work architectures for point cloud and fundamentally im-
proves their robustness against rotation transformation. Fi-
nally, we propose a deep hierarchical cluster network called
ClusterNet to better adapt to the proposed representation.
We employ hierarchical clustering to explore and exploit
the geometric structure of point cloud, which is embed-
ded in a hierarchical structure tree. Extensive experimen-
tal results have shown that our proposed method greatly
outperforms the state-of-the-arts in rotation robustness on
rotation-augmented 3D object classiﬁcation benchmarks.

1. Introduction

Rotation transformation is natural and common in 3D
world, however, it gives rise to an intractable challenge for
3D recognization. Theoretically, since SO(3)1 is an inﬁnite

∗Corresponding author is Guanbin Li. This work was supported in part
by the State Key Development Program under Grant 2016YFB1001004,
in part by the National Natural Science Foundation of China under Grant
No.61602533 and No.61702565, in part by the Fundamental Research
Funds for the Central Universities under Grant No.18lgpy63. This work
was also sponsored by SenseTime Research Fund.

13D rotation group, denoted as SO(3), contains all rotation transforma-

tions in R3 under the operation of composition.

group, a 3D object possesses rotated clones in inﬁnite atti-
tudes, thus a machine learning model is obliged to extract
features from an extremely huge input space. For exam-
ple, in 3D object classiﬁcation task, the category label of
an object is invariant against arbitrary rotation transforma-
tion in majority situations. However, from the perspective
of a classiﬁcation model, an object and its rotated clone are
distinct in input metric space, hence the model, such as neu-
ral network based methods, should have enough capacity to
learn rotation invariance from data and then approximate a
complex function that maps identical objects in inﬁnite atti-
tudes to similar features in feature metric space.

To alleviate the curse of rotation, a straightforward
method is to design a model with high capacity, such as a
deep neural network with considerable layers, and feed the
model with great amounts of rotation-augmented data [1]
based on a well-designed augmentation pipeline. Although
data augmentation is effective to some extent, it is computa-
tionally expensive in training phase and lacks solid guaran-
tee of rotation robustness. [11, 18] apply spatial transformer
network [5] to canonicalize the input data before feature ex-
traction, which improves the rotation-robustness of model
but still inherits all the defects of the data augmentation.
[16] proposes a rotation-equivariant network for 3D point
clouds using a special convolutional operation with local
rotation invariance as a basic block. The method attempts
to equip the neural network with rotation-symmetry. How-
ever, it is hard to guarantee the capacity of such network to
satisfy all rotation-equivariant constraints in each layer.

We address the issue by introducing a novel Rigorous
Rotation-Invariant (RRI) representation of point cloud.
Identical objects in different orientations are uniﬁed as a
consistent representation, which implies that the input space
is heavily reduced and the 3D recognization tasks become
much easier. It can be mathematically proved that the pro-
posed representation is rigorously rotation-invariant, and
information-lossless under a mild condition. Given any
data point in point cloud and a non-collinear neighbor ar-

4994

bitrarily, the whole point cloud can be restored intactly with
the RRI representation, even if the point cloud is under an
unknown orientation. In other words, the RRI representa-
tion maintains all necessary information of point cloud ex-
cept for the volatile orientation information which is associ-
ated with speciﬁc rotation transformation. Furthermore, the
RRI representation is ﬂexible to be plugged into the cur-
rent neural architectures and endows them with rigorous ro-
tation invariance. The major difference between rotation-
equivariant network and our proposed method is that the
former embeds the invariance property as a priori into neu-
ral network, but the latter separates the rotation invariance
from neural network and directly cut down the orientation-
redundancy of input space.

Moreover, we propose a deep hierarchical network
called ClusterNet to better adapt to our new representation.
Speciﬁcally, we employ unsupervised hierarchical cluster-
ing to learn the underlying geometric structure of point
cloud. As a result, we can obtain a hierarchical structure
tree and then employ it to guide hierarchical features learn-
ing. Similar to CNNs, ClusterNet extracts features corre-
sponding with small clusters, which learns ﬁne-grained pat-
terns of point cloud; the smaller cluster features are then
aggregated as larger cluster features capturing higher-level
information. The process of embedding is repeated along
the hierarchical structure tree from bottom to top until we
achieve the global features of the whole point cloud.

We summarize our major contributions as follows:

1. We propose a new point cloud representation that sat-
isﬁes, both theoretically and empirically, rotation in-
variance and information preservation;

2. The proposed representation is complementary with
the existing neural network frameworks and funda-
mentally improves their robustness against rotation
transformation;

3. We further introduce a novel deep hierarchical network
called ClusterNet to better adapt to our new repre-
sentation. Combing the novel point cloud representa-
tion and the elaborate ClusterNet, our method achieves
state-of-the-art robustness in standard 3D classiﬁcation
benchmarks.

2. Related Work

Deep Learning for 3D Objects.
In general, the develop-
ment of deep learning for 3D object is closely related to the
progress of representation form of 3D object from geomet-
ric regular data to irregular one. For the conventional CNNs,
it is intractable to handle the geometric irregular data, such
as meshes and point clouds. Thus, previous literatures strive
to transform such data into voxel representations [9, 12, 20]
or multi-images (views) [15, 20]. However, it is inevitable

0 .4

0 .2

Z

0 .0

− 0 .2

− 0 .4

-1 .0 -0 .8 -0 .6 -0 .4 -0 .2 0 .0 0 .2 0 .4 0 .6

X

0 .2

0 .1

Y

0 .0

-0 .1

Figure 1: The left ﬁgure is a dendrogram of a point cloud
learned by hierarchical clustering. The right ﬁgure shows
partition of the point cloud of plane in a merge-level re-
maining 8 clusters.

to suffer from loss of resolution and high computational
expense during transformation and subsequent processing.
In order to escape from the limit of volumetric grid, some
methods partition the R3 space by the traditional data struc-
tures, such as k-d trees [6] and octrees [14, 17], to allevi-
ate the issues. PointNet [11] is the most pioneering work
that takes point cloud as input and applies MLPs and max
pooling to construct a universal approximator with permu-
tation invariance. Since the lack of sensing capability for
local information, a variety of hierarchical neural networks
for point cloud, such as PointNet++ [13] and DGCNN [18],
are proposed to progressively abstract features along a hi-
erarchical structure designed in a heuristic way. Recently,
Chen et al. [?] proposed to leverages nonlinear Radial Basis
6 Function (RBF) convolution as basis feature extractor for
robust point cloud representation. As far as we known, the
existing methods merely design the hierarchical structure
by priori knowledge and none of them have made effort to
explore the geometric structure underlying the point cloud,
which is prone to cause lower capacity of the hierarchical
neural network.
Hierarchical Clustering.
In the area of unsupervised
learning, hierarchical clustering [10] is a classical method
to build a hierarchy (also called dendrogram) of clusters. It
generally consists of agglomerative type and divisive type.
The ﬁrst one considers all data points as the smallest cluster
and merges the two closest ones with respect to a particular
distance metric and a linkage criteria from bottom to top,
and the latter performs in an opposite direction. A typical
linkage criteria is ward linkage minimizing the total within-
cluster variance, which can remedy the degeneration case
of uneven cluster sizes. Furthermore, the point cloud in low
dimensional space, such as R3, is quite suitable for hier-
archical clustering. A dendrogram and a partition of point
cloud is shown in Figure 1.
Rotation-Equivariant Network for 3D Objects.
Point-
Net [11] solves the permutation invariance problem of point
cloud by a symmetric pooling operator, which remarkably
reduces the N ! cases (given a point cloud with N points) of
permutation into merely one case. However, rotation invari-

4995

ance is a more challenging problem needed to be solved,
since SO(3) is inﬁnite. Previous works have attempted to
upgrade the existing neural networks with the property of
rotation-equivariance [7, 19]. For example, [16] designs
a special convolutional operation with local rotation in-
variance and applies it as basic block to build a rotation-
equivariant network. Besides, [2] proposes a method that
transforms the 3D voxel data into spherical representation
and then employs a spherical convolution operator to extract
rotation-equivariant features. However, it is unavoidable to
suffer from loss of information as there is no bijection be-
tween R3 and S2. 2

3. Approach

3.1. Rotation Invariant Representation in R3

A point cloud with N data points is often expressed as a
point set S = {(xi, yi, zi) | xi, yi, zi ∈ R}N −1
i=0 in Cartesian
coordinate system. In another experssion, it can be repre-
sented as S ∈ RN ×3 in a matrix form. In terms of the point
cloud in RN ×3, rotation transformation is a linear mapping
in correspondence with a 3 × 3 real orthogonal matrix.

In order to precisely describe the rigorous rotation invari-

ance, we conduct a deﬁnition as below.

Deﬁnition 1 (RRI Mapping). If N, D ∈ N+, a rigor-
ously rotation-invariant (RRI) mapping is a set mapping
F : RN ×3 7→ RN ×D such that

F(S) = F(R(S))

holds for all point set S ∈ RN ×3 and all rotation mapping
R ∈ SO(3). Then F(S) is called as a rigorously rotation-
invariant representation of S.

The deﬁnition introduces an RRI mapping that not only
maintains rotation invariance but also rigorously preserves
the cardinality of output point set as same as the cardinality
of the input one, i.e., the input set with N points should be
mapped to output set with N features.

For example, given a point set S = {pi | pi ∈ R3}N −1
i=0 ,
it is obvious that kpik2 is rotation-invariant since the rota-
tion invariance of 2-norm:

kRxk2

2 = kxk2

2 , ∀x ∈ R3.

(1)

Hence the row-wise vector norm k·k2 can be deﬁned as an
RRI mapping from RN ×3 to RN ×1.

Since rotation transformation has the property of pre-
serving the relative positional relationships among several
points, a deﬁnition is conducted to describe the rotation-
invariance of k-ary operator as follow.

2Two-dimensional sphere, denoted as S 2, is the surface of a completely

round ball in R3.

Deﬁnition 2 (RRI k-ary Operator). A rigorously rotation-
(RRI) k-ary operator is an operator G :
invariant
R3 × R3 × · · · × R3

7→ Rn such that

|

{z

k

}

G(Rx1, Rx2, . . . , Rxk) = G(x1, x2, . . . , xk) ,

holds for all x1, x2, . . . , xk ∈ R3 and all rotation mapping
R ∈ SO(3).

Apparently, the vector norm k·k2 is a unary RRI operator
from R3 to R. At the same time, it can be shown that the in-
ner product of two arbitrary points in S is rotation-invariant,
because rotation transformation is orthonormal:

hRx, Ryi = (Rx)T(Ry) = xTy = hx, yi ,

(2)

holds for ∀x, y ∈ R3. Thus inner product is an RRI bi-
nary operator from R3 × R3 to R. Note that hx, yi =
kxk2kyk2 cos θxy holds when x, y ∈ R3, the formulas
(1,2) imply that the relative angle θxy between any two
points x, y ∈ S is a rotation-invariant quantity.

Similarly, it can be proved that for any point p ∈ S\{0},
if Tp is an orthogonal projection operator of R3 onto a
plane L past the origin and p is orthogonal to L, then the
inner product of two arbitrary points in Tp(S) is rotation-
invariant. The proof is given as below:

hTRp(Rx), TRp(Ry)i

= (cid:0)Rx − ((Rx)T
= (cid:0)x −(cid:0)xTn(cid:1) · n(cid:1)T(cid:0)y −(cid:0)yTn(cid:1) · n(cid:1)

Rn) · Rn(cid:1)T(cid:0)Ry − ((Ry)TRn) · Rn(cid:1)

= hTp(x), Tp(y)i ,

(3)
where x, y ∈ R3, R ∈ SO(3) and n = p
kpk . Hence
the composite operator G1(x, y, p) = hTp(x), Tp(y)i is an
RRI ternary operator.

Furthermore, according to the property of cross product,
it can be shown that the composite operator G2(x, y, p) =
hTp(x) × Tp(y), pi is also an RRI ternary operator. The
proof is as below.

hTRp(Rx) × TRp(Ry), Rpi

= hRTp(x) × RTp(y), Rpi

= h(det R)(R−1)T(cid:0)Tp(x) × Tp(y)(cid:1), Rpi
= hR(cid:0)Tp(x) × Tp(y)(cid:1), Rpi

= hTp(x) × Tp(y), pi.

(4)

Consequently,

four rotation-invariant operators have
been found in the previous discussion, and we can make
use of them to construct a rotation-invariant representation
and the construction method is just an RRI mapping.

In order to introduce the proposed representation, we
need to construct a K-nearest neighbor (K-NN) graph G =

4996

$"#

$"

%"

!"#

%"#

!"&

$"&

%"&

'"&

'"#

Figure 2: The diagram illustrates each elements in RRI rep-
resentation (cf. formula (5)) by a trivial case in which we
builds 2-NN graph on three points.

(S, E) on point set S, where E = {(x, y) ∈ S × S | y is
one of the K-NN of x}.

According to the K-NN graph, we can employ the RRI
operators to capture the relative positional patterns underly-
ing the K-NN neighborhood of each point in S, and beneﬁt
from the property of rotation invariance at the same time.

Speciﬁcally, given a K-NN graph G on point set S, the

proposed representation of each point pi ∈ S is

(ri, (ri1, θi1, φi1), (ri2, θi2, φi2), ..., (riK , θiK , φiK)) ,

(5)

where

ri = kpik2 ,
rik = kpikk2

θik = arccos(cid:0)h

(pik is one of the K-NN of pi with id k) ,
pi
ri

i(cid:1) ,

pik
rik

,

φik = ψj ∗ , min{ψj | 1 ≤ j ≤ K, j 6= k, ψj ≥ 0} ,
ψj = atan2(sin ψj, cos ψj) ,

sin ψj = h

Tpi (pik)

kTpi (pik)k2

×

cos ψj = h

Tpi (pik)

kTpi (pik)k2

,

Tpi (pij)

kTpi (pij)k2
Tpi (pij)

i .

kTpi (pij)k2

,

pi
ri

i ,

(6)
Note that for a given point pi and one of its k-nearest
neighbor pik, if we apply pi as normal vector, then ψj rep-
resents the relative angle between Tpi (pik) and Tpi (pij)
according to the right-hand rule, thereupon φik is the rel-
ative angle between Tpi (pik) and its rotation-nearest point
Tpi (pij ∗ ) in anti-clockwise direction. The representation
(5) has intuitive geometric meaning which is illustrated in
Figure 2. The function atan2(·, ·) in formula (6) is a special
arctan(·) choosing the quadrant correctly.

On the foundation of the four RRI operators, the pro-
posed representation in formula (5) is rigorously rotation-
invariant as the claim of the following theorem.

Theorem 1. The mapping deﬁned by (6) is a rigorously
rotation-invariant mapping and the representation (5) is
rigorously rotation-invariant.

Proof. Firstly, the computation method in (6) exactly de-
ﬁnes a set of mappings F : RN ×3 7→ RN ×(3K+1).

Note that the K-NN neighborhood of arbitrary point
x ∈ S is uniquely determined by kx − yk2 with respect to
all point y ∈ S, which is proved rotation-invariant by (1), so
searching K-NN of point x ∈ S is a rotation-invariant op-
eration. Besides, since rotation transformation has no inﬂu-
ence to the permutation of point cloud, we can obtain con-
sistent order of K-NN by stable sort algorithm that main-
tains the relative order of points with equal distance.

As the result of (1,2), it is obvious that ri, rik and θik
are rotation-invariant. On the basis of formulas (3,4), sin ψj
and cos ψj are both of rotation-invariance, hence ψj and φik
are also rotation-invariant.

Therefore formulas (6) deﬁne an RRI mapping and the

representation deﬁned by (5) is an RRI representation.

However, RRI mapping probably loses some essential
information from the original data because the pursuit of
rigorous rotation invariance may result in lower capacity of
the RRI representation. For example, the 2-norm k·k2 is
indeed an RRI mapping as the discussion of formula (1),
whereas it only captures the distance information of the
points in S and totally discards the relative positional pat-
tern of them.

It is remarkable to point out that the proposed represen-
tation not only satisﬁes the property of rigorous rotation
invariance but also preserves necessary information which
helps to reconstruct the original point cloud on a weak con-
dition as stated in the following theorem.

Theorem 2. Given a K-NN graph G = (S, E) on point
set S, if G is a strongly connected graph, then for ∀R ∈
SO(3), given Cartesian coordinates of a nonzero point and
one of its non-collinear K-NN neighbor, the Cartesian co-
ordinates of R(S) can be determined by the RRI represen-
tation deﬁned by (5).

Proof. Given the Cartesian coordinates of arbitrary point
pi ∈ R(S)\{0} and one of its non-collinear K-NN neigh-
bor pik, we can obtain the 2-norm of them and their relative
positional information, such as θik and φik. With the rep-
resentation (5), we will show that the coordinate of another
K-NN neighbor pij ∗ , which is the rotation-nearest point
of pik in anti-clockwise direction after applying orthogonal
projection Tpi , can be uniquely determined by the following

4997

equation system,

3.2.1 RRI Representation Processing

hpij ∗ , pij ∗ i = r2
ij ∗

hpi, pij ∗ i = ri rij ∗ cos θij ∗
hTpi (pik), Tpi (pij ∗ )i = tik tij ∗ cos ψj ∗

(7)

hTpi (pik) × Tpi (pij ∗ ), pii = ri tik tij ∗ sin ψj ∗ ,

where ψj ∗ = φik, tik = kTpi (pik)k2. The quantities in the
right hand side of the equation system (7) are all known in
the representation (5). In other words, the unique unknown
variable is pij ∗ . On the foundation of the rotation invariance
revealed from Theorem 1, it is apparent that there exists at
least one solution for the equation system (7) because it has
a solution for the original point cloud S.

Suppose that the solution set of the equation system (7)

contains at least two different solutions pij ∗ and epij ∗ , then

the equation system (7) would imply that

hpi , pij ∗ − epij ∗ i = 0
hTpi (pik) , pij ∗ − epij ∗ i = 0
hTpi (pik) × (pij ∗ − epij ∗ ) , pii = 0

(8)

for some α ∈ R\{0}. However, it would imply that

Since both Tpi (pik) and pij ∗ − epij ∗ are in the plane L =
{x ∈ R3 | x ⊥ pi}, Tpi (pik) × (pij ∗ − epij ∗ ) = αpi holds
hTpi (pik) × (pij ∗ − epij ∗ ) , pii = αhpi , pii = 0 .

Since α 6= 0 and pi 6= 0, the equation (9) causes a con-
tradiction. Thus, the solution set of (7) contains a unique
solution, i.e., given points pi and pik, the Cartesian coor-
dinate of pij ∗ can be uniquely determined by the equation
system (7).

(9)

Similarly, we can solve the coordinate of the next neigh-
bor which is rotation-nearest from pij ∗ in anti-clockwise
direction after applying orthogonal projection Tpi . The pro-
cess is repeated until all the K-NN neighbors of pi are re-
constructed intactly.

Since the graph G = (S, E) is strongly connected, two
arbitrary points a, b ∈ S are connected by at least one path
(x0, x1, . . . , xn) between them, where n is the path length,
and x0 = a, xn = b and (xi, xi+1) ∈ E holds for 0 ≤ i ≤
n − 1. Therefore, starting from the K-NN neighborhood
of pi, we can restore the coordinate of arbitrary point in S
step-by-step along a path with ﬁnite length.

3.2. Hierarchical Clustering based ClusterNet

We propose a hierarchical clustering based neural net-
work, called ClusterNet, to learn a hierarchical structure
tree for the instruction of hierarchical feature representation
of point clouds. With the assistance of unsupervised learn-
ing, we can explore and exploit distribution information of
point cloud with regard to the hierarchical structure tree.

We can reformulate the proposed representation (5) of each
point pi ∈ S as

}

((ri, ri1, θi1, φi1)

, (ri, ri2, θi2, φi2)

, ..., (ri, riK , θiK , φiK)

).

|

{z

Ti1

}

|

{z

Ti2

}

|

{z

TiK

(10)
In other words, we summarize the RRI information between
point pi and its K nearest neighbors as (Ti1, Ti2, . . . , TiK)
to characterize point pi. Hence the new representation of a
point cloud S ∈ RN ×3 is a tensor T ∈ RN ×K×4. Since
the local neighborhood pattern of pi is probably embed-
ded in its K nearest neighbors, the proposed representation
takes advantages of the property and captures the local pat-
tern in the K-NN neighborhood by an RRI and conditional
information-lossless mechanism.

Since the RRI representation of point pi can be regarded
as a mini point cloud (Ti1, Ti2, . . . , TiK), and PointNet is a
universal continuous set function approximator, we can ap-
ply PointNet as a basic block to learn a representation of the
mini point cloud and extract local features to characterize
the K-NN neighborhood. In other words, we can transform
the RRI representation, an N × K × 4 tensor, into a N × D
tensor of neighborhood features by means of PointNet as
the following formula.

p′

i = max
1≤k≤K

fΘ(Tik) ,

(11)

where fΘ(·) is a multi-layer perceptron network with pa-
rameters Θ shared with all output features. In other words,
we extract a feature p′
i corresponding to the original point
pi ∈ S.

In the view of DGCNN, the formula (11) is a special
case of the EdgeConv. However, we utilize an RRI repre-
sentation to describe the relationship between a point and
its K-NN neighbors while DGCNN only uses the differ-
ence vector pi − pik concatenated with pi, both of them
vary with rotation transformation.

3.2.2 Hierarchical Clustering Tree

Since point could embeds in low dimensional space R3
equipped with Euclidean metric, hierarchical clustering is
an appropriate method to analyze the hierarchical structure
of point cloud. With the support of hierarchical clustering,
we can learn a hierarchical clustering tree which illustrates
the arrangement of partition and the relationships between
different clusters.

Speciﬁcally, we employ the agglomerative hierarchical
clustering with ward-linkage criteria to learn the hierarchi-
cal structures of point cloud. The ward-linkage criteria
minimizes the total within-cluster variance, which tends to
partition the point cloud into several clusters with similar

4998

d
u
o
C

l

i

&
t
n
o
P

3
x
N

RRI (64)

3
x
N

Cluster(

Abstraction(

(64,64,128)(32)

8
2
1
x
2
3

Max
Pool

3
x
N

RRI

mapping

4
x
K
x
N

mlp (64)

mlp (64)

.
.
.

shared

mlp (64)

4
6
x
K
x
N

RRI (64)

global features

Cluster(

Abstraction(
(256,256)(8)

6
5
2
x
8

Cluster(

Abstraction(
(512,1024)(1)

4
2
0
1

EdgeConv block

mlp((512,256,c)

c

classification
output scores

4
6
x
N

D
x
M

KHNN graph

mlp(%1

, %2,..., %()

Max
Pool

x
M

x
K
x
M

Max pooling(
within cluster

(C)

x
C

Cluster(Abstraction
(%1, %2,..., %()(C)

Figure 3: Model architecture: it consists of an RRI module, three cluster abstraction modules and the last classiﬁer module.
The model takes N points as input, applies the RRI module to extract rigorously rotation-invariant features for each point,
extracts hierarchical cluster features using three cluster abstraction modules, and eventually obtains a global feature of the
whole point cloud, which is used to generate classiﬁcation scores for c categories. RRI module: the RRI module employs
RRI mapping to obtain the RRI representation of point cloud and then aggregate the point features in K-NN neighborhood
into local embedding of each point. Cluster Abstraction module: the module extracts edge features of each sub-cluster using
multi-layer perception (mlp) with the number of layer neurons deﬁned as {a1, a2, . . . , an} and then applies neighborhood
aggregation to obtain super-cluster features. Then it leverages hierarchical structure as a guidance for feature aggregation
within each cluster.

sizes. On a particular merge-level in hierarchical clustering
tree, hierarchical clustering method will make an optimal
partition with respect to the objective function concerning
ward’s minimum variance.

Similar to CNNs, ClusterNet learns the local features of
ﬁne-grained geometric structures from small clusters and
then the local features are further aggregated into a higher-
level feature of larger cluster according to the cluster re-
lationships revealed in the hierarchical clustering tree. In
other words, we can apply the hierarchical clustering tree
to instruct the neural network how to extract and aggregate
features in a more efﬁcient way.

3.2.3 EdgeConv for Cluster Feature

The EdgeConv layer is ﬁrst proposed by DGCNN [18],
which improves the PointNet++ by dense sampling, i.e., all
points are considered as sampled points and the feature of
each point is aggregated from its K-nearest neighbors. The
K-nearest neighbors are determined by a dynamic K-NN
graph since the graph is affected by a similarity matrix of
features from previous layer. The dynamic K-NN graph fa-
cilitates nonlocal diffusion of similar features in the feature
space.

Speciﬁcally, given a F -dimensional point set P =
{x1, x2, . . . , xN } ⊂ RF , we can construct a K-NN graph
E⊆P × P and then the output of EdgeConv can be obtained
by

x′

i = max

j:(i,k)∈E

fΘ(xi − xk, xi) .

(12)

Different from DGCNN, the input of EdgeConv is a set
of D-dimensional cluster features C = {c1, c2, . . . , cn} ∈
RD, where n is the number of clusters in a particular parti-
tion. Hence, if we apply EdgeConv to C, the features corre-
sponding to K-nearest clusters of ci will be aggregated as a
higher-level feature to characterize the cluster ci.

3.2.4 Aggregation within Cluster

Since the hierarchical clustering tree contains relationship
of clusters, we propose a novel aggregation method for
point cloud, which utilizes the relationship to aggregate sub-
cluster features into that of a super-cluster. In particular, we
can apply max pooling function to the sub-cluster features
according to cluster index which records how sub-clusters
are merged into a super-cluster in the hierarchical cluster-
ing tree. Therefore, it is feasible to learn the hierarchical
representation of each cluster passing along the hierarchical
clustering tree from bottom to top and ﬁnally the global fea-
ture of the whole point cloud can be obtained from the root
node of the tree.

The proposed aggregation method is similar to the pool-
ing methods in CNNs, since they both downsample the in-
put data and maintain the maximum signal.
In terms of
the property of downsampling, the proposed aggregation
method can improve robustness against mild corruptions of
input data. Besides, aggregation reduces the total compu-
tational expense and the memory usage of GPU compared
with DGCNN which extracts features for all points in the
original point cloud.

4999

Method

Input (size)

z/z

z/SO(3)

SO(3)/SO(3)

SO(3)/SO(3)∗

PointNet (without STN) [11]

PointNet++ (MSG without STN) [13]

SO-Net (without STN) [8]

DGCNN (without STN) [18]

PointNet [11]

PointNet++ (MSG) [13]

SO-Net [8]

DGCNN [18]

Spherical CNN [2]

Ours

pc (1024 × 3)

pc+normal (5000 × 6)
pc+normal (5000 × 6)

pc (1024 × 3)

pc (1024 × 3)

pc+normal (5000 × 6)
pc+normal (5000 × 6)

pc (1024 × 3)
voxel (2 × 642)

pc (1024 × 3)

88.5
91.9
93.4
91.2

89.2
91.8
92.6
92.2

88.9

87.1

14.4
16.0
19.6
16.2

16.4
18.4
21.1
20.6

76.9

87.1

70.5
74.7
78.1
75.3

75.5
77.4
80.2
81.1

86.9

87.1

72.5
78.5
81.4
76.4

76.4
79.3
81.9
82.0

86.9

87.1

Table 1: Comparison of Rotation Robustness on rotation-augmented benchmark.

3.2.5 Permutation Invariance of ClusterNet

Trivial K-nearest neighbor searching is not permutation-
invariant, since the K-nearest neighbors will become un-
stable when there exists some neighbors with exactly the
same 2-norm. In such degeneration case, the result of K-
NN searching is inevitably affected by the order of input
points. However, we can modify the method of K-NN
searching to avoid such degeneration. Speciﬁcally, if pk
is the k-th nearest neighbor of p, then we consider the set
of k-nearest neighbors of p as {q ∈ S | kqk2 ≤ kpkk2}.
On the foundation of the modiﬁed K-NN searching and the
permutation-symmetric aggregation, it is obvious that the
proposed ClusterNet is of permutation invariance.

4. Experiments

In this section, we propose a novel benchmark to evalu-
ate the rotation robustness, on which we compare the pro-
posed method with the state-of-the-art methods to empir-
ically validate the effectiveness of the RRI representation
and ClusterNet. Furthermore, we conduct an experiment to
validate the superiority of ClusterNet over other architec-
tures to learn deep hierarchical embeddings from the RRI
representation. Finally, we evaluate the effect of the unique
hyperparameter K in the RRI representation if we construct
a K-nearest neighbor (K-NN) graph on point cloud.

4.1. Benchmarks

We design a new benchmark to fairly evaluate the rota-
tion robustness of a model. Since the majority of objects
in the original dataset are in a ﬁxed postures, we are re-
quired to conduct rotation augmentation to enrich the test
set. Considering SO(3) is inﬁnite, it is infeasible to cover
all the postures thoroughly, so we uniformly sample a rea-
sonable amount of rotation transformations from SO(3).

According to Euler’s rotation theorem [3], any rotation
can be represented by a Euler axis and a rotation angle. The
Euler axis is a three-dimensional unit vector and the rotation
angle is a scalar. We can employ the following formulas to

solve the rotation matrix R corresponding to the Euler axis
e and the rotation angle θ,

R = I3 cos θ + (1 − cos θ)eeT + [e]× sin θ ,

[e]× , 


0
e3
−e2

−e3

0
e1

e2
−e1

0


 .

(13)

As [4] stated, Fibonacci lattice is a mathematical idealiza-
tion of natural patterns with optimal packing, where the
area represented by each point is almost identical. Owing
to the favorable property, we sample the Fibonacci lattice
(points) from unit sphere surface as Euler axes and then
uniformly sample the rotation angle in the space [0, 2π).
We choose such sampling method to generate Euler axes
and rotation angles, and then solve the rotation matrix by
the formulas (13). Consequently, we obtained a rotaion-
sampling method that can sample rotation transformations
from SO(3) uniformly.

In terms of dataset, we choose ModelNet40 [20], a
widely-used 3D object classiﬁcation dataset, as our basic
dataset. ModelNet40 dataset consists of 12,311 CAD mod-
els from 40 manmade object categories, in which 9,843 is
used for training and 2,468 is used for testing. Since each
CAD model in ModelNet40 is composed of many mesh
faces, we sample point cloud from them uniformly with re-
spect to face area and then shift and normalize each point
cloud into [−1, 1]3 with centroid on the origin. We employ
the sampling method to generate 500 Euler axes and 60 ro-
tation angles for each Euler axes, i.e., 30,000 rotation trans-
formations are sampled uniformly from SO(3) to augment
the test set. As a result, we obtain a rotation-augmented test
set with 74,040,000 point clouds in total as the benchmark
dataset. We employ the augmented test set to evaluate the
rotation robustness of each model.

4.2. Comparison of Rotation Robustness

We compare the proposed method with the state-of-the-
art approaches on the benchmark for rotation-robustness

5000

Method

Accuracy (%) Time (h)

test set.

RRI+PointNet
RRI+DGCNN

RRI+ClusterNet (8, 1)
RRI+ClusterNet (32, 1)

RRI+ClusterNet (32, 8, 1)

85.9
86.4

86.6
86.8
87.1

8.5
12

9

10.5
9.5

Table 2: Analysis of Architecture Design

evaluation. The results are summarized in Table 1 with
four comparison modes: (1) both training set and test set
are augmented by azimuthal rotation (z/z); (2) training
with azimuthal rotation and testing with arbitrary rotation
(z/SO(3)); (3) both training and testing with arbitrary ro-
tation (SO(3)/SO(3)); (4) conditions are almost as same as
(3), but test the model with multi-rotation voting strategy
(SO(3)/SO(3)∗).
In order to make the comparison more
comprehensive, we make use of the following methods to
improve the rotation-robustness of existing methods.

Rotation-augmentation is applied to the training set us-
ing two sampling strategies respectively. The ﬁrst strategy
only samples azimuthal rotations for augmentation, i.e., we
merely use z-axis as Euler axis. While the second one sam-
ples all rotations from SO(3). In a particular epoch, we ro-
tate each object using the sampled rotation transformation
so that the model might improve rotation robustness from
the objects under different orientations. We can use multi-
rotation voting strategy to boost the robustness of model.
Specially, we feed the model with test set in several orienta-
tion and then sum up the conﬁdence scores as a total one to
determine the classiﬁcation result. Variants of spatial trans-
former network [5] are used to alleviate the problem caused
by rotation transformation. For example, both PointNet and
DGCNN employ spatial transformation module to learn a
3 × 3 rotation matrix which transforms point cloud into the
canonical space.

Table 1 consists of four groups of methods. The ﬁrst
group from the top of the table consists of four models
without using spatial transformer network (STN), while the
methods in the second group are equipped with STN. In
the third group, we choose a representative method based
on rotation-equivariant network, spherical CNN[2], to com-
pare with our proposed method. As shown in Table 1, the
widely used augmentation using azimuthal rotations suf-
fers from a sharp decline on the rotation-augmented test
set. Furthermore, it illustrates that rotation-augmentation
and STN can improve the rotation robustness of models but
still have a large margin with our proposed method without
the demand of any data augmentation. Although the spheri-
cal CNN is rotation-equivalent, it is also dependent with ro-
tation augmentation and its performance is sensitive to the
strategy of augmentation. Besides, our proposed method
also outperforms spherical CNN on the rotation-augmented

4.3. Ablation Analysis

4.3.1 Analysis of Architecture Design

Since the proposed RRI representation can be processed
to be compatible with many architectures dealing with
point cloud data, we enhance PointNet and DGCNN with
the RRI representation, and Table 2 shows that Cluster-
Net outperforms both the enhanced PointNet and the en-
hanced DGCNN by a large margin on the foundation of the
same RRI representation. As is illustrated in Section 3.2,
DGCNN is a special case of ClusterNet without cluster ag-
gregation, thus Table 2 shows that the aggregation within
cluster along hierarchy indeed facilitates the hierarchical
features learning and then extracts more discriminative fea-
tures for 3D recognization.

4.3.2 Effectiveness of K in RRI Representation

K

40

50

60

70

80

90

Acc. (%)

85.6

86.4

86.8

87.0

87.1

87.1

Table 3: Effectiveness of K in RRI Representation

In terms of the proposed RRI representation, K is the
unique hyperparameter, which controls the connectivity of
the graph G, thus we analyze the effectiveness of different
K in RRI representation. As shown in Table 3, the archi-
tecture of ClusterNet is robust to diverse values of K even
when K is too small to satisfy the connectivity condition in
Theorem 2. For example, when K = 40, there exists nearly
25% of the point clouds not satisfying strongly connected
condition, however, it still achieves comparable classiﬁca-
tion accuracy. When K is gradually increased to over 70,
accuracies of the model remain stable.

5. Conclusion

In this paper, we step forward to enhance the rotation
robustness of 3D object recognization model. Speciﬁcally,
we introduce a novel RRI representation to assign a unique
and consistent data form for any identical object in inﬁnite
attitude. We theoretically and empirically demonstrate that
the representation is rigorously rotation-invariant and con-
ditional information-lossless. Besides, our representation
is complementary with prevailing 3D recognition architec-
ture and improves their rotation robustness. Finally, we fur-
ther design a deep hierarchical network called ClusterNet
to better adapt to RRI representation. Extensive experimen-
tal evaluation on augmented test split set from widely-used
3D classiﬁcation benchmark demonstrates the superiority of
our novel RRI representation as well as the elaborate Clus-
terNet.

5001

In Proceedings of the IEEE in-
for 3d shape recognition.
ternational conference on computer vision, pages 945–953,
2015.

[16] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann
Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor ﬁeld
networks: Rotation-and translation-equivariant neural net-
works for 3d point clouds. arXiv preprint arXiv:1802.08219,
2018.

[17] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,
and Xin Tong. O-cnn: Octree-based convolutional neu-
ral networks for 3d shape analysis. ACM Transactions on
Graphics (TOG), 36(4):72, 2017.

[18] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,
Michael M Bronstein, and Justin M Solomon. Dynamic
graph cnn for learning on point clouds.
arXiv preprint
arXiv:1801.07829, 2018.

[19] Maurice Weiler, Mario Geiger, Max Welling, Wouter
3d steerable cnns: Learn-
Boomsma, and Taco Cohen.
In
ing rotationally equivariant features in volumetric data.
Advances in Neural Information Processing Systems, pages
10402–10413, 2018.

[20] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao.
3d
shapenets: A deep representation for volumetric shapes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1912–1920, 2015.

References

[1] Etienne Barnard and David Casasent. Invariance and neural
nets. IEEE Transactions on Neural Networks, 2(5):498–508,
1991.

[2] Carlos Esteves, Christine Allen-Blanchette, Ameesh Maka-
dia, and Kostas Daniilidis. Learning so (3) equivariant repre-
sentations with spherical cnns. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV), pages 52–68,
2018.

[3] L Euler. General formulas for the translation of arbitrary
rigid bodies. Novi Commentarii academiae scientiarum
Petropolitanae, 20(1776):189–207, 1790.
´Alvaro Gonz´alez. Measurement of areas on a sphere using
ﬁbonacci and latitude–longitude lattices. Mathematical Geo-
sciences, 42(1):49, 2010.

[4]

[5] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Advances in neural infor-
mation processing systems, pages 2017–2025, 2015.

[6] Roman Klokov and Victor Lempitsky. Escape from cells:
Deep kd-networks for the recognition of 3d point cloud mod-
els.
In Computer Vision (ICCV), 2017 IEEE International
Conference on, pages 863–872. IEEE, 2017.

[7] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Cleb-
sch–gordan nets: a fully fourier space spherical convo-
lutional neural network.
In S. Bengio, H. Wallach, H.
Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
editors, Advances in Neural Information Processing Systems
31, pages 10117–10126. Curran Associates, Inc., 2018.

[8] Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-
organizing network for point cloud analysis.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 9397–9406, 2018.

[9] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-
volutional neural network for real-time object recognition.
In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ
International Conference on, pages 922–928. IEEE, 2015.

[10] Daniel M¨ullner. Modern hierarchical, agglomerative cluster-

ing algorithms. arXiv preprint arXiv:1109.2378, 2011.

[11] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁca-
tion and segmentation. Proc. Computer Vision and Pattern
Recognition (CVPR), IEEE, 1(2):4, 2017.

[12] Charles R Qi, Hao Su, Matthias Nießner, Angela Dai,
Mengyuan Yan, and Leonidas J Guibas. Volumetric and
multi-view cnns for object classiﬁcation on 3d data. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 5648–5656, 2016.

[13] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In Advances in Neural Informa-
tion Processing Systems, pages 5099–5108, 2017.

[14] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.
Octnet: Learning deep 3d representations at high resolutions.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, volume 3, 2017.

[15] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik
Learned-Miller. Multi-view convolutional neural networks

5002

