Adversarial Semantic Alignment for Improved Image Captions

Pierre Dognin∗, Igor Melnyk∗, Youssef Mroueh∗, Jerret Ross∗ & Tom Sercu∗

IBM Research, Yorktown Heights, NY

{pdognin,mroueh,rossja}@us.ibm.com, {igor.melnyk,tom.sercu1}@ibm.com

Abstract

In this paper we study image captioning as a conditional
GAN training, proposing both a context-aware LSTM cap-
tioner and co-attentive discriminator, which enforces seman-
tic alignment between images and captions. We empirically
focus on the viability of two training methods: Self-critical
Sequence Training (SCST) and Gumbel Straight-Through
(ST) and demonstrate that SCST shows more stable gradient
behavior and improved results over Gumbel ST, even without
accessing discriminator gradients directly. We also address
the problem of automatic evaluation for captioning models
and introduce a new semantic score, and show its correlation
to human judgement. As an evaluation paradigm, we argue
that an important criterion for a captioner is the ability to
generalize to compositions of objects that do not usually co-
occur together. To this end, we introduce a small captioned
Out of Context (OOC) test set. The OOC set, combined with
our semantic score, are the proposed new diagnosis tools
for the captioning community. When evaluated on OOC and
MS-COCO benchmarks, we show that SCST-based training
has a strong performance in both semantic score and human
evaluation, promising to be a valuable new approach for
efﬁcient discrete GAN training.

1. Introduction

Signiﬁcant progress has been made on the task of generat-
ing image descriptions using neural image captioning. Early
systems were traditionally trained using cross-entropy (CE)
loss minimization [27, 11, 28]. Later, reinforcement learning
techniques [22, 23, 14] based on policy gradient methods,
e.g., REINFORCE, were introduced to directly optimize
the n-gram matching metrics such as CIDEr [26], BLEU4
[20] or SPICE [1]. Along a similar idea, [23] introduced
Self-critical Sequence Training (SCST), a light-weight vari-
ant of REINFORCE, which produced state of the art image
captioning results using CIDEr as an optimization metric.
Although optimizing the above automatic metrics might be a

promising direction to take, these metrics unfortunately miss
an essential part of the semantic alignment between image
and caption. They do not provide a way to promote natural-
ness of the language, e.g., as measured by a Turing test, so
that the machine-generated text becomes indistinguishable
from the text created by humans.

To address the problem of diversity and naturalness, im-
age captioning has recently been explored in the framework
of GANs [6]. The main idea is to train a discriminator to
detect a signal on the misalignment between an image and a
generated sentence, while the generator (captioner) can use
this signal to improve its text generation mechanism to better
align the caption with a given image. Due to the discrete
nature of text generation, GAN training remains challenging
and has been generally tackled with either reinforcement
learning techniques [29, 3, 8, 21, 4] or by using the Gumbel
softmax relaxation [9], for example, as in [25, 12].

Despite these impressive advances, image captioning is
far from being a solved task. It still is a challenge to satis-
factory bridge a semantic gap between image and caption,
and to produce diverse, creative and human-like captions.
The current captioning systems also suffer from a dataset
bias: the models overﬁt to common objects co-occurring in
common context, and they struggle to generalize to scenes
where the same objects appear in unseen contexts. Although
the recent advances of applying GANs for image captioning
to promote human-like captions is a very promising direc-
tion, the discrete nature of the text generation process makes
it challenging to train such systems. The results in [4, 25]
are encouraging but the proposed solutions are still complex
and computationally expensive. Moreover, the recent work
of [2] showed that the task of text generation for the cur-
rent discrete GAN models is still challenging, many times
producing unsatisfactory results, and therefore requires new
approaches and methods. Finally, evaluation of image cap-
tioning using automated metrics such as CIDEr, BLEU4,
etc. is still unsatisfactory since simple n-gram matching,
that does not reference the image, remains inadequate and
sometimes misleading for scoring diverse and descriptive
captions.

∗Equal Contributions. Authors in alphabetical order.

In this paper, we propose to address the above issues by

110463

accomplishing the following three main objectives: 1) Archi-
tectural and algorithmic improvements: We propose a novel
GAN-based framework for image captioning that enables
better language composition and more accurate composi-
tional alignment of image and text (Section 2.1), as well
as a light-weight and efﬁcient approach for discrete GAN
training based on SCST (Section 2.2). 2) Automated scor-
ing metric: We propose the semantic score, which enables
quantitative automatic evaluation of caption quality and its
alignment to the image across multiple models (Section 3).
3) Diagnostic dataset: Finally, we introduce the Out of Con-
text (OOC) test set which is a quick and useful diagnostic
tool for checking a model’s generalization to out of context
scenes (Section 3).

2. Adversarial Caption Generation

In this Section we present our novel captioner and the
discriminator models. We employ SCST for discrete GAN
optimization and compare it with the approach based on the
Gumbel trick. Our experiments (Section 4) show that SCST
obtains better results, even though it does not directly access
the discriminator gradients.

2.1. Compositional Captioner and Discriminator

Here we introduce an image captioning model with at-
tention that we call context aware captioning based on [15].
This allows the captioner to compose sentences based on
fragments of observed visual scenes in the training. Fur-
thermore, we introduce a discriminator that scores the align-
ment between images and captions based on a co-attention
model [16]. This gives the generator a signal on the seman-
tic alignment and the compositional nature of visual scenes
and language. We show in Section 4 that we obtain better
results across evaluation metrics when using this co-attentive
discriminator.
Context Aware Captioner Gθ. For caption generation, we
use an LSTM with visual attention [28, 23] together with a
visual sentinel [15] to give the LSTM a choice of attending
to visual or textual cues. While [15] feeds only an average
image feature to the LSTM at each step, we feed a mixture
of image and visual sentinel features ˆct−1 from the previous
step to make the LSTM aware of the last attention context,
as seen in Figure 1. We call it Context Aware Attention. This
simple modiﬁcation gives signiﬁcant gains, as the captioner
is now aware of the contextual information used in the past.
As reported in Table 1, a captioner with an adaptive visual
sentinel [15] gives 99.7 CIDEr vs. 103.3 for our Context
Aware Attention on COCO validation set.
Co-attention Pooling Discriminator Dη. The task of the
discriminator is to score the similarity between an image
and a caption. Previous works jointly embed the modali-
ties at the similarity computation level, which we call late
joint embedding, see Figure 2 (a). Instead, we propose to

Attention Model

CE

RL

Att2All [23]
Sentinel [15]
Context Aware (ours)

98.5
99.7
103.3

115.7

118.6

Table 1: CIDEr performance of captioning systems given
various attention mechanisms, Att2All [23], sentinel atten-
tion [4] and Context Aware attention on COCO validation set.
Models are built using cross-entropy (CE) and SCST [23]
(RL). Context aware attention brings large gains in CIDEr
for both CE and RL trained models.

w∗
t

p(wt|ht, ˆct)

MLP
ˆct

Attention

st

LSTM

I

ht−1

ht

xt

WE

w∗

t−1

yt

Linear

ˆct−1

Figure 1: Context Aware Captioner. At each step t, the
textual information w∗
t−1, and the mixture of image features
and visual sentinel ˆct−1 from previous step t−1 are fed to
the LSTM to make it aware of past attentional contexts.

jointly embed image and caption in earlier stages using a
co-attention model [16, 5] and compute similarity on the
attentive pooled representation. We call this a Co-attention
discriminator (Figure 2 (b)) and provide architectural details
below.

Given a sentence w composed of a sequence of words
(w1, . . . wT ), the discriminator embeds each word using the
LSTM (state dimension m = 512) to get H = [h1, . . . hT ]⊤
for H ∈ RT ×m, where ht, ct = LSTM(ht−1, ct−1, wt).
For image I, we extract features (I1, . . . IC), where C =
14 × 14 = 196 (number of crops) and also embed them as
I = [W I1, . . . W IC]⊤ ∈ RC×m, where W ∈ Rm×dI , and
dI = 2048, our image feature size. Following [16], we
then compute a correlation Y between image and text using
bilinear projection Q ∈ Rm×m, Y = tanh(IQH ⊤) ∈
RC×T . Matrix Y is used to compute co-attention weights of

10464

Image

Embed

Linear

Caption

Embed

LSTM

I

H

Linear

Linear

v

u

uT v

Sigmoid

fake/real

Caption

Embed

LSTM

H

Image

Embed

Linear

I

(a) Joint-Embedding Discriminator (Joint-Emb) [4]
P βiHi

Y × I + H

Softmax

Linear

β

Linear

ES

Bilinear Q

Y

ET

I ES

Sigmoid

fake/real

I + Y × H

P αiIi
(b) Proposed Co-Attention Discriminator (Co-att)

Softmax

Linear

α

Linear

EI

Figure 2: Discriminator architectures. (a) Joint-Embedding Discriminator from [4]. (b) Our proposed Dη. By jointly
embedding the image and caption with a co-attention model, we give the discriminator the ability to modulate the image
features depending on the caption and vice versa.

Gs
θ

ws

Dη

I

r(ws) = log (Dη (I, (ws

1, . . . , ws

T )))

(r(ws) − r( ˆw)) ∇θ log pθ(ws)

G∗
θ

ˆw

Dη

r( ˆw) = log (Dη (I, ( ˆw1, . . . , ˆwT )))

Figure 3: SCST Training of GAN-captioning.

one modality conditioned on another:

α = Softmax(Linear(tanh(IWI + Y HWIh))) ∈ RC,
β = Softmax(Linear(tanh(HWh + Y ⊤IWhI ))) ∈ RT ,

where all new matrices are in Rm×m. The above weights
are used then to combine the word and image features:

j=1 βjhj(cid:17)

EI = UI (cid:16)PC

i=1 αiW Ii(cid:17) and ES = VS(cid:16)PT

for UI , VS ∈ Rm×m. Finally, the image-caption score is
computed as Dη(I, w) = Sigmoid(E⊤
I ES) (η, discrimina-
tor parameters). In Section 4 we compare Dη with the late
joint embedding approach of [4, 25], where EI is the aver-
age spatial pooling of CNN features and ES the last state of
LSTM. We refer to this discriminator as Joint-Emb and to
ours as Co-att (see Figure 2).

2.2. Adversarial Training

In this Section we describe the details of the adversarial

training of the discriminator and the captioner.
Training Dη. Our discriminator Dη is not only trained to
distinguish real captions from fake (generated), but also to
detect when images are coupled with random unrelated real
sentences, thus forcing it to check not only the sentence com-
position but also the semantic relationship between image

and caption. To accomplish this, we solve the following
optimization problem: maxη LD(η), where the loss LD(η)

EI,w∈S(I) logDη(I, w) +

1
2

EI,ws∼pθ(.|I) log (1−Dη(I, ws))

+

1
2

EI,w′ /∈S(I) log (1 − Dη(I, w′)) ,

(1)

where w is the real sentence, ws is sampled from generator
Gθ (fake caption), and w′ is a real but randomly picked
caption.
Training Gθ.
The generator is optimized to solve
maxθ LG(θ), where LG(θ) = EI log Dη(I, Gθ(I)). The
main difﬁculty is the discrete, non-differentiables nature of
the problem. We propose to solve this issue by adopting
SCST [23], a light-weight variant of the policy gradient
method, and compare it to the Gumbel relaxation approach
of [9].
Training Gθ with SCST. SCST [23] is a REINFORCE
variant that uses the reward under the decoding algorithm as
baseline. In this work, the decoding algorithm is a “greedy
max”, selecting at each step the most probable word from
arg max pθ(.|ht). For a given image, a single sample ws of
the generator is used to estimate the full sequence reward,
LI
G(θ) = log(D(I, ws)) where ws ∼ pθ(.|I). Using SCST,

10465

the gradient is estimated as follows:

Feature Matching (FM) as follows:

∇θLI

G(θ) ≈ (log Dη(I, ws) − log Dη(I, ˆw)
}

{z

Baseline

|

= (cid:18)log

Dη(I, ws)

Dη(I, ˆw) (cid:19) ∇θ log pθ(ws|I),

)∇θ log pθ(ws|I)

LI
G(θ) = log(Dη(I, (y1, . . . yT )))
− λI

1, . . . w∗

F (cid:0)||EI (w∗
F (cid:16)||ES=(w∗

− λS

1 ,...w∗

T ) − EI (y1, . . . yT )||2(cid:1)
T )(I) − ES=(y1,...yT )(I)||2(cid:17) ,

(2)

where ˆw is obtained using greedy max (see Figure 3). Note
that the baseline does not change the expectation of the
gradient but reduces the variance of the estimate.

Also, observe that the GAN training can be regularized
with any NLP metric rNLP (such as CIDEr) to enforce close-
ness of the generated captions to the provided ground truth
on the n-gram level; the gradient then becomes:

(cid:18)log

Dη(I, ws)
Dη(I, ˆw)

+λ (rNLP(ws)−rNLP( ˆw))(cid:19)∇θlog pθ(ws|I).

There are two main advantages of SCST over other policy
gradient methods used in the sequential GAN context: 1)
The reward in SCST can be global at the sentence level and
the training still succeeds. In other policy gradient methods,
e.g., [4, 14], the reward needs to be deﬁned at each word
generation with the full sentence sampling, so that the dis-
criminator needs to be evaluated T times (sentence length).
2) In [4, 14, 8], many Monte-Carlo rollouts are needed to
reduce variance of gradients, requiring many forward-passes
through the generator. In contrast, due to a strong baseline,
only a single sample estimate is enough in SCST.

Training Gθ : Gumbel Trick. An alternative way to deal
with the discreteness of the generator is by using Gumbel
re-parameterization [9]. Deﬁne the soft samples yj
t , for
t = 1, . . . T (sentence length) and j = 1, . . . K (vocabulary
size) such that: yj
where gj are samples from Gumbel distribution, τ is a tem-
perature parameter. We experiment with the Gumbel Soft
and Gumbel Straight-Through (Gumbel ST) approaches, re-
cently used in [25, 12].

τ (logitsθ(j|ht, I) + gj)(cid:1) ,

t = Softmax(cid:0) 1

For Gumbel Soft, we use the soft samples yt as LSTM

input ws

t+1 at the next time step and in Dη:

∇θLI

G(θ) = ∇θ log(Dη(I, (y1, . . . yT ))).

For Gumbel ST, we deﬁne one-hot encodings Ot =
OneHot(arg maxj yj
t ) and approximate the gradients
∂Oj
t = δjj ′ . To sample from Gθ we use the hard
Ot as LSTM input ws
t+1 at the next time step and in Dη,
hence the gradient becomes:

t /∂yj ′

∇θLI

G(θ) = ∇θ log(Dη(I, (O1, . . . OT ))).

Observe that this loss can be additionally regularized with

1, . . . w∗

where (w∗
T ) is the ground truth caption correspond-
ing to image I, and EI and ES are co-attention image and
sentence embeddings (as deﬁned in Section 2.1). Feature
matching enables us to incorporate more granular informa-
tion from discriminator representations of the ground truth
caption, similar to how SCST reward can be regularized with
CIDEr, computed with a set of baseline captions.

3. Evaluation: Semantic Score and OOC Set

Semantic Score. Traditional automatic language metrics,
such as CIDEr or BLEU4, are inadequate for evaluating
GAN-based image caption models. As an early alternative,
[4, 25] used GAN discriminator for evaluation, but this is
not a fair comparison across models since the GAN gener-
ator was trained to maximize the discriminator likelihood.
In order to enable automatic evaluation across models we
propose the semantic score. Analogous to “Inception Score”
[24] for image generation, leveraging a large pretrained clas-
siﬁcation network, the semantic score relies on a powerful
model, trained with supervision, to heuristically evaluate
caption quality and its alignment to the image. In Section 4
we show that our semantic score correlates well with human
judgement across metrics, algorithms and test sets.

The semantic score is based on a Canonical Correlation
Analysis (CCA) retrieval model [18] which brings the im-
age into the scoring loop by training on the combination of
COCO [13] and SBU [19] (∼1M images), ensuring a larger
exposure of the score to diverse visual scenes and captions,
and lowering the COCO dataset bias. The semantic score
is a cosine similarity in CCA space based on a 15k dimen-
sion image embedding from resnet-101 [7], and a sentence
embedding computed using a Hierarchical Kernel Sentence
Embedding [18] based on word2vec [17]:

s(x, y) = (cid:10)ΣU ⊤x, V ⊤y(cid:11)

kΣU ⊤xk2 kV ⊤y∗k2

,

where x and y are caption and image embedding vectors, re-
spectively; U , Σ, and V are matrices obtained from CCA as
described in details in [18]. Note that the use of word2vec al-
lows the computation of scores for captions whose words fall
outside of the COCO vocabulary. The computed score can
be interpreted as a likelihood of the image given a caption, it
also penalizes the sentences which mention non-existent at-
tributes or objects. See Table 4 in Appendix A for examples.
Out of Context Set (OOC). An important property of the
captioner is the ability to generalize to images with objects

10466

CE
CIDEr-RL

GAN1(SCST, Co-att, log(D))
GAN2(SCST, Co-att, log(D)+5×CIDEr)
GAN3(SCST, Joint-Emb, log(D))
GAN4(SCST, Joint-Emb, log(D)+5×CIDEr)

GAN5(Gumbel Soft, Co-att, log(D))
GAN6(Gumbel ST, Co-att, log(D))
GAN7(Gumbel ST, Co-att, log(D)+FM)
CE∗ – ∗denotes non-attentional models
CIDEr-RL∗

COCO Test Set

CIDEr

METEOR

Semantic
Score

Vocabulary
Coverage

OOC (Out of Context)

CIDEr

METEOR

Semantic
Score

Vocabulary
Coverage

101.6 ±0.4
116.1 ±0.2

0.260 ±.001
0.269 ±.000

0.186 ±.001
0.184 ±.001

9.2 ±0.1
5.1 ±0.1

42.2 ±0.6
45.0 ±0.6

0.169 ±.001
0.170 ±.003

0.118 ±.001
0.117 ±.002

2.8 ±0.1
2.1 ±0.0

97.5 ±0.8
111.1 ±0.7
97.1 ±1.2
108.2 ±4.9

93.6 ±3.3
95.4 ±1.5
92.1 ±5.4

0.256 ±.001
0.271 ±.002
0.256 ±.002
0.267 ±.004

0.253 ±.007
0.249 ±.004
0.243 ±.011

0.190 ±.000
0.192 ±.000
0.188 ±.000
0.190 ±.000

0.187 ±.002
0.184 ±.003
0.175 ±.006

11.0 ±0.1
7.3 ±0.2
11.2 ±0.1
8.3 ±1.6

11.1 ±1.2
10.1 ±0.9
8.6 ±0.8

41.0 ±1.6
45.8 ±0.9
41.8 ±1.6
45.4 ±1.4

38.3 ±3.7
38.5 ±1.9
36.8 ±2.3

0.168 ±.003
0.173 ±.001
0.167 ±.002
0.173 ±.002

0.164 ±.006
0.161 ±.005
0.157 ±.006

0.124 ±.000
0.122 ±.002
0.122 ±.001
0.122 ±.003

0.121 ±.004
0.116 ±.004
0.110 ±.005

3.2 ±0.1
2.8 ±0.1
3.3 ±0.0
2.8 ±0.2

3.3 ±0.3
3.0 ±0.2
2.5 ±0.2

87.6 ±1.2
100.4 ±7.9

0.242 ±.001
0.253 ±.006

0.175 ±.002
0.173 ±.002

9.9 ±0.8
6.8 ±1.4

32.0 ±0.4
33.4 ±1.4

0.152 ±.002
0.154 ±.003

0.103 ±.002
0.101 ±.003

2.6
2.1

±.1
±.2

GAN1
GAN2
GAN3
GAN4

∗(SCST, Co-att, log(D))
∗(SCST, Co-att, log(D) + 5×CIDEr)
∗(SCST, Joint-Emb, log(D))
∗(SCST, Joint-Emb, log(D) + 5×CIDEr)

89.7 ±0.9
103.1 ±0.5
90.7 ±0.1
102.7 ±0.4

0.246 ±.001
0.261 ±.001
0.248 ±.001
0.260 ±.001

0.184 ±.001
0.183 ±.001
0.181 ±.001
0.182 ±.001

13.2 ±0.2
7.1 ±0.2
12.9 ±0.1
7.7 ±0.1

30.8 ±1.0
33.7 ±1.9
30.8 ±2.1
33.3 ±2.4

0.155 ±.003
0.157 ±.001
0.153 ±.002
0.157 ±.004

0.111 ±.001
0.108 ±.001
0.108 ±.001
0.106 ±.000

3.4 ±0.1
2.7 ±0.1
3.5 ±0.1
2.7 ±0.1

G-GAN [4] from Table 1

79.5

–

0.224

–

–

–

–

–

–

–

–

–

–

–

–

–

Table 2: Results for all models mentioned in this work. Scores are reported for both COCO and OOC sets. All results are
averaged (± standard deviation) over 4 models trained with different random seeds. See Table 5 in Appendix B for a full set of
results.

falling outside of their common contexts. In order to test
the compositional and generalization properties to out-of-
context scenes (see Figure 7 for an example), we expanded
the original set of [10] (containing 218 images) to a total of
269 images and collected 5 captions per image on Amazon
MTurk. We call the resulting dataset the Out of Context
(OOC) set. We note that although the size of OOC set is
not large, its main purpose is to be a useful quick diagnostic
tool rather than a traditional dataset. The evaluation on OOC
is a good indicator of a captioner’s generalization: poor
performance is a sign that the model is over-ﬁtted to the
training context. Improving OOC scores remains an open
area for future work, and we plan to release the OOC set as
well as the scripts for computing the semantic score.

4. Experiments

Experimental Setup. We evaluate our proposed method
and the baselines on COCO dataset [13] (vocabulary size
is 10096) using data splits from [11]: training set of 113k
images with 5 captions each, validation and test sets 5k each;
as well as on the proposed OOC diagnostic set. Each image
is encoded by a resnet-101 [7] without rescaling or cropping,
followed by a spatial adaptive max-pooling to ensure a ﬁxed
size of 14×14×2048. An attention mask is produced over
the 14×14 spatial locations, resulting in a spatially averaged
2048-dimension representation. LSTM hidden state, image,
word, and attention embedding dimensions are ﬁxed to 512
for all models. Before the GAN training, all the models
are ﬁrst pretrained with cross entropy (CE) loss. We report
standard language evaluation metrics, the proposed semantic
score, and the vocabulary coverage (percentage of vocabu-
lary used at generation).

Experimental Results. Table 2 presents results for both
COCO and OOC datasets for two discriminator architec-
tures (ours Co-att, and baseline Joint-Emb) for all training
algorithms (SCST, Gumbel ST, and Gumbel Soft). For refer-
ence, we also include results for non-GANs captioners: CE
(trained only with cross entropy) and CIDEr-RL (pretrained
with CE, followed by SCST to optimize CIDEr), as well as
results from non-attentional models. As expected, CIDEr-RL
greatly improves the language metrics as compared to the
CE model (from 101.6 to 116.1 CIDEr on COCO), but this
also leads to a signiﬁcant drop in the vocabulary coverage
(from 9.2% to 5.1% for COCO), indicating that the n-gram
optimization can lead to vanilla sentences, discouraging style
deviations from the ground truth captions. In the table, GAN1
, . . . , GAN4 denote the GAN-based models, where we use
SCST training (with log(D) or log(D)+5×CIDEr rewards)
with either Co-att or Joint-Emb discriminators; and GAN5
, . . . , GAN7 are the models trained with the Gumbel relax-
ation. From our extensive experiments we observed that
SCST provides signiﬁcantly more stable training of the mod-
els and better results as compared to Gumbel approaches,
which often become unstable beyond 15 epochs and under-
perform SCST GANs on many evaluation metrics (see also
Section E in Supplement for additional discussion on SCST
vs. Gumbel).

It can noticed that SCST GAN models outperform CE
and CIDEr-RL captioners on semantic score and vocabulary
coverage for both COCO and OOC sets. The CIDEr regu-
larization of SCST GAN additionally improves CIDEr and
METEOR scores, and also results in the improvement of
the semantic score (at the cost of some vocabulary coverage
loss) as seen for GAN1 vs. GAN2 and GAN3 vs. GAN4. We
also see that SCST GANs using our Co-att discriminator

10467

(a) COCO Test

(b) OOC

Figure 4: Evolution of semantic scores over training epochs for COCO Test and OOC datasets. Our Co-att models achieve
consistently higher scores than CE, RL and Joint-Emb models [4].

(a) COCO Test

(b) OOC

Figure 5: Evolution of vocabulary coverage over training epochs for COCO and OOC datasets. As training progresses, we
see a correlation between vocabulary coverage and semantic scores for all models. Models without CIDEr-regularized SCST
GAN rewards achieve best vocabulary coverage.

(CE and RL Baselines)

(SCST, Co-att, ∗)

(SCST, Joint-Emb, ∗)

(Gumbel ∗, Co-att, ∗)

EnsCE(CE)
EnsRL(CIDEr-RL)

Ens1(GAN1)
Ens2(GAN2)
Ens12(GAN1,GAN2)

Ens3(GAN3)
Ens4(GAN4)
Ens34(GAN3,GAN4)

Ens5(GAN5)
Ens6(GAN6)
Ens7(GAN7)
Ens567(GAN5,GAN6,GAN7)

(SCST+Gumbel Soft, Co-att, ∗) Ens125(GAN1,GAN2,GAN5)

COCO Test Set

OOC (Out of Context)

CIDEr METEOR Semantic Vocabulary CIDEr METEOR Semantic Vocabulary

Score

Coverage

Score

Coverage

105.8
118.9

102.6
115.1
113.2

109.8
113.0
111.1

100.1
99.6
100.2
103.2

112.4

0.266
0.273

0.262
0.277
0.274

0.270
0.274
0.271

0.259
0.253
0.254
0.258

0.273

0.189
0.186

0.195
0.194
0.195

0.193
0.193
0.193

0.191
0.187
0.180
0.188

0.195

8.4
5.0

9.9
7.0
7.3

8.5
7.6
8.1

10.0
9.3
7.8
8.7

7.7

44.8
48.8

44.8
48.3
49.9

48.5
48.0
50.1

43.1
41.0
38.9
41.8

49.8

0.172
0.175

0.172
0.176
0.178

0.175
0.178
0.177

0.170
0.165
0.164
0.164

0.179

0.122
0.122

0.129
0.127
0.129

0.127
0.127
0.127

0.127
0.122
0.113
0.121

0.129

2.6
2.1

3.0
2.7
2.6

2.8
2.7
2.8

3.0
2.8
2.3
2.7

2.7

Table 3: Ensembling results for some GANs from Table 2 for COCO and OOC sets. See Table 6 in Appendix B for complete
set of results including BLEU4 and ROUGEL.

outperform their Joint-Emb [4] counterparts on every metric
except vocabulary coverage (for COCO). We conclude that
GAN2, a CIDEr-regularized SCST with Co-att discriminator,

is the model with the best overall performance on COCO and
OOC sets. For baselining, we also reproduced results from
[4] with non-attentional generators (same architecture as in

10468

0510152025303540Epoch0.1800.1850.1900.1950.2000.205Semantic ScoreGAN1 (SCST, Co-att, log(D))                  [Ours]GAN2 (SCST, Co-att, log(D) + 5*CIDEr) [Ours]GAN3 (SCST, Joint-Emb, log(D))GAN4 (SCST, Joint-Emb, log(D) + 5*Cider)CERLGT0510152025303540Epoch0.10750.11000.11250.11500.11750.12000.12250.1250Semantic ScoreGAN1 (SCST, Co-att, log(D))                  [Ours]GAN2 (SCST, Co-att, log(D) + 5*CIDEr) [Ours]GAN3 (SCST, Joint-Emb, log(D))GAN4 (SCST, Joint-Emb, log(D) + 5*Cider)CERLGT0510152025303540Epoch567891011Vocabulary CoverageGAN1 (SCST, Co-att, log(D))                  [Ours]GAN2 (SCST, Co-att, log(D) + 5*CIDEr) [Ours]GAN3 (SCST, Joint-Emb, log(D))GAN4 (SCST, Joint-Emb, log(D) + 5*Cider)CERL0510152025303540Epoch2.22.42.62.83.03.23.4Vocabulary CoverageGAN1 (SCST, Co-att, log(D))                  [Ours]GAN2 (SCST, Co-att, log(D) + 5*CIDEr) [Ours]GAN3 (SCST, Joint-Emb, log(D))GAN4 (SCST, Joint-Emb, log(D) + 5*Cider)CERLMTurk: “Which one is the best caption?”

MTurk:“Is this image caption written by a human?”

36

31

27

26

e
g
a
t
n
e
c
r
e
P

35

30

25

20

23

20

19

18

e
g
a
t
n
e
c
r
e
P

100

80

60

40

20

0

100

100

65

29

28

3

GAN

CE

RL DISAGREE

GAN

Human

COCO Test OOC

Majority “No” Disagree Majority “Yes”

e
r
o
c
S

n
o
i

n

i

p
O
n
a
e
M

3.5

3.4

3.3

3.2

3.1

Ens2

Ens1

EnsRL

Ens3

Ens7

EnsCE

0.180 0.185 0.190 0.195 0.200

Semantic Score

(a) Best Caption

(b) Turing Test

(c) MOS COCO

Figure 6: Human evaluations of EnsCE, EnsRL and several GAN ensembles on COCO and OOC sets. (a) A distribution of
preferences for the best caption determined by the majority of 5 human evaluators; here GAN label indicates the Ens2 model.
(b) Turing test on detecting the human-written versus GAN-generated captions on COCO. We assign "yes/no’ with at least 4
out of 5, disagree otherwise. (c) Mean opinion score vs. Semantic score on COCO test images.

[4]). Non-attentional models are behind in all metrics, except
for vocabulary coverage on both datasets. Interestingly, Co-
att discriminators still provide better semantic scores than
Joint-Emb despite non-attentional generators.

Figures 4 and 5 show the evolution of semantic scores
and vocabulary coverage over the training epochs for
GAN1, . . . GAN4, CE, CIDEr-RL and ground truth (GT) cap-
tions. Semantic scores increase steadily for all cost functions
and discriminator architectures as the training sees more data.
In Figure 4 (a), GAN models improve steadily over CE and
RL, ultimately surpassing both of them mid-training. More-
over, Co-att GANs achieve higher semantic scores across
the epochs than Joint-Emb GANs. For CIDEr-regularized
SCST GANs, the same trend is observed but with a faster
rate since the models start off worse than CE and RL. For
OOC in Figure 4 (b), we see the same trend: Co-att GANs
outperforming the other approaches. For COCO, GT seman-
tic score is higher than the other models while the opposite
is true for OOC. This may be caused by the vocabulary
mismatch between OOC and the combination of COCO
and SBU. Figures 4 and 5 show that the semantic score im-
provement of GAN-trained models correlates well with the
vocabulary coverage increase for both COCO and OOC.

Ensemble Models. Table 3 presents results for ensemble
models, where a caption is generated by ﬁrst averaging the
softmax scores from 4 different models before word se-
lection. EnsCE and EnsRL ensemble CE and CIDEr-RL
models. Similarly, Ens1, . . . Ens7 ensemble models from
GAN1, . . . GAN7 respectively (Ensijk denotes an ensemble
of GANi, GANj , and GANk). As compared to individual
models, the ensembles show improved results on all met-
rics. Ensembling SCST GANs provides the best results,
reinforcing the conclusion that SCST is a superior method
for a stable sequence GAN training. For comparison, we

also computed SPICE [1] scores on COCO dataset: EnsCE
19.69, Ens12 20.64 (GAN Co-attention) and Ens34 20.46
(GAN Joint embedding from [4]), showing that SCST GAN
training additionally improves the SPICE metric. Finally,
we observe that underperformance of GANs over CIDEr-RL
in terms of CIDEr is expected and explained by the fact
that in GAN the objective is to make the sentences more de-
scriptive and human-like, deviating from the vanilla ground
truth captions, and this can potentially sacriﬁce the CIDEr
performance. The generated captions are evaluated using the
proposed semantic score which showed a good correlation
with human judgment; see Figure 6 (c) for more details.

Gradient Analysis. Throughout the extensive experiments,
the SCST showed to be a more stable approach for training
discrete GAN nodels, achieving better results compared to
Gumbel relaxation approaches. Figure 8 compares gradient
behaviors during training for both techniques, showing that
the SCST gradients have smaller average norm and variance
across minibatches, conﬁrming our conclusion.

Human Evaluation. To validate the beneﬁts of the semantic
score, we also evaluate the image/caption pairs generated
by several GAN ensembles, EnsCE and EnsRL on Amazon
MTurk. For a given image, 5 workers are asked to rate
each caption on the scale 1 to 5 (from which we computed
mean opinion score (MOS)) as well as to select the best
caption overall (additional details are given in Appendix D).
Figure 6 (a) shows that GAN ensemble Ens2 scored higher
than CE and CIDEr-RL on a majority vote, conﬁrming that
GAN training signiﬁcantly improves perceived quality of the
captions as compared to a more vanilla CE or RL-based cap-
tions. Figure 6 (b) gives Turing test results where the workers
are asked if a given caption is human or machine-generated.
Here, our GANs again performed well, demonstrating a good
capacity at fooling humans. In Figure 6 (c) we show that

10469

GAN: a row of motorcycles parked on the side of a street
CE: a row of motorcycles parked on a street
RL: a group of motorcycles parked in front of a building
GT: a bunch of motorcycles parked along the side of the street

GAN: two giraffes standing next to each other in a ﬁeld
CE: two giraffes standing in a ﬁeld with trees in the background
RL: a couple of giraffes standing next to each other
GT: two giraffe standing next to each other on a grassy ﬁeld

GAN: a bird is sitting on the ﬂoor next to a laptop
CE: a bird is sitting on the ﬂoor next to a laptop
RL: a bird sitting on the ﬂoor next to a laptop
GT: a bird on the ground with keys to the laptop are scattered on the ﬂoor

GAN: a black car parked on the side of the road
CE: a black car parked on the side of the road
RL: a black car parked on the side of a street
GT: a black car has gotten lodged atop a silver metal barrier on a roadway

GAN: a bus driving down a highway next to cars
CE: a bus driving down a street next to a car
RL: a bus driving down a highway with cars
GT: a bus traveling on a freeway next to other trafﬁc

GAN: a stop sign with grafﬁti on it next to a street
CE: a stop sign with a sticker on it
RL: a stop sign with a street on top of it
GT: grafﬁti on a stop sign supporting the red sox

GAN: a man riding a bike next to a stop sign
CE: a man is walking down the street with a sign
RL: a man riding a bike down a street with a stop sign
GT: a person hangs from a street sign using the sign to cover their face

GAN: a person is standing in front of a coffee cup
CE: a person is walking down a street with a cup
RL: a cup of coffee sitting on top of a street
GT: a person looks up at a street light that is designed like a pot of coffee

(a) COCO Test

(b) OOC

Figure 7: Examples of captions for our proposed model on COCO and OOC sets.

score is able to capture semantic alignments pertinent to
humans, validating it as a viable alternative to automatic
language metrics and a proxy to human evaluation.

Finally, in Figure 7 we present a few examples of the
captions for COCO and OOC sets. As compared to the
traditional COCO dataset, the OOC images are difﬁcult and
illustrate the challenge for the automatic captioning in such
settings. The difﬁculty is not only to correctly recognize the
objects in the scene but also to compose a proper description,
which is challenging even for humans (see row denoted by
GT), as it takes more words to describe such unusual images.

5. Conclusion

In conlusion, we summarize the main messages from our
study: 1) SCST training for sequence GAN is a promissing
new approach that outperforms the Gumbel relaxation in
terms of stability of training and the overall performance.
2) The modeling part in the captioner is crucial for general-
ization to out-of-context: we demonstrate that the non-atten-
tion captioners and discriminators – while still widely used –
fail at generalizing to out of context, hinting at a memoriza-
tion of the training set. Attentive captioners and discrimina-
tors succeed at composing on unseen visual scenes, as was
demonstrated with our newly introduced OOC diagnostic
set. 3) Human evaluation is still the gold standard for as-
sessing the quality of GAN captioning. We showed that the
introduced semantic score correlates well with the human
judgement and can be a valuable addition to the existing
evaluation toolbox for image captioning.

10470

Figure 8: L2 norm of the gradient with respect to the log-
its during training of Gθ with different training strategies.
The plots show a minibatch-mean during the training; the
variance of each curve gives a good idea of the gradient
stability between minibatches. We can see that SCST with
pure discriminator reward has the lowest gradient norm.

MOS of human evaluations correlates well with our seman-
tic score (see Table 7 in Appendix D for all scores). There
is an overall trend (depicted with a red regression line for
better visualization), where models that have higher seman-
tic score are generally favored more by human evaluators.
For example, Co-att SCST GANs Ens1 and Ens2 score the
best semantic (resp. 0.195 and 0.194) and MOS scores (resp.
3.398 and 3.442) on COCO. We can see that the semantic

020000400006000080000100000#updates10−410−310−210−1100||∇LIG(θ)||gumbel-STgumbelsoftgumbel-ST,+FMSCSTSCST,+CIDEr[17] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁ-
cient estimation of word representations in vector space.
ArXiv, 2013. 4

[18] Y. Mroueh, E. Marcheret, and V. Goel. Multimodal
retrieval with asymmetrically weighted CCA and hier-
archical kernel sentence embedding. ArXiv, 2016. 4,
10

[19] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: De-
scribing images using 1 million captioned photographs.
In NIPS, 2011. 4

[20] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu:
A method for automatic evaluation of machine transla-
tion. In ACL, 2002. 1

[21] S. Rajeswar, S. Subramanian, F. Dutil, C. Pal, and
A. Courville. Adversarial generation of natural lan-
guage. arXiv:1705.10929, 2017. 1

[22] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba.
Sequence level training with recurrent neural networks.
ICLR, 2015. 1

[23] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and
V. Goel. Self-critical sequence training for image cap-
tioning. In CVPR, 2017. 1, 2, 3

[24] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung,
A. Radford, and X. Chen. Improved techniques for
training gans. NIPS, 2016. 4

[25] R. Shetty, M. Rohrbach, L. A. Hendricks, M. Fritz, and
B. Schiele. Speaking the same language: Matching
machine to human captions by adversarial training.
ICCV, 2017. 1, 3, 4

[26] R. Vedantam, C. L. Zitnick, and D. Parikh. Cider:
In

Consensus-based image description evaluation.
CVPR, 2015. 1

[27] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show
and tell: A neural image caption generator. CVPR,
2015. 1

[28] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville,
R. Salakhutdinov, R. S. Zemel, and Y. Bengio. Show,
attend and tell: Neural image caption generation with
visual attention. In ICML, 2015. 1, 2

[29] L. Yu, W. Zhang, J. Wang, and Y. Yu. Seqgan: Se-
quence generative adversarial nets with policy gradient.
CoRR, abs/1609.05473, 2016. 1

References

[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould.
Spice: Semantic propositional image caption evalua-
tion.
In European Conference on Computer Vision,
pages 382–398. Springer, 2016. 1, 7

[2] M. Caccia, L. Caccia, W. Fedus, H. Larochelle,
J. Pineau, and L. Charlin. Language gans falling short.
arXiv preprint arXiv:1811.02549, 2018. 1

[3] T. Che, Y. Li, R. Zhang, D. R. Hjelm, W. Li,
Y. Song, and Y. Bengio. Maximum-likelihood
augmented discrete generative adversarial networks.
arXiv:1702.07983, 2017. 1

[4] B. Dai, D. Lin, R. Urtasun, and S. Fidler. Towards
diverse and natural image descriptions via a conditional
GAN. ICCV, 2017. 1, 2, 3, 4, 5, 6, 7, 11, 13

[5] C. N. dos Santos, M. Tan, B. Xiang, and B. Zhou.
Attentive pooling networks. Arxiv, abs/1602.03609,
2016. 2

[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In NIPS, 2014. 1

[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual

learning for image recognition. In CVPR, 2016. 4, 5

[8] R. D. Hjelm, A. P. Jacob, T. Che, K. Cho, and Y. Ben-
gio. Boundary-seeking generative adversarial networks.
arXiv:1702.08431, 2017. 1, 4

[9] E. Jang, S. Gu, and B. Poole. Categorical repa-
rameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144, 2016. 1, 3, 4

[10] M. Jinchoi, A. Torralba, and A. S. Willsky. Context

models and out-of-context objects, 2012. 5

[11] A. Karpathy and F.-F. Li. Deep visual-semantic align-
ments for generating image descriptions. In CVPR,
2015. 1, 5

[12] M. J. Kusner and J. M. Hernández-Lobato. Gans for se-
quences of discrete elements with the gumbel-softmax
distribution. arXiv:1611.04051, 2016. 1, 4

[13] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B.
Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollár,
and C. L. Zitnick. Microsoft COCO: common objects
in context. EECV, 2014. 4, 5

[14] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Mur-
phy. Improved image captioning via policy gradient
optimization of spider. In ICCV, 2017. 1, 4

[15] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing
when to look: Adaptive attention via a visual sentinel
for image captioning. In CVPR, 2017. 2

[16] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical
question-image co-attention for visual question answer-
ing. In NIPS, 2016. 2

10471

