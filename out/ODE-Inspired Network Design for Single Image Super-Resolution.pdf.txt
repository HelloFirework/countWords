ODE-inspired Network Design for Single Image Super-Resolution

Xiangyu He1

,

2∗, Zitao Mo1

,

2 ∗, Peisong Wang1, Yang Liu4, Mingyuan Yang4, Jian Cheng1

2

,

,

3 

1 NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China

2 University of Chinese Academy of Sciences, Beijing, China

3 Center for Excellence in Brain Science and Intelligence Technology, CAS, Beijing, China

4 Alibaba Group, Hangzhou, China

{xiangyu.he, zitao.mo, peisong.wang, jcheng}@nlpr.ia.ac.cn

Abstract

Single image super-resolution, as a high dimensional
structured prediction problem, aims to characterize ﬁne-
grain information given a low-resolution sample. Recent
advances in convolutional neural networks are introduced
into super-resolution and push forward progress in this
ﬁeld. Current studies have achieved impressive perfor-
mance by manually designing deep residual neural net-
works but overly relies on practical experience.
In this
paper, we propose to adopt an ordinary differential equa-
tion (ODE)-inspired design scheme for single image super-
resolution, which have brought us a new understanding
of ResNet in classiﬁcation problems. Not only is it in-
terpretable for super-resolution but it provides a reliable
guideline on network designs. By casting the numerical
schemes in ODE as blueprints, we derive two types of net-
work structures: LF-block and RK-block, which correspond
to the Leapfrog method and Runge-Kutta method in numeri-
cal ordinary differential equations. We evaluate our models
on benchmark datasets, and the results show that our meth-
ods surpass the state-of-the-arts while keeping comparable
parameters and operations.

1. Introduction

Super-resolution (SR) enjoys the recent advances in deep
learning and has attracted much attention in recent years
due to the rapid growth of image and video data. Generally
speaking, super-resolution can be applied to many applica-
tions including medical image processing [19], satellite and
aerial imaging [46], facial image improvement [28], etc. Al-
though obtaining high-resolution images from one or sever-
al low-resolution samples can be an ill-posed problem, con-
volutional neural networks have powered this ﬁeld, making

∗These authors contributed equally to this work.

the resulting images natural and detailed. In this paper, we
focus on single image super-resolution (SISR).

In light of the empirical success of convolutional neural
networks (CNN) in high-level computer vision tasks such
as image classiﬁcation, Dong et al. [11] proposed a CNN-
based SR algorithm. Since then, convolutional neural net-
works have became the mainstream in the ﬁeld of super-
resolution [29, 20, 22, 12, 31]. Though the performance is
improving with tremendous effort, there remain some lim-
itations: 1) Previous researches tend to care less about the
computation overhead and introduce deeper convolutional
neural networks to enhance performance. The huge amount
of calculations makes it intractable to apply the algorithm to
real-world applications. 2) Another side effect is that with
the depth increases, more training tricks are required. Oth-
erwise, the training procedure becomes numerically unsta-
ble [35, 31]. 3) Super-resolution is different from high-level
visual tasks such as image classiﬁcation, which extracts se-
mantic features via a convolutional neural network. In con-
trast, super-resolution predicts pixel-level ﬁne-grained in-
formation. Directly employing state-of-the-art CNNs does
not necessarily lead to an optimal solution.

To alleviate these problems, we propose to apply ODE-
inspired schemes to Super-Resolution network designs
(OISR). First, we revisit the similarity between forward
Euler method and residual structure by adopting the view
of dynamical system, identifying that we can take advan-
tage of ODEs for SISR network designs. Second, we de-
velop two kinds of building blocks, corresponding to the
Leapfrog method and Runge-Kutta method in numerical
ODEs. To the best of our knowledge, this is the ﬁrst at-
tempt to introduce ODE-inspired schemes into single image
super-resolution network design directly, providing a help-
ful viewpoint of single image super-resolution and a rela-
tively reliable guidance on network designs. In this work,
both lightweight and deep networks are generated using
proposed building blocks. Experimental results on bench-

1732

mark datasets demonstrate that our methods outperform the
state-of-the-arts, which indicates a better trade-off between
performance and computing cost. Lastly, we explore differ-
ent module G while maintaining a relatively stable amount
of computation. It is shown that our deep networks con-
verge rapidly without extra training tricks.

ResNet [16], they further propose a hierarchical features fu-
sion architecture to utilize features at different scales.

Although the empirical success of CNN-based super-
resolution methods is encouraging, most state-of-the-art de-
sign their networks empirically. It is clear that this hand-
crafted process requires a lot of tricks and attempts.

2. Related work

2.1. Single image super resolution

Single image super-resolution is a classical computer
vision task. Learning mapping functions from the low-
resolution image to high-resolution one is popular in previ-
ous literatures. These algorithms apply traditional machine
learning techniques to image super-resolution, including P-
CA [3], kernel method [4], learning embedding [8], sparse-
coding [43], etc. Another kind of methods make use of the
image self-similarity without external databases. [14] ex-
ploits the patch redundancy to obtain a super-resolution im-
age. Freedman et al.[13] go further and develop a localized
searching algorithm. Huang et al. [18] extend this method
by using detected perspective geometry to guide the patch
search process.

Recent advances in SISR take advantage of the powerful
representation capability of convolutional neural networks.
Dong et al. [11] introduce SRCNN for SISR, they interpret
the hidden layers in CNN as extraction, non-linear mapping,
and reconstruction, corresponding to those steps in sparse
coding [43]. DRCN [23] takes it a step further by ﬁrstly in-
terpolating the LR images to the desired size, which suffers
from “details lost” and the huge computational complexity.
Kim et al. [20, 22] follow the same design pattern of using
bicubic interpolation to upsample the image to the desired
size, but they adopt a deep residual convolutional neural net-
work to obtain a better representation. Since then, several
deeper CNN-based super-resolution models have been pro-
posed to achieve superior performance, including DRNN
[36], LapSRN [24], SRResNet [27], etc.

However, deeper architectures bring about a larger
amount of computation with the impressive progress on
benchmark datasets. To address this problem, Dong et al.
[12] remove the bicubic interpolation in SRCNN and intro-
duced a deconvolution layer at the end of FSRCNN. They
also adopt smaller ﬁlter sizes and a deeper network struc-
ture.
In order to further reduce parameters, DRRN [37]
introduces the combination of the recursive and residual
blocks while compromising the runtime speed. Recently,
CARN [1] presents a cascading mechanism upon a residual
network, which utilizes the multi-level representation and
multiple shortcut connections. To exploit the inter-relation
of multi-scale factors, [31] proposes an MSRN model to
encourage the feature reuse of different upsampling fac-
tors. MSRN [29] is a similar multi-scale method based on

2.2. Bridging network design with ODEs

ResNet [16] and its variants [17, 41, 35] have become
popular in a wide range of applications, including low-level
visual tasks such as super-resolution. Inspired by its clever
insight, many studies on network topology have emerged
and promoted the process. Zhang et al. [45] propose Poly-
Inception module and enhance the generalization ability.
Larsson et al. [26] make use of the self-similarity and de-
velop FractalNet. These networks share a similar idea of
feature fusion by establishing multiple connections between
different layers, which is proved to be effective.

Many good efforts try to bridge the gap between good
performance and a poor understanding of where the effec-
tiveness of residual connections stemming from. These s-
tudies mainly start from searching for similar mathemati-
cal structures and expect to take advantage of these well-
developed theories. Liao and Poggio [30] show that deep
ResNet is equivalent to a shallow RNN. The author of [40]
ﬁrst observes the relationship between ResNet and ODE.
They grant deep neural networks as a discrete dynamical
system, identifying the similarity between ResNet and the
discretization of ODEs. Chang et al. [5] do more than ex-
planation. They make use of numerical ODEs to construct
reversible neural networks with the stability analysis. Lu
et al.
[32] focus on the discretization schemes of ODEs.
They analyze the similarity between several network struc-
tures and numerical ODEs and propose an LM-architecture,
which originates from the linear multistep method. Al-
though these studies may not fully identify the true roots
of ResNet’s success, they shed some light on providing a
guidline for network designs.

3. ODE-inspired network design for SISR

Recent advances in single image super-resolution are at-
tributed to the progress of deep-learning, which enables
SISR to take a powerful end-to-end framework. Broadly
speaking, CNN-based methods map a low-resolution input
to a high-resolution image. From a dynamical system per-
spective, it deﬁnes a map that takes input status forward x
units of time in the phase space. In CNN semantics, time
horizon x corresponds to layers that can be adaptively cho-
sen, while the ﬁnal status is restricted by labels. However,
the problem lies in that, how to design a network that is able
to achieve the goal.
[40] describes it as a controllability
problem and explore the simpliﬁed one-dimensional case,
giving that there exits such a map generated by an ODE if

1733

the problem is smooth enough. Since SISR is such a low-
level visual problem with the constraint of high similarity
between inputs and outputs, it tends to approach the condi-
tions intuitively. Therefore, we are encouraged to take the
perspective of dynamical system and apply the rich knowl-
edge in ODE to SISR.

3.1. Mapping numerical ODEs to building blocks

In this section, we ﬁrst revisit the similarity between
forward Euler method and ResNet for clearness and self-
containment. We consider the dynamical systems which can
be described as an ODE, deﬁned as

It should be noted that formula (4) is a ﬁrst-order method
in numerical ODEs. Higher-order methods are supposed to
bring about some merits on reaching a more accurate solu-
tion. We are enlightened to deploy other numerical methods
for a ﬁner block.

LF-Block: LeapFrog method is a second-order linear 2-
step method, as a reﬁnement of forward Euler scheme. By
doubling the time interval h, we rewrite the approximation
of y′ in the form of y′ ≈ (yn+1 − yn−1)/2h, then derive the
following equation

yn+1 = yn−1 + 2hf (xn, yn),

(7)

dy
dx

= f (x, y).

This system gives a map

Φ(y0, x) = y(x; y0)

which can be directly interpreted into CNN diagram using
the deﬁnition in formula (6). In order to retain ﬂexibility
and obtain a block architecture, every three formulas above
are grouped into a block as

(1)

(2)

with initial status y0 ∈ Rd. Suppose p(y0) is the distribution
of input feature y0 on a domain Ω, if we regard CNN-base
SISR as such a dynamical system, then we are supposed to
minimize

L = ZΩ

kΦ(y0, x) − yk dp(y0)

(3)

where Φ is a map should be learned in SISR, and it is also
associated to the solution of Equation (1). When the system
is nonlinear, in many cases, there is no simple formula de-
scribing the map, we have to turn to numerical methods. As
presented in [6], forward Euler method

yn+1 = yn + hf (xn, yn)

(4)

provides an approximation, which can be seen as a numeri-
cal ODE using the approximation to the integral of y′ over
an interval of width h: yn+1 − yn ≈ hy′. Residual block
takes a similar form as

yn+1 = yn + G(yn).

(5)

This formula suggests the relationship [40, 32, 7, 9] and we
are able to establish the bridge by deﬁning

G(yn) = hf (xn, yn),

(6)

thus mapping forward Euler to a residual block.

In this paper, we consider the supervised SISR problem
where training data is provided to learn such a map Φ from
low-resolution images to high-resolution targets.
It may
take many steps to reach the ﬁnal status, each step corre-
sponds to a CNN block. Either increasing the number of
steps or reﬁning motion of each step helps to achieve the
goal, corresponding to increasing block numbers and de-
signing ﬁner blocks.

yn−1 = yn−3 + 2hf (xn−2, yn−2)
yn = yn−2 + 2hf (xn−1, yn−1)

yn+1 = yn−1 + 2hf (xn, yn).

(8)

(9)

(10)

Hence, we obtain an interesting structure as shown in Fig-
ure 1 (b). Unlike ResNet where G is deﬁned to be a certain
combinations of ReLU and convolutions, we do not restric-
t G to be a ﬁxed module except for its nonlinearity. The
details are discussed in section 3.2.

RK2-Block: To further explore this design scheme, we
now consider the Runge-Kutta family, which is widely used
in numerical analysis. Making use of a trapezoidal formula

yn+1 = yn +

h
2

(f (xn, yn) + f (xn+1, ˜yn+1))

(11)

and replace ˜yn+1 with its ﬁrst-order approximation (4), we
will obtain the following equations

yn+1 = yn +

1
2

(G1 + G2)

G1 = hf (xn, yn)
G2 = hf (xn + h, yn + G1).

(12)

(13)

(14)

In mathematics,
these formulas are refered as Heun’s
method, which is also a two-stage second-order Runge-
Kutta method. In order to map it to a CNN block, we use the
aforementioned G. Figure 1(c) further illustrates the inter-
pretation of these formulas. Compared with ResNet, there
are multiple branches in RK2-block, which is commonly
used in recent popular networks.

RK3-Block: The knowledge of numerical ODEs sug-
gest that higher-order methods (e.g., with order p) obtain
a smaller local truncation error (e.g., O(hp+1)). This fact
inspires us to explore higher stages Runge-Kutta methods.

1734

G

(cid:1877)(cid:3041)

addition

(cid:1877)(cid:3041)(cid:2878)(cid:2869)

(a) Residual-block

addition

(cid:1877)(cid:3041)(cid:2879)(cid:2871)
(cid:1877)(cid:3041)(cid:2879)(cid:2870)
(cid:1877)(cid:3041)(cid:2879)(cid:2869)
(cid:1877)(cid:3041)

addition

addition

G

G

G

G

addition

(cid:1877)(cid:3041)(cid:2878)(cid:2869)

(b) LF-block

×2
×2
×2
×2

(cid:1877)(cid:3041)

G

addition

G

×12

×12

addition

(cid:1877)(cid:3041)(cid:2878)(cid:2869)

(c) RK2-block

(cid:1877)(cid:3041)

×23

addition

(cid:1877)(cid:3041)(cid:2878)(cid:2869)

G

×12
×(cid:3398)1
×16

addition

addition

G

G

×2
×16

(d) RK3-block

Figure 1. (a) Residual block is widely used in previous works, e.g., EDSR [31]; (b) LF-block is derived from the Leapfrog method, we
combine three Leapfrog steps into a CNN block to compensate the missing term yn in formula (7); (c) RK2-block is derived from the
second-order Runge-Kutta method, also known as Heun’s method; (d) RK3-block is derived from the third-order Runge-Kutta method. In
all ODE-inspired blocks, we do not constrain the speciﬁc form of G (detailed in section 3.2).

Formally, explicit iterative Runge-Kutta methods can be ex-
tended to arbitrary n stages by using the following formulas

yn+1 = yn +

n

Xi=1

γiGi

G1 = hf (xn, yn)

(15)

(16)

Gi = hf (xn + αih, yn +

i−1

X=1

βijkj)

(17)

In particular, 3-stage Lunge-Kutta with third order can be
described as

yn+1 = yn +

1
6

(G1 + 4G2 + G3)

G1 = hf (xn, yn)
h
2

G2 = hf (xn +

, yn +

1
2

G1)

G3 = hf (xn + h, yn − G1 + 2G2).

(18)

(19)

(20)

(21)

(Please refer to the Appendix for the derivation of α, β, γ
if you are not familiar with numerical ODEs).
It is s-
traightforward to map these equations to a CNN block:
we just replace G1, G2, G3 with module G as deﬁned
above. RK3-block has more branches because 3-stage
Runge-Kutta method takes a computation pattern with high-
er complexity. Generally, higher-order methods tend to gen-
erate more complicated blocks.

It should be noted that in (13), (14) and (19)∼(21), G is
deﬁned as a function with two variables. Compared with

y corresponding to the featuremaps, the semantics of x in
CNN is implicit. It can be granted as a snapshot of time,
indicating the position of G in a deep neural network. The
initial status y0 ∈ Rd is the input featuremaps of the ﬁrst
OISR-blocks. y(X; y0) ∈ Rd refers to the output of the
last OISR-blocks with ﬁx time horizon X (i.e., given ﬁnite
depth). d = C × H × W where C, H, and W is channel
number, the height of featuremaps and width of featuremap-
s, respectively. Note that we keep the dimension of input
and output featuremaps of G unchanged. This allows us to
ﬁnish the bridge between ODE and CNNs.

Identity Connection

(cid:28663)

OISR-blocks

Conv

Conv

Conv

Pixel-Shuffle

Figure 2. The overall architecture of the proposed ODE-inspired
super-resolution network (OISR). For ×2/×3 super-resolution, we
use PixelShufﬂe ×2/×3.
In the ×4 model, the upsampler is
changed to the cascading of two ”Conv+Shufﬂe×2” modules. We
follow the same setting in EDSR [31] since the standard residual-
block is widely used. It is clear that OISR can be easily combined
with attention mechanisms and dense connections to further im-
prove the performance.

1735

3.2. Overall architecture for OISR

The overall architecture is a Convolution-PixelShufﬂe
framework, depicted in Figure 2. We emphasize that the de-
sign scheme is fully inspired by the numerical methods of
ODEs, all we need to do is mapping the numerical scheme
into a CNN block. Following the setting in [1, 31, 29, 37],
we do not use batch normalization layers. Strictly speaking,
equation (4) takes the form: yn+1 = yn + hnf (xn, yn). By
adaptively choosing hn (in this case, learned parameter α in
ParametricReLU [15]), one can improve the efﬁciency and
stability of the algorithm at the same time [25]. Besides, the
small initialization of α corresponds to the fact that the pre-
cision of numerical ODEs is related to the step size hn. If it
weren’t for ﬁnite precision arithmetic, the truncation error
goes to zero as hn goes to zero [34].

Conv
Conv

ReLU
ReLU

Conv

(a) Original

Conv
Conv

PReLU
PReLU

PReLU
PReLU

Conv
Conv

PReLU
PReLU

Conv
Conv

PReLU
PReLU

Conv
Conv

(b) (cid:1833)

(c) (cid:1833)-v2

(d) (cid:1833)-v3

Figure 3. Different structures of module G. (a) is the original G
used in EDSR [31]. (b) and (c) consist of a ParametricReLU and a
convolutional module. (d) deﬁnes an augmented G with the same
computing cost as (a).

G(·): There is a large searching space to search G. Here,
we only choose three different forms to illustrate the gener-
al effectiveness of ODE-inspired schemes. Each of these
designs keeps at least one activation function and one con-
volutional layer, thus promising the nonlinearity.
In fac-
t, different numerical methods (in this case, ODE-inspired
blocks) describe different approximation strategies overall,
while different G determines local behaviors in each step.
Either higher-order method or reﬁned G tends to improve
performance.

We build small-scale networks using LF-block and RK2-
block for each G. Since different forms of G vary in compu-
tation overhead, these networks are developed using differ-
ent numbers of building blocks but maintaining comparable
computation and parameters. This ensures us to have a fair
comparison with other lightweight models. Then we de-
velop middle-scale models using LF-block and RK2-block
with a similar size and computing cost as the state-of-the-
art MSRN [29]. This also enables us to verify that the per-
formance will not degrade as the networks become larger.
Finally, we design a deep network using RK3-blocks. S-
ince 3-stage Runge-Kutta is a third-order method, which is

OISR-RK3

OISR-LF-s

s
s
o
l

1
L

5.00

4.75

4.50

4.25

4.00

3.75

3.50

3.25

3.00

100

200

300

400

500

600

700

800

Epoch

Figure 4. The error curves of training ×2 OISR-LF-s (1.37M pa-
rameters) and ×2 OISR-RK3 (42M parameters). Our deep model
is easy to train and enjoys similar convergence speed as shallow
networks.

Table 1. Ablation studies on G(·). PSNR (db) in terms of 2× SR
on DIV2K validation set was reported. ”-s” denotes the single
scale baseline of EDSR and the small-scale version of OISR.

Method

EDSR-s

OISR-LF-s

OISR-RK2-s

G(·)

PSNR

Params

Conv+ReLU+Conv

Conv + PReLU
PReLU + Conv

(PReLU + Conv) x2

Conv + PReLU
PReLU + Conv

(PReLU + Conv) x2

34.61
34.64
34.67
34.66
34.62
34.59
34.63

1.37M
1.37M
1.37M
1.37M
1.37M
1.37M
1.37M

supposed to outperform low-order methods in approxima-
tion accuracy. For a fair comparison, we keep the overall
parameters of RK3 model almost the same as EDSR [31],
since residual block has only one G module yet RK3 con-
sists of three G modules.

4. Experiments

4.1. Datasets

Following the setting in [31, 42, 1, 29], we train our mod-
els on the 1th ∼ 800th training images in DIV2K [39], and
evaluate our models on four standard benchmark datasets:
Set5 [2], Set14 [44], B100 [33] and Urban100 [18]. The
ablation studies on G(·) is determnied on the 100 validation
images from DIV2K dataset. We report the peak signal-to-
noise ratio (PSNR) and structural similarity index (SSIM)
on the Y channel (i.e., luminance) of transformed YCbCr
space and ignore the same amount of pixels as scale from
the border [31, 29]. Speciﬁcally, for DIV2K validation set,
we measure PSNR on full RGB channels and remove the
(6+scale) pixels from each border to make a fair compar-
sion with EDSR. Upscaling factors: ×2, ×3, ×4 are used
for training and testing.

1736

Table 2. Quantitative comparisons of our models with the well-designed lightweight methods on benchmark datasets (PSNR(dB) / SSIM).
Red indicates the best performance and blue indicates the second best. ”MAC” denotes the number of multiply-accumulate operations
(a ← a + (b × c)). The smale-scale network designs are sufﬁxed by ”-s”. We assume that the generated SR image is 720P (1280 × 720).

Method

Scale Params MAC

Set5

Set14

B100

Urban100

PSNR

SSIM PSNR

SSIM PSNR

SSIM PSNR

SSIM

FSRCNN [12]
DRRN [37]
MemNet [38]
SelNet [10]
CARN[1]
OISR-RK2-s
OISR-LF-s
MSRN [29]
OISR-RK2
OISR-LF

FSRCNN [12]
DRRN [37]
MemNet [38]
SelNet [10]
CARN[1]
OISR-RK2-s
OISR-LF-s
MSRN [29]
OISR-RK2
OISR-LF

FSRCNN [12]
DRRN [37]
MemNet [38]
SelNet [10]
CARN[1]
OISR-RK2-s
OISR-LF-s
MSRN [29]
OISR-RK2
OISR-LF

×2
×2
×2
×2
×2
×2
×2
×2
×2
×2
×3
×3
×3
×3
×3
×3
×3
×3
×3
×3
×4
×4
×4
×4
×4
×4
×4
×4
×4
×4

0.01M 6.0G
37.00
0.30M 6796.9G 37.74
37.78
0.68M 623.9G
0.97M 225.7G
37.89
37.76
1.59M 222.8G
37.98
1.37M 316.2G
1.37M 316.2G
38.02
5.89M 1356.8G 38.08
4.97M 1145.7G 38.12
4.97M 1145.7G 38.12

0.01M 5.0G
33.16
0.30M 6796.9G 34.03
0.68M 623.9G
34.09
34.27
1.16M 120.0G
34.29
1.59M 118.8G
34.43
1.55M 160.1G
1.55M 160.1G
34.39
34.38
6.08M 621.2G
34.55
5.64M 578.6G
5.64M 578.6G
34.56

0.01M 4.6G
30.48
0.30M 6796.9G 31.68
31.74
0.68M 623.9G
32.00
1.42M 83.1G
32.13
1.59M 90.9G
1.52M 114.2G
32.21
32.14
1.52M 114.2G
32.07
6.33M 365.1G
32.32
5.50M 412.2G
5.50M 412.2G
32.33

0.9558
0.9591
0.9597
0.9598
0.9590
0.9604
0.9605
0.9605
0.9609
0.9609

0.9140
0.9244
0.9248
0.9257
0.9255
0.9273
0.9272
0.9262
0.9282
0.9284

0.8628
0.8888
0.8893
0.8931
0.8937
0.8950
0.8947
0.8903
0.8965
0.8968

32.63
33.23
33.28
33.61
33.52
33.58
33.62
33.74
33.80
33.78

29.43
29.96
30.00
30.30
30.29
30.33
30.35
30.34
30.46
30.46

27.49
28.21
28.26
28.49
28.60
28.63
28.63
28.60
28.72
28.73

0.9088
0.9136
0.9143
0.9160
0.9166
0.9172
0.9178
0.9170
0.9193
0.9196

0.8242
0.8349
0.8385
0.8399
0.8407
0.8420
0.8426
0.8395
0.8443
0.8450

0.7503
0.7720
0.7723
0.7783
0.7806
0.7822
0.7819
0.7751
0.7843
0.7845

31.53
32.05
32.08
32.08
32.09
32.18
32.20
32.23
32.26
32.26

28.53
28.95
28.96
28.97
29.06
29.10
29.11
29.08
29.18
29.20

26.90
27.38
27.40
27.44
27.58
27.58
27.60
27.52
27.66
27.66

0.8920
0.8973
0.8978
0.8984
0.8978
0.8996
0.9000
0.9013
0.9006
0.9007

0.7910
0.8004
0.8001
0.8025
0.8034
0.8053
0.8058
0.8041
0.8075
0.8077

0.7101
0.7284
0.7281
0.7325
0.7349
0.7364
0.7369
0.7273
0.7390
0.7389

29.88
31.23
31.31

–

31.92
32.09
32.21
32.22
32.48
32.52

26.43
27.53
27.56

–

28.06
28.20
28.24
28.08
28.50
28.56

24.52
25.44
25.50

–

26.07
26.14
26.17
26.04
26.37
26.38

0.9020
0.9188
0.9195

–

0.9256
0.9281
0.9290
0.9326
0.9317
0.9320

0.8080
0.8378
0.8376

–

0.8493
0.8534
0.8544
0.8554
0.8597
0.8606

0.7221
0.7638
0.7630

–

0.7837
0.7874
0.7888
0.7896
0.7953
0.7953

4.2. Training details

In the training phase, we use the RGB input patches of
size 48 × 48 from the low-resolution image with the corre-
sponding high-resolution patches. All the images are pre-
processed by subtracting the mean RGB value of the DI-
V2K dataset and then augmented with random horizontal
ﬂips and 90◦ rotations [31]. We set the minibatch size as 16
and use ADAM optimizer to train our model with the set-
tings of β1 = 0.9, β2 = 0.999, ǫ = 10−8. The learning
rate is initialized as 0.0001 and halved at every 250 epochs.
Training is terminated at 800 epochs. The objective of train-
ing OISR is the popular ℓ1 loss function.

4.3. Results on benchmark datasets

We ﬁrst do the ablation study on the implementation
of G. As listed in Table 1, ”PReLU+Conv”, namely G-

v2 is suitable for LF-blocks and RK2-blocks should be
equipped with G-v3. As mentioned in previous work-
s [1, 31, 29], deep models are difﬁcult to train. To ful-
ly examine the effectiveness of the ODE-inspired scheme,
we conduct the seemingly irrational behavior that applying
the worst-performing G-v2 in OISR-RK2 to our deep mod-
el OISR-RK3. Then, we compare our results with other
state-of-the-arts on two commonly-used metrics PSNR and
SSIM. As presented in Table 2, our small-scale models out-
perform other methods on different upscaling factors and
datasets, except a slightly behind on Urban100 with upscal-
ing factor ×2. In addition, we compare our middle-scale
models with MSRN. Our networks surpass MSRN with on-
ly two exceptions on B100 and Urban100 SSIM when the
upscaling factor is 2. These results illustrate that our meth-
ods better overcome the dilemma of performance enhance-

1737

Table 3. Quantitative comparisons of our models with hand-crafted deep residual SISR networks on benchmark datasets (PSNR(dB) /
SSIM). Red indicates the best performance and blue indicates the second best. We assume that the generated SR image is 720P (1280×720).

Method

Scale Params MAC

Set5

Set14

B100

Urban100

PSNR

SSIM PSNR

SSIM PSNR

SSIM PSNR

SSIM

LapSRN [24] ×2
×2
VDSR [21]
×2
DRCN [23]
×2
MDSR [31]
×2
RDN [42]
×2
EDSR [31]
×2
OISR-RK3
×3
×3
×3
×3
×3
×3
LapSRN [24] ×4
×4
VDSR [21]
×4
DRCN [23]
×4
MDSR [31]
×4
RDN [42]
×4
EDSR [31]
×4
OISR-RK3

VDSR [21]
DRCN [23]
MDSR [31]
RDN [42]
EDSR [31]
OISR-RK3

37.52
0.81M 29.9G
0.67M 612.6G
37.53
1.77M 17974G 37.63
6.92M 1592.2G 38.11
22.12M 5096.2G 38.24
40.73M 9384.7G 38.11
41.91M 9656.5G 38.21

0.67M 612.6G
33.66
1.77M 17974G 33.82
7.51M 768.1G
34.66
22.31M 2281.2G 34.71
43.68M 4469.5G 34.65
44.86M 4590.1G 34.72

31.54
0.81M 149.4G
0.67M 612.6G
31.35
1.77M 17974G 31.53
7.88M 480.4G
32.50
22.27M 1309.2G 32.47
43.10M 2894.5G 32.46
44.27M 2962.5G 32.53

0.9581
0.9587
0.9588
0.9602
0.9614
0.9601
0.9612

0.9213
0.9226
0.9280
0.9296
0.9282
0.9297

0.8855
0.8838
0.8854
0.8973
0.8990
0.8968
0.8992

33.08
33.03
33.04
33.85
34.01
33.92
33.94

29.77
29.76
30.44
30.57
30.52
30.57

28.19
28.01
28.02
28.72
28.81
28.80
28.86

0.9109
0.9127
0.9118
0.9198
0.9212
0.9195
0.9206

0.8314
0.8311
0.8452
0.8468
0.8462
0.8470

0.7722
0.7674
0.7670
0.7857
0.7871
0.7876
0.7878

31.80
31.90
31.85
32.29
32.34
32.32
32.36

28.82
28.80
29.25
29.26
29.25
29.29

27.32
27.29
27.23
27.72
27.72
27.71
27.75

0.8949
0.8960
0.8942
0.9007
0.9017
0.9013
0.9019

0.7976
0.7963
0.8091
0.8093
0.8093
0.8103

0.7280
0.7251
0.7233
0.7418
0.7419
0.7420
0.7428

30.41
30.76
30.75
32.84
32.89
32.93
33.03

27.14
27.15
28.79
28.80
28.80
28.95

25.21
25.18
25.14
26.67
26.61
26.64
26.79

0.9112
0.9140
0.9133
0.9347
0.9353
0.9351
0.9365

0.8279
0.8276
0.8655
0.8653
0.8653
0.8680

0.7553
0.7524
0.7510
0.8041
0.8028
0.8033
0.8068

ment and computation overhead.

For current state-of-the-art deep residual methods in
Table 3, OISR-RK3 achieves the best performances in
most cases. Though we apply the worst-performing G in
OISR-RK2 to OISR-RK3, it still achieves noticeable re-
sults. Moreover, Figure 4 further presents a comparison of
the convergence rate between OISR-LF-s and OISR-RK3,
which indicates that deeper OISR-RK3 is not difﬁcult to
train compared with much smaller OISR-LF-s. These re-
sults empirically verify the effectiveness of ODE-inspired
design scheme, and the behaviors of OISR tend to vary from
different numerical ODEs with different order of truncation
error.

4.4. Discussion on ODE inspired design schemes

In this work, we have developed an ODE-Inspired
scheme for SISR. Novel architectures are designed by in-
troducing numerical ODEs into CNNs. Table 1,3 have illus-
trated that, in the case of comparable computation and pa-
rameters, OISR-LF and OISR-RK outperform EDSR [31].
These results suggest the superiority of our methods. Here
we expand the discussion in ODE viewpoint.

As mentioned in section 3.1, residual block, LF-block,
and RK2-block can be regarded as mappings of numeri-
cal ODEs. EDSR develops deep architecture using resid-
ual block, related to the ﬁrst-order forward Euler method.

We propose to build our networks using RK-block and LF-
block, corresponding to the higher order methods in numer-
ical ODEs. Experimental results suggest that higher order
methods tend to enhance the performances. A similar fact
holds for deep networks as presented in Table 3, third-order
method RK3 performs better than ﬁrst-order EDSR. If we
take the dynamical system viewpoint, higher-order method-
s tend to make a better approximation of the map locally,
which enables them to approach the ﬁnal target with small-
er global (truncation) error, sustained in all the steps.

Table 1 presents the performances of proposed models
with different G. Our small-scale model outperforms EDSR
for each G with the same parameters and computing cost.
These results illustrate that ODE-inspired scheme is gener-
ally effective for SISR. Note that for OISR-RK2 and OISR-
LF, the best-performing G is different. OISR-LF tends to
take a simple G-v2 while OISR-RK2 prefers an augmented
G-v3. By adopting the view of the dynamical system, since
we keep a comparable computation, networks with G-v2 al-
lows more steps to give rise to the target (smaller hn), while
networks with G-v3 provides a better local approximation.
In order to control the amount of computation, augmented
G and more building blocks become the two sides of a coin.
Leaves out the computation overhead, one could enhance
the performance by designing ﬁner G or using more build-
ing blocks, as two strategies in deepening our networks.

1738

HR

(PSNR / SSIM)

Bicubic

(22.01 db / 0.8284)

CARN

(24.07 db / 0.8990)

MSRN

(23.97 db / 0.8929)

58060 from BSDS100

EDSR

(24.49 db / 0.9099)

OISR-RK3

(24.50 db / 0.9105)

OISR-LF-s

(24.19 db / 0.9019)

OISR-LF

(24.31 db / 0.9047)

HR

(PSNR / SSIM)

Bicubic

(18.21 db / 0.5209)

CARN

(18.84 db / 0.6274)

MSRN

(18.53 db / 0.5912)

img073 from Urban100

EDSR

(19.13 db / 0.6565)

OISR-RK3

(19.94 db / 0.6812)

OISR-LF-s

(18.75 db / 0.6287)

OISR-LF

(18.62 db / 0.6356)

Figure 5. Qualitative comparsions of our methods with other state-of-the-arts on ×2 super resolution (top) and ×4 super resolution (bot-
tom). OISRs can reconstruct more detailed images with less blurring.

In general, these results not only illustrate the effective-
ness of ODE-inspired design scheme but suggest the rea-
sonability of ODE perspective.

cases. This ﬁnding provides us with a relatively reliable
guideline to design networks for SISR.

5. Conclusions

In this paper, we propose to apply ODE-inspired scheme
to design CNN for SISR. By introducing the concept of
dynamical system one can establish the bridge between C-
NN and numerical ODEs. This connection enables us to
design LF-block, RK2-block, and RK3-block inspired by
Leapfrog method and Runge-Kutta method in numerical
analysis. Experimental results show that our methods ex-
ceed other state-of-the-art methods while keeping compara-
ble computation, which relieves the dilemma of enhancing
performance and reducing computation overhead. To ful-
ly verify the effectiveness of our method, we design sever-
al building blocks with different G. Experimental results
demonstrate that ODE-inspired scheme works well in most

6. Acknowledgement

This work was supported in part by National Natural Sci-
ence Foundation of China (No.61876182, 61872364), the
Strategic Priority Research Program of Chinese Academy
of Science(No.XDB32050200).

References

[1] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Fast,
accurate, and lightweight super-resolution with cascading
residual network. In Computer Vision - ECCV 2018 - 15th
European Conference, Munich, Germany, September 8-14,
2018, Proceedings, Part X, pages 256–272, 2018.

[2] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and
Marie-Line Alberi-Morel. Low-complexity single-image
super-resolution based on nonnegative neighbor embedding.

1739

In British Machine Vision Conference, BMVC 2012, Surrey,
UK, September 3-7, 2012, pages 1–10, 2012.

[3] David P. Capel and Andrew Zisserman. Super-resolution
from multiple views using learnt image models.
In 2001
IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR 2001), with CD-ROM, 8-14 De-
cember 2001, Kauai, HI, USA, pages 627–634, 2001.

[4] Ayan Chakrabarti, A. N. Rajagopalan, and Rama Chellappa.
Super-resolution of face images using kernel pca-based prior.
IEEE Trans. Multimedia, 9(4):888–892, 2007.

[5] Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David
Begert, and Elliot Holtham. Reversible architectures for ar-
bitrarily deep residual neural networks. In Proceedings of the
Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
(AAAI-18), the 30th innovative Applications of Artiﬁcial In-
telligence (IAAI-18), and the 8th AAAI Symposium on Edu-
cational Advances in Artiﬁcial Intelligence (EAAI-18), New
Orleans, Louisiana, USA, February 2-7, 2018, pages 2811–
2818, 2018.

[6] Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, and
David Begert. Multi-level residual networks from dynami-
cal systems view. CoRR, abs/1710.10348, 2017.

[7] Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, and
David Begert. Multi-level residual networks from dynami-
cal systems view. CoRR, abs/1710.10348, 2017.

[8] Hong Chang, Dit-Yan Yeung, and Yimin Xiong. Super-
resolution through neighbor embedding. In 2004 IEEE Com-
puter Society Conference on Computer Vision and Pattern
Recognition (CVPR 2004), with CD-ROM, 27 June - 2 July
2004, Washington, DC, USA, pages 275–282, 2004.

[9] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and
David K. Duvenaud. Neural ordinary differential equations.
In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Sys-
tems 2018, NeurIPS 2018, 3-8 December 2018, Montr´eal,
Canada., pages 6572–6583, 2018.

[10] Jae-Seok Choi and Munchurl Kim. A deep convolution-
al neural network with selection units for super-resolution.
In 2017 IEEE Conference on Computer Vision and Pattern
Recognition Workshops, CVPR Workshops, Honolulu, HI,
USA, July 21-26, 2017, pages 1150–1156, 2017.

[11] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Learning a deep convolutional network for image
super-resolution. In Computer Vision - ECCV 2014 - 13th
European Conference, Zurich, Switzerland, September 6-12,
2014, Proceedings, Part IV, pages 184–199, 2014.

[12] Chao Dong, Chen Change Loy, and Xiaoou Tang. Accel-
erating the super-resolution convolutional neural network.
In Computer Vision - ECCV 2016 - 14th European Confer-
ence, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part II, pages 391–407, 2016.

[13] Gilad Freedman and Raanan Fattal.

scaling from local self-examples.
30(2):12:1–12:11, 2011.

Image and video up-
ACM Trans. Graph.,

[14] Daniel Glasner, Shai Bagon, and Michal Irani.

Super-
resolution from a single image. In IEEE 12th International
Conference on Computer Vision, ICCV 2009, Kyoto, Japan,
September 27 - October 4, 2009, pages 349–356, 2009.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation. In 2015 IEEE Internation-
al Conference on Computer Vision, ICCV 2015, Santiago,
Chile, December 7-13, 2015, pages 1026–1034, 2015.

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages
770–778, 2016.

[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In Computer
Vision - ECCV 2016 - 14th European Conference, Amster-
dam, The Netherlands, October 11-14, 2016, Proceedings,
Part IV, pages 630–645, 2016.

[18] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single
image super-resolution from transformed self-exemplars. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages
5197–5206, 2015.

[19] John A. Kennedy, Ora Israel, Alex Frenkel, Rachel Bar-
Shalom, and Haim Azhari. Super-resolution in PET imaging.
IEEE Trans. Med. Imaging, 25(2):137–147, 2006.

[20] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June
27-30, 2016, pages 1646–1654, 2016.

[21] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June
27-30, 2016, pages 1646–1654, 2016.

[22] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-
recursive convolutional network for image super-resolution.
In 2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30,
2016, pages 1637–1645, 2016.

[23] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-
recursive convolutional network for image super-resolution.
In 2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30,
2016, pages 1637–1645, 2016.

[24] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-
Hsuan Yang. Deep laplacian pyramid networks for fast
and accurate super-resolution.
In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017, pages 5835–5843,
2017.

[25] John Denholm Lambert. Numerical methods for ordinary
differential systems: the initial value problem. John Wiley &
Sons, Inc., 1991.

[26] Gustav

Larsson, Michael Maire,

and Gregory
Fractalnet: Ultra-deep neural network-

Shakhnarovich.
s without residuals. CoRR, abs/1605.07648, 2016.

[27] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P. Aitken,

1740

Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a
generative adversarial network.
In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017, pages 105–114, 2017.

[28] Bo Li, Hong Chang, Shiguang Shan, and Xilin Chen. Low-
resolution face recognition via coupled locality preserving
mappings. IEEE Signal Process. Lett., 17(1):20–23, 2010.

[29] Juncheng Li, Faming Fang, Kangfu Mei, and Guixu Zhang.
Multi-scale residual network for image super-resolution. In
Computer Vision - ECCV 2018 - 15th European Conference,
Munich, Germany, September 8-14, 2018, Proceedings, Part
VIII, pages 527–542, 2018.

[30] Qianli Liao and Tomaso A. Poggio. Bridging the gaps be-
tween residual learning, recurrent neural networks and visual
cortex. CoRR, abs/1604.03640, 2016.

[31] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for s-
ingle image super-resolution. In 2017 IEEE Conference on
Computer Vision and Pattern Recognition Workshops, CVPR
Workshops, Honolulu, HI, USA, July 21-26, 2017, pages
1132–1140, 2017.

[32] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong.
Beyond ﬁnite layer neural networks: Bridging deep archi-
tectures and numerical differential equations.
In Proceed-
ings of the 35th International Conference on Machine Learn-
ing, ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, Ju-
ly 10-15, 2018, pages 3282–3291, 2018.

[33] David R. Martin, Charless C. Fowlkes, Doron Tal, and Jiten-
dra Malik. A database of human segmented natural images
and its application to evaluating segmentation algorithms and
measuring ecological statistics.
In ICCV, pages 416–425,
2001.

[34] Philip Robinson.

systems approach, part 1.
76(477):430–430, 1992.

Differential equations:

a dynamical
The Mathematical Gazette,

[35] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A. Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning.
In Proceed-
ings of the Thirty-First AAAI Conference on Artiﬁcial Intelli-
gence, February 4-9, 2017, San Francisco, California, USA.,
pages 4278–4284, 2017.

[36] Ying Tai, Jian Yang, and Xiaoming Liu.

Image super-
resolution via deep recursive residual network. In 2017 IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages
2790–2798, 2017.

[37] Ying Tai, Jian Yang, and Xiaoming Liu.

Image super-
resolution via deep recursive residual network. In 2017 IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages
2790–2798, 2017.

[38] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem-
net: A persistent memory network for image restoration. In
IEEE International Conference on Computer Vision, ICCV
2017, Venice, Italy, October 22-29, 2017, pages 4549–4557,
2017.

[39] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-
Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon
Kim, Seungjun Nah, Kyoung Mu Lee, Xintao Wang, Yapeng
Tian, Ke Yu, Yulun Zhang, Shixiang Wu, Chao Dong, Liang
Lin, Yu Qiao, Chen Change Loy, Woong Bae, Jae Jun Yoo,
Yoseob Han, Jong Chul Ye, Jae-Seok Choi, Munchurl Kim,
Yuchen Fan, Jiahui Yu, Wei Han, Ding Liu, Haichao Yu,
Zhangyang Wang, Honghui Shi, Xinchao Wang, Thomas S.
Huang, Yunjin Chen, Kai Zhang, Wangmeng Zuo, Zhimin
Tang, Linkai Luo, Shaohui Li, Min Fu, Lei Cao, Wen Heng,
Giang Bui, Truc Le, Ye Duan, Dacheng Tao, Ruxin Wang,
Xu Lin, Jianxin Pang, Jinchang Xu, Yu Zhao, Xiangyu Xu,
Jin-shan Pan, Deqing Sun, Yujin Zhang, Xibin Song, Yuchao
Dai, Xueying Qin, Xuan-Phung Huynh, Tiantong Guo, Ho-
jjat Seyed Mousavi, Tiep Huu Vu, Vishal Monga, Crist´ov˜ao
Cruz, Karen O. Egiazarian, Vladimir Katkovnik, Rakesh
Mehta, Arnav Kumar Jain, Abhinav Agarwalla, Ch V. Sai
Praveen, Ruofan Zhou, Hongdiao Wen, Che Zhu, Zhiqiang
Xia, Zhengtao Wang, and Qi Guo. NTIRE 2017 challenge on
single image super-resolution: Methods and results. In 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops, CVPR Workshops, Honolulu, HI, USA, July
21-26, 2017, pages 1110–1121, 2017.

[40] E Weinan. A proposal on machine learning via dynamical
systems. Communications in Mathematics and Statistics,
5(1):1–11, 2017.

[41] Saining Xie, Ross B. Girshick, Piotr Doll´ar, Zhuowen Tu,
and Kaiming He. Aggregated residual transformations for
deep neural networks. In 2017 IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2017, Honolu-
lu, HI, USA, July 21-26, 2017, pages 5987–5995, 2017.

[42] Jiu Xu, Yeongnam Chae, Bj¨orn Stenger, and Ankur Datta.
Dense bynet: Residual dense network for image super res-
olution. In 2018 IEEE International Conference on Image
Processing, ICIP 2018, Athens, Greece, October 7-10, 2018,
pages 71–75, 2018.

[43] Jianchao Yang, John Wright, Thomas S. Huang, and Yi
Ma. Image super-resolution via sparse representation. IEEE
Trans. Image Processing, 19(11):2861–2873, 2010.

[44] Roman Zeyde, Michael Elad, and Matan Protter. On single
image scale-up using sparse-representations. In Curves and
Surfaces - 7th International Conference, Avignon, France,
June 24-30, 2010, Revised Selected Papers, pages 711–730,
2010.

[45] Xingcheng Zhang, Zhizhong Li, Chen Change Loy, and
Dahua Lin. Polynet: A pursuit of structural diversity in very
deep networks. In 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2017, Honolulu, HI,
USA, July 21-26, 2017, pages 3900–3908, 2017.

[46] Yun Zhang. Problems in the fusion of commercial high-
resolution satellites images as well as landsat 7 images and
initial solutions. In Proceedings of the ISPRS, CIG, and SDH
Joint International Symposium on Geospatial Theory, Pro-
cessing and Applications, pages 9–12, 2002.

1741

