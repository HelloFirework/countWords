RES-PCA: A Scalable Approach to Recovering Low-rank Matrices

Chong Peng1, Chenglizhao Chen1, Zhao Kang2, Jianbo Li1, and Qiang Cheng3

1College of Computer Science and Technology, Qingdao University

2 School of Computer Science and Engineering, University of Electronic Science and Technology of China

3 Department of Computer Science, University of Kentucky

{pchong1991, cclz123}@163.com, zkang@uestc.edu.cn, lijianbo@188.com, qiang.cheng@uky.edu

Abstract

Robust principal component analysis (RPCA) has drawn
signiﬁcant attentions due to its powerful capability in re-
covering low-rank matrices as well as successful appplica-
tions in various real world problems. The current state-of-
the-art algorithms usually need to solve singular value de-
composition of large matrices, which generally has at least
a quadratic or even cubic complexity. This drawback has
limited the application of RPCA in solving real world prob-
lems. To combat this drawback, in this paper we propose
a new type of RPCA method, RES-PCA, which is linearly
efﬁcient and scalable in both data size and dimension. For
comparison purpose, AltProj, an existing scalable approach
to RPCA requires the precise knowlwdge of the true rank;
otherwise, it may fail to recover low-rank matrices. By con-
trast, our method works with or without knowing the true
rank; even when both methods work, our method is faster.
Extensive experiments have been performed and testiﬁed to
the effectiveness of proposed method quantitatively and in
visual quality, which suggests that our method is suitable
to be employed as a light-weight, scalable component for
RPCA in any application pipelines.

1. Introduction

Principal component analysis (PCA) bas been one of the
most widely used techniques for unsupervised learning in
various applications. The classic PCA aims at seeking a
low-rank approximation of a given data matrix. Mathe-
matically, it uses the ℓ2 norm to ﬁt the reconstruction er-
ror, which is known to be sensitive to noise and outliers.
The harder problem of seeking a PCA effective for outlier-
corruped data is called robust PCA (RPCA). There has
been no mathematically precise meaning for the term “out-
lier” [24]. Thus multiple methods have been attempted to
deﬁne or quantify this term, such as alternating minimiza-
tion [14], random sampling techniques [9, 17], multivariate
trimming [11], and so on [7, 27].

Among these methods, a recently emerged one treats an
outlier as an additive sparse corruption [25], which leads
to decomposing the data into a low-rank and a sparse part.

Given data matrix X ∈ Rd×n, based on such a decomposi-

tion assumption, the corresponding RPCA method aims to
mathematically solve the following problem [6, 25]:

min
L,S

rank(L) + λkSk0,

s.t. X = L + S,

(1)

where λ ≥ 0 is a balancing parameter, and k · k0 is the ℓ0
(pseudo) norm that counts the number of nonzero elements
of the matrix. It is generally NP-hard to solve the rank func-
tion and ℓ0 norm-based optimization problems. Hence, in
practice (1) is often relaxed to the following convex prob-
lem [6]:

L,S kLk∗ + λkSk1,
min

s.t. X = L + S,

(2)

where k · k∗ is the nuclear norm that adds all singular
values of the input matrix and k · k1 = Pij |Sij| is the
ℓ1 norm of a matrix. A number of algorithms have been
developed to solve (2), such as singular value threshold-
ing (SVT) [5], accelerated proximal gradient (APG) [23],
and inexact agumented Lagrange multipliers (IALM) [16].
These algorithms, however, need to compute SVDs of ma-
trices of size d × n at each iteration, which, is known to
generally have at least a quadratic or even cubic complex-
ity [12]. Thus, due to the use of SVDs, high complexity
of these algorithms renders them less applicable to large-
scale data. To improve efﬁciency, an augmented Lagrange
multipliers (ALM)-based algorithm adopts the PROPACK
package [10] to solve partial, instead of full, SVDs. Even
with partial SVD, it is still computationally costy when d
and n are both large.

The convex RPCA in (2) has two known limitations: 1)
Without the incoherence guarantee of the underlying ma-
trix, or when the data is grossly corrupted, the results can be
much deviated from the truth [6]; 2) When the matrix has
large singular values, its nuclear norm may lead to an esti-
mation far from the rank [13]. To combact these drawbacks,

17317

several approaches to a better rank approximation have been
proposed. For example, the rank of L is ﬁxed and used as
a hard constraint in [15], and a nonconvex rank approxi-
mation is adopted to more accurately approximate the rank
function in [13]. However, these nonconvex approaches
also need to solve full SVDs of d×n matrices. Two methods
in [15, 19] need only to solve partial SVDs, which signiﬁ-
cantly reduces the complexity compared to full SVDs; for
example, AltProj has a complexity of O(r2dn) [19], with
r being the ground truth rank of L. However, if r is not
known a priori, [19] usually fails to recover L.

As large-scale data is increasingly ubiquitous, it is cru-
cial to handle them with more efﬁcient and scalable RPCA
methods which, nonetheless, are still largely missing. To
address such a need and challenge, in this paper, we pro-
pose a new RPCA method, called RES-PCA. This model
does not depend on rank approximation to recover the low-
rank component; rather, it effectivelly exploits the under-
lying group structural information of the low-rank compo-
nent for the recovery. Consequently, the new method does
not need to solve any SVDs as current state-of-the-art meth-
ods typically do, which avoids any quadratic or higher com-
plexity; more speciﬁcally, the proposed method has a linear
complexity in both n and p, rendering it lightweight, scal-
able, and thus suitable for large-scale data applications. We
summarize the contributions of this paper as follows:

• We propose a new type of RPCA model exploiting the
underlying group structures of the low-rank compo-
nent.

• We develop an ALM-based algorithm for optimiza-
tion, which uses no matrix decomposition and has lin-
early efﬁcient computation at each iteration. The new
method is scalable in data dimension and sample size,
suitable for large-scale data.

• Extensive experiments have demonstrated the effec-
tiveness of the proposed method quantitatively and
qualitatively.

The rest of this paper is organized as follows. We ﬁrst
brieﬂy review some related work. Then we introduce the
new method and its optimization. Next, we conduct experi-
ments to evaluate the new method. Finally, we conclude the
paper.

2. Related Work

The convex RPCA in (2) considers the sparsity of the
sparse component in an element-wise manner [5]. To ex-
ploit example-wise sparsity, the ℓ2,1 norm has been adopted
by replacing the ℓ1 norm in (2) [18, 26]:

min
L,S kLk∗ + λkSk2,1,

s.t. X = L + S,

(3)

where kSk2,1 = PjqPi S2

ij is the sum of ℓ2 norms of
the columns. The difference between (2) and (3) is that the
latter incorporates spatial connections of the sparse compo-
nent.

It is ponited out that the nuclear norm may be far from
accurate in approximating the rank function [22]. To alle-
viate this defﬁciency, some new rank approximations have
been used to replace the nuclear norm in (2) and (3), such as
γ-norm [13]. The γ-norm based RPCA solves the following
optimization problem:

min
L,S kLkγ + λkSk2,1,

s.t. X = L + S,

(4)

(1+γ)σi(L)

where kLkγ = Pi

, γ > 0, and σi(L) is the i-
th largest singular value of L. Here, with different values
used for λ, the γ-norm may have different performance in
approximating the rank function.

γ+σi(L)

Another recent nonconvex approach to RPCA, AltProj,
cobmines the simplicity of PCA and elegant theory of con-
vex RPCA [19]. It alternatively projects the ﬁtting residuals
onto the low-rank and sparse sets. Given that the desired
rank of L is r, AltProj computes a rank-k projection in each
of the total r stages, with k ∈ {1, 2,··· , r}. During this
process, matrix elements with large ﬁtting errors are dis-
carded such that sparse errors are suppressed. This method
enjoys several nice properties; however, it needs the precise
knowledge of the ground truth rank of L, which is not al-
ways available. Without such knowledge, AltProj may fail
to recover the low-rank component.

3. New Robust PCA Method

The classic RPCA and its variants usually require to
solve SVDs, which has a high complexity. To overcome this
drawback, in this paper we consider a new type of RPCA
model that has a linear complexity. Motivated by the con-
vex RPCA approach, we assume that the data can be de-
composed as X = L+S. Here, L is the low-rank component
of X and its columns are linearly dependent in linear al-
gebra; hence, it is true that many columns of L share high
similarities and thus are close geometrically in Euclidean
space. In the case of a single rank-1 subspace, the above
assumption naturally leads to the minimization of the sum
of squared mutual distances, or equivalently the variance
(scaled by n), of the column vectors of L:

min
L,S

λ

n

n

Xi=1

Xj=1

kLi − Ljk2

2 +kSk1, s.t. X = L + S, (5)

where λ ≥ 0 is a balancing parameter, Li is the ith column
of L, and k · k2 is the ℓ2 norm of a vector. It is noted that,
though not necessary, it is sufﬁcient that the minimization of
the ﬁrst term in (5) leads to low-rank structure for L. To see

7318

nPn

i=1 kLi − 1

this, we reformulate it as 2nλPn

j=1 Ljk2
2,
which is the sum of squares of residuals (SSR) from each
data point to the average of all data points. Thus, by mini-
mizing it, all columns are close to their average and the av-
erage is the minimizer of SSR, which ideally lead to rank-1
solution to L. Under some mild conditions, we have the
following theorem.
Theorem 3.1. Given a matrix L = [l1,··· , ln], with li ∈
Rp, and klik2
2 = si, i = 1,··· , n, we have that rank(L) =
1 is sufﬁcient and necessary for

L = argmin
Q∈Rp×n

Tr(Q(I −

1
n

11T )QT ),

(6)

s.t.

(7)
where Q = [q1,··· , qn], and 1 is an all-1 vector of dimen-
sion n.

2 ≤ si, i = 1,··· , n,

kqik2

(5) can be written as Tr(L(In − 1

It is noted that the double summation in the ﬁrst term of
n 11T )LT ), by minimizing
which we can obtain the desired low-rank structure. It is
natural to generalize the above idea. To this end, we con-
sider the case of multiple rank-1 subspaces with the follow-
ing model, which we refer to as Robust, linearly Efﬁcient,
Scalable PCA (RES-PCA):

min

L,S,{p1,··· ,pc}

λ

c

Xi=1

Tr(cid:16)Ld(pi)(cid:16)In −

1
kpik2

1n1T

n(cid:17)d(pi)LT(cid:17)

+ kSk1

s.t. X = L + S,

pi ∈ {0, 1}n, Xi

pi = 1n,

(8)
where In is an identity matrix of size n × n, 1n is an n-
dimensional column vector containing 1’s, d(·) is an op-
erator that returns a diagonal matrix from an input vector,
and pi is a binary vector with the positions of 1s indicating
which of the n column vectors belong to the i-th subspace.
It is evident that by automatically learning pi’s we are able
to obtain the structural information about the low-rank sub-
spaces. It is noted that different norms can be used for S,
such as ℓ1 and ℓ21 norms; in this paper, without loss of gen-
erality, we adopt the ℓ1 norm to capture the sparse structure
of S. In next section, we will develop an efﬁcient algorithm
to optimize (8).

Remark In the case that data have nonlinear relation-
ships, i.e., Li and Lj are close on manifold rather than in
Euclidean space if they come from the same subspace, a
direct extension of our method can be made, which is pre-
sented in Section 4.2. Since the linear model provides with
us the key ideas and contributions of this paper, and the ex-
periments have conﬁrmed its effectiveness in several real
world applications, we focus on the linear model in our pa-
per. Due to space limit, we do not fully expand the nonlin-
ear model and will consider it in further research and more
applications.

4. Optimization

In this section, we present an efﬁcient ALM-based algo-
rithm to solve (8). First, we deﬁne the augmented Lagrange
function of (8):

L = λ

c

Xi=1
+ kSk1 +

Tr(cid:16)Ld(pi)(cid:16)In −

1
kpik2
1
ρ
2kX − L − S +
ρ
pi = 1n.

s.t. pi ∈ {0, 1}n, Xi

1n1T

n(cid:17)d(pi)LT(cid:17)
Θk2

F

(9)

Then we adpot the alternating decent approach to opti-
mization, where at each step we optimize a subproblem with
respect to a variable while keeping the others ﬁxed. The de-
tailed optimization strategies for each variable are described
in the following.

4.1. L minimization

The L-subproblem is to solve the following problem:

min

L

λ

c

Xi=1

Tr(cid:16)Ld(pi)(cid:16)In −
ρ
2kX − L − S + Θ/ρk2

1
kpik2

+

F

1n1T

n(cid:17)d(pi)LT(cid:17)

(10)

Omitting the factor λ, it is seen that the ﬁrst term above

can be derived as

c

c

Xi=1
Xi=1

1n1T

Tr(cid:16)Ld(pi)(cid:16)In −
Tr(cid:16)Pi(L)(cid:16)Ikpik2 −

1
kpik2
1
kpik2

n(cid:17)d(pi)LT(cid:17)
kpik2(cid:17)P T

=

1kpik2 1T

i (L)(cid:17),
(11)
where the operator Pi(L) returns the submatrix of L that
contains the columns of L corresponding to nonzeros of pi.
Correspondingly, it is straightforward to see that the second
term of (10) can be decomposed in a similar way:

F

c

ρ
2kX − L − S + Θ/ρk2
ρ
Xi=1
2

kPi(X − S + Θ/ρ) − Pi(L)k2
F .

=

(12)

Hence, L can be solved by individually solving the follow-
ing subproblems for i = 1,··· , c:
1
kpik2

λTr(cid:16)Pi(L)(cid:16)Ikpik2 −
ρ
2kPi(X − S + Θ/ρ) − Pi(L)k2

kpik2(cid:17)P T

i (L)(cid:17)

1kpik2 1T

min
Pi(L)

+

F

(13)

7319

The above subproblems are convex and according to the
ﬁrst-order optimality condition we have

2λPi(L)Mi + ρPi(L) − ρPi(D) = 0,

(14)
where, for ease of presentation, we denote D = X−S +Θ/ρ,
and Mi = Ikpik2 − 1
. Hence, (14) leads to
the soluation of Pi(L):

1kpik2 1T

kpik2

kpik2

Pi(L) = ρ(2λMi + ρIkpik2 )−1Pi(D).

(15)

It is seen that (15) requires matrix inversion, which, unfor-
tunately, has a time complexity of O(n3) in general. To
avoid matrix inversion, we re-write this matrix to simplify
(15):

It is seen that

d(pi)1n1T

1n1T

c

c

Tr(cid:16)Ld(pi)(cid:16)In −
Tr(cid:16)L(cid:16)d(pi) −
kL(cid:16)d(pi) −
kLj −

Xi=1
Xi=1
Xi=1
Xi=1 X(pi)j =1

1
kpik2
1
kpik2
1
kpik2
1
kpik2 X(pi)j =1

c

c

d(pi)1n1T

n(cid:17)d(pi)LT(cid:17)
n d(pi)(cid:17)LT(cid:17)
n d(pi)(cid:17)k2
Ljk2
2,

F

(20)

=

=

=

1kpik2 1T

kpik2 .

where (pi)j denotes the j-th element of pi. Hence, the pi-
subproblems can be converted to

2λMi + ρIkpik2 = (2λ + ρ)Ikpik2−

(16)
It is notable that due to the special structure of (16) its inver-
sion has a simple analytic expression by using the Sherman-
Morrison-Woodbury formula:

2λ
kpik2

(cid:18)(2λ + ρ)Ikpik2 + (−

2λ
n

1kpik2 )1T

kpik2(cid:19)−1

2λ + ρ

Ikpik2
2λ+ρ Ikpik2 (− 2λ

1

1 + 1T

kpik2

1

−
1

=

=

Ikpik2 +

2λ + ρ

kpik2ρ(2λ + ρ)

kpik2

n 1kpik2 )1T
2λ+ρ Ikpik2 (− 2λ

1

2λ

1

2λ+ρ Ikpik2

n 1kpik2 )

1kpik2 1T

kpik2

Hence, it is apparent that that (15) can be written as follows:

Pi(L) =  ρ

2λ + ρ

Ikpik2

+

2λ

kpik2(2λ + ρ)

1kpik2 1T

kpik2!Pi(D)

(18)

ρ

=

2λ + ρPi(D)

2λ

+

kpik2(2λ + ρ)

(1kpik2 (1T

kpik2Pi(D))),

which has a linear complexity in both n and d by exploiting
matrix-vector multiplications. L can be obtained accord-
ingly after obtaining all Pi(L), for i = 1, 2,··· , c.
4.2. pi minimization

The subproblem associated with pi-minimization is

given as follows:

c

min
pi

Xi=1

Tr(cid:16)Ld(pi)(cid:16)In −
s.t. pi ∈ {0, 1}n, Xi

1
kpik2
pi = 1n.

1n1T

n(cid:17)d(pi)LT(cid:17)

c

min
pi

Xi=1 X(pi)j =1

kLj −
s.t. pi ∈ {0, 1}n, Xi

1

kpik2 X(pi)j =1

pi = 1n,

Ljk2

2

(21)

which is simply the standard K-means problem. This is
surprising in that we only need to perform K-means to L
and then the optimal [p1,··· , pc] ∈ {0, 1}n×c simply cor-

responds to the group indicator matrix:

(17)

[p1,··· , pc] ← K-means(L, c).

(22)

It should be noted that with its current form, (21) is solved
by K-means [20]. However, more general clustering meth-
ods can be also applicable if we consider solving pi as a
clustering rather than optimization problem. For example, if
we consider nonlinear clustering algorithms, such as spec-
tral clustering, the recovered L and p actually reﬂect non-
linear structures of the data, which can be treated as a direct
nonlinear extension of our method to account for nonlinear
relationships of the data.

4.3. S minimization

The S-subproblem is

min

S

1
ρkSk1 +

1
2kX − L − S + Θ/ρk2
F ,

(23)

which is solved using the soft-thresholding operator [3, 8]:

Sij = (|Bij| − 1/ρ)+ sign (Bij) ,

(24)

where B = X − L + Θ/ρ.
4.4. Θ, ρ updating

For the updating of Θ and ρ, we follow a standard ap-

proach in ALM framework:

(19)

7320

Θ = Θ + ρ(X − L − S),
ρ = ρκ,

(25)

Table 1. Description of Video Sequence Data Sets

Table 2. Results of Foreground-Background Separation

Data Set

data size

# of backgrounds

Highway

Bootstrap

Hall Airport

Shopping Mall

Escalator Airport

130×160 × 3,417
144×176 × 3,584
120×160 × 2,055
256×320 × 1,286
240×320 × 1,700
128×160 × 1,546
Camera Parameter 240×320 × 5,001
120×160 × 2,800
Light Switch-1
120×160 × 2,715
Light Switch-2

Lobby

1
1
1
1
1

2
2
2
2

where κ > 1 is a parameter that controls the increasing
speed of ρ.

Regarding the complexity of the above optimization pro-
cedure, it should be noted that each step requires O(nd)
complexity and typically ALM converges in a ﬁnite num-
ber of steps [4], thus the overall complexity of our method
is O(nd).

5. Experiments

In this section, we evaluate the proposed method
in comparison with several current state-of-the-art algo-
rithms, including variational Bayesian RPCA (VBRPCA)
[10], IALM for convex RPCA [6], AltProj [19], NSA
[1], and PCP [28].
In particular, we follow [13, 21]
and evaluate RES-PCA in three applications,
including
foreground-background separation from video sequences,
shadow removal from face images, and anamoly detec-
tion from hand-written digits. All
these experiments
are conducted under Ubuntu system with 12 Intel(R)
Xeon(R) W-2133 CPR 3.60GHz. All algorithms are ter-
minated if a maximum of 500 iterations is reached or
} ≤ 0.001 is

max{ kX−Lt−StkF

, kkSt+1−StkF

, kLt+1−LtkF

kXkF

kXkF

kXkF

satisﬁed.

5.1. Foreground Background Seperation

Foreground-background separation is to detect moving
objects or interesting activities in a scene, and remove back-
ground(s) from a video sequence. The background(s) and
moving objects correspond to the low-rank and sparse parts,
respectively. For this task, we use 9 datasets, whose char-
acteristics are summarized in Table 1. Among these video
datasets, the ﬁrst 5 contain a single background while the
remaining sequences have 2 backgrounds.

For the parameters, we set

For
IALM, we use the theoretically optimal balancing param-
. The same balancing parameter is used for
eter

them as follows.

1√max (n,d)

PCP and NSA as suggested in the original papers. For fair

comparison, we usepmax (n, d) for the proposed method.

For AltProj, we specify the ground truth rank; for VBR-

Data

Boot-
strap

Escala-
tor
Airport

Hall
Airport

High-
way

Shop-
ping
Mall

Method

AltProj

NSA

VBRPCA

IALM
PCP

RES-PCA

AltProj

NSA

VBRPCA

IALM
PCP

RES-PCA

AltProj

NSA

VBRPCA

IALM
PCP

RES-PCA

AltProj

NSA

VBRPCA

IALM
PCP

RES-PCA

AltProj

NSA

VBRPCA

IALM
PCP

RES-PCA

AltProj

NSA

Lobby

VBRPCA

IALM
PCP

RES-PCA

AltProj

NSA

VBRPCA

IALM
PCP

RES-PCA

AltProj

NSA

VBRPCA

IALM
PCP

RES-PCA

AltProj

NSA

VBRPCA

IALM
PCP

RES-PCA

Camera
Parameter

Light
Switch-
1

Light
Switch-
2

1

843

Rank(L) kSk0/(dn)
0.9397
0.7944
1.0000
0.8003
0.7859
0.9379

782
1174

1

1

1

1016

1

1065
1232

1

1

948

1

974
1292

1

1

166

1

357
531

1

1

174

1

151
290

1

2

161

2

104
502

2

——
——

1

1123
——

2

2

541

1

415
848

2

2

486

1

333
985

2

0.8987
0.6390
0.9839
0.6482
0.6670
0.8898

0.9573
0.7489
1.0000
0.6917
0.7055
0.9302

0.8846
0.9732
1.0000
0.7980
0.8440
0.9340

0.8907
0.9372
1.0000
0.8457
0.8898
0.9208

0.88.97
0.8073
1.0000
0.8229
0.8500
0.8963

——
——
1.0000
0.7020
——
0.8305

0.90.84
0.6559
1.0000
0.6298
0.6776
0.9708

0.8078
0.8041
1.0000
0.7815
0.8337
0.8608

kXkF

kX−L−SkF
4.22e-4
5.87e-4
9.90e-4
6.11e-4
3.45e-4
7.81e-4

3.86e-4
8.09e-4
9.76e-4
6.95e-4
3.59e-4
5.77e-4

1.69e-5
4.89e-4
9.90e-4
7.37e-4
4.27e-4
5.82e-4

4.63e-4
0.87e-4
9.87e-4
6.25e-4
2.27e-4
7.20e-4

8.12e-4
1.57e-4
9.92e-4
6.25e-4
2.85e-4
7.94e-4

3.77e-4
6.13e-4
9.92e-4
5.66e-4
2.59e-4
1.83e-4

——
——

9.95e-4
7.81e-4

——

2.48e-4

4.21e-4
5.87e-4
9.83e-4
9.21e-4
5.91e-4
4.15e-4

9.01e-4
4.90e-4
9.93e-4
7.79e-4
2.68e-4
2.82e-4

# of Iter.

Time

36
12
175
15
94
23

33
12
134
15
93
23

37
13
152
14
77
23

27
15
126
15
152
23

30
14
157
14
165
23

26
13
111
15
92
25

——
——
171
16

——

25

48
13
165
14
85
23

37
14
150
15
154
25

68.61

1343.22
186.90
1356.04
571.75
16.73

69.34

1793.35
168.01
1325.40
727.65
20.47

93.62

2189.99
240.17
2024.10
744.28
26.38

119.17
1238.95
287.27
1409.10
1013.00

35.32

85.92

1027.45
295.00
498.65
790.30
28.44

21.58
182.50
69.47
168.22
166.79
20.11

——
——

1108.20
9297.40

——
303.57

73.54
687.19
151.05
496.92
410.39
31.68

44.34
846.81
141.21
616.28
756.34
33.71

We set the rank to be the minimal number of singular values that contribute more

than 99.5% information to avoid the noise effect of small singular values.

“——” presents an “out of memory” issue.

PCA, we use the ground truth rank as its initial rank pa-
rameter. For fair comparison, we set c to be ground truth
rank for RES-PCA. For all methods that relay on ALM-
optimization, we set the parameters to be ρ = 0.0001 and
κ = 1.5. These settings remain the same throughout this
paper unless speciﬁed otherwise.

7321

We show the results in Table 2. It is observed that Alt-
Proj, VBRPCA, and RES-PCA are able to recover the back-
grounds from the video with low rank while IALM, NSA
and PCP with much higher ranks. However, it is noted that
VBRPCA may recover L with ranks lower than the ground
truth. For example, on Light Switch-1, Light Switch-2,
and Camera Parameter data sets, the ground truth rank of
the background is 2 whereas VBRPCA recovers the low
rank parts with rank 1. This may be a potential problem,
as will be clear later on in visual illustration. Although
IALM, NSA amd PCP do not recover L with desired low
ranks, they recovery S more sparsely than AltProj, VBR-
PCA, and RES-PCA. Besides, we observe that the speed of
the proposed method is superior to that of the other meth-
ods. From Table 2, it is observed that the proposed method
is about 3 times faster than AltProj, the second fastest one,
and more than 10 (even about 60 on some data sets) times
faster than IALM. Although the proposed method does not
obtain the smallest errors at convergence on some data, it is
noted that the levels of the errors are well comparable to the
other methods.

It should be noted that for mthods such as IALM, PCP,
and NSA, though they do not recover L with desired low
ranks, it is possible that by tunning their balancing param-
eters they may work well. However, tunning parameter for
unsupervised learning method is usually time consuming.
The proposed method has one balancing parameter, which
has been empirically veriﬁed that the theoretical parameter
as provided in [6] works well. A possible explaination is
that RES-PCA has a close connection and thus enjoies the
same optimal parameter with the convex RPCA. More the-
oretical validation is to be explored in further work.

Moreover, to visually compare the algorithms and illus-
trate the effectiveness of the proposed method, we show
some decomposition results in Figs. 1 and 2. Since IALM,
NSA and PCP cannot recover L with desired low ranks, they
cannot recover the backgrounds well. For example, we can
observe shadows of car on highway in Fig. 1. VBRPCA
reocvers L with ranks lower than the ground truth on some
data sets; consequently, on such data as Light Switch-2 in
Fig. 2 we can see that VBRPCA cannot work well on data
with different backgrounds. AltProj and RES-PCA can sep-
arate the backgrounds and foregrounds well.

To further assess the performance of the proposed
method, we conduct the following experiments to compare
the two methods that have achieved the top performance:
AltProj and RES-PCA. In this test we asume that the ground
truth rank of L is unknown, and we set it to 5 for AltProj and
c = 5 for the proposed method. Some obtained results are
given in Figs. 3 and 4. It is seen that RES-PCA can still
separate the background and foreground well while AltProj
fails. The success of RES-PCA in this kind of scenarios
can be explained as follows: With c greater than the ground

(a) Original

(b) AltProj

(c) Proposed

Figure 3.
Foreground-background separation in the Highway
video when the ground truth rank is unknown and, consequently, c
is speciﬁed to a wrong value. The top left is the original frame and
the rest are extracted background (top) and foreground (bottom).

(a) Original

(b) AltProj

(e) Proposed

Figure 4. Foreground-background separation in the Light Switch-
2 video. Within the two and bottom two rows, the top left is
the original frame and the rest are extracted background (top) and
foreground (bottom), respectively.

truth rank of L, a large group of backgrounds is usually di-
vided into smaller groups such that the backgrounds within
each group still share the same structure; as a consequence,
RES-PCA can still recover the low-rank matrices correctly.
This observation reveals that RES-PCA has superior perfor-
mance to AltProj when the precise knowledge of the ground
truth is unknown a priori.

5.2. Shadow removal from face images

Face recognition is an important topic; however, it is of-
ten plagued by heavy noise and shadows on face images [2].
Therefore, there is a need to handle shadows. In this test,
low-rank methods are used because the (unknown) clean
images reside in a low-rank subspace, corresponding to L,
while the shadows correspond to S. We use the Extended
Yale B (EYaleB) data set for comparative study. EYaleB
data contains face images from 38 persons, among which
we select images of the ﬁrst 2 persons, namely, subject 1
and subject 2. For each there are 64 images of 192 × 168

7322

(a) AltProj

(b) NSA

(c) VBPCA

(d) IALM

(e) PCP

(f) RES-PCA

Figure 1. Foreground-background separation in the Highway video. The top to the bottom are the original frame, extracted background,
and foreground, respectively.

(a) AltProj

(b) NSA

(c) VBPCA

(d) IALM

(e) PCP

(f) RES-PCA

Figure 2. Foreground-background separation in the Light Switch-2 video. Within the ﬁrst and last 3 lines, the top to bottom are the original
frame, extracted background, and foreground, respectively.

pixels. Following the common approach as in [6, 13], we
construct a data matrix for each person by vectorizing the
images and perform different RPCA algorithms on the ma-
trix. We show some results in Fig. 5 for visual inspection. It
is observed that all methods can successfully remove shad-
ows on subject 2, but some fail on subject 1. The proposed
method removes shadows from face images on both subject
1 and subject 2, which conﬁrms its effectiveness.

5.3. Anomaly Detection

Given a number of images from a subject, they form a
low-dimensional subspace. Those images with stark dif-

ferences from the majority can be regarded as outliers; be-
sides, a few images from another subject are also treated
as outliers. Anomaly detection is to identify such kinds of
outliers from the dominant images. It is modeled that L is
comprised of the dominant images while S captures the out-
liers. For this test, we use USPS data set which consists of
9,298 hand-written digits of size 16 × 16. We follow [13]
and vectorize the ﬁrst 190 images of ‘1’s and the last 10 of
‘7’s to construct a 256 × 200 data matrix. Since the dat set
contains much more ‘1’s than ‘7’s, we regard the former as
the dominant digit while the latter outlier. For visual illus-
tration, we show examples of these digit images in Fig. 6. It

7323

use different sampling ratios in sample size and data dimen-
sion, respectively, to collect its subsets of different sizes.
On each subset, we perform RES-PCA 10 times. From Ta-
ble 2, it is seen that all experiments are terminated within
about 23-25 iterations; hence, in this test we temporarily
ignore the terminating tolerance and terminate the experi-
ment within a reasonable number of iterations, which is set
to be 30. Then we report the average time cost and show the
results in Fig. 9. It is observed that the time cost of RES-
PCA increases linearly in both n and d, which conﬁrms the
scalability of the proposed method.

Figure 9. Time cost with respect to n and d on different data sets
(best viewed in color).

6. Conclusion

Existing RPCA methods typically need to solve SVDs
of large matrices, which generally has at least a quadratic
or even cubic complexity. To combat this drawback, in
this pape we propose a new type of RPCA method. The
new method recovers the low-rank component by exploit-
ing geometrical similarities of the data, without performing
any SVD that current state-of-the-art RPCA methods usu-
ally have to do. We develop an ALM-based optimization
algorithm which is linearly efﬁcient and scalable in both
data dimension and sample size. Extensive experiments in
different applications testify to the effectivenss of the pro-
posed method, in which we observe superior performance
in speed and visual quality to several current state-of-the-
art methods. These observations suggest that the proposed
method is suitable for large-scale data applications in real
world problems.

(a) Original

(b) AltProj

(c) NSA (d) VBPCA (e) IALM (f) PCP

(g) RES-PCA

Figure 5. Shadow removal from face images. Every two rows are
results for different original faces. For each original face, the ﬁrst
row are shadow removed face images, while the second row are
shadow images.

Figure 6. Selected ‘1’s and ‘7’s from USPS dataset.

is observed that all the ‘7’s are outliers. Besides, some ‘1’s
are quite different from the majority, such as the one with
an underline. We apply RES-PCA to this data set and obtain
the separated L and S. In S, those columns corresponding
to outliers have relatively larger values. Following [13], we
use the ℓ2 norm to measure the columns of S and show their
values in Fig. 7, where we have vanished values smaller
than 5 for clearer visualization. Then we show the corre-
sponding digits in Fig. 8, which are the detected outliers. It
is noted that RES-PCA has detected all the ‘7’s as well as
some ‘1’s, such as the one with an underline. This has veri-
ﬁed the effectiveness of RES-PCA in anomaly detection.

Figure 7. Values of kSik2.

Figure 8. Detected outliers from the data set.

5.4. Scalability

Acknowledgement

We have analyzed the scalability of the proposed method
in previous sections. In this test, we empirically verify the
result from our analysis regarding the linearity with n and d
using the data sets in Table 1. For each of these data sets, we

This work is supported by National Natural Science
Foundation of China under grants 61806106, 61802215,
61806045, 61502261, 61572457, and 61379132. C. Chen
and Q. Cheng are corresponding authors.

7324

[17] Ricardo A Maronna, R. Douglas Martin, and Victor J Yohai.
Robust statistics. John Wiley & Sons Ltd Chichester, 2006. 1
[18] Michael McCoy, Joel A Tropp, et al. Two proposals for ro-
bust pca using semideﬁnite programming. Electronic Jour-
nal of Statistics, 5:1123–1160, 2011. 2

[19] Praneeth Netrapalli, UN Niranjan, Sujay Sanghavi, Ani-
mashree Anandkumar, and Prateek Jain. Non-convex robust
pca. In Advances in Neural Information Processing Systems,
pages 1107–1115, 2014. 2, 5

[20] Chong Peng, Zhao Kang, Shuting Cai, and Qiang Cheng.
Integrate and conquer: Double-sided two-dimensional k-
means via integrating of projection and manifold construc-
tion. ACM Trans. Intell. Syst. Technol., 9(5):57:1–57:25,
June 2018. 4

[21] Chong Peng, Zhao Kang, and Qiang Cheng.

A fast
factorization-based approach to robust pca.
In 2016 IEEE
16th International Conference on Data Mining (ICDM),
pages 1137–1142. IEEE, 2016. 5

[22] Chong Peng, Zhao Kang, Huiqing Li, and Qiang Cheng.
Subspace clustering using log-determinant rank approxima-
tion.
In Proceedings of the 21th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Min-
ing, pages 925–934. ACM, 2015. 2

[23] Kim-Chuan Toh and Sangwoon Yun. An accelerated prox-
imal gradient algorithm for nuclear norm regularized lin-
ear least squares problems. Paciﬁc Journal of Optimization,
6(615-640):15, 2010. 1

[24] N. Vaswani, T. Bouwmans, S. Javed, and P. Narayana-
murthy. Robust subspace learning: Robust pca, robust sub-
space tracking, and robust subspace recovery. IEEE Signal
Processing Magazine, 35(4):32–55, July 2018. 1

[25] John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng,
and Yi Ma. Robust principal component analysis: Exact re-
covery of corrupted low-rank matrices via convex optimiza-
tion. In Advances in neural information processing systems,
pages 2080–2088, 2009. 1

[26] Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Ro-
bust pca via outlier pursuit. In Advances in Neural Informa-
tion Processing Systems, pages 2496–2504, 2010. 2

[27] Lei Xu and Alan L Yuille. Robust principal component anal-
ysis by self-organizing rules based on statistical physics ap-
proach. Neural Networks, IEEE Transactions on, 6(1):131–
143, 1995. 1

[28] Zihan Zhou, Xiaodong Li, John Wright, Emmanuel Candes,
and Yi Ma. Stable principal component pursuit.
In Infor-
mation Theory Proceedings (ISIT), 2010 IEEE International
Symposium on, pages 1518–1522. IEEE, 2010. 5

References

[1] Necdet Serhat Aybat, Donald Goldfarb, and Garud Iyengar.
Fast ﬁrst-order methods for stable principal component pur-
suit. arXiv preprint arXiv:1105.2126, 2011. 5

[2] Ronen Basri and David W Jacobs. Lambertian reﬂectance
and linear subspaces. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 25(2):218–233, 2003. 6

[3] Amir Beck and Marc Teboulle. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM
journal on imaging sciences, 2(1):183–202, 2009. 4

[4] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,
Jonathan Eckstein, et al. Distributed optimization and sta-
tistical learning via the alternating direction method of mul-
tipliers. Foundations and Trends R(cid:13) in Machine learning,
3(1):1–122, 2011. 5

[5] Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A
singular value thresholding algorithm for matrix completion.
SIAM Journal on Optimization, 20(4):1956–1982, 2010. 1,
2

[6] Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright.
Robust principal component analysis? Journal of the ACM
(JACM), 58(3):11, 2011. 1, 5, 6, 7

[7] Christophe Croux and Gentiane Haesbroeck. Principal com-
ponent analysis based on robust estimators of the covariance
or correlation matrix:
inﬂuence functions and efﬁciencies.
Biometrika, 87(3):603–618, 2000. 1

[8] Ingrid Daubechies, Michel Defrise, and Christine De Mol.
An iterative thresholding algorithm for linear inverse prob-
lems with a sparsity constraint. Communications on pure
and applied mathematics, 57(11):1413–1457, 2004. 4

[9] Fernando De La Torre and Michael J Black. A framework for
robust subspace learning. International Journal of Computer
Vision, 54(1-3):117–142, 2003. 1

[10] Xinghao Ding, Lihan He, and Lawrence Carin. Bayesian
robust principal component analysis. IEEE Transactions on
Image Processing, 20(12):3419–3430, 2011. 1, 5

[11] Ramanathan Gnanadesikan and John R Kettenring. Robust
estimates, residuals, and outlier detection with multiresponse
data. Biometrics, pages 81–124, 1972. 1

[12] Gene H Golub and Charles F Van Loan. Matrix computa-

tions, volume 3. JHU Press, 2012. 1

[13] Zhao Kang, Chong Peng, and Qiang Cheng. Robust pca via
nonconvex rank approximation.
In Data Mining (ICDM),
2015 IEEE International Conference on, pages 211–220.
IEEE, 2015. 1, 2, 5, 7, 8

[14] Qifa Ke and Takeo Kanade. Robust l1 norm factorization in
the presence of outliers and missing data by alternative con-
vex programming. In Computer Vision and Pattern Recogni-
tion, 2005. CVPR 2005. IEEE Computer Society Conference
on, volume 1, pages 739–746. IEEE, 2005. 1

[15] Wee Kheng Leow, Yuan Cheng, Li Zhang, Terence Sim, and
Lewis Foo. Background recovery by ﬁxed-rank robust prin-
cipal component analysis. In Computer Analysis of Images
and Patterns, pages 54–61. Springer, 2013. 2

[16] Zhouchen Lin, Minming Chen, and Yi Ma. The augmented
lagrange multiplier method for exact recovery of corrupted
low-rank matrices. arXiv preprint arXiv:1009.5055, 2010. 1

7325

