Single-frame Regularization for Temporally Stable CNNs

Gabriel Eilertsen1
Jonas Unger1
1Dept. of Science and Technology, Link¨oping University, Sweden

Rafał K. Mantiuk2

2Dept. of Computer Science and Technology, University of Cambridge, UK
{gabriel.eilertsen, jonas.unger}@liu.se
rafal.mantiuk@cl.cam.ac.uk

Abstract

Convolutional neural networks (CNNs) can model com-
plicated non-linear relations between images. However,
they are notoriously sensitive to small changes in the in-
put. Most CNNs trained to describe image-to-image map-
pings generate temporally unstable results when applied to
video sequences, leading to ﬂickering artifacts and other
inconsistencies over time. In order to use CNNs for video
material, previous methods have relied on estimating dense
frame-to-frame motion information (optical ﬂow) in the
training and/or the inference phase, or by exploring recur-
rent learning structures. We take a different approach to the
problem, posing temporal stability as a regularization of the
cost function. The regularization is formulated to account
for different types of motion that can occur between frames,
so that temporally stable CNNs can be trained without the
need for video material or expensive motion estimation. The
training can be performed as a ﬁne-tuning operation, with-
out architectural modiﬁcations of the CNN. Our evaluation
shows that the training strategy leads to large improvements
in temporal smoothness. Moreover, for small datasets the
regularization can help in boosting the generalization per-
formance to a much larger extent than what is possible with
na¨ıve augmentation strategies.

1. Introduction

Deep neural networks (DNNs) can represent complex
non-linear functions but tend to be very sensitive to the
input. For image data, this is manifested in sensitivity to
small changes in pixel values. For example, techniques
for generating adversarial examples have demonstrated that
there exists images that are visually indistinguishable from
each other, while generating widely different predictions
[32]. It is also possible to ﬁnd naturally occurring image
operations that can cause a convolutional neural network
(CNN) to fail in the learned task [2, 9, 39]. For image-
to-image CNNs applied on video sequences, this sensitiv-
ity results in abrupt and incoherent changes from frame to

frame. Such changes are seen as ﬂickering, or unnatural
movements of local features. Previous methods for apply-
ing CNNs to video material most often use dense motion
information between frames in order to enforce temporal
coherence [29, 13, 6, 24]. This requires ground truth opti-
cal ﬂow for training, modiﬁcations to the CNN architecture,
and computationally expensive training and/or prediction.
Moreover, there are many situations where reliable corre-
spondences between frames cannot be estimated, e.g. due
to occlusion or lack of texture.

Instead of relying on custom architectures, we take a
simple, efﬁcient, and general approach to the problem of
CNN temporal stability. We pose the stability as a regu-
larizing term in the loss function, which potentially can be
applied to any CNN. We formulate two different regular-
izations based on observations of the expected behavior of
temporally varying processing. The result is a light-weight
method for stabilizing CNNs in the temporal domain. It can
be applied through ﬁne-tuning of pre-trained CNN weights
and requires no special-purpose training data or CNN ar-
chitecture. Through extensive experimentation for the ap-
plication in colorization and single-exposure high dynamic
range (HDR) reconstruction, we show the efﬁciency of the
regularization strategies.

In summary, this paper explores regularization for the
purpose of stabilizing CNNs in the temporal domain and
presents the following main contributions:

• Two novel regularization formulations for temporal
stabilization of CNNs, which both model the dynamics
of consecutive frames in video sequences.

• A novel perceptually motivated smoothness metric for

evaluation of the temporal stability.

• An evaluation showing that the proposed training tech-
nique improves temporal stability signiﬁcantly while
maintaining or even increasing the CNN performance.

• For scenarios with limited training data, the general-
ization performance of the regularization strategies is
signiﬁcantly better than traditional data augmentation.

111176

2. Background and previous work

Adversarial examples: Adversarial examples introduce
minor perturbations of an input image, which makes a DNN
classiﬁer to fail [32, 12], also without access to the particu-
lar model [23], and by performing natural image operations
[2, 9, 39]. This points to the large sensitivity to the input
of DNNs and, for image-to-image CNNs, it is manifested
in inconsistent changes between frames when applied to a
video sequence. Our goal is to train for robustness when it
comes to the type of changes that can occur between frames
in video sequences, so that video processed with CNNs can
be expected to be well-behaved. This does not mean that the
CNN will be robust to other types of changes, such as those
created by certain adversarial example generation methods.

Regularization: While there exists a wide range of meth-
ods that classify as regularization [22], we are particularly
interested in those that are designed to address the issue of
neural networks input sensitivity. Depending on the context
and different deﬁnitions, the terms invariance, robustness,
insensitivity, stability, and contraction have been used inter-
changeably in the literature for describing the objective of
such regularization.

The most straightforward method for increasing robust-
ness and generalization is to employ data augmentation.
However, augmentation alone cannot compensate for a
CNN’s sensitivity to transformations of the input [2, 9] or
degradation operations [39].
It would require too much
training data to learn robustness for all transformations,
and will most likely result in under-ﬁtting. An explicit
constraint needs to be enforced to learn a mapping that
is smooth, so that small changes in input yield to small
changes in output. This concept has been explored in a va-
riety of formulations, e.g. by means of weight decay [21],
weight smoothing [19], label smoothing [38], or penalizing
the norm of the output derivative with respect to the net-
work weights [14]. Of particular interest to our problem are
methods that regularize by penalizing the norm of the Jaco-
bian with respect to the input [28, 39]. For example, Zheng
et al. [39] apply noise perturbations to the input images, and
construct a regularization term that contracts the prediction
of clean and noisy samples, resulting in an increased robust-
ness to image degradation.

While the aforementioned works mostly deal with classi-
ﬁcation, we show that the same reasoning is true for image-
to-image CNNs applied to video sequences — we cannot
simply train a CNN on separate video frames, or trans-
formed images by means of augmentation, and expect a ro-
bust behavior for temporal variations. Therefore, we formu-
late different regularization strategies particularly for train-
ing CNNs for video applications, and perform a study on
which is most efﬁcient for achieving temporal stability.

Temporal consistency: Methods for enforcing temporal
consistency in image processing are mostly based on es-
timating dense motion, or optical ﬂow, between frames
[26, 4, 7, 37]. This is also the case for previous work in tem-
porally consistent CNNs. For example, ﬂow-based methods
have been suggested for video style transfer [29, 13], video-
to-video operations by means of generative adversarial net-
works (GANs) [34], and for imposing temporal consistency
as a post-processing operation [24].

Another direction for video inference using neural net-
works is to employ recurrent learning structures, such as the
long short-term memory (LSTM) networks [15]. For image
data, CNNs have been constructed for recurrence using the
ConvLSTM [36] and its variants [20], which have been used
e.g. in video super-resolution [33] and video concistency
methods [24]. However, mostly these structures have been
explored in classiﬁcation and understanding. There are also
other recurrent or multi-frame based structures that have
been used for image-to-image applications, e.g. for video
super-resolution [16, 5], de-blurring [31], and different ap-
plications of GANs [34].

The ﬂow-based and recurrent methods all suffer from
one or more of the following problems: 1) high complex-
ity and application speciﬁc architectural modiﬁcations, 2)
need for special-purpose training data such as video frames
and motion information, 3) a signiﬁcant increase in compu-
tational complexity for training and/or inference, 4) failure
in situations where motion estimation is difﬁcult, such as
image regions with occlusion or lack of texture. The strat-
egy we propose handles all these limitations.
It is light-
weight, can be applied to any image-to-image CNN without
changes, and does not require video material or motion es-
timation. At the same time, it offers great improvements in
temporal stability without impeding the reconstruction per-
formance.

3. Temporal regularization

We consider supervised training of image-to-image

CNNs, with the total loss formulated as:

L = (1 − α)Lrec + αLreg.

(1)

The ﬁrst term is the main objective of the CNN, which pro-
motes reconstruction of ground truth images y from the in-
put images x. Given an arbitrary CNN that has been trained
with the loss Lrec, adding the term Lreg is the only modiﬁ-
cation we make in order to adapt a CNN for video material.
The scalar α is used to control the strength of the regular-
ization objective.

This section presents three different regularization strate-
gies, Lreg in Equation 1, for improving temporal stability of
CNNs. The ﬁrst was introduced by Zheng et al. [39], while
the two others are novel deﬁnitions that are speciﬁcally de-
signed to account for frame-to-frame changes in video. All

11177

three strategies rely on performing perturbations of the in-
put image, and a key aspect is to model these as common
transformations that occur in natural video sequences.

3.1. Stability regularization

The most similar to our work is the stability training pre-
sented by Zheng et al. [39]. Given an input image x, and a
variant of it with a small perturbation T (x) = x + ∆x, the
regularization term is formulated to make the prediction of
both images possibly similar. For an image-to-image map-
ping f , we can apply the term directly on the output image,

Lstability = ||f (x) − f (T (x))||2.

(2)

While different distance measures can be used, we only
consider the ℓ2 norm for simplicity. The perturbation ∆x
is described as per-pixel independent normally distributed
noise, ∆x ∼ N (0, Σ), with Σ = σ2I.

3.2. Transform invariance regularization

The typical measure of temporal incoherence [26, 4] is

formulated using two consecutive frames yt−1 and yt,

E = ||yt − W (yt−1)||2,

(3)

where W describes a warping operation from frame t−1 to t
using the optical ﬂow ﬁeld between the two frames. If there
are frame-to-frame changes that cannot be explained by the
ﬂow ﬁeld motion, these are registered as inconsistencies.

In order to use this measure for regularization, without
requiring video data or optical ﬂow information, we in-
troduce within-frame warping with a geometric transfor-
mation, W (x) = T (x) (the transformation is described
in more detail in Section 3.4). Then, x and T (x) mimic
two consecutive frames, which are used to infer f (x) and
f (T (x)). If these are temporally consistent, performing the
warping to register the two frames should yield the same re-
sult, either comparing f (x) to T −1(f (T (x))) or comparing
T (f (x)) to f (T (x)). This results in the regularization term

Ltrans-inv = ||f (T (x)) − T (f (x))||2 .

(4)

Note that this loss is fundamentally different from the stan-
dard reconstruction loss for an augmented patch:

Laugment = ||f (T (x)) − T (y)||2 .

(5)

While Laugment promotes an accurate reconstruction with
respect to an augmented (transformed) sample, Ltrans-inv
promotes the reconstruction that is consistent with a trans-
formation, but not necessarily accurate. If there is an er-
ror in the reconstruction, Laugment will minimize that er-
ror in the transformed (augmented) patch, potentially at the
cost of consistency, while Ltrans-inv will ensure that any
error is consistent between the original and the transformed
patches.

3.3. Sparse Jacobian regularization

Supervised learning typically relies on ﬁtting a function
to a number of training points without considering what is
the function behavior in the neighborhood of those points. It
would be arguably more desirable to provide to the training
not only the function values, but also the information about
partial derivatives in a form of a Jacobian of that function at
a given point. However, for typical image-to-image CNNs,
using a full Jacobian matrix would be impractical: if 32×32
patches are used, we need to train f : R1024→R1024 and
the Jacobian has over a million elements. However, we are
going to demonstrate that even if we use a sparse estimate of
the Jacobian and sample just a few random directions of our
input space, we can much improve stability and accuracy of
the predictions.

By providing sparse information on the Jacobian, we can
also infuse domain expertise into our training. In the case
of image-to-image mapping, we know that an input patch
transformed by translation, rotation and scaling, should re-
sult in a transformed output patch. Each of those transfor-
mations maps to a vector change in the input and output
space, for which we can numerically estimate partial deriva-
tives. That is, we want the partial derivatives of the trained
function f to be possibly close to those of the ground truth
output patches:

f (x + ∆x) − f (x)

∆x

≈

y(x + ∆x) − y(x)

∆x

,

(6)

where ∆x represents the effect of one of the transformations
on the input space, y(x) is the output patch from the training
set corresponding to x, and y(x + ∆x) is the transformed
output patch. For the consistency of notation, we deﬁne
T (x) = x + ∆x and T (y) = y(x + ∆x), so that we can
formulate a regularization term as:

Ljacobian = || (f (T (x)) − f (x)) − (T (y) − y) ||2

= || (f (T (x)) − T (y)) − (f (x) − y) ||2 .

(7)

(8)

Although the term may look similar to Laugment from
Equation 5, Ljacobian promotes consistency rather than ac-
curacy: the loss is minimized when the prediction error for
the transformed patches is similar to the prediction error for
the original patches.

3.4. Transformation speciﬁcation

The perturbation function T (·) in all of the introduced
regularization terms rely on a transformation of the input
image. For our purpose, this should capture the possible
motion that can occur between frames in a video sequence.
We make use of simple geometric transformations in or-
der to accomplish this. These include translation, rotation,
zooming, and shearing, which all can be described in a 2×3

11178

Table 1. Ranges of transformation parameters.

Table 2. CNN training setups used in the evaluation experiments.

Parameter

Translation
Rotation
Zoom
Shearing

Min

-2 px
-1◦
0.97×
-1◦

Max

2 px
1◦
1.03×
1◦

transformation matrix that transforms the indices of the im-
age x (see supplementary material for an exact formula-
tion). The matrix is randomly speciﬁed for each image,
with transformation parameters drawn from uniform distri-
butions in a selected range of values as speciﬁed in Table 1.
Although motion can occur on a more local level in real
videos, we argue that the transformations can make for good
regularization. It is important to note that we do not train the
network to predict the transformation or transformed patch,
which would arguably require training on local transforma-
tions. Instead, we train to produce an image in which pixels
do not shift and which is consistent with the input in the
presence of any transformation, both local or global. We
argue that the type of motion (global/local) is in this case
less relevant as long as the regularization term pushes the
trained model towards predicting consistent results.

3.5. Implementation

While it is possible to train for a loss function with one
of the regularization terms from scratch, we instead start
with a pre-trained network and include the regularization
in a second training stage for ﬁne-tuning. We found that
ﬁne-tuning makes training convergence more stable while
providing the same gain in temporal consistency as train-
ing from scratch. Another very important advantage is that
ﬁne-tuning can be applied to already optimized large-scale
CNNs, which take long time to train.

For each regularization method, we follow the exact
same loss evaluation scheme. The perturbed sample’s co-
ordinates are transformed as described in Section 3.4, with
randomly selected transformation parameters. Both the
original and the transformed sample, x and T (x), respec-
tively, are taken through the CNN by the means of a weight-
sharing (siamese) architecture. This gives us f (x) and
f (T (x)), which can be used with the three different regular-
ization deﬁnitions, Equation 2, 4, and 8, by complementing
with the transformations T (f (x)) and T (y).

4. Experiments

We evaluate the novel temporal CNN stabilization/regu-
larization techniques using two different applications: col-
orization of grayscale images and HDR reconstruction from
single-exposure images. These tasks were selected as they
are different in nature, and rely on different CNN architec-
tures. While colorization attempts to infer colors over the
complete image, the HDR reconstruction tries to recover

Colorization

HDR reconstruction

Autoencoder [17] Autoencoder [8]
Strided conv.
Resize + conv.

Architecture
Down-sampling
Up-sampling
Skip-connections No
Weights
Training data
Resolution
Training size
Epochs
Training time

1,568,698
CelebA [27]
128 × 128
20,000
50
≈35m

Max-pooling
Transposed conv.
Yes
1,289,653
Procedural images
128 × 128
10,000
50
≈20m

local pixel information that have been lost due to sensor
saturation.The colorization CNN uses the same design as
described by Iizuka et al. [17], but without the global fea-
tures network and with fewer weights. It implements an au-
toencoder architecture, with strided convolution for down-
sampling, and nearest neighbor resizing followed by con-
volution for up-sampling. The HDR reconstruction CNN
uses the same design as described by Eilertsen et al. [8], but
with fewer weights. This is also an autoencoder architec-
ture, but implemented using max-pooling and transposed
convolution, and it has skip-connections between encoder
and decoder networks. More details on the CNNs and train-
ing setups are listed in Table 2.

In order to be able to explore a broad range of hyper-
parameters, we use datasets that are restricted to speciﬁc
problems. For colorization, we only learn the task for close-
up face shots. For the HDR reconstruction, we restrict the
task to a simple procedural HDR animation.

Training data for the colorization task is 20,000 images
from the CelebA dataset [27]. For testing, we use 72 video
sequences from the YouTube Faces dataset [35]. These have
been selected to show close-up faces in order to be more
similar to the training data, and are cut to be between 50 −
200 frames long. Figure 1 shows an example of a test frame.
Training data for the HDR reconstruction task is 10,000
frames that have been generated in a completely procedural
manner. These contain a random selection of image fea-
tures with different amount of saturated pixels. The fea-
tures move in random patterns and are sometimes occluded
by randomly placed beams. For the training data we only
use static images, with no movement, and for the test data
we include motion to evaluate the temporal behavior. The
test set consists of 50 sequences, 200 frames each. Figure 2
shows an example of a test video frame.

4.1. Performance measures

The goal of the proposed regularization strategies is to
achieve temporally stable results while maintaining the re-
construction performance.
In order to evaluate whether
both goals are achieved, we measure reconstruction perfor-
mance by means of PSNR and introduce a new measure of

11179

Input

Prediction

Ground truth

Figure 1. Colorization test sample, from YouTube Faces [35].

Input

Prediction

Ground truth

Figure 2. HDR reconstruction test sample, from procedural HDR
video dataset. The image is displayed at a reduced exposure in
order to show differences in saturated regions.

smoothness over time. Our measure computes the ratio of
high temporal frequencies between the reference and recon-
structed video sequences. We ﬁrst extract the energy of the
high temporal frequency component from both sequences,

D(f (x))i,j,t = |f (x)i,j,t − (Gσ ∗ f (x))i,j,t|2 ,

(9)

where the convolution with the Gaussian ﬁlter Gσ is per-
formed in the temporal dimension t. The parameter σ is
selected to eliminate the low frequency components that
the eye is insensitive to, but which carry high energy. Fig-
ure 3 shows the spatio-temporal contrast sensitivity func-
tion of the visual system and the high-pass ﬁlter we use with
σ = 0.15 seconds. The smoothness is computed as the ratio
of the sum of the ground truth and the reconstruction video
energies,

S =s Pi,j,t D(y)i,j,t
Pi,j,t D(f (x)i,j,t)

.

(10)

If S < 1, the reconstructed video is less smooth than the
ground truth video and the opposite can be said for S > 1.

4.2. Experimental setup

We ﬁne-tune the CNNs in Table 2 for the two applica-
tions, and run a large number of trainings in order to sample
the performance at different settings. For the total loss in
Equation 1, we compare the three different regularization
formulations: stability (2), sparse Jacobian (8), and trans-
form invariance (4). These are evaluated using the transfor-
mation described in Section 3.4. For the stability regular-
ization we also include a setting with noise perturbations,
T (x) = x + ∆x, with ∆x ∼ N (0, σ2I), in order to com-
pare to previous work. We choose different σ for each im-
age, drawn from a uniform distribution, σ ∼ U(0.01, 0.04).

Figure 3. Colored lines: Spatio-temporal contrast sensitivity func-
tion for different spatial frequencies ρ in cycles per visual degree,
based on the model from [25]. Dashed-black line: the high-pass
ﬁlter used for our smoothness measure.

Finally, we also include trainings that use traditional aug-
mentation by means of the transformation. For each of the
aforementioned setups, we then run 10 individual trainings
in order to estimate the mean and standard deviation of each
datapoint.

We also experimented with incorporating the reconstruc-
tion loss of the transformed sample, Equation 5, but mostly
this degraded the performance, possibly due to under-
ﬁtting.

4.3. Results

The results of the experiments can be found in Fig-
ure 4 for colorization and in Figure 5 for HDR reconstruc-
tion. The baseline condition uses the pre-trained model be-
fore ﬁne-tuning and without regularization. The PSNR and
smoothness measures have been calculated on the a and b
channels of the CIE Lab color space for the colorization
application and only in saturated pixels for the HDR recon-
struction application. Such modiﬁed measures can better
capture small differences.

In both experiments we can observe signiﬁcant improve-
ments in both PSNR and smoothness for all regularization
strategies. However, the stability that relies on noise per-
forms visibly worse in both experiments than the same reg-
ularization but based on transformations. Transform invari-
ance and sparse Jacobian regularizations result in higher
PSNR and visually better reconstruction than the stability
regularization (refer to the video material). Although the
stability formulation can generate smoother video for HDR
reconstruction, this is at the cost of very high reconstruc-
tion error, and for α > 0.99 it most often learns the identity
mapping, f = x. The performance of the two novel for-
mulations are comparable. The sparse Jacobian results in
a slightly higher PSNR for HDR reconstruction and trans-
form invariance results in higher smoothness. The sparse
Jacobian also seems to be more robust to the choice of
the regularization strength. The traditional augmentation
using the transformations (the blue-dashed line) can im-
prove smoothness and PSNR but the improvement is much
smaller than the other regularization strategies.

11180

0102030405060Temporal frequency [Hz]101102Sensitivity=1=2=4=8Figure 4. Colorization performance, evaluated using PSNR (top)
and smoothness (bottom). The datapoints are estimated as the
mean over 10 individual trainings, and the shaded regions illustrate
the standard deviation. The Baseline condition is hidden under the
dashed-blue Augmentation line in the bottom plot.

In summary, the experiments give us a good indication
of the large improvements in temporal stability for widely
different applications that can be achieved from explicitly
regularizing for this objective. However, differentiating be-
tween the two proposed formulations is more difﬁcult, and
could potentially be application dependent. Finally, we
have large improvements in PSNR for our scenarios with
limited training data, indicating that the proposed regular-
ization strategies can improve generalization performance.

5. Example applications

In this section we demonstrate that the proposed regu-
larization terms improve the results not only for the lim-
ited scenarios in Section 4, but also for large-scale problems
trained on large amounts of data.

5.1. Colorization

For this application, we start from the architecture used
by Iizuka et al. [17]. However, we skip the global fea-
tures network and replace the encoder part of the CNN with
the convolutional layers from VGG16 [30].
In this way,
we can initialize the encoder using pre-trained weights for
classiﬁcation. This setup resulted in a signiﬁcant improve-
ment in the performance as compared to using the origi-
nal encoder design. In total, the network is speciﬁed from

Figure 5.
HDR reconstruction performance, evaluated using
PSNR (top) and smoothness (bottom). The notation is the same
as in Figure 4.

∼19M weights. We train it on the Places dataset [40],
and use weights pre-trained for classiﬁcation on the same
dataset. We remove from training around 5% of the images
that showed the least color saturation. The CNN was then
trained for ∼15 epochs on the remaining ∼2.1M images, at
a resolution of 224 × 224 pixels.

We ﬁne-tune the colorization CNN using two proposed
regularization strategies. The effect of the ﬁne-tuning is
measured in terms of PSNR and the smoothness measure,
see Table 3. The table also includes a ﬁne-tuning with-
out regularization for comparison, and processing the base-
line output using the method by Lai et al. [24]. Over-
all, the regularizations offer slight improvements in PSNR
(around 0.3−0.5dB) while increasing smoothness substan-
tially. This also goes for comparison to the ﬂow-based post-
processing network by Lai et al. The transform invariance
formulation with α = 0.95 gives the best smoothness, and
with a PSNR close to the other regularization settings.

Examples of the impact of the regularization techniques
are demonstrated in Figure 6. The baseline CNN can ex-
hibit large frame to frame differences, which is much less
likely after performing the regularized training. Also, there
is an overall increase in the reconstruction performance —
whereas the baseline has a tendency to fail in many of the
frames, this is less likely to happen when accounting for the
differences between frames in the loss evaluation. For ex-
ample, in the bottom example of Figure 6 the pixel values

11181

0.0000.3330.6670.8890.9700.9920.998Regularization strength131415161718PSNR [dB]BaselineAugmentationStability, noiseStability, transformSparse JacobianTransform invariance0.0000.3330.6670.8890.9700.9920.998Regularization strength100101SmoothnessBaselineAugmentationStability, noiseStability, transformSparse JacobianTransform invariance0.0000.3330.6670.8890.9700.9920.998Regularization strength1213141516171819PSNR [dB]BaselineAugmentationStability, noiseStability, transformSparse JacobianTransform invariance0.0000.3330.6670.8890.9700.9920.998Regularization strength100SmoothnessBaselineAugmentationStability, noiseStability, transformSparse JacobianTransform invarianceFigure 6. Two video colorization examples from the YouTube-8M dataset [1]. On the left there are two consecutive frames displayed for
each sequence, comparing baseline to the two video regularization techniques. The plots on the right show the pixel values of the locations
marked in the frames, over a larger range of frames. The values are taken from the a channel of the Lab color space. The vertical dashed
lines indicate where the displayed frames are. The transform invariance regularization has been performed at two strengths, α.

Table 3. Performance after ﬁne-tuning of the colorization CNN.
The measures have been evaluated and averaged over the a and b
channels in the Lab color encoding. Test data are 23 sequences
from the YouTube-8M dataset [1].

Training strategy

PSNR

Smoothness

Baseline
Fine-tuning (no regularization)
Transform invariance, α = 0.95
Transform invariance, α = 0.8
Sparse Jacobian, α = 0.95
Blind video consistency [24]

18.5805
18.4315
18.8880
18.9437
18.8852
18.6086

0.7243
0.6348
2.8934
1.9074
2.5079
1.0287

plotted for the baseline CNN are in many cases close to 0,
and occasionally spike to high values. This problem is alle-
viated by the regularization, resulting in both overall better
reconstruction and smoother changes between frames.

5.2. HDR reconstruction

In this application we employ the CNN that was used by
Eilertsen et al. [8] and initialize it with the trained weights
provided by the authors. The CNN contains in total ∼29M
weights. We perform ﬁne-tuning on a gathered set of ∼2.7K
HDR images from different online resources, which are
used to create a dataset of ∼125K 320 × 320 pixel training
images by means of random cropping and augmentation.

The ﬁne-tuning result is measured by PSNR and smooth-
ness in Table 4, demonstrating a signiﬁcant increase in
smoothness at the cost of a small decrease in PSNR. Com-

Table 4. Performance after ﬁne-tuning of the HDR reconstruction
CNN. The measures have been evaluated and averaged over the
saturated pixels only. Test data are 10 HDR video sequences from
two different sources [10, 3]. The blind video consistency has been
performed on gamma corrected HDR images.

Training strategy

PSNR

Smoothness

Baseline
Fine-tuning (no regularization)
Transform invariance, α = 0.95
Transform invariance, α = 0.8
Sparse Jacobian, α = 0.95
Blind video consistency [24]

25.5131
25.9865
24.1678
25.4374
24.7287
25.3702

5.9951
5.8538
10.6435
8.0798
7.3048
7.2035

pared to the colorization application, regularization of the
HDR reconstruction should be selected at a slightly lower
α in order to not degrade reconstruction performance. The
transform invariance formulation at α = 0.8 only reduces
the reconstruction performance by ∼0.1dB while provid-
ing better smoothness than the sparse Jacobian formulation.
This setting also shows better performance as compared to
the blind video consistency method by Lai et al. [24], both
in terms of PSNR and smoothness.

Figure 7 shows an example of the difference in perfor-
mance for one HDR video sequence. In contrast to the col-
orization application it is difﬁcult to clearly see the differ-
ences between consecutive frames in a side-by-side com-
parison. However, in the video material the differences in
the temporal robustness around saturated image regions are
evident. This can be seen in the pixel plots in Figure 7,

11182

10203040506070Frame05101520Lab a valueBaselineTransform invariance 0.95Transform invariance 0.8Sparse Jacobian 0.95Ground truthBaselineTransform invarianceSparse JacobianFrame 62Frame 63BaselineTransform invarianceSparse JacobianFrame 110Frame 111708090100110120130Frame0246810121416Lab a valueBaselineTransform invariance 0.95Transform invariance 0.8Sparse Jacobian 0.95Ground truthFigure 7. HDR video reconstruction example from the HdM-HDR dataset [10]. On the top left we have an example of the reconstruction
compared to input and ground truth, displayed at a reduced exposure (−3 stops) in order to demonstrate the differences in saturated pixels.
The bottom row displays the absolute difference between two consecutive frames, for an enlarged region of the image and for different
training strategies. The plot on the right shows the HDR luminance values of the pixels marked in the frames, over a larger range of frames.
The vertical dashed lines indicate the frames used for the difference evaluations.

where the regularized results are more stable over time for
the selected saturated pixel. The ﬁgure also shows the ab-
solute difference between two frames for an enlarged image
region, highlighting the improvements achieved from regu-
larization when comparing to the ground truth difference.

investigate how long-term temporal coherence can be en-
forced upon the solution. Finally, it would also be interest-
ing to explore regularization of more complicated loss func-
tions, such as those based on GANs [11], e.g. the pix2pix
[18] CNN or cycle-GANs [41].

6. Limitations and future work

7. Conclusion

Striking the right balance between reconstruction perfor-
mance and smoothness is still an open problem. A small
regularization strength leaves video with temporal artifacts,
whereas a too large strength may risk degrading the recon-
struction performance. Also, the tendency to impair recon-
struction performance with strong regularization could be
in some respect analogous to the reduced sharpness when
L2 norm is used as the loss function in reconstruction prob-
lems (denoising, deconvolution, etc.). We do not address
this problem in our current work, but believe that this can
be alleviated by exploring other regularization loss func-
tions, such as L1, perceptual loss (for color), or by means
of a GAN architecture. The method could also beneﬁt from
combining the reconstruction error and smoothness, for a
better measure of perceived quality. Moreover, although
the transform invariance formulation in some situations can
give a better trade-off between PSNR and smoothness, the
sparse Jacobian formulation tends to be more robust to large
regularization strengths, see e.g. Figure 5.

Our approach optimizes towards short-term temporal
stability without a guarantee for the long-term tempo-
ral consistency. For example, even if colors are consis-
tent in consecutive frames for the colorization application,
they may change inconsistently over a longer sequence of
frames. An interesting area for future work is therefore to

This paper explored how regularization using models of
the problem dynamics can be used to improve the tempo-
ral stability of pixel-to-pixel CNNs in video reconstruction
tasks. We proposed two formulations for temporal regular-
ization, which can be used when training a network from
scratch, or for ﬁne-tuning pre-trained networks. The strat-
egy is light-weight, it can be used without architectural
modiﬁcations of the CNN, and it does not require video or
motion information for training.
It avoids the costly and
often inaccurate estimation of optical ﬂow, inherent to pre-
vious stabilization methods. Our experiments showed that
the proposed approach leads to substantial improvements
in temporal stability while maintaining the reconstruction
performance. Moreover, for some situations, and especially
when training data is limited, the regularization can also im-
prove the reconstruction performance of the CNN, and to
a much larger extent than what is possible with traditional
augmentation techniques.

Acknowledgments This project was supported by the Wallen-
berg Autonomous Systems and Software Program (WASP), the
strategic research environment ELLIIT, and has received funding
from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme (grant
agreement n◦ 725253–EyeCode).

11183

10203040506070Frame0102030405060708090100Relative luminanceInputBaselineTransform invariance 0.95Transform invariance 0.8Sparse Jacobian 0.95Ground truthInputGround truthTransform invariance (0.95)Frame 31Frame 31 (enlarged)|Frame 31 - Frame 30|InputReconstruction (baseline)Ground truthBaselineSparse Jacobian (0.95)References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. YouTube-8M: A
large-scale video classiﬁcation benchmark. arXiv preprint
arXiv:1609.08675, 2016. 7

[2] A. Azulay and Y. Weiss. Why do deep convolutional net-
works generalize so poorly to small image transformations?
arXiv preprint arXiv:1805.12177, 2018. 1, 2

[3] A. Banitalebi-Dehkordi, M. Azimi, M. T. Pourazad, and
P. Nasiopoulos. Compression of high dynamic range video
using the HEVC and H. 264/AVC standards. In Proceedings
of International Conference on Heterogeneous Networking
for Quality, Reliability, Security and Robustness (QShine
2014), pages 8–12. IEEE, 2014. 7

[4] N. Bonneel, J. Tompkin, K. Sunkavalli, D. Sun, S. Paris, and
H. Pﬁster. Blind video temporal consistency. ACM Transac-
tions on Graphics, 34(6):196:1–196:9, 2015. 2, 3

[5] J. Caballero, C. Ledig, A. Aitken, A. Acosta, J. Totz,
Z. Wang, and W. Shi. Real-time video super-resolution with
spatio-temporal networks and motion compensation. In Pro-
ceedings of IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR 2017), 2017. 2

[6] D. Chen, J. Liao, L. Yuan, N. Yu, and G. Hua. Coherent
online video style transfer. In Proceedings of IEEE Interna-
tional Conference on Computer Vision (ICCV 2017), 2017.
1

[7] X. Dong, B. Bonev, Y. Zhu, and A. L. Yuille. Region-based
temporally consistent video post-processing. In Proceedings
of IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR 2015), 2015. 2

[8] G. Eilertsen, J. Kronander, G. Denes, R. K. Mantiuk, and
J. Unger. HDR image reconstruction from a single exposure
using deep CNNs. ACM Transactions on Graphics (TOG),
36(6):178, 2017. 4, 7

[9] L. Engstrom, D. Tsipras, L. Schmidt, and A. Madry. A ro-
tation and a translation sufﬁce: Fooling CNNs with simple
transformations. arXiv preprint arXiv:1712.02779, 2017. 1,
2

[10] J. Froehlich, S. Grandinetti, B. Eberhardt, S. Walter,
A. Schilling, and H. Brendel. Creating cinematic wide gamut
HDR-video for the evaluation of tone mapping operators and
HDR-displays. In Proceedings of SPIE, Digital Photography
X, volume 9023, 2014. 7, 8

[11] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Proceedings of International Con-
ference on Neural Information Processing Systems (NIPS
2014), pages 2672–2680, 2014. 8

[12] I. Goodfellow, J. Shlens, and C. Szegedy.

ing and harnessing adversarial examples.
arXiv:1412.6572, 2014. 2

Explain-
arXiv preprint

[13] A. Gupta, J. Johnson, A. Alahi, and L. Fei-Fei. Characteriz-
ing and improving stability in neural style transfer. In Pro-
ceedings of IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR 2017), pages 4067–4076, 2017. 1,
2

[14] S. Hochreiter and J. Schmidhuber. Simplifying neural nets
by discovering ﬂat minima. In Advances in Neural Informa-
tion Processing Systems (NIPS 1995), pages 529–536, 1995.
2

[15] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997. 2

[16] Y. Huang, W. Wang, and L. Wang. Bidirectional recurrent
convolutional networks for multi-frame super-resolution. In
Advances in Neural Information Processing Systems (NIPS
2015), pages 235–243, 2015. 2

[17] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Let there be
color!: Joint end-to-end learning of global and local image
priors for automatic image colorization with simultaneous
classiﬁcation. ACM Transactions on Graphics, 35(4):110:1–
110:11, 2016. 4, 6

[18] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks.
In Pro-
ceedings of IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR 2017), 2017. 8

[19] J. S. Jean and J. Wang. Weight smoothing to improve net-
work generalization. IEEE Transactions on neural networks,
5(5):752–763, 1994. 2

[20] N. Kalchbrenner, A. van den Oord, K. Simonyan, I. Dani-
helka, O. Vinyals, A. Graves, and K. Kavukcuoglu. Video
pixel networks. In Proceedings of International Conference
on Machine Learning (ICML 2017), volume 70, pages 1771–
1779, 2017. 2

[21] A. Krogh and J. A. Hertz. A simple weight decay can im-
In Advances in Neural Information

prove generalization.
Processing Systems (NIPS 1992), pages 950–957, 1992. 2

[22] J. Kukaˇcka, V. Golkov, and D. Cremers.

tion for deep learning: A taxonomy.
arXiv:1710.10686, 2017. 2

Regulariza-
arXiv preprint

[23] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial exam-
ples in the physical world. arXiv preprint arXiv:1607.02533,
2016. 2

[24] W.-S. Lai, J.-B. Huang, O. Wang, E. Shechtman, E. Yumer,
and M.-H. Yang. Learning blind video temporal consistency.
In European Conference on Computer Vision (ECCV 2018),
2018. 1, 2, 6, 7

[25] J. Laird, M. Rosen, J. Pelz, E. Montag, and S. Daly. Spatio-
velocity CSF as a function of retinal velocity using unsta-
bilized stimuli.
In Human Vision and Electronic Imaging,
volume 6057, page 605705, 2006. 5

[26] M. Lang, O. Wang, T. Aydin, A. Smolic, and M. Gross. Prac-
tical temporal consistency for image-based graphics appli-
cations. ACM Transactions on Graphics, 31(4):34:1–34:8,
2012. 2, 3

[27] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face
attributes in the wild. In Proceedings of IEEE International
Conference on Computer Vision (ICCV 2015), 2015. 4

[28] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio.
Contractive auto-encoders: Explicit invariance during fea-
ture extraction. In Proceedings of International Conference
on Machine Learning (ICML 2011), pages 833–840, 2011. 2
[29] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer
for videos. In German Conference on Pattern Recognition,
pages 26–36. Springer, 2016. 1, 2

11184

[30] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 6

[31] S. Su, M. Delbracio, J. Wang, G. Sapiro, W. Heidrich, and
O. Wang. Deep video deblurring for hand-held cameras.
In Proceedings of IEEE conference on Computer Vision and
Pattern Recognition (CVPR 2017), 2017. 2

[32] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199, 2013. 1, 2

[33] X. Tao, H. Gao, R. Liao, J. Wang, and J. Jia. Detail-revealing
deep video super-resolution. In Proceedings of the IEEE In-
ternational Conference on Computer Vision (ICCV 2017),
pages 22–29, 2017. 2

[34] X. Wei, J. Zhu, S. Feng, and H. Su. Video-to-video trans-
lation with global temporal consistency. In Proceedings of
ACM International Conference on Multimedia (MM 2018),
pages 18–25, 2018. 2

[35] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un-
constrained videos with matched background similarity. In
Proceedings of the IEEE International Conference on Com-
puter Vision (CVPR 2011), pages 529–534. IEEE, 2011. 4,
5

[36] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong,
and W.-c. Woo. Convolutional LSTM network: A ma-
chine learning approach for precipitation nowcasting.
In
Advances in Neural Information Processing Systems (NIPS
2015), pages 802–810, 2015. 2

[37] C.-H. Yao, C.-Y. Chang, and S.-Y. Chien. Occlusion-aware
video temporal consistency. In Proceedings of ACM Inter-
national Conference on Multimedia (MM 2017), pages 777–
785. ACM, 2017. 2

[38] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz.
mixup: Beyond empirical risk minimization.
In Inter-
national Conference on Learning Representations (ICLR
2018), 2018. 2

[39] S. Zheng, Y. Song, T. Leung, and I. Goodfellow. Improving
the robustness of deep neural networks via stability training.
In Proceedings of IEEE conference on Computer Vision and
Pattern Recognition (CVPR 2016), pages 4480–4488, 2016.
1, 2, 3

[40] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using Places
database.
In Proceedings of International Conference on
Neural Information Processing Systems (NIPS 2014), pages
487–495, 2014. 6

[41] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV 2017), 2017. 8

11185

