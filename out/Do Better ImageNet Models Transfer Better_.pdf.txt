Do Better ImageNet Models Transfer Better?

Simon Kornblith∗, Jonathon Shlens, and Quoc V. Le

Google Brain

{skornblith,shlens,qvl}@google.com

Abstract

Transfer learning is a cornerstone of computer vision,
yet little work has been done to evaluate the relationship
between architecture and transfer. An implicit hypothesis
in modern computer vision research is that models that per-
form better on ImageNet necessarily perform better on other
vision tasks. However, this hypothesis has never been sys-
tematically tested. Here, we compare the performance of 16
classiﬁcation networks on 12 image classiﬁcation datasets.
We ﬁnd that, when networks are used as ﬁxed feature ex-
tractors or ﬁne-tuned, there is a strong correlation between
ImageNet accuracy and transfer accuracy (r = 0.99 and
0.96, respectively). In the former setting, we ﬁnd that this re-
lationship is very sensitive to the way in which networks are
trained on ImageNet; many common forms of regularization
slightly improve ImageNet accuracy but yield penultimate
layer features that are much worse for transfer learning.
Additionally, we ﬁnd that, on two small ﬁne-grained image
classiﬁcation datasets, pretraining on ImageNet provides
minimal beneﬁts, indicating the learned features from Ima-
geNet do not transfer well to ﬁne-grained tasks. Together,
our results show that ImageNet architectures generalize well
across datasets, but ImageNet features are less general than
previously suggested.

1. Introduction

The last decade of computer vision research has pur-
sued academic benchmarks as a measure of progress. No
benchmark has been as hotly pursued as ImageNet [16, 58].
Network architectures measured against this dataset have
fueled much progress in computer vision research across
a broad array of problems, including transferring to new
datasets [17, 56], object detection [32], image segmentation
[27, 7] and perceptual metrics of images [35]. An implicit
assumption behind this progress is that network architec-
tures that perform better on ImageNet necessarily perform
better on other vision tasks. Another assumption is that bet-

∗Work done as a member of the Google AI Residency program (g.co/

airesidency).

Figure 1. Transfer learning performance is highly correlated with
ImageNet top-1 accuracy for ﬁxed ImageNet features (left) and
ﬁne-tuning from ImageNet initialization (right). The 16 points in
each plot represent transfer accuracy for 16 distinct CNN architec-
tures, averaged across 12 datasets after logit transformation (see
Section 3). Error bars measure variation in transfer accuracy across
datasets. These plots are replicated in Figure 2 (right).

ter network architectures learn better features that can be
transferred across vision-based tasks. Although previous
studies have provided some evidence for these hypotheses
(e.g. [6, 61, 32, 30, 27]), they have never been systematically
explored across network architectures.

In the present work, we seek to test these hypotheses by in-
vestigating the transferability of both ImageNet features and
ImageNet classiﬁcation architectures. Speciﬁcally, we con-
duct a large-scale study of transfer learning across 16 modern
convolutional neural networks for image classiﬁcation on
12 image classiﬁcation datasets in 3 different experimental
settings: as ﬁxed feature extractors [17, 56], ﬁne-tuned from
ImageNet initialization [1, 24, 6], and trained from random
initialization. Our main contributions are as follows:

• Better ImageNet networks provide better penultimate
layer features for transfer learning with linear classi-
ﬁcation (r = 0.99), and better performance when the
entire network is ﬁne-tuned (r = 0.96).

• Regularizers that improve ImageNet performance are
highly detrimental to the performance of transfer learn-
ing based on penultimate layer features.

• Architectures transfer well across tasks even when

12661

7274767880ImageNet Top-1 Accuracy (%)1.11.21.31.41.51.6Transfer Accuracy (Log Odds)MobileNet v1ResNet-50NASNet LargeInception-ResNet v2Logistic Regression7274767880ImageNet Top-1 Accuracy (%)1.81.92.02.12.22.3MobileNet v1ResNet-50NASNet LargeInception v4Fine-Tunedweights do not. On two small ﬁne-grained classiﬁca-
tion datasets, ﬁne-tuning does not provide a substantial
beneﬁt over training from random initialization, but
better ImageNet architectures nonetheless obtain higher
accuracy.

2. Related work

ImageNet follows in a succession of progressively larger
and more realistic benchmark datasets for computer vision.
Each successive dataset was designed to address perceived
issues with the size and content of previous datasets. Tor-
ralba and Efros [69] showed that many early datasets were
heavily biased, with classiﬁers trained to recognize or clas-
sify objects on those datasets possessing almost no ability to
generalize to images from other datasets.

Early work using convolutional neural networks (CNNs)
for transfer learning extracted ﬁxed features from ImageNet-
trained networks and used these features to train SVMs and
logistic regression classiﬁers for new tasks [17, 56, 6]. These
features could outperform hand-engineered features even for
tasks very distinct from ImageNet classiﬁcation [17, 56]. Fol-
lowing this work, several studies compared the performance
of AlexNet-like CNNs of varying levels of computational
complexity in a transfer learning setting with no ﬁne-tuning.
Chatﬁeld et al. [6] found that, out of three networks, the two
more computationally expensive networks performed better
on PASCAL VOC. Similar work concluded that deeper net-
works produce higher accuracy across many transfer tasks,
but wider networks produce lower accuracy [2]. More recent
evaluation efforts have investigated transfer from modern
CNNs to medical image datasets [51], and transfer of sen-
tence embeddings to language tasks [12].

A substantial body of existing research indicates that, in
image tasks, ﬁne-tuning typically achieves higher accuracy
than classiﬁcation based on ﬁxed features, especially for
larger datasets or datasets with a larger domain mismatch
from the training set [1, 6, 24, 74, 2, 43, 33, 9, 51]. In ob-
ject detection, ImageNet-pretrained networks are used as
backbone models for Faster R-CNN and R-FCN detection
systems [57, 14]. Classiﬁers with higher ImageNet accu-
racy achieve higher overall object detection accuracy [32],
although variability across network architectures is small
compared to variability from other object detection archi-
tecture choices. A parallel story likewise appears in image
segmentation models [7], although it has not been as system-
atically explored.

Several authors have investigated how properties of the
original training dataset affect transfer accuracy. Work exam-
ining the performance of ﬁxed image features drawn from
networks trained on subsets of ImageNet have reached con-
ﬂicting conclusions regarding the importance of the number
of classes vs. number of images per class [33, 2]. Yosinski et
al. [74] showed that the ﬁrst layer of AlexNet can be frozen

when transferring between natural and manmade subsets
of ImageNet without performance impairment, but freezing
later layers produces a substantial drop in accuracy. Other
work has investigated transfer from extremely large image
datasets to ImageNet, demonstrating that transfer learning
can be useful even when the target dataset is large [65, 47].
Finally, a recent work devised a strategy to transfer when
labeled data from many different domains is available [75].

3. Statistical methods

Much of the analysis in this work requires comparing ac-
curacies across datasets of differing difﬁculty. When ﬁtting
linear models to accuracy values across multiple datasets,
we consider effects of model and dataset to be additive.
In this context, using untransformed accuracy as a depen-
dent variable is problematic: The meaning of a 1% addi-
tive increase in accuracy is different if it is relative to a
base accuracy of 50% vs. 99%. Thus, we consider the
log odds, i.e., the accuracy after the logit transformation
logit(p) = log(p/(1 − p)) = sigmoid−1(p). The logit trans-
formation is the most commonly used transformation for
analysis of proportion data, and an additive change ∆ in
logit-transformed accuracy has a simple interpretation as a
multiplicative change exp ∆ in the odds of correct classiﬁ-
cation:

logit(cid:18)

ncorrect

ncorrect + nincorrect(cid:19) + ∆ = log(cid:18) ncorrect
= log(cid:18) ncorrect

nincorrect(cid:19) + ∆
exp ∆(cid:19)

nincorrect

We plot all accuracy numbers on logit-scaled axes.

We computed error bars for model accuracy averaged
across datasets, using the procedure from Morey [50] to
remove variance due to inherent differences in dataset dif-
ﬁculty. Given logit-transformed accuracies xmd of model
m ∈ M on dataset d ∈ D, we compute adjusted accuracies

take the mean and standard error of the adjusted accuracy
across datasets, and multiply the latter by a correction factor

acc(m, d) = xmd − Pn∈M xnd/|M|. For each model, we
p|M|/(|M| − 1).

When examining the strength of the correlation between
ImageNet accuracy and accuracy on transfer datasets, we
report r for the correlation between the logit-transformed
ImageNet accuracy and the logit-transformed transfer accu-
racy averaged across datasets. We report the rank correlation
(Spearman’s ρ) in Supp. Appendix A.1.2.

We tested for signiﬁcant differences between pairs of
networks on the same dataset using a permutation test or
equivalent binomial test of the null hypothesis that the pre-
dictions of the two networks are equally likely to be correct,
described further in Supp. Appendix A.1.1. We tested for
signiﬁcant differences between networks in average perfor-
mance across datasets using a t-test.

22662

Dataset

Classes

Size (train/test) Accuracy measure

Food-101 [5]
CIFAR-10 [37]
CIFAR-100 [37]
Birdsnap [4]
SUN397 [72]
Stanford Cars [36]
FGVC Aircraft [48]
PASCAL VOC 2007 Cls. [19]
Describable Textures (DTD) [10]
Oxford-IIIT Pets [53]
Caltech-101 [20]
Oxford 102 Flowers [52]

101
10
10
500
397
196
100
20
47
37
102
102

75,750/25,250
50,000/10,000
50,000/10,000
47,386/2,443
19,850/19,850
8,144/8,041
6,667/3,333
5,011/4,952
3,760/1,880
3,680/3,369
3,060/6,084
2,040/6,149

top-1
top-1
top-1
top-1
top-1
top-1
mean per-class
11-point mAP
top-1
mean per-class
mean per-class
mean per-class

Table 1. Datasets examined in transfer learning

4. Results

We examined 16 modern networks ranging in ImageNet
(ILSVRC 2012 validation) top-1 accuracy from 71.6% to
80.8%. These networks encompassed widely used Incep-
tion architectures [67, 34, 68, 66]; ResNets [28, 26, 25];
DenseNets [31]; MobileNets [30, 59]; and NASNets [78].
For fair comparison, we retrained all models with scale pa-
rameters for batch normalization layers and without label
smoothing, dropout, or auxiliary heads, rather than relying
on pretrained models. Supp. Appendix A.3 provides training
hyperparameters along with further details of each network,
including the ImageNet top-1 accuracy, parameter count,
dimension of the penultimate layer, input image size, and
performance of retrained models. For all experiments, we
rescaled images to the same image size as was used for
ImageNet training.

We evaluated models on 12 image classiﬁcation datasets
ranging in training set size from 2,040 to 75,750 images
(20 to 5,000 images per class; Table 1). These datasets
covered a wide range of image classiﬁcation tasks, includ-
ing superordinate-level object classiﬁcation (CIFAR-10 [37],
CIFAR-100 [37], PASCAL VOC 2007 [19], Caltech-101
[20]); ﬁne-grained object classiﬁcation (Food-101 [5], Bird-
snap [4], Stanford Cars [36], FGVC Aircraft [48], Oxford-
IIIT Pets [53]); texture classiﬁcation (DTD [10]); and scene
classiﬁcation (SUN397 [72]).

Figure 2 presents correlations between the top-1 accuracy
on ImageNet vs. the performance of the same model archi-
tecture on new image tasks. We measure transfer learning
performance in three settings: (1) training a logistic regres-
sion classiﬁer on the ﬁxed feature representation from the
penultimate layer of the ImageNet-pretrained network, (2)
ﬁne-tuning the ImageNet-pretrained network, and (3) train-
ing the same CNN architecture from scratch on the new
image task.

4.1. ImageNet accuracy predicts performance of lo-
gistic regression on ﬁxed features, but regu-
larization settings matter

We ﬁrst examined the performance of different networks
when used as ﬁxed feature extractors by training an L2-
regularized logistic regression classiﬁer on penultimate layer
activations using L-BFGS [44] without data augmentation.1
As shown in Figure 2 (top), ImageNet top-1 accuracy was
highly correlated with accuracy on transfer tasks (r = 0.99).
Inception-ResNet v2 and NASNet Large, the top two models
in terms of ImageNet accuracy, were statistically tied for
ﬁrst place.

Critically, results in Figure 2 were obtained with models
that were all trained on ImageNet with the same training
settings. In experiments conducted with publicly available
checkpoints, we were surprised to ﬁnd that ResNets and
DenseNets consistently achieved higher accuracy than other
models, and the correlation between ImageNet accuracy and
transfer accuracy with ﬁxed features was low and not statis-
tically signiﬁcant (Supp. Appendix B). Further investigation
revealed that the poor correlation arose from differences in
regularization used for these public checkpoints.

Figure 3 shows the transfer learning performance of In-
ception models with different training settings. We identify 4
choices made in the Inception training procedure and subse-
quently adopted by several other models that are detrimental
to transfer accuracy: (1) The absence of scale parameter (γ)
for batch normalization layers; the use of (2) label smooth-
ing [68] and (3) dropout [64]; and (4) the presence of an
auxiliary classiﬁer head [67]. These settings had a small
(< 1%) impact on the overall ImageNet top-1 accuracy of
each model (Figure 3, inset). However, in terms of average
transfer accuracy, the difference between the default and

1We also repeated these experiments with support vector machine clas-
siﬁers in place of logistic regression, and when using data augmentation for
logistic regression; see Supp. Appendix G. Findings did not change.

32663

Figure 2. ImageNet accuracy is a strong predictor of transfer accuracy for logistic regression on penultimate layer features and ﬁne-tuning.
Each set of panels measures correlations between ImageNet accuracy and transfer accuracy across ﬁxed ImageNet features (top), ﬁne-tuned
networks (middle) and networks trained from scratch (bottom). Left: Relationship between classiﬁcation accuracy on transfer datasets
(y-axis) and ImageNet top-1 accuracy (x-axis) in different training settings. Axes are logit-scaled (see text). The regression line and a 95%
bootstrap conﬁdence interval are plotted in blue. Right: Average log odds of correct classiﬁcation across datasets. Error bars are standard
error. Points corresponding to models not signiﬁcantly different from the best model (p > 0.05) are colored green.

optimal training settings was approximately equal to the dif-
ference between the worst and best ImageNet models trained
with optimal settings. This difference was visible not only
in transfer accuracy, but also in t-SNE embeddings of the
features (Figure 4). Differences in transfer accuracy between
settings were apparent earlier in training than differences

in ImageNet accuracy, and were consistent across datasets
(Supp. Appendix C.1).

Label smoothing and dropout are regularizers in the tra-
ditional sense: They are intended to improve generalization
accuracy at the expense of training accuracy. Although aux-
iliary classiﬁer heads were initially proposed to alleviate

42664

Logistic RegressionFine-TunedTrained from Random InitializationFigure 3. ImageNet training settings have a large effect upon per-
formance of logistic regression classiﬁers trained on penultimate
layer features. In the main plot, each point represents the logit-
transformed transfer accuracy averaged across the 12 datasets, mea-
sured using logistic regression on penultimate layer features from
a speciﬁc model trained with the training conﬁguration labeled at
the bottom. "+" indicates that a setting was enabled, whereas "−"
indicates that a setting was disabled. The leftmost, most heavily
regularized conﬁguration is typically used for Inception models
[68]; the rightmost is typically used for ResNets and DenseNets.
The inset plot shows ImageNet top-1 accuracy for the same training
conﬁgurations. See also Supp. Appendix C.1. Best viewed in color.

Figure 4. The default Inception training settings produce a subop-
timal feature space. Low dimensional embeddings of Oxford 102
Flowers using t-SNE [46] on features from the penultimate layer of
Inception v4, for 10 classes from the test set. Best viewed in color.

issues related to vanishing gradients [40, 67], Szegedy et al.
[68] instead suggest that they also act as regularizers. The
improvement in transfer performance when incorporating
batch normalization scale parameters may relate to changes
in effective learning rates [70, 76].

4.2. ImageNet accuracy predicts ﬁne-tuning perfor-

mance

We also examined performance when ﬁne-tuning Ima-
geNet networks (Figure 2, middle). We initialized each net-
work from the ImageNet weights and ﬁne-tuned for 20,000

Figure 5. ImageNet training settings have only a minor impact
on ﬁne-tuning performance. Each point represents transfer accu-
racy for a model pretrained and ﬁne-tuned with the same training
conﬁguration, labeled at the bottom. Axes follow Figure 3. See
Supp. Appendix C.2 for performance of models pretrained with
regularization but ﬁne-tuned without regularization.

steps with Nesterov momentum and a cosine decay learn-
ing rate schedule at a batch size of 256. We performed
grid search to select the optimal learning rate and weight
decay based on a validation set (for details, see Supp. Ap-
pendix A.5). Again, we found that ImageNet top-1 accuracy
was highly correlated with transfer accuracy (r = 0.96).

Compared with the logistic regression setting, regular-
ization and training settings had smaller effects upon the
performance of ﬁne-tuned models. Figure 5 shows average
transfer accuracy for Inception v4 and Inception-ResNet v2
models with different regularization settings. As in the lo-
gistic regression setting, introducing a batch normalization
scale parameter and disabling label smoothing improved
performance. In contrast to the logistic regression setting,
dropout and the auxiliary head sometimes improved perfor-
mance, but only if used during ﬁne-tuning. We discuss these
results further in Supp. Appendix C.2.

Overall, ﬁne-tuning yielded better performance than clas-
siﬁers trained on ﬁxed ImageNet features, but the gain dif-
fered by dataset. Fine-tuning improved performance over
logistic regression in 179 out of 192 dataset and model com-
binations (Figure 6; see also Supp. Appendix E). When
averaged across the tested architectures, ﬁne-tuning yielded
signiﬁcantly better results on all datasets except Caltech-101
(all p < 0.01, Wilcoxon signed rank test; Figure 6). The
improvement was generally larger for larger datasets. How-
ever, ﬁne-tuning provided substantial gains on the smallest
dataset, 102 Flowers, with 102 classes and 2,040 training
examples.

4.3. ImageNet accuracy predicts performance of
networks trained from random initialization

One confound of the previous results is that it is not
clear whether ImageNet accuracy for transfer learning is

52665

Training Settings0.81.01.21.41.6Average Transfer Accuracy (Log Odds)−BN Scale+Label Smooth+Dropout+BN Scale+Label Smooth+Dropout+BN Scale−Label Smooth+Dropout+BN Scale−Label Smooth−Dropout+BN Scale−Label Smooth−Dropout−Aux HeadInception-ResNet v2Inception v4Inception v3BN-InceptionInception v1Training Settings727476788082ImageNetAccuracyDefault Training SettingsOptimal Training Settings2.152.202.252.302.35Avg. Transfer Accuracy (Log Odds)−BN Scale+Label Smooth+Dropout+Aux Head+BN Scale+Label Smooth+Dropout+Aux Head+BN Scale−Label Smooth+Dropout+Aux Head+BN Scale−Label Smooth−Dropout+Aux Head+BN Scale−Label Smooth−Dropout−Aux HeadInception v4Inception-ResNet v2Training Settings8081ImageNetAccuracyFigure 6. Performance comparison of logistic regression, ﬁne-tuning, and training from random initialization. Bars reﬂect accuracy across
models (excluding VGG) for logistic regression, ﬁne-tuning, and training from random initialization. Error bars are standard error. Points
represent individual models. Lines represent previous state-of-the-art. Best viewed in color.

due to the weights derived from the ImageNet training or
the architecture itself. To remove the confound, we next
examined architectures trained from random initialization,
using a similar training setup as for ﬁne-tuning (see Supp.
Appendix A.6). In this setting, the correlation between Im-
ageNet top-1 accuracy and accuracy on the new tasks was
more variable than in the transfer learning settings, but there
was a tendency toward higher performance for models that
achieved higher accuracy on ImageNet (r = 0.55; Figure 2,
bottom).

Examining these results further, we found that a single
correlation averages over a large amount of variability. For
the 7 datasets with <10,000 examples, the correlation was
low and did not reach statistically signiﬁcance (r = 0.29; see
also Supp. Appendix D). However, for the larger datasets, the
correlation between ImageNet top-1 accuracy and transfer
learning performance was markedly stronger (r = 0.86).
Inception v3 and v4 were among the top-performing models
across all dataset sizes.

4.4. Beneﬁts of better models are comparable to

specialized methods for transfer learning

Given the strong correlation between ImageNet accuracy
and transfer accuracy, we next sought to compare simple
approaches to transfer learning with better ImageNet models
with baselines from the literature. We achieve state-of-the-
art performance on half of the 12 datasets if we evaluate
using the same image sizes as the baseline methods (Figure
6; see full results in Supp. Appendix F). Our results suggest
that the ImageNet performance of the pretrained model is a
critical factor in transfer performance.

Several papers have proposed methods to make better use
of CNN features and thus improve the efﬁcacy of transfer
learning [43, 11, 42, 23, 73, 63, 13, 41, 54]. On the datasets
we examine, we outperform all such methods simply by ﬁne-
tuning state-of-the-art CNNs (Supp. Appendix F). Moreover,
in some cases a better CNN can make up for dataset deﬁ-
ciencies: By ﬁne-tuning ImageNet-pretrained Inception v4,
we outperform the best reported single-model results for net-

Figure 7. For some datasets and networks, the gap between ﬁne-
tuning and training from random initialization is small. Each point
represents a dataset/model combination. Axes are logit-scaled. See
Figure 6 for network legend and Supp. Appendix E for scatter plots
of other settings. Best viewed in color.

works pretrained on the Places dataset [29, 77], which more
closely matches the domain of SUN397.

It is likely that improvements obtained with better mod-
els, specialized transfer learning methods, and pretraining
datasets with greater domain match are complementary.
Combining these approaches could lead to even better per-
formance. Nonetheless, it is surprising that simply using
a better model can yield gains comparable to specialized
techniques.

4.5. ImageNet pretraining does not necessarily im-

prove accuracy on ﬁne-grained tasks

Fine-tuning was more accurate than training from random
initialization for 189 out of 192 dataset/model combinations,
but on Stanford Cars and FGVC Aircraft, the improvement
was unexpectedly small (Figures 6 and 7). In both settings,
Inception v4 was the best model we tested on these datasets.
When trained at the default image size of 299 × 299, it
achieved 92.7% on Stanford Cars when trained from scratch
on vs. 93.3% when ﬁne-tuned, and 88.8% on FGVC Aircraft
when trained from scratch vs. 89.0% when ﬁne-tuned.

62666

Food-101CIFAR-10CIFAR-100BirdsnapSUN397CarsAircraftVOC2007DTDPetsCaltech-101Flowers50607080908593959798AccuracyLog. Reg.Fine TunedRandom InitSOTAInception v1BN-InceptionInception v3Inception v4Inception-ResNet v2ResNet-50 v1ResNet-101 v1ResNet-152 v1DenseNet-121DenseNet-169DenseNet-201MobileNet v1MobileNet v2MobileNet v2 (1.4)NASNet-A MobileNASNet-A LargeInception v4 @ 448px5060707580859095Trained from Random Init506070808590959798Fine TunedFood-101CIFAR-10CIFAR-100BirdsnapSUN397Stanford CarsFGVC AircraftVOC2007DTDOxford-IIIT PetsCaltech-101102 FlowersTrain Epochs

Figure 8. Networks pretrained on ImageNet converge faster, even when ﬁnal accuracy is the same as training from random initialization.
Each point represents an independent Inception v4 model trained with optimized hyperparameters. For ﬁne-tuning, we initialize with the
public TensorFlow Inception v4 checkpoint. Axes are logit-scaled.

Train Steps

ImageNet pretraining thus appears to have only marginal
accuracy beneﬁts for ﬁne-grained classiﬁcation tasks where
labels are not well-represented in ImageNet. At 100+ classes
and <10,000 examples, Stanford Cars and FGVC Aircraft
are much smaller than most datasets used to train CNNs [22].
In fact, the ImageNet training set contains more car images
than Stanford Cars (12,455 vs. 8,144). However, ImageNet
contains only 10 high-level car classes (e.g., sports car),
whereas Stanford Cars contains 196 car classes by make,
model, and year. Four other datasets (Oxford 102 Flowers,
Oxford-IIIT Pets, Birdsnap, and Food-101) require similarly
ﬁne-grained classiﬁcation, but the classes contained in the
latter three datasets are much better-represented in ImageNet.
Most of the cat and dog breeds present in Oxford-IIIT Pets
correspond directly to ImageNet classes, and ImageNet con-
tains 59 classes of birds and around 45 classes of fruits,
vegetables, and prepared dishes.

4.6. ImageNet pretraining accelerates convergence

Given that ﬁne-tuning and training from random initial-
ization achieved similar performance on Stanford Cars and
FGVC Aircraft, we next asked whether ﬁne-tuning still
posed an advantage in terms of training time. In Figure 8,
we examine performance of Inception v4 when ﬁne-tuning
or training from random initialization for different numbers
of steps. Even when ﬁne-tuning and training from scratch
achieved similar ﬁnal accuracy, we could ﬁne-tune the model
to this level of accuracy in an order of magnitude fewer steps.
To quantify this acceleration, we computed the number of
epochs and steps required to reach 90% of the maximum
odds of correct classiﬁcation achieved at any number of
steps, and computed the geometric mean across datasets.
Fine-tuning reached this threshold level of accuracy in an

average of 26 epochs/1151 steps (inter-quartile ranges 267-
4882 steps, 12-58 epochs), whereas training from scratch
required 444 epochs/19531 steps (inter-quartile ranges 9765-
39062 steps, 208-873 epochs) corresponding to a 17-fold
speedup on average.

4.7. Accuracy beneﬁts of ImageNet pretraining fade

quickly with dataset size

Although all datasets beneﬁt substantially from ImageNet
pretraining when few examples are available for transfer,
for many datasets, these beneﬁts fade quickly when more
examples are available. In Figure 9, we show the behavior
of logistic regression, ﬁne-tuning, and training from random
initialization in the regime of limited data, i.e., for dataset
subsets consisting of different numbers of examples per class.
When data is sparse (47-800 total examples), logistic regres-
sion is a strong baseline, achieving accuracy comparable to
or better than ﬁne-tuning. At larger dataset sizes, ﬁne-tuning
achieves higher performance than logistic regression, and,
for ﬁne-grained classiﬁcation datasets, the performance of
training from random initialization begins to approach re-
sults of pre-trained models. On FGVC Aircraft, training
from random initialization achieved parity with ﬁne-tuning
at only 1600 total examples (16 examples per class).

5. Discussion

Has the computer vision community overﬁt to ImageNet
as a dataset? In a broad sense, our results suggest the answer
is no: We ﬁnd that there is a strong correlation between
ImageNet top-1 accuracy and transfer accuracy, suggesting
that better ImageNet architectures are capable of learning
better, transferable representations. But we also ﬁnd that a

72667

Total Number of Examples

Figure 9. Pretraining on ImageNet improves performance on ﬁne-grained tasks with small amounts of data, but the gap narrows quickly
as dataset size increases. Performance of transfer learning with the public Inception v4 model at different dataset sizes. Error bars reﬂect
standard error over 3 subsets. Note that the maximum dataset size shown is not the full dataset. Best viewed in color.

Examples per Class

number of widely-used regularizers that improve ImageNet
performance do not produce better representations. These
regularizers are harmful to the penultimate layer feature
space, and have mixed effects when networks are ﬁne-tuned.

More generally, our results reveal clear limits to trans-
ferring features, even among natural image datasets. Im-
ageNet pretraining accelerates convergence and improves
performance on many datasets, but its value diminishes with
greater training time, more training data, and greater diver-
gence from ImageNet labels. For some ﬁne-grained classiﬁ-
cation datasets, a few thousand labeled examples, or a few
dozen per class, are all that are needed to make training from
scratch perform competitively with ﬁne-tuning. Surprisingly,
however, the value of architecture persists.

The last decade of computer vision research has demon-
strated the superiority of image features learned from data
over generic, hand-crafted features. Before the rise of con-
volutional neural networks, most approaches to image un-
derstanding relied on hand-engineered feature descriptors
[45, 15, 3]. Krizhevsky et al. [38] showed that, given the
training data provided by ImageNet [16], features learned
by convolutional neural networks could substantially outper-
form these hand-engineered features. Soon after, it became
clear that intermediate representations learned from Ima-
geNet also provided substantial gains over hand-engineered
features when transferred to other tasks [17, 56].

Is the general enterprise of learning widely-useful features
doomed to suffer the same fate as feature engineering? Given
differences between datasets [69], it is not entirely surpris-
ing that features learned on one dataset beneﬁt from some
amount of adaptation when applied to another. However,
given the history of attempts to build general natural-image
feature descriptors, it is surprising that common transfer

learning approaches cannot always proﬁtably adapt features
learned from a large natural-image to a much smaller natural-
image dataset.

ImageNet weights provide a starting point for features
on a new classiﬁcation task, but perhaps what is needed is
a way to learn adaptable features. This problem is closely
related to few-shot learning [39, 71, 55, 62, 21, 62, 49], but
these methods are typically evaluated with training and test
classes from the same distribution. Common few-shot learn-
ing methods do not seem to outperform classiﬁers trained on
ﬁxed features when domain shift is present [8], but it may
be possible to obtain better results with specialized meth-
ods [18] or by combining few-shot learning methods with
ﬁne-tuning [60]. It thus remains to be seen whether methods
can be developed or repurposed to adapt visual representa-
tions learned from ImageNet to provide larger beneﬁts across
natural image tasks.

Acknowledgements

We thank George Dahl, Boyang Deng, Sara Hooker,
Pieter-jan Kindermans, Rafael Müller, Jiquan Ngiam, Ruom-
ing Pang, Daiyi Peng, Kevin Swersky, Vishy Tirumalashetty,
Vijay Vasudevan, and Emily Xue for comments on the ex-
periments and manuscript, and Aliza Elkin and members of
the Google Brain team for support and ideas.

References

[1] Pulkit Agrawal, Ross B. Girshick, and Jitendra Malik. An-
alyzing the performance of multilayer neural networks for
object recognition. In European Conference on Computer
Vision (ECCV), 2014.

[2] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and S.
Carlsson. Factors of transferability for a generic convnet

82668

representation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 38(9):1790–1802, Sept 2016.

In IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 248–255. IEEE, 2009.

[3] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc
Van Gool. Speeded-up robust features (SURF). Computer
Vision and Image Understanding, 110(3):346–359, 2008.

[4] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L
Alexander, David W Jacobs, and Peter N Belhumeur. Bird-
snap: Large-scale ﬁne-grained visual categorization of birds.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 2019–2026. IEEE, 2014.

[5] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
Food-101 — mining discriminative components with random
forests. In European Conference on Computer Vision (ECCV),
pages 446–461. Springer, 2014.

[6] Ken Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and Andrew
Zisserman. Return of the devil in the details: delving deep
into convolutional nets. In British Machine Vision Conference,
2014.

[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected CRFs. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 40(4):834–848, 2018.

[8] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank
Wang, and Jia-Bin Huang. A closer look at few-shot classiﬁ-
cation. In International Conference on Learning Representa-
tions, 2019.

[9] Brian Chu, Vashisht Madhavan, Oscar Beijbom, Judy Hoff-
man, and Trevor Darrell. Best practices for ﬁne-tuning visual
classiﬁers to new domains. In Gang Hua and Hervé Jégou,
editors, Computer Vision – ECCV 2016 Workshops, pages
435–442, Cham, 2016. Springer International Publishing.

[10] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
Mohamed, and Andrea Vedaldi. Describing textures in the
wild. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 3606–3613. IEEE, 2014.

[11] Mircea Cimpoi, Subhransu Maji, and Andrea Vedaldi. Deep
ﬁlter banks for texture recognition and segmentation.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 3828–3836. IEEE, 2015.

[12] Alexis Conneau and Douwe Kiela. Senteval: An evaluation
toolkit for universal sentence representations. arXiv preprint
arXiv:1803.05449, 2018.

[13] Yin Cui, Feng Zhou, Jiang Wang, Xiao Liu, Yuanqing Lin,
and Serge Belongie. Kernel pooling for convolutional neu-
ral networks. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[14] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object
detection via region-based fully convolutional networks. In
Advances in neural information processing systems, pages
379–387, 2016.

[15] Navneet Dalal and Bill Triggs. Histograms of oriented gradi-
ents for human detection. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), volume 1, pages
886–893. IEEE, 2005.

[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. ImageNet: A large-scale hierarchical image database.

[17] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,
Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep
convolutional activation feature for generic visual recognition.
In International Conference on Machine Learning, pages
647–655, 2014.

[18] Nanqing Dong and Eric P Xing. Domain adaption in one-
shot learning. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pages 573–
588. Springer, 2018.

[19] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. International Journal of Computer
Vision, 88(2):303–338, 2010.

[20] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-
ative visual models from few training examples: An incre-
mental bayesian approach tested on 101 object categories. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) Workshop on Generative-Model Based Vision,
2004.

[21] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
In International Conference on Machine Learning, pages
1126–1135, 2017.

[22] Blair Hanley Frank. Google Brain chief: Deep learn-
In VentureBeat.

ing takes at
https://venturebeat.com/2017/10/23/google-brain-chief-
says-100000-examples-is-enough-data-for-deep-learning/,
2017.

least 100,000 examples.

[23] Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell.
Compact bilinear pooling.
In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 317–326,
2016.

[24] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 580–587,
2014.

[25] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noord-
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He. Accurate,
large mini-
batch SGD: training ImageNet in 1 hour. arXiv preprint
arXiv:1706.02677, 2017.

[26] Sam Gross

and Michael Wilber.

investigating residual nets.
http://torch.ch/blog/2016/02/04/resnets.html, 2016.

Training and
In The Torch Blog.

[27] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Gir-
shick. Mask R-CNN. In IEEE International Conference on
Computer Vision (ICCV), pages 2980–2988. IEEE, 2017.

[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR),
pages 770–778, 2016.

[29] Luis Herranz, Shuqiang Jiang, and Xiangyang Li. Scene
recognition with CNNs: objects, scales and dataset bias. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 571–579, 2016.

92669

[30] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. MobileNets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.

[31] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional networks.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 2261–2269, 2017.

[32] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,
Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wo-
jna, Yang Song, Sergio Guadarrama, and Kevin Murphy.
Speed/accuracy trade-offs for modern convolutional object de-
tectors. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017.

[33] Mi-Young Huh, Pulkit Agrawal, and Alexei A. Efros. What
CoRR,

makes ImageNet good for transfer learning?
abs/1608.08614, 2016.

[34] Sergey Ioffe and Christian Szegedy. Batch normalization: ac-
celerating deep network training by reducing internal covari-
ate shift. In International Conference on Machine Learning,
pages 448–456, 2015.

[35] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
European Conference on Computer Vision (ECCV), pages
694–711. Springer, 2016.

[36] Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei.
Collecting a large-scale dataset of ﬁne-grained cars. In Second
Workshop on Fine-Grained Visual Categorization, 2013.

[37] Alex Krizhevsky and Geoffrey Hinton. Learning multiple lay-
ers of features from tiny images. Technical report, University
of Toronto, 2009.

[38] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Ima-
geNet classiﬁcation with deep convolutional neural networks.
In Advances in Neural Information Processing Systems, pages
1097–1105, 2012.

[39] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenen-
baum. Human-level concept learning through probabilistic
program induction. Science, 350(6266):1332–1338, 2015.

[40] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou
Zhang, and Zhuowen Tu. Deeply-supervised nets. In Ar-
tiﬁcial Intelligence and Statistics, pages 562–570, 2015.

[41] Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, and
Wei Xu. Dynamic computational time for visual attention. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1199–1209, 2017.

[42] Tsung-Yu Lin and Subhransu Maji. Visualizing and under-
standing deep texture representations. In IEEE International
Conference on Computer Vision (ICCV), pages 2791–2799,
2016.

[43] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.
Bilinear CNN models for ﬁne-grained visual recognition. In
IEEE International Conference on Computer Vision (ICCV),
pages 1449–1457, 2015.

[44] Dong C Liu and Jorge Nocedal. On the limited memory bfgs
method for large scale optimization. Mathematical program-
ming, 45(1-3):503–528, 1989.

[45] David G Lowe. Object recognition from local scale-invariant
In IEEE International Conference on Computer

features.
Vision, volume 2, pages 1150–1157. Ieee, 1999.

[46] Laurens van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of Machine Learning Research,
9(Nov):2579–2605, 2008.

[47] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaim-
ing He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and
Laurens van der Maaten. Exploring the limits of weakly su-
pervised pretraining. In European Conference on Computer
Vision (ECCV), pages 181–196, 2018.

[48] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.
Fine-grained visual classiﬁcation of aircraft. Technical report,
2013.

[49] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter
Abbeel. A simple neural attentive meta-learner. In Interna-
tional Conference on Learning Representations, 2018.

[50] Richard D. Morey. Conﬁdence intervals from normalized data:
A correction to cousineau (2005). Tutorials in Quantitative
Methods for Psychology, 4(2):61–64, 2008.

[51] Romain Mormont, Pierre Geurts, and Raphaël Marée. Com-
parison of deep transfer learning strategies for digital pathol-
ogy. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops, pages 2262–2271,
2018.

[52] Maria-Elena Nilsback and Andrew Zisserman. Automated
ﬂower classiﬁcation over a large number of classes.
In
Computer Vision, Graphics & Image Processing, 2008.
ICVGIP’08. Sixth Indian Conference on, pages 722–729.
IEEE, 2008.

[53] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
CV Jawahar. Cats and dogs. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 3498–3505.
IEEE, 2012.

[54] Yuxin Peng, Xiangteng He, and Junjie Zhao. Object-part
attention model for ﬁne-grained image classiﬁcation. IEEE
Transactions on Image Processing, 27(3):1487–1500, 2018.
[55] Sachin Ravi and Hugo Larochelle. Optimization as a model
for few-shot learning. In International Conference on Ma-
chine Learning, 2016.

[56] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan,
and Stefan Carlsson. CNN features off-the-shelf: an astound-
ing baseline for recognition. In IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops (CVPRW),
pages 512–519. IEEE, 2014.

[57] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In Advances in neural information
processing systems, pages 91–99, 2015.

[58] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
Dec 2015.

[59] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. MobileNetV2: Inverted

102670

sual categorization. IEEE Transactions on Image Processing,
25(10):4858–4872, 2016.

[74] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How transferable are features in deep neural networks? In
Advances in Neural Information Processing Systems, pages
3320–3328, 2014.

[75] Amir R Zamir, Alexander Sax, William Shen, Leonidas
Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:
Disentangling task transfer learning. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
3712–3722, 2018.

[76] Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse.
Three mechanisms of weight decay regularization. In Inter-
national Conference on Learning Representations, 2019.

[77] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 2017.

[78] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V
Le. Learning transferable architectures for scalable image
recognition. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 8697–8710, 2018.

residuals and linear bottlenecks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 4510–4520, 2018.

[60] Tyler Scott, Karl Ridgeway, and Michael C Mozer. Adapted
deep embeddings: A synthesis of methods for k-shot induc-
tive transfer learning. In Advances in Neural Information
Processing Systems, pages 76–85, 2018.

[61] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. Interna-
tional Conference on Learning Representations, 2015.

[62] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypi-
cal networks for few-shot learning. In Advances in Neural
Information Processing Systems, pages 4080–4090, 2017.

[63] Yang Song, Fan Zhang, Qing Li, Heng Huang, Lauren J
O’Donnell, and Weidong Cai. Locally-transferred ﬁsher
vectors for texture classiﬁcation.
In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages
4912–4920, 2017.

[64] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple
way to prevent neural networks from overﬁtting. Journal of
Machine Learning Research, 15(1):1929–1958, 2014.

[65] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-
nav Gupta. Revisiting unreasonable effectiveness of data in
deep learning era. In Computer Vision (ICCV), 2017 IEEE
International Conference on, pages 843–852. IEEE, 2017.

[66] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning. In Proceedings
of the Thirty-First AAAI Conference on Artiﬁcial Intelligence
(AAAI-17), 2017.

[67] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2015.

[68] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception archi-
tecture for computer vision. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 2818–2826,
2016.

[69] Antonio Torralba and Alexei A Efros. Unbiased look at
dataset bias. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 1521–1528. IEEE, 2011.
[70] Twan van Laarhoven. L2 regularization versus batch and

weight normalization. CoRR, abs/1706.05350, 2017.

[71] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra,
et al. Matching networks for one shot learning. In Advances
in Neural Information Processing Systems, pages 3630–3638,
2016.

[72] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
and Antonio Torralba. Sun database: Large-scale scene recog-
nition from abbey to zoo. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 3485–3492.
IEEE, 2010.

[73] Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li,
and Qi Tian. Coarse-to-ﬁne description for ﬁne-grained vi-

112671

