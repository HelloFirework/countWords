Amodal Instance Segmentation with KINS Dataset

Lu Qi1

2

,

Li Jiang1

2

,

Shu Liu2 Xiaoyong Shen2

1The Chinese University of Hong Kong

Jiaya Jia1
2YouTu Lab, Tencent

2

,

{luqi, lijiang}@cse.cuhk.edu.hk

{shawnshuliu, dylanshen, jiayajia}@tencent.com

Abstract

Amodal instance segmentation, a new direction of in-
stance segmentation, aims to segment each object instance
involving its invisible, occluded parts to imitate human abil-
ity. This task requires to reason objects’ complex structure.
Despite important and futuristic, this task lacks data with
large-scale and detailed annotation, due to the difﬁculty of
correctly and consistently labeling invisible parts, which
creates the huge barrier to explore the frontier of visual
recognition.
In this paper, we augment KITTI with more
instance pixel-level annotation for 8 categories, which we
call KITTI INStance dataset (KINS). We propose the net-
work structure to reason invisible parts via a new multi-task
framework with Multi-Level Coding (MLC), which com-
bines information in various recognition levels. Extensive
experiments show that our MLC effectively improves both
amodal and inmodal segmentation. The KINS dataset and
our proposed method are made publicly available.

1. Introduction

Human has the natural ability to perceive objects’ com-
plete physical structure even under partial occlusion [24,
21]. This ability, which is called amodal perception, allows
us to gather integral information from not only visible clues
but also imperceptible signals. Practically, amodal percep-
tion in computer vision offers great beneﬁt in many scenar-
ios. Typical examples include enabling autonomous cars to
infer the whole shape of vehicles and pedestrians within the
range of vision, even if part of them is invisible, largely re-
ducing the risk of collision. It, thus, makes the moving de-
cision in complex trafﬁc or living environment easier. We
note most current autonomous cars and robots still do not
have this ability.

Challenges Although amodal perception is a common
human ability, most recent visual recognition tasks, includ-
ing object detection [16, 17, 36, 20, 8, 28], edge detection
[1, 11, 38], semantic segmentation [32, 41, 39] and instance
segmentation [19, 31], only focus on the visible parts of
instances. There are only a limited number of amodal seg-

(a)

(b)

(c)

Figure 1. Images in the KINS dataset are densely annotated with
object segments and contain relative occlusion order. (a) A sample
image, (b) the amodal pixel-level annotation of instances, and (c)
relative occlusion order of instances. Darker color means farther
instances in the cluster.

mentation methods [26, 43, 14] due to the difﬁculty in both
data preparation and network design.

Despite the community’s great achievement in image
collection, existing large-scale datasets [10, 12, 6, 29, 23]
for visual understanding are annotated without indicating
occlusion regions, thus cannot be used for amodal per-
ception. ImageNet [10] and OpenImages [23] are mainly
used for classiﬁcation and detection in image-or-box under-
standing. PASCAL VOC [12], COCO [29] and Cityscapes
[6] pay more attention to pixel-level segmentation, which
can be further classiﬁed into semantic segmentation and in-

3014

stance segmentation. These datasets have greatly promoted
the development of visual recognition techniques. However,
they only take into consideration the visible part of each in-
stance. The key challenge for amodal instance segmentation
data preparation is that annotation for occluded part has to
follow ground truth, where the latter may not be available
occasionally.

Our Contributions We put great effort to establish the
new KITTI [15] INStance segmentation dataset (KINS).
Using KITTI images, KINS has a lot of additional annota-
tions, including complicated amodal instance segmentation
masks and relative occlusion order following strict pixel-
level instance tagging rules. Each image is labeled by three
experienced annotators. The ﬁnal annotation of each in-
stance is determined by crowd-sourcing to deal with the
ambiguity. The ﬁnal labeled data for the unseen parts is
guaranteed to be consistent among all annotators.

So far, KINS is the largest amodal instance segmentation
dataset. Amodal instance segmentation is closely related
to other tasks such as scene ﬂow estimation, which means
that KINS also proﬁts other vision tasks to provide extra
information.

With this new large-scale dataset, we propose the effec-
tive Multi-Level Coding (MLC) to enhance the amodal per-
ception ability of conjecturing the integral pixel-level seg-
mentation masks for existing instance segmentation meth-
ods [31, 19]. MLC consists of two parts of extraction and
combination. The extraction part is mainly used to obtain
the abstract global representation of instances; and the com-
bination part integrates the abstract semantic information
and per-pixel speciﬁc features to produce the ﬁnal amodal
(or inmodal on visible parts) masks. A new branch for dis-
criminating the occluded regions is introduced to make the
network more sensitive to capture the amodal notion. Ex-
tensive experiments on the large-scale dataset verify that our
MLC improves both amodal and inmodal instance segmen-
tation by a large margin over different baselines.

2. Related Work

Object Recognition Datasets Most
large-scale visual
recognition datasets [10, 12, 6, 29, 23, 42] facilitate recog-
nizing visible objects in images. ImageNet [10] and Open-
Images [23] are used for classiﬁcation and detection without
considering objects’ precise mask. Meanwhile, segmenta-
tion datasets are built to explore the semantic mask of each
object in the pixel level. Pascal VOC [12], COCO [29] and
ADE20K [42] collect a large number of images in common
scenes. KITTI [15] and Cityscapes [6] are created for spe-
ciﬁc street scenarios. Although widely used in computer vi-
sion, these datasets do not contain labeling of invisible and
occluded part of objects, thus cannot be used for amodal
understanding.

Li and Malik [26] pioneered in building an amodal
dataset. Since the training data directly uses instance seg-
mentation annotation from Semantic Boundaries Dataset
(SBD) [18], there are inevitable noise and outliers. In [43],
Zhu et al. annotated part of the original COCO images [29]
and provided COCO amodal dataset, which contains 5,000
images. Empirically, we found it is hard for a network to
converge to an optimal point with this small-scale dataset
due to a large variety of instances, which motivates us to es-
tablish the KITTI INStance Segmentation Dataset (KINS)
with accurate annotation and image data in a much larger
scale. We show in experiments that KINS is rather bene-
ﬁcial and general for a variety of advanced vision under-
standing tasks.

Amodal Instance Segmentation Traditional
instance
segmentation is only concerned with visible part of each
instance. Popular frameworks are mainly proposal-based,
which exploits state-of-the-art detection models (e.g., R-
CNN [16], Fast R-CNN [17], Faster R-CNN [36], R-FCN
[8], FPN [28], etc.) to either classify mask regions or re-
ﬁne the predicted boxes to obtain masks. MNC [7] is the
ﬁrst end-to-end instance segmentation network, cascading
detection, segmentation and classiﬁcation. FCIS [27] em-
ploys the position-sensitive inside/outside score maps to
encode the foreground/background segmentation informa-
tion. Mask R-CNN [19] adds a mask head to obtain reﬁned
mask results from box prediction generated by FPN and
demonstrates outstanding performance. PANet [31] boosts
information ﬂow by bottom-up path augmentation, adap-
tive feature pooling and fully-connected fusion, which fur-
ther improves Mask R-CNN. The other stream is mainly
segmentation-based [2, 30, 22] with two-stage processing:
segmentation and clustering. They learn specially designed
transformation or instance boundaries. Instance masks are
then decoded from predicted transformation.

Research on amodal instance segmentation begins to ad-
vance. Li and Malik [26] proposed the ﬁrst method for
amodal instance segmentation. They extended their in-
stance segmentation approach [25] by iteratively enlarging
the modal bounding box of an object and recomputing the
mask. In order to evaluate on COCO amodal dataset, Zhu et
al. [43] use AmodalMask as a baseline, which is the Sharp-
Mask [34] trained on the amodal ground truth. Inspired by
multi-task ROI-based networks [37], in [14], instances are
segmented for both the amodal and inmodal setting. It adds
an independent segmentation branch for amodal mask pre-
diction on top of Mask R-CNN.

Several tasks encourage models to learn robust represen-
tation of input in a variety of applications, such as facial
landmark detection [40], natural language processing [5],
and steering prediction in autonomous driving [4]. Our de-
sign also extracts high-level semantic information to guide
the segmentation branch to better infer the occluded parts.

3015

Figure 2. A screenshot of our annotation tool for amodal segmen-
tation.

3. KINS: Amodal Instance Dataset

We annotate a total of 14, 991 images from KITTI to
form a large-scale amodal instance dataset, namely KINS.
The dataset is split into two parts where 7, 474 images are
for training and the other 7, 517 are for testing. All images
are densely annotated with instances by three skilled an-
notators. The annotation includes amodal instance masks,
semantic labels and relative occlusion order, from which in-
modal instance masks can be easily inferred. In this section,
we describe our KINS dataset and analyze it with a variety
of informative statistics.

3.1. Image Annotation

To obtain high-quality and consistent annotation, we
strictly follow three instance tagging rules of (1) only ob-
jects in speciﬁc semantic categories are annotated; (2) the
relative occlusion order among instances in an image is an-
notated; (3) each instance, including the occluded part, is
annotated in the pixel level. These rules make the annota-
tors to label instances in two steps. First, for each image,
one expert annotator locates instances within speciﬁc cate-
gories in the box level and indicates their relative occlusion
order. Afterwards, three annotators label the correspond-
ing amodal masks for each image regarding these box-level
instances. This process makes it easy for annotators to con-
sider instance relationship and infer scene geometry. As
shown in Figure 2, the annotation tool also well meets the
tagging requirement. The detailed process is as follows.

(1) Semantic Labels Our instances are in speciﬁc cate-
gories. Semantic labels in our KINS dataset are organized
into a 2-layer hierarchical structure, which deﬁnes an in-
clusion relationship between general- and sub-categories.
Given that all images in KITTI are street scenes, a total of
8 representative categories are chosen for annotation in the
second layer. General categories in KINS consist of ‘peo-
ple’ and ‘vehicle’. To keep consistency with KITTI detec-
tion dataset, the general category ‘people’ is further subdi-
vided into ‘pedestrian’, ‘cyclist’, and ‘person-siting’, while
the general category ‘vehicle’ is split into 5 sub-categories
of ‘car’, ‘tram’, ‘truck’, ‘van’, and ‘misc’. Here ‘misc’
refers to ambiguous vehicles that even experienced anno-
tators cannot specify the category.

(2) Occlusion Ordering For each image, an expert anno-
tator is asked to annotate instances with bounding boxes and
order them in relative occlusion. For the order among ob-
jects, instances in an image are ﬁrst partitioned into several
disconnected clusters, each with a few connected instances
for easy occlusion detection. Relative occlusion order is
based on the distance of each instance to the camera.
In
addition, as shown in Figure 1(c), instances in one cluster
are annotated in an order for near to distant objects where
orders of non-overlapping instances are labeled as 0. As for
the occluded instances in a cluster, order starts from 1 and
increases by 1 when occluded once. For occasional com-
plex occlusion situation (e.g. Figure 3), we impose another
important criterion that instances with the same relative oc-
clusion order should not occlude each other.

(3) Dense Annotation Three annotators then label each
instance densely in its corresponding bounding box. A spe-
cial focus in this step is to ﬁgure out occluded invisible parts
by three annotators independently. Given slightly different
predictions for the occluded pixels, our ﬁnal annotation is
decided by majority voting on the instance mask. For the
parts that do not reach consensus, e.g., location of invisible
car wheels as shown in Figure 3, more annotation iterations
are involved until high conﬁdence is reached for the wheel
position. An inmodal mask is also drawn if the instance is
occluded.

3.2. Dataset Statistics

In our KINS dataset, images are annotated following
aforementioned strict criteria. On average, each image has
12.53 labeled instances, and each object polygon consists
of 33.70 points. About 8.3% of image pixels are covered
by at least one object polygon. Of all regions, 53.6% are
partially occluded and the average occlusion ratio is 31.7%.
Annotating an entire image takes about 8 minutes where
each single instance needs 0.38 minute on average. 30%
of the time is spent on box-level localization and occlusion
ordering; the rest is on pixel-level annotation. The time cost
varies according to image and object structure complexity.
We analyze the detailed properties in several major aspects.

Semantic Labels Table 1 shows the distribution of in-
stance categories.
‘vehicle’ contains mostly cars, while
‘tram’ and ‘truck’ both contribute only 1% of the instances.
The occurrence frequency of ‘people’ is relatively low, tak-
ing 14.43% of all instances. Among them, 10.56% are
‘pedestrian’ and 2.69% are ‘cyclist’. Overall, the distribu-
tion follows Zipf’s law, same as the Cityscapes dataset [6].

Shape Complexity Intuitively, independent of scene ge-
ometry and occlusion patterns, amodal segments should
have relatively simpler shape than inmodel segments [43]
that are possibly occluded in any way. We calculate shape

3016

0

2

1

0

1

10

1

11

1

0

0

1

2 3

0

0

3

4

2

0

0

00

2

1

1

0

1

0

0

Figure 3. Examples of our amodal/inmodal masks. The digit in each amodal mask represents its relative occlusion order. Inmodal masks
are obtained with the amodal mask and relative occlusion order.

category

subcategory

pedestrian

number

ratio

20134
10.56%

people

cyclist
5120
2.69%

person-siting

2250
1.18%

car

van

misc
129164
18822
67.76% 5.93% 1.09% 0.92% 9.87%

truck
1756

11306

vehicle
tram
2074

Table 1. Class distribution of KINS.

simplicity

convexity

inmodal

amodal

inmodal

amodal

BSDS-A [43]
COCO-A [43]

KINS

.718
.746
.709

.834
.856
.830

.616
.658
.610

.643
.685
.639

Table 2. Comparison of shape statistics among amodal and in-
modal segments on BSDS, COCO and KINS.

convexity and simplicity following [43] as

convexity(S) =

simplicity(S) =

Area(S)

Area(ConvexHull(S))
p4π ∗ Area(S)

.

Perimeter(S)

(1)

Both metrics achieve the maximum value 1.0 when the
shape is a circle. Thus, simple segments statistically should
yield a large convexity-simplicity average value. Table 2
shows the comparison of shape simplicity and convexity
among three amodal datasets of KINS, BSDS and COCO.
The values of our KINS dataset are slightly smaller than
those of BSDS and COCO because KINS contains more
complex instances such as ‘cyclist’ and ‘person-siting’. We
also show the comparison between inmodal and amodal an-
notations of KINS. The amodal data yields stronger con-
vexity and simplicity, verifying that the amodal masks are
usually with more compact shapes.

Amodal Occlusion Occlusion level is deﬁned as the frac-
tion of area that is occluded. Figure 4(a) illustrates that
the occlusion level is nearly uniformly distributed in KINS.
Compared with COCO Amodal dataset, heavy occlusion is

more common in KINS. Occluded examples at different oc-
clusion levels are displayed in Figure 3. It is challenging to
reason out the exact shape of the car (Figure 3(a)) when the
occlusion level is high. This is why amodal segmentation
task is difﬁcult.

Max Occlusion Order The relative occlusion order is
valid only for instances in the same cluster. We accordingly
deﬁne the max occlusion order of a cluster as the number
of occluded instances in it. Besides, the max match num-
ber is the number of overlapping instances for each object.
The distributions of the order and number are drawn in Fig-
ure 4(c). Most clusters only contain a small amount of in-
stances. Clusters with the max occlusion order larger than
6 are only 1.54% in the whole dataset.

Segmentation Transformation With our amodal
in-
stance segments provided in KINS, the inmodal masks can
be easily obtained given the amodal masks and occlusion or-
ders. As shown in Figure 3, for two overlapping instances,
the intersection regions should belong to the instance with
the smaller occlusion order for inmodal annotation.

3.3. Dataset Consistency

Annotation consistency is a key property of any human-
labeled datasets since it determines whether the annotation
task is well deﬁned or not.
It is worth mentioning that
inferring the occluded part is subjective and open-ended.
However, due to our strict tagging criteria and human prior
knowledge of instances, the amodal annotations in KINS
are rather consistent. We evaluate it based on bounding box

3017

R
E
B
M
U
N

120000

100000

80000

60000

40000

20000

0

Max Match Number
Max Occlusion Order

97840

63920

45474

24987

7380

11819

4105

4129

4789

1821

2984

2895

1

2

3

4

5

>6

VALUE

(a) Occlusion Level

(b) Box Consistency

(c) Cluster Statistics

Figure 4. Three metrics to further evaluate our dataset.

(a) 20000 Iteration

(b) 24000 Iteration

Figure 5. Visualization of Mask R-CNN prediction for different
iterations. With more training iterations, masks of the orange car
and person shrink.

consistency and mask consistency. Considering that Inter-
section over Union (IoU) can measure the matching degree
of instance masks and bounding boxes from different anno-
tators, we calculate average IoU for all annotations.

First we measure the bounding box consistency by com-
paring the bounding boxes in KINS with those in origi-
nal KITTI detection dataset. Difference is found: bound-
ing boxes in KITTI detection dataset are annotated without
considering occluded pixels. Hence in KINS, the boxes are
generally larger. To fairly evaluate the consistency, we gen-
erate our own inmodal boxes by tightening the correspond-
ing inmodal masks. For each image, there are 12.74 objects
in KINS on average compared with 6.93 of them in KITTI
Detection dataset. The histogram in Figure 4(b) shows that
most annotations are consistent with the original detection
bounding boxes. Over 78.34% of the images have average
IoU larger than 0.65.

Second, for measuring mask consistency, we randomly
select 1,000 images from KINS (about 6.7% of the en-
tire dataset), and ask the three annotators to process them
again. There is a 4-month gap between the two annotation
stages. We denote the annotator i (i = 1, 2, 3, mv) in stage
j (j = 1, 2) as aij . The consistency scores between every
two annotators are shown in Table 3. Here, amvj represents
the annotation result after majority voting of the three an-
notators in stage j (j = 1, 2).

Although the two annotation periods have a few months
in between, annotators still tend to make similar prediction
about unseen parts. Thus the average IoUs of all images
in the diagonal of Table 3 are relatively high. We get the

ann12
ann22
ann32
annmv2

ann11
0.836
0.809
0.804
0.838

ann21
0.802
0.840
0.816
0.836

ann31
0.805
0.818
0.835
0.837

annmv1
0.834
0.836
0.833
0.843

Table 3. Consistency scores for three annotators in two stages.

highest score when matching the ﬁnal comprehensive re-
sults amv1 and amv2, which manifests that integrating an-
notation from the three annotators into a ﬁnal output by ma-
jority voting further improves data consistency.

4. Amodal Segmentation Network

Since amodal segmentation is a general and advanced
version of instance segmentation, we ﬁrst evaluate state-
of-the-art Mask R-CNN and PANet on amodal segmenta-
tion. Though designed for inmodal segmentation, these
frameworks are still applicable here by simply using amodal
masks and boxes. They can produce reasonable results. But
the problem is that increasing training iterations makes the
network suffer from severe overﬁtting, as shown in Figure
5. With more iterations, occlusion regions shrink or disap-
pear while the prediction of visible parts becomes stable.

Analysis of Amodel Properties To propose a suitable
framework for amodal segmentation, we ﬁrst analyze the
above overﬁtting issue by discussing important properties
of CNNs.
(1) Convolution operations, which are widely
used in mask prediction, help capture accurate local features
while losing a level of global information for the whole in-
stance region. (2) Fully Connected (FC) operations enable
the network to have comprehensive understanding instances
by integrating information in space and channels. In exist-
ing instance segmentation frameworks, the mask head usu-
ally consists of four convolution layers and one deconvo-
lution layer, making good use of local information. How-
ever, without global guidance or prior knowledge of the in-
stance, it is difﬁcult for the mask head to predict invisible
part caused by occlusion with only local information.

Importance of global information for inmodal mask pre-
diction was also mentioned in [31] especially for discon-
nected instances. Empirically, we also observe that strong
perception ability by global information is key for the net-

3018

work to recognize the occluded area.

We utilize more global information to infer occluded
parts. We ﬁrst explain the global features in Mask R-CNN.
Besides the region proposal network (RPN), there are three
branches in instance segmentation frameworks, including
box classiﬁcation, box regression and mask segmentation.
The ﬁrst two branches, sharing the same weight except two
independent FC layers, are respectively used to predict what
and where the instance is. They give attention to overall
perception where the features can be taken to help integral
instance inference.

Occlusion Classiﬁcation Branch We note only the
global box features are not enough for amodal segmenta-
tion because several instances may exist in one region of
interest (RoI). Features of other instances may cause am-
biguity in mask prediction. We accordingly introduce the
occlusion classiﬁcation branch to judge the possible exis-
tence of occlusion regions. The high classiﬁcation accuracy
in Table 6 shows that the occlusion features in this branch
provide essential invisible information and make mask pre-
diction balance the inﬂuence of several instances.

Amodal Segmentation Network (ASN) Based on above
consideration, Amodal Segmentation Network (ASN) is
proposed to predict complete shape of instances by combin-
ing box and occlusion features. As shown in Figure 6, our
framework is also a multi-task network with box, occlusion,
and mask branches.

Box branches, including classiﬁcation and regression,
share the same weight except the independent head. The
occlusion classiﬁcation branch is used to determine whether
occlusion exists in a RoI or not. The mask branch aims to
segment each instance. The input to all branches is the RoI
features; each branch consists of 4 cascaded convolutions
and ReLu operations. To predict the occlusion part, Multi-
Level Coding (MLC) is proposed to let the mask branch
segment complete instances by visible clues and inherent
perception of the integral region at the same time.

Moreover, to prove that our MLC is not restricted to
amodal segmentation, mask prediction of our network con-
sists of independent amodal and inmodal branches. For
each mask branch, the corresponding ground-truth is used
respectively. In the following, we explain the two most im-
portant and effective components in our framework, i.e., the
occlusion classiﬁcation branch and Multi-Level Coding.

4.1. Occlusion Classiﬁcation Branch

In general, 512 proposals are sampled from the result of
RPN with 128 foreground RoIs. According to our statistics,
at most 40 RoIs, in general, have overlapping parts even
considering background samples. What makes things more
challenging is that several occlusion regions only contain 1
to 10 pixels. The extreme imbalance between occlusion and

non-occlusion samples imposes extra difﬁculty for previous
networks to work here [35].

Based on common knowledge that features of extremely
small regions are weakened or even missed after RoI fea-
ture extraction [28, 9], we only regard regions with over-
lapping area larger than 5% of the total mask as occlusion
samples. To relieve the imbalance between occlusion and
non-occlusion samples, we set the weight loss of positive
RoI to 8. Besides, this branch leads to the backbone of our
network to extract robust image features under occlusion.

4.2. Multi Level Coding

Our network now contains occlusion information. To
further enhance the ability of predicting amodal or inmodal
masks given the currently long distance between the back-
bone and mask head, Multi-Level Coding (MLC) is pro-
posed to amplify the global information in mask prediction.
Albeit the same structure as box and occlusion branches,
the mask branch has its unique characteristics.
First,
this branch only aims to segment positive RoIs. There-
fore, in the box/occlusion classiﬁcation branch, only fea-
tures of positive samples are extracted and fed to MLC as
global guidance. Besides, sizes of feature maps for the
box/occlusion classiﬁcation branch and mask branch are re-
spectively 7 × 7 and 14 × 14. To utilize these features and
extract more information, our MLC has two modules of ex-
traction and combination. The extraction part incorporates
category and occlusion information into a global feature.
Then the combination part fuses the global feature and lo-
cal mask feature to help segment complete instances. More
details are given below. By default, the kernel size of the
convolution layer is C × C × 3 × 3, with stride and padding
size 1. C denotes the number of channels.

Extraction In this module, the box and occlusion classi-
ﬁcation features are ﬁrst concatenated and then up-sampled
by a deconvolution layer with a 2C × C × 3 × 3 kernel.
Next, to integrate information in two features, the upsam-
pled features are fed into two sequential convolution layers
followed by the ReLU operation.

Combination To combine the global and speciﬁc local
clues in the mask branch, the features from extraction part
are ﬁrst concatenated with mask feature. They are then fed
into three cascaded convolution layers followed by a ReLU
operation. The last convolution layer reduces the feature
channels by half, making the output dimension the same as
that of features in mask branches. At last, the output feature
is sent to the mask branch for ﬁnal semantic segmentation.

4.3. Multi Task Learning

Our network treats all branches of RPN, box recognition,
occlusion classiﬁcation and mask prediction similarly im-
portant with weight for each loss set to 1. It works decently

3019

Image Feature Extraction

RoI Align

Multi-Task Branch

Multi Head

Multi-Level Coding

FC

FC

FC

Box

Classification

CONCAT

Box

Regression
Occlusion

Classification

CONV

CONV

DECONV

BACKBONE

Multi-Level 

Coding

Multi-Level 

Coding

DECONV

Amodal

Segmentation

CONCAT

DECONV

Inmodal

Segmentation

CONV

CONV CONV

Figure 6. Apart from similar structure of Mask R-CNN, amodal segmentation network consists of an occlusion classiﬁcation branch and
Multi-Level Coding. Multi-Level Coding is used for combining multi-branch features to guide mask prediction by two modules including
extraction and combination. Yellow symbols represent features in corresponding branches.

for amodel segmentation in our experiments. The ﬁnal loss
is expressed as

L = Lcls + Lbox + Locclusion + Lmask, where
Lmask = Lmaska + Lmaski .

(2)

For inference, there is a small modiﬁcation. We calcu-
late the regressed boxes according to the output of the box
branch and proposal location. Then the updated boxes are
fed into the box branch again to extract class and occlusion
features. Afterwards, we only select remaining boxes after
NMS [13] for ﬁnal mask prediction.

5. Experiments

All experiments are performed on our new dataset with
7 object categories. ‘person-siting’ is excluded due to the
large number of annotation of crowds. Since the ground-
truth annotation of the testing set is available, 7,474 im-
ages are used for training; evaluation are conducted on the
7,518 test images. We integrate our occlusion classiﬁcation
branch and Multi-Level Coding on two baseline networks.
We train these network modules using the Pytorch li-
brary on 8 NVIDIA P40 GPUs with batch size 8 for 24, 000
iterations. Stochastic gradient descent with 0.02 learning
rate and 0.9 momentum is used as the optimizer. We de-
cay learning rate with 0.1 at 20, 000 and 22, 000 iterations
respectively. Results are reported in terms of mAP, which
is commonly used for detection and instance segmentation.
We use amodal bounding boxes as our ground truth in the
box branch in case of missing occlusion parts. We con-
ducted the same experiments ﬁve times and report the aver-
age results. The variance is 0.3.

5.1. Instance Segmentation

Model

MNC [7]
FCIS [27]

ORCNN [14]

Mask R-CNN [19]

Mask R-CNN + ASN

PANet [31]

PANet + ASN

Det

20.9
25.6
30.9
31.3
32.7
32.3
33.4

Amodal

Inmodal

Seg
18.5
23.5
29.0
29.3
31.1
30.4
32.2

Seg
16.1
20.8
26.4
26.6
28.7
27.6
29.7

Table 4. Comparison of our approach and other alternatives. All
super-parameters in both methods are the same.

stance segmentation, since the mask branch in our frame-
work can determine if the feature of invisible parts should
be enhanced or weakened. For amodal mask prediction,
MLC prefers to enlarge the mask area of invisible part by
global perception and prior knowledge about category and
occlusion prediction. Besides, connection of box, occlu-
sion and mask branches makes the feature in each branch
robust when serving different tasks, compared with in-
dependently working in previous networks. PANet with
44,056,576 parameters still performs worse than Mask R-
CNN + ASN with 13,402,240 parameters, indicating that
the performance gain is not only related to the number of
parameters. Note that the structure of ORCNN is similar to
Mask R-CNN with two independent mask heads, except for
a unique branch for predicting invisible parts.

5.2. Ablation Study

Ablation study on speciﬁc modules and their features fu-
sion locations is performed, as shown in Table 5. Inmodal
and amodal mask prediction along with Mask R-CNN per-
forms slightly better than each single mask prediction be-
cause more features in different aspects are learned.

Table 4 shows that our amodal segmentation network
produces decent mAPs for both amodal and inmodal in-

Performance further improves by adding the occlusion
classiﬁcation branch, which indicates that exploiting im-

3020

Model

Mask R-CNN [19]
Mask R-CNN [19]
Mask R-CNN [19]
Mask R-CNN + OC

Mask R-CNN + OC + MLC(0,0)
Mask R-CNN + OC + MLC(1,1)
Mask R-CNN + OC + MLC(2,2)
Mask R-CNN + OC + MLC(3,3)
Mask R-CNN + OC + MLC(0,3)
Mask R-CNN + OC + MLC(3,0)

Det

31.0
31.1
31.3
31.9
32.5
32.7
32.3
31.7
31.9
31.8

Amodal

Inmodal

Seg
×

29.2
29.3
30.0
31.0
31.1
30.6
29.8
29.8
29.7

Seg
26.4

×

26.6
27.9
28.6
28.7
28.2
28.0
27.9
27.8

Table 5. The ﬁrst three rows list Mask R-CNN performance for
inmodal segmentation, amodal segmentation and tackling both si-
multaneously. The fourth row is for the model that adds occlu-
sion classiﬁcation branch into Mask R-CNN. The remaining rows
show results with different fusion locations of modules’ features.
MLC(a, b) means the combination between features after the ath
convolution layer of box/occlusion classiﬁcation branch and that
after the bth convolution layer of the mask branch.

Model

MR [19]

MR + OC(0%)
MR + OC(5%)
MR + OC(15%)
MR + OC(20%)

Det

31.3
31.7
31.9
31.4
31.2

Amodal

Inmodal

Seg
29.3
29.7
30.0
29.6
29.4

Seg
26.6
27.5
27.9
27.3
26.7

Occlusion
Accuracy

0.866
0.871
0.872
0.869
0.866

Table 6. The ablation study for overlapping threshold in the occlu-
sion classiﬁcation branch. MR refers to Mask R-CNN.

age features with occlusion information to guide our mask
prediction are effective. The performance of feature fusion
location in Multi-Level Coding manifests that the box and
occlusion features in former layers are helpful to determine
whether the occlusion parts should be reasoned or not. The
best fusion location for different types of features is after
the ﬁrst convolution layer of each branch. These features
maintain not only the global information but also unique
properties for each speciﬁc-task branch.

Table 6 shows that overlapping threshold in occlusion
classiﬁcation branch is important to help get robust global
image features for the backbone network. Threshold 5%
is used for the best effect. An overly small threshold may
cause ambiguity to discriminate among RoIs with small oc-
clusion part, which are usually on the border. Contrarily,
it is also challenging for the network to capture sufﬁcient
amodal cases when using a too large threshold.

Further exploration for Multi-Level Coding is shown in
Table 7. MLC consists of four parts altogether. Each
module consists of concatenation or cascaded convolution,
as shown in Figure 6. The design of MLC yields very
good performance in both detection and segmentation. It
achieves effective feature fusing only using these a few sim-

Model

Modiﬁcation

0111
1011
1011
1011
1101
1110
1110
1110
1111

ADD

Order Adjustment

1 CONV
3 CONV

ADD

Order Adjustment

1 CONV
3 CONV

×

Det

32.1
32.4
31.9
32.7
32.3
32.6
32.0
32.6
32.7

Amodal

Inmodal

Seg
30.5
30.6
30.3
31.0
30.6
31.0
30.1
31.1
31.1

Seg
27.9
28.0
27.6
28.6
28.1
28.5
27.5
28.6
28.7

Table 7. Ablation study on differnet operations for each part of
Multi-Level Coding.
In column ‘Modiﬁcation’, ‘ADD’ means
adding features of two branches, ‘Order Adjustment’ means re-
versing the ‘special’ convolution, such as deconvolution with
stride 2, and other two cascaded convolutions. ’{x} CONV’ means
using x cascaded convolutions. ‘1011’ in column ‘model’ means
that we use default operations in column ‘Modiﬁcation’ except for
the second part.

ple operations. Due to the page limit, we have to put visual-
ization of our segmentation results into our supplementary
material.

5.3. Further Applications

Model

OSF [33]
ISF [3]

ISF with KINS

D1
4.74
3.61
3.56

D2
6.99
4.84
4.75

F1
8.55
6.50
6.39

SF
9.94
7.46
7.35

Table 8. Disparity (D1,D2), ﬂow (Fl) and scene ﬂow (SF) errors for
background and foreground are averaged over KITTI 2015 valida-
tion set.

Table 8 lists reﬁned ﬂow prediction assisted by instance
segmentation with the KINS dataset. For simplicity’s sake,
we train our ﬂow model following [3]. The improved per-
formance on the validation set of KITTI 2015 scene ﬂow
dataset indicates that KINS can be broadly beneﬁcial in
other vision tasks to provide extra information.

6. Conclusion

We have built a large dataset and presented a new multi-
task framework for amodal instance segmentation. KITTI
INStance dataset (KINS) are densely annotated with the
amodal mask and relative occlusion order for each speciﬁc
instance. Belonging to the augmented KITTI family, KINS
has great potential to beneﬁt other tasks in autonomous driv-
ing. Besides, a generic network design was proposed to
improve reasoning ability for invisible part with indepen-
dent occlusion classiﬁcation branch and Multi-Level Cod-
ing. More solutions for feature enhancement and models,
such as GAN, will be researched in future work.

3021

References

[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-
tendra Malik. Contour detection and hierarchical image seg-
mentation. PAMI, 2011. 1

[2] Min Bai and Raquel Urtasun. Deep watershed transform for

instance segmentation. In CVPR, 2017. 2

[3] Aseem Behl, Omid Hosseini

Jafari,

Siva Karthik
Mustikovela, Hassan Abu Alhaija, Carsten Rother, and
Andreas Geiger. Bounding boxes, segmentations and object
coordinates: How important is recognition for 3d scene
ﬂow estimation in autonomous driving scenarios? In ICCV,
2017. 8

[4] Rich Caruana. Multitask learning. Machine learning, 1997.

2

[5] Ronan Collobert and Jason Weston. A uniﬁed architecture
for natural language processing: Deep neural networks with
multitask learning. In ICML, 2008. 2

[6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016. 1, 2, 3

[7] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-
In

mantic segmentation via multi-task network cascades.
CVPR, 2016. 2, 7

[8] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN: object
detection via region-based fully convolutional networks. In
NIPS, 2016. 1, 2

[9] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. 2017. 6

[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 1, 2

[11] Piotr Doll´ar and C Lawrence Zitnick. Fast edge detection

using structured forests. PAMI, 2015. 1

[12] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. IJCV, 2010. 1, 2

[13] Pedro F Felzenszwalb, Ross B Girshick, David McAllester,
and Deva Ramanan. Object detection with discriminatively
trained part-based models. PAMI, 2010. 7

[14] Patrick Follmann, Rebecca K¨onig, Philipp H¨artinger, and
Michael Klostermann. Learning to see the invisible: End-to-
end trainable amodal instance segmentation. WACV, 2019.
1, 2, 7

[15] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In CVPR, 2012. 2

[16] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In CVPR, 2014. 1, 2

[17] Ross B. Girshick. Fast R-CNN. In ICCV, 2015. 1, 2
[18] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev,
Subhransu Maji, and Jitendra Malik. Semantic contours from
inverse detectors. In ICCV, 2011. 2

[19] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross B.

Girshick. Mask R-CNN. In ICCV, 2017. 1, 2, 7, 8

[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Spatial pyramid pooling in deep convolutional networks for
visual recognition. In ECCV, 2014. 1

[21] Philip J Kellman and Christine M Massey. Perceptual learn-
ing, cognition, and expertise. In Psychology of learning and
motivation. 2013. 1

[22] Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bog-
Instancecut: from

dan Savchynskyy, and Carsten Rother.
edges to instances with multicut. In CVPR, 2017. 2

[23] Ivan Krasin, Tom Duerig, Neil Alldrin, Andreas Veit, Sami
Abu-El-Haija, Serge Belongie, David Cai, Zheyun Feng, Vit-
torio Ferrari, Victor Gomes, et al. Openimages: A pub-
lic dataset for large-scale multi-label and multi-class im-
age classiﬁcation. Dataset available from https://github.
com/openimages, 2016. 1, 2

[24] Steven Lehar. Gestalt isomorphism and the quantiﬁcation of

spatial perception. Gestalt theory, 1999. 1

[25] Ke Li, Bharath Hariharan, and Jitendra Malik. Iterative in-

stance segmentation. In CVPR, 2016. 2

[26] Ke Li and Jitendra Malik. Amodal instance segmentation. In

ECCV, 2016. 1, 2

[27] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.
Fully convolutional instance-aware semantic segmentation.
In CVPR, 2017. 2, 7

[28] Tsung-Yi Lin, Piotr Doll´ar, Ross B. Girshick, Kaiming He,
Bharath Hariharan, and Serge J. Belongie. Feature pyramid
networks for object detection. In CVPR, 2017. 1, 2, 6

[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, 2014. 1, 2

[30] Shu Liu, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. Sgn:
Sequential grouping networks for instance segmentation. In
ICCV, 2017. 2

[31] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
In

Path aggregation network for instance segmentation.
CVPR, 2018. 1, 2, 5, 7

[32] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015. 1

[33] Moritz Menze and Andreas Geiger. Object scene ﬂow for

autonomous vehicles. In CVPR, 2015. 8

[34] Pedro O. Pinheiro, Tsung-Yi Lin, Ronan Collobert, and Piotr
Dollr. Learning to reﬁne object segments. In ECCV, 2016. 2
[35] Lu Qi, Shu Liu, Jianping Shi, and Jiaya Jia. Sequential con-

text encoding for duplicate removal. In NeuralPS, 2018. 6

[36] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: towards real-time object detection with re-
gion proposal networks. In NIPS, 2015. 1, 2

[37] Sebastian Ruder. An overview of multi-task learning in deep

neural networks. arXiv:1706.05098, 2017. 2

[38] Saining Xie and Zhuowen Tu. Holistically-nested edge de-

tection. In ICCV, 2015. 1

[39] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,
Gang Yu, and Nong Sang. Learning a discriminative feature
network for semantic segmentation. CVPR, 2018. 1

3022

[40] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou
Tang. Facial landmark detection by deep multi-task learning.
In ECCV, 2014. 2

[41] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
In

Wang, and Jiaya Jia. Pyramid scene parsing network.
CVPR, 2017. 1

[42] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Scene parsing through

Barriuso, and Antonio Torralba.
ade20k dataset. In CVPR, 2017. 2

[43] Yan Zhu, Yuandong Tian, Dimitris N Metaxas, and Piotr
Doll´ar. Semantic amodal segmentation. In CVPR, 2017. 1,
2, 3, 4

3023

