Privacy Protection in Street-View Panoramas using Depth

and Multi-View Imagery

Ries Uittenbogaard1,

∗Clint Sebastian2,

Julien Vijverberg3,

Bas Boom3, Dariu M. Gavrila1, Peter H.N. de With2

1Intelligent Vehicles Group, TU Delft,

2VCA Group, TU Eindhoven,

3Cyclomedia B.V

c.sebastian@tue.nl

∗

corresponding author

Abstract

The current paradigm in privacy protection in street-
view images is to detect and blur sensitive information. In
this paper, we propose a framework that is an alternative to
blurring, which automatically removes and inpaints moving
objects (e.g. pedestrians, vehicles) in street-view imagery.
We propose a novel moving object segmentation algorithm
exploiting consistencies in depth across multiple street-view
images that are later combined with the results of a seg-
mentation network. The detected moving objects are re-
moved and inpainted with information from other views, to
obtain a realistic output image such that the moving ob-
ject is not visible anymore. We evaluate our results on a
dataset of 1000 images to obtain a peak noise-to-signal
ratio (PSNR) and L1 loss of 27.2 dB and 2.5%, respec-
tively. To assess overall quality, we also report the results
of a survey conducted on 35 professionals, asked to visu-
ally inspect the images whether object removal and inpaint-
ing had taken place. The inpainting dataset will be made
publicly available for scientiﬁc benchmarking purposes at
https://research.cyclomedia.com/.

1. Introduction

In recent years, street-view services such as Google
Street View, Bing Maps Streetside, Mapillary have system-
atically collected and hosted millions of images. Although
these services are useful, they have been withdrawn or not
updated in certain countries [1, 2], due to serious privacy
concerns. The conventional way of enforcing privacy in
street-view images is by blurring sensitive information such
as faces and license plates. However, this has several draw-
backs. First, the blurring of an object like a face might
not ensure that the privacy of the person is sufﬁciently pro-
tected. The clothing, body structure, location and several
other aspects can lead to the identity of the person, even

Figure 1: Example of moving object segmentation (top) and
the results after inpainting (bottom). The regions that are
highlighted in orange are removed and inpainted.

if the face is hidden. Second, blurring objects creates arti-
facts that is undesirable in application when consistent view
of the infrastructure is required. For commercial purposes
such as change detection and localization of objects from
street-view imagery, blurring limits the scope of these ap-
plications.

It is therefore desirable to use a method that completely
removes the identity-related information. In this paper, we
present a method that automatically segments and replaces
moving objects from sequences of panoramic street-view
images by inserting a realistic background. Moving objects
are of primary interest because authentic information about
the background is present in the other views (we assume
that most moving objects are either vehicles or pedestrians).

110581

Method

Input data

Image matching

Detection

Class/No. of objects

Flores et al.

Grayscale,
Multi-view

Homography

computation using
SIFT & RANSAC

Leibe’s detector
(no moving object
detection, manual)

Real world positions

RGB-D,

from GPS, IMU

Novel deep learning
based moving object

Inpainting method
Homography-based

One pedestrian

warping + Davis

per image

Any number of

compositing

Reprojection from
multiple views +

objects and

multiview inpainting

Ours

Multi-view

with camera intrinsics

segmentation

classes per image

GAN

Table 1: Comparison of the proposed vs. closest related method.

It is risky to inpaint static objects after removing them, as
they may remove important or introduce new information
from context. We do not focus on inpainting static objects
such as parked cars, standing pedestrians etc. as no authen-
tic information of the background can be derived. However,
inpainting from spatial context is viable solution for non-
commercial applications. Using a segmentation network to
detect a moving object is a challenging task, since it needs
to learn to distinguish moving from static objects. To sim-
plify this problem, we generate a prior segmentation mask,
exploiting the consistencies in depth. The proposed mov-
ing object detection algorithm, combines with results from
a standard segmentation algorithm to obtain segmentation
masks for moving objects. Finally, to achieve an authentic
completion of the removed moving objects, we use inpaint-
ing Generative Adversarial Network (GAN) that utilizes
multi-view information. While multi-view GAN has been
explored for synthesizing an image to another view [3, 4], to
the best of our knowledge, this is the ﬁrst work that exploits
multi-view information for an inpainting GAN network.

2. Related Work

Privacy protection Several approaches have been pro-
posed for privacy protection in street-view imagery [5, 6,
7, 8, 9]. The most common way to hide sensitive informa-
tion is to detect the objects of interest and blur them [10].
However, few works have explored the removal of privacy-
sensitive information from street-view imagery for privacy
protection. Flores and Belongie [6] detects and inpaints
pedestrians from another view (details in Table 1). Simi-
larly, Nodari et al. [8] also focuses on pedestrians removal.
However, they remove the pedestrian with a coarse inpaint-
ing of the background. This is followed by replacement of
the inpainted region with a pedestrian obtained from a con-
trolled and authorized dataset. Although this method en-
sures the privacy, the replaced pedestrians tend to appear
unrealistic with respect to the background.
Object detection Due to the progress in deep learning,
there have been signiﬁcant improvements in object detec-
tion [11, 12, 13]. Detecting objects of interest provides a
reliable way to localize faces and license plates. Similarly,
for precise localization, semantic segmentation offers a bet-
ter alternative to bounding boxes [14, 15, 16]. Hence, we

rely on semantic segmentation approaching pixel accuracy,
as it requires fewer pixels to be replaced during inpainting.
We obtain our segmentation masks through a combination
of the proposed moving object segmentation and segmenta-
tion from a fully convolutional deep neural network.

In the recent years, LiDAR systems has become ubiq-
uitous for applications like self-driving cars and 3D scene
reconstruction. Several moving object detection methods
rely on LiDAR as it provides rich information about the
surroundings [17, 18, 19, 20]. Few approaches convert
LiDAR-based point-clouds into 3D occupancy grids or vox-
elize them [17, 18]. These are later segregated into occupied
and non-occupied building blocks. The occupied building
blocks are grouped into objects and are tracked over time
to determine moving objects [21]. Fusion of both LiDAR
and camera data has also been applied for object detec-
tion [20, 19, 21]. In this case, consistency across both im-
age and depth data (or other modalities) in several frames
are checked to distinguish static and moving objects.

Inpainting Prior works have tried to produce a realistic
inpainting by propagating known structures at the bound-
ary into the region that is to be inpainted [22]. However,
in street-view imagery, this is a challenging task especially
when it has large holes and require complex inpainting.
Therefore, few results relied on an exemplar or multi-view
based methods [23, 24]. State-of-the-art of inpainting meth-
ods adopt Generative Adversarial Networks (GANs) to pro-
duce high-quality inpainted images [25]. GANs are of-
ten applied for problems such as image inpainting [26, 27,
28], image-to-image translation [29, 30], conditional image
translation [31, 32] and super-resolution [33, 34].

Different approaches have been proposed for inpainting
images using deep neural networks. Pathak et al. proposed
one of the ﬁrst methods that utilized a deep neural net-
work [26]. They applied a combination of both reconstruc-
tion and adversarial losses to improve the quality of the in-
painted images. This was improved in [27], using dilated
convolutions and an additional local discriminator. To im-
prove the quality of details in the output image, Zhao et
al. proposes to use a cascade of deep neural networks [35].
The network ﬁrst inpaints with a coarse result, followed by a
deblurring-denoising network to reﬁne the output. A multi-
stage approach for inpainting is also proposed in [28]. Yu et

10582

Figure 2: Overview of the proposed method to segment moving objects and inpaint them from other views. The input image
is ﬁrst fed to segmentation network to produce the segmentation mask of both moving and static objects. The difference of
the convolution features of the reprojected images and input image is used to ﬁnd the moving objects in the segmentation
mask. The original input image (It), moving object segmentation mask (Bh
t ) and the reprojected images with regions active
in the segmentation mask ( ˆI r
t′→t) is fed to the generator. Note that the discriminators networks are not shown for simplicity.

al. introduces an attention based inpainting layer that learns
to inpaint by copying features from context. They also in-
troduce a spatially discounted loss function in conjunction
with improved Wasserstein GAN objective function [36] to
improve the inpainting quality.

Although inpainting based on context produces plausi-
ble outputs for accurate image completion, GANs may in-
troduce information that is not present in reality. This is
undesirable, especially in commercial applications, where
objects of interest are present or accurate localization are
required. A reasonable idea here is to utilize information
from other views as a prior. Other views provide a better
alternative than inpainted information from scratch. An ex-
ample would be the case when an object of interest (e.g.
trafﬁc-sign, bill-board) is occluded by a car or person. Af-
ter moving object detection, using a GAN to inpaint the hole
from context information would remove the object of inter-
est. However, multi-view information could alleviate this
problem as the object of interest is visible in the other views.
Our main paper contributions are:

• We propose a new multi-view framework for detect-
ing and inpainting moving objects, as an alternative to
blurring in street-view imagery

• We introduce a new moving object detection algorithm
based on convolutional features that exploits depth
consistencies from a set of consecutive images.

• We train an inpainting GAN that utilizes multi-view

information to obtain authentic and plausible results.

3. Method

First, we construct a method that combines standard seg-
mentation with a novel moving object segmentation, which
segments the moving objects from a consecutive set of im-
ages that have a large baseline. The moving object is es-
timated using an ego-motion based difference of convolu-
tional features. Second, we use a multi-view inpainting
GAN to ﬁll-in the regions that are removed from moving
object detection algorithm. The overview of the proposed
framework is shown in Fig 2.

3.1. Moving Object Segmentation

For supervised segmentation, we apply a Fully Convolu-
tional VGGNet (FC-VGGNet), due to its simplicity and the
rich features that are used for moving object segmentation.
We make slight modiﬁcations to VGGNet-16 by removing
fully-connected layers and appending bilinear upsampling
followed by convolution layer in the decoder. To create the
segmentation mask for a speciﬁc image It at time t, the de-
tection algorithm also uses the two images captured before
and after it, i.e. the RGB images at time t − 2, . . . , t + 2. Fi-
nally, from the LiDAR-based point cloud and the positions
of each recording, the depth images for these time steps are
created. Note that the RGB and the depth image are not
captured at the same time. Hence, the moving objects are
not at the same positions. Reprojecting the image It′ to the
position of image It is achieved using its respective depth
images Dt′ and Dt. Employing the depth images in con-
junction with recorded GPS positions leads to real-world
pixel positions ~p′
t′ , ~pt, resulting in the deﬁned image re-

10583

Figure 3: Results from features extracted from FC-VGGNet
(ﬁrst) and VGGNet (second). Features from FC-VGGNet
are well-localized and have strong activations.
projection ˆIt′→t. Evidently, some pixels in ˆIt′→t cannot
be ﬁlled due to occlusions. These pixels are replaced by
the pixel values of It by comparing the distance between
the real-world points to an heuristically deﬁned threshold ǫ.
This reprojection with a threshold is given by

ˆI ′

t′→t = (cid:26) ˆIt′→t

It

if ||~pt − ~p′
otherwise.

t′ || < ǫ,

(1)

Fig. 4 (Row 1) shows an example of reprojection for
4 neighbouring recordings of 5 consecutive images. A sim-
ple pixel-wise comparison between ˆI ′
t′→t and It yields poor
segmentation results, due to slight variations in the posi-
tion of the car. We have empirically found that patch-
based comparison utilizing pretrained network features pro-
duce better results than conventional pixel-wise and patch-
based features. Feature extraction is often applied to gener-
ate descriptors or rich representations for applications such
as image retrieval and person reidentiﬁcation [37, 38, 39].
However, here we utilize the extracted features to obtain
the moving objects.
Instead of using VGG [40] or other
pretrained network features, we extract features from FC-
VGGNet that is trained to detect static objects, as it is easier
to reuse the same network for moving object segmentation.
Besides the simplicity of relying on a single network and
higher performance, this also speeds up the pipeline. High-
dimensional features F(I) ∈ R64×256×512 are extracted
from the output of the 4th convolution block. The moving
object segmentation score is the average of the L1 norms
between each of the projected images and It, which is spec-
iﬁed by
s1/8
t =

||F(It) − F( ˆI ′

t+i→t)||1 ,

(2)

1

4 Xi∈{−2,−1,1,2}

where s1/8

t

is upsampled by factor of 8 to obtain a scor-
ing mask st of the original input size of 512 × 2048. Exam-
ples of the outputs st is shown in Fig. 3. To generate accu-
rate segmentation masks of moving objects, FC-VGGNet is
trained on the 4 classes that includes both moving and static
objects. For each of the extracted objects from the ﬁnal out-
put segmentation masks mt of FC-VGGNet, we compute
the element-wise product with the scoring mask st. We

classify an object as moving if the mean of the scores ex-
ceeds a threshold τ in the given object area A, yielding

1

n X(x,y)∈A

st(x, y) · mt(x, y) > τ,

(3)

where n is the number of elements in A. The value of the
threshold τ is discussed in Section 4.2.

3.2. Inpainting

After obtaining the segmentation masks from the mov-
ing object segmentation, we remove the detected objects.
With respect to previous approaches, our method requires
inpainting from other views that serve as a prior. Our input
images are also larger (512 × 512 pixels) compared to [28]
(256 × 256) and hence we added an additional strided con-
volution layer in the generator and two discriminators. Our
inputs consist of an RGB image with holes I h
t , the binary
mask with the holes Bh
t obtained from the moving object
detection and RGB images ˆI r
t′→t that are projected from
the other views. The images ˆI r
t′→t are obtained from re-
projection after removal of moving object from other views
(I r denoting removed objects) in the regions where holes
are present in the binary mask Bh
t . This is shown in the
third row of the Fig. 4. The ﬁnal input to the generator is a
16-channel input.

input

The 16-channel

is fed to the coarse network
from [28] to produce the ﬁnal output. We follow a similar
approach, however, the reﬁnement network is not used as no
performance improvement is observed. This occurs as the
input contains sufﬁcient prior information which alleviates
the need to produce a coarse output. We also observe that
we need to train for longer period of time with a single-stage
network to reach the performance of the two-stage network.
We follow the same strategy in [32, 28] of training multi-
ple discriminators to ensure both local and global consis-
tency. Hence, the output from the network is fed to a global
and local discriminator. For training, we use the improved
WGAN objective [36] along with a spatially discounted re-
construction loss [28]. The ﬁnal training objective L with
a generator G and discriminator networks Dc (where c de-
notes the context, global or local discriminator) is expressed
:

Lh

WGAN-GP(G, Dc) + Ld

L1(G, It),

(4)

L = min
G

max
Dc

where Lh
WGAN-GP is the WGAN adversarial loss with gra-
dient penalty applied to pixels within the holes and Ld
L1 is
the spatially discounted reconstruction loss. We follow the
same WGAN adversarial loss with gradient penalty in [28]
for our problem,

Lh

WGAN-GP(cid:0)G, D(cid:1) = E˜x∼Pf [D(˜x)] − Ex∼Pr [D(x)]
+ λ Eˆx∼Pˆx(cid:2)(k∇ˆxD(ˆx) ⊙ (1 − m)k2 − 1)2(cid:3),

where ∇ˆxD(ˆx) denotes the gradient of D(ˆx) with respect to
ˆx and ⊙ denotes the element-wise product. The samples x

(5)

10584

Figure 4: Images in the ﬁrst column are the input images (It) at time t. The ﬁrst row contains images ˆI ′
t′→t for t′ ∈
{−2, −1, +1, +2} that is projected to the view point of It. Results in the second row are obtained after removal of regions
around the area of interest. Finally, the third row ( ˆI r

t′→t) is obtained after removal of moving objects from other views.

t , ˆI r

and ˜x are sampled from real and generated distributions Pr
and Pf . Pf is implicitly deﬁned by ˜x = G([It, Bh
t′→t])
where [,] denotes the concatenation operation and t′ ∈
{−2, −1, 1, 2} . The sample ˆx is an interpolated point ob-
tained from a pair of real and generated samples. The gra-
dient penalty is computed only for pixels inside the holes,
hence, a mask 1 − m is multiplied with the input where the
values are 0 for missing pixels and 1 otherwise. The spatial
discounted reconstruction loss [28] Ld
L1 is simply weighted
L1 distance using a mask M and is given as

Ld

L1(G, It) = kM ⊙ G([It, Bh

t , ˆI r

t′→t]) − M ⊙ Itk1,

(6)

where each value in the mask M is computed as γl (l is the
distance of the pixel to nearest known pixel). We set the gra-
dient penalty coefﬁcient λ and the value γ to 10 and 0.99 re-
spectively as in [36, 28]. Intuitively, the Lh
WGAN-GP updates
the generator weights to learn plausible outputs whereas
Ld

L1 tries to the reconstruct the ground truth.

4. Experiments

We evaluate our method on the datasets described in the
next section. The ﬁnal results are evaluated using peak
signal-to-noise ratio (PSNR), L1 loss and an image qual-
ity assessment survey.

4.1. Datasets

The datasets consists of several high-resolution panora-
mas and depth maps derived from LiDAR point clouds.

Each of the high-resolution panoramas is obtained from a
ﬁve-camera system that has its focal point on a single line
parallel to the driving direction. The cameras are conﬁg-
ured such that the camera centers are on the same loca-
tion, in order to be able to construct a 360° panorama. The
parallax-free 360° panoramic images are taken at every 5-
meters and have a resolution of 100-megapixels. The im-
ages are well calibrated using multiple sensors (GPS, IMU)
and have a relative positioning error less than 2 centimeters
between consecutive images. The LiDAR scanner is a Velo-
dyne HDL-32E with 32 planes, which is tilted backwards
to maximize the density of the measurement. The RGB and
LiDAR are recorded together and they are matched using
pose graph optimization from several constraints such as
IMU, GPS. The point cloud from the LiDAR is meshed and
projected to a plane to obtain a depth map.

The segmentation dataset consists of 4,000 images of
512 × 512 pixels, 360° panoramas along with their depth
maps. The dataset is divided into 70% for training and
30% for testing. Our internal dataset consists of 96 classes
of objects out of which 22 are selected for training. The
22 classes are broadly segregated into 4 classes as record-
ing vehicle, pedestrians, two-wheelers and motorized ve-
hicles. The inpainting dataset contains of 8,000 images
where 1,000 are used for testing. The holes for inpainting
have varying sizes (128 × 128 to 384 × 384 pixels) that
are placed randomly at different parts of the image. The
inpainting dataset will be made publicly available.

10585

4.2. Moving Object Segmentation

We ﬁrst train FC-VGGNet on our internal dataset across
the pre-mentioned 4 sub-classes described in Section 4.1.
Due to high class imbalances (recording vehicle (5.3%),
pedestrians (0.05%), two-wheelers (0.09%) and motorized
vehicle (3.2%), other objects (91.4%)), the losses are re-
weighted inversely proportional to the percent of pixels of
each class. We observe the best performance at 160 epochs
to obtain a mean IoU of 0.583.

Since large public datasets of moving objects with
ground truth are not available, we resort to manual eval-
uation of the segmentation results. To evaluate the per-
formance of the moving object detection, we select every
moving object in 30 random images. We measure the clas-
siﬁcation accuracy of these extracted moving objects across
different layer blocks of FC-VGGNet to determine the best
performing layer block. From Fig. 5, we can conclude that
extracting features F(It) from convolution layers of the de-
coder (Layers 6-10) of FC-VGGNet leads to worse classi-
ﬁcation results than using the convolution layers of the en-
coder (Layers 1-5). The best results are obtained when the
outputs are extracted from the fourth convolution layer. We
also conducted experiments with a VGG-16, pretrained on
ImageNet dataset. We have found that the best results are
extracted from the eighth convolution layer, which produces
an output of size 28 × 28 × 512. However, on qualitative
analysis, we have found that the features from the encoder
layers of FC-VGGNet offer much better performance than
VGG features. This is visualized in Fig. 3. The activations
are stronger (higher intensities) for moving objects and have
lower false positives. This is expected as FC-VGGNet is
trained on the same data source as used for testing, whereas
VGG is trained on ImageNet.

It is interesting to observe that features from the shal-
lower layers (earlier layers) of FC-VGGNet perform much
better than deeper layers (later layers) for moving object de-
tection. This is due to features adapting to the ﬁnal segmen-
tation mask as the network grows deeper. As we compute
the L1 loss between F(It) and F( ˆI ′
t+i→t) at deeper layers,
the moving object segmentation is more close to the differ-
ence between the segmentation outputs of It and ˆI ′
t+i→t (ef-
fectively removing overlapping regions), resulting in poor
performance. The threshold τ to decide whether an object
is moving (as in Eq. (3)), is empirically determined. Sur-
prisingly, the threshold τ has minimal impact on the perfor-
mance. The mean IoU varies slightly, between 0.76 − 0.8
for τ ∈ [0.1, 0.9]. For all the experiments, we set τ to 0.7.

4.3. Inpainting

Initially, we train the inpainting network proposed by
Yu et al. [28]. However, we do not use the reﬁnement net-
work as we do not observe any performance improvements.
As input we supply 16 channels, 5 RGB images and a bi-

Figure 5: Classiﬁcation accuracy of extracted objects from
moving object segmentation results as moving/non-moving
from different layer blocks of FC-VGGNet.

nary mask. However, no real ground truth is present for an
input image, i.e. we do not have an image taken at the exact
same location without that moving object being there. Re-
projected images could serve as ground truth, but have ar-
tifacts and a lower visual quality. Therefore, we randomly
remove regions from the images (excluding regions of sky
and recording vehicle) to generate ground truth. Instead of
randomly selecting shapes for inpainting, the removed re-
gions have the shapes of moving objects (obtained from
moving object segmentation). The shapes of moving ob-
jects are randomly re-sized to different scales so as to learn
to inpaint objects of different sizes.

To provide an implicit attention for the inpainting GAN,
instead of feeding in the complete reprojected images, we
select only pixels from the non-empty regions of the binary
mask from the other views. However, simply feeding in
selected regions from other views have a drawback, since
a moving object that is partially visible in other views, is
also projected to the non-empty region. This is undesirable
as it causes the inpainting network to learn unwanted mov-
ing objects from other views. Therefore, the moving ob-
jects from other views are removed prior to projecting pixels
from other views. Therefore, the ﬁnal input for the genera-
tor is [It, Bh
t′→t], where [a, b] denotes the concatenation
operation of b after a. Optimization is performed using the
Adam optimizer [41] with a batch size of 8 for both the
discriminators and the generator. The learning rate is set
to 10−5 and is trained for 200 epochs. The discriminator-
to-generator training ratio is set to 2. The inpainted results
after moving object segmentation are shown in Fig. 6.

t , ˆI r

Evaluation Evaluation metrics such as Inception score
(IS) [42], MS-SSIM [43] and Birthday Paradox Test [44]
for evaluating GAN models are not suitable for inpainting
as inpainting focuses on ﬁlling in background rather than its
capacity to generate diverse samples. In the case of Fr´echet
Inception Distance (FID) [45] and IS [42], a deep network
is trained such that it is invariant to image transformations
and artifacts making it unsuitable for image inpainting task
as these metrics have low sensitivities to distortions.

10586

12345678910LayerBlock5060708090ClassiﬁcationaccuracyFigure 6: Inpainted results after removing objects obtained from moving object segmentation (bottom row). Input images are
shown in the top row. Inpainting in the second and third column have a slight ghosting effect. Participants have an average
conﬁdence of 20%, 62.9% and 80% (column 1-3) that it is an inpainted image.

For evaluation, we use both PSNR and L1 loss compar-
ing the ground-truth image against the inpainted image on a
test set of 1000 images. In our case, these metrics are suit-
able as they measure the reconstruction quality from other
views rather the plausibility or diversity of the inpainted
content. However, applying reconstruction losses as a eval-
uation metric favors multi-view based inpainting. As we
use multi-view information for inpainting, it is obvious that
the results can be improved signiﬁcantly making it difﬁcult
for a fair comparison. Nevertheless, we report the results
in PSNR and L1 losses on the validation set. We obtain a
PSNR and L1 loss of 27.2 dB and 2.5% respectively.

As a ﬁnal experiment to assess overall quality, we have
conducted a survey with 35 professionals within the do-
main. We have asked the participants to perform a strict
quality check on 30 randomly sampled image tiles out of
which 15 of them are inpainted after moving object de-
tection. The number of tiles in which a moving object is
removed is not revealed to the participants. Each partic-
ipant is asked to observe an image tile for approximately
10 seconds and then determine if a moving object has been
removed from the tile. Participants are also informed to
pay close attention to misplaced shadows, blurry areas and
other artifacts. The results of the survey is shown in Fig. 7.
In total of 1050 responses were collected from 35 par-

ticipants, 333 (31.7%) responses identiﬁed the true posi-
tives (inpainted images identiﬁed correctly as inpainted),
192 (18.3%) as false negatives (inpainted images not recog-
nized as inpainted), 398 (37.9%) as true negatives (not in-
painted and identiﬁed as not inpainted) and 127 (12.1%) as
false positives (not inpainted but recognized as inpainted).
Note that combination of true positives and false negatives
are disjoint from the combination true negatives and false
positives. The participants have an average conﬁdence of
63.4% ± 23.8% that a moving object(s) was inpainted in the
images where objects were removed (average of responses
in blue line of Fig. 7). However, it is interesting to note that
in cases when no object is removed, they have a conﬁdence
of 24.2% ± 13.5% stating that an object(s) is removed and
inpainted (average of responses in orange line of Fig. 7).

Clearly, in most cases with meticulous observation, par-
ticipants are able to discern if an object is removed. How-
ever, we also observe high deviation in the responses of
images where objects are removed and hence we inspect
images that have poor scores (high conﬁdence from partici-
pants stating that it is inpainted). Few of the worst perform-
ing results (conﬁdence higher than 90% on the blue line of
Fig. 7) are shown in Fig. 8. We have found that the worst re-
sults (Image 7, Image 10 with average conﬁdence of 94.3%,
97.1%) have the strongest artifacts. However, in the other

10587

e
c
n
e
d
ﬁ
n
o
c

.
g
v
A

100

80

60

40

20

0

With removed objects
Without removed objects

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15

Image index

Figure 7: The average conﬁdence per image of survey par-
ticipants indicating if an image has an inpainted region. The
blue line is for images that have objects removed and orange
for unaltered images.

cases (average conﬁdence of 82.9%, 85.7% and 82.9%), we
note that minuscule errors such as slight variations in edges,
lighting conditions, shadows, etc. are the reasons why par-
ticipants are able to distinguish the inpainted examples. Al-
though the poor cases are reported with high conﬁdence, we
believe that such artifacts would be hardly noticed in reality
if not explicitly searched. Despite these cases, this frame-
work ensures complete privacy, alleviates blurring artifacts
and removes occluded regions which is beneﬁcial for com-
mercial purposes.

5. Discussion

Although the proposed framework is a good alternative
to blurring, it is by no means perfect. The moving object
segmentation algorithm invokes a few challenges. Since
the moving object detection is class agnostic, there are false
positives from certain objects such as trafﬁc signs and light
poles. The comparison between features from different
viewpoints of a trafﬁc sign (front and back) leads to false
positives. Similarly, poles might be detected as moving
since small camera position errors lead to a minor mismatch
of depth pixels during reprojection. However, we are able
to suppress these false positives by combining the outputs
from FC-VGGNet. Even for a wide range of τ values [0.1
- 0.9], mIoU varies only by 4% ensuring the reliability and
robustness of the method. The proposed method may fail
when there is an overlap of moving and static objects. For
example, a car driving in front of parked vehicles can results
in all objects classiﬁed as moving or non-moving. However,
this can be mitigated by applying instance segmentation.

Limitations Poor results occur in a few cases when a driv-
ing vehicle is in the same lane as the recording vehicle
(Fig. 8, row 3). The moving object completely occludes
all the views making it difﬁcult for multi-view inpainting.

Figure 8: Worst performing results from the survey (high
conﬁdence that object is removed). Rows 1-3 with average
conﬁdence of 82.9%, 94.3% and 97.1% respectively.

In such a scenario inpainting based on context would be
an alternative, however, this does not guarantee a genuine
completion of the image. In non-commercial application,
this is still a viable solution. Even though few inpainting ar-
tifacts such as shadows, slightly displaced edges are visible,
we argue that they are still a better alternative to blurring as
it ensures complete privacy and far less noticeable artifacts
(Fig. 6, column 3 and Fig. 8, row 1). As the method does
not explicitly target shadow, this too may reveal privacy-
sensitive information in rare cases.

6. Conclusion

We presented a framework that is an alternative for blur-
ring in the context of privacy protection in street-view im-
ages. The proposed framework comprises of novel con-
volutional feature based moving object detection algorithm
that is coupled with a multi-view inpainting GAN to de-
tect, remove and inpaint moving objects. We demonstrated
through the multi-view inpainting GAN that legitimate in-
formation of the removed regions can be learned which is
challenging for a standard context based inpainting GAN.
We also evaluated overall quality by means of a user ques-
tionnaire. Despite the discussed challenges, the inpainting
results of the proposed method are often hard to notice and
ensures complete privacy. Moreover, the proposed approach
mitigates blurring artifacts and removes occluded regions
which are beneﬁcial for commercial applications. Although
most of the current solutions rely on blurring, we believe
that the future of privacy protection lies in the direction of
the proposed framework.

10588

References

[1] M. McGee, “Google has stopped street view photogra-
phy in Germany.” https://searchengineland.com/google-has-
stopped-street-view-photography-germany-72368. 1

view

street

“Google
in

[2] PTI,
sion
https://indianexpress.com/article/technology/tech-news-
technology/googles-street-view-turned-down-by-india-
2843618/. 1

denied

reason

India:

Heres

the

permis-
why.”

[3] M. Chen and L. Denoyer, “Multi-view generative adversarial
networks,” in Joint European Conference on Machine Learn-
ing and Knowledge Discovery in Databases, pp. 175–188,
Springer, 2017. 2

[4] L. Sun, W. Kang, Y. Han, and H. Ge, “Multi-view transfor-
mation via mutual-encoding infogenerative adversarial net-
works,” IEEE Access, vol. 6, pp. 43315–43326, 2018. 2

[5] A. Frome, G. Cheung, A. Abdulkader, M. Zennaro, B. Wu,
A. Bissacco, H. Adam, H. Neven, and L. Vincent, “Large-
scale privacy protection in Google street view,” in Com-
puter Vision, 2009 IEEE 12th International Conference on,
pp. 2373–2380, IEEE, 2009. 2

[6] A. Flores and S. Belongie, “Removing pedestrians from
google street view images,” in Computer Vision and Pattern
Recognition Workshops (CVPRW), 2010 IEEE Computer So-
ciety Conference on, pp. 53–58, IEEE, 2010. 2

[7] P. Agrawal and P. Narayanan, “Person de-identiﬁcation in
videos,” IEEE Transactions on Circuits and Systems for
Video Technology, vol. 21, no. 3, pp. 299–310, 2011. 2

[8] A. Nodari, M. Vanetti, and I. Gallo, “Digital privacy: Replac-
ing pedestrians from google street view images,” in Pattern
Recognition (ICPR), 2012 21st International Conference on,
pp. 2889–2893, IEEE, 2012. 2

[9] J. R. Padilla-L´opez, A. A. Chaaraoui, and F. Fl´orez-Revuelta,
“Visual privacy protection methods: A survey,” Expert Sys-
tems with Applications, vol. 42, no. 9, pp. 4177–4195, 2015.
2

[10] C. Sebastian, B. Boom, E. Bondarev, and P. H. N. de
With, “LiDAR-assisted Large-scale privacy protection in
street-view cycloramas,” To appear in Electronic Imaging,
vol. abs/1903.05598, 2019. 2

[11] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: To-
wards real-time object detection with region proposal net-
works,” in Advances in neural information processing sys-
tems, pp. 91–99, 2015. 2

[12] J. Dai, Y. Li, K. He, and J. Sun, “R-FCN: Object detec-
tion via region-based fully convolutional networks,” in Ad-
vances in neural information processing systems, pp. 379–
387, 2016. 2

[14] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional
networks for semantic segmentation,” in Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pp. 3431–3440, 2015. 2

[15] V. Badrinarayanan, A. Handa, and R. Cipolla, “Seg-
Net: A deep convolutional encoder-decoder architecture
for robust semantic pixel-wise labelling,” arXiv preprint
arXiv:1505.07293, 2015. 2

[16] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille, “Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs,” IEEE transactions on pattern analysis and ma-
chine intelligence, vol. 40, no. 4, pp. 834–848, 2018. 2

[17] F. Ferri, M. Gianni, M. Menna, and F. Pirri, “Dynamic ob-
stacles detection and 3D map updating,” 2015 IEEE/RSJ In-
ternational Conference on Intelligent Robots and Systems
(IROS), pp. 5694–5699, 2015. 2

[18] A. Azim and O. Aycard, “Detection, classiﬁcation and track-
ing of moving objects in a 3D environment,” 2012 IEEE In-
telligent Vehicles Symposium, pp. 802–807, 2012. 2

[19] H. Cho, Y.-W. Seo, B. V. K. V. Kumar, and R. Rajkumar,
“A multi-sensor fusion system for moving object detection
and tracking in urban driving environments,” 2014 IEEE In-
ternational Conference on Robotics and Automation (ICRA),
pp. 1836–1843, 2014. 2

[20] J. Yan, D. Chen, H. Myeong, T. Shiratori, and Y. Ma, “Au-
tomatic Extraction of Moving Objects from Image and LI-
DAR sequences,” 2014 2nd International Conference on 3D
Vision, vol. 1, pp. 673–680, 2014. 2

[21] A. Takabe, H. Takehara, N. Kawai, T. Sato, T. Machida,
S. Nakanishi, and N. Yokoya, “Moving object detection from
a point cloud using photometric and depth consistencies,” in
Pattern Recognition (ICPR), 2016 23rd International Con-
ference on, pp. 561–566, IEEE, 2016. 2

[22] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Gold-
man, “Patchmatch: A randomized correspondence algorithm
for structural image editing,” ACM Transactions on Graphics
(ToG), vol. 28, no. 3, p. 24, 2009. 2

[23] A. Criminisi, P. P´erez, and K. Toyama, “Region ﬁlling and
object removal by exemplar-based image inpainting,” IEEE
Transactions on image processing, vol. 13, no. 9, pp. 1200–
1212, 2004. 2

[24] P. Buyssens, M. Daisy, D. Tschumperl´e, and O. L´ezoray,
“Exemplar-based inpainting: Technical review and new
heuristics for better geometric reconstructions,” IEEE trans-
actions on image processing, vol. 24, no. 6, pp. 1809–1824,
2015. 2

[25] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,
“Generative Adversarial Nets,” in Advances in neural infor-
mation processing systems, pp. 2672–2680, 2014. 2

[13] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-
Y. Fu, and A. C. Berg, “SSD: Single shot multibox detec-
tor,” in European conference on computer vision, pp. 21–37,
Springer, 2016. 2

[26] D. Pathak, P. Kr¨ahenb¨uhl, J. Donahue, T. Darrell, and A. A.
Efros, “Context Encoders: Feature Learning by Inpaint-
ing,” 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 2536–2544, 2016. 2

10589

International Conference on Machine Learning and Com-
puting, pp. 172–177, ACM, 2018. 4

[40] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” in International
Conference on Learning Representations, 2015. 4

[41] D. P. Kingma and J. Ba, “Adam: A method for stochastic

optimization,” arXiv preprint arXiv:1412.6980, 2014. 6

[42] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Chen, “Improved techniques for training gans,”
in Advances in Neural Information Processing Systems,
pp. 2234–2242, 2016. 6

[43] J. Snell, K. Ridgeway, R. Liao, B. D. Roads, M. C. Mozer,
and R. S. Zemel, “Learning to generate images with percep-
tual similarity metrics,” in Image Processing (ICIP), 2017
IEEE International Conference on, pp. 4277–4281, IEEE,
2017. 6

[44] S. Arora, A. Risteski, and Y. Zhang, “Do GANs learn the dis-
tribution? some theory and empirics,” in International Con-
ference on Learning Representations, 2018. 6

[45] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter, “Gans trained by a two time-scale update rule
converge to a local nash equilibrium,” in Advances in Neural
Information Processing Systems, pp. 6626–6637, 2017. 6

[27] S. Iizuka, E. Simo-Serra, and H. Ishikawa, “Globally and
locally consistent image completion,” ACM Trans. Graph.,
vol. 36, pp. 107:1–107:14, 2017. 2

[28] J. Yu, Z. L. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang,
“Generative Image Inpainting with Contextual Attention,”
CoRR, vol. abs/1801.07892, 2018. 2, 4, 5, 6

[29] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired
image-to-image translation using cycle-consistent adversar-
ial networks,” 2017 IEEE International Conference on Com-
puter Vision (ICCV), pp. 2242–2251, 2017. 2

[30] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro, “High-resolution image synthesis and seman-
tic manipulation with conditional GANs,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2018. 2

[31] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-
image translation with conditional adversarial networks,”
2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 5967–5976, 2017. 2

[32] R. Uittenbogaard, C. Sebastian, J. Vijverberg, B. Boom, and
P. H. N. de With, “Conditional Transfer with Dense Resid-
ual Attention: Synthesizing trafﬁc signs from street-view im-
agery,” in 2018 24th International Conference on Pattern
Recognition (ICPR), pp. 553–559, Aug 2018. 2, 4

[33] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for
real-time style transfer and super-resolution,” in European
Conference on Computer Vision, pp. 694–711, Springer,
2016. 2

[34] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunning-
ham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang,
et al., “Photo-realistic single image super-resolution us-
ing a generative adversarial network,” arXiv preprint
arXiv:1609.04802, 2016. 2

[35] G. Zhao, J. Liu, J. Jiang, and W. Wang, “A deep cascade of
neural networks for image inpainting, deblurring and denois-
ing,” Multimedia Tools and Applications, vol. 77, pp. 29589–
29604, 2017. 2

[36] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
A. C. Courville, “Improved Training of Wasserstein GANs,”
in Advances in Neural Information Processing Systems 30
(I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett, eds.), pp. 5767–5777,
Curran Associates, Inc., 2017. 3, 4, 5

[37] R. Arandjelovi´c, P. Gronat, A. Torii, T. Pajdla, and J. Sivic,
“NetVLAD: CNN Architecture for Weakly Supervised Place
Recognition,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 40, pp. 1437–1451, June 2018.
4

[38] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han,
“Largescale image retrieval with attentive deep local fea-
tures,” in Proceedings of the IEEE International Conference
on Computer Vision, pp. 3456–3465, 2017. 4

[39] C. Liu, T. Bao, and M. Zhu, “Part-based feature extraction
for person re-identiﬁcation,” in Proceedings of the 2018 10th

10590

