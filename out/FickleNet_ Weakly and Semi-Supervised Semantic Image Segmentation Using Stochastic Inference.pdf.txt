FickleNet: Weakly and Semi-supervised Semantic Image Segmentation

using Stochastic Inference

Jungbeom Lee

Eunji Kim

Sungmin Lee

Jangho Lee

Sungroh Yoon†

Electrical and Computer Engineering, ASRI, INMC, and Institute of Engineering Research

Seoul National University, Seoul 08826, South Korea

sryoon@snu.ac.kr

Abstract

The main obstacle to weakly supervised semantic image
segmentation is the difﬁculty of obtaining pixel-level infor-
mation from coarse image-level annotations. Most methods
based on image-level annotations use localization maps ob-
tained from the classiﬁer, but these only focus on the small
discriminative parts of objects and do not capture precise
boundaries. FickleNet explores diverse combinations of lo-
cations on feature maps created by generic deep neural net-
works. It selects hidden units randomly and then uses them
to obtain activation scores for image classiﬁcation. Fick-
leNet implicitly learns the coherence of each location in the
feature maps, resulting in a localization map which identi-
ﬁes both discriminative and other parts of objects. The en-
semble effects are obtained from a single network by select-
ing random hidden unit pairs, which means that a variety of
localization maps are generated from a single image. Our
approach does not require any additional training steps and
only adds a simple layer to a standard convolutional neu-
ral network; nevertheless it outperforms recent compara-
ble techniques on the Pascal VOC 2012 benchmark in both
weakly and semi-supervised settings.

1. Introduction

Semantic segmentation is one of the most important and
interesting tasks in computer vision, and the development of
deep learning has produced tremendous progress in a fully
supervised setting [3, 36]. However, to use semantic image
segmentation in real life requires a large variety of object
classes and a great deal of labeled data for each class. Label-
ing pixel-level annotations of each object class is laborious,
and hampers the expansion of object classes. This problem
can be addressed by weakly supervised methods that use
annotations, which are less deﬁnite than those at the pixel
level and much easier to obtain. However, current weakly

†Correspondence to: Sungroh Yoon <sryoon@snu.ac.kr>.

Input image

  Convolution layers

왜 사냐건 웃지요. 나를

  Stochastic feature selection

X

X X X

X X

X

X X

X X

X

X

X X

X X

aeroplane

(a)

(b)

Figure 1. (a) FickleNet allows a single network to generate mul-
tiple localization maps from a single image. (b) Conceptual de-
scription of hidden unit selection. Selecting all hidden units (de-
terministic, left) produces smoothing effects as background and
foreground are activated together. Randomly selected hidden units
(stochastic, center and right) can provide more ﬂexible combina-
tions which can correspond more clearly to parts of objects.

supervised segmentation methods produce inferior results
to fully supervised segmentation.

Pixel-level annotations allow fully supervised semantic
segmentation to achieve reliability in learning the bound-
aries of objects and the relationship between their compo-
nents. But, it is difﬁcult to use image-level annotations to
train segmentation networks because weakly labeled data
only indicates the existence of objects of a certain class, and
does not provide any information about their locations or
boundaries. Most weakly supervised methods using image-
level annotations depend on localization maps obtained by
a classiﬁcation network [37] to bridge the gap between

5267

image-level and pixel-level annotations. However, these lo-
calization maps focus only on the small discriminative parts
of objects, without precise representations of their bound-
aries. To bring the performance of these methods closer to
that of fully supervised image segmentation means divert-
ing the classiﬁer from its primary task of discrimination be-
tween objects to discovering the relations between pixels.

We address this problem with FickleNet, which can gen-
erate a variety of localization maps from a single image us-
ing random combinations of hidden units in a convolutional
neural network, as shown in Figure 1(a). Starting with a fea-
ture map created by a generic classiﬁcation network such as
VGG-16 [26], FickleNet chooses hidden units at random for
each sliding window position, which corresponds to each
stride in the convolution operation, as shown in Figure 1(b).
This process is simply realized by the dropout method [28].
Selecting all the available hidden units in a sliding window
position (the deterministic approach) tends to produce a
smoothing effect that confuses foreground and background,
which can result in both areas being activated or deactivated
together. However, random selection of hidden units (the
stochastic approach) produces regions of different shapes
which can delineate objects more sharply. Since the patterns
of hidden units randomly selected by FickleNet include the
shapes of the kernel of the dilated convolution with different
dilation rates, FickleNet can be regarded as a generalization
of dilated convolution, but FickleNet can potentially match
objects of different scales and shapes using only a single
network because it is not limited to a square array of hidden
units, whereas dilated convolution requires networks with
different dilation rates just to scale its kernel.

The selection of random hidden units at each sliding win-
dow position is not an operation that is optimized at the
CUDA level in common deep-learning frameworks such as
PyTorch [22]. Thus, a naive implementation of FickleNet,
in which random hidden units are selected at each sliding
window position and then convolved, would require a large
number of iterative operations. However, we can use the
optimized convolution functions provided by deep-learning
frameworks, if we expand the feature maps before making
the random selection of hidden units. The maps need to be
expanded sufﬁciently to prevent successive sliding window
positions from overlapping. We can then apply dropout in
the spatial axis of the expanded feature maps, and perform a
convolution operation with a stride equal to the kernel size.
This saves a signiﬁcant amount of time without much in-
crease in GPU memory usage, because the number of pa-
rameters to be back-propagated remains constant.

While many existing networks use stochastic regulariza-
tion in their training process (e.g. Dropout [28]), stochas-
tic effects are usually excluded from the inference process.
However, our inference process contains random processes
and thus produces a variety of localization maps. The pixels

that were allocated to a speciﬁc class with high scores in
each localization map are discovered, and those pixels are
aggregated into a single localization map. The localization
map obtained from FickleNet is utilized as pseudo-labels
for the training of a segmentation network.

The main contributions of this paper can be summarized

as follows:

• We propose FickleNet, which is simply realized using
the dropout method, that discovers the relationship be-
tween locations in an image and enlarges the regions
activated by the classiﬁer.

• We introduce a method of expanding feature maps
which makes our algorithm much faster, with only a
small cost in GPU memory.

• Our work achieves state-of-the-art performance on the
Pascal VOC 2012 benchmark in both weakly super-
vised and semi-supervised settings.

2. Related Work

Weakly supervised semantic image segmentation meth-
ods substitute inexact annotations such scribbles, bound-
ing boxes, or image-level annotations, for strong pixel-
level annotations. The methods of recent introduction have
achieved successful results using annotations that provide
location information such as scribbles or bounding boxes [4,
29]. We now review some recently introduced weakly su-
pervised approaches which use image-level annotations.

A class activation map (CAM) [37] is a good starting-
point for the classiﬁcation of pixels from image-level anno-
tations. A CAM discovers the contribution of each hidden
unit in a neural net to the classiﬁcation score, allowing the
hidden units which make large contributions to be identi-
ﬁed. However, a CAM tends to focus on the small discrim-
inative region of a target object, which makes it unsuitable
for training a semantic segmentation network. Weakly su-
pervised methods of recent introduction expand the regions
activated by a CAM, operating on the image (Section 2.1),
on features (Section 2.2), or by growing the regions found
by a CAM (Section 2.3).

2.1. Image level Processing

Image-level hiding and erasure have been proposed [19,
27, 31] as ways of preventing a classiﬁer from focusing ex-
clusively on the discriminative parts of objects. HaS [27]
hides random regions of a training image, forcing the classi-
ﬁcation network to seek other parts of the object. However,
the process of hiding random regions does not consider the
semantics and sizes of objects. AE-PSL [31] starts with a
single small region in the object, and then drives the clas-
siﬁcation network to discover a sequence of new and com-
plement any object regions by erasing the regions that have

5268

already been found. Although it can progressively expand
regions belonging to an object, it requires multiple classiﬁ-
cation networks to perform the repetitive classiﬁcation and
erasure steps. GAIN [19] has a CAM which is trained to
erase regions in a way that deliberately confuses the clas-
siﬁer. This CAM has to be large enough to cover an entire
object. However, the classiﬁer mainly reacts to high acti-
vation, and so it can become confused if an object’s only
discriminative parts are erased.

(a)

(b)

2.2. Feature level Processing

Feature-level processing can be used to expand the re-
gions activated by a CAM. ACoL [35] and TPL [14] use
a classiﬁer to identify the discriminative parts of an ob-
ject and erase them based on features. A second classiﬁer
then is trained to ﬁnd the complementary parts of the ob-
ject from those erased features. This is an efﬁcient tech-
nique which operates at a relatively high level. However,
it has a similar drawback to image-level erasure, in that a
second classiﬁer and training step are essential for those
methods, which may cause a suboptimal performance. In
addition, features whose discriminative parts are erased can
confuse the second classiﬁer, which may not be correctly
trained. PG-CAM [18] collects features from each of sev-
eral densely connected layers and merges the resulting lo-
calization maps.

MDC [33] uses several convolutional blocks, dilated at
different rates, within a generic classiﬁcation network, and
aggregates CAMs obtained from each block in a process
that resembles ensemble learning. The different-sized re-
ceptive ﬁelds produced by different dilation rates can be
shown to capture different patterns, but MDC requires a
separate training procedure for each dilation rate, and its
limitation to integer dilation rates (e.g. 1, 3, 6, 9) means that
only a limited number of ensembles is possible. In addition,
the receptive ﬁeld produced by a standard dilated convolu-
tion is square with a ﬁxed size, so that MDC tends to iden-
tify false positive regions.

2.3. Region Growing

Region growing can be used to expand the localization
map produced by a CAM, which initially identiﬁes just the
small discriminative part of an object. AfﬁnityNet [1] learns
pixel-level semantic afﬁnities, which identify pixels belong-
ing to the same object, under the supervision of an initial
CAM, and then expands the initial CAM by a random walk
with the transition matrix computed from semantic afﬁni-
ties. However, the learning of semantic afﬁnities requires an
additional network, and the outcome depends heavily on the
quality of the CAM. SEC [16] uses a new type of loss func-
tion to expand the localization map and constrain it to object
boundaries using a conditional random ﬁeld (CRF) [17].
DSRG [12] reﬁnes initial localization maps during the train-

)
1

 
:
 

e
d
i
r
t
s
(
v
n
o
C

)

 
:
 

e
d
i
r
t
s
(
 
v
n
o
C

Figure 2. (a) Naive implementation of FickleNet, which requires
a dropout and convolution function call at each sliding window
position (the red and green boxes). (b) Implementation using map
expansion: convolution is now performed once with a stride of s.
The input feature map is expanded so that successive sliding ker-
nels (the red and green boxes) do not overlap.

ing of its segmentation network, so that DSRG does not re-
quire additional networks to grow regions. The seeds for re-
gion growing are obtained from a CAM, and if these seeds
only come from the discriminative parts of objects, it is
difﬁcult to grow regions into non-discriminative parts. We
therefore utilize as a segmentation network with the local-
ization maps produced by FickleNet.

3. Proposed Method

Our procedure has the following steps: FickleNet, which
uses stochastic selection of hidden units, is trained for
multi-class classiﬁcation. It
then generates localization
maps of training images. Finally, the localization maps are
used as pseudo-labels to train a segmentation network. We
denote the sort of feature map typically obtained from a
standard deep neural network as x ∈ Rk×h×w, where w and
h are the width and the height of each of k channels, respec-
tively. The procedures for training FickleNet and generating
localization maps are shown as Algorithm 1.

Algorithm 1: Training and Inference Procedure

Input: Image I, ground-truth label c, dropout rate p
Output: Classiﬁcation score S and localization maps M

1 x = Forward(I) until conv5 layer;
2 Stochastic hidden unit selection:

3

4

xexpand = Expand(x);
xexpand
S = Classiﬁer(xexpand

p

p

);

5
6 Training Classiﬁer:

= Center-ﬁxed spatial dropout(xexpand, p);

Update network by L=SigmoidCrossEntropy(S, c)

7
8 Inference CAMs:

9

10

11

For different random selections i (1 ≤ i ≤ N ):

M c[i] = Grad-CAM(x, S c);

M c = Aggregate(M c[i]);

5269

Sec. 3.1
Sec. 3.1.1

Sec. 3.1.2

Sec. 3.1.3

Sec. 3.2

Sec. 3.2.1
Sec. 3.2.2

3.1. Stochastic Hidden Unit Selection

Stochastic hidden unit selection is used in FickleNet to
discover relations between parts of objects by exploring the
classiﬁcation score computed from the randomly selected
pairs of hidden units, with the aim of associating a non-
discriminative part of an object with a discriminative part of
the same object. This process is realized by applying spatial
dropout [28] to the feature x at each sliding window posi-
tion, as shown in Figure 2(a). This differs from the standard
dropout technique, which only samples hidden units in the
feature maps once in each forward pass, and thus hidden
units which are not sampled cannot contribute to the class
scores. Our method samples hidden units at each sliding
window position, which means that a hidden unit may be
activated at some window positions and dropped at others.
This method of selecting hidden units can generate re-
ceptive ﬁelds of many different shapes and sizes, as shown
in Figure 3. Some of these ﬁelds are likely to be simi-
lar to those produced by a standard dilated convolution;
thus the results produced by this technique can be ex-
pected to contain those produced by standard dilated con-
volution at various rates. This selection process can be sim-
ply and efﬁciently realized by the expansion technique de-
scribed in Section 3.1.1 with a method which we call center-
preserving dropout, which is described in Section 3.1.2.

3.1.1 Feature Map Expansion

As our method needs to sample new combinations in each
sliding window position, we cannot directly utilize the
CUDA-level optimized convolution functions provided by
popular deep learning frameworks such as PyTorch [22]. If
we were to implement our method naively, as shown in Fig-
ure 2(a), we would have to call the convolution function and
the dropout function in w × h times in each forward pass.
By expanding the feature map, we reduce this to a single
call to each function during each forward pass.

Figure 2(b) shows how we expand the input feature maps
so that no sliding window positions overlap. Before expand-
ing the feature map, we apply zero padding on x so that
the size of the ﬁnal output is equal to that of the input.
The size of the feature map after zero padding becomes
k × (h + s − 1) × (w + s − 1), where s is the size of the
convolution kernel. We expand the zero-padded feature map
so that successive sliding window positions do not over-
lap, and the size of the expanded feature map xexpand is
k × (sh) × (sw). We then select hidden units on xexpand
using the center-preserving dropout technique explained in
Section 3.1.2. Examples of this expansion process are pre-
sented in the appendix (Section ??). Although the expanded
feature map requires more GPU memory, the number of pa-
rameters to be trained remains constant, and so the load on
the GPU does not increase signiﬁcantly.

Figure 3. Examples of the selection of 9 hidden units (marked as
blue) from a 7 × 7 kernel. Channels are not shown for simplicity.
The shapes of those selected hidden units sometimes contain the
shape of kernel of convolution with different dilation rates.

3.1.2 Center-preserving Spatial Dropout

We realize stochastic hidden unit selection by applying the
dropout method [28] to spatial locations. We can achieve
the same results as the naive implementation by applying
dropout only once to the expanded feature map xexpand.
Note that dropout is applied uniformly across all channels.
We do not drop the center of the kernel of each sliding
window position, so that relationships between kernel
center and other locations in each stride can be found. After
spatial dropout with a rate of p, we denote the modiﬁed
feature map as xexpand
is usually only
employed during training, we apply it to both training and
inference.

. While dropout

p

3.1.3 Classiﬁcation

p

In order to obtain classiﬁcation scores, convolution with
kernel of size s and stride s are applied to the dropped fea-
ture map xexpand
. We then obtain an output feature map of
size c × w × h, where c is the number of object classes. By
applying global average pooling and a sigmoid function to
this map, we obtain a classiﬁcation score S. We then up-
date FickleNet using the sigmoid cross-entropy loss func-
tion, which is widely used for multi-label classiﬁcation.

3.2. Inference Localization Map

We can now obtain various classiﬁcation scores from
a single image, which correspond to randomly selected
combinations of hidden units, and each random selec-
tion generates a various localization map. Section 3.2.1
describes how to obtain a localization map from each
random selection, and Section 3.2.2 describes how the
maps from the random selections are aggregated into a
single localization map.

3.2.1 Grad-CAM

We use gradient based CAM (Grad-CAM) [25], which is a
generalization of class activation map (CAM) [37], to ob-

5270

tain localization maps. Grad-CAM discovers the class spe-
ciﬁc contribution of each hidden unit to the classiﬁcation
score from gradient ﬂows. We compute the gradients of the
target class score with respect to x, which is the feature map
before expansion, and then sum the feature maps along the
channel axis, weighted by these gradients. We can express
Grad-CAM for each target class c as follows:

Grad-CAMc = ReLU(X

xk ×

k

∂Sc
∂xk

),

(1)

where xk ∈ Rw×h is the kth channel of the feature map x,
and Sc is the classiﬁcation score of class c.

3.2.2 Aggregate Localization Map

FickleNet allows many localization maps to be constructed
from a single image, because different combinations of hid-
den units are used to compute classiﬁcation scores at each
random selection. We construct N different localization
maps from a single image and aggregate them into a single
localization map. Let M [i] (1 ≤ i ≤ N ) denote the local-
ization map constructed from the ith random selection. We
aggregate the N localization maps so that a pixel located
at u in the aggregated map is allocated to class c if the ac-
tivation score for class c in any M [i] at u is higher than a
threshold θ. Pixels which are not allocated to any class are
ignored during training. If there is a pixel assigned to mul-
tiple classes, we examine its class score in a map averaged
over the N maps and assign the pixel to the class with the
highest score in the average map.

3.3. Training the Segmentation Network

The localization map, whose construction was described
in Sections 3.1 and 3.2, provides pseudo-labels to train a se-
mantic image segmentation network. We use the same back-
ground cues as DSRG [12]. We feed the generated localiza-
tion maps from FickleNet to DSRG as the seed cues for
weakly supervised segmentation.

For semi-supervised learning we introduce an additional
loss derived from data fully annotated by a person. Let C be
the set of classes that are present in the image. We train a
segmentation network with the following loss function:

L = Lseed + Lboundary + αLfull,

(2)

where Lseed and Lboundary respectively are the balanced seed-
ing loss and boundary loss used in DSRG [12], and

Lfull = −

1
|Fc| X

c∈C

P

c∈C

log Hu,c,

(3)

X

u∈Fc

where Hu,c is the probability of an entry of class c at lo-
cation u in the segmentation map H, and Fc is the ground-
truth mask.

4. Experiments

4.1. Experimental Setup

Dataset: We conducted experiments on the PASCAL VOC
2012 image segmentation benchmark [6], which contains
21 object classes, including one background class. Using
the same protocol as other work on weakly supervised se-
mantic segmentation, we trained our network using aug-
mented 10,582 training images with image-level annota-
tions. We report mean intersection-over-union (mIoU) for
1,449 validation images and 1,456 test images. The results
for the test images were obtained on the ofﬁcial PASCAL
VOC evaluation server.
Network details: FickleNet is based on the VGG-16 net-
work [26], pre-trained using the Imagenet [5] dataset. The
VGG-16 network was modiﬁed by removing all fully-
connected layers and the last pooling layer, and we replaced
the convolution layers of the last block with dilated convo-
lutions with a rate of 2. We set the kernel size s and the
dropout rate p to 9 and 0.9 respectively. Segmentation is
performed by DSRG [12].
Experimental details: We trained FickleNet using a mini-
batch size to 10. We cropped the training images to 321 ×
321 pixels at random locations, so that the size of feature
map x becomes 512 × 41 × 41. The initial learning rate was
set to 0.001 and halved every 10 epochs. We used the Adam
optimizer [15] with its default settings. During segmenta-
tion training, we use the same settings as DSRG [12]. We
set the number of different localization maps N for each
image to 200 and the threshold θ to 0.35. We set α to 2 for
semi-supervised learning.
Reproducibility: PyTorch [22] was used for training Fick-
leNet and conducting localization maps, and we used the
Caffe framework [13] in the segmentation step. All the ex-
periments were performed on an NVIDIA TITAN Xp GPU.

4.2. Comparison to the State of the Art

Weakly supervised segmentation: We compared our
method with other recently introduced weakly supervised
semantic segmentation methods with various levels of su-
pervision. Table 1 shows results on PASCAL VOC 2012
images. Our method outperformed others which provide
the same level of supervision through image-level annota-
tions, achieving mIoU values of 61.2 and 61.9 for validation
and test images respectively. This represents a 2.2% and
1.5% improvement respectively on validation and test im-
ages, when compared to DSRG, which is our backbone net-
work. Note that we do not need additional training steps or
additional networks, in contrast to many other recent tech-
niques, such as AfﬁnityNet [1], which requires an additional
network for learning semantic afﬁnities, or AE-PSL [31],
which requires several training steps.

Table 2 shows result on PASCAL VOC 2012 images

5271

Table 1. Comparison of weakly supervised semantic segmentation
methods on VOC 2012 validation and test image sets. The methods
listed here use DeepLab-VGG16 for segmentation.

Table 3. Comparison of semi-supervised semantic segmentation
methods on VOC 2012 validation sets. We also give the perfor-
mances of DeepLab using 1.4K and 10.6K strongly annotated data.

Methods

Training

val

test

Methods

Training Set

mIoU

Supervision: Image-level and additional annotations
MIL-seg CVPR ’15 [23]
40.6
51.2
STC TPAMI ’17 [32]
51.2
TransferNet CVPR ’16 [9]
CrawlSeg CVPR ’17 [10]
58.7
62.1
AISI ECCV ’18 [11]

700K
50K
70K
970K
11K

42.0
49.8
52.1
58.1
61.3

50.7
52.8
53.1
55.0
58.6

Supervision: Image-level annotations only
SEC ECCV ’16 [16]
CBTS-cues CVPR ’17 [24]
TPL ICCV ’17 [14]
AE_PSL CVPR ’17 [31]
DCSP BMVC ’17 [2]
MEFF CVPR ’18 [8]
GAIN CVPR ’18 [19]
MCOF CVPR ’18 [30]
AfﬁnityNet CVPR ’18 [1]
DSRG CVPR ’18 [12]
MDC CVPR ’18 [33]
FickleNet (Ours)

10K
10K
10K
10K
10K
10K
10K
10K
10K
10K
10K
10K

55.3
56.2
58.4
59.0
60.4
61.2

-

51.1
53.7
53.8
55.7
59.2
55.6
56.8
57.6
60.5
60.4
60.8
61.9

Table 2. Comparison of weakly supervised semantic segmentation
methods on VOC 2012 validation and test image sets. The methods
listed here use ResNet-based segmentation models.

Methods

Backbone

MCOF [30]
DCSP [2]
DSRG [12]
AfﬁnityNet [1]
FickleNet (ours)

ResNet 101
ResNet 101
ResNet 101
ResNet 38
ResNet 101

val

60.3
60.8
61.4
61.7
64.9

test

61.2
61.9
63.2
63.7
65.3

with a ResNet-based segmentation network. We achieved
mIoU values of 64.9 and 65.3 for validation and test im-
ages respectively using DeepLab-v2-ResNet101. This rep-
resents 3.5% and 2.1% improvement, respectively, on val-
idation and test images, when compared to DSRG. Afﬁn-
ityNet [1] uses ResNet-38 based network [34], which has
more powerful representation ability than ResNet-101.

Our method also signiﬁcantly outperforms methods
based on additional supervision except AISI [11]. These
methods include TransferNet [9], which was trained on
pixel-level annotations of 60 classes (not Pascal VOC
classes) of COCO [20] images, and CrawlSeg [10], which
was provided with a very large number of unlabeled
YouTube videos. AISI [11] utilized salient instance detec-
tor [7] which is trained using well-annotated instance-level
annotations.

Figure 4 shows qualitative results of predicted segmen-
tation masks, in FickleNet and DSRG. The supervision pro-

DeepLab [3]

1.4K strong

WSSL [21]
GAIN [19]
MDC [33]
DSRG [12] (baseline)
FickleNet (ours)

1.4K strong + 9K weak
1.4K strong + 9K weak
1.4K strong + 9K weak
1.4K strong + 9K weak
1.4K strong + 9K weak

DeepLab [3]

10.6K strong

62.5

64.6
60.5
65.7
64.3
65.8

67.6

Table 4. Run time and GPU memory usage for training and CAM
extraction without and with map expansion.

Methods

Training

CAM Extract GPU Usage

Naive
Expansion

20 sec/iter
1.3 sec/iter

2.98 sec/img
0.21 sec/img

8.4 GB
10.1 GB

Table 5. Comparison of mIoU scores using different dropout rates
(p) on PASCAL VOC 2012 validation images.

Methods

Dropout Rate (p) mIoU

Deterministic

General Dropout

FickleNet

0.0
0.5
0.9
0.3
0.5
0.7
0.9

56.3
45.6
49.1
58.8
59.4
60.0
61.2

vided by FickleNet produces larger and more accurate re-
gions of a target object than that used in DSRG, allowing
the segmentation network to consider a wider range of tar-
get objects. Thus, the segmentation network trained with lo-
calization maps generated by FickleNet produces more ac-
curate results than DSRG in that FickleNet can make fewer
false positives and cover larger regions of a target object.

Semi-supervised segmentation: Table 3 shows that the
mIoU of 65.8 produced by our method, trained on only
13.8% of images with pixel-level annotations in the PAS-
CAL dataset, was 97.3% of that of Deeplab, which is trained
with fully annotated data. The performance of FickleNet on
validation images was 1.5% better than that of DSRG which
is our baseline network. Note that GAIN shows lower per-
formance than Deeplab, trained on only 1.4K fully anno-
tated data. GAIN uses pixel-level annotations for the train-
ing of a classiﬁer, rather than a segmentation network so that
pixel-level ground-truth indirectly affects the training of the
segmentation network. Figure 4 shows examples of seg-
mentation maps from DSRG and FickleNet, which demon-
strate that our system is able to operate satisfactorily in a
semi-supervised manner.

5272

Input Image

Ground Truth

Weakly Supervised

Semi-supervised

DSRG

Ours--VGG16

Ours--ResNet101

DSRG

Ours

Figure 4. Examples of predicted segmentation masks for Pascal VOC 2012 validation images in weakly and semi-supervised manner.

9
.
0
=
p

7
.
0
=
p

5
.
0
=
p

3
.
0
=
p

N

300

200

100

50

10

(a)

0

57                       62

mIoU

(b)

Figure 5. (a) Localization maps from each random selection of hidden unit with different dropout rates p. (b) Performance on Pascal VOC
2012 validation images for different N .

4.3. Ablation studies
Effects of the Map Expansion Technique: In order to
show the effect of the map expansion technique presented
in Section 3.1.1, we compare runtime and GPU usage of
a naive implementation of FickleNet (Fig. 2(a)) with that
of an implementation of FickleNet with map expansion
(Fig. 2(b)). Table 4 shows that training and CAM extrac-
tion times are reduced factors of 15.4 and 14.2 respectively,
at a cost of 12% in GPU memory usage.

Analysis of Iterative Inference: We compare mIoU scores
with different numbers of localization maps N from a single
image. Figure 5(b) shows that the mIoU increases with the
number of maps N . We interpret this as meaning that addi-
tional random selection identiﬁes more regions of a target
object, so that larger regions of that object are represented
by the aggregated localization map. If N is greater than 200,
the mIoU converges to 61.2. Examples of different CAMs
obtained from a single image are shown in Figure 5(a).

Table 6. Effectiveness of each step. G− general dropout, S−
stochastic selection, D− deterministic approach.

Training
Inference

G
G

G
S

G
D

S
S

S
D

D
D

mIoU

49.1

55.5

57.1

61.2

59.6

59.0

Effects of dropout rate: We analyzed the effects of the
dropout rate used by FickleNet. Figure 6 shows that a
dropout rate p of 0.9 allows FickleNet to cover larger re-
gions of the target object than DSRG, which uses the local-
ization maps from deterministic classiﬁers. Higher dropout
rates also lead to more widely activated localization maps,
because it becomes more likely that the discriminative part
of an object will be dropped, leaving the non-discriminative
parts of the object to be considered for classiﬁcation. Con-
versely, if the dropout rate is low, the discriminative parts of
objects are unlikely to be dropped, and they usually sufﬁce
for classiﬁcation; so the classiﬁer is unlikely to activate non-

5273

Figure 6. Localization maps from DSRG and FickleNet, with various dropout rates (p = 0 denotes a deterministic network), and from the
general dropout method. Localization maps of DSRG (the 2nd column) were visualized using the publicly available DSRG localization cue.

discriminative parts. As shown in Figure 5(a), FickleNet
with a low dropout rate tends to activate only the discrimina-
tive part of objects, even though random sampling produces
many patterns of hidden units. Higher dropout rates result
in more randomness in the activated patterns so that differ-
ent non-discriminative parts of an object are more likely to
be considered for each random selection. This effect is also
reﬂected in the quantitative results shown in Table 5.

Comparison to general dropout: We compared FickleNet
with a network created using a general dropout method
rather than the hidden unit selection. Figure 6 shows that
localization maps from the network created with general
dropout tend to show noisy activation: hidden units which
are not sampled cannot contribute to the class score during a
forward pass, which means these dropped units do not con-
tribute to the localization map. Note that a hidden unit in
FickleNet may be activated at some window positions and
dropped at others so that every hidden unit is able to affect
the classiﬁcation score. In Table 5, a segmentation network
trained with localization maps from the network with gen-
eral dropout shows inferior results to FickleNet.

Effectiveness of each step: Table 6 shows results obtained
using several combinations of general dropout (G), stochas-
tic selection (S), and the deterministic approach (D) for
training and inference. As expected, “train S + infer D" is
better than “train D + infer D", because stochastic selection

lets the network consider the non-discriminative part, but
the best mIoU is obtained by “train S + infer S".

5. Conclusions

We have addressed the problem of semantic image seg-
mentation using only image-level annotations. By choos-
ing features at random during both training and inference,
we obtain many different localization maps from a single
image, and then aggregate those maps into a single local-
ization map. This map contains regions corresponding to
parts of objects which are both larger and more consistent
than those on a map produced by an equivalent determin-
istic technique. Our method can be implemented efﬁciently
using operations readily available on a GPU by expanding
the feature maps to avoid overlaps between the sliding ker-
nels used during convolution. We show that the results pro-
duced by FickleNet on both weakly supervised and semi-
supervised segmentation are better than those produced by
other state-of-the-art approaches.

Acknowledgements: This work was supported by the Na-
tional Research Foundation of Korea (NRF) grant funded
by the Korea government (Ministry of Science and ICT)
[2018R1A2B3001628], AIR Lab (AI Research Lab) in
Hyundai Motor Company through HMC-SNU AI Consor-
tium Fund, and the Brain Korea 21 Plus Project in 2019.

5274

References

[1] J. Ahn and S. Kwak. Learning pixel-level semantic afﬁnity
with image-level supervision for weakly supervised semantic
segmentation. arXiv preprint arXiv:1803.10464, 2018. 3, 5,
6

[2] A. Chaudhry, P. K. Dokania, and P. H. Torr. Discover-
ing class-speciﬁc pixels for weakly-supervised semantic seg-
mentation. arXiv preprint arXiv:1707.05821, 2017. 6

[3] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
volutional nets and fully connected crfs.
arXiv preprint
arXiv:1412.7062, 2014. 1, 6

[4] J. Dai, K. He, and J. Sun. Boxsup: Exploiting bounding
boxes to supervise convolutional networks for semantic seg-
mentation. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 1635–1643, 2015. 2

[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. Ieee, 2009. 5

[6] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. International journal of computer vision, 88(2):303–
338, 2010. 5

[7] R. Fan, Q. Hou, M.-M. Cheng, T.-J. Mu, and S.-M. Hu.
s4 net: Single stage salient-instance segmentation. arXiv
preprint arXiv:1711.07618, 2017. 6

[8] W. Ge, S. Yang, and Y. Yu. Multi-evidence ﬁltering and fu-
sion for multi-label classiﬁcation, object detection and se-
mantic segmentation based on weakly supervised learning.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1277–1286, 2018. 6

[9] S. Hong, J. Oh, H. Lee, and B. Han. Learning transferrable
knowledge for semantic segmentation with deep convolu-
tional neural network.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3204–3212, 2016. 6

[10] S. Hong, D. Yeo, S. Kwak, H. Lee, and B. Han. Weakly su-
pervised semantic segmentation using web-crawled videos.
arXiv preprint arXiv:1701.00352, 2017. 6

[11] S.-M. Hu. Associating inter-image salient instances for

weakly supervised semantic segmentation. 2018. 6

[12] Z. Huang, X. Wang, J. Wang, W. Liu, and J. Wang. Weakly-
supervised semantic segmentation network with deep seeded
region growing.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 7014–
7023, 2018. 3, 5, 6

[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding.
In Proceedings
of the 22nd ACM international conference on Multimedia,
pages 675–678. ACM, 2014. 5

[14] D. Kim, D. Cho, D. Yoo, and I. S. Kweon. Two-phase learn-
ing for weakly supervised object localization. arXiv preprint
arXiv:1708.02108, 2017. 3, 6

[15] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 5

[16] A. Kolesnikov and C. H. Lampert. Seed, expand and con-
strain: Three principles for weakly-supervised image seg-
mentation.
In European Conference on Computer Vision,
pages 695–711. Springer, 2016. 3, 6

[17] P. Krähenbühl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In Advances
in neural information processing systems, pages 109–117,
2011. 3

[18] S. Lee, J. Lee, J. Lee, C.-K. Park, and S. Yoon. Robust
tumor localization with pyramid grad-cam. arXiv preprint
arXiv:1805.11393, 2018. 3

[19] K. Li, Z. Wu, K.-C. Peng, J. Ernst, and Y. Fu. Tell me where
to look: Guided attention inference network. arXiv preprint
arXiv:1802.10171, 2018. 2, 3, 6

[20] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Dollár, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European conference on computer
vision, pages 740–755. Springer, 2014. 6

[21] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille.
Weakly-and semi-supervised learning of a dcnn for seman-
tic image segmentation. arXiv preprint arXiv:1502.02734,
2015. 6

[22] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017. 2, 4, 5

[23] P. O. Pinheiro and R. Collobert. From image-level to pixel-
level labeling with convolutional networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1713–1721, 2015. 6

[24] A. Roy and S. Todorovic. Combining bottom-up, top-down,
and smoothness cues for weakly supervised image segmen-
tation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 3529–3538, 2017. 6

[25] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, D. Batra, et al. Grad-cam: Visual explanations
from deep networks via gradient-based localization.
In
ICCV, pages 618–626, 2017. 4

[26] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 2, 5

[27] K. K. Singh and Y. J. Lee. Hide-and-seek: Forcing a net-
work to be meticulous for weakly-supervised object and ac-
tion localization. In The IEEE International Conference on
Computer Vision (ICCV), 2017. 2

[28] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: a simple way to prevent neural
networks from overﬁtting. The Journal of Machine Learning
Research, 15(1):1929–1958, 2014. 2, 4

[29] M. Tang, F. Perazzi, A. Djelouah, I. B. Ayed, C. Schroers,
and Y. Boykov. On regularized losses for weakly-supervised
cnn segmentation. arXiv preprint arXiv:1803.09569, 2018.
2

[30] X. Wang, S. You, X. Li, and H. Ma. Weakly-supervised se-
mantic segmentation by iteratively mining common object
features. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1354–1362,
2018. 6

5275

[31] Y. Wei, J. Feng, X. Liang, M.-M. Cheng, Y. Zhao, and S. Yan.
Object region mining with adversarial erasing: A simple
classiﬁcation to semantic segmentation approach. In IEEE
CVPR, volume 1, page 3, 2017. 2, 5, 6

[32] Y. Wei, X. Liang, Y. Chen, X. Shen, M.-M. Cheng, J. Feng,
Y. Zhao, and S. Yan. Stc: A simple to complex frame-
work for weakly-supervised semantic segmentation.
IEEE
transactions on pattern analysis and machine intelligence,
39(11):2314–2320, 2017. 6

[33] Y. Wei, H. Xiao, H. Shi, Z. Jie, J. Feng, and T. S. Huang. Re-
visiting dilated convolution: A simple approach for weakly-
and semi-supervised semantic segmentation. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 7268–7277, 2018. 3, 6

[34] Z. Wu, C. Shen, and A. v. d. Hengel. Wider or deeper: Revis-
iting the resnet model for visual recognition. arXiv preprint
arXiv:1611.10080, 2016. 6

[35] X. Zhang, Y. Wei, J. Feng, Y. Yang, and T. Huang. Adver-
sarial complementary learning for weakly supervised object
localization. In IEEE CVPR, 2018. 3

[36] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene
In IEEE Conf. on Computer Vision and

parsing network.
Pattern Recognition (CVPR), pages 2881–2890, 2017. 1

[37] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2921–2929, 2016. 1, 2, 4

5276

