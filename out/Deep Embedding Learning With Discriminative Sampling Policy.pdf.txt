Deep Embedding Learning with Discriminative Sampling Policy

Yueqi Duan1,2,3, Lei Chen1,2,3,4, Jiwen Lu1,2,3,∗, Jie Zhou1,2,3

1Department of Automation, Tsinghua University, China

2State Key Lab of Intelligent Technologies and Systems, China

3Beijing National Research Center for Information Science and Technology, China

4School of Electrical and Information Engineering, Tianjin University, China

duanyq14@mails.tsinghua.edu.cn; chen lei@tju.edu.cn; lujiwen@tsinghua.edu.cn;

jzhou@tsinghua.edu.cn

Abstract

Deep embedding learning aims to learn a distance metric
for effective similarity measurement, which has achieved
promising performance in various tasks. As the vast major-
ity of training samples produce gradients with magnitudes
close to zero, hard example mining is usually employed
to improve the effectiveness and efﬁciency of the training
procedure. However, most existing sampling methods are
designed by hand, which ignores the dependence between
examples and suffer from exhaustive searching.
In this
paper, we propose a deep embedding with discrimina-
tive sampling policy (DE-DSP) learning framework by
simultaneously training two models:
a deep sampler
network that learns effective sampling strategies, and a
feature embedding that maps samples to the feature space.
Rather than exhaustively calculating the hardness of all
the examples for mining through forward-propagation, the
deep sampler network exploits the strong prior of relations
among samples to learn discriminative sampling policy in
a more efﬁcient manner. Experimental results demonstrate
faster convergence and stronger discriminative power
of our DE-DSP framework under different embedding
objectives.

1. Introduction

Embedding learning for effective distance metric es-
timation has aroused much attention over the past few
decades [5, 10, 36, 16]. With the recent success of deep
learning [18, 23, 12, 15], deep embedding learning methods
present strong discriminative power in various tasks due to
the high nonlinearity, such as visual search [31, 30, 35], bio-
metric veriﬁcation [3, 25, 1] and zero-shot learning [2, 41].
The basic goal of deep embedding learning is to minimize

* Corresponding author

intra-class variations and maximize inter-class distances,
where numbers of objectives have been presented in the lit-
erature [3, 25, 33, 4, 29, 31, 30, 22, 35], including the most
commonly-used contrastive loss [3] and triplet loss [25].

While the sample sizes are usually quadratic or cubic in
deep embedding learning, the meaningful hard examples
only account for the tiny minority. Training with numer-
ous easy examples may suffer from inefﬁciency and poor
performance as they contribute little to the training pro-
cess by producing gradients with magnitudes close to ze-
ro [29, 14]. To this end, several hard example mining ap-
proaches have been proposed for effective sample selec-
tion [25, 42, 38, 11, 40, 39]. Recent studies have shown
that sampling plays an equal or even more crucial role than
the objective function in deep embedding learning [38].

An ideal sampling strategy should be targeted and adap-
tive. On one hand, the selected samples should target at
the requirements of the current state of embedding. On
the other hand, the sampling strategy should be adaptive-
ly updated during the training process of deep embedding
learning. Unfortunately, it is not easy to achieve the goals
for most existing sampling methods designed by hand as a
huge number of candidates are required to perform forward-
propagation and exhaustive search (to be targeted) at ev-
ery step (to be adaptive) of deep embedding learning. To
avoid the infeasible computation, most exsiting sampling
strategies employ an online method by selecting effective
examples within a small mini-batch as a suboptimal solu-
tion [25, 38]. However, limited sampling space may lead to
slow convergence and poor local optima.

In this paper, we consider that exhaustive search is far
from required for sampling. For example, our humans only
need a few attempts to acquire that there are large inter-class
distances between two speciﬁc classes with 100 samples in
each class, which are not probably able to construct hard
triplets. However, exhaustive search suffers from 1,980,000
(200 × 99 × 100) hardness calculations. It should be noted

14964

Figure 1. An overview of the proposed DE-DSP framework with the widely-used triplet embedding for an easy illustration. In the ﬁgure,
DSN is the deep sampling network, and M represents the feature embedding. Deep models in solid lines indicate ﬁxed parameters, and
the ones in dashed lines are ﬁne-tuned. We ﬁrst select a mini-batch of samples with high posibilities through the deep sampling network,
where the size of the mini-batch is better selected as an integral divisor of the embedding batchsize. Then, we receive the scores of the
samples from the feature embedding through forward-propagation, which are utilized to ﬁne-tune the deep sampler network. We iteratively
perform Step 1 and Step 2 until having generated enough triplets for the whole batch. Lastly, we train the feature embedding with the batch
of training samples.

that each hardness calculation requires to perform forward-
propagation through the deep embedding network, which
is usually very deep to achieve strong discriminative pow-
er. The main reason is that our humans have the ability
to capture the relations among candidates, while searching
based methods process each candidate independently. To
this end, we propose a deep embedding with discrimina-
tive sampling policy (DE-DSP) learning framework, where
we design a deep sampler network (DSN) to learn effective
sampling strategies rather than exhaustive search. With the
learned sampling policy, we obtain the possibility for each
candidate sample of being selected, and the deep embed-
ding provides the scores for the selected samples to opti-
mize the sampling policy. Figure 1 shows an overview of
the proposed DE-DSP. We observe that the number of hard-
ness calculation is equal to the batchsize, which is indepen-
dent to the candidate size and much less than exhaustive
search. Compared with the existing hand-crafted sampling
methods, there are two key advantages of DE-DSP:

1) To our best knowledge, it is the ﬁrst attempt to learn a
deep neural network for discriminative sampling pol-
icy. Compared with existing searching based meth-
ods which are especially designed for a few losses by
hand (usually contrastive and triplet losses), DE-DSP
is more general to various embedding structures and
does not require strong prior sampling knowledge of
human by directly consuming the embedding objec-
tive as the reward function. Moreover, DSN provides
the possibilities of candidates of being chosen rather
than a straightforward decision. As only choosing hard
samples may be sub-optimal [38], easy samples will
still have chances to be selected by DSN.

propagation on all (or a subset of) the examples
through a very deep embedding network. Instead, D-
SN learns an effective policy with only a few score
tests, which can be generalized to those untested sam-
ples. We consider that DSN possesses such general-
ization ability as it exploits the relation of distribution
consistency between the tested and untested samples
as strong prior, so that the untested samples obtain rea-
sonable score estimations through the policy learned
from the tested ones. For example, an untested sample
will obtain a close score estimation to a similar tested
sample. Also, while most existing sampling method-
s are independent at varying training steps and have
to re-compute all the distances at each step, DSN has
stronger adaptability as we can simply update the pa-
rameters based on the last step.

Due to the advantages above, DE-DSP is able to s-
elect more effective training samples from larger candi-
date size. Moreover, at the end of the training process
where most samples are invalid for embedding learning,
DE-DSP is more likely to capture the remaining effective
samples with the discriminative sampling policy. As a gen-
eral framework, we develop the widely-used triplet loss [25]
and the state-of-the-art N-pair loss [29] to demonstrate the
effectiveness of the proposed DE-DSP. Experimental re-
sults show that DE-DSP successfully boosts the perfor-
mance of the original objectives and outperforms existing
hand-crafted sampling methods on the CUB-200-2011 [34],
Cars196 [17], Stanford Online Products [31] and In-Shop
Clothes Retrieval [21] datasets.

2. Related Work

2) Existing methods assume that the hardness of each
candidate is independent and have to perform forward-

Metric Learning: Metric learning aims to obtain effec-
tive similarity measurements between input samples, where

4965

great progress has been made over the past few decades.
Conventional methods learn a linear Mahalanobis distance
to replace the simple Euclidean distance [26, 27, 9, 5, 10,
36, 19, 16]. For example, Weinberger et al. [36] separat-
ed samples from different classes by a large margin for k-
nearest neighbor classiﬁcation. Jain et al. [16] studied low-
dimensional metrics as a regularization including low-rank
and sparse metrics. While kernel tricks are employed to ad-
dress the nonlinear correlations of samples [37, 7], recent
deep metric learning methods present stronger discrimina-
tive power with the development of deep learning [3, 25].
Contrastive loss [3] focused on the absolute distances with-
in a margin for the pairwise input samples. Triplet loss [25]
constructed triplet input samples to ensure the relative dis-
tance ordering between positives and negatives. In recent
years, several effective objectives have been proposed by
constraining on more samples [33, 14, 31, 29, 8]. Repre-
sentative methods include histogram loss [33] and position-
dependent deep metric (PDDM) [14] for quadruplets, and
lifted structure [31] and N-pair loss [29] for the whole
batches. However, most training examples contribute lit-
tle for deep metric learning and data sampling is required to
enhance the effectiveness and efﬁciency.

Hard Example Mining: Hard example mining is wide-
ly applied to many tasks for effective model training [28, 42,
38, 6, 20]. In deep embedding learning, hard example min-
ing acts as bootstrapping by gradually selecting hard sam-
ples in the embedding space [28, 11]. For example, Schrof-
f et al. [25] trained FaceNet with “semi-hard” triplets, where
negative-anchor pairs had small distances but still farther
than positive-anchor pairs. Harwood et al. [11] presented a
smart mining strategy to improve the efﬁciency of data sam-
pling. Yuan et al. [40] adaptively trained the feature embed-
ding at multiple hard levels in a hard-aware deeply casaded
(HDC) manner. Wu et al. [38] proposed a margin based loss
for distance weighted sampling and demonstrated the sig-
niﬁcant importance of sampling in deep embedding learn-
ing. However, these methods are hand-crafted and suffer
from limited searching space. Rather than searching for ef-
fective training examples at every training step, we focus on
adaptively learning discriminative sampling policies.

3. Proposed Approach

Let X = {x1, x2, · · · , xn} be the input data and yi ∈
{1, 2, · · · , C} be the label of xi. Deep embedding learning
aims to train a deep forward network f (·; θ) : X → Rd,
where x is the input and θ is the learnable parameters. For
simplicity, we omit x and θ from f (x; θ) with superscripts
and subscripts inherited. We deﬁne the distance between a
pair of embedded features as Dij = ||fi −fj||2, where ||·||2
represents the Euclidean norm. In general, the goal of deep
embedding learning is to minimize the distances between
positive pairs and maximize the distances between negative

pairs. To achieve this, most deep embedding approaches
train the network with a well-designed objective Lemb(·; θ),
where representative objectives include contrastive loss and
triplet loss.

Contrastive loss focuses on absolute distances by taking
pairwise samples {xi, xj} as inputs. The objective function
of contrastive loss is shown as follows:

Lcont = 1{yi = yj}D2

ij + 1{yi 6= yj}[α − Dij]2
+,

(1)

where α is the margin and the operation of [·] represents the
hinge function max (0, ·).

Triplet loss constructs triplet samples {xa, xp, xn} for
training, which represent anchor, positive and negative sam-
ples, respectively. Compared with contrastive loss, triplet
loss only requires the relative ordering of distances:

Ltri = [D2

ap − D2

an + α]+.

(2)

Optimizing with all training pairs or triplets suffer from
infeasible computation as the sizes of samples are quadratic
or cubic. Hence, hard example mining methods are em-
ployed to select meaningful samples within a batch. Simple
hard negative mining is able to accelerate the convergence
of contrastive loss, while semi-hard criterion is presented
by FaceNet [25] for triplet loss:

x∗
n = arg min

xn:Dap<Dan

Dan,

(3)

where x∗
given anchor and positive samples.

n represents the selected semi-hard negative sample

3.1. Deep Sampler Network

In this paper, instead of computing the costly arg min
and arg max functions, we train a deep sampler network to
select effective training samples with a softmax policy. The
probability of choosing one sample is given by:

p(xs|X; η) =

exp (H(xs; η))

s} exp (H(x; η))

,

(4)

Px∈{x

where xs = {x(1), x(2), · · · , x(L)} represents a selected
sample, e.g. {xi, xj} for pairwise inputs and {xa, xp, xn}
for triplet inputs, and {xs} is the set of candidate train-
ing samples. H(xs; η) is deﬁned as the instructiveness of
the selected training sample xs, which measures how ef-
fective it is to train the embedding. Previous studies show
that H(xs; η) should be highly related to the hardness of
the selected sample [25, 38], and the deep sampler network
g(·; η) predicts H(xs; η) for each candidate sample with the
parameters η.

While the learned sampling policy is able to predict the
instructiveness of the candidate samples without exhaustive
search, the number of candidate training samples xs is still

4966

(a) Sampler for Triplet Embedding

(b) Sampler for N-pair Embedding

Figure 2. The architectures of deep sampler network for (a) triplet embedding, and (b) N-pair embedding. For triplet embedding, the ﬁrst
sampler selects the positive example and the negative class, and the second sampler chooses the negative example. For N-pair embedding,
the sampler independently selects N probe samples to construct N pairs with the gallery samples. (Best viewed in color.)

exponential. To address this, we employ L networks to se-
quentially select x(l) for the sample xs with the conditional
probability formula:

p(xs|X; η) = p(x(1), x(2), · · · , x(L)|X; η)

=

L

Yl=1

p(x(l)|x(1), · · · , x(l−1), X; η), (5)

where η = {η1, · · · , ηL} is the parameters of the networks.
In order to train the deep sampler network, the feature
embedding should judge the quality of each selected sam-
ple xs with a score function S(xs; θ), where more instruc-
tive samples gain higher scores. The goal of the deep sam-
pler network is to maximize the expected scores with the
sampling policy:

Ls(X; g, f ) = −Ex

s∼p(x

s|X;η)[S(xs; θ)],

(6)

and we omit the subscripts of Ex

s∼p(x

s|X;η) for short.

When f is ﬁxed, (6) has a global optima of

p(xs|X; η) =(1

0 else.

if xs = arg max

x

S(x; θ),

The global optima of (6) is equal to the result of ex-
haustive search in hand-crafted methods under a prop-
er deﬁnition of S, such as a score of hardness or semi-
hardness. However, the proposed deep sampler network re-
quires much less hardness tests and allows larger sampling
space. In this paper, we set S(xs; θ) = Lemb(xs; θ) for sim-
plicity, where Lemb is the loss function employed for deep
embedding learning.1 Moreover, it is necessary to avoid

1In the experiments, we minus the average score of the mini-batch for

each S(xs; θ) to provide both positive and negative scores.

repetitively selecting the same sample with a high score in
a mini-batch. A simple yet effective solution is to uniform-
ly specify x(1) (e.g. anchors in the triplet embedding) from
all the classes by hand, and select the others to construct
training examples with the learned sampling policy.

While it is usually hard to score each x(l) in the training
sample, we train all the L networks with the holistic score
S(xs; θ) considering L is relatively small. With the equali-
ty ∂
∂η ln p(xs|X; η), we obtain
the derivative of the expected reward as follows:

∂η p(xs|X; η) = p(xs|X; η) ∂

∂
∂η

E[S(xs; θ)] = E[S(xs; θ)

∂
∂η

p(xs|X; η)].

(8)

We ﬁrst sample xs according to the current policy, and
compute its score S(xs; θ) through embedding. In this way,
we obtain a mini-batch of xs with scores. Then, we rewrite
p(xs|X; η) with (5) as the product of the outputs of L net-
works. With each selected xs, ηl only affects the output of
the l-th network, and we can compute the derivative of each
network respectively.

(7)

3.2. Joint Deep Sampler and Embedding Learning

In DE-DSP, we jointly train the deep sampler network
and the feature embedding in an iterative manner, which
is a general framework and a wide range of existing su-
pervised embedding losses Lemb(·; θ) are applicable. The
deep sampler network learns discriminative sampling poli-
cy to generate targeted examples for the feature embedding,
and the parameters of embedding are optimized with the ef-
fective training examples. Algorithm 1 details the training
procedure of DE-DSP. In the following, we discuss the pro-
posed DE-DSP framework with two different losses: triplet
embedding and N-pair embedding.

4967

DE-DSP for Triplet Embedding: Triplet loss [25] is
widely-used in feature embedding, which is formulated as
(2).
In order to avoid repetitive sampling within a mini-
batch, we randomly select the anchors from varying classes
at each time, and learn the sampling policies of positive and
negative examples given the anchor with L = 2. Figure 2
(a) shows the network architecture of the sampler for triplet
embedding, where the two networks share the parameters
of the convolutional and fully connected layers. For the ﬁrst
network, we pile all the anchors (with the current anchor for
constructing the triplet as the ﬁrst) and candidate positives
up as the input, simultaneously employing two softmax lay-
ers for positive example and negative class selections. The
reason for negative class determination is to reduce the di-
mension of the softmax layer for the second network. The
second network takes the current anchor, the selected posi-
tive example and the candidate negative examples from the
choosing class as the input, and obtains the selected nega-
tive example through softmax.

DE-DSP for N-pair Embedding: N-pair loss [29] is a
recent embedding learning structure, where each batch con-
tains N pairs of probe and gallery samples. Each probe
sample together with all the gallery samples would con-
struct an (N + 1)-tuplet, which includes a positive example
and N negative examples. The objective function of N-pair
loss is written as follows:

Lnpair =

1
N

N

Xi=1

log (1 +Xj6=i

exp (f T

i f +

j − f T

i f +

i )),

(9)

where xi and x+
the readers referring to [29] for complete details.

i are probe and gallery samples. We suggest

While the N-pair loss achieves the state-of-the-art perfor-
mance, very few works research on mining more effective
batches. In DE-DSP, we aim to learn the policy of selecting
N probe samples with the pre-set gallery samples. Given
N gallery samplers, the selection of each probe sample is
independent due to the decomposability of (9). Therefore,
we employ the same deep sampler network to select each
probe sample separately. Figure 2 (b) shows the structure
of deep sampler network for N-pair embedding.

4. Experiments

We conducted experiments on the widely-used CUB-
200-2011 [34], Cars196 [17], Stanford Online Products [31]
and In-Shop Clothes Retrieval [21] datasets following the
standard evaluation protocols. The CUB-200-2011 dataset
contains 200 bird species with 11,788 images. We applied
the ﬁrst 100 species with 5,864 images for training, and the
rest 100 species with 5,924 images for testing. The Cars196
dataset includes 196 car models with 16,185 images. We
applied the ﬁrst 98 models with 8,054 images for training,
and the remaining 98 models with 8,131 images for testing.

Algorithm 1: DE-DSP
Input: Training set, number of mini-batches in each batch

K, and iteration number T .

Output: Parameters of the deep sampler network η, and

parameters of the feature embedding θ.

1: Initialize the parameters of feature embedding θ.
2: Pre-train the parameters of deep sampler network η.
3: for iter = 1, 2, · · · , T do
4:

for k = 1, 2, · · · , K do

5:

6:

7:

8:

Sample mini-batch of m training examples with

the sampling policy p(xs|X; η).

Set the mini-batch in the batch.
Compute the scores S(xs; θ) through forward-

propagation of the feature embedding.

Update the parameters of deep sampler network

η with the mini-batch using (8).

9:

10:

end for
Update the parameters of feature embedding θ with

the batch using Lemb.

11: end for
12: return η and θ.

The Stanford Online Products dataset has 22,634 products
from eBay.com with 120,053 images. We applied the ﬁrst
11,318 products with 59,551 images for training, and the
rest 11,316 products with 60,502 images for testing. The
In-Shop Clothes Retrieval dataset consists of 11,735 class-
es of clothes with 54,642 images. We applied the ﬁrst 3,997
classes with 25,882 images for training, 3,985 classes with
14,218 images for query, and the rest 3,985 classes with
12,612 images for gallery.

In the training procedure, We ﬁrst normalized each im-
age to 256 × 256 and then performed standard data aug-
mentation by random crop and horizontal mirroring. We
initialized all the embedding networks for baseline and pro-
posed methods with GoogLeNet [32] pretrained on the Ima-
geNet ILSVRC dataset [24], adding a fully connected layer
with random initialization. We trained the additional fully
connected layer with 10 times learning rate compared with
other layers. As the performance is not largely affected by
embedding sizes [31], we ﬁxed the embedding length as 512
throughout the experiments. The batchsizes are 120 for both
triplet embedding and N-pair embedding, and the size of a
mini-batch for triplet loss is 30.

In the experiments, we employed both retrieval and clus-
tering tasks for effectiveness demonstration. For the clus-
tering task, we computed the normalized mutual informa-
tion (NMI) and F1 metrics on the test set with the K-
means algorithm. NMI employs the ground truth classes
C = {c1, · · · , cK} and a set of clusters Ω = {ω1, · · · , ωK}
as the input, where ci represents the samples with the label
i, and ωj is the samples belonging to the jth cluster. NMI is

4968

Table 1. Experimental results (%) on the CUB-200-2011 and Cars196 datasets compared with the widely-used baseline methods.

Method

DDML
Lifted
Clustering
Angular
DAML

Triplet
Semi-hard (Triplet)
DE-DSP (Triplet)

N-pair
DE-DSP (N-pair)

CUB-200-2011

Cars196

NMI

F1

R@1 R@2

R@4

NMI

F1

R@1 R@2

R@4

47.3
56.4
59.2
61.0
61.3

49.8
50.3
53.7

60.2
61.7

13.1
22.6

-

30.2
29.5

15.0
16.4
19.8

28.2
30.5

31.2
46.9
48.2
53.6
52.7

35.9
37.9
41.0

51.9
53.6

41.6
59.8
61.4
65.0
65.4

47.7
50.4
53.2

64.3
65.5

54.7
71.2
71.8
75.3
75.5

59.1
63.0
64.8

74.9
76.9

41.7
57.8
59.0
62.4
66.0

52.9
53.3
55.0

62.7
64.4

10.9
25.1

-

31.8
36.4

17.9
18.5
22.3

31.8
33.3

32.7
59.9
58.1
71.3
75.1

45.1
52.4
59.3

68.9
72.9

43.9
70.4
70.6
80.7
83.8

57.4
65.2
71.3

78.9
81.6

56.5
79.6
80.3
87.0
89.7

69.7
75.1
81.3

85.8
88.8

Table 2. Experimental results (%) on the Stanford Online Products and In-Shop Clothes Retrieval datasets compared with the widely-used
baseline methods.

Method

DDML
Lifted
Clustering
Angular
DAML

Triplet
Semi-hard (Triplet)
DE-DSP (Triplet)

N-pair
DE-DSP (N-pair)

Stanford Online Products

In-Shop Clothes Retrieval

NMI

F1

R@1 R@10

R@100

R@1

R@10 R@20 R@30

R@40

83.4
87.2
89.5
87.8
89.4

86.3
86.9
87.4

87.9
89.2

10.7
25.3

-

26.5
32.4

20.2
21.7
22.7

27.1
30.6

42.1
62.6
67.0
67.9
68.4

53.9
55.9
58.2

66.4
68.9

57.8
80.9
83.7
83.2
83.5

72.1
73.5
75.8

82.9
84.0

73.7
91.2
93.2
92.2
92.3

85.7
86.7
88.4

92.1
92.6

24.4
75.3

-

80.4
78.9

56.1
57.0
58.7

76.4
78.6

47.8
93.1

-

93.9
93.8

82.0
82.8
84.4

93.6
93.8

55.6
95.5

-

95.7
95.7

86.8
87.2
89.3

94.7
95.5

60.4
96.4

-

96.5
96.6

89.2
90.1
91.5

95.6
96.2

64.2
97.0

-

97.1
97.1

90.6
91.6
92.8

96.2
96.7

the ratio of mutual information and the mean entropy of the
ground truth and clusters NMI(Ω, C) = 2I(Ω;C)
H(Ω)+H(C) , and
F1 metric is deﬁned as the harmonic mean of precision and
recall F1 = 2P R
P +R . For the retrieval task, we calculated the
percentage of test samples that had at least one sample from
the same class in R nearest neighbors.

4.1. Quantitative Results

We applied the DE-DSP framework on triplet loss [25]
and N-pair loss [29] for direct comparisons, and also com-
pared with widely-used baseline methods including D-
DML [13], lifted structure [31], clustering [30], angular
loss [35] and DAML [6]. In the experiments, we also tested
the performance of triplet loss with semi-hard negative min-
ing strategy for reference. Among the listed methods, con-
trastive loss and DDML are weakly supervised as they only
require pairwise inputs, while other methods are strongly
supervised with full annotations of identities.

Tables 1 and 2 show the experimental results on the
CUB-200-2011, Cars196, Stanford Online Products and In-
Shop Clothes Retrieval datasets respectively, where bold

numbers show the results which are improved with the
learned sampling policy compared with random/semi-hard
strategies. We observe that the proposed DE-DSP frame-
work successfully boosts the performance of existing triplet
embedding and N-pair embedding due to the selection of
effective training examples.
In general, the improvement
in Cars196 is larger than CUB-200-2011, where a possible
reason is that the images in Cars196 have smaller variations
and require more carefully sampling. For triplet embed-
ding, it is crucial to select effective training examples, and
the proposed DE-DSP leads to relatively large improvemen-
t. Semi-hard sampling strategy is also tested for training da-
ta mining. However, as a hand-crafted method, semi-hard
negative mining suffer from exhaustive search and limited
searching space.
Instead, the proposed DE-DSP exploit-
s the relationships between samples to learn an effective
sampling policy without additional prior sampling knowl-
edge of human. For N-pair embedding, while most existing
sampling strategies are not applicable as they are especially
designed for contrastive loss and triplet loss, DE-DSP is a
general framework which can be employed to varying em-

4969

Figure 3. Recall scores and loss plots on the CUB-200-2011 and Cars196 datasets. (a) Left: CUB-200-2011. Right: Cars196. Solid lines
represent the proposed DE-DSP based method and dash lines represent triplet loss. Lines in the same color share the same value of R. (b)
Top: CUB-200-2011. Bottom: Cars196. (Best viewed in color.)

(a) Recall Scores

(b) Loss Plots

bedding losses. N-pair embedding extends the triplet loss
to an (N + 1)-tuplet loss allowing more comparisons within
a batch, which partly addresses the problem of meaningful
training sample selection. Therefore, the improvement is
relatively small for the proposed DE-DSP compared with
the triplet embedding, yet it still improves the results and
achieves the state-of-the-art performance on both datasets.

Figure 3 shows the validation recall scores under differ-
ent R and loss plots of triplet and DE-DSP (triplet) embed-
dings on the CUB-200-2011 and Cars196 datasets. The loss
plots iterate over one batch and we drew each point training
the deep embedding with a batch. We observe that the DE-
DSP based method selects highly effective training sam-
ples, so that the recall scores increase rapidly and contin-
uously outperform triplet loss with random sampling. The
loss plots present signiﬁcant differences between triplet em-
bedding and DE-DSP (triplet) methods, which suffer from
severe vibration for the triplet embedding and descend s-
moothly for the proposed DE-DSP method. While there
are ﬂuctuations in training losses for random sampling as
it is not guaranteed to select meaningful training examples
at each step, DSN has learned the effective sampling policy
for the current embedding to ensure the decrease of train-
ing losses at each iteration. The smoothness of loss plots
demonstrate the discriminativeness of the sampling policy.

We also compared our DE-DSP with recent sampling
methods on the Cars196 dataset with the widely-used triplet
loss, which included semi-hard sampling, N-pair sampling,
and fast approximate nearest neighbour graph (FANNG) as
shown in Table 3. Semi-hard sampling is an easy but effec-
tive approach by selecting hard negatives which are farther
from the anchor than positives. N-pair sampling aims to s-
elect triplets using the structure of N-pair data. FANNG is
a recent smart mining method with low computational cost-

Table 3. Experimental results (%) on the Cars196 dataset com-
pared with varying sampling strategies.

Method

R@1

R@2 R@4 R@8

Triplet
N-pair sampling
Semi-hard sampling
FANNG sampling

45.1
46.3
52.4
58.2

57.4
59.9
65.2
70.6

69.7
71.4
75.1
78.9

79.2
81.3
84.3
86.7

DE-DSP (Triplet)

59.3

71.3

81.3

88.6

s. For fair comparisons, we ﬁxed the network architecture
as GoogLeNet and guaranteed the only difference as the
training data. We observe that DE-DSP achieves better re-
sults than the recent sampling methods, which demonstrates
the effectiveness of the learned sampling policy. Moreover,
DE-DSP can be generally applied to various training losses
compared with loss-speciﬁc methods.

4.2. Comparison in Sampling Time

We tested the computational time to select one negative
sample from 100 classes where each contained 100 images,
given an anchor and a positive sample. For the searching
method, we needed to obtain 10,000 GoogLeNet features
for all the candidates and computed 20,000 Euclidean dis-
tances, which took around 4.3s. For DE-DSP, we only per-
formed 102 forward passes of DSN (much shallower than
GoogLeNet) to determine the class, and another 102 for-
ward passes to choose the sample. The time was 0.016s,
where we bypassed exhaustive GoogLeNet feature extrac-
tion and distance computation due to the exploitation of re-
lationships among samples. As the training process of D-
SN might be costly, we ﬁne-tuned its parameters based on
the last step for quick convergence. The average time of

4970

(a) CUB-200-2011

(b) Cars196

Figure 4. Selected triplet samples with the learned sampling policy on (a) CUB-200-2011, and (b) Cars196. In the ﬁgure, A is the anchor
sample, P represents the candidate positive samples, and N represents the candidate negative samples. Through the learned sampling
policy, we obtain the possibility for each sample of being selected, where the images on the left are with high possibilities and right with
low possibilities. The deep sampler network selects the samples according to the possibilities for effective embedding learning.

ﬁne-tuning DSN was 5.6s per batch, and the total average
processing time for a batch (both sampling and embedding
learning) was 20.6s. In general, we train a lightweight DSN
model to exploit the relations among training samples, so
that it would not be required to perform exhaustive forward
passes through the heavyweight GoogLeNet model.

4.3. Qualitative Results

While the proposed DE-DSP successfully boosts the per-
formance and smooths the loss plots on the two datasets, it
is still required to visualize the selected samples for demon-
strating the effectiveness of the learned sampling policy. In
this subsection, we visualize the sampling results of the ﬁnal
deep sampler network for an intuitive observation. Figure 4
shows the sampling examples of triplet embedding on both
CUB-200-2011 and Cars196. In the ﬁgure, the ﬁrst row is
the selected anchor sample. The second and third rows are
candidate positive and negative samples, respectively. Ac-
cording to the sampler for triplet embedding shown in Fig-
ure 2 (a), the candidate negative samples are from the same
selected class for each anchor sample. As the learned poli-
cy is able to obtain the possibility of each sample, we show
the ﬁrst two positive/negative samples with highest/lowest
possibilities given the anchor sample. We observe that hard
samples, i.e. dissimilar positive samples and similar neg-
ative samples, have larger chances of being selected while
easy samples are with low possibilities. The visualization
results of sample selection demonstrate the discriminative-
ness of the learned sampling policy, which is able to mine
more meaningful training examples for effective embedding
learning. It should also be noticed that although hard sam-
ples have higher possibilities of being selected as shown in
Figure 4, easy samples still have chances to be chosen with

less opportunities. As pointed out in [38], training the deep
embedding network with only hard examples may lead to
sub-optima, and the proposed DE-DSP shares the similar
thoughts by selecting samples according to the possibilities.

5. Conclusion

In this paper, we have proposed a deep embedding with
discriminative sampling policy (DE-DSP) learning frame-
work, which can be generally applied to varying embed-
ding losses. Through joint learning of sampling policy and
feature embedding, DSN gradually selects targeted training
examples for effective training at each step. Compared with
the existing sampling strategies, DSN exploits the relations
among candidate samples to learn discriminative sampling
policy rather than exhaustive search, which does not require
strong prior sampling knowledge of human by simply set-
ting the embedding objective as the reward of DSN. More-
over, it is more adaptive to different training steps through
ﬁne-tuning the parameters based on last step, which avoids
repetitive computation at each step. Experimental result-
s show that the proposed deep sampler network is able to
learn discriminative sampling policy, and DE-DSP success-
fully boosts the performance of the widely-used triplet em-
bedding and N-pair embedding on the benchmark datasets.

Acknowledgement

This work was supported in part by the National Natural
Science Foundation of China under Grant 61672306, Grant
U1813218, Grant 61822603, Grant U1713214, and Grant
61572271. The authors would like to thank Mr. Qiyuan
Dong for valuable discussions.

4971

References

[1] Slawomir Bak and Peter Carr. One-shot metric learning for
person re-identiﬁcation. In CVPR, pages 1571–1580, 2017.
1

[2] Maxime Bucher, St´ephane Herbin, and Fr´ed´eric Jurie. Im-
proving semantic embedding consistency by metric learning
for zero-shot classifﬁcation. In ECCV, pages 730–746, 2016.
1

[3] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning
a similarity metric discriminatively, with application to face
veriﬁcation. In CVPR, pages 539–546, 2005. 1, 3

[4] Yin Cui, Feng Zhou, Yuanqing Lin, and Serge Belongie.
Fine-grained categorization and dataset bootstrapping using
deep metric learning with humans in the loop.
In CVPR,
pages 1153–1162, 2016. 1

[5] Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and
Inderjit S Dhillon. Information-theoretic metric learning. In
ICML, pages 209–216, 2007. 1, 3

[6] Yueqi Duan, Wenzhao Zheng, Xudong Lin, Jiwen Lu, and
Jie Zhou. Deep adversarial metric learning. In CVPR, pages
2780–2789, 2018. 3, 6

[20] Xudong Lin, Yueqi Duan, Qiyuan Dong, Jiwen Lu, and Jie
In ECCV, pages

Zhou. Deep variational metric learning.
689–704, 2018. 3

[21] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou
Tang. Deepfashion: Powering robust clothes recognition and
retrieval with rich annotations. In CVPR, pages 1096–1104,
2016. 2, 5

[22] Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Le-
ung, Sergey Ioffe, and Saurabh Singh. No fuss distance met-
ric learning using proxies. In ICCV, pages 360–368, 2017.
1

[23] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al.

Deep face recognition. In BMVC, pages 1–12, 2015. 1

[24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael S Bernstein, et al. Imagenet large
scale visual recognition challenge.
IJCV, 115(3):211–252,
2015. 5

[25] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniﬁed embedding for face recognition and clus-
tering. In CVPR, pages 815–823, 2015. 1, 2, 3, 5, 6

[7] Zheyun Feng, Rong Jin, and Anil Jain. Large-scale image
annotation by efﬁcient and robust kernel metric learning. In
ICCV, pages 1609–1616, 2013. 3

[26] Matthew Schultz and Thorsten Joachims. Learning a dis-
tance metric from relative comparisons. In NIPS, pages 41–
48, 2004. 3

[8] Weifeng Ge. Deep metric learning with hierarchical triplet

loss. In ECCV, pages 269–285, 2018. 3

[9] Amir Globerson and Sam T Roweis. Metric learning by col-

lapsing classes. In NIPS, pages 451–458, 2006. 3

[10] Matthieu Guillaumin, Jakob Verbeek, and Cordelia Schmid.
Is that you? Metric learning approaches for face identiﬁca-
tion. In CVPR, pages 498–505, 2009. 1, 3

[11] Ben Harwood, Gustavo Carneiro, Ian Reid, and Tom Drum-
In ICCV,

mond. Smart mining for deep metric learning.
pages 2821–2829, 2017. 1, 3

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016. 1

[13] Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Discriminative
deep metric learning for face and kinship veriﬁcation. TIP,
26(9):4269–4282, 2017. 6

[14] Chen Huang, Chen Change Loy, and Xiaoou Tang. Local
In NIPS, pages

similarity-aware deep feature embedding.
1262–1270, 2016. 1, 3

[15] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens
van der Maaten. Densely connected convolutional networks.
In CVPR, pages 4700–4708, 2017. 1

[16] Lalit Jain, Blake Mason, and Robert Nowak. Learning low-
dimensional metrics. In NIPS, pages 4142–4150, 2017. 1,
3

[17] Jonathan Krause, Michael Stark, Jia Deng, and Li Feifei. 3d
object representations for ﬁne-grained categorization. In IC-
CVW, 2013. 2, 5

[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, pages 1097–1105, 2012. 1

[19] Brian Kulis. Metric learning: A survey. Foundations and

Trends in Machine Learning, 5(4):287–364, 2013. 3

[27] Shai Shalev-Shwartz, Yoram Singer, and Andrew Y Ng. On-
line and batch learning of pseudo-metrics. In ICML, 2004.
3

[28] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.
Training region-based object detectors with online hard ex-
ample mining. In CVPR, pages 761–769, 2016. 3

[29] Kihyuk Sohn.

Improved deep metric learning with multi-
class N-pair loss objective. In NIPS, pages 1849–1857, 2016.
1, 2, 3, 5, 6

[30] Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin
Murphy. Deep metric learning via facility location. In CVPR,
pages 5382–5390, 2017. 1, 6

[31] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio
Savarese. Deep metric learning via lifted structured feature
embedding. In CVPR, pages 4004–4012, 2016. 1, 2, 3, 5, 6

[32] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott E Reed, Dragomir Anguelov, Dumitru Erhan, Vincen-
t Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, pages 1–9, 2015. 5

[33] Evgeniya Ustinova and Victor Lempitsky. Learning deep
embeddings with histogram loss. In NIPS, pages 4170–4178,
2016. 1, 3

[34] Catherine Wah, Steve Branson, Peter Welinder, Pietro Peron-
a, and Serge J Belongie. The Caltech-UCSD Birds-200-2011
dataset. Technical Report CNS-TR-2011-001, California In-
stitute of Technology, 2011. 2, 5

[35] Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing
Lin. Deep metric learning with angular loss. In ICCV, pages
2593–2601, 2017. 1, 6

[36] Kilian Q Weinberger and Lawrence K Saul. Distance met-
ric learning for large margin nearest neighbor classiﬁcation.
JMLR, 10(Feb):207–244, 2009. 1, 3

4972

[37] Kilian Q Weinberger and Gerald Tesauro. Metric learning

for kernel regression. In AISTATS, pages 612–619, 2007. 3

[38] Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and
Philipp Krahenbuhl. Sampling matters in deep embedding
learning. In ICCV, pages 2840–2848, 2017. 1, 2, 3, 8

[39] Rui Yu, Zhiyong Dou, Song Bai, Zhaoxiang Zhang, Y-
ongchao Xu, and Xiang Bai. Hard-aware point-to-set deep
metric for person re-identiﬁcation. In ECCV, pages 196–212,
2018. 1

[40] Yuhui Yuan, Kuiyuan Yang, and Chao Zhang. Hard-aware
deeply cascaded embedding. In ICCV, pages 814–823, 2017.
1, 3

[41] Ziming Zhang and Venkatesh Saligrama. Zero-shot learning
via joint latent similarity embedding. In CVPR, pages 6034–
6042, 2016. 1

[42] Jiahuan Zhou, Pei Yu, Wei Tang, and Ying Wu. Efﬁcient on-
line local metric adaptation via negative samples for person
re-identiﬁcation. In ICCV, pages 2420–2428, 2017. 1, 3

4973

