Hierarchy Denoising Recursive Autoencoders for 3D Scene Layout Prediction

Yifei Shi1, Angel Xuan Chang2, Zhelun Wu3, Manolis Savva2 and Kai Xu*1

1National University of Defense Technology

2Simon Fraser University

3Princeton University

Abstract

Indoor scenes exhibit rich hierarchical structure in 3D
object layouts. Many tasks in 3D scene understanding can
beneﬁt from reasoning jointly about the hierarchical context
of a scene, and the identities of objects. We present a varia-
tional denoising recursive autoencoder (VDRAE) that gen-
erates and iteratively reﬁnes a hierarchical representation
of 3D object layouts, interleaving bottom-up encoding for
context aggregation and top-down decoding for propaga-
tion. We train our VDRAE on large-scale 3D scene datasets
to predict both instance-level segmentations and a 3D ob-
ject detections from an over-segmentation of an input point
cloud. We show that our VDRAE improves object detection
performance on real-world 3D point cloud datasets com-
pared to baselines from prior work.

1. Introduction

The role of context in 3D scene understanding is central.
Much prior work has focused on leveraging contextual cues
to improve performance on various perception tasks such as
object categorization [12], semantic segmentation [35], and
object relation graph inference from images [62]. However,
the beneﬁt of hierarchical context priors in 3D object detec-
tion and 3D instance-level segmentation using deep learn-
ing is signiﬁcantly less explored. A key challenge in using
deep network formulations for capturing the patterns of hi-
erarchical object layout is that these patterns involve chang-
ing numbers of objects with varying semantic identities and
relative positions. In this paper, we propose a recursive au-
toencoder1 (RAE) approach that is trained to predict and
iteratively “denoise” a hierarchical 3D object layout for an
entire scene, inducing 3D object detections and object in-
stance segmentations on an input point cloud.

*corresponding author
1Also known as a recursive neural network (RvNN) autoencoder.

chair

desk

lamp

cabinet

TV

Figure 1: We present a hierarchy-aware variational denois-
ing recursive autoencoder (VDRAE) for predicting 3D ob-
ject layouts. The input is a point cloud which we over-
segment (top left). Our VDRAE constructs and reﬁnes a
3D object hierarchy, inducing semantic segmentations (top
right, category-colored point cloud), and 3D instance ori-
ented bounding boxes (bottom). The reﬁned 3D bounding
boxes tightly and fully contain observed objects.

Recent work has demonstrated that encoding the context
in 3D scene layouts using a set of pre-speciﬁed “scene tem-
plates” with restricted sets of present objects can lead to
improvements in 3D scene layout estimation and 3D object
detection [63]. However, manually specifying templates of
scene layouts to capture the diversity of real 3D environ-
ments is a challenging and expensive endeavor. Real envi-
ronments are hierarchical: buildings contain rooms such as
kitchens, rooms contain functional regions such as dining
table arrangements, and functional regions contain arrange-
ments of objects such as plates and cutlery. This implies
that explicit representation of the hierarchy structure of 3D
scenes can beneﬁt 3D scene understanding tasks such as ob-
ject detection and 3D layout prediction.

11771

Given a scene represented as a point cloud, we ﬁrst per-
form an over-segmentation. Our RvNN is trained to en-
code all segments in a bottom-up context aggregation of
per-segment information and inter-segment relations, form-
ing a segment hierarchy. The decoding phase performs a
top-down context propagation to regenerate subtrees of the
hierarchy and generate object proposals. This encoding-
decoding reﬁnement process is iterated, interleaving con-
text aggregation and hierarchy reﬁnement. By training our
denoising autoencoder in a generative fashion, this process
converges to a reﬁned set of object proposals whose layout
lies in the manifold of valid scene layouts learned by the
generative model (Figure 1). In summary, our approach is
an iterative 3D object layout denoising autoencoder which
generates and reﬁnes object proposals by recursive context
aggregation and propagation within the inferred hierarchy
structure. We make the following contributions:

• We predict and reﬁne a multi-object 3D scene layout
for an input point cloud, using hierarchical context ag-
gregation and propagation based on a denoising recur-
sive autoencoder (DRAE).

• We learn a variational DRAE (VDRAE) to model
the manifold of valid object layouts, thus facilitat-
ing layout optimization through an iterative infer-and-
generate process.

• We demonstrate that our approach improves 3D ob-
ject detection performance on large reconstructed 3D
indoor scene datasets.

2. Related work

Our goal is to improve 3D object detection by leveraging
contextual information with a hierarchical representation of
a 3D scene. Here we focus on reviewing the most relevant
work in object detection. We describe prior work that uses
context during object detection, work on object detection in
3D, and hierarchical context modeling.

Object detection in 2D. Object detection has long been
recognized as an important problem in computer vision
with much prior work in the 2D domain [11, 13, 14, 16,
25, 33, 41, 42]. Using contextual information to improve
object detection performance has also been studied exten-
sively [5, 6, 17, 38, 54]. Choi et al. [7] show that using
contextual information enables predictions in 3D from RGB
images. More recently, Zellers et al. [62] show improved
object detections by learning a global context using a scene
graph representation. These approaches operate in 2D and
are subject to the viewpoint dependency of single image in-
puts. The limited ﬁeld of view and information loss during
projection can signiﬁcantly limit the beneﬁt of contextual
information.

Object detection with RGB-D. The availability of com-
modity RGB-D sensors led to signiﬁcant advances in 3D
bounding box detection from RGB-D image inputs [9, 15,
51, 52]. However, at test time, these object detection al-
gorithms still only look at a localized region from a single
view input and do not consider relationships between ob-
jects (i.e. contextual information). There is another line
of work that performs contextual reasoning on single view
RGB-D image inputs [23, 29, 43, 45, 48, 56, 63] by lever-
aging patterns of multi-object primitives or point cloud seg-
ments to infer and classify small-scale 3D layouts. Zhang
et al. [63] model rooms using four predeﬁned templates
(each deﬁning a set of objects that may appear) to detect
objects in RGB-D image inputs. If the observed room con-
tains objects that are not in the initial template, they cannot
be detected. Another line of work on street and urban RGB-
D data uses bird’s eye view representation to capture context
for 3D object detection [2, 49, 59]. In contrast, we operate
with fused 3D point cloud data of entire rooms, and learn
a generative model of 3D scene layouts from a hierarchical
representation.

Object detection in 3D point clouds. Recently, the avail-
ability of large scale datasets [1, 3, 8] has fostered advances
in 3D scene understanding [58]. There has been an explo-
sion of methods that focus on the semantic segmentation
of point clouds [10, 18, 19, 24, 28, 30, 39, 53, 60]. How-
ever, there is far less work addressing instance segmenta-
tion or object detection in fused 3D point clouds, at room-
scale or larger. Both Qi et al. [39], Wang et al. [55] pro-
pose connected component-based heuristic approaches to
convert semantic segmentations to instances. Wang et al.
[55] is the state-of-the-art 3D point cloud instance seg-
mentation method. They use a learned point similarity
as a proxy for context. A related earlier line of work
segments a point cloud or 3D mesh input into individual
objects and then retrieves matching models from a CAD
database [4, 27, 36, 45] to create a synthetic 3D represen-
tation of the input scene. Our approach directly represents
object detections as a hierarchy of 3D bounding boxes and is
motivated by the observation that at the scale of 3D rooms,
modeling the hierarchical context of the 3D object layout
becomes important.

Hierarchical context in 3D. Hierarchical representations
have been used to learn grammars in natural language and
images [50], 2D scenes [46], 3D shapes [26, 61], and 3D
scenes [32]. A related line of work parses RGB or RGB-D
scenes hierarchically using And-Or graphs [20, 21, 34, 40,
65] for a variety of tasks. For full 3D scenes, there has been
very limited amount of available training data with ground
truth hierarchy annotations. Therefore, prior work in hier-
archical parsing of 3D scenes does not utilize high capacity
deep learning models. For example, Liu et al. [32] is lim-

1772

Training

Testing

…

Learn segments affinity

…

Train VDRAE

Over-segmentation

Hierarchy construction

Layout optimization

Figure 2: Our system involves two neural net components: a segment-segment afﬁnity prediction network which we use to
construct hierarchical groupings of 3D objects, and a variational denoising recursive autoencoder (VDRAE) which reﬁnes
the 3D object hierarchies. At test time, the afﬁnity prediction network is used to predict segment-segment afﬁnities. We
construct a hierarchy from the segment afﬁnity graph using normalized graph-cuts. The VDRAE then encodes this hierarchy
to aggregate contextual queues and decodes it to propagate information between nodes. These two stages are iterated to
produce a denoised set of 3D object detections and instance segmentations that better match the input scene.

ited to training and testing on a few dozens of manually
annotated scenes. Zhao and Zhu [64] evaluated on only 534
images. In this paper, we use a recursive autoencoder neural
network to learn a hierarchical representation for the entire
3D scene directly from large-scale scene datasets [1, 3].

3. Method

The input to our method is a 3D point cloud representing
an indoor scene. The output is a set B of objects represented
by oriented bounding boxes (OBBs), each with a category
label. We start from an initial over-segmentation S contain-
ing candidate object parts (Section 3.1). We then predict
segment pair afﬁnities and use a normalized cuts [47] ap-
proach to construct an initial hierarchy h used for context
propagation (Section 3.2). Having built the hierarchy, we
iteratively reﬁne the 3D object detections and the hierarchy
based on a recursive autoencoder network which adjusts the
structure of the hierarchy and its nodes to produce 3D object
detections at the leaf nodes (Section 3.3). We call the com-
bination of the object detections and the constructed hierar-
chy {B, h} the 3D scene layout. Our output set of labeled
bounding boxes B contains a category label for all object
detections, or a label indicating a particular box is not an
object. Figure 2 shows an overview of our method.

3.1. Initial Over-segmentation

Our input is a point cloud for which we create an initial
over-segmentation S as a starting point for our object detec-
tion. Distinct objects are represented by oriented bounding
boxes containing parts of the point cloud. We use features
of the object points as well as features of the spatial rela-
tions between objects to characterize the object layout and
to train our network such that it can detect objects.

Segment pair

MLP

Affinity
Segment graph construction

(a)

…

Hierarchy construction

(b)

Figure 3: We train an MLP to predict segment pair afﬁnities
and create a segment afﬁnity graph (a). We then construct
a hierarchy from the resulting segment afﬁnity graph using
normalized cuts (b).

a representative unsupervised method based on a greedy
graph-based approach [11] that was extended for point
clouds by [22]. Our method follows [22] in using graph cuts
for an over-segmentation of the point cloud based on point
normal differences to create the initial set of segments.

Each segment is extracted from the point cloud as an
individual set of points, for which we compute oriented
bounding boxes and point features as described in the fol-
lowing sections.

3.2. Hierarchy Initialization

During hierarchy construction we address the follow-
ing problem. The input is the initial over-segmentation S
and the output is a binary tree h representing a hierarchi-
cal grouping of the objects. Each object is represented as
a 3D point cloud with an oriented bounding box (OBB),
and a category label. The 3D point cloud is a set of points
{pi} = {xi, yi, zi, ri, gi, bi} with their 3D (x, y, z) position
and color (r, g, b). The leaves of this initial hierarchy h are
the segments and the internal nodes represent groupings of
the segments into objects and groups of objects. The root of
the tree represents the entire room.

There is much prior work that could be used to provide
an initial over-segmentation of the point cloud. We use

To construct the initial hierarchy from the input seg-
ments we ﬁrst train a multi-layer perceptron (MLP) to pre-

1773

dict segment pair afﬁnities which indicate whether the two
segments belong to the same object instance or not. The
input to the MLP is a set of features capturing segment-
segment geometric and color relationships, proposed by
prior work [57]. We also tried using learned features ob-
tained from a network trained on object-level label classi-
ﬁcation, but empirically found the features in [57] to work
better in our experiments. The MLP is trained to predict bi-
nary pair afﬁnity from these features under a squared hinge
loss. Once we computed the segment pair afﬁnities, the
segments are then grouped into a hierarchy by using nor-
malized cuts [47]. Starting from the root node, we split the
segments into two groups recursively. The splitting stops
when all groups have only one segment (leaf node). The cut
cost E(u, v) = ecea between two segments u and v in the
normalized cut is initially equal to the afﬁnity ea between
the segments, but is then adjusted by the factor ec during
layout optimization (as described in the next section). Fig-
ure 3 shows the process of our hierarchy construction.

3.3. Object Detection and Layout Reﬁnement

We describe our iterative optimization for predicting the
object layout {B, h}. We begin with the basic recursive au-
toencoder (RAE) for context aggregation and propagation.
We then discuss a denoising version of the RAE (DRAE)
designed for adjusting the object layout to better match a
observed layout in the training set. Based on that, we intro-
duce a Variational DRAE (VDRAE) which is a generative
model for object layout improvement. It maps a layout onto
a learned manifold of plausible layouts, and then generates
an improved layout to better explain the input point cloud.

l

l

r

p

, xenc

, xenc

), where xenc

p = fenc(xenc

Recursive autoencoder for context propagation. Given
the segments and the hierarchy, the recursive autoencoder
(RAE) performs a bottom-up RvNN encoding for context
aggregation, followed by a top-down RvNN decoding for
context propagation. The encoder network takes as input
the features (codes) of any two nodes to be merged (accord-
ing to the hierarchy) and outputs a merged code for their
parent node: xenc
and
xenc
denote the codes of two sibling nodes and their parent
node, respectively. fenc is a MLP with two hidden layers for
node grouping. The decoder takes the code of an internal
node of the hierarchy as input and generates the codes of its
two child nodes: [xdec
), where fdec
is a two-layer MLP decoder for node ungrouping (Figure 4).
An additional box encoder generates the initial codes
from the 3D point cloud within an OBB before the bottom-
up pass, and a box decoder generates the ﬁnal adjusted
OBBs from the codes of the leaf nodes after the top-down
pass: xenc
and
xdec
n denote the code for an node n in encoding and decod-
ing. Pn is the set of 3D points of node n. tn is the parameter

n ), where xenc

n = fpnt(Pn),

tn = fbox(xdec

] = fdec(xdec

p , xenc

, xdec

n

p

r

r

l

RvNN encoding

(cid:1858)(cid:2926)(cid:2924)(cid:2930)
(cid:1858)(cid:2926)(cid:2924)(cid:2930)
(cid:1858)(cid:2926)(cid:2924)(cid:2930)
(cid:1858)(cid:2926)(cid:2924)(cid:2930)

(cid:1858)(cid:2915)(cid:2924)(cid:2913)
(cid:1858)(cid:2926)(cid:2924)(cid:2930)
(cid:1858)(cid:2926)(cid:2924)(cid:2930)
(cid:1858)(cid:2915)(cid:2924)(cid:2913)

(cid:1858)(cid:2915)(cid:2924)(cid:2913)
(cid:1858)(cid:2915)(cid:2924)(cid:2913)

(cid:1858)(cid:2913)(cid:2922)(cid:2911)(cid:2929)(cid:2929)

Jump connection

(cid:1858)(cid:2915)(cid:2924)(cid:2913)

(cid:1858)(cid:2914)(cid:2915)(cid:2913)

KL loss

type

(cid:1858)(cid:2914)(cid:2915)(cid:2913)
(cid:1858)(cid:2912)(cid:2925)(cid:2934)

type

RvNN decoding

type

type

(cid:1858)(cid:2912)(cid:2925)(cid:2934)
(cid:1858)(cid:2912)(cid:2925)(cid:2934)

Input segment layout

Output object layout

Figure 4: Our denoising recursive autoencoder (RAE) takes
an input segment layout from an over-segmentation and
performs bottom-up encoding for context aggregation (left
side), followed by top-down decoding for context propaga-
tion (right side). The encoding-decoding process generates
a reﬁned hierarchy with 3D object detections as leaf nodes.

vector of an output OBB, encoding the offsets of its po-
sition, dimension and orientation. fpnt is a PointCNN [28]
module for box encoding, and fbox a two-layer MLP for box
decoding. The PointCNN2 module is pretrained on a clas-
siﬁcation task for predicting object category labels from the
point clouds of objects in the training set.

Denoising RAE for object detection and layout reﬁne-
ment. To endow the RAE with the ability to improve the
predicted layout with respect to a target layout (e.g. a ob-
served layout in the training set), a natural choice is to
train a denoising RAE. Given a noisy input segment lay-
out, we learn a Denoising RAE (DRAE) which generates
a denoised layout. By noise, we mean perturbations over
categorical labels, positions, dimensions and orientation of
the bounding boxes. In DRAE, denoising is accomplished
by the decoding phase which generates a new hierarchy
of OBBs that reﬁnes, adds or removes individual object
OBBs. The key for this generation lies in the node type
classiﬁer trained at each node (Figure 4) which determines
whether a node is a leaf ‘object’ node at which decoding
terminates, or an internal node at which the decoding con-
tinues: on = f node
n ), with on = 0 indicating
a leaf ‘object’ node and on = 1 an internal ‘non-object’
node For ’object’ nodes, another object classiﬁer is applied
to determine the semantic categories: cn = f obj
n ),
where cn is the categorical label for node n. For training,
we compute the IoU of all nodes in the encoding hierar-
chy against ground-truth object bounding boxes and mark
all nodes with IoU ≤ 0.5 as ‘object’.

cls (xdec

n , xenc

cls (xdec

n xenc

2Alternative encoding modules such as PointNet++ can be used instead.

1774

Algorithm 1: VDRAE 3D Scene Layout Prediction.

: Point cloud of indoor scene: P ; Trained VDRAE.

Input
Output: 3D object layout {B, h}.

1 S ← Over-segmentation(P );
2 h ← HierarchyConstruction(S, P );
3 repeat

4

5

B ← VDRAE(S, h, P );
h ← HierarchyConstruction(B, S, P );

6 until Termination condition met;
7 return {B, h};

Variational DRAE for generative layout optimization.
We train a Variational DRAE (VDRAE) to capture a man-
ifold of valid hierarchies of OBBs from our training data.
The training loss is:

L =

N

(cid:2)n

(Lnode

cls (on, o∗

n) + Lobj

cls (cn, c∗

n) + Lobj

obb(tn, t∗

n))+LKL

cls

cls

where N are all decoding nodes, Lnode
is a binary cross-
entropy loss over two categories (‘object’ vs ‘non-object’),
n is the ground-truth label, Lobj
o∗
is a multi-class cross-
entropy loss over semantic categories, o∗
n is the ground-truth
categorical label, Lobj
obb is an L1 regression loss on the OBB
parameters of the node, t∗
n is the ground-truth OBB param-
eters and LKL is the KL divergence loss at root node. Note
that the Lobj
obb terms exist only for ‘object’ nodes.
The last term serves as a regularizer which measures the KL
divergence between the posterior and a normal distribution
p(z) at the root node. This enables our VDRAE learning to
map to the true posterior distribution of observed layouts.

cls and Lobj

Layout reﬁnement using the VDRAE. Once trained, the
VDRAE can be used to improve an object layout. Due to the
coupling between object detection and hierarchy construc-
tion, we adopt an iterative optimization algorithm which al-
ternates between the two stages (see Algorithm 1). Given an
initial segment layout extracted from the input point cloud,
our method ﬁrst performs a VDRAE inference step (test-
time step) to generate a hierarchy of object bounding boxes
explaining the input point cloud. It then uses the decoding
feature to infer a new hierarchy, which will be used for the
VDRAE test in the next iteration. In the next iteration, the
binary classiﬁcation ‘object’ vs ‘non-object’ conﬁdence is
used to scale the normalized cut afﬁnity ea for two nodes u
and v using the following factor ec:

ec(u, v) = (cid:3) −log(1 − cs), u and v in same leaf node s

otherwise

0.1,

where cs is the classiﬁcation conﬁdence of node s to be la-
beled as ‘object’. The scaled afﬁnity E(u, v) = ecea is then

Figure 5: Example iterative reﬁnement of initial object lay-
out (leftmost column) under successive reﬁnement itera-
tions of our VDRAE network (columns to the right).

used to reﬁne the hierarchy construction. This process re-
peats until the structure of the hierarchy between iterations
remains unchanged. Figure 5 shows an example of the iter-
ative reﬁnement. The optimization converges with at most
5 iterations for all the scenes we have tested. This itera-
tive optimization gradually “pushes” the object layout into
the layout manifold learned by VDRAE. Please refer to the
supplemental material for a discussion of convergence.

4. Implementation Details

In this section, we describe the implementation details of
our network architectures, the relevant parameters, and the
training and testing procedures.

Initial over-segmentation and feature extraction. For
the initial over-segmentation we use threshold values k =
0.01 which we empirically found to perform well on train-
ing scenes (Section 3.1). For the PointCNN [28] features,
we train the PointCNN to predict object class labels using
the training set data. We train the network to minimize the
cross-entropy loss over 41 object classes taking 2048 points
per input and outputting to a 256-d vector for classiﬁcation.
Note that PointCNN is a pre-trained feature extractor and
we didn’t ﬁne-tune it during the training of VDRAE.

Hierarchy construction. The MLP for segment pair
afﬁnity prediction consists of 4 FC layers (with sigmoid lay-
ers). The input is a 25-d feature, and the output is a single
afﬁnity value. We use the detault parameter setting for the
solver used in normalized cuts. It takes about 0.1s to build
a hierarchy from a segment graph.

fenc has
Variational denoising recursive autoencoder.
two 1000-d inputs and one 1000-d output. fdec has one
1000-d input and two 1000-d outputs. f node
takes a 1000-
d vector as input, and outputs a binary label. f obj
takes a
cls
1000-d vector as input, and outputs a categorical label and
OBB parameter offsets. This is achieved by using a softmax
layer and a fully-connected layer. To deal with the large
imbalance between positive (‘object’) and negative (‘non-
object’) classes during training, we use a focal loss [31] with

cls

1775

γ = 0 for positives and γ = 5 for negative. This makes
the training focus on all positive samples and hard negative
samples. All the items in L can be trained jointly. However,
to make the training easier, we ﬁrst train by Lnode
cls and LKL to
make the network have the ability to distinguish whether a
cls and Lobj
node is a single object, and then ﬁne-tune by Lobj
obb.

Chair

Table

Sofa

Board mAP

Seg-Cluster [55]
Sliding PointCNN [28]
PointNet [39]
SGPN [55]

Ours (ﬂat context)
Ours

0.23
0.36
0.34
0.41

0.35
0.45

0.33
0.39
0.47
0.50

0.47
0.53

0.05
0.23
0.05
0.07

0.32
0.43

0.13
0.07
0.12
0.13

0.10
0.14

0.19
0.26
0.25
0.28

0.31
0.39

Training and testing details. We implement the segment
pair afﬁnity network and the VDRAE using PyTorch [37].
For VDRAE, We use the Adam optimizer with a base learn-
ing rate of 0.001. We use the default hyper-parameters of
β1 = 0.9, β2 = 0.999 and no weight decay. The batch
size is 8. The VDRAE can be trained in 15 hours on a
Nvidia Tesla K40 GPU. At testing time, a forward pass of
the VDRAE takes about 1s. An Non-Maximum Suppres-
sion with IOU 0.5 is performed on the detected boxes after
the inference of VDRAE.

5. Results

We evaluate our proposed VDRAE on 3D object detec-
tions in 3D point cloud scenes (see supplemental for seman-
tic segmentation evaluation).

5.1. Experimental Datasets

We use two RGB-D datasets that provide 3D point
clouds of interior scenes: S3DIS [1] and Matterport3D [3].
S3DIS consists of six large-scale indoor areas reconstructed
with the Matterport Pro Camera from three different univer-
sity buildings. These areas were annotated into 270 disjoint
spaces (rooms or distinct regions). We use the k-fold cross
validation strategy in [1] for train and test. Matterport3D
consists of semantically annotated 3D reconstructions based
on RGB-D images captured from 90 properties with a Mat-
terport Pro Camera. The properties are divided into room-
like regions. We follow the train/test split established by
the original dataset, with 1, 561 rooms in the training set
and 408 rooms in the testing set.

5.2. Evaluation

Our main evaluation metric is the average precision of
the detected object bounding boxes against the ground truth
bounding boxes at a threshold IoU of 0.5 (i.e. any de-
tected bounding box that has more than 0.5 intersection-
over-union overlap with its ground truth bounding box is
considered a match). We compare our method against
baselines from prior work on object detection in 3D point
clouds. We then present ablated versions of our method
to demonstrate the impact of different components on de-
tection performance, as well as experiments to analyze the
impact of the over-segmentation coarseness and the impact
of successive reﬁnement iterations.

Table 1: Comparison of our approach against prior work
on object detection in 3D point cloud data. Values report
average precision at IOU of 0.5 on the S3DIS dataset. Our
hierarchy-reﬁning VDRAE outperforms all prior methods.

Qualitative examples. Figure 6 shows detection results
on the Matterport3D test set (see supplement for more ex-
amples). Our VDRAE leverages hierarchical context to de-
tect and reﬁne 3D bounding boxes for challenging cases
such as pillows on beds, and lamps on nightstand cabinets.

Comparison to baseline methods. We evaluate our ap-
proach against several baselines from prior work that pro-
duce object detections for indoor 3D scene point clouds:

• Seg-Cluster: Approach proposed by [55] applies se-
mantic segmentation (SegCloud [53]) followed by Eu-
clidean clustering [44].

• PointNet: Predicts the category of points [39] and
uses breadth-ﬁrst search to group nearby points with
the same category, inducing object instances. We use
PointNet instead of other point-based neural networks
as PointNet proposed this object detection pipeline.

• Sliding PointCNN: A baseline using a 3D sliding win-

dow approach with PointCNN [28] features.

• SGPN: A state-of-the-art semantic instance segmenta-

tion approach for point clouds [55].

• Ours (ﬂat context): A baseline using a ﬂat con-
text representation instead of leveraging the hierarchy
structure, in which xdec
is the concatenation of en-
coded features xenc
and the average encoded features
n )/n.

n xenc

n

n

of all nodes ((cid:4)N

Tables 1 and 2 report average precision on the S3DIS and
Matterport3D datasets, showing that our approach outper-
forms all baselines. The ﬂat context baseline performs
worse than our hierarchy-aware VDRAE but better than the
baselines that do not explicitly represent context. Figure 7
qualitatively shows results from the Matterport3D test set,
comparing our approach with the highest performing prior
work baseline using SGPN.

Ablation of method components. We evaluate the im-
pact of each components using the following variants:

• No hierarchy: We use PointCNN [28] features for
each node to predict the object category and regress

1776

chair

desk

lamp

cabinet

TV

bed

cushion

sofa

bathtub

toilet

Figure 6: 3D scene layout predictions using our VDRAE on the Matterport3D test set. The ﬁrst column shows the input point
cloud. The second column is the over-segmentation from which we construct an initial segment hierarchy. The third column
shows the 3D object detections with colors by category. The ﬁnal two columns show bounding boxes for the detections.
Our approach predicts hierarchically consistent 3D layouts where objects such as lamps, pillows and cabinets are detected in
plausible positions and orientations relative to other objects and the global structure of the scene.

Chair

Table

Cabinet

Cushion

Sofa

Sliding PointCNN [28]

PointNet [39]

SGPN [55]

Ours (ﬂat context)

Ours

0.22
0.28
0.29

0.24
0.37

0.21
0.32
0.24

0.18
0.27

0.03
0.06
0.07

0.08
0.11

0.19
0.21
0.18

0.21
0.24

0.20
0.28
0.30

0.18
0.28

Bed

0.36
0.25
0.33

0.27
0.43

Sink

Toilet

TV

Bathtub

Lighting

mAP

0.07
0.17
0.15

0.22
0.23

0.16
0.08
0.17

0.25
0.35

0.05
0.10
0.09

0.07
0.19

0.15
0.11
0.16

0.21
0.27

0.10
0.06
0.11

0.07
0.19

0.16
0.18
0.19

0.18
0.27

Table 2: Average precision of object detection at IoU 0.5 on the Matterport3D dataset. We compare our full method (‘ours’)
against several baselines. Refer to text for the details of the baselines.

an OBB without using a hierarchy. We add 4 FC lay-
ers after the PointCNN layers to increase the number
of network parameter and make the comparison fair.

• No OBB regression: We turn off the OBB regression

module for leaf nodes and train from scratch.

• No iteration (bvh): No iteration for testing. The hi-
erarchy is constructed through recursive binary splits
considering only geometric separation between seg-

1777

(b)
chair
board

average
sofa

table

(a)

average
sofa

chair
board

table

0.8

0.6

l
l

a
c
e
R

0.4

0.2

0

0

2.5

1
2
Iteration

3

1.5

2

Segment size (m)

(c)
chair
board

average
sofa

table

0

1
2
Iteration

3

P
A
m

0.5
0.4
0.3
0.2
0.1
0

P
A
m

0.5
0.4
0.3
0.2
0.1
0

1

Figure 7: Qualitative 3D object detection results on Mat-
terport3D test set using our VDRAE (top row) and the best
performing baseline from prior work (SGPN[55], bottom
row). Our approach produces more accurate bounding box
detections and fewer category errors. For example, chairs
are correctly categorized and have tight bounding boxes at
the top left and top right.

(a) mAP plotted against over-segmentation
Figure 8:
coarseness (average segment size in meters).
(b) recall
against VDRAE iteration count. (c) mAP against VDRAE
iteration count.

ground-truth object with all node OBBs in encoding hierar-
chy. If one of the IoU values is larger than 0.5, we consider
that a match against the ground-truth. Figure 8 (c) shows
the object detection mAP plotted against iteration number.
The beneﬁt of iteration is apparent.

Chair

Table

Sofa

Board mAP

6. Conclusion

no hierarchy
no OBB regression
no iteration (bvh)
no iteration (our hier)

Ours

0.34
0.41
0.37
0.39

0.45

0.41
0.47
0.47
0.51

0.53

0.35
0.40
0.38
0.39

0.43

0.08
0.11
0.10
0.12

0.14

0.30
0.35
0.33
0.35

0.39

Table 3: Ablation of the components of our approach. Val-
ues report average precision at IoU of 0.5 on the S3DIS
dataset. Our full VDRAE outperforms all ablations.

ments, i.e. bounding volume hierarchy (bvh).

• No iteration (our hier): No iteration for testing. The
hierarchy is built by our hierarchy initialization ap-
proach.

Table 3 shows the results. The full method performs the
best. Not using a hierarchy degrades performance the most.
Removing OBB regression, and not performing iterative re-
ﬁnement are also detrimental but to a lesser extent.

Sensitivity to over-segmentation coarseness. We quan-
tify the impact of the over-segmentation coarseness thresh-
old parameter k of the method in [22] on S3DIS. We use ﬁve
threshold values k = 1.0, 0.1, 0.01, 0.001, 0.0001 to gener-
ate segments with different size and re-train the afﬁnity net-
work and VDRAE respectively. Larger k produce bigger
segments. Figure 8 (a) shows that the best performance is
achieved when average segment size is 1.45m (k = 0.01).

We presented an approach for predicting 3D scene layout
in fused point clouds by leveraging a hierarchical encoding
of the context. We train a network to predict segment-to-
segment afﬁnities and use it to propose an initial segment
hierarchy. We then use a variational denoising recursive au-
toencoder to iteratively reﬁne the hierarchy and produce 3D
object detections. We show signiﬁcant improvements in 3D
object detection relative to baselines taken from prior work.

Limitations. Our current method has several limitations.
First, the hierarchy proposal and VDRAE are trained sepa-
rately. Incorporating these two stages will leverage the syn-
ergy between parsing hierarchies and reﬁning the 3D scene
layouts. Second, the segment point features we use in our
VDRAE is trained independently on a classiﬁcation task.
These features can also be learned end-to-end, resulting in
further task-speciﬁc improvements in performance.

Future work. We have only taken a small step towards
leveraging hierarchical representations of 3D scenes. There
are many avenues to pursue for future research. Reasoning
about the hierarchical composition of scenes into objects,
object groups, functional regions, rooms, and entire resi-
dences can beneﬁt many tasks beyond 3D object detection.
We hope that our work will act as a catalyst in this promis-
ing research direction.

Acknowledgements

Effect of iteration. We evaluate the effect of VDRAE re-
ﬁnement iteration by analyzing the hierarchy and 3D object
detections at each step. Figure 8 (b) shows recall against
ground-truth objects plotted against iteration number. Re-
call is computed by calculating the IoU of the OBB of each

We are grateful to Thomas Funkhouser and Shuran Song
for the valuable discussion. This work was supported in part
by NSFC (61572507, 61532003, 61622212) and Natural
Science Foundation of Hunan Province for Distinguished
Young Scientists (2017JJ1002).

1778

References

[1] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis
Brilakis, Martin Fischer, and Silvio Savarese. 3D semantic
parsing of large-scale indoor spaces. In Proc. CVPR, 2016.

[2] Jorge Beltran, Carlos Guindel, Francisco Miguel Moreno,
Daniel Cruzado, Fernando Garcia, and Arturo de la Escalera.
Birdnet: a 3d object detection framework from lidar informa-
tion. arXiv preprint arXiv:1805.01195, 2018.

[3] Angel Chang, Angela Dai, Tom Funkhouser,

, Matthias
Nießner, Manolis Savva, Shuran Song, Andy Zeng, and
Yinda Zhang. Matterport3D: Learning from RGB-D data
in indoor environments. In Proceedings of the International
Conference on 3D Vision (3DV), 2017.

[4] Kang Chen, Yukun Lai, Yu-Xin Wu, Ralph Robert Martin,
and Shi-Min Hu. Automatic semantic modeling of indoor
scenes from low-quality RGB-D data using contextual infor-
mation. ACM Transactions on Graphics, 33(6), 2014.

[5] Myung Jin Choi, Antonio Torralba, and Alan S Willsky.
Context models and out-of-context objects. Pattern Recog-
nition Letters, 33(7):853–862, 2012.

[6] Myung Jin Choi, Antonio Torralba, and Alan S Willsky. A
tree-based context model for object recognition. IEEE trans-
actions on pattern analysis and machine intelligence, 34(2):
240–252, 2012.

[7] Wongun Choi, Yu-Wei Chao, Caroline Pantofaru, and Silvio
Savarese. Understanding indoor scenes using 3D geometric
phrases. In CVPR, pages 33–40. IEEE, 2013.

[8] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. ScanNet:
Richly-annotated 3D reconstructions of indoor scenes.
In
Proc. CVPR, 2017.

[9] Zhuo Deng and Longin Jan Latecki. Amodal detection of
3d objects: Inferring 3d bounding boxes from 2d ones in
rgb-depth images.
In Conference on Computer Vision and
Pattern Recognition (CVPR), volume 2, page 2, 2017.

[10] Francis Engelmann, Theodora Kontogianni, Alexander Her-
mans, and Bastian Leib. Exploring spatial context for 3D
semantic segmentation of point clouds. In Proc. ICCV, 2017.
[11] Pedro F Felzenszwalb, Ross B Girshick, David McAllester,
and Deva Ramanan. Object detection with discriminatively
trained part-based models.
IEEE transactions on pattern
analysis and machine intelligence, 32(9):1627–1645, 2010.
[12] Carolina Galleguillos and Serge Belongie. Context based
object categorization: A critical survey. Computer vision
and image understanding, 114(6):712–722, 2010.

[13] Ross Girshick. Fast R-CNN. Proc. ICCV, 2015.
[14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proc. CVPR, 2014.

[15] Saurabh Gupta, Ross Girshick, Pablo Arbelaez, and Jitendra
Malik. Learning rich features from RGB-D images for object
detection and segmentation. In Proc. ECCV, 2014.

[16] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Gir-
shick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE
International Conference on, pages 2980–2988. IEEE, 2017.
[17] Geremy Heitz, Stephen Gould, Ashutosh Saxena, and
Daphne Koller. Cascaded classiﬁcation models: Combin-

ing models for holistic scene understanding.
In Advances
in Neural Information Processing Systems, pages 641–648,
2009.

[18] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point-

wise convolutional neural networks. In Proc. CVPR, 2018.

[19] Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Recur-
rent slice networks for 3D segmentation of point clouds. In
Proc. CVPR, 2018.

[20] Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu,
Ying Nian Wu, and Song-Chun Zhu. Cooperative holistic
scene understanding: Unifying 3D object, layout, and cam-
era pose estimation. In Proc. NIPS, 2018.

[21] Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu
Xu, and Song-Chun Zhu. Holistic 3D scene parsing and re-
construction from a single RGB image. In European Confer-
ence on Computer Vision, pages 194–211. Springer, 2018.

[22] Andrej Karpathy, Stephen Miller, and Li Fei-Fei. Object dis-
covery in 3D scenes via shape analysis. In Proc. ICRA, pages
2088–2095. IEEE, 2013.

[23] Jean Lahoud and Bernard Ghanem. 2D-driven 3D object de-
tection in RGB-D images. In Computer Vision (ICCV), 2017
IEEE International Conference on, pages 4632–4640. IEEE,
2017.

[24] Loic Landrieu and Martin Simonovsky. Large-scale point
In

cloud semantic segmentation with superpoint graphs.
Proc. CVPR, 2018.

[25] Karel Lenc and Andrea Vedaldi. R-CNN minus R. Proc.

BMVC, 2015.

[26] Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao
Zhang, and Leonidas Guibas. Grass: Generative recursive
autoencoders for shape structures. ACM Trans. on Graphics.
(SIGGRAPH), 2017.

[27] Yangyan Li, Angela Dai, Leonidas Guibas, and Matthias
Nießner. Database-assisted object retrieval for real-time 3D
reconstruction. Computer Graphics Forum (Eurographics),
2015.

[28] Yangyan Li, Rui Bu, Mingchao Sun, and Baoquan Chen.

PointCNN. arXiv preprint arXiv:1801.07791, 2018.

[29] Dahua Lin, Sanja Fidler, and Raquel Urtasun. Holistic scene
understanding for 3D object detection with RGBD cameras.
In Proceedings of the IEEE International Conference on
Computer Vision, pages 1417–1424, 2013.

[30] Di Lin, Yuanfeng Ji, Dani Lischinski, Daniel Cohen-Or, and
Hui Huang. Multi-scale context intertwining for semantic
segmentation. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 603–619, 2018.

[31] Tsung-Yi Lin, Priyal Goyal, Ross Girshick, Kaiming He, and
Piotr Dollár. Focal loss for dense object detection.
IEEE
transactions on pattern analysis and machine intelligence,
2018.

[32] Tianqiang Liu, Siddhartha Chaudhuri, Vladimir G. Kim,
Qixing Huang, Niloy J. Mitra, and Thomas Funkhouser. Cre-
ating consistent scene graphs using a probabilistic grammar.
ACM Trans. on Graphics. (SIGGRAPH Asia), 2014.

[33] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In European con-
ference on computer vision, pages 21–37. Springer, 2016.

1779

[34] Xiaobai Liu, Yibiao Zhao, and Song-Chun Zhu. Single-view
3D scene reconstruction and parsing by attribute grammar.
IEEE transactions on pattern analysis and machine intelli-
gence, 40(3):710–725, 2018.

[35] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and
Alan Yuille. The role of context for object detection and
semantic segmentation in the wild.
In Proc. CVPR, pages
891–898, 2014.

[36] Liangliang Nan, Ke Xie, and Andrei Sharf. A search-classify
approach for cluttered indoor scene understanding. ACM
Trans. on Graphics. (SIGGRAPH Asia), 2012.

[37] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS-W, 2017.

[38] Jake Porway, Kristy Wang, Benjamin Yao, and Song Chun
Zhu. A hierarchical and contextual model for aerial image
understanding. In Computer Vision and Pattern Recognition,
2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE,
2008.

[39] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
PointNet: Deep learning on point sets for 3D classiﬁcation
and segmentation. In Proc. ICCV, 2017.

[40] Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and
Song-Chun Zhu. Human-centric indoor scene synthesis us-
ing stochastic grammar. In Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.

[41] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 779–788, 2016.

[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. Proc. NIPS, 2015.

[43] Zhile Ren and Erik B. Suddert. Three-dimensional object
detection and layout prediction using clouds of oriented gra-
dients. In Proc. CVPR, 2016.

[44] Radu Bogdan Rusu and Steve Cousins. 3D is here: Point
Cloud Library (PCL). In Proc. ICRA, Shanghai, China, May
9-13 2011.

[45] Tianjia Shao, Aron Monszpart, Youyi Zheng, Bongjin Koo,
Weiwei Xu, Kun Zhou, and Niloy J Mitra.
Imagining the
unseen: Stability-based cuboid arrangements for scene un-
derstanding. ACM Transactions on Graphics, 33(6), 2014.

[46] Abhishek Sharma, Oncel Tuzel, and Ming-Yu Liu. Recursive
context propagation network for semantic scene labeling. In
Advances in Neural Information Processing Systems, pages
2447–2455, 2014.

[47] Jianbo Shi and Jitendra Malik. Normalized cuts and image

segmentation. IEEE T-PAMI, 22(8):888–905, 2000.

[48] Yifei Shi, Pinxin Long, Kai Xu, Hui Huang, and Yueshan
Xiong. Data-driven contextual modeling for 3D scene un-
derstanding. Computers & Graphics, 2016.

[49] Martin Simon, Stefan Milz, Karl Amende, and Horst-
Michael Gross. Complex-yolo: Real-time 3d object detec-
tion on point clouds. arXiv preprint arXiv:1803.06199, 2018.
[50] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y

Ng. Parsing natural scenes and natural language with recur-
sive neural networks. In Proc. ICML, pages 129–136, 2011.
[51] Shuran Song and Jianxiong Xiao. Sliding Shapes for 3D

object detection in depth images. In Proc. ECCV, 2014.

[52] Shuran Song and Jianxiong Xiao. Deep Sliding Shapes for
In Proc.

amodal 3D object detection in RGB-D images.
CVPR, 2016.

[53] Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunY-
oung Gwak, and Silvio Savarese. SEGCloud: Semantic seg-
mentation of 3D point clouds. In Proceedings of the Interna-
tional Conference on 3D Vision (3DV), 2017.

[54] Zhuowen Tu, Xiangrong Chen, Alan L Yuille, and Song-
Chun Zhu. Image parsing: Unifying segmentation, detection,
and recognition. International Journal of computer vision,
63(2):113–140, 2005.

[55] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neu-
mann. SGPN: Similarity group proposal network for 3D
point cloud instance segmentation. In Proc. CVPR, 2018.

[56] Danfei Xu, Dragomir Anguelov, and Ashesh Jain. PointFu-
sion: Deep sensor fusion for 3D bounding box estimation. In
Proc. CVPR, 2018.

[57] Kai Xu, Hui Huang, Yifei Shi, Hao Li, Pinxin Long, Jianong
Caichen, Wei Sun, and Baoquan Chen. Autoscanning for
coupled scene reconstruction and proactive object analysis.
ACM Transactions on Graphics (TOG), 34(6):177, 2015.

[58] Kai Xu, Vladimir G Kim, Qixing Huang, Niloy Mitra, and
Evangelos Kalogerakis. Data-driven shape analysis and pro-
cessing. In SIGGRAPH ASIA 2016 Courses, page 4. ACM,
2016.

[59] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-
In Proceed-
time 3d object detection from point clouds.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 7652–7660, 2018.

[60] Xiaoqing Ye, Jiamao Li, Hexiao Huang, Liang Du, and Xi-
aolin Zhang. 3d recurrent neural networks with context fu-
sion for point cloud semantic segmentation.
In European
Conference on Computer Vision, pages 415–430. Springer,
2018.

[61] Li Yi, Leonidas Guibas, Aaron Hertzmann, Vladimir G.
Kim, Hao Su, and Ersin Yumer. Learning hierarchical shape
segmentation and labeling from online repositories. Proc. of
SIGGRAPH, 2017.

[62] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin
Choi. Neural motifs: Scene graph parsing with global con-
text. In Proc. CVPR, 2018.

[63] Yinda Zhang, Mingru Bai, Pushmeet Kohli, Shahram Izadi,
and Jianxiong Xiao. DeepContext: Context-encoding neu-
ral pathways for 3D holistic scene understanding. In Proc.
ICCV, 2017.

[64] Yibiao Zhao and Song-Chun Zhu.

Image parsing with
stochastic scene grammar. In Advances in Neural Informa-
tion Processing Systems, pages 73–81, 2011.

[65] Yibiao Zhao and Song-Chun Zhu. Scene parsing by inte-
grating function, geometry and appearance models. In 2013
IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 3119–3126, June 2013. doi: 10.1109/CVPR.
2013.401.

1780

