Attention Based Glaucoma Detection:

A Large-scale Database and CNN Model

Liu Li†, Mai Xu†‡∗, Xiaofei Wang†, Lai Jiang†, Hanruo Liu§,

† School of Electronic and Information Engineering, Beihang University, Beijing, China

‡ Hangzhou Innovation InstituteBeihang University, Hangzhou, Zhejiang, China
§Beijing Institute of Ophthalmology, Beijing Tongren Hospital, Beijing, China

†{liliu1995, maixu, xfwang, jianglai.china}@buaa.edu.cn

Abstract

(cid:38)(cid:82)(cid:85)(cid:85)(cid:72)(cid:70)(cid:87)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:44)(cid:81)(cid:70)(cid:82)(cid:85)(cid:85)(cid:72)(cid:70)(cid:87)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

Recently, the attention mechanism has been successful-
ly applied in convolutional neural networks (CNNs), sig-
niﬁcantly boosting the performance of many computer vi-
sion tasks. Unfortunately, few medical image recognition
approaches incorporate the attention mechanism in the C-
NNs.
In particular, there exists high redundancy in fun-
dus images for glaucoma detection, such that the attention
mechanism has potential in improving the performance of
CNN-based glaucoma detection. This paper proposes an
attention-based CNN for glaucoma detection (AG-CNN).
Speciﬁcally, we ﬁrst establish a large-scale attention based
glaucoma (LAG) database, which includes 5,824 fundus im-
ages labeled with either positive glaucoma (2,392) or neg-
ative glaucoma (3,432). The attention maps of the oph-
thalmologists are also collected in LAG database through a
simulated eye-tracking experiment. Then, a new structure of
AG-CNN is designed, including an attention prediction sub-
net, a pathological area localization subnet and a glaucoma
classiﬁcation subnet. Different from other attention-based
CNN methods, the features are also visualized as the local-
ized pathological area, which can advance the performance
of glaucoma detection. Finally, the experiment results show
that the proposed AG-CNN approach signiﬁcantly advances
state-of-the-art glaucoma detection.

1. Introduction

In recently years,

the attention mechanism has been
successfully applied in deep learning based computer vi-
sion tasks, i.e., object detection [3, 31, 28], image caption
[35, 39, 2] and action recognition [30]. The basic idea of
the attention mechanism is to locate the most salient parts
of the features in deep neural networks (DNNs), such that
redundancy is removed for the vision tasks. In general, the

∗Mai Xu is the corresponding author of this paper.

(cid:3)
(cid:86)
(cid:88)
(cid:71)
(cid:81)
(cid:88)
(cid:41)

(cid:86)
(cid:72)
(cid:74)
(cid:68)
(cid:80)

(cid:76)

(cid:3)

(cid:75)
(cid:87)
(cid:88)
(cid:85)
(cid:87)
(cid:16)
(cid:71)
(cid:81)
(cid:88)
(cid:82)
(cid:85)
(cid:42)

(cid:3)

(cid:86)
(cid:83)
(cid:68)
(cid:80)
(cid:81)
(cid:82)
(cid:76)
(cid:87)
(cid:81)
(cid:72)
(cid:87)
(cid:87)
(cid:68)

(cid:3)

(cid:71)
(cid:72)
(cid:93)
(cid:76)
(cid:79)
(cid:68)
(cid:88)
(cid:86)
(cid:76)
(cid:57)

(cid:86)
(cid:83)
(cid:68)
(cid:80)

(cid:3)
(cid:87)
(cid:68)
(cid:72)
(cid:75)

Figure 1. Examples of glaucoma fundus images, attention maps by oph-
thalmologists in glaucoma diagnosis and visualization results of a CN-
N model (Bottom) [15] by an occlusion experiment [40]. The Pearson
Correlation Coefﬁcient (CC) results between the visualized heat maps and
ground-truth ophthalmologist attention maps are 0.33 and 0.14 for correct
and incorrect glaucoma classiﬁcation, respectively.

attention mechanism is embedded in DNNs by leveraging
the attention maps. Speciﬁcally, on the one hand, the atten-
tion maps in [31, 28, 35, 30] are yielded in a self-learned
pattern, with other information weakly supervising the at-
tention maps, i.e., the classiﬁcation labels. On the other
hand, [39, 37] utilize the human attention information to
guide the DNNs focusing on the region of interest (ROI).

Redundancy also exists in medical image recognition, in-
terfering the recognition results. In particular, there exists
heavy redundancy in fundus images for disease recognition.
For example, the pathological areas of fundus images are in
the region of optic cup and disc, or its surrounding blood
vessel and optic nerve area [25]; other regions such as the
boundary of the eye ball are redundant for the medical di-
agnosis. As shown in Figure 1, glaucoma, an irreversible
optic disease, can be correctly detected by a convolutional
neural network (CNN) [15], when the visualized heat maps
are consistent with the attention maps of ophthalmologist-
s. Otherwise, glaucoma is mislabeled by the CNN model
when the visualized heat maps focus on redundant regions.

10571

Therefore, it is reasonable to combine the attention mecha-
nism in the CNN model for using fundus images to detect
ophthalmic disease.

However, to our best knowledge, there has been no work-
s incorporating the human attention in medical image recog-
nition. This is mainly because there lacks the doctor at-
tention database, which needs the qualiﬁed doctors and a
special technique of capturing the doctor attention in the di-
agnosis. As such, in this paper, we ﬁrst collect a large-scale
attention based fundus image database for glaucoma detec-
tion (LAG), including 5,824 images with diagnose labels
and human attention maps. Based on the real human atten-
tion, we propose an attention based CNN method (called
AG-CNN) for glaucoma detection based on fundus images.
Although human attention is able to reduce heavy redun-
dancy in fundus images for disease recognition, it may al-
so miss some of the pathological area which is helpful for
disease detection. As a result, the existing CNN models
have outperformed the doctors in medical image recogni-
tion [18, 27, 26]. Thus, we propose to reﬁne the predict-
ed attention maps by incorporating a feature visualization
structure for glaucoma detection. As such, the gap between
human attention and pathological area can be bridged. In
fact, there have been several methods for automatically lo-
cating the pathological area [41, 12, 8, 11, 24], based on
the class activation mapping model (CAM) [42]. However,
these methods cannot locate the pathological area at a small
region due to the limitation of its feature size. In this paper,
we employ the guided back propagation (BP) method [33]
to locate the tiny pathological area, based on the predicted
attention maps. Consequently, the attention maps can be re-
ﬁned and then used to highlight the most critical regions for
glaucoma detection.

The main contributions of this paper are: (1) We estab-
lish a LAG database with 5,824 fundus images, along with
their labels and attention maps. (2) We propose incorporat-
ing the attention maps in AG-CNN, such that the redundan-
cy can be removed from fundus images for glaucoma detec-
tion. (3) We develop a new architecture of AG-CNN, which
visualizes the CNN feature maps for locating pathological
area and then classiﬁes binary glaucoma.

2. Medical Background

The recent success of deep learning methods has bene-
ﬁtted medical diagnosis [7, 4, 38], especially for automat-
ically detecting oculopathy in fundus images [13, 10, 34].
Speciﬁcally, [13, 10] worked on classiﬁcation of diabetic
retinopathy using the CNN models. [34] further proposed
deep learning systems for multi-ophthalmological diseases
detection. However, the above works all transfered some
classic CNN model for nature image classiﬁcation to med-
ical image classiﬁcation, regardless of the characteristic of
fundus images.

Glaucoma detection methods can be basically divided in-
to 2 categories, i.e., heuristic methods and deep learning
methods. The heuristic glaucoma detection methods ex-
tract features based on some image processing techniques
[1, 6, 17, 32]. Speciﬁcally, [1] extracted the texture fea-
tures and higher order spectra features for glaucoma detec-
tion. [6] used the wavelet-based energy features for glauco-
ma detection. Both [1, 6] applied support vector machine
(SVM)and naive Bayesian classiﬁer to classify the hand-
crafted features. However, the above heuristic methods on-
ly consider a handful of features on fundus images, leading
to lower classiﬁcation accuracy.

Another category of glaucoma detection methods is
based on deep learning [29, 43, 5, 22, 23]. Speciﬁcally,
[29, 43] reported their deep learning work on glaucoma de-
tection based on automatic segmentation of optic cup and
disc. However, their work assume that only the optical cup
and disc are related to glaucoma, lacking end-to-end train-
ing. On the other hand, [5] ﬁrstly proposed a CNN method
for glaucoma detection in an end-to-end mannar. [22] fol-
lowed Chen’s work and proposed an advanced CNN struc-
ture combining the holistic and local features for glaucoma
classiﬁcation. To regularize the input images, both [5, 22]
preprocessed the original fundus images to remove the re-
dundant regions. However, due to the limited training data
and simple structure of networks, the previous works did
not achieve high sensitivity and speciﬁcity. Most recently,
a deeper CNN structure has been proposed in [23]. How-
ever, the fundus images exist large redundancy irrelevant to
glaucoma detection, leading to the low efﬁciency for [23].

3. Database

3.1. Establishment

In this work, we establish a large-scale attention based
glaucoma detection database. Our LAG database contains
5,824 fundus images with 2,392 positive and 3,432 nega-
tive glaucoma samples obtained from Beijing Tongren Hos-
pital 1. Our work is conducted according to the tenets of
Helsinki Declaration. As the retrospective nature and ful-
ly anonymized usage of color retinal fundus images, we are
exempted by the medical ethics committee to inform the pa-
tients. Each fundus image is diagnosed by qualiﬁed glauco-
ma specialists, taking the consideration of both morpholog-
ic and functional analysis, i.e, intra-ocular pressure, visual
ﬁeld loss and manual optic disc assessment. As a result, the
binary labels of positive or negative glaucoma of all fundus
images are conﬁrmed, seen as the gold standard.

Based on the above labelled fundus images, we further
conduct an experiment to capture the attention regions of
the ophthalmologists in glaucoma diagnosis. The experi-
ment is based on an alternative method for eye tracking [19],

1The database is available at https://github.com/smilell/AG-CNN.

10572

(cid:4318)

(cid:4319)

(cid:4320)

(cid:1005)(cid:1004)
(cid:1005)(cid:1005)

(cid:1005)(cid:1006)
(cid:1005)(cid:1007)
(cid:1005)(cid:1008)

(cid:1005)(cid:1009)

(cid:1013)

(cid:1005)

(cid:1007)

(cid:4321)

(cid:1012)

(cid:1008)

(cid:1011)

(cid:1010)

(cid:1006)

(cid:1009)

(cid:1005)(cid:1010)

(cid:1005)(cid:1011)

Figure 2. An example of capturing ﬁxations of an ophthalmologist in
glaucoma diagnosis.
(Left): Original blurred fundus images.
(Middle-
left): Fixations of the ophthalmologist with cleared regions.
(Middle-
right): The order of clearing the blurred regions. Note that the size of
the white circles represents the order of ﬁxations. (Right): The generated
attention map based on the captured ﬁxations.

Table 1. CC values of attention maps between one ophthalmologist and
the mean of the rest ophthalmologists.

Ophthalmologist

one v.s. others

one v.s. random

1st
2nd
3rd
4th

0.594
0.636
0.687
0.585

6.59 × 10−4
2.49 × 10−4
2.49 × 10−4
8.44 × 10−4

in which mouse clicks are used by the ophthalmologists to
explore ROI for glaucoma diagnosis. Speciﬁcally, all the
fundus images are initially displayed blurred, and then the
ophthalmologists use the mouse as an eraser to successive-
ly clear the circle regions for diagnosing glaucoma. Note
that the radius of all circle regions is set to 40 pixels, while
all fundus images are with 500 × 500 pixels. This ensures
that the circle regions are approximately equivalent to the
fovea (2◦ − 3◦) of the human vision system at a comfort-
able viewing distance (3-4 times of screen height). The or-
der of clearing the blurred regions represents the degree of
attention by ophthalmologists, as the GT of the attention
map. Once the ophthalmologist is able to diagnose glauco-
ma with the partly cleared fundus image, the above region
clearing process is terminated and the next fundus image is
displayed for diagnosis.

i , yj

i , yj

i )}Ij ,J

In the above experiment, the ﬁxations of ophthalmolo-
gists are represented by the center coordinate (xj
i ) of
the cleared circle region for the i-th ﬁxation of the j-th
ophthalmologist. Then, the attention map A of one fun-
dus image can be generated by convoluting all ﬁxations
{(xj
i=1,j=1 with the 2D Gaussian ﬁlter at square de-
cay according to the order of i, where J is the total number
of ophthalmologists (=4 in our experiment) and Ij is the
number of ﬁxations from the j-th ophthalmologist on the
fundus image. Here, the standard deviation of the Gaussian
ﬁlter is set to 25, according to [36]. Figure 2 shows an ex-
ample of the ﬁxations of one ophthalmologist and attention
map of all ophthalmologists for a fundus image.

3.2. Data analysis

Now, we mine our LAG database to investigate the at-
tention maps of all fundus images in glaucoma diagnosis.
Speciﬁcally, we have the following ﬁndings.

(cid:8)

(cid:3)
(cid:15)
(cid:86)
(cid:81)
(cid:82)
(cid:76)
(cid:74)
(cid:72)
(cid:85)
(cid:3)
(cid:73)
(cid:82)
(cid:81)
(cid:82)
(cid:76)
(cid:87)
(cid:85)
(cid:82)
(cid:83)
(cid:82)
(cid:85)
(cid:51)

(cid:3)

(cid:21)(cid:19)

(cid:20)(cid:24)

(cid:20)(cid:19)

(cid:24)

(cid:19)

(cid:20)

(cid:21)

(cid:23)
(cid:50)(cid:83)(cid:75)(cid:87)(cid:75)(cid:68)(cid:79)(cid:80)(cid:82)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:86)

(cid:22)

(cid:36)(cid:79)(cid:79)

(cid:8)

(cid:3)
(cid:15)
(cid:86)
(cid:81)
(cid:82)
(cid:76)
(cid:74)
(cid:72)
(cid:85)
(cid:3)
(cid:73)
(cid:82)
(cid:81)
(cid:82)
(cid:76)
(cid:87)
(cid:85)
(cid:82)
(cid:83)
(cid:82)
(cid:85)
(cid:51)

(cid:3)

(cid:1007)(cid:1004)

(cid:1006)(cid:1009)

(cid:1006)(cid:1004)

(cid:1005)(cid:1009)

(cid:1005)(cid:1004)

(cid:1009)

(cid:1004)

(cid:3)(cid:3)(cid:50)(cid:83)(cid:75)(cid:87)(cid:75)(cid:68)(cid:79)(cid:80)(cid:82)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:3)(cid:20)
(cid:3)(cid:3)(cid:50)(cid:83)(cid:75)(cid:87)(cid:75)(cid:68)(cid:79)(cid:80)(cid:82)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:3)(cid:21)
(cid:3)(cid:3)(cid:50)(cid:83)(cid:75)(cid:87)(cid:75)(cid:68)(cid:79)(cid:80)(cid:82)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:3)(cid:22)
(cid:3)(cid:3)(cid:50)(cid:83)(cid:75)(cid:87)(cid:75)(cid:68)(cid:79)(cid:80)(cid:82)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:3)(cid:23)
(cid:3)(cid:3)(cid:36)(cid:79)(cid:79)

(cid:1004)

(cid:1004)(cid:856)(cid:1006)

(cid:1004)(cid:856)(cid:1008)
(cid:1004)(cid:856)(cid:1010)
(cid:55)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71)

(cid:1004)(cid:856)(cid:1012)

(cid:1005)

Figure 3. (Left): Proportion of regions in the fundus images cleared by
different ophthalmologists for glaucoma diagnosis.
(Right): Proportion
of regions in attention maps with values being above a varying threshold.
Note that the values of the attention maps range from 0 to 1.

Finding 1: The ROI in fundus images is consistent across

ophthalmologists for glaucoma diagnosis.

Analysis: In this analysis, we calculate the Pearson cor-
relation coefﬁcients (CC) of attention maps between one
ophthalmologist and the remaining three ophthalmologist-
s. Table 1 reports the CC results averaged over all fundus
images in our LAG database. In this table, we also show
the CC results of attention maps between one ophthalmolo-
gist and the random baseline. Note that the random baseline
generates the attention maps by making their values follow
the Gaussian distribution. We can see from Table 1 that the
CC values of attention maps between one and the remain-
ing ophthalmologists are all above 0.55, signiﬁcantly larger
than those of the random baseline. This implies that atten-
tion exists consistency among ophthalmologists in glauco-
ma diagnosis. This completes the analysis of Finding 2.

Finding 2: The ROI in fundus images concentrates on

small regions for glaucoma diagnosis.

Analysis: In this analysis, we calculate the percentage
of regions that ophthalmologists cleared for glaucoma diag-
nosis. Figure 3 (Left) shows the percentage of the cleared
circle regions for each ophthalmologist, which is averaged
over all 5,824 fundus images of our LAG database. We can
see that the average ROI accounts for 14.3% of the total area
in the fundus images, with a maximum of 17.8% (the 3rd
ophthalmologist) and a minimum of 11.8% (the 4th oph-
thalmologist). Moreover, we calculate the proportion of re-
gions in attention maps, the values of which are above a
varying threshold. The result is shown in Figure 3 (Right).
The fast decreasing curve shows that most attention only
focuses on small regions of fundus images for glaucoma di-
agnosis. This completes the analysis of Finding 2.

Finding 3: The ROI for glaucoma diagnosis is of differ-

ent scales.

Analysis: Finding 2 shows that the ROI is small for glau-
coma diagnosis, comparing with the whole fundus images.
Here, although ROI is small, its scale is various across all
the fundus images. Figure 4 visualizes the ﬁxation maps of
some fundus images, in which the ROI are with different
scales. As shown in Figure 4, the sizes of the optic discs

10573

(cid:51)(cid:68)(cid:87)(cid:75)(cid:82)(cid:79)(cid:82)(cid:74)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:80)(cid:92)(cid:82)(cid:83)(cid:76)(cid:68)

(cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:89)(cid:72)

(cid:49)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)

(cid:68)
(cid:80)
(cid:82)
(cid:70)
(cid:88)
(cid:68)
(cid:79)
(cid:42)

(cid:72)
(cid:89)
(cid:76)
(cid:87)
(cid:76)
(cid:86)
(cid:82)
(cid:51)

(cid:72)
(cid:89)
(cid:76)
(cid:87)
(cid:68)
(cid:74)
(cid:72)
(cid:49)

(cid:22)(cid:19)

(cid:21)(cid:19)

(cid:20)(cid:19)

(cid:8)

(cid:3)
(cid:15)
(cid:44)

(cid:50)
(cid:53)
(cid:3)
(cid:72)
(cid:75)
(cid:87)
(cid:3)
(cid:3)
(cid:73)
(cid:82)
(cid:3)
(cid:81)
(cid:82)
(cid:76)
(cid:87)
(cid:85)
(cid:82)
(cid:83)
(cid:82)
(cid:85)
(cid:51)

(cid:19)

(cid:19)

(cid:3)(cid:3)(cid:87)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71)(cid:32)(cid:19)(cid:17)(cid:20)(cid:19)
(cid:3)(cid:3)(cid:87)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71)(cid:32)(cid:19)(cid:17)(cid:20)(cid:24)
(cid:3)(cid:3)(cid:87)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71)(cid:32)(cid:19)(cid:17)(cid:21)(cid:19)

(cid:20)(cid:19)(cid:19)(cid:19)

(cid:21)(cid:19)(cid:19)(cid:19)

(cid:22)(cid:19)(cid:19)(cid:19)
(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:86)

(cid:23)(cid:19)(cid:19)(cid:19)

(cid:24)(cid:19)(cid:19)(cid:19)

Figure 4. Fundus images with or without glaucoma for both positive and
negative pathological myopia.

Figure 5. Proportion of ROI above the threshold of 0.10, 0.15 and 0.20,
for all of the fundus images in LAG database.

for pathological myopia are considerably larger than others.
As such, we use myopia and non-myopia to select samples
with various scales of ROI (large or small optic cups). We
further ﬁnd that the images of both positive and negative
glaucoma have various-scaled ROI, as demonstrated in Fig-
ure 4. For each image in our LAG database, Figure 5 further
plots the proportion of the ROI in the ﬁxation maps, the val-
ues of which are larger than a threshold. We can see that the
ROI is at different scale for glaucoma diagnosis. Finally,
the analysis of Finding 3 can be accomplished.

4. Method

4.1. Framework

In this section, we discuss the proposed AG-CNN
method. Since Findings 1 and 2 show that glaucoma diag-
nosis is highly related to small ROI regions, the attention
prediction subnet is developed in AG-CNN for reducing
the redundancy of fundus images. In addition, we design
a pathological area localization subnet, which is achieved
by visualizing the CNN feature map, based on ROI regions
of the attention prediction subnet. Based on the patholog-
ical area, the glaucoma classiﬁcation subnet is developed
for producing the binary labels of glaucoma, in which the
multi-scale features are learned and extracted. The intro-
duction of multi-scale features is according to Finding 3.

The framework of AG-CNN is shown in Figure 6, and
its components, including multi-scale building block, de-
convolutional module and feature normalization, are further
demonstrated in Figure 7. As shown in Figure 6, the input
to AG-CNN is the RGB channels of a fundus image, while
the output is (1) the located pathological area and (2) the
binary glaucoma label. In addition, the located pathological
area is obtained in our AG-CNN in two 2 stages. In the ﬁrst
stage, the ROI of glaucoma detection is learned from the at-
tention prediction subnet, aiming to predict human attention
on diagnosing glaucoma. In the second stage, the predicted
attention map is embedded in the pathological area local-
ization subnet, and then the feature map of this subnet is
visualized to locate the pathological area. Finally, the lo-

cated pathological area is further used to to mask the input
and features of the glaucoma classiﬁcation subnet, for out-
putting the binary labels of glaucoma.

The main structure of AG-CNN is based on residual net-
works [15], in which the basic module is building block.
Note that all convolutional layers in AG-CNN are followed
by a batch normalization layer and a ReLU layer for in-
creasing the nonlinearity of AG-CNN, such that the conver-
gence rate can be sped up. The process of training AG-CNN
is in an end-to-end manner with three parts of supervision,
i.e., attention prediction loss, pathological area localization
loss and glaucoma classiﬁcation loss.

4.2. Attention prediction subnet

In AG-CNN, an attention prediction subnet is designed
to generate the attention maps of the fundus images, which
are then used for pathological area localization and glauco-
ma detection. Speciﬁcally, the input of the attention predic-
tion subnet is the RGB channels of a fundus image, which is
represented by the tensor (size: 224×224×3 ). Then, the in-
put tensor is fed to one convolutional layer with kernel size
of 7 × 7, followed by one max-pooling layer. Subsequently,
the features ﬂow into 8 building blocks for extracting the
hierarchical features. For more details about the building
blocks, refer to [15]. Afterwards, the features of 4 hierati-
cal building blocks are processed by feature normalization
(FN), the structure of which is shown in Figure 7 (Right).
As a result, four 28 × 28 × 128 features are obtained. They
are concatenated to form 28×28×512 deep multi-scale fea-
tures. Given the deep multi-scale features, a deconvolution-
al module is applied to generate the gray attention map with
the size of 112 × 112 × 1. The structure of the deconvolu-
tional module is also shown in Figure 7 (middle). As shown
in this ﬁgure, the deconvolutional module is comprised by 4
convolutional layers and 2 deconvolutional layers. Finally,
a 112 × 112 × 1 attention map can be yielded, the values
of which range from 0 to 1. In AG-CNN, the yielded atten-
tion maps are used to weight the input fundus images and
the extracted features of the pathological area localization
subnet. This is to be discussed in the next section.

10574

(cid:4)(cid:410)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)

(cid:87)(cid:396)(cid:286)(cid:282)(cid:349)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:94)(cid:437)(cid:271)(cid:374)(cid:286)(cid:410)

(cid:20)(cid:20)(cid:21)
(cid:20)(cid:20)(cid:20)(cid:20)(cid:20)(cid:21)(cid:21)

(cid:25)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23) (cid:25)(cid:23)

(cid:25)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)

(cid:20)(cid:20)(cid:21)

(cid:24)(cid:25)

(cid:38)(cid:69)

(cid:296)(cid:1005)

(cid:24)(cid:25)
(cid:24)(cid:24)(cid:24)(cid:24)(cid:25)(cid:24)(cid:25)(cid:24)(cid:25)

(cid:20)(cid:21)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27) (cid:20)(cid:21)(cid:27)
(cid:27)
(cid:21)(cid:27)

(cid:296)(cid:1006)

(cid:38)(cid:69)

(cid:21)(cid:27)
(cid:21)(cid:21)(cid:21)
(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)

(cid:38)(cid:69)

(cid:296)(cid:1007)

(cid:38)(cid:69)

(cid:296)(cid:1008)

(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:86)(cid:70)(cid:68)(cid:79)(cid:72)(cid:3)

(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)
(cid:87)(cid:88)(cid:85)(cid:72)
(cid:41)(cid:72)(cid:68)(cid:87)(cid:41)
(cid:21)(cid:27)

(cid:20)(cid:21)(cid:27) (cid:20)(cid:21)(cid:27) (cid:20)(cid:21)(cid:27) (cid:20)(cid:21)(cid:27)

(cid:20)(cid:23)
(cid:20)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)

(cid:21)(cid:24)(cid:25)(cid:25)(cid:25)(cid:25) (cid:21)(cid:24)(cid:25)
(cid:20)(cid:23)

(cid:24)(cid:20)(cid:21)(cid:21)(cid:21)(cid:21)(cid:21)

(cid:24)(cid:20)(cid:21) (cid:26)
(cid:26)

(cid:21)(cid:21)(cid:23)
(cid:21)(cid:21)(cid:23)

(cid:21)(cid:21)(cid:23)

(cid:25)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)

(cid:20)(cid:20)(cid:21)
(cid:20)(cid:20)(cid:20)(cid:20)(cid:20)(cid:21)(cid:21)

(cid:25)(cid:23)

(cid:24)(cid:25)

(cid:20)(cid:20)(cid:21)

(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:29)

(cid:3)(cid:41)(cid:88)(cid:81)(cid:71)(cid:88)(cid:86)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)
(cid:947)(cid:42)(cid:79)(cid:68)(cid:88)(cid:70)(cid:82)(cid:80)(cid:68)(cid:947)

(cid:24)(cid:25)(cid:25)(cid:25)(cid:25)(cid:25)(cid:25)(cid:25)

(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)
(cid:21)(cid:27)

(cid:20)(cid:23)
(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)

(cid:20)(cid:21)(cid:27)
(cid:20)(cid:23)

(cid:20)(cid:21)(cid:27)
(cid:21)(cid:27)

(cid:26)

(cid:21)(cid:24)(cid:25)

(cid:26)

(cid:20)(cid:21)(cid:27)

(cid:21)

(cid:17)
(cid:17)
(cid:17)

(cid:39)(cid:437)(cid:349)(cid:282)(cid:286)(cid:282)

(cid:17)(cid:87)

(cid:50)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87)(cid:20)(cid:29)

(cid:51)(cid:68)(cid:87)(cid:75)(cid:82)(cid:79)(cid:82)(cid:74)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:36)(cid:85)(cid:72)(cid:68)

(cid:51)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)

(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:68)(cid:83)

(cid:20)(cid:20)(cid:21)

(cid:20)(cid:20)(cid:21)

(cid:38)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)

(cid:48)(cid:68)(cid:91)(cid:3)(cid:51)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)

(cid:37)(cid:88)(cid:76)(cid:79)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:69)(cid:79)(cid:82)(cid:70)(cid:78)

(cid:62)(cid:381)(cid:400)(cid:400)(cid:1005)(cid:855)(cid:3)(cid:4)(cid:410)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:86)(cid:70)(cid:68)(cid:79)(cid:72)(cid:3)(cid:37)(cid:88)(cid:76)(cid:79)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:69)(cid:79)(cid:82)(cid:70)(cid:78)

(cid:24)(cid:25)(cid:25)(cid:25)(cid:25)(cid:25)(cid:25)(cid:25)

(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:27)(cid:27)(cid:27)
(cid:21)(cid:27)

(cid:20)(cid:23)
(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)

(cid:20)(cid:21)(cid:27)
(cid:20)(cid:23)

(cid:20)(cid:21)(cid:27)
(cid:21)(cid:27)

(cid:26)

(cid:21)(cid:24)(cid:25)

(cid:26)

(cid:20)(cid:21)(cid:27)

(cid:21)

(cid:17)
(cid:17)
(cid:17)

(cid:39)(cid:72)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)

(cid:42)(cid:88)(cid:76)(cid:71)(cid:72)(cid:71)(cid:3)(cid:37)(cid:68)(cid:70)(cid:78)(cid:3)(cid:51)(cid:85)(cid:82)(cid:83)(cid:68)(cid:74)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:41)(cid:88)(cid:79)(cid:79)(cid:92)(cid:3)(cid:38)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)

(cid:21)(cid:27)

(cid:25)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)(cid:23)

(cid:20)(cid:20)(cid:21)
(cid:20)(cid:20)(cid:20)(cid:20)(cid:20)(cid:21)

(cid:25)(cid:23)

(cid:24)(cid:25)

(cid:20)(cid:20)(cid:21)

(cid:87)(cid:258)(cid:410)(cid:346)(cid:381)(cid:367)(cid:381)(cid:336)(cid:349)(cid:272)(cid:258)(cid:367)(cid:3)(cid:4)(cid:396)(cid:286)(cid:258)(cid:3)
(cid:62)(cid:381)(cid:272)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:94)(cid:437)(cid:271)(cid:374)(cid:286)(cid:410)

(cid:62)(cid:381)(cid:400)(cid:400)(cid:1006)(cid:855)(cid:3)(cid:115)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:39)(cid:367)(cid:258)(cid:437)(cid:272)(cid:381)(cid:373)(cid:258)(cid:3)

(cid:18)(cid:367)(cid:258)(cid:400)(cid:400)(cid:349)(cid:296)(cid:349)(cid:272)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:94)(cid:437)(cid:271)(cid:374)(cid:286)(cid:410)

(cid:50)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87)(cid:21)(cid:29)

(cid:38)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)

(cid:41)(cid:49)

(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:49)(cid:82)(cid:85)(cid:80)(cid:68)(cid:79)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:62)(cid:381)(cid:400)(cid:400)(cid:1007)(cid:855)(cid:3)(cid:18)(cid:367)(cid:258)(cid:400)(cid:400)(cid:349)(cid:296)(cid:349)(cid:272)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)

Figure 6. Architecture of our AG-CNN network for glaucoma detection. The sizes of the feature maps and convolutional kernels are shown in this ﬁgure.

4.3. Pathological area localization subnet

After predicting the attention maps, we further design a
pathological area localization subnet to visualize the CNN
feature map in glaucoma classiﬁcation. The predicted at-
tention maps can effectively make the network focus on the
salient region with reduced redundancy; however, the net-
work may inevitably miss some potential features useful for
glaucoma classiﬁcation. Moreover, it has been veriﬁed that
the deep learning methods outperform human in the task
of image classiﬁcation both on nature images [14, 21] and
medical images [18, 27, 26]. Therefore, we further design a
subnet to visualize the CNN features for ﬁnding the patho-
logical area.

Speciﬁcally, the pathological area localization subnet is
mainly composed of convolutional layers and fully connect-
ed layers. In addition, the predicted attention maps are used
to mask the input fundus images and the extracted feature
maps at different layers of the pathological area localization
subnet. The structure of this subnet is the same as the glau-
coma classiﬁcation subnet, which is to be discussed in sec-
tion 4.4. Then, the visualization map of pathological area is
yielded through guided BP [33] from the output of the fully
connection layer to the input RGB channels fundus images.
Finally, the visualization map is down-sampled to 112×112
with its values being normalized to 0 − 1, as the output of
the pathological area localization subnet.

4.4. Glaucoma classiﬁcation subnet

In addition to the attention prediction subnet and patho-
logical area localization subnet, we design a glaucoma clas-
siﬁcation subnet for the binary classiﬁcation of positive or
negative glaucoma. Similar to the attention prediction sub-
net, the glaucoma classiﬁcation subnet is composed of one
7 × 7 convolutional layer, one max-pooling layer, 4 multi-
scale building blocks.

(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:54)(cid:70)(cid:68)(cid:79)(cid:72)(cid:3)(cid:37)(cid:88)(cid:76)(cid:79)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:37)(cid:79)(cid:82)(cid:70)(cid:78)
(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:48)(cid:48)(cid:48) (cid:16)(cid:54)(cid:70)(cid:68)(cid:79)(cid:72)(cid:3)(cid:37)(cid:88)(cid:76)(cid:79)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:37)(cid:79)(cid:82)(cid:70)(cid:78)

(cid:38)(cid:20)(cid:3)(cid:32)(cid:3)(cid:38)(cid:22)(cid:3)(cid:32)(cid:3)(cid:22)(cid:91)(cid:22)
(cid:38)(cid:21)(cid:3)(cid:32)(cid:3)(cid:24)(cid:91)(cid:24) (cid:38)(cid:23)(cid:3)(cid:32)(cid:3)(cid:26)(cid:91)(cid:26)
(cid:38)(cid:24)(cid:3)(cid:32)(cid:3)(cid:38)(cid:25)(cid:3)(cid:32)(cid:3)(cid:20)(cid:91)(cid:20)

(cid:38)(cid:20)
(cid:38)(cid:21)
(cid:38)(cid:22)
(cid:38)(cid:23)

fi-1

(cid:410)
(cid:410)
(cid:258)
(cid:258)
(cid:272)
(cid:272)
(cid:374)
(cid:374)
(cid:381)
(cid:381)
(cid:18)
(cid:18)

(cid:38)(cid:24)

(cid:38)(cid:25)

(cid:38)(cid:69)
(cid:38)(cid:69)

(cid:20)(cid:20)(cid:21)
(cid:20)(cid:20)(cid:21)
(cid:20)
(cid:20)

(cid:21)(cid:27)
(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:27)(cid:21)(cid:27)

(cid:39)(cid:72)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)
(cid:39)(cid:72)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)
(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)
(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)
(cid:24)(cid:25)(cid:25)(cid:25)(cid:25)(cid:25)
(cid:24)(cid:25)
(cid:20)(cid:25)
(cid:20)(cid:25)(cid:20)
(cid:24)(cid:25)
(cid:24)(cid:25)(cid:24)(cid:24)(cid:25)(cid:25)(cid:25)(cid:25)(cid:24)
(cid:11)(cid:22)(cid:91)(cid:22)(cid:12) (cid:11)(cid:20)(cid:91)(cid:20)(cid:12)(cid:11)(cid:22)(cid:91)(cid:22)(cid:12)

(cid:21)(cid:27)
(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:27)(cid:21)(cid:27)
(cid:20)(cid:21)(cid:27)
(cid:20)(cid:21)(cid:20)(cid:21)(cid:21)(cid:21)(cid:27)(cid:27)(cid:27)(cid:27)(cid:20)(cid:20)(cid:20)(cid:21)(cid:20)(cid:21)(cid:21)(cid:27)(cid:21)(cid:27)(cid:27)(cid:20)(cid:21)(cid:20)(cid:21)(cid:27)
(cid:21)(cid:27)
(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:21)

512
(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:21)(cid:27)(cid:27)
(cid:21)(cid:27)

(cid:25)(cid:23)
(cid:25)(cid:25)(cid:23)(cid:23)(cid:25)(cid:23)(cid:23)(cid:23)(cid:25)(cid:25)(cid:25)(cid:25)(cid:23)(cid:25)(cid:23)(cid:23)(cid:23)(cid:25)(cid:23)(cid:25)(cid:23)
(cid:21)(cid:27)
(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:27)(cid:21)(cid:21)(cid:21)

(cid:21)(cid:27)
(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)(cid:21)(cid:27)

(cid:20)(cid:20)(cid:21)
(cid:20)(cid:21)(cid:20)(cid:20)(cid:20)(cid:20)

(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)
(cid:41)
(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)
(cid:49)(cid:82)(cid:85)(cid:80)(cid:68)(cid:79)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:49)(cid:82)(cid:85)(cid:80)(cid:68)(cid:79)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:49)
(cid:49)
(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:21)(cid:27)
(cid:21)(cid:21)(cid:21)(cid:21)(cid:27)(cid:21)

128 128
(cid:27)(cid:27)(cid:21)(cid:21)(cid:21)(cid:27)(cid:21)(cid:27)
(cid:21)(cid:27)

(cid:11)(cid:20)(cid:91)(cid:20)(cid:12)

fi

(cid:38)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)
(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)

(cid:39)(cid:72)(cid:70)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)
(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)

(cid:53)(cid:72)(cid:86)(cid:76)(cid:93)(cid:72)(cid:3)
(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)

Figure 7. Components of the AG-CNN architecture.

shown in Figure 7 (Left), 4 channels of convolutional lay-
ers C1, C2, C3 and C4 with different kernel sizes are con-
catenated to extract multi scale features, comparing with the
traditional building block which only has a single convolu-
tional channel. Finally, 2 fully connected layers are applied
to output the classiﬁcation result.

The main difference between the glaucoma classiﬁcation
subnet and the conventional residual network [15] is that the
visualization maps of pathological area weight both the in-
put image and extracted features to focus on the ROI. As-
sume that the visualization map generated by the patholog-
ical area localization subnet is ˆV. Mathematically, the fea-
tures F in the glaucoma classiﬁcation subnet can be masked
by ˆV as follows,

F′ = F ⊙ (cid:2)(1 − θ) ˆV ⊕ θ(cid:3) ,

(1)

where θ (=0.5 in this paper) is a threshold to control the
impact of the visualization map. In the above equation, ⊙
and ⊕ represent the element-wise multiplication and addi-
tion. In the glaucoma classiﬁcation subnet, the input fun-
dus image is masked with the visualization map in the same
way. Finally, in our AG-CNN method, the redundant fea-
tures irrelevant to glaucoma detection can be inhibited and
the pathological area can be highlighted.

4.5. Loss function

The multi-scale building blocks differ from the tradition-
al building block of [15] from the following aspect. As

In order to achieve end-to-end training, we supervise the
training process of AG-CNN through attention prediction

10575

Table 2. Performance of three methods for glaucoma detection over the
test set of our LAG database.

Table 3. Performance of three methods for glaucoma detection over the
RIM-ONE database.

Method Accuracy Sensitivity Speciﬁcity AUC F2−score

Method Accuracy Sensitivity Speciﬁcity AUC F2−score

Ours

Chen et al.

Li et al.

95.3%
89.2%
89.7%

95.4%
90.6%
91.4%

95.2% 0.975
88.2% 0.956
88.4% 0.960

0.951
0.894
0.901

Ours

Chen et al.

Li et al.

85.2%
80.0%
66.1%

84.8%
69.6%
71.7%

85.5% 0.916
87.0% 0.831
62.3% 0.681

0.837
0.711
0.679

loss (denoted by Lossa) , feature visualization loss (denot-
ed by Lossf ) and glaucoma classiﬁcation loss (denoted by
Lossc), as shown in Figure 6. In our LAG database, both
the glaucoma label l (∈ {0, 1}) and the attention map A
(with its elements Ai,j ∈ [0, 1]) are available for each fun-
dus image, seen as the GT in the loss function. We assume
that ˆl (∈ {0, 1}) and ˆA (with its elements ˆAi,j ∈ [0, 1])
are the predicted glaucoma label and attention map, respec-
tively. Following [16], we utilize the Kullback-Leibler (K-
L) divergence function as the human-attention loss Lossa.
Speciﬁcally, the human-attention loss is represented by

Lossa =

1

I · J

I

J

(cid:4)

i=1

(cid:4)

j=1

Aij log(

Aij
ˆAij

),

(2)

where I and J are the length and width of attention maps.

Furthermore, the pathological area localization subnet
and glaucoma classiﬁcation subnet are all supervised by the
glaucoma label l based on the cross-entropy function, which
measures the distance between the predicted label ˆl and its
corresponding GT label l. Mathematically, Lossf is calcu-
lated as follows,

Lossc = l log(

1

1 + e−ˆlc

) + (1 − l) log(1 −

1

1 + e−ˆlc

), (3)

where ˆlc represents the predicted label from the glaucoma
classiﬁcation subnet. Similar way is used to calculate Lossf,
which replaces ˆlc by ˆlf in 3.

Finally, the overall loss is the linear combination of

Lossa, Lossf and Lossc:

Loss = α · Lossa + β · Lossf + γ · Lossc,

(4)

where α, β and γ are hyper-parameters for balancing the
trade-off among attention loss, visualization loss and clas-
siﬁcation loss. At the begining of training AG-CNN, we
choose to set α ≫ β = γ to speed the convergence of at-
tention prediction subnet. Then, we set α ≪ β = γ to
minimize the feature visualization loss and the classiﬁcation
loss, thus realizing the convergence of prediction. Given the
loss function of (4), our AG-CNN model can be end-to-end
trained for glaucoma detection and pathological location.

5. Experiments and Results

5.1. Settings

In this section, the experiment results are presented to
validate the performance of our method in glaucoma detec-

tion and pathological area localization. In our experiment,
the 5,824 fundus images in our LAG database are random-
ly divided into training (4,792 images), validation (200 im-
ages) and test (832 images) sets. To test the generalization
ability of our AG-CNN, we further validate the performance
of our method on another public database RIM-ONE [9].
Before inputting to AG-CNN, the RGB channels of fundus
images are all resized to 224 × 224. In training AG-CNN,
the gray attention maps are downsampled to 112 × 112 with
their values normalized to be 0 ∼ 1. The loss function of
(4) for training the AG-CNN model is minimized through
the gradient descent algorithm with Adam optimizer [20].
The initial learning rate is 1 × 10−5. We ﬁrst set α = 20
and β = γ = 1 in (4) until the loss of the attention predic-
tion subnet converges, and then set α = 1 and β = γ = 10
for focusing on the feature visualization loss and glaucoma
classiﬁcation loss. Additionally, batch size is set to be 8.

Given the trained AG-CNN model, our method is eval-
uated and compared with two other state-of-the-art glauco-
ma detection methods [5, 23], in terms of different metrics.
Speciﬁcally, the metrics of sensitivity and speciﬁcity are de-
ﬁned as follows,

Sensitivity =

Speciﬁcity =

TP

TP + FN

TN

TN + FP

,

,

(5)

(6)

where TP, TN, FP and FN are the numbers of the true pos-
itive glaucoma, true negative glaucoma, false positive glau-
coma and false negative glaucoma, respectively. Based on
TP, FP and FN, the Fβ−score is calculated by

Fβ−score =

(1 + β 2) · TP

(1 + β 2) · TP + β 2 · FN + FP

.

(7)

In the above equation, β is the hyper-parameter balancing
the trade-off between sensitivity and speciﬁcity, and it is set
to 2 as the sensitivity is more important in medical diag-
nosis.
In addition, receiver operating characteristic curve
(ROC) and area under ROC (AUC) are also evaluated for
comparing the performance of glaucoma detection. All ex-
periments are conducted on a computer with an Intel(R)
Core(TM) i7-4770 CPU@3.40GHz, 32GB RAM and a s-
ingle Nvidia GeForce GTX 1080 GPU. Beneﬁting from the
GPU, our method is able to detect glaucoma of 30 fundus
images per second, and it is comparable to 83 and 21 fundus
images per second for [5] and [23].

10576

(cid:8)

(cid:3)
(cid:15)

(cid:92)
(cid:87)
(cid:76)
(cid:89)
(cid:76)
(cid:87)
(cid:76)
(cid:86)
(cid:81)
(cid:72)
(cid:54)

(cid:20)(cid:19)(cid:19)

(cid:27)(cid:19)

(cid:25)(cid:19)

(cid:23)(cid:19)

(cid:21)(cid:19)

(cid:8)

(cid:3)
(cid:15)

(cid:92)
(cid:87)
(cid:76)
(cid:89)
(cid:76)
(cid:87)
(cid:76)
(cid:86)
(cid:81)
(cid:72)
(cid:54)

(cid:3)(cid:3)(cid:50)(cid:88)(cid:85)(cid:86)
(cid:3)(cid:3)(cid:38)(cid:75)(cid:72)(cid:81)(cid:3)(cid:72)(cid:87)(cid:3)(cid:68)(cid:79)(cid:17)
(cid:3)(cid:3)(cid:47)(cid:76)(cid:3)(cid:72)(cid:87)(cid:3)(cid:68)(cid:79)(cid:17)

(cid:19)

(cid:23)(cid:19)

(cid:21)(cid:19)
(cid:25)(cid:19)
(cid:20)(cid:19)(cid:19)(cid:16)(cid:54)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:76)(cid:87)(cid:92)(cid:15)(cid:3)(cid:8)

(cid:27)(cid:19)

(cid:20)(cid:19)(cid:19)

(cid:27)(cid:19)

(cid:25)(cid:19)

(cid:23)(cid:19)

(cid:21)(cid:19)

(cid:19)

(cid:3)(cid:3)(cid:50)(cid:88)(cid:85)(cid:86)
(cid:3)(cid:3)(cid:38)(cid:75)(cid:72)(cid:81)(cid:3)(cid:72)(cid:87)(cid:3)(cid:68)(cid:79)(cid:17)
(cid:3)(cid:3)(cid:47)(cid:76)(cid:3)(cid:72)(cid:87)(cid:3)(cid:68)(cid:79)(cid:17)

(cid:19)

(cid:21)(cid:19)

(cid:23)(cid:19)

(cid:25)(cid:19)

(cid:27)(cid:19)

(cid:20)(cid:19)(cid:19)

(cid:20)(cid:19)(cid:19)(cid:16)(cid:54)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:76)(cid:87)(cid:92)(cid:15)(cid:3)(cid:8)

Figure 8. Comparison of ROC curves among different methods. (Left):
Testing on our LAG testing set. (Right): Testing on RIM-ONE database.

5.2. Evaluation on glaucoma detection

In this section, we compare the glaucoma detection per-
formance of our AG-CNN method with two other methods
[5, 23]. Note that the models of other methods are retrained
over our LAG database for fair comparison. Table 2 lists the
results of accuracy, sensitivity, speciﬁcity, F2−score and
AUC. As seen in Table 2, our AG-CNN method achieves
95.3%, 95.4% and 95.2% in terms of accuracy, sensitivity
and speciﬁcity, respectively, which are considerably better
than other two methods. Then, the F2−score of our method
is 0.951, while [5] and [23] only have F2−scores of 0.894
and 0.901. The above results indicate that our AG-CNN
method signiﬁcantly outperforms other two methods in all
metrics.

In addition, Figure 8 (Left) plots the ROC curves of our
and other methods, for visualizing the trade-off between
sensitivity and speciﬁcity. We can see from this ﬁgure that
the ROC curve of our method is closer to the upper-left cor-
ner, when comparing with other two methods. This means
that the sensitivity of our method is always higher than those
of [5, 23] at the same speciﬁcity. We further quantify ROC
performance of three methods through AUC. The AUC re-
sults are also reported in Table 2. As shown in this table, our
method has larger AUC than other two compared method-
s. In summary, we can conclude that our method performs
better in all metrics than [5, 23] in glaucoma detection.

To evaluate the generalization ability, we further com-
pare the performance of glaucoma detection by our method
with other 2 methods [5, 23] on the RIM-ONE database [9].
To our best knowledge, there is no other public database
of fundus images for glaucoma. The results are shown in
Table 3 and Figure 8 (Right). As shown in Table 3, all met-
rics of our AG-CNN method over the RIM-ONE database
are above 0.83, despite slightly smaller than the results over
our LAG database. The performance of our method is con-
siderably better than other two methods (except speciﬁcity
of [23]). It is worth mentioning that the metric of sensitiv-
ity is more important than that of speciﬁcity in glaucoma
detection, as other indicators, e.g., intra-ocular pressure and
the ﬁeld of vision, can be further used for conﬁrming the

(cid:42)
(cid:36)
(cid:47)

(cid:75)
(cid:87)
(cid:88)
(cid:85)
(cid:87)
(cid:16)
(cid:71)
(cid:81)
(cid:88)
(cid:82)
(cid:85)
(cid:42)

(cid:71)
(cid:72)
(cid:87)
(cid:70)
(cid:76)
(cid:71)
(cid:72)
(cid:85)
(cid:51)

(cid:40)
(cid:49)
(cid:50)
(cid:48)
(cid:44)
(cid:53)

(cid:16)

(cid:71)
(cid:72)
(cid:87)
(cid:70)
(cid:76)
(cid:71)
(cid:72)
(cid:85)
(cid:51)

Figure 9. Attention maps predicted by AG-CNN ramdomly selected from
the test dataset. The fundus images are from our LAG (upper) and RIM-
ONE (lower) database. Note that the RIM-ONE database has not the GT
of the attention map.

diagnosis of glaucoma. This implies that our method has
high generalization ability.

More importantly, Table 3 and Figure 8 (Right) show that
our AG-CNN method performs signiﬁcantly better than oth-
er methods especially in terms of sensitivity. In particular,
the performance of [23] severely degrades, as incurring the
over-ﬁtting issue. In a word, our AG-CNN method perform-
s well in the generalization ability, considerably better than
other state-of-the-art methods [5, 23].

5.3. Evaluation on attention prediction and patho-

logical area localization

We ﬁrst evaluate the accuracy of the attention model em-
bedded in our AG-CNN model. Figure 9 visualizes the at-
tention maps predicted by our AG-CNN method over the
LAG database and RIM-ONE database. We can see from
this ﬁgure that the predicted attention maps of AG-CNN are
close to those of GT, when testing on our LAG database.
The CC between the predicted attention maps and the GT
is 0.934 on average, with a variance of 0.0032. This im-
plies the attention prediction subnet of AG-CNN is able to
predict attention maps with high accuracy. We can further
see from Figure 9 that the attention maps locate the salient
optic cup and disc for the RIM-ONE database, in which the
scales of fundus images are totally different from those of
LAG database. Thus, our method is robust to the scales of
fundus images in predicting attention maps.

Then, we focus on the performance of pathological area
localization. Figure 10 visualizes the located pathological
area over the LAG database. Comparing the GT pathologi-
cal area with our localization results, we can see from Fig-
ure 10 that our AG-CNN model can accurately located the
areas of optic cup and disc and the region of retinal nerve
ﬁber layer defect, especially for the pathological areas of
the upper and lower optic disc edge.

Besides, we calculate the CC between the located patho-
logical area and the GT attention maps of ophthalmologist-

10577

(cid:55)
(cid:42)

(cid:86)
(cid:85)
(cid:88)
(cid:50)

(cid:48)
(cid:36)
(cid:38)

(cid:86)
(cid:85)
(cid:88)
(cid:50)

(cid:44)

(cid:3)

(cid:50)
(cid:53)
(cid:50)
(cid:58)

(cid:18)

Figure 10. Comparison of pathological area localization results for glau-
coma detection. (1st row): The pathological areas located by ophthalmol-
ogists. Optic cup and disc are labeled in blue and the regions of retinal
nerve ﬁber layer defect are labeled in green. (2nd row): The result of our
method. (3rd row): The result of CAM based method. (4th row): The
result of ablation experiment.

s, with an average of 0.581 and a variance of 0.028. This
also implies that (1) on one hand, the pathological area lo-
calization results are consistent with the attention maps of
ophthalmologists; (2) on the other hand, the pathological
area cannot be completely covered by the attention maps.
Moreover, we also compare our attention based pathologi-
cal area localization results with a state-of-art method [12],
which is based on the CAM model [42]. The results of [12]
are shown in the 3rd row of Figure 10. We can see that it
can roughly highlight the ROI but cannot pinpoint the tiny
pathological area, e.g., the upper and lower edge of the optic
disc boundary. In some cases, [12] highlight the boundary
of the eyeball, indicating that the CAM based methods ex-
tracted some unuseful features (i.e., redundancy) for clas-
siﬁcation. Therefore, the pathological area localization in
our approach is effective and reliable, especially compared
to the CAM based method that does not incorporate human
attention.

5.4. Results of ablation experiments

In our ablation experiments, we ﬁrst illustrate the impact
of predicted attention maps for located pathological area.
To this end, we simply remove the attention prediction sub-
net, and then compare the pathological localization results
with and without predicted attention maps. The results are
shown in Figure 10. We can see that the pathological area
can be effectively localized by using the attention maps. In
contrast, the located pathological area distributes over the
whole fundus image, once the attention maps are not in-
corporated. Therefore, the above results verify the effec-
tiveness and necessity of predicting the attention maps for
pathological area localization in our AG-CNN approach.

Next, we assess the impact of the predicted attention
map and the located pathological area on the performance
of glaucoma detection. To this end, we simply remove the

Table 4. Ablation results over the test set of our LAG database. APS
represents the attention prediction subnet. PAL represents the pathological
area localization subnet.

Method

Accuracy Sensitivity Speciﬁcity AUC F2−score

Full AG-CNN

W APS W/O PAL
W/O APS W PAL

W/O APS W/O PAL
W/O multi-scale block

95.3%
94.0%
87.1%
90.8%
92.2%

95.4%
94.0%
87.7%
91.1%
92.0%

95.2% 0.975
94.0% 0.973
86.7% 0.941
90.5% 0.966
92.3% 0.974

0.951
0.936
0.867
0.904
0.915

attention prediction subnet and pathological area localiza-
tion subnet of AG-CNN, respectively, for classifying the
binary labels of glaucoma. The results are shown in Ta-
ble 4. As seen in this table, the introduction of both the
predicted attention map and located pathological area can
improve accuracy, sensitivity, speciﬁcity and F2−score by
4.5%, 4.3%, 4.7% and 4.7%. However, the performance
of only embedding the pathological area localization subnet
and without the attention prediction subnet is even worse
than removing them both. It veriﬁes the necessity of our at-
tention prediction subnet for pathological area localization
and glaucoma detection.

Hence, the attention prediction subnet and pathological
area localization subnet are able to improve the performance
of glaucoma detection in AG-CNN. Additionally, we show
the effectiveness of the proposed multi-scale block in AG-
CNN, via replacing it by the default conventional shortcut
connection in residual network [15]. The results are also
shown in Table 4. We can see that the multi-scale block can
also enhance the performance of glaucoma detection.

6. Conclusion

In this paper, we have proposed a new deep learning
method, named AG-CNN, for automatic glaucoma detec-
tion and pathological area localization upon fundus images.
Our AG-CNN model is composed of the subnets of atten-
tion prediction, pathological area localization and glauco-
ma classiﬁcation. As such, glaucoma could be detected us-
ing the deep features highlighted by the visualized maps of
pathological areas, based on the predicted attention maps.
For training the AG-CNN model, we established the LAG
database with 5,824 fundus images labeled with either pos-
itive or negative glaucoma, along with their attention map-
s on glaucoma detection. The experiment results showed
that the predicted attention maps signiﬁcantly improve the
performance of glaucoma detection and pathological area
localization in our AG-CNN method, far better than other
state-of-the-art methods.

7. Acknowledgement

This work was supported by BMSTC under Grants

Z181100001918035.

10578

References

[1] U. R Acharya, S Dua, Xian Du, S Vinitha Sree, and
Chua Kuang Chua. Automated diagnosis of glaucoma using
texture and higher order spectra features. IEEE Transactions
on Information Technology in Biomedicine, 15(3):449–455,
2011.

[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
visual question answering. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2018.
[3] Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Mul-
tiple object recognition with visual attention. arXiv preprint
arXiv:1412.7755, 2015.

[4] Hao Chen, Qi Dou, Xi Wang, Jing Qin, and Pheng Ann
Heng. Mitosis detection in breast cancer histology images
via deep cascaded networks.
In The AAAI Conference on
Artiﬁcial Intelligence, pages 1160–1166, 2016.

[5] Xiangyu Chen, Yanwu Xu, Damon Wing Kee Wong,
Tien Yin Wong, and Jiang Liu. Glaucoma detection based
on deep convolutional neural network.
In Engineering in
Medicine and Biology Society (EMBC), 37th Annual Inter-
national Conference of the IEEE, page 715, 2015.

[6] S Dua, U. R. Acharya, P Chowriappa, and S. V. Sree.
Wavelet-based energy features for glaucomatous image clas-
siﬁcation. IEEE Transactions on Information Technology in
Biomedicine, 16(1):80–7, 2012.

[7] A Esteva, B Kuprel, R. A. Novoa, J Ko, S. M. Swetter, H. M.
Blau, and S Thrun. Dermatologist-level classiﬁcation of skin
cancer with deep neural networks. Nature, 542(7639):115–
118, 2017.

[8] Xinyang Feng, Jie Yang, Andrew F Laine, and Elsa D
Angelini. Discriminative localization in cnns for weakly-
supervised segmentation of pulmonary nodules.
In In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention, pages 568–576. Springer,
2017.

[9] F. Fumero, S. Alayon, J. L. Sanchez, J. Sigut, and M.
Gonzalez-Hernandez. Rim-one: An open retinal image
database for optic nerve evaluation. In International Sympo-
sium on Computer-Based Medical Systems, pages 1–6, 2011.
[10] R Gargeya and T. Leng. Automated identiﬁcation of di-
abetic retinopathy using deep learning. Ophthalmology,
124(7):962–969, 2017.

[11] Zongyuan Ge, Sergey Demyanov, Rajib Chakravorty, Adri-
an Bowling, and Rahil Garnavi. Skin disease recognition
using deep saliency features and multimodal learning of der-
moscopy and clinical images. In International Conference
on Medical Image Computing and Computer-Assisted Inter-
vention, pages 250–258. Springer, 2017.

[12] Waleed M Gondal, Jan M K¨ohler, Ren´e Grzeszick, Gernot A
Fink, and Michael Hirsch. Weakly-supervised localization of
diabetic retinopathy lesions in retinal fundus images. In Im-
age Processing (ICIP), 2017 IEEE International Conference
on, pages 2069–2073. IEEE, 2017.

[13] V Gulshan, L. Peng, M Coram, M. C. Stumpe, D. Wu, A
Narayanaswamy, S Venugopalan, K Widner, T Madams, and

J Cuadros. Development and validation of a deep learning
algorithm for detection of diabetic retinopathy in retinal fun-
dus photographs. Jama, 316(22):2402, 2016.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation.
In Proceedings of the
IEEE international conference on computer vision, pages
1026–1034, 2015.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016.

[16] Xun Huang, Chengyao Shen, Xavier Boix, and Qi Zhao. Sal-
icon: Reducing the semantic gap in saliency prediction by
adapting deep neural networks. In IEEE International Con-
ference on Computer Vision, pages 262–270, 2015.

[17] Ashish Issac, M. Partha Sarathi, and Malay Kishore Dutta.
An adaptive threshold based image processing technique for
improved glaucoma detection and classiﬁcation. Elsevier
North-Holland, Inc., 2015.

[18] Daniel S. Kermany, Michael Goldbaum, Wenjia Cai, Car-
olina C. S. Valentim, Huiying Liang, Sally L. Baxter, Alex
Mckeown, Ge Yang, Xiaokang Wu, and Fangbing Yan. Iden-
tifying medical diagnoses and treatable diseases by image-
based deep learning. Cell, 172(5):1122C1131.e9, 2018.

[19] Nam Wook Kim, Zoya Bylinskii, Michelle A. Borkin,
Krzysztof Z. Gajos, Aude Oliva, Fredo Durand, and
Hanspeter Pﬁster. Bubbleview: an interface for crowd-
sourcing image importance maps and tracking visual atten-
tion. ACM Transactions on Computer-Human Interaction,
24(5):1–40, 2017.

[20] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. Computer Science, 2014.

[21] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep

learning. nature, 521(7553):436, 2015.

[22] A. Li, J. Cheng, D. W. Wong, J. Liu, Annan Li, Jun Cheng,
Damon Wing Kee Wong, Jiang Liu, J. Cheng, and D. W.
Wong. Integrating holistic and local deep features for glau-
coma classiﬁcation. In Engineering in Medicine and Biol-
ogy Society (EMBC), 38th Annual International Conference
of the IEEE, page 1328, 2016.

[23] Z Li, Y He, S Keel, W Meng, RT Chang, and M He. Efﬁ-
cacy of a deep learning system for detecting glaucomatous
optic neuropathy based on color fundus photographs. Oph-
thalmology, 2018.

[24] Zhe Li, Chong Wang, Mei Han, Yuan Xue, Wei Wei, Li-
Jia Li, and Fei-Fei Li. Thoracic disease identiﬁcation and
localization with limited supervision. arXiv preprint arX-
iv:1711.06373, 2017.

[25] J. Liang, D. R. Williams, and D. T. Miller. Supernormal
vision and high-resolution retinal imaging through adaptive
optics. Journal of the Optical Society of America A Optics
Image Science & Vision, 14(11):2884–92, 1997.

[26] Ryan Poplin, Avinash V. Varadarajan, Katy Blumer, Yun
Liu, Michael V. Mcconnell, Greg S. Corrado, Lily Peng,
and Dale R. Webster. Predicting cardiovascular risk factors
from retinal fundus photographs using deep learning. arXiv
preprint arXiv:1708.09843, 2017.

10579

[41] Qiang Zhang, Abhir Bhalerao, and Charles Hutchinson.
Weakly-supervised evidence pinpointing and description. In
International Conference on Information Processing in Med-
ical Imaging, pages 210–222. Springer, 2017.

[42] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2921–
2929, 2016.

[43] Julian Zilly, Joachim M. Buhmann, and Dwarikanath Maha-
patra. Glaucoma detection using entropy sampling and en-
semble learning for automatic optic cup and disc segmenta-
tion. Computerized Medical Imaging and Graphics, 55:28–
41, 2017.

[27] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang,
Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis
Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologist-
level pneumonia detection on chest x-rays with deep learn-
ing. arXiv preprint arXiv:1711.05225, 2017.

[28] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: towards real-time object detection with region
proposal networks. In International Conference on Neural
Information Processing Systems, pages 91–99, 2015.

[29] Sharath M Shankaranarayana, Keerthi Ram, Kaushik Mitra,
and Mohanasankar Sivaprakasam. Joint optic disc and cup
segmentation using fully convolutional and adversarial net-
works. In Fetal, Infant and Ophthalmic Medical Image Anal-
ysis, pages 168–176. Springer, 2017.

[30] Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Ac-
tion recognition using visual attention. arXiv preprint arX-
iv:1511.04119, 2016.

[31] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[32] Anushikha Singh, Malay Kishore Dutta, M. Parthasarathi,
Vaclav Uher, and Radim Burget.
Image processing based
automatic diagnosis of glaucoma using wavelet features of
segmented optic disc from fundus image. Computer Methods
& Programs in Biomedicine, 124(C):108, 2016.

[33] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas
Brox, and Martin Riedmiller. Striving for simplicity: The
all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
[34] Dsw Ting, C. Y. Cheung, G. Lim, Gsw Tan, N. D. Quang,
A. Gan, H. Hamzah, R. Garciafranco, Iy Yeo San, and S. Y.
Lee. Development and validation of a deep learning system
for diabetic retinopathy and related eye diseases using retinal
images from multiethnic populations with diabetes. Jama,
318(22):2211, 2017.

[35] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In International conference on
machine learning, pages 2048–2057, 2015.

[36] Mai Xu, Lai Jiang, Xiaoyan Sun, Zhaoting Ye, and Zulin
Wang. Learning to detect video saliency with hevc fea-
tures. IEEE Transactions on Image Processing, 26(1):369–
385, 2017.

[37] Mai Xu, Chen Li, Yufan Liu, Xin Deng, and Jiaxin Lu. A
subjective visual quality assessment method of panoramic
videos.
In 2017 IEEE International Conference on Multi-
media and Expo, pages 517–522. IEEE, 2017.

[38] Lequan Yu, Xin Yang, Chen Hao, Jing Qin, and Pheng Ann
Heng. Volumetric convnets with mixed residual connections
for automated prostate segmentation from 3d mr images. In
The AAAI Conference on Artiﬁcial Intelligence, 2017.

[39] Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo,
Sang-Hun Lee, and Gunhee Kim. Supervising neural atten-
tion models for video captioning by human gaze data. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2680–29, 2017.

[40] Matthew D. Zeiler and Rob Fergus. Visualizing and under-

standing convolutional networks. pages 818–833, 2014.

10580

