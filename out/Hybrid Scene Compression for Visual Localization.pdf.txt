Hybrid Scene Compression for Visual Localization

Federico Camposeco1 Andrea Cohen1 Marc Pollefeys1,2

Torsten Sattler3

1Department of Computer Science, ETH Zurich

2 Microsoft

3Chalmers University of Technology

Abstract

Localizing an image w.r.t. a 3D scene model represents a
core task for many computer vision applications. An in-
creasing number of real-world applications of visual lo-
calization on mobile devices, e.g., Augmented Reality or
autonomous robots such as drones or self-driving cars,
demand localization approaches to minimize storage and
bandwidth requirements. Compressing the 3D models used
for localization thus becomes a practical necessity. In this
work, we introduce a new hybrid compression algorithm
that uses a given memory limit in a more effective way.
Rather than treating all 3D points equally, it represents a
small set of points with full appearance information and
an additional, larger set of points with compressed infor-
mation. This enables our approach to obtain a more com-
plete scene representation without increasing the memory
requirements, leading to a superior performance compared
to previous compression schemes. As part of our contribu-
tion, we show how to handle ambiguous matches arising
from point compression during RANSAC. Besides outper-
forming previous compression techniques in terms of pose
accuracy under the same memory constraints, our compres-
sion scheme itself is also more efﬁcient. Furthermore, the
localization rates and accuracy obtained with our approach
are comparable to state-of-the-art feature-based methods,
while using a small fraction of the memory.

1. Introduction

Visual localization constitutes an essential step in 3D
computer vision. It plays an important role in large scale
Structure-from-Motion (SfM) [1, 15, 36] and SLAM [11].
Visual localization is also a key task for both robotics, e.g.,
self-driving cars [14], and mobile device applications such
as virtual and augmented reality [25].

The classical approach to visual localization requires a
sparse 3D scene model, where each point is associated to
a 3D position and one or more image descriptors [22, 32].
In order to localize a query image, a set of 2D-3D corre-
spondences can be established by using descriptor matching
between features in the image and 3D points in the scene.

Figure 1. Comparison of compressed scenes. (Top) 3D model
obtained using a state-of-the-art model compression scheme [9].
(Bottom) Output of our hybrid approach. Full-descriptor points
are shown in red and compressed points in green. Both models use
the same amount of memory, but ours leads to a better localization
performance: For a compression rate of ∼2%, the percentage of
localized query images improves from ∼70% to more than 85%.

These matches can then be used for robust pose estimation
based on RANSAC and a minimal pose solver [13].

In many applications, coarse localization priors are avail-
able, e.g., from GPS or WiFi signals. These priors can be
used to coarsely determine the part of the 3D model shown
in a given query image within tens to hundreds of meters.
Thus, it is often only necessary to match against a part of a
larger 3D model. Still, it is important to compress these
small- to medium-scale parts, e.g., to be able to transfer
them to mobile devices or to be able to handle more re-
quests in a server-side application [28]. In this work, we
thus focus on efﬁciently handling scenes of this size.

Previous works on 3D scene compression mostly fall
under two categories: either the models are compressed

7653

by selecting a subset of all original 3D points while keep-
ing their full visual information (i.e., full feature descrip-
tors) [9, 12, 22], or all or a subset of the points are kept but
their descriptors are compressed [25, 31]. On a different
line of work, CNN-based localization approaches such as
PoseNet [17–19, 42] or DSAC [4, 5] implicitly represent a
scene by the weights stored in a network and can thus also
serve as compression methods. However, we show that our
approach outperforms CNN-based methods in terms of pose
accuracy, memory consumption, or both. Our approach is
also more ﬂexible in the presence of scene changes [34] as
no expensive retraining of a CNN is required.

In this paper, we present a hybrid 3D scene compression
method that selects two subsets of scene points:
the ﬁrst
subset consists of very few, carefully selected points with
full descriptors, while the second, larger subset consists of
points associated to quantized descriptors. The ﬁrst set of
points is used to obtain high-quality correspondences via
full descriptor matching that can be used to generate pose
hypotheses inside RANSAC. As the number of points in this
set is very small due to memory requirements, they might
not lead to enough matches for pose veriﬁcation. There-
fore, the second (and larger) set of points for which we only
store compressed descriptors is used. While the resulting
matches are not unique and are thus not well suited to gen-
erate pose hypotheses, they are sufﬁcient for pose veriﬁca-
tion. Due to compressing the descriptors of the second set,
our approach allows us to select signiﬁcantly more points at
the same memory consumption as standard approaches that
store full descriptors per 3D point. One central insight of
this paper is that selecting more points for pose veriﬁcation
is important for accurate pose estimation. We thus show that
storing these two sets of points provides more geometric in-
formation for localization while keeping the same memory
consumption as other compression methods. An example of
our compression output is visualized on the right in Fig. 1.

This work also introduces a RANSAC variation for ro-
bust pose estimation that exploits the two types of 3D point
subsets, as well as 3D point co-visibility by using a guided
sampling strategy (inspired by [21, 32]) that aims to choose
only co-visible 3D points to produce minimal subsets. This
sampling strategy increases the chances of ﬁnding a good
minimal sample, which can be critical for accuracy and per-
formance when dealing with lower inlier ratios.

In short, this paper presents the following contributions:
1) We propose a novel hybrid 3D scene compression al-
gorithm that takes into account the visibility of the 3D
points, their coverage of the scene and the visual unique-
ness of their appearance. Both coverage and uniqueness
are enforced using a novel technique that allows for better
coverage while offering faster compression times than the
state-of-the-art. 2) We obtain two different types of com-
pressed points: spatially compressed but fully descriptive

points that result in good unique matches, and a larger set of
points compressed in appearance space that result in multi-
matches. This second set of points has a low memory con-
sumption while being very helpful for avoiding a loss in lo-
calization performance. 3) We introduce a novel RANSAC
variant that exploits multi-matches for model evaluation
during robust pose estimation and uses a co-visibility-based
sampling prior. This modiﬁed RANSAC improves both the
localization rate and the pose accuracy. 4) We validate our
method on six small- to medium-scale datasets, showing
that our compression scheme outperforms state-of-the-art
compression methods both in terms of number of localized
images, as well as pose estimation accuracy. Our approach
is competitive with state-of-the-art localization approaches
while using signiﬁcantly less memory.

2. Related Work

The literature on 3D scene compression for visual local-
ization can be divided into three main categories:
image
retrieval, neural network-based scene representations, and
scene compression for feature-based localization methods.
Image retrieval techniques [30, 31, 44] perform compres-
sion by representing each image through a set of visual
words (or using an even more compact representation such
as the VLAD descriptor [2, 3, 41]), meaning that only lit-
tle data needs to be kept in memory at all times. These
methods may provide an approximate pose estimate via
the known pose of the retrieved database images.
In or-
der to improve retrieval performance, [16] synthesizes a set
of minimal views from the 3D scene in order to cover the
whole scene and reduce memory requirements. As a next
step, the data needed for accurate pose estimation can ei-
ther be loaded from disk on demand (e.g. 3D points and
their descriptors) [8, 16], or the local structure can be re-
computed on the ﬂy [34]. As such, the retrieval step is typ-
ically efﬁcient, but the next stages are either slow due to
loading from disk (which can be alleviated by using com-
pressed models) or due to heavy computations (solving a
local structure-from-motion problem). Especially the latter
methods trade in memory compression rate for run-time and
are not ﬁt for embedded applications.

Rather than matching local features, approaches for
pose [17–19,29,42] or scene coordinate regression [4,5,26]
use CNNs for localization. They thus store information
about the 3D scene implicitly in the network rather than ex-
plicitly in a point cloud. Pose regression approaches such
as PoseNet and its variants [17–19, 42] claim to offer com-
pact scene representations. However, our experiments show
that our approach outperforms this type of methods both in
terms of memory consumption and pose accuracy. Scene
coordinate regression approaches predict the coordinates of
a corresponding 3D point for a given patch [4, 5, 26, 37].
They thus replace the descriptor matching stage of classical

7654

approaches through machine learning. While they set the
state-of-the-art in pose accuracy in small-scale scenes [5],
they require signiﬁcantly more memory than our approach.

The approach presented in this paper falls under the third
category: scene compression for localization methods based
on 2D-to-3D descriptor matching. Such methods compress
scenes by selecting a subset of points [9, 12, 22, 39] or by
compressing the point descriptors [25, 31]. Our method in-
troduces a hybrid approach, showing that we can combine a
set of points with full descriptors with a larger set of points
with quantized descriptors. [25] presents an approach which
combines both point selection with descriptor quantization.
They use both compression techniques on top of each other,
maintaining only one small set of points with quantized de-
scriptors. This double compression drastically reduces the
localization rate. They compensate by camera pose tracking
over multiple frames. In contrast, we are interested in com-
pressing the scene with minimal loss in localization perfor-
mance when using a single query image.

[22] formulates scene compression as a K-covering
problem: The newly compressed scene is composed of a
subset of points with high visibility, selected such that at
least K of them are seen from each of the database im-
ages used to construct the scene. The visibility of a 3D
point is deﬁned as the number of database images that par-
ticipated in the reconstruction of this point. A point with
high visibility usually has a more accurate position and
a higher probability of being observable from a large set
of viewpoints [22]. Ensuring that at least K points are
seen in each database image ensures that the compressed
scene covers the same area as the original scene. Our work
modiﬁes the K-covering formulation in order to enforce a
more uniform distribution of the selected 3D points when
projected into the database images.
Instead of trying to
cover K points per image, we ﬁrst divide each image into q
uniformly-sized cells and require the compressed scene to
cover K/q 3D points per cell. This improves localization
performance w.r.t. previous methods for the same compres-
sion rates without an increase in compression times.

Cao and Snavely [9] show that, although visibility is im-
portant when choosing the best subset of points to represent
the 3D scene, the distinctiveness, or visual uniqueness, of
the point descriptors should also be taken into account dur-
ing compression. This ensures that points with an unique
descriptor are selected, which improves matching perfor-
mance, especially for high compression rates. Distinctive-
ness is introduced to the compression by checking that each
new point added to the compressed scene has a minimum
descriptor distance to all the points that have already been
chosen. This procedure of comparing each new point to
all previously selected points is computationally expensive.
In this work, we also extend the K-covering algorithm by
taking distinctiveness into account. We show that it is sufﬁ-

cient to approximate similarity via quantization rather than
explicitly computing descriptor distances. We exploit quan-
tization both during K-cover to choose a proper set of dis-
tinctive and descriptive 3D points, as well as for selection
and appearance compression of the second, larger subset of
3D points. The use of quantization allows us to decrease
compression run-times considerably w.r.t. [9]. As such, our
approach is well-suited for scenarios in which the scene
model needs to be re-compressed frequently, e.g., in dy-
namic scenes in which the geometry of the scene changes
over time. At the same time, our hybrid sets of points in-
crease localization performance w.r.t. [9].

Similarly to our method, [31] also exploits one-to-many
2D-3D matches or multi-matches via quantization. Yet,
they attempt to resolve these ambiguities before pose es-
timation by ensuring that matches are locally unique. Non-
unique matches are simply discarded. In contrast, our work
actively uses ambiguous multi-matches during the hypoth-
esis evaluation step of pose estimation. This eliminates the
chance of rejecting correct matches before RANSAC. [31]
requires a large codebook of 16M words to avoid intro-
ducing too many ambiguous matches. Thus, [31] can be
used to compress large-scale scenes, but is unsuitable for
the smaller scene fragments used in practice [34].

[27] propose a RANSAC variant that handles multi-
matches based on matching probabilities (computed from
matching scores) and their involvement in (un)successful
hypotheses. Their method is not directly applicable in
our setting due to our binary similarity measure based on
whether descriptors fall into the same visual word. But sam-
pling multi-matches during hypothesis generation in our
RANSAC if they were inliers to previously generated poses
could potentially allow even higher compression rates.

3. 3D Scene Compression

A 3D scene is composed of 3D points and database im-
ages. Each 3D point is associated to a set of SIFT descrip-
tors corresponding to the image features from which the
point was triangulated. These descriptors can then be av-
eraged into a single SIFT descriptor that describes the ap-
pearance of that point to reduce memory consumption [22].
In order to localize a given image w.r.t. the 3D scene,
2D-to-3D matches are ﬁrst established, which are then used
in RANSAC-based pose estimation [13]. As in [32], we
employ a vocabulary-based approach to feature matching.
Using a pre-built vocabulary-tree, we ﬁrst assign each 3D
average descriptor to its corresponding K-means cell (visual
word). At search time, each query image descriptor is as-
signed to its closest visual word w. We then select every 3D
point in the scene that has the same visual word and search
for a nearest neighbor in descriptor space among those se-
lected points. As it will be seen next, we will make use of
this vocabulary tree again during compression. This will al-

7655

low us to both compress and query the 3D scene with a sin-
gle visual vocabulary. This yields faster compression times
and better localization performance w.r.t. previous work.

After averaging the SIFT descriptors, most of the mem-
ory consumption of the 3D scene is concentrated in the av-
eraged descriptors of the points. Therefore, we will aim to
reduce memory consumption by 1) reducing the set of 3D
points and 2) compressing the descriptor of a subset of non-
previously selected points using a visual word.

3.1. Reducing the Number of 3D Points

Let the initial set of 3D points be P. The goal of com-
pression is to select a subset P ′ ⊂ P such that |P ′| is
minimal under the condition that P ′ can be used to local-
ize as many query images as possible. To tackle this prob-
lem, we begin with the assumption that the spatial distribu-
tion of query images will be close to the distribution of the
database. This common assumption [7,22] is sensible given
that local features are not invariant to viewpoint changes.

We aim to select points such that each of the database
cameras sees enough 3D points, i.e., we want to enforce
that each of the cameras in the database are covered by at
least K points. This problem of choosing the optimal set of
3D points that cover all database cameras is an instance of
the Set Cover Problem (or K-cover when at least K points
must be seen from each camera) [22], which is NP-hard.
Furthermore, by choosing two 3D points whose descriptors
are too similar, a query image descriptor could be matched
to either of those points, resulting in ambiguities. This prob-
lem arises in the presence of repeated structures that pro-
duce very similar descriptors that are actually meters apart.
Therefore, the visual uniqueness of the selected 3D points
should also be taken into account.

Our primary concern is to select points which have a
complete coverage of the scene, while penalizing points
whose descriptors can be confused with other descriptors
selected. We can thus, as proposed by [9], cast this prob-
lem into an instance of the Weighted K-cover problem [23],
where the weight reﬂects the discriminative power of the
descriptor associated with a point. Since the Weighted K-
cover problem is NP-hard, we will follow common prac-
tice [16, 22, 32] and employ a greedy approach in order to
arrive at an approximate solution.

For compression, we make use of

the visibility
graph [22] deﬁned by each SfM model. The nodes of this
bipartite graph correspond to the 3D points and database
images in the reconstruction. The graph contains an undi-
rected edge between a point node and an image node if the
point was triangulated from the image. Let the binary ma-
trix M represent the visibility graph of the SfM model1,
where Mi,j is 1 if the i-th image Ii observes the j-th point
pj . Starting from an empty set of points P ′, our objective at

1In practice, M is very sparse and typically stored as an adjacency list.

each iteration is to ﬁnd pj that maximizes the gain

G(pj, P ′) = α(pj, P ′) · X
Ii∈I\C

Mi,j ,

(1)

where C is the set of images that have already been covered
by K points. The weights α(pj, P ′) measure how visually
distinctive point pj is w.r.t. the already selected points P ′.
In [9], α is computed by comparing the descriptor of
each candidate pj against all of the already selected 3D
points. This produces a rather slow procedure, since each
descriptor comparison can be costly and an adaptive search
structure (such as a KD-Tree) cannot be used as the size of
P ′ increases with each iteration.

Instead, we exploit the fact that we perform vocabulary-
based image localization using a pre-built vocabulary tree,
i.e., we assign each 3D point to a visual word before 2D-to-
3D matching. Thus, we deﬁne the weighting term as

α(pj, P ′) = 1 −

|P ′
w|
β

.

(2)

Here, w is the visual word of the descriptor of point pj , P ′
w
is the set of selected 3D points with descriptors assigned
to w, and β is the maximum number of allowed points per
word (set to 10 in our experiments). We thus penalize the
inclusion of 3D points whose visual word is too populated
with a linear function. The intuition behind this is that the
number of points assigned to a visual word gives an idea of
how unique points in this visual word are.

In contrast to [9], we opt to enforce that not only each
image is covered by K 3D points but that the distribution of
those points on the images is as even as possible. We thus
subdivide each database image into a grid with q equal-area
cells. We then regard the image cells instead of the images
themselves as the elements to be covered by the 3D points.
Let ch ∈ I c, where h = 1, . . . , q × N and N is the total
number of database images, be the h-th image cell to be
covered. I c represents the set of all images cells and C c the
set of cells already covered. The gain to maximize in each
iteration thus becomes

G(pj, P ′) = α(pj, P ′) · X

Mi,j .

(3)

ch∈I c\C c

Although the number of “cameras” to cover increases to
q × K, the number of non-zero entries remains the same.
We thus found no noticeable increase in compression run-
time since we also reduce the number of 3D points that are
enforced to be viewed by each image cell to K/q. As we
will show in Sec. 5, our approach has a positive impact on
the localization rates: the point selection has a more even
coverage of the scene and prevents the selection process to
be biased towards highly structured or textured parts of the
scene, which might not necessarily be visible at query time.

7656

Additionally, a uniform distribution in 2D typically leads to
more stable pose estimates compared to ﬁnding all matches
in a single region of an image [16].

Our approach achieves the same complexity reduction
as previous work: for n points and m images, the average
number of points per image is n/m. K-cover methods such
as [9, 22] reduce this number to Θ(K). The space reduction
for any constant K is thus Θ(n/(K · m)), i.e., linear in the
number of 3D points. Eq. 3 replaces m images by q ·m cells
and K with K/q, resulting in the same space complexity.

Once the greedy Weighted Set Cover algorithm is com-
pleted, we are left with a subset of 3D points P ′ ⊂ P which
should maximize scene coverage and descriptor unique-
ness.
Instead of discarding the rest of the 3D points [9],
we choose to select a second subset P ′′ ⊂ P of points for
which only a compressed descriptor will be kept. This pro-
cedure is detailed next.

3.2. Selecting Multi Matches

As previously mentioned, the most memory-consuming
part of a 3D scene are the feature descriptors. As such,
we may only select a small number of them to ensure that
we substantially reduce the amount of memory required to
represent a 3D model. If we only keep the 3D points se-
lected by our Set Covering procedure, we are prone to miss
matches due to the imperfect nature of the descriptors (fea-
tures might not match due to large viewpoint changes) and
the matching procedure. Additionally, only a few points
might be visible in the query image as the database im-
ages are only an approximation to the set of all viewpoints.
To mitigate this, we select a second subset of 3D points
P ′′ ⊂ P \ P ′, where we will keep only the quantized de-
scriptor (word assignment) for each selected 3D point and
its 3D location. The purpose of this second subset of com-
pressed points is the following: while the points in P ′ result
in high-quality matches that can be used for generating pose
hypotheses during RANSAC, they might not be enough to
properly verify these hypotheses. The set P ′′ is thus used
for veriﬁcation. As these last matches are only used with a
given hypothesis, they do not need to be unique as they are
disambiguated by the pose. Therefore, they can be stored at
little memory consumption.

We note that for each full 3D point we must store the 3D
point position, the full mean descriptor, and a list of cameras
that see the 3D point (which will be used within RANSAC
to discard bad samples, see Sec. 4). For compressed points,
we only need to store the 3D position and one visual word
index (1 integer). For 128 byte SIFT descriptor and a 32-
bit visual word, we measure that on average we can store
26.5 compressed descriptor for each full descriptor. Since
the only way of distinguishing compressed points is by their
visual word, their selection process will thus focus on word
uniqueness. To select the points for compression, we use the

following procedure: 1) score each point by its word occu-
pancy (assigning a high score to low occupancy) and 2) se-
lect the X points that have the highest score. This procedure
selects points without any regard to their camera coverage.
However, this criterion has already been prioritized by the
points selected in the ﬁrst step of the compression.

Since we will, in general, have more than one 3D point
with the same visual word, we must consider each 2D-to-3D
match against compressed points as a multi-match (which
will be handled differently within RANSAC, as explained in
the next section). Nevertheless, the selection procedure for
our compressed 3D points already minimizes the number
of points in each word. Thus, we can expect to have a low
number of multi-matches per query descriptor.

4. Multi-Match RANSAC

As explained in Sec. 3, matching the 2D features of the
query image against P ′ using a standard ratio test [24] pro-
vides a set of one-to-one 2D-3D matches. We will denote
this set of matches as U . Matching against P ′′ results in
a set of multi-matches, denoted as W. The total set of
matches will be referred to as M = {U ∪ W}. This section
explains how M can be used during the robust RANSAC-
based pose estimation stage in order to improve both local-
ization rates and run-time when dealing with highly com-
pressed scenes. Alg. 1 summarizes our approach.

Robust pose estimation is usually achieved by running
RANSAC [13] with a minimal geometric pose solver (e.g.
P3P [20] for the calibrated setting or P4P [6] for the un-
known focal length case). A general RANSAC approach
randomly selects minimal samples from M, generates pose
hypotheses with the minimal solver, and then evaluates
these poses by counting the number of inliers according
to a given threshold on the reprojection error. However,
the lower the inlier ratio, the more samples are required
by RANSAC to ensure that the correct pose is found with
a certain probability.
Indeed, the number of samples re-
quired increases exponentially with the outlier ratio. Thus,
using the multi-matches W during the sampling stage po-
tentially introduces a high number of outliers, resulting in a
prohibitively large number of RANSAC iterations. There-
fore, our RANSAC variant limits the sampling only to the
set U of unique matches, since these matches have a higher
probability of being actually good matches. Once a model
has been sampled using only unique matches, it needs to be
evaluated. Having a larger number of inliers increases the
conﬁdence in the quality of a sampled model. Therefore,
the whole set of matches M, including the multi-matches
W, is used to evaluate each pose.

In order to further reduce the number of RANSAC itera-
tions, previous methods such as [10] use a guided-sampling
strategy in order to increase the chances of ﬁnding a good
sample. Inspired by this strategy, we propose an efﬁcient

7657

variation of the method introduced by [21] in the form of
a simple co-visibility-based sampling strategy. This ad-
dresses the problem that even in U inlier ratios can be very
small in a highly compressed scene and that it is therefore
not enough to just limit sampling to this subset. It is de-
sirable that samples should be drawn from sets of matches
that correspond to co-visible points, i.e., those that are seen
from the same camera or neighboring cameras, as these
matches have a higher chance of being geometrically con-
sistent. Similarly to [21], our sampling should thus increase
the probability of drawing from such subsets of matches.

More speciﬁcally, given U = {sj ↔ pj}, where sj de-
notes a 2D feature and pj a 3D point, and n the cardinality
of a minimal sample S ⊂ U drawn in RANSAC, we would
like all points pi ∈ S to be co-visible. In [21], points are
considered to be co-visible if they are all observed together
in at least one database image. However, we found this def-
inition of co-visibility to be a rather slow at sampling time
due to the need to perform multiple set intersection, requir-
ing about 20ms per RANSAC iteration.

We therefore use a slightly different deﬁnition of co-
for a set of points Q = {pi, i = 1 . . . n},
visibility:
let C(pi), pi ∈ Q, be the set of database images in the
scene from which point pi was reconstructed. The set Q
can be represented as a graph GQ, where each point pi
is a node and an edge is added between a pair of points
pi, pj, i 6= j, if they share at least one image in the scene,
i.e., if C(pi) ∩ C(pj) 6= ∅. We consider the points in Q as
co-visible if each pair of points in Q are at most 2 edges
apart in the graph GQ.

This deﬁnition allows the efﬁcient sampling (∼1µs) of
sets of covisible points: we choose a random match m1 =
sj1 ↔ pj1 from U as the ﬁrst element of S. We sequen-
tially draw the following mi, i = 2 . . . n, samples and test
whether they have an image in common with pj1 . If they
do, then the sample mi is accepted and included in S. Oth-
erwise, mi is dropped and re-sampled. If no valid mi can
be found after F attempts, we drop the complete sample S.
Both for speed and simplicity, we choose to always look
for intersections with C(pj1 ) instead of checking all of the
points that are already part of S. Since the ﬁrst match is
drawn uniformly at random, this choice does not restrict
sampling. As a result of our simple sampling strategy, only
potentially good minimal subsets are used for pose estima-
tion and evaluation.

5. Experimental Evaluation

In this paper, we focus on a hybrid scene compression
that enables usage of a pre-computed 3D sparse scene for
visual localization. Thus, especially for computationally
constrained platforms, we are interested in: 1. the run-
time of the compression procedure, 2. the memory reduc-
tion with respect to using an uncompressed scene, 3. the

Algorithm 1 Modiﬁed RANSAC
Require: Minimal sample size n, minimal solver PnP, matches M = {U ∪ W},
max. number F of sample trials, max. number T of iterations, inlier threshold σ

S = ∅, f = 0
Randomly sample m1 = s1 ↔ p1 from U
S ← m1
while |S| < n and f < F do

Randomly sample mi = si ↔ pi from U
if C(pi) ∩ C(p1) 6= ∅ then

else

S ← mi

1: repeat
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: until T iterations are reached
20: return best model θ∗

if |S| < n then

f ← f + 1

if e(mi; θ) < σ then

Compute pose θ = PnP(S)
for all mi ∈ M do

θ∗ ← θ

Jump to next iteration at line 1

Inliers(θ) ← Inliers(θ) + 1

if Inliers(θ) > Inliers(θ∗) then

Grid MR

Median time (ms)

Query

RANSAC

•

•

•
•

181
182
190
191

2.4
1.5
7.7
6.5

% reg.
images
95.7%
96.0%
97.4%
97.6%

Median error

Pos. (m)

Rot (◦)

2.09
2.02
2.09
1.97

0.42
0.37
0.39
0.35

Table 1. Comparison of all the variants of our method for the
Dubrovnik dataset compressed to 1.5% of its original size. Where
a • in “Grid” means we use image subdivisions and a • in MR
stands for the usage of our modiﬁed RANSAC.

localization rate, and 4. the accuracy of the obtained poses.
In order to properly validate the proposed method, we
evaluate it with 6 different real-world datasets (cf . Tab. 2).
We chose these datasets since they are representative of
small- to medium-scale scenes and the use-cases we aim
to tackle with our method. By including datasets that have
a wide variation in the number of points and imaging condi-
tions, we show that our methods can work well in different
scenarios. For the evaluation, we use SIFT [24] descrip-
tors throughout and use a single 6K-word visual vocabulary,
pre-computed using the Dubrovnik dataset [22]. In practice,
we did not observe strong evidence that suggested that using
a vocabulary trained per dataset yielded consistently better
results. This is also shown in [32]. For the pose computa-
tion, we use the P4P solver by [6].

This section is organized as follows: ﬁrst, we do an ab-
lation study that analyzes the impact of the compression
method introduced in Sec. 3 and the RANSAC variant of
Sec. 4. Next, the best setting of our method is compared
against previous works, in terms of compression run-time,
localization rates, and localization accuracy.

5.1. Ablation Study

Tab. 1 compares four different settings of our method.
We test its performance with and without image subdi-
visions and with and without our modiﬁed RANSAC on
the Dubrovnik dataset. When not using our modiﬁed
RANSAC, we do not take into account P ′′ and its cor-

7658

Dataset

Dubrovnik [22]

Aachen [35]

King’s College [19]
Old Hospital [19]
Shop Facade [19]

St Mary’s Church [19]

# DB
images

6,044
3,047
1,220
895
231
1,487

# 3D
points

1.88M
1.54M
503K
308K
82K
667K

# Query
images

800
369
343
182
103
530

Compression

Time [s]

Ours
27.9
21.6
1.7
3.6
0.43
8.74

KCD [9]

50.4
45.5
2.5
12.5
1.15
26.1

Table 2. Datasets used and compression times (to compress the
model to 1.5% of the original points) for our method compared to
“KCD” [9] (using their implementation). The run-times for our
method are signiﬁcantly faster than those of [9].

responding multi-matches. Tab. 1 shows an improvement
with each of the proposed modiﬁcations. In the following,
we refer to using both image subdivisions and our modiﬁed
RANSAC as “Ours”.

5.2. Compression Run time

For each dataset in Tab. 2, we compress the scene to
1.5% of its size (factoring both the number of full-descriptor
3D points and compressed points into this memory budget)
and compare the run-times of our method to the state-of-the-
art in model compression [9]. We have chosen 1.5% to be
the default compression rate since it exhibits a good trade-
off between performance and memory size. However, the
relative speed-up in compression time of our method vs. [9]
does not vary for different compression rates. Tab. 2 shows
that we achieve consistently lower compression times than
[9]. This is due to the fact that, as explained in Sec. 3, our
method only needs to check the occupancy of a visual word
to ﬁgure out the cost of selecting a 3D point. Our approach
is thus more suitable in scenarios where the scene changes
over time, e.g., due to seasonal changes [33], and the 3D
model used for localization needs to be adapted frequently.

5.3. Registration Rates

For the second experiment (see Fig. 2), we want to eval-
uate the efﬁciency of our compression and pose estima-
tion methods in terms of image registration power. Follow-
ing [9, 21, 22, 32], we consider an image as registered if the
best pose found by RANSAC has at least 12 inliers.

To make the compression rate comparison with [9]
fair, we select a combination of compressed and non-
compressed points in order to match the memory used
by [9] in each experiment. This is done in the following
way: for a given budget of M 3D points selected by [9],
we divide the budget into two parts. The ﬁrst 75% of the
budget is utilized by selecting 0.75M uncompressed points
by performing our weighted K-cover. The second 25% of
the budget is then spent on compressed 3D points for multi-
matches. The number of compressed points selected is actu-
ally larger than 0.25M , since we can store (on average) 26.5
compressed points for each full point (see Sec. 3). Thus, we
select 0.25 × 26.5M compressed points. For example, for

Method

MB Used

#reg.
images

Median
pos. error

Ours (1.5% 3D pts)

PoseNet [18]

DenseVLAD [41]

Camera Pose Voting [43]

Camera Pose Voting + RANSAC [43]

City-Scale Localization [40]

Active Search [32]

3.79
∼50
94.44
288.57
288.57
288.57

953

782
800
800
798
794
798
795

1.97m
7.9m
3.9m
1.69m
0.56m
0.56m
1.4m

Table 3. Accuracy for the Dubrovnik [38] dataset. Our method
achieves a comparable accuracy and registration rate at a signiﬁ-
cantly lower memory consumption.

a compression rate of 2.04% for Aachen, [9] selects 40,377
full 3D points while we select 30,282 full 3D points plus
267,517 compressed points for the same memory budget.
We experimented with different rates of compressed vs full
3D points, and found that a 1:3 ratio on average performed
best over all datasets, although other splits might work bet-
ter on individual datasets.

As can be seen from Fig. 2, our method vastly outper-
forms the state-of-the-art method by [9]. This is due to our
two contributions, namely using an additional large set of
compressed points and a more even distribution of the se-
lected points in image space, as these are the main differ-
ences between our method and [9]. Notice that all com-
pared methods achieve consistently better performances for
Dubrovnik than for Aachen. This is due to the different ac-
quisition modes of the two datasets. For Dubrovnik, the
database images come from an internet photo-collection
(Flickr) and thus tend to cluster around touristic areas.
Thus, good camera coverage can be achieved with fewer
points. Query images for this dataset were obtained by ran-
domly removing images from a larger 3D reconstruction.
They thus follow a similar distribution as the database im-
ages and can be localized with relative ease. For Aachen,
the database images were taken more regularly to cover the
area more completely, presenting less overlap between cam-
eras. Thus, more points might be needed to properly cover
all cameras. The query images also were taken separately
and do not follow the same distribution as the database im-
ages, resulting in a harder localization scenario.

5.4. Localization Accuracy

Finally, we want to focus on the impact our scene com-
pression method has on the accuracy of the resulting cam-
era poses. To this end, we compare to state-of-the-art
feature-based methods [32, 40, 43], recent deep-learning-
based methods [5, 17–19, 42], and an image retrieval ap-
proach using compact image-level descriptors [41]. Results
are shown for the Dubrovnik [38] dataset in Tab. 3 and the
Cambridge Landmarks datasets [19] in Tab. 4. The methods
in [22, 32, 40, 43] are similar to ours in that they also make
use of SIFT features to localize a given query image to a
sparse 3D scene. For these approaches, we are able to accu-
rately compute the memory consumption for representing

7659

Figure 2. Comparison of the registration rates (percentage of localized query images) for different compression methods at different
compression rates. Our method consistently outperforms the current state of the art approaches from [9] for all compression rates.

Method

Ours (@ 1.5%)
DenseVLAD [41]

PoseNet [19]

Bayes PoseNet [17]
LSTM PoseNet [42]

σ2 PoseNet [18]

geom. PoseNet [18]

DSAC++ [5]

Active Search [32]

King’s College [19]

Old Hospital [19]

Shop Facade [19]

St Mary’s Chruch [19]

MB
used
1.01
10.06

50
50
∼50
∼50
∼50
207
275

# reg.
images

343
343
343
343
343
343
343

-

343

Median

errors [m,◦]
0.81, 0.59
2.80, 5.72
1.92, 5.4
1.74, 4.06
0.99, 3.65
0.99, 1.06
0.88, 1.04
0.18, 0.3
0.57, 0.7

MB
used
0.62
13.98

50
50
∼50
∼50
∼50
207
140

# reg.
images

178
182
182
182
182
182
182

-

180

Median

errors [m,◦]
0.75, 1.01
4.01, 7.13
2.31, 5.38
2.57, 5.14
1.51, 4.29
2.17, 2.94
3.20, 3.29
0.20, 0.3
0.52, 1.12

MB
used
0.16
3.61
50
50
∼50
∼50
∼50
207
38.7

# reg.
images

103
103
103
103
103
103
103

-

103

Median

errors [m,◦]
0.19, 0.54
1.11, 7.61
1.46, 8.08
1.25, 7.54
1.18, 7.44
1.05, 3.97
0.88, 3.78
0.06, 0.3
0.12, 0.41

MB
used
1.34
23.23

50
50
∼50
∼50
∼50
207
359

# reg.
images

530
530
530
530
530
530
530

-

530

Median

errors [m,◦]
0.50, 0.49
2.31, 8.00
2.65, 8.48
2.11, 8.38
1.52, 6.68
1.49, 3.43
1.57, 3.32
0.13, 0.4
0.22, 0.62

Table 4. Comparison with several state-of-the-art methods on the Cambridge Landmarks datasets [19].

the scene, but ignore potential overhead due to search struc-
tures. For the deep learning methods, an approximate size
of the network is provided.

It should be noted that the deep learning methods do not
intrinsically provide a way to judge if a particular queried
image was successfully localized (whereas in geometric
methods one can rely on, e.g., inlier statistics). Thus, reg-
istration rates cannot be fairly compared to our approach.
The registration rates for DSAC++ are not reported in [5].

As can be seen in Tab. 3 and Tab. 4, our method clearly
presents the best trade-off between memory consumption
and performance. Compared to the retrieval baseline [41]
and the pose regression techniques [17–19, 42], our ap-
proach achieves both a lower memory consumption and
higher pose accuracy. Compared to Active Search [32],
our approach provides a comparable pose accuracy while
achieving a reduction in memory consumption by more than
order of magnitude. DSAC++ [5] estimates poses that are
signiﬁcantly more accurate than those computed by our ap-
proach. However, DSAC++ is not able to provide a compact
representation for small scenes. Note that the high pose ac-
curacy obtained by [43] and [40] on the Dubrovnik dataset
comes at a high run-time costs of more than 2 seconds per
image on average.

Overall, our results show that our method represent the
best of both worlds:
it has a very small memory foot-
print (up to orders of magnitude lower than in the non-
compressed setting) while achieving a performance close to
the state-of-the-art feature-based methods for efﬁcient lo-
calization.

6. Conclusion

In this paper, we have proposed a hybrid 3D scene com-
pression scheme and a RANSAC variant that jointly man-
age to efﬁciently and accurately localize a given query im-
age, all while using a small fraction (∼ 1.5%) of the orig-
inal memory requirements. Differently to previously pro-
posed methods, we compress the 3D scene by producing
two disjoint sets of 3D points: 1) a set of points with their
full visual descriptors that can be used to produce one-to-
one matches given a query descriptor and 2) a second set
of points for which we only store a compressed descrip-
tor. This second set of points is used to produce one-to-
many matches, i.e., multi-matches. The hybrid output of
our compression scheme is carefully selected to ensure a
good scene coverage and visual uniqueness of the selected
3D points. To properly handle the special two-fold output
of our compression, we design a RANSAC variant to handle
multi-matches such that they are only used during the model
veriﬁcation step, thus increasing number of inliers without
negatively affecting the number of iterations. In addition,
we propose a highly efﬁcient co-visibility-based strategy to
guide sampling within RANSAC.

We have validated our approach using several real-world
datasets. Our results show that our method achieves local-
ization rates and a pose accuracy comparable to state-of-
the-art feature-based and CNN-based methods at a signiﬁ-
cantly lower memory footprint.

Acknowledgements. We thank Google’s Visual Position-
ing System for their support.

7660

0.20.40.60.811.21.41.61.8Compression Rate [%]406080100Registration Rate [%]Dubrovnik Ours KCKCD KCDP0.511.522.53CompressionRate [%]507090AachenReferences

[1] Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz,
and Richard Szeliski. Building Rome in a Day. In ICCV,
2009. 1

[2] Relja Arandjelovi´c, Petr Gronat, Akihiko Torii, Tomas Pa-
jdla, and Josef Sivic. NetVLAD: CNN architecture for
weakly supervised place recognition. In CVPR, 2016. 2

[3] Relja Arandjelovic and Andrew Zisserman. All About

VLAD. In CVPR, 2013. 2

[4] Eric Brachmann, Alexander Krull, Sebastian Nowozin,
Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten
Rother. DSAC - Differentiable RANSAC for Camera Local-
ization. In CVPR, 2017. 2

[5] Eric Brachmann and Carsten Rother. Learning Less is More-
In

6D Camera Localization via 3D Surface Regression.
CVPR, 2018. 2, 3, 7, 8

[6] Martin Bujnak, Zuzana Kukelova, and Tom´as Pajdla. A gen-
eral solution to the P4P problem for camera with unknown
focal length. In CVPR, 2008. 5, 6

[7] Federico Camposeco, Torsten Sattler, Andrea Cohen, An-
dreas Geiger, and Marc Pollefeys. Toroidal Constraints
for Two-Point Localization Under High Outlier Ratios.
In
CVPR, 2017. 4

[8] Song Cao and Noah Snavely. Graph-Based Discriminative

Learning for Location Recognition. In CVPR, 2013. 2

[9] Song Cao and Noah Snavely. Minimal scene descriptions
from structure from motion models. In CVPR, 2014. 1, 2, 3,
4, 5, 7, 8

[10] Ondrej Chum and Jiri Matas. Matching with prosac-

progressive sample consensus. In CVPR, 2005. 5

[11] Andrew J. Davison, Ian D. Reid, Nicholas D. Molton, and
Olivier Stasse. Monoslam: Real-time single camera slam.
PAMI, 29:2007, 2007. 1

[12] Marcin Dymczyk, Simon Lynen, Titus Cieslewski, Michael
Bosse, Roland Siegwart, and Paul Furgale. The gist of maps
- summarizing experience for lifelong localization. In ICRA,
2015. 2, 3

[13] Martin A. Fischler and Robert C. Bolles. Random sam-
ple consensus: A paradigm for model ﬁtting with applica-
tions to image analysis and automated cartography. ACM,
24(6):381–395, June 1981. 1, 3, 5

[14] Christian H¨ane, Lionel Heng, Gim Hee Lee, Friedrich Fraun-
dorfer, Paul Furgale, Torsten Sattler, and Marc Pollefeys.
3D Visual Perception for Self-Driving Cars using a Multi-
Camera System: Calibration, Mapping, Localization, and
Obstacle Detection. IMAVIS, 68:14 – 27, 2017. 1

[15] Jared Heinly, Johannes L Sch¨onberger, Enrique Dunn, and
Jan-Michael Frahm. Reconstructing the world* in six days
*(as captured by the yahoo 100 million image dataset). In
CVPR, 2015. 1

[16] Arnold Irschara, Christopher Zach, Jan-Michael Frahm, and
Horst Bischof. From structure-from-motion point clouds to
fast location recognition. In CVPR, 2009. 2, 4, 5

[17] Alex Kendall and Roberto Cipolla. Modelling Uncertainty in
Deep Learning for Camera Relocalization. In ICRA, 2016.
2, 7, 8

[18] Alex Kendall and Roberto Cipolla. Geometric loss functions
In CVPR,

for camera pose regression with deep learning.
2017. 2, 7, 8

[19] Alex Kendall, Matthew Grimes, and Roberto Cipolla.
Posenet: A Convolutional Network for Real-time 6-DoF
Camera Relocalization. In ICCV, 2015. 2, 7, 8

[20] Laurent Kneip, Davide Scaramuzza, and Roland Siegwart.
A novel parametrization of the perspective-three-point prob-
lem for a direct computation of absolute camera position and
orientation. In CVPR, 2011. 5

[21] Yunpeng Li, Noah Snavely, Daniel Huttenlocher, and Pascal
Fua. Worldwide pose estimation using 3D point clouds. In
ECCV, 2012. 2, 6, 7

[22] Yunpeng Li, Noah Snavely, and Daniel P Huttenlocher. Lo-
In

cation recognition using prioritized feature matching.
ECCV, 2010. 1, 2, 3, 4, 5, 6, 7

[23] Ching Lih Lim, Alistair Moffat, and Anthony Wirth. Lazy
and Eager Approaches for the Set Cover Problem. In Aus-
tralasian Computer Science Conference, 2014. 4

[24] David G Lowe. Distinctive image features from scale-

invariant keypoints. IJCV, 60(2):91–110, 2004. 5, 6

[25] Simon Lynen, Torsten Sattler, Michael Bosse, Joel Hesch,
Marc Pollefeys, and Roland Siegwart. Get out of my lab:
Large-scale, real-time visual-inertial localization.
In RSS,
2015. 1, 2, 3

[26] Daniela Massiceti, Alexander Krull, Eric Brachmann,
Carsten Rother, and Philip H.S. Torr. Random Forests versus
Neural Networks - What’s Best for Camera Relocalization?
In ICRA, 2017. 2

[27] Paul McIlroy, Edward Rosten, Simon Taylor, and Tom
Drummond. Deterministic Sample Consensus with Multiple
Match Hypotheses. In BMVC, 2010. 3

[28] Sven Middelberg, Torsten Sattler, Ole Untzelmann, and Leif
Kobbelt. Scalable 6-DOF Localization on Mobile Devices.
In ECCV, 2014. 1

[29] Tayyab Naseer and Wolfram Burgard. Deep regression for
monocular camera-based 6-DoF global localization in out-
door environments. In IROS, 2017. 2

[30] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and
Andrew Zisserman. Object Retrieval with Large Vocabular-
ies and Fast Spatial Matching. In CVPR. IEEE, 2007. 2

[31] Torsten Sattler, Michal Havlena, Filip Radenovic, Konrad
Schindler, and Marc Pollefeys. Hyperpoints and ﬁne vocab-
ularies for large-scale location recognition. In ICCV, 2015.
2, 3

[32] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efﬁcient &
Effective Prioritized Matching for Large-Scale Image-Based
Localization. PAMI, 39(9):1744–1756, 2017. 1, 2, 3, 4, 6, 7,
8

[33] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii,
Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi
Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and
Tomas Pajdla. Benchmarking 6DOF Outdoor Visual Local-
ization in Changing Conditions. In CVPR, 2018. 7

[34] Torsten Sattler, Akihiko Torii, Josef Sivic, Marc Pollefeys,
Hajime Taira, Masatoshi Okutomi, and Tomas Pajdla. Are
Large-Scale 3D Models Really Necessary for Accurate Vi-
sual Localization? In CVPR, 2017. 2, 3

7661

[35] Torsten Sattler, Tobias Weyand, Bastian Leibe, and Leif
Kobbelt. Image Retrieval for Image-Based Localization Re-
visited. In BMVC, 2012. 7

[36] Johannes Lutz Sch¨onberger and Jan-Michael Frahm.

Structure-from-motion revisited. In CVPR, 2016. 1

[37] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram
Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene
Coordinate Regression Forests for Camera Relocalization in
RGB-D Images. In CVPR, 2013. 2

[38] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Mod-
IJCV,

eling the world from internet photo collections.
80(2):189–210, 2008. 7

[39] Hyun Soo Park, Yu Wang, Eriko Nurvitadhi, James C Hoe,
Yaser Sheikh, and Mei Chen. 3d Point Cloud Reduction Us-
ing Mixed-Integer Quadratic Programming. In CVPR Work-
shops, 2013. 3

[40] Linus Sv¨arm, Olof Enqvist, Fredrik Kahl, and Magnus Os-
karsson. City-Scale Localization for Cameras with Known
Vertical Direction. PAMI, 39(7):1455–1461, 2017. 7, 8

[41] Akihiko Torii, Relja Arandjelovi´c, Josef Sivic, Masatoshi
Okutomi, and Tomas Pajdla. 24/7 Place Recognition by View
Synthesis. In CVPR, 2015. 2, 7, 8

[42] Florian Walch, Caner Hazirbas, Laura Leal-Taixe, Torsten
Sattler, Sebastian Hilsenbeck, and Daniel Cremers. Image-
based localization using lstms for structured feature correla-
tion. In ICCV, 2017. 2, 7, 8

[43] Bernhard Zeisl, Torsten Sattler, and Marc Pollefeys. Cam-
era pose voting for large-scale image-based localization. In
ICCV, 2015. 7, 8

[44] Wei Zhang and Jana Kosecka. Image based localization in

urban environments. In 3DPVT, 2006. 2

7662

