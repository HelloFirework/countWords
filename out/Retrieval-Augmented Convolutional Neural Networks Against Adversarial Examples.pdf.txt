Retrieval-Augmented Convolutional Neural Networks

against Adversarial Examples

Jake Zhao (Junbo) ∗
New York University

j.zhao@nyu.edu

Kyunghyun Cho

New York University & Facebook AI Research

CIFAR Azrieli Global Scholar

kyunghyun.cho@nyu.edu

Abstract

We propose a retrieval-augmented convolutional net-
work (RaCNN) and propose to train it with local mixup,
a novel variant of the recently proposed mixup algorithm.
The proposed hybrid architecture combining a convolu-
tional network and an off-the-shelf retrieval engine was de-
signed to mitigate the adverse effect of off-manifold adver-
sarial examples, while the proposed local mixup addresses
on-manifold ones by explicitly encouraging the classiﬁer to
locally behave linearly on the data manifold. Our eval-
uation of the proposed approach against seven readily-
available adversarial attacks on three datasets–CIFAR-10,
SVHN and ImageNet–demonstrate the improved robustness
compared to a vanilla convolutional network, and compa-
rable performance with the state-of-the-art reactive defense
approaches.

1. Introduction

Since the initial investigation in [35], adversarial exam-
ples have drawn a large interest. Various methods for both
generating adversarial examples as well as protecting a clas-
siﬁer from them have been proposed. According to [9], ad-
versarial examples can be categorized into those off the data
manifold, which is deﬁned as a manifold on which training
examples lie, and those on the data manifold. Off-manifold
adversarial examples occur as the classiﬁer does not have a
chance to observe any off-manifold examples during train-
ing, which is a natural consequence from the very deﬁni-
tion of the data manifold. On-manifold adversarial exam-
ples however exist between training examples on the data
manifold. There are two causes behind this phenomenon;
(1) the sparsity of training examples and (2) the non-smooth
behavior of the classiﬁer on the data manifold.

We propose to tackle both off- and on-manifold adver-
sarial examples by incorporating an off-the-shelf retrieval

∗Work done while the author worked at Facebook AI Research.

mechanism which indexes a large set of examples and train-
ing this combination of a deep neural network classiﬁer and
the retrieval engine to behave linearly on the data manifold
using a novel variant of the recently proposed mixup algo-
rithm [39], to which we refer as “local mixup.”

The retrieval mechanism efﬁciently selects a subset of
neighboring examples from a candidate set near the input.
These neighboring examples are used as a local approxi-
mation to the data manifold in the form of a feature-space
convex hull onto which the input is projected. The clas-
siﬁer then makes a decision based on this projected input.
This addresses off-manifold adversarial examples. Within
this feature-space convex hull, we encourage the classiﬁer
to behave linearly by using local mixup to further address
on-manifold adversarial examples.

We evaluate the proposed approach, called a retrieval-
augmented classiﬁer, with a deep convolutional net-
work [22] on object recognition. We extensively test the
retrieval-augmented convolutional network (RaCNN) on
datasets with varying scales; CIFAR-10 [20], SVHN [26]
as well as ImageNet [7], against seven readily-available ad-
versarial attacks including both white-box (FGSM, iFGSM,
DeepFool, L-BFGS, CW and PGD) and black-box attacks
(Boundary). Our experiments reveal that the RaCNN is
more robust to these attacks than the vanilla convolutional
network, and also achieve comparable robustness with other
reactive state-of-the-art defense methods. To the best of our
knowledge, RaCNN by far achieves the best results among
the proactive defense methods.

2. Retrieval-Augmented CNN

In [9], it was recently demonstrated that adversarial ex-
amples exist both on and off the data manifold. This result
suggests that it is necessary to tackle both types of adver-
sarial examples to improve the robustness of a deep neu-
ral network based classiﬁer to adversarial examples. In this
section, we describe our approach toward building a more
robust classiﬁer by combining an off-the-shelf retrieval en-

111563

gine and a variant of the recently proposed mix-up learning
strategy.

approximated manifold to maximize the classiﬁcation ac-
curacy.

1, y′

M , y′

1), . . . , (x′

Let D′ = {(x′

M )} be a candidate set
of examples. This set may be created as a subset from a
training set D = {(x1, y1), . . . , (xN , yN )} or may be an
entire separate set. We use D′ as a proxy to the underlying
data manifold. k(x, x′) is a distance function that measures
the dissimilarity between two inputs x and x′. In order to
facilitate the use of an off-the-shelf retrieval engine, we use
k(x, x′) = kφ′(x) − φ′(x′)k2, where φ′ is a predeﬁned, or
pretrained, feature extractor. We assume the existence of a
readily-available retrieval engine F that takes x as input and
returns the K nearest neighbors in D′ according to k(x, x′).
We then have a deep neural network classiﬁer composed of
a feature extraction φ and a classiﬁer g. This classiﬁer is
trained on a training set D, taking into account the extra set
D′ and the retrieval engine.

2.1. Inference

Local Characterization of Data Manifold Given a new
input x, we use the retrieval engine F to retrieve the
examples x′
k’s from D′ that are closest to x: F (x) =
1, . . . , x′
{x′
K}. We then build a feature-space convex hull

k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

C(F (x)) =( K
Xk=1

αkφ(x′

αk = 1 ∧ ∀k : αk ≥ 0) .

K

Xk=1

As observed earlier, linear interpolation of two input vec-
tors in the feature space of a deep neural network often cor-
responds to a plausible input vector [2, 19, 27]. Based on
this, we consider the feature-space convex hull C(F (x)) as
a reasonable local approximation to the data manifold.

Trainable Projection Exact projection of the input x onto
this convex hull C(F (x)) requires expensive optimization,
especially in the high-dimensional space. As we consider a
deep neural network classiﬁer, the dimension of the feature
space φ′ could be hundreds or more, making this exact pro-
jection computationally unfeasible. Instead, we propose to
learn a goal-driven projection procedure based on the atten-
tion mechanism [1].

We compare each input x′

k ∈ F (x) against x and com-
pute a score: βk = φ(x′
k)⊤U φ(x), where U is a trainable
weight matrix [24]. These scores are then normalized to
form a set of coefﬁcients: αk =
. These co-
efﬁcients αk’s are then used to form a projection point of x
in the feature-space convex hull

k′=1 exp(βk′ )

exp(βk)

PK

C(F (x)) : P(x) = PC(F (x))(x) =

αkφ(x′

k).

K

Xk=1

This trainable projection could be thought of as learn-
ing to project an off-manifold example on the locally-

Classiﬁcation The projected feature PC(F (x))(x) now
represents the original input x and is fed to a ﬁnal classi-
ﬁer g. In other words, we constrain the ﬁnal classiﬁer to
work only with a point inside a feature-space convex hull
of neighboring training examples. This constraint alleviates
the issue of the classiﬁer’s misbehaviors in the region out-
side the data manifold up to a certain degree.1

Randomization At the expense of computational over-
head, we can improve the robustness further. We retrieve
K ′ = cK examples, where c ≫ 1, and uniformly se-
lect K examples at random to form a feature-space convex
hull. We evaluate this approach later against the strongest
attack [5].

2.2. Training

The output of the classiﬁer g(P(x)) is almost fully dif-
ferentiable w.r.t. the classiﬁer g, both of the features extrac-
tors (φ′ and φ) and the attention weight matrix U , except
for the retrieval engine F .2 This allows us to train the en-
tire pipeline using backpropagation [31] and gradient-based
optimization.

Local Mixup This is however not enough to ensure the
robustness of the proposed approach to on-manifold ad-
versarial examples. During training, the classiﬁer g only
observes a very small subset of any feature-space convex
hull. Especially in a high-dimensional space, this greatly
increases the chance of the classiﬁer’s misbehaviors within
these feature-space convex hulls, as also noted in [9]. In
order to address this issue, we propose to augment learning
with a local variant of the recently proposed mix-up algo-
rithm [39].

The goal of the original mixup is to encourage a classiﬁer
to act linearly between any pair of training examples. This
is done by linearly mixing in two randomly-drawn train-
ing examples and creating a new linearly-interpolated ex-
ample pair during training. Let two randomly-drawn pairs
be (xi, yi) and (xj, yj), where yi and yj are one-hot vec-
tors in the case of classiﬁcation. Mixup creates a new pair
(λxi + (1 − λ)xj, λyi + (1 − λ)yj) and uses it as a training
example, where λ ∈ [0, 1] is a random sample from a beta
distribution. We call this original version global mixup, as
it increases the linearity of the classiﬁer between any pair of
training examples.

1 The quality of the local approximation may not be uniformly high

across the input space.

2 The introduction of this non-differentiable retrieval engine further

contributes to the increased robustness.

11564

It

is however unnecessary for our purpose to use
global mixup, as our goal is to make the classiﬁer bet-
linearly behave) within a feature-space
ter behave (i.e.,
convex hull C(F (x)).
Thus, we use a local mixup
in which we uniformly sample the convex coefﬁcients
αk’s at random to create a new mixed example pair
k). We use the Kraemer Algo-

k=1 αkφ(x′

k=1 αky′

(PK

rithm [33].

k),PK

Overall We use stochastic gradient descent (SGD) for
training. At each update, we perform NCE descent steps
for the usual classiﬁcation loss, and NMU descent steps for
the proposed local mixup.

2.3. Retrieval Engine F

The proposed approach does not depend on the speciﬁcs
of a retrieval engine F . Any off-the-shelf retrieval engine
that supports dense vector lookup could be used, enabling
the use of a very large-scale D′ with latest fast dense vector
lookup algorithms [16]. In this work, we used a more rudi-
mentary retrieval engine based on locality-sensitive hash-
ing [6] with a reduced feature dimension using random pro-
jection [3], as the sizes of candidate sets D′ contain approx-
imately 1M or less examples. The key φ′(x) was chosen to
be a pretrained deep neural network without the ﬁnal fully-
connected classiﬁer layers [21, 14].

3. Attack Scenarios

S1 (Direct Attack)
In this work, we consider the candi-
date set D′ and the retrieval engine which indexes it to be
“hidden” from the outside world. This property makes a
usual white-box attack more of a gray-box attack in which
the attacker has access to the entire system except for the
retrieval part.

S2 (Retrieval Attack) Despite the hidden nature of the
retrieval engine and the candidate set, it is possible for the
attacker to confuse the retrieval engine, if they could ac-
cess the feature extractor φ′. We furthermore give the at-
tacker the access not only to φ′ but the original classiﬁer g′
which was tuned together with φ′. This allows the attacker
to create an adversarial example on g′(φ′(x)) that could po-
tentially disrupt the retrieval process. Although unlikely in
practice, we test this second scenario to investigate the pos-
sibility of compromising the retrieval engine.

4. Attack Methods

Fast gradient sign method (FGSM) creates an adver-
sarial example by adding the scaled sign of the gradient of
the loss function L computed using a target class ˆy to the in-
put. Iterative FGSM (iFGSM) improves upon the FGSM
by iteratively modifying the input x for a ﬁxed number of

steps. DeepFool was proposed in [25] to create an adver-
sarial example by ﬁnding a residual vector r ∈ Rdim(x) with
the minimum Lp-norm with the constraint that the output
of a classiﬁer must ﬂip. L-BFGS from [36] more explic-
itly constrains the input to lie inside a tight box deﬁned by
training examples using L-BFGS-B [40]. CW from [5] di-
rectly solves the original formulation of adversarial exam-
ples [35] and has been found recently to defeat most of the
recently proposed defense mechanisms [4]. We consider
this the strongest attack and test the self-ensemble approach
against it.

Boundary brendel2017decision proposed a powerful
black-box attack, or more speciﬁcally decision-based at-
tack, that requires neither the gradient nor the predictive
distribution. It only requires the ﬁnal decision of the clas-
siﬁer. Starting from an adversarial example, potentially far
away from the original input, it iteratively searches for a
next adversarial example closer to the original input.

5. Related Work

Data-independent transformation aims at minimizing
regions that are off the data manifold. dziugaite2016study
demonstrated that JPEG-compressed images suffer less
from adversarial attacks. lu2017no suggest that trying vari-
ous scaling of an image size could overcome adversarial at-
tacks. guo2017countering use compressed sensing to trans-
form an input image by reconstructing it from its lower-
resolution version while minimizing the total variation [30].
More recently, buckman2018thermometer proposed to dis-
cretize each input dimension using thermometer coding.
These approaches are attractive due to their simplicity, but
there have some work showing that it is not enough to de-
fend against sophisticated adversarial examples [32].

[38].

Data-Dependent Transformation relies on data-
dependent transformation. gu2014towards use a denoising
samangouei2018defensegan and
autoencoder
song2017pixeldefend respectively use a pixelCNN [37]
and generative adversarial network [10] to replace an
input image with a nearby image.
Instead of using a
separately trained generative model, guo2017countering
uses a technique of image quilting [8]. These approaches
are similar to our use of a retrieval engine. They however
do not address the issue of misbehaviors of a classiﬁer on
the data manifold.

Adversarial training trains a classiﬁer on both train-
ing examples and adversarial examples generated on-
the-ﬂy [11].
lee2017generative extended this proce-
dure by introducing a GAN that learns to generate ad-
Instead, robust optimization di-
versarial examples.
rectly modiﬁes a learning algorithm to induce robustness.
cisse2017parseval proposed Parseval training that encour-
ages the Lipschitz constant to be less than one. More

11565

L2

Clean

0

Baseline
RC-K5

85.15
72.57
75.6
79.52
RC-K10-MU 80.80

RC-K5-MU

RC-K10

FGSM
2e-04

7.5

34.29
37.9
43.9
44.01

1e-04
14.05
42.97
46.37
52.95

53

4e-04
4.22
24.55
28.11
33.77
33.47

1e-05
55.2
72.57
74.89
79.12
79.87

iFGSM
2e-05
26.17
72.48
74.89

79

79.72

8e-05
2.59
45.46
48.12
55.27
54.36

1e-05
26.04
64.34
66.96
72.89
73.63

DeepFool

2e-05
11.72
61.34
63.84
71.81
72.35

8e-05
0.34
60.96
63.55
71.14
71.26

Table 1. The CIFAR-10 classiﬁers’ robustness to the adversarial attacks in the S2 (Retrieval Attack)

FGSM

iFGSM

DeepFool

L-BFGS

Boundary

Figure 1. The CIFAR-10 classiﬁers’ robustness to the adversarial attacks in the Scenario 1 (Direct Attack). The x-axis indicates the strength
of attack in terms of the normalized L2 distance. The y-axis corresponds to the accuracy.

recently, sinha2018certiﬁable proposed a tractable robust
optimization algorithm It ensures that the classiﬁer well-
behaves in the neighborhood of each training point.

the other hand, we ﬁx φ = φ′ and train g from the pre-
trained ResNet-18 above. The latter was done, as we ob-
served it greatly reduced training time in the preliminary
experiments.

Retrieval-Augmented Neural Networks The proposed
approach tightly integrates an off-the-shelf retrieval engine
into a deep neural network. A similar approach has recently
gained popularity. gu2017search use a text-based retrieval
engine to build a non-parametric neural machine translation
system. wang2017k proposed a similar approach to text
classiﬁcation, and guu2017generating to language model-
ing. It was also applied to online learning [23, 34].

6. Experiments

Datasets We test the proposed approach (RaCNN or RC)
on three datasets of different scales. CIFAR-10 has 50k
training and 10k test examples, with 10 classes. SVHN has
73k training and 26k test examples, with 10 classes. Ima-
geNet has 1.3M training and 50k validation examples with
1,000 classes. For CIFAR-10 and ImageNet, we use the
original training set as a candidate set, i.e., D′ = D, while
we use the extra set of 531k examples as a candidate set
in the case of SVHN. The overall training process involves
data augmentation on D but not D′.

Architectures We train a deep convolutional network for
each dataset, remove the ﬁnal fully-connected layers and
use the remaining stack as a ﬁxed feature extractor φ for re-
trieval. We use the same convolutional network from above
for the RaCNN (RC) as well (separated into φ and g by the
ﬁnal average pooling) for each dataset. For CIFAR-10 and
SVHN, we train φ and g from scratch. For ImageNet, on

Training and Evaluation We use Adam [18] as an opti-
mizer and investigate the inﬂuence of the newly introduced
components–retrieval and local mixup (MU)– by varying
K ∈ {5, 10} and NMU ∈ {0, 5}.

In addition to the accuracy on the clean test set,
we look at the accuracy per the amount of perturbation
used to create adversarial examples. We use the de-
fault MeanSquaredDistance from the Foolbox library;
this amount is computed as a normalized L2 distance be-
tween the original example x and its perturbed version
˜x: L2(x, ˜x) =

dim(x)∗(cid:0) max(x)−min(x)(cid:1)2 . We use Foolbox

[29, 28] for all the attacks other than CW for which we use
the implementation from [12, 13]. 3 Our attacks are gener-
ally performed with clipping the outbounded pixel values at
each step.

kx−˜xk2

2

6.1. CIFAR 10

In the CIFAR-10 experiments, our model contains 6 con-
volutional layers and 2 fully-connected layers. For each
layer we employ batch normalization [15] and ReLU fol-
lowing convolution.

3Noted that the Foolbox implementation searches over many attack hy-
perparameters to obtain the most optimized perturbation distance while
achieving fooling. The implementation from [12, 13] differs in that it uses
one empirically suitable set of hyperparameters to save computation. That
being said, it might not be fair to compare attack strengths if they are im-
plemented in different libraries.

11566

S1 (Direct Attack) We present in Fig. 1 the effect of ad-
versarial attacks with varying strengths (measured in the
normalized L2 distance) on both the vanilla convolutional
network (Baseline) and the proposed RaCNN’s with various
settings. Across all the adversarial attacks, it is clear that
the proposed RaCNN is more robust to adversarial exam-
ples than the vanilla classiﬁer is. The proposed local mixup
improves the robustness further, especially when the num-
ber of retrieved examples is small, i.e., K = 5. We conjec-
ture that this is due to the quadratically increasing number
of pairs, i.e., K(K−1)
, for which local mixup must take care
of, with respect to K.

2

S2 (Retrieval Attack)
In Table 1, we present the accuracy
of both the baseline and RaCNN’s with varying strengths of
white-box attacks, when the feature extractor φ′ for the re-
trieval engine is attacked. We observe that it is indeed pos-
sible to fool the proposed RaCNN by attacking the retrieval
process. Comparing Fig. 1 and Table 1, we however notice
that the performance degradation is much less severe in this
second scenario.

6.2. SVHN

We use the same architecture and hyper-parameter set-

ting as in the CIFAR-10 experiments.

S1 (Direct Attack) On SVHN, we observe a similar trend
from CIFAR-10. The proposed RaCNN is more robust
against all the adversarial attacks compared to the vanilla
convolutional network. Similarly to CIFAR-10, the pro-
posed approach is most robust to DeepFool and Boundary,
while it is most susceptible to L-BFGS. We however notice
that the impact of local mixup is larger with SVHN than
was with CIFAR-10.

Another noticeable difference is the impact of the num-
ber of retrieved examples on the classiﬁcation accuracy. In
the case of CIFAR-10, the accuracies on the clean test ex-
amples (the ﬁrst column in Table 1) between using 5 and
10 retrieved examples differ signiﬁcantly, while it is much
less so with SVHN (the ﬁrst column in Table 2.) We con-
jecture that this is due to a lower level of variation in input
examples in SVHN, which are pictures of house numbers
taken from streets, compared to those in CIFAR-10, which
are pictures of general objects.

S2 (Retrieval Attack) We observe a similar trend be-
tween CIFAR-10 and SVHN, when the feature extractor φ′
for retrieval was attacked, as shown in Tables 1–2.

6.3. ImageNet

We use ResNet-18 [14]. We pretrain it as a standalone
classiﬁer on ImageNet and use the feature extractor part φ′

for retrieval. We use the same feature extractor φ = φ′ for
the RaCNN without updating it. The classiﬁer g is initial-
ized with g′ and tuned during training. In the case of Ima-
geNet, we only try K = 10 retrieved examples with local
mixup. Due to the high computational cost of the L-BFGS
and Boundary attacks, we evaluate both the vanilla classi-
ﬁer and RaCNN against these two attacks on 200 images
drawn uniformly at random from the validation set. We use
Accuracy@5 which is a standard metric with ImageNet.

S1 (Direct Attack) A general trend with ImageNet is sim-
ilar to that with either CIFAR-10 or SVHN, as can be seen in
Fig. 3. The proposed RaCNN is more robust to adversarial
attacks. We however do observe some differences. First,
iFGSM is better at compromising both the baseline and
RaCNN than L-BFGS is, in this case. Second, DeepFool
is much more successful at fooling the baseline convolu-
tional network on ImageNet than on the other two datasets,
but is much less so at fooling the proposed RaCNN.

S2 (Retrieval Attack) Unlike CIFAR-10 and SVHN, we
have observed that the retrieval attack is sometimes more
effective than the direct attack in the case of ImageNet. For
instance, FGSM can compromise the retrieval feature ex-
tractor φ′ to decrease the accuracy from 77.68 down to 0.20
at L2 = 10−4. We observed a similar behavior with Deep-
Fool, but not with iFGSM.

6.4. CW and Randomization: S1 – Direct Attack

As the CW attack and its variants are the state-of-the-
art attacks available to date [5], we consider it separately
from the other attacks. We pick the proposed RaCNN with
K = 10 and local mixup and evaluate the randomiza-
tion strategy’s robustness to this attack. On CIFAR-10 and
SVHN, we use the second-most-likely prediction as a tar-
get and the strength κ = 0. On the other hand, we use the
sixth most-likely prediction and try both κ = 0 and 10 with
ImageNet, as we report Accuracy@5.

As demonstrated in Fig. 4, the CW attack is effective at
compromising the baseline. On CIFAR-10 and SVHN, CW
is further effective at compromising the proposed RaCNN,
although randomized inference greatly improves the robust-
ness. We however observe an opposite trend on ImageNet,
where randomized inference suffers when the strength of
the CW attack is increased. Nevertheless, we observe that
the proposed approach is more robust than the baseline is.

6.5. Comparison with other SOTA defense ap 

proaches

In Table 4, we compare RaCNN with other best pub-
lished defense approaches. We adopt another strong attack,
the projected gradient descent (PGD), as our benchmarking
attack protocol. We can see from these results that all the

11567

L2

Clean

0

Baseline
RC-K5

95.48
90.78
91.64
92.19
RC-K10-MU 92.49

RC-K5-MU

RC-K10

FGSM
4e-04
30.95
53.31
57.20
52.24
57.30

2e-04
42.09
64.87
68.31
64.94
68.72

8e-04
21.61
39.44
43.73
37.73
43.49

2e-05
70.41
90.73
91.55
92.10
92.45

iFGSM
8e-05
35.53
75.80
77.74
76.41
78.26

2e-04
11.17
63.41
65.75
62.70
65.50

2e-05
51.10
84.62
86.18
86.18
87.33

DeepFool

8e-05
16.00
81.30
83.20
84.25
84.73

2e-04
4.28
80.55
82.43
82.21
84.10

Table 2. The SVHN classiﬁers’ robustness to the adversarial attacks in the S2 (Retrieval Attack)

FGSM

iFGSM

DeepFool

L-BFGS

Boundary

Figure 2. The SVHN classiﬁers’ robustness to the adversarial attacks in the Scenario 1 (Direct Attack). The x-axis indicates the strength of
attack in terms of the normalized L2 distance. The y-axis corresponds to the accuracy.

reported defense methods have degraded the clean classiﬁ-
cation performance to some extent.

on clean examples, which is a trade-off that needs to be de-
termined per task.

Although RaCNN does not outperform the SOTA reac-
tive approaches proposed by [12], we argue that RaCNN as
a proactive defense approach can easily be combined with
any reactive defense method. We want to leave this combi-
nation to future work.

6.6. Summary Discussion

We have observed that the proposed RaCNN, when
trained with the local mixup, is more robust to adversarial
attacks, at least those seven considered in the experiments,
than the vanilla convolutional network. More speciﬁcally,
the RaCNN was most robust to the black-box, decision-
based attack, while it was more easily compromised by
white-box attacks, especially by the CW and L-BFGS at-
tacks. This suggests that the RaCNN could be an attractive
alternative to the vanilla convolutional network when de-
ployed , for instance, in a cloud-based environment.

In Fig. 5, we show retrieval results given a query im-
age from ImageNet. Although adversarial attack altered
the retrieval engine’s behavior, we see that the semantics of
the query image could still be maintained in retrieved im-
ages, suggesting two insights. First, the robustness of the
RaCNN is largely due to the robustness of the retrieval en-
gine to small perturbation. Even when the retrieval quality
degrades, a majority of retrieved examples are of a similar
class. Second, we could further improve the robustness by
designing the feature extractor φ′ more carefully. For in-
stance, an identity function φ′(x) = x would correspond to
retrieval based on the raw pixels, which would make the re-
trieval engine robust to any adversarial attack imperceptible
to humans. This may however results in a lower accuracy

The robustness of the proposed RaCNN comes at the ex-
pense of the generalization performance on clean input ex-
amples. We have observed however that this degradation
could be controlled at the expense of computational over-
head by varying the number of retrieved examples per input.
This controllability could be an important feature when de-
ploying such a model in production.

7. Conclusion

In this paper, we proposed a novel retrieval-augmented
convolutional network classiﬁer (RaCNN) that integrates
an off-the-shelf retrieval engine to counter adversarial at-
tacks. The RaCNN was designed to tackle both off- and
on-manifold adversarial examples, and to do so, we use a
retrieval engine to locally characterize the data manifold as
a feature-space convex hull and the attention mechanism to
project the input onto this convex hull. The entire model,
composed of the retrieval engine and a deep convolutional
network, is trained jointly by the novel local mixup learning
strategy which encourages the classiﬁer to behave linearly
within the feature-space convex hull.

We have evaluated the proposed approach on three
standard benchmarks–CIFAR-10, SVHN and ImageNet–
against six white-box attacks and one black-box, decision-
based attack. The experiments have revealed that the pro-
posed approach is more robust than the vanilla convolu-
tional network in all the cases. The RaCNN was found to be
especially robust to the black-box, decision-based attack.

The proposed approach consists of three major com-
ponents; (1) local characterization of data manifold, (2)
data manifold projection and (3) regularized learning on the

11568

L2

Clean

0

Baseline

88.98
RC-K10-MU 77.68

FGSM
2e-04
13.12
17.40

1e-04

15

20.17

4e-04
11.65
14.70

1e-05
9.59
77.28

iFGSM
2e-05
3.57
64.97

4e-05
1.82
17.67

1e-05
0.29
35.74

DeepFool

2e-05
0.17
35.72

4e-05
0.16
35.71

Table 3. The ImageNet classiﬁers’ robustness to the adversarial attacks in the S2 (Retrieval Attack).

FGSM

iFGSM

DeepFool

L-BFGS

Boundary

Figure 3. The ImageNet classiﬁers’ robustness to the adversarial attacks in the Scenario 1 (Direct Attack). The x-axis indicates the strength
of attack in terms of the normalized L2 distance. The y-axis corresponds to the accuracy. The adversary utilizes top-5 accuracies for
attacks.

CIFAR-10

SVHN

ImageNet

Figure 4. The robustness of the classiﬁers to the CW attack under
the Scenario 1 (Direct Attack). The x-axis indicates the strength
of attack in terms of the normalized L2 distance. The y-axis corre-
sponds to the accuracy. “ Rand” indicates that inference was done
with the randomization strategy.

Name

Vanilla CNN

Crop Ensemble [12]
TV Minimization [12]

Image Quilting [12]

Adversarial Logit Pairing [17]

RaCNN

RaCNN + Rand

Clean Acc

S1 / Gray-box Acc

69.5

63.2
60.9
39.8
52.8

60.2
60.1

3.1

41.5
32.5
35.4
24.5

25.9
28.4

Table 4. The comparison of RaCNN with other SOTA defense ap-
proaches. The results are obtained subject to kx− ˜xk2
kxk2

≤ 0.06.

Clean

iFGSM (S1 – Direct Attack) with L2 = 2 × 10−5

iFGSM (S2 – Retrieval Attack) with L2 = 2 × 10−5

iFGSM (S1 – Direct Attack) with L2 = 4 × 10−5

iFGSM (S2 – Retrieval Attack) with L2 = 4 × 10−5

Figure 5. The left-most column of each panel shows the query
image, and the next ﬁve images have been retrieved by F . We
show the retrieval results using the original image (Clean) and the
adversarial images one panel at a time. Even with noise largely
enough to fool any vanilla convolutional network, the retrieval en-
gine changes largely maintains the semantics of the query image.

manifold. There is a large room for improvement in each
of these components. For instance, a feature-space convex
hull may be replaced with a more sophisticated kernel esti-
mator. Projection onto the convex hull could be done better,
and a better learning algorithm could further improve the
robustness against on-manifold adversarial examples. We
leave these possibilities as future work.

References

[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473, 2014. 2

[2] Y. Bengio, G. Mesnil, Y. Dauphin, and S. Rifai. Better mix-
In Proceedings of the 30th

ing via deep representations.

11569

International Conference on Machine Learning (ICML-13),
2013. 2

[20] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. Technical report, 2009. 1

[3] E. Bingham and H. Mannila. Random projection in dimen-
sionality reduction: applications to image and text data. In
KDD. ACM, 2001. 3

[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, 2012. 3

[4] N. Carlini and D. Wagner. Adversarial examples are not eas-
ily detected: Bypassing ten detection methods. In Proceed-
ings of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security, pages 3–14. ACM, 2017. 3

[5] N. Carlini and D. Wagner. Towards evaluating the robustness
of neural networks. In Security and Privacy (SP), 2017 IEEE
Symposium on, pages 39–57. IEEE, 2017. 2, 3, 5

[6] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni.
Locality-sensitive hashing scheme based on p-stable distri-
butions. In Proceedings of the twentieth annual symposium
on Computational geometry. ACM, 2004. 3

[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, 2009. 1

[8] A. A. Efros and W. T. Freeman.

Image quilting for tex-
ture synthesis and transfer. In Proceedings of the 28th an-
nual conference on Computer graphics and interactive tech-
niques. ACM, 2001. 3

[9] J. Gilmer, L. Metz, F. Faghri, S. S. Schoenholz, M. Raghu,
M. Wattenberg, and I. Goodfellow. Adversarial spheres.
arXiv preprint arXiv:1801.02774, 2018. 1, 2

[10] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, 2014. 3

[11] I. J. Goodfellow, J. Shlens, and C. Szegedy.

ing and harnessing adversarial examples.
arXiv:1412.6572, 2014. 3

Explain-
arXiv preprint

[12] C. Guo, M. Rana, M. Cisse, and L. van der Maaten. Coun-
tering adversarial images using input transformations. arXiv
preprint arXiv:1711.00117, 2017. 4, 6, 7

[13] C. Guo, M. Rana, M. Cisse, and L. van der Maaten.
Countering adversarial
transforma-
tions. https://github.com/facebookresearch/
adversarial_image_defenses, 2017. [Online]. 4

images using input

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, 2016. 3,
5

[15] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International conference on machine learning, 2015. 4

[16] J. Johnson, M. Douze, and H. J´egou. Billion-scale similarity
search with gpus. arXiv preprint arXiv:1702.08734, 2017. 3

[17] H. Kannan, A. Kurakin, and I. Goodfellow. Adversarial logit

pairing. arXiv preprint arXiv:1803.06373, 2018. 7

[18] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 4

[19] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013. 2

[22] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 1998. 1

[23] X. Li,

J. Zhang, and C. Zong.

model for neural machine translation.
arXiv:1609.06490, 2016. 4

One sentence one
arXiv preprint

[24] M.-T. Luong, H. Pham, and C. D. Manning. Effective
approaches to attention-based neural machine translation.
arXiv preprint arXiv:1508.04025, 2015. 2

[25] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deep-
fool: a simple and accurate method to fool deep neural net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2016. 3

[26] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and unsu-
pervised feature learning, 2011. 1

[27] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 2

[28] J. Rauber and W. Brendel. Foolbox. http://foolbox.
readthedocs.io/en/latest/, 2017. [Online; Revi-
sion 2d468cb6]. 4

[29] J. Rauber, W. Brendel, and M. Bethge. Foolbox v0. 8.0:
A python toolbox to benchmark the robustness of machine
learning models. arXiv preprint arXiv:1707.04131, 2017. 4

[30] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total varia-
tion based noise removal algorithms. Physica D: nonlinear
phenomena, 1992. 3

[31] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning
representations by back-propagating errors. Nature, 1986. 2

[32] R. Shin and D. Song. Jpeg-resistant adversarial images. In
MAchine LEarning and Computer Security Workshop, 2017.
3

[33] N. A. Smith and R. W. Tromble. Sampling uniformly from
the unit simplex. Johns Hopkins University, Tech. Rep, 2004.
3

[34] P. Sprechmann, S. Jayakumar, J. Rae, A. Pritzel, A. P. Badia,
B. Uria, O. Vinyals, D. Hassabis, R. Pascanu, and C. Blun-
dell. Memory-based parameter adaptation.
International
Conference on Learning Representations, 2018. 4

[35] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199, 2013. 1, 3

[36] P. Tabacof and E. Valle. Exploring the space of adversarial
In Neural Networks (IJCNN), 2016 International

images.
Joint Conference on. IEEE, 2016. 3

[37] A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,
A. Graves, et al. Conditional image generation with pixel-
cnn decoders. In Advances in Neural Information Processing
Systems, 2016. 3

11570

[38] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.
Manzagol. Stacked denoising autoencoders: Learning use-
ful representations in a deep network with a local denoising
criterion. Journal of Machine Learning Research, 2010. 3

[39] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz.
mixup: Beyond empirical risk minimization. arXiv preprint
arXiv:1710.09412, 2017. 1, 2

[40] C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. Algorithm
778: L-BFGS-B: Fortran subroutines for large-scale bound-
constrained optimization. ACM Transactions on Mathemati-
cal Software (TOMS), 1997. 3

11571

