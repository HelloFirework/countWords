Automatic Face Aging in Videos via Deep Reinforcement Learning

Chi Nhan Duong 1, Khoa Luu 2, Kha Gia Quach 1, Nghia Nguyen 2,

Eric Patterson 3, Tien D. Bui 1, Ngan Le 4

1 Computer Science and Software Engineering, Concordia University, Canada
2 Computer Science and Computer Engineering, University of Arkansas, USA

3 School of Computing, Clemson University, USA

4 Electrical and Computer Engineering, Carnegie Mellon University, USA

1{dcnhan, kquach}@ieee.org, bui@encs.concordia.ca, 2{khoaluu, nhnguyen}@uark.edu,

3ekp@clemson.edu, 4thihoanl@andrew.cmu.edu

Abstract

This paper presents a novel approach to synthesize auto-
matically age-progressed facial images in video sequences
using Deep Reinforcement Learning. The proposed method
models facial structures and the longitudinal face-aging
process of given subjects coherently across video frames.
The approach is optimized using a long-term reward, Re-
inforcement Learning function with deep feature extraction
from Deep Convolutional Neural Network. Unlike previous
age-progression methods that are only able to synthesize an
aged likeness of a face from a single input image, the pro-
posed approach is capable of age-progressing facial like-
nesses in videos with consistently synthesized facial features
across frames. In addition, the deep reinforcement learning
method guarantees preservation of the visual identity of in-
put faces after age-progression. Results on videos of our
new collected aging face AGFW-v2 database demonstrate
the advantages of the proposed solution in terms of both
quality of age-progressed faces, temporal smoothness, and
cross-age face veriﬁcation.

1. Introduction

Age-related facial technologies generally address the
two areas of age estimation [4, 21, 20, 23, 8, 22] and age
progression [6, 29, 46, 30, 41, 35]. The face age-estimation
problem is deﬁned as building computer software that has
the ability to recognize the ages of individuals in a given
photograph. Comparatively, the face age-progression prob-
lem necessitates the more complex capability to predict the
future facial likeness of people appearing in images [19].
Aside from the innate curiosity of individuals, research
of face aging has its origins in cases of missing persons
and wanted fugitives, in either case law enforcement de-

Figure 1: Given an input video, while frame-based ap-
proaches produce inconsistent aging features, our video-
based method ensures consistency among video frames.

sires plausible age-progressed images to facilitate searches.
Accurate face aging also provides beneﬁts for numerous
practical applications such as age-invariant face recogni-
tion [44, 43, 17]. There have been numerous anthropo-
logical, forensic, computer-aided, and computer-automated
approaches to facial age-progression. However, the results
from previous methods for synthesizing aged faces that rep-
resent accurate physical processes involved in human ag-
ing are still far from perfect. This is especially so in age-
progressing videos of faces, due to the usual challenges for

110013

Table 1: The comparison of the properties between our video-based approach and other age progression methods.

Modality
Temporal Consistency
Aging Mechanism
Architecture
Tractability

Ours

Video-based

ICPGAN [41]
Image-based

TNVP [9]

Image-based

CAAE [46]
Image-based

RFA [40]

Image-based

TRBM [7]
Image-based

Yes

One-shot
DL + RL

✓

No

No

No

No

No

One-shot

Multiple-shot

One-shot

One-shot

Multiple-shot

DL
✓

DL
✓

DL
✓

DL
✓

DL
✗

face processing involving pose, illumination, and environ-
ment variation as well as differences between video frames.
There have been two key research directions in age pro-
gression for both conventional computer-vision approaches
and recent deep-learning methods – one-shot synthesis and
multiple-shot synthesis. Both approaches have used facial
image databases with longitudinal sample photos of indi-
viduals, where the techniques attempt to discover aging pat-
terns demonstrated over individuals or the population rep-
resented. In one-shot synthesis approaches, a new face at
the target age is directly synthesized via inferring the rela-
tionships between training images and their corresponding
age labels then applying them to generate the aged likeness.
These prototyping methods [2, 15, 33] often classify train-
ing images in facial image databases into age groups ac-
cording to labels. Then the average faces, or mean faces,
are computed to represent the key presentation or archetype
of their groups. The variation between the input age and the
target age archetypes is complimented to the input image
to synthesize the age-progressed faces at the requested age.
In a similar way, Generative Adversarial Networks (GANs)
[46, 41] methods present the relationship between semantic
representation of input faces and age labels by constructing
a deep neural network generator. It is then combined with
the target age labels to synthesize output results.

Meanwhile, in multiple-shot synthesis, the longitudinal
aging process is decomposed into multiple steps of aging
effects [9, 7, 35, 40, 45]. These methods build on the facial
aging transformation between two consecutive age groups.
Finally, the progressed faces from one age group to the next
are synthesized step-by-step until they reach the target age.
These methods can model the long-term sequence of face
aging using this strategy. However, these methods still have
drawbacks due to the limitations of long-term aging not be-
ing well represented nor balanced in face databases.

Existing age-progression methods all similarly suffer
from problems in both directions. Firstly, they only work
on single input images. Supposing there is a need to syn-
thesize aging faces presented in a captured video, these
methods usually have to split the input video into sepa-
rate frames and synthesize every face in each frame in-
dependently which may often present inconsistencies be-
tween synthesized faces. Since face images for each frame
are synthesized separately, the aging patterns of generated
faces of the same subject are also likely not coherent. Fur-
thermore, most aging methods are unable to produce high-

resolution images of age progression, important for features
such as ﬁne lines that develop fairly early in the aging pro-
cess. This may be especially true in the latent based meth-
ods [15, 9, 7, 35, 40, 45].

Contributions of this work: This paper presents a deep
Reinforcement Learning (RL) approach to Video Age Pro-
gression to guarantee the consistency of aging patterns in
synthesized faces captured in videos.
In this approach,
the age-transformation embedding is modeled as the opti-
mal selection using Convolutional Neural Network (CNN)
features under a RL framework. Rather than applying the
image-based age progression to each video frame indepen-
dently as in previous methods, the proposed approach has
the capability of exploiting the temporal relationship be-
tween two consecutive frames of the video. This prop-
erty facilitates maintaining consistency of aging informa-
tion embedded into each frame.
In the proposed struc-
ture, not only can a smoother synthesis be produced across
frames in videos, but also the visual ﬁdelity of aging data,
i.e. all images of a subject in different or the same age,
is preserved for better age transformations. To the best of
our knowledge, our framework is one of the ﬁrst face ag-
ing approaches in videos. Finally, this work contributes a
new large-scale face-aging database1 to support future stud-
ies related to automated face age-progression and age esti-
mation in both images and videos.

2. Related work

This section provides an overview of recent approaches
for age progression; these methods primarily use still im-
ages. The approaches generally fall into one of four
groups, i.e. modeling, reconstruction, prototyping, and
deep learning-based approaches.

Modeling-based approaches aim at modeling both shape
and texture of facial images using parameterization method,
then learning to change these parameters via an aging func-
tion. Active Appearance Models (AAMs) have been used
with four aging functions in [16, 28] to model linearly both
the general and the speciﬁc aging processes. Familial fa-
cial cues were combined with AAM-based techniques in
[24, 29]. [30] incorporated an AAM reconstruction method
to the synthesis process for a higher photographic ﬁdelity of
aging. An AGing pattErn Subspace (AGES) [14] was pro-
posed to construct a subspace for aging patterns as a chrono-

1https://face-aging.github.io/RL-VAP/

10014

Table 2: The properties of our collected AGFW-v2 in comparison with other aging databases. For AGFW-v2 video set, the
images of the subjects in old age are also collected for reference in terms of subject’s appearance changing.

Database
MORPH - Album 1 [31]
MORPH - Album 2 [31]
FG-NET [10]
AdienceFaces [18]
CACD [3]
IMDB-WIKI [32]
AgeDB [27]
AGFW [7]
AGFW-v2 (Image)
AGFW-v2 (Video)

# Images

# Subjects

1,690
55,134
1,002
26,580
163,446
52,3051
16,488
18,685
36,299
20,000

628

13,000

82

2,984
2,000
20,284

568

14,185
27,688

100

Label type
Years old
Years old
Years old
Age groups
Years old
Years old
Years old
Age groups
Age groups
Years old

Image type

Mugshot
Mugshot

In-the-wild
In-the-wild
In-the-wild
In-the-wild
In-the-wild

In-the-wild/Mugshot
In-the-wild/Mugshot
Interview/Movie-style

Subject type
Non-famous
Non-famous
Non-famous
Non-famous
Celebrities
Celebrities
Celebrities
Non-famous
Non-famous
Celebrities

Type

Image DB
Image DB
Image DB
Image DB
Image DB
Image DB
Image DB
Image DB
Image DB
Video DB

logical sequence of face images. In [38], AGES was en-
hanced with guidance faces consisting the subject’s charac-
teristics for more stable results. Three-layer And-Or Graph
(AOG) [37, 36] was used to model a face as a combination
of smaller parts, i.e. eyes, nose, mouth, etc. Then a Markov
chain was employed to learn the aging process for each part.

In reconstruction-based approaches, an aging basis is
uniﬁed in each group to model aging faces. Person-speciﬁc
and age-speciﬁc factors were independently represented by
sparse-representation hidden factor analysis (HFA) [45].
Aging dictionaries (CDL) [35] were proposed to model per-
sonalized aging patterns by attempting to preserve distinct
facial features of an individual through the aging process.

Prototyping-based approaches employed proto-typical
facial images in a method to synthesize faces. The average
face of each age group is used as the representative image
for that group, and these are named the “age prototypes”
[33]. Then, by computing the differences between the pro-
totypes of two age groups, an input face can be progressed
to the target age through image-based manipulation [2]. In
[15], high quality average prototypes constructed from a
large-scale dataset were employed in conjunction with the
subspace alignment and illumination normalization.

Recently, Deep learning-based approaches have yielded
promising results in facial age progression. Temporal and
Spatial Restricted Boltzmann Machines (TRBM) were in-
troduced in [7] to represent the non-linear aging process,
with geometry constraints, and to model a sequence of ref-
erence faces as well as wrinkles of adult faces. A Recur-
rent Neural Network (RNN) with two-layer Gated Recur-
rent Unit (GRU) was employed to approximate aging se-
quences [40]. Also, the structure of Conditional Adversarial
Autoencoder (CAAE) was applied to synthesize aged im-
ages in [1]. Identity-Preserved Conditional Generative Ad-
versarial Networks (IPCGANs) [41] brought the structure
of Conditional GANs with perceptual loss into place for
synthesis process. A novel generative probabilistic model,
called Temporal Non-Volume Preserving (TNVP) transfor-
mation [9] was proposed to model a long-term facial aging
as a sequence of short-term stages.

3. Data Collection

The quality of age representation in a face database is
one of the most important features affecting the aging learn-
ing process and could include such considerations as the
number of longitudinal face-image samples per subject, the
number of subjects, the range and distribution of age sam-
ples overall, and the population representation presented in
the database. Previous public databases used for age estima-
tion or progression systems have been very limited in the to-
tal number of images, the number of images per subject, or
the longitudinal separation of the samples of subjects in the
database, i.e. FG-NET [10], MORPH [31], AgeDB [27].
Some recent ones may be of larger scale but have noise
within the age labels, i.e. CACD [3], IMDB-WIKI [32].
In this work we introduce an extension of Aging Faces in
the Wild (AGFW-v2) in terms of both image and video col-
lections. Table 2 presents the properties of our collected
AGFW-v2 in comparison with others.

3.1. Image dataset

AGFW [7] was ﬁrst introduced with 18,685 images
with individual ages sampled ranging from 10 to 64 years
old. Based on the collection criteria of AGFW, a double-
sized database was desired. Compared to other age-related
databases, most of the subjects in AGFW-v2 are not public
ﬁgures and less likely to have signiﬁcant make-up or facial
modiﬁcations, helping embed accurate aging effects during
the learning process. In particular, AGFW-v2 is mainly col-
lected from three sources. Firstly, we adopt a search engine
using different keywords, e.g. male at 20 years old, etc.
Most images come from the daily life of non-famous sub-
jects. Besides the images, all publicly available meta-data
related to the subject’s age are also collected. The second
part comes from mugshot images that are accessible from
public domain. These are passport-style photos with ages
reported by service agencies. Finally, we also include the
Productive Aging Laboratory (PAL) database [26]. In total,
AGFW-v2 consists of 36,299 images divided into 11 age
groups with a span of ﬁve years.

10015

Figure 2: The structure of the face aging framework in video. Best viewed in color and 2× zoom in.

3.2. Video dataset

Along with still photographs, we also collected a video
dataset for temporal aging evaluations with 100 videos of
celebrities. Each video clip consists of 200 frames. In par-
ticular, searching based on the individuals’ names during
collection efforts, their interview, presentation, or movie
sessions were selected such that only one face, in a clear
manner, is presented in the frame. Age annotations were
estimated using the year of the interview session versus the
year of birth of the individual. Furthermore, in order to pro-
vide a reference for subject’s appearance in old age, the face
images of these individuals at the current age are also col-
lected and provided as meta-data for the subjects’ videos.

4. Video-based Facial Aging

In the simplest approach, age progression of a sequence
may be achieved by independently employing image-based
aging techniques on each frame of a video. However, treat-
ing single frames independently may result in inconsis-
tency of the ﬁnal aged-progressed likeness in the video, i.e.
some synthesized features such as wrinkles appear differ-
ently across consecutive video frames as illustrated in Fig.
1. Therefore, rather than considering a video as a set of
independent frames, this method exploits the temporal rela-
tionship between frames of the input video to maintain vi-
sually cohesive age information for each frame. The aging
algorithm is formulated as the sequential decision-making
process from a goal-oriented agent while interacting with
the temporal visual environment. At time sample, the agent
integrates related information of the current and previous
frames then modiﬁes action accordingly. The agent receives
a scalar reward at each time-step with the goal of maximiz-

ing the total long-term aggregate of rewards, emphasizing
effective utilization of temporal observations in computing
the aging transformation employed on the current frame.

Formally, given an input video, let I ∈ Rd be the image
o} be an image pair at time-step t
y ∈ I of the video at young
o ∈ I at old age. The goal is

domain and Xt = {xt
consisting of the t-th frame xt
age and the synthesized face xt
to learn a synthesis function G that maps xt

y, xt

y to xt

o as.

xt
o = G(xt

y)|X1:t−1

(1)

The conditional
term indicates the temporal constraint
needs to be considered during the synthesis process. To
learn G effectively, we decompose G into sub-functions as.

G = F1 ◦ M ◦ F2

(2)

y 7→ F1(xt

y) maps the young face im-
y to its representation in feature domain; M :
y); X1:t−1) 7→ F1(xt
o) deﬁnes the traversing func-
o is the map-

where F1 : xt
age xt
(F1(xt
tion in feature domain; and F2 : F1(xt
ping from feature domain back to image domain.

o) 7→ xt

Based on this decomposition, the architecture of our pro-
posed framework (see Fig. 2) consists of three main pro-
cessing steps: (1) Feature embedding; (2) Manifold traver-
sal; and (3) Synthesizing ﬁnal images from updated fea-
tures.
In the second step, a Deep RL based framework
is proposed to guarantee the consistency between video
frames in terms of aging changes during synthesis process.

4.1. Feature Embedding

The ﬁrst step of our framework is to learn an embed-
ding function F1 to map xt
y into its latent representation
F1(xt
y). Although there could be various choices for F1, to
produce high quality synthesized images in later steps, the

10016

chosen structure for F1 should produce a feature represen-
tation with two main properties: (1) linearly separable and
(2) detail preserving. On one hand, with the former prop-
erty, transforming the facial likeness from one age group to
another age group can be represented as the problem of lin-
early traversing along the direction of a single vector in fea-
ture domain. On the other hand, the latter property guaran-
tees a certain detail to be preserved and produce high qual-
ity results.
In our framework, CNN structure is used for
F1. It is worth noting that there remain some compromises
regarding the choice of deep layers used for the represen-
tation such that both properties are satisﬁed. Linear sep-
arability is preferred in deeper layers further along the lin-
earization process while details of a face are usually embed-
ded in more shallow layers [25]. As an effective choice in
several image-modiﬁcation tasks [12, 13], we adopt the nor-
malized VGG-192 and use the concatenation of three layers
{conv3 1, conv4 1, conv5 1} as the feature embedding.

4.2. Manifold Traversing

Given the embedding F1(xt

y), the age progression pro-
cess can be interpreted as the linear traversal from the
younger age region of F1(xt
y) toward the older age region
of F1(xt
o) within the deep-feature domain. Then the Mani-
fold Traversing function M can be written as in Eqn (3).

F1(xt

o) = M(F1(xt

y); X1:t−1)

= F1(xt

y) + α∆xt|X

1:t−1

(3)

where α denotes the user-deﬁned combination factor,
and ∆xt|X1:t−1
encodes the amount of aging information
needed to reach the older age region for the frame xt
y con-
ditional on the information of previous frames.

4.2.1 Learning from Neighbors

In order to compute ∆xt|X1:t−1
containing only aging ef-
fects without the presence of other factors, i.e.
identity,
pose, etc., we exploit the relationship in terms of the ag-
ing changes between the nearest neighbors of xt
y in the two
age groups. In particular, given xt
y, we construct two neigh-
bor sets N t
o that contain K nearest neighbors of
xt
y in the young and old age groups, respectively. Then
∆xt|X

is estimated by:

y and N t

1:t−1

1:t−1

= ∆xt|X
A(·,xt

y )

=

1:t−1

1
K

X




∆xt|X

F1(A(x, xt



where A(x, xt
y) denotes a face-alignment operator that po-
sitions the face in x with respect to the face location in xt
y.
Since only the nearest neighbors of xt
y are considered in the

y)) − X

F1(A(x, xt

x∈N t
y

x∈N t
o

y))

2This network is trained on ImageNet for better latent space.

two sets, conditions apart from age difference should be suf-
ﬁciently similar between the two sets and subtracted away
in ∆xt|X1:t−1
. Moreover, the averaging operator also helps
to ignore identity-related factors, and, therefore, emphasiz-
ing age-related changes as the main source of difference to
be encoded in ∆xt|X1:t−1
. The remaining question is how
to choose the appropriate neighbor sets such that the aging
changes provided by ∆xt|X1:t−1
are con-
sistent. In the next section, a Deep RL based framework is
proposed for selecting appropriate candidates for these sets.

and ∆xt−1|X1:t−2

4.2.2 Deep RL for Neighbor Selection

y

y, xt−1

A straightforward technique of choosing the neighbor sets
for xt
y in young and old age is to select faces that are close
to xt
y based on some closeness criteria such as distance in
feature domain, or number of matched attributes. However,
since these criteria are not frame-interdependent, they are
unable to maintain visually cohesive age information across
video frames. Therefore, we propose to exploit the relation-
ship presented in the image pair {xt
y } and the neigh-
bor sets of xt−1
as an additional guidance for the selection
process. Then an RL based framework is proposed and for-
mulated as a sequential decision-making process with the
goal of maximizing the temporal reward estimated by the
consistency between the neighbor sets of xt
Speciﬁcally, given two input frames {xt

y } and two
neighbor sets {N t−1
, the agent of a policy
network will iteratively analyze the role of each neighbor of
xt−1
in both young and old age in combination with the re-
lationship between F1(xt
) to determine new
suitable neighbors for {N t
y. A new neighbor
is considered appropriate when it is sufﬁciently similar to
xt
y and maintains aging consistency between two frames.
Each time a new neighbor is selected, the neighbor sets of
xt
y are updated and received a reward based on estimating
the similarity of embedded aging information between two
frames. As a result, the agent can iteratively explore an opti-
mal route for selecting neighbors to maximize the long-term
reward. Fig. 3 illustrates the process of selecting neighbors
for age-transformation relationship.

y) and F1(xt−1
o } of xt

y and xt−1
y, xt−1

} of xt−1

, N t−1

y, N t

y

y

y

y

y

o

.

st
i

i

i

y

y

y

at

i-th

step

state

, zt−1

of xt−1

State:
y, xt−1

=
The
, (N t)i, ¯N t, Mi(cid:3) is deﬁned as a com-
(1) the current frame xt
y;
; (3) the current considered

(cid:2)xt
position of six components:
(2) the previous frame xt−1
neighbor zt−1
, i.e. either in young and old age
groups; (4) the current construction of the two neighbor
sets (N t)i = {(N t
o )i} of xt
y until step i; (5) the
extended neighbor sets ¯N t = { ¯N t
y, ¯N t
o } consisting of N
neighbors, i.e. N > K, of xt
y for each age group. and
(6) a binary mask Mi indicating which samples in ¯N t
are already chosen in previous steps. Notice that in the
initial state st
o )0} are

0, the two neighbor sets {(N t

y)0, (N t

y)i, (N t

10017

Figure 3: The process of selecting neighbors for age-transformation relationship. Best viewed in color and 2× zoom in.

initialized using the K nearest neighbors of xt
y of the two
age groups, respectively. Two measurement criteria are
considered for ﬁnding the nearest neighbors:
the number
of matched facial attributes, e.g gender, expressions, etc.;
and the cosine distance between two feature embedding
vectors. All values of the mask Mi are set to 1 in st
0.

y

of xt−1

y, xt−1

, and the relationship of {xt

Action: Using the information from the chosen neighbor
zt−1
y }, an action
i
at
i is deﬁned as selecting the new neighbor for the current
frame such that with this new sample added to the neighbor
sets of the current frame, the aging-synthesis features be-
tween xt
are more consistent. Notice that since
not all samples in the database are sufﬁciently similar to xt
y,
we restrict the action space by selecting among N nearest
neighbors of xt
y. In our conﬁguration, N = n ∗ K where n
and K are set to 4 and 100, respectively.

y and xt−1

y

Policy Network: At each time step i, the policy network

ﬁrst encodes the information provided in state st

i as

i

y

1

1

1

1

y

y

y

F1

(4)

(xt

(zt−1

(xt−1

y, xt−1

y, xt−1

F1 (xt

) = F pool5

y) − F pool5

)i
y(cid:1) , Mi(cid:3)

(xt
y and xt−1

), F pool5
y(cid:1) , d(cid:0) ¯N t, xt

i = hδpool5
ut
vt
i = (cid:2)d(cid:0)(N t)i, xt
where F pool5
is the embedding function as presented in
Sec. 4.1, but the pool5 layer is used as the representa-
tion; δpool5
) em-
beds the relationship of xt
in the feature domain.
d (cid:0)(N t)i, xt
y(cid:1) is the operator that maps all samples in (N t)i
to their representation in the form of cosine distance to
xt
y. The last layer of the policy network is reformulated as
i /Pk ck
P (zt
i + b(cid:1)
and ht
i .θπ(cid:1) ; {W, b} are weight and bias of
the hidden-to-output connections. Since ht
i consists of the
features of the sample picked for neighbors of xt−1
and the
temporal relationship between xt−1
y, it directly en-
codes the information of how the face changes and what
aging information from the previous frame has been used.
This process helps the agent evaluate its choice to conﬁrm

i , where ci = Mi ⊙ (cid:0)Wht

i) = ecj
i, vt

i = Fπ (cid:0)ut

i = xj|st

and xt

y

y

i = xj|st

the optimal candidate of xt

y to construct the neighbor sets.
The output of the policy network is an N + 1-dimension
vector p indicating the probabilities of all available actions
P (zt
i), j = 1..N where each entry indicates the
probability of selecting sample xj for step i. It is noticed
that the N + 1-th value of p indicates an action that there
is no need to update the neighbor sets in this step. During
training, an action at
i is taken by stochastically sampling
from this probability distribution. During testing, the one
with highest probability is chosen for synthesizing process.
i in state st
i
i+1 can be obtained via the
i) where
i+1 in
. Then the neighbor that is least simi-
is replaced by xj
i. The terminate state is reached

has been made, the next state st
state-transition function st
zt−1
i
neighbor sets of xt−1
lar to xt
according to the action at
when all the samples of N t−1

is updated to the next unconsidered sample zt−1

State transition: After decision of action at

y in the corresponding sets of zt−1

i+1 = T ransition(st

are considered.

, N t−1

i, at

y

i

y

o

Reward: During training, the agent will receive a reward
signal rt
i from the environment after executing an action at
i
at step i. In our proposed framework, the reward is chosen
to measure aging consistency between video frames as.

rt
i =

k ∆

1
xt−1|X1:t−2
y ) − ∆
A(·,xt

y )

xt|X1:t−1
i,A(·,xt

k +ǫ

(5)

Notice that in this formulation, we align all neighbors of
both previous and current frames to xt
y. Since the same
alignment operator A(·, xt
y) on all neighbor sets of both
previous and current frames is used, the effect of alignment
factors, i.e. poses, expressions, location of the faces, etc.,
can be minimized in rt
i reﬂects only the dif-
ference in aging information embedded into xt

i . Therefore, rt

y and xt−1

y

.

Model Learning: The training objective is to maximize
the sum of the reward signals: R = Pi rt
i . We optimize the
recurrent policy network with the REINFORCE algorithm
[42] guided by the reward given at each time step.

10018

Figure 5: Age Progression Results. Given different frames
of a subject, our approach can consistently synthesized the
faces of that subject at different age groups.

database does not have age annotation. To obtain the age
label, we employ the age estimator in [32] for initial labels
which are manually corrected as needed after estimation.

300-VW [34]: includes 218595 frames from 114 videos.
Similar to the video set of AGFW-v2, the videos are movie
or presentation sessions containing one face per frame.

5.2. Implementation Details

Data Setting. In order to construct the neighbor sets for
an input frames in young and old ages, images from AGFW-
v2 and LFW-GOOGLE are combined and divided into 11
age groups from 10 to 65 with the age span of ﬁve years.

Model Structure and Training. For the policy network,
we employ a neural network with two hidden layers of 4096
and 2048 hidden units, respectively. Rectiﬁed Linear Unit
(ReLU) activation is adopted for each hidden layer. The
videos from 300-VW are used to train the policy network.

Computational time. Processing time of the synthe-
sized process depends on the resolution of the input video
frames. It roughly takes from 40 seconds per 240 × 240
frame or 4.5 minutes per video frame with the resolution of
900 × 700. We evaluate on a system using an Intel i7-6700
CPU@3.4GHz with an NVIDIA GeForce TITAN X GPU.

5.3. Age Progression

This section demonstrates the validity of the approach
for robustly and consistently synthesizing age-progressed
faces across consecutive frames of input videos.

Age Progression in frontal and off-angle faces. Figs.
4 and 5 illustrate our age-progression results across frames
from AGFW-v2 videos that contain both frontal and off-
angle faces. From these results, one can see that even
in case of frontal faces (i.e.
the major changes between
frames come from facial expressions and movements of the
mouth and lips), or off-angle faces (i.e. more challenging
due to the pose effects in the combination of other varia-

10019

Figure 4: Age Progression Results. For each subject, the
two rows shows the input frames at the young age, and the
age-progressed faces at 60-years old, respectively.

4.3. Synthesizing from Features

After the neighbor sets of xt

y are selected, the ∆xt|X1:t−1
can be computed as presented in Sec. 4.2.1 and the embed-
ding of xt
o) is estimated via Eqn.
(3). In the ﬁnal stage, F1(xt
o) can then be mapped back into
the image domain I via F2 which can be achieved by the
optimization shown in Eqn. (6) [25].

y in old age region F1(xt

xt∗

o = arg min

x

1
2

k F1(xt

o) − F1(x) k2

2 +λV β RV β (x)

(6)

where RV β represents the Total Variation regularizer en-
couraging smooth transitions between pixel values.

5. Experimental Results

5.1. Databases

The proposed approach is trained and evaluated using
training and testing databases that are not overlapped. Par-
ticularly, the neighbor sets are constructed using a large-
scale database composing face images from our collected
AGFW-v2 and LFW-GOOGLE [39]. Then Policy net-
work is trained using videos from 300-VW [34]. Finally,
the video set from AGFW-v2 is used for evaluation.

LFW-GOOGLE [39]: includes 44,697 high resolution
images collected using the names of 5,512 celebrities. This

Table 3: Comparison results in terms of consistency and
temporal smoothness (smaller value indicates better con-
sistency); and matching accuracy (higher value is better).

Method

Original Frames
FT [11]
TNVP [9]
IPCGANs [41]

Ours(Without RL)
Ours(With RL)

Aging

Temporal

Consistency

Smoothness

Matching
Accuracy

−

378.88
409.45
355.91

346.25
245.64

−

85.26
87.01
81.45

75.7
61.80

60.61%
67.5%
71.57%
73.17%

78.06%
83.67%

Figure 7: The distributions of the matching scores (of each
age group) between frames of original and age-progressed
videos against real faces of the subjects at the current age.

is employed to synthesize all video frames to the current
ages of the corresponding subjects in the videos. Then each
frame of the age-progressed videos is matched against the
real face images of the subjects at the current age. The
matching scores distributions between original (young) and
aged frames are presented in Fig. 7. Compared to the
original frames, our age-progressed faces produce higher
matching scores and, therefore, improve the matching per-
formance over original frames. Moreover, with the con-
sistency during aging process, the score deviation is main-
tained to be low. This also helps to improve the overall
performance further. The matching accuracy among differ-
ent approaches is also compared in Table 3 to emphasize the
advantages of our proposed model.

Figure 6: Comparisons between age progression ap-
proaches. For each subject, the top row shows frames in
the video at a younger age. The next three rows are our
results, TNVP [9] and Face Transformer [11], respectively.

tions), our proposed method is able to robustly synthesize
aging faces. Wrinkles of soft-tissue areas (i.e. under the
subject’s eyes; around the cheeks and mouth) are coherent
robust between consecutive synthesized frames. We also
compare our methods against Temporal Non-volume Pre-
serving (TNVP) approach [9] and Face Transformer (FT)
[11] in Fig. 6. These results further show the advantages of
our model when both TNVP and FT are unable to ensure the
consistencies between frames, and may result in different
age-progressed face for each input. Meanwhile, in our re-
sults, the temporal information is efﬁciently exploited. This
emphasizes the crucial role of the learned policy network.

Aging consistency. Table 3 compares the aging consis-
tency between different approaches. For the consistency
measurement, we adopt the average inverted reward r−1 of
all frames for each synthesis video. Furthermore, to vali-
date the temporal smoothness, we ﬁrstly compute the opti-
cal ﬂow, i.e. an estimation of image displacements, between
frames of each video to estimate changes in pixels through
time. Then we evaluate the differences (ℓ2-norm) between
the ﬂows of the original versus synthesis videos. From these
results, one can see that policy network has consistently and
robustly shown its role on maintaining an appropriate aging
amount embedded to each frame, and, therefore, producing
smoother synthesis across frames in the output videos.

5.4. Video Age Invariant Face Recognition

6. Conclusions

The effectiveness of our proposed approach is also vali-
dated in terms the performance gain for cross-age face ver-
iﬁcation. With the present of RL approach, not only is
the consistency guaranteed, but also are the improvements
made in both matching accuracy and matching score devia-
tion. We adapt one of the state-of-the-art deep face match-
ing models in [5] for this experiment. We set up the face
veriﬁcation as follows. For all videos with the subject’s age
labels in the video set of AGFW-v2, the proposed approach

This work has presented a novel Deep RL based ap-
proach for age progression in videos. The model inherits
the strengths of both recent advances of deep networks and
reinforcement learning techniques to synthesize aged faces
of given subjects both plausibly and coherently across video
frames. Our method can generate age-progressed facial
likenesses in videos with consistently aging features across
frames. Moreover, our method guarantees preservation of
the subject’s visual identity after synthesized aging effects.

10020

References

[1] Grigory Antipov, Moez Baccouche, and Jean-Luc Dugelay.
Face aging with conditional generative adversarial networks.
arXiv preprint arXiv:1702.01983, 2017.

[2] D Michael Burt and David I Perrett. Perception of age in
adult caucasian male faces: Computer graphic manipulation
of shape and colour information. Proceedings of the Royal
Society of London B: Biological Sciences, 259(1355):137–
143, 1995.

[3] Bor-Chun Chen, Chu-Song Chen, and Winston H. Hsu.
Cross-age reference coding for age-invariant face recogni-
tion and retrieval. In ECCV, 2014.

[4] C. Chen, W. Yang, Y. Wang, K. Ricanek, and K. Luu. Fa-
cial feature fusion and model selection for age estimation.
In Conf. on Automatic Face and Gesture Recognition (FG),
pages 1–7. IEEE, 2011.

[5] Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arcface:
Additive angular margin loss for deep face recognition. arXiv
preprint arXiv:1801.07698, 2018.

[6] Chi Nhan Duong, Khoa Luu, Kha Gia Quach, and Tien D
Bui. Beyond principal components: Deep boltzmann ma-
chines for face modeling. In CVPR, pages 4786–4794. IEEE,
2015.

[7] Chi Nhan Duong, Khoa Luu, Kha Gia Quach, and Tien D.
Bui. Longitudinal face modeling via temporal deep restricted
boltzmann machines. In CVPR, June 2016.

[8] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, Hoai Bac Le,
and Karl Ricanek Jr. Fine tuning age estimation with global
and local facial features. In Intl. Conf. on Acoustics, Speech
and Signal Processing (ICASSP), pages 1–7. IEEE, 2011.

[9] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, Ngan Le,
and Marios Savvides. Temporal non-volume preserving
approach to facial age-progression and age-invariant face
recognition. In The IEEE Int’l Conference on Computer Vi-
sion (ICCV), Oct 2017.

[10] FG-NET.

Fg-net

aging

database.

http://www.fgnet.rsunit.com.

[11] FT.

Face

transformer

(ft)

demo.

http://cherry.dcs.aber.ac.uk/transformer/.

In

In

[12] Leon Gatys, Alexander S Ecker, and Matthias Bethge. Tex-
ture synthesis using convolutional neural networks. In Ad-
vances in Neural Information Processing Systems, pages
262–270, 2015.

[13] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.
arXiv preprint

A neural algorithm of artistic style.
arXiv:1508.06576, 2015.

[14] Xin Geng, Zhi-Hua Zhou, and Kate Smith-Miles. Auto-
matic age estimation based on facial aging patterns. PAMI,
29(12):2234–2240, 2007.

[15] Ira Kemelmacher-Shlizerman, Supasorn Suwajanakorn, and
In

Illumination-aware age progression.

Steven M Seitz.
CVPR, pages 3334–3341. IEEE, 2014.

[17] H. N. Le, K. Seshadri, K. Luu, and M. Savvides. Facial aging
and asymmetry decomposition based approaches to identica-
tion of twins. Journal of Pattern Recognition, 48:3843–3856,
2015.

[18] Gil Levi and Tal Hassner. Age and gender classiﬁcation us-

ing convolutional neural networks. In CVPRW, 2015.

[19] K. Luu. Computer approaches for face aging problems.
In The 23th Canadian Conference on Artiﬁcial Intelligence
(CAI). Ottawa, Canada, 2010.

[20] K. Luu, T. D. Bui, K. Ricanek Jr., and C. Y. Suen. Age es-
timation using active appearance models and support vector
machine regression. In Intl. Conference on Biometrics: The-
ory, Applications and Systems (BTAS). IEEE, 2009.

[21] K. Luu, T. D. Bui, and C. Y. Suen. Kernel spectral regression
of perceived age from hybrid facial features.
In Conf. on
Automatic Face and Gesture Recognition (FG), pages 1–7.
IEEE, 2011.

[22] K. Luu, K. Ricanek Jr., T. D. Bui, and C. Y. Suen. The fa-
milial face database: A longitudinal study of family-based
growth and development on face recognition.
In Robust
Biometrics: Understanding Science Technology (ROBUST).
IEEE, 2008.

[23] K. Luu, K. Seshadri, M. Savvides, T. D. Bui, and C. Y.
Suen. Contourlet appearance model for facial age estima-
tion. In Intl. Joint Conf. on Biometrics (IJCB), pages 1–7.
IEEE, 2011.

[24] Khoa Luu, C.Y. Suen, T.D. Bui, and Jr. K. Ricanek. Auto-
matic child-face age-progression based on heritability factors
of familial faces. In BIdS, pages 1–6. IEEE, 2009.

[25] Aravindh Mahendran and Andrea Vedaldi. Understanding
In CVPR,

deep image representations by inverting them.
pages 5188–5196, 2015.

[26] M. Minear and D. C. Park. A life span database of adult facial
stimuli. Behavior Research Methods, Instruments, Comput-
ers, pages 630–633, 2004.

[27] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kot-
sia, and S. Zafeiriou. Agedb: the ﬁrst manually collected, in-
the-wild age database. In Proceedings of IEEE Intl Conf. on
Computer Vision and Pattern Recognition (CVPR-W 2017),
Honolulu, Hawaii, June 2017.

[28] Eric Patterson, K Ricanek, M Albert, and E Boone. Auto-
matic representation of adult aging in facial images. In Proc.
IASTED Intl Conf. Visualization, Imaging, and Image Pro-
cessing, pages 171–176, 2006.

[29] Eric Patterson, Amrutha Sethuram, Midori Albert, and Karl
Ricanek. Comparison of synthetic face aging to age progres-
sion by forensic sketch artist. In IASTED Int’l Conference
on Visualization, Imaging, and Image Processing, Palma de
Mallorca, Spain, 2007.

[30] Eric Patterson, Amrutha Sethuram, and Karl Ricanek. An
improved rendering technique for active-appearance-model-
based automated age progression. In Proceedings of ACM
SIGGRAPH 2013: SIGGRAPH Posters, 2013.

[16] Andreas Lanitis, Chris J Taylor, and Timothy F Cootes. To-
ward automatic simulation of aging effects on face images.
PAMI, 24(4):442–455, 2002.

[31] Karl Ricanek Jr and Tamirat Tesafaye. Morph: A longitudi-
nal image database of normal adult age-progression. In FGR
2006., pages 341–345. IEEE, 2006.

10021

[32] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep
expectation of real and apparent age from a single image
without facial landmarks. Int’l Journal of Computer Vision
(IJCV), July 2016.

[33] Duncan Rowland, David Perrett, et al. Manipulating facial
appearance through shape and color. Computer Graphics
and Applications, IEEE, 15(5):70–76, 1995.

[34] Jie Shen, Stefanos Zafeiriou, Grigoris G Chrysos, Jean Kos-
saiﬁ, Georgios Tzimiropoulos, and Maja Pantic. The ﬁrst
facial landmark tracking in-the-wild challenge: Benchmark
and results. In ICCVW, pages 50–58, 2015.

[35] Xiangbo Shu, Jinhui Tang, Hanjiang Lai, Luoqi Liu, and
Shuicheng Yan. Personalized age progression with aging
dictionary. In ICCV, December 2015.

[36] Jinli Suo, Xilin Chen, Shiguang Shan, Wen Gao, and Qiong-
hai Dai. A concatenational graph evolution aging model.
PAMI, 34(11):2083–2096, 2012.

[37] Jinli Suo, Song-Chun Zhu, Shiguang Shan, and Xilin Chen.
A compositional and dynamic model for face aging. PAMI,
32(3):385–401, 2010.

[38] Ming-Han Tsai, Yen-Kai Liao, and I-Chen Lin. Human face
aging with guided prediction and detail synthesis. Multime-
dia tools and applications, 72(1):801–824, 2014.

[39] Paul Upchurch, Jacob Gardner, Kavita Bala, Robert Pless,
Noah Snavely, and Kilian Weinberger. Deep feature in-
terpolation for image content changes.
arXiv preprint
arXiv:1611.05507, 2016.

[40] Wei Wang, Zhen Cui, Yan Yan, Jiashi Feng, Shuicheng Yan,
In

Xiangbo Shu, and Nicu Sebe. Recurrent face aging.
CVPR, pages 2378–2386, 2016.

[41] Z. Wang, W. Luo X. Tang, and S. Gao. Face aging with
identity-preserved conditional generative adversarial net-
works. In CVPR, 2018.

[42] Ronald J. Williams. Simple statistical gradient-following al-
In Ma-

gorithms for connectionist reinforcement learning.
chine Learning, pages 229–256, 1992.

[43] F. Xu, K. Luu, and M. Savvides. Spartans: Single-sample
periocular-based alignment-robust recognition technique ap-
plied to non-frontal scenarios. Trans. on Image Processing
(TIP), 24:4780–4795, 2015.

[44] J. Xu, K. Luu, M. Savvides, T. D. Bui, and C. Y. Suen. In-
vestigating age invariant face recognition based on periocular
biometrics. In Intl. Joint Conf. on Biometrics (IJCB). IEEE,
2011.

[45] Hongyu Yang, Di Huang, Yunhong Wang, Heng Wang, and
Yuanyan Tang. Face aging effect simulation using hidden
factor analysis joint sparse representation. TIP, 25(6):2493–
2507, 2016.

[46] Zhifei Zhang, Yang Song, and Hairong Qi. Age progres-
In

sion/regression by conditional adversarial autoencoder.
CVPR, July 2017.

10022

