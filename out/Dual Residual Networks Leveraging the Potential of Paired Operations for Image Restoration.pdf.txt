Dual Residual Networks Leveraging the Potential of Paired Operations

for Image Restoration

Xing Liu† Masanori Suganuma†‡

Zhun Sun‡

Takayuki Okatani†‡

†Graduate School of Information Sciences, Tohoku University

‡RIKEN Center for AIP

{ryu,suganuma,zhun,okatani}@vision.is.tohoku.ac.jp

Abstract

In this paper, we study design of deep neural networks
for tasks of image restoration. We propose a novel style of
residual connections dubbed “dual residual connection”,
which exploits the potential of paired operations, e.g., up-
and down-sampling or convolution with large- and small-
size kernels. We design a modular block implementing this
connection style; it is equipped with two containers to which
arbitrary paired operations are inserted. Adopting the “un-
raveled” view of the residual networks proposed by Veit et
al., we point out that a stack of the proposed modular blocks
allows the ﬁrst operation in a block interact with the second
operation in any subsequent blocks. Specifying the two op-
erations in each of the stacked blocks, we build a complete
network for each individual task of image restoration. We
experimentally evaluate the proposed approach on ﬁve im-
age restoration tasks using nine datasets. The results show
that the proposed networks with properly chosen paired op-
erations outperform previous methods on almost all of the
tasks and datasets.

1. Introduction

The task of restoring the original image from its de-
graded version, or image restoration, has been studied for
a long time in the ﬁelds of image processing and com-
puter vision. As in many other tasks of computer vision,
the employment of deep convolutional networks have made
signiﬁcant progress.
In this study, aiming at further im-
provements, we pursue better architectural design of net-
works, particularly the design that can be shared across dif-
ferent tasks of image restoration.
In this study, we pay
attention to the effectiveness of paired operations on var-
ious image processing tasks.
In [11], it is shown that
a CNN iteratively performing a pair of up-sampling and
down-sampling contributes to performance improvement
for image-superresolution. In [37], the authors employ evo-
lutionary computation to search for a better design of con-

(a)

(b)

(c)

(d)

(cid:1)(cid:1)

(cid:1)(cid:2)

(cid:1)(cid:3)

(cid:1)(cid:1)

(cid:1)(cid:1)

(cid:1)(cid:1)

(cid:2)(cid:1)

(cid:2)(cid:1)

(cid:2)(cid:1)

(cid:1)(cid:2)

(cid:1)(cid:2)

(cid:1)(cid:2)

(cid:2)(cid:2)

(cid:2)(cid:2)

(cid:2)(cid:2)

(cid:1)(cid:3)

(cid:1)(cid:3)

(cid:1)(cid:3)

(cid:2)(cid:3)

(cid:2)(cid:3)

(cid:2)(cid:3)

Figure 1: Different construction of residual networks with a
single or double basic modules. The proposed “dual resid-
ual connection” is (d).

volutional autoencoders for several tasks of image restora-
tion, showing that network structures repeatedly perform-
ing a pair of convolutions with a large- and small-size ker-
nels (e.g., a sequence of conv.
layers with kernel size 3,
1, 3, 1, 5, 3, and 1) perform well for image denoising. In
this paper, we will show further examples for other image
restoration tasks. Assuming the effectiveness of such repet-
itive paired operations, we wish to implement them in deep
networks to exploit their potential. We are speciﬁcally in-
terested in how to integrate them with the structure of resid-
ual networks. The basic structure of residual networks is
shown in Fig. 1(a), which have become an indispensable
component for the design of modern deep neural networks.
There have been several explanations for the effectiveness
of the residual networks. A widely accepted one is the
“unraveled” view proposed by Veit et al. [41]: a sequen-
tial connection of n residual blocks is regarded as an en-
semble of many sub-networks corresponding to its implicit
2n paths. A network of three residual blocks with mod-
ules f1, f2, and f3, shown in Fig. 1(a), has (23 =)8 im-
plicit paths from the input to output, i.e., f1 → f2 → f3,
f1 → f2, f1 → f3, f2 → f3, f1, f2, f3, and 1. Veit et
al. also showed that each block works as a computational
unit that can be attached/detached to/from the main net-
work with minimum performance loss. Considering such
a property of residual networks, how should we use resid-

7007

residual connection-1

(cid:1)(cid:4)(cid:6)(cid:5)(cid:3)

(cid:2)

(cid:2)

(cid:1)(cid:3)(cid:6)

(cid:1)(cid:4)(cid:6)

Rain streak removal

residual connection-2

input

result

Motion blur removal

Gaussian noise removal

input

result

input

result

Haze removal

Rain drop removal

input

result

input

result

Figure 2: Upper-left: the structure of a unit block having the
proposed dual residual connections; T l
2 are the con-
tainers for two paired operations; c denotes a convolutional
layer. Other panels: ﬁve image restoration tasks considered
in this paper.

1 and T l

ual connections for paired operations? Denoting the paired
operations by f and g, the most basic construction will be
to treat (fi, gi) as a unit module, as shown in Fig. 1(b). In
this connection style, fi and gi are always paired for any
i in the possible paths. In this paper, we consider another
connection style shown in Fig. 1(d), dubbed “dual residual
connection”. This style enables to pair fi and gj for any
i and j such that i ≤ j.
In the example of Fig.1(d), all
the combinations of the two operations, (f1, g1), (f2, g2),
(f3, g3), (f1, g2), (f1, g3), and (f2, g3), emerge in the pos-
sible paths. We conjecture that this increased number of po-
tential interactions between {fi} and {gj} will contribute to
improve performance for image restoration tasks. Note that
it is guaranteed that f· and g· are always paired in the possi-
ble paths. This is not the case with other connection styles
such as the one depicted in Fig. 1(c). We call the building
block for implementing the proposed dual residual connec-
tions Dual Residual Block (DuRB); see Fig. 2. We examine
its effectiveness on ﬁve image restoration tasks shown in
Fig. 2 using nine datasets. DuRB is a generic structure that
has two containers for the paired operations, and the users
choose two operations for them. For each task, we specify
the paired operations of DuRBs as well as the entire net-
work. Our experimental results show that our networks out-
perform the state-of-the-art methods in these tasks, which
supports the effectiveness of our approach.

2. Related Work

Gaussian noise removal Application of neural networks
to noise removal has a long history [1,18,43,53,54]. Mao et
al. [26] proposed REDNet, which consists of multiple con-

volutional and de-convolutional layers with symmetric skip
connections over them. Tai et al. [39] proposed MemNet
with local memory blocks and global dense connections,
showing that it performs better than REDNet. However,
Suganuma et al. [37] showed that standard convolutional
autoencoders with repetitive pairs of convolutional layers
with large- and small-size kernels outperform them by a
good margin, which are found by architectural search based
on evolutionary computation.

Motion blur removal
This task has a long history of
research. Early works [2, 7, 45, 46] attempt to simultane-
ously estimate both blur kernels and sharp images. Re-
cently, CNN-based methods [9, 20, 28, 38, 42] achieve good
performance for this task. Nah et al. [28] proposed a coarse-
to-ﬁne approach along with a modiﬁed residual block [14].
Kupyn et al. [20] proposed an approach based on Genera-
tive Adversarial Network (GAN) [10]. New datasets were
created in [28] and [20].

Haze removal Many studies assume the following model
of haze: I(x) = J(x)t(x) + A(x)(1 − t(x)), where I de-
notes a hazy scene image, J is the true scene radiance (the
clear image), t is a transmission map, A is global atmo-
spheric light. The task is then to estimate A, t, and thus
J(x) from the input I(x) [4,12,27,48,51]. Recently, Zhang
et al. [51] proposed a method that uses CNNs to jointly es-
timate t and A, which outperforms previous approaches by
a large margin. Ren et al. [32] and Li et al. [24] proposed
method to directly estimate J(x) without explicitly estimat-
ing t and A. Yang et al. [48] proposed a method that inte-
grates CNNs to classical prior-based method.

Raindrop detection and removal Various approaches
[19, 21, 34, 47, 50] have been proposed to tackle this prob-
lem in the literature. Kurihata et al. [21] proposed to de-
tect raindrops with raindrop-templates learned using PCA.
Ramensh [19] proposed a method based on K-Means clus-
tering and median ﬁltering to estimate clear images. Re-
cently, Qian et al. [30] proposed a hybrid network consist-
ing of a convolutional-LSTM for localizing raindrops and
a CNN for generating clear images, which is trained in a
GAN framework.

Rain-streak removal
Fu et al. [8] use “guided image
ﬁltering” [13] to extract high-frequency components of an
image, and use it to train a CNN for rain-streak removal.
Zhang et al. [52] proposed to jointly estimate rain density
and de-raining result to alleviate the non-uniform rain den-
sity problem. Li et al. [25] regards a heavy rainy image as
a clear image added by an accumulation of multiple rain-
streak layers and proposed a RNN-based method to restore
the clear image. Li et al. [23] proposed an non-locally en-
hanced version of DenseBlock [16] for this task, their net-
work outperforms previous approaches by a good margin.

7008

Table 1: Performance of the three connection types of Fig. 1(b)-(c). ‘-’s indicate infeasible applications.

Gaussian noise
24.92 / 0.6632
24.85 / 0.6568
25.05 / 0.6755

(b)
(c)
(d)

Real noise

Motion blur

Haze

Raindrop

Rain-streak

36.76 / 0.9620
36.81 / 0.9627
36.84 / 0.9635

29.46 / 0.9035

31.20 / 0.9803

-/-

-/-

29.90 / 0.9100

32.60 / 0.9827

24.70 / 0.8104
25.12 / 0.8151
25.32 / 0.8173

32.85 / 0.9214
33.13 / 0.9222
33.21 / 0.9251

(cid:2)

(cid:2)

(cid:2)(cid:6)(cid:8)(cid:10)
(cid:1)(cid:8)(cid:10)

DuRB-P

(cid:2)(cid:6)(cid:9)(cid:10)
(cid:1)(cid:9)(cid:10)

(cid:2) (cid:2)

(cid:2)(cid:6)(cid:9)(cid:10)
(cid:2)(cid:6)(cid:8)(cid:10)
(cid:5)(cid:3)
DuRB-S(cid:1)(cid:8)(cid:10)
(cid:1)(cid:9)(cid:10)

(cid:2) (cid:2)

(cid:7)(cid:4)(cid:2)(cid:6)(cid:8)(cid:10)
DuRB-U(cid:1)(cid:8)(cid:10)

(cid:2)(cid:6)(cid:9)(cid:10)
(cid:1)(cid:9)(cid:10)

(cid:2) (cid:2)

(cid:7)(cid:4)(cid:2)(cid:6)(cid:8)(cid:10)
(cid:1)(cid:8)(cid:10)

DuRB-US

(cid:2)(cid:6)(cid:9)(cid:10)
(cid:5)(cid:3)
(cid:1)(cid:9)(cid:10)

Figure 3: Four different implementations of the DuRB; c is
a convolutional layer with 3×3 kernels; ctl
2 are con-
volutional layers, each with kernels of a speciﬁed size and
dilation rate; up is up-sampling (we implemented it using
PixelShufﬂe [36]); se is SE-ResNet Module [15] that is in
fact a channel-wise attention mechanism.

1 and ctl

3. Dual Residual Blocks

1 and T l

The basic structure of the proposed Dual Residual Block
(DuRB) is shown in the upper-left corner of Fig. 2, in which
we use c to denote a convolutional layer (with 3 × 3 ker-
nels) and T l
2 to denote the containers for the paired
ﬁrst and second operations, respectively, in the lth DuRB
in a network. Normalization layers (such as batch normal-
ization [17] or instance normalization [40]) and ReLU [29]
layers can be incorporated when it is necessary. We de-
sign DuRBs for each individual task, or equivalently choose
the two operations to be inserted into the containers T l
1 and
T l
2. We will use four different designs of DuRBs, DuRB-
P, DuRB-U, DuRB-S, and DuRB-US, which are shown in
Fig. 3. The speciﬁed operations for [T l
2] are [conv.,
conv.]
for DuRB-P, [up-sampling+conv., down-sampling
(by conv. with stride=2)] for DuRB-U, [conv., channel-wise
attention1+conv.]
for DuRB-S, and [up-sampling+conv.,
channel-wise attention+down-sampling] for DuRB-US, re-
spectively. We will use DuRB-P for noise removal and rain-
drop removal, DuRB-U for motion blur removal, DuRB-
S for rain-streak and raindrop removal, and DuRB-US for
haze removal.

1, T l

Before proceeding to further discussions, we present
here experimental results that show the superiority of
the proposed dual residual connection to other connec-

1It is implemented using the SE-ResNet Module [15].

tion styles shown in Fig. 1(b) and (c).
In the experi-
ments, three networks build on the three base structures
(b), (c), and (d) of Fig. 1 were evaluated on the ﬁve tasks.
For Gaussian&real-world noise removal, motion blur re-
moval, haze removal, raindrop and rain-streak removal, we
use DuRB-P, DuRB-U, DuRB-US, DuRB-S&DuRB-P and
DuRB-S to construct the base structures. Number of blocks
and all the operations in the three structures as well as other
experimental conﬁgurations are ﬁxed in each comparison.
The datasets for the six comparisons are BSD-grayscale,
Real-World Noisy Image Dataset, GoPro Dataset, Dehaze
Dataset, RainDrop Dataset and DID-MDN Data. Table 1
shows their performance. Note that ‘-’ in the table indi-
cate that the connection cannot be applied to DuRB-U and
DuRB-US due to the difference in size between the output
of f and the input to g. It can be seen that the proposed
structure (d) performs the best for all the tests.

4. Five Image Restoration Tasks

In this section, we describe how the proposed DuRBs
can be applied to multiple image restoration tasks, noise
removal, motion blur removal, haze removal, raindrop re-
moval and rain-streak removal.

residual connection

(cid:6)(cid:5)+(cid:9)

(cid:6)(cid:5)+(cid:9)

DuRB-P(cid:1)(cid:10)(cid:2)

(cid:6)(cid:5)+(cid:9)

(cid:6) (cid:3)(cid:4)(cid:8)(cid:7)

Figure 4: DuRN-P: dual residual network with DuRB-P’s
[conv. w/ a large kernel and conv. w/ a small kernel] for
Gaussian noise removal. b+r is a batch normalization layer
followed by a ReLU layer; and T anh denotes hyperbolic
tangent function.

4.1. Noise Removal

Network Design We design the entire network as shown in
Fig. 4. It consists of an input block, the stack of six DuRBs,
and an output block, additionally with an outermost residual
connection from the input to output. The layers c, b + r and
T anh in the input and output blocks are convolutional layer
(with 3×3 kernels, stride = 1), batch normalization layer

7009

noise level = 50

DuRN-P

Ground truth

Figure 5: Some examples of the results by the proposed
DuRN-P for additive Gaussian noise removal. Sharp images
can be restored from heavy noises (σ = 50).

Noisy

DuRN-P

Mean

Figure 6: Examples of noise removal by the proposed
DuRN-P for images from Real-World Noisy Image Dataset.
The results are sometimes even better than the mean image
(used as the ground truth); see the artifact around the letters
in the bottom.

followed by a ReLU layer, and hyperbolic tangent function
layer, respectively.

We employ DuRB-P (i.e., the design in which each of the
two operations is single convolution; see Fig. 3) for DuRBs
in the network. Inspired by the networks discovered by neu-
ral architectural search for noise removal in [37], we choose
for T1 and T2 convolution with large- and small-size recep-
tive ﬁelds. We also choose the kernel size and dilation rate
for each DuRB so that the receptive ﬁeld of convolution in
each DuRB grows its size with l. More details are given in
the supplementary material. We set the number of channels
to 32 for all the layers. We call the entire network DuRN-P.
For this task, we employed l2 loss for training the DuRN-P.

Table 2: Results for additive Gaussian noise removal on
BSD200-grayscale and noise levels (30, 50, 70). The num-
bers are PSNR/SSIM.

30

50

70

REDNet [26]
MemNet [39]
E-CAE [37]

27.95 / 0.8019
28.04 / 0.8053
28.23 / 0.8047

25.75 / 0.7167
25.86 / 0.7202
26.17 / 0.7255

24.37 / 0.6551
24.53 / 0.6608
24.83 / 0.6636

DuRN-P (ours)

28.50 / 0.8156

26.36 / 0.7350

25.05 / 0.6755

Table 3: Results on the Real-World Noisy Image Dataset
[44]. The results were measured by PSNR/SSIM. The last
row shows the number of parameters for each CNN.

REDNet [26] MemNet [39]

E-CAE [37]

PSNR/SSIM 35.56 / 0.9475
# of param.

4.1 × 106

- / -

35.45 / 0.9492

2.9 × 106

1.1 × 106

DuRN (ours)
36.83 / 0.9635

8.2 × 105

More details of the experiments are provided in the supple-
mentary material. We show the quantitative results in Table
2 and qualitative results in Fig. 5. It is observed from Ta-
ble 2 that the proposed network outperforms the previous
methods for all three noise levels.

Results: Real-World Noise Removal We also tested
the DuRN-P on the Real-World Noisy Image Dataset [44],
which consists of 40 pairs of an instance image (a photo-
graph taken by a CMOS camera) and the mean image (mean
of multiple shots of the same scene taken by the CMOS
camera). We removed all the batch normalization layers
from the DuRN-P for this experiment, as the real-world
noise captured in this dataset do not vary greatly. The details
of the experiments are given in the supplementary material.
The quantitative results of three previous methods and our
method are shown in Table 3. We used the authors’ code to
evaluate the three previous methods. (As the MemNet failed
to produce a competitive result, we left the cell empty for it
in the table.) It is seen that our method achieves the best
result despite the smaller number of parameters. Examples
of output images are shown in Fig. 6. We can observe that
the proposed DuRN-P has cleaned noises well. It is note-
worthy that the DuRN-P sometimes provides better images
than the “ground truth” mean image; see the bottom exam-
ple in Fig. 6.

4.2. Motion Blur Removal

The task is to restore a sharp image from its motion
blurred version without knowing the latent blur kernels (i.e.,
the “blind-deblurring” problem).

Results: Additive Gaussian Noise Removal We tested
the proposed network on the task of removing additive
Gaussian noise of three levels (30, 50, 70) from a gray-
scale noisy image. Following the same experimental pro-
tocols used by previous studies, we trained and tested the
proposed DuRN-P using the training and test subsets (300
and 200 grayscale images) of the BSD-grayscale dataset.

Network Design Previous works such as [42] reported that
the employment of up- and down-sampling operations is ef-
fective for this task. Following this ﬁnding, we employ up-
sampling and down-sampling for the paired operation. We
call this as DuRB-U; see Fig. 3. We use PixelShufﬂe [36]
for implementing up-sampling. For the entire network de-
sign, following many previous works [20, 42, 51, 55], we

7010

Blurry

DeBlurGAN

DuRN-U

Sharp

Figure 7: Examples of motion blur removal on GoPro-test dataset.

Blurry

residual connection

Blurry

DuRN-U

Sharp

DeBlurGAN

(cid:3)(cid:5)+(cid:7)

(cid:3)(cid:5)+(cid:7)

(cid:3)(cid:5)+(cid:7)

DuRB-U(cid:9)6

(cid:8)(cid:6)

(cid:3)(cid:5)+(cid:7)

(cid:8)(cid:6)

(cid:3)(cid:5)+(cid:7)

(cid:3)

(cid:1)(cid:2)(cid:5)(cid:4)

Figure 9: DuRN-U: Dual Residual Network with DuRB-
U’s (up- and down-sampling) for motion blur removal.
n + r denotes an instance normalization layer followed by
a ReLU layer.

DuRN-U

Table 4: Results of motion blur removal for the GoPro-test
dataset.

Sharp

GoPro-test

Sun et al. [38]
Nah et al. [28]
Xu et al. [46]

DeBlurGAN [20]

24.6 / 0.84
28.3 / 0.92
25.1 / 0.89
27.2 / 0.95

DuRN-U (ours)

29.9 / 0.91

Figure 8: Examples of object detection from original
blurred images and their deblurred versions.

Table 5: Accuracy of object detection from deblurred
images obtained by DeBlurGAN [20] and the proposed
DuRN-U on Car Dataset.

mAP (%)

Blurred DeBlurGAN [20] DuRN-U (ours)
16.54

26.17

31.15

choose a symmetric encoder-decoder network; see Fig. 9.
The network consists of the initial block, which down-
scales the input image by 4:1 down-sampling with two con-
volution operations (c) with stride = 2, and instance nor-
malization + ReLU (n + r), and six repetitions of DuRB-
U’s, and the ﬁnal block which up-scales the output of the
last DuRB-U by applications of 1:2 up-sampling (up) to the
original size. We call this network DuRN-U. For this task,
we employed a weighted sum of SSIM and l1 loss for train-
ing the DuRN-U. The details are given in the supp. material.

Results: GoPro Dataset We tested the proposed DuRN-
U on the GoPro-test dataset [28] and compared its results
with the state-of-the-art DeblurGAN2 [20]. The GoPro
dataset consists of 2,013 and 1,111 non-overlapped train-
ing (GoPro-train) and test (GoPro-test) pairs of blurred and
sharp images. We show quantitative results in the Table 4.
DeblurGAN yields outstanding SSIM number, whereas the

2The DeblurGAN refers the “DeblurGAN-wild” introduced in the orig-

inal paper [20].

7011

Hazy image

DCPDN

DuRN-US

Ground truth

Hazy image

DCPDN

DuRN-US

Ground truth

Hazy image

GFN

DCPDN

DuRN-US

Hazy image

GFN

DCPDN

DuRN-US

(A)

(B)

(C)

Figure 10: Examples of de-hazing results obtained by DuRN-US and others on (A) synthesized images, (B) real images and
(C) light hazy images.

proposed DuRN-U is the best in terms of PSNR. Examples
of deblurred images are shown in Fig. 7. It is observed that
the details such as cracks on a stone-fence or numbers writ-
ten on the car plate are restored well enough to be recog-
nized.

Results: Object Detection from Deblurred Images
In
[20], the authors evaluated their deblurring method (De-
BlurGAN) by applying an object detector to the deblurred
images obtained by their method. Following the same pro-
cedure and data (Car Dataset), we evaluate our DuRN-U
that is trained on the GoPro-train dataset. The Car Dataset
contains 1,151 pairs of blurred and sharp images of cars. We
employ YOLO v3 [31] trained using the Pascal VOC [6] for
the object detector. The detection results obtained for the
sharp image by the same YOLO v3 detector are utilized as
the ground truths used for evaluation. Table 5 shows quan-
titative results (measured by mAP), from which it is seen
that the proposed DuRN-U outperforms the state-of-the-art
DeBlurGAN. Figure 8 shows examples of detection results
on the GoPro-test dataset and Car Dataset. It is observed
that DuRN-U can recover details to a certain extent that im-
proves accuracy of detection.

4.3. Haze Removal

Network Design In contrast with previous studies where
a CNN is used to explicitly estimate a transmission map
that models the effects of haze, we pursue a different strat-
egy, which is to implicitly estimate a transmission map us-
ing an attention mechanism. Our model estimates the de-
hazed image from an input image in an end-to-end fashion.
We design DuRB’s for this task by employing up-sampling
(up) implemented using PixelShufﬂe [36] with a convolu-
tional layer (ctl
1 and channel-wise attention (se) im-
plemented using SE-ResNet module [15] with a conv. layer
(ctl
2. More details are given in the supplementary

2) in T l

1) in T l

residual connection

(cid:5)(cid:9)

(cid:5)(cid:9)

(cid:5)(cid:9)

…

DuRB-US(cid:11)(cid:1)(cid:2)

(cid:10)(cid:8)

(cid:5)(cid:9)

(cid:5)(cid:10)(cid:8)

(cid:9)(cid:5)

(cid:3)(cid:4)(cid:7)(cid:6)

Figure 11: DuRN-US: dual residual network with DuRB-
US’s (up- and down-sampling and channel-wise attention
(SE-ResNet Module)) for haze removal.

Table 6: Results for haze removal on Dehaze-TestA dataset
and RESIDE-SOTS dataset.

Dehaze-TestA

RESIDE-SOTS

He et al. [12]
Zhu et al. [56]

Berman et al. [3]

Li et al. [22]

Zhang et al. [51]

0.8642
0.8567
0.7959
0.8842
0.9560

Berman et al. [3]

Ren et al. [32]
Cai et al. [5]
Li et al. [22]
Ren et al. [33]

17.27 / 0.75
17.57 / 0.81
21.14 / 0.85
19.06 / 0.85
22.30 / 0.88

DuRN-US (ours)

0.9827

DuRN-US (ours)

32.12 / 0.98

material. The entire network (named DuRN-US) has an
encoder-decoder structure similar to the DuRN-U designed
for motion blur removal, as shown in Fig. 11. We stack 12
DuRB-US’s in the middle of the network; the number of
channels is 64 for all the layers. In the supplementary mate-
rial, we demonstrate how our network estimates a transmis-
sion map inside its attention mechanisms. For this task, we
employed a weighted sum of SSIM and l1 loss for training
the DuRN-US.

Results
In order to evaluate the proposed DuRN-US, we
trained and tested it on two datasets, the Dehaze Dataset
and the RESIDE dataset. The training and test (Dehaze-
TestA) subsets in the Dehaze Dataset consist of 4,000 and
400 non-overlapped samples of indoor scenes, respectively.
RESIDE contains a training subset of 13,990 samples of in-
door scenes and a few test subsets. Following [33], we used

7012

Rainy image

Attention map

Residual  map

-

DuRN-S-P

Ground truth

Qian (cid:4)(cid:6)(cid:1)(cid:3)(cid:5)(cid:2)

Figure 12: Examples of raindrop removal along with internal activation maps of DuRN-S-P. The “Attention map” and “Resid-
ual map” are the outputs of the Attentive-Net and the last T anh layer shown in Fig. 13; they are normalized for better
visibility.

a subset SOTS (Synthetic Objective Testing Set) that con-
tains 500 indoor scene samples for evaluation.
It should
be noted that the state-of-the-art method on the Dehaze
Dataset, DCPDN [51], is trained using i) hazy images, ii)
ground truth images, iii) ground truth global atmosphere
light , iv) ground truth transmission maps; additionally,
its weights are initialized by those of DenseNet [16] pre-
trained on the ImageNet [35]. The proposed DuRN-US
is trained only using i) and ii). Table 6 show results on
Dehaze-TestA and RESIDE-SOTS datasets, respectively.

Figure 10 shows examples of the results obtained by the
proposed network and others for the same input images. In
sub-ﬁgure (A), we show results for two synthesized images
produced by the DCPDN (the second best approach in terms
of SSIM and PSNR) and our DuRN-US. It is observed that
DuRN-US yields better results for these two images. In sub-
ﬁgure (B), we show results for two real-world hazy images3
produced by two state-of-the-art methods, GFN [33] and
DCPDN [51], and by ours. It can be observed that our net-
work yields the most realistic dehazed images. It is note-
worthy that our DuRN-US can properly deal with strong
ambient light (sunshine coming behind the girl). See the
example in the left-bottom of Fig. 10.

4.4. Raindrop removal

Network Design The task can naturally be divided into
two stages, that of identifying the regions of raindrops and
that of recovering the pixels of the identiﬁed regions. The
second stage is similar to image inpainting and may not
be difﬁcult, as there are a lot of successful methods for
image inpainting. Then, the major issue is with the ﬁrst
stage. Following this two-stage approach, the state-of-the-
art method [30] uses an attentive-recurrent network to pro-
duce an attention map that conveys information about rain-
drops; then, the attention map along with the input image

3The images are available from https://github.com/rwenqi/GFN-

dehazing

residual connection

(cid:6)(cid:5)+(cid:10)

(cid:6)(cid:5)+(cid:10)

(cid:6)(cid:5)+(cid:10)

Attentive-Net

…

DuRB-S(cid:12)(cid:1)

DuRB-P(cid:12)(cid:2)

(cid:11)(cid:9)

(cid:6)(cid:5)+(cid:10)

(cid:11)(cid:9)

(cid:6)

(cid:3)(cid:4)(cid:8)(cid:7)

Figure 13: DuRN-S-P: Hybrid dual residual network with
DuRB-S’s and DuRB-P’s for raindrop removal.

Table 7: Quantitative result comparison on RainDrop
Dataset [30].

Qian et al. [30] DuRN-S-P (ours)

TestSetA 31.51 / 0.9213
24.92 / 0.8090
TestSetB

31.24 / 0.9259
25.32 / 0.8173

are fed to a convolutional encoder-decoder network to es-
timate the ground truth image. It also employs adversarial
training with a discriminator to make the generated images
realistic.

We show our DuRBs are powerful enough to perform
these two-stage computations in a standard feedforward net-
work, if we use properly designed DuRBs in proper po-
sitions in the entire network. To be speciﬁc, we choose
the encoder-decoder structure for the entire network, and in
its bottleneck part, we set three DuRB-S’s followed by six
DuRB-P’s. For ctl
1 in the three DuRB-S’s, we use convolu-
tion with a 3 × 3 kernel with decreasing dilation rates, 12,
8, and 6, in the forward direction, aiming to localize rain-
drops in a coarse-to-ﬁne manner in the three DuRB-S’s in a
row. For the six DuRB-P’s, we employ the same strategy as
in noise removal etc., which is to apply a series of convo-
lution with an increasing receptive ﬁeld size in the forward
direction. We call the entire network DuRN-S-P. For this
task, we employed a weighted sum of SSIM and l1 loss for
training the DuRN-S-P.

7013

Rainy

DDN

DID-MDN

RESCAN

DuRN-S

Ground truth

Figure 14: Examples of rain-streak removal obtained by four methods including ours (DuRN-S).

Results We trained and evaluated the DuRN-S-P on the
RainDrop Dataset.
It contains 861 training samples and
58/249 test samples called TestSetA/TestSetB. TestSetA is
a subset of TestSetB, and is considered to have better align-
ment4 than TestSetB. Table 7 shows the results. It is seen
that our method outperforms the state-of-the-art method for
three out of four combinations of two test sets and two eval-
uation metrics. It is noteworthy that our method does not
use a recurrent network or adversarial training. Figure 12
shows some examples of the results obtained by our method
and the method of [30].
It is seen that the results of our
method are visually comparable to the method of [30]. The
“Attention map” and “Residual map” of Fig. 12 are the over-
channel summation of the output of Attentive-Net and the
output of the last T anh layer, respectively; see Fig. 13.

4.5. Rain-streak Removal

Network Design It is shown in [23] that the mechanism
that selectively weighs feature maps using global informa-
tion works effectively for this task. Borrowing this idea,
we employ a channel-wise attention mechanism to perform
similar feature weighting. The overall design of the network
for this task is similar to the DuRN-P designed for Gaussian
noise removal. A difference is that we use DuRB-S instead
of DuRB-P to use the attention mechanism. The details are
given in the supplementary material. For this task, we em-
ployed a weighted sum of SSIM and l1 loss for training the
network.

Results We tested the proposed network (DuRN-S) on
two benchmark datasets, the DDN-Data, which consists of
9,100 training pairs and 4,900 test pairs of rainy and clear
images, and the DID-MDN Data, which consists of 12,000
training pairs and 1,200 test pairs. Table 8 shows the results.
Those for the previous methods except RESCAN [25] are
imported from [23]. It is seen that the proposed network
achieves the best performance. Examples of the output im-
ages are provided in Fig. 14.

4https://github.com/rui1996/DeRaindrop

Table 8: Results on two de-raining datasets.

DDN [8]

JORDER [49]
DID-MDN [52]
RESCAN [25]
NLEDN [23]

DDN Data

28.24 / 0.8654
28.72 / 0.8740
26.17 / 0.8409

-/-

29.79 / 0.8976

DID-MDN Data
23.53 / 0.7057
30.35 / 0.8763
28.30 / 0.8707
32.48 / 0.9096
33.16 / 0.9192

DuRN-S (ours)

31.30 / 0.9194

33.21 / 0.9251

5. Summary and Discussions

We have proposed a style of residual connection, dubbed
“dual residual connection”, aiming to exploit the potential
of paired operations for image restoration tasks. We have
shown the design of a modular block (DuRB) that imple-
ments this connection style, which has two containers for
the paired operations such that the user can insert any arbi-
trary operations to them. We have also shown choices of the
two operations in the block as well as the entire networks
(DuRN) containing a stack of the blocks for ﬁve different
image restoration tasks. The experimental results obtained
using nine datasets show that the proposed approach con-
sistently works better than previous methods.

Acknowledgement

This work was partly supported by JSPS KAKENHI
Grant Number JP15H05919, JST CREST Grant Num-
ber JPMJCR14D1, Council for Science, Technology and
Innovation (CSTI), Cross-ministerial Strategic Innovation
Promotion Program (Infrastructure Maintenance, Renova-
tion and Management ), and the ImPACT Program Tough
Robotics Challenge of the Council for Science, Technology,
and Innovation (Cabinet Ofﬁce, Government of Japan).

References

[1] Forest Agostinelli, Michael R Anderson, and Honglak Lee.
Adaptive multi-column deep neural networks with applica-
tion to robust image denoising. In Proc. International Con-
ference on Neural Information Processing Systems, 2013.

7014

[2] S. Derin Babacan, Rafael Molina, Minh N. Do, and Agge-
los K. Katsaggelos. Bayesian blind deconvolution with gen-
eral sparse image priors. In Proc. European Conference on
Computer Vision, 2012.

[3] Dana Berman, Tail Treibitz, and Shai Avidan. Non-local im-
age dehazing. In Proc. Conference on Computer Vision and
Pattern Recognition, 2016.

[4] Dana Berman, Tali Treibitz, and Shai Avidan. Air-light esti-
mation using haze-lines. In Proc. International Conference
on Computational Photography, 2017.

[5] Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, and
Dacheng Tao. Dehazenet: An end-to-end system for single
image haze removal. IEEE Transactions on Image Process-
ing, 2016.

[6] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.
Williams, J. Winn, and A. Zisserman. The pascal visual ob-
ject classes challenge: A retrospective. International Journal
of Computer Vision, 2015.

[7] Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T.
Roweis, and William T. Freeman. Removing camera shake
from a single photograph. In Proc. ACM SIGGRAPH, 2006.
[8] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao
Ding, and John Paisley. Removing rain from single images
via a deep detail network. In Proc. Conference on Computer
Vision and Pattern Recognition, 2017.

[9] Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian
D. Reid, Chunhua Shen, Anton van den Hengel, and Qin-
feng Shi. From motion blur to motion ﬂow: A deep learning
solution for removing heterogeneous motion blur. In Proc.
European Conference on Computer Vision, 2017.

[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Proc. Inter-
national Conference on Neural Information Processing Sys-
tems, 2014.

[11] Muhammad Haris, Greg Shakhnarovich, and Norimichi
Ukita. Deep back-projection networks for super-resolution.
In Proc. Conference on Computer Vision and Pattern Recog-
nition, 2018.

[12] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze
removal using dark channel prior. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 2011.

[13] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image ﬁl-
tering. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2013.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proc. Con-
ference on Computer Vision and Pattern Recognition, 2016.
[15] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In Proc. Conference on Computer Vision and Pattern
Recognition, 2018.

[16] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proc. Conference on Computer Vision and Pattern
Recognition, 2017.

[17] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-

variate shift. In Proc. International Conference on Machine
Learning, 2015.

[18] Viren Jain and Sebastian Seung. Natural image denoising
with convolutional networks. In Proc. International Confer-
ence on Neural Information Processing Systems, 2009.

[19] M. Ramesh Kanthan and S. Naganandini Sujatha. Rain drop
detection and removal using k-means clustering.
In Proc.
International Conference on Computational Intelligence and
Computing Research, 2015.

[20] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych,
Dmytro Mishkin, and Jiri Matas. Deblurgan: Blind mo-
tion deblurring using conditional adversarial networks.
In
Proc. Conference on Computer Vision and Pattern Recogni-
tion, 2018.

[21] Hiroyuki Kurihata, Tatsuro S Takahashi, Ichiro Ide, Y.
Mekada, Hiroshi Murase, Yukimasa Tamatsu, and Takayuki
Miyahara. Rainy weather recognition from in-vehicle cam-
era images for driver assistance. In Proc. Intelligent Vehicles
Symposium, 2005.

[22] Boyi Li, Xiulian Peng, Zhangyang Wang, Ji-Zheng Xu, and
Dan Feng. Aod-net: All-in-one dehazing network. In Proc.
International Conference on Computer Vision, 2017.

[23] Guanbin Li, Xiang He, Wei Zhang, Huiyou Chang, Le Dong,
and Liang Lin. Non-locally enhanced encoder-decoder net-
work for single image de-raining.
In Proc. ACM Interna-
tional Conference on Multimedia, 2018.

[24] Runde Li, Jinshan Pan, Zechao Li, and Jinhui Tang. Single
image dehazing via conditional generative adversarial net-
work. In Proc. Conference on Computer Vision and Pattern
Recognition, 2018.

[25] Xia Li, Jianlong Wu, Zhouchen Lin, Hong W. Liu, and
Hongbin Zha. Recurrent squeeze-and-excitation context ag-
gregation net for single image deraining. In Proc. European
Conference on Computer Vision, 2018.

[26] Xiao-Jiao Mao, Chunhua Shen, and Yu-Bin Yang.

Image
restoration using very deep convolutional encoder-decoder
networks with symmetric skip connections. In Proc. Inter-
national Conference on Neural Information Processing Sys-
tems, 2016.

[27] Gaofeng Meng, Ying Wang, Jiangyong Duan, Shiming Xi-
ang, and Chunhong Pan. Efﬁcient image dehazing with
boundary constraint and contextual regularization. In Proc.
International Conference on Computer Vision, 2013.

[28] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring.
In Proc. Conference on Computer Vision and
Pattern Recognition, 2017.

[29] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units im-
prove restricted boltzmann machines. In Proc. International
Conference on Machine Learning, 2015.

[30] Rui Qian, Robby T. Tan, Wenhan Yang, Jiajun Su, and Jiay-
ing Liu. Attentive generative adversarial network for rain-
drop removal from a single image. In Proc. Conference on
Computer Vision and Pattern Recognition, 2018.

[31] Joseph Redmon and Ali Farhadi. Yolov3: An incremental

improvement. arXiv:1804.02767, 2018.

7015

[32] Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao,
and Ming-Hsuan Yang. Single image dehazing via multi-
scale convolutional neural networks. In Proc. European Con-
ference on Computer Vision, 2016.

[33] Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun
Cao, Wei Liu, and Ming-Hsuan Yang. Gated fusion network
for single image dehazing. In Proc. Conference on Computer
Vision and Pattern Recognition, 2018.

[34] M. Roser and A. Geiger. Video-based raindrop detection for
improved image registration. In Proc. International Confer-
ence on Computer Vision Workshops, 2009.

[35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Fei-Fei Li. ImageNet Large Scale Visual Recognition Chal-
lenge. International Journal of Computer Vision, 2015.

[36] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz,
Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In
Proc. Conference on Computer Vision and Pattern Recogni-
tion, 2016.

[37] Masanori Suganuma, Mete Ozay, and Takayuki Okatani. Ex-
ploiting the potential of standard convolutional autoencoders
for image restoration by evolutionary search. In Proc. Inter-
national Conference on Machine Learning, 2018.

[38] Jian Sun, Wenfei Cao, Zongben Xu, and Jean Ponce. Learn-
ing a convolutional neural network for non-uniform motion
blur removal. In Proc. Conference on Computer Vision and
Pattern Recognition, 2015.

[39] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem-
net: A persistent memory network for image restoration. In
Proc. International Conference on Computer Vision, 2017.

[47] Atsushi Yamashita, Yuu Tanaka, and Toru Kaneko. Removal
of adherent waterdrops from images acquired with stereo
camera.
In Proc. International Conference on Intelligent
Robots and Systems, 2005.

[48] Dong Yang and Jian Sun. Proximal dehaze-net: A prior
learning-based deep network for single image dehazing. In
Proc. European Conference on Computer Vision, 2018.

[49] Wenhan Yang, Robby T. Tan, Jiashi Feng, Jiaying Liu, Zong-
ming Guo, and Shuicheng Yan. Joint rain detection and re-
moval from a single image. In Proc. Conference on Com-
puter Vision and Pattern Recognition, 2017.

[50] Shaodi You, Robby T.Tan, Rei Kawakami, Yasuhiro
Mukaigawa, and Katsushi Ikeuchi. Adherent raindrop mod-
eling, detection and removal in video. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2016.

[51] He Zhang and Vishal M Patel. Densely connected pyramid
dehazing network. In Proc. Conference on Computer Vision
and Pattern Recognition, 2018.

[52] He Zhang and Vishal M Patel. Density-aware single image
de-raining using a multi-stream dense network.
In Proc.
Conference on Computer Vision and Pattern Recognition,
2018.

[53] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learning of
deep cnn for image denoising. IEEE Transactions on Image
Processing, 2017.

[54] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward
a fast and ﬂexible solution for CNN based image denoising.
IEEE Transactions on Image Processing, 2018.

[55] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networkss. In Proc. International Con-
ference on Computer Vision, 2017.

[40] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky.
Instance normalization: The missing ingredient for fast styl-
ization. arXiv:1607.08022, 2016.

[56] Qingsong Zhu, Jiaming Mai, and Ling Shao. A fast single
image haze removal algorithm using color attenuation prior.
IEEE Transactions on Image Processing, 2015.

[41] Andreas Veit, Michael J. Wilber, and Serge Belongie. Resid-
ual networks behave like ensembles of relatively shallow net-
works. In Proc. International Conference on Neural Infor-
mation Processing Systems, 2016.

[42] Patrick Wieschollek, Michael Hirsch, Bernhard Sch¨olkopf,
and Hendrik P. A. Lensch. Learning blind motion deblur-
ring. In Proc. International Conference on Computer Vision,
2017.

[43] Junyuan Xie, Linli Xu, and Enhong Chen. Image denoising
and inpainting with deep neural networks.
In Proc. Inter-
national Conference on Neural Information Processing Sys-
tems, 2012.

[44] Jun Xu, Hui Li, Zhetong Liang, David Zhang, and Lei
Zhang. Real-world noisy image denoising: A new bench-
mark. arXiv:1804.02603, 2018.

[45] Li Xu and Jiaya Jia. Two-phase kernel estimation for robust
motion deblurring. In Proc. European Conference on Com-
puter Vision, 2010.

[46] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse
representation for natural image deblurring. In Proc. Confer-
ence on Computer Vision and Pattern Recognition, 2013.

7016

