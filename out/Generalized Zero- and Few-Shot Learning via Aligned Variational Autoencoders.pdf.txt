Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders

Edgar Sch¨onfeld1

Sayna Ebrahimi2

Samarth Sinha3

Trevor Darrell2

Zeynep Akata4

1Bosch Center for AI

2UC Berkeley 3 University of Toronto 4University of Amsterdam

Abstract

Many approaches in generalized zero-shot learning rely
on cross-modal mapping between the image feature space
and the class embedding space. As labeled images are ex-
pensive, one direction is to augment the dataset by gener-
ating either images or image features. However, the former
misses ﬁne-grained details and the latter requires learning
a mapping associated with class embeddings. In this work,
we take feature generation one step further and propose a
model where a shared latent space of image features and
class embeddings is learned by modality-speciﬁc aligned
variational autoencoders. This leaves us with the required
discriminative information about the image and classes in
the latent features, on which we train a softmax classiﬁer.
The key to our approach is that we align the distributions
learned from images and from side-information to construct
latent features that contain the essential multi-modal infor-
mation associated with unseen classes. We evaluate our
learned latent features on several benchmark datasets, i.e.
CUB, SUN, AWA1 and AWA2, and establish a new state
of the art on generalized zero-shot as well as on few-shot
learning. Moreover, our results on ImageNet with various
zero-shot splits show that our latent features generalize well
in large-scale settings.

1. Introduction

Generalized zero-shot learning (GZSL) is a challenging
task especially for unbalanced and large datasets such as
ImageNet [5]. Although at training time no visual data of
some classes, i.e. unseen classes, are provided the classiﬁer
must learn to differentiate between all classes, i.e. seen and
unseen classes. As visual data of unseen classes is not avail-
able at training time, typically knowledge transfer from seen
to unseen classes is achieved via some form of side infor-
mation that encode semantic relationship between classes,
i.e. class embeddings.

Most approaches to GZSL [6, 1, 20, 34, 2] learn a map-
ping between images and their class embeddings. An or-
thogonal approach is to augment data by generating artiﬁ-

Figure 1: Our CADA-VAE model learns a latent embedding
(z) of image features (x) and class embedding (c(y) of la-
bels y) via aligned VAEs optimized with cross-alignment
(LCA) and distribution alignment (LDA) objectives, and
subsequently trains a classiﬁer on sampled latent features
of seen and unseen classes.

cial images [23]. However, due to the level of detail missing
in the synthetic images, CNN features extracted from them
do not improve classiﬁcation accuracy. To alleviate this is-
sue, [36] proposed to generate image features via a con-
ditional WGAN, which simpliﬁes the task of the generative
model and directly optimizes the loss on image features. Al-
though the features generated by [36] improved GZSL sig-
niﬁcantly, GAN-based loss functions suffer from instabil-
ity in training. Hence, recently conditional variational au-
toencoders (VAE) [18, 14] have been employed for this pur-
pose. As GZSL is inherently a multi-modal learning task,
[29] proposed to transform both modalities to the latent
spaces of autoencoders and match the corresponding dis-
tributions by minimizing the Maximum Mean Discrepancy
(MMD). Learning such cross-modal embeddings is beneﬁ-
cial for potential downstream tasks that require multimodal
fusion, e.g. visual question answering. In this domain, [22]
recently used a cross-modal autoencoder to extend visual
question answering to previously unseen objects.

18247

latent spaceClassifierJoint VAE latent spaceClassifierSemantic feature spaceImage feature spaceImage feature spaceSemantic feature spaceImage spaceSemantic spacered headpink bellybrown wingsgray beakred headbrown bellybrown wingsgray beakfeature spaceE1E2D2D1In this work, we train VAEs to encode and decode fea-
tures from different modalities, e.g.
images and class at-
tributes, and use the learned latent features to train a gen-
eralized zero-shot learning classiﬁer. Our latent represen-
tations are aligned by matching their parametrized distribu-
tions and by enforcing a cross-modal reconstruction crite-
rion. Consequently, by explicitly enforcing alignment both
in the latent features and in the distributions of latent fea-
tures learned using different modalities, the VAEs enable
knowledge transfer to unseen classes without forgetting the
previously seen classes.

Our contributions are as follows.

(1) We propose the
CADA-VAE model that learns shared cross-modal latent
representations of multiple data modalities using VAEs
via distribution alignment and cross alignment objectives.
(2) We extensively evaluate our model using conventional
benchmark datasets, i.e. CUB, SUN, AWA1 and AWA2,
on zero-shot and few-shot learning settings. Our model es-
tablishes the new state-of-the-art performance on general-
ized zero-shot and few-shot learning settings on all these
datasets. Furthermore, we show that our model can be ex-
tended easily to more than two modalities that are trained
simultaneously. (3) Finally, we show that the latent features
learned by our model improve the state of the art in the truly
large-scale ImageNet dataset in all splits for the generalized
zero-shot learning task.

2. Related Work

In this section, we present related work on generalized
zero-shot learning, few-shot learning and cross-modal re-
construction.

Generalized Zero-and Few-Shot Learning. In zero-shot
learning, training and test classes are disjoint with shared
attributes annotated on class level, and the performance of
the method is solely judged on its classiﬁcation accuracy on
the novel, i.e. unseen classes. Generalized zero-shot learn-
ing is a more realistic variant of zero-shot learning, since
the same information is available at training time, but the
performance of the model is judged on the harmonic mean
of the classiﬁcation accuracy on seen and unseen classes.

In few-shot learning, there are k examples provided at
training time for the previously unseen classes [30, 26, 10,
32]. Using auxiliary information for few-shot learning was
introduced in [30], where attributes related to images were
used to improve the performance of the model. The use of
auxiliary information was also explored in ReViSE [29], in
which a common image-label semantic space for transduc-
tive few-shot learning is learned. Analogous to the relation
between ZSL and GZSL, we extend few-shot to the general-
ized few-shot learning (GFSL) setting, in which we evaluate
the model on both seen and unseen classes.

Data-Generating Models for GZSL. Generative models

are used for generating images or image feature as a data-
augmentation [36, 18, 14] mechanism in GZSL. These ap-
proaches treat GZSL as a missing data problem and train
conditional GANs or conditional VAEs to generate image
features for unseen classes from semantic side-information.
In this work, we create latent space features instead.

Cross-Modal Embedding Models. Recent cross-modal
embeddings for GZSL are based on autoencoders, such as
ReViSE [29] and DMAE [19], which learn to jointly repre-
sent features from different modalities in their latent space.
By making use of autoencoders, it is possible to learn rep-
resentations of visual and semantic information in a semi-
supervised fashion. Learning a joint representation for vi-
sual and semantic data is achieved by aligning the latent
distributions between different data types. ReViSE imple-
ments this distribution alignment by minimizing the max-
imum mean discrepancy between the two latent distribu-
tions [9]. DMAE aligns distributions by means of mini-
mizing the Squared-Loss Mutual Information [28]. In this
work, we use Variational Autoencoders instead, and align
the latent distributions by minimizing their Wasserstein dis-
tance. In contrast to [19] and [29], we also enforce a cross-
reconstruction loss, by decoding every encoded feature into
every other modality.

Cross-Reconstruction in Generative models. Recon-
structing data across domains,
referred to as cross-
alignment, is commonly used in the ﬁeld of domain adap-
tation. While models like CycleGAN [37] learn to generate
data across domains directly, latent space models use cross-
reconstruction to capture the common information con-
tained in both domains in their intermediate latent represen-
tations [31]. In this regard, cross-aligned VAE’s have been
used previously for text-style transfer [25] and image-to-
image translation [16]. In [25] a cross-aligned VAE ensures
that the latent representations of texts from different input
domains are similar, while in [16] a comparable approach
matches the latent representations of images from different
domains. Both methods have in common that they use a dif-
ferent variant of VAEs with an adversarial loss. Addition-
ally, [25] makes use of conditional encoders and decoders,
while [16] enforces cycle consistency and weight sharing.
Similarly, better generalization can be achieved if the shared
representation space is amenable to class-interpolation, sen-
tence interpolation [3] and image interpolation [11]. In this
paper, our building blocks are unconditional VAEs and we
achieve multi-modal alignment via cross-reconstruction and
latent distribution alignment in a highly reduced space.

3. CADA-VAE Model

Of the existing GZSL models, recent data generating ap-
proaches [36, 14, 18] achieve superior performance over
other methods on disjoint datasets. Classifying gener-

28248

Figure 2: Our Cross- and Distribution Aligned VAE (CADA-VAE ). Latent distribution alignment is achieved by minimizing
the Wasserstein distance between the latent distributions (LDA). Similarly, the cross-alignment loss (LCA) encourages the
latent distributions to align through cross-modal reconstruction.

ated image features from GANS [36] or conditional VAEs
[14, 18] runs at risk of being compromised by the curse of
dimensionality. On the other hand, CADA-VAE has a con-
trol over the dimensionality and structure (via a prior) of
the features to be classiﬁed. The main insight of our pro-
posed model is that instead of generating images or im-
age features, we generate low-dimensional latent features
and achieve both stable training and state-of-the-art perfor-
mance. Hence, the key to our approach is the choice of a
VAE latent-space, a reconstruction and cross-reconstruction
criterion to preserve class-discriminative information in
lower dimensions, as well as explicit distribution alignment
to encourage domain-agnostic representations.

3.1. Background

We ﬁrst provide the background as the task (GZSL) and

as the model building blocks (variational autoencoders).

Generalized Zero-shot Learning. The task deﬁnition of
GZSL is as follows. Let S = {(x, y, c(y))| x ∈ X, y ∈
Y S, c(y) ∈ C} be a set of training examples, consisting of
image-features x, e.g. extracted by a CNN, class labels y
available during training and class-embeddings c(y). Typi-
cal class-embeddings are vectors of hand-annotated contin-
uous attributes or Word2Vec features [17]. In addition, an
auxiliary training set U = {(u, c(u))| u ∈ Y u, c(u) ∈ C}
is used, where u denote unseen classes from a set Y u, which
is disjoint from Y S. Here, C(U ) = {c(u1), ..., c(uL)}
is the set of class-embeddings of unseen classes.
In the
legacy challenge of ZSL, the task is to learn a classiﬁer
fZSL : X → Y U . However, in this work, we focus on
the more realistic and challenging setup of GZSL where the
aim is to learn a classiﬁer fGZSL : X → Y U ∪ Y S.

Variational Autoencoder (VAE). The basic building block
of our model is the variational autoencoder (VAE) [13].
Variational inference aims at ﬁnding the true conditional
probability distribution over the latent variables, pφ(z|x).
Due to interactibility of this distribution,
it can be ap-
proximated by ﬁnding its closest proxy posterior, qθ(z|x),
through minimizing their distance using a variational lower
bound limit. The objective function of a VAE is the vari-
ational lower bound on the marginal likelihood of a given
datapoint and can be formulated as:

L = E

qφ(z|x)[log pθ(x|z)] − DKL(qφ(z|x)||pθ(z))

(1)

where the ﬁrst term is the reconstruction error and the
second term is the unpacked Kullback-Leibler divergence
between the inference model q(z|x), and p(z). A com-
mon choice for the prior is a multivariate standard Gaus-
sian distribution. The encoder predicts µ and Σ such that
qφ(z|x) = N (µ, Σ), from which a latent vector z is gener-
ated via the reparametrization trick [13].

3.2. Cross and Distribution Aligned VAE

The goal of our model is to learn representations within
a common space for a combination of M data modali-
ties. Hence, our model includes M encoders, one for ev-
ery modality, to map into this representation space. To
minimize information loss, the original data must be recon-
structed via the decoder networks. In effect, the basic VAE
loss of our model is the sum of M VAE-losses:

LV AE =

M

X

E

qφ(z|x)[log pθ(x(i)|z)]

(2)

i
− βDKL(qφ(z|x(i))||pθ(z))

38249

E1E2D1D2E1E2D1D2E1E2D1D2E1E2D1D2E1E2D1D2E1E2D1D2E1E2D1D2COMPACT FIGURES(SMALL ENOUGH TO PUT 3 IN A ROW)SLIGHTLY MORE DETAILED  FIGURES(PROBABLY TOO BIG TO PUT 3 IN A ROW)DETAILED  FIGURE(THE EQUATIONS ON THE RIGHT ARE THE CROSS-RECONSTRUCTION LOSS. THE BASIC VAE LOSS IS NOT SHOWN)CADA-VAE:DA-VAE:CA-VAE:Current choice:red headpink bellybrown wingsgray beakwhere β weights the KL-Divergence [11]. In the case of
matching image features with class embeddings, M = 2,
x(1) ∈ X and x(2) ∈ C(Y S). Yet, making the modality-
speciﬁc autoencoders learn similar representations across
modalities requires additional regularization terms. There-
fore, our model aligns the latent distributions explicitly and
In Figure 2 we
enforces a cross-reconstruction criterion.
show an overview of our model, depicting these two forms
of latent distribution matching, which we refer to as Cross-
Alignment (CA) and Distribution-Alignment (DA).

Cross-Alignment (CA) Loss. Here, reconstructions are ob-
tained by decoding the latent encoding of a sample from an-
other modality, but the same class. Hence, every modality-
speciﬁc decoder is trained on the latent vectors derived from
the other modalities. This cross-reconstruction loss is:

LCA =

M

M

X

X

i

j6=i

|x(j) − Dj(Ei(x(i)))|.

(3)

where Ei is the encoder of a feature of ith modality and
Dj is the decoder of a feature of the same class but the jth
modality.

Distribution-Alignment (DA) Loss. Generated image and
class representations can also be matched by minimizing
their distance. Here, we minimize the Wasserstein distance
between the latent multivariate Gaussian distributions. In
the case of multivariate Gaussians, a closed-form solution
of the 2-Wasserstein distance [8] between two distributions
i and j is given as:

Wij = (cid:2)||µi − µj||2

2

(4)

+ T r(Σi) + T r(Σj) − 2(Σ

1

1

2

i ΣiΣ

2

j )

1

2 .

1

2 (cid:3)

Since the encoder predicts diagonal covariance matrices,
which are commutative, this distance simpliﬁes to:

Wij = (cid:0)||µi − µj||2

2 + ||Σ

1

2

i − Σ

1

2

j ||2

Frobenius(cid:1)

1

2

(5)

and the Distribution Alignment (DA) loss for a group an
M-tuple is written as:

LDA =

M

M

X

X

i

j6=i

Wij.

(6)

Cross- and Distribution Alignment (CADA-VAE) Loss.
The cross- and distribution aligned VAE combines the basic
VAE-loss with LCA (CA-VAE) and LDA (DA-VAE):

LCADA−V AE = LV AE + γLCA + δLDA

(7)

where γ and δ are the weighting factors of the cross align-
ment and the distribution alignment loss, respectively. We

show in section 4.1 that our model can learn shared mul-
timodal embeddings of more than two modalities, without
examples of all modalities being available for all classes.

Implementation Details. All encoders and decoders are
multilayer perceptrons with one hidden layer. More hidden
layers degraded performance as CNN features and attributes
are already very high-level representations. We use 1560
hidden units for the image feature encoder and 1660 for the
decoder. The attribute encoder and decoder have 1450 and
660 hidden units, respectively.

The latent embedding size is 64. For ImageNet, we
chose a size of 128, and use two hidden layers of identical
size for the encoder, with the number of hidden units spec-
iﬁed above and the image feature decoder layers are of size
1160 and 1660, while the attribute decoder uses 460 and
660 units. The model is trained for 100 epochs by stochastic
gradient descent using the Adam optimizer [12] and a batch
size of 128 for ImageNet and 50 for all other datasets. Each
training batch consists of pairs of CNN features and match-
ing attributes, from different seen classes. A pair of data
always belongs to the same class. After individual VAEs
learn to encode features of only their speciﬁc datatype for
some epochs, we also start to compute cross- and distribu-
tion alignment losses. δ is increased from epoch 6 to epoch
22 by a rate of 0.54 per epoch, while γ is increased from
epoch 21 to 75 by 0.044 per epoch. For the KL-divergence
we use an annealing scheme [3], in which we increase the
weight β of the KL-divergence by a rate of 0.0026 per epoch
until epoch 90. A KL-annealing scheme serves the purpose
of ﬁrst letting the VAE learn “useful” representations before
they are “smoothed” out, since the KL-divergence would be
otherwise a very strong regularizer [3].

We empirically found that it is useful to use a variant of
the reparametrization trick [13], in which all dimensions of
the noise vector are sampled from a single unimodal Gaus-
sian. Also, using the L1 distance as reconstruction error
appeared to yield slightly better results than L2. After train-
ing, the VAE encoders transform the training and test set of
the ﬁnal linear classiﬁer into the latent space 1.

4. Experiments

We evaluate our framework on four widely used bench-
mark datasets CUB-200-2011 [33] (CUB), SUN attribute
(SUN) [21], Animals with Attributes 1 and 2 (AWA1 [15],
AWA2 [35]) for the GZSL and GFSL settings. All im-
age features for VAE training originate from the 2048-
dimensional ﬁnal pooling layer of a ResNet-101. To avoid
violating the zero-shot assumption, i.e. test classes need to
be disjoint from the classes that ResNet-101 was trained
with, we use the proposed training splits in [35].

1code at https://github.com/edgarschnﬂd/CADA-VAE-PyTorch

48250

Model

DA-VAE

CA-VAE

CADA-VAE

S

48.1
52.6
53.5

U

43.8
48.1
51.6

H

45.8
50.2
52.4

Table 1: Ablation study. We compare GZSL accuracy on
CUB for different multi-modal alignment objective func-
tions, i.e. DA-VAE (distribution aligned VAE) , CA-VAE
(cross-aligned VAE) and CADA-VAE (cross and distribu-
tion aligned VAE).

Attributes serve as class embeddings when available. For
CUB, we also use sentence embeddings extracted from 10
sentences annotated per image averaged per class [23] and
for ImageNet we used Word2Vec [17] embeddings provided
by [4]. All hyperparameters were chosen on a validation set
provided by [35]. We report the harmonic mean (H) be-
tween seen (S) and unseen (U) average per-class accuracy.

4.1. Analyzing CADA-VAE in Detail on CUB

In this section, we analyze several building blocks of our
proposed framework such as the model, the choice of class
embeddings as well as the size and the number of latent
embeddings generated by our model in the GZSL setting.

Analyzing Model Variants.
In this ablation study,
we present
the results of different objective functions
and the corresponding VAE variants, CA-VAE (cross-
aligned VAE), DA-VAE (distribution-aligned VAE) and
CADA-VAE (cross- and distribution-aligned VAE) on the
CUB dataset in GZSL setting.

As shown in Table 1, the cross-alignment objective noti-
cably improves performance compared to distribution align-
ment (50.2% vs. 45.8%). This is due to the fact that both
seen and unseen class accuracies increase, i.e. seen class
accuracy increases by 4.5% and unseen class accuracy in-
creases by 4.3%, when we use cross alignment loss rather
than the distribution alignment loss. Moreover, combin-
ing distribution alignment and the cross-alignment objec-
tives, i.e. CADA-VAE, increases the accuracy to 52.4% that
comes from adding the distribution alignment to the CA-
VAE. Our ablation study shows that aligning both the la-
tent representations and the latent spaces is complementary
since their combination leads to the highest result on both
seen, unseen classes and their harmonic mean.

Analyzing Side Information. In sparse data regimes es-
pecially in zero-shot learning semantic representation of
the classes, i.e.
class embeddings, are as important as
the image embeddings as they enable knowledge trans-
fer from seen to unseen classes. We compare the results
obtained with per-class attributes, per-class sentences and
class-based Word2Vec representations.

c
c
a

50
40
30
20
10
0

att
stc
w2v
H

S

 

U

 

XS

100

70

50

30

0

2

43

45

50

52
0

30

46

46

47

46
30

47

49

47

46

53

51

47

44

32
2
70 100

40

47

49

48

40
50
XU

Figure 3: Effect of different class embeddings. (Left) Seen,
unseen and harmonic mean accuracy for CUB using differ-
ent class embeddings as side information.
(Right) Using
both attributes and sentences as side information, i.e. XS:
the percentage of seen classes with sentences, XU : the per-
centage of unseen classes with sentences. Attributes are the
class embeddings for the (100 − X)% of the classes.

Our results in Figure 3 (left) show that per-class sen-
tence embeddings result in the best performance among all
three, i.e. 53.4%, attributes follow closely, i.e. 52.4%. With
Word2Vec, the difference between the seen and the unseen
class accuracy is large, indicating that the latent represen-
tations learned by Word2Vec are weak in robustness. This
is expected, given that Word2Vec features do not explic-
itly or exclusively represent visual characteristics. In sum-
mary, these results demonstrate that our model is able to
learn from various sources of side information. The results
also show that latent features learned with more discrimina-
tive class embeddings lead to better overall accuracy.

To investigate one of the most prominent aspects of our
model, i.e.
the ability to handle missing side informa-
tion, we train CADA-VAE such that XS% of seen class im-
age features are paired with sentence embeddings while the
other (100−XS)% of seen classes are paired with attributes.
The setup is evaluated for XS = 0, 30, 50, 70, 100. We also
vary the fraction XU % of unseen classes that are learned
from sentence features (whereas (100 − XU )% denotes the
fraction of unseen classes for which image features are only
paired with attributes).

Figure 3 (right) shows the results using different frac-
tions of sentence embeddings and attribute embeddings for
XS and XU . When XU is held stable at 50%, i.e. both
seen and unseen classes have access to sentences and at-
tributes equally half the time, we reach the highest accu-
racy for an XS-XU ratio of 50%-50%.
Interestingly, at
(XS = 0, XU = 50), i.e. no seen-class sentences while un-
seen classes are represented by both attributes, the accuracy
is 40%. On the other hand, at (XS = 50, XU = 0), i.e. no
unseen-class sentences while seen classes are represented
by both attributes sentences, the accuracy increases to 45%.
At (XS = 50, XU = 100), i.e. 50% attributes and 50% sen-

58251

H
c
c
a

55
50
45
40
35

12 25 50 64 100 200 250

latent dimensions (d)

s
s
a
l
c
 
n
e
e
s
n
u
 
r
e
p
 
s
e
r
u
t
a
e
f

300

200

100

50

5

38

38

52
52
38

52
37

18
15 50
300
features per seen class

100

200

H
c
c
a

60

40

20

0

fixed: RUS=1
fixed: RUS=2
fixed: RUS=3
dynamic
50

5

1
100
features per seen class

10

Figure 4: The inﬂuence of the dimentionality of the latent
features that are generated by CADA-VAE and used to train
the GZSL classiﬁer. We measure the harmonic mean accu-
racy on the CUB dataset

tences for seen classes but unseen classes are represented
by sentences only, the accuracy further increases to 47%.
These results indicate that sentences have an edge over at-
tributes. However, when either sentences or attributes are
not available, our model can recover the missing informa-
tion from the other modality and still learn discriminative
representations.

Increasing Number of Latent Dimensions. In this analy-
sis, we have explored the robustness of our method to the
dimensionality of the latent space. Higher dimensions al-
low more degrees of freedom but require more data, while
compact features capture the essential discriminative infor-
mation. Without loss of generality, we report the harmonic
mean accuracy of CADA-VAE for different latent dimen-
sions on CUB, i.e. 12, 25, 50, 64, 100, 200 and 250.

We observe in Figure 4 that the accuracy initially in-
creases with increasing dimensionality until it achieves its
peak accuracy of 52.4% at d = 64 and ﬂattens until d = 100
after which it declines upon further increase of the latent
dimension. We conclude from these experiments that the
most discriminative properties of two modalities are cap-
tured when the latent space has around 64 − 100 dimen-
sions. For efﬁciency reasons, we use 64 dimensional latent
features for the rest of the paper.

Increasing Number of Latent Features. Our model can
be used to generate an arbitrarily large number of latent fea-
tures. In this experiment, we vary the number of latent fea-
tures per class from 1 to 300 on CUB in the GZSL setting
and reach the optimum performance with 50 or more latent
features per seen class (Figure 5, left). In principle, seen
and unseen classes do not need to have the same number
of samples. We also vary the number of features per seen
and unseen classes. Indeed, the best accuracy is achieved
when there are approximately twice as many features per
unseen than seen classes which improves the accuracy from
37% to 52%. While 100 latent features per every class, i.e.

Figure 5: Analyzing the effect of the number of latent fea-
tures per class on the harmonic mean accuracy in GZSL. An
unseen-seen ratio RU S of 2 means that twice as many sam-
ples are generated for unseen classes than for seen classes
. The dynamic dataset (light blue) does not rely on a ﬁxed
number of sampled latent features.

200 × 100 = 20K, gives 38% accuracy, having 50 latent
features per seen classes and 100 latent features per unseen
class, i.e. 100×50+50×150 = 12.5K, leads to 52% accu-
racy. Hence, generating more features of under-represented
classes is important for better accuracy.

As for our results in Figure 5 on the right, we build a
dynamic training set by generating latent features continu-
ously at every iteration and do not use any sample more than
once. Hence, we eliminate one of the tunable parameters,
i.e. the number of latent features to generate. Because of
the non-deterministic mapping of the VAE encoder, every
latent feature of a different class is unique. Our results in-
dicate that the best accuracy is achieved when unseen and
seen class samples are equally balanced. In CUB, using a
dynamic training set reaches the same performance as us-
ing a ﬁxed dataset with 100 unseen examples and 50 seen
examples. On the other hand, using a ﬁxed dataset leads to
a faster training procedure. Hence, we use a ﬁxed dataset
with 200 examples per seen class and 400 examples per un-
seen class in every benchmark reported in this paper.

4.2. Comparing CADA-VAE on Benchmark Datasets

In this section, we compare our CADA-VAE on four
benchmark datasets, i.e. CUB, SUN, AWA1 and AWA2,
in the GZSL and GFSL setting.

Generalized Zero-Shot Learning. We compare our model
with 11 state-of-the-art models. Among those, CVAE [18],
SE [14], and f-CLSWGAN [36] learn to generate artiﬁcial
visual data and thereby treat the zero-shot learning task as
a data-augmentation task. On the other hand, the classic
ZSL methods DeViSE [6], SJE [2], ALE [1], EZSL [24]
and LATEM [34] use a linear compatibility function or
other similarity metrics to compare embedded visual and
semantic features; CMT [27] and LATEM [34] utilize mul-
tiple neural networks to learn a non-linear embedding; and

68252

Feature Size

S

U

H

S

U

H

S

U

H

S

U

H

CUB

SUN

AWA1

AWA2

Model

CMT [27]

SJE [2]

ALE [1]

LATEM [34]

EZSL [24]

SYNC [4]

DeViSE [6]

2048

f-CLSWGAN [36]

CVAE [18]

SE [14]

ReViSE [29]

ours (CADA-VAE)

1024

75/100

64

21.8
8.1
30.5 14.7
33.1 21.8
28.8 14.7
27.9 11.0
43.3
7.9
27.4 16.9
36.6 42.6

49.8
7.2
59.2 23.5
62.8 23.7
57.3 15.2
63.8 12.6
70.9 11.5
53.0 23.8
57.7 43.7

87.6
0.9
74.6 11.3
76.1 16.8
7.3
71.7
75.6
6.6
87.3
8.9
68.7 13.4
61.4 57.9

1.0
14.4
23.9
20.0
11.0
18.0
27.8
59.4
51.2
62.8
53.3 41.5
28.3 37.6
42.8
53.5 51.6 52.4 35.7 47.2 40.6 72.8 57.3 64.1 75.0 55.8 63.9

0.5
90.0
8.0
73.9
81.8 14.0
77.3 11.5
77.8
5.9
90.5 10.0
74.7 17.1
68.9 52.1

1.8
19.6
27.5
13.3
12.1
16.2
22.4
59.6
47.2
61.5
41.1

11.8
19.8
26.3
19.5
15.8
13.4
20.9
39.4
26.7
34.9
22.0

12.6
33.6
34.4
24.0
21.0
19.8
32.8
49.7
34.5
46.7
32.3

67.8 56.3
37.1 46.1

68.1 58.3
39.7 46.4

30.5 40.9
20.1 24.3

–

–

–

–

–

–

–

–

Table 2: Comparing CADA-VAE with the state of the art. We report per class accuracy for seen (S) and unseen (S) classes
and their harmonic mean (H). All reported numbers for our method are averaged over ten runs.

SYNC [4] aligns a class embedding space and a weighted
bipartite graph. ReViSE [29] learns a shared latent mani-
fold between the image features and class attributes using
autoencoders.

The results in Table 2 show that our CADA-VAE out-
performs all other methods on all datasets. The accuracy
difference between our model and the closest baseline, Re-
ViSE [29], is as follows: 52.4% vs 32.3% on CUB, 40.6%
vs 22.0% on SUN, 64.1% vs 41.1% on AWA1 and 63.9% vs
42.8% on AWA2. Moreover, our model achieves signiﬁcant
improvements over feature generating models most notably
on CUB. In doing so, CADA-VAE is the ﬁrst cross-modal
embedding model to outperform methods based on feature-
augmentation. Compared to the classic methods, our model
leads to at least 100% improvement in harmonic mean ac-
curacies.
In the legacy challenge of zero-shot learning,
CADA-VAE provides competitive performance, i.e. 60.4
on CUB, 61.8 on SUN, 62.3 on AWA1, 64.0 on AWA2.
However, in this work, we focus on the more practical and
challenging GZSL setting.

Since our model does not use any CNN features, i.e.
we generate 64-dimensional latent features for all classes,
it achieves a balance between seen and unseen class accu-
racies better than CNN feature-generating approaches espe-
cially on CUB. In addition, CADA-VAE learns a shared rep-
resentation in a weakly-supervised fashion, through a cross-
reconstruction objective. Since the latent features have to
be decoded into every involved modality, and since every
modality encodes complementary information, the model
is encouraged to learn an encoding that retains the infor-
mation contained in all used modalities. In doing so, our
method is less biased towards learning the distribution of

the seen class image features, which is known as the pro-
jection domain shift problem [7]. As we generate a certain
number of latent features per class using non-deterministic
encoders, our method is also akin to data-generating ap-
proaches. However, the learned representations lie in a
lower dimensional space, i.e. only 64, and therefore, are
less prone to bias towards the training set of image features.
In effect, our training is more stable than the adversarial
training schemes used for data generation [36]. In fact, we
did not conduct any dataset speciﬁc parameter tuning and
use the same parameters for all datasets.

Generalized Few-Shot Learning. We evaluate our mod-
els by using zero, two, ﬁve and ten shots for GFSL on four
datasets. We compare our results with the most similar pub-
lished work in this domain, i.e. ReViSE [30]. Figure 6
shows that our latent representations learned from the side
information improves over the GZSL setting signiﬁcantly
even by including only a few labeled samples. Speciﬁcally,
adding a single latent feature from unseen classes to the
training set improves the accuracy by 1-10%, depending
on the dataset. While on CUB the accuracy improvement
from 0 to 10 shots is 12%, on AWA1&2 this improvement
reaches 20%. Moreover, while the harmonic mean accu-
racy increases with the number of shots in both methods,
all variants of our method outperform the baseline by a large
margin across all the datasets indicating the generalization
capability of our method to the GFSL setting.

Furthermore, similar to the GZSL scenario, on the ﬁne-
grained CUB and SUN datasets, CADA-VAE reaches the
highest performance where it is followed by CA-VAE and
DA-VAE, respectively. However, on AWA1 and AWA2

78253

CUB

SUN

AWA1

AWA2

H
c
c
a

60

50

40

30

CADA-VAE
DA-VAE
CA-VAE
ReViSE
5

10

0

1

2

shots

45

40

35

30

25

H
c
c
a

CADA-VAE
DA-VAE
CA-VAE
ReViSE
5

10

0

1

2

shots

80

70

H
c
c
a

60

50

40

H
c
c
a

80

70

60

50

40

CADA-VAE
DA-VAE
CA-VAE
ReViSE
5

10

0

1

2

shots

CADA-VAE
DA-VAE
CA-VAE
ReViSE
5

10

0

1

2

shots

Figure 6: Comparing CA-VAE, DA-VAE, CADA-VAE with ReViSE [30] with increasing numbers of training samples from
unseen classes, i.e. in the generalized few-shot setting.

U
c
c
a

10.0

7.5

5.0

2.5

0.0

f-CLSWGAN
CADA-VAE

2H

3H M500 M1K M5K L500 L1K L5K

All

Figure 7: ImageNet results on GZSL. We report the top-
1 accuracy for unseen classes. Both f-CLSWGAN and
CADA-VAE use a linear softmax classiﬁer.

the difference between different models is not signiﬁcant.
We associate this with the fact that as AWA1 and AWA2
datasets are coarse-grained datasets, the image features are
already discriminative. Hence, aligning the latent space
with attributes does not lead to a signiﬁcant difference.

4.3. ImageNet Experiments

The ImageNet dataset serves as a challenging testbed for
GZSL. In [35] several evaluation splits were proposed with
increasing granularity and size both in terms of the num-
ber of classes and the number of images. Note that since
all the images of 1K classes are used to train ResNet-101,
measuring seen class accuracies would be biased. However,
we can still evaluate the accuracy of unseen class images
in the GZSL search space that contains both seen and un-
seen classes. Hence, at test time the 1K seen classes act
as distractors. This way, we can measure the transferability
of our latent representations to completely unseen classes,
i.e. classes that are not seen either during ResNet training
nor CADA-VAE training. For ImageNet, since attributes
are not available, we use Word2Vec features as class em-
beddings provided by [4]. We compare our model with
f-CLSWGAN [36], i.e. an image feature generating frame-
work which currently achieves the state of the art on Ima-
geNet. We use the same evaluation protocol on all the splits.
Among the splits, 2H and 3H are the classes 2 or 3 hops

away from the 1K seen training classes of ImageNet ac-
cording to the ImageNet hierarchy. M500, M1K and M5K
are the 500, 1000 and 5000 most populated classes, while
L500, L1K and L5K are the 500, 1000 and 5000 least pop-
ulated classes that come from the rest of the 21K classes.
Finally, ‘All‘ denotes the remaining 20K classes.

As shown in Figure 7, our model signiﬁcantly improves
the state of the art in all splits. The accuracy improvement
is signiﬁcant especially on M500 and M1K splits, i.e. for
M500 the search space is 1.5K classes, for M1K, the search
space consists of 2K classes. For the L500, L1K and L5K
splits, there are on average only 1, 3 and 5 images per class
available [35]. Since the test time search space in the ‘All‘
split is 22K dimensional, even a small improvement in ac-
curacy is considered to be compelling. The achieved sub-
stantial increase in performance by CADA-VAE shows that
our 128-dim latent feature space constitutes a robust gener-
alizable representation, surpassing the current state-of-the-
art image feature generating framework f-CLSWGAN.

5. Conclusion

In this work, we propose CADA-VAE, a cross-modal
embedding framework for generalized zero- and few-shot
learning. In CADA-VAE, we train a VAE for both visual
and semantic modalities. The VAE of each modality has to
jointly represent the information embodied by all modalities
in its latent space. The corresponding latent distributions
are aligned by minimizing their Wasserstein distance and
by enforcing cross-reconstruction. This procedure leaves
us with encoders that can encode features from different
modalities into one cross-modal embedding space, in which
a linear softmax classiﬁer can be trained. We present
different variants of cross-aligned and distribution aligned
VAEs and establish new state-of-the-art results in general-
ized zero-shot learning for four medium-scale benchmark
datasets as well as the large-scale ImageNet. We further
show that a cross-modal embedding model for generalized
zero-shot learning achieves better performance than data-
generating methods, establishing the new state of the art.

88254

References

[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-
embedding for image classiﬁcation. TPAMI, 38(7):1425–
1438, 2016. 1, 6, 7

[2] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-
uation of output embeddings for ﬁne-grained image classiﬁ-
cation. In CVPR, pages 2927–2936, 2015. 1, 6, 7

[3] S. R. Bowman, L. Vilnis, O. Vinyals, A. Dai, R. Jozefow-
icz, and S. Bengio. Generating sentences from a continuous
space. In CoNLL, pages 10–21, 2016. 2, 4

[4] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Syn-
thesized classiﬁers for zero-shot learning. In CVPR, pages
5327–5336, 2016. 5, 7, 8

[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 1

[6] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
T. Mikolov, et al. Devise: A deep visual-semantic embed-
ding model. In NIPS, pages 2121–2129, 2013. 1, 6, 7

[7] Y. Fu, T. M. Hospedales, T. Xiang, Z. Fu, and S. Gong.
Transductive multi-view embedding for zero-shot recogni-
tion and annotation. In ECCV, pages 584–599, 2014. 7

[8] C. R. Givens, R. M. Shortt, et al. A class of wasserstein
metrics for probability distributions. The Michigan Mathe-
matical Journal, 31(2):231–240, 1984. 4

[9] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and
A. Smola. A kernel two-sample test. JMLR, 13(Mar):723–
773, 2012. 2

[10] B. Hariharan and R. B. Girshick. Low-shot visual recogni-
tion by shrinking and hallucinating features. In ICCV, pages
3037–3046, 2017. 2

[11] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,
M. Botvinick, S. Mohamed, and A. Lerchner.
beta-vae:
Learning basic visual concepts with a constrained variational
framework. 2016. 2, 4

[12] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015. 4

[13] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. In ICLR, 2014. 3, 4

[14] V. Kumar Verma, G. Arora, A. Mishra, and P. Rai. General-
ized zero-shot learning via synthesized examples. In CVPR,
pages 4281–4289, 2018. 1, 2, 3, 6, 7

[15] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to
detect unseen object classes by between-class attribute trans-
fer. In CVPR, pages 951–958, 2009. 4

[16] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-
image translation networks. In NIPS, pages 700–708, 2017.
2

[17] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS, pages 3111–3119, 2013.
3, 5

[18] A. Mishra, S. Krishna Reddy, A. Mittal, and H. A. Murthy.
A generative model for zero shot learning using conditional
variational autoencoders. In CVPR, pages 2188–2196, 2018.
1, 2, 3, 6, 7

[19] T. Mukherjee, M. Yamada, and T. M. Hospedales. Deep
matching autoencoders. arXiv preprint arXiv:1711.06047,
2017. 2

[20] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,
A. Frome, G. S. Corrado, and J. Dean. Zero-shot learning
by convex combination of semantic embeddings. In ICLR,
2014. 1

[21] G. Patterson and J. Hays. Sun attribute database: Discover-
ing, annotating, and recognizing scene attributes. In CVPR,
pages 2751–2758, 2012. 4

[22] S. K. Ramakrishnan, A. Pal, G. Sharma, and A. Mittal. An
empirical evaluation of visual question answering for novel
objects. In CVPR, pages 4392–4401, 2017. 1

[23] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning deep
representations of ﬁne-grained visual descriptions. In CVPR,
pages 49–58, 2016. 1, 5

[24] B. Romera-Paredes and P. Torr. An embarrassingly simple
approach to zero-shot learning. In ICML, pages 2152–2161,
2015. 6, 7

[25] T. Shen, T. Lei, R. Barzilay, and T. Jaakkola. Style transfer
In NIPS, pages

from non-parallel text by cross-alignment.
6830–6841, 2017. 2

[26] J. Snell, K. Swersky, and R. Zemel. Prototypical networks

for few-shot learning. In NIPS, pages 4077–4087, 2017. 2

[27] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-
shot learning through cross-modal transfer. In NIPS, pages
935–943, 2013. 6, 7

[28] T. Suzuki and M. Sugiyama. Sufﬁcient dimension reduction
via squared-loss mutual information estimation. In AIStats,
pages 804–811, 2010. 2

[29] Y.-H. H. Tsai, L.-K. Huang, and R. Salakhutdinov. Learning
robust visual-semantic embeddings. In ICCV, pages 3591–
3600, 2017. 1, 2, 7

[30] Y.-H. H. Tsai and R. Salakhutdinov.

Improving one-shot
learning through fusing side information. arXiv preprint
arXiv:1710.08347, 2017. 2, 7, 8

[31] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversar-
ial discriminative domain adaptation. In CVPR, volume 1,
page 4, 2017. 2

[32] Y.-X. Wang, R. Girshick, M. Hebert, and B. Hariharan. Low-

shot learning from imaginary data. 2

[33] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-

longie, and P. Perona. Caltech-ucsd birds 200. 2010. 4

[34] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and
B. Schiele. Latent embeddings for zero-shot classiﬁcation.
In CVPR, pages 69–77, 2016. 1, 6, 7

[35] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-
shot learning-a comprehensive evaluation of the good, the
bad and the ugly. TPAMI, 2018. 4, 5, 8

[36] Y. Xian, T. Lorenz, B. Schiele, and Z. Akata. Feature gener-
ating networks for zero-shot learning. In CVPR, 2018. 1, 2,
3, 6, 7, 8

[37] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In CVPR, pages 2223–2232, 2017. 2

98255

