Predicting Future Frames using Retrospective Cycle GAN

Yong-Hoon Kwon

Min-Gyu Park

Advanced Camera Lab, LG Electronics, Korea

Korea Electronics Technology Institute

yonghoon.kwon@lge.com

mpark@keti.re.kr

Abstract

Recent advances in deep learning have signiﬁcantly im-
proved the performance of video prediction, however, top-
performing algorithms start to generate blurry predictions
as they attempt to predict farther future frames. In this pa-
per, we propose a uniﬁed generative adversarial network
for predicting accurate and temporally consistent future
frames over time, even in a challenging environment. The
key idea is to train a single generator that can predict both
future and past frames while enforcing the consistency of
bi-directional prediction using the retrospective cycle con-
straints. Moreover, we employ two discriminators not only
to identify fake frames but also to distinguish fake contained
image sequences from the real sequence. The latter discrim-
inator, the sequence discriminator, plays a crucial role in
predicting temporally consistent future frames. We experi-
mentally verify the proposed framework using various real-
world videos captured by car-mounted cameras, surveil-
lance cameras, and arbitrary devices with state-of-the-art
methods.

1. Introduction

Video prediction is the problem of generating future
frames given a set of consecutive frames, which can be used
for abnormal event detection [17], video coding [19], video
completion, robotics [6], and autonomous driving. This
problem has long been studied, and recently, deep learning
has substantially improved the performance of video pre-
diction algorithms, based on the deep architecture models
such as convolutional neural networks (CNNs) and genera-
tive adversarial networks (GANs).

Conventional video prediction approaches [25] generally
compute pixel-wise motion, and then, predict the motion of
pixels in the future frame assuming the linearity of motions.
A number of deep learning-based methods [16, 29, 31] in-
herit this idea. They explicitly compute pixel-wise motion
through deep networks, e.g., FlowNet [18], and then, the
motion information is used to generate future frames to-
gether with training images. Although the idea is similar

Figure 1. A comparison of predicted frames in a driving envi-
ronment [5]. The state-of-the-art method, PredNet [17], predicts
blurry images as the time step increases, whereas the proposed
method shows relatively sharp and accurate images. Here, PredNet
uses ten images as input whereas our method takes four images to
predict future.

to the conventional approach, deep networks show promis-
ing results while handling complex motions in a dynamic
scene. One major drawback of this approach is that com-
puting pixel-wise motion is prone to errors owing to illumi-
nation change, occlusion, and abrupt camera motion.

A number of studies [2, 11, 13, 19, 24, 32] conﬁrmed
that deep networks can predict realistic future images with-
out explicitly computing pixel-wise motion. The majority
of them takes CNNs to predict future frames [2, 11, 13, 19],
however, CNN-based methods often give blurry predictions
because they minimize the loss against all the training im-
ages [15]. To avoid the blurry artifact, Byeon et al. [2] ex-
ploited the convolutional long term short memory (ConvL-
STM) to capture both past and spatial contexts, which cur-
rently shows the best performance for a few of datasets. On
the other hand, GANs have received a considerable atten-
tion in predicting future frames [16, 24, 32], which simulta-
neously train a discriminator network and a generator net-

11811

Figure 2. An overview of the proposed method. Our network consists of one generator and two discriminators, frame generator G, frame
discriminator DA, and sequence discriminator DB. We propose a retrospective prediction scheme which allows the generator to predict
a past frame by utilizing the predicted future frame. Furthermore, we train the generator with reversed input sequences and impose retro-
spective cycle constraints for the generator by minimizing the reconstruction losses between predicted frames, e.g. x′
n+1. The
frame discriminator decides whether the predicted frame is real or fake and the sequence discriminator distinguishes a fake contained image
sequence from the real sequence to generate the temporally consistent frames.

n+1 and x′′

work. The discriminator classiﬁes the output image as real
or fake whereas the generator predicts an image that fools
the discriminator. Liang et al. [16] proposed to use dual
generators and dual discriminators, to generate both future
frames and pixel-wise motion at the same time.

Inspired by the success of deep networks in image gen-
eration [34, 35], we propose a deep network architecture for
generating future frames having several distinct features as
follows. First, we train a generator that is capable of pre-
dicting both future and past frames. We experimentally ver-
ify that this forward-backward compatible prediction yields
better prediction performance. Second, we impose the cy-
cle consistency between predicted frames with the aid of
the retrospective prediction scheme, as illustrated in Fig. 2.
The underlying idea of retrospective prediction is that if
the predicted future frame is realistic, the generator should
give a realistic past frame even the predicted future frame
is given as input. Above two features signiﬁcantly improves
the future frame prediction performance, especially when
predicting multiple frames ahead as shown in Fig. 1. Third,
we propose a sequence discriminator that takes fake con-

tained sequences as input, in addition to distinguishing a
fake frame. The sequence discriminator is designed to in-
crease the robustness and temporal consistency of predicted
frames, which is crucial for video prediction.

2. Related Work

We review relevant studies related to video prediction us-

ing deep neural networks.

CNNs and recurrent neural networks (RNNs) have
gained huge popularity over the last few years and a num-
ber of studies [2, 13, 19, 33] applied CNNs and RNNs to
predict future frames from an image sequence. Kalchbren-
ner et al. [13] proposed the video pixel network, a proba-
bilistic inference model consisting of resolution preserving
CNN encoders and PixelCNN [30] decoders. They utilized
convolutional LSTM to combine the output of the encoders
over time and used dilated convolutions to achieve large re-
ceptive ﬁelds. Several more studies [2, 6, 19] adopted con-
volutional LSTM to take spatial and temporal contexts into
account. Lotter et al. [19] introduced a predictive neural
network not only to predict the movement of an object but

1812

also to learn internal representation, e.g., the pose of an ob-
ject, based on a series of repeating stacked modules. Byeon
et al. [2] proposed parallel multi-dimensional LSTM units
and blending units to capture past and spatial contexts, re-
spectively. Finn et al. [6] proposed the action-conditioned
convolutional LSTMs, which can predict different futures
of an object conditioned on the action of an agent, e.g.,
a robot which holds the object. Xue et al. [33] tried to
ﬁnd an intrinsic representation of intensity changes, i.e.,
the difference image, through the conditional variational au-
toencoder. They used image-dependent convolution kernels
to synthesize a probable future frame from a single im-
age while considering various motions of an object. Ville-
gas et al. [31] used two separate encoders for motion and
content, but trained both encoders at simultaneously with
multi-scale motion-content residual and combination lay-
ers. Luo et al. [21] proposed an unsupervised approach to
predict long-term 3D motions based on the LSTM Encoder-
Decoder method for activity recognition.

After the invention of adversarial training [8], many
studies applied this scheme to generate images in the con-
text of image-to-image translation [10, 35], super resolu-
tion [15], style transfer [12], and video prediction [17, 24].
Mathieu et al. [24] employed an image gradient loss in a
multi-scale architecture, which signiﬁcantly reduces blur-
ring artifacts. Liu et al. [17] exploited spatial and mo-
tion constraints in addition to intensity and gradient losses.
They computed optical ﬂow through FlowNet [18] and the
ﬂow information is used to predict temporally consistent
frames. On the other hand, many researchers tried to ad-
vance GANs [1, 22, 34, 35]. For example, WGAN [1] and
LSGAN [22] modiﬁed a loss function for the discriminator
to improve the stability of training. Zhu et al. [35] suggested
a network having two generators, one takes the source im-
age and the other takes the target image as input to predict
each other image, respectively. This scheme enables to train
an arbitrary pair of images. Similarly, Yi et al. [34] sug-
gested using two discriminators to generate multiple types
of outputs. Interestingly, Liang et al. [16] employed dual
generators and dual discriminators for future frame predic-
tion. Their network predicts pixel-wise motion and a future
frame at the same time, but it requires ground truth ﬂow
information to train the network.

3. Proposed Method

Our framework consists of one generator and two dis-
criminators, frame and sequence discriminators, as de-
scribed in Fig. 2. The generator predicts both future and
past frames, even if when the input sequence contains a fake
frame. Moreover, the frame discriminator distinguishes fake
frames individually, whereas the sequence discriminator de-
cides whether the sequence contains fake frames or not.

For the clariﬁcation of explanation, we explain the nota-

tions used in the rest of the paper. Basically, we denote the
generator as G, the frame discriminator as DA, and the se-
quence discriminator as DB. The input sequence is deﬁned
as

Xm:n = {xm, xm+1, ..., xn−1, xn}

s.t. m < n,

(1)

where xi ∈ R2 is an image, m and n are indices to the ﬁrst
and last frames, the length of the sequence is n − m + 1,
and the frames are chronologically ordered. Using Xm:n as
input, the generator G predicts a future frame xn+1. Here,
the predicted frame, i.e. the fake frame, is denoted as x′
n+1
with an apostrophe. Similarly, the reversed input sequence
is deﬁned as

¯Xm:n = {xn, xn−1, ..., xm+1, xm} s.t. m < n.

(2)

Using ¯Xm:n, the generator predicts a past frame xm−1. We
also denote the sequence containing a fake frame as

X f

m:n = {xm:n−1 ∪ x′

n},

(3)

where the last frame is a fake assuming that x′
from xm−1:n−1. Similarly, its reversed case is deﬁned by

n is predicted

¯X f

m:n = {¯xm+1:n ∪ x′

m}.

(4)

m:n or ¯X f
When the sequence with fake frames, X f
given as input, we denote predicted frames as x′′
m−1, to distinguish them from predicted frames x′
x′′
x′
m−1 without fake frames.

m:n, is
n+1 or
n+1 and

3.1. Objective function

For training, we minimize the following objective func-

tion,

L = Limage + λ1LLoG + λ2Lframe

adv + λ3Lseq

adv

(5)

which consists of two reconstruction losses and two adver-
sarial losses. λ1, λ2, and λ3 are non-zero weights for bal-
ancing four loss functions.

3.1.1 Reconstruction losses

The two reconstruction loss functions are used to train the
generator. The ﬁrst loss function is formulated by

Limage = X

l1(p, q),

(p,q)∈S pair
m,n

(6)

where l1(·, ·) stands for L1 error between two images and
S pair

m,n is a set of image pairs deﬁned as

S pair
m,n = {(xm, x′
(xn+1, x′

m), (xm, x′′
n+1), (xn+1, x′′

m), (x′
n+1), (x′

m, x′′
m),
n+1, x′′

n+1)}.

(7)

1813

n+1) and (xm, x′′

n+1 is used to predict x′′

The ﬁrst loss function (6) minimizes image reconstruction
errors for six different pairs of images. (xn+1, x′
n+1) and
(xm, x′
m) are used to minimize prediction errors in forward
and backward directions. Therefore, the generator can pre-
dict both future and past frames. We deﬁne errors com-
puted by (xn+1, x′′
m) as retrospective pre-
diction errors because x′
m is
used to predict x′′
n+1. In other words, if the predicted image
n+1 is realistic, the generator can also take x′
x′
n+1 as one
of input frames to look back the past frame. The last two
pairs, (x′
n+1), take the role of cyclic
constraints because x′
m is generated by a forward sequence
and x′′
m is predicted by a backward sequence and similarly
for (x′
n+1, x′′
n+1). We say this loss function is retrospective
and cyclic, because it utilizes frames generated through ret-
rospective prediction. These pairs further constrain consis-
tency between fake frames.

m) and (x′

m and x′

n+1, x′′

m, x′′

Similarly, we deﬁne the second reconstruction loss func-

tion as

LLoG = X

l1(LoG(p), LoG(q)).

(p,q)∈S pair
m,n

(8)

This loss function computes difference between images
after applying Laplacian of Gaussian (LoG) [23] operation,
to better preserve image edges. In following study [3], they
efﬁciently suppressed low frequency information and high
frequency noise using laplacian pyramid for structurally en-
hanced image generation. We use the LoG operation to fo-
cus on the structural similarity that excludes noise.

3.1.2 Adversarial losses

Our proposed method is trained with two adversarial losses
as in (9): frame adversarial loss Lframe
adv , and sequence ad-
versarial loss Lseq
adv. The frame adversarial loss takes the
role of classifying a frame as real or fake, Speciﬁcally,
the frame adversarial loss determines whether four images,
(x′

m), are real or fake as follows,

n+1, x′′

n+1, x′

m, x′′

Lframe

adv = lA(Xm:n, xn+1) + lA(X f
+ lA( ¯Xm+1:n+1, xm) + lA( ¯X f

m:n, xn+1)
m+1:n+1, xm),

(9)

m:n, ¯Xm+1:n+1, and ¯X f

where Xm:n, X f
m+1:n+1 denote
four input sequences for the generator. The loss function,
lA(p, q), is deﬁned as

lA(p, q) = max

G

min
DA

[(DA(q) − 1)2 + (DA(G(p)))2].

(10)
Here, the generator G takes a frame sequence p and predicts
the future frame q, and DA aims to distinguish q from G(p).
Against an adversary DA, G aims at generating a fake frame
in which DA cannot distinguish it from the real frame. This
loss function is from the least square GAN [22].

Figure 3. Network architectures of the generator and discriminator
networks. Here, k, n, and s denote the kernel size, the number of
feature maps, and the stride, respectively. The generator network
learns to predict the next frame from the input image sequence,
and the discriminator network learns to classify between real or
generated frames from the generator network.

Similar to the frame adversarial loss, the sequence adver-
sarial loss takes the role of classifying an input sequence as
real or fake,

Lseq

adv = lB(Xm:n, Xm:n+1) + lB(X f
+ lB( ¯Xm+1:n+1, ¯Xm:n+1) + lB( ¯X f

m:n, Xm:n+1)

m+1:n+1, ¯Xm:n+1),
(11)

where lB(p, r) takes two sequences as input,

lB(p, r) = max

G

min
DB

[(DB(r) − 1)2 + (DB(Gc(p)))2].

(12)
Here, the generator G takes p as input to predict a new
frame, G(p), then compare it with a real image sequence
r after concatenating p and G(p). For the sake of sim-
plicity, we denote the concatenated sequence as Gc(p) =
{p ∪ G(p)}, and all the procedures rely on a single gener-
ator G. DB decides Gc(p) as fake if at least one of the im-
ages is fake. This sequence discriminator encourages tem-
porally consistent and robust prediction because it compares
sequences rather than individual frames.

3.2. Network architecture

The generator and discriminator networks are illustrated
in Fig. 3, in which we adopt an existing network architec-
ture [12] for the generator network. The difference from
[12] is that our generator takes multiple images as input to
predict a future frame. The generator network consists of
4 convolution layers, 9 residual blocks [9], and 2 transpose
convolution layers. The discriminator network consists of 5
convolution layers with leaky rectiﬁed linear units. More-
over, the network structure is the same for both frame and
sequence discriminators except the number of input images.

1814

In addition, we use the instance normalization scheme [4]
for all layers of the generator and discriminator networks
except the input and output layers.

4. Experimental Results

We evaluated the proposed method with three different
types of real-world data and compared our results with the
state-of-the-art methods. We also performed ablation stud-
ies to analyze the importance of each loss term.

4.1. Datasets

Videos captured by car-mounted cameras: We use two
popular datasets that were recorded while driving various
places using vehicle-mounted cameras: KITTI [7] and Cal-
tech pedestrian [5] datasets. Since it was recorded in a driv-
ing car, it involves relatively large motions of pixels com-
pared to other datasets.

Human action videos: The UCF101 [28] dataset consists
of 13K video clips that cover 101 classes of human actions,
captured with a variety of moving objects in static and dy-
namic environments.

Surveillance videos: The surveillance videos are captured
at a ﬁxed location. Therefore, it usually contains moving ob-
jects in a static environment. We used CUHK Avenue [26]
and ShanghaiTech Campus [20] datasets to evaluate our
method.

4.2. Training details

We set the length of an input sequence N to 4 and nor-
malized intensities to be [-1, 1]. We ﬂipped the input se-
quence horizontally with a probability of 0.3 for data aug-
mentation. We used the Adam optimizer [14] for mini-batch
stochastic gradient descent method with momentum param-
eters, β1 = 0.5 and β2 = 0.999, a batch size of 1, and
a learning rate 0.0003 with linearly decay per every 100
epochs. For balancing different losses, we set λ1 = 0.005,
λ2 = 0.003 and λ3 = 0.003. The negative slope of
Leaky ReLU is set to 0.2. To evaluate the Caltech pedes-
trian dataset, we followed experimental protocols of Pred-
Net [19]. To train the network, we used the KITTI training
dataset, that contains 41K images, and adjusted the frame
rate of the Caltech dataset to 10 fps. We cropped the input
images to 128×160 and resized the resolution of cropped
images to 256 ×256. For the UCF 101 dataset, we used 10%
of uniformly sampled images as the test set and the others
for training as in previous studies [2, 24], for the fair com-
parison. For the surveillance datasets, we resized images to
256×256. To evaluate the method of Liu et al. [17], we cal-
culated errors by using the pre-trained model provided by
the authors.

Training took four days to train our network using the
KITTI dataset on a single NVIDIA GTX 1080ti GPU. For

Table 1. Quantitative evaluation of video prediction algorithms
using various datasets: Caltech pedestrian, UCF 101, and two
surveillance datasets. The MSE is multiplied by 1,000 to clearly
show the differences among different algorithms. The table com-
pares four and ﬁve algorithms for Caltech and UCF101 datasets,
respectively. † indicates that the corresponding method explicitly
computes pixel-wise motion from images. Numbers are copied
from original papers or citing papers. We put a dash if it is not
presented in the papers.

Method

Last frame copy

PredNet [19]

DM-GAN† [16]

BeyondMSE [24]

ContextVP [2]

MCnet+RES† [31]

EpicFlow† [27]

DVF† [36]

Ours

Caltech pedestrian

UCF101

MSE PSNR SSIM MSE PSNR SSIM

0.779

4.09

30.2

0.89

7.95

2.42

2.41

3.26

1.94

-

-

-

23.3

27.6

-

-

28.7

-

-

-

0.905

0.899

0.881

0.921

-

-

-

-

-

-

-

-

-

-

1.61

29.2

0.919

1.37

-

-

32

34.9

31

31.6

33.4

35.0

-

-

0.92

0.92

0.91

0.93

0.94

0.94

Dataset

Method

MSE

PSNR

SSIM

CUHK Avenue

ShanghaiTech

Liu et al. † [17]

Ours

Liu et al. † [17]

Ours

0.51
0.39
0.93
0.64

34.8
35.2
31.4
34.1

0.98
0.98
0.97
0.97

Table 2. Quantitative evaluation of the proposed method accord-
ing to different lengths of input sequences. We differentiated the
length of input from 2 to 10 and computed prediction errors using
the Caltech pedestrian dataset trained on the KITTI dataset.

# of images

2

4

6

8

10

PSNR

SSIM

29.167 29.222 29.006 28.940 29.009

0.9193 0.9189 0.9208 0.9197 0.9189

testing, it took about 23ms to predict a single frame on a
single GPU.

4.3. Quantitative and qualitative evaluation

For quantitative evaluation, we use three metrics, mean
squared error (MSE), structural similarity square error
(SSIM), and peak signal to noise ratio (PSNR), that are fre-
quently used for video prediction. Lower is better for MSE
and higher is better for PSNR and SSIM.

Table 1 describes the quantitative evaluation result of the
state-of-the-art methods and the proposed method, with var-
ious datasets. The Caltech dataset is the most challenging
dataset due to the fast motion of a camera, therefore, the er-
rors tend to be high compared to other datasets. To deal with
abrupt camera motions, PredNet [19] and ContextVP [2]
took ten frames as input for this dataset whereas we used
four images as input. Nevertheless, our method shows the
best results in terms of MSE and PSNR and a couple of
predicted images are shown in Fig. 4.

1815

(a) PredNet [19]

(b) ContextVP∗ [2]

(c) Ours

(d) Ground Truth

(e) BeyondMSE [24]

(f) MCnet [31]

(g) ContextVP∗ [2]

(h) Ours

(i) Ground Truth

(j) Liu et al. [17]

(k) Ours

(l) Ground Truth

(m) Liu et al. [17]

(n) Ours

(o) Ground Truth

Figure 4. Qualitative comparisons of the predicted frame on the Caltech Pedestrian (a-d), UCF101 test set (e-i), CUHK Avenue test set,
and ShanghaiTech test set (j-o). Each row shows the prediction results from consecutive sequence and network trained on the according to
the dataset. Our method less artifact and blur around the ambiguity region that occur with fast motion, and denote the remarkable region in
color. (∗) This result is provided by ContextVP [2].

1816

For UCF101 dataset, we compare ﬁve state-of-the-art
methods. As in BeyondMSE [24], we exclude pixels in
static regions in computing errors. For this dataset, many
papers explicitly compute pixel-wise motion,
i.e. MC-
net [31], EpicFlow [27], and DVF [36]. However, the per-
formance of prediction is lower than ContextVP [2] and the
proposed method, that directly generate future frames from
an input sequence. For the surveillance datasets, we com-
pare the proposed method with the method of Liu et al. [17].
Here, the average accuracy is higher than other datasets,
because the surveillance video contains a large number of
static regions. Figure 4 compares a few results of Liu et al.
and ours, where the method of [17] shows unexpected arti-
facts because of the failure of motion estimation for pixels
undergo large motions.

In addition, we also evaluate the sensitivity to the num-
ber of input frames. The optimal length of input sequences
is four and six in terms of PSNR and SSIM, respectively.
There is no big difference according to the number of in-
put images as shown in Table 2; however, it is interesting
to see that the use of two images showed better results than
using eight or ten images. We presume that the use of two
images is adequate for predicting the next frame in most
cases, as long as a sufﬁcient amount of training data is used
for training. Hence, the larger number of input is desirable
for long-term prediction.

4.4. Multi step prediction evaluation

The multi-step prediction experiment is carried out to see
how far the proposed method can predict future frames, e.g.,
ﬁfteen frames later. The procedure of this experiment is as
follows. First, we predict the next frame from an input se-
quence, i.e. four consecutive images. Then, we construct
a new sequence by concatenating the last three frames of
the input sequence and the predicted frame. Then, the new
sequence is used to predict the next frame, this procedure
is repeated until the designated frame, e.g., ﬁfteen frames
ahead, is predicted. This experiment was frequently adopted
to verify the temporal and spatial consistency of predicted
frames [16, 24, 36]. Table 3 shows quantitative evaluation
results. Though the errors of predicted images increase as
we predict farther future, the proposed method consistently
shows better results than PredNet [19], which takes ten im-
ages as input. Qualitatively, the proposed method tends to
show distorted images as shown in Fig. 5. However, pre-
dicted images do not suffer from blurry artifacts while cap-
turing important characteristics of future frames, e.g. lanes
and cast shadow. These experiments verify that the pro-
posed network architecture is good at predicting far future
frames, with the aid of retrospective cycle constraints and
multiple discriminators.

Table 3. A quantitative comparison of multi-step prediction results
with PredNet [19] and the proposed method. T indicates the time
step, e.g., if T is 1 then the predicted frame corresponds to the im-
age at 1 time steps ahead. The performance of prediction gradually
decreases as T increases.

Method

T = 1

3

6

9

12

15

PredNet [19]

PSNR

27.6

21.7

20.3

19.1

18.3

17.5

SSIM 0.90

0.72

0.66

0.61

0.58

0.54

Ours

PSNR

29.2

25.9

22.3

20.5

19.3

18.4

SSIM 0.91

0.83

0.73

0.67

0.63

0.60

Table 4. An ablation study of the proposed method with various
loss conﬁgurations. ✔ and ✘ indicate that whether the correspond-
ing part, e.g. a discriminator, is used or not for training the net-
work. Forward and Backward with or without the retrospective
loss (w/ res. or w/o res.)

Forward

Backward

(w/o res.)

(w/ res.)

(w/o res.)

(w/ res.)

Limage LLoG Lframe

adv

L

seq
adv PSNR SSIM

✔

✔

✔

✔

✔

✔

✘

✘

✔

✔

✔

✔

✘

✔

✘

✔

✔

✔

✘

✘

✘

✔

✘

✔

✔

✔

✔

✔

✔

✔

✘

✔

✔

✔

✔

✔

✘

✘

✘

✘

✔

✔

✘

✘

✘

✘

✘

✔

26.3
26.8
26.9
27.5
28.4
29.2

0.892
0.899
0.900
0.904
0.912
0.919

4.5. Ablation study

We carried out an ablation study under various settings,
to see the impact of core ideas such as backward predic-
tion, frame discriminator, and sequence discriminator. Ta-
ble 4 compares quantitative results with different settings,
in the ascending order of PSNR from the top to the bottom
row. Overall, the absence of each module degraded the per-
formance of predicting future frames. It is important that
the absence of backward prediction implies that all the loss
terms related to the backward prediction are eliminated dur-
ing training; it reduces the number of input images into half
for the discriminators. Two different settings, forward pre-
diction with the frame discriminator and bi-directional pre-
diction with the frame discriminator, show near the state-
of-the-art performance. The use of all components, the pro-
posed method shows the best performing result, meaning
that the combination of all components is crucial for the
prediction of future frames.

5. Conclusion

We have proposed an unsupervised framework for pre-
dicting future frames, named as Retrospective Cycle GAN,
consisting of one generator and two discriminators. The
generator takes forward and backward sequences as input
during training and the consistency of bi-directional predic-
tion is leveraged through the retrospective cycle constraints.
In addition, we exploited two discriminators for adversarial

1817

Figure 5. A comparison of multi-step prediction results. The second results of each image (w/o res. cons.) represent without retrospective
constraint. The ﬁrst sequence is captured by a forward moving vehicle while changing the lane and the second sequence contains a cast
shadow which is going to dominate the entire road. The proposed method can predict the important characteristics of future frames; for
example, the position of cars and lane marking as well as the area of cast shadows. More results can be found in the supplementary material.

training, the frame discriminator is for discriminating fake
frames likewise conventional GANs. The sequence discrim-
inator takes fake contained sequences to improve the robust-
ness and accuracy of predicted frames over time under the
temporal consistency. We experimentally veriﬁed the supe-
riority of the proposed method from various perspectives,
showing the state-of-the-art performance in predicting fu-
ture frames.

Acknowledgement. This work was partially supported by the Na-
tional Research Foundation of Korea(NRF) grant funded by the
Korea government(MSIT) (No.NRF-2019R1C1C1003676) and by
“The Cross-Ministry Giga KOREA Project” grant funded by the
Korea government(MSIT) (No.GK19P0200, Development of 4D
reconstruction and dynamic deformable action model based hyper-
realistic service technology). We would also like to thank Ju Hong
Yoon and Yeong Won Kim for many helpful comments and dis-
cussions.

1818

References

[1] Martin Arjovsky, Soumith Chintala, and Léon Bottou.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017. 3

[2] Wonmin Byeon, Qin Wang, Rupesh Kumar Srivastava, and
Petros Koumoutsakos. Contextvp: Fully context-aware video
prediction.
In European Conference on Computer Vision
(ECCV), 2018. 1, 2, 3, 5, 6, 7

[3] Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep
generative image models using a laplacian pyramid of adver-
sarial networks. In Advances in neural information process-
ing systems (NIPS), 2015. 4

[4] Andrea Vedaldi Dmitry Ulyanov and Victor S. Lempitsky.
Instance normalization: The missing ingredient for fast styl-
ization. CoRR, abs/1607.08022, 2016. 5

[5] Piotr Dollár, Christian Wojek, Bernt Schiele, and Pietro Per-
ona. Pedestrian detection: An evaluation of the state of the
art.
In Pattern Analysis and Machine Intelligence (PAMI),
2012. 1, 5

[6] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsuper-
vised learning for physical interaction through video predic-
tion. In Advances in neural information processing systems
(NIPS), 2016. 1, 2, 3

[7] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. In Interna-
tional Journal of Robotics Research (IJRR), 2013. 5

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
neural information processing systems (NIPS), 2014. 3

[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In conference
on computer vision and pattern recognition (CVPR), 2016. 4

[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adversar-
ial networks. In Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 3

[11] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V
Gool. Dynamic ﬁlter networks. In Advances in Neural In-
formation Processing Systems (NIPS), 2016. 1

[12] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution.
In
European Conference on Computer Vision (ECCV), 2016. 3,
4

[13] Nal Kalchbrenner, Aäron Oord, Karen Simonyan, Ivo Dani-
helka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu.
Video pixel networks. In International Conference on Ma-
chine Learning (ICML), 2017. 1, 2

[14] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 5

[15] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network.
In Conference on Computer Vision and
Pattern Recognition (CVPR), 2017. 1, 3

[16] Xiaodan Liang, Lisa Lee, Wei Dai, and Eric P Xing. Dual
motion gan for future-ﬂow embedded video prediction.
In
International Conference on Computer Vision (ICCV), 2017.
1, 2, 3, 5, 7

[17] W. Liu, D. Lian W. Luo, and S. Gao. Future frame prediction
for anomaly detection – a new baseline. In Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 1,
3, 5, 6, 7

[18] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In
convolutional networks for semantic segmentation.
Conference on Computer Vision and Pattern Recognition
(CVPR), 2015. 1, 3

[19] William Lotter, Gabriel Kreiman, and David Cox. Deep
predictive coding networks for video prediction and unsu-
pervised learning. In International Conference on Learning
Representations (ICLR), 2017. 1, 2, 5, 6, 7

[20] Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse
coding based anomaly detection in stacked rnn framework.
In International Conference on Computer Vision (ICCV),
2017. 5

[21] Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, and
Li Fei-Fei. Unsupervised learning of long-term motion dy-
namics for videos. In Conference on Computer Vision and
Pattern Recognition (CVPR). 3

[22] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares generative
adversarial networks. In International Conference on Com-
puter Vision (ICCV), 2017. 3, 4

[23] David Marr and Ellen Hildreth. Theory of edge detection.

Proc. R. Soc. Lond. B, 1980. 4

[24] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep
multi-scale video prediction beyond mean square error. In In-
ternational Conference on Learning Representations (ICLR),
2016. 1, 3, 5, 6, 7

[25] Viorica P˘atr˘aucean, Ankur Handa, and Roberto Cipolla.
Spatio-temporal video autoencoder with differentiable mem-
ory.
In International Conference on Learning Representa-
tions (ICLR) Workshop, 2016. 1

[26] Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lu-
cio Marcenaro, Carlo Regazzoni, and Nicu Sebe. Abnormal
event detection in videos using generative adversarial nets. In
International Conference on Image Processing (ICIP), 2017.
5

[27] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and
Cordelia Schmid. Epicﬂow: Edge-preserving interpolation
of correspondences for optical ﬂow. In Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2015. 5, 7

[28] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402, 2012. 5

[29] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
Kautz. Mocogan: Decomposing motion and content for
video generation.
In Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 1

[30] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt,
Oriol Vinyals, Alex Graves, et al. Conditional image gen-
eration with pixelcnn decoders. In Advances in Neural In-
formation Processing Systems (NIPS), 2016. 2

1819

[31] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin,
and Honglak Lee. Decomposing motion and content for nat-
ural video sequence prediction. In International Conference
on Learning Representations (ICLR), 2017. 1, 3, 5, 6, 7

[32] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
In Advances In

Generating videos with scene dynamics.
Neural Information Processing Systems (NIPS), 2016. 1

[33] Tianfan Xue, Jiajun Wu, Katherine Bouman, and Bill Free-
man. Visual dynamics: Probabilistic future frame synthesis
via cross convolutional networks. In Advances in Neural In-
formation Processing Systems (NIPS), 2016. 2, 3

[34] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan:
Unsupervised dual learning for image-to-image translation.
In International Conference on Computer Vision (ICCV),
2017. 2, 3

[35] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In International Conference
on Computer Vision (ICCV), 2017. 2, 3

[36] Xiaoou Tang Yiming Liu Ziwei Liu, Raymond Yeh and
Aseem Agarwala. Video frame synthesis using deep voxel
ﬂow.
In International Conference on Computer Vision
(ICCV), 2017. 5, 7

1820

