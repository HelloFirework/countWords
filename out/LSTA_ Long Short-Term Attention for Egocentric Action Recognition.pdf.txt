LSTA: Long Short-Term Attention for Egocentric Action Recognition

Swathikiran Sudhakaran1,2, Sergio Escalera3,4, Oswald Lanz1

1Fondazione Bruno Kessler, Trento, Italy

2University of Trento, Trento, Italy

3Computer Vision Center, Barcelona, Spain
4Universitat de Barcelona, Barcelona, Spain

{sudhakaran,lanz}@fbk.eu, sergio@maia.ub.es

Abstract

Egocentric activity recognition is one of the most chal-
lenging tasks in video analysis. It requires a ﬁne-grained
discrimination of small objects and their manipulation.
While some methods base on strong supervision and atten-
tion mechanisms, they are either annotation consuming or
do not take spatio-temporal patterns into account. In this
paper we propose LSTA as a mechanism to focus on features
from relevant spatial parts while attention is being tracked
smoothly across the video sequence. We demonstrate the ef-
fectiveness of LSTA on egocentric activity recognition with
an end-to-end trainable two-stream architecture, achieving
state-of-the-art performance on four standard benchmarks.

1. Introduction

Recognizing human actions from videos is a widely
studied problem in computer vision. Most research is de-
voted to the analysis of videos captured from distant, third-
person views. Egocentric (ﬁrst-person) video analysis is an
important and relatively less explored branch with poten-
tial applications in robotics, indexing and retrieval, human-
computer interaction, or human assistance, just to mention
a few. Recent advances in deep learning highly beneﬁted
problems such as image classiﬁcation [12, 39] and object
detection [19, 11]. However, the performance of deep learn-
ing action recognition from videos is still not comparable to
the advances made in object recognition from still images
[12]. One of the main difﬁculties in action recognition is
the huge variations present in the data caused by the highly
articulated nature of the human body. Human kinesics, be-
ing highly ﬂexible in nature, results in high intra-subject and
low inter-subject variabilities. This is further challenged by
the variations introduced by the unconstrained nature of the
environment where the video is captured. Since videos are
composed of image frames, this introduces an additional di-
mension to the data, making it more difﬁcult to deﬁne a
model that properly focuses on the regions of interest that

better discriminate particular action classes. In order to mit-
igate these problems, one approach could be the design of a
large scale dataset with ﬁne-grain annotations covering the
space of spatio-temporal variabilities deﬁned by the prob-
lem domain, which would be unfeasible in practice.

Here, we consider the problem of identifying ﬁne-
grained egocentric activities from trimmed videos. This is
a comparatively difﬁcult task considered to action recogni-
tion since the activity class depends on the action and the
object on to which the action is applied to. This requires
the development of a method that can simultaneously rec-
ognize the action as well as the object.
In addition, the
presence of strong ego-motion caused by the sharp move-
ments of the camera wearer introduces noise to the video
that complicates the encoding of motion in the video frame.
While incorporating object detection can help the task of
egocentric action recognition, still this would require ﬁne-
grain frame level annotations, becoming costly and imprac-
tical in a large scale setup.

Attention in deep learning was recently proposed to
guide networks to focus on regions of interest relevant for a
particular recognition task. This prunes the network search
space and avoids computing features from irrelevant im-
age regions, resulting in a better generalization. Existing
works explore both bottom-up [41] and top-down attention
mechanisms [32]. Bottom-up attention relies on the salient
features of the data and is trained to identify such visual
patterns that distinguish one class from another. Top-down
attention applies prior knowledge about the data for devel-
oping attention, e.g. the presence of certain objects which
can be obtained from a network trained for a different task.
Recently, attention mechanisms have been successfully ap-
plied to egocentric action recognition [15, 32], surpassing
the performance of non-attentive alternatives. Still, very
few attempts have been done to track attention into spatio-
temporal egocentric action recognition data. As a result,
current models may lose a proper smooth tracking of atten-
tion regions in egocentric action videos. Furthermore, most

9954

current models base on separate pre-training with strong su-
pervision, requiring complex annotation operations.

To address these limitations, in this work we investigate
on the more general question of how a video CNN-RNN
can learn to focus on the regions of interest to better dis-
criminate the action classes. We analyze the shortcomings
of LSTMs in this context and derive Long Short-Term At-
tention (LSTA), a new recurrent neural unit that augments
LSTM with built-in spatial attention and a revised output
gating. The ﬁrst enables LSTA to attend the feature regions
of interest while the second constraints it to expose a dis-
tilled view of internal memory. Our study conﬁrms that it is
effective to improve the output gating of recurrent unit since
it does not only affect prediction overall but controls the re-
currence, being responsible for a smooth and focused track-
ing of the latent memory state across the sequence. Our
main contributions can be summarized as follows:

• We present Long Short-Term Attention (LSTA), a new
recurrent unit that addresses shortcomings of LSTM
when the discriminative information in the input se-
quence can be spatially localized;

• We deploy LSTA into a two stream architecture with
cross-modal fusion, a novel control of the bias param-
eter of one modality by using the other1;

• We report an ablation analysis of the model and eval-
uate it on egocentric activity recognition, providing
state-of-the-art results in four public datasets.

2. Related Work

We discuss the most relevant deep learning methods for

addressing egocentric vision problems in this section.

2.1. First Person Action Recognition

The works of [21, 30, 43] train specialized CNN for hand
segmentation and object localization related to the activi-
ties to be recognized. These methods base on specialized
pre-training for hand segmentation and object detection net-
works, requiring high amounts of annotated data for that
purpose. Additionally, they just base on single RGB images
for encoding appearance without considering temporal in-
formation. In [24, 40] features are extracted from a series
of frames to perform temporal pooling with different oper-
ations, including max pooling, sum pooling, or histogram
of gradients. Then, a temporal pyramid structure allows the
encoding of both long term and short term characteristics.
However, all these methods do not take into consideration
the temporal order of the frames. Techniques that use a
recurrent neural network such as Long Short-Term Mem-
ory (LSTM) [2, 36] and Convolutional Long Short-Term
Memory (ConvLSTM) [31, 32] are proposed to encode the

1Code is available at https://github.com/swathikirans/LSTA

temporal order of features extracted from a sequence of
frames. Sigurdsson et al. [28] proposes a triplet network
to develop a joint representation of paired third person and
ﬁrst person videos. Their method can be used for trans-
ferring knowledge from third person domain to ﬁrst per-
son domain thereby partially solving the problem of lack
of large ﬁrst person datasets. Tang et al. [34, 35] add an
additional stream that accepts depth maps to the two stream
networkenabling it to encode 3D information present in the
scene. Li et al. [15] propose a deep neural network to jointly
predict the gaze and action from ﬁrst person videos, which
requires gaze information during training.

Majority of the state-of-the-art techniques rely on addi-
tional annotations such as hand segmentation, object bound-
ing box or gaze information. This allows the network to
concentrate on the relevant regions in the frame and helps in
distinguishing each activity from one another better. How-
ever, manually annotating all the frames of a video with
these information is impractical. For this reason, develop-
ment of techniques that can identify the relevant regions of
a frame without using additional annotations is crucial.

2.2. Attention

Attention mechanism was proposed for focusing atten-
tion on features that are relevant for the task to be recog-
nized. This includes [32, 15, 26] for ﬁrst person action
recognition, [1, 20, 37] for image and video captioning and
[22, 1, 18] for visual question answering. The works of
[25, 10, 33, 32, 41, 15] use an attention mechanism for
weighting spatial regions that are representative for a par-
ticular task. Sharma et al. [25] and Zhang et al. [41] gener-
ate attention masks implicitly by training the network with
video labels. Authors of [10, 33, 32] use top-down attention
generated from the prior information encoded in a CNN pre-
trained for object recognition while [15] uses gaze informa-
tion for generating attention. The work of [23, 26] uses at-
tention for weighting relevant frames, thereby adding tem-
poral attention. This is based on the idea that not all frames
present in a video are equally important for understanding
the action being carried out. In [23] a series of temporal
attention ﬁlters is learnt that weight frame level features de-
pending on their relevance for identifying actions. [26] uses
change in gaze for generating the temporal attention. [17, 5]
apply attention on both spatial and temporal dimensions to
select relevant frames and the regions present in them.

Most existing techniques for generating spatial attention
in videos consider each frame independently. Since video
frame sequences have an absolute temporal consistency, per
frame processing results in the loss of valuable information.

2.3. Relation to state of the art alternatives

The proposed LSTA method generates the spatial atten-
tion map in a top-down fashion utilizing prior information

9955

encoded in a CNN pre-trained for object recognition and
another pre-trained for action recognition.
[32] proposes
a similar top-down attention mechanism. However, they
generate the attention map independently in each frame
whereas in the proposed approach, the attention map is gen-
erated in a sequential manner. This is achieved by propa-
gating the attention map generated from past frames across
time by maintaining an internal state for attention. Our
method uses attention on the motion stream followed by a
cross-modal fusion of the appearance and motion streams,
thereby enabling both streams to interact earlier in the lay-
ers to facilitate ﬂow of information between them.
[41]
proposes an attention mechanism that takes in to consid-
eration the inputs from past frames. Their method is based
on bottom-up attention and generates a single weight ma-
trix which is trained with the video level label. However,
the proposed method generates attention, based on the in-
put, from a pool of attention maps which are learned using
video level label alone.

3. Analysis of LSTM

LSTM is the widely adopted neuron design for process-
ing and/or predicting sequences. A latent memory state ct is
tracked across a sequence with a forget-update mechanism

ct = f ⊙ ct−1 + i ⊙ c

(1)

where (f, i) have a gating function on the previous state
ct−1 and an innovation term c. (f, i, c) are parametric func-
tions of input xt and a gated non-linear view of previous
memory state ot−1 ⊙ η(ct−1)

(i, f, ot, c) = (σ, σ, σ, η)(W [xt, ot−1 ⊙ η(ct−1)])

(2)

The latter, referred to as hidden state ht = ot ⊙η(ct), is of-
ten exposed to realize a sequence prediction. For sequence
classiﬁcation instead, the ﬁnal memory state can be used as
a ﬁxed-length descriptor of the input sequence.

Two features of LSTM design explain its success. First,
the memory update (Eq. 1) is ﬂexibly controlled by (f, i): a
state can, in a single iteration, be erased (0, 0), reset (0, 1),
left unchanged (1, 0), or progressively memorize new input.
(1, 1) resembles residual learning [12], a key design pattern
in very deep networks - depth here translates to sequence
length. Indeed, LSTMs has strong gradient ﬂow and learn
long-term dependencies [13]. Second, the gating functions
(Eq. 2) are learnable neurons and their interaction in mem-
ory updating is transparent (Eq. 1). When applied to video
classiﬁcation, a few limitations are to be discussed:
1. Memory. Standard LSTMs use fully connected neu-
ron gates and consequently, the memory state is unstruc-
tured. This may be desired e.g. for image captioning where
one modality (vision) has to be translated into another (lan-
guage). For video classiﬁcation it might be advantageous

×

σ

σ

η

ct−1

η

ot−1

×

k

RNN

+

s

×

at−1,
st−1

νa

ς

xt

+

×

×

k

νc

ς

ct

σ

ot

at, st

Figure 1: LSTA extends LSTM with two novel components:
recurrent attention and output pooling. The ﬁrst (red part)
tracks a weight map s to focus on relevant features, while
the second (green part) introduces a high-capacity output
gate. At the core of both is a pooling operation ς, that selects
one out of a pool of specialized mappings to realize smooth
attention tracking and ﬂexible output gating. Circles indi-
cate point-wise or concat operations, square blocks are lin-
ear/convolutional parametric nodes with non-linearities in-
dicated by their symbols. Recurrent variables in bold.

to preserve the spatial layout of images and their convo-
lutional features by propagating a memory tensor instead.
ConvLSTM [27] addresses this shortcoming through con-
volutional gates in the LSTM.
2. Attention. The discriminative information is often con-
ﬁned locally in the video frame. Thus, not all convolutional
features are equally important for recognition. In LSTMs
the ﬁltering of irrelevant features (and memory) is deferred
to the gating neurons, that is, to a linear transformation (or
convolution) and a non-linearity. Attention neurons were
introduced to suppress activations from irrelevant features
ahead of gating. We augment LSTM with built-in attention
that directly interacts with the memory tracking in Sec. 4.1.
3. Output gating. Output gating not only impacts sequence
prediction but it critically affects memory tracking too, cf.
Eq 2. We replace the output gating neuron of LSTM with
a high-capacity neuron whose design is inspired by that of
attention. There is indeed a relation among them, we make
this explicit in Sec. 4.2.
4. External bias control. The neurons in Eq. 2 have a
bias term that is learnt from data during training, and it is
ﬁxed at prediction time in standard LSTM. We leverage on
adapting the biases based on the input video for each pre-
diction. State-of-the-art video recognition is realized with
two-stream architectures, we use ﬂow stream to control ap-
pearance biases in Sec. 5.3.

4. Long Short-Term Attention

We present a schematic view of LSTA in Fig. 1. LSTA
extends LSTM [9] with two newly designed components.
The core operation is a pooling ς, that selects one out of a

9956

pool of specialized mappings to realize attention tracking
(red part) and output gating (green part). The pooling
ς on features xt returns a map νa that is fed through a
conventional RNN cell with memory at and output gate
st. Its output state st ⊙ η(at) is added to the input νa and
softmax calibrated to obtain an attention map s. The map s
is then applied to xt, that is, s ⊙ xt is the attention ﬁltered
feature for updating memory state ct using conventional
LSTM recurrence (black part). Our redesigned output
gating uses a ﬁltered view of the updated memory state,
νc ⊙ ct, instead of xt. To obtain νc through pooling
we use s ⊙ xt to control the bias of operator ς, hereby
coupling attention tracking with output gating. This model
is instantiated for action recognition from egocentric video
in its convolutional version as

νa = ς(xt, wa)

(3)

of mappings be learnable and self-consistent, and realized
with fewer tunable parameters.

A selector with parameters w maps an image features
x into a category-score space C from which the category
c∗ ∈ C obtaining the highest score is returned. Our se-
lector is of the form c∗ = arg maxc π(ǫ(x), θc) where ǫ is
a reduction and θc ∈ w are the parameters for scoring x
against category c. If π is chosen to be equivariant to re-
duction ǫ then π(ǫ(x), θc) = ǫ(π(x, θc)) and we can use
{ǫ⊥(π(·, θc)), c ∈ C} as the pool of category-speciﬁc map-
pings associated to ǫ. Here ǫ⊥ denotes the ǫ-orthogonal re-
duction, e.g. if ǫ is max-pooling along one dimension then
ǫ⊥ is max-pooling along the other dimensions. That is, our
pooling model is determined by the triplet

(ς) = (ǫ, π, {θc}) ,

π is ǫ-equivariant

(11)

(ia, fa, st, a) = (σ, σ, σ, η)(Wa ∗ [νa, st−1 ⊙ η(at−1)])(4)

and realized on a feature tensor x by

at = fa ⊙ at−1 + ia ⊙ a

s = softmax(νa + st ⊙ η(at))

(5)

(6)

(ic, fc, c) = (σ, σ, η)(Wc ∗ [s ⊙ xt, ot−1 ⊙ η(ct−1)])(7)

ct = fc ⊙ ct−1 + ic ⊙ c

νc = ς(ct, wc + woǫ(s ⊙ xt))
ot = σ(Wo ∗ [νc ⊙ ct, ot−1 ⊙ η(ct−1)])

(8)

(9)

(10)

Eqs. 3-6 implement our recurrent attention as detailed in
Sec. 4.1, Eqs. 9-10 is our coupled output gating of Sec. 4.2.
Bold symbols represent the recurrent variables: (at, st) of
shape N ×1, (ct, ot) of shape N ×K. Trainable parameters
are: (Wa, Wc) are both K convolution kernels, (wa, wc)
have shape K × C, wo has shape C × C. N, K, C are in-
troduced below. σ, η are sigmoid and tanh activation func-
tions, ∗ is convolution, ⊙ is point-wise multiplication. ς, ǫ
are from the pooling model presented next.

4.1. Attention Pooling

Given a matrix view xik of convolutional feature tensor
x where i indexes one of N spatial locations and k indexes
one of K feature planes, we aim at suppressing those activa-
tions xi that are uncorrelated with the recognition task. That
is, we seek a ς(x, w) of shape 1 × N such that parameters w
can be tuned in a way that ς(x, w)⊙ x are the discriminative
features for recognition. For egocentric activity recognition
these can be from objects, hands, or implicit patterns repre-
senting object-hand interactions during manipulation.

Our design of ς(x, w) is grounded on the assumption that
there is a limited number of pattern categories that are rel-
evant for an activity recognition task. Each category itself
can, however, instantiate patterns with high variability dur-
ing and across executions. We therefore want ς to select
from a pool of category-speciﬁc mappings, based on the
current input x. We want both the selector and the pool

ς(x, {θc}) = ǫ⊥(π(x, θc∗ ))
where c∗ = arg max

π(ǫ(x), θc)

(12)

(13)

c

In our model we choose

ǫ(x) ← spatial average pooling

π(ǫ, θc) ← linear mapping

so ς(x, {θc}) is a differentiable spatial mapping, i.e., we can
use ς as a trainable attention model for x. This is related to
class activation mapping [42] introduced for discriminative
localization. Note however that, in contrast to [42] that uses
strong supervision to train the selector directly, we lever-
age video-level annotation to implicitly learn an attention
mechanism for video classiﬁcation. Our formulation is also
a generalization: other choices are possible for the reduc-
tion ǫ, and the use of differentiable structured layers [14] in
this context are an interesting direction for future work.

To inﬂate attention in LSTA, we introduce a new state
tensor at of shape N × 1. Its update rule is that of stan-
dard LSTM (Eq. 5) with gatings (fa, ia, st) and innova-
tion a computed from the pooled νa = ς(xt, wa) as input
(Eq. 4). We compute the attention tensor s using the hidden
state st ⊙ η(at) as residual (Eq. 6), followed by a softmax
calibration. Eqs. 7-10 implement the LSTA memory update
based on the ﬁltered input s ⊙ xt, this is described next.

4.2. Output Pooling

If we analyze standard LSTM Eq. 2 with input s ⊙ xt
instead of xt, it becomes evident that ot−1 (output gating)
has on ct−1 a same effect as s (attention) has on xt. In-
deed, in Eq. 7 the gatings and innovation are all computed
from [s ⊙ xt, ot−1 ⊙ η(ct−1)]. We build upon this analogy
to enhance the output gating capacity of LSTA and, conse-
quently, its forget-update behavior of memory tracking.

9957

We introduce attention pooling in the output gating up-
date. Instead of computing ot as by Eq. 2 we replace s ⊙ xt
with νc ⊙ ct to obtain update Eqs. 9-10, that is

σ(Wo ∗ [s ⊙ xt, ot−1 ⊙ η(ct−1)]) ← standard

σ(Wo ∗ [νc ⊙ ct, ot−1 ⊙ η(ct−1)])

with νc = ς(ct, wc + woǫ(s ⊙ xt))

gating

← output

pooling

This choice is motivated as follows. We want to pre-
serve the recursive nature of output gating, which is we
keep right-concatenating ot−1 ⊙ η(ct−1) to obtain the
2N × K-shaped tensor to convolve and tanh point-wise.
Since the new memory state ct is available at this stage,
which already integrates s ⊙ xt, we can use this for left-
concatenating instead of the raw attention-pooled input ten-
sor. This is similar to a peephole connection in the output
gate [8]. We can even produce a ﬁltered version νc ⊙ ct
of it if we introduce a second attention pooling neuron for
localizing the actual discriminative memory component of
ct, that is via νc, Eq. 9. Note that ct integrates informa-
tion from past memory updates by design, so localizing cur-
rent activations is pretty much required here. Consequently,
and in contrast to feature tensors xt, the memory activations
might not be well localized spatially. We thus use a slightly
different version of Eq. 12 for output pooling, we remove
ǫ⊥ to obtain a full-rank N × K-shaped attention tensor νc.
To further enhance active memory localization, we use
s ⊙ xt to control the bias term of attention pooling, Eq. 9.
We apply a reduction ǫ(s ⊙ xt) followed by a linear regres-
sion with learnable parameters wo to obtain the instance-
speciﬁc bias woǫ(s⊙xt) for activation mapping. Note that ǫ
is the reduction associated to ς so this is consistent. We will
use a similar idea in Sec. 5.3 for cross-modal fusion in two-
stream architecture. Our ablation study in Sec. 6.4 conﬁrms
that this further coupling of ct with xt boosts the mem-
ory distillation in the LSTA recursion, and consequently its
tracking capability, by a signiﬁcant margin.

5. Two Stream Architecture

In this section, we explain our network architecture
for egocentric activity recognition incorporating the LSTA
module of Sec. 4. Like the majority of the deep learning
methods proposed for action recognition, we also follow the
two stream architecture; one stream for encoding appear-
ance information from RGB frames and the second stream
for encoding motion information from optical ﬂow stacks.

5.1. Attention on Appearance Stream

The network consists of a ResNet-34 pre-trained on im-
ageNet for image recognition. We use the output of the last
convolution layer of block conv5_3 of ResNet-34 as the
input of the LSTA module. From this frame level features,

LSTA generates the attention map which is used to weight
the input features. We select 512 as the depth of LSTA
memory and all the gates use a kernel size of 3 × 3. We
use the internal state (ct) for classiﬁcation.

We follow a two stage training.

In the ﬁrst stage, the
classiﬁer and the LSTA modules are trained while in the
second stage, the convolutional layers in the ﬁnal block
(conv5_x) and the FC layer of ResNet-34 along with the
layers trained in stage 1 are trained.

5.2. Attention on Motion Stream

We use a network trained on optical ﬂow stacks for ex-
plicit motion encoding. For this, we use a ResNet-34 CNN.
The network is ﬁrst trained on action verbs (take, put, pour,
open, etc.) using an optical ﬂow stack of 5 frames. We
average the weights in the input convolutional layer of an
imagenet pre-trained network and replicate it 10 times to
initialize the input layer. This is analogous to the ima-
geNet pre-training done on the appearance stream. The
network is then trained for activity recognition as follows.
We use the action-pretrained ResNet-34 FC weights as the
parameter initialization of attention pooling (Eqs. 12-13)
on conv5_3 ﬂow features. We use this attention map to
weight the features for classiﬁcation. Since the activities are
temporally located in the videos and they are not sequential
in nature, we take the optical ﬂow corresponding to the ﬁve
frames located in the temporal center of the videos.

5.3. Cross modal Fusion

Majority of the existing methods with two stream archi-
tecture perform a simple late fusion by averaging for com-
bining the outputs from the appearance and motion streams
[29, 38]. Feichtenhofer et al. [7] propose a pooling strat-
egy at the output of the ﬁnal convolutional layer for im-
proved fusion of the two streams. In [6] the authors observe
that adding a residual connection from the motion stream to
the appearance stream enables the network to improve the
joint modeling of the information ﬂowing through the two
streams. Inspired by the aforementioned observations, we
propose a novel cross-modal fusion strategy in the earlier
layers of the network in order to facilitate the ﬂow of infor-
mation across the two modalities.

In the proposed cross-modal fusion approach, each
stream is used to control the biases of the other as follows.
To perform cross-modal fusion on the appearance stream,
the ﬂow feature from the conv5_3 of the motion stream
CNN is applied as bias to the gates of the LSTA layer. To
perform cross-modal fusion on the motion stream instead,
the sequence of features from the conv5_3 of the RGB
stream CNN are 3D convolved into a summary feature. We
add a ConvLSTM cell of memory size 512 in the motion
stream as an embedding layer and use the RGB summary
feature to control the bias of the ConvLSTM gates.

9958

In this way, each individual stream is made to inﬂuence
the encoding of the other so that we have a ﬂow of informa-
tion between them deep inside the neural network. We then
perform a late average fusion of the two individual streams’
output to obtain the class scores.

6. Experiments and Results

6.1. Datasets

We evaluate the proposed method on four standard ﬁrst
person activity recognition datasets namely, GTEA 61,
GTEA 71, EGTEA Gaze+ and EPIC-KITCHENS. GTEA
61 and GTEA 71 are relatively small scale datasets with
61 and 71 activity classes respectively. EGTEA Gaze+
is a recently developed large scale dataset with approxi-
mately 10K samples having 106 activity classes. EPIC-
KITCHENS dataset
is the largest egocentric activities
dataset available now. The dataset consists of more than
28K video samples with 125 verb and 352 noun classes.

6.2. Experimental Settings

The appearance and motion networks are ﬁrst trained
separately followed by a combined training of the two
stream cross-modal fusion network. We train the networks
for minimizing the cross-entropy loss. The appearance
stream is trained for 200 epochs in stage 1 with a learning
rate of 0.001 which is decayed after 25, 75 and 150 epochs
at a rate of 0.1. In the second stage, the network is trained
with a learning rate of 0.0001 for 100 epochs. The learn-
ing rate is decayed by 0.1 after 25 and 75 epochs. We use
ADAM as the optimization algorithm. 25 frames uniformly
sampled from the videos are used as input. The number
of classes used in the output pooling (wc in 4.2) is chosen
as 100 for GTEA 61 and GTEA 71 datasets after empiri-
cal evaluation on the ﬁxed split of GTEA 61. For EGTEA
Gaze+ and EPIC-KITCHENS datasets, the value is scaled
to 150 and 300 respectively, in accordance with the relative
increase in the number of activity classes.

For the pre-training of the motion stream on action clas-
siﬁcation task, we use a learning rate of 0.01 which is re-
duced by 0.5 after 75, 150, 250 and 500 epochs and is
trained for 700 epochs. In the activity classiﬁcation stage,
we train the network for 500 epochs with a learning rate of
0.01. The learning rate is decayed after 50 and 100 epochs
by 0.5. SGD algorithm is used for optimizing the parameter
updates of the network.

The two stream network is trained for 200 epochs for
GTEA 61 and GTEA 71 datasets while EGTEA is trained
till 100 epochs, with a learning rate of 0.01 using ADAM al-
gorithm. Learning rate is reduced by 0.99 after each epoch.
We use a batch size of 32 for all networks. We use random
horizontal ﬂipping and multi-scale corner cropping tech-
niques proposed in [38] during training and the center crop
of the frame is used during inference.

Ablation

Accuracy (%)

Baseline
Baseline + output pooling
Baseline + attention pooling
Baseline + pooling
LSTA

LSTA two stream late fusion
LSTA two stream cross-modal fusion

51.72
62.07
66.38
68.1
74.14

78.45
79.31

Table 1: Ablation analysis on GTEA 61 ﬁxed split.

6.3. Ablation Study

An extensive ablation analysis2 has been carried out, on
the ﬁxed split of GTEA 61 dataset, to determine the perfor-
mance improvement obtained by each component of LSTA.
The results are shown in Tab. 1, which compares the perfor-
mance of RGB and two stream networks on the top and bot-
tom sections respectively. We choose a network with vanilla
ConvLSTM as the baseline since LSTA without attention
and output pooling converges to the standard ConvLSTM.
The baseline model results in an accuracy of 51.72%. We
then analyze the impact of each of the contributions ex-
plained in Sec 4. We ﬁrst analyze the effect of output pool-
ing on the baseline. By adding output pooling the perfor-
mance is improved by 8%. We analyzed the classes that are
improved by adding output pooling over the baseline model
and observe that the major improvement is achieved by pre-
dicting the correct action classes. Output pooling enables
the network to propagate a ﬁltered a version of the memory
which is localized on the most discriminative components.
Adding attention pooling to the baseline improves the
performance by 14%. Attention pooling enables the net-
work to identify the relevant regions in the input frame and
to maintain a history of the relevant regions seen in the past
frames. This enables the network to have a smoother track-
ing of attentive regions. Detailed analysis show that atten-
tion pooling enables the network to correctly classify ac-
tivities with multiple objects. It should be noted that this
is equivalent to a network with two ConvLSTMs, one for
attention tracking and one for frame level feature tracking.
Incorporating both attention and output pooling to the
baseline results in a gain of 16%. By analyzing the top im-
proved classes, we found that the model has increased its
capacity to correctly classify both actions and objects. By
adding bias control, as explained in Sec. 4, we obtain the
proposed LSTA model and gains an additional improvement
of 6% in recognition accuracy.

Compared to the network with the vanilla ConvLSTM,
LSTA achieves an improvement of 22%. From the previous
analyses we have seen the importance of attention pooling
and output pooling present in LSTA. This enables the net-
work to focus on encoding the features more relevant for

2Detailed analysis available in the supplementary document.

9959

Method

eleGAtt [41]
ego-rnn [32]
LSTA

ego-rnn two stream [32]
LSTA two stream

Accuracy (%)

59.48
63.79
74.14

77.59
79.31

Table 2: Comparative analysis on GTEA 61 ﬁxed split.

the concrete classiﬁcation task. Detailed analysis shows
ConvLSTM confuses with both activities involving same
action with different objects as well as activities consist-
ing of different action with same objects. With the attention
mechanism, LSTA weights the most discriminant features,
thereby allowing the network to distinguish between the dif-
ferent activity classes.

We also evaluated the performance improvement
achieved by applying attention to the motion stream. The
baseline is a ResNet-34 pre-trained on actions followed by
training for activities. We obtained an accuracy of 40.52%
for the network with attention compared to the 36.21% of
the baseline. Fig. 2 (fourth row) visualizes the attention map
generated by the network. For visualization, we overlay
the resized attention map on the RGB frames correspond-
ing to the optical ﬂow stack used as input. From the ﬁg-
ure, it can be seen that the network generates the attention
map around/near the hands, where the discriminant motion
is occurring, thereby enabling the network to recognize the
activity undertaken by the user.
It can also be seen that
the attention maps generated by the appearance stream and
the ﬂow stream are complementary to each other; appear-
ance stream focuses on the object regions while the mo-
tion stream focuses on hand regions. We also analyzed the
classes where the network with attention performs better
compared to the standard ﬂow network and found that the
network with attention is able to recognize actions better
than the standard network. This is because the attention
mechanism enables the network to focus on regions where
motion is occurring in the frame.

Next we compare the performance of the cross-modal
fusion technique explained in Sec. 5.3 over traditional late
fusion two stream approach. The cross-modal fusion ap-
proach improves by 1% over late fusion. Analysis shows
that the cross-modal fusion approach is able to correctly
identify activities with same objects. The ﬁfth and sixth
rows of Fig. 2 visualize the attention maps generated after
cross-modal fusion training. It can be seen that the motion
stream attention expands to regions containing objects. This
validates the effect of cross-modal fusion where the two net-
works are made to interact deep inside the network.

6.4. Comparative Analysis

In this section, we compare the performance of LSTA
over two closely related methods, namely, eleGAtt [41] and

ego-rnn [32]. Results are shown in Tab. 2. EleGAtt is an
attention mechanism which can be applied to any generic
RNN using its hidden state for generating the attention map.
We evaluated eleGAtt on LSTM, consisting of 512 hidden
units, with the same training setting as LSTA for fair com-
parison. EleGAtt learns a single weight matrix for gener-
ating the attention map irrespective of the input whereas
LSTA generates the attention map from a pool of weights
which are selected in a top-down manner based on input.
This enables the selection of a proper attention map for each
input activity class. This leads to a performance gain of
13% over eleGAtt. Analyzing the classes with the highest
improvement by LSTA compared to eleGAtt reveals that el-
eGAtt fails in identifying the object while correctly classi-
fying the action. Ego-rnn [32] derives an attention map gen-
erated from class activation map to weight the discriminant
regions in the image which are then applied to a ConvLSTM
cell for temporal encoding.
It generates a per frame at-
tention map which has no dependency on the information
present in the previous frames. This can result in select-
ing different objects in adjacent frames. On the contrary,
LSTA uses an attention memory to track the previous atten-
tion maps enabling their smooth tracking. This results in
a 10% improvement obtained by LSTA over ego-rnn. De-
tailed analysis on the classiﬁcation results show that ego-rnn
struggles to classify activities involving multiple objects.
Since the attention map generated in each frame is indepen-
dent of the previous frames, the network fails to track previ-
ously activated regions, thereby resulting in wrong predic-
tions. This is further illustrated by visualizing the attention
maps produced by ego-rnn and LSTA in Fig. 2. From the
ﬁgure, one can see that ego-rnn (second row) fails to iden-
tify the relevant object in the case of close chocolate exam-
ple and it failed to track the object in the ﬁnal frames in the
case of the scoop coffee example. LSTA with cross-modal
fusion performs 2% better than ego-rnn two stream.

6.5. State of the art comparison

Our approach is compared against the state-of-the-art
methods on Tab. 3. The methods listed in the ﬁrst sec-
tion of the table uses strong supervision signals such as gaze
[16, 15], hand segmentation [21] or object bounding boxes
[21] during the training stage. Two stream [29], I3D [3] and
TSN [38] are methods proposed for action recognition from
third person videos while all other methods except eleGAtt
[41] are proposed for ﬁrst-person activity recognition. ele-
GAtt [41] is proposed as a generic method for incorporating
attention mechanism to any RNN module. From the table,
we can see that the proposed method outperforms all the
existing methods for egocentric activity recognition.

In EPIC-KITCHENS dataset, the labels are provided in
the form of verb and noun, which are combined to form
an activity class. The fact that not all combinations of verbs

9960

Close chocolate

Scoop coffee

t
u
p
n
I

n
n
r
-
o
g
e

A
T
S
L

w
o
l
F

∗
A
T
S
L

∗
w
o
l

F

Figure 2: Attention maps generated by ego-rnn (second row) and LSTA (third) for two video sequences. We show the 5
frames that are uniformly sampled from the 25 frames used as input to the corresponding networks. Fourth row shows the
attention map generated by the motion stream. Fifth and sixth rows show the attention map generated by the appearance and
ﬂow streams after two stream cross-modal training. For ﬂow, we visualize the attention map on the ﬁve frames corresponding
to the optical ﬂow stack given as input. (∗: Attention map obtained after two stream cross-modal fusion training).

Methods
Li et al. [16]∗∗
Ma et al. [21]∗∗
Li et al. [15]∗∗
Two stream [29]
I3D [3]
TSN [38]
eleGAtt [41]
ego-rnn [32]
LSTA-RGB
LSTA

GTEA61∗ GTEA61 GTEA71 EGTEA

66.8
75.08

-

64

73.02

-

62.1
73.24

-

57.64

51.58

49.65

-

67.76
59.48
77.59
74.14
79.31

-

69.33
66.77

79

71.32
80.01

-

67.23
60.83

77

66.16
78.14

46.5

-

53.3
41.84
51.68
55.93
57.01
60.76
57.94
61.86

Table 3: Comparison with state-of-the-art methods on pop-
ular egocentric datasets, we report recognition accuracy in
%. (∗: ﬁxed split; ∗∗: trained with strong supervision).

and nouns are feasible and that not all test classes might
have a representative training sample make it a challeng-
ing problem. We train the network for multi-task classiﬁ-
cation with verb, noun and activity supervision. We
use activity classiﬁer activations to control the bias of
verb and noun classiﬁers. The dataset provides two eval-
uation settings, seen kitchens (S1) and unseen kitchens (S2).
We obtained an accuracy of 30.16% (S1) and 15.88% (S2)
using RGB frames. The best performing baseline is a two
stream TSN that achieves 20.54% (S1) and 10.89% (S2) [4].
Our model is particularly strong on verb prediction (58%)

where we gain +10% points over TSN. verb in this context
is typically describing actions that develop into an activity
over time, conﬁrming once more LSTA efﬁciently learns
encoding of sequences with localized patterns.

7. Conclusion

We presented LSTA that extends LSTM with two core
features: 1) attention pooling that spatially ﬁlters the in-
put sequence and 2) output pooling that exposes a distilled
view of the memory at each iteration. As shown in a de-
tailed ablation study, both contributions are essential for a
smooth and focused tracking of a latent representation of
the video to achieve superior performance in classiﬁcation
tasks where the discriminative features can be localized spa-
tially. We demonstrate its practical beneﬁts for egocentric
activity recognition with a two stream CNN-LSTA architec-
ture featuring a novel cross-modal fusion and we achieve
state-of-the-art accuracy on four standard benchmarks.

Acknowledgements: This work has been partially supported by
the Spanish project TIN2016-74946-P (MINECO/FEDER, UE),
CERCA Programme / Generalitat de Catalunya and ICREA under
the ICREA Academia programme. We gratefully acknowledge the
support of NVIDIA Corporation with the donation of GPUs used
for this research.

9961

References

[1] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S.
Gould, and L. Zhang. Bottom-up and top-down attention for
image captioning and visual question answering.
In Proc.
CVPR, 2018.

[2] C. Cao, Y. Zhang, Y. Wu, H. Lu, and J. Cheng. Egocentric
gesture recognition using recurrent 3d convolutional neural
networks with spatiotemporal transformer modules. In Proc.
ICCV, 2017.

[3] J. Carreira and A. Zisserman. Quo vadis, action recognition?
a new model and the kinetics dataset. In Proc. CVPR, 2017.
[4] D. Damen, H. Doughty, G.M. Farinella, S. Fidler, A. Furnari,
E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price,
and M. Wray. Scaling egocentric vision: The epic-kitchens
dataset. In Proc. ECCV, 2018.

[5] W. Du, Y. Wang, and Y. Qiao. Recurrent spatial-temporal
attention network for action recognition in videos.
IEEE
Transactions on Image Processing, 27(3):1347–1360, 2018.
[6] C. Feichtenhofer, A. Pinz, and R. Wildes. Spatiotempo-
ral residual networks for video action recognition. In Proc.
NIPS, 2016.

[7] C. Feichtenhofer, A. Pinz, and A. Zisserman. Convolutional
two-stream network fusion for video action recognition. In
Proc. CVPR, 2016.

[8] F.A. Gers and J. Schmidhuber. Recurrent nets that time and
count.
In Proceedings of the IEEE-INNS-ENNS Interna-
tional Joint Conference on Neural Networks(IJCNN), 2000.
[9] F.A. Gers, J. Schmidhuber, and F. Cummins. Learning to
Forget: Continual Prediction with LSTM. Neural Computa-
tion, 12(10):2451–2471, 2000.

[10] R. Girdhar and D. Ramanan. Attentional pooling for action

recognition. In Proc. NIPS, 2017.

[11] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask r-cnn.

In Proc. ICCV, 2017.

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning

for Image Recognition. In Proc. CVPR, 2016.

[13] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term

memory. Neural Comput., 9(8):1735–1780, 1997.

[14] C. Ionescu, O. Vantzos, and C. Sminchisescu. Matrix Back-
propagation for Deep Networks with Structured Layers. In
Proc. CVPR, 2015.

[15] Y. Li, M. Liu, and J.M. Rehg. In the eye of beholder: Joint
learning of gaze and actions in ﬁrst person video. In Proc.
ECCV, 2018.

[16] Y. Li, Z. Ye, and J.M Rehg. Delving into Egocentric Actions.

In Proc. CVPR, 2015.

[17] Z. Li, K. Gavrilyuk, E. Gavves, M. Jain, and C.G.M. Snoek.
Videolstm convolves, attends and ﬂows for action recogni-
tion. Computer Vision and Image Understanding, 166:41–
50, 2018.

[18] J. Liang, L. Jiang, L. Cao, L. Li, and A. Hauptmann. Focal
visual-text attention for visual question answering. In Proc.
CVPR, 2018.

[19] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu,
and A.C. Berg. Ssd: Single shot multibox detector. In Proc.
ECCV, 2016.

[20] C. Ma, A. Kadav, I. Melvin, Z. Kira, G. AlRegib, and H.P.
Graf. Attend and interact: Higher-order object interactions
for video understanding. In Proc. CVPR, 2018.

[21] M. Ma, H. Fan, and K.M. Kitani. Going deeper into ﬁrst-

person activity recognition. In Proc. CVPR, 2016.

[22] D. Nguyen and T. Okatani. Improved fusion of visual and
language representations by dense symmetric co-attention
for visual question answering. In Proc. CVPR, 2018.

[23] A. Piergiovanni, C. Fan, and M.S. Ryoo. Learning latent
sub-events in activity videos using temporal attention ﬁlters.
In AAAI Conference on Artiﬁcial Intelligence, 2017.

[24] M.S. Ryoo, B. Rothrock, and L. Matthies. Pooled motion

features for ﬁrst-person videos. In Proc. CVPR, 2015.

[25] S. Sharma, R. Kiros, and R. Salakhutdinov. Action recogni-

tion using visual attention. In Proc. ICLRW, 2015.

[26] Y. Shen, B. Ni, Z. Li, and N. Zhuang. Egocentric activity
In Proc. ECCV,

prediction via event modulated attention.
2018.

[27] X. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong, and W. Woo.
Convolutional LSTM Network: A Machine Learning Ap-
proach for Precipitation Nowcasting. In Proc. NIPS, 2015.

[28] G. Sigurdsson, A. Gupta, C. Schmid, A. Farhadi, and K. Ala-
hari. Actor and observer: Joint modeling of ﬁrst and third-
person videos. In Proc. CVPR, 2018.

[29] K. Simonyan and A. Zisserman. Two-Stream Convolutional
Networks for Action Recognition in Videos. In Proc. NIPS,
2014.

[30] S. Singh, C. Arora, and CV Jawahar. First person action
recognition using deep learned descriptors. In Proc. CVPR,
2016.

[31] S. Sudhakaran and O. Lanz. Convolutional long short-term
memory networks for recognizing ﬁrst person interactions.
In Proc. ICCVW, 2017.

[32] S. Sudhakaran and O. Lanz. Attention is all we need: Nailing
down object-centric attention for egocentric activity recogni-
tion. In Proc. BMVC, 2018.

[33] S. Sudhakaran and O. Lanz. Top-down attention recurrent
vlad encoding for action recognition in videos. In 17th Inter-
national Conference of the Italian Association for Artiﬁcial
Intelligence, 2018.

[34] Y. Tang, Y. Tian, J. Lu, J. Feng, and J. Zhou. Action recog-

nition in rgb-d egocentric videos. In Proc. ICIP, 2017.

[35] Y. Tang, Z. Wang, J. Lu, J. Feng, and J. Zhou. Multi-stream
deep neural networks for rgb-d egocentric action recognition.
IEEE Transactions on Circuits and Systems for Video Tech-
nology, 2018.

[36] S. Verma, P. Nagar, D. Gupta, and C. Arora. Making third
person techniques recognize ﬁrst-person actions in egocen-
tric videos. In Proc. ICIP, 2018.

[37] J. Wang, W. Jiang, L. Ma, W. Liu, and Y. Xu. Bidirectional
attentive fusion with context gating for dense video caption-
ing. In Proc. CVPR, 2018.

[38] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
L. Van Gool. Temporal Segment Networks: Towards Good
Practices for Deep Action Recognition.
In Proc. ECCV,
2016.

9962

[39] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In Proc.
CVPR, 2017.

[40] H.F.M. Zaki, F. Shafait, and A.S. Mian. Modeling sub-event
dynamics in ﬁrst-person action recognition. In Proc. CVPR,
2017.

[41] P. Zhang, J. Xue, C. Lan, W. Zeng, Z. Gao, and N. Zheng.
Adding attentiveness to the neurons in recurrent neural net-
works. In Proc. ECCV, 2018.

[42] B. Zhou, A. Khosla, Lapedriza. A., A. Oliva, and A. Tor-
ralba. Learning Deep Features for Discriminative Localiza-
tion. In Proc. CVPR, 2016.

[43] Y. Zhou, B. Ni, R. Hong, X. Yang, and Q. Tian. Cascaded
interactional targeting network for egocentric video analysis.
In Proc. CVPR, 2016.

9963

