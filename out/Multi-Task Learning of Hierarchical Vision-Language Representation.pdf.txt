Multi-task Learning of Hierarchical Vision-Language Representation

1Graduate School of Information Sciences, Tohoku University

2RIKEN Center for AIP

Duy-Kien Nguyen1 and Takayuki Okatani1

2

,

{kien, okatani}@vision.is.tohoku.ac.jp

Abstract

It is still challenging to build an AI system that can per-
form tasks that involve vision and language at human level.
So far, researchers have singled out individual tasks sepa-
rately, for each of which they have designed networks and
trained them on its dedicated datasets. Although this ap-
proach has seen a certain degree of success, it comes with
difﬁculties of understanding relations among different tasks
and transferring the knowledge learned for a task to others.
We propose a multi-task learning approach that enables to
learn vision-language representation that is shared by many
tasks from their diverse datasets. The representation is hier-
archical, and prediction for each task is computed from the
representation at its corresponding level of the hierarchy.
We show through experiments that our method consistently
outperforms previous single-task-learning methods on im-
age caption retrieval, visual question answering, and visual
grounding. We also analyze the learned hierarchical rep-
resentation by visualizing attention maps generated in our
network.

1. Introduction

Since the recent successes of deep learning on single
modality tasks, multi-modal tasks lying on the intersection
of vision and language, such as image captioning [20, 38],
visual question answering (VQA) [13, 3], visual grounding
[29] etc., have attracted increasing attention in the related
ﬁelds. Despite the fact that each of these tasks has basically
been studied independently of others, there must be close
connections among them. For instance, each task may oc-
cupy a different level in a hierarchy of sub-tasks compris-
ing the cognitive function associated with vision and lan-
guage. To gain deeper understanding of such hidden re-
lations among these vision-language tasks, we think that
multi-task learning of these tasks will be a promising di-
rection of research.

Although we have seen signiﬁcant progresses in multi-
task learning of unimodal tasks of vision [17, 30] or lan-
guage [24, 1, 33] so far,
there has been only a lim-

ited amount of progress in multi-task learning of vision-
language tasks. This may be attributable to the diversity of
these tasks. In addition to differences in inputs and outputs
of the tasks, their level of complexity differs, too. Even
though these tasks share some structures in common, it is
unclear how to learn them in the framework of multi-task
learning.

A solution to this difﬁculty is to create and use a dataset
designed for multi-task learning, where multiple objectives
are given to identical inputs. Indeed, recent studies follow
this approach, in which they have gained early successes by
joint training of different vision-language tasks using mul-
tiple objectives, such as answer and question generation for
VQA [18], and caption and scene graph generation for im-
age captioning [19]. However, this approach cannot be em-
ployed when such dedicated datasets are not available. It
may be impossible to create such datasets for arbitrary com-
binations of vision-language tasks. To do this, we need to
limit the range of tasks, which makes it hard for the learned
results to generalize to other tasks or datasets.

In this paper, aiming to resolve these issues, we propose
a framework for joint learning of multiple vision-language
tasks. Our goal is to enable to learn vision-language rep-
resentation that is shared by many tasks from their diverse
data sources. To make this possible, we employ Dense Co-
attention layers, which were developed for VQA and shown
to perform competitively with existing methods [27]. Using
a stack of Dense Co-attention layers, we can gradually up-
date the visual and linguistic features at each layer, in which
their ﬁne-grained interaction is considered at the level of in-
dividual image regions and words. We utilize this property
to learn hierarchical vision-language representation such
that each individual task takes the learned representation at
a different level of the hierarchy corresponding to its com-
plexity. We design a network consisting of an encoder for
computing shared hierarchical representation and multiple
task-speciﬁc decoders for making prediction from the rep-
resentation; see Fig. 1. This design enables multi-task learn-
ing from diverse data sources; to be rigorous, we train the
same network alternately on each task/dataset based on a
scheduling algorithm.

10492

We evaluate this method on three vision-language tasks,
image caption retrieval, visual question answering, and
visual grounding, using popular datasets, Flickr30K cap-
tions [38], MS-COCO captions [20], VQA 2.0 [13], and
Flickr30K-Entities [29]. The results show that our method
outperforms previous ones that are trained on individual
tasks and datasets. We also visualize the internal behaviours
of the task-speciﬁc decoders to analyze effects of joint
learning of the multiple tasks.

2. Related Work

representation

learning Recently,
Vision-language
tasks of vision and language
studies of multi-modal
have made signiﬁcant progress, such as image caption-
ing [2, 23], visual question answering [27, 34], visual
grounding [37, 29], image caption retrieval [26, 15], and
visual dialog [7].
In the last few years, researchers have
demonstrated the effectiveness of learning representations
shared by the two modalities in a supervised fashion.
However, these studies deal with a single task at a time.

Transfer learning A basic method of transfer learning in
deep learning is to train a neural network for a source task
and use it in some ways for a target task. This method
has been successful in a wide range of problems in com-
puter vision and natural language processing. For multi-
modal vision-language problems, early works explored sim-
ilar approaches that used pretrained models trained on some
source tasks. Plummer et al. [29] proposed to use a pre-
trained network trained on a visual grounding task to enrich
the shared representational space of images and captions,
improving accuracy of image caption retrieval. Lin et al.
[21] proposed to use pretrained models of VQA and CQA
(caption Q&A); they compute answer predictions for multi-
ple questions and then treat them as features of an image and
a caption, computing relevance between them. In this study,
instead of transferring knowledge from a source task to a
target task in a single direction (e.g., via pretrained mod-
els), we consider a multi-task learning framework in which
learning multiple tasks will be mutually beneﬁcial to each
individual task. This is made possible by the proposed net-
work and its training methodology; it sufﬁces only to train
our network for individual tasks with their loss functions
and supervised data.

Multi-task learning of vision-language tasks Since its
introduction [5], multi-task learning has achieved many suc-
cesses in several areas including computer vision and nat-
ural language processing. However, there have been only
a few works that explored joint learning of multiple multi-
modal tasks of vision and language. Li et al. [19] proposed
a method for learning relations between multiple regions

in the image by jointly reﬁning the features of three dif-
ferent semantic tasks, scene graph generation, object detec-
tion, and image/region captioning. Li et al. [18] showed
that joint training on VQA and VQG (visual question gen-
eration) contributes to improve VQA accuracy and also un-
derstanding of interactions among images, questions, and
answers. Although these works have demonstrated the po-
tential of multi-task learning for the vision-language tasks,
they strongly rely on the availability of the datasets provid-
ing supervision over multiple tasks, where an input is shared
by all the tasks while a different label is given to it for each
task.

3. Learning Vision-Language Interaction

3.1. Problem Formulation

We consider multiple vision-language tasks, in each of
which an output O is to be estimated from an input pair of
I and S, where I is an image and S is a sentence. The input
pair I and S have the same formats for all the tasks (with
differences in the interpretation of S for different tasks),
whereas the output O will naturally be different for each
task. For example, in VQA, O is a set of conﬁdent scores
of answers to the input question S in a predeﬁned answer
set; in image caption retrieval, O is a set of binary values
indicating the relevance of the input caption S; in visual
grounding, O is a set of binary variables specifying a set
of image regions corresponding to the phrases in the input
sentence S.

The input image is represented by a set of region fea-
tures, which we denote by I = [i1, ..., iT ]; in our exper-
iments, we use a bag of region features from a pretrained
Faster-RCNN [34]. The input sentence is represented by a
sequence S = [s1, ..., sN ] of word features, which are ob-
tained by ﬁrst computing GloVe embedding vectors [28] of
the input words and then inputting them to a two-layer bidi-
rectional LSTM.

An overview of the proposed network architecture is
shown in Fig. 1. It consists of a single encoder shared by
all the tasks and multiple task-speciﬁc decoders. We will
describe these two components below.

3.2. Shared Encoder

To construct the shared encoder, we employ the Dense
Co-attention layer [27]. We conjecture that different tasks
require different levels of vision-language fusion. Thus, we
stack multiple Dense Co-attention layers to extract hierar-
chical, fused features of the input image I and sentence S.
We attach a decoder for each task to the layer that is the best
ﬁt for the task in terms of the fusion hierarchy, as shown in
Fig. 1.

Starting with S0 = S and I0 = I, the shared encoder
incrementally updates the language and vision features at

10493

Task M Decoder

Task M 
pred.

this task. We explain below its design for each of the three
tasks considered in this study.

Shared Encoder

(SL, IL )

Layer L

(Sl+1, Il+1 )

Layer l+1

(Sl, Il )

Layer l

(S1, I1 )

Layer 1

(S, I )

Task m Decoder

Task 1 Decoder

Task m 
pred.

Task 1 

pred.

Figure 1: The proposed network consists of a shared en-
coder and task-speciﬁc decoders. The shared encoder is a
stack of L Dense Co-attention layers and computes hierar-
chical representation of the input sentence and image. Each
of M task-speciﬁc decoders receives one intermediate-layer
representation to compute prediction for its task.

each Dense Co-attention layer as

(Sl, Il) = DCLl(Sl−1, Il−1),

(1)

where Sl = [sl,1, ..., sl,N ] ∈ Rd×N , Il = [il,1, ..., il,T ] ∈
Rd×T , Sl−1 = [sl−1,1, ..., sl−1,N ] ∈ Rd×N , and Il−1 =
[il−1,1, ..., il−1,T ] ∈ Rd×T ; DCLl
indicates the input-
output function realized by the l-th Dense Co-attention
layer. In each Dense Co-attention layer, two attention maps
are generated in a symmetric fashion, i.e., the one over
image regions conditioned on each sentence word and the
other over sentence words conditioned on each image re-
gion, where multiplicative attention is employed. The gen-
erated attention maps are then applied to Il−1 and Sl−1 to
yield ˆIl−1 and ˆSl−1, respectively. Finally, the original and
attended features of image and sentence are fused by ﬁrst
concatenating them and then applying a linear transform
followed by ReLU. This is done for sentence feature and
image feature, respectively, as

sl,n = ReLU(cid:18)WSl(cid:20)sl−1,n
il,t = ReLU(cid:18)WIl(cid:20)il−1,t

ˆil−1,n(cid:21) + bSl(cid:19) + sl−1,n,
ˆsl−1,t(cid:21) + bIl(cid:19) + il−1,t,

(2)

(3)

where WSl ∈ Rd×2d, bSl ∈ Rd, WIl ∈ Rd×2d and bIl ∈ Rd
are learnable parameters.

3.3. Task speciﬁc Decoders

As shown in Fig. 1, we design a task-speciﬁc decoder
for each task and attach it to the layer of the shared encoder
selected for the task. Letting l be the index of this layer, the
decoder receives (Sl, Il) and produces the ﬁnal output O for

3.3.1

Image Caption Retrieval

In this task, we calculate the relevant score for the input
pair (I, S). The decoder for this task consists of two
summary networks and a scoring layer. Let lR be the in-
dex of the layer of the shared encoder to which this de-
coder is attached. The ﬁrst summary network computes
∈ Rd that summarizes the image features
a vector vIlR
IlR = [ilR,1, . . . , ilR,T ] of T regions. The second summary
∈ Rd that summarizes the sentence
network computes vSlR
features SlR = [slR,1, . . . , slR,N ] of N words.

The two summary networks have the same architecture.
Let us take the image summary network for explanation.
It consists of a two-layer feedforward network that yields
attention maps over the T image regions and a mechanism
that applies the attention maps to IlR to obtain the summary
vector vIlR

The feedforward network has d hidden units with ReLU
non-linearity, which receives the image feature of a single
region and outputs K scores. To be speciﬁc, denoting the
feedforward network by MLPI , it maps the feature vector
of each region t(= 1, . . . , T ) to K scores as

.

cI
t = [cI

1,t, . . . , cI

K,t] = MLPI (ilR,t),

t = 1, . . . , T.

(4)

These scores are then normalized by softmax across the
T regions to obtain K parallel attention maps over the T
regions, which are averaged to produce the ﬁnal attention
map [αI

T ]; more speciﬁcally,

1, . . . , αI

αI

t =

1
K

exp(cI
k,t)
t=1 exp(cI

k,t)

K

Xk=1

PT

,

t = 1, . . . , T.

(5)

The summary vector vIlR
feature vectors using this attention weights, i.e.,

is the weighted sum of the image

vIlR

=

αI

t ilR,t,

(6)

T

Xt=1

As mentioned above, we generate K parallel attention maps
and average them to obtain a single attention map. This is
to capture more diverse attention distribution.

We follow the same procedure to compute the summary
vector vSlR
, where a two-layer feedforward network MLPS
generating K parallel attention maps over N word features
SlR = [slR,1, . . . , slR,N ] is used. Using the two summary
∈ Rd thus obtained, the scoring layer
vectors vIlR
computes the relevant score of an image-caption pair (I, S)
as

and vSlR

score(I, S) = σ(v⊤
IlR

W vSlR

),

(7)

where σ is the logistic function and W ∈ Rd×d is a learn-
able weight matrix.

10494

3.3.2 Visual Question Answering

In this task, we compute the scores of a set of predeﬁned an-
swers for the input image-question pair. Let lQ be the index
of the layer to which the decoder is attached. We employ
the same architectural design as in the decoder for image
caption retrieval to compute the summary vectors vIlQ
and
vSlQ
from the input features, i.e., SlQ = [slQ,1, . . . , slQ,N ]
and IlQ = [ilQ,1, . . . , ilQ,T ]. To obtain these summary vec-
tors, two summary networks, each of which is two-layer
feedforward network with d hidden units and ReLU nonlin-
earity, are used to compute K attention maps, and then they
are applied to the input features.

Following [27], we compute scores for a set of the pre-
deﬁned answers by using a two-layer feedforward network
having d hidden units with ReLU non-linearity and output
units for the scores; the output units employ the logistic
function for their activation function. Denoting the network
by MLP, the scores are calculated as

(scores of answers) = σ(cid:16)MLP(cid:0)"vIlQ

vSlQ#(cid:1)(cid:17).

(8)

3.3.3 Visual Grounding

This is a task in which given an image and a phrase (usually
one contained in a caption describing the image), we want
to identify the image region corresponding to the phrase.
Previously proposed approaches attempt to learn to score
each region-phrase pair separately; any context in the cap-
tion is not taken into account, or any joint inference about
global interaction between all phrases in the caption is not
performed. We believe that context is important for under-
standing a local phrase in a sentence, needless to mention
its necessity for higher-level tasks.

Let lG be the index of the layer of the shared en-
coder connecting to the decoder for this task. Given P =
[(b1, e1), ..., (bH , eH )] where (bh, eh) indicates the start and
end indexes of the h-th phrase in the input N word caption
(1 ≤ bh ≤ eh ≤ N ), we compute the feature ph ∈ Rd for
the h-th phrase by pooling the word features in the index
range of [bh : eh] as

ph = AvgPooling(SlG [bh : eh]).

(9)

Here we use average pooling to produce a ﬁxed-size vec-
tor representation of a phrase ph. This can also be seen as
computing an attended feature using an attention map with
equal weights on words in the phrase and zero weights on
other words.

We then compute the score for a pair of a phrase ph ∈ Rd

and an image region ilG,t ∈ Rd as

where σ is the logistic function and W ∈ Rd×d is a learn-
able weight matrix.

4. Training on Multiple Tasks

We train the proposed network on the above multiple
tasks. Considering their diversity, we use a strategy to train
it on a single selected task at a time and iterate this by
switching between the tasks. (Note that we cannot simulta-
neously train the network on these tasks by minimizing the
sum of their losses, because the inputs differ among tasks.)

4.1. Task switching schedule

It is essential for which task and how many times we
update the parameters of the network. In this study, we em-
ploy two strategies. One is a curriculum learning approach
that starts from a single task and increases the number of
tasks one by one, i.e., training ﬁrst on single tasks, then
on pairs of tasks, and ﬁnally on all tasks. The other is a
scheduling method when training more than one task in this
curriculum. To be speciﬁc, we employ the strategy of peri-
odical task switching as in [8] but with different iterations
of parameter updates for each task. Following [24], we up-
date the network parameters for i-th task for Cαi iterations
before switching to a new task, where C is the number of
iterations in an updating cycle that we specify; αi is deter-
mined as explained below. Algorithm 1 shows the entire
procedure. More details are given in the supplementary ma-
terial.

4.2. Choosing Layers Best Fit for Tasks

We need to decide which layer l(i) of the shared encoder
is the best ﬁt for each task i. We pose it as a hyperparam-
eter search, in which we also determine other parameters
for training each task i, i.e., # stepi (step size for learn-
ing rate decay), the number of iteration # iteri (used to de-
termine αi), and the batch size bsi. To choose them, we
conduct a grid search by training the network on each indi-
vidual task. After that, these hyperparameters are used in
joint learning of the tasks. Denoting the number of tasks
to be learned by M ′(= 1, 2 or 3), the step size of training
i=1 # stepi; the total number of it-
i=1 # iteri; and αi is determined as
αi = # iteri/# iter. The batch size bsi and layer l(i) de-
termined as above are ﬁxed in all the subsequent training
processes.

is given by # step = PM ′
erations is # iter = PM ′

5. Experiments

score(ph, it) = σ(cid:16)p⊤

h W ilG,t(cid:17),

We conducted a series of experiments to test the effec-

(10)

tiveness of the proposed approach.

10495

Algorithm 1: Training the proposed network on M ′
tasks. El represents a sub-network of the shared en-
coder up to l-th layer (l = 1, . . . , L); D1, . . . , DM ′ are
M ′ task-speciﬁc decoders; θ indicates their parameters.
l(i) is the index of the layer to which the decoder for i-
th task is attached. We represent the output of this layer
for an input x as El(i).
1 num cycle = ⌊ # iter
C ⌋

2 S =(cid:0)[1] ∗ Cα1 + ... + [M ′] ∗ CαM ′(cid:1) ∗ num cycle

3 # Array operation in Python style:
4 # [1] * 3 + [2] * 2 = [1, 1, 1, 2, 2]
5 for task i in S do

6

7

8

9

1: Sample pairs of an input and output: x, y ∼ Pi
2: hix ← El(i)(x)
3: Output prediction ˆy ← Di(hix)
4: θ ← Adam(∇θL(y, ˆy))

10 end

5.1. Datasets and Evaluation Methods

Image Caption Retrieval We use two datasets for this
task, MS-COCO and Flickr30k. MS-COCO consists of
82,783 train and 40,504 val images. Following the standard
procedure [16], we use the 1,000 val images and the 1,000
or 5,000 test images, which are selected from the original
40,504 val images. We use all of the 82,783 train images
for training. Flickr30k consists of 31,783 images collected
from Flickr. Following the standard procedure [16], we split
them into train, val, and test sets; val and test contains 1,000
images for each and train contains all the others. We report
Recall@K(K = 1, 5, 10) (i.e., recall rates at the top 1, 5,
and 10 results).

Visual Question Answering We use VQA 2.0 [13],
which is the most popular and the largest (as of now) dataset
for this task. It contains questions and answers for images
of MS-COCO. There are 443,757 train, 214,354 val, and
447,793 test questions, respectively. The train questions
are for train images of MS-COCO and val and test ques-
tions are for val and test images of MS-COCO respectively.
Following the standard approach [34], we choose correct
answers appearing more than 8 times to form the predeﬁned
answer pool. We use the accuracy metric presented in the
original paper [3] in all the experiments.

Visual Grounding For visual grounding task, we evalu-
ate our approach on Flickr30k Entities [29], which contains
244,035 annotations to the image-caption pairs (31,783 im-
ages and 158,915 captions) of Flickr30k. It provides corre-
spondence between phrases in a sentence and boxes in an
image that represent the same entities. The train, val and

test are splitted as in the ICR task. We use 1,000 images
for val and test splits each and the rest for train split follow-
ing [29]. The task is to localize the corresponding box(es)
to each of the given phrases in a sentence. As proposed in
[29], we consider a predicted region to be a correct match
with a phrase if it has IOU ≥ 0.5 with the ground truth
bounding box for that phrase. By treating the phrase as the
query to retrieve the regions from the input image, we report
Recall@K(K = 1, 5, 10) similar to image caption retrieval
(the percentage of queries for which a correct match has
rank of at most K).

Avoiding Contamination of Training Samples As we
train the network by alternately switching the tasks, we need
to make sure that there is no contamination between training
and testing sets for all the tasks. To make a fair comparison
with previous studies of VQA, we need to train the network
using both train and val questions of VQA 2.0, as was done
in the previous studies. However, if we use val questions in
our joint learning framework, our network (i.e., the shared
encoder) can see the val set of MS-COCO, resulting in con-
tamination of training samples. To avoid this, we use the
following procedure: i) we ﬁrst train the network using all
the train sets for the three tasks and test it on the test sets
for ICR and VG; ii) we then train the network (from scratch)
using the train sets for ICR and VG and train+val sets for
VQA and test it on the test sets for VQA. This procedure
was employed in the experiments of Sec. 5.4, but not em-
ployed in the experiments of Sec. 5.3, because evaluation
was done only on val sets for all the tasks.

5.2. Optimal Layers and Training Parameters

As explained in Sec. 4.2, we ﬁrst train our network on
each individual task to ﬁnd the layers ﬁt for each task along
with other training parameters. The results are: lR = 3 (im-
age caption retrieval), lQ = 5 (VQA), and lG = 2 (visual
grounding). The training parameters were determined ac-
cordingly; see the supplementary material for details. We
freeze all these parameters throughout all the experiments.
We note here the training method used in all the exper-
iments. We used the Adam optimizer with the parameters
α = 0.001, β1 = 0.9, β2 = 0.99, and α decay = 0.5. We
employed a simple training schedule; we halve the learning
rate by “α decay” after each “# step” or step size, which
are determined above. All the weights in our network were
initialized by the method of Glorot et al. [12]. Dropout is
applied with probability of 0.3 and 0.1 over FC layers and
LSTM, respectively. The dimension d of the feature space
is set to 1024.

5.3. Effects of Joint Learning of Multiple Tasks

To evaluate the effectiveness of joint learning, we ﬁrst
trained the model on all possible combinations out of the

10496

is [anyone] riding [the bike]

is [anyone] riding [the bike]

what is [the woman] riding

what is [the woman] riding

is anyone riding the bike
matching score: 3.573e-07

is anyone riding the bike

matching score: 0.387

what is the woman riding

matching score: 0.083

what is the woman riding

matching score: 0.008

is anyone riding the bike

Pred: no, Ans: no

is anyone riding the bike

Pred: yes, Ans: yes

what is the woman riding

what is the woman riding

Pred: snowboard, Ans: snowboard

Pred: skis, Ans: skis

Figure 2: Example visualizations of behaviours of our network for two complementary image-question pairs (i.e., samples
with the same question but different images and answers) from VQA 2.0 dataset. The three rows (from top to bottom) show
the behaviours of the VG, ICR, and VQA decoders, respectively. For VG, top-1 regions corresponding to the entities (i.e.,
NP chunks) in the questions are shown. For ICR and VQA, the attention maps generated in their decoders are shown; the
brightness of image pixels and the redness of words indicate the attention weights.

three tasks and evaluated their performances. To be speciﬁc,
for each combination of tasks, we trained our model on their
train split(s) and calculating its performance for each of the
trained tasks on its val split. When training on two or more
tasks, we used the method explained in Sec. 4.1.

Table 1 shows the results. It is observed that the joint
learning of two tasks achieves better or comparable perfor-
mances than the learning on a single task; and that the joint
learning of all the three tasks yields the best performance.
These conﬁrm the effectiveness of our method for multi-
task learning.

For ICR, we use two datasets, MS-COCO and Flickr30k;

the former is about three times larger than the latter. We
evaluated how performances vary between when using the
former and when using the latter. Table 2 shows the results.
It is observed that the joint learning with VQA and VG is
more beneﬁcial for the smaller dataset (Flickr30k) than the
larger one (MS-COCO), e.g., from 67.16 to 72.07 vs. from
69.05 to 70.43 (ICR: image annotation). On the other hand,
the improvements of the other tasks (VQA and VG) due to
the joint training with ICR are smaller for Flickr30k than
for MS-COCO, e.g., from 65.50 to 66.09 vs. 65.50 to 66.35
(VQA).

10497

Table 1: Performances for different combinations of the
three tasks, VQA, VG(visual grounding), and ICR(image
caption retrieval). Accuracy (Acc) is reported for VQA, and
Recall@1 (R@1) is reported for VG and ICR; two numbers
of ICR are image annotation (upper) and image retrieval
(lower), respectively. MS-COCO dataset is used for ICR.

Task

VQA (Acc)

ICR (R@1)

VG (R@1)

VQA

65.50

ICR

VG

-

-

VQA + ICR

66.24

VQA + VG

65.85

ICR + VG

-

VQA + ICR

+ VG

66.35

-

69.05
56.47

-

69.52
56.74

-

69.23
57.40
70.43
57.50

-

-

58.09

-

58.07

58.28

58.26

Table 2: Effects of joint training when using different image
caption retrieval datasets, MS-COCO and Flickr30k. Single
means that each task is learned individually.

Task

Single

MS-COCO

Flickr30k

(w/o ICR)

Single +VQA+VG Single +VQA+VG

VQA (Acc)

65.50

ICR (R@1)

-
-

VG (R@1)

58.09

-

69.05
56.47

-

66.35
70.43
57.50
58.26

-

67.16
53.17

-

66.09
72.07
56.42
58.03

5.4. Full Results on Test Sets

We next show the performance of our method on test sets
for the three tasks. We employ the procedure for avoiding
training data contamination explained in the last paragraph
of Sec. 5.1. We show below comparisons of our method
with previous methods on each task. Note that our method
alone performs joint learning of the three tasks and others
are trained only on each individual task.

Image Caption Retrieval Table 3 shows the perfor-
mances of previous methods and our method on Flickr30k
and MS-COCO (The numbers for MS-COCO are perfor-
mance on the 1,000 testing images. In the supplementary
material, we report the performance on the 5,000 testing
images of MS-COCO). It is seen that our method is compa-
rable with the state-of-the-art method (S-E Model) on MS-
COCO. For Flickr30k, which is three times smaller than
MS-COCO, our method outperforms the best published re-
sult (S-E Model) by a large margin (about 9.5% in average)
on all six evaluation criteria, showing the effectiveness of

our method. This demonstrates that our method can lever-
age the joint learning with other tasks to cover insufﬁcient
amount of training data for ICR.

Visual Question Answering Table 4 shows comparisons
of our method to previous published results on VQA 2.0 in
both test-dev and test-standard sets. It is observed in Table
4 that our method outperforms the state-of-the-art method
(DCN [27]) by a noticable margin of ∼ 0.7% on the two
test sets. It is noted that the improvements are seen in all
the question types of test-standard set (Other with 0.5%,
Number with 0.2%, and Yes/No with 0.9%). Notably, its
accuracy for counting questions (Number) is on par with the
Counting Module, which is designed to improve accuracy
of this question type.

Visual Grounding Table 5 shows comparisons of our
method with previous methods on the Flickr30k Entities
dataset. Although our method shows lower performance
than RTP [29] on the R@5 and R@10 evaluation metrics,
it achieves a much better result on the hardest metric R@1.
It should be noted that our method uses only phrase-box
correspondences provided in the training dataset, and does
not use any other information, such as box size, color, seg-
mentation, or pose-estimation, which are used in previous
studies [29, 37].

5.5. Qualitative Evaluation

To analyze effects of joint learning of multiple tasks, we
visualize behaviours of our network. We use complemen-
tary image-question pairs contained in VQA 2.0 [13] for
better analyses. Figure 2 shows two examples of such vi-
sualization, each for a complementary image-question pair.
For visualization of VG, we extract NP chunks from the in-
put question and treat them as entities. We then compute
the score between each entity and all of the image regions,
as described in Sec. 3.3.3. The ﬁrst row of Fig. 2 shows the
correspondences between a few entities found in the ques-
tions and their top-1 image regions. For ICR and VQA, we
visualize attention maps generated in their decoders, which
are shown in the second and third rows of Fig. 2.

It can be seen from the ﬁrst row of Fig. 2 that the VG
decoder correctly aligns each entity to its corresponding im-
age region. From the second row of Fig. 2 (i.e., the attention
maps of the ICR decoder) we can observe that the ICR de-
coder is looking at the same entities as those found in the
VG decoder but with wider attention in the image and sen-
tence, implying that not only the relevant entities but their
relations are captured in the ICR decoder. It is then seen
from the third row of Fig. 2 (i.e., the attention weights on
image regions and question words of the VQA decoder) that
it narrows down its attention on the image regions and ques-
tion words that are relevant to properly answer the input

10498

Table 3: Results of image annotation and retrieval on the Flickr30K and MSCOCO (1000 testing) datasets.

Method

Image Annotation

Image Retrieval

Image Annotation

Image Retrieval

R@1

R@5

R@10

R@1

R@5

R@10

R@1

R@5

R@10

R@1

R@5

R@10

Flickr30k dataset

MSCOCO dataset

VQA [21]
RTP [29]
DSPE [35]

sm-LSTM [14]

RRF [22]

2WayNet [9]

DAN [26]
VSE++ [10]

S-E Model [15]

Ours

33.9
37.4
40.3
42.5
47.6
49.8
55.0
52.9
55.5
71.6

62.5
63.1
68.9
71.9
77.4
67.5
81.8
79.1
82.0
84.6

74.5
74.3
79.9
81.5
87.1

-

89.0
87.2
89.3
90.8

24.9
26.0
29.7
30.2
35.4
36.0
39.4
39.6
41.1
56.1

52.6
56.0
60.1
60.4
68.3
55.6
69.2
69.6
70.5
82.9

64.8
69.3
72.1
72.3
79.9

-

79.1
79.5
80.1
89.4

50.5

80.1

-

50.1
53.2
56.4
55.8

-

64.6
69.9
70.2

-

79.7
83.1
85.3
75.2

-

89.1
92.9
89.2

89.7

-

89.2
91.5
91.5

-
-

95.7
97.5
95.9

37.0

70.9

-

39.6
40.7
43.9
39.7

-

52.0
56.7
57.4

-

75.2
75.8
78.1
63.3

-

83.1
87.5
88.4

82.9

-

86.9
87.4
88.6

-
-

92.0
94.8
95.6

Table 4: Results of the proposed method along with published results of others on VQA 2.0 with single model.

Method

Feature

Test-dev

Test-standard

Overall

Other

Number

Yes/No

Overall

Other

Number

Yes/No

MCB [11] reported in [13]

MF-SIG-T3 [6]

Adelaide-Teney-MSR [34]

DCN [27]

Memory-augmented Net [25]

VKMN [32]

Adelaide-Teney-MSR [34]

DCN [27] in our experiments

Resnet

Counting Module [39]
MLB + DA-NTN [4]

Faster
RCNN

Ours

-

64.73
62.07
66.72

-
-

65.32
68.60
68.09
67.56
69.28

-

55.55
52.62
56.77

-
-

56.05
58.76
58.97
57.92
59.17

-

42.99
39.46
46.65

-
-

44.21
50.85
51.62
47.14
51.54

-

81.29
79.20
83.70

-
-

81.82
84.83
83.14
84.29
85.80

62.27

53.36

38.28

78.82

-

62.27
67.04
62.10
64.36
65.67
68.94
68.41
67.94
69.57

-

52.59
56.95
52.60
57.79
56.26
58.78
59.11
58.20
59.27

-

39.77
47.19
39.50
37.90
43.90
51.23
51.39
47.13
51.46

-

79.32
83.85
79.20
83.70
82.20
85.27
83.56
84.60
86.17

Table 5: Comparison of our method and previous ones on
the visual grounding task using Flickr30k Entities in the
same condition.

Method

R@1

R@5

R@10

Structured Matching [36]

DSPE [35]

GroundeR [31]

MCB [11]
RTP [29]
GOP [37]

Ours

42.08
43.89
48.38
48.69
50.89
53.97
57.39

-

-

64.46

68.66

-
-

-
-

71.09

75.73

-

-

69.37

71.03

questions, e.g., the bikes in the images and the phrase “is
enyone” in the questions; and the snowboard and the skis in
the images and the phrase “what is”.

Other observations can be made for the results in Fig. 2.
For instance,
the ICR decoder gives a very low score
(3.573e-07) for the pair of the ﬁrst image and the ques-
tion “is anyone riding the bike” and a high score (0.387)
for the second image and the same question. Considering
the word attention focusing only on the phrase ”anyone rid-
ing the bike”, we may think that the ICR decoder correctly
judges the (in)consistency between the contents of the im-

ages and the phrase. These agree well with their correct
answers in VQA (i.e., “No” and “Yes”), implying the in-
teraction between ICR and VQA. Further analyses will be
provided in supplementary material.

6. Summary and Conclusion

In this paper, we have presented a multi-task learning
framework for vision-language tasks. The key component
is the proposed network consisting of the representation en-
coder that learns to fuse visual and linguistic representations
in a hierarchical fashion, and task-speciﬁc decoders that uti-
lize the learned representation at their corresponding levels
in the hierarchy to make prediction. We have shown the ef-
fectiveness of our approach through a series of experiments
on three major tasks and their datasets. The shared hierar-
chical representation learned by the encoder has been shown
to generalize well across the tasks.

Acknowledgement

This work was partly supported by JSPS KAKENHI
Grant Number JP15H05919 and JST CREST Grant Num-
ber JPMJCR14D1.

10499

References

[1] Wasi Uddin Ahmad, Kai-Wei Chang, and Hongning Wang.
Multi-task learning for document ranking and query sugges-
tion. In International Conference on Learning Representa-
tions (ICLR), 2018. 1

[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
visual question answering. In International Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 2

[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi
Parikh. VQA: Visual Question Answering. In International
Conference on Computer Vision (ICCV), 2015. 1, 5

[4] Yalong Bai, Jianlong Fu, Tiejun Zhao, and Tao Mei. Deep
attention neural tensor network for visual question answer-
ing. In European Conference on Computer Vision (ECCV),
2018. 8

[5] Rich Caruana. Multitask learning. Machine Learning, 1997.

2

[6] Zhu Chen, Zhao Yanpeng, Huang Shuaiyi, Tu Kewei, and
Ma Yi. Structured attentions for visual question answering.
In International Conference on Computer Vision (ICCV),
2017. 8

[7] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,
Deshraj Yadav, Jos´e M.F. Moura, Devi Parikh, and Dhruv
Batra. Visual Dialog. In International Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2017. 2

[8] Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng
Wang. Multi-task learning for multiple language translation.
In International Joint Conference on Natural Language Pro-
cessing (IJCNLP), 2015. 4

[9] Aviv Eisenschtat and Lior Wolf. Capturing deep correlations
with 2-way nets. In International Conference on Computer
Vision and Pattern Recognition (CVPR), 2017. 8

[10] Fartash Faghri, David J. Fleet, Ryan Kiros, and Sanja Fi-
dler. VSE++: improved visual-semantic embeddings. arXiv
preprint arXiv:1707.05612, 2017. 8

[11] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,
Trevor Darrell, and Marcus Rohrbach. Multimodal com-
pact bilinear pooling for visual question answering and vi-
sual grounding. In Empirical Methods in Natural Language
Processing (EMNLP), 2016. 8

[12] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-
culty of training deep feedforward neural networks. In Inter-
national Conference on Artiﬁcial Intelligence and Statistics,
2010. 5

[13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the V in VQA matter: El-
evating the role of image understanding in Visual Question
Answering. In International Conference on Computer Vision
and Pattern Recognition (CVPR), 2017. 1, 2, 5, 7, 8

[15] Yan Huang, Qi Wu, Chunfeng Song, and Liang Wang.
Learning semantic concepts and order for image and sen-
tence matching. In International Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 2, 8

[16] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-
ments for generating image descriptions.
In International
Conference on Computer Vision and Pattern Recognition
(CVPR), 2015. 5

[17] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geome-
try and semantics. In International Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 1

[18] Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang,
Xiaogang Wang, and Ming Zhou. Visual question genera-
tion as dual task of visual question answering. In Interna-
tional Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 1, 2

[19] Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, and Xi-
aogang Wang. Scene graph generation from objects, phrases
and region captions. In International Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2017. 1, 2

[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C. Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European Conference on Computer Vision (ECCV), 2014. 1,
2

[21] Xiao Lin and Devi Parikh. Leveraging visual question an-
swering for image-caption ranking. In European Conference
on Computer Vision (ECCV), 2016. 2, 8

[22] Y. Liu, Y. Guo, E. M. Bakker, and M. S. Lew. Learning a re-
current residual fusion network for multimodal matching. In
International Conference on Computer Vision (ICCV), 2017.
8

[23] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
Neural baby talk. In International Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 2

[24] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence
learning. In International Conference on Learning Represen-
tations (ICLR), 2016. 1, 4

[25] Chao Ma, Chunhua Shen, Anthony R. Dick, and Anton
van den Hengel. Visual question answering with memory-
augmented networks. In International Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018. 8

[26] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual
attention networks for multimodal reasoning and matching.
In International Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 2, 8

[27] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion
of visual and language representations by dense symmet-
ric co-attention for visual question answering.
In Interna-
tional Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 1, 2, 4, 7, 8

[14] Yan Huang, Wei Wang, and Liang Wang. Instance-aware im-
age and sentence matching with selective multimodal LSTM.
In International Conference on Computer Vision and Pattern
Recognition (CVPR), 2016. 8

[28] Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. Glove: Global vectors for word representa-
tion. In Empirical Methods in Natural Language Processing
(EMNLP), 2014. 2

10500

[29] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,
Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazeb-
nik. Flickr30k entities: Collecting region-to-phrase corre-
spondences for richer image-to-sentence models.
Interna-
tional Journal of Computer Vision (IJCV), 2017. 1, 2, 5, 7,
8

[30] Jamie Ray, Heng Wang, Du Tran, Yufei Wang, Matt Feis-
zli, Lorenzo Torresani, and Manohar Paluri. Scenes-objects-
actions: A multi-task, multi-label video dataset. In European
Conference on Computer Vision (ECCV), 2018. 1

[31] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor
Darrell, and Bernt Schiele. Grounding of textual phrases in
images by reconstruction. In European Conference on Com-
puter Vision (ECCV), 2016. 8

[32] Zhou Su, Chen Zhu, Yinpeng Dong, Dongqi Cai, Yurong
Chen, and Jianguo Li. Learning visual knowledge mem-
ory networks for visual question answering.
In Interna-
tional Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2018. 8

[33] Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and
Christopher J. Pal. Learning general purpose distributed
sentence representations via large scale multi-task learning.
In International Conference on Learning Representations
(ICLR), 2018. 1

[34] Damien Teney, Peter Anderson, Xiaodong He, and Anton
van den Hengel. Tips and tricks for visual question an-
swering: Learnings from the 2017 challenge.
In Interna-
tional Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 2, 5, 8

[35] Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning
deep structure-preserving image-text embeddings. In Inter-
national Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2016. 8

[36] Mingzhe Wang, Mahmoud Azab, Noriyuki Kojima, Rada
Mihalcea, and Jia Deng. Structured matching for phrase
localization.
In European Conference on Computer Vision
(ECCV), 2016. 8

[37] Raymond Yeh, Jinjun Xiong, Wen-Mei Hwu, Minh Do, and
Alexander Schwing. Interpretable and globally optimal pre-
diction for textual grounding using image concepts. In Ad-
vances in Neural Information Processing Systems (NIPS),
2017. 2, 7, 8

[38] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-
maier. From image descriptions to visual denotations: New
similarity metrics for semantic inference over event descrip-
tions. In Transactions of the Association for Computational
Linguistics (TACL), 2014. 1, 2

[39] Yan Zhang, Jonathon Hare, and Adam Prgel-Bennett. Learn-
ing to count objects in natural images for visual question an-
swering. In International Conference on Learning Represen-
tations (ICLR), 2018. 8

10501

