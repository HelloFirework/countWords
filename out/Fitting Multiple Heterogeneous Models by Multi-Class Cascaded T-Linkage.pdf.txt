Fitting Multiple Heterogeneous Models by Multi-class Cascaded T-linkage

Luca Magri Andrea Fusiello

DPIA - Universit`a degli Studi di Udine, Italy

name.surname@uniud.it

Abstract

This paper addresses the problem of multiple model ﬁt-
ting in the general context where the sought structures
can be described by a mixture of heterogeneous paramet-
ric models drawn from different classes. To this end, we
conceive a multi-model selection framework that extends T-
linkage to cope with different nested classes of models. Our
method, called MCT, compares favourably with the state-
of-the-art on publicly available data-sets for various ﬁtting
problems: lines and conics, homographies and fundamental
matrices, planes and cylinders.

1. Introduction

During the past decades until nowadays, the need of
high-level geometric representations of visual content has
incessantly inspired the research towards the design of ro-
bust multi-model ﬁtting methods aimed at segmenting the
data in meaningful structures by simultaneously estimating
a collection of underlying parametric models1.
In partic-
ular, most recent efforts focused on dealing with multiple
instances of the same model in the presence of noise and
outliers.

In this work we address the problem of multi-model ﬁt-
ting in the more general context where the sought structures
can be described by a mixture of heterogeneous paramet-
ric models, i.e., of different type or class. To this end, we
conceive a multi-model selection framework that extends T-
linkage to cope with different nested classes of models ( e.g.
lines vs. circles, homographies vs. fundamental matrices).
This problem is also referred to as multi-class/multi-model
ﬁtting.

The main motivations can be ascribed to two orders of

reasons.

• Generally speaking, a wider choice between possible

1Although used somehow interchangeably, ”structure” and ”model”
have different nuances. The term structure refers to the arrangement and
relations among the data, and it is intrinsic to the data itself, whereas the
model is ”in the eye of the beholder”, being the mathematical description
of the data that an observer ﬁts onto them.

explanatory models enables higher level of abstraction
and allows more ﬂexibility in the interpretation of the
data. In this sense, dealing with multiple model and
classes of varying complexities can be regarded as a
further step towards a better and more elaborate un-
derstanding of visual content to ﬁll the gap between
low-level vision and higher level reasoning.

• From a practical point of view, in some applications,
degenerate structures may arise when the data do
not provide enough constraints to determine a model
uniquely. Then it is convenient to be able to switch to
a simpler and more appropriate model to circumvent
the pitfalls of ambiguous estimate which are extremely
fragile to outliers.

The success of such endeavours involves the explicit in-
tegration of model selection criteria in the multi-structure
recovery framework. In this regard, it is useful to keep in
mind, as a caveat, a well-known aphorism of scientiﬁc folk-
lore: ”all models are wrong but some are useful”. In many
concrete situations, a qualitative dichotomy between mod-
els lags behind the quantitative nuances of continuous phe-
nomena, and it may be puzzling to decide in favour of an
interpretation of the data rather than for another.

For instance, in stereo vision, assuming a rigid scene, the
borders between a planar setting – modelled by projectivi-
ties – and a full three-dimensional one – better described
by the epipolar geometry– are faintly traced by the relative
variation of the focal length of the camera compared to the
depth of the scene. Similar difﬁculties arise, as instance, if
a regular polygon with k sides has to be described either by
several lines or by a unique circle. In these situations, the
practical context of application should tip the scale of model
selection towards the more appropriate class of structure for
the task at hand.

Contributions On these premises, being aware that the
usefulness of a model is “in the eye of the beholder”, we
present a multi-model recovery framework that offers a hi-
erarchical interpretation of data in nested compatible mod-
els, which can be conveniently biased to serve speciﬁc pur-

17460

poses exploiting user provided criteria. Speciﬁcally, we
conceive a cascaded extension of T-linkage that sequentially
extract simpler nested models starting form the more gen-
eral ones. We leverage on geometric constraints (e.g. tan-
gency between line and circle, or compatibility between ho-
mography and fundamental matrix) to guide the sampling
towards simpler valid interpretation of the data and to break
down a complex model selection task into a collection of
easier local model selection instances.

Models belonging to different classes compete with each
other only if they explain the same points and are geomet-
rically compatible. In this way, all the intra-class model se-
lection problems are implicitly dealt by T-linkage, whereas
the inter-class model selection issues take explicitly the
form of one-vs-many or one-vs-one model comparison,
which can be efﬁciently tackled with classical tools such
as GRIC [25].

2. Previous work

Even if, besides few exceptions, the most recent litera-
ture on abstract multi-model ﬁtting has been mainly focused
on the case of multiple instances of the same model, earlier
work, tailored for speciﬁc applications, have faced, more
or less directly, the challenges entailed by the selection of
multiple models belonging to different classes.

In particular, a long and distinguished line of research on
motion segmentation deals with the development of model
selection strategy for the segmentation of different kinds of
motion, both in pairs of images and video sequences. As an
instance, the work of Torr [25, 24, 14], tackles the problem
of Multi-body Structure and Motion (M-SaM) in two views
and make use of fundamental matrices and homographies
to robustly cope with degenerate conﬁguration that arise in
presence of planar structures and pure rotational camera dis-
placements. Speciﬁcally, in [25] the geometric robust infor-
mation criterion (GRIC), suited for geometric problems is
introduced and the M-SaM problem is solved in a fashion
reminiscent of sequential-Ransac: at the beginning, candi-
date motions are randomly instantiated using both homo-
graphies and fundamental matrices and their GRIC score is
computed. Then, the models with the best GRIC are itera-
tively detected and their corresponding inliers are removed
from the matches until all the data are exhausted. At the
end, a segmentation is obtained minimising a cost function
that balances between data ﬁdelity and the complexity of
the retrieved models. The well known limitations typical of
sequential-Ransac affects as well this multi-class approach.
In addition, the method is meant to handle only direct com-
parison between pairs of models, as, in one-vs-many model
comparisons, the greedy selection of the model with the
lowest GRIC systematically favours the simpler ones.

For these reasons, in [14], the greedy selection strategy is
replaced by a quadratic binary integer programming. Along

the same line, Schindler, Suter and Wang [15] extracts mo-
tions across multiple frames. At the beginning, the pool
of mixed-motion hypotheses is pruned in order to avoid re-
dundant model by reasoning on their consensus sets. Then a
segmentation of the data is derived from the reﬁned models
exploiting integer linear programming coupled with model
selection criteria, remarkably special care is reserved for the
treatment of intersecting models.

Motion segmentation is also considered in [18, 19]
where, under the assumption of afﬁne camera model, the
feature trajectories are organised in linear spaces rather than
with fundamental matrices. In order to handle degenerate
motions – that give rise to subspace of lower dimension –
a multi-stage approach is presented: at ﬁrst degenerate 2D
subspace are detected and used to segment the data exploit-
ing an expectation maximisation scheme, then the attained
model are upgraded to general motion models. The main
idea is that, since a degenerate motion is a special case of a
general ones, an optimal solution for a degenerate structure
is unchanged when optimised by assuming a more general
model. This approach produces accurate results, but de-
pends on good initialisation and is computational expensive.
Moreover, in principle, if outliers contaminate the data, the
promotion of a degenerate motion to a general one can eas-
ily deteriorate the result.

The simultaneous ﬁtting of multi-class geometric prim-
itives to 3D data is addressed in [17], where the selection
of models instantiated by random sampling is performed
through a tabu-search optimisation implementing the Mini-
mum Description Length principle.

A trend common to all the previous approaches can be
singled out: after building a set of candidate models, all the
methods derive a segmentation of the data by optimising a
model selection criteria, namely minimising a global energy
function balancing geometric errors and model complexity
expressed in diverse forms, each grounded either on statis-
tics or information theory.

This idea has been systematised in the well established
framework of PEARL [6]: a general multi-model ﬁtting
method that relies on an efﬁcient iterative optimisation tech-
nique where the regularity of inlier clusters is also exploited.
Its success is conﬁrmed by the several variations that have
sprouted from its paradigm, such as [12] able to enforce ge-
ometric constraints, or [1] with improved efﬁciency, just to
name a few.

Although, in principle, PEARL can be extended to deal
with heterogeneous structures by specifying model penal-
ties conveniently, in practice, it has been customarily em-
ployed for the estimation of multiple model instances of the
same class. A step towards the multi-class case has been re-
cently made by Multi-X [2], that generalises and improves
PEARL by combining it with a Mean-Shift step carried out
separately for each distinct model class. Notably, an auto-

7461

matic parameter setting strategy is proposed to alleviate the
need of user intervention.

The extraction of multiple models is faced from a dif-
ferent perspective in [27], where the Generalised Principal
Component Analysis is introduced. One of the main merit
of this work is to elegantly formulate the multi-model ﬁtting
problem in the language of algebraic geometry to unify dif-
ferent speciﬁc applications as the recovery of a union of lin-
ear subspace, with possibly varying dimensions, by ﬁtting,
differentiating and dividing a set of homogeneous polyno-
mials to data. Unfortunately this approach turns to be frag-
ile in real noisy contexts.

Inspired by the Gestalt principles and by the studies on
human perceptions of patterns, the a-contrario framework
integrates probabilistic reasoning with robust estimation for
single model [13] and multi-model ﬁtting [10]. The idea
behind it, echoing Minpran [16], is to exploit the unlikeli-
ness of a structure, measured in term of the number of false
alarm (NFA), to assess its validity. This approach is very
ﬂexible and can be used as a model selection criterion for
the multi-class models scenario. As an instance, [11] pro-
poses a probabilistic model selection method to jointly ﬁt
ellipses or lines to edge maps.

The recent work of [29], which deals with the problem
of co-segmentation of motion trajectories, raises a relevant
objection. Data are often typiﬁed as ’almost-but-not-quite’
degenerate structure, and it is nearly unfeasible to select a
model with respect to another in such situations. For this
reason, the authors propose to use a spectral clustering fu-
sion scheme to combine together multiple partitions based
on “complementary geometric models”: homographies and
fundamental matrices. Unfortunately, in this way, only the
segmentation horn of the multi-model ﬁtting problem is ad-
dressed, and this solution can not be applied to recover the
model parameters underlying the attained clustering, as the
type of the models are not maintained during the clustering
fusion.

3. A multi-model selection framework

In this work, we also exploit the idea of combining clus-
tering deduced from different models, but rather than ex-
tracting a segmentation from a multi-class soup of hetero-
geneous models, we make models of different classes to
compete with each other only when they insist on the same
inliers sets and satisfy geometric constraints, breaking the
model selection in many easy-to-solve tasks.

For the convenience of the reader we brieﬂy recall the
T-linkage algorithm in Section 3.1 , while Section 3.2 de-
scribes how we propose to extend this approach to the case
of multi-class models.

3.1. T linkage

At high-level, T-linkage [8] performs a two steps ﬁrst-
represent-then-clusterise scheme: at ﬁrst, the input data are
represented by the “preferences” they grant to a pool of
provisional model hypotheses, then a greedy bottom-up ag-
glomerative clustering is performed to yield a partition of
the data merging points with similar preferences according
to the Tanimoto distance.

The concept of preferences can be speciﬁed as follows:
Let X be the set of data points, err : X × H → R an er-
ror function to measure residuals eij between data xi and
models hj , and ǫ an inlier threshold provided as input.
The method starts by generating a set of random tentative
models H = {h1, . . . , hm} by drawing m subsets of data
points with the minimum cardinality necessary to instanti-
ate a model. Then an n × m matrix P is built by deﬁning
its (i, j)-th entry as

P (i, j) =(φ(eij)

0

if eij = err(xi, hj) ≤ ǫ
otherwise.

(1)

φ is a decreasing function with range in [0, 1] and such that
φ(0) = 1. In this work φ was set to be a Gaussian function
φ(r) = exp(−e2/σ2) where σ2 = −ǫ2/ log(0.05) in order
to grant a preference of 0.05 in correspondence of residu-
als with magnitude ǫ. Each row Pi can be easily identiﬁed
with a preference function pf(xi) of a given point xi. The
rationale is that points belonging to the same model will
have similar preference, and therefore can be clustered in
the conceptual space [0, 1]m to reveal the structures hidden
in the data.

The preference function of a subset Y ⊆ X is composed

by the models that ﬁt all the points in Y :

pf(Y ) = min
x∈Y

pf(x).

(2)

The clustering algorithm proceeds in a bottom-up man-
ner. At ﬁrst every data point is put in its own cluster.
The distance between clusters is computed as the Tani-
moto distance [21] between the respective preference func-
tions. Starting from singletons, each sweep of the algo-
rithm merges the two clusters with the smallest Tanimoto
distance, until all the preference sets of clusters are disjoint.
The parameters of the returned models are estimated by
least squares ﬁtting on each cluster of points. It is worth
noting that, if outliers are not present in the data, the num-
ber of clusters is automatically detected by this algorithm.
Moreover this preference approach is robust to outliers, that
can be recognised as observations whose preferences devi-
ate signiﬁcantly from the rest of the data, and tend to emerge
as micro-clusters, that can be pruned out a posteriori with
probabilistic reasoning. The inlier threshold needs to be
speciﬁed as input.

7462

In principle, T-linkage could be used to extract a seg-
mentation from a multi-class soup of heterogeneous mod-
els, but in this way one cannot recover the models underly-
ing the clustering, as the class of the models are lost during
the clustering fusion, in much the same way as in [29].

In the next section we shall describe our general solution

based on T-linkage.

3.2. Multi class Cascaded T linkage

Without loss of generality, suppose we want to ﬁt a mix-
ture of models belonging to two nested classes, namely A
and B. In other words, we search for a partition of the data

of the form X = Si Ai ∪Sj Bj , where Ai = cs(αi) and

Bj = cs(βj) denote the consensus sets associated to mod-
els αi ∈ A and βj ∈ B respectively. The compact nota-
tion cs(µ) = {x ∈ X : err(x, µ) ≤ ǫ} implicitly assumes
a convenient error function for each class of model and a
proper inlier threshold to handle noise and to encode the
desired structure resolutions. The number of structures of
both classes is unknown.

Let A be the more “general” class, in the sense that the
structures underlying the simpler models can be also de-
scribed in terms of models of type A without impacting data
ﬁdelity. On the other hand, there are instances in A that can
not be explained by means of a unique model of B keep-
ing the same level of accuracy. One can formalise this as
follows:

1. ∀Bj = cs(βj), βj∈B (∃αk∈A (Bj⊆ cs(αk)))
2. ∃Ai = cs(αi), αi∈A (∀βk∈B (Ai 6= cs(βk))).

Classes of parametric models of varying complexity fall un-
der this category, e.g. polynomials of different degree. For
instance, A may consists of circles and B of lines. While
a ﬁnite set of collinear points can be explained by a single
circle of adequate radius (not uniquely determined) – see
Fig. 1a – points in general position on a circumference can
not be described by lines with a similar ﬁtting error, and
multiple instances are needed to achieve a similar accuracy.
Another example consists of fundamental matrices and ho-
mographies: two images of a planar object can be related
either by a homography or a fundamental matrix, even if
the latter is not unique, but a single homography can not be
used to describe with the same accuracy the epipolar geom-
etry of a full 3D scene.

The pivotal observation is straightforward. Since every
structure Bj can be described by models in A, we may
rewrite Bj = cs(αk) and, in ﬁrst instance, look for a parti-

tion PA of the form X = Si Ai ∪Sk Ak entirely induced

by A. This can be easily obtained with a ﬁrst run of T-
linkage where the pool of tentative hypotheses is instanti-
ated by randomly sampling models of type A.

To recover the less complex structures in B, we can re-
strict our search to the reﬁnement of the aforementioned

(a)

(b)

(c)

Figure 1: Circle A and lines B as nested models. (a) a circle
can describe a set of collinear points, but it is not uniquely
determined. (b) Sampling with geometric constraints from
the segmentation induced by A. A single point is sufﬁcient
to instantiate a line provided that it is tangent to the under-
lying circle. (c) The segmentation induced by B.

partition PA. Note that it may happen that some general
models explain several nested structures – such as the cir-
cle in Fig. 1 whose consensus set includes the points on the
sides of the octagon. The solution is therefore to perform T-
linkage separately on every A structure, and then to adopt a
model selection criterion to compare the sub-partitions in-
duced by models in B internally on each Ai. As an illus-
tration, in the case of Fig. 1 the model selection is used to
decide between the interpretation of the data provided by
the green circle and the one composed by the eight lines of
the octagon perimeter. More generally, the model selection
problem addressed here always compares a model of type
A with one or several models of type B.

The pool of tentative models of type B needed to perform
T-linkage are generated enforcing geometric compatibility
with the attained general models.

Our algorithm, called Multi-class Cascaded T-linkage or
MCT in short, is summarised below. Points 4,7, and 8 will
be expanded in the next paragraphs.

Algorithm 1 Multi-class Cascaded T-Linkage (MCT)

1. Extract models of class A with T-linkage
2. Reject outliers
3. For each A model, perform T-linkage sampling nested

models B compatible with A

4. Solve a model selection problem on each subset
5. Extract models of class B from the outliers with T-

linkage

6. Reject outliers
7. Reﬁne models

Please note that the algorithm does not have to compare
all the possible combinations of type B models against the
whole type A models since, inside each consensus set of the
more general model, the selection of more speciﬁc struc-
tures is performed by the inner T-Linkage, which is also

7463

able to automatically determine the correct number of in-
stances. The ﬁrst consensus set and all its nested structures
(as extracted by T-Linkage) are hence compared through
model selection.

σ(cid:1)2
2P(cid:0) ei

Model selection. Several model selection criterion have
Let us call L =
been proposed in the literature.
− 1
where ei/σ are the normalised residuals, and
let n be the number of data points, d the dimension of model
manifold, k the number of model parameters and r the di-
mension of the measurement space (where data points be-
long); let p = dn + k the total number of parameters to
be estimated. The following are the model selection criteria
we took into consideration (from [26]):

AIC − 2L + 2p

BIC − 2L + p log(rn)

MDL − 2L + p/2 log2(rn)
GBIC − 2L + log(r)dn + log(rn)k

GRIC − 2L + λ1dn + λ2k

(3)

(4)

(5)

(6)

(7)

When ℓ ≥ 2 type B models are compared against one type
A model, the score is computed substituting k with ℓk to
account for the actual number of parameters.

In our tests the GRIC resulted as the most versatile one,
thanks to the tuning parameters λ1 and λ2. In our experi-
ments we set λ1=1 and adjusted λ2 as reported in Tab. 1.

Line
Conic

Plane
Cylinder

Fundamental
Homography

k

2
3

4
5

7
8

d

1
1

2
2

3
2

r

2

3

4

λ1

λ2

1

1

1

3

4

2.5

Table 1: Model selection parameters

Outlier rejection. Following the route of the a-contrario
approaches, outlying models are pruned using the statistical
validation technique described in [23], the main difference
is that we use it as a post-processing outlier rejection cri-
teria on the attained models rather than as a pre-processing
reﬁnement of sampled model hypotheses. The idea is to
compute the distribution of the cardinality of the consensus
set of a model varying its inlier threshold and to express its
unlikeliness in terms of NFA. In the most simple terms, if
there are n inliers at a distance ǫ from a model, assuming
a local uniform distribution of the residuals, we expect to
have on average κ times more elements at a distance κǫ. If
this does not hold, the model has a very low probability of
occurring by chance and can be retained as inliers.

Reﬁnement. A ﬁnal reﬁnement step is carried out with
the double goal of removing small models subsumed by
a larger one, that may occasionally arise, and reassigning
points to models in such a way that the result is not a parti-
tion of the points but a cover, which is a more appropriate
output for multi-model ﬁtting, as argued in [9]. With these
aims, we re-assign the inlier points to the models produced
by the previous step (each point is assigned to the models
for which it is an inlier) and then solve a set cover problem
in a greedy fashion, selecting the models in decreasing car-
dinality order, until all the inlier points have been covered
by at least one model.

4. Experimental validation

This section is devoted to investigating the performances
of MCT in several multi-class model ﬁtting tasks, namely
2D and 3D primitive-ﬁtting problems (in Sec.4.1 and 4.2 re-
spectively) and two-views motion segmentation (Sec. 4.3).
The inlier threshold ε have been tuned per-problem. The
values of λ1 and λ2 are reported in Tab. 1. As custom-
ary the ﬁgure of merit is the misclassiﬁcation error (ME),
i.e. the percentage of misclassiﬁed point with respect to a
ground truth labelling. Since the deﬁnition of ME does not
envisage multiple labels, we assigned each inlier point to its
closest model for the sole purpose of computing the ME.

The Matlab code of MCT is available on-line [30].

4.1. Line and conic ﬁtting

To begin with, we take into account some synthetic 2D
primitive ﬁtting problems. Speciﬁcally, in Fig. 2 we attempt
to reproduce the experiment presented in [2]; the aim is to
extract lines, parabolas and circles from data contaminated
with 33% of outliers and a small amount of Gaussian noise.
It is interesting to note that, our method succeeds in
dealing also with structures which do not conform with
the nested-model assumption: we treat the linear and the
parabolic model as nested ones, circles represent a third ad-
ditional category that is extracted at the end from the out-
liers of the other two models. This is possible thanks to the
reﬁnement step, which remedies to the over-segmentation
caused by the fact that the lower and the upper circular
sectors of a circle can be accurately described as well by
parabolas.

In second instance, we consider the problems collected
in Fig. 3, where lines and circles are arranged in more
complex conﬁgurations, resembling somehow blueprints of
buildings. Tentative lines in circular structures are instanti-
ated by sampling individual points and enforcing the direc-
tion of tangency with the circumference.

In Tab. 2 MCT is contrasted with PEARL[31] in terms
of misclassication error (ME). Both methods were given
the same preference matrix as input and results come from
a single run;
the parameters of both have been tuned to

7464

achieve the best results and kept ﬁxed in all instances. MCT
consistently achieves the most accurate result in terms of
ME for all the datasets.

(a)

(b)

(c)

Figure 2: Line and conic ﬁtting on synthetic data generated
as in [2]. Detection of multiple lines, parabolas and circles
by MCT. Model assignment is colour coded.

(d)

(e)

(f)

(g)

Figure 3: Line and conic ﬁtting on ”blueprint-like” syn-
thetic data. Top row: input data. Bottom row: detection
of multiple lines and circles by MCT. Model assignment is
colour coded.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

PEARL [6]
MCT

6.00
0.67

16.22
2.00

14.44
2.33

8.05
5.23

8.33
7.12

17.38
5.38

19.21
6.23

Table 2: ME (%) for line and conic ﬁtting instances. Let-
ters refers to Figs. 2 and 3.

To complete this set of experiments, we took in consid-
eration circle and line detection from edge maps of real im-
ages. In Fig. 4 we collated the result of MCT with those of
ELSD [11] a state-of-the-art native ellipse and line detector
algorithm based on a-contrario reasoning speciﬁcally tai-
lored for edge map classiﬁcation and available online [32].
The results are promising, as our method succeeds in pro-
viding similar or even slightly better results without any fur-
ther adaptations or speciﬁc assumptions.

4.2. Plane and cylinder ﬁtting

Then we consider the problem of ﬁtting 3D primitives
to spatial data. Results on plane and cylinder ﬁtting are

T
C
M

]
1
1
[

D
S
L
E

Figure 4: Line and conic ﬁtting. Detection of multiple
lines and circles on real images. Edgels class assignment is
colour coded (circle / line) .

presented in Fig. 5. Since ground-truth is not available for
these data, evaluation can be only subjective. The ﬁrst two
point clouds, representing a bridge and a nut, are taken from
the Aim@Shape repository [33], while the third one is the
Pozzoveggiani dataset [34], which has been used in several
works, e.g. [12, 22, 9], but never in a multi-class framework:
as a matter of fact, usually only planes are detected and the
abside, which is a semi-cylinder, is left out.

The beneﬁts of the the ﬁnal reﬁnement with set cover are
shown in the bridge example, where portions of the pier
have been ﬁtted with cylinders which however ended up
to be redundant because they were already covered by the
plane that ﬁtted the rest of the facade of the bridge. In the
other two cases the reﬁnement brought no visible improve-
ment.

4.3. Homography and fundamental matrix ﬁtting

To conclude, in this section we consider homographies
and fundamental matrices. We used the ”cube*” subset of
the AdelaideRMF motion dataset [28], consisting of 8 im-
age pairs depicting a cube and other objects, with ground
truth point correspondences assigned to each object. In ad-
dition we manually labelled correspondences according to
the face of the cube they belong to. These examples pro-
vide a true multi-class problem, where homographies (cube
faces) and fundamental matrices (other objects) coexist, and
the homographies are contained in a fundamental model.
As the constrained sampling is concerned, we exploit the
following compatibility constraint [7, 3]: H is said to be
compatible with F if and only if the product H ⊤F is skew-
symmetric, in formula:

H ⊤F + F ⊤H = 0.

(8)

In practice, following [5, 4] once a fundamental matrix F is
recovered, three correspondence {mi ↔ m′
i=1 are sam-
pled to determine a compatible planar projectivity as

i}3

H = A − e′(M −1b)⊤,

(9)

7465

Image of the scene

colour coded class

colour coded model

Figure 5: Plane and cylinder ﬁtting by MCT on 3D point clouds from Aim@Shape. The central column shows the class
(point/cylinder) each point belongs to, whereas the rightmost column depicts the model each point belongs to. For the bridge
the class/model assignment is show before (top) and after (bottom) the ﬁnal reﬁnement with set cover.

where A = [e′]×F , [e′]× is the cross-product matrix of the
epipole in the second view, b denotes the 3-dimensional vec-
tor deﬁned component-wise as

Multi-H Multi-X T-lnkg MCT

mean
median

14.35
9.56

9.72
2.49

6.60
4.68

6.13
4.93

bi = (m′

i × (Ami))⊤(m′

i × e′)km′

i × e′k−2,

(10)

and M is a matrix collecting m⊤

i by rows.

Result are shown in Fig. 6, where it can be appreciated
that, in all the cases, the model selection mechanism pro-
motes reliably the extraction of homography in correspon-
dence of the planar facets of the cube while distinguishing,
at the same time, the remaining moving object through fun-
damental matrices. Moreover, thanks to constrained sam-
pling, the homographies estimated from the faces of the
cube are geometrically consistent with the 3D motion of the
object. The mean run time per image pair is 21s in Matlab
on 2.6 GHz i7 machine.

To allow a comparison (albeit somehow contrived) with
Multi-X and T-linkage [30], following the same protocol
used in [2], we applied MCT to a single class problem,

Table 3: ME (%) on AdelaideRMF for multiple homogra-
phy ﬁtting. The ﬁrst two columns are copied from [2].

namely multiple homography ﬁtting on the AdelaideRMF
homography dataset [28], consisting of 19 image pairs with
ground truth point correspondences assigned to planes (ho-
mographies). In order to bias MCT toward homographies
we set λ1 = 500 and λ2 = 0 in this experiment. Re-
sult shown in Tab. 3 are mixed: Multi-X achieved the best
median error, whereas MCT has the lowest mean error. In
this case MCT works almost like regular T-linkage, except a
ﬁrst clustering is made with F matrices that biases the sub-
sequent sampling of homographies. As observed by [20],
initial recovery of an F matrix can proﬁtably guide the sam-

7466

breadcube

cube

cubechips

cubetoy

ME 5.58%

ME 3.05%

ME 1.08%

ME 6.69%

breadcubechips

toycubecar

carchipscube

cubebreadtoychips

ME 6.52%

ME 10.61%

ME 5.90%

ME 3.82%

Figure 6: Homography and fundamental matrix ﬁtting by MCT on cube* image pairs of the AdelaideRMF dataset. We
sampled 5000 F matrices and 1000 H matrices per cluster. Odd rows: ﬁrst input image with outlier-contaminated points
overlaid. Even rows: F and H models superimposed on the second image (model assignment is colour coded).

pling towards valid homographies and also gain robustness
against outliers.

5. Conclusions

We presented a multi-model recovery framework that
offers a hierarchical interpretation of data in nested com-
patible models. Speciﬁcally, we conceived MCT, a cas-
caded extension of T-linkage that sequentially extract sim-
pler nested models starting form the more general ones. We
leverage on geometric constraints to guide the sampling to-
wards simpler valid interpretation of the data.

In this way, all the intra-class model selection problems
are implicitly solved by T-linkage, whereas the inter-class
model selection issues is cast into an explicit comparison of
GRIC scores among models explaining the same data.

Our model selection framework is simple and in prin-
ciple is agnostic about the multi-model ﬁtting technique
adopted. In practice it can be generalised to other prefer-
ence based multi-model ﬁtting algorithm which can beneﬁt
from model-constrained sampling.

7467

References

[1] P. Amayo, P. Pini´es, L. M. Paz, and P. Newman. Geomet-
ric multi-model ﬁtting with a convex relaxation algorithm.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 8138–8146, 2018. 2

[2] D. Barath and J. Matas. Multi-class model ﬁtting by energy
minimization and mode-seeking. CoRR, abs/1706.00827,
2017. 2, 5, 6, 7

[3] P. Chen and D. Suter. Simultaneously estimating the fun-
damental matrix and homographies. IEEE Transactions on
Robotics, 25(6):1425–1431, 2009. 6

[4] O. Chum, T. Werner, and J. Matas. Two-view geometry esti-
mation unaffected by a dominant plane. In Computer Vision
and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
Society Conference on, volume 1, pages 772–779. IEEE,
2005. 6

[5] R. Hartley and A. Zisserman. Multiple view geometry in

computer vision. Cambridge university press, 2003. 6

[6] H. Isack and Y. Boykov. Energy-based geometric multi-
International Journal of Computer Vision,

model ﬁtting.
97(2):123–147, 2012. 2, 6

[7] Q.-T. Luong and T. Vi´eville. Canonical representations for
the geometries of multiple projective views. Computer vision
and image understanding, 64(2):193–229, 1996. 6

[8] L. Magri and A. Fusiello. T-Linkage: A continuous relax-
ation of J-Linkage for multi-model ﬁtting.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3954–3961, June 2014. 3

[9] L. Magri and A. Fusiello. Multiple models ﬁtting as a set
coverage problem. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 3318–
3326, June 2016. 5, 6

[10] L. Moisan, P. Moulon, and P. Monasse. Automatic homo-
graphic registration of a pair of images, with a contrario
elimination of outliers. Image Processing On Line, 2:56–73,
2012. 3

[11] V. Patraucean, P. Gurdjos, and R. G. von Gioi. Joint a con-
trario ellipse and line detection. IEEE Trans. Pattern Anal.
Mach. Intell., 39(4):788–802, 2017. 3, 6

[12] T. T. Pham, T.-J. Chin, K. Schindler, and D. Suter. Interacting
geometric priors for robust multimodel ﬁtting. IEEE Trans-
actions on Image Processing, 23(10):4601–4610, 2014. 2,
6

[13] J. Rabin, J. Delon, Y. Gousseau, and L. Moisan. Mac-ransac:
a robust algorithm for the recognition of multiple objects. In
Fifth International Symposium on 3D Data Processing, Visu-
alization and Transmission (3DPTV 2010), page 051, 2010.
3

[14] K. Schindler and D. Suter. Two-view multibody structure-
and-motion with outliers.
2005 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition
(CVPR’05), 2:643–648 vol. 2, 2005. 2

[15] K. Schindler, D. Suter, and H. Wang. A Model-Selection
Framework for Multibody Structure-and-Motion of Image
Sequences.
International Journal of Computer Vision,
79(2):159–177, 2008. 2

[16] C. V. Stewart. MINPRAN: A new robust estimator for com-
puter vision. Pattern Analysis and Machine Intelligence,
17(10):925–938, 1995. 3

[17] M. Stricker and A. Leonardis. Exsel++: A general frame-
work to extract parametric models. In International Confer-
ence on Computer Analysis of Images and Patterns, pages
90–97. Springer, 1995. 2

[18] Y. Sugaya and K. Kanatani. Multi-stage unsupervised learn-
ing for multi-body motion segmentation.
IEICE Transac-
tions on Information and Systems, 87(7):1935–1942, 2004.
2

[19] Y. Sugaya and K. Kanatani.

Improved multistage learning
for multibody motion segmentation. In VISAPP (1), pages
199–206, 2010. 2

[20] Z. L. Szpak, W. Chojnacki, and A. van den Hengel. Ro-
bust multiple homography estimation: An ill-solved prob-
lem. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, June 2015. 7

[21] T. Tanimoto. An elementary mathematical theory of classiﬁ-
cation and prediction. Internal technical report, IBM, 1957.
3

[22] M. Tepper and G. Sapiro. A biclustering framework for
consensus problems. SIAM Journal on Imaging Sciences,
7(4):2488–2525, 2014. 6

[23] M. Tepper and G. Sapiro. Fast L1-NMF for multiple para-
metric model estimation. arXiv preprint arXiv:1610.05712,
2016. 5

[24] P. H. Torr, A. W. Fitzgibbon, and A. Zisserman. The problem
of degeneracy in structure and motion recovery from uncali-
brated image sequences. International Journal of Computer
Vision, 32(1):27–44, 1999. 2

[25] P. H. S. Torr. Geometric motion segmentation and model
selection. Philosophical Transactions of the Royal Society
of London, A 356:1321–1340, 1998. 2

[26] P. H. S. Torr. Model Selection for Two View Geometry:A
Review, pages 277–301. Springer Berlin Heidelberg, Berlin,
Heidelberg, 1999. 5

[27] R. Vidal, Y. Ma, and S. Sastry. Generalized principal com-
IEEE Trans. Pattern Anal. Mach.

ponent analysis (gpca).
Intell., 27(12):1945–1959, 2005. 3

[28] H. S. Wong, T.-J. Chin, J. Yu, and D. Suter. Dynamic and
hierarchical multi-structure geometric model ﬁtting. In Pro-
ceedings of the International Conference on Computer Vi-
sion, 2011. 6, 7

[29] X. Xu, L. F. Cheong, and Z. Li. Motion segmentation by
exploiting complementary geometric models.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2859–2867, 2018. 3, 4

[30] http://www.diegm.uniud.it/fusiello/demo/jlk. 5, 7
[31] http://mouse.cs.uwaterloo.ca/code/gco-v3.0.zip. 5
[32] http://dev.ipol.im/∼jirafa/ipol demo/elsdc/. 6
[33] http://visionair.ge.imati.cnr.it/ontologies/shapes/

viewmodels.jsp. 6

[34] http://www.diegm.uniud.it/fusiello/demo/samantha/. 6

7468

