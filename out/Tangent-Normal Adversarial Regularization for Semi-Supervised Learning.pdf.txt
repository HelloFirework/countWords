Tangent-Normal Adversarial Regularization for Semi-supervised Learning

Bing Yu1∗,

Jingfeng Wu1∗∗,

Jinwen Ma1, Zhanxing Zhu123††

1School of Mathematical Sciences, Peking University

2Center for Data Science, Peking University

3 Beijing Institute of Big Data Research

{byu,pkuwjf}@pku.edu.cn

jwma@math.pku.edu.cn

zhanxing.zhu@pku.edu.cn

Abstract

Compared with standard supervised learning, the key
difﬁculty in semi-supervised learning is how to make full
use of the unlabeled data. A recently proposed method,
virtual adversarial training (VAT), smartly performs adver-
sarial training without label information to impose a lo-
cal smoothness on the classiﬁer, which is especially bene-
ﬁcial to semi-supervised learning. In this work, we propose
tangent-normal adversarial regularization (TNAR) as an
extension of VAT by taking the data manifold into consid-
eration. The proposed TNAR is composed by two comple-
mentary parts, the tangent adversarial regularization (TAR)
and the normal adversarial regularization (NAR). In TAR,
VAT is applied along the tangent space of the data mani-
fold, aiming to enforce local invariance of the classiﬁer on
the manifold, while in NAR, VAT is performed on the nor-
mal space orthogonal to the tangent space, intending to im-
pose robustness on the classiﬁer against the noise causing
the observed data deviating from the underlying data man-
ifold. Demonstrated by experiments on both artiﬁcial and
practical datasets, our proposed TAR and NAR complement
with each other, and jointly outperforms other state-of-the-
art methods for semi-supervised learning.

1. Introduction

The main challenge in semi-supervised learning (SSL)
is how to utilize the large amount of the unlabeled data to
obtain useful information, beneﬁting the supervised learn-
ing on the relatively insufﬁcient amount of labeled data. For
this purpose, one of the important line of research focuses
on the manifold assumption on the data distribution, i.e., the
observed data is distributed on a low dimensional manifold
that could be characterized using the large amount of the un-
labeled data, and aims to learn a proper classiﬁer based on

∗Equal contributions.
†Corresponding author.

the data manifold [1, 24, 19, 11, 13]. Following this stream,
we sort out three reasonable assumptions to motivate our
idea for semi-supervised learning:

The manifold assumption The observed data x presented
in high dimensional space RD is with high probability
concentrated in the vicinity of some underlying man-
ifold with much lower dimensionality [3, 18, 4, 24],
denoted as M ∼= Rd.

The noisy observation assumption The observed data x
can be decomposed into two parts as x = x0 + n,
where x0 is exactly supported on the underlying mani-
fold M and n is some noise independent of x0 [2, 23].

The semi-supervised learning assumption If two points
x1, x2 ∈ M are close in manifold distance, then the
conditional probability p(y|x1) and p(y|x2) are simi-
lar [1, 24, 19]. In other words, the true classiﬁer, or
the true condition distribution p(y|X) varies smoothly
along the underlying manifold M.

According to the three assumptions, the best classiﬁer we
aim to obtain should be 1) smooth along the data manifold;
2) robust to the off-manifold noise. Hence it is natural to
formulate a loss function [1, 11] for SSL as,

Lssl := Lsupervised + Rmanifold + Rnoise,

(1)

where the ﬁrst term in Eq. (1) is the supervised learning loss,
the second term penalize the manifold smoothness of the
classiﬁer, and the third term smooths the classiﬁer so that it
is robust to noise, respectively. While the supervised learn-
ing loss Lsupervised concerning the labeled data is standard,
the key ingredient lies on how to design Rmanifold and Rnoise
smartly to 1) be effective for inducing the desired smooth-
ness on the classiﬁer, 2) be efﬁcient for optimization, and 3)
make full use of the unlabeled data.

Existing works construct Rmanifold based on the Jacobian,

for instance, tangent propagation [26, 11]

Rmanifold = E

x∼p(x) Xv∈TxM(cid:13)(cid:13)(Jxf ) · v(cid:13)(cid:13) ,

(2)

10676

x = x0 + n

r⊥

x0<

rk

M<

Figure 1. Illustration for tangent-normal adversarial regularization.
x = x0 + n is the observed data, where x0 is exactly supported
on the underlying manifold M and n is the noise independent of
x0. rk is the adversarial perturbation along the tangent space to in-
duce invariance of the classiﬁer on manifold; r⊥ is the adversarial
perturbation along the normal space to impose robustness on the
classiﬁer against noise n.

and manifold Laplacian norm [1, 13, 22]

Rmanifold = Zx∈M(cid:13)(cid:13)∇Mf (x)(cid:13)(cid:13) dp(x)
z∼p(z)(cid:13)(cid:13)∇Mf (g(z))(cid:13)(cid:13)
z∼p(z)(cid:13)(cid:13)Jzf (g(z))(cid:13)(cid:13) ,

≈ E

≈ E

(3)

where J is the Jacobian, f is the classiﬁer, TxM is the tan-
gent space of the data manifold and x = g(z) is the man-
ifold representation of data. They regularize the manifold
smoothness of the classiﬁer under the sense of the norm of
its Jacobian along the data manifold. The typical choice of
Rnoise is in a corresponding form as Rmanifold except the Ja-
cobian to penalize is with respect to the observation space
other than the tangent space [1, 11].

On the other hand, inspired by adversarial training [8],
virtual adversarial training (VAT) [17, 16] was proposed
for SSL, not relying on the label information. Unlike the
smoothness induced by Lp-norm of the Jacobian, VAT leads
to the robustness of classiﬁer by involving virtual adversar-
ial examples, thus inducing a new local smoothness of the
classiﬁer. Empirical results [17, 21] show that VAT achieves
state-of-the-art performance for SSL tasks, demonstrating
the superiority of the smoothness imposed by virtual adver-
sarial training.

Encouraged by the effectiveness of VAT, we propose to
construct manifold regularizer based on VAT, instead of the
Lp-norm of the Jacobian. Concretely, we propose tangent
adversarial regularization (TAR) by performing VAT along
the tangent space of the data manifold, and normal adver-
sarial regularization (NAR) by applying VAT orthogonal
to the tangent space of the data manifold, which are intu-
itively demonstrated in Figure 1. TAR enforces the local
smoothness of the classiﬁer along the underlying manifold,
while NAR imposes robustness on the classiﬁer against the
noise carried in the observed data. The two terms, comple-
menting with each other, establish our proposed approach
tangent-normal adversarial regularization (TNAR).

To realize TNAR, we have two challenges to conquer:

1) how to estimate the underlying manifold and 2) how
to efﬁciently perform TNAR. For the ﬁrst issue, we take
advantage of the generative models equipped with an ex-
tra encoder, to characterize the coordinate chart of mani-
fold [11, 13, 22]. More speciﬁcally, in this work we choose
variational autoendoer (VAE) [10] and localized GAN [22]
to estimate the underlying manifold from data. For the sec-
ond problem, we further extend the techniques introduced
in [17] with some sophisticatedly designed auxiliary func-
tions, implementing VAT restricted in tangent space (TAR)
and normal space (NAR) efﬁciently. The details are elabo-
rated in Section 3.

The remaining of the paper is organized as follows. In
Section 2 we introduce VAT and two generative models as
the background of TNAR. Based on that, we elaborate about
the technical details of TNAR in Section 3. In Section 4
we compare TNAR with other related approaches and ana-
lyze the advantages of TNAR over VAT and other manifold-
based regularization. Various experiments are conducted
for demonstrating the effectiveness of TNAR in Section 5.
And in Section 6 and Section 7, we discuss an existing prob-
lem about TNAR for future exploration and conclude the
paper.

2. Background

2.1. Notations

The labeled and unlabeled dataset are denoted as Dl =
{(xl, yl)} and Dul = {xul} respectively, thus D := Dl ∪
Dul is the full dataset. The output of classiﬁcation model
is written as p(y|x, θ), with θ being the model parameters
to be trained. ℓ(·, ·) represents the supervised loss function.
For data example, the observed space RD and the under-
lying manifold is M. The decoder (generator) and the en-
coder are denoted as g and h respectively, which form the
coordinate chart of manifold together. If not stated other-
wise, we always assume x and z correspond to the coordi-
nate of the same data point in observed space RD and on
manifold M, i.e., g(z) = x and h(x) = z. The tangent
space of M at point x is TxM = Jzg(Rd) ∼= Rd, where
Jzg is the Jacobian of g at point z. TxM is also the span
of the columns of Jzg. We use J to represent the Jacobian
when there is no ambiguity.

The perturbation in the observed space RD is denoted
as r ∈ RD, while the perturbation on the manifold rep-
resentation is denoted as η ∈ Rd. Hence the perturba-
tion on manifold is g(z + η) − g(z). When the pertur-
bation η is small enough for the holding of the ﬁrst or-
der Taylor’s expansion, the perturbation on manifold is ap-
proximately equal to the perturbation on its tangent space,
g(z + η) − g(z) ≈ J · η ∈ TxM. Therefore we say a
perturbation r ∈ RD is actually on manifold, if there is a
perturbation η ∈ Rd, such that r = J · η.

10677

l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
Z
j
t
D
I
t
e
A
F
0
f
b
O
t
p
X
/
k
4
o
q
q
X
C
s
w
=
"
>
A
A
A
C
H
X
i
c
b
V
D
L
S
s
N
A
F
J
3
U
V
4
2
v
q
k
s
3
g
6
W
g
I
C
U
R
Q
U
E
K
R
T
d
u
l
A
q
2
F
p
o
a
J
t
N
J
O
z
h
5
M
H
M
j
D
S
E
/
4
s
Z
f
c
e
N
C
E
R
d
u
x
L
9
x
+
l
j
4
O
n
D
h
c
M
6
9
3
H
u
P
F
w
u
u
w
L
I
+
j
c
L
M
7
N
z
8
Q
n
H
R
X
F
p
e
W
V
0
r
r
W
+
0
V
J
R
I
y
p
o
0
E
p
F
s
e
0
Q
x
w
U
P
W
B
A
6
C
t
W
P
J
S
O
A
J
d
u
3
d
n
o
7
8
6
z
s
m
F
Y
/
C
K
0
h
j
1
g
1
I
P
+
Q
+
p
w
S
0
5
J
Y
O
K
k
6
G
h
y
7
f
w
6
n
L
n
d
z
N
e
M
3
O
b
y
7
M
S
o
p
r
2
N
8
Z
H
m
M
H
B
g
z
I
r
u
k
E
B
A
a
U
i
O
w
8
d
0
t
l
q
2
q
N
g
f
8
S
e
0
r
K
a
I
q
G
W
3
p
3
e
h
F
N
A
h
Y
C
F
U
S
p
j
m
3
F
0
M
2
I
B
E
4
F
y
0
0
n
U
S
w
m
9
J
b
0
W
U
f
T
k
A
R
M
d
b
P
x
d
z
m
u
a
K
W
H
/
U
j
q
C
g
G
P
1
e
8
T
G
Q
m
U
S
g
N
P
d
4
5
O
V
L
+
9
k
f
i
f
1
0
n
A
P
+
p
m
P
I
w
T
Y
C
G
d
L
P
I
T
g
S
H
C
o
6
h
w
j
0
t
G
Q
a
S
a
E
C
q
5
v
h
X
T
A
Z
G
E
g
g
7
U
1
C
H
Y
v
1
/
+
S
1
r
7
V
d
u
q
2
p
c
H
5
f
r
J
N
I
4
i
2
k
L
b
a
A
f
Z
6
B
D
V
0
R
l
q
o
C
a
i
6
B
4
9
o
m
f
0
Y
j
w
Y
T
8
a
r
8
T
Z
p
L
R
j
T
m
U
3
0
A
8
b
H
F
+
U
0
n
/
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
Z
j
t
D
I
t
e
A
F
0
f
b
O
t
p
X
/
k
4
o
q
q
X
C
s
w
=
"
>
A
A
A
C
H
X
i
c
b
V
D
L
S
s
N
A
F
J
3
U
V
4
2
v
q
k
s
3
g
6
W
g
I
C
U
R
Q
U
E
K
R
T
d
u
l
A
q
2
F
p
o
a
J
t
N
J
O
z
h
5
M
H
M
j
D
S
E
/
4
s
Z
f
c
e
N
C
E
R
d
u
x
L
9
x
+
l
j
4
O
n
D
h
c
M
6
9
3
H
u
P
F
w
u
u
w
L
I
+
j
c
L
M
7
N
z
8
Q
n
H
R
X
F
p
e
W
V
0
r
r
W
+
0
V
J
R
I
y
p
o
0
E
p
F
s
e
0
Q
x
w
U
P
W
B
A
6
C
t
W
P
J
S
O
A
J
d
u
3
d
n
o
7
8
6
z
s
m
F
Y
/
C
K
0
h
j
1
g
1
I
P
+
Q
+
p
w
S
0
5
J
Y
O
K
k
6
G
h
y
7
f
w
6
n
L
n
d
z
N
e
M
3
O
b
y
7
M
S
o
p
r
2
N
8
Z
H
m
M
H
B
g
z
I
r
u
k
E
B
A
a
U
i
O
w
8
d
0
t
l
q
2
q
N
g
f
8
S
e
0
r
K
a
I
q
G
W
3
p
3
e
h
F
N
A
h
Y
C
F
U
S
p
j
m
3
F
0
M
2
I
B
E
4
F
y
0
0
n
U
S
w
m
9
J
b
0
W
U
f
T
k
A
R
M
d
b
P
x
d
z
m
u
a
K
W
H
/
U
j
q
C
g
G
P
1
e
8
T
G
Q
m
U
S
g
N
P
d
4
5
O
V
L
+
9
k
f
i
f
1
0
n
A
P
+
p
m
P
I
w
T
Y
C
G
d
L
P
I
T
g
S
H
C
o
6
h
w
j
0
t
G
Q
a
S
a
E
C
q
5
v
h
X
T
A
Z
G
E
g
g
7
U
1
C
H
Y
v
1
/
+
S
1
r
7
V
d
u
q
2
p
c
H
5
f
r
J
N
I
4
i
2
k
L
b
a
A
f
Z
6
B
D
V
0
R
l
q
o
C
a
i
6
B
4
9
o
m
f
0
Y
j
w
Y
T
8
a
r
8
T
Z
p
L
R
j
T
m
U
3
0
A
8
b
H
F
+
U
0
n
/
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
Z
j
t
D
I
t
e
A
F
0
f
b
O
t
p
X
/
k
4
o
q
q
X
C
s
w
=
"
>
A
A
A
C
H
X
i
c
b
V
D
L
S
s
N
A
F
J
3
U
V
4
2
v
q
k
s
3
g
6
W
g
I
C
U
R
Q
U
E
K
R
T
d
u
l
A
q
2
F
p
o
a
J
t
N
J
O
z
h
5
M
H
M
j
D
S
E
/
4
s
Z
f
c
e
N
C
E
R
d
u
x
L
9
x
+
l
j
4
O
n
D
h
c
M
6
9
3
H
u
P
F
w
u
u
w
L
I
+
j
c
L
M
7
N
z
8
Q
n
H
R
X
F
p
e
W
V
0
r
r
W
+
0
V
J
R
I
y
p
o
0
E
p
F
s
e
0
Q
x
w
U
P
W
B
A
6
C
t
W
P
J
S
O
A
J
d
u
3
d
n
o
7
8
6
z
s
m
F
Y
/
C
K
0
h
j
1
g
1
I
P
+
Q
+
p
w
S
0
5
J
Y
O
K
k
6
G
h
y
7
f
w
6
n
L
n
d
z
N
e
M
3
O
b
y
7
M
S
o
p
r
2
N
8
Z
H
m
M
H
B
g
z
I
r
u
k
E
B
A
a
U
i
O
w
8
d
0
t
l
q
2
q
N
g
f
8
S
e
0
r
K
a
I
q
G
W
3
p
3
e
h
F
N
A
h
Y
C
F
U
S
p
j
m
3
F
0
M
2
I
B
E
4
F
y
0
0
n
U
S
w
m
9
J
b
0
W
U
f
T
k
A
R
M
d
b
P
x
d
z
m
u
a
K
W
H
/
U
j
q
C
g
G
P
1
e
8
T
G
Q
m
U
S
g
N
P
d
4
5
O
V
L
+
9
k
f
i
f
1
0
n
A
P
+
p
m
P
I
w
T
Y
C
G
d
L
P
I
T
g
S
H
C
o
6
h
w
j
0
t
G
Q
a
S
a
E
C
q
5
v
h
X
T
A
Z
G
E
g
g
7
U
1
C
H
Y
v
1
/
+
S
1
r
7
V
d
u
q
2
p
c
H
5
f
r
J
N
I
4
i
2
k
L
b
a
A
f
Z
6
B
D
V
0
R
l
q
o
C
a
i
6
B
4
9
o
m
f
0
Y
j
w
Y
T
8
a
r
8
T
Z
p
L
R
j
T
m
U
3
0
A
8
b
H
F
+
U
0
n
/
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
Z
j
t
D
I
t
e
A
F
0
f
b
O
t
p
X
/
k
4
o
q
q
X
C
s
w
=
"
>
A
A
A
C
H
X
i
c
b
V
D
L
S
s
N
A
F
J
3
U
V
4
2
v
q
k
s
3
g
6
W
g
I
C
U
R
Q
U
E
K
R
T
d
u
l
A
q
2
F
p
o
a
J
t
N
J
O
z
h
5
M
H
M
j
D
S
E
/
4
s
Z
f
c
e
N
C
E
R
d
u
x
L
9
x
+
l
j
4
O
n
D
h
c
M
6
9
3
H
u
P
F
w
u
u
w
L
I
+
j
c
L
M
7
N
z
8
Q
n
H
R
X
F
p
e
W
V
0
r
r
W
+
0
V
J
R
I
y
p
o
0
E
p
F
s
e
0
Q
x
w
U
P
W
B
A
6
C
t
W
P
J
S
O
A
J
d
u
3
d
n
o
7
8
6
z
s
m
F
Y
/
C
K
0
h
j
1
g
1
I
P
+
Q
+
p
w
S
0
5
J
Y
O
K
k
6
G
h
y
7
f
w
6
n
L
n
d
z
N
e
M
3
O
b
y
7
M
S
o
p
r
2
N
8
Z
H
m
M
H
B
g
z
I
r
u
k
E
B
A
a
U
i
O
w
8
d
0
t
l
q
2
q
N
g
f
8
S
e
0
r
K
a
I
q
G
W
3
p
3
e
h
F
N
A
h
Y
C
F
U
S
p
j
m
3
F
0
M
2
I
B
E
4
F
y
0
0
n
U
S
w
m
9
J
b
0
W
U
f
T
k
A
R
M
d
b
P
x
d
z
m
u
a
K
W
H
/
U
j
q
C
g
G
P
1
e
8
T
G
Q
m
U
S
g
N
P
d
4
5
O
V
L
+
9
k
f
i
f
1
0
n
A
P
+
p
m
P
I
w
T
Y
C
G
d
L
P
I
T
g
S
H
C
o
6
h
w
j
0
t
G
Q
a
S
a
E
C
q
5
v
h
X
T
A
Z
G
E
g
g
7
U
1
C
H
Y
v
1
/
+
S
1
r
7
V
d
u
q
2
p
c
H
5
f
r
J
N
I
4
i
2
k
L
b
a
A
f
Z
6
B
D
V
0
R
l
q
o
C
a
i
6
B
4
9
o
m
f
0
Y
j
w
Y
T
8
a
r
8
T
Z
p
L
R
j
T
m
U
3
0
A
8
b
H
F
+
U
0
n
/
U
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
3
y
f
X
w
Z
g
e
Z
Q
A
u
E
F
B
z
g
4
J
1
L
t
a
z
q
a
A
=
"
>
A
A
A
C
L
H
i
c
b
V
D
R
S
t
x
A
F
J
1
o
W
2
1
q
a
7
S
P
f
b
m
4
L
C
g
t
S
y
K
C
g
g
h
S
X
/
p
S
U
X
D
d
h
U
0
a
J
r
M
T
d
3
A
y
C
T
M
3
s
i
H
k
g
/
r
i
r
w
i
l
D
x
X
x
1
e
9
w
d
s
1
D
q
z
1
w
4
X
D
O
v
d
x
7
T
1
J
I
Y
d
D
3
b
5
2
F
x
V
e
v
3
y
w
t
v
3
X
f
r
b
z
/
s
O
q
t
r
Z
+
b
v
N
S
M
9
1
k
u
c
z
1
M
q
O
F
S
K
N
5
H
g
Z
I
P
C
8
1
p
l
k
g
+
S
C
6
P
Z
v
7
g
i
m
s
j
c
n
W
G
V
c
G
j
j
F
4
o
k
Q
p
G
0
U
q
x
d
9
Q
N
a
5
j
G
4
g
t
U
s
Q
i
b
u
B
Y
H
Q
f
P
j
2
O
1
W
c
A
D
p
5
n
Q
f
Q
p
x
w
p
F
t
u
F
8
K
M
4
o
R
R
W
X
9
v
3
K
m
1
p
7
E
P
n
0
F
B
7
H
X
8
n
j
8
H
v
C
R
B
S
z
q
k
x
U
n
s
/
Q
r
H
O
S
s
z
r
p
B
J
a
s
w
o
8
A
u
M
a
q
p
R
M
M
k
b
N
y
w
N
L
y
i
7
p
B
d
8
Z
K
m
i
G
T
d
R
P
X
+
2
g
a
5
V
x
p
D
m
2
p
Z
C
m
K
t
/
T
9
Q
0
M
6
b
K
E
t
s
5
u
9
c
8
9
2
b
i
/
7
x
R
i
e
l
e
V
A
t
V
l
M
g
V
e
1
q
U
l
h
I
w
h
1
l
y
M
B
a
a
M
5
S
V
J
Z
R
p
Y
W
8
F
N
q
G
a
M
r
T
5
u
j
a
E
4
P
n
L
L
8
n
5
d
i
/
w
e
8
H
p
T
u
f
w
a
x
v
H
M
v
l
E
N
s
g
m
C
c
g
u
O
S
T
f
y
A
n
p
E
0
Z
+
k
h
v
y
h
9
w
6
1
8
5
v
5
8
6
5
f
2
p
d
c
N
q
Z
j
+
Q
f
O
A
+
P
H
W
+
j
z
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
3
y
f
X
w
Z
g
e
Z
Q
A
u
E
F
B
z
g
4
J
1
L
t
a
z
q
a
A
=
"
>
A
A
A
C
L
H
i
c
b
V
D
R
S
t
x
A
F
J
1
o
W
2
1
q
a
7
S
P
f
b
m
4
L
C
g
t
S
y
K
C
g
g
h
S
X
/
p
S
U
X
D
d
h
U
0
a
J
r
M
T
d
3
A
y
C
T
M
3
s
i
H
k
g
/
r
i
r
w
i
l
D
x
X
x
1
e
9
w
d
s
1
D
q
z
1
w
4
X
D
O
v
d
x
7
T
1
J
I
Y
d
D
3
b
5
2
F
x
V
e
v
3
y
w
t
v
3
X
f
r
b
z
/
s
O
q
t
r
Z
+
b
v
N
S
M
9
1
k
u
c
z
1
M
q
O
F
S
K
N
5
H
g
Z
I
P
C
8
1
p
l
k
g
+
S
C
6
P
Z
v
7
g
i
m
s
j
c
n
W
G
V
c
G
j
j
F
4
o
k
Q
p
G
0
U
q
x
d
9
Q
N
a
5
j
G
4
g
t
U
s
Q
i
b
u
B
Y
H
Q
f
P
j
2
O
1
W
c
A
D
p
5
n
Q
f
Q
p
x
w
p
F
t
u
F
8
K
M
4
o
R
R
W
X
9
v
3
K
m
1
p
7
E
P
n
0
F
B
7
H
X
8
n
j
8
H
v
C
R
B
S
z
q
k
x
U
n
s
/
Q
r
H
O
S
s
z
r
p
B
J
a
s
w
o
8
A
u
M
a
q
p
R
M
M
k
b
N
y
w
N
L
y
i
7
p
B
d
8
Z
K
m
i
G
T
d
R
P
X
+
2
g
a
5
V
x
p
D
m
2
p
Z
C
m
K
t
/
T
9
Q
0
M
6
b
K
E
t
s
5
u
9
c
8
9
2
b
i
/
7
x
R
i
e
l
e
V
A
t
V
l
M
g
V
e
1
q
U
l
h
I
w
h
1
l
y
M
B
a
a
M
5
S
V
J
Z
R
p
Y
W
8
F
N
q
G
a
M
r
T
5
u
j
a
E
4
P
n
L
L
8
n
5
d
i
/
w
e
8
H
p
T
u
f
w
a
x
v
H
M
v
l
E
N
s
g
m
C
c
g
u
O
S
T
f
y
A
n
p
E
0
Z
+
k
h
v
y
h
9
w
6
1
8
5
v
5
8
6
5
f
2
p
d
c
N
q
Z
j
+
Q
f
O
A
+
P
H
W
+
j
z
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
3
y
f
X
w
Z
g
e
Z
Q
A
u
E
F
B
z
g
4
J
1
L
t
a
z
q
a
A
=
"
>
A
A
A
C
L
H
i
c
b
V
D
R
S
t
x
A
F
J
1
o
W
2
1
q
a
7
S
P
f
b
m
4
L
C
g
t
S
y
K
C
g
g
h
S
X
/
p
S
U
X
D
d
h
U
0
a
J
r
M
T
d
3
A
y
C
T
M
3
s
i
H
k
g
/
r
i
r
w
i
l
D
x
X
x
1
e
9
w
d
s
1
D
q
z
1
w
4
X
D
O
v
d
x
7
T
1
J
I
Y
d
D
3
b
5
2
F
x
V
e
v
3
y
w
t
v
3
X
f
r
b
z
/
s
O
q
t
r
Z
+
b
v
N
S
M
9
1
k
u
c
z
1
M
q
O
F
S
K
N
5
H
g
Z
I
P
C
8
1
p
l
k
g
+
S
C
6
P
Z
v
7
g
i
m
s
j
c
n
W
G
V
c
G
j
j
F
4
o
k
Q
p
G
0
U
q
x
d
9
Q
N
a
5
j
G
4
g
t
U
s
Q
i
b
u
B
Y
H
Q
f
P
j
2
O
1
W
c
A
D
p
5
n
Q
f
Q
p
x
w
p
F
t
u
F
8
K
M
4
o
R
R
W
X
9
v
3
K
m
1
p
7
E
P
n
0
F
B
7
H
X
8
n
j
8
H
v
C
R
B
S
z
q
k
x
U
n
s
/
Q
r
H
O
S
s
z
r
p
B
J
a
s
w
o
8
A
u
M
a
q
p
R
M
M
k
b
N
y
w
N
L
y
i
7
p
B
d
8
Z
K
m
i
G
T
d
R
P
X
+
2
g
a
5
V
x
p
D
m
2
p
Z
C
m
K
t
/
T
9
Q
0
M
6
b
K
E
t
s
5
u
9
c
8
9
2
b
i
/
7
x
R
i
e
l
e
V
A
t
V
l
M
g
V
e
1
q
U
l
h
I
w
h
1
l
y
M
B
a
a
M
5
S
V
J
Z
R
p
Y
W
8
F
N
q
G
a
M
r
T
5
u
j
a
E
4
P
n
L
L
8
n
5
d
i
/
w
e
8
H
p
T
u
f
w
a
x
v
H
M
v
l
E
N
s
g
m
C
c
g
u
O
S
T
f
y
A
n
p
E
0
Z
+
k
h
v
y
h
9
w
6
1
8
5
v
5
8
6
5
f
2
p
d
c
N
q
Z
j
+
Q
f
O
A
+
P
H
W
+
j
z
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
3
y
f
X
w
Z
g
e
Z
Q
A
u
E
F
B
z
g
4
J
1
L
t
a
z
q
a
A
=
"
>
A
A
A
C
L
H
i
c
b
V
D
R
S
t
x
A
F
J
1
o
W
2
1
q
a
7
S
P
f
b
m
4
L
C
g
t
S
y
K
C
g
g
h
S
X
/
p
S
U
X
D
d
h
U
0
a
J
r
M
T
d
3
A
y
C
T
M
3
s
i
H
k
g
/
r
i
r
w
i
l
D
x
X
x
1
e
9
w
d
s
1
D
q
z
1
w
4
X
D
O
v
d
x
7
T
1
J
I
Y
d
D
3
b
5
2
F
x
V
e
v
3
y
w
t
v
3
X
f
r
b
z
/
s
O
q
t
r
Z
+
b
v
N
S
M
9
1
k
u
c
z
1
M
q
O
F
S
K
N
5
H
g
Z
I
P
C
8
1
p
l
k
g
+
S
C
6
P
Z
v
7
g
i
m
s
j
c
n
W
G
V
c
G
j
j
F
4
o
k
Q
p
G
0
U
q
x
d
9
Q
N
a
5
j
G
4
g
t
U
s
Q
i
b
u
B
Y
H
Q
f
P
j
2
O
1
W
c
A
D
p
5
n
Q
f
Q
p
x
w
p
F
t
u
F
8
K
M
4
o
R
R
W
X
9
v
3
K
m
1
p
7
E
P
n
0
F
B
7
H
X
8
n
j
8
H
v
C
R
B
S
z
q
k
x
U
n
s
/
Q
r
H
O
S
s
z
r
p
B
J
a
s
w
o
8
A
u
M
a
q
p
R
M
M
k
b
N
y
w
N
L
y
i
7
p
B
d
8
Z
K
m
i
G
T
d
R
P
X
+
2
g
a
5
V
x
p
D
m
2
p
Z
C
m
K
t
/
T
9
Q
0
M
6
b
K
E
t
s
5
u
9
c
8
9
2
b
i
/
7
x
R
i
e
l
e
V
A
t
V
l
M
g
V
e
1
q
U
l
h
I
w
h
1
l
y
M
B
a
a
M
5
S
V
J
Z
R
p
Y
W
8
F
N
q
G
a
M
r
T
5
u
j
a
E
4
P
n
L
L
8
n
5
d
i
/
w
e
8
H
p
T
u
f
w
a
x
v
H
M
v
l
E
N
s
g
m
C
c
g
u
O
S
T
f
y
A
n
p
E
0
Z
+
k
h
v
y
h
9
w
6
1
8
5
v
5
8
6
5
f
2
p
d
c
N
q
Z
j
+
Q
f
O
A
+
P
H
W
+
j
z
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
W
e
d
W
K
i
S
a
3
r
K
J
v
i
C
9
j
q
j
n
d
s
1
J
r
K
Q
=
"
>
A
A
A
C
P
H
i
c
b
V
B
N
S
x
x
B
E
O
0
x
x
o
8
x
6
i
Y
5
5
l
K
4
L
C
j
K
M
i
O
C
g
S
B
I
v
O
S
S
Y
E
h
W
h
Z
1
1
6
O
m
t
2
W
3
s
6
R
m
6
a
8
I
O
w
/
y
w
X
P
w
R
u
e
X
k
x
Y
M
h
5
J
p
z
e
t
c
9
J
J
o
H
D
a
/
f
q
6
K
q
X
l
I
o
a
S
k
I
v
n
s
L
T
x
a
f
L
i
2
v
r
P
p
r
z
9
Y
3
N
l
v
P
X
5
z
Z
v
D
Q
C
e
y
J
X
u
b
l
I
u
E
U
l
N
f
Z
I
k
s
K
L
w
i
D
P
E
o
X
n
y
d
X
J
1
D
/
/
g
s
b
K
X
H
+
m
q
s
B
B
x
k
d
a
p
l
J
w
c
l
L
c
+
t
S
J
a
p
j
E
c
g
+
q
W
E
Z
N
X
M
u
j
s
L
n
8
4
H
c
q
O
I
J
0
e
/
I
G
I
h
o
j
8
R
2
/
A
1
H
G
a
S
y
4
q
t
8
3
7
j
d
x
B
Z
M
4
g
F
3
Q
v
o
n
r
q
O
C
G
K
4
W
q
g
b
j
V
D
r
r
B
D
P
C
Y
h
H
P
S
Z
n
O
c
x
q
1
v
0
T
A
X
Z
Y
a
a
h
O
L
W
9
s
O
g
o
E
H
N
D
U
m
h
s
P
G
j
0
m
L
B
x
R
U
f
Y
d
9
R
z
T
O
0
g
3
p
2
f
A
M
d
p
w
w
h
z
Y
1
7
m
m
C
m
/
t
1
R
8
8
z
a
K
k
t
c
5
X
R
/
+
9
C
b
i
v
/
z
+
i
W
l
r
w
e
1
1
E
V
J
q
M
X
9
o
L
R
U
Q
D
l
M
k
4
S
h
N
C
h
I
V
Y
5
w
Y
a
T
b
F
c
T
Y
h
S
D
I
5
e
2
7
E
M
K
H
J
z
8
m
Z
/
v
d
M
O
i
G
H
w
/
a
x
2
/
n
c
a
y
w
V
2
y
L
b
b
O
Q
H
b
J
j
9
o
6
d
s
h
4
T
7
C
u
7
Y
X
f
s
h
3
f
t
3
X
o
/
v
V
/
3
p
Q
v
e
v
O
c
l
+
w
f
e
7
z
+
e
O
6
o
w
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
W
e
d
W
K
i
S
a
3
r
K
J
v
i
C
9
j
q
j
n
d
s
1
J
r
K
Q
=
"
>
A
A
A
C
P
H
i
c
b
V
B
N
S
x
x
B
E
O
0
x
x
o
8
x
6
i
Y
5
5
l
K
4
L
C
j
K
M
i
O
C
g
S
B
I
v
O
S
S
Y
E
h
W
h
Z
1
1
6
O
m
t
2
W
3
s
6
R
m
6
a
8
I
O
w
/
y
w
X
P
w
R
u
e
X
k
x
Y
M
h
5
J
p
z
e
t
c
9
J
J
o
H
D
a
/
f
q
6
K
q
X
l
I
o
a
S
k
I
v
n
s
L
T
x
a
f
L
i
2
v
r
P
p
r
z
9
Y
3
N
l
v
P
X
5
z
Z
v
D
Q
C
e
y
J
X
u
b
l
I
u
E
U
l
N
f
Z
I
k
s
K
L
w
i
D
P
E
o
X
n
y
d
X
J
1
D
/
/
g
s
b
K
X
H
+
m
q
s
B
B
x
k
d
a
p
l
J
w
c
l
L
c
+
t
S
J
a
p
j
E
c
g
+
q
W
E
Z
N
X
M
u
j
s
L
n
8
4
H
c
q
O
I
J
0
e
/
I
G
I
h
o
j
8
R
2
/
A
1
H
G
a
S
y
4
q
t
8
3
7
j
d
x
B
Z
M
4
g
F
3
Q
v
o
n
r
q
O
C
G
K
4
W
q
g
b
j
V
D
r
r
B
D
P
C
Y
h
H
P
S
Z
n
O
c
x
q
1
v
0
T
A
X
Z
Y
a
a
h
O
L
W
9
s
O
g
o
E
H
N
D
U
m
h
s
P
G
j
0
m
L
B
x
R
U
f
Y
d
9
R
z
T
O
0
g
3
p
2
f
A
M
d
p
w
w
h
z
Y
1
7
m
m
C
m
/
t
1
R
8
8
z
a
K
k
t
c
5
X
R
/
+
9
C
b
i
v
/
z
+
i
W
l
r
w
e
1
1
E
V
J
q
M
X
9
o
L
R
U
Q
D
l
M
k
4
S
h
N
C
h
I
V
Y
5
w
Y
a
T
b
F
c
T
Y
h
S
D
I
5
e
2
7
E
M
K
H
J
z
8
m
Z
/
v
d
M
O
i
G
H
w
/
a
x
2
/
n
c
a
y
w
V
2
y
L
b
b
O
Q
H
b
J
j
9
o
6
d
s
h
4
T
7
C
u
7
Y
X
f
s
h
3
f
t
3
X
o
/
v
V
/
3
p
Q
v
e
v
O
c
l
+
w
f
e
7
z
+
e
O
6
o
w
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
W
e
d
W
K
i
S
a
3
r
K
J
v
i
C
9
j
q
j
n
d
s
1
J
r
K
Q
=
"
>
A
A
A
C
P
H
i
c
b
V
B
N
S
x
x
B
E
O
0
x
x
o
8
x
6
i
Y
5
5
l
K
4
L
C
j
K
M
i
O
C
g
S
B
I
v
O
S
S
Y
E
h
W
h
Z
1
1
6
O
m
t
2
W
3
s
6
R
m
6
a
8
I
O
w
/
y
w
X
P
w
R
u
e
X
k
x
Y
M
h
5
J
p
z
e
t
c
9
J
J
o
H
D
a
/
f
q
6
K
q
X
l
I
o
a
S
k
I
v
n
s
L
T
x
a
f
L
i
2
v
r
P
p
r
z
9
Y
3
N
l
v
P
X
5
z
Z
v
D
Q
C
e
y
J
X
u
b
l
I
u
E
U
l
N
f
Z
I
k
s
K
L
w
i
D
P
E
o
X
n
y
d
X
J
1
D
/
/
g
s
b
K
X
H
+
m
q
s
B
B
x
k
d
a
p
l
J
w
c
l
L
c
+
t
S
J
a
p
j
E
c
g
+
q
W
E
Z
N
X
M
u
j
s
L
n
8
4
H
c
q
O
I
J
0
e
/
I
G
I
h
o
j
8
R
2
/
A
1
H
G
a
S
y
4
q
t
8
3
7
j
d
x
B
Z
M
4
g
F
3
Q
v
o
n
r
q
O
C
G
K
4
W
q
g
b
j
V
D
r
r
B
D
P
C
Y
h
H
P
S
Z
n
O
c
x
q
1
v
0
T
A
X
Z
Y
a
a
h
O
L
W
9
s
O
g
o
E
H
N
D
U
m
h
s
P
G
j
0
m
L
B
x
R
U
f
Y
d
9
R
z
T
O
0
g
3
p
2
f
A
M
d
p
w
w
h
z
Y
1
7
m
m
C
m
/
t
1
R
8
8
z
a
K
k
t
c
5
X
R
/
+
9
C
b
i
v
/
z
+
i
W
l
r
w
e
1
1
E
V
J
q
M
X
9
o
L
R
U
Q
D
l
M
k
4
S
h
N
C
h
I
V
Y
5
w
Y
a
T
b
F
c
T
Y
h
S
D
I
5
e
2
7
E
M
K
H
J
z
8
m
Z
/
v
d
M
O
i
G
H
w
/
a
x
2
/
n
c
a
y
w
V
2
y
L
b
b
O
Q
H
b
J
j
9
o
6
d
s
h
4
T
7
C
u
7
Y
X
f
s
h
3
f
t
3
X
o
/
v
V
/
3
p
Q
v
e
v
O
c
l
+
w
f
e
7
z
+
e
O
6
o
w
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
W
e
d
W
K
i
S
a
3
r
K
J
v
i
C
9
j
q
j
n
d
s
1
J
r
K
Q
=
"
>
A
A
A
C
P
H
i
c
b
V
B
N
S
x
x
B
E
O
0
x
x
o
8
x
6
i
Y
5
5
l
K
4
L
C
j
K
M
i
O
C
g
S
B
I
v
O
S
S
Y
E
h
W
h
Z
1
1
6
O
m
t
2
W
3
s
6
R
m
6
a
8
I
O
w
/
y
w
X
P
w
R
u
e
X
k
x
Y
M
h
5
J
p
z
e
t
c
9
J
J
o
H
D
a
/
f
q
6
K
q
X
l
I
o
a
S
k
I
v
n
s
L
T
x
a
f
L
i
2
v
r
P
p
r
z
9
Y
3
N
l
v
P
X
5
z
Z
v
D
Q
C
e
y
J
X
u
b
l
I
u
E
U
l
N
f
Z
I
k
s
K
L
w
i
D
P
E
o
X
n
y
d
X
J
1
D
/
/
g
s
b
K
X
H
+
m
q
s
B
B
x
k
d
a
p
l
J
w
c
l
L
c
+
t
S
J
a
p
j
E
c
g
+
q
W
E
Z
N
X
M
u
j
s
L
n
8
4
H
c
q
O
I
J
0
e
/
I
G
I
h
o
j
8
R
2
/
A
1
H
G
a
S
y
4
q
t
8
3
7
j
d
x
B
Z
M
4
g
F
3
Q
v
o
n
r
q
O
C
G
K
4
W
q
g
b
j
V
D
r
r
B
D
P
C
Y
h
H
P
S
Z
n
O
c
x
q
1
v
0
T
A
X
Z
Y
a
a
h
O
L
W
9
s
O
g
o
E
H
N
D
U
m
h
s
P
G
j
0
m
L
B
x
R
U
f
Y
d
9
R
z
T
O
0
g
3
p
2
f
A
M
d
p
w
w
h
z
Y
1
7
m
m
C
m
/
t
1
R
8
8
z
a
K
k
t
c
5
X
R
/
+
9
C
b
i
v
/
z
+
i
W
l
r
w
e
1
1
E
V
J
q
M
X
9
o
L
R
U
Q
D
l
M
k
4
S
h
N
C
h
I
V
Y
5
w
Y
a
T
b
F
c
T
Y
h
S
D
I
5
e
2
7
E
M
K
H
J
z
8
m
Z
/
v
d
M
O
i
G
H
w
/
a
x
2
/
n
c
a
y
w
V
2
y
L
b
b
O
Q
H
b
J
j
9
o
6
d
s
h
4
T
7
C
u
7
Y
X
f
s
h
3
f
t
3
X
o
/
v
V
/
3
p
Q
v
e
v
O
c
l
+
w
f
e
7
z
+
e
O
6
o
w
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
f
j
E
y
Y
9
J
5
C
G
f
U
l
R
v
p
4
t
P
7
5
Z
Z
a
A
t
s
=
"
>
A
A
A
C
O
H
i
c
b
V
B
N
S
x
x
B
E
O
0
x
H
+
q
Y
x
D
U
5
e
i
m
y
L
C
g
J
y
4
w
I
C
i
J
I
v
O
S
i
M
Z
B
V
Y
W
c
z
9
P
T
W
u
I
0
9
P
U
N
3
j
e
w
w
z
M
/
K
x
Z
+
R
W
/
D
i
Q
R
G
v
+
Q
X
p
X
f
e
Q
a
B
4
0
v
H
6
v
i
q
p
6
S
a
G
k
p
S
D
4
5
c
0
9
e
/
7
i
5
f
z
C
o
r
/
0
6
v
W
b
5
d
b
K
2
2
O
b
l
0
Z
g
T
+
Q
q
N
6
c
J
t
6
i
k
x
h
5
J
U
n
h
a
G
O
R
Z
o
v
A
k
O
d
+
f
+
C
c
X
a
K
z
M
9
T
e
q
C
h
x
k
/
E
z
L
V
A
p
O
T
o
p
b
X
z
p
R
D
e
N
Y
f
o
Q
q
l
l
E
T
1
3
I
3
b
L
4
f
+
p
0
K
d
i
F
d
G
+
9
A
R
C
M
k
v
u
5
3
I
M
o
4
j
Q
R
X
9
U
H
j
f
m
N
X
M
I
4
D
+
A
D
a
N
3
E
d
F
W
i
K
B
u
J
W
O
+
g
G
U
8
B
T
E
s
5
I
m
8
1
w
F
L
d
+
R
s
N
c
l
B
l
q
E
o
p
b
2
w
+
D
g
g
Y
1
N
y
S
F
w
s
a
P
S
o
s
F
F
+
f
8
D
P
u
O
a
p
6
h
H
d
T
T
w
x
v
o
O
G
U
I
a
W
7
c
0
w
R
T
9
e
+
O
m
m
f
W
V
l
n
i
K
i
e
7
2
8
f
e
R
P
y
f
1
y
8
p
3
R
7
U
U
h
c
l
o
R
Y
P
g
9
J
S
A
e
U
w
S
R
G
G
0
q
A
g
V
T
n
C
h
Z
F
u
V
x
A
j
b
r
g
g
l
7
X
v
Q
g
g
f
n
/
y
U
H
G
9
0
w
6
A
b
f
t
1
s
7
3
2
a
x
b
H
A
V
t
l
7
t
s
Z
C
t
s
X
2
2
G
d
2
x
H
p
M
s
B
/
s
i
t
2
w
W
+
/
S
u
/
b
u
v
P
u
H
0
j
l
v
1
v
O
O
/
Q
P
v
9
x
8
6
h
a
h
y
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
f
j
E
y
Y
9
J
5
C
G
f
U
l
R
v
p
4
t
P
7
5
Z
Z
a
A
t
s
=
"
>
A
A
A
C
O
H
i
c
b
V
B
N
S
x
x
B
E
O
0
x
H
+
q
Y
x
D
U
5
e
i
m
y
L
C
g
J
y
4
w
I
C
i
J
I
v
O
S
i
M
Z
B
V
Y
W
c
z
9
P
T
W
u
I
0
9
P
U
N
3
j
e
w
w
z
M
/
K
x
Z
+
R
W
/
D
i
Q
R
G
v
+
Q
X
p
X
f
e
Q
a
B
4
0
v
H
6
v
i
q
p
6
S
a
G
k
p
S
D
4
5
c
0
9
e
/
7
i
5
f
z
C
o
r
/
0
6
v
W
b
5
d
b
K
2
2
O
b
l
0
Z
g
T
+
Q
q
N
6
c
J
t
6
i
k
x
h
5
J
U
n
h
a
G
O
R
Z
o
v
A
k
O
d
+
f
+
C
c
X
a
K
z
M
9
T
e
q
C
h
x
k
/
E
z
L
V
A
p
O
T
o
p
b
X
z
p
R
D
e
N
Y
f
o
Q
q
l
l
E
T
1
3
I
3
b
L
4
f
+
p
0
K
d
i
F
d
G
+
9
A
R
C
M
k
v
u
5
3
I
M
o
4
j
Q
R
X
9
U
H
j
f
m
N
X
M
I
4
D
+
A
D
a
N
3
E
d
F
W
i
K
B
u
J
W
O
+
g
G
U
8
B
T
E
s
5
I
m
8
1
w
F
L
d
+
R
s
N
c
l
B
l
q
E
o
p
b
2
w
+
D
g
g
Y
1
N
y
S
F
w
s
a
P
S
o
s
F
F
+
f
8
D
P
u
O
a
p
6
h
H
d
T
T
w
x
v
o
O
G
U
I
a
W
7
c
0
w
R
T
9
e
+
O
m
m
f
W
V
l
n
i
K
i
e
7
2
8
f
e
R
P
y
f
1
y
8
p
3
R
7
U
U
h
c
l
o
R
Y
P
g
9
J
S
A
e
U
w
S
R
G
G
0
q
A
g
V
T
n
C
h
Z
F
u
V
x
A
j
b
r
g
g
l
7
X
v
Q
g
g
f
n
/
y
U
H
G
9
0
w
6
A
b
f
t
1
s
7
3
2
a
x
b
H
A
V
t
l
7
t
s
Z
C
t
s
X
2
2
G
d
2
x
H
p
M
s
B
/
s
i
t
2
w
W
+
/
S
u
/
b
u
v
P
u
H
0
j
l
v
1
v
O
O
/
Q
P
v
9
x
8
6
h
a
h
y
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
f
j
E
y
Y
9
J
5
C
G
f
U
l
R
v
p
4
t
P
7
5
Z
Z
a
A
t
s
=
"
>
A
A
A
C
O
H
i
c
b
V
B
N
S
x
x
B
E
O
0
x
H
+
q
Y
x
D
U
5
e
i
m
y
L
C
g
J
y
4
w
I
C
i
J
I
v
O
S
i
M
Z
B
V
Y
W
c
z
9
P
T
W
u
I
0
9
P
U
N
3
j
e
w
w
z
M
/
K
x
Z
+
R
W
/
D
i
Q
R
G
v
+
Q
X
p
X
f
e
Q
a
B
4
0
v
H
6
v
i
q
p
6
S
a
G
k
p
S
D
4
5
c
0
9
e
/
7
i
5
f
z
C
o
r
/
0
6
v
W
b
5
d
b
K
2
2
O
b
l
0
Z
g
T
+
Q
q
N
6
c
J
t
6
i
k
x
h
5
J
U
n
h
a
G
O
R
Z
o
v
A
k
O
d
+
f
+
C
c
X
a
K
z
M
9
T
e
q
C
h
x
k
/
E
z
L
V
A
p
O
T
o
p
b
X
z
p
R
D
e
N
Y
f
o
Q
q
l
l
E
T
1
3
I
3
b
L
4
f
+
p
0
K
d
i
F
d
G
+
9
A
R
C
M
k
v
u
5
3
I
M
o
4
j
Q
R
X
9
U
H
j
f
m
N
X
M
I
4
D
+
A
D
a
N
3
E
d
F
W
i
K
B
u
J
W
O
+
g
G
U
8
B
T
E
s
5
I
m
8
1
w
F
L
d
+
R
s
N
c
l
B
l
q
E
o
p
b
2
w
+
D
g
g
Y
1
N
y
S
F
w
s
a
P
S
o
s
F
F
+
f
8
D
P
u
O
a
p
6
h
H
d
T
T
w
x
v
o
O
G
U
I
a
W
7
c
0
w
R
T
9
e
+
O
m
m
f
W
V
l
n
i
K
i
e
7
2
8
f
e
R
P
y
f
1
y
8
p
3
R
7
U
U
h
c
l
o
R
Y
P
g
9
J
S
A
e
U
w
S
R
G
G
0
q
A
g
V
T
n
C
h
Z
F
u
V
x
A
j
b
r
g
g
l
7
X
v
Q
g
g
f
n
/
y
U
H
G
9
0
w
6
A
b
f
t
1
s
7
3
2
a
x
b
H
A
V
t
l
7
t
s
Z
C
t
s
X
2
2
G
d
2
x
H
p
M
s
B
/
s
i
t
2
w
W
+
/
S
u
/
b
u
v
P
u
H
0
j
l
v
1
v
O
O
/
Q
P
v
9
x
8
6
h
a
h
y
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
f
j
E
y
Y
9
J
5
C
G
f
U
l
R
v
p
4
t
P
7
5
Z
Z
a
A
t
s
=
"
>
A
A
A
C
O
H
i
c
b
V
B
N
S
x
x
B
E
O
0
x
H
+
q
Y
x
D
U
5
e
i
m
y
L
C
g
J
y
4
w
I
C
i
J
I
v
O
S
i
M
Z
B
V
Y
W
c
z
9
P
T
W
u
I
0
9
P
U
N
3
j
e
w
w
z
M
/
K
x
Z
+
R
W
/
D
i
Q
R
G
v
+
Q
X
p
X
f
e
Q
a
B
4
0
v
H
6
v
i
q
p
6
S
a
G
k
p
S
D
4
5
c
0
9
e
/
7
i
5
f
z
C
o
r
/
0
6
v
W
b
5
d
b
K
2
2
O
b
l
0
Z
g
T
+
Q
q
N
6
c
J
t
6
i
k
x
h
5
J
U
n
h
a
G
O
R
Z
o
v
A
k
O
d
+
f
+
C
c
X
a
K
z
M
9
T
e
q
C
h
x
k
/
E
z
L
V
A
p
O
T
o
p
b
X
z
p
R
D
e
N
Y
f
o
Q
q
l
l
E
T
1
3
I
3
b
L
4
f
+
p
0
K
d
i
F
d
G
+
9
A
R
C
M
k
v
u
5
3
I
M
o
4
j
Q
R
X
9
U
H
j
f
m
N
X
M
I
4
D
+
A
D
a
N
3
E
d
F
W
i
K
B
u
J
W
O
+
g
G
U
8
B
T
E
s
5
I
m
8
1
w
F
L
d
+
R
s
N
c
l
B
l
q
E
o
p
b
2
w
+
D
g
g
Y
1
N
y
S
F
w
s
a
P
S
o
s
F
F
+
f
8
D
P
u
O
a
p
6
h
H
d
T
T
w
x
v
o
O
G
U
I
a
W
7
c
0
w
R
T
9
e
+
O
m
m
f
W
V
l
n
i
K
i
e
7
2
8
f
e
R
P
y
f
1
y
8
p
3
R
7
U
U
h
c
l
o
R
Y
P
g
9
J
S
A
e
U
w
S
R
G
G
0
q
A
g
V
T
n
C
h
Z
F
u
V
x
A
j
b
r
g
g
l
7
X
v
Q
g
g
f
n
/
y
U
H
G
9
0
w
6
A
b
f
t
1
s
7
3
2
a
x
b
H
A
V
t
l
7
t
s
Z
C
t
s
X
2
2
G
d
2
x
H
p
M
s
B
/
s
i
t
2
w
W
+
/
S
u
/
b
u
v
P
u
H
0
j
l
v
1
v
O
O
/
Q
P
v
9
x
8
6
h
a
h
y
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
v
o
A
x
a
M
+
o
b
w
r
T
4
7
t
H
V
I
T
M
R
l
7
n
k
U
=
"
>
A
A
A
C
P
n
i
c
b
V
B
N
a
9
t
A
E
F
2
l
T
e
o
q
X
2
5
y
7
G
W
p
M
T
g
k
G
C
k
U
W
i
g
G
k
1
5
6
a
U
m
h
/
g
D
L
E
a
v
1
K
F
6
8
W
o
n
d
U
b
A
Q
+
m
W
9
5
D
f
0
l
m
M
v
P
b
S
U
X
n
v
s
y
v
G
h
+
R
h
Y
e
P
P
e
G
2
b
n
R
Z
k
U
B
j
3
v
x
t
l
4
8
n
R
z
6
1
n
j
u
b
u
9
s
7
u
3
3
3
x
x
M
D
R
p
r
j
k
M
e
C
p
T
P
Y
6
Y
A
S
k
U
D
F
C
g
h
H
G
m
g
S
W
R
h
F
G
0
e
F
/
r
o
y
v
Q
R
q
T
q
C
x
Y
Z
T
B
N
2
q
U
Q
s
O
E
N
L
h
c
1
B
O
y
j
p
M
h
Q
n
t
A
h
F
U
I
W
l
6
P
n
V
x
S
e
3
X
d
A
e
j
T
v
L
d
z
T
A
O
S
A
7
c
t
s
0
S
B
j
O
O
Z
P
l
x
8
p
2
S
2
t
Y
h
h
4
9
p
s
p
2
O
i
y
D
D
H
R
W
u
T
U
X
N
l
t
e
1
1
s
V
f
Q
j
8
N
W
i
R
d
Z
2
H
z
W
/
B
L
O
V
5
A
g
q
5
Z
M
Z
M
f
C
/
D
a
c
k
0
C
i
6
h
c
o
P
c
Q
M
b
4
g
l
3
C
x
E
L
F
E
j
D
T
c
n
V
+
R
d
u
W
m
d
E
4
1
f
Y
p
p
C
v
2
/
4
m
S
J
c
Y
U
S
W
S
d
9
Q
X
m
v
l
a
T
j
2
m
T
H
O
O
3
0
1
K
o
L
E
d
Q
/
H
Z
R
n
E
u
K
K
a
2
z
p
D
O
h
g
a
M
s
L
G
B
c
C
/
t
X
y
u
d
M
M
4
4
2
c
d
e
G
4
N
8
/
+
S
E
Y
n
n
Z
9
r
+
t
/
f
t
3
q
n
6
3
j
a
J
C
X
5
B
X
p
E
J
+
8
I
X
3
y
g
Z
y
T
A
e
H
k
K
/
l
O
f
p
J
f
z
r
X
z
w
/
n
t
/
L
m
1
b
j
j
r
m
U
N
y
p
5
y
/
/
w
C
0
+
a
o
E
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
v
o
A
x
a
M
+
o
b
w
r
T
4
7
t
H
V
I
T
M
R
l
7
n
k
U
=
"
>
A
A
A
C
P
n
i
c
b
V
B
N
a
9
t
A
E
F
2
l
T
e
o
q
X
2
5
y
7
G
W
p
M
T
g
k
G
C
k
U
W
i
g
G
k
1
5
6
a
U
m
h
/
g
D
L
E
a
v
1
K
F
6
8
W
o
n
d
U
b
A
Q
+
m
W
9
5
D
f
0
l
m
M
v
P
b
S
U
X
n
v
s
y
v
G
h
+
R
h
Y
e
P
P
e
G
2
b
n
R
Z
k
U
B
j
3
v
x
t
l
4
8
n
R
z
6
1
n
j
u
b
u
9
s
7
u
3
3
3
x
x
M
D
R
p
r
j
k
M
e
C
p
T
P
Y
6
Y
A
S
k
U
D
F
C
g
h
H
G
m
g
S
W
R
h
F
G
0
e
F
/
r
o
y
v
Q
R
q
T
q
C
x
Y
Z
T
B
N
2
q
U
Q
s
O
E
N
L
h
c
1
B
O
y
j
p
M
h
Q
n
t
A
h
F
U
I
W
l
6
P
n
V
x
S
e
3
X
d
A
e
j
T
v
L
d
z
T
A
O
S
A
7
c
t
s
0
S
B
j
O
O
Z
P
l
x
8
p
2
S
2
t
Y
h
h
4
9
p
s
p
2
O
i
y
D
D
H
R
W
u
T
U
X
N
l
t
e
1
1
s
V
f
Q
j
8
N
W
i
R
d
Z
2
H
z
W
/
B
L
O
V
5
A
g
q
5
Z
M
Z
M
f
C
/
D
a
c
k
0
C
i
6
h
c
o
P
c
Q
M
b
4
g
l
3
C
x
E
L
F
E
j
D
T
c
n
V
+
R
d
u
W
m
d
E
4
1
f
Y
p
p
C
v
2
/
4
m
S
J
c
Y
U
S
W
S
d
9
Q
X
m
v
l
a
T
j
2
m
T
H
O
O
3
0
1
K
o
L
E
d
Q
/
H
Z
R
n
E
u
K
K
a
2
z
p
D
O
h
g
a
M
s
L
G
B
c
C
/
t
X
y
u
d
M
M
4
4
2
c
d
e
G
4
N
8
/
+
S
E
Y
n
n
Z
9
r
+
t
/
f
t
3
q
n
6
3
j
a
J
C
X
5
B
X
p
E
J
+
8
I
X
3
y
g
Z
y
T
A
e
H
k
K
/
l
O
f
p
J
f
z
r
X
z
w
/
n
t
/
L
m
1
b
j
j
r
m
U
N
y
p
5
y
/
/
w
C
0
+
a
o
E
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
v
o
A
x
a
M
+
o
b
w
r
T
4
7
t
H
V
I
T
M
R
l
7
n
k
U
=
"
>
A
A
A
C
P
n
i
c
b
V
B
N
a
9
t
A
E
F
2
l
T
e
o
q
X
2
5
y
7
G
W
p
M
T
g
k
G
C
k
U
W
i
g
G
k
1
5
6
a
U
m
h
/
g
D
L
E
a
v
1
K
F
6
8
W
o
n
d
U
b
A
Q
+
m
W
9
5
D
f
0
l
m
M
v
P
b
S
U
X
n
v
s
y
v
G
h
+
R
h
Y
e
P
P
e
G
2
b
n
R
Z
k
U
B
j
3
v
x
t
l
4
8
n
R
z
6
1
n
j
u
b
u
9
s
7
u
3
3
3
x
x
M
D
R
p
r
j
k
M
e
C
p
T
P
Y
6
Y
A
S
k
U
D
F
C
g
h
H
G
m
g
S
W
R
h
F
G
0
e
F
/
r
o
y
v
Q
R
q
T
q
C
x
Y
Z
T
B
N
2
q
U
Q
s
O
E
N
L
h
c
1
B
O
y
j
p
M
h
Q
n
t
A
h
F
U
I
W
l
6
P
n
V
x
S
e
3
X
d
A
e
j
T
v
L
d
z
T
A
O
S
A
7
c
t
s
0
S
B
j
O
O
Z
P
l
x
8
p
2
S
2
t
Y
h
h
4
9
p
s
p
2
O
i
y
D
D
H
R
W
u
T
U
X
N
l
t
e
1
1
s
V
f
Q
j
8
N
W
i
R
d
Z
2
H
z
W
/
B
L
O
V
5
A
g
q
5
Z
M
Z
M
f
C
/
D
a
c
k
0
C
i
6
h
c
o
P
c
Q
M
b
4
g
l
3
C
x
E
L
F
E
j
D
T
c
n
V
+
R
d
u
W
m
d
E
4
1
f
Y
p
p
C
v
2
/
4
m
S
J
c
Y
U
S
W
S
d
9
Q
X
m
v
l
a
T
j
2
m
T
H
O
O
3
0
1
K
o
L
E
d
Q
/
H
Z
R
n
E
u
K
K
a
2
z
p
D
O
h
g
a
M
s
L
G
B
c
C
/
t
X
y
u
d
M
M
4
4
2
c
d
e
G
4
N
8
/
+
S
E
Y
n
n
Z
9
r
+
t
/
f
t
3
q
n
6
3
j
a
J
C
X
5
B
X
p
E
J
+
8
I
X
3
y
g
Z
y
T
A
e
H
k
K
/
l
O
f
p
J
f
z
r
X
z
w
/
n
t
/
L
m
1
b
j
j
r
m
U
N
y
p
5
y
/
/
w
C
0
+
a
o
E
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
h
P
+
6
L
r
U
f
2
d
3
t
Z
a
l
d
q
a
Q
Q
v
E
K
M
X
y
w
=
"
>
A
A
A
B
2
X
i
c
b
Z
D
N
S
g
M
x
F
I
X
v
1
L
8
6
V
q
1
r
N
8
E
i
u
C
o
z
b
n
Q
p
u
H
F
Z
w
b
Z
C
O
5
R
M
5
k
4
b
m
s
k
M
y
R
2
h
D
H
0
B
F
2
5
E
f
C
9
3
v
o
3
p
z
0
J
b
D
w
Q
+
z
k
n
I
v
S
c
u
l
L
Q
U
B
N
9
e
b
W
d
3
b
/
+
g
f
u
g
f
N
f
z
j
k
9
N
m
o
2
f
z
0
g
j
s
i
l
z
l
5
j
n
m
F
p
X
U
2
C
V
J
C
p
8
L
g
z
y
L
F
f
b
j
6
f
0
i
7
7
+
g
s
T
L
X
T
z
Q
r
M
M
r
4
W
M
t
U
C
k
7
O
6
o
y
a
r
a
A
d
L
M
W
2
I
V
x
D
C
9
Y
a
N
b
+
G
S
S
7
K
D
D
U
J
x
a
0
d
h
E
F
B
U
c
U
N
S
a
F
w
7
g
9
L
i
w
U
X
U
z
7
G
g
U
P
N
M
7
R
R
t
R
x
z
z
i
6
d
k
7
A
0
N
+
5
o
Y
k
v
3
9
4
u
K
Z
9
b
O
s
t
j
d
z
D
h
N
7
G
a
2
M
P
/
L
B
i
W
l
t
1
E
l
d
V
E
S
a
r
H
6
K
C
0
V
o
5
w
t
d
m
a
J
N
C
h
I
z
R
x
w
Y
a
S
b
l
Y
k
J
N
1
y
Q
a
8
Z
3
H
Y
S
b
G
2
9
D
7
7
o
d
B
u
3
w
M
Y
A
6
n
M
M
F
X
E
E
I
N
3
A
H
D
9
C
B
L
g
h
I
4
B
X
e
v
Y
n
3
5
n
2
s
u
q
p
5
6
9
L
O
4
I
+
8
z
x
8
4
x
I
o
4
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
q
Q
o
L
q
D
u
E
Y
8
8
6
J
V
S
1
X
2
p
S
O
V
D
+
J
U
=
"
>
A
A
A
C
M
3
i
c
b
V
B
N
S
x
x
B
F
H
z
j
R
6
K
j
J
p
t
c
v
T
T
K
g
k
F
Z
Z
n
J
J
I
A
h
C
L
r
l
E
F
F
w
V
d
j
Z
D
T
+
8
b
t
7
G
n
Z
+
h
+
I
z
s
M
8
8
t
y
y
W
/
w
l
q
M
X
D
4
r
4
F
+
x
Z
9
+
B
X
Q
U
O
9
q
t
d
0
V
y
W
F
k
p
a
C
4
L
8
3
N
7
+
w
+
O
7
9
0
r
K
/
s
r
r
2
4
W
P
n
0
+
q
x
z
U
s
j
s
C
9
y
l
Z
v
T
h
F
t
U
U
m
O
f
J
C
k
8
L
Q
z
y
L
F
F
4
k
p
z
/
b
P
2
T
C
z
R
W
5
v
q
I
q
g
K
H
G
T
/
T
M
p
W
C
k
5
P
i
T
r
8
b
1
W
w
S
y
x
1
W
x
T
J
q
4
l
r
u
h
s
2
f
f
b
9
b
s
V
2
W
b
k
1
+
s
I
j
G
S
P
y
L
3
2
V
R
x
m
k
s
u
K
p
/
N
2
6
a
u
I
V
J
H
L
B
t
p
t
1
k
4
j
o
q
0
B
S
N
3
2
p
x
Z
z
P
o
B
V
O
w
1
y
S
c
k
U
2
Y
4
S
D
u
X
E
a
j
X
J
Q
Z
a
h
K
K
W
z
s
I
g
4
K
G
N
T
c
k
h
c
L
G
j
0
q
L
B
R
f
n
/
A
w
H
j
m
q
e
o
R
3
W
0
/
g
N
6
z
p
l
x
N
L
c
u
K
O
J
T
d
W
n
N
2
q
e
W
V
t
l
i
d
t
s
E
9
i
X
X
i
u
+
5
Q
1
K
S
r
8
P
a
6
m
L
k
l
C
L
x
4
f
S
U
j
H
K
W
d
s
l
G
0
m
D
g
l
T
l
C
B
d
G
u
r
8
y
M
e
a
G
C
3
K
N
+
6
6
E
8
G
X
k
1
+
T
4
a
y
8
M
e
u
F
h
A
E
u
w
D
h
u
w
B
S
F
8
g
z
3
4
B
Q
f
Q
B
w
F
/
4
Q
p
u
4
N
b
7
5
1
1
7
d
4
9
1
z
X
m
z
3
j
7
D
M
3
j
3
D
3
B
g
q
F
s
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
N
q
Q
o
L
q
D
u
E
Y
8
8
6
J
V
S
1
X
2
p
S
O
V
D
+
J
U
=
"
>
A
A
A
C
M
3
i
c
b
V
B
N
S
x
x
B
F
H
z
j
R
6
K
j
J
p
t
c
v
T
T
K
g
k
F
Z
Z
n
J
J
I
A
h
C
L
r
l
E
F
F
w
V
d
j
Z
D
T
+
8
b
t
7
G
n
Z
+
h
+
I
z
s
M
8
8
t
y
y
W
/
w
l
q
M
X
D
4
r
4
F
+
x
Z
9
+
B
X
Q
U
O
9
q
t
d
0
V
y
W
F
k
p
a
C
4
L
8
3
N
7
+
w
+
O
7
9
0
r
K
/
s
r
r
2
4
W
P
n
0
+
q
x
z
U
s
j
s
C
9
y
l
Z
v
T
h
F
t
U
U
m
O
f
J
C
k
8
L
Q
z
y
L
F
F
4
k
p
z
/
b
P
2
T
C
z
R
W
5
v
q
I
q
g
K
H
G
T
/
T
M
p
W
C
k
5
P
i
T
r
8
b
1
W
w
S
y
x
1
W
x
T
J
q
4
l
r
u
h
s
2
f
f
b
9
b
s
V
2
W
b
k
1
+
s
I
j
G
S
P
y
L
3
2
V
R
x
m
k
s
u
K
p
/
N
2
6
a
u
I
V
J
H
L
B
t
p
t
1
k
4
j
o
q
0
B
S
N
3
2
p
x
Z
z
P
o
B
V
O
w
1
y
S
c
k
U
2
Y
4
S
D
u
X
E
a
j
X
J
Q
Z
a
h
K
K
W
z
s
I
g
4
K
G
N
T
c
k
h
c
L
G
j
0
q
L
B
R
f
n
/
A
w
H
j
m
q
e
o
R
3
W
0
/
g
N
6
z
p
l
x
N
L
c
u
K
O
J
T
d
W
n
N
2
q
e
W
V
t
l
i
d
t
s
E
9
i
X
X
i
u
+
5
Q
1
K
S
r
8
P
a
6
m
L
k
l
C
L
x
4
f
S
U
j
H
K
W
d
s
l
G
0
m
D
g
l
T
l
C
B
d
G
u
r
8
y
M
e
a
G
C
3
K
N
+
6
6
E
8
G
X
k
1
+
T
4
a
y
8
M
e
u
F
h
A
E
u
w
D
h
u
w
B
S
F
8
g
z
3
4
B
Q
f
Q
B
w
F
/
4
Q
p
u
4
N
b
7
5
1
1
7
d
4
9
1
z
X
m
z
3
j
7
D
M
3
j
3
D
3
B
g
q
F
s
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
d
2
K
I
C
3
F
5
h
4
H
K
s
o
D
j
O
8
K
3
C
i
c
W
Y
S
Y
=
"
>
A
A
A
C
P
n
i
c
b
V
D
B
a
t
t
A
E
F
0
5
a
Z
q
o
b
e
I
m
x
1
6
W
G
I
N
L
i
5
F
y
S
a
E
Y
T
H
P
p
p
S
W
B
2
A
5
Y
r
l
i
t
R
/
a
S
1
U
r
s
j
o
q
F
0
J
f
l
k
m
/
I
r
c
d
c
e
m
g
J
v
f
a
Y
l
e
1
D
m
n
R
g
4
c
1
7
b
5
i
d
F
2
V
S
G
P
S
8
H
0
5
j
Y
/
P
Z
1
v
P
t
H
f
f
F
y
1
e
7
e
8
3
X
+
0
O
T
5
p
r
D
g
K
c
y
1
R
c
R
M
y
C
F
g
g
E
K
l
H
C
R
a
W
B
J
J
G
E
U
X
Z
7
U
+
u
g
7
a
C
N
S
d
Y
5
F
B
p
O
E
z
Z
S
I
B
W
d
o
q
b
A
5
a
A
c
l
X
Y
T
i
P
S
1
C
E
V
R
h
K
X
p
+
9
e
2
r
2
y
5
o
j
8
a
d
x
U
c
a
4
B
y
Q
v
X
X
b
N
E
g
Y
z
j
m
T
5
Z
f
K
d
g
t
r
W
I
Q
e
f
U
e
V
7
X
R
Y
B
h
n
o
r
H
J
r
L
m
y
2
v
K
6
3
L
P
o
U
+
G
v
Q
I
u
s
6
D
Z
s
3
w
T
T
l
e
Q
I
K
u
W
T
G
j
H
0
v
w
0
n
J
N
A
o
u
o
X
K
D
3
E
D
G
+
C
W
b
w
d
h
C
x
R
I
w
k
3
J
5
f
k
X
b
l
p
n
S
O
N
X
2
K
a
R
L
9
u
F
E
y
R
J
j
i
i
S
y
z
v
o
C
8
1
i
r
y
f
9
p
4
x
z
j
D
5
N
S
q
C
x
H
U
H
y
1
K
M
4
l
x
Z
T
W
W
d
K
p
0
M
B
R
F
h
Y
w
r
o
X
9
K
+
V
z
p
h
l
H
m
7
h
r
Q
/
A
f
n
/
w
U
D
I
+
6
v
t
f
1
z
7
x
W
/
9
M
6
j
m
3
y
h
h
y
S
D
v
H
J
M
e
m
T
z
+
S
U
D
A
g
n
V
+
S
W
/
C
K
/
n
W
v
n
p
3
P
n
/
F
l
Z
G
8
5
6
5
o
D
8
U
8
7
f
e
7
O
5
q
g
A
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
v
o
A
x
a
M
+
o
b
w
r
T
4
7
t
H
V
I
T
M
R
l
7
n
k
U
=
"
>
A
A
A
C
P
n
i
c
b
V
B
N
a
9
t
A
E
F
2
l
T
e
o
q
X
2
5
y
7
G
W
p
M
T
g
k
G
C
k
U
W
i
g
G
k
1
5
6
a
U
m
h
/
g
D
L
E
a
v
1
K
F
6
8
W
o
n
d
U
b
A
Q
+
m
W
9
5
D
f
0
l
m
M
v
P
b
S
U
X
n
v
s
y
v
G
h
+
R
h
Y
e
P
P
e
G
2
b
n
R
Z
k
U
B
j
3
v
x
t
l
4
8
n
R
z
6
1
n
j
u
b
u
9
s
7
u
3
3
3
x
x
M
D
R
p
r
j
k
M
e
C
p
T
P
Y
6
Y
A
S
k
U
D
F
C
g
h
H
G
m
g
S
W
R
h
F
G
0
e
F
/
r
o
y
v
Q
R
q
T
q
C
x
Y
Z
T
B
N
2
q
U
Q
s
O
E
N
L
h
c
1
B
O
y
j
p
M
h
Q
n
t
A
h
F
U
I
W
l
6
P
n
V
x
S
e
3
X
d
A
e
j
T
v
L
d
z
T
A
O
S
A
7
c
t
s
0
S
B
j
O
O
Z
P
l
x
8
p
2
S
2
t
Y
h
h
4
9
p
s
p
2
O
i
y
D
D
H
R
W
u
T
U
X
N
l
t
e
1
1
s
V
f
Q
j
8
N
W
i
R
d
Z
2
H
z
W
/
B
L
O
V
5
A
g
q
5
Z
M
Z
M
f
C
/
D
a
c
k
0
C
i
6
h
c
o
P
c
Q
M
b
4
g
l
3
C
x
E
L
F
E
j
D
T
c
n
V
+
R
d
u
W
m
d
E
4
1
f
Y
p
p
C
v
2
/
4
m
S
J
c
Y
U
S
W
S
d
9
Q
X
m
v
l
a
T
j
2
m
T
H
O
O
3
0
1
K
o
L
E
d
Q
/
H
Z
R
n
E
u
K
K
a
2
z
p
D
O
h
g
a
M
s
L
G
B
c
C
/
t
X
y
u
d
M
M
4
4
2
c
d
e
G
4
N
8
/
+
S
E
Y
n
n
Z
9
r
+
t
/
f
t
3
q
n
6
3
j
a
J
C
X
5
B
X
p
E
J
+
8
I
X
3
y
g
Z
y
T
A
e
H
k
K
/
l
O
f
p
J
f
z
r
X
z
w
/
n
t
/
L
m
1
b
j
j
r
m
U
N
y
p
5
y
/
/
w
C
0
+
a
o
E
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
v
o
A
x
a
M
+
o
b
w
r
T
4
7
t
H
V
I
T
M
R
l
7
n
k
U
=
"
>
A
A
A
C
P
n
i
c
b
V
B
N
a
9
t
A
E
F
2
l
T
e
o
q
X
2
5
y
7
G
W
p
M
T
g
k
G
C
k
U
W
i
g
G
k
1
5
6
a
U
m
h
/
g
D
L
E
a
v
1
K
F
6
8
W
o
n
d
U
b
A
Q
+
m
W
9
5
D
f
0
l
m
M
v
P
b
S
U
X
n
v
s
y
v
G
h
+
R
h
Y
e
P
P
e
G
2
b
n
R
Z
k
U
B
j
3
v
x
t
l
4
8
n
R
z
6
1
n
j
u
b
u
9
s
7
u
3
3
3
x
x
M
D
R
p
r
j
k
M
e
C
p
T
P
Y
6
Y
A
S
k
U
D
F
C
g
h
H
G
m
g
S
W
R
h
F
G
0
e
F
/
r
o
y
v
Q
R
q
T
q
C
x
Y
Z
T
B
N
2
q
U
Q
s
O
E
N
L
h
c
1
B
O
y
j
p
M
h
Q
n
t
A
h
F
U
I
W
l
6
P
n
V
x
S
e
3
X
d
A
e
j
T
v
L
d
z
T
A
O
S
A
7
c
t
s
0
S
B
j
O
O
Z
P
l
x
8
p
2
S
2
t
Y
h
h
4
9
p
s
p
2
O
i
y
D
D
H
R
W
u
T
U
X
N
l
t
e
1
1
s
V
f
Q
j
8
N
W
i
R
d
Z
2
H
z
W
/
B
L
O
V
5
A
g
q
5
Z
M
Z
M
f
C
/
D
a
c
k
0
C
i
6
h
c
o
P
c
Q
M
b
4
g
l
3
C
x
E
L
F
E
j
D
T
c
n
V
+
R
d
u
W
m
d
E
4
1
f
Y
p
p
C
v
2
/
4
m
S
J
c
Y
U
S
W
S
d
9
Q
X
m
v
l
a
T
j
2
m
T
H
O
O
3
0
1
K
o
L
E
d
Q
/
H
Z
R
n
E
u
K
K
a
2
z
p
D
O
h
g
a
M
s
L
G
B
c
C
/
t
X
y
u
d
M
M
4
4
2
c
d
e
G
4
N
8
/
+
S
E
Y
n
n
Z
9
r
+
t
/
f
t
3
q
n
6
3
j
a
J
C
X
5
B
X
p
E
J
+
8
I
X
3
y
g
Z
y
T
A
e
H
k
K
/
l
O
f
p
J
f
z
r
X
z
w
/
n
t
/
L
m
1
b
j
j
r
m
U
N
y
p
5
y
/
/
w
C
0
+
a
o
E
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
v
o
A
x
a
M
+
o
b
w
r
T
4
7
t
H
V
I
T
M
R
l
7
n
k
U
=
"
>
A
A
A
C
P
n
i
c
b
V
B
N
a
9
t
A
E
F
2
l
T
e
o
q
X
2
5
y
7
G
W
p
M
T
g
k
G
C
k
U
W
i
g
G
k
1
5
6
a
U
m
h
/
g
D
L
E
a
v
1
K
F
6
8
W
o
n
d
U
b
A
Q
+
m
W
9
5
D
f
0
l
m
M
v
P
b
S
U
X
n
v
s
y
v
G
h
+
R
h
Y
e
P
P
e
G
2
b
n
R
Z
k
U
B
j
3
v
x
t
l
4
8
n
R
z
6
1
n
j
u
b
u
9
s
7
u
3
3
3
x
x
M
D
R
p
r
j
k
M
e
C
p
T
P
Y
6
Y
A
S
k
U
D
F
C
g
h
H
G
m
g
S
W
R
h
F
G
0
e
F
/
r
o
y
v
Q
R
q
T
q
C
x
Y
Z
T
B
N
2
q
U
Q
s
O
E
N
L
h
c
1
B
O
y
j
p
M
h
Q
n
t
A
h
F
U
I
W
l
6
P
n
V
x
S
e
3
X
d
A
e
j
T
v
L
d
z
T
A
O
S
A
7
c
t
s
0
S
B
j
O
O
Z
P
l
x
8
p
2
S
2
t
Y
h
h
4
9
p
s
p
2
O
i
y
D
D
H
R
W
u
T
U
X
N
l
t
e
1
1
s
V
f
Q
j
8
N
W
i
R
d
Z
2
H
z
W
/
B
L
O
V
5
A
g
q
5
Z
M
Z
M
f
C
/
D
a
c
k
0
C
i
6
h
c
o
P
c
Q
M
b
4
g
l
3
C
x
E
L
F
E
j
D
T
c
n
V
+
R
d
u
W
m
d
E
4
1
f
Y
p
p
C
v
2
/
4
m
S
J
c
Y
U
S
W
S
d
9
Q
X
m
v
l
a
T
j
2
m
T
H
O
O
3
0
1
K
o
L
E
d
Q
/
H
Z
R
n
E
u
K
K
a
2
z
p
D
O
h
g
a
M
s
L
G
B
c
C
/
t
X
y
u
d
M
M
4
4
2
c
d
e
G
4
N
8
/
+
S
E
Y
n
n
Z
9
r
+
t
/
f
t
3
q
n
6
3
j
a
J
C
X
5
B
X
p
E
J
+
8
I
X
3
y
g
Z
y
T
A
e
H
k
K
/
l
O
f
p
J
f
z
r
X
z
w
/
n
t
/
L
m
1
b
j
j
r
m
U
N
y
p
5
y
/
/
w
C
0
+
a
o
E
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
v
o
A
x
a
M
+
o
b
w
r
T
4
7
t
H
V
I
T
M
R
l
7
n
k
U
=
"
>
A
A
A
C
P
n
i
c
b
V
B
N
a
9
t
A
E
F
2
l
T
e
o
q
X
2
5
y
7
G
W
p
M
T
g
k
G
C
k
U
W
i
g
G
k
1
5
6
a
U
m
h
/
g
D
L
E
a
v
1
K
F
6
8
W
o
n
d
U
b
A
Q
+
m
W
9
5
D
f
0
l
m
M
v
P
b
S
U
X
n
v
s
y
v
G
h
+
R
h
Y
e
P
P
e
G
2
b
n
R
Z
k
U
B
j
3
v
x
t
l
4
8
n
R
z
6
1
n
j
u
b
u
9
s
7
u
3
3
3
x
x
M
D
R
p
r
j
k
M
e
C
p
T
P
Y
6
Y
A
S
k
U
D
F
C
g
h
H
G
m
g
S
W
R
h
F
G
0
e
F
/
r
o
y
v
Q
R
q
T
q
C
x
Y
Z
T
B
N
2
q
U
Q
s
O
E
N
L
h
c
1
B
O
y
j
p
M
h
Q
n
t
A
h
F
U
I
W
l
6
P
n
V
x
S
e
3
X
d
A
e
j
T
v
L
d
z
T
A
O
S
A
7
c
t
s
0
S
B
j
O
O
Z
P
l
x
8
p
2
S
2
t
Y
h
h
4
9
p
s
p
2
O
i
y
D
D
H
R
W
u
T
U
X
N
l
t
e
1
1
s
V
f
Q
j
8
N
W
i
R
d
Z
2
H
z
W
/
B
L
O
V
5
A
g
q
5
Z
M
Z
M
f
C
/
D
a
c
k
0
C
i
6
h
c
o
P
c
Q
M
b
4
g
l
3
C
x
E
L
F
E
j
D
T
c
n
V
+
R
d
u
W
m
d
E
4
1
f
Y
p
p
C
v
2
/
4
m
S
J
c
Y
U
S
W
S
d
9
Q
X
m
v
l
a
T
j
2
m
T
H
O
O
3
0
1
K
o
L
E
d
Q
/
H
Z
R
n
E
u
K
K
a
2
z
p
D
O
h
g
a
M
s
L
G
B
c
C
/
t
X
y
u
d
M
M
4
4
2
c
d
e
G
4
N
8
/
+
S
E
Y
n
n
Z
9
r
+
t
/
f
t
3
q
n
6
3
j
a
J
C
X
5
B
X
p
E
J
+
8
I
X
3
y
g
Z
y
T
A
e
H
k
K
/
l
O
f
p
J
f
z
r
X
z
w
/
n
t
/
L
m
1
b
j
j
r
m
U
N
y
p
5
y
/
/
w
C
0
+
a
o
E
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
v
o
A
x
a
M
+
o
b
w
r
T
4
7
t
H
V
I
T
M
R
l
7
n
k
U
=
"
>
A
A
A
C
P
n
i
c
b
V
B
N
a
9
t
A
E
F
2
l
T
e
o
q
X
2
5
y
7
G
W
p
M
T
g
k
G
C
k
U
W
i
g
G
k
1
5
6
a
U
m
h
/
g
D
L
E
a
v
1
K
F
6
8
W
o
n
d
U
b
A
Q
+
m
W
9
5
D
f
0
l
m
M
v
P
b
S
U
X
n
v
s
y
v
G
h
+
R
h
Y
e
P
P
e
G
2
b
n
R
Z
k
U
B
j
3
v
x
t
l
4
8
n
R
z
6
1
n
j
u
b
u
9
s
7
u
3
3
3
x
x
M
D
R
p
r
j
k
M
e
C
p
T
P
Y
6
Y
A
S
k
U
D
F
C
g
h
H
G
m
g
S
W
R
h
F
G
0
e
F
/
r
o
y
v
Q
R
q
T
q
C
x
Y
Z
T
B
N
2
q
U
Q
s
O
E
N
L
h
c
1
B
O
y
j
p
M
h
Q
n
t
A
h
F
U
I
W
l
6
P
n
V
x
S
e
3
X
d
A
e
j
T
v
L
d
z
T
A
O
S
A
7
c
t
s
0
S
B
j
O
O
Z
P
l
x
8
p
2
S
2
t
Y
h
h
4
9
p
s
p
2
O
i
y
D
D
H
R
W
u
T
U
X
N
l
t
e
1
1
s
V
f
Q
j
8
N
W
i
R
d
Z
2
H
z
W
/
B
L
O
V
5
A
g
q
5
Z
M
Z
M
f
C
/
D
a
c
k
0
C
i
6
h
c
o
P
c
Q
M
b
4
g
l
3
C
x
E
L
F
E
j
D
T
c
n
V
+
R
d
u
W
m
d
E
4
1
f
Y
p
p
C
v
2
/
4
m
S
J
c
Y
U
S
W
S
d
9
Q
X
m
v
l
a
T
j
2
m
T
H
O
O
3
0
1
K
o
L
E
d
Q
/
H
Z
R
n
E
u
K
K
a
2
z
p
D
O
h
g
a
M
s
L
G
B
c
C
/
t
X
y
u
d
M
M
4
4
2
c
d
e
G
4
N
8
/
+
S
E
Y
n
n
Z
9
r
+
t
/
f
t
3
q
n
6
3
j
a
J
C
X
5
B
X
p
E
J
+
8
I
X
3
y
g
Z
y
T
A
e
H
k
K
/
l
O
f
p
J
f
z
r
X
z
w
/
n
t
/
L
m
1
b
j
j
r
m
U
N
y
p
5
y
/
/
w
C
0
+
a
o
E
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
U
v
o
A
x
a
M
+
o
b
w
r
T
4
7
t
H
V
I
T
M
R
l
7
n
k
U
=
"
>
A
A
A
C
P
n
i
c
b
V
B
N
a
9
t
A
E
F
2
l
T
e
o
q
X
2
5
y
7
G
W
p
M
T
g
k
G
C
k
U
W
i
g
G
k
1
5
6
a
U
m
h
/
g
D
L
E
a
v
1
K
F
6
8
W
o
n
d
U
b
A
Q
+
m
W
9
5
D
f
0
l
m
M
v
P
b
S
U
X
n
v
s
y
v
G
h
+
R
h
Y
e
P
P
e
G
2
b
n
R
Z
k
U
B
j
3
v
x
t
l
4
8
n
R
z
6
1
n
j
u
b
u
9
s
7
u
3
3
3
x
x
M
D
R
p
r
j
k
M
e
C
p
T
P
Y
6
Y
A
S
k
U
D
F
C
g
h
H
G
m
g
S
W
R
h
F
G
0
e
F
/
r
o
y
v
Q
R
q
T
q
C
x
Y
Z
T
B
N
2
q
U
Q
s
O
E
N
L
h
c
1
B
O
y
j
p
M
h
Q
n
t
A
h
F
U
I
W
l
6
P
n
V
x
S
e
3
X
d
A
e
j
T
v
L
d
z
T
A
O
S
A
7
c
t
s
0
S
B
j
O
O
Z
P
l
x
8
p
2
S
2
t
Y
h
h
4
9
p
s
p
2
O
i
y
D
D
H
R
W
u
T
U
X
N
l
t
e
1
1
s
V
f
Q
j
8
N
W
i
R
d
Z
2
H
z
W
/
B
L
O
V
5
A
g
q
5
Z
M
Z
M
f
C
/
D
a
c
k
0
C
i
6
h
c
o
P
c
Q
M
b
4
g
l
3
C
x
E
L
F
E
j
D
T
c
n
V
+
R
d
u
W
m
d
E
4
1
f
Y
p
p
C
v
2
/
4
m
S
J
c
Y
U
S
W
S
d
9
Q
X
m
v
l
a
T
j
2
m
T
H
O
O
3
0
1
K
o
L
E
d
Q
/
H
Z
R
n
E
u
K
K
a
2
z
p
D
O
h
g
a
M
s
L
G
B
c
C
/
t
X
y
u
d
M
M
4
4
2
c
d
e
G
4
N
8
/
+
S
E
Y
n
n
Z
9
r
+
t
/
f
t
3
q
n
6
3
j
a
J
C
X
5
B
X
p
E
J
+
8
I
X
3
y
g
Z
y
T
A
e
H
k
K
/
l
O
f
p
J
f
z
r
X
z
w
/
n
t
/
L
m
1
b
j
j
r
m
U
N
y
p
5
y
/
/
w
C
0
+
a
o
E
<
/
l
a
t
e
x
i
t
>
2.2. Virtual adversarial training

VAT [17] is an effective regularization method for SSL.
The virtual adversarial loss introduced in VAT is deﬁned by
the robustness of the classiﬁer against local perturbation in
the input space RD. Hence VAT imposes a kind of smooth-
ness condition on the classiﬁer. Mathematically, the virtual
adversarial loss in VAT for SSL is

L(Dl, Dul, θ) :=E(xl,yl)∈Dl ℓ(yl, p(y|xl, θ))

+ αEx∈DRvat(x, θ).

(4)

The VAT regularization Rvat is deﬁned as

Rvat(x; θ) := max
krk2≤ǫ

dist(p(y|x, θ), p(y|x + r, θ)),

(5)

where dist(·, ·) is some distribution distance measure and
ǫ controls the magnitude of the adversarial example. For
simplicity, deﬁne

F (x, r, θ) := dist(p(y|x, θ), p(y|x + r, θ)).

(6)

Then Rvat = maxkrk2≤ǫ F (x, r, θ). And the so called vir-
tual adversarial example is r∗ := argmaxkrk≤ǫF (x, r, θ).
Once we have r∗, the VAT loss can be optimized with the
objective as L(Dl, Dul, θ) = E(xl,yl)∈Dl ℓ(yl, p(y|xl, θ))
+ αEx∈DF (x, r∗, θ).

To obtain the virtual adversarial example r∗,

[17]
suggested to apply second order Taylor’s expansion to
F (x, r, θ) around r = 0 as

F (x, r, θ) ≈

1
2

rT Hr,

(7)

where H := ∇2
rF (x, r, θ)|r=0 denotes the Hessian of F
with respect to r. The vanishing of the ﬁrst two terms in
Taylor’s expansion occurs because that dist(·, ·) is a dis-
tance measure with minimum zero and r = 0 is the corre-
sponding optimal value, indicating that at r = 0, both the
value and the gradient of F (x, r, θ) are zero. Therefore for
1
small enough ǫ, r∗ ≈ argmaxkrk2≤ǫ
2 rT Hr, which is an
eigenvalue problem and the direction of r∗ can be solved
by power iteration.

2.3. Generative models for data manifold

We take advantage of generative model with both en-
coder h and decoder g to estimate the underlying data man-
ifold M and its tangent space TxM. As assumed by pre-
vious works [11, 13], perfect generative models with both
decoder and encoder can describe the data manifold, where
the decoder g(z) and the encoder h(x) together serve as
the coordinate chart of manifold M. Note that the encoder
is indispensable for it helps to identify the manifold coordi-
nate z = h(x) for point x ∈ M. With the trained generative
model, the tangent space is given by TxM = Jzg(Rd), or
the span of the columns of J = Jzg.

In this work, we adopt VAE [10] and localized GAN [22]
to learn the targeted underlying data manifold M as sum-
marized below.

VAE VAE [10] is a well known generative model con-
sisting of both encoder and decoder. The training of VAE is
by optimizing the variational lower bound of log likelihood,

log p(x, θ) ≥E

z∼q(z|x,θ)(cid:2)log p(x|z, θ)(cid:3)

− KL(q(z|x, θ)kp(z)).

(8)

Here p(z) is the prior of hidden variable z, and q(z|x, θ),
p(x|z, θ) models the encoder and decoder in VAE, respec-
tively. The derivation of the lower bound with respect to θ
is well deﬁned thanks to the reparameterization trick, thus
it could be optimized by gradient based method. The lower
bound could also be interpreted as a reconstruction term
plus a regularization term [10]. With a trained VAE, the
encoder and decoder are given as h(x) = argmaxzq(z|x)
and g(z) = argmaxxq(x|z) accordingly.

Localized GAN Localized GAN [22] suggests to use a
localized generator G(x, z) to replace the global generator
g(z) in vanilla GAN [7]. The key difference between lo-
calized GAN and previous generative model for manifold
is that, localized GAN learns a distinguishing local coordi-
nate chart for each point x ∈ M, which is given by G(x, z),
rather than one global coordinate chart. To model the local
coordinate chart in data manifold, localized GAN requires
the localized generator to satisfy two more regularity con-
ditions:

locality G(x, 0) = x, so that G(x, z) is localized around

x;

∂z (cid:17)T
orthogonmality (cid:16) ∂G(x,z)

G(x, z) is non-degenerated.

∂G(x,z)

∂z

= I,

to ensure

The two conditions are achieved by the following penalty
during training of localized GAN:

Rlocalized GAN := µ1(cid:13)(cid:13)G(x, 0) − x(cid:13)(cid:13)

2

+

(cid:18) ∂G(x, z)

∂z (cid:19)T ∂G(x, z)

∂z

µ2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(9)

2

.

− I(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Since G(x, z) deﬁnes a local coordinate chart for each x
separately, in which the latent encode of x is z = 0, there
is no need for the extra encoder to provide the manifold
representation of x.

3. Method

In this section we elaborate our proposed tangent-normal
adversarial regularization (TNAR) strategy. The TNAR

10678

loss to be minimized for SSL is

framework is as

L(Dl, Dul, θ) :=E(xl,yl)∈Dl ℓ(cid:0)yl, p(y|xl, θ)(cid:1)

+ α1Ex∈DRtangent(x, θ)
+ α2Ex∈DRnormal(x, θ).

(10)

The ﬁrst term in Eq. (10) is a common used supervised loss,
e.g., negative cross entropy. Rtangent and Rnormal is the so
called tangent adversarial regularization (TAR) and nor-
mal adversarial regularization (NAR) accordingly, jointly
forming the proposed TNAR. In the following section, we
assume that we already have a well trained generative model
for the underlying data manifold M, with encoder h and de-
coder g, which can be obtained as described in Section 2.3.

3.1. Tangent adversarial regularization

Vanilla VAT penalizes the variety of the classiﬁer against
local perturbation in the input space RD [17], which might
overly regularize the classiﬁer, since the semi-supervised
learning assumption only indicates that the true conditional
distribution varies smoothly along the underlying manifold
M, but not the whole input space RD [1, 24, 19]. To avoid
this shortcoming of vanilla VAT, we propose the tangent ad-
versarial regularization (TAR), which restricts virtual adver-
sarial training to the tangent space of the underlying man-
ifold TxM, to enforce manifold invariance property of the
classiﬁer.

Rtangent(x; θ) :=

max

krk2≤ǫ,r∈TxM=Jz g(Rd)

F (x, r, θ),

(11)

where F (x, r, θ) is deﬁned as in Eq. (6). To optimize
Eq. (11), we ﬁrst apply Taylor’s expansion to F (x, r, θ) so
that

Rtangent(x; θ) ≈

max

krk2≤ǫ,r∈TxM=Jz g(Rd)

1
2

rT Hr,

(12)

where the notations and the derivation are as in Eq. (7). We
further reformulate Rtangent as

v ← J T HJη;
µ ← (J T J)−1v;

η ←

µ
kµk2

.

(15)

Now we elaborate the detailed implementation of each step
in Eq. (15).

Computing J T HJη. Note that z = h(x), x = g(z).

Deﬁne

For

r(η) := g(z + η) − g(z).

F (cid:0)x, r(η), θ(cid:1) = dist(p(y|x, θ)kp(y|x + r(η), θ)),

we have

(16)

(17)

∇2

ηF (x, r(η), θ) = (Jz+ηg)T ∇2

rF (x, r(η), θ)(Jz+ηg)

+∇2

ηg(z + η) · ∇rF (x, r(η), θ).

(18)
While on the other hand, since dist(·, ·) is some distance
measure with minimum zero and r(0) = 0 is the corre-
sponding optimal value, we have

F (x, r(0), θ) = 0, ∇rF (x, r(0), θ) = 0.

(19)

Therefore,

∇2

ηF (x, r(0), θ) = (Jzg)T ∇2

rF (x, r(0), θ)Jzg = J T HJ.
(20)
Thus the targeted matrix vector product could be efﬁciently
computed as

J T HJη = ∇2

ηF (x, r(0), θ)·η = ∇η (cid:0)∇ηF (x, r(0), θ) · η(cid:1) .

(21)
Note that ∇ηF (x, r(0), θ) · η is a scalar, hence the gradient
of which could be obtained by back propagating the net-
work for once. And it only costs twice back propagating for
the computation of J T HJη.

maximize

r∈RD

1
2

rT Hr,

η ∈ Rd.
s.t. krk2 ≤ ǫ,
(J := Jzg ∈ RD×d, H ∈ RD×D)

r = Jη,

Solving J T Jµ = v. Similarly, deﬁne

(13)

K(η) := (cid:0)g(z + η) − g(z)(cid:1)T (cid:0)g(z + η) − g(z)(cid:1) .

We have

(22)

∇2

ηK(η) = (Jz+ηg)T Jz+ηg + ∇2

ηg(z + η) · K(η). (23)

Or equivalently,

maximize

η∈Rd

1
2

ηT J T HJη,

s.t. ηT J T Jη ≤ ǫ2.

(14)

∇2

ηK(0) = (Jzg)T Jzg = J T J.

(24)

Since K(0) = 0, we have

This is a classic generalized eigenvalue problem, the opti-
mal solution η∗ of which could be obtained by power it-
eration and conjugate gradient (and scaling). The iteration

Thus the matrix vector product J T Jµ could be evaluated
similarly as

J T Jµ = ∇η (cid:0)∇ηK(0) · µ(cid:1) .

(25)

10679

The extra cost for evaluating J T Jµ is still back propagating
the network for twice. Due to J T J being positive deﬁnite
(g is non-degenerated), we can apply several steps of con-
jugate gradient to solve J T Jµ = v efﬁciently.

By iterating Eq. (15), we obtain the optimal solution ηk
of Eq. (14). The desired optimal solution is then rk =
ǫJηk/kJηkk, hence Rtangent(x; θ) = F (x, rk, θ), which
could be optimized by popular gradient optimizers.

3.2. Normal adversarial regularization

Motivated by the noisy observation assumption indicat-
ing that the observed data contains noise driving them off
the underlying manifold, we further come up with the nor-
mal adversarial regularization (NAR) to enforce the robust-
ness of the classiﬁer against such noise, by performing vir-
tual adversarial training in the normal space. The mathe-
matical description is

Rnormal(x; θ) :=

max

krk2≤ǫ,r⊥TxM

F (x, r, θ)

≈

max

krk2≤ǫ,r⊥TxM

1
2

rT Hr.

(26)

Note that TxM is spanned by the columns of J = Jzg,
thus r⊥TxM ⇔ J T · r = 0. Therefore we could reformu-
late Eq. (26) as

maximize

r∈RD

s.t.

rT Hr,

1
2
krk2 ≤ ǫ,

J T · r = 0.

(27)

However, Eq. (27) is not easy to optimize since J T · r can-
not be efﬁciently computed. To overcome this, instead of
requiring r being orthogonal to the whole tangent space
TxM, we take a step back to demand r being orthogonal
to only one speciﬁc tangent direction, i.e., the tangent space
adversarial perturbation rk. Thus the constraint J T · r = 0
is relaxed to (rk)T · r = 0. And we further replace the
constraint by a regularization term,

maximize

r∈RD

s.t.

rT Hr − λrT (rkrT

k )r,

1
2
krk2 ≤ ǫ,

(28)

where λ is a hyperparameter introduced to control the or-
thogonality of r.

Since Eq. (28) is again an eigenvalue problem, and we
can apply power iteration to solve it. Note that a small
identity matrix λkrkkI is needed to be added to keep
1
2 H − λrkrT
k + λkrkkI semi-positive deﬁnite, which does
not change the optimal solution of the eigenvalue problem.
The power iteration is as

r ←

1
2

Hr − λ(rk)T rkr + λkrkkr.

(29)

And the evaluation of Hr is by

Hr = ∇r(cid:0)∇rF (x, 0, θ) · r(cid:1) ,

(30)

which could be computed efﬁciently. After ﬁnding the
optimal solution of Eq. (28) as r⊥,
the NAR becomes
Rnormal(x, θ) = F (x, r⊥, θ).

Finally, as suggested in [17], we add entropy regulariza-
tion to our loss function. It ensures neural networks to out-
put more determinate predictions and has implicit beneﬁts
for performing virtual adversarial training.

Rentropy(x, θ) := −Xy

Our ﬁnal loss for SSL is

p(y|x, θ) log p(y|x, θ).

(31)

L(Dl, Dul, θ) :=E(xl,yl)∈Dl ℓ(cid:0)yl, p(y|xl, θ)(cid:1)

+ α1Ex∈DRtangent(x, θ)
+ α2Ex∈DRnormal(x, θ)
+ α3Ex∈DRentropy(x, θ).

(32)

4. Comparison to other methods

Virtual adversarial training Our proposed TNAR serves
as an extension of VAT, by taking the information of data
manifold into consideration. VAT equally penalizes the
smoothness along each dimension of the whole observation
space, not discriminating different directions. In contrast,
TNAR enforces the smoothness of the classiﬁer along the
manifold and orthogonal to the manifold separately. This
separate treatment along the two directions allows TNAR
to impose different scales of smoothness along the tangent
space and the normal space of the data manifold, which
is particularly crucial for inducing desired regularization
effect. To illustrate this, considering an image sample,
its Euclidean neighborhood in the input space could con-
tain many inter-class samples, besides intra-class ones, as
demonstrated in Figure 2. Thus the output of the ideal clas-
siﬁer must vary signiﬁcantly inside such Euclidean neigh-
borhood to correctly classify the contained samples, which
makes it essentially improper for VAT to enforce that the
classiﬁer does not change much inside this Euclidean ball.
A more reasonable treatment is to adopt the manifold as-
sumption and impose different scales of smoothness of the
classiﬁer along the manifold and its orthogonal direction, as
TNAR has done.

Jacobian based manifold regularization As explained
in Eq. (2) and Eq. (3), tangent propagation [26, 11] and
manifold Laplacian norm [1, 13, 22] are also popular meth-
ods for realizing manifold regularization for SSL. However,
our TNAR is the ﬁrst to use VAT constructing manifold reg-
ularization. The difference between TNAR and Jacobian
norm based manifold regularization is two folds.

10680

3500

3000

2500

2000

1500

e
c
n
a
t
s
i
D
 
n
a
e
d

i
l
c
u
E
 
s
s
a
l
c
-
r
e
t
n

I

1000

1000

1500
3500
Intra-class Euclidean Distance

2000

2500

3000

l

l

s
e
p
m
a
s
 
t
s
e
r
a
e
n
-
K
 
n
i
 
s
e
p
m
a
S
 
s
s
a
l
c
-
a
r
t
n
I
 
f
o
 
o
i
t
a
R

1.0

0.8

0.6

0.4

0.2

0.0

20

40

K

60

80

100

the smallest
Figure 2. Left: the smallest intra-class distance vs.
inter-class distance for CIFAR-10 dataset. X-axis:
the smallest
Euclidean distance to the other examples of the same class. Y-
axis: the smallest Euclidean distance to the other examples of the
different class. We only plot such coordinate for 500 examples.
Right: the ratio of intra-class example among its K-nearest neigh-
borhood for CIFAR-10 dataset. The ratio is averaged over 500
examples. From the ﬁgures we clearly see that for most examples,
1) the smallest inter-class distance is shorter or at least about the
same scale as the least intra-class distance, and 2) its K-nearest
neighborhood contains more inter-class examples than intra-class
examples.

Firstly, they lead to different manifold smoothness con-
ditions on the classiﬁer. Tangent propagation and mani-
fold Laplacian norm smooth the classiﬁer by regularizing its
norm of the manifold Jacobian. TNAR, on the other hand,
smooths the classiﬁer through penalizing the virtual adver-
sarial loss deﬁned by the distance of an example with its tan-
gent directional virtual adversarial example. This involves
the second order information of the virtual adversarial loss
along the manifold. Theoretically, it is not easy to say that
one smoothness is superior to the other. Nonetheless, em-
pirical experiments on multiple datasets (Section 5) suggest
that our proposed TNAR achieves better performance on
SSL. We leave the theoretical analysis as future work.

Secondly, as shown in Eq. (2) and Eq. (3), all the exist-
ing Jacobian based manifold regularization requires evalu-
ating the Jacobian of either the classiﬁer, or the generator
as manifold coordinate chart, which is prohibitively feasi-
ble for modern high-dimensional datasets given large neural
networks. Alternatively, some works suggested stochasti-
cally evaluating these Jacobian based regularization terms.
Kumar at.el. [11] proposed to randomly preserve several
columns of Jzg as the approximation of the tangent space
TxM, and Lecouat at.el. [13] applied the norm of several
directional gradients to approximate the norm of the Ja-
cobian. However, such stochastic strategies, unfortunately
with high variance, could cause implicit side affects on the
manifold smoothness of the classiﬁer. Compared with them,
the computational cost of our proposed TNAR does not
rely on the dimensionality of the datasets, since performing
VAT only requires several times of power iteration (typi-
cally once), and TNAR adds constant extra times of back or

forward propagation to VAT. This advantage makes TNAR a
potentially better manifold regularization method for mod-
ern semi-supervised learning tasks.

Other approaches for SSL There is also a wide class of
SSL framework based on GAN [25, 20, 6, 5]. Most of them
modify the discriminator to include a classiﬁer, by splitting
the real class of original discriminator into K subclasses,
where K is the number of classes of labeled data. The fea-
tures extracted for distinguishing the example being real or
fake, which can be viewed as a kind of coarse label, have
implicit beneﬁts for supervised classiﬁcation task. Though
in TNAR, GAN with encoder could be adopted as a method
to identify the underlying manifold, these two kinds of ap-
proaches are motivated from different perspectives. TNAR
focuses on the manifold regularization other than the feature
sharing as in the GAN frameworks for SSL.

Besides above, there are also other strategies for SSL,
e.g., Tripple GAN [14], Mean Teacher [27], Π model [12],
CCLP [9] etc. We leave the comparison of the performance
with TNAR in Section 5.

5. Experiments

To demonstrate the advantages of our proposed TNAR
for SSL, we conduct a series of experiments on both ar-
tiﬁcial and real datasets. The tested TNAR based meth-
ods for SSL include: (1) TNAR-VAE: TNAR with the un-
derlying manifold estimated by VAE; (2) TNAR-LGAN:
TNAR with the underlying manifold estimated by localized
GAN; (3) TNAR-Manifold: TNAR with oracle underly-
ing manifold for the observed data, only used for artiﬁcial
dataset; (4) TNAR-AE: TNAR with the underlying man-
ifold estimated roughly by autoendoer, only used for arti-
ﬁcial dataset; (5) TAR: tangent adversarial regularization
for ablation study; (6) NAR: normal adversarial regulariza-
tion for ablation study. If not stated otherwise, all the above
methods contain entropy regularization term.

5.1. Two rings artiﬁcial dataset

We ﬁrst introduce experiments on a two-rings artiﬁcial
dataset to show the effectiveness of TNAR intuitively. In
this experiments, there is 3, 000 unlabeled data (gray dots)
and 6 labeled data (blue dots), 3 for each class. The detailed
construction could be found in Supplementary Materials.

The performance of each compared methods is shown
in Table 1, and the corresponding classiﬁcation boundary is
demonstrated in Figure 3. The TNAR under true underly-
ing manifold (TNAR-Manifold) perfectly classiﬁes the two-
rings dataset with merely 6 labeled data, while the other
methods fail to predict the correct decision boundary. The
failure of VAT supports our claims of its shortcut in Sec-
tion 4. Even with the underlying manifold roughly approxi-

10681

1.0

0.5

0.0

−0.5

−1.0

SL
VAT
TNAR-AE
TNAR-Manifold

−1.0

−0.5

0.0

0.5

1.0

Figure 3. The decision boundaries of compared methods on two-
rings artiﬁcial dataset. Gray dots distributed on two rings: the un-
labeled data. Blue dots (3 in each ring): the labeled data. Colored
curves: the decision boundaries found by compared methods.

Table 1. Classiﬁcation errors (%) of compared methods on two-
ring artiﬁcial dataset. We test with and without entropy regulariza-
tion in each method and report the best one. In VAT and TNAR-
AE, without entropy regularization is better; For TNAR-Manifold,
adding entropy regularization is better.

Model
Labeled data only
VAT
TNAR-AE
TNAR-Manifold
TNAR-Manifold (ent)

Error (%)
32.95
23.80
12.45
9.90
0

mated by an autoendoer, our approach (TNAR-AE) outper-
forms VAT in this artiﬁcial dataset. However, the perfor-
mance of TNAR-AE is worse than TNAR-Manifold, indi-
cating that the effectiveness of TNAR relies on the quality
of estimating the underlying manifold.

5.2. FashionMNIST

We also conduct experiments on FashionMNIST
dataset1. There are three sets of experiments with the num-
ber of labeled data being 100, 200 and 1, 000, respectively.
The details about the networks are in Supplementary Mate-
rials.

The corresponding results are shown in Table 2, from
which we observe at least two phenomena. The ﬁrst is that
our proposed TNAR methods (TNAR-VAE, TNAR-LGAN)
achieve lower classiﬁcation errors than VAT in all circum-
stances with different number of labeled data. The second
is that the performance of our method depends on the es-
timation of the underlying manifold of the observed data.
In this case, TNAR-VAE brings larger improvement than
TNAR-LGAN, since VAE produces better diverse examples

1https://github.com/zalandoresearch/

fashion-mnist

according to our observation.

5.3. CIFAR 10 and SVHN

There are two classes of experiments for demonstrating
the effectiveness of TNAR in SSL, SVHN with 1, 000 la-
beled data, and CIFAR-10 with 4, 000 labeled data. The ex-
periment setups are identical with [17]. We test two kinds
of convolutional neural networks as classiﬁer (denoted as
”small” and ”large”) as in [17]. We test both VAE and
Localized GAN as the underlying data manifold. More
detailed experimental settings are included in Supplemen-
tary Materials. We test the performance of TNAR with or
without data augmentation, with the identical augmentation
strategy used in [17]. Note that when perform TNAR with
data augmentation, the corresponding data manifold should
also be trained with data augmentation. It is worth to re-
mark that VAT [17] and VAT + SNTG [15] adopts ZCA as
pre-processing on CIFAR-10 experiments, while we do not
use this trick implementing TNAR experiments.

In Table 3 we report the experiments results on SVHN
and CIFAR-10, without data augmentation. And in Ta-
ble 4 the results on SVHN and CIFAR-10 with data aug-
mentation are presented. The comparison demonstrates that
our proposed TNAR outperforms all the other state-of-the-
art SSL methods as far as we known on both SVHN and
CIFAR-10, with or without data augmentation. Especially,
compared with VAT or manifold regularization like Im-
proved GAN + JacobRegu + tagent [11] or Improved GAN
+ ManiReg [13], TNAR brings an evident improvements to
them, as our analysis in Section 4 has suggested. Similar
to experiments on FashionMNIST datasets, we observe that
for TNAR, the underlying manifold identiﬁed by VAE ben-
eﬁts more than the manifold identiﬁed by Localized GAN.
We attribute this phenomenon to the relatively lacking of
diversity of the images generated by Localized GAN.

5.4. Ablation study

We conduct ablation study on FashionMNIST, SVHN
and CIFAR-10 datasets to demonstrate that both of the two
regularization terms in TNAR are crucial for SSL. The re-
sults are reported in Table 2 and the last two lines in Ta-
ble 3. Removing either tangent adversarial regularization or
normal adversarial regularization will harm the ﬁnal perfor-
mance, since they fail to enforce the manifold invariance or
the robustness against the off-manifold noise. In together,
the proposed TNAR achieves the best performance.

Furthermore, the adversarial perturbations and adver-
sarial examples from FashionMNIST and CIFAR-10 are
shown in Figure 4. We can easily observe that the tangent
adversarial perturbation focuses on the edges of foreground
objects, while the normal space perturbation mostly appears
as certain noise over the whole image. This is consistent
with our understanding on the role of perturbation along the

10682

Table 2. Classiﬁcation errors (%) of compared methods on FashionMNIST dataset.

Method

VAT

100 labels

27.69

200 labels

20.85

1000 labels

14.51

TNAR/TAR/NAR-LGAN 23.65/24.87/28.73
23.35/26.45/27.83
TNAR/TAR/NAR-VAE

18.32/19.16/24.49
17.23/20.53/24.81

13.52/14.09/15.94
12.86/14.02/15.44

Table 3. Classiﬁcation errors (%) of compared methods on SVHN
and CIFAR-10 datasets without data augmentation.

Method

VAT (small) [17]
VAT (large) [17]
VAT + SNTG [15]
Π model [12]
Mean Teacher [27]
CCLP [9]
ALI [6]
Improved GAN [25]
Tripple GAN [14]
Bad GAN [5]
LGAN [22]
Improved GAN +
JacobRegu + tangent [11]
Improved GAN +
ManiReg [13]
TNAR-LGAN (small)
TNAR-LGAN (large)
TNAR-VAE (small)
TNAR-VAE (large)
TAR-VAE (large)
NAR-VAE (large)

SVHN

1,000 labels
6.83 ± 0.24
4.28 ± 0.10
4.02 ± 0.20
5.43 ± 0.25
5.21 ± 0.21
5.69 ± 0.28
7.41 ± 0.65
8.11 ± 1.3
5.77 ± 0.17
4.25 ± 0.03
4.73 ± 0.16

CIFAR-10
4,000 labels
14.87 ± 0.13
13.15 ± 0.21
12.49 ± 0.36
16.55 ± 0.29
17.74 ± 0.30
18.57 ± 0.41
17.99 ± 1.62
18.63 ± 2.32
16.99 ± 0.36
14.41 ± 0.30
14.23 ± 0.27

4.39 ± 1.20

16.20 ± 1.60

4.51 ± 0.22

14.45 ± 0.21

4.25 ± 0.09
4.03 ± 0.13
3.99 ± 0.08
3.80 ± 0.12
5.62 ± 0.19
4.05 ± 0.04

12.97 ± 0.31
12.76 ± 0.04
12.39 ± 0.11
12.06 ± 0.35
13.87 ± 0.32
15.91 ± 0.09

Table 4. Classiﬁcation errors (%) of compared methods on SVHN
and CIFAR-10 datasets with data augmentation.

Method

VAT (large) [17]
VAT + SNTG [15]
Π model [12]
Temporal ensembling [12]
Mean Teacher [27]
LGAN [22]
TNAR-VAE (large)

SVHN

1,000 labels
3.86 ± 0.11
3.83 ± 0.22
4.82 ± 0.17
4.42 ± 0.16
3.95 ± 0.19
-
3.74 ± 0.04

CIFAR-10
4,000 labels
10.55 ± 0.05
9.89 ± 0.34
12.36 ± 0.31
12.16 ± 0.24
12.31 ± 0.28
9.77 ± 0.13
8.85 ± 0.03

two directions that capture the different aspects of smooth-
ness.

6. Discussion

As shown in our experiments, the data manifold is cru-
cial for the improvement of our proposed TNAR. Though
TNAR seems to work with a wide range of manifold coor-
dinate chart, e.g., VAE and Localized GAN, it is still not
clear which kind of manifold beneﬁts most for TNAR. Dai

Figure 4. The perturbations and adversarial examples in the tan-
gent space and the normal space. Note that the perturbations is
actually too small to distinguish easily, thus we show the scaled
perturbations. First row: FashionMNIST dataset; Second row:
CIFAR-10 dataset. From left to right: original example, tangent
adversarial perturbation, normal adversarial perturbation, tangent
adversarial example, normal adversarial example.

at.el [5] suggested that a bad generator works better for
GAN based framework for semi-supervised learning. Our
experiments agree with this argument to some extent. Lo-
calized GAN could produce detailed images than VAE, but
the latter cooperates better with TNAR in all our experi-
ments. At current stage, we conjecture that a more diverse
generator helps more for TNAR, since diversity on gen-
erator enables TNAR to explore more different directions
along the data manifold. The throughout analysis is left for
future work.

7. Conclusion

We present the tangent-normal adversarial regularization
for semi-supervised learning, a novel regularization strategy
based on virtual adversarial training and manifold regular-
ization. TNAR is composed of regularization on the tan-
gent and normal space separately. The tangent adversar-
ial regularization enforces manifold invariance of the clas-
siﬁer, while the normal adversarial regularization imposes
robustness of the classiﬁer against the noise contained in the
observed data. Experiments on synthetic and real datasets
demonstrate that our approach outperforms other state-of-
the-art methods for semi-supervised learning.

Acknowledgement

This work was supported by National Natural Sci-
ence Foundation of China (Grant No.61806009), Beijing
Natural Science Foundation (Grant No. 4184090), and
Intelligent Manufacturing Action Plan of Industrial Solid
JCKY2018204C004).
Foundation Program (Grant No.

10683

References

[1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regular-
ization: A geometric framework for learning from labeled
and unlabeled examples. Journal of machine learning re-
search, 7(Nov):2399–2434, 2006.

[2] Y. Bengio, L. Yao, G. Alain, and P. Vincent. Generalized
denoising auto-encoders as generative models. In Advances
in Neural Information Processing Systems, pages 899–907,
2013.

[3] L. Cayton. Algorithms for manifold learning. Univ. of Cali-

fornia at San Diego Tech. Rep, 12(1-17):1, 2005.

[4] O. Chapelle, B. Scholkopf, and A. Zien. Semi-supervised
learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE
Transactions on Neural Networks, 20(3):542–542, 2009.

[5] Z. Dai, Z. Yang, F. Yang, W. W. Cohen, and R. R. Salakhut-
dinov. Good semi-supervised learning that requires a bad
gan. In Advances in Neural Information Processing Systems,
pages 6510–6520, 2017.

[6] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversarially
learned inference. arXiv preprint arXiv:1606.00704, 2016.

[7] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.

[8] I. J. Goodfellow, J. Shlens, and C. Szegedy.

ing and harnessing adversarial examples.
arXiv:1412.6572, 2014.

Explain-
arXiv preprint

[9] K. Kamnitsas, D. C. Castro, L. L. Folgoc,

I. Walker,
R. Tanno, D. Rueckert, B. Glocker, A. Criminisi, and
A. Nori. Semi-supervised learning via compact latent space
clustering. arXiv preprint arXiv:1806.02679, 2018.

[10] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013.

[11] A. Kumar, P. Sattigeri, and T. Fletcher. Semi-supervised
learning with gans: Manifold invariance with improved in-
ference. In Advances in Neural Information Processing Sys-
tems, pages 5540–5550, 2017.

[12] S. Laine and T. Aila.

supervised learning.
2016.

Temporal ensembling for semi-
arXiv preprint arXiv:1610.02242,

[13] B. Lecouat, C.-S. Foo, H. Zenati, and V. R. Chandrasekhar.
Semi-supervised learning with gans: Revisiting manifold
regularization. arXiv preprint arXiv:1805.08957, 2018.

[14] C. Li, K. Xu, J. Zhu, and B. Zhang. Triple generative adver-

sarial nets. arXiv preprint arXiv:1703.02291, 2017.

[15] Y. Luo, J. Zhu, M. Li, Y. Ren, and B. Zhang. Smooth neigh-
bors on teacher graphs for semi-supervised learning. arXiv
preprint arXiv:1711.00258, 2017.

[16] T. Miyato, A. M. Dai, and I. Goodfellow. Adversarial train-
ing methods for semi-supervised text classiﬁcation. arXiv
preprint arXiv:1605.07725, 2016.

[17] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii. Virtual ad-
versarial training: a regularization method for supervised and
semi-supervised learning. arXiv preprint arXiv:1704.03976,
2017.

[18] H. Narayanan and S. Mitter. Sample complexity of testing
the manifold hypothesis. In Advances in Neural Information
Processing Systems, pages 1786–1794, 2010.

[19] P. Niyogi. Manifold regularization and semi-supervised
learning: Some theoretical analyses. The Journal of Machine
Learning Research, 14(1):1229–1250, 2013.

[20] A. Odena. Semi-supervised learning with generative adver-

sarial networks. arXiv preprint arXiv:1606.01583, 2016.

[21] A. Oliver, A. Odena, C. Raffel, E. D. Cubuk, and I. J. Good-
fellow. Realistic evaluation of deep semi-supervised learning
algorithms. arXiv preprint arXiv:1804.09170, 2018.

[22] G.-J. Qi, L. Zhang, H. Hu, M. Edraki, J. Wang, and X.-S.
Hua. Global versus localized generative adversarial nets. In
Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018.

[23] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and
T. Raiko. Semi-supervised learning with ladder networks. In
Advances in Neural Information Processing Systems, pages
3546–3554, 2015.

[24] S. Rifai, Y. N. Dauphin, P. Vincent, Y. Bengio, and X. Muller.
The manifold tangent classiﬁer. In Advances in Neural Infor-
mation Processing Systems, pages 2294–2302, 2011.

[25] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, and X. Chen. Improved techniques for training gans. In
Advances in Neural Information Processing Systems, pages
2234–2242, 2016.

[26] P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri.
Transformation invariance in pattern recognitiontangent dis-
tance and tangent propagation. In Neural networks: tricks of
the trade, pages 239–274. Springer, 1998.

[27] A. Tarvainen and H. Valpola. Mean teachers are better role
models: Weight-averaged consistency targets improve semi-
supervised deep learning results. In Advances in neural in-
formation processing systems, pages 1195–1204, 2017.

10684

