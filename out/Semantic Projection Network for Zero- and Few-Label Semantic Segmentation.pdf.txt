Semantic Projection Network for Zero- and Few-Label Semantic Segmentation

Yongqin Xian1 ∗ Subhabrata Choudhury1∗

Yang He1

Bernt Schiele1

Zeynep Akata1

2

,

1Max Planck Institute for Informatics

2Amsterdam Machine Learning Lab

Saarland Informatics Campus

University of Amsterdam

Abstract

Semantic segmentation is one of the most fundamental
problems in computer vision. As pixel-level labelling in this
context is particularly expensive, there have been several
attempts to reduce the annotation effort, e.g. by learning
from image level labels and bounding box annotations. In
this paper we take this one step further and propose zero-
and few-label learning for semantic segmentation as a new
task and propose a benchmark on the challenging COCO-
Stuff and PASCAL VOC12 datasets.
In the task of zero-
label semantic image segmentation no labeled sample of
that class was present during training whereas in few-label
semantic segmentation only a few labeled samples were
present. Solving this task requires transferring the knowl-
edge from previously seen classes to novel classes. Our
proposed semantic projection network (SPNet) achieves this
by incorporating class-level semantic information into any
network designed for semantic segmentation, and is trained
in an end-to-end manner. Our model is effective in segment-
ing novel classes, i.e. alleviating expensive dense annota-
tions, but also in adapting to novel classes without forget-
ting its prior knowledge, i.e. generalized zero- and few-
label semantic segmentation.

1. Introduction

In semantic image segmentation the aim is assign a label
to every pixel in an image by partitioning it into several se-
mantic regions and then learning the appearance of various
classes as well as the background. Although deep CNN-
based approaches have achieved good performance for this
task, they require costly dense annotations to learn their nu-
merous parameters. Hence, leveraging weak annotations
via image-level labels [34, 32, 31] or point [5], bounding
box [20], scribble-level annotations [25] recently gained in-
terest. On the other hand, as humans, we easily learn to
recognize a previously unseen, i.e. novel, class by associ-
ating it with classes that we know. However, segmenting

∗Equal contributions

Figure 1: We propose (generalized) zero- and few-label se-
mantic segmentation tasks, i.e. segmenting classes whose
labels are not seen by the model during training or the model
has a few labeled samples of those classes. To tackle these
tasks, we propose a model that transfers knowledge from
seen classes to unseen classes using side information, e.g.
semantic word embedding trained on free text corpus.

such novel classes via modern machine learning techniques
is still an open problem as this process requires knowledge
transfer from known classes to previously unseen ones.

Knowledge transfer to novel classes is not a new task.
Learning to predict novel classes has been studied exten-
sively in the context of image classiﬁcation, i.e. zero-shot
learning [23, 57, 7, 2].
In zero-label semantic segmenta-
tion (ZLSS), our aim is to segment previously unseen, i.e.
novel, classes, in few-label semantic segmentation (FLSS)
these novel classes have a small number of labeled train-
ing examples (see Figure 1).
In this work, we also aim
for learning without forgetting the previously seen classes,
i.e. generalized ZLSS and FLSS. To achieve these aims, we
propose Semantic Projection Network (SPNet) that incor-
porates semantic word embeddings to an arbitrary semantic
segmentation network inspired by the success of zero-shot
learning. Prior models that tackle few-shot semantic seg-
mentation [42, 11] operate in the foreground-background
segmentation setting. However, in our deﬁnition of FLSS
the model has to predict all the classes in an image sepa-
rately, which is more challenging and realistic. Our frame-
work utilizes the similarity between different categories in
a semantic segmentation network, enabling it to transfer

18256

classes  with many samplesclasses with few samplesZero-label  semantic segmentationFew-label  semantic segmentationTraining setTest setSemantic knowledgeOur predictionOur predictionlearned representations to other classes. Consequently, our
model is able to segment scenes containing novel classes.

Our main contributions are as follows. (1) We introduce
the (generalized) zero-label and few-label semantic image
segmentation task in a realistic settings inspired by zero-
shot learning for image classiﬁcation. (2) We propose se-
mantic projection network (SPNet), an end-to-end semantic
segmentation model which maps each image pixel to a se-
mantic word embedding space where it is projected with a
ﬁxed word embedding to class probabilities optimizing the
cross-entropy loss. (3) We create a benchmark for (general-
ized) zero- and few-label semantic image segmentation with
two challenging datasets, i.e. COCO-Stuff and PASCAL-
VOC. Our analysis shows that the SPNet model achieves
impressive results both quantitatively and qualitatively in
(generalized) zero-label and few-label tasks. Furthermore,
as a side-product, our model improves the state of the art
in zero-shot image classiﬁcation demonstrating that it suc-
cessfully generalizes to other tasks.

2. Related work

In this section, we review prior work on zero-shot learn-

ing, semantic segmentation and their combination.

Zero- and few-shot image classiﬁcation. Most advances
in zero-shot image classiﬁcation were achieved by visual-
semantic embedding models [13, 1, 57, 50, 41, 14, 55] that
learn a compatibility function between the image embed-
ding space i.e. CNN image feature [16], and the class em-
bedding space, i.e. class-level attributes [23]. As comple-
mentary tasks [24, 40] focuses on assigning multiple pre-
viously unseen labels to a single image, [22, 52, 53] on
predicting novel actions in a video, and recently, [4] on
unseen object detection. For a comprehensive overview
of zero-shot learning models, we refer the reader to [51].
As for few-shot learning, [48, 15] stand out as generating
data of weakly represented classes and meta learning ap-
proaches [46, 38, 44] regularize the model by sharing pa-
rameters and applying episode training strategy. In contrast
to image classiﬁcation where an image has only one class
label, in semantic segmentation, each image has a dense la-
bel map that assigns a label for each pixel from a set of
possible object classes. Given the large amount of class co-
occurrence in images, it is unrealistic to build a training set
that contains no pixels from target classes for semantic seg-
mentation. Therefore, we allow models to see pixels from
target classes without accessing their labels, which explains
our terminology “zero-label”.

Semantic segmentation with weak supervision. Modern
semantic segmentation systems [27, 9, 3] are built on the
encoder-decoder networks and trained with densely labeled
annotations. Much efforts focus on improving semantic
segmentation under fully supervised settings, e.g. adding

global context information [59, 54, 26], applying graphical
models as a post-processing step to reﬁne the output [60, 9],
etc. On the other hand, weakly supervised semantic seg-
mentation, i.e. reducing the annotation effort, has recently
gained momentum. As weak supervision, prior works use
image-level annotation [34, 32, 31], point [5], scribble [25]
and bounding box [20] annotations. Those methods propa-
gate the supervision to larger regions by measuring object-
ness [5] and saliency [31], or applying graphical models
[25]. Other methods reﬁne the coarse annotated regions to
more accurate ones [20, 32]. However, those models still
require all the classes to be seen during training, thus can-
not easily be adapted to new classes. In contrast, we focus
on segmenting completely novel classes.

Semantic segmentation of novel classes. The term zero-
shot semantic segmentation appears in prior works [18, 58].
The aim of [18] is to segment novel actor-action patterns
during test time. While [58] proposes open-vocabulary
scene parsing task that segments novel objects by perform-
ing hierarchical parsing, we leverage word embeddings to
predict the exact unseen classes and address the few-label
problem in a uniﬁed framework. For few-shot semantic seg-
mentation, previous approaches [42, 37, 11, 56] follow the
meta-learning setup [46, 44], which uses a support set to
predict an query image. However, those approaches are re-
stricted to output a binary mask and fail to segment an im-
age with multiple classes. In contrast, our approach is oper-
ating in the more realistic (generalized) few-label semantic
segmentation setting, i.e. pixel-level labeling of an image
where labels come from both base and novel classes.

Semantic embeddings.
In learning with limited labels,
some form of side information is required to transfer the
knowledge learned from seen classes to unseen classes.
One popular form of side information is attributes [23] that,
however, require costly expert annotation. Thus, there has
been a large group of studies [2, 39, 36, 10] utilizing other
sources such as Word2vec [29], fastText [19], or hierar-
chies [30] for building semantic embeddings. In this work,
we utilize Word2Vec and fastText as they do not require
dataset speciﬁc human annotation.

3. SPNet Model for Segmenting Novel Classes

Modern semantic segmentation models are built on fully
convolutional encoder-decoder architectures [9, 27] that
output intermediate feature maps and posteriors for individ-
ual classes. However, to segment novel classes these models
need to be adapted to transfer knowledge from one class to
the other. Such knowledge can be obtained from class-level
semantic embeddings associating different classes. Hence,
the main insight of our approach is to leverage semantic
word embeddings, i.e. word2vec [29] or fast-text [19],
to transfer knowledge learned from base classes to novel

8257

Train 

Test

Segmentation

Networks

FCN

DeepLab

...

GT

CNN Feature Maps

loss
={horse, bush, ...}

={cow, grass, ...}

prediction

Word Embedding Matrices

Visual-semantic 

Embedding

Semantic 
Projection

Figure 2: Our zero-label and few-label semantic segmentation model, i.e. SPNet, consists of two steps: visual semantic
embedding and semantic projection. Zero-label semantic segmentation is drawn as an instance of our model. Replacing
different components of SPNet, four tasks are addressed (Solid/dashed lines show the training/test procedures respectively).

classes in a two-step process. First, we propose to learn
a visual-semantic embedding module that produces inter-
mediate feature maps in the word embedding space. Sec-
ond, we project those feature maps into class probabili-
ties via a ﬁxed word embedding projection matrix. At test
time, by replacing the projection matrix with word embed-
dings of novel classes, our model is able to segment un-
seen categories. Our model is trained end-to-end and can
be incorporated into any semantic segmentation network,
i.e. FCN [27] and deeplab [9]. We illustrate our overall
pipeline in Figure 2.

Task formulation. We denote the set of seen classes as
S and a disjoint set of unseen classes as U . Let Ds =
{(x, y)|x ∈ X , y ∈ Y s} be our labeled training data of
seen classes where x is an image in the image space X , y is
its corresponding label mask in the dense label mask space
Y s ⊂ S a∗b of seen classes with a and b being the height
and the width of the image respectively. Similarly, we de-
ﬁne the label mask space of unseen classes as Y u ⊂ U a∗b.
In addition, W s ∈ Rdw×|S| and W u ∈ Rdw×|U | denote the
word embedding matrices of seen and unseen classes where
dw is the word embedding dimension. Given Ds, W s, and
W u, the task of zero-label semantic segmentation (ZLSS)
is to learn a model that takes an image as an input and pre-
dicts the label of each pixel among unseen classes. A more
realistic setting is generalized zero-label semantic segmen-
tation (GZLSS) where the learned model predicts both seen
and unseen classes. As for the (generalized) few-label se-
mantic segmentation task, a few labeled samples from un-
seen classes Du = {(x, y)|x ∈ X , y ∈ Y u} are provided
to the model during training. The test time target classes in-
clude only seen classes in few-label semantic segmentation
(FLSS) whereas they include both seen and unseen classes
in generalized few-label semantic segmentation (GFLSS).
Here, we refer to the classes with a few labeled samples

as unseen or novel, interchangeably. We summarize train
class, test class and word embeddings used in different set-
tings in Figure 2.

3.1. Semantic Projection Network (SPNet)

We address all four tasks with an uniﬁed model SPNet,
which consists of two parts: visual-semantic embedding
module and semantic projection layer.

i. Visual-semantic embedding module. This module is
parameterized by a CNN and maps an input image x ∈ X
into dw feature maps via φ : X → Ra×b×dw of size a × b.
This is equivalent to embedding each pixel at (i, j) into a dw
dimensional class embedding vector φ(x)ij that lies in the
semantic embedding space shared by all the classes. The se-
mantic embedding space constrains the output of the visual-
semantic embedding extractor φ and transfers knowledge
from seen to unseen classes. Note that this is different from
a standard CNN where pixels are mapped into an uncon-
strained feature space.

ii. Semantic projection layer. The semantic projection
layer maps the feature embedding φ(x)ij into unnormalized
logit scores followed by a softmax activation that outputs
the probability distribution over each training category,

p(ˆyij = s|x; W s) =

exp (w⊤
P

exp (w⊤

c∈S

s φ(x)ij)

c φ(x)ij)

(1)

where ˆyij represents the prediction for pixel (i, j), wc is the
c-th column of W s normalized to have unit length.

In contrast to standard CNNs that predict the class pos-
terior by adding 1 × 1 convolution layer or fully con-
nected layer with learnable weights, our classiﬁer weights
W s are predeﬁned by a word embedding model, e.g.
word2vec [29], and then ﬁxed during training. The W s and

8258

the semantic projection layer estimate the compatibility be-
tween class prototypes and a feature embedding in terms of
inner product similarity. Our proposed semantic projection
layer is easy to implement by computing the tensor prod-
uct between feature maps φ(x) and word embedding matrix
W s followed by the softmax activation function. After this
layer, we directly optimize the standard cross-entropy loss
over the spatial dimensions (i, j) ∈ I,

− log p(ˆyij = yij|x)

(2)

X

(i,j)∈I

which can be viewed as maximizing the negative log like-
lihood of predicting each pixel as its true label yij . Since
there are no learnable parameters at the semantic projec-
tion layer, the optimization is over parameters of the visual-
semantic embedding extractor φ. Compared to the standard
semantic segmentation network, we have made subtle yet
critical changes, i.e. mapping pixels to the semantic word
embedding space followed by stacking a projection layer.

Inference. At the test time, in ZLSS and FLSS, we predict
unseen classes by replacing the word embedding matrix in
Eq. (1) with W u. Each pixel label is predicted by:

argmax

p(ˆyij = u|x; W u).

(3)

u∈U

On the other hand, for GZLSS and GFLSS, we predict both
seen and unseen class labels via their word embedding:

argmax
u∈S∪U

p(ˆyij = u|x; [W s; W u]).

(4)

The extreme case of the imbalanced data problem occurs
when there is no labeled training images of unseen classes,
and this results in predictions being biased to seen classes.
To ﬁx this issue, we follow [8] and calibrate the prediction
by reducing the scores of seen classes, which leads to:

argmax
u∈S∪U

p(ˆyij = u|x; [W s; W u]) − γI[u ∈ S]

(5)

where I = 1 if u is a seen class and 0 otherwise, γ ∈ [0, 1]
is the calibration factor tuned on a held-out validation set.

Theoretically, the semantic projection layer allows our
model to predict any class by simply copying its word em-
bedding to the classiﬁer weights. However, intuitively, the
model can only perform well on the classes that share visual
similarities with training classes. Hence, the word embed-
ding ought to capture the similarity between classes.

Two-stage training in few-label setting. In our FLSS and
GFLSS, we train a model with both Ds that includes a large
number of samples per seen class and Du that has only a
few samples per unseen, i.e. novel, class. This is a typical
imbalanced learning problem. The naive idea is to learn us-
ing both seen and unseen class samples within a mini-batch

sampled uniformly from the whole training data. As ex-
pected, this leads to good performance on seen classes but
inferior performance on unseen classes. Another strategy is
to oversample unseen classes by ﬁrst uniformly sampling a
mini-batch of classes and selecting one sample from each
of those classes. We found that this strategy remedies the
imbalance issues to some extent but the results still remain
unsatisfactory. On the other hand, ﬁne-tuning the learned
classiﬁer on unseen class samples, i.e. after the initial opti-
mization with only seen class samples, yields better results
on unseen classes in FLSS as well as better overall results
in GFLSS. Hence, we report our results in this setting.

3.2. Baseline: Hinge Visual Semantic Loss (HVSL)

The choice of the loss function turns out to be important
in zero-label semantic segmentation. Hence, in this section,
we develop a baseline that shares the same embedding ex-
tractor φ as our SPNet but adopts the hinge visual-semantic
loss instead of cross-entropy loss.
Indeed hinge visual-
semantic loss constitutes the most widely used loss function
for zero-shot image classiﬁcation [1, 4, 13, 57, 50]. In the
context of semantic segmentation, we deﬁne the following
hinge ranking loss for a single training example (x, y) as,

X

X

[∆(s, yij) + w⊤

s φ(x)ij − w⊤
yij

φ(x)ij]+ (6)

(i,j)∈I

s∈S

where ∆(s, yij) = 1 if s 6= yij otherwise 0, φ(x)ij is
the visual-semantic embedding for pixel (i, j) in image x,
yij is its corresponding ground-truth label. In practice, we
follow [13] to truncate the sum by randomly sampling one
class that is not ground-truth.

4. Experiments

In this section, we present both quantitative and quali-
tative results of zero-label semantic segmentation and few-
label semantic segmentation.

Datasets. We evaluate our model on the challenging
COCO-stuff [6] and PASCAL-VOC 2012 [12] datasets.
COCO-stuff has 164K images with dense pixel-level anno-
tations from 172 classes including 80 thing classes, 91 stuff
classes. PASCAL-VOC is a smaller dataset which contains
13K images from 20 classes.

Word embeddings.
Encoding the semantic similarity
between labels plays an important role in bridging the
gap between seen and unseen class predictions.
In this
work, we study two different word embedding models,
i.e. word2vec [29] trained on Google News [47] and fast-
Text [19] trained on Common Crawl [28]. The word embed-
dings of classes that contain multiple words are obtained by
averaging the embeddings of each individual word.

8259

# classes

# images

fastText (ft) word2vec (w2v)

ft + w2v

train+val

test

train+val

test

COCO-Stuff

155+12

PASCAL-VOC

12+3

15

5

116287+2000 5000

11185 + 500

1449

Table 1: Statistics of data splits for COCO-Stuff and
PASCAL-VOC datasets in terms of the number of classes
and the number of images in the training and test splits.

HVSL

SPNet

25.8
33.1

25.3
32.1

31.8
35.2

Table 2: Effect of word embeddings: Mean IoU of unseen
classes in ZLSS with different word2vec, fastText and their
combination on COCO-Stuff. Both HVSL and SPNet are
based on ResNet101.

Implementation details. We implement our SPNet model
with PyTorch [33]. We apply ImageNet pretrained VGG-
16 [43] and ResNet-101 [17] as our backbone to extract
features, and our model is built on the DeepLab-v2 [9]
that ﬁrst extract features and apply atrous spatial pyramid
pooling layer to produce the visual features, whose dimen-
sion is the same as the dimension of the semantic em-
bedding space (i.e., 300 for fast-text and word2vec; 600
for their concatenation).
In this work, for VGG back-
bone we apply Adam solver [21] with initial learning rate
1.0 × 10−4, and for ResNet we use SGD with initial learn-
ing rate 2.5 × 10−4. Following [9], we use the “poly” learn-
ing rate policy where current learning rate is the initial one
multiplied by (1 − iter
max iter )power, and we set power to 0.9.
Momentum and weight decay are set to 0.9 and .0005.

4.1. Zero Label Semantic Segmentation Task

One of the contributions of our work is to propose a new
task of zero-label semantic segmentation (ZLSS). In this
section, we propose two benchmarks with zero-label data
splits and detail the zero-label evaluation protocol.

Proposed zero-label dataset splits. The zero-label as-
similar to the zero-shot assumption [51],
sumption, i.e.
states that none of the pixel values of the query images are
allowed to belong to the classes that were used in any part of
the training procedure, i.e. be it the model training or CNN
training. This means that as CNNs are commonly trained on
ImageNet 1K, none of the test classes should overlap with
it. Following this rule, in COCO-Stuff dataset, we create a
new zero-label class split by selecting 15 classes as unseen
and the rest of the 167 classes as seen classes as they appear
in ImageNet 1K which was used to pretrain ResNet.

In contrast to zero-shot image classiﬁcation, we do not
remove images that contain unseen classes from the train-
ing set, otherwise most of training images will be elimi-
nated because seen and unseen classes co-occur frequently.
Instead, we utilize the whole training set but ignore the la-
bels of pixels belonging to unseen classes during training,
i.e.
these pixels do not effect the loss we optimize in any
stage of the training. For PASCAL-VOC, since (a) only 4
classes are unseen in ImageNet 1K, (b) one of the candidate
class ‘person’ has no semantically similar class present in

the dataset, (c) all vehicles appear in ImageNet thus reduc-
ing candidate diversity - we simply take the ﬁrst 15 classes
as seen classes and the last 5 classes as unseen classes. We
use the train/val split provided by the COCO-Stuff dataset:
118K training images as our training set and 5K validation
images as our test set, and PASCAL-VOC: 11K training im-
ages and 1.4K test images. Following the cross-validation
procedure of [51], we further hold out a subset of train-
ing classes as our validation set for tuning hyperparameters.
More details about our data splits are shown in Table 1.

Evaluation protocol. The intersection-over-union (IoU),
i.e.
the standard evaluation criteria commonly used in se-
mantic segmentation, quantizes the overlap between the
predicted mask and the target mask. It is deﬁned to be the
size of the intersection between predicted and target regions
divided by the union of them. For each class, its mean IoU
is computed by averaging the IoU over all the query images.
In ZLSS, as the test-time search space is restricted to
be unseen classes we report the mean IoU averaged over
unseen classes. In GZLSS, the search space becomes the
union of seen and unseen classes. In analogy to generalized
zero-shot image classiﬁcation [51], we report the mean IoU
on seen classes, the mean IoU on unseen classes and the
harmonic mean (H) of them, which is deﬁned as,

H =

2 ∗ mIoUseen ∗ mIoUunseen

mIoUseen + mIoUunseen

(7)

where mIoUseen and mIoUunseen represents the mean IoU
of seen classes and unseen classes respectively. Similarly,
in few-label semantic segmentation, we report mean IoU on
unseen classes, but in generalized few-label semantic seg-
mentation, the mean IoU over all classes is reported.

4.1.1 SPNet Model Analysis for ZLSS

In this section, we provide an extensive evaluation for dif-
ferent design choices of our model.

Effect of word embeddings. We compare our SPNet
model with HVSL and study the effect of different word
embeddings in Table 2. We investigate three types of word
embeddings, i.e. fastText, word2vec and their concatena-
tion. Our ﬁrst observation is that SPNet performs sig-
niﬁcantly better than HVSL wrt. all the word embedding

8260

COCO-Stuff PASCAL VOC

SPNet-VGG

SPNet-ResNet101

26.3
35.2

47.4
49.5

Table 3: Effect of CNN architectures: ZLSS with different
CNN architectures, i.e. VGG and ResNet101 on COCO-
Stuff and PASCAL-VOC. Word embedding is the ft + w2v.

Figure 3: mIoU of unseen classes on COCO-Stuff ordered
wrt average object size (left to right).

types, e.g. SPNet achieves 33.1 vs 25.8 with fastText, and
32.1 vs 25.3 with word2vec compared to HVSL. This im-
plies that the cross-entropy loss is more suitable to the ZLSS
task than hinge loss. Furthermore, we observe that fastText
and word2vec achieve comparable results, and combining
them signiﬁcantly boosts the performance, e.g. mean IoU
of SPNet are improved from 33.1 and 32.1 to 35.2. This
indicates that fastText and word2vec contain complemen-
tary information. Hence, for the rest experiments, we use
SPNet with fastText and word2vec combined.

Effect of CNN architectures. Our aim here is to com-
pare different CNN architectures that are used as the back-
bone network to encode images in DeepLab-v2 [9]. Ta-
ble 3 shows the ZLSS results with VGG16 [43] and
ResNet101 [17]. We ﬁrst observe that with VGG16, the
results are lower than with ResNet101 on both COCO-Stuff
and PASCAL-VOC which implies that ResNet101 generate
stronger features than VGG16 for this task. Besides, these
results show that our SPNet achieves reasonably good re-
sults in ZLSS with both CNN architectures. Speciﬁcally,
on COCO-stuff, SPNet obtains 26.3% mIoU with VGG16
and 35.2% mIoU with ResNet101. This is promising be-
cause our model does not require expensive dense pixel-
level annotations for each class, e.g. it is not trained with
any of the 15 unseen class labels of COCO-Stuff. This also
indicates that our model is easily adapted to various seman-
tic segmentation architectures.

Effect of the object size. We study the difﬁculty of zero-
label semantic segmentation as a function of object sizes.

Figure 4: GZLSS results on COCO-Stuff and PASCAL-
VOC. We report mean IoU of unseen classes, seen classes
and their harmonic mean (perception model is based on
ResNet101 and the semantic embedding is ft + w2v).
SPNet-C represents SPNet with calibration.

Figure 3 presents a plot of per class mIoU score for the un-
seen classes in COCO-Stuff. The classes are ordered ac-
cording to their average object sizes – with the largest on
the right. It shows that there is a tendency that the perfor-
mance is better for classes with larger objects. The plot
also indicates that the knowledge transfer from seen to un-
seen classes is in general successful for the challenging
stuff classes, such as, tree (59.3%), grass (59.7%, clouds
(62.2%), considering the fact that they do not have seman-
tically similar classes present in ImageNet 1K. We also ob-
serve that our model performs well for cow (61.3%) how-
ever the result is quite poor the other unseen animal class
giraffe (0.2%).

4.1.2 Generalized Zero-Label Semantic Segmentation

GZLSS is a practical segmentation setting as the test time
search space contains both seen and unseen classes, i.e. the
pixel can be assigned to one of the seen or one of the unseen
classes. Since the training images contain only labeled pix-
els of seen classes, at the test time, prediction will be biased
to seen classes. Hence, this is a particularly challenging
task. We alleviate this issue by using the calibrated clas-
siﬁer formulated in Eq. (5), which reduces the prediction
scores of seen classes by a calibration factor γ. We select
the optimal γ value based on the best harmonic mean IoU
on a held-out validation set. Figure 4 shows the mean IoU
on unseen classes, seen classes and their harmonic mean on
COCO-Stuff and PASCAL VOC datasets.

On COCO-Stuff SPNet obtains 0.2% mean IoU on un-
seen classes while IoU on seen classes is high, i.e. 34.05%.
This is expected, in fact the same trend is observed in gen-
eralized zero-shot image classiﬁcation task [51, 8]. On the
other hand, after calibration i.e. SPNet-C, on COCO-
Stuff, mean IoU of unseen classes jumps to 8.33% while
maintaining high mIoU on seen classes, i.e. 34.52% and
overall SPNet-C achieves a harmonic mean of 13.42%.
This is due to the fact that after calibration, i.e. reducing

8261

ZSL

GZSL

)

CUB SUN AWA CUB SUN AWA
26.3 27.5
54.9
ALE [1]
19.6
19.8
53.9
SJE [2]
56.3
13.4
16.2
SYNC [7]
0.0
GFZSL [45] 49.3
3.5
56.5 60.7 66.2 36.6 39.6 24.7
SPNet

59.9
58.1
65.6
53.7
54.0
55.6
60.6 68.3

34.4
33.6
19.8
0.0

Table 4: SPNet loss on (generalized) zero-shot learning
tasks. Top-1 accuracy on unseen classes is reported for ZSL
and harmonic mean of seen and unseen classes is for GZSL.

prediction scores of seen classes, pixels get predicted as
seen classes less frequently.

On PASCAL-VOC we observe a similar trend. While
SPNet performs poorly on unseen classes, i.e. 0.01%
mIoU, with calibration this increases to 29.33% mIoU. Ac-
cordingly, SPNet-C achieves an impressive 42.45% har-
monic mIoU. These results demonstrate that our SPNet
does not only tackle ZLSS but also can handle the more
practical GZLSS via predictor calibration.

4.1.3

(Generalized) Zero-Shot Image Classiﬁcation

We evaluate our SPNet on the zero-shot image classiﬁca-
tion task on three benchmark datasets, i.e. CUB [49] (200
types of birds with 312 attributes), SUN [35] (717 scenes
with 102 attributes) and AWA [23] (50 classes of animals
with 85 attributes) with various sizes and complexities, fol-
lowing the data splits and evaluation protocol of [51]. We
train SPNet with cross-entropy loss:

L(x, y) = − log

exp (φ(x)⊤V wy)

Pc∈S exp (φ(x)⊤V wc)

(8)

where φ(x) is 2048-dim image feature extracted from a pre-
trained ResNet101 (no ﬁne-tuning on the task), wc ∈ Rdw
is the class attribute of class c, V ∈ R2048×dw is the linear
embedding we aim to learn. Table 4 shows that both in ZSL
and GZSL settings, our SPNet improves over the state of
the art on both CUB and SUN while it obtains the second
best results on AWA despite the simplicity of our model.
Both ALE [1] and SJE [2] utilize the visual-semantic hinge
loss, SYNC [7] align visual and semantic embedding space
using manifold learning, and GFZSL [45] learns a gener-
ative model to capture the class conditional distribution.
However, our SPNet simply projects image feature into the
class embedding space and apply the standard softmax clas-
siﬁer with the class embedding being the weights.

4.2. Few Label Semantic Segmentation Task

 

%
n
i
(
 
s
e
s
s
a
C
n
e
e
s
n
U

 

l

 

 
r
e
v
o
U
o
m

I

)

 

%
n
i
(
 

U
o
m

I

 
l
l

a
r
e
v
O

70

60

50

40

30

20

34

32

30

28

26

24

22

20

1

1

COCO-Stuff

PASCAL-VOC

100

)

%
 
n
i
(
 
s
e
s
s
a
C
 
n
e
e
s
n
U

l

 
r
e
v
o
 
U
o
I
m

80

60

40

20

1

SPNet

Baseline

2

10
# training samples per class

5

20

SPNet

Baseline

2

10
# training samples per class

5

20

(a) Few-Label Semantic Segmentation

COCO-Stuff

PASCAL-VOC

90

80

70

60

50

40

30

20

10

)

 

%
n
i
(
 

U
o
m

I

 
l
l

a
r
e
v
O

1

SPNet

Baseline

2

10
# training samples per class

5

20

SPNet

Baseline

2

10
# training samples per class

5

20

(b) Generalized Few-Label Semantic Segmentation

Figure 5: (Generalized) few-label semantic segmentation
on COCO-Stuff and PASCAL VOC with increasing number
of training samples per class, i.e. n ∈ {1, 2, 5, 10, 20}.

since class distribution in semantic segmentation is usually
skewed, e.g.
there are far more road pixels than bicycles.
In contrast to ZLSS where the training set has no labeled
example from unseen (novel) classes, in FLSS and GFLSS,
the model is trained with all classes. At the evaluation time,
the goal of FLSS is to segment only the novel classes, while
GFLSS aims to segment both base and novel classes. For
each novel class, we randomly draw n ∈ {1, 2, 5, 10, 20}
images that contain this class from the training set and dis-
able ignore-label condition for those novel pixels. In ad-
dition, we develop a simple baseline based on the original
DeepLab-v2 [9], which is ﬁnetuned on novel classes after
an initial optimization on base classes. We carry out ex-
periments in FLSS and GFLSS with the baseline and our
SPNet on COCO-Stuff and PASCAL-VOC.

In FLSS task, Figure 5 (a) shows the comparison results
with the baseline model [9]. Our SPNet yields signiﬁcantly
better results than the baseline in all cases on both COCO-
Stuff and PASCAL VOC. In particular, when there is only 1
labeled example, our SPNet signiﬁcantly outperforms the
baseline, achieving a mean IoU of 47.90% over 27.69% in
COCO-Stuff and 71.52% over 29.17% in PASCAL VOC
on FZLSS. The accuracy improvement from 1 labeled sam-
ple to 5 labeled samples is signiﬁcant, i.e. ≈ 20% mIoU
for both COCO-Stuff and PASCAL VOC. These results
demonstrate the effectiveness of our SPNet when the train-
ing samples are scarce.

The (Generalized) few-label semantic segmentation (FLSS
and GFLSS) tasks arise in many real-world applications

As for GFLSS in Figure 5 (b), a similar trend is observed.
Our SPNet improves over DeepLab in all cases. The accu-

8262

Figure 6: Qualitative results of our SPNet in 0-, 1- and 5-label semantic segmentation settings on COCO-Stuff on 15 novel
classes (color coded at the top). Base classes are masked out with black color. (a) promising results (b) failure cases.

racy improvement is steady from 1 to 2, 5, 10, 20 especially
on COCO-Stuff. The difference between DeepLab and ours
is 21.24% mIoU over both seen and unseen classes on PAS-
CAL VOC when our model has access to only one labeled
sample from novel classes.

4.3. Qualitative Results

Figure 6 shows the qualitative results obtained by our
SPNet in ZLSS and FLSS on COCO-Stuff. Our target 15
novel classes are encoded with the colors shown at the top.
Base classes are masked out with black color. Some inter-
esting results are as follows. In the ﬁrst row and left col-
umn, our SPNet is already able to segment two previously
unseen classes cows and grass at ZLSS, i.e. 0-label, and
results get reﬁned after the model sees more examples. It
is also worth noting that our SPNet is able to predict stuff
classes, such as road, river, clouds etc., in ZLSS setting. For
instance, SPNet successfully segments clouds and roads in
the image at the second row and right column, and perfectly
segments the river in the image at the third row and left
column. Another interesting result is in the left column of
4th row where the model correctly segments the frisbee in
0-label setting but incorrectly labels most pixels as ‘skate-
board’ which in fact is another sports category object. On
the other hand, some failure cases are shown in the bottom

row. Our SPNet fails to predict giraffe at 0-label because
shape and appearance of a giraffe vary signiﬁcantly from
seen classes. However, seeing only 1 example is enough
to recognize and segment it, which demonstrates the ability
of our SPNet in learning from few examples. Again, the
result gets reﬁned with 5 labeled examples.

These results support our observations in the previous
sections and indicate that our SPNet, although simple,
adapts its knowledge attained in previously seen examples
to unseen ones.

5. Conclusions

In this work, we propose SPNet to semantically seg-
ment novel classes with no labeled examples or with only
a few samples, within the new tasks of zero-label semantic
segmentation and few-label semantic segmentation respec-
tively. This model consists of a visual-semantic embedding
module that encodes images in the word embedding space
and a semantic projection layer that produces class proba-
bilities. Our SPNet is both conceptually and computation-
ally simple but surprisingly effective and end-to-end train-
able. We have shown its applicability across zero-shot im-
age classiﬁcation to zero-label and few-label semantic seg-
mentation tasks on various benchmark datasets.

8263

(b)(a)References

[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-

embedding for image classiﬁcation. TPAMI, 2016. 2, 4, 7

[2] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-
uation of output embeddings for ﬁne-grained image classiﬁ-
cation. In CVPR, 2015. 1, 2, 7

[3] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. TPAMI, 2017. 2

[4] A. Bansal, K. Sikka, G. Sharma, R. Chellappa, and A. Di-

vakaran. Zero-shot object detection. In ECCV, 2018. 2, 4

[5] A. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei.
Whats the point: Semantic segmentation with point super-
vision. In ECCV, 2016. 1, 2

[6] H. Caesar, J. Uijlings, and V. Ferrari. Coco-stuff: Thing and

stuff classes in context. In CVPR, 2018. 4

[7] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthe-
sized classiﬁers for zero-shot learning. In CVPR, 2016. 1,
7

[8] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empir-
ical study and analysis of generalized zero-shot learning for
object recognition in the wild. In ECCV, 2016. 4, 6

[9] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. TPAMI, 2018. 2, 3, 5, 6, 7

[10] Z. Ding, M. Shao, and Y. Fu. Low-rank embedded ensemble
semantic dictionary for zero-shot learning. In CVPR, 2017.
2

[11] N. Dong and E. P. Xing. Few-shot semantic segmentation

with prototype learning. BMVC, 2018. 1, 2

[12] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object Classes
Challenge 2012 (VOC2012) Results.
http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html.
4

[13] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. A.
Ranzato, and T. Mikolov. Devise: A deep visual-semantic
embedding model. In NIPS, 2013. 2, 4

[14] Y. Fu, T. M. Hospedales, T. Xiang, and S. Gong. Transduc-

tive multi-view zero-shot learning. TPAMI, 2015. 2

[15] B. Hariharan and R. Girshick. Low-shot visual recognition

by shrinking and hallucinating features. In ICCV, 2017. 2

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 2

[21] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 5

[22] E. Kodirov, T. Xiang, Z. Fu, and S. Gong. Unsupervised
domain adaptation for zero-shot learning. In ICCV, 2015. 2
[23] C. Lampert, H. Nickisch, and S. Harmeling. Attribute-
based classiﬁcation for zero-shot visual object categoriza-
tion. TPAMI, 2013. 1, 2, 7

[24] C.-W. Lee, W. Fang, C.-K. Yeh, and Y.-C. F. Wang. Multi-
label zero-shot learning with structured knowledge graphs.
In CVPR, 2018. 2

[25] D. Lin, J. Dai, J. Jia, K. He, and J. Sun.

Scribble-
sup: Scribble-supervised convolutional networks for seman-
tic segmentation. In CVPR, 2016. 1, 2

[26] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking

wider to see better. ICLR workshop, 2016. 2

[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. In CVPR, 2015. 2, 3

[28] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and
A. Joulin. Advances in pre-training distributed word rep-
resentations. In LREC, 2018. 4

[29] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS, 2013. 2, 3, 4

[30] G. A. Miller. Wordnet: a lexical database for english. CACM,

1995. 2

[31] S. J. Oh, R. Benenson, A. Khoreva, Z. Akata, M. Fritz,
B. Schiele, et al. Exploiting saliency for object segmenta-
tion from image level labels. In CVPR, 2017. 1, 2

[32] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille.
Weakly-and semi-supervised learning of a dcnn for semantic
image segmentation. In ICCV, 2015. 1, 2

[33] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. In NIPS-W, 2017. 5

[34] D. Pathak, E. Shelhamer, J. Long, and T. Darrell. Fully con-
In ICLR

volutional multi-class multiple instance learning.
workshop, 2015. 1, 2

[35] G. Patterson and J. Hays. Sun attribute database: Discover-
ing, annotating, and recognizing scene attributes. In CVPR,
2012. 7

[36] R. Qiao, L. Liu, C. Shen, and A. v. d. Hengel. Less is more:
Zero-shot learning from online textual documents with noise
suppression. In CVPR, 2016. 2

[37] K. Rakelly, E. Shelhamer, T. Darrell, A. Efros, and S. Levine.
Conditional networks for few-shot semantic segmentation.
ICLR workshop, 2018. 2

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

[38] S. Ravi and H. Larochelle. Optimization as a model for few-

for image recognition. In CVPR, 2016. 5, 6

shot learning. In ICLR, 2016. 2

[18] J. Ji, S. Buch, A. Soto, and J. C. Niebles. End-to-end joint
In

semantic segmentation of actors and actions in video.
ECCV, 2018. 2

[39] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning deep
representations of ﬁne-grained visual descriptions. In CVPR,
2016. 2

[19] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J´egou,
and T. Mikolov. Fasttext.zip: Compressing text classiﬁcation
models. arXiv preprint arXiv:1612.03651, 2016. 2, 4

[20] A. Khoreva, R. Benenson, J. H. Hosang, M. Hein, and
B. Schiele. Simple does it: Weakly supervised instance and
semantic segmentation. In CVPR, 2017. 1, 2

[40] Z. Ren, H. Jin, Z. Lin, C. Fang, and A. Yuille. Multiple

instance visual-semantic embedding. In BMVC, 2017. 2

[41] B. Romera-Paredes and P. H. Torr. An embarrassingly simple

approach to zero-shot learning. In ICML, 2015. 2

[42] A. Shaban, S. Bansal, Z. Liu, I. Essa, and B. Boots. One-shot

learning for semantic segmentation. In BMVC, 2017. 1, 2

8264

[43] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 5, 6

[44] J. Snell, K. Swersky, and R. Zemel. Prototypical networks

for few-shot learning. In NIPS, 2017. 2

[45] V. K. Verma and P. Rai. A simple exponential family frame-

work for zero-shot learning. In ECML, 2017. 7

[46] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al.
In NIPS, 2016.

Matching networks for one shot learning.
2

[47] X. Wang, Y. Ye, and A. Gupta. Zero-shot recognition via se-
mantic embeddings and knowledge graphs. In CVPR, 2018.
4

[48] Y. Wang, R. Girshick, M. Hebert, and B. Hariharan. Low-

shot learning from imaginary data. In CVPR, 2018. 2

[49] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-
longie, and P. Perona. Caltech-UCSD Birds 200. Technical
Report CNS-TR-2010-001, Caltech, 2010. 7

[50] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and
B. Schiele. Latent embeddings for zero-shot classiﬁcation.
In CVPR, 2016. 2, 4

[51] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-
shot learning-a comprehensive evaluation of the good, the
bad and the ugly. TPAMI, 2018. 2, 5, 6, 7

[52] X. Xu, T. Hospedales, and S. Gong. Transductive zero-shot
action recognition by word-vector embedding. IJCV, 2017.
2

[53] X. Xu, T. M. Hospedales, and S. Gong. Multi-task zero-
shot action recognition with prioritised data augmentation.
In ECCV, 2016. 2

[54] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and
A. Agrawal. Context encoding for semantic segmentation.
In CVPR, 2018. 2

[55] L. Zhang, T. Xiang, and S. Gong. Learning a deep embed-

ding model for zero-shot learning. In CVPR, 2017. 2

[56] X. Zhang, Y. Wei, Y. Yang, and T. Huang. Sg-one: Similarity
guidance network for one-shot semantic segmentation. arXiv
preprint arXiv:1810.09091, 2018. 2

[57] Z. Zhang and V. Saligrama. Zero-shot learning via joint se-

mantic similarity embedding. In CVPR, 2016. 1, 2, 4

[58] H. Zhao, X. Puig, B. Zhou, S. Fidler, and A. Torralba. Open

vocabulary scene parsing. In ICCV, 2017. 2

[59] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene

parsing network. In CVPR, 2017. 2

[60] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. H. Torr. Conditional ran-
dom ﬁelds as recurrent neural networks. In ICCV, 2015. 2

8265

