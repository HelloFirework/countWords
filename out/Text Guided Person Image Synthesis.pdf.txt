Text Guided Person Image Synthesis

Xingran Zhou1 Siyu Huang1‚àó Bin Li1 Yingming Li1

Jiachen Li2 Zhongfei Zhang1

{xingranzh, siyuhuang, bin li, yingming, zhongfei}@zju.edu.cn, jiachen li nju@163.com

1 Zhejiang University

2 Nanjing University

An Asian man in [a white shirt], black pants, and carrying
a jug of water. He is walking forward to the camera.

A woman in a yellow shirt, [a pair of gray pants] and a pair
of pink and white shoes. She has head inclined forward.

input

yellow

blue

pink

purple

green

input

green

black

blue

pink

white

Figure 1: Samples of text guided person image synthesis. Given the reference images and the natural language descriptions,
our algorithm correspondingly generates pose and attribute transferred person images. As shown in the left, our algorithm
transfers the person pose based on ‚ÄòHe is walking forward to the camera‚Äô, and also synthesizes shirts of various different
colors. Similarly for the right example.

Abstract

This paper presents a novel method to manipulate the
visual appearance (pose and attribute) of a person image
according to natural language descriptions. Our method
can be boiled down to two stages: 1) text guided pose gen-
eration and 2) visual appearance transferred image synthe-
sis. In the Ô¨Årst stage, our method infers a reasonable target
human pose based on the text.
In the second stage, our
method synthesizes a realistic and appearance transferred
person image according to the text in conjunction with the
target pose. Our method extracts sufÔ¨Åcient information from
the text and establishes a mapping between the image space
and the language space, making generating and editing im-
ages corresponding to the description possible. We con-
duct extensive experiments to reveal the effectiveness of our
method, as well as using the VQA Perceptual Score as a
metric for evaluating the method. It shows for the Ô¨Årst time
that we can automatically edit the person image from the
natural language descriptions.

1. Introduction

Person images are produced in any time today due to the
popularization of visual capturing devices such as mobile
phones, wearable cameras, and surveillance systems. The

* Corresponding author

demand for a user-friendly algorithm to manipulate person
images is growing rapidly. In practice, people usually rep-
resent the concept about a person‚Äôs appearance and status
in a very Ô¨Çexible form, i.e., the natural languages. It is our
understanding that the guidance through text description for
generating and editing images is a friendly and convenient
way for person image synthesis.

In this paper, we propose a new task of editing a person
image according to natural language descriptions. Two ex-
amples of this task are shown in Fig. 1. Given an image of
a person, the goal is to transfer the visual appearance of the
person under the guidance of text description, while keep-
ing the invariance of person identity. SpeciÔ¨Åcally, the pose,
attributes (e.g., cloth color), and the other properties of the
identity are simultaneously edited to satisfy the description.
Generative Adversarial Networks (GANs) have offered a
solution to conditioned realistic image generation. The text-
to-image approaches [28, 22, 32, 30, 29] synthesize images
with given texts without the reference images, where the
semantic features extracted from texts are converted to the
visual representations to constitute the generated images.
However, most of these approaches only succeed in Ô¨Çower
or bird image generation. In regard to person editing, the
pose guided generation methods [18, 17, 5, 1] transfer the
person pose by taking the target pose as input to instruct the
generation process, while the editing of person image un-
der the guidance of natural language descriptions has rarely

3663

Text: A man in a gray shirt, a pair of black pants and
a pair of white shoes. He is walking toward the left.

Pose Inference 

Text

Stage I

Stage II

Pose and Attribute 

Transfer 

Input image

Figure 2: Simple illustration of our approach. In Stage-
I, we infer a reasonable human pose from the natural lan-
guage description. In Stage-II, our method takes the pre-
dicted pose, the reference image, and the text as input to
synthesize a pose and attribute transferred person image by
keeping the person identity.

been studied in the existing literature.

Motivated by these considerations, we propose a novel
text guided person image synthesis framework, which is
able to semantically edit the pose and the attributes of the
person in consistence with the text description while retain-
ing the identity of the person. As shown in Fig. 2, our
method is comprised of two stage successively. The two
stages are both built upon the adversarial learning condi-
tioned on the text description. SpeciÔ¨Åcally, Stage-I is a
newly proposed pose inference network, in which a reason-
able target pose is inferred from the text description as an
intermediate product to lay the foundation for the subse-
quent pose transferring. A set of basic poses is drawn from
the training dataset. The pose inference network Ô¨Årst se-
lects a basic pose with respect to the exact direction, and
then reÔ¨Ånes every joint in details to conform to the text de-
scription. By the pose inference network, the target pose
is guaranteed to model the shape and the layout of unique
body pose structure of a person.

Stage-II takes the predicted pose, the reference image,
and the text description as input to yield a realistic-look
pedestrian image by manipulating both the pose and the ap-
pearance of the reference image. A multi-modal learning
loss involved an attention mechanism is proposed to estab-
lish the link among different words in the text and the sub-
regions in the image. Moreover, a novel attention upsam-
pling module is developed in this stage for better combining
the pose feature and the semantic embedding. Compared
with the previous image editing methods, our model is able
to simultaneously manipulate multiple person attributes, en-
abling a more interactive and Ô¨Çexible approach to person
image synthesis.

The contribution of this paper is listed as follows. 1) We
propose a new task of manipulating person images based
on natural language descriptions, towards the goal of user-
friendly image editing. 2) For the Ô¨Årst time, we propose
a GAN-based pose inference network to generate the hu-
man pose according to the text description, to the best of
our knowledge. 3) We present a novel two-stage framework
for text guided person image synthesis, in which the mod-
ules of attention upsampling and multi-modal loss are in-
troduced to establish semantic relationships among images,
poses, and natural language descriptions. 4) We propose
the VQA Perceptual Score to evaluate the correctness of at-
tribute changes corresponding to speciÔ¨Åc body parts.

2. Related works
Deep generative models.
In recent years, deep gener-
ative models including Generative Adversarial Networks
(GANs) [6], Variational Auto-encoders (VAEs) [12], and
Autoregressive (AR) models [25] have attracted wide inter-
ests in the literature. The advances of generative models
also drive further studies on generating and translating im-
ages, including the image to image translation [8, 34, 2],
super-resolution [13, 20, 10], and the style transfer [31, 35,
9]. These techniques are of great importance for the com-
puter vision research and with a plenty of applications.
Person image generation. Recent work has achieved im-
pressive results in generating person images in the expected
poses. For instance, Ma et al. [18] propose Pose Guided
Person Image Generation (PG2) which initially generates a
coarse image and then reÔ¨Ånes the blurry result in an adver-
sarial way. Balakrishnan et al. [1] present a modular gen-
erative network which separates a scene into various layers
and moves the body parts to the desired pose. Ma et al. [17]
use a disentangled representation of the image factors (fore-
ground, background, and pose) to composite a novel person
image. Esser et al. [5] present a conditional U-Net shape-
guided image generator based on VAE for person image
generation and transfer. It is desirable to edit and manip-
ulate person images according to natural language descrip-
tions.
Text conditioned generation. Reed et al. [22] Ô¨Årst pro-
pose an end-to-end architecture based on conditional GANs
framework, which generates realistic 64 √ó 64 images for
birds and Ô¨Çowers from natural language descriptions. Their
follow-up work [23] is able to generate 128 √ó 128 images
by incorporating additional annotations of object parts. The
StackGAN [30, 29] is proposed to generate natural images
by utilizing a stacked structure consisting of multiple gen-
erators and discriminators to generate images of different
sizes. Tao et al. [28] employ the attention mechanism into
this problem into their solution, which is able to synthesize
images with Ô¨Åne-grained details from the text. Another line
of literature concentrates on editing images by natural lan-
guage description. For instance, Dong et al. [4] manipulate

3664

basic poses

A woman walking toward
the forward right side. She
is wearing black shorts, and
a white shirt, and carrying a
backpack.

LSTMs

ùúë"	

ùêπ&‚Äô(

ùëú

+,-(.

ùëù*

Pose

Encoder

1√ó256 ‚àí dim

fake pose ùëù8

Generator

‚Ñí:-;

1√ó256 ‚àí dim

Discriminator

‚Ñí<=

Figure 3: Stage-I: Text guided pose generator. We conclude the prior about poses in the training dataset as a series of basic
poses. We Ô¨Årst predict the orientation of the pose from the text by the orientation selection net F o. Then, we train a single
generator G1 that learns to manipulate every joint in the pose for Ô¨Åne-grained details.

images semantically with text descriptions. Nam et al. [19]
enhance Ô¨Åne-grained details by learning disentangled visual
attributes from text-adaptive discriminator. However, most
of them only succeed in the Ô¨Çower or bird image genera-
tion. In this paper, we present a text guided person image
synthesis framework which can generate and edit the person
pose and attribute according to natural language text while
retaining the identity of the person.

3. Method
3.1. Problem DeÔ¨Ånition

Our goal is to simultaneously transfer the pose and the
appearance of a person in the reference image correspond-
ing to the given text description.
Training data.
For each person in the training dataset,
there is a tuple (x, x‚Ä≤, p, t) containing the source (reference)
image x and the target image x‚Ä≤ of the same identity with a
different pose. p and t are the pose and the text description
of x‚Ä≤, respectively.
Our pipeline. To tackle this challenging problem, we fac-
torize it into two stages:

‚Ä¢ Stage-I: We infer a reasonable pose based on the given

text t. (See Sec. 3.2)

‚Ä¢ Stage-II: We generate a person image in which the
pose and the attribute details of that person are changed
according to target pose p and text t. (See Sec. 3.3)

3.2. Text Guided Pose Generator

In Stage-I, we propose a novel approach (see Fig. 3),
named text guided pose generator, to infer a reasonable
pedestrian pose satisfying the description.

We obtain the prior about poses in the training dataset as
the basic poses and manipulate joints in these poses. Gen-
erally, the direction of a target pose is Ô¨Årst estimated based
on the description, then the target pose is generated in con-
junction with detailed Ô¨Åne-tuning.

i

i

Basic poses. Synthesizing a pose directly from the text is
difÔ¨Åcult, as both the orientation and the other details (e.g.,
motions, posture) of a pose need to be considered. Follow-
ing [21], we group the poses of all training images into K
clusters and compute the mean pose pbasic
of the i-th clus-
}K
ter, forming a basic pose set {pbasic
i=1 (see Fig. 3 for basic
poses, where we use K = 8 like [21]). We assume that the
basic poses orient toward all K different directions.
Pose inference. Given the text description t correspond-
ing to the target image x‚Ä≤, we take the output of the Ô¨Å-
nal hidden layer of LSTMs as the sentence representation
vector œït. We predict the orientation of the pose, o =
arg maxo F ori(œït), o ‚àà {1, ..., K}. F ori is the orientation
selection net implemented as fully-connected layers. The
basic pose pbasic
o which matches the orientation o is selected
from the K basic poses.

We observe that verbs in the text can be vague in spec-
ifying the speciÔ¨Åc action of limbs. For example, the word
walking does not specify which leg to stride. The predicted
pose by a regression method could either be striding on both
legs or staying upright. Therefore, we train a single gener-
ator G1 that learns to adjust the details of the pose, formu-
lated as G1(pbasic
, œït) ‚Üí Àúp. The discriminator D1 outputs
a probability that a pose is real conditioned on the text. D1
forces G1 to concern about posture details depicted by the
text consistent with the real pose. The adversarial loss of
discriminator D1 is deÔ¨Åned as

o

LD1 = ‚àí E

p‚àºPr(p)[log D1(p)]

‚àí E Àúp‚àºPr( Àúp)[log(1 ‚àí D1(Àúp))]

(1)

And the adversarial loss of generator G1 is
LG1 = ‚àíEt‚àºpdata [log D1(G1(pbasic

o

, œït))]

(2)

However, we Ô¨Ånd that only using the adversarial loss
makes the generated poses lack of pedestrian pose struc-
ture, as the values of pose heat-map are 1 merely within the

3665

A woman walking toward
the forward right side. She
is wearing black shorts, and
a white shirt, and carrying a
backpack.

text

encoder

+

text feature matrix

Attention Upsampling Module (AU) 

'3

/5667

+

*3

,34&

,3

.

-8

Image Encoder

'&

'(

')

Conv3x3 Residual Upsampling

-.

‚Ñí"#

+

+

AU

,&

+

AU

,(

+

,)

AU

*&

*(

*)

Pose Encoder

Discriminator

‚Ñí$%

mask

‚Ñí&

Figure 4: Stage-II: Pose and attribute transferred person image generator. It is a multi-modal learning scheme which
builds the link between modalities of image, text, and pose. In addition, we propose a basic attentional upsampling (AU)
module to better incorporate information of different modalities and spatial scales into image generation. The conjunction
symbol in the AU module means the concatenation operation.

radius while the rest are almost 0. Thus, we add a mean-
square-error item, Lmse = kÀúp ‚àí pk2, to the adversarial loss
of generator G1, helping maintaining the unique structure.
The objective function of text guided pose generator is

‚Ä¢ The pose encoder [17] extracts the pose feature repre-
sentations (s1, s2, ..., sm) of different scales by taking
the target pose Àúp as input, similar to the image encoder,
si ‚àà Rli√óhi√ówi .

Ô¨Ånally formulated as

LStage-I = LG1 + Œª1Lmse + Œª2Lcls

(3)

Here, Œª1 and Œª2 are hyper-parameters for balancing the
three terms of Eq. (3). Lcls is the cross entropy between
the estimated orientation o and the real orientation oreal.

3.3. Pose and Attribute Transferred Person Image

Generator

We have predicted the target pose Àúp based on the text t so
far. In Stage-II, our goal is to transfer the pose to the target
pose Àúp and edit the appearance (e.g., cloth color) according
to certain key words in description t. 1 To tackle this chal-
lenging problem, we propose a multi-task pose and attribute
transferred image generator, which is shown in Fig. 4.

Our multi-task person image synthesis framework is

built upon the encoder-decoder structure.

‚Ä¢ The image encoder extracts the image feature maps
(v1, v2, ..., vm) of different scales by taking the source
image x as input. vi ‚àà Rli√óhi√ówi , where li, hi, wi are
the dimension, height, and width of the feature map at
the i-th scale, i ‚àà [1, ...m], m is the total number of
downsampling in the encoder.

‚Ä¢ The text encoder is a bi-directional LSTM which ex-
tracts the text feature matrix e ‚àà RL√óN of text t.
L is the dimension of hidden states, N is the num-
ber of words. e is concatenated by the hidden states
(h1, h2, ..., hN ) corresponding to every word in t.

1The attribute that we focus on mainly is the color of clothes in this
work, while in principle, our method can be easily extended to accommo-
dating other attributes.

Text-to-visual attention. We take the text feature matrix
e and the image feature map vi as input to calculate a dy-
namic text-to-visual attention which indicates the trend that
each word takes care of each local visual region when gen-
erating images. The text-to-visual attention at the i-th scale
is calculated as

F attn

i

(ÀÜei, ¬Øvi) = ÀÜeiSoftmax(ÀÜe‚ä§

i ¬Øvi)

(4)

where the visual feature vi ‚àà Rli√óhi√ówi is reshaped to ¬Øvi ‚àà
Rli√óhiwi , and the text feature matrix e is converted to a
common semantic space ÀÜei by an embedding layer as ÀÜei =
Wie, Wi ‚àà Rli√óL.
Attentional upsampling. We propose a basic module,
named attentional upsampling (AU). The motivation is that
our pose and attribute transfer problem contains multiple
modalities of data (image, pose, and text). We apply this
module to better incorporate the text-to-visual attention fea-
tures and the pose features at different scales. The pose fea-
tures conduct the layout and the structure, while the text-
to-visual attentional features integrate attribute information
from words into visual representation. In our experiments,
we observe that the module is capable of transferring the
pose and the attribute appearance of a person in the source
image, while keeping the invariant identity of the source
image and the generated image.

Our attentional upsampling operates on the image fea-
ture maps and pose feature maps at the same scale (see
Fig. 4 Attentional Upsampling). To better retain informa-
tion in the source image, the generators for image synthesiz-
ing and upsampling are weight-sharing, in which the fused
features of different scales correspond to lower resolution

3666

to higher resolution, respectively. The total m attentive op-
erations in the upsampling correspond to those in the down-
sampling.

Multi-task person image generator. The total objective
function of our multi-task person image generator is deÔ¨Åned
as

By using Eq. (4), we calculate the text-to-visual atten-
tion at the i-th scale as zi = F attn
(ÀÜe, ¬Øvi). Then, zi, si and
the previous upsampling result ui‚àí1 are incorporated and
upsampled by

i

ui = F up

i (zi, si, ui‚àí1)

(5)

1 (z1, s1).

For the smallest scale (i.e., i = 1), z1 and the pose
feature s1 are concatenated and upsampled as u1 =
F up
In such a recursive manner, the information
of all different scales is incorporated in the Ô¨Ånal attentional
upsampling result um. um is passed through a ConvNet to
output the generated image Àúx. In practice, we implement
F up as ConvNets with a nearest neighbor upsampling.
Multimodal loss. The multimodal loss function helps to
establish the mapping between every word in the text and
regions of images at different scales. The multimodal loss
impose alignment among them for subsequently transfer-
ring appearance controlled by the text.

LStage-II = LG2 + Œ≥1L1 + Œ≥2LMS

(9)

where Œ≥1 and Œ≥2 are hyper-parameters. L1 is the L1 distance
between generated image Àúx and real image x‚Ä≤, written as

L1 = k(Àúx ‚àí x‚Ä≤) ‚äô M k1

(10)

where M is the mask of the target pose [18]. We use three
conditional probabilities to improve the quality of the gen-
erated images. The adversarial loss for the generator G2 is
deÔ¨Åned as

LG2 = ‚àíEÀúx‚àºPr(Àúx)[log D2(Àúx, e)]

|

{z

}

‚àíEÀúx‚àºPr(Àúx)[log D2(Àúx, p)]
}
|

{z

+

text conditional loss

pose conditional loss

‚àíEÀúx‚àºPr(Àúx)[log D2(Àúx, e, p)]
}
|

{z

text and pose conditional loss

(11)

and the adversarial loss for the discriminator D2 is
LD2 = ‚àíE

x‚Ä≤‚àºpdata [log D2(x‚Ä≤, e)] ‚àí EÀúx‚àºPr(Àúx)[log(1 ‚àí D2(Àúx, e)]

+

|

{z

text conditional loss

}

‚àíE

x‚Ä≤‚àºpdata [log D2(x‚Ä≤, p)] ‚àí EÀúx‚àºPr(Àúx)[log(1 ‚àí D2(Àúx, p)]

+

Similar to Eq. (4), the visual-to-text attention is calcu-

|

{z

pose conditional loss

}

lated by

ci = ÀÜviSoftmax( ÀÜvi

‚ä§e)

(6)

The visual feature vi is Ô¨Årst reshaped to ¬Øvi ‚àà Rli√óhiwi
and then converted to a common semantic space as ÀÜvi =
Ui¬Øvi, Ui ‚àà RL√óli . ci ‚àà RL√óN , where the j-th column of ci
denotes the visual text attention for the j-th word at the i-th
scale.

Inspired by [28], we calculate the similarity between
the visual-to-text representation and the text feature matrix.
The multi-scale visual-to-text distance is

‚àíE

x‚Ä≤‚àºpdata [log D2(x‚Ä≤, e, p)] ‚àí EÀúx‚àºPr(Àúx)[log(1 ‚àí D2(Àúx, e, p)]

|

{z

text and pose conditional loss

}

(12)

3.4. VQA Perceptual Score

The evaluation metrics of GANs in the existing literature

are not speciÔ¨Åcally designed for the attribute transfer task.

Text: The man is wearing a [purple->black] shirt. He has
on [blue -> purple] shorts and sandals. He is walking
toward the forward left side.

D(Q, T ) =

m

X

i=1

log(cid:0)

N

X

j=1

exp(r(cij, ej))(cid:1)

(7)

Text

Pose and Attribute 

Transfer 

where Q refers to the image (query) and T refers to the
description. r(¬∑, ¬∑) is the cosine similarity between two vec-
tors, m is the number of scales.

For a batch of our training pairs {(xi, ti)}I

i=1, we cal-
culate the multi-scale visual-to-text distance matrix Œõ; the
element Œõ(i,j) = D(xi, tj). Following [28], the posterior
probability that the text ti matches with the image xi is cal-
culated as P (ti|xi) = Softmax(Œõ)(i,i). Similarly, the pos-
terior that the image xi matches the text ti is P (xi|ti) =
Softmax(Œõ‚ä§)(i,i).

The multimodal similarity LMS measures the interaction
responses for the pairing of sentences and images in a batch

LMS = ‚àí

I

X

i=1

log P (ti|xi) ‚àí

I

X

i=1

Questions

VQA

A1: Black. (‚àö)
A2: Green. ( √ó )

Q1: What color is the man‚Äôs shirt?
Q2: What is the color of the man‚Äôs shorts?

Figure 5: Illustration of VQA perceptual Score. The ac-
curacy of answers returned by the VQA model denotes the
attribute transfer correctness of generative models.

log P (xi|ti)

(8)

The Inception Score (IS) [24] measures the authenticity
of synthesis and Structural Similarity (SSIM) [26] measures

3667

the structural integrity of images. To this end, we propose a
novel metric, named VQA perceptual score, for the assess-
ment of the attribute transfer correctness, i.e., whether the
attributes of a person in the generated images are in agree-
ment with the text description.

We Ô¨Årst generate images using the method we propose
by randomly changing the color adjectives of the clothes in
the text (10 colors are considered). Correspondingly, the
color word is recorded as the correct answer. Then a re-
lated question is automatically generated about the body
part (shirt, pants, etc.) and its color. We ask the VQA
model [11] with the question and the image. Finally, we
gather the responses from the VQA model and calculate the
accuracy, i.e., the VQA perceptual score. Assuming that T
is the number of images which receive all the correct an-
swers from the VQA model, and that there are N images in
total, the VQA perceptual score is deÔ¨Åned as T
N .

4. Experiments
4.1. Dataset
CUHK-PEDES dataset [14] is the only caption-annotated
pedestrian image dataset as far as we know.
The
dataset
contains 40,206 images of 13,003 persons
collected from Ô¨Åve person re-identiÔ¨Åcation datasets,
CUHK03 [16], Market-1501 [33], SSM [27], VIPER [7],
and CUHK01 [15]. Each image in the dataset is annotated
with descriptions by crowd-sourcing.

In order to train the text guided pose generator, we add
some phrases which describe the orientation, since the orig-
inal descriptions rarely contain the information. An orienta-
tion phrase is an important guidance because otherwise the
orientation in the generated image can be arbitrary and ran-
domly different when lacking the orientation information.
This may bring troubles to both the model training and test-
ing.

For each image, a short phrase is appended according
to the result of the clustering mentioned in Sec. 3.2. Ev-
ery phrase corresponds to one of the K = 8 basic orienta-
tions. We have manually checked the phrases to ensure a
high quality dataset.

Following [18], the identities of training set and testing
set are exclusive. All images in the dataset are resized to
128 √ó 64. In the training set, we have 149,049 pairs each of
which is composed of two images of the same identity but
different poses. We have 63,878 pairs in the testing set.

4.2. Comparison with Baselines

As there is no existing work exactly comparable with this
work, we implement four different baselines with appropri-
ate modiÔ¨Åcations to make them comparable with our model
as follows.2

2We do not use any extra pre-trained models in our framework, such
that all the parameters in our model are trained from scratch, which is
different from [28].

1. ModiÔ¨Åed Semantic Image Synthesis (SIS) [4] (mSIS).
SIS uses a plain text encoder without our proposed at-
tentional upsampling module. SIS only edits the at-
tribute, but the pose is not involved. We append a pose
encoder to it for pose transfer. The generator synthe-
sizes a new person image based on the encoded refer-
ence image feature and the conditional representations
of the target pose and the target text description.

2. ModiÔ¨Åed AttnGAN [28] (mAttnGAN). We add an
image encoder and a pose encoder to the origi-
nal AttnGAN [28, 19]. SpeciÔ¨Åcally, an additional
inception v3 network is adopted to establish the
link among different words in the text and the sub-
regions in the image.

3. ModiÔ¨Åed PG2 [18] (mPG2). Pose guided person image
generation (PG2) only generates the pose transferred
person image. In this baseline, we append a text en-
coder for attribute transfer. Our multi-task problem is
separated into two single-task problems, in which the
pose transferred image is Ô¨Årst synthesized and then the
image is edited according to the text description step
by step.

4. Single attentional upsampling (SAU). It conducts only
m = 1 attentional upsampling module at the smallest
scale, serving as an ablation study for our complete
attentional upsampling modules.

Quantitative analysis.
Following [17], we use the
Inception Score (IS) [24] and the Structural Similarity
(SSIM) [26] to measure the quality of generated person im-
ages. We evaluate the IS on tasks of pose transfer (PT) and
pose and attribute transfer (P&AT). We only evaluate the
SSIM on PT, as the SSIM is calculated based on images‚Äô
mean and variance values which vary during attribute trans-
ferring.

We evaluate the four baselines and our method on met-
rics of IS and SSIM, as shown in Table 1. We can see that
mSIS, mAttnGAN, and mPG2 are the improved variants of
the existing methods while their IS and SSIM values are
lower than that of our model. It indicates that simply re-
plenishing the rest procedure of the existing methods may
not be feasible for the challenging problem proposed in this
paper. SAU is better than the other baselines, while it is also
worse than our complete framework. It indicates that the at-
tentional upsampling module proposed in this work enables
a robust learning of pose and attribute transferring.
VQA perceptual score. The VQA perceptual score of our
model and the baselines are shown in Table 2. mSIS gains
a relatively high score, while its generated images almost
lose the human structure which is intolerable for visual ef-
fects. The scores of mAttnGAN and mPG2 are relatively
low, conÔ¨Årming that a separate training of the two tasks is

3668

(a) The man walking with [the yellow shirt] is wearing dark colored shorts and
shoes. He is carrying a black messenger bag, and walking toward the right.
Source
image

image (GT)

image (Ours)

orange

purple

brown

Target

Target

green

red

(b) A man who is wearing [a white shirt], a pair of black pants and a pair of gray
shoes. He is going down the stairs and looking at the front.

Source
image

Target

image (GT)

Target

image (Ours)

yellow

blue

pink

purple

green

(c) The man is wearing [a blue striped shirt] with different colors. He has on black
shorts and black shoes with white socks. He is walking toward the camera.
Source
image

brown

yellow

Target

Target

green

black

image (GT)

image (Ours)

red

(d) The man is wearing [a red book bag] on his back and has on black jeans. He is
also wearing a grey long sleeved shirt. He is walking backward.
Source
image

image (Ours)

orange

purple

yellow

Target

Target

green

black

image (GT)

Figure 6: Four examples of text guided person image synthesis by our model. The Ô¨Årst two columns are reference and target
images (i.e., ground truth (GT)) from the training set. The third column is the target image generated by our model. We
additionaly show that our model is capable of transferring attribute if we dedicate to changing the attribute (e.g., color) in the
description.

(a-1) A man in a wrinkled
[white -> purple]
shirt a
pair of [black -> gray] pants
and a pair of gray shoes. He
is
facing
forward the left.

standing

and

(a-2) A man in a wrinkled
[white -> purple] shirt a pair
of [black -> gray] pants and
a pair of gray shoes. He is
walking back of the camera
and looking down.

(b-1) She has on a shirt with
short sleeves that is [pink ->
white] on the top. She is in
[black -> purple] Capri
length pants, and walking
forward with head inclined.

(b-2) She has on a shirt with
short sleeves that is [pink ->
white] on the top. She is in
[black -> purple] Capri
length pants, and walking
toward the backward right.

(c-1) The woman is wearing
a [white -> pink] top with a
[green -> black] skirt and
some tan wedges. She is
facing away from the camera.

The woman

(c-2)
is
wearing a [white -> yellow]
top with a [green -> blue]
skirt and some tan wedges.
She is walking toward the
right.

Figure 7: Interactive editing. By arbitrarily changing input words, our model can change a person to different poses for the
same reference image. Our model can also transfer different attributes in one image at the same time, indicating a Ô¨Çexible
and robust image generation procedure.

Model

SSIM (PT)

IS (PT)

IS (P&AT)

mSIS

0.239 ¬± .106

3.707 ¬± .185

3.790 ¬± .182

mAttnGAN
mPG2
SAU

0.298 ¬± .126

3.695 ¬± .110

3.726 ¬± .123

0.273 ¬± .120

3.473 ¬± .009

3.486 ¬± .125

0.305 ¬± .121

4.015 ¬± .009

4.071 ¬± .149

Ours

0.364 ¬± .123

4.209 ¬± .165

4.218 ¬± .195

Table 1: The SSIM score for pose transfer, and the IS for
pose transfer and pose & attribute transfer (higher is better).

hard to achieve a balance between transfers of pose and at-
tribute. Our model jointly addresses the two tasks with a
multi-scale module to achieve competititve results.

4.3. Qualitative Results

Fig. 6 shows that our Stage-II framework generates re-
alistic person images based on the text description and the
predicted poses from Stage-I. In fact, the text guided image
synthesis can be regarded as a semi-supervised problem, as
there are only image-text pairs in the training dataset with-

Model

VQA score

mSIS
mAttnGAN
mPG2
SAU

Ours

0.275
0.139
0.110
0.205

0.334

Table 2: VQA perceptual score (higher is better).

out ground-truth images corresponding to different text de-
scriptions w.r.t the same identity. Nevertheless, by editing
the descriptive words of various pedestrian appearance parts
(e.g., shirt, pants), our model is able to accurately change
these parts in the image generation procedure. It indicates
that our model is able to capture sufÔ¨Åcient information from
the text, while holding an effective control on the text.

Our two-stage based method can edit both the pose and
the attribute of the identity in the reference image by the
natural language description, which is an interactive editing
process for users. Fig. 7 shows the results of the predicted

3669

(a) The woman wears a (cid:1)(cid:11)(cid:5)(cid:4)(cid:1)(cid:2) (cid:10)(cid:12)(cid:11)(cid:10)(cid:7)(cid:5)(cid:2) shirt with black pants and a black fanny
pack around her waist with black and white tennis shoes.

(a) A man wearing glasses is walking
toward the camera while wearing a grey
sweatshirt and light blue jeans.

(b) A man walks toward the left with arms
and legs extended while dressed in a
brown, polo shirt with white trim at the
collar and sleeves.

(b) The man has short dark hair and is wearing a bright (cid:1)(cid:14)(cid:5)(cid:7)(cid:7)(cid:9)(cid:13) (cid:1)(cid:2) (cid:11)(cid:5)(cid:4)(cid:2) shirt over
white shorts that end just below the knee, dark, open heeled shoes and is carrying
a khaki back pack.

(c) The man walking backward is wearing
a light colored short sleeve shirt with beige
colored shorts.

(d) The man has a red and white striped
shirt, brown pants with a brown belt, and is
carrying a black briefcase, and facing to
the forward right.

(c) Young Asian male wearing a light (cid:1)(cid:6)(cid:11)(cid:5)(cid:5)(cid:8) (cid:1)(cid:2) (cid:3)(cid:7)(cid:12)(cid:5)(cid:2) polo shirt, gray ankle
length shorts, dark socks, and shoes. He is also wearing a dark backpack and
carrying an unknown item.

Source
image

Target

image (GT)

mSIS

mAttnGAN

mPG!

SAU

Ours

Figure 8: Qualitative comparison of our method and the
baselines. Our method generates more valid and vivid per-
son images.

poses and the generated images. Our method enhances the
ability of the text for both the pose and the attribute inter-
polation. We can even change more than one word about
the color attribute in the text description, and the synthesis
is reasonable and correct corresponding to the text.

Fig. 8 shows a qualitative comparison of our method and
different baselines. In the experiment, we Ô¨Ånd that there is
a trade-off among identity invariance, pose transfer, and at-
tribute transfer. For instance, mPG2 Ô¨Årst changes the pose,
and then transfers the attribute of the person. However, the
better the pose changes, the more difÔ¨Åcult mPG2 is to trans-
fer the attribute. It is mainly because the distribution of the
optimal pose is different from the distribution of the optimal
attribute. This is also pointed out by [3] that the distinction
of learned distribution may hurt the learning of feature rep-
resentations when generating images. It is worth mention-
ing that although SAU adopts a single attentional module,
its results are relatively better than those of the other base-
lines. However, SAU only integrates embedded features of
different modalities at the smallest scale. In the experiment,
we observe that this leads to more unreal attribute transfer.
Thus, we use m = 3 attentional upsampling in our com-
plete framework.
Intuitively, the multi-scale upsampling
module exploits receptive Ô¨Åelds of different ranges to learn
visual-word mapping on diverse spatial scales so as to bet-
ter generate more realistic details. (e.g., the woman‚Äôs pack
is preserved by our method in Fig. 8(a).)
Pose inference using GANs. Fig. 9 shows the selected ba-

Ground

truth (GT)

Basic
pose

Coordinate
regression

Ours

Ground

truth (GT)

Basic
pose

Coordinate
regression

Ours

Figure 9: Qualitative comparison between our text guided
pose generator and the coordinate regression method. The
coordinate regression method may result in some distortions
of joints, and our text guided pose generator generates more
reasonable poses.

sic poses and the inferred poses given text descriptions. The
inferred poses are largely different from the basic poses,
and our Stage-I model is able to concentrate on speciÔ¨Åc
key words in the text (e.g., walking, carrying) as these key
words imply large changes in the posture of speciÔ¨Åc body
parts (e.g., arms, legs). Our model learns to adjust these de-
tails so that the inferred poses are much closer to the real
ones, providing precise target poses for subsequent proce-
dure of person image generation. We also implement a co-
ordinate regression method as the baseline. As shown in
Fig. 9, the coordinate regression method may lead to the
distortion of some joints.

5. Conclusion

In this paper, we present a novel two-stage pipeline to
manipulate the visual appearance (pose and attribute) of a
person image based on natural language descriptions. The
pipeline Ô¨Årst learns to infer a reasonable target human pose
based on the description, and then synthesizes an appear-
ance transferred person image according to the text in con-
junction with the target pose. Extensive experiments show
that our method can interactively exert control over the pro-
cess of person image generation by natural language de-
scriptions.

Acknowledgments.
This work is supported in part
by NSFC (61672456), Zhejiang Lab (2018EC0ZX01-2),
the fundamental research funds for central universities in
China (No.
2017FZA5007), ArtiÔ¨Åcial Intelligence Re-
search Foundation of Baidu Inc., the Key Program of Zhe-
jiang Province, China (No.
the funding
from HIKVision, and ZJU Converging Media Computing
Lab.

2015C01027),

3670

References

[1] Guha Balakrishnan, Amy Zhao, Adrian V Dalca, Fredo Du-
rand, and John Guttag. Synthesizing images of humans in
unseen poses. In CVPR, 2018. 1, 2

[2] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: UniÔ¨Åed genera-
tive adversarial networks for multi-domain image-to-image
translation. In CVPR, 2018. 2

[3] Jeff Donahue, Philipp Kr¬®ahenb¬®uhl, and Trevor Darrell. Ad-

versarial feature learning. In ICLR, 2017. 8

[4] Hao Dong, Simiao Yu, Chao Wu, and Yike Guo. Semantic
image synthesis via adversarial learning. In ICCV, 2017. 2,
6

[5] Patrick Esser, Ekaterina Sutter, and Bj¬®orn Ommer. A varia-
tional u-net for conditional appearance and shape generation.
In CVPR, 2018. 1, 2

[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
2

[7] Douglas Gray, Shane Brennan, and Hai Tao. Evaluating ap-
pearance models for recognition, reacquisition, and tracking.
In Proc. IEEE International Workshop on Performance Eval-
uation for Tracking and Surveillance, 2007. 6

[8] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, 2017. 2

[9] Levent Karacan, Zeynep Akata, Aykut Erdem, and Erkut
Erdem. Learning to generate images of outdoor scenes
from attributes and semantic layouts.
arXiv preprint
arXiv:1612.00215, 2016. 2

[10] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196, 2017. 2

[11] Vahid Kazemi and Ali Elqursh. Show, ask, attend, and an-
swer: A strong baseline for visual question answering. arXiv
preprint arXiv:1704.03162, 2017. 6

[12] Diederik P Kingma and Max Welling. Auto-encoding varia-

tional bayes. arXiv preprint arXiv:1312.6114, 2013. 2

[13] Christian Ledig, Lucas Theis, Ferenc Husz¬¥ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a
generative adversarial network. In CVPR, 2017. 2

[14] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, 2017. 6

[15] Wei Li, Rui Zhao, and Xiaogang Wang. Human reidentiÔ¨Åca-

tion with transferred metric learning. In ACCV, 2012. 6

[18] Liqian Ma, Jia Xu, Qianru Sun, Bernt Schiele, Tinne Tuyte-
laars, and Luc Van Gool. Pose guided person image genera-
tion. In NIPS, 2017. 1, 2, 5, 6

[19] Seonghyeon Nam, Yunji Kim, and Seon Joo Kim. Text-
adaptive generative adversarial networks: Manipulating im-
ages with natural language. In NIPS, 2018. 3, 6

[20] Augustus Odena, Christopher Olah, and Jonathon Shlens.
Conditional image synthesis with auxiliary classiÔ¨Åer gans.
arXiv preprint arXiv:1610.09585, 2016. 2

[21] Xuelin Qian, Yanwei Fu, Wenxuan Wang, Tao Xiang, Yang
Wu, Yu-Gang Jiang, and Xiangyang Xue. Pose-normalized
image generation for person re-identiÔ¨Åcation. arXiv preprint
arXiv:1712.02225, 2017. 3

[22] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In ICML, 2016. 1, 2

[23] Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka,
Bernt Schiele, and Honglak Lee. Learning what and where
to draw. In NIPS, 2016. 2

[24] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In NIPS, 2016. 5, 6

[25] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt,
Oriol Vinyals, Alex Graves, et al. Conditional image gen-
eration with pixelcnn decoders. In NIPS, 2016. 2

[26] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
Image quality assessment: from error visibil-
Simoncelli.
ity to structural similarity. IEEE Trans. Image Processing,
13(4):600‚Äì612, 2004. 5, 6

[27] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xi-
aogang Wang. End-to-end deep learning for person search.
arXiv preprint arXiv:1604.01850, 2016. 6

[28] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In CVPR, 2018. 1, 2, 5, 6

[29] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xi-
aogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stack-
gan++: Realistic image synthesis with stacked generative ad-
versarial networks. arXiv preprint arXiv:1710.10916, 2017.
1, 2

[30] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan:
Text to photo-realistic image synthesis with stacked genera-
tive adversarial networks. In ICCV, 2017. 1, 2

[31] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful

image colorization. In ECCV, 2016. 2

[32] Zizhao Zhang, Yuanpu Xie, and Lin Yang. Photographic
text-to-image synthesis with a hierarchically-nested adver-
sarial network. In CVPR, 2018. 1

[16] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-
reid: Deep Ô¨Ålter pairing neural network for person re-
identiÔ¨Åcation. In CVPR, 2014. 6

[33] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jia-
hao Bu, and Qi Tian. Person re-identiÔ¨Åcation meets image
search. arXiv preprint arXiv:1502.02171, 2015. 6

[17] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc
Van Gool, Bernt Schiele, and Mario Fritz. Disentangled per-
son image generation. In CVPR, 2018. 1, 2, 4, 6

[34] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In CVPR, 2017. 2

3671

[35] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation. In NIPS, 2017.
2

3672

