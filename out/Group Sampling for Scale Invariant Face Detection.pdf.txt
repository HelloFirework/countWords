Group Sampling for Scale Invariant Face Detection

Xiang Ming1∗

Fangyun Wei2

Ting Zhang2

Dong Chen2

Fang Wen2

Xi’an Jiaotong University1

Microsoft Research Asia2

xjtustu.mx@stu.xjtu.edu.cn

{fawe,tinzhan,doch,fangwen}@microsoft.com

Abstract

Detectors based on deep learning tend to detect multi-
scale faces on a single input image for efﬁciency. Recent
works, such as FPN and SSD, generally use feature maps
from multiple layers with different spatial resolutions to de-
tect objects at different scales, e.g., high-resolution feature
maps for small objects. However, we ﬁnd that such multi-
layer prediction is not necessary. Faces at all scales can be
well detected with features from a single layer of the net-
work.
In this paper, we carefully examine the factors af-
fecting face detection across a large range of scales, and
conclude that the balance of training samples, including
both positive and negative ones, at different scales is the key.
We propose a group sampling method which divides the an-
chors into several groups according to the scale, and ensure
that the number of samples for each group is the same dur-
ing training. Our approach using only the last layer of FPN
as features is able to advance the state-of-the-arts. Compre-
hensive analysis and extensive experiments have been con-
ducted to show the effectiveness of the proposed method.
Our approach, evaluated on face detection benchmarks in-
cluding FDDB and WIDER FACE datasets, achieves state-
of-the-art results without bells and whistles.

1. Introduction

Face detection is the key step of many subsequent face
related applications, such as face alignment [5, 77, 27, 28,
60], face synthesis [48, 1, 2, 78, 10, 24, 62] and face recog-
nition [63, 7, 55, 39, 56]. Among the various factors that
confront real-world face detection, extreme scale variations
and small facial remain a big challenge.

Previous deep learning detectors detect multi-scale faces
on a single feature map, e.g., Fast R-CNN [15] and Faster
R-CNN [46]. They offer a good trade-off between accu-
racy and speed. However, these methods tend to miss faces
at small scale because of the large stride size of the an-
chor (e.g., 16 pixels in [46]), making small faces difﬁcult

to match the appropriate anchor and thus have few positive
samples during training.

To alleviate these problems arising from scale variation
and small object instances, multiple solutions have been
proposed, including: 1) using image pyramid for training
and inference [22, 51]; 2) combining features from shallow
and deep layers for prediction [17, 29, 4]; 3) using top-down
and skip connections to produce a single high-level feature
map with ﬁne resolution [47, 50, 44]; 4) using multiple lay-
ers with different resolutions to predict object instances of
different scales [61, 6, 38, 31, 41, 35]. All of these solutions
signiﬁcantly improve the performance of detectors. Among
them, adopting several layers with different resolutions for
prediction is the most popular one, since it achieves better
performance, especially for detecting small objects.

It is generally believed that the advantage of prediction
over multiple layers stems from the multi-scale feature rep-
resentation, which is more robust to scale variation than the
feature from a single layer. However, we ﬁnd that this is
not the case, at least for face detection. We observe that
making predictions on multiple layers will produce differ-
ent numbers of anchors for different scales1, which is the
reason why pyramid features outperform single layer fea-
ture instead of the pyramid representation, and this factor
is overlooked in the comparison between pyramid features
and single layer feature conducted in FPN [35]. Empirically
we show that single layer predictions, if imposed with the
same number of anchors as that in FPN [35], achieve almost
the same accuracy.

Motivated by this observation, we carefully examine the
factors affecting face detection performance through exten-
sive empirical analysis and identify a key issue in existing
anchor based face detectors, i.e., the anchors sampled at
different scales are imbalanced. To show this, we use two
representative detection architectures, the Region Proposal
Network (RPN) in Faster R-CNN [46] and FPN [35], as ex-
amples. Figure 1 illustrates the network architectures. We
calculate the number of training samples received by an-
chors at each scale during the training process and report
them in Figure 2 (a) and (b) for RPN and FPN, respectively.

∗Work done during the internship at Microsoft Research Asia.

1The scale of a bounding box with size (w, h) is deﬁned as √wh.

3446

(a) RPN

(b) FPN

(c) FPN-ﬁnest-stride

(d) FPN-ﬁnest

(e) FPN-ﬁnest-sampling

Figure 1: Illustration of network architectures of ﬁve different detectors described in Section 3. The term “16/4” associated
with the feature map denote that anchors with scale 16 and anchor stride 4 (with respect to the input image) have been placed
on that feature map. The difference between (d) FPN-ﬁnest and (e) FPN-ﬁnest-smapling which have the same network
architecture, is that (e) used the proposed group sampling in (d).

(a) RPN

(b) FPN

(c) FPN-ﬁnest-stride

(d) FPN-ﬁnest

(e) FPN-ﬁnest-sampling

Figure 2: The distribution of positive and negative anchors at different scales on WIDER FACE training set with different
network architectures. The number is normalized by the total amount of training samples.

For RPN, since the strides of anchors for different scales
are the same, the small anchors are more difﬁcult to match
an appropriate labeled object, resulting in fewer positive
samples and thus lower accuracy for small objects. On the
other hand, FPN uses high-resolution feature maps for small
object prediction, this gives rise to several more times neg-
ative training samples for small objects than large objects.

Such trained classiﬁers would get higher accuracy for
small objects than RPN like detectors, which might be
the reason why FPN based methods perform better on the
COCO detection benchmark [37] and the WIDER FACE
database [65], where small objects are dominant. We fur-
ther report the number of training samples when only using
the last layer of FPN, which is shown in Figure 2 (d). The
distribution is similar to RPN, though the absolute values
are different. Empirically we show that similar to RPN,
predictions made only on the last layer of FPN get lower
accuracy for small objects due to the insufﬁcient positive
samples.

To handle this issue, we propose a group sampling
method, which is simple and straightforward. The idea is
to randomly sample the same number of positive samples
as well as the same number of negative samples at each
scale during each iteration of the training process. Thus
the classiﬁer is fed with balanced training samples at dif-
ferent scales. Figure 2 (e) shows the anchor distribution of
our method, where the distribution becomes more balanced.
With our approach, only using the feature map on the ﬁnest
level of FPN with group sampling is able to achieve better

performance than FPN on WIDER FACE [65].

In addition, we notice that the second stage of the Faster
R-CNN [46] also suffers from data imbalance issue, since
the scale distribution of the objects in the dataset is imbal-
anced. We show that our proposed method can be used here
by evenly sampling the features after RoI Pooling for differ-
ent scales and thus further improve the detection accuracy.
In summary, our main contribution lies in three aspects:
(1) We observe that anchor distribution across scales in-
stead of the multi-scale feature representation is the key
factor when making predictions over multiple layers, which
challenges our understanding of FPN; (2) We further care-
fully examine the factors affecting detection performance
and identify the key issue in existing anchor-based detec-
tors: the anchors are imbalanced at different scales; (3) We
present a simple and straightforward solution to handle that
issue and the proposed solution achieves noticeable effect
on detection performance.

2. Related Work

Single scale feature for multi-scale detection. Modern de-
tectors, such as Fast R-CNN [15] and Faster R-CNN [46],
use single scale feature for multi-scale detection by extract-
ing scale-invariant features through RoI operation. They
offer a good trade-off between accuracy and speed, while
still perform not well on small objects. One of the reasons
might be the insufﬁcient positive training samples for small
objects, and we show that the proposed group sampling al-
leviates this issue and improves the detection accuracy.

3447

𝑪𝟐𝑪3𝑪416/1632/1664/16128/16𝑪𝟐𝑪3𝑪4𝑪5𝑷𝟐𝑷3𝑷4𝑷5128/3264/1632/816/4𝑪𝟐𝑪3𝑪4𝑪5𝑷𝟐𝑷3𝑷4𝑷516/432/864/16128/32𝑪𝟐𝑪3𝑪4𝑪5𝑷𝟐𝑷3𝑷4𝑷516/432/464/4128/4𝑪𝟐𝑪3𝑪4𝑪5𝑷𝟐𝑷3𝑷4𝑷516/4+GS32/4+GS64/4+GS128/4+GS!489;00,9;0!489;00,9;0!489;00,9;0!489;00,9;0!489;00,9;0Top-down architecture with lateral connections. Top-
down structure with lateral connections is getting popular
ever since proposed and has been widely used in various
computer vision tasks, e.g., U-Net [47] and SharpMask [44]
for semantic segmentation, Recombinator Network [20] for
face detection, and Stacked Hourglass Network [42] for hu-
man pose estimation. The advantage of such an architecture
is that a single high-resolution feature map which captures
both semantic information and ﬁne grained details can be
obtained through the combination of the low-level feature
maps and the high-level feature maps. In our experiments,
we show that top-down architecture with lateral connections
is indeed very helpful for face detection.
Multi-scale feature for multi-scale detection. Some re-
cent works adopt the feature pyramid for object detection,
in which features at different levels are used to handle ob-
jects at different scales, e.g., SSD [38], MS-CNN [6] and
FPN [35]. FPN also exploits the top-down architecture
with lateral connections for strong feature representation.
Such multi-scale feature representation is also widely used
for face detection to improve accuracy, e.g., SSH [41] and
S3FD [75]. However, we show that the improvement comes
from the anchor distribution toward small objects rather
than the multi-level feature representation.
Data imbalance. Learning from class imbalanced data,
in which the distribution of training data across different
object classes is signiﬁcantly skewed, is a long-standing
problem. A common approach to address class imbal-
ance issue in machine learning is re-sampling the training
data [8, 16, 18, 68, 3], e.g., under-sampling instances of the
majority classes [40] or over-sampling the instances of the
minority classes with generative and discriminative mod-
els [21, 79, 12]. Another common approach is cost-sensitive
learning, which reformulates existing learning algorithms
by penalizing the misclassiﬁcations of the minority classes
more heavily than the misclassiﬁcations of the majority
class [54, 76]. For detection, the imbalanced scale distri-
bution can also be regarded as one kind of class imbalance
issue. Previous works using hard example mining [49] or
carefully designed loss functions [36] have implicitly miti-
gated this issue to some degree. For instance, S3FD [75] has
observed that the positive training samples is insufﬁcient for
small objects and propose to increase the number of small
positive training samples by decreasing the IoU threshold.
In this work, we point out that the scale imbalance distribu-
tion over not only the positive samples but also the negative
samples is a key issue and propose a simple group sampling
method to explicitly handle it, resulting in better detection
accuracy.

3. Motivation: Scale Imbalance Distribution

In this section, we provide an in-depth analysis of two
factors that might affect detection accuracy: multi-scale fea-

Figure 3: Network architecture used in our experiments, ×2
is bilinear upsampling and L is element wise summation.

ture representation and scale imbalance distribution. We
use ResNet-50 [19] combined with top down and skip con-
nections. Figure 3 brieﬂy illustrates the network structure.
The output features from the last residual block of conv2,
conv3, conv4 and conv5 are denoted as C2, C3, C4, C5
respectively. The bottom-up feature map ﬁrst undergoes
a 1 × 1 convolution layer to reduce the channel dimen-
sions and then is merged with the up-sampled feature
map by element-wise addition. This process is repeated
three times. We denote the ﬁnal output feature maps as
{P2, P3, P4, P5}, and Pi has the same spatial size with Ci.
The anchor scales are {16, 32, 64, 128} and the aspect ratio
is set as 1. Based on this network architecture, we compare
ﬁve types of detectors:

1. RPN: The feature map C4 is used as the detection layer

where all anchors are tiled with stride 16 pixels.

2. FPN: {P2, P3, P4, P5} are used as the detection layers
with the anchor scales {16, 32, 64, 128} corresponding
to feature stride {4, 8, 16, 32} pixels respectively.

3. FPN-ﬁnest-stride: All anchors are tiled on the
i.e., P2. The
ﬁnest layer of the feature pyramid,
stride is {4, 8, 16, 32} pixels for anchors with scale
{16, 32, 64, 128}, respectively. This is implemented
by sub-sampling P2 for larger strides.

4. FPN-ﬁnest: All anchors are also tiled on P2. The stride

for each anchor is 4 pixels.

5. FPN-ﬁnest-sampling: This adopts the same setting
with FPN-ﬁnest. Additionally, we use the proposed
group sampling method to balance the training sam-
ples for different scales.

To ensure fair comparison, all detectors use the same
setting for both training and inference on the challenging
WIDER FACE dataset [65]. The results are evaluated on
the WIDER FACE validation dataset. we have following
observations.

3448

𝒃𝒍𝒐𝒄𝒌𝒄𝒐𝒏𝒗𝟐𝒃𝒍𝒐𝒄𝒌𝒄𝒐𝒏𝒗𝟑𝒃𝒍𝒐𝒄𝒌𝒄𝒐𝒏𝒗𝟒𝒃𝒍𝒐𝒄𝒌𝒄𝒐𝒏𝒗𝟓𝟏×𝟏/𝟐𝟓𝟔×𝟐+𝟏×𝟏/𝟐𝟓𝟔𝟏×𝟏/𝟐𝟓𝟔𝟏×𝟏/𝟐𝟓𝟔++×𝟐×𝟐𝑷𝟐𝑷𝟑𝑷𝟒𝑷𝟓𝒃𝒍𝒐𝒄𝒌𝒄𝒐𝒏𝒗𝟏ResNet-50𝑪𝟐𝑪𝟑𝑪𝟒𝑪𝟓Using multiple layer features is little helpful. The only
difference between FPN and FPN-ﬁnest-stride lies in that
whether the features used for detection come from one sin-
gle layer or come from multiple layers. The Average Preci-
sion (AP) of FPN is 90.9%, 91.3%, 87.6% for easy, medium
and hard subsets respectively.
In contrast, the results for
FPN-ﬁnest-stride are 90.4%, 91.0%, 87.1%. The results are
comparable, showing that using single layer feature is sufﬁ-
cient for face detection.

Scale imbalance distribution matters. We further com-
pare FPN-ﬁnest-stride with FPN-ﬁnest, which set the stride
for all different anchors as 4 pixels. We observe that 1)
FPN-ﬁnest gets better performance on easy and medium
subsets as expected, since more training examples for large
anchors are selected than FPN-ﬁnest-stride, and 2) loses
1.1% AP on hard subset, even though FPN-ﬁnest has the
same number of anchors for scale 16. To ﬁnd out the reason
behind, we plot the proportions of training positive samples
and negative ones at different scales for all the compared
detectors in Figure 2 and show the AP results in Table 1.

First, the performance of FPN and FPN-ﬁnest-stride are
almost the same and their anchor distribution at different
scales are also similar as shown in Figure 2 (b) and (c), sug-
gesting that similar distribution, when the total number of
anchors is the same, gives rise to similar performance.

Second, as shown in Figure 2 (c) and (d), the sample
distribution at different scales for both FPN-ﬁnest-stride
and FPN-ﬁnest is imbalanced and quite different. It seems
that FPN-ﬁnest-stride has more small negative anchors and
achieves higher accuracy on hard set, while FPN-ﬁnest has
more large positive anchors and achieves higher accuracy
on easy set. This leads to our hypothesis that scale imbal-
ance distribution is a key factor affecting the detection accu-
racy. RPN as shown in Figure 2 (a) also has more large pos-
itive anchors and gets 2.0% higher on easy subset compared
with FPN-ﬁnest-stride, future supporting our hypothesis.

Motivated by above observations, we propose a group
sampling method to handle the scale imbalance distribu-
tion. Figure 2 (e) shows that the anchor distribution of FPN-
ﬁnest-sampling which uses the proposed group sampling
method during training is more balanced, and as a result,
FPN-ﬁnest-sampling achieves the best performance.

4. Group Sampling Method

For anchor based face detection, there is an important
step which is to match ground-truth boxes with anchors and
assign those anchors with labels based on their IoU ratios.
Therefore the classiﬁers are optimized based on these as-
signed positive and negative anchors.
In this section, we
ﬁrst introduce the anchor matching strategy that we adopt
and then present the proposed group sampling method.

4.1. Anchor Matching Strategy

Current anchor matching strategies usually follow a
two-pass policy, which has been widely used in detection
works [46, 38]. In the ﬁrst pass, each anchor is matched
with all ground-truth boxes and it is assigned with a posi-
tive/negative label if its highest IoU is above/below a prede-
ﬁned threshold. However, some ground-truth boxes may be
unmatched in this step. The second pass is to further asso-
ciate those unmatched ground-truth boxes with anchors. We
also adopt such policy and the details are described below.
i=1, where
i is the index of the anchor and n is the number of the an-
chors for all scales. Similarly, the ground-truth boxes are
denoted as {gj}m
j=1, where j is the index of the ground-
truth and m is he number of ground-truth boxes. Before
the matching step, a matching matrix M ∈ Rn×m is ﬁrst
constructed, representing the IoUs between anchors and
ground-truth boxes, i.e., M(i, j) = IoU(pi, gj).

Formally, the set of anchors is denoted as {pi}n

In the ﬁrst pass, each anchor pi is matched with all
the ground-truth boxes to ﬁnd the highest IoU, denoted as
C(i) = max
M(i, j). Hence pi is assigned with a label
1≤j≤m

according to the following equation:

L(i) =




λ1 ≤ C(i)

1,
−1, λ2 ≤ C(i) < λ1
0,

C(i) < λ2

(1)

where λ1 and λ2 are two preset thresholds, the label 1 rep-
resents the positive samples, 0 represents the negative sam-
ples, and −1 means that pi will be ignored during training.
It is likely that some ground-truth bounding boxes are
not matched to any anchor in the ﬁrst pass, especially for
small objects. So the second pass often aims to make full
use of all ground-truth boxes to increase the number of pos-
itive training samples. Speciﬁcally, for each unmatched
ground-truth box, say gj , we match it with the anchor
pi which satisﬁes three conditions: 1) this anchor is not
matched to any other ground-truth boxes; 2) IoU(pi, gj) ≥
λ2; 3) j = argmax
1≤u≤m

IoU(pi, gu).

4.2. Group Sampling

After each anchor is associated with a label, we ﬁnd that
there exist two kinds of imbalance in the training samples.

• Positive and negative samples are not balanced:

the
number of negative samples in the image is much
greater than the number of positive samples due to the
nature of object detection task.

• Samples at different scales are not balanced: small ob-
jects are more difﬁcult to ﬁnd a suitable anchor than
large objects due to IoU based matching policy.

3449

Previous methods often notice the ﬁrst point and usually
handle it by hard negative example mining, e.g., the posi-
tive and negative sample ratio is set to 1:3 when sampling
training examples. But they all ignored the second point.

To handle above two issues, we propose a scale aware
sampling strategy called group sampling. We ﬁrst divide
all the training samples into several groups according to the
anchor scale, i.e., all anchors in each group have the same
scale. Then we randomly sample the same number of train-
ing samples for each group, and ensure that the ratio of pos-
itive and negative samples in each sampled group is 1:3. If
there is a shortage of positive samples in a group, we will
increase the number of negative samples in this group to
make sure that the total number of samples for each group
remains the same.

Formally, let Ps and Ns represent the set of randomly
sampled positive and negative anchors with scale s, that is
Ps ⊆ {pi|L(i) = 1, S(i) = s} and Ns ⊆ {pi|L(i) =
0, S(i) = s}. Thus our proposed approach is to ﬁrst guar-
antee that |Ps| + |Ns| = N where N is a constant, and then
ensure that 3|Ps| = |Ns| for scale s. Therefore, for all the
scales, each classiﬁer would have sufﬁcient and balanced
positive and negative samples for training.

Grouped Fast R-CNN. It is known that after obtaining the
candidate regions, using Region-of-Interest (RoI) operation
to extract features for each proposal and then feeding these
features into another network to further improve the de-
tection accuracy. However, directly applying Fast R-CNN
brings a little performance improvement (about 1%). Con-
sidering the huge computation cost introduced, this prac-
tice is quite cost-ineffective. Interestingly, we notice that
the scale distribution of the training samples for Fast R-
CNN is also unbalanced, where the proposed group sam-
pling method can be used again. Therefore, we use group
sampling here to ensure that the number of training sam-
ples in each group is the same, and the ratio for positive and
negative sample remains 1:3. We show that this can effec-
tively improve the accuracy of Fast R-CNN. We denote Fast
R-CNN with group sampling as Grouped Fast R-CNN.

Relation to OHEM and Focal Loss. Online hard exam-
ple mining (OHEM) [38] is to keep the top K samples with
highest training loss for back propagation. Focal Loss [36]
proposes giving each sample a speciﬁc weight. Both seem
similar to the cost-sensitive learning often used in address-
ing data imbalance by penalizing the misclassiﬁcations of
the minority class more heavily. However, the weight for
each sample in OHEM and Focal Loss is set with respect
to the sample’s loss in a hard/soft manner, which can be
viewed as an implicit and dynamic way of handling data
imbalance. Our approach, on the other hand, is able to ex-
plicitly handle the data imbalance for different scales and
achieve better performance as shown in Table 4.

5. Training Process

In this section, we introduce the training dataset, loss
function and other implementation details. Note that we
propose a new IoU based loss function for regression to get
better performance compared with Smooth-L1 loss [46].

Training dataset. As with previous works [38, 75], we
train our models on the WIDER FACE training set which
contains 12, 880 images and test on the WIDER FACE val-
idation and testing set, as well as the FDDB dataset.

Loss function. We use softmax loss for classiﬁcation. For
regression, we propose a new IoU based loss, denoted as
IoU least square loss,

Lreg =

1
Nreg X

(pi,gj )

k1 − IoU(pi, gj)k2
2,

(2)

where (pi, gj) is a matching pair of an anchor pi and a
ground-truth gj . Compared to smooth-L1 loss, this loss
function directly optimizes the IoU ratio, which is consis-
tent with the evaluation metric. Another IoU based loss
function is − ln(IoU) proposed in [69]. It is clear that when
IoU equals to 1, which is the ideal case, previous IoU loss
will get non-zero gradient, while our IoU least square loss
gets zero gradient, allowing the network to converge stably.
Empirically we show that the proposed IoU loss achieves
better performance.

Optimization details.
All models are initialized
with the pre-trained weights of ResNet-50 provided by
torchvision2 and ﬁne-tuned on the WIDER FACE train-
ing set. Each training iteration contains one image per GPU
for an 8 NVIDIA Tesla M40 GPUs server. We set the initial
learning rate to 0.01 and decrease the learning rate by 0.1
at 60th and 80th epoch. All the models are trained for 100
epochs by synchronized SGD. The momentum and weight
decay is set to 0.9 and 5 × 10−5 respectively. Our code is
based on PyTorch [43].

During training, we use scale jitter and random horizon-
tal ﬂip for data augmentation. For scale jitter, each image
will be resized by 0.25 × n, and n is randomly chosen from
[1, 8]. Then we randomly crop a patch from the resized im-
age to ensure that each side of the image does not exceed
1,200 pixels due to the GPU memory limitation. We set
λ1 = 0.6 and λ2 = 0.4 for the two-pass anchor matching
policy. At the inference stage, we build the image pyramid
for multi-scale test. The proposals from each level of the
image pyramid will be merged by Non-Maximum Suppres-
sion (NMS). Due to the GPU memory limitation, each side
of the test image will not exceed 3,000 pixels.

2https://github.com/pytorch/vision

3450

Table 1: Average Precision (AP) of face detection on WIDER FACE validation set. GS represents the proposed group
sampling method. @16 represents the AP on all data when only using outputs from the sub-dector at scale 16 for detection.
So dose @32, @64 and @128.

Methods
RPN
FPN-ﬁnest
FPN
FPN-ﬁnest-stride
FPN-ﬁnest-sampling

Feature

Anchor Stride GS

C4
P2

{P2, P3, P4, P5}

P2
P2

16
4

{4, 8, 16, 32}
{4, 8, 16, 32}

4

X

Easy Medium Hard
83.0
92.5
94.1
86.6
87.6
90.9
87.1
90.4
94.7
88.7

91.0
93.0
91.3
91.0
93.8

All @16 @32 @64 @128
21.5
74.0
80.2
22.4
21.2
82.1
21.8
81.6
82.8
24.5

48.6
65.6
72.3
72.2
74.1

65.1
66.8
67.3
66.6
72.9

43.8
43.9
43.2
43.3
47.8

6. Experiments

In this section, we ﬁrst examine the factors affect-
ing detection accuracy and then present extensive ablation
experiments to demonstrate the effectiveness of our ap-
proach. Finally, we introduce that our approach using single
layer predictions advances the state-of-the-arts on WIDER
FACE [65] and FDDB [25] datasets.

6.1. Factors Affecting Detection Accuracy

We further present a thorough analysis about the ﬁve de-
tectors: RPN, FPN, FPN-ﬁnest, FPN-ﬁnest-stride and FPN-
ﬁnest-sampling, which have been introduced in Section 3.
There are two differences among them: 1) the feature map
on which the anchors are tiled; 2) the stride for different
anchors. The stride of an anchor indicates the number of
anchors and smaller stride gives rise to more anchors. Usu-
ally, the size of the feature map will have a corresponding
anchor stride with respect to the original image. For FPN-
ﬁnest-stride, we tile the anchors of scale {16, 32, 64, 128}
with the stride of {1, 2, 4, 8} on the feature map P2, equiv-
alent to stride of {4, 8, 16, 32} on the original image.

We adopt Average Precision (AP) as the evaluation met-
Previous methods usually report AP on the easy,
ric.
medium and hard subsets for evaluation. However, the re-
sults cannot reﬂect the ability of the sub-detector which is
used to handle objects in a speciﬁc scale range. This is be-
cause a large face (e.g., 128 × 128 pixels) which is usually
detected by the anchor with scale 128, is possible to be ac-
tually detected by the anchor with scale 16 due to the multi-
scale test. Therefore, to clearly show the ability of each sub-
detector, we also report the performance of 4 sub-detectors
in our model on the ‘All’ subsets and they are denoted as
@16, @32, @64 and @128. The performance comparison is
shown in Table 1. We have following observations:

Imbalanced training data at scales leads to worse (bet-
ter) accuracy for the minority (majority). The only dif-
ference between FPN-ﬁnest and FPN-ﬁnest-stride is the an-
chor stride, i.e., the number of anchors for different scales
are different. For scale 16, its stride is the same in the two
models. Therefore, the number of anchors at scale 16 is also
the same. However, this is not the case for performance over
@16. FPN-ﬁnest-tride achieves 72.3%, 6.7% higher than

that in FPN-ﬁnest. This is because that in FPN-ﬁnest, the
number of positive samples at scale 16 is fewer than that at
other scales, resulting in lower accuracy. On the contrary, in
FPN-ﬁnest-stride, the number of positive as well as negative
samples at scale 16 is greater than other scales, resulting in
higher accuracy.

Similar anchor distribution, similar performance. As we
can see, the results of FPN and FPN-ﬁnest-stride are very
close. The only difference between the two models is the
features used for detection coming from multiple layers or
a single layer. This suggests that using multi-level feature
representation is of little help for improving detection accu-
racy. Therefore, we ask a question: does similar anchor dis-
tribution leads to similar performance ? Consider another
comparison between RPN and FPN-ﬁnest, whose sample
distributions are similar: both have more large positive ex-
amples, the two models have the same tendency of achiev-
ing lower accuracy for @16 and higher accuracy for @128
compared with FPN (or FPN-ﬁnest-stride), suggesting that
similar anchor distribution leads to similar performance.

Data balance achieves better result. All the above dis-
cussed four detectors have imbalanced anchor distribu-
tions. Comparing FPN-ﬁnest and FPN-ﬁnest-sampling,
which adopts the proposed group sampling method in FPN-
ﬁnest, the only difference is the distribution of the training
data at each scale. We can see that using more evenly dis-
tributed training data can signiﬁcantly improve the results,
increasing from 80.2% to 82.8% on the whole dataset.

6.2. Ablation Experiments

The effect of feature map. We ﬁrst compare detection ac-
curacy with and without group sampling when using differ-
ent feature maps. Table 2 shows the detection performance
when using {P2, P3, P4, P5}, P2, and other feature maps.
We have following observations: 1) using top down and
lateral connections to provide more semantic information
always helps, the performance of Pn is superior to Cn un-
der all these settings; 2) using high resolution feature map
produces more small training samples and helps detecting
small faces; 3) regardless of the feature maps, using group
sampling can always improve the results. For the sake of
simplicity, we use P2 as the feature in our ﬁnal model.

3451

(a) All

(b) @16

(c) @32

(d) @64

(e) @128

Figure 4: Illustrating the effect of the number of training samples N . Our approach (FPN-ﬁnest-sampling) gets better
performance when N increases, beneﬁting from more training examples. The performance of FPN and FPN-ﬁnest decreases
as N get larger, suffering from more imbalanced data.

Table 2: Comparison of models with/without group sam-
pling using different feature maps.

Table 3: Comparison of different loss function for the re-
gression task. The proposed loss function performs better.

Feature

{P2, P3, P4, P5}

P2

P3

P4

C3

C4

X

X

X

GS @16 @32 @64 @128
21.2
24.9
22.4
24.5
22.4
25.3
22.1
25.2
18.0
21.2
21.5
24.6

72.3
75.7
65.6
74.1
62.5
72.1
47.9
57.8
59.8
68.8
48.6
58.0

67.3
73.4
66.8
72.9
66.4
73.2
65.6
71.0
61.8
68.9
65.1
70.8

43.2
48.2
43.9
47.8
44.2
48.7
44.2
48.4
39.2
44.3
43.8
48.2

X

X

X

All
82.1
83.6
80.3
82.8
79.6
83.7
74.4
79.6
71.0
75.4
74.0
78.9

Loss
Smooth-L1
− ln(IoU)
k1 − IoUk2

2

@16 @32 @64 @128
24.5
74.1
25.1
74.6
75.0
24.9

47.8
48.0
48.2

72.9
73.1
73.2

All
82.8
83.5
83.7

Table 4: Performance comparison of the proposed group
sampling, OHEM and Focal Loss, showing that our ap-
proach achieves better performance.

Method
FPN-ﬁnest (baseline)
OHEM
Focal Loss
Group Sampling

@16 @32 @64 @128
22.4
65.6
76.0
22.0
75.8
21.5
24.5
74.1

43.9
43.9
44.2
47.8

66.8
68.9
68.5
72.9

All
80.2
81.5
81.2
82.8

The effect of the number of training samples N . As in-
troduced in Section 4.2, we randomly choose N training
samples for each scale during training. The performance
under different N is shown in Figure 4. It can be seen that:
1) the performance gets better when N increases; 2) the ac-
curacy gets saturated when N is greater than 2048. Besides,
we also plot the results of FPN and FPN-ﬁnest under differ-
ent values of N . We can see that the performance of both
models degrades when N increases, because the distribu-
tion of training examples become more imbalanced.

The effect of the proposed loss. We propose a new IoU
based loss for regression, namely least square IoU loss, to
allow the network converge stably. Here we compare dif-
ferent loss functions, including Smooth-L1, − ln(IoU) and
k1 − IoUk2
2. The detector we used is FPN-ﬁnest-sampling.
The comparison results are shown in Table 3. We can see
that the two IoU based loss functions perform better then
Smooth-L1, as they directly optimize the evaluation metric.
Compared with − ln(IoU), our proposed least square IoU
loss achieves better performance.

OHEM dynamically select B samples with highest loss
among all samples during training. We experiment with dif-
ferent values of B and ﬁnd that using a relatively smaller B
is important to make OHEM work. Hence we set B = 1024
in our experiment. For Focal Loss, we adopt the same set-
ting in [36], in which α = 0.25 and γ = 2.

The performance comparison is shown in Table 4. Both
OHEM and Focal Loss can effectively improve the perfor-
mance of detecting small faces. Take sub-detector @16 as
an example, OHEM and Focal Loss achieve 76.0% and
75.8% respectively, about 10% higher than the baseline
model. However, the performance of sub-detectors for large
scales decrease. For example, the performance of sub-
detector @128 is worse than the baseline. In contrast, our
approach gets improvement for all the sub-detectors com-
pared with the baseline by simply using the proposed group
sampling method, and also achieves better performance on
the whole dataset compared with OHEM and Focal Loss.

6.3. Grouped Fast R CNN

Comparison with OHEM and Focal Loss. Here we com-
pare our approach with two methods: OHEM [49] and Fo-
cal loss [36], both adopting hard example mining, which
can be regarded as a way of handling data imbalance.

We show that the proposed group sampling method can
be applied to Fast R-CNN to further improve the detec-
tion accuracy. We use FPN-ﬁnest-sampling as the baseline
model. The AP is increased from 82.8% to 83.9% through

3452

!
13089!!
13089
8,253!
13089!!
13089
8,253!
13089!!
13089
8,253!
13089!!
13089
8,253!
13089!!
13089
8,253n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

DSFD-0.966
SRN-0.964
Ours-0.962
PyramidBox-0.961
FDNet-0.959
FANet-0.956
FAN-0.952
Zhu et al.-0.949
Face R-FCN-0.947
SFD-0.937
Face R-CNN-0.937
SSH-0.931
HR-0.925
MSCNN-0.916
CMS-RCNN-0.899
ScaleFace-0.868
Multitask Cascade CNN-0.848
LDCF+-0.790
Faceness-WIDER-0.713
Multiscale Cascade CNN-0.691
Two-stage CNN-0.681
ACF-WIDER-0.659

0.1

0.2

0.3

0.4

0.5

Recall

0.6

0.7

0.8

0.9

1

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

DSFD-0.957
Ours-0.955
SRN-0.952
PyramidBox-0.95
FANet-0.947
FDNet-0.945
FAN-0.940
Face R-FCN-0.935
Zhu et al.-0.933
SFD-0.925
Face R-CNN-0.921
SSH-0.921
HR-0.910
MSCNN-0.903
CMS-RCNN-0.874
ScaleFace-0.867
Multitask Cascade CNN-0.825
LDCF+-0.769
Multiscale Cascade CNN-0.664
Faceness-WIDER-0.634
Two-stage CNN-0.618
ACF-WIDER-0.541

0.1

0.2

0.3

0.4

0.5

Recall

0.6

0.7

0.8

0.9

1

n
o
i
s
i
c
e
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Ours-0.911
DSFD-0.904
SRN-0.901
FAN-0.900
FANet-0.895
PyramidBox-0.889
FDNet-0.879
Face R-FCN-0.874
Zhu et al.-0.861
SFD-0.859
SSH-0.845
Face R-CNN-0.831
HR-0.806
MSCNN-0.802
ScaleFace-0.772
CMS-RCNN-0.624
Multitask Cascade CNN-0.598
LDCF+-0.522
Multiscale Cascade CNN-0.424
Faceness-WIDER-0.345
Two-stage CNN-0.323
ACF-WIDER-0.273

0.1

0.2

0.3

0.4

0.5

Recall

0.6

0.7

0.8

0.9

1

(a) Easy

(b) Medium

(c) Hard

Figure 5: Performance comparison with state-of-the-arts in terms of precision-recall curves on WIDER FACE validation set.

Table 5: The results of using group sampling method in Fast
R-CNN, showing that the proposed method is also effective
in Fast R-CNN.

Method
FPN-ﬁnest-sampling
+Fast R-CNN

+IoU loss

+ Group Sampling

Easy Medium Hard
88.8
95.1
89.7
96.2
90.3
96.4
96.2
91.1

94.1
95.1
95.3
95.5

All
82.7
83.9
84.6
85.7

Figure 6: Performance comparison with state-of-the-arts on
FDDB dataset. The values are calculated by setting the
maximum number of false positives to 200.

directly using Fast R-CNN as shown in Table 5. After using
the proposed least square IoU loss as described in Equa-
tion 2, we can increase the AP for easy, medium and hard
subsets by 0.2%, 0.2% and 0.6% respectively. Next, we fur-
ther adopt the group sampling method and the AP for easy,
medium and hard subsets are 96.3%, 95.6% and 91.2%,
which advances the state-of-the-art on WIDER FACE val-
idation set.
It is worth noting that using group sampling
gets 0.9% improvement on hard set, which is signiﬁcant as
the accuracy after using the least square IoU loss is already
90.3%. The model shown in the last line of Table 5 is our
ﬁnal model, which is used to compare with other methods.

7. Comparison with State-of-the-Art

We compare our approach on two benchmark datasets:
WIDER FACE and FDDB datasets with other face detection
methods [53, 11, 71, 58, 32, 59, 75, 52, 23, 73, 74, 67, 70,
72, 64, 66, 34, 9, 45, 26, 13, 14, 30, 33, 57].

Results on WIDER FACE dataset. WIDER FACE [65]
has 393,703 faces in 32,203 images. The faces have high
degree of variability in scale, pose and occlusion. All faces
are divided into three subsets, i.e., Easy, Medium and Hard
according to the difﬁculties of the detection. Our model
based on group sampling is trained only on the training set
and tested on the validation set. The comparison results in
terms of precision-recall curves and AP values are shown in
Figure 5. It can be seen that our method achieves 96.2%,
95.7% and 91.1% on the three subsets, and outperforms all
other methods on Hard subset by a large margin.

Results on FDDB dataset. FDDB [25] has 5,171 faces
in 2,845 images. FDDB adopts the bounding ellipse for
evaluation, while our method only outputs rectangle bound-
ing boxes. Hence we adopt the regressor provided by
S3FD [75] to generate a bounding ellipse from our rectan-
gle output. The performance comparison under discontinu-
ous score is shown in Figure 6. We can see that our method
achieves the best performance in terms of ROC curve.

8. Conclusion

In this paper, we examine the factors affecting detection
accuracy and identify that the scale imbalance distribution
is the key factor. Motivated by this observation, we propose
a simple group sampling method to handle the sample im-
balance across different scales. We show that the proposed
method is effective in existing frameworks, e.g., Faster R-
CNN and FPN, achieving better performance without extra
computational cost. On the challenging benchmarks like
WIDER FACE and FDDB, our method achieves state-of-
the-art performance. In future work, we tend to verify the
effectiveness of group sampling in general object detection.

3453

050100150200False Positives0.600.650.700.750.800.850.900.951.00True Positive RateOurs (0.966) *SFD (0.960)PyramidBox (0.952)Xiaomi Inc (0.952)DeepIR (0.952)DSFD (0.951)RSA (0.951)FANet (0.946)ScaleFace (0.945)HRER (0.943)FaceRFCN (0.938)ICCCNN (0.938)ICCCNN (0.938)FaceBoxes (0.933)UnitBox (0.931)STN (0.915)FDCNN (0.906)FastCNN (0.894)LDCF (0.883)References

[1] J. Bao, D. Chen, F. Wen, H. Li, and G. Hua. Cvae-gan:
ﬁne-grained image generation through asymmetric training.
CoRR, abs/1703.10155, 5, 2017. 1

[2] J. Bao, D. Chen, F. Wen, H. Li, and G. Hua. Towards open-
set identity preserving face synthesis. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 6713–6722, 2018. 1

[3] G. E. Batista, R. C. Prati, and M. C. Monard. A study of
the behavior of several methods for balancing machine learn-
ing training data. ACM SIGKDD explorations newsletter,
6(1):20–29, 2004. 3

[4] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick.
Inside-outside net: Detecting objects in context with skip
pooling and recurrent neural networks.
In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, pages 2874–2883, 2016. 1

[5] A. Bulat and G. Tzimiropoulos. How far are we from solv-
ing the 2d & 3d face alignment problem?(and a dataset of
230,000 3d facial landmarks). In International Conference
on Computer Vision, volume 1, page 4, 2017. 1

[6] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A uniﬁed
multi-scale deep convolutional neural network for fast ob-
ject detection. In European Conference on Computer Vision,
pages 354–370. Springer, 2016. 1, 3

[7] K. Cao, Y. Rong, C. Li, X. Tang, and C. C. Loy. Pose-robust
face recognition via deep residual equivariant mapping. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5187–5196, 2018. 1

[8] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Kegelmeyer. Smote: synthetic minority over-sampling tech-
nique. Journal of artiﬁcial intelligence research, 16:321–
357, 2002. 3

[9] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun.

Joint cas-
cade face detection and alignment.
In Computer Vision -
ECCV 2014 - 13th European Conference, Zurich, Switzer-
land, September 6-12, 2014, Proceedings, Part VI, pages
109–122, 2014. 8

[10] Y. Chen, Y. Tai, X. Liu, C. Shen, and J. Yang. Fsrnet: End-to-
end learning face super-resolution with facial priors. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2492–2501, 2018. 1

[11] C. Chi, S. Zhang, J. Xing, Z. Lei, S. Z. Li, and X. Zou. Selec-
tive reﬁnement network for high performance face detection.
CoRR, abs/1809.02693, 2018. 8

[12] G. Douzas and F. Bacao. Effective data generation for imbal-
anced learning using conditional generative adversarial net-
works. Expert Systems with applications, 91:464–471, 2018.
3

[13] S. S. Farfade, M. J. Saberian, and L. Li. Multi-view face
detection using deep convolutional neural networks. In Pro-
ceedings of the 5th ACM on International Conference on
Multimedia Retrieval, Shanghai, China, June 23-26, 2015,
pages 643–650, 2015. 8

[14] G. Ghiasi and C. C. Fowlkes. Occlusion coherence: Detect-
ing and localizing occluded faces. CoRR, abs/1506.08347,
2015. 8

[15] R. Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015. 1, 2

[16] H. Han, W.-Y. Wang, and B.-H. Mao. Borderline-smote: a
new over-sampling method in imbalanced data sets learning.
In International conference on intelligent computing, pages
878–887. Springer, 2005. 3

[17] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hy-
percolumns for object segmentation and ﬁne-grained local-
ization. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 447–456, 2015. 1

[18] H. He, Y. Bai, E. Garcia, and S. A. Li. Adaptive syn-
thetic sampling approach for imbalanced learning. ieee in-
ternational joint conference on neural networks, 2008 (ieee
world congress on computational intelligence), 2008. 3

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 3

[20] S. Honari, J. Yosinski, P. Vincent, and C. Pal. Recombi-
nator networks: Learning coarse-to-ﬁne feature aggregation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5743–5752, 2016. 3

[21] T. M. Hospedales, S. Gong, and T. Xiang. Finding rare
classes: Active learning with generative and discriminative
models. IEEE transactions on knowledge and data engineer-
ing, 25(2):374–386, 2013. 3

[22] P. Hu and D. Ramanan. Finding tiny faces.

In Computer
Vision and Pattern Recognition (CVPR), 2017 IEEE Confer-
ence on, pages 1522–1530. IEEE, 2017. 1

[23] P. Hu and D. Ramanan. Finding tiny faces. In 2017 IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages
1522–1530, 2017. 8

[24] R. Huang, S. Zhang, T. Li, R. He, et al. Beyond face rota-
tion: Global and local perception gan for photorealistic and
identity preserving frontal view synthesis. arXiv preprint
arXiv:1704.04086, 2017. 1

[25] V. Jain and E. Learned-Miller. Fddb: A benchmark for face
detection in unconstrained settings. Technical Report UM-
CS-2010-009, University of Massachusetts, Amherst, 2010.
6, 8

[26] H. Jiang and E. G. Learned-Miller. Face detection with the
faster R-CNN.
In 12th IEEE International Conference on
Automatic Face & Gesture Recognition, FG 2017, Washing-
ton, DC, USA, May 30 - June 3, 2017, pages 650–657, 2017.
8

[27] A. Jourabloo and X. Liu. Pose-invariant face alignment via
cnn-based dense 3d model ﬁtting. International Journal of
Computer Vision, 124(2):187–203, 2017. 1

[28] A. Jourabloo, M. Ye, X. Liu, and L. Ren. Pose-invariant face
alignment with a single cnn.
In Computer Vision (ICCV),
2017 IEEE International Conference on, pages 3219–3228.
IEEE, 2017. 1

[29] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards
accurate region proposal generation and joint object detec-
tion.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 845–853, 2016. 1

3454

[30] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua. A con-
volutional neural network cascade for face detection.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages
5325–5334, 2015. 8

[31] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan. Scale-
aware fast r-cnn for pedestrian detection. IEEE Transactions
on Multimedia, 20(4):985–996, 2018. 1

[32] J. Li, Y. Wang, C. Wang, Y. Tai, J. Qian, J. Yang, C. Wang,
J. Li, and F. Huang. DSFD: dual shot face detector. CoRR,
abs/1810.10220, 2018. 8

[33] J. Li and Y. Zhang. Learning SURF cascade for fast and ac-
curate object detection. In 2013 IEEE Conference on Com-
puter Vision and Pattern Recognition, Portland, OR, USA,
June 23-28, 2013, pages 3468–3475, 2013. 8

[34] Y. Li, B. Sun, T. Wu, and Y. Wang. Face detection with end-
to-end integration of a convnet and a 3d model. In Computer
Vision - ECCV 2016 - 14th European Conference, Amster-
dam, The Netherlands, October 11-14, 2016, Proceedings,
Part III, pages 420–436, 2016. 8

[35] T.-Y. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan, and
S. J. Belongie. Feature pyramid networks for object detec-
tion. In CVPR, volume 1, page 4, 2017. 1, 3

[36] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar. Focal
loss for dense object detection. IEEE transactions on pattern
analysis and machine intelligence, 2018. 3, 5, 7

[37] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European conference on computer
vision, pages 740–755. Springer, 2014. 2

[38] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-
Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector.
In European conference on computer vision, pages 21–37.
Springer, 2016. 1, 3, 4, 5

[39] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
Sphereface: Deep hypersphere embedding for face recog-
nition.
In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), volume 1, 2017. 1

[40] X.-Y. Liu, J. Wu, and Z.-H. Zhou. Exploratory under-
sampling for class-imbalance learning.
IEEE Transactions
on Systems, Man, and Cybernetics, Part B (Cybernetics),
39(2):539–550, 2009. 3

[41] M. Najibi, P. Samangouei, R. Chellappa, and L. S. Davis.
In ICCV, pages

Ssh: Single stage headless face detector.
4885–4894, 2017. 1, 3

[46] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in neural information processing systems, pages
91–99, 2015. 1, 2, 4, 5

[47] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation.
In
International Conference on Medical image computing and
computer-assisted intervention, pages 234–241. Springer,
2015. 1, 3

[48] Y. Shen, P. Luo, J. Yan, X. Wang, and X. Tang. Faceid-
gan: Learning a symmetry three-player gan for identity-
preserving face synthesis. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
821–830, 2018. 1

[49] A. Shrivastava, A. Gupta, and R. Girshick. Training region-
based object detectors with online hard example mining. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 761–769, 2016. 3, 7

[50] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-
yond skip connections: Top-down modulation for object de-
tection. arXiv preprint arXiv:1612.06851, 2016. 1

[51] B. Singh and L. S. Davis. An analysis of scale invariance
in object detection–snip. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3578–3587, 2018. 1

[52] X. Sun, P. Wu, and S. C. H. Hoi. Face detection using deep
learning: An improved faster RCNN approach. Neurocom-
puting, 299:42–50, 2018. 8

[53] X. Tang, D. K. Du, Z. He, and J. Liu. Pyramidbox: A
context-assisted single shot face detector. In Computer Vi-
sion - ECCV 2018 - 15th European Conference, Munich,
Germany, September 8-14, 2018, Proceedings, Part IX,
pages 812–828, 2018. 8

[54] K. M. Ting. A comparative study of cost-sensitive boost-
ing algorithms. In In Proceedings of the 17th International
Conference on Machine Learning. Citeseer, 2000. 3

[55] W. Wan, Y. Zhong, T. Li, and J. Chen. Rethinking feature
distribution for loss functions in image classiﬁcation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 9117–9126, 2018. 1

[56] F. Wang, W. Liu, H. Liu, and J. Cheng. Additive margin soft-
max for face veriﬁcation. arXiv preprint arXiv:1801.05599,
2018. 1

[57] H. Wang, Z. Li, X. Ji, and Y. Wang. Face R-CNN. CoRR,

abs/1706.01061, 2017. 8

[42] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In European Conference
on Computer Vision, pages 483–499. Springer, 2016. 3

[58] J. Wang, Y. Yuan, and G. Yu. Face attention network:
An effective face detector for the occluded faces. CoRR,
abs/1711.07246, 2017. 8

[43] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017. 5

[59] Y. Wang, X. Ji, Z. Zhou, H. Wang, and Z. Li. Detect-
ing faces using region-based fully convolutional networks.
CoRR, abs/1709.05256, 2017. 8

[44] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar. Learn-
In European Conference on

ing to reﬁne object segments.
Computer Vision, pages 75–91. Springer, 2016. 1, 3

[45] R. Ranjan, V. M. Patel, and R. Chellappa. Hyperface: A deep
multi-task learning framework for face detection, landmark
localization, pose estimation, and gender recognition. CoRR,
abs/1603.01249, 2016. 8

[60] W. Wu, C. Qian, S. Yang, Q. Wang, Y. Cai, and Q. Zhou.
Look at boundary: A boundary-aware face alignment algo-
rithm. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2129–2138, 2018. 1

[61] F. Yang, W. Choi, and Y. Lin. Exploit all the layers: Fast
and accurate cnn object detector with scale dependent pool-
ing and cascaded rejection classiﬁers. In Proceedings of the

3455

[76] Z.-H. Zhou and X.-Y. Liu. Training cost-sensitive neural
networks with methods addressing the class imbalance prob-
lem. IEEE Transactions on Knowledge and Data Engineer-
ing, 18(1):63–77, 2006. 3

[77] S. Zhu, C. Li, C.-C. Loy, and X. Tang. Unconstrained face
alignment via cascaded compositional learning. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3409–3417, 2016. 1

[78] S. Zhu, S. Liu, C. C. Loy, and X. Tang. Deep cascaded bi-
network for face hallucination. In European Conference on
Computer Vision, pages 614–630. Springer, 2016. 1

[79] X. Zhu, Y. Liu, J. Li, T. Wan, and Z. Qin. Emotion classi-
ﬁcation with data augmentation using generative adversarial
networks. In Paciﬁc-Asia Conference on Knowledge Discov-
ery and Data Mining, pages 349–360. Springer, 2018. 3

IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2129–2137, 2016. 1

[62] H. Yang, D. Huang, Y. Wang, and A. K. Jain. Learning
face age progression: A pyramid architecture of gans. arXiv
preprint arXiv:1711.10352, 2017. 1

[63] J. Yang, P. Ren, D. Zhang, D. Chen, F. Wen, H. Li, and
G. Hua. Neural aggregation network for video face recog-
nition. In CVPR, volume 4, page 7, 2017. 1

[64] S. Yang, P. Luo, C. C. Loy, and X. Tang. From facial parts
responses to face detection: A deep learning approach. In
2015 IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015, pages
3676–3684, 2015. 8

[65] S. Yang, P. Luo, C.-C. Loy, and X. Tang. Wider face: A
face detection benchmark. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5525–5533, 2016. 2, 3, 6, 8

[66] S. Yang, P. Luo, C. C. Loy, and X. Tang. WIDER FACE:
A face detection benchmark. In 2016 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2016, Las
Vegas, NV, USA, June 27-30, 2016, pages 5525–5533, 2016.
8

[67] S. Yang, Y. Xiong, C. C. Loy, and X. Tang. Face detection
through scale-friendly deep convolutional networks. CoRR,
abs/1706.02863, 2017. 8

[68] S.-J. Yen and Y.-S. Lee. Cluster-based under-sampling ap-
proaches for imbalanced data distributions. Expert Systems
with Applications, 36(3):5718–5727, 2009. 3

[69] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang. Unitbox:
An advanced object detection network.
In Proceedings of
the 2016 ACM on Multimedia Conference, pages 516–520.
ACM, 2016. 5

[70] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. S. Huang. Unitbox:
An advanced object detection network.
In Proceedings of
the 2016 ACM Conference on Multimedia Conference, MM
2016, Amsterdam, The Netherlands, October 15-19, 2016,
pages 516–520, 2016. 8

[71] J. Zhang, X. Wu, J. Zhu, and S. C. H. Hoi. Feature ag-
glomeration networks for single stage face detection. CoRR,
abs/1712.00721, 2017. 8

[72] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detec-
tion and alignment using multitask cascaded convolutional
networks.
IEEE Signal Process. Lett., 23(10):1499–1503,
2016. 8

[73] K. Zhang, Z. Zhang, H. Wang, Z. Li, Y. Qiao, and W. Liu.
Detecting faces using inside cascaded contextual CNN. In
IEEE International Conference on Computer Vision, ICCV
2017, Venice, Italy, October 22-29, 2017, pages 3190–3198,
2017. 8

[74] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
Faceboxes: A CPU real-time face detector with high accu-
racy. In 2017 IEEE International Joint Conference on Bio-
metrics, IJCB 2017, Denver, CO, USA, October 1-4, 2017,
pages 1–9, 2017. 8

[75] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
Sˆ 3fd: Single shot scale-invariant face detector.
In Com-
puter Vision (ICCV), 2017 IEEE International Conference
on, pages 192–201. IEEE, 2017. 3, 5, 8

3456

