Density Map Regression Guided Detection Network for RGB-D

Crowd Counting and Localization

Dongze Lian1∗, Jing Li1∗, Jia Zheng1, Weixin Luo1,2, Shenghua Gao1†

1 ShanghaiTech University

2 Yoke Intelligence

{liandz, lijing1, zhengjia, luowx, gaoshh}@shanghaitech.edu.cn

Abstract

To simultaneously estimate head counts and localize
heads with bounding boxes, a regression guided detection
network (RDNet) is proposed for RGB-D crowd counting.
Speciﬁcally, to improve the robustness of detection-based
approaches for small/tiny heads, we leverage density map to
improve the head/non-head classiﬁcation in detection net-
work where density map serves as the probability of a pixel
being a head. A depth-adaptive kernel that considers the
variances in head sizes is also introduced to generate high-
ﬁdelity density map for more robust density map regression.
Further, a depth-aware anchor is designed for better initial-
ization of anchor sizes in detection framework. Then we use
the bounding boxes whose sizes are estimated with depth
to train our RDNet. The existing RGB-D datasets are too
small and not suitable for performance evaluation on data-
driven based approaches, we collect a large-scale RGB-D
crowd counting dataset. Experiments on both our RGB-
D dataset and the MICC RGB-D counting dataset show
that our method achieves the best performance for RGB-
D crowd counting and localization. Further, our method
can be readily extended to RGB image based crowd count-
ing and achieves comparable performance on the Shang-
haiTech Part B dataset for both counting and localization.

1. Introduction

Crowd counting is a task of estimating the number of
persons in images or surveillance videos, and it has drawn
a lot of attention in computer vision community due to its
potential applications in security-related scenarios. Almost
all previous works target at RGB image based crowd count-
ing [37, 21, 25, 17] and achieve satisfactory performance
on this task. With the popularity of depth sensor, people
also propose to study RGB-D crowd counting [36, 1, 4] in

∗Equal contribution.
†Corresponding author.

surveillance scenarios. Compared with RGB image, depth
map provides additional information about the localization
of heads [5, 33]. In this paper, we propose to simultaneously
count and localize heads with RGB-D data.

Crowd counting methods can be roughly categorized
into regression-based approaches and detection-based ap-
proaches.
Recent works have shown the success of
regression-based approaches [37, 18, 21, 25, 17] for den-
sity map estimation in crowd counting. However, a crucial
issue in these regression-based approaches is that the posi-
tion of each head is not explicitly given, which restricts the
application of regression-based approaches in some related
video surveillance tasks,
including pedestrian detection
[22], anomaly detection [16] and person re-identiﬁcation
[23], etc.. In contrast, detection-based crowd counting ap-
proaches [29, 30, 31] can provide such head localization
information. However, detection-based approaches usually
encounter underestimation issues because of the low re-
call rate for small/tiny heads. Motivated by the success of
regression-based approaches as well as advantages of RGB-
D data for object detection [5, 33], we propose to leverage
density map for more robust detection-based crowd count-
ing with RGB-D data. Next we will analyze the challenges
in detection-based approaches, and give our solutions by
leveraging RGB-D data.

Challenge 1: Underestimation. Underestimation,
which means the number of detected heads is much smaller
than the total number of heads (i.e. low recall), is a com-
mon problem in detection-based approaches. Especially
when the heads are small/tiny or occluded, detection-based
approaches usually fail to detect them [15]. However,
small/tiny heads are very common in real scenarios. For
example, about 23% of heads are smaller than 8×8 pixels
in ShanghaiTech Part B as shown in Figure 1 (a).
Our solution. We alleviate this underestimation prob-
lem from the following aspects: i) a density map provides
a prior about the probability of a pixel being a head. Exist-
ing work [37, 12] has shown that the effectiveness of density
map estimation for those small/tiny even occluded heads (as
shown in Figure 1 (c)), which motivates us to leverage den-

1821

sity map to facilitate the classiﬁcation branch in detection-
based approaches. Thus we propose a regression guided de-
tection network (RDNet) for crowd counting; ii) regression
methods would greatly beneﬁt from high-ﬁdelity ground-
truth density map in training phase. However, ground-truth
density map is usually generated based on a Gaussian kernel
with a ﬁxed bandwidth centered at each head without con-
sidering the changes of head sizes, whereas such changes
can be very signiﬁcant even within each image, as shown in
Figure 1 (b). Obviously such density map generation is not
desirable. As depth helps the estimation of head sizes, we
propose a depth-adaptive kernel for Gaussian based ground-
truth density map generation. Our depth-adaptive kernel
generates a high-quality density map for training a more ro-
bust regression network, which consequently boosts the per-
formance of detection-based crowd counting; iii) RetinaNet
[14] is used for head detection in our paper. One reason for
RetinaNet failing to detect small heads is that the anchors
are set in higher layers, while for those small/tiny heads,
the anchor should be set in lower layers. Luckily, depth
provides a prior for estimating the size of heads, which is
helpful to determine in which layers we should set anchors
as well as the initialization of anchor sizes. We term the
strategy of leveraging depth for anchor sizes initialization
as depth-aware anchor.

Challenge 2: ground-truth annotation. Detection-
based approaches need the annotations of bounding boxes
for all heads, but the bounding box annotation is extremely
time-consuming compared with point annotation at head
center. Besides, occlusion is common in the crowded
scenes, such the annotation for those occluded heads with
bounding boxes is also much difﬁcult.

Our solution. We propose to estimate size of a bound-
ing box based on depth of the head center, and use the es-
timated bounding box as ground-truth to train our network.
As shown in Figure 1 (b), our estimated bounding boxes
can well locate heads. Experiments also show that our strat-
egy achieves state-of-the-art performance for crowd count-
ing and localization.

In view of the importance of RGB-D for detection-based
approaches, it is highly demanded to have a large-scale
RGB-D crowd counting dataset. However, existing RGB-
D dataset is too small [1] for data-driven approaches. Thus
we introduce a large-scale RGB-D dataset by capturing im-
ages from crowded scenes at different places. Our dataset
contains 2,193 images and 144,512 head counts in total. As
far as we know, it is the largest RGB-D dataset for crowd
counting. In our dataset, each head is annotated with a point
at head center, and the bounding box of each head is also
provided in test set to facilitate the evaluation of head de-
tection.

(cid:28580)
(cid:28580)
(cid:28580)
(cid:28580)
(cid:28581)
(cid:28564)
(cid:28652)

s
d
a
e
h
 
f
o
 
r
e
b
m
u
n
 
e
h
T

(cid:28592)(cid:28588)

(cid:28588)(cid:28658)(cid:28581)(cid:28585)

(cid:28581)(cid:28586)(cid:28658)(cid:28583)(cid:28581) (cid:28583)(cid:28582)(cid:28658)(cid:28586)(cid:28583) (cid:28586)(cid:28584)(cid:28658)(cid:28581)(cid:28582)(cid:28587)

Width range of bounding box

(a)

(b)

(c)

Figure 1. (a) shows the range of bounding boxes width in Shang-
haiTech Part B training data (We generate these bounding boxes
with nearest neighbors.). (b) shows the estimated bounding boxes
using depth information. (c) is the density map.

RGB-D crowd counting and localization; ii) depth-adaptive
kernel and depth-aware anchor are designed to facilitate
density map generation in regression and anchor initializa-
tion in detection. We further leverage depth to estimate the
bounding box sizes of all heads and use them as the ground-
truth to train RDNet; iii) we introduce a large-scale RGB-
D crowd counting dataset named ShanghaiTechRGBD for
performance evaluation, and such a dataset would accel-
erate the study of detection-based approaches for crowd
counting; iv) our method can be easily extended to RGB
image based crowd counting and localization. Extensive ex-
periments validate the effectiveness of our method for both
RGB-D and RGB crowd counting.

2. Related Work

2.1. Detection based Crowd Counting

Early detection-based approaches [20, 29, 30, 31, 11]
mainly rely on hand-crafted features, whose performance
usually decays seriously for those very crowded scenes
with occlusions. Recently, deep learning based approaches
have demonstrated their performance for object detection
[13, 14]. Thus people attempt to leverage these more ad-
vanced detection framework for crowd counting. One ex-
ample is that Stewart et al.
[28] proposed an end-to-end
people detector for crowded scenes. In very crowd scenes,
the head sizes can be extremely small, consequently bound-
ing box annotations may be very difﬁcult sometimes. Thus
the ground-truth for crowd counting is usually annotated
with a dot at head center, which restricts the exploration
of detection-based approaches for crowd counting. Fur-
thermore, most previous objects detection methods cannot
well handle the small/tiny objects, which are common in
crowd counting. Thus the performance of detection-based
approaches are usually inferior to that of regression-based
approaches. In this paper, we will show that detection-based
approaches can also achieve comparable even better perfor-
mance by leveraging RGB-D data.

2.2. Regression based Crowd Counting

Our main contributions are summarized as follows: i) we
propose a regression guided detection network (RDNet) for

Regression-based approaches map an image to its den-
sity map where the integration is total number of heads.

1822

Recently, CNN based approaches [37, 18, 21, 25, 17] have
shown their advantages over hand-crafted features [10, 3] in
learning this nonlinear mapping. According to the change
of view angles as well as the change in density at differ-
ent regions, many networks [37, 21, 25, 12] have been care-
fully designed and shown their good performance for crowd
counting, such as MCNN [37], switch-CNN [21], CSRNet
[12], etc. We refer readers to a survey paper [26] for more
details about CNN based crowd counting. Recently, Liu et
al. [15] also propose to take advantage of the results of de-
tection for density map regression. In contrast, we leverage
regression to improve the detection for crowd counting, and
our solution can also provide the location information of
heads. To achieve this aim, Idrees et al. [7] also propose
to simultaneously solve counting, density map regression
and localization in recent work. Speciﬁcally, their method
estimates a binary localization map where head centers cor-
respond to 1’s, and all the rest are 0’s, but its optimization
is not easy, and the estimated locations are coarse due to the
downsampling layer in CNN.

2.3. RGB D Crowd Counting

Although the depth sensors are very popular, only a few
works focus on RGB-D crowd counting [32, 1, 36] due to
the lack of RGB-D crowd counting dataset. In these works,
the depth information was usually used to segment the fore-
ground/background in RGB image or detect the position of
head directly. Bondi et al.
[1] leveraged depth image to
help detect the position of head and a RGB-D dataset was
proposed in their work. Similarly, Zhang et al. [36] pro-
posed an unsupervised water ﬁlling method to count people.
Song et al. [27] utilized the deep region proposal network
to perform head detection on the depth images collected by
an overhead vertical Kinect sensor. In [4], Fu et al. uti-
lized RGB-D information and detected head-shoulder for
ﬁnal crowd counting. However, there are only two RGB-D
datasets, and the amount of people is small in both datasets,
as shown in Table 1. In this paper, we introduce a large-
scale RGB-D dataset, and we leverage depth for design-
ing anchors, generating more accurate ground-truth density
maps and estimating bounding boxes for detection-based
crowd counting.

3. Method

The overall network architecture of our regression
guided detection network (RDNet) for crowd counting is
shown in Figure 2. It contains two modules: a density map
regression module and a head detection module. In the den-
sity map regression module, depth-adaptive kernel is intro-
duced to generate high-ﬁdelity ground-truth density map.
In the detection module, we leverage a RetinaNet [14] for
detection in view of its advantages in both speed and perfor-
mance. We feed the estimated density map to the classiﬁca-

tion branch in the detection network to facilitate the classi-
ﬁcation of heads, meanwhile, the depth-aware anchor strat-
egy is also proposed to initialize appropriate anchor, which
also helps the improvement of detection performance.

3.1. Density Map Regression Module

Density map regression module takes an image as input
and leverages CNN for density map estimation. The most
commonly used ground-truth density map generation strat-
egy utilizes a Gaussian with ﬁxed bandwidth for approxi-
mating the density map. Given a head with location xi, and
if the image contains N heads in total, then the density map
of this image can be written as:

D(x) =

N

Xi=1

δ(x − xi) ∗ Gσ(x).

(1)

Gσ(x) is a 2D Gaussian kernel with ﬁxed bandwidth σ.
Therefore, the crowd counting problem is converted to the
following problem: F : I(x) → D(x), which learns a
mapping from an image space I(x) to a density map space
D(x). Once the mapping function F is learnt, the density
map of any given image can be obtained and the integration
over the whole image is an estimation of total head counts.
A high-ﬁdelity ground-truth density map is desired. Ac-
tually, the sizes of heads vary signiﬁcantly, even for the
heads within an image, as shown in Figure 1 (b). There-
fore, it is desirable to design different σ’s for different heads
other than using the same σ for all heads. Bounding box
annotation can provide such information, but it is time-
consuming than point annotation and it is also hard to an-
notate bounding boxes for those tiny or occluded heads. In
[37], a distance based strategy is used to determine σ for
each head, which sets σ linearly proportional to distance be-
tween the target head and its nearest neighbors. Such strat-
egy works well for those very crowd areas, while it fails in
the areas where people are very sparse. Considering that
depth provides information of head sizes within an image
under the assumption that all heads are of the same sizes
in the real world, we propose a depth-adaptive kernel for
density map generation.

As shown in Figure 3, the projection radius of a human
head in practice and head in an image are R and r, respec-
tively. f is the focal length of the camera, and d is the depth
of head 1. Because the distance between head and the cam-
era is much larger than the radius of head, we can approx-
imate the diameter of head as 2R shown in the Figure 3.
According to the camera projection and triangle similarity,
we have the following equations:

r
R

=

s2
s1

=

f
d

(2)

1Here, we use a stereo camera and we assume the head radius R is the
same. In fact, the height of the camera can slightly impact R. The speciﬁc
formulation can refer to [6].

1823

RDNet

Depth-aware

anchors

Depth-aware 

anchors

Conv  1 × 1

Conv  1 × 1

Conv  1 × 1

Regression module

Depth map

Binarization

(cid:1839)

(cid:1838)

(cid:28631)(cid:28664)(cid:28673)(cid:28678)(cid:28668)(cid:28679)(cid:28684)(cid:28595)

(cid:28672)(cid:28660)(cid:28675)(cid:28595)

(cid:28666)(cid:28664)(cid:28673)(cid:28664)(cid:28677)(cid:28660)(cid:28679)(cid:28674)(cid:28677)

Element-wise 
multiplication

Depth-adaptive 

density map

F(cid:2869)

F(cid:3039)

F(cid:3013)

(cid:3002)
(cid:1830)(cid:2869)

(cid:3002)
(cid:1830)(cid:3039)

(cid:3002)
(cid:1830)(cid:3013)

Detection module

(cid:1855)(cid:1864)(cid:1853)(cid:1871)(cid:1871) + (cid:1854)(cid:1867)(cid:1876)

(cid:1871)(cid:1873)(cid:1854)(cid:1866)(cid:1857)(cid:1872)(cid:1871)

(cid:1855)(cid:1864)(cid:1853)(cid:1871)(cid:1871) + (cid:1854)(cid:1867)(cid:1876)

(cid:1871)(cid:1873)(cid:1854)(cid:1866)(cid:1857)(cid:1872)(cid:1871)

(cid:1855)(cid:1864)(cid:1853)(cid:1871)(cid:1871) + (cid:1854)(cid:1867)(cid:1876)

(cid:1871)(cid:1873)(cid:1854)(cid:1866)(cid:1857)(cid:1872)(cid:1871)

Feature 
from FPN
F(cid:3039)

(cid:1854)(cid:1867)(cid:1876) (cid:1871)(cid:1873)(cid:1854)(cid:1866)(cid:1857)(cid:1872)

×4

(cid:1849) × (cid:1834)

× 256

(cid:1849) × (cid:1834)

× 256

(cid:1849) × (cid:1834)

× (cid:886)(cid:1827)

(cid:28630)

(cid:1849) × (cid:1834)

× 256

×4

(cid:1849) × (cid:1834)

× 256

(cid:1849) × (cid:1834)

× (cid:1837)(cid:1827)

Concatenate

& Conv  1 × 1

(cid:3002)
(cid:1830)(cid:3039)

(cid:1855)(cid:1864)(cid:1853)(cid:1871)(cid:1871) (cid:1871)(cid:1873)(cid:1854)(cid:1866)(cid:1857)(cid:1872)

(cid:3002)
(cid:1830)(cid:2869)

(cid:3002)
(cid:1830)(cid:2896)

Figure 2. Our RDNet consists of two modules: regression module and detection module.

(cid:1841)(cid:1868)(cid:1872)(cid:1861)(cid:1855)(cid:1853)(cid:1864) (cid:1853)(cid:1876)(cid:1861)(cid:1871)

(cid:1829)(cid:1853)(cid:1865)(cid:1857)(cid:1870)(cid:1853)

(cid:1871)(cid:2870)

2(cid:1870)

(cid:1834)(cid:1857)(cid:1853)(cid:1856)

2(cid:1844)

(cid:1841)

(cid:1858)

(cid:1871)(cid:2869)

(cid:1835)(cid:1865)(cid:1853)(cid:1859)(cid:1857) (cid:1868)(cid:1864)(cid:1853)(cid:1866)(cid:1857)

(cid:1856)

(cid:1833)(cid:1870)(cid:1867)(cid:1873)(cid:1866)(cid:1856)

Figure 3. The relationship between the radii of human head in
practice and head in image.

and

σ = βr = β

Rf
d

= β

γ
d

.

(3)

Here we let bandwidth σ in Gaussian based density map
be proportional with the radius of head in image. We can
see that σ is inversely proportional with its depth d for a
given head. We generate the density map according to the
depths of different head centers and denote such density as
the depth-adaptive density map. Speciﬁcally, we replace
Gσ(x) in Eq.
(1) with a depth-adaptive Gaussian kernel
Gσ(d)(x) and get depth-adaptive density map DA(x).

DA(x) =

N

Xi=1

δ(x − xi) ∗ Gσ(di)(x).

(4)

Here di corresponds to depth of xi, and σ(di) = β γ
. With
di
depth-adaptive density map generation, we employ a CSR-
Net B [12] (dilation rate = 2) as our regression module in
light of its state-of-the-art performance for crowd counting.

3.2. Detection Module

Our detection network is based on a RetinaNet [14] be-
cause of its advantages in speed and accuracy. Speciﬁcally,
RetinaNet is based on a feature pyramid network (FPN) and
it contains multi-scale encoding and decoding layers. For
each decoding layer, it takes features from corresponding
encoding layers as well as outputs from its previous decod-
ing layers as inputs. The detection is conducted on every
scale feature map, which includes a class subnet for classi-
fying and a box subnet for regressing bounding boxes.

However, RetinaNet cannot be directly applied for head
counting because it fails to detect small/tiny heads, mean-
while crowd counting is only with point based ground-truth
annotations rather than bounding boxes. Thus we propose
to use estimated density map from regression module and
depth-aware anchor to improve the robustness of RetinaNet
for small/tiny heads detection and use depth to generate
bounding boxes for training RetinaNet.

Density map guided classiﬁcation. RetinaNet fails to
detect those small/tiny heads because the class subnet fails
to classify those anchor boxes as positive. However, such
class subnet would beneﬁt from density map. Density map
shows the distribution of heads, and its value at each pixel
is related to the probability of the pixel being a head. There-
fore we propose to feed the estimated density map into the
detection network to boost the performance of small/tiny
heads. RetinaNet detects heads of different scales at differ-
ent decoding layers. The lower layers respond to the de-
tection of smaller heads, and higher layers respond to the
detection of larger heads. We thus propose to mask density
map based on the depth map. Speciﬁcally, for a given de-
coding layer l (l=1,. . . ,L), suppose the sizes of heads to be
detected in this layer is between [r1, r2], based on Eq. 3,

1824

r2

, γ
r1

we can estimate the depth of the heads d ∈ [ γ
]. Then
we generate a binarization matrix M ∈ BL×Hd×Wd based
on depth map, where Hd and Wd are the height and width
of generated density map, respectively. In binarization, the
depth map is downsampled to the same size as the density
map. For each channel l in M , the values of pixels with
larger or smaller corresponding depth are set to 0’s, and the
values of pixels within the range are 1’s. We denote this bi-
nary mask as Ml, and use it to mask our estimated density
map:

DA
l = DA ⊙ Ml

(5)

where ⊙ means the element-wise multiplication, and DA
is
the masked density map corresponding to lth layer. Then
we simply concatenate this masked density map with fea-
tures Fl from the lth decoding layers to help heads/non-
heads classiﬁcation (as shown in Figure 2). In the speciﬁc
implementation, we choose L = 5, which means extracting
5 scale feature maps for classiﬁcation and regression.

l

Depth-aware anchor. One reason for the general de-
tector failing to detect small/tiny heads directly is that the
anchors are set in higher layers, while for those small/tiny
heads, the anchor should be set in lower layers. With depth
information, we can get a prior for estimating the size of
heads, which is helpful to determine which layer we should
set anchors as well as the initialization of anchor sizes. We
term the strategy of leveraging depth for anchor initializa-
tion as depth-aware anchor. Our depth-aware anchor not
only reduces the search space [35], as well as facilitates the
initialization of anchor sizes. We follow the Eq. 3 to gener-
ate depth anchor: H(m, n) = γ
d(m,n) , where (m, n) is the
index of height and width in depth-aware anchor map, and
d(m, n) is the corresponding depth at this location.

Generation of bounding box for training. Deﬁne
bounding boxes set B = {b1, ..., bN} for N heads. Accord-
ing to the Eq. 3, we can estimate width wi of bi as follows:
wi =
, where we assume that γ is the same for all im-

γ
di

ages 2. We set the bounding boxes as squares (wi = hi). For
those position with invalid depth value, we employ nearest
neighbor to generate bounding box according to [37].

3.3. Loss Function

For regression module, we adopt the Euclidean distance
to measure the distance between the estimated density map
and the ground-truth. The loss function can be deﬁned as:

LR(Θ) =

1
2M

M

Xk=1(cid:13)(cid:13)Ek(Ik; Θ) − DA
k (cid:13)(cid:13)

2
2

(6)

where Θ are the CNN model parameters to be learned. The
Ik is the k-th training image and M is the total number of
2Actually γ is slightly related to angle and height of the camera, here

we just ignore their effect.

training images. The DA
K is the ground-truth depth-adaptive
density map and EK is the density map estimated by the
regression module. The LR(Θ) is the loss between the esti-
mated density map and the ground-truth density map.

For detection module, detection loss consists of classiﬁ-

cation loss and bounding box regression loss as follows:

LD = Lcls + λLreg

where

Lreg(p) = (cid:26) 0.5(p)2,
|p| − 0.5,

if |p| ≤ 1
otherwise

(7)

(8)

and λ is the weight to balance classiﬁcation loss and bound-
ing box regression loss. We ﬁrstly optimize the regression
module according to the Eq. 7, and train the detection mod-
ule. Finally we ﬁne-tune the whole network.

We implement our proposed method with the PyTorch

1
9

, γ = 5 in
[19] framework. We empirically choose λ =
our implementation. The size of input image is 540 × 960
for efﬁciency. We conduct our experiments on NVIDIA Ti-
tan X Maxwell GPU with batch size 4 and learning rate
10−4, respectively. We train and test on each dataset inde-
pendently and Adam [9] optimizer is employed. Following
RetinaNet [14], we only randomly ﬂip images horizontally
for data augmentation.

4. Experiments

4.1. Evaluation Metrics

We follow the standard evaluation metrics for crowd
counting evaluation [37, 21]: mean absolute error (MAE)
and mean square error (MSE).

MAE =

1
M

MSE = vuut

1
M

M

,

Nj − ˆNj(cid:12)(cid:12)(cid:12)
Xj=1(cid:12)(cid:12)(cid:12)
Xj=1(cid:16)Nj − ˆNj(cid:17)2

M

(9)

(10)

where M is the number of the test images, Nj and ˆNj repre-
sent ground-truth and estimated number of heads in the jth
test image, respectively. The estimated number of heads ˆNj
is the total number of all the detection bounding boxes.

In addition, to evaluate the detection performance of RD-
Net, we manually label the bounding boxes in the test set of
our RGB-D dataset as ground-truth. We follow the stan-
dard binary Average Precision (AP) calculation method and
classify a sample as positive one if IOU > 0.5 between the
bounding boxes of ground-truth and prediction. For those
images only with point annotations, we evaluate the local-
ization performance and calculate Average Precision (AP)

1825

Table 1. Comparisons of ShanghaiTechRGBD with some existing datasets: Num is the number of images; Max is the maximal crowd
count within one image; Min is the minimal crowd count; Ave is the average crowd count; Total is total number of labeled heads.

Dataset

CBSR [36]

Dataset 1
Dataset 2

MICC [1]

ShanghaiTechRGBD

Resolution
240 × 320
240 × 320
480 × 640
1080 × 1920

Num Max Min
2834
1500
3358
2193

7
7
11
234

0
0
0
6

Ave
1.6
1

5.32
65.9

Depth

Modality

Total
4,541
1,553
RGB + depth
17,630
RGB + depth
144,512 RGB + depth

according to [7]. Following [7], we classify whether a pre-
dicted head point is positive example or not based on the
following criteria:

Prediction = (cid:26) Positive,

Negative,

if dist ≤ θ
otherwise

(11)

where dist is the distance between the predicted head point
and ground-truth. We calculate AP by varying the threshold
θ. In order to distinguish between AP for detection and AP
for localization, we denote AP for detection as AP det, and
AP for localization as AP loc.

4.2. Evaluations on RGB D Crowd Counting

Datasets

4.2.1 Datasets

ShanghaiTechRGBD. To facilitate the performance evalu-
ation of data-driven approaches for crowd counting, we
introduce a large-scale RGB-D dataset named Shang-
haiTechRGBD that contains 2,193 images with 144,512 an-
notated head counts. The images in ShanghaiTechRGBD
are captured by a stereo camera (ZED3) whose valid depth
ranges from 0 to 20 meters. The scenes in our dataset in-
clude busy streets of metropolitan areas and crowded public
parks. The lighting condition ranges from very bright to
very dark in different scenarios. Some representative im-
ages in ShanghaiTechRGBD are shown in Figure 4. The
histograms of crowd counts and the statistics of heads with
different depth in ShanghaiTechRGBD are shown in Fig-
ure 5. We also compare ShanghaiTechRGBD with other
RGB-D crowd counting datasets in Table 1, and we can see
that ShanghaiTechRGBD is the most challenging RGB-D
crowd counting dataset in terms of the number of images
and heads. We randomly choose 1,193 images as training
set and use the remaining as test set.
The MICC dataset. The MICC dataset is introduced by
[1].
It is acquired by a surveillance camera in indoor
scenes. There are three video sequences in the MICC
dataset: FLOW, QUEUE and GROUPS. In the FLOW se-
quence, people walk from one point to another, while in
the QUEUE sequence, people move slowly in line. In the
GROUPS sequence, people do not move out of a controlled

3https://www.stereolabs.com/

Figure 4. Some images on the ShanghaiTechRGBD dataset.

(a)

(b)

Figure 5. (a) Statistics histogram of ShanghaiTechRGBD. (b)
Depth distribution (values over 20 m are invalid.).

area. It should be noticed that the participants are of the
same for these three sequences. There are 3,542 head counts
in 1,260 frames in FLOW sequence, 5,031 head counts in
918 frames in QUEUE sequence and 9,057 head counts in
1,180 frames. The ground-truths are annotated with bound-
ing boxes. Previous work [1] leverages unsupervised learn-
ing on MICC. Here we use 20% images of each scene in
this dataset as training set and use the remaining as test set.

4.2.2 Performance Comparison

To evaluate the effectiveness of our method, we conduct ex-
periments on the ShanghaiTechRGBD dataset and MICC
dataset with some state-of-the-art methods. i) MCNN [37].
MCNN leverages a multi-column convolution network with
kernels of different sizes to count heads with different sizes;
ii) MCNN-adaptive. We replace the density map generated
by Gaussian with ﬁxed bandwidth with our depth-adaptive
kernel in MCNN; iii) CSRNet [12]. CSRNet leverages a
dilated CNN to expand the reception ﬁeld. It achieves the
state-of-the-art performance on many datasets; iv) CSRNet-
adaptive: We replace the density map generated by Gaus-
sian with ﬁxed bandwidth with our depth-adaptive kernel

1826

Figure 6. The detection results of RDNet on ShanghaiTechRGBD, MICC and ShanghaiTech Part B are from left to right, respectively.
More detection results and failure cases are shown in supplementary material.

[8].

in CSRNet; v) DecideNet (DetNet) [15]. DecideNet lever-
ages the results of detection for density map estimation;
vi) Idrees et al.
Idrees et al. design the composi-
tional loss to estimate density map, localization map and
head counts; vii) RetinaNet. Here, we use the estimated
bounding boxes with depth information to train the Reti-
naNet; viii) RetinaNet∗. As comparison, we also use the
bounding boxes of the ﬁxed size to train the RetinaNet, and
the size is ﬁxed as average sizes of all estimated bounding
boxes with depth information.

Table 2. Performance evaluations on ShanghaiTechRGBD.

Methods

MCNN [37]

MCNN-adaptive

CSRNet [12]

CSRNet-adaptive

RetinaNet [14]
RetinaNet∗ [14]

DecideNet (DetNet) [15]

Idrees et al. [8]

RDNet

MAE MSE
10.92
7.56
9.99
7.14
7.34
5.11
7.11
4.91
10.25
14.56
36.19
21.84
13.14
9.74
10.48
7.32
4.96
7.22

AP det

-
-
-
-

0.356
0.136
0.383

-

0.610

The performance of different methods are shown in Ta-
ble 2 and Table 3. We can see that our RDNet achieves the
best performance compared with detection-based methods
and comparable performance compared with regression-
based methods. Further, we have the following observa-
tions:
i) depth-adaptive kernel always outperforms Gaus-
sian kernel with ﬁxed bandwidth for all regression-based
approaches, which validates its effectiveness for ground-
truth generation; ii) the results of RetinaNet are not satis-

Table 3. Performance evaluations on the MICC dataset.

Methods

MCNN [37]

MCNN-adaptive

CSRNet [12]

CSRNet-adaptive

RetinaNet [14]

DecideNet (DetNet) [15]

Idrees et al. [8]

RDNet

MAE MSE
2.259
1.500
2.114
1.489
1.359
2.125
2.007
1.343
2.554
1.641
2.382
1.541
2.642
1.396
1.380
2.551

AP det

-
-
-
-

0.476
0.481

-

0.505

factory because of the underestimation problem, as shown
in Figure 7. While with the help of density map and depth-
aware anchor, our method greatly improves head counting,
especially for those small/tiny heads; iii) the improvement
of RetinaNet over RetinaNet∗ validates the effectiveness of
our bounding box estimation strategy for training RDNet.

To evaluate the accuracy of localization, we compare our
[8] in terms of AP loc metric
method with Idrees et al.
in Figure 8. We can see that our method always achieves
better AP loc than Idrees et al. in the three datasets, which
validates the effectiveness of our method for localization.

4.3. Ablation Studies

To understand the effectiveness of different modules in
our RDNet, we conduct ablation studies, as shown in Table
4. It is worth noting that our method is a detection-based
method. As shown in the last three rows in Table 4, depth-
adaptive kernel (DAK) and depth-aware anchor (DAA) help
detect and count heads. Meanwhile, the ﬁrst two rows in
Table 4 show the effectiveness of DAK for the regression-

1827

RetinaNet

Our regression module

Our detection module

Figure 7. From left to right: the detection results from RetinaNet
on ShanghaiTechRGBD. The density map regression results from
our regression module. The detection results from our RDNet. We
can ﬁnd our method can detect small/tiny heads than RetinaNet.

1

0.8

0.6

0.4

0.2

l

c
o
_
P
A

0

0

Our dataset

Our
Idrees et al.

10
Distance threshold 

20

30

(a)

1

0.8

0.6

0.4

0.2

l

c
o
_
P
A

0

0

MICC

Our
Idrees et al.

10
Distance threshold 

20

30

(b)

1

0.8

0.6

0.4

0.2

l

c
o
_
P
A

0

0

ShanghaiTech Part_B

Our
Idrees et al.

10
Distance threshold 

20

30

(c)

Figure 8. (a), (b), (c) are AP loc comparisons on our RGB-D
dataset, MICC and ShanghaiTech Part B, respectively.

based method, where DAA is not applicable. The DAK im-
proves the performance of regression-based crowd count-
ing, and the DAA facilitates the detection.

Table 4. Ablation studies on our dataset (DAA: depth-aware an-
chor; DAK: depth-adaptive kernel.).

5.11
4.91

DAK DAA MAE MSE AP det
×
√
×
×
√

N/A
N/A
×
√
√

0.593
0.604
0.610

5.64
5.31
4.96

8.04
7.54
7.22

7.34
7.11

N/A
N/A

Reg

Det

Our

4.4. Evaluation on RGB Crowd Counting Dataset

Our RDNet can be easily extended to RGB image crowd
counting by removing the depth-aware anchor and depth-
adaptive kernel. Since there is no depth, we only simply
feed the density map in layer one without masking into the
class subnet. We evaluate the performance of RDNet on
ShanghaiTech Part B [37] for RGB based crowd counting.
Similar to our dataset, ShanghaiTech Part B is also a dataset
with surveillance view. Here, we use the bounding boxes
estimated by nearest neighbors strategy [37] as ground-truth
to train RDNet due to the lack of depth.

We compare our method with other state-of-the-art
methods on ShanghaiTech Part B in Table 5. We can see
that our method achieves the comparable performance with
some regression-based methods. It is worth noting that we
only use coarse bounding boxes of heads as supervision in

the training phase due to the lack of bounding box annota-
tions and depth, the performance can be further improved
if the bounding box are provided. We choose CSRNet [12]
as regression module, the improvement of our method over
CSRNet validates the effectiveness regression guided detec-
tion strategy. Further, by comparing the performance of im-
provement of SANet over CSRNet (2.2 MAE), and the im-
provement of our method over CSRNet (about 1.8 in terms
of MAE), we can see that our model would probably beneﬁt
from better regression module, such as SANet.

We also compare our method with Idrees et al. [8] in
terms of localization metric AP loc, and show the results in
Figure 8 (c). Our method achieves higher localization preci-
sion. Some bounding boxes prediction results are shown in
Figure 6. We can see that our method can locate the heads
accurately, even for those small ones.

Table 5. Evaluation results on the ShanghaiTech Part B dataset.

Methods

Zhang et al. [34]

MCNN [37]

Cascaded-MTL [24]

Switch-CNN [21]

CP-CNN [25]

SANet [2]

DecideNet (DetNet) [15]

Idrees et al. [8]

CSRNet [12]

Ours

MAE MSE
49.8
32.0
41.3
26.4
20.0
31.1
33.4
21.6
30.1
20.1
13.6
8.4
73.18
24.9
16.0
15.3

44.90
15.5
10.6
8.8

5. Conclusion

A regression guided detection network (RDNet) is pro-
posed for RGB-D crowd counting and localization, which
leverages a density map to boost the performance of detec-
tion for crowd counting. With the help of depth, i) a depth
adaptive kernel is designed, which generates high-ﬁdelity
ground-truth density map and facilitates the regression-
based crowd counting; ii) a depth-aware anchor is designed.
Our depth-aware anchor facilitates the anchor initializa-
tion, and improves the detection of small heads; iii) even
with point annotations, we can still use depth to estimate
the sizes of bounding boxes, which shows their effective-
ness for training RDNet. We further collect the large-scale
ShanghaiTechRGBD crowd counting dataset for perfor-
mance evaluation. Experiments on our dataset and MICC
show that our method achieves the best performance for
RGB-D crowd counting. Further, our method can be ex-
tended to RGB crowd counting and achieves comparable
performance on the ShanghaiTech Part B dataset.
Acknowledgement. We want to thank Desen Zhou,
Yingying Zhang, Siqin Chen for their help in collecting
data.

1828

References

[1] Enrico Bondi, Lorenzo Seidenari, Andrew D Bagdanov, and
Alberto Del Bimbo. Real-time people counting from depth
imagery of crowded environments. In Advanced Video and
Signal Based Surveillance (AVSS), 2014 11th IEEE Interna-
tional Conference on, pages 337–342. IEEE, 2014.

[2] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su.
Scale aggregation network for accurate and efﬁcient crowd
counting. In The European Conference on Computer Vision
(ECCV), September 2018.

[3] Antoni B Chan and Nuno Vasconcelos. Bayesian poisson re-
gression for crowd counting. In Computer Vision, 2009 IEEE
12th International Conference on, pages 545–551. IEEE,
2009.

[4] Huiyuan Fu, Huadong Ma, and Hongtian Xiao. Real-time
accurate crowd counting based on rgb-d information. In Im-
age Processing (ICIP), 2012 19th IEEE International Con-
ference on, pages 2685–2688. IEEE, 2012.

[5] Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, and Jiten-
dra Malik. Learning rich features from rgb-d images for ob-
ject detection and segmentation. In European Conference on
Computer Vision, pages 345–360. Springer, 2014.

[6] Derek Hoiem, Alexei A Efros, and Martial Hebert. Putting
International Journal of Computer

objects in perspective.
Vision, 80(1):3–15, 2008.

[7] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong
Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak
Shah. Composition loss for counting, density map estimation
and localization in dense crowds. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), pages 532–
546, 2018.

[8] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong
Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak
Shah. Composition loss for counting, density map estima-
tion and localization in dense crowds. In The European Con-
ference on Computer Vision (ECCV), September 2018.

[9] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[10] Victor Lempitsky and Andrew Zisserman. Learning to count
objects in images. In Advances in neural information pro-
cessing systems, pages 1324–1332, 2010.

[11] Min Li, Zhaoxiang Zhang, Kaiqi Huang, and Tieniu Tan.
Estimating the number of people in crowded scenes by mid
based foreground segmentation and head-shoulder detection.
In Pattern Recognition, 2008. ICPR 2008. 19th International
Conference on, pages 1–4. IEEE, 2008.

[12] Yuhong Li, Xiaofan Zhang, and Deming Chen. Csrnet: Di-
lated convolutional neural networks for understanding the
highly congested scenes. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1091–1100, 2018.

[14] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
In

Piotr Doll´ar. Focal Loss for Dense Object Detection.
ICCV, 2017.

[15] Jiang Liu, Chenqiang Gao, Deyu Meng, and Alexander G
Hauptmann. Decidenet: Counting varying density crowds
through attention guided detection and density estimation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5197–5206, 2018.

[16] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Fu-
ture frame prediction for anomaly detection–a new baseline.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 6536–6545, 2018.

[17] Xialei Liu, Joost van de Weijer, and Andrew D Bagdanov.
Leveraging unlabeled data for crowd counting by learning to
rank. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 7661–7669, 2018.

[18] Daniel Onoro-Rubio and Roberto J L´opez-Sastre. Towards
perspective-free object counting with deep learning. In Eu-
ropean Conference on Computer Vision, pages 615–629.
Springer, 2016.

[19] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.

[20] Mikel Rodriguez, Ivan Laptev, Josef Sivic, and Jean-Yves
Audibert. Density-aware person detection and tracking in
crowds. In 2011 International Conference on Computer Vi-
sion, pages 2423–2430. IEEE, 2011.

[21] D Babu Sam, Shiv Surya, and R Venkatesh Babu. Switching
convolutional neural network for crowd counting.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, volume 1, page 6, 2017.

[22] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu,
Xiangyu Zhang, and Jian Sun. Crowdhuman: A bench-
mark for detecting human in a crowd.
arXiv preprint
arXiv:1805.00123, 2018.

[23] Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen,
and Xiaogang Wang. Person re-identiﬁcation with deep
similarity-guided graph neural network.
In The European
Conference on Computer Vision (ECCV), September 2018.

[24] Vishwanath A Sindagi and Vishal M Patel. Cnn-based cas-
caded multi-task learning of high-level prior and density es-
timation for crowd counting. In Advanced Video and Signal
Based Surveillance (AVSS), 2017 14th IEEE International
Conference on, pages 1–6. IEEE, 2017.

[25] Vishwanath A Sindagi and Vishal M Patel. Generating high-
quality crowd density maps using contextual pyramid cnns.
In 2017 IEEE International Conference on Computer Vision
(ICCV), pages 1879–1888. IEEE, 2017.

[26] Vishwanath A Sindagi and Vishal M Patel. A survey of re-
cent advances in cnn-based single image crowd counting and
density estimation. Pattern Recognition Letters, 107:3–16,
2018.

[13] Tsung-Yi Lin, Piotr Doll´ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, volume 1, page 4,
2017.

[27] Diping Song, Yu Qiao, and Alessandro Corbetta. Depth
driven people counting using deep region proposal network.
In Information and Automation (ICIA), 2017 IEEE Interna-
tional Conference on, pages 416–421. IEEE, 2017.

1829

[28] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng.
End-to-end people detection in crowded scenes. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 2325–2333, 2016.

[29] Paul Viola, Michael J Jones, and Daniel Snow. Detecting
pedestrians using patterns of motion and appearance. In null,
page 734. IEEE, 2003.

[30] Meng Wang and Xiaogang Wang. Automatic adaptation
of a generic pedestrian detector to a speciﬁc trafﬁc scene.
In Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, pages 3401–3408. IEEE, 2011.

[31] Bo Wu and Ramakant Nevatia. Detection of multiple, par-
tially occluded humans in a single image by bayesian com-
bination of edgelet part detectors. In Computer Vision, 2005.
ICCV 2005. Tenth IEEE International Conference on, vol-
ume 1, pages 90–97. IEEE, 2005.

[32] Mingliang Xu, Zhaoyang Ge, Xiaoheng Jiang, Gaoge Cui,
Bing Zhou, Changsheng Xu, et al. Depth information guided
crowd counting for complex crowd scenes. Pattern Recogni-
tion Letters, 2019.

[33] Xiangyang Xu, Yuncheng Li, Gangshan Wu, and Jiebo Luo.
Multi-modal deep feature learning for rgb-d object detection.
Pattern Recognition, 72:300–313, 2017.

[34] Cong Zhang, Hongsheng Li, Xiaogang Wang, and Xiaokang
Yang. Cross-scene crowd counting via deep convolutional
neural networks.
In Computer Vision and Pattern Recog-
nition (CVPR), 2015 IEEE Conference on, pages 833–841.
IEEE, 2015.

[35] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and
Stan Z Li. Single-shot reﬁnement neural network for object
detection. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 4203–4212,
2018.

[36] Xucong Zhang, Junjie Yan, Shikun Feng, Zhen Lei, Dong Yi,
and Stan Z Li. Water ﬁlling: Unsupervised people counting
via vertical kinect sensor.
In Advanced Video and Signal-
Based Surveillance (AVSS), 2012 IEEE Ninth International
Conference on, pages 215–220. IEEE, 2012.

[37] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao,
and Yi Ma. Single-image crowd counting via multi-column
convolutional neural network. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pages 589–597, 2016.

1830

