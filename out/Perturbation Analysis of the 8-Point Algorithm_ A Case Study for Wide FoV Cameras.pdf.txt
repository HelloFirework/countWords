Perturbation Analysis of the 8-Point Algorithm: a Case Study for Wide FoV

Cameras

Thiago L. T. da Silveira and Claudio R. Jung

Institute of Informatics, Federal University of Rio Grande do Sul, Brazil

{tltsilveira,crjung}@inf.ufrgs.br

Abstract

This paper presents a perturbation analysis for the esti-
mate of epipolar matrices using the 8-Point Algorithm (8-
PA). Our approach explores existing bounds for singular
subspaces and relates them to the 8-PA, without assuming
any kind of error distribution for the matched features. In
particular, if we use unit vectors as homogeneous image co-
ordinates, we show that having a wide spatial distribution
of matched features in both views tends to generate lower
error bounds for the epipolar matrix error. Our experimen-
tal validation indicates that the bounds and the effective er-
rors tend to decrease as the camera Field of View (FoV) in-
creases, and that using the 8-PA for spherical images (that

present 360◦×180◦ FoV) leads to accurate essential matri-

ces. As an additional contribution, we present bounds for
the direction of the translation vector extracted from the es-
sential matrix based on singular subspace analysis.

1. Introduction

Obtaining 3D scene structure and camera poses based
on two or more views of the same scene have been widely
studied by the computer vision community, and the epipolar
geometry encoded by either the fundamental or the essen-
tial matrix is a keystone in the context of Structure from
Motion [28]. For simplicity, we will use the name epipolar
matrix to denote either one of these two matrices, as in [1].
There are several approaches for estimating epipolar ma-
trices [18, 21, 25], but the 8-Point Algorithm (8-PA) pro-
posed by Longuet-Higgins [20] is still very popular, serving
as an initial estimate or as the basis for further developments
such as feature normalization schemes [15, 24], the impo-
sition of rank constraints [49], the inclusion of additional
matrix factorizations [44], the use of robust loss functions
to better handle outliers [16], or the inclusion of weights and
deep learning strategies for simultaneous keypoint matching
and motion recovery [46]. A class of approaches focuses on
estimating optimal inlier sets [45], which is particularly im-

portant when the number of matched features is scarce and
outliers are present. When using wider Field of View (FoV)
cameras, however, the spatial region captured by both cam-
eras present more overlap on the image domain, typically
leading to more matched features. In particular, spherical
images present full 360◦×180◦ FoV and are becoming in-
creasingly popular in the context of multiview 3D recon-
struction and/or pose estimation [12, 29, 30, 31, 32].

As noted in [46], the typical pipeline for estimating
epipolar matrices consists of ﬁnding correspondence points
across images, and then applying an outlier removal strat-
egy (e.g. RANSAC [9] or its variants [33]) to retrieve a sub-
set of potential inliers. This pruned set of correspondences
is then fed to the estimation method itself, such as the 8-PA,
and errors in the matching step can degrade the estimated
epipolar matrix. They are caused by noisy feature matching
(features are correctly matched but not at the exact loca-
tion) or wrong feature matching, and determining the im-
pact of these errors on the estimated matrix is a relevant re-
search topic. Existing error analysis approaches [5, 36, 42]
focus on noisy features, assuming that badly matched fea-
tures can be effectively removed by the outlier rejection
method. However, outliers are typically detected based on
some kind of distance to the epipolar line (or curve when
using spherical images), such as algebraic or Sampson dis-
tances [39]. Hence, wrong matches along (or close to) the
epipolar line/curve might be erroneously considered inliers.
This paper presents a perturbation analysis for the 8-PA
that does not assume any kind of matching error distribu-
tion. We represent the matched features in homogeneous
coordinates as 3D unit vectors, which relate to the viewing
directions of the rays that connect the 3D world points and
the camera centers in the calibrated case. This is a particu-
larly natural choice when using spherical cameras [12, 17],
but can also be explored when the pinhole model is as-
sumed. We present error bounds for epipolar matrix esti-
mation as a function of matching errors and shed some light
on the impact of the Field of View (FoV) of the cameras on
error propagation. In particular, we show that the 8-PA po-
tentially provides good estimates for the essential matrix E

11757

when using spherical images, which present a 360◦×180◦
FoV. Additionally, we present bounds for the translation
vector (up to a scale) extracted from the essential matrix.

2. Related work

Estimating the uncertainty when computing epipolar ma-
trices has been studied by several authors [5, 15, 22, 24, 36,
42], focusing on the relationship between correspondence
errors and the epipolar matrices themselves or directly on
the errors of estimated 3D structure and/or pose.

Weng et al. [42] presented an error analysis of the essen-
tial matrix estimation based on ﬁrst order perturbations, re-
lating the covariance matrix of the matching errors with the
covariance of the essential matrix. This assumption implies
that the variance of the feature detector error must be deter-
mined, which is very dependent on the detector itself and
the scene, and very sensitive to outliers. Furthermore, they
assumed the un-normalized 8-PA, which was later shown
by Hartley that leads to numerical instabilities [15].

M¨uhlich and Mester [24] related the error produced by
the 8-PA with the perturbation of eigenvalues and singular
values. They assumed that the covariance matrix of the cor-
respondence matching errors is known to obtain a bound
for the essential matrix error, and use this bound to pro-
pose a new feature normalization scheme. Mair and col-
leagues [22] extended the analysis in [42] by including the
normalization schemes presented in [15] and [24]. Notice
that both [24] and [22] present the drawback of assuming
known feature matching variances.

Csurka et al. [5] presented an error analysis of the fun-
damental matrix F obtained by a non-linear method. They
model F as a random vector, such that the mean of the dis-
tribution is the actual matrix and the covariance encodes the
uncertainty errors. They assume that outliers were rejected
in a previous step so that the analysis is focused only on
noisy correspondences. Sur and colleagues [36] follow a
similar path but focus on the errors of epipolar matrix esti-
mation using the 8-PA. However, as in [5], they present the
uncertainty as a covariance matrix and discard the presence
of outliers, which limits the application of their method.
Also, they consider the un-normalized version of the 8-PA,
as [42].

Hartley [15] evaluated the condition number of the mea-
surement matrix used in the 8-PA, suggesting a normal-
ized version that is numerically more stable. The core of
his analysis was that using “raw” homogeneous coordinates
(just appending the value 1 to the pixel coordinates) leads
to a magnitude imbalance and hence ill-conditioned ma-
trices. When using normalized homogeneous coordinates
(unit vectors), as suggested in [13] and used in most ap-
proaches that explore spherical cameras [12, 29, 30, 31, 32],
the reasoning used by Hartley no longer applies.

Most of approaches reviewed in this section assume a

distribution model for the matching errors, and disregard
the impact of outliers. They also empirically evaluate the
effect of the camera FoV on the errors, without any mathe-
matical formalism. Next, we present our bounds for epipo-
lar matrices based on correspondence errors, and provide a
tighter relationship between the distribution of matched fea-
tures and error propagation, showing that wider FoV cam-
eras tend to lead to better epipolar matrix estimation.
In
particular, spherical cameras present a 360-degree FoV and
potentially allows several matches if descriptors tailored to
the spherical domain [4, 48] are used, as noted in [7].

3. Perturbation analysis for epipolar matrices

In this section, we brieﬂy review the formulation behind
epipolar matrices, provide some generic bounds for singular
spaces, and then present error bounds for epipolar matrices.
For the particular case of the essential matrix, we also pro-
vide bounds for the translation direction.

3.1. Epipolar matrices and the 8 PA

Let us consider a set of n ≥ 8 3D points Xi viewed
simultaneously by two cameras C1 and C2, which are pro-
jected to image points xi
2 (in homogeneous coordi-
nates), respectively. The core concept of the epipolar ge-
1, xi
ometry is that each pair of correspondence points (xi
2)
is related by the epipolar constraint [14]

1 and xi

2(cid:1)⊤ E xi
(cid:0)xi

1 = 0,

(1)

(2)

where E3×3 is an epipolar matrix that depends on the param-
eters of the two cameras.
The epipolar constraint provided by Eq. (1) is ﬂexible to
a variety of scenarios and camera types. For instance, when
using two uncalibrated perspective cameras, the correspon-
dence points are given by

xi

xi

1

yi
1

1 = K1Xi =(cid:2)xi
2 = K2 (RXi + t) =(cid:2)xi

2

zi

1(cid:3)⊤ ,

yi
2

zi

2(cid:3)⊤ ,

for i = 1, 2, ..., n, where K1 and K2 are the intrinsic pa-
rameters of the two cameras, R is the rotation matrix of the
second camera w.r.t. the ﬁrst, and t is the translation vector.
In this case, E = F is called the fundamental matrix, which
presents 7-degrees of freedom (DoF).
When using calibrated cameras (i.e., the intrinsics are
known), we can pre-multiply pixel values by K−1
and
K−1
2 , respectively. This is equivalent to use the notation
of Eq. (2) with K1 = K2 = I3×3 (identity matrix), so that
the image coordinates represent the viewing directions from
each camera center to the corresponding 3D point Xi. Sim-
ilarly, spherical cameras capture the full 360◦×180◦ neigh-
borhood, and image coordinates are typically unit vectors
that point from the camera center to the corresponding 3D

1

11758

point [2]. This is the same formulation of calibrated per-
spective cameras, and in both cases, the epipolar matrix
E = E is called the essential matrix, which presents 5-DoF.
In all these cases, the scale of the two-view coordinates
xi
c (for c ∈ {1, 2}) is arbitrary, since they relate to homoge-
neous coordinates. In the calibrated (or spherical) case, the
unit vectors xi
ck2 represent the normalized ray (in the
Euclidean space) instead of pixel coordinates (in the pro-
jective space). Since they are generic to different central
optical systems (e.g. calibrated perspective and spherical),
they have been used as the parametrization for the corre-
spondences in recent papers such as [13].

c/kxi

The core of the 8-PA is to express the epipolar constraint
provided by Eq. (1) as linear combinations of the matrix
entries. For simplicity, we will derive all results assuming
the calibrated case, i.e., E = E = [eij], for i, j ∈ {1, 2, 3}.
If e = [e11 e21 e31 e12 e22 e32 e13 e23 e33]⊤ is de-
ﬁned as the row-wise concatenation of eij, then the epipolar
constraint can be written as Ae = 0, where A is an n×9
measurement matrix for which the ith row is given by:

Ai =hxi

1(cid:0)xi

2(cid:1)⊤ yi

1(cid:0)xi

2(cid:1)⊤ zi

1(cid:0)xi

2(cid:1)⊤i .

The least squares solution for e subject to kek = 1 is
provided by the least right singular value of A [15], so that
the accuracy of the 8-PA relates to the singular subspaces
of A. Next, we present some bounds on singular subspaces,
and apply them to the context of epipolar matrix estimation.

(3)

3.2. Bounds for singular subspaces

Perturbation bounds aim to quantify how the spectrum
changes after adding a small perturbation to a matrix, and
they play an important role in SVD and spectral methods
analysis [3]. Given an approximately rank-r matrix M and
a perturbation matrix P , both of dimension n×m, an im-
portant problem is to understand how much the (left and/or
right) singular spaces of M and ˜M = M + P differ from
each other [3]. Consider that the SVD decomposition of
matrix M is given by

0

(4)

V⊥(cid:3)⊤ ,

M =(cid:2)U U⊥(cid:3)(cid:20)Σ1

0 Σ2(cid:21)(cid:2)V
V⊥(cid:3) are orthogonal matrices of
where (cid:2)U U⊥(cid:3) and (cid:2)V
orders n and m, respectively, Σ1 = diag(σ1, . . . , σr) and
Σ2 = diag(σr+1, . . .) are r×r and (n−r)×(m−r) matrices
with null off-diagonal values, respectively. Variables σ1 ≥
σ2 ≥ ··· ≥ 0 are the singular values of M in descending
order. Decomposing the perturbed matrix ˜M as

˜M = M + P =(cid:2) ˜U ˜U⊥(cid:3)(cid:20) ˜Σ1

0

0

˜Σ2(cid:21)(cid:2) ˜V

˜V⊥(cid:3)⊤ ,

produces submatrices having the same structures as
U, U⊥, V, V⊥, Σ1 and Σ2.

(5)

A well known bound for estimating the perturbation in-
ﬂuence within the singular subspaces comes from Wedin’s
sin Θ theorem [41], which provides a uniform bound for
both the left and right singular spaces in terms of the singu-
lar value gap and perturbation level. Precisely, it states that

if the gap δ = min( ˜Σ1) − max(Σ2) > 0, then:

maxn(cid:13)(cid:13)(cid:13)
sin Θ(V, ˜V )(cid:13)(cid:13)(cid:13)2
maxn(cid:13)(cid:13)(cid:13)
P ˜V(cid:13)(cid:13)(cid:13)2
,(cid:13)(cid:13)(cid:13)

,(cid:13)(cid:13)(cid:13)
sin Θ(U, ˜U )(cid:13)(cid:13)(cid:13)2o
˜U⊤P(cid:13)(cid:13)(cid:13)2o
≤ kPk2

≤

δ

δ

,

(6)

where Θ(M1, M2) = diag(cid:0)cos(ˆσ1)−1, cos(ˆσ2)−1, . . . ,
cos(ˆσr)−1(cid:1) are the canonical angles between two p × r or-
thogonal columns M1 and M2 [35], and ˆσ1 ≥ ˆσ1 ≥ . . . ≥
ˆσr ≥ 0 are the singular values of M⊤1 M2. Despite of the
wide application range, Wedin’s theorem may not be sufﬁ-
ciently precise for some analysis where left and right singu-
lar spaces change in different orders of magnitude after the
perturbation.

There is a number of works that present tighter per-
turbation bounds, but applicable only to problems with
known noise properties [26, 40]. On the other hand, Cai
and Zhang [3] recently established rate-optimal perturba-
tion bounds for the left and right singular spaces separately
without any noise assumption. In short, these bounds are
given by:

and

, 1(cid:19)

ξ2 − ζ 2 − ς

sin Θ(U, ˜U )(cid:13)(cid:13)(cid:13) ≤ min(cid:18) ξz21 + ζz12
(cid:13)(cid:13)(cid:13)
sin Θ(V, ˜V )(cid:13)(cid:13)(cid:13) ≤ min(cid:18) ξz12 + ζz21
, 1(cid:19) ,
(cid:13)(cid:13)(cid:13)

ξ2 − ζ 2 − ς

(7)

(8)

21, z2

˜M V⊥k, ς = min(z2

provided that ξ2 > ζ 2 + ς, where ξ = σmin(U⊤ ˜M V ),
ζ = kU⊤
12), z12 = kPU P PV⊥k
⊥
and z21 = kPU⊥ P PV k. Here, PD is projection operator
onto the column space of a matrix D [3], and k · k is either
the spectral (k · k2) or the Frobenius (k · kF ) matrix norm.
The bounds provided in Eqs. (7) and (8) tackle separately
the left and right subspaces and are tighter than Wedin’s
bound [3]. However, they involve projections of the per-
turbation matrix onto the noiseless right and left singular
subspaces, which are not known in practical applications.

3.3. Perturbation bounds and the 8 PA

Here, we relate generic bounds for singular subspaces
and the 8-PA. Ideally, the measurement matrix An×9 used in
the 8-PA (recall Eq. (3)) presents rank r = 8. Using the no-
tation of Eq. (4), its SVD generates an 8×8 diagonal matrix
Σ1 containing all the non-zero singular values of A, with the
corresponding left and right singular vectors provided in U

11759

and V , respectively. Also, Σ2 should be an (n − 8)×1 null
matrix, and in particular e = V⊥ is the least right singular
vector that contains the elements of the epipolar matrix.

In practice, feature matching is not exact. Without loss
of generality (as done in [24]), let us assume that xi
1 corre-
sponds to the exact feature points in the ﬁrst image and ˜xi
2
to the noisy correspondences in the second image, leading
to an approximate matrix ˜A = A + P , where P is the per-
turbation. Due to matching errors, there is no guarantee that
˜A presents rank 8, so that ˜Σ2 may not be null.

Our goal here is to estimate the error between the actual
essential matrix e = V⊥ and the estimated one ˜e = ˜V⊥,
both expressed in vector form. A natural distance measure
is the angular distance between them, computed as

θ = ∠(e, ˜e) = cos−1(cid:12)(cid:12)

e⊤ ˜e(cid:12)(cid:12) ,

which is a particular case of the canonical angles [35].

(9)

Furthermore, as analyzed in [8], the canonical angles re-
late to projection errors. More precisely, if PV = V V ⊤ and
P ˜V = ˜V ˜V ⊤ are the orthogonal projection matrices onto
the subspaces spanned by the columns of V and ˜V , respec-
tively, then

kPV − P ˜V k2 =(cid:13)(cid:13)(cid:13)

sin Θ(V, ˜V )(cid:13)(cid:13)(cid:13)2

.

(10)

Also, since e and ˜e are the orthogonal complements of

V and ˜V , respectively, then

|sin θ| = kPe − P˜ek2 = k(I − PV ) − (I − P ˜V )k2

(11)

= kPV − P ˜V k2 =(cid:13)(cid:13)(cid:13)

sin Θ(V, ˜V )(cid:13)(cid:13)(cid:13)2

,

recalling that θ is the angle between e and ˜e. Combining
Eq. (11) with Wedin’s bound provided by Eq. (6), we can
conclude that

| sin θ| ≤ kPk2

δ ≤ kPkF

δ

,

(12)

meaning that the error in the estimate of the essential ma-
trix is proportional to the norm of the perturbation P and
inversely scaled by the second least singular value of ˜A.
Although the spectral norm provides a tighter bound, the
Frobenius norm will be used, since it can be expressed only
in terms of the matching errors. In fact, we can express the

perturbation matrix P = ˜A−A as a function of the matched
2. Based on Eq. (3), the ith row of P is
points xi
given by

2 and ˜xi

1, xi

Pi =hxi

1(cid:0)∆xi

2(cid:1)⊤ yi

1(cid:0)∆xi

2(cid:1)⊤ zi

1(cid:0)∆xi

2(cid:1)⊤i ,

(13)

where ∆xi

9

n

2 = ˜xi

2. Hence,

2 − xi
kPkF =vuut
Xj=1
Xi=1
=vuut2
Xi=1
(1 − cos αj),

|Pij|2 =vuut

n

n

Xi=1

k∆xi

2k2kxi

1k2

(14)

2, xi

where αj = ∠( ˜xi
2) is the angular matching error.
Hence, the total perturbation kPkF does not depend on the
choice of the feature points xi
1 on the ﬁrst image, but solely
on the matching errors on the second image.

Furthermore, a typical error measure when estimating
fundamental/essential matrices is based on the relative er-
ror using the Frobenius norm. If E and ˜E denote the matrix
forms of the essential matrices related to e and ˜e, respec-
tively, then d(E, ˜E) = min{kE − ˜EkF ,kE + ˜EkF}, can

be used to measure the error of the estimated matrix. Hence,

d(E, ˜E) = min{ke − ˜ek2,ke + ˜ek2}

(15)

=p2 (1 − cos θ′),

where θ′ = min{θ, π − θ} = sin−1 | sin θ| ∈ [0, π/2].
Since Eq. (12) provides bounds for | sin θ|, we have
d(E, ˜E) ≤s2(cid:20)1 − cos(cid:18)sin−1 min(cid:26)1, kPk
δ (cid:27)(cid:19)(cid:21).

(16)

3.4. Relationship between the gap δ and the spatial

distribution of the features

One interesting aspect of the bound presented in inequal-
ity (6) is that the denominator δ is fully computable based
on the observed matrix ˜A, without any knowledge on the
noiseless matrix A. More precisely, δ = ˜σ8 is the second
least singular value of ˜A, which depends on several aspects:
the 3D structure of the scene (noting that points along a sin-
gle plane lead to degeneracy), the locations of selected key-
points, the relative camera poses and the FoV of the cam-
eras. In particular, some authors [13, 45, 47] have empiri-
cally studied the effect of the camera FoV. Here, we provide
a more formal relationship by relating the gap δ with the
spatial distribution of the features, which is highly related
to the camera FoV.

Let us consider the singular values of ˜A given by ˜σ1 ≥
˜σ2 ≥ ··· ≥ ˜σ9, where ˜σi = p˜λi and ˜λi is one of the
ﬁrst nine eigenvalues of ˜A⊤ ˜A (or, equivalently, of ˜A ˜A⊤).
Hartley [15] showed that when using un-normalized homo-
geneous coordinates, the entries along the diagonal of ˜A⊤ ˜A
vary considerably in magnitude and used interlacing proper-
ties to ﬁnd estimates on the eigenvalues and condition num-
ber of the matrix. When using unit vectors, however, such

11760

Instead, we evaluate the impact
analysis is not possible.
of the spatial distribution of the matched features, which is
strongly affected by the FoV of the cameras.

Merikoski et al. [23] presented several bounds for sin-
gular values and eigenvalues based on traces. In particular,
they showed that for a square matrix Bp×p with real non-
negative eigenvalues (in decreasing order), the second least
eigenvalue satisﬁes the following condition:

λp−1(B) ≤

tr(B)

p − 1−s

1

(p − 1)(p − 2)(cid:18)tr(B2) −

tr(B)2

p − 1 (cid:19).

(17)
If we consider B = ˜A⊤ ˜A (so that p = 9), we have that

tr(B) = k ˜Ak2

F =

n

Xi=1

kxi

1k2k ˜xi

2k2 = n,

(18)

recalling that n is the number of matched points. Since B is
symmetric, we also have that tr(B2) = tr(B⊤B) = kBk2
F .
Let us also consider C = ˜A ˜A⊤, so that tr(B⊤B) =
tr(C⊤C) = kCk2
F . Matrix C = [cij]n×n presents an inter-
esting structure, since each element is given as a dot product
of rows from ˜A:
cij = ˜Ai ˜A⊤j
1xj
1( ˜xi
= xi
1)⊤xj
= (xi

1yj
1zj
2 = (cos βij)(cos γij),

2 + yi
2)⊤ ˜xj

2)⊤ ˜xj
1( ˜xi

2)⊤ ˜xj

2)⊤ ˜xj

2 + zi

1( ˜xi

1( ˜xi

2

(19)

1, xj

1) and γij = ∠( ˜xi

where βij = ∠(xi
2) are the angles
between features i and j in the ﬁrst and second images, re-
spectively. Clearly, both βij and γij are limited by the FoV
of the camera: if it is small, the entries cij tend to be closer
to one. Also, we have that

2, ˜xj

c2

(cos2 βij)(cos2 γij). (20)

ij =Xi Xj

F =Xi Xj
kCk2
Recalling that ˜σ8 =p˜λ8, we simplify Eq. (17) to obtain

˜σ8 ≤s n
8 −

1

8r 8kCk2

F − n2
7

.

(21)

If all the angles βij and γij are small, kCkF tends to be
larger, yielding a smaller value for δ = ˜σ8 and hence more
potential sensibility to perturbations. In the limit, we have
kCkF ≈ n, which leads to δ ≈ 0. In this case, even small
perturbations P can lead to highly degraded estimates for
the epipolar matrix. On the other hand, the bound in in-
equality (21) is at most √n/8, which is an “optimistic” up-
per bound for δ (best case scenario), leading to δ = O(√n).
Also, note that a single outlier can signiﬁcantly increase the
perturbation kPkF according to Eq. (14), so that n must be
very large to compensate for the presence of “bad” outliers.

Our analysis can be easily extended to weighted ver-
sions of the 8-PA, which is used in Iterative Reweighted
Least-Squares schemes (IRLS) [39] or in the loss func-
tion of recent deep learning approaches [46]. This in-
volves deﬁning an n×n diagonal matrix W = [wij] with
the weights for each correspondence pair and minimiz-
ing kW Aek2.
the perturbation error is
kW PkF = pPn
2k2, so that the inﬂuence
of outliers can be alleviated by choosing small weights for
bad matches.

In that case,
i=1 w2

2 − xi

iik ˜xi

3.5. Perturbation analysis of 5 DoF pose estimation

from the essential matrix

For calibrated or spherical cameras, the 5-DoF pose pa-
rameters – rotation and the direction of the translation vec-
tor – can be extracted from the essential matrix through the
SVD [14]. In particular, the direction of the translation vec-
tor is given by the least left singular vector t of E, and it is
more prone to errors in the essential matrix than the rotation
matrix, as noted in [25, 38].

Let us consider a true rank-2 essential matrix E with
kEkF = 1, and let t and ˜t denote the least left singu-
lar values of E and its estimate ˜E, respectively, and as-
sume that the direction ambiguity was solved (e.g. by using
cheirality constraints [37]). Using the notation of Eqs. (4)
and (5), the SVD of E generates a 2×2 diagonal matrix
Σ1 = 1√2
I2 containing the two equal singular values of
E, and an 1×1 null matrix Σ2 (consider an analogous no-
tation for the SVD of ˜E). The least left singular vectors
of E and ˜E are given by t = U⊥ and ˜t = ˜U⊥, respec-
tively. Note that the gap between ˜Σ1 and Σ2 is given by
δE = min{ ˜Σ1} − max{Σ2} = σ2( ˜E), so that Wedin’s

theorem gives

| sin ω| ≤

1

σ2( ˜E)kE − ˜EkF =

1

σ2( ˜E)ke − ˜ek,

(22)

where ω = ∠(˜t, t) is the angle between the actual and
the estimated translation values, and σ2( ˜E) is the second
least singular value of ˜E. For small perturbations, we ex-
pect σ2( ˜E) ≈ σ2(E) = 1/√2. More precisely, Weyl’s
bound [43] relates the qth pair of singular values σq( ˜E) and
σq(E) through

(cid:12)(cid:12)(cid:12)

σq( ˜E) − σq(E)(cid:12)(cid:12)(cid:12) ≤ kE − ˜Ek2 ≤ kE − ˜EkF ,

so that a looser version of the bound in Eq. (22) can be ex-
pressed solely based on the difference between e and ˜e:

(23)

| sin ω| ≤

√2ke − ˜ek
1 − √2ke − ˜ek

.

(24)

11761

(a) 54.4◦×37.8◦ (b) 65.5◦×46.4◦ (c) 195◦×195◦

(d) 360◦×180◦

Figure 1: Unitary feature projections in different FoVs.

4. Experimental results

4.1. Synthetic feature matching

In our experimental setup, we ﬁrst present results using
a set of synthetic 3D points projected to calibrated cam-
eras with known parameters, and add artiﬁcial noise to the
feature locations on the second view. Since the feature
are 3D unit vectors (i.e., on the unit sphere), we add von
Mises-Fisher (vMF) noise, as done in [12]. We consider the
FoVs of typical perspective cameras with 54.4◦×37.8◦ and
65.5◦×46.4◦ [27], a 195◦ ﬁsheye wide-angle camera [19]
and a full-spherical camera [2].
Noise is controlled by parameter κ in the vMF distri-
bution. Here, we select κ ∈ {500; 1, 000; 2, 000; 10, 000}
corresponding to average angular matching errors equiva-
lent to 3.21◦, 2.27◦, 1.60◦ and 0.72◦, respectively. For each
combination of FoV and noise level, we generate 1,000 ex-
periments, each one containing 100 3D points randomly
selected within a 5-10m radius (constrained to the camera
FoV), simulating a large indoor environment. The second
camera was randomly placed within a [−1, 1]3 cube with
arbitrary rotation. For the sake of illustration, the spheri-
cal projection of one set of features using the four selected
FoVs is depicted in Figure 1.

Table 1 presents the average sine error between e and
˜e for each combination, as well as the Wedin’s bound
(Eq. (12)) and Cai and Zhangs’ bound (Eq. (8))1. Both
bounds decrease as the noise level decreases and the FoV
increases, as expected. However, on average they showed to
be quite loose bounds when compared to the actual errors.
Cai and Zhangs’ bound tends to be tighter than the Wedin’s,
but it is important to recall that it is not computable on prac-
tical applications, and thus we will focus only on Wedin’s
bound hereafter in our analysis.

Although Wedin’s bound showed to be loose, it still pro-
vides useful insights into the essential matrix accuracy as
a function of the camera FoVs. Figure 2 illustrates the
averaged results for the singular gap δ, the sine error and
Wedin’s perturbation bound by using an extensive combi-
nation of the horizontal and vertical FoVs (abbreviated as
HFoV and VFoV, respectively). A total of 100 simulations

1Since these bounds produce trivial values (≥ 1) for narrow FoVs, we

only show the results for wider FoVs.

per HFoV×VFoV was performed using the same setup ex-
plained before, and Wedin’s bound was truncated in value 1.
Also, we vary the noise levels, setting κ = 500, κ = 10, 000
and κ = 1, 000, 000 which correspond to an average an-
gular error of 3.21◦, 0.72◦ and 0.071◦, respectively. The
selected range for κ encompasses the tolerances of 0.5625◦
and 2◦ for considering corresponding points as true matches
as argued in [48] and [11], respectively.

Figure 2 shows that the maximum δ occurs around the
full 360 degrees FoV (ﬁrst row), presenting a practically
stable value regardless of the tested noise level.
In fact,
the variance on the full FoV for all noise levels is around
2.8×10−5. Interestingly, the “optimal” VFoV is slightly be-
low 180◦. We believe that this happens because using the
full VFoV leads to a circular domain, which might increase
the number of neighboring features. Moreover, as expected,
when the noise level increases the sine error also increases
(second row), and Wedin’s bound behaves similarly (third
row).
It is also possible to see that for higher noise lev-
els combined with narrower FoVs, Wedin’s bound turns to
be useless because its value is even greater than the trivial
bound 1. Last but not least, we found in our experiments
that the Spearman’s correlation [34] between the the gap δ
and the diagonal FoV (DFoV) is around 0.775, 0.863 and
0.877 for κ = {500; 10, 000; 1, 000, 000}, respectively (p-
value ≪ 0.01), indicating a strong relationship.
The second part of the analysis consists of estimating
the accuracy of the 5-DoF pose extracted from the essen-
tial matrix (here, we assume the calibrated/spherical case).
Our evaluation metric for the 2-DoF translation vector is the
angular error [45] given by

εt = cos−1(t⊤˜t).

(25)

For the sake of illustration, we also show the rotation
matrix error, given as the angles between the actual matrix
R and the estimate ˜R [38], which is deﬁned as

εR = cos−1  tr(R⊤ ˜R) − 1

2

! .

(26)

Figure 3 presents the average translation and rotation er-
rors as a function of the angular matching error. The dis-
tribution of the 3D points, minimum and maximum val-
ues for κ and the FoVs are the same as in the experiment
related to Table 1. Note that the rotation error is much
smaller than the translation error not only for narrow FoVs,
as noted in [25, 38], but also for wider FoV cameras. It is
also evident that the translation error decreases as the FoV
increases, since the essential matrix is estimated more ac-
curately. For the wider FoVs (195◦×195◦ and 360◦×180◦)
we also present Wedin’s bound for the angle between the
actual and the estimated 2-DoF translation vector, as given
in Eq. (22). For narrower FoVs the bound is greater than the
trivial value, and hence not shown.

11762

κ

500
1,000
2,000
10,000

54.4◦×37.8◦

65.5◦×46.4◦

195◦×195◦

360◦×180◦

195◦×195◦

360◦×180◦

Sine error

Wedin’s bound [41]

195◦×195◦
360◦×180◦
Cai and Zhangs’ bound [3]

0.782 ± 0.230 0.778 ± 0.226 0.340 ± 0.252 0.085 ± 0.045
0.781 ± 0.221 0.756 ± 0.241 0.190 ± 0.180 0.054 ± 0.027
0.780 ± 0.223 0.756 ± 0.234 0.103 ± 0.090 0.036 ± 0.017
0.679 ± 0.248 0.563 ± 0.253 0.033 ± 0.025 0.015 ± 0.007

0.868 ± 0.088 0.607 ± 0.076
0.769 ± 0.136 0.446 ± 0.054
0.666 ± 0.165 0.326 ± 0.039
0.356 ± 0.117 0.149 ± 0.017

0.891 ± 0.073 0.671 ± 0.148
0.705 ± 0.174 0.335 ± 0.082
0.543 ± 0.205 0.177 ± 0.036
0.223 ± 0.145 0.053 ± 0.011

Table 1: Impact of the variation in FoV and noise level when computing the perturbation levels.

Figure 2: Average results for the delta value, the sine error and the Wedin’s bound (in the rows) for different noise levels (in
the columns) and FoVs. From the left to the right, κ = 500, κ = 10, 000 and κ = 1, 000, 000.

(a) 54.4◦×37.8◦

(b) 65.5◦×46.4◦

(c) 195◦×195◦

(d) 360◦×180◦

Figure 3: 5-DoF pose error for different noise levels.

To evaluate the impact of outliers, we corrupted a set of
n ∈ [10; 3, 500] noisy matchings (κ = 16, 250) with a sin-
gle outlier. The actual sine error and Wedin’s bound decay
proportionally to O(1/√n), leading to Pearson’s correla-

tions ρ = 0.9932 and ρ = 0.9973 (p-value ≪ 0.01), re-
spectively. This corroborates our ﬁndings in Section 3.4.

4.2. Real feature matching

Although the vMF noise model is suitable for the chosen
features, matching noise in real images is highly dependent
on the feature extractor and the local appearance of the im-
ages. Since we are not aware of existing datasets with wide
FoV cameras (e.g., spherical) and ground truth data w.r.t.
the essential matrix, we use realistic computer generated
scenes as done in [10, 11, 47]. We rendered non-aligned and
non-rectiﬁed pairs of spherical images using the Blender
Cycles models Urban Canyon and Indoor, made available
by [47], and the Classroom, recently used in [6]. We also
considered scene captures of the Medieval Port model along
with the 6-DoF pose ground-truth from [10]. The former
and the latter datasets are outdoors, and the other two are
indoors. All images are rendered at a 1280×640 resolution
in equirectangular format, and Figure 4 illustrates a single
spherical view from each dataset.

To obtain the required correspondences for the 8-PA, we

11763

1080150220290360HFoV (deg.)104080110150180VFoV (deg.)0.10.20.30.40.51080150220290360HFoV (deg.)104080110150180VFoV (deg.)0.10.20.30.40.51080150220290360HFoV (deg.)104080110150180VFoV (deg.)0.10.20.30.40.51080150220290360HFoV (deg.)104080110150180VFoV (deg.)0.20.40.60.81.01080150220290360HFoV (deg.)104080110150180VFoV (deg.)0.20.40.60.81.01080150220290360HFoV (deg.)104080110150180VFoV (deg.)0.20.40.60.81.01080150220290360HFoV (deg.)104080110150180VFoV (deg.)0.60.70.80.91.01080150220290360HFoV (deg.)104080110150180VFoV (deg.)0.20.30.40.50.60.70.80.91.01080150220290360HFoV (deg.)104080110150180VFoV (deg.)0.20.40.60.81.01.01.52.02.53.0Matching error (deg.)204060Pose error (deg.)TranslationRotation1.01.52.02.53.0Matching error (deg.)204060Pose error (deg.)TranslationRotation1.01.52.02.53.0Matching error (deg.)02040Pose error (deg.)TranslationRotationBound1.01.52.02.53.0Matching error (deg.)0510Pose error (deg.)TranslationRotationBound54.4◦×37.8◦ 65.5◦×46.4◦
0.815±0.698
0.849±0.687

360◦×180◦
Metric
d(E, ˜E)
0.100±0.260
εt (8-PA) 39.727±36.301 37.445±35.618 11.941±37.707 8.936±27.426
εt (NLR) 38.497±37.456 35.770±37.816 11.521±37.899 8.214±27.770
εR (8-PA) 8.383±20.913
1.684±8.952
εR (NLR) 8.276±20.852
1.453±8.705
0.033±0.017
0.257±0.157

7.629±18.385
7.235±17.511
0.038±0.022

1.923±7.802
1.500±7.022
0.150±0.087

195◦×195◦
0.134±0.287

δ

(a) Medieval Port [10]

(b) Urban Canyon [47]

Table 2: Results for synthetic imagery for different FoVs
when the number of keypoints is limited.

(c) Indoor [47]

(d) Classroom [6]

Figure 4: Datasets used for validation.

used the spherical ORB (SPHORB) [48], which is suited for
spherical images, faster than spherical SIFT (SSIFT) [4] and
with publicly available code (https://github.com/
tdsuper/SPHORB). Given the correspondence pairs, we
robustly estimate the epipolar matrix by using RANSAC,
and consider a feature pair as inlier if its symmetric pro-
jected distance [29] is smaller than 10−2. We accept a
model if it has at least 70% of inliers.

Tables 2 and 3 present the average translation and ro-
tation errors (Eqs. (25) and (26)), as well as the epipolar
error (Eq. (15)) and the δ value for the four datasets and
the four different FoVs (set as in the synthetic matching ex-
periments). For each experiment, a total of 1,000 pairs of
images was randomly selected, and the FoV was restricted
so that the narrow FoV cameras are pointing out to some
location aligned to the scene’s horizon. Besides the results
for 8-PA, we also show the pose errors after applying the
non-linear 5-DoF pose reﬁnement (NLR) based on the pro-
jected distance that is, among other three options, pointed
out as the best performing in [29].

In the experiment shown in Table 2,

the number of
matchings is restricted to be the same as the one of the
smaller FoV, so that the main observable variable on the
results is the spreading of the features. The average num-
ber of keypoints in this experiment was 132.8 ± 83.9. Note
that the results in synthetic images with real feature match-
ing corroborate the results from Section 4.1, i.e., the wider
the FoV the smaller the epipolar and 5-DoF pose errors, and
larger the value of δ.

Table 3 shows the metrics using all available matches
for each tested FoV (more matches are expected for wider
FoVs). The average number of correspondences in this
new test indeed increased with the FoV: 134.8 ± 86.0,
166.8± 115.1, 722.9± 681.3 and 1288.4± 1189.0, respec-
tively. Our results indicate that increasing the number of
(“good”) features indeed helps to improve even more the
8-PA results, especially for wider FoVs. Also, as noted

54.4◦×37.8◦ 65.5◦×46.4◦
0.777±0.675
0.818±0.667

360◦×180◦
Metric
d(E, ˜E)
0.038±0.052
εt (8-PA) 37.280±33.974 35.323±33.994 8.480±34.474 3.427±15.588
εt (NLR) 35.936±34.534 33.728±36.002 8.421±34.469 3.157±15.566
εR (8-PA) 6.053±10.479
0.2865±0.5924 0.083±0.077
εR (NLR) 6.203±12.603
0.065±0.066
0.208±0.519
0.034±0.017
0.388±0.188
0.851±0.365

5.417±7.015
5.126±7.209
0.043±0.023

195◦×195◦
0.053±0.611

δ

Table 3: Results for synthetic imagery for different FoVs
with free number of keypoints.

by [47], wider FoVs greatly improve pose and 3D estima-
tion based on non-linear bundle adjustment algorithms since
features are more likely to be visible in more than two cap-
tures of temporally aligned image sets.

5. Conclusions

We present a perturbation analysis for epipolar matrix
estimation using the well-known 8-PA by exploring singu-
lar subspace analysis. We show that the bound is inversely
proportional to the second least singular value of the ob-
servation matrix, which is strongly affected by the spatial
distribution of the matched features. In particular, the fea-
tures extracted when using narrow FoV images are spatially
concentrated, leading to larger bounds (and according to our
experiments, also larger errors in the estimate of the epipo-
lar matrix). On the other hand, cameras with wider FoV (in
the limit case spherical images) present a much better spa-
tial distribution of features, leading to smaller bounds and
smaller effective errors in the estimated matrix. This sug-
gests that expensive non-linear approaches for pose (or pose
plus 3D) reﬁnement might be relaxed or even suppressed
when using spherical cameras.

In the future, we intend to approximate tighter bounds
(as [3]) using only observable data, better explore the sin-
gular gaps of the epipolar matrices for the 2-DoF translation
vector, and extend our bounds for the rotation matrix case.

Acknowledgments

This study was ﬁnanced in part by the Coordenac¸ ˜ao
de Aperfeic¸oamento de Pessoal de N´ıvel Superior - Brasil
(CAPES) - Finance Code 001 - and the Conselho Nacional
de Desenvolvimento Cient´ıﬁco e Tecnol´ogico (CNPq). The
authors also thank Christiano Gava, Didier Stricker and col-
leagues for giving access to the Medieval Port dataset.

11764

References

[1] Sameer Agarwal, Hon-Leung Lee, Bernd Sturmfels, and
Rekha R Thomas. On the existence of epipolar matrices.
International Journal of Computer Vision, 121(3):403–415,
2017.

[2] Torii Akihiko, Imiya Atsushi, and Naoya Ohnishi. Two-and
three-view geometry for spherical cameras. Proc. of the Sixth
Workshop on Omnidirectional Vision, Camera Networks and
Non- classical Cameras, 105:29–34, 2005.

[3] T Tony Cai, Anru Zhang, et al. Rate-optimal perturba-
tion bounds for singular subspaces with applications to high-
dimensional statistics. The Annals of Statistics, 46(1):60–89,
2018.

[4] Javier Cruz-Mota, Iva Bogdanova, Benoˆıt Paquier, Michel
Bierlaire, and Jean Philippe Thiran. Scale invariant feature
transform on the sphere: Theory and applications. Interna-
tional Journal of Computer Vision, 98(2):217–241, 2012.

[5] Gabriella Csurka, Cyril Zeller, Zhengyou Zhang, and
Olivier D Faugeras. Characterizing the uncertainty of the
fundamental matrix. Computer vision and image under-
standing, 68(1):18–36, 1997.

[6] Thiago L.T. da Silveira, Lorenzo P. Dalaqua, and Claudio R.
Jung.
Indoor Depth Estimation from Single Spherical Im-
ages. In 2018 25th IEEE International Conference on Image
Processing (ICIP), pages 2935–2939. IEEE, oct 2018.

[7] Thiago L.T. da Silveira and Claudio R. Jung. Evaluation of
Keypoint Extraction and Matching for Pose Estimation Us-
ing Pairs of Spherical Images. 2017 30th SIBGRAPI Confer-
ence on Graphics, Patterns and Images (SIBGRAPI), pages
374–381, 2017.

[8] Zlatko Drmac. On principal angles between subspaces of
euclidean space. SIAM Journal on Matrix Analysis and Ap-
plications, 22(1):173–194, 2000.

[9] Martin A. Fischler and Robert C. Bolles. Random sample
consensus: A paradigm for model ﬁtting with applications to
image analysis and automated cartography. Commun. ACM,
24(6):381–395, June 1981.

[10] Christiano Couto Gava, Didier Stricker, and Soichiro Yokota.
Dense Scene Reconstruction from Spherical Light Fields. In
2018 25th IEEE International Conference on Image Process-
ing (ICIP), pages 4178–4182. IEEE, oct 2018.

[11] Hao Guan and William A P Smith. BRISKS: Binary Features
for Spherical Images on a Geodesic Grid. In Conference on
Computer Vision and Pattern Recognition, 2017.

[12] Hao Guan and William A. P. Smith. Structure-From-Motion
in Spherical Video Using the von Mises-Fisher Distribution.
IEEE Transactions on Image Processing, 26(2):711–723, feb
2017.

[13] Simon James Hadﬁeld, Karel Lebeda, and Richard Bowden.
HARD-PnP: PnP Optimization Using a Hybrid Approximate
Representation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 8828(c):1–1, 2018.

[14] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision. Cambridge university press,
2003.

[15] Richard I Hartley. In defence of the 8-point algorithm. In
Computer Vision, 1995. Proceedings., Fifth International
Conference on, pages 1064–1070. IEEE, 1995.

[16] Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa.
Fast and robust estimation for unit-norm constrained linear
ﬁtting problems.
In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 8147–8155,
June 2018.

[17] Jingwei Huang, Zhili Chen, Duygu Ceylan, and Hailin Jin. 6-
DOF VR Videos with a Single 360-Camera. In Proceedings
of the IEEE Virtual Reality. IEEE, 2017.

[18] John Lim, Nick Barnes, and Hongdong Li. Estimating rel-
ative camera motion from the antipodal-epipolar constraint.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 32(10):1907–1914, 2010.

[19] I-chan Lo, Kuang-tsu Shih, and Homer H Chen.

Image
Stitching for Dual Fisheye Cameras. 2018 25th IEEE In-
ternational Conference on Image Processing (ICIP), pages
3164–3168, 2018.

[20] H Christopher Longuet-Higgins. A computer algorithm
for reconstructing a scene from two projections. Nature,
293(5828):133, 1981.

[21] Quan-Tuan Luong and Olivier D Faugeras. The fundamental
matrix: Theory, algorithms, and stability analysis. Interna-
tional Journal of Computer Vision, 17(1):43–75, jan 1996.

[22] Elmar Mair, Michael Suppa, and Darius Burschka. Er-
ror propagation in monocular navigation for zinf compared
to eightpoint algorithm.
In Intelligent Robots and Systems
(IROS), 2013 IEEE/RSJ International Conference on, pages
4220–4227. IEEE, 2013.

[23] Jorma Kaarlo Merikoski, Humberto Sarria, and Pablo
Tarazaga. Bounds for singular values using traces. Linear
Algebra and its Applications, 210:227–254, 1994.

[24] Matthias M¨uhlich and Rudolf Mester. The role of total least
In European Conference on

squares in motion analysis.
Computer Vision, pages 305–321. Springer, 1998.

[25] David Nist´er. An efﬁcient solution to the ﬁve-point relative
IEEE transactions on pattern analysis and

pose problem.
machine intelligence, 26(6):756–770, 2004.

[26] Sean O’Rourke, Van Vu, and Ke Wang. Random perturba-
tion of low rank matrices: Improving classical bounds. arXiv
preprint arXiv:1311.2657, 2013.

[27] Thomas Wesley Osborne, Todor Georgiev Georgiev, and
Sergiu Radu Goma. Wide ﬁeld of view array camera for
hemispheric and spherical imaging, 2014. Qualcomm Inc.
US9819863B2.

[28] Onur Ozyesil, Vladislav Voroninski, Ronen Basri, and Amit
Singer. A Survey of Structure from Motion. pages 305–364,
2017.

[29] Alain Pagani and Didier Stricker. Structure from Motion us-
ing full spherical panoramic cameras. In 2011 IEEE Inter-
national Conference on Computer Vision Workshops (ICCV
Workshops), pages 375–382. IEEE, nov 2011.

[30] Sarthak Pathak, Alessandro Moro, Hiromitsu Fujii, Atsushi
Yamashita, and Hajime Asama. 3D reconstruction of struc-
tures using spherical cameras with small motion.
In 2016
16th International Conference on Control, Automation and

11765

Systems (ICCAS), number Iccas, pages 117–122. IEEE, oct
2016.

Notes in Artiﬁcial Intelligence and Lecture Notes in Bioin-
formatics), volume 8689 LNCS, pages 111–126. 2014.

[46] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit,
Mathieu Salzmann, and Pascal Fua. Learning to ﬁnd good
correspondences.
In Proceedings of the 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 2666–2674, 2018.

[47] Zichao Zhang, Henri Rebecq, Christian Forster, and Davide
Scaramuzza. Beneﬁt of large ﬁeld-of-view cameras for vi-
sual odometry. Proceedings - IEEE International Conference
on Robotics and Automation, 2016-June:801–808, 2016.

[48] Qiang Zhao, Wei Feng, Liang Wan, and Jiawan Zhang.
SPHORB: A Fast and Robust Binary Feature on the Sphere.
International Journal of Computer Vision, 113(2):143–159,
2014.

[49] Yinqiang Zheng, Shigeki Sugimoto, and Masatoshi Oku-
tomi. A practical rank-constrained eight-point algorithm for
fundamental matrix estimation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1546–1553, 2013.

[31] Sarthak Pathak, Alessandro Moro, Hiromitsu Fujii, Atsushi
Yamashita, and Hajime Asama. Distortion-Robust Spheri-
cal Camera Motion Estimation via Dense Optical Flow. In
2018 25th IEEE International Conference on Image Process-
ing (ICIP), pages 3358–3362. IEEE, oct 2018.

[32] Sarthak Pathak, Alessandro Moro, Atsushi Yamashita, and
Hajime Asama. Optical Flow-Based Epipolar Estimation
of Spherical Image Pairs for 3D Reconstruction.
SICE
Journal of Control, Measurement, and System Integration,
10(5):476–485, 2017.

[33] Rahul Raguram, Jan-Michael Frahm, and Marc Pollefeys. A
comparative analysis of ransac techniques leading to adap-
tive real-time random sample consensus. In European Con-
ference on Computer Vision, pages 500–513. Springer, 2008.
[34] C. Spearman. The proof and measurement of association
between two things. The American Journal of Psychology,
15(1):72–101, 1904.

[35] Gilbert W Stewart. Matrix perturbation theory. 1990.
[36] Fr´ed´eric Sur, Nicolas Noury, and Marie-Odile Berger. Com-
puting the uncertainty of the 8 point algorithm for funda-
mental matrix estimation.
In 19th British Machine Vision
Conference-BMVC 2008, page 10, 2008.

[37] Richard Szeliski. Computer Vision: Algorithms and Applica-
tions. Springer-Verlag, Berlin, Heidelberg, 1st edition, 2010.
[38] Tina Yu Tian, Carlo Tomasi, and David J Heeger. Com-
parison of approaches to egomotion computation. In Com-
puter Vision and Pattern Recognition, 1996. Proceedings
CVPR’96, 1996 IEEE Computer Society Conference on,
pages 315–320. IEEE, 1996.

[39] Philip HS Torr and David W Murray. The development
and comparison of robust methods for estimating the fun-
damental matrix. International journal of computer vision,
24(3):271–300, 1997.

[40] Rongrong Wang. Singular vector perturbation under gaus-
sian noise. SIAM Journal on Matrix Analysis and Applica-
tions, 36(1):158–177, 2015.

[41] Per- ˚Ake Wedin. Perturbation bounds in connection with sin-
gular value decomposition. BIT Numerical Mathematics,
12(1):99–111, 1972.

[42] Juyang Weng, Thomas S. Huang, and Narendra Ahuja. Mo-
tion and structure from two perspective views: Algorithms,
error analysis, and error estimation.
IEEE Transactions
on Pattern Analysis & Machine Intelligence, (5):451–476,
1989.

[43] Hermann Weyl. Das asymptotische verteilungsgesetz der
eigenwerte linearer partieller differentialgleichungen (mit
einer anwendung auf die theorie der hohlraumstrahlung).
Mathematische Annalen, 71(4):441–479, 1912.

[44] FC Wu, ZY Hu, and FQ Duan. 8-point algorithm revisited:
Factorized 8-point algorithm.
In Computer Vision, 2005.
ICCV 2005. Tenth IEEE International Conference on, vol-
ume 1, pages 488–494. IEEE, 2005.

[45] Jiaolong Yang, Hongdong Li, and Yunde Jia. Optimal Essen-
tial Matrix Estimation via Inlier-Set Maximization. In Lec-
ture Notes in Computer Science (including subseries Lecture

11766

