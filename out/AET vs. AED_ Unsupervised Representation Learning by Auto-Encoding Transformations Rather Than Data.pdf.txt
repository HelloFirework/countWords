AET vs. AED: Unsupervised Representation Learning by Auto-Encoding

Transformations rather than Data

Liheng Zhang 1

,∗, Guo-Jun Qi 1

2

,

,†, Liqiang Wang3 , Jiebo Luo4

1Laboratory for MAchine Perception and LEarning (MAPLE)

http://maple-lab.net/

2Huawei Cloud, 3University of Central Florida, 4University of Rochester

guojun.qi@huawei.com

http://maple-lab.net/projects/AET.htm

Abstract

The success of deep neural networks often relies on a
large amount of labeled examples, which can be difﬁcult to
obtain in many real scenarios. To address this challenge,
unsupervised methods are strongly preferred for training
neural networks without using any labeled data.
In this
paper, we present a novel paradigm of unsupervised repre-
sentation learning by Auto-Encoding Transformation (AET)
in contrast to the conventional Auto-Encoding Data (AED)
approach. Given a randomly sampled transformation, AET
seeks to predict it merely from the encoded features as ac-
curately as possible at the output end. The idea is the fol-
lowing: as long as the unsupervised features successful-
ly encode the essential information about the visual struc-
tures of original and transformed images, the transforma-
tion can be well predicted. We will show that this AET
paradigm allows us to instantiate a large variety of trans-
formations, from parameterized, to non-parameterized and
GAN-induced ones. Our experiments show that AET great-
ly improves over existing unsupervised approaches, set-
ting new state-of-the-art performances being greatly clos-
er to the upper bounds by their fully supervised counter-
parts on CIFAR-10, ImageNet and Places datasets. Our
source codes are available at https://github.com/
maple-research-lab/AET.

1. Introduction

The great success in applying deep neural networks to
image classiﬁcation, object detection and semantic segmen-
tation has inspired us to explore their full ability in a wide
variety of computer vision tasks. Unfortunately, training

∗The work was done while L. Zhang was interning at Huawei Cloud.
†Corresponding author: G.-J. Qi. Email: guojunq@gmail.com

(a) Auto-Encoding Data (AED)

(b) Auto-Encoding Transformation (AET)

Figure 1: An illustrative comparison between AED and
AET, where AET attempts to estimate the input transfor-
mation rather than the data at the output end. This forces
the encoder network E to extract the features that contain
the sufﬁcient information about visual structures to decode
the input transformation.

deep neural networks often requires a large amount of la-
beled data to learn adequate feature representations for visu-
al understanding tasks. This greatly limits the applicability
of deep neural networks when only a limited amount of la-
beled data is available for training the networks. Therefore,
there has been an increasing interest in literature to learn
deep feature representations in an unsupervised fashion to
solve emerging visual understanding tasks with insufﬁcient
labeled data.

Among the efforts on unsupervised learning methods,
the most representative ones are Auto-Encoders and Gen-
erative Adversarial Nets (GANs) [11]. The former trains an
encoder network to output feature representations with suf-

12547

ﬁcient information to reconstruct input images by a paired
decoder. Many variants of auto-encoders [12, 15] have been
proposed in literature but all of them stick to essentially the
same idea of reconstructing input data at the output end, and
thus we classify them into the Auto-Encoding Data (AED)
paradigm illustrated in Figure 1(a).

On the other hand, GANs learn the feature representa-
tion in an unsupervised fashion by generating images from
input noises with a pair of adversarially trained generator
and discriminator. The input noises into the generator can
be viewed as the feature representations of its output, s-
ince they contain necessary information to produce the cor-
responding images through the generator. To obtain the
“noise” feature representation for each image, an encoder
can be trained to form an auto-encoder architecture with the
generator as the decoder. In this way, given an input image,
the encoder can directly output its noise representation pro-
ducing the original image through the generator [6, 8]. This
combines the strength of both AED and GAN models. Re-
cently, these models become a popular alternative to auto-
encoders in many unsupervised and semi-supervised tasks,
as they can generate the distribution of photo-realistic im-
ages as a whole so that better feature representations can be
derived from the trained generator.

Besides auto-encoders and GANs, various paradigms of
self-supervised learning methods exist without using man-
ually labeled data. These methods create self-supervised
objectives to train the networks. For example, Doersch et
al. [5] propose to train neural networks by predicting the
relative positions of two randomly sampled patches. Mehdi
and Favaro [18] report to train a convolutional neural net-
work by solving Jigsaw puzzles.
Image colorization has
also been used as a self-supervised task to train convolu-
tional networks in literature [30, 17]. Instead, Dosovitskiy
et al. [7] train neural networks by discriminating among
a set of surrogate classes artiﬁcially formed by applying
various transformations to image patches, while Gidaris et
al. [10] attempt to classify image rotations of four discrete
angles. These approaches explore supervisory signals at
various levels of visual structures to train networks without
manually labeling data. Unsupervised features have also
been extracted from videos by estimating the self-motion of
moving objects between consecutive frames [1].

In contrast, we are motivated to learn unsupervised
feature representations by Auto-Encoding Transformation-
s (AET) rather than the data themselves. Speciﬁcally, by
sampling some operators to transform images, we seek to
train auto-encoders that can directly reconstruct these op-
erators from the learned feature representations between o-
riginal and transformed images. We believe as long as the
trained features are sufﬁciently informative, we can decode
the transformations from the features that well encode vi-
sual structures of images. As compared with the conven-

tional paradigm of Auto-Encoding Data (AED) in Figure 1,
AET focuses on exploring dynamics of feature represen-
tations under different transformations, thereby revealing
not only static visual structures but also how they would
change by applying different transformations. Moreover,
there is no restriction on the form of transformations ap-
plicable in the proposed AET framework. This allows us
to ﬂexibly explore a large variety of transformations, rang-
ing from simple image warping to any parametric and non-
parametric transformations. We will demonstrate the AET
representations outperform the other unsupervised model-
s in experiments, greatly pushing the state-of-the-art unsu-
pervised method much closer to the upper bound set by the
fully supervised counterparts.

The remainder of the paper is organized as follows. We
ﬁrst review the related work in Section 2, and then formally
present the proposed AET model in Section 3. We conduct
experiments in Section 4 to compare its performances with
the other state-of-the-art unsupervised models. Finally, we
summarize conclusions in Section 5.

2. Related Work

Auto-Encoders. The use of auto-encoder architecture in
learning representations in an unsupervised fashion has
been extensively studied in literature [13, 14, 27]. These
existing auto-encoders are all based on reconstructing the
input data at the output end through a pair of encoder and
decoder. The encoder acts as an extractor of features usu-
ally compactly representing the most essential information
about input data, while a decoder is jointly trained to recov-
er the input data upon the extracted features. The idea is that
a good feature representation should contain sufﬁcient in-
formation to reconstruct the input data. A wide spectrum of
auto-encoders have been proposed following this paradigm
of auto-encoding data (AED). For example, the variation-
al auto-encoder [15] explicitly introduces probabilistic as-
sumption about the distribution of features extracted from
data. Denoising auto-encoder [27] aims to learn more ro-
bust representation by reconstructing original inputs from
noise-corrupted inputs. Contrastive Auto-Encoder [26] pe-
nalizes abrupt changes of representations around given da-
ta, thus encouraging representation invariance to small per-
turbation on input data. Zhang et al. [29] present a cross-
channel auto-encoder by reconstructing a subset of data
channels from another subset with the cross-channel fea-
tures being concatenated as data representation. Hinton et
al. [12] propose a transforming auto-encoder in the context
of capsule nets, which is still trained in the AED fashion by
minimizing the discrepancy between the reconstructed and
target images. Conceptually, this differs from the proposed
AET that aims to learn unsupervised features by directly
minimizing the input and output transformations in an end-
to-end auto-encoder architecture.

2548

Generative Adversarial Nets. Besides the auto-encoders,
Generative Adversarial Nets (GANs) become popular for
training network representations of data in an unsupervised
fashion. Unlike the auto-encoders, GANs attempt to direct-
ly generate data from noises drawn from a random distri-
bution. By viewing the sampled noises as the coordinates
over the manifold of real data, one can use them as the fea-
tures to represent data. For this purpose, one usually needs
to train a data encoder to ﬁnd the noise that can generate
the input images through the GAN generator. This can be
implemented by jointly training a pair of mutually inverse
generator and encoder [6, 8]. A prominent characteristic of
GANs that make them different from auto-encoders is they
do not rely on one-to-one reconstruction of input data at
the output end. Instead, they focus on discovering and gen-
erating the entire distribution of data over the underlying
manifold. Recent progress has shown the promising gener-
alization ability of regularized GANs in generating unseen
data based on the Lipschitz assumption on the real data dis-
tribution [23, 2], and this shows great potential of GANs in
providing expressive representation of images [6, 8, 9].
Self-Supervised Representation Learning.
In addition
to auto-encoders and GANs, other unsupervised learning
methods explore various self-supervised signals to train
deep neural networks. These self-supervised signals can
be directly derived from data themselves without having
to be manually labeled. For example, Doersch et al. [5]
use the relative positions of two randomly sampled patch-
es from an image as self-supervised information to train the
model. Mehdi and Favaro [18] propose to train a convolu-
tional neural network by solving Jigsaw puzzles. Noroozi
et al. [19] learn counting features that satisfy equivalence
relations between downsampled and tiled images, and Gi-
daris et al. [10] train neural networks by classifying image
rotations in a discrete set. Dosovitskiy et al. [7] train C-
NNs by classifying a set of surrogate classes, each of which
is formed by applying various transformations to an indi-
vidual image. However, the resultant features could over-
discriminate visually similar images as they always belong
to different surrogate classes, and the training cost is much
more expensive as every training example results in an in-
dividual surrogate class. The idea of self-supervised learn-
ing has been employed to train feature representations for
videos through the self-motion of moving objects [1]. In
summary, this type of approaches train networks using self-
supervised objectives instead of manually labeled data.

3. AET: The Proposed Approach

We elaborate on the proposed paradigm of auto-
encoding transformations (AET) in this section. First, we
will formally present the formulation of AET in Section 3.1.
Then we will instantiate AET with different genres of trans-
formations in Section 3.2.

3.1. The Formulation

Suppose that we sample a transformation t from a dis-
tribution T (e.g., image warping, projective transformation
and even GAN-induced transformation, c.f. Section 3.2 for
more details). It is applied to an image x drawn from a data
distribution X , resulting in the transformed version t(x) of
x.

Our goal is to learn an encoder E : x 7→ E(x),
which aims to extract the representation E(x) for a sam-
ple x. Meanwhile, we wish to learn a decoder D :
[E(x), E(t(x))] 7→ ˆt, which gives an estimate ˆt of input
transformation by decoding from the encoded representa-
tions of original and transformed images. Since the predic-
tion on the input transformation is made through the encod-
ed features rather than the original and transformed images,
it forces the model to extract expressive features as a proxy
to represent images.

The learning problem of Auto-Encoding Transforma-
tions (AET) now boils down to jointly training the feature
encoder E and the transformation decoder D. To this end,
let us choose a loss function ℓ(t, ˆt) that quantiﬁes the dif-
ference between a transformation t and its estimate ˆt. Then
the AET can be solved by minimizing this loss as

min
E,D

E

t∼T ,x∼X

ℓ(t, ˆt)

where the transformation estimate ˆt is a function of the en-
coder E and the decoder D such that

ˆt = D [E(x), E(t(x))] ,

and the expectation E is taken over the sampled transforma-
tions and data. Like in training other deep neural networks,
the network parameters of E and D are jointly updated over
mini-batches by back-propagating the gradient of the loss ℓ.

3.2. The AET Family

A large variety of transformations can be easily incorpo-
rated into the AET formulation. Here we discuss three gen-
res, parameterized, GAN-induced and non-parameterized
transformations, to instantiate the AET models.
Parameterized Transformations. Suppose that we have a
family of transformations T = {tθ|θ ∼ Θ} with their pa-
rameters θ sampled from a distribution Θ. This equivalent-
ly deﬁnes a distribution of parameterized transformations,
where each transformation can be represented by its param-
eter and the loss ℓ(tθ, tˆθ) between transformations can be
captured by the difference in terms of their parameters. For
example, many transformations such as afﬁne and projec-
tive transformations can be represented by a parameterized
matrix M (θ) ∈ R3×3 between homogeneous coordinates
of images before and after transformations. Such a matrix
captures the change of geometric structures caused by a giv-
en transformation, and thus it is straightforward to deﬁne

2549

1
2

kM (θ) − M (ˆθ)k2

2 to model the difference

ℓ(tθ, t ˆθ) =
between the target and the estimated transformations.
In
experiments, we will compare different instances of param-
eterized transformations in this category and demonstrate
they can yield competitive performances on training AET.
GAN-Induced Transformations. One can choose other
forms of transformations without explicit geometric impli-
cations like the afﬁne and the projective transformations.
Let us consider a GAN generator that transforms an input
over the manifold of real images. For example, in [24], a
local generator G(x, z) is learned with a sampled random
noise z that parameterizes the underlying transformation
around a given image x. This effectively deﬁnes a GAN-
induced transformation such that tz(x) = G(x, z) with
the transformation parameter z. One can directly choose

1
2

kz − ˆzk2

the loss ℓ(tz, tˆz) =
2 between noise parameter-
s, and train a network D to decode the parameter ˆz from
the features E(x) and E(tz(x)) by the encoder network
E. Compared with the classical transformations that change
low-level appearance and geometric structures in images,
the GAN-induced transformations can change high-level se-
mantics in images. For example, the GANs have demon-
strated their ability of manipulating attributes such as ages,
hairs, genders and wearing glasses in facial images as well
as changing the furniture layout in bedroom images [25].
This enables AET to explore a richer family of transforma-
tions to learn more expressive representations.
Non-Parametric Transformations. Even if a transforma-
tion t ∈ T is hard to parameterize, we can still deﬁne the
loss ℓ(t, ˆt) by measuring the average difference between the
transformations of randomly sampled images. Formally,

ℓ(t, ˆt) = E

x∼X

dist(t(x), ˆt(x))

(1)

where dist(·, ·) is a distance between two transformed im-
ages, and the expectation is taken over random samples. For
an input non-parametric transformation t, we also need a
decoder network that outputs a transformation ˆt to estimate
the input transformation. This can be done by choosing
a parameterized transformation tˆθ as ˆt to estimate t. Al-
though the non-parametric t may not fall in the space of pa-
rameterized transformations, such an approximation should
be enough for unsupervised learning since our ultimate goal
is not to obtain an accurate estimate of input transformation;
instead, we aim at learning a good feature representation to
give us the best estimate that can be achieved in the param-
eterized transformation space.

Note that parameterized transformations can also be
plugged into Eq. (1) to train the corresponding AET by min-
imizing this loss function. However, in experiments, we
ﬁnd the performance is not as good as the AET trained with
the parameter-based loss. This is probably caused by the

Figure 2: An illustration of the network architectures for
training and evaluating AET on the CIFAR-10 dataset.

fact that the loss (1) cannot accurately reﬂect the actual dif-
ference between transformations unless a sufﬁciently large
number of images are sampled. Thus, we suggest using the
parameter-based loss for the AET with parameterized trans-
formations.

We have shown that a wide spectrum of transformations
can be adopted in training AET. In this paper, we will focus
on the parameterized transformations as they do not involve
training extra models like GAN-induced transformations, or
require choosing auxiliary transformations to approximate
non-parametric forms. This allows us to make a straightfor-
ward and fair comparison with the unsupervised methods
in literature as shown in the experiments. Moreover, the
GAN-induced transformations greatly rely on the quality
of transformed images, but existing GAN models are stil-
l unable to generate high-quality images with ﬁne-grained
details at a high resolution. Thus, we leave it in future to s-
tudy the GAN-induced and non-parametric transformations
for training the AET representations.

4. Experiments

In this section, we evaluate the proposed AET model on
the CIFAR-10, ImageNet and Places datasets by comparing
it against different unsupervised methods. Unsupervised
learning is usually evaluated indirectly based on the clas-
siﬁcation performance by using the learned representations.
For the sake of fair comparison, we follow the test protocols
widely adopted in literature.

4.1. CIFAR 10 Experiments

First, we evaluate the AET model on the CIFAR-10
dataset. We consider two different transformations – afﬁne
and projective transformations – to train AET, and name
the resultant models AET-afﬁne and AET-project for brevi-
ty, respectively.

2550

4.1.1 Architecture and Implementation Details

To make a fair and direct comparison with existing unsu-
pervised models, we adopt the Network-In-Network (NIN)
architecture that has shown competitive performance previ-
ously on the CIFAR-10 dataset for the unsupervised learn-
ing task [10]. As illustrated in the top of Figure 2, the NIN
consists of four convolutional blocks, each of which con-
tains three convolutional layers. AET has two NIN branch-
es, each taking the original and the transformed images as
its input, respectively. The output features of the forth block
of two branches are concatenated and average-pooled to for-
m a 384-d feature vector. Then an output layer follows to
predict the parameters of input transformation. The two
branches share the same network weights, and are used as
the encoder network producing the feature representations
for input images.

The AET networks are trained by SGD with a batch size
of 512 original images and their transformed counterparts.
Momentum and weight decay are set to 0.9 and 5 × 10−4.
The learning rate is initialized to 0.1 and scheduled to drop
by a factor of 5 after 240, 480, 640, 800 and 1, 000 epochs.
The model is trained for 1, 500 epochs in total. For AET-
afﬁne, the afﬁne transformation is a composition of a ran-
dom rotation with [−180◦, 180◦], a random translation by
±0.2 of image height and width in both vertical and hori-
zontal directions, and a random scaling factor of [0.7, 1.3],
along with a random shearing of [−30◦, 30◦] degree. For
the AET-projective, the projective transformation is formed
by randomly translating four corners of an image in both
horizontal and vertical directions by ±0.125 of its height
and width, after it is randomly scaled by [0.8, 1.2] and ro-
tated by 0◦, 90◦, 180◦, or 270◦. We compare the results for
both models below, and demonstrate both outperform the
other existing models and AET-project performs better than
AET-afﬁne.

4.1.2 Evaluation Protocol

To evaluate the quality of the representation by an unsuper-
vised model, a classiﬁer is usually trained upon the learned
features. Speciﬁcally, in our experiments on CIFAR-10, we
follow the existing evaluation protocols [21, 7, 25, 20, 10]
by building a classiﬁer on top of the second convolutional
block. See the bottom of Figure 2, where the ﬁrst two blocks
are frozen while the classiﬁer on top of them is trained with
labeled examples.

We evaluate the classiﬁcation results by using the AET
features with both model-based and model-free classiﬁers.
For the model-based classiﬁer, we train a non-linear classi-
ﬁer with three Fully-Connected (FC) layers – each of the t-
wo hidden layers has 200 neurons with batch-normalization
and ReLU activations, and the output layer is a soft-max
layer with ten neurons each for an image class. Alternative-
ly, we also test a convolutional classiﬁer upon the unsuper-

Table 1: Comparison between unsupervised feature learn-
ing methods on CIFAR-10. The fully supervised NIN and
the random Init. + conv have the same three-block NIN ar-
chitecture, but the ﬁrst is fully supervised while the second
is trained on top of the ﬁrst two blocks that are randomly
initialized and stay frozen during training.

Method

Error rate

Supervised NIN (Lower Bound)
Random Init. + conv (Upper Bound)

Roto-Scat + SVM [21]
ExamplarCNN [7]
DCGAN [25]
Scattering [20]
RotNet + FC [10]
RotNet + conv [10]

(Ours) AET-afﬁne + FC
(Ours) AET-afﬁne + conv
(Ours) AET-project + FC
(Ours) AET-project + conv

7.20
27.50

17.7
15.7
17.2
15.3
10.94
8.84

9.77
8.05
9.41
7.82

Figure 3: The comparison of the KNN error rates by differ-
ent models with varying numbers K of nearest neighbors
on CIFAR-10.

vised features by adding a third NIN block whose output
feature map is averaged pooled and connected to a linear
soft-max classiﬁer.

Moreover, we also test the model-free KNN classiﬁer
based on the averaged-pooled output features from the sec-
ond convolutional block. The KNN classiﬁer has an advan-
tage without need to train a model with labeled examples.
This makes a more direct evaluation on the quality of unsu-
pervised feature representation at the evaluation stage.

4.1.3 Results
In Table 1, we compare the AET models with both fully
supervised and unsupervised methods on CIFAR-10. First,
we note that the unsupervised AET-project with the con-
volutional classiﬁer almost achieves the same error rate as
its fully supervised NIN counterpart with four convolution-
al blocks (7.82% vs. 7.2%). This is a remarkable result
demonstrating AET is capable of training unsupervised fea-

2551

Table 2: Comparison of RotNet vs. AETs on CIFAR-10 with different classiﬁers on top of learned representations for
evaluation. The RotNet is chosen as the baseline since it has the exactly same architecture for the unsupervised training.
Here n-FC denotes a n-layer fully connected (FC) classiﬁer, and the KNN is obtained with K = 10 nearest neighbors. The
numbers in parentheses are the relative reduction in error rates w.r.t. the RotNet baseline.

RotNet baseline [10]

KNN

24.97

1-FC

18.21

2-FC

11.34

3-FC

10.94

conv

8.84

AET-afﬁne
AET-project

23.07 (↓7.6%)
22.39 (↓ 10.3%)

17.16 (↓ 5.8%)
16.65 (↓ 8.6%)

9.77 (↓ 13.8%)
9.41 (↓ 17.0%)

10.16 (↓ 7.1%)
9.92 (↓ 9.3%)

8.05(↓ 8.9%)
7.82(↓ 11.5%)

tures with a much narrower gap of performance to its super-
vised counterpart on CIFAR-10.

Moreover, the AETs outperform the other unsupervised
methods in Table 1. For example, ExamplarCNN also ap-
plies various transformations to images, including rotation-
s, translations, scaling and even more such as manipulating
contrasts and colors. Then it trains unsupervised CNNs by
classifying the resultant surrogate classes each containing
all transformed versions of an individual images. Compared
with ExamplarCNN [7], AET still has a signiﬁcant lead in
error rate, implying it can explore the image transformations
more effectively in training unsupervised networks.

It is worth pointing out on CIFAR-10, the other reported
methods [21, 7, 25, 20, 10] are usually based on different
unsupervised networks and supervised classiﬁers for eval-
uation, making it difﬁcult to make a direct comparison be-
tween them. The results still suggest that the state-of-the-art
performances can be reached by AETs, as their error rates
are very close to the pre-assumptive lower bound set by the
fully supervised counterpart.

Indeed, one can choose the RotNet in Table 1 as the base-
line for comparison as it is trained with the same network
and classiﬁer as the AETs. Thus we can make a fair com-
parison directly. From the results, AETs successfully beat
the RotNet with both fully connected (FC) and convolution-
al classiﬁers on top of the learned representations. We also
compare AETs with this baseline when they are trained with
the KNN classiﬁer and varying FC layers in Table 2. The
results show that AET-project can consistently achieve the
smallest errors no matter which classiﬁers are used. In Fig-
ure 3, we also compare the KNN results with varying num-
ber of nearest neighbors. Again, AET-project performs the
best without involving any labeled examples. The model-
free KNN results suggest the AET model has an advantage
when no labels are available in training classiﬁers upon the
unsupervised features.

For the following ImageNet experiments, many existing
methods have been compared in literature with the same
unsupervised AlexNet architecture as well as the classiﬁers
upon it for the evaluation. We will make a fair comparison
directly, and the results show that AET still greatly outper-
forms the other unsupervised methods.

Table 3: Top-1 accuracy with non-linear layers on Ima-
geNet. AlexNet is used as backbone to train the unsu-
pervised models. After unsupervised features are learned,
nonlinear classiﬁers are trained on top of Conv4 and Con-
v5 layers with labeled examples to compare their perfor-
mances. We also compare with the fully supervised mod-
els and random models that give upper and lower bounded
performances. For a fair comparison, only a single crop is
applied in AET and no dropout or local response normal-
ization is applied during the testing.

Method

Conv4 Conv5

ImageNet Labels [3](Upper Bound)
Random [19] (Lower Bound)

Tracking [28]
Context [5]
Colorization [30]
Jigsaw Puzzles [18]
BiGAN [6]
NAT [3]
DeepCluster [4]
RotNet [10]

(Ours) AET-project

59.7
27.1

38.8
45.6
40.7
45.3
41.9

-
-

50.0

53.2

59.7
12.0

29.8
30.4
35.2
34.6
32.2
36.0
44.0
43.8

47.0

4.2. ImageNet Experiments

We further evaluate the performance by AET on the Im-
ageNet dataset. The AlexNet is used as the backbone to
learn the unsupervised features. As shown by the results on
CIFAR-10, the projective transformation has better perfor-
mance on training the AET model, and thus we report the
AET-project results here.
Architectures and Training Details.
Two AlexNet
branches with shared parameters are created with original
and transformed images as inputs respectively to train un-
supervised AET-project. The 4, 096-d output features from
the second last fully connected layer in two branches are
concatenated and fed into the output layer producing eight
projective transformation parameters. We still use SGD to
train the network, with a batch size of 768 images and their
corresponding transformed version, a momentum of 0.9, a
weight decay of 5 × 10−4. The initial learning rate is set to

2552

Table 4: Top-1 accuracy with linear layers on ImageNet. AlexNet is used as backbone to train the unsupervised models under
comparison. A 1, 000-way linear classiﬁer is trained upon various convolutional layers of feature maps that are spatially
resized to have about 9, 000 elements. Fully supervised and random models are also reported to show the upper and the lower
bounds of unsupervised model performances. Only a single crop is used and no dropout or local response normalization is
used during testing for the AET, except the models denoted with * where ten crops are applied to compare results.

Method

Conv1 Conv2

Conv3 Conv4 Conv5

ImageNet Labels (Upper Bound) [10]
Random (Lower Bound)[10]
Random rescaled [16](Lower Bound)

Context [5]
Context Encoders [22]
Colorization[30]
Jigsaw Puzzles [18]
BiGAN [6]
Split-Brain [29]
Counting [19]
RotNet [10]

(Ours) AET-project

DeepCluster* [4]

(Ours) AET-project*

19.3
11.6
17.5

16.2
14.1
12.5
18.2
17.7
17.7
18.0
18.8

19.2

13.4

19.3

36.3
17.1
23.0

23.3
20.7
24.5
28.8
24.5
29.3
30.6
31.7

32.8

32.3

35.4

44.2
16.9
24.5

30.2
21.0
30.4
34.0
31.0
35.4
34.3
38.7

40.6

41.0

44.0

48.3
16.3
23.2

31.7
19.8
31.5
33.9
29.9
35.2
32.5
38.2

39.7

39.6

43.6

50.5
14.1
20.6

29.6
15.5
30.3
27.1
28.0
32.8
25.7
36.5

37.7

38.2

42.4

0.01, and it is dropped by a factor of 10 at epoch 100 and
150. AET is trained for 200 epochs in total. Finally, the
projective transformations applied are randomly sampled in
the same fashion as on CIFAR-10.

Results. First we report the Top-1 accuracies of compared
methods in Table 3 on ImageNet by following the evalua-
tion protocol in [18]. Two settings are adopted for evalua-
tion – Conv4 and Conv5 denote to train the remaining part
of AlexNet on top of Conv4 and Conv5 with the labeled da-
ta, while all the bottom convolutional layers up to Conv4
and Conv5 are frozen after they are trained in an unsuper-
vised fashion. For example, in the Conv4 setting, Conv5
and three fully connected layers are trained on the labeled
examples, including the last 1000-way output layer. From
the results, in both settings, the AET model successfully
beats the other compared unsupervised models. In partic-
ular, among the compared models is the BiGAN [6] that
trains a GAN-based unsupervised model, and learns a data-
based auto-encoder as well to map an image to an unsuper-
vised representation. Thus, it can be seen as combing the
strengths of both GAN and AED models. The results show
AET outperforms BiGAN by a signiﬁcant lead, suggesting
its advantage over the GAN and AED paradigms at least in
this experiment setting.

We also compare with the fully supervised models that
give the upper bounded performance by training the entire
AlexNet with all labeled data. The classiﬁers of random
models are trained on top of Conv4 and Conv5 with ran-
domly sampled weights, and they set up the lower bounded

performance. From the comparison, the AET models great-
ly narrow the performance gap to the upper bound – the gap
to the upper bound Top-1 accuracy has been decreased from
9.7% and 15.7% by RotNet and DeepCluster on Conv4 and
Conv5, respectively, to 6.5% and 12.7% by AET, which is
relatively narrowed by 33% and 19%, respectively.

Moreover, we also follow the testing protocol adopted in
[29] to compare the models by training a 1, 000-way linear
classiﬁer on top of different numbers of convolutional layers
in Table 4. Again, AET obtains the best accuracy among all
the compared unsupervised models.

4.3. Places Experiments

We also conduct experiments on the Places dataset. As
shown in Table 5, we evaluate unsupervised models that are
pretrained on the ImageNet dataset. Then a single-layer
logistic regression classiﬁer is trained on top of differen-
t layers of feature maps with Places labels. Thus, we as-
sess the generalizability of unsupervised features from one
dataset to another. Our models are still based on AlexNet
variants like those used in the ImageNet experiments. We
also compare with the fully supervised models trained with
the Places labels and ImageNet labels,as well as the random
networks. The results show the AET models outperform the
other unsupervised models in most of cases, except on Con-
v1 and Conv2, Counting [19] performs slightly better.

4.4. Analysis of Predicated Transformations

Although our ultimate goal is to learn good representa-
tions of images, it is insightful to look into the accuracy of
predicting transformations and its relation with the super-

2553

Table 5: Top-1 accuracy on the Places dataset with linear layers. A 205-way logistic regression classiﬁer is trained on top
of various layers of feature maps that are spatially resized to have about 9, 000 elements. All unsupervised features are
pre-trained on the ImageNet dataset, which are frozen when training the logistic regression layer with Places labels. We also
compare them with fully-supervised networks trained with Places Labels and ImageNet labels, along with random models.
The highest accuracy values are in bold and the second highest accuracy values are underlined.

Method

Conv1 Conv2

Conv3 Conv4 Conv5

Places labels [31]
ImageNet labels
Random
Random rescaled [16]

Context [5]
Context Encoders [22]
Colorization[30]
Jigsaw Puzzles [18]
BiGAN [6]
Split-Brain [29]
Counting [19]
RotNet [10]

(Ours) AET-project

22.1
22.7
15.7
21.4

19.7
18.2
16.0
23.0
22.0
21.3
23.3
21.5

22.1

35.1
34.8
20.3
26.2

26.7
23.2
25.7
31.9
28.7
30.7
33.9
31.0

32.9

40.2
38.4
19.8
27.1

31.9
23.4
29.6
35.0
31.8
34.0
36.3
35.1

37.1

43.3
39.4
19.1
26.1

32.7
21.9
30.3
34.2
31.3
34.1
34.7
34.6

36.2

44.6
38.7
17.5
24.0

30.9
18.4
29.7
29.3
29.7
32.5
29.6
33.7

34.7

(a) CIFAR-10

(b) ImageNet

Figure 4: Error rate(top-1 accuracy) vs. AET loss over e-
pochs on the CIFAR-10 and ImageNet datasets.

vised classiﬁcation performance. As illustrated in Figure 4,
the trend of transformation prediction loss (i.e. the AET loss
being minimized to train the model) is well aligned with that
of classiﬁcation error and Top-1 accuracy on CIFAR-10 and
ImageNet. This suggests that better prediction of transfor-
mations is a good surrogate of better classiﬁcation result by
using the learned features. This justiﬁes our choice of AET
to supervise the learning of feature representations.

In Figure 5, we also compare some examples of orig-
inal images, along with the transformed images at the in-
put and the output ends of the AET model. These exam-
ples show how well the model can decode the transforma-
tions from the encoded image features, thereby delivering
unsupervised representations that offer competitive perfor-
mances on classifying images in our experiments.

Figure 5: Some examples of original images (top), along
with the counterparts of input (middle) and predicted (bot-
tom) transformations by the AET model.

neural networks in contrast
to the conventional Auto-
Encoding Data (AED) approach. By estimating random-
ly sampled transformations at output end, AET forces the
encoder to learn good representations so that they contain
sufﬁcient information about visual structures of both the
original and transformed images. We demonstrate that a
wide variety of transformations can be easily incorporated
into this framework and the experiment results demonstrate
substantial improvements over the state-of-the-art perfor-
mances, signiﬁcantly narrowing the gap with the fully su-
pervised counterparts in literature.

6. Acknowledgement

5. Conclusions

In this paper, we present a novel Auto-Encoding Trans-
formation (AET) paradigm for unsupervised training of

This work was done during Liheng Zhang was interning
at Huawei Cloud, Seattle, WA, while the idea was conceived
and formulated by Guo-Jun Qi.

2554

References

[1] P. Agrawal, J. Carreira, and J. Malik. Learning to see by
moving. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 37–45, 2015. 2, 3

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan.

arXiv preprint arXiv:1701.07875, 2017. 3

[3] P. Bojanowski and A. Joulin. Unsupervised learning by pre-

dicting noise. arXiv preprint arXiv:1704.05310, 2017. 6

[4] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep
clustering for unsupervised learning of visual features. arXiv
preprint arXiv:1807.05520, 2018. 6, 7

[5] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-
sual representation learning by context prediction. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 1422–1430, 2015. 2, 3, 6, 7, 8

[6] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial fea-
ture learning. arXiv preprint arXiv:1605.09782, 2016. 2, 3,
6, 7, 8

[7] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and
T. Brox. Discriminative unsupervised feature learning with
convolutional neural networks. In Advances in Neural Infor-
mation Processing Systems, pages 766–774, 2014. 2, 3, 5,
6

[8] V. Dumoulin,

I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversarially
learned inference. arXiv preprint arXiv:1606.00704, 2016.
2, 3

[9] M. Edraki and G.-J. Qi. Generalized loss-sensitive adversar-
ial learning with manifold margins. In Proceedings of Euro-
pean Conference on Computer Vision (ECCV 2018), 2018.
3

[10] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised rep-
resentation learning by predicting image rotations. arXiv
preprint arXiv:1803.07728, 2018. 2, 3, 5, 6, 7, 8

[11] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014. 1

[12] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transform-
ing auto-encoders. In International Conference on Artiﬁcial
Neural Networks, pages 44–51. Springer, 2011. 2

[13] G. E. Hinton and R. S. Zemel. Autoencoders, minimum de-
scription length and helmholtz free energy. In Advances in
neural information processing systems, pages 3–10, 1994. 2
[14] N. Japkowicz, S. J. Hanson, and M. A. Gluck. Nonlinear
autoassociation is not equivalent to pca. Neural computation,
12(3):531–545, 2000. 2

[15] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013. 2

[16] P. Kr¨ahenb¨uhl, C. Doersch, J. Donahue, and T. Darrell. Data-
dependent initializations of convolutional neural networks.
arXiv preprint arXiv:1511.06856, 2015. 7, 8

[17] G. Larsson, M. Maire, and G. Shakhnarovich. Learning rep-
resentations for automatic colorization.
In European Con-
ference on Computer Vision, pages 577–593. Springer, 2016.
2

[18] M. Noroozi and P. Favaro. Unsupervised learning of visual
representations by solving jigsaw puzzles. In European Con-
ference on Computer Vision, pages 69–84. Springer, 2016. 2,
3, 6, 7, 8

[19] M. Noroozi, H. Pirsiavash, and P. Favaro. Representation
In The IEEE International

learning by learning to count.
Conference on Computer Vision (ICCV), 2017. 3, 6, 7, 8

[20] E. Oyallon, E. Belilovsky, and S. Zagoruyko. Scaling the
scattering transform: Deep hybrid networks. In International
Conference on Computer Vision (ICCV), 2017. 5, 6

[21] E. Oyallon and S. Mallat. Deep roto-translation scattering for
object classiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2865–
2873, 2015. 5, 6

[22] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.
Efros. Context encoders: Feature learning by inpainting.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2536–2544, 2016. 7, 8

[23] G.-J. Qi. Loss-sensitive generative adversarial networks on
lipschitz densities. arXiv preprint arXiv:1701.06264, 2017.
3

[24] G.-J. Qi, L. Zhang, H. Hu, M. Edraki, J. Wang, and X.-S.
Hua. Global versus localized generative adversarial nets. In
Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 4

[25] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015. 4,
5, 6

[26] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio.
Contractive auto-encoders: Explicit invariance during fea-
ture extraction.
In Proceedings of the 28th International
Conference on International Conference on Machine Learn-
ing, pages 833–840. Omnipress, 2011. 2

[27] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol.
Extracting and composing robust features with denoising au-
toencoders. In Proceedings of the 25th international confer-
ence on Machine learning, pages 1096–1103. ACM, 2008.
2

[28] X. Wang and A. Gupta. Unsupervised learning of visual rep-
resentations using videos. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 2794–2802,
2015. 6

[29] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders:

Unsupervised learning by cross-channel prediction. 2, 7, 8

[30] R. Zhang, P. Isola, and A. A. Efros. Colorful image coloriza-
In European Conference on Computer Vision, pages

tion.
649–666. Springer, 2016. 2, 6, 7, 8

[31] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In Advances in neural information processing sys-
tems, pages 487–495, 2014. 8

2555

