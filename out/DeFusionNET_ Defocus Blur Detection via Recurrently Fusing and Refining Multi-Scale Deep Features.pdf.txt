DeFusionNET: Defocus Blur Detection via Recurrently Fusing and Reﬁning

Multi-scale Deep Features

Chang Tang1, Xinzhong Zhu2, Xinwang Liu3, Lizhe Wang1, Albert Zomaya4

1School of Computer Science, China University of Geosciences, Wuhan 430074, China

2College of Mathematics, Physics and Information Engineering, Zhejiang Normal University, Jinhua 321004, China

3School of Computer Science, National University of Defense Technology, Changsha 410073, China

4School of Information Technologies, University of Sydney, NSW 2006, Australia

{tangchang@cug.edu.cn,zxz@zjnu.edu.cn,xinwangliu@nudt.edu.cn,Lizhe.Wang@gmail.com,albert.zomaya@sydney.edu.au}

Abstract

Defocus blur detection aims to detect out-of-focus re-
gions from an image. Although attracting more and more
attention due to its widespread applications, defocus blur
detection still confronts several challenges such as the in-
terference of background clutter, sensitivity to scales and
missing boundary details of defocus blur regions. To deal
with these issues, we propose a deep neural network which
recurrently fuses and reﬁnes multi-scale deep features (De-
FusionNet) for defocus blur detection. We ﬁrstly utilize a
fully convolutional network to extract multi-scale deep fea-
tures. The features from bottom layers are able to capture
rich low-level features for details preservation, while the
features from top layers can characterize the semantic infor-
mation to locate blur regions. These features from different
layers are fused as shallow features and semantic features,
respectively. After that, the fused shallow features are prop-
agated to top layers for reﬁning the ﬁne details of detected
defocus blur regions, and the fused semantic features are
propagated to bottom layers to assist in better locating the
defocus regions. The feature fusing and reﬁning are carried
out in a recurrent manner. Also, we ﬁnally fuse the output
of each layer at the last recurrent step to obtain the ﬁnal
defocus blur map by considering the sensitivity to scales
of the defocus degree. Experiments on two commonly used
defocus blur detection benchmark datasets are conducted
to demonstrate the superority of DeFusionNet when com-
pared with other 10 competitors. Code and more results
can be found at: http://tangchang.net

1. Introduction

As a common phenomenon, defocus blur occurs when
the objects of a scene are not exactly at the camera’s focus
distance. Defocus blur detection, which aims to detect the

(a) Input

(b) LBP

(c) HiFST

(d) BTBNet

(e) DeFusionNet

(f) GT

Figure 1. Some challenging cases for defocus blur detection. (a)
Input image, defocus blur detection maps obtained by (b) LBP
[41], (c) HiFST [1], (d) BTBNet [48], (e) our DeFusionNet, and
(f) ground truth (GT).

out-of-focus regions from an image, has obtained much at-
tention due to its wide potential applications such as image
quality assessment [38, 32], salient object detection [9, 34],
image deblurring [17, 25], defocus magniﬁcation [33, 2]
and image refocusing [44, 45], just list a few.

In the past decades, a variety of defocus blur detec-
tion methods have been proposed. Based on the used
image features, these methods can be roughly classiﬁed
into two categories, i.e., traditional hand-crafted features
based methods and deep learning based methods. As to
the former kind of methods, they often extract features
such as gradient and frequency which can model the edge
changes since defocus blur usually blunts object edges in an
image[15, 50, 29, 37, 46, 49, 25, 19, 35, 24, 21]. Although
great success has been achieved by using these tradition-

2700

al hand-crafted features based methods, they still confront
several challenges and the detected results are still not very
perfect. Firstly, traditional low-level features can not work
well for separating the blurred smooth regions which do not
contain structural information from the in-focus smooth re-
gions. Secondly, these methods can not well capture the
global semantic information which is critical for detecting
low-contrast focal regions (as shown in the red rectangular
region of Figure 1a) and suppressing the background clutter
(as shown in the yellow rectangular region of Figure 1a).
In addition, the edge information of in-focus objects have
not been well preserved (as shown in the green rectangular
region of Figure 1a).

Recently, due to the strong feature extraction and learn-
ing capability, deep convolutional neural networks (CNNs)
have made remarkable advances in various computer vision
tasks, such as image classiﬁcation [12, 28], object detection
[11, 14], object tracking [13, 30, 23], scene semantic seg-
mentation [18, 16, 47], image de-noising [10, 42] and super-
resolution [5, 27]. As a result, CNNs are also used for image
defocus blur region detection. In [40], a pre-trained deep
neural network and a general regression neural network are
proposed to classify the blur type and then estimate its pa-
rameters. By systematically analyzing the effectiveness of
different defocus detection features, Park et al. [21] extract-
ed deep and hand-crafted features in image patches which
contain sparse strong edges. However, low-contrast focal
regions are still not well distinguished. In addition, a series
of spatial pooling and convolution operations result in los-
ing much of the ﬁne details of image structure. In [48], Zhao
et al. proposed a multi-stream bottom-top-bottom fully con-
volutional network (BTBNet), which is the ﬁrst attempt to
develop an end-to-end deep network for defocus blur detec-
tion.
In BTBNet, low-level cues and high-level semantic
information are integrated to promote the ﬁnal results and a
multi-stream strategy is leveraged to handle the defocus de-
gree’s sensitivity to image scales. Although signiﬁcant im-
provement has been obtained by BTBNet, it uses a forward
stream and a backward stream to integrate features from dif-
ferent levels for each image scale, this causes high computa-
tional complexity for both network training and testing, and
the complementary information of different layers cannot
been fully exploited, which causes some background clut-
ters in the ﬁnal results. In addition, some low-contrast focal
areas are still mistakenly detected as defocus blur regions.
In this work, we propose a novel efﬁcient pixel-wise fully
convolutional network for defocus blur detection via recur-
rently fusing and reﬁning multi-scale deep features (DeFu-
sionNET). Particularly, we recurrently fuse and reﬁne the
deep features across deep and shallow layers in a we sum-
marize the technical contributions of this work as follows:n
alternate and cross-layer manner, then the complementary
information of features from different layers can be fully ex-

ploited for maximized defocus blur detection performance.
In detail,

• We design a new efﬁcient pixel-wise fully convolu-
tional network for defocus blur detection from the raw
input image. The proposed network fuses and reﬁne
multi-scale deep features to effectively suppress the
background clutter and distinguish low-contrast focal
regions from defocus blur areas.

• Instead of directly reﬁning the detected defocus blur
map, we develop a feature fusing and reﬁning mod-
ule (FFRM) to recurrently reﬁne the features of differ-
ent layers in an alternate and cross-layer manner. By
considering that different layers extract features of d-
ifferent scales for an image, we aggregate the output
score maps of different layers at the last recurrent step
to generate the ﬁnal defocus blur map.

• We evaluate our network on two benchmark dataset-
s and compare it with 10 state-of-the-art defocus blur
detection methods. The experimental results demon-
strate that our method consistently outperforms other
competitors on the two datasets. In the meanwhile, our
network is very efﬁcient and it takes only less than 0.1s
by using a single GTX Titan Xp GPU with 12G memo-
ry to generate the defocus blur map for a testing image
in the two datasets.

• We aim to set up a benchmark for comparison of var-
ious defocus blur detection methods. The results of
various methods on different datasets will be publicly
released for academic usage.

2. Related Work

2.1. Hand crafted Features based Methods

Since defocus blur usually degenerates object edges in
an image, traditional methods often extract features such as
gradient and frequency which can describe the change of
edges [6, 31, 50, 33, 4, 32]. Based on the observation that
the ﬁrst few most signiﬁcant eigen-images of a blurred im-
age patch usually have higher weights (i.e. singular values)
than an image patch with no blur, Su et al. [29] detected blur
regions by examining singular value information for each
image pixels. Shi et al. [25] studied a series of blur feature
representations such as gradient, Fourier domain, and data-
driven local ﬁlters features to enhance discriminative power
for differentiating blurred and unblurred image regions. In
[19], Pang et al. developed a kernel-speciﬁc feature for blur
detection, the blur regions and in-focus regions are classi-
ﬁed using SVM. Considering that feature descriptors based
on local information cannot distinguish the just noticeable

2701

blur reliably from unblurred structures, Shi et al. [26] pro-
posed a simple yet effective blur feature via sparse repre-
sentation and image decomposition. Yi and Eramian [41]
designed a sharpness metric based on local binary pattern-
s and the in- and out-of-focus image regions are separated
by using the metric. Tang et al. [36] designed a log aver-
aged spectrum residual metric to obtain a coarse blur map,
then an iterative updating mechanism is proposed to reﬁne
the blur map from coarse to ﬁne based on the intrinsic rel-
evance of similar neighbor image regions. Golestaneh and
Karam [1] proposed to detect defocus blur maps based on a
novel high-frequency multiscale fusion and sort transform
of gradient magnitudes. Based on the maximum ranks of
the corresponding local patches with different orientations
in gradient domain, Xu et al. [39] presented a fast yet ef-
fective approach to estimate the spatially varying amounts
of defocus blur at edge locations, then the complete defocus
map is generated by a standard propagation procedure.

Although previous hand-crafted methods have earned
great success for defocus blur region detection, they can on-
ly work well for images with simple structures but are not
robust enough for complex scenes. Therefore, extracting
high level and more discriminative features are necessary.

2.2. Deep Learning based Methods

Due to their high level feature extraction and learn-
ing power, deep CNNs based methods have refreshed the
records of many computer vision tasks [28, 11, 13, 47, 27],
including defocus blur detection [21, 48].
In [21], high-
dimensional deep features are ﬁrst extracted by using a
CNN-based model, then these features and traditional hand-
crafted features are concatenated together and fed into a ful-
ly connected neural network classiﬁer for defocus degree
[22] proposed to train two
determination. Purohit et al.
sub-networks which aim to learn global context and local
features respectively, then the pixel-level probabilities esti-
mated by two networks are aggregated and feed into a MRF
based framework for blur regions segmentation. Zhang et
al. [43] proposed a dilated fully convolutional neural net-
work with pyramid pooling and boundary reﬁnement layers
to generate blur response maps. Considering that the de-
gree of defocus blur is sensitive to scales, Zhao et al. [48]
proposed a multi-stream bottom-top-bottom fully convolu-
tional network (BTBNet) which integrates low-level cues
and high-level semantic information for defocus blur detec-
tion. Since it uses two streams, i.e., a forward stream and
a backward stream, to integrate features from different lev-
els for multiple image scales, the computational complexity
for both network training and testing of BTBNet is high.
Meanwhile, some low-contrast focal areas still cannot be
differentiated.

In this work, we propose an effective and efﬁcient defo-
cus blur detection deep neural network via recurrently fus-

ing and reﬁning multi-scale deep features (DeFusionNET).
Instead of directly reﬁning the output score map as many
previous deep CNNs based detection methods do, we re-
currently reﬁne the features of different layers in DeFusion-
NET. Particularly, we design a feature fusing and reﬁning
module (FFRM) to exploit the complementary information
of low-level cues and high-level semantic features by reﬁn-
ing them in a cross-level manner, i.e., features from low-
level layers are fused and used to reﬁne features extracted
from high-level layers, and vice versa. Note that different
layers extract features of different scales for an image and
the degree of defocus blur is sensitive to image scales, we
fuse the output score maps of different layers at the last
recurrent step to generate the ﬁnal defocus blur map. Ex-
perimental results demonstrate that the proposed DeFusion-
NET performs better than other state-of-the-art approaches
in terms of both accuracy and efﬁciency.

3. Proposed DeFusionNET

In this work, we aim to develop an efﬁcient defocus blur
detection deep neural network which takes an image as in-
put and output a defocus blur detection map with the same
resolution as the input image. Figure 2 shows the entire ar-
chitecture of our proposed defocus blur detection network.
For an effective defocus blur detection network, it should
be power to extract both low-level cues and high-level se-
mantic information for generate the ﬁnal accurate detected
defocus blur map. The low-level features can help reﬁne
the sparse and irregular detection regions, while the high-
level semantic features can serve to locate the blurry regions
as well as suppress background clutters. In addition, there
are often some smooth in-focus regions within an object,
the high-level semantic information produced by deep lay-
ers can avoid these regions being detected as blurry regions.
Furthermore, since the defocus degree is sensitive to im-
age scales, the network should be capable of making use of
multi-scale features for improving the ﬁnal results. Finally,
the network should be easily to be ﬁne-tuned because there
are no sufﬁcient labeled defocus blur images for training
such a deep network.

Speciﬁcally, we choose the VGG network [28] as our
backbone feature extraction network and use the pre-trained
VGG16 model to initialize our network. Firstly, we use our
network to extract a set of hierarchical features which en-
code the low-level details and high-level semantic informa-
tion with different scales of an image. On the one hand,
since a series of spatial pooling and convolution operations
progressively downsample the resolution of the initial im-
age, the ﬁne details of image structure are inevitably dam-
aged, which is harmful for densely separating in-focus and
out-of-focus image regions. On the other hand, the high-
level semantic features extracted by deep layers can help to
locate the defocus blur regions. Therefore, how to exploit

2702

Figure 2. The pipeline of our DeFusionNET. The dark gray block represents the proposed FFRM module. For a given image, we ﬁrst extract
its multi-scale features by using the basic VGG network. Then the features from shallow layers and deep layers are fused as FSHF and
FSEF, respectively. Considering the complementary information between FSHF and FSEF, we use them to reﬁne the features of deep and
shallow layers in a cross-layer manner. The feature fusion and reﬁnement are performed step by step in a recurrent manner to alternatively
reﬁne FSHF, FSEF and the features at each layer (the times of recurrent step is empirically set to 3 in our experiments). In addition, the
deep supervision mechanism is imposed at each step and the prediction result of each layer are fused to obtain the ﬁnal defocus blur map.

the complementary information of features extracted from
shallow layers and deep layers to improve the ﬁnal results
is critical. As to the low-level and high-level feature maps,
we upsample them to the size of input image by using the
deconvolution operation and concatenate them together to
form fused shallow features (FSHF) and fused semantic fea-
tures (FSEF), respectively. In order to reﬁne the detailed in-
formation of features at deep layers, we aggregate the FSHF
with each deep layer as FSHF encompasses more details of
image contents. In order to facilitate the defocus blur re-
gion location information of features at shallow layers, we
also aggregate the FSEF with each shallow layer as FSEF
captures more semantic information of image contents. The
feature fusing and aggregating are recurrently carried out in
a cross-layer manner. Since different layers extract features
with different scales for an image and the degree of defocus
blur is sensitive to image scales, the output score maps of d-
ifferent layers at the last recurrent step are fused to generate
the ﬁnal defocus blur map.

3.1. Feature Fusing and Reﬁning Module

The success of deep CNNs owes to its strong capaci-
ty of hierarchically extracting abundant semantic as well
as ﬁne details information by different layers. For defo-
cus blur region detection, the features represent ﬁne details
are necessary since they can beneﬁt to preserve the bound-
aries between in-focus regions and out-of-focus regions for
promoting detection accuracy. The high-level semantic in-
formation can serve to accurately locate the defocus blur
regions and avoid the smooth in-focus regions being falsely
regarded as blur regions, which is also critical. As a re-
sult, we can integrate multi-level features to enhance the
discrimination ability for defocus blur detection. In deep C-
NNs, deep layers can capture highly semantic information
which describe the attributes of image contents as a whole,
while shallow layers focus more on subtly ﬁne details which
represent delicate structures of objects, directly fusing the
features from different layers for generating ﬁnal detection

results may not be appropriate. In this work, we propose
a feature fusing and reﬁning module (FFRM) which inte-
grates high-level semantic features and low-level shallow
features separately and reﬁnes them in a cross-layer man-
ner. Figure 3 shows the architecture of the proposed FFRM
model.

Figure 3. The architecture of the proposed feature fusing and re-
ﬁning module (FFRM).

Supposing there are n total layers in our network, we re-
gard the ﬁrst m layers as shallow layers and the rest ones as
deep layers. For the feature maps generated from each shal-
low layer, we ﬁrst upsample them to the size of input image
by using the deconvolution operation and concatenate them
together, then a convolution layer with 1 × 1 kernel follows
the concatenated feature maps is used to generate FSHF.
The FSHF can be mathematically deﬁned as follows:

F SHF = ReLU (Wl ∗ Cat(F1, F2, · · · , Fm)) + bl), (1)

where Fi ∈ W × H × C denotes the upsampled feature
maps from the i-th layer with C channels; W × H is the
resolution of input image; Cat represents the concatenation
operation across channels; ∗ represents convolution opera-
tion; Wl and bl are the weights and bias of the convolution
need to be learned during training and ReLU is the ReLU
activation function [12].

Similarly, the high-level semantic features are fused to

form FSEF as follows:

F SEF = ReLU (Wh∗Cat(Fm+1, Fm+2, · · · , Fn))+bh).
(2)

2703

Since FSHF encodes the ﬁne details while FSEF cap-
tures more semantic information of image contents, one can
directly fuse them to generate defocus blur maps. Howev-
er, the quality of the results cannot be well guaranteed and
there are still many in-focus regions being wrongly detect-
ed. This is because the fused FSHF still contains some in-
focus details and FSEF also contains some incorrect seman-
tic information. Directly using FSHF and FSEF not only
provides wrong guidance for defocus blur region detection,
but also harms the useful information originally contained
in individual layers. To this end, we propose to recurrently
fuse and reﬁne the layer-wise features in a cross-layer man-
ner.

In order to leverage the complementary advantages of
both shallow layers and deep layers, we aggregate FSHF to
each individual deep layer and aggregate FSEF to each in-
dividual shallow layer. In such a cross-layer manner, the
features extracted by each layer can be reﬁne step by step.
Speciﬁcally, since the features of shallow layers focus on
the ﬁne detail information but lack of semantic information
of defocus blur regions, the FSEF can be used to help them
better locate semantic defocus blur regions. Similarly, as
the features of deep layers capture semantic information but
lack of ﬁne details, the FSHF can be used to promote the
ﬁne details preservation. In the recurrent aggregation pro-
cess, the reﬁned feature maps from shallow layers and deep
layers are fused again to generate reﬁned FSHF and FSEF,
respectively. Then the reﬁned FSHF and FSEF are aggre-
gated respectively to feature maps from shallow layers and
deep layers in the next recurrent step.

In order to select the useful multi-level information with
respect to the features of each individual layer and reduce
the number of feature channels to the original number be-
fore next aggregation, we add a convolutional layer for the
aggregated feature maps of each layer. The reﬁned feature
maps of each layer at the j-th recurrent step can be formu-
lated as follows:

i

i

Fj

ReLU (Wj

i ∗ Cat(Fj−1
i ∗ Cat(Fj−1

i = m + 1, · · · , n
i = 1, · · · , m

, F SHF j ) + bj
i )
, F SEF j ) + bj
i )

i = (ReLU (Wj
(3)
where Fj
i represents the feature maps for the i-th layer at
the j-th recurrent step. F SEF j and F SHF j represent the
FSEF and FSHF at the j-th recurrent step, respectively. Wj
i
and bj
i represent the convolutional kernel and bias of the
i-th layer at the j-th recurrent step.

3.2. Defocus Maps Fusing

Since the degree of defocus blur is sensitive to image
scales, we need to capture multi-scale information for im-
proving ﬁnal defocus blur detection results. In [48], Zhao
et al. proposed to use a multi-stream strategy to fuse the
detection results from different image scales. However, this
inevitably increase the computational burden of the whole

network. In this work, by considering that different layer-
s just extract features of original image in different scales,
we impose a supervision signal to each layer by using the
deeply supervised mechanism at each recurrent step, then
the output score maps of all the layers at the last step are
fused to generate the ﬁnal defocus blur map.

Speciﬁcally, we ﬁrst concatenate the defocus blur maps
predicted from n different layers, then a convolution layer is
imposed on the concatenated maps to obtain the ﬁnal output
defocus blur map B, which can be formulated as:

B = ReLU (WB ∗ Cat(Bt

1, Bt

2, · · · , Bt

n) + bB),

(4)

where t denotes the last recurrent step; Bt
i denotes the pre-
dicted defocus blur map from the i-th layer at the t-th step;
WB and bB are the weight and bias of the convolution layer
on the concatenated defocus blur maps to learn the relation-
ship among these maps. Note that Hu et al. [7] used a simi-
lar manner to aggregate deep features for saliency detection,
but they did not distinguish features of shallow layers and
deep layers.

3.3. Model Training and Testing

Our network uses the VGG [28] as backbone and we im-
plement it by Caffe [8]. We use conv1 2, conv2 2, conv3 3,
conv4 3, conv5 3 and pool5 of the VGG network to repre-
sent the features of each individual layer, i.e., n = 6 in De-
FusionNET. The ﬁrst three layers are regarded as shallow
layers, and the rest ones are set as deep layers, i.e., m = 3.
In addition, in order to enhance the discrimination capabil-
ity of feature maps at each layer, two more convolutional
layers are appended. More details will be found in the re-
leased code.
Training: The cross-entropy loss is used for each output of
this network during the training process. For the i-th layer
at the j-th recurrent step, the pixel-wise cross entropy loss
between Bj
i and the ground truth blur mask G is calculated
as:

Lj

i (θ) = −

W

H

X

x=1

X

y=1

X

l∈{0,1}

nlog Pr(Bj

·1(G(x,y)=l)

i (x,y)=l|θ)

o (5)

where 1(·) is the indicator function. The notation l ∈
{0, 1} indicates the out-of-focus or in-focus label of the pix-
el at location (x, y) and P r(Bj
i (x, y) = l|θ) represents its
corresponding probability of being predicted as blurry pixel
or not. θ denotes the parameters of all network layers.

Based on Eq. (5), the ﬁnal loss function is deﬁned as the

loss summation of all immediate predictions:

L = λf Lf +

n

t

X

i=1

X

j=1

λj
i Lj

i (θ),

(6)

2704

where Lf is loss for the ﬁnal fusion layer; Lf is the weight
for the fusion layer and λj
i represents the weight of the i-
th layer at the j-th recurrent step. In our experiment, we
empirically set all the weights to 1.

Our model is initialized by the pre-trained VGG-16 mod-
el and ﬁne tuned on part of Shi et al.’s public blurred im-
age dataset [25], which consists of 1000 blurred images and
their manually annotated ground truths. 704 of these im-
ages are partially defocus blurred and the rest 296 ones are
motion blurred. We divide the 704 defocus blurred images
into two parts, i.e., 604 for training and the remaining 100
ones for testing. Since the number of training images is not
enough to train a deep neural network, we perform data aug-
mentation by randomly rotating, resizing and horizontally
ﬂipping all of the images and their corresponding ground
truths, and ﬁnally the training set is enlarged to 9,664 im-
ages. We train our model on a machine equipped with an
Intel 3.4GHz CPU with 128G memory and 2 GPUs (one
Nvidia GTX 1080Ti and one Nvidia GTX Titan Xp). We
optimize the whole network by using Stochastic gradien-
t descent (SGD) algorithm with the momentum of 0.9 and
the weight decay of 0.0005. The learning rate is initially set
to 1e-8 and reduced by a factor of 0.1 at 5k iterations. The
training batch size is set to 4 and the whole learning process
stops after 10k iterations. The training process is completed
after approximately 11.6 hours.
Inference: In the testing phase, for each input image, we
feed it into our network and obtain the ﬁnal defocus blur
map. Only approximately 0.056s is needed for generating
the ﬁnal defocus blur map for a testing image with 320×320
pixels by using a single Nvidia GTX Titan Xp GPU, which
is very efﬁcient.

4. Experiments

4.1. Datasets

In our experiments, two datasets are used for evaluating

the performance of our proposed network.
Shi et al.’s dataset [25] contains the rest 100 defocus
blurred images as mentioned above.
DUT [48] is a new defocus blur detection dataset which
consists of 500 images with pixel-wise annotations. This
dataset is very challenging since numerous images contain
homogeneous regions,
low contrast focal regions and
background clutter.

4.2. Evaluation Metrics

Four widely-used metrics are used to quantitatively e-
valuate the performance of the proposed model: precision-
recall (PR) curves, F-measure curves, F-measure scores
(Fβ) and mean absolute error (MAE) scores. As an over-
all performance measurement, the F-measure is deﬁned

(1+β 2)·precision·recall

β 2·precision+recall

, where β2 is set to 0.3
as: Fβ =
to emphasize precision.The MAE score calculates the av-
erage difference between the detected defocus blur map
B and the ground truth G, it is computed as: M AE =

1

W ×H

W

H

Px=1

Py=1

|B(x, y) − G(x, y)|, where H and W are the

height and width of the input image, respectively.

4.3. Comparison with the state of the art methods

We compare our method against other 10 state-of-the-
art algorithms, including 2 deep learning-based methods,
i..e, multi-scale deep and hand-crafted features for defo-
cus estimation (DHDE) [21] and multi-stream bottom-top-
bottom fully convolutional network (BTBNet) [48], and 8
classic defocus blur detection methods, including analyz-
ing spatially-varying blur (ASVB) [3], Singular Value De-
composition based blur detection (SVD) [29], just notice-
able defocus blur detection (JNB) [26], discriminative blur
detection features (DBDF) [25], spectral and spatial ap-
proach (SS) [35], local binary patterns (LBP) [41], classify-
ing discriminative features (KSFV) [20] and high-frequency
multi-scale fusion and sort transform of gradient magni-
tudes (HiFST) [1]. For all of these methods except BTBNet,
we use the authors’ original implementations with recom-
mended parameters. As to BTBNet, we directly download
the results from the authors’ project page since they have
not released their implementation.
Quantitative Comparison. Table 1 presents the compared
results of MAE and F-measure scores. It is observed that
our method consistently performs favorably against other
methods on the two datasets, which indicates the superior-
ity of our method over other approaches. In Figure 4 and
Figure 5, we plot the PR curves and F-measure curves of d-
ifferent methods on different datasets. From the results, we
observe that our method also consistently outperforms other
counterparts.
Qualitative Comparison. Figure 6 shows a visual com-
parison of our method and other ones. As can be seen, our
method generates more accurate defocus blur maps when
the input image contains in-focus smooth regions and back-
ground clutter. In addition, the boundary information of the
in-focus objects can be well preserved in our results. More
visual comparison results can be found in the supplemen-
tary ﬁle.
Running Efﬁciency Comparison. In addition to the ap-
pealing results, our proposed DeFusionNet is also efﬁcient
for both training and testing. The whole training process
of our DeFusionNet takes only about 11.6 hours. As to the
testing phase, we use only one GPU (Nvidia GTX Titan
Xp). The average running time for an image of different
methods on the two different datasets are shown in Table 2.
As can be seen, when our DeFusionNet is well trained, it
is faster than all of other methods for detecting the defocus

2705

 

1

0.9

0.8

0.7

0.6

0.5

0.4

n
o
s

i

i

c
e
r
P

ASVB

SVD

0.3

JNB

DBDF

0.2

SS

LBP

KSFV

DHDE

0.1

HiFST

BTBNet

 
0

DeFusionNet

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

e
r
u
s
a
e
m
−
F

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 
0

 

ASVB

SVD

JNB

DBDF

SS

LBP

KSFV

DHDE

HiFST

BTBNet

DeFusionNet

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Threshold

Figure 4. Comparison of precision-recall curves and F-measure curves of different methods on Shi et al.’s dataset.

(a) Precision-recall curves

(b) F-measure curves

 

1

0.9

0.8

0.7

0.6

0.5

0.4

n
o
s

i

i

c
e
r
P

ASVB

SVD

0.3

JNB

SS

0.2

DBDF

LBP

KSFV

DHDE

0.1

HiFST

BTBNet

 
0

DeFusionNet

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

e
r
u
s
a
e
m
−
F

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 
0

 

ASVB

SVD

JNB

DBDF

SS

LBP

KSFV

DHDE

HiFST

BTBNet

DeFusionNet

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Threshold

(a) Precision-recall curves

(b) F-measure curves

Figure 5. Comparison of precision-recall curves and F-measure curves of different methods on DUT dataset.

Figure 6. Visual comparison of detected defocus blur maps generated from different methods. The results demonstrate that our method
consistently outperforms other approaches, and produces defocus blur maps more close to the ground truth.

blur regions from an input image. As to BTBNet, although
we cannot evaluate its running time since we do not have its
implementation, the authors claimed in their paper that n-
early 5 days needed for training BTBNet and approximately
25s is needed to generate the defocus blur map for a testing
image with 320 × 320 pixels. By contrast, the training and
testing phases of our DeFusionNet is more efﬁcient.

4.4. Ablation Analysis

Effectiveness of FFRM. In order to validate the efﬁcacy of
FFRM, we change the network by fusing the feature maps
from all of layers to one group at each recurrent step, then
the fused features are used to reﬁne the features of each
layer. We denote this network as DeFusionNet noFFRM
for comparison. The F-measure and MAE scores on the t-

2706

Table 1. Quantitative comparison of F-measure and MAE scores. The best two results are shown in red and blue colors, respectively.

Datasets

Metric

ASVB

SVD

JNB

DBDF

SS

LBP

KSFV

DHDE

HiFST

BTBNet

DeFusionNet

Shi et al.’s dataset

DUT

Fβ
MAE
Fβ
MAE

0.731
0.636
0.747
0.651

0.806
0.301
0.818
0.301

0.797
0.355
0.748
0.424

0.841
0.323
0.802
0.369

0.787
0.298
0.784
0.296

0.866
0.186
0.874
0.173

0.733
0.380
0.751
0.399

0.850
0.390
0.823
0.408

0.856
0.232
0.866
0.302

0.892
0.105
0.887
0.190

0.917
0.116
0.922
0.115

Table 2. Average running time (seconds) for an image of different methods on different datasets.

Methods

ASVB

SVD

JNB

DBDF

SS

LBP

KSFV

DHDE

HiFST

BTBNet

DeFusionNet

Datasets

Shi et al.’s dataset

DUT

2.04
1.59

21.09
10.91

11.47
5.12

214.83
110.37

2.76
1.20

57.34
30.38

32.748
20.139

47.06
21.51

2576.24
1169.57

–
–

0.094
0.056

wo datasets are shown in Table 3, and the precision-recall
curves are shown in the supplementary. As can be seen,
our DeFusionNet with FFRM module performs better than
DeFusionNet noFFRM, which demonstrates that the cross-
layer feature fusion manner can effectively capture the com-
plementary information between shallow features and deep
semantic features for improving the ﬁnal results. In addi-
tion, DeFusionNet noFFRM also performs better than oth-
er previous methods, this also validates the efﬁcacy of our
proposed network structure.
Effectiveness of the Final Defocus Maps Fusion. By con-
sidering that the degree of defocus in an image is sensitive
to image scales, we fuse the output of different layers at
the last recurrent step to form the ﬁnal result. We also
perform ablation experiments to evaluate the effectiveness
of the ﬁnal fusing step. The ﬁnal outputs of all the lay-
ers are represented as DeFusionNet O1, DeFusionNet O2,
DeFusionNet O3, DeFusionNet O4, DeFusionNet O5, De-
FusionNet O6. We also show the F-measure, MAE scores
in Table 3 and the precision-recall curves of these outputs
in the supplementary. It can be seen that the fusing mecha-
nism effectively improves the ﬁnal results.
Effectiveness of the Times of Recurrent Steps. In our De-

Table 3. Ablation analysis using F-measure and MAE scores.

Methods

Shi et al.’s dataset

DUT

Fβ

DeFusionNet noFFRM 0.907
0.914
0.914
0.914
0.911
0.915
0.915
0.917

DeFusionNet O1
DeFusionNet O2
DeFusionNet O3
DeFusionNet O4
DeFusionNet O5
DeFusionNet O6

DeFusionNet

MAE
0.154
0.118
0.118
0.118
0.127
0.118
0.117
0.116

Fβ
0.904
0.915
0.915
0.918
0.915
0.919
0.920
0.922

MAE
0.155
0.118
0.118
0.118
0.125
0.117
0.117
0.115

FusionNet, we fuse and reﬁne the features of each layer in
a recurrent and cross-layer manner, the feature maps can be
improved step by step. In order to validate whether the fea-
tures can be improved in a recurrent manner, we report the
F-measure and MAE scores by using different times of re-
current step in Table 4. As can be seen from Table 4, the
more times of recurrent step, the better results can be ob-
tained. In addition, it should be noted that DeFusionNet can
obtain relatively stable results when the times of recurrent is

Table 4. Ablation analysis of the times of recurrent steps (DeFu-
sionNet Step k represents using k times of recurrent steps in De-
FusionNet).

Methods

Shi et al.’s dataset

DUT

DeFusionNet Step 1
DeFusionNet Step 2
DeFusionNet Step 3
DeFusionNet Step 4
DeFusionNet Step 5
DeFusionNet Step 6

Fβ
0.702
0.883
0.917
0.918
0.918
0.919

MAE
0.253
0.132
0.116
0.116
0.115
0.115

Fβ
0.756
0.893
0.922
0.923
0.924
0.924

MAE
0.321
0.134
0.115
0.115
0.116
0.116

3. Therefore, we empirically set 3 times of recurrent step in
our experiments for the tradeoff between effectiveness and
efﬁciency.

5. Conclusions

In this work, we propose a deep convolutional network
(DeFusionNet) for efﬁcient and accurate defocus blur de-
tection. Firstly, DeFusionNet combines both shallow-layer
features and deep-layer features for generating the ﬁnal
high-resolution defocus blur maps. Secondly, DeFusion-
Net fuses and reﬁnes the features from different players in a
cross-layer manner, which can effectively capture the com-
plementary information between shallow features and deep
semantics features. Finally, DeFusionNet obtains the ﬁnal
accurate defocus blur map by fusing the outputs from all
the layers. Extensive experimental results demonstrate that
the proposed DeFusionNet consistently outperforms other
state-of-the-art methods in terms of both accuracy and efﬁ-
ciency.

6. Acknowledgments

The work was supported by the National Natural Sci-
ence Foundation of China (NO. 61701451 and 61773392)
and the Fundamental Research Funds for the Central U-
niversities, China University of Geosciences (Wuhan) NO.
CUG170654. We would also like to thank NVIDIA Cor-
poration for the donation of a Titan Xp GPU card used for
this research. Xinzhong Zhu and Xinwang Liu are the cor-
responding authors of this paper.

2707

References

[1] S Alireza Golestaneh and Lina J Karam. Spatially-varying
blur detection based on multiscale fused and sorted transfor-
m coefﬁcients of gradient magnitudes. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 5800–
5809, 2017. 1, 3, 6

[2] Soonmin Bae and Fr´edo Durand. Defocus magniﬁcation.

Computer Graphics Forum, 26(3):571–579, 2007. 1

[3] Ayan Chakrabarti, Todd Zickler, and William T. Freeman.
Analyzing spatially-varying blur.
In IEEE Conference on
Computer Vision and Pattern Recognition, pages 2512–
2519, 2010. 6

[4] Florent Couzinie-Devy, Jian Sun, Karteek Alahari, and Jean
Ponce. Learning to estimate and remove non-uniform image
blur. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1075–1082, 2013. 2

[5] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-
works. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(2):295–307, 2016. 2

[6] James H Elder and Steven W Zucker. Local scale control for
edge detection and blur estimation.
IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(7):699–716,
1998. 2

[7] Xiaowei Hu, Lei Zhu, Jing Qin, Chi-Wing Fu, and Pheng-
Ann Heng. Recurrently aggregating deep features for salient
object detection. In AAAI, pages 6943–6950, 2018. 5

[8] Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev,
Sergey, Long, and Jonathan. Caffe: Convolutional architec-
ture for fast feature embedding. In ACM MM, pages 675–
678, 2014. 5

[9] Peng Jiang, Haibin Ling, Jingyi Yu, and Jingliang Peng.
Salient region detection by ufo: Uniqueness, focusness and
objectness. In Proceedings of the IEEE international confer-
ence on computer vision, pages 1976–1983, 2013. 1

[10] Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey,
and Michael Unser. Deep convolutional neural network for
inverse problems in imaging. IEEE Transactions on Image
Processing, 26(9):4509–4522, 2017. 2

[11] Kai Kang, Wanli Ouyang, Hongsheng Li, and Xiaogang
Wang. Object detection from video tubelets with convolu-
tional neural networks.
In IEEE Conference on Computer
Vision and Pattern Recognition, pages 817–825, 2016. 2, 3

[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, pages 1097–1105, 2012. 2, 4

[13] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep
visual tracking: Review and experimental comparison. Pat-
tern Recognition, 76:323–338, 2018. 2, 3

[14] Tsung Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Dollar. Focal loss for dense object detection.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
PP(99):2999–3007, 2017. 2

[15] Renting Liu, Zhaorong Li, and Jiaya Jia. Image partial blur
detection and classiﬁcation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1–8, 2008. 1

[16] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Ful-
ly convolutional networks for semantic segmentation.
In
IEEE conference on computer vision and pattern recogni-
tion, pages 3431–3440, 2015. 2

[17] Belen Masia, Adrian Corrales, Lara Presa, and Diego Gutier-
rez. Coded apertures for defocus deblurring. In Symposium
Iberoamericano de Computacion Graﬁca, volume 5, 2011. 1
[18] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmenta-
tion. In IEEE International Conference on Computer Vision,
pages 1520–1528, 2015. 2

[19] Y. Pang, H. Zhu, X. Li, and X. Li. Classifying discriminative
features for blur detection. IEEE Transactions on Cybernet-
ics, 46(10):2220–2227, 2015. 1, 2

[20] Yanwei Pang, Hailong Zhu, Xinyu Li, and Xuelong Li.
Classifying discriminative features for blur detection. IEEE
Transactions on Cybernetics, 46(10):2220–2227, 2016. 6

[21] Jinsun Park, Yu Wing Tai, Donghyeon Cho, and In So K-
weon. A uniﬁed approach of multi-scale deep and hand-
crafted features for defocus estimation. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 2760–
2769, 2017. 1, 2, 3, 6

[22] Kuldeep Purohit, Anshul B Shah, and AN Rajagopalan.
Learning based single image blur detection and segmenta-
tion. In 2018 25th IEEE International Conference on Image
Processing (ICIP), pages 2202–2206. IEEE, 2018. 3

[23] Yuankai Qi, Shengping Zhang, Lei Qin, Qingming Huang,
Hongxun Yao, Jongwoo Lim, and Ming-Hsuan Yang. Hedg-
ing deep features for visual tracking. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2018. 2

[24] E Saad and K Hirakawa. Defocus blur-invariant scale-space
feature extractions. IEEE Transactions on Image Processing,
25(7):3141–3156, 2016. 1

[25] Jianping Shi, Li Xu, and Jiaya Jia. Discriminative blur detec-
tion features. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 2965–2972, 2014. 1, 2, 6

[26] Jianping Shi, Li Xu, and Jiaya Jia. Just noticeable defocus
blur detection and estimation. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 657–665, 2015.
3, 6

[27] Assaf Shocher, Nadav Cohen, and Michal Irani. “zero-shot”
super-resolution using deep internal learning. In IEEE Con-
ference on computer vision and pattern recognition, pages
3118–3126, 2018. 2, 3

[28] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In Inter-
national Conference on Representation Learning, 2015. 2, 3,
5

[29] Bolan Su, Shijian Lu, and Chew Lim Tan. Blurred image
In ACM International
region detection and classiﬁcation.
Conference on Multimedia, pages 1397–1400, 2011. 1, 2, 6
[30] Chong Sun, Huchuan Lu, and Ming-Hsuan Yang. Learning
spatial-aware regressions for visual tracking. In IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
8962–8970, 2018. 2

[31] Yu-Wing Tai and Michael S Brown. Single image defocus
map estimation using local contrast prior. In IEEE Interna-

2708

[47] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network.
In
IEEE conference on computer vision and pattern recogni-
tion, pages 2881–2890, 2017. 2, 3

[48] Wenda Zhao, Fan Zhao, Dong Wang, and Huchuan Lu. De-
focus blur detection via multi-stream bottom-top-bottom ful-
ly convolutional network. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 3080–3088, 2018. 1,
2, 3, 5, 6

[49] X. Zhu, S Cohen, S Schiller, and P Milanfar. Estimating spa-
tially varying defocus blur from a single image. IEEE Trans-
actions on Image Processing, 22(12):4879–4891, 2013. 1

[50] Shaojie Zhuo and Terence Sim. Defocus map estimation
from a single image. Pattern Recognition, 44(9):1852–1858,
2011. 1, 2

tional Conference on Image Processing, pages 1797–1800.
IEEE, 2009. 2

[32] Chang Tang, Chunping Hou, Yonghong Hou, Pichao Wang,
and Wanqing Li. An effective edge-preserving smoothing
method for image manipulation. Digital Signal Processing,
63:10–24, 2017. 1, 2

[33] Chang Tang, Chunping Hou, and Zhanjie Song. Defocus
map estimation from a single image via spectrum contrast.
Optics letters, 38(10):1706–1708, 2013. 1, 2

[34] Chang Tang, Pichao Wang, Changqing Zhang, and Wanqing
Li. Salient object detection via weighted low rank matrix
recovery.
IEEE Signal Processing Letters, 24(4):490–494,
2017. 1

[35] Chang Tang, Jin Wu, Yonghong Hou, Pichao Wang, and
Wanqing Li. A spectral and spatial approach of coarse-to-
ﬁne blurred image region detection. IEEE Signal Processing
Letters, 23(11):1652–1656, 2016. 1, 6

[36] Chang Tang, Jin Wu, Yonghong Hou, Pichao Wang, and
Wanqing Li. A spectral and spatial approach of coarse-to-
ﬁne blurred image region detection. IEEE Signal Processing
Letters, 23(11):1652–1656, 2016. 3

[37] Cuong T Vu, Thien D Phan, and Damon M Chandler. s3:
A spectral and spatial measure of local perceived sharpness
in natural images. IEEE Transactions on Image Processing,
21(3):934, 2012. 1

[38] Xin Wang, Baofeng Tian, Chao Liang, and Dongcheng Shi.
Blind image quality assessment for measuring image blur. In
Congress on Image and Signal Processing, pages 467–470.
IEEE, 2008. 1

[39] Guodong Xu, Yuhui Quan, and Hui Ji. Estimating defocus
blur via rank of local patches. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), Venice, Italy,
pages 22–29, 2017. 3

[40] Ruomei Yan and Ling Shao. Blind image blur estimation
via deep learning. IEEE Transactions on Image Processing,
25(4):1910–1921, 2016. 2

[41] Xin Yi and Mark Eramian. Lbp-based segmentation of
IEEE Transactions on Image Processing,

defocus blur.
25(4):1626–1638, 2016. 1, 3, 6

[42] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learning of
deep cnn for image denoising. IEEE Transactions on Image
Processing, 26(7):3142–3155, 2017. 2

[43] Shanghang Zhang, Xiaohui Shen, Zhe Lin, Radomır Mech,
Joao P Costeira, and Jos´e MF Moura. Learning to understand
image blur.
In IEEE Conference on Computer Vision and
Pattern Recognition, pages 6586–6595, 2018. 3

[44] Wei Zhang and Wai-Kuen Cham. Single image focus edit-
ing. In Computer Vision Workshops (ICCV Workshops), 2009
IEEE 12th International Conference on, pages 1947–1954.
IEEE, 2009. 1

[45] Wei Zhang and Wai-Kuen Cham. Single-image refocusing
IEEE Transactions on Image Processing,

and defocusing.
21(2):873–882, 2012. 1

[46] Yi Zhang and Keigo Hirakawa. Blur processing using double
discrete wavelet transform. In IEEE Conference on Comput-
er Vision and Pattern Recognition, pages 1091–1098, 2013.
1

2709

