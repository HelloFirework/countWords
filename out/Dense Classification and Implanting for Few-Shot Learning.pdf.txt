Dense Classiﬁcation and Implanting for Few-Shot Learning

Yann Lifchitz1,2 Yannis Avrithis1

Sylvaine Picard2 Andrei Bursuc3

1Univ Rennes, Inria, CNRS, IRISA

2Safran

3valeo.ai

Abstract

Few-shot learning for deep neural networks is a highly
challenging and key problem in many computer vision tasks.
In this context, we are targeting knowledge transfer from
a set with abundant data to other sets with few available
examples. We propose two simple and effective solutions:
(i) dense classiﬁcation over feature maps, which for the
ﬁrst time studies local activations in the domain of few-
shot learning, and (ii) implanting, that is, attaching new
neurons to a previously trained network to learn new, task-
speciﬁc features.
Implanting enables training of multiple
layers in the few-shot regime, departing from most related
methods derived from metric learning that train only the ﬁ-
nal layer. Both contributions show consistent gains when
used individually or jointly and we report state of the art
performance on few-shot classiﬁcation on miniImageNet.

1. Introduction

Current state of the art on image classiﬁcation [40,
11, 15], object detection [21, 35, 10], semantic segmenta-
tion [51, 2, 20], and practically most tasks with some degree
of learning involved, rely on deep neural networks. Those
are powerful high-capacity models with trainable parame-
ters ranging from millions to tens of millions, which require
vast amounts of annotated data to ﬁt. When such data is
plentiful, supervised learning is the solution of choice.

Tasks and classes with limited available data, i.e. from
the long-tail [47], are highly problematic for this type of ap-
proaches. The performance of deep neural networks poses
several challenges in the low-data regime, in particular in
terms of overﬁtting and generalization. The subject of few-
shot learning is to learn to recognize previously unseen
classes with very few annotated examples. This is not a
new problem [4], yet there is a recent resurgence in interest
through meta-learning [18, 43, 38, 1, 5] inspired by early
work in learning-to-learn [42, 13].

In meta-learning settings, even when there is single large
training set with a ﬁxed number of class, it is treated as a
collection of datasets of different classes, where each class
has a few annotated examples. This is done so that both

meta-learning and meta-testing are performed in a similar
manner [43, 38, 5]. However this choice does not always
come with best performance. We argue that a simple con-
ventional pipeline using all available classes and data with
a parametric classiﬁer is effective and appealing.

Most few-shot learning approaches do not deal explicitly
with spatial information since feature maps are usually ﬂat-
tened or pooled before the classiﬁcation layer. We show that
performing dense classiﬁcation over feature maps leads to
more precise classiﬁcation and consistently improves per-
formance on standard benchmarks.

While incremental learning touches similar aspects with
few-shot learning by learning to adapt to new tasks using
the same network [26, 25] or extending an existing network
with new layers and parameters for each new task [37], few
of these ideas have been adopted in few shot learning. The
main impediment is the reduced number of training exam-
ples which make it difﬁcult to properly deﬁne a new task.
We propose a solution for leveraging incremental learning
ideas for few-shot learning.

Contributions: We make the following contributions.
First, we propose a simple extension for few-shot learn-
ing pipelines consisting of dense classiﬁcation over feature
maps. Through localized supervision, it enables reaping ad-
ditional knowledge from the limited training data. Second,
we introduce neural implants, which are layers attached to
an already trained network, enabling it to quickly adapt to
new tasks with few examples. Both are easy to implement
and show consistent performance gains.

2. Problem formulation and background

Problem formulation. We are given a collection of train-
ing examples X := (x1, . . . , xn) with each xi ∈ X , and
corresponding labels Y := (y1, . . . , yn) with each yi ∈ C,
where C := [c]1 is a set of base classes. On this training
data we are allowed to learn a representation of the domain
X such that we can solve new tasks. This representation
learning we shall call stage 1.

In few-shot learning, one new task is that we are given a
n′ )

collection of few support examples X ′

1, . . . , x

:= (x

′

′

1We use the notation [i] := {1, . . . , i} for i ∈ N.

19258

′

1, . . . , y′

n′ ) with each y′

i ∈ X , and corresponding labels Y ′

:=
with each x
i ∈ C ′, where C ′ := [c′] is a set
(y′
of novel classes disjoint from C and n′ ≪ n; with this new
data, the objective is to learn a classiﬁer that maps a new
query example from X to a label prediction in C ′. The lat-
ter classiﬁer learning, which does not exclude continuing
the representation learning, we shall call stage 2.

Classiﬁcation is called c′-way where c′ is the number of
novel classes; in case there is a ﬁxed number k of support
examples per novel class, it is called k-shot. As in stan-
dard classiﬁcation, there is typically a collection of queries
for evaluation of each task. Few-shot learning is typically
evaluated on a large number of new tasks, with queries and
support examples randomly sampled from (X ′, Y ′).

Network model. We consider a model that is conceptually
composed of two parts: an embedding network and a classi-
ﬁer. The embedding network φθ : X → Rr×d maps the in-
put to an embedding, where θ denotes its parameters. Since
we shall be studying the spatial properties of the input, the
embedding is not a vector but rather a tensor, where r rep-
resents the spatial dimensions and d the feature dimensions.
For a 2d input image and a convolutional network for in-
stance, the embedding is a 3d tensor in Rw×h×d taken as the
activation of the last convolutional layer, where r = w × h
is its spatial resolution. The embedding can still be a vector
in the special case r = 1.

The classiﬁer network can be of any form and depends
on the particular model, but it is applied on top of φθ and
its output represents conﬁdence over c (resp. c′) base (resp.
novel) classes. If we denote by fθ : X → Rc (resp. Rc′
)
the network function mapping the input to class conﬁdence,
then a prediction for input x ∈ X is made by assigning the
label of maximum conﬁdence, arg maxi f i
Prototypical networks. Snell et al. [41] introduce a simple
classiﬁer for novel classes that computes a single prototype
per class and then classiﬁes a query to the nearest prototype.
More formally, given X ′, Y ′ and an index set S ⊂ N ′ :=
[n′], let the set Sj := {i ∈ S : y′
i = j} index the support
examples in S labeled in class j. The prototype of class j is
given by the average of those examples

θ(x)2.

pj =

1

|Sj| Xi∈Sj

φθ(x

′

i)

for j ∈ C ′. Then, the network function is deﬁned as3

fθ[P ](x) := σ(cid:16)[s(φθ(x), pj)]c′

j=1(cid:17)

(1)

(2)

for x ∈ X , where P := (p1, . . . , pc′ ) and s is a similarity
function that may be cosine similarity or negative squared

2Given vector x ∈ Rm, xi denotes the i-th element of x. Similarly for

f : A → Rm, f i(a) denotes the i-the element of f (a) for a ∈ A.

3We deﬁne [e(i)]n

i=1 := (e(1), . . . , e(n)) for n ∈ N and any expres-

sion e(i) of variable i ∈ N.

Euclidean distance and σ : Rm → Rm is the softmax func-
tion deﬁned by

σ(x) := (cid:20)

for x ∈ Rm and m ∈ N.

exp(xj)

i=1 exp(xi)(cid:21)m
Pm

j=1

(3)

Given a new task with support data (X ′, Y ′) over novel
classes C ′ (stage 2), the full index set N ′ is used and com-
puting class prototypes (1) is the only learning to be done.
When learning from the training data (X, Y ) over base
classes C (stage 1), a number of ﬁctitious tasks called
episodes are generated by randomly sampling a number
classes from C and then a number of examples in each class
from X with their labels from Y ; these collections, denoted
as X ′, Y ′ respectively and of length n′, are supposed to be
support examples and queries of novel classes C ′, where
labels are now available for the queries and the objective
is that queries are classiﬁed correctly. The set N ′ := [n′]
is partitioned into a support set S ⊂ N ′ and a query set
Q := N ′ \ S. Class prototypes P are computed on index
set S according to (1) and the network function fθ is deﬁned
on these prototypes by (2). The network is then trained by
minimizing over θ the cost function

J(X ′, Y ′; θ) := Xi∈Q

ℓ(fθ[P ](x

′

i), y′
i)

(4)

on the query set Q, where ℓ is the cross-entropy loss

ℓ(a, y) := − log ay

(5)

for a ∈ Rm, y ∈ [m] and m ∈ N.
Learning with imprinted weights. Qi et al. [31] follow a
simpler approach when learning on the training data (X, Y )
over base classes C (stage 1). In particular, they use a fully-
connected layer without bias as a parametric linear classi-
ﬁer on top of the embedding function φθ followed by soft-
max and they train in a standard supervised classiﬁcation
setting. More formally, let wj ∈ Rr×d be the weight pa-
rameter of class j for j ∈ C. Then, similarly to (2), the
network function is deﬁned by

fθ,W (x) := σ(cid:0)[sτ (φθ(x), wj)]c

j=1(cid:1)

(6)

for x ∈ X , where W := (w1, . . . , wc) is the collection of
class weights and sτ is the scaled cosine similarity

sτ (x, y) := τ hˆx, ˆyi

(7)

for x, y ∈ Rr×d; ˆx := x/ kxk is the ℓ2-normalized coun-
terpart of x for x ∈ Rr×d; h·, ·i and k·k denote Frobenius
inner product and norm respectively; and τ ∈ R+ is a train-
able scale parameter. Then, training amounts to minimizing

9259

over θ, W the cost function

feature (d)

class weights

J(X, Y ; θ, W ) :=

n

Xi=1

ℓ(fθ,W (xi), yi).

(8)

Given a new task with support data (X ′, Y ′) over novel
classes C ′ (stage 2), class prototypes P are computed on N ′
according to (1) and they are imprinted in the classiﬁer, that
is, W is replaced by W ′ := (W, P ). The network can now
make predictions on n + n′ base and novel classes. The
network is then ﬁne-tuned based on (8), which aligns the
class weights W with the prototypes P at the cost of having
to store and re-train on the entire training data (X, Y ).

Few-shot learning without forgetting. Gidaris and Ko-
modakis [6], concurrently with [31], develop a similar
model that is able to classify examples of both base and
novel classes. The main difference to [31] is that only the
weight parameters of the base classes are stored and not the
entire training data. They use the same parametric linear
classiﬁer as [31] in both stages, and they also use episode-
style training like [41] in stage 2.

3. Method

Given training data of base classes (stage 1), we use a
parametric classiﬁer like [31, 6], which however applies at
all spatial locations rather than following ﬂattening or pool-
ing; a very simple idea that we call dense classiﬁcation and
discuss in §3.1. Given support data of novel classes (stage
2), we learn in episodes as in prototypical networks [41],
but on the true task. As discussed in §3.2, the embedding
network learned in stage 1 remains ﬁxed but new layers
called implants are trained to learn task-speciﬁc features.
Finally, §3.3 discusses inference of novel class queries.

3.1. Dense classiﬁcation

As discussed in §2, the embedding network φθ : X →
Rr×d maps the input to an embedding that is a tensor. There
are two common ways of handling this high-dimensional
representation, as illustrated in Figure 1.

The ﬁrst is to apply one or more fully connected layers,
for instance in networks C64F, C128F in few-shot learn-
ing [43, 41, 6]. This can be seen as ﬂattening the activation
into a long vector and multiplying with a weight vector of
the same length per class; alternatively, the weight param-
eter is a tensor of the same dimension as the embedding.
This representation is discriminative, but not invariant.

The second way is to apply global pooling and reduce the
embedding into a smaller vector of length d, for instance in
small ResNet architectures used more recently in few-shot
learning [27, 6, 30]. This reduces dimensionality signiﬁ-
cantly, so it makes sense if d is large enough. It is an invari-
ant representation, but less discriminative.

s
p
a
t
i
a
l

(
r
)

s
p
a
t
i
a
l

(
r
)

φ(x)

w1

w2

w3

s

(a)

σ

ℓ

class weights

w1

w2

w3

s

(b)

σ

ℓ

feature (d)

φ(x)

Σ

a

Figure 1. Flattening and pooling. Horizontal (vertical) axis repre-
sents feature (spatial) dimensions. Tensors w1, w2, w3 represent
class weights, and φ(x) the embedding of example x. An embed-
ding is compared to class weights by similarity (s) and then soft-
max (σ) and cross-entropy (ℓ) follow. (a) Flattening is equivalent
to class weights having the same r × d shape as φ(x). (b) Global
pooling. Embedding φ(x) is pooled (Σ) into vector a ∈ Rd before
being compared to class weights, which are in Rd too.

In this work we follow a different approach that we
call dense classiﬁcation and is illustrated in Figure 2.
We view the embedding φθ(x) as a collection of vectors
[φ(k)(x)]r
k=1, where φ(k)(x) ∈ Rd for k ∈ [r]4. For a 2d
image input and a convolutional network, φθ(x) consists of
the activations of the last convolutional layer, that is a tensor
in Rw×h×d where r = w × h is its spatial resolution. Then,
φ(k)(x) is an embedding in Rd that represents a single spa-
tial location k on the tensor.

When learning from the training data (X, Y ) over base
classes C (stage 1), we adopt the simple approach of train-
ing a parametric linear classiﬁer on top of the embedding
function φθ, like [31] and the initial training of [6]. The
main difference in our case is that the weight parameters
do not have the same dimensions as φθ(x); they are rather
vectors in Rd and they are shared over all spatial locations.
More formally, let wj ∈ Rd be the weight parameter of
class j for j ∈ C. Then, similarly to (6), the classiﬁer map-
ping fθ,W : X → Rr×c is deﬁned by

4Given tensor a ∈ Rm×n, denote by a(k) the k-th n-dimensional

slice along the ﬁrst group of dimensions for k ∈ [m].

9260

feature (d)

s
p
a
t
i
a
l

(
r
)

φ(x)

φ(1)(x)

φ(2)(x)

φ(3)(x)

class weights

w1

w1

w1

w2

w2

w2

w3

w3

w3

s

s

s

σ

σ

σ

ℓ

ℓ

ℓ

+

Figure 2. Dense classiﬁcation. Notation is the same as in Figure 1. The embedding a := φ(x) ∈ Rr×d is seen as a collection of vectors
(r)) in Rd (here r = 3) with each being a vector in Rd and representing a region of the input image. Each vector is compared
(a
independently to the same class weights and the losses are added, encouraging all regions to be correctly classiﬁed.

(1), . . . , a

tor in Rc representing conﬁdence over the c classes. On the
other hand, f (:,j)
θ,W (x) is a vector in Rr representing conﬁ-
dence of class j for j ∈ [c] as a function of spatial location.6
For a 2d image input, f (:,j)
θ,W (x) is like a class activation map
(CAM) [53] for class j, that is a 2d map roughly localizing
the response to class j, but differs in that softmax suppresses
all but the strongest responses at each location.

Given the deﬁnition (9) of fθ,W , training amounts to

minimizing over θ, W the cost function

J(X, Y ; θ, W ) :=

n

r

Xi=1

Xk=1

ℓ(f (k)

θ,W (xi), yi),

(10)

where ℓ is cross-entropy (5). The loss function applies to all
spatial locations and therefore the classiﬁer is encouraged
to make correct predictions everywhere.

Learning a new task with support data (X ′, Y ′) over
novel classes C ′ (stage 2) and inference are discussed
in §3.2.2 and §3.3 respectively.
Discussion. The same situation arises in semantic segmen-
tation [23, 29], where given per-pixel labels, the loss func-
tion applies per pixel and the network learns to make local-
ized predictions on upsampled feature maps rather than just
classify. In our case there is just one image-level label and
the low resolution, e.g. 5 × 5, of few-shot learning settings
allows us to assume that the label applies to all locations
due to large receptive ﬁeld.

Dense classiﬁcation improves the spatial distribution of
class activations, as shown in Figure 3. By encouraging all
spatial locations to be classiﬁed correctly, we are encourag-
ing the embedding network to identify all parts of the object
of interest rather than just the most discriminative details.
Since each location on a feature map corresponds to a re-
gion in the image where only part of the object may be vis-
ible, our model behaves like implicit data augmentation of
exhaustive shifts and crops over a dense grid with a single
forward pass of each example in the network.

3.2. Implanting

From the learning on the training data (X, Y ) of base
classes C (stage 1) we only keep the embedding network

6Given tensor a ∈ Rm×n, denote by a(:,j) the j-th m-dimensional

slice along the second group of dimensions for j ∈ [n].

9261

pooling

dense

pooling

dense

Figure 3. Examples overlaid with correct class activation
maps [53] (red is high activation for ground truth) on Resnet-12
(cf . §5) trained with global average pooling or dense classiﬁcation
(cf . (9)). From top to bottom: base classes, classiﬁed correctly by
both (walker hound, tile roof); novel classes, classiﬁed correctly
by both (king crab, ant); novel classes, dense classiﬁcation is bet-
ter (ferret, electric guitar); novel classes, pooling is better (mixing
bowl, ant). In all cases, dense classiﬁcation results in smoother
activation maps that are more aligned with objects.

fθ,W (x) := hσ(cid:16)[sτ (φ(k)

θ (x), wj)]c

j=1(cid:17)ir

k=1

(9)

for x ∈ X , where W := (w1, . . . , wc) is the collection of
class weights and sτ is the scaled cosine similarity deﬁned
by (7), with τ being a learnable parameter as in [31, 6]5.
Here fθ,W (x) is a r × c tensor: index k ranges over spatial
resolution [r] and j over classes [c].

This operation is a 1 × 1 convolution followed by depth-
θ,W (x) at spatial location k is a vec-

wise softmax. Then, f (k)

5Temperature scaling is frequently encountered in various formats in
several works to enable soft-labeling [12] or to improve cosine similarity
in the ﬁnal layer [45, 30, 6, 31, 14].

Forward all classes 

Forward novel classes 

Forward base classes 

Backprop novel classes 

Backprop base classes 

C
o
n
v

 
 

φ0

θ,θ0 (x)

C
o
n
v

 

fθ,θ0 (x)

i

l

m
p
a
n
t
 

i

l

m
p
a
n
t
 

CNN 

C
o
n
v

 

Image 

x<

φθ(x)

C
o
n
v

 

fθ(x)

Figure 4. Neural implants for CNNs. The implants are convolutional ﬁlters operating in a new processing stream parallel to the base
network. The input of an implant is the depth-wise concatenation of hidden states from both streams. When training neural implants,
previously trained parameters are frozen. Purple and black arrows correspond to stage 1 ﬂows; red and black to stage 2.

φθ and we discard the classiﬁcation layer. The assumption
is that features learned on base classes are generic enough to
be used for other classes, at least for the bottom layers [50].
However, given a new few-shot task on novel classes C ′
(stage 2), we argue that we can take advantage of the sup-
port data (X ′, Y ′) to ﬁnd new features that are discrimina-
tive for the task at hand, at least in the top layers.

of new parameters, while preserving the previously trained
ones intact. Useful visual representations and parameters
learned from base classes can be quickly squashed dur-
ing ﬁne-tuning on the novel classes. With implants, we
freeze them and train only the new neurons added to the
network, maximizing the contribution of the knowledge ac-
quired from base classes.

3.2.1 Architecture

We begin with the embedding network φθ, which we call
base network. We widen this network by adding new con-
volution kernels in a number of its top convolutional layers.
We call these new neurons implants. While learning the im-
plants, we keep the base network parameters frozen, which
preserves the representation of the base classes.

Let al denote the output activation of the convolutional
layer l in the base network. The implant for this layer, if it
exists, is a distinct convolutional layer with output activa-
′
l. Then the input of an implant at the next layer l + 1
tion a
is the depth-wise concatenation [al, a
l exists, and just
al otherwise. If θ′
l are the parameters of the l-th implant,
then we denote by θ′ := (θ′
L) the set of all new
parameters, where l0 is the ﬁrst layer with an implant and
L the network depth. The widened embedding network is
denoted by φθ,θ′ .

l0 , . . . , θ′

l] if a

′

′

As illustrated in Figure 4, we are creating a new stream
of data in parallel to the base network. The implant stream
is connected to the base stream at multiple top layers and
leverages the previously learned features by learning addi-
tional connections for the new tasks.

Why implanting? In several few-shot learning works, in
particular metric learning, it is common to focus on the top
layer of the network and learn or generate a new classiﬁer
for the novel classes. The reason behind this choice un-
derpins a major challenge in few-shot learning: deep neu-
ral networks are prone to overﬁtting. With implanting, we
attempt to diminish this risk by adding a limited amount

3.2.2 Training

To learn the implants only makes sense when a new task
is given with support data (X ′, Y ′) over novel classes C ′
(stage 2). Here we use an approach similar to prototypical
networks [41] in the sense that we generate a number of
ﬁctitious subtasks of the new task, the main difference being
that we are now working on the novel classes.

We choose the simple approach of using each one of the
given examples alone as a query in one subtask while all the
rest are used as support examples. This involves no sam-
pling and the process is deterministic. Because only one
example is missing from the true support examples, each
subtask approximates the true task very well.

In particular, for each i ∈ N ′ := [n′], we deﬁne a query
set Qi := {i} and a support set Si := N ′ \ Qi. We com-
pute class prototypes Pi on index set Si according to (1),
where we replace φθ by φθ,θ′ and θ′ are the implanted pa-
rameters. We deﬁne the widened network function fθ,θ′ [Pi]
on these prototypes by (2) with a similar replacement. We
then freeze the base network parameters θ and train the im-
plants θ′ by minimizing a cost function like (4). Similarly
to (4) and taking all subtasks into account, the overall cost
function we are minimizing over θ′ is given by

J(X ′, Y ′; θ, θ′) :=

n′

Xi=1

ℓ(fθ,θ′ [Pi](x

′

i), y′

i),

(11)

where ℓ is cross-entropy (5).

In (11), activations are assumed ﬂattened or globally
pooled. Alternatively, we can densely classify them and ap-
ply the loss function to all spatial locations independently.

9262

l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
3
1
K
N
S
C
x
e
k
s
8
q
5
l
M
5
E
0
h
d
g
M
w
5
f
L
c
=
"
>
A
A
A
C
D
3
i
c
b
Z
C
7
T
s
M
w
F
I
Y
d
r
q
X
c
A
o
w
s
E
R
W
X
q
U
o
Q
E
o
w
V
L
I
x
F
o
h
c
p
j
S
r
H
c
V
q
r
v
k
S
2
A
1
R
R
3
o
C
F
V
2
F
h
A
C
F
W
V
j
b
e
B
q
f
N
A
C
1
H
s
v
T
p
P
x
e
f
8
4
c
J
J
U
q
7
7
r
e
1
s
L
i
0
v
L
J
a
W
a
u
u
b
2
x
u
b
d
s
7
u
2
0
l
U
o
l
w
C
w
k
q
Z
D
e
E
C
l
P
C
c
U
s
T
T
X
E
3
k
R
i
y
k
O
J
O
O
L
o
q
8
p
0
7
L
B
U
R
/
F
a
P
E
x
w
w
O
O
A
k
J
g
h
q
I
/
X
t
o
x
7
H
9
0
g
w
B
n
m
U
F
Z
z
7
X
p
B
l
N
S
8
/
z
q
s
9
B
v
U
w
j
L
O
H
v
G
/
X
3
L
o
7
C
W
c
e
v
B
J
q
o
I
x
m
3
/
7
q
R
Q
K
l
D
H
O
N
K
F
T
K
9
9
x
E
B
x
m
U
m
i
C
K
z
e
h
U
4
Q
S
i
E
R
x
g
3
y
C
H
D
K
s
g
m
9
y
T
O
4
d
G
i
Z
x
Y
S
P
O
4
d
i
b
q
7
4
4
M
M
q
X
G
L
D
S
V
x
Y
Z
q
N
l
e
I
/
+
X
8
V
M
c
X
Q
U
Z
4
k
m
r
M
0
f
S
j
O
K
W
O
F
k
5
h
j
h
M
R
i
Z
G
m
Y
w
M
Q
S
W
J
2
d
d
A
Q
S
o
i
0
s
b
B
q
T
P
B
m
T
5
6
H
9
m
n
d
M
3
x
z
V
m
t
c
l
n
Z
U
w
D
4
4
A
C
f
A
A
+
e
g
A
a
5
B
E
7
Q
A
A
o
/
g
G
b
y
C
N
+
v
J
e
r
H
e
r
Y
9
p
6
Y
J
V
9
u
y
B
P
2
F
9
/
g
B
0
G
J
z
m
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
3
1
K
N
S
C
x
e
k
s
8
q
5
l
M
5
E
0
h
d
g
M
w
5
f
L
c
=
"
>
A
A
A
C
D
3
i
c
b
Z
C
7
T
s
M
w
F
I
Y
d
r
q
X
c
A
o
w
s
E
R
W
X
q
U
o
Q
E
o
w
V
L
I
x
F
o
h
c
p
j
S
r
H
c
V
q
r
v
k
S
2
A
1
R
R
3
o
C
F
V
2
F
h
A
C
F
W
V
j
b
e
B
q
f
N
A
C
1
H
s
v
T
p
P
x
e
f
8
4
c
J
J
U
q
7
7
r
e
1
s
L
i
0
v
L
J
a
W
a
u
u
b
2
x
u
b
d
s
7
u
2
0
l
U
o
l
w
C
w
k
q
Z
D
e
E
C
l
P
C
c
U
s
T
T
X
E
3
k
R
i
y
k
O
J
O
O
L
o
q
8
p
0
7
L
B
U
R
/
F
a
P
E
x
w
w
O
O
A
k
J
g
h
q
I
/
X
t
o
x
7
H
9
0
g
w
B
n
m
U
F
Z
z
7
X
p
B
l
N
S
8
/
z
q
s
9
B
v
U
w
j
L
O
H
v
G
/
X
3
L
o
7
C
W
c
e
v
B
J
q
o
I
x
m
3
/
7
q
R
Q
K
l
D
H
O
N
K
F
T
K
9
9
x
E
B
x
m
U
m
i
C
K
z
e
h
U
4
Q
S
i
E
R
x
g
3
y
C
H
D
K
s
g
m
9
y
T
O
4
d
G
i
Z
x
Y
S
P
O
4
d
i
b
q
7
4
4
M
M
q
X
G
L
D
S
V
x
Y
Z
q
N
l
e
I
/
+
X
8
V
M
c
X
Q
U
Z
4
k
m
r
M
0
f
S
j
O
K
W
O
F
k
5
h
j
h
M
R
i
Z
G
m
Y
w
M
Q
S
W
J
2
d
d
A
Q
S
o
i
0
s
b
B
q
T
P
B
m
T
5
6
H
9
m
n
d
M
3
x
z
V
m
t
c
l
n
Z
U
w
D
4
4
A
C
f
A
A
+
e
g
A
a
5
B
E
7
Q
A
A
o
/
g
G
b
y
C
N
+
v
J
e
r
H
e
r
Y
9
p
6
Y
J
V
9
u
y
B
P
2
F
9
/
g
B
0
G
J
z
m
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
3
1
K
N
S
C
x
e
k
s
8
q
5
l
M
5
E
0
h
d
g
M
w
5
f
L
c
=
"
>
A
A
A
C
D
3
i
c
b
Z
C
7
T
s
M
w
F
I
Y
d
r
q
X
c
A
o
w
s
E
R
W
X
q
U
o
Q
E
o
w
V
L
I
x
F
o
h
c
p
j
S
r
H
c
V
q
r
v
k
S
2
A
1
R
R
3
o
C
F
V
2
F
h
A
C
F
W
V
j
b
e
B
q
f
N
A
C
1
H
s
v
T
p
P
x
e
f
8
4
c
J
J
U
q
7
7
r
e
1
s
L
i
0
v
L
J
a
W
a
u
u
b
2
x
u
b
d
s
7
u
2
0
l
U
o
l
w
C
w
k
q
Z
D
e
E
C
l
P
C
c
U
s
T
T
X
E
3
k
R
i
y
k
O
J
O
O
L
o
q
8
p
0
7
L
B
U
R
/
F
a
P
E
x
w
w
O
O
A
k
J
g
h
q
I
/
X
t
o
x
7
H
9
0
g
w
B
n
m
U
F
Z
z
7
X
p
B
l
N
S
8
/
z
q
s
9
B
v
U
w
j
L
O
H
v
G
/
X
3
L
o
7
C
W
c
e
v
B
J
q
o
I
x
m
3
/
7
q
R
Q
K
l
D
H
O
N
K
F
T
K
9
9
x
E
B
x
m
U
m
i
C
K
z
e
h
U
4
Q
S
i
E
R
x
g
3
y
C
H
D
K
s
g
m
9
y
T
O
4
d
G
i
Z
x
Y
S
P
O
4
d
i
b
q
7
4
4
M
M
q
X
G
L
D
S
V
x
Y
Z
q
N
l
e
I
/
+
X
8
V
M
c
X
Q
U
Z
4
k
m
r
M
0
f
S
j
O
K
W
O
F
k
5
h
j
h
M
R
i
Z
G
m
Y
w
M
Q
S
W
J
2
d
d
A
Q
S
o
i
0
s
b
B
q
T
P
B
m
T
5
6
H
9
m
n
d
M
3
x
z
V
m
t
c
l
n
Z
U
w
D
4
4
A
C
f
A
A
+
e
g
A
a
5
B
E
7
Q
A
A
o
/
g
G
b
y
C
N
+
v
J
e
r
H
e
r
Y
9
p
6
Y
J
V
9
u
y
B
P
2
F
9
/
g
B
0
G
J
z
m
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
3
1
K
N
S
C
x
e
k
s
8
q
5
l
M
5
E
0
h
d
g
M
w
5
f
L
c
=
"
>
A
A
A
C
D
3
i
c
b
Z
C
7
T
s
M
w
F
I
Y
d
r
q
X
c
A
o
w
s
E
R
W
X
q
U
o
Q
E
o
w
V
L
I
x
F
o
h
c
p
j
S
r
H
c
V
q
r
v
k
S
2
A
1
R
R
3
o
C
F
V
2
F
h
A
C
F
W
V
j
b
e
B
q
f
N
A
C
1
H
s
v
T
p
P
x
e
f
8
4
c
J
J
U
q
7
7
r
e
1
s
L
i
0
v
L
J
a
W
a
u
u
b
2
x
u
b
d
s
7
u
2
0
l
U
o
l
w
C
w
k
q
Z
D
e
E
C
l
P
C
c
U
s
T
T
X
E
3
k
R
i
y
k
O
J
O
O
L
o
q
8
p
0
7
L
B
U
R
/
F
a
P
E
x
w
w
O
O
A
k
J
g
h
q
I
/
X
t
o
x
7
H
9
0
g
w
B
n
m
U
F
Z
z
7
X
p
B
l
N
S
8
/
z
q
s
9
B
v
U
w
j
L
O
H
v
G
/
X
3
L
o
7
C
W
c
e
v
B
J
q
o
I
x
m
3
/
7
q
R
Q
K
l
D
H
O
N
K
F
T
K
9
9
x
E
B
x
m
U
m
i
C
K
z
e
h
U
4
Q
S
i
E
R
x
g
3
y
C
H
D
K
s
g
m
9
y
T
O
4
d
G
i
Z
x
Y
S
P
O
4
d
i
b
q
7
4
4
M
M
q
X
G
L
D
S
V
x
Y
Z
q
N
l
e
I
/
+
X
8
V
M
c
X
Q
U
Z
4
k
m
r
M
0
f
S
j
O
K
W
O
F
k
5
h
j
h
M
R
i
Z
G
m
Y
w
M
Q
S
W
J
2
d
d
A
Q
S
o
i
0
s
b
B
q
T
P
B
m
T
5
6
H
9
m
n
d
M
3
x
z
V
m
t
c
l
n
Z
U
w
D
4
4
A
C
f
A
A
+
e
g
A
a
5
B
E
7
Q
A
A
o
/
g
G
b
y
C
N
+
v
J
e
r
H
e
r
Y
9
p
6
Y
J
V
9
u
y
B
P
2
F
9
/
g
B
0
G
J
z
m
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
D
6
n
M
D
a
W
O
e
e
i
H
7
F
5
s
2
p
V
r
K
7
H
S
1
t
Y
=
"
>
A
A
A
C
T
H
i
c
b
Z
B
L
S
8
Q
w
F
I
X
T
8
T
2
+
R
l
2
6
K
Q
6
i
g
k
g
r
g
i
4
F
N
y
4
V
H
B
W
m
Z
U
g
z
t
0
4
w
S
U
t
y
q
w
6
h
P
9
C
N
C
3
f
+
C
j
c
u
F
B
F
M
Z
0
b
w
d
S
H
w
c
c
4
J
u
T
l
J
L
r
j
B
I
H
j
y
a
m
P
j
E
5
N
T
0
z
P
1
2
b
n
5
h
c
X
G
0
v
K
5
y
Q
r
N
o
M
U
y
k
e
n
L
h
B
o
Q
X
E
E
L
O
Q
q
4
z
D
V
Q
m
Q
i
4
S
K
6
P
K
v
/
i
B
r
T
h
m
T
r
D
f
g
6
x
p
F
e
K
p
5
x
R
d
F
K
n
w
S
I
F
t
y
y
T
k
q
q
u
r
b
h
s
h
7
G
1
z
b
D
c
K
O
t
R
3
u
M
d
G
2
E
P
k
J
a
b
k
a
T
Y
S
1
J
7
V
2
7
V
1
6
u
o
r
f
z
y
K
7
A
9
l
I
b
h
H
+
l
O
o
x
n
s
B
I
P
x
/
0
I
4
g
i
Y
Z
z
U
m
n
8
R
h
1
M
1
Z
I
U
M
g
E
N
a
Y
d
B
j
n
G
l
m
r
k
T
I
D
b
q
z
C
Q
U
3
Z
N
r
6
D
t
U
F
E
J
J
r
a
D
M
k
p
/
3
S
l
d
P
8
2
0
O
w
r
9
g
f
r
9
h
q
X
S
m
L
5
M
X
L
J
a
0
f
z
2
K
v
E
/
r
1
1
g
e
h
B
b
r
v
I
C
Q
b
H
h
Q
2
k
h
f
M
z
8
q
l
m
/
y
z
U
w
F
H
0
H
l
G
n
u
d
v
V
Z
j
2
r
K
0
P
V
f
d
y
W
E
v
7
/
8
F
8
5
3
d
0
L
H
p
3
v
N
w
+
N
R
H
d
N
k
l
a
y
R
T
R
K
S
f
X
J
I
j
s
k
J
a
R
F
G
7
s
k
z
e
S
V
v
3
o
P
3
4
r
1
7
H
8
N
o
z
R
v
d
W
S
E
/
p
j
b
5
C
Z
R
y
t
g
Q
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
D
6
n
M
D
a
W
O
e
e
i
H
7
F
5
s
2
p
V
r
K
7
H
S
1
t
Y
=
"
>
A
A
A
C
T
H
i
c
b
Z
B
L
S
8
Q
w
F
I
X
T
8
T
2
+
R
l
2
6
K
Q
6
i
g
k
g
r
g
i
4
F
N
y
4
V
H
B
W
m
Z
U
g
z
t
0
4
w
S
U
t
y
q
w
6
h
P
9
C
N
C
3
f
+
C
j
c
u
F
B
F
M
Z
0
b
w
d
S
H
w
c
c
4
J
u
T
l
J
L
r
j
B
I
H
j
y
a
m
P
j
E
5
N
T
0
z
P
1
2
b
n
5
h
c
X
G
0
v
K
5
y
Q
r
N
o
M
U
y
k
e
n
L
h
B
o
Q
X
E
E
L
O
Q
q
4
z
D
V
Q
m
Q
i
4
S
K
6
P
K
v
/
i
B
r
T
h
m
T
r
D
f
g
6
x
p
F
e
K
p
5
x
R
d
F
K
n
w
S
I
F
t
y
y
T
k
q
q
u
r
b
h
s
h
7
G
1
z
b
D
c
K
O
t
R
3
u
M
d
G
2
E
P
k
J
a
b
k
a
T
Y
S
1
J
7
V
2
7
V
1
6
u
o
r
f
z
y
K
7
A
9
l
I
b
h
H
+
l
O
o
x
n
s
B
I
P
x
/
0
I
4
g
i
Y
Z
z
U
m
n
8
R
h
1
M
1
Z
I
U
M
g
E
N
a
Y
d
B
j
n
G
l
m
r
k
T
I
D
b
q
z
C
Q
U
3
Z
N
r
6
D
t
U
F
E
J
J
r
a
D
M
k
p
/
3
S
l
d
P
8
2
0
O
w
r
9
g
f
r
9
h
q
X
S
m
L
5
M
X
L
J
a
0
f
z
2
K
v
E
/
r
1
1
g
e
h
B
b
r
v
I
C
Q
b
H
h
Q
2
k
h
f
M
z
8
q
l
m
/
y
z
U
w
F
H
0
H
l
G
n
u
d
v
V
Z
j
2
r
K
0
P
V
f
d
y
W
E
v
7
/
8
F
8
5
3
d
0
L
H
p
3
v
N
w
+
N
R
H
d
N
k
l
a
y
R
T
R
K
S
f
X
J
I
j
s
k
J
a
R
F
G
7
s
k
z
e
S
V
v
3
o
P
3
4
r
1
7
H
8
N
o
z
R
v
d
W
S
E
/
p
j
b
5
C
Z
R
y
t
g
Q
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
D
6
n
M
D
a
W
O
e
e
i
H
7
F
5
s
2
p
V
r
K
7
H
S
1
t
Y
=
"
>
A
A
A
C
T
H
i
c
b
Z
B
L
S
8
Q
w
F
I
X
T
8
T
2
+
R
l
2
6
K
Q
6
i
g
k
g
r
g
i
4
F
N
y
4
V
H
B
W
m
Z
U
g
z
t
0
4
w
S
U
t
y
q
w
6
h
P
9
C
N
C
3
f
+
C
j
c
u
F
B
F
M
Z
0
b
w
d
S
H
w
c
c
4
J
u
T
l
J
L
r
j
B
I
H
j
y
a
m
P
j
E
5
N
T
0
z
P
1
2
b
n
5
h
c
X
G
0
v
K
5
y
Q
r
N
o
M
U
y
k
e
n
L
h
B
o
Q
X
E
E
L
O
Q
q
4
z
D
V
Q
m
Q
i
4
S
K
6
P
K
v
/
i
B
r
T
h
m
T
r
D
f
g
6
x
p
F
e
K
p
5
x
R
d
F
K
n
w
S
I
F
t
y
y
T
k
q
q
u
r
b
h
s
h
7
G
1
z
b
D
c
K
O
t
R
3
u
M
d
G
2
E
P
k
J
a
b
k
a
T
Y
S
1
J
7
V
2
7
V
1
6
u
o
r
f
z
y
K
7
A
9
l
I
b
h
H
+
l
O
o
x
n
s
B
I
P
x
/
0
I
4
g
i
Y
Z
z
U
m
n
8
R
h
1
M
1
Z
I
U
M
g
E
N
a
Y
d
B
j
n
G
l
m
r
k
T
I
D
b
q
z
C
Q
U
3
Z
N
r
6
D
t
U
F
E
J
J
r
a
D
M
k
p
/
3
S
l
d
P
8
2
0
O
w
r
9
g
f
r
9
h
q
X
S
m
L
5
M
X
L
J
a
0
f
z
2
K
v
E
/
r
1
1
g
e
h
B
b
r
v
I
C
Q
b
H
h
Q
2
k
h
f
M
z
8
q
l
m
/
y
z
U
w
F
H
0
H
l
G
n
u
d
v
V
Z
j
2
r
K
0
P
V
f
d
y
W
E
v
7
/
8
F
8
5
3
d
0
L
H
p
3
v
N
w
+
N
R
H
d
N
k
l
a
y
R
T
R
K
S
f
X
J
I
j
s
k
J
a
R
F
G
7
s
k
z
e
S
V
v
3
o
P
3
4
r
1
7
H
8
N
o
z
R
v
d
W
S
E
/
p
j
b
5
C
Z
R
y
t
g
Q
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
D
6
n
M
D
a
W
O
e
e
i
H
7
F
5
s
2
p
V
r
K
7
H
S
1
t
Y
=
"
>
A
A
A
C
T
H
i
c
b
Z
B
L
S
8
Q
w
F
I
X
T
8
T
2
+
R
l
2
6
K
Q
6
i
g
k
g
r
g
i
4
F
N
y
4
V
H
B
W
m
Z
U
g
z
t
0
4
w
S
U
t
y
q
w
6
h
P
9
C
N
C
3
f
+
C
j
c
u
F
B
F
M
Z
0
b
w
d
S
H
w
c
c
4
J
u
T
l
J
L
r
j
B
I
H
j
y
a
m
P
j
E
5
N
T
0
z
P
1
2
b
n
5
h
c
X
G
0
v
K
5
y
Q
r
N
o
M
U
y
k
e
n
L
h
B
o
Q
X
E
E
L
O
Q
q
4
z
D
V
Q
m
Q
i
4
S
K
6
P
K
v
/
i
B
r
T
h
m
T
r
D
f
g
6
x
p
F
e
K
p
5
x
R
d
F
K
n
w
S
I
F
t
y
y
T
k
q
q
u
r
b
h
s
h
7
G
1
z
b
D
c
K
O
t
R
3
u
M
d
G
2
E
P
k
J
a
b
k
a
T
Y
S
1
J
7
V
2
7
V
1
6
u
o
r
f
z
y
K
7
A
9
l
I
b
h
H
+
l
O
o
x
n
s
B
I
P
x
/
0
I
4
g
i
Y
Z
z
U
m
n
8
R
h
1
M
1
Z
I
U
M
g
E
N
a
Y
d
B
j
n
G
l
m
r
k
T
I
D
b
q
z
C
Q
U
3
Z
N
r
6
D
t
U
F
E
J
J
r
a
D
M
k
p
/
3
S
l
d
P
8
2
0
O
w
r
9
g
f
r
9
h
q
X
S
m
L
5
M
X
L
J
a
0
f
z
2
K
v
E
/
r
1
1
g
e
h
B
b
r
v
I
C
Q
b
H
h
Q
2
k
h
f
M
z
8
q
l
m
/
y
z
U
w
F
H
0
H
l
G
n
u
d
v
V
Z
j
2
r
K
0
P
V
f
d
y
W
E
v
7
/
8
F
8
5
3
d
0
L
H
p
3
v
N
w
+
N
R
H
d
N
k
l
a
y
R
T
R
K
S
f
X
J
I
j
s
k
J
a
R
F
G
7
s
k
z
e
S
V
v
3
o
P
3
4
r
1
7
H
8
N
o
z
R
v
d
W
S
E
/
p
j
b
5
C
Z
R
y
t
g
Q
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
p
4
Z
x
E
T
j
0
O
U
9
t
u
J
W
n
B
m
B
7
E
H
R
S
t
W
c
=
"
>
A
A
A
C
M
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
3
f
l
u
/
q
h
6
9
B
I
u
o
I
J
K
I
o
M
e
C
l
x
4
V
r
A
p
N
K
J
v
t
x
C
7
u
b
s
L
u
R
C
1
L
/
p
I
X
/
4
l
4
8
a
C
I
V
/
+
E
m
7
Y
H
v
w
Y
W
n
n
1
n
h
p
l
5
k
1
x
w
g
0
H
w
4
k
1
M
T
k
3
P
z
M
7
N
1
x
Y
W
l
5
Z
X
6
q
t
r
F
y
Y
r
N
I
M
2
y
0
S
m
r
x
J
q
Q
H
A
F
b
e
Q
o
4
C
r
X
Q
G
U
i
4
D
K
5
O
a
n
y
l
7
e
g
D
c
/
U
O
Q
5
y
i
C
W
9
V
j
z
l
j
K
K
T
u
v
V
W
p
O
C
O
Z
V
J
S
1
b
M
V
l
5
0
w
t
r
Y
R
l
t
t
l
r
f
r
b
K
O
/
z
s
m
s
j
7
A
P
S
v
Z
E
0
5
L
L
c
i
S
T
F
f
p
L
a
+
3
K
3
W
2
8
E
+
8
E
w
/
L
8
Q
j
q
F
B
x
n
H
a
r
T
9
F
v
Y
w
V
E
h
Q
y
Q
Y
3
p
h
E
G
O
s
a
U
a
O
R
P
g
h
h
c
G
c
s
p
u
6
D
V
0
H
C
o
q
w
c
R
2
e
H
H
p
b
z
m
l
5
6
e
Z
d
k
+
h
P
1
S
/
d
1
g
q
j
R
n
I
x
F
V
W
K
5
r
f
u
U
r
8
L
9
c
p
M
D
2
O
L
V
d
5
g
a
D
Y
a
F
B
a
C
B
8
z
v
7
L
P
7
3
E
N
D
M
X
A
A
W
W
a
u
1
1
9
1
q
e
a
M
n
Q
m
1
5
w
J
4
e
+
T
/
8
L
F
w
X
7
o
+
O
y
w
0
W
y
N
7
Z
g
j
G
2
S
T
7
J
C
Q
H
J
E
m
a
Z
F
T
0
i
a
M
P
J
B
n
8
k
r
e
v
E
f
v
x
X
v
3
P
k
a
l
E
9
6
4
Z
5
3
8
C
O
/
z
C
9
Y
U
q
9
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
p
4
Z
x
E
T
j
0
O
U
9
t
u
J
W
n
B
m
B
7
E
H
R
S
t
W
c
=
"
>
A
A
A
C
M
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
3
f
l
u
/
q
h
6
9
B
I
u
o
I
J
K
I
o
M
e
C
l
x
4
V
r
A
p
N
K
J
v
t
x
C
7
u
b
s
L
u
R
C
1
L
/
p
I
X
/
4
l
4
8
a
C
I
V
/
+
E
m
7
Y
H
v
w
Y
W
n
n
1
n
h
p
l
5
k
1
x
w
g
0
H
w
4
k
1
M
T
k
3
P
z
M
7
N
1
x
Y
W
l
5
Z
X
6
q
t
r
F
y
Y
r
N
I
M
2
y
0
S
m
r
x
J
q
Q
H
A
F
b
e
Q
o
4
C
r
X
Q
G
U
i
4
D
K
5
O
a
n
y
l
7
e
g
D
c
/
U
O
Q
5
y
i
C
W
9
V
j
z
l
j
K
K
T
u
v
V
W
p
O
C
O
Z
V
J
S
1
b
M
V
l
5
0
w
t
r
Y
R
l
t
t
l
r
f
r
b
K
O
/
z
s
m
s
j
7
A
P
S
v
Z
E
0
5
L
L
c
i
S
T
F
f
p
L
a
+
3
K
3
W
2
8
E
+
8
E
w
/
L
8
Q
j
q
F
B
x
n
H
a
r
T
9
F
v
Y
w
V
E
h
Q
y
Q
Y
3
p
h
E
G
O
s
a
U
a
O
R
P
g
h
h
c
G
c
s
p
u
6
D
V
0
H
C
o
q
w
c
R
2
e
H
H
p
b
z
m
l
5
6
e
Z
d
k
+
h
P
1
S
/
d
1
g
q
j
R
n
I
x
F
V
W
K
5
r
f
u
U
r
8
L
9
c
p
M
D
2
O
L
V
d
5
g
a
D
Y
a
F
B
a
C
B
8
z
v
7
L
P
7
3
E
N
D
M
X
A
A
W
W
a
u
1
1
9
1
q
e
a
M
n
Q
m
1
5
w
J
4
e
+
T
/
8
L
F
w
X
7
o
+
O
y
w
0
W
y
N
7
Z
g
j
G
2
S
T
7
J
C
Q
H
J
E
m
a
Z
F
T
0
i
a
M
P
J
B
n
8
k
r
e
v
E
f
v
x
X
v
3
P
k
a
l
E
9
6
4
Z
5
3
8
C
O
/
z
C
9
Y
U
q
9
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
p
4
Z
x
E
T
j
0
O
U
9
t
u
J
W
n
B
m
B
7
E
H
R
S
t
W
c
=
"
>
A
A
A
C
M
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
3
f
l
u
/
q
h
6
9
B
I
u
o
I
J
K
I
o
M
e
C
l
x
4
V
r
A
p
N
K
J
v
t
x
C
7
u
b
s
L
u
R
C
1
L
/
p
I
X
/
4
l
4
8
a
C
I
V
/
+
E
m
7
Y
H
v
w
Y
W
n
n
1
n
h
p
l
5
k
1
x
w
g
0
H
w
4
k
1
M
T
k
3
P
z
M
7
N
1
x
Y
W
l
5
Z
X
6
q
t
r
F
y
Y
r
N
I
M
2
y
0
S
m
r
x
J
q
Q
H
A
F
b
e
Q
o
4
C
r
X
Q
G
U
i
4
D
K
5
O
a
n
y
l
7
e
g
D
c
/
U
O
Q
5
y
i
C
W
9
V
j
z
l
j
K
K
T
u
v
V
W
p
O
C
O
Z
V
J
S
1
b
M
V
l
5
0
w
t
r
Y
R
l
t
t
l
r
f
r
b
K
O
/
z
s
m
s
j
7
A
P
S
v
Z
E
0
5
L
L
c
i
S
T
F
f
p
L
a
+
3
K
3
W
2
8
E
+
8
E
w
/
L
8
Q
j
q
F
B
x
n
H
a
r
T
9
F
v
Y
w
V
E
h
Q
y
Q
Y
3
p
h
E
G
O
s
a
U
a
O
R
P
g
h
h
c
G
c
s
p
u
6
D
V
0
H
C
o
q
w
c
R
2
e
H
H
p
b
z
m
l
5
6
e
Z
d
k
+
h
P
1
S
/
d
1
g
q
j
R
n
I
x
F
V
W
K
5
r
f
u
U
r
8
L
9
c
p
M
D
2
O
L
V
d
5
g
a
D
Y
a
F
B
a
C
B
8
z
v
7
L
P
7
3
E
N
D
M
X
A
A
W
W
a
u
1
1
9
1
q
e
a
M
n
Q
m
1
5
w
J
4
e
+
T
/
8
L
F
w
X
7
o
+
O
y
w
0
W
y
N
7
Z
g
j
G
2
S
T
7
J
C
Q
H
J
E
m
a
Z
F
T
0
i
a
M
P
J
B
n
8
k
r
e
v
E
f
v
x
X
v
3
P
k
a
l
E
9
6
4
Z
5
3
8
C
O
/
z
C
9
Y
U
q
9
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
p
4
Z
x
E
T
j
0
O
U
9
t
u
J
W
n
B
m
B
7
E
H
R
S
t
W
c
=
"
>
A
A
A
C
M
X
i
c
b
Z
B
N
S
8
N
A
E
I
Y
3
f
l
u
/
q
h
6
9
B
I
u
o
I
J
K
I
o
M
e
C
l
x
4
V
r
A
p
N
K
J
v
t
x
C
7
u
b
s
L
u
R
C
1
L
/
p
I
X
/
4
l
4
8
a
C
I
V
/
+
E
m
7
Y
H
v
w
Y
W
n
n
1
n
h
p
l
5
k
1
x
w
g
0
H
w
4
k
1
M
T
k
3
P
z
M
7
N
1
x
Y
W
l
5
Z
X
6
q
t
r
F
y
Y
r
N
I
M
2
y
0
S
m
r
x
J
q
Q
H
A
F
b
e
Q
o
4
C
r
X
Q
G
U
i
4
D
K
5
O
a
n
y
l
7
e
g
D
c
/
U
O
Q
5
y
i
C
W
9
V
j
z
l
j
K
K
T
u
v
V
W
p
O
C
O
Z
V
J
S
1
b
M
V
l
5
0
w
t
r
Y
R
l
t
t
l
r
f
r
b
K
O
/
z
s
m
s
j
7
A
P
S
v
Z
E
0
5
L
L
c
i
S
T
F
f
p
L
a
+
3
K
3
W
2
8
E
+
8
E
w
/
L
8
Q
j
q
F
B
x
n
H
a
r
T
9
F
v
Y
w
V
E
h
Q
y
Q
Y
3
p
h
E
G
O
s
a
U
a
O
R
P
g
h
h
c
G
c
s
p
u
6
D
V
0
H
C
o
q
w
c
R
2
e
H
H
p
b
z
m
l
5
6
e
Z
d
k
+
h
P
1
S
/
d
1
g
q
j
R
n
I
x
F
V
W
K
5
r
f
u
U
r
8
L
9
c
p
M
D
2
O
L
V
d
5
g
a
D
Y
a
F
B
a
C
B
8
z
v
7
L
P
7
3
E
N
D
M
X
A
A
W
W
a
u
1
1
9
1
q
e
a
M
n
Q
m
1
5
w
J
4
e
+
T
/
8
L
F
w
X
7
o
+
O
y
w
0
W
y
N
7
Z
g
j
G
2
S
T
7
J
C
Q
H
J
E
m
a
Z
F
T
0
i
a
M
P
J
B
n
8
k
r
e
v
E
f
v
x
X
v
3
P
k
a
l
E
9
6
4
Z
5
3
8
C
O
/
z
C
9
Y
U
q
9
E
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
e
7
b
a
0
X
T
n
z
h
3
9
D
8
5
2
Y
2
c
b
m
V
b
s
x
D
s
=
"
>
A
A
A
C
i
n
i
c
j
V
F
b
a
9
s
w
F
J
a
9
9
b
K
s
F
7
d
9
7
I
t
Z
C
O
1
g
L
X
Y
p
v
d
C
X
w
v
r
Q
x
w
6
W
J
h
C
b
I
C
v
H
j
Y
g
k
G
+
m
4
b
R
D
+
M
f
t
L
f
e
u
/
m
Z
x
k
Y
7
0
8
7
I
D
g
4
7
t
I
R
+
d
k
p
e
A
G
o
+
j
Z
8
z
9
8
X
F
p
e
W
f
3
U
+
r
y
2
v
r
E
Z
b
G
3
f
m
q
L
S
D
L
q
s
E
I
X
u
Z
9
S
A
4
A
q
6
y
F
F
A
v
9
R
A
Z
S
a
g
l
0
2
+
N
3
r
v
H
r
T
h
h
f
q
J
0
x
J
S
S
e
8
U
z
z
m
j
6
K
h
h
8
C
t
R
8
M
A
K
K
a
k
a
2
Q
b
X
g
z
i
1
t
h
3
X
e
3
W
r
k
w
9
t
g
m
N
A
W
u
8
n
k
u
I
4
y
+
1
j
/
b
X
1
l
/
7
W
J
P
5
Y
X
n
o
6
S
T
n
m
7
8
c
7
8
5
T
T
6
/
+
4
a
B
i
0
o
8
N
o
V
u
F
b
E
C
9
A
m
y
z
q
Z
h
g
8
J
a
O
C
V
R
I
U
M
k
G
N
G
c
R
R
i
a
m
l
G
j
k
T
U
L
e
S
y
k
B
J
2
Y
T
e
w
c
B
B
R
S
W
Y
1
M
5
G
W
Y
c
d
x
4
z
C
v
N
D
u
K
A
x
n
7
L
8
J
S
6
U
x
U
5
k
5
Z
9
O
j
e
a
0
1
5
H
v
a
o
M
L
8
L
L
V
c
l
R
W
C
Y
v
O
H
8
k
q
E
W
I
T
N
X
s
I
R
1
8
B
Q
T
B
2
g
T
H
P
X
a
8
j
G
V
F
O
G
b
n
v
N
E
O
L
X
X
3
4
L
b
o
8
O
Y
4
d
/
H
L
c
v
r
x
f
j
W
C
W
7
5
A
v
Z
J
z
E
5
J
Z
f
k
m
t
y
Q
L
m
H
e
i
n
f
g
n
X
i
n
/
p
p
/
5
J
/
7
F
3
O
r
7
y
0
y
O
+
R
F
+
V
e
/
A
X
1
A
x
5
g
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
e
7
b
a
0
X
T
n
z
h
3
9
D
8
5
2
Y
2
c
b
m
V
b
s
x
D
s
=
"
>
A
A
A
C
i
n
i
c
j
V
F
b
a
9
s
w
F
J
a
9
9
b
K
s
F
7
d
9
7
I
t
Z
C
O
1
g
L
X
Y
p
v
d
C
X
w
v
r
Q
x
w
6
W
J
h
C
b
I
C
v
H
j
Y
g
k
G
+
m
4
b
R
D
+
M
f
t
L
f
e
u
/
m
Z
x
k
Y
7
0
8
7
I
D
g
4
7
t
I
R
+
d
k
p
e
A
G
o
+
j
Z
8
z
9
8
X
F
p
e
W
f
3
U
+
r
y
2
v
r
E
Z
b
G
3
f
m
q
L
S
D
L
q
s
E
I
X
u
Z
9
S
A
4
A
q
6
y
F
F
A
v
9
R
A
Z
S
a
g
l
0
2
+
N
3
r
v
H
r
T
h
h
f
q
J
0
x
J
S
S
e
8
U
z
z
m
j
6
K
h
h
8
C
t
R
8
M
A
K
K
a
k
a
2
Q
b
X
g
z
i
1
t
h
3
X
e
3
W
r
k
w
9
t
g
m
N
A
W
u
8
n
k
u
I
4
y
+
1
j
/
b
X
1
l
/
7
W
J
P
5
Y
X
n
o
6
S
T
n
m
7
8
c
7
8
5
T
T
6
/
+
4
a
B
i
0
o
8
N
o
V
u
F
b
E
C
9
A
m
y
z
q
Z
h
g
8
J
a
O
C
V
R
I
U
M
k
G
N
G
c
R
R
i
a
m
l
G
j
k
T
U
L
e
S
y
k
B
J
2
Y
T
e
w
c
B
B
R
S
W
Y
1
M
5
G
W
Y
c
d
x
4
z
C
v
N
D
u
K
A
x
n
7
L
8
J
S
6
U
x
U
5
k
5
Z
9
O
j
e
a
0
1
5
H
v
a
o
M
L
8
L
L
V
c
l
R
W
C
Y
v
O
H
8
k
q
E
W
I
T
N
X
s
I
R
1
8
B
Q
T
B
2
g
T
H
P
X
a
8
j
G
V
F
O
G
b
n
v
N
E
O
L
X
X
3
4
L
b
o
8
O
Y
4
d
/
H
L
c
v
r
x
f
j
W
C
W
7
5
A
v
Z
J
z
E
5
J
Z
f
k
m
t
y
Q
L
m
H
e
i
n
f
g
n
X
i
n
/
p
p
/
5
J
/
7
F
3
O
r
7
y
0
y
O
+
R
F
+
V
e
/
A
X
1
A
x
5
g
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
e
7
b
a
0
X
T
n
z
h
3
9
D
8
5
2
Y
2
c
b
m
V
b
s
x
D
s
=
"
>
A
A
A
C
i
n
i
c
j
V
F
b
a
9
s
w
F
J
a
9
9
b
K
s
F
7
d
9
7
I
t
Z
C
O
1
g
L
X
Y
p
v
d
C
X
w
v
r
Q
x
w
6
W
J
h
C
b
I
C
v
H
j
Y
g
k
G
+
m
4
b
R
D
+
M
f
t
L
f
e
u
/
m
Z
x
k
Y
7
0
8
7
I
D
g
4
7
t
I
R
+
d
k
p
e
A
G
o
+
j
Z
8
z
9
8
X
F
p
e
W
f
3
U
+
r
y
2
v
r
E
Z
b
G
3
f
m
q
L
S
D
L
q
s
E
I
X
u
Z
9
S
A
4
A
q
6
y
F
F
A
v
9
R
A
Z
S
a
g
l
0
2
+
N
3
r
v
H
r
T
h
h
f
q
J
0
x
J
S
S
e
8
U
z
z
m
j
6
K
h
h
8
C
t
R
8
M
A
K
K
a
k
a
2
Q
b
X
g
z
i
1
t
h
3
X
e
3
W
r
k
w
9
t
g
m
N
A
W
u
8
n
k
u
I
4
y
+
1
j
/
b
X
1
l
/
7
W
J
P
5
Y
X
n
o
6
S
T
n
m
7
8
c
7
8
5
T
T
6
/
+
4
a
B
i
0
o
8
N
o
V
u
F
b
E
C
9
A
m
y
z
q
Z
h
g
8
J
a
O
C
V
R
I
U
M
k
G
N
G
c
R
R
i
a
m
l
G
j
k
T
U
L
e
S
y
k
B
J
2
Y
T
e
w
c
B
B
R
S
W
Y
1
M
5
G
W
Y
c
d
x
4
z
C
v
N
D
u
K
A
x
n
7
L
8
J
S
6
U
x
U
5
k
5
Z
9
O
j
e
a
0
1
5
H
v
a
o
M
L
8
L
L
V
c
l
R
W
C
Y
v
O
H
8
k
q
E
W
I
T
N
X
s
I
R
1
8
B
Q
T
B
2
g
T
H
P
X
a
8
j
G
V
F
O
G
b
n
v
N
E
O
L
X
X
3
4
L
b
o
8
O
Y
4
d
/
H
L
c
v
r
x
f
j
W
C
W
7
5
A
v
Z
J
z
E
5
J
Z
f
k
m
t
y
Q
L
m
H
e
i
n
f
g
n
X
i
n
/
p
p
/
5
J
/
7
F
3
O
r
7
y
0
y
O
+
R
F
+
V
e
/
A
X
1
A
x
5
g
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
e
7
b
a
0
X
T
n
z
h
3
9
D
8
5
2
Y
2
c
b
m
V
b
s
x
D
s
=
"
>
A
A
A
C
i
n
i
c
j
V
F
b
a
9
s
w
F
J
a
9
9
b
K
s
F
7
d
9
7
I
t
Z
C
O
1
g
L
X
Y
p
v
d
C
X
w
v
r
Q
x
w
6
W
J
h
C
b
I
C
v
H
j
Y
g
k
G
+
m
4
b
R
D
+
M
f
t
L
f
e
u
/
m
Z
x
k
Y
7
0
8
7
I
D
g
4
7
t
I
R
+
d
k
p
e
A
G
o
+
j
Z
8
z
9
8
X
F
p
e
W
f
3
U
+
r
y
2
v
r
E
Z
b
G
3
f
m
q
L
S
D
L
q
s
E
I
X
u
Z
9
S
A
4
A
q
6
y
F
F
A
v
9
R
A
Z
S
a
g
l
0
2
+
N
3
r
v
H
r
T
h
h
f
q
J
0
x
J
S
S
e
8
U
z
z
m
j
6
K
h
h
8
C
t
R
8
M
A
K
K
a
k
a
2
Q
b
X
g
z
i
1
t
h
3
X
e
3
W
r
k
w
9
t
g
m
N
A
W
u
8
n
k
u
I
4
y
+
1
j
/
b
X
1
l
/
7
W
J
P
5
Y
X
n
o
6
S
T
n
m
7
8
c
7
8
5
T
T
6
/
+
4
a
B
i
0
o
8
N
o
V
u
F
b
E
C
9
A
m
y
z
q
Z
h
g
8
J
a
O
C
V
R
I
U
M
k
G
N
G
c
R
R
i
a
m
l
G
j
k
T
U
L
e
S
y
k
B
J
2
Y
T
e
w
c
B
B
R
S
W
Y
1
M
5
G
W
Y
c
d
x
4
z
C
v
N
D
u
K
A
x
n
7
L
8
J
S
6
U
x
U
5
k
5
Z
9
O
j
e
a
0
1
5
H
v
a
o
M
L
8
L
L
V
c
l
R
W
C
Y
v
O
H
8
k
q
E
W
I
T
N
X
s
I
R
1
8
B
Q
T
B
2
g
T
H
P
X
a
8
j
G
V
F
O
G
b
n
v
N
E
O
L
X
X
3
4
L
b
o
8
O
Y
4
d
/
H
L
c
v
r
x
f
j
W
C
W
7
5
A
v
Z
J
z
E
5
J
Z
f
k
m
t
y
Q
L
m
H
e
i
n
f
g
n
X
i
n
/
p
p
/
5
J
/
7
F
3
O
r
7
y
0
y
O
+
R
F
+
V
e
/
A
X
1
A
x
5
g
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
i
R
b
F
6
Q
V
r
H
N
E
4
X
h
h
D
O
q
u
x
D
O
Q
M
Y
g
s
=
"
>
A
A
A
C
Z
H
i
c
d
V
F
N
S
+
w
w
F
E
3
7
9
K
l
V
n
/
W
J
K
0
G
K
g
+
i
D
h
7
Q
i
6
F
J
w
4
1
L
B
U
W
F
a
h
j
R
z
6
w
S
T
t
C
S
3
6
h
D
6
J
9
2
5
d
O
P
v
M
J
0
Z
w
c
8
L
g
c
M
5
5
9
7
c
n
O
S
V
4
A
b
j
+
M
n
z
f
8
3
M
/
p
6
b
X
w
g
W
l
5
b
/
r
I
S
r
f
y
9
N
W
W
s
G
X
V
a
K
U
l
/
n
1
I
D
g
C
r
r
I
U
c
B
1
p
Y
H
K
X
M
B
V
f
n
v
S
6
l
d
3
o
A
0
v
1
Q
W
O
K
s
g
k
v
V
G
8
4
I
y
i
o
/
q
h
T
R
X
c
s
1
J
K
q
g
Z
j
3
P
S
S
z
N
p
O
0
u
w
0
Q
d
G
3
K
Q
4
B
a
b
O
b
S
o
r
D
v
L
A
P
z
b
9
g
O
6
2
G
/
C
f
J
j
b
C
t
3
r
w
Z
/
k
+
o
i
f
m
D
u
x
9
2
4
r
1
4
X
N
F
X
k
E
x
B
h
0
z
r
r
B
8
+
p
o
O
S
1
R
I
U
M
k
G
N
6
S
V
x
h
Z
m
l
G
j
k
T
0
A
R
p
b
a
C
i
7
J
b
e
Q
M
9
B
R
S
W
Y
z
I
5
D
a
q
J
t
x
w
y
i
o
t
T
u
K
I
z
G
7
P
s
O
S
6
U
x
I
5
k
7
Z
7
u
i
+
a
y
1
5
H
d
a
r
8
b
i
K
L
N
c
V
T
W
C
Y
p
O
L
i
l
p
E
W
E
Z
t
4
t
G
A
a
2
A
o
R
g
5
Q
p
r
n
b
N
W
J
D
q
i
l
D
9
y
+
B
C
y
H
5
/
O
S
v
4
H
J
/
L
3
H
4
/
K
B
z
f
D
q
N
Y
5
5
s
k
C
2
y
S
x
J
y
S
I
7
J
K
T
k
j
X
c
L
I
s
z
f
n
h
d
6
q
9
+
I
v
+
W
v
+
+
s
T
q
e
9
O
e
N
f
K
h
/
M
1
X
p
L
e
5
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
i
R
b
F
6
Q
V
r
H
N
E
4
X
h
h
D
O
q
u
x
D
O
Q
M
Y
g
s
=
"
>
A
A
A
C
Z
H
i
c
d
V
F
N
S
+
w
w
F
E
3
7
9
K
l
V
n
/
W
J
K
0
G
K
g
+
i
D
h
7
Q
i
6
F
J
w
4
1
L
B
U
W
F
a
h
j
R
z
6
w
S
T
t
C
S
3
6
h
D
6
J
9
2
5
d
O
P
v
M
J
0
Z
w
c
8
L
g
c
M
5
5
9
7
c
n
O
S
V
4
A
b
j
+
M
n
z
f
8
3
M
/
p
6
b
X
w
g
W
l
5
b
/
r
I
S
r
f
y
9
N
W
W
s
G
X
V
a
K
U
l
/
n
1
I
D
g
C
r
r
I
U
c
B
1
p
Y
H
K
X
M
B
V
f
n
v
S
6
l
d
3
o
A
0
v
1
Q
W
O
K
s
g
k
v
V
G
8
4
I
y
i
o
/
q
h
T
R
X
c
s
1
J
K
q
g
Z
j
3
P
S
S
z
N
p
O
0
u
w
0
Q
d
G
3
K
Q
4
B
a
b
O
b
S
o
r
D
v
L
A
P
z
b
9
g
O
6
2
G
/
C
f
J
j
b
C
t
3
r
w
Z
/
k
+
o
i
f
m
D
u
x
9
2
4
r
1
4
X
N
F
X
k
E
x
B
h
0
z
r
r
B
8
+
p
o
O
S
1
R
I
U
M
k
G
N
6
S
V
x
h
Z
m
l
G
j
k
T
0
A
R
p
b
a
C
i
7
J
b
e
Q
M
9
B
R
S
W
Y
z
I
5
D
a
q
J
t
x
w
y
i
o
t
T
u
K
I
z
G
7
P
s
O
S
6
U
x
I
5
k
7
Z
7
u
i
+
a
y
1
5
H
d
a
r
8
b
i
K
L
N
c
V
T
W
C
Y
p
O
L
i
l
p
E
W
E
Z
t
4
t
G
A
a
2
A
o
R
g
5
Q
p
r
n
b
N
W
J
D
q
i
l
D
9
y
+
B
C
y
H
5
/
O
S
v
4
H
J
/
L
3
H
4
/
K
B
z
f
D
q
N
Y
5
5
s
k
C
2
y
S
x
J
y
S
I
7
J
K
T
k
j
X
c
L
I
s
z
f
n
h
d
6
q
9
+
I
v
+
W
v
+
+
s
T
q
e
9
O
e
N
f
K
h
/
M
1
X
p
L
e
5
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
i
R
b
F
6
Q
V
r
H
N
E
4
X
h
h
D
O
q
u
x
D
O
Q
M
Y
g
s
=
"
>
A
A
A
C
Z
H
i
c
d
V
F
N
S
+
w
w
F
E
3
7
9
K
l
V
n
/
W
J
K
0
G
K
g
+
i
D
h
7
Q
i
6
F
J
w
4
1
L
B
U
W
F
a
h
j
R
z
6
w
S
T
t
C
S
3
6
h
D
6
J
9
2
5
d
O
P
v
M
J
0
Z
w
c
8
L
g
c
M
5
5
9
7
c
n
O
S
V
4
A
b
j
+
M
n
z
f
8
3
M
/
p
6
b
X
w
g
W
l
5
b
/
r
I
S
r
f
y
9
N
W
W
s
G
X
V
a
K
U
l
/
n
1
I
D
g
C
r
r
I
U
c
B
1
p
Y
H
K
X
M
B
V
f
n
v
S
6
l
d
3
o
A
0
v
1
Q
W
O
K
s
g
k
v
V
G
8
4
I
y
i
o
/
q
h
T
R
X
c
s
1
J
K
q
g
Z
j
3
P
S
S
z
N
p
O
0
u
w
0
Q
d
G
3
K
Q
4
B
a
b
O
b
S
o
r
D
v
L
A
P
z
b
9
g
O
6
2
G
/
C
f
J
j
b
C
t
3
r
w
Z
/
k
+
o
i
f
m
D
u
x
9
2
4
r
1
4
X
N
F
X
k
E
x
B
h
0
z
r
r
B
8
+
p
o
O
S
1
R
I
U
M
k
G
N
6
S
V
x
h
Z
m
l
G
j
k
T
0
A
R
p
b
a
C
i
7
J
b
e
Q
M
9
B
R
S
W
Y
z
I
5
D
a
q
J
t
x
w
y
i
o
t
T
u
K
I
z
G
7
P
s
O
S
6
U
x
I
5
k
7
Z
7
u
i
+
a
y
1
5
H
d
a
r
8
b
i
K
L
N
c
V
T
W
C
Y
p
O
L
i
l
p
E
W
E
Z
t
4
t
G
A
a
2
A
o
R
g
5
Q
p
r
n
b
N
W
J
D
q
i
l
D
9
y
+
B
C
y
H
5
/
O
S
v
4
H
J
/
L
3
H
4
/
K
B
z
f
D
q
N
Y
5
5
s
k
C
2
y
S
x
J
y
S
I
7
J
K
T
k
j
X
c
L
I
s
z
f
n
h
d
6
q
9
+
I
v
+
W
v
+
+
s
T
q
e
9
O
e
N
f
K
h
/
M
1
X
p
L
e
5
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
i
R
b
F
6
Q
V
r
H
N
E
4
X
h
h
D
O
q
u
x
D
O
Q
M
Y
g
s
=
"
>
A
A
A
C
Z
H
i
c
d
V
F
N
S
+
w
w
F
E
3
7
9
K
l
V
n
/
W
J
K
0
G
K
g
+
i
D
h
7
Q
i
6
F
J
w
4
1
L
B
U
W
F
a
h
j
R
z
6
w
S
T
t
C
S
3
6
h
D
6
J
9
2
5
d
O
P
v
M
J
0
Z
w
c
8
L
g
c
M
5
5
9
7
c
n
O
S
V
4
A
b
j
+
M
n
z
f
8
3
M
/
p
6
b
X
w
g
W
l
5
b
/
r
I
S
r
f
y
9
N
W
W
s
G
X
V
a
K
U
l
/
n
1
I
D
g
C
r
r
I
U
c
B
1
p
Y
H
K
X
M
B
V
f
n
v
S
6
l
d
3
o
A
0
v
1
Q
W
O
K
s
g
k
v
V
G
8
4
I
y
i
o
/
q
h
T
R
X
c
s
1
J
K
q
g
Z
j
3
P
S
S
z
N
p
O
0
u
w
0
Q
d
G
3
K
Q
4
B
a
b
O
b
S
o
r
D
v
L
A
P
z
b
9
g
O
6
2
G
/
C
f
J
j
b
C
t
3
r
w
Z
/
k
+
o
i
f
m
D
u
x
9
2
4
r
1
4
X
N
F
X
k
E
x
B
h
0
z
r
r
B
8
+
p
o
O
S
1
R
I
U
M
k
G
N
6
S
V
x
h
Z
m
l
G
j
k
T
0
A
R
p
b
a
C
i
7
J
b
e
Q
M
9
B
R
S
W
Y
z
I
5
D
a
q
J
t
x
w
y
i
o
t
T
u
K
I
z
G
7
P
s
O
S
6
U
x
I
5
k
7
Z
7
u
i
+
a
y
1
5
H
d
a
r
8
b
i
K
L
N
c
V
T
W
C
Y
p
O
L
i
l
p
E
W
E
Z
t
4
t
G
A
a
2
A
o
R
g
5
Q
p
r
n
b
N
W
J
D
q
i
l
D
9
y
+
B
C
y
H
5
/
O
S
v
4
H
J
/
L
3
H
4
/
K
B
z
f
D
q
N
Y
5
5
s
k
C
2
y
S
x
J
y
S
I
7
J
K
T
k
j
X
c
L
I
s
z
f
n
h
d
6
q
9
+
I
v
+
W
v
+
+
s
T
q
e
9
O
e
N
f
K
h
/
M
1
X
p
L
e
5
5
w
=
=
<
/
l
a
t
e
x
i
t
>
Combining with (10), the cost function in this case is

Network

Pooling

1-shot

5-shot

10-shot

J(X ′, Y ′; θ, θ′) :=

n′

r

Xi=1

Xk=1

ℓ(f (k)

θ,θ′ [Pi](x

′

i), y′

i).

(12)

C128F
C128F
ResNet-12
ResNet-12

GAP
DC
GAP
DC

54.28 ±0.18
49.84 ±0.18
58.61 ±0.18
61.26 ±0.20

71.60 ±0.13
69.64 ±0.15
76.40 ±0.13
79.01 ±0.13

76.92 ±0.12
74.61 ±0.13
80.76 ±0.11
83.04 ±0.12

Prototypes in (11) or (12) are recomputed at each iteration
based on the current version of implants. Note that this
training setup does not apply to the 1-shot scenario as it
requires at least two support samples per class.

3.3. Inference on novel classes

Inference is the same whether the embedding network
has been implanted or not. Here we adopt the prototypical
network model too. What we have found to work best is
to perform global pooling of the embeddings of the support
examples and compute class prototypes P := (p1, . . . , pc′ )
by (1). Given a query x ∈ X , the standard prediction is then
to assign it to the nearest prototype

arg max
j∈C ′

s(φθ,θ′ (x), pj),

(13)

where s is cosine similarity [41]. Alternatively, we can
densely classify the embedding φθ,θ′ (x), soft-assigning in-
dependently the embedding φ(k)
θ,θ′ (x) of each spatial loca-
tion, then average over all locations k ∈ [r] according to

fθ,θ′ [P ](x) :=

1
r

r

Xk=1

σ(cid:16)[sτ (φ(k)

θ,θ′ (x), pj)]c′

j=1(cid:17) ,

(14)

where sτ is the scaled cosine similarity (7), and ﬁnally clas-
sify to arg maxj∈C ′ f j

θ,θ′ [P ](x).

4. Related work

Metric learning is common in few-shot learning. Multiple
improvements of the standard softmax and cross-entropy
loss are proposed by [48, 22, 52, 44, 9] to this end. Tra-
ditional methods like siamese networks are also consid-
ered [3, 39, 18] along with models that learn by comparing
multiple samples at once [43, 49, 41]. Learning to generate
new samples [9] is another direction. Our solution is related
to prototypical networks [41] and matching networks [43]
but we rather use a parametric classiﬁer.

Meta-learning is the basis of a large portion of the few-shot
learning literature. Recent approaches can be roughly clas-
siﬁed as: optimization-based methods, that learn to initial-
ize the parameters of a learner such that it becomes faster
to ﬁne-tune [5, 27, 28]; memory-based methods leverag-
ing memory modules to store training samples or to encode
adaptation algorithms [38, 34]; data generation methods
that learn to generate new samples [46]; parameter gener-
ating methods that learn to generate the weights of a clas-
siﬁer [6, 32] or the parameters of a network with multiple
layers [1, 7, 47, 8]. The motivation behind the latter is that

Table 1. Average 5-way accuracy on novel
classes of
miniImageNet, stage 1 only. Pooling refers to stage 1 train-
ing. GAP: global average pooling; DC: dense classiﬁcation. At
testing, we use global max-pooling on queries for models trained
with dense classiﬁcation, and global average pooling otherwise.

it should be easier to generate new parameters rather than
to ﬁne-tune a large network or to train a new classiﬁer from
scratch. By generating a single linear layer at the end of the
network [6, 31, 32], one neglects useful coarse visual in-
formation found in intermediate layers. We plug our neural
implants at multiple depth levels, taking advantage of such
features during ﬁne-tuning and learning new ones.

Network adaptation is common when learning a new task
or new domain. One solution is to learn to mask part of the
network, keeping useful neurons and re-training/ﬁne-tuning
the remaining neurons on the new-task [25, 26]. Rusu et
al. [37] rather widen the network by adding new neurons
in parallel to the old ones at every layer. New neurons
receive data from all hidden states, while previously gen-
erated weights are frozen when training for the new task.
Our neural implants are related to [37] as we add new neu-
rons in parallel and freeze the old ones. Unlike [37], we
focus on low-data regimes, keeping the number of new im-
planted neurons small to diminish overﬁtting risks and train
faster, and adding them only at top layers, taking advantage
of generic visual features from bottom layers [50].

5. Experiments

We

evaluate

the
miniImageNet and FC100 datasets. We describe the
experimental setup and report results below.

our method

extensively

on

5.1. Experimental setup

Networks. In most experiments we use a ResNet-12 net-
work [30] as our embedding network, composed of four
residual blocks [11], each having three 3×3 convolutional
layers with batch normalization [16] and swish-1 activa-
tion function [33]. Each block is followed by 2×2 max-
pooling. The shortcut connections have a convolutional
layer to adapt to the right number of channels. The ﬁrst
block has 64 channels, which is doubled at each subsequent
block such that the output has depth 512. We also test dense
classiﬁcation on a lighter network C128F [6] composed of
four convolutional layers, the ﬁrst (last) two having 64 (128)
channels, each followed by 2×2 max-pooling.

9263

Stage 1 training

Support/query pooling at testing

Global average pooling

Dense classiﬁcation

Support →
Queries →

Base classes
Novel classes
Both classes

Base classes
Novel classes
Both classes

GMP

GAP

GMP

DC

GAP

DC

63.55 ±0.20
72.25 ±0.13
37.74 ±0.07

79.28 ±0.10
79.01 ±0.13
42.45 ±0.07

77.17 ±0.11
70.71 ±0.14
38.65 ±0.05

80.67 ±0.10
77.93 ±0.13
57.98 ±0.10

79.37 ±0.09
76.40 ±0.13
56.25 ±0.10

80.61 ±0.10
78.55 ±0.13
67.53 ±0.10

77.15 ±0.11
73.28 ±0.14
54.80 ±0.09

80.70 ±0.10
78.95 ±0.13
67.78 ±0.10

Table 2. Average 5-way 5-shot accuracy on base, novel and both classes of miniImageNet with ResNet-12, stage 1 only. GMP: global
max-pooling; GAP: global average pooling; DC: dense classiﬁcation. Bold: accuracies in the conﬁdence interval of the best one.

Stage 2 training

Query pooling at testing

Support Queries

GAP

GMP

DC

GMP
GMP
GAP
GAP

GMP
DC
GAP
DC

79.03 ± 0.19
79.06 ± 0.19
79.62 ± 0.19
79.56 ± 0.19

78.92 ± 0.19
79.37 ± 0.18
74.57 ± 0.22
74.58 ± 0.22

79.04 ± 0.19
79.15 ± 0.19
79.77 ± 0.19
79.52 ± 0.19

Table 3.
Average 5-way 5-shot accuracy on novel classes of
miniImageNet with ResNet-12 and implanting in stage 2. At
testing, we use GAP for support examples. GMP: global max-
pooling; GAP: global average pooling; DC: dense classiﬁcation.

Datasets. We use miniImageNet [43], a subset of Im-
ageNet ILSVRC-12 [36] of 60,000 images of resolution
84 × 84, uniformly distributed over 100 classes. We use
the split proposed in [34]: C = 64 classes for training, 16
for validation and 20 for testing.

We also use FC100, a few-shot version of CIFAR-100
recently proposed by Oreshkin et al. [30]. Similarily to
miniImageNet, CIFAR-100 [19] has 100 classes of 600 im-
ages each, although the resolution is 32 × 32. The split is
C = 60 classes for training, 20 for validation and 20 for
testing. Given that all classes are grouped into 20 super-
classes, this split does not separate super-classes: classes
are more similar in each split and the semantic gap between
base and novel classes is larger.

Evaluation protocol. The training set X comprises images
of the base classes C. To generate the support set X ′ of
a few-shot task on novel classes, we randomly sample C ′
classes from the validation or test set and from each class we
sample k images. We report the average accuracy and the
corresponding 95% conﬁdence interval over a number of
such tasks. More precisely, for all implanting experiments,
we sample 5,000 few-shot tasks with 30 queries per class,
while for all other experiments we sample 10,000 tasks. Us-
ing the same task sampling, we also consider few-shot tasks
involving base classes C, following the benchmark of [6].
We sample a set of extra images from the base classes to
form a test set for this evaluation, which is performed in two

ways: independently of the novel classes C ′ and jointly on
the union C ∪ C ′. In the latter case, base prototypes learned
at stage 1 are concatenated with novel prototypes [6].

Implementation details.
In stage 1, we train the em-
bedding network for 8,000 (12,500) iterations with mini-
batch size 200 (512) on miniImageNet (FC100). On
miniImageNet, we use stochastic gradient descent with Nes-
terov momentum. On FC100, we rather use Adam opti-
mizer [17]. We initialize the scale parameter at τ = 10
(100) on miniImageNet (FC100). For a given few-shot task
in stage 2, the implants are learned over 50 epochs with
AdamW optimizer [24] and scale ﬁxed at τ = 10.

5.2. Results

Networks.
In Table 1 we compare ResNet-12 to C128F,
with and without dense classiﬁcation. We observe that
dense classiﬁcation improves classiﬁcation accuracy on
novel classes for ResNet-12, but it is detrimental for the
small network. C128F is only 4 layers deep and the recep-
tive ﬁeld at the last layer is signiﬁcantly smaller than the one
of ResNet-12, which is 12 layers deep. It is thus likely that
units from the last feature map correspond to non-object ar-
eas in the image. Regardless of the choice of using dense
classiﬁcation or not, ResNet-12 has a large performance gap
over C128F. For the following experiments, we use exclu-
sively ResNet-12 as our embedding network.

Dense classiﬁcation. To evaluate stage 1, we skip stage 2
and directly perform testing. In Table 2 we evaluate 5-way
5-shot classiﬁcation on miniImageNet with global average
pooling and dense classiﬁcation at stage 1 training, while
exploring different pooling strategies at inference. We also
tried using global max-pooling at stage 1 training and got
similar results as with global average pooling. Dense classi-
ﬁcation in stage 1 training outperforms global average pool-
ing in all cases by a large margin. It also improves the abil-
ity of the network to integrate new classes without forget-
ting the base ones. Using dense classiﬁcation at testing as
well, the accuracy on both classes is 67.78%, outperforming
the best result of 59.35% reported by [6]. At testing, dense

9264

Method

1-shot

5-shot

10-shot

GAP
DC (ours)
DC + WIDE
DC + IMP (ours)

MAML [5]
PN [41]
Gidaris et al. [6]
PN [30]
TADAM [30]

58.61 ± 0.18
62.53 ± 0.19
61.73 ± 0.19

-

48.70 ± 1.8
49.42 ± 0.78
55.45 ± 0.7
56.50 ± 0.4

76.40 ± 0.13
78.95 ± 0.13
78.25 ± 0.14
79.77 ± 0.19

63.10 ± 0.9
68.20 ± 0.66
73.00 ± 0.6
74.20 ± 0.2

58.50

76.70

80.76 ± 0.11
82.66 ± 0.11
82.03 ± 0.12
83.83 ± 0.16

-
-
-

78.60 ± 0.4

80.80

Table 4.
Average 5-way accuracy on novel classes of mini-
ImageNet. The top part is our solutions and baselines, all on
ResNet-12. GAP: global average pooling (stage 1); DC: dense
classiﬁcation (stage 1); WIDE: last residual block widened by 16
channels (stage 1); IMP: implanting (stage 2). In stage 2, we use
GAP on both support and queries. At testing, we use GAP on
support examples and GAP or DC on queries, depending on the
choice of stage 1. The bottom part results are as reported in the lit-
erature. PN: Prototypical Network [41]. MAML [5] and PN [41]
use four-layer networks; while PN [30] and TADAM [30] use the
same ResNet-12 as us. Gidaris et al. [6] use a Residual network of
comparable complexity to ours.

Method

1-shot

5-shot

10-shot

GAP
DC (ours)
DC + IMP (ours)

41.02 ± 0.17
42.04 ± 0.17

-

56.63 ± 0.16
57.05 ± 0.16
57.63 ± 0.23

61.65 ±0.15
61.91 ± 0.16
62.91 ± 0.22

PN [30]
TADAM [30]

37.80 ± 0.40
40.10 ± 0.40

53.30 ± 0.50
56.10 ± 0.40

58.70 ± 0.40
61.60 ± 0.50

Table 5. Average 5-way accuracy on novel classes of FC100 with
ResNet-12. The top part is our solutions and baselines. GAP:
global average pooling (stage 1); DC: dense classiﬁcation (stage
1); IMP: implanting (stage 2). In stage 2, we use GAP on both
support and queries. At testing, we use GAP on support exam-
ples and GAP or DC on queries, depending on the choice of stage
1. The bottom part results are as reported in the literature. All
experiments use the same ResNet-12.

classiﬁcation of the queries with global average pooling of
the support samples is the best overall choice. One excep-
tion is global max-pooling on both the support and query
samples, which gives the highest accuracy for new classes
but the difference is insigniﬁcant.

Implanting. In stage 2, we add implants of 16 channels to
all convolutional layers of the last residual block of our em-
bedding network pretrained in stage 1 on the base classes
with dense classiﬁcation. The implants are trained on the
few examples of the novel classes and then used as an inte-
gral part of the widened embedding network φθ,θ′ at testing.
In Table 3, we evaluate different pooling strategies for sup-
port examples and queries in stage 2. Average pooling on
both is the best choice, which we keep in the following.

Ablation study.

In the top part of Table 4 we compare

our best solutions with a number of baselines on 5-way
miniImageNet classiﬁcation. One baseline is the embed-
ding network trained with global average pooling in stage
1. Dense classiﬁcation remains our best training option. In
stage 2, the implants are able to further improve on the re-
sults of dense classiﬁcation. To illustrate that our gain does
not come just from having more parameters and greater fea-
ture dimensionality, another baseline is to compare it to
widening the last residual block of the network by 16 chan-
nels in stage 1.
It turns out that such widening does not
bring any improvement on novel classes. Similar conclu-
sions can be drawn from the top part of Table 5, showing
corresponding results on FC100. The difference between
different solutions is less striking here. This may be at-
tributed to the lower resolution of CIFAR-100, allowing
for less gain from either dense classiﬁcation or implanting,
since there may be less features to learn.

Comparison with the state-of-the-art. In the bottom part
of Table 4 we compare our model with previous few-shot
learning methods on the same 5-way miniImageNet clas-
siﬁcation. All our solutions outperform by a large margin
other methods on 1, 5 and 10-shot classiﬁcation. Our im-
planted network sets a new state-of-the art for 5-way 5-shot
classiﬁcation of miniImageNet. Note that prototypical net-
work on ResNet-12 [30] is already giving very competitive
performance. TADAM [30] builds on top of this baseline
to achieve the previous state of the art.
In this work we
rather use a cosine classiﬁer in stage 1. This setting is our
baseline GAP and is already giving similar performance to
TADAM [30]. Dense classiﬁcation and implanting are both
able to improve on this baseline. Our best results are at
least 3% above TADAM [30] in all settings. Finally, in the
bottom part of Table 5 we compare our model on 5-way
FC100 classiﬁcation against prototypical network [30] and
TADAM [30]. Our model outperforms TADAM here too,
though by a smaller margin.

6. Conclusion

In this work we contribute to few-shot learning by build-
ing upon a simpliﬁed process for learning on the base
classes using a standard parametric classiﬁer. We inves-
tigate for the ﬁrst time in few-shot learning the activation
maps and devise a new way of handling spatial information
by a dense classiﬁcation loss that is applied to each spatial
location independently, improving the spatial distribution of
the activation and the performance on new tasks. It is im-
portant that the performance beneﬁt comes with deeper net-
work architectures and high-dimensional embeddings. We
further adapt the network for new tasks by implanting neu-
rons with limited new parameters and without changing the
original embedding. Overall, this yields a simple architec-
ture that outperforms previous methods by a large margin
and sets a new state of the art on standard benchmarks.

9265

References

[1] Luca Bertinetto, Jo˜ao F Henriques, Jack Valmadre, Philip
Torr, and Andrea Vedaldi. Learning feed-forward one-shot
learners. In NIPS, 2016. 1, 6

[2] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for seman-
tic image segmentation. arXiv preprint arXiv:1706.05587,
2017. 1

[3] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning
a similarity metric discriminatively, with application to face
veriﬁcation. In CVPR, 2005. 6

[4] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning

of object categories. IEEE Trans. PAMI, 2006. 1

[5] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
In ICML, 2017. 1, 6, 8

[6] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot
visual learning without forgetting. In CVPR, 2018. 3, 4, 6,
7, 8

[7] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks.

ICLR, 2017. 6

[8] Chunrui Han, Shiguang Shan, Meina Kan, Shuzhe Wu, and
Xilin Chen. Face recognition with contrastive convolution.
In ECCV, 2018. 6

[9] Bharath Hariharan and Ross B. Girshick. Low-shot visual
recognition by shrinking and hallucinating features. ICCV,
2017. 6

[10] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-

shick. Mask r-cnn. ICCV, 2017. 1

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 1, 6

[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
arXiv preprint

ing the knowledge in a neural network.
arXiv:1503.02531, 2015. 4

[13] Sepp Hochreiter, A Steven Younger, and Peter R Conwell.
In International

Learning to learn using gradient descent.
Conference on Artiﬁcial Neural Networks, 2001. 1

[14] Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your clas-
siﬁer: the marginal value of training the last weight layer.
ICLR, 2018. 4

[15] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q. Weinberger. Densely connected convolutional net-
works. In CVPR, 2017. 1

[16] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. arXiv preprint arXiv:1502.03167, 2015. 6

[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. arXiv, 2014. 7

[18] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov.
Siamese neural networks for one-shot image recognition. In
ICMLW, 2015. 1, 6

[19] Alex Krizhevsky. Learning multiple layers of features from

tiny images. Technical report, 2009. 7

[21] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 1
[22] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang.
Large-margin softmax loss for convolutional neural net-
works. In ICML, 2016. 6

[23] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015. 4

[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay

regularization. ICLR, 2019. 7

[25] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy-
back: Adapting a single network to multiple tasks by learn-
ing to mask weights. In ECCV, 2018. 1, 6

[26] Arun Mallya and Svetlana Lazebnik. Packnet: Adding mul-
tiple tasks to a single network by iterative pruning. CVPR,
2018. 1, 6

[27] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter
Abbeel. A simple neural attentive meta-learner. ICLR, 2018.
3, 6

[28] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-
order meta-learning algorithms. CoRR, abs/1803.02999,
2018. 6

[29] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmentation.
In ICCV, 2015. 4

[30] Boris N Oreshkin, Alexandre Lacoste, and Pau Rodriguez.
Tadam: Task dependent adaptive metric for improved few-
shot learning. NIPS, 2018. 3, 4, 6, 7, 8

[31] Hang Qi, Matthew Brown, and David G Lowe. Low-shot

learning with imprinted weights. In CVPR, 2018. 2, 3, 4, 6

[32] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan Yuille. Few-
shot image recognition by predicting parameters from acti-
vations. CVPR, 2018. 6

[33] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Search-

ing for activation functions. ICLR, 2018. 6

[34] Sachin Ravi and Hugo Larochelle. Optimization as a model

for few-shot learning. ICLR, 2017. 6, 7

[35] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,

stronger. CVPR, 2017. 1

[36] O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z
Huang, A Karpathy, A Khosla, M Bernstein, et al. Imagenet
large scale visual recognition challenge. arXiv, 2014. 7

[37] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
arXiv preprint arXiv:1606.04671, 2016. 1, 6

[38] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan
Wierstra, and Timothy Lillicrap. Meta-learning with
memory-augmented neural networks.
In ICML, 2016. 1,
6

[39] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniﬁed embedding for face recognition and clus-
tering. In CVPR, 2015. 6

[20] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
In

Path aggregation network for instance segmentation.
CVPR, 2018. 1

[40] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. ICLR,
2015. 1

9266

[41] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical
networks for few-shot learning. In NIPS, 2017. 2, 3, 5, 6, 8
[42] Sebastian Thrun. Lifelong learning algorithms. In Learning

to learn. 1998. 1

[43] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wier-
stra, et al. Matching networks for one shot learning. In NIPS,
2016. 1, 3, 6, 7

[44] Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng
Chen. Rethinking feature distribution for loss functions in
image classiﬁcation. In CVPR, 2018. 6

[45] Feng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon
Yuille. Normface: l 2 hypersphere embedding for face veri-
ﬁcation. In ACM Multimedia, 2017. 4

[46] Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath
Hariharan. Low-shot learning from imaginary data. CVPR,
2018. 6

[47] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learn-

ing to model the tail. In NIPS, 2017. 1, 6

[48] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A
discriminative feature learning approach for deep face recog-
nition. In ECCV, 2016. 6

[49] Flood Sung Yongxin Yang, Li Zhang, Tao Xiang, Philip HS
Torr, and Timothy M Hospedales. Learning to compare: Re-
lation network for few-shot learning. In CVPR, 2018. 6

[50] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How transferable are features in deep neural networks? In
NIPS, 2014. 5, 6

[51] Fisher Yu, Vladlen Koltun, and Thomas A Funkhouser. Di-

lated residual networks. In CVPR, 2017. 1

[52] Yutong Zheng, Dipan K Pal, and Marios Savvides. Ring loss:
Convex feature normalization for face recognition. In CVPR,
2018. 6

[53] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In CVPR, 2016. 4

9267

