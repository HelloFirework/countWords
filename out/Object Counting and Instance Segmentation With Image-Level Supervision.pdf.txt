Object Counting and Instance Segmentation with Image-level Supervision

Hisham Cholakkal1 ∗

Guolei Sun1∗
Ling Shao1

Fahad Shahbaz Khan1

2

,

1Inception Institute of Artiﬁcial Intelligence, UAE

2Computer Vision Laboratory, Department of Electrical Engineering, Link¨oping University, Sweden

{hisham.cholakkal, guolei.sun, fahad.khan, ling.shao}@inceptioniai.org

Abstract

Common object counting in a natural scene is a chal-
lenging problem in computer vision with numerous real-
world applications. Existing image-level supervised com-
mon object counting approaches only predict the global ob-
ject count and rely on additional instance-level supervision
to also determine object locations. We propose an image-
level supervised approach that provides both the global ob-
ject count and the spatial distribution of object instances
by constructing an object category density map. Motivated
by psychological studies, we further reduce image-level su-
pervision using a limited object count information (up to
four). To the best of our knowledge, we are the ﬁrst to
propose image-level supervised density map estimation for
common object counting and demonstrate its effectiveness
in image-level supervised instance segmentation. Compre-
hensive experiments are performed on the PASCAL VOC
and COCO datasets. Our approach outperforms existing
methods, including those using instance-level supervision,
on both datasets for common object counting. Moreover,
our approach improves state-of-the-art image-level super-
vised instance segmentation [34] with a relative gain of
17.8% in terms of average best overlap, on the PASCAL
VOC 2012 dataset1.

1. Introduction

Common object counting, also referred as generic ob-
ject counting, is the task of accurately predicting the num-
ber of different object category instances present in natural
scenes (see Fig. 1). The common object categories in natu-
ral scenes can vary from fruits to animals and the counting
must be performed in both indoor and outdoor scenes (e.g.
COCO or PASCAL VOC datasets). Existing works employ
a localization-based strategy [13, 27] or utilize regression-
based models [4] directly optimized to predict object count,

∗Equal contribution
1Code is publicly available at github.com/GuoleiSun/CountSeg

Figure 1. Object counting on COCO dataset. The ground-truth and
our predictions are shown in black and green, respectively. Despite
being trained using image-level object counts within the subitiz-
ing range [1-4], it accurately counts objects beyond the subitiz-
ing range (11 persons) under heavy occlusion (marked with blue
arrow to show two persons) in the left image and diverse object
categories in the right.

where the latter has been shown to provide superior re-
sults [15]. However, regression-based methods only pre-
dict the global object count without determining object lo-
cations. Beside global counts, the spatial distribution of ob-
jects in the form of a per-category density map is helpful
in other tasks, e.g., to delineate adjacent objects in instance
segmentation (see Fig. 2).

The problem of density map estimation to preserve the
spatial distribution of people is well studied in crowd count-
ing [3,16,18,22,32]. Here, the global count for the image is
obtained by summing over the predicted density map. Stan-
dard crowd density map estimation methods are required to
predict large number of person counts in the presence of
occlusions, e.g., in surveillance applications. The key chal-
lenges of constructing a density map in natural scenes are
different to those in crowd density estimation, and include
large intra-class variations in generic objects, co-existence
of multiple instances of different objects in a scene (see
Fig. 1), and sparsity due to many objects having zero count
on multiple images.

Most methods for crowd density estimation use instance-
level (point-level or bounding box) supervision that re-
quires manual annotation of each instance location. Image-

112397

person: 11 (11)sports ball: 1 (1)  knife: 1 (1) cake: 2 (2)chair: 1 (1) clock: 1 (1)person: 3 (3)fork: 1 (1)dinning table: 1 (1)sheep

sheep sheep

dog

dog

(a) Input Image

(b) PRM [34]

(c) Our Approach

(d) Our Density Map

Figure 2. Instance segmentation examples using the PRM method [34] (b) and our approach (c), on the PASCAL VOC 2012. Top row:
The PRM approach [34] fails to delineate spatially adjacent two sheep category instances. Bottom row: single person parts predicted as
multiple persons along with inaccurate mask separation results in over-prediction (7 instead of 5). Our approach produces accurate masks
by exploiting the spatial distribution of object count in per-category density maps (d). Density map accumulation for each predicted mask
is shown inside the contour drawn for clarity. In the top row, density maps for sheep and dog categories are overlaid.

level supervised training alleviates the need for such user-
intensive annotation by requiring only the count of differ-
ent object instances in an image. We propose an image-
level supervised density map estimation approach for natu-
ral scenes, that predicts the global object count while pre-
serving the spatial distribution of objects.

Even though image-level supervised object counting re-
duces the burden of human annotation and is much weaker
compared to instance-level supervisions, it still requires
each object instance to be counted sequentially. Psycho-
logical studies [2, 6, 11, 20] have suggested that humans are
capable of counting objects non-sequentially using holis-
tic cues for fewer object counts, termed as a subitizing
range (generally 1-4). We utilize this property to further
reduce image-level supervision by only using object count
annotations within the subitizing range. For short, we call
this image-level lower-count (ILC) supervision. Chattopad-
hyay et al. [4] also investigate common object counting,
where object counts (both within and beyond the subitizing
range) are used to predict the global object count. Alter-
natively, instance-level (bounding box) supervision is used
to count objects by dividing an image into non-overlapping
regions, assuming each region count falls within the subitiz-
ing range. Different to these strategies [4], our ILC super-
vised approach requires neither bounding box annotation
nor information beyond the subitizing range to predict both
the count and the spatial distribution of object instances.

In addition to common object counting, the proposed
ILC supervised density map estimation is suitable for other
scene understanding tasks. Here, we investigate its effec-
tiveness for image-level supervised instance segmentation,
where the task is to localize each object instance with pixel-
level accuracy, provided image-level category labels. Re-
cent work of [34], referred as peak response map (PRM),
tackles the problem by boosting the local maxima (peaks)

in the class response maps [23] of an image classiﬁer us-
ing a peak stimulation module. A scoring metric is then
used to rank off-the-shelf object proposals [21, 25] corre-
sponding to each peak for instance mask prediction. How-
ever, PRM struggles to delineate spatially adjacent object
instances from the same object category (see Fig. 2(b)). We
introduce a penalty term into the scoring metric that assigns
a higher score to object proposals with a predicted count
of 1, providing improved results (Fig. 2(c)). The predicted
count is obtained by accumulating the density map over the
entire object proposal region (Fig. 2(d)).

Contributions: We propose an ILC supervised density map
estimation approach for common object counting. A novel
loss function is introduced to construct per-category density
maps with explicit terms for predicting the global count and
spatial distribution of objects. We also demonstrate the ap-
plicability of the proposed approach for image-level super-
vised instance segmentation. For common object counting,
our ILC supervised approach outperforms state-of-the-art
instance-level supervised methods with a relative gain of
6.4% and 2.9%, respectively, in terms of mean root mean
square error (mRMSE), on the PASCAL VOC 2007 and
COCO datasets. For image-level supervised instance seg-
mentation, our approach improves the state of the art from
37.6 to 44.3 in terms of average best overlap (ABO), on the
PASCAL VOC 2012 dataset.

2. Related work

Chattopadhyay et al. [4] investigated regression-based
common object counting, using image-level (per-category
count) and instance-level (bounding box) supervisions. The
image-level supervised strategy, denoted as glancing, used
count annotations from both within and beyond the subitiz-
ing range to predict the global count of objects, without pro-
viding information about their location. The instance-level

12398

personpersonpersonpersonpersonpersonper-sonpersonpersonpersonpersonpersonFigure 3. Overview of our overall architecture. Our network has an image classiﬁcation and a density branch, trained jointly using ILC
supervision. The image classiﬁcation branch predicts the presence and absence of objects. This branch is used to generate pseudo ground-
truth for training the density branch. The density branch has two terms (spatial and global) in the loss function and produces a density map
to predict the global object count and preserve the spatial distribution of objects.

(bounding box) supervised strategy, denoted as subitizing,
estimated a large number of objects by dividing an image
into non-overlapping regions, assuming the object count in
each region falls within the subitizing range. Instead, our
ILC supervised approach requires neither bounding box an-
notation nor beyond subitizing range count information dur-
ing training. It then predicts the global object count, even
beyond the subitizing range, together with the spatial dis-
tribution of object instances. Our novel loss function along
with a pseudo ground-truth generation strategy enables ob-
ject counting with ILC supervision. Recently, Laradji et
al. [14] proposed a localization-based counting approach,
trained using instance-level (point) supervision [1]. During
inference, the model outputs blobs indicating the predicted
locations of objects of interest and uses [30] to estimate ob-
ject counts from these blobs. Different to [14], our approach
is image-level supervised and directly predicts the object
count through a simple summation of the density map with-
out requiring any post-processing [30].

Reducing object count supervision for salient object
subitizing was investigated in [31]. However, their task is
class-agnostic and subitizing is used to only count within
the subitizing range.
Instead, our approach constructs
category-speciﬁc density maps and accurately predicts ob-
ject counts both within and beyond the subitizing range.
Common object counting has been previously used to im-
prove object detection [4, 8]. Their approach only uses the
count information during detector training with no explicit
component for count prediction. In contrast, our approach
explicitly learns to predict the global object count.

3. Proposed method

Here, we present our image-level lower-count (ILC)
supervised density map estimation approach. Our ap-

proach is built upon an ImageNet pre-trained network back-
bone (ResNet50) [10]. The proposed network architecture
has two output branches: image classiﬁcation and density
branch (see Fig. 3). The image classiﬁcation branch esti-
mates the presence or absence of objects, whereas the den-
sity branch predicts the global object count and the spatial
distribution of object instances by constructing a density
map. We remove the global pooling layer from the back-
bone and adapt the fully connected layer with a 1 × 1 con-
volution having 2P channels as output. We divide these 2P
channels equally between the image classiﬁcation and den-
sity branches. We then add a 1 × 1 convolution having C
output channels in each branch, resulting in a fully convo-
lutional network [19]. Here, C is the number of object cat-
egories and P is empirically set to be proportional to C. In
each branch, the convolution is preceded by a batch normal-
ization and a ReLU layer. The ﬁrst branch provides object
category maps and the second branch produces a density
map for each object category.

3.1. The Proposed Loss Function

Let I be a training image and t = {t1, t2, ..., tc, ..., tC}
be the corresponding vector for the ground-truth count of
C object categories.
Instead of using an absolute object
count, we employ a lower-count strategy to reduce the
amount of image-level supervision. Given an image I, ob-
ject categories are divided into three non-overlapping sets
based on their respective instance counts. The ﬁrst set,
A, indicates object categories which are absent in I (i.e.,
tc = 0). The second set, S, represents categories within
the subitizing range (i.e, 0 < tc ≤ 4). The ﬁnal set, ˜S, in-
dicates categories beyond the subitizing range (i.e, tc ≥ ˜t,
where ˜t = 5).

Let M = {M1, M2, ..., Mc, ..., MC} denote the object
category maps in the image classiﬁcation branch, where

12399

Mc ∈ RH×W . Let D = {D1, D2, ..., Dc, ..., DC} rep-
resent density maps produced by the density branch, where
Dc ∈ RH×W . Here, H × W is the spatial size of both the
object category and density maps. Each pixel in the den-
sity map Dc indicates the number of objects present in the
corresponding image region. The accumulation of a density
map over any image region estimates the count of objects
over that region [15]. On the other hand, the pixel in the
object category map Mc indicates the conﬁdence that the
corresponding image pixels belong to the object category c.
The image classiﬁcation and density branches are jointly
trained, in an end-to-end fashion, given only ILC supervi-
sion with the following loss function:

L = Lclass + Lspatial + Lglobal

.

(1)

Density map branch

{z

}

|

Here, the ﬁrst term refers to multi-label image classiﬁcation
loss [12] (see Sec. 3.1.1). The last two terms, Lspatial and
Lglobal, are used to train the density branch (Sec. 3.1.2).

3.1.1

Image Classiﬁcation Branch

Generally, training a density map requires instance-level su-
pervision, such as point-level annotations [15]. Such infor-
mation is unavailable in our ILC supervised setting. To ad-
dress this issue, we propose to generate pseudo ground-truth
by exploiting the coarse-level localization capabilities of an
image classiﬁer [23, 33] via object category maps. These
object category maps are generated from a fully convolu-
tional architecture shown in Fig. 3.

While specifying classiﬁcation conﬁdence at each image
location, class activation maps (CAMs) struggle to delineate
multiple instances from the same object category [23, 33].
Recently, the local maxima of CAMs are further boosted,
to produce object category maps, during an image-classiﬁer
training for image-level supervised instance segmentation
[34]. Boosted local maxima aim at falling on distinct object
instances. For details on boosting local maxima, we refer
to [34]. Here, we use local maxima locations to generate
pseudo ground-truth for training the density branch.

As described earlier, object categories in I are divided
into three non-overlapping sets: A, S and ˜S. To train a one-
versus-rest image classiﬁer, we derive binary labels from tc
that indicate the presence ∀c ∈ {S, ˜S} or absence ∀c ∈ A
c
of object categories. Let ˜M
∈ RH×W be the peak map
derived from cth object category map (Mc) of M such that:

c
˜M

(i, j) = (cid:26)Mc(i, j),

0,

if Mc(i, j) > Mc(i − ri, j − rj),
otherwise.

Here, −r ≤ ri ≤ r, −r ≤ rj ≤ r where r is the radius
for the local maxima computation. We set r = 1, as in [34].
The local maxima are searched at all spatial locations with a
stride of one. To train an image classiﬁer, a class conﬁdence

score sc of the cth object category is computed as the aver-
age of non-zero elements of ˜M
. In this work, we use the
multi-label soft-margin loss [12] for binary classiﬁcation.

c

3.1.2 Density Branch

The image classiﬁcation branch described above predicts
the presence or absence of objects by using the class con-
c
ﬁdence scores derived from the peak map ˜M
. However,
it struggles to differentiate between multiple objects and
single object parts due to the lack of prior information
about the number of object instances (see Fig. 2(b)). This
causes a large number of false positives in the peak map
˜M
. Here, we utilize the count information and introduce a
pseudo ground-truth generation scheme that prevents train-
ing a density map at those false positive locations.

c

When constructing a density map, it is desired to esti-
mate accurate object counts at any image sub-region. Our
spatial loss term Lspatial in Eq. 1 ensures that individual
object instances are localized while the global term Lglobal
constrains the global object count to that of the ground-
truth. This enables preservation of the spatial distribution
of object counts in a density map. Later, we show that this
property helps to improve instance segmentation.
Spatial Loss: The spatial loss Lspatial is divided into the
loss Lsp+ which enhances the positive peaks correspond-
ing to instances of object categories within S, and the loss
Lsp− which suppresses false positives of categories within
A. Due to the unavailability of absolute object count, the
set ˜S is not used in the spatial loss and treated separately
later. To enable ILC supervised density map training using
Lspatial, we generate a pseudo ground-truth binary mask
from peak map ˜M
Pseudo Ground-truth Generation: To compute the spatial
loss Lsp+, a pseudo ground-truth is generated for set S. For
all object categories c ∈ S, the tc-th highest peak value of
peak map ˜M c is computed using the heap-max algorithm
[5]. The tc-th highest peak value hc is then used to generate
a pseudo ground-truth binary mask Bc as,

c

.

Bc = u( ˜M

c

− hc).

(2)

Here, u(n) is the unit step function which is 1 only if n ≥ 0.
Although the non-zero elements of the pseudo ground-truth
mask Bc indicate object locations, its zero elements do not
necessarily point towards the background. Therefore, we
construct a masked density map ˜D
to exclude density map
Dc values at locations where the corresponding Bc values
are zero. Those density map Dc values should also be ex-
cluded during the loss computation in Eq. 4 and backprop-
agation (see Sec. 3.2), due to their risk of introducing false
negatives. This is achieved by computing the Hadamard
product between the density map Dc and Bc as,

c

c
˜D

= Dc ⊙ Bc.

(3)

12400

The spatial loss Lsp+ for object categories within the
c
subitizing range S is computed between Bc and ˜D
using
a logistic binary cross entropy (logistic BCE) [24] loss for
positive ground-truth labels. The logistic BCE loss transfers
c
the network prediction ( ˜D
) through a sigmoid activation
layer σ and computes the standard BCE loss as,

Lsp+( ˜D

c

, Bc) = − X∀c∈S

c
kBc ⊙ log(σ( ˜D

))ksum

|S| · kBcksum

.

(4)

Here, |S| is the cardinality of the set S and the norm k ksum
is computed by taking the summation over all elements in
a matrix. For example, kBc ksum = 1hBc1w, where 1h
and 1w are all-ones vectors of size 1 × H and W × 1, re-
spectively. Here, the highest tc peaks in ˜M
are assumed to
fall on tc instances of object category c ∈ S. Due to the
unavailability of ground-truth object locations, we use this
assumption and observe that it holds in most scenarios.

c

The spatial loss Lsp+ for the positive ground-truth la-
bels enhances positive peaks corresponding to instances of
object categories within S. However, the false positives of
the density map for c ∈ S are not penalized in this loss. We
therefore introduce another term, Lsp−, into the loss func-
tion to address the false positives of c ∈ A. For c ∈ A,
positive activations of Dc indicate false detections. A zero-
valued mask 0H×W is used as ground-truth to reduce such
false detections using logistic BCE loss,

Lsp−(Dc, 0H×W ) = −Xc∈A

k log(1 − σ(Dc)ksum

|A| · H · W

.

(5)

Though the spatial loss ensures the preservation of spatial
distribution of objects, only relying on local information
may result in deviations in the global object count.
Global Loss: The global loss penalizes the deviation of the
predicted count ˆtc from the ground-truth. It has two com-
ponents: ranking loss Lrank for object categories beyond
the subitizing range (i.e., ∀c ∈ ˜S) and mean-squared error
(MSE) loss LM SE for the rest of the categories. LM SE
penalizes the predicted density map, if the global count pre-
diction does not match with the ground-truth count. i.e.,

LM SE( ˆtc, tc) = Xc∈{A,S}

( ˆtc − tc)2
|A| + |S|

.

(6)

Here, the predicted count ˆtc is the accumulation of the den-
sity map for a category c over its entire spatial region. i.e.
ˆtc = kDcksum. Note that object categories in ˜S were
not previously considered in the computation of spatial loss
Lspatial and mean-squared error loss LM SE. Here, we in-
troduce a ranking loss [29] with a zero margin that penalizes
under-counting for object categories within ˜S,

Lrank( ˆtc, ˜t) = Xc∈ ˜S

max(0, ˜t − ˆtc)

| ˜S|

.

(7)

The ranking loss penalizes the density branch if the pre-
dicted object count ˆtc is less than ˜t for c ∈ ˜S. Recall, the
beyond subitizing range ˜S starts from ˜t = 5.

Within the subitizing range S,

the spatial loss term
Lspatial is optimized to locate object instances while the
global MSE loss (LM SE) is optimized for accurately pre-
dicting the corresponding global count. Due to the joint op-
timization of both these terms within the subitizing range,
the network learns to correlate between the located objects
and the global count. Further, the network is able to locate
object instances, generalizing beyond the subitizing range
˜S (see Fig. 2). Additionally, the ranking loss Lrank term in
the proposed loss function ensures the penalization of under
counting beyond the subitizing range ˜S.
Mini-batch Loss: Normalized loss terms ˆLsp+, ˆLsp−,
ˆLM SE and ˆLrank are computed by averaging respective
loss terms over all images in the mini-batch. The Lspatial
is computed by ˆLsp+ + ˆLsp−. For categories beyond the
subitizing range, ˆLrank can lead to over-estimation of the
count. Hence, Lglobal is computed by assigning a rela-
tively lower weight (λ = 0.1) to ˆLrank (see Table. 2). i.e.,
Lglobal = ˆLM SE + λ ∗ ˆLrank.
3.2. Training and Inference

Our network is trained in two stages. In the ﬁrst stage,
the density branch is trained with only LM SE and Lrank
losses using S and ˜S respectively. The spatial loss Lspatial
in Eq. 1 is excluded in the ﬁrst stage, since it requires a
pseudo ground-truth generated from the image classiﬁca-
tion branch. The second stage includes the spatial loss.
Backpropagation: We use Bc derived from the image clas-
siﬁcation branch as a pseudo ground-truth to train the den-
sity branch. Therefore, the backproapation of gradients
through Bc to the classiﬁer branch is not required (shown
with green arrows in Fig. 3). The image classiﬁcation
branch is backpropagated as in [34]. In the density branch,
we use Hadamard product of the density map with Bc in
Eq. 3 to compute Lsp+ for c ∈ S. Hence, the gradients
(δc) for the cth channel of the last convolution layer of the
density branch, due to Lsp+ , are computed as,

δc
sp+ =

∂ ˆLsp+
∂ ˜D

c ⊙ Bc.

(8)

Since LM SE, Lrank and Lsp− are computed using MSE,
ranking and logistic BCE losses on convolution outputs,
their respective gradients are computed using off-the-shelf
pytorch implementation [24].
Inference: The image classiﬁcation branch outputs a class
conﬁdence score sc for each class, indicating the presence
( ˆtc > 0, if sc > 0) or absence ( ˆtc = 0, if sc ≤ 0 ) of the ob-
ject category c. The predicted count ˆtc is obtained by sum-
ming the density map Dc for category c over its entire spa-
tial region. The proposed approach only utilizes subitizing

12401

annotations (tc ≤ 4) and accurately predicts object counts
for both within and beyond subitizing range (see Fig. 6).
3.3. Image level Supervised Instance Segmentation

The proposed ILC supervised density map estimation ap-
proach can also be utilized for instance segmentation. Note
that the local summation of an ideal density map over a
ground-truth segmentation mask is one. We use this prop-
erty to improve state-of-the-art image-level supervised in-
stance segmentation (PRM) [34]. PRM employs a scoring
metric that combines instance level cues from peak response
maps R, class aware information from object category maps
and spatial continuity priors from off-the-shelf object pro-
posals [21, 25]. Here, the peak response maps are gener-
ated from local maxima (peaks of ˜M
) through a peak back-
propagation process [34]. The scoring metric is then used
to rank object proposals corresponding to each peak for in-
stance mask prediction. We improve the scoring metric by
introducing an additional term dp in the metric. The term
dp penalizes an object proposal Pr, if the predicted count in
those regions of the density map Dc is different from one,
as dp= |1 − kDc · Prksum|. Here, | | is the absolute value op-
erator. For each peak, the new scoring metric Score selects
the highest scoring object proposal Pr.

c

Score = α · R ∗ Pr + R ∗ ˆPr − β · Q ∗ Pr − γ · dp.

(9)

Here, the background mask Q is derived from object cat-
egory map and ˆPr is the contour mask of the proposal Pr
derived using morphological gradient [34]. Parameters α, β
and γ are empirically set as in [34].

4. Experiments

Implementation details: The number of input channels P
of 1 × 1 convolutions at each branch is set to P = 1.5 × C.
A mini-batch size of 16 is used for the SGD optimizer.
An initial learning rate of 10−4 is used for the pre-trained
ResNet-50 backbone, while image classiﬁcation and den-
sity branches are trained with an initial learning rate of 0.01.
The momentum is set to 0.9 and weight decay to 10−4. Con-
sidering high imbalance between non-zero and zero counts
in COCO dataset (e.g. 79 negative categories for each posi-
tive category), only 10% of samples in the set A are used to
train the density branch.
Datasets: We evaluate common object counting on the
PASCAL VOC 2007 [7] and COCO [17] datasets. For
fair comparison, we employ same splits, named as count-
train, count-val and count-test, as used in the state-of-the-
art methods [14], [4]. For COCO dataset, the training set
is used as count-train, ﬁrst half of the validation set as the
count-val and its second half as the count-test. Best models
on count-val set are used to report the results on count-test
set. In Pascal VOC 2007 dataset, we evaluated against the
count of non-difﬁcult instances in the count-test as in [14].
For instance segmentation, we train and report the results

Approach

SV mRMSE

CAM+MSE
Peak+MSE

Proposed

IC
IC

ILC

0.45
0.64

0.29

mRMSE

-nz
1.52
2.51

1.14

m-rel
RMSE

m-rel

RMSE-nz

0.29
0.30

0.17

0.64
1.06

0.61

Table 1. Counting performance on the Pascal VOC 2007 count-
test set using our approach and two baselines. Both baselines are
obtained by training the network using the MSE loss function.

(a) Input Image

(b) Class+MSE

(c) +Spatial

(d) +Ranking

Figure 4. Progressive improvement in density map quality with
the incremental introduction of spatial and ranking loss terms. In
both cases (top row: person and bottom row: bicycle), our overall
loss function integrating all three terms provides the best density
maps. The global object count is accurately predicted (top row:
5 persons and bottom row: 4 bicycles) by accumulation of the
respective density map.

i=1

tic+1

(tic− ˆtic)2

T PT

T PT

on the PASCAL VOC 2012 dataset similar to [34].
Evaluation Criteria: The predicted count ˆtc is rounded
to the nearest integer. We evaluate common object count-
ing, as in [4, 14], using root squared error (RMSE) met-
ric and its three variants namely RMSE non-zero (RMSE-
nz), relative RMSE (relRMSE) and relative RMSE non-
zero (relRMSE-nz). The RM SEc and relRM SEc er-
i=1(tic − ˆtic)2

rors for category c are computed as q 1
and, q 1

respectively. Here, T is the total
number of images in the test set and ˆtic, tic are the pre-
dicted and ground-truth counts for image i. The errors are
then averaged across all categories to obtain the mRMSE
and m-relRMSE on a dataset. The above metrics are also
evaluated for ground-truth instances with non-zero counts
as mRMSE-nz and m-relRMSE-nz. For all error metrics,
smaller numbers indicate better performance. We refer to
[4] for more details. For instance segmentation, the perfor-
mance is evaluated using Average Best Overlap (ABO) [26]
and mAP r, as in [34]. The mAP r is computed with inter-
section over union (IoU) thresholds of 0.25, 0.5 and 0.75.
Supervision Levels: The level of supervision is indicated
as SV in Tab. 3 and 4. BB indicates bounding box supervi-
sion and PL indicates point-level supervision for each object
instance. Image-level supervised methods using only within
subitizing range counts are denoted as ILC, while the meth-
ods using both within and beyond subitizing range counts
are indicated as IC.

4.1. Common Object Counting Results
Ablation Study: We perform an ablation study on the
PASCAL VOC 2007 count-test.
the impact of

First,

12402

personLclass+
LM SE

mRMSE

mRMSE-nz

0.36
1.52

Lclass+
Lspatial
+LM SE

0.33
1.32

L

L

L

L

L

λ = 0.1

λ = 0.01

λ = 0.05

λ = 0.5

λ = 1

0.29
1.14

0.31
1.27

0.30
1.16

0.32
1.23

0.36
1.40

Table 2. Left: Progressive integration of different terms in loss
function and its impact on the ﬁnal counting performance on the
PASCAL VOC count-test set. Right: inﬂuence of the weight (λ)
of ranking loss.

Approach

SV mRMSE

Aso-sub-ft-3×3 [4]
Seq-sub-ft-3×3 [4]

ens [4]

Fast-RCNN [4]
LC-ResFCN [14]
LC-PSPNet [14]
glance-noft-2L [4]

Proposed

BB
BB
BB
BB
PL
PL
IC
ILC

0.43
0.42
0.42
0.50
0.31
0.35
0.50
0.29

mRMSE

-nz
1.65
1.65
1.68
1.92
1.20
1.32
1.83
1.14

m-rel
RMSE

m-rel

RMSE-nz

0.22
0.21
0.20
0.26
0.17
0.20
0.27
0.17

0.68
0.68
0.65
0.85
0.61
0.70
0.73
0.61

Table 3. State-of-the-art counting performance comparison on the
Pascal VOC 2007 count-test. Our ILC supervised approach out-
performs existing methods.

our two-branch architecture is analyzed by comparing it
with two baselines: class-activation [33] based regression
(CAM+MSE) and peak-based regression (Peak+MSE) us-
ing the local-maximum boosting approach of [34]. Both
baselines are obtained by end-to-end training of the net-
work, employing the same backbone, using MSE loss func-
tion to directly predict global count. Tab. 1 shows the com-
parison. Our approach largely outperforms both baseline
highlighting the importance of having a two-branch archi-
tecture with explicit terms in the loss function to preserve
the spatial distribution of objects. Next, we evaluate the
contribution of each term in our loss function towards the
ﬁnal count performance.

Fig. 4 shows the systematic improvement in density
maps (top row: person and bottom row: bicycle) quality
with the incremental addition of (c) spatial Lspatial and (d)
ranking (Lrank) loss terms to the (b) MSE (Lrank) loss
term. Incorporating the spatial loss term improves the spa-
tial distribution of objects in both density maps. The den-
sity maps are further improved by the incorporation of the
ranking term that penalizes the under-estimation of count
beyond the subitizing range (top row) in the loss function.
Moreover, it also helps to reduce the false positives within
the subitizing range (bottom row). Tab. 2 shows the sys-
tematic improvement, in terms of mRMSE and mRMSE-nz,
when integrating different terms in our loss function. The
best results are obtained when integrating all three terms
(classiﬁcation, spatial and global) in our loss function. We
also evaluate the inﬂuence of λ that controls the relative
weight of the ranking loss. We observe λ = 0.1 provides
the best results and ﬁx it for all datasets.
State-of-the-art Comparison: Tab. 3 and 4 show state-
of-the-art comparisons for common object counting on
the PASCAL VOC 2007 and COCO datasets respectively.
On the PASCAL VOC 2007 dataset (Tab. 3), the glanc-

Approach

SV mRMSE

Aso-sub-ft-3×3 [4]
Seq-sub-ft-3×3 [4]

ens [4]

Fast-RCNN [4]
LC-ResFCN [14]
glance-ft-2L [4]

Proposed

BB
BB
BB
BB
PL
IC
ILC

0.38
0.35
0.36
0.49
0.38
0.42
0.34

mRMSE

-nz
2.08
1.96
1.98
2.78
2.20
2.25
1.89

m-rel
RMSE

m-rel

RMSE-nz

0.24
0.18
0.18
0.20
0.19
0.23
0.18

0.87
0.82
0.81
1.13
0.99
0.91
0.84

Table 4. State-of-the-art counting performance comparison on the
COCO count-test set. Despite using reduced supervision, our ap-
proach provides superior results compared to existing methods on
three metrics. Compared to the image-level count (IC) supervised
approach [4], our method achieves an absolute gain of 8% in terms
of mRMSE.

Figure 5. Object counting examples on the COCO dataset. The
ground-truth, point-level supervised counts [14] and our predic-
tions are shown in black, red and green respectively. Our ap-
proach accurately performs counting beyond the subitizing range
and on diverse categories (fruits to animals) under heavy occlu-
sions (highlighted by a red arrow in the left image).

ing approach (glance-noft-2L) of [4] using image-level
supervision both within and beyond the subitizing range
(IC) achieves mRMSE score of 0.50. Our ILC super-
vised approach considerably outperforms the glance-noft-
2L method with a absolute gain of 21% in mRMSE. Fur-
thermore, our approach achieves consistent improvements
on all error metrics, compared to state-of-the-art point-level
and bounding box based supervised methods.

Tab. 4 shows the results on COCO dataset. Among
the existing methods, the two BB supervised approaches
(Seq-sub-ft-3x3 and ens) yields mRMSE scores of 0.35
and 0.36 respectively. The PL supervised LC-ResFCN ap-
proach [14] achieves mRMSE score of 0.38. The IC super-
vised glancing approach (glance-noft-2L) obtains mRMSE
score of 0.42. Our approach outperforms the glancing ap-
proach with an absolute gain of 8% in mRMSE. Further-
more, our approach also provides consistent improvements
over the glancing approach in the other three error metrics
and is only below the two BB supervised methods (Seq-
sub-ft3x3 and ens) in m-relRMSE-nz. Fig. 5 shows object
counting examples using our approach and the point-level
(PL) supervised method [14]. Our approach performs accu-
rate counting on various categories (fruits to animals) under
heavy occlusions. Fig. 6 shows counting performance com-
parison in terms of RMSE, across all categories, on COCO
count-test. The x-axis shows different ground-truth count
values. We compare with the different IC, BB and PL su-
pervised methods [4, 14]. Our approach achieves superior
results on all count values compared to glancing method [4]

12403

person: 4, 1 (1)broccoli: 1, 5 (5)orange: 2, 8 (8) remote: 2, 1 (1)zebra: 15, 12 (12) person: 5, 6 (6)tv: 1, 1 (1)carrot: 2, 5 (5)   bowl: 0, 1 (1)              
Beyond Subitizing Range

(b) PRM [34]

(c) Our Approach
(a) Input Image
Figure 7. Instance segmentation examples of PRM [34] and our
approach. Our approach accurately delineates spatially adjacent
multiple object instances of horse and cow categories.

Figure 6. Counting performance comparison in RMSE, across all
categories, at different ground-truth count values on the COCO
count-test set. Different methods, including BB and PL super-
vision, are shown in the legend. Our ILC supervised approach
provides superior results compared to the image-level supervised
glancing method. Furthermore, our approach performs favourably
compared to other methods using instance-level supervision.

despite not using the beyond subitizing range annotations
during training. Furthermore, we perform favourably com-
pared to other methods using higher supervision.
Evaluation of density map: We employ a standard grid
average mean absolute error (GAME) evaluation metric [9]
used in crowd counting to evaluate spatial distribution con-
sistency in the density map. In GAME(n), an image is di-
vided into 4n non-overlapping grid cells. Mean absolute
error (MAE) between the predicted and the ground-truth lo-
cal counts are reported for n = 0, 1, 2 and 3, as in [9]. We
compare our approach with the state-of-the-art PL super-
vised counting approach (LCFCN) [14] on the 20 categories
of the PASCAL VOC 2007 count-test set. Furthermore, we
also compare with recent crowd counting approach (CSR-
net) [16] on the person category of the PASCAL VOC 2007
by retraining it on the dataset. For the person category,
the PL supervised LCFCN and CSRnet approaches achieve
scores of 2.80 and 2.44 in GAME(3).The proposed method
outperforms LCFCN and CSRnet in GAME (3) with score
of 1.83, demonstrating the capabilities of our approach in
the precise spatial distribution of object counts. Moreover,
our method outperforms LCFCN for all 20 categories.

Method

MELM+MCG [28]
CAM+MCG [33]
SPN+MCG [35]

PRM [34]

Ours

mAP r
36.9
20.4
26.4
44.3
48.5

0.25 mAP r
22.9
7.8
12.7
26.8
30.2

0.5 mAP r
8.4
2.5
4.4
9.0
14.4

0.75 ABO
32.9
23.0
27.1
37.6
44.3

Table 5. Image-level supervised instance segmentation results on
the PASCAL VOC 2012 val. set in terms of mean average pre-
cision (mAP%) and Average Best Overlap(ABO). Our approach
ourperforms the state-of-the-art PRM [34] with a relative gain of
17.8% in terms of ABO.

PRM, the image-level supervised object detection methods
MELM [28], CAM [33] and SPN [35] used with MCG and
reported by [34] are also included in Tab. 5.

The proposed method largely outperforms all the base-
line approaches and [34], in all four evaluation metrics.
Even though our approach marginally increases the level
of supervision (lower-count information), it improves the
state-of-the-art PRM with a relative gain of 17.8% in terms
of average best overlap (ABO). Compared to PRM, the
gain obtained at lower IoU threshold (0.25) highlights the
improved location prediction capabilities of the proposed
method. Furthermore, the gain obtained at higher IoU
threshold (0.75), indicates the effectiveness of the proposed
scoring function in assigning higher scores to the object pro-
posal that has highest overlap with the ground-truth object,
as indicated by the improved ABO performance. Fig. 7
shows qualitative instance segmentation comparison be-
tween our approach and PRM.

4.2. Image level supervised Instance segmentation

5. Conclusion

Finally, we evaluate the effectiveness of our density map
to improve the state-of-the-art image-level supervised in-
stance segmentation approach (PRM) [34] on the PASCAL
VOC 2012 dataset (see Sec. 3.3). For a fair comparison, we
utilize the same proposals (MCG) as used in [34]. Follow-
ing [34], the combinatorial grouping framework of [25] is
used in conjunction with the region hierarchies of [21], re-
ferred as MCG. Note that our approach is generic and can
be used with any object proposal method.
In addition to

We proposed an ILC supervised density map estimation
approach for common object counting in natural scenes.
Different to existing methods, our approach provides both
the global object count and the spatial distribution of object
instances with the help of a novel loss function. We further
demonstrated the applicability of the proposed density map
in instance segmentation. Our approach outperforms exist-
ing methods for both common object counting and image-
level supervised instance segmentation.

12404

horsehorsehorsehorsepersonpersonpersonhorsehorsepersonpersoncowcowcowReferences

[1] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li
Fei-Fei. Whats the point: Semantic segmentation with point
supervision. In ECCV, 2016.

[2] Sarah T Boysen and E John Capaldi. The development of nu-
merical competence: Animal and human models. Psychol-
ogy Press, 2014.

[3] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale
aggregation network for accurate and efﬁcient crowd count-
ing. In ECCV, 2018.

[4] Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ram-
prasaath R. Selvaraju, Dhruv Batra, and Devi Parikh. Count-
ing everyday objects in everyday scenes. In CVPR, 2017.

[5] Chhavi. k largest(or smallest) elements in an array-added

min heap method, 2018.

[6] Douglas H Clements. Subitizing: What is it? why teach it?

Teaching children mathematics, 5, 1999.

[7] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes challenge: A retrospective. IJCV,
111(1), 2015.

[8] Mingfei Gao, Ang Li, Ruichi Yu, Vlad I. Morariu, and
Larry S. Davis. C-wsl: Count-guided weakly supervised lo-
calization. In ECCV, 2018.

[9] R Guerrero, B Torre, R Lopez, S Maldonado, and D Onoro.

Extremely overlapping vehicle counting. In IbPRIA, 2015.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[11] Brenda RJ Jansen, Abe D Hofman, Marthe Straatemeier,
Bianca MCW van Bers, Maartje EJ Raijmakers, and Han LJ
van der Maas. The role of pattern recognition in children’s
exact enumeration of small numbers. British Journal of De-
velopmental Psychology, 32(2), 2014.

[12] Maksim Lapin, Matthias Hein, and Bernt Schiele. Analysis
and optimization of loss functions for multiclass, top-k, and
multilabel classiﬁcation. TPAMI, 40(7), 2018.

[13] Issam H. Laradji, Negar Rostamzadeh, Pedro O. Pinheiro,
David Vazquez, and Mark Schmidt. Where are the blobs:
Counting by localization with point supervision. In ECCV,
2018.

[14] Issam H. Laradji, Negar Rostamzadeh, Pedro O. Pinheiro,
David Vazquez, and Mark Schmidt. Where are the blobs:
Counting by localization with point supervision. In ECCV,
2018.

[15] Victor Lempitsky and Andrew Zisserman. Learning to count

objects in images. In NIPS. 2010.

[16] Yuhong Li, Xiaofan Zhang, and Deming Chen. Csrnet: Di-
lated convolutional neural networks for understanding the
highly congested scenes. In CVPR, 2018.

[17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV. Springer, 2014.

[18] Xialei Liu, Joost van de Weijer, and Andrew D Bagdanov.
Leveraging unlabeled data for crowd counting by learning to
rank. In CVPR, 2018.

[19] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 15.

[20] George Mandler and Billie J Shebo. Subitizing: an analysis
of its component processes. Journal of Experimental Psy-
chology: General, 111(1), 1982.

[21] Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbel´aez,
and Luc Van Gool. Convolutional oriented boundaries. In
ECCV. Springer, 2016.

[22] Marsden Mark, McGuiness Kevin, Little Suzanne, and
OConnor. NoelE. Fully convolutional crowd counting on
highly congested scenes. In ICCV, 2017.

[23] Maxime Oquab, L´eon Bottou, Ivan Laptev, and Josef Sivic.
Is object localization for free?-weakly-supervised learning
with convolutional neural networks. In CVPR, 2015.

[24] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. In NIPS-W, 2017.

[25] Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T Barron, Fer-
ran Marques, and Jitendra Malik. Multiscale combinatorial
grouping for image segmentation and object proposal gener-
ation. TPAMI, 39(1), 2017.

[26] Jordi Pont-Tuset and Luc Van Gool. Boosting object propos-

als: From pascal to coco. In ICCV, 2015.

[27] Girshick Ross. Fast r-cnn. In ICCV, 2015.
[28] Fang Wan, Pengxu Wei, Jianbin Jiao, Zhenjun Han, and Qix-
iang Ye. Min-entropy latent model for weakly supervised
object detection. In CVPR, 2018.

[29] Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg,
Jingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learn-
ing ﬁne-grained image similarity with deep ranking.
In
CVPR, 2014.

[30] Kesheng Wu, Ekow Otoo, and Arie Shoshani. Optimizing
connected component labeling algorithms. In Medical Imag-
ing 2005: Image Processing, volume 5747. International So-
ciety for Optics and Photonics, 2005.

[31] Jianming Zhang, Shugao Ma, Mehrnoosh Sameki, Stan
Sclaroff, Margrit Betke, Zhe Lin, Xiaohui Shen, Brian Price,
and Radomir Mech. Salient object subitizing.
In CVPR,
2015.

[32] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao,
and Yi Ma. Single-image crowd counting via multi-column
convolutional neural network. In CVPR, 2016.

[33] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In CVPR, 2016.

[34] Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, and Jianbin
Jiao. Weakly supervised instance segmentation using class
peak response. In CVPR, 2018.

[35] Yi Zhu, Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin
Jiao. Soft proposal networks for weakly supervised object
localization. In ICCV, 2017.

12405

