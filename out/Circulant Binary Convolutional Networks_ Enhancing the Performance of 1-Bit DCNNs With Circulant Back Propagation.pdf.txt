Circulant Binary Convolutional Networks: Enhancing the Performance of 1-bit

DCNNs with Circulant Back Propagation

jironronggChunlei Liu,1 Wenrui Ding,2 Xin Xia,1 Baochang Zhang,4 ∗j Jiaxin Gu,4jiergrong

Jianzhuang Liu,3 Rongrong Ji,5,6 David Doermann7

1 School of Electronic and Information Engineering, Beihang University,

2 Unmanned System Research Institute, Beihang University, 3 Huawei Noah’s Ark Lab,

4 School of Automation Science and Electrical Engineering, Beihang University,

5 School of Information Science and Engineering, Xiamen University,

6 Peng Cheng Laboratory, 7 University at Buffalo
{liuchunlei, ding, xiaxin, bczhang}@buaa.edu.cn

Abstract

The rapidly decreasing computation and memory cost
has recently driven the success of many applications in the
ﬁeld of deep learning. Practical applications of deep learn-
ing in resource-limited hardware, such as embedded de-
vices and smart phones, however, remain challenging. For
binary convolutional networks, the reason lies in the de-
graded representation caused by binarizing full-precision
ﬁlters. To address this problem, we propose new circulant
ﬁlters (CiFs) and a circulant binary convolution (CBCon-
v) to enhance the capacity of binarized convolutional fea-
tures via our circulant back propagation (CBP). The CiFs
can be easily incorporated into existing deep convolution-
al neural networks (DCNNs), which leads to new Circulant
Binary Convolutional Networks (CBCNs). Extensive exper-
iments conﬁrm that the performance gap between the 1-bit
and full-precision DCNNs is minimized by increasing the
ﬁlter diversity, which further increases the representational
ability in our networks. Our experiments on ImageNet show
that CBCNs achieve 61.4% top-1 accuracy with ResNet18.
Compared to the state-of-the-art such as XNOR, CBCNs
can achieve up to 10% higher top-1 accuracy with more
powerful representational ability.

1. Introduction

Deep convolutional neural networks (DCNNs) have been
successfully demonstrated on many computer vision tasks
such as object detection and image classiﬁcation. DC-

∗Baochang Zhang is the corresponding author.

Figure 1. Circulant back propagation (CBP). We manipulate the
learned convolution ﬁlters using the circulant transfer matrix,
which is employed to build our CBP. By doing so, the capacity
of the binarized convolutional features are signiﬁcantly enhanced,
e.g., robustness to the orientation variations in objects, and the per-
formance gap between the 1-bit and full-precision DCNNs is min-
imized. In the example, 4 CiFs are produced based on the learned
ﬁlter and the circular matrix.

NNs deployed in practical environments, however, still face
many challenges. It is particularly true when the portability
and real time performance are required. This is critical be-
cause models of vision applications can require very large
memory, making them impractical for most embedded plat-
forms. Besides, ﬂoating-point inputs and network weights
along with forward or backward data ﬂow can result in a
signiﬁcant computational burden.

2691

Figure 2. Circulant Binary Convolutional Networks (CBCNs) are designed based on circulant and binary ﬁlters to variate the orientations
of the learned ﬁlters in order to increase the representational ability. By considering the center loss and softmax loss in a uniﬁed framework,
we achieve much better performance than state-of-the-art binarized models. Most importantly, our CBCNs also achieve the performance
comparable to well-known full-precision ResNets and WideResNets. The circulant binary ﬁlters are only shown for demonstrating the
computation procedure, which are not saved for testing.

Binary ﬁlters instead of using full-precision weights
have been investigated in DCNNs to compress the deep
models to handle the aforementioned problems. They are
also called 1-bit DCNNs, as each weight parameter and ac-
tivation can be represented by a single bit. As presented in
[10], XNOR has both the convolution weights and input-
s attached to the convolution be approximated with binary
values, providing an efﬁcient implementation of convolu-
tional operations, particularly by reconstructing the unbina-
rized ﬁlters with a single scaling factor. More recently, Bi-
Real Net [20] explores a new variant of residual structure
to preserve the real activations before the sign function and
TBN [16] replaces the sign function with a threshold-based
ternary function to obtain ternary input tensor. Both pro-
vide an optimal tradeoff among memory, efﬁciency and per-
formance. A warm-restart learning-rate schedule in [9] is
adopted to accelerate the training for 1-bit-per-weight net-
works. Furthermore, a method called WAGE [17] is pro-
posed to discretize both training and inference, where not
only weights and activations but also gradients and errors
are quantized. In these previous methods, however, the bi-
narization of the ﬁlters often degrades the representational
ability of the models for the rotation variations in objects.

In this paper, we introduce circulant ﬁlters (CiFs) and
the circulant binary convolution (CBConv) to actively cal-
culate diverse feature maps, which can improve the repre-
sentational ability of the resulting binarized DCNNs. The
key insight of producing CiFs to help back propagation is
shown in Fig. 1. Compared to previous binarized DCN-
N ﬁlters, CiFs are deﬁned based on a circulant operation
on each learned ﬁlter. A new circulant back propagation
(CBP) algorithm is also introduced to develop an end-to-end
trainable DCNN. During the convolution, CiFs are used to
produce diverse feature maps which provide the binarized
DCNNs with the ability to capture variations previously un-
seen.
Instead of introducing extra functional modules or

new network topologies, our method implements CBCon-
v onto the most basic element of DCNNs, the convolution
operator. Thus, it can be naturally fused with modern DC-
NN architectures, upgrading them to more expressive and
compact Circulant Binary Convolutional Networks (CBC-
Ns) for resource limited applications. We design a simple
and unique variation process, which is deployed at each lay-
er and can be solved within the same pipeline of the new
CBP algorithm.
In addition, we consider the center loss
to further enhance the performance of CBCNs as shown in
Fig. 2. Thanks to the low model complexity, such an archi-
tecture is less prone to over-ﬁtting and suitable for resource-
constrained environments. Our CBCNs reduce the required
storage space of full-precision models by a factor of 32,
while achieving better performance than existing binarized
ﬁlters based DCNNs. The contributions of this paper are
summarized as follows:

(1) CiFs are used to obtain more robust feature repre-
sentation, e.g., orientation variations in objects, which min-
imize the performance gap between full-precision DCNNs
and binarized DCNNs.

(2) We develop a CBP algorithm to reduce the loss
during back propagation and make convolutional networks
more compact and efﬁcient. Experimental results show that
CBP is not only effective, but also converged quickly.

(3) The presented circulant convolution is generic, and
can be easily used on existing DCNNs, such as ResNets and
conventional DCNNs. Our highly compressed models out-
perform state-of-the-art binarized models by a large margin
on MNIST, CIFAR and ImageNet databases.

2. Related Work

DCNNs with a large number of parameters consume
considerable computational resources. From our practical
study, about half of the power consumption of a DCNN is

2692

Table 1. A brief description of variables and operators used in the paper.

M :circulant transfer matrix
δW : the gradient of the learned ﬁlter W P : inverse circulant transfer matrix
K : number of orientations for each ﬁlter
g : input feature map index

i : ﬁlter index
h : output feature map index

G : circulant ﬁlter

W : learned ﬁlter
ˆG : binarized ﬁlter
j : orientation index
η : learning rate

F : feature map
L: loss function
l : layer index

related to the model size. Many attempts have been made to
accelerate and simplify DCNNs while avoiding the increase
of the errors. Network binarization is one of the most pop-
ular approaches, which is brieﬂy reviewed below.

The research in [6] demonstrates that the storage of
real-valued deep neural networks such as AlexNet [4],
GoogLeNet [15] and VGG-16 [13] can be reduced signif-
icantly when their 32-bit parameters are quantized to 1-bit.
Expectation BackPropagation (EBP) in [14] uses a varia-
tional Bayesian approach to achive high performance with
a network with binary weights and activations. BinaryCon-
nect (BC) [1] extends the idea of EBP by employing 1-bit
precision weights (1 and -1). Later, Courbariaux et al. [2]
propose BinaryNet (BN) that is an extension of BC and fur-
ther constrains activations to +1 and -1, binaring the input
(except the ﬁrst layer) and the output of each layer. BC
and BN both achieve sufﬁciently high accuracy on smal-
l datasets such as MNIST, CIFAR10 and SVHN. Accord-
ing to Rastegari et al. [11], BC and BN are not very suc-
cessful on large-scale data sets. XNOR [10] has a differ-
ent binarization method and a network architecture. Both
the weights and inputs attached to the convolution are ap-
proximated with binary values, which results in an efﬁcient
implementation of the convolutional operations by recon-
structing the unbinarized ﬁlters with a single scaling fac-
tor. Recent studies such as MCN [18] and Bi-Real Net [20]
have been conducted to explore new network structures and
training techniques for binarizing both weights and activa-
tions while minimizing accuracy degradation using a con-
cept similar to XNOR. MCN introduces modulated ﬁlters to
recover the unbinarized ﬁlters and leads to a new architec-
ture to calculate the network model. Bi-Real Net connects
the real activations to the activations of consecutive blocks
through an identity shortcut.

The results of these studies are encouraging, but due to
the weight binarization process, the representational ability
of the networks can be degraded. This inspires us to seek
a way to increase the ﬁlter variations in order to increase
the network representation ability. In particular, for the ﬁrst
time, we use the circulant matrix to build CiFs for our bi-
narized CNNs. We also develop a CBP algorithm to make
the DCNNs more compact and effective in an end-to-end
framework.

Figure 3. Illustration of the circulant transfer matrix M for K = 8.
The center position stays unchanged, and the remaining numbers
are circled in a counter-clockwise direction. Each column of M is
obtained from m0 with a rotation angle ∈ {0◦, 45◦, ..., 315◦}. It
clearly shows that a circulant ﬁlter explicitly encodes the position
and orientation.

3. Methodology

We design a speciﬁc architecture in CBCNs based on
CiFs, and train it with a new BP algorithm. Attempting
to increase the representational ability reduced by the bina-
rization process, CiFs are designed to enrich the binarized
ﬁlters for the enhancement of the network performance. As
shown in the experiments, the performance drop is marginal
even when the learned network parameters are highly com-
pressed. First of all, Table 1 gives the main notation used in
this paper.

3.1. Circulant Transfer Matrix M

A circulant matrix M is deﬁned by a single vector in
the ﬁrst column, with cyclic permutations of the vector
with offset equal to the column index in the remaining
columns. An important property of the circulant matrix
is that it can produce different representations using sim-
ple vectors or matrices. With this unique characteristic,
we deﬁne the circulant transfer matrix of K columns as
M = (m0, ..., mj, ..., mK−1), j = {0, 1, .., K − 1}:




0 7 6 5 4 3 2 1
1 0 7 6 5 4 3 2
2 1 0 7 6 5 4 3
3 2 1 0 7 6 5 4
4 3 2 1 0 7 6 5
5 4 3 2 1 0 7 6
6 5 4 3 2 1 0 7
7 6 5 4 3 2 1 0





,

(1)

M =

2693

where Wi is a 1D vector containing the ith learned ﬁlter’s
weights (including the unchanged central one during rota-
tion), and ◦ denotes the rotation operation of Wi with mj
(see Fig. 3). Gi0 corresponds to the ith learned ﬁlter and
the other columns of Gi are introduced to increase the rep-
resentational ability.

(a) Traditional convolution

3.3. Forward Propagation of CBCNs based on the

CBConv Module

In CBCNs, a binary CiF denoted by ˆGi is calculated as:

ˆGi = sign(Gi),

(3)

where Gi is the corresponding full-precision CiF, and the
values of ˆGi are 1 or -1. Both Gi and ˆGi are jointly ob-
tained in the end-to-end learning framework. Let the set
of all the binarized ﬁlters in the lth layer be ˆGl. Then the
output feature maps F l+1 are obtained by:

F l+1 = CBConv(sign(F l); ˆGl),

(4)

where CBConv is the convolution operation implemented
as a new module including the CiF generation process (the
blue part in Fig. 2). Fig. 4(b) shows a simple example of
a forward convolution where there is one input feature map
with one generated output feature map. In the CBConv, the
channels of an output feature map are generated as follows:

h,j = X
F l+1

i,g

g ∗ ˆGl
F l
ij,

(5)

(6)

h,0 , F l+1

h,1 , ..., F l+1

F l+1
h = {F l+1

h,K−1},
where ∗ denotes the convolution operation. F l+1
h,k is the kth
g denotes the gth fea-
channel of the hth feature map, and F l
ture map of the input in the lth convolutional layer. In Fig.
4(b), h = 1 and g = 1, where after the CBConv with one
binary CiF, the number of the channels of the output feature
map is the same as that of the input feature map.

3.4. Circulant Back Propagation (CBP)

During the back-propagation, what needs to be learned
and updated are the learned ﬁlters only. And the inverse
transformation of the circulant transfer matrix M is in-
volved in the process of BP to further enhance the repre-
sentational ability of our CBCNs. To facilitate the math de-
scription below, we deﬁne the inverse circulant matrix P of
K columns as P = (p0, ..., pj, ..., pK−1), j = {0, 1, .., K−
1}, where K = 8 and 8 vector inverse rotations are used to
form P . Let δWi be the gradient of a learned ﬁlter Wi. Note
that we need to sum up the gradients of the K sub-ﬁlters in
the corresponding CiF, Gi. Thus:

(b) CBConv

Figure 4. CiF and CBConv examples for K = 4 orientations (0◦,
90◦, 180◦, 270◦) and H = 3. (a) The generation of a CiF and
its corresponding binary CiF based on a learned ﬁlter and M . To
obtain the 4D CiF, the original 2D H×H learned ﬁlter is modiﬁed
to 3D by copying it 3 times. (b) CBConv on an input feature map.
Note that in this paper, a feature map is deﬁned as 3D with K
channels, and these channels are usually not the same.

where K = 8 and 8 vector rotations are used to form M .
The ﬁrst column m0 corresponds to the numbers in Fig. 3,
and the other columns are obtained by a counter-clockwise
rotation of the numbers. Each column of M represents one
rotation angle ∈ {0◦, 45◦, 90◦, 135◦, 180◦, 225◦, 270◦,
315◦}. We set m0 to correspond to the learned ﬁlter and
m1−7 to the derived rotated versions of m0.

3.2. Circulant Filters (CiFs)

We now design the speciﬁc convolutional ﬁlters CiF-
s used in our CBCNs. A CiF is a 4D tensor of size
K×K×H×H, generated based on a learned ﬁlter and M .
These CiFs are deployed across all convolutional layers. As
shown in Fig. 4(a), the original 2D H × H learned ﬁlter is
modiﬁed to 3D by replicating it three times and concatenat-
ing them to obtain the 4D CiF. For K = 4, every channel of
the network input is replicated as a group of four elements.
By doing so, we can easily implement our CBCNs using
PyTorch. One example of the CBConv is illustrated in Fig.
4(b).

To facilitate the math description below, we represent a

2D H × H learned ﬁlter as a 1D vector of size H 2 so that
its corresponding 4D CiF can be represented using a 2D
matrix of size H 2 × K (see Fig. 4(a)). Let Gi be such a
matrix representing the ith CiF. Then

Gi = (Gi0, ..., Gij, ..., Gi(K−1))

= (Wi ◦ m0, ..., Wi ◦ mj, ..., Wi ◦ mK−1),

(2)

δWi = (

K−1
X

j=0

∂ ˆL

∂Gij(pj(0))

, ...,

K−1
X

j=0

∂ ˆL

∂Gij(pj(7))

),

(7)

2694

Wi ← Wi − ηδWi ,

(8)

4. Experiments

where ˆL is the network loss function, and η is the learning
rate. Note that since the central weights of CiFs are not
rotated, their gradients are obtained as in the common BP
procedure and are not presented in Eq. 7. As is shown, the
circular operation involves in our BP process, which makes
CBP be adaptive to orientation variations in objects.

For the gradient of the sign function, some special pro-
cess is necessary due to its discontinuity property. In [2]
and [20], the sign function is approximated by the clip func-
tion and the piecewise polynomial function, respectively,
as shown in Fig. 5(a) and Fig. 5(b) where their corre-
sponding derivatives are also given. Since the derivative
of the sign function (an impulse) can be represented as
σ2 ), in this work, we use this Gaussian
lim
σ→0
function (Fig. 5(c)) as the approximation of the gradient:

σ√π exp(− G2

1

∂ ˆGi
∂Gi

=

A
σ√π

exp(−

G2
i
σ2 ),

(9)

where A and σ are the amplitude gain and variance of the
Gaussian function, respectively, which are determined em-
pirically. In our experiments, we ﬁnd that our approxima-
tion in Fig. 5(c) is better than those in Fig. 5(a) and Fig.
5(b). From the equations above, we can see that the BP
process can be easily implemented. Thus only updating
the learned convolution ﬁlters with the help of CiFs, our
CBCNs are signiﬁcantly compact and efﬁcient, reducing the
memory storage by 32. Finally, the learning algorithm to
train CBCNs is given in Algorithm 1.

Algorithm 1 CBCN Training.
Require: The training dataset; the full-precision learned
ﬁlters W ; the circulant transfer matrix M ; the num-
ber of orientations K; hyper-parameters such as ini-
tial learning rate, weight decay, convolution stride and
padding size.

Ensure: A CBCN based on the CiFs.

1: Initialize W randomly;
2: repeat
3:

4:

5:

6:

7:

8:

9:

10:

11:

// Forward propagation
for all l = 1 to L convolutional layer do

Use Eqs. 1 and 2 to obtain Gl;
F l+1 = CBConv(sign(F l), sign(Gl));

end for
// Back propagation
for all l = L to 1 do

Calculate the gradients δW ; // Using Eq. 7
W ← W − ηδW ; // Update the parameters

end for

12:
13: until the maximum epoch.

Our CBCNs are evaluated on object classiﬁcation us-
ing MNIST [7], CIFAR10/100 [5] and ILSVRC12 Ima-
geNet datasets [12]. LeNet [7], WideResNet (WRN) [19]
and ResNet18 [3] are employed as the backbone networks
to build our CBCNs simply by replacing the full-precision
convolution with CBConv. Also, binarizing the neuron ac-
tivations is carried out in all of our experiments.

4.1. Datasets and Implementation Details

Datasets: The MINIST [7] dataset is composed of a
training set of 60,000 and a testing set of 10,000 32 × 32
grayscale images of hand-written digits from 0 to 9. Each
sample is randomly rotated in [−45◦, 45◦] yielding MNIST-
rot.

CIFAR10 [5] is a natural image classiﬁcation dataset
containing a training set of 50, 000 and a testing set of
10, 000 32 × 32 color images across the following 10 class-
es: airplanes, automobiles, birds, cats, deers, dogs, frogs,
horses, ships, and trucks, while CIFAR100 consists of 100
classes. And we randomly rotate each sample in the CI-
FAR10 dataset between [0, 360◦] to yield CIFAR10-rot.

ImageNet object classiﬁcation dataset [12] is more chal-
lenging due to its large scale and greater diversity. There are
1000 classes and 1.2 million training images and 50k vali-
dation images in it. We compare our method with the state-
of-the-art on the ImageNet dataset and we adopt ResNet18
to validate the superiority and effectiveness of CBCNs.

In the implementation, LeNet, WRN, and ResNet18
backbone networks are used to build CBCNs. We simply re-
place the full-precision convolution with CBConv, and keep
other components unchanged. The parameters σ and A for
the Gaussian function in the Eq. 9 are set to 1 and 3√2π,

respectively. More details are elaborated below.

LeNet Backbone: LetNet contains four simple convolu-
tional layers. We adopt Max-pooling and ReLU after each
convolution layer, and a dropout layer after the fully con-
nected layer to avoid over-ﬁtting. The initial learning rate
is 0.01 with no degradation before reaching the maximum
epoch of 50 for MNIST and MNIST-rot.

WRN Backbone: WRN is a network structure similar
to ResNet with a depth factor k to control the feature map
depth dimension expansion through 3 stages, within which
the dimensions remain unchanged. For simplicity we ﬁx the
depth factor to 1. Each WRN has a parameter i which indi-
cates the channel dimension of the ﬁrst stage and we set it to
16 leading to a network structures 16-16-32-64. The train-
ing details are the same as in [19]. The initial learning rate is
0.01 with a degradation of 10% for every 60 epochs before
it reaches the maximum epoch of 200 for CIFAR10/100 and
CIFAR10-rot. For example, WRN22 is a network with 22
convolutional layers and similarly for WRN40.

2695

(a)

(b)

(c)

Figure 5. Three approximations of the sign function for its gradient computation. (a) The clip function and its derivative in [2]. (b) The
piecewise polynomial function and its derivative in [20]. (c) Our proposed function and its derivative.

Table 2. Error rates on the MNIST and CIFAR10 and their variants. ‘fp’ denotes the full precision result. The bold denotes the best result
among the binary networks.

Dataset

Backbone

kernel stage

original (%)

rot (%)

fp

XNOR CBCN

fp

XNOR CBCN

MNIST

LeNet

CIFAR10 ResNet18

5-10-20-40
10-20-40-80
16-16-32-64
32-32-64-128
32-64-128-256

0.91
0.69
8.94
6.63
5.27

3.76
1.50
22.88
15.55
13.43

1.91
1.24
10.9
8.13
8.09

2.77
1.89
19.07
12.96
10.47

17.26
7.77
40.75
33.69
21.93

5.76
4.95
19.68
16.2
15.11

Figure 6. Network architectures of ResNet18, XNOR on ResNet18 and CBCN on ResNet18. Note that CBCN doubles the shortcuts.

Table 3. Performance (accuracy, %) contributions of the compo-
nents in CBCNs on CIFAR10, where ConvComp, S, C, and G
denote the convolution comparison between BConv in XNOR and
CBConv, doubled shortcuts, using the center loss, and using the
Gaussian gradient function, respectively. The bold number repre-
sents the best result.

Conv
-Comp

76.3
81.84
84.79
86.79

S

S+C

S+G

80.53
85.79
89.10
90.80

80.97
86.23
89.6
91.27

81.65
86.67
90.22
91.53

S+C
+G

82.32
87.56
90.83
92.02

XNOR

CBCN (K=2)
CBCN (K=4)
CBCN (K=8)

ResNet18 Backbone: Fig. 6 respectively illustrates the

architectures of ResNet18, XNOR and CBCNs. SGD is
used as the optimization algorithm with a momentum of
0.9 and a weight decay 1e-4. The initial learning rate is
0.01 with a degradation of 10% for every 20 epochs before
reaching the maximum epoch of 70 on ImageNet, while on
CIFAR10/100, the initial rate is 0.01 with a degradation of
10% for every 60 epochs before reaching the maximum e-
poch of 200.

4.2. Rotation Invariance

With LeNet and ResNet18 backbones, we build XNOR
and CBCNs and compare them on MNIST, MNIST-rot, CI-
FAR10, and CIFAR10-rot. K is set to 4 in CBCNs.

Table 2 gives the results in terms of error rates, and ‘f-
p’ represents the full-precision results. The state-of-the-

2696

art XNOR has a dramatical performance drop on the more
challenging rotated datasets. On MNIST-rot, with the k-
ernel stage 5-10-20-40, CBCN shows impressive perfor-
mance improvement 11.5% over XNOR, while 1.85% im-
provement is achieved on MNIST. On CIFAR10-rot, with
the kernel stage 16-16-32-64, CBCN has about 20% im-
provement over XNOR. From Table 2, we can also see
that on CIFAR10-rot, the performance gap between CBCN
and XNOR decreases from about 20% to 17% to 6% with
the increase of the kernel stage (parameters), meaning that
the improvement of CBCN over XNOR is more signiﬁcant
when they have fewer parameters. The results in Table 2
conﬁrm that with the improved representation ability from
the proposed CiFs, CBCNs are more robust than conven-
tional binarization methods for rotation variations of input
images.

4.3. Ablation Study

In this section, we study the performance contributions
of the components in CBCNs, which include CBConv, cen-
ter loss, additional shortcuts (Fig. 6), and the Gaussian gra-
dient function (Eq. 9). CIFAR10 and ResNet18 with kernel
stage 16-16-32-64 are used in this experiment. The details
are given below.

1) We only replace the convolution BConv in XNOR
with our CBConv convolution and compare the results.
As shown in the ConvComp column in Table 3, CBC-
N (K=4) achieves about 8% accuracy improvement over
XNOR (84.79% vs. 76.3%) using the same network struc-
ture and shortcuts as in ResNet18. This signiﬁcant improve-
ment veriﬁes the effectiveness of our CBConv.

2) In CBCNs, if we double the shortcuts (Fig. 6), we
can also ﬁnd a decent improvement from 84.79% to 89.10%
(see the column under S in Table 3), which shows that the
increase of shortcuts can also enhance binarized deep net-
works.

3) Fine-tuning CBCN with the center loss can also im-
prove the performance of CBCN by 0.5% as shown in the
column under S+C in Table 3).

4) Replacing the piecewise polynomial function in [20]
with the Gaussian function for back propagation, CBCN
obtains 1.12% improvement (90.22% vs. 89.10%), which
shows that the gradient function we use is a better choice.

5) From the column under S in Table 3, we can see
that CBCN performs better using more orientations in CiF-
s. More orientations can better deal with the problem of
degraded representation caused by network binarization.

4.4. Accuracy Comparison with State of the Art

(a) Train accuracy on CIFAR10.

(b) Test accuracy on CIFAR10.

Figure 7. Training and Testing error curves of CBCN and XNOR
based on WRN40 for the CIFAR10 experiments.

son with the original WRNs with the initial channel dimen-
sion 16 in Table 4. Then, we compare our results with other
state-of-the-arts such as BNN [1], BWN [10], and XNOR
[10]. It is observed that at least 1.84% (= 93.42%-91.58%)
accuracy improvement is gained with our CBCN, and in
other cases, larger margins are achieved. Also, we plot the
training and testing loss curves of XNOR and CBCN, re-
spectively, in Fig. 7, which clearly show that CBCN (CBP)
converges faster than XNOR (BP).

CIFAR10/100: The same parameter settings are used in
CBCNs on both CIFAR10 and CIFAR100. We ﬁrst com-
pare our CBCNs with original ResNet18 with stage kernels
as 16-16-32-64 and 32-64-128-256, followed by a compari-

ImageNet: Four state-of-the-art methods on ImageNet
are chosen for comparison: Bi-Real Net [20], BinaryNet
[2], XNOR [10] and ABC-Net [8]. These four networks are
representative methods of binarizing both network weights

2697

Table 5. Classiﬁcation accuracy (%) on ImageNet. The bold represents the best result among the binary networks. K = 4 in CBCN.

Full-Precision XNOR ABC-Net BinaryNet Bi-Real CBCN

ResNet18

Top-1
Top-5

69.3
89.2

51.2
73.2

42.7
67.6

42.2
67.1

56.4
79.5

61.4
82.80

Table 4. Classiﬁcation accuracy (%) based on ResNet18 and
WRN40, respectively, on CIFAR10/100. The bold represents the
best result among the binary networks. K = 4 in CBCN.

Model

Kernel Stage

CIFAR CIFAR

Dataset

BNN
BWN

-
-

XNOR (ResNet18)
XNOR (WRN40)

64-64-128-256
64-64-128-256

ResNet18

CBCN
CBCN
WRN40
WRN22
CBCN

16-16-32-64
16-16-32-64

32-64-128-256

16-16-32-64
16-16-32-64
16-16-32-64

-10

89.85
90.12
87.1
91.58
94.84
90.22
91.60
95.8
90.32
93.42

-100

-
-

66.08
73.18
75.37
69.97
70.07
79.41
67.19
74.80

and activations and achieve state-of-the-art results. All the
methods in Table 5 perform the binarization of ResNet18.
For a fair comparison, our CBCN contains the same amount
of learned ﬁlters as ResNet18. The comparative results in
Table 5 are quoted directly from the references, except that
the result of BinaryNet is from [8]. The comparison clearly
indicates that the proposed CBCN outperforms the four bi-
nary networks by a considerable margin in terms of both the
top-1 and top-5 accuracies. Speciﬁcally, for top-1 accura-
cy CBCN outperforms BinaryNet and ABC-Net with a gap
over 18%, achieves about 10% improvement over XNOR,
and about 5% over the latest Bi-Real Net. In Fig. 8, we plot
the training and testing loss curves of XNOR and CBCN,
respectively. It clearly shows that using our CBP algorithm,
CBCN converges faster than XNOR.

5. Conclusion

In this paper, we have proposed new circulant binary
convolutional networks (CBCNs) that are implemented by
a set of binary circulant ﬁlters (CiFs). The proposed CiFs
and circulant binary convolution (CBConv) are used to en-
hance the representation ability of binary networks. CBC-
Ns can be trained end-to-end with the developed circulant
BP (CBP) algorithm. Our extensive experiments demon-
strate that CBCNs have superiority over state-of-the-art bi-
nary networks, and obtain results that are more close to the
full-precision backbone networks ResNets and WRNs, with
a storage reduction of about 32 times. As a generic convo-
lutional layer, CBConv can also be used on various tasks,

(a) Top 1 accuracy on ImageNet.

(b) Top 5 accuracy on ImageNet.

Figure 8. Training and Testing error curves of CBCN and XNOR
based on the ResNet18 backbone on ImageNet.

which is our future work.

6. Acknowledgment

The work was supported by the National Key Re-
search and Development Program of China (Grant No.
2016YFB0502602) and the National Key R&D Plan
(2017YFC0821102). Baochang Zhang is the corresponding
author.

2698

[17] S. Wu, G. Li, F. Chen, and L. Shi. Training and inference
with integers in deep neural networks. International Confer-
ence on Learning Representations, 2018.

[18] C. L. R. J. J. H. X. C. Xiaodi Wang, Baochang Zhang and
J. Liu. Modulated convolutional networks. In Computer Vi-
sion and Pattern Recognition, 2018.

[19] S. Zagoruyko and N. Komodakis. Wide residual networks.

arXiv preprint arXiv:1605.07146, 2016.

[20] W. L. X. Y. W. L. Zechun Liu, Baoyuan Wu and K.-T. Cheng.
Bi-real net: Enhancing the performance of 1-bit cnns with
improved representational capability and advanced training
algorithm.
In European Conference on Computer Vision,
2018.

References

[1] M. Courbariaux, Y. Bengio, and J. P. David. Binaryconnect:
Training deep neural networks with binary weights during
propagations. In Advances in Neural Information Processing
Systems.

[2] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and
Y. Bengio. Binarized neural networks: Training deep neu-
ral networks with weights and activations constrained to +1
or -1. arXiv preprint arXiv:1602.02830, 2016.

[3] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer

for image recognition.
Vision and Pattern Recognition, pages 770–778, 2016.

[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
International Conference on Neural Information Processing
Systems, pages 1097–1105, 2012.

[5] N. Krizhevsky and Hinton. The cifar-10 dataset. online:

http://www. cs. toronto. edu/kriz/cifar. html.

[6] L. Lai, N. Suda, and V. Chandra. Deep convolutional neu-
ral network inference with ﬂoating-point weights and ﬁxed-
point activations. arXiv:1703.03073v1.

[7] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[8] X. Lin, C. Zhao, and W. Pan. Towards accurate binary con-

volutional neural network. arXiv:1711.11294, 2017.

[9] M. D. McDonnell. Training wide residual networks for de-
ployment using a single bit for each weight. In International
Conference on Learning Representations, 2018.

[10] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-
net: Imagenet classiﬁcation using binary convolutional neu-
ral networks. In European Conference on Computer Vision,
pages 525–542, 2016.

[11] R. Rigamonti, A. Sironi, V. Lepetit, and P. Fua. Learning
separable ﬁlters. In Computer Vision and Pattern Recogni-
tion, pages 2754–2761, 2013.

[12] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, and M. Bernstein.
Imagenet large scale visual recognition challenge. Interna-
tional Journal of Computer Vision, 115(3):211–252, 2015.

[13] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556., 2014.

[14] D. Soudry, I. Hubara, and R. Meir. Expectation backpropa-
gation: parameter-free training of multilayer neural networks
with continuous or discrete weights. In International Confer-
ence on Neural Information Processing Systems, pages 963–
971, 2014.

[15] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions.
In IEEE Conference on
Computer Vision and Pattern Recognition, 2015.

[16] D. Wan, F. Shen, L. Liu, F. Zhu, J. Qin, L. Shao, and H. T.
Shen. Tbn: Convolutional neural network with ternary inputs
and binary weights. In European Conference on Computer
Vision, pages 315–332, 2018.

2699

