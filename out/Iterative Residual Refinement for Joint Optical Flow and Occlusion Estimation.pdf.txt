Iterative Residual Reﬁnement for Joint Optical Flow and Occlusion Estimation

Junhwa Hur

Stefan Roth

Department of Computer Science, TU Darmstadt

Abstract

Deep learning approaches to optical ﬂow estimation
have seen rapid progress over the recent years. One com-
mon trait of many networks is that they reﬁne an initial
ﬂow estimate either through multiple stages or across the
levels of a coarse-to-ﬁne representation. While leading to
more accurate results, the downside of this is an increased
number of parameters. Taking inspiration from both clas-
sical energy minimization approaches as well as residual
networks, we propose an iterative residual reﬁnement (IRR)
scheme based on weight sharing that can be combined with
several backbone networks. It reduces the number of pa-
rameters, improves the accuracy, or even achieves both.
Moreover, we show that integrating occlusion prediction
and bi-directional ﬂow estimation into our IRR scheme can
further boost the accuracy. Our full network achieves state-
of-the-art results for both optical ﬂow and occlusion esti-
mation across several standard datasets.

1. Introduction

Akin to many areas of computer vision, deep learning
has had a signiﬁcant impact on optical ﬂow estimation. But
in contrast to, e.g., object detection [19] or human pose
estimation [55], the accuracy of deep learning-based ﬂow
methods on public benchmarks [10, 17, 41] had initially not
surpassed that of classical approaches. Still, the efﬁcient
test-time inference has led to their widespread adoption as
a sub-module in applications requiring to process tempo-
ral information, including video object segmentation [13],
video recognition [15, 43, 64], and video style transfer [11].
FlowNet [14] pioneered the use of convolutional neural
networks (CNNs) for estimating optical ﬂow and relied on a
– by now standard – encoder-decoder architecture with skip
connections, similar to semantic segmentation [36], among
others. Since the ﬂow accuracy remained behind that of
classical methods based on energy minimization, later work
has focused on designing more powerful CNN architectures
for optical ﬂow. FlowNet2 [26] remedied the accuracy lim-
itations of FlowNet and started to outperform classical ap-
proaches. Its main principle is to stack multiple FlowNet-

l

n
a
e
C
n
a
r
T

i

 

 
l

i

 

e
t
n
S
n
o
E
P
E
A

 

5

4.5

4

3.5

3

2.5

2

0

FlowNetS

FlowNetC

SpyNet

PWC-Net

Ours (PWC-Net + Occ)

Ours (PWC-Net + Bi)

LiteFlowNet

Ours (PWC-Net + IRR)

Ours (IRR-PWC)

10

20

30

40

50

Number of parameters (million)

Figure 1. Accuracy / network size tradeoff of CNNs for opti-
cal ﬂow: Combining our iterative residual reﬁnement (IRR), as
well as bi-directional (Bi) and occlusion estimation (Occ) with
PWC-Net [52] in comparison to previous work. Our full model
(IRR-PWC), combining all three components, yields signiﬁcant
accuracy gains over [52] while having many fewer parameters.

family networks [14], such that later stages effectively re-
ﬁne the output from the previous ones. However, one of the
side effects of this stacking is the linearly and strongly in-
creasing number of parameters, being a burden for the adop-
tion in other applications. Also, stacked networks require
training the stages sequentially rather than jointly, resulting
in a complex training procedure in practice.

More recently, SpyNet [45], PWC-Net [52], and Lite-
FlowNet [24] proposed lightweight networks that still
achieve competitive accuracy (cf . Fig. 1). SpyNet adopts
coarse-to-ﬁne estimation in the network design, a well-
known principle in classical approaches. It residually up-
dates the ﬂow across the levels of a spatial pyramid with in-
dividual trainable weights and demonstrates better accuracy
than FlowNet but with far fewer model parameters. Lite-
FlowNet and PWC-Net further combine the coarse-to-ﬁne
strategy with multiple ideas from both classical methods
and recent deep learning approaches. Particularly PWC-Net
outperformed all published methods on the common public
benchmarks [10, 17, 41].

Interestingly, many recent deep learning approaches for
ﬂow [24, 26, 45, 52] have a common structure: From a
rough ﬁrst ﬂow estimate, later modules or networks re-

5754

5755

5756

5757

5758

fw, backward optical ﬂow f i

ﬂow maps f i
bw, occlusion maps
in the ﬁrst image oi
2 for each
iteration step, where i = 1, . . . , N . Forward and backward
optical ﬂow are supervised using the L2,1 norm as

1 and in the second image oi

ﬂow = 1
li

2 X (cid:0)kf i

fw − ffw,GTk2 + kf i

bw − fbw,GTk2(cid:1),

(4)

number of foreground objects, object size, and random pa-
rameters for generating the motion of each object. As the
motion is parametrized by a 3 × 3 matrix, it is easy to calcu-
late not only backward ground-truth ﬂow but also occlusion
maps by conducting visibility checks. The number of im-
ages in the training and validation sets are the same as in
FlyingChairs (i.e. 22232 and 640, respectively).

whereas for the supervision of the two occlusion maps we
use a weighted binary cross-entropy

4.2. Implementation details

occ = − 1
li

2 X (cid:0)wi
+wi

1oi
2oi

1 log o1,GT + ¯wi
2 log o2,GT + ¯wi

1(1−oi
2(1−oi

1) log(1−o1,GT)
2) log(1−o2,GT)(cid:1).
(5)

Here, we apply the weights wi

1 =
1)+P (1−o1,GT) to take into account the number of

H ·W
1+P o1,GT

and ¯wi

1 =

P oi

H ·W

P (1−oi
predictions and true labels.

Our ﬁnal loss is the weighted sum of the two losses
above, taken over all iteration steps using the same multi-
scale weights αs as in the original papers.
In case of
FlowNet [14], the ﬁnal loss becomes

lFlowNet =

1
N

N

S

X

X

i=1

s=s0

αs(li,s

ﬂow + λ · li,s

occ),

(6)

where s denotes the scale index given in Fig. 3 of [14]. In
case of PWC-Net [52], the number of scales is equal to the
number of iterations, hence the ﬁnal loss is

lPWC-Net =

1
N

N

X

i=1

αi(li

ﬂow + λ · li

occ).

(7)

λ weighs the ﬂow against the occlusion loss. In every itera-
tion, we calculate the λ that makes the loss of the ﬂow and
the occlusion be equal. We empirically found that this strat-
egy yields better accuracy than just using a ﬁxed trade-off.

4. Experiments

4.1. FlyingChairsOcc dataset

Lacking a suitable dataset, we create our own dataset for
the supervision of bi-directional ﬂow and the two occlusion
maps, with ground truth for forward ﬂow, backward ﬂow,
and occlusion maps at the ﬁrst and second frame. To build
the dataset, we follow the exact protocol of the FlyingChairs
dataset [14]. We refer to this dataset as FlyingChairsOcc.

We crawl 964 background images with a resolution of
1024 × 768 from Flickr and Google using the keywords
cityscape, street, and mountain. As foreground objects,
we use 809 chair images rendered from CAD models with
varying views and angles [3]. Then we follow the exact
protocol of [14] for generating image pairs, including the

Training details. We follow the training settings of
FlowNet respective PWC-Net for a fair comparison. We use
the same geometric and photometric augmentations with
additive Gaussian noise as described in [26]. After applying
the geometric augmentation on the occlusion ground truth,
we additionally check for pixels moving outside of the im-
age boundary (i.e. out-of-bound pixels) and set them as oc-
cluded. Note that no multi-stage training is needed.

We ﬁrst train the proposed model on our FlyingChairs-
Occ dataset with learning rate schedule Sshort (instead of
Slong), described in [26]. Next, we ﬁne-tune on the Flying-
Things3D-subset dataset [39], which contains much larger
displacements; we use half the Sﬁne learning rate schedule
[26]. We empirically found that using shorter schedules
was enough as our model converged faster. We ﬁnally ﬁne-
tune on different public benchmark datasets, including Sin-
tel [10] and KITTI [17], following the ﬁne-tuning protocol
of [53]. We use a smaller minibatch size of 4, as our model
implicitly increases the batch size by performing iterative
bi-directional estimation with a single model.

Lacking other ground truth, we only use the forward ﬂow
and the occlusion map for the ﬁrst frame for supervision
on Sintel; for KITTI we only use the forward ﬂow.
Im-
portantly, our model is still trainable when ground truth is
available only for one direction (e.g., forward ﬂow with oc-
clusion map at the ﬁrst frame), since both temporal direc-
tions share the same “unidirectional” decoder.

4.3. Ablation study

To see the effectiveness of each proposed component, we
conduct an ablation study by training our model in mul-
tiple settings. All models are trained on the FlyingChair-
sOcc dataset with the Sshort schedule and tested on multiple
datasets to assess generalization across datasets. We use a
minibatch size of 4 when either bi-directional estimation or
iterative residual reﬁnement is on, or the original minibatch
size of 8, otherwise. For a simpler ablation study, we use
two iteration steps when applying IRR on FlowNet [14].

Table 1 assesses the optical ﬂow in terms of the average
end-point error (EPE) and occlusion estimation with the av-
erage F1-score, if applicable for the respective conﬁgura-
tion. In contrast to ﬁndings in recent work [27], estimating
occlusion together yields a gradual improvement of the ﬂow
of up to 5% on the training domain, and an even bigger im-

5759

Figure 8. Qualitative examples from the ablation study on PWC-Net: (left to right) overlapped input images, ground-truth ﬂow, the
original PWC-Net [52], our PWC-Net with IRR, our PWC-Net with Bi-Occ-IRR, and our full model (i.e. IRR-PWC).

R Chairs ChairsOcc
R
Validation

Full

I

Sintel Clean
Training

Sintel Final
Training

Rel.
Param.

Method

Chairs ChairsOcc
Full
Validation

Sintel Clean Sintel Final
Training

Training

Rel.
Param.

i

B

c
c
O

✓

✓

✓ ✓

✓

✓

✓

✓ ✓

✓ ✓ ✓
✓ ✓ ✓+

✓

✓

✓ ✓

✓

✓

✓

✓ ✓

✓ ✓ ✓
✓ ✓ ✓+

]
4
1
[

t
e
N
w
o
l

F

]
2
5
[

-

t
e
N
C
W
P

2.39
2.43
2.29
2.36
2.31
2.14
2.22
2.05
1.92

2.03
2.06
1.94
2.01
1.99
2.08
1.91
1.98
1.67

2.27
2.30
2.18 (0.690)
2.22
2.20 (0.691)
2.00
2.10 (0.689)
1.91 (0.699)
1.77 (0.736)

1.89
1.87
1.79 (0.706)
1.83
1.82 (0.696)
1.90
1.73 (0.700)
1.81 (0.698)
1.48 (0.757)

4.35
4.40
4.26 (0.521)
3.77
4.21 (0.515)
3.45
3.56 (0.507)
3.40 (0.528)
3.32 (0.596)

3.13
2.98
3.16 (0.616)
2.79
3.01 (0.618)
2.80
2.64 (0.630)
2.69 (0.633)
2.34 (0.677)

5.44
5.53
5.51 (0.493)
5.00
5.46 (0.488)
4.96
5.03 (0.486)
5.08 (0.502)
4.92 (0.560)

4.41
4.14
4.35 (0.581)
4.10
4.39 (0.581)
4.13
4.09 (0.593)
4.03 (0.598)
3.95 (0.624)

0 %
0 %
+38.5%
0 %
+38.5%
0 %
+38.5%
+38.5%
+40.7%

0 %
0 %
+87.4%
−61.2%
+87.4%
−61.2%
−34.7%
−34.7%
−26.4%

Table 1. Ablation study of our design choices on the two base-
line models. The numbers indicate the average end-point error
(EPE) for optical ﬂow (the lower the better) and the average F1-
score for occlusion in parentheses, where available (the higher the
better). Bi: Bi-directional estimation, Occ: Joint occlusion estima-
tion, IRR: Iterative residual reﬁnement, IRR+: Iterative residual
reﬁnement including bilateral reﬁnement and occlusion upsam-
pling layer. The ﬁnal column reports the relative changes on the
number of parameters comparing to the vanilla baseline.

provement across different datasets when combined on top
of bi-directional estimation (Bi) or IRR. We believe this to
mainly stem from using a separate occlusion decoder in-
stead of a joint decoder [27]. Bi-directional estimation by
itself yields at most a marginal improvement on ﬂow, but it
is important for the input of the occlusion upsampling layer,
which brings very large beneﬁts on occlusion estimation. It-
erative residual reﬁnement yields consistent improvements
in ﬂow accuracy on the training domain, and perhaps sur-
prisingly a much better generalization across datasets, with
up to 10% improvement in EPE. We presume that this bet-
ter generalization comes from training a single decoder to
handle feature maps from all iteration steps or pyramid lev-
els, which encourages generalization even across datasets.
The beneﬁts of using IRR become even clearer when com-
bined with other components. For example, FlowNet with
Bi, Occ, and IRR demonstrates up to 20% improvement in
EPE on Sintel Clean compared to only using Bi and Occ.
Additionally, the bilateral reﬁnement and the upsampling

No reﬁnement
Ours
LiteFlowNet’s [24]

1.98
1.66
1.74

1.81 (0.698)
1.45 (0.735)
1.58 (0.688)

2.69 (0.633)
2.32 (0.648)
2.34 (0.596)

4.03 (0.598)
3.90 (0.602)
3.86 (0.543)

0 %
+12.3%
+29.5%

Table 2. Comparison of our bilateral reﬁnement layer against that
of LiteFlowNet [24].

Method

Chairs
Full

ChairsOcc
Validation

Sintel Clean
Training

Sintel Final
Training

Rel.
Param.

No upsampling
Ours
[26, 27]

1.66
1.67
2.18

1.45 (0.735)
1.48 (0.757)
2.01 (0.712)

2.32 (0.648)
2.34 (0.677)
2.90 (0.624)

3.90 (0.602)
3.95 (0.624)
4.37 (0.577)

0 %
+0.49%
+9.21%

Table 3. Comparison of our occlusion upsampling layer and the
reﬁnement network from FlowNet2 [26, 27].

layer signiﬁcantly improve the accuracy of both ﬂow and
occlusion with a small overhead of only 0.83M parame-
ters. For PWC-Net, we obtain a signiﬁcant accuracy boost
of 17.7% on average over the baseline, while reducing the
number of parameters by 26.4%. We name the full ver-
sions of the models including all modules IRR-FlowNet
and IRR-PWC. Fig. 8 highlights the improvement of the
ﬂow from our proposed components with qualitative exam-
ples. Please note the completeness and sharp boundaries.

Bilateral reﬁnement. We compare our bilateral reﬁnement
layer with the reﬁnement layer of LiteFlowNet [24] based
on a PWC-Net with Bi, Occ, and IRR components enabled.
Table 2 shows that the beneﬁt of our design choice (i.e. shar-
ing weights) holds for bilateral reﬁnement as well, yielding
better accuracy for ﬂow and particularly for occlusion, with
2.5× fewer parameters than that of [24].

Occlusion upsampling layer. Similar to our upsampling
layer, [27] uses a reﬁnement network from FlowNet2 [26]
to upsample the intermediate quarter-resolution outcome
back to the original resolution. We compare our upsam-
pling layer with the reﬁnement network from [26, 27],
adding it to our network based on a PWC-Net backbone
with Bi, Occ, IRR, and the bilateral reﬁnement layer en-
abled. Table 3 shows the clear beneﬁts of using our upsam-
pling layer, yielding signiﬁcant gains in both tasks while
requiring fewer parameters. The reﬁnement network from
FlowNet2 [26] actually degrades the accuracy of ﬂow es-
timation. We presume this may stem from differences in

5760

Number of iterations or stacking stages

1

2

3

4

5

Method

IRR on a single FlowNetS
Stacking multiple FlowNetS

4.358
4.445

3.545
3.553

3.325
3.377

3.303
3.391

3.302
3.517

Table 4. n× IRR vs. n× stacking: EPE on Sintel Clean.

Method

Training

Test

Parameters

Clean

Final

Clean

Final

ContinualFlow ROB†§ [42]
MFF§ [46]
IRR-PWC (Ours)
PWC-Net+† [53]
ProFlow§[37]
PWC-Net-ft-ﬁnal [53]
DCFlow [61]
FlowFieldsCNN [6]
MR-Flow [59]
LiteFlowNet [24]
S2F-IF [62]
SfM-PM [38]
FlowFields++ [49]
FlowNet2 [26]

–
–

(1.92)
(1.71)

–

–
–

(2.51)
(2.34)

–

(2.02)

(2.08)

–
–

–
–

1.83
(1.35)

3.59
(1.78)

–
–
–

–
–
–

(2.02)

(3.14)

3.34
3.42
3.84
3.45
2.82
4.39
3.54
3.78
2.53
4.54
3.50
2.91
2.94
3.96

4.53
4.57
4.58
4.60
5.02
5.04
5.12
5.36
5.38
5.38
5.42
5.47
5.49
6.02

14.6 M
N/A

6.36M
8.75M
–
8.75M
–
5.00M
–
5.37M
–
–
–

162.5 M

Table 5. MPI Sintel Flow: Average end-point error (EPE) and
number of CNN parameters. §using more than 2 frames, †using
additional datasets (KITTI and HD1k) for better accuracy.

training. FlowNet2’s reﬁnement layer may require piece-
wise training, while our model is trained all at once.

Different IRR steps on FlowNet. For FlowNet, we can
freely choose the number of IRR steps as we iteratively re-
ﬁne previous estimates by re-using a single network. We try
different numbers of IRR steps on vanilla FlowNetS [14]
(i.e. without Bi or Occ) and compare with stacking multi-
ple FlowNetS networks. All networks are trained on Fly-
ingChairsOcc with the Sshort schedule, minibatch size of
8, and tested on Sintel Clean.
As shown in Table 4, the
accuracy keeps improving with more IRR steps and stably
settles at more than 4 steps.
In contrast, stacking multi-
ple FlowNetS networks overﬁts on the training data after
3 steps, and is consistently outperformed by IRR with the
same number of stages. This clearly demonstrates the ad-
vantage of our IRR scheme over stacking: better accuracy
without linearly increasing the number of parameters.

4.4. Optical ﬂow benchmarks

We test the accuracy of our IRR-PWC on the public Sin-
tel [10] and KITTI [17, 41] benchmarks. When ﬁne-tuning,
we use the robust training loss as in [24, 52, 53] for ﬂow,
and standard binary cross-entropy for occlusion. On Sintel
Final, our IRR-PWC achieves a new state of the art among
2-frame methods. Comparing to the PWC-Net baseline (i.e.
PWC-Net-ft-ﬁnal) trained in the identical setting, our con-
tributions improve the ﬂow accuracy by 9.18% on Final and
12.36% on Clean, while using 26.4% fewer parameters. On
KITTI 2015, our IRR-PWC again outperforms all published
2-frame methods, improving over the baseline PWC-Net.

When ﬁne-tuning on benchmarks, our important obser-

MFF§ [46]
IRR-PWC (Ours)
PWC-Net+ [53]
LiteFlowNet [24]
PWC-Net [52]
ContinualFlow ROB†§ [42]
MirrorFlow [25]
FlowNet2 [26]

Training

AEPE

Fl-all

–

(1.63)
(1.45)
(1.62)
(2.16)

–
–

(2.30)

–

(5.32%)
(7.59%)
(5.58%)
(9.80%)

–

9.98%
(8.61%)

Test

Fl-All

7.17%
7.65%
7.72%
9.38%
9.60%
10.03%
10.29%
10.41%

Table 6. KITTI Optical Flow 2015: Average end-point error
(EPE) and outlier rates (Fl-Noc and Fl-all).

Method

IRR-PWC (Ours)
FlowNet-CSSR [27]
OccAwareFlow [58]
Back2FutureFlow [30]
MirrorFlow [25]

Type

Sintel Training

supervised
supervised

unsupervised
unsupervised

estimated

Clean

0.712
0.703
0.54
0.49
0.390

Final

0.669
0.654
0.48
0.44
–

Table 7. Occlusion estimation results on Sintel Training.

vations are that our model (i) converges much faster than the
baseline and (ii) overﬁts to the training split less, demon-
strating much better accuracy on the test set despite slightly
higher error on training split. This highlights the beneﬁt of
our IRR scheme: better generalization even on the training
domain as well as across datasets.

4.5. Occlusion estimation

We ﬁnally evaluate the accuracy of occlusion estimation
on the Sintel training set as no public benchmarks are avail-
able for the task. Table 7 shows the comparison with state-
of-the-art algorithms. Supervised methods are trained on
FlyingChairs and FlyingThings3D; unsupervised methods
are trained on Sintel without the use of ground truth. We
achieve state-of-the-art accuracy with far fewer parameters
(6.00M instead of 110M) and much simpler training sched-
ules than the previous state of the art [27].

5. Conclusion

We proposed an iterative residual reﬁnement (IRR)
scheme based on weight sharing for generic optical ﬂow
networks, with additional components for bi-directional es-
timation and occlusion estimation. Applying our scheme
on top of two representative ﬂow networks, FlowNet and
PWC-Net, signiﬁcantly improves ﬂow accuracy with a bet-
ter generalization while even reducing the number of pa-
rameters in case of PWC-Net. We also show that our design
choice of jointly estimating occlusion together with ﬂow
brings accuracy improvements on both domains, setting the
state of the art on public benchmark datasets. We believe
that our powerful IRR scheme can be combined with other
baseline networks and can form the basis of other follow-up
approaches, including multi-frame methods.

5761

References

[1] Aria Ahmadi and Ioannis Patras. Unsupervised convolu-
tional neural networks for motion estimation. In ICIP, pages
1629–1633, 2016. 2

[2] Luis ´Alvarez, Rachid Deriche, Th´eodore Papadopoulo, and
Javier S´anchez P´erez. Symmetrical dense optical ﬂow es-
timation with occlusions detection. Int. J. Comput. Vision,
75(3):371–385, 2007. 2, 3, 4

[3] Mathieu Aubry, Daniel Maturana, Alexei A. Efros, Bryan C.
Russell, and Josef Sivic. Seeing 3D chairs: Exemplar part-
based 2D-3D alignment using a large dataset of CAD mod-
els. In CVPR, pages 3762–3769, 2014. 6

[4] Min Bai, Wenjie Luo, Kaustav Kundu, and Raquel Urtasun.
Exploiting semantic information and deep matching for op-
tical ﬂow. In ECCV, volume 6, pages 154–170, 2016. 2

[5] Christian Bailer, Bertram Taetz, and Didier Stricker. Flow
Fields: Dense correspondence ﬁelds for highly accurate large
displacement optical ﬂow estimation. In ICCV, pages 4015–
4023, 2015. 2

[6] Christian Bailer, Kiran Varanasi, and Didier Stricker. CNN-
based patch matching for optical ﬂow with thresholded hinge
embedding loss. In CVPR, pages 2710–2719, 2017. 2, 8

[7] Coloma Ballester, Llu´ıs Garrido, Vanel Lazcano, and Vicent
Caselles. A TV-L1 optical ﬂow method with occlusion de-
tection. In DAGM, pages 31–40, 2012. 2, 4

[8] Michael J. Black and Paul Anandan. The robust estimation
of multiple motions: Parametric and piecewise-smooth ﬂow
ﬁelds. Comput. Vis. Image Und., 63(1):75–104, 1996. 2, 3

[9] Thomas Brox, Andr´es Bruhn, Nils Papenberg, and Joachim
Weickert. High accuracy optical ﬂow estimation based on a
theory for warping. In ECCV, volume 4, pages 25–36, 2004.
2, 3

[10] Daniel J. Butler, Jonas Wulff, Garrett B. Stanley, and
Michael J. Black. A naturalistic open source movie for op-
tical ﬂow evaluation. In ECCV, volume 6, pages 611–625.
2012. 1, 6, 8

[11] Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang
Hua. Coherent online video style transfer. In ICCV, pages
1114–1123, 2017. 1

[12] Qifeng Chen and Vladlen Koltun. Full Flow: Optical ﬂow es-
timation by global optimization over regular grids. In CVPR,
pages 4706–4714, 2016. 2

[13] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-
Hsuan Yang. SegFlow: Joint learning for video object seg-
mentation and optical ﬂow. In ICCV, pages 686–695, 2017.
1

[14] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
H¨ausser, Caner Hazırbas¸, Vladimir Golkov, Patrick van der
Smagt, Daniel Cremers, and Thomas Brox. FlowNet: Learn-
ing optical ﬂow with convolutional networks. In ICCV, pages
2758–2766, 2015. 1, 2, 3, 5, 6, 7, 8

[15] Raghudeep Gadde, Varun Jampani, and Peter V. Gehler.
In

Semantic video CNNs through representation warping.
ICCV, pages 4463–4472, 2017. 1

[16] David Gadot and Lior Wolf. PatchBatch: A batch augmented

loss for optical ﬂow. In CVPR, pages 4236–4245, 2016. 2

[17] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? The KITTI vision benchmark
suite. In CVPR, pages 3354–3361, 2012. 1, 6, 8

[18] Spyros Gidaris and Nikos Komodakis. Detect, replace, re-
ﬁne: Deep structured prediction for pixel wise labeling. In
CVPR, pages 7187–7196, 2017. 3

[19] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Region-based convolutional networks for accurate
object detection and segmentation.
IEEE T. Pattern Anal.
Mach. Intell., 38(1):142–158, Jan. 2016. 1

[20] Fatma G¨uney and Andreas Geiger. Deep discrete ﬂow. In

ACCV, volume 4, pages 207–224, 2016. 2

[21] Adam W. Harley, Konstantinos G. Derpanis, and Iasonas
Kokkinos. Segmentation-aware convolutional networks us-
ing local attention masks. In ICCV, pages 5048–5057, 2017.
4

[22] Yinlin Hu, Yunsong Li, and Rui Song. Robust interpolation
of correspondences for large displacement optical ﬂow. In
CVPR, pages 4791–4799, 2017. 2

[23] Yinlin Hu, Rui Song, and Yunsong Li. Efﬁcient coarse-
In

to-ﬁne PatchMatch for large displacement optical ﬂow.
CVPR, pages 5704–5712, 2016. 2

[24] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Lite-
FlowNet: A lightweight convolutional neural network for
optical ﬂow estimation. In CVPR, pages 8981–8989, 2018.
1, 2, 3, 4, 5, 7, 8

[25] Junhwa Hur and Stefan Roth. MirrorFlow: Exploiting sym-
In

metries in joint optical ﬂow and occlusion estimation.
ICCV, pages 312–321, 2017. 2, 3, 4, 8

[26] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. FlowNet 2.0: Evolu-
tion of optical ﬂow estimation with deep networks. In CVPR,
pages 1647–1655, 2017. 1, 2, 3, 6, 7, 8

[27] Eddy Ilg, Tonmoy Saikia, Margret Keuper, and Thomas
Brox. Occlusions, motion and depth boundaries with a
generic network for disparity, optical ﬂow or scene ﬂow es-
timation. In ECCV, volume 12, pages 626–643. 2018. 2, 3,
6, 7, 8

[28] Serdar Ince and Janusz Konrad. Occlusion-aware optical
ﬂow estimation. IEEE T. Image Process., 17(8):1443–1451,
Aug. 2008. 2, 3, 4

[29] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and
In

Spatial transformer networks.

Koray Kavukcuoglu.
NIPS*2015, pages 2017–2025. 3

[30] Joel Janai, Fatma G¨uney, Anurag Ranjan, Michael J. Black,
and Andreas Geiger. Unsupervised learning of multi-frame
optical ﬂow with occlusions.
In ECCV, volume 16, pages
713–731. 2018. 3, 4, 8

[31] Wei-Sheng Lai, Jia-Bin Huang, and Ming-Hsuan Yang.
Semi-supervised learning for optical ﬂow with generative ad-
versarial networks. In NIPS*2017, pages 354–364. 2

[32] Yu Li, Dongbo Min, Michael S. Brown, Minh N. Do, and
Jiangbo Lu. SPM-BP: Sped-up PatchMatch belief propaga-
tion for continuous MRFs. In ICCV, pages 4006–4014, 2015.
2

[33] Yu Li, Dongbo Min, Minh N. Do, and Jiangbo Lu. Fast
guided global interpolation for depth and motion. In ECCV,
volume 3, pages 717–733, 2016. 2

5762

[34] Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Wei
Chen, Linbo Qiao, Li Zhou, and Jianfeng Zhang. Learning
for disparity estimation through feature constancy. In CVPR,
pages 2811–2820, 2018. 3

[35] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung Mu Lee. Enhanced deep residual networks for single
image super-resolution. In CVPR Workshops, pages 1132–
1140, 2017. 5

[36] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, pages 3431–3440, 2015. 1

[37] Daniel Maurer and Andr´es Bruhn. ProFlow: Learning to

predict optical ﬂow. In BMVC, 2018. 8

[38] Daniel Maurer, Nico Marniok, Bastian Goldluecke, and
Andr´es Bruhn. Structure-from-Motion-Aware PatchMatch
for adaptive optical ﬂow estimation.
In ECCV, volume 8,
pages 575–592. 2018. 8

[39] Nikolaus Mayer, Eddy Ilg, Philip H¨ausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical ﬂow, and scene ﬂow estimation.
In CVPR, pages
4040–4048, 2016. 6

[40] Simon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Un-
supervised learning of optical ﬂow with a bidirectional cen-
sus loss. In AAAI, pages 7251–7259, 2018. 2, 4

[41] Moritz Menze and Andreas Geiger. Object scene ﬂow for
autonomous vehicles. In CVPR, pages 3061–3070, 2015. 1,
8

[42] Michal Neoral, Jan ˇSochman, and Jiˇr´ı Matas. Continual oc-
clusions and optical ﬂow estimation. In ACCV, 2018. 3, 4,
8

[43] David Nilsson and Cristian Sminchisescu. Semantic video
segmentation by gated recurrent ﬂow propagation. In CVPR,
pages 6819–6828, 2018. 1

[44] Jiahao Pang, Wenxiu Sun, Jimmy SJ. Ren, Chengxi Yang,
and Qiong Yan. Cascade residual learning: A two-stage
convolutional neural network for stereo matching. In ICCV
Workshops, pages 878–886, 2017. 3

[45] Anurag Ranjan and Michael J. Black. Optical ﬂow estima-
tion using a spatial pyramid network. In CVPR, pages 2720–
2729, 2017. 1, 2, 3

[46] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang,
Erik B. Sudderth, and Jan Kautz. A fusion approach for
multi-frame optical ﬂow estimation. In WACV, pages 2077–
2086, 2019. 8

[47] Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang,
and Hongyuan Zha. Unsupervised deep learning for optical
ﬂow estimation. In AAAI, pages 1495–1501, 2017. 2

[48] J´erˆome Revaud, Philippe Weinzaepfel, Za¨ıd Harchaoui, and
Cordelia Schmid. EpicFlow: Edge-preserving interpolation
of correspondences for optical ﬂow. In ICCV, pages 1164–
1172, 2015. 2

[49] Ren´e Schuster, Christian Bailer, Oliver Wasenm¨uller, and
Didier Stricker. FlowFields++: Accurate optical ﬂow corre-
spondences meet robust interpolation. In ICIP, pages 1463–
1467, 2018. 8

[50] Deqing Sun, Ce Liu, and Hanspeter Pﬁster. Local layer-
ing for joint motion estimation and occlusion detection. In
CVPR, pages 1098–1105, 2014. 2, 3, 4

[51] Deqing Sun, Stefan Roth, and Michael J. Black. A quan-
titative analysis of current practices in optical ﬂow estima-
tion and the principles behind them. Int. J. Comput. Vision,
106(2):115–137, Jan. 2014. 2, 3

[52] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
PWC-Net: CNNs for optical ﬂow using pyramid, warping,
and cost volume. In CVPR, pages 8934–8943, 2018. 1, 2, 3,
4, 5, 6, 7, 8

[53] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
Models matter, so does training: An empirical study of
CNNs for optical ﬂow estimation.
IEEE T. Pattern Anal.
Mach. Intell., 2019, to appear. 6, 8

[54] Jian Sun, Yin Li, Sing Bing Kang, and Heung-Yeung Shum.
In

Symmetric stereo matching for occlusion handling.
CVPR, pages 399–406, 2005. 3

[55] Jonathan J. Tompson, Arjun Jain, Yann LeCun, and
Christoph Bregler.
Joint training of a convolutional net-
work and a graphical model for human pose estimation. In
NIPS*2014, pages 1799–1807. 1

[56] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko-
laus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas
Brox. DeMoN: Depth and motion network for learning
monocular stereo. In CVPR, pages 5622–5631, 2017. 3

[57] Markus Unger, Manuel Werlberger, Thomas Pock, and Horst
Bischof. Joint motion estimation and segmentation of com-
plex scenes with label costs and occlusion modeling.
In
CVPR, pages 1878–1885, 2012. 2, 3, 4

[58] Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, and Wei
Xu. Occlusion aware unsupervised learning of optical ﬂow.
In CVPR, pages 4884–4893, 2018. 2, 3, 4, 8

[59] Jonas Wulff, Laura Sevilla-Lara, and Michael J. Black. Opti-
cal ﬂow in mostly rigid scenes. In CVPR, pages 6911–6920,
2017. 2, 8

[60] Jiangjian Xiao, Hui Cheng, Harpreet S. Sawhney, Cen Rao,
and Michael A. Isnardi. Bilateral ﬁltering-based optical ﬂow
estimation with occlusion detection.
In ECCV, volume 1,
pages 211–224, 2006. 2, 3, 4

[61] Jia Xu, Ren´e Ranftl, and Vladlen Koltun. Accurate opti-
cal ﬂow via direct cost volume processing. In CVPR, pages
5807–5815, 2017. 2, 8

[62] Yanchao Yang and Stefano Soatto. S2F: Slow-to-fast inter-

polator ﬂow. In CVPR, pages 3767–3776, 2017. 8

[63] Jason J. Yu, Adam W. Harley, and Konstantinos G. Derpa-
nis. Back to basics: Unsupervised learning of optical ﬂow
via brightness constancy and motion smoothness. In ECCV
Workshops, volume 3, pages 3–10, 2016. 2

[64] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen
In CVPR,

Wei. Deep feature ﬂow for video recognition.
pages 4141–4150, 2017. 1

[65] Yi Zhu, Zhenzhong Lan, Shawn Newsam, and Alexander G.
Hauptmann. Guided optical ﬂow learning. In CVPR 2017
Workshops, 2017. 2

[66] Yi Zhu and Shawn D. Newsam. DenseNet for dense ﬂow. In

ICIP, pages 790–794, 2017. 2

[67] Shay Zweig and Lior Wolf. InterpoNet, a brain inspired neu-
ral network for optical ﬂow dense interpolation. In CVPR,
pages 6363–6372, 2017. 2

5763

