4273

4274

4275

4276

Table 2: Architectures used in Kinetics experiments in Table 3(d).

layer

output size

conv1

56 × 56 × 8

res2

56 × 56 × 8

res3

28 × 28 × 8

res4

14 × 14 × 8

res5

7 × 7 × 8

1 × 1 × 1

C2D

baseline
7 × 7, 64,

stride 2, 2(, 1)

CPNet (Ours)
6 CP modules

7 × 7, 64
stride 2, 2

,

,

3 × 3

(cid:20)3 × 3
(cid:20)3 × 3

3 × 3

,

,

(cid:20)3 × 3

3 × 3

,

,

(cid:20)3 × 3

3 × 3

,

,

64

128

64(cid:21) × 2
128(cid:21) × 2 

256(cid:21) × 2 

512(cid:21) × 2 


256

512

(cid:20)3 × 3

3 × 3

64

64(cid:21) × 2

,

,

3 × 3

3 × 3

128

128

,

,

CP module
3 × 3
256

,

3 × 3

,

256

CP module
3 × 3
512

,

3 × 3

,

512

CP module


 × 2

 × 2

 × 2

global average pooling, fc 400

The training and validation results are listed in Table 1.
Our model can overﬁt the toy dataset, while other models
simply generate random guesses and fail in learning the mo-
tion. It’s easy to understand that ARTNet and TRN have
insufﬁcient convolution receptive ﬁelds to cover the step of
the motion of the square. However, it’s intriguing that NL
Net, which should have a global receptive ﬁeld, also fails.

We provide an explanation as follows. Though the toy
NL Net gets by the problem of insufﬁcient convolution re-
ceptive ﬁelds, its NL block fails to include positional in-
formation thus can’t learn long-range motion. However,
it’s not straightforward to directly add pairwise positional
information to NL block without signiﬁcantly increasing
the memory and computation workload to an intractable
amount. Through this experiment, we show another ad-
vantage of our CPNet: by only focusing on top k potential
correspondences, memory and computation can be saved
signiﬁcantly thus allow positional information and semantic
feature be learned together with more a complicated method
such as a neural network.

5. Experiment Results

To validate the choice of our architecture for data in the
wild, we ﬁrst did a sequence of ablation studies on Kinet-
ics dataset [16]. Then we re-implemented several recently
published and relevant architectures with the same dataset
and experiment settings to produce results as good as we
can and compare with our results. Next, we experiment
with very large models and compare with the state-of-the-
art methods on Kinetics validation set. Finally, we did ex-
periments on action-centric datasets Something-something
v2 [10] and Jester v1 [28] and report our results on both val-
idation and testing sets. Visualizations are also provided to
help the understanding of our architecture.

5.1. Ablation Studies

Kinetics [16] is one of the largest well-labelled datasets
for human action recognition from videos in the wild. Its

It con-
classiﬁcation task involves 400 action classes.
tains around 246,000 training videos and 20,000 validation
videos. We used C2D ResNet-18 as backbone for all ab-
lation experiments. The architectures we used are derived
from the last column of Table 2. We included C2D baseline
for comparison. We downsampled the video frames to be
only 1/12 of the original frame rate and used only 8 frames
for each clip. This ensures that the clip duration are long
enough to cover a complete action while still maintain fast
iteration of experiment. The single-clip single-center-crop
validation results are shown in Table 3(a)(b)(c).

Ablation on the Number of CP modules. We explored
the effect of the number of CP modules on the accuracy.
We experimented with adding one or two CP modules to
the res4 group, two CP modules to each of res3 and res4
groups, and two CP modules to each of res3, res4 and res5
groups. The results are shown in Table 3(a). As the number
of CP modules increases, the accuracy gain is consistent.

Ablation on k. We explored the the combination of
training-testing time k values and compared the results in
Table 3(b). When ks are the same during training and test-
ing, highest validation accuracy are achieved. It suggests
that using different k forces the architecture to learn dif-
ferent distribution and highest accuracy are achieved only
when training and test distribution are similar.

We also notice that the highest accuracy are achieved at a
sweet point when both k = 8. An explanation is that when
k is too small, CP module can’t get enough correspondence
candidates to select from; when k is too large, clearly unre-
lated elements are also included and introduce noise.

Ablation on the position of CP modules. We explored
effect of the position of CP modules. We added two CP
modules to three different groups: res3, res4 and res5, re-
spectively. The results are shown in Table 3(c). The high-
est accuracy are achieved when adding two CP modules to
res4 group. A possible explanation is that res3 doesn’t con-
tain enough semantic information for ﬁnding correct k-NN
while resolution of res5 is too low (7 × 7).

5.2. Comparison with Other Architectures

We compare our architecture with C2D/C3D baselines,
C2D NL Networks [31] and ARTNet [30], on Kinetics. We
did two sets of experiments, with frame rate downsampling
ratio of 12 and 4 respectively. Both experiment sets used
8 frames per clip. The settings enable us to compare the
performance under both low and high frame rates. The ar-
chitecture used in the experiments are illustrated in Table 2.
We experimented with two inference methods: 25-clip 10-
crop with averaged softmax score as in [30] and single-clip
single-center-crop. The results are shown in Table 3(d).

Our architecture outperforms C2D/C3D baselines by a
signiﬁcant margin, which proves the efﬁcacy of CP mod-
ule. It also outperforms NL Net and ARTNet given fewer

4277

Table 3: Kinetics datasets results for ablations and comparison with other prior works. The top-1/top-5 accuracies are shown.

(a) number of CP modules

(b) Ablation on CP module’s k values used in training and testing time.

model
C2D
1 CP
2 CPs
4 CPs
6 CPs

top-1
56.9
60.3
60.4
61.0
61.1

top-5
79.5
82.4
82.4
83.1
83.1

top-1/top-5
accuracy

k = 1

k = 2

k = 4

k = 8

k = 16

k = 32

test

train

k = 1

k = 2

k = 4

k = 8

k = 16

k = 32

59.9/82.3
59.1/81.8
59.0/81.2
53.4/76.3
51.3/75.1
52.6/76.6

59.2/81.6
60.2/82.5
60.2/82.4
56.8/79.5
53.8/77.3
53.8/77.7

56.6/79.4
59.6/81.8
60.5/82.6
59.6/81.9
56.8/79.7
55.5/79.1

52.5/76.1
56.9/80.1
59.0/81.7
60.7/82.8
59.8/82.1
58.2/80.8

49.0/72.6
53.0/77.1
55.3/79.2
59.7/82.1
60.6/82.8
60.0/82.2

44.6/58.5
48.9/73.5
49.2/73.5
57.0/80.3
59.2/81.8
60.4/82.4

(c) CP module positions

(d) Kinetics validation accuracy of architectures in Table 2. Clip length is 8 frames.

model
C2D
res3
res4
res5

top-1
56.9
60.4
60.8
59.2

top-5
79.5
82.4
82.8
81.6

frame rate
val conﬁguration
accuracy

C2D
C3D [26]
NL C2D Net [31]
ARTNet [30]
CPNet (Ours)

1/12 of original frame rate

1/4 of original frame rate

1-clip, 1 crop
top-1
top-5

25-clip, 10 crops
top-1

top-5

1-clip, 1 crop
top-1
top-5

25-clip, 10 crops
top-1

top-5

56.9
58.3
58.6
59.1
61.1

79.5
80.7
81.3
81.1
83.1

61.3
64.4
63.3
65.1
66.3

83.6
85.8
85.1
86.1
87.1

54.1
55.0
55.3
56.1
57.2

77.4
78.5
78.6
78.7
80.8

60.8
63.3
62.1
64.2
64.9

83.3
85.2
84.2
85.6
86.5

Table 4: Large RGB-only models on Kinetics validation accuracy.
Clip length for NL Net and our CPNet is 32 frames.

model
I3D Inception [2]
Inception-ResNet-v2 [1]
NL C2D ResNet-101 [31]
CPNet C2D ResNet-101 (ours)

params (M)

25.0
50.9
48.2
42.1

top-1
72.1
73.0
75.1
75.3

top-5
90.3
90.9
91.7
92.4

parameters, further showing the superiority of our CPNet.

5.3. Large Models on Kinetics

We train a large model with C2D ResNet-101 as back-
bone. We applied three phases of training where we pro-
gressively increase the number of frames in a clip from 8
to 16 and then to 32. We freeze batch normalization layers
starting the second phase. During inference, we use 10-clip
in time dimension, 3-crop spatially fully-convolutional in-
ference. The results are illustrated in Table 4.

Compared with large models of several previous RGB-
only architectures, our CPNet achieves higher accuracy
with fewer parameters. We point out that Kinetics is an
appearance-centric dataset where static appearance infor-
mation dominates the classiﬁcation. We will show later
that our CPNet has larger advantage on other action-centric
datasets where dynamic component more important.

thing from left to right”. Thus solely recognizing the object
doesn’t guarantee correct classiﬁcation in this dataset.

We trained two different CPNet models with ResNet-18
and -34 C2D as backbone respectively. We applied two
phases of training where we increase the number of frames
in a clip from 12 to 24. We freeze batch normalization
layers in the second phase. The clip length are kept to
be 2s 1. During inference, we use 6-crop spatially fully-
convolutional inference. We sample 16 clips evenly in tem-
poral dimension from a full-length video and compute the
averaged softmax scores over 6 × 16 clips. The results are
listed in Table 5(a).

Our CPNet model with ResNet-34 backbone achieves
the state-of-the-art results on both validation and testing ac-
curacy. Our model size is less than half but beat Two-stream
TRN [37] by more than 2% in validation accuracy and more
than 1% testing accuracy. Our CPNet model with ResNet-
18 also achieves competing results. With fewer than half
parameters, it beats MultiScale TRN [37] by more than 5%
in validation and more than 2% in testing accuracy. Be-
sides, we also showed the effect of CP modules by com-
paring against respective ResNet C2D baselines. Although
parameter size increase due to CP module is tiny, the vali-
dation accuracy gain is signiﬁcant (>14%).

5.5. Results on Jester

5.4. Results on Something-Something

Something-Something [10] is a recently released dataset
for recognizing human-object interaction from video.
It
has 220,847 videos in 174 categories. This challenging
dataset is action-centric and especially suitable for eval-
uating recognition of motion components in videos. For
example its categories are in the form of ”Pushing some-

Jester [28] is a dataset for recognizing hand gestures
from video. It has 148,092 videos in 27 categories. This
dataset is also action-centric and especially suitable for
evaluating recognizing motion components in video recog-
nition models. One example of its categories is ”Turn-
ing Hand Clockwise”: solely recognizing the static gesture

1There are space for accuracy improvement when using 48 frames.

4278

Table 5: TwentyBN datasets results. Our CPNet outperforms all published results, with fewer number of parameters.

(a) Something-Something v2 Results

model

Goyal et al. [10]
MultiScale TRN [37]
Two-stream TRN [37]
C2D Res18 baseline
C2D Res34 baseline
CPNet Res18, 5 CP (ours)
CPNet Res34, 5 CP (ours)

params

val

test

(M)
22.2
22.8
46.4
10.7
20.3
11.3
21.0

top-1
51.33
48.80
55.52
35.24
39.64
54.08
57.65

top-5
80.46
77.64
83.06
64.49
69.61
82.10
83.95

top-1
50.76
50.85
56.24

-
-

top-5
80.77
79.33
83.15

-
-

53.31
57.57

81.00
84.26

(b) Jester v1 Results

params

model

BesNet [9]
MultiScale TRN [37]
TPRN [33]
MFNet [18]
MFF [17]
C2D Res34 baseline
CPNet Res34, 5 CP (ours)

(M)
37.8
22.8
22.0
41.1
43.4
20.3
21.0

val

-

95.31
95.40
96.68
96.33
84.73
96.70

test

94.23
94.78
95.34
96.22
96.28

-

96.56

doesn’t guarantee correct classiﬁcation in this dataset. We
used the same CPNet with ResNet-34 C2D backbone and
the same training strategy as subsection 5.4. During infer-
ence, we use 6-crop spatially fully-convolutional inference.
We sample 8 clips evenly in temporal dimension from a full-
length video and compute the averaged softmax scores over
6 × 8 clips. The results are listed in Table 5(b).

Our CPNet model outperforms all published results on
both validation and testing accuracy, while having the
smallest parameter size. The effect of CP modules is also
shown by comparing against ResNet-34 C2D baselines.
Again, although parameter size increase due to CP module
is tiny, the validation accuracy gain is signiﬁcant (≈12%).

5.6. Visualization

To understand the behavior of CP module and demystify
why it works, we provide visualization in three aspects with
the datasets used in previous experiments as follows.

What correspondences are proposed? We are inter-
ested to see whether CP module is able to learn to propose
reasonable correspondences purely based on semantic fea-
ture similarity. As illustrated in Figure 5, in general CP
module can ﬁnd majority of reasonable correspondences.
Due to k being a ﬁxed hyperparameter, its k-NN in seman-
tic space may also include wrong correspondences.

Which of proposed correspondences activate output
neurons? We are curious about CP module’s robustness to
wrong proposals. We trace which of the k proposed corre-
spondence pairs affect the value of output neurons after max
pooling. Mathematically, let gi0
be the dimen-
sion c of gi0 and ζ(f i0 , f ij , tij − ti0 , hij − hi0 , wij − wi0 )
from Equation (1) respectively, we are interested in the set

c and ζ

(i0,ij )
c

c

= gi0

Ai0 = {j ∈ {1, . . . , k} | ∃c ∈ {1, . . . , C}, ζ (i0,ij )

c }
(2)
associated with a feature i0, where j not being in Ai0 means
pair (i0, ij) is entirely overwhelmed by other proposed cor-
respondence pairs and thus ﬁltered by max pooling when
calculating output feature i0. We illustrate Ai0 of several
selected features in Figure 5 and show that CP module is
robust to incorrectly proposed correspondences.

How semantic feature map changes? We show in Fig-
ure 5 the heatmap of change in L1 distance of the semantic
feature map for each frame after going through CP module.
We found that CP modules make more changes to features
that correspond to moving pixels. Besides, CP modules on
a later stage focus more on the moving parts with speciﬁc
semantic information that helps ﬁnal classiﬁcation.

6. Discussion

6.1. Relation to Other Single-stream Architectures

Note that since the MLPs in CP modules can potentially
learn to approximate any continuous set functions, CPNet
can be seen as a generalization of several previous RGB-
only architectures for video recognition.

CPNet can be reduced to a C3D [26] with kernel size
u × v × w, if we set the k of CP modules to be uvw − 1,
determine the k nearest neighbors in spatiotemporal space
with L1 distance and let the MLP learn to compute inner
product operation within the u × v × w neighborhood.

CPNet can also be reduced to an NL Net [31], if we set
the k of CP modules to be maximum T HW − 1 and let the
MLP learn to perform the same distance and normalization
functions as the NL block.

CPNet can also be reduced to a TRN [37], if we put one
ﬁnal CP module at the end of C2D, determine the k nearest
neighbors in temporal-only space, and let the MLP learn to
perform the same gθ and hφ functions deﬁned in [37].

6.2. Pixel-level Motion vs. Feature-level Motion

In two-stream architectures, motion in pixel level, i.e.
optical ﬂow ﬁelds, are ﬁrst estimated before sent into deep
networks. In contrast, CP modules captures motion in se-
mantic feature level. We point out that, though CP module
process positional information at a lower spatial resolution
(e.g. 14 × 14), detailed motion feature can still be captured,
since the semantic features already encode rich information
within the receptive ﬁelds [20].

In fact, migrating positional reasoning from the original
input data to semantic representation has contributed to sev-
eral successes in computer vision research. For example, in

4279

4280

References

[1] Y. Bian, C. Gan, X. Liu, F. Li, X. Long, Y. Li, H. Qi, J. Zhou,
S. Wen, and Y. Lin. Revisiting the effectiveness of off-the-
shelf temporal modeling approaches for large-scale video
classiﬁcation. arXiv preprint arXiv:1708.03805, 2017. 6

[2] J. Carreira and A. Zisserman. Quo vadis, action recognition?

a new model and the kinetics dataset. In CVPR, 2017. 2, 6

[3] A. Z. Christoph Feichtenhofer, Axel Pinz. Convolutional
two-stream network fusion for video action recognition. In
CVPR, 2016. 2

[4] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
recognition and description. In CVPR, 2015. 2

[5] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In ICCV, 2015. 2, 8

[6] L. Fan, W. Huang, C. Gan, S. Ermon, B. Gong, and J. Huang.
End-to-end learning of motion representation for video un-
derstanding. In CVPR, 2018. 2

[7] R. Girshick. Fast r-cnn. In ICCV, 2015. 8

[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 8

[9] E. G¨olge. Random Dilation Networks for Action Recog-
nition in Videos. http://www.erogol.com/random-dilation-
networks-action-recognition-videos, 2017. 7

[10] R. Goyal, S. E. Kahou, V. Michalski, J. Materzynska,
S. Westphal, H. Kim, V. Haenel, I. Fr¨und, P. Yianilos,
M. Mueller-Freitag, F. Hoppe, C. Thurau, I. Bax, and
R. Memisevic. The ”something something” video database
for learning and evaluating visual common sense. CoRR,
abs/1706.04261, 2017. 2, 5, 6, 7

[11] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 4

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 4

[13] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation
with deep networks. In CVPR, 2017. 2, 8

[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
CoRR, abs/1502.03167, 2015. 4

[15] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014. 2

[16] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier,
S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev,
M. Suleyman, and A. Zisserman. The kinetics human action
video dataset. CoRR, abs/1705.06950, 2017. 2, 5

[17] O. Kopuklu, N. Kose, and G. Rigoll. Motion fused frames:
Data level fusion strategy for hand gesture recognition. In
CVPR Workshops, 2018. 7

[18] M. Lee, S. Lee, S. Son, G. Park, and N. Kwak. Motion fea-
ture network: Fixed motion ﬁlter for action recognition. In
ECCV, 2018. 7

[19] X. Liu, C. R. Qi, and L. J. Guibas. Learning scene ﬂow in 3d

point clouds. arXiv preprint, 2018. 2, 3, 4

[20] J. L. Long, N. Zhang, and T. Darrell. Do convnets learn

correspondence? In NIPS, 2014. 7

[21] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
In CVPR, 2017. 2, 3

[22] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
NIPS, 2017. 2, 3

[23] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 8

[24] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,
R. Pascanu, P. Battaglia, and T. Lillicrap. A simple neural
network module for relational reasoning. In NIPS, 2017. 2

[25] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In NIPS, 2014. 2
[26] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In ICCV, 2015. 2, 6, 7

[27] D. Tran, J. Ray, Z. Shou, S. Chang, and M. Paluri. Convnet
architecture search for spatiotemporal feature learning. arXiv
preprint, 2017. 2

[28] TwentyBN. The 20BN-jester Dataset V1. https://

20bn.com/datasets/jester. 2, 5, 6

[29] J. ˇZbontar and Y. LeCun. Stereo matching by training a con-
volutional neural network to compare image patches. JMLR,
17(1), 2016. 2

[30] L. Wang, W. Li, W. Li, and L. V. Gool. Appearance-and-
relation networks for video classiﬁcation. In CVPR, 2018. 2,
4, 5, 6

[31] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural

networks. In CVPR, 2018. 2, 4, 5, 6, 7

[32] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and
J. M. Solomon. Dynamic graph cnn for learning on point
clouds. arXiv preprint arXiv:1801.07829, 2018. 2, 3

[33] K. Yang, R. Li, P. Qiao, Q. Wang, D. Li, and Y. Dou. Tempo-
ral pyramid relation network for video-based gesture recog-
nition. In ICIP, 2018. 7

[34] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan,
O. Vinyals, R. Monga, and G. Toderici. Beyond short snip-
pets: Deep networks for video classiﬁcation. In CVPR, 2015.
2

[35] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos,
R. Salakhutdinov, and A. Smola. Deep sets. In NIPS, 2017.
2

[36] J. Zbontar and Y. LeCun. Computing the stereo matching
cost with a convolutional neural network. In CVPR, 2015. 2
[37] B. Zhou, A. Andonian, A. Oliva, and A. Torralba. Temporal

relational reasoning in videos. In ECCV, 2018. 2, 4, 6, 7

[38] M. Zolfaghari, K. Singh, and T. Brox. Eco: Efﬁcient convo-
lutional network for online video understanding. In ECCV,
2018. 2

4281

