Deep Surface Normal Estimation with Hierarchical RGB-D Fusion

Jin Zeng1 Yanfeng Tong1

,

2∗ Yunmu Huang1 ∗ Qiong Yan1 Wenxiu Sun1

Jing Chen2 Yongtian Wang2

1SenseTime Research

2Beijing Institute of Technology

1{zengjin, tongyanfeng, huangyunmu, yanqiong, sunwenxiu}@sensetime.com

2{chen74jing29, wyt}@bit.edu.cn

Abstract

The growing availability of commodity RGB-D cameras
has boosted the applications in the ﬁeld of scene under-
standing. However, as a fundamental scene understand-
ing task, surface normal estimation from RGB-D data lacks
thorough investigation.
In this paper, a hierarchical fu-
sion network with adaptive feature re-weighting is proposed
for surface normal estimation from a single RGB-D image.
Speciﬁcally, the features from color image and depth are
successively integrated at multiple scales to ensure global
surface smoothness while preserving visually salient de-
tails. Meanwhile, the depth features are re-weighted with
a conﬁdence map estimated from depth before merging into
the color branch to avoid artifacts caused by input depth
corruption. Additionally, a hybrid multi-scale loss function
is designed to learn accurate normal estimation given noisy
ground-truth dataset. Extensive experimental results val-
idate the effectiveness of the fusion strategy and the loss
design, outperforming state-of-the-art normal estimation
schemes.

1. Introduction

Per-pixel surface normal estimation has been exten-
sively studied in the recent years. Previous works on nor-
mal estimation mostly assume single RGB image input
[8, 26, 1, 33], providing satisfying results in most cases
despite loss of shape features and erroneous results at the
highlight or dark areas, as shown in Fig. 1(c).

RGB-D cameras are now commercially available, lead-
ing to a great performance enhancement in the applica-
tions of scene understanding, e.g., semantic segmentation
[27, 5, 23], object detection [11, 20], 3D reconstruction
[15, 18, 12], etc. With the depth given by sensors, nor-
mal can be easily calculated via a least square optimiza-
tion [21, 9] as used in the widely used NYUv2 dataset [22],

∗indicates equal contribution.

(a) RGB Image

(b) Sensor Depth

(c) Using RGB

(d) Using Depth

(e) Early Fusion

(f) Hierarchical Fusion

Figure 1. Example in Matterport3D dataset. (a) RGB input; (b)
depth input; normal estimation with (c) single RGB [33], (d) depth
inpainting [9], (e) RGB-D early fusion [32], (f) proposed hierar-
chical RGB-D fusion.

but the quality of the normal suffers from the corruption in
depth, e.g., sensor noise along object edges or missing pix-
els due to glossy, black, transparent, and distant surfaces
[24], as shown in Fig. 1(d).

This motivates us to combine the advantages of color
and depth inputs while compensating for the deﬁciency of
each other in the task of normal estimation. Speciﬁcally,
the RGB information is utilized to ﬁll the missing pixels in
depth; meanwhile the depth clue is merged into RGB re-
sults to enhance sharp edges and correct erroneous estima-
tion, resulting in a complete normal map with ﬁne details.
However, research on combining RGB and depth for nor-
mal estimation has not been extensively studied. To the best
of our knowledge, the only work considering RGB-D input
for normal estimation adopts early fusion, i.e., using depth
as an additional channel to the RGB input, leading to little
performance improvement compared with the methods us-
ing the RGB input only [32]. The lack of proper network
design for combining the geometric information in depth
and color image is an impediment to fully take advantage

6153

of the depth sensor.

Different from previous works on normal estimation
with RGB-D using early fusion [32], we propose to merge
the features from RGB and depth branches at multiple
scales at the decoder side in a hierarchical manner, in order
to guarantee both global surface smoothness and local sharp
features in the fusion results. Additionally, a pixel-wise
conﬁdence map is estimated from the depth input for re-
weighting depth features before merging into RGB branch,
so as to reduce artifact from depth with a smaller conﬁdence
on missing pixels and those along the object edges. An ex-
ample is shown in Fig. 1, where the proposed scheme out-
performs state-of-the-art RGB-based, depth-based, RGBD-
based methods.

Apart from the lack of RGB-D fusion schemes, the short-
age of datasets providing sensor depth and ground-truth
depth pairs is another obstacle for RGB-D normal estima-
tion since the performance of DNN approaches is affected
by the dataset quality [19, 30]. The widely used train-
ing datasets for normal estimation, e.g., NYUv2 [22], do
not provide complete ground-truth normal for the captured
RGB-D images since it is directly computed from the cap-
tured depth after inpainting [14]. If trained on NYUv2, the
network is up to approximate an inpainting algorithm.

Instead we use Matterport3D [2] and ScanNet [6]
datasets with RGB-D captured by camera and ground-truth
normal obtained via multiview reconstruction provided by
[32]. Nevertheless, the ground-truth is not perfect due to the
multiview reconstruction error, especially at object edges
which is crucially for visual evaluation. To overcome the
artifact in the ground-truth, we propose a hybrid multi-scale
loss function based on the noise statistics in the ground-truth
normal map, using L1 loss at the large resolution to obtain
sharper results, and L2 loss at small resolution to ensure
coarse scale accuracy.

In summary, the main contributions of our work are:

• By incorporating RGB and depth inputs via the pro-
posed hierarchical fusion scheme, the two inputs are
able to complement each other in the normal estima-
tion, reﬁne details with depth, and ﬁll the missing
depth pixels with color;

• With the conﬁdence map for depth feature re-
weighting, the effect of artifacts in the depth features
is reduced;

• A hybrid multi-scale loss function is designed by ana-
lyzing the noise statistics in the ground-truth, provid-
ing sharp results with high ﬁdelity despite the imper-
fect ground-truth.

Comparison with the state-of-the-art approaches and exten-
sive ablation study validates the design of network structure

and loss function. The paper is organized as follows. Re-
lated works are discussed in Section 2, and Section 3 pro-
vides a detailed discussion of the proposed method. Ab-
lation study and comparison with state-of-the-art methods
are demonstrated in Section 4 and the work is concluded in
Section 5.

2. Related Work

2.1. Surface Normal Estimation

RGB-based Previous works mostly used a single RGB
image as input. Eigen et al. [8] designed a three-scale con-
volution network architecture that produced a coarse global
prediction with full image ﬁrst and then reﬁned it with lo-
cal ﬁner-scale network. Wang et al. [28] proposed a net-
work structure that integrated different geometric informa-
tion like local, global, and vanishing point information to
predict the surface normal. More recently, Bansal et al.
[1] proposed a skip-connected structure to concatenate the
CNN response at different scales to capture corresponding
details at each scale, and Zhang et al. [33] adopted a U-Net
structure and achieved state-of-the-art performance.

Due to the difﬁculty in extracting geometric information
and texture interference from the RGB input, the details of
predictions are poor, with wrong results in the area of insuf-
ﬁcient lighting or high lighting.

Depth-based Surface normal can be inferred from depth
with geometric method, which depends on the neighbor-
ing pixels’ relative depth geometrically [32]. However,
the depth camera used in common datasets, e.g., NYUv2
[22], Matterport3D [2], ScanNet [6] often fails to sense the
depth on glossy, bright, transparent and faraway surfaces
[32, 29], resulting in holes and corruptions in the obtained
depth images. To overcome missing pixels in normal map
inferred from depth, some works proposed to inpaint depth
images using RGB images [7, 10, 16, 25, 31]. Silberman
et al. [22] used optimization-based method [14] to ﬁll the
holes in depth maps. Zhang et al. [32] used a convolutional
network to predict pixel-wise surface normal with a single
RGB image, then used the predicted normal to ﬁll holes in
raw depth.

Nevertheless, depth inpainting cannot handle large holes
in depth; also, the noise in depth will undermine depth-
based normal estimation performance.

Normal-depth consistency based There is a strong ge-
ometric correlation between the depth and the surface nor-
mal. Normal can be calculated from the depth of neigh-
boring pixels, and depth can be reﬁned with normal varia-
tion. For example, Wang et al. [26] proposed a four-stream
convolutional neural network to detect planar regions, then
used a dense conditional random ﬁeld to smooth results
based on depth and surface normal correlation in planar re-
gion and planar boundary respectively. Chen et al. [3] es-

6154

Figure 2. Proposed hierarchical RGB-D fusion composed of RGB branch at the upper side, depth branch at the lower-right side, conﬁdence
map module at the lower-left side. The fusion module is abstracted as a fusion layer in the fusion network and illustrated at the lower-left
side. An input with size 320 × 240 is used for demonstration.

tablished a new dataset, and proposed two loss functions
to measure the consistency between predicted normal and
depth label for depth and normal prediction. Qi et al. [21]
proposed to predict initial depth and surface normal using
color image, then cross-reﬁne each other using geometric
consistency.

These methods provide different schemes to promote ge-
ometric consistency between normal and depth, but rely on
a single RGB input and do not consider noise from depth
sensors.

RGB-D based The RGB-D based normal estimation has
not been extensively studied in previous works. Normal es-
timation with RGB-D input has been brieﬂy discussed in
[32] where an early fusion was adopted, reported to be al-
most the same as using RGB input. However the method is
not properly designed and the conclusion is not comprehen-
sive. Although 3D reconstruction based methods like [18]
can be used in normal estimation, a series of RGB-D images
is required for those methods, which is beyond the scope of
this paper. The lack of design in RGB-D fusion for surface
normal estimation motivates our work.

2.2. RGB D Fusion Schemes

Despite the lack of study in RGB-D based normal esti-
mation, RGB-D fusion scheme has been explored for other

tasks, among which semantic segmentation is the most ex-
tensively studied one, e.g., early fusion using RGB-D as a
four-channel input [8], late fusion [4], depth-aware convo-
lution [27], or using 3D point cloud format [20].

The difference from those works is that they do not re-
quire per-pixel accuracy as much as normal prediction, i.e.,
the label interior of one object is constant, but for normal
estimation, correct prediction at each pixel is required, and
the most signiﬁcant difﬁculty lies in accurate sharp details.
Therefore, we adopt hierarchical fusion with conﬁdence
map re-weighting to enhance edge preservation in the fu-
sion result without bringing artifacts in depth.

3. Method

As illustrated in Fig. 2, the hierarchical RGB-D fusion
network is composed of three modules: RGB branch, depth
branch, and conﬁdence map estimation. In this section, we
introduce the pipeline for the hierarchical fusion of RGB
and depth branches with the fusion module at different
scales, and conﬁdence map estimation used inside the fu-
sion module for depth conditioning, after which the hybrid
loss function design is detailed. A detailed architecture of
the deep network is provided in the supplementary.

6155

3.1. Hierarchical RGB D Fusion

3.1.2 Comparison with Existing RGB-D Fusion

Given color image Ic and sensor depth Id, we are aimed
as estimating surface normal map In by minimizing its dis-
tance from the ground-truth normal I (gt)

n , i.e.,

min

θ

L(I (gt)

n , fθ(Ic, Id)),

(1)

where fθ denotes the fusion network function to generate
normal estimation In parameterized by the parameters θ,
which are end-to-end trained via back propagation. A hi-
erarchical fusion scheme is adopted to merge depth branch
into RGB branch for both overall surface orientation recti-
ﬁcation and visually salient feature enhancement.

3.1.1 Network Design

First, in the RGB branch where the input is the color image
Ic, we adopt a similar network structure as used in [33],
where a fully convolutional network (FCN) [17] is built
with VGG-16 back-bone as illustrated in the RGB branch
in Fig. 2. Speciﬁcally, the encoder is the same as VGG-
16 except that in the last two convolution blocks of the en-
coder, i.e., conv4 and conv5, the channel number is reduced
from 512 to 256 to remove redundant model parameters.
The encoder is accompanied with a symmetric decoder, and
equipped with skip-connections and shared pooling masks
for learning local image features.

Meanwhile, Id is fed into the depth branch to extract
geometric features with a similar network structure as the
RGB branch, except that the last convolution block in the
RGB encoder is removed to give a simpliﬁed model.

The fusion takes place at the decoder side. As shown
in Fig. 2,
the depth features (colored in green) at each
scale in the decoder are passed into the fusion module and
re-weighted with the conﬁdence map (colored in purple)
down-sampled and repeated to the same resolution as the
depth feature. Then the re-weighted depth features are con-
catenated with the color features with the same resolution
and passed through a deconvolution layer to give the fusion
output features (colored in yellow). Consequently, the fu-
sion module (denoted as FM for short) at scale l is given
as,

Schemes

Existing RGB-D fusion schemes mostly adopt single-scale
[32] fused RGB-D at the input, i.e., using depth
fusion.
as an additional channel along with RGB. However, RGB
and depth are from different domains and cannot be prop-
erly handled using the same encoder as a four-channel in-
put. For example, we adopt the same network structure as
in [33], composed of VGG-16 encoder and a symmetric de-
coder with skip-connection, and use a RGB-D four-channel
input instead of a single RGB to generate the normal as
shown in Fig. 7(d). The output normal does not exhibit
global smoothness, especially in area where depth pixels
are missing. This is because a CNN network is incapable
of handling different domains information from RGB and
depth without prior knowledge about depth artifact.

Late fusion with probability map for RGB and depth is
adopted in [4] for segmentation, and here we generalize the
network structure for normal estimation, by replacing the
probability map with a binary mask indicating whether the
depth pixel is available or not, giving the result in Fig. 7(e).
The role of binary mask we use is consistent with that of the
probability map in [4] which indicates how much the source
is trustworthy. Similar to early fusion, the result of late fu-
sion has noticeable artifacts along the depth holes indicating
the fusion is not smooth.

In light of this, single-scale fusion is not efﬁcient for fus-
ing RGB and depth when RGB and depth contain different
noise. RGB is sensitive to lighting conditions while depth
is corrupted at object edges and distant surfaces, indicating
that the output from RGB and depth can be inconsistent. If
depth is integrated into RGB in a single scale, the fusion
is hard to eliminate the difference between two sources and
give a smooth result. This motivates us to merge depth fea-
tures into RGB branch at four different scales in a hierar-
chical manner. In this way, the features from two branches
are successively merged, where the global surface orienta-
tion error would be corrected at small resolution features,
while detail reﬁnement would take place at the ﬁnal scale.
As shown in Fig. 7, the result of the proposed hierarchical
fusion gives smoother result with detail well preserved.

FM(F l

c, F l

d|Cl) = deconv(F l

c ⊕ (F l

d ⊙ Cl)),

3.2. Conﬁdence Map Estimation

(2)

c, F l

where F l
d are the features from RGB and depth
branches at scale l, and Cl is the conﬁdence map for depth
conditioning. ⊙ denotes element-wise multiplication and ⊕
denotes the concatenation operation. The concatenation re-
sult after deconvolution layer gives the fusion output. The
fusion is implemented at four scales, where the last scale
output gives the ﬁnal normal estimation. The conﬁdence
map estimation is addressed later in Section 3.2.

While hierarchical fusion improves normal estimation
over existing fusion schemes, further examination at pixels
around depth holes shows that the transition is not smooth
as shown in Fig. 8(e) where the right side of the table has
erroneous prediction close to depth hold boundary. This in-
dicates that a binary masking is not sufﬁcient for depth con-
ditioning, and a more adaptive re-weighting would be more
favorable. Therefore, a light-weight network for depth con-
ﬁdence map is designed as follows.

6156

(a) RGB Image

(b) Corresponding ground-truth normal

Figure 3. Enlarged patches from input image and ground-truth nor-
mal map in horizontal direction. Upper row: input image, patch in
red rectangle, patch in green rectangle. Bottom row: ground-truth
normal map, patch in red rectangle, patch in green rectangle.

Depth along with a binary mask indicating missing pix-
els in depth are fed into a convolutional network with ﬁve
layers as shown in Fig. 2, where the ﬁrst two layers are with
3×3 kernel size and the following three layers are with 1×1
kernels. In this way, the receptive ﬁeld is small enough to
restrict local adaption to depth variation. Then the con-
ﬁdence map is down-sampled using shared pooling mask
with depth branch and passed into the fusion module to fa-
cilitate fusion operation as described in Eq. 2. By compar-
ing Fig. 8(e) and (f), the conﬁdence map leads to a more
accurate fusion result, correcting the error at the right side
of the table.

To understand the role of the conﬁdence map, we show
the conﬁdence map in Fig. 8(d). The edge pixels are with
the smallest conﬁdence value indicating a high likelihood
of outlier or noise, while the hole area is with a small yet
non-zero value, suggesting that to enable smooth transition,
information in depth holes can be passed into the merge re-
sult as long as RGB features take the dominant role.

3.3. Hybrid Loss

As mentioned in Section 1, we use Matterport3D and
ScanNet datasets for training and testing because RGB-D
data captured by camera and ground-truth normal pairs are
provided. However, the ground-truth normal suffers from
multiview reconstruction errors as shown in Fig. 3(b) where
the normal map is piece-wise constant inside the mesh trian-
gular and the edge does not align with the RGB input. Given
noisy ground-truth like this, improper handling of loss func-
tion during training will lead to deﬁcient performance. The
reason is as follows.

Given the similar inputs in green and red rectangular in
Fig. 3(a), the output would be similar. However, the corre-
sponding ground-truth normal maps are different as shown
Fig. 3(b), thus by minimizing the loss function, the network

(a) mean

(b) median

Figure 4. Mean and median results from normal observations with
the same RGB input.

will learn an expectation of all pairs of input and ground-
truth [13]:

min

θ

E

(Ic,Id,I

(gt)
n )

L(I (gt)

n , fθ(Ic, Id)).

(3)

n , In) = kI (gt)

For L2 loss L2(I (gt)
2, the minimiza-
tion will lead to an arithmetic mean of the observations,
while L1 loss L1(I (gt)
n − In| will lead to
median of the observations.

n , In) = |I (gt)

n − Ink2

To see which loss is more proper for the given dataset,
we sample patches along the edge in Fig. 3 with same hori-
zontal position as patches in the color rectangles, and com-
pute the mean and median normal results of these sampled
patches shown in Fig. 4 where both generate reasonable re-
sults though median result has sharper edges than mean re-
sult, indicating that L1 loss will generate a more visually
appealing result with sharp details.

In this work, we adopt hybrid multi-scale loss function:

L(I (gt)

n , In) = X

wlL2(I (gt)

n (l), In(l))

(4)

l=1,2

+ X

wlL1(I (gt)

n (l), In(l)),

l=3,4

where l = 1, 2, 3, 4 denotes the scales from small to large,
and wl is the weight for loss at different scales and is set
to be [0.2, 0.4, 0.8, 1.0]. L1 loss is used for large scale out-
puts for detail enhancement, while L2 loss is used for coarse
scale outputs for overall accuracy. Using hybrid loss gen-
erates clean and visually better result than L2 loss widely
used for normal estimation [21, 33, 1] as shown in Fig. 7.
The proposed method is named as Hierarchical RGB-D Fu-
sion with Conﬁdence Map, and referred to as HFM-Net for
short.

4. Experiment

4.1. Implementation Details

Dataset We evaluate our approach on two datasets, Mat-
terport3D [2] and ScanNet [6]. For the corresponding
ground-truth normal data, we use the render normal pro-
vided by [32] which was generated with multiview recon-
struction. Matterport3D is divided into 105432 images for

6157

RGB-based

Depth-based

RGBD-based

Skip-Net Zhang’s Colorization

Metrics

[1]

mean
Matter- median
11.25◦
port3D
22.5◦
30◦
mean
median
11.25◦
22.5◦
30◦

Scan-
Net

runtime

26.081
19.089
31.76
57.61
67.60
26.174
20.598
28.78
54.30
67.00
2.501s

[33]

19.346
12.070
52.64
72.12
79.44
23.306
15.95
40.43
63.08
71.88
0.039s

[14]

21.588
12.079
58.07
69.59
75.00
33.071
23.451
34.52
49.47
56.37

DC
[32]

19.126
9.563
61.48
74.08
79.22
30.652
20.762
39.35
55.27
60.03

GeoNet-D

[21]

17.234
8.744
64.89
78.5
83.75
23.289
15.725
46.41
64.04
76.78

GFMM

[10]

16.537
8.028
65.3
79.94
84.16
21.174
13.598
50.78
67.30
77.00

Ours

HFM-Net

13.062
6.090
72.23
84.41
88.31
14.590
7.468
65.65
81.21
86.21
0.085s

0.156+0.9s

0.156+0.058s

0.156+0.041s

0.156+0.041s

Table 1. Performance of surface normal prediction on Matterport3D and ScanNet dataset.

training and 11302 for testing; ScanNet is divided into
59743 for training and 7517 for testing with ﬁle lists pro-
vided in [32]. Since ground-truth normal data in the Mat-
terport3D suffer from reconstruction noise, e.g., in outdoor
scenes or mirror area, we remove the samples in the testing
dataset with large error so as to avoid unreliable evaluation.
After data pruning, 6.47% (782 out of 12084) testing im-
ages are removed, leading to 11302 remaining. Details of
data pruning can be found in the supplementary.

Training Details We use RMSprop optimizer with initial
learning rate set to 1e−3 and decayed at epoch [2, 4, 6, 9, 12]
with decay rate 0.5. The model is trained from scratch with-
out pretrained model for 15 epochs. We ﬁrst use L2 loss for
all scales in the ﬁrst 4 epochs and then change to hybrid
loss deﬁned in Eq. 4 to ensure stable training at the begin-
ning. We implement with PyTorch on NVIDIA GeForce
GTX Titan X GPU.

Evaluation Metrics The normal prediction performance
is evaluated with ﬁve metrics. We compute the per-pixel
angle distance between prediction and ground-truth, then
compute mean and median for valid pixels with given
ground-truth normal. In addition to mean and median, we
also compute the fraction of pixels with angle difference
with ground-truth less than t where t = 11.25◦, 22.5◦, and
30◦ as used in [9].

4.2. Main Results

We compare our proposed HFM-Net with the state-of-
the-art normal estimation methods, which are classiﬁed
into three categories in accordance with Section 2, while
normal-depth consistency based methods are adopted as al-
ternatives for RGB-D fusion thus also put in the RGB-D
category.

RGB-based methods include Skip-Net [1] and Zhang’s
algorithm [33]. Pretrained models on Matterport3D and

ScanNet of Zhang’s are provided in [32], and Skip-Net
is ﬁne-tuned for Matterport3D and ScanNet based on the
pre-trained model on NYUv2 dataset using public available
training code.

Depth-based Depth information is used to compute sur-
face normal in existing works [22, 6, 2] based on geometric
relation between depth and surface normal. Since the in-
put depth is incomplete, we ﬁrst implement depth inpaint-
ing before converting into normal map. Two algorithms are
used to preprocess the input depth images: colorization al-
gorithm in [14] as used in NYUv2 and the state-of-the-art
depth completion (shortened as DC) [32]. After depth in-
painting, we follow the same procedure in [21] to generate
normal from depth.

RGBD-based For the RGB-D fusion methods, we adopt
methods in GFMM [10] and the state-of-the-art GeoNet
[21] to merge depth input into initial RGB-based normal
output for reﬁnement. Speciﬁcally, we choose Zhang’s
method [33] for initial normal estimation from RGB, and
calculate a rough normal from raw depth image at the same
time, then merge the two normal estimations using methods
in GFMM [10] and GeoNet [21] to estimate the ﬁnal surface
normal map.

We test on two datasets respectively with the ﬁve met-
rics as shown in Table 1, where HFM-Net outperforms all
the other schemes in different metrics. In terms of mean
value, HFM-Net outperforms RGB-based methods by at
least 6.284, and 6.064 over depth-inpainting based methods,
and 3.475 over RGBD-based methods. Visual evaluation
results are shown in Fig. 5 and Fig. 6. RGB-based meth-
ods miss details such as the sofa in Fig. 5 with blurry edges.
Depth-based methods have serious errors at the depth hole
regions and noticeable noise. Competing RGB-D fusion
methods fail to generate accurate results at areas where
depth is noisy or corrupted. On the contrary, our HFM-Net

6158

(a) RGB Image

(b) Depth Image

(c) Ground-truth

(d) Skip-Net [1]

(e) Zhang’s [33]

(f) Colorization [14]

(g) DC [32]

(h) GeoNet-D [21]

(i) GFMM [16]

(j) HFM-Net

Figure 5. Surface normal estimation with different algorithms, test on Matterport3D dataset.

(a) RGB Image

(b) Depth Image

(c) Ground-truth

(d) Skip-Net [1]

(e) Zhang’s [33]

(f) Colorization [14]

(g) DC [32]

(h) GeoNet-D [21]

(i) GFMM [16]

(j) HFM-Net

Figure 6. Surface normal estimation with different algorithms, test on ScanNet dataset.

is exhibiting nice normal prediction both at smooth planar
areas and along sharp edges.

4.3. Ablation Study

For better understanding of how HFM-Net works, we in-
vestigate the effect of each component in the network with
the following ablation study.

Hierarchical Fusion We compare hierarchical fusion
(HF) with single-scale fusion including early fusion and
late fusion as described Section 3, denoted as Early-F and
Late-F in Table 2 respectively. The binary mask is used for
Late-F and HF, and all are trained using hybrid loss if not
speciﬁed. As can be seen from Table 2, Early-F and Late-F
is less effective than HF+Mask+Hybrid, validating the use
of HF. Furthermore, Fig. 7(d-f) show the difference between
single-scale and hierarchical fusion. The hierarchical fusion
provides more accurate results in a planar surface especially
in depth hole areas marked in black rectangles.

Conﬁdence Map We compare conﬁdence map with bi-
nary mask. Fig. 8 shows the difference between fusion with

conﬁdence map and fusion with binary mask. Fusion with
conﬁdence map can reduce the negative effect of a depth
hole during the fusion, and smooth the prediction around
the boundary region of depth holes.

Hybrid Loss Apart from fusion method, different com-
binations of loss function are examined in the experiment.
In comparison of hybrid loss, the conﬁdence map is used
in fusion. If the network use L2 loss function in all layers,
the prediction will tend to be blurry. On the other hand, a
network with L1 loss will tend to preserve more details. A
hybrid loss function design, as described in Section 3.3 can
generate results with both smooth surface and ﬁne object
details, as shown in the comparison in Fig. 7 (g-l).

4.4. Model Complexity and Runtime

Table 1 reports the runtime of our method and other
state-of-the-art methods. Skip-Net method uses the ofﬁcial
evaluation code in MatCaffe. Colorization method uses the
code provided in NYUv2 dataset. GeoNet-D is the GeoNet
with RGBD input, and we implement it in PyTorch. The

6159

(a) RGB Image

(b) Sensor Depth

(c) Ground-truth

(a) RGB Image

(b) Sensor Depth

(c) Ground-truth

(d) Early Fusion

(e) Late Fusion

(f) Hierarchical Fusion

(d) Conﬁdence-map

(e) HF (Mask)

(f) HF (Map)

(g) L2 Loss

(h) L1 Loss

(i) Hybrid loss

(j) L2 Loss Detail

(k) L1 Loss Detail

(l) Hybrid Loss Detail

Figure 7. Surface normal estimation with different fusion schemes
and different loss functions: (a) RGB input, (b) depth input, (c)
ground truth, result of (d) early fusion, (e) late fusion, and (f) hi-
erarchical fusion; result of using (g) L2 loss, (h) L1 loss, (i) pro-
posed hybrid loss; (j-l) are the enlarged patches from (g-i). The
hierarchical fusion produces a more accurate prediction in the area
marked in black rectangles. The hybrid loss design preserves the
advantages of both L2 (smooth surface) and L1 loss (local details),
with sharper details and more accurate results in depth holes.

Metrics

Early-F

Late-F

+L2

HF+Map

HF+Mask
+Hybrid

HF+Map
+Hybrid

Matter-
port3D

Scan-
Net

mean
median
11.25◦
22.5◦
30◦
mean
median
11.25◦
22.5◦
30◦

13.968
6.855
71.93
83.54
87.44
16.045
8.949
61.17
79.32
84.87

13.645
6.567
70.79
83.68
87.75
17.425
10.277
56.01
76.93
83.26

13.688
7.235
69.21
83.45
87.94
14.946
8.322
62.87
80.12
85.72

13.437
6.507
70.98
83.96
88.05
14.696
7.545
65.42
81.10
86.11

13.062
6.090
72.23
84.41
88.31
14.590
7.468
65.65
81.21
86.21

Table 2. Evaluation of variants of the proposed HFM-Net on Mat-
terport3D and ScanNet datasets.

consistency loss is added to GeoNet-D as a comparison
scheme. The network forward runtime is averaged over
Matterport3D test set with input images of size 320×256
on NVIDIA GeForce GTX TITAN X GPU. Apart from the
time cost in neural network forward pass, the runtime of

Figure 8. Surface normal estimation with different map/mask:
(a) RGB input, (b) depth input, (c) ground truth, (d) conﬁdence
map, (e) hierarchical-fusion with mask, (f) hierarchical-fusion
with map.

depth-based and RGBD-based methods also includes the
time spent on geometric calculation. As in shown in Table
1, our method exceeds competing schemes in metric perfor-
mance while taking a reasonably fast time.

5. Conclusion

In this work, we propose a hierarchical fusion scheme
to combine RGB-D features at multiple scales with a con-
ﬁdence map estimated from depth input for depth condi-
tioning to facilitate feature fusion. Moreover, a hybrid loss
function is designed to generate clean normal estimation
even if the training targets suffer from reconstruction noise.
Extensive experimental results demonstrate that our HFM-
Net outperforms the state-of-the-art methods in providing
more accurate surface normal prediction and sharper visu-
ally salient features. Ablation studies validate the superior-
ity of the proposed hierarchical fusion scheme over single-
scale fusion schemes in existing works, the effectiveness
of conﬁdence map in producing accurate estimation around
missing pixels in depth input, and the advantage of the hy-
brid loss function in overcoming dataset deﬁciency.

References

[1] A. Bansal, B. Russell, and A. Gupta. Marr revisited: 2d-
3d alignment via surface normal prediction.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 5965–5974, 2016. 1, 2, 5, 6, 7

[2] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner,
M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3D:
Learning from RGB-D data in indoor environments. Inter-
national Conference on 3D Vision (3DV), 2017. 2, 5, 6

[3] W. Chen, D. Xiang, and J. Deng. Surface normals in the wild.
In Proceedings of the 2017 IEEE International Conference
on Computer Vision, Venice, Italy, pages 22–29, 2017. 2

6160

[4] Y. Cheng, R. Cai, Z. Li, X. Zhao, and K. Huang. Local-
ity sensitive deconvolution networks with gated fusion for
rgb-d indoor semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), volume 3, 2017. 3, 4

[5] H. Chu, W.-C. M. K. Kundu, R. Urtasun, and S. Fidler. Sur-
fconv: Bridging 3d and 2d convolution for rgbd images. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 3002–3011, 2018.
1

[6] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Nießner. ScanNet: Richly-annotated 3d reconstruc-
tions of indoor scenes. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2017. 2, 5, 6

[7] H. C. Daniel, J. Kannala, L. Ladick, and J. Heikkil. Depth
Map Inpainting under a Second-Order Smoothness Prior.
Springer Berlin Heidelberg, 2013. 2

[8] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
2650–2658, 2015. 1, 2, 3

[9] D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3d prim-
itives for single image understanding. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 3392–3399, 2013. 1, 6

[10] X. Gong, J. Liu, W. Zhou, and J. Liu. Guided depth en-
Image & Vision

hancement via a fast marching method .
Computing, 31(10):695–703, 2013. 2, 6

[11] S. Gupta, R. Girshick, P. Arbel´aez, and J. Malik. Learn-
ing rich features from rgb-d images for object detection and
segmentation. In European Conference on Computer Vision
(ECCV), pages 345–360. Springer, 2014. 1

[12] S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. Newcombe,
P. Kohli, J. Shotton, S. Hodges, D. Freeman, A. Davison,
et al. Kinectfusion: real-time 3d reconstruction and inter-
action using a moving depth camera. In Proceedings of the
24th annual ACM symposium on User interface software and
technology, pages 559–568. ACM, 2011. 1

[13] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Kar-
ras, M. Aittala, and T. Aila. Noise2Noise: Learning image
restoration without clean data.
In Proceedings of the 35th
International Conference on Machine Learning, volume 80,
pages 2965–2974, 2018. 5

[14] A. Levin, D. Lischinski, and Y. Weiss. Colorization using
optimization. In ACM transactions on graphics (TOG), vol-
ume 23, pages 689–694. ACM, 2004. 2, 6, 7

[15] O. Litany, A. Bronstein, M. Bronstein, and A. Makadia. De-
formable shape completion with graph convolutional autoen-
coders. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 1886–1895,
2018. 1

[16] J. Liu, X. Gong, and J. Liu. Guided inpainting and ﬁlter-
ing for kinect depth maps. In International Conference on
Pattern Recognition, pages 2055–2058, 2012. 2, 7

[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 3431–3440, 2015. 4

[18] R. A. Newcombe, D. Fox, and S. M. Seitz. Dynamicfusion:
Reconstruction and tracking of non-rigid scenes in real-time.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 343–352, 2015. 1, 3

[19] J. Pang, W. Sun, C. Yang, J. Ren, R. Xiao, J. Zeng, and
L. Lin. Zoom and learn: Generalizing deep stereo match-
ing to novel domains. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2018. 2

[20] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas. Frustum
pointnets for 3d object detection from rgb-d data.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2018. 1, 3

[21] X. Qi, R. Liao, Z. Liu, R. Urtasun, and J. Jia. Geonet: Ge-
ometric neural network for joint depth and surface normal
estimation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 283–291, 2018.
1, 3, 5, 6, 7

[22] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus.

Indoor
segmentation and support inference from rgbd images.
In
European Conference on Computer Vision, pages 746–760.
Springer, 2012. 1, 2, 6

[23] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H.
Yang, and J. Kautz. Splatnet: Sparse lattice networks for
point cloud processing. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 2530–2539, 2018. 1

[24] S. Su, F. Heide, G. Wetzstein, and W. Heidrich. Deep end-
to-end time-of-ﬂight imaging. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 6383–6392, 2018. 1

[25] A. K. Thabet, J. Lahoud, D. Asmar, and B. Ghanem. 3d
aware correction and completion of depth maps in piecewise
planar scenes.
In Asian Conference on Computer Vision,
pages 226–241, 2014. 2

[26] P. Wang, X. Shen, B. Russell, S. Cohen, B. Price, and A. L.
Yuille. Surge: Surface regularized geometry estimation from
a single image. In Advances in Neural Information Process-
ing Systems, pages 172–180, 2016. 1, 2

[27] W. Wang and U. Neumann. Depth-aware cnn for rgb-d seg-
In European Conference on Computer Vision

mentation.
(ECCV). Springer, 2018. 1, 3

[28] X. Wang, D. Fouhey, and A. Gupta. Designing deep net-
works for surface normal estimation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 539–547, 2015. 2

[29] J. Zeng, G. Cheung, M. Ng, J. Pang, and C. Yang. 3d
point cloud denoising using graph laplacian regularization
of a low dimensional manifold model.
arXiv preprint
arXiv:1803.07252, 2018. 2

[30] J. Zeng, J. Pang, W. Sun, G. Cheung, and R. Xiao.
arXiv preprint

Deep graph laplacian regularization.
arXiv:1807.11637, 2018. 2

[31] H.-T. Zhang, J. Yu, and Z.-F. Wang. Probability contour
guided depth map inpainting and superresolution using non-
local total generalized variation. Multimedia Tools and Ap-
plications, 77(7):9003–9020, 2018. 2

6161

[32] Y. Zhang and T. Funkhouser. Deep depth completion of a
single rgb-d image. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
175–185, 2018. 1, 2, 3, 4, 5, 6, 7

[33] Y. Zhang, S. Song, E. Yumer, M. Savva, J.-Y. Lee, H. Jin, and
T. Funkhouser. Physically-based rendering for indoor scene
understanding using convolutional neural networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 5057–5065. IEEE, 2017.
1, 2, 4, 5, 6, 7

6162

