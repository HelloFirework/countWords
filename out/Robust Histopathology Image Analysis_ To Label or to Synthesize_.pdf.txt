Robust Histopathology Image Analysis: to Label or to Synthesize?

Le Hou1, Ayush Agarwal1

,

2, Dimitris Samaras1, Tahsin M. Kurc1, Rajarsi R. Gupta1, Joel H. Saltz1

1Stony Brook University

2Stanford University, California

{lehhou,samaras}@cs.stonybrook.edu

ayush94582@gmail.com

{tahsin.kurc,joel.saltz}@stonybrook.edu

rajarsi.gupta@stonybrookmedicine.edu

Abstract

Detection, segmentation and classiﬁcation of nuclei are
fundamental analysis operations in digital pathology. Ex-
isting state-of-the-art approaches demand extensive amount
of supervised training data from pathologists and may still
perform poorly in images from unseen tissue types. We pro-
pose an unsupervised approach for histopathology image
segmentation that synthesizes heterogeneous sets of training
image patches, of every tissue type. Although our synthetic
patches are not always of high quality, we harness the mot-
ley crew of generated samples through a generally applica-
ble importance sampling method. This proposed approach,
for the ﬁrst time, re-weighs the training loss over synthetic
data so that the ideal (unbiased) generalization loss over
the true data distribution is minimized. This enables us
to use a random polygon generator to synthesize approxi-
mate cellular structures (i.e., nuclear masks) for which no
real examples are given in many tissue types, and hence,
GAN-based methods are not suited.
In addition, we pro-
pose a hybrid synthesis pipeline that utilizes textures in real
histopathology patches and GAN models, to tackle hetero-
geneity in tissue textures. Compared with existing state-of-
the-art supervised models, our approach generalizes signif-
icantly better on cancer types without training data. Even
in cancer types with training data, our approach achieves
the same performance without supervision cost. We release
code and segmentation results1 on over 5000 Whole Slide
Images (WSI) in The Cancer Genome Atlas (TCGA) repos-
itory, a dataset that would be orders of magnitude larger
than what is available today.

1. Introduction

Existing state-of-the-art supervised image analysis meth-
ods [11, 22, 13, 48, 3, 62, 59, 61, 9, 66, 64, 24, 40] largely
rely on the availability of large annotated training datasets
which requires the involvement of domain experts. This
is a time-consuming and expensive process. Moreover, for

1www3.cs.stonybrook.edu/˜cvl/nuclei_seg.html

(a)

Hundreds of hours to 

label a few types

(b)

Unlabeled patches sampled from 
every whole slide image in TCGA

...

Synthesize both texture and ground truth 
structures in an unsupervised manner

Resulting model fails on 
some unseen tissue types

...

Model Generalizes well on every tissue type

Figure 1. (a). Standard learning methods learn and perform well
only with tissue types for which ground truth training data ex-
ists. (b). We propose to synthesize both image texture and ground
truth structures for training a supervised model, even when no real
ground truth structures are given. As a result, our model general-
izes well on unseen tissue types.

methods that generalize on various input types, supervised
data must be collected for every input type. For example,
labeled satellite images from regions such as north Europe
and south Africa are all needed to train a robust satellite
image analysis method [65, 49]. In pathology image anal-
ysis, to achieve optimal performance, the data annotation
phase often must be repeated for different tissue types such
as different cancer sites, fat tissue, necrotic regions, blood
vessels, and glands, because of tissue heterogeneity as well
as variations in tissue preparation and image acquisition.
The detection, segmentation, and classiﬁcation of nuclei are
core analysis steps in virtually all pathology imaging stud-
ies [11, 22, 13, 48, 3, 62, 59, 61, 9, 66, 64, 40, 23, 2, 29] and
precision medicine [17, 12]. It is the ﬁrst step in extracting
interpretable features that provide valuable diagnostic and
prognostic cancer indicators [14, 15, 1, 43, 20]. Manual
generation of nucleus segmentation ground truth data takes
a long time. In our experience, a training dataset consisting
of 50 image patches (12M pixels) takes 120-230 hours of an
expert pathologist’s time. This training dataset is extremely
small compared with the volume of data in a large study
(e.g. 10k whole slide images, 50T pixels). This is a major
impediment to robust nucleus segmentation.

8533

Synthesis 
module

(1)

Sample a mask, from a 
predefined distribution

Nuclear mask

Task-specific 
learning module

Robust segmentation

(green contour)

GAN-free 
module

(2)

Real patch of any type

Weight of this sample

(4)

0.353

(5)

Task-
specific 
CNN

Initial synthetic patch

Refined synthetic patch

(3)

GAN

Figure 2. Overview of our pipeline: we use a GAN-free module to synthesize (sample) an initial synthetic pathology image patch with its
nuclear mask. We then reﬁne the initial synthetic patch using a GAN and compute its sample weight. We ﬁnally train a task-speciﬁc (e.g.
segmentation, classiﬁcation, etc.) CNN on this sampled instance. If a sampled ground truth structure does not produce a realistic synthetic
example, the impact of this instance on the training loss is down-weighted.

One approach to address this problem is training data
synthesis [26, 16, 51]. All existing training data synthesis
approaches assume that the distribution of synthetic data is
the same as the distribution of real data. However, this is of-
ten not the case, especially for synthesis of histopathology
images with cellular structure (e.g. nuclear masks), since no
real examples of nuclear masks are given for most cancer
types. We propose an importance sampling based approach
that minimizes the ideal (unbiased) generalization loss over
the distribution of real data, even when given a biased dis-
tribution (of synthetic data). This allows us to enumerate
possible cellar structures for training data synthesis. Our
pipeline (see Fig. 2):

1. Samples a nucleus segmentation mask from a prede-

ﬁned, approximate ground truth generator;

2. Constructs an initial synthetic patch utilizing real tex-

tures (Fig. 3) of the input tissue type;

3. Uses a GAN model to make the initial synthetic patch

more realistic;

4. Computes an importance weight of this synthetic exam-
ple, from the discriminator’s output simply using Bayes’
theorem; and

5. Trains a task-speciﬁc (e.g. segmentation) CNN using the

synthetic patch, mask and importance weight.

In other words, we enumerate possible ground truth struc-
tures during generation of synthetic training patches. If a
resulting patch is not realistic, we decrease its impact in the
training loss. Similarly, if a resulting patch is not only very
realistic, but also rarely synthesized, then we increase its
impact in the training loss.

To summarize, our contributions are: (1) Synthesizing
perfectly realistic training patches with masks is almost im-
possible when we are not given any real examples of nuclear
masks. We propose an importance sampling based method

that reweighs the losses of approximately generated exam-
ples, for training a task-speciﬁc (e.g. nucleus segmenta-
tion) network, minimizing the ideal (unbiased) generaliza-
tion loss over the real data distribution. (2) We show how to
compute importance weights from the outputs of the GAN
discriminator by simply using the Bayes’ theorem, without
any computational overhead. (3) We propose a hybrid syn-
thesis pipeline that utilizes textures in real histopathology
(4) The pro-
patches for synthesis of any tissue patches.
posed method is robust to tissue heterogeneity. When there
are no supervised datasets for a test cancer type, our nu-
cleus segmentation CNN signiﬁcantly outperforms super-
vised methods in across-cancer generalization. Even for the
few tissue types for which supervised data exist, our method
(5) We
matches the performance of supervised methods.
release nucleus segmentation results on over 5000 Whole
Slide Images (WSI) of 13 major cancer types in The Can-
cer Genome Atlas (TCGA) repository. These results are at
least four orders of magnitude larger than currently avail-
able human annotated datasets. We believe that this large-
scale dataset, even though not as accurately annotated, is a
useful feature for future pathology image analysis research.

2. Related Work

Detection and segmentation of nuclei is a fundamen-
tal analytical step in virtually all pathology imaging stud-
ies [11, 22, 13, 48, 3, 62, 59, 61, 9, 66, 64, 40, 23, 2, 29]
and precision medicine [17, 12]. Recent works in im-
age analysis have proposed crowd-sourcing or high-level,
less accurate annotations, such as scribbles, to generate
large training datasets manually [34, 57, 64]. Work by
Zhou et al. [68] segments nuclei inside a tissue image
and redistributes the segmented nuclei inside the image.
The segmentation masks of the redistributed nuclei are

8534

assumed to be the predicted segmentation masks. This
work requires segmentation masks and does not generate
new textures and shapes. Generative Adversarial Networks
(GANs) [44] have been proposed for generation of realistic
images [16, 6, 4, 51, 8, 67, 42, 25, 46, 38]. For example,
an image-to-image translation GAN [26, 16] synthesizes
eye fundus images. However, it requires an accurate su-
pervised segmentation network to segment eye vessels out,
as part of the synthesis pipeline. The S+U learning frame-
work [51] reﬁnes initially synthesized images via a GAN
to increase their realism. This method achieves state-of-
the-art results in eye gaze and hand pose estimation tasks.
Recently, a GAN based approach [37] is able to synthesize
realistic pathology images with nuclear masks. It is limited
to cancer types with ground truth masks, since it requires
real mask examples. GANs are also used to synthesize im-
ages of various styles of the same content. Cycle-GAN
etc. [35, 69] transfers content of images to target styles with-
out training with paired images. The universal style transfer
approach [32, 54] solves this problem by providing a ref-
erence style to the generator network. However, to apply
any of the GAN models for synthesizing image and masks,
examples of both real images and masks are required.

3. Importance Sampling for Loss Estimation

In this section we show how to minimize the ideal
(unbiased) task-speciﬁc (e.g. segmentation, classiﬁcation,
etc.) generalization loss over the distribution of real data,
given an approximate sampling distribution (of synthetic
data). We deﬁne a random variable X representing an im-
age/patch, with its ground truth T , and the probability den-
sity function of real images as p((cid:2)X, T (cid:3)). In practice, X
and T are discrete. The task-speciﬁc generalization loss
LR(θR) with model parameters θR is:

LR(θR) = (cid:2)

X,T

fθR ((cid:2)X, T (cid:3))p((cid:2)X, T (cid:3)),

(1)

where fθ(·) is the loss function such as the conventional seg-
mentation loss [36, 41]. To minimize the generalization loss
deﬁned by Eq. 1, we sample one example (cid:2)X, T (cid:3) from the
distribution deﬁned by p((cid:2)X, T (cid:3)), then minimize the loss
fθ((cid:2)X, T (cid:3)). If there are inﬁnite real samples, the empirical
loss converges exactly to Eq. 1. In this work, we synthe-
size training examples (cid:2)X, T (cid:3). We deﬁne the probability
density function of synthetic images as g((cid:2)X, T (cid:3)). Ideally
p((cid:2)X, T (cid:3)) is equivalent to g((cid:2)X, T (cid:3)). However, for syn-
thesizing unbiased examples and corresponding “ground
truth” nuclear masks, an unbiased modeling of nuclear
masks is needed – existing training image synthesis meth-
ods [51] heavily depend on unbiased ground truth image
structure modeling, such as size of eyeballs, color of iris.
This is almost impossible for histopathology images be-
cause of the paucity of annotated data and the cellular struc-
ture heterogeneity across tissue types.

To estimate the ideal (unbiased) generalization loss with

g((cid:2)X, T (cid:3)), we formulate the task-speciﬁc loss as follows:

LR(θR) = (cid:2)

fθ((cid:2)X, T (cid:3))

X,T

p((cid:2)X, T (cid:3))
g((cid:2)X, T (cid:3))

g((cid:2)X, T (cid:3)).

(2)

Instead of sampling (cid:2)X, T (cid:3) from the real pdf p((cid:2)X, T (cid:3)),
we can now sample (cid:2)X, T (cid:3) from the synthetic pdf
g((cid:2)X, T (cid:3)) and minimize a new loss function f ′((cid:2)X, T (cid:3)) =
fθ((cid:2)X, T (cid:3))p((cid:2)X, T (cid:3))/g((cid:2)X, T (cid:3)). This is the standard im-
portance sampling approach [7]: when sampling from
p((cid:2)X, T (cid:3)) is expensive, we sample from g((cid:2)X, T (cid:3)) then
re-weight each sample by multiplying its loss with weight
p((cid:2)X, T (cid:3))/g((cid:2)X, T (cid:3)). Note that for the resulting general-
ization loss estimation to be unbiased, for all (cid:2)X, T (cid:3) with
p((cid:2)X, T (cid:3)) > 0, it is required that also g((cid:2)X, T (cid:3)) > 0.

Given an image X, the underlying ground truth T is

ﬁxed. Thus, we can simply drop T in PDFs:

p((cid:2)X, T (cid:3))
g((cid:2)X, T (cid:3))

=

p(X)
g(X)

.

(3)

The right hand side of Eq. 3 can be derived from the out-
put of a GAN discriminator. A discriminator trained with
cross-entropy (log-likelihood) loss estimates the probability
that X is sampled from the real distribution instead of the
synthetic distribution: Pr(X ∼ p|X). The discriminator is
trained with real and synthetic examples. Denote a constant
c as the ratio between the numbers of synthetic input sam-
ples and real input samples: c = Pr(X ∼ g)/Pr(X ∼ p).
Thus p(X) = Pr(X|X ∼ p), g(X) = Pr(X|X ∼ g).
Using Bayes’ theorem, we have:

Pr(X ∼ p|X) =

Pr(X|X ∼ p)

Pr(X|X ∼ p) + Pr(X|X ∼ g) c

=

p(X)

p(X) + g(X)c

.

(4)

Rearranging Eq. 4 gives us the importance weight formu-
lated by the discriminator’s output Pr(X ∼ p|X):

p(X)
g(X)

= c ·

Pr(X ∼ p|X)

1 − Pr(X ∼ p|X)

.

(5)

If a synthetic patch is unrealistic (Pr(X ∼ p|X) ≪ 0.5),
it will be down-weighted (contribute less to the loss). If a
synthetic patch is realistic and rarely generated, it will be
up-weighted (contribute more to the loss). We show the
visualization of importance weights in Fig. 7.

Optimality of unbiased loss minimization: Since we
learn Pr(X ∼ p|X) via training the discriminator on the
unbiased dataset (i.e. unlimited samples of X ∼ p and
X ∼ g), we can easily show that this yield unbiased gen-
eralization loss minimization: The unbiased generalization
loss over the distribution of real data deﬁned by Eq. 1 is

8535

equivalent to Eq. 2. Since we can sample from the syn-
thetic data distribution g easily, the only term in Eq. 2
need to learn is the importance weight p(X)/g(X), de-
ﬁned by Eq. 5. Hence, an unbiased discriminator output
Pr(X ∼ p|X) yields unbiased importance weights, and
further, unbiased generalization loss.

Real image 

patch

Color-based super 
segmentation mask

Inpainted nuclei free 
patch (background)

Blurring nuclear 

boundary and modeling 

chromatin clearing

Random 
polygons

Interpolate

Sampled 

segmentation masks

Randomly 
diluted mask

Initial synthetic 

patch

Texture and color 

obtained from 
real patches

Foreground texture

Inside our “GAN-free module”:

Figure 3.
synthesizing a
histopathology image patch utilizing textures in any given tissue
type. This step generates an image patch matches the given mask.

patch to refine
reference type

input fake patch
reference type

another real patch 
from reference type

reference type

Refiner 

(generator) 

CNN

refined patch (fake)

Discrimi-
nator CNN

Probability of 
input patch is 
real in the 
reference type

Figure 4. Inside our “GAN module”:
in addition to the input
real/fake patches, we provide additional “reference type” patches
extracted from nearby regions of the real patches. If the fake patch
is realistic, but does not reﬂect the same tissue type as the refer-
ence type, the discriminator is still able to tell the difference. As a
result, the reﬁner learns to generate patches in the reference style.

4. Heterogeneous Patch Synthesis

We now show how to synthesize (sample) training ex-
amples. Fig. 2 shows the overview of our method which
learns from unlabeled real histopathology images of hetero-
geneous texture and cellular structure (e.g. nuclear mask).

4.1. Initial synthesis

This step generates synthetic patches that are not nec-
essarily realistic for all given target tissue types. Thus, a
signiﬁcant part of this process is predeﬁned regardless of

Initial 
synthetic 
image

Conv.

Convs

… +

Residual

Conv.

Pool.

Tiling

Concat.

Adding texture features 
of the reference image 
in the early stage of 
refinement

Real 

reference 

style 
image

Figure 5. Our reﬁner (generator) CNN adds information of the ref-
erence type patch into the reﬁnement stage, so that the initial syn-
thetic patch will be reﬁned according to the reference type.

the target tissue type. First, we randomly generate a set of
polygons as nuclear masks. In particular, we perturb points
on a circle closer/further away from the center according to
a random irregularity value. These polygons are of variable
sizes and irregularities and are allowed to randomly over-
lap with each other by a predeﬁned number of pixels. To
model the correlation between the shapes of nearby nuclei,
all polygons are distorted by a random quadrilateral trans-
form. The purpose of such a mask is to provide a generic
representation of the basic structures in tissues and to induce
greater variability in the synthetic images. We consider the
generated masks as foreground/background masks (nuclei
as the foreground and tissue as the background) and utilize
textures from real histopathology image patches to generate
initial synthetic image patches in a background/foreground
manner. This is a fast process; synthesizing a 200×200
pixel patch at 40X magniﬁcation takes one second using a
single CPU core.

Generating Background Patches: First, we remove the
nuclei in a source image patch to create a background
patch on which we add the synthetic nuclei. We apply a
simple Ostu’s threshold-based super-segmentation method
[33] on the source image patch to determine the nuclear
material.
In super-segmentation, a segmented region al-
ways fully contains the foreground object (nucleus in this
case). We replace the pixels corresponding to the segmented
nuclear material with color and texture values similar to
the background pixels via image inpainting [55]. Super-
segmentation may not precisely delineate nucleus bound-
aries and may include non-nuclear material in segmented
nuclei. This is acceptable, because the objective of this
step is to guarantee that only background tissue texture and
intensity properties are used to synthesize the background
patch.

Simulating Foreground Nuclear Textures: We ap-
ply a sub-segmentation method to the source patch to
gather nuclear textures from segmented regions.
In sub-
segmentation, a segmented region is fully contained in the
foreground object. This ensures that pixels within real nu-
clei are used for generating realistic foreground (nuclei) in
synthetic images. Since nuclei are generally small and make

8536

up a small portion of tissue, sub-segmentation will yield a
very limited amount of nuclear material which is not enough
for existing reconstruction methods. Thus, our approach
utilizes textures in the Eosin channel [19] of a randomly
extracted real patch and combines them with nuclear color
obtained via sub-segmentation of the source patch to gener-
ate nuclear textures.

Combining Foreground and Background: Let us de-
ﬁne Ii,j , Ai,j , Bi,j , Mi,j as pixel values at position i, j
in the resulting synthetic patch, the nuclear texture patch,
the nucleus free patch, and the nucleus mask patch, re-
spectively. To combine nuclear and non-nuclear textures
according to the nucleus mask patch, Ii,j can be set to
Ai,jMi,j + Bi,j(1 − Mi,j). This may result in signiﬁcant
artifacts, such as obvious nuclear boundaries. Additionally,
clear chromatin phenomena in certain types of nuclei are
not modeled. Thus, our method randomly clears the inte-
rior of the polygons in the nucleus mask patch and blurs
their boundaries before applying the above equation.

4.2. Reﬁning the Initial Synthesis

These initial synthetic image patches are reﬁned via ad-
versarial training. We also use the discriminator’s output to
compute the importance sampling weight deﬁned by Eq. 5.
For this phase we have implemented a reﬁner (generator)
CNN and a discriminator CNN.

Given an input image patch I and a reference type patch
S, the reﬁner G with trainable parameters θG outputs a re-
ﬁned patch X = G(I, S; θG). Ideally, an output patch is
(1). Regularized: The pixel-wise difference between the
initial synthetic patch and the reﬁned patch is small enough
so that the synthetic “ground truth” remains unchanged. (2).
Realistic for the given type: It is a realistic representation of
the type of the reference patch. (3). Informative and hard:
It provides a challenging example for the task-speciﬁc CNN
so that the trained task-speciﬁc CNN will be robust.

G , Lreal

We construct three losses: Lreg

for
each of the properties above, respectively. The ﬁrst two
losses, Lreg
G , are based on the S+U method [51].
The weighted average of these losses is deﬁned as the ﬁnal
loss LG for training the reﬁner CNN:

G , and Lhard

G and Lreal

G

LG = αLreg

G + βLreal

G + γLhard
G .

(6)

We set hyperparameters α = 1.0, β = 1.0, γ = 0.0000001
in experiments.

The regularization loss Lreg

G is deﬁned as an elastic
net [70]: Lreg
G (θG) = E(cid:3)λ1||I − X||1 + λ2||I − X||2(cid:4),
where E[·] is the expectation function applied on the train-
ing set, || · ||1 and || · ||2 are the L-1 and L-2 norms and λ1
and λ2 are predeﬁned parameters. We use λ1 = 0.00001
and λ2 = 0.0001 in experiments.

The loss for achieving a realistic representation in the
reference type, by training the reﬁner (generator) G, is

G (θG) = E(cid:3)log(cid:5)1−D(X, S; θD)(cid:6)(cid:4), where D(X, S; θD)
Lreal
is the output of the discriminator D with trainable parame-
ters θD given the reﬁned patch X and the same reference
type patch S as input. It is the estimated probability by D
that input X matches the tissue type of S. The discrimina-
tor D has two classes of input: pairs of real patches within
the same type (cid:2)S ′, S(cid:3) and a pair with one synthetic patch
(cid:2)X, S(cid:3). Its loss is the standard classiﬁcation loss LD(θD) =
−E(cid:3) log (cid:5)D(S ′, S; θD)(cid:6)(cid:4) − E(cid:3) log (cid:5)1 − D(X, S; θD)(cid:6)(cid:4).

The generator and discriminator both take a reference
patch and reﬁne or classify the other input patch according
to textures in the reference patch. This feature is imple-
mented with an asymmetric siamese network [10, 28], as
shown in Fig. 4 and Fig. 5.

G

It has been shown that GANs are able to generate
challenging training examples that yield robust classiﬁca-
tion/segmentation models [30, 50, 31, 21, 60]. Thus, the re-
ﬁner is trained with loss Lhard
G to generate challenging train-
ing examples (with larger loss) for the task-speciﬁc CNN.
We simply deﬁne Lhard
as the negative of the task-speciﬁc
loss: Lhard
G (θG) = −LR(θR), where LR(θR) is the loss of a
task-speciﬁc model R with trainable parameters θR. When
training the reﬁner, we update θG to produce reﬁned patches
that maximize LR. When training the task-speciﬁc CNN,
we update θR to minimize LR. The underlying segmenta-
tion ground truth of the reﬁned patches would change sig-
G (θG) overpowered Lreg
niﬁcantly if Lhard
G (θG). We down-
weight Lhard
by a factor of 0.0001 to minimize the likeli-
hood of this unwanted outcome.

G

4.3. Visual Evaluation by a Human Expert

Fig. 6, 7, 8 show examples of our initial synthetic and
reﬁned patches. To verify that synthetic patches are re-
alistic, we asked a pathologist to distinguish real versus
synthetic patches.
In particular, we showed the patholo-
gist 100 randomly extracted real patches, 100 randomly se-
lected initial synthetic patches, and 100 randomly selected
reﬁned patches. Out of this set, the pathologist selected the
patches he thought were real. The pathologist classiﬁed al-
most half of the initial synthetic patches (46%) and most of
the reﬁned patches (64%) as real. The pathologist classi-
ﬁed (83%) of the real patches as real. This is because many
of those real patches are out-of-focus or contain no nuclei.
Fig. 7 shows the distributions of weights of the realistic syn-
thetic patches versus the unrealistic synthetic patches. This
veriﬁes that the realistic synthetic patches have higher im-
portance sampling weights and vice versa.

5. Experiments

We conducted experiments with datasets from the MIC-
CAI18 and MICCAI17 nucleus segmentation challenges
[39, 58] and the generalized nucleus segmentation dataset

8537

(cid:53)(cid:72)(cid:68)(cid:79)(cid:3)(cid:86)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)(cid:3)(cid:87)(cid:76)(cid:86)(cid:86)(cid:88)(cid:72)(cid:3)

(cid:87)(cid:92)(cid:83)(cid:72)(cid:3)(cid:83)(cid:68)(cid:87)(cid:70)(cid:75)(cid:72)(cid:86)

(cid:51)(cid:82)(cid:86)(cid:86)(cid:76)(cid:69)(cid:79)(cid:72)(cid:3)(cid:74)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:3)
(cid:87)(cid:85)(cid:88)(cid:87)(cid:75)(cid:3)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86)

(cid:53)(cid:72)(cid:68)(cid:79)(cid:3)(cid:85)(cid:72)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:87)(cid:76)(cid:86)(cid:86)(cid:88)(cid:72)(cid:3)

(cid:87)(cid:92)(cid:83)(cid:72)(cid:3)(cid:83)(cid:68)(cid:87)(cid:70)(cid:75)(cid:72)(cid:86)

(cid:51)(cid:68)(cid:87)(cid:70)(cid:75)(cid:72)(cid:86)(cid:3)(cid:87)(cid:82)(cid:3)(cid:85)(cid:72)(cid:73)(cid:76)(cid:81)(cid:72)

(cid:11)(cid:68)(cid:12)(cid:17)(cid:3)(cid:56)(cid:87)(cid:76)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:72)(cid:91)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:89)(cid:68)(cid:85)(cid:76)(cid:82)(cid:88)(cid:86)(cid:3)(cid:87)(cid:76)(cid:86)(cid:86)(cid:88)(cid:72)(cid:3)(cid:87)(cid:92)(cid:83)(cid:72)(cid:86)(cid:3)(cid:71)(cid:88)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)

(cid:76)(cid:81)(cid:76)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:86)(cid:92)(cid:81)(cid:87)(cid:75)(cid:72)(cid:86)(cid:76)(cid:86)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:17)

(cid:11)(cid:69)(cid:12)(cid:17)(cid:3)(cid:53)(cid:72)(cid:73)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:86)(cid:92)(cid:81)(cid:87)(cid:75)(cid:72)(cid:87)(cid:76)(cid:70)(cid:3)(cid:83)(cid:68)(cid:87)(cid:70)(cid:75)(cid:72)(cid:86)(cid:3)(cid:87)(cid:82)(cid:3)(cid:69)(cid:72)(cid:3)

(cid:80)(cid:82)(cid:85)(cid:72)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:85)(cid:72)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:87)(cid:92)(cid:83)(cid:72)(cid:17)

Figure 6. The effect of using different source tissue texture patches and reference type patches. The resulting synthetic patches have the
same textures/types as the source/reference patches.

s
e
i
t
i
l
i
b
a
b
o
r
P

0.8

0.6

0.4

0.2

0

(a). Distribution of importance sampling weights

Patches classified fake by pathologist
Patches classified real by pathologist

0

0.5
1.5
Importance sampling weights

1

2

(cid:11)(cid:69)(cid:12)(cid:17)(cid:3)(cid:40)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:76)(cid:80)(cid:83)(cid:82)(cid:85)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:86)

(cid:39)(cid:82)(cid:3)(cid:81)(cid:82)(cid:87)(cid:3)(cid:80)(cid:68)(cid:87)(cid:70)(cid:75)(cid:3)(cid:85)(cid:72)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:87)(cid:76)(cid:86)(cid:86)(cid:88)(cid:72)(cid:3)(cid:87)(cid:92)(cid:83)(cid:72)(cid:86)(cid:29)

(cid:71)(cid:82)(cid:90)(cid:81)(cid:16)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:72)(cid:71)

(cid:48)(cid:68)(cid:87)(cid:70)(cid:75)(cid:3)(cid:85)(cid:72)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:87)(cid:76)(cid:86)(cid:86)(cid:88)(cid:72)(cid:3)(cid:87)(cid:92)(cid:83)(cid:72)(cid:86)(cid:29)

(cid:88)(cid:83)(cid:16)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:72)(cid:71)

(cid:53)(cid:72)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)
(cid:87)(cid:76)(cid:86)(cid:86)(cid:88)(cid:72)(cid:3)(cid:87)(cid:92)(cid:83)(cid:72)(cid:3)
(cid:83)(cid:68)(cid:87)(cid:70)(cid:75)(cid:72)(cid:86)

(cid:54)(cid:92)(cid:81)(cid:87)(cid:75)(cid:72)(cid:87)(cid:76)(cid:70)(cid:3)
(cid:87)(cid:76)(cid:86)(cid:86)(cid:88)(cid:72)(cid:3)
(cid:83)(cid:68)(cid:87)(cid:70)(cid:75)(cid:72)(cid:86)

(cid:58)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:86)

(cid:19)(cid:17)(cid:20)(cid:24)

(cid:19)(cid:17)(cid:22)(cid:20)

(cid:19)(cid:17)(cid:25)(cid:28)

(cid:20)(cid:17)(cid:20)(cid:27)

(cid:20)(cid:17)(cid:25)(cid:22)

(cid:21)(cid:17)(cid:24)(cid:23)

Figure 7. Evaluation and visualization of importance sampling weights. (a). Synthetic patches classiﬁed as real by pathologists have higher
importance weights than patches classiﬁed as fake. (b). Visualization of importance sampling weights.

[29] containing seven cancer types. Additionally, we evalu-
ated our method with a lymphocyte detection dataset [23].
We implemented the reﬁner, outlined in Fig. 5, with 21
convolutional layers and 2 pooling layers. The discrimina-
tor has 15 convolutional layers and 3 pooling layers. As the
task-speciﬁc CNNs, we used U-net [47] and a network with
15 convolutional layers and 2 pooling layers for nucleus de-
tection and segmentation, and a network with 11 convolu-
tional layers for classiﬁcation. For details, please refer to
our source code. We used an open source implementation
of GAN [27, 51] as part of our implementation. We initial-
ize all networks randomly (no pretraining). During testing,
we normalize the color of input H&E patches [45].

5.1. Nucleus Segmentation Experiments

Supervised methods heavily depend on representative
datasets. However, currently only a few cancer types have
supervised datasets due to the extensive amount of labor and
expert domain knowledge required for histopathology im-
age annotation. For cancer types without labeled data, su-
pervised methods achieve worse performance than on can-
cer types with labeled data. We veriﬁed this argument using
the MICCAI18 and MICCAI16/17 nucleus segmentation
datasets [39, 58]. The MICCAI18 nucleus segmentation

challenge dataset [39] contains 15 training and 18 testing
tissue images extracted from whole slide images of two can-
cer types. The MICCAI17 dataset [58] contains 32 training
and 32 testing images, extracted from whole slide images
of four cancer types. A typical resolution is 600×600 pix-
els. In addition, we tested the across dataset generalization
ability of our method using the test set of the generalized
nucleus segmentation dataset [29]. The test set contains 14
1000×1000 pixel patches in seven cancer types.

Note that annotating one nucleus takes about 2 minutes.
It would take about 225 man-hours to generate these train-
ing datasets. Unsupervised synthetic image generation and
training can result in signiﬁcant time savings in such cases,
while enabling the generation of larger training datasets.

We evaluated several methods in the nucleus segmenta-
tion experiments; these methods are listed below.
In the
following, Universal denotes the proposed method trained
with patches extracted from whole slide images for all can-
cer types in the TCGA repository. More speciﬁcally, we
randomly extracted a 500×500-pixel tissue patch at 40X
(for 20X images, we upsampled the patch to 40X) from each
diagnostic whole slide image in the TCGA repository. This
generated about 10k tissue patches.

Universal U-net. The proposed method with U-net [47]

8538

as the task-speciﬁc CNN. Our U-net has two outputs: one
for nucleus detection, and one for class-level nucleus seg-
mentation. We then combined detection and class-level
segmentation results to achieve instance-level segmenta-
tion using watershed [5, 2].

Universal CNN. The proposed method with a 15 layer
segmentation/detection network.

Universal U-net + real data. Since U-net is computation-
ally efﬁcient, we train a U-net with both synthetic as well
as real data from the MICCAI18 training dataset, as the
model we deploy on over 5000 WSIs.

Type-speciﬁc U-net / CNN. We use the semi-supervised
U-nets [47] and the 15/11 layer CNN as standalone super-
vised networks, trained with real, human annotated tissue
image patches from up to four cancer types. We augment
the real patches by rotation, mirroring, and scaling.

In order to obtain every tissue type for unsupervised
learning of our method, we synthesized 75×75-pixel and
200×200-pixel patches according to patches sampled from
every TCGA WSI. The “GAN-free module” generated 100k
initial synthetic patches. Then we used GAN for image re-
ﬁnement and importance sampling based task-speciﬁc train-
ing on those initial synthetic patches.

We tested the supervised methods with the following
two setups: (1) Within cancer type. We trained the type-
speciﬁc, supervised CNNs with the training sets of all two
MICCAI18 and four MICCAI17 cancer types. (2) Across
cancer types. We excluded the training images of one can-
cer type, trained a type-speciﬁc, supervised CNN with the
training images from all of the other cancer types, and eval-
uated the trained CNN on the images of the excluded type.
We repeated this for all two/four cancer types and report
performance as the average of all runs.

We used the average of two deﬁnitions of DICE coefﬁ-
cients as the performance metric. The ﬁrst version is the
standard DICE coefﬁcient [18, 53]: denote the set of seg-
mented pixels as S and the set of ground truth nuclear pix-
els as T , DICE= 2 ∗ |S ∩ T |/(|S| + |T |). The second is
a variant of the original to capture mismatch in the way the
segmented objects are split, while the overall segmentation
may be very similar. The evaluation results are shown in
Tab. 1. Our approach outperforms the supervised methods
signiﬁcantly on testing cancer types without supervised data
(across cancer types). Even when supervised data exists for
every cancer type (within cancer type), our approach per-
forms as well as the state-of-the-art approaches.

To further verify that our method outperforms baseline
methods on tissue types without supervised data, we eval-
uated nucleus segmentation methods across datasets: we
trained supervised method on the MICCAI17 training set
and tested it on the test set of the generalized nucleus seg-
mentation dataset [29]. As shown in Tab. 2, our method

Nucleus segmentation
methods

MICCAI18 MICCAI17
DICE Avg. DICE Avg.

Supervised methods tested within cancer types

Type-speciﬁc CNN
Type-speciﬁc U-net
Contour-aware net [9]
CSP-CNN [23]
MICCAI18 winner
MICCAI17 winner [58]

0.8013
0.8391
0.812
0.8362
0.870

-

0.7713
0.7645

-

0.7681

-

0.783

Supervised methods tested across cancer types

0.7818
0.8010

Type-speciﬁc CNN
Type-speciﬁc U-net

0.7314
0.7179
Proposed unsupervised method for all cancer types
0.7708
0.7612
0.7863

Universal CNN
Universal U-net
Universal U-net + real data

0.8180
0.8401
0.8678

Table 1. Nucleus segmentation results on the MICCAI18 and
MICCAI17 nucleus segmentation datasets. For each of the three
network architecture, our approach outperforms the supervised
methods signiﬁcantly on cancer types without supervised data
(across cancer). Even when supervised data exists for all cancer
types (within cancer), our approach performs as well as state-of-
the-art approaches without any supervision cost, due to the large
scale of the synthetic dataset. The MICCAI18 winner’s approach
is unknown to us.

generalizes signiﬁcantly better across datasets, than the su-
pervised, type-speciﬁc method. Thus, we release segmen-
tation results on 5000 WSIs in the TCGA repository [56].
Existing largest human annotated dataset [29] contains 100
patches of size 1000×1000 pixels. The scale of our seg-
mentation results are larger than 10M such patches. We
believe that this large-scale dataset, even though not as ac-
curately annotated, is a useful feature for future pathology
image analysis research.

Nucleus segmentation methods
Type-speciﬁc U-net, across dataset
Universal U-net + real data

DICE Avg.

0.7328
0.7713

Table 2. Across dataset evaluation results. The type-speciﬁc CNN
is trained on the MICCAI17 training set amd evaluated on the test
set of the generalized nucleus segmentation dataset [29]. Our un-
supervised method generalizes signiﬁcantly better, than the super-
vised type-speciﬁc method.

5.2. Ablation Studies

We evaluated the importance of three components of our
method: importance weights in the loss function, utilizing
a real reference type patch for reﬁnement, and generating
hard examples for CNN training. We removed one feature at
a time and measured performance for nucleus segmentation
on the MICCAI17 dataset. The experimental results using
U-net are shown in Tab. 3. The proposed methods reduce
the segmentation error by 5.4%, 7.8%, and 3.2%.

8539

+

+

+

+

+

+

+

+

(a). Synthetic examples for nucleus segmentation

(b). Synthetic examples for lymphocyte (indicated by +) detection

Figure 8. Examples of various kinds of synthetic patches we generated.

DICE Avg.

5.4. Lymphocyte Detection Experiments

Nucleus segmentation methods
No hard examples
No reference patch during reﬁnement
No importance weights
Universal CNN (proposed)

0.7476
0.7410
0.7533
0.7612

Table 3. Ablation study using the MICCAI17 nucleus segmenta-
tion challenge dataset. Proposed methods reduce the segmentation
error (1−DICE average) by 5.4%, 7.8%, and 3.2%.

Lymphocyte detection methods
Level Set features + supervised net [67]
Fine-tuning VGG16 (supervised) [52]
Universal CNN (proposed)

AUROC
0.7132
0.6925
0.7149

Table 4. Lymphocyte detection on the lymphocyte dataset [23].
Without any supervision cost, our method outperforms all super-
vised models trained on patches of just one cancer type.

(a)

(b)

(c)

Figure 9. Three failure cases: Dark pigment in melanoma (a)
and out-of-focus (b) scenarios are not modeled by our synthesis
pipeline. Some light-colored nuclei with clear chromatin (c) are
not detected when they are close to dark, easy-to-detect nuclei.

5.3. Human evaluation on 13 cancer types in TCGA

To evaluate nucleus segmentation methods in an uncon-
trolled environment, we randomly extracted 133 500×500
pixel patches from 13 major cancer types (that have more
than 500 WSIs each) in TCGA [56], applied segmentation
methods on those patches, and blindly compared the seg-
mentation quality between our method and the baseline. For
segmentation methods, we use the fully supervised U-net
(type-speciﬁc U-net) trained on the MICCAI18 training set
as the baseline, and the U-net trained on both synthetic and
real MICCAI18 training data (Universal U-net + real data)
as our method. For human evaluation, an expert patholo-
gist blindly compared the segmentation results in terms of
T rueP ositives − F alseP ositives − F alseN egatives in
each patch. As a result, out of the 133 patches, in 83 patches
our method is better than the baseline, in 46 patches our
method is worse, in 4 patches they are similar. We show
three failure cases in Fig. 9.

The lymphocyte detection dataset [23] has 1367 labeled
training patches and 418 testing patches cropped from 12
representative lung adenocarcinoma whole slide tissue im-
ages. Patches with lymphocytes in the center are labeled
positive. Our method synthesized lymphocytes as round
and dark objects with around 7 microns in diameter. Some
synthetic image examples are shown in Fig. 8. Table 4
shows experimental evaluation of our method against a
level set features based method [67] and supervised VGG16
method [52]. We used the Area Under the ROC curve (AU-
ROC) measure as the evaluation metric.

6. Conclusions

Supervised methods rely on large volumes of labeled
histopathology data which are expensive to generate. We
introduced a method that learns from heterogeneous pathol-
ogy patches in an unsupervised manner. Our method syn-
thesizes training patches with importance weights, such that
the task-speciﬁc (e.g. segmentation) CNN is trained to mini-
mize the ideal (unbiased) generalization error over real data.
When no supervised data exists for a cancer type, our result
is signiﬁcantly better than across-cancer generalization re-
sults by supervised methods. Even when supervised data
exists, our approach performs as well as supervised meth-
ods, due to the much larger scale of synthetic data. We
release segmentation results on over 5000 WSIs, which is
orders of magnitude larger than currently available human
annotated datasets. In future work we will demonstrate the
generality of our importance sampling based loss minimiza-
tion approach on other tasks such as mixed-quality image
classiﬁcation [63].

3U24CA215109-02,

1U24CA180924-01A1,

Acknowledgement This work was supported in part
by
and
1UG3CA225021-01 from the National Cancer Institute,
R01LM011119-01 and R01LM009239 from the U.S.
National Library of Medicine, and a gift from Adobe.
This work used the Extreme Science and Engineering
Discovery Environment (XSEDE), which is supported by
National Science Foundation grant number ACI-1548562.
Speciﬁcally, it used the Bridges system, which is supported
by NSF award number ACI-1445606, at the Pittsburgh
Supercomputing Center (PSC).

8540

References

[1] H. J. Aerts, E. R. Velazquez, R. T. Leijenaar, C. Parmar,
P. Grossmann, S. Cavalho, J. Bussink, R. Monshouwer,
B. Haibe-Kains, D. Rietveld, et al. Decoding tumour pheno-
type by noninvasive imaging using a quantitative radiomics
approach. Nature communications, 2014.

[2] M. Bai and R. Urtasun. Deep watershed transform for in-

stance segmentation. In CVPR, 2017.

[3] N. Bayramoglu and J. Heikkil¨a. Transfer learning for cell nu-
clei classiﬁcation in histopathology images. In ECCV Work-
shops, 2016.

[4] N. Bayramoglu, M. Kaakinen, L. Eklund, and J. Heikkila.
Towards virtual h&e staining of hyperspectral lung histology
images using conditional generative adversarial networks. In
CVPR, 2017.

[5] S. Beucher. Watershed, hierarchical segmentation and water-
fall algorithm. In Mathematical morphology and its applica-
tions to image processing. 1994.

[6] L. Bi, J. Kim, A. Kumar, D. Feng, and M. Fulham. Synthe-
sis of positron emission tomography (pet) images via multi-
channel generative adversarial networks (gans).
In Molec-
ular Imaging, Reconstruction and Analysis of Moving Body
Organs, and Stroke Imaging and Treatment. 2017.

[7] C. M. Bishop. Pattern recognition and machine learning.

2006.

[8] F. Calimeri, A. Marzullo, C. Stamile, and G. Terracina.
Biomedical data augmentation using generative adversarial
neural networks.
In International Conference on Artiﬁcial
Neural Networks, 2017.

[9] H. Chen, X. Qi, L. Yu, Q. Dou, J. Qin, and P.-A. Heng. Dcan:
Deep contour-aware networks for object instance segmenta-
tion from histology images. Medical Image Analysis, 2017.

[10] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005.

[11] R. Colen, I. Foster, R. Gatenby, M. E. Giger, R. Gillies,
D. Gutman, M. Heller, R. Jain, A. Madabhushi, S. Mad-
havan, et al. Nci workshop report: clinical and computa-
tional requirements for correlating imaging phenotypes with
genomics signatures. Translational oncology, 2014.

[12] F. S. Collins and H. Varmus. A new initiative on precision

medicine. New England Journal of Medicine, 2015.

[13] L. A. Cooper, A. B. Carter, A. B. Farris, F. Wang, J. Kong,
D. A. Gutman, P. Widener, T. C. Pan, S. R. Cholleti,
A. Sharma, et al. Digital pathology: Data-intensive frontier
in medical imaging. Proceedings of the IEEE, 2012.

[14] L. A. Cooper, J. Kong, D. A. Gutman, F. Wang, S. R. Chol-
leti, T. C. Pan, P. M. Widener, A. Sharma, T. Mikkelsen,
A. E. Flanders, et al. An integrative approach for in silico
glioma research.
IEEE Transactions on Biomedical Engi-
neering, 2010.

[15] L. A. Cooper, J. Kong, D. A. Gutman, F. Wang, J. Gao,
C. Appin, S. Cholleti, T. Pan, A. Sharma, L. Scarpace, et al.
Integrated morphologic analysis for the identiﬁcation and
characterization of disease subtypes. Journal of the Amer-
ican Medical Informatics Association, 2012.

[16] P. Costa, A. Galdran, M. I. Meyer, M. D. Abr`amoff,
M. Niemeijer, A. M. Mendonc¸a, and A. Campilho. Towards
adversarial retinal image synthesis. arXiv, 2017.

[17] N. R. Council et al. Toward precision medicine: building a
knowledge network for biomedical research and a new tax-
onomy of disease. National Academies Press, 2011.

[18] L. R. Dice. Measures of the amount of ecologic association

between species. Ecology, 1945.

[19] A. H. Fischer, K. A. Jacobson, J. Rose, and R. Zeller. Hema-
toxylin and eosin staining of tissue and cell sections. Cold
Spring Harbor Protocols, 2008.

[20] R. J. Gillies, P. E. Kinahan, and H. Hricak. Radiomics: im-
ages are more than pictures, they are data. Radiology, 2015.
[21] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and

harnessing adversarial examples. In ICLR, 2015.

[22] M. N. Gurcan and A. Madabhushi. Digital pathology. SPIE,

2013.

[23] L. Hou, V. Nguyen, A. B. Kanevsky, D. Samaras, T. M. Kurc,
T. Zhao, R. R. Gupta, Y. Gao, W. Chen, D. Foran, et al.
Sparse autoencoder for unsupervised nucleus detection and
representation in histopathology images. Pattern Recogni-
tion, 86:188–200, 2019.

[24] L. Hou, D. Samaras, T. M. Kurc, Y. Gao, J. E. Davis, and
J. H. Saltz. Patch-based convolutional neural network for
whole slide tissue image classiﬁcation. In CVPR, 2016.

[25] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie.

Stacked generative adversarial networks. In CVPR, 2017.

[26] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017.

[27] T. Kim.

Simulated+unsupervised learning in ten-
https://github.com/carpedm20/

sorﬂow.
simulated-unsupervised-tensorflow.

[28] G. Koch. Siamese neural networks for one-shot image recog-

nition. In ICML workshop, 2015.

[29] N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane,
and A. Sethi. A dataset and a technique for generalized nu-
clear segmentation for computational pathology. IEEE trans-
actions on medical imaging, 2017.

[30] H. Le, T. F. Y. Vicente, V. Nguyen, M. Hoai, and D. Sama-
ras. A+ D net: Training a shadow detector with adversarial
shadow attenuation. In ECCV, 2018.

[31] J. Lemley, S. Bazrafkan, and P. Corcoran.

Smart
augmentation-learning an optimal data augmentation strat-
egy. IEEE Access, 2017.

[32] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.
In NIPS,

Universal style transfer via feature transforms.
2017.

[33] P.-S. Liao, T.-S. Chen, P.-C. Chung, et al. A fast algorithm

for multilevel thresholding. J. Inf. Sci. Eng., 2001.

[34] D. Lin, J. Dai, J. Jia, K. He, and J. Sun.

Scribble-
sup: Scribble-supervised convolutional networks for seman-
tic segmentation. In CVPR, 2016.

[35] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-

image translation networks. In NIPS, 2017.

[36] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. In CVPR, 2015.

8541

[37] F. Mahmood, D. Borders, R. Chen, G. N. McKay, K. J. Sal-
imian, A. Baras, and N. J. Durr. Deep adversarial training for
multi-organ nuclei segmentation in histopathology images.
arXiv, 2018.

[38] A. Mauricio, J. L´opez, R. Huauya, and J. Diaz. High-
resolution generative adversarial neural networks applied to
histological images generation. In International Conference
on Artiﬁcial Neural Networks, 2018.

[39] MICCAI 2018 Challenge.

in
Pathology Images. http://miccai.cloudapp.net/
competitions/83, 2018.

Segmentation of Nuclei

[40] V. Murthy, L. Hou, D. Samaras, T. M. Kurc, and J. H. Saltz.
Center-focusing multi-task CNN with injected features for
classiﬁcation of glioma nuclear images. In Winter Confer-
ence on Applications of Computer Vision (WACV), 2017.

[41] H. Noh, S. Hong, and B. Han. Learning deconvolution net-

work for semantic segmentation. In CVPR, 2015.

[42] A. Osokin, A. Chessel, R. E. C. Salas, and F. Vaggi. Gans

for biological image synthesis. In ICCV, 2017.

[43] C. Parmar, R. T. Leijenaar, P. Grossmann, E. R. Velazquez,
J. Bussink, D. Rietveld, M. M. Rietbergen, B. Haibe-Kains,
P. Lambin, and H. J. Aerts. Radiomic feature clusters and
prognostic signatures speciﬁc for lung and head & neck can-
cer. Scientiﬁc reports, 2015.

[44] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. In ICLR, 2016.

[45] E. Reinhard, M. Adhikhmin, B. Gooch, and P. Shirley. Color
transfer between images. IEEE Computer graphics and ap-
plications, 21(5):34–41, 2001.

[46] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for

data: Ground truth from computer games. In ECCV, 2016.

[47] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, 2015.

[48] J. Saltz, J. Almeida, Y. Gao, A. Sharma, E. Bremer,
T. DiPrima, M. Saltz, J. Kalpathy-Cramer, and T. Kurc.
Towards generation, management, and exploration of com-
bined radiomics and pathomics datasets for cancer research.
AMIA Summits on Translational Science Proceedings, 2017.
[49] S. Sankaran, L. R. Khot, C. Z. Espinoza, S. Jarolmasjed,
V. R. Sathuvalli, G. J. Vandemark, P. N. Miklas, A. H. Carter,
M. O. Pumphrey, N. R. Knowles, et al. Low-altitude, high-
resolution aerial imaging systems for row and ﬁeld crop phe-
notyping: A review. European Journal of Agronomy, 2015.
[50] A. Shrivastava, A. Gupta, and R. Girshick. Training region-
based object detectors with online hard example mining. In
CVPR, 2016.

[51] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,
and R. Webb. Learning from simulated and unsupervised
images through adversarial training. In CVPR, 2017.

[52] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2014.
[53] T. Sørensen. {A method of establishing groups of equal
amplitude in plant sociology based on similarity of species
and its application to analyses of the vegetation on Danish
commons}. Biol. Skr., 1948.

[54] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-

domain image generation. ICLR, 2017.

[55] A. Telea. An image inpainting technique based on the fast

marching method. Journal of graphics tools, 2004.

[56] The TCGA team. The Cancer Genome Atlas. https://

cancergenome.nih.gov/.

[57] T. Vicente, L. Hou, C.-P. Yu, M. Hoai, and D. Sama-
ras. Large-scale training of shadow detectors with noisily-
annotated shadow examples. In ECCV, 2016.

[58] Q. D. Vu, S. Graham, M. N. N. To, M. Shaban, T. Qaiser,
N. A. Koohbanani, S. A. Khurram, T. Kurc, K. Farahani,
T. Zhao, et al. Methods for segmentation and classiﬁcation
of digital microscopy tissue images. arXiv, 2018.

[59] S. Wang, J. Yao, Z. Xu, and J. Huang. Subtype cell detec-
tion with an accelerated deep convolution neural network. In
MICCAI, 2016.

[60] X. Wang, A. Shrivastava, and A. Gupta. A-fast-rcnn: Hard
In

positive generation via adversary for object detection.
CVPR, 2017.

[61] Y. Xie, F. Xing, X. Kong, H. Su, and L. Yang. Beyond classi-
ﬁcation: structured regression for robust cell detection using
convolutional neural network. In MICCAI, 2015.

[62] J. Xu, L. Xiang, Q. Liu, H. Gilmore, J. Wu, J. Tang, and
A. Madabhushi. Stacked sparse autoencoder (ssae) for nuclei
detection on breast cancer histopathology images. Medical
Imaging, 2016.

[63] F. Yang, Q. Zhang, M. Wang, and G. Qiu. Quality classiﬁed
image analysis with application to face detection and recog-
nition. arXiv, 2018.

[64] L. Yang, Y. Zhang, J. Chen, S. Zhang, and D. Z. Chen. Sug-
gestive annotation: A deep active learning framework for
biomedical image segmentation. In MICCAI, 2017.

[65] C. Yuan, Y. Zhang, and Z. Liu. A survey on technologies for
automatic forest ﬁre monitoring, detection, and ﬁghting us-
ing unmanned aerial vehicles and remote sensing techniques.
Canadian journal of forest research, 2015.

[66] Y. Zhang, L. Yang, J. Chen, M. Fredericksen, D. P. Hughes,
and D. Z. Chen. Deep adversarial networks for biomedical
image segmentation utilizing unannotated images. In MIC-
CAI, 2017.

[67] J. Zhao, L. Xiong, K. Jayashree, J. Li, F. Zhao, Z. Wang,
S. Pranata, S. Shen, and J. Feng. Dual-agent gans for pho-
torealistic and identity preserving proﬁle face synthesis. In
NIPS, 2017.

[68] N. Zhou, X. Yu, T. Zhao, S. Wen, F. Wang, W. Zhu, T. Kurc,
A. Tannenbaum, J. Saltz, and Y. Gao. Evaluation of nucleus
segmentation in digital pathology images through large scale
image synthesis. SPIE Medical Imaging. International Soci-
ety for Optics and Photonics, 2017.

[69] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In CVPR, 2017.

[70] H. Zou and T. Hastie. Regularization and variable selection
via the elastic net. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 2005.

8542

