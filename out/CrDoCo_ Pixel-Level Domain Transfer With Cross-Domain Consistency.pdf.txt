CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency

Yun-Chun Chen1

2

,

Yen-Yu Lin1

2National Taiwan University

Ming-Hsuan Yang3
3UC Merced

4

,

Jia-Bin Huang5

4Google

5Virginia Tech

1Academia Sinica

.

g
e
s

c
i
t
n
a
m
e
S

n
o
i
t
c
i
d
e
r
p
h
t
p
e
D

w
o
ﬂ

l
a
c
i
t
p
O

Labeled examples (source domain)

Input (target domain)

Output

Figure 1: Applications of the proposed method. Our method has the applications ranging from semantic segmentation (top
row), depth prediction (middle row), to optical ﬂow estimation (bottom row).

Abstract

1. Introduction

Unsupervised domain adaptation algorithms aim to
transfer the knowledge learned from one domain to another
(e.g., synthetic to real images). The adapted representa-
tions often do not capture pixel-level domain shifts that are
crucial for dense prediction tasks (e.g., semantic segmenta-
tion). In this paper, we present a novel pixel-wise adversar-
ial domain adaptation algorithm. By leveraging image-to-
image translation methods for data augmentation, our key
insight is that while the translated images between domains
may differ in styles, their predictions for the task should be
consistent. We exploit this property and introduce a cross-
domain consistency loss that enforces our adapted model to
produce consistent predictions. Through extensive experi-
mental results, we show that our method compares favor-
ably against the state-of-the-art on a wide variety of unsu-
pervised domain adaptation tasks.

Deep convolutional neural networks (CNNs) are ex-
tremely data hungry. However, for many dense predic-
tion tasks (e.g., semantic segmentation, optical ﬂow esti-
mation, and depth prediction), collecting large-scale and di-
verse datasets with pixel-level annotations is difﬁcult since
the labeling process is often expensive and labor intensive
(see Figure 1). Developing algorithms that can transfer the
knowledge learned from one labeled dataset (i.e., source
domain) to another unlabeled dataset (i.e., target domain)
thus becomes increasingly important. Nevertheless, due to
the domain-shift problem (i.e., the domain gap between the
source and target datasets), the learned models often fail to
generalize well to new datasets.

To address these issues, several unsupervised domain
adaptation methods have been proposed to align data dis-
tributions between the source and target domains. Existing
methods either apply feature-level [39, 26, 44, 42, 15, 14]

1791

or pixel-level [1, 36, 7, 14] adaptation techniques to mini-
mize the domain gap between the source and target datasets.
However, aligning marginal distributions does not necessar-
ily lead to satisfactory performance as there is no explicit
constraint imposed on the predictions in the target domain
(as no labeled training examples are available). While sev-
eral methods have been proposed to alleviate this issue via
curriculum learning [34, 6] or self-paced learning [53], the
problem remains challenging since these methods may only
learn from cases where the current models perform well.

Our work.
In this paper, we present CrDoCo, a pixel-
level adversarial domain adaptation algorithm for dense
prediction tasks. Our model consists of two main mod-
ules: 1) an image-to-image translation network and 2) two
domain-speciﬁc task networks (one for source and the other
for target). The image translation network learns to translate
images from one domain to another such that the translated
images have a similar distribution to those in the translated
domain. The domain-speciﬁc task network takes images of
source/target domain as inputs to perform dense prediction
tasks. As illustrated in Figure 2, our core idea is that while
the original and the translated images in two different do-
mains may have different styles, their predictions from the
respective domain-speciﬁc task network should be exactly
the same. We enforce this constraint using a cross-domain
consistency loss that provides additional supervisory signals
for facilitating the network training, allowing our model to
produce consistent predictions. We show the applicability
of our approach to multiple different tasks in the unsuper-
vised domain adaptation setting.

Our contributions. First, we present an adversarial
learning approach for unsupervised domain adaptation
which is applicable to a wide range of dense predic-
tion tasks. Second, we propose a cross-domain consis-
tency loss that provides additional supervisory signals for
network training, resulting in more accurate and consis-
tent task predictions. Third, extensive experimental re-
sults demonstrate that our method achieves the state-of-
the-art performance against existing unsupervised domain
adaptation techniques. Our source code is available at
https://yunchunchen.github.io/CrDoCo/

2. Related Work

Unsupervised domain adaptation. Unsupervised do-
main adaptation methods can be categorized into two
groups: 1) feature-level adaptation and 2) pixel-level adap-
tation. Feature-level adaptation methods aim at aligning
the feature distributions between the source and target do-
mains through measuring the correlation distance [39], min-
imizing the maximum mean discrepancy [26], or apply-
ing adversarial learning strategies [44, 42] in the feature

n
o

i
t

l

a
s
n
a
r
t
 

e
g
a
m

Consistency

Loss

I

Images of

different styles

Segmentation

network

Prediction

Figure 2: Main idea. While images may have different
appearances/styles in different domains, their task predic-
tions (e.g., semantic segmentation as shown in this exam-
ple) should be exactly the same. Our core idea in this paper
is to impose a cross-domain consistency loss between the
two task predictions.

space. In the context of image classiﬁcation, several meth-
ods [10, 11, 26, 27, 43, 44] have been developed to address
the domain-shift issue. For semantic segmentation tasks,
existing methods often align the distributions of the fea-
ture activations at multiple levels [15, 17, 42]. Recent ad-
vances include applying class-wise adversarial learning [4]
or leveraging self-paced learning policy [53] for adapting
synthetic-to-real or cross-city adaptation [4], adopting cur-
riculum learning for synthetic-to-real foggy scene adapta-
tion [34], or progressively adapting models from daytime
scene to nighttime [6]. Another line of research focuses on
pixel-level adaptation [1, 36, 7]. These methods address the
domain gap problem by performing data augmentation in
the target domain via image-to-image translation [1, 36] or
style transfer [7] methods.

Most recently, a number of methods tackle joint feature-
level and pixel-level adaptation in image classiﬁcation [14],
semantic segmentation [14], and single-view depth predic-
tion [48] tasks. These methods [14, 48] utilize image-
to-image translation networks (e.g., the CycleGAN [51])
to translate images from source domain to target domain
with pixel-level adaptation. The translated images are then
passed to the task network followed by a feature-level align-
ment.

While both feature-level and pixel-level adaptation have
been explored, aligning the marginal distributions without
enforcing explicit constraints on target predictions would
not necessarily lead to satisfactory performance. Our model
builds upon existing techniques for feature-level and pixel-
level adaptation [14, 48]. The key difference lies in our
cross-domain consistency loss that explicitly penalizes in-
consistent predictions by the task networks.

Cycle consistency. Cycle consistency constraints have
been successfully applied to various problems. In image-
to-image translation, enforcing cycle consistency allows the
network to learn the mappings without paired data [51, 22].

1792

In semantic matching, cycle or transitivity based consis-
tency loss help regularize the network training [50, 49, 3].
In motion analysis, forward-backward consistency check
can be used for detecting occlusion [28, 20, 52] or learn-
ing visual correspondence [45]. Similar to the above meth-
ods, we show that enforcing two domain-speciﬁc networks
to produce consistent predictions leads to substantially im-
proved performance.

Learning from synthetic data. Training the model on
large-scale synthetic datasets has been extensively studied
in semantic segmentation [41, 42, 15, 14, 7, 17, 34, 35,
53], multi-view stereo [18], depth estimation [48], optical
ﬂow [40, 19, 21], amodal segmentation [16], and object de-
tection [7, 30].
In our work, we show that the proposed
cross-domain consistency loss can be applied not only to
synthetic-to-real adaptation but to real-to-real adaptation
tasks as well.

3. Method

In this section, we ﬁrst provide an overview of our ap-
proach. We then describe the proposed loss function for en-
forcing cross-domain consistency on dense prediction tasks.
Finally, we describe other losses that are adopted to facili-
tate network training.

3.1. Method overview

We consider the task of unsupervised domain adaptation
for dense prediction tasks. In this setting, we assume that
we have access to a source image set XS, a source label
set YS, and an unlabeled target image set XT . Our goal is
to learn a task network FT that can reliably and accurately
predict the dense label for each image in the target domain.
To achieve this task, we present an end-to-end train-
able network which is composed of two main modules: 1)
the image translation network GS→T and GT →S and 2)
two domain-speciﬁc task networks FS and FT . The im-
age translation network translates images from one domain
to the other. The domain-speciﬁc task network takes input
images to perform the task of interest.

As shown in Figure 3, the proposed network takes an
image IS from the source domain and another image IT
from the target domain as inputs. We ﬁrst use the image
translation network to obtain the corresponding translated
images IS→T = GS→T (IS) (in the target domain) and
IT →S = GT →S(IT ) (in the source domain). We then pass
IS and IT →S to FS, IT and IS→T to FT to obtain their task
predictions.

3.2. Objective function

the translated images and the images in the corresponding
domain. Second, the reconstruction loss Lrec regularizes
the image translation network GS→T and GT →S to perform
self-reconstruction when translating an image from one do-
main to another followed by a reverse translation. Third,
the feature-level adversarial loss Lfeat
adv aligns the distribu-
tions between the feature representations of the translated
images and the images in the same domain. Fourth, the task
loss Ltask guides the two domain-speciﬁc task networks
FS and FT to perform dense prediction tasks. Fifth, the
cross-domain consistency loss Lconsis enforces consistency
constraints on the task predictions. Such a cross-domain
loss couples the two domain-speciﬁc task networks FS and
FT during training and provides supervisory signals for the
unlabeled target domain image IT and its translated one
IT →S. Speciﬁcally, the training objective L is deﬁned as

L = Ltask + λconsis · Lconsis + λrec · Lrec

+ λimg · Limg

adv + λfeat · Lfeat
adv ,

(1)

where λconsis, λrec, λimg, and λfeat are the hyper-
parameters used to control the relative importance of the
respective loss terms. Below we outline the details of each
loss function.

3.3. Cross domain consistency loss Lconsis

Since we do not have labeled data in the target domain,
to allow our model to produce accurate task predictions on
unlabeled data, we ﬁrst generate a translated version of IT
(i.e., IT →S) by passing IT to the image translation network
GT →S (i.e., IT →S = GT →S(IT )). Our key insight is that
while IT (belongs to the target domain) and IT →S (belongs
to the source domain) may differ in appearance or styles,
these two images should have the same task prediction re-
sults (i.e., FT (IT ) and FS(IT →S) should be exactly the
same). We thus propose a cross-domain consistency loss
Lconsis that bridges the outputs of the two domain-speciﬁc
task networks (i.e., FS and FT ). The loss enforces the
consistency between the two task predictions FT (IT ) and
FS(IT →S). For semantic segmentation task, we compute
the bi-directional KL divergence loss and deﬁne the cross-
domain consistency loss for semantic segmentation Lconsis
task as

Lconsis(XT ; GS→T , GT →S, FS, FT )

= − EIT ∼XT Xh,w,c
− EIT ∼XT Xh,w,c

fT →S(h, w, c) log(cid:18)fT (h, w, c)(cid:19)
fT (h, w, c) log(cid:18)fT →S(h, w, c)(cid:19),

(2)

The overall training objective L for training the proposed
network consists of ﬁve loss terms. First, the image-level
adversarial loss Limg
adv aligns the image distributions between

where fT = FT (IT ) and fT →S = FS(IT →S) are the task
predictions for IT and IT →S, respectively, while c denotes
the number of classes.

1793

-).#/%'
()*"+,

%
$
#
"
!

"
$
#
%
!

!"#$%&'
()*"+,

'S
'S
'T
'T

.*
!,/0&34
.()*
.*)(
!,/0&34
.(

!,/012,+

!,/012,+

!+,%-

!+,%-

"
&
'
(
)
*

+
,

-
.
*

,

fS

f()*
f*)(
f(

!"#$%&%

Figure 3: Overview of the proposed method. Our model is composed of two main modules: an image translation network
(highlighted in gray) and two domain-speciﬁc task networks (highlighted in blue and green, respectively). The image trans-
lation network learns to translate input images from one domain to the other. The input and the translated images are then
fed to their corresponding domain-speciﬁc task networks to perform task predictions. Our main contribution lies in the use
of cross-domain consistency loss Lconsis for regularizing the network training.

As our task models produce different outputs for differ-
ent tasks, our cross-domain consistency loss Lconsis is task-
dependent. For depth prediction task, we use the ℓ1 loss for
the cross-domain consistency loss Lconsis. For optical ﬂow
estimation task, the cross-domain consistency loss Lconsis
computes the endpoint error between the two task predic-
tions.

3.4. Other losses

In addition to the proposed cross-domain consistency
loss Lconsis, we also adopt several other losses introduced
in [14, 48, 51] to facilitate the network training.

Task loss Ltask. To guide the training of the two task net-
works FS and FT using labeled data, for each image-label
pair (IS, ys) in the source domain, we ﬁrst translate the
source domain image IS to IS→T by passing IS to GS→T
(i.e., IS→T = GS→T (IS)). Similarly, images before and
after translation should have the same ground truth label.
Namely, the label for IS→T is identical to that of IS which
is ys.

We can thus deﬁne the task loss Ltask for training the
two domain-speciﬁc task networks FS and FT using la-
beled data. For semantic segmentation, we calculate the

cross-entropy loss between the task predictions and the cor-
responding ground truth labels as our task loss Ltask. Like-
wise, the task loss Ltask is also task dependent. We use ℓ1
loss for depth prediction task and endpoint error for optical
ﬂow estimation.

Feature-level adversarial loss Lfeat
adv .
In addition to im-
posing cross-domain consistency and task losses, we apply
two feature-level discriminators Dfeat
(for source domain)
(for target domain) [51]. The discriminator Dfeat
and Dfeat
helps align the distributions between the feature maps of IS
(i.e., fS) and IT →S (i.e., fT →S). To achieve this, we deﬁne
the feature-level adversarial loss in the source domain as

S

S

T

Lfeat

adv (XS, XT ; GT →S, FS, Dfeat
S )
= EIS ∼XS [log(Dfeat
+ EIT ∼XT [log(1 − Dfeat

S (fS))]

S (fT →S))].

(3)

Similarly, Dfeat

T

tween fT and fS→T .
feature-level adversarial
Lfeat

adv (XT , XS; GS→T , FT , Dfeat

T ).

aligns

the

distributions

be-
This corresponds to another
loss in the target domain as

1794

loss Limg
adv.

Image-level adversarial
In addition to
feature-level adaptation, we also consider image-level adap-
tation between the translated images and those in the corre-
sponding domain. Similar to Zhu et al. [51], we deploy two
image-level discriminators Dimg
(for source domain) and
Dimg
aims at aligning the
distributions between the image IS and the translated one
IT →S. To accomplish this, we deﬁne the image-level ad-
versarial loss in the source domain as

(for target domain). The Dimg

S

S

T

adv (XS, XT ; GT →S, Dimg
Limg
S )
= EIS ∼XS [log(Dimg
S (IS))]
+ EIT ∼XT [log(1 − Dimg

S (IT →S))].

(4)

Similarly, we have another image-level adversarial loss

in the target domain as Limg

adv(XT , XS; GS→T , Dimg
T ).

Reconstruction loss Lrec. Finally, we use an image re-
construction loss Lrec to regularize the training of the im-
age translation network. We exploit the property that when
translating an image from one domain to another followed
by performing a reverse translation, we should obtain the
same image. Namely, GT →S(GS→T (IS)) ≈ IS for any IS
in the source domain and GS→T (GT →S(IT )) ≈ IT for any
IT in the target domain hold.

More precisely, we deﬁne the reconstruction loss Lrec as

Lrec (XS, XT ; GS→T , GT →S)

= EIS ∼XS [kGT →S(GS→T (IS)) − ISk1]
+ EIT ∼XT [kGS→T (GT →S(IT )) − IT k1].

(5)

Following Zhu et al. [51], we use the ℓ1 norm to deﬁne

the reconstruction loss Lrec.

Based on the aforementioned loss functions, we aim to
T by optimizing

solve for a target domain task network F ∗
the following min-max problem:

F ∗

T = arg min

FT

min
FS ,

GS→T
GT →S

max
S ,Dimg
S ,Dfeat

Dimg
Dfeat

T

T

L.

(6)

Namely, to train our network using labeled source do-
main images and unlabeled target domain images, we min-
imize the cross-domain consistency loss Lconsis, the task
loss Ltask, and the reconstruction loss Lrec. The image-
level adversarial loss Limg
adv and the feature-level adversar-
ial loss Lfeat
adv are optimized to align the image and feature
distributions within the same domain. The proposed cross-
domain consistency loss, in contrast, aligns the task predic-
tions in two different domains.

3.5. Implementation details

We implement our model using PyTorch. We use the
CycleGAN [51] as our image-to-image translation network

T

S

S

and Dfeat

and Dimg

GS→T and GT →S. The structure of the image-level dis-
criminators Dimg
consists of four residual blocks,
each of which is composed of a convolutional layer fol-
lowed by a ReLU activation. For the feature-level discrim-
inators Dfeat
T , we use the same architecture as
Tsai et al. [42]. The image-to-image translation network
S , Dimg
GS→T and GT →S, and the discriminators Dimg
T ,
Dfeat
are all randomly initialized. We have a
batch size of 1, a learning rate of 10−3 with momentum
0.9, and set the weight decay as 5 × 10−4. Our hyper-
parameters setting: λconsis = 10, λrec = 10, λimg = 0.1,
and λfeat = 0.001. We train our model on a single NVIDIA
GeForce GTX 1080 GPU with 12 GB memory.

S , and Dfeat

T

4. Experimental Results

4.1. Semantic segmentation

We present experimental results for semantic segmenta-
tion in two different settings: 1) synthetic-to-real: adapt-
ing from synthetic GTA5 [32] and SYNTHIA [33] datasets
to real-world images from Cityscapes dataset [5] and 2)
real-to-real: adapting the Cityscapes dataset to different
cities [4].

4.1.1 GTA5 to Cityscapes

Dataset. The GTA5 dataset [32] consists of 24, 966 syn-
thetic images with pixel-level annotations of 19 categories
(compatible with the Cityscapes dataset [5]). Following
Hoffman et al. [14], we use the GTA5 dataset and adapt
the model to the Cityscapes training set with 2, 975 images.

Evaluation protocols. We evaluate our model on the
Cityscapes validation set with 500 images using the mean
intersection-over-union (IoU) and the pixel accuracy as the
evaluation metrics.

Task network. We evaluate our proposed method using
two task networks: 1) dilated residual network-26 (DRN-
26) [46] and 2) FCN8s-VGG16 [25]. For the DRN-26, we
initialize our task network from Hoffman et al. [14]. For
the FCN8s-VGG16, we initialize our task network from
Sankaranarayanan et al. [35].

Results. We compare our approach with the state-of-the-
art methods [41, 51, 24, 15, 14, 7, 17, 35, 47]. The top block
of Table 1 presents the experimental results. Results on both
feature backbones show that our method performs favorably
against the state-of-the-art methods, outperforming the pre-
vious best competitors by 4.9% in mean IoU [17] when us-
ing the DRN-26 [46] and 1.0% in mean IoU [35] when us-
ing FCN8s-VGG16 [25]. We show that the proposed cross-
domain consistency loss Lconsis is critical for the improved
performance (e.g., adding Lconsis improves the mean IoU

1795

Table 1: Experimental results of synthetic-to-real adaptation for semantic segmentation. We denote the top results as
bold and underlined.

Method

Synth. [7]

DR [41]

CycleGAN [51]

UNIT [24]

FCNs ITW [15]

CyCADA [14]

DS [7]

GAM [17]
Ours w/o Lconsis
Ours

Synth. [47]

Curr. DA [47]

LSD [35]

Ours

Synth. [7]

DR [41]

CycleGAN [51]

UNIT [24]

FCNs ITW [15]

DS [7]
Ours w/o Lconsis
Ours

Synth. [47]

Curr. DA [47]

LSD [35]

Ours

Backbone

DRN-26 [46]

FCN8s [25]

DRN-26 [46]

FCN8s [25]

k
l
a
w
e
d
i

S

19.9

23.5

45.1

38.5

32.4

33.1

43.5

-

44.9

49.2

6.8

22.0

30.5

33.2

10.8

16.7

20.4

20.6

19.6

28.0

17.2

21.2

11.2

26.1

29.1

32.8

g
n
i
d
l
i
u
B

52.8

65.7

81.6

81.1

62.1

77.9

81.5

-

80.9

86.4

64.1

71.7

78.6

80.1

49.6

59.5

71.6

73.2

30.8

75.3

64.3

72.8

59.6

74.9

77.5

80.1

d
a
o
R

68.9

67.5

89.3

90.5

70.4

79.1

89.0

-

89.1

95.1

18.1

74.9

88.0

89.1

28.5

31.3

58.8

56.3

11.5

67.0

58.3

62.2

5.6

65.2

80.1

84.9

l
l
a

W

6.5

6.7

27.5

23.5

14.9

23.4

22.1

-

27.5

35.2

7.3

6.0

25.2

26.9

0.2

2.2

1.6

1.8

4.4

4.0

2.0

4.2

8.0

0.1

2.8

4.3

e
c
n
e
F

13.6

12.0

18.6

16.3

5.4

17.3

8.5

-

18.8

22.1

8.7

11.9

23.5

25.0

0.0

0.0

0.7

0.3

0.0

0.2

0.7

0.8

0.5

0.5

0.4

0.4

e
l
o
P

9.3

11.6

29.0

30.2

10.9

32.1

27.5

-

30.2

36.1

21.0

8.4

16.7

18.3

18.5

19.7

27.9

29.0

20.3

29.9

24.3

30.1

21.5

10.7

26.8

29.4

GTA5 → Cityscapes

t
h
g
i
L
c
ﬁ
f
a
r
T

11.7

16.1

35.7

25.2

14.2

33.3

30.7

-

35.6

40.9

14.9

16.3

23.5

23.4

n
g
i

S
c
ﬁ
f
a
r
T

8.0

13.7

17.3

18.5

2.7

31.8

18.9

-

17.1

29.1

16.8

11.1

11.6

12.8

n
o
i
t
a
t
e
g
e
V

75.0

70.3

79.3

79.5

79.2

81.5

84.8

-

79.5

85.0

45.9

75.7

78.7

77.0

n
i
a
r
r
e
T

11.0

8.3

29.4

26.8

21.3

26.7

28.3

-

27.2

33.1

2.4

13.3

27.2

29.1

SYNTHIA → Cityscapes

0.7

0.4

2.7

4.0

0.1

3.8

2.6

4.1

8.0

3.5

11.1

14.2

5.6

6.2

8.5

11.8

11.7

15.7

5.9

10.7

5.3

3.0

18.0

21.0

65.3

64.7

73.5

72.2

42.3

78.6

72.2

76.3

72.4

76.1

78.1

79.2

-

-

-

-

-

-

-

-

-

-

-

-

n
o
s
r
e
P

36.9

39.6

59.7

59.2

44.1

62.8

55.7

-

59.7

67.3

41.6

38.0

51.3

55.1

36.6

43.1

45.3

50.7

51.2

54.0

41.9

45.6

35.1

47.1

48.2

50.2

y
k
S

56.5

71.3

71.5

77.8

64.6

69.0

84.1

-

71.6

75.8

64.4

66.5

71.9

72.4

71.6

67.3

73.1

74.5

68.7

78.0

70.8

73.6

75.6

70.6

76.7

78.3

r
e
d
i
R

0.1

1.6

15.7

17.4

4.2

14.7

5.4

-

16.1

26.8

17.5

9.3

19.5

20.2

6.4

3.9

16.2

18.4

3.8

15.4

10.3

14.9

9.0

8.2

15.2

15.9

r
a
C

51.3

55.0

85.3

84.4

70.4

74.5

83.2

-

84.6

88.9

55.3

55.2

80.4

79.9

43.8

35.1

67.2

67.3

54.0

69.7

64.2

69.2

23.6

43.2

70.5

69.8

k
c
u
r
T

8.5

15.1

18.2

22.2

8.0

20.9

20.3

-

18.1

23.4

8.4

18.8

19.8

22.3

-

-

-

-

-

-

-

-

-

-

-

-

s
u
B

4.7

3.0

14.8

16.1

7.3

25.6

28.3

-

14.6

19.3

5.0

18.9

18.3

19.5

2.7

8.3

14.9

15.1

3.2

12.0

12.5

14.1

4.5

20.7

17.4

23.4

n
i
a
r
T

0.0

0.6

1.4

1.6

0.0

6.9

0.1

-

1.4

4.3

6.9

0.0

0.9

1.0

-

-

-

-

-

-

-

-

-

-

-

-

e
k
i
b
r
o
t
o
M

0.1

0.2

21.9

16.7

3.5

18.8

8.7

-

22.1

25.3

4.3

16.8

20.8

20.1

0.8

0.3

7.9

6.7

0.2

9.9

8.0

12.2

0.5

0.7

8.7

11.0

e
l
c
y
c
i
B

0.0

3.3

12.5

16.9

0.0

20.4

6.2

-

10.9

13.5

13.8

16.6

18.4

18.7

10.0

5.5

24.7

29.5

0.6

19.2

21.3

23.0

18.0

13.1

16.7

15.6

U
o
I

n
a
e
m

22.9

25.5

39.6

39.1

27.1

39.5

38.3

40.2

39.4

45.1

22.3

28.9

37.1

38.1

18.5

19.2

27.1

28.0

17.0

29.5

29.8

33.4

22.0

29.0

36.1

38.2

.
c
c
a

l
e
x
i

P

71.9

73.8

86.6

87.1

-

82.3

87.2

81.1

85.8

89.2

-

-

-

86.3

54.6

57.9

71.4

70.8

-

76.5

75.3

79.5

-

-

-

84.7

by 5.7% and the pixel accuracy by 3.4% when adopting
the DRN-26 [46] as the task network). Figure 4 presents
an example that demonstrates the effectiveness of the pro-
posed cross-domain consistency loss Lconsis. We discover
that by applying the cross-domain consistency loss Lconsis,
our model produces more consistent and accurate results be-
fore and after image translation.

4.1.2 SYNTHIA to Cityscapes

the

use

Dataset. We
SYNTHIA-RAND-
CITYSCAPES [33] set as the source domain which
contains 9, 400 images compatible with the Cityscapes
annotated classes. Following Dundar et al. [7], we evaluate
images on the Cityscapes validation set with 16 classes.

Results. We compare our approach with the state-of-the-
art methods [41, 51, 24, 15, 7]. The bottom block of Table 1
In either DRN-26 [46]
presents the experimental results.
or FCN8s [25] backbone, our method achieves state-of-the-
art performance. Likewise, we show sizable improvement
using the proposed cross-domain consistency loss Lconsis.

which is a real-to-real adaptation. The dataset contains four
different cities: Rio, Rome, Tokyo, and Taipei, where each
city has 3, 200 images without annotations and 100 images
with pixel-level ground truths for 13 classes. Following
Tsai et al. [42], we use the Cityscapes [5] training set as
our source domain and adapt the model to each target city
using 3, 200 images, and use the 100 annotated images for
evaluation.

Results. We compare our approach with the Cross-
City [4], the CBST [53], and the AdaptSegNet [42]. Table 2
shows that our method achieve state-of-the-art performance
on two out of four cities. Note that the results in Adapt-
SegNet [42] are obtained by using a ResNet-101 [13]. We
run their publicly available code with the default settings
and report the results using the ResNet-50 [13] as the fea-
ture backbone for a fair comparison. Under the same exper-
imental setting, our approach compares favorably against
state-of-the-art methods. Furthermore, we show that en-
forcing cross-domain consistency constraints, our method
effectively and consistently improves the results evaluated
on all four cities.

4.1.3 Cityscapes to Cross-City

4.2. Single view depth estimation

Dataset.
In addition to the synthetic-to-real adaptation,
we conduct an experiment on the Cross-City dataset [4]

To show that our formulation is not

limited to se-
mantic segmentation, we present experimental results for

1796

Input images

Ground truth

Ours w/o Lconsis

Ours

Figure 4: Visual results of semantic segmentation. We translate an image from Cityscapes to GTA5. For each input image,
we present the segmentation results with and without applying the cross-domain consistency loss.

Table 2: Experimental results of real-to-real adaptation for semantic segmentation. Adaptation: Cityscapes →
Cross-City.

City

Method

Feature backbone

Rome

Rio

Tokyo

Taipei

Cross-City [4]

-

CBST [53]

ResNet-38 [13]

AdaptSegNet [42]

ResNet-101 [13]

AdaptSegNet [42]
Ours w/o Lconsis
Ours

ResNet-50 [13]

ResNet-50 [13]

ResNet-50 [13]

Cross-City [4]

-

CBST [53]

ResNet-38 [13]

AdaptSegNet [42]

ResNet-101 [13]

AdaptSegNet [42]
Ours w/o Lconsis
Ours

ResNet-50 [13]

ResNet-50 [13]

ResNet-50 [13]

Cross-City [4]

-

CBST [53]

ResNet-38 [13]

AdaptSegNet [42]

ResNet-101 [13]

AdaptSegNet [42]
Ours w/o Lconsis
Ours

ResNet-50 [13]

ResNet-50 [13]

ResNet-50 [13]

Cross-City [4]

-

CBST [53]

ResNet-38 [13]

AdaptSegNet [42]

ResNet-101 [13]

AdaptSegNet [42]
Ours w/o Lconsis
Ours

ResNet-50 [13]

ResNet-50 [13]

ResNet-50 [13]

k
l
a
w
e
d
i
S

29.3

43.9

34.2

34.6

31.2

37.2

43.9

55.2

44.7

43.9

44.1

43.3

35.4

33.6

26.0

25.3

24.9

29.3

28.6

35.2

29.5

27.8

26.9

28.1

g
n
i
d
l
i
u
B

84.5

89.7

88.3

88.1

87.7

91.2

79.0

85.4

84.6

80.7

81.2

81.2

72.8

80.4

77.8

78.1

77.6

78.2

80.0

84.2

85.2

83.2

84.1

85.1

d
a
o
R

79.5

87.1

83.9

85.4

84.4

90.2

74.2

84.3

76.2

75.8

74.7

77.5

83.4

85.2

81.5

76.0

72.3

82.1

78.6

86.1

81.7

81.8

79.6

79.7

t
h
g
i
L

0.0

14.8

18.8

18.9

18.6

22.0

2.4

19.6

9.3

7.7

5.3

10.1

12.3

8.3

17.8

15.4

14.3

18.2

13.1

15.0

26.4

24.4

23.7

24.4

n
g
i
S

22.2

47.7

40.2

39.1

38.0

41.1

7.5

30.1

25.5

21.1

19.2

23.2

12.7

31.1

26.8

22.3

23.1

27.5

7.6

22.2

15.6

12.6

14.1

16.4

n
o
i
t
a
t
e
g
e
V

80.6

85.4

86.2

82.3

80.7

86.3

77.8

80.5

81.8

80.8

80.7

79.7

77.4

83.9

82.7

81.3

80.9

83.1

68.2

75.6

76.7

74.1

72.8

74.3

n
o
s
r
e
P

29.5

45.4

47.8

43.2

43.5

47.1

39.3

55.2

55.3

51.2

52.3

57.4

42.7

53.2

55.8

45.2

43.6

56.4

16.8

22.7

31.0

30.9

30.3

29.5

r
e
d
i
R

13.0

26.6

21.7

22.4

19.8

25.1

10.3

28.6

32.7

27.4

27.7

31.9

21.5

28.9

38.0

34.6

35.2

37.8

9.4

33.1

12.5

11.1

9.9

12.8

y
k
S

82.8

90.3

93.1

89.1

85.4

91.7

69.5

77.9

87.3

88.0

86.3

88.2

64.3

78.2

90.9

91.1

90.7

91.2

82.1

74.9

91.7

88.7

86.5

87.9

r
a
C

71.7

85.4

80.9

79.9

79.4

83.0

67.9

79.7

74.3

71.1

69.2

72.2

64.1

72.7

72.1

69.3

68.9

74.3

60.4

78.0

71.5

70.8

69.9

69.8

s
u
B

37.5

20.5

47.8

44.6

45.3

48.0

41.2

33.2

28.9

25.6

24.1

29.1

20.8

4.4

4.2

2.3

3.1

9.5

34.0

37.6

41.1

40.2

40.6

40.0

e
k
i
b
r
o
t
o
M

25.9

49.8

48.3

46.0

44.2

47.5

27.9

37.6

43.0

43.7

45.4

38.9

8.9

27.0

24.5

20.7

19.8

26.0

26.5

58.0

47.3

45.3

44.7

46.8

e
l
c
y
c
i
B

1.0

10.3

8.6

5.3

5.1

6.2

10.9

11.5

27.6

26.9

25.2

22.4

40.3

47.0

50.8

48.2

42.4

52.1

9.9

30.9

27.7

26.2

25.8

28.1

U
o
I
n
a
e
m

42.9

53.6

53.8

52.2

51.0

55.1

42.5

52.2

51.6

49.5

48.9

50.4

42.8

48.8

49.9

46.9

45.9

51.2

39.6

50.3

49.1

47.5

46.8

47.9

single-view depth prediction task. Speciﬁcally, we use
SUNCG [38] as the source domain and adapt the model to
the NYUDv2 [37] dataset.

tal, we generate 130, 190 valid views from 4, 562 different
houses.

Dataset. To generate the paired synthetic training data,
we rendered RGB images and depth map from the SUNCG
dataset [38], which contains 45, 622 3D houses with vari-
ous room types. Following Zheng et al. [48], we choose the
camera locations, poses and parameters based on the distri-
bution of real NYUDv2 dataset [37] and retain valid depth
maps using the criteria described by Song et al. [38]. In to-

Evaluation protocols. We use the root mean square error
(RMSE) and the log scale version (RMSE log.), the squared
relative difference (Sq. Rel.) and the absolute relative dif-
ference (Abs. Rel.), and the accuracy measured by thresh-
olding (δ < threshold).

Task network. We initialize our task network from the
unsupervised version of Zheng et al. [48].

1797

Table 3: Synthetic-to-real (SUNCG → NYUv2) adaptation for depth prediction. The column “Supervision” indicates
methods trained with NYUv2 training data. We denote the top two results as bold and underlined.

Method

Supervision

Abs. Rel. ↓

Sq. Rel. ↓

RMSE ↓

RMSE log. ↓

δ < 1.25 ↑

δ < 1.252 ↑

δ < 1.253 ↑

X

X

X

X

Liu et al. [23]

Eigen et al. [9] Fine

Eigen et al. [8] (VGG)
T2Net [48]

Synth.

Baseline (train set mean)
T2Net [48]
Ours w/o Lconsis
Ours

0.213

0.215

0.158

0.157

0.304

0.439

0.257

0.254

0.233

-

0.212

0.121

0.125

0.394

0.641

0.281

0.283

0.272

0.759

0.907

0.641

0.556

1.024

1.148

0.915

0.911

0.898

-

0.285

0.214

0.199

0.369

0.415

0.305

0.306

0.289

0.650

0.611

0.769

0.779

0.458

0.412

0.540

0.541

0.562

0.906

0.887

0.950

0.943

0.771

0.692

0.832

0.835

0.853

0.976

0.971

0.988

0.983

0.916

0.586

0.948

0.947

0.952

Results. Table 3 shows the comparisons with prior meth-
ods [23, 9, 8, 48]. Here, the column “Supervision” indicates
that the method is learned in a supervised fashion. While
not directly comparable, we report their results for com-
pleteness. Under the same experimental settings, we ob-
serve that our method achieves state-of-the-art performance
on all adopted evaluation metrics. Moreover, with the in-
tegration of the cross-domain consistency loss Lconsis, our
method shows consistently improved performance.

4.3. Optical ﬂow estimation

We show evaluations of the model trained on a synthetic
dataset (i.e., MPI Sintel [2]) and test the adapted model on
real-world images from the KITTI 2012 [12] and KITTI
2015 [29] datasets.

Dataset. The MPI Sintel dataset [2] consists of 1, 401 im-
ages rendered from artiﬁcial scenes. There are two ver-
sions: 1) the ﬁnal version consists of images with motion
blur and atmospheric effects, and 2) the clean version does
not include these effects. We use the clean version as the
source dataset. We report two results obtained by 1) using
the KITTI 2012 [12] as the target dataset and 2) using the
KITTI 2015 [29] as the target dataset.

Evaluation protocols. We adopt the average endpoint er-
ror (AEPE) and the F1 score for both KITTI 2012 and
KITTI 2015 to evaluate the performance.

Task network. Our task network is initialized from the
PWC-Net [40] (without ﬁnetuning on the KITTI dataset).

Results. We compare our approach with the state-of-the-
art methods [40, 31, 19]. Table 4 shows that our method
achieves improved performance on both datasets. When
incorporating the proposed cross-domain consistency loss
Lconsis, our model improves the results by 1.76 in terms
of average endpoint error on the KITTI 2012 test set and
10.6% in terms of F1-all on the KITTI 2015 test set.

Table 4: Experimental results of synthetic-to-real adap-
tation for optical ﬂow estimation. Left: MPI Sintel →
KITTI 2012. Right: MPI Sintel → KITTI 2015.
The column “ﬁnetune” indicates that method is ﬁnetuned
on the KITTI dataset. The bold and the underlined numbers
indicate top two results, respectively.

KITTI 2012

KITTI 2015

Method

ﬁnetune

AEPE

AEPE

F1-Noc

AEPE

train

4.13

1.28

1.45

4.09

4.14

4.16

2.19

test

4.7

1.8

1.7

-

4.22

4.92

3.16

test

train

12.31%

4.82%

4.22%

-

8.10%

13.52%

8.57%

-

2.30

2.16

10.06

10.35

10.76

8.02

X

X

X

SpyNet [31]

FlowNet2 [19]

PWC-Net [40]

FlowNet2 [19]

PWC-Net [40]
Ours w/o Lconsis
Ours

4.4. Limitations

F1-all

train

-

8.61%

9.80%

30.37%

33.67%

34.01%

23.14%

F1-all

test

35.05%

10.41%

9.60%

-

-

36.43%

25.83%

Our method is memory-intensive as the training involves
multiple networks at the same time. Potential approaches
to alleviate this issue include 1) adopting partial sharing on
the two task networks, e.g., share the last few layers of the
two task networks, and 2) sharing the encoders in the image
translation network (i.e., GS→T and GT →S).

5. Conclusions

We have presented a simple yet surprisingly effective
loss for improving pixel-level unsupervised domain adap-
tion for dense prediction tasks. We show that by incorporat-
ing the proposed cross-domain consistency loss, our method
consistently improves the performances over a wide range
of tasks. Through extensive experiments, we demonstrate
that our method is applicable to a wide variety of tasks.

Acknowledgement. This work was supported in part by
NSF under Grant No. 1755785, No. 1149783, Ministry of
Science and Technology (MOST) under grants 107-2628-
E-001-005-MY3 and 108-2634-F-007-009, and gifts from
Adobe, Verisk, and NEC. We thank the support of NVIDIA
Corporation with the GPU donation.

1798

References

[1] Konstantinos Bousmalis, Nathan Silberman, David Dohan,
Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-
level domain adaptation with generative adversarial net-
works. In CVPR, 2017.

[2] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and
Michael J Black. A naturalistic open source movie for opti-
cal ﬂow evaluation. In ECCV, 2012.

[3] Yun-Chun Chen, Po-Hsiang Huang, Li-Yu Yu, Jia-Bin
Huang, Ming-Hsuan Yang, and Yen-Yu Lin. Deep semantic
matching with foreground detection and cycle-consistency.
In ACCV, 2018.

[4] Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen, Bo-Cheng Tsai,
Yu-Chiang Frank Wang, and Min Sun. No more discrimi-
nation: Cross city adaptation of road scene segmenters. In
ICCV, 2017.

[5] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016.

[6] Dengxin Dai and Luc Van Gool. Dark model adaptation: Se-
mantic image segmentation from daytime to nighttime.
In
International Conference on Intelligent Transportation Sys-
tems (ITSC), 2018.

[7] Aysegul Dundar, Ming-Yu Liu, Ting-Chun Wang, John
Zedlewski, and Jan Kautz. Domain stylization: A strong,
simple baseline for synthetic to real image domain adapta-
tion. arXiv, 2018.

[8] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale convo-
lutional architecture. In ICCV, 2015.

[9] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. In NIPS, 2014.

[10] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain

adaptation by backpropagation. In ICML, 2015.

[11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. JMLR, 2016.

[12] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In CVPR, 2012.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[14] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Dar-
rell. Cycada: Cycle-consistent adversarial domain adapta-
tion. In ICML, 2018.

[15] Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell.
Fcns in the wild: Pixel-level adversarial and constraint-based
adaptation. arXiv, 2016.

[16] Yuan-Ting Hu, Hong-Shuo Chen, Kexin Hui, Jia-Bin Huang,
and Alexander Schwing. Sail-vos: Semantic amodal in-

stance level video object segmentation - a synthetic dataset
and baselines. In CVPR, 2019.

[17] Haoshuo Huang, Qixing Huang, and Philipp Kr¨ahenb¨uhl.
In

Domain transfer through deep activation matching.
ECCV, 2018.

[18] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra
Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view
stereopsis. In CVPR, 2018.

[19] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-
tion of optical ﬂow estimation with deep networks. In CVPR,
2017.

[20] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,
Ersin Yumer, and Ming-Hsuan Yang. Learning blind video
temporal consistency. In ECCV, 2018.

[21] Wei-Sheng Lai, Jia-Bin Huang, and Ming-Hsuan Yang.
Semi-supervised learning for optical ﬂow with generative ad-
versarial networks. In NIPS, 2017.

[22] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh
Singh, and Ming-Hsuan Yang. Diverse image-to-image
translation via disentangled representations. In ECCV, 2018.

[23] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian D Reid.
Learning depth from single monocular images using deep
convolutional neural ﬁelds. TPAMI, 2016.

[24] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised

image-to-image translation networks. In NIPS, 2017.

[25] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015.

[26] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I
Jordan. Learning transferable features with deep adaptation
networks. In ICML, 2015.

[27] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I
Jordan. Unsupervised domain adaptation with residual trans-
fer networks. In NIPS, 2016.

[28] Simon Meister, Junhwa Hur, and Stefan Roth. Unﬂow: Un-
supervised learning of optical ﬂow with a bidirectional cen-
sus loss. In AAAI, 2018.

[29] Moritz Menze and Andreas Geiger. Object scene ﬂow for

autonomous vehicles. In CVPR, 2015.

[30] Xingchao Peng, Ben Usman, Kuniaki Saito, Neela Kaushik,
Judy Hoffman, and Kate Saenko. Syn2real: A new bench-
mark for synthetic-to-real visual domain adaptation.
In
ECCV, 2018.

[31] Anurag Ranjan and Michael J Black. Optical ﬂow estimation

using a spatial pyramid network. In CVPR, 2017.

[32] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen
Koltun. Playing for data: Ground truth from computer
games. In ECCV, 2016.

[33] German Ros, Laura Sellart, Joanna Materzynska, David
Vazquez, and Antonio M Lopez. The synthia dataset: A large
collection of synthetic images for semantic segmentation of
urban scenes. In CVPR, 2016.

[34] Christos Sakaridis, Dengxin Dai, Simon Hecker, and Luc
Van Gool. Model adaptation with synthetic and real data for
semantic dense foggy scene understanding. In ECCV, 2018.

1799

[53] Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang.
Domain adaptation for semantic segmentation via class-
balanced self-training. In ECCV, 2018.

[35] Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser
Nam Lim, and Rama Chellappa. Learning from synthetic
data: Addressing domain shift for semantic segmentation. In
CVPR, 2018.

[36] Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Joshua
Susskind, Wenda Wang, and Russell Webb.
Learning
from simulated and unsupervised images through adversarial
training. In CVPR, 2017.

[37] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Indoor segmentation and support inference from

Fergus.
rgbd images. In ECCV, 2012.

[38] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene comple-
tion from a single depth image. In CVPR, 2017.

[39] Baochen Sun and Kate Saenko. Deep coral: Correlation

alignment for deep domain adaptation. In ECCV, 2016.

[40] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
Pwc-net: Cnns for optical ﬂow using pyramid, warping, and
cost volume. In CVPR, 2018.

[41] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Woj-
ciech Zaremba, and Pieter Abbeel. Domain randomization
for transferring deep neural networks from simulation to the
real world. In IROS, 2017.

[42] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki-
hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker.
Learning to adapt structured output space for semantic seg-
mentation. In CVPR, 2017.

[43] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko.
In

Simultaneous deep transfer across domains and tasks.
ICCV, 2015.

[44] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
In CVPR,

Adversarial discriminative domain adaptation.
2017.

[45] Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learn-
ing correspondence from the cycle-consistency of time. In
CVPR, 2019.

[46] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated

residual networks. In CVPR, 2017.

[47] Yang Zhang, Philip David, and Boqing Gong. Curricu-
lum domain adaptation for semantic segmentation of urban
scenes. In ICCV, 2017.

[48] Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. T2net:
Synthetic-to-realistic translation for solving single-image
depth estimation tasks. In ECCV, 2018.

[49] Tinghui Zhou, Yong Jae Lee, Stella X Yu, and Alyosha A
Efros. Flowweb: Joint image set alignment by weaving con-
sistent, pixel-wise correspondences. In CVPR, 2015.

[50] Xiaowei Zhou, Menglong Zhu, and Kostas Daniilidis. Multi-
image matching via fast alternating minimization. In ICCV,
2015.

[51] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017.

[52] Yuliang Zou, Zelun Luo, and Jia-Bin Huang. Df-net: Un-
supervised joint learning of depth and ﬂow using cross-task
consistency. In ECCV, 2018.

1800

