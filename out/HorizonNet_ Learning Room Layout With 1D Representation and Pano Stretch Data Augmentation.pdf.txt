HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch

Data Augmentation

Cheng Sun

Chi-Wei Hsiao

Min Sun

Hwann-Tzong Chen

National Tsing Hua University

{chengsun, chiweihsiao}@gapp.nthu.edu.tw

sunmin@ee.nthu.edu.tw

htchen@cs.nthu.edu.tw

Figure 1: Some examples of 3D reconstructed room layouts by our HorizonNet.

Abstract

We present a new approach to the problem of estimating
the 3D room layout from a single panoramic image. We rep-
resent room layout as three 1D vectors that encode, at each
image column, the boundary positions of ﬂoor-wall and
ceiling-wall, and the existence of wall-wall boundary. The
proposed network, HorizonNet, trained for predicting 1D
layout, outperforms previous state-of-the-art approaches.
The designed post-processing procedure for recovering 3D
room layouts from 1D predictions can automatically infer
the room shape with low computation cost—it takes less
than 20ms for a panorama image while prior works might
need dozens of seconds. We also propose Pano Stretch Data
Augmentation, which can diversify panorama data and be
applied to other panorama-related learning tasks. Due to
the limited data available for non-cuboid layout, we re-
label 65 general layout from the current dataset for ﬁne-
tuning. Our approach shows good performance on general
layouts by qualitative results and cross-validation.

1. Introduction

The goal of this work is to predict the room layout from
a panoramic image. Most of the state-of-the-art methods

solve this problem by adopting more effective deep net-
work architectures for their models to learn from different
cues in the image. Assumptions about the room structures
are often made to constrain the solution space so that the
predictions of the deep model would not deviate from the
common cases too much. Post-processing steps can fur-
ther be performed to reﬁne the predictions. Given a num-
ber of images with annotated layouts for training, state-of-
the-art methods are able to achieve good results on the test
data. However, acquiring high-quality room-layout annota-
tions for panoramic images is labor-demanding. The anno-
tations done by different people might be inconsistent due
to ambiguities about the locations of wall boundaries, espe-
cially for well-decorated rooms. Moreover, currently avail-
able datasets do not include more images of complex room
layouts. The annotation for a complex layout would just be
approximated as a cuboid-shaped or L-shaped layout, intro-
ducing even more ambiguities for training and testing.

Two important and correlated issues may be further ad-
dressed for improving state-of-the-art methods. The ﬁrst
issue is the lack of more training and validation data with
precise annotations. The second issue is that, without more
annotated data for training, the deep networks cannot be too
large, otherwise the test accuracy might be low due to over-
ﬁtting. Collecting more data to train a more sophisticated

1047

model is indeed beneﬁcial and doable, but a more efﬁcient
way to improve the performance should also be welcome.
We argue that, if we have some better understanding of the
problem and make good use of domain knowledge, we may
improve the performance without acquiring a lot more an-
notated data or using a larger deep network. Data augmen-
tation is a common procedure in deep learning to generate
more data for training. Standard data augmentation heuris-
tics such as random cropping or luminance change for im-
age classiﬁcation or object detection might not be effective
for layout prediction. Our idea is to take account of the
underlying geometric constraints and design a better data
augmentation mechanism speciﬁcally for training layout-
predicting deep networks. On the other hand, instead of
increasing the model complexity, we aim to enhance the
model by devising a compact representation with respect
to the geometric constraints. We can, therefore, remove re-
dundant degrees of freedom and force the model to focus
more on learning critical properties for layout prediction.

We characterize our contributions as follows:

• We introduce a 1D O(W ) representation that encodes
the whole-room layout for a panoramic scene. Train-
ing with such a representation allows our method to
outperform previous state-of-the-art results, yet re-
quires fewer parameters and less computation time.

• We propose a data augmentation mechanism called
Pano Stretch Data Augmentation, which generates
panorama images on the ﬂy during training and im-
proves the accuracy under all settings in our experi-
ments. This data augmentation mechanism also has
the potential for boosting other tasks (e.g., semantic
segmentation, object detection) that directly work on a
panorama.

• We show that leveraging RNNs in a layout predic-
tion task is helpful for improving the accuracy. RNNs
are able to capture the long-range geometric pattern of
room layouts.

• Owing to the 1D representation and our efﬁcient post-
processing procedure,
the computation cost of our
model is very low, and the model can be easily ex-
tended to handle complex scenes with layouts other
than cuboid-shaped or L-shaped.

Code and data are available at: https://sunset1995.
github.io/HorizonNet/.

2. Related Work

Room layout estimation from a single-view RGB image
is an active research topic over the past decade. Many ap-
proaches have been developed in this ﬁeld. Most of them

exploit the Manhattan world assumption that the room lay-
outs, and even the furniture, are aligned with the three prin-
cipal axes [3]. The Manhattan world assumption imposes
constraints on the layout estimation problem, and, based
on the assumption, the Manhattan aligned vanishing points
could also be used to rectify the image and extract features
for inferring the layout.

Delage et al. [6] train a dynamic Bayesian network to
recognize the ﬂoor-wall boundary in each column of the
perspective image. Many approaches search the Manhat-
tan aligned layout based on extracted geometric cues. Lee
et al. [18] test the hypothesis using Orientation Map (OM)
while Hedau et al. [12] using Geometric Context (GC) [14].
Hedau et al. [10] further jointly inference the room layout
with 3D objects, e.g. beds. Similar strategies have also been
used by later methods, such as introducing an improved
scoring function [26, 27], generating layout hypothesis with
Manhattan junction [22], and modeling the interaction be-
tween objects and layout [5, 10, 34].

The aforementioned methods only deal with perspective
images. Zhang et al. [32] propose to estimate the layout
from a 360◦ H-FOV panoramic image. They extend the
previous methods of vanishing point detection, hypothesis
generation, and scoring hypotheses based on OM, GC and
object interaction, and apply all of them to panoramas. Xu
et al. [28] also use the OM, GC, object detection, and ob-
ject orientation to reconstruct 3D layout. Yang et al. [29]
use superpixels and Manhattan aligned line segments as fea-
tures, and formulate the problem by constraint graphs. The
method of [31] follows a similar approach using more geo-
metric and semantic features. Other approaches attempt to
recover the ﬂoor plan from a panorama using image gradi-
ent cues [21] or from multiple panorama images [2].

Recent methods rely more on deep networks to improve
layout estimation. Most of them leverage dense predic-
tion models to classify geometric or semantic label for each
pixel. For perspective images, common ways are to predict
the boundary probability map [19, 23], classes of bound-
aries [33, 23], classes of layout surface [4, 15], and corner
keypoints heatmaps [17]. The predicted dense maps can
be post-processed to generate layouts. A few deep learn-
ing methods have been developed for panorama-based lay-
out estimation. Zou et al. [35] predict the corner proba-
bility map and boundary map directly from a panorama.
They also extend Stanford 2D-3D dataset [1] with annotated
layouts for training and evaluation. Fernandez-Labrador et
al. [9] train the deep network on perspective images. During
testing, they stitch the predicted perspective boundary maps
into a panorama and combine them with geometric cues to
infer the layout. Two concurrent works DuLa-Net [30] and
CFL [8] show improved quantitative results with the ability
to produce general room shape not limited to cuboid shape.
DuLa-Net [30] combines the surface semantic mask from

1048

conventional equirectangular view and the projected ﬂoor
and ceiling view. CFL [8] proposes convolution kernel spe-
cialized for equirectangular image.

Unlike all the existing methods that use neural networks
to perform dense prediction for layout estimation, we lever-
age the property of aligned panorama image to predict the
positions of ﬂoor-wall and ceiling-wall boundaries, as well
as the existence of wall-wall boundary for each column of
an equirectangular image. Our model only produces three
values for each column of an image, and thus the output size
of the model is reduced from O(HW ) to O(W ). The pro-
posed output representation is similar to [6] but they only
predict ﬂoor-wall boundary for each column of a perspec-
tive image using a Dynamic Bayesian Network. In contrast,
our work can handle panoramas and recognize ﬂoor-wall,
ceiling-wall and wall-wall boundaries using a deep neural
network. Existing works [35, 9, 30, 8] on the same task
learn to make dense O(HW ) predictions over the entire im-
age while our model predicts only three values for each im-
age column. RoomNet [17] imitates RNN’s recurrent struc-
ture with “time steps” equal to reﬁnement steps. We use
RNN where each “time step” is responsible for estimating
the result across a few image columns.

3. Approach

The goal of our approach is to estimate Manhattan
room layout from a panoramic image that covers 360◦ H-
FOV. Unlike conventional dense prediction (target output
size = O(HW )) for layout estimation using deep learn-
ing [4, 9, 7, 15, 19, 23, 33], we formulate the problem as re-
gressing the boundaries and classifying the corner for each
column of image (target output size = O(W )). The pro-
posed HorizonNet trained for predicting the O(W ) target is
presented in Sec. 3.1. In Sec. 3.2, we introduce a simple
yet fast and effective post-processing procedure to derive
the layout from output of HorizonNet. Finally in Sec. 3.3,
we introduce Pano Stretch Data Augmentation which effec-
tively augments the training data on-the-ﬂy by stretching
the image and ground-truth layout along x or z axis (Fig. 5).
All training and test images are pre-processed by the
panoramic image alignment algorithm mentioned in [35].
Our approach exploits the properties of the aligned panora-
mas that the wall-wall boundaries are vertical lines under
equirectangular projection. Therefore, we can use only one
value to indicate the column position of wall-wall boundary
instead of two (each for a boundary endpoint).

3.1. HorizonNet

Fig. 2 shows an overview of our network, which com-
prises a feature extractor and a recurrent neural network.
The network takes a single panorama image with the dimen-
sion of 3 × 512 × 1024 (channel, height, width) as input.

1D Layout Representation: The size of network output
is 3 × 1 × 1024. As illustrated in Fig. 3, two of the three
output channels represent the ceiling-wall (yc) and the ﬂoor-
wall (yf ) boundary position of each image column, and the
other one (yw) represents the existence of wall-wall bound-
ary (i.e. corner). The values of yc and yf are normalized to
[−π/2, π/2]. Since deﬁning yw as a binary-valued vector
with 0/1 labels would make it too sparse to detect (only 4
out of 1024 non-zero values for simple cuboid layout), we
set yw(i) = cdx where i indicates the ith column, dx is the
distance from the ith column to the nearest column where
wall-wall boundary exists, and c is a constant. To check the
robustness of our method against the choice of c, we have
tried 0.6, 0.8, 0.9, 0.96, 0.99 and get similar results. There-
fore, we stick to c = 0.96 for all the experiments. One
beneﬁt of using 1D representation is that it is less affected
by zero dominant backgrounds. 2D whole-image represen-
tations of boundaries and corners would result in 95% zero
values even after smoothing [35]. Our 1D boundaries repre-
sentation introduces no zero backgrounds because the pre-
diction for each component of yc or yf is simply a real-
valued regression to the ground truth. The 1D wall-wall
(corners) representation also changes the peak-background
ratio of ground truth from 2N
1024 where N is the
number of wall-wall corners. Therefore, the 1D wall-wall
representation is also less affected by zero-dominated back-
ground. In addition, computation of 1D compact output is
more efﬁcient compared to 2D whole-image output. As de-
picted in Sec. 3.2, recovering the layout from our three 1D
representations is simple, fast, and effective.

512·1024 to N

Feature Extractor: We adopt ResNet-50 [11] as our fea-
ture extractor. The output of each block of ResNet-50
has half spatial resolution compared to that of the previous
block. To capture both low-level and high-level features,
each block of the ResNet-50 contains a sequence of convo-
lution layers in which the number of channels and the height
is reduced by a factor of 8 (= 2×2×2) and 16 (= 4×2×2),
respectively. More speciﬁcally, each block contains three
convolution layers with 4 × 1, 2 × 1, 2 × 1 kernel size and
stride, and the number of channels after each Conv is re-
duced by a factor of 2. All the extracted features from each
layer are upsampled to the same width 256 (a quarter of in-
put image width) and reshaped to the same height. The ﬁnal
concatenated feature map is of size 1024 × 1 × 256. The
activation function after each Conv is ReLU except the ﬁnal
layer in which we use Sigmoid for yw and an identity func-
tion for yc, yf . We have tried various settings for the feature
extractor, including deeper ResNet-101, different designs of
the convolution layers after each ResNet block, and upsam-
pling to the image width 1024, and ﬁnd that the results are
similar. Therefore, we stick to the simpler and computa-
tionally efﬁcient setting.

1049

Figure 2: An illustration of the HorizonNet architecture.

strate the difference between models with or without RNN.

3.2. Post processing

We recover general room layouts that are not limited to
cuboid under following assumptions: i) intersecting walls
are perpendicular to each other (Manhattan world assump-
tion); ii) all rooms have the one-ﬂoor-one-ceiling layout
where ﬂoor and ceiling are parallel to each other; iii) camera
height is 1.6 meters following [32]; iv) the pre-processing
step correctly align the ﬂoor orthogonal to y-axis.

Figure 3: Visualization of our 1D ground truth represen-
tations. yw denotes the existence probability of wall-wall
boundary. yc, yf (plotted in green and blue) denote the po-
sitions of the ceiling-wall boundary and ﬂoor-wall bound-
ary respectively. For better visualization, we plot yw, yc, yf
with line width greater than one pixel.

Recurrent Neural Network for Capturing Global Infor-
mation: Recurrent neural networks (RNNs) are capable
of learning patterns and long-term dependencies from se-
quential data. Geometrically speaking, any corner of a room
can be roughly inferred from the positions of other corners;
therefore, we use the capability of RNN to capture global
information and long-term dependencies.
Intuitively, be-
cause LSTM [13], a type of RNN architecture, stores in-
formation about its prediction for other regions in the cell
state, it has the ability to predict for occluded area accu-
rately based on the geometric patterns of the entire room. In
our model, RNN is used to predict y′
w column by col-
umn. That is, the sequence length of RNN is proportional
to the image width. In our experiment, RNN predicts for
four columns instead of one column per time step, which re-
quires less computational time without loss of accuracy. As
the yc, yf , yw of a column is related to both its left and right
neighbors, we adopt the bidirectional RNN [25] to capture
the information from both sides. Fig. 7 and Table 1 demon-

f , y′

c, y′

c, y′
w

f and y′

As described in Sec. 3.1, raw outputs of our deep model
∈ R1024 contain the layout information for each
y′
f , y′
image column. Each value in y′
c is the position of
ﬂoor-wall boundary and ceiling-wall boundary at the corre-
sponding image column. y′
w represents the probability of
wall-wall existence of each image column.
Recovering the Floor and Ceiling Planes: For each col-
umn of the image, we can use the corresponding values in
y′
f , y′
c to vote for the ceiling-ﬂoor distance. Based on the as-
sumed camera height, we can project the ﬂoor-wall bound-
ary y′
f from image to 3D XY Z position (they all shared the
same Y ). The ceiling-wall boundary y′
c shares the same 3D
X, Z position with the y′
f on the same image column, and
therefore the distance between ﬂoor and ceiling can be cal-
culated. We take the average of results calculated from all
image columns as the ﬁnal ﬂoor-ceiling distance.
Recovering Wall Planes: We ﬁrst ﬁnd the prominent
peaks on the estimated wall-wall probability y′
w with two
criteria: i) the signal should be larger than any other sig-
nal within 5°H-FOV, and ii) the signal should be larger than
0.05.

Fig. 4a shows the projected y′

c (red points) on ceiling
plane. The green lines are the detected prominent peaks
which split the ceiling-wall boundary (red points) into mul-
tiple parts. To handle possibly failed horizontal alignment
in the pre-processing step, we calculate the ﬁrst principal
component of each part, then rotate the scene by the aver-

1050

age angle of all ﬁrst principal components (top right ﬁgure
in Fig. 4a).
So now we have two types of walls: i) X-
axis orthogonal walls and ii) Z-axis orthogonal walls. We
construct the walls from low to high variance suggested by
the ﬁrst principal component. Adjacency walls are forced
to be orthogonal to each other, thus only walls whose two
adjacent walls are not yet constructed have the freedom to
decide the orthogonal type. We use a simple voting strat-
egy: each projected red point votes for all planes within
0.16 meters (bottom right ﬁgure in Fig. 4a). The most voted
plane is selected. Two special cases are depicted in Fig 4b
which occur when the two adjacency walls are already con-
structed and they are orthogonal to each other. Finally, the
XY Z positions of all corners are decided according to the
intersection of three adjacent Manhattan junction planes.

The time complexity of our post-processing procedure
is O(W ), where W is the image width. Thus the post-
processing can be efﬁciently done; in average, it takes less
than 20ms to ﬁnish.

Prominent peak

Projected boundary

Camera Center

First PCA vector

Voting for Walls

Recover in 3D
with the Floor and Ceiling Planes

(a) Depicting how we recover the wall planes from our model
output.

Occluded Corner

False Negative

Camera Center

Camera Center

(b) Two special cases: Instead of voting for a wall, we add a
corner according to the two prominent peaks and the positions
of two walls.

Figure 4: Visualization of wall planes recovering. Fig. 4a
is an example that the pre-processing algorithm fails to cor-
rectly align the horizontal rotation of panorama.

kx = 1.0, kz = 1.0 (original)

kx = 2.0, kz = 1.0

kx = 1.0, kz = 2.0

kx = 2.0, kz = 2.0

Figure 5: Visualization of the proposed Pano Stretch Data
Augmentation. The image and ground-truth layout (green
lines) are stretched along x or z axis (the effect of scaling y
can be covered by x and z). This can augment the data by
changing the room’s length and width. This augmentation
strategy improves our quantitative results under all experi-
ment settings (Table 3).

3.3. Pano Stretch Data Augmentation

For a 360◦ H-FOV panoramic image, we propose to
stretch along axes in 3D space to augment training data.
To achieve this goal, we ﬁrst represent each pixel under UV
space as (u, v) where u ∈ [−π, π], v ∈ [−π/2, π/2]. The
coordinate (u, v) can be easily computed as the column and
row of an equirectangular image, subject to a rotation angle
of the camera. Here we introduce an additional variable d,
which denotes the depth of a pixel. We will show that d can
be eliminated later so our ﬁnal equation does not depend on
it.

We project the pixels to 3D space and multiply their
x, y, z by kx, ky, kz. The equation of stretched x′, y′, z ′ are
shown in Eq. 1.

x′ = kx · x = kx · d · cos(v) · cos(u) ;
y′ = ky · y = ky · d · sin(v) ;
z ′ = kz · z = kz · d · cos(v) · sin(u) .

(1)




We can then project the stretched points back to the
sphere by Eq. 2 for further equirectangular projection.
atan2 in the equation is 2-argument arctangent. The depth
d is eliminated since it exists in both terms of atan2. We
ﬁx ky = 1 because setting ky to a value other than one is
equivalent to multiplying kx, kz by the same value.




u′ = atan2(kz · sin(u), kx · cos(u)) ;
v′ = atan2(ky · sin(v),

qk2

x cos2(u) + k2

z sin2(u) · cos(v) ) .

(2)
In our implementation, we do the inverse mapping by
Eq. 3. For each pixel in the target image, we compute

1051

the corresponding coordinate and sample its value from the
source image via bilinear interpolation. Fig. 5 shows a vi-
sualization sample.

Method

3D IoU(%)

Corner
error(%)

Pixel

error(%)

Train on PanoContext dataset

(cid:26)u = atan2(kx · sin(u′), kz · cos(u′)) ;

v = arctan(kz · tan(v′) · csc(u′) · sin(u)) .

(3)

Note that our Pano Stretch Data Augmentation procedure
could also be used on other tasks (e.g., ground-truth map
of semantic segmentation, bounding box for object detec-
tion) that directly work on panoramas. The augmentation
procedure has the potential to boost the accuracy of those
tasks.

PanoContext [32]
LayoutNet [35]
DuLa-Net [30]

CFL [8]

ours

67.23
74.48
77.42
78.79
82.17

1.60
1.06

-

0.79
0.76

4.55
3.34

-

2.49
2.20

Train on PanoContext + Stnfd.2D3D datasets

LayoutNet [35]

ours

75.12
84.23

1.02
0.69

3.18
1.90

4. Experiments

4.1. Datasets

We train and evaluate our model using the same dataset
as LayoutNet [35]. The dataset consists of PanoContext
dataset [32] and the extended Stanford 2D-3D dataset [1]
annotated by [35]. To train our model, we generate 3 × 1 ×
1024 ground truth from the annotation. We follow the same
training/validation/test split of LayoutNet.

4.2. Training Details

The Adam optimizer [16] is employed to train the net-
work for 300 epochs with batch size 24 and learning rate
0.0003. The L1 Loss is used for the ceiling-wall bound-
ary (yc) and ﬂoor-wall boundary (yf ). The Binary Cross-
Entropy Loss is used for the wall-wall corner (yw). The
network is implemented in PyTorch [20]. It takes four hours
to ﬁnish the training on three NVIDIA GTX 1080 Ti GPUs.
The data augmentation techniques we adopt include
standard left-right ﬂipping, panoramic horizontal rotation,
and luminance change. Moreover, we exploit the proposed
Pano Stretch Data Augmentation (Sec. 3.3) during train-
ing. The stretching factors kx, kz are sampled from uni-
form distribution U [1, 2], and then take the reciprocals of
sampled values with probability 0.5. The process time
of Pano Stretch Data Augmentation is roughly 130ms per
512 × 1024 RGB image. Therefore, it is feasible to be ap-
plied on-the-ﬂy during training.

4.3. Cuboid Room Results

We generate cuboid room by only selecting the four most

prominent peaks in the post-processing step (Sec. 3.2).
Quantitative Results: Our approach is evaluated on three
i) 3D IoU: intersection over union be-
standard metrics:
tween 3D layout constructed from our prediction and the
ground truth; ii) Corner Error: average Euclidean distance
between predicted corners and ground-truth corners (nor-
malized by image diagonal length); iii) Pixel Error: pixel-
wise error between predicted surface classes and ground-
truth surface classes.

Table 1. Quantitative results of cuboid layout estimation
evaluated on the PaonContext [32] dataset. Our method out-
performs all existing methods under all settings.

The quantitative results of different training and testing
settings are summarized in Table 1 and Table 2. To clarify
the difference, the input resolution of DuLa-Net [30] and
CFL [8] are 256 × 512 while LayoutNet [35] and ours are
512 × 1024. Other than conventional augmentation tech-
nique, CFL [8] is trained with Random Erasing while ours
is trained with the proposed Pano Stretch. DuLa-Net [30]
did not report corner errors and pixel errors. Our approach
achieves state-of-the-art performance and outperforms ex-
isting methods under all settings.
Qualitative Results: The qualitative results are shown in
Fig. 6. We present the results from the best to the worst
based on their corner errors. Please see more results in the
supplemental materials.
Computation time: The 1D layout representation is easy
to compute. Forward passing a single 512 x 1024 RGB im-
age takes 8ms and 50ms for our HorizonNet with and with-
out RNN respectively. The post-processing step for extract-
ing layout from our 1D representation takes only 12ms. We
evaluate the result on a single NVIDIA Titan X GPU and an
Intel i7-5820K 3.30GHz CPU. The reported execution time
is averaged across all the testing data.

4.4. Ablation Study

Ablation experiments are presented in Table 3. We report
the result averaged across all the testing instances. For a fair
comparison, we also experiment with dense O(HW ) pre-
diction following LayoutNet [35] but replace the U-Net [24]
with the same backbone as our architecture. 1 The results of
this setting are presented in the ﬁrst two rows. We do not try
dense O(HW ) output with RNN since it would consume

1To output dense (full-image) probability map, we change the Conv
layer after each ResNet block from reducing both height and channels to
reducing only channels, and then upsample to the same spatial dimension
as the input image. Finally, the processed features of four blocks are con-
catenated and passed through a Conv layer to generate the ﬁnal result.

1052

Figure 6: Qualitative results of cuboid layout estimation. The results are separately sampled from four groups that comprise
results with the best 0–25%, 25–50%, 50–75% and 75–100% corner errors (displayed from the ﬁrst to the fourth columns).
The green lines are ground truth layout while the orange lines are estimated. The images in the ﬁrst row are from PanoContext
dataset [32] while second row are from Stanford 2D-3D dataset [1].

Method

3D IoU(%)

Corner
error(%)

Pixel

error(%)

Train on PanoContext dataset

CFL [8]

ours

65.13
75.57

1.44
0.94

Train on Stnfd.2D3D dataset

LayoutNet [35]
DuLa-Net [30]

ours

76.33
79.36
79.79

1.04

-

0.71

4.75
3.18

2.70

-

2.39

Train on PanoContext + Stnfd.2D3D datasets

LayoutNet [35]

ours

77.51
83.51

0.92
0.62

2.42
1.97

Table 2. Quantitative results of cuboid layout estimation
evaluated on the Stanford-2D3D [1] dataset. Our method
outperforms all existing methods under all settings.

too many computing resources. We can see that learning on
our 1D O(W ) layout representation is better than conven-
tional dense O(HW ) layout representation.

We observe that training with the proposed Pano Stretch
Data Augmentation can always boost the performance.
Note that the proposed data augmentation method can also
be adopted in other tasks on panoramas and has the poten-
tial to increase their accuracy as well. See supplemental
material for the experiment using Pano Stretch Data Aug-
mentation on semantic segmentation task.

For the rows where RNN columns are unchecked, the
RNN components shown in Fig 2 are replaced by fully con-
nected layers. Our experiments show that using RNN in net-
work architecture also improves performance. Fig. 7 shows
some representative results with and without RNN. The raw
output of the model with RNN is highly consistent with
the Manhattan world even without post-processing, which

Figure 7: Visualization of model outputs with and with-
out RNN. We plot the ground truth (green), outputs of the
model with RNN (yellow), and outputs of the model with-
out RNN (magenta). Both predictions are raw network out-
puts without post-processing. The model with RNN per-
forms better than the model without RNN in images contain
ceiling beam, black missing polar region caused by smaller
camera V-FOV, and occluded area.

demonstrates the ability of RNN to capture the geometric
pattern of the entire room.

4.5. Non cuboid Room Results

Since the non-cuboid rooms in PanoContext and Stan-
ford 2D-3D dataset are labeled as cuboids, our model is
never trained to recognize non-cuboid layouts and con-
cave corners. This bias makes our model tend to predict
complex-shaped rooms as cuboids. To estimate general
room layouts, we re-label 65 rooms from the training split
to ﬁne-tune our trained model. We ﬁne-tune our model for
300 epochs with learning rate 5e−5 and batch size 2.

To quantitatively evaluate the ﬁne-tuning result on
general-shaped rooms, we use 13-fold cross validation on
the 65 re-annotated non-cuboid data. The results are sum-

1053

Output Shape

Stretch Aug. RNN

3D IoU(%) Corner error(%)

Pixel error(%)

#params

FPS

dense O(HW )
dense O(HW )

our O(W )
our O(W )
our O(W )
our O(W )

V

V

V

V
V

77.87
79.64
80.65
81.22
81.23
83.74

1.02
0.74
0.80
0.71
0.72
0.65

2.73
2.39
2.43
2.28
2.20
1.95

67M
67M
25M
25M
57M
57M

98
98
119
119
20
20

Table 3. Ablation study demonstrates the effectiveness of each component in our approach. We show that all of our proposed
designs can improve the quantitative result. Besides, our proposed 1D layout representation signiﬁcantly reduces the number
of parameters. FPS is measured for forward-pass of a 3 × 512 × 1024 image on an NVIDIA TITAN X GPU.

Figure 8: Qualitative results of non-cuboid layout estimation. The occluded walls are ﬁlled with black. The blue lines in the
equirectangular images are the estimated room layout boundary.

marized in Table 4. We depict some examples of recon-
structed non-cuboid layouts from the testing and valida-
tion splits in Fig.1 and Fig.8. See supplemental material
for more reconstructed layouts. The results show that our
approach can work well on general room layout even with
corners occluded by other walls.

Method

Finetuning

3D IoU(%)

LayoutNet
LayoutNet

ours
ours

V

V

74.1
75.1
77.4
82.5

Table 4. Quantitative results on the 65 re-annotated non-
cuboid datas. The result of ﬁne-tuning is evaluated by 13-
fold validation.

5. Conclusion

We have presented a new 1D representation for the task
of estimating room layout from a panorama. The proposed
HorizonNet trained with such 1D representation outper-
forms previous state-of-the-art methods and requires fewer
computation resources. Our post-processing method which
recovers 3D layout from the model output is fast and effec-
tive, and it also works for complex room layouts even with
occluded corners. The proposed Pano Stretch Data Aug-
mentation further improves our results, and can also be ap-
plied to the training procedure of other panorama tasks for
potential improvement.

Acknowledgement: This research was partially supported
by iStaging and by MOST grants 106-2221-E-007-080-
MY3, 107-2218-E-007-047, and 108-2634-F-001-007.

1054

References

[1] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2D-
3D-Semantic Data for Indoor Scene Understanding. ArXiv
e-prints, Feb. 2017.

[2] Ricardo Cabral and Yasutaka Furukawa. Piecewise planar
and compact ﬂoorplan reconstruction from images. In Com-
puter Vision and Pattern Recognition (CVPR), 2014 IEEE
Conference on, pages 628–635. IEEE, 2014.

[3] James M Coughlan and Alan L Yuille. Manhattan world:
Compass direction from a single image by bayesian infer-
ence.
In Computer Vision, 1999. The Proceedings of the
Seventh IEEE International Conference on, volume 2, pages
941–947. IEEE, 1999.

[4] Saumitro Dasgupta, Kuan Fang, Kevin Chen, and Silvio
Savarese. Delay: Robust spatial layout estimation for clut-
tered indoor scenes. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 616–
624, 2016.

[5] Luca Del Pero, Joshua Bowdish, Bonnie Kermgard, Emily
Hartley, and Kobus Barnard. Understanding bayesian rooms
using composite 3d object models.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 153–160, 2013.

[6] Erick Delage, Honglak Lee, and Andrew Y Ng. A dynamic
bayesian network model for autonomous 3d reconstruction
from a single indoor image. In Computer Vision and Pattern
Recognition, 2006 IEEE Computer Society Conference on,
volume 2, pages 2418–2428. IEEE, 2006.

[7] Clara Fernandez-Labrador, Jose M Facil, Alejandro Perez-
Yus, Cedric Demonceaux, and Jose J Guerrero. Panoroom:
From the sphere to the 3d layout.
arXiv preprint
arXiv:1808.09879, 2018.

[8] Clara Fernandez-Labrador, Jos M Fcil, Alejandro Perez-Yus,
Cdric Demonceaux, Javier Civera, and Jos J Guerrero. Cor-
ners for layout: End-to-end layout recovery from 360 im-
ages. arXiv:1903.08094, 2019.

[9] Clara Fernandez-Labrador, Alejandro Perez-Yus, Gonzalo
Lopez-Nicolas, and Jose J Guerrero.
Layouts from
panoramic images with geometry and deep learning. arXiv
preprint arXiv:1806.08294, 2018.

[10] Abhinav Gupta, Martial Hebert, Takeo Kanade, and David M
Blei. Estimating spatial layout of rooms using volumetric
reasoning about objects and surfaces. In Advances in neural
information processing systems, pages 1288–1296, 2010.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages
770–778, 2016.

[12] Varsha Hedau, Derek Hoiem, and David Forsyth. Recover-
ing the spatial layout of cluttered rooms. In Computer vision,
2009 IEEE 12th international conference on, pages 1849–
1856. IEEE, 2009.

[13] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term

memory. Neural computation, 9(8):1735–1780, 1997.

[14] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recov-
ering surface layout from an image. International Journal of
Computer Vision, 75(1):151–172, 2007.

[15] Hamid Izadinia, Qi Shan, and Steven M Seitz. Im2cad. In

CVPR, 2017.

[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. CoRR, abs/1412.6980, 2014.

[17] Chen-Yu Lee, Vijay Badrinarayanan, Tomasz Malisiewicz,
and Andrew Rabinovich. Roomnet: End-to-end room layout
estimation. In Computer Vision (ICCV), 2017 IEEE Interna-
tional Conference on, pages 4875–4884. IEEE, 2017.

[18] David C Lee, Martial Hebert, and Takeo Kanade. Geometric
reasoning for single image structure recovery. In Computer
Vision and Pattern Recognition, 2009. CVPR 2009. IEEE
Conference on, pages 2136–2143. IEEE, 2009.

[19] Arun Mallya and Svetlana Lazebnik. Learning informative
edge maps for indoor scene layout prediction. In Proceed-
ings of the IEEE international conference on computer vi-
sion, pages 936–944, 2015.

[20] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.

[21] Giovanni Pintore, Valeria Garro, Fabio Ganovelli, Enrico
Gobbetti, and Marco Agus. Omnidirectional image capture
on mobile devices for fast automatic generation of 2.5 d in-
door maps.
In Applications of Computer Vision (WACV),
2016 IEEE Winter Conference on, pages 1–9. IEEE, 2016.

[22] Srikumar Ramalingam, Jaishanker K Pillai, Arpit Jain, and
Yuichi Taguchi. Manhattan junction catalogue for spatial
reasoning of indoor scenes. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3065–3072, 2013.

[23] Yuzhuo Ren, Shangwen Li, Chen Chen, and C-C Jay Kuo.
A coarse-to-ﬁne indoor layout estimation (cﬁle) method.
In Asian Conference on Computer Vision, pages 36–51.
Springer, 2016.

[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention, pages 234–241.
Springer, 2015.

[25] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent
neural networks. IEEE Transactions on Signal Processing,
45(11):2673–2681, 1997.

[26] Alexander G Schwing and Raquel Urtasun. Efﬁcient exact
inference for 3d indoor scene understanding.
In European
Conference on Computer Vision, pages 299–313. Springer,
2012.

[27] R Urtasun, M Pollefeys, T Hazan, and AG Schwing. Efﬁ-
cient structured prediction for 3d indoor scene understand-
ing. In 2012 IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 2815–2822. IEEE, 2012.

[28] Jiu Xu, Bj¨orn Stenger, Tommi Kerola, and Tony Tung.
Pano2cad: Room layout from a single panorama image. In
Applications of Computer Vision (WACV), 2017 IEEE Winter
Conference on, pages 354–362. IEEE, 2017.

1055

[29] Hao Yang and Hui Zhang. Efﬁcient 3d room shape recovery
from a single panorama. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
5422–5430, 2016.

[30] Shang-Ta Yang, Fu-En Wang, Chi-Han Peng, Peter Wonka,
Min Sun, and Hung-Kuo Chu. Dula-net: A dual-projection
network for estimating room layouts from a single rgb
panorama. arXiv preprint arXiv:1811.11977, 2018.

[31] Yang Yang, Shi Jin, Ruiyang Liu, Sing Bing Kang, and
Jingyi Yu. Automatic 3d indoor scene modeling from single
panorama. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 3926–3934,
2018.

[32] Yinda Zhang, Shuran Song, Ping Tan, and Jianxiong Xiao.
Panocontext: A whole-room 3d context model for panoramic
scene understanding. In European Conference on Computer
Vision, pages 668–686. Springer, 2014.

[33] Hao Zhao, Ming Lu, Anbang Yao, Yiwen Guo, Yurong
Chen, and Li Zhang. Physics inspired optimization on se-
mantic transfer features: An alternative method for room lay-
out estimation. arXiv preprint arXiv:1707.00383, 2017.

[34] Yibiao Zhao and Song-Chun Zhu. Scene parsing by integrat-
ing function, geometry and appearance models. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3119–3126, 2013.

[35] Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem.
Layoutnet: Reconstructing the 3d room layout from a sin-
gle rgb image.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2051–
2059, 2018.

1056

