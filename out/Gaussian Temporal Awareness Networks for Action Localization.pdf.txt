Gaussian Temporal Awareness Networks for Action Localization∗

Fuchen Long†, Ting Yao‡, Zhaofan Qiu†, Xinmei Tian†, Jiebo Luo§ and Tao Mei‡

†University of Science and Technology of China, Hefei, China

‡JD AI Research, Beijing, China

§University of Rochester, Rochester, NY USA

{longfc.ustc, tingyao.ustc, zhaofanqiu}@gmail.com; xinmei@ustc.edu.cn;

jluo@cs.rochester.edu; tmei@live.com

Abstract

Temporally localizing actions in a video is a fundamental
challenge in video understanding. Most existing approach-
es have often drawn inspiration from image object detection
and extended the advances, e.g., SSD and Faster R-CNN, to
produce temporal locations of an action in a 1D sequence.
Nevertheless, the results can suffer from robustness prob-
lem due to the design of predetermined temporal scales,
which overlooks the temporal structure of an action and
limits the utility on detecting actions with complex varia-
tions.
In this paper, we propose to address the problem
by introducing Gaussian kernels to dynamically optimize
temporal scale of each action proposal. Speciﬁcally, we
present Gaussian Temporal Awareness Networks (GTAN) —
a new architecture that novelly integrates the exploitation
of temporal structure into an one-stage action localization
framework. Technically, GTAN models the temporal struc-
ture through learning a set of Gaussian kernels, each for a
cell in the feature maps. Each Gaussian kernel corresponds
to a particular interval of an action proposal and a mixture
of Gaussian kernels could further characterize action pro-
posals with various length. Moreover, the values in each
Gaussian curve reﬂect the contextual contributions to the
localization of an action proposal. Extensive experiments
are conducted on both THUMOS14 and ActivityNet v1.3
datasets, and superior results are reported when comparing
to state-of-the-art approaches. More remarkably, GTAN
achieves 1.9% and 1.1% improvements in mAP on testing
set of the two datasets.

1. Introduction

With the tremendous increase of online and personal me-
dia archives, people are generating, storing and consuming
a large collection of videos. The trend encourages the devel-

∗This work was performed at JD AI Research.

Figure 1. The intuition of a typical one-stage action localization
(upper) and our GTAN (lower). The typical method ﬁxes temporal
scale in each feature map and seldom explores temporal structure
of an action. In contrast, temporal structure is taken into account
in our GTAN through learning a set of Gaussian kernels.

opment of effective and efﬁcient algorithms to intelligently
parse video data. One fundamental challenge that underlies
the success of these advances is action detection in videos
from both temporal aspect [6, 9, 17, 30, 39, 43] and spatio-
temporal aspect [11, 18]. In this work, the main focus is
temporal action detection/localization, which is to locate
the exact time stamps of the starting and the ending of an
action, and recognize the action with a set of categories.

One natural way of temporal action localization is to ex-
tend image object detection frameworks, e.g., SSD [23] or
Faster R-CNN [27], for producing spatial bounding boxes in
a 2D image to temporal localization of an action in a 1D se-
quence [4, 19]. The upper part of Figure 1 conceptualizes a
typical process of one-stage action localization. In general,
the frame-level or clip-level features in the video sequence
are ﬁrst aggregated into one feature map, and then multiple
1D temporal convolutional layers are devised to increase the

344

1D conv1D conv1D conv1D conv1D conv1D convPole vault 0.21Pole vault 0.32Pole vault 0.61Pole vault 0.83Pole vault 0.51Pole vault 0.31Pole vault 0.23Pole vault 0.45Pole vault 0.96Pole vault 0.33size of temporal receptive ﬁelds and predict action propos-
als. However, the temporal scale corresponding to the cell
in each feature map is ﬁxed, making such method unable
to capture the inherent temporal structure of an action. As
such, one ground-truth action proposal in the green box is
detected as three ones in this case. Instead, we propose to
alleviate the problem by exploring the temporal structure
of an action through learning a Gaussian kernel for each
cell, which dynamically indicates a particular interval of an
action proposal. A mixture of Gaussian kernels could even
be grouped to describe an action, which is more ﬂexible
to localize action proposals with various length as illustrat-
ed in the bottom part of Figure 1. More importantly, the
contextual information is naturally involved with the feature
pooling based on the weights in Gaussian curve.

By delving into temporal structure of an action, we
present a novel Gaussian Temporal Awareness Networks
(GTAN) architecture for one-stage action localization. Giv-
en a video, a 3D ConvNet is utilized as the backbone to
extract clip-level features, which are sequentially concate-
nated into a feature map. A couple of convolutional lay-
ers plus max-pooling layer are ﬁrstly employed to shorten
the feature map and increase the temporal size of recep-
tive ﬁelds. Then, a cascaded of 1D temporal convolutional
layers (anchor layers) continuously shorten the feature map
and output anchor feature map, which consists of features of
each cell (anchor). On the top of each anchor layer, a Gaus-
sian kernel is learnt for each cell to dynamically predict a
particular interval of an action proposal corresponding to
that cell. Multiple Gaussian kernels could even be mixed
to capture action proposals with arbitrary length. Through
Gaussian pooling, the features of each cell is upgraded by
aggregating the features of contextual cells weighted by
the values in the Gaussian curve for ﬁnal action proposal
prediction. The whole architecture is end-to-end optimized
by minimizing one classiﬁcation loss plus two regression
losses, i.e., localization loss and overlap loss.

The main contribution of this work is the design of an
one-stage architecture GTAN for addressing the issue of
temporal action localization in videos. The solution also
leads to the elegant view of how temporal structure of an ac-
tion should be leveraged for detecting actions with various
length and how contextual information should be utilized
for boosting temporal localization, which are problems not
yet fully understood in the literature.

2. Related Work

We brieﬂy group the related works into two categories:
temporal action proposal and temporal action detection.
The former focuses on investigating how to precisely local-
ize video segments which contain actions, while the latter
further classiﬁes these actions into known classes.

We summarize the approaches on temporal action pro-

posal mainly into two directions: content-independent pro-
posal and content-dependent proposal. The main stream
of content-independent proposal algorithms is uniformly
or sliding window-ly sampling in a video [24, 33, 41],
which leads to huge computations for further classiﬁca-
tion. In contrast, content-dependent proposal methods, e.g.,
[3, 5, 7, 8, 21], utilize the label of action proposals during
training. For instance, Escorcia et al. [5] leverage Long
Short-Term Memory cells to learn an appropriate encoding
of a video sequence as a set of discriminative states to in-
dicate proposal scores. Though the method avoids running
sliding windows of multiple scales, there is still the need of
executing an overlapping sliding window that is inapplica-
ble when the video duration is long. To address this prob-
lem, Single Stream Temporal proposal (SST) [3] generates
proposals with only one single pass by utilizing a recurrent
GRU-based model, and Temporal Unit Regression Network
(TURN) [8] builds video units in a pyramid manner to avoid
window overlapping. Different from the above method-
s which generate proposals in a ﬁxed multi-scale manner,
Boundary Sensitive Network (BSN) [21] localizes the ac-
tion boundaries based on three actionness curves in a more
ﬂexible way. Nevertheless, such actionness-based methods
may fail in locating dense and short actions because of the
difﬁculty to discriminate between very close starting and
ending peaks in the curve.

Once the localization of action proposals completes, the
natural way for temporal action detection is to further classi-
fy the proposals into known action classes, making the pro-
cess in two-stage manner [4, 12, 29, 30, 38, 43]. However,
the separate of proposal generation and classiﬁcation may
result in sub-optimal solutions. To further facilitate tempo-
ral action detection, there have been several one-stage tech-
niques [2, 19, 40] being proposed recently. For example,
Single Stream Temporal Action Detection (SS-TAD) [2] u-
tilizes the Recurrent Neural Network (RNN) based archi-
tecture to jointly learn action proposal and classiﬁcation.
Inspired by SSD [23], Lin et al. [19] devise 1D temporal
convolution to generate multiple temporal action anchors
for action proposal and detection. Moreover, with the devel-
opment of reinforcement learning, Yeung et al. [40] explore
RNN to learn a glimpse policy for predicting the starting
and ending points of actions in an end-to-end manner. N-
evertheless, most of one-stage methods are still facing the
challenge in localizing all the action proposals due to the
predetermined temporal scales.

In short, our approach belongs to one-stage temporal
action detection techniques. Different from the aforemen-
tioned one-stage methods which often predetermine tem-
poral scales of action proposals, our GTAN in this paper
contributes by studying not only learning temporal struc-
ture through Gaussian kernels, but also how the contextual
information can be better leveraged for action localization.

345

Figure 2. An overview of our Gaussian Temporal Awareness Networks (GTAN) architecture. The input video is encoded into a series
of clip-level features via a 3D ConvNet, which are sequentially concatenated as a feature map. Two 1D convolutional layers plus one
max-pooling layer are followed to increase the temporal size of receptive ﬁelds. Eight 1D convolutional layers are cascaded to generate
multiple feature maps in different temporal resolution. On the top of each feature map, a Gaussian kernel is learnt on each cell to predict
a particular interval of an action proposal. Moreover, multiple Gaussian kernels with high overlap are mixed to a larger one for detecting
long actions with various length. Through Gaussian pooling, the action proposal is generated by aggregating the features of contextual cells
weighted by the values in the Gaussian curve. The GTAN is jointly optimized with action classiﬁcation loss plus two regression losses,
i.e., localization loss and overlap loss for each proposal. Better viewed in original color pdf.

3. Gaussian Temporal Awareness Networks

In this section we present the proposed Gaussian Tempo-
ral Awareness Networks (GTAN) in detail. Figure 2 illus-
trates an overview of our architecture for action localization.
It consists of two main components: a base feature network
and a cascaded of 1D temporal convolutional layers with
Gaussian kernels. The base feature network is to extract
feature map from sequential video clips, which will be fed
into cascaded 1D convolutional layers to generate multiple
feature maps in different temporal resolution. For each cell
in one feature map, a Gaussian kernel is learnt to control
temporal scale of an action proposal corresponding to that
cell as training proceeds. Furthermore, a Gaussian Kernel
Grouping algorithm is devised to merge multiple Gaussian
kernels with high overlap to a larger one for capturing long
actions with arbitrary length. Speciﬁcally, each action pro-
posal is generated by aggregating the features of contextual
cells weighted by the values in the Gaussian curve. The
whole network is jointly optimized with action classiﬁca-
tion loss plus two regression losses, i.e., localization loss
and overlap loss, which are utilized to learn action catego-
ry label, default temporal boundary adjustment and overlap
conﬁdence score for each action proposal, respectively.

3.1. Base Feature Network

The ultimate target of action localization is to detect ac-
tion instances in temporal dimension. Given an input video,

we ﬁrst extract clip-level features from continuous clips via
a 3D ConvNet which could capture both appearance and
motion information of the video. Speciﬁcally, a sequence
of features {fi}T −1
i=0 are extracted from 3D ConvNet, where
T is the temporal length. We concatenate all the features
into one feature map and then feed the map into two 1D
convolutional layers (“conv1” and “conv2” with temporal
kernel size 3, stride 1) plus one max-pooling layer (“pool1”
with temporal kernel size 3, stride 2) to increase the tem-
poral size of receptive ﬁelds. The base feature network is
composed of 3D ConvNet, two 1D convolutional layers and
max-pooling layer. The outputs of the base feature network
are further exploited for action proposal generation.

3.2. Gaussian Kernel Learning

Given the feature map output from the base feature net-
work, a natural way for one-stage action localization is to
stack 1D temporal convolutional layers (anchor layers) to
generate proposals (anchors) for classiﬁcation and bound-
ary regression. This kind of structure with predetermined
temporal scale in each anchor layer can capture action pro-
posals whose temporal intervals are well aligned with the
size of receptive ﬁelds, however, posts difﬁculty to the de-
tection of proposals with various length. The design limits
the utility on localizing actions with complex variations.

To address this issue, we introduce temporal Gaussian
kernel to dynamically control the temporal scales of propos-
als in each feature map. In particular, as shown in Figure

346

......pool1conv1...conv2Gaussian Pooling...conv_v1conv_a1...Gaussian Pooling...conv_v2conv_a2...Gaussian Poolingconv_v8conv_a8σ1 1 ...Localization LossOverlap LossClassification Loss.........σ2 1 σ3 1 σ4 1 σ5 1 σ6 1 σ7 1 σ8 1 σ1 n-2σ1 n-1σ1 n...σ1 2 σ2 2 σ3 2 σ4 2 σ52 σ6 2 2 m-2σ2 m-1σ2 mσσ1 8 σ2 8 σ3 8 σ3 2 σ3 1 3D CNN3D CNN3D CNN3D CNN......σ3 1 2σ' 1 σ' 1 2......σ' 2 σ' 2 2......σ1 8 2σ2 8 2......Base Feature NetworkGaussian KernelOptimizerProposalσ2 8 F31 F18 F28 Gaussian KernelGaussian Kernel128F1' F2' Figure 3. Visualization of Gaussian Kernel Grouping.

2, eight 1D temporal convolutional layers (anchor layers)
are ﬁrst cascaded for action proposal generation in different
temporal resolution. For each cell in the feature map of the
anchor layer, a Gaussian kernel is learnt to predict a particu-
lar interval of an action proposal corresponding to that cell.
Formally, we denote the feature map of j-th convolutional
layer as {fi}T j −1
, 1 ≤ j ≤ 8, where T j and
Dj are the temporal length and feature dimension of the
feature map. For a proposal P j
t whose center location is t,
we leverage its temporal scale by a Gaussian kernel Gj
t . The
standard deviation σj
t is learnt via a 1D convolutional
layer on a 3 × Dj feature map cell, and the value is con-
strained within the range (0, 1) through a sigmoid operation.
The weights of the Gaussian kernel Gj

i=0 ∈ RT j ×Dj

t of Gj

t are deﬁned as

(pi − µt)2

W j

t [i] =

exp(−

1
Z

s.t. pi =

2

2σj
t
t
T j ,
i ∈ {0, 1, ..., T j − 1},

i
T j , µt =

) ,

(1)

t ∈ {0, 1, ..., T j − 1},

where Z is the normalizing constant. Taking the spirit from
the theory that the σj
t could be considered as a measure of
width (Root Mean Square width, RMS) in Gaussian kernel
Gj
t , we utilize σj
t as the interval measure of action proposal
P j
t . Speciﬁcally, the σj
t can be multiplied with a certain
ratio to represent the default temporal boundary:

ac = (t + 0.5)/T j,

aw = rd · 2σj

t /T j,

(2)

where ac and aw are the center location and width of de-
fault temporal boundary and rd represents temporal scale
ratio. The W j
t is also utilized for feature aggregation with
a pooling mechanism to generate action proposals, which
will be elaborated in Section 3.4.

Compared to the conventional 1D convolutional anchor
layer which ﬁxes the temporal scale as 1/T j in j-th layer,
ours employs the dynamic temporal scales by leveraging
the learned Gaussian kernel of each proposal to explore the
action instances with complex variations.

3.3. Gaussian Kernel Grouping

Through learning temporal Gaussian kernels, the tempo-
ral scales of most action instances can be characterized with

Algorithm 1 Gaussian Kernel Grouping
Input:

Original Gaussian kernel set S = {G(ti, σi)}T −1
i=0 ;
Intersection over Union (IoU) threshold ε;

Output:

Mixed Gaussian kernel set G;

1: Choose the beginning grouping position p = 0;
2: Initialize mixed Gaussian kernel set G = ∅;
3: Initialize base Gaussian kernel Gbs = G(tp, σp), the ending grouping

position z = p + 1;
4: while p ≤ T − 1 do
5:
6:
7:

Compute IoU value O between kernel Gbs and G(tz, σz);
if O > ε then

Group Gbs and G(tz, σz) to G′ according to Eq.(3), replace Gbs
with the new mixed kernel G′;

Add kernel Gbs to mixed kernel set G;
p = z, Gbs = G(tp, σp);

else

8:
9:
10:
11:
12:
13: end while
14: return G

end if
z = z + 1;

the predicted standard deviation. However, if the learned
Gaussian kernels span and overlap with each other, that may
implicitly indicate a long action centered at a ﬂexible posi-
tion among these Gaussian kernels. In other words, utilizing
the center locations of these original Gaussian kernels to
represent this long proposal may not be appropriate. To
alleviate this issue, we attempt to generate a set of new
Gaussian kernels to predict center location and temporal
scales of proposals for long action. Inspired by the idea of
temporal actionness grouping in [43], we propose a novel
Gaussian Kernel Grouping algorithm for this target.

Figure 3 illustrates the process of temporal Gaussian
Kernel Grouping. Given two adjacent Gaussian kernels
G(t1, σ1) and G(t2, σ2) whose center location and standard
deviation are t and σ, we compute the temporal intersection
and union between two kernels by using the width aw of
the default temporal boundary deﬁned in Section 3.2.
In
upper part of Figure 3, the length of temporal intersection
between two kernels is H, while the length of union is L. If
the Intersection over Union (IoU) between the two kernels
H/L exceeds a certain threshold ε, we merge them into one
Gaussian kernel (bottom part of Figure 3). The new mixed
Gaussian kernel is formulated as follows

(pi − µ′)2

W [i] =

1
Z

exp(−

s.t. pi =

i
T

, µ′ =

2σ′ 2
t1 + t2
2 · T

) ,

, σ′ =

L
2

,

(3)

i ∈ {0, 1, ..., T − 1}.

In each feature map, Algorithm 1 details the grouping

steps to generate merged kernels.

3.4. Gaussian Pooling

With the learned and mixed Gaussian kernels, we cal-
culate the weighted sum of the feature map based on the
values in Gaussian curve and obtain the aggregated feature
F . Speciﬁcally, given the weighting coefﬁcients W j
t of

347

σ' σ1 σ2 Kernel GroupingL HWidth of Width of Intersection of G(t1,σ1)G(t2,σ2)G(t1,σ1)andG(t2,σ2)Union ofG(t1,σ1)andG(t2,σ2)Figure 4. Comparisons of manual extension plus average-pooling
strategy (left) and Gaussian pooling strategy (right) for involving
temporal contextual information of action proposals.
Gaussian kernel Gj
aggregated feature for proposal P j
T j XT j −1

t at center location t in j-th layer, the

t is formulated as

t [i] · fi,

t =

W j

F j

(4)

i=0

1

where the representation F j
tion classiﬁcation and temporal boundary regression.

t is further exploited for the ac-

The above Gaussian pooling mechanism inherently takes
the contextual contributions around each action proposal
into account.
In contrast to the manual extension plus
average-pooling strategy to capture video context informa-
tion (left part of Figure 4), ours provides an elegant alter-
native to adaptively learn the weighted representation (right
part of Figure 4) based on the importance.

3.5. Network Optimization

Given the representation of each proposal from Gaussian
pooling, three 1D convolutional layers are utilized in paral-
lel to predict action classiﬁcation scores, localization pa-
rameters and overlap parameter, respectively. Action clas-
C] indicate the probabil-
siﬁcation scores y
ities belonging to C action classes plus one “background”
class. Localization parameters (∆c, ∆w) denote temporal
offsets relative to default center location ac and width aw,
which are leveraged to adjust the temporal coordinate

1 , ..., ya

a = [ya

0 , ya

ϕc = ac + α1aw∆c and ϕw = aw exp (α2∆w) ,

(5)

where ϕc, ϕw are reﬁned center location and width of the
proposal. The α1, α2 are utilized to control the impact of
temporal offsets. In particular, we deﬁne an overlap param-
eter yov to represent the precise IoU prediction of the pro-
posal, which beneﬁts the proposal re-ranking in prediction.
In the training stage, we accumulate all the proposals
from Gaussian pooling and produce the action instances
through prediction layer. The overall training objective in
our GTAN is formulated as a multi-task loss by integrating
action classiﬁcation loss (Lcls) and two regression losses,
i.e., localization loss (Lloc) and overlap loss (Lov):

L = Lcls + βLloc + γLov,

(6)

where β and γ are the trade-off parameters. Speciﬁcally, we
measure the classiﬁcation loss Lcls via the softmax loss:

Lcls = −

C

X

n=0

In=c log(ya

n),

(7)

where indicator function In=c = 1 if n equals to ground
truth action label c, otherwise In=c = 0. We denote giou as
the IoU between default temporal boundary of this proposal
and its corresponding closest ground truth. If the giou of this
proposal is larger than 0.8, we set it as a foreground sample.
If giou is lower than 0.3, it will be set as background sample.
The ratio between foreground and background samples is
set as 1.0 during training. The localization loss is devised
as Smooth L1 loss [10] (SL1) between the predicted fore-
ground proposal and the closest ground truth instance of the
proposal, which is computed by

Lloc = SL1(ϕc − gc) + SL1(ϕw − gw),

(8)

where gc and gw represents the center location and width
of the proposal’s closest ground truth instance, respectively.
For overlap loss, we adopt the mean square error (MSE)
loss to optimize it as follows:

Lov = (yov − giou)2.

(9)

Eventually, the whole network is trained in an end-to-end

manner by penalizing the three losses.

3.6. Prediction and Post processing

During prediction of action localization, the ﬁnal ranking
score yf of each candidate action proposal depends on both
action classiﬁcation scores y

a and overlap parameter yov:

yf = max(y

a) · yov.

(10)

Given the predicted action instance φ = {ϕc, ϕw, Ca, yf }
with reﬁned boundary (ϕc, ϕw), predicted action label Ca,
and ranking score yf , we employ the soft non-maximum
suppression (soft-NMS) [1] for post-processing.
In each
iteration of soft-NMS, we represent the action instance with
the maximum ranking score yfm as φm. The ranking score
yfk of other instance φk will be decreased or not, according
to the IoU computed with φm:

y′

fk = 


yfk

yfk · e−

iou(φk ,φm )2

ξ

,

,

if iou(φk, φm) < ρ

if iou(φk, φm) ≥ ρ

, (11)

where ξ is the decay parameter and ρ is the NMS threshold.

4. Experiments

We empirically verify the merit of our GTAN by con-
ducting the experiments of temporal action localization on
two popular video recognition benchmarks, i.e., Activi-
tyNet v1.3 [13] and THUMOS14 [16].

4.1. Datasets

The ActivityNet v1.3 dataset contains 19,994 videos in
200 classes collected from YouTube. The dataset is divided
into three disjoint subsets: training, validation and testing,

348

d/2Average Poolingdd/2d/2Table 1. The details of 1D temporal convolutional (anchor) layers.
RF represents the size of receptive ﬁelds.

id
1
2
3
4
5
6
7
8

type

kernel size

#channels

#stride

conv_a1
conv_a2
conv_a3
conv_a4
conv_a5
conv_a6
conv_a7
conv_a8

3
3
3
3
3
3
3
3

512
512
1024
1024
2048
2048
4096
4096

2
2
2
2
2
2
2
2

RF
11
19
35
67
131
259
515
1027

by 2:1:1. All the videos in the dataset have temporal anno-
tations. The labels of testing set are not publicly available
and the performances of action localization on ActivityNet
dataset are reported on validation set. The THUMOS14
dataset has 1,010 videos for validation and 1,574 videos for
testing from 20 classes. Among all the videos, there are 220
and 212 videos with temporal annotations in validation and
testing set, respectively. Following [43], we train the model
on validation set and perform evaluation on testing set.

4.2. Experimental Settings

Implementations. We utilize Pseudo-3D [26] network
as our 3D backbone. The network input is a 16-frame
clip and the sample rate of frames is set as 8. The 2,048-
way outputs from pool5 layer are extracted as clip-level
features. Table 1 summarizes the structures of 1D anchor
layers. Moreover, we choose three temporal scale ratios
{rd}3
d=1 = [20, 21/3, 22/3] derived from [22]. The IoU
threshold ε in Gaussian grouping is set as 0.7 by cross val-
idation. The balancing parameters β and γ are also deter-
mined on a validation set and set as 2.0 and 75. ξ and ρ are
set as 0.8 and 0.75 in soft-NMS. The parameter α1 and α2
are all set as 1.0 by cross validation. We implement GTAN
on Caffe [15] platform. In all the experiments, our networks
are trained by utilizing stochastic gradient descent (SGD)
with 0.9 momentum. The initial learning rate is set as 0.001,
and decreased by 10% after every 2.5k iterations on THU-
MOS14 and 10k iterations on ActivityNet. The mini-batch
size is 16 and the weight decay parameter is 0.0001.

Evaluation Metrics. We follow the ofﬁcial evaluation
metrics in each dataset for action detection task. On Activ-
ityNet v1.3, the mean average precision (mAP) values with
IoU thresholds between 0.5 and 0.95 (inclusive) with a step
size 0.05 are exploited for comparison. On THUMOS14,
the mAP with IoU threshold 0.5 is measured. We evaluate
performances on top-100 and top-200 returned proposals in
ActivityNet v1.3 and THUMOS14, respectively.

4.3. Evaluation on Temporal Action Proposal

We ﬁrst examine the performances on temporal action
proposal task, which is to only assess the boundary qual-
ity of action proposals, regardless of action classes. We
compare the following advanced approaches: (1) Structure
Segment Network (SSN) [43] generates action proposals

(a)

(b)

Figure 5. (a) Recall-IoU and (b) AR-AN curve on ActivityNet.

Table 2. AR and AUC values on action proposal. IoU threshold:
[0.5:0.05:1.0] for THUMOS14, [0.5:0.05:0.95] for ActivityNet.
THUMOS14 ActivityNet ActivityNet (test server)

Approach

SST [3]
CTAP [7]
BSN [21]

GTAN

AR
37.9
50.1
53.2
54.3

AR AUC

-

-

73.2 65.7
74.2 66.2
74.8 67.1

AUC

-
-

66.3
67.4

by temporal actionness grouping. (2) Single Shot Action
Detection (SSAD) [19] is the 1D variant version of Sin-
gle Shot Detection [23], which generates action propos-
als by multiple temporal anchor layers.
(3) Convolution-
De-Convolution Network (CDC) [29] builds a 3D Conv-
Deconv network to precisely localize the boundary of action
instances at frame level. (4) Boundary Sensitive Network
(BSN) [21] locates temporal boundaries with three action-
ness curves and reranks proposals with neural networks. (5)
Single Stream Temporal action proposal (SST) [3] build-
s a RNN-based action proposal network, which could be
implemented in a single stream over long video sequences
to produce action proposals. (6) Complementary Tempo-
ral Action Proposal (CTAP) [7] balances the advantages
and disadvantages between sliding window and actionness
grouping approaches for ﬁnal action proposal.

We adopt the standard metric of Average Recall in differ-
ent IoU (AR) for action proposal on both datasets. More-
over, following the ofﬁcial evaluations in ActivityNet, we
plot both Recall-IoU curve and Average Recall vs. Average
Number of proposals per video (AR-AN) curve in Figure
5. In addition to AR metric, the area under AR-AN curve
(AUC) is also reported in Table 2 as AUC is the measure on
test server of ActivityNet. Overall, the performances across
different metrics and two datasets consistently indicate that
our GTAN leads to performance boost against baselines.
In particular, AR of GTAN achieves 54.3% and 74.8% on
THUMOS14 and ActivityNet respectively, making the ab-
solute improvement over the best competitor BSN by 1.1%
and 0.6%. GTAN surpasses BSN by 1.1% in AUC when
evaluating on online ActivityNet test server. The results
demonstrate the advantages of exploiting temporal structure
for localizing actions. Furthermore, as shown in Figure 5,
the improvements are constantly attained across different
IoU. In terms of AR-AN curve, GTAN also exhibits better

349

0.00.20.40.60.81.0IoU0.00.20.40.60.81.0RecallSSNCTAPSSADBSNCDCGTANSST20406080100AverageNumberofProposalsperVideo0.00.10.20.30.40.50.60.70.8AverageRecallSSNCTAPSSADBSNCDCGTANSSTFigure 6. Visualization of action localization on a video example from ActivityNet by GTAN. The Gaussian kernels are learnt on the outputs
of “conv_a5” layer. The second and third kernels are mixed into a larger one. The default boxes (DB) are predicted by Gaussian kernels.

Table 3. Performance contribution of each design in GTAN.

Approach
Fixed Scale

Gaussian Kernel

Gaussian Grouping

THUMOS14

X

ActivityNet v1.3
X

X

X

X

X

X

X

mAP

33.5

37.1

38.2

29.8

31.6

34.3

Table 4. The evaluations of Gaussian grouping on actions with
different lengths. GTAN− excludes Gaussian grouping in GTAN.

Approach

GTAN−
GTAN

THUMOS14
All
37.1
38.2

≥ 128
22.1
25.9

ActivityNet v1.3
≥ 2048
All
31.6
34.3

49.4
54.2

performance on different number of top returned proposal-
s. Even in the case when only less than 10 proposals are
returned, GTAN still shows apparent improvements, indi-
cating that GTAN is beneﬁted from the mechanism of dy-
namically optimizing temporal scale of each proposal and
the correct proposals are ranked at the top.

4.4. Evaluation on Gaussian Kernel and Grouping

Next, we study how each design in GTAN inﬂuences the
overall performance on temporal action localization task.
Fixed Scale simply employs a ﬁxed temporal interval for
each cell or anchor in an anchor layer and such way is
adopted in SSAD. Gaussian Kernel leverages the idea of
learning one Gaussian kernel for each anchor to model tem-
poral structure of an action and dynamically predict tempo-
ral scale of each action proposal. Gaussian Grouping further
mixes multiple Gaussian kernels to characterize action pro-
posals with various length. In the latter two cases, Gaussian
pooling is utilized to augment the features of each anchor
with contextual information.

Table 3 details the mAP performances by considering
one more factor in GTAN on both datasets. Gaussian Kernel
successfully boosts up the mAP performance from 33.5%
to 37.1% and from 29.8% to 31.6% on THUMOS14 and
ActivityNet v1.3, respectively. This somewhat reveals the
weakness of Fixed Scale, where the temporal scale of each
anchor is independent of temporal property of the action
proposal. Gaussian Kernel, in comparison, models tem-

(a)

(b)

Figure 7. (a) AUC and (b) Average mAP performances of SSAD
and GTAN with different number of anchor layers on temporal
action proposal and localization tasks in ActivityNet.

poral structure and predicts a particular interval for each
anchor on the ﬂy. As such, the temporal localization or
boundary of each action proposal is more accurate. More-
over, the features of each action proposal are simultaneous-
ly enhanced by contextual aggregation through Gaussian
pooling and lead to better action classiﬁcation. Gaussian
grouping further contributes a mAP increase of 1.1% and
2.7%, respectively. The results verify the effectiveness and
ﬂexibility of mixing multiple Gaussian kernels to capture
action proposals with arbitrary length. To better validate
the impact of Gaussian grouping, we additionally evaluate
GTAN on long action proposals. Here, we consider actions
longer than 128 frames in THUMOS14 and 2048 frames in
ActivityNet v1.3 as long actions, since the average duration
of action instances in THUMOS14 is ∼4 seconds which
is much smaller than that (∼50 seconds) of ActivityNet.
Table 4 shows the mAP comparisons between GTAN and
its variant GTAN− which excludes Gaussian grouping. As
expected, larger degree of improvement is attained on long
action proposals by involving Gaussian grouping.

4.5. Evaluation on the Number of Anchor Layers

In existing one-stage methods, e.g., SSAD, temporal s-
cale is ﬁxed in each anchor layer and the expansion of
multiple temporal scales is implemented through increasing
the number of anchor layers.
Instead, our GTAN learns

350

20.5s30.5s32.5s69.7s74.0s80.0s82.3s90.4s20.3s29.8s32.1s70.5s73.6s80.7s83.1s91.2sGTANGTAN (DB)19.3s28.9s31.9s67.2s75.6s78.3s83.5s87.1sGroupconv_a5Horseback ridingBackgroundTimeGroundTruth 45678NumberofAnchorLayers203040506070AUCGTANSSAD45678NumberofAnchorLayers510152025303540AveragemAPGTANSSADTable 5. Performance comparisons of temporal action detection on
THUMOS14, measured by mAP at different IoU thresholds α.

Table 6. Comparisons of temporal action detection on ActivityNet.

ActivityNet v1.3, mAP

Approach

THUMOS14, mAP@α
0.3

0.1

0.2

0.4

0.5

Two-stage Action Localization

Wang et.al. [35]

18.2

17.0

14.0

11.7

FTP [14]
DAP [5]

Oneata et.al. [25]
Yuan et.al. [41]

S-CNN [30]

SST [3]
CDC [29]
TURN [8]
R-C3D [38]

SSN [43]
CTAP [7]
BSN [21]

-
-

36.6
51.4
47.7

-
-

54.0
54.5
66.0

-
-

-
-

33.6
42.6
43.5

-
-

50.9
51.5
59.4

-
-

-
-

27.0
33.6
36.3
37.8
40.1
44.1
44.8
51.9

-

53.5

One-stage Action Localization

Richard et.al. [28]
Yeung et.al. [40]

SMS [42]
SSAD [19]
SS-TAD [2]

GTAN (C3D)

GTAN

39.7
48.9
51.0
50.1

-

67.2
69.1

35.7
44.0
45.2
47.8

-

61.1
63.7

30.0
36.0
36.5
43.0
45.7

56.9
57.8

-
-

20.8
26.1
28.7

-

29.4
34.9
35.6
41.0

-

45.0

23.2
26.4
27.8
35.0

-

46.5
47.2

8.3
13.5
13.9
14.4
18.8
19.0
23.0
23.3
25.6
28.9
29.8
29.9
36.9

15.2
17.1
17.8
24.6
29.2

37.9
38.8

one Gaussian kernel for each anchor in every anchor lay-
er and dynamically predicts temporal scale of the action
proposal corresponding to each anchor. The grouping of
multiple Gaussian kernels makes the temporal scale more
ﬂexible. Even with a small number of anchor layers, our
GTAN should be more responsible to localize action pro-
posals with various length in theory. Figure 7 empirically
compares the performances between SSAD and our GTAN
on ActivityNet v1.3 when capitalizing on different number
of anchor layers. As indicated by the results, GTAN consis-
tently outperforms SSAD across different depths of anchor
layers from 4 to 8 on both temporal action proposal and
localization tasks. In general, more anchor layers provide
better AUC and mAP performances. It is expected that the
performance of SSAD decreases more sharply than that of
GTAN when reducing the number of anchor layers. In the
extreme case of 4 layers, GTAN still achieves 26.77% in
average mAP while SSAD only reaches 5.12%, which again
conﬁrms the advantage of exploring temporal structure and
predicting temporal scale of action proposals.

4.6. Comparisons with State of the Art

We compare with several state-of-the-art techniques on
THUMOS14 and ActivityNet v1.3 datasets. Table 5 list-
s the mAP performances with different IoU thresholds on
THUMOS14. For fair comparison, we additionally imple-
ment GTAN using C3D [34] as 3D ConvNet backbone. The
results across different IoU values consistently indicate that
GTAN exhibits better performance than others. In particu-
lar, the mAP@0.5 of GTAN achieve 37.9% with C3D back-
bone, making the improvements over one-stage approaches
SSAD and SS-TAD by 13.3% and 8.7%, which also employ
C3D. Compared to the most advanced two-stage method B-

validation

testing
0.95 Average Average

Approach

Wang et.al. [36]
Singh et.al. [31]
Singh et.al. [32]

CDC [29]

TAG-D [37]

SSN [43]

Lin et.al. [20]

BSN [21]

0.5

45.11
26.01
22.71
45.30
39.12

-

48.99
52.50

0.75

4.11
15.22
10.82
26.00
23.48

-

32.91
33.53

0.05
2.61
0.33
0.20
5.49

-

7.87
8.85

GTAN

52.61

34.14

8.91

16.41
14.62
11.31
23.80
23.98

-

32.26
33.72

34.31

14.62
17.68
17.83
22.90
26.05
28.28
33.40
34.42

35.54

SN, our GTAN leads to 1.0% and 1.9% performance gains
with C3D and P3D backbone, respectively. The superior
results of GTAN demonstrate the advantages of modeling
temporal structure of actions through Gaussian kernel.

On ActivityNet v1.3, we summarize the performance
comparisons on both validation and testing set in Table 6.
For testing set, we submitted the results of GTAN to online
ActivityNet test server and evaluated the performance on
the localization task. Similarly, GTAN surpasses the best
competitor BSN by 0.6% and 1.1% on validation and testing
set, respectively. Moreover, our one-stage GTAN is poten-
tially simpler and faster than two-stage solutions, and tends
to be more applicable to action localization in videos.

Figure 6 showcases temporal localization results of one
video from ActivityNet. The Gaussian kernels and grouping
learnt on the outputs of “conv_a5” layer are also visualized.
As shown in the Figure, Gaussian kernels nicely capture the
temporal structure of each action proposal and predict accu-
rate default boxes for the ﬁnal regression and classiﬁcation.

5. Conclusions

We have presented Gaussian Temporal Awareness Net-
works (GTAN) which aim to explore temporal structure of
actions for temporal action localization. Particularly, we
study the problem of modeling temporal structure through
learning a set of Gaussian kernels to dynamically predict
temporal scale of each action proposal. To verify our claim,
we have devised an one-stage action localization framework
which measures one Gaussian kernel for each cell in ev-
ery anchor layer. Multiple Gaussian kernels could be even
mixed for the purpose of representing action proposals with
various length. Another advantage of using Gaussian kernel
is to enhance features of action proposals by leveraging con-
textual information through Gaussian pooling, which bene-
ﬁts the ﬁnal regression and classiﬁcation. Experiments con-
ducted on two video datasets, i.e., THUMOS14 and Activi-
tyNet v1.3, validate our proposal and analysis. Performance
improvements are also observed when comparing to both
one-stage and two-stage advanced techniques.

Acknowledgments This work was supported in part by
the National Key R&D Program of China under contract
No. 2017YFB1002203 and NSFC No. 61872329.

351

References

[1] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Lar-
ry S. Davis. Soft-NMS – Improving Object Detection With
One Line of Code. In ICCV, 2017.

[2] Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-
Fei, and Juan Carlos Niebles. End-to-End, Single-Stream
Temporal Action Detection in Untrimmed Videos. In BMVC,
2017.

[3] Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard
Ghanem, and Juan Carlos Niebles. SST: Single-Stream Tem-
poral Action Proposals. In CVPR, 2017.

[4] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Sey-
bold, David A. Ross, Jia Deng, and Rahul Sukthankar. Re-
thinking the Faster R-CNN Architecture for Temporal Ac-
tion Localization. In CVPR, 2018.

[5] Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles,
and Bernard Ghanem. DAPs: Deep Action Proposals for
Action Understanding. In ECCV, 2016.

[6] Adrien Gaidon, Zaid Harchaoui, and Cordelia Schmid. Tem-
poral Localization of Actions with Actoms. IEEE Trans. on
PAMI, 35(11):2782–2795, 2013.

[7] Jiyang Gao, Kan Chen, and Ram Nevatia. CFAP: Comple-
In ECCV,

mentary Temporal Action Proposal Generation.
2018.

[8] Jiyang Gao, Zhenheng Yang, Chen Sun, Kan Chen, and Ram
Nevatia. TURN TAP: Temporal Unit Regression Network
for Temporal Action Proposals. In ICCV, 2017.

[9] Roeland De Geest, Efstratios Gavves, Amir Ghodrati,
Zhenyang Li, Cees Snoek, and Tinne Tuytelaars. Online
Action Detection. In ECCV, 2016.

[10] Ross Girshick. Fast R-CNN. In ICCV, 2015.
[11] Georgia Gkioxari and Jitendra Malik. Finding Action Tubes.

In CVPR, 2015.

[12] Fabian Caba Heilbron, Wayner Barrios, Victor Escorica, and
Bernard Ghanem. SCC: Semantic Context Cascade for Efﬁ-
cient Action Detection. In CVPR, 2017.

[13] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. ActivityNet: A Large-Scale Video
Benchmark for Human Activity Understanding.
In CVPR,
2015.

[14] Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard
Ghanem. Fast Temporal Activity Proposals for Efﬁcient De-
tection of Human Actions in Untrimmed Videos. In CVPR,
2016.

[15] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross B. Girshick, Sergio Guadar-
rama, and Trevor Darrell. Caffe: Convolutional Archi-
tecture for Fast Feature Embedding. arXiv preprint arX-
iv:1408.5093, 2014.

[16] Yu-Gang Jiang, Jingen Liu, Amir R.Zamir, and George
Toderici. THUMOS challenge: Action recognition with
a large number of classes. http://crcv.ucf.edu/
THUMOS14, 2014.

[17] Colin Lea, Rene Vidal Michael D. Flynn, Austin Reiter, and
Gregory D. Hager. Temporal Convolutional Netowrk for Ac-
tion Segmentation and Detection. In CVPR, 2017.

[18] Dong Li, Zhaofan Qiu, Qi Dai, Ting Yao, and Tao Mei.
Recurrent Tubelet Proposal and Recognition Networks for
Action Detection. In ECCV, 2018.

[19] Tianwei Lin, Xu Zhao, and Zheng Shou. Single Shot Tem-

poral Action Detection. In ACM MM, 2017.

[20] Tianwei Lin, Xu Zhao, and Zheng Shou. Temporal convolu-
tion based action proposal: Submission to activitynet 2017.
arXiv preprint arXiv:1707.06750, 2017.

[21] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and
Ming Yang. BSN: Boundary Sensitive Network for Temporal
Action Proposal Generation. In ECCV, 2018.

[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
In

Piotr Dollar. Focal Loss for Dense Object Detection.
ICCV, 2017.

[23] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.
Berg. SSD: Single Shot MultiBox Detector. In ECCV, 2016.

[24] Dan Oneata, Jakob Verbeek, and Cordelia Schmid. Action
and Event Recognition with Fisher Vectors on a Compact
Feature Set. In ICCV, 2013.

[25] Dan Oneata, Jakob Verbeek, and Cordelia Schmid. The
In ECCV THUMOS

LEAR submission at Thumos 2014.
Challenge Workshop, 2014.

[26] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning Spatio-
Temporal Representation with Pseudo-3D Residual Net-
works. In ICCV, 2017.

[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards Real-Time Object Detection with
Region Proposal Networks. In NIPS, 2015.

[28] Alexander Richard and Juergen Gall. Temporal Action De-
tection using a Statistical Language Model. In CVPR, 2016.

[29] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki
Miyazawa, and Shih-Fu Chang. CDC: Convolutional-De-
Convolutional Network for Precise Temporal Action Local-
ization in Untrimmed Videos. In CVPR, 2017.

[30] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal
Action Localization in Untrimmed Videos via Multi-stage
CNNs. In CVPR, 2016.

[31] Bharat Singh, Tim K. Marks, Michael Jones, Oncel Tuzel,
and Ming Shao. A Multi-Stream Bi-Directional Recurren-
t Neural Network for Fine-Grained Action Detection.
In
CVPR, 2016.

[32] Gurkirt Singh and Fabio Cuzzolin. Untrimmed Video Clas-
siﬁcation for Activity Detection: submission to ActivityNet
Challenge. arXiv preprint arXiv:1607.01979, 2016.

[33] Kevin Tang, Bangpeng Yao, Li Fei-Fei, and Daphne Koller.
Combining the Right Features for Complex Event Recogni-
tion. In ICCV, 2013.

[34] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning Spatiotemporal Features with
3D Convolutional Networks. In ICCV, 2015.

[35] Liming Wang, Yu Qiao, and Xiaoou Tang. Action Recog-
nition and Detection by Combining Motion and Apperance
Feature. In ECCV THUMOS Challenge Workshop, 2014.

[36] Ruxing Wang and Dacheng Tao. UTS at activitynet 2016. In

CVPR ActivityNet Challenge Workshop, 2016.

352

[37] Yuanjun Xiong, Yue Zhao, Limin Wang, Dahua Lin, and
Xiaoou Tang. A Pursuit of Temporal Accuracy in General
Activity Detection. arXiv preprint arXiv:1703.02716, 2017.
[38] Huijuan Xu, Abir Das, and Kate Saenko. R-C3D: Region
Convolutional 3D Network for Temporal Activity Detection.
In ICCV, 2017.

[39] Ting Yao, Yehao Li, Zhaofan Qiu, Fuchen Long, Yingwei
Pan, Dong Li, and Tao Mei. MSR Asia MSM at ActivityNet
Challenge 2017: Trimmed Action Recognition, Temporal
Action Proposals and Dense-Captioning Events in Videos.
In CVPR ActivityNet Challenge Workshop, 2017.

[40] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-
Fei. End-to-end Learning of Action Detection from Frame
Glimpses in Videos. In CVPR, 2016.

[41] Jun Yuan, Bingbing Ni, Xiaokang Yang, and Ashraf
A.Kassim. Temporal Action Localization With Pyramid of
Score Distribution Features. In CVPR, 2016.

[42] Zehuan Yuan, Jonathan C. Stroud, Tong Lu, and Jia Deng.
Temporal Action Localization by Structured Maximal Sums.
In CVPR, 2017.

[43] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xi-
aoou Tang, and Dahua Lin. Temporal Action Detection with
Structured Segment Networks. In ICCV, 2017.

353

