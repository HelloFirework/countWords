2337

2338

2339

2340

Label

Ground Truth

CRN [8]

pix2pixHD [45]

Ours

Figure 5: Visual comparison of semantic image synthesis results on the COCO-Stuff dataset. Our method successfully
synthesizes realistic details from semantic labels.

Label

Ground Truth

CRN [8]

SIMS [40]

pix2pixHD [45]

Ours

Figure 6: Visual comparison of semantic image synthesis results on the ADE20K outdoor and Cityscapes datasets. Our
method produces realistic images while respecting the spatial semantic layout at the same time.

COCO-Stuff

ADE20K

ADE20K-outdoor

Cityscapes

Method
CRN [8]
SIMS [40]
pix2pixHD [45]
Ours

mIoU accu
40.4
23.7
N/A
N/A
14.6
45.8
67.9
37.4

FID
70.4
N/A
111.5
22.6

mIoU accu
68.8
22.4
N/A
N/A
20.3
69.2
79.9
38.5

FID mIoU accu
68.6
73.3
74.7
N/A
81.8
71.6
82.9
33.9

16.5
13.1
17.4
30.8

FID mIoU accu
77.1
99.0
75.5
67.7
97.8
81.4
81.9
63.3

52.4
47.2
58.3
62.3

FID
104.7
49.7
95.0
71.8

Table 1: Our method outperforms current leading methods in semantic segmentation scores (mean IoU and overall pixel
accuracy) and FID [17] on all the benchmark datasets. For mIoU and pixel accuracy, higher is better. For FID, lower is better.

the distance between the distributions of synthesized results
and the distribution of real images.

Baselines. We compare our method with three leading se-
mantic image synthesis models: the pix2pixHD model [45],
the cascaded reﬁnement network model (CRN) [8], and
the semi-parametric image synthesis model (SIMS) [40].
pix2pixHD is the current state-of-the-art GAN-based con-
ditional image synthesis framework. CRN uses a deep net-
work that repeatedly reﬁnes the output from low to high res-
olution, while the SIMS takes a semi-parametric approach

that composites real segments from a training set and reﬁnes
the boundaries. Both the CRN and SIMS are mainly trained
using image reconstruction loss. For a fair comparison, we
train the CRN and pix2pixHD models using the implemen-
tations provided by the authors. As synthesizing an image
using SIMS requires many queries to the training dataset,
it is computationally prohibitive for a large dataset such as
COCO-stuff and the full ADE20K. Therefore, we use the
result images provided by the authors whenever possible.

Quantitative comparisons. As shown in Table 1, our

52341

Figure 7: Semantic image synthesis results on the Flickr Landscapes dataset. The images were generated from semantic
layout of photographs on Flickr.

method outperforms the current state-of-the-art methods by
a large margin in all the datasets. For COCO-Stuff, our
method achieves a mIoU score of 35.2, which is about 1.5
times better than the previous leading method. Our FID
is also 2.2 times better than the previous leading method.
We note that the SIMS model produces a lower FID score
but has poor segmentation performances on the Cityscapes
dataset. This is because the SIMS synthesizes an image by
ﬁrst stitching image patches from the training dataset. As
using the real image patches, the resulting image distribu-
tion can better match the distribution of real images. How-
ever, because there is no guarantee that a perfect query (e.g.,
a person in a particular pose) exists in the dataset, it tends
to copy objects with mismatched segments.

Qualitative results.
In Figures 5 and 6, we provide a
qualitative comparison of the competing methods. We ﬁnd
that our method produces results with much better visual
quality and fewer artifacts, especially for diverse scenes in
the COCO-Stuff and ADE20K dataset. When the training
dataset size is small, the SIMS model also renders images
with good visual quality. However, the depicted content
often deviates from the input segmentation mask (e.g., the
shape of the swimming pool in the second row of Figure 6).
In Figures 7 and 8, we show more example results from
the Flickr Landscape and COCO-Stuff datasets. The pro-
posed method can generate diverse scenes with high image
ﬁdelity. More results are included in the appendix of our

Dataset

COCO-Stuff
ADE20K
ADE20K-outdoor
Cityscapes

Ours vs.

Ours vs.

Ours vs.

CRN
79.76
76.66
66.04
63.60

pix2pixHD

86.64
83.74
79.34
53.64

SIMS
N/A
N/A
85.70
51.52

Table 2: User preference study. The numbers indicate the
percentage of users who favor the results of the proposed
method over the competing method.

arXiv version.

Human evaluation. We use Amazon Mechanical Turk
(AMT) to compare the perceived visual ﬁdelity of our
method against existing approaches. Speciﬁcally, we give
the AMT workers an input segmentation mask and two
synthesis outputs from different methods and ask them to
choose the output image that looks more like a correspond-
ing image of the segmentation mask. The workers are given
unlimited time to make the selection. For each comparison,
we randomly generate 500 questions for each dataset, and
each question is answered by 5 different workers. For qual-
ity control, only workers with a lifetime task approval rate
greater than 98% can participate in our evaluation.

Table 2 shows the evaluation results. We ﬁnd that users
strongly favor our results on all the datasets, especially on
the challenging COCO-Stuff and ADE20K datasets. For the
Cityscapes, even when all the competing methods achieve

62342

Figure 8: Semantic image synthesis results on COCO-Stuff. Our method successfully generates realistic images in diverse
scenes ranging from animals to sports activities.

Method
decoder w/ SPADE (Ours)
compact decoder w/ SPADE
decoder w/ Concat
pix2pixHD++ w/ SPADE
pix2pixHD++ w/ Concat
pix2pixHD++
compact pix2pixHD++
pix2pixHD [45]

#param COCO. ADE. City.
62.3
62.5
61.1
62.2
57.1
58.8
57.6
58.3

96M
61M
79M
237M
195M
183M
103M
183M

35.2
35.2
31.9
34.4
32.9
32.7
31.6
14.6

38.5
38.0
33.6
39.0
38.9
38.3
37.3
20.3

Table 3: mIoU scores are boosted when SPADE lay-
ers are used, for both the decoder architecture (Figure 4)
and encoder-decoder architecture of pix2pixHD++ (our im-
proved baseline over pix2pixHD [45]). On the other hand,
simply concatenating semantic input at every layer fails to
do so. Moreover, our compact model with smaller depth at
all layers outperforms all baselines.

high image ﬁdelity, users still prefer our results.

The effectiveness of SPADE. To study the impor-
tance of SPADE, we introduce a strong baseline called
pix2pixHD++, which combines all the techniques we ﬁnd
useful for enhancing the performance of pix2pixHD except
SPADE. We also train models that receive segmentation
mask input at all the intermediate layers via concatenation
(pix2pixHD++ w/ Concat) in the channel direction. Finally,
the model that combines the strong baseline with SPADE
is denoted as pix2pixHD++ w/ SPADE. Additionally, we
compare models with different capacity by using a different
number of convolutional ﬁlters in the generator.

Method
segmap input
random input
kernelsize 5x5
kernelsize 3x3
kernelsize 1x1
#params 141M
#params 96M
#params 61M
Sync Batch Norm
Batch Norm
Instance Norm

COCO ADE20K Cityscapes

35.2
35.3
35.0
35.2
32.7
35.3
35.2
35.2
35.0
33.7
33.9

38.5
38.3
39.3
38.5
35.9
38.3
38.5
38.0
39.3
37.9
37.4

62.3
61.6
61.8
62.3
59.9
62.5
62.3
62.5
61.8
61.8
58.7

Table 4: The SPADE generator works with different con-
ﬁgurations. We change the input of the generator, the con-
volutional kernel size acting on the segmentation map, the
capacity of the network, and the parameter-free normaliza-
tion method. The settings used in the paper are boldfaced.

As shown in Table 3 the architectures with the pro-
posed SPADE consistently outperforms its counterparts, in
both the decoder-style architecture described in Figure 4
and more traditional encoder-decoder architecture used in
pix2pixHD. We also ﬁnd that concatenating segmentation
masks at all intermediate layers, an intuitive alternative to
SPADE to provide semantic signal, does not achieve the
same performance as SPADE. Furthermore, the decoder-
style SPADE generator achieves better performance than
the strong baselines even when using a smaller number of
parameters.

72343

Figure 9: Our model attains multimodal synthesis capability when trained with the image encoder. During deployment,
by using different random noise, our model synthesizes outputs with diverse appearances but all having the same semantic
layouts depicted in the input mask. For reference, the ground truth image is shown inside the input segmentation mask.

Variations of SPADE generator. Table 4 reports the per-
formance of variations of our generator. First, we compare
two types of the input to the generator: random noise or
downsampled segmentation maps. We ﬁnd that both ren-
der similar performance, and conclude that the modulation
by SPADE alone provides sufﬁcient signal about the input
mask. Second, we vary the type of parameter-free normal-
ization layers before applying the modulation parameters.
We observe that SPADE works reliably across different nor-
malization methods. Next, we vary the convolutional kernel
size acting on the label map, and ﬁnd that kernel size of
1x1 hurts performance, likely because it prohibits utilizing
the context of the label. Lastly, we modify the capacity of
the generator network by changing the number of convolu-
tional ﬁlters. We present more variations and ablations in
the arXiv version for more detailed investigation.

Multi-modal synthesis.
In Figure 9, we show the mul-
timodal image synthesis results on the Flickr Landscape
dataset. For the same input segmentation mask, we sam-
ple different noise inputs to achieve different outputs. More
results are included in the arXiv paper.

Semantic manipulation and guided image synthesis. In
Figure 1, we show an application where a user draws dif-

ferent segmentation masks, and our model renders the cor-
responding landscape images. Moreover, our model allows
users to choose an external style image to control the global
appearances of the output image. We achieve it by replac-
ing the input noise with the embedding vector of the style
image computed by the image encoder.

5. Conclusion

We have proposed the spatially-adaptive normalization,
which utilizes the input semantic layout while performing
the afﬁne transformation in the normalization layers. The
proposed normalization leads to the ﬁrst semantic image
synthesis model that can produce photorealistic outputs for
diverse scenes including indoor, outdoor, landscape, and
street scenes. We further demonstrate its application for
multi-modal synthesis and guided image synthesis.

Acknowledgments We thank Alexei A. Efros and Jan
Kautz for insightful advice. Taesung Park contributed to
the work during his internship at NVIDIA. His Ph.D. is sup-
ported by a Samsung Scholarship.

82344

References

[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gen-
erative adversarial networks. In International Conference on
Machine Learning (ICML), 2017. 3

[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.

arXiv preprint arXiv:1607.06450, 2016. 2

[3] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Gold-
man. Patchmatch: A randomized correspondence algorithm
for structural image editing. In ACM SIGGRAPH, 2009. 1

[4] D. Bau, J.-Y. Zhu, H. Strobelt, Z. Bolei, J. B. Tenenbaum,
W. T. Freeman, and A. Torralba. Gan dissection: Visualizing
and understanding generative adversarial networks. In Pro-
ceedings of the International Conference on Learning Rep-
resentations (ICLR), 2019. 2

[5] A. Brock, J. Donahue, and K. Simonyan. Large scale gan
training for high ﬁdelity natural image synthesis. In Inter-
national Conference on Learning Representations (ICLR),
2019. 1, 2

[6] H. Caesar, J. Uijlings, and V. Ferrari. Coco-stuff: Thing and
stuff classes in context. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 2, 4

[7] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 40(4):834–848, 2018. 4

[8] Q. Chen and V. Koltun. Photographic image synthesis with
cascaded reﬁnement networks. In IEEE International Con-
ference on Computer Vision (ICCV), 2017. 1, 2, 4, 5

[9] T. Chen, M. Lucic, N. Houlsby, and S. Gelly. On self mod-
ulation for generative adversarial networks. In International
Conference on Learning Representations, 2019. 2

[10] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2016. 2, 4

[11] H. De Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin,
and A. C. Courville. Modulating early visual processing
by language.
In Advances in Neural Information Process-
ing Systems (NeurIPS), 2017. 2

[12] V. Dumoulin, J. Shlens, and M. Kudlur. A learned repre-
In International Conference on

sentation for artistic style.
Learning Representations (ICLR), 2016. 2, 3

[13] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in Neural Information
Processing Systems (NeurIPS), 2014. 2

[14] J. Hays and A. A. Efros. Scene completion using millions of

photographs. In ACM SIGGRAPH, 2007. 1

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer

for image recognition.
Vision and Pattern Recognition (CVPR), 2016. 3

[16] A. Hertzmann, C. E. Jacobs, N. Oliver, B. Curless, and D. H.
In Proceedings of the 28th an-
Salesin.
nual conference on Computer graphics and interactive tech-
niques, pages 327–340. ACM, 2001. 2

Image analogies.

[17] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter. GANs trained by a two time-scale update rule
converge to a local Nash equilibrium. In Advances in Neural
Information Processing Systems (NeurIPS), 2017. 4, 5

[18] S. Hong, D. Yang, J. Choi, and H. Lee.

Inferring seman-
tic layout for hierarchical text-to-image synthesis. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 2

[19] X. Huang and S. Belongie. Arbitrary style transfer in real-
In IEEE Inter-
time with adaptive instance normalization.
national Conference on Computer Vision (ICCV), 2017. 2,
3

[20] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal
unsupervised image-to-image translation. European Confer-
ence on Computer Vision (ECCV), 2018. 2, 3

[21] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
In International Conference on Machine Learning (ICML),
2015. 2, 3

[22] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.

Image-to-
image translation with conditional adversarial networks. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2, 3

[23] L. Karacan, Z. Akata, A. Erdem, and E. Erdem. Learning
to generate images of outdoor scenes from attributes and se-
mantic layouts. arXiv preprint arXiv:1612.00215, 2016. 2

[24] L. Karacan, Z. Akata, A. Erdem, and E. Erdem. Manipu-
lating attributes of natural scenes via hallucination. arXiv
preprint arXiv:1808.07413, 2018. 2

[25] T. Karras, S. Laine, and T. Aila. A style-based generator
architecture for generative adversarial networks.
In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2019. 2

[26] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In International Conference on Learning Rep-
resentations (ICLR), 2015. 4

[27] D. P. Kingma and M. Welling. Auto-encoding variational
bayes. In International Conference on Learning Representa-
tions (ICLR), 2014. 2, 4

[28] A. Kolliopoulos,

J. M. Wang,

Segmentation-based 3d artistic rendering.
Techniques, pages 361–370, 2006. 2

and A. Hertzmann.
In Rendering

[29] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-
siﬁcation with deep convolutional neural networks. In Ad-
vances in Neural Information Processing Systems (NeurIPS),
2012. 2

[30] J. H. Lim and J. C. Ye. Geometric gan. arXiv preprint

arXiv:1705.02894, 2017. 3

[31] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European Conference on Com-
puter Vision (ECCV), 2014. 2, 4

[32] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-
image translation networks. In Advances in Neural Informa-
tion Processing Systems (NeurIPS), 2017. 2

92345

[33] X. Mao, Q. Li, H. Xie, Y. R. Lau, Z. Wang, and S. P. Smol-
ley. Least squares generative adversarial networks. In IEEE
International Conference on Computer Vision (ICCV), 2017.
3

[34] L. Mescheder, A. Geiger, and S. Nowozin. Which training
In International

methods for gans do actually converge?
Conference on Machine Learning (ICML), 2018. 2, 3

[35] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spec-
tral normalization for generative adversarial networks. In In-
ternational Conference on Learning Representations (ICLR),
2018. 3, 4

[36] T. Miyato and M. Koyama. cGANs with projection discrim-
inator. In International Conference on Learning Representa-
tions (ICLR), 2018. 2, 3

[37] K. Nakashima. Deeplab-pytorch. https://github.

com/kazuto1011/deeplab-pytorch, 2018. 4

[38] A. Odena, C. Olah, and J. Shlens. Conditional image synthe-
sis with auxiliary classiﬁer GANs. In International Confer-
ence on Machine Learning (ICML), 2017. 2

[39] E. Perez, H. De Vries, F. Strub, V. Dumoulin, and
A. Courville. Learning visual reasoning without strong
priors.
In International Conference on Machine Learning
(ICML), 2017. 2

[40] X. Qi, Q. Chen, J. Jia, and V. Koltun. Semi-parametric im-
age synthesis. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 4, 5

[41] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and
H. Lee. Generative adversarial text to image synthesis. In In-
ternational Conference on Machine Learning (ICML), 2016.
2

[42] T. Salimans and D. P. Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In Advances in Neural Information Processing
Systems (NeurIPS), 2016. 2

[43] D. Ulyanov, A. Vedaldi, and V. Lempitsky.

Instance nor-
malization: The missing ingredient for fast stylization. arxiv
2016. arXiv preprint arXiv:1607.08022, 2016. 2, 3

[44] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, G. Liu, A. Tao, J. Kautz,
and B. Catanzaro. Video-to-video synthesis. In Advances in
Neural Information Processing Systems (NeurIPS), 2018. 1,
4

[45] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and semantic
manipulation with conditional gans. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 1,
2, 3, 4, 5, 7

[46] X. Wang, K. Yu, C. Dong, and C. Change Loy. Recover-
ing realistic texture in image super-resolution by deep spatial
feature transform. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 606–615,
2018. 2

[47] Y. Wu and K. He. Group normalization. In European Con-

ference on Computer Vision (ECCV), 2018. 2

[48] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun. Uniﬁed per-
ceptual parsing for scene understanding. In European Con-
ference on Computer Vision (ECCV), 2018. 4

[49] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and
X. He. Attngan: Fine-grained text to image generation with
attentional generative adversarial networks. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2018. 2

[50] F. Yu, V. Koltun, and T. Funkhouser. Dilated residual net-
works. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 4

[51] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-
attention generative adversarial networks. arXiv preprint
arXiv:1805.08318, 2018. 1, 2, 3

[52] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and
D. Metaxas. Stackgan: Text to photo-realistic image synthe-
sis with stacked generative adversarial networks.
In IEEE
International Conference on Computer Vision (ICCV), 2017.
1, 2

[53] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang,
and D. Metaxas. Stackgan++: Realistic image synthesis
with stacked generative adversarial networks. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
2018. 1

[54] B. Zhao, L. Meng, W. Yin, and L. Sigal. Image generation
from layout. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019. 2

[55] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and
A. Torralba. Scene parsing through ade20k dataset.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2017. 2, 4

[56] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In IEEE International Conference on Computer Vi-
sion (ICCV), 2017. 2

[57] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros,
O. Wang, and E. Shechtman. Toward multimodal image-to-
image translation. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), 2017. 2, 3

102346

