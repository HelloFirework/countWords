Learning to Learn Relation for Important People Detection

in Still Images

Wei-Hong Li1,2∗ , Fa-Ting Hong1,3,4∗ , and Wei-Shi Zheng1,4†

1 School of Data and Computer Science, Sun Yat-sen University, China

2 VICO Group, School of Informatics, University of Edinburgh, United Kingdom

3 Accuvision Technology Co. Ltd.

4 Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China.

w.h.li@ed.ac.uk, hongft3@mail2.sysu.edu.cn, wszheng@ieee.org

Abstract

!"#$%&'"( )ℎ+ ',-./)#"0+ .1 -+.-$+ 2')ℎ 3'11+/+") '"1./,#)'.".

Humans can easily recognize the importance of people in
social event images, and they always focus on the most im-
portant individuals. However, learning to learn the relation
between people in an image, and inferring the most impor-
tant person based on this relation, remains undeveloped. In
this work, we propose a deep imPOrtance relatIon NeTwork
(POINT) that combines both relation modeling and feature
learning. In particular, we infer two types of interaction
modules: the person-person interaction module that learns
the interaction between people and the event-person inter-
action module that learns to describe how a person is in-
volved in the event occurring in an image. We then estimate
the importance relations among people from both interac-
tions and encode the relation feature from the importance
relations. In this way, POINT automatically learns several
types of relation features in parallel, and we aggregate these
relation features and the person’s feature to form the impor-
tance feature for important people classiﬁcation. Extensive
experimental results show that our method is effective for
important people detection and verify the efﬁcacy of learn-
ing to learn relations for important people detection.

1. Introduction

In our daily lives, we often see wonderful live broad-
casting as the cameraman can easily recognize the impor-
tance of people in an event and take shots or videos of the
important people in the event to present what is occurring
at the moment. Additionally, when a social event image is
presented, humans can easily recognize the distinct impor-
tance of different faces (persons) in the event and focus on
the most important people (e.g., when people are watch-
ing a basketball game, they are more likely to focus on the
shooter or the player with the basketball). It is natural to ask

∗Equal contribution. Work done at Sun Yat-sen University.
†Corresponding author

(#)

(7)

(0)

8ℎ+ 9#,+ -+.-$+ '" 3'11+/+") ',#(+9.

(3)

(+)

(1)

Figure 1. Inferring the importance of persons from an image is
inherently complex and difﬁcult as it relates to diverse information
(i.e., individual features of persons (Figure (a)), relations among
persons (Figure (b)) and the event information (Figure(c)) from
the whole image). The great visual variations lead to difﬁculties as
well. The person in the red bounding box in all the images shown
in the second row is the same person while he plays diverse roles
in these images. He is the most important person in Figure (e) and
(f) while his appearance, location and the event in both images
are completely distinct. Comparing between Figure (d) and Figure
(e), he wears the same clothes in both images but his importance
in these images is different.

whether a computer vision model can be built to automati-
cally detect the important people in event images. It is also
known that correctly detecting the most important people
in images can beneﬁt other vision tasks such as event de-
tection [13], event/activity recognition [13, 17] and image
captioning [14].

Important people detection has only recently become the
focus of research. To detect the important people in still
images, a straightforward approach is to exploit classiﬁca-
tion or regression models to infer the importance of people
directly from their individual features [14]. Another solu-
tion considers the relations among persons by estimating
their interaction sequentially (i.e., sequential relation mod-
els [14, 10]). Solomon et al. [14] studied the relative impor-
tance between a pair of faces, either in the same image or
separate images, and developed a regression model to pre-

5003

dict the relative importance between any pair of faces using
manually designed features. Li et al. [10] modeled all pre-
viously detected people in a hybrid interaction graph and
developed PersonRank, a graphical model to rank the peo-
ple from the interaction graphs.

Despite these efforts on important people detection, the
problem remains challenging, as the importance of people
is related to not only their appearance but also, more im-
portantly, the relations among the people. Only relying on
the appearance features is not effective. For instance, we
would be unable to determine whether the lady with the red
bounding box in Figure 1(c) is important or not if we were
given the patch inside the red bounding box as shown in
Figure 1(a). However, if we know who and how others indi-
viduals are interacting with the lady (Figure 1), it becomes
easier to separate the lady from the others. Although rela-
tion modeling is important, the relation between two people
in an image is still determined by customized features (e.g.,
[14, 10]). The customized features are highly affected by
variations in pose, appearance and actions. How to auto-
matically exploit the reliable and effective relation features
that describe the relations between people is still unsolved.

In this work, we cast the important people detection
problem as learning the relation network among detected
people in an image and inferring the most active person
there. Thus, we attempt to develop a deep imPOrtance re-
latIon NeTwork (POINT) to allow machine learning to ex-
ploit the relations automatically. In POINT, we mainly in-
troduce the relation module, which contains several relation
submodules to automatically construct interaction graphs
and model their importance relations from the interaction
graphs. In each relation submodule, we form two types of
interaction modules, the person-person interaction module
and the event-person interaction module. The person-person
interaction module describes the pairwise person interac-
tions and the event-person interaction module indicates the
probability of a person being involved in the event. We then
introduce two methods to estimate the importance relations
among persons from both the interaction graphs and encode
the relation feature based on the importance relations. Fi-
nally, we concatenate the relation features from all relation
submodules into one relation feature and employ the resid-
ual connection to aggregate the concatenated relation fea-
ture and the person feature, resulting in the importance fea-
ture for the ﬁnal importance classiﬁcation. In summary, the
POINT method is a classiﬁcation framework consisting of
a feature representation module, a relation module and an
importance classiﬁcation module.

To the best of our knowledge, POINT is the ﬁrst to inves-
tigate deep learning for exploring and encoding the relation
features and exploiting them for important people detection.
In our experiments, we investigate and discuss the effect
of various types of basic interaction functions (i.e., additive

function and scaled dot product function) on modeling pair-
wise persons interactions and the effect of different types
of information on important people detection. The experi-
mental results show that our deep relation network achieves
state-of-the-art performance on two public datasets and ver-
ify its efﬁcacy for important people detection.

2. Related Work

Persons and General Object Importance. Recently, the
importance of generic object categories and persons has at-
tracted increased attention and has been studied by several
researchers [2, 5, 6, 7, 15, 8, 14, 10]. Solomon et al. [14]
focused on studying the relative importance between a pair
of faces, either in the same image or separate images, and
developed a regression model for predicting the importance
of faces. The authors designed customized features contain-
ing spatial and saliency information of faces for important
face detection. In addition, Ramanathan et al. [13] trained
an attention-based model with event recognition labels to
assign attention/importance scores to all detected individ-
uals to measure how related they were to basketball game
videos. More speciﬁcally, they proposed utilizing spatial
and appearance features of persons including temporal in-
formation to infer the importance score of all detected per-
sons. Recently, Li et al. [10] modeled all detected people
in a hybrid interaction graph by organizing the interaction
among persons sequentially and developed PersonRank, a
graphical model to rank the persons by inferring the im-
portance scores of persons from person-person interactions
constructed on four types of features that have been pre-
trained for other tasks.

Different from the aforementioned methods, which de-
sign both handcrafted relations as well as features, or those
pretrained for other tasks, as far as we know, our work is the
ﬁrst to design a deep architecture to combine the learning of
relations and features for important people detection. The
relation module is learned to construct interaction graphs
and automatically encode relation features. Thus, our net-
work can not only encode more effective features from a
persons individual information but also efﬁciently encode
the relations from other people and the event in the image.
Relation Networks on Vision Tasks. Relation modeling is
not limited to important people detection and has broad ap-
plication, such as object detection [4], AI gaming [22], im-
age captioning [20], video classiﬁcation [19], and few-shot
recognition [21]. Related to our method, Hu et al. [4] pro-
posed adapting the attention module by embedding a new
geometric weight and applying it in a typical object detec-
tion CNN model to enhance the features for object classiﬁ-
cation and duplicate removal. Zambaldi et al. [22] exploited
the attention module to iteratively identify the relations be-
tween entities in a scene and to guide a model-free policy in
a novel navigation and planning task called Box-World.

5004

Figure 2. An illustration of our deep imPOrtance relatIons NeTworks (POINT). We exploit the feature representation module to extract the
person feature for persons and the global feature for the whole image (Figure (a)). These features are fed into the relation module, which
contains r relation submodules. In each relation submodule, we construct two interaction graphs and estimate importance relations from
both graphs, which are used for encoding relation features. In this way, the POINT learns r relation features in parallel and these features
are concatenated into a relation feature vector. We add this concatenated relation feature to the person feature, resulting in the importance
feature. Finally, the importance classiﬁcation module is employed to infer the importance point of people.

In this work, we have the different purpose of building a
relation network for important people detection, while the
related relation models are not suitable for our task. In par-
ticular, in previous works, they learn the relation that de-
scribes the appearance and location similarity between two
objects/entities to ﬁnd the similar objects. These relational
models will bias the important people detection model to
detect the people with certain appearance or the people in a
speciﬁc location, but not for purpose of telling how people
are interacting with each other and who is the most active
one. In our experiments, we have shown only using appear-
ance features or speciﬁc location is not effective for impor-
tant people detection (see Table 1, the SVR-person only us-
ing appearance and location information is not effective.)
For estimating the important relations, we introduce two in-
teraction modules (i.e. person-person and event-person in-
teractions) to learn the interactions that describe the rela-
tion between two people and how people are involved in the
event occurring in an image automatically.

3. Approach

Detecting important people in still images is a more chal-
lenging task than conventional people detection as it re-
quires extracting higher semantic information than other de-
tection tasks. In this work, under the same setting as that in
previous works [13, 14, 10]1, we aim to design a deep rela-
tion network called the deep imPOrtance relatIons NeTwork
(POINT) (Section 3.1), which learns to build the relations
and combines the relation modeling with feature learning
for important people detection. We brieﬂy introduce the ar-

1Similar to the aforementioned works [13, 14, 10], we assume that all
persons appearing in images are successfully detected by existing state-of-
the-art person (face or pedestrian) detectors.

-.01+-*+ 7/08ℎ

1901+-*+ 7/08ℎ

)**+,-./01

224×224
ℎ1/0 6/7

Conv

Conv

Conv

Conv

'

!"#$%&#

'
!(

Figure 3. The feature representation module.

chitecture of the proposed POINT (Section 3.1) before de-
tailing three speciﬁc modules and loss (Section 3.2, Section
3.3 and Section 3.4).

3.1. Overview

An illustration of our proposed model’s architecture is
shown in Figure 2. Given a social event image I and all
detected (N ) persons {pi}N
i=1, to analyze the importance
of these persons, we build our POINT as a classiﬁcation
pipeline. Our model processes an arbitrary number of de-
tected people in parallel (as opposed to sequential relation
modeling [14, 10]) and is fully differentiable (as opposed
to the previous relation models using customized features
[10]). For the ith person pi in an image, its label (i.e., im-
portant or non-important person) si is estimated by:

si = f O(I; pi|θO)◦f R(f O

1 , ..., f O

N , f O

global|θR)◦f S(f I

i |θS), (1)

where ◦ denotes module composition, f I
i is the importance
i |θS) is the importance classiﬁcation
feature of pi and f S(f I
module parameterized by θS, which follows the relation
module f R(·) parameterized by the parameter groups θR.
In addition, the feature representation module f O(I; pi|θO)
parameterized by θO is employed to extract the person fea-

5005

𝑎𝐹𝑒𝑎𝑡𝑢𝑟𝑒𝑅𝑒𝑝𝑟𝑒𝑠𝑒𝑛𝑡𝑎𝑡𝑖𝑜𝑛𝑀𝑜𝑑𝑢𝑙𝑒𝑏𝑅𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝑀𝑜𝑑𝑢𝑙𝑒𝑓𝑖𝑂𝑓𝑔𝑙𝑜𝑏𝑎𝑙𝑂𝑟𝑅𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝑆𝑢𝑏𝑚𝑜𝑑𝑢𝑙𝑒𝑠𝐶𝑜𝑛𝑐𝑎𝑡ℋ𝑝ℋ𝑔𝑐𝐼𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑐𝑎𝑡𝑖𝑜𝑛𝑀𝑜𝑑𝑢𝑙𝑒1.00.260.25𝐼𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒𝑃𝑜𝑖𝑛𝑡𝑓𝑖𝐼{𝑓𝑖𝑅1}𝑖=1𝑁𝑟𝑅𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝐹𝑒𝑎𝑡𝑢𝑟𝑒𝑠𝑅𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝐹𝑒𝑎𝑡𝑢𝑟𝑒𝑠…{𝑓𝑖𝑅𝑟}𝑖=1𝑁#
!$

0.3

1.5

0.2

(()

#
!$

0.1914

0.6354

0.1732

#
!"

#
!"

#
!%

#
!%

#
!&

#
!&

#
!"

#
!"

#
!$

1.5

(*)

#
!$

0.09

#
!%

2.5

3.5

#
!&

#
!%

#
!&

(1)

(6)

1 and the output person-person interactions of V p

Figure 4. Figure (a) and (b) present the input person-person inter-
actions of V p
3 . Our
method (i.e., Eq. (4) weakens the effect of the interaction from V p
to V p
3 has too many outputs (Figure (d)). The
attention model [18] treats each node equally, and the interaction
from V p

1 has a larger impact (Figure (c)).

1 (the red link) as V p

3 to V p

3

i of pi and the global feature f O

ture f O
image I and ◦ is the operator to connect three modules.

global of the whole

i=1 and the global features f O

The relation module f R(·) exploits the input features of
persons {pi}N
global to automati-
cally construct interaction graphs and encode effective rela-
tion features. Similar to existing attention modules [18] and
relational modules [4, 22], we adopt the residual connec-
tion to aggregate the person feature and the relation feature,
resulting in a ﬁnal importance feature f I
i , which comprises
individual information, the relation information from other
persons and the event information in an image. The details
of each module are described in Section 3.2, Section 3.3 and
Section 3.4.

3.2. Feature Representation Module

Since feature representation is the ﬁrst step in impor-
tant people detection, we require the feature representa-
tion module (Figure 3) to be capable of extracting effec-
tive features from local to global information (i.e., the peo-
ple’s interior/individual information, the exterior/contextual
information around the people and the global information
illustrating the event cues). As with most vision works
[16, 10, 13, 14], it is natural to use the information in-
side the bounding box of the detected person, called the
interior patch in this work, to represent the person’s inte-
rior/individual feature. The location is also an indispensable
element of a person’s individual feature for illustrating the
importance of the person and the coordinate of the person
in the image is included in our feature. The reason is, from
the photographer’s perspective, when the images of an event
are captured, the photographer tends to place the important
people in the center of the image, and the important peo-
ple usually look clearer than other people in the image. Ad-
ditionally, the exterior/contextual information around each
person must be considered for analyzing the importance of
persons as this more global information, for instance, some

objects that the person uses can aid in distinguishing the
important people from the non-important people. For this
purpose, for each person, we crop an exterior patch2, which
is an image patch inside a box that is centered on the per-
son’s bounding box and is C 2 times larger than the scale of
the person’s bounding box.

In this work, we use the ResNet-50 to extract features
from each interior and exterior patch because it has demon-
strated its superiority in terms of important people detection
[10] and other vision tasks such as object detection [11]. As
shown in Figure 3, for each person in an image, we feed the
interior and the exterior patches into separate Resnet-50s,
transforming them into two 7 × 7 × 2048 features (i.e., the
interior feature and the exterior feature). While the coordi-
nate is a four dimensional vector, we produce a heat map,
which is a 224 × 224 grid where one or several cells corre-
spond to the person’s coordinate are assigned as 1 and the
others zero. We apply convolutional kernels to this heat map
to produce a 7 × 7 × 256 feature. Then, we concatenate the
interior, the exterior and the location features, resulting in a
7 × 7 × 4352 feature and employ two convolutional layers
with one fully-connected (fc) layer to transform this con-
catenated feature into a 1024 dimensional vector f O
i , called
the person feature.

As the important person is inevitably related to the event
that the person is involved in, the global information that
represents this event should be considered as well. Simi-
lar to the interior and the exterior features, the whole im-
age (denoted as the global patch) is fed into another deep
network, which comprises the convolutional layers of the
ResNet-50, two additional convolutional layers and one fc
layer for encoding a 1024 dimensional f O
global. We call this
feature the global feature.

3.3. Relation Module

Given the person feature and the global feature, we aim
to design a relation module that can encode the effective im-
portance feature by aggregating the relation feature and the
person feature. More speciﬁcally, we aggregate r relation
features encoded by r parallel relation submodules3 and
concatenate them into one relation feature vector. Then, we
employ the residual connection to merge the relation feature
and the person feature, yielding the importance feature for
each person pi:

f I
i = f O

i + Concat[f R1

i

, · · · , f Rr

i

], (i = 1, · · · N ),

(2)

where f R1
is a relation feature of person pi computed by
the ﬁrst relation submodule. We use this parallel structure

i

2In this work, C is trained on the validation set. Details of extracting

the exterior patch and C are reported in the Supplementary Material

3The structure of these relation submodules are the same while the pa-
rameters are NOT shared, which enables POINT to automatically learn
various types of relations.

5006

an importance interaction from a node that has too many
importance interaction outputs is less important, and this
weakens the effect of the importance interaction on the im-
portance relation (Figure 4).
Constructing Interaction Graphs. In order to estimate
the importance interaction ˆE p
ji, we ﬁrst create the person-
person interaction graph and event-person interaction
graph, which are deﬁned as Hp = (V p, E p) and Hg =
(V g, E g), respectively. Here, V p = {V p
i=1 are nodes rep-
resenting persons and V g = {V p
i=1 ∪ {V e} are nodes in
Hg, where V e is a node representing the event occurring in
the image. In addition, each element E p
ji in E p models the
person-person interaction from pj to pi indicating how pj
is interacting with pi, and each element E g
i in E g represents
the event-person interaction indicating the probability of a
person being involved in the event.

i }N

i }N

In the person-person interaction graph Hp, the interac-
tion between pairwise persons is computed by the person-
person interaction module, which is an additive attention
function [3, 1]4:

E p
ji = max{0, wP · (WQf O

i + WK f O

j )},

(5)

i and f O
j

where both WQ and WK are matrices that project the per-
son features f O
into subspaces and the vector wP
is applied to measure how pj is interacting with pi in the
subspace. Additionally, the max{·} function is employed to
trim the person-person interaction at zero if the person is
not interacting with the other person.

In the meantime, we estimate the event-person interac-

tion by the event-person interaction module 5 :

E g
i = max{0, wG · (f O

i + f O

global)},

(6)

i +f O

where f O
global is transformed into a scalar weight by wG
to indicate the probability of the person (pi) being involved
in the event. The event-person interaction is trimmed at 0,
acting as a ReLU nonlinearity. The zero trimming operation
restricts the event-person interactions only of the people be-
ing not related to the event.
Estimating Importance Interaction from Both Graphs.
Since we have two interaction graphs, the person-person in-
teraction graph and the event-person interaction graph, the
method for estimating the importance interaction ˆE p
ji from

4There are two commonly used attention functions/mechanisms: the
additive attention function [1] and the less expensive scale dot product
function [12, 18]. While the two are similar in theoretical complexity, the
additive operation slightly and consistently outperforms the scale dot prod-
uct operation [3]. This outcome is also veriﬁed in our experiments, so we
use the additive attention function for the person-person interaction mod-
eling.

5Eq. 6 is different from Eq. 5 as the event-person interaction differs
from the person-person interaction (asymmetric) presenting how a person
is interacting with another people: the event-person interaction should be
equal to the person-event interaction (symmetric) and is estimated to ﬁnd
whether a person is involved in the event.

5007

Figure 5. Figure (a) and (b) illustrate two methods introduced in
this work to embed global information into the person-person in-
teraction (they are the illustrations of the relation submodule as
well). Figure (a) is the method using Eq. (3) and Eq. (7) while
Figure (b) is the method using Eq. (8). The blue rectangle boxes
show the difference between our method and the attention model
[18] and the relation module in [4, 22] while the green boxes illus-
trate the difference between the two methods we proposed. (Better
viewed in color).

because it it allows POINT to automatically model various
types of people relations and has been shown to be more
effective in our work and others [4, 18]

Relations Modeling in the Relation Submodule. We now
describe our importance relation computation in the ℓth
(ℓ = 1, ..., r) relation submodule. For each given im-
age with N detected persons, we obtain a feature set
{f O
i with
respect to the ith person is computed by

global}, and then the relation feature f R

1 , ..., f O

N , f O

f R
i =

N

X

j=1

Eji · (WV f O

j ).

(3)

i

and use f R
i

Here, we remove the superscript of f Rℓ
for de-
scription convenience. The output of Eq. (3) aggregates the
feature from the others by a weighted sum of the person fea-
tures from the other people, and is linearly transformed by
WV . We formulate Eji, the importance relation indicating
the impact from the other people by:

Eji =

exp( ˆE p
ji)
k=1 exp( ˆE p

jk)

PN

,

(4)

where ˆE p
ji is the importance interaction among persons and
introduced in the following, and it is estimated from both
the person-person interaction graph and the event-person in-
teraction graph. Here, we compute the importance relation
from person pj to person pi as the importance interaction
from person pj to person pi scaled by the summation of
the output importance interactions of person pj . Inspired by
the PageRank algorithm [9], our model reﬂects the fact that

𝑊𝑄{𝑓𝑖𝑂}𝑖=1𝑁𝑓𝑔𝑙𝑜𝑏𝑎𝑙𝑂𝑊𝐾𝑊𝑉𝐴𝑑𝑑𝑖𝑡𝑖𝑣𝑒𝑅𝑒𝐿𝑈𝐴𝑑𝑑𝑖𝑡𝑖𝑣𝑒𝑊𝐺𝑅𝑒𝐿𝑈𝐸𝑞.(7)𝐸𝑞.(4)𝑚𝑎𝑡𝑚𝑢𝑙𝑡𝑖𝑝𝑙𝑦{𝑓𝑖𝑅𝑟}𝑖=1𝑁𝑊𝑄{𝑓𝑖𝑂}𝑖=1𝑁𝑓𝑔𝑙𝑜𝑏𝑎𝑙𝑂𝑊𝐾𝑊𝑉1𝐴𝑑𝑑𝑖𝑡𝑖𝑣𝑒𝑅𝑒𝐿𝑈𝐴𝑑𝑑𝑖𝑡𝑖𝑣𝑒𝑊𝐺𝑅𝑒𝐿𝑈𝑚𝑎𝑡𝑚𝑢𝑙𝑡𝑖𝑝𝑙𝑦{𝑓𝑖𝑅𝑟}𝑖=1𝑁𝑊𝑉2𝑚𝑎𝑡𝑚𝑢𝑙𝑡𝑖𝑝𝑙𝑦𝐴𝑑𝑑𝑖𝑡𝑖𝑣𝑒(𝑎)(𝑏)𝐸𝑞.(4)! $

! $

#
!%

#
!&

#
!'

#
!%

#
!&

#
!'

#
!"

#
!"

())

(+)

Figure 6. We introduce two methods to integrate the event-person
interaction graph with the person-person interaction graph. First,
we treat the event-person interaction as the prior importance acting
as the regulator to adjust the weight of the person-person interac-
tions (Figure (a)). Second, we treat the event-person interaction as
an extra input link for each person (Figure (b)).

both graphs can signiﬁcantly affect the importance relation
computation and then impact the ﬁnal results. In this work,
we introduce two methods (Figure 6) to estimate the impor-
tance interaction from multiple graphs. Intuitively, we treat
the event-person interaction as a prior importance and esti-
mate the importance interaction ˆE p

ji as:

ˆE p
ji = E p

ji · E g
j .

(7)

The advantage of this strategy is that the prior importance
E g
j acts as a regulator to adjust the effect of the person-
person interaction E p
ji on aggregating the relation features
by enhancing the effect when the prior importance is large
and reducing the impact in the opposite case.

An alternative strategy is to treat the event-person inter-
action as an additional graph to the person-person interac-
tion graph. In other words, we deﬁne the importance inter-
action as the person-person interaction (i.e., ˆE p
ji) and
the relation feature is aggregated as:

ji = E p

f R
i =

N

X

j=1

Eji · (WV1 f O

j ) + E g

i · (WV2 fglobal),

(8)

where Eji is computed by Eq. (4). Here, the relation feature
aggregates the feature from the others by a weighted sum of
person features from the other people, linearly transformed
by WV1 and the global feature transformed by WV2 . In this
way, the global information can be considered during en-
coding the importance features without affecting the effect
of the person-person interaction.

The above two strategies are veriﬁed to be effective
for combining both person-person interactions and event-
person interactions, and they have comparable results.
Parameters of the Relation Module. The relation mod-
ule Eq. (2) is summarized in Figure 2. It is easy to imple-
ment using basic operators, as illustrated in Figure 5. As
the dimension of the output feature is the same as the in-
put feature, we can stack more than one relation module
(Nr relation modules) to reﬁne the importance feature. In
Eq. (2), since we have r relation submodules in one rela-
tion module, the parameters are 5 × r projections: θR =

G ∈ Rdf }r

V ∈ Rdf ×dv , wℓ

Q ∈ Rdf ×dk , Wℓ

K ∈ Rdf ×dk , Wℓ

{Wℓ
P ∈
Rdf , wℓ
ℓ=1, where df = 1024 is the dimension
of the person feature and dk = dv = df
r Due to the reduced
dimension of each relation submodule, the total computa-
tional cost is similar to that of the single relation submodule
with full dimensionality.

3.4. Classiﬁcation Module for End to End Learning

After we obtain the importance feature for each person
in an image, we utilize two fully connected layers (i.e.,
i |θS)) to transform the fea-
the classiﬁcation module f S(f I
ture into two scalar values indicating the probability of the
person belonging to the important people or non-important
people classes. During training, the commonly used cross-
entropy loss is employed to penalize the model, and the
SGD is used to optimize the model for backward compu-
tation. During testing, the probability of the important peo-
ple class is used as the importance point for each people.
In each image, and the people with the highest importance
point will be selected as the most important people.

4. Experiments

In this section, we conducted extensive experiments on
two publicly available image-based important people detec-
tion datasets. We followed the standard evaluation proto-
col in the dataset [10]. The mean average precision (mAP)
and some visual comparisons are reported. The CMC curve,
other visual results and the classiﬁcation accuracy of all
people tested are reported and analyzed in the Supplemen-
tary Material.

4.1. Datasets

For evaluation on important people detection in still im-
ages, there are two publicly available datasets [10]: 1) The
Multi-scene Important People Image Dataset (MS Dataset)
and 2) the NCAA Basketball Image Dataset (NCAA
Dataset).
1) The MS Dataset. The MS Dataset contains 2310 images
from more than six types of scenes. This dataset includes
three subsets: a training set (924 images), a validation set
(232 images), and a testing set (1154 images). The detected
face bounding box and importance labels are provided.
2) The NCAA Dataset. The NCAA Dataset is formed
by extracting 9,736 frames of an event detection video
dataset [13] covering 10 different types of events. The per-
son bounding box and the importance annotations are pro-
vided as well.

4.2. Comparison with Other Methods

We ﬁrst compared our method with existing impor-
tant people detection models: 1) the VIP model [14], 2)
Ramanathan’s model [13] and 3) the PersonRank (PR)

5008

Method

Max-
Face

Table 1. The mAP (%) of Different Methods on both Datasets
Ramanathan’s

Max-

Max-

Pedestrian

Saliency

Most- Max-
Scale
Center

SVR-
Person

MS Dataset

NCAA Dataset

35.7

31.4

30.7

24.7

40.3

26.4

50.9

30.0

73.9

31.8

75.9

64.5

VIP

76.1

53.2

model [13]

- -

61.8

PR

88.6

74.1

Ours

(POINT)

92.0

97.3

Table 2. The mAP (%) for Evaluating Different Components of our
POINT on Both Datasets.

Table 4. The mAP (%) for Comparison of our Method and the one in [18]
for Estimating the Importance Relation on both Datasets.

Dataset

MS Dataset

NCAA Dataset

Method
BaseInter

BaseInter+Loca

BaseInter+Exter+Loca

BaseInter

BaseInter+Loca

BaseInter+Exter+Loca

mAP

72.6

79.5

89.2

89.1

89.9

95.8

Method
POINTInter

POINTInter+Loca

POINTInter+Exter+Loca

POINTInter

POINTInter+Loca

POINTInter+Exter+Loca

mAP

76.5

85.6

92.0

90.3

93.9

97.3

Table 3. The mAP (%) for Evaluating our Methods of Integrating Global
Information on both Datasets.

MS Dataset

NCAA Dataset

Method
POINTHp
POINTEq. (8)

POINTEq. (3)+Eq. (7)

mAP

91.2

91.3

92.0

Method
POINTHp
POINTEq. (8)

POINTEq. (3)+Eq. (7)

mAP

96.0

96.7

97.3

model [10] as well as all baselines (i.e., max-face, max-
pedestrian, max-saliency, most-center, max-scale and SVR-
person) provided in [10]. The experimental results are
shown in Table 16. From the table, it is clear that our POINT
obtains state-of-the-art results. It is noteworthy that our
POINT achieves a signiﬁcant improvement of 23.2 % on the
NCAA Dataset over the PersonRank method that achieved
the best performance previously (i.e., 74.1 %). This veri-
ﬁes the efﬁcacy of our POINT method for extracting higher
level semantic feature that embraces more effective infor-
mation for important people detection, compared to those
customized or deep features trained for other tasks. This
also indicates the effectiveness of incorporate the relation
modeling with feature learning for important people detec-
tion. Interestingly, the improvement on the MS Dataset is
signiﬁcantly less than that on the NCAA Dataset (i.e., 3.4%
vs 23.2%, respectively). The reason is that there are limited
numbers of images (i.e., 2310 images in total), which lim-
ited the training of our deep model, even though the data
augmentation of the training data (such as RandomCrop)
has been used on the MS Dataset.

4.3. Evaluation of Our POINT

Evaluating Different Components of POINT. Since there
is a lack of end-to-end trainable deep learning models for
important people detection, we form a baseline that only
comprises the feature representation module and the impor-

6On the MS Dataset, we did not compare Ramanathan’s model [13] as
it uses temporal information, which is not provided in the MS Dataset. All
the results of other methods are from [10]

MS Dataset

NCAA Dataset

Method

Attention [18]

Ours (POINT)

mAP

90.0

92.0

Method

Attention [18]

Ours (POINT)

mAP

95.8

97.3

Table 5. The mAP (%) for Evaluating the Effect of r on Both Datasets

Dataset

Baseline

MS Dataset

NCAA Dataset

89.2

95.8

r=1

90.7

96.2

r=2

91.4

96.8

r=4

92.0

97.3

r=8

91.4

96.8

r=16

r=32

91.8

97.0

91.4

96.6

Ours (POINT)

Table 6. The mAP (%) for Evaluating the Effect of Nr on Both Datasets

Dataset

Baseline

Ours (POINT)

Nr =1 Nr =2 Nr =4 Nr =6

MS Dataset

NCAA Dataset

89.18

95.84

91.96

97.28

91.97

97.24

90.99

97.29

90.90

96.02

Table 7. The mAP (%) for Evaluating Different Types of Attention Func-
tions on both Datasets.

MS Dataset

Method

POINTScaled Dot Product

POINTAdditive

mAP

90.7

92.0

NCAA Dataset

Method

POINTScaled Dot Product

POINTAdditive

mAP

96.2

97.3

tance classiﬁcation module. This approach predicts the im-
portance of persons without considering their relations with
others and the event-person relations. It is deﬁned as:

sBaseline
i

= f O(pi|θO) ◦ f S(f O

i |θS).

(9)

It is formed to evaluate the effect of the relation module
(i.e., our POINT) and different components of the feature
(i.e., the interior feature, the location feature and the exte-
rior/contextual feature). The results are reported in Table 2
where the BaseInter indicates the baseline using only the in-
terior feature and POINTInter + Loca +Exter is our full model.
The POINT, using the feature comprising all features, is de-
scribed in Section 3.2.

From Table 2, it is noteworthy that our POINT consis-
tently obtains better mAP values than the baseline using
different types of features (e.g., 92.0% vs 89.2%, respec-
tively, on the MS Dataset using three types of cues). This
result indicates that embedding the relation module intro-
duced in this paper can signiﬁcantly aid in extracting more
discriminant, higher level semantic information, which dra-
matically increases the performance. Additionally, we can
see that both the baseline and POINT improve the mAP on

5009

!"#$%&'(") ") *ℎ, 01 /%*%(,*

23, 56783
59

23, 56783

59

!"#$%&'(") ") *ℎ, -!.. /%*%(,*

59

59

23, 56783

23, 56783

Figure 7. Visual results of detecting important people and compar-
ison with related work (i.e., PersonRank (PR)) on Both Datasets.

important people detection by using more cues compared to
those using less information or a single type of information
(e.g., the BaseIndi+Cont+Loca has an improvement of 16.6%
mAP over the BaseIndi, which obtains 72.6% mAP on the
MS Dataset).
Integrating the Additional Global Information and Es-
timating the Importance Relation. In this work, we in-
troduce two methods to integrate the event-person interac-
tion graph with the person-person interaction graph. Table
3 presents the results of our POINT for detecting impor-
tant people without global information (i.e., POINTHp
), our
POINT using the global information in different ways (i.e.,
POINTEq. (3)+Eq. (7) and POINTEq. (8)). It is clearly shown
that both methods successfully integrate the global infor-
mation into the importance feature and improve the perfor-
mance. In general, the improvement when using the global
information as a prior importance is higher than that of
treating the event-interaction graph as an additional graph
(e.g., 1.3% vs 0.7%, respectively, on the NCAA Dataset).

We also compared our method of estimating the impor-
tance relation with that of the attention weight [18] (i.e., the
relation module in other vision tasks [4, 22]), and the results
on both datasets are reported in Table 4. While the whole re-
lation network is completely different from [18, 4, 22] due
to different tasks, it is clear that our relation module is more
effective than the relation model used in [18, 4, 22], as we
have consistent improvement (e.g., 92.0% vs 90.0%, respec-
tively, on the MS Dataset). This result veriﬁes the efﬁcacy
of Eq. (4).
Visual Results and Comparisons. In this section, selected
visual results and comparisons are reported in Figure 7 to
further evaluate our POINT. As shown in Figure 7, it is clear
that our POINT can detect the important people in some
complex cases (e.g. in the both image in the second row, the
defender and the shooter are very closed and our POINT
can correctly assign most points to the shooter while the
PersonRank (PR) usually pick the defender or other player
as the important people.

Effect of r and Nr on Important People Detection. The
number of relation submodule r and the number of stacked
relation module Nr can slightly affect our POINT. To evalu-
ate the effect of both parameters, we report the results of our
POINT using r ranging from 1 to 32 and keeping Nr = 1
in Table 5. Then, we select r = 4 as it yields the best result
and set Nr ranging from 1 to 6. The evaluation results of the
effect of Nr are reported in Table 6. The results shows that
using the r > 1 relation submodule in a relation module
enables our POINT to obtain better results because using
multiple relation submodules allows our POINT to model
various types of relations. In addition, we ﬁnd that when
we set Nr > 1, the POINT obtains slightly better results
(e.g., setting Nr = 2 on the MS dataset and Nr = 4 on
the NCAA dataset are the best) because the added relation
modules can aid in reﬁning the importance features.
Evaluation of the Attention Functions. Currently, there
are two commonly used attention functions for modeling
interaction between any pairs of entities, the additive and
the scaled dot product attention functions. Similar to [3],
we ﬁnd the additive attention function works slightly but
consistently better than the scaled dot product function from
Table 7 (e.g., 97.3% vs 96.2%, respectively, on the NCAA
Dataset).
Running time. We implement our model using PyTorch on
a machine with CPU E5 2686 2.3 GHz, GTX 1080 Ti and
256 GB RAM. The running time of our POINT for pro-
cessing an image is sensitive to the number of persons in
the image. On average, POINT can process 10 frames per
second (fps), which is signiﬁcantly faster than the Person-
Rank (0.2 fps) and the VIP (0.06 fps). This result indicates
that our POINT largely improves the speed of the important
people detection model.

5. Conclusion

We have proposed a deep importance relation network to
investigate deep learning for exploring and encoding the re-
lation features and exploiting them for important people de-
tection. More importantly, we have shown that POINT suc-
cessfully integrate the relation modeling with feature learn-
ing to learn the feature for relation modeling. In addition,
POINT can learn to encode and exploit the relation feature
for important people detection. It was clearly shown that our
proposed POINT could obtain state-of-the-art performance
on two public datasets.

6. Acknowledgement

This work was supported partially by the National
Key Research and Development Program of China
(2018YFB1004903), NSFC(61522115), and Guangdong
Province Science and Technology Innovation Leading Tal-
ents (2016TX03X157).

5010

[16] Christian Szegedy, Scott Reed, Dumitru Erhan, and
Dragomir Anguelov. Scalable, high-quality object detection.
arXiv, 2014.

[17] Yongyi Tang, Peizhen Zhang, Jian-Fang Hu, and Wei-Shi
Zheng. Latent embeddings for collective activity recogni-
tion.
In Advanced Video and Signal Based Surveillance,
2017.

[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems, 2017.

[19] Limin Wang, Wei Li, Wen Li, and Luc Van Gool.
Appearance-and-relation networks for video classiﬁcation.
In Computer Vision and Pattern Recognition, 2018.

[20] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In International conference on
machine learning, 2015.

[21] Flood Sung Yongxin Yang, Li Zhang, Tao Xiang, Philip HS
Torr, and Timothy M Hospedales. Learning to compare: Re-
lation network for few-shot learning.
In Computer Vision
and Pattern Recognition, 2018.

[22] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor
Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Re-
ichert, Timothy Lillicrap, Edward Lockhart, et al. Relational
deep reinforcement learning. arXiv, 2018.

References

[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate.
International Conference on Machine Learning,
2014.

[2] Alexander C Berg, Tamara L Berg, Hal Daume, Jesse
Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret
Mitchell, Aneesh Sood, Karl Stratos, et al. Understanding
and predicting importance in images.
In Computer Vision
and Pattern Recognition, 2012.

[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc
Le. Massive exploration of neural machine translation ar-
chitectures. In Conference on Empirical Methods in Natural
Language Processing, 2017.

[4] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen
Wei. Relation networks for object detection. In Computer
Vision and Pattern Recognition, 2018.

[5] Sung Ju Hwang and Kristen Grauman. Learning the rela-
tive importance of objects from tagged images for retrieval
and cross-modal search. International journal of computer
vision, 100(2):134–153, 2012.

[6] Duy-Dinh Le, Shin’ichi Satoh, Michael E Houle, and Dat
Phuoc Tat Nguyen. Finding important people in large news
video databases using multimodal and clustering analysis. In
International Conference on Data Engineering, 2007.

[7] Yong Jae Lee, Joydeep Ghosh, and Kristen Grauman. Dis-
covering important people and objects for egocentric video
summarization.
In Computer Vision and Pattern Recogni-
tion, 2012.

[8] Yong Jae Lee and Kristen Grauman. Predicting important
International

objects for egocentric video summarization.
Journal of Computer Vision, 114(1):38–55, 2015.

[9] Jure Leskovec, Anand Rajaraman, and Jeffrey David Ull-
man. Mining of massive datasets. Cambridge University
Press, 2014.

[10] Wei-Hong Li, Benchao Li, and Wei-Shi Zheng. Personrank:
Detecting important people in images. In International Con-
ference on Automatic Face & Gesture Recognition, 2018.

[11] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In European con-
ference on computer vision, 2016.

[12] Minh-Thang Luong, Hieu Pham, and Christopher D Man-
ning. Effective approaches to attention-based neural machine
translation. Conference on Empirical Methods in Natural
Language Processing, 2015.

[13] Vignesh Ramanathan, Jonathan Huang, Sami Abu-El-Haija,
Alexander Gorban, Kevin Murphy, and Li Fei-Fei. Detect-
ing events and key actors in multi-person videos. Computer
Vision and Pattern Recognition, 2016.

[14] Clint Solomon Mathialagan, Andrew C Gallagher, and
Dhruv Batra. Vip: Finding important people in images. In
Computer Vision and Pattern Recognition, 2015.

[15] Merrielle Spain and Pietro Perona. Measuring and predict-
International Journal of Computer

ing object importance.
Vision, 91(1):59–76, 2011.

5011

