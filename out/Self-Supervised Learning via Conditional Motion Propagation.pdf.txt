Self-Supervised Learning via Conditional Motion Propagation

Xiaohang Zhan1, Xingang Pan1, Ziwei Liu1, Dahua Lin1, and Chen Change Loy2

1CUHK - SenseTime Joint Lab, The Chinese University of Hong Kong

2Nanyang Technological University

1{zx017, px117, zwliu, dhlin}@ie.cuhk.edu.hk

2ccloy@ntu.edu.sg

Abstract

Intelligent agent naturally learns from motion. Various
self-supervised algorithms have leveraged motion cues to
learn effective visual representations. The hurdle here is
that motion is both ambiguous and complex, rendering pre-
vious works either suffer from degraded learning efﬁcacy,
or resort to strong assumptions on object motions. In this
work, we design a new learning-from-motion paradigm to
bridge these gaps. Instead of explicitly modeling the mo-
tion probabilities, we design the pretext task as a condi-
tional motion propagation problem. Given an input image
and several sparse ﬂow guidance vectors on it, our frame-
work seeks to recover the full-image motion. Compared
to other alternatives, our framework has several appealing
properties: (1) Using sparse ﬂow guidance during train-
ing resolves the inherent motion ambiguity, and thus eas-
ing feature learning.
(2) Solving the pretext task of con-
ditional motion propagation encourages the emergence of
kinematically-sound representations that poss greater ex-
pressive power. Extensive experiments demonstrate that
our framework learns structural and coherent features;
and achieves state-of-the-art self-supervision performance
on several downstream tasks including semantic segmen-
tation, instance segmentation, and human parsing. Fur-
thermore, our framework is successfully extended to sev-
eral useful applications such as semi-automatic pixel-level
annotation. Project page: http://mmlab.ie.cuhk.
edu.hk/projects/CMP/.

1. Introduction

Humans have a remarkable ability of gaining useful
knowledge without direct supervision. The visual world
around us is highly structural, thus containing abundant nat-
ural supervisions to learn from.
In daily navigation, we

CMP 
Training

CMP 
Testing

(a)

(b)

Figure 1. An illustration of our conditional motion propagation
task. In training, the goal is to predict optical ﬂow from a static
image conditioned on sparse motion guidance. The guidance con-
sists of sparse velocities sampled from the target optical ﬂow with
a “watershed” strategy (see Section 3.2). In testing, the guidance
can be arbitrary, and the model is able to predict kinematically-
sound results. For example, as shown in (b), given a guidance on
left foot, the model predicts that the shin is rotating. The optical
ﬂows are visualized with Middlebury color wheel, and should be
viewed in color.

constantly perform the task of visual prediction by hallu-
cinating what’s behind the corner. The recently introduced
self-supervised learning aims to empower machines with a
similar capacity, learning without explicit annotations. By
carefully designing pretext tasks comprising natural super-
visions, self-supervised learning learns effective represen-
tations that can be used for several downstream scenarios.

In comparison to static pretext tasks such as coloriza-
tion [34, 15] and inpainting [29], motion provides richer
and more structural information for us to exploit. The mo-
tion of a moving object generally indicates its kinematic

1881

properties, which further reveals its inner structure. Pre-
vious works have leveraged the motion cues from two di-
rections: The ﬁrst direction [31, 30] is to learn image rep-
resentations by predicting motion from static images. For
example, Walker et al. [31, 30] proposed to predict dense
optical ﬂow from a static image and use the learned fea-
tures for action recognition. However, since motion is in-
herently ambiguous, direct modeling of future motion cre-
ates large learning burden and sometimes results in unstable
training. The second direction [28, 20] is to exploit the re-
lationships between motion and objects to derive a motion-
based constraining loss. For example, Mahendran et al. [20]
assumed that pixels with similar features should have simi-
lar motions, and designed a cross pixel ﬂow similarity loss
to optimize the representations. Though these methods have
shown promising results, they made too strong assumptions
on objects, i.e., all pixels on the same object should have
similar motion. However, most of the objects are intrinsi-
cally with high degrees of freedom. For example, a person
is an articulated object and a curtain is deformable. We can-
not claim that they still follow such simple assumption.

The ambiguity and complexity of motion pose great
challenges on self-supervised algorithms. In this work, to
overcome these challenges and make better use of motion
cues, we propose a new paradigm to leverage motion for
representation learning. The key idea is to deﬁne the pretext
task as a Conditional Motion Propagation (CMP) problem.
The framework is composed of an image encoder, a sparse
motion encoder and a dense motion decoder. As shown in
Figure 1, our task is to predict optical ﬂow from a single
image conditioned on sparse motion guidance.

Our approach has several merits. Firstly, using sparse
motion as guidance during training avoids the motion am-
biguity problem, thus easing the pressure in representation
learning. Secondly, in order to recover dense optical ﬂow
from the given sparse motions, the image encoder must en-
code kinematically-sound properties so that the decoder is
able to propagate motions from the guidance according to
the properties. Hence, in this way, the image encoder can
automatically learn complex kinematic properties from mo-
tions, instead of predeﬁning a speciﬁc relationship between
motion and objects. As shown in Figure 1 (b), in testing
time, given an arbitrary guidance arrow, the CMP model
produces kinematically reasonable results. Leveraging such
characteristics, CMP can also be applied to guided video
generation and semi-automatic pixel-level annotation 4.3.

Thanks

to the kinematically-sound representations
learned by CMP, our method can beneﬁt several down-
stream tasks, especially for segmentation tasks. Our pro-
posed CMP achieves state-of-the-art performance on sev-
eral benchmarks under the condition of unsupervised pre-
training, including PASCAL VOC 2012 semantic segmen-
tation, COCO instance segmentation, and LIP human pars-

ing. We summarize our contributions as follows: First, we
propose a new paradigm to better leverage motion in rep-
resentation learning and achieve promising performance on
various benchmarks. Second, our CMP model is capable
of capturing kinematic properties of various objects with-
out any manual annotations. Third, the CMP model can be
applied to guided video generation and semi-automatic an-
notation.

2. Related Work

Self-supervised learning can be divided into two cate-

gories, respectively exploiting context and videos.
Learning from Context. Context-based self-supervised
learning methods typically distort or decompose the images
and then learn to recover the missing information. For in-
stance, Doersch et al. [6] design a task to predict relative
locations of patch pairs. Pathak et al. [29] learn representa-
tions by image in-painting. Noroozi et al. [24] deﬁne jigsaw
puzzles of image patches and train a CNN to solve them.
Zhang et al. [34] and Larsson et al. [15] learn features via
colorizing gray images. Gidaris et al. [8] rotate images and
then use CNN to predict the rotations.
Learning from Temporal Consistency. For video-based
representation learning, supervisions come from tempo-
ral information and thus images are usually undistorted.
Some of them rely on temporal consistency of contexts.
Mobahi et al. [22] make a temporal coherence assump-
tion that successive frames tend to contain similar con-
tents. Jayaraman et al. [12] train a CNN with a regularizer
that feature changes over time should be smooth. Wang et
al. [32] ﬁnd corresponding pairs by visual tracking. Other
works [18, 16, 21, 33] learn representations by synthesizing
frames or predicting correct temporal order.
Learning from Motion. Other video-based methods focus
on motions to discover object-level information. Pathak et
al. [28] use foreground segment masks extracted from
videos as supervision. Mahendran et al. [20] assume that
similar features should have similar motions, and design a
cross pixel ﬂow similarity loss to optimize the represen-
tation. These works rely on a strong assumption, i.e., all
pixels on the same object should have similar motion. As
mentioned before, most objects are intrinsically with high
degrees of freedom. Even the same object may have diverse
motion patterns under different circumstances. For exam-
ple, pixels’ motions on a bar are similar if it is shifting, but
vary if it is rotating.

An alternative way to leverage motion for

self-
supervised learning is through performing optical ﬂow pre-
diction from static images. Walker et al. [31] propose to
predict dense optical ﬂow from a static image. And the
follow-up work [30] uses a Variational Auto Encoder to
model the motion uncertainty. However, due to the am-
biguity of motion, it is a daunting task to predict motion

1882

sparse motion + mask

sparse motion 

encoder

image encoder

dense motion decoder

propagation 

nets

fusion net

static image

optical flow

Figure 2. Our conditional motion propagation framework mainly contains three modules: sparse motion encoder, image encoder and dense
motion decoder. Sparse motions are sampled from target optical ﬂow with a “watershed” strategy illustrated in Section 3.2. The target
optical ﬂow is extracted using off-the-shelf method.

without any hints, especially when coupled with camera
ego-motion. Recall that our target is to predict motion
from static images conditioned on sparse motion guidance.
Hence motion forecasting is a degenerate case of our work
when the amount of guidance points decreases to zero. Us-
ing sparse motion as guidance during training avoids motion
ambiguity problem, thus easing the difﬁculty in representa-
tion learning.

3. Conditional Motion Propagation

Our goal is to learn image representation by designing
the pretext task as a conditional motion propagation prob-
lem. Speciﬁcally, our training framework seeks to recover
the full-image motion from static images conditioned on
sparse motion guidance.

3.1. Framework

As shown in Figure 2, the framework contains three
modules: image encoder, sparse motion encoder, and dense
motion decoder.
Image Encoder. The image encoder is a standard back-
bone Convolutional Neural Network (CNN). After the CMP
training completed, it serves as a pre-train model for the
subsequent tasks. CMP does not restrict the backbone archi-
tecture, though in our experiments the backbone is AlexNet
or ResNet-50, depending on different target tasks. We add
an additional convolution layer at the top of the image en-
coder to encode the feature to 256 channels.
Sparse Motion Encoder. It is a shallow CNN aiming at en-
coding the sparse motion into compact features. It contains
two stacked Conv-BN-ReLU-Pooling blocks and encodes
sparse motion into 16 channels. The spatial stride depends
on the stride of the image encoder. The inputs to the sparse
motion encoder include: 1) The two-channels sparse optical
ﬂow as guidance sampled from the target optical ﬂow using
a “watershed” strategy discussed in Section 3.2. The ﬂow

values of positions that are not sampled are set to zero. 2)
A binary mask indicating the positions of selected guidance
points. It serves to distinguish the sampled positions with
zero motion and those unsampled positions. We concate-
nate the sparse motion and the mask as a 3-channel input to
the sparse motion encoder. The motion and image features
are concatenated and fed into the dense motion decoder.

Dense Motion Decoder. The decoder is designed to prop-
agate motion to the full image according to the encoded
kinematic properties. The decoder contains several prop-
agation nets and a fusion net. The propagation nets are
CNNs with different spatial strides. Those with larger spa-
tial strides have larger receptive ﬁelds, hence they result in
longer distances of propagation. And those with smaller
spatial strides focus on shorter distance, thus producing
ﬁne-grained results. Each propagation net is composed of
a max pooling layer with respective stride, and two stacked
Conv-BN-ReLU blocks. We design the propagation nets to
be rather shallow, so as to force the image encoder to learn
more meaningful information. Finally, the output of prop-
agation nets are up-sampled to the same spatial resolution
and concatenated into the fusion net, a single convolution
layer, to produce predictions.

Loss Function. Optical ﬂow prediction is typically re-
garded as a regression problem, as in [7], since regres-
sion produces averagely accurate velocity values. However,
regression usually cannot produce discriminative gradients,
and the results tend to be smoothed. This issue could pre-
vent us from learning good representations from scratch.
Fortunately, CMP does not need the output ﬂow to be abso-
lutely accurate. Hence, we quantize the target ﬂow and for-
mulate it as a classiﬁcation task. Different from Walker et
al. [31] who quantize optical ﬂow by clustering, we adopt a
simple yet efﬁcient method. We clip the target ﬂow within
a loose boundary, and partition the ﬂow into C bins linearly
in x and y coordinates respectively. They are then classi-
ﬁed by two linear classiﬁers. We use a cross-entropy loss

1883

separately for x and y ﬂows. It is formulated as:

Lx = −

Ly = −

1
N

1
N

N

C

X

X

i=1

c=1

N

C

X

X

i=1

c=1

(✶ (Qx

i = c) log Pi

x
c ) ,

(✶ (Qy

i = c) log Pi

y
c ) ,

(1)

where N is the total number of pixels, P is the probability
from SoftMax layer, Q is the quantized labels, and ✶ is an
indicator function. We apply the same weight to Lx and Ly.

3.2. Guidance Selection

Sampling from Watershed. Sparse motion guidance is
sampled from the target optical ﬂow. For effective prop-
agation, those guidance vectors should be placed at some
key-points where the motions are representative. We adopt
a watershed-based [4] method to sample such key-points.
As shown in Figure 3, given the optical ﬂow of an image,
we ﬁrst extract motion edges using a Sobel ﬁlter. Then we
assign each pixel a value to be the distance to its nearest
edge, resulting in the topological-distance watershed map.
Finally, we apply Non-maximum Suppression (NMS) [5]
with kernel size K on the watershed map to obtain the key-
points. We can adjust K to control the average number
of sampled points. A larger K results in sparser samples.
Points on image borders are removed. With the watershed
sampling strategy, all the key-points are roughly distributed
on the moving objects. Since background motion actually
reﬂects camera ego-motion, to avoid ambiguity in learning,
we also add several grid points in each image. The grid
stride G is used to adjust the density of grids. For a good
practice in our experiments, there are on average 13.5 sam-
pled guidance points in a 384 × 384 image.
Outlier Handling. In some cases, the optical ﬂow may not
be ideal, as shown in the third row of Figure 3. The dis-
ordered ﬂow edges create disconnected watersheds, which
result in a large number of key-points selected. However,
it does not affect learning. These image examples are actu-
ally easy cases, since the abundant guidance ease the pres-
sure in learning those meaningless motions. In other words,
these examples with collapsed ﬂows are ignored to some
extent. Hence, our framework is robust to the quality of
optical ﬂow.

4. Experiments

Training Sets. CMP does not rely on a speciﬁc optical
ﬂow estimation method. Considering that our datasets are
million-level, we choose LiteFlowNet [11], an extremely
fast optical ﬂow estimation tool to compute optical ﬂows.
In this way, we prepare 4 training sets for CMP training.
(a) YFCC100m-Videos. YFCC100m contains about 700k
in-the-wild videos. We use the set of sampled frames pro-

image

flow

flow edge watershed

Figure 3. The ﬁgure shows how we sample guidance from optical
ﬂow. We ﬁrst extract motion edges and then create a watershed
map based on the edges. At last, we use NMS to obtain the key-
points. Low-quality ﬂow as shown in the third row results in a
large number of key-points which instead eases the pressure to
learn from those meaningless motions.

vided by [28], which originally contains 1.6M images from
205k video clips. We use the image pairs with an interval of
fewer than 10 frames in sequence to compute optical ﬂow.
For example, given a video clip containing 5 frames, and
the frame IDs are 1, 4, 10, 21, 28, we get 3 image pairs,
{1, 4}, {4, 10}, {21, 28}. We use the ﬁrst image in a pair
and the computed ﬂow to create an image-ﬂow pair. From
those frames, we create about 1.26M image-ﬂow pairs to
form the training set (hereinafter referred to as “YFCC”).
(b) YouTube9K. To show the beneﬁts from more unlabeled
data, we sample about 9,000 videos containing common ob-
jects from YouTube-8M [1]. We sample the videos using
keywords including “bird”, “cat”, “dog”, etc., which com-
monly exist in the visual world. Since CMP is an unsuper-
vised method, we do not use the tags in training. In the same
way, we create 1.96M image-ﬂow pairs from these videos.
(c) VIP and MPII. Apart from the above datasets with
general objects, we also use the videos in Video Instance-
level Parsing (VIP) dataset [9], and MPII Human Pose
Dataset [3]. They mainly contain multiple persons in vari-
ous events. The former results in 0.377M image-ﬂow pairs
and the latter 0.976M image-ﬂow pairs. We create the two
datasets aiming at training a human-centric CMP model, so
as to prove its effectiveness in understanding human kine-
matic properties. Of course, we do not use any annotations
from these two datasets.
Training Details. We implement our framework with Py-
Torch [27]. We resize the image and ﬂow so that the shorter
side is 416 and random crop to 384 × 384.
In guidance
sampling, for YFCC and YouTube9K, we set the NMS ker-
nel size K to be 81, and the grid stride G to be 200 pixels,
which results in averagely 9.5 watershed-based points and

1884

4 grid points per image. For VIP and MPII, K is 15 and
G is 80, when the images mainly contain multiple persons
whose degrees of freedom are high. We also analyze the
inﬂuence of the number of guidance points in Sec. 4.2.

is efﬁcient.

Training a CMP model

For example,
the ResNet-50 CMP model except that for human pars-
ing is trained for 42K iterations, about 5.3 epochs using
YFCC. It costs 7.5 hours on 16 GTX-1080-Ti GPUs. The
AlexNet CMP model is trained for 140K iterations using
YFCC+YouTube9K. The CMP model for LIP human pars-
ing is trained on all the 4 datasets for 70K iterations, about
2.1 epochs. The convergence is fast, hence we do not have
to train CMP for an excessive number of epochs. For all the
cases, we use SGD with learning rate 0.1, momentum 0.9,
weight decay 1e−4. We drop the learning by 10 times at
iteration 2

3.5 I, where I is the total iteration.

3.5 I and 3

4.1. Evaluations for Representation Learning

Using a CMP model as a pre-trained model, we show
its effectiveness in feature learning by ﬁne-tuning it on sev-
eral downstream tasks covering semantic segmentation, in-
stance segmentation, and human parsing. Most of the previ-
ous works report their transfer learning results on AlexNet.
However, AlexNet is regarded as obsolete. To build up-to-
date self-supervised learning baselines, we also perform ex-
periments with ResNet-50 in addition to AlexNet. Hence,
we adopt 4 benchmarks for evaluation, i.e., PASCAL VOC
2012 Semantic Segmentation (AlexNet), PASCAL VOC
2012 Semantic Segmentation (ResNet50), COCO 2017 In-
stance Segmentation (ResNet50), and LIP Human Parsing
(ResNet50). The ﬁne-tuning details can be found in the sup-
plementary materials.
Baselines. For AlexNet, most previous works report their
results on PASCAL VOC 2012 semantic segmentation.
However, previous studies do not support ResNet-50, hence
we have to reimplement them. For comparisons, we reim-
plemented recent works that use motion as supervision and
have achieved impressive results. Those methods include
Pathak et al. [28] and Walker et al. [31]. Among them,
Walker et al. [31] is a special case of CMP when the guid-
ance points number is zero. We optimize their hyper-
parameters to achieve their best performances in these
benchmarks.
VOC2012 Semantic Segmentation (AlexNet). Following
previous works, we ﬁne-tune the pre-trained weights on
AlexNet for PASCAL VOC 2012 semantic segmentation
task with FCN-32s [19] as the head. As shown in Table 1,
we achieve state-of-the-art performance with mIoU 44.5%
and surpass the baselines by a large margin.
VOC2012 Semantic Segmentation (ResNet-50).
As
shown in Table 2, we achieve 59.0% mIoU, with an 16.6%
improvement from a randomly initialized model. The per-
formance is also much higher than the baseline models.

Table 1. PASCAL VOC 2012 benchmark for semantic segmenta-
tion, with AlexNet. Our method achieves state-of-the-art and sur-
passes the baselines by a large margin. Methods marked † have not
reported the results in their paper, hence we reimplemented them
to obtain the results.

Method
(AlexNet)

Krizhevsky et al. [14]
Random
Pathak et al. [29]
Zhang et al. [34]
Zhang et al. [35]
Noroozi et al. [25]
Noroozi et al. [24]
Noroozi et al. [26]
Jenni et al. [13]
Larsson et al. [15]
Gidaris et al. [8]
Pathak et al. [28]†
Walker et al. [31]†
Mundhenk et al. [23]
Mahendran et al. [20]
Ours

Supervision

ImageNet labels
-
In-painting
Colorization
Split-Brain
Counting
Jigsaw
Jigsaw++
Spot-Artifacts
Colorization
Rotation
Video-Seg
Flow Prediction
Context
Flow similarity
CMP

VOC12 Seg.

% mIoU

48.0
19.8
29.7
35.6
36.0
36.6
37.6
38.1
38.1
38.4
39.1
39.7
40.4
40.6
41.4
44.5

Table 2. Results on PASCAL VOC 2012 Semantic Segmentaion
validation set and COCO 2017 Instance Segmentation validation
set, with ResNet-50.

Method
(ResNet-50)

ImageNet [14]
Random
Pathak [28]
Walker [31]
CMP (ours)

VOC12 Seg.

COCO17 (% mAP)

% mIoU

69.0
42.4
54.6
54.5
59.0

Det.

37.2
19.7
27.7
31.5
32.3

Seg.

34.1
18.8
25.8
29.2
29.8

COCO Instance Segmentation (ResNet-50). We con-
struct new baselines and the upper bound for self-supervised
learning on COCO Instance Segmentation. We use ResNet-
50 as the backbone and Mask R-CNN [10] with FPN [17] as
the head. As shown in Table 2, we achieve 32.3% bounding
box mAP and 29.8% mask mAP. It indicates that CMP is an
effective pre-training method for instance segmentation.
LIP Human Parsing (ResNet-50). Human parsing aims
at partitioning a human image into pre-deﬁned parts, e.g.,
head, arm, and leg. Look-Into-Person (LIP) [9] is a large-
scale benchmark for human parsing. We perform compar-
isons on the validation sets of two sub-tasks, including LIP
Single-Person Parsing and LIP Multi-Person Parsing. As
shown in Table 3, we surpass baseline methods on both sub-
tasks. We further assemble our model with the model pre-
trained on ImageNet, and observe higher performance than
either of them. It indicates that CMP pre-training is com-
plementary with ImageNet pre-training.

1885

Table 3. LIP Human Parsing results on the validation set, with
ResNet-50. The reported indicator is mIoU. The results marked
with ⋆ are obtained from the ensemble of our model and the model
pre-trained on ImageNet.

Method
(ResNet-50)

ImageNet [14]
Random
Pathak [28]
Walker [31]
CMP (ours)
CMP⋆ (ours)

LIP-Single

LIP-Multiple

42.5
32.5
36.6
36.7
40.2
42.9

55.4
35.0
50.9
52.5
52.9
55.8

VOC12 Semantic Segmentation (AlexNet)

U
o
I
m
%

 

46

44

42

40

38

0

13.5

50

100

150

200

Average number of guidances per image

Figure 4. Inﬂuence of guidance number.

4.2. Further Analysis

Inﬂuence of Guidance Number. The number of guidance
points is used to adjust the difﬁculty of pre-text CMP task.
An appropriate number of guidance points would allow a
more effective CMP learning from images. In this experi-
ment, we adjust the NMS kernel size K and grid stride G
to control the number of guidance points and perform an
evaluation on the VOC 2012 semantic segmentation task
with AlexNet. As shown in Figure 4, the performance is
low when the number of guidance is zero, and it is exactly
the case in [31]. The peak occurs when the average num-
ber of guidance points is 13.5. As further guidance points
join, the CMP task becomes easier. Then the needed in-
formation to recover motions mostly comes from guidance
rather than images. Hence, the image encoder is weakened
to capture essential information from images, and the per-
formance drops. Note that this optimal number of guidance
points is related to the number of objects, and the degrees
of freedom of each object in an image. When the number
of objects increases or the degrees of freedom goes higher,
the number of guidance points should also increase accord-
ingly.
Inﬂuence of Propagation Nets. Recall that the propaga-
tion nets are the combination of several CNNs with different
spatial strides. We study the inﬂuence of different combi-
nations of the propagation nets. We implement 4 propaga-

VOC12 Semantic Segmentation (AlexNet)

U
o
I
m
%

 

44.50

44.25

44.00

43.75

43.50

43.25

{ 1 }

{ 1,2 }

{ 1,2,4 }

{ 1,2,4,8 }

U
o
I
m
%

 

48

46

44

42

40

38

36

ImageNet

Ours

Larsson

Walker

Mahendran

Mundhenk
Pathak

Gidaris

Jenni,Noroozi

Zhang

0.5 1.0 1.5 2.0 2.5 3.0

Strides of propagation nets

Amount of Unlabeled Data

Figure 5. Inﬂuence of the combination of propagation nets and the
amount of unlabeled data.

tion nets with spatial stride 1, 2, 4, 8, and construct 4 com-
binations, {1}, {1, 2}, {1, 2, 4}, and {1, 2, 4, 8}. We test
them on PASCAL VOC 2012 semantic segmentation with
AlexNet. As shown in Figure 5, an optimal combination
occurs at {1, 2, 4}.
It indicates that the propagation nets
with different strides form a collaborative group to solve
the CMP problem effectively. However, additional propa-
gation net with overly large stride leads to the loss of spatial
information while increasing the parameter count in the de-
coder, hence a combination of {1, 2, 4, 8} strides is worse.
Nevertheless, the performance is still much better than the
baseline methods.
Inﬂuence of the Amount of Unlabeled Data. We cre-
ate 5 training sets using 1/4, 1/2 YFCC, full YFCC,
and YFCC+YouTube9K. The amount of data ranges from
0.32M to 3.22M . We test the AlexNet models trained re-
spectively on these training sets on PASCAL VOC 2012 se-
mantic segmentation task. As shown in Figure 5, as the
amount of unlabeled data increases, CMP achieves steady
improvements. The performance is much better than the
baselines in a comparable amount of unlabeled data.
CMP’s Characteristics. Given a test image exclusive to
the training set, we test a trained CMP model by giving ar-
bitrary guidance vectors. As shown in Figure 6, given an
increasing number of guidance vectors, CMP infers more
complete motions accordingly. The results clearly reﬂect
the structures of objects even with high degrees of freedom.
From the results, we observe three interesting characteris-
tics of CMP:

1) Rigidity-aware. Given a single guidance vector on a rigid
part, e.g., head, forearm, or thigh, CMP propagates motion
on the whole part.

2) Kinematically-coherent. Given a guidance vector on a
part, CMP is capable of inferring whether the part should be
shifting or rotating. As shown in the ﬁrst group in Figure 6,
the body should be shifting, then it predicts uniform motion
on the body, and the left leg should be rotating, hence the
motion is fading.

3) Physically-feasible. For example, in the ﬁrst column of

1886

the second group in Figure 6, given a single guidance vector
on the left thigh, there are responses on left thigh, shank,
and foot.
It is due to the observation that the left leg is
hovering. However, in the last column, given a guidance
vector on the right leg, the right foot keeps still, because it
is on the ground.

Motion, though coarse and noisy, is the manifestation of
kinematics and physics. To achieve sensible motion propa-
gation in complicated environments, our model must learn
to imagine the intrinsic kinematic properties and physically-
sound laws from static images. It accounts for these three
characteristics.

4.3. Applications

CMP shows its effectiveness in capturing structural kine-
matic properties of objects. With such characteristics, sev-
eral applications can be extended from a trained CMP
model. The image encoders for these applications are im-
plemented with ResNet-50.
CMP for Guided Video Generation. An interesting appli-
cation of CMP is guided video generation. With CMP, this
application is reminiscent of marionette control. Given an
image and the guidance arrows from a user, we ﬁrst use a
CMP model to predict the optical ﬂow and then warp the
original image to produce the future frame.
In this way,
we can create a sequence of frames by giving continuous
guidance. Since CMP is strong in perceiving rigid parts
of an object from a single image, we can perform sophisti-
cated marionette control on the image. A demo video can
be found in the project page 1.
CMP for Semi-automatic Pixel-level Annotation. We
show that CMP can also assist pixel-level annotation. Fig-
ure 7 (a) shows its workﬂow. A user only needs to click sev-
eral positive points on the object. We make dummy guid-
ance vectors on these points in different directions, then a
CMP model predicts the optical ﬂow in each direction. Fi-
nally, we fuse the results to obtain the mask. If the mask
covers some wrong areas, then the user clicks negative
points on the wrong areas. For CMP, the negative points
serve as the static guidance points with zero motion. Hence,
there will be no response around those negative points. In
this way, the mask gets reﬁned. Such interactive annotating
mode allows the user to freely reﬁne the mask via adding or
deleting the two types of points.

Since CMP is an unsupervised method, it does not pre-
deﬁne a speciﬁc category set like other semi-automatic an-
notation tools. Instead, it captures the spatial structures of
objects. Hence, we can use it to annotate any unseen or un-
common objects, e.g., carton, rearview mirror, and robot, as
shown in the second row of Figure 7 (b).

We compare our method with a state-of-the-art su-
pervised semi-automatic annotating method, Polygon

1Project page: http://mmlab.ie.cuhk.edu.hk/projects/CMP/

rotating

shifting

hovering

fixed

Figure 6. CMP testing results. In each group, the ﬁrst row includes
the original image and the guidance arrows given by users, the
second row shows the predicted motion. The results demonstrate
three characteristics of CMP: 1. CMP propagates motion on the
whole rigid part. 2. CMP can infer whether a part is shifting or
rotating (motion uniform if shifting, fading if rotating) as shown in
the ﬁrst group. 3. The results are physically feasible. For example,
in the second group, given a single guidance vector on the left
thigh, there are also responses on left shank and foot. It is due
to the observation that the left leg is hovering. However, in the
last column, although given a guidance vector on the right leg, the
right foot keeps still because it is on the ground.

RNN++ [2]. For a fair comparison, we test on the images
from the web demo of Polygon RNN++. As shown in Fig-
ure 7 (c), Polygon RNN++ requires a user to draw a bound-
ing box at ﬁrst, and then generates vertexes to form an ini-
tial mask. However, the initial mask is usually imperfect.
The user needs to drag the vertexes to reﬁne the mask. In
comparison, our method generates robust masks with only
a few clicks. The reﬁnement is also simple and intuitive,
conducted via interactive point-and-click to add or delete
points.

1887

positive
points

negative

points

Polygon RNN++

Ours

click points (by user)

guidances sets

flow prediction

fusion results

(a) workflow

draw 1 box

drag 15 times

result

7 clicks

result

draw 1 box

drag 10 times

result

5 clicks

result

(b) more results (first row: common, second row: uncommon)

(c) comparison

draw 1 box

drag 10 times

result

5 clicks

result

Figure 7. CMP for semi-automatic pixel-level annotation. (a) shows its workﬂow, where a user only needs to click several positive points
(green) on the object, then the mask is automatically generated. If the mask covers some wrong areas, then the user clicks negative points
(red) on the wrong areas, and the mask gets reﬁned. (b) shows a single CMP model is able to assist users to annotate objects in any category,
even the categories that the model has never seen. We compare our method with Polygon RNN++ in (c). For a fair comparison, we use the
images from the web demo of Polygon RNN++. It requires a user to draw a bounding box at ﬁrst and then drag the generated vertexes to
reﬁne. In some cases, it fails to capture the target object (second row). While our method does not require tedious dragging. It generates
robust masks with only a few clicks.

Polygon RNN++

Ours

7 clicks

result

8 clicks

result

normal

disconnected

multiply-
connected

5 clicks

result

Figure 8. This ﬁgure illustrates the limitations of Polygon RNN++,
and how CMP solves those cases.

In addition, Polygon RNN++ has some limitations as
shown in Figure 8: 1) In some cases, it fails to capture the
target object. 2) It cannot correctly segment objects with
disconnected regions (e.g., a car behind a tree.) 3) It can-
not handle multiply-connected objects (e.g., a doughnut).
While our method can handle all of those cases by clicking
positive and negative points. The comparisons are summa-
rized in Table 4. Note that Polygon RNN++ relies on su-
pervised models, while our method is unsupervised without
any manual annotations.

Table 4. Comparisons with Polygon RNN++.
“sup” and “un-
sup” stand for “supervised” and “unsupervised”. “MC” and “DC”
stand for whether they support “multiply-connected” and “discon-
nected” objects. Time per instance is tested on a randomly chosen
subset from COCO dataset.
model

fail

Method
Polygon [2]
Ours

speed
17.6s
10.2s

sup

unsup

25/170
0/170

MC DC
✘
✘

✔

✔

5. Conclusion

To summarize, we propose a new self-supervised learn-
ing paradigm, Conditional Motion Propagation (CMP). It
learns effective visual representations for structural predic-
tion. We achieve state-of-the-art performance in standard
self-supervised representation learning benchmarks. We
also establish new benchmarks with ResNet-50 beyond just
AlexNet. CMP shows appealing characteristics in captur-
ing kinematic properties of various objects with unlabeled
data. We observe kinematically sound results when testing a
CMP model. Furthermore, CMP can be extended to several
useful applications. For semi-automatic pixel-level annota-
tion, we achieve encouraging usability when compared with
a state-of-the-art supervised method.
Acknowledgement: This work is partially supported by
the Collaborative Research grant from SenseTime Group
(CUHK Agreement No. TS1712093), the Early Career
Scheme (ECS) of Hong Kong (No. 24204215), the Gen-
eral Research Fund (GRF) of Hong Kong (No. 14241716,
14224316. 14209217, 14236516, 14203518), and Singa-
pore MOE AcRF Tier 1 (M4012082.020).

1888

References

[1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul
Natsev, George Toderici, Balakrishnan Varadarajan, and
Youtube-8m: A large-
Sudheendra Vijayanarasimhan.
scale video classiﬁcation benchmark.
arXiv preprint
arXiv:1609.08675, 2016. 4

[2] David Acuna, Huan Ling, Amlan Kar, and Sanja Fidler. Ef-
ﬁcient interactive annotation of segmentation datasets with
polygon-rnn++. In CVPR, 2018. 7, 8

[3] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and
Bernt Schiele. 2D human pose estimation: New benchmark
and state of the art analysis. In CVPR, June 2014. 4

[4] Serge Beucher. Use of watersheds in contour detection. In
Proceedings of the International Workshop on Image Pro-
cessing. CCETT, 1979. 4

[5] John Canny. A computational approach to edge detection.

TPAMI, (6):679–698, 1986. 4

[6] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-
vised visual representation learning by context prediction. In
ICCV, 2015. 2

[7] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical ﬂow with convolutional networks. In ICCV,
2015. 3

[8] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-
supervised representation learning by predicting image rota-
tions. In ICLR, 2018. 2, 5

[9] Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen,
and Liang Lin. Look into person: Self-supervised structure-
sensitive learning and a new benchmark for human parsing.
In CVPR, 2017. 4, 5

[10] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-

shick. Mask R-CNN. In ICCV. IEEE, 2017. 5

[11] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Lite-
FlowNet: A lightweight convolutional neural network for
optical ﬂow estimation. In CVPR, 2018. 4

[12] Dinesh Jayaraman and Kristen Grauman. Slow and steady
feature analysis: higher order temporal coherence in video.
In CVPR, 2016. 2

[19] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
In

convolutional networks for semantic segmentation.
CVPR, 2015. 5

[20] A. Mahendran, J. Thewlis, and A. Vedaldi. Cross pixel op-
tical ﬂow similarity for self-supervised learning. In ACCV,
2018. 2, 5

[21] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuf-
ﬂe and learn: unsupervised learning using temporal order
veriﬁcation. In ECCV. Springer, 2016. 2

[22] Hossein Mobahi, Ronan Collobert, and Jason Weston. Deep
learning from temporal coherence in video. In ICML. ACM,
2009. 2

[23] T Nathan Mundhenk, Daniel Ho, and Barry Y Chen.
Improvements to context based self-supervised learning.
CVPR, 2018. 5

[24] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of
visual representations by solving jigsaw puzzles. In ECCV.
Springer, 2016. 2, 5

[25] Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Rep-
In ICCV, 2017.

resentation learning by learning to count.
5

[26] Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, and
Hamed Pirsiavash. Boosting self-supervised learning via
knowledge transfer. In CVPR, 2018. 5

[27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017. 4

[28] Deepak Pathak, Ross B Girshick, Piotr Doll´ar, Trevor Dar-
rell, and Bharath Hariharan. Learning features by watching
objects move. In CVPR, 2017. 2, 4, 5, 6

[29] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In CVPR, 2016. 1, 2, 5

[30] Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial
Hebert. An uncertain future: Forecasting from static images
using variational autoencoders. In ECCV. Springer, 2016. 2
[31] Jacob Walker, Abhinav Gupta, and Martial Hebert. Dense
optical ﬂow prediction from a static image. In ICCV, 2015.
2, 3, 5, 6

[13] Simon Jenni and Paolo Favaro. Self-supervised feature learn-

[32] Xiaolong Wang and Abhinav Gupta. Unsupervised learning

ing by learning to spot artifacts. In CVPR, 2018. 5

of visual representations using videos. In ICCV, 2015. 2

[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NeurIPS, 2012. 5, 6

[33] Donglai Wei,

Joseph Lim, Andrew Zisserman,

and
William T Freeman. Learning and using the arrow of time.
In CVPR, 2018. 2

[15] Gustav

Larsson, Michael Maire,

and Gregory
Shakhnarovich. Colorization as a proxy task for visual
understanding. In CVPR, 2017. 1, 2, 5

[16] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-
Hsuan Yang. Unsupervised representation learning by sort-
ing sequences. In ICCV. IEEE, 2017. 2

[17] Tsung-Yi Lin, Piotr Doll´ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, 2017. 5

[18] Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and
Aseem Agarwala. Video frame synthesis using deep voxel
ﬂow. In ICCV, 2017. 2

[34] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful

image colorization. In ECCV. Springer, 2016. 1, 2, 5

[35] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain
autoencoders: Unsupervised learning by cross-channel pre-
diction. In CVPR, 2017. 5

1889

