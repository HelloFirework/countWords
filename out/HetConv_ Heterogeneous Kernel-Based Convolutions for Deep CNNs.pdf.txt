HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs

Pravendra Singh

Vinay Kumar Verma

Piyush Rai

Vinay P. Namboodiri

Department of Computer Science and Engineering, IIT Kanpur, India

{psingh, vkverma, piyush, vinaypn}@cse.iitk.ac.in

Abstract

We present a novel deep learning architecture in which
the convolution operation leverages heterogeneous kernels.
The proposed HetConv (Heterogeneous Kernel-Based Con-
volution) reduces the computation (FLOPs) and the num-
ber of parameters as compared to standard convolution op-
eration while still maintaining representational efﬁciency.
To show the effectiveness of our proposed convolution,
we present extensive experimental results on the standard
convolutional neural network (CNN) architectures such as
VGG [31] and ResNet [8]. We ﬁnd that after replacing
the standard convolutional ﬁlters in these architectures with
our proposed HetConv ﬁlters, we achieve 3X to 8X FLOPs
based improvement in speed while still maintaining (and
sometimes improving) the accuracy. We also compare our
proposed convolutions with group/depth wise convolutions
and show that it achieves more FLOPs reduction with sig-
niﬁcantly higher accuracy.

1. Introduction

Convolutional neural networks [8, 19, 31] have shown
remarkable performance in domains like Vision and NLP.
The general trend to improve performance further has made
models more complex and deeper. Increasing the accuracy
by increasing model complexity with a deeper network is
not for free; it comes with the cost of a tremendous in-
crease in computation (FLOPs). Therefore, various types of
convolution operations/convolutional ﬁlters have been pro-
posed to reduce FLOPs to the model more efﬁcient.

Existing convolutional ﬁlters can be roughly divided
into three categories: 1- Depthwise Convolutional Filter
to perform depthwise convolution (DWC) [39], 2- Point-
wise Convolutional Filter to perform pointwise convolution
(PWC) [37] and 3- Groupwise Convolutional Filter to per-
form groupwise convolution (GWC) [19]. Most of the re-
cent architectures [12, 36, 2, 15, 38, 43] use a combination
of these convolutional ﬁlters to make the model efﬁcient.
Using these convolutions (e.g., DWC, PWC, and GWC),
many of the popular models [15, 12, 2] have explored new

architectures to reduce FLOPs. However, designing a new
architecture requires a lot of work to ﬁnd out the best com-
bination of ﬁlters that result in minimal FLOPs.

Another popular approach to increase the efﬁciency of a
model is to use model compression [28, 1, 24, 11, 21, 10,
42]. Model compression can be broadly categorized into
three categories: connection pruning [6], ﬁlter pruning [24,
11, 21, 10, 33, 32, 34] and quantization [6, 28].

In ﬁlter pruning, the idea is to prune a ﬁlter that has a
minimal contribution in the model, and after removing this
ﬁlter/connection, the model is usually ﬁne-tuned to main-
tain its performance. While pruning the model, we require a
pre-trained model (possibly requiring a computationally ex-
pensive training as a preprocessing step), and then later we
discard the ﬁlter that has a minimal contribution. Hence it
is a very costly and tricky process. Therefore, using an efﬁ-
cient convolutional ﬁlter or convolution operation to design
an efﬁcient architecture is a more popular approach than
pruning. This does not require expensive training and then
pruning since training is done from scratch efﬁciently.

Using efﬁcient convolutional ﬁlters, there are two differ-
ent objectives. One kind of work focuses on designing ar-
chitectures that have minimal FLOPs while compromising
on accuracy. These works focus on developing the model
for the IoT/low-end device [12, 43]. These models suffer
from the low accuracy hence they have to search the best
possible model to create a balance between accuracy and
FLOPs. So there is a tradeoff between FLOPs and model
accuracy.

Another set of work focuses on increasing accuracy
while keeping the model FLOPs the same as the original
architecture. The recent architectures, such as Inception
[36], RexNetXt [41] and Xception [2] are examples of this
kind of work. Their objective is to design a more complex
model using efﬁcient convolutional ﬁlters while keeping the
FLOPs the same as the base model. It is usually expected
that a more complex model would learn better features, re-
sulting in better accuracies. However, these methods are
not focused on designing a new architecture, but primarily
on using existing efﬁcient ﬁlters in standard base architec-
tures. Therefore these works keep the number of layers and

4835

the architecture the same as the base model and increase
the ﬁlters on each layer such that it does not increase the
FLOPs.

In contrast to these two approaches, the primary fo-
cus of our work is to reduce the FLOPs of the given
model/architecture by designing new kernels, without com-
promising on the loss of accuracy. Experimentally we ﬁnd
that the proposed approach has much lower FLOPs than
the state-of-art pruning approaches while maintaining the
accuracy of the base model/architecture. The pruning ap-
proaches are very costly and show a signiﬁcant drop in ac-
curacy to achieve FLOPs compression.

In the proposed approach, we are choosing a different
strategy to increase the efﬁciency of the existing model
without sacriﬁcing accuracy. An architecture search re-
quires years of research to get an optimized architecture.
Therefore, instead of designing a new efﬁcient architecture,
we design an efﬁcient convolution operation (convolutional
ﬁlter) that can be directly plugged into any existing standard
architecture to reduce FLOPs. To achieve this, we propose
a new type of convolution - heterogeneous convolution.

The convolution operation can be divided into two cate-

gories based on the types of the kernel:

• Homogeneous convolution using a traditional convolu-
tional ﬁlter (for example standard convolution, group-
wise convolution, depthwise convolution, pointwise
convolution). Homogeneous convolution can be per-
formed using a homogeneous ﬁlter. A ﬁlter is said to
be homogeneous if it contains all kernels of the same
size (for example, in a 3 × 3 × 256 CONV2D ﬁlter, all
256 kernels will be of size 3 × 3).

• Heterogeneous convolution uses a heterogeneous con-
volutional ﬁlter (HetConv). A ﬁlter is said to be het-
erogeneous if it contains different sizes of kernels (for
example, in a HetConv ﬁlter, out of 256 kernels some
kernels are of size 3 × 3 and remaining kernels are of
size 1 × 1).

Using a heterogeneous ﬁlter in deep CNN overcomes the
limitation of the existing approaches that are based on efﬁ-
cient architecture search and model compression. One of
the latest efﬁcient architecture MobileNet [12] uses depth-
wise and pointwise convolution. The standard convolu-
tional layer is replaced by two convolutional layers; hence it
has more latency (latency one). Please refer to Section-3.3
and Figure-4 for more details about latency. But our pro-
posed HetConv has same latency as the original architecture
(latency zero) unlike [12, 36, 37, 2] that have latency greater
than zero.

Compared to model compression that suffers from high
accuracy drop, our approach is very competitive to the state-
of-art result of the standard model like ResNet [8] and VG-
GNet [31]. Using HetConv ﬁlters, we can train our model

from scratch, unlike pruning approaches that need a pre-
trained model, without sacriﬁcing accuracy. The pruning
approaches also suffer from sharp accuracy drop if we in-
crease the degree of FLOP pruning. Using proposed Het-
Conv ﬁlters, we have state-of-art result regarding FLOPs
compare to the FLOP pruning methods. Also, the pruning
process is inefﬁcient as it takes a lot of time in training and
ﬁne tuning after pruning. Our approach is highly efﬁcient
and gives a similar result compared to the original model
while training from scratch.

To the best of our knowledge, this is the ﬁrst convolu-
tion/ﬁlter that is heterogeneous. This heterogeneous design
helps to increase efﬁciency (FLOPs reduction) of the exist-
ing architecture without sacriﬁcing accuracy. We did exten-
sive experiment on different architectures like ResNet [8],
VGG-16 [31] etc just by replacing their original ﬁlters to
our proposed ﬁlters. We found that without sacriﬁcing the
accuracy of these models, we have a high degree of FLOPs
reduction (3X to 8X). These FLOPs reductions are even sig-
niﬁcantly better as compared to existing pruning approach.

Our main contributions are as follows:

• We design an efﬁcient heterogeneous convolutional ﬁl-
ter, that can be plugged into any existing architecture
to increase the efﬁciency (FLOPs reduction of order
3X to 8X) of the architecture without sacriﬁcing the
accuracy.

• The proposed HetConv ﬁlters are designed in such a
way that it has zero latency. Therefore, there is a neg-
ligible delay from input to output.

2. Related Work

The recent success of deep neural network [19, 29, 8,
14, 4, 5, 26, 40, 35, 27] depends on the model design. To
achieve a minimal error rate, the model becomes more and
more complex. The complex and deeper architecture con-
tain millions of parameters and requires billions of FLOPs
(computations) [8, 31, 14]. These models require machines
with high-end speciﬁcations, and these type of architecture
are very inefﬁcient on low computing resources. This raises
interest in designing efﬁcient models [7]. The work to in-
crease the efﬁciency of the model can be divided into two
parts.

2.1. Efﬁcient Convolutional Filter

To design the efﬁcient architecture recently few novel
convolutional ﬁlters have been proposed. Among them
Groupwise Convolution (GWC) [19], Depthwise convolu-
tion (DWC) [39] and Pointwise Convolution (PWC) [8] are
the popular convolutional ﬁlters. These are widely used to
design efﬁcient architecture. GoogleNet [37] use the incep-
tion module and irregular stacking architecture. Inception

4836

module uses GWC and PWC to reduce FLOPs. ResNet
[8, 9] uses a bottleneck structure to design an efﬁcient ar-
chitecture with residual connection. They use PWC and the
standard convolution that help to go deeper without increas-
ing the model parameter and reduces the FLOPs explosion.
Therefore they can design a much deeper architecture com-
pare to VGG [31]. ResNetxt [41] use the ResNet archi-
tecture and they divide each layer with GWC and PWC.
Therefore without increasing FLOPs, they can increase the
cardinality 1. They show that increasing cardinality is much
more effective than a deeper or wider network. SENet [13]
design a new connection that gives the weight to each out-
put feature map with a minor increase in FLOPs but shows
a boost in the performance.

MobileNet [12] is another popular architecture specially
designed for the IoT devices contains DWC and PWC. This
architecture is very light and highly efﬁcient in term of
FLOPs. This reduction in FLOPs are not for free and come
with the cost of a drop in the accuracy compared to the
state-of-art models. [17, 16] use different types of convo-
lutional ﬁlers at the same layers, but each ﬁlter performs a
homogeneous convolution due to the presence of same types
of kernels in each ﬁlter. Using different types of convolu-
tional ﬁlers at the same layers also helps in reducing param-
eters/FLOPs. In our proposed convolution, the convolution
operation is heterogeneous due to the presence of different
types of kernels in each ﬁlter.

2.2. Model Compression

Another popular approach to increase the efﬁciency of
CNN is model compression. These can be categorised as: 1-
Connection Pruning [6, 44], 2- Filter Pruning [23, 3, 33, 32,
34] and 3- Bit Compression [28]. Filter pruning approaches
are more effective as compared to other approaches and
give a high compression rate in terms of FLOPs. Also,
the ﬁlter pruning approaches do not need any special hard-
ware/software support (sparse library).

Most of the works in ﬁlter pruning calculates the impor-
tance of the ﬁlter and prunes them based on some criteria
followed by re-training to recover the accuracy drop. [20]
used l1 norm as a metric for ranking ﬁlters. But the prun-
ing is done on the pre-trained model and involves iterative
training and the pruning which is costly. Also, ﬁlter prun-
ing shows a sharp accuracy drop in accuracy, if the degree
of ﬂop pruning increases [3, 42, 24].

3. Proposed Method

In this work, we propose a novel ﬁlter/convolution (Het-
Conv) that contains a heterogeneous kernel (e.g., few ker-
nels are of size 3 × 3, and others may be 1 × 1) to reduce
the FLOPs of existing models with the same accuracy as the

Figure 1. Difference between standard convolutional ﬁlter (homo-
geneous) and heterogeneous convolutional ﬁlter (HetConv). Here
M is the input depth (number of input channels), and P is the part.
Out of M kernels, M/P kernels will be of size 3 × 3 and remaining
will be 1 × 1 kernels.

original model. This is very different from the standard con-
volutional ﬁlter that is made of homogeneous kernels (say
all 3 × 3 or all 5 × 5). The heterogeneous ﬁlter is very efﬁ-
cient in terms of FLOPs. It can be approximated as a com-
bined ﬁlter of a groupwise convolutional ﬁlter (GWC) and
pointwise convolutional ﬁlter (PWC). To reduce the FLOPs
of a convolutional layer, we generally replace it by two or
more layers (GWC/DWC and PWC), but it increases the la-
tency because next layer’s input is the previous layer’s out-
put. Hence all computations have to be done sequentially to
get the correct output. In contrast, our proposed HetConv
has the same latency. Difference between the standard ﬁlter
and HetConv ﬁlter is shown in the Figure- 1 and Figure- 2.

In the standard convolutional layer, let us assume input
(input feature map) of size Di × Di × M . Here Di is the
input square feature map spatial width and height and M is
the input depth (number of input channels). Also consider
Do × Do × N is the output feature map. Here Do is the
output square feature map spatial width and height and N
is the output depth (number of output channels). An output
feature map is obtained by applying the N ﬁlters of size
K × K × M . Here K is the kernel size. Therefore the total
computational cost at this layer L can be given as:

1The size of the set of transformations

F LS = Do × Do × M × N × K × K

(1)

4837

Figure 2. Comparison between the proposed Convolutional ﬁlters (HetConv) with other efﬁcient convolutional ﬁlters. Our heterogeneous
ﬁlters have zero latency while other (GWC+PWC or DWC+PWC) have a latency of one unit.

It is clear from the Equation-1 the computational cost de-
pends on the kernel size (K), feature map size, input chan-
nels M and output channels N . This computational cost is
very high which can be further reduced by carefully design-
ing the new convolution operation. To reduce the high com-
putation, various convolutions like DWC, PWC and GWC
are proposed which is used in the many recent architecture
[12, 43, 36] to reduce the FLOPs but all of them increase
the latency.

The standard convolution operation and some recent
convolution operations [12, 43, 36] use a homogeneous ker-
nel (i.e., each kernel is of the same size for the whole ﬁlter).
Here to increase the efﬁciency we are using the heteroge-
neous kernels. This contains different size kernels for the
same ﬁlter. Please refer to Figure-3 to visualize all ﬁlters
at a particular layer L. Let us deﬁne part P which controls
the number of different types of kernels in a convolutional
ﬁlter. For part P , a fraction 1/P out of total kernels will be
for K × K kernels and remaining fraction (1 − 1/P ) will
be for 1 × 1 kernels. For better understanding, Let’s take an
example, in a 3 × 3 × 256 standard CONV2D ﬁlter if you
replace (1 −1/P ) ∗256, 3 ×3 kernels with 1 ×1 (along with
the central axis), you will get a HetConv ﬁlter with part P .
Please refer to Figure-1 and 2.

The computational cost for K × K size kernels in the

HetConv ﬁlters with part P on the layer L is given as:

F LK = (Do × Do × M × N × K × K)/P

(2)

It reduces the cost P times since instead of M kernels of
size K ×K, now we have only M/P kernels of size K ×K.
The remaining (M − M/P ) kernels are of size 1 × 1.
The computational cost of the remaining 1 × 1 kernels can
be given as:

Figure 3. Convolutional ﬁlters at a layer L: Proposed Convolu-
tional Filter (HetConv) using Heterogeneous Kernel. In this Fig-
ure, each channel is made of using the heterogeneous kernel of
size 3 × 3 and 1 × 1. Replacing 3 × 3 kernels with 1 × 1 kernels
in standard convolutional ﬁlter reduces the FLOPs dramatically
while maintaining the accuracy. Filters of a particular layer are ar-
ranged in a shifted manner (i.e., if the ﬁrst ﬁlter starts 3 × 3 kernel
from the ﬁrst position then the second ﬁlter starts the 3 × 3 kernel
from the second position and so on).

Therefore the total computational cost at layer L is given as:

F LHC = F LK + F L1

(4)

The total reduction (R) in the computation as compared

to standard convolution can be given as:

RHetConv =

F LK + F L1

F LS

=

1
P

+

(1 − 1/P )

K 2

(5)

In the Equation-5 if we put P = 1 then it becomes standard
convolutional ﬁlter.

F L1 = (Do × Do × N ) ×(cid:18)M −

M

P (cid:19)

(3)

By reducing the size of the ﬁlter on some channels from
says 3 × 3 to 1 × 1, we are reducing the spatial extent of a

4838

ﬁlter. However, by retaining the size to be 3 × 3 on some
channel, we ensure that the ﬁlter does cover the spatial cor-
relation on some channels and need not to have the same
spatial correlation on all channels. We observe in the ex-
perimental section that by doing so, one can obtain similar
accuracies as a homogeneous ﬁlter. On the other hand, if
we avoid and retain a 1 × 1 ﬁlter size on all channels, then
we would not have the necessary spatial correlation infor-
mation covered, and the accuracy would suffer.

3.1. Comparision with DepthWise followed by

PointWise Convolution

In the extreme case when P = M in HetConv, HetConv
can be compared with DWC+PWC (DepthWise followed
by PointWise Convolution). MobileNet [12] use this type
of convolution. While MobileNet takes more FLOPs than
our extreme case with more delay since MobileNet [12] has
latency one.

The total FLOPs for DWC+PWC (MobileNet) for layer

L can be computed as:

F LM obN et = Do × Do × M × K × K + M × N × Do × Do
(6)
Therefore the total reduction in computation as compared
to the standard convolution:

RM obN et =

F LM obN et

F LS
1
K 2

+

=

1
N

It is clear from the Equation-5 that we can change the
part P value to trade off between the accuracy and FLOPs.
If we decrease the P value, the resulting convolution will
be closer to the standard convolution. To show the effec-
tiveness of the proposed HetConv ﬁlter, we have shown re-
sults in the experimental section where HetConv achieves
signiﬁcantly better accuracy with similar FLOPs.

In the extreme case when P = M , from Eq.-5 and 7 (for

MobileNet N = M ),we can conclude:

1
M

+

(1 − 1/M )

K 2

<

1
M

+

1
K 2

(8)

Reduction = Total reduction in the computation as com-

pared to standard convolution

Speedup =

1

Reduction

(9)

Therefore from Eq.-8, it is clear that MobileNet takes
more computation than our approach. In our HetConv, we
have latency zero while MobileNet has latency one. In this
extreme case, we have signiﬁcantly better accuracy than
MobileNet (refer to the experiment section).

(7)

Similarly when P = G, from Eq.-5 and 11 we have:

Figure 4. The ﬁgure shows the comparison with the different types
of convolution in terms of latency.

3.2. Comparision with GroupWise followed by

PointWise Convolution

In the case of groupwise convolution followed by point-
wise convolution (GWC+PWC) with the group size G, the
total FLOPs for GWC+PWC for layer L can be computed
as:

F LG = (Do×Do×M ×N ×K ×K)/G+M ×N ×Do×Do
(10)
Therefore the total reduction in the computation as compare
to the standard convolution:

RGroup =

=

F LG
F LS
1
G

+

1
K 2

(11)

1
P

+

(1 − 1/P )

K 2

<

1
P

+

1
K 2

(12)

Therefore from Eq.-12, it is clear that GWC+PWC takes
more computation than our approach. In our HetConv, we
have latency zero while GWC+PWC has latency one.

3.3. Running Latency

Most of the previous approaches [36, 37, 43, 12] de-
signed efﬁcient convolution to reduce the FLOPs, but they
increase the latency2 in the architecture. The latency in the
different types of convolutions is shown in the Figure-4. In
the Inception module [37, 36], one layer is broken down
into two or more sequential layers. Therefore, the latency
in architecture is greater than zero. In the Xception [2] ﬁrst
GWC is applied, and on the output of GWC, PWC is ap-
plied. PWC waits for the completion of the GWC. Hence
this approach reduces the FLOPs but increases latency in
the system. Similarly in the MobileNet [12] ﬁrst DWC and
then PWC is applied therefore it has latency one. This la-
tency includes a delay in parallel devices like GPU. In our

2One parallel step is converted to multiple sequential step hence reduc-
tion in parallelizability. Later stage of layers waits for the execution to be
ﬁnished on the previous stage because all computations have to be done
sequentially across layers

4839

depthwise convolution followed by pointwise convolution;
XXX PC: PC is part value P = number of input channels
(input depth).

4.2. VGG 16 on CIFAR 10

In this experiment, we use the VGG-16 architecture [31].
In the CIFAR-10 dataset, each image size is of 32 × 32 size
on the RGB scale.
In VGG-16 architecture, there are 13
convolutional layers which use standard CONV2D convo-
lution, and after each layer, we add batch normalization. We
are using the same setting as described in [20]. The values
of hyper-parameters are: weight decay = 5e-4, batch size =
128, initial learning rate = 0.1 which is decade by 0.1 after
every 50 epochs.

Except for the initial convolution layer (i.e., CONV 1),
All remaining 12 standard convolutional layers are replaced
by our HetConv layers (same P value for all 12 layers)
while keeping the number of ﬁlers same as earlier. As
shown in Table-1,
the values
of FLOPs (computation) decreases without any signiﬁcant
drop in accuracy. We also experimented for HetConv with
SE technique and found that SE increases the accuracy ini-
tially, but later due to over-ﬁtting, it starts degrading the
model performance (accuracy) as shown in Table-1.

the value of P increase,

4.2.1 Comparison with groupwise followed by point-

wise convolution

We experimented with groupwise followed by pointwise
convolution, where all standard convolutional layers (except
the initial convolution layer, i.e., CONV 1) are replaced by
two layers (groupwise convolutional layer with group size
4 and pointwise convolutional layer). Now the model has
latency one. As shown in Table-1, VGG-16 GWC4 PWC
has 92.76% accuracy whereas our model VGG-16 P4 has
signiﬁcantly higher 93.93% accuracy with lesser FLOPs.

4.2.2 Comparison with depthwise followed by point-

wise convolution

We experimented with depthwise followed by pointwise
convolution, where all standard convolutional layers (except
the initial convolution layer, i.e., CONV 1) are replaced
by two layers (depthwise convolutional layer and pointwise
convolutional layer). Now the model has latency one. As
shown in Table-1, VGG-16 DWC PWC has only 91.27%
accuracy on CIFAR-10 whereas our model VGG-16 P64
has signiﬁcantly higher 93.42% accuracy with comparable
FLOPs.

We also experimented with different P values for dif-
ferent layers. Except for the initial convolution layer
(i.e., CONV 1), all remaining 12 standard convolutional
layers are replaced by our HetConv layers with P =
number of input channels. As shown in Table-1, our model

4840

Figure 5. Speedup over standard convolution for different values
of P for a HetConv Filter with 3 × 3 and 1 × 1 kernels.

proposed approach any layers are not replaced by sequen-
tial layers hence has the latency zero. We directly design
the ﬁlter such that without increasing any latency we can
reduce the FLOPs. Our proposed approach is very compet-
itive in terms of FLOPs as compared to previous efﬁcient
convolutions while maintaining the latency zero.

3.4. Speedup over standard convolution for differ 

ent values of P

As shown in Figure-5, Speedup increases with the P
value. We can use P value to trade off between the accu-
racy and FLOPs. If we decrease the P value, the resulting
convolution will be closer to the standard convolution. To
show the effectiveness of the proposed HetConv ﬁlter, we
have shown results in the experimental section where Het-
Conv achieved signiﬁcantly better accuracy with respect to
the other types of convolution with similar FLOPs.

4. Experiments and Results

To show the effectiveness of the proposed HetConv ﬁlter
we perform extensive experiments with the current state-of-
art architectures. We replaced their standard convolutional
ﬁlters from these architecture with the proposed one. We
performed three large scale experiment on the ImageNet
[30] with the ResNet-34, ResNet-50 [8] and VGG-16 [31]
architectures. We have shown three small scale experiment
on the CIFAR-10 [18] for the VGG-16, ResNet-56, and Mo-
bileNet [12] architectures. We set the value of the reduction
ratio to 8 for Squeeze-and-Excitation (SE) [13] in all our
experiments.

4.1. Notations

XXX Pα: XXX is the architecture, and part value is
P = α; XXX Pα SE: SE for Squeeze-and-Excitation with
reduction-ratio = 8; XXX GWCβ PWC: GWCβ PWC is
the groupwise convolution with group size β followed by
pointwise convolution; XXX DWC PWC: DWC PWC is

Model

VGG-16 P1
VGG-16 P1 SE

VGG-16 P2
VGG-16 P2 SE

VGG-16 P4
VGG-16 P4 SE
VGG-16 GWC4 PWC

VGG-16 P8
VGG-16 P8 SE

VGG-16 P16
VGG-16 P16 SE

VGG-16 P32
VGG-16 P32 SE

VGG-16 P64
VGG-16 P64 SE

VGG-16 DWC PWC
VGG-16 PC
VGG-16 PC SE

Acc%

94.06
94.13

93.89
94.11

93.93
94.29
92.76

93.92
93.97

93.96
93.63

93.73
93.41

93.42
93.33

91.27
92.53
93.08

FLOPs

313.74M
314.19M

175.23M
175.67M

105.98M
106.42M
107.67M

71.35M
71.79M

54.04M
54.48M

45.38M
45.82M

41.05M
41.49M

38.53M
38.18M
38.62M

FLOPs Reduced (%)

Parameters

Parameters Reduced (%)

–
–

44.15
44.00

66.22
66.08
65.68

77.26
77.12

82.78
82.64

85.54
85.39

86.92
86.77

87.72
87.83
87.69

15.00M
15.22M

8.45M
8.68M

5.17M
5.41M
5.42M

3.54M
3.77M

2.72M
2.95M

2.31M
2.54M

2.11M
2.34M

1.97M
1.93M
2.15M

–
–

43.68
42.99

65.45
64.48

–

76.40
75.22

81.86
80.59

84.58
83.28

85.95
84.63

–
–
–

Table 1. The table shows the detail results for VGG-16 on CIFAR-10 in different setups.

Method

Li-pruned [21]
SBP [25]
SBPa [25]
AFP-E [3]
AFP-F [3]

VGG-16 P32 (Ours)
VGG-16 P64 (Ours)

Error%

FLOPs Reduced(%)

6.60
7.50
9.00
7.06
7.13

6.27
6.58

34.20
56.52
68.35
79.69
81.39

85.54
86.92

Table 2. The table shows the comparison of our models with state-
of-art model compression methods for VGG-16 architecture on the
CIFAR-10 dataset.

Method

Li-B [21]
NISP [42]
CP [11]
SFP [10]
AFP-G [3]

ResNet-56 P1
ResNet-56 P2
ResNet-56 P4

ResNet-56 P1 SE
ResNet-56 P2 SE
ResNet-56 P4 SE

Error%

FLOPs Reduced (%)

6.94
6.99
8.20
6.65
7.06

6.41
6.40
6.71

7.16
6.75
7.79

27.6
43.6
50.0
52.6
60.9

–

44.30
66.45

–

44.27
66.42

VGG-16 PC and VGG-16 PC SE still performing better
than VGG-16 DWC PWC which shows the superior per-
formance of our HetConv over DWC+PWC.

Table 3. The table shows the detail results and comparison with
state-of-art model compression methods for ResNet-56 on CIFAR-
10 in different setups.

4.2.3 Comparison with FLOPs compression methods

As shown in Table-2, our models VGG-16 P32, and VGG-
16 P64 have signiﬁcantly better accuracy as compare to
state-of-art model compression methods. We reduced ∼
85% FLOPs with no loss in accuracy whereas FLOPs com-
pression methods suffer a signiﬁcant loss in accuracy (more
than 1%) as shown in Table-2.

ers are replaced by our HetConv layers while keeping the
number of ﬁlers the same as earlier.

As shown in Table-3, our models ResNet-56 P2, and
ResNet-56 P4 have signiﬁcantly better accuracy as com-
pare to state-of-art model compression methods with higher
FLOPs reduction. We also experimented for HetConv with
SE technique and found that SE performance is not as ex-
pected due to over-ﬁtting.

4.3. ResNet 56 on CIFAR 10

4.4. MobileNet on CIFAR 10

We experimented with ResNet-56 architecture [8] on the
CIFAR-10 dataset. ResNet-56 consists of three stages of
the convolutional layer of size 16-32-64 where each convo-
lution layer in each stage contains the same 2.36M FLOPs,
and the total FLOP is 126.01M. We trained the model using
the same parameters proposed by [8]. Except for the initial
convolution layer, all remaining standard convolutional lay-

We experimented with MobileNet architecture on the
CIFAR-10 dataset. Except for the initial convolution layer,
all remaining convolutional layers are replaced by our Het-
Conv layers while keeping the number of ﬁlers the same as
earlier. In our model, two convolutional layers (depthwise
convolutional layer and pointwise convolutional layer) is re-
placed by one HetConv convolutional layer which reduces

4841

Method

Accuracy (%)

MobileNet [12]
MobileNet P32
MobileNet P32 SE

91.17
92.06
92.17

FLOPs

46.36M
55.94M
56.91M

Table 4. The table shows the results for MobileNet [12] on CIFAR-
10 in different setups.

Method

Acc%(Top-1)

Acc%(Top-5)

FLOPs Reduced %

RNP (3X)[22]
ThiNet-70 [24]
CP 2X [11]

VGG-16 P1
VGG-16 P4

–

69.8

–

71.3
71.2

87.57
89.53
89.90

90.2
90.2

66.67
69.04
50.00

–

65.8

Table 5. Table shows the results for the VGG-16 on ImageNet
[30]. Our model has no loss in accuracy as compare to state-of-
art [11, 24] pruning approaches while signiﬁcantly higher FLOPs
reduction.

Method

Li-B [21]
NISP [42]

ResNet-34 P1
ResNet-34 P4
ResNet-34 P4 SE

Error (top-1)% FLOPs FLOPs Reduced(%)

27.83
27.69

26.80
27.00
26.50

2.7G

–

3.6G
1.3G
1.3G

24.2
43.76

–

64.48
64.48

Table 6. Table shows the results for ResNet-34 on ImageNet [30].
Our model has no loss in accuracy as compare to state-of-art
[21, 42] pruning approaches while signiﬁcantly higher FLOPs re-
duction in different setups.

the latency from one to zero.

As shown in Table-4, our models MobileNet P32, and
MobileNet P32 SE have the signiﬁcantly better accuracy
(close to 1%) as compare to MobileNet model with al-
most similar FLOPs on MobileNet architecture which again
shows the superior performance of our proposed HetConv
convolution over depthwise+pointwise convolution.

4.5. VGG 16 on ImageNet

We experimented with VGG-16 [31] architecture on the
large-scale ImageNet [30] dataset. Except for the initial
convolution layer, all remaining convolutional layers are re-
placed by our HetConv layers while keeping the number of
ﬁlers the same as earlier. Our model VGG-16 P4 shows the
state-of-art result over the recent approach proposed for ﬂop
compression. Channel-Pruning (CP) [11] has the 50.0%
FLOP compression while we have 65.8% FLOP compres-
sion with no loss in accuracy. Please refer to Table-5 for the
more detail results.

4.6. ResNet 34 on ImageNet

We experimented with ResNet-34 [8] architecture on the
large-scale ImageNet [30] dataset. Except for the initial
convolution layer, all remaining convolutional layers are re-
placed by our HetConv layers. Our model ResNet-34 P4
shows the state-of-art result over the previously proposed
methods. NISP [42] has the 43.76% FLOP compression

Method

Error (top-1)% FLOPs FLOPs Reduced(%)

ThiNet-70 [24]
NISP [42]

ResNet-50 P1
ResNet-50 P4

27.90
27.33

23.86
23.84

–
–

4.09G
2.85G

36.8
27.31

–

30.32

Table 7. Table shows the results for ResNet-50 on ImageNet [30].
Our model has no loss in accuracy as compare to state-of-art [24,
42] ﬂop pruning approaches.

while we have 64.48% FLOP compression with signiﬁ-
cantly better accuracy. For more details, please refer to
Table-6.

4.7. ResNet 50 on ImageNet

ResNet-50 [8] is a deep convolutional neural network
having 50 layers with the skip/residual connection.
In
this architecture, we replace standard convolutions with
our proposed HetConv convolution. The values of hyper-
parameters are: weight decay = 1e-4, batch size = 256, ini-
tial learning rate = 0.1 which is decade by 0.1 after every 30
epochs and model is trained in 90 epochs.

It is clear from Table-7 that our model ResNet-50 P4 has
no loss in accuracy while ﬂop pruning approaches [24, 42]
suffers from the heavy accuracy drop in top-1 accuracy.
Our model is trained from scratch, but pruning approaches
[24, 42] requires a pre-trained model and involve iterative
pruning and ﬁne-tuning which is a very time-consuming
process.

5. Conclusion

In this work, we proposed a new type of convolution
using heterogeneous kernels. We have compared our pro-
posed convolution with the popular convolutions (depth-
wise convolution, groupwise convolution, pointwise con-
volution, and standard convolution) on various existing ar-
chitectures (VGG-16, ResNet and MobileNet). Experimen-
tal results show that our HetConv convolution is more ef-
ﬁcient (lesser FLOPs with better accuracy) as compared to
existing convolutions. Since our proposed convolution does
not increase the layer (replacing a layer with a number of
layers, for example, MobileNet) to get FLOPs reduction,
hence has latency zero. We also compared HetConv convo-
lution based model with the FLOPs compression methods
and shown that it produces far better results as compared
to compression methods. In the future, using this type of
convolution, we can design more efﬁcient architectures.

Acknowledgment:

Pravendra Singh acknowledges his travel support from
Google. Vinay Verma acknowledges support from Visves-
varaya fellowship.

4842

References

[1] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Wein-
berger, and Yixin Chen. Compressing neural networks with
the hashing trick. In ICML, pages 2285–2294, 2015. 1

[2] Francois Chollet. Xception: Deep learning with depthwise

separable convolutions. CVPR, 2017. 1, 2, 5

[3] Xiaohan Ding, Guiguang Ding, Jungong Han, and Sheng
Tang. Auto-balanced ﬁlter pruning for efﬁcient convolu-
tional neural networks. AAAI, 2018. 3, 7

[4] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
ICML, 2017. 2

[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014. 2

[6] Song Han, Huizi Mao, and William J Dally. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding. ICLR, 2016. 1, 3

[7] Kaiming He and Jian Sun. Convolutional neural networks at
In 2015 IEEE Conference on Com-
constrained time cost.
puter Vision and Pattern Recognition (CVPR), pages 5353–
5360. IEEE, 2015. 2

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016. 1, 2, 3, 6, 7, 8

[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Identity mappings in deep residual networks. In European
conference on computer vision, pages 630–645. Springer,
2016. 3

[10] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi
Yang. Soft ﬁlter pruning for accelerating deep convolutional
neural networks. IJCAI, 2018. 1, 7

[11] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
for accelerating very deep neural networks. In ICCV, page 6,
2017. 1, 7, 8

[12] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017. 1, 2, 3, 4, 5, 6, 8

[13] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. CVPR, 2018. 3, 6

[14] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In CVPR, 2017. 2

[15] Forrest N Iandola, Song Han, Matthew W Moskewicz,
Khalid Ashraf, William J Dally,
and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer pa-
rameters and¡ 0.5 mb model size.
arXiv preprint
arXiv:1602.07360, 2016. 1

[17] Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto
Cipolla, and Antonio Criminisi. Training cnns with low-
rank ﬁlters for efﬁcient image classiﬁcation. arXiv preprint
arXiv:1511.06744, 2015. 3

[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple

layers of features from tiny images. 2009. 6

[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, pages 1097–1105, 2012. 1, 2

[20] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. arXiv
preprint arXiv:1608.08710, 2016. 3, 6

[21] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. ICLR,
2017. 1, 7, 8

[22] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime
neural pruning. In Advances in Neural Information Process-
ing Systems, pages 2181–2191, 2017. 8

[23] Christos Louizos, Karen Ullrich, and Max Welling. Bayesian
compression for deep learning. In NIPS, pages 3288–3298,
2017. 3

[24] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ﬁlter
level pruning method for deep neural network compression.
In CVPR, pages 5058–5066, 2017. 1, 3, 8

[25] Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and
Dmitry P Vetrov. Structured bayesian pruning via log-normal
multiplicative noise. In NIPS, pages 6775–6784, 2017. 7

[26] Mehdi Noroozi and Paolo Favaro. Unsupervised learning
of visual representations by solving jigsaw puzzles.
In
European Conference on Computer Vision, pages 69–84.
Springer, 2016. 2

[27] Badri Patro and Vinay P. Namboodiri. Differential atten-
tion for visual question answering. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2018. 2

[28] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi-
nary convolutional neural networks. In ECCV, pages 525–
542. Springer, 2016. 1, 3

[29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, pages 91–99, 2015. 2

[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge.
IJCV, 115(3):211–252,
2015. 6, 8

[31] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. ICLR,
2015. 1, 2, 3, 6, 8

[32] Pravendra Singh, Vinay Sameer Raja Kadi, Nikhil Verma,
and Vinay P Namboodiri. Stability based ﬁlter pruning for
accelerating deep cnns. WACV, 2019. 1, 3

[16] Yani Ioannou, Duncan Robertson, Roberto Cipolla, Antonio
Criminisi, et al. Deep roots: Improving cnn efﬁciency with
hierarchical ﬁlter groups. 2017. 3

[33] Pravendra Singh, Neeraj Matiyali, Vinay P Namboodiri,
et al. Multi-layer pruning framework for compressing sin-
gle shot multibox detector. WACV, 2019. 1, 3

4843

[34] Pravendra Singh, Vinay Kumar Verma, Piyush Rai, and
Vinay P Namboodiri. Leveraging ﬁlter correlations for deep
model compression. arXiv preprint arXiv:1811.10559, 2018.
1, 3

[35] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypi-
cal networks for few-shot learning. In Advances in Neural
Information Processing Systems, pages 4077–4087, 2017. 2
[36] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning.
In AAAI, vol-
ume 4, page 12, 2017. 1, 2, 4, 5

[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1–9, 2015.
1, 2, 5

[38] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception archi-
tecture for computer vision. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
2818–2826, 2016. 1

[39] Vincent Vanhoucke. Learning visual representations at scale.

ICLR invited talk, 2014. 1, 2

[40] V Kumar Verma, Gundeep Arora, Ashish Mishra, and Piyush
Rai. Generalized zero-shot learning via synthesized exam-
ples. In The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2018. 2

[41] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In Computer Vision and Pattern Recogni-
tion (CVPR), 2017 IEEE Conference on, pages 5987–5995.
IEEE, 2017. 1, 3

[42] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I
Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, and
Larry S Davis. Nisp: Pruning networks using neuron impor-
tance score propagation. CVPR, 2018. 1, 3, 7, 8

[43] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. 2018. 1, 4, 5

[44] Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, and
Jian Sun. Efﬁcient and accurate approximations of nonlinear
convolutional networks. In NIPS, pages 1984–1992, 2015. 3

4844

