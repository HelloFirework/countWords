Understanding the Disharmony between Dropout and Batch Normalization by

Variance Shift

Xiang Li∗1,2, Shuo Chen1, Xiaolin Hu†3 and Jian Yang‡1

1PCALab, Nanjing University of Science and Technology

2Momenta

3Tsinghua University

Abstract

This paper ﬁrst answers the question “why do the two
most powerful techniques Dropout and Batch Normaliza-
tion (BN) often lead to a worse performance when they are
combined together in many modern neural networks, but
cooperate well sometimes as in Wide ResNet (WRN)?” in
both theoretical and empirical aspects. Theoretically, we
ﬁnd that Dropout shifts the variance of a speciﬁc neural
unit when we transfer the state of that network from train-
ing to test. However, BN maintains its statistical variance,
which is accumulated from the entire learning procedure, in
the test phase. The inconsistency of variances in Dropout
and BN (we name this scheme “variance shift”) causes the
unstable numerical behavior in inference that leads to er-
roneous predictions ﬁnally. Meanwhile, the large feature
dimension in WRN further reduces the “variance shift” to
bring beneﬁts to the overall performance. Thorough experi-
ments on representative modern convolutional networks like
DenseNet, ResNet, ResNeXt and Wide ResNet conﬁrm our
ﬁndings. According to the uncovered mechanism, we get
better understandings in the combination of these two tech-
niques and summarize guidelines for better practices.

1. Introduction

Srivastava et al. [28] brought Dropout as a simple way to
prevent neural networks from overﬁtting. It has been proved
to be signiﬁcantly effective over a large range of machine
learning areas, such as image classiﬁcation [26, 2], speech

∗Xiang Li, Shuo Chen and Jian Yang are with PCA Lab, Key Lab of
Intelligent Perception and Systems for High-Dimensional Information of
Ministry of Education, and Jiangsu Key Lab of Image and Video Under-
standing for Social Security, School of Computer Science and Engineering,
Nanjing University of Science and Technology, China. Xiang Li is also a
visiting scholar at Momenta. Email: xiang.li.implus@njust.edu.cn

†Xiaolin Hu is with the Tsinghua National Laboratory for Information
Science and Technology (TNList) Department of Computer Science and
Technology, Tsinghua University, China.

‡Corresponding author.

Figure 1. Up: a simpliﬁed mathematical illustration of “variance
shift”. In test mode, the neural variance of X is different from
that in train mode caused by Dropout, yet BN attempts to treat that
variance as the popular statistics accumulated from training. Note
that p denotes the Dropout retain ratio and a comes from Bernoulli
distribution which has probability p of being 1. Down: variance
shift in experimental statistics on DenseNet trained on CIFAR100
dataset. The curves are both calculated from the same training
data. “moving vari” is the moving variance (take its mean value
instead if it is a vector) that the i-th BN layer accumulates during
the entire learning, and “real vari” stands for the real variance of
neural response before the i-th BN layer in inference.

recognition [9, 5, 3] and even natural language processing
[18, 15]. Before the birth of Batch Normalization (BN),
it became a necessity of almost all the state-of-the-art net-
works and successfully boosted their performances against
overﬁtting risks, despite its amazing simplicity.

Ioffe and Szegedy [17] demonstrated BN, a powerful

2682

𝑋=𝑥෠𝑋=𝑋−𝐸𝑀𝑜𝑣𝑖𝑛𝑔(𝑋)𝑉𝑎𝑟𝑀𝑜𝑣𝑖𝑛𝑔𝑋+𝜀𝑋𝑉𝑎𝑟𝑇𝑟𝑎𝑖𝑛𝑋=1𝑝𝑉𝑎𝑟𝑇𝑒𝑠𝑡𝑋=1𝑉𝑎𝑟𝑀𝑜𝑣𝑖𝑛𝑔𝑋=𝐸(1𝑝)𝑉𝑎𝑟𝑀𝑜𝑣𝑖𝑛𝑔𝑋=𝐸(1𝑝)𝑥~𝒩(0,1)Train ModeTest  Mode𝑋=𝑎1𝑝𝑥𝑋𝑥~𝒩(0,1)𝜇=𝐸𝑋,𝜎2=𝑉𝑎𝑟𝑋,෠𝑋=𝑋−𝜇𝜎2+𝜀𝐸𝑀𝑜𝑣𝑖𝑛𝑔𝑋←𝐸(𝜇)𝑉𝑎𝑟𝑀𝑜𝑣𝑖𝑛𝑔𝑋←𝐸(𝜎2)Dropout𝑎~Bernoulli(𝑝)BN020406080100BN layer index on DenseNet trained on CIFAR1000.51.01.52.02.53.03.5max(real_varimoving_vari,moving_varireal_vari)Test Acc 77.42%, No Dropout in each bottleneckTest Acc 68.55%, Dropout 0.5 in each bottleneckskill that not only sped up all the modern architectures but
also improved upon their strong baselines by acting as reg-
ularizers. Therefore, BN has been adopted in nearly all the
recent network structures [31, 30, 13, 34] and demonstrates
its great practicability and effectiveness.

However, the above two powerful methods always fail
to obtain an extra reward when combined together prac-
tically [19].
In fact, a modern network even performs
worse and unsatisfactorily when it is equipped with BN
and Dropout simultaneously in their bottleneck blocks.
[17] had already realized that BN eliminates the need for
Dropout in some cases, and thus conjectured that BN pro-
vides similar regularization beneﬁts as Dropout intuitively.
More evidences are provided in recent architectures such as
ResNet/PreResNet [10, 11], ResNeXt [32], DenseNet [16],
where the best performances are all obtained by BN with
the absence of Dropout. Interestingly, a recent study Wide
ResNet (WRN) [33] show that it is positive for Dropout to
be applied in the WRN’s bottleneck blocks with a very large
feature dimension. So far, previous clues leave us a mys-
tery about the confusing and complicated relations between
Dropout and BN. Why do they conﬂict in most of the com-
mon modern architectures? Why do they cooperate friendly
sometimes as in WRN?

We discover the key to understand the disharmony be-
tween Dropout and BN is the inconsistent behaviors of neu-
ral variance [12] during the switch of networks’ state. Con-
sidering one neural response X as illustrated in Fig. 1, when
the state changes from training to test, Dropout will scale
the response by its Dropout retain ratio (i.e. p) that actu-
ally changes the neural variance as in learning. However,
BN still maintains its statistical moving variance of X, as in
most of the common Deep Learning toolboxes’ (e.g., ten-
sorﬂow [1], pytorch [24] and mxnet [4]) implementations.
This mismatch of variance could lead to a instability (see
red curve in Fig. 1). As the signals go deeper, the numer-
ical deviation on the ﬁnal predictions may amplify, which
drops the system’s peformance. We name this scheme as
“variance shift” for simplicity. Instead, without Dropout in
every bottleneck block, the real neural variances in infer-
ence appear very closely to the moving ones accumulated
by BN (see blue curve in Fig. 1), which is also preserved
with a higher test accuracy.

Theoretically, we deduced the “variance shift” under two
general conditions in modern networks’ bottleneck blocks,
and found a satisﬁed explanation for the aforementioned
mystery between Dropout and BN. Furthermore, a large
range of experimental statistics from four representative
modern convolutional networks (i.e., PreResNet, ResNeXt,
DenseNet, Wide ResNet) on CIFAR10/100 datasets veriﬁed
our ﬁndings. Finally, we summarized the understandings
based on our theory and experiments, which can serve as
guidelines in practice.

2. Related Work and Preliminaries

Dropout [28] can be interpreted as a way of regulariz-
ing a neural network by adding noise to its hidden units.
Speciﬁcally, it involves multiplying hidden activations by
Bernoulli distributed random variables which take the value
1 with probability p (0 ≤ p ≤ 1) and 0 otherwise.
Im-
portantly, the test scheme is quite different from training.
During training, the information ﬂow goes through the dy-
namic sub-network. In the test phase, the neural responses
are scaled by the Dropout retain ratio. In order to approxi-
mate an equally weighted geometric mean of the predictions
of an exponential number of learned models that share pa-
rameters. Consider a feature vector x = (x1 . . . xd) with
channel dimension d, xk = akxk(k = 1 . . . d) during the
training phase if we apply Dropout on x, where ak ∼ P
comes from the Bernoulli distribution [7]:

P (ak) =(cid:26) 1 − p,

p,

ak = 0
ak = 1

,

(1)

and a = (a1 . . . ad) is a vector of independent Bernoulli
random variables. At test time for Dropout, one should
scale down the weights by multiplying them by a factor of
p. As introduced in [28], another way to achieve the same
effect is to scale up the retained activations by multiplying
by 1
p at training time and not modifying the weights at test
time. It is more popular on practical implementations, thus
we employ this formula of Dropout in both analyses and
experiments. Therefore, the hidden activation in the train-
1
p xk, whilst in inference it becomes

ing phase is: bxk = ak
simple like: bxk = xk.
Batch Normalization (BN) [17] proposes a determinis-
tic information ﬂow by normalizing each neuron into zero
mean and unit variance. Considering values of x (for clar-
ity, x ≡ xk) over a mini-batch: B = {x(1)...(m)} with m

instances, we have the form of “normalize” part:

µ =

1
m

mXi=1

x(i), σ2 =

1
m

mXi=1

(x(i) − µ)2,bx(i) =

x(i) − µ
√σ2 + ǫ

,

(2)
where µ and σ2 participate in the backpropagation. Note
that we do not consider the “scale and shift” part in BN be-
cause the key of “variance shift” exists in its “normalize”
part. The normalization of activations that depends on the
mini-batch allows efﬁcient training, but is neither necessary
nor desirable during inference [25]. Therefore, BN accu-
mulates the moving averages of neural means and variances
during learning to track the accuracy of a model as it trains:

EM oving(x) ← EB(µ), V arM oving(x) ← E
where EB(µ) denotes the expectation of µ from multiple
B(σ2) denotes the expecta-
training mini-batches B and E
m−1 · EB(σ2))
tion of the unbiased variance estimate (i.e., m

B(σ2),

(3)

′

′

2683

Figure 2. Two general cases for analyzing variance shift in modern
networks’ bottleneck blocks.

over multiple training mini-batches. They are all obtained
by implementations of moving averages [17] and are ﬁxed
for linear transform during inference:

x − EM oving(x)
qV arM oving(x) + ǫ

.

bx =

(4)

3. Theoretical Analyses

From the preliminaries, one can notice that Dropout only
ensures an “equally weighted geometric mean of the pre-
dictions of an exponential number of learned models” by
the approximation from its test policy, as introduced in the
original paper [28]. This scheme poses the variance of the
hidden units unexplored in a Dropout model. Therefore, the
central idea is to investigate the variance of the neural re-
sponse before a BN layer, where the Dropout is previously
applied. Following [8], we ﬁrst start by studying the lin-
ear regime. Further, if a Dropout layer is applied after the
last BN layer in this bottleneck block, it will be followed by
the ﬁrst BN layer in the next bottleneck block. Therefore,
we only need to consider the cases where Dropout comes
before BN. Meanwhile, we also need to consider the num-
ber of convolutional layers between Dropout and BN. 0 or
1 convolutional layer is obviously necessary for investiga-
tions, yet 2 or more convolutional layers can be attributed to
the 1 case via similar analyses. To conclude, we have two
cases generally, as shown in Fig. 2. Importantly, the Wide
ResNet with Dropout exactly follows the case (b) formula-
tion.

In case (a), the BN layer is directly subsequent to the
Dropout layer and we only need to consider one neural re-
1
sponse X = ak
p xk, where k = 1 . . . d in training phase
and X = xk in test phase.

In case (b), the feature vector x = (x1 . . . xd) is passed
into a convolutional layer (similar deduction can be con-
ducted here if it is a fully connected layer) to form the neu-
ral response X. We also regard its corresponding weights
1
p xi

to be w = (w1 . . . wd), hence we get X = Pd
for training and X =Pd

For the ease of deduction, we assume that the inputs all
come from the same distribution with mean c and variance
v (i.e., E(xi) = c, V ar(xi) = v, v > 0 for any i = 1 . . . d).
We let the ai and xi be mutually independent, considering

i=1 wixi for testing.

i=1 wiai

the property of Dropout. Due to the aforementioned deﬁni-
tion, ai and aj are mutually independent as well.

3.1. Case (a)

By using the deﬁnition of the variance and following the

paradigms above, we have that

V arT rain(X)

=

1
p2

E(a2

k)E(x2

k)−

1
p2 (E(ak)E(xk))2 =

1
p

(c2 + v)−c2.

′

B( 1

(5)
In inference, BN keeps the moving average of variance (i.e.,
p (c2 + v) − c2)) ﬁxed. That is, BN wishes that the
E
variance of neural response X, which comes from the input
p (c2 + v) −
images initially, is supposed to be close to E
c2). However, Dropout breaks the harmony when it comes
to its test stage by having X = xk to get V arT est(X) =
V ar(xk) = v.

B( 1

′

′

If putting V arT est(X) into the unbiased variance esti-
B(v) which is obviously different from
mate, it becomes E
p (c2 + v)− c2) of BN during train-
the popular statistic E
ing when Dropout (p < 1) is applied. Therefore, the shift
ratio △ is obtained by

B( 1

′

△(p) =

V arT est(X)
V arT rain(X)

=

v

1

p (c2 + v) − c2

.

(6)

In case (a), the variance shift happens via a coefﬁcient
△(p) ≤ 1. Since modern neural networks carry a deep
feedforward topologic structure, the deviate numerical ma-
nipulations can lead to more uncontrollable numerical out-
puts of subsequent layers (Fig. 1). It brings the chain reac-
tion of ampliﬁed shift of variances (even affects the means
further) in every BN layers sequentially, as the networks go
deeper. We will show that it directly leads to a dislocation
of ﬁnal predictions and makes the system suffer from a per-
formance drop later in the statistical experimental part (e.g.,
Figs. 4 and 5 in Section 4).

In this design (i.e., BN directly follows Dropout), if we
want to alleviate the variance shift risks, i.e., △(p) → 1, the
only thing we can do is to eliminate Dropout which means
setting the Dropout retain ratio p → 1. Fortunately, the
architectures where Dropout brings beneﬁts (e.g., in Wide
ResNet) do not follow this type of arrangement.
In fact,
they adopt the case (b) in Fig. 2, which is more common in
practice, and we will describe it in details as follows.

3.2. Case (b)

At this time, X is obtained by Pd

1
p xi during
training, where w denotes for the corresponding weights
for x, along with the Dropout applied. For the ease of de-
duction, we assume that in the very later epoch of training,
the weights of w remains constant, giving that the gradi-
ents become signiﬁcantly close to zero. Similarly, we can

i=1 wiai

2684

DropoutBNDropoutConvolutionBN𝚾𝚾(a)(b)𝑋𝑋𝑋=𝑑𝑚Χ෠𝑋=𝑋−𝐸𝑀𝑜𝑣𝑖𝑛𝑔[𝑋]𝑉𝑎𝑟𝑀𝑜𝑣𝑖𝑛𝑔𝑋+𝜀𝑋𝑉𝑎𝑟𝑇𝑟𝑎𝑖𝑛𝑋=1−𝑑/𝑚𝑉𝑎𝑟𝑇𝑒𝑠𝑡𝑋=1−𝑑/𝑚2𝑉𝑎𝑟𝑀𝑜𝑣𝑖𝑛𝑔𝑋=𝐸1−𝑑/𝑚𝑉𝑎𝑟𝑀𝑜𝑣𝑖𝑛𝑔𝑋=𝐸1−𝑑/𝑚ΧΧ~𝑁(0,1)Χ~𝑁(0,1)≠............Figure 3. Statistical mean values of (cos θ)2 and d(cos θ)2.
These four modern architectures are trained without Dropout
on CIFAR100, respectively. We observe that (cos θ)2 lies in
(0.01, 0.10) approximately in every network structure and various
datasets. Interestingly, the term d(cos θ)2 in WRN is signiﬁcantly
bigger than those on other networks mainly due to its larger chan-
nel width d.

expand V arT rain(X) as:

V arT rain(X) = Cov(

xi,

wiai

wiai

1
p

dXi=1
dXi=1

dXi=1
dXi=1
dXj6=i

w2

i + ρax

wiwj),

1
p

xi)

(7)

= (

1
p

(c2 + v) − c2)(

where ρax

i,j =

Cov(aixi,aj xj )

√V ar(aixi)√V ar(aj xj ) ∈ [−1, 1]. For the

ease of deduction, we simplify all the linear correlation co-
efﬁcients to be the same as a constant ρax = ρax
i,j,∀i, j =
1 . . . d, i 6= j. Similarly, V arT est(X) is obtained by

V arT est(X) = Cov(

wixi,

dXi=1
dXi=1

w2

dXi=1
dXi=1

wixi)

dXj6=i

wiwj),

= v(

i + ρx

Cov(xi,xj )

i,j =

where ρx

√V ar(xi)√V ar(xj ) ∈ [−1, 1], and we also
have a constant ρx = ρx
i,j,∀i, j = 1 . . . d and i 6= j. Since
ai and xi, ai and aj are mutually independent, we can get
the relation between ρax and ρx:

ρax = ρax

i,j =

Cov(aixi, ajxj)

pV ar(aixi)pV ar(ajxj)

v

v

ρx
i,j =

=

1

p (c2 + v) − c2

1

p (c2 + v) − c2

(9)

ρx.

According to Eqs. (7), (8) and (9), the variance shift for

case (b) can be written as:

△(p, d) =

=

1

V arT est(X)
V arT rain(X)
v + vρx(d(cos θ)2 − 1)

p (c2 + v) − c2 + vρx(d(cos θ)2 − 1)

where (cos θ)2 comes from the expression:

(10)

,

Table 1. Averaged means of (cos θ)2 and d(cos θ)2 over all the
convolutional layers on four representative networks.

Networks

CIFAR10

CIFAR100

(cos θ)2

d(cos θ)2

(cos θ)2

d(cos θ)2

PreResNet-110 [11]

0.03546

2.91827

0.03169

2.59925

ResNeXt-29 [32]

0.02244

14.78266

0.02468

14.72835

WRN-28-10 [33]

0.02292

52.73550

0.02118

44.31261

DenseNet-BC [16]

0.01538

3.83390

0.01390

3.43325

i

i

2

)

= (

i=1 w2

= (cos θ)2,

i=1 wi)2
i=1 w2

Pd
i=1 1 · wi
i=1 12qPd
qPd

(Pd
d ·Pd
(11)
and θ denotes for the angle between vector w and vector
(1 . . . 1) ∈ Rd. To empirically prove that d(cos θ)2 scales
approximately linear to d, here we made rich calculations
w.r.t the terms d(cos θ)2 and (cos θ)2 on four modern ar-
chitectures1 trained on CIFAR10/100 datasets (Table 1 and
Fig. 3.2). Based on Table 1 and Fig. 3.2, we observe that
(cos θ)2 lies in (0.01, 0.10) stably in every type of the net-
work whilst d(cos θ)2 tends to increase in parallel when d
grows. From Eq. (10), the inequation V arT est(X)
V arT rain(X) < 1
holds obviously when p < 1. If we want V arT est(X) to be
close with V arT rain(X), we need this term

△(p, d) =

vρx + v−vρx
d(cos θ)2
v−vρx+( 1
p
d(cos θ)2

1
p

vρx +

−1)c2

=

vρx(d(cos θ)2 − 1) + v

vρx(d(cos θ)2 −1)+ 1

p (c2 + v) − c2

(12)

to approach 1. There are two ways to achieve △(p, d) → 1:
• p → 1: maximizing the Dropout retain ratio p (ideally

up to 1 which means Dropout is totally eliminated);

(8)

• d → ∞: growing the width of channel exactly as the

Wide ResNet did to enlarge d.

4. Statistical Experiments

We conduct extensive statistical experiments to check the
correctness of above deduction in this section. Four mod-
ern architectures including DenseNet [16], PreResNet [11],
ResNeXt [32] and Wide ResNet (WRN) [33] are adopted
on CIFAR10 and CIFAR100 datasets.

Datasets. The two CIFAR datasets [20] consist of col-
ored natural scene images, with 32×32 pixel each. The
training and test sets contain 50k images and 10k images
respectively. CIFAR10 (C10) has 10 classes and CIFAR100
(C100) has 100. For data preprocessing, we normalize the
data by using the channel means and standard deviations.
For data augmentation, we adopt a standard scheme that is
widely used in [11, 16, 21, 23, 22, 27, 29]: the images are
ﬁrst zero-padded with 4 pixels on each side, then a 32×32
1For the convolutional ﬁlters which have larger than 1 ﬁlter size as
k × k, k > 1, we vectorise them by expanding its channel width d to
d × k × k while maintaining all the weights.

2685

020406080100Convolutional Layer Index of Networks0.00.10.20.30.40.50.60.70.80.91.0Mean of (cos(θ))2PreResNetResNeXtWRNDenseNet0100020003000400050006000Weight Dimension d of Convolutional Filter020406080100Mean of d(cos(θ))2PreResNetResNeXtWRNDenseNetFigure 4. See by columns. Visualizations about “variance shift” on BN layers of four modern networks w.r.t: 1) Dropout type; 2) Dropout
drop ratio; 3) dataset, along with their test error rates (the 5th row). Obviously, WRN is less inﬂuenced by Dropout (e.g., in 3rd row and 4th
column) when the Dropout-(b) drop ratio ≤ 0.5, and thus it even enjoys an improvement with Dropout applied with BN in each bottleneck.

crop is randomly sampled from them and half of the images
are horizontally ﬂipped.

Networks with Dropout. The four modern architectures
are all chosen from the open-source codes written in pytorch
that can reproduce the results reported in previous papers.
Speciﬁcally, there are PreResNet-110 [11], ResNeXt-29, 8
× 64 [32], WRN-28-10 [33] and DenseNet-BC (L=100,
k=12) [16]. Since the BN layers are already developed as
the indispensible components of their body structures, we
arrange Dropout that follows the two cases in Fig. 2:

(a) We assign all the Dropout layers only and right be-
fore all the bottlenecks’ last BN layers in these four net-
works, neglecting their possible Dropout implementations
(as in DenseNet [16] and Wide ResNet [33]). We denote
this design to be models of Dropout-(a).

(b) We follow the assignment of Dropout

in Wide

ResNet [33], which ﬁnally improves WRNs’ overall perfor-
mances, to place the Dropout before the last Convolutional
layer in every bottleneck block of PreResNet, ResNeXt and
DenseNet. This scheme is denoted as Dropout-(b) models.
Statistics of variance shift. Assume a network G con-
tains n BN layers in total. We arrange these BN layers from
shallow to deep by giving them indices that range from 1
to n accordingly. The whole statistical manipulation is con-
ducted by the following three steps:

(1) Calculate moving vari, i ∈ {1, ..., n}: when G is
trained until convergence, each BN layer obtains the mov-
ing average of neural variance (the unbiased variance esti-
mate) from the feature-map that it receives during the en-
tire learning procedure. We denote that variance as mov-
ing var. Since the moving var for every BN layer is a vector
(whose length is equal to the amount of channels of previous

2686

020406080100120140160180[Dropout-(a) C10] BN index on PreResNet1.01.52.02.53.0shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7051015202530[Dropout-(a) C10] BN index on ResNeXt1.01.52.02.53.0shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.70510152025[Dropout-(a) C10] BN index on WRN1.01.52.02.53.0shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7020406080100[Dropout-(a) C10] BN index on DenseNet1.01.52.02.53.0shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7020406080100120140160180[Dropout-(a) C100] BN index on PreResNet1.01.52.02.53.0shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7051015202530[Dropout-(a) C100] BN index on ResNeXt1.01.52.02.53.0shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.70510152025[Dropout-(a) C100] BN index on WRN1.01.52.02.53.0shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7020406080100[Dropout-(a) C100] BN index on DenseNet1.01.52.02.53.0shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7020406080100120140160180[Dropout-(b) C10] BN index on PreResNet1.001.051.101.151.20shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7051015202530[Dropout-(b) C10] BN index on ResNeXt1.001.051.101.151.20shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.70510152025[Dropout-(b) C10] BN index on WRN1.001.051.101.151.20shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7020406080100[Dropout-(b) C10] BN index on DenseNet1.001.051.101.151.20shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7020406080100120140160180[Dropout-(b) C100] BN index on PreResNet1.001.051.101.151.20shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7051015202530[Dropout-(b) C100] BN index on ResNeXt1.001.051.101.151.20shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.70510152025[Dropout-(b) C100] BN index on WRN1.001.051.101.151.20shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7020406080100[Dropout-(b) C100] BN index on DenseNet1.001.051.101.151.20shift ratioDropout 0.0Dropout 0.1Dropout 0.3Dropout 0.5Dropout 0.7PreResNetResNeXtWRNDenseNetDropout-(a) C1005101520Error rate (%)0.00.10.30.50.7PreResNetResNeXtWRNDenseNetDropout-(a) C1000510152025303540Error rate (%)0.00.10.30.50.7PreResNetResNeXtWRNDenseNetDropout-(b) C10012345678Error rate (%)0.00.10.30.50.7PreResNetResNeXtWRNDenseNetDropout-(b) C100051015202530Error rate (%)0.00.10.30.50.7Table 2. Averaged shift ratios over all the BN layers of Dropout-(b)
models on CIFAR100 dataset. Smaller is better for stability.

Dropout ratios:

0.1

0.3

0.5

0.7

PreResNet-110 [11]

1.008945

1.040766

1.052092

1.076403

ResNeXt-29 [32]

1.006296

1.032514

1.058549

1.134871

WRN-28-10 [33]

1.003485

1.006466

1.013873

1.033254

DenseNet-BC [16]

1.013859

1.015065

1.036019

1.042925

feature-map), we leverage its mean value to represent mov-
ing var instead, in purpose of an ease visualization. Further,
we denote moving vari as the moving var of i-th BN layer.
(2) Calculate real vari, i ∈ {1, ..., n}: after training,
we ﬁx all the parameters of G and set its state to the evalua-
tion mode (hence the Dropout will apply its inference policy
and BN will freeze its moving averages of means and vari-
ances). The training data is again utilized for going through
G within a certain of epochs, in order to get the real expec-
tation of neural variances on the feature-maps before each
BN layer. Data augmentation is also kept to ensure that ev-
ery possible detail for calculating neural variances remains
exactly the same with training. Importantly, we adopt the
same moving average algorithm to accumulate the unbiased
variance estimates. Similarly in (1), we let the mean value
of real variance vector be real vari before the i-th BN layer.
), i ∈
[1, n]: since we focus on the shift, the scalings are all kept
above 1 by their reciprocals if possible in purpose of a bet-
ter view. Various Dropout drop ratios [0.0, 0.1, 0.3, 0.5, 0.7]
are applied for comparisons in Fig. 4. The corresponding
error rates are also included in each column. To be spe-
ciﬁc, we also calculate all the averaged shift ratios over the
entire networks under drop ratio 0.1, 0.3, 0.5, 0.7 to show
the quantitive analyses based on Fig. 4 in Table 2. The re-
sults demonstrate that WRNs’ shift ratios are considerably
smaller than other counterparts in every Dropout setting.

(3) Get “shift ratio” = max( real vari
moving vari

, moving vari
real vari

The statistical experiments conﬁrm our analyses. In
these four columns of Fig. 4, we discover that when the
drop ratio is relatively small (i.e., 0.1), the green curves
go close to the blue ones (i.e., models without Dropout),
thus their performances are comparable or even better to the
baselines. It agrees with our previous deduction that when-
ever in (a) or (b) case, decreasing drop ratio 1 − p will al-
leviate the variance shift risks. Furthermore, in Dropout-(b)
models (i.e., the last two columns) we ﬁnd that, for WRNs,
the curves with drop ratio 0.1, 0.3 even 0.5 approach closer
to the one with 0.0 than other networks, and they all out-
perform the baseline. It also aligns with our analyses since
WRN has a signiﬁcantly larger channel dimension d, and
it ensures that a slightly larger p will not explode the neu-
ral variance too much. Furthermore, the statistics on Ta-
ble 2 also support our previous deduction that WRN is
less inﬂuenced by Dropout in terms of variance shift ratio,
and its performance consistently improves when drop ra-

Figure 5. Examples of inconsistent neural responses between train
mode and test mode of DenseNet Dropout-(a) 0.5 trained on CI-
FAR10 dataset. These samples are from the training data, whilst
they are correctly classiﬁed by the model during learning yet er-
roneously judged in inference, despite all the ﬁxed model param-
eters. Variance shift ﬁnally leads to the prediction shift that drops
the performance.

tio < 0.5, whilst other models get stucked or perform even
worse when drop ratio reaches 0.3 (last row in Fig. 4).

Neural responses (of the last layer before softmax)
for training data are unstable from training stage to test
stage. To get a clearer understanding of the numerical dis-
turbance that the variance shift brings ﬁnally, a bundle of
images (from training data) are drawn with their neural re-
sponses before the softmax layer in both training stage and
test stage (Fig. 5). From those pictures and their responses,
we can ﬁnd that with all the weights of networks ﬁxed, only
a mode transfer (from train to test) will change the distri-
bution of the ﬁnal responses even in the training set, and
it leads to a wrong classiﬁcation consequently.
It proves
that the predictions of training data differs between train-
ing stage and test stage when a network is equipped with
Dropout and BN layers in their bottlenecks. Therefore, we
conﬁrm that the unstable numerical behaviors are the fun-
damental reasons for the performance drop.

Only an adjustment for moving means and variances
will bring an improvement, despite all other parameters
ﬁxed. Given that the moving means and variances of BN
will not match the real ones during test, we attempt to ad-
just these values by passing the training data again under
the evaluation mode. In this way, the moving average algo-
rithm [17] can also be applied. After shifting the moving
statistics to the real ones by using the training data, we can
have the model performed on the test set. From Table 3,
All the Dropout-(a)/(b) 0.5 models outperform their base-
lines by having their moving statistics adjusted. Signiﬁcant
improvements (e.g., ∼ 2 and ∼ 4.5 gains for DenseNet on
CIFAR10 and on CIFAR100 respectively) can be observed
in Dropout-(a) models. It again veriﬁes that the drop of per-
formance could be attributed to the “variance shift”: a more

2687

Table 3. Adjust BN’s moving mean/variance by running moving
average algorithm on training data under test mode. These error
rates (%) are all averaged from 5 parallel runnings with different
random initial seeds. “-A” means the corresponding adjustment.
For comparisons, we also list the performances of these models
without Dropout. The best records are marked red.

C10

PreResNet
ResNeXt
WRN
DenseNet

C100

Dropout-(a)
0.5

0.5-A

Dropout-(b)
0.5

0.5-A

8.42
4.43
4.59
8.70

6.42
3.96
4.20
6.82

5.85
4.09
3.81
5.63

5.77
3.93
3.71
5.29

Dropout-(a)
0.5

0.5-A

Dropout-(b)
0.5

0.5-A

w/o Dropout

5.02
3.77
3.97
4.72

w/o Dropout

PreResNet
ResNeXt
WRN
DenseNet

32.45
19.04
21.08
31.45

26.57
18.24
20.70
26.98

25.50
19.33
19.48
25.00

25.20
19.09
19.15
23.92

23.73
17.78
19.17
22.58

Theoretically, applying Dropout in the test phase will avoid
the “variance shift” yet slightly harm the performance. Al-
though it is shown very expensive in [28], we are still in-
teresting how many samples networks are needed to match
the performance of the approximate averaging method or
the baseline models without Dropout. Here we take the
Dropout-(b) 0.5 PreResNet model as an example and do
classiﬁcation on CIFAR100 by averaging the predictions of
k randomly sampled neural networks.

From Fig. 6, we can ﬁnd that nearly 10 samples of net-
works can approach the results of weight scaling. And more
rounds of runnings will give a slight gain in the end but can
not reach the performance of the baseline without Dropout.
To conclude, these sampled networks still cannot compen-
sate the performance drop with such an expensive way in
the test phase.

5. Strategy to Combine Them Better

Since we get a clear knowledge about the disharmony be-
tween Dropout and BN, we can easily develop an approach
to combine them together, to see whether an extra improve-
ment can be obtained. In this section, we introduce one pos-
sible solution that slightly modiﬁes the formula of Dropout
and make it less sensitive to variance, which can alleviate
the shift problem and stabilize the numerical behaviors.

Figure 6. Monte-Carlo model averaging vs. weight scaling vs. no
Dropout. The ensemble of models which avoid “variance shift”
risks still underperforms the baseline trained without Dropout.

proper popular statistics with smaller variance shift could
recall a bundle of erroneously classiﬁed samples back to the
right ones. However, except for WRN, the performances of
other architectures after adjusting statistics still underper-
form their counterparts without Dropout. This cue shows
that for most structures, shifting moving statistics via train-
ing data can not make up for the performance gap.

Although Monte-Carlo model averaging can avoid
“variance shift”, it costs plenty of time and limits the
performance.. The efﬁcient test time procedure that the
original Dropout [28] propose is to do an approximate
model combination by scaling down the weights of the
trained neural network. And it is exactly the central rea-
son which is responsible for the variance shift risks, as it
only ensures the stability of neural means, rather the vari-
ances. Therefore, a natural question comes out: what if
we try to make predictions by sampling k neural nets using
Dropout for each test case and average their predictions?

1

v

The drawbacks of vanilla Dropout lie in the weight scale
during the test phase, which may lead to a large disturbance
on statistical variance. This clue can push us to think: if
we ﬁnd a scheme that functions like Dropout but carries a
lighter variance shift, we may stabilize the numerical be-
haviors of neural networks, thus the ﬁnal performance will
probably beneﬁt from such stability. Here we take the case
(a) as an example for investigations where the variance shift
p (c2+v)−c2 = p (we let c = 0 for simplicity in
rate is
this discussion). That is, if we set the drop ratio (1 − p)
as 0.1, the variance would be scaled by 0.9 when the net-
work is switched from training to test. Inspired by the orig-
inal Dropout [28], where the authors also proposed another
form of Dropout that amounts to adding a Gaussian dis-
tributed random variable with zero mean and standard de-
viation equal to the activation of the unit, i.e., xi + xir and
r ∼ N (0, 1), we further modify r as a uniform distribution
that lies in [−β, β], where 0 ≤ β ≤ 1. Therefore, each hid-
den activation would be X = xi + xiri and ri ∼ U(−β, β)
[6]. We name this form of Dropout as “Uout” for simplicity.
With the mutually independent distribution between xi and
ri hold, we apply X = xi +xiri, ri ∼ U(−β, β) in training
mode and X = xi in test. Similarly, in the simpliﬁed case

2688

01020304050607080Number of samples used for Monte-Carlo averaging (k) on CIFAR10023242526272829Test error (%)Monte-Carlo Model Averaging on Dropout-(b) 0.5 PreResNetApproximate Averaging by Weight Scaling on Dropout-(b) 0.5 PreResNetDropout-(b) 0.0 PreResNet (without Dropout)Table 4. Apply new form of Dropout (i.e. Uout) in Dropout-(b)
models. These error rates (%) are all averaged from 5 parallel run-
nings with different random initial seeds. The numbers in brackets
denote the values of β relating to the performances.

C10 β
PreResNet
ResNeXt
WRN
DenseNet
C100 β
PreResNet
ResNeXt
WRN
DenseNet

0.0
5.02
3.77
3.97
4.72
0.0

23.73
17.78
19.17
22.58

[0.2, 0.3, 0.5]

4.85 (0.2)
3.75 (0.3)
3.79 (0.5)
4.61 (0.5)

[0.2, 0.3, 0.5]

23.53 (0.3)
17.72 (0.2)
18.87 (0.5)
22.30 (0.5)

of c = 0, we can deduce the variance shift again as follows:

V arT est(X)
V arT rain(X)

=

V ar(xi)

V ar(xi + xiri)

=

v

E((xi + xiri)2)

v

=

3

3 + β2

.

=

E(x2

i ) + 2E(x2

i )E(ri) + E(x2

i )E(r2
i )

300

(13)
Given β as 0.1, the new variance shift rate would be
301 ≈ 0.9966777 which is much closer to 1.0 than the
previous 0.9 in case (a). A list of experiments is hence
employed based on those four modern networks under
Dropout-(b) settings in Table 4. We search β in range
of [0.2, 0.3, 0.5] to ﬁnd optimal results. We observe that
“Uout” with larger ratios tends to perform favorably well,
which indicates its superior stability. Except for ResNeXt,
nearly all the architectures achieved up to 0.2 ∼ 0.3
increase of accuracy on both CIFAR10 and CIFAR100
dataset.

Beyond Uout, we discover that adding only one Dropout
layer right before the softmax layer can avoid the variance
shift risks since there are no following BN layers. We eval-
uate several state-of-the-art models on the ImageNet vali-
dation set (Table 5), and observe consistent improvements
when drop ratio 0.2 is employed after the last BN layers on
the large scale dataset. The beneﬁts of doing so also conﬁrm
the effectiveness of our theory.

ImageNet drop ratio

ResNet-200 [10]
ResNeXt-101 [32]

top-1 err.

0.0

21.70
20.40

0.2

21.48
20.17

top-5 err.
0.0
0.2

5.80
5.30

5.55
5.12

Table 5. Error rates (%) on ImageNet validation set.

6. Summary of Guidelines

According to the analyses and experiments, we can get

the following understandings as guidelines:

• In modern CNN architectures, the original Dropout

and BN are not recommended to appear in the bottle-
neck part due to their variance shift conﬂict, except
that we have a relatively large feature dimension. We
also suggest the drop ratio < 0.5 since the deduction
Eq. (12) and the experiments (Fig. 4) show higher drop
ratio will still break the stability of neural responses in
any case. To conclude, the shift risk depends on both
the Dropout ratio and feature dimension.

• Adjusting the moving means and variances through
training data is beneﬁcial for improvements, but it
can not compensate the entire loss in performance,
compared to the baselines which are trained without
Dropout. Moreover, the ensemble of predictions from
networks which apply Dropout during test to avoid
“variance shift” still underperforms these baselines.

• We understand why some recent models

(e.g.
Inception-v4 [30], SENet [14]) have adopted one
Dropout layer after the last BN layer of the entire net-
work, because it will not lead to the variance shift es-
sentially based on our theory.

• We also discover that the form of Dropout can be mod-
iﬁed, in purpose of reducing their variance shift to
boost their performances even when they are in the bot-
tleneck building blocks.

7. Conclusion

In this paper, we investigate the “variance shift” phe-
nomenon when Dropout layers are applied with Batch Nor-
malization on modern convolutional networks. We discover
that due to their distinct test policies, neural variance will be
improper and shifted as the information ﬂows in inference,
and it leads to the unexpected ﬁnal predictions that drops
the performance. These understandings can serve as prac-
tical guidelines for designing novel regularizers or getting
better practices in the area of Deep Learning.

Acknowledgments

The authors would like to thank the editor and the anony-
mous reviewers for their critical and constructive comments
and suggestions. This work was supported by the National
Science Fund of China under Grant No. U1713208, Pro-
gram for Changjiang Scholars and National Natural Sci-
ence Foundation of China under Grant No. 61836014.
It was also supported by NSF of China (No: 61602246),
NSF of Jiangsu Province (No: BK20171430), the Funda-
mental Research Funds for the Central Universities (No:
30918011319), the open project of State Key Laboratory
of Integrated Services Networks (Xidian University, ID:
ISN19-03), the Summit of the Six Top Talents Program
(No: DZXX-027), and the Young Elite Scientists Sponsor-
ship Program by CAST (No: 2018QNRC001).

2689

[19] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter.
Self-normalizing neural networks. In NeurIPs, pages 971–
980, 2017.

[20] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009.

[21] G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:
Ultra-deep neural networks without residuals. arXiv preprint
arXiv:1605.07648, 2016.

[22] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-

supervised nets. In ICAIS, pages 562–570, 2015.

[23] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv

preprint arXiv:1312.4400, 2013.

[24] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017.

[25] T. Salimans and D. P. Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In NeurIPs, pages 901–909, 2016.

[26] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,
and R. Webb. Learning from simulated and unsupervised
images through adversarial training.
In CVPR, volume 2,
page 5, 2017.

[27] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-
miller. Striving for simplicity: The all convolutional net.
arXiv preprint arXiv:1412.6806, 2014.

[28] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: a simple way to prevent neural
networks from overﬁtting. JMLR, 15(1):1929–1958, 2014.

[29] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training

very deep networks. In NeurIPS, pages 2377–2385, 2015.

[30] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, pages 4278–4284, 2017.
[31] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, pages 2818–2826, 2016.

[32] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In CVPR,
pages 5987–5995, 2017.

[33] S. Zagoruyko and N. Komodakis. Wide residual networks.

arXiv preprint arXiv:1605.07146, 2016.

[34] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An
extremely efﬁcient convolutional neural network for mobile
devices. arXiv preprint arXiv:1707.01083, 2017.

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-
ﬂow: A system for large-scale machine learning. In OSDI,
volume 16, pages 265–283, 2016.

[2] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-
uation of output embeddings for ﬁne-grained image classiﬁ-
cation. In CVPR, pages 2927–2936, 2015.

[3] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai,
E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng,
G. Chen, et al. Deep speech 2: End-to-end speech recog-
nition in english and mandarin.
In ICML, pages 173–182,
2016.

[4] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,
B. Xu, C. Zhang, and Z. Zhang. Mxnet: A ﬂexible and efﬁ-
cient machine learning library for heterogeneous distributed
systems. arXiv preprint arXiv:1512.01274, 2015.

[5] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-
dependent pre-trained deep neural networks for
large-
vocabulary speech recognition. IEEE Transactions on audio,
speech, and language processing, 20(1):30–42, 2012.

[6] J. Friedman, T. Hastie, and R. Tibshirani. The elements of
statistical learning, volume 1. Springer series in statistics
New York, NY, USA:, 2001.

[7] Y. Gal, J. Hron, and A. Kendall. Concrete dropout.

In

NeurIPs, pages 3581–3590, 2017.

[8] X. Glorot and Y. Bengio. Understanding the difﬁculty of
training deep feedforward neural networks. In ICAIS, pages
249–256, 2010.

[9] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,
E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates,
et al. Deep speech: Scaling up end-to-end speech recogni-
tion. arXiv preprint arXiv:1412.5567, 2014.

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770–778, 2016.

[11] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in

deep residual networks. In ECCV, pages 630–645, 2016.

[12] D. Hendrycks and K. Gimpel. Adjusting for dropout variance

in batch normalization and weight initialization. 2017.

[13] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. arXiv preprint arXiv:1704.04861, 2017.

[14] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-

works. arXiv preprint arXiv:1709.01507, 2017.

[15] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
In CVPR, pages

rell. Natural language object retrieval.
4555–4564, 2016.

[16] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten.
Densely connected convolutional networks. arXiv preprint
arXiv:1608.06993, 2016.

[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, pages 448–456, 2015.

[18] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush. Character-
aware neural language models. In AAAI, pages 2741–2749,
2016.

2690

