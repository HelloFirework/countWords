Versatile Multiple Choice Learning and Its Application to Vision Computing

Kai Tian1 Yi Xu1 Shuigeng Zhou1

,

2∗ Jihong Guan3

1Shanghai Key Lab of Intelligent Information Processing, and School of

Computer Science, Fudan University, Shanghai 200433, China

2Shanghai Institute of Intelligent Electronics & Systems, Fudan University, Shanghai 200433, China

3Department of Computer Science & Technology, Tongji University, Shanghai 201804, China

Abstract

Most existing ensemble methods aim to train the un-
derlying embedded models independently and simply ag-
gregate their ﬁnal outputs via averaging or weighted vot-
ing. As many prediction tasks contain uncertainty, most
of these ensemble methods just reduce variance of the pre-
dictions without considering the collaborations among the
ensembles. Different from these ensemble methods, mul-
tiple choice learning (MCL) methods exploit the cooper-
ation among all the embedded models to generate multi-
ple diverse hypotheses. In this paper, a new MCL method,
called vMCL (the abbreviation of versatile Multiple Choice
Learning), is developed to extend the application scenarios
of MCL methods by ensembling deep neural networks. Our
vMCL method keeps the advantage of existing MCL meth-
ods while overcoming their major drawback, thus achieves
better performance. The novelty of our vMCL lies in three
aspects: (1) a choice network is designed to learn the con-
ﬁdence level of each specialist which can provide the best
prediction base on multiple hypotheses; (2) a hinge loss
is introduced to alleviate the overconﬁdence issue in MCL
settings; (3) Easy to be implemented and can be trained
in an end-to-end manner, which is a very attractive feature
for many real-world applications. Experiments on image
classiﬁcation and image segmentation task show that vMCL
outperforms the existing state-of-the-art MCL methods.

1. Introduction

Machine learning tasks are often accompanied with am-
biguity in many application areas, such as computer vi-
sion [16, 24], language understanding [11, 21], and recom-
mendation systems [10]. Human beings interact with the
world through various types of information ﬂows. Some-
times it is hard to make a perceptron recognition in just
one view. Due to the ambiguity, we cannot expect to get

∗Correspondence author.

predictions from one model that are accurate for all data.
Therefore, researchers suggested generating multiple plau-
sible outputs [8]. This is important, especially for interac-
tive intelligence systems, such as machine translation [1],
image classiﬁcation and denoising [7]. Generating multiple
plausible predictions promotes the diversity of the solutions.

To generate multiple diverse predictions, two types of
methods were developed. One is to train a model and
generate multiple predictions during the inference pro-
cess [2, 5, 12, 13]. Usually, graphical models are used to
generate structured outputs. By optimizing the dissimilarity
between different solutions, those methods can ﬁnd a set of
m-best conﬁgurations. Another is to train multiple models
and aggregate their predictions to generate the ﬁnal output.
Such methods focus on the design of the learning process.

Among the second type of approaches, some methods
ensemble many independent models and collect all the pre-
dictions into a candidate set. These methods, including
Bayesian averaging [18], boosting [23] and bagging [3], of-
ten perform better than using a single model in many ma-
chine learning tasks, especially classiﬁcation. As ensemble
methods usually train all the embedded models indepen-
dently, they may obtain low diversity in their predictions.
Thus, multiple choice learning (MCL) [9] was proposed to
overcome this defect by establishing cooperation among all
the embedded models, each of which is trained to be a spe-
cialist on one particular data subset. The oracle loss concept
was proposed, which focuses on one model that gives the
most accurate prediction for each sample. And the oracle
error rate was used to measure MCL performance, which
means the ratio that none of the predictions are correct for
the test examples.

Recently, Lee et al. [15] adopted deep neural networks
into MCL and proposed stochastic Multiple Choice Learn-
ing (sMCL) to train diverse deep ensemble models. By min-
imizing the oracle loss directly, each model focuses on a
subset of data to make high accuracy prediction. Although
sMCL achieves high oracle performance and outperforms
many existing baselines, it often fails to make satisfactory

6349

ﬁnal decision since each network tends to be overconﬁdent
in its own prediction. Thus, simply aggregating these pre-
dictions by averaging or voting will result in bad ﬁnal pre-
diction. This leads sMCL to perform poorly in terms of
top-1 accuracy measure and cannot be used for the scenar-
ios that need one exact prediction. In other words, sMCL
cannot take full advantage of the high oracle performance.
To solve the overconﬁdence problem, [14] developed the
conﬁdent MCL (CMCL) algorithm that employs a new loss
function named conﬁdent oracle loss to sMCL. The conﬁ-
dent oracle loss adds a new term after the original oracle
loss to minimize the Kullback-Leibler divergence between
the predictive distribution of the non-specialized models
and a uniform distribution. Although CMCL boosts the
top-1 accuracy a lot, it does not take the diversity of the hy-
potheses into consideration which make it lose the merits of
multiple choice learning. Consequently, it under-performs
sMCL in terms of oracle performance.

In this paper, we argue that there exists a method that can
extend MCL methods to general prediction scenario while
keeps the merits of MCL settings. To this end, we develop a
new MCL method, called versatile MCL (vMCL in short),
which tries to harvest the advantages of existing MCL meth-
ods while overcoming their drawbacks effectively. Con-
cretely, vMCL aims to maintain high diversity while re-
straining overconﬁdence. Thus, vMCL is good in terms of
both oracle and top-1 metric which enables MCL more ap-
plicable in real-world. The major novelties and merits of
vMCL are as follows: 1) A conﬁdent hinge loss is proposed
to tackle the overconﬁdence problem, which can prevent
non-specialized models from making inaccurate predictions
with high conﬁdence. 2) A choice network is adopted to
learn the conﬁdence level of each specialist, so that more
reliable ﬁnal decision can be obtained by aggregating the
models’ diverse predictions. 3) vMCL can be easily imple-
mented and can be trained in an end-to-end manner, which
is very attractive for many real-world applications.

We evaluate vMCL on two vision computing tasks: im-
age classiﬁcation and segmentation. Experiments on four
public datasets show that vMCL not only signiﬁcantly lifts
the oracle performance (compared to sMCL) but also out-
performs the existing MCL methods in terms of top-1 accu-
racy. For a clear and quick understanding of the advantage
of our vMCL over the existing MCL methods which based
on deep learning, in Tab.1 we present a qualitative compari-
son among sMCL, CMCL and vMCL from ﬁve dimensions:
overﬁtting, overconﬁdence, hypotheses diversity, top-1 er-
ror and oracle error. In summary, vMCL is better (or not
worse) than sMCL and CMCL in all the ﬁve dimensions.

2. Related Work

Diversity is a good way to handle the uncertainty in AI
tasks. Generally, there are two types of approaches to gener-

Table 1: A qualitative comparison with state of the art MCL meth-
ods. ‘H’, ‘M’ and ‘L’ indicate high, medium and low respectively.
For overﬁtting and overconﬁdence, ‘H’ is bad and ‘L’ is good. For
hypotheses diversity, ‘H’ is good and ‘L’ is bad. For top-1 error
and oracle error, ‘H’ and ‘L’ means large and small, and the larger
the worse. In all cases, ‘M’ means the state between ‘H’ and ‘L’.

MCL
method

sMCL
CMCL
vMCL

Over-
ﬁtting

Over-

conﬁdent

Hypotheses

diversity

Top-1
error

Oracle
error

H
M
L

H
M
L

H
M
H

H
M
L

M
H
L

ate multiple diverse outputs. One is to infer m-best diverse
predictions from a single model, the other is to treat diver-
sity as a learning task by training multiple models.

Most methods of the ﬁrst type are probabilistic graphical
models that generate multiple predictions at the inference
step. [2] proposed an algorithm to generate diverse m-best
solutions. They approached the m best formulations in a
sequential mode, where the next solution is searched by an
integer programming optimization procedure. It is a greedy
algorithm that enables each prediction to be the lowest en-
ergy state but different from the previously ones. However,
due to the greedy nature, each solution is only inﬂuenced
by the previously predictions but not the upcoming ones.
To address this issue, [12] proposed a novel formulation to
jointly construct the m-best-diverse solutions by solving an
energy minimization in a speciﬁcally constructed graphical
model. They claimed that the method of [2] can be seen as
a greedy approximation of their algorithm. Recently, Di-
verseNet [5] learns to produce multiple hypotheses with a
control variable and for each example its training diagram
requires a set of labels rather than one label.

There are different ways to train multiple models, in-
clude the classical ensemble methods. In this work, we fo-
cus on multiple choice learning (MCL), which is a novel ap-
proach to generate multiple diverse solutions. The stochas-
tic multiple choice learning (sMCL) algorithm [15] ﬁrst
introduces oracle scheme into deep neural networks, then
minimizes the oracle loss of multiple deep networks. As a
result, each network is able to handle a subset of classes of
the classiﬁcation task. However, due to the oracle scheme,
each sample can be assigned only to one network and do
backward on that network. Thus, it is prone to overﬁtting
for each model when training data is not enough. Besides,
sMCL focuses only on the oracle performance, it may fail
in scenarios that need a deterministic output. Thus, sMCL
[22] extends
performs poorly in terms of top-1 accuracy.
the idea of sMCL by providing a mathematical understand-
ing why this formulation is beneﬁcial.

Recently, [14] proposed the conﬁdent MCL (CMCL)
algorithm that employs a new loss function named con-
ﬁdent oracle loss to alleviate the overconﬁdence problem

6350

Low­level 

Layers 

...

...

...

Specialist

Specialist

Specialist

Data

High­level 

Layers 

Classifiers

Predictions

p

1

Aggregation

p

2

p

g

0.9

0.08

p

3

Candidates 

Pool 

p

g

p

1

p

2

p

3

Dog
Dog
Cat
Cow

0.9 
0.08 
0.02 

0.02

Choice Network

Confidences

Figure 1: The architecture of our vMCL method with three networks, each of them is referred to as a specialist, where the choice network
takes the concatenation of the features produced by the low-level layers as the input and generates a conﬁdence distribution, which can be
used to aggregate the diverse predictions from all the networks (specialists) for generating a high-quality ﬁnal prediction. The candidates
pool is used to evaluate the diversity of those hypotheses as well as the accuracy of the ensemble.

of sMCL. The conﬁdent oracle loss adds a new term after
the original oracle loss to minimize the Kullback-Leibler
divergence between the predictive distribution of the non-
specialized models and a uniform distribution. Though
CMCL boosts the top-1 accuracy a lot, the diversity of the
hypotheses are much less than sMCL in many cases. This
indicates that KL-divergence is a double-edged sword, it
can help to avoid the overconﬁdence issue, meanwhile it
also reduce the diversity.

3. Preliminaries

Let D = {xi, yi}N

i=1 be a dataset where each instance xi
is a training example and yi is the label, fm(x) (m=1, ...,
M ) be an individual model, and M is the ensemble size (i.e.
the number of embedded individual models). Traditional
independent ensemble (IE) methods train each model over
the whole dataset by adopting the following objective:

min LIE =

N

M

Xi=1

Xm=1

ℓ(cid:16)yi, fm(xi)(cid:17).

(1)

Above, ℓ(·, ·) indicates a loss function. The predictions of
IE methods often have low variance.

Different from traditional ensemble methods, multiple
choice learning (MCL) [8] aims to specialize each individ-
ual model on a subset of the data, by minimizing the oracle

loss as follows:

min Loracle =

N

Xi=1

min

m∈{1,...,M }

ℓ(cid:16)yi, fm(xi)(cid:17) (2)

As the oracle loss is a non-continuous function, an iterative
block coordinate decent algorithm is used to optimize this
objective function.

Stochastic multiple choice learning (sMCL) [15] imple-
ments multiple choice learning by deep neural networks
with the following objective:

min LsM CL =

N

M

Xi=1

Xm=1

vm

i ℓ(cid:16)yi, pm(xi)(cid:17)

(3)

vm
i = 1,

vm
i ∈ {0, 1}.

s.t.

M

Xm=1

where pm(xi) is the prediction of the m-th network, and
vm
is an indicator variable that takes only 0 or 1. In each
i
iteration of the training procedure, sMCL feeds the training
data to each neural network and gets the output by doing
forward propagation over these networks respectively. Af-
ter the computation of oracle loss, it chooses the most ac-
curate network for i-th sample, say the m-th network, and
sets vm
i = 1. Then, the i-th training example does back-
ward propagation only on the m-th network. Consequently,
each network performs better on some classes than the other

6351

networks, i.e. each network becomes a specialist on some
particular classes.

4. Versatile Multiple Choice Learning

Fig.1 shows the architecture of the vMCL method, which
consists of two major parts:
(a) multiple specialist net-
works; and (b) a choice network. Specialist networks aim to
provide diverse outputs and the choice network makes the
ﬁnal decision of those specialists if necessary. There is no
restriction on network architecture selection for these spe-
cialists, which makes our method general and ﬂexible.

4.1. Choice Network

As we mentioned before, existing MCL algorithms per-
form poorly in terms of top-1 error rate because they lack
a scheme to aggregate the diverse predictions from all the
embedded models. These MCL algorithms are not suitable
for the scenarios that do not need human interference (e.g.
to select the best prediction). Here, we adopt a choice net-
work to predict the conﬁdence of each model. As shown
in Fig.1, the choice network is deployed just after a few
low-level layers of the specialists and it learns to generate
the conﬁdence of each specialist. Generally, the choice net-
work is a neural network whose input is the concatenation
of all specialist features and the output size is the ensemble
size M . The target labels of the choice network are dynam-
ically generated according to the MCL mechanism for each
iteration in the training phase.

Speciﬁcally, suppose we have M networks (specialists)
and {θm}M
m=1 are the parameters of those specialists. The
parameters of the choice network are denoted as ϑ. Let
Pθm(y|xi) be the prediction distribution of the m-th spe-
cialist on example xi. The output of choice network for
xi can be denoted as [wi
], which is calculated by
i = 1.
Popt(c|xi) is the aggregated prediction over all the special-
ists that output the probability of xi belonging to class c.
Formally,

a softmax layer on the logits and thus PM

i, ..., wM
i

m=1 wm

Popt(c|xi) =

M

Xm=1

wm

i Pθm (y = c|xi).

(4)

It superﬁcially looks like the mixture of experts (MoE)
method [26, 19]. They all provide a way to decide how
much a model can be relied on. The main difference lies
in that our method has explicit target labels for the choice
network, while MoE does not provide ground truth labels
for the gating neural network [6] because it does not need to
know which model is the specialist for a speciﬁc example.
MoE considers only the correctness of aggregated output,
thus it may not capable to provide multiple diverse outputs.
Though there are some approaches to estimate the labels for
the gating network, they are usually time-consuming.

4.2. Conﬁdent Hinge Loss

Overconﬁdence can be seen as a generalization issue in
machine learning. It happens in many scenarios, such as im-
balanced classiﬁcation [4]. This problem also exists in deep
learning, especially when the training dataset is not large
enough. It is quite normal that a deep neural network clas-
siﬁes a never-seen example to some particular classes with
high conﬁdence. Recently, some solutions were proposed to
deal with the overconﬁdence problem. For example, penal-
izing the conﬁdence of outputs that have low entropy distri-
butions [20].

Here, we propose a conﬁdent hinge loss to tackle the
overconﬁdent problem in MCL. The objective of vMCL is
deﬁned as follows:

min
v,θm,ϑ

L(D) =

N

M

C

vm

Xm=1

i ℓ(cid:16)yi, Pθm (y|xi)(cid:17) + ℓ(vi, wi)
Xi=1h
max(cid:16)Popt(c|xi) − Popt(yi|xi) + β, 0(cid:17)i
Xc6=yi
Xm=1

vm
i = 1, ∀i

M

vm
i ∈ {0, 1} , ∀i, m

+ α

s.t.

(5)

where vm
is the indicator variable as deﬁned in Eq.3, and
i
vm
i = 1 means that the m-th model is the best one for the i-
th example. In classiﬁcation tasks, ℓ(·, ·) is often selected as
cross entropy function. α is a hyper-parameter for balancing
the importance of the margin-based loss and β is a hyper-
parameter indicating the conﬁdence margin.

This objective function has three parts. The ﬁrst part is
the oracle loss, which aims to minimize the loss of the most
accurate model. The second part is the choice network loss
that enables vMCL to generate an accurate prediction by
learning the conﬁdence of each specialist. The choice net-
work learns to produce the best prediction by selection or
aggregation on these diverse outputs. The third part is our
conﬁdent hinge loss, which aims to address the overcon-
ﬁdence issue. The hinge loss is set up for the aggregated
prediction probability so that the correct class of each im-
age has a higher probability than the incorrect classes by a
ﬁxed margin β.

Formally, Popt(yi|xi) − Popt(c|xi) ≥ β for c 6= yi.
The idea is to depress the predictive probability of the non-
specialists when they make wrong predictions with high
conﬁdence. Meanwhile, it promotes the specialist to pre-
dict the true labels with high probability. Compared to
the conﬁdent oracle loss in [14], which desires each non-
specialist to output a uniform distribution that is meaning-
less for the candidates pool, our new loss is a sparse reg-
ularization term, which only rectiﬁes the incorrect predic-

6352

Figure 2: Histogram of the predictive distribution of our model (5 networks) tested on CIFAR-10 dataset. (a) Histogram of Popt which
is aggregated from all models. (b)-(f) show the probability residual between Popt and Pθm for m=1, 2, 3, 4 and 5 respectively. The
probability residuals near zero indicate the m-th model is specialized on these samples, while the probability residuals near 1 indicate the
m-th model is a non-specialist for those data.

tions of high conﬁdence. In other word, even if the maximal
probability in some hypotheses is very high (say 0.9), it will
not be punished if it does not affect the ﬁnal optimal predic-
tion. This property promotes the diversity among multiple
hypotheses.

We investigate the specialization of models by analyz-
ing their probability residuals. Here, the probability resid-
ual of the m-th model is deﬁned as rm(y|x)=Popt(y|x) -

Pθm(y|x)=Pj6=m wjPθj (y|x). Some empirical results are

shown in Fig.2. As shown in Fig.2(a), most optimal prob-
abilities are near 1, which indicates that our choice net-
work gives the optimal prediction with high conﬁdence.
Fig.2(b)-(f) show that each model only specializes a part
of samples/classes. The reason is that when the residual
rm(y|x) ≈ 0, the optimal prediction Popt is dominated by
the m-th model. On the contrary, rm(y|x) ≈ 1 means that
the m-th model has no contribution to Popt.

4.3. Training and Inference

i , v2

i , ..., vM
i

Training: We modify the optimization algorithm of
sMCL to solve Eq. (5). Considering that ℓ(vi, wi) is differ-
entiable for the network parameters, and we can get the tar-
get labels vi = [v1
] of the choice network from
the indicator variable vm
i . The predictive distribution wi is
obtained by a softmax function on the output of the choice
network. Thus, vMCL can be trained in an end to end man-
ner. Alg.1 presents the training procedure of vMCL, which
is based on stochastic gradient decent (SGD). Note that this
algorithm can be easily adopted to batch-wise SGD while
here we present sample-wise SGD for clarity.

Inference: For a test example xi, vMCL generates M
diverse outputs Pθm (y|xi) (m=1,..,M ) that are used to eval-
uate the oracle performance, and the aggregation of diverse
outputs Popt(xi) is used for generating a ﬁnal decision.

4.4. Feature Sharing

To cure the overﬁtting problem in MCL, we share the
weights of a few foremost convolutional layers among the
specialists, which is referred to as shared layers. It is differ-

Algorithm 1 Training Algorithm of vMCL
Input: Input dataset D, hyperparameters α, β
Output: the well-trained vMCL model.
1: Initialize parameters of specialists {θm}M

m=1 and

choice network ϑ

2: repeat
3:

Sample a batch S ∈ D
for m = 1 → M do

// Compute outputs for batch for each specialists
ym,1, ..., ym,|S| ← Pθm(S)

end for
for i = 1 → |S| do

vm
i = 0, m = 1, ..., M
// Select lowest error model per example
m∗ ← arg minm∈[1...M ] ℓ(yi, ym,i)
vm∗
i = 1
// Set the target of choice network
vi = [v1
// Compute the optimal prediction

i , ..., vM
i

]

i Pθm(y|xi)

m=1 wm

Popt(y|xi) =PM

// Compute the gradient of ϑ
∂L(xi)/∂ϑ
// Compute the gradient of θm, ∀m
∂L(xi)/∂θm

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

end for
Update the model parameters

22:
23: until convergence

ent from the feature sharing method in CMCL, where the
features of some speciﬁc layers are shared randomly. As
previous works [25] have proven that the ﬁrst a few layers
learn common patterns in deep CNNs, shared layers will
learn more general features on the whole dataset.

5. Performance Evaluation

We evaluate vMCL on two tasks:

image classiﬁcation
and segmentation. Three real-world image datasets includ-

6353

0.00.51.0(a)0200040006000Popt0.00.51.0(b)1-th0.00.51.0(c)2-th0.00.51.0(d)3-th0.00.51.0(e)4-th0.00.51.0(f)5-thFigure 3: Class-wise accuracy of different ensemble methods on CIFAR-10. Horizontal axis indicates the model’s indexes (labeled from 1
to 5) in each ensemble classiﬁer with 5 models, and vertical axis indicates the classes in the classiﬁcation task. The more concentration of
each column indicates a better specialization of the corresponding model.

Table 2: Classiﬁcation error rates on CIFAR-10 when different
techniques are optionally used in vMCL. ‘√’ means selected.

Ensemble
Method

Shared
Layers

Choice
Network

Oracle

Top-1

Error Rate

Error Rate

IE

sMCL

vMCL

-

-

-
√
-
√

-

-

-
-
√
√

7.2%

2.43%

1.79%
1.55%
1.14%
1.37%

15.74%

54.95%

15.14%
13.74%
13.64%
12.03%

ing CIFAR-10, SVHN and CIFAR-100 are used for classi-
ﬁcation, and the image dataset iCoseg is used for segmenta-
tion. In all experiments, we compare vMCL with the tradi-
tional independent ensemble (IE), stochastic MCL (sMCL)
and conﬁdent MCL (CMCL). For fairness, similar network
architecture and training strategy are used for all methods.

5.1. Datasets

• CIFAR-10 consists of 50,000 training examples and
10,000 test examples. Each image is of 32 × 32 pixel
size and the category number is 10.

• CIFAR-100 has the same basic statistics as CIFAR-10,

except that it contains 100 classes.

• SVHN is a digital image dataset that consists of 73,257
training images and 26,032 test images. It has the same
image size and category number as CIFAR-10. Fol-
lowing [14] and [27], we preprocess the images with
global contrast normalization and ZCA whitening.

• iCoseg consists of 38 groups of images with pixel-
level ground truth of foreground-background segmen-
tation of each image. We preprocess this dataset as
suggested in [14], i.e., randomly splitting the training
and test sets and resizing the images.

5.2. Image Classiﬁcation

Performance measures. We use the oracle and top-1 er-
ror rates to measure classiﬁcation performance. The top-1

error rate is evaluated by the weighted sum of all the predic-
tions of vMCL, and by averaging the output probabilities of
all models for the other methods. The oracle error rate indi-
cates the ratio of test images that are not correctly predicted
by any specialist, which can be formulated as follows:

eoracle =

N

M

1
N

Xi=1
1(x) =(0

1

1(ym,i 6= yi).

(6)

Ym=1

x = False,
x = True.

(7)

Above, 1(·) is an indicator function, ym,i is the m-th net-
work’s prediction on the i-th sample.

Training settings. We evaluate vMCL on a small net-
work with 3 conv layers and a large-scale ResNet. The
ensemble size of all methods is 5. The choice network is
deployed just after the last convolutional layer of the spe-
cialists. All methods are optimized by SGD with an initial
learning rate of 0.1, which is reduced linearly after a few
epochs. We use the Nesterov momentum that is set to 0.9.
The weight decay and the minibatch size are set to 0.0005
and 128, respectively. For each method, we run 5 times and
average the results.

Specialization comparison. Fig.3 gives the empirical
class-wise accuracy results of the four ensemble methods on
the test set of CIFAR-10. For each model in these methods,
the distribution of accuracy over different classes shows its
specialization. The more uniform the distribution is, the less
specialized the model is. We can see that IE lacks diversity
as each model performs similarly and has nearly uniform
distribution. sMCL and vMCL have higher specialization
than the models of CMCL, as sMCL and vMCL focus on
fewer classes with high accuracy than CMCL.

Ablation analysis. We check the effectiveness of major
techniques (shared layers and choice network) in vMCL by
conducting experiments with or without these techniques
on CIFAR-10. The results are shown in Tab.2, which are
compared with that of IE and sMCL.

6354

12345(a) IETruckShipHorseFrogDogDeerCatBirdAutomobileAirplane89.0%88.1%88.6%88.0%88.4%89.5%91.4%89.8%89.9%89.4%80.6%82.0%82.0%82.3%82.2%88.3%89.2%88.0%87.9%88.1%73.9%71.4%72.2%73.1%72.6%80.7%80.9%82.2%82.5%80.4%70.3%69.2%69.1%70.1%67.0%75.9%74.3%73.6%75.7%74.9%89.1%88.4%88.8%89.5%90.8%82.9%82.9%83.4%83.1%84.0%12345(b) sMCL0.0%0.0%0.0%0.0%98.5%0.0%0.0%0.0%97.7%0.0%0.0%0.0%0.0%97.6%0.0%0.0%0.0%96.1%0.0%0.0%98.5%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%99.1%0.0%95.2%0.0%0.0%0.0%0.0%0.0%77.2%45.6%0.0%98.3%0.0%0.0%0.0%0.0%0.0%97.0%0.0%0.0%0.0%12345(c) CMCL98.4%0.4%0.0%0.0%0.1%0.0%7.3%0.2%93.0%9.1%0.1%0.0%0.1%0.0%99.5%94.8%0.2%0.0%0.0%1.7%1.7%0.4%0.0%85.8%0.0%8.4%1.1%92.9%0.0%0.7%4.0%19.9%5.4%47.5%11.8%0.0%97.6%0.0%0.1%0.0%0.1%25.0%6.2%93.8%6.3%0.0%0.0%95.9%0.1%0.0%12345(d) rMCL0.0%99.5%0.0%0.0%0.0%96.6%0.0%45.8%0.0%0.0%0.0%0.0%97.6%0.0%0.0%0.0%0.0%0.0%0.0%95.5%0.0%99.0%0.0%0.0%0.0%0.0%0.0%0.0%98.9%0.0%0.0%0.0%61.4%0.0%95.6%98.2%0.0%0.0%67.6%0.0%96.8%0.0%0.0%0.0%0.0%0.0%0.0%96.7%0.0%0.0%Table 3: Performance comparison of different methods on CIFAR-10, SVHN and CIFAR100. Best results are in bold.

CIFAR10

SVHN

CIFAR100

Top-1 error Oracle error Top-1 error Oracle error Top-1 error Oracle error

Method

IE

sMCL
CMCL
vMCL

15.74%
58.56%
13.82%
12.03%

7.20%
2.43%
2.98%
1.37%

5.64%
35.74%
6.43%
5.88%

3.02%
1.55%
1.62%
1.22%

Cifar10
SVHN
CIFAR100

41.95%
52.86%
41.25%
38.07%

26.49%
24.38%
26.76%
19.32%

sMCL
CMCL
vMCL

7

6

5

4

3

2

)

%

(
 
e
t
a
r
 
r
o
r
r
e
 
e
l
c
a
r
O

)

%

(
 
e
t
a
r
 
r
o
r
r
e
 
e
l
c
a
r
O

25.0

20.0

15.0

10.0

5.0

0.0

Cifar10
SVHN
CIFAR100

SingleNet

IE

sMCL

CMCL

vMCL

)

%

(
 
e
t
a
r
 
r
o
r
r
e
 
1
-
p
o
T

50.0

40.0

30.0

20.0

10.0

0.0

SingleNet

IE

sMCL

CMCL

vMCL

1

2

3

K

4

5

(a) Oracle error rate of ResNet20

(b) Top1 error rate of ResNet20

(c) Overlapping K on CIFAR-10

Figure 4: (a) and (b) are classiﬁcation error rates of ResNet-20 on three datasets. (c) Oracle error rate vs. K overlapping.

We can see that both shared layers and choice network
can boost the performance of multiple choice learning obvi-
ously. And when both techniques are used, vMCL achieves
the best performance. In the following experiments, vMCL
is referred to as the one with shared layers by default.

Results on a small network. We ﬁrst compare vMCL
with other methods on a small network with only 3 convo-
lutional layers and 2 fully connected layers. The results are
in Tab.3. On CIFAR-10, vMCL achieves the best oracle er-
ror rate, which is nearly 43% lower than sMCL. In terms
of top-1 error rate, vMCL is relatively 12.95% better than
CMCL, though CMCL performs better than sMCL and IE.
Generally, images in SVHN contain relatively simpler
patterns than images in CIFAR-10, they are relatively easier
to be classiﬁed than that in CIFAR-10. So it is not surprised
to see that IE achieves a little better top-1 error rate than
vMCL. But vMCL is still better than sMCL and CMCL in
top-1 error rate. And vMCL outperforms the other methods
in oracle error rate, with the improvement up to 27.05%.

For CIFAR-100 that has a relatively large number of
classes, vMCL still achieves better oracle error rate than
sMCL, surprisingly with about 20% improvement. What is
more, vMCL has the lowest top-1 error rate, which is about
7.27% better than that of the CMCL method.

Results on a large network. We then compare vMCL
with the other methods on a large convolutional network
ResNet-20, whose architecture is the same as that in [14].
We use a single ResNet as the baseline, denoted by Sin-
gleNet, and set ensemble size to 5 for the four methods.
The results are shown in Fig.4(a), (b). Obviously, vMCL
outperforms the other methods.

As IE lacks diversity, it performs worse than the MCL

methods in oracle error rate. Each model in sMCL is de-
signed to be specialized on some subset of the data, the
overconﬁdence and overﬁtting problems make it perform
poorly in top-1 measure. Although CMCL improves top-1
error rate signiﬁcantly, it fails to reduce oracle error. This is
because its conﬁdent oracle loss impacts the specialization
of each network, by minimizing the KL-divergence between
the predictive distribution on non-specialized data and the
uniform distribution. Thanks to the conﬁdent hinge loss,
vMCL achieves much better oracle measure than sMCL.
With the shared layers and choice network, vMCL achieves
the best performance in terms of oracle error rate on the
three datasets. Moreover, vMCL is much better than CMCL
in terms of top-1 error rate.

m=1 vm

we meanPM

Overlapping effect. Here we check the effect of pick-
ing top-K best specialists at the training stage, which was
also investigated in previous MCL works. By overlapping,
i = K, where K is the overlapping size.
The results are shown in Fig.4(c). As K increases, the per-
formance of all methods turns better. However, when K ap-
proaches to the ensemble size M , the performance becomes
worse, because sMCL is degenerated to IE when K = M .
Effect of hyperparameters. We also investigate the sen-
sitivity of hyperparameters in vMCL. Due to the limit of
space, here we present only the results of β that indicates
the conﬁdence margin of the margin loss. If we do not use
the choice network, the ﬁnal prediction for each test exam-
ple is the average of all specialists’ predictions. Thus, β is
suggested to be larger than 1
M , where M is the number of
the specialists. As shown in Fig.5, given the dataset, the
performance is quite stable when β varies. The best values
of β for CIFAR-10, SVHN and CIFAR-100 are 0.3, 0.8 and

6355

30.0

20.0

10.0

Oracle error
Top-1 error

30.0

20.0

10.0

Oracle error
Top-1 error

0.0

0.2

0.4

0.6

0.8

0.0

0.2

0.4

0.6

0.8

60.0

40.0

20.0

Oracle error
Top-1 error

0.2

0.4

0.6

0.8

(a) CIFAR-10

(b) SVHN

(c) CIFAR-100

Figure 5: Sensitivity of β on three image datasets.

Table 4: Foreground-background segmentation results on iCoseg. The model size M is varied from 1 to 5. Best results are in bold.

Method

IE

sMCL

CMCL

vMCL

M
1
2
3
4
5

Top-1 error Oracle error

Top-1 error Oracle error

Top-1 error Oracle error

Top-1 error Oracle error

15.41%
14.79%
12.09%
11.69%
11.42%

15.41%
11.60%
10.85%
8.57%
7.41%

15.41%
16.65%
16.54%
15.58%
14.96%

15.41%
10.59%
7.00%
6.35%
6.35%

15.41%
11.60%
11.39%
10.99%
10.36%

15.41%
10.82%
8.26%
7.77%
7.8%

15.41%
10.98%
10.57%
9.99%
10.28%

15.41%
9.37%
7.02%
3.52%
3.07%

0.6 respectively. We also check the sensitivity of α, and
ﬁnd that the performance is insensitive to α value. So we
set α = 1 in all experiments.

5.3. Image Segmentation

Here, we evaluate vMCL on the segmentation task. As
iCoseg is a foreground-background segmentation dataset,
this task is formulated as a pixel-level classiﬁcation problem
with 2 classes. We select the images larger than 300×500
pixels, and randomly split the selected images into training
and test datasets for each class by a ratio of 80% (training)
: 20% (test). As suggested in [14], we resize the images
into 75×125 using bicubic interpolation, and design a Fully
Convolutional Network (FCN) [17] to do the segmentation
task. For each method, we change the ensemble size from 1
to 5 and train the network up to 300 epochs.

Different from the classiﬁcation task, here the prediction
error rate is deﬁned as the percentage of incorrectly labeled
pixels [8]. For IE, sCML and CMCL, the top-1 error rate is
measured by selecting the prediction that has a lower pixel-
wise entropy among the outputs. For vMCL, the top-1 error
rate is measured by using the ﬁnal aggregated prediction.
This is understandable as we choose the most conﬁdent pre-
diction from a candidate set. For all methods, the oracle
error rate is calculated as the lowest error rate over all out-
puts. We change the ensemble size from 1 to 5 and record
the results of both oracle and top-1 measures. The results
are shown in Tab.4.

As in the classiﬁcation task, compared with sMCL,
CMCL reduces the top-1 error rate signiﬁcantly. However,

it performs worse than sMCL in terms of oracle error rate.
vMCL not only outperforms sMCL in oracle error rate but
also has lower top-1 error than all the other methods.
In
summary, vMCL shows high specialization on the segmen-
tation task and handles the overconﬁdence problem well.

6. Conclusion

This paper develops a new MCL approach vMCL for
learning deep ensemble networks. vMCL aims to extend the
application scenarios of deep learning based MCL methods
include sMCL. By introducing some important techniques,
vMCL is able to maintain the diversity among multiple hy-
potheses and it can aggregate a better ﬁnal prediction that
is better than CMCL or independent ensemble (IE) method.
vMCL distinguishes itself from the existing MCL methods
in four aspects: 1) using a novel conﬁdent hinge loss to ad-
dress the overconﬁdence issue; 2) employing a choice net-
work to aggregate the diverse predictions; 3) exploring the
feature sharing technique to avoid overﬁtting; 4) can be eas-
ily implemented and can be trained in an end-to-end fash-
ion. Extensive experiments on image classiﬁcation and seg-
mentation show that vMCL signiﬁcantly outperforms the
state-of-the-art MCL methods.

Acknowledgement

This work was partially supported by National Natural
Science Foundation of China under grants No. U1636205
and No. 61772367.

6356

References

[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. Computer Science, 2014. 1

[2] Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera,
and Gregory Shakhnarovich. Diverse m-best solutions in
markov random ﬁelds.
In European Conference on Com-
puter Vision, pages 1–16. Springer, 2012. 1, 2

[3] Leo Breiman. Bagging predictors. Machine learning,

24(2):123–140, 1996. 1

[4] David A Cieslak and Nitesh V Chawla. Learning decision
trees for unbalanced data. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases,
pages 241–256. Springer, 2008. 4

[5] Michael Firman, Neill DF Campbell, Lourdes Agapito, and
Gabriel J Brostow. Diversenet: When one right answer is not
enough.
In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 5598–5607,
2018. 1, 2

[6] ZongYuan Ge, Alex Bewley, Christopher McCool, Peter
Corke, Ben Upcroft, and Conrad Sanderson. Fine-grained
classiﬁcation via mixture of deep convolutional neural net-
works.
In Applications of Computer Vision (WACV), 2016
IEEE Winter Conference on, pages 1–6. IEEE, 2016. 4

[7] Gabriela Ghimpet¸eanu, Thomas Batard, Marcelo Bertalm´ıo,
and Stacey Levine. A decomposition framework for image
denoising algorithms. IEEE transactions on Image Process-
ing, 25(1):388–399, 2016. 1

[8] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli.
Multiple choice learning: Learning to produce multiple
structured outputs. In Advances in Neural Information Pro-
cessing Systems, pages 1799–1807, 2012. 1, 3, 8

[9] Abner Guzman-Rivera, Pushmeet Kohli, Dhruv Batra, and
Rob Rutenbar. Efﬁciently enforcing diversity in multi-output
structured prediction. In Artiﬁcial Intelligence and Statistics,
pages 284–292, 2014. 1

[10] Hubert Kadima and Maria Malek. Toward ontology-based
personalization of a recommender system in social network.
In Soft Computing and Pattern Recognition (SoCPaR), 2010
International Conference of, pages 119–122. IEEE, 2010. 1

[11] Saurabh S Kataria, Krishnan S Kumar, Rajeev R Rastogi,
Prithviraj Sen, and Srinivasan H Sengamedu. Entity disam-
biguation with hierarchical topic models. In Proceedings of
the 17th ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 1037–1045. ACM,
2011. 1

[12] Alexander Kirillov,

Bogdan

Savchynskyy, Dmitrij
Schlesinger, Dmitry Vetrov, and Carsten Rother.
Infer-
ring m-best diverse labelings in a single one. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 1814–1822, 2015. 1, 2

[13] Alexander Kirillov, Dmytro Shlezinger, Dmitry P Vetrov,
Carsten Rother, and Bogdan Savchynskyy. M-best-diverse
labelings for submodular energies and beyond. In Advances
in Neural Information Processing Systems, pages 613–621,
2015. 1

[14] Kimin Lee, Changho Hwang, Kyoung Soo Park, and Jinwoo
Shin. Conﬁdent multiple choice learning.
In Proceedings
of the 34th International Conference on Machine Learning-
Volume 70, pages 2014–2023. JMLR. org, 2017. 2, 4, 6, 7,
8

[15] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael
Cogswell, Viresh Ranjan, David Crandall, and Dhruv Batra.
Stochastic multiple choice learning for training diverse deep
ensembles. In Advances in Neural Information Processing
Systems, pages 2119–2127, 2016. 1, 2, 3

[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, 2014. 1

[17] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3431–3440, 2015. 8

[18] David Madigan, Adrian E Raftery, C Volinsky, and J Hoet-
ing. Bayesian model averaging. In Proceedings of the AAAI
Workshop on Integrating Multiple Learned Models, Port-
land, OR, pages 77–83, 1996. 1

[19] Saeed Masoudnia and Reza Ebrahimpour. Mixture of ex-
perts: a literature survey. Artiﬁcial Intelligence Review,
pages 1–19, 2014. 4

[20] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz
Kaiser, and Geoffrey Hinton. Regularizing neural networks
by penalizing conﬁdent output distributions. arXiv preprint
arXiv:1701.06548, 2017. 4

[21] Dan Roth. Learning to resolve natural language ambiguities:
A uniﬁed approach. In AAAI/IAAI, pages 806–813, 1998. 1
[22] Christian Rupprecht, Iro Laina, Robert DiPietro, Maximil-
ian Baust, Federico Tombari, Nassir Navab, and Gregory D
Hager. Learning in an uncertain world: Representing ambi-
guity through multiple hypotheses. In International Confer-
ence on Computer Vision (ICCV), 2017. 2

[23] Robert E Schapire. The strength of weak learnability. Ma-

chine learning, 5(2):197–227, 1990. 1

[24] V. Sharmanska, D. Hern´andez-Lobato, J. M. Hern´andez-
Lobato, and N. Quadrianto. Ambiguity helps: Classiﬁcation
with disagreements in crowdsourced annotations.
In 2016
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2194–2202, June 2016. 1

[25] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How transferable are features in deep neural networks? In
Advances in neural information processing systems, pages
3320–3328, 2014. 5

[26] Seniha Esen Yuksel, Joseph N Wilson, and Paul D Gader.
Twenty years of mixture of experts.
IEEE transactions
on neural networks and learning systems, 23(8):1177–1193,
2012. 4

[27] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-

works. In BMVC, 2016. 6

6357

