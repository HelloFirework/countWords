Mutual Learning of Complementary Networks via Residual Correction for

Improving Semi-Supervised Classiﬁcation

Si Wu12

Jichang Li1 Cheng Liu2 Zhiwen Yu1 Hau-San Wong2

1School of Computer Science and Engineering, South China University of Technology

2Department of Computer Science, City University of Hong Kong

cswusi@scut.edu.cn, cslijichang@mail.scut.edu.cn, cliu272-c@my.cityu.edu.hk

zhwyu@scut.edu.cn, cshswong@cityu.edu.hk

Abstract

Deep mutual

learning jointly trains multiple essen-
tial networks having similar properties to improve semi-
supervised classiﬁcation. However, the commonly used
consistency regularization between the outputs of the net-
works may not fully leverage the difference between them.
In this paper, we explore how to capture the complemen-
tary information to enhance mutual learning. For this pur-
pose, we propose a complementary correction network (C-
CN), built on top of the essential networks, to learn the
mapping from the output of one essential network to the
ground truth label, conditioned on the features learnt by
another. To make the second essential network increasingly
complementary to the ﬁrst one, this network is supervised
by the corrected predictions. As a result, minimizing the
prediction divergence between the two complementary net-
works can lead to signiﬁcant performance gains in semi-
supervised learning. Our experimental results demonstrate
that the proposed approach clearly improves mutual learn-
ing between essential networks, and achieves state-of-the-
art results on multiple semi-supervised classiﬁcation bench-
marks. In particular, the test error rates are reduced from
previous 21.23% and 14.65% to 12.05% and 10.37% on
CIFAR-10 with 1000 and 2000 labels, respectively.

1. Introduction

One of the main limitations of applying deep convolu-
tional networks [16] [36] [12] is the need for massive col-
lection of labeled images. To bypass expensive manual
annotations, many studies have been performed on semi-
supervised learning [21] [5] [40] [2], such that the model-
s can be trained on partially labeled data, since it is more
practical to expect that only a small fraction of samples
can receive human annotations. In order to use unlabeled
data to improve the generalization capability of classiﬁers,

Figure 1. An example to illustrate how the proposed CCN im-
proves semi-supervised classiﬁcation on CIFAR-10 with 1000 la-
bels (classes 0-9 denote ‘plane’, ‘auto’, ‘bird’, ‘cat’, ‘deer’, ‘dog’,
‘frog’, ‘horse’, ‘ship’ and ‘truck’, respectively). CCN learns the
mapping from the raw output (input of the softmax layer) of one
network (Net 1) to the ground truth label, conditioned on the fea-
tures learnt by another network (Net 2). According to the raw
output, the ‘ship’ image is misidentiﬁed as ‘truck’. CCN is able to
produce a compensatory residual to correct the misclassiﬁcation.

semi-supervised methods rely on an important assumption
that it is more likely for neighboring data points to belong
to the same class, which means that the decision bound-
aries should be located in low-density regions. There are
many deep models that have been developed based on this
assumption, such as [35] [25] [34]. The prediction of a clas-
siﬁer should be consistent on the unlabeled data irrespective
of whether perturbations have been added. Previous meth-
ods including Temporal Ensembling [18], virtual adversari-
al training (VAT) [26] and adversarial dropout (VAdD) [29]
follow similar principles. On the other hand, mutual learn-
ing between separate networks is also effective for deter-
mining more reliable decision boundaries. Recent method-
s, such as dual learning [11], Mean-Teacher [38] and deep
mutual learning (DML) [42], have brought improvement in
semi-supervised classiﬁcation. Most of them penalize in-
consistent predictions of different networks on unlabeled
data. However, these methods only consider the difference
between them, while ignoring the complementarity.

We are concerned with the task of improving mutual

16500

Figure 2. An overview of our enhanced mutual learning model for semi-supervised classiﬁcation. Our model consists of two essential
networks having similar properties and a complementary correction network (CCN). CCN learns to more accurately classify the unlabeled
instances, conditioned on the output of one network and the features of the other network. The second essential network is supervised by
the output of CCN, and becomes increasingly complementary to the ﬁrst one as it learns. The resulting essential networks lead to signiﬁcant
performance gains due to complementary knowledge transfer via mutual learning.

learning for semi-supervised classiﬁcation. To fully utilize
the complementary information contained in the different
networks, we aim to learn the mapping from the output of
one network to the ground truth label, conditioned on the
feature learnt by another, as shown in Figure 1. This map-
ping not only learns the prediction deviation, but also helps
in improving classiﬁcation on unlabeled data. The result-
ing better predictions provide further guidance to the train-
ing of complementary networks, such that mutual learning
between these networks is able to bring signiﬁcant perfor-
mance gains.

In this paper, we present an enhanced mutual learn-
ing approach to train complementary networks for improv-
ing semi-supervised classiﬁcation. Speciﬁcally, we extend
the DML model by including a complementary correction
network (CCN) to capture complementary information be-
tween two essential networks. This new network is built on
top of the essential networks, and is conditionally depen-
dent on the raw output (input of the softmax layer) of one
network and the features provided by another. We adopt a
residual architecture, such that CCN is able to learn the dif-
ference between the raw output and ground truth label con-
ditioned on the input features. As a result, more accurate
classiﬁcation on unlabeled data is produced and utilized to
train the second essential network, which in turn becomes
more discriminative and complementary to the ﬁrst one as
it learns. By minimizing the divergence between these t-
wo essential networks, the knowledge learnt by CCN can
be ultimately transferred to the ﬁrst one, and lead to addi-
tional performance gains. An overview of the proposed ap-
proach is shown in Figure 2. In the experiments, we present
state-of-the-art results achieved on multiple standard semi-
supervised classiﬁcation benchmarks, and insights on why
the proposed approach works.

This work makes the following contributions.

(1) In-
stead of directly minimizing the prediction divergence be-
tween separate networks, we propose the CCN to signiﬁ-

cantly improve semi-supervised mutual learning, by captur-
ing and transferring complementary knowledge between the
networks. (2) CCN is able to use the learnt features of one
network to help correct the outputs of the other network.
The resulting more accurate classiﬁcation on unlabeled da-
ta is further leveraged to guide model training, such that
the networks become increasingly complementary as they
learn. (3) We demonstrate that the proposed enhanced mu-
tual learning model is more effective than the DML model,
and improves the state-of-the-art results on multiple stan-
dard semi-supervised learning benchmarks.

2. Related Work

We restrict our review to the closely related work, espe-
cially the recent advances in semi-supervised learning using
deep models. To aid classiﬁers to explore the categories of
unlabeled data, Generative Adversarial Networks (GANs)
[8] [30] [24] have been applied to semi-supervised learn-
ing, such as [14] [28] [17]. In [37], Springenberg proposed
a categorical GAN to regularize a discriminatively trained
classiﬁer, such that a robust classiﬁcation model can be
achieved. In [33], Salimans et al. explored various practical
techniques for improving the training of generative mod-
els and semi-supervised classiﬁcation. Furthermore, Wei
et al. [39] improved the training of Wasserstein GANs [1]
by including a consistency regularization to the discrimina-
tor, such that the Lipschitz continuity can be enhanced and
promising results are achieved. To characterize the class-
conditional distributions, Li et al.
[20] proposed a triple
generative adversarial network to include a classiﬁer in a
three-player formulation. Another similar work reported in
[7] presented a triangle GAN framework, in which two gen-
erators and two discriminators are employed to characterize
the joint distribution of instances and labels. In contrast to
the above GAN-based methods which aim to generate im-
ages as good as possible, the GAN in [4] generates ‘bad’
images which are located at the low density regions and

26501

thus may be close to the decision boundaries in the latent
space, based on which the discriminative capability of the
classiﬁer can be improved.

The perturbation-based models have shown promising
results through introducing noise to model training for re-
ducing overﬁtting, such as [31] [32]. In [18], the training is
performed by penalizing the difference between the predic-
tions of the network with and without stochastic augmenta-
tion, such that the smoothness in the output of the network
with respect to the input is encouraged. Similar to adver-
sarial training [9], Miyato et al. [26] [25] proposed a virtual
adversarial training method to select the perturbations in the
direction sensitive to the prediction of the classiﬁer. From
another perspective, adversarial dropout [29] was proposed
to generate the perturbation to model updating by maximiz-
ing the divergence between the predicted class distribution
and ground truth label.

Mutual learning is another effective strategy for improv-
ing semi-supervised learning. To acquire training experi-
ence from another network, distillation based methods [13]
were proposed to train a separate and relatively small net-
work. Different from distillation, mutual learning starts
with a set of essential networks, which jointly learn to solve
the tasks. In [3], Batra and Parikh proposed a cooperative
learning paradigm to jointly train multiple models special-
izing to different domains, and learn domain-invariant visu-
al attributes. In [42], Zhang et al. proposed a deep mutu-
al learning model which minimizes the divergence between
the outputs of two networks having different parameter ini-
tializations and dropout. To construct a better teacher model
for enhancing mutual learning, Tarvainen and Valpola [38]
adopted the exponential moving average of a student net-
work as a teacher to provide training targets for the student.
There are substantial differences between our proposed
framework and existing works. The main difference is in
the way the models are learnt. We propose the CCN which
is built on top of two essential networks. Its main role is
to learn to correct the output of one network, and guide the
training of the other network. As a result, the complemen-
tarity between the essential networks can be signiﬁcantly
enhanced. To our best knowledge, there have been no pre-
vious attempts to capture the complementary information
between separate networks for enhancing mutual learning,
in the way that our CCN is designed to do.

3. Proposed Approach

The semi-supervised setting naturally occurs for cases
in which a large number of images can be easily collected
from the web but only a small portion of them are man-
ually labeled. In our problem, we consider that the train-
ing set X = L ∪ U contains N instances, out of which
the subset L = {(xi, yi)}NL
i=1 is labeled and the remainder
U = {xj}NU
j=1 is unlabeled, where (xi, yi) denotes a labeled

instance and the corresponding class label, and xj denotes
an unlabeled instance. In the semi-supervised setting, we
have NL ≪ NU .

Deep mutual learning models usually consist of two or
more essential networks. Since deep convolutional net-
works for image classiﬁcation have high capacity, joint-
ly training two networks can achieve a trade-off between
performance gains and computational cost in most cases.
Here we introduce a dual-net based mutual learning mod-
el. Speciﬁcally, we design a CCN, parameterized by θC , to
leverage the complementary information between the two
essential networks parameterized by θ1 and θ2, respective-
ly. CCN can be expected to produce more accurate classiﬁ-
cation on unlabeled data, and guide the training of comple-
mentary essential networks in our model.

3.1. Enhanced Mutual Learning Model

We extend the DML model by including a CCN to lever-
age complementary information from essential networks.
CCN has two separate inputs, the raw output of one es-
sential network, as well as the learnt features of the other
essential network for modeling the divergence between the
raw output and ground truth label. Compared to the ﬁrst
network, CCN is able to produce more accurate classiﬁca-
tion on unlabeled data, which can be utilized for guiding
the training of the second network. By minimizing the di-
vergence between the two essential networks, both of them
can be further improved.

Speciﬁcally, the overall loss function L1 for the ﬁrst es-

sential network is composed of the following four terms:

L1(θ1; X ) = X

ℓ(cid:0)yi, hθ1 (xi)(cid:1) + X

H(cid:0)hθ1 (xj)(cid:1)

(xi,yi)∈L
A(θ1; xj) + λ X
xj ∈U

+ η X
xj ∈U

xj ∈U

DKL(cid:0)hθ2 (xj)khθ1 (xj)(cid:1),

(1)

where hθ1 (·) (hθ2 (·)) denotes the predicted class probabil-
ity distribution of the network θ1 (θ2) for an input, ℓ(·, ·)
denotes the cross-entropy function, H(·) denotes the con-
ditional entropy function with respect to the posterior class
probability distribution, A denotes a perturbation-based vir-
tual adversarial training term, and DKL(·k·) denotes the
Kullback-Leibler (KL) divergence between two differen-
t distributions. The coefﬁcients η and λ are the weighting
factors for achieving a balance among the terms in L1. In
Eq.(1), H(·) is used to quantify the amount of information
needed to describe the class label of an unlabeled instance
according to the network prediction as follows:

H(cid:0)hθ1 (xj)(cid:1) = −hθ1 (xj)T ln hθ1 (xj).

Minimization of the conditional entropy term enhances the
conﬁdence of the classiﬁer on unlabeled instances, which
in turn drives the decision boundaries away from data-dense

(2)

36502

regions to facilitate semi-supervised learning, as pointed out
in [10] [26]. To stabilize the estimation of the conditional
entropy on the unlabeled instances, A is used to smooth the
classiﬁer with respect to input perturbations as follows:

A(θ1; xj) = max
kνk≤ǫ

DKL(cid:0)hθ1 (xj)khθ1 (xj + ν)(cid:1),

(3)

where ǫ denotes a hyper-parameter controlling the intensity
of the adversarial perturbation ν. Furthermore, minimizing
the last term in L1 encourages the two networks to produce
consistent predictions. In fact, this mutual learning term is
important for providing training experience in the form of
predicted class distributions on unlabeled instances.

In addition, the overall loss function L2 for the second

essential network is deﬁned as follows:

L2(θ2; X ) = X

ℓ(cid:0)yi, hθ2 (xi)(cid:1) + X

H(cid:0)hθ2 (xj)(cid:1)

(xi,yi)∈L
+ X
xj ∈U
+ λ X
xj ∈U

xj ∈U

θC
j

, hθ2 (xj)(cid:1) + η X

A(θ2; xj)

xj ∈U

(4)

ℓ(cid:0)y

DKL(cid:0)hθ1 (xj)khθ2 (xj)(cid:1),

where yθC
j denotes the pseudo label of instance xj accord-
ing to the prediction of CCN (to be introduced in detail in
the next subsection). Note that the two essential networks
are trained under different supervision. Different from the
ﬁrst one, the second network learns to predict class labels
of unlabeled instances by imitating the outputs of CCN as
ground truth targets. As a result, the second network be-
comes increasingly complementary to the ﬁrst one, since it
should have similar classiﬁcation performance with CCN.

3.2. Complementary Correction Networks

We propose CCN to leverage the complementary infor-
mation from essential networks to produce more accurate
predictions. This network learns a mapping from the output
of one essential network to the ground truth label, condi-
tioned on the high level features of the other essential net-
work. Inspired by the work of He et al. [12], an important
feature of our CCN is an identity-skip connection, which
adds the raw output of the ﬁrst essential network to the end
of this correction module. This skip connection is different
from the residual network, due to the reason that we take
into account the learnt features of the second essential net-
work as side input, and thus our correction network is able
to capture the complementary information.

As shown in Figure 3, the raw output of the ﬁrst network
is projected into a higher dimensional embedding. The ab-
stract features of the second network are similarly projected
into a lower dimensional embedding. To combine these two
modalities, we concatenate the two embedding vectors, and
feed the resulting vector to two fully connected layers, such
that the vector is projected back into a valid label space. To

Figure 3. Illustration of the proposed CCN.

Table 1. The architecture of the CCN used in the proposed enhanced mu-
tual learning model.

Layer

Input
L − 4

L − 3

L − 2
L − 1
L − 0

Description

Raw output of Net 1

Features of Net 2

Fully connected 10→64,

Fully connected 128→64,

LReLU

LReLU

Fully connected 64→64,

LReLU

Concatenation, Fully connected 128→32, LReLU

Fully connected 32→10

Addition to the raw output, Softmax

formulate the overall loss function of CCN, we adopt the
cross entropy function as a classiﬁcation term to capture the
difference between the predicted and ground truth labels.
In addition, we choose the mean square distance to measure
the difference between the current and temporal ensemble
predictions as follows:

LC(θ1, θ2, θC; X ) = X(xi,yi)∈L

ℓ(cid:0)yi, hθC (xi)(cid:1)
+ µ Xxj ∈U(cid:13)(cid:13)hθC (xj) − τ θC
j (cid:13)(cid:13)

(5)

2

,

j

where τ θC
denotes the temporal ensemble prediction of C-
CN to the label of instance xj over previous training epochs.
Since there are only a small number of labeled samples, the
majority of training samples are unlabeled and may dom-
inate the overall loss of CCN. Similar to [18], we use a
ramp-up coefﬁcient µ for the second term at the beginning
to avoid this dominance. In our model, the temporal ensem-
ble predictions are the following exponential moving aver-
ages of label predictions

τ θC
j ← ατ θC

j + (1 − α)hθC (xj).

(6)

In each training epoch, the output of the network is accu-
mulated into a temporal ensemble output, and a momentum
coefﬁcient α is used to control the extent of ensembling in
the temporal dimension. Aggregating the previous predic-
tions is expected to be more accurate.

Since CCN learns the mapping from the raw output of
the ﬁrst essential network to the ground truth label, the cor-
rected class probability distribution can be computed as fol-
lows:

hθC (xj) = N(cid:16)gθ1 (xj) + δθC(cid:0)gθ1 (xj), fθ2 (xj)(cid:1)(cid:17),

(7)

46503

where N (·) denotes the normalized exponential function,
gθ1 (·) denotes the raw output of the ﬁrst essential network,
fθ2 (·) denotes the learnt representation on the global pool-
ing layer of the second essential network, and δθC (·, ·) de-
notes the residual learnt by CCN.

To make the second essential network complementary to
the ﬁrst one, the prediction of CCN can be used to produce
the training target of the second essential network. Specif-
ically, hθC (xj) = [hj;1, hj;2, . . . , hj;M ] is transformed to
an one-hot vector yθC
j = [yj;1, yj;2, . . . , yj;M ] as a pseudo
label of instance xj as follows:

yj;m =( 1,

0,

if hj;m = arg maxl(cid:2)hθC (xj)(cid:3)l

otherwise,

,

(8)

where M denotes the number of classes, and [·]l denotes
the l-th component of the predicted class probability vector,
indicating the probability of an instance belonging to the
l-th class.

The architecture of our CCN is shown in Table 1.

In
the training process, an input image is processed by the es-
sential networks to compute high-level image features and
produce class probability predictions. Then, the raw out-
put of the ﬁrst network and the features learnt by the sec-
ond network pass through CCN. The prediction of CCN is
transformed into the pseudo label of the unlabeled instance
with respect to the second network, but cannot incur gradi-
ents propagated back to itself. The implementation details
of our proposed model are summarized in Algorithm 1.

4. Experiments and Discussion

In this section, we perform extensive experiments to
verify the effectiveness of the proposed enhanced mutual
learning model for improving semi-supervised classiﬁca-
tion. Speciﬁcally, we ﬁrst evaluate our proposed approach,
and then compare with the state-of-the-art methods on mul-
tiple semi-supervised learning benchmarks. For better un-
derstandings of our work, we also investigate the effective-
ness of our proposed CCN and enhanced mutual learning
mechanism through ablation studies and visualization.

4.1. Experimental Settings

We highlight the effectiveness of our CCN with a toy
example, and then evaluate the proposed approach on the
MNIST [19], SVHN [27], CIFAR-10 and CIFAR-100 [15]
benchmarks, on which existing state-of-the-art methods for
semi-supervised classiﬁcation mostly focus. We report the
average classiﬁcation error and the corresponding standard
deviation over 10 runs on the test data.

Model Variants. We build the following variants of our
proposed model to assess the effectiveness of the improve-
ment strategies to the ﬁnal classiﬁcation performance.

Algorithm 1 Pseudo-code of our enhanced mutual learning model for
training two essential networks and CCN.

1: Input: Labeled data (xi, yi) ∈ L and unlabeled data xj ∈ U , weight-

s η, λ and µ, and number of training epochs T.

2: Initialize: Essential networks θ1 and θ2, CCN θC , temporal ensemble
of unlabeled samples, and

and pseudo labels yθC

j

predictions τ θC
j
learning rate γ.

3: for t = 1 to T do
4:
5:
6:

Randomly sample mini-batches from L and U .
for each mini-batch B do

according to Eq.(8).

Compute the raw outputs gθ1 (xi) and gθ1 (xj ), and evaluate
the ﬁrst essential network hθ1 (xi) and hθ1 (xj ).
Compute the features fθ2 (xi) and fθ2 (xj ), and evaluate the
second essential network hθ2 (xi) and hθ2 (xj ).
Evaluate CCN hθC (xi) and hθC (xj ).
θC
Compute y
j
Apply stochastic gradient descent and update θC ←
Adam(cid:0)∇θC
Apply stochastic gradient descent and update θ1 ←
Adam(cid:0)∇θ1 (cid:0)LC (θ1, θ2, θC ; B) + L1(θ1; B)(cid:1), θ1, γ(cid:1).
Apply stochastic gradient descent and update θ2 ←
Adam(cid:0)∇θ2 (cid:0)LC (θ1, θ2, θC ; B) + L2(θ2; B)(cid:1), θ2, γ(cid:1).
Update τ θC

LC (θ1, θ2, θC ; B), θC , γ(cid:1).

according to Eq.(6).

7:

8:

9:
10:

11:

12:

j

end for

13:
14:
15: end for
16: Return θ1, θ2 and θC .

‘Baseline’. We train two essential networks having the
same architecture as the proposed model by adopting the
DML model [42]. The ‘Baseline’ results serve as the lower
bound for our evaluation.

‘Our Model w/o ML’. We disable mutual learning be-
tween essential networks, by removing the divergence term
of their predictions from the corresponding loss functions,
to analyze the capability of CCN in correcting the predic-
tion of the ﬁrst essential network.

‘Our Model w/o CCN’. We remove the CCN from our
model to investigate its effectiveness in exploiting the com-
plementary information for enhancing mutual learning be-
tween the essential networks.

‘Our Model w/o VAT’. We remove the divergence ter-
m of virtual adversarial training from the loss functions of
the essential networks to train another variant of our mod-
el, such that we can investigate the complementarity of our
model with the existing technique [26].

4.2. Toy Example

To highlight the effectiveness of our CCN, we test the
variant ‘Our Model w/o ML’ on the well-known ‘two-
spirals’ synthetic dataset. We generate 1000 data points per
class, and there are a total of 40 labeled data points. We
adopt two essential networks consisting of 3 hidden layer-
s of size 300 nodes with ReLU, and a corresponding CCN
in our model. In Figure 4, we visualize the learnt decision
boundaries during training to illustrate how the CCN cor-
rects the predictions of the ﬁrst essential network.

56504

Table 2. Test error rates (%) of our models and the previous state-of-the-art methods on the MNIST, SVHN and CIFAR-10 datasets. The proposed approach
achieves more accurate classiﬁcation than the competing methods in all the cases.

MNIST

SVHN

CIFAR-10

Method

50 labels

100 labels

500 labels

1000 labels

1000 labels

2000 labels

4000 labels

LadderNetwork[31]
CatGAN[37]
Improved GAN[33]
ALI[6]
TripleGAN[20]
GoodBadGAN[4]
SPCTN[41]
Π-model[18]
Temporal-Ensembling[18]
Mean-Teacher[38]
VAT[26]
VAdD[29]
VAdD+VAT[29]
SNTG+Π-model[22]
SNTG+VAT[22]
CT-GAN[39]

-
-

2.21±1.36

-

1.56±0.72

-

1.72±0.13
1.02±0.37

1.06±0.37
1.39±0.28
0.93±0.07

-

0.91±0.58
0.80±0.10
1.00±0.11
0.89±0.15

-
-
-
-
-

-
-
-
-
-

-
-
-
-
-
-

9.79±1.24
6.65±0.53
5.12±0.13
4.18±0.27

-
-
-

0.94±0.42

0.66±0.07

4.52±0.30

-
-

-

0.89±0.13

-
-

-
-

8.11±1.30
7.42±0.65
5.77±0.17
4.25±0.03
7.37±0.30
4.82±0.17
4.42±0.16
3.95±0.19
3.74±0.09
4.16±0.08
3.55±0.05
3.82±0.25
3.83±0.22

-

-
-
-
-
-
-
-

31.65±1.20
23.31±1.01

-
-
-
-

-
-

19.61±2.09

-
-
-

17.99±0.50
17.57±0.44
15.64±0.39
15.73±0.31

-
-
-

21.23±1.27

14.65±0.31

-
-

-
-

20.40±0.47
19.58±0.58
18.63±2.32
17.99±1.62
16.99±0.36
14.41±0.03
14.17±0.27
12.36±0.31
12.16±0.24
12.31±0.28
11.96±0.10
11.68±0.19
10.07±0.11
11.00±0.13
9.89±0.34
9.98±0.21

Baseline
Our Model

8.48±1.03
0.67±0.13

3.47±0.67
0.42±0.11

15.03±0.11
3.63±0.21

10.74±0.10
3.36±0.18

29.57±0.89
12.05±0.42

20.97±0.37
10.37±0.31

15.33±0.31
8.80±0.24

Table 3. Test error rates (%) of our model and the variants on the
CIFAR-10 dataset.

Method

1000 labels

2000 labels

4000 labels

Baseline
Our Model w/o ML
Our Model w/o CCN
Our Model w/o VAT
Our Model

29.57±0.89
19.71±0.86
20.41±0.42
16.74±0.19
12.05±0.42

20.97±0.37
14.59±0.75
13.34±0.27
13.06±0.20
10.37±0.31

15.33±0.31
11.50±0.42
11.45±0.22
10.54±0.18
8.80±0.24

Figure 4. Comparison between the ﬁrst essential network (upper
row) and CCN (bottom row) in ‘Our Model w/o ML’ during train-
ing on the synthetic dataset. The labeled data points are marked
black. Different colors indicate different classes. CCN efﬁciently
converges to a better solution than the ﬁrst network.

4.3. Comparison on Benchmarks

Comparison to Previous Work. We ﬁrst report the re-
sults of the proposed approach, and perform a comparison
with the existing state-of-the-art semi-supervised learning
methods on the MNIST, SVHN and CIFAR-10 benchmark-
s. Table 2 shows the results of our model and the competing
methods on these benchmarks for the cases where different
number of labels are given. For a fair comparison, we evalu-
ate the ﬁrst essential network of our model in the test phase,
instead of the ensemble of the essential networks, although
there are three well-trained networks available at the end
of training process. Compared to the competing methods,
‘Our Model’ achieves the best results in all the cases. In par-

Figure 5. Comparison of the two essential networks and CCN in
our model on the MNIST, SVHN and CIFAR-10 datasets. The
three networks achieve very similar performance in all cases due
to complementary knowledge transfer during mutual learning.

ticular, the test error rate of the proposed approach reaches
12.05% and 10.37% on CIFAR-10 with 1000 and 2000 la-
bels, which are lower than those of the second best method
‘SNTG+Π-model’ (21.23% and 14.65%) by about 9.2 and
4.3 percentage points, respectively.
It is noted that ‘Our
Model’ outperforms the previous state-of-the-art methods
by a large margin.

Comparison to Model Variants. To conﬁrm the effec-
tiveness of the proposed approach, we also report the re-

66505

Figure 6. An example to illustrate the effectiveness of CCN on
CIFAR-10 with 1000 labels. In the left subﬁgure, CCN outper-
forms the ﬁrst essential network. In the right subﬁgure, the amount
of true corrections is much greater than that of false corrections,
which indicates that CCN is able to produce more accurate classi-
ﬁcation on unlabeled data.

sults of ‘Baseline’ on the benchmarks. Table 2 shows that
‘Our Model’ signiﬁcantly outperforms ‘Baseline’ in all the
cases. On MNIST with 50 labels, SVHN with 500 labels
and CIFAR-10 with 1000 labels, the test error rates are re-
duced from 8.48%, 15.03% and 29.57% to 0.67%, 3.63%
and 12.05%, and the corresponding performance gains are
7.8, 11.4 and 17.5 percentage points, respectively. Since the
essential networks in ‘Baseline’ have the same architecture
as the networks in ‘Our Model’, we consider that our CCN
and enhanced mutual learning mechanism lead to the sig-
niﬁcant performance gains. To investigate the relative con-
tributions of the improvement strategies, we perform a com-
parison between our model and the variants on CIFAR-10,
and Table 3 shows that removing the corresponding terms
leads to a signiﬁcant drop in performance. We consider that
CCN is important in facilitating mutual learning, and addi-
tional performance gains can be achieved by incorporating
perturbation-based adversarial training.

4.4. Model Analysis

To provide insights on why the proposed approach work-
s, we investigate how the proposed CCN and enhanced mu-
tual learning mechanism improve the classiﬁcation perfor-
mance of the ﬁnal model in the following four aspects.

Comparison of Member Networks. Our enhanced mu-
tual learning model consists of three networks: two essen-
tial networks (‘Net 1’ and ‘Net 2’) and CCN. We compare
these networks on all the three benchmarks. Figure 5 shows
the average test error rates of the three networks for the dif-
ferent cases. One can observe that the three networks have
very similar performance. This phenomenon is consisten-
t with the characteristics of mutual learning. The second
essential network learns to mimic CCN, and transfers the
learnt knowledge to the ﬁrst essential network by minimiz-
ing their prediction divergence.

Effectiveness of CCN. To verify the capability of our
CCN in correcting the raw output of the ﬁrst essential net-
work, we compare the three networks of the variant ‘Our

Figure 7. Representative results of CCN correcting the raw out-
put of the ﬁrst essential network on CIFAR-10 with 1000 label-
s (classes 0-9 denote ‘plane’, ‘auto’, ‘bird’, ‘cat’, ‘deer’, ‘dog’,
‘frog’, ‘horse’, ‘ship’ and ‘truck’, respectively). Although these
images are misclassiﬁed according to the raw outputs, compen-
satory residuals can be learnt by CCN such that these misclassiﬁ-
cations can be corrected.

Model w/o ML’ in Figure 6. The left subﬁgure shows the
performance of these three networks on CIFAR-10 with
1000 labels. Since CCN learns the residual between the
raw output and ground truth label by exploiting the comple-
mentary information from the second essential network, it
performs better than the ﬁrst essential network. In addition,
the second essential network is supervised by the output of
CCN, and thus these two networks have very similar per-
formance. In the right subﬁgure, we plot the numbers of the
test instances on which the outputs of the ﬁrst essential net-
work are truly corrected and falsely corrected, respectively.
The result shows that the amount of true corrections is much
greater than that of false corrections, which indicates that
our CCN does improve classiﬁcation on unlabeled data by
utilizing the complementary information between essential
networks. Some representative corrections are visualized in
Figure 7.

Effectiveness of Enhanced Mutual Learning. In our
model, CCN contributes to forming a teacher by guiding
the training of the second essential network, and transfer-
ring the knowledge to the ﬁrst essential network via mutual
learning with the second essential network. To demonstrate
the superiority of the proposed model, Figure 8 shows the
performance improvement of ‘Our Model’ over ‘Baseline’
during the training on SVHN with 500 labels and CIFAR-

76506

Figure 8. Comparison of the baseline model and our model on
SVHN with 500 labels (left) and CIFAR10 with 1000 labels
(right). Compared to the two networks of ‘Baseline’, the three
networks of ‘Our Model’ consistently and efﬁciently converge to
better solutions during training, which veriﬁes the effectiveness
and superiority of the proposed approach in mutual learning.

Figure 9. The t-SNE plot of the last hidden layer on the test data
of CIFAR-10 with 1000 labels: the baseline model (left) and our
model (right). Our model can learn more discriminative represen-
tations on which separating the data points of the difﬁcult classes
including ‘cat’, ‘deer’ and ‘dog’ becomes easier.

Table 4. Test error rates (%) of our model and the previous state-
of-the-art methods on the CIFAR-100 dataset.

Method

5000 labels

10000 labels

Π-model[18]
Temporal Ensembling[18]
SNTG+Π-model[22]

-
-
-

39.19±0.36
38.65±0.51
37.97±0.29

Baseline
Our Model

53.58±0.45
43.42±0.31

40.83±0.29
35.28±0.23

10 with 1000 labels. In contrast to ‘Baseline’ which only
penalizes the prediction divergence between essential net-
works, CCN explores more information from both essential
networks, and is thus able to improve the prediction qual-
ity. Furthermore, the second essential network is able to
learn better abstract representations by using the corrected
prediction. In turn, the second network contributes to the
performance gains of the ﬁrst network through penalizing
the prediction divergence between them.

Visualization. We further visualize the learnt represen-
tations of the baseline model and our model on CIFAR-
10 with 1000 labels. We use the ﬁrst essential networks
in ‘Baseline’ and ‘Our Model’ for comparison. Figure 9
shows the features of the last hidden layer projected to 2 di-
mensions by using t-SNE [23]. The instances are all from
the test data, and different classes are encoded by different

colors. It can be observed that the learned representations
of the proposed model are more concentrated, and can be
easily divided into different groups.

4.5. Results on CIFAR 100

CIFAR-100 is a more challenging benchmark for semi-
supervised classiﬁcation due to the reason that there are 100
categories. There are a few methods tested on this bench-
mark. Table 4 shows the results of our model and the com-
peting methods. Similar to the results achieved on the other
benchmarks, ‘Our Model’ signiﬁcantly improves ‘Baseline’
in both cases. When given 10000 labels, the test error rate is
reduced to 35.28%, which is lower than the previous state-
of-the-art result (37.97%). The results on CIFAR-100 veri-
fy the effectiveness of our enhanced mutual learning when
dealing with more difﬁcult benchmarks.

5. Conclusion

This work explores how to enhance mutual learning be-
tween deep convolutional networks for improving semi-
supervised classiﬁcation. We show that simply minimiz-
ing the prediction divergence between two separate essen-
tial networks may not fully leverage the difference between
them. To capture this information, we propose a comple-
mentary correction network, built on top of the essential
networks, to correct the prediction of one network, con-
ditioned on the features learnt by another. The resulting
more accurate class predictions for the unlabeled instances
are used as the training targets to make the second essential
network become more complementary to the ﬁrst one. As
a result, our enhanced mutual learning model leads to sig-
niﬁcant performance gains, due to the reason that the learnt
knowledge can be ultimately transferred to the ﬁrst essential
network. Our experiments demonstrate that the proposed
approach improves the state-of-the-art results on multiple
semi-supervised classiﬁcation benchmarks.

Acknowledgments

This work was supported in part by the National Natu-
ral Science Foundation of China (Project No. 61502173,
U1611461, 61722205, 61751205, 61572199), in part by
the Research Grants Council of the Hong Kong Special
Administration Region (Project No. CityU 11300715),
in part by City University of Hong Kong (Project No.
7005055), in part by the Natural Science Foundation of
Guangdong Province (Project No. 2016A030310422), in
part by Key R&D Program of Guangdong Province (Project
No. 2018B010107002), and in part by the Fundamental
Research Funds for the Central Universities (Project No.
2018ZD33).

86507

References

[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein genera-
tive adversarial networks. In Proc. International Conference
on Machine Learning, 2017.

[2] P. Bachman, O. AIsharif, and D. Precup. Learning with
pseudo-ensembles. In Proc. Advances in Neural Information
Processing Systmes, pages 3365 – 3373, 2014.

[3] T. Batra and D. Parikh. Cooperative learning with visual

attributes. In arXiv preprint arXiv:1705.05512, 2017.

[4] Z. Dai, Z. Yang, F. Yang, W. Cohen, and R. Salakhutdinov.
Good semi-supervised learning that requires a bad GAN. In
Proc. Advances in Neural Information Processing Systems,
pages 6513 – 6523, 2017.

[5] Z. Ding, N. Nasrabadi, and Y. Fu. Semi-supervised deep do-
main adaptation via coupled neural networks. IEEE Trans-
actions on Image Processing, 27(11):5214 – 5224, 2018.

[6] V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A.
Lamb, M. Arjovsky, and A. Courville. Adversarially learned
inference.
In Proc. International Conference on Learning
Representation, 2017.

[7] Z. Gan, L. Chen, W. Wang, Y. Pu, Y. Zhang, H. Liu, C. Li,
and L. Carin. Triangle generative adversarial networks. In
Proc. Advances in Neural Information Processing Systems,
2017.

[8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.
Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gener-
ative adversarial nets. In Proc. Advances in Neural Informa-
tion Processing Systems, pages 2672 – 2680, 2014.

[9] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples. In Proc. International Con-
ference on Learning Representation, 2015.

[10] Y. Grandvalet and Y. Bengio. Semi-supervised learning by
entropy minimization. In Proc. Advances in Neural Informa-
tion Processing Systems, 2004.

[11] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W. Ma.
Dual learning for machine translation.
In Proc. Advances
in Neural Information Processing Systems, pages 820 – 828,
2016.

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proc. IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 770 – 778, 2016.

[13] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowl-
edge in a neural network. In Proc. NIPS Deep Learning and
Representation Learning Workshop, 2014.

[14] D. Kingma, S. Mohamed, D. Rezende, and M. Welling.
Semi-supervised learning with deep generative models.
In
Proc. Neural Information Processing Systmes, pages 3581 –
3589, 2017.

[15] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. In Univ. Toronto, Toronto, ON,
Canada, Tech. Rep., 2009.

[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Proc. Neural Information Processing Systmes, pages 1106 –
1114, 2014.

[17] A. Kumar, P. Sattigeri, and T. Fletcher. Semi-supervised
learning with GANs: manifold invariance with improve in-
ference. In Proc. Advances in Neural Information Processing
Systmes, pages 5534 – 5544, 2017.

[18] S. Laine and T. Aila.

supervised learning.
Learning Representations, 2017.

Temporal ensembling for semi-
In Proc. International Conference on

[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278 – 2324, 1998.

[20] C. Li, K. Xu, J. Zhu, and B. Zhang. Triple generative ad-
versarial nets. In Proc. Advances in Neural Information Pro-
cessing Systems, pages 1195 – 1204, 2017.

[21] C. Li, J. zhu, and B. Zhang. Max-margin deep generative
models for (semi-) supervised learning. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2018.

[22] Y. Luo, J. Zhu, M. Li, Y. Ren, and B. Zhang. Smooth neigh-
bors on teacher graphs for semi-supervised learning. In Proc.
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2018.

[23] L. Maaten and G. Hinton. Visualizing data using t-sne. Jour-
nal of Machine Learning Research, 9(11):2579 – 2605, 2008.
[24] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral
normalization for generative adversarial networks. In Proc.
International Conference on Learning Representation, 2018.
[25] T. Miyato, S. Maeda, S. Ishii, and M. Koyama. Virtual
adversarial training: a regularization method for supervised
and semi-supervised learning. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2018.

[26] T. Miyato, S. Maeda, M. Koyama, K. Nakae, and S. Ishii.
Distributional smoothing with virtual adversarial training. In
Proc. International Conference on Learning Representation-
s, 2016.

[27] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A.
Ng. Reading digits in natural images with unsupervised fea-
ture learning.
In Proc. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.

[28] A. Odena. Semi-supervised learning with generative adver-
sarial networks. In Proc. International Conference on Learn-
ing Representation, 2016.

[29] S. Park, J. Park, S. Shin, and I. Moon. Adversarial dropout
for supervised and semi-supervised learning. In Proc. AAAI
Conference on Artiﬁcial Intelligence, 2018.

[30] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. In Proc. International Conference on Learn-
ing Representation, 2016.

[31] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T.
Raiko. Semi-supervised learning with ladder networks. In
Proc. Neural Information Processing Systmes, pages 3546 –
3554, 2015.

[32] M. Sajjadi, M. Javanmardi, and T. Tasdizen. Regularization
with stochastic transformations and perturbations for deep
semi-supervised learning. In Proc. Advances in Neural In-
formation Processing Systems, pages 1163 – 1171, 2016.

[33] T. Salimans, I. Goodfellow, W. Zaremba, and V. Cheung. Im-
proved techniques for training GANs. In Proc. Neural Infor-
mation Processing Systmes, pages 2234 – 2242, 2016.

96508

[34] U. Shaham, K. Stanton, H. Li, B. Nadler, R. Basri, and Y.
Kluger. SpectralNet: spectral clustering using deep neural
networks.
In Proc. International Conference on Learning
Representation, 2018.

[35] R. Shu, H. Bui, H. Narui, and S. Ermon. A DIRT-T approach
In Proc. International

to unsupervised domain adaptation.
Conference on Learning Representations, 2018.

[36] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In Proc. Inter-
national Conference on Learning Representation, 2015.

[37] J. Springenberg. Unsupervised and semi-supervised learning
with categorical generative adversarial networks. In Proc. In-
ternational Conference on Learning Representations, 2016.
[38] A. Tarvainen and H. Valpola. Mean teachers are better role
models: weight-averaged consistency targets improve semi-
supervised deep learning results. In Proc. Advances in Neu-
ral Information Processing Systems, 2017.

[39] X. Wei, B. Gong, Z. Liu, W. Lu, and L. Wang. Improving
the improved training of wasserstein GANs: a consistency
term and its dual effect. In Proc. International Conference
on Learning Representations, 2018.

[40] H. Wu and S. Prasad. Semi-supervised deep learning using
pseudo labels for hyperspectral image classiﬁcation. IEEE
Transactions on Image Processing, 27(3):1259 – 1270, 2018.
[41] S. Wu, Q. Ji, S. Wang, H. Wong, Z. Yu, and Y. Xu. Semi-
supervised image classiﬁcation with self-paced cross-task
networks.
IEEE Transactions on Multimedia, 20(4):851–
865, 2018.

[42] Y. Zhang, T. Xiang, T. Hospedales, and H. Lu. Deep mutual
learning. In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, 2018.

106509

