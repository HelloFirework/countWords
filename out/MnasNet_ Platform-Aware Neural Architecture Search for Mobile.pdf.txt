MnasNet: Platform-Aware Neural Architecture Search for Mobile

Mingxing Tan1

Bo Chen2 Ruoming Pang1 Vijay Vasudevan1 Mark Sandler2 Andrew Howard2 Quoc V. Le1

{tanmingxing, bochen, rpang, vrv, sandler, howarda, qvl}@google.com

1Google Brain, 2Google Inc.

Abstract

Designing convolutional neural networks (CNN) for
mobile devices is challenging because mobile models need
to be small and fast, yet still accurate. Although signiﬁcant
efforts have been dedicated to design and improve mobile
CNNs on all dimensions, it is very difﬁcult to manually
balance these trade-offs when there are so many architec-
tural possibilities to consider.
In this paper, we propose
an automated mobile neural architecture search (MNAS)
approach, which explicitly incorporate model latency into
the main objective so that the search can identify a model
that achieves a good trade-off between accuracy and
latency. Unlike previous work, where latency is considered
via another, often inaccurate proxy (e.g., FLOPS), our
approach directly measures real-world inference latency
by executing the model on mobile phones.
To further
strike the right balance between ﬂexibility and search
space size, we propose a novel factorized hierarchical
search space that encourages layer diversity throughout
the network. Experimental results show that our approach
consistently outperforms
state-of-the-art mobile CNN
models across multiple vision tasks. On the ImageNet
classiﬁcation task, our MnasNet achieves 75.2% top-1
accuracy with 78ms latency on a Pixel phone, which is
1.8× faster than MobileNetV2 [29] with 0.5% higher
accuracy and 2.3× faster than NASNet [36] with 1.2%
higher accuracy. Our MnasNet also achieves better mAP
quality than MobileNets for COCO object detection. Code
is at https://github.com/tensorflow/tpu/
tree/master/models/official/mnasnet.

Figure 1: An Overview of Platform-Aware Neural Archi-
tecture Search for Mobile.

Figure 2: Accuracy vs. Latency Comparison – Our Mnas-
Net models signiﬁcantly outperforms other mobile models
[29, 36, 26] on ImageNet. Details can be found in Table 1.

1. Introduction

Convolutional neural networks (CNN) have made signif-
icant progress in image classiﬁcation, object detection, and
many other applications. As modern CNN models become
increasingly deeper and larger [31, 13, 36, 26], they also be-
come slower, and require more computation. Such increases
in computational demands make it difﬁcult to deploy state-
of-the-art CNN models on resource-constrained platforms

such as mobile or embedded devices.

Given restricted computational resources available on
mobile devices, much recent research has focused on de-
signing and improving mobile CNN models by reducing
the depth of the network and utilizing less expensive oper-
ations, such as depthwise convolution [11] and group con-
volution [33]. However, designing a resource-constrained
mobile model is challenging: one has to carefully balance
accuracy and resource-efﬁciency, resulting in a signiﬁcantly
large design space.

2820

Sample models from search spaceTrainerMobile phonesMulti-objective rewardlatencyrewardControlleraccuracy050100150200InferenceLatency(ms)7071727374757677ImagenetTop1Accuracy(%)MobileNetV1MobileNetV2MobileNetV2(1.4)NASNet-AAmoebaNet-AMnasNetIn this paper, we propose an automated neural architec-
ture search approach for designing mobile CNN models.
Figure 1 shows an overview of our approach, where the
main differences from previous approaches are the latency
aware multi-objective reward and the novel search space.
Our approach is based on two main ideas. First, we formu-
late the design problem as a multi-objective optimization
problem that considers both accuracy and inference latency
of CNN models. Unlike in previous work [36, 26, 21] that
use FLOPS to approximate inference latency, we directly
measure the real-world latency by executing the model on
real mobile devices. Our idea is inspired by the observa-
tion that FLOPS is often an inaccurate proxy: for exam-
ple, MobileNet [11] and NASNet [36] have similar FLOPS
(575M vs. 564M), but their latencies are signiﬁcantly dif-
ferent (113ms vs. 183ms, details in Table 1). Secondly, we
observe that previous automated approaches mainly search
for a few types of cells and then repeatedly stack the same
cells through the network. This simpliﬁes the search pro-
cess, but also precludes layer diversity that is important for
computational efﬁciency. To address this issue, we propose
a novel factorized hierarchical search space, which allows
layers to be architecturally different yet still strikes the right
balance between ﬂexibility and search space size.

We apply our proposed approach to ImageNet classiﬁca-
tion [28] and COCO object detection [18]. Figure 2 sum-
marizes a comparison between our MnasNet models and
other state-of-the-art mobile models. Compared to the Mo-
bileNetV2 [29], our model improves the ImageNet accuracy
by 3.0% with similar latency on the Google Pixel phone.
On the other hand, if we constrain the target accuracy, then
our MnasNet models are 1.8× faster than MobileNetV2
and 2.3× faster thans NASNet [36] with better accuracy.
Compared to the widely used ResNet-50 [9], our MnasNet
model achieves slightly higher (76.7%) accuracy with 4.8×
fewer parameters and 10× fewer multiply-add operations.
By plugging our model as a feature extractor into the SSD
object detection framework, our model improves both the
inference latency and the mAP quality on COCO dataset
over MobileNetsV1 and MobileNetV2, and achieves com-
parable mAP quality (23.0 vs 23.2) as SSD300 [22] with
42× less multiply-add operations.

To summarize, our main contributions are as follows:

1. We introduce a multi-objective neural architecture
search approach that optimizes both accuracy and real-
world latency on mobile devices.

2. We propose a novel factorized hierarchical search
space to enable layer diversity yet still strike the right
balance between ﬂexibility and search space size.

3. We demonstrate new state-of-the-art accuracy on both
ImageNet classiﬁcation and COCO object detection
under typical mobile latency constraints.

2. Related Work

Improving the resource efﬁciency of CNN models has
been an active research topic during the last several years.
Some commonly-used approaches include 1) quantizing the
weights and/or activations of a baseline CNN model into
lower-bit representations [8, 16], or 2) pruning less impor-
tant ﬁlters according to FLOPs [6, 10], or to platform-aware
metrics such as latency introduced in [32]. However, these
methods are tied to a baseline model and do not focus on
learning novel compositions of CNN operations.

Another common approach is to directly hand-craft more
efﬁcient mobile architectures: SqueezeNet [15] reduces the
number of parameters and computation by using lower-
cost 1x1 convolutions and reducing ﬁlter sizes; MobileNet
[11] extensively employs depthwise separable convolution
to minimize computation density; ShufﬂeNets [33, 24] uti-
lize low-cost group convolution and channel shufﬂe; Con-
densenet [14] learns to connect group convolutions across
layers; Recently, MobileNetV2 [29] achieved state-of-the-
art results among mobile-size models by using resource-
efﬁcient inverted residuals and linear bottlenecks. Unfortu-
nately, given the potentially huge design space, these hand-
crafted models usually take signiﬁcant human efforts.

Recently, there has been growing interest in automating
the model design process using neural architecture search.
These approaches are mainly based on reinforcement learn-
ing [35, 36, 1, 19, 25], evolutionary search [26], differen-
tiable search [21], or other learning algorithms [19, 17, 23].
Although these methods can generate mobile-size models
by repeatedly stacking a few searched cells, they do not in-
corporate mobile platform constraints into the search pro-
cess or search space. Closely related to our work is MONAS
[12], DPP-Net [3], RNAS [34] and Pareto-NASH [4] which
attempt to optimize multiple objectives, such as model size
and accuracy, while searching for CNNs, but their search
process optimizes on small tasks like CIFAR. In contrast,
this paper targets real-world mobile latency constraints and
focuses on larger tasks like ImageNet classiﬁcation and
COCO object detection.

3. Problem Formulation

We formulate the design problem as a multi-objective
search, aiming at ﬁnding CNN models with both high-
accuracy and low inference latency. Unlike previous ar-
chitecture search approaches that often optimize for indi-
rect metrics, such as FLOPS, we consider direct real-world
inference latency, by running CNN models on real mobile
devices, and then incorporating the real-world inference la-
tency into our objective. Doing so directly measures what
is achievable in practice: our early experiments show it is
challenging to approximate real-world latency due to the
variety of mobile hardware/software idiosyncrasies.

2821

where α and β are application-speciﬁc constants. An empir-
ical rule for picking α and β is to ensure Pareto-optimal so-
lutions have similar reward under different accuracy-latency
trade-offs. For instance, we empirically observed doubling
the latency usually brings about 5% relative accuracy gain.
Given two models: (1) M1 has latency l and accuracy a; (2)
M2 has latency 2l and 5% higher accuracy a · (1 + 5%),
they should have similar reward: Reward(M 2) = a · (1 +
5%) · (2l/T )β ≈ Reward(M 1) = a · (l/T )β. Solving this
gives β ≈ −0.07. Therefore, we use α = β = −0.07 in
our experiments unless explicitly stated.

Figure 3 shows the objective function with two typical
values of (α, β). In the top ﬁgure with (α = 0, β = −1),
we simply use accuracy as the objective value if measured
latency is less than the target latency T ; otherwise, we
sharply penalize the objective value to discourage mod-
els from violating latency constraints. The bottom ﬁgure
(α = β = −0.07) treats the target latency T as a soft con-
straint, and smoothly adjusts the objective value based on
the measured latency.

4. Mobile Neural Architecture Search

In this section, we will ﬁrst discuss our proposed novel
factorized hierarchical search space, and then summarize
our reinforcement-learning based search algorithm.

4.1. Factorized Hierarchical Search Space

As shown in recent studies [36, 20], a well-deﬁned
search space is extremely important for neural architecture
search. However, most previous approaches [35, 19, 26]
only search for a few complex cells and then repeatedly
stack the same cells. These approaches don’t permit layer
diversity, which we show is critical for achieving both high
accuracy and lower latency.

In contrast to previous approaches, we introduce a novel
factorized hierarchical search space that factorizes a CNN
model into unique blocks and then searches for the oper-
ations and connections per block separately, thus allowing
different layer architectures in different blocks. Our intu-
ition is that we need to search for the best operations based
on the input and output shapes to obtain better accurate-
latency trade-offs. For example, earlier stages of CNNs
usually process larger amounts of data and thus have much
higher impact on inference latency than later stages. For-
mally, consider a widely-used depthwise separable convo-
lution [11] kernel denoted as the four-tuple (K, K, M, N )
that transforms an input of size (H, W, M )2 to an output of
size (H, W, N ), where (H, W ) is the input resolution and
M, N are the input/output ﬁlter sizes. The total number of
multiply-adds can be described as:

Figure 3: Objective Function Deﬁned by Equation
2, assuming accuracy ACC(m)=0.5 and target latency
T =80ms: (top) show the object values with latency as a
hard constraint; (bottom) shows the objective values with
latency as a soft constraint.

Given a model m, let ACC(m) denote its accuracy on
the target task, LAT (m) denotes the inference latency on
the target mobile platform, and T is the target latency. A
common method is to treat T as a hard constraint and max-
imize accuracy under this constraint:

maximize

m

ACC(m)

subject to LAT (m) ≤ T

(1)

However, this approach only maximizes a single metric and
does not provide multiple Pareto optimal solutions. Infor-
mally, a model is called Pareto optimal [2] if either it has
the highest accuracy without increasing latency or it has the
lowest latency without decreasing accuracy. Given the com-
putational cost of performing architecture search, we are
more interested in ﬁnding multiple Pareto-optimal solutions
in a single architecture search.

While there are many methods in the literature [2], we
use a customized weighted product method1 to approximate
Pareto optimal solutions, with optimization goal deﬁned as:

maximize

m

ACC(m) ×(cid:20) LAT (m)

T

(cid:21)w

where w is the weight factor deﬁned as:

w =(α,

β,

if LAT (m) ≤ T

otherwise

(2)

(3)

1We pick the weighted product method because it is easy to customize,

but we expect methods like weighted sum should be also ﬁne.

2We omit batch size dimension for simplicity.

2822

204060801001201401600.20.40.6Objectiveα=0β=−1Acc(m)=0.5,T=8020406080100120140160ModelLatency(ms)0.20.40.6Objectiveα=−0.07β=−0.07Acc(m)=0.5,T=80Figure 4: Factorized Hierarchical Search Space. Network layers are grouped into a number of predeﬁned skeletons, called
blocks, based on their input resolutions and ﬁlter sizes. Each block contains a variable number of repeated identical layers
where only the ﬁrst layer has stride 2 if input/output resolutions are different but all other layers have stride 1. For each block,
we search for the operations and connections for a single layer and the number of layers N , then the same layer is repeated
N times (e.g., Layer 4-1 to 4-N4 are the same). Layers from different blocks (e.g., Layer 2-1 and 4-1) can be different.

H ∗ W ∗ M ∗ (K ∗ K + N )

(4)

Here we need to carefully balance the kernel size K and
ﬁlter size N if the total computation is constrained. For in-
stance, increasing the receptive ﬁeld with larger kernel size
K of a layer must be balanced with reducing either the ﬁlter
size N at the same layer, or compute from other layers.

Figure 4 shows the baseline structure of our search space.
We partition a CNN model into a sequence of pre-deﬁned
blocks, gradually reducing input resolutions and increasing
ﬁlter sizes as is common in many CNN models. Each block
has a list of identical layers, whose operations and con-
nections are determined by a per-block sub search space.
Speciﬁcally, a sub search space for a block i consists of the
following choices:

• Convolutional ops ConvOp: regular conv (conv), depthwise

conv (dconv), and mobile inverted bottleneck conv [29].

• Convolutional kernel size KernelSize: 3x3, 5x5.

• Squeeze-and-excitation [13] ratio SERatio: 0, 0.25.

• Skip ops SkipOp: pooling, identity residual, or no skip.

• Output ﬁlter size Fi.

• Number of layers per block Ni.

ConvOp, KernelSize, SERatio, SkipOp, Fi determines
the architecture of a layer, while Ni determines how many
times the layer will be repeated for the block. For exam-
ple, each layer of block 4 in Figure 4 has an inverted bot-
tleneck 5x5 convolution and an identity residual skip path,
and the same layer is repeated N4 times. We discretize all
search options using MobileNetV2 as a reference: For #lay-
ers in each block, we search for {0, +1, -1} based on Mo-
bileNetV2; for ﬁlter size per layer, we search for its relative
size in {0.75, 1.0, 1.25} to MobileNetV2 [29].

Our factorized hierarchical search space has a distinct
advantage of balancing the diversity of layers and the size
of total search space. Suppose we partition the network into
B blocks, and each block has a sub search space of size S
with average N layers per block, then our total search space
size would be SB, versing the ﬂat per-layer search space
with size SB∗N . A typical case is S = 432, B = 5, N = 3,
where our search space size is about 1013, versing the per-
layer approach with search space size 1039.

4.2. Search Algorithm

Inspired by recent work [35, 36, 25, 20], we use a re-
inforcement learning approach to ﬁnd Pareto optimal solu-
tions for our multi-objective search problem. We choose
reinforcement learning because it is convenient and the re-
ward is easy to customize, but we expect other methods like
evolution [26] should also work.

Concretely, we follow the same idea as [36] and map
each CNN model in the search space to a list of tokens.
These tokens are determined by a sequence of actions a1:T
from the reinforcement learning agent based on its parame-
ters θ. Our goal is to maximize the expected reward:

J = EP (a1:T ;θ)[R(m)]

(5)

where m is a sampled model determined by action a1:T , and
R(m) is the objective value deﬁned by equation 2.

As shown in Figure 1, the search framework consists of
three components: a recurrent neural network (RNN) based
controller, a trainer to obtain the model accuracy, and a
mobile phone based inference engine for measuring the la-
tency. We follow the well known sample-eval-update loop
to train the controller. At each step, the controller ﬁrst sam-
ples a batch of models using its current parameters θ, by

2823

Block1Block 2Block3Block4Block5Block6Block7Layer2-1InputimageoutputLayer2-N2Layer4-1Layer4-N4conv1x1dconv5x5conv1x1+......conv3x3Blocks are predefined Skeletons. Search Space Per Block i:●ConvOp: dconv, conv, ...●KernelSize: 3x3, 5x5●SERatio: 0, 0.25, ...●SkipOp: identity, pool, ...●FilterSize: Fi●#Layers: NiContents in blue are searchedF2 F4 SE0.25Model

Type

#Params

#Mult-Adds Top-1 Acc. (%) Top-5 Acc. (%)

Inference Latency

manual
MobileNetV1 [11]
SqueezeNext [5]
manual
ShufﬂeNet (1.5x) [33]
manual
manual
ShufﬂeNet (2x)
ShufﬂeNetV2 (1.5x) [24]
manual
manual
ShufﬂeNetV2 (2x)
CondenseNet (G=C=4) [14] manual
manual
CondenseNet (G=C=8)
manual
MobileNetV2 [29]
MobileNetV2 (1.4x)
manual

NASNet-A [36]
AmoebaNet-A [26]
PNASNet [19]
DARTS [21]

MnasNet-A1
MnasNet-A2
MnasNet-A3

auto
auto
auto
auto

auto
auto
auto

4.2M
3.2M
3.4M
5.4M

-
-

2.9M
4.8M
3.4M
6.9M

5.3M
5.1M
5.1M
4.9M

3.9M
4.8M
5.2M

575M
708M
292M
524M
299M
597M
274M
529M
300M
585M

564M
555M
588M
595M

312M
340M
403M

70.6
67.5
71.5
73.7
72.6
75.4
71.0
73.8
72.0
74.7

74.0
74.5
74.2
73.1

75.2
75.6
76.7

89.5
88.2

-
-
-
-

90.0
91.7
91.0
92.5

91.3
92.0
91.9
91

92.5
92.7
93.3

113ms

-
-
-
-
-
-
-

75ms
143ms

183ms
190ms

-
-

78ms
84ms
103ms

Table 1: Performance Results on ImageNet Classiﬁcation [28]. We compare our MnasNet models with both manually-
designed mobile models and other automated approaches – MnasNet-A1 is our baseline model;MnasNet-A2 and MnasNet-A3
are two models (for comparison) with different latency from the same architecture search experiment; #Params: number of
trainable parameters; #Mult-Adds: number of multiply-add operations per image; Top-1/5 Acc.: the top-1 or top-5 accuracy
on ImageNet validation set; Inference Latency is measured on the big CPU core of a Pixel 1 Phone with batch size 1.

predicting a sequence of tokens based on the softmax logits
from its RNN. For each sampled model m, we train it on the
target task to get its accuracy ACC(m), and run it on real
phones to get its inference latency LAT (m). We then cal-
culate the reward value R(m) using equation 2. At the end
of each step, the parameters θ of the controller are updated
by maximizing the expected reward deﬁned by equation 5
using Proximal Policy Optimization [30]. The sample-eval-
update loop is repeated until it reaches the maximum num-
ber of steps or the parameters θ converge.

5. Experimental Setup

Directly searching for CNN models on large tasks like
ImageNet or COCO is expensive, as each model takes
days to converge. While previous approaches mainly per-
form architecture search on smaller tasks such as CIFAR-
10 [36, 26], we ﬁnd those small proxy tasks don’t work
when model latency is taken into account, because one typ-
ically needs to scale up the model when applying to larger
problems. In this paper, we directly perform our architec-
ture search on the ImageNet training set but with fewer
training steps (5 epochs). As a common practice, we re-
serve randomly selected 50K images from the training set
as the ﬁxed validation set. To ensure the accuracy improve-
ments are from our search space, we use the same RNN
controller as NASNet [36] even though it is not efﬁcient:

each architecture search takes 4.5 days on 64 TPUv2 de-
vices. During training, we measure the real-world latency
of each sampled model by running it on the single-thread
big CPU core of Pixel 1 phones.
In total, our controller
samples about 8K models during architecture search, but
only 15 top-performing models are transferred to the full
ImageNet and only 1 model is transferred to COCO.

For full ImageNet training, we use RMSProp optimizer
with decay 0.9 and momentum 0.9. Batch norm is added
after every convolution layer with momentum 0.99, and
weight decay is 1e-5. Dropout rate 0.2 is applied to the last
layer. Following [7], learning rate is increased from 0 to
0.256 in the ﬁrst 5 epochs, and then decayed by 0.97 every
2.4 epochs. We use batch size 4K and Inception preprocess-
ing with image size 224×224. For COCO training, we plug
our learned model into SSD detector [22] and use the same
settings as [29], including input size 320 × 320.

6. Results

In this section, we study the performance of our models
on ImageNet classiﬁcation and COCO object detection, and
compare them with other state-of-the-art mobile models.

6.1. ImageNet Classiﬁcation Performance

Table 1 shows the performance of our models on Ima-
geNet [28]. We set our target latency as T = 75ms, similar

2824

(a) Depth multiplier = 0.35, 0.5, 0.75, 1.0, 1.4, corresponding
to points from left to right.

(b) Input size = 96, 128, 160, 192, 224, corresponding to points
from left to right.

Figure 5: Performance Comparison with Different Model Scaling Techniques. MnasNet is our baseline model shown in
Table 1. We scale it with the same depth multipliers and input sizes as MobileNetV2.

Inference Latency Top-1 Acc.

w/o SE

w/ SE

MobileNetV2

NASNet

MnasNet-B1

MnasNet-A1
MnasNet-A2

75ms
183ms

77ms

78ms
84ms

72.0%
74.0%

74.5%

75.2%
75.6%

Table 2: Performance Study for Squeeze-and-Excitation
SE [13] – MnasNet-A denote the default MnasNet with SE
in search space; MnasNet-B denote MnasNet with no SE in
search space.

to MobileNetV2 [29], and use Equation 2 with α=β=-0.07
as our reward function during architecture search. After-
wards, we pick three top-performing MnasNet models, with
different latency-accuracy trade-offs from the same search
experiment and compare them with existing mobile models.
As shown in the table, our MnasNet A1 model achieves
75.2% top-1 / 92.5% top-5 accuracy with 78ms latency and
3.9M parameters / 312M multiply-adds, achieving a new
state-of-the-art accuracy for this typical mobile latency con-
straint. In particular, MnasNet runs 1.8× faster than Mo-
bileNetV2 (1.4) [29] on the same Pixel phone with 0.5%
higher accuracy. Compared with automatically searched
CNN models, our MnasNet runs 2.3× faster than the
mobile-size NASNet-A [36] with 1.2% higher top-1 ac-
curacy. Notably, our slightly larger MnasNet-A3 model
achieves better accuracy than ResNet-50 [9], but with 4.8×
fewer parameters and 10× fewer multiply-add cost.

Given that squeeze-and-excitation (SE [13]) is relatively
new and many existing mobile models don’t have this extra

optimization, we also show the search results without SE in
the search space in Table 2; our automated approach still
signiﬁcantly outperforms both MobileNetV2 and NASNet.

6.2. Model Scaling Performance

Given the myriad application requirements and device
heterogeneity present in the real world, developers often
scale a model up or down to trade accuracy for latency or
model size. One common scaling technique is to modify
the ﬁlter size using a depth multiplier [11]. For example,
a depth multiplier of 0.5 halves the number of channels in
each layer, thus reducing the latency and model size. An-
other common scaling technique is to reduce the input im-
age size without changing the network.

Figure 5 compares the model scaling performance of
MnasNet and MobileNetV2 by varying the depth multipli-
ers and input image sizes. As we change the depth mul-
tiplier from 0.35 to 1.4, the inference latency also varies
from 20ms to 160ms. As shown in Figure 5a, our Mnas-
Net model consistently achieves better accuracy than Mo-
bileNetV2 for each depth multiplier. Similarly, our model
is also robust to input size changes and consistently outper-
forms MobileNetV2 (increaseing accuracy by up to 4.1%)
across all input image sizes from 96 to 224, as shown in
Figure 5b.

In addition to model scaling, our approach also allows
searching for a new architecture for any latency target. For
example, some video applications may require latency as
low as 25ms. We can either scale down a baseline model, or
search for new models speciﬁcally targeted to this latency
constraint. Table 4 compares these two approaches. For fair
comparison, we use the same 224x224 image sizes for all

2825

020406080100120140160InferenceLatency(ms)60.062.565.067.570.072.575.077.5ImagenetTop1Accuracy(%)64.1%68.9%73.3%75.2%77.2%74.7%72%69.8%65.4%60.3%MnasNet-A1MobileNetV21020304050607080InferenceLatency(ms)58606264666870727476ImagenetTop1Accuracy(%)64.4%69.1%72.0%74.0%75.2%72%70.7%68.8%65.3%60.3%MnasNet-A1MobileNetV2Network

#Params

#Mult-Adds mAP mAPS mAPM mAPL

Inference Latency

YOLOv2 [27]
SSD300 [22]
SSD512 [22]

MobileNetV1 + SSDLite [11]
MobileNetV2 + SSDLite [29]

MnasNet-A1 + SSDLite

50.7M
36.1M
36.1M
5.1M
4.3M

4.9M

17.5B
35.2B
99.5B
1.3B
0.8B

0.8B

21.6
23.2
26.8
22.2
22.1

23.0

5.0
5.3
9.0

-
-

3.8

22.4
23.2
28.9

-
-

35.5
39.6
41.9

-
-

21.7

42.0

-
-
-

270ms
200ms

203ms

Table 3: Performance Results on COCO Object Detection – #Params: number of trainable parameters; #Mult-Adds:
number of multiply-additions per image; mAP : standard mean average precision on test-dev2017; mAPS, mAPM , mAPL:
mean average precision on small, medium, large objects; Inference Latency: the inference latency on Pixel 1 Phone.

Params MAdds Latency Top1 Acc.

MobileNetV2 (0.35x)
MnasNet-A1 (0.35x)

MnasNet-search1
MnasNet-search2

1.66M
1.7M
1.9M
2.0M

59M
63M
65M
68M

21.4ms
22.8ms
22.0ms
23.2ms

60.3%
64.1%
64.9%
66.0%

Table 4: Model Scaling vs. Model Search – MobileNetV2
(0.35x) and MnasNet-A1 (0.35x) denote scaling the base-
line models with depth multiplier 0.35; MnasNet-search1/2
denotes models from a new architecture search that targets
22ms latency constraint.

models. Although our MnasNet already outperforms Mo-
bileNetV2 with the same scaling parameters, we can further
improve the accuracy with a new architecture search target-
ing a 22ms latency constraint.

6.3. COCO Object Detection Performance

For COCO object detection [18], we pick the MnasNet
models in Table 2 and use them as the feature extractor for
SSDLite, a modiﬁed resource-efﬁcient version of SSD [29].
Similar to [29], we compare our models with other mobile-
size SSD or YOLO models.

Table 3 shows the performance of our MnasNet mod-
els on COCO. Results for YOLO and SSD are from [27],
while results for MobileNets are from [29]. We train our
models on COCO trainval35k and evaluate them on test-
dev2017 by submitting the results to COCO server. As
shown in the table, our approach signiﬁcantly improve the
accuracy over MobileNet V1 and V2. Compare to the stan-
dard SSD300 detector [22], our MnasNet model achieves
comparable mAP quality (23.0 vs 23.2) as SSD300 with
7.4× fewer parameters and 42× fewer multiply-adds.

7. Ablation Study and Discussion

In this section, we study the impact of latency constraint
and search space, and discuss MnasNet architecture details
and the importance of layer diversity.

(a) α = 0, β = −1

(b) α = β = −0.07

Figure 6: Multi-Objective Search Results based on equa-
tion 2 with (a) α=0, β=-1; and (b) α=β=−0.07. Target la-
tency is T =75ms. Top ﬁgure shows the Pareto curve (blue
line) for the 3000 sampled models (green dots); bottom ﬁg-
ure shows the histogram of model latency.

7.1. Soft vs. Hard Latency Constraint

Our multi-objective search method allows us to deal with
both hard and soft latency constraints by setting α and β to
different values in the reward equation 2. Figure 6 shows
the multi-objective search results for typical α and β. When
α = 0, β = −1, the latency is treated as a hard constraint,
so the controller tends to focus more on faster models to
avoid the latency penalty. On the other hand, by setting
α = β = −0.07, the controller treats the target latency as a
soft constraint and tries to search for models across a wider
latency range.
It samples more models around the target
latency value at 75ms, but also explores models with latency
smaller than 40ms or greater than 110ms. This allows us
to pick multiple models from the Pareto curve in a single
architecture search as shown in Table 1.

2826

507510030405060ProxyTaskAccuracy(%)5075100InferenceLatency(ms)0100200300400HistogramCount507510030405060ProxyTaskAccuracy(%)5075100InferenceLatency(ms)0100200300400HistogramCount7.2. Disentangling Search Space and Reward

To disentangle the impact of our two key contributions:
multi-objective reward and new search space, Figure 5 com-
pares their performance. Starting from NASNet [36], we
ﬁrst employ the same cell-base search space [36] and sim-
ply add the latency constraint using our proposed multiple-
object reward. Results show it generates a much faster
model by trading the accuracy to latency. Then, we ap-
ply both our multi-objective reward and our new factorized
search space, and achieve both higher accuracy and lower
latency, suggesting the effectiveness of our search space.

Reward

Search Space

Latency Top-1 Acc.

Single-obj [36] Cell-based [36]
Multi-obj
Cell-based [36]
Multi-obj
MnasNet

183ms
100ms
78ms

74.0%
72.0%
75.2%

Table 5: Comparison of Decoupled Search Space and
Reward Design – Multi-obj denotes our multi-objective
reward; Single-obj denotes only optimizing accuracy.

7.3. MnasNet Architecture and Layer Diversity

Figure 7(a) illustrates our MnasNet-A1 model found by
our automated approach. As expected, it consists of a vari-
ety of layer architectures throughout the network. One in-
teresting observation is that our MnasNet uses both 3x3 and
5x5 convolutions, which is different from previous mobile
models that all only use 3x3 convolutions.

In order to study the impact of layer diversity, Table
6 compares MnasNet with its variants that only repeat a
single type of layer (ﬁxed kernel size and expansion ra-
tio). Our MnasNet model has much better accuracy-latency
trade-offs than those variants, highlighting the importance
of layer diversity in resource-constrained CNN models.

8. Conclusion

This paper presents an automated neural architecture
search approach for designing resource-efﬁcient mobile
CNN models using reinforcement learning. Our main ideas
are incorporating platform-aware real-world latency infor-
mation into the search process and utilizing a novel factor-
ized hierarchical search space to search for mobile models
with the best trade-offs between accuracy and latency. We
demonstrate that our approach can automatically ﬁnd sig-
niﬁcantly better mobile models than existing approaches,
and achieve new state-of-the-art results on both ImageNet
classiﬁcation and COCO object detection under typical mo-
bile inference latency constraints. The resulting MnasNet
architecture also provides interesting ﬁndings on the impor-
tance of layer diversity, which will guide us in designing
and improving future mobile CNN models.

Figure 7: MnasNet-A1 Architecture – (a) is a representa-
tive model selected from Table 1; (b) - (d) are a few cor-
responding layer structures. MBConv denotes mobile in-
verted bottleneck conv, DWConv denotes depthwise conv,
k3x3/k5x5 denotes kernel size, BN is batch norm, HxWxF
denotes tensor shape (height, width, depth), and ×1/2/3/4
denotes the number of repeated layers within the block.

Top-1 Acc.

Inference Latency

MnasNet-A1

MBConv3 (k3x3) only
MBConv3 (k5x5) only
MBConv6 (k3x3) only
MBConv6 (k5x5) only

75.2%

71.8%
72.5%
74.9%
75.6%

78ms

63ms
79ms
116ms
146ms

Table 6: Performance Comparison of MnasNet and Its
Variants – MnasNet-A1 denotes the model shown in Figure
7(a); others are variants that repeat a single type of layer
throughout the network. All models have the same number
of layers and same ﬁlter size at each layer.

9. Acknowledgments

We thank Barret Zoph, Dmitry Kalenichenko, Guiheng
Zhou, Hongkun Yu, Jeff Dean, Megan Kacholia, Menglong
Zhu, Nan Zhang, Shane Almeida, Sheng Li, Vishy Tiru-
malashetty, Wen Wang, Xiaoqiang Zheng, and the larger
device automation platform team, TensorFlow Lite, and
Google Brain team.

2827

SepConv (k3x3)MBConv6 (k3x3)MBConv3 (k5x5), SEMBConv6 (k3x3)MBConv6 (k3x3), SEMBConv6 (k5x5), SEMBConv6 (k3x3)imagesPooling, FClogits224x224x3112x112x1656x56x2414x14x80x1x2x3x4x2x3x128x28x4014x14x1127x7x1607x7x320DWConv5x5, BN, ReluConv1x1, BN, ReluConv1x1, BN+HxWxFHxWx3FHxWx3FHxWxFDWConv3x3, BN, ReluConv1x1, BNHxWxFHxWxFHxWxF(a) MnasNet-A1(b) MBConv3 (k5x5)(d) SepConv (k3x3)Conv3x3112x112x32DWConv3x3, BN, ReluConv1x1, BN, ReluConv1x1, BN+HxWxFHxWx6FHxWx6FHxWxF(c) MBConv6 (k3x3)SE (Pooling, FC, Relu, FC, SIgmoid, MUL)HxWx3FReferences

[1] B. Baker, O. Gupta, N. Naik, and R. Raskar. Designing
neural network architectures using reinforcement learning.
ICLR, 2017.

[2] K. Deb. Multi-objective optimization. Search methodolo-

gies, pages 403–449, 2014.

[3] J.-D. Dong, A.-C. Cheng, D.-C. Juan, W. Wei, and M. Sun.
DPP-Net: Device-aware progressive search for pareto-
optimal neural architectures. ECCV, 2018.

[4] T. Elsken, J. H. Metzen, and F. Hutter. Multi-objective archi-
tecture search for cnns. arXiv preprint arXiv:1804.09081,
2018.

[5] A. Gholami, K. Kwon, B. Wu, Z. Tai, X. Yue, P. Jin, S. Zhao,
and K. Keutzer. Squeezenext: Hardware-aware neural net-
work design. ECV Workshop at CVPR, 2018.

[6] A. Gordon, E. Eban, O. Nachum, B. Chen, H. Wu, T.-J.
Yang, and E. Choi. Morphnet: Fast & simple resource-
constrained structure learning of deep networks. CVPR,
2018.

[7] P. Goyal,

P. Doll´ar, R. Girshick,

P. Noordhuis,
L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He.
Accurate, large minibatch sgd: training imagenet in 1 hour.
arXiv preprint arXiv:1706.02677, 2017.

[8] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. ICLR, 2016.

[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. CVPR, pages 770–778, 2016.

[10] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. Amc:
Automl for model compression and acceleration on mobile
devices. ECCV, 2018.

[11] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. arXiv preprint arXiv:1704.04861, 2017.

[12] C.-H. Hsu, S.-H. Chang, D.-C. Juan, J.-Y. Pan, Y.-T. Chen,
W. Wei, and S.-C. Chang. MONAS: Multi-objective neu-
ral architecture search using reinforcement learning. arXiv
preprint arXiv:1806.10332, 2018.

[13] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-

works. CVPR, 2018.

[14] G. Huang, S. Liu, L. van der Maaten, and K. Q. Weinberger.
Condensenet: An efﬁcient densenet using learned group con-
volutions. CVPR, 2018.

[15] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and ¡0.5 mb model size. arXiv
preprint arXiv:1602.07360, 2016.

[16] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,
H. Adam, and D. Kalenichenko. Quantization and training
of neural networks for efﬁcient integer-arithmetic-only infer-
ence. CVPR, 2018.

[17] K. Kandasamy, W. Neiswanger, J. Schneider, B. Poczos, and
E. Xing. Neural architecture search with bayesian optimisa-
tion and optimal transport. NeurIPS, 2018.

[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon objects in context. ECCV, 2014.

[19] C. Liu, B. Zoph, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei,
A. Yuille, J. Huang, and K. Murphy. Progressive neural ar-
chitecture search. ECCV, 2018.

[20] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and
K. Kavukcuoglu. Hierarchical representations for efﬁcient
architecture search. ICLR, 2018.

[21] H. Liu, K. Simonyan, and Y. Yang. DARTS: Differentiable

architecture search. ICLR, 2019.

[22] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-
Y. Fu, and A. C. Berg. SSD: Single shot multibox detector.
ECCV, 2016.

[23] R. Luo, F. Tian, T. Qin, and T.-Y. Liu. Neural architecture

optimization. NeurIPS, 2018.

[24] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun. Shufﬂenet
v2: Practical guidelines for efﬁcient cnn architecture design.
ECCV, 2018.

[25] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean.
Efﬁcient neural architecture search via parameter sharing.
ICML, 2018.

[26] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le. Regular-
ized evolution for image classiﬁer architecture search. AAAI,
2019.

[27] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

CVPR, 2017.

[28] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015.

[29] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
Chen. Mobilenetv2: Inverted residuals and linear bottle-
necks. CVPR, 2018.

[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
O. Klimov. Proximal policy optimization algorithms. arXiv
preprint arXiv:1707.06347, 2017.

[31] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. AAAI, 4:12, 2017.

[32] T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, V. Sze,
and H. Adam. Netadapt: Platform-aware neural network
adaptation for mobile applications. ECCV, 2018.

[33] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An
extremely efﬁcient convolutional neural network for mobile
devices. CVPR, 2018.

[34] Y. Zhou, S. Ebrahimi, S. ¨O. Arık, H. Yu, H. Liu, and G. Di-
amos. Resource-efﬁcient neural architect. arXiv preprint
arXiv:1806.07912, 2018.

[35] B. Zoph and Q. V. Le. Neural architecture search with rein-

forcement learning. ICLR, 2017.

[36] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learn-
ing transferable architectures for scalable image recognition.
CVPR, 2018.

2828

