Domain-Speciﬁc Batch Normalization for Unsupervised Domain Adaptation

Woong-Gi Chang∗ 1

2

,

Tackgeun You∗ 1

2

,

Seonguk Seo∗∗ 1

Suha Kwak2 Bohyung Han1

1Computer Vision Lab., ECE & ASRI, Seoul National University, Korea

2Computer Vision Lab., CSE, POSTECH, Korea

Abstract

We propose a novel unsupervised domain adaptation
framework based on domain-speciﬁc batch normalization
in deep neural networks. We aim to adapt to both do-
mains by specializing batch normalization layers in con-
volutional neural networks while allowing them to share
all other model parameters, which is realized by a two-
stage algorithm.
In the ﬁrst stage, we estimate pseudo-
labels for the examples in the target domain using an ex-
ternal unsupervised domain adaptation algorithm—for ex-
ample, MSTN [27] or CPUA [14]—integrating the pro-
posed domain-speciﬁc batch normalization. The second
stage learns the ﬁnal models using a multi-task classiﬁca-
tion loss for the source and target domains. Note that the
two domains have separate batch normalization layers in
both stages. Our framework can be easily incorporated into
the domain adaptation techniques based on deep neural net-
works with batch normalization layers. We also present that
our approach can be extended to the problem with multiple
source domains. The proposed algorithm is evaluated on
multiple benchmark datasets and achieves the state-of-the-
art accuracy in the standard setting and the multi-source
domain adaption scenario.

1. Introduction

Unsupervised domain adaptation is a learning frame-
work to transfer knowledge learned from source domains
with a large number of annotated training examples to target
domains with unlabeled data only. This task is challenging
due to domain shift problem, which is the phenomenon that
source and target datasets have different characteristics. The
domain shift is common in real-world problems and should
be handled carefully for broad applications of trained mod-
els. Unsupervised domain adaptation aims to learn robust
models for handling the issue, and is getting popular these
days since it can be a rescue for visual recognition tasks
relying on the datasets with limited diversity and variety.

∗indicates equal contribution.

Recent advances in unsupervised domain adaptation owe
to its success of deep neural networks. Traditional domain
adaptation techniques based on shallow learning are refor-
mulated using deep neural networks with proper loss func-
tions. Strong representation power of deep networks redis-
covers effectiveness of the previous methods and facilitates
development of brand-new algorithms. There is a large vol-
ume of research on unsupervised domain adaptation based
on deep neural networks [3, 4, 10, 14, 23, 27, 30], and in
recent years we have witnessed signiﬁcant performance im-
provement.

One of the drawbacks in many existing unsupervised do-
main adaptation techniques [3, 4, 14, 23, 27] is the fact
that source and target domains share the whole network for
training and prediction. Shared components between both
domains are inevitable because there is something common
in the two domains; we often need to rely on information
in the source domain to learn the networks adapting to un-
labeled target domain data. However, we believe that bet-
ter generalization performance can be achieved by separat-
ing domain-speciﬁc information from domain-invariant one
since the two domains obviously have different characteris-
tics that are not compatible within a single model.

To separate domain-speciﬁc information for unsuper-
vised domain adaptation, we propose a novel building block
for deep neural networks, referred to as Domain-Speciﬁc
Batch Normalization (DSBN). A DSBN layer consists of
two branches of Batch Normalization (BN), each of which
is in charge of a single domain exclusively. DSBN cap-
tures domain-speciﬁc information using BN parameters and
transforms domain-speciﬁc data into domain-invariant rep-
resentations using the parameters. Since this idea is generic,
DSBN is universally applicable to various deep neural net-
works for unsupervised domain adaptation that have BN
layers. Furthermore, it can be easily extended for multi-
source domain adaptation scenarios.

Based on the main idea, we introduce a two-stage frame-
work based on DSBN for unsupervised domain adaptation,
where our network ﬁrst generates pseudo-labels of unla-
beled data in the target domain and then learns a fully super-
vised model using the pseudo-labels. Speciﬁcally, the ﬁrst

17354

stage estimates the initial pseudo-labels of target domain
data through an existing unsupervised domain adaptation
network incorporating DSBN. In the second stage, a multi-
task classiﬁcation network with DSBN layers is trained with
full supervision using the data from both source and tar-
get domains, where the pseudo-labels generated at the ﬁrst
stage are assigned to the target domain data. To further im-
prove accuracy, we iterate the second stage training and re-
ﬁne the labels of the examples in the target domain.

Our main contributions are summarized as follows:

• We propose a novel unsupervised domain adaptation
framework based on DSBN, which is a generic method
applicable to various deep neural network models for
domain adaptation.

• We introduce a two-stage learning method with DSBN
comprising pseudo-label estimation followed by multi-
task classiﬁcation, which is naturally integrated into
existing unsupervised domain adaptation methods.

• Our framework provides a principled algorithm for un-
supervised domain adaptation with multiple sources
via its straightforward extension.

• We achieve the state-of-the-art performance on the
standard benchmarks including Ofﬁce-31 and VisDA-
C datasets by integrating our framework with two re-
cent domain adaptation techniques.

The rest of our paper has the following organization. We
ﬁrst review unsupervised domain adaptation approaches in
Section 2, and then discuss two recent backbone models to
integrate DSBN in Section 3. Section 4 presents the main
idea of DSBN and Section 5 describes how to integrate our
framework into the existing unsupervised domain adapta-
tion techniques. We show experimental results in Section 6,
and conclude this paper with a brief remark in Section 7.

2. Related Work

This section reviews mainstream approaches in unsuper-

vised domain adaptation that are related to our approach.

learn domain-invariant representations using domain adver-
sarial loss while Adversarial Discriminative Domain Adap-
tation (ADDA) [24] combines adversarial learning with dis-
criminative feature learning.
Joint Adaptation Network
(JAN) [12] combines adversarial learning with MMD [1] by
aligning the joint distributions of multiple domain-speciﬁc
layers across domains.

Local alignment approaches learn domain-invariant
models by aligning examples of the same classes across
source and target domains. Cycle-Consistent Adversar-
ial Domain Adaptation (CyCADA) [4] introduces a cycle-
consistency loss, which enforces the model to ensure a cor-
rect semantic mapping. Conditional Domain Adaptation
Network (CDAN) [10] conditions an adversarial adaptation
model on discriminative information conveyed in the clas-
siﬁer predictions.

2.2. Domain speciﬁc Learning

Approaches in this category learn domain-speciﬁc in-
formation together with domain-invariant one for domain
adaptation. Domain Separation Network (DSN) [2] learns
feature representations by separating domain-speciﬁc net-
work components from shared ones. Collaborative and Ad-
versarial Network (CAN) [30] proposes a new loss function
that enforces lower-level layers of the network to have high
domain-speciﬁcity and low domain-invariance while mak-
ing higher-level layers to have the opposite properties.

Batch normalization parameters have been used to model
domain-speciﬁc information in unsupervised domain adap-
tation scenarios. AdaBN [7] proposes a post-processing
method to re-estimate batch normalization statistics using
target samples. Domain alignment layers used in [15] deal
with the domain shift problem by aligning source and tar-
get distributions to a reference Gaussian distribution. Also,
[13] learns to automatically discover multiple latent source
domains, which are used to align target and source distribu-
tions. Note that our algorithm is more ﬂexible and efﬁcient
as it learns domain-speciﬁc properties only with afﬁne pa-
rameters of batch normalization and all network parameters
except them are used for domain invariant representation.

2.1. Domain invariant Learning

2.3. Learning with Pseudo Labels

Learning domain-invariant representation is critical for
success of unsupervised domain adaptation, and many ex-
isting approaches attempt to align data distributions be-
tween source and target domains either globally or locally.
Global alignment techniques include Maximum Mean
Discrepancy (MMD) [1, 28], Central Moment Discrepancy
(CMD) [29], Wassterstein distance [21], and CORrelation
ALignment (CORAL) [22], some of which are reformulated
using deep neural networks [12, 23, 28]. Also, adversar-
ial learning has been widely used for global domain align-
ment. Domain Adversarial Neural Networks (DANN) [3]

Recent algorithms often estimate pseudo labels in tar-
get domain and directly learn a domain-speciﬁc model for
target domain. To obtain the pseudo labels, Moving Se-
mantic Transfer Network (MSTN) [27] exploits semantic
matching and domain adversarial losses while Class Pre-
diction Uncertainty Alignment (CPUA) [14] employs class
scores as feature for adversarial learning. Asymmetric Tri-
training Network (ATN) [18] and Collaborative and Adver-
sarial Network (CAN) [30] estimate pseudo labels based on
the consensus of multiple models.

Our framework also exploits pseudo labels to learn

7355

domain-speciﬁc models effectively. Compared to the exist-
ing method, our framework estimates more reliable pseudo
labels since domain-speciﬁc information is captured effec-
tively by domain-speciﬁc batch normalization.

3. Preliminaries

In unsupervised domain adaptation, we are given two
datasets; XS is for the labeled source domain and XT is the
unlabeled dataset for target domain, where nS and nT de-
note the cardinalities of XS and XT , respectively. Our goal
is to classify examples in the target domain by transferring
knowledge of classiﬁcation learned from the source domain
based on full supervision. This section discusses the details
of two state-of-the-art approaches for the integration of our
domain-speciﬁc batch normalization technique.

3.1. Moving Semantic Transfer Network

MSTN [27] proposes a semantic matching loss function
to align the centroids of the same classes across domains,
based on pseudo-labels of unlabeled target domain samples.
The formal deﬁnition of the total loss function L is given by
(1)

L = Lcls(XS) + λLda(XS,XT ) + λLsm(XS,XT ).

The classiﬁcation loss Lcls(XS) is the cross-entropy loss for
source dataset, and the domain adversarial loss Lda makes
a network confused about the domain membership of an
example as discussed in [3]. The semantic matching loss
aligns the centroids of the same classes across domains.
Note that pseudo-labels should be estimated to compute the
semantic matching losses. Intuitively, the loss function in
Eq. (1) encourages two domains to have the same distribu-
tion, especially by adding adversarial and semantic match-
ing loss terms. Hence, the learned network based on the
loss function can be applied to examples in target domain.

3.2. Class Prediction Uncertainty Alignment

CPUA [14] is a strikingly simple approach that only
aligns the class probabilities across domains. CPUA ad-
dresses class imbalance issue in both domain and introduces
a class weighted loss function to exploit the class priors.

Let pS(c) = nc

tion of targets samples that have pseudo-label c. nc

S/nS be the fraction of source samples
T /nT be the frac-
T de-

that have class label c and epT (c) = nc
notes the cardinality of {x ∈ XT | ey(x) = c}, where
ey(x) = argmaxi∈C F (x)[i]. Then the class weights for

each domain is respectively given by

and

wS(x, y) =

maxy′ pS(y′)

pS(y)

wT (x) =

maxy′ epT (y′)
epT (ey(x))

.

(2)

(3)

Their total loss function can be written as

L = Lcls(XS) + λLda(XS,XT ),

(4)

where

Lcls(XS) =

Lda(XS,XT ) =

+

(x,y)∈XS

1

1

nS X
nS X
nT X

1

x∈XT

(x,y)∈XS

wS(x, y)ℓ(F (x), y),

(5)

wS(x, y)ℓ(D(F (x))), 1)

wT (x)ℓ(D(F (x))), 0).

(6)

Note that F (·) is a classiﬁcation network, ℓ(·,·) denotes the
cross-entropy loss and D(·) is a domain discriminator.
4. Domain-Speciﬁc Batch Normalization

This section brieﬂy reviews Batch Normalization (BN)
for a comparison to our DSBN, and then presents DSBN
and its extension for multi-source domain adaptation.

4.1. Batch Normalization

BN [5] is a widely used training technique in deep net-
works. A BN layer whitens activations within a mini-batch
of N examples for each channel dimension and transforms
the whitened activations using afﬁne parameters γ and β.
Denoting by x ∈ RH×W ×N activations in each channel,

BN is expressed as

BN(x[i, j, n]; γ, β) = γ · ˆx[i, j, n] + β,

where

ˆx[i, j, n] =

x[i, j, n] − µ
√σ2 + ǫ

.

(7)

(8)

The mean and variance of activations within a mini-batch,
µ and σ, are computed by

x[i, j, n]

µ = PnPi,j
σ2 = PnPi,j (x[i, j, n] − µ)2

N · H · W

,

(9)

.

(10)

N · H · W

and ǫ is a small constant to avoid divide-by-zero.

During training, BN estimates the mean and variance of
the entire activations, denoted by ¯µ and ¯σ, through exponen-
tial moving average with update factor α. Formally, given
the tth mini-batch, the mean and variance are given by

¯µt+1 = (1 − α)¯µt + αµt,

(cid:0)¯σt+1(cid:1)2

= (1 − α)(cid:0)¯σt(cid:1)2

+ α(cid:0)σt(cid:1)2

(11)

(12)

.

In the testing phase, BN uses the estimated mean and vari-
ance for whitening input activations. Note that sharing the
mean and variance for both source and target domain are
inappropriate if domain shift is signiﬁcant.

7356

Figure 1. Illustration of difference between BN and DSBN. A DSBN layer consists of two branches in the batch normalization layer—one
is for source domain (S), and the other is for target domain (T ). Each input example chooses one of the branches according to its domain.
In a domain adaptation network with DSBN layers, all parameters except those of DSBN are shared across the two domains and learn the
information common in both domains effectively, while domain-speciﬁc properties are captured efﬁciently through the domain speciﬁc BN
parameters of DSBN layers. Note that DSBN layers can be plugged in any unsupervised domain adapatation networks with BN layers.

4.2. Domain Speciﬁc Batch Normalization

DSBN is implemented by using multiple sets of BN [5]
reserved for each domain. Figure 1 illustrates the differ-
ence between BN and DSBN. Formally, DSBN allocates
domain-speciﬁc afﬁne parameters γd and βd for each do-
main label d ∈ {S, T}. Let xd ∈ RH×W ×N denote ac-
tivations at each channel belong to a domain label d, then
DSBN layer can be written as

DSBNd(xd[i, j, n]; γd, βd) = γd · ˆxd[i, j, n] + βd,

(13)

where

and

ˆxd[i, j, n] =

xd[i, j, n] − µd

pσ2

d + ǫ

,

(14)

xd[i, j, n]

µd = PnPi,j
d = PnPi,j (xd[i, j, n] − µd)2

N · H · W

σ2

,

N · H · W

(15)

.

(16)

During training, DSBN estimates the mean and variance
of activations for each domain separately by exponential
moving average with update factor α, which is given by

¯µt+1
d = (1 − α)¯µt
d + αµt
d,
= (1 − α)(cid:0)¯σt
(cid:1)2
d(cid:1)2

(cid:0)¯σt+1

+ α(cid:0)σt
d(cid:1)2

d

(17)

(18)

.

The estimated mean and variance for each domain are used
for the examples in the corresponding domain at the testing
phase of DSBN.

We expect DSBN to capture the domain-speciﬁc infor-
mation by estimating batch statistics and learning afﬁne pa-
rameters for each domain separately. We believe that DSBN
allows the network to learn domain-invariant features bet-
ter because domain-speciﬁc information within the network

can be removed effectively by exploiting the captured statis-
tics and learned parameters from the given domain.

DSBN is easy to be plugged in existing deep neural net-
works for unsupervised domain adaptation. An existing
classiﬁcation network F (·) can be converted to a domain-
speciﬁc network by replacing all BN layers with DSBN lay-
ers and training the entire network using data with domain
labels. The domain-speciﬁc network is denoted by Fd(·),
which is specialized to either the source or the target do-
main depending on the domain variable d ∈ {S, T}.
4.3. Extension to Multi Source Domain Adaptation

DSBN is easily extended for multi-source unsupervised
domain adaptation by adding more domain branches in it.
In addition, a new loss function for the multi-source domain
adaptation is simply deﬁned by the sum of losses from all
source domains as follows:

L =

|DS |X

i

1
|DS|

(cid:16)Lcls(XSi ) + Lalign(XSi ,XT )(cid:17),

(19)

where DS = {XS1 ,XS2 , ...} is a set of source domains and
Lalign can be any kind of loss for aligning source and target
domains. The remaining procedure for training is identical
to the single-source domain adaptation case.

5. Domain Adaptation with DSBN

DSBN is a generic technique for unsupervised domain
adaptation, and can be integrated into various algorithms
based on deep neural networks with batch normalization.
Our framework trains deep networks for unsupervised do-
main adaptation in two stages. In the ﬁrst stage, we train an
existing unsupervised domain adaptation network to gen-
erate initial pseudo-labels of target domain data. The sec-
ond stage learns the ﬁnal models of both domains using the
ground-truth in the source domain the pseudo-labels in the
target domain as supervision, where the pseudo-labels in the

7357

where

Lcls(XS) = X
(XT ) = X
Lpseudo

cls

(x,y)∈XS

x∈XT

ℓ(F 2

S(x), y),

ℓ(F 2

T (x), y′).

(21)

(22)

Figure 2. Overview of the second stage training. To use inter-
mediate pseudo labels for target domain samples, we employ the
network trained in the ﬁrst stage, F 1
T (x), as a pseudo labeler in the
second stage. In this stage, the network is trained with classiﬁca-
tion losses only in both domains.

target domain are reﬁned progressively during the training
procedure. The networks in both stages incorporate DSBN
layers to learn domain-invariant representations more effec-
tively and better adapt to the target domain consequently. To
further improve accuracy, we can perform additional itera-
tions of the second stage training, where the pseudo-labels
are updated using the results in the preceding iteration. The
remaining parts of this section present details of our two
stage training method with DSBN.

5.1. Stage 1: Training Initial Pseudo Labeler

Since our framework is generic and ﬂexible, any unsu-
pervised domain adaptation network can be employed to es-
timate initial pseudo-labels of target domain data as long as
it has BN layers. In this paper, we choose two state-of-the-
art models as the initial pseudo-label generator: MSTN [27]
and CPUA [14]. As described in Section 4.2, we replace
their BN layers with DSBNs so that they learn domain-
invariant representations more effectively. The networks are
then trained by their original losses and learning strategies.
A trained initial pseudo-label generator is denoted by F 1
T .

In Eq. (21) and (22), ℓ(·,·) is the cross-entropy loss and y′
denotes the pseudo-label assigned to a target domain exam-
ple x ∈ XT .
sively reﬁned by F 2

The pseudo-label y′ is initialized by F 1

T and progres-

T as follows:

y′ = argmax

c∈C

n(1 − λ)F 1

T (x)[c] + λF 2

T (x)[c]o,

(23)

T since the prediction by F 2

where F i
T (x)[c] indicates the prediction score of the class c
given by F i
T and λ is a weight factor that changes gradually
from 0 to 1 during training. This approach can be consid-
ered as a kind of self-training since F 2
T takes part in the
pseudo-label generation while trained. At an early phase of
training, we put more weights on the initial pseudo-labels
given by F 1
T may be unreliable.
The weight λ then increases gradually, and at the last phase
of training, the pseudo labels rely totally on F 2
T . We follow
[3] to suppress potentially noisy pseudo-labels; the adapta-
tion factor λ is gradually increased by λ =
1+exp(−γ·p) − 1
with γ = 10.
Target domain images are recognized more accurately by
F 2
T than F 1
T exploits reasonable initial
pseudo-labels given by F 1
T relies only
on weak information for domain alignment. It is natural to
employ F 2
T to estimate more accurate initial pseudo-labels
for the purpose of further accuracy improvement. Thus, we
conduct the second stage procedure iteratively, where the
initial pseudo-labels are updated using the prediction results
of the model in the preceding iteration. We emperically
observe that this iterative approach is effective to improve
classiﬁcation accuracy in the target domain.

T for training while F 1

T in general since F 2

2

5.2. Stage 2: Self training with Pseudo Labels

6. Experiments

In the second stage, we utilize data and their labels from
both domains to take advantage of rich domain-invariant
representations and train the ﬁnal models for both do-
mains with full supervision. The network is trained with
two classiﬁcation losses—one for the source domain with
ground-truth labels and the other for the target domain with
pseudo-labels—and the resulting networks are denoted by
F 2
d where d ∈ {S, T}. The total loss function is given by
a simple summation of two loss terms from two domains as
follows:

L = Lcls(XS) + Lpseudo

cls

(XT )

(20)

We present the empirical results to validate the proposed
framework and compare our method with the state-of-the-
art domain adaptation methods.

6.1. Experimental Settings

We discuss datasets used for training and evaluation and
present implementation details including hyperparameter
setting.

Datasets We employ three datasets for our experiment:
VisDA-C [16], Ofﬁce-31 [17] and Ofﬁce-Home [26].
VisDA-C is a large-scale benchmark dataset used for Vi-
sual Domain Adaptation Challenge 2017. It consists of two

7358

Figure 3. Sample images of each dataset. (a) VisDA-C dataset images of two domains, (b) Ofﬁce-31 dataset images of three domains, (c)
Ofﬁce-Home dataset images of four domains.

Table 1. Classiﬁcation performance (%) of multiple algorithms on VisDA-C validation dataset using a ResNet-101 backbone network. The
results clearly show that our two-stage learning framework with DSBN is effective to improve accuracy.

Method
Source only
DAN [9]
DANN [3]
MCD [20]
ADR [19]
MSTN (reproduced)

with DSBN (Stage 1)
with DSBN (Stage 1 and 2)

CPUA (reproduced)

with DSBN (Stage 1)
with DSBN (Stage 1 and 2)

aero
55.1
87.1
81.9
87.0
87.8
89.3
90.3
94.7
90.7
91.1
93.0

bicycle
53.3
63.0
77.7
60.9
79.5
49.5
78.4
86.7
53.4
77.2
83.6

bus
61.9
76.5
82.8
83.7
83.7
74.3
75.0
76.0
79.6
84.0
79.9

car
59.1
42.0
44.3
64.0
65.3
67.6
53.5
72.0
59.9
49.5
55.8

horse
80.6
90.3
81.2
88.9
92.3
90.1
90.1
95.2
87.9
90.7
94.7

knife motor
17.9
42.9
29.5
79.6
61.8
16.6
42.8
75.1
18.7
37.9
20.6

79.7
85.9
65.1
84.7
88.9
93.6
85.5
87.9
94.5
87.8
89.7

person
31.2
53.1
28.6
76.9
73.2
70.1
76.5
81.3
61.6
76.6
80.2

plant
81.0
49.7
51.9
88.6
87.8
86.5
86.7
91.1
90.4
85.6
91.1

skate
26.5
36.3
54.6
40.3
60
40.4
64.1
68.9
51.3
60.9
80.8

train
73.5
85.8
82.8
83.0
85.5
83.2
81.5
88.3
81.8
78.1
84.8

truck Avg
52.4
8.5
20.7
61.1
57.4
7.8
71.9
25.8
74.8
32.3
18.5
65.0
72.3
43.6
80.2
45.5
66.6
29.3
71.9
43.9
59.7
76.2

domains—synthetic and real—and have 152,409 synthetic
images and 55,400 real images of 12 common object classes
from MS-COCO [8] dataset. Ofﬁce-31 is a standard bench-
mark for domain adaptation, which consists of three dif-
ferent domains in 31 categories: Amazon (A) with 2,817
images, Webcam (W) with 795 images and DSLR (D) with
498 images. Ofﬁce-Home [26] has four domains: Art (Ar)
with 2,427 images, Clipart (Cl) with 4,365 images, Product
(Pr) with 4,439 images, and Real-World (Rw) with 4,357
images. Each domain contains 65 categories of common
daily objects. We adopt the fully transductive protocol in-
troduced in [3] to evaluate our framework on the datasets.

We use Adam optimizer [6] with β1 = 0.9, β2 = 0.999. We
set initial learning rate η0 = 1.0 × 10−4 and 5.0 × 10−5 for
stage 1 and 2, respectively. As suggested in [3], the learning
rate is adjusted by a formula, ηp = η0
(1+αp)β , where α = 10,
β = 0.75, and p denotes training progress linearly chang-
ing from 0 to 1. The maximum number of iterations of the
optimizer is set to 50,000.

6.2. Results

We present the experimental results on the standard
benchmark datasets based on single- and multi-source do-
main adaptation.

Implementation details Following [3, 20], as the back-
bone networks of our framework, we adopt ResNet-101
for the VisDA-C dataset and ResNet-50 for Ofﬁce-31 and
Ofﬁce-Home datasets. All the networks have BN layers and
are pre-trained on the ImageNet. To compare the sheer dif-
ference between BN and DSBN layers, we construct mini-
batches for each domain and forward them separately. The
batch size is set to 40, which is identical for all experiments.

VisDA-C Table 1 quantiﬁes performance of our approach
employing MSTN and CPUA as its initial pseudo-label gen-
erators, and compares them with state-of-the-art records on
the VisDA-C dataset. In the table, “DSBN (Stage 1)” means
that we replace BN layers with DSBNs and perform the ﬁrst
stage training, and “DSBN (Stage 1 and 2)” indicates that
we conduct both the ﬁrst and second stage training. Our
proposed method improves accuracy substantially and con-

7359

Table 2. Classiﬁcation accuracies (%) on Ofﬁce-31 dataset (ResNet-50). *The original paper reported average accuracy of 79.1% with
AlexNet.†The original paper reported average accuracy of 87.9% with ResNet-50.

Method
Source Only
DDC [25]
DAN [9]
RTN [11]
DANN [3]
JAN [12]
iCAN [30]
CDAN-M [10]
MSTN (reproduced) [27]∗
with DSBN (Stage 1)
with DSBN (Stage 1 and 2)
CPUA (reproduced) [14]†
with DSBN (Stage 1)
with DSBN (Stage 1 and 2)

A → W W → A A → D D → A W → D D → W Avg
76.5
73.5
79.5
76.0
80.5
80.4
81.6
84.5
80.9
79.3
84.6
86.0
92.5
87.2
87.7
93.1
86.5
91.3
87.8
92.5
88.3
92.7
90.1
86.4
87.5
92.3
93.3
88.3

99.0
98.2
99.6
99.4
99.6
99.7
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0

56.7
67.0
63.6
66.2
65.3
69.2
72.1
71.0
72.7
72.2
71.7
71.3
72.0
72.7

93.6
94.8
97.1
96.8
97.3
96.7
98.8
98.6
98.9
98.5
99.0
98.6
99.1
99.1

76.5
77.5
78.6
77.5
80.7
85.1
90.1
93.4
90.4
90.6
92.2
86.8
88.8
90.8

59.8
63.7
62.8
64.8
63.2
70.7
69.9
70.3
65.6
73.1
74.4
71.6
72.7
73.9

Table 3. Classiﬁcation accuracies (%) in the multi-source scenario
on Ofﬁce-31 dataset using MSTN as a baseline. (ResNet-50)

Table 4. Classiﬁcation accuracies (%) in the multi-source scenario
on Ofﬁce-Home dataset using MSTN as a baseline. (ResNet-50)

Single
BN
DSBN

Merged
BN
DSBN
Separate
BN
DSBN

A→W D→W A→D W→D W→A D→A
65.6
72.7
91.3
98.9
73.1
72.2
92.5
98.5
(W+D)→A
(A+D)→W

90.4
100.0
90.6
100.0
(A+W) →D

99.6
99.1

99.6
99.6

71.3
73.2

A, D→W

99.9
99.9

A, W → D

99.8
100.0

W, D→A

69.9
75.6

Single
BN
DSBN

Merged
BN
DSBN
Separate
BN
DSBN

Ar→Rw Cl→Rw Pr→Rw
76.9
76.4
75.4
77.7

69.3
70.4

(Ar+Cl+Pr)→Rw

81.2
82.3

Ar, Cl, Pr→Rw

81.4
83.0

sistently by applying DSBNs to the baseline models, and
achieves the state-of-the-art performance when combined
with MSTN [27]. Note also that our model reliably recog-
nizes hard classes such as knife, person, skate, and truck.

Ofﬁce-31 Table 2 presents overall scores of our approach
using MSTN and CPUA on the Ofﬁce-31 dataset. Mod-
els with DSBN trained in both stages achieve state-of-the-
art performance and consistently outperform two baseline
models. Table 2 also shows that our framework can be ap-
plied to current domain adaptation algorithm successfully
and greatly improves performance.

Multiple Source Domains Table 3 and Table 4 presents
the results of multiple source domain adaptation on the
Ofﬁce-31 and Ofﬁce-Home datasets, respectively. To com-
pare multi-source to single-source domain adaptation, we
report single-source results on the top of the table as
“Single” and append two different multi-source scenarios:
“Merged” and “Separate”. Merged means that data from
multiple source domains are combined and constructed a
new larger source domain dataset while separate indicates
that each source domain is considered separately. In the sep-
arate case, we have a total of |DS|+1 domains and the same

number of DSBN branches in the network. While there is
a marginal performance gain between BN and DSBN when
target tasks are easy, our models consistently outperform the
BN models for all settings. In particular, for the hard do-
main adaptation to task “A” on Table 3, DSBN with source
domain separation is considerably better than the merged
case. This results imply that DSBN has an advantage in
multi-source domain adaptation tasks, too. Note that the
seperate case is not always better than the merged case with-
out DSBN.

6.3. Analysis

Ablation Study We perform ablative experiments on our
framework to analyze the effect of DSBN in comparison to
BN. Table 5 summarizes the ablation results on VisDA-C
dataset using MSTN and CPUA as baseline architectures,
where the last column in the table presents the accuracy
gain by the second stage training with respect to the results
from the ﬁrst stage training only. We test several combina-
tions of different training procedures for two stage training.
The results directly show that DSBN plays a crucial role
on both the training procedures. Another important point
is that the second stage training with DSBN boosts perfor-

7360

Table 5. Ablation results for the combination of batch normalization variations on VisDA-C validation dataset. (ResNet-101), where ∆
means the accuracy gain by the second stage training with respect to the results from the ﬁrst stage training only.

Baseline Stage 1 Stage 2

MSTN

CPUA

BN

BN

DSBN

BN
BN

aero
81.2
87.4
DSBN 92.8
DSBN DSBN 94.7
88.9
89.3
DSBN 91.1
DSBN DSBN 93.0

BN
BN

DSBN

BN

BN

bicycle

48.7
67.7
70.3
86.7
43.9
53.6
66.0
83.6

bus
77.8
77.2
78.4
76.0
69.8
77.9
79.3
79.9

car
75.3
62.3
76.9
72.0
68.8
54.2
68.8
55.8

horse
90.2
92.7
93.3
95.2
90.8
91.7
92.6
94.7

knife motor
94.4
4.6
94.4
17.8
92.7
14.9
75.1
87.9
94.0
9.5
93.7
16.1
18.5
91.2
20.6
89.7

person
73.5
74.5
81.7
81.3
46.0
74.3
73.1
80.2

plant
91.1
92.5
91.9
91.1
92.1
91.6
91.3
91.1

skate
32.5
63.7
80.2
68.9
49.2
58.4
65.7
80.8

train truck Avg ∆
85.3
84.4
85.0
88.3
82.8
79.6
80.2
84.8

63.4 -1.6
71.1 -1.2
73.2 +7.2
80.2 +7.9
63.4 -2.8
69.4 -2.6
71.3 +4.7
76.2 +4.3

5.9
38.3
20.3
45.5
24.8
52.4
37.9
59.7

Table 6. Classiﬁcation accuracies (%) of Iterative learning on
VisDA-C dataset using MSTN as a baseline. (ResNet-101)

Stage 1

72.3

Stage 2

Iter 1
80.2

Iter 2
81.4

Iter 3
82.2

Iter 4
82.7

7. Conclusion

We presented the domain-speciﬁc batch normalization
for unsupervised domain adaptation. The proposed frame-
work has separate branches of batch normalization lay-
ers, one for each domain while sharing all other parame-
ters across domains. This idea is generically applicable to
deep neural networks with batch normalization layers. This
framework with two stage training strategy is applied to two
recent unsupervised domain adaptation algorithms, MSTN
and CPUA, and demonstrates outstanding performance in
both cases on the standard benchmark datasets. We also
present the capability of our framework to be extended to
multi-source domain adaptation problems, and report sig-
niﬁcantly improved results compared to other methods.

Acknowledgments This work was partly supported by
Kakao and Kakao Brain Corporation and Korean ICT R&D
program of the MSIP/IITP grant [2016-0-00563, 2017-0-
01778].

References

[1] Karsten M. Borgwardt, Arthur Gretton, Malte J. Rasch, Hans-
Peter Kriegel, Bernhard Sch¨olkopf, and Alex J. Smola. Inte-
grating Structured Biological Data by Kernel Maximum Mean
Discrepancy. Bioinformatics, 22(14):e49–e57, July 2006. 2

[2] Konstantinos Bousmalis, George Trigeorgis, Nathan Silber-
man, Dilip Krishnan, and Dumitru Erhan. Domain Separation
Networks. In NIPS, 2016. 2

[3] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal
Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario Marc-
hand, and Victor Lempitsky. Domain-Adversarial Training of
Neural Networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 3, 5,
6, 7

7361

Figure 4. t-SNE plots of the sample representations from ResNet-
101 models trained with BN (left) and DSBN (right) using MSTN
as a baseline algorithm on VisDA-C validation dataset. They il-
lustrate that DSBN improves the consistency of representations
across domains,

mance additionally by large margins while the ordinary BN
in the second stage is not helpful. It implies that separating
domain-speciﬁc information during training phase helps to
get reliable pseudo-labels. Note that, especially for hard
classes, such a tendency is more pronounced.

Feature Visualization Figure 4 visualizes the instance
embedding of BN (left) and DSBN (right) using MSTN as
a baseline on VisDA-C dataset. We observe that the exam-
ples of two domains in the same class are aligned better by
integrating DSBN, which implies that DSBN is effective to
learn the domain-invariant representations.

Iterative Learning Our framework adopts the network
obtained in the ﬁrst stage as a pseudo labeler in the sec-
ond stage, and the network learned in the second stage is
stronger than the pseudo labeler. Thus we can expect further
performance improvement by applying the second stage
learning procedure iteratively, where the pseudo-labels in
the current iteration are given by the results in the preced-
ing one. To validate this idea, we evaluate the classiﬁcation
accuracy in each iteration on VisDA-C dataset using MSTN
as a baseline algorithm. As shown in Table 6, the iterative
learning of the second stage gradually improves accuracy
over iterations.

stein Distance Guided Representation Learning for Domain
Adaptation. In AAAI, 2018. 2

[22] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of Frus-

tratingly Easy Domain Adaptation. In AAAI, 2016. 2

[23] Baochen Sun and Kate Saenko. Deep CORAL: Correlation
In ECCV Work-

Alignment for Deep Domain Adaptation.
shops, 2016. 1, 2

[24] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
In CVPR,

Adversarial Discriminative Domain Adaptation.
2017. 2

[25] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and
Trevor Darrell. Deep Domain Confusion: Maximizing for Do-
main Invariance. CoRR, abs/1412.3474, 2014. 7

[26] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty,
and Sethuraman Panchanathan. Deep Hashing Network for
Unsupervised Domain Adaptation. In CVPR, 2017. 5, 6

[27] Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen.
Learning Semantic Representations for Unsupervised Domain
Adaptation. In ICML, 2018. 1, 2, 3, 5, 7

[28] Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang,
Yong Xu, and Wangmeng Zuo. Mind the Class Weight Bias:
Weighted Maximum Mean Discrepancy for Unsupervised Do-
main Adaptation. In CVPR, 2017. 2

[29] Werner Zellinger, Thomas Grubinger, Edwin Lughofer,
Thomas Natschl¨ager, and Susanne Saminger-Platz. Central
Moment Discrepancy (CMD) for Domain-Invariant Represen-
tation Learning. In ICLR, 2017. 2

[30] Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Col-
laborative and Adversarial Network for Unsupervised domain
adaptation. In CVPR, 2018. 1, 2, 7

[4] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei A. Efros, and Trevor Dar-
rell. CyCADA: Cycle Consistent Adversarial Domain Adap-
tation. In ICML, 2018. 1, 2

[5] Sergey Ioffe and Christian Szegedy. Batch Normalization:
Accelerating Deep Network Training by Reducing Internal
Covariate Shift. In ICML, 2015. 3, 4

[6] Diederik P. Kingma and Jimmy Ba. Adam: A Method for

Stochastic Optimization. In ICLR, 2015. 6

[7] Yanghao Li, Naiyan Wang, Jianping Shi, Xiaodi Hou, and Ji-
aying Liu. Adaptive Batch Normalization for practical domain
adaptation. Pattern Recognition, 80:109–117, 2018. 2

[8] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft COCO: Common Objects in Context. In
ECCV, 2014. 6

[9] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jor-
dan. Learning Transferable Features with Deep Adaptation
Networks. In ICML, 2015. 6, 7

[10] Mingsheng Long, Zhangjie Cao,

Jianmin Wang, and
Michael I Jordan. Conditional Adversarial Domain Adapta-
tion. In NIPS, 2018. 1, 2, 7

[11] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I
Jordan. Unsupervised Domain Adaptation with Residual
Transfer Networks. In NIPS, 2016. 7

[12] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I
Jordan. Deep Transfer Learning with Joint Adaptation Net-
works. In ICML, 2017. 2, 7

[13] Massimiliano Mancini, Lorenzo Porzi, Samuel Rota Bul,
Barbara Caputo, and Elisa Ricci. Boosting Domain Adap-
tation by Discovering Latent Domains. In CVPR, 2018. 2

[14] Jeroen Manders, Elena Marchiori, and Twan van Laarhoven.
Simple Domain Adaptation with Class Prediction Uncertainty
Alignment. arXiv preprint arXiv:1804.04448, 2018. 1, 2, 3,
5, 7

[15] Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa
Ricci, and Samuel Rota Bulo. AutoDIAL: Automatic DomaIn
Alignment Layers. In ICCV, 2017. 2

[16] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman,
Dequan Wang, and Kate Saenko. VisDA: The Visual Domain
Adaptation Challenge, 2017. 5

[17] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell.
Adapting Visual Category Models to New Domains. In ECCV,
2010. 5

[18] Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada.
Asymmetric Tri-training for Unsupervised Domain Adapta-
tion. In ICML, 2017. 2

[19] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate
Saenko. Adversarial Dropout Regularization.
In Proc. In-
ternational Conference on Learning Representations (ICLR),
2018. 6

[20] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat-
suya Harada. Maximum Classiﬁer Discrepancy for Unsuper-
vised Domain Adaptation. In CVPR, 2018. 6

[21] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasser-

7362

