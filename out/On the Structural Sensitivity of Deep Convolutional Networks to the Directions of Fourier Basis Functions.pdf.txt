On the Structural Sensitivity of Deep Convolutional Networks

to the Directions of Fourier Basis Functions

Issei Sato1,2
Yusuke Tsuzuku1,2,
1The University of Tokyo, 2RIKEN

tsuzuku@ms.k.u-tokyo.ac.jp, sato@k.u-tokyo.ac.jp

Abstract

Data-agnostic quasi-imperceptible perturbations on in-
puts are known to degrade recognition accuracy of deep
convolutional networks severely. This phenomenon is con-
sidered to be a potential security issue. Moreover, some re-
sults on statistical generalization guarantees indicate that
the phenomenon can be a key to improve the networks‚Äô
generalization. However, the characteristics of the shared
directions of such harmful perturbations remain unknown.
Our primal Ô¨Ånding is that convolutional networks are sen-
sitive to the directions of Fourier basis functions. We de-
rived the property by specializing a hypothesis of the cause
of the sensitivity, known as the linearity of neural networks,
to convolutional networks and empirically validated it. As
a by-product of the analysis, we propose an algorithm to
create shift-invariant universal adversarial perturbations
available in black-box settings.

1. Introduction

Malicious perturbations on inputs can easily change pre-
dictions of deep learning models [36]. These perturba-
tions are called adversarial perturbations or adversarial ex-
amples. They have been intensively studied concerning
deep convolutional networks for object recognition tasks
[4, 7, 19, 24, 36, 39]. They are attracting attention be-
cause they are potential security issues. One of the intrigu-
ing aspects of adversarial perturbations is their universality.
Szegedy et al. [36] observed transferability of the perturba-
tions between classiÔ¨Åers. Papernot et al. [28, 29] exploited
the transferability to attack black-box models. Some ad-
versarial perturbations transfer not only between classiÔ¨Åers
but also between inputs. Goodfellow et al. [7] Ô¨Årst dis-
covered the universality, and Moosavi-Dezfooli et al. [22]
studied this phenomenon in more detail. They found that
a single perturbation can change models‚Äô predictions for
a signiÔ¨Åcant portion of data points. Such input-agnostic
perturbations are called universal adversarial perturbations

Figure 1. Examples of images perturbed by single Fourier attack.
Added perturbation is the same as in Figure 3. The size of perturba-
tions is 10/255 in ‚Ñì‚àû-distance for the Ô¨Årst row and 20/255 for the
second. In Sec. 5.7, we show that the single 10/255 and 20/255
perturbations could change predictions for around 40% and 70%
of inputs for various architectures, respectively.

(UAPs). The perturbations also generalize between differ-
ent networks to some extent.

We are primarily concerned with UAPs because of
their relation to statistical generalization guarantees of
deep learning models. For example, studies using PAC-
Bayes [27], compression [1], and minimum description
length [10] are all concerned with how perturbation prop-
agates networks.1 In these analyses, how each perturbation
changes accuracy on training data and true data distribution
matters.
In other words, we have an interest in perturba-
tions transferable between inputs. These are nothing else
but UAPs. We try to shed lights on the tendency of UAPs

1In these studies, we consider perturbations on weights, not inputs.
However, perturbations on weights become noises on inputs of the net-
work‚Äôs subnetworks and investigating UAPs is still useful.

51

Figure 2. Illustration of our UAPs creation algorithm. We only tune frequency of noise. We do not need access to model parameters, output
logits, or training data.

and how they propagate in convolutional neural networks.

Several prior studies tried to understand the properties
of the universality and transferability of adversarial pertur-
bations. Goodfellow et al. [7] explained the existence of
adversarial examples, their transferability, and their univer-
sality using linearity of deep neural networks. Tram`er et
al. [38] investigated the transferable subspace of adversar-
ial perturbations and suggested that it will consist of a high-
dimensional continuous subspace. Moosavi-Dezfooli et al.
[23] showed that the existence of universal adversarial per-
turbations is inevitable given strong geometrical assump-
tions on the decision boundaries of models.

Given the transferability and the universality of adversar-
ial perturbations, it is natural to expect the existence of a set
of directions to which most networks are input-agnostically
sensitive. If we can characterize such directions, it enables
us to improve robustness against such perturbations in prin-
cipled manners. Additionally, we may design better posteri-
ors, weights, or compression algorithms to achieve empiri-
cally better generalization bounds. However, prior work can
only generate such perturbations by sequential optimization
and lacks their useful characterization. We provide a miss-
ing characterization of directions by analyzing Fourier basis
functions.

The motivation of our analysis comes from two parts.
The Ô¨Årst is the linear hypothesis of vulnerability, and the
second is a property of linear convolutional layers that the
singular vectors of which are Fourier basis functions. The
property indicates that sensitive directions of convolutional
networks are a combination of a few Fourier basis func-
tions. Through extensive experiments on various architec-
tures and datasets, we found networks are sensitive to the
directions of Fourier basis functions of some speciÔ¨Åc fre-
quencies.
In other words, we could characterize at least
a subset of universal and transferable adversarial perturba-
tions through Fourier basis functions. We also observed
that some adversarial perturbations exploit the sensitivity to
Fourier basis functions. These Ô¨Åndings not only provide a

new characterization of adversarial perturbations with ben-
eÔ¨Åts described in the preceding paragraph but also suggest
a possibility that some known properties of the universality
of adversarial perturbations might be due to the structure of
convolutional networks.

As a by-product of our analysis, we also developed a
method to create shift-invariant universal adversarial pertur-
bations, which is available in black-box settings. Figure 1
shows examples of perturbed images created by our algo-
rithm, which is explained in Sec. 4. Our perturbations have
simple and shift-invariant patterns, yet achieved high fool
ratio on various pairs of architectures and datasets.

Our contributions are summarized below.

1. We characterized spaces UAPs lie using Fourier basis

functions.

2. We evaluated our hypothesis in extensive experiments.

3. We proposed a black-box algorithm to create shift-

invariant universal adversarial perturbations.

2. Related work

2.1. Adversarial perturbations

One of the most famous algorithms for creating ad-
versarial perturbations is the fast gradient sign method
(FGSM) [7]. Let ùêΩ(ùúÉ, ùë•, ùë°) be a loss with parameter ùúÉ,
an input ùë•, and a target label ùë°. Then, FGSM uses ùúñ 
Sign (‚àáùë•ùêΩ(ùúÉ, ùë•, ùë°)) as the perturbation, where ùúñ is a scaling
parameter. Another popular approach is performing gradi-
ent ascent on some loss ùêΩ(ùúÉ, ùë•, ùë°). Depending on the choice
of the loss and the optimization methods, there are numer-
ous variants for attacks [4, 24]. Adversarial training [7] is
a current effective countermeasure against adversarial per-
turbations. Kurakin et al. [19] conducted a large-scale
study on adversarial training, and Tram`er et al. [37] ex-
tensively studied the transferability for defended and unde-
fended models. Evaluations of defense methods are noto-

52

riously difÔ¨Åcult [2, 40]. Thus, some studies have provided
theoretically grounded defense methods [17, 41].

2.2. Universal adversarial perturbations

Moosavi-Dezfooli et al. [22] showed that some input-
independent perturbations can signiÔ¨Åcantly degrade classi-
Ô¨Åers‚Äô prediction accuracy. Such perturbations are called uni-
versal adversarial perturbations (UAPs). Moosavi-Dezfooli
et al. [22] created UAPs by sequentially optimizing pertur-
bations until we achieve the desired fool ratio. During the
creation, they did not need access to test data. They showed
that UAPs could change over 80% of the predictions of vari-
ous networks trained on ILSVRC2012 [30]. UAPs also gen-
eralize between network architectures to some extent. Re-
cently, Mopuri et al. [25] and Khrulkov et al. [16] pro-
posed activation-maximization approaches for the creation
of UAPs. UAPs degrade the average performance of sys-
tems and have different nature from other kinds of adversar-
ial examples.

2.3. Analysis of transferability and universality

Goodfellow et al. [7] explained the existence of adversar-
ial examples, their transferability, and their universality by
linear hypothesis. In their explanation, the directions of per-
turbations are the most important in adversarial examples.
The hypothesis is based on the following three factors: (1)
modern networks behave like linear classiÔ¨Åers, (2) adversar-
ial perturbations are aligned with the weight vectors of mod-
els, (3) different models learn similar functions. Thus, ad-
versarial perturbations generalize between clean examples,
and also different models. Tram`er et al. [38] analyzed the
dimensionality of the subspace that adversarial examples lie
in. Using Ô¨Årst-order approximation, they found that adver-
sarial examples lie in a high-dimensional subspace, suggest-
ing overwrap of the subspace between classiÔ¨Åers. However,
the structure of the subspace is unknown except for its es-
timated dimensionality. Moosavi-Dezfooli et al. [23] an-
alyzed the existence of UAPs using strong geometrical as-
sumptions. They also proposed an algorithm to Ô¨Ånd UAPs
using Hessian on input, while it is prohibitively slow with
large inputs.

We explain the existence of UAPs on the basis of the
linear hypothesis of Goodfellow et al. [7]. We push forward
the analysis concerning convolutional networks.

2.4. Fourier basis

Jo and Bengio [14] examined whether CNNs learn high-
level features by using Fourier features. Some prior work
used eps compression or other transformations as defenses
against adversarial examples [15, 8, 33]. They remove
high-frequency features from images and relates to this pa-
per. However, connections to universality have not been
explored. Also, the effects of each frequency have not

been studied. In a later section (5), our experiments show
that adversarial perturbations do not necessarily lie in high-
frequency spots.

3. Preliminary

In this section, we describe the relationship between con-
volutional layers and Fourier basis. Notations are summa-
rized in the supplementary material.

3.1. Fourier basis and discrete Fourier transforma-

tion

ùëÅ ùúîùëó

ùëÅ = ùúîùëñ

Let us deÔ¨Åne ùúîùëñ,ùëó

ùëÅ ‚àà ‚ÑÇùëÅ √óùëÅ , where ùúîùëÅ =
exp(2ùúã‚àö‡¢§1/ùëÅ ) is the ùëÅ -th root of an imaginary number.
We deÔ¨Åne ùêπùëÅ be a matrix such that colums are ùëõ fourier
basis functions with different frequencies. In other words,
ùêπùëÅ is a matrix such that

(ùêπùëÅ )ùë¢,ùë£ =

1
‚àöùëÅ

exp(‡¢§2ùúã‚àö‡¢§1(ùë¢ + ùë£)/ùëÅ ).

(1)

We notate the ùëñ-th row of ùêπùëÅ as (ùêπùëÅ )ùëñ. Let us deÔ¨Åne a
transformation ùëÜ : ‚ÑÇùëÅ √óùëÅ ‚Üí ‚ÑÇùëÅ √óùëÅ as follows.

ùëÜ(ùë•)ùë¢,ùë£ =

ùëÅ ‡¢§1

‚àëùëö=0

ùëÅ ‡¢§1

‚àëùëõ=0

ùë•ùëö,ùëõ exp(‡¢§2ùúã‚àö‡¢§1(ùë¢ùëö + ùë£ùëõ)/ùëÅ )

(2)
This transformation ùëÜ is called discrete Fourier transforma-
tion (DFT). Both the transformation and its inverse can be
calculated in the running time of ùëÇ(ùëÅ log ùëÅ ) by using fast
Fourier transformation [5].

3.2. Decomposition of convolution operator

We deÔ¨Åne ùëÑùëÅ := 1

ùëÅ ùêπùëÅ ‚äó ùêπùëÅ , where ‚äó is a Kronecker
product. The eigenvectors of a doubly block circulant ma-
trix are known to be ùëÑùëÅ [12]. Since ùëÑùëÅ is unitary, a dou-
bly block circulant matrix can be decomposed as ùëÑùëÅ ùê∑ùëÑH
ùëÅ ,
where ùëÑH
ùëÅ is an adjoint matrix of ùëÑùëÅ , and ùê∑ is a complex
diagonal matrix. In a case where channel size is one, since
convolution is a doubly circulant matrix when the padding
is ‚Äúwraps around‚Äù[6, 31], the above analysis is directly ap-
plicable. We can extend the result to multi-channel cases,
i.e., ùëö ‚â• 1.
Proposition 1. Let ùëÄ be a matrix which represents a con-
volutional layer with input channel size ùëöin, output channel
size ùëöout, and input size ùëöin √ó ùëÅ √ó ùëÅ . Then, ùëÄ can be
decomposed as

ùëÄ = (ùêºùëöout ‚äó ùëÑùëÅ ) ùêø (ùêºùëöin ‚äó ùëÑùëÅ )H ,

(3)

where ùêø is a block matrix such that each block is a ùëÅ 2√óùëÅ 2

diagonal matrix.

53

4. Fourier analysis

4.2. Reduction layers

In this section, we show that the most sensitive direction
of linear convolutional networks is a combination of a few
Fourier basis functions. The analysis pushes forward the lin-
ear hypothesis of the cause of adversarial examples in Good-
fellow et al. [7]. The linear approximation may not hold
well for deep non-linear networks. However, we can still
expect that adding some Fourier basis functions to inputs
can largely disturb hidden representations of networks. We
assume that the padding of convolutional layers are ‚Äúwraps
around.‚Äù Notations are summarized in the supplementary
material. Proofs of propositions are deferred to the supple-
mentary material.

4.1. Sensitivity of stacked convolutional layers

We Ô¨Årst consider stacked stride-1 convolutional layers
without activation functions. In the case, we can show that
the singular vectors of the whole layers can be represented
by a linear combination of single Fourier basis functions
between input channels.

Proposition 2. Let ùëÄ (ùëñ) be a convolitional layer with in-
put channel size ùëö(ùëô), output channel size ùëö(ùëô+1), input
size ùëö(ùëô) √ó ùëÅ √ó ùëÅ , and stride 1. Let ùëÄ be a stacked
convolutional layers with linear activation, i.e., ùëÄ (ùëã) =
ùëÄ (1) ‚àò ùëÄ (2) ‚àò  ‚àò ùëÄ (ùëë) (ùëã). Then, the right singular
vectors of ùëÄ can be represented by ‚Éóùëé ‚äó (ùêπùëÅ )ùëñ ‚äó (ùêπùëÅ )ùëó for
some ùëñ, ùëó ‚àà {0, . . . , ùëÅ ‡¢§ 1} and ‚Éóùëé ‚àà ‚Ñùùëö(1)

.

In other words, the most sensitive directions of linear
convolutional neural networks without reduction layers is
a single Fourier basis function. We can further extend the
result to cases when there are normalization layers or skip
connections.

volutional layers with linear activation plus a skip connec-

Proposition 3. Let ùëÄ (ùëñ) be a convolitional layer with in-
put channel size ùëö(ùëô), output channel size ùëö(ùëô+1), input
size ùëö(ùëô) √ó ùëÅ √ó ùëÅ , and stride 1. Let ùëÄ be a stacked con-
tion, i.e., ùëÄ (ùëã) = ùëÄ (1) ‚àò ùëÄ (2) ‚àò  ‚àò ùëÄ (ùëë) (ùëã) + ùëã.
Then, the right singular vectors of ùëÄ can be represented by
‚Éóùëé ‚äó (ùêπùëÅ )ùëñ ‚äó (ùêπùëÅ )ùëó for some ùëñ, ùëó ‚àà {0, . . . , ùëÅ ‡¢§ 1} and
‚Éóùëé ‚àà ‚Ñùùëö(1)

.

Proposition 4. A convolutional layer followed by a nor-
malization layer such as batch-normalization or weight-
normalization can be rerepresented as another convolu-
Thus,
tional
Props. 2 and 3 also hold when normalization layers exist.

layer without normalization at

time.

test

These propositions show that manipulating a single
Fourier basis function on input can be most effective to
disturb internal representations of convolutional neural net-
works.

In this section, we show that the singular values of the
convolutional layers can be written by a combination of a
few Fourier basis functions even when there are reduction
layers, such as convolutional layers with stride > 1 or aver-
age pooling layers.

Proposition 5. Let ùëÄ be a convolutional layer with stride
ùë† > 1 where ùëÅ = 0 (mod ùë†). Then,
the singular
value of the layer can be represented by a linear combi-
nation of Fourier basis functions {(ùêπùëÅ )ùëñ‚Ä≤ ‚äó (ùêπùëÅ )ùëó ‚Ä≤‚à£ùëñ‚Ä≤ =
ùëñ (modùë†), ùëó ‚Ä≤ = ùëó (modùë†)} for any ùëñ and ùëó.

Since the average pooling layer is a special case of con-
volutional layers, we can apply the above theorem to the
layer.

4.3. Single Fourier attack

We propose an algorithm to Ô¨Ånd universal adversarial
perturbations using Fourier basis functions. The attack
exploits the sensitivity of convolutional networks to the
Fourier basis directions analyzed in the previous section.
While the linear approximation in the analysis might not
hold well in deep networks, we can still expect that the di-
rections will disturb hidden representations.

A sketch of the algorithm is as follows. We select one
Fourier basis function and use it as a UAP. The method to
select the frequency is described later in this section. The
sketch of the algorithm is incompatible with the restriction
that the inputs must be real. To satisfy the condition, we
have the following proposition.

Proposition 6. ùëÜ(ùë•)ùëñ,ùëó = ùëÜ(ùë•)‚àó
real-valued, where ùëÜ(ùë•)‚àó is a conjugate of ùëÜ(ùë•).

ùëÅ ‡¢§ùëñ,ùëÅ ‡¢§ùëó iff the input ùë• is

Thus, we make ùëÜ(ùë•)ùëñ,ùëó = ùëÜ(ùë•)‚àó

ùëÅ ‡¢§ùëñ,ùëÅ ‡¢§ùëó satisÔ¨Åed to
meet the real-value constraint. Algorithm 1 shows the pseu-
docode of the algorithm, which is named single Fourier at-
tack (SFA). Figure 3 shows a visualization of Fourier ba-
sis in 8 √ó 8 space and an example of perturbations created
by SFA. Figure 1 shows examples of perturbed images. It
seems that this attack does not change human‚Äôs predictions,
and models should be robust against the attack.

To perform the attack, we need to Ô¨Ånd effective frequen-
cies of the target classiÔ¨Åers. To test the sensitivity, Ô¨Årst, we
query a pair of an original image and its perturbed version.
Next, we check whether the classiÔ¨Åer‚Äôs output differs or not.
We repeat the procedure and solve a black-box optimization
problem formulated as follows.

Problem 1. Given a data distribution ùê∑ ‚äÇ ‚ÑùùëÅ √óùëÅ √ó3, tar-
get function ùëì : ‚ÑùùëÅ √óùëÅ √ó3√ó({1, . . . , ùëÅ} √ó {1, . . . , ùëÅ}) ‚Üí
{0, 1}, Ô¨Ånd a frequency ùë§ ‚àà {1, . . . , ùëÅ} √ó {1, . . . , ùëÅ}
which maximizes

‚à´ùê∑

ùëì (ùë•, ùë§)ùëëùë•.

(4)

54

Algorithm 1: Single Fourier attack

hyperparam :ùëñ, ùëó: frequency, ùúñ: size of perturbation
input
foreach c in channel:

:ùë• : image

ùë•ùëê ‚Üê ùë•ùëê + ùúñ((1 + ùëñ)(ùêπùëÅ )ùëñ ‚äó (ùêπùëÅ )ùëó
+(1 ‡¢§ ùëñ)(ùêπùëÅ )ùëÅ ‡¢§ùëñ ‚äó (ùêπùëÅ )ùëÅ ‡¢§ùëó);
ùë•ùëê ‚ÜêClip(ùë•ùëê, 0, 1);

Figure 3. Left: Visualization of Fourier basis in 8 √ó 8 space. Row
ùëñ and column ùëó shows (ùêπ8)ùëñ ‚äó (ùêπ8)ùëó . Right: An example of per-
turbations created by Single Fourier attack in Alg. 1. This pertur-
bation was used in later evaluation (Sec. 5.7).

One naive approach to approximately solve the problem
is testing all frequencies with a batch of images and Ô¨Ånd a
frequency with the highest fool ratio. The batchsize controls
the variance of the evaluation of each frequency. Even if
we do the brute-force search, we can create UAPs within a
reasonable amount of time thanks to the simplicity of our
formulation. As more query efÔ¨Åcient methods, we can also
use Bayesian optimization techniques [34, 3]. We show that
the search of the frequency has a favorable property for such
methods in Sec. 5.4. This suggests our algorithm is useful
even when only small numbers of queries are allowed to
create UAPs.

Our formulation and algorithm have the following two
key features. First, we formulated the creation of UAPs as
an optimization problem of two discrete variables. On the
other hand, the original problem has the same number of
parameters with the input size, which can be tens of thou-
sands. This reduction of parameters to optimize is a signif-
icant simpliÔ¨Åcation. Second, our algorithm requires neither
model parameters nor output logits. Prior UAPs creation
algorithms require access to models or substituted models
created by attackers. These requirements have made the at-
tacks less practical. In our algorithm, we only require the
information on the predicted label by the target. Thus, the
algorithm is available in broader settings.

5. Experiments

We presented a characterization of the universal adversar-
ial directions through Fourier basis functions in Sec. 4. To

show that the characterization well describes the nature of
the universal adversarial directions, we conducted a series
of experiments. Primarily, we answer the following ques-
tions.

1. Whether Fourier basis characterization is better than
others such as characterization using the standard basis
(Sec. 5.2).

2. Whether the sensitivity to the Fourier basis directions

is unique to convolutional networks (Sec. 5.3).

3. Whether UAPs are related to Fourier basis directions

(Sec. 5.5).

4. Whether current white-box attacks are also related to

Fourier basis directions (Sec 5.6).

5. Whether manipulation on a single Fourier basis can
image-agnostically change predictions of various con-
volutional neural networks and datasets (Sec. 5.7).

5.1. Evaluation setups

This section describes the evaluation setups.

A
more detailed explanation can be found in the sup-
plementary material. We used MNIST [21],
fashion-
MNIST [42], SVHN [26], CIFAR10, CIFAR100 [18], and
ILSVRC2015 [30] as datasets. We used a multi-layer per-
ceptron (MLP) consisting of 1000‚Äì1000 hidden layer with
ReLU activation, LeNet [20], WideResNet [43], DenseNet-
BC [11], and VGG [32] with batch-normalization for
evaluations on datasets except for ILSVRC2015.
For
ILSVRC2015, we used ResNet50 [9], DenseNet, VGG16,
and GoogLeNet [35]. For VGG16 and GoogLeNet, we
added a batch-normalization layer after each convolution
for faster training. We used the fool ratio as a metric, which
is the percentage of data that models changed its prediction,
following Moosavi-Dezfooli et al. [22].

5.2. Fourier domain vs pixel domain

We analyzed the sensitivity of deep convolutional neu-
ral networks to the directions of Fourier basis functions
in Sec. 4. To empirically support the analysis, we inves-
tigated the sensitivity on each Fourier basis. For compar-
ison, we checked the sensitivity on the standard basis di-
rections, which is the manipulation on each pixel. We also
tested the sensitivity in random directions (see Sec. 5.7). We
Ô¨Årst describe the method we used to study the sensitivity.
For Fourier basis, we applied a single Fourier attack (Algo-
rithm. 1) and calculated its fool ratio on a single minibatch
for each frequency. We bounded the size of perturbations
by 30/255 in ‚Ñì‚àû-norm for MNIST, FMNIST, and SVHN,
20/255 for ILSVRC2015, and 10/255 for CIFAR10 and
CIFAR100. For a standard basis, we added 255/255 to
each pixel and then clipped to range from zero to one for

55

Algorithm 2: Creation of heatmap

foreach (i,j) in frequencies:

ùêµ := Randomly select Minibatch;
ùë¶ ‚Üê Forward(ùêµ);
ùë¶‚Ä≤ ‚Üê Forward(ùêµ + noise);
Heatmapùëñ,ùëó ‚Üê FoolRatio(ùë¶, ùë¶‚Ä≤);

Figure 4. Visualization of sensitive spot of convolutional networks
in Fourier domain. Coordinate (ùëñ, ùëó) of each image represents fool
ratio on a single minibatch when we used Algorithm 1 as a per-
turbation. White areas are spots with high fool ratio. The cen-
ter of each image corresponds to a high-frequency area. The per-
turbation sizes were 30/255 for MNIST, FMNIST, and SVHN,
and 10/255 for CIFAR10 and CIFAR100. The creation of this
heatmap is described in Algorithm. 2.

Figure 5. Visualization of sensitivity in Fourier domain. Visulaiza-
tion procedure is the same with Figure 4. We can see that most
sensitive frequency is neither hight nor low frequencies, and it lies
in the middle. For reference, frequency distributions in natural
images and random noise can be found in Figure 8.

attack creation, which is an analogy of Algorithm 1. Us-
ing heat maps, we visualized the results for Fourier basis
on ILSVRC2015 in Figure 5 and the results on the other
datasets in Figure 4. The algorithm to create the heat map is
described in Algorithm 2.

We observed that in most cases except for MNIST, ar-
chitectures tend to have some sensitive spots in the Fourier
domain. Especially on CIFAR10 and CIFAR100, VGG and
Wide-ResNet showed near 90% and 99% fooling ratio to
some directions. The result means that the predictions be-

Figure 6. Visualization of the sensitivity of multilayer perceptrons
(MLPs) in the Fourier domain. MLPs did not have sensitive spot
as CNNs in most cases and they were more resistant to directions
of Fourier basis.

came almost random guess. Since all Fourier basis direc-
tions are orthogonal, Figure 4 highlights that there are hun-
dreds of directions that networks are weak independent of
their inputs. While it has been known that there are tens of
orthogonal directions for transferable or universal adversar-
ial examples, to the best of our knowledge, this is the fastest
method to Ô¨Ånd a large number of orthogonal directions for
which networks are universally vulnerable. Contrastive to
Fourier basis, experiments in standard basis achieved al-
most 0% fool ratio in all settings. In this experiment, we
showed the existence of sensitive spots of convolutional net-
works in the Fourier domain and the effectiveness of the
characterization by Fourier basis directions.

5.3. Convolutional networks vs. MLP

In Sec. 5.2 we observed that various convolutional neu-
ral networks are sensitive to some Fourier basis directions.
To see whether the sensitivity to the Fourier basis functions
is caused by network architectures as suggested in Sec. 4 or
the nature of image processing, we compared the sensitivity
of convolutional neural networks and MLP to Fourier ba-
sis functions. We used the same method as Sec. 5.2 for the
comparison. Figure 6 shows the results for the MLP trained
on various datasets. The MLP did not show vulnerability
to some vectors in the Fourier basis. The contrastive acti-
vation pattern of convolutional networks and multilayer per-
ceptrons supports our analysis of the sensitivity in Sec. 4.
This result suggests the possibility that changing architec-
tures is a useful measure to mitigate adversarial examples,
especially UAPs. Since prior defense work has mostly fo-
cused on training methods [7, 19], this opens another re-
search direction for defense methods. For example, we may
use the information of the weak spots in the Fourier domain
to choose which models to use for ensembles.

5.4. Co-occurrence of sensitivity

In the evaluation in Secs. 5.2 and 5.3, we observed that
convolutional networks showed similar sensitivity to the
Fourier basis directions with similar frequencies. Since
Sec. 4 does not cover this phenomenon, we explain it here.
In convolutional networks, the convolution kernel size is
typically much smaller than the input size. The size of the
kernel restricts the expressiveness of convolutional layers.
This restriction makes convolutional layers respond simi-

56

Figure 7. Coordinate (ùëñ, ùëó) of each image shows the magnitude of
outputs of a convolutional layer when input was (ùêπ32)ùëñ ‚äó (ùêπ32)ùëó .
The kernel size of each convolutional layer is 3. They were
trained to maximize the output against (ùêπ32)8 ‚äó (ùêπ32)8, (ùêπ32)8 ‚äó
(ùêπ32)16, (ùêπ32)12 ‚äó (ùêπ32)12, ùëéùëõùëë(ùêπ32)16 ‚äó (ùêπ32)16, respectively.

Figure 8. Visualization of UAPs calculated for various architec-
tures on ILSVRC2012 by Moosavi-Dezfooli et al. [22] in the
Fourier domain. Coordinate (ùëñ, ùëó) corresponds to the fool ratio
of (ùêπ224)ùëñ ‚äó (ùêπ224)ùëó . White spots had higher fool ratio.

larly to similar frequencies. To see the co-occurrence of
the sensitivity, we trained convolutional layers with kernel
size 3 √ó 3 and the input size 32 √ó 32 so that the ‚Ñì2-norm of
their outputs are maximized when one speciÔ¨Åc Fourier basis
is fed as its input. Then we tested the ‚Ñì2-norm of the layer‚Äôs
outputs when their inputs are other Fourier basis functions.
Figure 7 shows the result. The result conÔ¨Årms the hypothe-
sis that convolutional layers respond similarly to Fourier ba-
sis directions with similar frequency. In other words, the op-
timization problem of frequency of Algorithm 1 has a small
Lipschitz constant. This property is known to be favorable
for optimizations in many algorithms including Bayesian
optimizations [34].

5.5. UAPs in Fourier domain

In this section, we investigate whether UAPs created
by an existing method also have some speciÔ¨Åc patterns in
the Fourier domain. For this analysis, we used precom-
puted UAPs for VGG16, VGG19, VGG-F, CaffeNet [13],
ResNet152, and GoogLeNet by Moosavi-Dezfooli et al.
[22]. Figure 8 shows the magnitude of each frequency of
each UAP in log scale. For reference, Figure 8 also shows
those of random noise and average magnitudes of each fre-
quency of original training data in ILSVRC2015.

While architectures and training procedures differ, Fig-
ure 8 and Figure 5 share a similar tendency compared to

Figure 9. Visualization of FGSM attack in the Fourier domain in
the same way as in Figure 8. FGSM had larger values in sensi-
tive spots revealed in Figure 4. Center of each image is a high-
frequency area.

the random noise and original images. For example, we
can see from Figure 5 that the networks are relatively ro-
bust against high-frequency noises and sensitive to low and
middle-frequency noises. From Figure 8, current UAPs ap-
pear to exploit the sensitivity. This suggests the effective-
ness to consider Fourier domain to analyze existing UAPs.

5.6. Adversarial attacks in Fourier domain

In this section, we investigate whether current white box
adversarial attacks also have some tendency in the Fourier
domain. We studied FGSM [7], which is known to transfer
better than naive iterative attacks [19]. Figure 9 shows the
average magnitude of each vector in the Fourier basis of a
perturbation created by FGSM on test data. Compared with
Figure 4, which revealed sensitive spots in the Fourier do-
main, Figure 9 shows that the mass of FGSM concentrates
almost in the sensitive spots. This experiment also shows
that adversarial perturbations do not necessarily lie in a
high-frequency area, which denies a common myth that ad-
versarial perturbations tend to be high-frequency. Figure 9
also shows that the tendency of adversarial perturbations dif-
fers across datasets and architectures, which reminds us to
test defense methods in various settings.

5.7. Effectivceness of Fourier attack

The analysis in Sec. 4 and experiments in Sec. 5.2 ‚Äì
Sec. 5.6 suggests the effectiveness of the Fourier basis func-
tions as universal adversarial perturbations. We evaluated

57

Table 1. Fool ratio of random noise (upper rows) and SFA (Algo-
rithm 1, lower rows) on various architectures and datasets. Despite
the simpleness of our algorithm, some pairs dropped their accuracy
to almost chance ratio. This results show that our characterization
through Fourier basis functions effectively captures the sensitivity
of networks.

LeNet WResNet VGG DenseNet

MNIST

Fashion MNIST

SVHN

CIFAR10
CIFAR100

MNIST

Fashion MNIST

SVHN

CIFAR10
CIFAR100

0.1
5.4
3.1
5.0
13.0

0.4
12.5
64.9
63.3
83.4

55.8
11.4
5.2
8.1
26.4

90.2
48.1
90.8
82.3
93.7

0.0
8.3
0.0
6.8
25.9

0.1
83.7
0.0
72.2
95.8

0.1
12.6
4.6
5.4
22.5

0.2
56.9
50.5
50.7
72.3

Table 2. Fool ratio of Fourier basis attack on various architec-
tures on ILSVRC2015. Rand is random noise, SSFA is deÔ¨Åned in
Sec. 5.7. Attacks are bounded in 10/255 and 20/255 in ‚Ñì‚àû-norm.
UAP denotes the best performing precomputed UAP in [22] per
architecture. Since naively scaling them to 20/255 can be unfairly
advantageous to ours and we just omitted the evaluation. While
comparable, our algorithm does not assume access to the same
training data and also has no need to train models locally.

GoogLeNet

ResNet VGG DenseNet

Rand(10)
Rand(20)

UAP(10) [22]

SFA(10)
SFA(20)

SSFA(10)
SSFA(20)

8.2
14.9

45.5

34.6
62.3

44.1
74.1

8.5
16.1

49.7

38.7
68.5

40.1
66.9

11.5
19.7

64.8

49.7
76.3

53.3
79.0

9.5
16.7

56.0

36.8
63.5

39.5
62.5

its ability to Ô¨Çip predictions on various datasets and architec-
tures. We set the size of perturbations to 10/255 in ‚Ñì‚àû for
CIFAR, and to 30/255 for MNIST, FMNIST, and SVHN.
We used frequencies with the highest fool ratio in Figure 4
as the perturbations. In the evaluation, we used Algorithm 1
with one Ô¨Åxed frequency per pair of dataset and architec-
ture. For comparison, we calculated the fool ratio of ran-
dom noise sampled from the ùúñ-ball bounded in ‚Ñì‚àû-norm.
Table 1 shows the result. Given the dataset and architecture-
agnostic search space, the attack showed strong attack abil-
ity. Especially in CIFAR10 and CIFAR100 experiments,
some architectures dropped prediction accuracy almost to
that of random guessing. This effectiveness of Fourier ba-
sis attack highlights the sensitivity of current convolutional

networks against Fourier features. In MNIST, however, the
fool ratio was not as high as other datasets. Since MNIST is
highly normalized dataset and easiest among them, we sus-
pect that networks can better capture true signal from the
inputs and are more robust to change of a single Fourier ba-
sis direction. From the viewpoints of architectures, LeNet
and DenseNet were more robust than others. We explain
this by their max-pooling layers. As max-pooling layers are
not supported in Sec. 4, they add additional nonlinearities
and mix Fourier basis.

We also tested Algorithm 1 on ILSVRC2015. For the
evaluation, we Ô¨Åxed one frequency for all architectures and
inputs2. In other words, we selected a single perturbation in-
put and architecture agnostically. To choose the frequency,
we took the average of Figure 5 and picked the frequency
with the highest fool ratio. Figure 1 shows examples of cre-
ated adversarial examples. We used 10/255 and 20/255
for the size of perturbations. Note that previous work used
10/255 for the evaluation [22]. Examples of created UAPs
are shown in Figure 1. We empirically found that taking the
sign of Fourier basis can sometimes boost the performance
of the attack. We named this attack Signed-SFA (SSFA),
and we also tested the attack.
In the evaluation, we also
tested random perturbations and the best precomputed UAP
from Moosavi-Dezfooli et al. [22] per architecture. The re-
sult is shown in Table 2. Compared to Moosavi-Dezfooli et
al. [22], the fool ratio is comparable to their perturbations
under this black-box setting. Note, since our algorithm does
not need to train local model, our algorithm is more suitable
in black-box settings.

6. Conclusion

From the analysis of linearized convolutional neural
networks, we hypothesized that convolutional networks
are sensitive to the directions of Fourier basis functions.
Through empirical evaluations, we validated the sensitiv-
ity. The Ô¨Ånding provides a better characterization of univer-
sal adversarial perturbations using Fourier basis functions.
The characterization might be beneÔ¨Åcial to the development
of defense methods and the analysis of statistical general-
ization guarantees. As a by-product of our analysis, we
proposed a black-box method to create universal adversar-
ial perturbations. The algorithm does not require locally
trained models for black-box attack and extends the poten-
tial use cases of universal adversarial perturbations.

Acknowledgement

YT was supported by Toyota/Dwango AI scholarship. IS

was supported by KAKENHI 17H04693.

2The input sizes were the same (224√ó224) among the all architectures

we tested.

58

References

[1] S. Arora, R. Ge, B. Neyshabur, and Y. Zhang. Stronger
Generalization Bounds for Deep Nets via a Compression Ap-
proach. In Proceedings of the 35th International Conference
on Machine Learning, pages 254‚Äì263, 2018.

[2] A. Athalye, N. Carlini, and D. Wagner. Obfuscated Gradi-
ents Give a False Sense of Security: Circumventing Defenses
to Adversarial Examples. In Proceedings of the 35th Inter-
national Conference on Machine Learning, pages 274‚Äì283,
2018.

[3] A. D. Bull. Convergence Rates of EfÔ¨Åcient Global Optimiza-
tion Algorithms. Journal of Machine Learnining Research,
pages 2879‚Äì2904, 2011.

[4] N. Carlini and D. A. Wagner. Towards Evaluating the Robust-
ness of Neural Networks. In Proceedings of the 2017 IEEE
Symposium on Security and Privacy, pages 39‚Äì57.
IEEE
Computer Society, 2017.

[5] J. W. Cooley and J. W. Tukey. An Algorithm for the Ma-
chine Calculation of Complex Fourier Series. Mathematics
of Computation, pages 297‚Äì301, 1965.

[6] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning.

MIT Press, 2016.

[7] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
Harnessing Adversarial Examples. International Conference
on Learning Representations, 2015.

[8] C. Guo, M. Rana, M. Cisse, and L. v. d. Maaten. Countering
Interna-

Adversarial Images using Input Transformations.
tional Conference on Learning Representations, 2018.

[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learn-
ing for Image Recognition. In The IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 770‚Äì778, 2016.
[10] G. E. Hinton and D. v. Camp. Keeping the Neural Net-
works Simple by Minimizing the Description Length of the
Weights. In Proceedings of the Sixth Annual Conference on
Computational Learning Theory, pages 5‚Äì13, 1993.

[11] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger.
Densely Connected Convolutional Networks. In The IEEE
Conference on Computer Vision and Pattern Recognition,
pages 2261‚Äì2269, 2017.

[12] A. K. Jain.

Fundamentals of digital image processing.

Prentice-Hall, Inc., 1989.

[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding.
In ACM Interna-
tional Conference on Multimedia, page 675‚Äì678, 2014.

[14] J. Jo and Y. Bengio.

Measuring the tendency of
CNNs to Learn Surface Statistical Regularities. CoRR,
abs/1711.11561, 2017.

[15] G. Karolina Dziugaite, Z. Ghahramani, and D. M. Roy. A
study of the effect of JPG compression on adversarial images.
CoRR, abs/1608.00853, 2016.

[16] V. Khrulkov and I. Oseledets. Art of singular vectors and

universal adversarial perturbations. CoRR, 2017.

[17] J. Z. Kolter and E. Wong. Provable Defenses against Adver-
sarial Examples via the Convex Outer Adversarial Polytope.
In Proceedings of the 35th International Conference on Ma-
chine Learning, pages 5286‚Äì5295, 2018.

[18] A. Krizhevsky. Learning Multiple Layers of Features from
Tiny Images. Computer Science Department, University of
Toronto, Technical Report, 2009.

[19] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial Ma-
chine Learning at Scale. International Conference on Learn-
ing Representations, 2017.

[20] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based Learning Applied to Document Recognition. In Pro-
ceedings of the IEEE, pages 2278‚Äì2324, 1998.

[21] Y. LeCun,

C. Cortes,
Database

and C.
of

J. C. Burges.
Digits.

Handwritten

The MNIST
http://yann.lecun.com/exdb/mnist/, 1998.

[22] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard.
In Proceedings of the
Universal adversarial perturbations.
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1765‚Äì1773, 2017.

[23] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, P. Frossard, and
S. Soatto. Robustness of ClassiÔ¨Åers to Universal Perturba-
tions: A Geometric Perspective.
International Conference
on Learning Representations, 2018.

[24] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. DeepFool:
A Simple and Accurate Method to Fool Deep Neural Net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2574‚Äì2582, 2016.

[25] K. R. Mopuri, U. Garg, and R. V. Babu. Fast Feature Fool:
A data independent approach to universal adversarial pertur-
bations. In Proceedings of the British Machine Vision Con-
ference, 2017.

[26] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading Digits in Natural Images with Unsupervised
Feature Learning. Neural Information Processing Systems
Workshop, 2011.

[27] B. Neyshabur, S. Bhojanapalli, and N. Srebro.

A
PAC-Bayesian Approach to Spectrally-Normalized Margin
Bounds for Neural Networks. In International Conference
on Learning Representations, 2018.

[28] N. Papernot, P. McDaniel, and I. J. Goodfellow. Transfer-
ability in Machine Learning: from Phenomena to Black-Box
Attacks using Adversarial Samples. CoRR, abs/1605.07277,
2016.

[29] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha, Z. B.
Celik, and A. Swami. Practical Black-Box Attacks against
Machine Learning.
In Proceedings of the 2017 ACM on
Asia Conference on Computer and Communications Security,
pages 506‚Äì519, 2017.

[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), pages 211‚Äì252, 2015.

[31] H. Sedghi, V. Gupta, and P. M. Long. The Singular Values of
Convolutional Layers. In International Conference on Learn-
ing Representations, 2019.

[32] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations, 2015.

59

[33] S. Song, Y. Chen, N.-M. Cheung, and C.-C. J. Kuo. Defense
Against Adversarial Attacks with Saak Transform. CoRR,
abs/1808.01785, 2018.

[34] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian
Process Optimization in the Bandit Setting: No Regret and
Experimental Design.
In Proceedings of the 27th Interna-
tional Conference on International Conference on Machine
Learning, pages 1015‚Äì1022, 2010.

[35] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going Deeper with Convolutions. In The IEEE Conference
on Computer Vision and Pattern Recognition, 2015.

[36] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. J. Goodfellow, and R. Fergus. Intriguing Properties of Neu-
ral Networks. International Conference on Learning Repre-
sentations, 2014.

[37] F. Tram`er, A. Kurakin, N. Papernot, D. Boneh, and P. D. Mc-
Daniel. Ensemble Adversarial Training: Attacks and De-
fenses.
International Conference on Learning Representa-
tions, 2018.

[38] F. Tram`er, N. Papernot, I. J. Goodfellow, D. Boneh, and P. D.
McDaniel. The Space of Transferable Adversarial Examples.
CoRR, abs/1704.03453, 2017.

[39] Y. Tsuzuku, I. Sato, and M. Sugiyama. Lipschitz-Margin
Training: Scalable CertiÔ¨Åcation of Perturbation Invariance
for Deep Neural Networks. In Advances in Neural Informa-
tion Processing Systems 31, pages 6542‚Äì6551. 2018.

[40] J. Uesato, B. O‚ÄôDonoghue, P. Kohli, and A. Oord. Adver-
sarial Risk and the Dangers of Evaluating Against Weak At-
tacks. In Proceedings of the 35th International Conference
on Machine Learning, pages 5025‚Äì5034, 2018.

[41] L. Weng, H. Zhang, H. Chen, Z. Song, C. Hsieh, L. Daniel,
D. Boning, and I. Dhillon. Towards Fast Computation of
CertiÔ¨Åed Robustness for ReLU Networks.
In Proceedings
of the 35th International Conference on Machine Learning,
pages 5276‚Äì5285, 2018.

[42] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a Novel
Image Dataset for Benchmarking Machine Learning Algo-
rithms. CoRR, abs/1708.07747, 2017.

[43] S. Zagoruyko and N. Komodakis. Wide Residual Networks.
In Proceedings of the British Machine Vision Conference,
pages 87.1‚Äì87.12, 2016.

60

