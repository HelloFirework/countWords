Character Region Awareness for Text Detection

Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee∗

Clova AI Research, NAVER Corp.

{youngmin.baek, bado.lee, dongyoon.han, sangdoo.yun, hwalsuk.lee}@navercorp.com

Abstract

Scene text detection methods based on neural networks
have emerged recently and have shown promising results.
Previous methods trained with rigid word-level bounding
boxes exhibit limitations in representing the text region in
an arbitrary shape. In this paper, we propose a new scene
text detection method to effectively detect text area by ex-
ploring each character and afﬁnity between characters. To
overcome the lack of individual character level annotations,
our proposed framework exploits both the given character-
level annotations for synthetic images and the estimated
character-level ground-truths for real images acquired by
the learned interim model. In order to estimate afﬁnity be-
tween characters, the network is trained with the newly
proposed representation for afﬁnity. Extensive experiments
on six benchmarks, including the TotalText and CTW-1500
datasets which contain highly curved texts in natural im-
ages, demonstrate that our character-level text detection
signiﬁcantly outperforms the state-of-the-art detectors. Ac-
cording to the results, our proposed method guarantees high
ﬂexibility in detecting complicated scene text images, such
as arbitrarily-oriented, curved, or deformed texts.

1. Introduction

Scene text detection has attracted much attention in the
computer vision ﬁeld because of its numerous applications,
such as instant translation, image retrieval, scene parsing,
geo-location, and blind-navigation. Recently, scene text de-
tectors based on deep learning have shown promising per-
formance [7, 40, 20, 3, 9, 8, 10, 11, 16, 23, 24, 31, 25].
These methods mainly train their networks to localize word-
level bounding boxes. However, they may suffer in difﬁcult
cases, such as texts that are curved, deformed, or extremely
long, which are hard to detect with a single bounding box.
Alternatively, character-level awareness has many advan-
tages when handling challenging texts by linking the suc-
cessive characters in a bottom-up manner. Unfortunately,

∗Corresponding author.

l
a
t
n
o
z
i
r
o
H

d
e
v
r
u
C

y
r
a
r
t
i
b
r
A

(a)

(b)

Figure 1. Visualization of character-level detection using CRAFT.
(a) Heatmaps predicted by our proposed framework. (b) Detection
results for texts of various shape.

most of the existing text datasets do not provide character-
level annotations, and the work needed to obtain character-
level ground truths is too costly.

In this paper, we propose a novel text detector that local-
izes the individual character regions and links the detected
characters to a text instance. Our framework, referred to as
CRAFT for Character Region Awareness For Text detec-
tion, is designed with a convolutional neural network pro-
ducing the character region score and afﬁnity score. The
region score is used to localize individual characters in
the image, and the afﬁnity score is used to group each
character into a single instance. To compensate for the
lack of character-level annotations, we propose a weakly-
supervised learning framework that estimates character-
level ground truths in existing real word-level datasets.

Figure. 1 is a visualization of CRAFT’s results on
various shaped texts. By exploiting character-level re-
gion awareness, texts in various shapes are easily repre-
sented. We demonstrate extensive experiments on ICDAR
datasets [13, 12, 27] to validate our method, and the experi-

19365

ments show that the proposed method outperforms state-of-
the-art text detectors. Furthermore, experiments on MSRA-
TD500, CTW-1500, and TotalText datasets [36, 38, 2] show
the high ﬂexibility of the proposed method on complicated
cases, such as long, curved, and/or arbitrarily shaped texts.

2. Related Work

The major trend in scene text detection before the
emergence of deep learning was bottom-up, where hand-
crafted features were mostly used – such as MSER [26] or
SWT [4]– as a basic component. Recently, deep learning-
based text detectors have been proposed by adopting pop-
ular object detection/segmentation methods like SSD [19],
Faster R-CNN [29], and FCN [22].

Regression-based text detectors Various text detectors
using box regression adapted from popular object detec-
tors have been proposed. Unlike objects in general, texts
are often presented in irregular shapes with various as-
pect ratios. To handle this problem, TextBoxes [17] mod-
iﬁed convolutional kernels and anchor boxes to effectively
capture various text shapes. DMPNet [21] tried to further
reduce the problem by incorporating quadrilateral sliding
windows. In recent, Rotation-Sensitive Regression Detec-
tor (RSDD) [18] which makes full use of rotation-invariant
features by actively rotating the convolutional ﬁlters was
proposed. However, there is a structural limitation to cap-
turing all possible shapes that exist in the wild when using
this approach.

Segmentation-based text detectors Another common
approach is based on works dealing with segmentation,
which aims to seek text regions at the pixel level. These ap-
proaches that detect texts by estimating word bounding ar-
eas, such as Multi-scale FCN [6], Holistic-prediction [37],
and PixelLink [3] have also been proposed using segmen-
tation as their basis. SSTD [7] tried to beneﬁt from both
the regression and segmentation approaches by using an at-
tention mechanism to enhance text related area via reduc-
ing background interference on the feature level. Recently,
TextSnake [23] was proposed to detect text instances by pre-
dicting the text region and the center line together with ge-
ometry attributes.

End-to-end text detectors An end-to-end approach
trains the detection and recognition modules simultaneously
so as to enhance detection accuracy by leveraging the recog-
nition result. FOTS [20] and EAA [8] concatenate popu-
lar detection and recognition methods, and train them in
an end-to-end manner. Mask TextSpotter [24] took advan-
tage of their uniﬁed model to treat the recognition task as a
semantic segmentation problem. It is obvious that training
with the recognition module helps the text detector be more
robust to text-like background clutters.

Conv[1×1×16]

Conv[3×3×16]

Conv[3×3×32]

Conv[3×3×32]

UpConv Block
(h/2×w/2×32)

Stage4

UpSample (2x)
UpConv Block
(h/4×w/4×64)

Stage3

UpSample (2x)
UpConv Block
(h/8×w/8×128)

Stage2

UpSample (2x)
UpConv Block

(h/16×w/16×256)

Stage1

Image

(h×w×3)

VGG16-BN

Conv Stage1
(h/2×w/2×64)

Conv Stage2

(h/4×w/4×128)

Conv Stage3

(h/8×w/8×256)

Conv Stage4

(h/16×w/16×512)

Conv Stage5

(h/32×w/32×512)

Conv Stage6

(h/32×w/32×512)

Region score 
(h/2×w/2×1)

Affinity score
(h/2×w/2×1)

UpConv Block

Batch_norm

Conv[3×3×out_ch]

Batch_norm

Conv[1×1×(out_ch×2)]

:

Concat

Figure 2. Schematic illustration of our network architecture.

Most methods detect text with words as its unit, but
deﬁning the extents to a word for detection is non-trivial
since words can be separated by various criteria, such as
meaning, spaces or color. In addition, the boundary of
the word segmentation cannot be strictly deﬁned, so the
word segment itself has no distinct semantic meaning. This
ambiguity in the word annotation dilutes the meaning of
the ground truth for both regression and segmentation ap-
proaches.

Character-level text detectors Zhang et al. [39] pro-
posed a character level detector using text block candidates
distilled by MSER [26]. The fact that it uses MSER to iden-
tify individual characters limits its detection robustness un-
der certain situations, such as scenes with low contrast, cur-
vature, and light reﬂection. Yao et al. [37] used a prediction
map of the characters along with a map of text word re-
gions and linking orientations that require character level
annotations. Instead of an explicit character level predic-
tion, Seglink [31] hunts for text grids (partial text seg-
ments) and associates these segments with an additional
link prediction. Even though Mask TextSpotter [24] predicts
a character-level probability map, it was used for text recog-
nition instead of spotting individual characters.

This work is inspired by the idea of WordSup [10], which
uses a weakly supervised framework to train the character-
level detector. However, a disadvantage of Wordsup is that
the character representation is formed in rectangular an-
chors, making it vulnerable to perspective deformation of
characters induced by varying camera viewpoints. More-
over, it is bound by the performance of the backbone struc-
ture (i.e. using SSD and being limited by the number of
anchor boxes and their sizes).

9366

Affinity Box Generation

Character Boxes

Each

Character

Box

Score Generation Module

A box

Transformed
2D Gaussian

Region Score GT

Character box

Affinity box

Center of a character box

Center of a triangle

Each

Affinity

Box

2D Gaussian

Affinity Boxes

Perspective 
Transform

Affinity Score GT

Figure 3. Illustration of ground truth generation procedure in our framework. We generate ground truth labels from a synthetic image that
has character level annotations.

3. Methodology

Our main objective is to precisely localize each individ-
ual character in natural images. To this end, we train a deep
neural network to predict character regions and the afﬁn-
ity between characters. Since there is no public character-
level dataset available, the model is trained in a weakly-
supervised manner.

3.1. Architecture

A fully convolutional network architecture based on
VGG-16 [33] with batch normalization is adopted as our
backbone. Our model has skip connections in the decoding
part, which is similar to U-net [30] in that it aggregates low-
level features. The ﬁnal output has two channels as score
maps: the region score and the afﬁnity score. The network
architecture is schematically illustrated in Fig. 2.

3.2. Training

3.2.1 Ground Truth Label Generation

For each training image, we generate the ground truth label
for the region score and the afﬁnity score with character-
level bounding boxes. The region score represents the prob-
ability that the given pixel is the center of the character,
and the afﬁnity score represents the center probability of the
space between adjacent characters.

Unlike a binary segmentation map, which labels each
pixel discretely, we encode the probability of the character
center with a Gaussian heatmap. This heatmap representa-
tion has been used in other applications, such as in pose es-
timation works [1, 28] due to its high ﬂexibility when deal-
ing with ground truth regions that are not rigidly bounded.
We use the heatmap representation to learn both the region
score and the afﬁnity score.

Fig. 3 summarizes the label generation pipeline for a syn-
thetic image. Computing the Gaussian distribution value di-
rectly for each pixel within the bounding box is very time-

consuming. Since character bounding boxes on an image
are generally distorted via perspective projections, we use
the following steps to approximate and generate the ground
truth for both the region score and the afﬁnity score: 1) pre-
pare a 2-dimensional isotropic Gaussian map; 2) compute
perspective transform between the Gaussian map region and
each character box; 3) warp Gaussian map to the box area.
For the ground truths of the afﬁnity score, the afﬁnity
boxes are deﬁned using adjacent character boxes, as shown
in Fig. 3. By drawing diagonal lines to connect opposite cor-
ners of each character box, we can generate two triangles –
which we will refer to as the upper and lower character tri-
angles. Then, for each adjacent character box pair, an afﬁn-
ity box is generated by setting the centers of the upper and
lower triangles as corners of the box.

The proposed ground truth deﬁnition enables the model
to detect large or long-length text instances sufﬁciently, de-
spite using small receptive ﬁelds. On the other hand, pre-
vious approaches like box regression require a large recep-
tive ﬁeld in such cases. Our character-level detection makes
it possible for convolutional ﬁlters to focus only on intra-
character and inter-character, instead of the entire text in-
stance.

3.2.2 Weakly-Supervised Learning

Unlike synthetic datasets, real images in a dataset usu-
ally have word-level annotations. Here, we generate char-
acter boxes from each word-level annotation in a weakly-
supervised manner, as summarized in Fig. 4. When a real
image with word-level annotations is provided, the learned
interim model predicts the character region score of the
cropped word images to generate character-level bound-
ing boxes. In order to reﬂect the reliability of the interim
model’s prediction, the value of the conﬁdence map over
each word box is computed proportional to the number of
the detected characters divided by the number of the ground
truth characters, which is used for the learning weight dur-

9367

Cropped

Splitting 
Characters

(6/6)

(5/7)

(5/6)

Confidence  map

Real  Image

Synthetic Image

Generate Pseudo-GT

Train with Real Image 

Train with Synthetic Image 

Loss

Loss

Pseudo GT

Synthetic GT

Figure 4. Illustration of the overall training stream for the proposed method. Training is carried out using both real and synthetic images in
a weakly-supervised fashion.

ing training.

Fig. 6 shows the entire procedure for splitting the char-
acters. First, the word-level images are cropped from the
original image. Second, the model trained up to date pre-
dicts the region score. Third, the watershed algorithm [35]
is used to split the character regions, which is used to make
the character bounding boxes covering regions. Finally, the
coordinates of the character boxes are transformed back into
the original image coordinates using the inverse transform
from the cropping step. The pseudo-ground truths (pseudo-
GTs) for the region score and the afﬁnity score can be gen-
erated by the steps described in Fig. 3 using the obtained
quadrilateral character-level bounding boxes.

When the model is trained using weak-supervision, we
are compelled to train with incomplete pseudo-GTs. If the
model is trained with inaccurate region scores, the output
might be blurred within character regions. To prevent this,
we measure the quality of each pseudo-GTs generated by
the model. Fortunately, there is a very strong cue in the
text annotation, which is the word length. In most datasets,
the transcription of words is provided and the length of the
words can be used to evaluate the conﬁdence of the pseudo-
GTs.

For a word-level annotated sample w of the training data,
let R(w) and l(w) be the bounding box region and the word
length of the sample w, respectively. Through the charac-
ter splitting process, we can obtain the estimated character
bounding boxes and their corresponding length of charac-
ters lc(w). Then the conﬁdence score sconf (w) for the sam-
ple w is computed as,

sconf (w) =

l(w) − min(l(w), |l(w) − lc(w)|)

l(w)

,

(1)

Wordbox

Epoch #1

Epoch #2

Epoch #3

Epoch #4

.

.

.

Epoch #10

Charbox

.

.

.

Figure 5. Character region score maps during training.

and the pixel-wise conﬁdence map Sc for an image is com-
puted as,

Sc(p) =(sconf (w) p ∈ R(w),

otherwise,

1

(2)

where p denotes the pixel in the region R(w). The objective
L is deﬁned as,

L =Xp

Sc(p)·(cid:0)||Sr(p) − S ∗

r (p)||2

2 + ||Sa(p) − S ∗

a(p)||2

2(cid:1) ,

r (p) and S ∗

(3)
a(p) denote the pseudo-ground truth re-
where S ∗
gion score and afﬁnity map, respectively, and Sr(p) and
Sa(p) denote the predicted region score and afﬁnity score,
respectively. When training with synthetic data, we can ob-
tain the real ground truth, so Sc(p) is set to 1.

As training is performed, the CRAFT model can pre-
dict characters more accurately, and the conﬁdence scores
sconf (w) are gradually increased as well. Fig. 5 shows the
character region score map during training. At the early
stages of training, the region scores are relatively low for
unfamiliar text in natural images. The model learns the ap-

9368

Word-level annotation

Character-level annotation

Character  split

Cropping

Unwarping

Word box

Region score

Watershed labeling

Character box

Figure 6. Character split procedure for achieving character-level annotation from word-level annotation: 1) crop the word-level image; 2)
predict the region score; 3) apply the watershed algorithm; 4) get the character bounding boxes; 5) unwarp the character bounding boxes.

pearances of new texts, such as irregular fonts, and synthe-
sized texts that have a different data distribution against that
of the SynthText dataset.

If the conﬁdence score sconf (w) is below 0.5, the esti-
mated character bounding boxes should be neglected since
they have adverse effects when training the model. In this
case, we assume the width of the individual character is con-
stant and compute the character-level predictions by simply
dividing the word region R(w) by the number of characters
l(w). Then, sconf (w) is set to 0.5 to learn unseen appear-
ances of texts.

3.3. Inference

At the inference stage, the ﬁnal output can be delivered
in various shapes, such as word boxes or character boxes,
and further polygons. For datasets like ICDAR, the evalua-
tion protocol is word-level intersection-over-union (IoU), so
here we describe how to make word-level bounding boxes
QuadBox from the predicted Sr and Sa.

The post-processing for ﬁnding bounding boxes is sum-
marized as follows. First, the binary map M covering the
image is initialized with 0. M (p) is set to 1 if Sr(p) > τr
or Sa(p) > τa, where τr is the region threshold and τa is
the afﬁnity threshold. Second, Connected Component La-
beling (CCL) on M is performed. Lastly, QuadBox is ob-
tained by ﬁnding a rotated rectangle with the minimum area
enclosing the connected components corresponding to each
of the labels. The functions like connectedComponents and
minAreaRect provided by OpenCV can be applied for this
purpose.

Note that an advantage of CRAFT is that it does not need

Scanning direction

QuadBox

q

q

Polygon

: Character  region
: Local  maxima  along scanning direction

: Center  line of local maxima
: Line  of control points (tilted  from local  maxima)

: Control points of text polygon

: Polygon text instance

Figure 7. Polygon generation for arbitrarily-shaped texts.

any further post-processing methods, like Non-Maximum
Suppression (NMS). Since we have image blobs of word
regions separated by CCL, the bounding box for a word is
simply deﬁned by the single enclosing rectangle. On a dif-
ferent note, our character linking process is conducted at
a pixel-level, which differs from other linking-based meth-
ods [31, 10] relying on searching relations between text
components explicitly.

Additionally, we can generate a polygon around the en-
tire character region to deal with curved texts effectively.
The procedure of polygon generation is illustrated in Fig. 7.
The ﬁrst step is to ﬁnd the local maxima line of character
regions along the scanning direction, as shown in the ﬁgure
with arrows in blue. The lengths of the local maxima lines

9369

Method

Zhang et al. [39]
Yao et al. [37]
SegLink [31]

SSTD [7]

Wordsup [10]
EAST∗ [40]
He et al. [9]
R2CNN [11]
TextSnake [23]

TextBoxes++∗ [16]

EAA [8]

Mask TextSpotter [24]

PixelLink∗ [3]

RRD∗ [18]

Lyu et al.∗ [25]

FOTS [20]

IC13(DetEval)
H
R

P

IC15

IC17

R

P

H

R

P

H

MSRA-TD500
H
R

P

78
80.2
83.0
86
87.5

-
81
82.6

-
86
87
88.1
87.5
86
84.4

-

88
88.8
87.7
89
93.3

-
92
93.6

-
92
88
94.1
88.6
92
92.0

-

83
84.3
85.3
88
90.3

-
86
87.7

-
89
88
91.0
88.1
89
88.0
87.3

43
58.7
76.8
73
77.0
78.3
80
79.7
80.4
78.5
83
81.2
82.0
80.0
79.7
82.0

71
72.3
73.1
80
79.3
83.3
82
85.6
84.9
87.8
84
85.8
85.5
88.0
89.5
88.8

54
64.8
75.0
77
78.2
80.7
81
82.5
82.6
82.9
83
83.4
83.7
83.8
84.3
85.3

-
-
-
-
-
-
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-
-
-
-
-
-

70.6
57.5

74.3
79.5

72.4
66.7

67
75.3
70
-
-

67.4
70
-

73.9

-
-
-

73.2
73
76.2

-

83
76.5
86
-
-

87.3
77
-

83.2

-
-
-

83.0
87
87.6

-

74
75.9
77
-
-

76.1
74
-

78.3

-
-
-

77.8
79
81.5

-

FPS

0.48
1.61
20.6
7.7
1.9
13.2
1.1
0.4
1.1
2.3

-

4.8
3.0
10
5.7
23.9

CRAFT(ours)

93.1

97.4

95.2

84.3

89.8

86.9

68.2

80.6

73.9

78.2

88.2

82.9

8.6

Table 1. Results on quadrilateral-type datasets, such as ICDAR and MSRA-TD500. ∗ denote the results based on multi-scale tests. Methods
in italic are results solely from the detection of end-to-end models for a fair comparison. R, P, and H refer to recall, precision and H-mean,
respectively. The best score is highlighted in bold. FPS is for reference only because the experimental environments are different. We report
the best FPSs, each of which was reported in the original paper.

are equally set as the maximum length among them to pre-
vent the ﬁnal polygon result from becoming uneven. The
line connecting all the center points of the local maxima is
called the center line, shown in yellow. Then, the local max-
ima lines are rotated to be perpendicular to the center line
to reﬂect the tilt angle of characters, as expressed by the red
arrows. The endpoints of the local maxima lines are the can-
didates for the control points of the text polygon. To fully
cover the text region, we move the two outer-most tilted lo-
cal maxima lines outward along the local maxima center
line, making the ﬁnal control points (green dots).

4. Experiment

4.1. Datasets

ICDAR2013 (IC13) was released during the ICDAR 2013
Robust Reading Competition for focused scene text detec-
tion, consisting of high-resolution images, 229 for training
and 233 for testing, containing texts in English. The anno-
tations are at word-level using rectangular boxes.
ICDAR2015 (IC15) was introduced in the ICDAR 2015
Robust Reading Competition for incidental scene text de-
tection, consisting of 1000 training images and 500 testing
images, both with texts in English. The annotations are at
the word level using quadrilateral boxes.
ICDAR2017 (IC17) contains 7,200 training images, 1,800
validation images, and 9,000 testing images with texts in 9
languages for multi-lingual scene text detection. Similar to
IC15, the text regions in IC17 are also annotated by the 4

Method

TotalText

CTW-1500

R

P

H

R

P

H

CTD+TLOC [38]
MaskSpotter [24] 55.0 69.0 61.3

-

-

-

69.8 77.4 73.4

-

-

-

TextSnake [23]

74.5 82.7 78.4 85.3 67.9 75.6

CRAFT(ours)

79.9 87.6 83.6 81.1 86.0 83.5

Table 2. Results on polygon-type datasets, such as TotalText and
CTW-1500. R, P and H refer to recall, precision and H-mean, re-
spectively. The best score is highlighted in bold.

vertices of quadrilaterals.
MSRA-TD500 (TD500) contains 500 natural
images,
which are split into 300 training images and 200 testing im-
ages, collected both indoors and outdoors using a pocket
camera. The images contain English and Chinese scripts.
Text regions are annotated by rotated rectangles.
TotalText (TotalText), recently presented in ICDAR 2017,
contains 1255 training and 300 testing images. It especially
provides curved texts, which are annotated by polygons and
word-level transcriptions.
CTW-1500 (CTW) consists of 1000 training and 500 test-
ing images. Every image has curved text instances, which
are annotated by polygons with 14 vertices.

4.2. Training strategy

The training procedure includes two steps: we ﬁrst use
the SynthText dataset [5] to train the network for 50k iter-
ations, then each benchmark dataset is adopted to ﬁne-tune

9370

the model. Some “DO NOT CARE” text regions in ICDAR
2015 and ICDAR 2017 datasets are ignored in training by
setting sconf (w) to 0. We use the ADAM [15] optimizer in
all training processes. For multi-GPU training, the training
and supervision GPUs are separated, and pseudo-GTs gen-
erated by the supervision GPUs are stored in the memory.
During ﬁne-tuning, the SynthText dataset is also used at a
rate of 1:5 to make sure that the character regions are surely
separated. In order to ﬁlter out texture-like texts in natural
scenes, On-line Hard Negative Mining [32] is applied at a
ratio of 1:3. Also, basic data augmentation techniques like
crops, rotations, and/or color variations are applied.

Weakly-supervised training requires two types of data;
quadrilateral annotations for cropping word images and
transcriptions for calculating word length. The datasets
meeting these conditions are IC13, IC15, and IC17. Other
datasets such as MSRA-TD500, TotalText, and CTW-1500
do not meet the requirements. MSRA-TD500 does not pro-
vide transcriptions, while TotalText and CTW-1500 provide
polygon annotations only. Therefore, we trained CRAFT
only on the ICDAR datasets, and tested on the others with-
out ﬁne-tuning. Two different models are trained with the
ICDAR datasets. The ﬁrst model is trained on IC15 to eval-
uate IC15 only. The second model is trained on both IC13
and IC17 together, which is used for evaluating the other
ﬁve datasets. No extra images are used for training. The
number of iterations for ﬁne-tuning is set to 25k. All exper-
iments are performed with NAVER Smart Machinie Learn-
ing (NSML) platform [14, 34].

4.3. Experimental Results

Quadrilateral-type datasets
(ICDARs, and MSRA-
TD500) All experiments are performed with a single image
resolution. The longer side of the images in IC13, IC15,
IC17, and MSRA-TD500 are resized to 960, 2240, 2560,
and 1600, respectively. Table 1 lists the experimental results
of various methods on ICDAR and MSRA-TD500 datasets.
To have a fair comparison with end-to-end methods, we in-
clude their detection-only results by referring to the origi-
nal papers. We achieve state-of-the-art performances on all
the datasets. In addition, CRAFT runs at 8.6 FPS on IC13
dataset, which is comparatively fast, thanks to the simple
yet effective post-processing.

For MSRA-TD500, annotations are provided at the line-
level, including the spaces between words in the box. There-
fore, a post-processing step for combining word boxes is
applied. If the right side of one box and the left side of an-
other box are close enough, the two boxes are combined
together. Even though ﬁne-tuning is not performed on the
TD500 training set, CRAFT outperforms all other methods
as shown in Table 1.
Polygon-type datasets (TotalText, CTW-1500) It is chal-
lenging to directly train the model on TotalText and CTW-

Method

IC13 IC15 IC17

Mask TextSpotter [24]

EAA [8]
FOTS [20]

91.7
90
92.8

86.0
87
89.8

-
-

70.8

CRAFT(ours)

95.2

86.9

73.9

Table 3. H-mean comparison with end-to-end methods. Our
method is not trained in an end-to-end manner, yet shows com-
parable results, or even outperforms popular methods.

1500 because their annotations are in polygonal in shape,
which complicates text area cropping for splitting character
boxes during weakly-supervised training. Consequently, we
only used the training images from IC13 and IC17, and ﬁne-
tuning was not conducted to learn the training images pro-
vided by these datasets. At the inference step, we used the
polygon generation post-processing from the region score
to cope with the provided polygon-type annotations.

The experiments for these datasets are performed with a
single image resolution, too. The longer sides of the images
within TotalText and CTW-1500 are resized to 1280 and
1024, respectively. The experimental results for polygon-
type datasets are shown in Table 2. The individual-character
localization ability of CRAFT enables us to achieve more
robust and superior performance in detecting arbitrarily
shaped texts compared to other methods. Particularly, the
TotalText dataset has a variety of deformations, including
curved texts as shown in Fig. 8, for which adequate in-
ference by quadrilateral-based text detectors is infeasible.
Therefore, a very limited number of methods can be evalu-
ated on those datasets.

In the CTW-1500 dataset’s case, two difﬁcult character-
istics coexist, namely annotations that are provided at the
line-level and are of arbitrary polygons. To aid CRAFT in
such cases, a small link reﬁnement network, which we call
the LinkReﬁner, is used in conjunction with CRAFT. The
input of the LinkReﬁner is a concatenation of the region
score, the afﬁnity score, and the intermediate feature map of
CRAFT, and the output is a reﬁned afﬁnity score adjusted
for long texts. To combine characters, the reﬁned afﬁnity
score is used instead of the original afﬁnity score, then the
polygon generation is performed in the same way as it was
performed for TotalText. Only LinkReﬁner is trained on the
CTW-1500 dataset while freezing CRAFT. The detailed im-
plementation of LinkReﬁner is addressed in the supplemen-
tary materials. As shown in Table 2, the proposed method
achieves state-of-the-art performance.

4.4. Discussions

Robustness to Scale Variance We solely performed single-
scale experiments on all the datasets, even though the size of
texts are highly diverse. This is different from the majority
of other methods, which rely on multi-scale tests to handle

9371

Figure 8. Results on the TotalText dataset. First row: each column shows the input image (top) with its respective region score map (bottom
left) and afﬁnity map (bottom right). Second row: each column only shows the input image (left) and its region score map (right).

the scale variance problem. This advantage comes from the
property of our method localizing individual characters, not
the whole text. The relatively small receptive ﬁeld is suf-
ﬁcient to cover a single character in a large image, which
makes CRAFT robust in detecting scale variant texts.

Multi-language issue The IC17 dataset contains Bangla
and Arabic characters, which are not included in the syn-
thetic text dataset. Moreover, both languages are difﬁcult
to segment into characters individually because every char-
acter is written cursively. Therefore, our model could not
distinguish Bangla and Arabic characters as well as it does
Latin, Korean, Chinese, and Japanese. In East Asian char-
acters’ cases, they can be easily separated with a constant
width, which helps train the model to high performance via
weakly-supervision.

Comparison with End-to-end methods Our method is
trained with the ground truth boxes only for detection, but
it is comparable with other end-to-end methods, as shown
in Table. 3. From the analysis of failure cases, we expect
our model to beneﬁt from the recognition results, especially
when the ground truth words are separated by semantics,
rather than visual cues.

capturing general characteristics of texts, rather than over-
ﬁtting to a particular dataset.

5. Conclusion

We have proposed a novel text detector called CRAFT,
which can detect individual characters even when character-
level annotations are not given. The proposed method pro-
vides the character region score and the afﬁnity score that,
together, fully cover various text shapes in a bottom-up
manner. Since real datasets provided with character-level
annotations are rare, we proposed a weakly-supervised
learning method that generates pseudo-ground truthes from
an interim model. CRAFT shows state-of-the-art perfor-
mances on most public datasets and demonstrates general-
ization ability by showing these performances without ﬁne-
tuning. As our future work, we hope to train our model
with a recognition model in an end-to-end fashion to see
whether the performance, robustness, and generalizability
of CRAFT translates to a better scene text spotting system
that can be applied in more general settings.

Generalization ability Our method achieved state-of-the-
art performances on 3 different datasets without additional
ﬁne-tuning. This demonstrates that our model is capable of

Acknowledgements. The authors would like to thank
Beomyoung Kim, Daehyun Nam, and Donghyun Kim for
helping with extensive experiments.

9372

References

[1] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-
person 2d pose estimation using part afﬁnity ﬁelds. In CVPR,
pages 1302–1310. IEEE, 2017. 3

[2] C. K. Ch’ng and C. S. Chan. Total-text: A comprehensive
dataset for scene text detection and recognition. In ICDAR,
volume 1, pages 935–942. IEEE, 2017. 2

[3] D. Deng, H. Liu, X. Li, and D. Cai. Pixellink: Detecting
scene text via instance segmentation. In AAAI, 2018. 1, 2, 6

[4] B. Epshtein, E. Ofek, and Y. Wexler. Detecting text in natural
scenes with stroke width transform. In CVPR, pages 2963–
2970. IEEE, 2010. 2

[5] A. Gupta, A. Vedaldi, and A. Zisserman. Synthetic data for
text localisation in natural images. In CVPR, pages 2315–
2324, 2016. 6

[6] D. He, X. Yang, C. Liang, Z. Zhou, G. Alexander, I. Ororbia,
D. Kifer, and C. L. Giles. Multi-scale fcn with cascaded in-
stance aware segmentation for arbitrary oriented word spot-
ting in the wild. In CVPR, pages 474–483, 2017. 2

[7] P. He, W. Huang, T. He, Q. Zhu, Y. Qiao, and X. Li. Single
shot text detector with regional attention. In ICCV, volume 6,
2017. 1, 2, 6

[8] T. He, Z. Tian, W. Huang, C. Shen, Y. Qiao, and C. Sun. An
end-to-end textspotter with explicit alignment and attention.
In CVPR, pages 5020–5029, 2018. 1, 2, 6, 7

[9] W. He, X.-Y. Zhang, F. Yin, and C.-L. Liu. Deep direct re-
gression for multi-oriented scene text detection. In CVPR,
pages 745–753, 2017. 1, 6

[10] H. Hu, C. Zhang, Y. Luo, Y. Wang, J. Han, and E. Ding.
Wordsup: Exploiting word annotations for character based
text detection. In ICCV, 2017. 1, 2, 5, 6

[11] Y. Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu,
and Z. Luo. R2cnn: rotational region cnn for orientation ro-
bust scene text detection. arXiv preprint arXiv:1706.09579,
2017. 1, 6

[12] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. Ghosh,
A. Bagdanov, M. Iwamura, J. Matas, L. Neumann, V. R.
Chandrasekhar, S. Lu, et al. Icdar 2015 competition on ro-
bust reading. In ICDAR, pages 1156–1160. IEEE, 2015. 1

[13] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Big-
orda, S. R. Mestre, J. Mas, D. F. Mota, J. A. Almazan, and
L. P. De Las Heras. Icdar 2013 robust reading competition.
In ICDAR, pages 1484–1493. IEEE, 2013. 1

[14] H. Kim, M. Kim, D. Seo, J. Kim, H. Park, S. Park, H. Jo,
K. Kim, Y. Yang, Y. Kim, et al. Nsml: Meet the mlaas
platform with a real-world case study.
arXiv preprint
arXiv:1810.09957, 2018. 7

[15] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015. 7

[16] M. Liao, B. Shi, and X. Bai. Textboxes++: A single-shot
oriented scene text detector. Image Processing, 27(8):3676–
3690, 2018. 1, 6

[17] M. Liao, B. Shi, X. Bai, X. Wang, and W. Liu. Textboxes: A
fast text detector with a single deep neural network. In AAAI,
pages 4161–4167, 2017. 2

[18] M. Liao, Z. Zhu, B. Shi, G.-s. Xia, and X. Bai. Rotation-
In

sensitive regression for oriented scene text detection.
CVPR, pages 5909–5918, 2018. 2, 6

[19] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
Fu, and A. C. Berg. Ssd: Single shot multibox detector. In
ECCV, pages 21–37. Springer, 2016. 2

[20] X. Liu, D. Liang, S. Yan, D. Chen, Y. Qiao, and J. Yan. Fots:
Fast oriented text spotting with a uniﬁed network. In CVPR,
pages 5676–5685, 2018. 1, 2, 6, 7

[21] Y. Liu and L. Jin. Deep matching prior network: Toward
tighter multi-oriented text detection. In CVPR, pages 3454–
3461, 2017. 2

[22] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, pages 3431–
3440, 2015. 2

[23] S. Long, J. Ruan, W. Zhang, X. He, W. Wu, and C. Yao.
Textsnake: A ﬂexible representation for detecting text of ar-
bitrary shapes. arXiv preprint arXiv:1807.01544, 2018. 1, 2,
6

[24] P. Lyu, M. Liao, C. Yao, W. Wu, and X. Bai. Mask textspot-
ter: An end-to-end trainable neural network for spotting text
with arbitrary shapes.
arXiv preprint arXiv:1807.02242,
2018. 1, 2, 6, 7

[25] P. Lyu, C. Yao, W. Wu, S. Yan, and X. Bai. Multi-oriented
scene text detection via corner localization and region seg-
mentation. In CVPR, pages 7553–7563, 2018. 1, 6

[26] J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust wide-
baseline stereo from maximally stable extremal regions. Im-
age and Vision Computing, 22(10):761–767, 2004. 2

[27] N. Nayef, F. Yin, I. Bizid, H. Choi, Y. Feng, D. Karatzas,
Z. Luo, U. Pal, C. Rigaud, J. Chazalon, et al. Icdar2017 ro-
bust reading challenge on multi-lingual scene text detection
and script identiﬁcation-rrc-mlt. In ICDAR, volume 1, pages
1454–1459. IEEE, 2017. 1

[28] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In ECCV, pages 483–499.
Springer, 2016. 3

[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: to-
wards real-time object detection with region proposal net-
works. PAMI, (6):1137–1149, 2017. 2

[30] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, pages 234–241. Springer, 2015. 3

[31] B. Shi, X. Bai, and S. Belongie. Detecting oriented text in
natural images by linking segments. In CVPR, pages 3482–
3490. IEEE, 2017. 1, 2, 5, 6

[32] A. Shrivastava, A. Gupta, and R. Girshick. Training region-
based object detectors with online hard example mining. In
CVPR, pages 761–769, 2016. 7

[33] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
3

[34] N. Sung, M. Kim, H. Jo, Y. Yang, J. Kim, L. Lausen, Y. Kim,
G. Lee, D. Kwak, J.-W. Ha, et al. Nsml: A machine learning
platform that enables you to focus on your models. arXiv
preprint arXiv:1712.05902, 2017. 7

9373

[35] L. Vincent and P. Soille. Watersheds in digital spaces: an
efﬁcient algorithm based on immersion simulations. PAMI,
(6):583–598, 1991. 4

[36] C. Yao, X. Bai, W. Liu, Y. Ma, and Z. Tu. Detecting texts
of arbitrary orientations in natural images. In CVPR, pages
1083–1090. IEEE, 2012. 2

[37] C. Yao, X. Bai, N. Sang, X. Zhou, S. Zhou, and Z. Cao.
Scene text detection via holistic, multi-channel prediction.
arXiv preprint arXiv:1606.09002, 2016. 2, 6

[38] L. Yuliang, J. Lianwen, Z. Shuaitao, and Z. Sheng. Detecting
curve text in the wild: New dataset and new solution. arXiv
preprint arXiv:1712.02170, 2017. 2, 6

[39] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, and X. Bai.
Multi-oriented text detection with fully convolutional net-
works. In CVPR, pages 4159–4167, 2016. 2, 6

[40] X. Zhou, C. Yao, H. Wen, Y. Wang, S. Zhou, W. He, and
J. Liang. East: an efﬁcient and accurate scene text detector.
In CVPR, pages 2642–2651, 2017. 1, 6

9374

