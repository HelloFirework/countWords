Deep Flow-Guided Video Inpainting

Rui Xu1 Xiaoxiao Li1 Bolei Zhou1 Chen Change Loy2

1 CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, 2 Nanyang Technological University

{xr018, bzhou@ie.cuhk.edu.hk

lxx1991@gmail.com

ccloy@ntu.edu.sg }

Abstract

Video inpainting, which aims at ﬁlling in missing re-
gions of a video, remains challenging due to the difﬁculty
of preserving the precise spatial and temporal coherence of
video contents. In this work we propose a novel ﬂow-guided
video inpainting approach. Rather than ﬁlling in the RGB
pixels of each frame directly, we consider video inpaint-
ing as a pixel propagation problem. We ﬁrst synthesize a
spatially and temporally coherent optical ﬂow ﬁeld across
video frames using a newly designed Deep Flow Comple-
tion network. Then the synthesized ﬂow ﬁeld is used to guide
the propagation of pixels to ﬁll up the missing regions in the
video. Speciﬁcally, the Deep Flow Completion network fol-
lows a coarse-to-ﬁne reﬁnement to complete the ﬂow ﬁelds,
while their quality is further improved by hard ﬂow exam-
ple mining. Following the guide of the completed ﬂow, the
missing video regions can be ﬁlled up precisely. Our method
is evaluated on DAVIS and YouTube-VOS datasets qualita-
tively and quantitatively, achieving the state-of-the-art per-
formance in terms of inpainting quality and speed. Codes
and models are available at https://github.com/
nbei/Deep-Flow-Guided-Video-Inpainting

1. Introduction

The goal of video inpainting is to ﬁll in missing regions
of a given video sequence with contents that are both spa-
tially and temporally coherent [4, 12, 22, 24]. Video in-
painting, also known as video completion, has many real-
world applications such as undesired object removal [9] and
video restoration [32].

Inpainting real-world high-deﬁnition video sequences
remains challenging due to the camera motion and the com-
plex movement of objects. Most existing video inpainting
algorithms [12, 21, 22, 27, 30] follow the traditional im-
age inpainting pipeline, by formulating the problem as a
patch-based optimization task, which ﬁlls missing regions
through sampling spatial or spatial-temporal patches of the
known regions then solve minimization problem. Despite
some good results, these approaches suffer from two draw-

backs. First, these methods typically assume smooth and
homogeneous motion ﬁeld in the missing region, therefore
they cannot handle videos with complex motions. A failure
case is shown in Fig. 1(b). Second, the computational com-
plexity of optimization-based methods is high thus those
methods are infeasible for the real-world applications. For
instance, the method by Huang et al. [12] requires approx-
imately 3 hours to inpaint a 854×480-sized video with 90
frames containing 18% missing regions.

Although signiﬁcant progress has been made in image
inpainting [15, 17, 23, 26, 35] through the use of Convo-
lutional Neural Network (CNN) [18], video inpainting us-
ing deep learning remains much less explored. There are
several challenges for extending deep learning-based image
inpainting approaches to the video domain. As shown in
Fig. 1(c), a direct application of an image inpainting algo-
rithm on each frame individually will lead to temporal arti-
facts and jitters. On the other hand, due to the large amount
of RGB frames, feeding the entire video sequence at once to
a 3D CNN is also difﬁcult to ensure the temporal coherence.
Meanwhile, an extremely large model capacity is needed to
directly inpaint the entire video sequence, which is not com-
putationally practical given its large memory consumption.

Rather than ﬁlling the RGB pixels, we propose an alter-
native ﬂow-guided approach for video inpainting. The mo-
tivation behind our approach is that completing a missing
ﬂow is much easier than ﬁlling in pixels of a missing region
directly, while using the ﬂow to propagate pixels tempo-
rally preserves the temporal coherence naturally. As shown
in Fig. 1(d), compared with RGB pixels, the optical ﬂow
is far less complex and easier to complete since the back-
ground and most objects in a scene typically have trackable
motion. This observation inspires us to design our method
to alleviate the difﬁculty of video inpainting by ﬁrst syn-
thesizing a coherent ﬂow ﬁeld across frames. Most pixels
in the missing regions can then be propagated and warped
from the visible regions. Finally we can ﬁll up the small
amount of regions that are not seen in the entire video using
the pixel hallucination [35].

In order to ﬁll up the optical ﬂows in videos, we design
a novel Deep Flow Completion Network (DFC-Net) with

3723

l
a
v
o
m
e
r
 
t
c
e
j
b
o
 
d
n
u
o
r
g
e
r
o
f

g
n
i
t
n
i
a
p
n

i
 

n
o
i
g
e
r
 
d
e
x
i
f

(a) missing region

(b) patch-based approach

(c) image inpainting

(d) Flow-guided Video Inpainting

Figure 1: In this example, we show two common inpainting settings, foreground object removal and ﬁxed region inpainting.
(a) Missing regions are shown in orange. (b) The result of patch-based optimization approach is affected by complex motions.
(c) The image inpainting approach is incapable of maintaining the temporal coherence. (d) Our approach considers the video
inpainting as a pixel propagation problem, in which the optical ﬂow ﬁeld is completed (shown on the left) and then the
synthesized ﬂow ﬁeld is used to guide the propagation of pixels to ﬁll up missing regions (shown on the right). Our inpainting
preserves the detail and video coherence.

the following technical novelties:
(1) Coarse-to-ﬁne reﬁnement: The proposed DFC-Net is
designed to recover accurate ﬂow ﬁeld from missing re-
gions. This is made possible through stacking three similar
subnetworks (DFC-S) to perform coarse-to-ﬁne ﬂow com-
pletion. Speciﬁcally, the ﬁrst subnetwork accepts a batch of
consecutive frames as the input and estimates the missing
ﬂow of the middle frame on a relatively coarse scale. The
batch of coarsely estimated ﬂow ﬁelds is subsequently fed
to the second subnetwork followed by the third subnetwork
for further spatial resolution and accuracy reﬁnement.
(2) Temporal coherence maintenance: Our DFC-Net is de-
signed to naturally encourage global temporal consistency
even though its subnetworks only predict a single frame
each time. This is achieved through feeding a batch of con-
secutive frames as inputs, which provide richer temporal in-
formation. In addition, the highly similar inputs between
adjacent frames tend to produce continuous results.
(3) Hard ﬂow example mining: We introduce hard ﬂow ex-
ample mining strategy to improve the inpainting quality on
ﬂow boundary and dynamic regions.

In summary, the main contribution of this work is a novel
ﬂow-guided video inpainting approach. We demonstrate
that compelling video completion in complex scenes can be
achieved via high-quality ﬂow completion and pixel prop-
agation . A Deep Flow Completion network is designed to
cope with arbitrary shape of missing regions, complex mo-
tions, and maintain temporal consistency.
In comparison

to previous methods, our approach is signiﬁcantly faster in
runtime speed, while it does not require any assumptions
about the missing regions and the motions of the video con-
tents. We show the effectiveness of our approach on both
the DAVIS [25] and YouTube-VOS [34] datasets with the
state-of-the-art performance.

2. Related Work

Non-learning-based Inpainting. Prior to the prevalence of
deep learning, most image inpainting approaches fall into
two categories, i.e., diffusion-based or patch-based meth-
ods, which both aim to ﬁll the target holes by borrowing
appearance information from known regions. A diffusion-
based method [1, 5, 19] propagates appearance informa-
tion around the target hole for image completion. This ap-
proach is incapable of handling the appearance variations
and ﬁlling large holes. A patch-based method [6, 8, 10, 29]
completes missing regions by sampling and pasting patches
from known regions or other source images. This kind
of approach has been extended to the temporal domain
for video inpainting [21, 22, 27]. Strobel et al. [30] and
Huang et al. [12] further estimate the motion ﬁeld in the
missing regions to address the temporal consistency prob-
lem.
In comparison to diffusion-based methods, patch-
based methods can better handle non-stationary visual data.
However, the dense computation of patch similarity is a
very time-consuming operation. Even by using the Patch-

3724

Match [2, 3] to accelerate the patch matching process, the
speed of [12] is still approximately 20 times slower than
our approach. Importantly, unlike our deep learning based
approach, all the aforementioned methods cannot capture
high-level semantic information. They thus fall short in re-
covering content in regions that encompasses complex and
dynamic motion from multiple objects.
Learning-based Inpainting. The emergence of deep learn-
ing inspires recent works to investigate various deep archi-
tectures for image inpainting. Earlier works [17, 26] at-
tempted to directly train a deep neural network for inpaint-
ing. With the advent of Generative Adversarial Networks
(GAN), some studies [15, 23, 35] formulate inpainting as
a conditional image generation problem. By using GAN,
Pathak et al. [23] train an inpainting network that can han-
dle large-sized holes. Iizuka et al. [15] improved [23] by
introducing both global and local discriminators for deriv-
ing the adversarial losses. More recently, Yu et al. [35] pre-
sented a contextual attention mechanism in a generative in-
painting framework, which further improves the inpainting
quality. These methods achieve excellent results in image
inpainting. Extending them directly to the video domain
is, however, challenging due to the lack of temporal con-
straints modeling. In this paper we formulate an effective
framework that is specially designed to exploit redundant
information across video frames. The notion of pixel propa-
gation through deeply estimated ﬂow ﬁelds is new in the lit-
erature. The proposed techniques, e.g., coarse-to-ﬁne ﬂow
completion, maintaining temporal coherence, and hard ﬂow
example mining are shown effective in the experiments, out-
performing existing optimization-based and deep learning-
based methods.

3. Methodology

Figure 2 depicts the pipeline of our ﬂow-guided video
inpainting approach. It contains two steps, the ﬁrst step is
to complete the missing ﬂow while the second step is to
propagate pixels with the guidance of completed ﬂow ﬁelds.
In the ﬁrst step, a Deep Flow Completion Network
(DFC-Net) is proposed for coarse-to-ﬁne ﬂow completion.
DFC-Net consists of three similar subnetworks named as
DFC-S. The ﬁrst subnetwork estimates the ﬂow in a rela-
tively coarse scale and feeds them into the second and third
subnetwork for further reﬁnement. In the second step, af-
ter the ﬂow is obtained, most of the missing regions can
be ﬁlled up by pixels in known regions through a ﬂow-
guided propagation from different frames. A conventional
image inpainting network [35] is ﬁnally employed to com-
plete the remaining regions that are not seen in the entire
video. Thanks to the high-quality estimated ﬂow in the ﬁrst
step, we can easily propagate these image inpainting results
to the entire video sequence.

Section 3.1 will introduce our basic ﬂow completion sub-

network DFC-S in detail. The stacked ﬂow completion net-
work, DFC-Net, is speciﬁed in Sec. 3.2. Finally, the RGB
pixel propagation procedure will be clariﬁed in Sec. 3.3.

3.1. Deep Flow Completion Subnetwork (DFC S)

Two types of inputs are provided to the ﬁrst DFC-S in our
network: (i) a concatenation of ﬂow maps from consecutive
frames, and (ii) the associated sequence of binary masks,
each of which indicating the missing regions of each ﬂow
map. The output of this DFC-S is the completed ﬂow ﬁeld
of the middle frame. In comparison to using a single ﬂow
map input, using a sequence of ﬂow maps and the corre-
sponding masks improves the accuracy of ﬂow completion
considerably.

i→(i+1), ..., f 0

More speciﬁcally, suppose f 0

(i−k)→(i−k+1), ..., f 0

i→(i+1) represents the ini-
tial ﬂow between i-th and (i + 1)-th frames and Mi→(i+1)
denotes the corresponding indicating mask. We ﬁrst ex-
tract the ﬂow ﬁeld using FlowNet 2.0 [16] and initialize
all holes in f 0
∗ by smoothly interpolating the known val-
ues at the boundary inward. To complete f 0
i→(i+1), the in-
put {f 0
(i+k)→(i+k+1)} and
{M(i−k), ..., Mi, ..., M(i+k)} are concatenated along the
channel dimension and then fed into the ﬁrst subnetwork,
where k denotes the length of consecutive frames. Gener-
ally, k = 5 is sufﬁcient for the model to acquire related
information and feeding more frames do not produce ap-
parent improvement. With this setting, the number of input
channels is 33 for the ﬁrst DFC-S (11 ﬂow maps each for
the x- and y-direction ﬂows, and 11 binary masks). For the
second and third DFC-S, inputs and outputs are different.
Their settings will be discussed in Sec. 3.2.

As shown in Fig. 2(a), considering the tradeoff between
model capacity and speed, DFC-S uses the ResNet-50 [11]
as the backbone. ResNet-50 consists of ﬁve blocks named
as ‘conv1’, ‘conv2 x’ to ‘conv5 x’. We modify the input
channel of the ﬁrst convolution in ‘conv1’ to ﬁt the shape
of our inputs (e.g., 33 in the ﬁrst DFC-S). To increase the
resolution of features, we decrease the convolutional strides
and replace convolutions by dilated convolutions from the
‘conv4 x’ to ‘conv5 x’ similar to [7]. An upsampling mod-
ule that is composed of three alternating convolution, relu
and upsampling layers are then appended to enlarge the pre-
diction. To project the prediction to the ﬂow ﬁeld, we re-
move the last activation function in the upsampling module.

3.2. Reﬁne Flow by Stacking

Figure 2(a) depicts the architecture of DFC-Net, which is
constructed by stacking three DFC-S. Typically, the smaller
the hole, the easier the missing ﬂow can be completed, so
we ﬁrst shrink the size of input frames of the ﬁrst subnet-
work to obtain good initial results. The frames are then
gradually enlarged in the second and third subnetwork to
capture more details, following a coarse-to-ﬁne reﬁnement

3725

{f 0, M}

DFC-S

R
e
s
N
e
t
-
5
0

u
p
s
a
m
p
l
e

{f 1}

f
o
r
w
a
r
d

b
a
c
k
w
a
r
d

{f 1, M}

DFC-S

{f 2}

f
o
r
w
a
r
d

b
a
c
k
w
a
r
d

{f 2, M}

DFC-S

{f 3}

Subnetwork 1

Subnetwork 2

Subnetwork 3

(a) Deep Flow Completion Network (DFC-Net)

forward propagation

Image Inpainting

bidirection merge

backward propagation

(1) Flow Guided Pixel Propagation

(2) Inpaint Unseen Regions in Video

known pixel

not connected to a known pixel

flow guided warping

(b) Flow Guided Frame Inpainting

Figure 2: The pipeline of our deep ﬂow-guided video inpainting approach. Best viewed with zoom-in.

paradigm. Compared with the original size, inputs for three
subnetworks are resized as 1/2, 2/3 and 1 respectively.

After obtaining the coarse ﬂow from the ﬁrst subnet-
work,
the second subnetwork focuses on further ﬂow
reﬁnement. To better align the ﬂow ﬁeld, the forward
and backward ﬂows are reﬁned jointly in the second
subnetwork. Suppose f 1 is the coarse ﬂow ﬁeld generated
by the ﬁrst subnetwork. For each pair of the consecutive
frames, i-th frame and (i + 1)-th frame, the second sub-
network takes a sequence of estimated bidirectional ﬂow
{f 1
(i+k)→(i+k+1)}
and
{f 1
(i+k)←(i+k+1)} as in-
put and produces reﬁned ﬂows {f 2
i←(i+1)}.
binary
Similar
masks
and
{M(i−k+1), ..., M(i+1), ..., M(i+k+1)} are also fed into the
second subnetwork to indicate masked regions of the ﬂow
ﬁeld. The second subnetwork shares the same architecture
as the ﬁrst subnetwork, however, the number of input and
output channels is different.

(i−k)→(i−k+1), ..., f 1
(i−k)←(i−k+1), ..., f 1

i→(i+1), ..., f 1
i←(i+1), ..., f 1

{M(i−k), ..., Mi, ..., M(i+k)}

i→(i+1), f 2

subnetwork,

ﬁrst

the

to

Finally, predictions from the second subnetwork are en-
larged and further fed into the third subnetwork, which
strictly follows the same procedure as the second subnet-
work to obtain the ﬁnal results. A step-by-step visualization
is provided in Fig. 3, the quality of the ﬂow ﬁeld is gradually
improved through the coarse-to-ﬁne reﬁnement.
Training. During training, for each video sequence, we
randomly generate the missing regions. The optimization
goal is to minimize the l1 distance between predictions and
ground-truth ﬂows. Three subnetworks are ﬁrst pre-trained
separately and then jointly ﬁne-tuned in end-to-end manner.
Speciﬁcally, the loss of the i-th subnetwork is deﬁned as:

missing region

ground truth

initial flow

stage-1

stage-2

stage-3

Figure 3: Visualization of different subnetworks outputs. The
quality of the completed ﬂows is improved over the coarse-to-ﬁne
reﬁnement. Best viewed with zoom-in.

Li =

kM ⊙ (f i − ˆf )k1

kM k1

,

(1)

where ˆf is the ground-truth ﬂow and ⊙ is element-wise
multiplication. For the joint ﬁne-tuning, the overall loss is a
linear combination of subnetwork losses.
Hard Flow Example Mining (HFEM). Because the ma-
jority of the ﬂow area is smooth in video sequences, there
exists a huge bias in the number of training samples be-
tween the smooth region and the boundary region. In our
experiments, we observe that directly using l1 loss generally
leads to the imbalanced problem, in which the training pro-
cess is dominated by smooth areas and the boundary region
in the prediction is blurred. What is worse, the incorrect
edge of ﬂow can lead to serious artifacts in the subsequent
propagation step.

To overcome this issue, inspired by [28], we leverage the
hard ﬂow example mining mechanism to automatically fo-
cus more on the difﬁcult areas thus to encourage the model

3726

hard region

w/o HFEM

w/ HFEM

Figure 4: Hard ﬂow example mining.

to produce sharp boundaries. Speciﬁcally, we sort all pixels
in a descending order of the loss. The top p percent pixels
are labeled as hard samples. Their losses are then enhanced
by a weight λ to enforce the model to pay more attention to
those regions. The l1 loss with hard ﬂow example mining is
deﬁned as:

Li =

kM ⊙ (f i − ˆf )k1

kM k1

+ λ ∗

kM h ⊙ (f i − ˆf )k1

kM hk1

,

(2)

where M h is the binary mask indicating the hard regions.
As shown in Fig. 4, the hard examples are mainly dis-
tributed around the high frequency regions such as the
boundaries. Thanks to the hard ﬂow example mining, the
model learns to focus on producing sharper boundaries.

3.3. Flow Guided Frame Inpainting

The optical ﬂow generated by DFC-Net establishes a
connection between pixels across frames, which could be
used as the guidance to inpaint missing regions by propa-
gation. Figure 2(b) illustrates the detailed process of ﬂow-
guided frame inpainting .
Flow Guided Pixel Propagation. As the estimated ﬂow
may be inaccurate in some locations, we ﬁrst need to check
the validity of the ﬂow. For a forward ﬂow f 3
i→(i+1) and
a location xi, we verify a simple condition based on pho-
tometric consistency: k(xi+1 + f 3
<
ǫ,, where xi+1 = xi + f 3
i→(i+1)(xi) and ǫ is a relatively
small threshold (i.e., 5). This condition means that after
the forward and backward propagation, the pixel should go
back to the original location. If it is not satisﬁed, we shall
believe that f 1
i→(i+1)(xi) is unreliable and ignore it in the
propagation. The backward ﬂow can be veriﬁed with the
same approach.

i←(i+1)(xi+1)) − xik
2

After the consistency check, as shown in Fig. 2(b)(1), all
known pixels are propagated bidirectionally to ﬁll the miss-
ing regions based on the valid estimated ﬂow. In particu-
lar, if an unknown pixel is connected with both forward and
backward known pixels, it will be ﬁlled by a linear com-
bination of their pixel values whose weights are inversely
proportional to the distance between the unknown pixel and
known pixels.
Inpaint Unseen Regions in Video.
In some cases, the
missing region cannot be ﬁlled by the known pixels tracked

by optical ﬂow (e.g., white regions in Fig. 2(b)(2)), which
means that the model fails to connect certain masked re-
gions to any pixels in other frames. The image inpaint-
ing technique [35] is employed to complete such unseen
regions. Figure 2(b)(2) illustrates the process of ﬁlling un-
seen regions. In practice, we pick the a frame with unﬁlled
regions in the video sequence and apply [35] to complete
it. The inpainting result is then propagated to the entire
video sequence based on the estimated optical ﬂow. A sin-
gle propagation may not ﬁll all missing regions, so image
inpainting and propagation steps are applied iteratively un-
til no more unﬁlled regions can be found. In average, for
a video with 12% missing regions, there are usually 1% of
unseen pixels and they can be ﬁlled after 1.1 iterations.

4. Experiments

Inpainting Settings. Two common inpainting settings are
considered in this paper. The ﬁrst setting aims to remove the
undesired foreground object, which has been explored in the
previous work [12, 22]. In this setting, a mask is given to
outline the region of the foreground object. In the second
setting, we want to ﬁll up an arbitrary region in the video,
which might contain either foreground or background. This
setting corresponds to some real-world applications such as
watermark removal and video restoration. To simulate this
situation, following [15, 35], a square region in the center of
video frames is marked as the missing region to ﬁll up. Un-
less otherwise indicated, for a video frame with size H ×W ,
we ﬁx the size of the square missing region as H/4 × W/4.
The non-foreground mask typically leads to inaccurate ﬂow
ﬁeld estimation, which makes this setting more challenging.
Datasets. To demonstrate the effectiveness and generaliza-
tion ability of the ﬂow-guided video inpainting approach,
we evaluate our method on DAVIS [25] and YouTube-
VOS [34] datasets. DAVIS dataset contains 150 high-
quality video sequences. A subset of 90 videos has all
frames annotated with the pixel-wise foreground object
masks, which is reserved for testing. For the remaining
60 unlabeled videos, we adopt them for training. Although
DAVIS is not originally proposed for the evaluation of video
inpainting algorithms, it is adopted here because of the pre-
cise object mask annotations. YouTube-VOS [34] consists
of 4,453 videos, which are split into 3,471 for training, 474
for validation and 508 for testing. Since YouTube-VOS
does not provide dense object mask annotations, we only
use it to evaluate the performance of the models in second
inpainting setting.
Data Preparation and Evaluation Metric.
FlowNet
2.0 [16] is used for ﬂow extraction. The data preparation
is different for the two inpainting settings as follows.
(1) Setting 1: foreground object removal. To prepare the
training set, we synthesize and overlay a mask of random
shape onto each frame of a video. Random motion is in-

3727

Table 1: Quantitative results for the ﬁxed region inpainting.

foreground object removal

fixed region inpainting

DAVIS

YouTube-VOS
PSNR SSIM PSNR SSIM
0.14
Deepﬁll [35]
16.68
Newson et al. [22] 23.92
0.43
0.44
26.48
Huang et al. [12]
27.49
0.48
Ours

16.47
24.72
27.39
28.26

0.15
0.37
0.39
0.41

100%

80%

60%

40%

20%

0%

time1(min.)

0.3

∼270
∼180

8.5

100%

80%

60%

40%

20%

0%

rank 1

rank 2

rank 3

rank 1

rank 2

rank 3

Ours

Huang et al.

Deepfill

troduced to simulate the actual object mask. Masked and
unmasked frames form the training pairs. For testing, since
the ground-truths of removed regions are not available, eval-
uations are thus conducted through a user study.
(2) Setting 2: ﬁxed region inpainting. Each of the training
frame is covered by a ﬁxed square region at the center of
the frame. Again, masked and unmasked frames form the
training pairs. For testing, besides the user study, we also
report the PSNR and SSIM following [20, 33] in this setting.
PSNR measures image’s distortion, while SSIM measures
the similarity in structure between the two images.

4.1. Main Results

We quantitatively and qualitatively compare our ap-
proach with other existing methods on DAVIS and
YouTube-VOS datasets. For YouTube-VOS, our model is
trained on its training set. The data in DAVIS dataset is in-
sufﬁcient for training a model from scratch. We thus use the
pretrained model from YouTube-VOS and ﬁne-tune it using
the DAVIS training set. The performances are reported on
their respective test set.
Quantitative Results. We ﬁrst make comparison with ex-
isting methods quantitatively on the second inpainting task
that aims to ﬁll up a ﬁxed missing region. The results are
summarized in Table 1.

Our approach achieves the best performance on both
datasets. As shown in Table 1, directly applying the image
inpainting algorithm [35] on each frame leads to inferior
results. Compared with conventional video inpainting ap-
proaches [12, 22], our approach could better handle videos
with complex motions. Meanwhile, our approach is signiﬁ-
cantly faster in runtime speed and thus it is more well-suited
for real-world applications.
User study. Evaluation metrics in terms of reconstruction
errors are not perfect as there are many reasonable solu-
tions for the original video frames. Therefore, we perform
a user study to quantify the performance of our approach
and existing works [12, 35] for their inpainting quality. We
use the models trained on DAVIS dataset for this experi-
ment. Speciﬁcally, we randomly choose 15 videos from
DAVIS testing set for each participant. The videos are

1Following [12], we report the running time on the “CAMEL” video in
DAVIS dataset. While Newson et al. [22] have not reported the execution
time in the paper, we use the similar environment with [12] to test their
execution time.

Figure 5: User study. “Rank x” means the percentage of inpaint-
ing results from each approach being chosen as the x-th best.

then inpainted by three approaches (ours, Deepﬁll [35], and
Huang et al. [12]) under two different settings. To better
display the details, the video is played at a low frame rate
(5 FPS). For each video sample, participants are requested
to rank the three inpainting results after the video is played.
We invited 30 participants for the user study. The result
is summarized in Fig. 5, which is consistent with the quan-
titative result. Our approach signiﬁcantly outperforms the
other two baselines, while the image inpainting method per-
forms the worst since it is not designed to maintain temporal
consistency on its output. Figure 6 shows some examples of
our inpainting results2.
Qualitative Comparison.
In Fig. 7, we compare our
method with Huang et al.’s method in two different settings.
From the ﬁrst case, it is evident that our DFC-Net can bet-
ter complete the ﬂow. Thanks to the completed ﬂow, the
model can easily ﬁll up the region with correct pixel value.
In the more challenging case shown in the second example,
our method is much more robust on inpainting the complex
masked region such as the part of a woman, compared to
the notable artifacts in Huang et al.’s result.

4.2. Ablation Study

In this section, we conduct a series of ablation studies
to analyze the effectiveness of each component in our ﬂow-
guided video inpainting approach. Unless otherwise indi-
cated we employ the training set of YouTube-VOS for train-
ing. For better quantitative comparison, all performances
are reported on the validation set of YouTube-VOS under
the second inpainting setting, since we have the ground-
truth of the removed regions under this setting.
Comparison with Image Inpainting Approach. Our ﬂow-
guided video inpainting approach signiﬁcantly eases the
task of video inpainting by using the synthesized ﬂow ﬁelds
as a guidance, which transforms the video completion prob-
lem into a pixel propagation task. To demonstrate the ef-
fectiveness of this paradigm, we compare it with a direct
image inpainting network for each individual frame. For
a fair comparison, we adopt the Deepﬁll architecture but
with multiple color frames as input, which is named as
‘Deepﬁll+Multi-Frame’. Then the ‘Deepﬁll+Multi-Pass’

2We highly recommend watching the video demo in https://

youtu.be/zqZjhFxxxus

3728

Figure 6: Results of our ﬂow-guided video inpainting approach. For each input sequence (odd row), we show representative frames with
mask of missing region overlay. We show the inpainting results in even rows. Best viewed with zoom-in.

missing region

Huang et al.

Ours

Figure 7: Comparison with Huang et al.

architecture stacks three ‘Deepﬁll+Multi-Frame’ like DFC-
Net.
Table 2 presents the inpainting results on both
DAVIS and YouTube-VOS. Although the multi-frame in-
put and stacking architecture can bring marginal improve-
ments compared to Deepﬁll. The signiﬁcant gap between
‘Deepﬁll+Multi-Frame’ and our method demonstrates that
using the high-quality completed ﬂow ﬁeld as guidance can
ease the task of video inpainting.
Effectiveness of Hard Flow Example Mining. As intro-
duced in Sec. 3.2, most of the area of optical ﬂow is smooth
and that may result in degenerate models. Therefore, a hard
ﬂow example mining mechanism is proposed to mitigate the
inﬂuence of the label bias in the problem of ﬂow inpainting.
Similarly, in this experiment, we adopt the ﬁrst DFC-S to
examine the effectiveness of hard ﬂow example mining

Table 3 lists the ﬂow completion accuracy under differ-
ent mining settings, as well as the corresponding inpainting
performance. The parameter p represents the percentage of
samples that are labeled as the hard one. We use the stan-
dard end-point-error (EPE) metric to evaluate our inpainted
ﬂow. For clear demonstration, all ﬂow samples are divided
into smooth and non-smooth sets according to their vari-
ance. Overall, the hard ﬂow example mining mechanism
improves the performance under all settings. When p is
smaller, which means samples are harder, it will increase

Table 2: Quantitative results for the ﬁxed region inpaint-
ing. “Deepﬁll+Multi-Frame” uses Deepﬁll architecture but
with multiple frames as input. “Deepﬁll+Multi-Pass” stacks
three “Deepﬁll+Multi-Frame” networks.

DAVIS

YouTube-VOS
PSNR SSIM PSNR SSIM
0.14
Deepﬁll
16.68
0.15
Deepﬁll+Multi-Frame 16.71
0.17
17.02
Deepﬁll+Multi-Pass
27.49
0.48
Ours

16.47
16.55
16.94
28.26

0.15
0.15
0.16
0.41

Table 3: Ablation study on hard ﬂow example mining.

p (%)

w/o HFEM
70
50
30
10

Flow completion (EPE)

Video inpainting

smooth region hard region overall PSNR SSIM

0.13
0.13
0.13
0.13
0.13

1.17
1.13
1.04
1.04
1.08

1.03
1.01
0.99
0.99
1.00

24.43
24.63
26.15
26.15
25.92

0.36
0.36
0.37
0.37
0.37

the difﬁculty during training. However, if p is larger, the
model would not get much improvement compared with the
baseline. The best choice of p ranges from 30% to 50%. In
our experiments, we ﬁx p as 50%.
Effectiveness of Stacked Architecture. Table 4 depicts the

3729

Table 4: Ablation study on stacked architecture.

Flow completion Video inpainting

(EPE)

PSNR SSIM

Region-Fill
Stage-1
Stage-2
DFC-Single
DFC-Net (w/o MS)
DFC-Net (Stage-3)

1.07
0.99
0.94
0.97
0.95
0.93

23.85
26.15
27.10
26.58
27.02
27.50

0.35
0.37
0.38
0.37
0.40
0.41

Table 5: Ablation study on ﬂow-guided pixel propagation.

PSNR SSIM

w/o pixel propagation
w/ pixel propagation

19.43
27.50

0.24
0.41

Table 6: Ablation study on the quality of the initial ﬂow on
DAVIS.

EPE

PSNR SSIM

Huang et al. w/o Flownet2
Huang et al. w/ FlowNet2
ours

–

1.02
0.93

27.39
27.73
28.26

0.44
0.45
0.48

step-by-step reﬁnement results of DFC-Net, including ﬂows
and the corresponding inpainting frames. To further demon-
strate the effectiveness of stacked DFC-Net, Table 4 also in-
cludes two other baselines that are constructed as follows:

• DFC-Single: DFC-Single is a single stage ﬂow completion
network that is similar to DFC-S. To ensure a fair compari-
son, DFC-Single adopts a deeper backbone, i.e. ResNet-101.
• DFC-Net (w/o MS): The architecture of DFC-Net (w/o MS)
is the same as DFC-Net. However, in each stage of this base-
line model, the input’s scale does not change and the data is
full resolution from the start to the end.

By inspecting Table 4 closer, we could ﬁnd that the end-
point-error is gradually reduced by the coarse-to-ﬁne reﬁne-
ment. The result of DFC-Single is somewhat inferior to
the second stage, which suggests the effectiveness of us-
ing the stacked architecture in this task. To further indicate
the effectiveness of using multi-scale input in each stage,
we compare our DFC-Net with DFC-Net (w/o MS). The
performance gap veriﬁes that the strategy of using multi-
scale input in each stage improves the result of our model
since using the large scale’s input in the early stage typically
causes the instability of training.
Effectiveness of Flow-Guided Pixel Propagation. Af-
ter obtaining the completed ﬂow, all known pixels are ﬁrst
propagated bidirectionally to ﬁll the missing regions based
on the valid estimated ﬂow. This step produces high-quality
results and also reduces the size of missing regions that have
to be handled in the subsequent step.

As shown in Table 5, compared with a baseline approach
that directly use the image inpainting and ﬂow warping to
inpaint unseen regions, this intermediate step greatly eases
the task and improves the overall performance.

Missing Region

Huang et al+FlowNet2

Ours

Figure 8: Comparison of completed ﬂow between Huang et al.
and ours.

Figure 9: A failure case. The input is shown in the ﬁrst row, and
the output is shown in the second row.

Ablation Study on Initial Flow. The ﬂow estimation al-
gorithm is important but not vital since it only affects the
ﬂow quality outside the missing regions. By contrast, the
quality of the completed ﬂow inside the missing regions is
more crucial. We substitute the initial ﬂow of [12] with ﬂow
estimated by FlowNet2 to ensure a fair comparison. Table 6
and Fig. 8 demonstrate the effectiveness of our method.
Failure Case. A failure case is shown in Fig. 9. Our method
failed in this case mainly because the completed ﬂow is in-
accurate on the edge of the car. The propagation process
cannot amend that. In the future, we will use the learning
based propagation method to mitigate the inﬂuence of the
inaccuracy of the estimated ﬂow. Other more contemporary
ﬂow estimation methods [13, 14, 31] will be investigated
too.

5. Conclusion

We propose a novel deep ﬂow-guided video inpainting
approach, showing that high-quality ﬂow completion could
largely facilitate inpainting videos in complex scenes. Deep
Flow Completion network is designed to cope with arbi-
trary missing regions, complex motions, and yet maintain
temporal consistency. In comparison to previous methods,
our approach is signiﬁcantly faster in runtime speed, while
it does not require any assumption about the missing regions
and the movements of the video contents. We show the ef-
fectiveness of our approach on both the DAVIS [25] and
YouTube-VOS [34] datasets with the state-of-the-art perfor-
mance.

Acknowledgements. This work is supported by Sense-
Time Group Limited, the General Research Fund sponsored
by the Research Grants Council of the Hong Kong SAR
(CUHK 14241716, 14224316. 14209217), and Singapore
MOE AcRF Tier 1 (M4012082.020).

3730

References

[1] C. Ballester, M. Bertalmio, V. Caselles, G. Sapiro, and
J. Verdera. Filling-in by joint interpolation of vector ﬁelds
and gray levels.
IEEE Transactions on Image Processing,
10(8):1200–1211, 2001. 2

[2] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Gold-
man. Patchmatch: A randomized correspondence algorithm
for structural image editing. ACM Transactions on Graphics
(ToG), 28(3):24, 2009. 3

[3] C. Barnes, E. Shechtman, D. B. Goldman, and A. Finkel-
stein. The generalized patchmatch correspondence algo-
rithm. In European Conference on Computer Vision, pages
29–43. Springer, 2010. 3

[4] M. Bertalmio, A. L. Bertozzi, and G. Sapiro. Navier-stokes,
ﬂuid dynamics, and image and video inpainting.
In IEEE
Conference on Computer Vision and Pattern Recognition,
volume 1, pages I–I. IEEE, 2001. 1

[5] M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester. Image
inpainting. In Proceedings of the 27th annual conference on
Computer graphics and interactive techniques, pages 417–
424. ACM Press/Addison-Wesley Publishing Co., 2000. 2

[6] M. Bertalmio, L. Vese, G. Sapiro, and S. Osher. Simultane-
ous structure and texture image inpainting. IEEE Transac-
tions on Image Processing, 12(8):882–889, 2003. 2

[7] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
volutional nets and fully connected crfs. 2014. 3

[8] S. Darabi, E. Shechtman, C. Barnes, D. B. Goldman, and
P. Sen. Image melding: Combining inconsistent images us-
ing patch-based synthesis. ACM Transactions on Graphics
(ToG), 31(4):82–1, 2012. 2

[9] M. Ebdelli, O. Le Meur, and C. Guillemot. Video inpainting
with short-term windows: application to object removal and
error concealment. IEEE Transactions on Image Processing,
24(10):3034–3047, 2015. 1

[10] A. A. Efros and W. T. Freeman.

Image quilting for tex-
ture synthesis and transfer. In Proceedings of the 28th an-
nual conference on Computer graphics and interactive tech-
niques, pages 341–346. ACM, 2001. 2

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer

for image recognition.
Vision and Pattern Recognition, pages 770–778, 2016. 3

[12] J.-B. Huang, S. B. Kang, N. Ahuja, and J. Kopf. Temporally
coherent completion of dynamic video. ACM Transactions
on Graphics (TOG), 35(6):196, 2016. 1, 2, 3, 5, 6, 8

[13] T.-W. Hui, X. Tang, and C. C. Loy. LiteFlowNet: A
lightweight convolutional neural network for optical ﬂow es-
timation. In IEEE Conference on Computer Vision and Pat-
tern Recognition, 2018. 8

[14] T.-W. Hui, X. Tang, and C. C. Loy. A lightweight optical
ﬂow CNN–revisiting data ﬁdelity and regularization. arXiv
preprint arXiv:1903.07414, 2019. 8

[15] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Globally and
locally consistent image completion. ACM Transactions on
Graphics (TOG), 36(4):107, 2017. 1, 3, 5

[16] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. Flownet 2.0: Evolution of optical ﬂow estimation

with deep networks. In IEEE Conference on Computer Vi-
sion and Pattern Recognition, volume 2, page 6, 2017. 3,
5

[17] R. K¨ohler, C. Schuler, B. Sch¨olkopf, and S. Harmeling.
Mask-speciﬁc inpainting with deep neural networks. In Ger-
man Conference on Pattern Recognition, pages 523–534.
Springer, 2014. 1, 3

[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems, pages
1097–1105, 2012. 1

[19] A. Levin, A. Zomet, and Y. Weiss. Learning how to inpaint
from global image statistics. In IEEE International Confer-
ence on Computer Vision, page 305. IEEE, 2003. 2

[20] G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and
B. Catanzaro. Image inpainting for irregular holes using par-
tial convolutions. 2018. 6

[21] A. Newson, A. Almansa, M. Fradet, Y. Gousseau, and
P. P´erez. Towards fast, generic video inpainting.
In Pro-
ceedings of the 10th European Conference on Visual Media
Production, page 7. ACM, 2013. 1, 2

[22] A. Newson, A. Almansa, M. Fradet, Y. Gousseau, and
P. P´erez. Video inpainting of complex scenes. SIAM Journal
on Imaging Sciences, 7(4):1993–2019, 2014. 1, 2, 5, 6

[23] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.
Efros. Context encoders: Feature learning by inpainting. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2536–2544, 2016. 1, 3

[24] K. A. Patwardhan, G. Sapiro, and M. Bertalm´ıo. Video in-
painting under constrained camera motion. IEEE Transac-
tions on Image Processing, 16(2):545–553, 2007. 1

[25] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,
M. Gross, and A. Sorkine-Hornung. A benchmark dataset
and evaluation methodology for video object segmentation.
In IEEE Conference on Computer Vision and Pattern Recog-
nition, 2016. 2, 5, 8

[26] J. S. Ren, L. Xu, Q. Yan, and W. Sun. Shepard convolu-
tional neural networks. In Advances in Neural Information
Processing Systems, pages 901–909, 2015. 1, 3

[27] T. K. Shih, N. C. Tang, and J.-N. Hwang. Exemplar-based
video inpainting without ghost shadow artifacts by maintain-
ing temporal continuity. IEEE transactions on circuits and
systems for video technology, 19(3):347–360, 2009. 1, 2

[28] A. Shrivastava, A. Gupta, and R. Girshick. Training region-
based object detectors with online hard example mining. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 761–769, 2016. 4

[29] D. Simakov, Y. Caspi, E. Shechtman, and M. Irani. Sum-
marizing visual data using bidirectional similarity. In IEEE
Conference on Computer Vision and Pattern Recognition.
IEEE, 2008. 2

[30] M. Strobel, J. Diebold, and D. Cremers. Flow and color in-
In German Conference on

painting for video completion.
Pattern Recognition, pages 293–304. Springer, 2014. 1, 2

[31] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. PWC-Net: CNNs
for optical ﬂow using pyramid, warping, and cost volume. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2018. 8

3731

[32] N. C. Tang, C.-T. Hsu, C.-W. Su, T. K. Shih, H.-Y. M. Liao,
et al. Video inpainting on digitized vintage ﬁlms via main-
taining spatiotemporal continuity.
IEEE Transactions on
Multimedia, 13(4):602–614, 2011. 1

[33] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
from error visibility to struc-
IEEE Transactions on Image Processing,

Image quality assessment:
tural similarity.
13(4):600–612, 2004. 6

[34] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and
T. Huang. Youtube-vos: A large-scale video object segmen-
tation benchmark. arXiv preprint arXiv:1809.03327, 2018.
2, 5, 8

[35] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang.
Generative image inpainting with contextual attention.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2018. 1, 3, 5, 6

3732

