Quantization Networks

Jiwei Yang1

,

2∗, Xu Shen2, Jun Xing3, Xinmei Tian1†, Houqiang Li1,

Bing Deng2, Jianqiang Huang2, Xian-sheng Hua2‡

1 Department of Electronic Engineering and Information Science,

University of Science and Technology of China

2Damo Academy, Alibaba Group

3Institute for Creative Technologies, University of Southern California

yjiwei@mail.ustc.edu.cn, junxnui@gmail.com, {xinmei,lihq}@ustc.edu.cn,

{shenxu.sx,dengbing.db,jianqiang.hjq,xiansheng.hxs}@alibaba-inc.com

Abstract

Although deep neural networks are highly effective, their
high computational and memory costs severely hinder their
applications to portable devices. As a consequence, low-
bit quantization, which converts a full-precision neural net-
work into a low-bitwidth integer version, has been an active
and promising research topic. Existing methods formulate
the low-bit quantization of networks as an approximation or
optimization problem. Approximation-based methods con-
front the gradient mismatch problem, while optimization-
based methods are only suitable for quantizing weights and
can introduce high computational cost during the training
stage. In this paper, we provide a simple and uniform way
for weights and activations quantization by formulating it
as a differentiable non-linear function. The quantization
function is represented as a linear combination of several
Sigmoid functions with learnable biases and scales that
could be learned in a lossless and end-to-end manner via
continuous relaxation of the steepness of Sigmoid functions.
Extensive experiments on image classiﬁcation and object
detection tasks show that our quantization networks outper-
form state-of-the-art methods. We believe that the proposed
method will shed new lights on the interpretation of neural
network quantization.

1. Introduction

Although deep neural networks (DNNs) have achieved
huge success in various domains, their high computational
and memory costs prohibit their deployment in scenarios

∗This work was done when the author was visiting Alibaba as a re-

search intern.

†Corresponding author.
‡Corresponding author.

(a) Sigmoid

(b) ReLU

(c) Maxout

(d) Quantization

Figure 1: Non-linear functions used in neural networks.

where both computational and storage resources are limited.
Thus, the democratization of deep learning hinges on the
development of efﬁcient DNNs. Various techniques have
been proposed to lighten DNNs by either reducing the num-
ber of weights and connections or by quantizing the weights
and activations to lower bits. As exempliﬁed by ResNet
[8], SqueezeNet [13] and MobileNet [11], numerous efforts
have been devoted to designing networks with compact lay-
ers and architectures. Once trained, these networks can be
further compressed with techniques such as network prun-
ing [7], weight sharing [3] or matrix factorization [16].

Approaches for quantizing full-precision networks into
low-bit networks can be roughly divided into two cate-

17308

gories: approximation-based and optimization-based ap-
proaches. Methods in the ﬁrst category approximate the
full-precision (32-bit) values with discrete low-bit (e.g., bi-
nary) values via step functions in the forward pass [27, 30,
33, 19, 34, 15, 21, 22, 1]. Because the gradients of such
approximations are saturated, additional approximations in
the backward process are needed. As a consequence, the
use of different forward and backward approximations re-
sults in a gradient mismatch problem, which makes the op-
timization unstable. To avoid the approximation of gradi-
ents, some methods formulate the quantization of neural
networks as a discretely constrained optimization problem,
where the losses of the networks are incorporated [20, 10].
Unfortunately, optimization-based methods are only suit-
able for the quantization of weights. Moreover, the iterative
solution of the optimization problem suffers from a high
computational complexity during training.

Intuitively, if we can formulate the quantization opera-
tion as a simple non-linear function similar to common acti-
vation functions (e.g., Sigmoid [17], ReLU [25] and Maxout
[6]), no approximation of gradients would be needed, and
the quantization of any learnable parameters in the DNNs,
including activations and weights, can be learned straight-
forwardly and efﬁciently.
Inspired by this, we present a
novel perspective for interpreting and implementing quan-
tization in neural networks. Speciﬁcally, we formulate
quantization as a differentiable non-linear mapping func-
tion, termed the quantization function. As shown in Fig. 1,
the quantization function is formed as a linear combina-
tion of several Sigmoid functions with learnable biases and
scales. In this way, the proposed quantization function can
be learned in a lossless and end-to-end manner and works
for any weights and activations in neural networks, thereby
avoiding the gradient mismatch problem. As illustrated in
Fig. 2, the quantization function can be trained via continu-
ous relaxation of the steepness of the Sigmoid functions.

Our main contributions are summarized as follows:

• In contrast to existing low-bit quantization methods,
we are the ﬁrst to formulate quantization as a differen-
tiable non-linear mapping function, thereby providing
a simple/straightforward and general/uniform solution
for any-bit weight and activation quantization without
suffering the severe gradient mismatch problem.

• We implement a simple and effective form of quanti-
zation networks that can be learned in a lossless and
end-to-end manner and that outperforms state-of-the-
art quantization methods on both image classiﬁcation
and object detection tasks.

2. Related Work

In this paper, we propose formulating the quantization
In this

operation as a differentiable non-linear function.

section, we give a brief review of both low-bit quantization
methods and non-linear functions used in neural networks.

2.1. Low Bit Quantization of Neural Networks

Approaches for quantizing full-precision networks into
low-bit networks can be roughly divided into two cate-
gories: approximation-based and optimization-based ap-
proaches. The ﬁrst approach is to approximate the 32-
bit full-precision values with discrete low-bit values in the
forward pass of the networks. BinaryConnect [4] directly
optimizes the loss of the network with weights W re-
placed by sign(W ), and it approximates the sign function
with the “hard tanh” function in the backward process to
avoid the zero-gradient problem. The binary weight net-
work (BWN) [27] adds scale factors for the weights during
binarization. Ternary weight network (TWN) [21] intro-
duces ternary weights and achieves improved performance.
Trained ternary quantization (TTQ) [34] proposes learning
both ternary values and scaled gradients for 32-bit weights.
DoReFa-Net [33] proposes quantizing 32-bit weights, ac-
tivations and gradients using different bit widths. Gradi-
ents are approximated by a customized form based on the
mean of the absolute values of the full-precision weights.
In [30], weights, activations, gradients and errors are all
approximated by low-bitwidth integers based on rounding
and shifting operations. Jacob et al. [15] propose an afﬁne
mapping of integers to real numbers that allows inference to
be performed using integer-only arithmetic. As discussed
before, approximation-based methods use different forward
and backward approximations, which causes a gradient mis-
match problem. Friesen and Domingos [5] observe that
setting targets for hard-threshold hidden units to minimize
losses is a discrete optimization problem. Zhuang et al. [35]
propose a two-stage approach to quantize the weights and
activations in a two-step manner. Lin et al. [23] approx-
imate full-precision weights with a linear combination of
multiple binary weight bases. Zhang et al. [31] propose a
ﬂexible non-uniform quantization method to quantize both
network weights and activations. Cai et al. [1] use several
piece-wise backward approximators to overcome the gradi-
ent mismatch problem. Zhou et al.
[32] propose a step-
by-step decoupling operation to efﬁciently convert a pre-
trained full-precision convolutional neural network (CNN)
model into a low-precision version. As a speciﬁc quanti-
zation, HashNet [2] adopts a similar continuous relaxation
to train the hash function, where a single tanh function is
used for binarization. However, our training case (multi-
bit quantization of both activations and weights in multiple
layers) is substantially more complicated and challenging.

To

the

avoid

gradient

problem,
optimization-based quantization methods have recently
been proposed.
Such methods directly formulate the
quantization of neural networks as a discretely constrained

approximation

7309

optimization problem [20, 10]. Leng et al. [20] introduce
convex linear constraints for the weights and solve the
problem by the alternating direction method of multipliers
(ADMM). Hou and Kwok [10] directly optimize the loss
function w.r.t.
the ternarized weights using a proximal
Newton algorithm. However,
these methods are only
suitable for the quantization of weights and such iterative
solutions suffer from high computational costs for training.

2.2. Non Linear Functions in Neural Networks

In neural networks, the design of hidden units is distin-
guished by the choice of the non-linear activation function
g(x) for hidden units [12]. The simplest form of a neural
network is the perceptron [28], where a unit step function is
introduced to produce a binary output:

g(x) = A(x) =(cid:26) 1 x ≥ 0,

0 x < 0.

(1)

This form is similar to the binary quantization operation,
i.e., discretize the continuous inputs into binary values.
However, the problem is that it is not immediately obvious
how to learn the perceptron networks [26].

To solve this problem, the sigmoid activation function is

adopted in the early form of feedforward neural networks:

g(x) = σ(x) =

1

1 + exp(−x)

,

(2)

which has smooth and non-zero gradients everywhere
so that the sigmoid neurons can be learned via back-
propagation. When the absolute value of x is very large,
the output of a sigmoid function is close to a unit step func-
tion.

Currently, rectiﬁed linear units (ReLU) are more fre-
quently used as the activation functions in deep neural net-
works, and a generalization of ReLU is Maxout:

g(x) = max

j

(aj ∗ x + cj), j = 1, . . . , k

(3)

where {aj} and {cj} are learned parameters. The form of
Maxout indicates that a complex convex function can be ap-
proximated by a combination of k simple linear functions.
We adopt a similar idea to formulate the quantization func-
tion.

3. Quantization Networks

The main idea of this work is to formulate the quantiza-
tion operation as a differentiable non-linear function, which
can be applied to any weights and activations in deep neu-
ral networks. We ﬁrst present our novel interpretation of
quantization from the perspective of non-linear functions,
followed by the learning of the quantization networks.

3.1. Reformulation of Quantization

The quantization operation maps continuous inputs into
discrete integer numbers, and a binary quantization opera-
tion can be seen as an unit step function. Inspired by the
design of Maxout units (Eq. 3), quantizing continuous val-
ues into a set of integer numbers can be formulated as a
combination of several binary quantizations. That said, the
ideal low-bit quantization function could be represented as
a combination of several unit step functions with speciﬁed
biases and scales, as shown in Fig. 2(e):

y =

nXi=1

siA(βx − bi) − o,

(4)

where x is the full-precision weight/activation to be
quantized, and y is the quantized integer set Y (e.g.,
{−4, −2, −1, 0, 1, 2, 4}) with n + 1 (e.g., 7) quantization
intervals. A is the standard unit step function, with β be-
ing the overall scale factor of inputs, si and bi being the
scale and bias of each unit step function. In particular, β
and bi are parameters to be learned, and si is calculated by
si = Yi+1 − Yi. The global offset o = 1
i=1 si keeps the
quantized output czero-centered. Once the target quantiza-
tion integer set Y is given, n = |Y| − 1, si and the offset o
can be directly obtained.

2Pn

3.2. Training and Inference with Quantization Net 

works

Since the ideal step function is not smooth, we adopt a
continuous relaxation method to train it [2], by replacing
each unit step function in the ideal quantization function
(Eq. 4) with a sigmoid function. Such ”soft” quantization
function is differentiable (Fig. 2(c)), and can be learned in
an end-to-end manner via back-propagation without suffer-
ing the gradient mismatching.

Since the ideal quantization function (Eq.4) is ﬁnally ap-
plied in the inference stage, we gradually narrow the gap
between the ideal and soft quantization functions during the
training stage. Motivated by the distilling idea in [9], we
introduce a temperature factor T to the Sigmoid function:

σ(T x) =

1

1 + exp(−T x)

.

(5)

When the value of T gets larger, the gap between two quan-
tization functions becomes smaller, but the learning capac-
ity of the quantization networks is also lower due to the sat-
urated gradient. Thus, during the training stage, we start
with a small T to ensure a stable and effective learning, and
gradually increase T w.r.t.
the training epochs to ﬁnally
approach the ideal quantization functions, as illustrated in
Fig. 2.

Forward Propagation.

For a set of full-precision
weights or activations that need to be quantized X =

7310

(a) No Quantization

(b) T=1

(c) T=11

(d) T=121

(e) Complete Quantization

Figure 2: The relaxation process of a quantization function during training, which goes from a straight line to steps as the
temperature T increases.

{xd, d = 1, · · · , D}, the quantization function is applied
to each xd independently:

yd = Q(xd) = α(

nXi=1

siσ(T (βxd − bi)) − o),

(6)

where β and α are the scale factors of the input and output,
respectively; bi is the beginning of i-th quantization inter-
val, and particularly, b0 is set to −∞.

Like any other non-linear activation functions, Eq. (6)
provides a simple and uniform quantization operation, with-
out introducing any changes to the original network struc-
ture.

Backward Propagation. During the training stage, we
need to back-propagate the gradients of the loss ℓ through
the quantization function, as well as compute the gradients
with respect to the involved parameters:

∂ℓ
∂yd

·

nXi=1

T β
αsi

gi
d(αsi − gi

d),

∂ℓ
∂xd

∂ℓ
∂α

∂ℓ
∂β

∂ℓ
∂bi

=

=

=

=

DXd=1
DXd=1
DXd=1

∂ℓ
∂yd

∂ℓ
∂yd

∂ℓ
∂yd

·

·

·

1
α

yd,

nXi=1

−T
αsi

T xd
αsi

gi
d(αsi − gi

d),

gi
d(αsi − gi

d).

(10)

(7)

(8)

(9)

Algorithm 1 Training quantization networks

(or other parameters) {Θ(m)}M

Input: Network N with M modules MM
corresponding activations/inputs {X (m)}M
able weights
and
{Y (m)
Output: Quantized network for inference, N inf
N tr
for m ← 1 to M do

Q ← N // Training quantization network

corresponding
m=1.

X , Y (m)

Θ }M

quantized

target

Q

m=1,

m=1 and their
train-
m=1,
set

integers

i

X , Y (m)

Θ , X (m), Θ(m)} .

, o(m)} and intialize {α(m), β(m), b(m)
}
i
X , Q(m)
Θ }

Infer {s(m)
(Eq. 6) in the soft quantization function {Q(m)
based on {Y (m)
Apply the soft quantization function to each element
d in X (m) and each element θm
xm
d = Q(m)
ym
d = Q(m)
Forward propagate module m with the quantized
weights and activations.

d in Θ(m):

Θ ,b(m)
Θ }

bθm

Θ ,β(m)

X ,b(m)
X }

X ,β(m)

{α(m)

{α(m)

(xm

(θm

d ).

d ),

end for
for epoch ← 1 to M ax Epochs do

N tr
Train
to
Q
Θ ∪ {α(m)
Θ , β(m)
gradually increased temperature T

optimize
Θ , α(m)

Θ , b(m)

X , β(m)

X , b(m)
X }

the

parameters
M
m=1 with

Q ← N tr

end for
N inf
frozen parameters
for m ← 1 to M do

Q // Inference quantization network with

where gi
d = σ(T (βxd − bi)), and the gradients of n, si
and the offset o can be directly obtained by Y. Our soft
quantization function is a differentiable transformation that
introduces quantized weights and activations into the net-
work.

Training and Inference. To quantize a network, we
specify a set of weights or activations and insert the quanti-
zation function for each of them according to Eq. (6). For
example, a layer that previously receives x as an input be-
comes Q(x), and a module that previously uses W as pa-
rameters now becomes Q(W ). Once the network has been

Replace the soft quantization functions with Eq. (11)
for inference.

end for

trained, we replace the sigmoid function in Eq. (6) with the
unit step function for inference:

y = α(

nXi=1

siA(βx − bi) − o).

(11)

Algorithm 1 summarizes the procedure for training

7311

W/A

Methods

BinaryConnect [4]

BWN [27]

DoReFa [33]

TWN [21]
TTQ [34]

ADMM [20]
HWGQ [1]
TBN [29]

LQ-Net [31]

1/32

2/32

3(±2)/32

3(±4)/32

1/1

35.4/61.0
56.8/79.4
53.9/76.3

-
-

57.0/79.7

-
-
-

-
-
-

54.5/76.8
57.5/79.7
58.2/80.6

-
-

60.5/82.7
60.9/83.2

-
-
-
-
-

-
-
-
-
-

59.2/81.8

60.0/82.2

-
-
-

-
-
-

1/2

-
-

27.9/50.42
44.2/69.2

39.5/-

47.7/-

-
-
-
-
-
-

-
-
-

52.7/76.3
49.7/74.2
55.7/78.8
55.4/78.8

Ours

58.8/81.7

61.5/83.5

61.9/83.6

47.9/72.5

Table 1: Top-1 and Top-5 accuracies (%) of AlexNet on ImageNet classiﬁcation. The performance of the full-precision model
is 61.8/83.5. “W” and “A” represent the quantization bits of the weights and activations, respectively.

quantization networks. For a full-precision network N
with M modules, where a module can be either a convo-
lutional layer or a fully connected layer, we denote all the
activations to be quantized in the m-th module as X (m),
and all the weights to be quantized in the m-th module
as Θ(m). All elements in X (m) share the same quantiza-
tion function parameters {α(m)
(m)
X }. All elements
in Θ(m) share the same quantization function parameters
{α(m)
(m)
Θ }. We apply the quantization function
module by module, and train the network with a gradually
increased temperature T .

X , β(m)

Θ , β(m)

X , b

Θ , b

4. Experiments

4.1. Image Classiﬁcation

To evaluate our method, we compare with the state-
of-the-art classiﬁcation methods on ImageNet dataset
(ILSVRC 2012). ImageNet consists of approximately 1.2
million training images from 1,000 categories and 50,000
validation images. We evaluate our method on AlexNet [18]
(over-parameterized architectures) and ResNet-18/ResNet-
50 [8] (compact-parameterized architectures). We report
our classiﬁcation performance using Top-1 and Top-5 ac-
curacies with networks quantized to Binary({0, 1}, 1 bit),
Ternary({-1, 0, 1}, 2 bits), {-2, -1, 0, 1, 2} (denoted as 3
bits(±2)), {-4, -2, -1, 0, 1, 2, 4 } (denoted as 3 bits(±4)),
and {-15, -14, · · · , -1, 0, 1, · · · , 14, 15 } (5 bits). All the pa-
rameters are ﬁne-tuned from pretrained full-precision mod-
els.

All the images from ImageNet are resized to 256 pix-
els for the smaller edge, followed by a random crop of
224 × 224. Each pixel value of the input images is sub-
tracted by the mean values and divided by the variances.
Random horizontal ﬂipping is introduced for preprocessing.
No other data augmentation tricks are used in the learning

process. We choose a batch size of 256 during training.
Similar to [27] and [21], the parameters of the ﬁrst convolu-
tional layer and the last fully connected layer for classiﬁca-
tion are not quantized. For testing, images are preprocessed
in the same way.

For our quantization function Eq. (6), to ensure all the in-
put full-precision values lie in the linear region of our quan-
tization function, the input scale β is initialized to 5p
4 × 1
q ,
where p is the max absolute value of elements in Y and q is
the max absolute value of the elements in {X , Θ}. Here ac-
tivation set X consists of the activations of randomly sam-
pled 1000 samples from the dataset. The output scale α
is initialized to 1
β , which keeps the magnitude of the in-
puts unchanged after quantization. We adopt a layer-wise
quantization in this paper, i.e., the weights/activations from
the same layer share the same quantization function and the
weights/activations from different layers use different quan-
tization functions.

Weight quantization: For binary quantization, only 1
sigmoid function is needed: n = 1, b = 0, s = 2, and o =
1. For the quantization of other bits, we ﬁrst group the full-
precision inputs into n + 1 clusters via k-means clustering,
then rank the centers of the clusters in an ascending order,
{c1, . . . , cn+1}. The biases are initialized by bi = ci+ci+1
.
Activation quantization: The outputs of the ReLU units
are used for the activation quantization (Conv-BN-ReLU(-
Pooling)-Quant). The o in Eq. (6) is set to 0 because all ac-
tivations are non-negative. For binary quantization({0, 1}),
only 1 sigmoid function is needed, i.e., n = 1 and s = 1.
For two-bit quantization of the activations ({0, 1, 2, 3}),
n = 3 and si = 1. We randomly take 1000 samples from
the dataset, and use the min/max activation values of the
output in each layer for the initialization of q. And bi is
obtained by clustering as in weight quantization with this
1000 samples.

2

7312

W/A

Methods

BWN [27]
TWN [21]
TTQ [34]
INQ [32]

ABC-Net [23]

HWGQ [1]
ADMM [20]
ICLR18 [5]
TBN [29]

LQ-Net [31]

1/32

2/32

3(±2)/32

3(±4)/32

5/32

1/1

1/2

32/2

60.8/83.0

-

-
-
-
-
-

61.8/84.2
66.6/87.2
66.0/87.1

-
-

-
-
-
-
-
-

-
-
-

68.1/88.4

-
-

64.8/86.2

67.0/87.5

67.5/87.9

68.0/88.3

-
-
-

-
-

68.0/88.0
69.1/88.9

-
-
-

69.9/89.3

-
-

69.3/88.8
70.4/89.6

-
-
-

69.0/89.1
68.3/87.9

51.2/73.2

-
-
-

42.7/67.6

-
-
-
-
-

-
-
-
-
-

-
-
-
-
-

70.6/89.6

53.6/75.3

59.6/82.2

-
-

55.6/79.0
62.6/84.3
63.4/84.9

-
-
-
-
-
-
-

64.3/-

-
-

65.7/86.5

Ours

66.5/87.3

Table 2: Top-1 and Top-5 accuracies (%) of ResNet-18 on ImageNet classiﬁcation. The performance of the full-precision
model are 70.3/89.5.

W/A

Methods

BWN [27]
TWN [21]
INQ [32]

LQ-Net [31]

1/32

68.7/-

-
-
-

Ours

72.8/91.3

2/32

3(±2)/32

3(±4)/32

5/32

-

72.5/-

-

75.1/92.3
75.2/92.6

-
-
-
-

-
-
-
-

-
-

74.8/-

-

75.5/92.8

76.2/93.2

76.4/93.2

Table 3: Top-1 and Top-5 accuracies (%) of ResNet-50 on ImageNet classiﬁcation. The performance of the full-precision
model are 76.4/93.2.

The whole training process consists of 3 phases. Firstly,
we disable the activation quantization and only train the
quantization of the weights. Secondly, we ﬁx the quan-
tization of the weights and only train the quantization of
the activations. Finally, we train the quantization of both
weights and activations until the model converges. In prac-
tice, freezing T = 1 for the back-propagation of binary
quantization achieves better performance.

AlexNet: This network consists of ﬁve convolutional
layers and two fully connected layers. This network is the
mostly widely used benchmark for the quantization of neu-
ral networks. As in [27, 21, 20], we use AlexNet coupled
with batch normalization [14] layers. We update the model
by stochastic gradient descent (SGD) with the momentum
set to 0.9. The learning rate is initialized to 0.001 and de-
cays by 0.1 at epochs 25 and 40, respectively. The model
is trained for at most 55 epochs in total. The weight de-
cay is set to 5e−4. The temperature T is set to 10 and in-
creased linearly w.r.t.
the number of training epochs, i.e.,
T = epoch × 10. The gradients are clipped with a maxi-
mum L2 norm of 5.

The results of different quantization methods are shown
in Table 1, where 1/1 means both weights and activations

are binary quantized. As shown by the results, our quanti-
zation network outperforms state-of-the-art methods in both
weight quantization and activation quantization.

ResNet: The most common baseline architectures, in-
cluding AlexNet, VGG and GoogleNet, are all over-
parameterized by design to achieve improved accuracy.
Therefore, it is easy to obtain sizable compression of these
architectures with a small accuracy degradation. A more
meaningful benchmark would be to quantize the model ar-
chitectures that already possess efﬁcient parameters, e.g.,
ResNet. We use the ResNet-18 and ResNet-50 networks
proposed in [8] for evaluation.

The learning rate is decayed by 0.1 at epochs 30 and
45, and the model is trained for at most 55 epochs in to-
tal. The weight decay is set to 1e−4. The temperature T
is set to 5 and increased linearly w.r.t the training epochs
(T = epoch × 5). The other settings are the same as those
for AlexNet. The results of different quantization meth-
ods are shown in Table 2 and Table 3 for ResNet-18 and
ResNet-50, respectively. We can see that the performance
degradation of the quantized models is larger than that on
AlexNet. This is reasonable because the parameters of the
original model are more compact. Note that even in such

7313

Methods

W/A

2/32

3(±4)/32

3(±4)/8

ADMM [20]

Ours

76.2
76.3

77.6
77.7

-

76.1

Quantization methods

linear

non-uniform

linear

non-uniform

W/A

2/32
2/32

3(±4)/32
3(±4)/32

Top-1

Top-5

60.6
60.9
60.7
61.9

82.8
83.2
83.0
83.6

Table 4: mAP (%) of SSD on Pascal VOC object detection.
The performance of the full-precision model is 77.8.

a compact model, our method still achieves lossless results
with only 3 bits. In addition, as far as we know, we are the
ﬁrst to surpass the full-precision model on ResNet-18 with
3-bit weight quantization.

4.2. Object Detection

To evaluate our quantization network on object detection
tasks, we test it on the popular SSD (single shot multibox
detection) architecture [24]. The models are trained on the
Pascal VOC 2007 and 2012 training datasets and tested on
Pascal VOC 2007 test dataset. We follow the same settings
in [24], and the input images are resized to 300 × 300. Ex-
cept for the ﬁnal convolutional layers with 1 × 1 kernels and
the ﬁrst convolution layer, the parameters of all other layers
in the backbone VGG16 are quantized.

We update the model by SGD with the momentum set to
0.9. The initial learning rate is set to 1e−5 for the quantized
parameters and 1e−7 for the non-quantized parameters and
then decayed by 0.1 at epochs 70 and 90. The models are
trained for 100 epochs in total. The batch size is set to 16,
and the weight decay is 5e−4. We increase the temperature
T by 10 every epoch, i.e., T = epoch × 10. The gradients
are clipped with maximum L2 norm of 20.

Since other baseline quantization methods did not report
their performances on object detection tasks, we only com-
pare our model with ADMM. As shown by the results in
Table 4, our model is slightly better than ADMM. This re-
sult is very promising since our method is much simpler and
substantially more general than ADMM.

4.3. Ablation Experiments

In this section, we discuss the settings of our quantiza-
tion network. All statistics are collected from the training
process for Alexnet and ResNet-18 on ImageNet.

1

are

linearly

Usually,
(e.g.,

Conﬁguration of Bias b.

values
k , 0,
2 , . . . , − 1

set
k , . . . , k−1
k , 1}) or
2k−1 , 0,

the quan-
{−1, − k−1
k ,
tized
. . . , − 1
logarithmically (e.g.,
{−1, − 1
2 , 1) with a scale factor
α [21, 30, 20, 33, 10, 15].
In this paper, we ﬁnd that
the distribution of the full-precision parameters of the
pre-trained model roughly follows a Gaussian distribution,
which indicates that quantizing weights into linear or
logarithmic intervals may fail to preserve such distribution.

2k−1 , . . . , 1

1

Table 5: Ablation study concerning training the quantiza-
tion of AlexNet on ImageNet classiﬁcation: using linear vs.
non-uniform quantization.

(a) Top-1

(b) Top-5

Figure 3: The gap between the training model and test-
ing model along with the training process for ResNet-18
{−4, +4}. The gap between the training and testing model
converges as the learning proceeds.

Thus, we adopt a non-uniform quantization (e.g., K-means
clustering) to counterbalance this, based on the n + 1
clustering centers for determining the quantization intervals
{bi}. The experimental results in Table 5 demonstrate the
superior of the non-uniform quantization over the linear
quantization. We also found that ﬁxing the biases during
training achieves better performance than learning an
adaptive biases. Therefore, we freeze the biases after the
initialization in all experiments.

Effect of Temperature. As discussed in Section 3, the
temperature T controls the gap between the hard quanti-
zation function Eq. (11) in the inference stage and the soft
quantization function Eq. (6) in the training stage. To inves-
tigate the effect of this gap on the performance of quantized
networks, we compare the testing accuracy of the models
(trained with different T ) when soft and hard quantization
functions are adopted, as shown in Fig. 3. We can see that as
the temperature T increases, the difference between them is
gradually reduced. Thus, gradually increasing the tempera-
ture T during training can achieve a good balance between
model learning capacity and quantization gap.

Training from pre-trained model. In our training, the
temperature parameter T is increased linearly w.r.t.
the
training epochs. When training from scratch, the temper-
ature T may become quite large before the network is well-
converged, and the saturated neurons will slow down the

7314

Training methods

W/A

Top-1

Top-5

from scratch

from pre-trained

3(±4)/32
3(±4)/32

55.3
70.4

78.8
89.6

Table 6: Ablation study of training the quantization of
ResNet-18 for ImageNet classiﬁcation:
from scratch vs.
from a pre-trained model.

Binary

Ternary

Full-precision

Time
Space

1x
1x

1.4x
2x

45x
32x

Table 7: Time-space complexity of ﬁnal inference based on
the VU9P FPGA evaluation. Each number indicates the ra-
tio to the complexities of the binary network. Binary: 1-bit
weights and 1-bit activations. Ternary: 2-bit weights and
2-bit activations.

network training process and cause the network to become
stuck in local minima. According to Table 6, training from
a pre-trained model can greatly improve the performance
compared to training from scratch.

Time-space complexity of the ﬁnal model for infer-
ence. Table 7 shows the time-space complexities of the ﬁ-
nal quantization networks for inference based on the VU9P
FPGA evaluation. We can see that both the time and space
complexities are signiﬁcantly reduced resulting from the
low-bit quantization of the neural networks.

Convergence of Temperature T . The training process
is very stable w.r.t. different T (shown in Fig. 4). The
approximation of the ﬁnal “soft” quantization function to
a “hard” step function is determined by the ﬁnal tempera-
ture, which is controlled by the maximum training epoch
(T = epoch ∗ 10). The increasing speed of the tempera-
ture (e.g., 10) controls the speed of convergence (or learning
rate) from a “soft” to “hard” quantization (shown in Figure 4
in our paper), and it is consistent with the learning progress
of the backbone model. Practically, for different backbone
models, we can tune T in {5, 10, 20, 40} based on the per-
formance on validation set as for the learning rate for the
DL models.

5. Conclusion

This work focused on interpreting and implementing the
low-bit quantization of deep neural networks from the per-
spective of non-linear functions.
Inspired by activation
functions in DNNs, a soft quantization function is proposed
and incorporated into deep neural networks as a new type
of activation function. With this differentiable non-linear
quantization function embedded, the quantization networks

The training error curve and the train-
Figure 4:
ing/validation accuracy curve for AlexNet quantization (left
to right: T = 5/10/20 ∗ epoch). Similar curves are ob-
served for T = 1/30/40 ∗ epoch; we do not show them
here because of space limitation.

can be learned in an end-to-end manner. Our quantization
method is both highly ﬂexible and suitable for arbitrary-bit
quantization and can be applied for the quantization of both
weights and activations. Extensive experiments on image
classiﬁcation and object detection tasks have veriﬁed the ef-
fectiveness of the proposed method.

Acknowledgements

This work was

supported in part by the Na-
tional Key R&D Program of China under contract No.
2017YFB1002203 and NSFC No. 61872329.

References

[1] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconce-
los. Deep learning with low precision by half-wave gaussian
quantization. In CVPR, pages 5406–5414, 2017.

[2] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and S Yu
Philip. Hashnet: Deep learning to hash by continuation. In
ICCV, pages 5609–5618, 2017.

[3] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Wein-
berger, and Yixin Chen. Compressing neural networks with
the hashing trick. In ICML, pages 2285–2294, 2015.

[4] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
David. Binaryconnect: Training deep neural networks with
binary weights during propagations. In NIPS, pages 3123–
3131, 2015.

[5] Abram L Friesen and Pedro Domingos. Deep learning as
a mixed convex-combinatorial optimization problem. arXiv
preprint arXiv:1710.11573, 2017.

[6] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron C. Courville, and Yoshua Bengio. Maxout networks.
In ICML, pages 1319–1327, 2013.

[7] Song Han, Jeff Pool, John Tran, and William J. Dally. Learn-
ing both weights and connections for efﬁcient neural net-
works. In NIPS, pages 1135–1143, 2015.

7315

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[25] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units im-
prove restricted boltzmann machines. In ICML, pages 807–
814, 2010.

[26] Michael A. Nielsen. Neural networks and deep learning. De-

termination Press, 2015.

[27] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi-
nary convolutional neural networks. In ECCV, pages 525–
542, 2016.

[28] Frank Rosenblatt. The perceptron: A probabilistic model for
information storage and organization in the brain. Psycho-
logical Review, pages 65–386, 1958.

[29] Diwen Wan, Fumin Shen, Li Liu, Fan Zhu, Jie Qin, Ling
Shao, and Heng Tao Shen. Tbn: Convolutional neural net-
work with ternary inputs and binary weights. In ECCV, pages
315–332, 2018.

[30] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Train-
ing and inference with integers in deep neural networks. In
ICLR, 2018.

[31] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang
Hua. Lq-nets: Learned quantization for highly accurate and
compact deep neural networks.
In ECCV, pages 365–382,
2018.

[32] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong
Chen. Incremental network quantization: Towards lossless
cnns with low-precision weights. In ICLR, 2017.

[33] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen,
and Yuheng Zou. Dorefa-net: Training low bitwidth convo-
lutional neural networks with low bitwidth gradients. arXiv
preprint arXiv:1606.06160, 2016.

[34] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally.

Trained ternary quantization. In ICLR, 2017.

[35] Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu,
and Ian Reid. Towards effective low-bitwidth convolutional
neural networks. In CVPR, pages 7920–7928, 2018.

[9] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
arXiv preprint

ing the knowledge in a neural network.
arXiv:1503.02531, 2015.

[10] Lu Hou and James T. Kwok. Loss-aware weight quantization

of deep networks. In ICLR, 2018.

[11] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.

[12] Yoshua Bengio Ian Goodfellow and Aaron Courville. Deep

learning. Book in preparation for MIT Press, 2016.

[13] Forrest N Iandola, Song Han, Matthew W Moskewicz,
Khalid Ashraf, William J Dally,
and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer pa-
rameters and¡ 0.5 mb model size.
arXiv preprint
arXiv:1602.07360, 2016.

[14] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, pages 448–456, 2015.

[15] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
Matthew Tang, Andrew G. Howard, Hartwig Adam, and
Dmitry Kalenichenko. Quantization and training of neural
networks for efﬁcient integer-arithmetic-only inference. In
CVPR, pages 2704–2713, 2018.

[16] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
Speeding up convolutional neural networks with low rank
expansions. In BMVC, 2014.

[17] Joe Kilian and Hava T Siegelmann. On the power of sigmoid

neural networks. In COLT, pages 137–143, 1993.

[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, pages 1106–1114, 2012.

[19] Abhisek Kundu, Kunal Banerjee, Naveen Mellempudi,
Dheevatsa Mudigere, Dipankar Das, Bharat Kaul, and
Pradeep Dubey. Ternary residual networks. arXiv preprint
arXiv:1707.04679, 2017.

[20] Cong Leng, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely
low bit neural network: Squeeze the last bit out with admm.
In AAAI, pages 3466–3473, 2018.

[21] Fengfu Li and Bin Liu. Ternary weight networks. arXiv

preprint arXiv:1605.04711v2, 2016.

[22] Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and
Wen Gao. Performance guaranteed network acceleration
via high-order residual quantization. In ICCV, pages 2603–
2611, 2017.

[23] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate
binary convolutional neural network. In NIPS, pages 345–
353, 2017.

[24] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C.
Berg. Ssd: Single shot multibox detector. In ECCV, pages
21–37, 2016.

7316

