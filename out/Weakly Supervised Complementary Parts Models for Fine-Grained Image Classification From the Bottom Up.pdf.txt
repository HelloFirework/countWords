Weakly Supervised Complementary Parts Models for Fine-Grained

Image Classiﬁcation from the Bottom Up

Weifeng Ge1,2∗

Xiangru Lin2∗

Yizhou Yu1†

1Deepwise AI Lab

2The University of Hong Kong

Abstract

Given a training dataset composed of images and cor-
responding category labels, deep convolutional neural net-
works show a strong ability in mining discriminative parts
for image classiﬁcation. However, deep convolutional neu-
ral networks trained with image level labels only tend to fo-
cus on the most discriminative parts while missing other ob-
ject parts, which could provide complementary information.
In this paper, we approach this problem from a different per-
spective. We build complementary parts models in a weak-
ly supervised manner to retrieve information suppressed by
dominant object parts detected by convolutional neural net-
works. Given image level labels only, we ﬁrst extract rough
object instances by performing weakly supervised objec-
t detection and instance segmentation using Mask R-CNN
and CRF-based segmentation. Then we estimate and search
for the best parts model for each object instance under the
principle of preserving as much diversity as possible. In the
last stage, we build a bi-directional long short-term memory
(LSTM) network to fuze and encode the partial information
of these complementary parts into a comprehensive feature
for image classiﬁcation. Experimental results indicate that
the proposed method not only achieves signiﬁcant improve-
ment over our baseline models, but also outperforms state-
of-the-art algorithms by a large margin (6.7%, 2.8%, 5.2%
respectively) on Stanford Dogs 120, Caltech-UCSD Birds
2011-200 and Caltech 256.

1. Introduction

Deep neural networks have demonstrated its ability to
learn representative features for image classiﬁcation [34,
25, 37, 17]. Given training data, image classiﬁcation [9, 25]
often builds a feature extractor that accepts an input image
and a subsequent classiﬁer that generates prediction prob-
ability for the image. This is a common pipeline in many
high-level vision tasks, such as object detection [14, 16],

∗These authors have equal contribution.
†Corresponding author is Yizhou Yu.

tracking [42, 33, 38], and scene understanding [8, 31].

Although a model

trained with the aforementioned
pipeline can achieve competitive results on many image
classiﬁcation benchmarks, its performance gain primarily
comes from the model’s capacity to discover the most dis-
criminative parts in the input image. To better understand a
trained deep neural network and obtain insights about this
phenomenon, many techniques [1, 54, 2] have been pro-
posed to visualize the intermediate results of deep networks.
In Fig 1, it can be found that deep convolutional neural net-
works trained with image labels only tend to focus on the
most discriminative parts while missing other object parts.
However, focusing on the most discriminative parts alone
can have limitations. Some image classiﬁcation tasks need
to grasp object descriptions that are as complete as possi-
ble. A complete object description does not have to come in
one piece, but could be assembled together using multiple
partial descriptions. To remove redundancies, such partial
descriptions should be complementary to each other. Image
classiﬁcation tasks, that could beneﬁt from such complete
descriptions, include ﬁne-grained classiﬁcation tasks on S-
tanford Dogs 120 [21] and CUB 2011-200 [47], where ap-
pearances of different object parts collectively contribute to
the ﬁnal classiﬁcation performance.

According to the above analysis, we approach image
classiﬁcation from a different perspective and propose a
new pipeline that aims to mine complementary parts instead
of the aforementioned most discriminative parts, and fuse
the mined complementary parts before making ﬁnal classi-
ﬁcation decisions.

Object Detection Phase. Object detection [10, 14, 16] is
able to localize objects by performing a huge number of
classiﬁcations at a large number of locations. In Fig 1, the
red bounding boxes are the ground truth, the green ones
are positive object proposals, and the blue ones are nega-
tive proposals. The differences between the positive and
negative proposals are whether they contain sufﬁcient infor-
mation (overlap ratio with the ground truth bounding box)
to describe objects.
If we look at the activation map in
Fig 1, it is obvious that the positive bounding boxes spread
much wider than the core regions. As a result, we hypoth-

13034

esize that the positive object proposals that lay around the
core regions can be helpful for image classiﬁcation since
they contain partial information of the objects in the image.
However, the challenges in improving image classiﬁcation

(a) Input

(b) CAM

(c) Detections

Figure 1. Visualization of class activation map (CAM [54]) and
weakly supervised object detections.

by detection are two-fold. First, how can we perform objec-
t detection without groundtruth bounding box annotations?
Second, how can we exploit object detection results to boost
the performance of image classiﬁcation? In this paper, we
attempt to tackle these two challenges in a weakly super-
vised manner.

To avoid missing any important object parts, we pro-
pose a weakly supervised object detection pipeline regular-
ized by iterative object instance segmentation. We start by
training a deep classiﬁcation neural network that produces a
class activation map (CAM) as in [54]. Then the activations
in CAM are taken as the pixelwise probabilities of the corre-
sponding class. A conditional random ﬁeld (CRF) [40] then
incorporates low level pairwise appearance information to
perform unsupervised object instance segmentation. To re-
ﬁne object locations and pixel labels, a Mask R-CNN [16]
is trained using the object instance masks from the CRF.
Results from the Mask R-CNN are used as a pixel probabil-
ity map to replace the CAM in the CRF. We alternate Mask
R-CNN and CRF regularization a few times to generate the
ﬁnal object instance masks.
Image Classiﬁcation Phase. Directly reporting classiﬁca-
tion results in the object detection phase gives rise to infe-
rior performance because object detection algorithms make
much effort to determine location in addition to class labels.
In order to mine representative object parts with the help of
object detection, we utilize the proposals generated in the
previous object detection phase and build a complementary
parts model, which consists of a subset of the proposals that
cover as much complementary object information as possi-
ble. At the end, we exploit a bi-directional long short-term
memory network to encode the deep features of the object
parts for ﬁnal image classiﬁcation.

In summary, this paper has the following contributions:

∙ We introduce a new representation for image classiﬁca-
tion, called weakly supervised complementary parts model,
that attempts to grasp complete object descriptions using a
selected subset of object proposals. It is an important step

forward in exploiting weakly supervised detection to boost
image classiﬁcation performance.

∙ We develop a novel pipeline for weakly supervised ob-
ject detection and instance segmentation. Speciﬁcally, we
iterate the following two steps, object detection and seg-
mentation using Mask R-CNN, and instance segmentation
enhancement using CRF. In this way, we get strong object
detection results and build accurate object part model.

∙ To encode complementary information in different object
parts, we exploit a bi-directional long short-term memory
network to make the ﬁnal classiﬁcation decision. Experi-
mental results demonstrate that we achieve state-of-the-art
performance on multiple image classiﬁcation tasks, includ-
ing ﬁne-grained classiﬁcation on Stanford Dogs 120 [21]
and Caltech-UCSD Birds 200-2011 [47], and generic clas-
siﬁcation on Caltech 256 [15].

2. Related Work

Weakly Supervised Object Detection and Segmentation.
Weakly supervised object detection and segmentation re-
spectively locates and segments objects with image label
only [5]. In [7, 6], the object detection is solved as a clas-
siﬁcation problem by speciﬁc pooling layers in CNNs. The
method in [44] proposed an iterative bottom-up and top-
down framework to expand object regions and optimize seg-
mentation network iteratively. Ge et al.
in [12] progres-
sively mine the object locations and pixel labels with the
ﬁltering and fusion of multiple evidences.

While here we perform the weakly supervised object in-
stance detection and segmentation by feeding a coarse seg-
mentation mask and proposal for Mask R-CNN [16] using
CAM [54] and rectifying the object locations and masks
with CRF [40] iteratively. In this way, we avoid losing im-
portant object parts for subsequent object parts modeling.
Part Based Fine-grained Image Classiﬁcation. Learn-
ing a diverse collection of discriminative parts in a
supervised[51, 50] or unsupervised manner [35, 52, 26] is
very popular in ﬁne-grained image classiﬁcation. Many
works [51, 50] have been done to build object part models
with part bounding box annotations. The method in [51]
builds two deformable part models [10] to localize objects
and discriminative parts. Zhang et al. in [50] treats objects
and semantic parts equally by assigning them in differen-
t object classes with R-CNN [14]. Another line of work-
s [35, 52, 26, 44] estimate the part location in a unsuper-
vised setting. In [35], parts are discovered based the neural
activation, and then are optimized using a EM similar algo-
rithm. The work in [35] extracts the highlight responses in
CNN as the part prior to initialize convolutional ﬁlters, and
then learn discriminative patch detectors end-to-end.

3035

In this paper, we do not aim to build strong part detectors
to provide local appearance information for the ﬁnal clas-
siﬁcation decision. The goal of our complementary parts
model is to efﬁciently utilize the rich information hidden
in the object proposals produced during object detection
phase.

Context Encoding with LSTM. LSTM network shows its
powerfulness in encoding the context information for im-
age classiﬁcation. In [26], Lam et al. address ﬁne-grained
image classiﬁcation by mining informative image parts us-
ing a heuristic network, a successor network and a single
layer LSTM. The heuristic network is responsible for ex-
tracting features from proposals and the successor network
is responsible for predicting the new proposal offset. A sin-
gle layer LSTM is used to fuse the information both for ﬁnal
object class prediction and also for the offset prediction. At-
tentional regions is discovered recurrently by incorporating
a LSTM sub-network for multi-label image classiﬁcation in
[46]. The LSTM sub-network sequentially predict seman-
tic labeling scores on the located regions and captures the
spatial dependencies at the same time.

LSTM is used in our complementary part model to in-
tegrate the rich information hidden in different object pro-
posals detected. Different from the single direction LSTM
in [26, 46], we exploit a bi-directional LSTM to learn deep
hierachical representation of all image patches. Experimen-
tal results show this strategy improve the performance sub-
stantially compared to the single layer LSTM.

3. Weakly Supervised Complementary Parts

Model

3.1. Overview

Given an image 𝑰 and its corresponding image label 𝒄,
the method proposed in this paper aims to mine discrim-
inative parts ℳ of an object that capture complementary
information via object detection and then fuse the mined
complementary parts for image classiﬁcation. This is a re-
versal of a current trend [16, 32, 29], which ﬁne-tunes image
classiﬁcation models for object detection. Since we do not
have labeled part locations but image level labels only, we
formulate our problem in a weakly supervised manner. We
adopt an iterative reﬁnement pipeline to improve the estima-
tion of object parts. Then we build a classiﬁer utilizing the
rich context representation focusing on object parts to boost
classiﬁcation performance. We decompose our pipeline into
three stages, as shown in Fig 2, namely, weakly supervised
object detection and instance segmentation, complementary
part model mining and image classiﬁcation with context en-
coding.

3.2. Weakly Supervised Object Detection and In-

stance Segmentation

Coarse Object Mask Initialization. Given an image 𝑰 and
its image label 𝒄, the feature map of the last convolutional
layer of a classiﬁcation network is denoted as 𝜙 (𝑰, 𝜃) ∈
ℝ𝐾×ℎ×𝑤, where 𝜃 represents the parameters of network 𝜙,
𝐾 is the number of channels, ℎ and 𝑤 are the height and
width of the feature map respectively. Next, global average
pooling is performed on 𝜙 to obtain the pooled feature 𝐹𝑘 =

∑𝑥,𝑦 𝜙𝑘(𝑥, 𝑦). The classiﬁcation layer is added at the end

and thus, the class activation map (CAM) for class 𝑐 is given
as follows,

𝑴 𝑐(𝑥, 𝑦) =∑𝑘

𝑤𝑐

𝑘𝜙𝑘(𝑥, 𝑦),

(1)

where 𝑤𝑐
𝑘 is the weight corresponding to class 𝑐 for the 𝑘-th
channel in the global average pooling layer. The obtained
class activation map 𝑴 𝑐 is upsampled to the original image
size ℝ𝐻×𝑊 through bilinear interpolation. Since an image
could have multiple object instances, multiple locally max-
imum responses could be observed on the class activation
map 𝑴 𝑐. We apply multi-region level set segmentation [3]
to this map to segment candidate object instances. Next,
for each instance, we normalize the class activation to the
range, [0, 1]. Suppose we have 𝑛 object instances in CAM,
we set up an object probability map 𝑭 ∈ ℝ(𝑛+1)×𝐻×𝑊 ac-
cording to the normalized CAM. The ﬁrst 𝑛 object probabil-
ity maps denote the probability of a certain object existing
in the image and the (𝑛 + 1)-th probability map represents
the probability of the background. The background proba-
bility map is calculated as

𝑭 𝑛+1

𝑖∈ℝ𝐻×𝑊 = max(1 ࢤ

𝑛∑𝜄=1

𝑭 𝜄

𝑖∈ℝ𝐻×𝑊 , 0).

(2)

Then a conditional random ﬁeld (CRF) [40] is used to
extract higher-quality object instances.
In order to apply
CRFs, a label map 𝑳 is generated according to the following
formula,

𝑳𝑖∈ℝ𝐻×𝑊 ={𝜆, arg max𝜆 𝑭 𝜆

0, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒

𝑖∈ℝ𝐻×𝑊 > 𝜎𝑐

(3)

where 𝜎𝑐 is always set to 0.8, a ﬁxed threshold used to de-
termine how certain a pixel belongs to an object or back-
ground. The label map 𝑳 is then fed into a CRF to gen-
erate object instance segments, that are treated as pseudo
groundtruth annotations for Mask-RCNN training. The pa-
rameters in the CRF are the same as in [23]. Fig 2 stage 1
shows the whole process of object instance segmentation.
Jointly Detect and Segment Object Instances. Given a set
of segmented object instances, 𝒮 = [𝒮1, 𝒮2, ...𝒮𝑛] of 𝑰, and
their corresponding class labels, generated in the previous

3036

Stage 1: Weakly Supervised Instance Detection and Segmentation

1

 
 
v
n
o
C

1

 
l

o
o
P

2
 
v
n
o
C

2

 
l

o
o
P

3
 
v
n
o
C

3

 
l

o
o
P

4
 
v
n
o
C

4

 
l

o
o
P

5
 
v
n
o
C

n
g

i
l

A

 
I

O
R

n
g

i
l

A

 
I

O
R

N
N
C
R

v
n
o
C

n
o
i
t
c
e
t
e
D

k
s
a
M

Class Activation Map

Initial Segmentation

Mask R-CNN

Probability Map

Instance Segmentation

CRFs

Stage 2: Complementary Parts Model

…

CRFs

…

Redundant Object Proposals

Constrained Object Part Model

NMS

Searching

Input

Weakly Supervised Object Detection

Detections

Complementary Part Model

Complementary Parts

Stage 3: Image Classification with Context Encoding

(cid:6)(cid:2868)

(cid:2165)

CNN
LSTM 1

LSTM 2

Softmax

(cid:138)(cid:2869)(cid:481)(cid:6)(cid:2869)
(cid:138)(cid:2924)(cid:2878)(cid:2870)(cid:4593)
(cid:481)(cid:6)(cid:2924)(cid:2878)(cid:2870)(cid:4593)

(cid:2157)(cid:2778)

CNN
LSTM 1

LSTM 2

Softmax

(cid:2157)(cid:2779)

CNN
LSTM 1

LSTM 2

Softmax

(cid:138)(cid:2870)(cid:481)(cid:6)(cid:2870)
(cid:138)(cid:2924)(cid:2878)(cid:2869)(cid:4593)
(cid:481)(cid:6)(cid:2924)(cid:2878)(cid:2869)(cid:4593)

(cid:138)(cid:2871)(cid:481)(cid:6)(cid:2871)
(cid:138)(cid:2924)(cid:4593)(cid:481)(cid:6)(cid:2924)(cid:4593)

…

(cid:2157)(cid:2196)

CNN
LSTM 1

LSTM 2

Softmax

(cid:138)(cid:2924)(cid:2878)(cid:2869)(cid:481)(cid:6)(cid:2924)(cid:2878)(cid:2869)
(cid:138)(cid:2870)(cid:4593)(cid:481)(cid:6)(cid:2870)(cid:4593)

(cid:138)(cid:2924)(cid:2878)(cid:2870)(cid:481)(cid:6)(cid:2924)(cid:2878)(cid:2870)
(cid:138)(cid:2869)(cid:4593)(cid:481)(cid:6)(cid:2869)(cid:4593)

(cid:2157)(cid:2196)(cid:2878)(cid:2778)

CNN
LSTM 1

LSTM 2

Softmax

(cid:6)(cid:2868)(cid:4593)

Figure 2. The proposed image classiﬁcation pipeline based on weakly supervised complementary parts model. From top to bottom: (a)
Weakly Supervised Object Detection and Instance Segmentation: The ﬁrst step initializes the segmentation probability map by CAM [54],
and obtaining coarse instance segmentation maps by CRF [40]. Then the segments and bounding boxes are used as groundtruth annotations
for training Mask R-CNN [16] in an iterative manner. (b) Complementary Parts Model: Search for complementary object proposals to
form the object parts model. (c) Image Classiﬁcation with Context Encoding: Two LSTMs [18] are stacked together to fuse and encode
the partial information provided by different object parts.

stage, we obtain the minimum bounding box of each seg-
ment to form a set of proposals, 𝒫 = [𝒫1, 𝒫2, ...𝒫𝑛]. The
proposals 𝒫, segments 𝒮 and their corresponding class la-
bels are used for training Mask R-CNN for further proposal
and mask reﬁnement.
In this way, we turn object detec-
tion and instance segmentation into fully supervised learn-
ing. We train Mask R-CNN with the same setting as in [16].
CRF-Based Segmentation. Suppose there are 𝑚 object
proposals, 𝒫 ★ = [𝒫 ★
𝑚], and their corresponding
segments, 𝒮 ★ = [𝒮 ★
𝑚] for image class 𝑐, whose
classiﬁcation score is above 𝜎0, a threshold used to remove
outlier proposals. Then, a non-maximum suppression (N-
MS) procedure is applied to 𝑚 proposals with overlapping
threshold 𝜏 . Suppose 𝑛 object proposals remain afterwards,
𝒪 = [𝒪1, 𝒪2, ..., 𝒪𝑛], where 𝑛 ≪ 𝑚.

2 , ..., 𝒫 ★
2 , ..., 𝒮 ★

1 , 𝒫 ★
1 , 𝒮 ★

obtain a small number of distinct object proposals. Howev-
er, in our weakly supervised setting, proposals suppressed
in the NMS process actually contain rich object parts in-
formation as shown in Fig 2. Speciﬁcally, each proposal
𝑖 ∈ 𝒫 ★ suppressed by object proposal 𝒪𝑗 can be consid-
𝒫 ★
ered as a complementary part of 𝒪𝑗 . Therefore, the sup-
pressed proposals, 𝒫 ★
𝑖 , can be used to further reﬁne 𝒪𝑗 . We
implement this idea by initializing a class probability map
𝑭 ★ ∈ ℝ(𝑛+1)×𝐻×𝑊 . For each proposal 𝒫 ★
𝑖 suppressed by
𝒪𝑗 , we add the probability map of its proposal segmentation
mask 𝒮 ★
𝑗 by bilinear
interpolation. The class probability map is then normalized
to [0, 1]. For the (𝑛 + 1)-th probability map for the back-
ground, it is deﬁned as

𝑖 to the corresponding locations on 𝑭 ★

Most existing research utilizes NMS to suppress a large
number of proposals sharing the same class label in order to

𝑭 ★,𝑛+1

𝑖∈ℝ𝐻×𝑊 = max(1 ࢤ

3037

𝑭 ★,𝜄

𝑖∈ℝ𝐻×𝑊 , 0).

(4)

𝑛∑𝜄=1

Given the class probability maps 𝑭 ★, CRF is applied a-
gain to reﬁne and rectify instance segmentation results as
described in the previous stage.
Iterative Instance Reﬁnement. We alternate CRF-based
segmentation and Mask R-CNN based detection and in-
stance segmentation several times to gradually reﬁne the
localization and segmentation of object instances. Fig 2
shows the iterative instance reﬁnement process.

3.3. Complementary Parts Model

1 , 𝒫 ★,𝑖

2 , ..., 𝒫 ★,𝑖

𝑘 ], may

Model Deﬁnition. According to the analysis in the pre-
vious stage, given a detected object 𝒪𝑖, its corresponding

suppressed proposals, 𝒫 ★,𝑖 = [𝒫 ★,𝑖

contain useful object information and can localize correct
object position. Then, it is necessary to identify the most
informative proposals for the following classiﬁcation task.
In this section, we propose a complementary parts model 𝒜
for image classiﬁcation. This model is deﬁned by a root part
covering the entire object as well as its context, a center part
covering the core region of the object and a ﬁxed number of
surrounding proposals that cover different object parts but
still keep enough discriminative information.

A complementary parts model for an object with 𝑛 part-
s is deﬁned as a (𝑛 + 1)-tuple 𝒜 = [𝑨1, ..., 𝑨𝑛, 𝑨𝑛+1],
where 𝑨1 is the object center part, 𝑨𝑛+1 is the root part,
and 𝑨𝑖 is the 𝑖-th part. Each part model is deﬁned by a
tuple 𝑨𝑖 = [𝜙𝑖, 𝒖𝑖], where 𝜙𝑖 is the feature of the 𝑖-th
part, 𝒖𝑖 is a ℝ4 dimensional tuple that describes the geo-
metric information of a part, namely part center and part
size (𝑥𝑖, 𝑦𝑖, 𝑤𝑖, ℎ𝑖). A potential parts model without any
missing parts is called an object hypothesis. To make object
parts complementary to each other, the differences in their
appearance features or locations should be as large as possi-
ble while the combination of parts scores should also be as
large as possible. Such criteria serve as constraints during
the search for discriminative parts that are complementary
to each other. The score 𝒮 (𝒜) of an object hypothesis is
given by the summed score of all object parts minus ap-
pearance similarities and spatial overlap between different
parts.

𝒮 (𝒜) =

𝑛+1∑𝜄=1

𝑓 (𝜙𝜄)

𝑛∑𝑝=1

𝑛+1∑𝑞=𝑝+1

ࢤ 𝜆0

[𝑑𝑠(𝜙𝑝, 𝜙𝑞) + 𝛽0𝐼𝑜𝑈 (𝒖𝑝, 𝒖𝑞)] ,

(5)
where 𝑓 (𝜙𝑘) is the score of the 𝑘-th part in the classiﬁcation
branch of Mask R-CNN, 𝑑𝑠(𝜙𝑝, 𝜙𝑞) = ∥𝜙𝑝 ࢤ 𝜙𝑞∥2 is the
semantic similarity and 𝐼𝑜𝑈 (𝒖𝑝, 𝒖𝑞) is the spatial overlap
between parts 𝑝 and 𝑞, and there are two constant parame-
ters 𝜆0 = 0.01 and 𝛽0 = 0.1. Given a set of object hypothe-
ses, we can choose a hypothesis that achieves the maximum

score as the ﬁnal object part model. Searching for the op-
timal subset of proposals maximizing the above score is a
combinatorial optimization problem, which is computation-
ally expensive. In the following, we seek an approximate
solution using a fast heuristic algorithm.
Part Location Initialization. To initialize a parts mod-
el, we simplify part estimation by designing a grid-based
object parts template that follows two basic rules. First,
every part should contain enough discriminative informa-
tion; Second, the differences between part pairs should be
as large as possible. As shown in Fig 2, deep convolutional
neural networks have demonstrated its ability in localizing
the most discriminative parts of an object. Thus, we set the
root part 𝑨𝑛+1 to be the object proposal 𝒪𝑖 that represents
the entire object. Then, a 𝑠 × 𝑠(= 𝑛) grid centered at 𝑨𝑛+1
is created. The size of each grid cell is 𝑤𝑛+1
, where
𝑤𝑛+1 and ℎ𝑛+1 are the width and height of the root part
𝑨𝑛+1. The center grid cell is assigned to the object center
part. The rest of the grid cells are assigned to part 𝑨𝑖, where
𝑖 ∈ [2, 3, ..., 𝑛]. Then, we initialize each part 𝑨𝑖 ∈ 𝑨 to be
the proposal 𝒫 ★
Parts Model Search. For a model with 𝑛 object parts (we
exclude the (𝑛 + 1)-th part as it is a root part) and 𝑘 candi-
date suppressed proposals, the objective function is deﬁned
as

𝑗 ∈ 𝒫 ★ closest to the assigned grid cell.

𝑠 × ℎ𝑛+1

𝑠

𝒮 (𝒜) ,

(6)

where 𝐾 = 𝐶 𝑛

𝑘 , 𝑘 ≫ 𝑛 is the total number of object hy-

quentially go through every 𝑨𝑖 in 𝑨 and ﬁnd the optimal

potheses. As mentioned earlier, directly searching for an
optimal parts model can be intractable. Thus, we adopt a

pothesises, 𝒮𝒜 =[𝒜1, 𝒜1, ..., 𝒜𝐾] is the set of object hy-
greedy search strategy to search for ˆ𝒜. Speciﬁcally, we se-
object part for 𝑨𝑖 in 𝒫 ★ that minimizes ˆ𝒜. The overall time

complexity is reduced from exponential to linear (𝑂(𝑛𝑘)).
In Fig 2, we can see that the object hypotheses generated
during the search process cover different parts of the object
and do not focus on the core region only.

ˆ𝒜 = arg max

𝒜∈𝒮𝒜

3.4. Image Classiﬁcation with Context Encoding

CNN Feature Extractor Fine-tuning. Given an input
image 𝑰 and the parts model 𝒜 = [𝑨1, ..., 𝑨𝑛, 𝑨𝑛+1]
constructed in the previous stage,
the image patches
corresponding to the parts are denoted as 𝑰 (𝒜) =
[𝑰 (𝑨1) , 𝑰 (𝑨2) , ..., 𝑰 (𝑨𝑛) , 𝑰 (𝑨𝑛+1)]. During image
classiﬁcation, random crops of images are often used to
train the model. Thus, apart from the (𝑛+1) patches, we ap-
pend a random crop of the original image as the (𝑛 + 2)-nd
image patch. The motivation for adding a randomly cropped
patch is to include more context information during training
since those patches corresponding to object parts primarily
focus on the object itself. Every patch shares the same la-
bel with the original image it is cropped from. All patches

3038

Softmax

FC

tanh

(cid:1860)(cid:3047)(cid:4593)

(cid:1829)(cid:3047)(cid:4593)
(cid:1860)(cid:3047)(cid:4593)
(cid:1829)(cid:3047)(cid:2879)(cid:2869)
(cid:1858)(cid:3047)
(cid:1860)(cid:3047)(cid:2879)(cid:2869)
(cid:2026)
(cid:1486)(cid:3030)(cid:4666)(cid:2165)(cid:4666)(cid:2157)(cid:3047)(cid:4667)(cid:482)(cid:2016)(cid:3030)(cid:4667)

h

(cid:1861)(cid:3047)(cid:2879)(cid:2869)(cid:4593)
(cid:4634)(cid:1829)(cid:3047)(cid:2879)(cid:2869)(cid:4593)
(cid:2026)
(cid:1867)(cid:3047)(cid:2879)(cid:2869)
(cid:2026)

(cid:2869)

tanh

(cid:2026)(cid:1858)(cid:3047)(cid:2879)(cid:2869)(cid:4593)
(cid:1860)(cid:3047)(cid:2879)(cid:2869)

tanh

(cid:2026)

(cid:2026)

(cid:1858)(cid:3047)(cid:4593)
(cid:1860)(cid:3047)

tanh

(cid:1867)(cid:3047)(cid:4593)
(cid:2026)

(cid:4634)(cid:1829)(cid:3047)(cid:4593)

tanh

(cid:1861)(cid:3047)
(cid:2026)

(cid:4634)(cid:1829)(cid:3047)

tanh

(cid:1861)(cid:3047)(cid:4593)
(cid:2026)
(cid:1867)(cid:3047)
(cid:2026)

Softmax

FC

(cid:1860)(cid:3047)(cid:2878)(cid:2869)(cid:4593)
tanh (cid:1867)(cid:3047)(cid:2878)(cid:2869)(cid:4593)
(cid:2026)
(cid:1861)(cid:3047)(cid:2878)(cid:1829)
(cid:2026)

(cid:1829)(cid:3047)(cid:2878)(cid:2869)(cid:4593)
(cid:1860)(cid:3047)(cid:2878)(cid:2869)(cid:4593)
(cid:1829)(cid:3047)
(cid:1858)(cid:3047)(cid:2878)(cid:2869)
(cid:1860)(cid:3047)
(cid:2026)
(cid:1486)(cid:3030)(cid:4666)(cid:2165)(cid:4666)(cid:2157)(cid:3047)(cid:2878)(cid:2869)(cid:4667)(cid:482)(cid:2016)(cid:3030)(cid:4667)

tanh

Fully Connected Layer + Sigmoid / Tanh Activation

Copy and Split

Concatenation

tanh

Elementwise Product / Sum / Tanh

Figure 3. Context encoded image classiﬁcation based on LSTMs.
Two standard LSTMs [18] are stacked together. They have oppo-
site scanning orders.

First,

from all the original training images form a new training set,
which is used to ﬁne-tune a CNN model pretrained on Ima-
geNet. This ﬁne-tuned model serves as the feature extractor
for all image patches.
Stacked LSTM for Feature Fusion. Here we pro-
pose a stacked LSTM module 𝜙𝑙 (; 𝜃𝑙) for feature fu-
sion and performance boosting, which is shown in
the (𝑛 + 2) patches from a comple-
Fig 3.
mentary parts model are fed through the CNN fea-
ture extractor 𝜙𝑐 (; 𝜃𝑐) trained in the previous step.
from this step is denoted as Ψ (𝑰) =
The output
[𝜙𝑐 (𝑰; 𝜃𝑐) , 𝜙𝑐 (𝑰 (𝑨1) ; 𝜃𝑐) , ..., 𝜙𝑐 (𝑰 (𝑨𝑛+2) ; 𝜃𝑐)]. Next,
we build a two-layer stacked LSTM to fuse the extracted
features Ψ (𝑰). The hidden state of the ﬁrst LSTM is fed
into the second LSTM layer, but the second LSTM fol-
lows the reversed order of the ﬁrst one. Let 𝐷(= 256)
be the dimension of the hidden state. We use softmax
to generate the class probability vector for each part 𝑨𝑖,
𝑓 (𝜙𝑙 (𝑰 (𝑨𝑖) ; 𝜃𝑙)) ∈ ℝ𝒞×1. The loss function for ﬁnal im-
age classiﬁcation is deﬁned as follows,

constant weight for the 𝑖-th patch. Here we have two set-
tings: ﬁrst, the single loss sets 𝛾𝑖 = 0 (𝑖 = 2, ..., 𝑛 + 2),
and keeps only one loss at the start of the sequence; second,
the multiple losses sets 𝛾𝑖 = 1 (𝑖 = 2, ..., 𝑛 + 2). Experi-
mental results indicate that, in comparison to a single loss
for the last output from the second LSTM, multiple losses
used here improve classiﬁcation accuracy by a signiﬁcant
margin.

4. Experimental Results

4.1. Implementation Details

All experiments have been conducted on NVIDIA
TITAN X(Maxwell) GPUs with 12GB memory using
Caffe [20]. No annotated parts are used. 𝑛 is set to 9 for
all experiments.

In the mask initialization stage, we ﬁne-tune from Ima-
geNet pre-trained GoogleNet with batch normalization [19]
on target datasets. The initial learning rate is 0.001 and is
divided by 10 after every 40000 iterations with the standard
SGD optimizer. Training converges after 70000 iterations.
In the Mask R-CNN reﬁnement process, we adopt ResNet-
50 with Feature Pyramid Network (FPN) as the backbone
and pre-train the network on the COCO dataset following
the same setting described in [16]. We then ﬁne-tune the
model on our target datasets. During training, image-centric
training is used and the input images are resized such that
their shorter side is 800 pixels. Each mini-batch contains
1 image per GPU and each image has 512 sampled ROIs.
The model is trained on 4 GPUs for 150k iterations with an
initial learning rate 0.001, which is divided by 10 at 120k it-
erations. We use the standard SGD optimizer and a weight
decay of 0.0001. The momentum is set to 0.9. Unless speci-
ﬁed, the settings we use for different algorithms follow their
original settings respectively [54, 41, 3, 23, 16]. Example
intermediate results of Mask R-CNN training are shown in
Fig 4.

(cid:11)(cid:68)(cid:12)(cid:3)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)

(cid:11)(cid:69)(cid:12)(cid:3)(cid:38)(cid:36)(cid:48)

(cid:11)(cid:70)(cid:12)(cid:3)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:3)(cid:48)(cid:68)(cid:83)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:38)(cid:53)(cid:41) (cid:11)(cid:71)(cid:12)(cid:3)(cid:38)(cid:53)(cid:41)(cid:3)(cid:54)(cid:72)(cid:74)(cid:80)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:11)(cid:72)(cid:12)(cid:3)(cid:44)(cid:81)(cid:76)(cid:87)(cid:76)(cid:68)(cid:79)(cid:76)(cid:93)(cid:72)(cid:71)(cid:3)(cid:48)(cid:68)(cid:86)(cid:78)

ℒ(𝑰, 𝒚𝐼 ) = ࢤ

ࢤ

𝒞∑𝑘=1
𝑛+2∑𝑖=1

𝑦𝑘 log 𝑓 𝑘 (𝜙𝑙 (𝑰; 𝜃𝑙))

𝛾𝑖𝑦𝑘 log 𝑓 𝑘 (𝜙𝑙 (𝑰 (𝑨𝑖) ; 𝜃𝑙)) ,

𝒞∑𝑘=1

(7)

(cid:11)(cid:73)(cid:12)(cid:3)(cid:39)(cid:72)(cid:87)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)

(cid:11)(cid:74)(cid:12)(cid:3)(cid:48)(cid:68)(cid:86)(cid:78)(cid:86)

(cid:11)(cid:75)(cid:12)(cid:3)(cid:50)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:3)(cid:43)(cid:72)(cid:68)(cid:87)(cid:80)(cid:68)(cid:83) (cid:11)(cid:76)(cid:12)(cid:3)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:3)(cid:48)(cid:68)(cid:83)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:38)(cid:53)(cid:41)

(cid:11)(cid:77)(cid:12)(cid:3)(cid:38)(cid:53)(cid:41)(cid:3)(cid:53)(cid:72)(cid:86)(cid:88)(cid:79)(cid:87)

Figure 4. Example intermediate results for training Mask R-CNN.
First row: pseudo object mask and object bounding box are gener-
ated with CAM and CRF reﬁnement. Second row: With previous
pseudo groundtruth generated, object mask and object bounding
box are further reﬁned with Mask R-CNN.

where 𝑓 𝑘 (𝜙𝑙 (𝑰; 𝜃𝑙)) is the probability that image 𝑰 belongs
to the 𝑘-th class, 𝑓 𝑘 (𝜙𝑙 (𝑰 (𝑨𝑖) ; 𝜃𝑙)) is the probability that
image patch 𝑰 (𝑨𝑖) belongs to the 𝑘-th class, and 𝛾𝑖 is a

For the last stage, we adopt GoogleNet with batch nor-
malization [19] as the backbone network for Stanford Dogs
120 and Caltech-UCSD Birds 2011-200 datasets and the

3039

Caltech256 dataset. First, we ﬁne-tune the pretrained net-
work on the target dataset with the generated object parts.
The parameters are the same as those used in the ﬁrst stage.
Next, we build a Stacked LSTM module and treat the fea-
tures of the 𝑛 + 2 image patches as training sequences. We
train the model with 4 GPUs and set the learning rate to
0.001, which is decreased by a factor of 10 for every 8000
iterations. We adopt the standard SGD optimizer, momen-
tum is set to 0.9, and the weight decay is 0.0002. Training
converges at 16000 iterations.

4.2. Fine-grained Image Classiﬁcation

Stanford Dogs 120. Stanford Dogs 120 contains 120 cat-
egories of dogs. There are 12000 images for training, and
8580 images for testing. The training procedure follows the
steps described in Section 4.1.

To perform fair comparisons with existing state-of-the-
art algorithms, we divide our experiments into two group-
s. The ﬁrst group consists of algorithms that use the orig-
inal training data only and the second group is composed
of methods that use extra training data. In each group, we
set our baseline accordingly. In the ﬁrst group, we directly
ﬁne-tune the GoogleNet pretrained on ImageNet with the
input image size set to 448 x 448, which is adopted by other
algorithms [11, 30, 39] in the comparison and the classiﬁca-
tion accuracy achieved is 85.2%. This serves as our baseline
model and we then add the proposed stacked LSTM over a
complementary parts model. Our stacked LSTM is trained
with both single loss and multiple losses, which achieves
a classiﬁcation accuracy of 92.4% and 93.9% respectively.
Both of our proposed variants outerperform existing state-
of-the-art by a clear margin. In the second group, we perfor-
m selective joint ﬁne-tuning (SJFT) with images retrieved
from ImageNet, and the input image size is set to 224 x 224
to obtain our baseline network. The classiﬁcation accuracy
of our baseline is 92.1%, 1.8% higher than the SJFT with
ResNet-152 counterpart. With our stacked LSTM plugged
in and trained with both single loss and multiple losses,
the performance is further boosted to 96.3% and 97.1% re-
spectively, surpassing the current state of the art by 6% and
6.8%. These experimental results suggest that our proposed
pipeline is superior than all existing algorithms. It is worth
noting that the method in [24] is not directly comparable to
ours because it uses a large amount of extra training data
from the Internet in addition to ImageNet.
Caltech-UCSD Birds 2011-200. Caltech-UCSD Birds
2011-200 (CUB200) consists of 200 bird categories. 5994
images are used for training, and 5794 images for testing.

Our experiments here are split into two groups. In the
ﬁrst group, no extra training data is used. Our baseline
model in this group is a directly ﬁne-tuned GoogleNet mod-
el that achieves a classiﬁcation accuracy of 82.6%. We
then add the Stacked LSTM module and train the model

Method

Accuracy(%)

MAMC [39]
Inception-v3 [24]
RA-CNN [11]
FCAN [30]
GoogleNet (our baseline)
baseline + Feature Concatenation
baseline + Multiple Average
baseline + Stacked LSTM + Single Loss
baseline + Stacked LSTM + Multi-Loss (default)

Web Data + Original Data [24]
SJFT with ResNet-152 [13]
SJFT with GoogleNet (our baseline)
baseline + Feature Concatenation
baseline + Multiple Average
baseline + Stacked LSTM + Single Loss
baseline + Stacked LSTM + Multi-Loss (default)

85.2
85.9
87.3
88.9
85.2
88.1
85.2
92.4
93.9

85.9
90.3
92.1
93.2
92.2
96.3
97.1

Table 1. Classiﬁcation results on Stanford Dogs 120. Two sec-
tions are divided by the horizontal separators, namely (from top to
bottom) Experiments without SJFT and Experiments with SJFT.

with both single loss and multiple losses, which achieves
a classiﬁcation accuracy of 87.6% and 90.3% respective-
ly, outperforming all other algorithms in this compari-
son [53, 48, 45, 27]. Compared to HSNet, our model does
not use any parts annotations in the training stage while
HSNet is trained with groundtruth parts annotations.
In
the second group, our baseline model still uses GoogleNet
as the backbone and performs SJFT with images retrieved
from ImageNet.
It achieves a classiﬁcation accuracy of
82.8%. By adding the Stacked LSTM module, the accu-
racy of the model trained with single loss is 87.7% and the
model trained with multiple losses is 90.4%. When the top
performing result in the ﬁrst group is compared to that of
the second group, it can be concluded that SJFT contributes
little to the performance gain (0.1% gains) and our proposed
method is effective and solid, contributing much to the ﬁnal
performance (7.7% higher than the baseline).
It is worth
noting that, in [4], a subset of ImageNet and iNaturalist [43]
most similar to CUB200 are used for training, and in [24], a
large amount of web data are also used in the training phase.

4.3. Generic Object Recognition

Caltech 256. There are 256 object categories and 1 back-
ground cluster class in Caltech 256. A minimum number of
80 images per category are provided for training, validation
and testing. As a convention, results are reported with the
number of training samples per category falling between 5
and 60. We follow the same convention and report the result
with the number of training sample per category set to 60.
In this experiment, GoogleNet is adopted as our backbone
network and the input image size is 224 x 224. We train our

3040

Method

Accuracy(%)

4.4. Ablation Study

MACNN [53]
HBP [48]
DFB [45]
HSNet [27]
GoogleNet (our baseline)
baseline + Stacked LSTM + Single Loss
baseline + Stacked LSTM + Multi-Loss

ImageNet + iNat Finetuning [4]
SJFT with GoogleNet (our baseline)
baseline + Stacked LSTM + Single Loss
baseline + Stacked LSTM + Multi-Loss

86.5
87.2
87.4
87.5
82.6
87.6
90.3

89.6
82.8
87.7
90.4

Table 2. Classiﬁcation results on CUB200. Two sections are di-
vided by the horizontal separators, namely (from top to bottom)
Experiments without SJFT and Experiments with SJFT.

Method

Accuracy(%)

ZF Net [49]
VGG-19 + VGG-16 [36]
VGG-19 + GoogleNet +AlexNet [22]
𝐿2-SP [28]
GoogleNet (our baseline)
baseline + Stacked LSTM + Single Loss
baseline + Stacked LSTM + Multi-Loss

SJFT with ResNet-152 [13]
SJFT with GoogleNet (our baseline)
baseline + Stacked LSTM + Single Loss
baseline + Stacked LSTM + Multi-Loss

74.2±0.3
86.2±0.3

86.1

87.9±0.2
84.1±0.2
90.1±0.2
93.5±0.2

89.1±0.2
86.3±0.2
90.1±0.2
94.3±0.2

Table 3. Classiﬁcation results on Caltech 256. Two sections are
divided by the horizontal separators, namely (from top to bottom)
Experiments without SJFT and Experiments with SJFT.

model with mini-batch size set to 8 on each GPU.

In Table 3, as described previously, we conduct our ex-
periments under two settings. For the ﬁrst setting, no extra
training data is used. We ﬁne-tune the pretrained GoogleNet
on the target dataset and treat the ﬁne-tuned model as our
baseline model, which achieves a classiﬁcation accuracy of
84.1%. By adding our proposed Stacked LSTM module, the
accuracy is increased by a large margin to 90.1% for Single
Loss and to 93.5% for multiple losses respectively, outer-
performing all methods listed in the table. Also, it is 4.1%
higher than its ResNet-152 counterpart. For the second set-
ting, we adopt SJFT [13] with GoogleNet as our baseline
model, which achieves a classiﬁcation accuracy of 86.3%.
Then we add our proposed Stacked LSTM module and the
ﬁnal performance is increased by 3.8% for single loss and
8.0% for multiple losses. Our method with GoogleNet as
backbone network outerperfoms current state-of-the-art by
5.2%, demonstrating that our proposed algorithm is solid
and effective.

Ablation Study on Complementary Parts Mining.
The ablation study is performed on the CUB200 dataset
with GoogleNet as the backbone network. The classiﬁca-
tion accuracy of our reference model with 𝑛 = 9 parts on
this dataset is 90.3%. First, when the number of parts 𝑛 is
set to 2, 4, 6, 9, 12, 16, and 20 in our model, the correspond-
ing classiﬁcation accuracy is respectively 85.3%, 87.9%,
89.1%, 90.3%, 87.6%, 86.8% and 85.9%. Obviously the
best result is achieved when 𝑛 = 9. Second, if we use ob-
ject features only in our reference model, the classiﬁcation
accuracy drops to 90.0%. Third, if we use image features
only, the performance drops to 82.8%. Fourth, if we simply
use the uniform grid cells as the object parts without fur-
ther optimization, the performance drops to 78.3%, which
indicates our search for the best parts model plays an im-
portant role in escalating the performance. Fifth, instead of
a grid-based object parts initialization, we randomly sam-
ple 𝑛 = 9 suppressed object proposals around the bounding
box of the surviving proposal, and the performance drops
to 86.9%. Lastly, we discover that the part order in LSTM
does not matter. We randomly shufﬂe the part order during
training and testing, and the classiﬁcation accuracy remains
the same.

4.5. Inference Time Complexity.

The inference time of our implementation is summarised
as follows: in the complementary parts model search phase,
the time for processing an image with its shorter edge set to
800 pixels is around 277𝑚𝑠; in the context encoding phase,
the running time on an image of size 448 × 448 is about
63𝑚𝑠, and on an image of size 224 × 224 is about 27𝑚𝑠.

5. Conclusions

In this paper, we have presented a new pipeline for ﬁne-
grained image classiﬁcation, which is based on a comple-
mentary part model. Different from previous work which
focuses on learning the most discriminative parts for image
classiﬁcation, our scheme mines complementary parts that
contain partial object descriptions in a weakly supervised
manner. After getting object parts that contain rich informa-
tion, we fuse all the mined partial object descriptions with
bi-directional stacked LSTM to encode these complemen-
tary information for classiﬁcation. Experimental results in-
dicate that the proposed method is effective and outperform-
s existing state-of-the-art by a large margin. Nevertheless,
how to build the complementary part model in a more efﬁ-
cient and accurate way remains an open problem for further
investigation.

References

[1] Sebastian Bach, Alexander Binder, Gr´egoire Montavon,
Frederick Klauschen, Klaus-Robert M¨uller, and Wojciech

3041

Samek. On pixel-wise explanations for non-linear classiﬁ-
er decisions by layer-wise relevance propagation. PloS one,
2015.

ference on computer vision and pattern recognition, pages
580–587, 2014.

[15] Gregory Grifﬁn, Alex Holub, and Pietro Perona. Caltech-256

[2] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and
Antonio Torralba. Network dissection: Quantifying inter-
pretability of deep visual representations.
In 2017 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR). IEEE, 2017.

[3] Thomas Brox and Joachim Weickert. Level set segmentation
with multiple regions. IEEE Transactions on Image Process-
ing, 2006.

[4] Yin Cui, Yang Song, Chen Sun, Andrew Howard, and
Serge J. Belongie. Large scale ﬁne-grained categorization
and domain-speciﬁc transfer learning. 2018.

[5] Ali Diba, Vivek Sharma, Ali Mohammad Pazandeh, Hamed
Pirsiavash, and Luc Van Gool. Weakly supervised cascaded
convolutional networks. In 2017 IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2017, Honolu-
lu, HI, USA, July 21-26, 2017, 2017.

[6] Thibaut Durand, Taylor Mordan, Nicolas Thome, and
Matthieu Cord. Wildcat: Weakly supervised learning of deep
convnets for image classiﬁcation, pointwise localization and
segmentation. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR 2017), 2017.

[7] Thibaut Durand, Nicolas Thome, and Matthieu Cord. Wel-
don: Weakly supervised learning of deep convolutional neu-
ral networks.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2016.

[8] Nikita Dvornik, Konstantin Shmelkov, Julien Mairal, and
Cordelia Schmid. Blitznet: A real-time deep network for
scene understanding. In The IEEE International Conference
on Computer Vision (ICCV), Oct 2017.

[9] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge.
International journal of computer
vision, 2010.

[10] Pedro Felzenszwalb, David McAllester, and Deva Ramanan.
A discriminatively trained, multiscale, deformable part mod-
el. In Computer Vision and Pattern Recognition, 2008. CVPR
2008. IEEE Conference on, pages 1–8. IEEE, 2008.

[11] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see
better: Recurrent attention convolutional neural network for
ﬁne-grained image recognition.
In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017, 2017.

[12] Weifeng Ge, Sibei Yang, and Yizhou Yu. Multi-evidence ﬁl-
tering and fusion for multi-label classiﬁcation, object detec-
tion and semantic segmentation based on weakly supervised
learning. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018.

[13] W. Ge and Y. Yu. Borrowing treasures from the wealthy:
Deep transfer learning through selective joint ﬁne-tuning. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), July 2017.

object category dataset. 2007.

[16] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE
International Conference on. IEEE, 2017.

[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, 2016.

[18] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term

memory. Neural computation, 1997.

[19] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. pages 448–456, 2015.

[20] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadarra-
ma, and Trevor Darrell. Caffe: Convolutional architecture
for fast feature embedding. In Proceedings of the 22nd ACM
international conference on Multimedia. ACM, 2014.

[21] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng
Yao, and Li Fei-Fei. Novel dataset for ﬁne-grained image
categorization.
In First Workshop on Fine-Grained Visual
Categorization, IEEE Conference on Computer Vision and
Pattern Recognition, Colorado Springs, CO, 2011.

[22] Yong-Deok Kim, Taewoong Jang, Bohyung Han, and Se-
ungjin Choi. Learning to select pre-trained deep representa-
tions with bayesian evidence framework. In 2016 IEEE Con-
ference on Computer Vision and Pattern Recognition, CVPR
2016, Las Vegas, NV, USA, June 27-30, 2016, 2016.

[23] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference
in fully connected crfs with gaussian edge potentials. In J.
Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and
K. Q. Weinberger, editors, Advances in Neural Information
Processing Systems 24. Curran Associates, Inc., 2011.

[24] Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard
Zhou, Alexander Toshev, Tom Duerig, James Philbin, and
Fei-Fei Li. The unreasonable effectiveness of noisy data for
ﬁne-grained recognition. 2015.

[25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, 2012.

[26] Michael Lam, Behrooz Mahasseni, and Sinisa Todorovic.
Fine-grained recognition as hsnet search for informative im-
age parts.
In 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,
July 21-26, 2017, 2017.

[27] Michael Lam, Behrooz Mahasseni, and Sinisa Todorovic.
Fine-grained recognition as hsnet search for informative im-
age parts.
In 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,
July 21-26, 2017, 2017.

[14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE con-

[28] Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit
inductive bias for transfer learning with convolutional net-
works. In ICML, 2018.

3042

[29] Tsung-Yi Lin, Piotr Doll´ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, 2017.

[30] Xiao Liu, Tian Xia, Jiang Wang, and Yuanqing Lin. Fully
convolutional attention localization networks: Efﬁcient at-
tention localization for ﬁne-grained recognition. 2016.

[31] Cewu Lu, Hao Su, Yonglu Li, Yongyi Lu, Li Yi, Chi-
Keung Tang, and Leonidas J. Guibas. Beyond holistic object
recognition: Enriching image understanding with part states.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018.

[32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, 2015.

[33] Ergys Ristani and Carlo Tomasi. Features for multi-target
multi-camera tracking and re-identiﬁcation.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018.

[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large s-
cale visual recognition challenge. International Journal of
Computer Vision, 2015.

[35] Marcel Simon and Erik Rodner. Neural activation constella-
tions: Unsupervised part model discovery with convolutional
networks. In Proceedings of the IEEE International Confer-
ence on Computer Vision, 2015.

[36] Karen Simonyan and Andrew Zisserman. Very deep convo-

lutional networks for large-scale image recognition. 2014.

[37] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations, 2015.

[38] Chong Sun, Dong Wang, Huchuan Lu, and Ming-Hsuan
Yang. Learning spatial-aware regressions for visual track-
ing. In The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2018.

[39] Ming Sun, Yuchen Yuan, Feng Zhou, and Errui Ding. Multi-
attention multi-class constraint for ﬁne-grained image recog-
nition. In Computer Vision - ECCV 2018 - 15th European
Conference, Munich, Germany, September 8-14, 2018, Pro-
ceedings, Part XVI, 2018.

[40] Charles Sutton, Andrew McCallum, et al. An introduction
to conditional random ﬁelds. Foundations and Trends R⃝ in
Machine Learning, 4(4), 2012.

[41] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking the in-
ception architecture for computer vision. In Proceedings of
IEEE Conference on Computer Vision and Pattern Recogni-
tion,, 2016.

[42] Zhu Teng, Junliang Xing, Qiang Wang, Congyan Lang,
Songhe Feng, and Yi Jin. Robust object tracking based on
temporal and spatial deep networks. In The IEEE Interna-
tional Conference on Computer Vision (ICCV), 2017.

[43] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,
Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and

Serge Belongie. The inaturalist species classiﬁcation and de-
tection dataset. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018.

[44] Xiang Wang, Shaodi You, Xi Li, and Huimin Ma. Weakly-
supervised semantic segmentation by iteratively mining
common object features. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018.

[45] Yaming Wang, Vlad I. Morariu, and Larry S. Davis. Learn-
ing a discriminative ﬁlter bank within a cnn for ﬁne-grained
recognition.
In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.

[46] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and
Liang Lin. Multi-label image recognition by recurrently dis-
covering attentional regions.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
2017.

[47] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-
longie, and P. Perona. Caltech-UCSD Birds 200. Technical
Report CNS-TR-2010-001, California Institute of Technolo-
gy, 2010.

[48] Chaojian Yu, Xinyi Zhao, Qi Zheng, Peng Zhang, and Xinge
You. Hierarchical bilinear pooling for ﬁne-grained visual
recognition. In The European Conference on Computer Vi-
sion (ECCV), 2018.

[49] Matthew D. Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In Computer Vision - EC-
CV 2014 - 13th European Conference, Zurich, Switzerland,
September 6-12, 2014, Proceedings, Part I, 2014.

[50] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Dar-
rell. Part-based r-cnns for ﬁne-grained category detection.
In European conference on computer vision, pages 834–849.
Springer, 2014.

[51] Ning Zhang, Ryan Farrell, Forrest Iandola, and Trevor Dar-
rell. Deformable part descriptors for ﬁne-grained recognition
and attribute prediction. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, 2013.

[52] Heliang Zheng, Jianlong Fu, Tao Mei, and Jiebo Luo. Learn-
ing multi-attention convolutional neural network for ﬁne-
grained image recognition.
In IEEE International Confer-
ence on Computer Vision, ICCV 2017, Venice, Italy, October
22-29, 2017, 2017.

[53] Heliang Zheng, Jianlong Fu, Tao Mei, and Jiebo Luo. Learn-
ing multi-attention convolutional neural network for ﬁne-
grained image recognition.
In IEEE International Confer-
ence on Computer Vision, ICCV 2017, Venice, Italy, October
22-29, 2017, 2017.

[54] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2016.

3043

