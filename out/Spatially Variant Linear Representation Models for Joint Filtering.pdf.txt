Spatially Variant Linear Representation Models for Joint Filtering

Jinshan Pan1

Jiangxin Dong2

Jimmy Ren3

Liang Lin4

Jinhui Tang1 Ming-Hsuan Yang5,6

1Nanjing University of Science and Technology

3SenseTime Research

4Sun Yat-Sen University

2Dalian University of Technology
6Google Cloud

5UC Merced

(a) Input & guidance

(b) Pan et al. [37]

(c) He et al. [18]

(d) Ours

(e) Estimated coefﬁcients

Figure 1. One application of the proposed joint ﬁltering on image deblurring. Our algorithm is based on a spatially variant linear repre-
sentation model (SVLRM), where the target image (i.e., the deblurred image (d)) can be linearly represented by the guidance image (i.e.,
the short exposure image in (a)). We develop an efﬁcient algorithm to estimate the linear representation coefﬁcients (i.e., (e)) by a deep
convolutional neural network which is constrained by the SVLRM. Our analysis shows that the SVLRM is able to capture the structural
details of the input and guidance image well (see (e)). Thus, our method generates better results than those based on the locally linear
representation model (e.g., [18]) and favorable results against the state-of-the-art methods on each task (e.g., image deblurring [37]).

Abstract

Joint ﬁltering mainly uses an additional guidance image
as a prior and transfers its structures to the target image
in the ﬁltering process. Different from existing algorithm-
s that rely on locally linear models or hand-designed ob-
jective functions to extract the structural information from
the guidance image, we propose a new joint ﬁlter based
on a spatially variant linear representation model (SVLR-
M), where the target image is linearly represented by the
guidance image. However, the SVLRM leads to a highly ill-
posed problem. To estimate the linear representation coef-
ﬁcients, we develop an effective algorithm based on a deep
convolutional neural network (CNN). The proposed deep C-
NN (constrained by the SVLRM) is able to estimate the s-
patially variant linear representation coefﬁcients which are
able to model the structural information of both the guid-
ance and input images. We show that the proposed algo-
rithm can be effectively applied to a variety of application-
s, including depth/RGB image upsampling and restoration,
ﬂash/no-ﬂash image deblurring, natural image denoising,
scale-aware ﬁltering, etc. Extensive experimental results
demonstrate that the proposed algorithm performs favor-
ably against state-of-the-art methods that have been spe-
cially designed for each task.

1. Introduction

Image ﬁlters, as fundamental tools in many vision and
graphics problems, are mainly used to suppress ﬁne-scale
details while preserving primary structures. The linear
translation-invariant (LTI) ﬁlters usually use spatially in-

variant kernels such as mean, Gaussian, and Laplacian k-
ernels. As the spatially invariant kernels are independent of
image content, these LTI ﬁlters usually smooth image struc-
tures, details, and noise evenly without discrimination and
thus are less effective for preserving main structures [53].

To overcome this problem, joint ﬁltering using addition-
al guidance images has been proposed. Joint ﬁltering aims
to transfer the important structural details of the guidance
image to the output image so that the important structures
of the output image can be preserved in the ﬁltering pro-
cess. As the guidance image can be the input image itself
or the image from different domains [18, 25, 47], joint ﬁl-
tering has been widely used in image editing [27], optical
ﬂow [49, 40, 45], stereo matching [40, 43, 31]. Although
achieving impressive performance, the joint ﬁltering usual-
ly introduces erroneous or extraneous artifacts in the target
image when the guidance image and input image are from
different domains, such as RGB/depth [56, 39, 12, 25], op-
tical ﬂow/RGB [49, 40, 45], ﬂash/no-ﬂash [54, 18]. Thus,
it is of great interest to explore the proprieties of guidance
image and input image so that the correct structural infor-
mation can be transferred in the ﬁltering process.

To explore common structures between the input and
guidance images, existing joint ﬁltering methods [43, 17,
16, 22] usually develop kinds of hand-crafted priors to mod-
el the structural co-occurrence property. However, using
hand-crafted priors usually leads to complex objective func-
tions, which are difﬁcult to solve.

Motivated by the success of deep learning, the joint ﬁlter-

11702

ing algorithms based on deep convolutional neural networks
(CNNs) [41, 28, 15, 19, 48] have been proposed. These al-
gorithms are efﬁcient and usually outperform conventional
methods by large margins. However, they are less effective
as using deep CNNs to directly predict the target images
may not explore the useful structural details from the guid-
ance image well.

Different from existing methods, we propose a novel
joint ﬁltering algorithm based on a spatially variant linear
representation model (SVLRM). Instead of directly predict-
ing the target image using a deep CNN, we learn a deep
CNN to estimate the spatially variant linear representation-
s coefﬁcients which model the structural information of the
guidance and input images and are then used to generate the
target image. We show that the proposed algorithm is able
to transfer the meaningful structural details of the guidance
and input images to the target image and can be applied to
a variety of applications, including depth/RGB image up-
sampling and restoration, ﬂash/no-ﬂash image deblurring,
natural image denoising, scale-aware ﬁltering, etc. Figure 1
shows a ﬂash/no-ﬂash image deblurring example where the
proposed method generates a clearer image.

The contributions of this work are as follows: (1) we pro-
pose the SVLRM for joint ﬁltering, where the target image
can be represented by the guidance image with the SVLRM;
(2) we develop an efﬁcient optimization method based on a
deep CNN (which is constrained by the SVLRM) to esti-
mate the spatially variant linear representation coefﬁcients.
Moreover, we analyze that the estimated coefﬁcients model
the structural details of input image and guidance well and
can determine whether the structures should be transferred
to the target image or not; (3) our algorithm achieves state-
of-the-art performance on a variety of applications includ-
ing depth/RGB image upsampling and restoration, ﬂash/no-
ﬂash image deblurring, natural image denoising, and scale-
aware ﬁltering.

2. Related Work

In this section, we discuss the joint ﬁltering methods

most related to this work within proper contexts.

Local joint ﬁltering methods. The representative local
joint ﬁltering methods include the bilateral ﬁlter (BF) [47,
3, 10], guided ﬁlter (GF) [18], weighted median ﬁlter (WM-
F) [31, 59], geodesic distance-based ﬁlter [5, 13], weighted
mode ﬁlter [33], the rolling guidance ﬁlter [58], and the mu-
tual structure-based joint ﬁlter [43], etc. In these methods,
the target image is usually computed by a weighted average
of neighboring pixels in the input image. The main success
of these methods is due to the use of the locally linear mod-
el or different types of afﬁnities among neighboring pixels.
For example, the bilateral ﬁlter deﬁnes the afﬁnity by col-
or difference and spatial distance of the neighboring pixels;
the guided ﬁlter assumes that the target image can be lin-
early represented by the guidance image in a local image

patch. However, these algorithms usually introduce erro-
neous or extraneous structures into the target image as only
the local structures of the guidance image are explored. Al-
though the common structure has been explored by [43] to
solve this problem, this method usually introduces halo ar-
tifacts due to the use of the locally linear model [18].
Global joint ﬁltering methods. The global joint ﬁlter-
ing algorithms are usually achieved by solving global ob-
jective functions. These methods design different hand-
crafted priors to enforce the target images to have simi-
lar structures with the guidance images, e.g., the weighted
least squares (WLS) ﬁlter [11], total generalized variation
(TGV) [12], L0-regularized prior [51], relative total vari-
ation (RTV) [53], scale map scheme [54], and improved
RTV [16], etc. Different from the local ﬁlters, the algo-
rithms based on such priors are able to exploit global struc-
tures in the guidance image. However, using hand-crafted
priors may not reﬂect inherent structural details in the target
image. In addition, the objective functions with such priors
usually lead to highly non-convex problems which cannot
be efﬁciently solved.

Deep learning-based methods. Image ﬁltering has been
signiﬁcantly advanced due to the use of deep CNNs. Sev-
eral algorithms use deep CNNs to approximate a number of
ﬁlters [52, 29, 35, 55, 4, 21, 14]. To deal with depth im-
age upsampling, Hui et al. [19] develop a CNN based on a
multi-scale guidance strategy. In [15], Gu et al. use a CNN
based on a weighted representation model to dynamically
learn structural details from guidance images for the depth
image restoration. However, these algorithms are limited to
speciﬁc application domains. Motivated by the success of
GF [18], Wu et al. [48] develop a CNN to approximate the
guided image ﬁlter. Li et al. [28] propose a joint ﬁlter based
on an end-to-end trainable network, where the structural in-
formation from the guidance image and input image are ex-
tracted based on independent CNNs. However, using deep
CNNs to estimate target images in a regression way does
not always help the structural details transferring as some
structural details are smoothed in the convolution process.

Different from existing methods, we propose a joint ﬁl-
tering algorithm based on a SVLRM. The structural infor-
mation of the guidance image and input image can be effec-
tively transferred to the target image constrained by the s-
patially variant linear representation coefﬁcients, which are
estimated by a deep CNN.

3. Revisiting Guided Image Filtering

To motivate our work, we ﬁrst revisit the guided image

ﬁlter [18] and then discuss its role in the ﬁltering process.

Let G, I, and F denote the guidance image, input image,
and target image, respectively. The guided ﬁlter assumes
that the value of the ﬁltered image F at pixel x is represent-
ed by the locally linear model,

F (x) = akG(x) + bk,

x ∈ ωk.

(1)

1703

(a) Guidance

(b) ¯a

(c) ¯b

(d) He et al. [18]

(e) Noisy input

(f) α(G, I)

(g) β(G, I)

(h) Ours

Figure 2. Problems of the locally linear representation model (1)
in joint ﬁltering. As shown in (b) and (c), the linear representation
coefﬁcients by local image patches do not model the structural
information of the guidance image well. By applying the linear
representation model (3), the target image (d) contains extraneous
textures, and the edges are not preserved well.

The coefﬁcients ak and bk are assumed to be constant in
each image patch ωk. By introducing a constraint of ak
(i.e., a2

k in [18]), ak and bk can be obtained by solving

min

ak,bk Xx∈ωk(cid:0)(akG(x) + bk − I(x))2 + γa2
k(cid:1) ,

(2)

where γ is a positive weight parameter. We note that ak and
bk can be easily obtained as (2) is a least squares problem.
With ak and bk in each local image patch, the mean ﬁlter is
then used to estimate pixel-wise linear coefﬁcients ¯a and ¯b.
Finally, the target image is obtained by

As the target image F is mainly determined by the rep-
resentation coefﬁcients ¯a and ¯b, ideal representation coefﬁ-
cients should model the structural details of both guidance
and input images well so that they can determine whether
the structures of the guidance image G should be transferred
to the target image or not.

To solve this problem, we propose a SVLRM and devel-
op a deep CNN to estimate the linear representation coefﬁ-
cients. Figure 2(f) and (g) shows that the estimated spatial-
ly variant linear representation coefﬁcients model the struc-
tural information of guidance and input image well, which
accordingly leads to a better target image.

4. Proposed Algorithm

In this section, we ﬁrst present the SVLRM and then pro-
pose an efﬁcient algorithm based on a deep CNN to solve it
for joint ﬁltering.

4.1. Spatially variant linear representation model

Different from the locally linear model (1), we assume

that the target image F can be represented by

F = α(G, I)G + β(G, I),

(5)

where α(G, I) and β(G, I) are the spatially variant linear
representation coefﬁcients which are determined by G and
I. The coefﬁcients α(G, I) and β(G, I) could determine
whether the structural details in G and I should be trans-
ferred to F or not (Figure 2(f) and (g)).

F (x) = ¯a(x)G(x) + ¯b(x).

(3)

4.2. Optimization

Although the ﬁltering algorithm with the locally linear
model (1) has been demonstrated effective in lots of appli-
cations, the assumption of constant ak and bk in each image
patch usually introduces extraneous textures in the target
images (see the parts enclosed in the red and blue boxes of
Figure 2(d)). We note that the gradient of the target image
and guidance image in each image patch should satisfy

∇F (x) = ak∇G(x),

x ∈ ωk,

(4)

according to (1). This constraint ensures that the target im-
age has the similar structures to the guidance image as ak is
a constant. Therefore, the structural details of G are directly
transferred to the target image F , which accordingly leads
to a target image with extraneous structures from G.

In addition, as the mean ﬁlter is further applied to ob-
tain the pixel-wise representation coefﬁcients ¯a and ¯b, this
will suppress the high-frequency information which usually
corresponds to the important structural details in the guid-
ance image. The parts enclosed in the green boxes in Fig-
ure 2(b) and (c) show that the representation coefﬁcients
¯a and ¯b are over-smoothed. Thus, using such representa-
tion coefﬁcients accordingly interferes the structures of the
target image (e.g., the edges enclosed in the green box in
Figure 2(d) are not sharp).

Without the assumption of the local constant representa-
tion coefﬁcients, estimating α(G, I) and β(G, I) from (5)
is quite challenging as (5) is highly ill-posed. A common
approach is to use the regularization w.r.t. α(G, I) and
β(G, I) to minimize the following objective function

E(α, β) = kαG + β − Ik2 + ϕ(α) + φ(β),

(6)

where ϕ(α) and φ(β) are the constraints of α(G, I) and
β(G, I).
If ϕ(α) and φ(β) are differentiable, the prob-
lem (6) can be solved by gradient descent:

αt = αt−1 − λ(cid:18) ∂E(α, βt−1)
βt = βt−1 − λ(cid:18) ∂E(αt−1, β)

∂α

∂β

(cid:19)α=αt−1
(cid:19)β=βt−1

,

,

(7a)

(7b)

where λ and t denote the step size and the iteration number.
However, it is not trivial to determine ϕ(α) and φ(β) for
joint ﬁltering as the properties of α(G, I) and β(G, I) are
quite different from the statistical properties of natural im-
ages [18, 43]. Instead of using hand-crafted constraints for
α(G, I) and β(G, I), we propose a deep CNN to estimate
α(G, I) and β(G, I) based on the SVLRM (5).

1704

Table 1. Quantitative evaluations for the depth image upsampling problem on the synthetic benchmark dataset [44] in terms of RMSE.

Methods
×4
×8
×16

Bicubic MRF [7]

GF [18]

JBU [25]

TGV [12]

3D-TOF [39]

SDF [17]

FBS [1]

DMSG [19]

DJF [28]

8.16
14.22
22.32

7.84
13.98
22.20

7.32
13.62
22.03

4.07
8.29
13.35

6.98
11.23
28.13

5.21
9.56
18.10

5.27
12.31
19.24

4.29
8.94
14.59

3.78
6.37
11.16

3.54
6.20
10.21

Ours
1.74
5.59
7.23

Learning. We note that (7) is in spirit similar to the stochas-
tic gradient descent which is widely used to solve deep C-
NNs. This motivates us to develop a deep CNN to estimate
α(G, I) and β(G, I).
Let {Gn, I n, F n
gt}N

n=1 denote a set of N training samples
and F denote the deep CNN. Our goal is to learn the net-
work parameters Θ = {Θα, Θβ} so that FΘα and FΘβ are
able to approximate the spatially variant linear coefﬁcients
α(G, I) and β(G, I).

(8)

M (5), which is deﬁned as

To this end, we constrain the network F by the SVLR-
FΘ(Gn; I n) = FΘα (Gn; I n)Gn + FΘβ (Gn; I n),

where FΘα (Gn; I n) and FΘβ (Gn; I n) are the results of the
network F w.r.t. parameters Θα and Θβ.
In the training process, we use the L1-norm as the loss
function to constrain the network F , which is deﬁned as
L(FΘ(Gn; I n); Fgt) =
As the L1-norm is non-differentiable, we use the Charbon-
nier penalty function ρ(x) = √x2 + ε2 to approximate it.

Xn=1kFΘ(Gn; I n) − F n

gtk1.

(9)

N

At each training iteration, the gradients of the loss func-

,

(10a)

+ ε2

=

∂L
∂FΘβ
Based on (10), we update the network parameters by

Xn=1

+ ε2

.

(10b)

tion w.r.t. FΘα and FΘβ are

N

∂L
∂FΘα

=

Xn=1

N

gt

Gn(cid:0)FΘ(Gn; I n) − F n
gt(cid:1)
q(cid:0)FΘ(Gn; I n) − F n
gt(cid:1)2
FΘ(Gn; I n) − F n
q(cid:0)FΘ(Gn; I n) − F n
gt(cid:1)2
∂L
∂FΘα
∂L
∂FΘβ

∂FΘα
∂Θα
∂FΘβ
∂Θβ

α − λ

β − λ

,

.

Θt

α = Θt−1

Θt

β = Θt−1

(11a)

(11b)

After obtaining {Θα, Θβ}, we set the spatially variant lin-
ear coefﬁcients α(G, I) and β(G, I) to be FΘα (G; I) and
FΘβ (G; I). Finally, the target image can be obtained by (5).
We empirically ﬁnd that using deep CNNs to estimate
α(G, I) and β(G, I) is effective (Section 5). More detailed
analysis about this is included in Section 6.
Network architecture. Based on above considerations, we
can use existing network architectures to deﬁne the network
F . In this work, we use a CNN with 12 convolution layers.
The ﬁlter size is set to be 3 × 3 pixels, and the stride value
is set to be 1. The feature number at the ﬁrst 11 convolution
layers are set to be 64. Each convolution layer is followed
by ReLU except the ﬁnal convolution layer.

5. Experimental Results

We evaluate the proposed algorithm on several appli-
cations including depth image upsampling, depth image
restoration, scale-aware ﬁltering, natural image denoising,
and ﬂash image deblurring. The main results are presented
in this section, and more results can be found in the sup-
plemental material. The code is publicly available on the
authors’ websites.

5.1. Parameter settings

In the learning process, we introduce the momentum
when updating (11) and use the ADAM optimizer [24] with
parameters β1 = 0.9, β2 = 0.999, and ǫ = 10−4. The batch
size is set to be 20. The step size λ (i.e., learning rate) is ini-
tialized as 10−4 which is halved at every minibatch update.
The parameter ε is set to be 10−3.

5.2. Depth image upsampling

Training data. For depth image upsampling, we random-
ly choose 1000 RGB/D image pairs from the NYU depth
dataset [44] as the training dataset and follow the proto-
cols of [28] to generate the training data. To evaluate the
proposed method, we use the remaining 449 RGB/D image
pairs [28] as the test dataset.

We quantitatively and qualitatively evaluate the proposed
algorithm against state-of-the-art methods including MR-
F [7], GF [18], JBU [25], TGV [12], 3D-TOF [39], SD-
F [17], FBS [1], DMSG [19], and DJF [28]. The quantita-
tive evaluations in Table 1 show that the proposed algorithm
performs favorably against state-of-the-art methods.

We show one example from the test dataset in Figure 3.
As the GF algorithm [18] is likely to transfer the textures
of the guidance image to the depth image according to our
analysis in Section 3, the generated result in Figure 3(e)
contains extraneous details (e.g., the textures of the ﬂower-
s). We note that the DJF algorithm [28] uses deep CNNs to
learn the dynamic guidance features for joint image upsam-
pling. This algorithm ﬁrst concatenates the features of the
guidance image and input image and then uses a CNN [8]
to estimate the target image in a regression way. Howev-
er, we note the method [8] is less effective for structural
details restoration as evidenced by [23]. Thus, the edges
of the results by the DJF algorithm [28] are not well esti-
mated as shown in Figure 3(g). Different from the end-to-
end trainable CNN-based algorithms, the proposed algorith-
m explores the SVLRM for joint image ﬁltering and devel-
ops a deep CNN to estimate the representation coefﬁcients.
Under the guidance of the estimated coefﬁcients, the SVL-
RM is able to transfer the correct structural details of the

1705

(a) Guidance

(b) GT

(c) Bicubic

(d) JBU [25]

(e) GF [18]

(f) SDF [17]

(g) DJF [28]

(h) Ours

Figure 3. On the depth image upsampling application (×8). The proposed method generates the depth images with sharper boundaries.

guidance image and input image to the target image. Thus,
the sharp edges of the super-resolved depth image are p-
reserved well (Figure 3(h)), and the generated results have
lower RMSE values (Table 1). All of these indicate the ef-
fectiveness of the proposed algorithm.

Instead of using a deep CNN to directly estimates the target
image, the proposed algorithm predicts the target image by
the SVLRM, where the representation coefﬁcients are esti-
mated by a deep CNN. The generated image contains sharp
edges as shown in (Figure 4(h)).

5.3. Depth image restoration

5.4. Scale aware ﬁltering

The proposed algorithm can be applied to depth image

restoration.
Training data. To generate the training data for depth im-
age restoration, we use the same training dataset as used in
Section 5.2. For each ground truth depth image, we add
the Gaussian noise where the noise level ranges from 0 to
10%. We use the test dataset by [30] to evaluate the pro-
posed method, where the training dataset and test dataset
do not overlapped. For each test image, we add Gaussian
noise with a noise level of 8%.

Table 2 shows the quantitative evaluations against state-
of-the-art algorithms. Overall, the proposed method per-
forms favorably against state-of-the-art methods.

Figure 4 shows the depth image denoising results from
the evaluated methods. The GF algorithm [18] does not ef-
fectively preserve structures as shown in Figure 4(d). We
note that the MUJF algorithm by Shen et al. [43] uses mutu-
al structures of the input image and guidance image to avoid
the extraneous details in the depth images. However, this
method is still based on the locally linear assumption [18]
and uses the mean ﬁlter to compute the pixel-wise linear
representation coefﬁcients. Sharp edges in the restored re-
sults are not preserved well (Figure 4(e)) because of less
accurate linear representation coefﬁcients. The MUGIF al-
gorithm [16] develops relative structures for joint ﬁltering
and generates a better depth image compared to [43]. How-
ever, this method does not preserve the sharp edges well
as shown in the red boxes of Figure 4(f). The DJF algo-
rithm [28] is able to preserve sharp edges. However, the
restored result contains signiﬁcant artifacts (Figure 4(g)).

With the trained models of the depth image denoising,
we show that the proposed algorithm can be straightfor-
wardly applied to scale-aware ﬁltering. Similar to the DJF
algorithm [28], we use the input image itself as the guid-
ance image and adopt the rolling guidance strategy [58] to
remove small-scale structures and details.

Figure 5 shows an example from [53]. The goal of
the scale-aware ﬁltering is to extract meaningful structures
from textured surfaces. However, the DJF algorithm [28]
and RGF algorithm [58] do not remove the small textures
from the input images. The backgrounds of the target im-
ages by these two algorithms still contain small scale struc-
tures.
In contrast, the proposed algorithm removes the
small-scale structures from the input images and generates
competitive results compared to [53].

5.5. Natural image denoising

As the guidance image can be the input image itself, we
evaluate the proposed algorithm on single natural image de-
noising.

Training data. To generate training data, we use the train-
ing dataset from the BSDS500 dataset [32]. For each clear
image, we randomly add the Gaussian noise where the noise
level ranges from 0 to 10%. The obtained noisy images are
as the inputs of the network. We use the test dataset with
200 clean images by [32] to evaluate the proposed method.
The Gaussian noise with a random noise level of 0 to 10%
is added to each test image.

We quantitatively and qualitatively evaluate the pro-
posed algorithm against state-of-the-art methods including

1706

Table 2. Quantitative evaluations for the depth image restoration problem on the benchmark dataset [30] in terms of PSNR, SSIM, and
RMSE.

Methods
Avg. PSNRs
Avg. SSIMs
Avg. RMSEs

Input
22.03
0.1872
20.18

GF

JBU [25] MUJF [43] MUGIF [16]

DJF [28]

30.79
0.9214

7.75

26.08
0.7820
12.96

30.67
0.9282

7.76

34.07
0.9657

5.26

32.58
0.9016

6.14

Ours
36.44
0.9762
4.02

(a) Guidance

(b) GT

(c) Noisy input

(d) GF [18]

(e) MUJF [43]

(f) MUGIF [16]

(g) DJF [28]

(h) Ours

Figure 4. On the depth image restoration application. The parts enclosed in the red boxes in (f) are over-smoothed. The proposed method
generates the depth images with sharper boundaries.

(a) Input image

(b) DJF [28]

(c) RTV [53]

(d) RGF [58]

(e) Ours

Figure 5. On the scale-aware ﬁltering application. The comparisons in (b-d) are obtained from the reported results. The proposed algorithm
is able to remove small-scale structures while preserving the main sharp edges .

BM3D [6], EPLL [61], CSF [42], MLP [2], and IRCN-
N [57]. The quantitative evaluations shown in Table 3
demonstrate that the proposed algorithm is able to generate
high-quality images.

Figure 6 shows an example from the test dataset. The
structures of the restored results by state-of-the-art methods
are over-smoothed. In contrast, the proposed algorithm de-
velops a deep CNN to estimate the spatially variant linear
representation coefﬁcients which can determine whether the
structural details from the input image are transferred to the
target image. Thus, some main structures in the denoised
image are preserved well as shown in Figure 6(f).

5.6. Flash image deblurring

In [60], Zhuo et al., propose to deblur a no-ﬂash image
under the guidance of its ﬂash image. We show that the pro-
posed method can be applied to this problem. To generate
the training data for deblurring, we use the image enhance-
ment dataset by [20] as the ﬂash and no-ﬂash image pairs.

We use the algorithm by [9, 36] to generate blur kernels and
apply them to the no-ﬂash images to generate blurred im-
ages. Finally, we use 100,000 images to train the proposed
model.

We evaluate the proposed method using real examples
by [60] in Figure 7. As the blurred image contains signiﬁ-
cant blur, single image deblurring methods [50, 26, 34, 38]
do not recover clear images. The generated results still con-
tain signiﬁcant blur and artifacts. We note that using the
guidance image is able to help the deblurring problem as
shown in Figure 7(g). However, some structural details are
not estimated well because only the sparsity of gradient pri-
or is used in image restoration. In contrast, the proposed
algorithm is able to remove the blur and generates a clearer
image with ﬁne details (Figure 7(h)).

6. Analysis and Discussion

We have shown that the SVLRM with the coefﬁcients
learned by a deep CNN for joint ﬁltering outperforms state-

1707

Table 3. Quantitative evaluations for the image denoising problem on the BSDS dataset [32] in terms of PSNR, SSIM, and RMSE.

Methods
Avg. PSNRs
Avg. SSIMs
Avg. RMSEs

Input
27.23
0.6350
13.19

BM3D [6]

GF [18]

EPLL [61]

CSF [42] MLP [2]

IRCNN [57]

31.60
0.8765

6.90

20.35
0.6173
24.85

29.34
0.8000

8.96

30.10
0.8164

8.43

28.91
0.7854

9.39

31.86
0.8811

6.64

Ours
33.04
0.8957

6.32

(a) Noisy input

(b) EPLL [61]

(c) CSF [42]

(d) MLP [2]

(e) IRCNN [57]

(f) Ours

Figure 6. On the image denoising application. The proposed method generates the images with clearer structures.

(a) Blurred image

(b) Flash image

(c) Xu and Jia [50]

(d) Krishnan et al. [26]

(e) Pan et al. [34]

(f) Pan et al. [37]

(g) Zhuo et al. [60]

(h) Ours

Figure 7. On the image deblurring application. The proposed algorithm is able to generate effective representation coefﬁcients. Thus the
deblurred image contains clearer structures and textures.

of-the-art methods on a variety of applications. In this sec-
tion, we further analyze the effect of the proposed algorithm
and compare it with the most related methods.
Relation with locally linear model-based methods. Sev-
eral notable methods (e.g., [43]) improve the original GF al-
gorithm [18] based on the locally linear model (4). In [43],
Shen et al. use the mutual structures of the guidance image
and input image to estimate the linear representation coefﬁ-
cients ak and bk. The estimated linear representation coef-
ﬁcients contain sharper structures than those of GF [18] and
do not introduce additional textures. Thus, the text-copy ef-
fect is avoided from the comparisons in Figure 4(d) and (e).
However, as the mean ﬁlter used in the estimate of linear
representation coefﬁcients may smooth the important edge
information (Figure 8(b) and (e)), the smoothed structures
may affect the sharp edge restoration (Figure 4(e)).

In contrast, the proposed algorithm is based on the SVL-
RM. The representation coefﬁcients are estimated by a deep
CNN. Our estimated linear representation coefﬁcients in
Figure 8(c) and (f) are able to better model the structural
details of the guidance image and input image, thus facili-
tating depth image restoration (Figure 4(h)).
Effect of the spatially variant linear representation mod-
el.
Instead of directly using an end-to-end-trainable net-
work for joint ﬁltering, we propose a new algorithm that
learns the SVLRM. The SVLRM is able to capture the
structural information of guidance image to help joint ﬁl-
tering. To demonstrate the effect of the proposed linear rep-
resentation model, we compare it with the method by an
end-to-end trainable network (E2ETN). We disable the co-
efﬁcient learning step and directly estimate the desired out-
put in our implementation to ensure fair comparisons. As

1708

Table 4. Effect of the proposed SVLRM on image denoising.

Depth image denoising

HC

E2ETN

Ours

Natural image denoising
HC

E2ETN

Ours

(a) ¯a by [18]

(b) ¯a by [43]

(c) Proposed α(G, I)

(d) ¯b by [18]

(e) ¯b by [43]

(f) Proposed β(G, I)

Figure 8. The effect of the proposed SVLRM. The guidance image
and noisy image are shown in Figure 4(a) and (c). The proposed
deep CNN is able to learn the linear representation coefﬁcients
which contain the important structural information for joint ﬁlter-
ing (Best viewed on high-resolution display with zoom-in).

(a) Guidance

(b) Noisy input

(c) α(G, I) by (12)

(d) β(G, I) by (12)

(e) Proposed α(G, I)

(f) Proposed β(G, I)

(g) HC

(h) E2ETN

(i) Ours

Figure 9. The effect of the proposed SVLRM on depth image de-
noising (Best viewed on high-resolution display with zoom-in).

shown in Figure 9(h) and (i), the proposed linear represen-
tation model learning algorithm generates the results with
more sharp edges. In addition, the quantitative evaluation-
s in Table 4 show that the proposed linear representation
model consistently improves the performance1. All these

1The depth image denoising in Table 4 are tested on the dataset by [19],
where we add Gaussian noise with a noise level of 8% in each depth image.

Avg. PSNRs
Avg. SSIMs

21.84
0.2044

35.31
0.9633

35.98
0.9652

24.87
0.6452

32.44
0.8920

33.04
0.8957

Table 5. Run-time (seconds) performance. All the algorithms are
tested on the same machine using the depth image upsampling test
dataset.

Methods
Avg. run-time

JBU [25]

DMSG [19]

DJF [28]

4.8

0.78

1.04

Ours
0.08

results concretely demonstrate the effectiveness of the pro-
posed linear representation model learning algorithm.

We further note that one alternative approach to estimate
α(G, I) and β(G, I) is to use hand-crafted priors in (6).
Similar to [18, 46], we take ϕ(α) and φ(β) as µα2 and ηβ 2,
where µ and η are positive weight parameters. Thus, the
solutions of (6) are

α =

ηGI

ηG2 + µ + µη

,

β =

I − αG
1 + η

.

(12)

The derivations of (12) and algorithm details of are included
in the supplemental material. We empirically set µ and η to
be 0.1 on the depth image denoising and natural image de-
noising problems for fair comparisons. The proposed algo-
rithm with hand-crafted prior (HC for short in Table 4) does
not generate better results compared to the method with the
deep CNN, indicating the effectiveness of the deep CNN.
Moreover, the estimated coefﬁcients in Figure 9(c) and (d)
contain signiﬁcant noise, which accordingly leads to noisy
results (Figure 9(g)). In contrast, the proposed algorithm
denoises the image well.
Run-time performance. We benchmark the run-time of all
methods on a machine with an Intel Core i7-7700 CPU and
an NVIDIA GTX 1080Ti GPU. Table 5 shows that the pro-
posed algorithm performs more efﬁciently than other deep
learning-based approaches.

7. Concluding Remarks

In this paper, we have proposed a new joint ﬁlter based
on the SVLRM and developed an efﬁcient algorithm based
on a deep CNN to estimate the linear representation coef-
ﬁcients. The proposed CNN which is constrained by the
SVLRM is able to estimate the spatially variant linear rep-
resentation coefﬁcients. We show that the spatially vari-
ant linear representation coefﬁcients model the structural
information of both guidance image and input image well.
Thus, the linear representation model with the spatially vari-
ant representation coefﬁcients is able to transfer meaningful
structures to the target image. We show that the proposed
algorithm can be effectively applied to a variety of applica-
tions and performs favorably against state-of-the-art meth-
ods that have been specially designed for each task.
Acknowledgements. This work has been supported in part
by the NSFC (No. 61872421, 61732007), the NSF of Jiangsu
Province (No. BK20180471), and NSF CAREER (No. 1149783).

1709

References

[1] Jonathan T. Barron and Ben Poole. The fast bilateral solver.

In ECCV, pages 617–632, 2016. 4

[2] Harold Christopher Burger, Christian J. Schuler, and Stefan
Image denoising: Can plain neural networks
In CVPR, pages 2392–2399, 2012.

Harmeling.
compete with bm3d?
6, 7

[3] Jiawen Chen, Sylvain Paris, and Fr´edo Durand. Real-time
edge-aware image processing with the bilateral grid. ACM
TOG, 26(3):103, 2007. 2

[4] Qifeng Chen, Jia Xu, and Vladlen Koltun. Fast image pro-
cessing with fully-convolutional networks. In ICCV, pages
2516–2525, 2017. 2

[5] Antonio Criminisi, Toby Sharp, Carsten Rother, and Patrick
P´erez. Geodesic image and video editing. ACM TOG,
29(5):134:1–134:15, 2010. 2

[6] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and
Image denoising by sparse 3-
IEEE TIP,

Karen O. Egiazarian.
d transform-domain collaborative ﬁltering.
16(8):2080–2095, 2007. 6, 7

[7] James Diebel and Sebastian Thrun. An application of
markov random ﬁelds to range sensing. In NIPS, pages 291–
298, 2005. 4

[8] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Learning a deep convolutional network for image
super-resolution. In ECCV, pages 184–199, 2014. 4

[9] Jiangxin Dong, Jinshan Pan, Deqing Sun, Zhixun Su, and
Ming-Hsuan Yang. Learning data terms for non-blind de-
blurring. In ECCV, pages 777–792, 2018. 6

[10] Fr´edo Durand and Julie Dorsey. Fast bilateral ﬁltering for
the display of high-dynamic-range images. In SIGGRAPH,
pages 257–266, 2002. 2

[11] Zeev Farbman, Raanan Fattal, Dani Lischinski, and Richard
Szeliski. Edge-preserving decompositions for multi-scale
tone and detail manipulation. ACM TOG, 27(3):67:1–67:10,
2008. 2

[12] David Ferstl, Christian Reinbacher, Rene Ranftl, Matthias
R¨uther, and Horst Bischof. Image guided depth upsampling
using anisotropic total generalized variation. In ICCV, pages
993–1000, 2013. 1, 2, 4

[13] Eduardo Simoes Lopes Gastal and Manuel M. Oliveira. Do-
main transform for edge-aware image and video processing.
ACM TOG, 30(4):69:1–69:12, 2011. 2

[14] Micha¨el Gharbi,

Jiawen Chen,

Jonathan T. Barron,
Samuel W. Hasinoff, and Fr´edo Durand. Deep bilater-
al learning for real-time image enhancement. ACM TOG,
36(4):118:1–118:12, 2017. 2

[15] Shuhang Gu, Wangmeng Zuo, Shi Guo, Yunjin Chen,
Chongyu Chen, and Lei Zhang. Learning dynamic guidance
for depth image enhancement.
In CVPR, pages 712–721,
2017. 2

[16] Xiaojie Guo, Yu Li, and Jiayi Ma. Mutually guided image

ﬁltering. In ACM MM, pages 1283–1290, 2017. 1, 2, 5, 6

[17] Bumsub Ham, Minsu Cho, and Jean Ponce. Robust guided
IEEE TPAMI,

image ﬁltering using nonconvex potentials.
40(1):192–207, 2018. 1, 4, 5

[18] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image ﬁl-
tering. IEEE TPAMI, 35(6):1397–1409, 2013. 1, 2, 3, 4, 5,
6, 7, 8

[19] Tak-Wai Hui, Chen Change Loy, and Xiaoou Tang. Depth
map super-resolution by deep multi-scale guidance. In EC-
CV, pages 353–369, 2016. 2, 4, 8

[20] Andrey Ignatov, Radu Timofte, Thang Van Vu, Tung Minh
Luu, and et al. Pirm challenge on perceptual image enhance-
ment on smartphones: Report. In ECCV Workshops, 2018.
6

[21] Varun Jampani, Martin Kiefel, and Peter V. Gehler. Learning
sparse high dimensional ﬁlters: Image ﬁltering, dense crfs
and bilateral neural networks. In CVPR, pages 4452–4461,
2016. 2

[22] Roy Josef Jevnisek and Shai Avidan. Co-occurrence ﬁlter. In

CVPR, pages 3816–3824, 2017. 1

[23] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate
image super-resolution using very deep convolutional net-
works. In CVPR, pages 1646–1654, 2016. 4

[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. CoRR, abs/1412.6980, 2014. 4

[25] Johannes Kopf, Michael F. Cohen, Dani Lischinski, and
Joint bilateral upsampling. ACM

Matthew Uyttendaele.
TOG, 26(3):96, 2007. 1, 4, 5, 6, 8

[26] Dilip Krishnan, Terence Tay, and Rob Fergus. Blind de-
convolution using a normalized sparsity measure. In CVPR,
pages 2657–2664, 2011. 6, 7

[27] Anat Levin, Dani Lischinski, and Yair Weiss. A closed-form
solution to natural image matting. IEEE TPAMI, 30(2):228–
242, 2008. 1

[28] Yijun Li, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan
Yang. Deep joint image ﬁltering. In ECCV, pages 154–169,
2016. 2, 4, 5, 6, 8

[29] Sifei Liu, Jinshan Pan, and Ming-Hsuan Yang. Learning re-
cursive ﬁlters for low-level vision via a hybrid neural net-
work. In ECCV, pages 560–576, 2016. 2

[30] Si Lu, Xiaofeng Ren, and Feng Liu. Depth enhancement via
In CVPR, pages 3390–3397,

low-rank matrix completion.
2014. 5, 6

[31] Ziyang Ma, Kaiming He, Yichen Wei, Jian Sun, and En-
hua Wu. Constant time weighted median ﬁltering for stereo
matching and beyond. In ICCV, pages 49–56, 2013. 1, 2

[32] David R. Martin, Charless C. Fowlkes, Doron Tal, and Jiten-
dra Malik. A database of human segmented natural images
and its application to evaluating segmentation algorithms and
measuring ecological statistics.
In ICCV, pages 416–425,
2001. 5, 7

[33] Dongbo Min, Jiangbo Lu, and Minh N. Do. Depth video
enhancement based on weighted mode ﬁltering. IEEE TIP,
21(3):1176–1190, 2012. 2

[34] Jinshan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang. L0-
regularized intensity and gradient prior for deblurring text
images and beyond. IEEE TPAMI, 39(2):342–355, 2017. 6,
7

[35] Jinshan Pan, Sifei Liu, Deqing Sun, Jiawei Zhang, Yang Li-
u, Jimmy Ren, Zechao Li, Jinhui Tang, Huchuan Lu, Yu-
Wing Tai, and Ming-Hsuan Yang. Learning dual convolu-
tional neural networks for low-level vision. In CVPR, pages
3070–3079, 2018. 2

1710

[36] Jinshan Pan, Wenqi Ren, Zhe Hu, and Ming-Hsuan Yang.
IEEE TPAMI,

Learning to deblur images with exemplars.
2018. 6

[37] Jinshan Pan, Deqing Sun, Hanspeter Pﬁster, and Ming-
Hsuan Yang. Blind image deblurring using dark channel pri-
or. In CVPR, pages 1628–1636, 2016. 1, 7

[38] Jinshan Pan, Deqing Sun, Hanspeter Pﬁster, and Ming-
Hsuan Yang. Deblurring images via dark channel prior. IEEE
TPAMI, 40(10):2315–2328, 2018. 6

[39] Jaesik Park, Hyeongwoo Kim, Yu-Wing Tai, Michael S.
Brown, and In-So Kweon. High quality depth map upsam-
pling for 3d-tof cameras. In ICCV, pages 1623–1630, 2011.
1, 4

[40] Christoph Rhemann, Asmaa Hosni, Michael Bleyer, Carsten
Rother, and Margrit Gelautz. Fast cost-volume ﬁltering for
visual correspondence and beyond. In CVPR, pages 3017–
3024, 2011. 1

restoration via scale map. In ICCV, pages 1537–1544, 2013.
1, 2

[55] Zhicheng Yan, Hao Zhang, Baoyuan Wang, Sylvain Paris,
and Yizhou Yu. Automatic photo adjustment using deep neu-
ral networks. ACM TOG, 35(2):11:1–11:15, 2016. 2

[56] Qingxiong Yang, Ruigang Yang, James Davis, and David
Nist´er. Spatial-depth super resolution for range images. In
CVPR, 2007. 1

[57] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.
Learning deep CNN denoiser prior for image restoration. In
CVPR, pages 2808–2817, 2017. 6, 7

[58] Qi Zhang, Xiaoyong Shen, Li Xu, and Jiaya Jia. Rolling

guidance ﬁlter. In ECCV, pages 815–830, 2014. 2, 5, 6

[59] Qi Zhang, Li Xu, and Jiaya Jia. 100+ times faster weighted
median ﬁlter (WMF). In CVPR, pages 2830–2837, 2014. 2
[60] Shaojie Zhuo, Dong Guo, and Terence Sim. Robust ﬂash

deblurring. In CVPR, pages 2440–2447, 2010. 6, 7

[41] Gernot Riegler, David Ferstl, Matthias R¨uther, and Horst
Bischof. A deep primal-dual network for guided depth super-
resolution. In BMVC, 2016. 2

[61] Daniel Zoran and Yair Weiss. From learning models of natu-
ral image patches to whole image restoration. In ICCV, pages
479–486, 2011. 6, 7

[42] Uwe Schmidt and Stefan Roth. Shrinkage ﬁelds for effective

image restoration. In CVPR, pages 2774–2781, 2014. 6, 7

[43] Xiaoyong Shen, Chao Zhou, Li Xu, and Jiaya Jia. Mutual-
structure for joint ﬁltering. IJCV, 125(1-3):19–33, 2017. 1,
2, 3, 5, 6, 7, 8

[44] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Indoor segmentation and support inference from

Fergus.
RGBD images. In ECCV, pages 746–760, 2012. 4

[45] Deqing Sun, Stefan Roth, and Michael J. Black. Secrets of
optical ﬂow estimation and their principles. In CVPR, pages
2432–2439, 2010. 1

[46] Jinhui Tang, Xiangbo Shu, Guo-Jun Qi, Zechao Li, Meng
Wang, Shuicheng Yan, and Ramesh Jain. Tri-clustered ten-
sor completion for social-aware image tag reﬁnement. IEEE
TPAMI, 39(8):1662–1674, 2017. 8

[47] Carlo Tomasi and Roberto Manduchi. Bilateral ﬁltering for
gray and color images. In ICCV, pages 839–846, 1998. 1, 2
[48] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang.
In CVPR, pages

Fast end-to-end trainable guided ﬁlter.
1838–1847, 2018. 2

[49] Jiangjian Xiao, Hui Cheng, Harpreet S. Sawhney, Cen Rao,
and Michael A. Isnardi. Bilateral ﬁltering-based optical ﬂow
estimation with occlusion detection. In ECCV, pages 211–
224, 2006. 1

[50] Li Xu and Jiaya Jia. Two-phase kernel estimation for robust

motion deblurring. In ECCV, pages 157–170, 2010. 6, 7

[51] Li Xu, Cewu Lu, Yi Xu, and Jiaya Jia. Image smoothing via
L0 gradient minimization. ACM TOG, 30(6):174:1–174:12,
2011. 2

[52] Li Xu, Jimmy S. J. Ren, Qiong Yan, Renjie Liao, and Jiaya
Jia. Deep edge-aware ﬁlters. In ICML, pages 1669–1678,
2015. 2

[53] Li Xu, Qiong Yan, Yang Xia, and Jiaya Jia. Structure ex-
traction from texture via relative total variation. ACM TOG,
31(6):139:1–139:10, 2012. 1, 2, 5, 6

[54] Qiong Yan, Xiaoyong Shen, Li Xu, Shaojie Zhuo, Xiaopeng
Zhang, Liang Shen, and Jiaya Jia. Cross-ﬁeld joint image

1711

