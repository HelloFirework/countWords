Salient Object Detection with Pyramid Attention and Salient Edges

Wenguan Wang∗ 1, Shuyang Zhao ∗ 2, Jianbing Shen† 1,2, Steven C. H. Hoi 3,4, Ali Borji 5

1

Inception Institute of Artiﬁcial Intelligence, UAE 2Beijing Institute of Technology, China

3

Singapore Management University, Singapore

4Salesforce Research Asia, Singapore

5MarkableAI, USA

wenguanwang.ai@gmail.com, zsyprich@bit.edu.cn

shenjianbingcg@gmail.com, chhoi@smu.edu.sg, aliborji@gmail.com

Abstract

This paper presents a new method for detecting salient
objects in images using convolutional neural networks
(CNNs). The proposed network, named PAGE-Net, makes
two major novel contributions. The ﬁrst is to devise an
essential pyramid attention structure for salient object de-
tection, which enables the network to concentrate more on
salient regions while exploiting multi-scale saliency infor-
mation. Such a stacked attention design offers a power-
ful way to efﬁciently enhance the representation ability of
the corresponding network layer with an enlarged receptive
ﬁeld. The second contribution is to propose a salient edge
detection module, which lies in the emphasis on the impor-
tance of salient edge information since it offers a strong cue
to better segment salient objects and reﬁne object bound-
aries. Such a salient edge detection module learns for pre-
cise salient boundary estimation, and thus encourages bet-
ter edge-preserving salient object segmentation. Exhaus-
tive experiments show that both of the proposed pyramid
attention and salient edges are effective for salient object
detection, and our PAGE-Net outperforms state-of-the-art
approaches on several popular benchmarks with a fast in-
ference speed (25FPS on a single GPU).

1. Introduction

Salient Object Detection (SOD) refers to the problem of
locating and segmenting the most salient objects or regions
in an image. It can be widely applied for improving a vari-
ety of vision tasks, such as object proposal generation [2],
object segmentation [42, 44], photo cropping [45, 41], and
video object tracking [13], among others. SOD has been
extensively studied in computer vision. Traditional meth-
ods often design hand-crafted low-level features and make
heuristic hypothesizes [49, 17], which often fail to yield
satisfactory results for images with complex scenarios. Re-

∗Equal contribution.
†Corresponding author: Jianbing Shen.

cently, deep learning approaches have emerged as an impor-
tant trend for SOD and often reported signiﬁcant improve-
ments. Despite being studied actively, how to devise an ef-
fective yet efﬁcient deep neural network model for SOD re-
mains an open challenge.

In this paper, we propose a novel Pyramid Attentive and
salient edGE-aware saliency model, named PAGE-Net, for
saliency object detection, which is equipped with two key
modules: (1) a pyramid attention module that efﬁciently en-
hances saliency representations by accounting for the multi-
scale attention and enlarging receptive ﬁeld of the saliency
model; and (2) a salient edge detection module that ex-
plicitly learns salient object boundaries to better locate and
sharpen salient objects. The design of the proposed PAGE-
Net is motivated by the following two aspects.

First, feature representation is the crux of deep learn-
ing based saliency models, and it is always desirable to ex-
plore more efﬁcient strategies for approaching scale-space
feature learning problem. As witnessed in many saliency
studies [34, 57, 14], multi-scale saliency features are cru-
cial for SOD. As such, recent deep saliency models have
mainly focused on combining the outputs from intermedi-
ate network layers. Unlike the existing work, we propose
a novel pyramid attention model that inherits the feature-
enhancing ability of attention mechanisms, and explicitly
handles the problem of multi-scale saliency feature learn-
ing. Incorporating attention mechanisms into networks has
proven useful for selecting task-relevant features [33]. As
shown in Fig. 1, we extend attention mechanisms with hier-
archical structures to enhance saliency computation. Such
a design is signiﬁcant because it efﬁciently increases the
receptive ﬁeld of the convolution layer (even for a shallow
layer). Our saliency model is encouraged to focus on impor-
tant regions using multi-scale information (Fig. 1 (b)). With
pyramid attention, the background responses in the origi-
nal features (Fig. 1 (c)) are successfully suppressed, leading
to more discriminative saliency representations (Fig. 1 (d))
and better results (Fig. 1 (g)). Such an attention module
also provides an additional dimension of interpretability by

1448

Figure 1: Motivating examples and ideas for the proposed PAGE-Net. (a) Image. (b) Pyramid attention maps (§3.1). (c)
Original saliency features. (d) The reﬁned saliency features via applying the proposed pyramid attention in (b). (e) Detected
saliency edge map (§3.2). (f) Saliency results w/o. attention and salient edge detection modules. (g) Improved results via
considering pyramid attention and salient edge cue. (h) Ground truth saliency map.

explaining where our saliency model is looking at.

Second, it is also desirable to ﬁnd an effective means
of enhancing the sharpness of salient object detection re-
sults. CNNs are designed to produce hierarchical feature
maps through repeated pooling and subsampling opera-
tions, where higher layers gain larger receptive ﬁelds and
stronger representation ability but loose much detailed spa-
tial information. This can be useful for high-level tasks,
but unfortunately degrades the accuracy of low-level tasks
such as salient object segmentation where precise pixel-
wise activations are required, especially on salient object
boundaries. In the ﬁeld of salient object detection, although
densely connected or bottom-up/top-down network archi-
tectures [54, 14, 26] (see the scheme in Fig. 2 (a)) have
been extensively studied to gradually recover salient object
details in a top-down fashion, the issue of sharpness still re-
mains a challenge. Inspired by recent advances in semantic
segmentation [4, 6], we propose to equip saliency models
with a salient-edge detection module, specially designed to
detect the salient object boundaries. Thus, the network can
leverage more explicit salient edges (Fig. 1 (e)) to better lo-
cate salient objects and sharpen the results (Fig. 1 (g)).

In summary, our main contributions are three-fold: (i)
we present a pyramid attention model for discriminative
saliency representations with multi-scale feature learning
and an extended receptive ﬁeld (§3.1); (ii) we propose a
salient edge detection module that exploits salient edge
information explicitly for salient object detection (§3.2);
and (iii) we perform extensive experiments on six popular
benchmarks, i.e., ECCSD [49], DUT-OMRON [50], HKU-
IS [21], PASCAL-S [25], SOD [30] and DUTS-TE [35], in
which the proposed deep saliency model yields consistent
improvements over a number of strong baselines. Finally,
the proposed model runs very fast on modern GPUs, achiev-
ing a real-time inference speed of 25FPS.

2. Related Work

2.1. Salient Object Detection

The pioneering work for salient object detection can
be dated back to Liu et al., [28] and Achanta et al., [1].
Since then, numerous subsequent works have been reported,

mainly using contrast based assumption [9, 49, 17] and
background prior [46, 58]. These early methods [43, 10]
often heavily rely on hand-crafted features and heuristic hy-
pothesizes.

Recently, due to the great successes of CNNs in com-
puter vision, deep learning has emerged as a promising al-
ternative for SOD. CNN-based saliency models allow ﬂex-
ible saliency representations with a powerful end-to-end
learning ability, thus achieving signiﬁcantly better perfor-
mance than classic methods. A variety of deep learning
approaches have been proposed in literature. For exam-
ple, some methods integrate deep learning models with
hand-crafted features [20], heuristic saliency priors [36],
level set [15], contextual information [57], or explicit vi-
sual ﬁxation [40]. Other methods leverage global and local
saliency information [21, 34, 54, 29], combine pixel- and
segment-level features [22], inspire connections between
network layers [14], or explore more complex deep archi-
tectures [18, 26, 55, 37, 32].

One distinct difference of our method from the existing
studies lies in the salient-edge-preservation property. Cur-
rent saliency network architectures tend to stack multi-layer
features. Although the ﬁnal prediction layer accesses multi-
scale and multi-level information and produces more pre-
cise saliency segmentation, the issue of sharpening remains
unsolved due to the smoothness of convolution kernel and
downsampling of spatial pooling. Some post-processing
heuristics [36, 14, 22] have been adopted, but few ex-
plores how to embed salient edge information into a deep
saliency model via end-to-end training. A few recent meth-
ods [53, 23] also explored the boundary cues, but they are
very different from ours. For example, Zhang [53] et al.
simply used an extra loss to emphasize the detection error
for the pixels within the salient object boundaries. In [23],
they considered semantic contour information from a pre-
trained contour detector [51]. By contrast, we extend each
side-out layer with a salient edge detection module and
learn the combination of edge and object information end-
to-end.

1449

Figure 2: Architecture designs of the proposed PAGE-Net. (a) Typical bottom-up/top-down network architecture used in
previous saliency methods. (b) PAGE-Net is equipped with two essential modules: pyramid attention module and salient
edge detection module. (c) Architecture of the pyramid attention module (§3.1), where the attention is learned for enhancing
saliency representation in multi-scales. (d) The pyramid attention module assigns corresponding convolution layer a global
view with increased receptive ﬁeld. (e) The edge detection module (§3.2) offers explicit edge information, which is used for
locating salient objects and sharping salient object boundary.

2.2. Trainable Attention Mechanism in Network

Attention mechanisms of deep neural networks have
been actively studied recently, which was ﬁrst proposed by
Bahdanau et al. [3] for neural machine translation. Later,
it was proven useful in many natural language processing
and vision tasks, e.g., caption generation [48], question an-
swering [52], and scene recognition [5, 33], among others.
In such studies, attention is learned in an automatic, top-
down, and goal-driven way, allowing the network to focus
on the most task-relevant parts of images or sentences. Only
a few very recent methods for SOD [56, 27, 7] employ at-
tention networks. But our approach very differs from theirs,
as they often only consider a single-layer attention design.
In our approach, for each convolution layer, a pyramid of at-
tentions is equipped for essentially learning to assign higher
importance to salient regions while simultaneously address-
ing the issue of multi-scale learning. More importantly,
such a pyramid attention design enables our model with a
global view and improved learning ability via an enlarged
receptive ﬁled.

3. Our Method

Fig. 2 (b) gives a simpliﬁed illustration of PAGE-Net,
which consists of three components: a backbone network
for feature extraction, a pyramid attention module, and a
salient edge detection module. We begin by describing our
in Fig. 2 (b)) in §3.1. A
pyramid attention module (
detailed description of our salient edge detection module
in Fig. 2 (b)) is proved in §3.2. Finally, in §3.3, we
(
present more implementation details.

3.1. Pyramid Attention Module

For each saliency network layer, a pyramid attention
module is ﬁrst incorporated to generate a more discrimina-

tive feature representation. In contrast to previous saliency
models that treat all positions of saliency features equally,
our model focuses on the features in important regions and
considers multi-scale information. This is achieved using
a stacked attention architecture: multiple attention layers
built upon multi-scale features are stacked to form a uniﬁed
pyramid attention model.

More technically, let X denote a 3D feature tensor from
in Fig. 2 (c)).
a convolution layer of a saliency network (
This typically consists of C channels of width M and
height M : X ∈ RM ×M ×C . Our goal is to learn a set
of equally-spatial-sized attention masks that softly weight
output saliency features X based on multi-scale informa-
tion. Essentially, we obtain multi-scale features by grad-
ually down-sampling X into multiple-resolutions {Xn :
Xn∈R
2n×C, n = 1, 2, 3, . . . , N } with N steps. For Xn
within a certain scale n, we use a soft attention mecha-
nism [48] that predicts an importance map l ∈ [0, 1]
2n .
Speciﬁcally, a softmax operation is applied over M
2n spa-
tial locations. The location softmax can be thought of as the
probability with which our model believes the correspond-
ing region in the input feature is important. It is deﬁned as:

2n×M

M
2n ×

M
2n×

M

M

exp(Wn
i

Xn
i )

ln
i = p(L = i|Xn) =

,

(1)

M

2n × M
2n
j=1

P
where i ∈ 1, . . . , M
2n , Wn
i are the weights of the hidden
layer that maps to the i-th element of the location softmax,
L is a random variable which can take 1-of- M
2n values.

2n × M

2n × M

exp(Wn
j

Xn
i )

M
2n

M
2n ×
i

l is the attention map, where P
li = 1. Through
the operations above, our model learns a normalized im-
portance weight (attention map) for each region at a certain
scale (
in Fig. 2 (c)). This is essential for saliency rep-
resentation since salient areas should have higher weights.
n=1 over all

Once the attention probabilities {ln}N

1450

Figure 3: Illustration of our pyramid attention module. (a) Shows the work-ﬂow of our attention module. (d) Gives the
attention hierarchy that captures multi-scale information and emphasizes important regions. Comparing the features in (c)
and (e), we ﬁnd that the background responses have been successfully suppressed by the attention module. (f) and (g) show
the results before/after applying attention. It can be observed that the PAGE-Net generates more accurate results through the
attention module. See §3.1 for more details.

{Xn}N
n=1 are obtained, upsampling operations are adopted
{l′n ∈
to resize them to their original resolutions:
[0, 1]M ×M }N
n=1. Fig. 3 offers a more detailed illustra-
tion of our attention module. Clearly, these attention maps
(Fig. 3 (d)) correspond to different resolutions and can re-
veal important regions. More importantly, the pyramid at-
tention module is equipped with stacked pooling opera-
tions, dramatically improving the receptive ﬁeld of the cor-
responding feature extraction layer.

After calculating these importance probabilities,

the
original feature representation X is improved by accounting
for the expectation of the feature slices in different regions:

Yj =

1

N XN

n=1

l′n

j Xj ,

j ∈ 1, . . . , M × M,

(2)

where Y is the updated feature and Yj is the j-th slice of
the feature cube. Here, the model computes the expected
value of the inputs by taking the expectation over the im-
age features in different regions. Our attention module not
only serves to enhance saliency representations in a focused
location, but also accounts for multi-scale information. As
discussed in [33], the features reﬁned by the attention map
usually have a large number of values close to zero. Thus,
a stack of many reﬁned features makes back-propagation
difﬁcult. To solve this, we apply identity mapping [12] in
Eq. 2:

Yj =

1

N XN

n=1

(1 + l′n

j )Xj ,

j ∈ 1, . . . , M × M.

(3)

Even with a very small attention (l′
j ≈ 0), information from
the original feature X will still be preserved by residual con-
nection. As demonstrated in Fig. 3 (c) and (e), the atten-
tion module is able to enhance the feature map for more ef-
fective saliency representation. Such pyramid attention ar-
chitecture provides a feasible method of assigning a global

view of each corresponding convolution layer (with a sig-
niﬁcantly enlarged receptive ﬁeld; see Fig. 2 (d)). A more
detailed architecture of the attention module is presented in
§3.3.
Discussion. Features from different positions do not con-
tribute equally to saliency computation. Hence, we intro-
duce the attention mechanism to focus on those positions
most essential to the nature of salient objects. With our de-
sign, the attention module can quickly collect multi-scale
information by iteratively downsampling the feature maps.
Such a pyramid structure enables the receptive ﬁeld of the
feature layer to be easily and rapidly enlarged. Compared
to previous attentive models, our pyramid attention is more
favorable due to its effective use of multi-scale features and
powerful representations with enlarged receptive ﬁelds, all
of which are essential for pixel-wise saliency estimation.

3.2. Salient Edge Detector

With the reﬁned saliency features Y, a saliency map can
be generated by directly feeding Y into a small stack of con-
volution layers with sigmoid, as done in previous methods.
However, we observed that the detection cannot produce a
clear boundary between the salient objects and the back-
ground (see Fig. 4 (b)). This is mainly due to the smooth-
ness of the convolution kernel and the downsampling of the
pooling layers. To deal with this, we design an extra salient
edge detection module (see Fig. 2 (d)) to force the network
to emphasize the saliency boundary alignment and learn to
reﬁne saliency maps with the use of salient edge informa-
tion.

Let {(Ik, Gk, Pk)}K

k=1 denote the training data, where
Ik, Gk, and Pk are the color image, the corresponding
ground truth saliency map and the salient object boundary
map, respectively. Notice that the edge map Pk (Fig. 4 (d))

1451

Figure 4: Illustration of salient edge detection module
of PAGE-Net. The detected salient object edges in (c)
offer important information on the location of salient ob-
jects. With this salient edge information, PAGE-Net is able
to generate more accurate and better boundary-adherent re-
sults (e), compared with (b). See § 3.2 for more details.

can easily be obtained from the ground truth saliency map
Gk (Fig. 4 (f)). We ﬁrst build a salient edge detection mod-
ule F(YIk ) (
in Fig. 2 and Fig. 4 (c)), which can gen-
in Fig. 2) for an
erate an estimated salient edge map (
input image Ik. Here F denotes the salient edge detection
module consisting of a stack of convolution layers and YIk
corresponds to the enhanced feature of Ik. F can be learned
by minimizing the following L2 norm loss function:

1

K XK

k=1

LEdg(Pk, F (YIk )),

(4)

LEdg(Pk, F (YIk )) = ||Pk − F (YIk )||2
2.

A saliency readout network R(YIk , F(YIk )) is then
in Fig. 2) by ac-
built to generate the saliency estimate (
counting for both saliency features YIk and salient edge in-
formation F(YIk ). Thus the whole module can be learned
by minimizing the following combination loss:
1
K XK
k=1(cid:16)LSal(cid:0)Gk,R(YIk , F (YIk ))(cid:1)+LEdg(cid:0)Pk, F (YIk)(cid:1)(cid:17), (5)
where the saliency loss LSal is a weighted cross-entropy
loss that accounts for data imbalance between salient and
non-salient pixels:
LSal(cid:0)G, R(YI , F (YI ))(cid:1) = −Xi

β(1 − Gi)log(1 − Si)

(6)

+ (1 − β)Gilog(Si),

where i ∈ ΩI , and ΩI is the lattice domain of image I. S
indicates the saliency estimate for R and Si ∈ S. β refers
to the ratio of salient pixels in the ground truth G. With the
loss function in Eq. 5 and the salient edge detection mod-
ule F , the readout network R learns to optimize the salient
object estimates by leveraging explicit edge information.

Due to the hierarchical nature of the neural network, we
introduce dense connection [16] to our model to make use
of the information from different layers and increase rep-
resentational ability. The saliency feature Yℓ in the ℓ-th
layer is enhanced by considering all multi-layer saliency
estimates {Sℓ−1, . . . , S1}, as well as edge information
{Eℓ−1, . . . , E1} from all preceding ℓ − 1 layers:

Yℓ ← [Yℓ, Hℓ(Eℓ−1, . . . , E

1, Sℓ−1, . . . , S

1)],

(7)

where H indicates a small network that upsamples and con-
catenates the additional inputs from all preceding layers.
Detailed architectures of F , R, H can be found in § 3.3.
Discussion. To preserve more boundary information, we
add a salient edge detection module F that speciﬁcally fo-
cuses on segmenting salient object boundaries under the su-
pervision of the ground truth edge map P. Notice that F
is general enough to incorporate other edge-aware ﬁlters
like [6]. A readout network R for detecting salient objects is
then learned using both the saliency feature Y and explicit
salient edge information from F . Dense connection is fur-
ther introduced to draw representational power by reusing
information from other layers.

3.3. Detailed Network Architecture

Backbone Network. The backbone network is built from
the VGG-16 [31] model, which is well known for its ele-
gance and simplicity and is widely used in saliency models.
The ﬁrst ﬁve convolutional blocks of VGG-16 are adopted.
As shown in Fig. 5, we omit the last pooling layer (pool5)
to preserve more spatial information.
Pyramid Attention Module. Let {X5, X4, X3, X2, X1}
denote the features from the last convolution layers of
ﬁve conv blocks: conv1-2, conv2-2, conv3-3, conv4-3, and
conv5-3. For each Xℓ, we ﬁrst downsample Xℓ into multi-
ple scales. For scale n, the attention module is deﬁned over
three consecutive operations: BN→Conv(1×1, 1)→ReLU,
where the smallest attention map is set to 14 × 14. Up-
sampling operation is applied to resize the attention maps
{ln}n over all scales to their original size. Then we obtain
an enhanced saliency representation Yℓ through Eq. 3.
Edge Detection Module. The edge detection module F
is deﬁned as: BN → Conv(3 × 3, 64) → ReLU → Conv(1 ×
1, 1) → sigmoid. The saliency readout function R is built
as: BN → Conv(3 × 3, 128) → ReLU → BN → Conv(3 ×
3, 64) → ReLU → Conv(1×1, 1) → sigmoid. For ℓ-th layer,
a set of upsampling operations (Hℓ) is adopted in order to
enlarge all salient object estimations and salient edge infor-
mation from all preceding layers with current feature res-
olutions. We then update the saliency representation Yℓ
through Eq. 7. Next, the edge detection module F and
saliency readout function R are adopted to generate the cor-
responding saliency map Sℓ.

Take conv3-3 layer as an example. Given an input image
I ∈ R224×224×3, the saliency maps S2, S1 and edge maps
E2, E1 from conv4-3 and conv5-3 layers are ﬁrst upsampled
into the current spatial resolution 56×56. Then are then fed
into H3 and feature Y3 is updated accordingly. After ap-
plying the edge detection module F 3 and saliency readout
function R3, we obtain a saliency map S3 ∈ [0, 1]56×56.
In this way, we get ﬁve saliency maps {S5, S4, S3, S2, S1}
from conv1-2, conv2-2, conv3-3, conv4-3, and conv5-3, re-
spectively, where S5 ∈ [0, 1]224×224 is the ﬁnal, most accu-

1452

Figure 5: Illustration of side outputs of PAGE-Net. For better visualization, we omit the salient edge results. It can be
observed that the saliency from different convolution blocks of VGG-16 can be gradually optimized in a top-down manner.
See § 3.3 for details.

(a) ECCSD

(b) DUT-OMRON

(c) HKU-IS

(d) PASCAL-S

Figure 6: Quantitative results with PR-curve on four widely used benchmarks: ECCSD [49], DUT-OMRON [50], HKU-
IS [21] and PASCAL-S [25]. PAGE-Net gains promising performance. Best viewed in color. See § 4.1 for details.

rate saliency estimate.
Overall Loss. All the training images {Ik}K
k=1 are re-
sized to ﬁxed dimensions of 224 × 224 × 3. The salient
boundary maps Pk ∈ {0, 1}224×224 are generated from
the corresponding ground truth salient object map Gk ∈
{0, 1}224×224 and dilated to a three-pixel radius. Consider-
ing all ﬁve-side outputs, the overall training loss for a train-
ing image Ik is:
X

ℓ=1(cid:16)LSal(cid:0)Gℓ

k, Rℓ(Yℓ

Ik ))(cid:1)

5

(8)

Ik , F ℓ(Yℓ
Ik )(cid:1)(cid:17).

+ LEdg(cid:0)Pℓ

k, F ℓ(Yℓ

With the hierarchical loss functions, ﬁve intermediate layers
in PAGE-Net have direct access to the gradients from the
loss function, leading to implicit deep supervision [19].
Implementation Details. PAGE-Net is implemented in
Keras. Following the training protocol in [54, 20, 36], we
use THUS10K [9], containing 10,000 images with pixel-
wise annotations, for training. During the training phase,
the learning rate is set to 0.0001 and is decreased by a fac-
tor of 10 every two epochs. In each training iteration, we
use a mini-batch of 10 images. The entire training proce-
dure takes about 7 hours using an Nvidia TITAN X GPU.
Since our model does not need any pre- or post-processing,
the inference only takes 0.04s to process an image of size
224 × 224. This makes it faster than most deep learning

based competitors (see § 4.1 for a detailed comparison).

4. Experiments

We conduct extensive experiments on six popular bench-
marks: ECCSD [49], DUT-OMRON [50], HKU-IS [21],
PASCAL-S [25], SOD [30], and DUTS-TE [35], which are
all publicly available and are human-labeled with pixel-wise
ground truth for quantitative evaluations. For evaluation, we
adopt three widely used metrics [11], i.e., precision-recall
(PR) curves, F-measure and mean absolute error (MAE).

4.1. Performance Comparison

We compare the proposed PAGE-Net against 19 recent
deep learning based alternatives: MDF [21], LEGS [34],
DS [24], DCL [22], ELD [20], MC [57], RFCN [36], DHS
[26], HEDS [14], KSR [38], NLDF [29], DLS [15], AMU
[54], UCF [55], SRM [37], FSN [8], PAGR [56], RAS
[7] and C2S [23]. we use either the implementations with
the recommended parameter settings or the saliency maps
shared by the authors. For a fair comparison, we exclude
other ResNet-based models such as [39], or the ones us-
ing more training data [40]. Since fully connected condi-
tional random ﬁeld (CRF) has been used in [22, 14] as post-
processing, we further offer a baseline PAGE-Net+CRF that
uses CRF.

1453

Methods

MDF* [21]

LEGS [34]

DS [24]

DCL [22]

ELD [20]

MC [57]

RFCN [36]

DHS* [26]

HEDS [14]

KSR [38]

NLDF [29]

DLS [15]

AMU [54]

UCF [55]

SRM [37]

FSN [8]

PAGR [56]

RAS* [7]

C2S [23]

PAGE-Net

PAGE-Net+CRF

ECCSD [49]

DUT-OMRON [50] HKU-IS [21]

PASCAL-S [25]

SOD [30]

DUTS-TE [35]

F-score ↑ MAE ↓ F-score ↑ MAE ↓ F-score ↑ MAE ↓ F-score ↑ MAE ↓ F-score ↑ MAE ↓ F-score ↑ MAE ↓

0.831

0.831

0.810

0.898

0.865

0.822

0.898

0.905

0.915

0.801

0.905

0.825

0.889

0.868

0.910

0.910

0.904

0.908

0.902

0.924

0.926

0.108

0.119

0.160

0.071

0.080

0.107

0.109

0.061

0.053

0.133

0.063

0.090

0.059

0.078

0.056

0.053

0.061

0.056

0.054

0.042

0.035

0.694

0.723

0.603

0.732

0.700

0.702

0.701

-

0.714

0.742

0.753

0.714

0.733

0.713

0.707

0.741

-

0.758

0.731

0.770

0.770

0.092

0.133

0.120

0.087

0.092

0.088

0.111

-

0.093

0.157

0.080

0.093

0.097

0.132

0.069

0.073

-

0.068

0.080

0.066

0.063

0.860

0.812

0.848

0.907

0.844

0.781

0.895

0.892

0.913

0.759

0.902

0.806

0.918

0.905

0.892

0.895

0.897

0.900

0.887

0.918

0.920

0.129

0.101

0.078

0.048

0.071

0.098

0.089

0.052

0.040

0.120

0.048

0.072

0.052

0.074

0.046

0.044

0.048

0.045

0.046

0.037

0.030

0.764

0.749

0.818

0.822

0.767

0.721

0.827

0.820

0.830

0.649

0.831

0.719

0.834

0.771

0.783

0.827

0.815

0.804

0.834

0.835

0.835

0.145

0.155

0.170

0.108

0.121

0.147

0.118

0.091

0.112

0.137

0.112

0.136

0.103

0.128

0.127

0.095

0.094

0.105

0.082

0.078

0.074

0.785

0.691

0.781

0.784

0.760

-

0.805

0.793

0.802

0.698

0.808

-

0.773

0.776

0.792

0.781

-

0.809

0.786

0.796

0.796

0.155

0.197

0.150

0.126

0.154

-

0.161

0.127

0.126

0.199

0.130

-

0.145

0.169

0.132

0.127

-

0.124

0.124

0.110

0.108

0.657

0.611

-

0.742

0.697

-

0.752

0.799

0.796

0.660

0.777

-

0.750

0.742

0.798

0.761

-

0.807

0.783

0.815

0.817

0.114

0.137

-

0.150

0.092

-

0.090

0.065

0.057

0.123

0.066

-

0.085

0.117

0.059

0.066

-

0.059

0.062

0.051

0.047

∗DHS [26] uses THUS10K and DUT-OMRON for training. MDF [21] and RAS [7] are trained on a subset of HKU-IS.

Table 1: Quantitative results with F-measure (higher is better) and MAE (lower is better) on six well-known SOD
benchmarks: ECCSD [49], DUT-OMRON [50], HKU-IS [21], PASCAL-S [25], SOD [30] and DUTS-TE [35]. For each
column, the top two best entries are highlighted in red and blue, respectively. See § 4.1 for details.

Figure 7: Quantitative comparison of visual results on some representative challenging examples. It can be observed
that the proposed PAGE-Net is able to handle diverse challenging scenes. Best viewed in color. See § 4.1 for details.

Quantitative Evaluation. The precision-recall curves of
all methods are given in Fig. 6. Due to limited space, we
only show the results on four datasets. As seen, our PAGE-
Net outperforms its counterparts across all datasets, con-
vincingly demonstrating the effectiveness of the method.
We also compare our method to current state-of-the-art
models in terms of F-measure and MAE scores. It is ev-
ident from Table 1 that PAGE-Net achieves excellent re-

sults for all the datasets, across the metrics.
In particu-
lar, PAGE-Net shows a signiﬁcantly improved F-measure
compared to the second best method, RAS, for the DUT-
OMRON dataset (0.770 vs 0.758), which is one of the most
challenging benchmarks. This clearly demonstrates the su-
perior performance of PAGE-Net in complex scenes.

Qualitative Evaluation. Fig. 7 shows a visual comparison
of the results of our method against those of ﬁve other top-

1454

Method LEGS [34] MDF [21] DS [24] DCL [22] ELD [20]

Time(s)

1.54

7.83

0.13

0.39

0.55

Aspects

Methods

ECCSD [49] DUT-OMRON [50]
F-score ↑ MAE ↓ F-score ↑ MAE ↓

Method RFCN [36] DHS [26] HEDS [14] KSR [38] NLDF [29]

Time(s)

4.65

0.04

0.57

49.64

0.09

Method DLS [15] AMU [54] UCF [55] SRM [37] PAGE-Net
Time(s)

0.08

0.07

0.07

0.04

0.04

Table 2: Runtime comparison (GPU time) with previous
deep learning based saliency models. See § 4.1 for details.

performing competitors. For better visualization, we high-
light the main difﬁculties of each image group. We ﬁnd that
PAGE-Net performs well in a variety of challenging scenar-
ios, e.g., for large salient objects (ﬁrst row), low contrast
between objects and backgrounds (second row), cluttered
backgrounds (forth row), and multiple disconnected objects
(last row). Additionally, we observe that our method cap-
tures salient boundaries quite well due to its use of salient
edge detection modules.
Runtime Comparison. We also report the runtime of sev-
eral deep saliency methods in Table 2. These evaluations
were conducted on a machine with an i7 CPU and a Titan-X
GPU. PAGE-Net is faster than most of the others methods,
achieving a real-time speed of 25 FPS.

4.2. Ablation Studies

In this section, we analyze the contribution of each com-
ponent to the model’s overall performance. We conduct ex-
periments using the ECCSD [49] and DUT-OMRON [50]
datasets. The results are summarized in Table 3.
Multi-Scale Attention. To validate the effectiveness of our
multi-scale attention structure (§ 3.1), we compare three
variants: w/o attention, w/ single scale and w/o identity
mapping. Baseline w/o attention refers to the results ob-
tained by retraining PAGE-Net without any attention mod-
ule. The baseline w/ single scale corresponds to the results
obtained with a single-scale attention module (N = 1 in
Eq. 3). For w/o identity mapping, we retrain our atten-
tion module without identity mapping (Eq. 2). As shown
in Table 3, the network with multi-scale attention achieves
better performance, compared to those without an atten-
tion module or using single-scale attention. This conﬁrms
that the attention module beneﬁts from multi-scale infor-
mation. These results additionally demonstrate that identify
mapping also boosts performance. The visual comparison
between the results of PAGE-Net w/ and w/o an attention
module can be found in Fig. 3 (f) and (g).
Salient Edge Information. Next, we study the effect of
salient object edge information (§ 3.2). The baseline w/o
salient edge is obtained by disabling our salient edge detec-
tion module. We observe a drop in performance (ECCSD:
0.042→0.054, DUT-OMRON: 0.066→0.074) when using
MAE. This suggests that the salient edge information does
indeed improve salient object segmentation. To provide

Full
Model

Side
Outputs

Pyramid
Attention
Module

PAGE-Net

conv 1-output

conv 2-output
conv 3-output
conv 4-output
conv 5-output

w/o attention
w/ single scale

w/o identity

mapping (Eq. 2)

0.924

0.042

0.770

0.066

0.914
0.906
0.887
0.854
0.897
0.901

0.051
0.056
0.068
0.090
0.059
0.057

0.764
0.761
0.740
0.706
0.706
0.720

0.070
0.072
0.083
0.099
0.080
0.078

0.916

0.051

0.755

0.071

Salient-Edge w/o salient edge
Detection
Module

0.910
0.911
w/ canny detector 0.907

w/ HED [47]

0.054
0.052
0.053

0.746
0.751
0.748

0.074
0.073
0.073

Table 3: Ablation study of PAGE-Net on ECCSD [49] and
DUT-OMRON [50]. We change one component at a time,
to assess individual contributions. See § 4.2 for details.

deeper insight into the importance of salient edge informa-
tion, we est the model again after replacing the salient edge
detection module with two different edge detectors: HED
[47] and the canny ﬁlter. We also observe a minor decrease
in performance in both cases. This indicates that the use of
salient edge information is crucial for obtaining better per-
formance. This is because salient edges offer an informative
cue for detecting and segmenting salient objects, rather than
simply determining color or intensity changes.
Side Outputs. Finally, we study the effect of our hierarchi-
cal architecture on inferring saliency in a top-down manner
(Fig. 2 (b) and § 3.3). We introduced four additional base-
lines corresponding to the outputs from the intermediate
layers of PAGE-Net: conv2-output, conv3-output, conv4-
output, and conv5-output. Note that the ﬁnal prediction
of PAGE-Net can be viewed as the output from the conv1
layer. We ﬁnd that the saliency results are gradually opti-
mized by adding more details from the lower layers.

5. Conclusion

PAGE-Net

In this paper, we presented a novel deep saliency model,
PAGE-Net, for salient object detection.
is
equipped with two essential components: a pyramid atten-
tion module and a salient edge detection module. The for-
mer extends the regular attention mechanisms with multi-
scale information to improve saliency representation, en-
abling more efﬁcient training and better performance. The
latter emphasizes on the detection of salient edge infor-
mation, which can be leveraged for sharpening salient ob-
ject segments. Extensive experimental evaluations over six
well-known benchmark datasets verify that the aforemen-
tioned contributions signiﬁcantly improve the saliency de-
tection performance. Finally, the proposed model enjoys
efﬁcient inference speed and runs fast on GPU in real-time.

1455

References

[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada,
and Sabine Susstrunk. Frequency-tuned salient region detec-
tion. In CVPR, 2009. 2

[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. Mea-
IEEE TPAMI,

suring the objectness of image windows.
34(11):2189–2202, 2012. 1

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. In ICLR, 2015. 3

[4] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Se-
mantic segmentation with boundary neural ﬁelds. In CVPR,
2016. 2

[5] Chunshui Cao, Xianming Liu, Yi Yang, Yinan Yu, Jiang
Wang, Zilei Wang, Yongzhen Huang, Liang Wang, Chang
Huang, Wei Xu, et al. Look and think twice: Capturing
top-down visual attention with feedback convolutional neu-
ral networks. In ICCV, 2015. 3

[6] Liang-Chieh Chen, Jonathan T Barron, George Papandreou,
Kevin Murphy, and Alan L Yuille. Semantic image segmen-
tation with task-speciﬁc edge detection using cnns and a dis-
criminatively trained domain transform. In CVPR, 2016. 2,
5

[7] Shuhan Chen, Xiuli Tan, Ben Wang, and Xuelong Hu. Re-
verse attention for salient object detection. In ECCV, 2018.
3, 6, 7

[8] Xiaowu Chen, Anlin Zheng, Jia Li, and Feng Lu. Look,
perceive and segment: Finding the salient objects in images
via two-stream ﬁxation-semantic cnns. In ICCV, 2017. 6, 7

[9] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS
Torr, and Shi-Min Hu. Global contrast based salient region
detection. IEEE TPAMI, 37(3):569–582, 2015. 2, 6

[10] Runmin Cong, Jianjun Lei, Huazhu Fu, Qingming Huang,
Xiaochun Cao, and Chunping Hou. Co-saliency detection for
rgbd images based on multi-constraint feature matching and
cross label propagation. IEEE TIP, 27(2):568–579, 2018. 2

[11] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang-
Hua Gao, Qibin Hou, and Ali Borji. Salient objects in clut-
ter: Bringing salient object detection to the foreground. In
ECCV, pages 186–202, 2018. 6

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 4

[13] Seunghoon Hong, Tackgeun You, Suha Kwak, and Bohyung
Han. Online tracking by learning discriminative saliency
map with convolutional neural network. In ICML, 2015. 1

[14] Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji,
Zhuowen Tu, and Philip Torr. Deeply supervised salient ob-
ject detection with short connections. In CVPR, 2017. 1, 2,
6, 7, 8

[15] Ping Hu, Bing Shuai, Jun Liu, and Gang Wang. Deep level

sets for salient object detection. In CVPR, 2017. 2, 6, 7, 8

[17] Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nan-
ning Zheng, and Shipeng Li. Salient object detection: A dis-
criminative regional feature integration approach. In CVPR,
2013. 1, 2

[18] Jason Kuen, Zhenhua Wang, and Gang Wang. Recurrent at-
In CVPR, 2016.

tentional networks for saliency detection.
2

[19] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou
Zhang, and Zhuowen Tu. Deeply-supervised nets. In AIS-
TATS, 2015. 6

[20] Gayoung Lee, Yu-Wing Tai, and Junmo Kim. Deep saliency
with encoded low level distance map and high level features.
In CVPR, 2016. 2, 6, 7, 8

[21] Guanbin Li and Yizhou Yu. Visual saliency based on multi-

scale deep features. In CVPR, 2015. 2, 6, 7, 8

[22] Guanbin Li and Yizhou Yu. Deep contrast learning for salient

object detection. In CVPR, 2016. 2, 6, 7, 8

[23] Xin Li, Fan Yang, Hong Cheng, Wei Liu, and Dinggang
Shen. Contour knowledge transfer for salient object detec-
tion. In ECCV, 2018. 2, 6, 7

[24] Xi Li, Liming Zhao, Lina Wei, Ming-Hsuan Yang, Fei Wu,
Yueting Zhuang, Haibin Ling, and Jingdong Wang. Deep-
saliency: Multi-task deep neural network model for salient
object detection. IEEE TIP, 25(8):3919 – 3930, 2016. 6, 7,
8

[25] Yin Li, Xiaodi Hou, Christof Koch, James M Rehg, and
Alan L Yuille. The secrets of salient object segmentation.
In CVPR, 2014. 2, 6, 7

[26] Nian Liu and Junwei Han. DHSNet: Deep hierarchical
saliency network for salient object detection. In CVPR, 2016.
2, 6, 7, 8

[27] Nian Liu, Junwei Han, and Ming-Hsuan Yang. PiCANet:
Learning pixel-wise contextual attention for saliency detec-
tion. In CVPR, 2018. 3

[28] Tie Liu, Zejian Yuan, Jian Sun, Jingdong Wang, Nanning
Zheng, Xiaoou Tang, and Heung-Yeung Shum. Learning to
detect a salient object. In CVPR, 2007. 2

[29] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin
Eichel, Shaozi Li, and Pierre-Marc Jodoin. Non-local deep
features for salient object detection. In CVPR, 2017. 2, 6, 7,
8

[30] Vida Movahedi and James H Elder. Design and perceptual
validation of performance measures for salient object seg-
mentation. In CVPR - Workshops, 2010. 2, 6, 7

[31] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR,
2015. 5

[32] Hongmei Song, Wenguan Wang, Sanyuan Zhao, Jianbing
Shen, and Kin-Man Lam. Pyramid dilated deeper convlstm
for video salient object detection. In ECCV, 2018. 2

[33] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.
Residual attention network for image classiﬁcation.
In
CVPR, 2017. 1, 3, 4

[16] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens
van der Maaten. Densely connected convolutional networks.
In CVPR, 2017. 5

[34] Lijun Wang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan
Yang. Deep networks for saliency detection via local esti-
mation and global search. In CVPR, 2015. 1, 2, 6, 7, 8

1456

[54] Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang,
and Xiang Ruan. Amulet: Aggregating multi-level convolu-
tional features for salient object detection. In ICCV, 2017. 2,
6, 7, 8

[55] Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang,
and Baocai Yin. Learning uncertain convolutional features
for accurate saliency detection. In ICCV, 2017. 2, 6, 7, 8

[56] Xiaoning Zhang, Tiantian Wang, Jinqing Qi, Huchuan Lu,
and Gang Wang. Progressive attention guided recurrent net-
work for salient object detection. In CVPR, 2018. 3, 6, 7

[57] Rui Zhao, Wanli Ouyang, Hongsheng Li, and Xiaogang
Wang. Saliency detection by multi-context deep learning.
In CVPR, 2015. 1, 2, 6, 7

[58] Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun.
Saliency optimization from robust background detection. In
CVPR, 2014. 2

[35] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,
Dong Wang, Baocai Yin, and Xiang Ruan. Learning to de-
tect salient objects with image-level supervision. In CVPR,
2017. 2, 6, 7

[36] Linzhao Wang, Lijun Wang, Huchuan Lu, Pingping Zhang,
and Xiang Ruan. Saliency detection with recurrent fully con-
volutional networks. In ECCV, 2016. 2, 6, 7, 8

[37] Tiantian Wang, Ali Borji, Lihe Zhang, Pingping Zhang, and
Huchuan Lu. A stagewise reﬁnement model for detecting
salient objects in images. In ICCV, 2017. 2, 6, 7, 8

[38] Tiantian Wang, Lihe Zhang, Huchuan Lu, Chong Sun, and
Jinqing Qi. Kernelized subspace ranking for saliency detec-
tion. In ECCV, 2016. 6, 7, 8

[39] Tiantian Wang, Lihe Zhang, Shuo Wang, Huchuan Lu, Gang
Yang, Xiang Ruan, and Ali Borji. Detect globally, reﬁne
locally: A novel approach to saliency detection. In CVPR,
2018. 6

[40] Wenguan Wang, Jianbing Shen, Xingping Dong, Ali Borji,
and Ruigang Yang. Inferring salient objects from human ﬁx-
ations. IEEE PAMI, 2019. 2, 6

[41] Wenguan Wang, Jianbing Shen, and Haibin Ling. A deep
network solution for attention and aesthetics aware photo
cropping. IEEE TPAMI, 2018. 1

[42] Wenguan Wang, Jianbing Shen, and Fatih Porikli. Saliency-
aware geodesic video object segmentation. In CVPR, 2015.
1

[43] Wenguan Wang, Jianbing Shen, Ling Shao, and Fatih
Porikli. Correspondence driven saliency transfer. IEEE TIP,
25(11):5025–5034, 2016. 2

[44] Wenguan Wang, Jianbing Shen, Hanqiu Sun, and Ling Shao.
IEEE TCSVT,

Video co-saliency guided co-segmentation.
28(8):1727–1736, 2018. 1

[45] Wenguan Wang, Jianbing Shen, Yizhou Yu, and Kwan-Liu
Ma. Stereoscopic thumbnail creation via efﬁcient stereo
saliency detection. IEEE TVCG, 23(8):2014–2027, 2017. 1
[46] Yichen Wei, Fang Wen, Wangjiang Zhu, and Jian Sun.
Geodesic saliency using background priors. In ECCV, 2012.
2

[47] Saining Xie and Zhuowen Tu. Holistically-nested edge de-

tection. In ICCV, 2015. 8

[48] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In ICML, 2015. 3

[49] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical

saliency detection. In CVPR, 2013. 1, 2, 6, 7, 8

[50] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and
Ming-Hsuan Yang. Saliency detection via graph-based man-
ifold ranking. In CVPR, 2013. 2, 6, 7, 8

[51] Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, and
Ming-Hsuan Yang. Object contour detection with a fully
convolutional encoder-decoder network. In CVPR, 2016. 2

[52] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and
Alex Smola. Stacked attention networks for image question
answering. In CVPR, 2016. 3

[53] Jing Zhang, Yuchao Dai, Fatih Porikli, and Mingyi He.
arXiv preprint

Deep edge-aware saliency detection.
arXiv:1708.04366, 2017. 2

1457

