Natural and Realistic Single Image Super-Resolution with Explicit Natural

Manifold Discrimination

Jae Woong Soh
Nam Ik Cho
Department of ECE, INMC, Seoul National University, Seoul, Korea

Gu Yong Park

Junho Jo

{soh90815, benkay, jottue}@ispl.snu.ac.kr, nicho@snu.ac.kr

Abstract

Recently, many convolutional neural networks for single
image super-resolution (SISR) have been proposed, which
focus on reconstructing the high-resolution images in terms
of objective distortion measures. However, the networks
trained with objective loss functions generally fail to recon-
struct the realistic Ô¨Åne textures and details that are essential
for better perceptual quality. Recovering the realistic de-
tails remains a challenging problem, and only a few works
have been proposed which aim at increasing the percep-
tual quality by generating enhanced textures. However, the
generated fake details often make undesirable artifacts and
the overall image looks somewhat unnatural. Therefore, in
this paper, we present a new approach to reconstructing re-
alistic super-resolved images with high perceptual quality,
while maintaining the naturalness of the result. In particu-
lar, we focus on the domain prior properties of SISR prob-
lem. SpeciÔ¨Åcally, we deÔ¨Åne the naturalness prior in the low-
level domain and constrain the output image in the natural
manifold, which eventually generates more natural and re-
alistic images. Our results show better naturalness com-
pared to the recent super-resolution algorithms including
perception-oriented ones.

1. Introduction

Single image super-resolution (SISR) is a classical
image restoration problem which aims to recover a
high-resolution (HR) image from the corresponding low-
resolution (LR) image.
In SISR problems, the given im-
age is usually assumed to be a low-pass Ô¨Åltered and down-
sampled version of an HR image. Hence, recovering the
HR is an ill-posed problem since multiple HR images can
correspond to one LR image. That is, the SISR is a chal-
lenging one-to-many problem which attracted researchers
to Ô¨Ånd many interesting solutions and applications, and thus
numerous algorithms have been proposed so far.

Recently, convolutional neural networks (CNNs) have

(a) HR

(b) EnhanceNet [29]

(c) SFT-GAN [37]

(d) Our NatSR

Figure 1: Super-resolved results (√ó4) of ‚Äú0823‚Äù in DIV2K
validation set [34]. A part of the image is cropped and
zoomed for visualization. Our NatSR result is more nat-
ural with less artifacts which is perceptually plausible than
other algorithms‚Äô results.

shown great success in most computer vision areas includ-
ing the SISR. In typical CNN-based SISR methods, the
distortion-oriented loss functions are considered. SpeciÔ¨Å-
cally, the CNNs attempt to achieve higher peak-signal-to-
noise ratio (PSNR), i.e., low distortion in terms of mean
squared error (MSE). There have been lots of distortion-
oriented CNNs for SISR [5, 18, 30, 19, 21, 32, 36, 23, 42,
15, 11], and the performance of SISR is ever increasing as
many researchers are still creating innovative architectures
and also as the possible depth and connections of the net-

8122

works are growing. However, they yield somewhat blurry
results and do not recover the Ô¨Åne details even with very
deep and complex networks. It is because the distortion-
oriented models‚Äô results are the average of possible HR im-
ages.

To resolve the above-stated issues, perception-oriented
models have also been proposed for obtaining better percep-
tual quality HR images. For some examples, the perceptual
loss was introduced in [16], which is deÔ¨Åned as the distance
in the feature domain. More recently, SRGAN [22] and
EnhanceNet [29] have been proposed for producing better
perceptual quality. The SRGAN employed generative mod-
els, particularly the generative adversarial nets (GAN) [8],
and adopted the perceptual loss. The EnhanceNet added an
additional texture loss [7] for better texture reconstruction.
However, they sometimes generate unpleasant and unnatu-
ral artifacts along with the reconstructed details.

There have also been some methods that consider the
naturalness of super-resolved images. One of these ap-
proaches is to implicitly supervise the naturalness through
the reÔ¨Åned dataset. SpeciÔ¨Åcally, as the CNN is very sensi-
tive to the training dataset, several methods [23, 42] consid-
ered using the reÔ¨Åned dataset. For example, patches with
low gradient magnitudes are discarded from the training
dataset, which provides better naturalness implicitly. This
approach might increase the PSNR performance by con-
straining the possible HR space to the rich-textured one.
Another approach is to provide explicit supervision by con-
ditioning the feature spaces. For example, the recently de-
veloped SFT-GAN [37] has shown great perceptual quality
by constraining the features with its high-level semantics
while adopting the adversarial loss. However, its practical
usage is limited because it requires the categorical prior,
and also it is limited to the categories which are included
in the training process. For the out-of-category inputs, this
framework is the same as SRGAN [22]. Moreover, SFT-
GAN strongly relies on the ability of the adopted semantic
segmentation method because the wrong designation of se-
mantics might cause worse perceptual quality.

For obtaining realistic and natural perceptual quality HR
images, we propose a new SISR approach which constrains
the low-level domain prior instead of high-level semantics.
For this, we Ô¨Årst investigate the process and the domain
knowledge of SISR. By exploiting the domain knowledge,
we explicitly model the HR space of corresponding LR im-
age, and build a discriminator which determines the deci-
sion boundary between the natural manifold and unnatural
manifold. By constraining the output image into the natural
manifold, our generative model can target only one of the
multi-modal outputs in the desired target space. As a re-
sults, our method shows less artifacts than other perception-
oriented methods as shown in Figure 1.

In summary, the main contributions of this paper are as

follows.

‚Ä¢ We model the SISR problem explicitly and investigate

the desirable HR space.

‚Ä¢ We design a CNN-based natural manifold discrimina-

tor and show our model is reasonable.

‚Ä¢ We adopt a CNN structure with fractal residual learn-
ing (FRL) and demonstrate a distortion-oriented model
named fractal residual super-resolution (FRSR), which
achieves comparable results to recent CNNs.

‚Ä¢ We propose a perception-oriented SISR method named
as natural and realistic super-resolution (NatSR),
which generates realistic textures and natural details
effectively while achieving high perceptual quality.

The rest of this paper is organized as follows. In Sec. 3,
we explicitly model the LR-HR space and the SISR prob-
lem, and investigate its inherent properties. Then in Sec.
4, we divide the target HR space into three disjoint sets
where two sets are in the unnatural manifold and the one
is in the natural manifold. In Sec. 5, we demonstrate our
main method and the NatSR, and in Sec. 6 we discuss and
analyze the feasibility in several ways. The experimental
results are shown in Sec. 7.

2. Related Work

2.1. Single Image Super Resolution

The conventional non-CNN methods mainly focused on
the domain and feature priors. Early methods explored the
domain priors to predict missing pixels. For example, in-
terpolation methods such as bicubic and Lanczos generate
the HR pixels by the weighted average of neighboring LR
pixels. Later, the priors such as edge feature, gradient fea-
ture [33, 31] and internal non-local similarity [14] were in-
vestigated. Also, dictionary learning sparse coding meth-
ods were exploited for the SISR [40, 6, 39, 35]. Recently,
it has been shown that CNN-based methods outperform the
earlier non-CNN algorithms, showing great breakthrough
in accuracy. These CNN-based methods implicitly adopt
image and domain priors which are inscribed in training
datasets. The SRCNN [5] was the Ô¨Årst CNN-based method
which uses three convolution layers, and many other works
with deeper and heavier structure have been proposed after-
ward [18, 30, 19, 32, 21, 36, 23, 42, 15, 11]. All these meth-
ods are discriminative and distortion-oriented approaches,
which aim to achieve higher PSNR.

2.2. Perception Oriented Super Resolution

The problem of distortion-oriented models recently drew
the attention of researchers that the super-resolved results

8123

    f 

    f 

LR

(a) HR

(b) LR

(cid:1847):(cid:882),(cid:883)ùêª√óùëä√óùê∂
(cid:1848): HR space

Too blurry

Natural

Too noisy

(cid:1827)
ùëÅ
(cid:1828)

ùú∂
ùùà

GT

    f 

    f 

(c) Blurry HR

(d) Noisy HR

Figure 2: A simple explanation of LR-HR relationship and
SISR in the frequency domain.

often lack the high-frequency details and are not perceptu-
ally satisfying. Also, Blau et al. [4] showed that there is a
trade-off between the perceptual quality and distortion, and
some perception-oriented models have been proposed ac-
cordingly. For example, Johnson et al. [16] have shown that
the loss in the pixel domain is not optimal for the perceptual
quality, and instead, the loss in the feature space might be
closer to the human perception model. Then, Ledig et al.
[22] introduced the SRGAN which adopted the generative
model with GAN [8] and employed the perceptual loss as
in [16]. Hence, unlike the distortion-oriented methods that
produce the average of possible HR images, the SRGAN
generates one of the candidates in the multi-modal target
HR space. EnhanceNet [29] goes one step further by ex-
ploiting the texture loss [7] for better producing image de-
tails. However, due to the inherent property of one-to-many
inverse problem, it is required to consider the semantics for
the generated pixels. In this respect, SFT-GAN [37] restricts
the feature space by conditioning the semantic categories of
target pixels.

3. Modeling the SISR

In this section, we explicitly deÔ¨Åne and model the LR-
HR space and the SISR problem. First of all, let us deÔ¨Åne
the LR image ILR as the low-pass Ô¨Åltered and downsampled
HR image IHR. Formally, the LR-HR relation is described
as

ILR = h(IHR)‚Üì,

(1)

where h(¬∑) denotes a low-pass Ô¨Ålter and ‚Üì denotes down-
sampling. Figure 2a and Figure 2b show a simple expla-
nation of HR and LR correspondence in the frequency do-
main where we assume that the spatial domain is inÔ¨Ånite.
Both Figure 2c and Figure 2d are possible HRs for the cor-
responding LR in Figure 2b, and moreover, there can be

Figure 3: Our proposed LR-HR model of the natural mani-
fold and its discrimination for SISR. U is the image space,
V is the possible HR space, and A, B, and N are three dis-
joint sets of V . Œ± and œÉ control the boundary between the
manifolds.

inÔ¨Ånite number of possible HRs that have the same low fre-
quency components but different high-frequency parts (de-
noted noisy in Figure 2d). As the SISR is to Ô¨Ånd an HR
for the given LR, it is usually modeled as Ô¨Ånding the condi-
tional likelihood p(IHR|ILR). Due to its one-to-many prop-
erty, it is better to model it as a generative model rather than
a discriminative one.

4. Natural Manifold Discrimination

4.1. Designing Natural Manifold

We now go into the real situation to Ô¨Ånd the natural
manifold. Figure 3 shows our LR-HR image space mod-
eling, where U : [0, 1]H√óW √óC is the overall image set with
height H, width W , and channel C with the normalized
pixel value. For a certain ILR, V is the space whose ele-
ments all results into the same ILR by the low-pass Ô¨Åltering
and downsampling. Conversely, an LR image is mapped to
an element in V by any SR method. We may also interpret
the early CNNs with our LR-HR model. For the distortion-
oriented models, the output is the average of the elements in
the HR space, i.e., P wiIHRi where IHRi ‚àà V , for some
i and weights wi, and thus the result is blurry. To allevi-
ate this problem, some methods [23, 42] reÔ¨Åned the training
set. SpeciÔ¨Åcally, they discarded the training patches with
low gradient magnitudes, which gives implicit constraints
on the candidate IHRi ‚Äôs to keep the resulting outputs away
from the blurry images.

To model the natural manifold, we divide V into three
disjoint sets as illustrated in Figure 3. The Ô¨Årst one is the
blurry set A, the elements of which are modeled as the con-
vex combination of interpolated LR and the original HR.
SpeciÔ¨Åcally, the set A is deÔ¨Åned as

A = {IA|IA = (1 ‚àí Œ±)h(I ‚Üë

LR) + Œ±IHR},

(2)

8124

where h(¬∑) is the same low-pass Ô¨Ålter as in eq. (1), and ‚Üë de-
notes upsampling with zero insertion between original val-
ues. Hence, h(I ‚Üë
LR) corresponds to Figure 2c which also
means the interpolation of ILR to the size of IHR. Also,
the Œ± ‚àà [0, 1] is a hyper-parameter which decides the deci-
sion boundary between the set A and N , i.e., between the
Figure 2c and Figure 2a. We can easily show that the IA
deÔ¨Åned above is also an element of V , i.e., A ‚äÇ V . To be
speciÔ¨Åc, if we apply low-pass Ô¨Åltering and downsampling
to the IA, it becomes an LR as follows:

h(IA)‚Üì

LR) + Œ±IHR)‚Üì
LR))‚Üì + h(Œ±IHR)‚Üì

LR)‚Üì + Œ±h(IHR)‚Üì

=h((1 ‚àí Œ±)h(I ‚Üë
=h((1 ‚àí Œ±)h(I ‚Üë
=(1 ‚àí Œ±)h(I ‚Üë
=(1 ‚àí Œ±)ILR + Œ±ILR
=ILR.

(3)

(4)

(5)

(6)

(7)

(8)

0.2

0.1

0

-0.1

0

2

4

8

6

6

8

0

4

2

-3

#10

20

15

10

5

0

-5

0

50

100

50

150

0

150

100

(a) 8 √ó 8 DCT.

(b) 128 √ó 128 DCT.

Figure 4: DCT coefÔ¨Åcients of bicubic up/downsampling
kernels for the scaling factor of √ó4.

k3n64s1

k2s2

k3n128s1

k2s2

k3n256s1

k2s2

k3n512s1

k2s2

k3n1s1

v
n
o
C

v
n
o
C

l

o
o
p
x
a
M

v
n
o
C

v
n
o
C

l

o
o
p
x
a
M

v
n
o
C

v
n
o
C

l

o
o
p
x
a
M

v
n
o
C

v
n
o
C

l

o
o
p
x
a
M

v
n
o
C

 
l

a
b
o
G

l

 

e
g
a
r
e
v
A

g
n

i
l

o
o
P

Figure 5: Our NMD network architecture.

i

d
o
m
g
S

i

Hence, from eq.(1), it is shown that IA ‚àà V . In other words,
the weighted sum of Figure 2c and Figure 2a is of course in
the V .

The second set to consider is the noisy set B, which con-
tains the images like Figure 2d. SpeciÔ¨Åcally, we can model
the set as:

B = {IB|IB = IHR + n}

(9)

where n is the noise in the high-frequency, with standard
deviation œÉ. We can also see that B ‚äÇ V , because

h(IB)‚Üì

=h(IHR + n)‚Üì
=h(IHR)‚Üì + h(n)‚Üì
=h(IHR)‚Üì
=ILR.

(10)

(11)

(12)

(13)

(14)

Also, IB can be interpreted as the convex combination of
IHR and IHR + n0 (weighted sum of Figure 2a and Fig-
ure 2d), because

(1 ‚àí Œ≤)IHR + Œ≤(IHR + n0)
=IHR ‚àí Œ≤IHR + Œ≤IHR + Œ≤n0
=IHR + Œ≤n0.

(15)

(16)

(17)

where n = Œ≤n0.

The blurry IA and noisy IB are used for training our
natural manifold discriminator that will be explained in the
next subsection. In practice, we perform the noise injection
in the frequency domain using 2D-discrete cosine transform
(DCT). We set the low-pass Ô¨Ålter for up/downsampling in
eq.(1) and eq.(2) as the bicubic Ô¨Ålter, and its DCT is shown
in Figure 4. To generate a wide range of noisy images, we
inject the noise into the last column and row. In the experi-
ments, we use the 8 √ó 8 2D-DCT for brevity.

4.2. Natural Manifold Discriminator

To narrow the target space to the natural manifold, we
design a discriminator that differentiates the natural image
(the elements that belong to N as in Figure 3) from the
blurry/noisy ones (A or B). For this, we design a CNN-
based classiÔ¨Åer that discriminates N (natural manifold) and
A ‚à™ B (unnatural manifold), which will be called natural
manifold discriminator (NMD). The training is performed
with the sigmoid binary cross entropy loss function deÔ¨Åned
as

‚àí Ex‚ààA‚à™B[log(1 ‚àí DNM (x))] ‚àí Ex‚ààN [log(DNM (x))],
(18)
where DNM (¬∑) denotes the output sigmoid value of NMD.
For the expectation, we use the empirical mean of the train-
ing dataset. The network architecture of our NMD is shown
in Figure 5, which is a simple VGG-style CNN. Fully-
connected layers for the last stage is not used in our case.
Instead, one convolution layer and global average pooling
are used.

For the training, we start from Œ± = 0.5 and œÉ = 0.1.
We update both hyper-parameters according to the average
of 10 validation accuracies (AVA). When it reaches above
95%, we update Œ± and œÉ following the rules below:

if AVA of Œ± ‚â• 0.95 then

Œ± ‚Üê Œ± + 0.1

if AVA of œÉ ‚â• 0.95 then

œÉ ‚Üê 0.8 √ó œÉ.

(19)

(20)

(21)

(22)

We stop training with the Ô¨Ånal Œ± and œÉ equal to 0.8 and
0.0044, respectively.

8125

5. Natural and Realistic Super-Resolution

5.2.2 Naturalness Loss

In this section, we explain the proposed natural and re-
alistic super-resolution (NatSR) generator model and the
training loss function.

5.1. Network Architecture

The overall architecture of our NatSR is shown in Fig-
ure 6, which takes the ILR a the input and generates the
SR output. As shown in the Ô¨Ågure, our network is based
on residual learning, which has long been used as a basic
skill to mitigate the degradation problem in very deep net-
works. Typically, two types of residual learnings are used:
local residual learning (LRL) which bypasses the input to
the output in a local range [12], and global residual learning
(GRL) which provides the skip-connection between the in-
put and the output in a global scale of the network [18]. For-
mer approaches [18, 10] have shown that learning the sparse
features is much more effective than learning the pixel do-
main values directly. Hence, recent models adopt both local
residual learning (short-path) and global residual learning
(long-path) [22, 23, 42].

Inspired by former studies, we adopt a connection
scheme shown in Figure 6, named as fractal residual learn-
ing (FRL) structure in that the connection has a fractal pat-
tern. Also, as a basic building block of our NatSR, we em-
ploy the residual dense block (RDBlock) [42] shown in Fig-
ure 7, and adopt the residual scaling [23] in our RDBlock.
By using the FRL and RDBlock, all from short- to long-path
skip-connection can be employed.

As a discriminator for GAN, we apply a similar network
architecture as NMD. Instead of using only convolution lay-
ers, we adopt spectral normalization [28] to make the dis-
criminator satisfy Lipschitz condition. Also, we use strided
convolutions instead of max-pooling layers. SpeciÔ¨Åc archi-
tecture details are provided in the supplementary material.

5.2. Training Loss Function

5.2.1 Reconstruction Loss

We design the naturalness loss based on our pre-trained
natural manifold discriminator (NMD). To concentrate the
target manifold within the natural manifold, the output of
NMD should be nearly 1. We may use the loss as a negative
of the sigmoid output, but we use its log-scale to boost the
gradients:

LNatural = E[‚àí log(DNM (ISR))]

(24)

where DNM (¬∑) denotes the output sigmoid value of NMD.

5.2.3 Adversarial Loss

As it is well-known that GANs are hard to train and unsta-
ble, there have been lots of variations of GANs [43, 2, 9, 25,
17]. Recently, GAN with relativistic discriminator has been
proposed [17], which shows quite robust results with stan-
dard GAN [8] in generating fake images in terms of Fr¬¥echet
Inception Distance [13]. Thus, we employ RaGAN for our
adversarial training, which is described as:

LG = ‚àíExr ‚àºPr [log( ÀúD(xr))] ‚àí Exf ‚àºPg [log(1 ‚àí ÀúD(xf ))]
(25)
LD = ‚àíExf ‚àºPg [log( ÀúD(xf ))] ‚àí Exr ‚àºPr [log(1 ‚àí ÀúD(xr))],
(26)

where Pr and Pg are distributions of HR and SR respec-
tively, xr and xf mean real and fake data respectively, and

ÀúD(xr) = sigmoid(C(xr) ‚àí Exf ‚àºPg [C(xf )])
ÀúD(xf ) = sigmoid(C(xf ) ‚àí Exr ‚àºPr [C(xr)])

(27)

(28)

where C(¬∑) denotes the output logit of discriminator. In our
case, the motivation of RaGAN discriminator is to measure
‚Äúthe probability that the given image is closer to real HR
images than the generated SR images on average.‚Äù

To model the p(IHR|ILR), we adopt the pixel-wise recon-
struction loss, speciÔ¨Åcally the mean absolute error (MAE)
between the ground-truths and the super-resolved images:

5.2.4 Overall Loss

The overall loss term to train our NatSR is deÔ¨Åned as the
weighted sum of loss terms deÔ¨Åned above:

LRecon = E[||IHR ‚àí ISR||1],

(23)

L = Œª1LRecon + Œª2LNatural + Œª3LG.

(29)

where ISR denotes the super-resolved output. Although all
the perception-oriented models apply perceptual losses, we
do not adopt such losses, because it is found that the per-
ceptual loss causes undesirable artifacts in our experiments.
To boost high-frequency details, we instead use our NMD
as a solution.

As our baseline, we train the distortion-oriented model
where Œª2 = Œª3 = 0, which means that the overall loss
is just the reconstruction loss LRecon. We name our baseline
model as fractal residual super-resolution network (FRSR).
For our NatSR which is perception-oriented, we use the full
loss above with Œª1 = 1, Œª2 = 10‚àí3 and Œª3 = 10‚àí3.

8126

v
n
o
C

l

k
c
o
B
D
R

l

k
c
o
B
D
R

l

k
c
o
B
D
R

l

k
c
o
B
D
R

l

k
c
o
B
D
R

l

k
c
o
B
D
R

l

k
c
o
B
D
R

l

k
c
o
B
D
R

v
n
o
C

l

i

e
x
p
-
b
u
S

l

i

e
x
p
-
b
u
S

v
n
o
C

Figure 6: Our NatSR network architecture. We adopt fractal residual learning for mid- and long-path skip connection and
employ the residual dense block (RDBlock) for short-path connection.

ùüë√óùüë

v
n
o
C

U
L
e
R

ùüë√óùüë

v
n
o
C

U
L
e
R

‚Ä¶

ùüë√óùüë

v
n
o
C

U
L
e
R

vùüè√óùüè

n
o
C

Figure 7: Residual Dense Block (RDBlock) that we employ
for our NatSR.

Method

NMD Score
1.000 ¬± 0.001
HR
0.000 ¬± 0.000
Bicubic
0.032 ¬± 0.009
SRResNet
0.043 ¬± 0.012
EDSR
0.044 ¬± 0.011
FRSR (Ours)
0.755 ¬± 0.063
SRGAN-MSE
0.584 ¬± 0.202
SRGAN-VGG22
0.832 ¬± 0.109
SRGAN-VGG54
EnhanceNet-PAT 0.367 ¬± 0.095
1.000 ¬± 0.000
NatSR (Ours)

6. Discussion and Analysis

Table 1: Results of NMD score.

6.1. Effectiveness of Proposed Discriminator

To demonstrate the meaning and effectiveness of our
NMD, we test the NMD scores for the perception-oriented
methods such as SRGAN variants [22], EnhanceNet,
NatSR, and also for the distortion-oriented methods includ-
ing our FRSR. Table 1 shows the results on BSD100 [26],
where the NMD is designed to output score 1 when the in-
put image is close to the natural original image, and output
lower score when the input is blurry or noisy. We can see
that previous perception-oriented methods score between 0
and 1 which means that they lie near the boundary of the
natural and unnatural manifold in our LR-HR model. Also,
the original HR scores 1 and bicubic interpolation scores 0,
which means that our NMD discriminates HR and LR with
high conÔ¨Ådence. Additionally, SRResNet, EDSR, and our
FRSR, which are distortion-oriented, score almost 0. We
may interpret the result that the distortion-oriented methods
produce the image which also lie on the blurry manifold.
On the other hand, our NatSR results in the scores close to
1 which is much higher than the other perception-oriented
algorithms. In summary, it is believed that our model of nat-
ural manifold and NMD are reasonable, and the NMD well

discriminates the natural and unnatural manifold.

6.2. Study on the Plausibility of SR Images

As we approach the SISR by interpreting the input and
output images in our LR-HR space model, we analyze the
plausibility of super-resolved images of various methods ac-
cording to our model. The super-resolved images must lie
on the set V in Figure 3, which means that the downsam-
pling of a super-resolved image must be in the LR space,
i.e., it must be similar to the input LR image as

ILR ‚âà h(ISR)‚Üì.

(30)

For the analysis, we show the RGB-PSNR between
h(ISR)‚Üì and ILR in Table 2 which are tested on Set5 [3].
The results are in the ascending order of SRGAN, En-
hanceNet, and our NatSR. Even though we do not give any
constraints on the LR space, our NatSR results mostly lie
on the feasible set V . On the other hand, SRGAN result is
about 36 dB, which means that the SRGAN barely reÔ¨Çects
the LR-HR properties.

8127

Method

RGB-PSNR (dB)

SRGAN
ENet-PAT
NatSR

36.16
41.65
45.94

Table 2: Results of RGB-PSNR between LR input and
downsampled SR image in LR domain.

7. Experimental Results

7.1. Implementation details

We train both NMD and NatSR (including FRSR) with
recently released DIV2K [34] dataset which consists of
high-quality (2K resolution) 800 training images, 100 val-
idation images, and 100 test images. The size of the input
LR patch is set to 48 √ó 48, and we only train with scal-
ing factor √ó4. ADAM optimizer [20] is used for training
with the initial learning rate of 2 √ó 10‚àí4, and halved once
during the training. We implement our code with Tensor-
Ô¨Çow [1]. For the test, we evaluate our model with famous
SISR benchmarks: Set5 [3], Set14 [41], BSD100 [26], and
Urban100 [14].

7.2. Evaluation Metrics and Comparisons

For the evaluation of distortion-oriented models, popular
FR-IQA (full reference image quality assessment), PSNR
and SSIM (structure similarity) [38] are used. But since
these measures are not appropriate for measuring the qual-
ity of perceptual models, we use one of the recently pro-
posed NR-IQA (no reference image quality assessment)
called NQSR [24] which is for SISR and well-known for Ma
et al.‚Äôs score. Additionally, another NR-IQA, NIQE [27]
is used to measure the naturalness of images. The higher
NQSR and the lower NIQE mean the better perceptual qual-
ity. However, it is questionable whether so many variants
of NR-IQA methods perfectly reÔ¨Çect the human perceptual
quality. Hence, we need to use the NR-IQA results just for
rough reference.

We compare our FRSR with other distortion-oriented
methods such as LapSRN, SRDenseNet, DSRN, and EDSR
[21, 36, 11, 23], and compare our NatSR with other
perception-oriented ones such as SRGAN, ENet, and SFT-
GAN [22, 29, 37] (We denote SRGAN-VGG54 as SRGAN
and EnhanceNet-PAT as ENet for short).

7.3. FR IQA Results

In this subsection, we discuss the distortion-oriented
methods and their results. The overall average PSNR/SSIM
results are listed in Table 3, which shows that our FRSR
shows comparable or better results compared to the oth-
ers. The EDSR [23] shows the best result, however, con-

HR

SRGAN

NatSR

ENet

EDSR

Bicubic

  3.18

  3.33

  3.80

  4.46

  6.21

  7.57

SRGAN

ENet

HR

NatSR

EDSR

Bicubic

  8.77

  8.76

  8.74

  8.55

  5.96

  3.89

0

2

4

6

8

10

0

2

4

6

8

10

NIQE in ascending order

NQSR in descending order

(a) NIQE in ascending order.

(b) NQSR in descending order.

Figure 8: NR-IQA results in the sorted order (left: NIQE
[27], and right: NQSR [24]). The best is at the top and the
worst is at the bottom.Our NatSR result is highlighted with
darker color.

sidering the number of parameters shown in the last row
of Table 3, our FRSR is also a competent method. As a
sub-experiment, we also evaluate the FR-IQA results on
the perception-oriented methods. Of course, the results are
worse than the distortion-oriented algorithms, sometimes
even worse than the bicubic interpolated images. Nonethe-
less, ours are slightly nearer to the original image in the
pixel-domain than the SRGAN and EnhanceNet.

7.4. NR IQA Results

We assess the methods with the NR-IQAs and the re-
sults are summarized in Figure 8, which shows the average
NIQE and NQSR tested with BSD100. As can be observed,
our NatSR is not the best but yields comparable measures
to other perception-oriented methods and the original HR.
As expected, one of the state-of-the-art distortion-oriented
methods, EDSR scores the worst in both metrics except for
the bicubic interpolation. For NIQE, besides the ground-
truth HR, SRGAN scores the best. Our NatSR scores the
second best for this metric. For NQSR, SRGAN scores the
best among all methods including the HR. Our NatSR ranks
lower than SRGAN and ENet, but the scores of all the meth-
ods including the HR show a slight difference. Although
the NatSR is not the best in both scores, we believe NatSR
shows quite consistent results to human visual perception
as shown in Figures 1 and 9, by suppressing the noisy and
blurry outputs through the NMD cost.

8. Subjective Assessments

8.1. Mean Opinion Score (MOS)

To better assess the perceptual quality of several results,
we conduct a mean opinion score (MOS) test with DIV2K
validation set [34]. For the fair comparison with recent
perception-oriented methods, SFT-GAN [37] is evaluated
with proper semantic segmentation mask to generate the
best performance. The details are in supplementary mate-
rial.

8128

Dataset

Scale

Bicubic

LapSRN

SRDenseNet

DSRN

EDSR

FRSR

SRGAN

ENet

NatSR

Set5

Set14

BSD100

Urban100

Parameters

4

4

4

4

4

28.42/0.8104

31.54/0.8850

32.02/0.8934

31.40/0.8830

32.46/0.8976

32.20/0.8939

29.41/0.8345

28.56/0.8093

30.98/0.8606

26.00/0.7027

28.19/0.7720

28.50/0.7782

28.07/0.7700

28.71/0.7857

28.54/0.7808

26.02/0.6934

25.67/0.6757

27.42/0.7329

25.96/0.6675

27.32/0.7280

27.53/0.7337

27.25/0.7240

27.72/0.7414

27.60/0.7366

25.18/0.6401

24.93/0.6259

26.44/0.6827

23.14/0.6577

25.21/0.7560

26.05/0.7819

25.08/0.7470

26.64/0.8029

26.21/0.7904

-

23.54/0.6926

25.46/0.7602

-

0.8 M

2.0 M

1.2 M

43 M

4.8 M

1.5 M

0.8 M

4.8 M

Table 3: FR-IQA results. The average PSNR/SSIM values on benchmarks. Red color indicates the best results, and the blue
indicates the second best.

HR

Bicubic

EDSR

FRSR (Ours)

ENet

NatSR (Ours)

Figure 9: Visualized results on ‚Äúimg031‚Äù of Urban100.

8.2. Visual Comparisons

We visualize some results in Figure 1, 9. As shown in
Figure 1, our NatSR shows the least distortion compared to
other perception-oriented methods. Also, Figure 9 shows
that distortion-oriented methods show blurry results while
perception-oriented ones show better image details. How-
ever, ENet produces unnatural cartoony scenes, and SFT-
GAN fails to produce natural details in buildings. More
results can be found in supplementary material.

9. Conclusion

In this paper, we have proposed a new approach for SISR
which hallucinates natural and realistic textures. First, we
start from the modeling of LR-HR space and SISR pro-
cess. From this work, we developed a CNN-based nat-
ural manifold discriminator, which enables to narrow the
target space into the natural manifold. We have also pro-

posed the SR generator based on the residual dense blocks
and fractal residual learning. The loss function is designed
such that our network works either as a distortion-oriented
or perception-oriented model. From the experiments, it is
shown that our distortion-oriented network (FRSR) shows
considerable gain compared to the models with similar pa-
rameters. Also, our perception-oriented network (NatSR)
shows perceptually plausible results compared to others.
We expect that with deeper and heavier network for gen-
erating better super-resolved images and also with better
classiÔ¨Åer as NMD, our method would bring more natural-
ness and realistic details. The codes are publicly available
at https://github.com/JWSoh/NatSR.
Acknowledgments This research was Ô¨Ånancially sup-
ported by the Ministry of Trade, Industry, and Energy
(MOTIE), Korea, under the ‚ÄúRegional Specialized Indus-
try Development Program(R&D, P0002072)‚Äù supervised by
the Korea Institute for Advancement of Technology (KIAT).

8129

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-
Ô¨Çow: a system for large-scale machine learning. In OSDI,
volume 16, pages 265‚Äì283, 2016. 7

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gen-
erative adversarial networks. In International Conference on
Machine Learning, pages 214‚Äì223, 2017. 5

[3] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-
Morel. Low-complexity single-image super-resolution based
on nonnegative neighbor embedding. In BMVC, 2012. 6, 7

[4] Y. Blau and T. Michaeli. The perception-distortion tradeoff.
In Proc. 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition, Salt Lake City, Utah, USA, pages
6228‚Äì6237, 2018. 3

[5] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep
convolutional network for image super-resolution. In Euro-
pean conference on computer vision. Springer, 2014. 1, 2

[6] X. Gao, K. Zhang, D. Tao, and X. Li. Image super-resolution
with sparse neighbor embedding. IEEE Transactions on Im-
age Processing, 21(7):3194‚Äì3205, 2012. 2

[7] L. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
using convolutional neural networks. In Advances in Neural
Information Processing Systems, pages 262‚Äì270, 2015. 2, 3

[8] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672‚Äì2680, 2014. 2, 3, 5

[9] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and
A. C. Courville. Improved training of wasserstein gans. In
Advances in Neural Information Processing Systems, pages
5767‚Äì5777, 2017. 5

[10] T. Guo, H. S. Mousavi, T. H. Vu, and V. Monga. Deep
wavelet prediction for image super-resolution. In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) Workshops, 2017. 5

[11] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S.
Huang. Image super-resolution via dual-state recurrent net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, 2018. 1, 2, 7

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770‚Äì778, 2016. 5

[13] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter. Gans trained by a two time-scale update rule
converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pages 6626‚Äì6637, 2017. 5

[14] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5197‚Äì5206, 2015. 2, 7

[15] Z. Hui, X. Wang, and X. Gao. Fast and accurate single im-
age super-resolution via information distillation network. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 723‚Äì731, 2018. 1, 2

[16] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution.
In European
Conference on Computer Vision, pages 694‚Äì711. Springer,
2016. 2, 3

[17] A. Jolicoeur-Martineau. The relativistic discriminator: a
arXiv preprint

key element missing from standard gan.
arXiv:1807.00734, 2018. 5

[18] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image super-
resolution using very deep convolutional networks. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 1646‚Äì1654, 2016. 1, 2, 5

[19] J. Kim, J. Kwon Lee, and K. Mu Lee. Deeply-recursive con-
volutional network for image super-resolution. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 1637‚Äì1645, 2016. 1, 2

[20] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 7

[21] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep
laplacian pyramid networks for fast and accurate superreso-
lution. In Proceedings of the IEEE conference on computer
vision and pattern recognition, 2017. 1, 2, 7

[22] C. Ledig, L. Theis, F. Husz¬¥ar, J. Caballero, A. Cunningham,
A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, 2017. 2, 3,
5, 6, 7

[23] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
deep residual networks for single image super-resolution. In
The IEEE conference on computer vision and pattern recog-
nition (CVPR) workshops, 2017. 1, 2, 3, 5, 7

[24] C. Ma, C.-Y. Yang, X. Yang, and M.-H. Yang. Learn-
ing a no-reference quality metric for single-image super-
resolution. Computer Vision and Image Understanding,
158:1‚Äì16, 2017. 7

[25] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smol-
ley. Least squares generative adversarial networks. In Com-
puter Vision (ICCV), 2017 IEEE International Conference
on, pages 2813‚Äì2821. IEEE, 2017. 5

[26] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of
human segmented natural images and its application to eval-
uating segmentation algorithms and measuring ecological
statistics. In Computer Vision, 2001. ICCV 2001. Proceed-
ings. Eighth IEEE International Conference on, volume 2,
pages 416‚Äì423. IEEE, 2001. 6, 7

[27] A. Mittal, R. Soundararajan, and A. C. Bovik. Making a‚Äù
completely blind‚Äù image quality analyzer. IEEE Signal Pro-
cess. Lett., 20(3):209‚Äì212, 2013. 7

[28] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spec-
tral normalization for generative adversarial networks. arXiv
preprint arXiv:1802.05957, 2018. 5

[29] M. S. Sajjadi, B. Sch¬®olkopf, and M. Hirsch. Enhancenet:
Single image super-resolution through automated texture
synthesis. In Computer Vision (ICCV), 2017 IEEE Interna-
tional Conference on, pages 4501‚Äì4510. IEEE, 2017. 1, 2,
3, 7

[30] W. Shi, J. Caballero, F. Husz¬¥ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single

8130

image and video super-resolution using an efÔ¨Åcient sub-
pixel convolutional neural network.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1874‚Äì1883, 2016. 1, 2

[31] J. Sun, Z. Xu, and H.-Y. Shum. Image super-resolution us-
ing gradient proÔ¨Åle prior.
In Computer Vision and Pattern
Recognition, 2008. CVPR 2008. IEEE Conference on, pages
1‚Äì8. IEEE, 2008. 2

[32] Y. Tai, J. Yang, and X. Liu. Image super-resolution via deep
recursive residual network. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, 2017. 1,
2

[33] Y.-W. Tai, S. Liu, M. S. Brown, and S. Lin. Super resolution
using edge prior and single image detail synthesis. In Com-
puter Vision and Pattern Recognition (CVPR), 2010 IEEE
Conference on, pages 2400‚Äì2407. IEEE, 2010. 2

[34] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang,
L. Zhang, B. Lim, S. Son, H. Kim, S. Nah, K. M. Lee,
et al. Ntire 2017 challenge on single image super-resolution:
Methods and results. In Computer Vision and Pattern Recog-
nition Workshops (CVPRW), 2017 IEEE Conference on,
pages 1110‚Äì1121. IEEE, 2017. 1, 7

[35] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted
anchored neighborhood regression for fast super-resolution.
In Asian Conference on Computer Vision, pages 111‚Äì126.
Springer, 2014. 2

[36] T. Tong, G. Li, X. Liu, and Q. Gao. Image super-resolution
In 2017 IEEE international

using dense skip connections.
conference on computer vision. IEEE, 2017. 1, 2, 7

[37] X. Wang, K. Yu, C. Dong, and C. C. Loy. Recovering realis-
tic texture in image super-resolution by deep spatial feature
transform. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, 2018. 1, 2, 3, 7

[38] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-
from error visibility to
IEEE transactions on image process-

Image quality assessment:

celli.
structural similarity.
ing, 13(4):600‚Äì612, 2004. 7

[39] J. Yang, Z. Wang, Z. Lin, S. Cohen, and T. Huang. Coupled
dictionary training for image super-resolution. IEEE trans-
actions on image processing, 21(8):3467‚Äì3478, 2012. 2

[40] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-
IEEE transactions on

resolution via sparse representation.
image processing, 19(11):2861‚Äì2873, 2010. 2

[41] R. Zeyde, M. Elad, and M. Protter. On single image scale-up
using sparse-representations. In International conference on
curves and surfaces, pages 711‚Äì730. Springer, 2010. 7

[42] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Residual
dense network for image super-resolution. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, 2018. 1, 2, 3, 5

[43] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based genera-
tive adversarial network. arXiv preprint arXiv:1609.03126,
2016. 5

8131

