Normalized Diversiﬁcation

Shaohui Liu1∗ Xiao Zhang2∗

Jianqiao Wangni2

Jianbo Shi2

1Tsinghua University

2University of Pennsylvania

b1ueber2y@gmail.com, {zhang7, wnjq, jshi}@seas.upenn.edu

Abstract

Generating diverse yet speciﬁc data is the goal of the
generative adversarial network (GAN), but it suffers from
the problem of mode collapse. We introduce the concept of
normalized diversity which force the model to preserve the
normalized pairwise distance between the sparse samples
from a latent parametric distribution and their correspond-
ing high-dimensional outputs. The normalized diversiﬁca-
tion aims to unfold the manifold of unknown topology and
non-uniform distribution, which leads to safe interpolation
between valid latent variables. By alternating the maxi-
mization over the pairwise distance and updating the total
distance (normalizer), we encourage the model to actively
explore in the high-dimensional output space. We demon-
strate that by combining the normalized diversity loss and
the adversarial loss, we generate diverse data without suf-
fering from mode collapsing. Experimental results show
that our method achieves consistent improvement on un-
supervised image generation, conditional image generation
and hand pose estimation over strong baselines.

1. Introduction

Diversity is an important concept in many areas, e.g.
portfolio analysis [33], ecological science [26] and recom-
mendation system [45]. This concept is also crucial to gen-
erative models which have wide applications in machine
learning and computer vision. Several representative exam-
ples include Variational Autoencoder (VAE) [21] and Gen-
erative Adversarial Network (GAN) [14], which are capable
of modeling complicated data. One ideal principle shared
by all generative models, simple or complex, is quite sim-
ilar, that the generated data should be diverse. Otherwise,
the model may have a so-called problem of mode collapse,
where all generated outputs are highly similar. This prob-
lem is more common in GAN [14] since the objective func-
tion is mostly about the validity of generated samples but
not the diversity of them.

∗ indicates equal contribution

Figure 1: Comparison of generative models’ capability to
learn from sparse samples with unknown topology (a donut
shape). Generated samples from GAN [14], BourGAN [39]
and ours are illustrated. GAN [14] suffers from mode col-
lapse. BourGAN [39] concentrates tightly around the train-
ing data points. Ours generates dense coverage around the
training samples with limited outliers.

Our goal
is to learn a generative model for high-
dimensional
image data which are non-uniformly dis-
tributed over a space with unknown topology, only using
very sparse samples. These conditions stated above, even
one of them being removed, may result in a much easier
problem to solve. We formulate the solution as learning a
mapping from a low-dimensional variable in latent space Z
to an image deﬁned in output space Y. However, we face
a dilemma that, in order to ﬁt the complicated topology of
output data, we need the learned mapping to be complex
and highly expressive, which actually requires dense sam-
ples from real-world. This problem might be alleviated if
the topology is simpler since a parametrized function of
smaller VC dimension might be able to ﬁt. This situation
however is most likely not the case if we need to deal with
arbitrary images.

We start from an idea that is orthogonal to previous re-
search: that we aim to expand as well as unfold the mani-
fold of generated images, actively and safely. We want to
model the data with complex topology in output space us-
ing a parametrized mapping from a latent space with sim-
ple topology. To learn a model that is speciﬁc to training
images and also generalizable, we wish to achieve that the
interpolation and extrapolation on a pair of latent variables
should generate valid samples in output space. This requires
the generative model, globally, to generate diverse and valid
images via exploration, which we refer to as safe extrapo-
lation, and locally, to preserve the neighbouring relation so
that interpolation is also safe.

10306

Z

norm. gθ(z)
Target Y pairwise dist.

norm. gθ(z)
iter. 1000 pairwise dist.

norm. gθ(z)
iter. 4000 pairwise dist.

iter. 8000

gθ(z)

iter. 8000

norm. z

norm. gθ(z)
pairwise dist. Target Y pairwise dist.

iter. 100

norm. gθ(z)
pairwise dist.

iter. 200

norm. gθ(z)
pairwise dist.

iter. 1000

gθ(z)

iter. 1000

Figure 2: Illustration on the training procedure with proposed normalized diversiﬁcation on highly irregular topology (top
row) and non-uniform data density (bottom row). The generative model can effectively learn from sparse data by constructing
a mapping from the latent space Z to the target distribution by minimizing normalized diversity loss. Top Left: The latent
variable z ∈ Z in 2D space is sampled from an uniform distribution. 5 points (P1-P5, colored blue) along the diagonal
are used for illustration. Bottom Left: The normalized pairwise distance matrix on P1-P5. From Left to Right: We show
qualitative results on two synthetic cases: ‘HI’ and ‘Ring’. We visualize the mapping from the latent space to the output
space for several iterations together with DY ∈ R5×5 and we illustrate safe interpolation on the diagonal of the latent space
onto the output space. Right Most Column: We generate dense samples from the learned model, to illustrate the diversity
and the consistency w.r.t. the ground-truth distribution shown in the second column.

In this paper, we propose normalized diversiﬁcation fol-
lowing the motivation above. This is achieved by combin-
ing the adversarial loss of GAN and a novel normalized di-
versity loss. This diversity loss encourages the learned map-
ping to preserve the normalized pairwise distance between
every pair of inputs z, z′ ∈ Z and their corresponding out-
puts gθ(z), gθ(z′) ∈ Y, where gθ is a parametrized function.
This utilizes the same insight as manifold unfolding [37].
During training, we also ﬁx the normalization term while
maximizing the pairwise distance, which also encourages
the model to visit outer modes i.e. to extrapolate.

To illustrate the concept, we sample sparse points from a
donut shape as training data for GAN [14], BourGAN [39]
and our method, shown in Figure 1. After the model be-
ing trained, we generate 5k new samples from the learned
distribution for visualization. Our method achieves safe in-
terpolation, ﬁlls in the gap in sparse samples and generates
dense coverage while GAN [14] and BourGAN [39] fails to
generalize.

Our paper is organized as follows: Section 3 presents
more motivations towards a better understanding of normal-
ized diversiﬁcation. Section 4 describes how we apply nor-
malized diversiﬁcation onto multiple vision applications by
employing different metric spaces on the outputs. Finally
in Section 5, we show promising experimental results on
multiple tasks to demonstrate the effectiveness.

2. Related Work

Early attempts for addressing the problem of mode col-
lapse include maximum mean discrepancy [10], boundary
equilibrium [4] and training multiple discriminators [9].

Later, some other works [1, 5, 27] modiﬁed the objective
function. The usage of the distance matrix was implicitly
introduced as a discrimination score in [32] or covariance
in [20]. Recently, several novel methods via either statisti-
cal or structural information are proposed in [35, 24, 39].

Most problems in computer vision are fundamentally ill-
posed that they have multiple or inﬁnite number of solu-
tions. To obtain a better encoder with diverse generated
data, there are a variety of ideas making full use of the VAE
[21], GAN [14] and its conditional version to develop better
models [3, 18, 23, 31, 34, 44]. Huang et al. [17] proposed
to learn a disentangled representation, Ye et al. [41] used
a parametrized GMM, other works like [2, 6, 11, 12] selec-
tively back-propagated the gradients of multiple samples.
The normalized diversiﬁcation, deﬁned with pairwise terms
on the mapping function itself, appears orthogonal to most
of the existing methods.

3. Method

We consider a generative model gθ that generates an im-
age y ∈ Y based on a latent variable z. The target of
training the model is to ﬁt an unknown target distribution
pdata(y) based on limited samples. In this paper, we con-
sider two different kinds of implicit generative models that
solve different problems but intrinsically share the same
spirit of diversiﬁcation.

• Unsupervised generative model. This model is used
for tasks that do not depend on auxiliary information,
e.g.
image synthesis without supervision. Existing
methods such as GAN [14] and VAE [21] use a latent

10307

variable z ∈ Z that follows a parametric distribution
p(z) in latent space and learn the mapping gθ : Z → Y
to ﬁt the target y ∈ Y.

• Conditional generative model.

This model uti-
lizes additional information to generate more speciﬁc
outputs, e.g.
text-to-image (image-to-image) trans-
lation, pose estimation, future prediction. Related
works include a variety of conditional generative mod-
els [3, 18, 23, 31, 44]. They also use a predeﬁned latent
space Z and aim to ﬁt the joint distribution (X , Y) for
the input domain X and output domain Y (for conve-
nience we also use Y here). Speciﬁcally, an encoder
E : X → C is used to get the latent code c ∈ C.
Then, either addition (VAE) or concatenation (CGAN)
is employed to combine c and z for training generator
gθ : C × Z → Y.

The problem of mode collapse is frequently encountered es-
pecially in generative models like GAN [14], that the model
generates highly similar data but satisﬁes the criterion for
training. Our motivation for normalized diversiﬁcation is
to encourage gθ to generate data with enough diversity, so
the model visits most of the important modes. In the mean-
time, we wish gθ to be well-conditioned around the visited
modes, so we could infer latent variables from some valid
image samples, and ensure safe interpolation and extrapola-
tion between these latent variables to generate meaningful
images. We could address the problem of mode collapse by
diversifying the outputs, which is similar to enlarging the
variance parameter of a Gaussian distribution, but a hard
problem is how to measure the diversity of real-world im-
ages analytically and properly, as the variance of a distribu-
tion might be too universal for general tasks to be speciﬁc
to our problem.

We start with an intuition to prevent the pairwise distance
d(·, ·) between two generated points A and B from being
too close. Note that if the outputs are linearly scaled, so
will the pairwise distance d(A, B): the samples seem to di-
versify from each other, without actually solving the intrin-
sic problem of mode collapse. Thus, we measure whether
the mapping preserves the normalized pairwise distance be-
tween inputs kz − z′k and outputs kgθ(z) − gθ(z′)k.

Since we do not have access to inﬁnite amount of data,
we sample a limited amount of data from a well-deﬁned
parametric distribution p(z), and try to visit more mode via
diversiﬁcation. We measure the diversity of generated sam-
ples by the pairwise distance in Y through the parametrized
mapping gθ. The objective function is simpliﬁed as a ﬁnite-
sum form on N samples {zi}N
i=1, along with corresponding
images {yi|yi = gθ(zi)}N
i=1. We denote two metric spaces
MZ = (Z, dZ) and MY = (Y, dY ). We use two addi-
tional functions hY and hZ for some task-speciﬁc usage,
and then deﬁne the metric as composite functions upon Eu-

Figure 3: Quantitative comparison between normalized di-
versity loss and BourGAN loss [39] on the learned distri-
bution in Fig. 1. We discretized the donut region into uni-
form mesh grids and measured “cover rate” (the percentage
of grids which have generated samples in them). We also
measured “outlier rate”:
the ratio of samples outside the
donut. “Data sparsity” measures the cover rate of the train-
ing samples over the donut. Our method improve the cover
rate over GAN [14] and BourGAN [39] while maintaining
comparable outlier rate.

clidean distance, as follows,

dZ(zi, zj) = khZ(zi) − hZ(zj)k2
dY (yi, yj) = khY (yi) − hY (yj)k2

(1)

(2)

To explain with a concrete application in diverse video syn-
thesis that we try to generate a realistic video from a moving
vehicle based on a sequence of segmentation mask, here dY
may be used for extracting deep features using an off-the-
shelf network like the perceptual loss [19]. We also deﬁne
DZ, DY ∈ RN ×N to be the normalized pairwise distance
matrix of {zi}N
i=1 respectively in Eq.(3). Each
element in the matrix is deﬁned as

i=1 and {yi}N

DZ

ij =

dZ(zi, zj)

Pj dZ(zi, zj)

, DY

ij =

dY (yi, yj)

Pj dY (yi, yj)

,

(3)

for ∀i, j ∈ [N ]. The normalized diversity loss can be for-
mulated as in Eq.(4), where α ≈ 1 is a slack factor of this
diversity loss.

Lndiv(θ, p) =

1

N 2 − N

N

N

X

X

i=1

j6=i

max(αDZ

ij −DY

ij , 0). (4)

To enforce the extrapolation, we treat the normalizer in
Eq.(3) as a constant when back-propagating the gradient to
generator. As a result minimizing Eq.(4) would also force
the model to actively expand in the high-dimensional out-
put space, we refer to Algorithm 1 and Section 3.1 for more
details.

For the purpose of unfolding the manifold, we could fo-
cus on the expansion of densely connected pairs of DZ
ij >
DY
ij or on the contraction of loosely connected pairs of
ij < DY
DZ
ij , 0) would
encourage active extrapolation when we hold the normal-
izer constant.

ij . Only minimizing max(αDZ

ij − DY

10308

Combining the diversity loss, the objective function over

θ can be written in a compact form as

min

θ

L(θ, p) = Lgen(θ, p) + Lndiv(θ, p)

(5)

where Lgen(θ, p) is the original objective function for learn-
ing the generator gθ. In the conditional model, the objective
function also depends on x which we omitted here.

3.1. Interpolation and Extrapolation

A fundamental motivation behind the normalized diver-
siﬁcation is to achieve that interpolation and extrapolation
on a pair of latent variables should generate valid samples
in output space. This pursues a trade-off between pairwise
diversity and validity. This motivation aligns with the con-
cept of local isometry [38], which has a nice property that
the near neighbors in Z, are encoded to Y using a rotation
plus a translation. Local isometry requires the output man-
ifold to be smooth and without invalid ‘holes’ within the
neighbor region of a valid point, and the interpolation in Z
generates valid points in Y through a parametrized mapping
gθ where θ is the learned parameter. Differently, Variational
Autoencoder (VAE) [21] mostly cares about local perturba-
tion, or pointwise diversity.

Normalized diversiﬁcation is orthogonal to previous re-
search as it aims to unfold the manifold of generated images
[37]. Interpolation. By pushing apart the pairwise distance
of the sample points, we prevent the ’short-cuts’ that links
samples through the exterior space. As shown in Figure 1,
given a set of points, our goal is to discover the underlying
parametrization so we can densely generate new valid sam-
ples in the interior (on the donut), without crossing over to
the exterior (the donut hole). This leads to safe interpola-
tion. Extrapolation. For active exploration of output space
Y with the current model, in each iteration, we ﬁrst cal-
culate the normalizer of the pairwise distance matrix, then
use the gradient back-propagated from the dY
ij to force ex-
pansion, after which we update the normalizer. With these
alternating steps, we ensure the stability of exploration. We
illustrate the evolution of the training procedure on two syn-
thetic 2D distributions in Figure 2

3.2. Understanding from Geometric Perspective

Simply trying to enlarge the pairwise distance between
samples in Y can explore the unobserved space, but a cru-
cial problem is how to make sure the interpolated points
are still valid. From a geometric perspective, imagine that
A, B, C are on a curvy 1D line in 2D space, the transitive
distance between them, d(A, B) + d(B, C) on the 1D curve
is much longer than 2D distance d(A, C), which violates
the triangle inequality. If we make an interpolation point
D in the inner part of the line (A, C), although the direct
2D distance between d(A, C) could be very small, the line

between them might lie in the part of unreasonable space
out of the manifold. However, by pushing (A, C) as far as
possible away, we ‘discover’ the true 1D distance.

This insight is different from existing approaches, e.g.
BourGAN [39], which aims at matching the pairwise dis-
tance between output space and latent space which can pre-
serve the data modality but hinder its generalization ability.
Our method, besides the mode preserving ability, can also
actively expand and unfold the manifold which enables safe
interpolation on the latent space where the generated sam-
ples will neither lie outside the valid region nor overﬁt the
existing data (See Figure 1 and 3).

3.3. Understanding via Simpliﬁed Models

To understand the normalized diversiﬁcation, we start
from another perspective by using simple functions to illus-
trate the functionality of the regularization term. We assume
the generator to be a simple linear model as gθ(z) = θT z,
where θ is the matrix that characterizes the linear trans-
formation. A simple calculation induces that the diversity
regularization thus encourages kθk⋆ (or kθk2) being sufﬁ-
ciently large. However, this alone does not prevent some de-
generated cases. Suppose that the generator is constructed
in the following way

gθ(z) = θT z,

θ = U T diag[β, 0, 0, · · · , 0]V ∈ RK×D,

where K and D are the dimension of the latent variable z
and generated data respectively, and U, V are the matrices
consisting of all singular-vectors. When β is sufﬁciently
large, this model seems to be strongly diversiﬁed, however,
is actually not a reasonable model as it measures the dif-
ference of two vectors along only one direction, i.e.
the
singular-vector corresponding to the singular value β. Nor-
malization over the diversity helps to prevent these kinds of
degenerated cases, as the normalized distance does not scale
with β. The normalization also helps to adapt to a condi-
tional setting that some related works e.g. BourGAN [39]
might fail to adapt, based on an input variable x and a latent
variable z, especially with large variations w.r.t. x. Imag-
ine a simple example where y = gθ(x, z) = (x + z)3, and
x ∈ [1, 10]. The upper bound of Lipschitz constant could be
larger than 3max(x)2 = 300 while the lower bound should
be lower than 3min(x)2 = 3. However, this value is mean-
ingless for the most part of the whole domain [1, 10]. Nor-
malizing the distance could ﬁx it to a reasonable range.

Another side problem is on how to keep the semi-
deﬁniteness of the distance metric, to concord with metric
learning research [8, 37, 38, 40]. Under this property, a pair
of samples (z, z′) deviating from each other in whichever
direction, should increase the pairwise distance in the out-
put space. This property should also be enforced by di-
versiﬁcation, although may not explicitly. This formulation
of taking nonnegative part using max(·, 0) encourages the

10309

Figure 4: Qualitative results on ‘2D Gaussian ring’ and ‘2D
Gaussian grid’. For baseline methods, We directly used the
visualization results in [39].

learned metric in Y to be positive semideﬁnite (PSD). To
understand with a counter-example, we consider a metric
deﬁned as

dY (y, y′) = (y − y′)T Φ(y − y′), where Φ ∈ RD×D.

We assume the eigen-decomposition of Φ to be

Φ = U T diag[−β, 100β, 0, 0, · · · , 0]U

(6)

where β > 0 (eigen-decomposition), and Φ is not PSD.
Suppose there is a pair of latent variables (y, y′) that y −
y′ = Pd Ud/D, where summation is taken over all eigen-
vectors. This pair of data incurs a variation of 99β of Lndiv
in Eq.(4) if the nonnegative operator max(·, 0) is not used,
which leads to the objective function being minimized if β
positively scales, but clearly, the metric is not reasonable as
it is not PSD. However, if the operator is applied, the same
case incurs a variation of β of Lndiv from a deviation in
the subspace corresponding to the negative eigenvalue −β,
as the max(·, 0) operator strongly contrast those directions
like U1 that wrongly contribute to Lndiv, and ignore other
subspace even if they correspond to strongly positive eigen-
values.

4. Real-World Applicability

In this section, we present the real-world applicability of
normalized diversiﬁcation for different vision applications.
For different tasks, we need a validity checking function F
of the generated samples to validate that the data satisﬁes
some domain-speciﬁc constraints. For example, for the un-
supervised setting, F can be interpreted as the trained dis-
criminator in GAN [14].

Our training pipeline is summarized in Algorithm 1. For
a given latent distribution p(z) and the target distribution
pdata, we sample a ﬁnite set of latent samples {zi}n
i=1 from
p(z) and {yi}n
i=1 from pdata. We then compute normal-
ized pairwise distance matrix over {zi}n
i=1 and the gener-
ated samples {gθ(zi)}n
i=1 with the functions dZ and dY . We
update the normalizer and block its gradients as previously

Algorithm 1 Training generative model with normalized di-
versity.

1: Given: Latent distribution p(z), Target distribution

pdata

2: Given: Current generator gθ, Distance function dZ , dY
3: function NORMDIST({qj}N
4:

for i ← 1 to N do

i=1, d):

Si = PN
Si ← Si.value (treated as constant value with

j=1 d(qi, qj)

back-propagated gradients blocked)
Dij = d(qi, qj)/Si, j ∈ [N ]

8:

end for
return D
9:
10: end function
11: while not converged do
12:

i=1 ∼ p(z)
i=1 ∼ pdata

{zi}N
{yi}N
DZ =NORMDIST({zi}N
DY =NORMDIST({gθ(zi)}N
(Optional for GAN) Update the discriminator F .
Compute total loss L by Eq. 5
Update model parameter θ = θ − η∂L/∂θ

i=1, dY )

i=1, dZ )

18:
19: end while

5:

6:

7:

13:

14:

15:

16:

17:

discussed. Finally, we compute the total loss L in Eq.(5)
and update the model with the back-propagated gradients
for each step.

4.1. Applications

The proposed normalized diversity term is general and
can be applied to many vision applications including image
generation, text-to-image (image-to-image) translation and
hand pose estimation, etc. We specify the conditional in-
put domain X , the output domain Y and the function hY
for each application to compute normalized diversity loss
in Eq.(4). We use a predeﬁned Z with uniform distribution
and hz(z) = z for all tasks.

• Image generation. There is no conditional input for
this unsupervised generative model setting. Y is the
output image domain. The normalized pairwise dis-
tance on Y can be computed either with ℓ1 or ℓ2 dis-
tance or employing deep metrics such as using the
GAN [14] discriminator as the function hY .

• Conditional image generation. The conditional input
domain X can be the input text and input image for
text-to-image and image-to-image translation respec-
tively. Y is the output image domain. The computation
of normalized pairwise distance on Y is similar to that
in image generation.

• Hand pose estimation. The conditional input domain
X is the input RGB hand image. Y ∈ R3K is the

10310

GANUnrolledVEEGANPacGANOursTable 1: Results on the synthetic dataset. We followed the
experimental setting in [39].

2D Ring

2D Grid

Method

#modes

fail (%)

#modes

fail (%)

GAN [14]

Unrolled [27]

Ours

VEEGAN [35]
PacGAN [24]
BourGAN [39]

1.0
7.6
8.0

8.0
7.8
8.0

0.1
12.0
10.3

13.2
1.8
0.1

17.7
14.9
25.0

24.4
24.3
25.0

17.7
95.1
11.1

22.8
20.5
4.1

output 3D pose, where K denotes the number of the
joints. For the function hY , we employ the visibility
mask V ∈ {0, 1}3K to gate each joint of the output 3D
pose inversely, encouraging diversity on the occluded
joints, i.e. hY (y) = (1 − V ) ◦ y.

Note that [39] cannot ﬁt in the conditional setting be-
cause the number of possible conditional inputs, e.g. input
text, RGB hand image are inﬁnite, making the pairwise in-
formation for each conditional input impossible to be pre-
computed. Speciﬁcally for hand pose estimation, we de-
velop a pseudo-renderer by using morphological clues to
get a visibility mask.

The application of normalized diversity can ﬁt in arbi-
trary checking function F for different tasks. When com-
pared to baseline adversarial methods, we use adversar-
ial loss for image generation either unsupervised or condi-
tional. For hand pose estimation, we choose to combine the
l2 distance on the visible joints and the joint adversarial dis-
criminator of (xi, ˆyi) (image-pose GAN), where ˆyi denotes
the 2D projection of the output 3D pose yi.

F (x, y) = Fvis(x, y) + Fgan(x, y)

The two checking functions are formulated as follows

Fvis(x, y) = kV ◦ (y − yr)k2
2
Fgan(x, y) = Ex,ˆyr ∼pdata [log D(x, ˆyr)]

+ E

x∼pdata,z∼p(z)[log(1 − D(x, ˆy))]

(7)

(8)

(9)

With the image-pose GAN, the system has the capability to
learn much subtle relationship between x and y. By con-
straining only on the visible joints, our system gets less
noisy gradients and can learn better pose estimation along
with better image features.

5. Experiments

We conducted experiments on multiple vision tasks un-
der both unsupervised setting (Section 5.1 and 5.2) and con-
ditional setting (Section 5.3 and 5.4) to demonstrate the ef-
fectiveness of the proposed idea.

“SN”
Table 2: FID Results [16] on Image Synthesis.
and “GP” denote spectral normalization [28] and gradient
penalty [15] respectively. Our method achieved consistent
improvement over various GANs.

CIFAR-10

CelebA

w/o ndiv w ndiv w/o ndiv w ndiv

GAN+SN

GAN+SN+GP
WGAN+GP

WGAN+SN+GP

23.7
22.9
25.1
23.7

22.9
22.0
23.9
23.3

10.5
9.4
9.9
9.2

10.2
9.1
9.5
9.0

Compared to the baseline, we added the normalized di-
versiﬁcation loss with all other settings unchanged. We sim-
ply used α = 0.5 for Eq.(4) in most experiments except
conditional image generation, where we used α = 0.8.

5.1. Synthetic Datasets

We tested our method on the widely used 2D Gaus-
sian ring and 2D Gaussian grid to study the behavior of
mode collapse. For each iteration in our method, we per-
formed line 16 and 18 in Algorithm 1 all repeatedly for
3 times for faster convergence. Results are shown in Ta-
ble 1. We compared several baselines [14, 27, 35, 24, 39].
Of those methods VEEGAN [35], PacGAN [24] and Bour-
GAN [39] require pairwise information of the real image
sets, which is an expensive pre-requisite and cannot ﬁt in
the conditional setting. Moreover, BourGAN [39] suffered
from severely overﬁtting the training samples (See Fig. 1).
Our method, with the normalized diversiﬁcation, achieved
promising results in the long-term training (50k iterations)
with only on-demand pairwise information. In Fig. 4, we
qualitatively demonstrate the results on synthetic point sets
where our method achieve comparable or even better results
over VEEGAN [35] and PacGAN [24] without using two or
more real samples at a time.

5.2. Image Generation

We further tested our method on the widely accepted task
of image generation. We conducted experiments on CIFAR-
10 [22] and CelebA [25] with state-of-the-art methods via
the off-the-shelf library1. Because there exist relatively
dense samples in this setting, conventional methods ﬁt the
problem relatively well. Nevertheless, as shown in Ta-
ble 2, with the normalized diversiﬁcation loss, our method
achieves consistent improvement on CIFAR-10 [22] and
CelebA [25] over strong baselines with spectral normal-
ization [28] and gradient penalty [15]. By regularizing the
model behavior around the visited modes through normal-
ized diversiﬁcation, the generator maintains a safe extrapo-
lation depending on the current outputs.

1https://github.com/google/compare gan

10311

Table 3: Results of text-to-image translation on [30]. 15
volunteers voted for the better of the randomly sampled
pairs for a random text on the webpage. Diversity was com-
puted via the perceptual metric [43] using Inception model
[36].

GAN-CLS [31]

Ours

Neutral

votes

287 (31.3%)
291 (31.7%)
340 (37.0%)

diversity
43.7±16.8
58.1±18.5

-

Table 4: Quantitative comparison with BicycleGAN [44] on
conditional facade image generation [7]. We computed FID
[16] with the real data to measure the image quality (lower
better) and used LPIPS [43] to measure the output diversity
(higher better).

Real data
BicycleGAN [44]
Ours

FID ↓
0.0
83.0
76.3

LPIPS ↑
0.291
0.136 ± 0.049
0.188 ± 0.048

5.3. Conditional Image Generation

We tested the performance of conditional image gener-
ation described in Section 4.1. For text-to-image transla-
tion, we directly followed the architecture used in [31] for
fair comparison and added the diversity loss term to the
overall objective function. We used a discriminator as hY
to extract features from synthesized images and measured
the pairwise image distance in feature space. Experiments
were conducted on Oxford-102 Flower dataset [30] with 5
human-generated captions per image. We generated 5 sam-
ples for each text at training stage. Both the baseline and our
method were trained for 100 epochs on the training set. We
employed the zero-shot setting where we used the 3979 test
sets to evaluate both methods. We evaluated the quality and
the diversity of the generated images. For the diversity mea-
surement, we ﬁrst generated 10 images for each input text,
then used the perceptual metric [43] via Inception model
[36] to compute the variance of the 10 generated samples
and took average over test sets. Table 3 shows the results
on [30]. With comparable image quality, we signiﬁcantly
improved upon the baseline in terms of the diversity.

We compare with a strong baseline, BicycleGAN [44],
on image-to-image translation to further demonstrate our
ability to model multimodal distribution. We removed the
conditional latent regression branch of BicycleGAN [44]
and added normalized diversiﬁcation. We used ℓ1 as the
pairwise image distance. Table 4 and Fig. 5 show the re-
sults on [7]. Even without the conditional latent regression
branch [44], our method outperforms BicycleGAN [44] on
both image quality and diversity.

Figure 5: Qualitative results of conditional image genera-
tion on facade dataset [7]. Our method improves both im-
age quality and diversity over BicycleGAN [44]. Top left:
input image. Bottom Left: corresponding groundtruth im-
age. Top Right: ﬁve generated images from BicycleGAN
[44]. Bottom Right: ﬁve generated images from Ours.

5.4. Hand Pose Estimation

We conducted experiments on three RGB hand datasets
including GANerated hands [29], Stereo [42], and FPHAB
[13]. For all datasets, we manually cropped the padded hand
bounding box and resized the input to 128x128. Follow-
ing [29], we directly used the net architecture of their re-
leased model. The weights were initialized from ImageNet
pretrained model. For each image, our multimodal system
generated 20 samples for training and 100 samples for test-
ing. We used zdim = 10 for all the conditional model. Our
method used channel-wise concatenation to aggregate the
encoded features ci and zi. Each dimension of zi was sam-
pled from a uniform distribution U (0, 1).

Evaluating the multimodal predictions is a non-trivial
task. Conventional methods put a max operation on top
of the multiple predictions and use the samples nearest the
groundtruth for evaluation. Some works even use this max
operation over each joint. This results in relatively unfair
comparison because simply drawing a sample uniformly
distributed in U (0, 1) will result in near zero error. Thus,
we introduce a better evaluation protocol for the multimodal
hand pose estimation: 1) Visible joint accuracy. For the
visible joints, we use the Percentage of Correct Keypoints
(PCK) following conventional methods. 2) Standard devia-
tion. We compute standard deviation of the outputs for each

10312

Table 5: Visible joint accuracy
on GANerated hands [29]. Using
a conditional model along with
the proposed normalized diversi-
ﬁcation helps on the visible joints
predictions. Our method achieves
1.6 (91.6→93.2) accuracy gain
over deterministic regression.

Regression
Ours w/o diversity loss
Ours
Ours+

91.6
91.7
92.8
93.2

Figure 6: Results on successful rate and
standard deviation. We gets signiﬁcant im-
provements on both quality and diversity.

Figure 7: Results on Stereo dataset [42].
Our method outperforms existing state-of-
the-art alternatives by a large margin.

Figure 8: Qualitative results of the multiple pose predictions on GANerated hands [29] and FPHAB [13]. We show 3D hand
predictions and its projections on 2D image (better viewed when zoomed in). With comparable variance, while VAE [21]
failed to generated high quality samples, our model generates multiple valid 3D poses with subtle image-pose correlations.

image and take average. 3) Successful rate. For each im-
age, we use the pre-computed hand mask and object mask
to check validity of the samples. The sample is considered
valid if the whole hand conﬁguration including the occluded
joints lie inside the foreground mask.

each sample. With normalized diversiﬁcation, the model
maintains a centralized structure which has good property
for pairwise interpolation. This promotes “safer” extrapo-
lation for robust occluded joints detection which leads to
valid yet diversiﬁed outputs.

We ﬁrst compare our method with the deterministic re-
gression method on the GANerated hand dataset [29]. We
concatenate the encoded feature ci ∈ R100 with zi at bot-
tleneck which is contrast to the architecture of VAE [21]
where they restricted the latent space zi to be in the form of
Gaussian distribution. ‘Ours+’ denotes our model adapting
a higher dimensional latent space ci ∈ R16384. We use 100
dimensional latent space zi in all experiments. Combining
Table 5 and Figure 6, it is clear that using a one-to-many
conditional model beneﬁts much on the visible joints as well
as the successful rate. Our method achieves signiﬁcant im-
provements on all three metrics, while VAE [21] struggles
more on its quality-diversity trade-off.

Moreover, we tested our method on two real-world
datasets including the Stereo Tracking Benchmark [42] and
FPHAB [13]. Our method outperforms state-of-the-art
methods both quantitatively and qualitatively. As is shown
in Figure 8, our method captures much subtle ambiguity
and could generate accurate predictions for visible joints on

6. Conclusion

In this paper, we proposed normalized diversiﬁcation,
a generalized loss on the mapping function measuring
whether the mapping preserves the relative pairwise dis-
tance to address the problem of mode collapse. We aim
to diversify the outputs with normalized pairwise distance,
encouraging safe interpolation in the latent space and ac-
tive extrapolation towards outer important states simulta-
neously. Results show that by employing different metric
spaces, normalized diversiﬁcation can be applied to mul-
tiple vision applications and achieves consistent improve-
ments on both encoding quality and output diversity.

Acknowledgements

We gratefully appreciate the support from Honda Re-
search Institute Curious Minded Machine Program. We sin-
cerely thank Dr. Gedas Bertasius and Xin Yuan for valuable
discussions.

10313

0.000.050.100.150.200.250.300.35Standard Deviation0.260.280.300.320.340.360.380.40Successful RateRegressionVAEOurs w/o diversity lossOursOurs+20253035404550Error Thresholds (mm)0.50.60.70.80.91.03D PCKICPPSO (AUC=0.748)CHPR (AUC=0.839)Z&B (AUC=0.948)Mueller et al.(AUC=0.955)Ours avg(AUC=0.980)Ours max(AUC=0.988)References

[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gener-
ative adversarial networks. In ICML, pages 214–223, 2017.
2

[2] A. Bansal, Y. Sheikh, and D. Ramanan. Pixelnn: Example-

based image synthesis. 2018. 2

[3] J. Bao, D. Chen, F. Wen, H. Li, and G. Hua. Cvae-gan: Fine-
In

grained image generation through asymmetric training.
ICCV, pages 2745–2754, 2017. 2, 3

[4] D. Berthelot, T. Schumm, and L. Metz. Began: boundary
equilibrium generative adversarial networks. arXiv preprint
arXiv:1703.10717, 2017. 2

[5] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li. Mode
regularized generative adversarial networks. In ICLR, 2017.
2

[6] Q. Chen and V. Koltun. Photographic image synthesis with
cascaded reﬁnement networks. In ICCV, volume 1, page 3,
2017. 2

[7] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In CVPR, pages 3213–3223, 2016. 7

[8] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon.
Information-theoretic metric learning. In ICML, pages 209–
216. ACM, 2007. 4

[9] I. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-

adversarial networks. In ICLR, 2017. 2

[10] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training
generative neural networks via maximum mean discrepancy
optimization. In UAI, 2015. 2

[11] H. Fan, H. Su, and L. J. Guibas. A point set generation net-
In

work for 3d object reconstruction from a single image.
CVPR, volume 2, page 6, 2017. 2

[12] M. Firman, N. D. Campbell, L. Agapito, and G. J. Brostow.
Diversenet: When one right answer is not enough. In CVPR,
pages 5598–5607, 2018. 2

[13] G. Garcia-Hernando, S. Yuan, S. Baek, and T.-K. Kim. First-
person hand action benchmark with rgb-d videos and 3d
hand pose annotations. In CVPR, 2018. 7, 8

[14] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, pages 2672–2680, 2014. 1,
2, 3, 5, 6

[15] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.
Courville. Improved training of wasserstein gans. In NIPS,
pages 5767–5777, 2017. 6

[16] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter. Gans trained by a two time-scale update rule
converge to a local nash equilibrium. In NIPS, pages 6629–
6640. 2017. 6, 7

[17] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal
unsupervised image-to-image translation. In ECCV, 2018. 2

[19] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution.
In European
conference on computer vision, pages 694–711. Springer,
2016. 3

[20] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
In ICLR, 2018. 2

[21] D. P. Kingma and M. Welling. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013. 1, 2, 4, 8

[22] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009. 6

[23] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and
O. Winther. Autoencoding beyond pixels using a learned
similarity metric. In ICML, pages 1558–1566, 2016. 2, 3

[24] Z. Lin, A. Khetan, G. Fanti, and S. Oh. Pacgan: The power of
two samples in generative adversarial networks. In NeurIPS,
2018. 2, 6

[25] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face

attributes in the wild. In ICCV, pages 3730–3738, 2015. 6

[26] A. E. Magurran. Why diversity? In Ecological diversity and

its measurement, pages 1–5. Springer, 1988. 1

[27] L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled

generative adversarial networks. In ICLR, 2017. 2, 6

[28] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral
normalization for generative adversarial networks. In ICLR,
2018. 6

[29] F. Mueller, F. Bernard, O. Sotnychenko, D. Mehta, S. Srid-
har, D. Casas, and C. Theobalt. Ganerated hands for real-
time 3d hand tracking from monocular rgb. In CVPR, pages
49–59, 2018. 7, 8

[30] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Computer Vision,
Graphics & Image Processing, 2008. ICVGIP’08. Sixth In-
dian Conference on, pages 722–729. IEEE, 2008. 7

[31] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and
H. Lee. Generative adversarial text to image synthesis. In
ICML, pages 1060–1069, 2016. 2, 3, 7

[32] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad-
ford, X. Chen, and X. Chen. Improved techniques for train-
ing gans. In NIPS, pages 2234–2242. 2016. 2

[33] B. H. Solnik. Why not diversify internationally rather than
Financial analysts journal, pages 48–54,

domestically?
1974. 1

[34] A. Spurr, J. Song, S. Park, and O. Hilliges. Cross-modal
deep variational hand pose estimation. In CVPR, pages 89–
98, 2018. 2

[35] A. Srivastava, L. Valkoz, C. Russell, M. U. Gutmann, and
C. Sutton. Veegan: Reducing mode collapse in gans us-
ing implicit variational learning. In NIPS, pages 3308–3318,
2017. 2, 6

[36] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 7

[18] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 2, 3

[37] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric
learning for large margin nearest neighbor classiﬁcation. In
NIPS, pages 1473–1480, 2006. 2, 4

10314

[38] K. Q. Weinberger and L. K. Saul. Unsupervised learning
IJCV,

of image manifolds by semideﬁnite programming.
70(1):77–90, 2006. 4

[39] C. Xiao, P. Zhong, and C. Zheng. Bourgan: Generative net-
works with metric embeddings. In NeurIPS, 2018. 1, 2, 3, 4,
5, 6

[40] E. P. Xing, M. I. Jordan, S. J. Russell, and A. Y. Ng. Dis-
tance metric learning with application to clustering with
side-information. In NIPS, pages 521–528, 2003. 4

[41] Q. Ye and T.-K. Kim. Occlusion-aware hand pose estimation
using hierarchical mixture density network. In ECCV, 2018.
2

[42] J. Zhang, J. Jiao, M. Chen, L. Qu, X. Xu, and Q. Yang.
3d hand pose tracking and estimation using stereo matching.
arXiv preprint arXiv:1610.07214, 2016. 7, 8

[43] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang.
The unreasonable effectiveness of deep features as a percep-
tual metric. In CVPR, 2018. 7

[44] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros,
O. Wang, and E. Shechtman. Toward multimodal image-
to-image translation. In NIPS, pages 465–476, 2017. 2, 3,
7

[45] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and G. Lausen.
Improving recommendation lists through topic diversiﬁca-
tion. In WWW, pages 22–32. ACM, 2005. 1

10315

