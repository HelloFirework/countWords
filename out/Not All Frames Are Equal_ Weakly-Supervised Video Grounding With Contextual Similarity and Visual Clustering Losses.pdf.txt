Not All Frames Are Equal: Weakly-Supervised Video Grounding

with Contextual Similarity and Visual Clustering Losses

Jing Shi†

Jia Xu‡

Boqing Gong‡

Chenliang Xu†

†University of Rochester

‡Tencent AI Lab

†{j.shi,chenliang.xu}@rochester.edu

‡

xujianjucs@gmail.com

‡

boqinggo@outlook.com

Abstract

Description: add the potatoes to the pot. Query: potato

We investigate the problem of weakly-supervised video
grounding, where only video-level sentences are provided.
This is a challenging task, and previous Multi-Instance
Learning (MIL) based image grounding methods turn to
fail in the video domain. Recent work attempts to de-
compose the video-level MIL into frame-level MIL by ap-
plying weighted sentence-frame ranking loss over frames,
but it is not robust and does not exploit the rich tempo-
ral information in videos. In this work, we address these
issues by extending frame-level MIL with a false positive
frame-bag constraint and modeling the visual feature con-
sistency in the video.
In speciﬁc, we design a contextual
similarity between semantic and visual features to deal with
sparse objects association across frames. Furthermore, we
leverage temporal coherence by strengthening the cluster-
ing effect of similar features in the visual space. We con-
duct an extensive evaluation on YouCookII and RoboWatch
datasets, and demonstrate our method signiﬁcantly outper-
forms prior state-of-the-art methods.

1. Introduction

Grounding textual signals to visual-spatial regions have
various applications, e.g., robotics [3, 2], human-computer
interaction [27] and image retrieval [11]. While vi-
sual grounding in static images has witnessed great
progress [11, 24, 4, 34, 35], visual grounding in videos
is still challenging—ﬁrst, a video contains many frames,
which induces the temporal visual-language alignment
problem that is unique to video grounding; second, de-
spite rich source of online videos, constructing a large-
scale video dataset with grounding annotation is expensive
and time-consuming. Therefore, in this paper, we aim to
do weakly-supervised video grounding: localize language
queries in video frames without object location annotation.
Kapathy and Fei-Fei [11] introduce a Multiple Instance
Learning (MIL) based grounding method that only requires

Video 
segment

V1

V2

V3

V4

Label

potato

potato

potato

potato

potato

Bag

(a) Video-level MIL 

(b) Frame-level MIL 

Figure 1. The illustration of (a) video-level MIL and (b) frame-
level MIL. V1 to V4 are uniformly sampled from a video seg-
ment. Region proposals in different frames are distinguished by
color. Video-level MIL puts region proposals from all frames into
one bag while frame-level MIL constructs a bag for each frame.
The positive instances are denoted with black shadow. Here is the
dilemma: video-level MIL suffers from monotonically increased
bag size w.r.t. the number of frames, while frame-level MIL may
contain false positive bags such as bags for V3, and V4.

the alignment of images and sentences. It reasonably as-
sumes that each image contains at least one region corre-
sponding to the sentence query. If we deﬁne an image as the
“bag,” regions as instances in the bag and language query as
the label of the bag, then the image satisﬁes the deﬁnition
of the positive bag in MIL: a bag is positive if at least one of
its instances is positive. However, directly extending MIL
based grounding method from image to video easily falls
into a dilemma as shown in Fig. 1. The ﬁrst way is to
regard each video as a bag, which contains all region pro-
posals across frames as the instances. However, the bag size
will drastically increase as video becomes longer. We call
this brute-force video-level MIL. Another option is to con-
struct a bag for every frame, and assign the same video label
to all frame bags, but it is easy to trigger false-positive bags.
This option is named as frame-level MIL. Zhou et al. [40]

432110444

try to jump out of the dilemma by choosing the frame-level
MIL, but weight the loss function for each frame by mea-
suring how “positive” each frame is. Namely, each frame
loss is multiplied with a positive index which is deﬁned by
the similarity between the frame and the query. However,
such method suffers from a problematic penalty term, which
will indistinguishably enlarge the similarity score of both
aligned and unaligned pairs and has very sensitive hyper-
parameters.

To overcome the above limitations, we ﬁrst compare
the performance of vanilla brute-force video-level MIL and
frame-level MIL and decide to follow the latter choice.
Then, to better conquer the downsides of Zhou et al. [40],
we propose a contextual similarity to measure the similarity
score between the frame and the language query based on
two intuitions:

1. If a sentence contains multiple queries,

then each

query should focus on its most relevant frames.

2. If an object appears sparsely across frames, the no-
object frames should be insigniﬁcant compared with
the frames where the object appears.

In the case of MIL, the contextual similarity can be viewed
as an augmented similarity by considering the possibility
of a frame to be the true positive bag of a query. Moreover,
such possibility for one frame is calculated by looking at the
other frames in the same video, which makes it more reli-
able. By replacing ordinary frame-sentence similarity with
our contextual similarity, one can alleviate the difﬁculty of
false positive bags in frame-level MIL.

Furthermore, the aforementioned methods fail to con-
sider the visual consistency in the video, which is a unique
property to video grounding; hence, we propose visual clus-
tering to leverage the temporal information better. Visual
clustering is inspired by the idea:

3. If two regions have high similarity to a common query,

then they should also be similar to each other.

In this case, the visual similarity is not restricted to the
adjacent frames, but can also work with sparsely sampled
frames in a video segment.

We conduct extensive experiments on YouCookII
dataset [41], which is the largest unconstrained instructional
video dataset available for visual grounding. Experimental
results demonstrate the effectiveness of our proposed tech-
niques compared to other state-of-the-art methods. Further-
more, we show that our techniques can also lead to im-
proved performance on RoboWatch dataset [26].

The rest of this paper is organized as follows. We review
related work in visual grounding, weakly-supervised object
localization and feature embedding in Sec. 2. We present

formal description of contextual similarity and visual clus-
tering in Sec. 3. Experimental settings and evaluation re-
sults are presented in Sec. 4. Finally, the paper is concluded
in Sec. 5.

2. Related Work

Visual grounding.
Supervised image grounding has
been successfully explored in [21, 20, 37]; however, the
task requires expensive labels for box location. Recently,
weakly-supervised image grounding draws much attention
from the community. Most weakly-supervised grounding
methods can be classiﬁed as either proposal-based [11, 24,
4] or proposal-free [34, 35]. Given region proposals, Karpa-
thy and Fei-Fei [11] formulated it as a ranking problem to
rank the proposals according to visual-semantic similarity
scores in a MIL fashion. Rohrbach et al. [24] encoded a
phrase as its most similar region to reconstruct the region
back to the phrase. Chen et al. [4] transferred the knowl-
edge from the off-the-shelf object detector to help phrase
grounding. For proposal-free methods, the region location
is often obtained from phrase-salient map via subwindow
search. Xiao et al. [34] generated the salient map by re-
garding language structure as additional supervision for the
location relationship among objects. Raymond et al. [35]
conducted hypothesis tests over the existence of image con-
cept given words in a statistic view.

Weakly-supervised grounding has also been attempted
in videos [36, 10, 40]. Yu and Siskind [36] grounded sen-
tence to object in constraint lab-recorded videos. Huang et
al. [10] addressed language reference and grounding to-
gether to enhance the grounding performance with the
inspiration of graphical structure modeling [12, 38, 30]
Zhou et al. [40] extended [11] to the video domain with
frame-wise weighting and achieved the best performance so
far on video visual grounding. In this work, we follow the
proposal-based MIL methods [11, 40] due to the simplicity
and effectiveness of the MIL learning framework.
Weakly-supervised object localization. Methods, e.g.,
[8, 6, 7, 18, 29], are related to visual grounding, but they
typically localize an predeﬁned object class or a video tag,
while, in visual grounding, the target can be any words or
phrases that are loosely deﬁned. Most weakly-supervised
object localization problem can be formulated as a MIL
problem as well. The image that contains the label is re-
garded as positive instance and otherwise not. Among the
methods, [15, 22] have studied the weakly-supervised video
localization. Kwak et al. [15] combined object discovery
and object tracking while Prest et al. [22] extracted candi-
date spatio-temporal tubes for a better localization. Com-
paring to these methods, we propose an easier way to em-
ploy temporal information on the feature level that does not
require tracking or forming tubes, which are often compu-
tationally expensive.

10445

Feature embedding.
In metric learning, contrastive loss
[9, 31] and triplet loss [25] are widely used to enhance the
feature space with a clustering property. When it comes
to the cross-model embedding, they are still feasible with
the elements forming pairs and triplets coming from dif-
ferent modalities (e.g., language and image [11]). How-
ever, Collell and Moens [5] showed that the projection of
the source modality does not resemble the target modal-
ity, in the sense of neighborhood topology, which drives
researchers to develop more discriminative mappings. One
way is to reduce the intra-class feature variations using cen-
ter loss [33] which has been used in tasks such as face ver-
iﬁcation [17], and object retrieval [39]. However, center
loss typically needs supervision and cannot ﬁt into our task.
Other methods such as the structure-preserving loss [32]
would introduce extra hyper-parameters due to the margin
and the neighborhood. Different from the above works, we
employ temporal visual consistency as an additional cue to
reduce intra-class feature variations.

3. Methodology

3.1. Problem Formulation

Given a video segment and its sentence description, we
would like to locate each query in the sentence to each
frame of the video, where the query can be either a word
or a phrase. Formally, we denote a video segment as a set
of T frames V = {Vt}T
t=1, and each frame Vt contains a
set of N region proposals {vt
n=1, where the superscript t
indexes the frames and the subscript n indexes the propos-
als on the current frame. We denote a sentence as a set of
K queries Q = {qk}K
k=1, and each qk corresponds to one
or more words in the sentence. Here, the visual feature and
query feature are all encoded into a common d-dimensional
space such that vt

n}N

n, qk ∈ Rd.

Following [11] and [40], we deﬁne the similarity be-

tween the query qk and the region vt

n as:

at,n
k = qT

k vt

n ,

(1)

where T denotes transpose. We deﬁne the negative samples
Q′ and V ′ as queries and region proposals that are neither
paired with Q nor V. Next, we introduce two approaches
for visual grounding: the brute-force video-level MIL and
the frame-level MIL. Our ﬁnal model builds upon the latter.
Brute-force video-level MIL.
Brute-force video-level
MIL regards a video as a bag and all regions across frames
in the video as the instances in the bag, and then be trained
with ranking loss on the bag level. Hence the similarity
score between the video segment V and the description Q is
written as:

S(V, Q) =

1
K

K

X

k=1

max
t,n

at,n
k

,

(2)

and the ranking loss with margin ∆ is deﬁned as:

Lrank = max(0, S(V, Q′) − S(V, Q) + ∆)+
max(0, S(V ′, Q) − S(V, Q) + ∆) .

(3)

Intuitively, Eq. (2) transforms the region-query similarity
to video-sentence similarity, where max is the key opera-
tion in MIL to select the most positive instance from the
positive bag, which can be paraphrased as to select the re-
gion from the video with the highest similarity to the query.
Then the loss is constructed as a pair-wise ranking loss to
embed the aligned video-sentence pairs with higher simi-
larity than the unaligned pairs. However, such method has
a fatal drawback—the bag size will monotonically increase
as the number of frames in video increases. Nonetheless,
we still compare it with our model in Sec. 4.2.
Frame-level MIL.
The frame-level MIL is an alternative
approach to the brute-force video-level MIL. Frame-level
MIL regards a frame as a bag and all regions in the frame
as the instances in the bag, and then be trained with ranking
loss on the frame level. Here, we deﬁne the similarity score
between sentence and frame as:

S(Vt, Q) =

1
K

K

X

k=1

max

n

at,n
k ,

and the ranking loss on each frame with margin ∆ is:

Lt

rank = max(0, S(Vt, Q′) − S(Vt, Q) + ∆)+
t , Q) − S(Vt, Q) + ∆) .

max(0, S(V ′

(4)

(5)

Therefore, the ﬁnal ranking loss averages over all frames:

Lrank =

1
T

T

X

t=1

Lt

rank .

(6)

Intuitively, frame-level MIL allows the queries to ﬁnd their
most similar regions in each frame to represent the similar-
ity score. While this method has ﬁxed bag size, it assumes
that all frames in a video segment are positive bags. This
assumption breaks when the queried object sparsely ap-
pears across frames and would trigger false positive bags, as
shown in Fig. 1. We follow this framework because it makes
use of more positive instances in a video segment; this can
potentially increase the training samples and is more ﬂexi-
ble. Next, we show how to alleviate these drawbacks of a
vanilla frame-level MIL.

3.2. Contextual Similarity

We alleviate the false positive frame bag problem by cre-
ating a contextual similarity between frame and query; its
high-level illustration is shown in Fig. 2. In the perspective
of MIL, the contextual similarity can be viewed as a bet-
ter similarity augmented by considering the possibility of a

10446

Description

Place  lettuce and 
tomatoes on bread

query

bread

region-query similarity

frame-query similarity

S

max

0-1 normalization

!"

0

1

0.3

frame-query contextual similarity

̅"

Figure 2. Diagram for evaluating the frame-query contextual simi-
larity score. Region-query similarity scores are calculated between
each region and the query “bread” with inner product similarity,
which is best viewed in color. A 0-1 normalization along frames
is applied to obtain its contextual gain.

frame to be the true positive bag of a query. Furthermore,
such possibility for one frame is calculated by looking at
the other frames in the same video making it more reliable.
Concretely, we start by deﬁning the original similarity be-
tween the frame Vt and the query qk as:

S(Vt, qk) = max

n

at,n
k

,

(7)

then the contextual similarity between frame and query is
deﬁned as:

¯S(Vt, qk) = S(Vt, qk) ˜S(Vt, qk) ,

(8)

where ˜S(Vt, qk) is expanded as:

˜S(Vt, qk) =

S(Vt, qk) − min

t

S(Vt, qk)

max

t

S(Vt, qk) − min

t

S(Vt, qk)

.

(9)

In fact, ˜S(Vt, qk) is a 0-1 normalization of the original
frame-query similarity over all frames in a video segment,
but plays an important role as a weighting score over frames
so as to guide the qk to match its most correlated frames.
Multiplying such weighting score to the original frame-
query similarity yields the contextual score. Then, by av-
eraging the contextual frame-query scores over all queries
in a sentence, we obtain the sentence-frame score as:

S(Vt, Q) =

1
K

K

X

k=1

¯S(Vt, qk) .

(10)

Next, we put Eq. (10) into Eq. (5) to get Lt
video-level ranking loss is the same as Eq. (6).

rank, and the

The reason to design Eq. (9) is that the ˜S(Vt, qk) guar-
antees the validity of the key frame with the highest frame-
query score in the video segment, because it corresponds

to ˜S(Vt, qk) = 1. And, it can directly abandon the triv-
ial frame which has the lowest frame-query score since its
˜S(Vt, qk) = 0. Hence, we decay the importance of each
frames by their relative importance to the key frame and the
trivial frame. Furthermore, our formulation will not intro-
duce additional hyper-parameters and is robust in training.
Also, we ﬁnd that letting gradient propagate to Eq. (9) leads
to better performance.

3.3. Visual Clustering

Visual grounding is intrinsically a cross-model mapping
problem. We would like to map the visual and textual fea-
tures to a common space. In this sense, regions grounded by
the same query should be embedded as a neighbor structure
in feature space and will form a cluster. The visual cluster-
ing method assumes that the queried objects show similar
appearance across video frames, and their visual features
are within the same cluster. If we have region class label, it
is natural to use center loss [33], which directly drives ob-
jects in the same class to be close to the class center. How-
ever, in weakly-supervised setting, the class label for each
object is unknown. Instead, we ﬁrst let query qk select its
most similar region proposal in frame t, and we denote the
selected region as:

ˆvt,k = arg max
t ,...,vN

t ∈{v1

vn

t }

k vn
qT

t

.

(11)

Then we want to further cluster all the visual features ˆvt,k
in different frame t together because they all belong to the
common query qk. Hence, we minimize the negative cosine
similarity of any two region features belonging to the same
query in a video segment, which is deﬁned as:

Lvis = − X

X

cos(ˆvt,k, ˆvt′,k) .

(12)

k

t<t′

The cluster hypothesis tries to make use of the temporal
connectivity so as to learn a more discriminative visual em-
bedding.

Nonetheless, Eq. (12) has an implicit assumption that the
queried object is required to appear in each frame of a video
segment. According to the validation set of YouCookII
dataset [40], the queried objects show up in 51.32% of the
total frames, and in our experiment, such assumption does
not hurt the performance. In order to better relax such as-
sumption, we weight the visual similarity by the similar-
ity between word feature and visual feature. Therefore, the
contextual visual similarity is formulated as:

vis = − X
Lctx

X

cos(ˆvt,k, ˆvt′,k) ˜S(Vt, qk) ˜S(Vt′ , qk) ,

k

t<t′

where ˜S(Vt, qk) is deﬁned in Eq. (9).

(13)

10447

Finally, the full loss function combining the contextual

similarity and visual clustering is constructed as:

L =

T

X

t=1

Lt

rank + λLctx

vis ,

(14)

where λ is the weighting parameter of the two parts of
the loss function. The visual clustering can contribute to
a more discriminative visual feature by reducing the intra-
class variance, which achieves the same effect as the center
loss. Moreover, we also have tried the loss that reduces the
similarity of visual features of different classes, but the per-
formance does not improve; hence we only force the sim-
ilarity between similar visual features across frames in a
same video segment.

frame Vt is a false positive bag. By multiplying this term to
frame-wise ranking loss, the model down-weights the false
negative bags and thus yields better result. Moreover, in
order to avoid the trivial solution S(Vt, Q) = 0, the second
term in Eq. (15) is the penalty term that pulls S(Vt, Q) to be
greater than 0. While Eq. (15) tries to construct better posi-
tive sentence-frame pairs by applying a strong (weak) frame
ranking loss if the frame has higher (lower) similarity to the
sentence, such method has two obvious disadvantages: (1)
the hyper-parameter λ is very sensitive to the model ground-
ing accuracy; (2) the penalty term tries to penalize all the
similarity of sentence-frame pairs even if the frame does
not contain the queried object, which is not reasonable.

4. Experiments

3.4. Method Details

4.1. Datasets and Evaluation Metric

Learning & Inference. We employ two training strate-
gies: Finite-Class Training (FCT) and Inﬁnite-Class Train-
ing (ICT). In FCT, only words from a small size of vocabu-
lary set are chosen to construct the ranking loss; whereas in
ICT, any noun contributes to the loss. FCT has the advan-
tage of higher grounding accuracy on the vocabulary but
sacriﬁces the generalizability to other datasets. On the con-
trary, ICT generalizes easily by compromising accuracy in
ﬁnite vocabulary set. We conduct both strategies in Sec. 4.
To get visual embedding v, we
Visual embedding.
ﬁrst extract the 4096-dimensional vcnn from the last fully-
connected (FC) layer of a convolutional network, then add
an additional FC layer with parameter Wv and hyperbolic
tangent function to encode it to a 512-dimensional common
space. In other words, v = tanh(Wvvcnn).
Textual embedding.
Each word is ﬁrst embeded with
200-dimensional GloVe [19] feature sglv. For FCT, each
word s = tanh(Wssglv), where Ww is a linear layer.
While for ICT, the ith word in a sentence is formulated as
si = tanh(Ws[BiLST M (si)]i, where BiLST M (·) repre-
sents a bi-directional LSTM. If the query is a phrase, simply
average the word features in the phrase, which has the same
dimension as the visual feature.
Compare to other methods. We are not the ﬁrst one to
explore video grounding with MIL. Zhou et al. [40] con-
structed the frame-sentence similarity ranking loss with the
weighted frame importance:

L =

1
T

T

X

t=1

[λS(Vt, Q)Lt

rank + (1 − λ)(− log(2S(Vt, Q))],

(15)
which does weighted-sum over each frame loss by the
frame-sentence similarity S(Vt, Q). Notice that it does not
simply calculate Lrank like Eq. (6) because it tries to reduce
the negative effect of false positive bags in frame-level MIL.
The lower S(Vt, Q) indicates the higher possibility that the

We train our model in a weakly-supervised manner on
YouCookII dataset [41] and conduct generalizability analy-
sis on RoboWatch dataset [26].
YouCookII. YouCookII [41] is a large-scale dataset in-
cluding 2000 YouTube cooking videos from 89 recipes.
Each video recipe consists of 3 to 15 steps, where each
step is annotated with a sentence description and temporal
boundaries of the corresponding video segment. For evalu-
ation and testing, [10] and [40] contribute to the bounding
box annotation independently. [10] focuses on the union of
grounding and co-reference, hence it annotates roughly 5
frames per object in a video segment with the reference of
previous step with phrase. [40] aims at general video ob-
ject grounding and thus annotates the boxes at 1 fps with 67
kinds of objects in vocabulary. We conduct experiments on
YouCookII dataset following [40], which is more similar to
our work.
RoboWatch.
The test set of RoboWatch [26] contains
225 YouTube videos mainly about cooking. Similar to
YouCookII, these videos are annotated with temporal inter-
vals and description for each step. [10] extends the bound-
ing box annotation for a part of those videos, and the query
can be either word or phrase. One important difference of
[10] compared with this paper is that [10] is a reference-
aware grounding method which can ground a query to its
unaligned video segment referred by such query, while our
paper focuses more on the MIL strategy within a single
video segment. Hence we only evaluate our model on the
aligned video segment and query pairs in RoboWatch.
Evaluation metric. We evaluate the models using both
Box accuracy [40] and Query accuracy [10]. For each
query, we propose its top-1 grounded box. The box accu-
racy is deﬁned as the ratio of correctly grounded boxes to all
grounded boxes, where the correctly grounded boxes have
more than 50% Intersection-over-Union (IoU) with ground-
truth boxes. Query accuracy is deﬁned as the ratio of cor-

10448

Method

Box accuracy (%)

macro

micro

Query accuracy (%)

macro

micro

val

test

val

test

val

test

val

test

Upper Bound

62.42

62.41

Compared method

GroundR [24]
DVSAfrm∗[11]
DVSAvid∗[11]
Zhou et al. [40]
Zhou et al. *[40]

Our method

VisClus
CtxSim

VisClus+CtxSim

19.63
36.90
36.67
30.31
35.69

37.80
38.12
39.54

19.94
37.55
36.30
31.73
35.08

38.04
38.78
40.71

-

-

-

-

-

-

-

-

-

-

-

-

44.26
43.62

-

44.16
42.87

-

38.48
38.20

-

39.31
37.98

-

46.27
45.60

-

46.14
44.79

-

43.04

42.42

37.26

36.69

44.99

44.34

45.35
46.10
46.41

45.53
45.74
46.33

39.44
39.78
41.29

39.72
40.45
42.45

47.41
48.20
48.52

47.58
47.80
48.41

Table 1. Weakly-supervised grounding results on YouCookII in FCT. The compared methods implemented by us are indicated with *.

rectly grounded queries to all queries, and a grounded query
is correct if it is matched with correctly grounded box. Also,
we denote macro-accuracy as the average of each class ac-
curacy and denote micro-accuracy as the global accuracy
without distinction of classes.
Implementation details.
The description sentence is
parsed by Stanford CoreNLP parser [16] into nouns. For
each video segment, 5 frames are uniformly sampled and
then fed into RPN [23] with VGG-Net [28] backbone pre-
trained on [14] to get top-20 region proposals. The num-
ber of sampled frames and region proposals are set follow-
ing [10] and [41]. We use Adam [13] with learning rate
0.01 for optimization, and dropout rate 0.1 for regulariza-
tion. The hyper-parameters are searched by Bayesian opti-
mization [1] as λ = 4.13 and ∆ = 10. At training phase,
each batch contains 8 aligned video-sentence pairs and can
form totally 64 pairs for ranking loss.
Grounding approaches.
models and variants of our model for visual grounding:
- Deep Visual-Semantic Alignment (DVSA) [11]. DVSA is
the grounding by ranking method upon which we build our
models. For a fair comparison, the image-based DVSA has
been adapted to videos in both frame-level MIL (DVSAfrm)
as in Eq. (3) and video-level MIL (DVSAvid) as in Eq. (6).
- Zhou et al. [40]. This approach weights the frame loss by
using Eq. (15) and is test on a limited word vocabulary. we
re-implemented this method to draw a fair comparison.
- RA-MIL [10]. We compare this method for testing the gen-
eralizability of our model.
- CtxSim. Our model variant that uses only the contextual
similarity loss deﬁned in Sec. 3.2.
- VisClus. Our model variant that uses only the visual clus-
tering loss in Sec. 3.3.
- Upper Bound. This is calculated by regarding all 20 pro-
posed boxes as the grounded boxes of each query, rather
than the top-1 box.

We compare the following

4.2. Main Results

We conduct FCT on YouCookII dataset for fair compar-
ison with Zhou et al. [40], because the ground-truth object
belongs to a ﬁnite set of words. And, for comparison with
RA-MIL on generalization test on RoboWatch, ICT is em-
ployed on YouCookII, due to the ground-truth query is a
word or a phrase without constraints. Quantitative results
on YouCookII in FCT mode are shown in Table 1. We re-
port macro-accuracy and micro-accuracy on both box and
query accuracy.

Frame-level MIL and video-level MIL. We report both
video-level and frame-level MIL extensions of DVSA and
show that the frame-level MIL outperforms video-level
MIL. To further analyze the reason, we go through the val-
idation set of YouCookII and ﬁnd that the queried objects
show up in 51.32% of the total frames, suggesting that
half of the frame-level MIL bags are false positive. On
the other side, frame-level MIL has intrinsically more bags
than video-level MIL, which is equivalent to say frame-level
MIL have more training data. Experimental results show
that even half of the positive bags are false positive, frame-
level MIL still outweighs its video counterpart due to more
training samples.

Contextual similarity and visual clustering. Both con-
textual similarity and visual clustering outperform DVSA
and Zhou et al.’s method. Experimental results show
that contextual similarity has larger improvement compared
with visual clustering. We suspect that it is because visual
clustering relies more heavily on the occurrence of objects
in frames. Our full model outperforms the individual mod-
els in all the metrics, and higher Box macro-accuracy than
DVSAfrm, i.e., 3.16%, which demonstrates that visual clus-
tering and contextual similarity are mutually beneﬁcial. No-
tice that Zhou et al. [40]’s implementation has a lower ac-
curacy than ours, which can be partially attributed to our

10449

DVSAfrm

VisClus

CtxSim

VisClus+CtxSim

(a) Put a pan on medium to high heat

(b) Heat the butter and some sea salt flakes in the pan

Figure 3. Visualization of grounding results from frame-level DVSA and our proposed methods on YouCookII. Bold words are queries.
Red, green and grey boxes represent model prediction, ground-truth and region proposals, respectively.

query:

bread

16.86

-4.85

38.86

-43.18

0.73

12.30

0.47

-2.28

1

38.86

0

0

"

!"

̅"

Figure 4. Example to show how the contextual similarity works
with numerical demos. The grounded boxes are localized by our
full model and box color is deﬁned the same as Fig. 2.

higher upper bound then theirs. Our RPN is pretrained on
Visual Gnome [14], which has richer visual semantic and
enables RPN to generate better proposals. In addition, our
implementation of Zhou et al. [40] is lower than DVSAfrm
baseline, because of the sensitivity of the hyper-parameter
in its loss function rendering the difﬁculty for parameter
tuning.

Analysis.
We analyze the reason why our proposed
methods work with the help of qualitative results. Fig. 3
visualizes two sequences of video frames from YouCookII
Dataset. As expected, in both sequences, our proposed
methods ground better than DVSA basline and our full
model looks better than the individual ones. We observe
that visual clustering performs better when localizing tem-
porally consistent object. For example, in video segment
(a) in Fig. 3, visual clustering and full model capture all
the pans across frames while contextual similarity missed
half of them. This observation indicates that visual cluster-
ing will push the model learn a more discriminative visual
feature embedding. Also, the small objects such as butter in

Wrap sauce egges tofu and green onions and place in jar.

Figure 5. Failure cases. The grounded box are localized by our full
model and box color is deﬁned the same as Fig. 3.

video segment (b), which cannot be grounded by DVSA, are
correctly matched in our proposed method. This is another
evidence that our model has higher recognition ability.
Contextual similarity qualitative analysis.
Contextual
loss attaches the normalized weights to different frames,
which is experimentally proved effective. For instance, in
Fig. 4, the query “bread” is not a positive label for the last
frame, which will mislead the model by feeding the model
with wrong samples. Fortunately, with the help of con-
textual loss, the normalization ˜S assigns the false positive
frame bag with lower importance, with 0 in extreme.
Failure cases.
Figure 5 presents the common failure
cases: the object occlusion, object out of scene, and small
size object. For example, the egg is out of scene in the left
ﬁgure and occluded by hand in the right ﬁgure, which are
quite often in cooking, e.g., the camera is set statically and
human-object interaction can easily deviate from the screen.
Also, ingredients or foods can be easily occluded by hand
due to manual operation over them. In addition, objects be-
come smaller while camera zooming out, which also adds
difﬁculty to the grounding task. Overall, objects falling into

10450

ሚ𝑆ҧ𝑆

Method

YouCookII
val (%)

test (%)

RoboWatch

test (%)

egg

Upper Bound

62.42

62.41

-

Compared method

DVSAfrm [11]
RA-MIL [10]
Our method

VisClus
CtxSim

VisClus+CtxSim

35.87

37.33

-

-

36.44
37.99
37.43

37.80
37.67
38.49

28.25
19.80

28.68
31.08
31.68

Table 2. Generalizability to unseen video classes (RoboWatch)
in ICT. The score for YouCookII is the box macro-accuracy, for
RoboWatch is the query micro-accuracy.

the three typical failure cases are usually not covered by re-
gion proposals, which is also true for the egg in Fig. 5.

4.3. Generalizability Test

To further test the generalization ability, we do ICT on
YouCookII and test it on videos in RoboWatch including
different recipes and other miscellaneous videos such as
“How to remove gum from clothes,” and “How to tie a tie.”
And, there is no recipe or video overlap with YouCookII.

The generalization performance is shown in Table 2 with
metric of query micro-accuracy. For consistency consid-
eration, the number reported on YouCookII is still evalu-
ated in [40]’s box annotation, even if [10] also annotated
the box in YouCookII. We observe that visual clustering
and contextual similarity both show good generalizability
and our full model outperforms all the other methods on the
testing set of both datasets, with 3.43% higher than frame-
level MIL baseline in RoboWatch, proving our method has
a good generalization ability. Contextual similarity has a
higher score on the validation set of YouCookII, but lower
on testing set, suggesting it overﬁts to the validation set.
Though we are not in an absolute fair comparison with [10]
due to the reference-awareness, we list [10]’s result as a ref-
erence. Different from the experiment set up in [10], we
have ﬁltered out those ground-truth boxes corresponding to
language queries in unpaired descriptions, which means the
number of testing samples are smaller than [10]. However,
we still can observe an improvement of our method by test-
ing accuracy on RoboWatch.

The qualitative results of RoboWatch are shown in Fig. 6.
We observe that the model has comparative grounding abil-
ity for the queries known by YouCookII, but for some un-
seen query such as “hanger,” the model can still correctly
ground it. Also, we ﬁnd the model tend to localize hand,
so we suspect that “hanger” has similar textual embedding
with “hand” thus the model transfers the knowledge of hand
toward hanger.
FCT and ICT. FCT and ICT are adopted respectively

mongo

Cream,
pan

hanger

Figure 6. Visualization of grounding results with our full model
on RoboWatch. The green queries have been seen in YouCookII
while the red one has not. Box colors are deﬁned in Fig. 3.

.

over all methods in Tables 1 and 2. Comparing their per-
formance on YouCookII uncovers that the ICT is inferior to
FCT according to accuracy but stronger then FCT on gener-
alizability. The accuracy gain of FCT can be explained by
a reduced complexity in feature embedding space for FCT
because it only need to push the visual feature embedded to
ﬁnite word feature cluster centers.

5. Conclusion

In this paper, we propose two techniques to improve the
video grounding accuracy. Contextual similarity remedies
the overly-strong assumption that each frame in a video seg-
ment needs to contain the grounded object. Visual cluster-
ing better exploits the temporal consistency in video and
embeds a more discriminative visual feature. Experimental
results on two prevalent datasets demonstrate the effective-
ness and generalizability of our methods.

Limitations.
As pointed out in the failure case, our
model is limited by the quality of region proposals, which
constrains the model’s upper bound. The model’s ability
is also conﬁned by the quality of the pretrained visual and
textual feature encoders. With shallow learnable embedding
layers, our model mainly relies on pretrained deep feature
extractor.

Future work.
This work tries to improve frame-wise
MIL and incorporate temporal visual information extractor.
However,
the visual consistency is now only employed
at feature level. We plan to add the visual consistency
constraint at spatial level as future work.

Acknowledgement. This work was supported in part by
NSF IIS 1813709, IIS 1741472, and the Tencent Rhino-Bird
gift. This article solely reﬂects the opinions and conclusions
of its authors and not the funding agents.

10451

References

[1] Bayesian optimization. https://github.com/fmfn/

BayesianOptimization.

[2] M. Al-Omari, P. Duckworth, D. C. Hogg, and A. G. Cohn.
Natural language acquisition and grounding for embodied
robotic systems. In AAAI, 2017.

[3] L. Anne Hendricks, R. Hu, T. Darrell, and Z. Akata. Ground-

ing visual explanations. In ECCV, 2018.

[4] K. Chen, J. Gao, and R. Nevatia. Knowledge aided consis-
In CVPR,

tency for weakly supervised phrase grounding.
2018.

[5] G. Collell and M.-F. Moens. Do neural network cross-modal

mappings really bridge modalities? In ACL, 2018.

[6] T. Deselaers, B. Alexe, and V. Ferrari. Weakly supervised
IJCV,

localization and learning with generic knowledge.
100(3), 2012.

[7] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning ev-
erything about anything: Webly-supervised visual concept
learning. In CVPR, 2014.

[8] R. Gokberk Cinbis, J. Verbeek, and C. Schmid. Multi-fold
In

mil training for weakly supervised object localization.
CVPR, 2014.

[9] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-

tion by learning an invariant mapping. In CVPR, 2006.

[10] D.-A. Huang, S. Buch, L. Dery, A. Garg, L. Fei-Fei, and J. C.
Niebles. Finding it: Weakly-supervised reference-aware vi-
sual grounding in instructional videos. In CVPR, 2018.

[11] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-

ments for generating image descriptions. In CVPR, 2015.

[12] C. Kiddon, G. T. Ponnuraj, L. Zettlemoyer, and Y. Choi.
Mise en place: Unsupervised interpretation of instructional
recipes. In EMNLP, 2015.

[13] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

[14] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al. Vi-
sual genome: Connecting language and vision using crowd-
sourced dense image annotations. IJCV, 123(1), 2017.

[15] S. Kwak, M. Cho, I. Laptev, J. Ponce, and C. Schmid. Unsu-
pervised object discovery and tracking in video collections.
In ICCV, 2015.

[16] C. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. Bethard,
and D. McClosky. The stanford corenlp natural language
processing toolkit. In ACL (system demonstrations), 2014.

[17] Z. Ming, J. Chazalon, M. Muzzamil Luqman, M. Visani, and
J.-C. Burie. Simple triplet loss based on intra/inter-class met-
ric learning for face veriﬁcation. In ICCV, 2017.

[18] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object lo-
calization for free?-weakly-supervised learning with convo-
lutional neural networks. In CVPR, 2015.

[19] J. Pennington, R. Socher, and C. Manning. Glove: Global

vectors for word representation. In EMNLP, 2014.

[20] B. A. Plummer, P. Kordas, M. H. Kiapour, S. Zheng, R. Pira-
muthu, and S. Lazebnik. Conditional image-text embedding
networks. In ECCV, 2018.

[21] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. In ICCV, 2015.

[22] A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Fer-
rari. Learning object class detectors from weakly annotated
video. In CVPR, 2012.

[23] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015.

[24] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and
B. Schiele. Grounding of textual phrases in images by re-
construction. In ECCV, 2016.

[25] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In CVPR,
2015.

[26] O. Sener, A. R. Zamir, S. Savarese, and A. Saxena. Unsuper-
vised semantic parsing of video collections. In ICCV, 2015.
[27] M. Shridhar and D. Hsu. Interactive visual grounding of re-
ferring expressions for human-robot interaction. In Proceed-
ings of Robotics: Science and Systems, 2018.

[28] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
[29] H. O. Song, R. Girshick, S. Jegelka, J. Mairal, Z. Harchaoui,
and T. Darrell. On learning to localize objects with minimal
supervision. In ICML, 2014.

[30] L. Song, Y. Zhang, Z. Wang, and D. Gildea. A graph-to-

sequence model for amr-to-text generation. In ACL, 2018.

[31] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
2014.

[32] L. Wang, Y. Li, and S. Lazebnik. Learning deep structure-

preserving image-text embeddings. In CVPR, 2016.

[33] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discrimina-
tive feature learning approach for deep face recognition. In
European Conference on Computer Vision, pages 499–515.
Springer, 2016.

[34] F. Xiao, L. Sigal, and Y. Jae Lee. Weakly-supervised visual
In CVPR,

grounding of phrases with linguistic structures.
2017.

[35] R. A. Yeh, M. N. Do, and A. G. Schwing. Unsupervised tex-
tual grounding: Linking words to image concepts. In CVPR,
2018.

[36] H. Yu and J. M. Siskind. Sentence directed video object

codiscovery. IJCV, 124, 2017.

[37] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L.
Berg. Mattnet: Modular attention network for referring ex-
pression comprehension. In CVPR, 2018.

[38] Y. Zhang, Q. Liu, and L. Song. Sentence-state lstm for text

representation. In ACL, 2018.

[39] X. Zheng, R. Ji, X. Sun, Y. Wu, F. Huang, and Y. Yang. Cen-
tralized ranking loss with weakly supervised localization for
ﬁne-grained object retrieval. In IJCAI, 2018.

[40] L. Zhou, N. Louis, and J. J. Corso. Weakly-supervised video
object grounding from text by loss weighting and object in-
teraction. In BMVC, 2018.

[41] L. Zhou, C. Xu, and J. J. Corso. Towards automatic learning
of procedures from web instructional videos. In AAAI, 2018.

10452

