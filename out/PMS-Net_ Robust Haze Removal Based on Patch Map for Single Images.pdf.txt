 

 

 

 

 

PMS-Net: Robust Haze Removal Based on Patch Map for Single Images 

Wei-Ting Chen 1 

Jian-Jiun Ding 2 

Sy-Yen Kuo 2 

1 Graduate Institute of Electronics Engineering, National Taiwan University, Taipei 106, Taiwan 

2Department of Electrical Engineering, National Taiwan University, Taipei 106, Taiwan 

{r05943089, jjding, sykuo}@ntu.edu.tw 

Abstract 

 

feature  called 

the  patch  map 

In this paper, a novel haze removal algorithm based on a 
new 
is  proposed. 
Conventional  patch-based  haze  removal  algorithms  (e.g. 
the dark channel prior) usually performs dehazing with a 
fixed patch size. However, it may lead to several problems 
such as oversaturation and color distortion. Therefore, in 
this paper, we designed an adaptive and automatic patch 
size  selection  model  called  the  Patch  Map  Selection 
Network (PMS-Net) to select the patch size corresponding 
to  each  pixel.  This  network  is  designed  based  on  the 
convolutional neural network (CNN), which can generate 
the patch map from the input image. Experimental results 
on both synthesized and real-world hazy images show that, 
with  the  combination  of  the  proposed  PMS-Net,  the 
performance  in  haze  removal  is  much  better  than  that  of 
state-of-the-art  algorithms  and  we  can  address  the 
problems caused by the fixed patch size. 
 

1. Introduction 

Haze is an atmospheric phenomenon which compose of 
smoke, dust and other floating particles. They may degrade 
the  visibility  and  lead  to  the  poor  performance  of  image 
processing  such  as  object  detection  and  classification. 
Narasimhan et al. [1] modeled the hazy scene as: 

 
I x

 
( )
J x t x







A

                (1) 
where J(x) is the haze-free image, I(x) is the observed hazy 
image, A is the global atmospheric light which indicates the 
luminance of the light source from infinite distance away, 
and t(x) is the transmission map which can be denoted as 
t(x)=e-βd(x), where β is the scattering coefficient and d(x) is 
the path length. We can rewrite (1) as: 
                           

 .                            (2) 

 
J x



A

A

 
t x


1



( )

I x
( )
t x

Then, one can notice that, to get better recovered results in 
haze removal, the critical parameters are t(x) and A. 

 
 

 

11681

                                    (a)                                  (b) 

 

      (c) PSNR:15.21              (d) PSNR:14.39            (e) PSNR = 21.7   
           SSIM: 0.769                    SSIM: 0.592                 SSIM = 0.9       

 

 

                                   (f)                          (g) PSNR:28.74  
                                                                      SSIM: 0.947          
Figure 1: Dehazing results of the proposed patch map compare to 
the method with fixed patch size. (a) Input image; (b) the ground 
truth; (c)-(e) recovered by the fixed patch sizes of 1, 120, 15; (f) 
the  patch  map  determined  by  the  proposed  algorithm;  (g) 
recovered image by the patch map.  
 

There have been many algorithms [2-22] addressing the 
image dehazing problem. Tarel et al. [2] applied the special 
bilateral filter to investigate the atmospheric veil. He et al. 
[4]  proposed  the  dark  channel  prior  (DCP)  based  on  the 
statistical results of natural haze-free images to compute the 
transmission  map.  In  [14],  Tang  et  al.  applied  machine 
learning  techniques  to  predict  the  transmission  map.  In 
recent  years,  several  deep-learning  based  dehazing 
methods  [15-22],  including  the  DehazeNet  [15],  the 
multi-scale  CNN  (MSCNN)  [16],  the  residue  learning 
technique [18], the quad-tree CNN [19], and the densely 
connected pyramid dehazing network [21], were proposed 
for haze removal. For haze removal in the night time case, 
various  algorithms  [23-27]  were  proposed  to  solve  this 
problem. 

Although  there  have  been  many  dehazing  algorithms, 
there  are  still  several  limitations  that  constrain  the 
performance of recovered results. Traditional algorithms  

 

 

 

 

11682

(a) 

(b) 

Figure  2:  Comparing  the  performance  with  state-of-the-art 
algorithms by using the patch map based DCP. 
 
are usually based on the prior related to human perception. 
However, these priors may be invalid in some scenarios. 
For  example,  the  DCP  may  have  limited  performance  in 
sky  regions  or  white  scenarios  because  the  transmission 
map cannot be well predicted in these cases. On the other 
hand,  for  the  learning-based  methods,  without  using 
human- selected features, the transmission map may not be 
predicted accurately in general since they may bound the 
mapping space with less haze-relevant features. 
 
In this paper, inspired by the DCP, we proposed a novel 
algorithm based on the new feature called the patch map, 
which can select the patch size adaptively for each pixel. In 
the traditional DCP algorithm, a fixed patch size is applied 
to estimate the atmospheric light and the transmission map. 
However, from Fig. 1, one can see that when we perform 
the DCP with the fixed-size patch (in this figure, we use 1, 
15, and 120) to recover the hazy scene, the performance is 
limited. However, when we apply the patch map to estimate 
the  dark  channel  with  the  adaptive  patch  size,  the 
performance of the DCP can be effectively enhanced.  In 
Fig.  2,  we  use  the  SOTS  dataset  in  [30]  to  compare  the 
performance  of  the  patch  map-based  DCP  with  other 
algorithms. From Fig. 2, one can see that the performance 
of the traditional DCP with the fixed patch size is not better 
than that of other state of the art algorithms. Nevertheless, 
when we applied the patch map to choose the patch size in 
each  pixel,  the  performance  become  the  best  among  all 

algorithms. From this analysis, we can conclude that, with 
the use of the proposed patch map, the performance of the 
DCP can be effectively improved.  

Therefore, in this article, first, we analyze the reasons 
why the DCP is invalid in the certain scenarios and why 
using a fixed patch size will lead to the bad performance. 
Second, we define a new feature named the patch map to 
address this problem. Third, we proposed the patch map 
selection network (PMS-Net) to generate the patch map. In 
order  to  enhance  the  performance  of  this  network,  the 
Multiscale U-module with pyramid style is proposed. Then, 
with the patch map, the more precise atmospheric light and 
the transmission map can be predicted. With the proposed 
architecture,  the  problems  of  the  traditional  DCP  can  be 
avoided (e.g. the error recovery in white or bright scenes) 
and the quality of the recovered images is higher than that 
of other algorithms. Last, we analyze the properties of the 
patch map and conclude the rule of selecting the patch size 
in the DCP. To the best of our knowledge, this is the first 
work  to  perform  the  CNNs  for  patch  map  generation  to 
remove  haze.  Simulations  show  that,  compared  to  other 
existing  algorithms,  the  proposed  method  has  the  best 
performance in both the synthetic dataset and the real world 
images.  

This article is organized as follows. In Section 2, we will 
review  several  related  works.  In  Section  3,  the  detailed 
procedure of our proposed algorithm will be demonstrated. 
In  Section  4,  the  experimental  results  compared  to  other 
state of the art haze removal algorithms are provided. In 
Section 5, a conclusion is given. 

2. Related Works 

2.1. Dark Channel Prior 





From statistical analysis, He et al. [4] found that the dark 

channels of natural haze-free images are close to zero: 

k

k

J

J

Dark





0.

( )
x



,
,

r g b

( ) min min
x
 

y
x

           (3) 
where  Jk(x)  is  one  of  the  color  channel  in  the  haze-free 
image and Ω(x) is a local patch centered at x. We call (3) the 
dark channel prior (DCP). According to the DCP, we can 
reformulate (1) as: 
( )
y
k
A


min min



( )
,
,



x
y
r g b
                                       (4) 
( ).
t x

min min


,
,


y
r g b
                                  1
 
Then, we can estimate the transmission map from: 
                      
t x

               (5) 

( ) 1
 
t x

( )
y
k
A

1
 

( )
t x

















( )
x



J

y

 

I

I

k

k

k

k

k

min min


,
,


y
r g b

 
x

k


A

k











where  ω  is  a  constant.  With  the  estimation  of  the 
transmission map, the haze-free results can be recovered.

 

Figure 3: The flowchart of the proposed haze removal algorithm. 

 

 

Although this method is powerful, it may be invalid in 
some  scenarios,  which  lead  to  some  problems  such  as 
oversaturation  and  underestimation  of  the  transmission 
maps. From our analysis, the patch size selection for each 
hazy  pixel  may  affect  the  recovered  result  a  lot.  Thus, 
instead of using a fixed patch size, we propose a network to 
learn the proper patch size automatically and adaptively for 
each hazy pixel. 

2.2. Traditional Algorithms for Haze Removal 

In  addition  to  the  DCP  method  [4],  Tarel  et  al.  [2] 
applied the bilateral filter and white balanced technique and 
Tan et al. [3] adopted the Markov random field to cope with 
the haze problem. Fattal et al. [5, 6] estimated the albedo of 
the scene and used the color-line concept to recover hazy 
images.  Ancuti  et  al.  [7-9]  used  the  semi-inverse  image 
method and the fusion method to perform dehazing. Zhu et 
al. [10] proposed a color attenuation prior based dehazing 
method. Meng et al. [11] applied the boundary constraint 
derived  from  the  haze  model  and  Berman  et  al.  [12] 
proposed a non-local image dehazing method. Chen et al. 
[13] proposed the Gradient Residual Minimization (GRM) 
recover the hazy scene and minimize the visual artifacts. 

2.3. Learning-Based Haze Removal 

Recently,  many  haze  and  smoke  removal  algorithms 
based on learning techniques have been proposed. Tang et 
al. [14] used the random forest regression [29] to predict 
the  transmission  map  by  extracting  several  haze-relevant 
features (e.g. the dark channel, saturation, contrast, and the 
hue disparity). They found that the dark channel is the most 
dominant feature while predicting the transmission map.  

For deep learning based methods, Cai et al. [15] built an 
end  to  end  CNN-based  haze  removal  system,  which  is 
called  the  DehazeNet  to  predict  the  transmission  map  of 
hazy  images.  Ren  et  al.  [16]  developed  the  multi-scale 
CNN  (MSCNN)  composed  of  the  coarse-scale  and  the 
fine-scale  networks.  Moreover,  instead  of  computing  the 

 

11683

the  atmospheric 

transmission  map  and 
light  value 
separately,  Li  et  al.  [17]  combined  these  two  variables. 
Yang et al. [20] proposed the proximal dehaze-net, which 
combined  the  haze  model,  the  dark  channel,  and  the 
transmission prior by using the energy function. Zhang et al. 
[21]  developed  the  densely  connected  pyramid  dehazing 
Network (DCPDN) to evaluate the transmission map, the 
atmospheric  light,  and  the  dehazed  image  jointly.  For 
smoke removal, Chen  et al. [28] proposed a novel model 
based  on 
[29]  and 
channel-based smoke removal. 

regression 

random 

forest 

the 

3. Proposed Method 

In this section, we illustrate the details of the proposed 
dehazing algorithm and the flow chart is shown in Fig. 3. 

3.1. Wrong Recovery in the Dark Channel Prior 

Although the conventional DCP is powerful and can deal 
with the hazy scenario effectively, it usually leads to some 
problems,  including  the  oversaturation  problem  and  the 
color distortion in white scenes or ordinary scenes. (see Fig. 
8) The reasons causing these problems are as follows. In 
the  original  assumption,  the  minimum  of  three  color 
channels  in the local patch is  close to zero for haze-free 
images. Then, with this prior, (4) can be reduced to (6).  

           
t x

 

1 min min


,
,

r g b


y

 
x

k






I



y

k


A

k

.       

(6) 






However,  this  reduction  is  the  main  reason  causing  the 
problems  mentioned  above  since  conventionally  one 
usually set a fixed patch size. If a small or a medium patch 
size such as 5 or 15 is chosen, the value of the dark channel 
may  not  always  be  zero,  especially  for  the  regions  with 
larger intensity (see the examples in Fig. 6). 

Eq. (6) is derived based on the assumption that the dark 
channel  is  near  to  zero.  If  the  patch  size  is  small,  the 
assumption will fail and (6) may be invalid. In this case, 
instead of (6), the transmission map  t(x) is formulated as

 

 

Figure 4: The overview of our proposed PMS-Net. 

Multiscale-W-ResBlock-1 Multiscale-W-ResBlock-2 Multiscale-W-ResBlock-3

Input

Input

Input

5×5 Conv

3×3 Conv

1×1 Conv

3×3 Conv

1×1 Conv

1×1 Conv

BN

BN

BN

BN

BN

ReLU

ReLU

ReLU

ReLU

ReLU

BN

ReLU

5×5 Conv

3×3 Conv

1×1 Conv

3×3 Conv

1×1 Conv

1×1 Conv

Multi-Deconv-1

Multi-Deconv-2

Input

Input

Deconv

3×3

Deconv

5×5

Deconv

3×3

ReLU

ReLU

ReLU

BN

BN

BN

BN

BN

ReLU

ReLU

ReLU

ReLU

ReLU

Output

BN

ReLU

Concat

Output

Output

Output

Output

Figure 5: The structure of the proposed Multi-W-ResBlock and the Multi-Deconv block. 

 

However,  since  Jk(y)  is  usually  unable  to  know,  the 
transmission map cannot be determined from (7). Note that 
t(x)  determined  from  (6)  will  be  smaller  than  that 
determined  from  (7)  since  the  denominator  in  (7)  is  less 
than  1.  Therefore,  using  (6)  may  underestimate  the 
transmission  map  if  the  patch  is  fixed  to  a  small  size, 
especially for the regions with white and bright color.  

By  contrast,  if  the  patch  size  is  fixed  to  a  large  size, 
although the dark channel is usually near to zero, the halo 
artifacts [4] is caused and the computation is increased. As  
a result, selecting the patch size adaptively for each pixel is 
crucial to get high quality recovered results.  

Therefore, we propose the Patch Map Selection Network 
the  patch  size  adaptively.  Its 

to  select 

(PMS-Net) 
architecture is shown as in Fig. 4. 
 
 

 

 
Figure 6: White scenes may have the dark channel far from zero. 
(1st column is the input; 2nd and 3rd is the dark channel estimated 
by the patch size 15 and 120). 

               
t x



1 min min


,
,
r g b


y

 
x




k

1 min min


,
,
r g b


y

 
x




k












y
k



y

I

J

k


A

A

k

k

.                           (7) 










 

11684

 

3.3. Patch Map Selection Net 

The structure of the proposed PMS-Net is shown in Fig. 
4. It can be divided into the encoder part and the decoder 
part. Originally, the input hazy image will be projected to 
the  higher  dimension  space  by  convolving  a  3×3  kernel 
with 16 filters. Then, the proposed Multiscale U-module is 
applied to extract the features from this higher dimension 
data. 

The design of the multiscale U-module is shown in Fig. 4. 
The  input  will  pass  several  Multiscale–W-ResBlocks 
(MSWR) which are shown in the left side of Fig. 5. For the 
design of the MSWR, inspired by the Wide-ResNet (WRN) 
[31],  which  improves  the  ResNet  [32]  by  increasing  the 
width and decrease the depth of the network, we apply the 
WRN  in  our  network.  In  each  block,  we  perform  the 
Conv-BN-ReLu-Dropout-Conv-BN-ReLu with a shortcut to 
extract  the  information.  Moreover,  the  design  of  the 
multi-scale  concept 
inspired  by 
Inception-ResNet [42] and [9,  14,  15,  16,  17,  21]  which 
adopted the multi-level technique to enhance the variety of 
the  information  and  extract  the  detailed  information.  By 
this operation, the network can find the transmission map 
or the haze-free image more effectively. Although in this 
network, the output is different from previous works, we 
still apply this technique because we believed that the patch 
map is the haze-related feature.  

the  MSWR 

in 

is 

Besides,  instead  of  using  the  same  architecture  of  the 
MSWR, the pyramid style is performed in order to extract 
the information in different level. More specifically, for the 
first  block,  we  connect  three  convolutional  kernels  with 
various sizes (5×5, 3×3, 1×1) because we need to preserve 
more information in different scale in the higher layer. For 
the second and the third blocks, the convolutional kernels 
with (3×3, 1×1) and (1×1) are adopted respectively. By this 
operation, the use of parameters can be reduced as well.  

For the other part in the Multiscale U-module, we use the 
Multi-Deconv module to concatenate the information with 
the  output  of  the  MSWR  instead  of  the  traditional 
deconvolution, since the deconvolution layer can help the 
network to reconstruct shape information of the input data 
[33].  Therefore,  with  the  combination  of  the  multiscale 
deconvolution,  we  can  reconstruct  more  precise  feature 
maps from the previous layers. In addition, in the design of 
the Multi-Deconv, we also performed the pyramid style to 
up-scale the information to concatenate with MSWR. The 
reason is the same as the design of the MSWR. That is, the 
feature maps in the different levels should be deconvolved 
with various scale (see the right side in Fig. 5) 

To preserve the high resolution features, the outputs of 
the MSWR and the Multi-Deconv module are concatenated 
directly.  Then,  the  feature  maps  will  be  fed  into  the 
Multi-Deconv in the higher layers and the decoder. For the 
design  of 
the  global 

the  decoder,  we 

adopt 

 
Figure 7: The patch map generated by the method in sub-section 
3.2.  (The  upper  row:  input  hazy  images;  The  lower  row:  
corresponding patch maps) 

3.2. Patch Map Generator 

To train the PMS-Net, we need to generate the patch map 
from  the  training  data.  Thus,  we  propose  the  patch  map 
generator (PMG). 

The design of the PMG is that, initially, we apply (8) and 

(9) to recover the image with different patch sizes.  

( ) 1
 
t x
i



min min

,
,

y
r g b

( )
x




k

i





k

I

( )
y
( )
A y

k
i

, 

i



1,...,

n

        (8)  









x



J

i

k

( )

A
I x
i
( )
t x
i



k

A
i

, 

k



{ ,
, }
r g b



1,...,

n

      (9) 

i

i

i

i

gth

J

n







, 

1,...,

( )
J x

( )
E x

            

where I(x) is the input, ti(x), Ji(x), Ai
k(x) are the transmission 
maps,  the  haze-free  images  and  the  atmospheric  light 
values with different patch sizes i, respectively. Then, we 
calculate the error function between the recovered results 
and the ground truth by:  
( )
x

                 (10) 
where Ei(x) is the error function of the recovered result with 
patch size i and Jgth(x) is the ground truth. We determine the 
error functions for different patch sizes i and find the patch 
size with the lowest error function for each pixel and assign 
it as the patch map value of the pixel:  
( ) min

    
n
(11) 
where PMG(x) is the patch map and k is the patch size that 
can minimize the error function at the location  x. In this 
case,  we  set  the  n  as  120.  With  this  operation,  we  can 
produce the patch map for all images and use them for the 
training procedure. 
  The examples of patch map are shown in Fig. 7. One can 
see that, in the white, gray, bright, and sky region, the patch 
size prefers  to be higher  in  order  to  fit  the  dark  channel 
prior. (see the sky and white region) Otherwise, in the other 
dark region, the patch size is preferred to be small. 
 

k ,  k  

( )
PMG x

( )
E x

1,...,

.s t  

E x









,

i

k

i

 

 

11685

Table 1: Qualitative analysis on the Test A dataset. 
AOD-Net 

Table 2: Qualitative analysis on the Test B dataset. 
AOD-Net 

 

FVR 
[2] 

0.0441 
0.6915 
14.24 
15.045 

FVR 
[2] 

0.0283 
0.7944 
15.999 
12.402 

 

MSE 
SSIM 
PSNR 

CIEDE2000 

 

MSE 
SSIM 
PSNR 

CIEDE2000 

CAP 
[10] 
0.0509 
0.6906 
13.735 
13.331 

CAP 
[10] 
0.0363 
0.6838 
14.973 
13.275 

MSCNN 

[16] 
0.0627 
0.7028 
12.7597 
15.8265 

MSCNN 

[16] 

0.02083 
0.85407 
18.3836 
9.04877 

 

 

[17] 
0.0703 
0.7115 
12.396 
16.063 

[17] 
0.0264 
0.8644 
17.244 
10.282 

NL 
[12] 
0.0526 
0.6635 
13.3394 
15.3333 

NL 
[12] 

0.02833 
0.74903 
16.8172 
11.8897 

Table 3: Quantitative MSE evaluation for ablation study on the 

 

MSE 

Module A 
0.04415 

set of Test A+B 
Module B 

0.0398 

Module C 
0.03635 

 
convolutional network modules (GCN) [34]. The boundary 
refinement module (BR) [34] is also applied to preserve the 
edge information. For the up-scale layer, the up-sampling 
operation is adopted. Furthermore, the densely connected 
style [35, 36] is applied to merge the high-resolution and 
the 
the  proposed 
PMS-Net, the patch map can be predicted well. 

information.  With 

low-resolution 

4. Experimental Results 

In this section, we will demonstrate the performance of 
our proposed haze removal algorithm. We will adopt two 
different  state-of-the-art  synthetic  haze  datasets  and  one 
real world hazy image dataset to compare the performance.  

4.1 Datasets and the Training Implementation 

In this work, we adopt the NYU-depth2 dataset [37] and 
the RESIDE dataset [30]. In the training procedure, we use 
2192 images in total which consists of 1200 images from 
the  NYU-depth2  dataset,  492  images  from  the  RESIDE 
dataset, and 500 synthesized images as our training data. In 
the training model, the learning rate is  e-4 and the Adam 
optimizer [38] is adopted. The batch size is set to 4 and the 
loss function is the mean square error (MSE). In each epoch, 
we adopt early stopping and use the 10 % of the original 
data  as  the  validation  set.  Similar  to  other  works  of 
dehazing, for the test dataset, we use the remained images 
in these datasets as the test set. We ensure that none of these 
test images was used in the training process. We take 150 
images 
the  NYU-depth2  dataset  and  apply 
synthesized haze on these images as Test A. Then, we take 
200 outdoor images and 100 indoor images from RESIDE 
dataset and NYU-depth2 dataset as Test B.   

from 

 

11686

GRM 
[13] 
0.0398 
0.7863 
14.862 
11.381 

GRM 
[13] 
0.0315 
0.8088 
16.293 
11.822 

Meng 
[11] 
0.0246 
0.7749 
16.4119 
10.1913 

Meng 
[11] 
0.0342 
0.7859 
15.564 
12.7754 

DEFADE 
[39-41] 
0.0604 
0.6435 
12.7167 
16.4385 

DEFADE 
[39-41] 
0.03693 
0.7428 
15.7237 
13.949 

DCP 
[4] 

0.0249 
0.8234 
16.553 
8.9547 

DCP 
[4] 

0.0307 
0.8131 
16.568 
11.351 

Ours 

0.0139 
0.8684 
19.2121 
6.0652 

Ours 

0.0139 
0.8775 
20.152 
7.2965 

4.2  Qualitative Comparison on Synthetic Data 

We apply four assessment metrics: the mean square error 
(MSE), the structural similarity (SSIM), the peak-to-peak 
signal  to  noise  ratio  (PSNR),  and  the  CIEDE2000  color 
difference.  Then,  we  perform  nine  existing  dehazing 
algorithms which are the DCP [4] (CVPR’ 09), the CAP 
[10] (TIP’ 15), the MSCNN [16] (ECCV’ 16), the NL [12] 
(CVPR’  16),  the  Meng  [11]  (ICCV,  13),  the  FVR  [2] 
(CVPR’  09),  the GRM [13]  (ECCV’  16),  the  DEFADE 
[39-41] (TIP’ 15), and the AOD-Net [17] (ICCV’ 17). 

The comparison results are shown in Table 1 and Table 2. 
One  can  find  that  our  proposed  method  has  the  best 
performance in haze removal in all metrics comparing to 
other  methods  in  both  two  test  datasets.  Note  that  the 
proposed method has high fidelity  for  color  preservation 
since it has  much  lower  values  of  the  CIEDE2000  color 
difference than other methods. Furthermore, comparing to 
the  conventional  DCP,  with  the  fixed  patch  size,  the 
performance of proposed method is much better (the MSE 
is 49% less, the SSIM is increased by 6%, the PSNR is 
19% higher, and the CIEDE2000 color difference is 34% 
less).  Thus,  our  proposed  method  can  enhance  the 
performance of the conventional DCP effectively. 

4.3  Dehazed Results in Real World Images  

We  demonstrate  the  improvement  of  the  proposed 
method  by  comparing  the  recovered  images  with  other 
methods  visually  in  Fig.  8.  We  adopt  the  images  which 
were also used as the test data in previous works.  

From Fig. 8, one can see that our proposed method can 
well  remove  the  haze  from  images  without  hurting  the 
image  quality  (see  the  1st (waterfall),  3rd  (mountain),  5th 
(stadium), 6th (girls)). Moreover, with the proposed method, 
the color shift problem and the oversaturation problem can 
be  well  avoided.  Especially,  one  can  observe  that  our 
proposed algorithm can prevent losing the color fidelity in 
the white hazy scenes (see the pictures in the 1st(waterfall),  

 

        Input             FVR[2]           CAP[10]         Meng[11]      MSCNN[16]   AOD-Net[17]      DCP[4]             Ours 

Figure 8: Comparison of dehazing results of state-of-the-art methods and our proposed method. 

 

 
2nd(canyon), 6th (girls), 7th(city), and 8th rows (marble)). 

In  Fig.  9,  we  amplify  the  scenarios  which  contain  the 
white, bright, and sky scenes. One can see that, when using 
the original DCP with fixed patch size, the color distortion 
is  severe  (see  the  red  bounding  boxes  in  the  2nd  and  4th 
columns).  However,  with  the  use  of  the  proposed  patch 
map, the color distortion problems in these scenarios can be 
well avoided (see the blue bounding boxes in the 3rd and 4th 
columns). One can notice that the image presented in the 3th 
row  is  the  picture  which  is  used  to  demonstrate  the 
limitation of the DCP in [4]. The reasons why our proposed 
method can solve the color distortion problem is that the 
PMS-Net can select the proper patch size for each pixel in 
the  haze  removal  process.  More  specifically,  in  white  or 

bright part, the patch map will be bigger in order to make 
the  dark  channel  be  near  to  zero  (see  the  pictures  in  5th 
column). 

With this mechanism, the transmission map in the white 
and  bright  scenes  may  be  higher  comparing  to  original 
DCP method since the estimation of the transmission map 
in  (5)  can  be  applied  without  the  error  caused  from  the 
nonzero value of the dark channel.  

4.4. Ablation Study 

In  this  subsection,  to  demonstrate  the  improvement  of 
using the proposed Multiscale U-module with the pyramid 
style in our PMS-Net, we perform the ablation study. This  

 

11687

 

 
Figure 9: The comparison of the recovered results in white and bright scenes. (1st column): the input image; (2nd column): the recovered 
results  by  the  conventional  fixed  patch  size  DCP;  (3rd  column):  the  recovered  results  by  our  proposed  method;  (4th  column):  the 
amplification  of  the  white  or  bright  portion  in  the  2nd  and  the  3rd  columns;  (5th  column):  the  patch  map;  (6th  and  7th  columns):  the 
transmission maps estimated by the DCP and our proposed method, respectively.  
 
experiment  includes  five  modules:  1)  The  PMS-Net 
without  the  Multiscale  U-module  and  the  pyramid  style 
(Module A); 2) The PMS-Net with Multiscale U-module 
but without pyramid style (Module  B); 3) The proposed 
algorithm using the Multiscale U-module with the pyramid 
style (Module C). We combine both Test A and Test B as 
one  dataset  in  this  experiment  and  apply  the  MSE  for 
evaluation.  

Multiscale-W-ResBlocks and the Multi-Deconv blocks, to 
improve the performance. Moreover, in the design of the 
Multiscale  U-module,  we  adopt  the  pyramid  style  to 
enhance  the  performance.  With  the  patch  map,  a  more 
accurate transmission map and a high-quality reconstructed 
results can be achieved. Furthermore, the bad performance 
in white and  bright  scenes  can  be  well  addressed  by  the 
patch map.  

The results are shown in Table 3. The results show that, 
when  we  apply  the  multiscale  U-module,  comparing  to 
Module A, the MSE be reduced by 8.7%. Therefore, the 
proposed Multiscale U-module can enhance the accuracy 
of the patch map prediction effectively. Moreover, in our 
proposed  Multiscale  U-module,  we  perform  the  pyramid 
style in our network. One can notice that this architecture 
can  not  only  improve  8.6%  accuracy  of  the  network  but 
also decrease 30% of the parameters. 

5. Conclusion and Analysis 

In  this  paper,  we  demonstrate  a  novel  haze  removal 
algorithm by introducing a new feature-patch map. First, 
we  investigate  the  reasons  that  limit  the  quality  of  the 
dehazed  images  when  applying  the  conventional  DCP 
based method. We find that the bad performances in white 
and bright scenes are due to the fixed patch size. Then, we 
notice  that,  after  selecting  the  patch  size  properly  and 
adaptively for each pixel, the recovered images can have 
outstanding  performance  comparing  to  other  methods. 
Therefore, we define the patch map, and design the patch 
map selection network (PMS-Net) to estimate the proper 
patch size. In the design of the PMS-Net, we proposed the 
Multiscale  U-module  architecture,  which  consists  of  the 

 

11688

In experimental results, both qualitative evaluations and 
visual results show that our proposed method have better 
performance  than  existing  methods.  Moreover,  with  the 
ablation study, the Multiscale U-module with the pyramid 
style  show  that  it  can  enhance  the  performance  of  the 
PMS-Net effectively. 
 
Acknowledgments  This  work  was  supported  by  the 
Ministry of Science and Technology of Taiwan under grant 
MOST 105-2221-E-002-120-MY3. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

[22] R. Li, J. Pan, Z. Li, and J. Tang. Single image dehazing via 
conditional generative adversarial network. In CVPR, 2018. 
[23] D. Park, D. K. Han and H. Ko. Nighttime image dehazing 
with local atmospheric light and weighted entropy. In ICIP, 
2016. 

[24] Y. Li, R. T. Tan, and M. S. Brown. Nighttime haze removal 

with glow and multiple light colors. In ICIP, 2015. 

[25] J.  Zhang,  Y.  Cao,  and  Z.  Wang.  Nighttime  haze  removal 

based on a new imaging model. In ICIP, 2014 

[26] C.  Ancuti,  C.  O.  Ancuti,  A.C.  Bovik,  and  Christophe  De 
Vleeschouwer.  Night  time  dehazing  by  fusion.  In  ICIP, 
2014. 

[27]  J. Zhang, Y. Cao, S. Fang, Y. Kang, and C. W. Chen. Fast 
image  using  maximum 

haze  removal  for  nighttime 
reflectance prior. In CVPR, 2017. 

[28] W. T. Chen, S. Y. Yuan, G. C. Tsai, H. C. Wang, and S. Y. 
Kuo. Color channel-based smoke removal algorithm using 
machine learning for static images. In ICIP, 2018. 

[29] L. Breiman. Random forests. Mach. Learn., 45(1), 2001 
[30] B. Li, W. Ren, D. Fu, D.  Tao, D.  Feng, W. Zeng, and  Z. 
Wang.  Reside:  A  benchmark  for  single  image  dehazing. 
arXiv preprint arXiv:1712.04143, 2017. 

[31] S. Zagoruyko and N. Komodakis, “Wide residual networks,” 

arXiv preprint arXiv:1605.07146, 2016. 

[32] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning 

for image recognition. In Proc. CVPR, 2016. 

[33] H.  Noh,  S.  Hong,  and  B.  Han.  Learning  deconvolution 

network for semantic segmentation. In ICCV, 2015. 

[34] C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun. Large kernel 
segmentation  by  global 

matters 
convolutional network. In CVPR, 2017 

semantic 

improve 

[35] D. Liu, D. Zhang, S. Liu, Y. Song, H. Jia, D. Feng, Y. Xia 
and W. Cai. Densely Connected Large Kernel Convolutional 
Network 
in 
Microscopy Images. In ICIP, 2018 

for  Semantic  Membrane  Segmentation 

[36] G. Huang, Z. Liu, K. Q. Weinberger, and L. Maaten. Densely 

connected convolutional networks. In CVPR, 2017. 

[37] N.  Silberman,  D.  Hoiem,  P.  Kohli,  and  R.  Fergus.  Indoor 
segmentation  and  support  inference  from  rgbd  images.  In 
ECCV, 2012. 

[38] D.  Kingma  and  J.  Ba.  Adam:  A  method  for  stochastic 

optimization. In ICLR, 2015. 

[39] L.  K.  Choi,  J.  You,  and  A.  C.  Bovik.  Referenceless 
prediction  of  perceptual  fog  density  and  perceptual  image 
defogging. TIP, 24(11):3888– 3901, 2015. 

[40] L.  K.  Choi,  J.  You,  and  A.  C.  Bovik.  Referenceless 

perceptual image defogging. In SSIAI, 2014. 

[41] L.  K.  Choi,  J.  You,  and  A.  C.  Bovik.  DEFADE  Software 
Release:http://live.ece.utexas.edu/research/fog/DEFADE_re
lease.zip, 2015. 

[42] C.  Szegedy,  S.  Ioffe,  and  V.  Vanhoucke.  Inception-v4, 
inception-resnet and the impact of residual connections on 
learning. In ICLR Workshop, 2016. 

 

CVPR, 2008. 

27(3):72, 2008. 

34(1):13, 2014 

References 
[1]  S. G. Narasimhan and S. K. Nayar. Chromatic framework for 

vision in bad weather. In CVPR, 2000. 

[2]  J.-P. Tarel and N. Hautiere. Fast visibility restoration from a 

single color or gray level image. In CVPR, 2009. 

[3]  R.  Tan.  Visibility  in  bad  weather  from  a  single  image.  In 

[4]  K. He, J. Sun, and X. Tang. Single image haze removal using 

dark channel prior. In CVPR, 2009. 

[5]  R. Fattal. Single image dehazing. ACM Trans. Graph., vol. 

[6]  R. Fattal. Dehazing using color-lines.  ACM  Trans.  Graph., 

[7]  C. O. Ancuti, C. Ancuti, C. Hermans, and P. Bekaert. A fast 
semi-inverse approach to detect and remove the haze from a 
single image. In ACCV, 2011. 

[8]  C.  O.  Ancuti,  C.  Ancuti,  and  P.  Bekaert.  Effective  single 

image dehazing by fusion. In ICIP, 2010. 

[9]  C.  O.  Ancuti  and  C.  Ancuti.  Single  image  dehazing  by 
multiscale  fusion.  IEEE  Trans.  on  Image  Processing, 
22(8):3271–3282, 2013 

[10] Q.  Zhu,  J.  Mai,  and  L.  Shao.  A  fast  single  image  haze 
removal algorithm using color attenuation prior. IEEE Trans. 
Image Processing, 24(11):3522–3533, 2015. 

[11] G. Meng, Y. Wang, J. Duan, S. Xiang, and C. Pan. Efficient 
image  dehazing  with  boundary  constraint  and  contextual 
regularization. In ICCV, 2013. 

[12] D.  Berman,  T.  Treibitz,  and  S.  Avidan.  Non-local  image 

dehazing. In CVPR, 2016. 

[13] C. Chen, M. N. Do, and J. Wang. Robust Image and Video 
Dehazing  with  Visual  Artifact  Suppression  via  Gradient 
Residual Minimization. In ECCV 2016. 

[14] K. Tang, J. Yang, and J. Wang. Investigating haze-relevant 
features  in  a  learning  framework  for  image  dehazing.  In 
CVPR, 2014. 

[15] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao. Dehazenet: An 
end-to-end  system  for  single  image  haze  removal.  TIP, 
25(11):5187–5198, 2016. 

[16] W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao, and M.-H. Yang. 
Single image dehazing via multi-scale convolutional neural 
networks. In ECCV, 2016. 

[17] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng. Aod-net: All 

in-one dehazing network. In ICCV, 2017. 

[18] Y. Huang, Y Wang, and Z. Su. Single Image Dehazing Via a 

Joint Deep Modeling. In ICIP, 2018. 

[19] G.  Kim,  S.  Ha,  and  J.  Kwon.  Adaptive  Patch  Based 
Convolutional Neural Network for Robust Dehazing. In ICIP, 
2018. 

[20] D.  Yang  and  J.  Sun.  Proximal  dehaze-net:  A  prior 
learning-based deep network for single image dehazing, In 
ECCV, 2018. 

[21] H.  Zhang  and  V.  M.  Patel.  Densely  connected  pyramid 

dehazing network. In CVPR, 2018. 

 
 
 
 
 
 

 

11689

