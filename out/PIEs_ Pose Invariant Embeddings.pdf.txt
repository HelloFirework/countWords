PIEs: Pose Invariant Embeddings

Chih-Hui Ho

Pedro Morgado

Amir Persekian

Nuno Vasconcelos

University of California, San Diego

{chh279,pmaravil,aperseki,nvasconcelos}@ucsd.edu

Abstract

The role of pose invariance in image recognition and
retrieval is studied. A taxonomic classiﬁcation of embed-
dings, according to their level of invariance, is introduced
and used to clarify connections between existing embed-
dings, identify missing approaches, and propose invariant
generalizations. This leads to a new family of pose invariant
embeddings (PIEs), derived from existing approaches by a
combination of two models, which follow from the interpre-
tation of CNNs as estimators of class posterior probabili-
ties: a view-to-object model and an object-to-class model.
The new pose-invariant models are shown to have interest-
ing properties, both theoretically and through experiments,
where they outperform existing multiview approaches. Most
notably, they achieve good performance for both 1) classiﬁ-
cation and retrieval, and 2) single and multiview inference.
These are important properties for the design of real vision
systems, where universal embeddings are preferable to task
speciﬁc ones, and multiple images are usually not available
at inference time. Finally, a new multiview dataset of real
objects, imaged in the wild against complex backgrounds,
is introduced. We believe that this is a much needed com-
plement to the synthetic datasets in wide use and will con-
tribute to the advancement of multiview recognition and re-
trieval.

1. Introduction

Convolutional neural networks (CNNs) are frequently
used for classiﬁcation and metric learning, among other
tasks. Classiﬁcation is the central problem of important
computer vision applications, such as object and action
recognition or detection. Metric learning plays a similar
role for image retrieval, face recognition and identiﬁcation,
or zero shot learning. Despite the many different applica-
tions, the two tasks are closely related, since they both learn
an embedding g : X → G that maps images x ∈ X into
features g(x) ∈ G and are implemented with several CNN
layers. Classiﬁcation aims to produce a discriminant feature
space F , which separates the different classes, while metric

Figure 1. Taxonomy of embeddings learned by different methods
according to different level of invariance. Green solid boxes rep-
resent methods in the literature and yellow dashed boxes repre-
sent methods proposed in this work. The proposed pose invariant
embedding incorporates single view and multiview invariance and
can be applied to different methods, including CNN, proxy-NCA
and triplet center. While CNN is designed for classiﬁcation, the
other two aim for metric learning (retrieval task).

learning aims to produce a feature space F with a certain
metric structure, where similarity can be captured by some
distance function, typically the Euclidean distance.

As shown in the bottom row of Figure 1, classiﬁcation
and metric learning have evolved in lockstep. While details
of the architecture of g(.) may favor one or the other, ap-
proaches to the two problems have differed mostly in the
subsequent network layers and loss function. Classiﬁers
complement the embedding g with a softmax layer trained
with the logistic loss. Classic metric learning uses no ad-
ditional layers and a different loss. While several variants
have been proposed [5, 10, 21, 15], the most popular is the
triplet loss of [27, 26, 1, 19]. In practice, however, the dif-
ferences can be signiﬁcant. Since triplets raise the dataset
size to its cube, metric learning networks are more difﬁcult
to train than classiﬁers. To address this, much of the embed-
ding literature has been devoted to triplet sampling strate-
gies [26, 1, 19, 17, 21], aimed to increase training speed.
Recently however, [15] has shown that much faster training
is possible by using proxy embeddings, which make metric
learning a lot more like classiﬁcation. Inspired by a met-
ric learning approach known as neighborhood component
analysis (NCA) [9], it adds a layer that resembles a soft-

12377

max classiﬁer to the embedding and uses the logistic loss
for training. A similar generalization of triplet embeddings
has been proposed in [11], and denoted as triplet center em-
beddings.

Ideally, an embedding should map all the images of an
object collected from multiple views, depths or under dif-
ferent illuminations, into a single point, known as the ob-
ject invariant to these transformations. However, this is
hard to achieve on datasets such as ImageNet, which tend
to emphasize class diversity and maximize the number of
objects imaged per class. They do not provide a dense cov-
ering of the transformations (imaged from different cam-
era positions, variable lighting, etc) where an object may
be subjected to. Recently, this problem has received sig-
niﬁcantly more attention, with the introduction of datasets
such as ModelNet [29] or ShapeNet [4]. Being datasets of
synthetic images rendered from 3D CAD models, these al-
low the generation of many views of each object labelled
for view angle, also known as object pose.

The introduction of these multiview synthetic datasets
motivated a new wave of algorithms for multiview [22, 12,
6] classiﬁcation and retrieval, as shown in the middle row of
Figure 1. These methods have been shown competitive, if
not superior, to many methods based on 3D representations,
such as voxels [29, 14, 3, 28] or point clouds [7, 31, 30].
This is important because view-based representations can
be easily deployed in the real world, where 3D representa-
tions are much more expensive, if not completely infeasi-
ble. The most popular architecture for view-based classiﬁ-
cation is the multiview-CNN (MV-CNN) [22], which com-
plements a standard CNN embedding with a view pooling
mechanism that produces a shape descriptor. The shape de-
scriptor is then fed to the softmax layer for classiﬁcation.
Similarly, [11] have introduced the triplet-center loss for
multiview metric learning. This is a generalization of the
triplet loss and center loss to a multiview level for NCA
style metric learning.

While these approaches have been shown to be effec-
tive for multiview classiﬁcation and retrieval, which can
be performed easily in the CAD world (e.g ModelNet and
ShapeNet), their usefulness for real vision systems is more
questionable, for two reasons. First, it is not known how
well they work on real images due to the absence of datasets
of real images in the wild, with coverage of pose trajecto-
ries. While some dense pose datasets exist [2, 12, 8, 16],
they are small and tend to depict objects on turntables, with-
out complex backgrounds. Second, and more important,
these approaches do not really learn pose invariant embed-
dings. While the shape descriptor is a summary of all the
views of the object, the embedding of a single image is not
constrained to be similar to this descriptor. In result, these
methods tend not to perform well for single view recogni-
tion or retrieval, where they frequently have weaker perfor-

mance than standard CNNs. This is important because the
multiview setting is not realistic for most real world appli-
cations. While multiview training is of interest to enable
learning algorithms to capture object variability under var-
ious transformations, applications frequently constrain in-
ference to single views. To support the latter, multiview
training must produce truly pose invariant embeddings.

In this work, we address these limitations through a com-
bination of contributions. First we perform a review of
various approaches in the literature, placing various meth-
ods on equal footing and enabling a better understanding
of their relative strengths and weaknesses. This results in
Figure 1, which groups embeddings by their level of invari-
ance. Existing methods are identiﬁed by green boxes. It
is clear that no truly pose invariant embeddings are avail-
able. While view-based embeddings have little invariance,
multiview embeddings produce a shape descriptor that rep-
resents multiple views, but do not map individual views to
this descriptor.

Second, we propose a number of new approaches,
showed as yellow boxes in Figure 1. Some of these just ﬁll
holes in the layers populated by existing methods. For ex-
ample, MV-Proxy is simple variants of [15] for multiview
level and triplet center is variants of [11] for single view
level. Other yellow boxes in pose invariant level (top row)
are based on new loss functions that encourage embeddings
that cluster individual images in the neighborhood of shape
descriptors. This makes the shape descriptors truly invari-
ant and enables better performance on single view retrieval
and recognition tasks. Finally, we introduce a new multi-
view dataset for object recognition in the wild. This dataset
is composed of objects belonging to ImageNet, and are in
all aspects similar to ImageNet images. However, each ob-
ject is imaged under a set of pre-deﬁned poses, which are
provided as additional labels. Similarly to ShapeNet and
ModelNet, this enables the learning of pose invariant repre-
sentations. However, because the images are real, the new
dataset enables the testing of invariance in a more realistic
setting. Experiments on both the proposed dataset and syn-
thetic datasets show that the proposed pose invariant embed-
ding is more robust to a variable number of views provided
for inference.

2. Related work

Many works have addressed embeddings for classiﬁca-
tion and retrieval. We review the literature in this section,
emphasizing the ideas that are directly relevant to this work.

Classiﬁcation: Given observations and class labels drawn
from random variables X ∈ Rm and Y ∈ {1, . . . , C}
the classiﬁer of minimum probability of error is y∗ =
arg maxy PY |X(y|x). A CNN is a model for the posterior

12378

probabilities

PY |X(y|x) = hy(x; W, b) =

y g(x)+by

ewT
PC
k=1 ewT

k g(x)+bk

composed of two stages. The ﬁrst is an embedding g(x) ∈
F ⊂ Rd, implemented by the layers of the network up to the
last one, where g is a d dimension feature extractor. Usu-
ally, g consists of a combination of convolutions, pooling,
and a ReLU non-linearity. The second is a softmax layer
that resides at the top of the network and computes (1) us-
ing a layer of weights W ∈ Rd×C and biases b ∈ RC . To
minimize notational clutter, we will omit the bias vector in
many of the expressions below. This follows the common
practice of absorbing it in W and using homogeneous coor-
dinates. CNNs are trained by cross-entropy minimization.
Given a dataset D = {(xi, yi)}n
i=1 this consists of ﬁnding
W and the parameters of g that minimize the risk

R(D) = Xi

L(xi, yi),

(2)

deﬁned by the logistic loss L(x, y) = − log hy(x; W).
Metric learning: Metric learning aims to endow the feature
space F with a metric, usually the Euclidean distance

d(g(x), g(y)) = ||g(x) − g(y)||2,

(3)

so as to allow the geometric implementation of operations
like classiﬁcation, e.g. using nearest neighbors. While
many losses have been proposed [5, 10, 21], this is usually
done with a loss function that operates on example triplets,
pulling together (pushing apart) similar (dissimilar) exam-
ples [27, 26, 1, 19]. Given an anchor x, a similar x+ and a
dissimilar example x−, the triplet loss is deﬁned as

L(x, x+, x−) = φ(cid:0)d(g(x), g(x−)) − d(g(x), g(x+))(cid:1) ,

(4)
where φ(.) is a margin loss, e.g.
the hinge loss φ(v) =
max(0, m − v) or the logistic loss φ(v) = log(1 + e−v). In
general, similar and dissimilar examples are determined by
the class labels of D. We refer to these methods as triplet
embeddings.

Modern CNNs are learned by stochastic gradient descent
(SGD), processing the data in batches of relatively small
size, e.g. b = 32. On a dataset of size n there are O(n) ex-
amples and O(n3) triplets. Similarly, there are O(b) exam-
ples and O(b3) triplets in a batch. Hence, while the number
of batches needed to cover the dataset is O(n/b) for exam-
ples, it becomes O((n/b)3) for triplets [15]. Since n/b is
in the tens of thousands, triplet learning is cubically more
complex than example-based learning. While many sam-
pling strategies have been proposed to address this prob-
lem [19, 26, 17, 23], metric learning methods are substan-
tially harder to use and slower to converge than classiﬁca-
tion methods.

Recently, [15] has shown that this problem can be over-
came using a loss function inspired by neighborhood com-
ponent analysis (NCA) [9]. This consists of deﬁning a proxy
py per class, adding a softmax-like layer

(1)

sy(x; P) =

(5)

e−d(g(x),py)

Pk6=y e−d(g(x),pk) ,

where P is the matrix of proxies pk, and learning both P
and g(x) by minimizing the risk of (2) with the logistic loss
L(x, y) = − log sy(x; P). We refer to this method as proxy
embedding.
Multiview classiﬁcation: In multiview classiﬁcation, each
observation consists of a set of V views X = {xk}V
k=1
and parameters are learned from a multiview dataset Dm =
{(Xi, yi)}n
i=1. The goal is to
jointly classify all these views. A popular approach is the
multiview-CNN (MV-CNN) [22], which implements two
embeddings. Each individual image xk, where xk is imaged
at kth predeﬁned viewpoint, is processed by a shared fea-
ture extractor g and all the resulting view descriptors g(xk)
is then averaged to produce a shape descriptor

i=1 = {(xi1, . . . , xiV , yi)}n

gm(X) =

1
V

V

Xk=1

g(xk),

(6)

where subscript m denotes multiview. The embedding pa-
rameters are learned from a multiview dataset Dm by using
gm with softmax layer (1), the risk of (2), and the logis-
tic loss. Several variants of this approach have been pro-
posed, either making speciﬁc architectural enhancements
to the embedding g [25, 18], or using weighted versions
of (6) [6]. Similar enhancements are possible for all meth-
ods discussed in this work.
Multiview metric learning: Substantially less work has
been devoted to multiview metric learning. [11] combined
the MV-CNN embedding with the proxy-based idea of [15],
but applied to the triplet loss. They denote proxies as cen-
ters and deﬁne the multiview triplet-center loss

L(X, y, P) = φ(cid:18)min

j6=y

d(gm(X), pj) − d(gm(X), py)(cid:19)

(7)
where P is the matrix of centers pj and gm is deﬁned as
in (6). We refer to this method as multiview triplet center
(MV-TC) embedding.

3. Bringing object invariants to the real world

In this section, we discuss a number of contributions that

follow from the above review.

3.1. New view based and mutiview embeddings

Figure 1 provides a functional organization of embed-
dings for classiﬁcation and metric learning. The bottom two

12379

a) view-based

b) multi-view

c) invariant

Figure 2. Embeddings produced by methods at the three levels of invariance of Figure 1. In all plots, there are three classes, two objects
per class, and each dot represents the embedding of an image. Dots of the same color correspond to different views of the same object. In
b) and c), a ′+′ is used to denote the shape descriptor and a dashed circle to denote the distribution of views of the associated object. Only
the invariant embedding of c) guarantees a good clustering of both shape descriptors per class and individual views per object.

rows summarize the state of the literature, with green boxes
identifying the approaches that have been proposed. They
group these methods according to whether they embed sin-
gle or multiple views. One immediate contribution is that
there are a number of “missing” approaches (e.g multiview
proxy and single view triplet center). We propose to ﬁll
the gaps, introducing several new embeddings, which are
extensions of those available: the triplet center embedding
is the view-based equivalent of the multiview triplet center
embedding[11], replacing multiview triplet-center loss (7)
with single view

L(x, y, P) = φ(cid:18)min

j6=y

d(g(x), pj) − d(g(x), py)(cid:19) ,

(8)

and the MV-proxy generalizes the single view proxy embed-
ding (5) to multiview

sm
y (X; P) =

e−d(gm(x),py)

Pk6=y e−d(gm(x),pk) ,

where superscript m denotes multiview.

(9)

3.2. The need for invariant embeddings

A second, and practically more important, contribution
of Figure 1 is to show that no attention has been given to
the design of truly invariant embeddings. This is important
for many real-world systems, where one would like to lever-
age multiview data for training but perform classiﬁcation or
retrieval on single views. In general, it is not realistic to
expect that multiple views of an object will be available at
classiﬁcation or retrieval time. We refer to this problem as
pose invariant classiﬁcation and retrieval. Figure 2 illus-
trates the limitations of existing approaches to address this
problem.

View-based embeddings do not leverage multiple object
views during training, treating all views of all objects in
the same class equally. In result, as illustrated in Figure 2

a), there is no guarantee that these embeddings will clus-
ter views from same object. While clustering views into
classes, they are free to intertwine the views of different
objects in the same class. On the other hand, multiview em-
beddings (6) only constrain the shape descriptor, i.e. the av-
erage of single view embedding. As illustrated by Figure 2
b), where shape descriptors are denoted by a ’+’, this suf-
ﬁces to produce a good shape descriptor clustering. How-
ever, it does not guarantee a good clustering of all individ-
ual views from an object. Note that the shape descriptors
are all correctly classiﬁed, but this is not the case for the
individual views, which can spread across class boundaries.
This is illustrated by the dashed circles, which identify the
distribution of images of each object. Due to this prob-
lem, multiview approaches tend to underperform the single
view embeddings of a) for single view classiﬁcation and re-
trieval [12, 6].

In order to address these problem, a new form of em-
beddings is needed. Figure 2 c) shows the behavior desired
for a truly invariant embedding, which should be both sin-
gle view invariant and multiview invariant. We denote this
new form of embedding as pose invariant embedding (PIE).
PIE guarantees two properties: that 1) single view embed-
dings (image descriptors) of an object are clustered around
multiview embedding (shape descriptor) and 2) multiview
embedding is clustered around the descriptor of its labeled
class.

To guarantee the two properties, we return to the prob-
abilistic formulation and introduce an intermediate object
variable O, leading to

PY |X(y|x) = Xn
= Xn

PY |O,X(y|n, x)PO|X(n|x)

PY |O(y|n)PO|X(n|x)

(10)

where we have used the fact that once the object is known
the class is independent of the view. This provides a decom-

12380

++++++++++++position of the posterior probabilities into an object-to-class
PY |O(y|n) and a view-to-object PO|X(n|x) model. This
decomposition can be exploited to enforce the two proper-
ties above. We next discuss how to do this for the various
approaches of Figure 1.

3.3. Pose invariant proxy embedding

We start by extending the proxy embedding [15] of (5)
with the conditional probabilities of (10). We then note that
the multiview form of proxy embedding, given by (9), is
an object-to-class model, if the shape descriptors is pro-
duced by averaging image descriptors associated with the
same object (6). Hence, the object-to-class model can be
identical to the multiview proxy embedding (9)

PY |O(y|n) = sm

y (Xn; P).

(11)

The view-to-object model should be similar to single view
proxy (5) but use a set of object proxies. To encourages
the clustering of Figure 2 c), we propose adopting the shape
descriptor produced by (6) as the proxy for the associated
object. This leads to the model

PO|X(n|x) =

e−d(g(x),gm(Xn))

Pj6=n e−d(g(x),gm(Xj )) .

(12)

Pose invariant proxy (PI-Proxy) embedding can then be de-
rived by combining the two models with conditional prob-
ability (10). The approximated probabilities in [15] is then
used and we have

sinv
y

(x, P) = Pn e−dinv(x,Xn,py)
Pi6=y,n e−dinv(x,Xn,pi) ,

(13)

where

dinv(x, Xn, py) = αd(g(x), gm(Xn)) + βd(gm(Xn), py)
(14)
is denoted as the pose invariant distance. α, β are two hy-
perparmeters that enable control over the contribution of the
two components of the distance. Note that the feature ex-
tractor g is exactly the same as in the MV-CNN, i.e. there is
no additional parameters and no change in the network.

3.4. Properties of pose invariant distance

The pose invariant distance of (14) has several properties
of interest. First, setting α = 0 and β = 1 results in the
distance of the MV-proxy embedding (9), which leads to
Figure 2 b). Second, for α = β = 1, it becomes Figure 2 c)
and follows from the triangle inequality that

dinv(x, Xn, py) = d(g(x), gm(Xn)) + d(gm(Xn), py)
(15)

≥ d(g(x), py),

i.e. the invariant distance is an upper bound on the distance
of the single view proxy. While the α term encourages clus-
tering of individual views around the object (shape descrip-
tor), the β term encourages clustering of objects into object
class. Hence, the PI-Proxy embedding offers a range of so-
lutions between the behaviors of Figure 2 b) and c).

3.5. Generating pose invariant embeddings

The procedure above can be generalized to all ap-
proaches of Figure 1 that use proxies. This is also true
for classiﬁers, where the weights wy of (1) play the role
of proxies . The procedure for producing a pose invariant
model is as follows.

1. use the multiview model as object-to-class model

PY |O(y|n).

2. use the view-based model as view-to-object model

PO|X(n|x).

3. replace the proxies of PO|X(n|x) by the shape descrip-
tors of (6). Use the shape descriptor of object O as
proxy for this object.

4. use the conditional probability (10) to combine the two

models into a pose invariant model.

Applying this procedure to the CNN of (1) leads to the pose
invariant-CNN (PI-CNN)

hinv
y

(x, y; W) = Pn edinv(x,Xn,py)
Pn,j edinv(x,Xn,pj ) ,

(16)

where dinv(x, Xn, py) is deﬁned as in (14). This is iden-
tical to the MV-CNN when (α, β) = (0, 1). For larger α,
the classiﬁer also discriminates between objects in the same
class, assigning each view to the corresponding object de-
scriptor. only assigns views to objects.

Applying the procedure to the triplet center approach,
leads to the pose invariant triplet center (PI-TC) embed-
ding. This combines the multiview triplet center distance
of (7) and the triplet center loss of (8), using shape descrip-
tor as centers, leading to the loss function

L(x, y, P) =

= φ(α(min
k6=n

d(x, Xk) − d(x, Xn))

+ β(min
i6=y

d(Xn, pi) − d(Xn, py))))

(17)

3.6. Learning and inference

The models of (13), (16), and (17) are all functions of the
view and multiview embeddings, g and gm. However, since
view feature extractor g is shared by all views and gm is
the average over view features given by (6), the total num-
ber of parameters is equal to that of a single CNN. In this
aspect, all invariant embeddings of Figure 1 have the same

12381

refer to the dataset as the object pose invariance (ObjectPI)1
dataset.

5. Experiments

In this section, we report on an experimental evaluation
of the methods of Figure 1 on 5 different tasks, covering
classiﬁcation and retrieval at different levels of invariance.

5.1. Experimental setup

Dataset All experiments are based on three datasets.
ModelNet40 [29] is a 3D CAD dataset, of 40 object classes
and 3183 objects . We use the training and testing splits
of [22, 11], with 80 training and 20 test objects. For each
object, 12 views are rendered uniformly (viewpoint interval
30 degree), identical to [22] and case (i) of [12]. Note that
all reported results are for instance accuracy.
MIRO [12] is a dataset of real world objects. Each object
is imaged from 10 elevations and 16 azimuths, to produce
160 images. We use the 16 images of 0o elevation.
ObjectPI is described in Section 4.

Tasks All embeddings are tested on retrieval and classi-
ﬁcation and trained with all object views. Both single and
multi-view inference are considered.
Classiﬁcation: For CNN based methods, class is deter-
mined by the probabilities generated by the network, while
for proxy and triplet center (TC) based methods, a nearest
neighbor classiﬁer is used. Classiﬁcation accuracy is re-
ported. Single view classiﬁcation predicts the class of one
image. Multiview classiﬁcation predicts the class of a set
of object views. For a CNN, this is done by averaging class
probabilities over all views. For proxy and triplet center
methods, a nearest neighbor classiﬁer compares the shape
descriptors extracted from the set of views to the class de-
scriptors obtained from the training set.
Retrieval: Retrieval results are reported in terms of mean
average precision (mAP). Three retrieval tasks are consid-
ered. Single view retrieval aims to retrieve images in the
class of a query view. Object retrieval aims to retrieve
other views of the object in the query view. These methods
compare view descriptors. Multiview retrieval compares
shape descriptors, aiming to retrieve the objects in the same
class of the object used to generate a set of query views.
Implementation All experiments use a VGG16 [20] model
implemented on Pytorch. For MV approaches, view pool-
ing is performed before the softmax function. Learning rate
is 1e-5 and Adam[13] optimizer is used in all experiments.

5.2. Joint classiﬁcation and retrieval

The development of representations for joint classiﬁca-
tion and retrieval has shown to be difﬁcult. Most methods

1All data collected in this work will be made available publicly.

12382

Figure 3. Examples of the 8 viewpoints of ObjectPI, for 2 objects.

complexity. Training boils down to learning the parame-
ters of CNN, using (13), (16), and the logistic loss or (17)
in risk R (2). This is a standard backpropagation learning
problem. For inference, several modes are possible. In the
multiview mode, only the model PY |O(y|X) is used. This
is equivalent to using the multivew methods in the second
row of Figure 1, i.e. MV-CNN, MV-proxy (9), and MV-
triplet center (7). However, these models can still beneﬁt
from invariant training. For pose invariant recognition and
classiﬁcation, the models are those of (13), (16), and (17).
In the case where a single view x is available at inference
time, i.e. on = gm(Xn) is not available, all expressions
can be simpliﬁed. For example, the PI-CNN reduces to
hinv

(x, y) = e−d(x,py )

If partial views are available

y

Pj e−d(x,pj ) .

at inference time, the multiview mode is again used, but (6)
is reformulated as gm(X) = 1
k=1 g(xk), where V ′ is
the number of views available.

V ′ PV ′

4. Pose invariance dataset

Existing multiview object datasets can be grouped in two
classes. The ﬁrst includes synthetic datasets such as Mod-
elNet [29] or ShapeNet [4]. These are large and popular,
but only depict computer graphics rendered objects. The
second includes “turntable datasets”, i.e. datasets imaged in
the lab, by collecting images placed on a turntable, as it is
rotated [2, 8, 16]. These are more realistic, but still lack nat-
ural backgrounds. In this work, we introduce a new dataset
that addresses these limitations. It consists of images col-
lected in the wild, by placing each object in a scene and
taking pictures with a camera, which is moved around the
object. An example of the views collected for an object is
shown in Figure 3. The dataset contains 8 views per object,
for 500 objects from 25 classes. These classes are chosen
from ImageNet, to enable the use of CNNs pre-trained on
the latter. The dataset is split into a training and test set,
containing 16 and 4 objects respectively for each class. We

Task

Proxy MV-Proxy PI-Proxy

Class.

(Acc.) Avg

Single 68.5
Multi 78.8
73.7

Object 47.7
Single 59.7
Multi 76.8
61.4

Retr.

(mAP) Avg

63.2
78.3
70.7

49.3
57.9
74.7
60.6

68.7
80.0
74.4

49.4
62.6
78.2
63.4

Proxy

MV-Proxy

PI-Proxy (α = β = 1)

Figure 4. TSNE visualization of proxy based embeddings on ObjectPI. Each dot is an object view,
objects are identiﬁed by color, and their shape descriptors by ’x’s.

Table 1. Proxy based methods on Ob-
jectPI. α = β = 1 for PI-Proxy.

specialize on one of the tasks, to the point that the papers
do not even present results for the other. For example, [12]
only addresses classiﬁcation, while [11] is mainly designed
for retrieval. The few works that report both classiﬁcation
and retrieval results use additional steps to prop at least one
of the tasks. For example, [6, 22] train an additional low
rank Mahalanobis metric to boost retrieval performance. In
addition, only few methods report single image retrieval and
classiﬁcation result on classiﬁer trained with multiview. It
is simply accepted that view based embeddings have bet-
ter performance for view classiﬁcation and retrieval, while
multiview embeddings are better for multiview classiﬁca-
tion and retrieval. It has so far not been shown that a single
embedding can perform well on both tasks for both single
and multiview.

Visualization: To study this issue in more detail, we
consider the proxy based approaches of Figure 1, namely
Proxy, MV-Proxy, and PI-Proxy. We start by visualizing, in
Figure 4, the embeddings produced by the three approaches,
using TSNE [24]2. To simplify the plots, only 12 classes
and 1 object per class are shown. Objects are identiﬁed
by dots of the same color, which correspond to individual
views. The shape descriptor of (6) is also shown as an ’x’.
The classes and objects used in the visualization were cho-
sen randomly. This plot conﬁrms the predictions of Fig-
ure 2. While all methods succeed at separating the shape
descriptors, the placement of individual views is very dif-
ferent. For Proxy and MV-Proxy, these may be embed-
ded far away from the shape feature. MV-Proxy, which
only optimizes the shape embedding (ignores the placement
of views) produces the most scattered distribution. Proxy
methods have more clustered embeddings, but the cluster-
ing is signiﬁcantly inferior to that of PI-Proxy. In this case,
most views cluster around the shape embedding produce ob-
ject clusters of very small overlap. This is a direct conse-
quence of the use of the pose invariant distance of (14).

Classiﬁcation & Retrieval: Table 1 shows that PI-Proxy
achieves the best performance of the three methods on all
retrieval and classiﬁcation tasks. While this is not surpris-
ing, given the clusterings of Figure 4, the differences can
be quite signiﬁcant, depending on the the task. Note that

2Similar TSNE visualizations of all approaches of Fig 1 can be found

in the supplementary materials.

Figure 5. Classiﬁcation accuracy of proxy based embedding on
ObjectPI as a function of number of views at inference time.

MV-Proxy is particularly poor for single view classiﬁcation.
This is explained by the poor view clustering and is a well
known limitation of multiview methods [6]. Proxy, is com-
petitive with PI-Proxy on image classiﬁcation, but inferior
(2-3% points weaker) on the other tasks.

Robustness to number of views: Although multiview
training improves classiﬁcation accuracy [22], the latter of-
ten decreases dramatically for single view inference [6, 12].
In this setting multiview CNNs frequently underperform a
standard single view classiﬁer. This is unlike the proposed
pose invariant embeddings, as shown in Figure 5.

The PI-Proxy embedding has performance comparable
to that of MV-Proxy for multiple views, but much supe-
rior performance as the number of views decreases. This is
again justiﬁed by the improved view clustering of Figure 4.

5.3. Comparison to the state of the art

We next performed a comparison of all embeddings of
Figure 1 to other methods in the literature, on ModelNet,
MIRO and ObjectPI datasets. Since most previous work
has been done on ModelNet, we used the results on this
dataset as guidance to select some state of the art mod-
els.
It should be said that this is not easy, because the
existing methods vary along many dimensions. This in-
cludes the use of different backbone network architectures
(e.g. VGG-M instead of the more popular VGG16 that we
adopt), architectural enhancements (e.g. view pooling lay-
ers that implement operations different from averaging view
descriptor (6)) and complementary steps (e.g. optimizing
the distance metric used for retrieval after the embedding is
learned). All these variations are orthogonal to the invari-

12383

12345678# of view given at inference time0.6250.6500.6750.7000.7250.7500.7750.8000.825AccuracyPI-ProxyMV-ProxyProxyTable 2. Comparison with state of the art methods on 3 different dataset for 5 different tasks on VGG16. The best result of each task is
marked in bold and shadow denotes that the result of pose invariant based method is better or comparable than that of multiview based.

Method

Classiﬁcation
(Accuracy %)

Retrieval
(mAP %)

ModelNet (12 views)

MIRO (16 views)

Classiﬁcation
(Accuracy %)

Retrieval
(mAP %)

ObjectPI (8 views)

Classiﬁcation
(Accuracy %)

Retrieval
(mAP %)

RN[12]

MV-CNN[22]

PI-CNN

MV-TC[11]

PI-TC

MV-Proxy
PI-Proxy

Single Multi Avg. Object Single Multi Avg.
35.6
80.2
71.0
47.6
70.0
85.4
61.4
77.3
65.7
81.2
62.1
79.7
85.1
68.6

20.2
41.7
77.5
63.5
71.5
66.1
79.9

89.0
87.9
88.0
88.9
88.9
89.6
88.7

22.6
29.6
50.8
36.6
41.4
35.0
40.6

63.9
71.5
81.8
84.0
84.2
85.1
85.1

84.6
79.4
86.7
83.1
85.1
84.7
86.9

Single Multi Avg. Object Single Avg.
33.0
93.2
100
92.0
100
100
100
99.8
100
100
100
99.8
100
100

96.6
100
100
100
100
100
100

33.0
92.0
100
99.8
100
99.8
100

33.0
92.0
100
99.8
100
99.8
100

100
100
100
100
100
100
100

Single Multi Avg. Object Single Multi Avg.
35.7
37.5
62.1
56.2
63.9
66.5
62.9
65.7
67.4
69.3
60.6
63.2
68.7
63.4

25.2
53.8
58.9
59.5
63.8
57.9
62.6

40.1
42.6
60.7
51.8
61.8
49.3
49.4

41.9
72.3
72.1
77.3
76.7
74.7
78.2

50.3
68.1
71.5
72.4
73.2
70.7
74.4

63.2
74.1
76.5
79.2
77.5
78.3
80.0

ance issue studied in this work, and could be applied to any
of the embeddings of Figure 1.

Furthermore, most existing methods only report results
for few, sometimes even only one, of the 5 tasks that we
consider. This allows for the detailed optimization of the
embeddings for these tasks. Such optimization is not fea-
sible under the experimental protocol now proposed, given
the need to compare many embeddings on the 5 tasks and
the goal of identifying embeddings that perform well across
the 5 tasks. We believe that this is a set-up of greater prac-
tical signiﬁcance, which future works in this area should
adopt. Nevertheless, we used existing results to identify
two state of the art models on ModelNet: the RotationNet
(RN) [12] for classiﬁcation and the triplet-center of [11] for
retrieval. The later is what we denote by MV-triplet center
(MV-TC) in Figure 1. For fair comparison, we re-trained
these models under our set-up and tested them on the 5 tasks
and 3 datasets that we now consider. For example, RN is re-
trained with VGG16 instead of AlexNet3. We also present
results for the other existing methods of Figure 1, namely
the MV-CNN [22] and the proxy embedding of [15].

Table 2 summarizes the results of multiview and PIE
based methods on the three datasets. Shadowed cells indi-
cate that the PI-embedding outperforms the MV-embedding
above it. Several conclusions can be drawn. First, pose
invariant embeddings (PIEs) are clearly more robust than
multiview embeddings (MVEs) on both classiﬁcation and
retrieval tasks. Among the 60 results listed in the table, PIEs
outperformed MVEs on 46. In some cases, the difference
was drastic. For example, for single view classiﬁcation on
ModelNet, the PI-CNN achieved 85.4% accuracy, outper-
forming the MVCNN by 14%. Second, one possibility to
compare the performance of the different PIEs is to count
the number of boldfaced entries. These indicate the num-
ber of ”wins,” i.e. how many times the method had equal
or better performance than all others. Under this metric PI-
proxy (12 wins) had slightly better performance, followed
by PI-TC (10 wins), and PI-CNN (9 wins). However, the
difference was not very signiﬁcant. This shows that adding
PIEs increases robustness regardless the approaches being
used in the multiview level. Third, regarding classiﬁca-
tion vs. retrieval, the methods behave somewhat differently.

3Results of AlexNet model provided by [12] are reported in supple-

mentary material.

While PI-Proxy achieved the best classiﬁcation results on
all datasets, PI-CNN had the best retrieval results in Mod-
elNet and PI-TC on ObjectPI. However, the results of the
three PIEs were close in most cases. Again, the most sig-
niﬁcant observation is how this differs from the behavior
of the embeddings in the literature. For example, the Ro-
tationNet(RN) is competitive for classiﬁcation but has very
weak retrieval performance. Fourth, regarding datasets, best
results were obtained on MIRO, then ModelNet, with Ob-
jectPI posing the greatest challenge to most embeddings.
This is not totally surprising, since MIRO and ModelNet
have no backgrounds, MIRO is a relatively small dataset
(120 objects), and ModelNet has no object textures. Nev-
ertheless, these results conﬁrm the need for a more realistic
dataset, such as ObjectPI.

6. Conclusion

This work makes several contributions to the study of
pose invariance for image classiﬁcation and retrieval tasks.
We started by introducing a functional organization of
embeddings to elaborate the relationships between existing
methods. As the taxonomy is organized according to
different level of invariance, some missing approaches are
identiﬁed and existing approaches are further generalized.
A new family of pose invariant embeddings (PIEs) is then
derived from existing methods, by combining a view-to-
object model and a object-to-class model. We show that the
proposed PIEs have mathematically interesting properties
and have good performance for both 1) classiﬁcation and
retrieval, and 2) single and multiview inference. The gen-
eralization of PIEs is important because such embeddings
can be applied to different tasks and circumstances, which
is a more realistic scenario for vision application. Finally,
we introduced a multiview dataset, ObjectPI, with images
of real objects captured with in the wild backgrounds.
We believe that the proposed dataset will complement the
synthetic datasets and contribute to the advancement of
multiview study.

Acknowledgments This work was partially funded by NSF
awards IIS-1546305 and IIS-1637941, a gift from Northrop
Grumman, and NVIDIA GPU donations. We thank Bran-
don Leung, Erik Sandstroem, David Orozco and Yen Chang
for the dataset collection.

12384

References

[1] Sean Bell and Kavita Bala. Learning visual similarity for
product design with convolutional neural networks. ACM
Transactions on Graphics (TOG), 34(4):98, 2015.

[2] A. Borji, S. Izadi, and L. Itti. ilab-20m: A large-scale con-
trolled object dataset to investigate deep learning. In 2016
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2221–2230, June 2016.

[3] Andr´e Brock, Theodore Lim, James M. Ritchie, and Nick
Weston. Generative and discriminative voxel modeling
with convolutional neural networks. CoRR, abs/1608.04236,
2016.

[4] Angel X. Chang, Thomas A. Funkhouser, Leonidas J.
Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio
Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong
Xiao, Li Yi, and Fisher Yu. Shapenet: An information-rich
3d model repository. CoRR, abs/1512.03012, 2015.

[5] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning
a similarity metric discriminatively, with application to face
veriﬁcation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, volume 1, pages 539–
546. IEEE, 2005.

[6] Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and
Yue Gao. Gvcnn: Group-view convolutional neural networks
for 3d shape recognition. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018.

[7] A. Garcia-Garcia, F. Gomez-Donoso, J. Garcia-Rodriguez,
S. Orts-Escolano, M. Cazorla, and J. Azorin-Lopez. Point-
net: A 3d convolutional neural network for real-time object
class recognition. In 2016 International Joint Conference on
Neural Networks (IJCNN), pages 1578–1584, July 2016.

[8] Jan-Mark Geusebroek, Gertjan J Burghouts, and Arnold WM
Smeulders. The amsterdam library of object images. Inter-
national Journal of Computer Vision, 61(1):103–112, 2005.
[9] Jacob Goldberger, Geoffrey E Hinton, Sam T Roweis, and
Ruslan R Salakhutdinov. Neighbourhood components anal-
ysis. In Advances in neural information processing systems,
pages 513–520, 2005.

[10] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensional-
ity reduction by learning an invariant mapping. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognitionon, pages 1735–1742. IEEE, 2006.

[11] Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang
Bai. Triplet-center loss for multi-view 3d object retrieval.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2018.

[12] Asako Kanezaki. Rotationnet: Learning object classiﬁ-
cation using unsupervised viewpoint estimation. CoRR,
abs/1603.06208, 2016.

[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. CoRR, abs/1412.6980, 2014.

[14] D. Maturana and S. Scherer. Voxnet: A 3d convolutional
neural network for real-time object recognition.
In 2015
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pages 922–928, Sept 2015.

[15] Yair Movshovitz-Attias, Alexander Toshev, Thomas K Le-
ung, Sergey Ioffe, and Saurabh Singh. No fuss distance met-

ric learning using proxies. Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2017.

[16] Sameer A Nene, Shree K Nayar, and Hiroshi Murase.

Columbia object image library (coil-100).

[17] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio
Savarese. Deep metric learning via lifted structured fea-
ture embedding.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4004–
4012, 2016.

[18] Charles Ruizhongtai Qi, Hao Su, Matthias Nießner, Angela
Dai, Mengyuan Yan, and Leonidas J. Guibas. Volumetric and
multi-view cnns for object classiﬁcation on 3d data. CoRR,
abs/1604.03265, 2016.

[19] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniﬁed embedding for face recognition and clus-
tering. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 815–823, 2015.

[20] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[21] Kihyuk Sohn.

Improved deep metric learning with multi-
class n-pair loss objective. In Advances in Neural Informa-
tion Processing Systems, pages 1857–1865, 2016.

[22] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik
Learned-Miller. Multi-view convolutional neural networks
for 3d shape recognition.
In Proceedings of the IEEE in-
ternational conference on computer vision, pages 945–953,
2015.

[23] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang.
Deep learning face representation by joint identiﬁcation-
veriﬁcation. In Advances in neural information processing
systems, pages 1988–1996, 2014.

[24] Laurens van der Maaten and Geoffrey Hinton. Visualizing
data using t-SNE. Journal of Machine Learning Research,
9:2579–2605, 2008.

[25] Chu Wang, Marcello Pelillo, and Kaleem Siddiqi. Dominant
set clustering and pooling for multi-view 3d object recogni-
tion. In BMVC, 2017.

[26] Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg,
Jingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learn-
ing ﬁne-grained image similarity with deep ranking. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1386–1393, 2014.

[27] Kilian Q Weinberger, John Blitzer, and Lawrence K Saul.
Distance metric learning for large margin nearest neighbor
classiﬁcation. In Advances in neural information processing
systems, pages 1473–1480, 2006.

[28] Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Free-
man, and Joshua B. Tenenbaum. Learning a probabilistic
latent space of object shapes via 3d generative-adversarial
modeling. CoRR, abs/1610.07584, 2016.

[29] Zhirong Wu, S. Song, A. Khosla, Fisher Yu, Linguang
Zhang, Xiaoou Tang, and J. Xiao. 3d shapenets: A deep
representation for volumetric shapes. In 2015 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 1912–1920, June 2015.

12385

[30] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian.
Foldingnet: Interpretable unsupervised learning on 3d point
clouds. CoRR, abs/1712.07262, 2017.

[31] Haoxuan You, Yifan Feng, Rongrong Ji, and Yue Gao. Pvnet:
A joint convolutional network of point cloud and multi-view
for 3d shape recognition. CoRR, abs/1808.07659, 2018.

12386

