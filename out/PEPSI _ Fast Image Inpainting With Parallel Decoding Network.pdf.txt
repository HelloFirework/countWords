PEPSI : Fast Image Inpainting with Parallel Decoding Network

Min-cheol Sagong1, Yong-goo Shin1, Seung-wook Kim1, Seung Park1, Sung-jea Ko2

Korea university

1{mcsagong, ygshin, swkim, spark}@dali.korea.ac.kr

2

sjko@korea.ac.kr

Abstract

Recently, a generative adversarial network (GAN)-based
method employing the coarse-to-ﬁne network with the con-
textual attention module (CAM) has shown outstanding re-
sults in image inpainting. However, this method requires
numerous computational resources due to its two-stage pro-
cess for feature encoding. To solve this problem, in this pa-
per, we present a novel network structure, called PEPSI:
parallel extended-decoder path for semantic inpainting.
PEPSI can reduce the number of convolution operations by
adopting a structure consisting of a single shared encoding
network and a parallel decoding network with coarse and
inpainting paths. The coarse path produces a preliminary
inpainting result with which the encoding network is trained
to predict features for the CAM. At the same time, the in-
painting path creates a higher-quality inpainting result us-
ing reﬁned features reconstructed by the CAM. PEPSI not
only reduces the number of convolution operation almost
by half as compared to the conventional coarse-to-ﬁne net-
works but also exhibits superior performance to other mod-
els in terms of testing time and qualitative scores.

1. Introduction

Image inpainting techniques have been widely re-
searched [1–3, 5, 6, 10, 13, 16, 17, 19, 21, 25, 26, 28] for re-
moving an unwanted object or synthesizing missing parts
of an image in various applications such as photo edit-
ing, image-based rendering, and computational photogra-
phy [15, 20, 28]. Image inpainting methods can be divided
into two groups [28]. The diffusion-based and patch-based
methods belong to the ﬁrst group. The diffusion-based
method propagates the pixel information from the existing
regions of an image, i.e. background regions, to the missing
regions, i.e. hole regions [2,5,19,28]. This method performs
well on plain textures and small holes but often fails to ﬁll
in the complex hole region such as face and objects with the
non-repetitive structures. In contrast to the diffusion-based
method, the patch-based method samples patches from the

background region and then pastes them into the hole re-
gion [22, 28]. Barnes et al. [1] proposed a fast approxi-
mate nearest neighbor patch search algorithm, called Patch-
Match, which has shown notable results for image editing
applications including image inpainting. PatchMatch, how-
ever, smoothly ﬁlls in the hole region without considering
the visual semantics or the global structure of an image.

The second group is a generation-based method which
applies the deep convolutional neural network (CNN) to
predict structures for the hole regions [13, 16, 24]. Thanks
to a decade of advances in CNNs, image inpainting meth-
ods adopting an encoder-decoder structure have achieved a
signiﬁcant progress [13, 24]. However, these methods of-
ten create an image with artifacts such as a blurry image
and a distorted image. To cope with this problem, Pathak et
al. [21] introduced a method called context encoder adopt-
ing the generative adversarial network (GAN) [7]. In this
method, they utilize a combined loss, the l2 pixel-wise re-
construction loss and adversarial loss, which helps the net-
works to generate a more natural image by minimizing
the difference between the reference and inpainted image.
However, this method has a limitation that it can ﬁll only
square holes at the center of an image.

Iizuka et al. [10] proposed an improved network struc-
ture which can extract features in wider receptive ﬁelds by
employing the dilated convolution layers to complete hole
regions effectively.
In addition, they use two sibling dis-
criminators: global and local discriminators. The local dis-
criminator focuses on the inpainted region to distinguish lo-
cal texture consistency while the global discriminator in-
spects if the result is coherent in a whole image. Yu et
al. [28] have extended this work by using the coarse-to-
ﬁne network and the contextual attention module (CAM).
The CAM learns the relation among background and fore-
ground feature patches by computing the cosine similar-
ity. To collect the background features involved with the
missing region, this method needs the features at the miss-
ing region encoded from roughly completed images. Thus,
they designed two-stage coarse-to-ﬁne networks to produce
an intermediate result of a roughly restored image. This

111360

Figure 1. An architecture of PEPSI. The coarse path and inpainting path share their weights to improve each other. The coarse path is
trained only with the ℓ1 reconstruction loss while the inpaiting path is trained with both of ℓ1 and adversarial loss

method shows a remarkable performance as compared with
recent state-of-the-art inpainting methods; however, it re-
quires considerable computational resources due to use of
the two-stage network structure.

In this paper, we propose a novel parallel network called
PEPSI: parallel extended-decoder path for semantic inpaint-
ing, which has the small number of operation by employing
a single-stage encoder-decoder network to solve this prob-
lem. As shown in Figure 1, PEPSI extracts features via a
single encoding network and generates a high-quality in-
painted result via a single decoding network. To make a
single shared encoding network handle two different tasks,
feature generation both for a roughly completed result and
for a high-quality result, we propose a joint learning method
using a parallel decoding network which has coarse and in-
painting paths. The coarse path produces a roughly com-
pleted result with which the encoding network is trained to
predict features for the CAM. At the same time, the inpaint-
ing path creates a higher-quality inpainting result using the
reﬁned features reconstructed by the CAM. We also mod-
ify the CAM to use Euclidean distance instead of the cosine
similarity to learn the relationship of patches more suitably.
We conduct extensive experiments to demonstrate that
our method outperforms conventional methods on various
dataset such as Celeb-a [11, 18], place2 [30], and Image-
net [14]. We use both of the random square mask and free-
form mask mimicking human brushings. The experimen-
tal results indicate that the proposed method not only ex-
hibits superior performance compared to the conventional
ones but also signiﬁcantly reduces the computational time.

In summary, in this paper we present:

• A novel generative network that improves the inpaint-
ing performance while reducing the number of compu-
tational resources by unifying cascade network of the
coarse-to-ﬁne network and modifying the CAM.

• A novel discriminator distinguishing image regions

separately which is more suitable in real user appli-
cations.

2. Preliminaries

2.1. Generative adversarial networks

The GAN was ﬁrst introduced by Goodfellow et al. [7]
for the image generation. In the GAN, two networks are
simultaneously trained: a generative network, G, is trained
to create a new image which is indistinguishable from real
images, whereas a discriminative network, D is trained to
differentiate between real and generated images. This re-
lation can be considered as a two-player min-max game in
which G and D compete. To this end, the G (D) tries to
minimize (maximize) the loss function, i.e. adversarial loss,
as follows:

min

G

max

Ex∼Pdata(x)[log D(x)]

D
+ Ez∼Pz(z) [log(1 − D(G(z)))],

(1)

where z and x denote a random noise vector and a real im-
age sampled from the noise Pz(z) and real data distribution
Pdata(x), respectively. Recently, the GAN have been ap-
plied to several semantic inpainting techniques [10, 21, 28]
in order to complete the hole region naturally.

2.2. Coarse to ﬁne network

Yu et al. [27, 28] proposed a novel image inpainting
framework consisting of two networks: the coarse network
and the reﬁnement network. This two-stage network, called
a coarse-to-ﬁne network, performs a couple of tasks sepa-
rately. First, it produces an initial coarse prediction with the
coarse network, and reﬁnes the results by extracting fea-
tures from the roughly ﬁlled prediction with the reﬁnement
network. To produce a higher-quality image inpainting with
the generative network, the network should understand the

11361

Figure 2. The illustration of the CAM. The conventional CAM re-
constructs foreground patches by measuring the cosine similarities
with background patches. In contrast, the modiﬁed CAM uses the
Euclidean distance to compute similarity scores.

relation between background and hole regions. The reﬁne-
ment network learns the relation by using the CAM, which
computes the cosine similarity between those regions. As
shown in Figure 2, the CAM ﬁrst divides features into a
target foreground and its surrounding background and ex-
tracts 3 × 3 patches. Then, the similarity score s(x,y),(x′,y′)
between the foreground patch at (x, y), fx,y, and the back-
ground patch at (x′, y′), bx′,y′ , can be obtained as

s(x,y),(x′,y′) =(cid:28) fx,y

kfx,yk

,

bx′,y′

kbx′,y′ k(cid:29) ,

s∗
(x,y),(x′,y′) = softmax(λs(x,y),(x′,y′)),

(2)

(3)

where λ is a hyper-parameter for scaled softmax. By using
(s∗
(x,y),(x′,y′)) as weights, the CAM reconstructs features
of foreground regions by a weighted sum of background
patches to learn the relation between them. The coarse-to-
ﬁne network shows outstanding a performance among state-
of-the-arts image inpainting techniques.

3. Proposed Method

As described in Section 2.2, the CAM learns where to
borrow or copy the feature information from known back-
ground feature patches to generate missing feature patches
by computing the between-patch similarity. Thus, it is nec-
essary to extract features from a roughly completed image,
i.e., the coarse result. The reﬁnement network without the
coarse result shows worse results than the full coarse-to-
ﬁne network as shown in Table 1 and Figure 3 (these re-
sults were obtained by training the reﬁnement network us-
ing raw masked images as an input). This means that, if
the coarse feature of the hole region is not encoded well,
the CAM produces the missing features using unrelated
feature patches, yielding contaminated results as shown in
Figure 3(d).
In other words, the coarse-to-ﬁne network
must pass through a two-stage encoder-decoder network
which needs massive computational complexity, especially
on high-resolution images.

Figure 3. The toy example about coarse network. (a) The ground
truth (b) The masked input image (c) The result from the coarse-
to-ﬁne network (d) The result without the coarse result.

GatedConv [27]

GatedConv ∗

PSNR

24.67
23.50

Square mask

SSIM PSNR

Free-form mask
SSIM

Time

0.8949
0.8822

27.78
16.35

0.9252
0.9098

21.39ms
14.28ms

Table 1. The toy example about coarse network. * means a model
without coarse results.

3.1. Architecture of PEPSI

As shown in Figure 1, our proposed network, PEPSI,
uniﬁes the two-stage cascade network of the coarse-to-ﬁne
network into a single-stage encoder-decoder network. The
PEPSI consists of a single shared encoding network and a
parallel decoding network which has coarse and inpainting
paths. The encoding network is jointly learned for extract-
ing features from input images with hole regions as well
as completing the missing features without the coarse re-
sult. As listed in Table 2, our feature encoding network is
composed of a series of 3 × 3 convolution layers. In this
network, we use a 5 × 5 kernel in the ﬁrst convolution layer
to fully exploit the latent information in the input image.
In addition, we employ the dilated convolution layers with
different dilation rate in the last four convolution layers to
extract the features with large receptive ﬁelds.

Table 3 shows a detailed architecture of the decoding
network. In the proposed method, a parallel decoding net-
work consists of two sibling paths: coarse and inpainting
paths. The coarse path attempts to produce a roughly com-
pleted result from the encoded feature map. On the other
hand, taking the encoded features as an input, the inpainting
path ﬁrst reconstructs the feature map by using the CAM.
Then, the reconstructed feature map is decoded to gener-
ate a higher-quality inpainting result. By sharing the weight
parameters of the two paths, we attempt to regularize the in-
painting path of the decoding network. Moreover, two dif-
ferent paths employ the same encoded feature map as their
input, and thus they compel the single encoder to generate
valuable features for two different image generation tasks.
Note that we employ only the inpainting path during tests,
which substantially reduces the computational resources. In
the proposed method, the coarse path is trained with the re-
construction L1 loss explicitly, whereas the inpainting path
is trained with the L1 loss as well as the GAN losses. More
detailed information will be described in Section 3.4.

Similar to traditional image inpainting networks [10, 21,

11362

Type

Kernel

Dilation

Stride

Outputs

Convolution
Convolution
Convolution
Convolution
Convolution
Convolution

Dialated convolution
Dialated convolution
Dialated convolution
Dialated convolution

5 × 5
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3

1
1
1
1
1
1
2
4
8
16

1 × 1
2 × 2
1 × 1
2 × 2
1 × 1
2 × 2
1 × 1
1 × 1
1 × 1
1 × 1

32
64
64
128
128
256
256
256
256
256

Table 2. Detail architecture of encoding network.

Type

Kernel

Dilation

Convolution ×2

Nearest Neighbor (×2 ↑)

Convolution ×2

Nearest Neighbor (×2 ↑)

Convolution ×2

Nearest Neighbor (×2 ↑)

Convolution ×2

Convolution (Output)

3 × 3

-

3 × 3

-

3 × 3

-

3 × 3
3 × 3

1
-
1
-
1
-
1
1

Stride

1 × 1

-

1 × 1

-

1 × 1

-

1 × 1
1 × 1

Outputs

128

-
64
-
32
-
16
3

Table 3. Detail architecture of decoding network. The output layer
consists of a convolution layer clipped value to the [-1, 1].

27, 28], the proposed network takes a masked image and
a binary mask indicating the background regions as input
pairs. The masked image includes holes with the variable
numbers, sizes, shapes, and locations randomly sampled
during every iteration. In terms of layer implementations,
we use mirror padding for all convolution layers and em-
ploy the exponential liner unit (ELU) [4] as an activation
function instead of ReLU except the last layer. Also, we
utilize [−1, 1] normalized image with 256 × 256 pixels as
an input image of the network, and generate an output im-
age with the same resolution by clipping the output values
into [−1, 1] instead of using tanh functions.

3.2. Modiﬁed CAM

The conventional CAM [28] measures similarity scores
by applying the cosine similarity. However, normalizing the
feature patch vector in (2) can distort the semantic feature
representation. Thus, we propose a modiﬁed CAM which
directly measures distance similarity scores (d(x,y),(x′,y′))
using the Euclidean distance. It is more suitable for a re-
construction because the Euclidean distance considers not
only an angle between two vectors of feature patches but
also magnitudes of them. Since the distance similarity
scores are hard to be applied softmax having the output
range of [0, ∞), we deﬁne the truncated distance similar-

ity scores, ed(x,y),(x′,y′), as
ed(x,y),(x′,y′) = tanh (−(

d(x,y),(x′,y′) − m(d(x,y),(x′,y′))

σ(d(x,y),(x′,y′))

where

d(x,y),(x′,y′) = kfx,y − bx′,y′ k.

)),

(4)

(5)

Figure 4. A comparison of the image reconstruction between the
cosine similarity and the truncated distance similarity: (a) The
original image, (b) masked image, (c) image reconstructed by us-
ing the cosine similarity and (d) image reconstructed by using the
truncated distance similarity.

Cosine similarity
Euclidean distance

PSNR

25.16
25.57

square mask

SSIM PSNR

free-form mask
SSIM

0.8950
0.9007

27.95
28.59

0.9218
0.9293

Table 4. Comparison of the performance between the cosine simi-
larity and the Euclidean distance applying on the PEPSI.

In (4), the truncated distance similarity score has limited
values within [−1, 1].
It operates like a threshold which
sorts out the distance score less than the mean value be-
cause tanh function changes rapidly across zero. It means
that the truncated distance similarity score helps to divide
background patches into two groups which are related to
the foreground patch and not. We perform toy examples
comparing the cosine similarity and the truncated distance
similarity. We reconstruct the hole region by the weighted
sum of existing image patches where the weights are ob-
tained by using the cosine similarity scores or the truncated
distance similarity scores. As can be seen in Figure 4, re-
construction applying the truncated distance similarity can
collect more similar patches than the cosine similarity. Fur-
thermore, we evaluate the results between PEPSI with con-
ventional and modiﬁed CAMs to conﬁrm the improvement
of the modiﬁed CAM. As shown in Table 4, the modiﬁed
CAM increases the performance as compared to the conven-
tional CAM, which means that the modiﬁed CAM is more
suitable to express the relationship between background and
hole regions.

Similar to the conventional CAM, modiﬁed one also
weigh them with scaled softmax and reconstruct the fore-
ground patch by a weighted sum of background patches at
last. Consequently, it supports the module to reconstruct
foreground patches from a related patch vector group.

3.3. Region Ensemble Discriminator(RED)

PEPSI is learned based on the GAN, which consists of
In [27, 28], the conven-
the generator and discriminator.
tional global and local discriminators aim at not only coher-
ence in a whole image but also the local texture of hole re-
gion. However, the local discriminator can handle only the
hole region with the ﬁxed size of the square shape, while
holes can be appeared with arbitrary locations, shapes, and
sizes in real applications. Thus, it is hard to employ the

11363

Figure 5. The overview of the RED. The RED aims to classify hole
regions which may appear any region with any sizes in an image.

local discriminator to train the inpainting network for the
irregular hole. To cope with this problem, we unify global
and local discriminators into the region ensemble discrimi-
nator (RED), which is inspired by the region ensemble net-
work [8] detecting a target object that appears anywhere in
images by handling multiple feature regions individually.
As depicted in Figure 5, the RED divides the feature of the
last layer as a pixel-wise block and differentiates each fea-
ture is real or fake individually by fully-connected layers.
Since the RED tries to classify each feature block which
has different receptive ﬁelds separately, it assumes different
image regions are real or fake individually. In contrast to the
local discriminator, the RED can handle the various hole re-
gions that may appear anywhere in images of any sizes. A
detailed architecture of the RED is listed in Table 5.

3.4. Loss function

To train PEPSI, we jointly optimize two different paths:
the inpainting path and the coarse path. For the inpainting
path, we adopt the GAN [7] optimization framework in (1),
which is described in Section 2.1.
It is well known that,
in the original GAN [7], the gradient of the generator can
easily disappear, which yields unsatisfactory results of gen-
erated images. To solve this problem, motivated by [29],
we employ the adversarial loss functions of the generator
and the discriminator using the hinge loss and the spectral
normalization, which are deﬁned as

LG = −Ex∼PXi

[D(x)],

(6)

Type

Kernel

Stride

Outputs

Convolution
Convolution
Convolution
Convolution
Convolution
Convolution

FC

5 × 5
5 × 5
5 × 5
5 × 5
5 × 5
5 × 5
1 × 1

2 × 2
2 × 2
2 × 2
2 × 2
2 × 2
2 × 2
1 × 1

64
128
256
256
256
512

1

Table 5. Detailed architecture of RED. After each convolution
layer, except last one, there is a leaky-ReLU as the activation func-
tion. Every layer is normalized by a spectral normalization. The
fully-connected layer is applied to every pixel-wise feature block.

i

where X (n)
and Y (n) are the nth image pair of the gen-
erated image via the inpainting path and its corresponding
original input image in a mini-batch, respectively, N is the
number of image pairs in a mini-batch, and λi and λadv are
hyper-parameters to balance between two loss terms. As
mentioned in Section 3.3, we respectively average the ad-
versarial losses of each feature elements in the last layer of
a discriminator in (7).

The role of the coarse path loss is to complete the miss-
ing features properly for the CAM. Thus, we optimize the
following simple l1 loss function deﬁned as

LC =

1
N

NXn=1

kX (n)

c − Y (n)k1,

(9)

c

where X (n)
and Y (n) are the nth image pair of the gener-
ated image via the coarse path and its corresponding orig-
inal input image in a mini-batch, respectively. Finally, we
deﬁne the total loss function of the generative network of
PEPSI as follows:

Ltotal = LG + λc(1 −

k

kmax

)LC ,

(10)

where λc is a hyper-parameter controlling the contributions
from each loss term, and k and kmax represent the iteration
of the learning procedure and the maximum number of it-
erations, respectively. In the proposed method, as the train-
ing progresses, we slightly reduce the weights of the coarse
path loss for the decoding network to focus on the image
reconstruction process.

LD = Ex∼PY [min(0, −1 + D(x))]
[min(0, −1 − D(x))],

−Ex∼PXi

(7)

4. Experiments

where PXi and PY denote the data distributions of inpaint-
ing results and input images. Since goal of image inpainting
is not only to generate the natural hole ﬁlling but also to re-
store the missing part of the original image accurately, we
add a strong constraint using ℓ1 norm to (6) as follows:

LG =

λi
N

NXn=1

kX (n)

i − Y (n)k1 − λadvEx∼PXi

[D(x)],

(8)

4.1. Implementation details

Free-Form Mask As shown in Figure 7(b), traditional
methods [10, 21, 28] usually adopt the regular mask (e.g.
hole region with rectangular shape) during the training pro-
cedure. Thus, the network trained with regular mask of-
ten yields visual artifacts such as color discrepancy and
blurriness when the hole region has irregular shapes. To
cope with this problem, Yu et al. [27] adopt the free-form

11364

Figure 6. Comparison of our method and conventional methods on randomly square masked CelebA-HQ datasets. (a) The ground truth
(b) The input image of the network (c) Results of the Context Encoder [21] (d) Results of the Globally-Locally [10] (e) Results of the gated
convolution [27] (f) Results of the proposed method

Figure 7. Examples of (a) the original image, (b) its square masked
image, and (c) the free-form masked image

mask algorithm during the training procedure, which auto-
matically generates multiple random free-form holes as de-
picted in Figure 7(c). In particular, this algorithm produces
the free-form mask by drawing multiple different lines and
erasing pixels closer than an arbitrary distance from these
lines. For a fair comparison, we adopt the same free-form
mask generation algorithm to our method.

Training Procedure PEPSI is trained for one million it-
erations using a batch size of 8 in an end-to-end manner.
We perform optimization using the ADAM optimizer [12],
which is a stochastic optimization method with adaptive es-
timation of moments. The parameters of Adam optimizer,
β1 and β2, are set to 0.5 and 0.9, respectively.
Inspired
by [9], we employ two-timescale update rule (TTUR) where
the learning rates of the discriminator and generator are
0.0004 and 0.0001, respectively. In addition, we reduce the
learning rate as 1/10 after 0.9 million iterations. The hyper-
parameters in our model are set to λi = 10, λc = 5, and
λadv = 0.1. Our experiments were conducted on CPU In-
tel(R) Xeon(R) CPU E3-1245 v5 and GPU TITAN X (Pas-
cal), and implemented in TensorFlow v1.8.

4.2. Performance Evaluation

For our experiments, we use the CelebA-HQ [11, 18],
ImageNet [14], and Place2 [30] datasets which consist of
human faces, things, and various scenes, respectively.
In
the CelebA-HQ dataset, we randomly sample the 27,000
images as a training set and 3,000 ones as a test set. We
also train the network with all the images in the ImageNet
dataset and test it on Place2 dataset to measures the perfor-
mance of trained deep learning models on other datasets to
conﬁrm the generalization ability of PEPSI. In addition, to
demonstrate the superiority of PEPSI, we compare its qual-
itative, quantitative, and operation speed results with those
of the conventional generative methods: CE [21], GL [10],
GCA [28], and GatedConv [27].
Qualitative Comparison We compare the qualitative
performance of PEPSI with the conventional methods us-
ing the image masked with the free-form mask as well as
that with the squared mask. The conventional methods are
implemented by following the training procedure in each
paper. As shown in Figures 6 and 8, CE [21] and GL [10]
show obvious visual artifacts including blurred or distorted
images in the masked region, especially on the free-form
mask. Although GatedConv [27] shows a ﬁne performance,
it shows lack of relevance between hole and background re-
gions such as symmetry of eyes. In contrast to the conven-
tional methods, PEPSI shows visually pleasing results and
high relevance between hole and background regions.

Moreover, we show the real application of PEPSI by
testing on the challenging datasets, ImageNet and Place2
datasets. We compare PEPSI with GatedConv and the
widely available non-generative method, PatchMatch [1],

11365

Method

CE [21]
GL [10]
GCA [28]

GatedConv [27]

GatedConv ∗
PEPSI(Ours)

PEPSI ∗

Square mask
PSNR

SSIM

Free-form mask
PSNR

Time (ms)

SSIM

Local Global

Local Global

17.7
19.4
19.0
18.7
17.5
19.5
19.2

23.7
25.0
24.9
24.7
23.5
25.6
25.2

0.872
0.896
0.898
0.895
0.882
0.901
0.894

9.7
15.1
12.4
21.2
19.8
22.0
21.6

16.3
21.5
18.9
27.8
26.4
28.6
28.2

0.794
0.843
0.798
0.925
0.910
0.929
0.923

5.8
39.4
22.5
21.4
14.3

9.2

Table 6. Results of global and local PSNR, SSIM and operation time with both of square and free-formed masks on CelebA-HQ dataset.
* means a model without coarse results.

Figure 8. Comparison of our method and conventional methods on free-form masked CelebA-HQ datasets. (a) The ground truth (b) The in-
put image of the network (c)Results of the Context Encoder [21] (d) Results of the Globally-Locally [10] (e) Results of the GatedConv [27]
(f) Results of the proposed method

on the Place2 dataset with 256 × 256 image resolution.
As depicted in Figure 9, PatchMatch shows visually poor
performance especially on the edge of images because it
ﬁlls the hole region without understands of the contexts of
scenes. GatedConv generates more realistic results with-
out color discrepancy or edge distortion but still produces
wrong textures. In contrary, PEPSI generates the most natu-
ral images without artifacts or distortion on various contents
and complex scenes for real applications.

Quantitative Comparison We evaluate a performance
of the proposed and conventional methods by measuring
the peak signal-to-noise ratio (PSNR) of the local and
global regions, i.e. the hole region and the whole image,
and the structural similarity (SSIM) [23]. Table 6 pro-
vides the comprehensive performance benchmarks between
PEPSI and conventional ones [10, 21, 27, 28] on CelebA-
HQ datasets [11]. As shown in Table 6, CE [21], GL [10],
and [28] effectively ﬁll the hole region with a square shape,

but they could not complete the hole region with an irregu-
lar shape. Since these methods mainly focus on ﬁlling the
holes with a rectangular shape, they could not generalize
well on the free-form masks. Note that GL [10] shows a
competitive PSNR value with the PEPSI only in the local
region of the square mask since it applies a image blend-
ing technique as the post-processing. However, this post-
processing yields blurred results as shown in Figure 6(d)
and needs more computation time. GatedConv [27] shows
ﬁne performance on both of square and free-form holes, but
also needs much computation time. Contrary to the conven-
tional methods, PEPSI can complete any shape of the hole
region, while reducing the operation time signiﬁcantly.

For further study, we conduct an experiment in which the
models, GatedConv and PEPSI, are trained without using
the coarse results, i.e. GatedConv without using the coarse
network and PEPSI without using the coarse path learn-
ing. As shown in Table 6 (models without coarse results

11366

Figure 9. Comparison of our method and conventional methods on Place2 datasets. (a) The ground truth (b) The input image of the network
(c) Results of the non-generative method, PatchMatch (d) Results of the GatedConv [27] (e) Results of the proposed method

Mask

Method

PSNR

Local

Global

Square

Free-form

GatedConv [27]

PEPSI(Ours)

GatedConv [27]

PEPSI(Ours)

14.2
15.2
17.4
18.2

20.3
21.2
24.0
24.8

SSIM

0.818
0.832
0.875
0.882

Table 7. Results of global and local PSNR and SSIM on the
Places2 dataset.

are denoted by *), even without the coarse results, PEPSI
shows better results compared to the full model of Gat-
edConv thanks to the modiﬁed CAM and RED. With the
coarse path learning, PEPSI exhibits the better than PEPSI
without using coarse results in terms of all the Quantitative
metrics, which indicates that the coarse path drives the en-
coding network to produce missing features properly for the
CAM. In other words, the single-stage network structure of
PEPSI can overcome the limitation of the two-stage coarse-
to-ﬁne network through a parallel learning scheme.

To demonstrate the generalization ability of PEPSI, we
conduct another experiment using the challenging datasets,
ImageNet [14], and Place2 [30] datasets. Table 7 shows the
experimental results of the test using the input image with
the resolution of 256 × 256. We compare the performance

of PEPSI with GatedConv [27], which exhibits superior
performance compared to other conventional methods in
Celeb-A dataset. As shown in Table 7, PEPSI achieves bet-
ter performance than GatedConv on Place2 dataset, which
indicates that the PEPSI can consistently generate the high-
quality results from various contents and complex scenes
either.

5. Conclusion

In this paper, a novel image inpainting method, called
PEPSI, has been proposed. As shown in the experimen-
tal results, the proposed method not only achieves supe-
rior performance as compared to conventional ones, but
also signiﬁcantly reduces the operation time by redesign-
ing unifying two-stage coarse-to-ﬁne network into an efﬁ-
cient single-stage network structure and adopting an effec-
tive joint learning scheme for training the proposed archi-
tecture. Therefore, it is expected that PEPSI can be widely
employed in various applications including image genera-
tion, style transfer, and image editing. Further improve-
ments can be achieved by reducing the parameters of the
network, which helps to be applied to restricted hardware
systems.

11367

References

[1] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Gold-
man. Patchmatch: A randomized correspondence algorithm
for structural image editing. ACM Transactions on Graphics
(ToG), 28(3):24, 2009. 1, 6

[2] M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester. Image
inpainting. In Proceedings of the 27th annual conference on
Computer graphics and interactive techniques, pages 417–
424. ACM Press/Addison-Wesley Publishing Co., 2000. 1

[3] N. Cai, Z. Su, Z. Lin, H. Wang, Z. Yang, and B. W.-K.
Ling. Blind inpainting using the fully convolutional neural
network. The Visual Computer, 33(2):249–261, 2017. 1

[4] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and
accurate deep network learning by exponential linear units
(elus). arXiv preprint arXiv:1511.07289, 2015. 4

[5] A. A. Efros and W. T. Freeman.

Image quilting for tex-
ture synthesis and transfer. In Proceedings of the 28th an-
nual conference on Computer graphics and interactive tech-
niques, pages 341–346. ACM, 2001. 1

[6] A. Fawzi, H. Samulowitz, D. Turaga, and P. Frossard. Image
inpainting through neural networks hallucinations.
In Im-
age, Video, and Multidimensional Signal Processing Work-
shop (IVMSP), 2016 IEEE 12th, pages 1–5. Ieee, 2016. 1

[7] I. Goodfellow,

J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014. 1, 2, 5

[8] H. Guo, G. Wang, X. Chen, C. Zhang, F. Qiao, and H. Yang.
Region ensemble network:
Improving convolutional net-
work for hand pose estimation. In Image Processing (ICIP),
2017 IEEE International Conference on, pages 4512–4516.
IEEE, 2017. 5

[9] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter. Gans trained by a two time-scale update rule
converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pages 6626–6637, 2017. 6

[10] S. Iizuka, E. Simo-Serra, and H. Ishikawa. Globally and
locally consistent image completion. ACM Transactions on
Graphics (TOG), 36(4):107, 2017. 1, 2, 3, 5, 6, 7

[11] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
arXiv preprint arXiv:1710.10196, 2017. 2, 6, 7

[12] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014. 6

[13] R. K¨ohler, C. Schuler, B. Sch¨olkopf, and S. Harmeling.
Mask-speciﬁc inpainting with deep neural networks. In Ger-
man Conference on Pattern Recognition, pages 523–534.
Springer, 2014. 1

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012. 2, 6, 8

[15] A. Levin, A. Zomet, S. Peleg, and Y. Weiss. Seamless image
stitching in the gradient domain. In European Conference on
Computer Vision, pages 377–389. Springer, 2004. 1

[16] C. Li and M. Wand. Combining markov random ﬁelds and
convolutional neural networks for image synthesis. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2479–2486, 2016. 1

[17] H. Li, G. Li, L. Lin, and Y. Yu. Context-aware semantic

inpainting. arXiv preprint arXiv:1712.07778, 2017. 1

[18] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face at-
tributes in the wild. In Proceedings of the IEEE International
Conference on Computer Vision, pages 3730–3738, 2015. 2,
6

[19] H. Noori, S. Saryazdi, and H. Nezamabadi-Pour. A convolu-
tion based image inpainting. In 1st International Conference
on Communication and Engineering, 2010. 1

[20] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C.
Berg. Transformation-grounded image generation network
for novel 3d view synthesis. In 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages
702–711. IEEE, 2017. 1

[21] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.
Efros. Context encoders: Feature learning by inpainting. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2536–2544, 2016. 1, 2, 3,
5, 6, 7

[22] D. Simakov, Y. Caspi, E. Shechtman, and M. Irani. Summa-
rizing visual data using bidirectional similarity. In Computer
Vision and Pattern Recognition, 2008. CVPR 2008. IEEE
Conference on, pages 1–8. IEEE, 2008. 1

[23] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-
from error visibility to
IEEE transactions on image process-

Image quality assessment:

celli.
structural similarity.
ing, 13(4):600–612, 2004. 7

[24] L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convolutional neural
network for image deconvolution.
In Advances in Neural
Information Processing Systems, pages 1790–1798, 2014. 1
[25] C. Yang, X. Lu, Z. Lin, E. Shechtman, O. Wang, and H. Li.
High-resolution image inpainting using multi-scale neural
patch synthesis. In The IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), volume 1, page 3,
2017. 1

[26] R. A. Yeh, C. Chen, T.-Y. Lim, A. G. Schwing,
M. Hasegawa-Johnson, and M. N. Do. Semantic image in-
painting with deep generative models. In CVPR, volume 2,
page 4, 2017. 1

[27] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang.
Free-form image inpainting with gated convolution. arXiv
preprint arXiv:1806.03589, 2018. 2, 3, 4, 5, 6, 7, 8

[28] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang.
Generative image inpainting with contextual attention. arXiv
preprint, 2018. 1, 2, 3, 4, 5, 6, 7

[29] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-
attention generative adversarial networks. arXiv preprint
arXiv:1805.08318, 2018. 5

[30] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba.
Places: A 10 million image database for scene recognition.
IEEE transactions on pattern analysis and machine intelli-
gence, 40(6):1452–1464, 2018. 2, 6, 8

11368

