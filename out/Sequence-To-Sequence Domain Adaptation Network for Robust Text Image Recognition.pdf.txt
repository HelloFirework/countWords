Sequence-to-Sequence Domain Adaptation Network

for Robust Text Image Recognition

Yaping Zhang1

,

2, Shuai Nie1, Wenju Liu1 ∗, Xing Xu3

,

5, Dongxiang Zhang4

,

5, Heng Tao Shen3

1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CASIA)

2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences (UCAS)

3Center for Future Media & School of Computer Science and Engineering, University of Electronic Science and Technology of China

4College of Computer Science and Technology, Zhejiang University, China

5Afanti AI Lab, China

{yaping.zhang, shuai.nie, lwj}@nlpr.ia.ac.cn, xing.xu@uestc.edu.cn, zhangdongxiang37@gmail.com, shenhengtao@hotmail.com

Abstract

Domain adaptation has shown promising advances for
alleviating domain shift problem. However, recent visual
domain adaptation works usually focus on non-sequential
object recognition with a global coarse alignment, which
is inadequate to transfer effective knowledge for sequence-
like text images with variable-length ﬁne-grained charac-
ter information. In this paper, we develop a Sequence-to-
Sequence Domain Adaptation Network (SSDAN) for robust
text image recognition, which could exploit unsupervised
sequence data by an attention-based sequence encoder-
decoder network. In the SSDAN, a gated attention similar-
ity (GAS) unit is introduced to adaptively focus on aligning
the distribution of the source and target sequence data in an
attended character-level feature space rather than a global
coarse alignment. Extensive text recognition experiments
show the SSDAN could efﬁciently transfer sequence knowl-
edge and validate the promising power of the proposed
model towards real world applications in various recogni-
tion scenarios, including the natural scene text, handwritten
text and even mathematical expression recognition.

1. Introduction

Deep learning methods have achieved remarkable results
on text image reading [3, 5, 7, 13, 21, 23, 31]. However, it
remains challenging to build a robust text recognizer that
can handle varying data in new scenarios effectively, due to
the inevitable domain shift when the actual data is encoun-
tered at “test time”. As shown in Figure 1, the text data dis-
tribution tends to be changed by multiple factors, such as,
the different appearances in natural scene texts [21], var-
ious handwriting styles in handwritten texts [3], and even

∗Corresponding author.

Figure 1. Examples of different types of domain shift in text image
recognition scenarios.

diverse structures in mathematical expressions [7]. To build
a robust text recognizer for the shifted target text image, a
general solution is to collect large scale annotated text im-
ages, while they are high-cost and cannot cover all diver-
sities. However, unsupervised target text images are easily
available. If we could take advantage of the unsupervised
text images to reduce domain shift, it will be helpful.

Unsupervised domain adaptation is an effective way us-
ing the unlabeled target domain data to mitigate the do-
main shift, which is to align the feature distribution be-
tween the source and target domain. Recent research en-
deavors [30, 38] on domain adaptation have shown the po-
tential results on character recognition. They generally op-
timize the global representation of a character to minimize
some measure of domain shift, such as maximum mean dis-
crepancy (MMD) [24, 38], correlation alignment distance
(CORAL) [35, 41], or adversarial loss [9, 30, 36], where
feature dimensions are ﬁxed in the source and target do-
main. However, a text image is the combination of differ-
ent characters, which is a variable-length label sequence in-
stead of an isolation. Consequently, the most popular do-
main adaptation methods cannot be directly applied to the
sequence prediction, since a global ﬁxed-length represen-
tation lacks important ﬁne-grained information at the char-
acter level, which in turn cannot appropriately describe the
content of sequence-like images.

In this paper, to address the aforementioned issues, we

2740

Scene textHandwritten textMathematicalexpression textTraining (Source)Test (Target)Domain shift(Type)Synthetic/RealisticWriting styleStructure diversityFigure 2. The structure of SSDAN consists of: a CNN encoder to map the input images into a sequence of high-level feature vectors
, an attention unit between the encoder and decoder to adaptively focus on the location of character, a GRU decoder to convert encoded
features into output strings recurrently, and a GAS unit to offer the guidance for model to adaptively ﬁnd character-level domain-invariant
features between the source and target domain. Overall, the unsupervised sequence-to-sequence domain adaptation is achieved by jointly
minimizing character-level similarity loss Lattn and source decoding loss Ldec.

develop a Sequence-to-Sequence Domain Adaptation Net-
work (SSDAN) for robust text image recognition. As shown
in Figure 2, the proposed SSDAN is an attention based
encoder-decoder model for handling sequences, which is
derived from [7, 21].
It could automatically concentrate
on the most relevant region of the character while decod-
ing, which frees a sequence-like text image from having
to squash all the information of a source sequence into a
global ﬁxed-length vector. Furthermore, a gated attention
similarity (GAS) unit is introduced to align distributions of
the source and target domain at an attended character-level
feature space, where we adopt a gate function to control the
model focusing on effective character-level features, instead
of global coarse alignment. In GAS unit, an unsupervised
character-level similarity loss is used to guide the model
to reduce the domain shift between the source and target
sequence. The unsupervised sequence-to-sequence domain
adaptation is then achieved by jointly minimizing unsuper-
vised character-level similarity loss and supervised source
decoding loss, which could learn both domain-invariant and
discriminative features that are effective for the shifted tar-
get domain.

We summarize our contributions as follows:

• We propose a novel Sequence-to-sequence Domain
Adaptation Network dubbed SSDAN for robust text
image recognition, which could be generalized to dif-
ferent scenes, such as natural scene text, handwritten
text and mathematical expression recognition.

• We introduce a novel GAS unit in SSDAN to bridge
the sequence-like text
image recognition and do-
main adaptation, which could adaptively transfer ﬁne-
grained character-level knowledge instead of perform-
ing domain adaptation by global features.

• The proposed SSDAN is capable of using unsuper-
vised sequence data to reduce domain shift effectively.

Extensive experiments on six benchmark datasets vali-
date the promising power of the proposed model towards
large scale real world application in natural scene text,
handwritten text and even more difﬁcult mathematical ex-
pressions recognition.

2. Related Work

In this section, we review the literature of text recogni-
tion methods. Then we discuss the recent trials of applying
domain adaptation techniques on text recognition.
Text Recognition Methods. Deep learning methods have
achieved remarkable results on image text reading [3, 5, 7,
13, 20, 21, 23, 31]. However, the literature is relatively
sparse on building a robust text recognizer that can handle
varying data in abundance of scenarios effectively. Some
methods were designed to handle perspective distortion ex-
hibited in the scene text. For example, [32] and [22] in-
troduce a spatial transformer network to rectify the entire
text before recognition. Furthermore, CharNet [21] tried
to introduce a character-level spatial transformer to rectify
individual characters, which was capable of handling more
complicated forms of distortion that cannot be modeled by
a single global transformation easily. However, they were
only designed for spatial afﬁne distortions and hard to gen-
eralize to the distortion caused by handwriting styles or
various structures in mathematical expressions.
In sum-
mary, existing text image recognition methods are usually
designed for a speciﬁc scenario, and cannot be generalized
effectively to different tasks. While our domain adaptation
model is designed for different scenarios, including the na-
ture scene text, handwritten text, and mathematical expres-

2741

AttentionCNNEncoderGRUDecoder𝜶𝒌𝜶𝟏𝜶𝑻𝒄𝒌𝒄𝟏𝒄𝑻……Attention Unit… …… ……Distribution AlignedSpaceGGGAS UnitSource imagesTarget imagesAttention-based character-level similarity lossGAS“models”…“Blenched”Sequence  decoding lossℱ(𝐱s)𝒜(𝐱s)𝐗sℱ(𝐱t)𝒜(𝐱t)𝐗t𝒜(𝐱t)ሚ𝒜(𝐱t)ℱ(𝐱)𝒜(𝐱s)ሚ𝒜(𝐱s)ℒ𝑎𝑡𝑡𝑛ℒ𝑑𝑒𝑐sion recognition. Furthermore, the intrinsic domain shift in
the text image data is commonly neglected in existing meth-
ods. On the contrary, our SSDAN utilizes the domain adap-
tation technique to tackle the domain shift problem, which
adaptively performs the character-level adaption in text im-
ages.
Domain Adaptation For Text Recognition. There have
been a plethora of recent works in the ﬁeld of visual domain
adaptation addressing the domain shift problem [30, 38, 41].
Some methods are evaluated on the character-level hand-
written or natural scene digital dataset for recognition tasks
and have shown effective performance. However, the ma-
jority of recent works use deep convolutional architectures
to map the source and target domains into a shared space
where the domains are aligned. They generally optimize the
global representation via minimizing some measure of do-
main shift, such as MMD [24, 38], CORAL [35, 41], or ad-
versarial loss [9, 30, 36]. Therefore, these methods cannot
be directly applied on sequential text images with multiple
characters, as the domain shift are locally in the characters
rather than the global image. Recently, other methods have
been proposed to adapt the different font styles for image-
to-image translation via adversarial learning [1]. Similarly,
these methods limitedly translate the font in different style
of signal characters on a global image, which are still cannot
be extended to text-line images. To address these problems,
we develop a sequence-to-sequence domain adaptation to
focus on ﬁne-grained character-level features to transfer
variable-length sequence knowledge successfully.

3. Proposed Method

In this paper, unsupervised sequence-to-sequence do-
main adaptation is developed for robust text recognition.
Speciﬁcally,
the source domain text images with well-
annotated text labels (a sequence of characters or symbols)
are available, while we only have an access to unlabeled
text images in target domain, which is in a different distri-
bution. More formally, we assume that there are N s an-
notated source domain samples X s = {xs
i=0 with the
i }N s
i=0, and N t unlabeled
corresponding labels Y s = {ys
i}N t
target-domain samples X t = {xt
i=0 without any avail-
able annotated labels in the training time. For y ∈ Y s,
y = {y1, y2, ..., yT }, where yk and T denotes a character
label and the variable length of text, respectively.

i }N s

Considering that typical global domain adaptation meth-
ods lack ﬁne-grained character-level information, we de-
velop a Sequence-to-Sequence Domain Adaptation Net-
work (SSDAN) for robust text image recognition, aligning
the distribution of the source and target sequence data in an
attended character-level feature space rather than a global
coarse alignment. As shown in Figure 2, the proposed SS-
DAN is an attention-based sequence encoder-decoder net-
work, which encodes a text image into a sequence of at-
tended character-level features that are then recomposed

through a GRU decoder with an attention mechanism. In
the proposed SSDAN, a GAS unit is further introduced to
adaptively guide model ﬁnding the character-level domain-
invariant features between the source and target domain.

3.1. Attentive Text Recognition

The attentive text recognition can be essentially con-
sidered as learning a mapping between a sequence of fea-
ture maps encoded from sequence-like text image x, and a
ground truth label sequence y = {y1, y2, ..., yT }. As shown
in Figure 2, the attentive text recognition pipeline consists
of: 1) a CNN encoder that learns high-level visual represen-
tations from an input image. 2) an attention model between
the encoder and the decoder driving the focus of attention
of the model towards a speciﬁc part of the sequence of en-
coded features. 3) a GRU decoder that generates a sequence
of symbols as output, one at every time step.
CNN Encoder. CNN encoder F takes the raw input image
x from the source or target domain, and produces a feature
grid F(x) of size H ′ × W ′ × D, where D denotes the num-
ber of channels, H ′ and W ′ are the resulted feature map
height and width, respectively. The encoder output is then
reshaped as a grid sequence of L elements, L = H ′ × W ′.
Each of these elements is a D-dimensional feature vector
that corresponds to a local region of the image through its
corresponding receptive ﬁeld. Hence, the whole encoded
image F(x) could be reformatted as,

F(x) = [f1, ..., fL] , fi ∈ RD,

(1)

where fi corresponds to i-th grid of the encoded image
F(x), which preserves speciﬁc spatial information of the
input image x.
Attention.
Although the CNN encoder keeps the spa-
tial information, we cannot decide the location of a speciﬁc
character in a text image. Therefore, an attention model is
introduced to learn which part of the text image is the most
relevant to a decoding character. As shown in Figure 2, the
attention is a T -step process, at time-step k, the represen-
tation of the most relevant part to character yk of encoding
feature map F(x) is deﬁned as a context vector ck:

ck =

L

Xi=0

αk,ifi,

where, the attention weights αk,i is calculated by

(2)

(3)

αk,i =

exp(sk,i)
j=0 exp(sk,j)

,

PL

where the attention score sk,i indicates the probability of
that the model attends to the i-th sub-region in the encoded
map F(x) when decoding the k-th character of the text im-
age. Following the past empirical work [7], we deﬁned the
attention score as

sk,i = β⊤ tanh(Whhk−1 + Wf fi),

(4)

2742

where β, Wh and Wf are the parameters to be learnt, hk−1
is the previous decoding state in the decoder.
GRU Decoder. A GRU decoder is employed to predict the
string of an input text image recurrently, where we use gated
recurrent unit (GRU) neural network. At decoding time step
k, the GRU leverages the context vector ck, previous state
hk−1 and previous predicted character yk−1 to generate a
new hidden state

hk = GRU (hk−1, yk−1, ck),

(5)

where, ck is generated by the attention mechanism, which
focuses on the most relevant region of current decoding
character. Then, the probability of current predicted sym-
bol yk is computed by :

p(yk|yk−1, ck) = g (Wo tanh(E˜yk−1 + Wdhk + Wcck)) ,
(6)
where g denotes a softmax activation function, Wo, Wd
and Wc are the mapping matrices, E is the embedding ma-
trix, and ˜yk−1 is the one-hot vector of character label yk−1.
The probability of the sequential labels y is ﬁnally given

by the product of the probability of each label:

P (y|A(x)) =

T

Yk=1

p(yk|yk−1, ck),

(7)

where A(x) = {c1, c2, ..., cT }, which could be regarded
as a sequence of attended character-level features from an
input text image x.

3.2. Gated Attention Similarity Unit

Given the misalignment of ground truth strings between
the source and target sequence domain, we introduced
Gated Attention Similarity (GAS) Unit, based on an at-
tention encoder-decoder mechanism, to convert a variable-
length input text image into a sequence of character fea-
tures. By decomposing the text strings into a set of charac-
ters, the source and target domain will statistically share the
same label space in character-level, and thus the inﬂuence
of the misalignment problem can be alleviated. More for-
mally, through attention mechanism, an input image x can
be adaptively decomposed into a series of character-level
feature set A(x) = {c1, c2, ..., cT }, where ck presents the
feature of k-th character in the text image x. Speciﬁcally,
a source text image xs and a target text image xt are de-
composed into a source and target attended character-level
feature set A(xs) and A(xt), respectively.

We notice that if the attention context vector fails to fo-
cus on the region of effective character, the adaptation on
the attention context vector will not help. To overcome this
problem, we introduce a gate mechanism to select effective
attention context vectors to perform domain adaptation. An
intuition is that if the current attention context vector ck
is distinguishable, the probability that ck belongs to one
speciﬁc character yk will be relatively higher than others.

Hence, we further introduce an adaption gate function δ(ck)
to judge if a context vector ck is attending to a valid char-
acter,

δ(ck) =(1

0

if p (yk|yk−1, ck) > pc
if p (yk|yk−1, ck) < pc

,

(8)

where pc is a conﬁdence threshold. Furthermore, a gate
function set G is adaptively changed according to the spe-
ciﬁc input image x, which is expressed as:

G(x) = {δ(c1), ..., δ(cT )} ,

(9)

Through the gate function, we can update attention context
vector set by adaptation gate function set G(x),

˜A(x) = A(x) ⊗ G(x),

(10)

where ⊗ denotes element-wise product operator. Speciﬁ-
cally, if ck × δ(ck) = 0, then current context vector ck will
not be added in a new attention context vector set.

A gated attention similarity loss Lattn is accordingly
introduced to measure the distance on the valid attended
character-level feature set of source and target domain as

(11)

Lattn = E[xs∈Xs,xt ∈Xt]ndist(cid:16) ˜A(xs), ˜A(xt)(cid:17)o .

There are multiple choices for the distance function dist,
such as (1) MMD [24] computing the norm of difference
between two domain means, (2) CORAL [35] computing
the distance of covariance of two domain, or even (3) adver-
sarial loss [9] minimizing the loss of a domain classiﬁer to
learn a representation that is simultaneously discriminative
of source labels while not being able to distinguish between
domains. In the experiment, we have explored these differ-
ent measurements, and experimentally found that CORAL
is more appropriate for our model. Speciﬁcally, CORAL is
to align the second-order statistics-correlation of the source
and target data , which is deﬁned as

dist(Us, Ut) =

1
4d2 ||cov(Us) − cov(Ut)||2
F ,

(12)

i }, us ∈ Rd, Ut = {ut

i}, ut ∈ Rd, and
F denotes the squared matrix Frobenius norm, cov(Us)

where Us = {us
|| · ||2
is the covariance matrix of samples Us, denoted by

cov(Us) =

1

N − 1 (cid:18)Us

⊤Us −

1
N

(1⊤Us)⊤(1⊤Us)(cid:19) ,

(13)

where 1 is a column vector with all elements equal to 1, N is
the number of samples Us, and Us(i, j) (Ut(i, j)) indicates
the j-th dimension of the i-th source (target) data example.
In our GAS unit, Us and Ut are replaced by the valid
attended character-level feature set ˜A(xs) and ˜A(xt), re-
spectively. Note that ˜A(xs) and ˜A(xt) need to be reformat-
ted as a matrix, respectively. Speciﬁcally, suppose ˜A(xs)
and ˜A(xt) contain T1 and T2 elements, respectively. Then
˜A(xs) and ˜A(xt) could be reformatted as matrices with
T1 × D and T2 × D elements, and their covariance matrices
are with the same dimension D × D.

2743

3.3. Overall Objective Function

With the well-annotated source-domain data, we could
learn an optimized source text image recognizer by mini-
mizing a supervised decoding loss, where we can use the
negative log likelihood of sequential probability as the de-
coding loss Ldec to measure the differences between the
predicted and the source labeled character sequences:

Ldec = E(xs,ys)∼(X s,Y s) {− log p(ys|A(xs))} .

(14)

Directly optimizing Ldec may cause overﬁtting in source
domain, and thus fails to perform well for the shifted tar-
get domain. The GAS unit in our model is introduced to
offer guidance to learn domain-invariant features between
the source and target domain. The learnt robust representa-
tions should work effectively on the target domain, where
they are also required to be discriminative. Therefore, the
attention similarity loss Lattn in Eq. 11 is combined with
the discriminative decoder loss Ldec in source domain. The
overall objective function of the attentional domain adapta-
tion model is deﬁned as:

LSSDAN = Ldec + λLattn,

(15)

where λ is a hyper-parameter to balance two terms. The
model parameters can be directly optimized by minimizing
the overall objective through stochastic gradient descent op-
timization algorithms.

4. Experiments

Datasets. We conduct extensive experiments to validate
the proposed SSDAN on six general recognition benchmark
datasets, including three different types of text image,
i.e.,
scene text, handwritten text, and mathematical expressions
with more complex structure, as shown in Figure 1.

• ICDAR-2003 (IC-03) [25] contains 860 cropped
scene text images, following the protocol used in [31].

• ICDAR-2013 (IC-13) [18] contains 857 cropped

scene images after ﬁltering as did in IC-03.

• Street View Text (SVT) [37] consists of 647 test scene

word images from Google Street View.

• IIIT5K-words (IIIT5K) [27] contains 3, 000 cropped

test scene text images from the Internet.

• IAM [26] is a handwritten English text dataset, written
by 657 different writers. It is partitioned into writer-
independent training, validation and test partitions of
6161, 976 and 2915 lines, respectively. That contains
a total of 46945, 7554 and 20306 correctly segmented
words in each partition.

• CROHME 2014 [28] is a handwritten mathematical
expression dataset. It contains 8836 training and 986

test math expressions. There are 101 math symbols.
The handwritten expressions or LaTeX notations in the
test set never appear in the train set.

Evaluation Metric. For different recognition task, we
adopt different evaluation metric as follows:

• Scene text. The word prediction accuracy is used to
evaluate scene text recognition model, following sev-
eral benchmark [21, 31].

• Handwritten text. Two metrics are used to evalu-
ate the handwritten text recognition model: the Char-
acter Error Rate (CER) and the Word Error Rate
(WER) [3, 34]. CER is deﬁned as the Levenstein
distance between the predicted and real character se-
quence of the word. WER denotes the percentage of
words improperly recognized. For CER and WER,
small values indicate better performance.

• Mathematical expression. We use a global perfor-
mance metric expression recognition rate (ExpRate) to
denote the percentage of predicted formula sequences
matching the real formula sequences [7].

Implementation Details. The architecture of the CNN
encoder is derived from the DenseNet [14], where dense
blocks are densely concatenation of 1×1 convolution layers
and 3 × 3 convolution layers. while the transition layers are
composed of 1 × 1 convolution and 2 × 2 average pooling,
and the channel 0.5 refers to the compression rate, which
is to reduce the number of feature map of each block to
half. All convolutions are followed by batch normalization
layer [15] and rectiﬁed linear unit (Relu) activation func-
tion [29] . In order to make the encoder suitable for recog-
nizing text, we use the pooling layer with kernel size 2 × 1
to reduce feature dimension along the height axis only. As
a result, the resolution of feature maps produced by encoder
is H/32 × W/4, where the values of H and W are set ac-
cording to the speciﬁc dataset. After the CNN encoder, we
use a bi-directional LSTM to capture more context informa-
tion for attention, and each LSTM has 256 hidden units. For
the decoder, we use a GRU cell with 512 memory blocks.

All of our experiments are implemented with Tensorﬂow.
The complete model is initially pre-trained to minimize the
decoding loss of the source training data, and then is ﬁne-
tuned to minimize the overall domain adaptation objective
with unsupervised target data. The model is trained with the
Adadelta optimizer [39].

4.1. Comparison with Existing Methods

In this section, we investigate the generalization of our
model in three different domains,
including scene text,
handwritten text and mathematical expressions. To validate
the performance of our SSDAN model, we focus on un-
constrained text recognition without any language model or
lexicon. On each task , we consider a baseline for SSDAN

2744

as SSDAN-base that omits the GAS unit to switch off the
domain adaption process. SSDAN-base is used to investi-
gate the capability of SSDAN for domain adaptation on the
text image recognition task.
Results on Scene Text.
In this scenario, we explore the
capability of SSDAN for domain adaptation on the scene
text recognition, where synthetic dataset MJSYNTH [17]
is used as the source training data, and the real scene text
data is used as target test data. MJSYNTH [17] contains 8
millions annotated synthetic images, which are generated
to simulate natural scene text images. Table 1 presents
the test results on four real scene text datasets. Com-
pared to the baseline model SSDAN-base, our SSDAN
method could obtain consistent improvement in different
datasets.
It’s mainly attributed to sequence-to-sequence
domain adaptation, which is able to learn more domain-
invariant features. Furthermore, we investigate the perfor-
mance of our model among the recent state-of-the-art ap-
proaches [10, 11, 16, 20–22, 33], which are tailored for
scene text recognition. We can observe that the perfor-
mance of our baseline SSDAN-base are at average level.
However, the SSDAN model with sequence domain adap-
tation can achieve comparable results with the best com-
petitor [21, 33].
It’s notable that the motivations in our
method are substantially different from these works. For
example, RARE [32], STAR-Net [22], ASTER [33] and
Char-Net [21] target the irregular scene text recognition,
which are designed for spatial distortions. They would not
be easily generalized to different distortions, such as vari-
ous handwriting style and complex structures in mathemat-
ical expressions. In contrast, our method aims to perform
sequence-to-sequence domain adaptation to reduce the do-
main shift, and correspondingly allows us to relieve differ-
ent distortions using a general framework in different sce-
narios.

Table 1. Scene text recognition accuracies on general scene text
recognition benchmarks.

Model

IIIT5K

SVT

IC-03

IC-13

−
83.3
78.4
81.2
81.9

ANN [16]
STAR-Net [22]
R2AM [20]
CRNN [31]
RARE [32]
Ghosh et al [12] −
Gao et al [10]
ASTER [33]
Char-Net [21]

81.8
83.2
83.6

SSDAN-base
SSDAN

81.1
83.8

71.7
83.6
80.7
82.7
81.9
75.1
82.7
87.6
84.4

82.1
84.5

89.6
89.9
88.7
91.9
90.1
89.3
89.2
92.4
91.5

91.2
92.1

81.8
89.1
90.0
89.6
88.6
−
88.0
89.7
90.8

91.0
91.8

Results on Handwritten Text. To verify the generalization
capability of our model, we evaluate our model on IAM to
validate the effectiveness of the sequence-to-sequence do-
main adaptation on the handwriting recognition.
In this

case, the source and target data are the writer-independent
training and test data, respectively. Various handwriting
styles are primary causes of domain shift. What’s more,
it may suffer character-touching problem, which is differ-
ent from scene text. We note that [6] achieved a state-of-
the-art performance on IAM, however, the test data used
in [6] wasn’t same with [2, 3, 34]. For a fair compari-
son, we only show the results using the same test data and
without any language model. Table 2 illustrates the hand-
writing recognition results. Although the performance of
our baseline is not better than sueiras2018ofﬂine [34], our
SSDAN model can still achieve signiﬁcant improvement,
which demonstrates the effectivity of model.

Table 2. Results on handwritten text.

Method

WER

CER

Average

bluche2015deep [2]
bluche2016joint [3]
sueiras2018ofﬂine [34]

SSDAN-base
SSDAN

24.7
24.6
23.8

23.9
22.2

7.3
7.9
8.8

9.2
8.5

16.00
16.25
16.30

16.55
15.35

Results on Handwritten Mathematical Expression. To
show the ﬂexibility of our model, we also conduct exper-
iments on handwritten mathematical expression recogni-
tion. Handwritten mathematical expression recognition is
to convert an image into structured language, such as LaTex
strings, which not only denotes the text itself but also de-
notes its structural information. It’s a more complex prob-
lem than traditional scene text or handwriting recognition.
In particular, it suffers variant scales of handwritten math
symbols with more complicated structure, which results in
more difﬁcult domain shift.
In the experiment, the train-
ing expressions and the unseen test expressions are used as
source and target data, respectively. The results of the ex-
pression recognition rate (ExpRate) are listed in Table 3.
We note that the WAP approach [40] achieved the state-
of-the-art result, which involved 5 ensemble models to im-
prove the performance. However, our model doesn’t use
any ensemble trick, which might be investigated as the fu-
ture work. Compared with the best 3 systems in CROHME
2014 competition, which only use the CROHME training
data, our model obviously outperforms these participat-
ing systems with large gaps. Furthermore, compared to
[7, 8, 19], our SSDAN model can still achieve better per-
formance.
It’s remarkable that our model is competitive,
especially after domain adaptation. Hence, we can believe
that our SSDAN model is able to capture the complex do-
main shift in structural images.

4.2. Ablation Study

We ﬁrstly evaluate the necessity of character-level do-
main adaptation. Then we analyze the contributions of dif-
ferent components, and investigate the effect of different

2745

Table 3. Results on handwritten mathematical expression.

Method

ExpRate

I [28]
VI [28]
VII [28]
WYCIWYS [8]
Le et al [19]
IM2TEX [7]

SSDAN-base
SSDAN

37.2
25.7
26.1
28.7
35.2
38.7

39.9
41.6

domain shift measurement and parameter sensitivity. Fur-
thermore, we visualize some recognition results, and ex-
plore the effect of unsupervised data.
Comparison to Standard Domain Adaptation. Measur-
ing similarity on CNN outputs directly can be treated as a
STandard Domain Adaptation method (STDA), which lacks
of ﬁne-grained character-level information in a text image.
Our SSDAN method introduces a GAS unit to adaptively
perform domain adaptation on a set of character-level fea-
ture vectors via attention scheme, which focuses on most
relevant region towards a speciﬁc character instead of global
CNN outputs. To validate the the necessity of character-
level domain adaptation, we have done experiment to com-
pare the STDA with our SSDAN on IAM dataset. The re-
sults in Table 4 show that STDA obtains worse results than
the baseline SSDAN-base, while SSDAN gets signiﬁcant
improvement.
It validates the advantages of our SSDAN
that the ﬁne-grained character-level knowledge transfer be-
tween the source and target sequence data is more effective
in the decoder than the CNN outputs.

Table 4. Comparison to standard domain adaptation.

Method

WER
CER

SSDAN-base

SSDAN

STDA

23.9
9.2

22.2
8.5

25.0
11.1

Component Analysis.
In this scenario, we evaluate
the contribution of different components of the proposed
model. These variants include: 1) using different CNN en-
coder to investigate the contribution of encoder,
i.e., V1,
V3 and V5 using VGG-based (VGG) [31], ResNet-based
(ResNet) [4], and DenseNet encoder, respectively; 2) intro-
ducing the GAS unit for different encoder to evaluate the
effect of GAS unit among different encoders, i.e., V2, V4,
and V6 are developed based on the V1, V3, and V5, re-
spectively. For the analysis, we choose handwritten text
dataset IAM to evaluate model from both CER and WER,
and all the experiments are on the same training protocol.
Table 5 presents the comparison between the variants of our
model. Firstly, we can observe that DenseNet is a more
powerful encoder from the comparisons among the model
V1, V3, and V5. Furthermore, the comparison pairs (V1,
V2), (V3, V4) and (V5, V6) show that the GAS unit could

always improve performance despite of the types of en-
coders, which demonstrates that considering the sequence-
to-sequence domain adaptation makes sense.

Table 5. Component Analysis.

Components Model

V1 V2 V3 V4 V5 V6

Encoder

X

X

VGG
ResNet
DenseNet

X

X

Adaptation GAS

X

X

X

X

X

Evaluation

WER
CER

32.8 26.9 29.9 27.9 23.9 22.2
15.9 12.6 14.3 13.1 9.2

8.5

Effect of Different Domain Shift Measurement. Our
SSDAN learns domain invariant representations by min-
imizing some measure of domain shift between the dis-
tributions of attended ﬁne-grained character-level features
from the source and target text images.
In this scenario,
we investigate the effect of different domain shift measure-
ment among CORAL, MMD and adversarial loss (Adver-
sarial). The adversarial loss is measured by an extra domain
classiﬁer, which is a single layer fully-connected network
with 128 hidden units. Speciﬁcally, adversarial loss based
method needs to minimize the adversarial loss with respect
to parameters speciﬁc to the domain classiﬁer, while maxi-
mizing it with respect to the parameters of text image rec-
ognizer. To unify the training procedure in a single step,
we use a a gradient reversal layer [9] for the minimax opti-
mization. Table 6 shows the results using different measure-
ment of domain shift on the IAM dataset. We can observe
that CORAL-based method outperforms the MMD-based
method and adversarial loss-based method. This may show
that CORAL is more appropriate for adapting attended ﬁne-
grained character-level features.

Table 6. Effect of Different Domain Shift Measurement.

Method

WER
CER

CORAL

MMD

Adversarial

22.2
8.5

22.7
8.8

24.6
10.8

Parameter Sensitive Analysis. In this subsection, we eval-
uate the sensitiveness of the hyper-parameter pc and λ in
the Eq. 9 and Eq. 15, respectively. Here, we conduct the
experiments on the MJSYNTH → SVT task. Speciﬁcally,
we explore the different λ and pc from {0.01, 0.1, 1, 10}
and {0, 0.1, 0.2, 0.4, 0.8}, respectively. The evaluation is
conducted by changing one parameter while keeping the
other hyper-parameters ﬁxed. The λ in the objective func-
tion Eq. 15 controls the contribution of sequence domain
adaptation. λ = 0 indicates the proposed model switch-
ing off the sequence domain adaptation, which equals to
the SSDAN-base. While λ > 0 means to perform domain
adaptation. Furthermore, the pc in the gate function of Eq. 9

2746

decides whether an attended feature performs domain adap-
tation or not. Speciﬁcally, if the probability that the current
feature vector belongs to a valid character is larger than pc,
the vector will be performed domain adaptation, otherwise,
it will be neglected as a noise. From other perspective, if
pc = 0, the gate function will not work, which means per-
forming sequence domain adaptation on character-level fea-
ture without any guidance. While pc is too large, the gate
function will be too strict to select enough valid features.
Figure 3 shows different gains of pc values, where λ = 1.
The results experimentally prove that the gate function is
important to the overall performance.

85

y
c
a
r
u
c
c
a

80

75

SSDAN-base
SSDAN

70

-2

-1.5

-1

-0.5

0

0.5

1

log( )

85

y
c
a
r
u
c
c
a

80

75

Figure 4. Examples showing the recognition result, the left col-
umn is the input image with ground truth, the second column and
the last column denote the recognition result without and with do-
main adaptation, respectively. Each result is shown in the pair of
attention visualization and prediction text.

SSDAN-base
SSDAN

70

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

p

c

Figure 3. The effect of model parameters λ (left) and pc (right).

IC-03

IIIT5K

Visualization. In this section, we visualized some recog-
nition results from IAM. The results are shown in the pair
of attention visualization and prediction text. The selected
attention visualization shows the attending location at one
speciﬁc time, where the SSDAN-base model suffers recog-
nition error. As shown in Figure 4, while SSDAN-base
failed to deal with the distortion of individual character
caused by handwriting style, SSDAN successfully worked.
As the ﬁrst two cases shown in Figure 4, even though the
SSDAN-base and SSDAN model attend to the same loca-
tion at one speciﬁc time, SSDAN could achieve a better
performance through alleviating the domain shift. More in-
terestingly, we ﬁnd the SSDAN model can learn more pre-
cise alignment, according to the last two cases in Figure 4.
These results again validate the effectiveness of SSDAN.

Effect of Unsupervised Data. In order to quantify the ef-
fectiveness of unsupervised data, we train our model with
different size of labeled data and unlabeled data, while keep
other hyper-parameters ﬁxed. Figure 5 presents the results
with different data size. Firstly, we observe the SSDAN-
base model, which is a full supervised learning, with dif-
ferent amounts of labeled samples randomly sampling from
the MJSYNTH dataset. The more labeled samples are used,
the higher accuracies on real test datasets get. It’s notable
that using additional unlabeled samples can get consistent
performance improvement by SSDAN, where the size of
unsupervised data is in accordance with the amount of la-
beled data. It indicates that our SSDAN is able to learn the
knowledge from unsupervised data. We also observe that
our model could get signiﬁcant improvement when avail-
able annotated data is small.

y
c
a
r
u
c
c
a

80

75

70

65

60

55

50

45

40

y
c
a
r
u
c
c
a

80

75

70

65

60

55

50

45

40

SSDAN-base
SSDAN

SSDAN-base
SSDAN

1

2

3

4

5

dataset size

6
104

1

2

3

4

5

dataset size

6
104

Figure 5. The effect of training dataset size on IC-03 (left) and
IIIT5K (right).

5. Conclusion

In this paper, we present a novel SSDAN model for ro-
bust text image recognition, which bridges the sequence-
like text image recognition and domain adaptation. It’s ca-
pable of taking advantage of unsupervised sequence data
to learn more robust representations. The proposed model
could also be generalized to different scenes, which include
scene text, handwritten text and mathematical expression
recognition. Comprehensive experimental results on sev-
eral datasets and extensive analyses have demonstrated the
effectiveness of our algorithm. An interesting open issue
for future research is to further adjust SSDAN framework
to better deal with various sequence domain shift.

Acknowledgements. The ﬁrst and the fourth authors have
equal contribution in this work. This work was supported
by the China National Nature Science Foundation with fol-
lowing No. 61573357, No. 61503382, No. 61602089, No.
61403370, No. 61273267 and No. 91120303; Project of
Xueersi Online School (No. Y9D2M10101).

2747

LenowknowlaughedlaughedpracticallydracticallyknowmiserablemiscracblemiserablepracticallylanghedInput imageSSDAN-baseSSDANReferences

[1] Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen
Wang, Eli Shechtman, and Trevor Darrell. Multi-content
gan for few-shot font style transfer.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, volume 11, page 13, 2018.

[2] Th´eodore Bluche. Deep neural networks for large vocab-
ulary handwritten text recognition. PhD thesis, Universit´e
Paris Sud-Paris XI, 2015.

[3] Th´eodore Bluche.

Joint line segmentation and transcrip-
tion for end-to-end handwritten paragraph recognition.
In
Advances in Neural Information Processing Systems, pages
838–846, 2016.

[4] Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang
Pu, and Shuigeng Zhou. Focusing attention: Towards ac-
curate text recognition in natural images. In Computer Vi-
sion (ICCV), 2017 IEEE International Conference on, pages
5086–5094. IEEE, 2017.

[5] Zhanzhan Cheng, Yangliu Xu, Fan Bai, Yi Niu, Shiliang Pu,
and Shuigeng Zhou. Aon: Towards arbitrarily-oriented text
recognition.
In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.

[6] Arindam Chowdhury and Lovekesh Vig. An efﬁcient end-to-
end neural model for handwritten text recognition. In BMVC,
2018.

[7] Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and Alexan-
Image-to-markup generation with coarse-to-
In International Conference on Machine

der M Rush.
ﬁne attention.
Learning, pages 980–989, 2017.

[8] Yuntian Deng, Anssi Kanervisto, and Alexander M Rush.
What you get is what you see: A visual markup decompiler.
arXiv preprint arXiv, 1609, 2016.

[9] Yaroslav Ganin and Victor Lempitsky. Unsupervised do-
main adaptation by backpropagation. In Proceedings of the
32nd International Conference on International Conference
on Machine Learning-Volume 37, pages 1180–1189. JMLR.
org, 2015.

[10] Yunze Gao, Yingying Chen, Jinqiao Wang, and Hanqing Lu.
Reading scene text with attention convolutional sequence
modeling. arXiv preprint arXiv:1709.04303, 2017.

[11] Yuting Gao, Zheng Huang, and Yuchen Dai. Double su-
pervised network with attention mechanism for scene text
recognition. arXiv preprint arXiv:1808.00677, 2018.

[12] Suman K Ghosh, Ernest Valveny, and Andrew D Bagdanov.
Visual attention models for scene text recognition. In 2017
14th IAPR International Conference on Document Analysis
and Recognition (ICDAR). IEEE, 2017.

[13] Pan He, Weilin Huang, Yu Qiao, Chen Change Loy, and Xi-
aoou Tang. Reading scene text in deep convolutional se-
quences. In AAAI, volume 16, pages 3501–3508, 2016.

[14] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In CVPR, volume 1, page 3, 2017.

[15] Sergey Ioffe and Christian Szegedy. Batch normalization:
accelerating deep network training by reducing internal co-
variate shift. In Proceedings of the 32nd International Con-
ference on International Conference on Machine Learning-
Volume 37, pages 448–456. JMLR. org, 2015.

[16] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-
drew Zisserman. Deep structured output learning for uncon-

strained text recognition. arXiv preprint arXiv:1412.5903,
2014.

[17] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-
drew Zisserman. Synthetic data and artiﬁcial neural net-
works for natural scene text recognition.
arXiv preprint
arXiv:1406.2227, 2014.

[18] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,
Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles
Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Al-
mazan, and Lluis Pere De Las Heras.
Icdar 2013 robust
reading competition.
In Document Analysis and Recogni-
tion (ICDAR), 2013 12th International Conference on, pages
1484–1493. IEEE, 2013.

[19] Anh Duc Le and Masaki Nakagawa. Training an end-to-end
system for handwritten mathematical expression recognition
by generated patterns. In Document Analysis and Recogni-
tion (ICDAR), 2017 14th IAPR International Conference on,
volume 1, pages 1056–1061. IEEE, 2017.

[20] Chen-Yu Lee and Simon Osindero. Recursive recurrent nets
In Proceed-
with attention modeling for ocr in the wild.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2231–2239, 2016.

[21] Wei Liu, Chaofeng Chen, and Kwan-Yee K Wong. Char-
net: A character-aware neural network for distorted scene
text recognition.
In AAAI Conference on Artiﬁcial Intelli-
gence. New Orleans, Louisiana, USA, 2018.

[22] Wei Liu, Chaofeng Chen, Kwan-Yee K Wong, Zhizhong Su,
and Junyu Han. Star-net: A spatial attention residue network
for scene text recognition. In BMVC, volume 2, page 7, 2016.
[23] Zichuan Liu, Yixing Li, Fengbo Ren, Wang Ling Goh, and
Hao Yu. Squeezedtext: A real-time scene text recognition by
binary convolutional encoder-decoder network, 2018.

[24] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I
Jordan. Learning transferable features with deep adaptation
networks. arXiv preprint arXiv:1502.02791, 2015.

[25] Simon M Lucas, Alex Panaretos, Luis Sosa, Anthony Tang,
Shirley Wong, Robert Young, Kazuki Ashida, Hiroki Nagai,
Masayuki Okamoto, Hiroaki Yamamoto, et al.
Icdar 2003
robust reading competitions: entries, results, and future di-
rections.
International Journal of Document Analysis and
Recognition (IJDAR), 7(2-3):105–122, 2005.

[26] U-V Marti and Horst Bunke. The iam-database: an english
sentence database for ofﬂine handwriting recognition.
In-
ternational Journal on Document Analysis and Recognition,
5(1):39–46, 2002.

[27] Anand Mishra, Karteek Alahari, and CV Jawahar. Scene text
recognition using higher order language priors. In BMVC-
British Machine Vision Conference. BMVA, 2012.

[28] Harold Mouchere, Christian Viard-Gaudin, Richard Zanibbi,
and Utpal Garain.
Icfhr 2014 competition on recognition
of on-line handwritten mathematical expressions (crohme
2014). In Frontiers in handwriting recognition (icfhr), 2014
14th international conference on, pages 791–796. IEEE,
2014.

[29] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units im-
prove restricted boltzmann machines. In Proceedings of the
27th international conference on machine learning (ICML-
10), pages 807–814, 2010.

[30] Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin
Wang. Multi-adversarial domain adaptation. In AAAI Con-

2748

ference on Artiﬁcial Intelligence, 2018.

[31] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end
trainable neural network for image-based sequence recog-
nition and its application to scene text recognition.
IEEE
transactions on pattern analysis and machine intelligence,
39(11):2298–2304, 2017.

[32] Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao,
and Xiang Bai. Robust scene text recognition with auto-
matic rectiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4168–
4176, 2016.

[33] Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan
Lyu, Cong Yao, and Xiang Bai. Aster: An attentional scene
text recognizer with ﬂexible rectiﬁcation. IEEE transactions
on pattern analysis and machine intelligence, 2018.

[34] Jorge Sueiras, Victoria Ruiz, Angel Sanchez, and Jose F
Velez. Ofﬂine continuous handwriting recognition using
sequence to sequence neural networks. Neurocomputing,
289:119–128, 2018.

[35] Baochen Sun and Kate Saenko. Deep coral: Correlation
alignment for deep domain adaptation.
In European Con-
ference on Computer Vision, pages 443–450. Springer, 2016.
[36] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
Adversarial discriminative domain adaptation. In Computer
Vision and Pattern Recognition (CVPR), volume 1, page 4,
2017.

[37] Kai Wang, Boris Babenko, and Serge Belongie. End-to-end
scene text recognition.
In Computer Vision (ICCV), 2011
IEEE International Conference on, pages 1457–1464. IEEE,
2011.

[38] Baoyao Yang, Andy Jinhua Ma, and Pong C Yuen. Domain-
shared group-sparse dictionary learning for unsupervised do-
main adaptation. In AAAI, 2018.

[39] Matthew D Zeiler. Adadelta: an adaptive learning rate

method. arXiv preprint arXiv:1212.5701, 2012.

[40] Jianshu Zhang, Jun Du, Shiliang Zhang, Dan Liu, Yulong
Hu, Jinshui Hu, Si Wei, and Lirong Dai. Watch, attend
and parse: An end-to-end neural network based approach to
handwritten mathematical expression recognition. Pattern
Recognition, 71:196–206, 2017.

[41] Junbao Zhuo, Shuhui Wang, Weigang Zhang, and Qingming
Huang. Deep unsupervised convolutional domain adapta-
tion. In Proceedings of the 2017 ACM on Multimedia Con-
ference, pages 261–269. ACM, 2017.

2749

