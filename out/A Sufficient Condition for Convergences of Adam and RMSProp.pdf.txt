A Sufﬁcient Condition for Convergences of Adam and RMSProp

Fangyu Zou†∗, Li Shen‡∗ , Zequn Jie‡, Weizhong Zhang‡, Wei Liu‡

‡Tencent AI Lab

†Stony Brook University

fangyu.zou@stonybrook.edu, mathshenli@gmail.com, zequn.nus@gmail.com,

zhangweizhongzju@gmail.com, wl2223@columbia.edu

Abstract

Adam and RMSProp are two of the most inﬂuential adap-
tive stochastic algorithms for training deep neural networks,
which have been pointed out to be divergent even in the con-
vex setting via a few simple counterexamples. Many attempts,
such as decreasing an adaptive learning rate, adopting a
big batch size, incorporating a temporal decorrelation tech-
nique, seeking an analogous surrogate, etc., have been tried
to promote Adam/RMSProp-type algorithms to converge. In
contrast with existing approaches, we introduce an alterna-
tive easy-to-check sufﬁcient condition, which merely depends
on the parameters of the base learning rate and combina-
tions of historical second-order moments, to guarantee the
global convergence of generic Adam/RMSProp for solving
large-scale non-convex stochastic optimization. Moreover,
we show that the convergences of several variants of Adam,
such as AdamNC, AdaEMA, etc., can be directly implied via
the proposed sufﬁcient condition in the non-convex setting.
In addition, we illustrate that Adam is essentially a speciﬁ-
cally weighted AdaGrad with exponential moving average
momentum, which provides a novel perspective for under-
standing Adam and RMSProp. This observation coupled
with this sufﬁcient condition gives much deeper interpreta-
tions on their divergences. At last, we validate the sufﬁcient
condition by applying Adam and RMSProp to tackle a certain
counterexample and train deep neural networks. Numerical
results are exactly in accord with our theoretical analysis.

1. Introduction

Large-scale non-convex stochastic optimization [5], cov-
ering a slew of applications in statistics and machine learning
[13, 5] such as learning a latent variable from massive data
whose probability density distribution is unknown, takes the
following generic formulation:

min
x∈Rd

f (x) = Eξ∼P(cid:2)ef (x, ξ)(cid:3),

(1)

∗The ﬁrst two authors contribute equally. †This work was partially done

when Fangyu Zou was a research intern at Tencent AI Lab, China.

where f (x) is a non-convex function and ξ is a random
variable satisfying an unknown distribution P.

Due to the uncertainty of distribution P, the batch gradi-
ent descent algorithm [4] is impractical to employ full gra-
dient ∇f (x) to solve problem (1). Alternatively, a compro-
mised approach to handle this difﬁculty is to use an unbiased
stochastic estimate of ∇f (x), denoted as g(x, ξ), which
leads to the stochastic gradient descent (SGD) algorithm
[21]. Its coordinate-wise version is deﬁned as follows:

xt+1,k = xt,k − ηt,kgt,k(xt, ξt),

(2)

for k = 1, 2, . . . , d, where ηt,k ≥ 0 is the learning rate of
the k-th component of stochastic gradient g(xt, ξt) at the
t-th iteration. A sufﬁcient condition [21] to ensure the global
convergence of vanilla SGD (2) is to require ηt to meet

∞Pt=1kηtk = ∞ and

∞Pt=1kηtk2 < ∞.

(3)

Although the vanilla SGD algorithm with learning rate ηt
satisfying condition (3) does converge, its empirical perfor-
mance could be still stagnating, since it is difﬁcult to tune an
effective learning rate ηt via condition (3).

To further improve the empirical performance of SGD,
a large variety of adaptive SGD algorithms, including Ada-
Grad [9], RMSProp [11], Adam [14], Nadam [8], etc., have
been proposed to automatically tune the learning rate ηt by
using second-order moments of historical stochastic gradi-
ents. Let vt,k and mt,k be the linear combinations of the
historical second-order moments (g2
t,k) and
stochastic gradient estimates (g1,k, g2,k,··· , gt,k), respec-
tively. Then, the generic iteration scheme of these adaptive
SGD algorithms [20, 6] is summarized as

2,k,··· , g2

1,k, g2

xt+1,k = xt,k − ηt,kmt,k, with ηt,k = αt/√vt,k,

(4)

for k = 1, 2, . . . , d, where αt > 0 is called base learning
rate and it is independent of stochastic gradient estimates
(g1,k, g2,k,··· , gt,k) for all t ≥ 1. Although RMSProp,
Adam, and Nadam work well for solving large-scale con-
vex and non-convex optimization problems such as training
deep neural networks, they have been pointed out to be di-
vergent in some scenarios via convex counterexamples [20].

11127

This ﬁnding thoroughly destroys the ﬂuke of a direct use of
these algorithms without any further assumptions or correc-
tions. Recently, developing sufﬁcient conditions to guarantee
global convergences of Adam and RMSProp-type algorithms
has attracted much attention from both machine learning and
optimization communities. The existing successful attempts
can be divided into four categories:

(C1) Decreasing a learning rate. Reddi et al. [20] have
declared that the core cause of divergences of Adam and
RMSProp is largely controlled by the difference between the
two adjacent learning rates, i.e.,

Γt = 1/ηt − 1/ηt−1 = √vt/αt − √vt−1/αt−1.

(5)

Once positive deﬁniteness of Γt is violated, Adam and RM-
SProp may suffer from divergence [20]. Based on this obser-
vation, two variants of Adam called AMSGrad and AdamNC
have been proposed with convergence guarantees in both the
convex [20] and non-convex [6] stochastic settings by re-
quiring Γt ≻ 0. In addition, Padam [24] extended from
AMSGrad has been proposed to contract the generalization
gap in training deep neural networks, whose convergence
has been ensured by requiring Γt ≻ 0.
In the strongly
convex stochastic setting, by using the long-term memory
technique developed in [20], Huang et al. [12] have pro-
posed NosAdam by attaching more weights on historical
second-order moments to ensure its convergence. Prior to
that, the convergence rate of RMSProp [19] has already been
established in the convex stochastic setting by employing
similar parameters to those of AdamNC [20].

(C2) Adopting a big batch size. Basu et al. [2], for the
ﬁrst time, showed that deterministic Adam and RMSProp
with original iteration schemes are actually convergent by
using full-batch gradient. On the other hand, both Adam
and RMSProp can be reshaped as speciﬁc signSGD-type

algorithms [1, 3] whose O(1/√T ) convergence rates have
[3]. Recently, Zaheer et al. [23] have established O(1/√T )

been provided in the non-convex stochastic setting by setting
batch size as large as the number of maximum iterations

convergence rate of original Adam directly in the non-convex
stochastic setting by requiring the batch size to be the same
order as the number of maximum iterations. We comment
that this type of requirement is impractical when Adam and
RMSProp are applied to tackle large-scale problems like (1),
since these approaches cost a huge number of computations
to estimate big-batch stochastic gradients in each iteration.

(C3) Incorporating a temporal decorrelation. By explor-
ing the structure of the convex counterexample in [20], Zhou
et al. [25] have pointed out that the divergence of RMSProp
is fundamentally caused by the unbalanced learning rate
rather than the absence of Γt ≻ 0. Based on this viewpoint,
Zhou et al. [25] have proposed AdaShift by incorporating a
temporal decorrelation technique to eliminate the inappro-
priate correlation between vt,k and the current second-order

moment g2
t,k, in which the adaptive learning rate ηt,k is re-
quired to be independent of g2
t,k. However, convergence
of AdaShift in [25] was merely restricted to RMSProp for
solving the convex counterexample in [20].

(C4) Seeking an analogous surrogate. Due to the diver-
gences of Adam and RMSProp [20], Zou et al. [26] recently
proposed a class of new surrogates called AdaUSM to ap-
proximate Adam and RMSProp by integrating weighted
AdaGrad with uniﬁed heavy ball and Nesterov accelerated

gradient momentums. Its O(log (T )/√T ) convergence rate

has also been provided in the non-convex stochastic setting
by requiring a non-decreasing weighted sequence. Besides,
many other adaptive stochastic algorithms without combin-
ing momentums, such as AdaGrad [22, 18] and stagewise
AdaGrad [7], have been guaranteed to be convergent and
work well in the non-convex stochastic setting.

In contrast with the above four types of modiﬁcations
and restrictions, we introduce an alternative easy-to-check
sufﬁcient condition (abbreviated as (SC)) to guarantee the
global convergences of original Adam and RMSProp. The
proposed (SC) merely depends on the parameters in estimat-
ing vt,k and base learning rate αt. (SC) neither requires the
positive deﬁniteness of Γt like (C1) nor needs the batch size
as large as the same order as the number of maximum itera-
tions like (C2) in both the convex and non-convex stochastic
settings. Thus, it is easier to verify and more practical com-
pared with (C1)-(C3). On the other hand, (SC) is partially
overlapped with (C1) since the proposed (SC) can cover
AdamNC [20], AdaGrad with exponential moving average
(AdaEMA) momentum [6], and RMSProp [19] as instances
whose convergences are all originally motivated by requir-
ing the positive deﬁniteness of Γt. While, based on (SC),
we can directly derive their global convergences in the non-
convex stochastic setting as byproducts without checking
the positive deﬁniteness of Γt step by step. Besides, (SC)
can serve as an alternative explanation on divergences of
original Adam and RMSProp, which are possibly due to
incorrect parameter settings for accumulating the historical
second-order moments rather than the unbalanced learning
rate caused by the inappropriate correlation between vt,k
and g2
t,k like (C3). In addition, AdamNC and AdaEMA are
convergent under (SC), but violate (C3) in each iteration.

Moreover, by carefully reshaping the iteration scheme
of Adam, we obtain a speciﬁc weighted AdaGrad with ex-
ponential moving average momentum, which extends the
weighted AdaGrad with heavy ball momentum and Nesterov
accelerated gradient momentum [26] in two aspects: the new
momentum mechanism and the new base learning rate set-
ting provide a new perspective for understanding Adam and
RMSProp. At last, we experimentally verify (SC) by apply-
ing Adam and RMSProp with different parameter settings
to solve the counterexample [20] and train deep neural net-
works including LeNet [16] and ResNet [10]. In summary,

11128

the contributions of this work are ﬁve-fold:

(1) We introduce an easy-to-check sufﬁcient condition to
ensure the global convergences of original Adam and
RMSProp in the non-convex stochastic setting. More-
over, this sufﬁcient condition is distinctive from the
existing conditions (C1)-(C4) and is easier to verify.

(2) We reshape Adam as weighted AdaGrad with exponen-
tial moving average momentum, which provides a new
perspective for understanding Adam and RMSProp and
also complements AdaUSM in [26].

(3) We provide a new explanation on the divergences of
original Adam and RMSProp, which are possibly due
to an incorrect parameter setting of the combinations of
historical second-order moments based on (SC).

(4) We ﬁnd that the sufﬁcient condition extends the re-
strictions of RMSProp [19] and covers many conver-
gent variants of Adam, e.g., AdamNC, AdaGrad with
momentum, etc. Thus, their convergences in the non-
convex stochastic setting naturally hold.

(5) We conduct experiments to validate the sufﬁcient con-
dition for the convergences of Adam/RMSProp. The
experimental results match our theoretical results.

2. Generic Adam

For readers’ convenience, we ﬁrst clarify a few necessary
notations used in the forthcoming Generic Adam. We denote

xt,k as the k-th component of xt ∈ Rd, and gt,k as the k-th
component of the stochastic gradient at the t-th iteration, and
call αt > 0 base learning rate and βt momentum parameter,
respectively. Let ǫ > 0 be a sufﬁciently small constant.
Denote 0 = (0,··· , 0)⊤ ∈ Rd, and ǫ = (ǫ,··· , ǫ)⊤ ∈ Rd.

All operations, such as multiplying, dividing, and taking
square root, are executed in the coordinate-wise sense.

Algorithm 1 Generic Adam
1: Parameters: Choose {αt}, {βt}, and {θt}. Choose
2: for t = 1, 2, . . . , T do
3:

x1 ∈ Rd and set initial values m0 = 0 and v0 = ǫ.

Sample a stochastic gradient gt;
for k = 1, 2, . . . , d do

4:

5:

6:

7:

end for

8:
9: end for

vt,k = θtvt−1,k + (1 − θt)g2
t,k;
mt,k = βtmt−1,k + (1 − βt)gt,k;
xt+1,k = xt,k − αtmt,k/√vt,k;

Generic Adam covers RMSProp by setting βt = 0. More-
over, it covers Adam with a bias correction [14] as follows:

Remark 1. The original Adam with the bias correction [14]
takes constant parameters βt = β and θt = θ. The iteration

scheme is written as xt+1 = xt−bαt cmt√bvt
, withcmt = mt
1−βt
√1−θt
andbvt = vt
1−βt . Then, the above can be
rewritten as xt+1 = xt − αtmt/√vt. Thus, it is equivalent
to taking constant βt, constant θt, and new base learning
rate αt in Generic Adam.

1−θt . Let αt =bαt

2.1. Weighted AdaGrad Perspective

Now we show that Generic Adam can be reformulated as
a new type of weighted AdaGrad algorithms with exponen-
tial moving average momentum (Weighted AdaEMA).

Algorithm 2 Weighted AdaEMA
1: Parameters: Choose parameters {αt}, momentum fac-
tors {βt}, and weights {wt}. Set W0 = 1, m0 = 0,
V0 = ǫ, and x1 ∈ Rn.
2: for t = 1, 2, . . . , T do
3:

Sample a stochastic gradient gt;

4: Wt = Wt−1 + wt;

for k = 1, 2, . . . , d do

5:

6:

7:

8:

t,k;

Vt,k = Vt−1,k + wtg2
mt,k = βtmt−1,k + (1 − βt)gt,k;

xt+1,k = xt,k − αtmt,k/pVt,k/Wt;

end for

9:
10: end for

Remark 2. The Weighted AdaEMA is a natural generaliza-
tion of the AdaGrad algorithm. The classical AdaGrad is to
take the weights wt = 1, the momentum factors βt = 0, and
the parameters αt = η/√t + 1 for constant η.

The following proposition states the equivalence between

Generic Adam and Weighted AdaEMA.

Proposition 3. Algorithm 1 and Algorithm 2 are equivalent.

The divergence issue of Adam/RMSProp. When θt is
taken constant, i.e., θt = θ, Reddi et al. [20] have pointed
out that Adam and RMSProp (βt = 0) can be divergent even
in the convex setting. They conjectured that the divergence
is possibly due to the uncertainty of positive deﬁniteness of
Γt in Eq. (5). This idea has motivated many new convergent
variants of Adam by forcing Γt ≻ 0. Recently, Zhou et al.
[25] further argued that the nature of divergences of Adam
and RMSProp is possibly due to the unbalanced learning
rate ηt caused by the inappropriate correlation between vt,k
and g2
t,k by studying the counterexample in [20]. However,
this explanation can be violated by many existing conver-
gent Adam-type algorithms such as AdamNC, NosAdam
[12], etc. So far, there is no satisfactory explanation for the
core reason of the divergence issue. We will provide more
insights in Section 4 based on our theoretical analysis.

11129

3. Main Results

In this section, we characterize the upper-bound of gra-
dient residual of problem (1) as a function of parameters
(θt, αt). Then the convergence rate of Generic Adam is de-
rived directly by specifying appropriate parameters (θt, αt).
Below, we state the necessary assumptions that are com-
monly used for analyzing the convergence of a stochastic
algorithm for non-convex problems:

(A1) The minimum value of problem (1) is lower-bounded,

i.e., f∗ = minx∈Rd f (x) > −∞;

(A2) The gradient of f is L-Lipschitz continuous, i.e.,

k∇f (x) − ∇f (y)k ≤ Lkx − yk, ∀x, y ∈ Rd;

(A3) The stochastic gradient gt is an unbiased estimate, i.e.,

E [gt] = ∇ft(xt);

(A4) The second-order moment of stochastic gradient gt is

uniformly upper-bounded, i.e., Ekgtk2 ≤ G.

To establish the upper-bound, we also suppose that the

parameters {βt}, {θt}, and {αt} satisfy the restrictions:
(R1) The parameters {βt} satisfy 0 ≤ βt ≤ β < 1 for all t

for some constant β;

(R2) The parameters {θt} satisfy 0 < θt < 1 and θt is

non-decreasing in t with θ := limt→∞ θt > β2;
(R3) The parameters {αt} satisfy that χt := αt√1−θt

is “al-
most” non-increasing in t, by which we mean that there
exist a non-increasing sequence {at} and a positive con-
stant C0 independent of t such that at ≤ χt ≤ C0at.
The restriction (R3) indeed says that χt is the prod-
uct between some non-increasing sequence {at} and some
bounded sequence. This is a slight generalization of χt it-
self being non-decreasing. If χt itself is non-increasing, we
can then take at = χt and C0 = 1. For most of the well-
known Adam-type methods, χt is indeed non-decreasing,
for instance, for AdaGrad with EMA momentum we have

αt = η/√t and θt = 1−1/t, so χt = η is constant; for Adam
with constant θt = θ and non-increasing αt (say αt = η/√t
or αt = η), χt = αt/√1 − θ is non-increasing. The motiva-

tion, instead of χt being decreasing, is that it allows us to
deal with the bias correction steps in Adam [14].

We ﬁx a positive constant θ′ > 01 such that β2 < θ′ < θ.

Let γ := β2/θ′ < 1 and

C1 :=QN

j=1(cid:0) θj
θ′(cid:1),

(6)

where N is the maximum of the indices j with θj < θ′. The
ﬁniteness of N is guaranteed by the fact that limt→∞ θt =
θ > θ′. When there are no such indices, i.e., θ1 ≥ θ′, we
take C1 = 1 by convention. In general, C1 ≤ 1. Our main
results on estimating gradient residual state as follows:

1In the special case that θt = θ is constant, we can directly set θ′

= θ.

Theorem 4. Let {xt} be a sequence generated by Generic
Adam for initial values x1, m0 = 0, and v0 = ǫ. Assume
that f and stochastic gradients gt satisfy assumptions (A1)-
(A4). Let τ be randomly chosen from {1, 2, . . . , T} with
equal probabilities pτ = 1/T . We have

t=1 αt√1 − θt

,

(cid:16)Ehk∇f (xτ )k4/3i(cid:17)3/2

where C′ = 2C 2

2C0√G2 +ǫd

C =

≤

T αT

C + C′PT
0 C3d√G2 +ǫd(cid:14)[(1−β)θ1] and
(cid:2)(C4 +C3C0dχ1 log(cid:0)1+
C1(1−√γ)2 + 2(cid:0)
β/(1−β)
√C1(1−γ)θ1

1 − β

√C1(1−√γ)(cid:2) C 2

where C4 and C3 are deﬁned as C4 = f (x1) − f∗ and
C3 =

0 χ1L

C0

G2

ǫd(cid:1)(cid:3),
+ 1(cid:1)2
G(cid:3).

Theorem 5. Suppose the same setting and hypothesis as
Theorem 4. Let τ be randomly chosen from {1, 2, . . . , T}
with equal probabilities pτ = 1/T . Then for any δ > 0, the
following bound holds with probability at least 1 − δ2/3:

k∇f (xτ )k2 ≤

C + C′PT

δT αT

t=1 αt√1 − θt

:= Bound(T ),

where C and C′ are deﬁned as those in Theorem 4.

4

3

3 ,

Remark 6. (i) The constants C and C′ depend on apriori
known constants C0, C1, β, θ′, G, L, ǫ, d, f∗ and θ1, α1, x1.
(ii) Convergence in expectation in Theorem 4 is slightly
stronger than convergence in probability in Theorem 5. Con-
2 ])
vergence in expectation is on the term (E[k∇f (xτ )k
which is slightly weaker than E[k∇f (xτ )k2]. The latter

is adopted for most SGD variants with global learning
rates, namely, the learning rate for each coordinate is the
same. This is due to that

t=1αt k∇f (xt)k2 is
exactly E[k∇f (xτ )k2] if τ is randomly selected via dis-
tribution P(τ = k) = αkPT
coordinate-wise adaptive methods because the learning rate
for each coordinate is different, and hence unable to ran-
domly select an index according to some distribution uni-
form for each coordinate. On the other hand, the proofs of
AMSGrad and AdaEMA [6] are able to achieve the bound

. This does not apply to

EPT

1PT

t=1αt

t=1αt

for E[k∇f (xτ )k2]. This is due to the strong assumption
kgtk≤ G which results in a uniform lower bound for each
coordinate of the adaptive learning rate ηt,k ≥ αt/G. Thus,
the proof of AMSGrad [6] can be dealt with in a way similar
to the case of global learning rate. In our paper we use a
coordinate-wise adaptive learning rate and assume a weaker

assumption E[kgtk2]≤ G instead of kgtk2≤ G. To separate
the term k∇f (xt)k from k∇f (xt)k2
H¨older theorem to obtain a bound for (E[k∇f (xτ )k

, we can only apply the

2 ])

3 .

ˆηt

3

4

11130

Corollary 7. Take αt = η/ts with 0 ≤ s < 1. Suppose
limt→∞ θt = θ < 1. Then the Bound(T ) in Theorem 5 is
bounded from below by constants

Bound(T ) ≥

C′√1 − θ

δ

.

(7)

In particular, when θt = θ < 1, we have the following more
subtle estimate on lower and upper-bounds for Bound(T )

.

δ

C

C

δηT 1−s+

C′√1 − θ

≤ Bound(T )≤

C′√1−θ
δηT 1−s +
δ(1 − s)
Remark 8. (i) Corollary 7 shows that if limt→∞ θt = θ <
1, the bound in Theorem 5 is only O(1), hence not guaran-
teeing convergence. This result is not surprising as Adam
with constant θt has already shown to be divergent [20].
Hence, O(1) is its best convergence rate we can expect. We
will discuss this case in more details in Section 4.
(ii) Corollary 7 also indicates that in order to guarantee
convergence, the parameter has to satisfy limt→∞ θt = 1.
Although we do not assume this in our restrictions (R1)-(R3),
it turns out to be the consequence from our analysis. Note
that if β < 1 in (R1) and limt→∞ θt = 1, then the restriction
limt→∞ θt > β2 is automatically satisﬁed in (R2).

We are now ready to give the Sufﬁcient Condition (SC)

for convergence of Generic Adam based on Theorem 5.

Corollary 9 (Sufﬁcient Condition(SC)). Generic Adam is
convergent if the parameters {αt}, {βt}, and {θt} satisfy
1. βt ≤ β < 1;
2. 0 < θt < 1 and θt is non-decreasing in t;
3. χt := αt/√1 − θt is “almost” non-increasing;
4. (cid:0)PT

t=1 αt√1 − θt(cid:1)(cid:14)(cid:0)T αT(cid:1) = o(1).

3.1. Convergence Rate of Generic Adam

We now provide the convergence rate of Generic Adam

with a speciﬁc class of parameters {(θt, αt)}, i.e.,

αt = η/ts and θt =(1 − α/K r,
1 − α/tr,

t < K,
t ≥ K,

(8)

for positive constants α, η, K, where K is taken such that
α/K r < 1. Note that α can be taken bigger than 1. When
α < 1, we can take K = 1 and then θt = 1 − α/tr, t ≥ 1.
To guarantee (R3), we require r ≤ 2s. For such a family of
parameters we have the following corollary.

Corollary 10. Generic Adam with the above family of pa-
rameters converges as long as 0 < r ≤ 2s < 2, and its
non-asymptotic convergence rate is given by

k∇f (xτ )k2 ≤

O(T −r/2),
O(log(T )/T 1−s),
O(1/T 1−s),

r/2 + s < 1

r/2 + s = 1

.

r/2 + s > 1

Remark 11. Corollary 10 recovers and extends the results
of some well-known algorithms below:

• AdaGrad with exponential moving average (EMA).
When θt = 1 − 1/t, αt = η/√t, and βt = β < 1,
Generic Adam is exactly AdaGrad with EMA momen-
tum (AdaEMA) [6]. In particular, if β = 0, this is the
vanilla coordinate-wise AdaGrad. It corresponds to
taking r = 1 and s = 1/2 in Corollary 10. Hence,
AdaEMA has convergence rate log(T )/√T .
• AdamNC. Taking θt = 1 − 1/t, αt = η/√t, and βt =
Its O(log (T )/√T ) convergence rate can be directly

βλt in Generic Adam, where λ < 1 is the decay factor
for the momentum factors βt, we recover AdamNC [20].

derived via Corollary 10.

• RMSProp. Mukkamala and Hein [19] have reached the
same O(log (T )/√T ) convergence rate for RMSprop
with θt = 1 − α/t, when 0 < α ≤ 1 and αt = η/√t

under the convex assumption. Since RMSProp is essen-
tially Generic Adam with all momentum factors βt = 0,
we recover Mukkamala and Hein’s results by taking
r = 1 and s = 1/2 in Corollary 10. Moreover, our
result generalizes to the non-convex stochastic setting,
and it holds for all α > 0 rather than only 0 < α≤ 1.
As Weighted AdaEMA is equivalent to Generic Adam,
we present its convergence rate with speciﬁc polynomial
growth weights in the following corollary.

Corollary 12. Suppose in Weighted AdaEMA the weights

wt = tr for r≥ 0, and αt = η/√t. Then Weighted AdaEMA
has the O(log(T )/√T ) non-asymptotic convergence rate.

Remark 13. Zou et al. [26] proposed weighted AdaGrad
with a uniﬁed momentum form which incorporates Heavy
Ball (HB) momentum and Nesterov Accelerated Gradients
(NAG) momentum. The same convergence rate was estab-
lished for weights with polynomial growth. Our result com-
plements [26] by showing that the same convergence rate
also holds for exponential moving average momentum.

Remark 14. (i) Huang et al. [12] proposed Nostalgic Adam
(NosAdam) which corresponds to taking the learning rate
αt = η/√t and θt = Bt−1/Bt with Bt = Pt
i=1 bi for
bi > 0, i ≥ 0, and B0 > 02 in Generic Adam. The idea
of NosAdam is to guarantee Γt ≻ 0 by laying more weights
on the historical second-order moments. A special case of
NosAdam is NosAdam-HH which takes Bt =Pt
i=1 i−r for
r ≥ 0 as the hyper-harmonic series. Its O(1/√T ) conver-
taking αt = η/√t and wt = t−r for r ≥ 0.

gence rate is established in the strongly convex stochastic set-
ting. NosAdam-HH can be viewed as the Weighted AdaEMA

2We directly use Bt and bi along with the notations of NosAdam [12].

11131

(ii) Corollary 12 differs from the motivation of NosAdam as

the weights we consider are wt = tr for r ≥ 0. Note that
in both cases when r = 0, this is the AdaGrad algorithm,
which corresponds to assigning equal weights to the past
squares of gradients. Hence, we are actually in the opposite
direction of NosAdam. We are more interested in the case
of assigning more weights to the recent stochastic gradients.
This can actually be viewed as a situation between AdaGrad
and the original Adam with constant θt’s.

Comparison between (SC) and (C1). Most of the conver-
gent modiﬁcations of original Adam, such as AMSGrad,
AdamNC, and NosAdam, all require Γt ≻ 0 in Eq. (5),
which is equivalent to decreasing the adaptive learning rate
ηt step by step. Since the term Γt (or adaptive learning
rate ηt) involves the past stochastic gradients (hence not
deterministic), the modiﬁcation to guarantee Γt ≻ 0 either
needs to change the iteration scheme of Adam (like AMS-
Grad) or needs to impose some strong restrictions on the
base learning rate αt and θt (like AdamNC). Our sufﬁcient
condition provides an easy-to-check criterion for the conver-
gence of Generic Adam in Corollary 9. It is not necessary to
require Γt ≻ 0. Moreover, we use exactly the same iteration
scheme as original Adam without any modiﬁcations. Our
work shows that the positive deﬁniteness of Γt may not be
the essential issue for divergence of original Adam. It is
probably due to that the parameters are not set correctly.

4. Constant θt case: insights for divergence

The currently most popular RMSProp and Adam’s pa-
rameter setting takes constant θt, i.e., θt = θ < 1. The
motivation behind is to use the exponential moving average
of squares of past stochastic gradients. In practice, parameter
θ is recommended to be set very close to 1. For instance, a
commonly adopted θ is taken as 0.999.

Although great performance in practice has been ob-
served, such a constant parameter setting has the serious
ﬂaw that there is no convergence guarantee even for convex
optimization, as proved by the counterexamples in [20]. Ever
since much work has been done to analyze the divergence
issue of Adam and to propose modiﬁcations with conver-
gence guarantees, as summarized in the introduction section.
However, there is still not a satisfactory explanation that
touches the fundamental reason of the divergence. In this
section, we try to provide more insights for the divergence
issue of Adam/RMSProp with constant parameter θt, based
on our analysis of the sufﬁcient condition for convergence.
From the sufﬁcient condition perspective. Let αt = η/ts
for 0 ≤ s < 1 and θt = θ < 1. According to Corollary 7,
Bound(T ) in Theorem 5 has the following estimate:
C′√1−θ
δ(1− s)

≤ Bound(T )≤

C′√1−θ

δηT 1−s +

δηT 1−s +

C

C

δ

.

The bounds tell us some points on Adam with constant θt:

1. Bound(T ) =O(1), so the convergence is not guaran-
teed. This result coincides with the divergence issue
demonstrated in [20]. Indeed, since in this case Adam
is not convergent, this is the best bound we can have.

2. Consider the dependence on parameter s. The bound
is decreasing in s. The best bound in this case is when
s = 0, i.e., the base learning rate is taken constant.
This explains why in practice taking a more aggressive
constant base learning rate often leads to even better
performance, comparing with taking a decaying one.

3. Consider the dependence on parameter θ. Note that the
constants C and C′ depend on θ1 instead of the whole
sequence θt. We can always set θt = θ for t ≥ 2 while
ﬁx θ1 < θ, by which we can take C and C′ independent
of constant θ. Then the principal term of Bound(T )
is linear in √1 − θ, so decreases to zero as θ → 1.

This explains why setting θ close to 1 in practice often
results in better performance in practice.

t } with r ∈ [0, 1]:

Moreover, Corollary 10 shows us how the convergence
rate continuously changes when we continuously verify pa-

rameters θt. Let us ﬁx αt = 1/√t and consider the following
continuous family of parameters {θ(r)
θ(r)
t = 1−α(r)/tr, where α(r) = r ¯θ+(1− ¯θ), 0 < ¯θ < 1.
Note that when r = 1, then θt = 1−1/t, this is the AdaEMA,
which has the convergence rate O(log T /√T ); when r = 0,
then θt = ¯θ < 1, this is the original Adam with constant θt,
which only has the O(1) bound; when 0 < r < 1, by Corol-
lary 10, the algorithm has the O(T −r/2) convergence rate.
Along this continuous family of parameters, we observe that
the theoretical convergence rate continuously deteriorates as
the real parameter r decreases from 1 to 0, namely, as we
gradually shift from AdaEMA to Adam with constant θt. In
the limiting case, the latter is not guaranteed with conver-
gence any more. This phenomenon is empirically veriﬁed
by the Synthetic Counterexample in Section 5.

From the Weighted AdaEMA perspective. Since Generic
Adam is equivalent to Weighted AdaEMA, we can examine
Adam with θt = θ < 1 in terms of Weighted AdaEMA. In
this case, we ﬁnd that the associated sequence of weights

lary 12 shows that as long as the weights are in polyno-
mial growth, Weighted AdaEMA is convergent and its con-

wt = (1 − θ)θ−t is growing in an exponential order. Corol-
vergence rate is O(log T /√T ). This indicates that the

exponential-moving-average technique in the estimate of
second-order moments may assign a too aggressive weight
to the current gradient, which leads to the divergence.

11132

r=0.0
r=0.25
r=0.5
r=0.75
r=1.0

0e6

2e6

4e6

iters
(a)

6e6

8e6

10e6

r=0.0
r=0.25
r=0.5
r=0.75
r=1.0

0.8

0.6

t
/
t

R

0.4

0.2

0.0

1.0

0.5

t

x

0.0

-0.5

-1.0

t
/
t

R

0.6

0.4

0.2

0.0

1.0

0.5

t

x

0.0

-0.5

-1.0

r=0.0
r=0.25

s=0.8
s=0.6
s=0.4

0.2

t
/
t

R

0.0

0e6

2e6

4e6

iters
(b)

6e6

8e6

10e6

0e6

2e6

6e6

8e6

10e6

4e6

iters
(c)

s=0.8
s=0.6
s=0.4

r=0.0
r=0.25

1.0

0.5

t

x

0.0

-0.5

-1.0

0e6

2e6

4e6

iters
(d)

6e6

8e6

10e6

0e6

2e6

6e6

8e6

10e6

0e6

2e6

4e6

iters
(e)

6e6

8e6

10e6

4e6

iters
(f)

Figure 1. The above ﬁgures are for average regret and x values with different r and s values, respectively. Figures (a) and (d) plot the
performance proﬁles of Generic Adam with different r values. Figures (b) and (e) plot the performance proﬁles of Generic Adam with
θ(r)
t = 1 − 0.01

tr and r = 0 and 0.25. Figures (c) and (f) plot the performance proﬁles of Generic Adam with different s values.

5. Experiments

In this section, we experimentally validate the proposed
sufﬁcient condition by applying Generic Adam and RM-
SProp to solve the counterexample [20] and to train LeNet
[16] on the MNIST dataset [17] and ResNet [10] on the
CIFAR-100 dataset [15], respectively.

5.1. Synthetic Counterexample

In this experiment, we verify the phenomenon described
in Section 4 that how the convergence rate of Generic Adam
gradually changes along a continuous path of families of
parameters on the one-dimensional counterexample in [20]:

TPt=1

TPt=1

R(T ) =

ft(xt) − min
x∈[−1,1]

ft(x),

(9)

where T is the number of maximum iterations, ft(x) =
1010x with probability 0.01, and ft(x) = 10x with probabil-
ity 0.99.
Sensitivity of parameter r. We set T = 107, αt =
0.5/√t, β = 0.9, and θt as θ(r)
t = 1 − (0.01 + 0.99r)/tr
with r ∈ {0, 0.25, 0.5, 0.75, 1.0}, respectively. Note that
when r = 0, Generic Adam reduces to the originally diver-
gent Adam [14] with (β, ¯θ) = (0.9, 0.99). When r = 1,
Generic Adam reduces to the AdaEMA [6] with β = 0.9.

The experimental results are shown in Figures 1(a) and
1(d). We can see that for r = 1.0, 0.75, and 0.5, Generic

Adam is convergent. Moreover, the convergence becomes
slower when r decreases, which exactly matches Corollary
10. On the other hand, for r = 0 and 0.25, Figure 1(d) shows
that they do not converge. It seems that the divergence for
r = 0.25 contradicts our theory. However, this is because
when r is very small, the O(T −r/2) convergence rate is so
slow that we may not see a convergent trend in even 107
iterations. Indeed, for r = 0.25, we actually have

θ(0.25)
t

≤ 1 − (0.01 + 0.25 ∗ 0.99)/107∗0.25 ≈ 0.9954,

which is not sufﬁciently close to 1. As a complementary
experiment, we ﬁx the numerator and only change r when
r is small. We take αt and βt as the same, while θ(r)
t =
1 − 0.01
for r = 0 and 0.25, respectively. The result is
shown in Figures 1(b) and 1(e). We can see that Generic
Adam with r = 0.25 is indeed convergent in this situation.

tr

Sensitivity of parameter s. Now, we show the sensitivity
of s of the sufﬁcient condition (SC) by ﬁxing r = 0.8 and
selecting s from the collection s = {0.4, 0.6, 0.8}. Figures
1(c) and 1(e) illustrate the sensitivity of parameter s when
Generic Adam is applied to solve the counterexample (9).
The performance shows that when s is ﬁxed, smaller r can
lead to a faster and better convergence speed, which also
coincides with the convergence results in Corollary 10.

11133

1.5

1

s
s
o

l
 

i

g
n
n
a
r
t

i

AMSGrad
RMSProp
GAdam (r=0)
GAdam (r=0.25)
GAdam (r=0.50)
GAdam (r=0.75)
GAdam (r=1)

)

%

(
 
y
c
a
r
u
c
c
a

 
t
s
e

t

100

98

96

94

92

90

88

86

s
s
o

l
 
t
s
e

t

0.6

0.5

0.4

0.3

0.2

0.1

0

0

AMSGrad
RMSProp
GAdam (r=0)
GAdam (r=0.25)
GAdam (r=0.50)
GAdam (r=0.75)
GAdam (r=1)

0

20

40

60

80

100

epochs

(b)

AMSGrad
RMSProp
GAdam (r=0)
GAdam (r=0.25)
GAdam (r=0.50)
GAdam (r=0.75)
GAdam (r=1)

20

40

60

80

100

epochs

(c)

0.5

0

0

20

40

60

80

100

epochs

(a)

Figure 2. Performance proﬁles of Generic Adam with r = {0, 0.25, 0.5, 0.75, 1}, RMSProp, and AMSGrad for training LeNet on the
MNIST dataset. Figures (a), (b), and (c) illustrate training loss vs. epochs, test accuracy vs. epochs, and test loss vs. epochs, respectively.

s
s
o

l
 

i

g
n
n
a
r
t

i

5

4

3

2

1

0

AMSGrad
RMSProp
GAdam (r=0)
GAdam (r=0.25)
GAdam (r=0.50)
GAdam (r=0.75)
GAdam (r=1)

0

20

40

60

80

100

epochs

(a)

70

60

50

40

30

20

)

%

(
 
y
c
a
r
u
c
c
a

 
t
s
e

t

10

0

s
s
o

l
 
t
s
e

t

4

3.5

3

2.5

2

1.5

1

0

AMSGrad
RMSProp
GAdam (r=0)
GAdam (r=0.25)
GAdam (r=0.50)
GAdam (r=0.75)
GAdam (r=1)

20

40

60

80

100

epochs

(b)

AMSGrad
RMSProp
GAdam (r=0)
GAdam (r=0.25)
GAdam (r=0.50)
GAdam (r=0.75)
GAdam (r=1)

20

40

60

80

100

epochs

(c)

Figure 3. Performance proﬁles of Generic Adam with r = {0, 0.25, 0.5, 0.75, 1}, RMSProp, and AMSGrad for training ResNet on the
CIFAR-100 dataset. Figures (a), (b), and (c) illustrate training loss vs. epochs, test accuracy vs. epochs, and test loss vs. epochs, respectively.

5.2. LeNet on MNIST and ResNet 18 on CIFAR 100

In this subsection, we apply Generic Adam to train LeNet
on the MNIST dataset and ResNet-18 on the CIFAR-100
dataset, respectively, in order to validate the convergence
rates in Corollary 10. Meanwhile, the comparisons between
Generic Adam and AMSGrad [20, 6] are also provided to
distinguish their differences in training deep neural networks.
We illustrate the performance proﬁles in three aspects: train-
ing loss vs. epochs, test loss vs. epochs, and test accuracy vs.
epochs, respectively. Besides, the architectures of LeNet and
ResNet-18, and the statistics of the MNIST and CIFAR-100
datasets are described in the supplementary material.

In the experiments, for Generic Adam, we set θ(r)

t =
1 − (0.001 + 0.999r)/tr with r ∈ {0, 0.25, 0.5, 0.75, 1}
and βt = 0.9, respectively; for RMSProp, we set βt = 0
and θt = 1 − 1
t along with the parameter settings in [19].
For fairness, the base learning rates αt in Generic Adam,
RMSProp, and AMSGrad are all set as 0.001/√t. Figures 2

and 3 illustrate the results of Generic Adam with different
r, RMSProp, and AMSGrad for training LeNet on MNIST
and training ResNet-18 on CIFAR-100, respectively. We can
see that AMSGrad and Adam (Generic Adam with r = 0)
decrease the training loss slowest and show the worst test
accuracy among the compared optimizers. One possible

reason is due to the use of constant θ in AMSGrad and
original Adam. By Figures 2 and 3, we can observe that the
convergences of Generic Adam are extremely sensitive to
the choice of parameter θt. Larger r can contribute to a faster
convergence rate of Generic Adam, which corroborates the
theoretical result in Corollary 10. Additionally, the test
accuracies in Figures 2(b) and 3(b) indicate that a smaller
training loss can contribute to a higher test accuracy for
Generic Adam.

6. Conclusions

In this work, we delved into the convergences of Adam
and RMSProp, and presented an easy-to-check sufﬁcient
condition to guarantee their convergences in the non-convex
stochastic setting. This sufﬁcient condition merely depends
on the base learning rate αt and the linear combination pa-
rameter θt of second-order moments. Relying on this suf-
ﬁcient condition, we found that the divergences of Adam
and RMSProp are possibly due to the incorrect parameter
settings of αt and θt. In addition, we reformulated Adam as
weighted AdaGrad with exponential moving average momen-
tum, which provides a novel perspective for understanding
Adam and RMSProp. At last, the correctness of theoretical
results was also veriﬁed via the counterexample in [20] and
training deep neural networks on real-world datasets.

11134

References

[1] Lukas Balles and Philipp Hennig. Dissecting Adam:
The sign, magnitude and variance of stochastic gradi-
ents. In International Conference on Machine Learn-
ing, pages 404–413, 2018. 2

[2] Amitabh Basu, Soham De, Anirbit Mukherjee, and
Enayat Ullah. Convergence guarantees for RMSProp
and ADAM in non-convex optimization and their com-
parison to nesterov acceleration on autoencoders. arXiv
preprint arXiv:1807.06766, 2018. 2

[3] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzade-
nesheli, and Animashree Anandkumar. signSGD: Com-
pressed optimisation for non-convex problems. In In-
ternational Conference on Machine Learning, pages
560–569, 2018. 2

[4] Dimitri P Bertsekas. Nonlinear programming. Athena

scientiﬁc Belmont, 1999. 1

[5] L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Op-
timization methods for large-scale machine learning.
SIAM Review, 60(2):223–311, 2018. 1

[6] Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
On the convergence of a class of Adam-type algo-
rithms for non-convex optimization. arXiv preprint
arXiv:1808.02941, 2018. 1, 2, 4, 5, 7, 8

[7] Zaiyi Chen, Tianbao Yang, Jinfeng Yi, Bowen Zhou,
and Enhong Chen. Universal stagewise learning for
non-convex problems with convergence on averaged
solutions. arXiv preprint arXiv:1808.06296, 2018. 2

[8] Timothy Dozat. Incorporating Nesterov momentum
International Conference on Learning

into Adam.
Representations Workshop, 2016. 1

[9] John Duchi, Elad Hazan, and Yoram Singer. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12(Jul):2121–2159, 2011. 1

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 770–778, 2016.
2, 7

[11] Geoffrey Hinton, Nitish Srivastava, and Kevin Swer-
sky. Neural networks for machine learning lecture
6a overview of mini-batch gradient descent. page 14,
2012. 1

[12] Haiwen Huang, Chang Wang, and Bin Dong. Nostal-
gic Adam: Weighing more of the past gradients when
designing the adaptive learning rate. arXiv preprint
arXiv:1805.07557, 2018. 2, 3, 5

[13] Prateek Jain, Purushottam Kar, et al. Non-convex
optimization for machine learning. Foundations and

Trends R(cid:13) in Machine Learning, 10(3-4):142–336, 2017.
1

[14] Diederik P Kingma and Jimmy Ba. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. 1, 3, 4, 7

[15] A Krizhevsky. Learning multiple layers of features
from tiny images. Master’s thesis, University of Tront,
2009. 7

[16] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998. 2, 7

[17] Yann LeCun, Corinna Cortes, and Christopher JC
Burges. Mnist handwritten digit database. 2010. URL
http://yann. lecun. com/exdb/mnist, 2010. 7

[18] Xiaoyu Li and Francesco Orabona. On the convergence
of stochastic gradient descent with adaptive stepsizes.
arXiv preprint arXiv:1805.08114, 2018. 2

[19] Mahesh Chandra Mukkamala and Matthias Hein. Vari-
ants of RMSProp and Adagrad with logarithmic re-
gret bounds. In International Conference on Machine
Learning, pages 2545–2553, 2017. 2, 3, 5, 8

[20] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On
the convergence of Adam and beyond. In International
Conference on Learning Representations, 2018. 1, 2,
3, 5, 6, 7, 8

[21] Herbert Robbins and Sutton Monro. A stochastic ap-
In Herbert Robbins Selected

proximation method.
Papers, pages 102–109. Springer, 1985. 1

[22] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Ada-
Grad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. arXiv preprint
arXiv:1806.01811, 2018. 2

[23] Manzil Zaheer, Sashank Reddi, Devendra Sachan,
Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. In Advances in Neural
Information Processing Systems, 2018. 2

[24] Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and
Quanquan Gu. On the convergence of adaptive gradient
methods for nonconvex optimization. arXiv preprint
arXiv:1808.05671, 2018. 2

[25] Zhiming Zhou, Qingru Zhang, Guansong Lu, Hong-
wei Wang, Weinan Zhang, and Yong Yu. AdaShift:
Decorrelation and convergence of adaptive learning
rate methods. arXiv preprint arXiv:1810.00143, 2018.
2, 3

[26] Fangyu Zou and Li Shen. On the convergence of Ada-
Grad with momentum for training deep neural net-
works. arXiv preprint arXiv:1808.03408, 2018. 2, 3,
5

11135

