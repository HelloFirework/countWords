Multi-task Self-supervised Object Detection
via Recycling of Bounding Box Annotations

Wonhee Lee
Joonil Na Gunhee Kim
Seoul National University, Seoul, Korea

{wonhee, joonil}@vision.snu.ac.kr, gunhee@snu.ac.kr

http://vision.snu.ac.kr/projects/mtl-ssl-detection

Abstract

In spite of recent enormous success of deep convolu-
tional networks in object detection, they require a large
amount of bounding box annotations, which are often time-
consuming and error-prone to obtain. To make better use
of given limited labels, we propose a novel object detec-
tion approach that takes advantage of both multi-task learn-
ing (MTL) and self-supervised learning (SSL). We propose
a set of auxiliary tasks that help improve the accuracy of
object detection. They create their own labels by recycling
the bounding box labels (i.e. annotations of the main task)
in an SSL manner, and are jointly trained with the object
detection model in an MTL way. Our approach is inte-
grable with any region proposal based detection models.
We empirically validate that our approach effectively im-
proves detection performance on various architectures and
datasets. We test two state-of-the-art region proposal object
detectors, including Faster R-CNN [39] and R-FCN [10],
with three CNN backbones of ResNet-101 [22], Inception-
ResNet-v2 [45], and MobileNet [23] on two benchmark
datasets of PASCAL VOC [14] and COCO [30].

1. Introduction

Recently, there has been signiﬁcant progress in the ﬁeld
of object detection [39, 10, 21], leveraging deep convolu-
tional networks that can learn hierarchical feature represen-
tation of input images. However, training a practical ob-
ject detection model requires a large amount of bounding
box annotations, which are often time-consuming and error-
prone to obtain.

To mitigate the label shortage problem for deep neural
networks, many studies have been conducted; in the context
of object detection, multi-task learning and self-supervised
learning may be two exemplary approaches for the problem.
Multi-task learning (MTL) aims at jointly training multiple
relevant tasks with less annotations to improve the perfor-

mance of each task [33, 13, 46]. Its effectiveness has been
well studied; for example, it leads the effect of regular-
ization by providing inductive bias to each other [5]. It is
also proven that as the number of tasks increases, the up-
per bound of the number of annotated data for better gen-
eralization decreases [3]. One of the most successful exam-
ples where MTL is helpful for object detection is Mask R-
CNN [21], which enhances the performance of object detec-
tion by jointly performing an instance segmentation task.
However, one of its practical limitations is that segmenta-
tion mask labels, which are more expensive than bounding
box annotations, must be provided.

Self-supervised learning (SSL) aims at

training the
model from the annotations generated by itself with no
additional human effort [9, 42]. In object detection lit-
erature, SSL has been applied to replace ImageNet pre-
training [34, 53, 36, 25]. Its motivation is that creating
a large-scale database like ImageNet is highly challeng-
ing and requires a lot of human effort, so it tries to pre-
train the network from relevant tasks that do not require
human-annotated data such as Jigsaw puzzles [34] or col-
orization [53]. However, the performance of most SSL al-
gorithms is not as good as ImageNet pretraining, and thus it
mostly fail to deliver practical beneﬁt to object detection.

In this work, we propose a novel object detection ap-
proach that takes advantage of both multi-task learning
and self-supervised learning. We start from a normal su-
pervised object detection setting, where a region proposal
based detector (e.g. Faster R-CNN [39] and R-FCN [10])
is given and bounding box annotations are available from
the dataset. The key to our approach is to propose a set
of auxiliary tasks that are relevant but not identical to ob-
ject detection. They create their own labels by recycling the
bounding box labels (i.e. annotations of the main task) in an
SSL manner while regarding the bounding box as metadata.
Then these auxiliary tasks are jointly trained with the object
detection model in an MTL way. Our focus here is to im-
prove the performance of the main task (object detection)
rather than all main and auxiliary tasks.

14984

Although auxiliary tasks are supposed to stimulate the
main task to achieve better accuracy, it is hard to deﬁne
appropriate and valid auxiliary tasks that actually help. In
many cases, auxiliary task might be ineffective or even in-
terfere with the main task. We empirically ﬁnd three syn-
ergetic auxiliary tasks, including multiple-object, closeness
and foreground labeling tasks. Thorough experimental re-
sults show that the proposed auxiliary tasks are substantially
effective for accurate object detection.

The contributions of this work are outlined as follows.

1. To the best of our knowledge, this work is a ﬁrst at-
tempt to develop a multi-task self-supervised learning
approach for two-stage object detection models. Our
approach is orthogonal to any choice of region pro-
posal based detection models.

2. We design a set of three auxiliary tasks that help im-
prove the performance of object detection, while re-
using bounding box annotations without any additional
human effort. As far as we know, there has been no pre-
vious work to recycle bounding box annotations like
ours in the self-supervised learning literature.

3. We demonstrate the accuracy improvement of our ap-
proach in multiple architectural combinations. We test
two state-of-the-art region proposal detectors, includ-
ing Faster R-CNN [39] and R-FCN [10], with three
base CNN backbones of ResNet-101 [22], Inception-
ResNet-v2 [45] and MobileNet [23] on two benchmark
datasets of PASCAL VOC [14] and COCO [30].

2. Related Work

2.1. Multi task Learning (MTL)

The MTL trains related tasks together to overcome the
shortage of annotated data. It provides each task with in-
ductive bias [5] to trigger regularization effect between one
another, and decreases the upper-bound on the number of
annotated data for better generalization as the number of
tasks increases [3]. MTL has demonstrated its usefulness
in a number of computer vision tasks, including depth es-
timation and scene parsing [49], synthetic imagery gener-
ation [40, 50], attributes prediction [1], immediacy predic-
tion [8], person re-identiﬁcation [43], and facial action unit
detection [2].

Parameter Sharing. MTL methods can be classiﬁed
into two groups according to how to share the parameters
between different task models. In the hard parameter shar-
ing, all task models share the exact same feature extrac-
tor and perform its own task through each branch head.
Therefore, the main issue here is to design appropriate tasks
and objective functions. Some examples in this category in-
clude TCDCN [55], HyperFace [38], Mask R-CNN [21],
ResNetCrowd [31], LASSO architecture [13]. In the soft

parameter sharing, each task has its own model with its
own parameters. Hence, the methods in this category fo-
cus on how to design weight sharing methodology, such as
what constraints and distance metrics are utilized between
the parameters. Examples include cross-stitch network [33],
DCNet [46], cross connection [15], partially shared struc-
ture [4], Sluice Networks [41] and NDDR-CNN [16].

2.2. Self supervised Learning (SSL)

While it takes a lot of human effort to create high-quality
annotations for supervised learning, SSL [9, 42] creates la-
bels by models themselves without additional human effort.
In computer vision research, different types of information
have been adopted as a signal for SSL, including coloriza-
tion [53, 54], inpainting [37], spatial patches [12, 34] or
temporal clues [29, 44, 47], text [6, 19, 26], sound [35], op-
tical ﬂow [36] and tracking [27, 48].

Transfer Learning. One of the primary uses of SSL is
in transfer learning. In the ﬁeld of object detection, many
self-supervision tasks have been applied to replace Ima-
geNet pretraining. Noroozi et al. [34] propose a pretext task
to solve Jigsaw puzzles, and transfer the networks learned
for Jigsaw puzzles to solve object classiﬁcation and de-
tection. Pathak et al. [36] pretrain networks via motion-
based grouping cues on videos. Jenni and Favaro [25] in-
troduce a pretext task to differentiate between real and
artifact-containing images, and train the model in an adver-
sarial manner for transferring to object detection. Zhang et
al. [53] train a network for the colorization task and ﬁne-
tune it for the detection task. There have been many other
attempts to pretrain the network from relevant tasks in self-
supervised or unsupervised manner, but most of them still
have not reached the performance of ImageNet pretraining.
Our work is distinguishable from this line of work in that
we do not try to replace the ImageNet pretraining but intro-
duce a set of complementary auxiliary tasks that are train-
able with no additional annotations and improve the perfor-
mance of object detection.

Annotation Reuse. Gong et al. [20] and Zhan et al. [51]
show that reusing labels of one task is not only helpful to
create new tasks and their labels but also capable of improv-
ing the performance of the main task through pretraining.
They use pixel-wise segmentation masks as the annotation
to be reused in the context of human parsing and semantic
segmentation, respectively. On the other hand, our work fo-
cuses on recycling bounding box labels for object detection,
which has not been discussed yet.

3. Approach

We design a multi-task self-supervised learning model
for object detection. We assume that annotations are avail-
able only for the main task (i.e. bounding box labels for
object detection). We introduce a set of three auxiliary tasks

4985

Region Proposal 

Network

7×7×1024

ROI 

pooling

layer

1024

Nr×7×7×1024

Feature maps

Pooled feature

Proposal box

Window

Main task

Object 
detection

Closeness 

labeling

Multi-object 

labeling

Foreground 

labeling

Refinement

Box

Class

+

FC

K + 1

K + 1

Nr×(K + 1)

Predicted class distribution

Auxiliary tasks

Pooled feature

conv5_x

7×7×1024
Auxiliary task predictor

7×7×2048

Avg

FC

Class

2048

Figure 1: Overall architecture of our multi-task self-supervised approach. It shows how the object detector (i.e. main task
model) such as Faster R-CNN [39] makes a prediction for a given proposal box (red) with assistance of three auxiliary tasks
(section 3.1) at inference. The auxiliary task models (shown in the bottom right) are almost identical to the main task predictor
except no box regressor (section 3.2). The reﬁnement of detection prediction (shown in right) is also collectively done by
cooperation of the main and auxiliary task models (section 3.3). K is the number of categories.

Main task predictor

Box

that are jointly trained with the main task in a multi-task
learning manner. The auxiliary task models are free to recy-
cle bounding-box annotations for building their own ground
truth (GT) labels in a self-supervised way. Here our goal is
to improve the performance of the main task (object detec-
tion), instead of the averaged performance of all main and
auxiliary tasks. We discuss the details of the auxiliary tasks
and their models in section 3.1–3.2.

The proposed auxiliary tasks are beneﬁcial in both fea-
ture extraction and prediction. First, the three auxiliary tasks
enhance the quality of the shared features through cooper-
ative feature learning. Second, the outputs of the auxiliary
tasks provide contextual information to reﬁne the object de-
tection prediction, especially classiﬁcation accuracy of re-
gion proposals. We refer to this process as reﬁnement, which
will be discussed in section 3.3.

Figure 1 shows the overall architecture of our model,
where three auxiliary task models are integrated with a re-
gion proposal-based object detector (e.g. Faster R-CNN). It
shows how a single RoI is processed at inference time. We
will present the training of the whole model in section 3.4.

3.1. Auxiliary tasks

We below describe our three auxiliary tasks, including

multiple-object, closeness and foreground labeling tasks.

Multi-object labeling. The annotation for object detec-
tion normally consists of two types of information: i) the co-
ordinates of the smallest bounding box that encloses the tar-
get object and ii) one-hot encoding for a single correspond-
ing class. The ﬁrst auxiliary task named as multi-object la-
beling relaxes these two labeling conditions. It randomly

samples a bounding box window in the image and assigns a
soft label to it rather than a hard one-hot encoding, to be in-
terpreted as a probability of several classes in a single win-
dow. The key beneﬁt of this auxiliary task is to populate
many positive boxes even though their quality may not be
as good as that of GTs. Nonetheless, it can alleviate one im-
portant issue of the general object detection pipeline where
positive boxes are too few compared to negative boxes per
image. This has similar intention to mixup [52], which com-
bines pairs of images and their labels linearly.

Figure 2 shows some examples of windows for multi-
object labeling. We ﬁrst sample Nt number of windows
(e.g. Nt = 64 in our experiments) by randomly picking
their top-left corners and width/height at the minimum size
of 32. We constrain that the windows should have nonzero
intersection with any GT boxes in the image. We then obtain
a soft label lm for each window, following Algorithm 1. The
label lm acts as a GT annotation for the multi-object label-
ing task. Simply, we assign a class probability to a window
W , according to its area portion with GT classes. The length
of lm is K + 1, where K is the number of classes and lm[0]
denotes the no object probability that is proportional to the
background area in W .

Closeness labeling. The distribution of objects in an im-
age is not random. For example, in the PASCAL VOC im-
ages, there is likely to be chairs near a dining table and
cars near a bus. In the skip-gram model of natural language
processing [32], the model learns the meaning of a speciﬁc
word from those of surrounding words in a sentence. Like-
wise, if an auxiliary task enforces the model to learn to pre-
dict both the class and its surrounding classes using the fea-

4986

GT boxes 

Windows 

0.4 
0.5 

0.3 
0.4 

0.7 

0.6 

0.3 

0.3 

0.5 

1 

Binary 
mask 

1 

0.1 

0.5  0.4 

0 

1 

multi-object 

soft labels (     ) 

closeness 
labels (    ) 

 

  

foreground 
labels (    ) 

 

Figure 2: An example of how to generate labels of auxiliary
tasks via recycling of GT bounding boxes. The multi-object
soft label assigns the area portions occupied by each class’s
GT boxes within a window. The closeness label scores the
distances from the center of the GT box to those of other
GT boxes. The foreground label is a binary mask between
foreground and background.

tures of a region proposal, the learned features are likely to
encode the context information of the image region. This
may enable the model to predict the class of the box bet-
ter. We coin this auxiliary task as closeness labeling. Figure
2 and Algorithm 2 show how the closeness auxiliary task
recycles the GT boxes to obtain its own labels. Note that
the closeness label lc is only deﬁned for GT boxes, whereas
the previous multi-object label lm is for a randomly sam-
pled window. Therefore, the closeness auxiliary task pre-
dicts possible objects around the box, while the multi-object
labeling task predicts the possible objects within the win-
dow. The closeness label lc for each GT box b assigns a
higher value to the object whose GT box is closer to b. lc[0]
is 1 if there is no GT box nearby.

Foreground labeling. The ﬁnal auxiliary task, named as
foreground labeling, aims at predicting the foreground and
background regions in the entire image. This task can aid
the feature learning to be more accurate for the coordinate
regression of region proposals. As shown in Figure 2, it is
simple to generate a label lf for this task; we simply assign
1 to the GT box regions and 0 to the other regions.

3.2. Auxiliary Task Models

Algorithm 1: OBTAINING A MULTI-OBJECT LABEL

i=1, Window W

Input: Image I, GT boxes {Bi}K
Output: The multi-object soft label lm for W
lm ← An array of length (K + 1)
lm[0] ← parea(W ) − area((∪i∈K{Bi}))
for i ← 1 to K do

lm[i] ← parea(W ∩ {Bi})

return lm/sum(lm)

Algorithm 2: OBTAINING CLOSENESS LABEL

i=1, A GT box b

Input: Image I, GT boxes {Bi}K
Output: The closeness soft label lc for b
lc ← An array of zeros with a length (K + 1)
D ← The diagonal distance of I
if {B} − b = ∅ then

lc[0] ← 1

else

for i ← 1 to K do

lc[i] ← D − min

b′ ∈{Bi}

return lc/sum(lc)

kcenter(b) − center(b

′

)k2

the purpose of each task is different one another. Hence, we
design the models for auxiliary tasks to have the same archi-
tecture with the head of the main task model. For instance,
the lower part of Figure 1 shows the auxiliary task model for
Faster R-CNN [39] with ResNet-101 [22]. Only difference
between the main and auxiliary predictor is the existence of
box regression, which is unnecessary for auxiliary tasks.

Such architecture sharing is advantageous in several as-
pects. First, it makes our multi-task approach easily inte-
grable with object detection models, because the implemen-
tation of auxiliary tasks is straightforward. Second, it makes
it easy to initialize weights for auxiliary models by simply
duplicating those of the pretrained detector. Such replicate
initialization empirically leads to better performance than
training the heads of auxiliary tasks from scratch.

3.3. Detection Reﬁnement

The auxiliary tasks are beneﬁcial in both feature extrac-
tion and prediction. In the region proposal stage, the aux-
iliary tasks are jointly trained with the main task to learn
shared features that help object detection prior to RoI pool-
ing. In the prediction stage, the outputs of auxiliary tasks
can directly reﬁne the detection prediction, especially clas-
siﬁcation of region proposals. In this section, we discuss the
detection reﬁnement in the second stage.

As shown in Figure 2, the three auxiliary tasks eventually
predict class probability labels (i.e. lm, lc, lf ), even though

The multi-object labeling model can predict soft class
memberships for a given proposal and boxes surrounding

4987

Proposal 
box

we obtain

x

x'

x′ = reﬁne(x|c, m1, . . . , mNr )

(1)

= Wr[x, c, m1, . . . , mNr ] + x,

Object detection (Main)

All 
proposal 
boxes

Closeness labeling (Aux)

Windows

Multi-object labeling (Aux)

Avg.

c

…

FC layers

x

c m1 m2

…

m5

Concat.

m1
m2
m3
m4
m5

Figure 3: Detection reﬁnement. The main detection head
computes x as a classiﬁcation result for a proposal box. It
is updated into x′ using a single FC layer on the prediction
outputs of the two auxiliary models, c and m1, · · · , mr.

it. The closeness labeling model can predict the possible
co-occurrence of nearby objects even if they do not actu-
ally exist. The key idea of our detection reﬁnement is to
let the main task head (i.e. object detector) take advantage
of the predictions of the two auxiliary tasks, because they
can provide useful contextual information for the detector to
make better decision for classiﬁcation. That is, for a given
proposal box that the object detector needs to predict, the
multi-object model provides soft label prediction for the lo-
cal and global context around the box, while the closeness
model delivers predicted proximity to the surrounding ob-
jects. We here do not use the foreground labeling task’s out-
put because it has no additional information beyond the two
auxiliary tasks.

Figure 3 shows the process of reﬁnement. In a normal
object detector (e.g. Faster R-CNN [39]), the detection head
computes a classiﬁcation result x ∈ RK×1 for a given pro-
posal box, and passes it through a softmax layer to generate
a class probability y. Our reﬁnement updates x into x′ using
the auxiliary models’ outputs as follows.

First, in order to leverage the prediction by the learned
multi-object labeling model, we create Nr number of win-
dows (e.g. Nr = 5 in our experiments) surrounding the
proposal with various sizes. We set the sizes by dividing
the space between the whole image and the proposal box in
Nr −1 uniform intervals. We then obtain multi-object labels
of Nr windows, denoted by m1, · · · , mNr . Second, we ob-
tain the closeness labels for all proposals in the image, and
average them into a single vector denoted c. It is used as a
context summary of the image, which is empirically better
than using individual outputs for each proposal box. Finally,

where Wr is a projection matrix. In summary, we con-
catenate x, c and m1, · · · , mr, and feed it into a fully-
connected layer with a residual connection. The presented
reﬁnement model is designed after thorough validation; for
example, we tried multiple FC layers instead of Eq.(1), but
they were not as good as the single-layer version.

3.4. Training

Loss functions. We deﬁne the loss for each auxiliary
task as a cross-entropy loss, since they basically perform
prediction of class labels:

L∗ = −

1
N∗

N∗

X

j=1

y∗
j

T log(cid:0)softmax(a∗

j )(cid:1).

(2)

j = am

j and a∗

For the loss of multi-object labeling Lm, we set N∗ = Nt,
y∗
j = ym
j where Nt is the number of win-
dows, ym
is the GT soft label vector for the j-th window
j
and softmax(am
j ) is its predicted class probability by the
auxiliary head in section 3.2. For the loss of closeness label-
ing task Lc, we use Np as the number of positive proposal
boxes matched with GT boxes, yc
j) are GT
and predicted soft label vectors for the surrounding area of
the j-th box. Finally, for the foreground labeling task Lf ,
we use Nf as the number of pixels on the foreground mask,
yf
j and softmax(af
j ) as the GT and predicted foreground la-
bel of the j-th pixel.

j and softmax(ac

The overall auxiliary loss is the weighted sum of all task

losses:

Laux = λmLm + λcLc + λf Lf .

(3)

As the loss for the reﬁnement, we use the same crossover
entropy loss as in the main task for classiﬁcation. We also
apply stop gradient operation, which ensures that the re-
ﬁnement loss does not affect the predictor of each task and
the feature extractor. That is, since the main task and each
auxiliary task have their own losses, the reﬁnement loss up-
dates only the weights of the reﬁnement layers.

Finally, the overall loss Ltotal is the sum of the object
detection loss Lmain of the base detector, the auxiliary loss
Laux and the reﬁnement loss Lref . We set λm = λf =
λr = 1 and λc = 0.3 in our experiments.

Ltotal = Lmain + Laux + λrLref .

(4)

Training. We initialize the backbone CNN by ImageNet
pretraining [11, 18]. We then simultaneously train the whole
network using the GTs of both main and auxiliary tasks.

4988

We freeze conv1 and conv2 x layers in ResNet-101 [22]
for fast convergence. For MobileNet [23] and Inception-
ResNet-v2 [45], we do not freeze any layer.

Implementation. We resize images so that their mini-
mum size is 600. We use the TensorFlow object detection
API [24] for training. The crop and resize [7] method is
employed instead of ROI pooling operation [17]. We use
the momentum optimizer with a rate of 0.9 and a weight
decay of 0.0001. We use only random horizontal ﬂips for
data augmentation.

4. Experiments

Our approach is applicable to any two-stage object de-
tection models with region proposal. To show the general-
ity of our approach, we evaluate with various architectures
and datasets (section 4.1–4.2). We carry out ablation experi-
ments about the effect of multi-task learning and reﬁnement
(section 4.3) and show some qualitative results (section 4.4).
We present more results in the supplementary ﬁle.

4.1. Experimental Settings

Datasets. We test various conﬁgurations of datasets, fol-
lowing previous literature on object detection. We use three
training settings: VOC07 trainval, VOC07+12 trainval, and
COCO17 train, and four test settings: VOC07 test, VOC12
test, COCO17 val, COCO17 test-dev. More exact train-
ing/test splits are described in each table.

Object detection architecture. Our model is integrable
with any two-stage object detection model. We choose
Faster R-CNN [39] as one of the state-of-the-art detec-
tors, and R-FCN [10] as another fully convolutional region
proposal-based detection model.

Backbone CNNs. ResNet-101 [22] is one of the most
popular backbones on object detection. MobileNet [23] is a
lightweight efﬁcient architecture for mobile and embedded
applications. Inception-ResNet-v2 [45] is another state-of-
the-art network that has a bigger size than ResNet-101 and
attains better results in our experiments.

Evaluation metrics. We report the standard metrics
for each dataset:
the mean Average Precision (mAP)
for VOC [14], and mAP over IoU from 0.5 to 0.95
(mAP@[.5:.95]) for COCO [30].

4.2. Detection Results

Table 1 shows the detection improvement of our ap-
proach over the baseline on the two datasets. We use Faster
R-CNN with ResNet-101 as the baseline. More speciﬁcally,
we present the detailed performance for VOC07+12 train-
val and VOC12 test in Table 2 and for COCO17 train and
COCO17 test-dev in Table 3. That is, we present the detailed
detection accuracies over all 20 object classes of VOC in
Table 2, and multiple mAP over IoU values, performance

Dataset
Training
Test

Baseline
+ Task1
+ Task2
+ Task3
+ Task1,2
+ Task1,2,3

07
07

77.0
78.9
77.3
77.0
78.5
78.7

VOC

07+12

COCO
17 train

07

81.7
83.8
83.0
82.0
83.7
83.7

12

17 val

17 test-dev

75.3
77.4
76.0
75.1
77.3
77.5

32.7
34.1
33.3
32.9
34.5
34.6

32.8
34.2
33.5
32.8
34.6
34.7

Table 1: Detection accuracies (mAP (%)) on VOC and
COCO. Baseline is Faster R-CNN [39] with ResNet-
101 [22]. Task1,2,3 indicate multi-object, closeness and
foreground labeling auxiliary task, respectively.

variation according to object sizes and additional average
recall scores in Table 3. All the results assure that our aux-
iliary tasks consistently enhance the detection performance
in various dataset splits. Table 1 shows that mAP values in-
crease on average by about 2.0%p in VOC and about 1.9%p
in COCO. Encouragingly, our approach leads to better per-
formance in all 20 categories of VOC as in Table 2 and does
too in all metrics of precision and recall regardless of object
sizes in COCO as in Table 3.

Table 4 summarizes the performance variation according
to backbone networks. Fixing Faster R-CNN as the detec-
tion architecture, we test MobileNet and Inception-ResNet-
v2, in addition to ResNet-101 in Table 1. The performance
gains by our method are more signiﬁcant in the order of Mo-
bileNet, ResNet-101 and Inception-ResNet-v2. Given that
it is the reverse order of the backbone’s detection accu-
racy, the beneﬁt of our approach could be larger when the
network capability is inferior. Importantly, no matter what
backbone CNN is used, our approach consistently improves
the detection accuracy with substantial margins.

Table 5 reports the results for another object detection
architecture, R-FCN, which turns out to be slightly worse
than Faster R-CNN in Table 1. Still, we can see the same
pattern that our approach nontrivially increases the detec-
tion accuracy in all experiments of PASCAL VOC.

Our experiments show the following trends of results
about our approach. First, it is highly promising that our
approach is constantly effective, regardless of base detec-
tors, backbone CNNs and datasets. These results could suf-
ﬁciently validate the generality of our MTL SSL approach.
Second, the auxiliary tasks are individually more helpful
for object detection in the order of task 1 (multi-object),
2 (closeness), and 3 (foreground). The task 1 is the most
useful because it can generate many windows as needed,
compared to the other tasks that create only a ﬁxed handful
number of labels; the labels of task 2 per image are bounded
by the number of GT boxes and the label of task 3 is always

4989

Method

mAP

aero

bike

bird

boat

bottle bus

car

cat

chair cow table dog

horse mbike person plant

sheep sofa

train

tv

Baseline
+ Task1
+ Task2
+ Task3
+ Task1,2
+ Task1,2,3

75.3
77.4
76.0
75.1
77.3
77.5

86.2 83.0 78.0 62.8 59.9 78.0 81.2 90.7 56.4 79.5 56.1 88.2 83.3 83.8 84.9 53.9 81.9 66.7 83.5 68.9
88.0 84.3 79.9 63.6 60.7 79.8 82.6 93.2 58.0 84.5 59.4 91.5 86.3 86.5 86.0 56.7 84.8 67.3 84.8 71.0
86.1 84.4 77.8 63.2 58.9 78.5 81.8 91.2 57.3 81.5 57.7 89.1 84.9 84.7 85.7 54.2 81.7 67.5 83.6 70.0
86.2 82.3 76.8 61.6 59.5 78.5 81.4 90.3 56.1 79.3 57.4 88.4 83.9 83.3 85.2 54.1 80.8 65.2 82.9 68.6
87.7 84.3 79.6 62.9 59.9 80.1 82.5 92.8 57.6 83.5 58.5 91.3 86.8 85.9 85.4 57.8 85.1 70.1 84.9 70.2
87.6 84.4 80.4 63.4 61.2 79.1 82.6 92.6 57.7 84.3 59.3 91.4 87.1 86.0 86.0 57.9 84.1 68.7 85.7 70.3

Table 2: Detailed performance on VOC 2012 test. The mAP values over 20 object classes of PASCAL VOC are also reported.

Method

Baseline
+Task1
+Task2
+Task3
+Task1,2
+Task1,2,3

IoU=.50:.95

IoU=.50

IoU=.75

small medium large max=1 max=10 max=100

small medium large

Average Precision

Average Recall

32.8
34.2
33.5
32.8
34.6
34.7

52.7
55.2
54.1
52.6
55.6
55.8

34.7
36.1
35.4
34.6
36.6
36.6

13.3
14.1
14.0
13.2
14.4
14.5

36.1
37.7
36.7
35.9
38.1
38.1

47.1
49.7
48.0
46.8
50.0
50.0

29.5
29.7
29.5
29.5
29.7
29.9

46.3
46.6
46.3
46.3
46.6
46.7

48.7
49.1
48.7
48.6
49.2
49.2

24.1
23.9
24.2
24.1
24.1
24.5

53.3
53.9
53.4
53.0
53.9
53.9

69.3
70.6
69.2
69.3
70.2
70.3

Table 3: Detailed performance on COCO 2017 test-dev. The mAP metrics over multiple IoU values are reported. The results
are also separately shown for the subset of small (area ≤ 32 × 32), medium (32 × 32 < area ≤ 96 × 96) and large
(area > 96 × 96) objects. The average recall values are measured given {1, 10, 100} detections at maximum per image.

Backbone
Training
Test

Baseline
+ Task1
+ Task2
+ Task3
+ Task1,2
+ Task1,2,3

MobileNet [23]
07
07+12
07

07

12

Inception-ResNet-v2 [45]
07
07

07+12

07

12

61.2
63.4
62.5
61.3
63.9
63.8

68.6
71.3
69.3
68.8
70.9
70.8

62.0
64.5
62.6
61.7
64.5
64.4

80.7
81.7
81.0
80.6
81.8
81.8

84.3
85.9
84.8
84.2
86.1
86.0

78.2
80.5
79.0
78.3
80.1
80.0

Training
Test

Baseline
+ Task1
+ Task2
+ Task3
+ Task1,2
+ Task1,2,3

07
07

73.4
74.3
73.5
73.3
75.0
74.7

07+12

07

78.6
80.1
78.7
78.4
80.4
80.6

12

72.1
74.0
72.2
71.9
74.2
73.9

Table 4: Detection accuracies (mAP) with various backbone
networks on VOC. Baseline is Faster R-CNN [39].

Table 5: Detection accuracies (mAP) on VOC. Baseline is
R-FCN [10] with ResNet-101 [22] backbone.

one per image. The task 3 is the worst among the auxiliary
tasks since it is the simplest and may deliver the least infor-
mation. Third, when using all three tasks jointly, the results
are the best or closest to the best.

4.3. Ablation Experiments on Reﬁnement

We perform an ablation study about the effect of reﬁne-
ment. In normal MTL, the outputs of auxiliary tasks do not
directly reﬁne the results of the main task. On the other
hand, our auxiliary tasks can improve the classiﬁcation of
the main task, because they provide contextual information
about the surroundings of RoIs. Table 6 shows how much
detection accuracies are improved by the reﬁnement. The
mAP values increase by an average of 0.7, thanks to the ef-
fect of reﬁnement when compared to using MTL only.

To further investigate the effect of reﬁnement alone, we
apply the stop gradient to prevent the losses of auxiliary
tasks from affecting the learning of the shared features. Its
result is shown in the row (+Reﬁnement) of Table 6. The

Training
Test

07
07

07+12

07

12

77.0

Baseline
78.0 (+1.0) 83.0 (+1.3) 76.7 (+1.4)
+ MTL
+ Reﬁnement 78.3 (+1.3) 82.7 (+1.0) 76.4 (+1.1)
78.7 (+1.7) 83.7 (+2.0) 77.5 (+2.2)
+ Both

75.3

81.7

Table 6: Ablation results of multi-task learning and reﬁne-
ment on VOC. Baseline is Faster R-CNN with ResNet-101.

mAP increases by 1.2 on average compared to the base-
line, although the best performance is achieved with the fea-
ture learning together. These results assure that both feature
learning by MTL and inference reﬁnement are proﬁtable.

4.4. Qualitative Results

Figure 4 shows some qualitative examples of detection
improvement of our approach on VOC and COCO. In each
set, we show the result of the baseline (upper) and our ap-

4990

e
n

i
l

e
s
a
B

s
r
u
O

e
n

i
l

e
s
a
B

s
r
u
O

e
n

i
l

e
s
a
B

s
r
u
O

Figure 4: Comparison of detection between baseline (upper) and our approach (lower). Our approach improves the baseline’s
detection by correcting several false negatives and false positives such as background, similar object and redundant detection.

proach (lower). Our method is often able to correct several
false negatives and false positives such as background, sim-
ilar objects and redundant detection.

5. Conclusion

We proposed a novel multi-task self-supervised learn-
ing approach for object detection, where three auxiliary
tasks were designed to improve the performance of object
detection. They created their own labels by recycling the
bounding box labels, and were jointly trained with the ob-
ject detection model. Our experiments validate that our ap-
proach improves detection accuracies with various archi-
tectures and backbones. Our approach was helpful for de-
tection regardless of the dataset size, as it achieved con-

sistent improvement from small (VOC07, 25K objects) to
large (COCO, 850K objects) datasets in our experiments.

There are several possible directions beyond this work.
First, we could design auxiliary tasks that help box re-
gression while this work dealt with only classiﬁcation-
enhancing tasks. Second, auxiliary tasks can be extended
to recycle other labels such as segmentation masks for de-
tection improvement. Lastly, we may verify our method in
an extremely large dataset like OpenImages [28].

Acknowledgements. This work was supported by Sam-
sung Research Funding Center of Samsung Electronics un-
der Project Number SRFC-TC1603-01. Gunhee Kim is the
corresponding author.

4991

References

[1] A. H. Abdulnabi, G. Wang, J. Lu, and K. Jia. Multi-
Task CNN Model for Attribute Prediction. Multimedia,
17(11):1949–1959, 2015. 2

[2] T. Almaev, B. Martinez, and M. Valstar. Learning to Trans-
fer: Transferring Latent Task Structures and Its Application
to Person-Speciﬁc Facial Action Unit Detection. In ICCV,
2015. 2

[3] J. Baxter. A Bayesian/Information Theoretic Model of
Learning to Learn via Multiple Task Sampling. Machine
learning, 28(1):7–39, 1997. 1, 2

[4] J. Cao, Y. Li, and Z. Zhang. Partially Shared Multi-Task Con-
volutional Neural Network with Local Constraint for Face
Attribute Learning. In CVPR, 2018. 2

[5] R. Caruana. Multitask Learning: A Knowledge-Based

Source of Inductive Bias. In ICML, 1993. 1, 2

[6] X. Chen and A. Gupta. Webly Supervised Learning of Con-

volutional Networks. In ICCV, 2015. 2

[7] X. Chen and A. Gupta. An Implementation of Faster RCNN
with Study for Region Sampling. arXiv:1702.02138, 2017.
6

[8] X. Chu, W. Ouyang, W. Yang, and X. Wang. Multi-Task Re-
current Neural Network for Immediacy Prediction. In ICCV,
2015. 2

[9] H. Dahlkamp, A. Kaehler, D. Stavens, S. Thrun, and G. R.
Self-supervised Monocular Road Detection in

Bradski.
Desert Terrain. In RSS, 2006. 1, 2

[10] J. Dai, Y. Li, K. He, and J. Sun. R-FCN: Object Detection
via Region-based Fully Convolutional Networks. In NIPS,
2016. 1, 2, 6, 7

[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR, 2009. 5

[12] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised Visual
In ICCV,

Representation Learning by Context Prediction.
2015. 2

[13] C. Doersch and A. Zisserman. Multi-task Self-Supervised

Visual Learning. In ICCV, 2017. 1, 2

[14] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object Classes
Challenge 2012 (VOC2012) Results.
http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html.
1, 2, 6

[15] S. Fukuda, R. Yoshihashi, R. Kawakami, S. You, M. Iida,
and T. Naemura. Cross-connected Networks for Multi-task
Learning of Detection and Segmentation. arXiv:1805.05569,
2018. 2

[16] Y. Gao, Q. She, J. Ma, M. Zhao, W. Liu, and A. L.
Yuille. NDDR-CNN: Layer-wise Feature Fusing in Multi-
Task CNN by Neural Discriminative Dimensionality Reduc-
tion. arXiv:1801.08297, 2018. 2

[17] R. Girshick. Fast R-CNN. In ICCV, 2015. 6

[19] L. Gomez, Y. Patel, M. Rusinol, D. Karatzas, and C. V. Jawa-
har. Self-Supervised Learning of Visual Features Through
Embedding Images Into Text Topic Spaces. In CVPR, 2017.
2

[20] K. Gong, X. Liang, X. Shen, and L. Lin. Look into Per-
son: Self-supervised Structure-sensitive Learning and A New
Benchmark for Human Parsing. In CVPR, 2017. 2

[21] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask R-

CNN. In ICCV, 2017. 1, 2

[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning

for Image Recognition. In CVPR, 2016. 1, 2, 4, 6, 7

[23] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. MobileNets: Efﬁ-
cient Convolutional Neural Networks for Mobile Vision Ap-
plications. arXiv:1704.04861, 2017. 1, 2, 6, 7

[24] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.
Speed/Accuracy Tradeoffs for Modern Convolutional Object
Detectors. In CVPR, 2017. 6

[25] S. Jenni and P. Favaro. Self-Supervised Feature Learning by

Learning to Spot Artifacts. In CVPR, 2018. 1, 2

[26] A. Joulin, L. van der Maaten, A. Jabri, and N. Vasilache.
Learning Visual Features from Large Weakly Supervised
Data. In ECCV, 2016. 2

[27] K. Kumar Singh, F. Xiao, and Y. Jae Lee. Track and Transfer:
Watching Videos to Simulate Strong Human Supervision for
Weakly-Supervised Object Detection. In CVPR, 2016. 2

[28] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin,
J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, T. Duerig,
and V. Ferrari. The Open Images Dataset V4: Uniﬁed image
classiﬁcation, object detection, and visual relationship detec-
tion at scale. In arXiv:1811.00982, 2018. 8

[29] H.-Y. Lee, J.-B. Huang, M. Singh, and M.-H. Yang. Unsu-
pervised Representation Learning by Sorting Sequences. In
ICCV, 2017. 2

[30] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon Objects in Context. In ECCV, 2014. 1, 2, 6

[31] M. Marsden, K. McGuinness, S. Little, and N. E. O’Connor.
ResnetCrowd: A Residual Deep Learning Architecture for
Crowd Counting, Violent Behaviour Detection and Crowd
Density Level Classiﬁcation. In AVSS, 2017. 2

[32] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient Es-
timation of Word Representations in Vector Space. In ICLR
Workshop, 2013. 3

[33] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-
Stitch Networks for Multi-Task Learning. In CVPR, 2016.
1, 2

[34] M. Noroozi and P. Favaro. Unsupervised Learning of Visual
Representations by Solving Jigsaw Puzzles. In ECCV, 2016.
1, 2

[35] A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and
A. Torralba. Ambient Sound Provides Supervision for Vi-
sual Learning. In ECCV, 2016. 2

[18] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich Fea-
ture Hierarchies for Accurate Object Detection and Semantic
Segmentation. In CVPR, 2014. 5

[36] D. Pathak, R. Girshick, P. Doll´ar, T. Darrell, and B. Hari-
In

haran. Learning Features by Watching Objects Move.
CVPR, 2017. 1, 2

4992

[37] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.
Efros. Context Encoders: Feature Learning by Inpainting. In
CVPR, 2016. 2

[47] J. Walker, C. Doersch, A. Gupta, and M. Hebert. An Un-
certain Future: Forecasting from Static Images Using Varia-
tional Autoencoders. In ECCV, 2016. 2

[38] R. Ranjan, V. M. Patel, and R. Chellappa. HyperFace:
A Deep Multi-task Learning Framework for Face Detec-
tion, Landmark Localization, Pose Estimation, and Gender
Recognition. TPAMI, 2017. 2

[39] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal
Networks. In NIPS, 2015. 1, 2, 3, 4, 5, 6, 7

[40] Z. Ren and Y. Jae Lee. Cross-Domain Self-Supervised Multi-
Task Feature Learning Using Synthetic Imagery. In CVPR,
2018. 2

[41] S. Ruder, J. Bingel,

I. Augenstein, and A. Sogaard.
Learning What to Share between Loosely Related Tasks.
arXiv:1705.08142, 2017. 2

[42] D. Stavens and S. Thrun. A Self-supervised Terrain Rough-
In UAI,

ness Estimator for Offroad Autonomous Driving.
2006. 1, 2

[43] C. Su, F. Yang, S. Zhang, Q. Tian, L. S. Davis, and W. Gao.
Multi-Task Learning With Low Rank Attribute Embedding
for Person Re-Identiﬁcation. In ICCV, 2015. 2

[44] O. Sumer, T. Dencker, and B. Ommer.

Self-supervised
Learning of Pose Embeddings from Spatiotemporal Rela-
tions in Videos. In ICCV, 2017. 2

[45] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, 2017. 1, 2, 6, 7

[46] L. Trottier, P. Gigu`ere, and B. Chaib-draa. Multi-Task Learn-
ing by Deep Collaboration and Application in Facial Land-
mark Detection. arXiv:1711.00111, 2017. 1, 2

[48] X. Wang, K. He, and A. Gupta. Transitive Invariance for
In ICCV,

Self-supervised Visual Representation Learning.
2017. 2

[49] D. Xu, W. Ouyang, X. Wang, and N. Sebe. PAD-Net: Multi-
Tasks Guided Prediction-and-Distillation Network for Si-
multaneous Depth Estimation and Scene Parsing. In CVPR,
2018. 2

[50] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim. Ro-
tating Your Face Using Multi-Task Deep Neural Network. In
CVPR, 2015. 2

[51] X. Zhan, Z. Liu, P. Luo, X. Tang, and C. C. Loy. Mix-and-
Match Tuning for Self-Supervised Semantic Segmentation.
In AAAI, 2018. 2

[52] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz.
In ICLR,

mixup: Beyond Empirical Risk Minimization.
2018. 3

[53] R. Zhang, P. Isola, and A. A. Efros. Colorful Image Col-

orization. In ECCV, 2016. 1, 2

[54] R. Zhang, P. Isola, and A. A. Efros. Split-Brain Autoen-
coders: Unsupervised Learning by Cross-Channel Predic-
tion. In CVPR, 2017. 2

[55] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Facial Landmark

Detection by Deep Multi-task Learning. In ECCV, 2014. 2

4993

