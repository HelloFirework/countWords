MAP inference via Block-Coordinate Frank-Wolfe Algorithm

Paul Swoboda∗

MPI for Informatics, Germany

Vladimir Kolmogorov

IST Austria

pswoboda@mpi-inf.mpg.de

vnk@ist.ac.at

Abstract

We present a new proximal bundle method for Maximum-
A-Posteriori (MAP) inference in structured energy minimiza-
tion problems. The method optimizes a Lagrangean relax-
ation of the original energy minimization problem using a
multi plane block-coordinate Frank-Wolfe method that takes
advantage of the speciﬁc structure of the Lagrangean decom-
position. We show empirically that our method outperforms
state-of-the-art Lagrangean decomposition based algorithms
on some challenging Markov Random Field, multi-label dis-
crete tomography and graph matching problems.

1. Introduction

Maximum-A-Posteriori (MAP) inference, that is mini-
mizing an energy function f : X → R over a discrete
set of labelings X is a central tool in computer vision and
machine learning. Many solvers have been proposed for
various special forms of energy f and labeling space X ,
see [11] for an overview of solvers and applications for the
prominent special case of Markov Random Fields (MRF).
Solvers can roughly be categorized into three categories:
(i) Exact solvers that use search techniques (e.g. branch-
and-bound) and possibly rely on solving lower bounds with
LP-solvers to speed up search, (ii) primal heuristics that
ﬁnd a suboptimal solution with an ad-hoc search strategy
adapted to the speciﬁc problem and (iii) Lagrangean decom-
position (a.k.a. dual decomposition) based algorithms that
decompose the original problem into smaller efﬁciently opti-
mizable subproblems and exchange Lagrangean multipliers
between subproblems until consensus between subproblems
is achieved.

Except when the energy fulﬁlls special assumptions, exact
solvers are usually not applicable, since problem sizes in
computer vision are too large. On the other hand, primal
heuristics can be fast but solution quality need not be good

∗The work was performed while the ﬁrst author was at IST Austria.
The work was supported by the European Research Council under the
European Unions Seventh Framework Programme (FP7/2007-2013)/ERC
grant agreement no 616160.

and no information is given to judge it. Moreover, algorithms
from the ﬁrst two paradigms are usually developed ad-hoc for
speciﬁc optimization problems and cannot be easily extended
to other ones. Lagrangean decomposition based algorithms
are a good middle ground, since they optimize a dual lower
bound, hence can output a gap that shows the distance to
optimum, yet use techniques that scale to large problem sizes.
Generalization to new problems is also usually much easier,
since subproblems can be readily combined.

A large number of algorithmic techniques have been pro-
posed for optimizing a Lagrangean decomposition for MRFs,
including (i) Message passing [16, 42, 38, 6] (a.k.a. block
coordinate ascent, belief propagation), (ii) ﬁrst order prox-
imal splitting methods [26, 31] and (iii) Smoothing based
methods [9, 29], (iv) Nesterov schemes [28, 10], (v) mirror
descent [22], (vi) subgradient based algorithms [30, 36, 18].
In the case of MAP inference in MRFs, the study [11] has
shown that message passing techniques outperform compet-
ing Lagrangean decomposition based methods by a large
margin. However, there are two main practical shortcomings
of message passing algorithms: (i) they need not converge
to the optimum of the relaxation corresponding to the La-
grangean decomposition: while well-designed algorithms
monotonically improve a dual lower bound, they may get
stuck in suboptimal ﬁxed points. (ii) So called min-marginals
must be computable fast for all the subproblems in the given
Lagrangean decomposition. While the above problems do
not seem to be an issue for most MAP inference tasks in
MRFs, for other problems they are. In such cases, alternative
techniques must be used. Subgradient based methods can
help here, since they do not possess the above shortcomings:
They converge to the optimum of the Lagrangean relaxation
and only require ﬁnding solutions to the subproblems of
the decomposition, which is easier than their min-marginals
(as needed for (i)), proximal steps (as needed for (ii)) or
smoothed solutions (as needed for (iii) and (iv)).

The simplest subgradient based algorithm is subgradient
ascent. However, its convergence is typically slow. Bundle
methods, which store a series of subgradients to build a local
approximation of the function to be optimized, empirically
converge faster.

111146

1.1. Contribution & Organization

We propose a multi plane block-coordinate version of
the Frank-Wolfe method to ﬁnd minimizing directions in
a proximal bundle framework, see Section 2. Our method
exploits the structure of the problem’s Lagrangean decompo-
sition and is inspired by [34]. Applications of our approach
to MRFs, discrete tomography and graph matching are pre-
sented in Section 3. An experimental evaluation on these
problems is given in Section 4 and suggests that our method
is superior to comparable established methods.

All proofs are given in the extended version [37]. A
C++-implementation of our Frank-Wolfe method is avail-
able at http://pub.ist.ac.at/~vnk/papers/
FWMAP.html. The MRF, discrete tomography and graph
matching solvers built on top of our method can be obtained
at https://github.com/LPMP/LPMP.

1.2. Related work

To our knowledge, the Frank-Wolfe method has not yet
been used in our general setting, i.e. underlying a proximal
bundle solver for a general class of structured energy mini-
mization problems. Hence, we subdivide related work into
(i) subgradient/bundle methods for energy minimization, (ii)
ad-hoc approaches that use Frank-Wolfe for speciﬁc tasks
and (iii) proximal bundle methods.
have ﬁrst been proposed
Subgradient based solvers
by [36] for MAP-inference in MRFs and were later pop-
ularized by [18]. These works rely on a decomposition of
MRFs into trees. New decompositions for certain MRFs
were introduced and optimized in [25] for max-ﬂow sub-
problems and in [32] for perfect matching subproblems. The
work [44] used a covering of the graph by a tree and opti-
mized additional equality constraints on duplicated nodes
via subgradient ascent. Usage of bundle methods that store
a series of subgradients to build a local approximation of
the objective function was proposed by [12, 32] for MAP
inference in MRFs.
The Frank-Wolfe algorithm was developed in the 50s [4]
and was popularized recently by [8]. In [20] a block coor-
dinate version of Frank-Wolfe was proposed and applied to
training structural SVMs. Further improvements were given
in [34, 24] where, among other things, caching of planes
was proposed for the Frank-Wolfe method. Several works
have applied Frank-Wolfe to the MAP-MRF inference prob-
lem: (1) [33] used Frank-Wolfe to compute an approximated
steepest-descent direction in the local polytope relaxation for
MRFs. (2) [23] used Frank-Wolfe to solve a modiﬁed prob-
lem obtained by adding a strictly convex quadratic function
to the original objective (either primal or dual). In contrast to
these works, we use Frank-Wolfe inside a proximal method.
Furthermore, the papers above use a ﬁne decomposition into
many small subproblems (corresponding to pairwise terms
of the energy function), while we decompose the problem

into much larger subproblems. In general, our decomposi-
tion results in fewer dual variables to optimize over, and each
Frank-Wolfe update can be expected to give a much larger
gain. Frank-Wolfe was also used in [1] for MAP inference
in dense MRFs with Potts interactions and Gaussian weights.
As we do, they use Frank-Wolfe to optimize proximal steps
for MAP-inference. In constrast to our work, they do not
optimize Lagrangean multipliers, but the original variables
directly. In other words, they work in the primal, while we
work in the dual. We remark that our formulation is appli-
cable to more general integer optimization problems than
either [33, 23, 1] and it does not seem straightforward to
apply these approaches to our more general setting while
only requiring access to MAP-oracles of subproblems.
Proximal bundle methods were introduced in [13, 21] to
accelerate subgradient descent algorithms. They work by
locally building an approximation (bundle) to the function
to be optimized and use this bundle to ﬁnd a descent direc-
tion. For stability, a quadratic (proximal) term is added [14].
While not theoretically guaranteed, proximal bundle meth-
ods are often faster than subgradient methods.

2. Method

Original problem We consider the problem of minimizing
a function of Boolean variables represented as a sum of
individual terms:

min

x∈{0,1}d

f (x),

f (x) :=Xt∈T

ft(xAt )

(1)

Here term t ∈ T is speciﬁed by a subset of variables
At ⊆ [d] and a function ft : {0, 1}At → R ∪ {+∞} of
|At| variables. Vector xAt ∈ RAt is the restriction of vec-
tor x ∈ Rd to At. The arity |At| of function ft can be
arbitrarily large, however we assume the existence of an efﬁ-
cient min-oracle that for a given vector λ ∈ RAt computes
[ft(x) + hλ, xi] together with the cost ft(x),
x ∈ arg min
x∈dom ft

where dom ft = {x ∈ {0, 1}At | ft(x) < +∞} 6= ∅ is the
effective domain of ft. It will be convenient to denote

ht(λ) = min

[ft(x) + hλ, xi] = min
y∈Yt

hy, [λ 1]i = min
y∈Yt

hy, [λ 1]i

x∈dom ft

where subsets Yt, Yt ⊆ [0, 1]At ⊗ R are deﬁned as follows:

Yt = {[x ft(x)] : x ∈ dom ft} Yt = conv(Yt)

The assumption means that we can efﬁciently compute a
supergradient of concave function ht(λ) at a given λ ∈ RAt.
Since (1) is in general an NP-hard problem, our goal will
be to solve a certain convex relaxation of (1), which will
turn out to be equivalent to the Basic LP relaxation (BLP)
of (1) [17]. This relaxation has been widely studied in the
literature, especially for the MAP-MRF problem (in which

11147

case it is usually called the local polytope relaxation [41, 42]).
We emphasize, however, that our methodology is different
from most previous works: before applying the BLP relax-
ation, we represent the objective as a function of Boolean
indicator variables. This allows expressing complicated com-
binatorial constraints such as those in multi-label discrete
tomography and graph matching problems (see Section 3).
Lagrangean relaxation For a vector y ∈ Yt let us denote
the ﬁrst |At| components as y⋆ ∈ [0, 1]At and the last one as

y◦ ∈ R (so that y = [y⋆ y◦]). We also denote Y =Nt∈T Yt
and Y = Nt∈T

Yt = conv(Y). The t-th component of
vector y ∈ Y will be denoted as yt ∈ Yt. Problem (1) can
now be equivalently written as

min

⋆=xAt ∀t∈T Xt∈T

y∈Y , x∈{0,1}d
yt

yt
◦

(2)

for a center point µ ∈ Λ. The proximal quadratic terms
act as a trust-region term in the vicinity of µ and make the
function strongly concave, hence smoothing the dual. A suc-
cessively reﬁned polyhedral approximation [21] is typically
used for solving (6). We develop a proximal method that will
alternate between minimizing (6) with the help of a multi-
plane block coordinate Frank-Wolfe method and updating
the proximal center µ.

2.1. Maximizing hµ,c(λ): BCFW algorithm

Objectives similar to (6) (without the summation con-
straint on the λ-variables) are used for training structural
Support Vector Machines (SSVMs). Following [20, 34, 24],
we use a block-coordinate Frank-Wolfe algorithm (BCFW)
applied to the dual of (6), more speciﬁcally its multi-plane
version MP-BCFW [34]. The dual of (6) is formulated below.

We form the relaxation of (2) by removing the non-convex
constraint x ∈ {0, 1}d:

Proposition 2. The dual problem to maxλ∈Λ hµ,c(λ) is

min

⋆=xAt ∀t∈T Xt∈T

y∈Y , x∈Rd
yt

yt
◦

(3)

min
y∈Y

fµ,c(y),

It can be shown that problem (3) is equivalent to the BLP
relaxation of (1), see [37].

We will not directly optimize this relaxation, but its La-
grangean dual [35]. For each equality constraint yt
⋆ = xAt
we introduce Lagrange multipliers λt ∈ RAt. The collection
RAt. The

dual will be optimized over the set

of these multipliers will be denoted as λ ∈Nt∈T
i = 0 ∀i ∈ [d])

Λ =(λ : Xt∈Ti

λt

(4)

where we denoted

Ti = {t ∈ T : i ∈ At} .

Proposition 1. The dual of (3) w.r.t. the equality constraints
yt
⋆ = xAt is

max
λ∈Λ

h(λ),

h(λ) :=Xt∈T

ht(λt)

(5)

Furthermore, the optimal values of problems (3) and (5) coin-
cide. (This value can be +∞, meaning that (3) is infeasible
and (5) is unbounded).

Next, we describe how we maximize function h(λ).

Proximal term We have a non-smooth concave maximiza-
tion problem in λ, hence algorithms that require a differen-
tiable objective functions will not work. In proximal bundle
methods [14] an additional proximal term is added. This
results in the new objective

max
λ∈Λ

hµ,c(λ),

hµ,c(λ) := h(λ) −

1
2c

kλ − µk2

(6)

fµ,c(y) := max

Deﬁne ν ∈ Rd by νi = 1
Then the optimal λ in (7) is λt = c · yt

i) for i ∈ [d].

⋆ + µt − νAt and

kλt − µtk2(cid:21) (7)

1
2c

λ∈Λ Xt∈T(cid:20)hyt, [λt 1]i −
|Ti|Pt∈Ti
⋆k2 + hyt, [µt 1]i(cid:17) −

i + µt

(c · yt

kyt

2

fµ,c(y) = Xt∈T(cid:16) c

∇tfµ,c(y) = [λt 1]

d

Xi=1

|Ti|
2c

ν2
i

(8)

where ∇t denotes the derivative w.r.t. variables yt.

Next, we review and adapt to our setting BCFW and
MP-BCFW algorithms for minimizing function fµ,c(y) over
y ∈ Y. We will also describe a practical improvement to the
implementation, namely compact representation of planes,
and discuss the problem of estimating the duality gap.
BCFW [20] The algorithm maintains feasible vectors
y ∈ Y. At each step BCFW tries to decrease the objec-
tive fµ,c(y) by updating component yt for a chosen term t
(while maintaining feasibility). To achieve this, it ﬁrst lin-
earizes the objective by using the Taylor expansion around
the current point:

fµ,c(z) ≈ h∇fµ,c(y), zi + const .

(9)

The optimal solution zt ∈ Yt of the linearized ob-
jective is computed by calling the t-th oracle: zt ←
h∇tfµ,c(y), zti. The new vector y is obtained as the
arg min

zt∈Yt
best interpolation of yt and zt with all other components s 6=

t ﬁxed to ys, i.e. ys(γ) ← (cid:26) ys,

(1 − γ)yt + γzt,

s 6= t
s = t

.

11148

Algorithm 1 One pass of BCFW. Input: vectors y ∈ Y,
µ ∈ Λ and ν ∈ Rd computed as in Prop. 2.
1: for each t ∈ T do in a random order
2: set λt = c · yt
3: call t-th oracle for λt: zt ← arg min

⋆ + µt − νAt

hzt, [λt 1]i

zt∈Yt

[ft(x) + hλt, xi] and zt = [x ft(x)](cid:1)

s 6= t
s = t

(cid:0)i.e. let x ← arg min
interpolate y(γ)s ← (cid:26) ys,

x∈Xt

4:

(1 − γ)yt + γzt,
5: compute γ ← arg minγ∈[0,1] fµ,c(y(γ)):
and clip γ to [0, 1]

set γ ← h[λt 1],zt−yti
⋆k2

ckyt

⋆−zt
|Ti| (y(γ)t
6: set νi ← νi + c
7: end for

i − yt

i ) for i ∈ At and yt ← y(γ)t

The step size γ ∈ [0, 1] is chosen to minimize the objective.
The optimal γ can be easily computed in closed form (see
Lemma 1 in [37]). One pass of BCFW is summarized in
Algorithm 1. To avoid the expensive recomputation of the
sum ν in Prop. 2 needed for computing the gradient and the
step size, it is held and updated explicitly in Algorithm 1.
MP-BCFW [34] In this paper we use the multi-plane ver-
sion of BCFW. This method caches planes zt returned by
min-oracles for terms ht(λt) = minzt∈Yt hzt, [λt 1]i. Let
˜Yt ⊂ Yt be the set of planes currently stored in mem-
ory for the t-th subproblem. It deﬁnes an approximation
˜ht(λt) = minzt∈ ˜Y t hzt, [λt 1]i of term ht(λt). Note that
˜ht(λt) ≥ ht(λt) for any λt.

MP-BCFW uses exact passes (that call the “exact” or-
acle for ht(λt) in line 3 of Algorithm 1) and approximate
passes (that call the “approximate” oracle for ˜ht(λt)). One
MP-BCFW iteration consists of one exact pass followed by
several approximate passes. The number of approximate
passes is determined automatically by monitoring how fast
the objective decreases per unit of time. Namely, the method
computes the ratio fµ,c(y◦)−fµ,c(y)
where y◦ is the vector at
the beginning of the MP-BCFW iteration, y is the current
vector and ∆t is the time passed since the beginning of the
MP-BCFW iteration. If this ratio drops after an approximate
pass then the iteration terminates, and a new MP-BCFW
iteration is started with an exact pass. The number of ap-
proximate passes will thus depend on the relative speed of
exact and approximate oracles.

∆t

Note that the time for an approximate oracle call is propor-
tional to the size of the working set | ˜Yt|. The method in [34]
uses a common strategy for controlling this size: planes that
are not used during the last K iterations are removed from
Yt. We use K = 10, which is the default parameter in [34].
Compact representation of planes Recall that planes in
the set ˜Yt have the form [x ft(x)] for some x ∈ dom ft ⊆
{0, 1}At. A naive approach (used, in particular, in previous
works [34, 24]) is to store them explicitly as vectors of size
|At| + 1. We observe that in some applications a special

structure of Xt allows storing and manipulating these planes
more efﬁciently. For example, in MAP-MRF inference prob-
lems a variable with k possible values can be represented by
a single integer, rather than k indicator variables.

In our implementation we assume that each vector x ∈
dom ft can be represented by an object s in a possibly more
compact space Xt. To specify term ft, the user must provide
an array mapt : [|At|] → [d] that determines set At ⊆ [d]
in a natural way, specify the size of objects s ∈ Xt, and
implement the following functions:

F1. A bijection σt : Xt → Xt.

F2. Min-oracle that for a given λt computes x ∈
[ft(x) + hλt, xi] and returns its compact repre-

arg min
x∈dom ft
sentation s (i.e. σt(s) = x) together with the cost ft(x).

F3. A function that computes inner product hλt, σt(s)i for

a given vector λt and object s ∈ Xt.

Note, calling the approximate oracle in line 3 involves
calling the function in (F3) | ˜Yt| times; it typically takes
O(| ˜Yt| · sizet) time where sizet is the length of the array
for storing s ∈ Xt.

Remark 1. The efﬁcient plane storage mechanism gives
roughly a 25% speedup of a single MP-BCFW pass on the
protein-folding MRF dataset (see Sections 3 and 4). More-
over, it typically results in a slightly better objective value
obtained after each MP-BCFW pass, since more approxi-
mate passes can be done before an exact pass is called (since
approximate passes are accelerated by the compact plane
storage, their objective decrease per unit of time is higher,
hence they are called more often).

2.2. Algorithm’s summary

Recall that our goal is to maximize function h(λ) over
λ ∈ Λ; this gives a lower bound on the original discrete
optimization problem minx∈X f (x). We now summarize
our algorithm for solving maxλ∈Λ h(λ), and describe our
choices of parameters. To initialize, we set µt = 0, yt ←
arg maxyt∈Yt hyt, [µt 1]i and ˜Y = {yt} for each t ∈ T .
Then we start the main algorithm. After every 10 iterations
of MP-BCFW we update µ ← λ∗ (keeping vectors yt and
sets ˜Yt unchanged), where λ∗ is the vector with the largest
value of objective h(λ∗) seen so far. Since evaluating h(λ)
is an expensive operation, we do it for the current vector λ
only after every 5 iterations of MP-BCFW.

Remark 2 (Convergence). If we evaluated the inner itera-
tions in MP-BCFW exactly, our method would amount to the
proximal point algorithm which is convergent [27]. Even
with non-exact evaluation, convergence can be proved when
the error in the evaluation of the proximal is shrinking fast

11149

enough towards zero. However, we have use a more ag-
gressive scheme that updates the proximal point every 10
iterations and does not bound the inexactness of the proximal
step evaluation. Experimentally, we have found that it gives
good results w.r.t. the objective of the overall problem (5).

2.3. Estimating duality gap

To get a good stopping criterion, it is desirable to have
a bound on the gap h(λ) − h(λ∗) between the current and
optimal objectives. This could be easily done if we had
feasible primal and dual solutions. Unfortunately, in our
case vector y ∈ Y is not a feasible solution of problem (3),
⋆ = xAt. 1
since it does not satisfy equality constraints yt
To get a handle on the duality gap, we propose to use the
following quantities:

hyt, [λt 1]i−h(λ) , By =

Ay,λ =Xt∈T

d

Xi=1(cid:20)max

t∈Ti

yt
i − min
t∈Ti

yt

i(cid:21)

(10)

Proposition 3. Consider pair (y, λ) ∈ Y×Λ.
(a) There holds Ay,λ ≥ 0, By ≥ 0 and
h(λ∗) − h(λ) ≤ Ay,λ + By · kλ∗ − λk1,∞ ∀λ∗ ∈ Λ (11)

where we denoted kδk1,∞ = maxi∈[d]Pt∈Ti

(b) We have Ay,λ = By = 0 if and only if y and λ are
optimal solutions of problems (3) and (5), respectively.

|δt
i |.

Note,

if we knew that an optimal solution λ∗ ∈
maxλ∈Λ h(λ) belongs to some bounded region then we
could use (11) to obtain a bound on the duality gap. Such
region can be obtained for some applications, but we did not
pursue this direction.

3. Applications

In this section we give a detailed description of the three
applications used in the evaluation: Markov Random Fields
(MRFs), discrete tomography and the graph matching prob-
lem. The latter two are both extensions of the MAP-inference
problems for MRFs. Those three problems are reviewed be-
low.

3.1. Markov Random Fields

An MRF consists of a graph G = (V, E) and a dis-
crete set of labels Xv for each v ∈ V . The goal in
Maximum-A-Posteriori (MAP) inference is to ﬁnd labeling

(xv)v∈V ∈Nv∈V Xv =: X that is minimal with respect to

the potentials:

min
x∈X

f (x), f (x) := Xv∈V

θv(xv)+ Xuv∈E

θuv(xu, xv) . (12)

1We say that vector y is a feasible (optimal) solution of (3) if there
exists a vector x ∈ Rd so that (y, x) is a feasible (optimal) solution of (3).
Clearly, x can be easily computed from feasible y, so omit it for brevity.

This problem is NP-hard for general graphs G, but can be
solved efﬁciently for trees. Hence, we choose a covering G
by trees, as done in [18]. Additionally, we seek a small num-
ber of trees, such that the number of Lagrangean variables
stays small and optimization becomes faster.
Arboricity A tree covering of a graph is called a mini-
mal tree cover, if there is no covering consisting of fewer
trees. The associated number of trees is called the graph’s
arboricity. We compute the graph’s arboricity together with
a minimal tree cover efﬁciently with the method [5]
Boolean encoding To phrase the problem as an instance
of (1), we encode labelings x ∈ X via indicator variables
xi;a = [xi = a] ∈ {0, 1} for i ∈ V, a ∈ Xi while adding

constraintsPa xi;a = 1 (i.e. assigning inﬁnite cost to con-

ﬁgurations that do not satisfy this constraint).

A tree cover and a Boolean encoding are also used for the
two problems below; we will not explicitly comment on this
anymore.

3.2. Discrete tomography

The discrete tomography problem is to reconstruct an
image from a set of linear (tomographic) projections taken at
different angles, where image intensity values are restricted
to be from a discrete set of intensities. See Figure 1 for an
illustration. The problem is ill-posed, since the number of
linear projections is smaller than the number of pixels to
reconstruct. Hence, we use a regularizer to penalize non-
regular image structures. Formally, we have an MRF G =
(V, E), where the node set corresponds to the pixel grid and
the edges connect nodes which correspond to neighboring
pixels. The label space is Xv = {0, 1, . . . , k} for some
k ∈ N. Additionally, the labeling x ∈ X must satisfy linear
projections Ax = b with A ∈ {0, 1}|V |×l. Usually, no local
information is available, hence the unary potentials are zero:
θv ≡ 0 for v ∈ V . The problem reads

min
x∈X

f (x),

f (x) := Xij∈E

θij(xi, xj)

(13)

where X = {x ∈ {0, 1, . . . , k}V : Ax = b}. A typical
choice for the pairwise potentials θij is the truncated L1-
norm θij(xi, xj) = min(|xi − xj|, c). Each row of the
projections Ax = b forms another subproblem. The i-th

row of Ax = b hence is of the formPv∈V :Aiv=1 xv = bi.

Efﬁcient solvers for this problem were recently considered
in [19]. We follow their recursive decomposition approach
for the solution of the projection subproblems. Details are
given below.

Discrete tomography subproblems We use a simpliﬁed
version of the recursive decomposition approach of [19] for
efﬁciently solving the summation constraints Ax = b of
the discrete tomography problem. Below we give the corre-
sponding details. Let the i-th row of Ax = b be of the form

11150

Constraints on summation variables Whenever we have
partitions Πi:j = Πi:k ∪ Πk+1:j, we add the constraint
si:j = sj:k + sk+1:j.

Solving (14) We will propose a dynamic programming
approach to solving (14). First, for each value l of summa-
tion variable si:j we store a value φi:j(l) ∈ R. We com-
pute φi:j recursively from leaves to the root. For partitions
Πi:j = Πi:k ∪ Πk+1:j we compute

φi:j(l) = min

l′=0,...,l

φi:k(l′) + φk+1,j(l − l′) ∀l .

(15)

After having computed φ1:n, we set s∗
1:n = b. Subsequently,
we make a pass from root to leaves to compute the optimal
label sum for each variable si:j as follows:
s∗
i:k, s∗

φi:k(si:k) + φk+1:j(sk+1:j) .

k+1:j =

min

si:k+sk+1:j =s∗

i:j

(16)

Fast dynamic programming Naively computing (15)
needs O(((j − i) · k)2) steps. However, we use an efﬁ-
cient heuristic [3] that tends to have subquadratic complexity
in practice.

3.3. Graph matching

The graph matching problem consists of ﬁnding a MAP-
solution in an MRF G = (V, E) where the labels for each
node come from a common universe Xv ⊂ L ∀v ∈ V .
The additional matching constraint requires that two nodes
cannot take the same label: xu 6= xv ∀u, v ∈ V, u 6= v.
Hence, any feasible solution deﬁnes an injective mapping
into the set of labels. For an illustration see Figure 2. The
problem is

min
x∈X

f (x)

s.t.

xu 6= xv ∀u 6= v .

(17)

We use a minimum cost ﬂow solver for handling the match-
ing constraint, see e.g. [40, 45, 39] for an explanation of the
minimum cost ﬂow solver construction.

4. Experiments

We have chosen a selection of challenging MRF, discrete
tomography and graph matching problems where message
passing methods struggle or are not applicable. Detailed
descriptions of these problems can be found in [37].

Remark 3. Note that there are a large number of MRF
and graph matching problems where message passing is
the method of choice due to its greater speed and sufﬁcient
solution quality, see [11, 39].
In such cases there is no
advantage in using subgradient based solvers, which tend to
be slower. However, our chosen evaluation problems contain
some of the most challenging MRF and graph matching
problems with corresponding Lagrangean decompositions
not solved satisfactorily with message passing solvers.

11151

Figure 1.
Illustration of a discrete tomography problem. Im-
age intensity values are 0 (white), 1 (gray) and 2 (black). Small
arrows on the side denote the three tomographic projection direc-
tions (horizontal, vertical, and diagonal). Values at arrow heads
denote the intensity value along tomographic projections.

Figure 2.
Illustration of a graph matching problem matching
nose and left/right feet of two penguins. The blue nodes on the
left penguin correspond to the underlying node set V , while the
blue nodes on the right penguin correspond to the labels L. The
green lines denotes the matching. Note that no two labels are
matched twice. The red springs denote pairwise costs θij that
encourage geometric rigidity of the matching.

Pv∈V :Aiv=1 xv = bi and recall that xv ∈ {0, 1, . . . , k}.

Each such tomographic projection will correspond to a sin-
gle subproblem. Taking care of Lagrangean multipliers λ,
we can rename variables and rewrite the problem as

n

k

n

min

x1,...,xn∈{0,...,k}n

Xi=1

Xl=0

λi(l) · ✶xi=l

s.t.

xi = b

Xi=1

(14)
We will follow a recursive decomposition approach. To
this end, we introduce helper summation variables si:j =

u=i xu.

Pj

partition

partitions We
2 ⌋ = {x1, . . . , x⌊ n

set
[1, n]
2 ⌋+1:n =
2 ⌋+1, . . . , xn}. We recursively partition Π1:⌊ n
2 ⌋
2 ⌋+1:n analoguously until reaching single variables.

Variable
into Π1:⌊ n
{x⌊ n
and Π⌊ n
This results in a tree with Π1:n as root.

2 ⌋} and Π⌊ n

the

Figure 3. Averaged lower bound vs. runtime plots for the protein folding MRF dataset, discrete tomography (synthetic images with 2, 4
and 6 projections, sheep logan image of sizes 64 × 64 and 256 × 256 with 2, 4 and 6 projections), and the 6d scene ﬂow graph matching
dataset. Values are averaged over all instances of the dataset.

Dataset

MRF

protein folding

Discrete tomography

# I

|V |

|E|

FWMAP

CB

SA

MP

11 33-40

528-780

-12917.44 -12970.61 -12960.30 -13043.67

synthetic 2 proj.
synthetic 4 proj.
synthetic 6 proj.

1024
1024
1024
4096
sheep logan 256 × 256 3 65536

sheep logan 64 × 64

9
9
9
3

1984
1984
1984
8064
130560

266.12
337.88
424.36
897.18
4580.06

265.89
336.33
417.76
847.87
4359.24

239.39
316.61
391.09
701.93
370.63

†
†
†
†
†

Graph matching
6d scene ﬂow

6 48-126 1148-5352

-2864.2

-2865.61 -2867.60 -2877.08

Table 1. Dataset statistics and averaged maximum lower bound. # I denotes number of instances in dataset, |V | the number of nodes and |E|
the number of edges in the underlying graphical model. † means method is not applicable. Bold numbers indicate highest lower bound
among competing algorithms.

Figure 4. lower bound vs. run-
time on instance 1CKK of
the protein folding dataset
solved with FWMAP and differ-
ent values of proximal weight c
from (6).

Figure 5. Duality gap quantities
Ay,λ and By from (10) over
time for the synthetic 6 proj.
dataset from discrete tomogra-
phy.

We have excluded primal heuristics that do not solve a
relaxation corresponding to our Lagrangean decomposition
at all from comparison. First, they do not deliver any lower

bounds, hence cannot be directly compared. Second, we see
them as complementary solvers, since, when they are applied
on the solved dual problem, their solution quality typically
improves. We have run every instance for 10 minutes on
a Intel i5-5200U CPU. Per-dataset plots showing averaged
lower bound over time can be seen in Figure 3. Dataset
statistics and ﬁnal lower bounds averaged over datasets can
be seen in Table 1. Detailed results for each instance in each
dataset can be found in [37].
Solvers We focus our evaluation on methods that are able to
handle general MAP inference problems in which the access
to subproblems is given by min-oracles.
FWMAP: Our solver as described in Section 2.

CB: The state-of-the-art bundle method ConicBun-
dle [7], which does not treat individual subprob-
lems individually, but performs ascent on all La-

11152

grangean multipliers λ simultaneously.

4.2. Discrete tomography.

SA: Subgradient ascent with a Polyak step size rule.

This solver was also used in [12] for MRFs.

Additionally, we tested state-of-the-art versions of message
passing (MP) solvers, when applicable. MP is a popular
method for MAP-MRF problems, and has recently been
applied to the graph matching problem [39].

All solvers optimize the same underlying linear program-
ming relaxation. Additionally, all subgradient based solvers
also use the same decomposition. This ensures that we
compare the relevant solver methodology, not differences
between relaxations or decompositions.
Choice of proximal weight c from (6) The performance of
our algorithm depends on the choice of the proximal weight
parameter c from (6). A too large value will make each
proximal step take long, while a too small value will mean
too many proximal steps until convergence. This behaviour
can be seen in Figure 4, where we have plotted lower bound
against time for an MRF problem and FWMAP with different
choices of proximal weight c. We see that there is an optimal
value of 100, with larger and smaller values having inferior
performance. However, we can also observe that perfor-
mance of FWMAP is good for values an order of magnitude
larger or smaller, hence FWMAP is not too sensitive on c. It
is enough to choose roughly the right order of magnitude for
this parameter.

We have observed that the more subproblems there are
in a problem decomposition (1), the smaller the proximal
weight c should be. Since more subproblems usually trans-
late to more complex dependencies in the decomposition, a
smaller value of c will be beneﬁcial, as it makes the result-
ing more complicated proximal steps better conditioned. A
good formula for c will hence be decreasing for increasing
numbers of subproblems |T |. We have taken three instances
out of the 50 we evaluated on and roughly ﬁtted a curve
that takes suitable values of proximal weight for these three
instances, resulting in

c =

1500000

(|T | + 22)2

.

(18)

Duality gap We have plotted the duality gap quantities
Ay,λ and By from (10) for the synthetic 6 proj. dataset
from discrete tomography in Figure 5.

4.1. Markov Random Fields.

Most MRFs can be efﬁciently solved with message pass-
ing techniques [11], e.g. with the TRWS algorithm [15],
which we denote by MP. However, there are a few hard
problems where TRWS gets stuck in suboptimal points, an
example being the protein folding dataset [43] from com-
putational biology.

In [19] a dual decomposition based solver was proposed
for the multi-label discrete tomography problem. The de-
composition was optimized with ConicBundle [7]. For our
decomposition, message passing solvers are unfortunately
not applicable. The main problem seems that due to the
unary potentials being zero, min-marginals for all subprob-
lems are also zero. Hence any min-marginal based step used
in message passing will result in no progress. In other words,
the initially zero Lagrangean multipliers are a local ﬁx-point
for message passing algorithms. Therefore, we only compare
against SA and CB.
Datasets We compare on the synthetically generated text
images from [19], denoted by synthetic. These are 32 ×
32 images of random objects with 2, 4 and 6 projections
directions. We also compare on the classic sheep logan
image with resolution 64 × 64 and 256 × 256 and 2, 4 and
6 projections.

4.3. Graph matching.

As shown in [40, 45], Lagrangean decomposition based
solvers are superior to solvers based on primal heuristics also
in terms of the quality of obtained primal solutions. In par-
ticular, [39] has proposed a message passing algorithm that
is typically the method of choice and is on par/outperforms
other message passing and subgradient based techniques on
most problems. We denote it by MP.
Datasets There are a few problems where message passing
based solvers proposed so far get stuck in suboptimal ﬁxed
points. This behaviour occurred e.g. on the dataset [2] in [39],
which we denote by graph ﬂow.

4.4. Discussion

Our solver FWMAP achieved the highest lower bound on
each instance.
It was substantially better on the hardest
and largest problems, e.g. on sheep logan. While message
passing solvers were faster (whenever applicable) in the
beginning stages of the optimization, our solver FWMAP
was fastest among subgradient based ones and eventually
achieved a higher lower bound than the message passing
one. We also would like to mention that our solver had a
much lower memory usage than the competing bundle solver
CB. On the larger problem instances, CB would often use all
available memory on our 8 GB machine.

References

[1] T. Ajanthan, A. Desmaison, R. Bunel, M. Salzmann, P.H.S.
Torr, and M. P. Kumar. Efﬁcient linear programming for
dense CRFs. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[2] H. A. Alhaija, A. Sellent, D. Kondermann, and C. Rother.
Graphﬂow - 6d large displacement scene ﬂow via graph
matching. In GCPR, 2015.

11153

[3] M. Bussieck, H. Hassler, G. J. Woeginger, and U. T. Zimmer-
mann. Fast algorithms for the maximum convolution problem.
Oper. Res. Let., 15(3):133–141, 1994.

[4] M. Frank and P. Wolfe. An algorithm for quadratic program-
ming. Naval Research Logistics Quarterly, 3:149–154, 1956.
[5] H. N. Gabow and H. H. Westermann. Forests, frames, and
games: Algorithms for matroid sums and applications. Algo-
rithmica, 7(1):465, Jun 1992.

[6] A. Globerson and T. S. Jaakkola. Fixing max-product: Con-
vergent message passing algorithms for MAP LP-relaxations.
In Conference on Neural Information Processing Systems
(NIPS), 2007.
[7] C. Helmberg.

The conicbundle library for convex
https://www-user.

optimization v0.3.11., 2011.
tu-chemnitz.de/~helmberg/ConicBundle/.

[8] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse
convex optimization. In International Conference on Machine
Learning (ICML), pages 427–435, 2013.

[9] J. Johnson, D. M. Malioutov, and A. S. Willsky. Lagrangian
relaxation for map estimation in graphical models. In 45th
Annual Allerton Conference on Communication, Control and
Computing, 2007.

[10] V. Jojic, S. Gould, and D. Koller. Accelerated dual decom-
position for MAP inference. In International Conference on
Machine Learning (ICML), 2010.

[11] J. H. Kappes, B. Andres, F. A. Hamprecht, C. Schnörr, S.
Nowozin, D. Batra, S. Kim, B. X. Kausler, T. Kröger, J. Lell-
mann, N. Komodakis, B. Savchynskyy, and C. Rother. A
comparative study of modern inference techniques for struc-
tured discrete energy minimization problems. International
Journal of Computer Vision, 115(2):155–184, 2015.

[12] J. H. Kappes, B. Savchynskyy, and C. Schnörr. A bundle ap-
proach to efﬁcient MAP-inference by Lagrangian relaxation.
In Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on, pages 1688–1695. IEEE, 2012.

[13] K. C. Kiwiel. Methods of Descent for Nondifferentiable Op-
timization. Lecture Notes in Computer Science. Springer-
Verlag, 1985.

[14] K. C. Kiwiel. Proximity control in bundle methods for convex
nondifferentiable minimization. Mathematical Programming,
46(1):105–122, Jan 1990.

[15] V. Kolmogorov. Convergent tree-reweighted message passing
for energy minimization. IEEE Trans. Pattern Anal. Mach.
Intell., 28(10):1568–1583, 2006.

[16] V. Kolmogorov. A new look at reweighted message pass-
ing. IEEE Trans. Pattern Anal. Mach. Intell., 37(5):919–930,
2015.

[17] V. Kolmogorov, J. Thapper, and S. Živný. The power of
linear programming for general-valued csps. SIAM Journal
on Computing, 44(1):1–36, 2015.

[18] N. Komodakis, N. Paragios, and G. Tziritas. MRF energy
minimization and beyond via dual decomposition.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
33(3):531–552, March 2011.

[19] J. Kuske, P. Swoboda, and S. Petra. A novel convex relaxation
for non-binary discrete tomography.
In Scale Space and
Variational Methods in Computer Vision - 6th International

Conference, SSVM 2017, 2017, Proceedings, pages 235–246,
2017.

[20] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher.
Block-coordinate Frank-Wolfe optimization for structural
SVMs. In International Conference on Machine Learning
(ICML), 2013.

[21] C. Lemarechal. Constructing bundle methods for convex
optimization. North-Holland Mathematics Studies, 129:201–
240, 1986.

[22] D. V. N. Luong, P. Parpas, D. Rueckert, and B. Rustem. Solv-
ing MRF minimization by mirror descent. In Advances in
Visual Computing - 8th International Symposium, ISVC 2012,
Rethymnon, Crete, Greece, July 16-18, 2012, Revised Selected
Papers, Part I, pages 587–598, 2012.

[23] Ofer Meshi, Mehrdad Mahdavi, and Alex Schwing. Smooth
and strong: MAP inference with linear convergence. In Con-
ference on Neural Information Processing Systems (NIPS),
2015.

[24] A. Osokin, J.-B. Alayrac, I. Lukasewitz, P. K. Dokania, and
S. Lacoste-Julien. Minding the gaps for block Frank-Wolfe
optimization of structured SVMs. In International Conference
on Machine Learning (ICML), 2016.

[25] A. Osokin and D. P. Vetrov. Submodular relaxation for in-
ference in markov random ﬁelds.
IEEE Transactions on
Pattern Analysis & Machine Intelligence, 37(7):1347–1359,
July 2015.

[26] P. Ravikumar, A. Agarwal, and M. J. Wainwright. Message-
passing for graph-structured linear programs: Proximal meth-
ods and rounding schemes. Journal of Machine Learning
Research, 11:1043–1080, 2010.

[27] R Tyrrell Rockafellar. Monotone operators and the proximal
point algorithm. SIAM journal on control and optimization,
14(5):877–898, 1976.

[28] B. Savchynskyy, J. Kappes, S. Schmidt, and C. Schnörr. A
study of nesterov’s scheme for lagrangian decomposition and
map labeling. In Computer Vision and Pattern Recognition
(CVPR), 2011 IEEE Conference on, pages 1817–1823. IEEE,
2011.

[29] B. Savchynskyy, S. Schmidt, J. H. Kappes, and C. Schnörr.
Efﬁcient MRF energy minimization via adaptive diminishing
smoothing. In Uncertainty in Artiﬁcial Intelligence (UAI),
2012.

[30] M. I. Schlesinger and V. V. Giginyak. Solution to structural
recognition (MAX,+)-problems by their equivalent transfor-
mations. (2):3–18, 2007.

[31] S. Schmidt, B. Savchynskyy, J. H. Kappes, and C. Schnörr.
Evaluation of a ﬁrst-order primal-dual algorithm for mrf en-
ergy minimization.
In International Workshop on Energy
Minimization Methods in Computer Vision and Pattern Recog-
nition, pages 89–103. Springer, 2011.

[32] N. N. Schraudolph. Polynomial-time exact inference in NP-
hard binary MRFs via reweighted perfect matching. In 13th
Intl. Conf. Artiﬁcial Intelligence and Statistics (AIstats), vol-
ume 9 of Workshop and Conference Proceedings, pages 717–
724. Journal of Machine Learning Research (JMLR), 2010.
[33] Alexander Schwing, Tamir Hazan, Marc Pollefeys, and
Raquel Urtasun. Globally convergent parallel MAP LP re-

11154

laxation solver using the Frank-Wolfe algorithm. In Interna-
tional Conference on Machine Learning (ICML), 2014.

[34] N. Shah, V. Kolmogorov, and C. H. Lampert. A multi-plane
block-coordinate Frank-Wolfe algorithm for training struc-
tural SVMs with a costly max-oracle. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015.

[35] David Sontag, Amir Globerson, and Tommi Jaakkola. In-
troduction to dual decomposition for inference.
In Suvrit
Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Opti-
mization for Machine Learning, pages 219–254. MIT Press,
2012.

[36] G. Storvik and G. Dahl. Lagrangian-based methods for ﬁnd-
ing map. IEEE Trans. on Image Processing, 9(3):469–479,
march 2000.

[37] Paul Swoboda and Vladimir Kolmogorov. MAP infer-
ence via block-coordinate Frank-Wolfe algorithm. CoRR,
abs/1806.05049, 2018.

[38] P. Swoboda, J. Kuske, and B. Savchynskyy. A dual ascent
framework for Lagrangean decomposition of combinatorial
problems. In CVPR, 2017.

[39] P. Swoboda, C. Rother, H. Abu Alhaija, D. Kainmueller, and
B. Savchynskyy. Study of Lagrangean decomposition and
dual ascent solvers for graph matching. In CVPR, 2017.

[40] L. Torresani, V. Kolmogorov, and C. Rother. A dual decom-
position approach to feature correspondence. IEEE Trans.
Pattern Anal. Mach. Intell., 35(2):259–271, 2013.

[41] Martin J Wainwright, Michael I Jordan, et al. Graphical
models, exponential families, and variational inference. Foun-
dations and Trends R(cid:13) in Machine Learning, 1(1–2):1–305,
2008.

[42] T. Werner. A linear programming approach to max-sum prob-
lem: A review. IEEE Trans. Pattern Analysis and Machine
Intelligence, 29(7):1165–1179, 2007.

[43] Chen Yanover, Ora Schueler-Furman, and Yair Weiss. Mini-
mizing and learning energy functions for side-chain predic-
tion.
In Annual International Conference on Research in
Computational Molecular Biology, pages 381–395. Springer,
2007.

[44] Julian Yarkony, Charless Fowlkes, and Alexander Ihler. Cov-
ering trees and lower-bounds on quadratic assignment. In
Computer Vision and Pattern Recognition (CVPR), 2010
IEEE Conference on, pages 887–894. IEEE, 2010.

[45] Z. Zhang, Q. Shi, J. McAuley, W. Wei, Y. Zhang, and A.
van den Hengel. Pairwise matching through max-weight
bipartite belief propagation.
In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016.

11155

