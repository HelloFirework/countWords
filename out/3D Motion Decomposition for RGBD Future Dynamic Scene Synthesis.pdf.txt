3D Motion Decomposition for RGBD Future Dynamic Scene Synthesis

Xiaojuan Qi*

Zhengzhe Liu*

Qifeng Chen

Jiaya Jia

University of Oxford

DJI

HKUST

CUHK & YouTu Lab

Abstract

A future video is the 2D projection of a 3D scene with
predicted camera and object motion. Accurate future video
prediction inherently requires understanding of 3D motion
and geometry of a scene. In this paper, we propose a RGBD
scene forecasting model with 3D motion decomposition. We
predict ego-motion and foreground motion that are com-
bined to generate a future 3D dynamic scene, which is then
projected into a 2D image plane to synthesize future motion,
RGB images and depth maps. Optional semantic maps can
be integrated. Experimental results on KITTI and Driving
datasets show that our model outperforms other state-of-
the-arts in forecasting future RGBD dynamic scenes.

1. Introduction

Future prediction is an exciting direction with limitless
potential applications in decision-making, control system
design, and navigation for intelligent agents. In this paper,
we study RGBD future scene synthesis, which refers to pre-
diction of videos and depth given a number of past frames.
Most approaches on future prediction aim to predict a
speciﬁc component in the future scene. They are mostly to
predict future color video frames [37, 30, 6, 15, 11, 12, 13]
or facilitate semantic understanding, including future se-
mantic segmentation [15, 11], instance segmentation pre-
diction [14], and 2D motion trajectories [12, 11].

Depth prediction is still new in this area with early work
of [17]. RGBD future prediction makes it possible to model
real-world dynamics. Existing approaches are mostly with
2D data and are self-supervised learning based. These ap-
proaches take past frames as input. Then a deep neural net-
work is utilized to directly generate future frames [4, 13]
with 2D optical ﬂow [12, 11] as an intermediate represen-
tation. Approaches of [29, 6, 31] disentangled foreground
and background in 2D space. Luo et al. [16] proposed an
unsupervised solution for forecasting 3D motion in RGBD
data. This framework only operates in 2D domain and does
not explicitly reason the future 3D scene.

* indicates co-ﬁrst authorship.

We note when the underlying 3D geometry of a scene is
ignored, it becomes difﬁcult to obtain accurate optical ﬂow
prediction since optical ﬂow reﬂects a level of 2D projec-
tion of 3D motion in the physical world. Further, without
full geometric understanding, it is challenging to estimate
future depth where its change is affected by both 3D cam-
era motion and object motion. Our experiments show that
simply training a deep neural network in 2D for depth pre-
diction is not feasible.

With this understanding, unlike previous work, we ex-
plicitly reason scene dynamics in 3D space, jointly pre-
dicting semantic segmentation, RGB pixel color, and depth
information. For 3D motion, we separately forecast ego-
motion and object motion in the future. Our main contribu-
tion is threefold.

First, we raise the RGBD future prediction problem and
propose a self-supervised 3D motion decomposition ap-
proach for forecasting 3D motion without labeled data. Sec-
ond, on top of the predicted 3D motion, we present a general
framework for holistic future scene prediction for motion,
semantic, depth, and RGB images. Finally, our experimen-
tal results on KITTI [8], Driving [20] datasets show that our
method is effective to solve this new problem.

2. Related Work

Prior work on future prediction can be roughly catego-
rized into two groups. The ﬁrst focuses on designing deep
neural network architectures or loss functions to directly
predict future RGB video frames [13, 19, 4, 33, 7, 2] or
high-level semantic description [15, 14, 30, 31]. Direct fu-
ture prediction is challenging because the solution space is
enormously large with high uncertainty. Xue et al. [37]
proposed the Cross Convolutional Network to model fu-
ture frames in a probabilistic fashion. Similarly, Byeon
et al. [4] proposed a future prediction framework that ag-
gregates contextual information with LSTM to avoid “blind
spot problem”. Progressive GAN [2] progressively synthe-
sizes frames from coarse to ﬁne resolution. Luc et al. [15]
proposed an auto-regressive model for predicting semantic
segmentation. The following work [14] contains a feature
prediction network for future instance segmentation.

The other group concentrates on exploiting or modeling

17673

Figure 1: Motion forecasting with decomposition and composition. The input includes images (It−1, It), depth maps
(Dt−1, Dt), and semantic maps (St−1, St). (a) Motion decomposition module decomposes motion into ego motion [R|T ]t−1,t
and moving object motion Mt−1,t. (b) The ego-motion prediction network and (c) the foreground motion prediction network
generate future ego-motion [R|T ]t,t+1 and foreground motion Mt,t+1 respectively. (d) The motion composition module
composes a predicted motion ﬁeld and a new 3D point cloud Pt+1. Pt+1 is then projected to a 2D image plane. Mt−1,t and
Mt,t+1 are color coded where R, G, B channels represent movement along x, y, z directions.

motion for understanding future dynamics [12, 11, 29, 26,
23, 32]. Liang et al. [12] jointly reasoned the duality rela-
tionship of optical ﬂow and RGB videos with an adversar-
ial objective. Terwilliger et al. [26] suggested a recurrent
ﬂow prediction framework for semantic prediction. Reda et
al. [23] learned a motion vector and a kernel for each pixel
to synthesize future frames. Jin et al. [11] designed a model
that jointly predicts complementary optical ﬂow and seman-
tic information. Walker et al. [32] utilized variational meth-
ods to capture future uncertainty for motion trajectory pre-
diction from a static image. Luo et al. [16] directly pre-
dicted future 3D trajectories via LSTM for RGBD videos.

Mahjourian et al. [18] and Villegas et al. [29] are most
related to ours. Mahjourian et al. [18] synthesized frames
with the estimated depth maps and given future ego-motion.
Motion of objects and camera trajectories are not modeled
explicitly. Thus, this approach is limited to static scenes
where independently moving objects do not exist. In con-
trast, we explicitly model scene dynamics in 3D space by
separately predicting camera and object motion to produce
future frames. Villegas et al. [29] decomposed motion
and content to generate dynamics in videos. An encoder-
decoder architecture is utilized to synthesize frames di-
rectly, which may result in distortion of rigid objects. On
the contrary, our approach follows the geometry constraints
and can preserve rigid objects better.

Our work also shares similar spirit with unsupervised
motion estimation [38, 39, 28], where motion is decom-

posed into ego motion and camera motion, and depth es-
timation [36]. These methods estimate motion and depth in
current frame, while we predict future dynamics.

3. Overview

The proposed holistic RGBD future scene synthesis task
is to predict future motion and frames. The input in-
cludes two most recent RGBD frames (and possibly se-
mantic maps). The goal is to jointly predict future motion,
RGB frames, depth maps, and semantic maps. The variation
without semantic segmentation as input will be discussed in
Section 6.2. Our holistic prediction framework predicts fu-
ture frames by ﬁrst forecasting 3D motion (Figure 1) and
then synthesizing frames (Figure 2).

To predict motion of future frame t + 1, we ﬁrst
decompose motion into ego-motion [R|T ]t−1,t and fore-
ground object motion Mt−1,t (Figure 1(a)). Then an ego-
motion prediction network and a foreground motion predic-
tion network are used to synthesize future camera motion
[R|T ]t,t+1 (Figure 1(b)) and 3D foreground motion Mt,t+1
(Figure 1(c)) separately. 3D points Pt are then locally trans-
formed by Mt,t+1 and globally transformed by [R|T ]t,t+1
to generate 3D point cloud Pt+1 in next frame (Figure 1(d)).
It, along with RGBD and semantics, is projected to the im-
age plane in frame t + 1, resulting in intermediate RGB
image ˜It+1, depth map ˜Dt+1, and semantic map ˜St+1.

These intermediate results are updated by a three-branch

7674

It-1ItDt-1DtSt-1StInput Feature EncoderEncoderDecoderMt-1,tMt,t+1Ego motion estimationForeground motion estimation3D-2D projection�Dt+1�St+1Ft,t+1Ft-1,tPt-1Pt[R|T]t-1,tMaskt-1Pt+1(a) Motion decomposition(b) Ego-motion prediction(d) Motion reconstruction(c) Foreground motion predictioñIt+1[R| T]t,t+1reﬁnement network (Figure 2). It outputs the reﬁned color
image It+1, depth map Dt+1 and semantic segmentation
St+1, as illustrated in Figure 2. Further, it ﬁlls in miss-
ing pixels, removes noise and harmonizes structure. A se-
quence of future video frames can be synthesized by apply-
ing this model recurrently in future frames.

4. Motion Forecasting

We introduce the motion decomposition module to esti-
mate ego-motion [R|T ]t,t−1 (equivalent to ﬁnding camera
pose) and 3D foreground object motion Mt−1,t. We present
an optical ﬂow based method to separate motion in 3D.

Ego-motion estimation. Ego-motion is estimated based
on matching of background pixels. We ﬁnd corresponding
background points and estimate camera trajectory in exist-
ing frames. First, we compute point clouds Pt−1 and Pt
from the depth maps as shown in Figure 1(a). Let (ui, vi) be
the 2D coordinates of pixel i and zt
i be corresponding depth
in frame t. The 3D coordinates Pt(ui, vi) = (xt
i ) in
the camera coordinate system are derived as

i , zt

i, yt

i = (ui   cx) ⇤ zt
xt
yt
i = (vi   cy) ⇤ zt

i /fx,
i /fy,

(1)

i

, vi + ∆vt−1,t

where (cx, cy) are the coordinates of the camera princi-
pal point. fx and fy are camera focal lengths. We apply
FlowNet 2.0 [10] to obtain 2D correspondence (ui, vi) in
frame t   1 and (ui + ∆ut−1,t
) in frame t.
Also, the 3D location of correspondent points is derived ac-
cording to Equation (1). Then [R|T ]t−1,t is estimated with
these points in the background (e.g. road, building). A back-
ground segmentation mask visualization is given in Figure 1
(black pixels in M askt−1). With point pairs Pt−1(ui, vi)
and Pt(ui + ∆ut−1,t
), the SVD based algo-
rithm [24] is adopted to estimate ego-motion [R|T ]t−1,t.

, vi + ∆vt−1,t

i

i

i

Foreground motion estimation. To compute foreground
motion, ego-motion [R|T ]t−1,t is utilized to transform Pt
to the camera coordinate system in frame t   1. The trans-
formed location is denoted as ¯Pt−1 = [R|T ]−1
t−1,tPt. Then,
the 3D motion ﬁeld Mt−1,t (shown in Figure 1) at location
(ui, vi) is computed as

Mt−1,t(ui, vi) = M askt−1 
[ ¯Pt−1(ui + ∆ut−1,t

, vi + ∆vt−1,t

i

i

)   Pt−1(ui, vi)].

(2)

for pixel

The motion vector

is
, ∆yt−1,t
, ∆zt−1,t
Mt−1,t(ui, vi) = (∆xt−1,t
(∆xt−1,t
represents motion
x, y, z regarding camera coordinates of frame t   1.

represented as
) where
along

i
, ∆zt−1,t

, ∆yt−1,t

)

i

i

i

i

i

i

[R|T ] can be represented as a 6D vector
[R|T ]t,t+1.
(θp, θr, θy, Tx, Ty, Tz), where (θp, θr, θy) encodes rotation
and (Tx, Ty, Tz) denotes translation.

for

Structure of

feature encoder

We ﬁrst design input

the in-
image It−1, It, depth map Dt−1, Dt,
put of color
and semantic map St−1, St.
the input
layers followed by
feature encoder has convolutional
a fully connected layer
to generate encoded feature.
Meanwhile, a geometric network with three fully con-
nected layers maps previously estimated ego-motion i.e.
(θt−1,t
) to intermedi-
ate feature. The output features of the two networks are then
processed by a fully connected layer to produce the differ-
ence of ego motion between frames t and t + 1.

, T t−1,t

, T t−1,t

, T t−1,t

, θt−1,t

, θt−1,t

x

p

y

y

z

r

4.2. Foreground Motion Prediction

Our foreground motion prediction network predicts a 3D
motion ﬁeld on foreground pixels. Since background ob-
ject motion can be determined by the ego-motion combined
with depth, we focus on estimating foreground motion. We
use a binary mask M askt to indicate (potentially) moving
objects in frame t. The mask is determined based on the
semantic class of each object. For example, a car is in fore-
ground while buildings go to background. The foreground
motion prediction network is an encoder-decoder that out-
puts a three-channel prediction map Mt,t+1 representing the
3D motion of frame t. The architecture of this network is
provided in the supplementary material.

4.3. Motion Reconstruction

The motion reconstruction module reconstructs 3D mo-
tion combining the ego-motion [R|T ]t,t+1 and foreground
motion Mt,t+1.
In this process, a 3D point cloud Pt in
frame t corresponds to Pt+1 in frame t + 1 with relation
of

Pt+1 = [R|T ]t,t+1[Pt + Mt,t+1   M askt].

(3)

Then the 3D point Pt+1 is projected onto the image plane
in frame t + 1 as

i = fxxt+1
ut+1
vt+1
i = fyyt+1

i

i

/zt+1
/zt+1

i + cx,
i + cy,

(4)

i

, vt+1

where (ut+1
) represents the corresponding location at
frame t + 1 for pixel i in frame t. With this formulation, the
future optical ﬂow Ft,t+1 can be derived as

i

Ft,t+1(ui, vi) = (ut+1

i   ui, vt+1

i   vi),

and ˜It+1, ˜Dt+1, ˜St+1 are represented as

(5)

(6)

4.1. Ego-motion Prediction

The ego-motion prediction network shown in Figure 1(b)
predicts the next-frame ego-motion. We design a net-
work to estimate the difference between [R|T ]t−1,t and

7675

i

˜Dt+1(ut+1
˜It+1(ut+1
˜St+1(ut+1

i

i

i

, vt+1
, vt+1
, vt+1

i

i

) = zt+1

i

,

) = It(ui, vi),

) = St(ui, vi).

Figure 2: Reﬁnement network. Taking as input the color images (It−1, It, ˜It+1), depth maps (Dt−1, Dt, ˜Dt+1), and semantic
maps (St−1, St, ˜St+1), the reﬁnement network synthesizes image It+1, depth map Dt+1 and semantic map St+1 by reﬁning
the projected image ˜It+1, depth ˜Dt+1 and ˜St+1.

The color and semantic information is directly copied from
the previous frame. Depth is determined by the 3D point
Pt+1. Further, depth {zt+1
} associated with each pixel is
used to determine the order of projection to handle occlu-
sion. When two points project into the same 2D location,
the point with larger depth is discarded.

i

4.4. Training

All modules in the motion prediction framework with
ego-motion prediction, motion reconstruction, and fore-
ground motion prediction are differentiable. Thus the whole
framework can be trained in an end-to-end manner.

Note that it is hard to obtain labeled data to supervise
foreground motion. To self-supervise 3D motion prediction
during training, we utilize the estimated optical ﬂow ˆFt,t+1
[10] and the ground-truth depth ˆDt+1 to penalize incorrect
prediction on ˜Dt+1 and Ft,t+1. The predicted depth map
˜Dt+1 in Figure 1 is incomplete. We thus use V t+1
D , a binary
mask, to represent pixels with depth. The loss function LM
for training this framework is

5. Reﬁnement Network

The reﬁnement network is visualized in Figure 2. The
semantic map is updated ﬁrst, which is then utilized as guid-
ance to facilitate updating of depth map Dt+1 and RGB im-
age It+1. The predicted semantic map provides category
speciﬁc information beneﬁcial to color image and depth
map prediction. This framework utilizes the auxiliary in-
formation from multiple tasks for future video prediction.

The reﬁnement network consists of three encoder-
decoders as sub-networks for predicting semantics, color,
and depth respectively. The encoder-decoders for image
and depth synthesis are trained to learn the difference be-
tween ˜It+1, ˜Dt+1 and the ground truth. We add a reﬁne-
ment module of three convolution layers with ReLU and
layer normalization to produce the ﬁnal results.

5.1. Training

The reﬁnement network is trained in an end-to-end man-
ner supervised by task speciﬁc targets. The overall loss
function LC for this network is deﬁned as

LM = ˜LF + ˜LD,

(7)

LC = LI + LS + LD,

where ˜LF and ˜LD are the loss functions for optical ﬂow and
depth respectively. They are expressed as

˜LF = X

||Ft,t+1(ui, vi)   ˆFt,t+1(ui, vi)||1,

i

˜LD = X

|| ˜Dt+1(ui, vi)   ˆDt+1(ui, vi)||1V t+1

D (ui, vi).

i

(8)

By combing ˜LF and ˜LD, training of the 3D motion predic-
tion network is well constrained. It can learn valid physical
movement of the camera and objects in 3D.

LI =

LD =

HXW

X

i=1

H×W

X

i=1

||It+1(ui, vi)   ˆIt+1(ui, vi)||1,

||Dt+1(ui, vi)   ˆDt+1(ui, vi)||1,

(9)

LS =

H×W

K

X

i=1

X

k=1

  ˆSt+1(ui, vi, k) log Sp

t+1(ui, vi, k),

where LI , LS, and LD are task-speciﬁc loss functions for
color images, semantics and depth maps. H and W are im-

7676

Semantic  SynthesisImage        Synthesis RefinementSkip ConnectionsIt-1ItDt-1DtSt-1StDepth         Synthesis RefinementSkip ConnectionsIt+1St+1Dt+1ሚIt+1෨St+1෩Dt+1age spatial sizes. K is the number of categories for seman-
tic segmentation. ˆIt+1, ˆSt+1, and ˆDt+1 are ground-truth for
image, semantic and depth respectively.

6. Experiments

on

conduct

experiments

[9] and the scene ﬂow driving dataset

Dataset. We
the KITTI
dataset
[20].
The KITTI dataset contains 375x1242-resolution stereo
image sequences for driving scenes captured at 10FPS. The
dense depth maps are generated with the stereo matching
approach CRL [22]. The optical ﬂow ﬁelds are derived
with FlowNet 2.0 [10]. We obtain semantic segmentation
by ﬁne-tuning the method of [35] on KITTI semantic
segmentation dataset.

The KITTI dataset for our training and evaluation in-
cludes totally 29 video sequences (with 5k frames). We
randomly select 4 sequences (1.7k frames) for evaluation.
Hyper-parameters in experiments are tuned on the training
set. We note that the depth and semantic maps are not per-
fect as they are generated by existing algorithms. We also
evaluate the method on the Driving dataset [20] with syn-
thetic videos of perfect depth maps and optical ﬂow ﬁelds
without segmentation information. We train our model on
the ﬁrst 600 frames and test on the remaining 200 frames.
The frame resolution in the Driving dataset is 540x960.

Implementation details. Our whole model
is imple-
mented with Tensorﬂow 1.2.1 [1]. For all networks, the
batch size is set to 1 with 50 epochs for training. Our learn-
ing rate is 1e   4 in the ﬁrst 10 epochs and 1e   5 for others.
In all experiments, our model takes two frames as input and
outputs one or multiple future frames.

Evaluation metrics. We evaluate our model, baselines,
and prior work using several metrics measuring the accu-
racy of motion ﬁelds, video frames, depth maps, and seman-
tic segmentation in the future. Predicted motion ﬁelds are
measured using the average endpoint error (EPE) [3]. We
also evaluate predicted camera poses by comparison against
the ground-truth odometry. The translation components in
camera poses are measured with the root mean square error
(RMSE) [25]; the rotation components are evaluated with
the relative angle error (RAE) [25]. Semantic segmentation
is evaluated with mean intersection-over-union (IoU) [5].
Depth maps are evaluated in terms of the mean absolute er-
ror (MAE) [27] and the mean absolute error of the inverse
depth (iMAE) [27]. Future video frames are evaluated using
Peak Signal to Noise Ratio (PSNR) and Structural Similar-
ity (SSIM) index [34].

• “Copy the previous frame” (Copy): The next-frame
optical ﬂow Ft,t+1 is copied from previous motion
ﬁeld Ft−1,t. The image, depth map, and semantic seg-
mentation in frame t+1 are directly copied from frame
t. It is a simple baseline assuming static future.

• “Warp the previous frame” (Warp): We replace our
3D motion decomposition module with optical ﬂow
Ft−1,t. We obtain a warped optical ﬂow Ft,t+1 by
warping Ft−1,t. Ft,t+1 is then used to generate inter-
mediate image ˜It+1, depth map ˜Dt+1, and semantic
segmentation ˜St+1, which are further processed with
our reﬁnement network to generate ﬁnal results. This
baseline veriﬁes that recurrently warping the optical
ﬂow is not sufﬁcient to model future motion.

• “2D optical ﬂow prediction” (Pred2D): We replace our
3D motion synthesis network with 2D optical ﬂow pre-
diction network. The network takes as input images
(It−1, It), semantic segmentation maps (St−1, St) and
depth maps (Dt−1, Dt) to predict the next-frame opti-
cal ﬂow. This baseline models motion only in 2D.

• “Copy 3D motion” (Copy3D): We remove the ego-
motion and foreground motion prediction modules
from Figure 1(b)&(c). Also, we directly copy the ego-
motion [R|T ]t−1,t and Mt−1,t to the next frame. To
produce 3D motion Mt,t+1, Mt−1,t is warped accord-
ing to motion ﬁeld Ft−1,t. This baseline aims to evalu-
ate the necessity of camera ego-motion and foreground
motion prediction in our model.

• “Directly predict 3D motion” (Pred3D): We design a
network to directly predict the whole 3D motion ﬁeld
of the scene without motion decomposition. This base-
line is to evaluate importance of our 3D motion decom-
position module in Figure 1(a).

• “Without reﬁnement” (WR): We evaluate the perfor-
mance without the reﬁnement network to evaluate the
efﬁciency of reﬁnement module.

• “Without joint reﬁnement” (WJR): We optimize the re-
ﬁnement module ﬁxing all other parts of the network
to validate the efﬁciency of joint reﬁnement strategy.

• S2S [15]: S2S is a state-of-the-art method for future
semantic prediction. We ﬁnetune the released model
on our dataset with the publicly available code. Four
consecutive frames are used as input for S2S, in con-
trast to the two-frame input in our method.

Baselines. To evaluate our 3D motion decomposition
framework for future prediction, we compare our model
with the following baselines where the ﬁrst seven are vari-
ants of our model.

• PredNet [13]: This is a previous approach to next-
frame prediction. We directly adopt released code and
model trained on KITTI. For multiple-frame predic-
tion, we apply the PredNet recurrently by taking the

7677

Seg
Flow
EPE # MAE # iMAE # PSNR " SSIM " IoU"

Image

Depth

S2S [15]

PredNet [13]
MCNet [29]

-
-
-

Copy
Warp

Pred2D
Pred3D
Copy3D

WR
WJR
Ours

11.73
10.39
7.56
8.74
5.43

-
-

3.65

-

-

1.23

2.10

-

1.38
1.30
1.24
1.15
1.07

-

0.87
0.83

-

2.29
2.31
2.68
1.99
1.62

-

1.41
1.32

-

13.54
17.25
15.50
15.65
16.44
16.23
17.52
14.61
19.78
19.83

-

60.42

0.44
0.52
0.48
0.48
0.53
0.56
0.55
0.38
0.65
0.66

-
-

53.30
54.57
62.13
58.85
67.14
57.74
67.38
69.07

Table 1: Next-frame prediction on the KITTI dataset. "
means the higher the better and # is contrary. “-” means
invalid ﬁeld.

Flow
Seg
EPE # MAE # iMAE # PSNR " SSIM " IoU "

Image

Depth

S2S [15]

PredNet [13]

-
-

Copy
Warp

Pred2D
Pred3D

Ours

11.88
11.51
8.63
10.56
5.57

-

3.71
3.25
3.32
3.92
3.09
2.63

-

5.72
5.38
5.67
7.77
5.38
4.17

-

12.37
12.36
12.48
12.41
11.99
13.05

-

37.31

0.35
0.36
0.35
0.37
0.38
0.41

-

31.85
32.67
37.33
31.87
41.70

Table 2: Qualitative results of predicting ﬁve future frames.
" means the higher the better and # means contrary. “-”
means invalid ﬁeld.

prediction results in the current frame to generate the
next frame prediction. We train PredNet to predict
both video frames and depth maps.

• MCNet [29] : This is a state-of-the-art approach to
next-frame prediction. Our method shares a similar
idea with MCNet to decompose the scene into motion
and content. We train and evaluate their method with
the released code on our dataset.

6.1. Evaluation on KITTI Dataset

We conduct both quantitative and qualitative experi-
ments on the KITTI dataset concerning the capability of
predicting future motion, images, depth maps, and seman-
tic segmentation. We also experiment with both next- and
multiple-frame prediction.

Next-frame prediction. Quantitative comparison be-
tween our approach and the baselines are shown in Table 1.
In terms of all the metrics, our method consistently outper-
forms the baselines. Compared with Pred2D, our method

RMSE # RAE #
0.024
0.013

0.483
0.380

Copy
Ours

Table 3: Next frame pose evaluation on KITTI dataset.
RAE means relative angle error for the rotation component.
RMSE represents root mean square error for the translation
component. # means the lower the better.

Flow
EPE# MAE# iMAE# PSNR" SSIM"

Image

Depth

9.56

Copy 20.16 6.39
6.06
Warp
Pred2D 5.47 14.70
3.14
Pred3D 6.43
1.87
1.88

Ours

3.21
3.75
5.55
3.11
1.27

17.58
17.45
17.22
18.48
22.08

0.62
0.63
0.63
0.67
0.77

Table 4: Qualitative results on Driving dataset for next
frame prediction. " means the higher the better, and # means
the lower the better.

achieves a much lower EPE, i.e. 3.65 vs 7.56. This demon-
strates that our 3D motion prediction framework can predict
more accurate optical ﬂow compared to 2D-based solutions.
Our approach outperforms Pred3D with more accurate fu-
ture 3D motion.

Our method also works better than PredNet in terms of
image and depth prediction, and better than S2S regard-
ing semantic prediction. Further, our approach performs
better than MCNet in synthesizing future frames in terms
of both PSNR and SSIM. More importantly, our method
achieves more accurate future depth prediction than 2D-
based baselines such as Pred2D, manifesting that a 3D-
based model can potentially capture more complete geom-
etry of the scene for future depth. In addition, we also eval-
uate our model regarding the reﬁnement module (WR) and
the joint reﬁnement strategy (WJR). The reﬁnement module
improves results by ﬁlling holes and harmonizing overall
appearance. Joint reﬁnement is also helpful.

Visual comparisons are shown in Figure 3. Compared
with MCNet and Pred2D, our method preserves higher
quality structure of objects. Our results also do not contain
the blur visual artifacts that are however noticed in others.
More qualitative comparisons are contained in the supple-
mentary material.

Compared with the segmentation prediction results of
S2S [15] and Pred2D, our results retain small and thin ob-
jects in segmentation prediction. For example, the pole is
left out in the segmentation results of S2S and Pred2D.
Without motion decomposition, Pred3D does not distin-
guish between camera and moving-object motion. It makes

7678

s
t
u
p
n
I

A
T
O
S

D
2
d
e
r
P

D
3
d
e
r
P

s
r
u
O

T
G

RGB Image

Depth Map

Semantic Segmentation

Figure 3: Visualization of different methods on next-frame prediction on the KITTI dataset. Input images are at time t. In
the second row, the image is produced by MCNet [29] and depth map is produced by PredNet [13] while the segmentation
map is from S2S [15].

GT (FlowNet2.0 [10])

Pred2D

Ours

Ours (3D motion)

Figure 4: Future motion prediction results. The 3D motion is color coded where R-G-B corresponds to movement in x-y-z
directions respectively. In this case, the car is moving closer, corresponding to the example shown in Figure 3.

static objects not well regularized and possibly generate un-
desired effect (e.g. the static trafﬁc sign is distorted).
In
comparison, our results are closer to the next-frame ground
truth (the ﬁfth row in Figure 3) while the baselines fail on
large motion regions (e.g. the nearest white car).

Evaluation of our predicted camera poses is listed in Ta-
ble 3. Compared with directly copying from [R|T ]t−1,t, our
ego-motion prediction module reduces the mean angle error
(RAE) by approximately 50%. Our approach also improves
the translation metric RMSE by over 20%, which demon-
strates that our self-supervised framework for ego-motion
prediction can predict accurate future camera poses without
ground-truth for supervision.

We show the predicted next-frame motion produced by
our approach in Figure 4. Compared with the motion ﬁeld

produced by Pred2D, our results are more natural regarding
e.g. the shape of cars. Visualization of moving-object mo-
tion is shown in Figure 4 where the car moves forward. Our
method generates 3D movement without 3D supervision.

Multiple-frame prediction. We compare our approach
with baselines on generating multiple future frames (5
frames on KITTI). Note that the frame rate of the KITTI
dataset is 10FPS and it contains large motion between
frames, which makes KITTI challenging to predict multi-
ple steps ahead. For all the approaches evaluated, we repeat
them to produce multiple future frames. We show qualita-
tive comparisons in Table 2. Our method outperforms all
the baselines regarding all the metrics. Our 3D motion de-
composition model facilitates long-term future prediction.

Qualitative comparison of generating 5 future frames is

7679

t
e
N
e
r
P

D
2
d
e
r
P

D
3
d
e
r
P

s
r
u
O

T
G

t + 1

t + 3

t + 5

Figure 5: Results of predicting multiple frames. Depth and segmentation are provided in the supplement.

Our (Image)

GT (Image)

Our (Depth)

GT (Depth)

Figure 6: Visualization of our results on the Driving dataset for next frame prediction. “GT” stands for ground truth.

shown in Figure 5. In the video produced by PredNet [13],
the video frames are blurry. Similarly, in the results of
Pred3D, objects are distorted. In contrast, our results pre-
serve the global structure of the scene and details of the ob-
jects. More results on multi-frame prediction are shown in
the supplement.

6.2. Evaluation on Driving Dataset

Driving dataset does not have segmentation annota-
tion. Therefore we train a deep neural network to produce
moving-object masks. We obtain the ground-truth masks
by the unsupervised motion segmentation method [21]. We
replace the semantic segmentation by estimated moving-
object masks in our model. The reﬁnement network is mod-
iﬁed to update the color images and depth maps.

Quantitative results are listed in Table 4. Our method
outperforms all the baselines on all the metrics. We demon-
strate that our method achieves competitive performance
even without semantic segmentation as input. Our approach
is applicable to RGBD videos without the need of semantic

segmentation. Visual illustrations are shown in Figure 6.

7. Conclusion

We have presented a 3D motion decomposition model
for future RGBD dynamic scene synthesis. Our method
predicts future scenes by ﬁrst modeling scene dynamics into
camera motion and moving-object motion. We forecast fu-
ture ego-motion and object motion separately to avoid inﬂu-
ence between them. We then integrate the two motion ﬁelds
for future scene synthesis. In our extensive experiments, we
have demonstrated that 3D motion decomposition is effec-
tive for future prediction. We believe our work shows a new
and promising direction for future scene prediction.

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,
et al. Tensorﬂow: a system for large-scale machine
learning. In OSDI, 2016. 5

7680

[2] S. Aigner and M. K¨orner. Futuregan: Anticipating
the future frames of video sequences using spatio-
temporal 3d convolutions in progressively growing au-
toencoder gans. arXiv, 2018. 1

[3] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black,
and R. Szeliski. A database and evaluation methodol-
ogy for optical ﬂow. IJCV, 2011. 5

[4] W. Byeon, Q. Wang, R. K. Srivastava, P. Koumout-
sakos, P. Vlachas, Z. Wan, T. Sapsis, F. Raue, S. Pala-
cio, T. Breuel, et al. Contextvp: Fully context-aware
video prediction. In ECCV, 2018. 1

[5] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. En-
zweiler, R. Benenson, U. Franke, S. Roth, and
B. Schiele. The cityscapes dataset for semantic urban
scene understanding. In CVPR, 2016. 5

[6] E. L. Denton et al. Unsupervised learning of disentan-

gled representations from video. In NIPS, 2017. 1

[7] C. Finn, I. Goodfellow, and S. Levine. Unsupervised
learning for physical interaction through video predic-
tion. In NIPS, 2016. 1

[8] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision

meets robotics: The kitti dataset. IJRR, 2013. 1

[9] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for
autonomous driving? the kitti vision benchmark suite.
In CVPR, 2012. 5

[10] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovit-
skiy, and T. Brox. Flownet 2.0: Evolution of optical
ﬂow estimation with deep networks. In CVPR, 2017.
3, 4, 5, 7

[11] X. Jin, H. Xiao, X. Shen, J. Yang, Z. Lin, Y. Chen,
Z. Jie, J. Feng, and S. Yan. Predicting scene parsing
and motion dynamics in the future. In NIPS, 2017. 1,
2

[12] X. Liang, L. Lee, W. Dai, and E. P. Xing. Dual mo-
tion gan for future-ﬂow embedded video prediction. In
ICCV, 2017. 1, 2

[13] W. Lotter, G. Kreiman, and D. Cox. Deep predic-
tive coding networks for video prediction and unsu-
pervised learning. In ICLR, 2017. 1, 5, 6, 7, 8

[14] P. Luc, C. Couprie, Y. Lecun, and J. Verbeek. Predict-
ing future instance segmentation by forecasting con-
volutional features. In ECCV, 2018. 1

[15] P. Luc, N. Neverova, C. Couprie, J. Verbeek, and
Y. LeCun. Predicting prednetinto the future of seman-
tic segmentation. In ICCV, 2017. 1, 5, 6, 7

[16] Z. Luo, B. Peng, D.-A. Huang, A. Alahi, and L. Fei-
Fei. Unsupervised learning of long-term motion dy-
namics for videos. In CVPR, 2017. 1, 2

[17] R. Mahjourian, M. Wicke,

and A. Angelova.
Geometry-based next frame prediction from monoc-
ular video. In IVS, 2017. 1

[18] R. Mahjourian, M. Wicke,

and A. Angelova.
Geometry-based next frame prediction from monoc-
ular video. In IV, 2017. 2

[19] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-
scale video prediction beyond mean square error.
arXiv, 2015. 1

[20] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers,
A. Dosovitskiy, and T. Brox. A large dataset to train
convolutional networks for disparity, optical ﬂow, and
scene ﬂow estimation. In CVPR, 2016. 1, 5

[21] P. Ochs, J. Malik, and T. Brox. Segmentation of mov-
ing objects by long term video analysis. TPAMI, 2014.
8

[22] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan.
Cascade residual learning: A two-stage convolutional
neural network for stereo matching. In ICCV, 2017. 5

[23] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker,
D. Tarjan, A. Tao, and B. Catanzaro. Sdc-net: Video
prediction using spatially-displaced convolution.
In
ECCV, 2018. 2

[24] O. Sorkine-Hornung and M. Rabinovich.

Least-
squares rigid motion using svd. Technical notes, 2017.
3

[25] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and
D. Cremers. A benchmark for the evaluation of rgb-d
slam systems. In IROS, 2012. 5

[26] A. M. Terwilliger, G. Brazil, and X. Liu. Recurrent

ﬂow-guided semantic forecasting. arXiv, 2018. 2

[27] J. Uhrig, N. Schneider, L. Schneider, U. Franke,
In

T. Brox, and A. Geiger. Sparsity invariant cnns.
3DV, 2017. 5

[28] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Suk-
thankar, and K. Fragkiadaki. Sfm-net: Learning of
structure and motion from video. arXiv, 2017. 2

[29] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee. De-
composing motion and content for natural video se-
quence prediction. arXiv, 2017. 1, 2, 6, 7

[30] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and
H. Lee. Learning to generate long-term future via hi-
erarchical prediction. In ICML, 2017. 1

[31] C. Vondrick, H. Pirsiavash, and A. Torralba. Generat-

ing videos with scene dynamics. In NIPS, 2016. 1

[32] J. Walker, C. Doersch, A. Gupta, and M. Hebert. An
uncertain future: Forecasting from static images using
variational autoencoders. In ECCV, 2016. 2

[33] Y. Wang, M. Long, J. Wang, Z. Gao, and S. Y. Philip.
Predrnn: Recurrent neural networks for predictive
learning using spatiotemporal lstms. In NIPS, 2017.
1

7681

[34] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-
celli. Image quality assessment: from error visibility
to structural similarity. TIP, 2004. 5

[35] Z. Wu, C. Shen, and A. Van Den Hengel. Wider or
deeper: Revisiting the resnet model for visual recog-
nition. PR, 90:119–133, 2019. 5

[36] L. Xu and J. Jia. Stereo matching: An outlier conﬁ-

dence approach. In ECCV, 2008. 2

[37] T. Xue, J. Wu, K. Bouman, and B. Freeman. Vi-
sual dynamics: Probabilistic future frame synthesis
via cross convolutional networks. In NIPS, 2016. 1

[38] Z. Yin and J. Shi. Geonet: Unsupervised learning of
dense depth, optical ﬂow and camera pose. In CVPR,
2018. 2

[39] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe.
Unsupervised learning of depth and ego-motion from
video. In CVPR, pages 1851–1858, 2017. 2

7682

