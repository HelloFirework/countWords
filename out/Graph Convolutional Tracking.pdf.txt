Graph Convolutional Tracking

Junyu Gao1

2

,

,

3, Tianzhu Zhang1

2

,

,

4 and Changsheng Xu1

2

,

,

3

1 National Lab of Pattern Recognition (NLPR),

Institute of Automation, Chinese Academy of Sciences (CASIA)

2 University of Chinese Academy of Sciences (UCAS)

3 Peng Cheng Laboratory, ShenZhen, China

4 University of Science and Technology of China

{junyu.gao, csxu}@nlpr.ia.ac.cn, tzzhang10@gmail.com

Abstract

Tracking by siamese networks has achieved favorable
performance in recent years. However, most of existing
siamese methods do not take full advantage of spatial-
temporal target appearance modeling under different con-
textual situations.
In fact, the spatial-temporal informa-
tion can provide diverse features to enhance the target rep-
resentation, and the context information is important for
online adaption of target localization. To comprehensive-
ly leverage the spatial-temporal structure of historical tar-
get exemplars and get beneﬁt from the context informa-
tion, in this work, we present a novel Graph Convolutional
Tracking (GCT) method for high-performance visual track-
ing. Speciﬁcally, the GCT jointly incorporates two types
of Graph Convolutional Networks (GCNs) into a siamese
framework for target appearance modeling. Here, we adopt
a spatial-temporal GCN to model the structured representa-
tion of historical target exemplars. Furthermore, a context
GCN is designed to utilize the context of the current frame
to learn adaptive features for target localization. Extensive
results on 4 challenging benchmarks show that our GCT
method performs favorably against state-of-the-art trackers
while running around 50 frames per second.

1. Introduction

Visual tracking is a fundamental task in computer vi-
sion community, where the target object is localized in a
changing video sequence automatically. It has various ap-
plications such as intelligent video surveillance, human-
computer interaction, robotics, and autonomous driving, to
name a few [71, 33, 13, 24, 74, 42, 17]. Despite much
progress has been achieved in recent years [38, 3, 45, 9,
66, 36, 20, 82, 22], visual tracking remains difﬁcult due to
tremendous challenges such as occlusion, background clut-
ter, illumination variation, scale variation, motion blur, fast
motion, and deformation.

Figure 1. Comparison of our proposed tracker with the popular
SiamFC tracker [2] and other two state-of-the-art methods.

Recently, tracking by siamese networks has attracted
an increasing attention in the tracking community, which
learns a similarity metric between the target object and can-
didate patches of the current search image in an end-to-end
framework [60, 2, 63, 23, 66, 36, 25, 84]. With the pow-
erful deep network and large-scale labeled video frames
for ofﬂine training, siamese based trackers achieve favor-
able performance and efﬁciency. One notable example is
the SiamFC tracker [2] which learns a matching function in
an embedding space and wins the VOT2017 real-time chal-
lenge [33, 32]. However, based on the results in existing
tracking benchmarks [70, 71], SiamFC does not achieve a
better accuracy than many other types of trackers, such as
ECO-HC [10] and TRACA [5]. Figure 1 also shows that
SiamFC encounters difﬁculties when the target object has
signiﬁcant appearance change [25]. To improve the robust-
ness of siamese based methods, various strategies have been
proposed such as attention learning [66], dynamic updat-
ing [23] and structured modeling [84], which have obtained
promising performance.

Despite the above signiﬁcant progress, most siamese
based tracking methods do not
take full advantage of
spatial-temporal target appearance modeling under differ-
ent contextual situations : (1) Many siamese trackers use the
initial target template from the ﬁrst frame to match candi-
date patches [60, 2, 36]. However, since visual tracking is a

4649

#0034#0015#0056GCTSiamFCTRACAECO-HC#0011#0060#0124plars for updating the target appearance model but ignore
the context information from the current search image.

Inspired by the above observation, it is desirable to au-
tomatically capture the spatial-temporal patterns of target
appearance under the context information of current search
image. During the tracking process of siamese based meth-
ods, the target exemplar sequence can be organized as a
3D spatial-temporal graph where each target part is con-
sidered as a node. Although 3D CNN [62] can be applied
for spatial-temporal modeling, it is computationally expen-
sive [52] and cannot handle arbitrary graph structures. Re-
cently, Graph Convolutional Networks (GCNs), which can
model the dependencies and propagate messages between
different nodes in an arbitrary graph, have received increas-
ing attention and successfully been adopted in various com-
puter vision tasks [35, 46, 67, 72, 56]. Until now, the appli-
cation of GCNs to visual tracking is yet to be explored.

In this paper, we propose an end-to-end Graph Convolu-
tional Tracking (GCT) method based on a siamese frame-
work, which can jointly consider both the spatial-temporal
target appearance structure of historical frames and the con-
text information of the current search image. As shown
in Figure 3, for target appearance modeling, we construct
a spatial-temporal graph to form a structured representation
of the historical target exemplars. A Spatial-Temporal GCN
(ST-GCN) is employed to learn a robust target appearance
model on this graph and generate a spatial-temporal feature
(ST-Feature). Furthermore, to incorporate context informa-
tion of the current search image for target localization, we
combine the ST-Feature and the context feature to produce
an adaptive graph. A ConText GCN (CT-GCN) then op-
erates on this graph and generates the adaptive feature for
target localization. To make the tracking highly efﬁcient,
all the learning processes are performed in ofﬂine training.
We validate the effectiveness and efﬁciency of our approach
on ﬁve popular tracking benchmarks [70, 71, 33, 40, 47].

To summarize, the main contributions of this paper are

three-fold:

• An end-to-end graph convolutional tracking frame-
work is explored. To the best of our knowledge, this
is the ﬁrst work to train GCNs in a deep siamese net-
work for visual tracking.

• Both ST- and CT-GCN are designed in a siamese net-
work. The proposed GCT can jointly achieve spatial-
temporal
target appearance modeling and context-
guided adaptive learning for robust target localization.
• Extensive experimental results on ﬁve visual tracking
benchmarks demonstrate that the proposed GCT algo-
rithm performs favorably against state-of-the-art track-
ers and runs in real-time.

2. Related Work
Tracking by Siamese Network. One simple yet effective
manner of using deep learning for visual tracking is to di-

4650

Figure 2. Illustration of our motivation. From bottom to top: (1)
In spatial-temporal appearance modeling, the historical target ex-
emplar images are converted to a ST-graph, where each target part
corresponds to a graph node (red, green, blue ones in this exam-
ple). By leveraging this graph, diverse target parts are considered
to generate a robust ST-Feature for representing the target object.
(2) In context-guided feature adaption, the current search image
provides useful foreground/ background information, which help-
s conduct graph learning for feature adaption. Here, the red and
green parts are more important than the blue one because the blue
part is occluded in the current search image. With the adaptive
feature, robust target localization can be achieved.

dynamic process with changing scenes, there exists a strong
spatial-temporal relationship between the target object ap-
pearances in consecutive frames. Features from different
frames and locations provide diverse information for target
appearance modeling [76, 61], such as different parts and
viewpoints, motion, deformation, and varied illuminations.
In the tracking process, to characterize rotation and transla-
tion invariance of target objects, image patches can be mod-
eled as a grid graph [8]. As shown in the bottom of Fig-
ure 2, different target parts from historical exemplar images
can be organized as a spatial-temporal graph, where the ST-
feature can be learned comprehensively for representing the
target appearance. (2) The surrounding context of the tar-
get object has a big impact on tracking performance [48].
However, most existing siamese tracking methods ignore
context information of search images for guiding the adap-
tion of target appearance model. Due to lack of the online
adaptability, they can hardly capture the variations of target
objects, backgrounds or situations in search images well,
which may lead to tracking failures [23]. We point out that
visual tracking can beneﬁt from the current context infor-
mation. As shown in the top of Figure 2, with the help of
the current context, a new graph is learned as the adaption
guidance. Based on the learned graph, the feature used for
target localization can be adaptively changed by focusing
on the ﬁrst two parts (green and red ones) and paying less
attention to the last part (the blue one) since the part is oc-
cluded in the search image. While some methods utilize
attention learning [87, 66] or transformation learning [23]
for online adaption, they only use the previous target exem-

ST-FeatureAdaptive Feature# t-3# t-2# t-1# tTarget LocalizationContextCurrent Search Imagespatial edgestemporal edgeSpatial-Temporal Graph (ST-Graph)target partSpatial-TemporalAppearance Modeling OcclusionContext-GuidedFeature Adaption adaptiveedgesGraph learningHistorical Target Exemplar Images graph noderectly apply siamese networks as a matching function be-
tween target object and candidate patches [60, 2, 26, 63,
23, 66, 87, 36]. The pioneering work, SINT [60], learn-
s a matching function in the off-line phase and applies it
to ﬁnd the most similar target candidate in online tracking.
Despite SINT achieves promising tracking performance, its
speed is only 2 fps because of the candidate sampling pro-
cess. To improve running speed, Bertinetto et al. [2] pro-
pose a fully convolutional Siamese framework (SiamFC) to
conduct similarity learning in an embedding space, which
runs nearly 86 fps with a GPU. Recently, more siamese
network based tracking methods have been proposed with
real-time high quality. Guo et al. [23] propose a dynam-
ic Siamese network (DSiam), which adopts a transforma-
tion learning model to adaptively conduct online learning.
Wang et al. [66] introduce different kinds of attention mech-
anisms in siamese learning, which mitigates the over-ﬁtting
problem and enhances its discriminative capacity. More-
over, other strategies are also adopted to improve the perfor-
mance of siamese tracking, such as two-fold learning [25],
triplet loss optimization [12], region proposal network [36],
adversarial learning [68], deep reinforcement learning [30],
distractor-aware module [86], and structured modeling [84].
Different from the above methods, we are among the ﬁrst to
utilize graph convolutional operators in siamese networks to
comprehensively model the structured cues of a target ob-
ject, which can jointly consider the spatial-temporal struc-
ture and current context information.

Structured Target Appearance Modeling. To handle
various challenges in visual tracking scenes, a number of
tracking algorithms have been proposed to impose struc-
ture information on target appearance modeling. Some
trackers explore spatial-temporal modeling in visual track-
ing [76, 59, 57, 87, 37, 61, 77, 79, 83]. However, these
methods are either not end-to-end trainable [76, 37] or only
using a holistic target appearance model [57, 87, 61, 21].
For example, although FlowTrack [87] utilizes optical ﬂow
to get beneﬁt from inter-frame motion cues, it only adopt-
s the holistic model for target representation and ignores
detailed information such as interactions between local tar-
get parts. Recently, part-based methods that decompose
target object into several parts have been studied active-
ly [8, 78, 22, 39, 7, 8, 80, 20, 82, 85]. For example, a
spectral tracking method [8] is proposed to operate on lo-
calized surrounding regions of each pixel via graph ﬁlters.
With the development of deep learning techniques, some
part-based methods learn structured information in an end-
to-end fashion [84, 15]. Zhang et al. [84] utilize conditional
random ﬁeld as a message passing module for learning lo-
cal structure in a siamese network. However, most existing
part-based trackers only consider spatial-structure informa-
tion of previous frames for locating target object, and can
hardly beneﬁt from the long-range temporal information. In

this paper, we make full use of both spatial-temporal target
structure and context information of search images for tar-
get localization in an end-to-end siamese framework.
Graph Neural Networks for Computer Vision. General-
ization of neural networks for arbitrarily structured graphs
has drawn great attention in recent years. There are two typ-
ical ways to develop graph neural networks. On one hand,
some methods adopt feed-forward neural networks to every
node in a spatial manner [55]. On the other hand, spec-
tral methods provide well-deﬁned localization operators on
graphs via convolutions in the Fourier domain [31]. For
computer vision tasks, Wang et al. [67] propose to represent
videos as space-time region graphs which capture similari-
ty relationships and spatial-temporal relationships. To mod-
el dynamic skeletons for human action recognition, Yan et
al. [72] propose a spatial-temporal graph convolutional net-
work with several types of kernels. Shen et al. [56] uti-
lize graph convolutional operator to learn probe-gallery re-
lationships for person re-identiﬁcation. Gao et al. utilize
graph neural networks to improve the performance of video
classiﬁcation [18] and zero-shot video classiﬁcation [19].

3. Graph Convolutional Tracking

In this work, we propose a Graph Convolutional Track-
er, GCT, which jointly performs spatial-temporal target ap-
pearance modeling and context-guided adaptive learning in
an end-to-end manner. Figure 3 overviews the pipeline of
the proposed tracking algorithm based on a siamese archi-
tecture (SiamFC) [2]. The SiamFC learns a similarity func-
tion f (z, x) to compare a 127 × 127 exemplar image z to a
255 × 255 search image x in a learned convolutional feature
embedding space φ (we denote Z = φ(z), X = φ(x)):

f (z, x) = φ(z) ⋆ φ(x) + b

= Z ⋆ X + b,

(1)

where ⋆ represents cross-correlation between two feature
maps, b ∈ R denotes a bias for each location. By using E-
q. (1), the most similar patch from the search image will
be selected as the target object. Despite the favorable efﬁ-
ciency and expansibility of SiamFC, it only uses the ﬁrst
frame as a ﬁxed template in the whole tracking process,
which can hardly beneﬁt from the spatial-temporal struc-
ture of target appearance under different contextual situa-
tions. In fact, features from different frames and locations
provide diverse and abundant information for the target ap-
pearance modeling [76], such as different parts and view-
points, motion, deformation, and varied illuminations. For
target localization, these features should be adaptively ag-
gregated in spatial-temporal domain guided by the context
information of the current search image. To this end, we de-
sign a graph convolutional transformation into the siamese
architecture to jointly consider target appearance modeling

4651

Figure 3. The pipeline of our GCT, which can jointly perform spatial-temporal target appearance modeling and context-guided feature
adaption in a siamese framework. Speciﬁcally, we use a ST-GCN to model the historical exemplars with a spatial-temporal graph. Then,
the generated ST-feature is combined with the current context feature to learn an adaptive graph, which is used by CT-GCN to produce the
adaptive feature. This feature is evaluated on the search image embedding via a cross-correlation layer (XCorr) for target localization.

with context information of the current search image:

f (zt−T :t−1, xt) = ψGCN (Zt−T :t−1, Xt) ⋆ Xt + b,

(2)

where ψGCN denotes the proposed graph convolutional
transformation. It aims to learn robust spatial-temporal fea-
tures of the target object in previous frames t − T : t − 1,
guided by the context information of the current search im-
age embedding Xt. T controls the time range for remem-
bering historical information. However, learning ψGCN is
not efﬁcient since it suffers from high computational burden
for modeling the message passing between current contex-
t information Xt and each of the historical exemplar em-
beddings Zt−T :t−1. To reduce the computational cost, we
further decompose ψGCN into two sequential graph con-
volution modules named Spatial-Temporal GCN (ST-GCN)
ψ1 and ConText GCN (CT-GCN) ψ2. As a result, the de-
composed formulation is:

f (zt−T :t−1, xt) = ψ2(ψ1(Zt−T :t−1), Xt) ⋆ Xt + b,

(3)

where ψ1 conducts spatial-temporal target appearance mod-
eling for historical exemplars and generates aggregated ST-
feature V1 = ψ1(Zt−T :t−1). ψ2 takes V1 and the con-
text information of current search image embedding Xt for
learning the adaptive feature V2, which is then evaluated
on the search image embedding Xt via cross-correlation. In
the ofﬂine training stage, the loss of an exemplars-instance
pair is generally represented as a logistic function [2] :

L(zt−T :t−1, xt, Y) =

1
|∇| X

u∈∇

log(1 + exp(−Y[u]R[u])),

(4)
where ∇ is the set of all the shifting positions on the search
image and u denotes a sample of the same size with the

target template. Y[u] ∈ {+1, −1} is the ground-truth label
as in [2], and R[u] = V2[u] · Xt[u] is the response score.

In the following, we ﬁrst introduce the preliminary of our
main building block, GCN [31], which generalizes CNN to
graphs. Then we illustrate both ST-GCN and CT-GCN. The
details of our tracking method are ﬁnally presented.

3.1. Preliminary: Graph Convolutional Networks

Given an undirected graph G = (V, E) with M nodes
V, a set of edges E between nodes, an adjacency matrix
A ∈ RM ×M , and a degree matrix Λii = Pj
Aij . We for-
mulate a linear transformation of graph convolution as the
multiplication of a graph signal X ∈ RD×M (the colum-
n vector Xi· ∈ RD is the feature representation at the ith
node) with a ﬁlter W ∈ RD×C :

2 X⊤W,

V = ˆΛ− 1

2 ˆA ˆΛ− 1

(5)
ˆAij .
where ˆA = A + I, I is the identity matrix. ˆΛii = Pj
In this formulation, the output is a C × M matrix V. Note
that a GCN can be built by stacking multiple graph convolu-
tional layers of the form of Eq. (5), each layer followed by
a non-linear operation (such as ReLU). Readers can refer
to [31] for more details and an in-depth discussion.

3.2. Target Appearance Modeling via ST GCN

Spatial-temporal structure of target object is crucial for
robust visual tracking. However, most existing siamese
network based methods either describe the target appear-
ance from the global view or ignore the historical informa-
tion in an end-to-end training, resulting in high sensitivity
to signiﬁcant appearance change.
In this section, we de-
sign a spatial-temporal graph to form a structured represen-
tation of the historical exemplar (target object) sequence.

4652

Speciﬁcally, the shared ConvNet φ in the exemplar branch
(the top of Figure 3) takes the historical exemplar images
{zi}t−T
i=t−1 as inputs and produces the corresponding em-
beddings {Zi}t−T
i=t−1. Here, Zi ∈ RD1×Mz , where D1 and
Mz represent the feature dimensionality and the number of
parts respectively. Although other automatic part genera-
tion methods [84, 67] can be exploited, for simplicity and
efﬁciency, we follow [7, 8] to consider each D1 × 1 × 1
grid of the feature map Zi as a target part. To perform
spatial-temporal modeling of target object, we construct an
undirected ST-graph G1 = (V1, E1) on an exemplar embed-
ding sequence with Mz parts (nodes) and T frames featur-
ing both intra-exemplar and inter-exemplar relationships.

In the graph G1, the node set V1 = {vij|i = t − 1, ..., t −
T, j = 1, ..., Mz} consists of all the target parts in an
exemplar embedding sequence. The edge set E1 is com-
posed of two types of edges: (1) Spatial edges E S
1 repre-
sents the intra-exemplar connection at each frame: E S
1 =
{vijvik|1 6 j, k 6 Mz, j 6= k}. Note that similar with [8],
we adopt a fully-connected graph to depict the spatial re-
lationships since all the target parts may have interactions
under various appearance changes. In addition, in our ex-
periment, we ﬁnd that the fully-connected graph achieves
favorable performance while needs less graph convolutional
layers than other types of graphs such as k-nearest neighbor
graph [8]. (2) Follow [72], we connect the parts with the
same location in consecutive frames as the temporal edges
E T
1 = {vijvi+1,j}. As a result, the information can be prop-
agated in the temporal domain. With both types of edges,
each node is connected to at most Mz +1 nodes among a to-
tal of MzT nodes in V1, which makes the ST-graph sparse
and reduces the computational cost of graph convolution.
Based on the ST-graph, we can obtain the corresponding
adjacency matrix A1 and stack multiple graph convolution-
al layers of Eq. (5) to construct the ST-GCN. The ST-GCN
t−T
then generates reﬁned feature vectors {ˆZi}
i=t−1 for each
node of the spatial-temporal graph, ˆZi ∈ RD2×Mz . To re-
duce the computational burden of the following layers, we
then aggregate the features along the temporal axis to pro-
duce the compact ST-feature V1 ∈ RD2×Mz :

V1 = MaxPoolingT ([ˆZt−T , ˆZt−T +1, ..., ˆZt−1]),

(6)

where the MaxPoolingT operation is applied with a time
range T . V1 is then taken as input of the CT-GCN.

3.3. Target Feature Adaption via CT GCN

Our framework not only models the spatial-temporal
structure between target exemplars, but also incorporates
the context information of current search images to guide
the adaptive feature learning. To take full advantage of the
context information, we integrate a graph learning model
to our framework as shown in Figure 3, which generates an
adaptive graph structure for guiding the CT-GCN. As shown

in the bottom of Figure 3, taking the current search image
xt as input, the shared ConvNet produces the instance em-
bedding Xt ∈ RD1×Mx . To get the global information of
the search image, we utilize a convolutional layer followed
by a max pooling layer to generate a global feature xt with
the size of D1 × 1. Here, the convolutional layer has D2
ﬁlters with a kernel size of 3 × 3 and stride 1, and the size of
the pooling layer is Mx. Taking the global feature xt as the
current context information, we use a deconvolutional layer
to get an enlarged feature ˆXt, which is the same size as the
ST-feature V1. ˆXt is then fused with V1 by element-wise
addition as follows:

Vx = V1 + ˆXt,

(7)

where Vx considers both the spatial-temporal feature of tar-
get object and the context information of current frame. To
perform graph learning for robust feature adaption, we use
Vx to generate an adaptive graph G2 = (V2, E2) with the
adjacency matrix A2 deﬁned as:

Aji

2 =

exp(g(Vx,i)⊤h(Vx,j))
i=1 exp(g(Vx,i)⊤h(Vx,j))

PMz

,

(8)

where Vx,i is the ith column vector of Vx, g(·) and h(·) are
two 1 × 1 convolutional layers with D1 ﬁlters.

With the learned graph, we are able to construct the CT-
GCN via Eq. (5), which takes the ST-feature as input and
produces the adaptive feature V2 ∈ RD1×Mz for target lo-
calization in tracking process.

3.4. The Proposed Tracking Algorithm

Network Structure. As shown in Figure 3, we use the
modiﬁed AlexNet [34] pre-trained on ImageNet [54] as the
shared ConvNet. The weights of the ﬁrst three conv lay-
ers are ﬁxed and only the last two conv layers are ﬁne-
tuned. We also add an additional 3 × 3 conv layer to reduce
the output channel dimensionality to D1 = 256. The part
numbers of the exemplar and search image embedding are
Mz = 6 × 6 = 36 and Mx = 22 × 22 = 484, respec-
tively. For the ST-GCN, we adopt 2 graph convolutional
layers with the output channel dimensionality of 512 and
256 (D2). The CT-GCN also has 2 graph convolutional lay-
ers with 384 and 256 channel numbers. Following [69], we
apply the LeakyReLU as the activation function for both
ST-GCN and CT-GCN.
Ofﬂine Training. We use videos from the video object de-
tection dataset of the ImageNet Large Scale Visual Recog-
nition Challenge (ILSVRC2015) [54] as training data. The
dataset contains almost 4500 videos with a total of more
than one million annotated frames. In each video snippet of
an object, we collect each training sample of T + 1 frames
within the nearest 100 frames. We use the former T frames
as exemplar images and take the last one as the search im-
age. We adopt the ADAM optimizer with learning rate of

4653

0.005 and set weight decay to 5e − 5. The model is trained
for 50 epochs with a batch size of 24.
Tracking Inference. For the tracker initialization, we du-
plicate the ﬁrst frame T times as the exemplar images. We
set T = 10 in our experiments.
In the tracking process,
we use an interval τ = 7 to update the exemplar images,
which enables our method to effectively remember a long
range of historical information. Speciﬁcally, for every τ
frames, the ﬁrst exemplar image is removed and the new
exemplar is added. We use a ratio of 0.4 to smooth the new
exemplar with the initial exemplar. The target center can be
determined by locating the maximum value in the response
map RG generated by the cross-correlation layer, as shown
in Figure 3. Since different layers in a deep network charac-
terize the target from different perspectives [45, 25], we fur-
ther use the features from the 5-th conv layer of the shared
ConvNet to generate the other response map RS. The ﬁnal
response map is calculated by balancing RG and RS with a
coefﬁcient γ: R = γRG + (1 − γ)RS. γ is set to 0.7. Fol-
low [2], a cosine window is further added to the response
map to penalize large displacement.
Scale Estimation. To handle scale variations, we follow [2]
to search on three scales of the current search image with
scale factors of 1.0375{−1,0,1}. We update the scale by lin-
ear interpolation with a factor of 0.59 to provide damping.
To further speed up the tracker, we only use the response
map RS to estimate the scales, which also shows favorable
performance in the experiments.
Discussion. The proposed GCT consists of both ST-GCN
and CT-GCN, which can jointly perform spatial-temporal
target appearance modeling and feature adaption with con-
text information in an end-to-end framework. For the ST-
GCN, we design a ﬁxed spatial-temporal graph in consid-
eration of two factors. (1) Since the spatial-temporal graph
is large with MzT nodes, ﬁxing the adjacency matrix A1
is more computationally efﬁcient than ﬁne-tuning it [18].
Note that another graph-based tracking method [8] also
adopts a ﬁxed graph for appearance modeling. (2) Although
the temporal edges may not connect the same target part in
consecutive frames, the message can still be passed between
any related parts because the spatial edges in each frame are
fully-connected. In addition, ST-GCN has multiple layers
which can enlarge the receptive ﬁeld of each node. For the
CT-GCN, we use the search image to provide rich context
information such as target object and the surrounding back-
ground for guiding the feature adaption1. After the end-to-
end ofﬂine training with large-scale training videos, in the
online tracking process, the graph G2 can be automatical-
ly and adaptively produced with different ST-features and
context features. The effectiveness of both types of GCN is
veriﬁed in our experiments.

e
t
a
r
 
s
s
e
c
c
u
S

e
t
a
r
 
s
s
e
c
c
u
S

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

Success plots of OPE

CREST[2fps] [0.673] 
GCT[50fps] [0.670] 
MetaCREST[12fps] [0.667] 
ADNet[3fps] [0.659] 
BACF[35fps] [0.657] 
ACT[30fps] [0.657] 
PTAV[25fps] [0.654] 
ECO-HC[60fps] [0.652] 
TRACA[101fps] [0.652] 
MCCT-H[45fps] [0.641]

0.2

0.4

0.6

0.8

1

Overlap threshold

i

i

n
o
s
c
e
r
P

1

0.8

0.6

0.4

0.2

0

0

Precision plots of OPE

CREST[2fps] [0.908] 
ACT[30fps] [0.905] 
ADNet[3fps] [0.903] 
TRACA[101fps] [0.898] 
HDT[10fps] [0.889] 
MetaCREST[12fps] [0.883] 
PTAV[25fps] [0.879] 
ECO-HC[60fps] [0.874] 
GCT[50fps] [0.873] 
MUSTer[4fps] [0.865]

02

10
04
Location error threshold

03

(a) Results for OTB-2013 benchmark [70]

Success plots of OPE

GCT[50fps] [0.648] 
ADNet[3fps] [0.647] 
ECO-HC[60fps] [0.643] 
MetaCREST[12fps] [0.637] 
STRCF[31fps] [0.632] 
PTAV[25fps] [0.631] 
MCCT-H[45fps] [0.628] 
ACT[30fps] [0.625] 
CREST[2fps] [0.624] 
BACF[35fps] [0.622]

0.2

0.4

0.6

0.8

1

Overlap threshold

i

i

n
o
s
c
e
r
P

1

0.8

0.6

0.4

0.2

0

0

Precision plots of OPE

ADNet[3fps] [0.880]
ACT[30fps] [0.859]
ECO-HC[60fps] [0.856] 
MetaCREST[12fps] [0.855] 
GCT[50fps] [0.854]
HDT[10fps] [0.848] 
PTAV[25fps] [0.841] 
CREST[2fps] [0.839] 
MCCT-H[45fps] [0.834] 
BACF[35fps] [0.824]

02

10
04
Location error threshold

03

5
0

5
0

(b) Results for OTB-2015 benchmark [71]

Figure 4. Quantitative results on OTB datasets. Our GCT method
performs favorably against the state-of-the-art trackers.

Figure 5. 11 attributes comparison of 7 real-time trackers on OTB-
2015 in term of AUC. The proposed GCT method performs favor-
ably against the state-of-the-arts.

4. Experimental Results

We conduct extensive experiments 2 on 4 challenging
datasets including the OTB-2013 Object Tracking Bench-
mark [70] with 50 sequences, its updated version OTB-
2015 [71] with 100 sequences, VOT2017 benchmark [33]
with 60 videos, and UAV123 benchmark [47] with 123 aeri-
al tracking videos. Our tracker is implemented on Tensor-
Flow. The hardware environment includes an Intel E5-2687
3.0GHz CPU, 256GB RAM and a NVidia 1080Ti GPU.

4.1. Experiments on OTB

Evaluation Protocol. Following the protocol used in the
recently published methods [84, 73, 81, 57], we report the
results in one-pass evaluation (OPE) [70]. The evaluation is
based on two metrics: success plot and precision plot. (1)
The success plot illustrates the ratios of successful frames

1In siamese learning, the exemplar images also include background in-

2http://nlpr-web.ia.ac.cn/mmc/homepage/jygao/

formation surrounding the target object.

gct_cvpr2019.html (project page of our GCT)

4654

00.10.20.30.40.50.60.70.8IVOPRSVOCCDEFMBFMIPROVBCLRAUCGCTECO-HCSTRCFPTAVMCCT-HACTTRACASiamFCTable 1. Comparison with 6 state-of-the-art trackers on the OTB-
2013 and OTB-2015, based on AUC score. Our method provides
comparable results against the state-of-the-art trackers.
VITAL

MDNet

SANet

CCOT

Method

OTB-2013
OTB-2015
Speed(FPS)

[49]
70.8
67.8
2.6

[15]
68.6
69.2
1.0

ECO
[9]
70.9
69.1
6.0

DSLT
[43]
68.3
66.0
5.7

[11]
67.2
67.1
0.6

[58]
71.0
68.2
1.5

GCT
(Ours)
67.0
64.8
49.8

over the range of thresholds [0, 1], where Area-under-the-
curve (AUC) is included in the legend. (2) The precision
plot shows the average distance precision along with a range
of thresholds, and the average Distance precision (DP) score
at 20 pixels for each tracker is reported.
Baseline Methods. We evaluate our GCT method with 29
trackers in the OTB benchmark [70, 71] and other state-of-
the-art tracking methods that presented at top conferences
and journals, including MetaCREST (ECCV 2018) [50],
ACT (ECCV 2018) [4], MCCT-H (CVPR 2018) [64], TRA-
CA (CVPR 2018) [5], STRCF (CVPR 2018) [37], CREST
(ICCV 2017) [57], PTAV (ICCV 2017) [14], BACF (IC-
CV 2017) [16], ECO-HC (CVPR 2017) [9], ACFN (CVPR
2017) [6], ADNet (CVPR 2017) [73], CSR-DCF (CVPR
2017) [44], Staple CA (CVPR 2017) [48], CFNet (CVPR
2017) [63], SINT (CVPR 2016, only for OTB-2013) [60],
Staple (CVPR 2016) [1], HDT (CVPR 2016) [51], SiamFC
(ECCVW 2016) [2], SRDCF (ICCV 2015) [10], MUSTer
(CVPR 2015) [29], CNN-SVM (ICML 2015) [28], RPT
(CVPR 2015) [39], KCF (T-PAMI 2015) [27] and MEEM
(ECCV 2014) [75].
Quantitative Evaluation. Figure 4 illustrates the success
and precision plots of the overall performance among com-
pared trackers. To make it clear, we only plot the top
10 ranked methods. The proposed GCT approach per-
forms favorably with AUC of (67.0%, 64.8%) and DP of
(87.3%, 85.4%) on the OTB-2013 and OTB-2015, respec-
tively. SINT [60], CFNet [63], and SiamFC [2] are three
state-of-the-art siamese based trackers, which provide the
results with an AUC score of 63.5%, 61.0%, and 60.7% on
OTB-2013, respectively. Compared to them, our method
gets an absolute gain of 3.5%, 6.0%, and 6.3%. Another
siamese based tracker, DaSiamRPN [86], has the AUC s-
core of 65.9% on OTB-2015, which is slightly better than
our method (64.8%). However, DaSiamRPN uses other
large-scale datasets for model training, such as COCO De-
tection dataset [41] and Youtube-BB [53]. This strategy can
also be used to further boost the performance of our method.
Overall, compared with the state-of-the-arts, the proposed
GCT achieves better or comparable results. Note that the
DP score of our method is not very signiﬁcant, which may
because of the low resolution of the response map (17 × 17)
and its interpolation process in target localization. This
can be improved by training a siamese network with high-
resolution response map like [65]. We also compare GC-
T to the currently topmost non-realtime trackers including
MDNet (CVPR2016) [49], SANet (CVPRW2017) [15], E-

0.35
0.3
0.25
0.2
0.15
0.1
0.05

GCT

SiamFC

LSART[0.323] 
CFWCR[0.303] 
CFCF[0.286] 
ECO[0.280] 
GCT[0.274] 
Gnet[0.274] 
MCCT[0.270] 
CCOT[0.267] 
CSRDCF[0.256] 
SiamDCF[0.249] 
MCPF[0.248]

49

45

41

37

33

29

25

21

17

13

9

5

1

Figure 6. Comparison of EAO scores on VOT2017 challenge. The
gray horizontal line denotes the VOT2017 state-of-the-art bound.
0.3

0.25

0.2

0.15

0.1

0.05

0

49

45

41

37

33

29

25

21

17

13

9

5

1

GCT[0.269] 
CSRDCF++[0.212] 
SiamFC[0.182] 
ECOhc[0.177] 
Staple[0.170] 
KFebT[0.169] 
ASMS[0.168] 
SSKCF[0.164] 
CSRDCF[0.158] 
UCT[0.145] 
MOSSEca[0.139]

Figure 7. The EAO scores for the real-time experiment on
VOT2017 challenge. GCT performs the best.

CO (CVPR2017) [9], CCOT (ECCV2016) [11], DSLT (EC-
CV2018) [43], and VITAL (CVPR2018) [58] . In Table 1,
the AUC scores of the algorithms on both benchmarks are
presented along with the run-time speed. Our method has
comparable performance and achieves a signiﬁcant speed
improvement. Moreover, MDNet, SANet, and VITAL train
and test deep models for tracking using videos from the
same ALOV/OTB/VOT domain, which is forbidden in VOT
challenges due to the overﬁtting problem [2].
Attribute-based Evaluation. We further analyze the per-
formance of our GCT tracker under different attributes on
OTB-2015 benchmark. Figure 5 shows the comparison of
GCT and another seven state-of-the-art real-time trackers.
Speciﬁcally, our method achieves the best under 6 out of 11
attributes. For the rest ﬁve, GCT performs favorably.
4.2. Experiments on VOT2017

We compare our GCT with the state-of-the-art methods
on VOT 2017 benchmark [33, 32]. The performance is eval-
uated by Expected Average Overlap (EAO), which reﬂects
both robustness and accuracy. Figure 6 reports the results
of ours against other 51 trackers with respect to the EAO
score. As presented in the VOT2017 report [33], tracker-
s whose EAO values exceed 0.203 will be considered as
state-of-the-art methods. Our proposed GCT ranks the ﬁfth
with the EAO score of 0.274. Figure 7 shows the EAO s-
cores in the real-time experiment of VOT2017. Our tracker
achieves the best performance with the EAO score of 0.269
and outperforms other real-time methods by a large margin.
4.3. Experiments on UAV123

Finally, we evaluate the proposed GCT on the recently
proposed aerial video dataset, UAV123 [47], which has 123
UAV tracking sequences with more than 110K frames. GCT
is compared with all 14 trackers reported in [47] and other
real-time state-of-the-art methods including MCCT-H [64],
STRCF [37], ECO-HC [9], and Staple [1]. Figure 8 again

4655

e

t

a
r
 
s
s
e
c
c
u
S

0.8

0.6

0.4

0.2

0

0

Success plots of OPE

GCT [0.508]
ECO-HC [0.506] 
STRCF [0.481] 
SRDCF [0.464] 
MCCT-H [0.458] 
Staple [0.450]
ASLA [0.407] 
SAMF [0.396] 
MEEM [0.392] 
MUSTER [0.391]

0.2

0.4

0.6

0.8

1

Overlap threshold

i

i

n
o
s
c
e
r
P

0.8

0.6

0.4

0.2

0

0

Precision plots of OPE

GCT [0.732]
ECO-HC [0.725] 
STRCF [0.681] 
SRDCF [0.676] 
Staple [0.666] 
MCCT-H [0.662]
MEEM [0.627] 
SAMF [0.592] 
MUSTER [0.591] 
DSST [0.586]

01

20

03

04

Location error threshold

5
0

e
t
a
r
 
s
s
e
c
c
u
S

1

0.8

0.6

0.4

0.2

0

0

Success plots of OPE

GCT[50fps] [0.670] 
ST-4L[41fps] [0.665]
ST-knn-4L[44fps] 
 [0.660]
ST-knn-2L[53fps] [0.652]
ST-1L[56fps] [0.645]

0.2

0.4

0.6

0.8

1

Overlap threshold

e
t
a
r
 
s
s
e
c
c
u
S

1

0.8

0.6

0.4

0.2

0

0

Success plots of OPE

GCT[50fps] [0.670]
CT-4L[47fps] [0.667] 
CT-1L[52fps] [0.661] 
CT-noGCN[55fps] [0.655]

0.2

0.4

0.6

0.8

1

Overlap threshold

Figure 8. Quantitative results on the UAV123 benchmark [40]. Our
proposed GCT method performs favorably.
Table 2. Analysis of our approach on the OTB-2013 and OTB-
2015. The impact of progressively integrating one component at
the time, from left to right, is displayed.

OTB-2013(%)
OTB-2015(%)
FPS(OTB-2015)

SiamFC =⇒ S-GCN =⇒ST-GCN =⇒CT-GCN
60.7
57.7
76.1

67.0
64.8
49.8

62.5
60.2
66.7

64.9
63.5
58.6

shows that our proposed GCT performs favorably.

4.4. Further Remarks

Component Contribution. To verify the contributions of
each component in our framework, we implement and eval-
uate four variants of our approach on OTB-2013 and OTB-
2015 benchmarks. In Table 2, the impact of progressively
adding one component, from left to right, is presented. For
simplicity, we take the results on OTB-2015 for illustration
here. The ﬁrst is the baseline SiamFC3, which removes the
following GCN modules and only uses the response map
RS for target localization. We then add a spatial GCN
(S-GCN) on SiamFC and use the fused response map in
tracking process. Speciﬁcally, S-GCN removes the tempo-
ral edges in ST-GCN and sets T = 1. S-GCN outperforms
SiamFC by an absolute gain of 2.5%, which shows the part-
based spatial modeling is useful in visual tracking. Addi-
tionally incorporating our proposed ST-GCN elevates us to
an AUC score of 63.5%, leading to a relative gain of 5.5%
compared to S-GCN. The signiﬁcant result clearly shows
the effectiveness of our spatial-temporal appearance mod-
eling. Finally, we add CT-GCN to our framework, which
obtains a relative gain of 2.0% compared to ST-GCN. Ta-
ble 2 also shows the impact on the tracker speed achieved by
our components. Overall, the proposed GCT with both ST-
GCN and CT-GCN achieves the best tracking performance
and a favorable run-time speed.
Detailed Analysis of the ST-GCN. To quantitatively an-
alyze different depths of the ST-GCN, we design another
two variants, ST-1L and ST-4L. ST-1L has 1 graph convo-
lutional layers with output channel number of 256. The 4-
layer model ST-4L has channel numbers as 512 → 1024 →
512 → 256. In the left of Figure 9, we do not ﬁnd much
gain by adding more layers above our 2-layer ST-GCN
model. To make our tracker efﬁcient, we set the number
of layers in ST-GCN to 2. We also explore other graph

3Since this baseline is implement by ourselves, the results are slightly

different from the initial SiamFC tracker [2]

Figure 9. Ablation study of both ST-GCN and CT-GCN on OTB-
2013 benchmark.

structures for ST-GCN. As shown in Figure 9, we design
two baselines ST-knn-2L and ST-knn-4L, which adopt an
8-nearest-neighbor graph used in [8] to represent spatial
edges. Although ST-knn-4L can achieve similar perfor-
mance compared to our method, it is less efﬁcient since it
needs more graph convolutional layers.
Detailed Analysis of the CT-GCN. We also evaluate the
effect of different numbers of layers in CT-GCN and design
similar baselines with them in ST-GCN. Figure 9 (b) shows
CT-1L gets inferior results while CT-4L has lower running
speed.
In addition, to verify the effectiveness of the CT-
GCN, we develop a baseline method, CT-noGCN, which
removes the graph convolutional layers. CT-noGCN only
uses the scores produced by Eq. (8) to generate the adaptive
feature via linear combination. We can ﬁnd that our pro-
posed GCT outperforms it by a relative gain of 2.3%. In
fact, GCT can further conduct message passing between re-
lated parts based on the learned graph, which is better than
the linear combination with the generated scores.

5. Conclusions

In this paper, we propose a graph convolutional tracking
framework, which can jointly achieve spatial-temporal tar-
get appearance modeling and context-aware adaptive learn-
ing for robust target localization in a uniﬁed framework. We
show that by carefully designing the spatial-temporal GCN
and the context GCN, the proposed GCT achieves state-of-
the-art results in both accuracy and speed. The encourag-
ing performance is demonstrated in extensive experiments
of four challenging benchmarks. In the future, we intend
to explore other types of graph neural networks for visu-
al tracking, such as graph embedding and graph attention
model. We will also apply our method in other computer
vision tasks, e.g. multi-object tracking and person re-id.

Acknowledgements

This work is supported in part by the National Natu-
ral Science Foundation of China under Grants 61432019,
61572498, 61532009, 61728210, 61721004, 61751211,
61572296, 61720106006 and U1705262, and the Key Re-
search Program of Frontier Sciences, CAS, Grant NO.
QYZDJ-SSW-JSC039, the Beijing Natural Science Foun-
dation 4172062, and Youth Innovation Promotion Associa-
tion CAS 2018166.

4656

References

[1] Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej
Miksik, and Philip H. S. Torr. Staple: Complementary learn-
ers for real-time tracking. In CVPR, 2016.

[2] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea
Vedaldi, and Philip HS Torr. Fully-convolutional siamese
networks for object tracking. In ECCV Workshops, 2016.

[3] Adel Bibi, Matthias Mueller, and Bernard Ghanem. Target
response adaptation for correlation ﬁlter tracking. In ECCV,
2016.

[4] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and
Huchuan Lu. Real-time actor-critictracking. In ECCV, 2018.

[5] Jongwon Choi, Hyung Jin Chang, Tobias Fischer, Sangdoo
Yun, Kyuewang Lee, Jiyeoup Jeong, Yiannis Demiris, and
Jin Young Choi. Context-aware deep feature compression
for high-speed visual tracking. In ECCV, 2018.

[6] Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fis-
cher, Yiannis Demiris, and Young Choi Jin. Attentional cor-
relation ﬁlter network for adaptive visual tracking. In CVPR,
2017.

[7] Zhen Cui, Shengtao Xiao, Jiashi Feng, and Shuicheng Yan.

Recurrently target-attending tracking. In CVPR, 2016.

[8] Zhen Cui, Jian Yang, et al. Spectral ﬁlter tracking. arXiv

preprint arXiv:1707.05553, 2017.

[9] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Michael Felsberg. Eco: Efﬁcient convolution operators for
tracking. In CVPR, 2017.

[10] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and
Michael Felsberg. Learning spatially regularized correlation
ﬁlters for visual tracking. In ICCV, 2015.

[11] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,
and Michael Felsberg. Beyond correlation ﬁlters: learning
continuous convolution operators for visual tracking. In EC-
CV, 2016.

[12] Xingping Dong and Jianbing Shen. Triplet loss in siamese

network for object tracking. In ECCV, 2018.

[13] Lingyu Duan, Yihang Lou, Shiqi Wang, Wen Gao, and Yong
Rui. Ai oriented large-scale video management for smart c-
ity: Technologies, standards and beyond. IEEE MultiMedia,
2018.

[14] Heng Fan and Haibin Ling. Parallel tracking and verifying:
A framework for real-time and high accuracy visual tracking.
In ICCV, 2017.

[15] Heng Fan and Haibin Ling. Sanet: Structure-aware network
for visual tracking. In IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW), 2017.

[16] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.
Learning background-aware correlation ﬁlters for visual
tracking. In ICCV, 2017.

[17] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. A uniﬁed
personalized video recommendation via dynamic recurrent
neural networks. In ACM MM, 2017.

[19] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. I know the
relationships: Zero-shot action recognition via two-stream
graph convolutional networks and knowledge graphs.
In
AAAI, 2019.

[20] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. Smart:
IEEE

Joint sampling and regression for visual tracking.
Transactions on Image Processing, PP(99):1–1, 2019.

[21] Junyu Gao, Tianzhu Zhang, Xiaoshan Yang, and Chang-
IEEE Transactions on

sheng Xu. Deep relative tracking.
Image Processing, 26(4):1845–1858, 2017.

[22] Junyu Gao, Tianzhu Zhang, Xiaoshan Yang, and Chang-
sheng Xu. P2t: Part-to-target tracking via deep regres-
sion learning.
IEEE Transactions on Image Processing,
27(6):3074–3086, 2018.

[23] Qing Guo, Wei Feng, Ce Zhou, Rui Huang, Liang Wan, and
Song Wang. Learning dynamic siamese network for visual
object tracking. In ICCV, 2017.

[24] Junwei Han, Xiang Ji, Xintao Hu, Dajiang Zhu, Kaiming Li,
Xi Jiang, Guangbin Cui, Lei Guo, and Tianming Liu. Rep-
resenting and retrieving video shots in human-centric brain
imaging space.
IEEE Transactions on Image Processing,
22(7):2723–2736, 2013.

[25] Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. A
In

twofold siamese network for real-time object tracking.
CVPR, 2018.

[26] David Held, Sebastian Thrun, and Silvio Savarese. Learning
to track at 100 fps with deep regression networks. In ECCV,
2016.

[27] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge
Batista. High-speed tracking with kernelized correlation ﬁl-
ters.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 37(3):583–596, 2015.

[28] Seunghoon Hong, Tackgeun You, Suha Kwak, and Bohyung
Han. Online tracking by learning discriminative saliency
map with convolutional neural network. In ICML, 2015.

[29] Zhibin Hong, Zhe Chen, Chaohui Wang, Xue Mei, Danil
Prokhorov, and Dacheng Tao. Multi-store tracker (muster):
A cognitive psychology inspired approach to object tracking.
In CVPR, 2015.

[30] Chen Huang, Simon Lucey, and Deva Ramanan. Learning
policies for adaptive tracking with deep feature cascades. In
ICCV, 2017.

[31] Thomas N Kipf and Max Welling. Semi-supervised classiﬁ-

cation with graph convolutional networks. In ICLR, 2016.

[32] Matej Kristan, Jiri Matas, Aleˇs Leonardis, Tom´aˇs Voj´ıˇr, Ro-
man Pﬂugfelder, Gustavo Fernandez, Georg Nebehay, Fatih
Porikli, and Luka ˇCehovin. A novel performance evaluation
methodology for single-target trackers. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2016.

[33] Matej Kristan, Roman Pﬂugfelder, Ales Leonardis, Jir-
i Matas, Fatih Porikli, Luka Cehovin, Georg Nebehay, Gus-
tavo Fernandez, Tomas Vojir, Adam Gatt, et al. The visual
object tracking vot2017 challenge results. In ICCV, 2017.

[18] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. Watch,
think and attend: End-to-end video classiﬁcation via dynam-
ic knowledge evolution modeling. In ACM MM, 2018.

[34] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, 2012.

4657

[35] Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, and Yu-
Chiang Frank Wang. Multi-label zero-shot learning with
structured knowledge graphs. In CVPR, 2018.

[36] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.
High performance visual tracking with siamese region pro-
posal network. In CVPR, 2018.

[37] Feng Li, Cheng Tian, Wangmeng Zuo, Lei Zhang, and Ming-
Hsuan Yang. Learning spatial-temporal regularized correla-
tion ﬁlters for visual tracking. In CVPR, 2018.

[38] Yang Li and Jianke Zhu. A scale adaptive kernel correlation

ﬁlter tracker with feature integration. In ECCV, 2014.

[39] Yang Li, Jianke Zhu, and Steven CH Hoi. Reliable patch
trackers: Robust visual tracking by exploiting reliable patch-
es. In CVPR, 2015.

[40] Pengpeng Liang, Erik Blasch, and Haibin Ling.

En-
coding color information for visual tracking: algorithms
and benchmark.
IEEE Transactions on Image Processing,
24(12):5630–5644, 2015.

[41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollar, and Larry Zit-
nick. Microsoft coco: Common objects in context. In ECCV,
2014.

[42] Si Liu, Zhen Wei, Yao Sun, Xinyu Ou, Junyu Lin, Bin Li-
u, and Ming-Hsuan Yang. Composing semantic collage for
image retargeting. IEEE Transactions on Image Processing,
27(10):5032–5043, 2018.

[43] Xiankai Lu, Chao Ma, Bingbing Ni, Xiaokang Yang, Ian
Reid, and Ming-Hsuan Yang. Deep regression tracking with
shrinkage loss. In ECCV, 2018.

[44] Alan Lukei, Tom Voj, Luka ehovin, Ji Matas, and Matej Kris-
tan. Discriminative correlation ﬁlter with channel and spatial
reliability. In CVPR, 2017.

[45] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan
Yang. Hierarchical convolutional features for visual tracking.
In ICCV, 2015.

[46] Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gup-
ta. The more you know: Using knowledge graphs for image
classiﬁcation. In CVPR, 2017.

[47] Matthias Mueller, Neil Smith, and Bernard Ghanem. A
benchmark and simulator for uav tracking. In ECCV, 2016.
[48] Matthias Mueller, Neil Smith, and Bernard Ghanem.

Context-aware correlation ﬁlter tracking. In CVPR, 2017.

[49] Hyeonseob Nam and Bohyung Han. Learning multi-domain
convolutional neural networks for visual tracking. In CVPR,
2016.

[50] Eunbyung Park and Alexander C Berg. Meta-tracker: Fast
In

and robust online adaptation for visual object trackers.
ECCV, 2018.

[51] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao,
Qingming Huang, and Jongwoo Lim Ming-Hsuan Yang.
Hedged deep tracking. In CVPR, 2016.

[52] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-
temporal representation with pseudo-3d residual networks.
In ICCV, 2017.

[53] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan,
and Vincent Vanhoucke. Youtube-boundingboxes: A large
high-precision human-annotated data set for object detection
in video. In CVPR, 2017.

[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large s-
cale visual recognition challenge.
International journal of
computer vision, 115(3):211–252, 2015.

[55] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Ha-
genbuchner, and Gabriele Monfardini. The graph neural
network model.
IEEE Transactions on Neural Networks,
20(1):61–80, 2009.

[56] Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen, and X-
iaogang Wang. Person re-identiﬁcation with deep similarity-
guided graph neural network. In ECCV, 2018.

[57] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson
Lau, and Ming-Hsuan Yang. Crest: Convolutional residual
learning for visual tracking. In ICCV, 2017.

[58] Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao
Bao, Wangmeng Zuo, Chunhua Shen, Rynson Lau, and
Ming-Hsuan Yang. Vital: Visual tracking via adversarial
learning. In ECCV, 2018.

[59] Chong Sun, Dong Wang, and Huchuan Lu. Occlusion-aware
fragment-based tracking with spatial-temporal consistency.
IEEE Transactions on Image Processing, 25(8):3814–3825,
2016.

[60] Ran Tao, Efstratios Gavves, and Arnold W M Smeulders.

Siamese instance search for tracking. In CVPR, 2016.

[61] Zhu Teng, Junliang Xing, Qiang Wang, Congyan Lang,
Songhe Feng, Yi Jin, et al. Robust object tracking based
on temporal and spatial deep networks. In ICCV, 2017.

[62] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In ICCV, 2015.

[63] Jack Valmadre, Luca Bertinetto, Joo F Henriques, Andrea
Vedaldi, and Philip H. S Torr. End-to-end representation
learning for correlation ﬁlter based tracking. In CVPR, 2017.
[64] Ning Wang, Wengang Zhou, Qi Tian, Richang Hong, Meng
Wang, and Houqiang Li. Multi-cue correlation ﬁlters for ro-
bust visual tracking. In CVPR, 2018.

[65] Qiang Wang, Jin Gao, Junliang Xing, Mengdan Zhang, and
Weiming Hu. Dcfnet: Discriminant correlation ﬁlters net-
work for visual tracking. arXiv preprint arXiv:1704.04057,
2017.

[66] Qiang Wang, Zhu Teng, Junliang Xing, Jin Gao, Weiming
Hu, and Stephen Maybank. Learning attentions: residual
attentional siamese network for high performance online vi-
sual tracking. In CVPR, 2018.

[67] Xiaolong Wang and Abhinav Gupta. Videos as space-time

region graphs. In ECCV, 2018.

[68] Xiao Wang, Chenglong Li, Bin Luo, and Jin Tang. Sint++:
Robust visual tracking via adversarial positive instance gen-
eration. In CVPR, 2018.

[69] Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot
recognition via semantic embeddings and knowledge graphs.
In CVPR, 2018.

[70] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object

tracking: A benchmark. In CVPR, 2013.

[71] Yi Wu, Jongwoo Lim, and Ming Hsuan Yang. Object track-
ing benchmark. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 37:1834–1848, 2015.

4658

[72] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-
ral graph convolutional networks for skeleton-based action
recognition. arXiv preprint arXiv:1801.07455, 2018.

[73] Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun,
and Young Choi Jin. Action-decision networks for visual
tracking with deep reinforcement learning. In CVPR, 2017.
[74] Dingwen Zhang, Deyu Meng, and Junwei Han. Co-saliency
detection via a self-paced multiple-instance learning frame-
work. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 39(5):865–878, 2017.

[75] Jianming Zhang, Shugao Ma, and Stan Sclaroff. MEEM:
robust tracking via multiple experts using entropy minimiza-
tion. In ECCV, 2014.

[76] Kaihua Zhang, Lei Zhang, Qingshan Liu, David Zhang, and
Ming-Hsuan Yang. Fast visual tracking via dense spatio-
temporal context learning. In ECCV, 2014.

[77] Tianzhu Zhang, Bernard Ghanem, Si Liu, and Narendra
Ahuja. Robust visual tracking via multi-task sparse learn-
ing. In CVPR, 2012.

[78] Tianzhu Zhang, Bernard Ghanem, Si Liu, and Narendra
Ahuja. Robust visual tracking via structured multi-task s-
parse learning.
International Journal of Computer Vision,
101(2):367–383, 2013.

[79] Tianzhu Zhang, Si Liu, Narendra Ahuja, Ming-Hsuan Yang,
and Bernard Ghanem. Robust Visual Tracking via Consis-
tent Low-Rank Sparse Learning.
International Journal of
Computer Vision, 111(2):171–190, 2015.

[80] Tianzhu Zhang, Si Liu, Changsheng Xu, Bin Liu, and Ming-
Hsuan Yang. Correlation particle ﬁlter for visual tracking.
IEEE Transactions on Image Processing, 27(6):2676–2687,
2018.

[81] Tianzhu Zhang, Changsheng Xu, and Ming-Hsuan Yang.
Multi-task correlation particle ﬁlter for robust object track-
ing. In CVPR, 2017.

[82] Tianzhu Zhang, Changsheng Xu, and Ming-Hsuan Yang.
Learning multi-task correlation particle ﬁlters for visual
tracking.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 41(2):365–378, 2019.

[83] Tianzhu Zhang, Changsheng Xu, and Ming-Hsuan Yang.
Robust structural sparse tracking.
IEEE Transactions on
Pattern Analysis and Machine Intelligence, 41(2):473–486,
2019.

[84] Yunhua Zhang, Lijun Wang, Jinqing Qi, Dong Wang,
Mengyang Feng, and Huchuan Lu. Structured siamese net-
work for real-time visual tracking. In ECCV, 2018.

[85] Yuhui Zheng, Le Sun, Shunfeng Wang, Jianwei Zhang, and
Jifeng Ning. Spatially regularized structural support vector
machine for robust visual tracking.
IEEE transactions on
neural networks and learning systems, (99):1–11, 2018.

[86] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and
Weiming Hu. Distractor-aware siamese networks for visual
object tracking. In ECCV, 2018.

[87] Zheng Zhu, Wei Wu, Wei Zou, and Junjie Yan. End-to-end
ﬂow correlation tracking with spatial-temporal attention. In
CVPR, 2018.

4659

