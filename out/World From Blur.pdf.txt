World from Blur

Jiayan Qiu1

Xinchao Wang2

Stephen J. Maybank3

Dacheng Tao1

1 UBTECH Sydney AI centre, School of Compurter Science, FEIT, University of Sydney, Australia

2 Department of Computer Science, Stevens Institute of Technology, USA

3 Department of Computer Science and Information Systems, Birkbeck College, University of London, UK

jqiu3225@uni.sydney.edu.au

xinchao.w@gmail.com

sjmaybank@dcs.bbk.ac.uk

dacheng.tao@sydney.edu.au

Figure 1: Revealing the hidden 3D world in a blurred image. The proposed model, once trained, takes as input a single
blurred image and produces the reconstructed 3D scene concealed in the blurs.

Abstract

What can we tell from a single motion-blurred image?
We show in this paper that a 3D scene can be revealed. Un-
like prior methods that focus on producing a deblurred im-
age, we propose to estimate and take advantage of the hid-
den message of a blurred image, the relative motion trajec-
tory, to restore the 3D scene collapsed during the exposure
process. To this end, we train a deep network that jointly
predicts the motion trajectory, the deblurred image, and the
depth one, all of which in turn form a collaborative and
self-supervised cycle that supervise one another to repro-
duce the input blurred image, enabling plausible 3D scene
reconstruction from a single blurred image. We test the pro-
posed model on several large-scale datasets we constructed
based on benchmarks, as well as real-world blurred images,
and show that it yields very encouraging quantitative and
qualitative results.

1. Introduction

Motion blur is caused by the relative motion between
the scene objects and the camera during the exposure pro-
cess. When the motion of the scene objects, or the camera,

or both, is signiﬁcant during the exposure time, the image
tends to appear smeared along the direction of the relative
motion. Motion-blurred images are in many cases favored
by photographers and artists for aesthetic purpose, but sel-
dom by computer vision researchers, as many standard vi-
sion tools including detectors, trackers, and feature extrac-
tors have a hard time dealing with the blurs.

Much effort has thus been made in the image processing
and computer vision community to remove the “negative”
inﬂuences of the blurs. A straightforward and crude way is
to ignore blurred images, as done in SLAM systems [52]
because matching algorithms tend to fail on blurred im-
ages. Another more analytical way is to conduct deblurring,
which recovers a deblurred image from a blurred one. Over
the past decades, there has been a series of seminal work
along this line, demonstrating very promising and visually-
pleasing results.

Despite the excellent results achieved, deblurring meth-
ods limit its goal to producing a blur-free image and omit
the physical rationale behind the blurs. Since a blurred im-
age is the result of relative motions, it actually encodes the
motion information, though in a degraded way. The work
of [35] pioneered to extract a sequence of deblurred images
from a blurred one, yet still overlooked the motions con-

43218493

Self-supervised Cycle

Blurred image
Blurred image

Deblurred Image
Deblurred Image

Reconstruct
Rececon

Generate
Geene

Camera   
Motion

at t-1
aaa

at t

at t+1

Base 
BBB
coordinate
ccoor
cocoor
ccoor

Depth Map
Depth Map
Depth Map

Spatial Reconstruction

Generate
GG
GG

Figure 2: Illustration of our model. Given a blurred image, we construct a network with three modules to estimate camera motions, the
deblurred image, and the depth map, all of which form a self-supervised cycle to reconstruct the input blurred image and further enable the
3D scene reconstruction from the blurs.

cealed.

We show in this paper that more hidden message can
be revealed in a blurred image. As motion is encoded in
the blurs, we propose to explicitly estimate the concealed
motion trajectories buried under the smearings, based on
which the static 3D scene can be restored, as demonstrated
in Fig. 1. To this end, we train a collaborative network,
which jointly infers motion trajectory and depth, both of
which in absolute scale, as well as deblurred frame instant.
All the three estimations, in turn, form a self-supervised cy-
cle to reproduce the original blurred image, in aim to imitate
the physical blurring process. Through this cycle, the dif-
ferent modules supervise and enhance one another, enabling
plausible 3D reconstruction, as shown in Fig. 2.

Unarguably, estimating motion trajectories from a single
blurred image is an inverse problem. To recover the most le-
gitimate motion process while preserving a reasonable com-
putational load, we approximate a blurred image, for which
the creation process is continuous, as an average of a se-
quence of frames.
In this regard, we construct datasets
upon popular benchmarks, wherein each blurred image is
generated by taking the average of a clean-frame sequence
induced by deterministic motion. The constructed datasets
thus provide us with ground truths for training the collab-
orative network, and allow us to conduct depth-, motion-,
and frame-estimation, as well as the consequent 3D recon-
struction. The proposed model, once trained, yields very
promising results on synthetic and real-world blurred im-
ages.

Our contribution is therefore a novel approach that, for
the ﬁrst time, attempts to recover the absolute-scale 3D
scene from a single blurred image. It is accomplished by
training an innovative collaborative network that simultane-
ously estimates depth, clean images, and motion trajecto-
ries, each of which supervises another via a self-consistent
cycle to reproduce the input blurred image, on large-scale
datasets we build upon popular benchmarks. The proposed
approach produces encouraging results on synthetic and
real-world blurred images. Our code, model and datasets
will be released.

2. Related work

There have been numerous reconstruction methods aim-
ing to recover the 3D scene from one or multiple images, in-

cluding but not limited to reconstruction from shading [95],
from image texture [7, 8, 28], from camera motion [9], from
stereo [49], from scene recognition [48, 22, 26], from track-
ing process [90, 50, 84, 83] and from focus [56].

Our approach, however, focuses on estimating 3D re-
construction from a single blurred image, not relies on the
tracking process, which to our best knowledge is the ﬁrst
attempt along this line. As our cyclic strategy involves
three modules, camera trajectory estimation, deblurring,
and depth estimation, in what follows, we brieﬂy review
related work on these topics.

Camera trajectory estimation.

Recent camera-
trajectory estimation models can be broadly divided into
three categories, based on the supervision level. The ﬁrst
category is fully-supervised methods. For example, Agr-
wal et al. [1] learn good visual features from moving cam-
eras and predict the camera motion from a sequence of
images. Wang et al. [80, 81] implement a recurrent Con-
vNet architecture for visual odometry estimation. Ummen-
hofer et al. [77] design an architecture to learn the depth and
motion information from stereo images. The second cate-
gory is weakly-supervised models. Examples include the
approach of [34], which estimates the inter-frame motion by
utilizing the stereo geometry known a priori. Approaches in
the third category are unsupervised. For example, Vijaya-
narasimhan et al. [78] and Zhou et al. [97] propose unsu-
pervised methods to estimate the camera ego-motion using
the photometric error. The ones of [17, 18, 92] use stereo
information to estimate the odometry from a sequence of
images. Existing methods, however, conduct motion esti-
mation from clean images, which differs from our focus on
blurred images.

Deblurring. Blind deconvolution methods [62, 51, 4,
10, 88, 27, 93, 2, 19] for image deblurring have been widely
studied and achieved promising results.
. Recently, the
models of [31, 32, 76, 61, 20, 60, 57, 53, 58] are designed to
handle images with more than single-motion blurs. Another
line of work focuses on video deblurring. For example,
Zhang et al. [94] propose a method that jointly estimates
the motions between consecutive frames, while Sellent et
al. [71] instead utilize stereo information. Wieschollek et
al. [86] introduce a recurrent ConvNet to deblur an image
by using temporal information. Kim et al. [38] propose a
method to simultaneously conduct deblurring and estimate

43228494

the optical ﬂow between consecutive images. Ren et al. [66]
exploit semantic information to guide the deblurring and
optical ﬂow estimation. Su et al. [75] propose a ConvNet
for deblurring by utilizing inter-frame information. Pan et
al. [63] jointly estimate the scene ﬂow and deblur the image.
There are some approaches for estimating the spatial in-
formation from blurs, but they all focus on image sequences
instead of a single image. For example, Park et al. [64]
develop a method for the joint estimation of camera pose,
depth, deblurring, and super-resolution from a sequence of
blurred images. More recently, Jin et al. [35] propose a
framework to extract a video sequence from a single blurred
image, yet overlook the spatial information that enables 3D
reconstruction.

Depth estimation. Earlier methods for depth esti-
mation rely on geometry-based algorithms from stereo
pairs [70, 14, 13]. Saxena et al. [68] ﬁrst propose to ex-
ploit the monocular cues to estimate the scene depth, based
on which many methods are proposed, yielding encourag-
ing results [69, 29, 42, 45, 6, 39, 3, 73, 65, 16, 24, 91].
The methods of [98, 47, 36, 59, 89, 85], on the other
hand, exploit not only local but also global image cues.
Given the success of ConvNet in image processing, many
deep learning based methods have been proposed [21, 96,
44, 54, 72, 82, 67, 46, 37, 11]. Thanks to multi-level
contextual and structural information derived from deep
networks, such as AlexNet [40], VGG [74], and ResNet
[25], depth estimation has been boosted to a high-accuracy
level [12, 17, 41, 43, 87, 79, 15]. Although these methods
work well on single image depth estimation, they are not
designed for estimating depth from a blurred image, which
is the focus of our approach.

3. Preliminaries

Before introducing our model, we brieﬂy review some
preliminaries including the creation of a blurred image and
the fundamental of 3D geometry, upon which we build our
network and the self-supervised cycle.

Blurring Process. The process of image blurring is con-

tinuous within the exposure time t of the camera:

B =

1

t (cid:2) t

I(t) dt,

(1)

where B is the resulting blurred image, t is the exposure
time, and I(t) is the clean image of the scene at time t. To
model the blurring process in a computationally tractable
way, we approximate this continuous process using the av-
erage of a sequence of 2n + 1 frames in the exposure pro-
cess. We take the very middle frame, the n+1-frame, as the
reference frame, and compute the relative motions at other
frames with respect to this frame, as discussed in Sec. 4.

Vision geometry. Let p denote the 2D homogeneous
coordinate of a pixel in image I, and P denote the cor-
responding 3D homogeneous coordinate in I’s coordinate

system. Also, let D denote the depth map of an image I,
with D(p) being the absolute distance between the camera’s
focal point and the real-world point P , whose projection on
I is p. Finally, let T denote the transformation matrix that
describes the absolute-scale motion of the camera, governed
by six parameters, three for translation and three for rota-
tion. For a pixel p, the corresponding 2D coordinate p′ after
the transformation T is computed as

p′ = KT D(p)K −1p,

(2)

where the intrinsic parameter matrix K of the camera is as-
sumed known, as done in [92, 97]. In this process, the pixel
p of the original image I is ﬁrst inversely projected back to
the 3D space, and then the obtained 3D point is transferred
to a new 3D location according to the transformation matrix
T . Finally, the new 3D point is re-projected to the new 2D
scene by applying K to the coordinates of the 3D points.

4. Method

In this section, we introduce the proposed approach to
recovering 3D scene from a single blurred image. We ﬁrst
give an overview of our approach, then discuss the modules
of our network, and ﬁnally, show the self-supervised strat-
egy to jointly optimize all the modules.

4.1. Overview

three modules

Our model comprises

for motion-
estimation, deblurring, and depth-estimation, as well as an
innovative self-supervised scheme that optimizes all mod-
ules together. The self-supervision is achieved by forming
a cycle of three modules, all of which collaborate with each
other, in aim to together reproduce the input blurred image.
In other words, the input blurred image itself is utilized as
a supervision signal for computing the reconstruction loss,
during which process all the modules interact with and en-
hance each other.

We focus on static scene reconstruction and assume the
relative motion is caused by the camera movement. We thus
aim to estimate a static frame instant or reference frame, as
well as a sequence of relative camera motions with respect
to the reference that gives rise to the blurs. In our imple-
mentation, we take the frame instant in the very middle of
the sequence as the reference frame, as discussed in Sec. 3.
We follow a two-stage training strategy, which we ﬁnd
to be more efﬁcient and effective than the single-stage strat-
egy that trains modules with the cycle all at once. In the
ﬁrst stage, we train the three modules independently, all in
a supervised manner. In the second stage, we stack the three
modules to form a self-supervised cycle, for which the goal
is, again, to allow the predictions to reproduce the original
blurred image so that the different modules can supervise
and beneﬁt one another.
It is noteworthy that in the sec-
ond stage, we provide ground truths for only the motion-
estimation module but not the other two, in order to avoid

43238495

Blurred Image
Blurred Imagge
/
/D bl
d I

Clean/Deblurred Image
Cleaan/Den/Deblurblurredred Imag
Cl
Imagee

1
5
2

R
e
s
N
e
t
 

1
5
2

R
e
s
N
e
t
 

x20
1x2048

Cam Pose t1

Weight Sharing
Cam Pose t2

Linear 2

Linear 2

at
concat

L
i
n
e
a
r
 
1

L
S
T
M

x20
1x2048

L
S
T
M

Linear 3

Cam Pose t1

Cam Pose t2n+1

Linear 2

L
S
T
M

Linear 3

Cam Pose t2n

Figure 3: The architecture of our LSTM module for motion es-
timation.
It takes as input a pair of images. The upper branch
receives a blurred image, while the lower receives a clean or de-
blurred image, taken to be the reference frame. It outputs a se-
quence of 2n motions with respect to the reference. Notably, the
lower branch is fed with clean images in the ﬁrst training stage, and
with deblurred images from the deblurring module in the cyclic
self-supervision stage.

overﬁtting. Our experiments demonstrate that, compared to
the single-stage strategy, the two-stage training converges
much faster.

In what follows, we give more details on the three mod-
ules and the self-supervision strategy. To highlight the fea-
sibility of inferring the static 3D scene from a blurred im-
age, we mainly rely on compact networks to handle depth-,
deblur-, and motion-estimation tasks. More sophisticated
end-to-end networks can be readily applied as well and are
likely to yield even better performances.

4.2. Motion-estimation Module

Our motion-estimation module, as depicted in Fig. 3,
takes as input a blurred image, as well as a clean image
or a deblurred one estimated by the deblurring module de-
scribed in Sec. 4.3. It outputs a sequence of 2n + 1 relative
camera motions with respect to the reference frame. This
network architecture is motivated by the recent success of
image captioning [5], whose goal is to produce a sequence
of words describing an input image. The major difference
is that our network is fed with a pair of images instead of
one.

Speciﬁcally, we employ a ResNet152 [25] to extract the
features from the second last fully-connected layer for both
input images, and then concatenate the obtained features
into one, which is fed as input to a Long-Short Term Mem-
ory (LSTM) network comprising 2n LSTM blocks with
shared parameters. The LSTM network is expected to learn
the temporal coherence of the camera motion and to output
a sequence of 2n camera poses with respect to the reference
frame. To unify the size of the feature vectors fed to the
LSTM blocks, we introduce a linear3 layer, which is im-
plemented for the second to the last frame instants but not
for the ﬁrst one.

Recall that camera motion, described with the trans-
formation matrix T , is characterized by a rotation vector
u ∈ R3 and a translation vector v ∈ R3, where the former

128

320

320
320

320 128 128

512

128

128

128 128 128 128 256
128
256

128

128

64
64

RGB 
Image

Deblurred 

Image

Figure 4: The architecture of our deblurring module.

one depicts the Pitch-, Yaw-, and Roll-rotation and the lat-
ter represents the translation along the X-, Y-, and Z-axis.
Through out experiments, we ﬁnd out that learning u and
v separately leads to more favorable results. The losses of
our LSTM for learning the two three-dimension variables
are taken to be

N

1
N

(cid:2)u

Lu =

i − ˆu

i(cid:2)2, Lv =

(cid:3)i=1
i are the ground truths of the i-th motion, ˆu
where u
are their estimations, and N is the number of samples.

(cid:3)i=1

i − ˆv

i(cid:2)2,

i, v

(cid:2)v

(3)

i, ˆv

i

N

1
N

4.3. Deblurring Module

The deblurring module takes the blurred image as input
and produces a deblurred image, which we take to be the
reference frame. In our implementation, we adopt the CNN-
L15 model [30] that shows state-of-the-art performance yet
comes in a compact size, with some minor modiﬁcations.
The rough network structure shows in Fig. 4. We add batch
normalization [33] on each layer except the last layer and
change the active function of last layer from ReLU to be
Tanh. The loss for deblurring is taken to be pixel-level
square loss between the deblurred image and the ground
truth:

Lb =

1
N

N

(cid:3)i=1

(cid:2)I i − ˆI i(cid:2)2,

(4)

where I i and ˆI i represent the i-th ground truth and the de-
blurred image respectively, and N denotes the number of
samples.

4.4. Depth-estimation Module

The case for the depth-estimation module is slightly
more complicated than the other two, as it has to handle
heterogeneous inputs in the two stages of training. Recall
that in the ﬁrst stage we train the three modules separately
all in the supervised way, yet in the second stage, as to be
discussed in Sec. 4.5, we provide supervision signal only
to the motion estimation and allow the cycle to enhance the
depth and deblurring module. In other words, in the ﬁrst
stage the depth-estimation module is fed with clean images
as input to produce depth, but in the second it is provided
with deblurred images, which may still contain smearings.
The depth-estimation module is, therefore, expected to
produce reasonable results even when the input images still
contain blurs. To this end, we devise a two-branch network

43248496

Blurred Image
Blurred Image

Deblurring 
Deblurring 
Module
Module

Deblurred 

Image
Image

Depth Estimation Module

Depth Map
Depth Map

Self-supervised 
SelSSSSSS f-supe
rvised 
Cycle
e
Cycley le

Average
Average

Projection
Projection

Projected Deblurred Image Sequence
Projected Deblurred Image Sequence
Projjec

q

gg

Motion 
Motion 
Estimation 
Estimation
Module
Module

Camera Poses
a Poses

Figure 5: Illustration of the proposed cyclic self-supervision scheme. The predicted camera motions, deblurred image, and depth map are
utilized to produce a sequence of frame instants, all of which are then averaged to reconstruct the input blurred image for computing the
loss. The different modules, via this cycle, supervise and improve one another.

128

320

320
320

320 128 128

512

128

4.5. Self-supervised Scheme

64

RGB 
Image

128 128

256 256 256

upsampling

384
384

(a)
256
256

concat

(b)

128 128 128 128 256
128
256

128

128

128

64
64

Depth 
Map

Figure 6: The architecture of our depth-estimation module.
Branch (a) is inherited from the deblurring module to jointly han-
dle the blur information and extract local depth clues. Branch (b)
is implemented with the ﬁrst six layers of VGG net to extract
global depth clues.

for depth estimation, as shown in Fig. 6. The branch (a) has
the same structure as the deblurring module, which simulta-
neously handles the remaining blur information in the input
image and extracts local depth clues. The branch (b), on the
other hand, focuses on extracting global depth clues. It is
implemented by taking the ﬁrst six layers from VGG [74],
followed by upsampling the features to the same size as
those in branch (a). The features from both branches are
then concatenated and fed to a network of the same archi-
tecture as the deblurring one, with the only difference being
that the activation function is ReLU in the last layer. The
loss of the depth-estimation module is taken to be

Ld =

1
N

N

(cid:3)i=1

(cid:2)Di − ˆDi(cid:2)2,

(5)

where Di and ˆDi denote respectively the i-th ground truth
and the prediction, and N is the number of samples.

If the predictions of the motion-estimation module, the
deblurring module, and the depth-estimation module are
plausible, then together they should reconstruct the orig-
inal blurred image. With this motivation, we stack the
three modules in a cycle, for which the goal is to ensure
all the predictions, in turn, reproduce the input blurred im-
age. With the cycle, the blurred image itself is treated as the
supervision signal, allowing the different modules to col-
laboratively supervise and beneﬁt one another.

Our design for the cycle is depicted in Fig. 5. Intuitively,
given an input blurred image, the deblurring module pro-
duces a deblurred image as the reference frame, which is
then fed to both the depth module and the motion mod-
ule. The former module outputs a depth map and the lat-
ter generates a motion sequence. Both outputs are, together
with the deblurred reference frame, utilized to produce a
sequence of clean images, which are further averaged to re-
produce the input blurred image and for computing the loss.
Speciﬁcally, let p denote the homogeneous coordinate of
a pixel in the deblurred reference frame. Given a camera
motion ˆT estimated by the motion module and depth map
ˆD estimated by the depth module, the corresponding pixel
coordinate p′ after undergoing the motion is computed, ac-
cording to Eq. 2,

p′ = K ˆT ˆD(p)K −1p,

(6)

where again K is assumed to be given as done in previous
works [92, 97]. We repeat this process for all the pixels by
applying bilinear interpolation and in this way get a com-
plete image, I ′, that undergoes a motion of ˆT with respect
to the reference frame I.

As the motion module estimates 2n relative motions, we

43258497

compute 2n such images using Eq. 6, all of which are then
averaged to approximate the input blurred image for com-
puting the loss. We write the cyclic-reconstruction loss as

Lr =

1
N

N

(cid:3)i=1

(cid:2)Bi − ˆBi

r(cid:2)2,

(7)

Term

Translationx
Translationy
Translationz

Yaw
Pitch
Roll

Pre-NYU C-NYU Pre-ICL C-ICL
2.961
3.142
2.112
0.201
0.147
0.144

3.589
3.735
2.446
0.209
0.184
0.180

2.584
2.746
1.492
0.110
0.084
0.082

3.813
3.796
2.452
0.239
0.185
0.206

1

r =
where B is the i-th input blurred image,
ˆBi,k, with ˆBi,k being the image at k-th frame
2n+1 (cid:2)2n+1
instant within the sequence, and N is the number of sam-
ples.

k=1

ˆBi

As discussed, we only utilize the motion supervision in
the cycle-tuning stage. We thus have the ﬁnal loss functions,
one for rotation and one for translation, as follows,

˜Lu = Lu + αLr,

˜Lv = Lv + αLr,

(8)

where α is taken to be 10−3.

5. Experiments

In this section, we provide our experimental setups and
show the results. Since we are not aware of any existing
work that performs exactly the same task as we do here, we
mainly focus on showing the promise of the proposed net-
work especially the self-supervised cycle design. We also
compare part of our network with other popular models, and
then substitute our module with others to verify the value of
the cycle by comparing the performance of other models
without and with the cycle.

Our goal is, again, to show the possibility of recovering
the 3D scene from a blurred image, rather than trying to
beat the state-of-the-art deblurring, depth- and trajectory-
estimation, and 3D reconstruction models. More compli-
cated networks, as long as they are end-to-end trainable, can
be adopted in our cycle with possibly better performances.

5.1. Datasets and Implementation Details.

NYU Depth v2 [55]. It comprises 464 indoor scenes,
among which we use 364 scenes for training and 100 for
testing. Blurred images are created by averaging 7 consec-
utive frames. In total, we create 57K blurred samples for
training and 13K for testing using about 420K frames. We
adopt this dataset for constructing blurred images, because
it provides a depth map for each video frame and the frame
rate is high with respect to the camera motions. We also
tried KITTI but found spatial gaps between two consecutive
frames are too large, making the synthetic blurs unrealistic.
ICL-NUIM dataset [23]. It is smaller in size as com-
pared to the NYU one. By following the same procedure as
done for NYU, we create 706 blurred samples using 4.9K
frames from two scenes for training and 604 samples using
4.2K frames from another two scenes for testing. Due to the
limited training samples, we adopt the network pre-trained
on NYU and ﬁnetune it on this dataset.

Table 1: Results of the motion-estimation module. Translations
are measured in centimeters and rotations in degrees. Pre-NYU
refers to the network trained using ground-truth clean images on
NYU, and C-NYU is the one with the self-supervised cycle, for
which the input is the output of the deblur module. Pre-ICL
and C-ICL refer to the corresponding networks on the ICL-NUIM
dataset.

Term Pre-NYU C-NYU Pre-ICL C-ICL
27.19
PSNR
SSIM
0.9206

25.94
0.8543

27.22
0.8931

26.43
0.8895

Table 2: Results of deblurring without (Pre-NYU/ICL) and with
(C-NYU/ICL) the self-supervised cycle on the two datasets.

Implementation. Our networks are implemented using
PyTorch and with two Tesla V-100 SXM2 GPUs. The batch
sizes for the motion estimation, deblurring and depth esti-
mation module are 64, 4 and 4, respectively. During the
cycle stage, the batch size is set to 2 for all modules due to
the memory limitation. As our blurred dataset is trained by
averaging 7 images, we train the LSTM model of Sec. 4.2
to predict 6 motions with respective to the reference frame.

5.2. Motion Estimation

Tab. 1 shows the absolute errors of translation (in cen-
timeters), and of rotation angles along three axes (in de-
grees). It can be seen that with the self-supervised cycle,
the errors on translations decrease about 1cm and those on
rotations reduce up to 50%. It is noteworthy that the im-
provements on ICL are smaller than those on NYU, due to
limited training samples.

5.3. Deblurring

We show the deblurring results in Tab. 2, where the self-
supervised cycle again yields signiﬁcant improvements. On
the NYU dataset, the PNSR get increased by more than 1dB
and the SSIM by 0.04. The same trend is observed on ICL,
where PSNR improves more than 0.75dB and the SSIM im-
proves more than 0.03. These results indicate that the self-
supervised cycle enhances not only the pixel-based appear-
ance of the deblurred images, but also the more global struc-
tural patterns, which are crucial for the succeeding depth
estimation, motion estimation, and reconstruction tasks.

5.4. Depth Estimation

As shown in Tab. 3, the self-supervised cycle improves
the performance of depth estimation by a large margin, in

43268498

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 7: Results on the NYU dataset in the top 5 rows and the ICL dataset in the bottom 2 rows. Column (a) depicts the input blurred
images, (c) depicts the deblurred reference frames, (b) and (d) are the ﬁrst and the last projected frames, (e) corresponds to the ground-truth
clean reference frames, (f) displays the predicted depth maps, and (g) demonstrates the 3D reconstruction results.

Term

Abs Rel
SqRel
RMSE

RMSE log
δ < 1.25
δ < 1.252
δ < 1.253

Pre-NYU C-NYU Pre-ICL C-ICL
0.206
0.180
0.661
0.244
0.684
0.918
0.972

0.184
0.156
0.607
0.222
0.733
0.932
0.982

0.217
0.213
0.911
0.289
0.607
0.884
0.969

0.220
0.216
0.918
0.293
0.603
0.879
0.961

Table 3: Results of the depth estimation without (Pre-NYU/ICL)
and with (C-NYU/ICL) the self-supervised cycle on the two
datasets.

terms of both the error metrics including Abs Rel, SqRel,
RMSE and RMSE log, and the accuracy ones δ < 1.25n.
The large improvement on δ < 1.25 shows that our cy-
cle improves depth estimation on a large number of pix-
els across the image, indicating that the cycle beneﬁts the
global depth-estimation performance. Please note that,
as discussed in Sec. 4.4, the pre-trained depth module is
learned on clean images.

5.5. Analysis

Results on real-world blurred images. We show in
Fig. 8 the results of our model on some real-world blurred
images, taken by Asus Xtion Pro as camera parameters are
close to those of Kinect v2. We show the blurred images on
column (a), followed by three recovered clean frames, the
ground-truth clean reference frames, depth maps, and 3D
reconstructions. The results are visually pleasing despite
not perfect.

Comparisons to other models. Here we conduct abla-
tion studies to verify the performance of our depth estima-
tion module and show why it ﬁts our purpose. Speciﬁcally,
we compare our network with a popular one from Eigen et
al. [12]. When training Eigen’s network, we followed the
training strategy provided in [12]. All ablation experiments
are conducted on the NYU dataset.

We compare the performances of Eigen’s network and
ours when trained on the clean image. As shown in Tab. 4,
the results of the two models are very similar. When trained

43278499

(a)

(b)

(c)

(d)

(e)

(f)

Figure 8: Results on real-world blurred images. Column (a) shows the blurred images, (c) corresponds to the deblurred reference frames,
(b) and (d) are the ﬁrst and the last projected frames, (e) shows the predicted depth maps, and (f) shows the 3D reconstruction results.

Term

Pre-Eigen’s [12]

on Clean

Pre-Ours
on Clean

Pre-Eigen’s
on Deblurred

Pre-Ours

C-Eigen’s C-Ours

on Deblurred

Abs Rel
SqRel
RMSE

RMSE log
δ < 1.25
δ < 1.252
δ < 1.253

0.215
0.212
0.907
0.285
0.611
0.887
0.971

0.217
0.213
0.911
0.289
0.607
0.884
0.969

0.231
0.244
0.921
0.291
0.583
0.869
0.964

0.224
0.232
0.917
0.290
0.604
0.880
0.967

0.198
0.177
0.651
0.237
0.696
0.922
0.979

0.184
0.156
0.607
0.222
0.733
0.932
0.982

Table 4: Results of Eigen’s depth network and ours under different setups. We compare the performances of the two networks pre-trained
on clean images (Pre-Eigen’s/Ours on Clean), the performances of the two networks trained using outputs of the deblur network (Pre-
Eigen’s/Ours on Deblurred), and those of the two using the proposed self-supervised cycle (C-Eigen’s/Ours).

Term With Eigen’s depth With our depth
PSNR
SSIM

26.13
0.8697

27.22
0.8931

the proposed model yields superior results thanks to the bet-
ter depth estimation. The results also indicate the important
role that depth plays within the self-supervised cycle.

Table 5: Comparing deblurring network after self-supervised cy-
cle with ours depth estimation module and Eigen’s depth network.

6. Conclusion

Term

With Eigen’s depth With our depth

Translationx
Translationy
Translationz

Yaw
Pitch
Roll

2.762
3.008
1.699
0.135
0.107
0.096

2.584
2.746
1.492
0.110
0.084
0.082

Table 6: Comparing motion estimation module after self-
supervised cycle with ours depth estimation module and Eigen’s
depth network.

on deblurred images and trained using the cycle, however,
our network produces visibly better results, indicating that
the proposed depth module with the two-branch architecture
depicted in Fig. 6 can better handle blur information.

We further show the results of the deblurring and mo-
tion estimation using Eigen’s depth network and ours in
Tabs. 5 and 6 respectively. From both tables, we see that

We show in this paper that given a blurred image, one can
recover the 3D world hidden under the blurs given the cam-
era intrinsic parameters. We accomplish this via training
a deep network of three modules, one for motion estima-
tion, one for deblurring, and one for depth estimation, all of
which form a cycle to in turn reproduce the input blurred
image and supervise one another. We construct datasets
upon several large-scale benchmarks for training our model,
and demonstrate the effectiveness of the proposed model on
these datasets as well as real-world blurred images. In the
future work, we will endeavor to estimate dynamic scenes
from single blurred images, and incorporate more tasks like
scene parsing into the framework.

Acknowledgement This research was supported by Australian
Research Council Projects FL-170100117, DP-180103424, IH-
180100002 and the startup funding of Stevens Institute of Tech-
nology. Xinchao Wang is the corresponding author of this paper.

43288500

References

[1] P. Agrawal, J. Carreira, and J. Malik. Learning to see
by moving. In Proceedings of the IEEE International
Conference on Computer Vision, pages 37–45, 2015.

[2] Y. Bahat, N. Efrat, and M. Irani. Non-uniform blind
deblurring by reblurring. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pages 3286–3294, 2017.

[3] M. H. Baig and L. Torresani. Coupled depth learn-
ing.
In Applications of Computer Vision (WACV),
2016 IEEE Winter Conference on, pages 1–10. IEEE,
2016.

[4] A. Chakrabarti. A neural approach to blind motion
deblurring. In European Conference on Computer Vi-
sion, pages 221–235. Springer, 2016.

[5] X. Chen and C. Lawrence Zitnick. Mind’s eye: A
recurrent visual representation for image caption gen-
eration.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 2422–
2431, 2015.

[6] S. Choi, D. Min, B. Ham, Y. Kim, C. Oh, and
K. Sohn. Depth analogy: Data-driven approach
for single image depth estimation using gradient
samples.
IEEE Transactions on Image Processing,
24(12):5953–5966, 2015.

[7] A. Criminisi and A. Zisserman. Shape from texture:
Homogeneity revisited. In BMVC, volume 1, page 2,
2000.

[8] E. Davis and G. Marcus. Commonsense reasoning
and commonsense knowledge in artiﬁcial intelligence.
Communications of the ACM, 58(9):92–103, 2015.

[9] F. Dellaert, S. M. Seitz, C. E. Thorpe, and S. Thrun.
In
Structure from motion without correspondence.
Computer Vision and Pattern Recognition, 2000. Pro-
ceedings. IEEE Conference on, volume 2, pages 557–
564. IEEE, 2000.

[10] J. Dong, J. Pan, Z. Su, and M.-H. Yang. Blind im-
age deblurring with outlier handling. In IEEE Interna-
tional Conference on Computer Vision (ICCV), pages
2478–2486, 2017.

[11] D. Eigen and R. Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale
convolutional architecture. In Proceedings of the IEEE
International Conference on Computer Vision, pages
2650–2658, 2015.

[13] J. Flynn, I. Neulander, J. Philbin, and N. Snavely.
Deepstereo: Learning to predict new views from the
world’s imagery.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition,
pages 5515–5524, 2016.

[14] D. A. Forsyth and J. Ponce. A modern approach.
Computer vision: a modern approach, pages 88–101,
2003.

[15] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and
D. Tao. Deep ordinal regression network for monocu-
lar depth estimation. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition,
pages 2002–2011, 2018.

[16] R. Furukawa, R. Sagawa, and H. Kawasaki. Depth
estimation using structured light ﬂow–analysis of pro-
jected pattern ﬂow on an object’s surface–.
arXiv
preprint arXiv:1710.00513, 2017.

[17] R. Garg, G. BGV Kumar, Carneiro, and I. Reid. Unsu-
pervised cnn for single view depth estimation: Geom-
etry to the rescue. In European Conference on Com-
puter Vision, pages 740–756. Springer, 2016.

[18] C. Godard, O. Mac Aodha, and G. J. Brostow. Un-
supervised monocular depth estimation with left-right
consistency. In CVPR, volume 2, page 7, 2017.

[19] D. Gong, M. Tan, Y. Zhang, A. Van den Hengel, and
Q. Shi. Blind image deconvolution by automatic gra-
dient activation.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition,
pages 1827–1836, 2016.

[20] D. Gong, J. Yang, L. Liu, Y. Zhang, I. D. Reid,
C. Shen, A. Van Den Hengel, and Q. Shi. From mo-
tion blur to motion ﬂow: A deep learning solution for
removing heterogeneous motion blur. In CVPR, vol-
ume 1, page 5, 2017.

[21] R. A. G¨uler, G. Trigeorgis, E. Antonakos, P. Snape,
S. Zafeiriou, and I. Kokkinos. Densereg: Fully convo-
lutional dense shape regression in-the-wild. In CVPR,
volume 2, page 5, 2017.

[22] A. Gupta, A. A. Efros, and M. Hebert. Blocks world
revisited: Image understanding using qualitative ge-
ometry and mechanics.
In European Conference on
Computer Vision, pages 482–496. Springer, 2010.

[23] A. Handa, T. Whelan, J. McDonald, and A. J. Davi-
son. A benchmark for rgb-d visual odometry, 3d
reconstruction and slam.
In Robotics and automa-
tion (ICRA), 2014 IEEE international conference on,
pages 1524–1531. IEEE, 2014.

[12] D. Eigen, C. Puhrsch, and R. Fergus. Depth map pre-
diction from a single image using a multi-scale deep
network. In Advances in neural information process-
ing systems, pages 2366–2374, 2014.

[24] C. Hane, L. Ladicky, and M. Pollefeys. Direction mat-
ters: Depth estimation with a surface normal classiﬁer.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 381–389, 2015.

43298501

[25] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-
ual learning for image recognition. In Proceedings of
the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[26] G. Heitz, S. Gould, A. Saxena, and D. Koller. Cas-
caded classiﬁcation models: Combining models for
holistic scene understanding.
In Advances in Neu-
ral Information Processing Systems, pages 641–648,
2009.

[27] M. Hirsch, C.

and
B. Scholkopf. Fast removal of non-uniform camera
shake. 2011.

J. Schuler, S. Harmeling,

[28] D. Hoiem, A. A. Efros, and M. Hebert. Geometric
In Computer Vision,
context from a single image.
2005. ICCV 2005. Tenth IEEE International Confer-
ence on, volume 1, pages 654–661. IEEE, 2005.

[29] D. Hoiem, A. A. Efros, and M. Hebert. Recovering
surface layout from an image. International Journal
of Computer Vision, 75(1):151–172, 2007.

[30] M. Hradiˇs, J. Kotera, P. Zemc´ık, and F. ˇSroubek. Con-
volutional neural networks for direct text deblurring.
In Proceedings of BMVC, volume 10, page 2, 2015.

[31] T. Hyun Kim, B. Ahn, and K. Mu Lee. Dynamic scene
deblurring. In Proceedings of the IEEE International
Conference on Computer Vision, pages 3160–3167,
2013.

[32] T. Hyun Kim and K. Mu Lee. Segmentation-free dy-
namic scene deblurring. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pages 2766–2773, 2014.

[33] S. Ioffe and C. Szegedy. Batch normalization: Accel-
erating deep network training by reducing internal co-
variate shift. arXiv preprint arXiv:1502.03167, 2015.

[34] J. Y. Jason, A. W. Harley, and K. G. Derpanis. Back
to basics: Unsupervised learning of optical ﬂow via
brightness constancy and motion smoothness. In Eu-
ropean Conference on Computer Vision, pages 3–10.
Springer, 2016.

[35] M. Jin, G. Meishvili, and P. Favaro. Learning to ex-
tract a video sequence from a single motion-blurred
image. arXiv preprint arXiv:1804.04065, 2018.

[36] K. Karsch, C. Liu, and S. B. Kang. Depth trans-
fer: Depth extraction from video using non-parametric
sampling. IEEE transactions on pattern analysis and
machine intelligence, 36(11):2144–2158, 2014.

[37] A. Kendall and Y. Gal. What uncertainties do we need
in bayesian deep learning for computer vision?
In
Advances in neural information processing systems,
pages 5574–5584, 2017.

[38] T. H. Kim, S. Nah, and K. M. Lee. Dynamic scene
deblurring using a locally adaptive linear blur model.
arXiv preprint arXiv:1603.04265, 2016.

[39] J. Konrad, M. Wang, P.

Ishwar, C. Wu, and
D. Mukherjee. Learning-based, automatic 2d-to-3d
image and video conversion.
IEEE Transactions on
Image Processing, 22(9):3485–3496, 2013.

[40] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Im-
agenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information process-
ing systems, pages 1097–1105, 2012.

[41] Y. Kuznietsov, J. St¨uckler, and B. Leibe.

Semi-
supervised deep learning for monocular depth map
prediction. In Proc. of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 6647–
6655, 2017.

[42] L. Ladicky, J. Shi, and M. Pollefeys. Pulling things out
of perspective. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
89–96, 2014.

[43] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari,
and N. Navab. Deeper depth prediction with fully
convolutional residual networks. In 3D Vision (3DV),
2016 Fourth International Conference on, pages 239–
248. IEEE, 2016.

[44] J. Li, R. Klein, and A. Yao. A two-streamed network
for estimating ﬁne-scaled depth maps from single rgb
images.
In Proceedings of the 2017 IEEE Interna-
tional Conference on Computer Vision, Venice, Italy,
pages 22–29, 2017.

[45] X. Li, H. Qin, Y. Wang, Y. Zhang, and Q. Dai. Dept:
depth estimation by parameter transfer for single still
images.
In Asian Conference on Computer Vision,
pages 45–58. Springer, 2014.

[46] F. Liu, C. Shen, G. Lin, and I. D. Reid. Learning depth
from single monocular images using deep convolu-
tional neural ﬁelds. IEEE Trans. Pattern Anal. Mach.
Intell., 38(10):2024–2039, 2016.

[47] M. Liu, M. Salzmann, and X. He. Discrete-continuous
depth estimation from a single image. In Proceedings
of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 716–723, 2014.

[48] X. Liu, Y. Zhao, and S.-C. Zhu. Single-view 3d scene
parsing by attributed grammar.
In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pages 684–691, 2014.

[49] B. D. Lucas, T. Kanade, et al. An iterative image regis-
tration technique with an application to stereo vision.
1981.

[50] A. Maksai, X. Wang, F. Fleuret, and P. Fua. Non-
markovian globally consistent multi-object tracking.

43308502

In Proceedings of the IEEE International Conference
on Computer Vision, pages 2544–2554, 2017.

[51] T. Michaeli and M. Irani. Blind deblurring using in-
ternal patch recurrence. In European Conference on
Computer Vision, pages 783–798. Springer, 2014.

[52] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos.
Orb-slam: a versatile and accurate monocular slam
system. IEEE Transactions on Robotics, 31(5):1147–
1163, 2015.

[53] S. Nah, T. H. Kim, and K. M. Lee. Deep multi-scale
convolutional neural network for dynamic scene de-
blurring. In CVPR, volume 1, page 3, 2017.

[54] T. Narihira, M. Maire, and S. X. Yu. Learning light-
ness from human judgement on relative reﬂectance.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2965–2973,
2015.

[55] P. K. Nathan Silberman, Derek Hoiem and R. Fergus.
Indoor segmentation and support inference from rgbd
images. In ECCV, 2012.

[56] S. K. Nayar.

Shape from focus.

Technical re-
port, CARNEGIE-MELLON UNIV PITTSBURGH
PA ROBOTICS INST, 1989.

[57] T. M. Nimisha, A. K. Singh, and A. N. Rajagopalan.
Blur-invariant deep learning for blind-deblurring. In
ICCV, pages 4762–4770, 2017.

[58] M. Noroozi, P. Chandramouli, and P. Favaro. Motion
deblurring in the wild. In German Conference on Pat-
tern Recognition, pages 65–77. Springer, 2017.

[59] A. Oliva and A. Torralba. Modeling the shape of
the scene: A holistic representation of the spatial
envelope.
International journal of computer vision,
42(3):145–175, 2001.

[60] J. Pan, J. Dong, Y.-W. Tai, Z. Su, and M.-H. Yang.
Learning discriminative data ﬁtting functions for blind
image deblurring. In ICCV, pages 1077–1085, 2017.

[61] J. Pan, Z. Hu, Z. Su, H.-Y. Lee, and M.-H. Yang. Soft-
segmentation guided object motion deblurring. In Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 459–468, 2016.

[62] J. Pan, D. Sun, H. Pﬁster, and M.-H. Yang. Blind im-
age deblurring using dark channel prior. In Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1628–1636, 2016.

[64] H. Park and K. M. Lee. Joint estimation of camera
pose, depth, deblurring, and super-resolution from a
blurred image sequence. In Proc. of the IEEE Interna-
tional Conference on Computer Vision, 2017.

[65] R. Ranftl, V. Vineet, Q. Chen, and V. Koltun.
Dense monocular depth estimation in complex dy-
namic scenes. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
4058–4066, 2016.

[66] W. Ren, J. Pan, X. Cao, and M.-H. Yang. Video
deblurring via semantic segmentation and pixel-wise
non-linear kernel. arXiv preprint arXiv:1708.03423,
2017.

[67] A. Roy and S. Todorovic. Monocular depth estima-
tion using neural regression forest. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5506–5514, 2016.

[68] A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth
from single monocular images. In Advances in neu-
ral information processing systems, pages 1161–1168,
2006.

[69] A. Saxena, M. Sun, and A. Y. Ng. Make3d: Learn-
ing 3d scene structure from a single still image. IEEE
transactions on pattern analysis and machine intelli-
gence, 31(5):824–840, 2009.

[70] D. Scharstein and R. Szeliski. A taxonomy and eval-
uation of dense two-frame stereo correspondence al-
gorithms.
International journal of computer vision,
47(1-3):7–42, 2002.

[71] A. Sellent, C. Rother, and S. Roth. Stereo video de-
In European Conference on Computer Vi-

blurring.
sion, pages 558–575. Springer, 2016.

[72] E. Shelhamer, J. T. Barron, and T. Darrell. Scene in-
trinsics and depth from a single image. In Proceedings
of the IEEE International Conference on Computer Vi-
sion Workshops, pages 37–44, 2015.

[73] J. Shi, X. Tao, L. Xu, and J. Jia. Break ames room illu-
sion: depth from general single images. ACM Trans-
actions on Graphics (TOG), 34(6):225, 2015.

[74] K. Simonyan and A. Zisserman. Very deep convo-
lutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.

[75] S. Su, M. Delbracio, J. Wang, G. Sapiro, W. Heidrich,
and O. Wang. Deep video deblurring for hand-held
cameras. In CVPR, volume 2, page 6, 2017.

[63] L. Pan, Y. Dai, M. Liu, and F. Porikli. Simultane-
ous stereo video deblurring and scene ﬂow estimation.
In Computer Vision and Pattern Recognition (CVPR),
2017 IEEE Conference on, pages 6987–6996. IEEE,
2017.

[76] J. Sun, W. Cao, Z. Xu, and J. Ponce. Learning a convo-
lutional neural network for non-uniform motion blur
removal. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 769–
777, 2015.

43318503

[89] E. Yang, C. Deng, W. Liu, X. Liu, D. Tao, and X. Gao.
Pairwise relationship guided deep hashing for cross-
modal retrieval. In AAAI, pages 1618–1625, 2017.

[90] X. Yin, X. Wang, J. Yu, M. Zhang, P. Fua, and
D. Tao. Fisheyerecnet: A multi-context collaborative
deep network for ﬁsheye image rectiﬁcation. In Pro-
ceedings of the European Conference on Computer Vi-
sion (ECCV), pages 469–484, 2018.

[91] X. You, Q. Li, D. Tao, W. Ou, and M. Gong. Local
metric learning for exemplar-based object detection.
IEEE Transactions on Circuits and Systems for Video
Technology, 24(8):1265–1276, 2014.

[92] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agar-
wal, and I. Reid. Unsupervised learning of monocular
depth estimation and visual odometry with deep fea-
ture reconstruction. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition,
pages 340–349, 2018.

[93] H. Zhang and D. Wipf. Non-uniform camera shake
removal using a spatially-adaptive sparse penalty. In
Advances in Neural Information Processing Systems,
pages 1556–1564, 2013.

[94] H. Zhang and J. Yang. Intra-frame deblurring by lever-
aging inter-frame camera motion. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4036–4044, 2015.

[95] R. Zhang, P.-S. Tsai, J. E. Cryer, and M. Shah. Shape-
from-shading: a survey.
IEEE transactions on pat-
tern analysis and machine intelligence, 21(8):690–
706, 1999.

[96] Z. Zhang, A. G. Schwing, S. Fidler, and R. Urtasun.
Monocular object instance segmentation and depth or-
dering with cnns. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pages 2614–
2622, 2015.

[97] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe.
Unsupervised learning of depth and ego-motion from
video. In CVPR, volume 2, page 7, 2017.

[98] W. Zhuo, M. Salzmann, X. He, and M. Liu. Indoor
scene structure analysis for single image depth estima-
tion. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 614–622,
2015.

[77] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg,
A. Dosovitskiy, and T. Brox. Demon: Depth and mo-
tion network for learning monocular stereo. In IEEE
Conference on computer vision and pattern recogni-
tion (CVPR), volume 5, page 6, 2017.

[78] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Suk-
thankar, and K. Fragkiadaki. Sfm-net: Learning of
structure and motion from video.
arXiv preprint
arXiv:1704.07804, 2017.

[79] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L.
Yuille. Towards uniﬁed depth and semantic prediction
from a single image. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition,
pages 2800–2809, 2015.

[80] S. Wang, R. Clark, H. Wen, and N. Trigoni. Deepvo:
Towards end-to-end visual odometry with deep recur-
rent convolutional neural networks. In Robotics and
Automation (ICRA), 2017 IEEE International Confer-
ence on, pages 2043–2050. IEEE, 2017.

[81] S. Wang, R. Clark, H. Wen, and N. Trigoni. End-to-
end, sequence-to-sequence probabilistic visual odom-
etry through deep neural networks. The International
Journal of Robotics Research, 37(4-5):513–542, 2018.

[82] X. Wang, D. Fouhey, and A. Gupta. Designing deep
networks for surface normal estimation. In Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 539–547, 2015.

[83] X. Wang, E. T¨uretken, F. Fleuret, and P. Fua. Tracking
interacting objects optimally using integer program-
ming. In European Conference on Computer Vision,
pages 17–32. Springer, 2014.

[84] X. Wang, E. T¨uretken, F. Fleuret, and P. Fua. Track-
ing interacting objects using intertwined ﬂows. IEEE
transactions on pattern analysis and machine intelli-
gence, 38(11):2312–2326, 2016.

[85] Y. Wang, C. Xu, J. Qiu, C. Xu, and D. Tao. Towards
evolutionary compression. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining, KDD 2018, London,
UK, August 19-23, 2018, pages 2476–2485, 2018.

[86] P. Wieschollek, M. Hirsch, B. Sch¨olkopf, and H. P.
Lensch. Learning blind motion deblurring. In ICCV,
pages 231–240, 2017.

[87] J. Xie, R. Girshick, and A. Farhadi. Deep3d: Fully
automatic 2d-to-3d video conversion with deep con-
volutional neural networks. In European Conference
on Computer Vision, pages 842–857. Springer, 2016.

[88] Y. Yan, W. Ren, Y. Guo, R. Wang, and X. Cao. Image
deblurring via extreme channels prior. In IEEE Con-
ference on Computer Vision and Pattern Recognition,
volume 2, page 6, 2017.

43328504

