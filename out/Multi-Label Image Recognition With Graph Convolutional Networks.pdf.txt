Multi-Label Image Recognition with Graph Convolutional Networks∗

Zhao-Min Chen1,2

Xiu-Shen Wei2

Peng Wang3

Yanwen Guo1

1National Key Laboratory for Novel Software Technology, Nanjing University, China

2Megvii Research Nanjing, Megvii Technology, China

3School of Computer Science, The University of Adelaide, Australia

{chenzhaomin123, weixs.gm}@gmail.com, peng.wang@adelaide.edu.au, ywguo@nju.edu.cn

Abstract

The task of multi-label image recognition is to predict
a set of object labels that present in an image. As objects
normally co-occur in an image, it is desirable to model the
label dependencies to improve the recognition performance.
To capture and explore such important dependencies, we
propose a multi-label classiﬁcation model based on Graph
Convolutional Network (GCN). The model builds a directed
graph over the object labels, where each node (label) is
represented by word embeddings of a label, and GCN is
learned to map this label graph into a set of inter-dependent
object classiﬁers. These classiﬁers are applied to the image
descriptors extracted by another sub-net, enabling the whole
network to be end-to-end trainable. Furthermore, we pro-
pose a novel re-weighted scheme to create an effective label
correlation matrix to guide information propagation among
the nodes in GCN. Experiments on two multi-label image
recognition datasets show that our approach obviously out-
performs other existing state-of-the-art methods. In addition,
visualization analyses reveal that the classiﬁers learned by
our model maintain meaningful semantic topology.

1. Introduction

Multi-label image recognition is a fundamental and prac-
tical task in Computer Vision, where the aim is to predict
a set of objects present in an image. It can be applied to
many ﬁelds such as medical diagnosis recognition [7], hu-
man attribute recognition [19] and retail checkout recog-
nition [8, 30]. Comparing to multi-class image classiﬁca-
tion [21], the multi-label task is more challenging due to the

∗Z.-M. Chen’s contribution was made when he was an intern in Megvii
Research Nanjing. X.-S. Wei and Y. Guo are the corresponding authors.
Yanwen Guo is also with the Science and Technology on Information
Systems Engineering Laboratory, China, and the 28th Research Institute of
China Electronics Technology Group Corporation, Nanjing 210007, China.
This research was supported by National Key R&D Program of China (No.
2017YFA0700800), the National Natural Science Foundation of China
under Grants 61772257 and 61672279.

Person, Sports Ball,

Tennis Racket

Person,  Tie

Person,  Ski

Sports Ball

Tennis
Racket

Person

Tie

Ski

Figure 1. We build a directed graph over the object labels to model
label dependencies in multi-label image recognition. In this ﬁgure,
“LabelA → LabelB”, means when LabelA appears, LabelB is
likely to appear, but the reverse may not be true.

combinatorial nature of the output space. As the objects nor-
mally co-occur in the physical world, a key for multi-label
image recognition is to model the label dependencies, as
shown in Fig. 1.

A Na¨ıve way to address the multi-label recognition prob-
lem is to treat the objects in isolation and convert the multi-
label problem into a set of binary classiﬁcation problems
to predict whether each object of interest presents or not.
Beneﬁted from the great success of single-label image clas-
siﬁcation achieved by deep Convolutional Neural Networks
(CNNs) [10, 26, 27, 12], the performance of the binary solu-
tions has been greatly improved. However, these methods
are essentially limited by ignoring the complex topology
structure between objects. This stimulates research for ap-
proaches to capture and explore the label correlations in var-
ious ways. Some approaches, based on probabilistic graph
model [18, 17] or Recurrent Neural Networks (RNNs) [28],
are proposed to explicitly model label dependencies. While
the former formulates the multi-label classiﬁcation problem
as a structural inference problem which may suffer from a
scalability issue due to high computational complexity, the

5177

latter predicts the labels in a sequential fashion, based on
some orders either pre-deﬁned or learned. Another line of
works implicitly model the label correlations via attention
mechanisms [36, 29]. They consider the relations between
attended regions of an image, which can be viewed as local
correlations, but still ignore the global correlations between
labels which require to be inferred from knowledge beyond
a single image.

In this paper, we propose a novel GCN based model (aka
ML-GCN) to capture the label correlations for multi-label
image recognition, which properties with scalability and
ﬂexibility impossible for competing approaches. Instead of
treating object classiﬁers as a set of independent parameter
vectors to be learned, we propose to learn inter-dependent
object classiﬁers from prior label representations, e.g., word
embeddings, via a GCN based mapping function. In the fol-
lowing, the generated classiﬁers are applied to image repre-
sentations generated by another sub-net to enable end-to-end
training. As the embedding-to-classiﬁer mapping parameters
are shared across all classes (i.e., image labels), the gradients
from all classiﬁers impact the GCN based classiﬁer genera-
tion function. This implicitly models the label correlations.
Furthermore, to explicitly model the label dependencies for
classiﬁer learning, we design an effective label correlation
matrix to guide the information propagation among nodes
in GCN. Speciﬁcally, we propose a re-weighted scheme to
balance the weights between a node and its neighborhood
for node feature update, which effectively alleviates overﬁt-
ting and over-smoothing. Experiments on two multi-label
image recognition datasets show that our approach obviously
outperforms existing state-of-the-art methods. In addition,
visualization analyses reveal that the classiﬁers learned by
our model maintain meaningful semantic structures.

The main contributions of this paper are as follows:

• We propose a novel end-to-end trainable multi-label
image recognition framework, which employs GCN to
map label representations, e.g., word embeddings, to
inter-dependent object classiﬁers.

• We conduct in-depth studies on the design of correlation
matrix for GCN and propose an effective re-weighted
scheme to simultaneously alleviate the over-ﬁtting and
over-smoothing problems.

• We evaluate our method on two benchmark multi-label
image recognition datasets, and our proposed method
consistently achieves superior performance over previ-
ous competing approaches.

2. Related Work

The performance of image classiﬁcation has recently
witnessed a rapid progress due to the establishment of
large-scale hand-labeled datasets such as ImageNet [4], MS-
COCO [20] and PASCAL VOC [5], and the fast development

of deep convolutional networks [10, 11, 35, 3, 32]. Many
efforts have been dedicated to extending deep convolutional
networks for multi-label image recognition.

A straightforward way for multi-label recognition is to
train independent binary classiﬁers for each class/label. How-
ever, this method does not consider the relationship among
labels, and the number of predicted labels will grow expo-
nentially as the number of categories increase. For instance,
if a dataset contains 20 labels, then the number of predicted
label combination could be more than 1 million (i.e., 220).
Besides, this baseline method is essentially limited by ignor-
ing the topology structure among objects, which can be an
important regularizer for the co-occurrence patterns of ob-
jects. For example, some combinations of labels are almost
impossible to appear in the physical world.

In order to regularize the prediction space, many re-
searchers attempted to capture label dependencies. Gong et
al. [9] used a ranking-based learning strategy to train deep
convolutional neural networks for multi-label image recogni-
tion and found that the weighted approximated-ranking loss
worked best. Additionally, Wang et al. [28] utilized recurrent
neural networks (RNNs) to transform labels into embedded
label vectors, so that the correlation between labels can be
employed. Furthermore, attention mechanisms were also
widely applied to discover the label correlation in the multi-
label recognition task. In [36], Zhu et al. proposed a spatial
regularization network to capture both semantic and spatial
relations of these multiple labels based on weighted attention
maps. Wang et al. [29] introduced a spatial transformer layer
and long short-term memory (LSTM) units to capture the
label correlation.

Compared with the aforementioned structure learning
methods, the graph was proven to be more effective in mod-
eling label correlation. Li et al. [18] created a tree-structured
graph in the label space by using the maximum spanning tree
algorithm. Li et al. [17] produced image-dependent condi-
tional label structures base on the graphical Lasso framework.
Lee et al. [15] incorporated knowledge graphs for describing
the relationships between multiple labels. In this paper, we
leverage the graph structure to capture and explore the label
correlation dependency. Speciﬁcally, based on the graph,
we utilize GCN to propagate information between multiple
labels and consequently learn inter-dependent classiﬁers for
each of image labels. These classiﬁers absorb information
from the label graph, which are further applied to the global
image representation for the ﬁnal multi-label prediction. It
is a more explicit way for evaluating label co-occurrence.
Experimental results validate our proposed approach is effec-
tive and our model can be trained in an end-to-end manner.

3. Approach

In this part, we elaborate on our ML-GCN model for
multi-label image recognition. Firstly, we introduce the moti-

5178

Representation learning

fcnn

CNN

D<

h<

w<

D<

Dot product

Global Max 

Pooling

C<

.
.
.

M
u
l
t
i
-
L
a
b
e
l
 

L
o
s
s

Graph Convolutional Network (GCN)

Baseball Bat

Baseball Glove

d<

Baseball Bat
d0

Person

GC

Person

Baseball Glove

Baseball Bat

Baseball Glove

D<

GC

Person

D<

Ball

Ball

Ball

Generated 
classifiers

...<

C<

Figure 2. Overall framework of our ML-GCN model for multi-label image recognition. The object labels are represented by word embeddings
Z ∈ RC×d (C is the number of categories and d is the dimensionality of word-embedding vector). A directed graph is built over these label
representations, where each node denotes a label. Stacked GCNs are learned over the label graph to map these label representations into a
set of inter-dependent object classiﬁers, i.e., W ∈ RC×D, which are applied to the image representation extracted from the input image via
a convolutional network for multi-label image recognition.

vation for our method. Then, we introduce some preliminary
knowledge of GCN, which is followed by the detailed illus-
tration of the proposed ML-GCN model and the re-weighted
scheme for correlation matrix construction.

3.1. Motivations

How to effectively capture the correlations between ob-
ject labels and explore these label correlations to improve the
classiﬁcation performance are both important for multi-label
image recognition. In this paper, we use a graph to model
the inter dependencies between labels, which is a ﬂexible
way to capture the topological structure in the label space.
Speciﬁcally, we represent each node (label) of the graph
as word embeddings of the label, and propose to use GCN
to directly map these label embeddings into a set of inter-
dependent classiﬁers, which can be directly applied to an
image feature for classiﬁcation. Two factors motivated the
design of our GCN based model. Firstly, as the embedding-
to-classiﬁer mapping parameters are shared across all classes,
the learned classiﬁers can retain the weak semantic struc-
tures in the word embedding space, where semantic related
concepts are close to each other. Meanwhile, the gradients
of all classiﬁers can impact the classiﬁer generation function,
which implicitly models the label dependencies. Secondly,
we design a novel label correlation matrix based on their
co-occurrence patterns to explicitly model the label depen-

dencies by GCN, with which the update of node features
will absorb information from correlated nodes (labels).

3.2. Graph Convolutional Network Recap

Graph Convolutional Network (GCN) was introduced
in [14] to perform semi-supervised classiﬁcation. The essen-
tial idea is to update the node representations by propagating
information between nodes.

Unlike standard convolutions that operate on local Eu-
clidean structures in an image, the goal of GCN is to learn
a function f (·, ·) on a graph G, which takes feature descrip-
tions H l ∈ Rn×d and the corresponding correlation matrix
A ∈ Rn×n as inputs (where n denotes the number of nodes
and d indicates the dimensionality of node features), and
updates the node features as H l+1 ∈ Rn×d′
. Every GCN
layer can be written as a non-linear function by

H l+1 = f (H l, A).

(1)

After employing the convolutional operation of [14], f (·, ·)
can be represented as

H l+1 = h(bAH lW l),

where W l ∈ Rd×d′

is a transformation matrix to be learned

and bA ∈ Rn×n is the normalized version of correlation

matrix A, and h(·) denotes a non-linear operation, which is

(2)

5179

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
V
s
K
X
1
M
c
n
C
D
E
P
v
6
e
U
+
V
h
O
S
s
v
A
8
m
Y
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
6
I
b
F
y
4
q
2
A
f
U
U
p
L
p
t
A
7
N
i
8
l
E
K
K
E
7
f
8
C
t
f
p
j
4
B
/
o
X
3
h
l
T
U
I
v
o
h
C
R
n
z
j
3
n
z
t
x
7
/
S
Q
Q
q
X
K
c
1
5
K
1
s
L
i
0
v
F
J
e
X
V
v
f
2
N
z
a
r
u
z
s
t
t
I
4
k
4
w
3
W
R
z
E
s
u
N
7
K
Q
9
E
x
J
t
K
q
I
B
3
E
s
m
9
0
A
9
4
2
x
9
f
6
H
j
7
n
s
t
U
x
N
G
N
m
i
S
8
F
3
q
j
S
A
w
F
8
x
R
R
7
W
E
/
Z
1
E
0
7
V
e
q
T
s
0
x
y
5
4
H
b
g
G
q
K
F
Y
j
r
r
z
g
F
g
P
E
Y
M
g
Q
g
i
O
C
I
h
z
A
Q
0
p
P
F
y
4
c
J
M
T
1
k
B
M
n
C
Q
k
T
5
5
h
i
j
b
w
Z
q
T
g
p
P
G
L
H
9
B
3
R
r
l
u
w
E
e
1
1
z
t
S
4
G
Z
0
S
0
C
v
J
a
e
O
Q
P
D
H
p
J
G
F
9
m
m
3
i
m
c
m
s
2
d
9
y
5
y
a
n
v
t
u
E
/
n
6
R
K
y
R
W
4
Y
7
Y
v
3
w
z
5
X
9
9
u
h
a
F
I
c
5
M
D
Y
J
q
S
g
y
j
q
2
N
F
l
s
x
0
R
d
/
c
/
l
K
V
o
g
w
J
c
R
o
P
K
C
4
J
M
+
O
c
9
d
k
2
n
t
T
U
r
n
v
r
m
f
i
b
U
W
p
W
7
1
m
h
z
f
C
u
b
0
k
D
d
n
+
O
c
x
6
0
j
m
u
u
U
3
O
v
T
6
r
1
8
2
L
U
Z
e
z
j
A
E
c
0
z
1
P
U
c
Y
k
G
m
q
b
K
R
z
z
h
2
b
q
y
p
D
W
x
8
k
+
p
V
S
o
8
e
/
i
2
r
I
c
P
U
E
G
S
P
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
V
s
K
X
1
M
c
n
C
D
E
P
v
6
e
U
+
V
h
O
S
s
v
A
8
m
Y
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
6
I
b
F
y
4
q
2
A
f
U
U
p
L
p
t
A
7
N
i
8
l
E
K
K
E
7
f
8
C
t
f
p
j
4
B
/
o
X
3
h
l
T
U
I
v
o
h
C
R
n
z
j
3
n
z
t
x
7
/
S
Q
Q
q
X
K
c
1
5
K
1
s
L
i
0
v
F
J
e
X
V
v
f
2
N
z
a
r
u
z
s
t
t
I
4
k
4
w
3
W
R
z
E
s
u
N
7
K
Q
9
E
x
J
t
K
q
I
B
3
E
s
m
9
0
A
9
4
2
x
9
f
6
H
j
7
n
s
t
U
x
N
G
N
m
i
S
8
F
3
q
j
S
A
w
F
8
x
R
R
7
W
E
/
Z
1
E
0
7
V
e
q
T
s
0
x
y
5
4
H
b
g
G
q
K
F
Y
j
r
r
z
g
F
g
P
E
Y
M
g
Q
g
i
O
C
I
h
z
A
Q
0
p
P
F
y
4
c
J
M
T
1
k
B
M
n
C
Q
k
T
5
5
h
i
j
b
w
Z
q
T
g
p
P
G
L
H
9
B
3
R
r
l
u
w
E
e
1
1
z
t
S
4
G
Z
0
S
0
C
v
J
a
e
O
Q
P
D
H
p
J
G
F
9
m
m
3
i
m
c
m
s
2
d
9
y
5
y
a
n
v
t
u
E
/
n
6
R
K
y
R
W
4
Y
7
Y
v
3
w
z
5
X
9
9
u
h
a
F
I
c
5
M
D
Y
J
q
S
g
y
j
q
2
N
F
l
s
x
0
R
d
/
c
/
l
K
V
o
g
w
J
c
R
o
P
K
C
4
J
M
+
O
c
9
d
k
2
n
t
T
U
r
n
v
r
m
f
i
b
U
W
p
W
7
1
m
h
z
f
C
u
b
0
k
D
d
n
+
O
c
x
6
0
j
m
u
u
U
3
O
v
T
6
r
1
8
2
L
U
Z
e
z
j
A
E
c
0
z
1
P
U
c
Y
k
G
m
q
b
K
R
z
z
h
2
b
q
y
p
D
W
x
8
k
+
p
V
S
o
8
e
/
i
2
r
I
c
P
U
E
G
S
P
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
V
s
K
X
1
M
c
n
C
D
E
P
v
6
e
U
+
V
h
O
S
s
v
A
8
m
Y
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
6
I
b
F
y
4
q
2
A
f
U
U
p
L
p
t
A
7
N
i
8
l
E
K
K
E
7
f
8
C
t
f
p
j
4
B
/
o
X
3
h
l
T
U
I
v
o
h
C
R
n
z
j
3
n
z
t
x
7
/
S
Q
Q
q
X
K
c
1
5
K
1
s
L
i
0
v
F
J
e
X
V
v
f
2
N
z
a
r
u
z
s
t
t
I
4
k
4
w
3
W
R
z
E
s
u
N
7
K
Q
9
E
x
J
t
K
q
I
B
3
E
s
m
9
0
A
9
4
2
x
9
f
6
H
j
7
n
s
t
U
x
N
G
N
m
i
S
8
F
3
q
j
S
A
w
F
8
x
R
R
7
W
E
/
Z
1
E
0
7
V
e
q
T
s
0
x
y
5
4
H
b
g
G
q
K
F
Y
j
r
r
z
g
F
g
P
E
Y
M
g
Q
g
i
O
C
I
h
z
A
Q
0
p
P
F
y
4
c
J
M
T
1
k
B
M
n
C
Q
k
T
5
5
h
i
j
b
w
Z
q
T
g
p
P
G
L
H
9
B
3
R
r
l
u
w
E
e
1
1
z
t
S
4
G
Z
0
S
0
C
v
J
a
e
O
Q
P
D
H
p
J
G
F
9
m
m
3
i
m
c
m
s
2
d
9
y
5
y
a
n
v
t
u
E
/
n
6
R
K
y
R
W
4
Y
7
Y
v
3
w
z
5
X
9
9
u
h
a
F
I
c
5
M
D
Y
J
q
S
g
y
j
q
2
N
F
l
s
x
0
R
d
/
c
/
l
K
V
o
g
w
J
c
R
o
P
K
C
4
J
M
+
O
c
9
d
k
2
n
t
T
U
r
n
v
r
m
f
i
b
U
W
p
W
7
1
m
h
z
f
C
u
b
0
k
D
d
n
+
O
c
x
6
0
j
m
u
u
U
3
O
v
T
6
r
1
8
2
L
U
Z
e
z
j
A
E
c
0
z
1
P
U
c
Y
k
G
m
q
b
K
R
z
z
h
2
b
q
y
p
D
W
x
8
k
+
p
V
S
o
8
e
/
i
2
r
I
c
P
U
E
G
S
P
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
V
s
K
X
1
M
c
n
C
D
E
P
v
6
e
U
+
V
h
O
S
s
v
A
8
m
Y
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
6
I
b
F
y
4
q
2
A
f
U
U
p
L
p
t
A
7
N
i
8
l
E
K
K
E
7
f
8
C
t
f
p
j
4
B
/
o
X
3
h
l
T
U
I
v
o
h
C
R
n
z
j
3
n
z
t
x
7
/
S
Q
Q
q
X
K
c
1
5
K
1
s
L
i
0
v
F
J
e
X
V
v
f
2
N
z
a
r
u
z
s
t
t
I
4
k
4
w
3
W
R
z
E
s
u
N
7
K
Q
9
E
x
J
t
K
q
I
B
3
E
s
m
9
0
A
9
4
2
x
9
f
6
H
j
7
n
s
t
U
x
N
G
N
m
i
S
8
F
3
q
j
S
A
w
F
8
x
R
R
7
W
E
/
Z
1
E
0
7
V
e
q
T
s
0
x
y
5
4
H
b
g
G
q
K
F
Y
j
r
r
z
g
F
g
P
E
Y
M
g
Q
g
i
O
C
I
h
z
A
Q
0
p
P
F
y
4
c
J
M
T
1
k
B
M
n
C
Q
k
T
5
5
h
i
j
b
w
Z
q
T
g
p
P
G
L
H
9
B
3
R
r
l
u
w
E
e
1
1
z
t
S
4
G
Z
0
S
0
C
v
J
a
e
O
Q
P
D
H
p
J
G
F
9
m
m
3
i
m
c
m
s
2
d
9
y
5
y
a
n
v
t
u
E
/
n
6
R
K
y
R
W
4
Y
7
Y
v
3
w
z
5
X
9
9
u
h
a
F
I
c
5
M
D
Y
J
q
S
g
y
j
q
2
N
F
l
s
x
0
R
d
/
c
/
l
K
V
o
g
w
J
c
R
o
P
K
C
4
J
M
+
O
c
9
d
k
2
n
t
T
U
r
n
v
r
m
f
i
b
U
W
p
W
7
1
m
h
z
f
C
u
b
0
k
D
d
n
+
O
c
x
6
0
j
m
u
u
U
3
O
v
T
6
r
1
8
2
L
U
Z
e
z
j
A
E
c
0
z
1
P
U
c
Y
k
G
m
q
b
K
R
z
z
h
2
b
q
y
p
D
W
x
8
k
+
p
V
S
o
8
e
/
i
2
r
I
c
P
U
E
G
S
P
Q
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
u
i
c
e
R
V
J
D
Q
q
k
i
o
7
z
N
O
P
U
v
W
g
z
L
7
S
M
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
k
U
x
G
U
L
t
h
Z
q
k
S
S
d
t
k
O
n
S
Z
h
M
h
F
L
0
B
9
z
q
t
4
l
/
o
H
/
h
n
X
E
K
a
h
G
d
k
O
T
M
u
f
e
c
m
X
t
v
m
A
q
e
K
c
9
7
L
T
g
L
i
0
v
L
K
8
X
V
0
t
r
6
x
u
Z
W
e
X
u
n
l
S
W
5
j
F
g
z
S
k
Q
i
2
2
G
Q
M
c
F
j
1
l
R
c
C
d
Z
O
J
Q
v
G
o
W
D
X
4
e
h
c
x
6
/
v
m
M
x
4
E
l
+
p
S
c
q
6
4
2
A
Q
8
z
6
P
A
k
V
U
Y
3
h
b
r
n
h
V
z
y
x
3
H
v
g
W
V
G
B
X
P
S
m
/
4
A
Y
9
J
I
i
Q
Y
w
y
G
G
I
q
w
Q
I
C
M
n
g
5
8
e
E
i
J
6
2
J
K
n
C
T
E
T
Z
z
h
H
i
X
S
5
p
T
F
K
C
M
g
d
k
T
f
A
e
0
6
l
o
1
p
r
z
0
z
o
4
7
o
F
E
G
v
J
K
W
L
A
9
I
k
l
C
c
J
6
9
N
c
E
8
+
N
s
2
Z
/
8
5
4
a
T
3
2
3
C
f
1
D
6
z
U
m
V
m
F
I
7
F
+
6
W
e
Z
/
d
b
o
W
h
T
5
O
T
Q
2
c
a
k
o
N
o
6
u
L
r
E
t
u
u
q
J
v
7
n
6
p
S
p
F
D
S
p
z
G
P
Y
p
L
w
p
F
R
z
v
r
s
G
k
1
m
a
t
e
9
D
U
z
8
z
W
R
q
V
u
8
j
m
5
v
j
X
d
+
S
B
u
z
/
H
O
c
8
a
B
1
V
f
a
/
q
N
4
4
r
t
T
M
7
6
i
L
2
s
I
9
D
m
u
c
J
a
r
h
E
H
U
3
j
/
Y
g
n
P
D
s
X
j
n
A
y
J
/
9
M
d
Q
p
W
s
4
t
v
y
3
n
4
A
E
U
c
j
2
0
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
u
i
c
e
R
V
J
D
Q
q
k
i
o
7
z
N
O
P
U
v
W
g
z
L
7
S
M
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
k
U
x
G
U
L
t
h
Z
q
k
S
S
d
t
k
O
n
S
Z
h
M
h
F
L
0
B
9
z
q
t
4
l
/
o
H
/
h
n
X
E
K
a
h
G
d
k
O
T
M
u
f
e
c
m
X
t
v
m
A
q
e
K
c
9
7
L
T
g
L
i
0
v
L
K
8
X
V
0
t
r
6
x
u
Z
W
e
X
u
n
l
S
W
5
j
F
g
z
S
k
Q
i
2
2
G
Q
M
c
F
j
1
l
R
c
C
d
Z
O
J
Q
v
G
o
W
D
X
4
e
h
c
x
6
/
v
m
M
x
4
E
l
+
p
S
c
q
6
4
2
A
Q
8
z
6
P
A
k
V
U
Y
3
h
b
r
n
h
V
z
y
x
3
H
v
g
W
V
G
B
X
P
S
m
/
4
A
Y
9
J
I
i
Q
Y
w
y
G
G
I
q
w
Q
I
C
M
n
g
5
8
e
E
i
J
6
2
J
K
n
C
T
E
T
Z
z
h
H
i
X
S
5
p
T
F
K
C
M
g
d
k
T
f
A
e
0
6
l
o
1
p
r
z
0
z
o
4
7
o
F
E
G
v
J
K
W
L
A
9
I
k
l
C
c
J
6
9
N
c
E
8
+
N
s
2
Z
/
8
5
4
a
T
3
2
3
C
f
1
D
6
z
U
m
V
m
F
I
7
F
+
6
W
e
Z
/
d
b
o
W
h
T
5
O
T
Q
2
c
a
k
o
N
o
6
u
L
r
E
t
u
u
q
J
v
7
n
6
p
S
p
F
D
S
p
z
G
P
Y
p
L
w
p
F
R
z
v
r
s
G
k
1
m
a
t
e
9
D
U
z
8
z
W
R
q
V
u
8
j
m
5
v
j
X
d
+
S
B
u
z
/
H
O
c
8
a
B
1
V
f
a
/
q
N
4
4
r
t
T
M
7
6
i
L
2
s
I
9
D
m
u
c
J
a
r
h
E
H
U
3
j
/
Y
g
n
P
D
s
X
j
n
A
y
J
/
9
M
d
Q
p
W
s
4
t
v
y
3
n
4
A
E
U
c
j
2
0
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
u
i
c
e
R
V
J
D
Q
q
k
i
o
7
z
N
O
P
U
v
W
g
z
L
7
S
M
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
k
U
x
G
U
L
t
h
Z
q
k
S
S
d
t
k
O
n
S
Z
h
M
h
F
L
0
B
9
z
q
t
4
l
/
o
H
/
h
n
X
E
K
a
h
G
d
k
O
T
M
u
f
e
c
m
X
t
v
m
A
q
e
K
c
9
7
L
T
g
L
i
0
v
L
K
8
X
V
0
t
r
6
x
u
Z
W
e
X
u
n
l
S
W
5
j
F
g
z
S
k
Q
i
2
2
G
Q
M
c
F
j
1
l
R
c
C
d
Z
O
J
Q
v
G
o
W
D
X
4
e
h
c
x
6
/
v
m
M
x
4
E
l
+
p
S
c
q
6
4
2
A
Q
8
z
6
P
A
k
V
U
Y
3
h
b
r
n
h
V
z
y
x
3
H
v
g
W
V
G
B
X
P
S
m
/
4
A
Y
9
J
I
i
Q
Y
w
y
G
G
I
q
w
Q
I
C
M
n
g
5
8
e
E
i
J
6
2
J
K
n
C
T
E
T
Z
z
h
H
i
X
S
5
p
T
F
K
C
M
g
d
k
T
f
A
e
0
6
l
o
1
p
r
z
0
z
o
4
7
o
F
E
G
v
J
K
W
L
A
9
I
k
l
C
c
J
6
9
N
c
E
8
+
N
s
2
Z
/
8
5
4
a
T
3
2
3
C
f
1
D
6
z
U
m
V
m
F
I
7
F
+
6
W
e
Z
/
d
b
o
W
h
T
5
O
T
Q
2
c
a
k
o
N
o
6
u
L
r
E
t
u
u
q
J
v
7
n
6
p
S
p
F
D
S
p
z
G
P
Y
p
L
w
p
F
R
z
v
r
s
G
k
1
m
a
t
e
9
D
U
z
8
z
W
R
q
V
u
8
j
m
5
v
j
X
d
+
S
B
u
z
/
H
O
c
8
a
B
1
V
f
a
/
q
N
4
4
r
t
T
M
7
6
i
L
2
s
I
9
D
m
u
c
J
a
r
h
E
H
U
3
j
/
Y
g
n
P
D
s
X
j
n
A
y
J
/
9
M
d
Q
p
W
s
4
t
v
y
3
n
4
A
E
U
c
j
2
0
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
u
i
c
e
R
V
J
D
Q
q
k
i
o
7
z
N
O
P
U
v
W
g
z
L
7
S
M
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
k
U
x
G
U
L
t
h
Z
q
k
S
S
d
t
k
O
n
S
Z
h
M
h
F
L
0
B
9
z
q
t
4
l
/
o
H
/
h
n
X
E
K
a
h
G
d
k
O
T
M
u
f
e
c
m
X
t
v
m
A
q
e
K
c
9
7
L
T
g
L
i
0
v
L
K
8
X
V
0
t
r
6
x
u
Z
W
e
X
u
n
l
S
W
5
j
F
g
z
S
k
Q
i
2
2
G
Q
M
c
F
j
1
l
R
c
C
d
Z
O
J
Q
v
G
o
W
D
X
4
e
h
c
x
6
/
v
m
M
x
4
E
l
+
p
S
c
q
6
4
2
A
Q
8
z
6
P
A
k
V
U
Y
3
h
b
r
n
h
V
z
y
x
3
H
v
g
W
V
G
B
X
P
S
m
/
4
A
Y
9
J
I
i
Q
Y
w
y
G
G
I
q
w
Q
I
C
M
n
g
5
8
e
E
i
J
6
2
J
K
n
C
T
E
T
Z
z
h
H
i
X
S
5
p
T
F
K
C
M
g
d
k
T
f
A
e
0
6
l
o
1
p
r
z
0
z
o
4
7
o
F
E
G
v
J
K
W
L
A
9
I
k
l
C
c
J
6
9
N
c
E
8
+
N
s
2
Z
/
8
5
4
a
T
3
2
3
C
f
1
D
6
z
U
m
V
m
F
I
7
F
+
6
W
e
Z
/
d
b
o
W
h
T
5
O
T
Q
2
c
a
k
o
N
o
6
u
L
r
E
t
u
u
q
J
v
7
n
6
p
S
p
F
D
S
p
z
G
P
Y
p
L
w
p
F
R
z
v
r
s
G
k
1
m
a
t
e
9
D
U
z
8
z
W
R
q
V
u
8
j
m
5
v
j
X
d
+
S
B
u
z
/
H
O
c
8
a
B
1
V
f
a
/
q
N
4
4
r
t
T
M
7
6
i
L
2
s
I
9
D
m
u
c
J
a
r
h
E
H
U
3
j
/
Y
g
n
P
D
s
X
j
n
A
y
J
/
9
M
d
Q
p
W
s
4
t
v
y
3
n
4
A
E
U
c
j
2
0
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
1
q
0
O
d
U
V
N
U
B
1
X
P
s
0
o
5
M
Z
V
r
J
H
B
9
s
M
=
"
>
A
A
A
C
x
X
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
0
o
c
s
q
9
g
G
1
S
D
K
d
1
q
F
5
M
Z
l
U
S
h
F
/
w
K
3
+
m
v
g
H
+
h
f
e
G
V
N
Q
i
+
i
E
J
G
f
O
v
e
f
M
3
H
v
9
J
B
C
p
c
p
z
X
g
j
U
3
v
7
C
4
V
F
w
u
r
a
y
u
r
W
+
U
N
7
e
a
a
Z
x
J
x
h
s
s
D
m
L
Z
9
r
2
U
B
y
L
i
D
S
V
U
w
N
u
J
5
F
7
o
B
7
z
l
D
0
9
1
v
D
X
i
M
h
V
x
d
K
X
G
C
e
+
G
3
i
A
S
f
c
E
8
R
d
T
l
X
e
m
m
X
H
G
q
j
l
n
2
L
H
B
z
U
E
G
+
6
n
H
5
B
d
f
o
I
Q
Z
D
h
h
A
c
E
R
T
h
A
B
5
S
e
j
p
w
4
S
A
h
r
o
s
J
c
Z
K
Q
M
H
G
O
e
5
R
I
m
1
E
W
p
w
y
P
2
C
F
9
B
7
T
r
5
G
x
E
e
+
2
Z
G
j
W
j
U
w
J
6
J
S
l
t
7
J
E
m
p
j
x
J
W
J
9
m
m
3
h
m
n
D
X
7
m
/
f
E
e
O
q
7
j
e
n
v
5
1
4
h
s
Q
q
3
x
P
6
l
m
2
b
+
V
6
d
r
U
e
j
j
2
N
Q
g
q
K
b
E
M
L
o
6
l
r
t
k
p
i
v
6
5
v
a
X
q
h
Q
5
J
M
R
p
3
K
O
4
J
M
y
M
c
t
p
n
2
2
h
S
U
7
v
u
r
W
f
i
b
y
Z
T
s
3
r
P
8
t
w
M
7
/
q
W
N
G
D
3
5
z
h
n
Q
f
O
g
6
j
p
V
9
+
K
w
U
j
v
J
R
1
3
E
D
n
a
x
T
/
M
8
Q
g
3
n
q
K
N
B
3
n
0
8
4
g
n
P
1
p
k
V
W
s
o
a
f
a
Z
a
h
V
y
z
j
W
/
L
e
v
g
A
p
r
O
P
k
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
1
q
0
O
d
U
V
N
U
B
1
X
P
s
0
o
5
M
Z
V
r
J
H
B
9
s
M
=
"
>
A
A
A
C
x
X
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
0
o
c
s
q
9
g
G
1
S
D
K
d
1
q
F
5
M
Z
l
U
S
h
F
/
w
K
3
+
m
v
g
H
+
h
f
e
G
V
N
Q
i
+
i
E
J
G
f
O
v
e
f
M
3
H
v
9
J
B
C
p
c
p
z
X
g
j
U
3
v
7
C
4
V
F
w
u
r
a
y
u
r
W
+
U
N
7
e
a
a
Z
x
J
x
h
s
s
D
m
L
Z
9
r
2
U
B
y
L
i
D
S
V
U
w
N
u
J
5
F
7
o
B
7
z
l
D
0
9
1
v
D
X
i
M
h
V
x
d
K
X
G
C
e
+
G
3
i
A
S
f
c
E
8
R
d
T
l
X
e
m
m
X
H
G
q
j
l
n
2
L
H
B
z
U
E
G
+
6
n
H
5
B
d
f
o
I
Q
Z
D
h
h
A
c
E
R
T
h
A
B
5
S
e
j
p
w
4
S
A
h
r
o
s
J
c
Z
K
Q
M
H
G
O
e
5
R
I
m
1
E
W
p
w
y
P
2
C
F
9
B
7
T
r
5
G
x
E
e
+
2
Z
G
j
W
j
U
w
J
6
J
S
l
t
7
J
E
m
p
j
x
J
W
J
9
m
m
3
h
m
n
D
X
7
m
/
f
E
e
O
q
7
j
e
n
v
5
1
4
h
s
Q
q
3
x
P
6
l
m
2
b
+
V
6
d
r
U
e
j
j
2
N
Q
g
q
K
b
E
M
L
o
6
l
r
t
k
p
i
v
6
5
v
a
X
q
h
Q
5
J
M
R
p
3
K
O
4
J
M
y
M
c
t
p
n
2
2
h
S
U
7
v
u
r
W
f
i
b
y
Z
T
s
3
r
P
8
t
w
M
7
/
q
W
N
G
D
3
5
z
h
n
Q
f
O
g
6
j
p
V
9
+
K
w
U
j
v
J
R
1
3
E
D
n
a
x
T
/
M
8
Q
g
3
n
q
K
N
B
3
n
0
8
4
g
n
P
1
p
k
V
W
s
o
a
f
a
Z
a
h
V
y
z
j
W
/
L
e
v
g
A
p
r
O
P
k
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
1
q
0
O
d
U
V
N
U
B
1
X
P
s
0
o
5
M
Z
V
r
J
H
B
9
s
M
=
"
>
A
A
A
C
x
X
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
0
o
c
s
q
9
g
G
1
S
D
K
d
1
q
F
5
M
Z
l
U
S
h
F
/
w
K
3
+
m
v
g
H
+
h
f
e
G
V
N
Q
i
+
i
E
J
G
f
O
v
e
f
M
3
H
v
9
J
B
C
p
c
p
z
X
g
j
U
3
v
7
C
4
V
F
w
u
r
a
y
u
r
W
+
U
N
7
e
a
a
Z
x
J
x
h
s
s
D
m
L
Z
9
r
2
U
B
y
L
i
D
S
V
U
w
N
u
J
5
F
7
o
B
7
z
l
D
0
9
1
v
D
X
i
M
h
V
x
d
K
X
G
C
e
+
G
3
i
A
S
f
c
E
8
R
d
T
l
X
e
m
m
X
H
G
q
j
l
n
2
L
H
B
z
U
E
G
+
6
n
H
5
B
d
f
o
I
Q
Z
D
h
h
A
c
E
R
T
h
A
B
5
S
e
j
p
w
4
S
A
h
r
o
s
J
c
Z
K
Q
M
H
G
O
e
5
R
I
m
1
E
W
p
w
y
P
2
C
F
9
B
7
T
r
5
G
x
E
e
+
2
Z
G
j
W
j
U
w
J
6
J
S
l
t
7
J
E
m
p
j
x
J
W
J
9
m
m
3
h
m
n
D
X
7
m
/
f
E
e
O
q
7
j
e
n
v
5
1
4
h
s
Q
q
3
x
P
6
l
m
2
b
+
V
6
d
r
U
e
j
j
2
N
Q
g
q
K
b
E
M
L
o
6
l
r
t
k
p
i
v
6
5
v
a
X
q
h
Q
5
J
M
R
p
3
K
O
4
J
M
y
M
c
t
p
n
2
2
h
S
U
7
v
u
r
W
f
i
b
y
Z
T
s
3
r
P
8
t
w
M
7
/
q
W
N
G
D
3
5
z
h
n
Q
f
O
g
6
j
p
V
9
+
K
w
U
j
v
J
R
1
3
E
D
n
a
x
T
/
M
8
Q
g
3
n
q
K
N
B
3
n
0
8
4
g
n
P
1
p
k
V
W
s
o
a
f
a
Z
a
h
V
y
z
j
W
/
L
e
v
g
A
p
r
O
P
k
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
1
q
0
O
d
U
V
N
U
B
1
X
P
s
0
o
5
M
Z
V
r
J
H
B
9
s
M
=
"
>
A
A
A
C
x
X
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
0
o
c
s
q
9
g
G
1
S
D
K
d
1
q
F
5
M
Z
l
U
S
h
F
/
w
K
3
+
m
v
g
H
+
h
f
e
G
V
N
Q
i
+
i
E
J
G
f
O
v
e
f
M
3
H
v
9
J
B
C
p
c
p
z
X
g
j
U
3
v
7
C
4
V
F
w
u
r
a
y
u
r
W
+
U
N
7
e
a
a
Z
x
J
x
h
s
s
D
m
L
Z
9
r
2
U
B
y
L
i
D
S
V
U
w
N
u
J
5
F
7
o
B
7
z
l
D
0
9
1
v
D
X
i
M
h
V
x
d
K
X
G
C
e
+
G
3
i
A
S
f
c
E
8
R
d
T
l
X
e
m
m
X
H
G
q
j
l
n
2
L
H
B
z
U
E
G
+
6
n
H
5
B
d
f
o
I
Q
Z
D
h
h
A
c
E
R
T
h
A
B
5
S
e
j
p
w
4
S
A
h
r
o
s
J
c
Z
K
Q
M
H
G
O
e
5
R
I
m
1
E
W
p
w
y
P
2
C
F
9
B
7
T
r
5
G
x
E
e
+
2
Z
G
j
W
j
U
w
J
6
J
S
l
t
7
J
E
m
p
j
x
J
W
J
9
m
m
3
h
m
n
D
X
7
m
/
f
E
e
O
q
7
j
e
n
v
5
1
4
h
s
Q
q
3
x
P
6
l
m
2
b
+
V
6
d
r
U
e
j
j
2
N
Q
g
q
K
b
E
M
L
o
6
l
r
t
k
p
i
v
6
5
v
a
X
q
h
Q
5
J
M
R
p
3
K
O
4
J
M
y
M
c
t
p
n
2
2
h
S
U
7
v
u
r
W
f
i
b
y
Z
T
s
3
r
P
8
t
w
M
7
/
q
W
N
G
D
3
5
z
h
n
Q
f
O
g
6
j
p
V
9
+
K
w
U
j
v
J
R
1
3
E
D
n
a
x
T
/
M
8
Q
g
3
n
q
K
N
B
3
n
0
8
4
g
n
P
1
p
k
V
W
s
o
a
f
a
Z
a
h
V
y
z
j
W
/
L
e
v
g
A
p
r
O
P
k
A
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
r
q
G
F
K
Y
8
X
a
m
w
i
b
o
g
c
7
E
6
T
+
P
M
3
g
+
0
=
"
>
A
A
A
C
x
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
I
i
g
y
6
K
b
L
i
v
a
B
9
Q
i
y
X
R
a
h
+
Z
F
M
l
F
K
E
f
w
B
t
/
p
p
4
h
/
o
X
3
h
n
n
I
J
a
R
C
c
k
O
X
P
u
P
W
f
m
3
h
u
k
o
c
i
l
6
7
6
W
r
L
n
5
h
c
W
l
8
v
L
K
6
t
r
6
x
m
Z
l
a
7
u
V
J
0
X
G
e
J
M
l
Y
Z
J
1
A
j
/
n
o
Y
h
5
U
w
o
Z
8
k
6
a
c
T
8
K
Q
t
4
O
R
m
c
q
3
r
7
l
W
S
6
S
+
F
K
O
U
9
6
L
/
G
E
s
B
o
L
5
k
q
g
L
x
3
G
u
K
1
X
X
c
f
W
y
Z
4
F
n
Q
B
V
m
N
Z
L
K
C
6
7
Q
R
w
K
G
A
h
E
4
Y
k
j
C
I
X
z
k
9
H
T
h
w
U
V
K
X
A
8
T
4
j
J
C
Q
s
c
5
7
r
F
C
2
o
K
y
O
G
X
4
x
I
7
o
O
6
R
d
1
7
A
x
7
Z
V
n
r
t
W
M
T
g
n
p
z
U
h
p
Y
5
8
0
C
e
V
l
h
N
V
p
t
o
4
X
2
l
m
x
v
3
l
P
t
K
e
6
2
5
j
+
g
f
G
K
i
J
W
4
I
f
Y
v
3
T
T
z
v
z
p
V
i
8
Q
A
J
7
o
G
Q
T
W
l
m
l
H
V
M
e
N
S
6
K
6
o
m
9
t
f
q
p
L
k
k
B
K
n
c
J
/
i
G
W
G
m
l
d
M
+
2
1
q
T
6
9
p
V
b
3
0
d
f
9
O
Z
i
l
V
7
Z
n
I
L
v
K
t
b
0
o
C
9
n
+
O
c
B
a
1
D
x
3
M
d
7
/
y
o
W
j
s
1
o
y
5
j
F
3
s
4
o
H
k
e
o
4
Y
6
G
m
i
S
9
x
C
P
e
M
K
z
V
b
d
i
q
7
D
u
P
l
O
t
k
t
H
s
4
N
u
y
H
j
4
A
4
e
i
P
o
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
r
q
G
F
K
Y
8
X
a
m
w
i
b
o
g
c
7
E
6
T
+
P
M
3
g
+
0
=
"
>
A
A
A
C
x
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
I
i
g
y
6
K
b
L
i
v
a
B
9
Q
i
y
X
R
a
h
+
Z
F
M
l
F
K
E
f
w
B
t
/
p
p
4
h
/
o
X
3
h
n
n
I
J
a
R
C
c
k
O
X
P
u
P
W
f
m
3
h
u
k
o
c
i
l
6
7
6
W
r
L
n
5
h
c
W
l
8
v
L
K
6
t
r
6
x
m
Z
l
a
7
u
V
J
0
X
G
e
J
M
l
Y
Z
J
1
A
j
/
n
o
Y
h
5
U
w
o
Z
8
k
6
a
c
T
8
K
Q
t
4
O
R
m
c
q
3
r
7
l
W
S
6
S
+
F
K
O
U
9
6
L
/
G
E
s
B
o
L
5
k
q
g
L
x
3
G
u
K
1
X
X
c
f
W
y
Z
4
F
n
Q
B
V
m
N
Z
L
K
C
6
7
Q
R
w
K
G
A
h
E
4
Y
k
j
C
I
X
z
k
9
H
T
h
w
U
V
K
X
A
8
T
4
j
J
C
Q
s
c
5
7
r
F
C
2
o
K
y
O
G
X
4
x
I
7
o
O
6
R
d
1
7
A
x
7
Z
V
n
r
t
W
M
T
g
n
p
z
U
h
p
Y
5
8
0
C
e
V
l
h
N
V
p
t
o
4
X
2
l
m
x
v
3
l
P
t
K
e
6
2
5
j
+
g
f
G
K
i
J
W
4
I
f
Y
v
3
T
T
z
v
z
p
V
i
8
Q
A
J
7
o
G
Q
T
W
l
m
l
H
V
M
e
N
S
6
K
6
o
m
9
t
f
q
p
L
k
k
B
K
n
c
J
/
i
G
W
G
m
l
d
M
+
2
1
q
T
6
9
p
V
b
3
0
d
f
9
O
Z
i
l
V
7
Z
n
I
L
v
K
t
b
0
o
C
9
n
+
O
c
B
a
1
D
x
3
M
d
7
/
y
o
W
j
s
1
o
y
5
j
F
3
s
4
o
H
k
e
o
4
Y
6
G
m
i
S
9
x
C
P
e
M
K
z
V
b
d
i
q
7
D
u
P
l
O
t
k
t
H
s
4
N
u
y
H
j
4
A
4
e
i
P
o
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
r
q
G
F
K
Y
8
X
a
m
w
i
b
o
g
c
7
E
6
T
+
P
M
3
g
+
0
=
"
>
A
A
A
C
x
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
I
i
g
y
6
K
b
L
i
v
a
B
9
Q
i
y
X
R
a
h
+
Z
F
M
l
F
K
E
f
w
B
t
/
p
p
4
h
/
o
X
3
h
n
n
I
J
a
R
C
c
k
O
X
P
u
P
W
f
m
3
h
u
k
o
c
i
l
6
7
6
W
r
L
n
5
h
c
W
l
8
v
L
K
6
t
r
6
x
m
Z
l
a
7
u
V
J
0
X
G
e
J
M
l
Y
Z
J
1
A
j
/
n
o
Y
h
5
U
w
o
Z
8
k
6
a
c
T
8
K
Q
t
4
O
R
m
c
q
3
r
7
l
W
S
6
S
+
F
K
O
U
9
6
L
/
G
E
s
B
o
L
5
k
q
g
L
x
3
G
u
K
1
X
X
c
f
W
y
Z
4
F
n
Q
B
V
m
N
Z
L
K
C
6
7
Q
R
w
K
G
A
h
E
4
Y
k
j
C
I
X
z
k
9
H
T
h
w
U
V
K
X
A
8
T
4
j
J
C
Q
s
c
5
7
r
F
C
2
o
K
y
O
G
X
4
x
I
7
o
O
6
R
d
1
7
A
x
7
Z
V
n
r
t
W
M
T
g
n
p
z
U
h
p
Y
5
8
0
C
e
V
l
h
N
V
p
t
o
4
X
2
l
m
x
v
3
l
P
t
K
e
6
2
5
j
+
g
f
G
K
i
J
W
4
I
f
Y
v
3
T
T
z
v
z
p
V
i
8
Q
A
J
7
o
G
Q
T
W
l
m
l
H
V
M
e
N
S
6
K
6
o
m
9
t
f
q
p
L
k
k
B
K
n
c
J
/
i
G
W
G
m
l
d
M
+
2
1
q
T
6
9
p
V
b
3
0
d
f
9
O
Z
i
l
V
7
Z
n
I
L
v
K
t
b
0
o
C
9
n
+
O
c
B
a
1
D
x
3
M
d
7
/
y
o
W
j
s
1
o
y
5
j
F
3
s
4
o
H
k
e
o
4
Y
6
G
m
i
S
9
x
C
P
e
M
K
z
V
b
d
i
q
7
D
u
P
l
O
t
k
t
H
s
4
N
u
y
H
j
4
A
4
e
i
P
o
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
r
q
G
F
K
Y
8
X
a
m
w
i
b
o
g
c
7
E
6
T
+
P
M
3
g
+
0
=
"
>
A
A
A
C
x
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
I
i
g
y
6
K
b
L
i
v
a
B
9
Q
i
y
X
R
a
h
+
Z
F
M
l
F
K
E
f
w
B
t
/
p
p
4
h
/
o
X
3
h
n
n
I
J
a
R
C
c
k
O
X
P
u
P
W
f
m
3
h
u
k
o
c
i
l
6
7
6
W
r
L
n
5
h
c
W
l
8
v
L
K
6
t
r
6
x
m
Z
l
a
7
u
V
J
0
X
G
e
J
M
l
Y
Z
J
1
A
j
/
n
o
Y
h
5
U
w
o
Z
8
k
6
a
c
T
8
K
Q
t
4
O
R
m
c
q
3
r
7
l
W
S
6
S
+
F
K
O
U
9
6
L
/
G
E
s
B
o
L
5
k
q
g
L
x
3
G
u
K
1
X
X
c
f
W
y
Z
4
F
n
Q
B
V
m
N
Z
L
K
C
6
7
Q
R
w
K
G
A
h
E
4
Y
k
j
C
I
X
z
k
9
H
T
h
w
U
V
K
X
A
8
T
4
j
J
C
Q
s
c
5
7
r
F
C
2
o
K
y
O
G
X
4
x
I
7
o
O
6
R
d
1
7
A
x
7
Z
V
n
r
t
W
M
T
g
n
p
z
U
h
p
Y
5
8
0
C
e
V
l
h
N
V
p
t
o
4
X
2
l
m
x
v
3
l
P
t
K
e
6
2
5
j
+
g
f
G
K
i
J
W
4
I
f
Y
v
3
T
T
z
v
z
p
V
i
8
Q
A
J
7
o
G
Q
T
W
l
m
l
H
V
M
e
N
S
6
K
6
o
m
9
t
f
q
p
L
k
k
B
K
n
c
J
/
i
G
W
G
m
l
d
M
+
2
1
q
T
6
9
p
V
b
3
0
d
f
9
O
Z
i
l
V
7
Z
n
I
L
v
K
t
b
0
o
C
9
n
+
O
c
B
a
1
D
x
3
M
d
7
/
y
o
W
j
s
1
o
y
5
j
F
3
s
4
o
H
k
e
o
4
Y
6
G
m
i
S
9
x
C
P
e
M
K
z
V
b
d
i
q
7
D
u
P
l
O
t
k
t
H
s
4
N
u
y
H
j
4
A
4
e
i
P
o
w
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
J
n
B
j
P
+
a
j
5
V
V
L
W
D
v
C
K
M
v
w
N
Y
+
8
Z
k
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
2
J
B
X
L
Z
g
H
1
C
L
J
O
m
0
D
p
0
m
Y
T
I
R
S
t
E
f
c
K
v
f
J
v
6
B
/
o
V
3
x
i
m
o
R
X
R
C
k
j
P
n
3
n
N
m
7
r
1
h
K
n
i
m
P
O
+
1
4
C
w
s
L
i
2
v
F
F
f
X
1
j
c
2
t
7
Z
L
O
7
u
t
L
M
l
l
x
J
p
R
I
h
L
Z
C
Y
O
M
C
R
6
z
p
u
J
K
s
E
4
q
W
T
A
O
B
W
u
H
o
5
q
O
t
+
+
Y
z
H
g
S
X
6
l
J
y
n
r
j
Y
B
j
z
A
Y
8
C
R
V
S
j
d
l
M
q
e
x
X
P
L
H
c
e
+
B
a
U
Y
V
c
9
K
b
3
g
G
n
0
k
i
J
B
j
D
I
Y
Y
i
r
B
A
g
I
y
e
L
n
x
4
S
I
n
r
Y
U
q
c
J
M
R
N
n
O
E
e
a
6
T
N
K
Y
t
R
R
k
D
s
i
L
5
D
2
n
U
t
G
9
N
e
e
2
Z
G
H
d
E
p
g
l
5
J
S
h
e
H
p
E
k
o
T
x
L
W
p
7
k
m
n
h
t
n
z
f
7
m
P
T
W
e
+
m
4
T
+
o
f
W
a
0
y
s
w
i
2
x
f
+
l
m
m
f
/
V
6
V
o
U
B
j
g
z
N
X
C
q
K
T
W
M
r
i
6
y
L
r
n
p
i
r
6
5
+
6
U
q
R
Q
4
p
c
R
r
3
K
S
4
J
R
0
Y
5
6
7
N
r
N
J
m
p
X
f
c
2
M
P
E
3
k
6
l
Z
v
Y
9
s
b
o
5
3
f
U
s
a
s
P
9
z
n
P
O
g
d
V
z
x
v
Y
r
f
O
C
l
X
z
+
2
o
i
9
j
H
A
Y
5
o
n
q
e
o
4
h
J
1
N
I
3
3
I
5
7
w
7
F
w
4
w
s
m
c
/
D
P
V
K
V
j
N
H
r
4
t
5
+
E
D
7
S
2
P
S
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
J
n
B
j
P
+
a
j
5
V
V
L
W
D
v
C
K
M
v
w
N
Y
+
8
Z
k
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
2
J
B
X
L
Z
g
H
1
C
L
J
O
m
0
D
p
0
m
Y
T
I
R
S
t
E
f
c
K
v
f
J
v
6
B
/
o
V
3
x
i
m
o
R
X
R
C
k
j
P
n
3
n
N
m
7
r
1
h
K
n
i
m
P
O
+
1
4
C
w
s
L
i
2
v
F
F
f
X
1
j
c
2
t
7
Z
L
O
7
u
t
L
M
l
l
x
J
p
R
I
h
L
Z
C
Y
O
M
C
R
6
z
p
u
J
K
s
E
4
q
W
T
A
O
B
W
u
H
o
5
q
O
t
+
+
Y
z
H
g
S
X
6
l
J
y
n
r
j
Y
B
j
z
A
Y
8
C
R
V
S
j
d
l
M
q
e
x
X
P
L
H
c
e
+
B
a
U
Y
V
c
9
K
b
3
g
G
n
0
k
i
J
B
j
D
I
Y
Y
i
r
B
A
g
I
y
e
L
n
x
4
S
I
n
r
Y
U
q
c
J
M
R
N
n
O
E
e
a
6
T
N
K
Y
t
R
R
k
D
s
i
L
5
D
2
n
U
t
G
9
N
e
e
2
Z
G
H
d
E
p
g
l
5
J
S
h
e
H
p
E
k
o
T
x
L
W
p
7
k
m
n
h
t
n
z
f
7
m
P
T
W
e
+
m
4
T
+
o
f
W
a
0
y
s
w
i
2
x
f
+
l
m
m
f
/
V
6
V
o
U
B
j
g
z
N
X
C
q
K
T
W
M
r
i
6
y
L
r
n
p
i
r
6
5
+
6
U
q
R
Q
4
p
c
R
r
3
K
S
4
J
R
0
Y
5
6
7
N
r
N
J
m
p
X
f
c
2
M
P
E
3
k
6
l
Z
v
Y
9
s
b
o
5
3
f
U
s
a
s
P
9
z
n
P
O
g
d
V
z
x
v
Y
r
f
O
C
l
X
z
+
2
o
i
9
j
H
A
Y
5
o
n
q
e
o
4
h
J
1
N
I
3
3
I
5
7
w
7
F
w
4
w
s
m
c
/
D
P
V
K
V
j
N
H
r
4
t
5
+
E
D
7
S
2
P
S
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
J
n
B
j
P
+
a
j
5
V
V
L
W
D
v
C
K
M
v
w
N
Y
+
8
Z
k
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
2
J
B
X
L
Z
g
H
1
C
L
J
O
m
0
D
p
0
m
Y
T
I
R
S
t
E
f
c
K
v
f
J
v
6
B
/
o
V
3
x
i
m
o
R
X
R
C
k
j
P
n
3
n
N
m
7
r
1
h
K
n
i
m
P
O
+
1
4
C
w
s
L
i
2
v
F
F
f
X
1
j
c
2
t
7
Z
L
O
7
u
t
L
M
l
l
x
J
p
R
I
h
L
Z
C
Y
O
M
C
R
6
z
p
u
J
K
s
E
4
q
W
T
A
O
B
W
u
H
o
5
q
O
t
+
+
Y
z
H
g
S
X
6
l
J
y
n
r
j
Y
B
j
z
A
Y
8
C
R
V
S
j
d
l
M
q
e
x
X
P
L
H
c
e
+
B
a
U
Y
V
c
9
K
b
3
g
G
n
0
k
i
J
B
j
D
I
Y
Y
i
r
B
A
g
I
y
e
L
n
x
4
S
I
n
r
Y
U
q
c
J
M
R
N
n
O
E
e
a
6
T
N
K
Y
t
R
R
k
D
s
i
L
5
D
2
n
U
t
G
9
N
e
e
2
Z
G
H
d
E
p
g
l
5
J
S
h
e
H
p
E
k
o
T
x
L
W
p
7
k
m
n
h
t
n
z
f
7
m
P
T
W
e
+
m
4
T
+
o
f
W
a
0
y
s
w
i
2
x
f
+
l
m
m
f
/
V
6
V
o
U
B
j
g
z
N
X
C
q
K
T
W
M
r
i
6
y
L
r
n
p
i
r
6
5
+
6
U
q
R
Q
4
p
c
R
r
3
K
S
4
J
R
0
Y
5
6
7
N
r
N
J
m
p
X
f
c
2
M
P
E
3
k
6
l
Z
v
Y
9
s
b
o
5
3
f
U
s
a
s
P
9
z
n
P
O
g
d
V
z
x
v
Y
r
f
O
C
l
X
z
+
2
o
i
9
j
H
A
Y
5
o
n
q
e
o
4
h
J
1
N
I
3
3
I
5
7
w
7
F
w
4
w
s
m
c
/
D
P
V
K
V
j
N
H
r
4
t
5
+
E
D
7
S
2
P
S
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
J
n
B
j
P
+
a
j
5
V
V
L
W
D
v
C
K
M
v
w
N
Y
+
8
Z
k
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
2
J
B
X
L
Z
g
H
1
C
L
J
O
m
0
D
p
0
m
Y
T
I
R
S
t
E
f
c
K
v
f
J
v
6
B
/
o
V
3
x
i
m
o
R
X
R
C
k
j
P
n
3
n
N
m
7
r
1
h
K
n
i
m
P
O
+
1
4
C
w
s
L
i
2
v
F
F
f
X
1
j
c
2
t
7
Z
L
O
7
u
t
L
M
l
l
x
J
p
R
I
h
L
Z
C
Y
O
M
C
R
6
z
p
u
J
K
s
E
4
q
W
T
A
O
B
W
u
H
o
5
q
O
t
+
+
Y
z
H
g
S
X
6
l
J
y
n
r
j
Y
B
j
z
A
Y
8
C
R
V
S
j
d
l
M
q
e
x
X
P
L
H
c
e
+
B
a
U
Y
V
c
9
K
b
3
g
G
n
0
k
i
J
B
j
D
I
Y
Y
i
r
B
A
g
I
y
e
L
n
x
4
S
I
n
r
Y
U
q
c
J
M
R
N
n
O
E
e
a
6
T
N
K
Y
t
R
R
k
D
s
i
L
5
D
2
n
U
t
G
9
N
e
e
2
Z
G
H
d
E
p
g
l
5
J
S
h
e
H
p
E
k
o
T
x
L
W
p
7
k
m
n
h
t
n
z
f
7
m
P
T
W
e
+
m
4
T
+
o
f
W
a
0
y
s
w
i
2
x
f
+
l
m
m
f
/
V
6
V
o
U
B
j
g
z
N
X
C
q
K
T
W
M
r
i
6
y
L
r
n
p
i
r
6
5
+
6
U
q
R
Q
4
p
c
R
r
3
K
S
4
J
R
0
Y
5
6
7
N
r
N
J
m
p
X
f
c
2
M
P
E
3
k
6
l
Z
v
Y
9
s
b
o
5
3
f
U
s
a
s
P
9
z
n
P
O
g
d
V
z
x
v
Y
r
f
O
C
l
X
z
+
2
o
i
9
j
H
A
Y
5
o
n
q
e
o
4
h
J
1
N
I
3
3
I
5
7
w
7
F
w
4
w
s
m
c
/
D
P
V
K
V
j
N
H
r
4
t
5
+
E
D
7
S
2
P
S
A
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
J
n
B
j
P
+
a
j
5
V
V
L
W
D
v
C
K
M
v
w
N
Y
+
8
Z
k
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
2
J
B
X
L
Z
g
H
1
C
L
J
O
m
0
D
p
0
m
Y
T
I
R
S
t
E
f
c
K
v
f
J
v
6
B
/
o
V
3
x
i
m
o
R
X
R
C
k
j
P
n
3
n
N
m
7
r
1
h
K
n
i
m
P
O
+
1
4
C
w
s
L
i
2
v
F
F
f
X
1
j
c
2
t
7
Z
L
O
7
u
t
L
M
l
l
x
J
p
R
I
h
L
Z
C
Y
O
M
C
R
6
z
p
u
J
K
s
E
4
q
W
T
A
O
B
W
u
H
o
5
q
O
t
+
+
Y
z
H
g
S
X
6
l
J
y
n
r
j
Y
B
j
z
A
Y
8
C
R
V
S
j
d
l
M
q
e
x
X
P
L
H
c
e
+
B
a
U
Y
V
c
9
K
b
3
g
G
n
0
k
i
J
B
j
D
I
Y
Y
i
r
B
A
g
I
y
e
L
n
x
4
S
I
n
r
Y
U
q
c
J
M
R
N
n
O
E
e
a
6
T
N
K
Y
t
R
R
k
D
s
i
L
5
D
2
n
U
t
G
9
N
e
e
2
Z
G
H
d
E
p
g
l
5
J
S
h
e
H
p
E
k
o
T
x
L
W
p
7
k
m
n
h
t
n
z
f
7
m
P
T
W
e
+
m
4
T
+
o
f
W
a
0
y
s
w
i
2
x
f
+
l
m
m
f
/
V
6
V
o
U
B
j
g
z
N
X
C
q
K
T
W
M
r
i
6
y
L
r
n
p
i
r
6
5
+
6
U
q
R
Q
4
p
c
R
r
3
K
S
4
J
R
0
Y
5
6
7
N
r
N
J
m
p
X
f
c
2
M
P
E
3
k
6
l
Z
v
Y
9
s
b
o
5
3
f
U
s
a
s
P
9
z
n
P
O
g
d
V
z
x
v
Y
r
f
O
C
l
X
z
+
2
o
i
9
j
H
A
Y
5
o
n
q
e
o
4
h
J
1
N
I
3
3
I
5
7
w
7
F
w
4
w
s
m
c
/
D
P
V
K
V
j
N
H
r
4
t
5
+
E
D
7
S
2
P
S
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
J
n
B
j
P
+
a
j
5
V
V
L
W
D
v
C
K
M
v
w
N
Y
+
8
Z
k
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
2
J
B
X
L
Z
g
H
1
C
L
J
O
m
0
D
p
0
m
Y
T
I
R
S
t
E
f
c
K
v
f
J
v
6
B
/
o
V
3
x
i
m
o
R
X
R
C
k
j
P
n
3
n
N
m
7
r
1
h
K
n
i
m
P
O
+
1
4
C
w
s
L
i
2
v
F
F
f
X
1
j
c
2
t
7
Z
L
O
7
u
t
L
M
l
l
x
J
p
R
I
h
L
Z
C
Y
O
M
C
R
6
z
p
u
J
K
s
E
4
q
W
T
A
O
B
W
u
H
o
5
q
O
t
+
+
Y
z
H
g
S
X
6
l
J
y
n
r
j
Y
B
j
z
A
Y
8
C
R
V
S
j
d
l
M
q
e
x
X
P
L
H
c
e
+
B
a
U
Y
V
c
9
K
b
3
g
G
n
0
k
i
J
B
j
D
I
Y
Y
i
r
B
A
g
I
y
e
L
n
x
4
S
I
n
r
Y
U
q
c
J
M
R
N
n
O
E
e
a
6
T
N
K
Y
t
R
R
k
D
s
i
L
5
D
2
n
U
t
G
9
N
e
e
2
Z
G
H
d
E
p
g
l
5
J
S
h
e
H
p
E
k
o
T
x
L
W
p
7
k
m
n
h
t
n
z
f
7
m
P
T
W
e
+
m
4
T
+
o
f
W
a
0
y
s
w
i
2
x
f
+
l
m
m
f
/
V
6
V
o
U
B
j
g
z
N
X
C
q
K
T
W
M
r
i
6
y
L
r
n
p
i
r
6
5
+
6
U
q
R
Q
4
p
c
R
r
3
K
S
4
J
R
0
Y
5
6
7
N
r
N
J
m
p
X
f
c
2
M
P
E
3
k
6
l
Z
v
Y
9
s
b
o
5
3
f
U
s
a
s
P
9
z
n
P
O
g
d
V
z
x
v
Y
r
f
O
C
l
X
z
+
2
o
i
9
j
H
A
Y
5
o
n
q
e
o
4
h
J
1
N
I
3
3
I
5
7
w
7
F
w
4
w
s
m
c
/
D
P
V
K
V
j
N
H
r
4
t
5
+
E
D
7
S
2
P
S
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
J
n
B
j
P
+
a
j
5
V
V
L
W
D
v
C
K
M
v
w
N
Y
+
8
Z
k
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
2
J
B
X
L
Z
g
H
1
C
L
J
O
m
0
D
p
0
m
Y
T
I
R
S
t
E
f
c
K
v
f
J
v
6
B
/
o
V
3
x
i
m
o
R
X
R
C
k
j
P
n
3
n
N
m
7
r
1
h
K
n
i
m
P
O
+
1
4
C
w
s
L
i
2
v
F
F
f
X
1
j
c
2
t
7
Z
L
O
7
u
t
L
M
l
l
x
J
p
R
I
h
L
Z
C
Y
O
M
C
R
6
z
p
u
J
K
s
E
4
q
W
T
A
O
B
W
u
H
o
5
q
O
t
+
+
Y
z
H
g
S
X
6
l
J
y
n
r
j
Y
B
j
z
A
Y
8
C
R
V
S
j
d
l
M
q
e
x
X
P
L
H
c
e
+
B
a
U
Y
V
c
9
K
b
3
g
G
n
0
k
i
J
B
j
D
I
Y
Y
i
r
B
A
g
I
y
e
L
n
x
4
S
I
n
r
Y
U
q
c
J
M
R
N
n
O
E
e
a
6
T
N
K
Y
t
R
R
k
D
s
i
L
5
D
2
n
U
t
G
9
N
e
e
2
Z
G
H
d
E
p
g
l
5
J
S
h
e
H
p
E
k
o
T
x
L
W
p
7
k
m
n
h
t
n
z
f
7
m
P
T
W
e
+
m
4
T
+
o
f
W
a
0
y
s
w
i
2
x
f
+
l
m
m
f
/
V
6
V
o
U
B
j
g
z
N
X
C
q
K
T
W
M
r
i
6
y
L
r
n
p
i
r
6
5
+
6
U
q
R
Q
4
p
c
R
r
3
K
S
4
J
R
0
Y
5
6
7
N
r
N
J
m
p
X
f
c
2
M
P
E
3
k
6
l
Z
v
Y
9
s
b
o
5
3
f
U
s
a
s
P
9
z
n
P
O
g
d
V
z
x
v
Y
r
f
O
C
l
X
z
+
2
o
i
9
j
H
A
Y
5
o
n
q
e
o
4
h
J
1
N
I
3
3
I
5
7
w
7
F
w
4
w
s
m
c
/
D
P
V
K
V
j
N
H
r
4
t
5
+
E
D
7
S
2
P
S
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
9
J
n
B
j
P
+
a
j
5
V
V
L
W
D
v
C
K
M
v
w
N
Y
+
8
Z
k
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
o
i
g
y
2
J
B
X
L
Z
g
H
1
C
L
J
O
m
0
D
p
0
m
Y
T
I
R
S
t
E
f
c
K
v
f
J
v
6
B
/
o
V
3
x
i
m
o
R
X
R
C
k
j
P
n
3
n
N
m
7
r
1
h
K
n
i
m
P
O
+
1
4
C
w
s
L
i
2
v
F
F
f
X
1
j
c
2
t
7
Z
L
O
7
u
t
L
M
l
l
x
J
p
R
I
h
L
Z
C
Y
O
M
C
R
6
z
p
u
J
K
s
E
4
q
W
T
A
O
B
W
u
H
o
5
q
O
t
+
+
Y
z
H
g
S
X
6
l
J
y
n
r
j
Y
B
j
z
A
Y
8
C
R
V
S
j
d
l
M
q
e
x
X
P
L
H
c
e
+
B
a
U
Y
V
c
9
K
b
3
g
G
n
0
k
i
J
B
j
D
I
Y
Y
i
r
B
A
g
I
y
e
L
n
x
4
S
I
n
r
Y
U
q
c
J
M
R
N
n
O
E
e
a
6
T
N
K
Y
t
R
R
k
D
s
i
L
5
D
2
n
U
t
G
9
N
e
e
2
Z
G
H
d
E
p
g
l
5
J
S
h
e
H
p
E
k
o
T
x
L
W
p
7
k
m
n
h
t
n
z
f
7
m
P
T
W
e
+
m
4
T
+
o
f
W
a
0
y
s
w
i
2
x
f
+
l
m
m
f
/
V
6
V
o
U
B
j
g
z
N
X
C
q
K
T
W
M
r
i
6
y
L
r
n
p
i
r
6
5
+
6
U
q
R
Q
4
p
c
R
r
3
K
S
4
J
R
0
Y
5
6
7
N
r
N
J
m
p
X
f
c
2
M
P
E
3
k
6
l
Z
v
Y
9
s
b
o
5
3
f
U
s
a
s
P
9
z
n
P
O
g
d
V
z
x
v
Y
r
f
O
C
l
X
z
+
2
o
i
9
j
H
A
Y
5
o
n
q
e
o
4
h
J
1
N
I
3
3
I
5
7
w
7
F
w
4
w
s
m
c
/
D
P
V
K
V
j
N
H
r
4
t
5
+
E
D
7
S
2
P
S
A
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
r
q
G
F
K
Y
8
X
a
m
w
i
b
o
g
c
7
E
6
T
+
P
M
3
g
+
0
=
"
>
A
A
A
C
x
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
I
i
g
y
6
K
b
L
i
v
a
B
9
Q
i
y
X
R
a
h
+
Z
F
M
l
F
K
E
f
w
B
t
/
p
p
4
h
/
o
X
3
h
n
n
I
J
a
R
C
c
k
O
X
P
u
P
W
f
m
3
h
u
k
o
c
i
l
6
7
6
W
r
L
n
5
h
c
W
l
8
v
L
K
6
t
r
6
x
m
Z
l
a
7
u
V
J
0
X
G
e
J
M
l
Y
Z
J
1
A
j
/
n
o
Y
h
5
U
w
o
Z
8
k
6
a
c
T
8
K
Q
t
4
O
R
m
c
q
3
r
7
l
W
S
6
S
+
F
K
O
U
9
6
L
/
G
E
s
B
o
L
5
k
q
g
L
x
3
G
u
K
1
X
X
c
f
W
y
Z
4
F
n
Q
B
V
m
N
Z
L
K
C
6
7
Q
R
w
K
G
A
h
E
4
Y
k
j
C
I
X
z
k
9
H
T
h
w
U
V
K
X
A
8
T
4
j
J
C
Q
s
c
5
7
r
F
C
2
o
K
y
O
G
X
4
x
I
7
o
O
6
R
d
1
7
A
x
7
Z
V
n
r
t
W
M
T
g
n
p
z
U
h
p
Y
5
8
0
C
e
V
l
h
N
V
p
t
o
4
X
2
l
m
x
v
3
l
P
t
K
e
6
2
5
j
+
g
f
G
K
i
J
W
4
I
f
Y
v
3
T
T
z
v
z
p
V
i
8
Q
A
J
7
o
G
Q
T
W
l
m
l
H
V
M
e
N
S
6
K
6
o
m
9
t
f
q
p
L
k
k
B
K
n
c
J
/
i
G
W
G
m
l
d
M
+
2
1
q
T
6
9
p
V
b
3
0
d
f
9
O
Z
i
l
V
7
Z
n
I
L
v
K
t
b
0
o
C
9
n
+
O
c
B
a
1
D
x
3
M
d
7
/
y
o
W
j
s
1
o
y
5
j
F
3
s
4
o
H
k
e
o
4
Y
6
G
m
i
S
9
x
C
P
e
M
K
z
V
b
d
i
q
7
D
u
P
l
O
t
k
t
H
s
4
N
u
y
H
j
4
A
4
e
i
P
o
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
r
q
G
F
K
Y
8
X
a
m
w
i
b
o
g
c
7
E
6
T
+
P
M
3
g
+
0
=
"
>
A
A
A
C
x
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
I
i
g
y
6
K
b
L
i
v
a
B
9
Q
i
y
X
R
a
h
+
Z
F
M
l
F
K
E
f
w
B
t
/
p
p
4
h
/
o
X
3
h
n
n
I
J
a
R
C
c
k
O
X
P
u
P
W
f
m
3
h
u
k
o
c
i
l
6
7
6
W
r
L
n
5
h
c
W
l
8
v
L
K
6
t
r
6
x
m
Z
l
a
7
u
V
J
0
X
G
e
J
M
l
Y
Z
J
1
A
j
/
n
o
Y
h
5
U
w
o
Z
8
k
6
a
c
T
8
K
Q
t
4
O
R
m
c
q
3
r
7
l
W
S
6
S
+
F
K
O
U
9
6
L
/
G
E
s
B
o
L
5
k
q
g
L
x
3
G
u
K
1
X
X
c
f
W
y
Z
4
F
n
Q
B
V
m
N
Z
L
K
C
6
7
Q
R
w
K
G
A
h
E
4
Y
k
j
C
I
X
z
k
9
H
T
h
w
U
V
K
X
A
8
T
4
j
J
C
Q
s
c
5
7
r
F
C
2
o
K
y
O
G
X
4
x
I
7
o
O
6
R
d
1
7
A
x
7
Z
V
n
r
t
W
M
T
g
n
p
z
U
h
p
Y
5
8
0
C
e
V
l
h
N
V
p
t
o
4
X
2
l
m
x
v
3
l
P
t
K
e
6
2
5
j
+
g
f
G
K
i
J
W
4
I
f
Y
v
3
T
T
z
v
z
p
V
i
8
Q
A
J
7
o
G
Q
T
W
l
m
l
H
V
M
e
N
S
6
K
6
o
m
9
t
f
q
p
L
k
k
B
K
n
c
J
/
i
G
W
G
m
l
d
M
+
2
1
q
T
6
9
p
V
b
3
0
d
f
9
O
Z
i
l
V
7
Z
n
I
L
v
K
t
b
0
o
C
9
n
+
O
c
B
a
1
D
x
3
M
d
7
/
y
o
W
j
s
1
o
y
5
j
F
3
s
4
o
H
k
e
o
4
Y
6
G
m
i
S
9
x
C
P
e
M
K
z
V
b
d
i
q
7
D
u
P
l
O
t
k
t
H
s
4
N
u
y
H
j
4
A
4
e
i
P
o
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
r
q
G
F
K
Y
8
X
a
m
w
i
b
o
g
c
7
E
6
T
+
P
M
3
g
+
0
=
"
>
A
A
A
C
x
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
I
i
g
y
6
K
b
L
i
v
a
B
9
Q
i
y
X
R
a
h
+
Z
F
M
l
F
K
E
f
w
B
t
/
p
p
4
h
/
o
X
3
h
n
n
I
J
a
R
C
c
k
O
X
P
u
P
W
f
m
3
h
u
k
o
c
i
l
6
7
6
W
r
L
n
5
h
c
W
l
8
v
L
K
6
t
r
6
x
m
Z
l
a
7
u
V
J
0
X
G
e
J
M
l
Y
Z
J
1
A
j
/
n
o
Y
h
5
U
w
o
Z
8
k
6
a
c
T
8
K
Q
t
4
O
R
m
c
q
3
r
7
l
W
S
6
S
+
F
K
O
U
9
6
L
/
G
E
s
B
o
L
5
k
q
g
L
x
3
G
u
K
1
X
X
c
f
W
y
Z
4
F
n
Q
B
V
m
N
Z
L
K
C
6
7
Q
R
w
K
G
A
h
E
4
Y
k
j
C
I
X
z
k
9
H
T
h
w
U
V
K
X
A
8
T
4
j
J
C
Q
s
c
5
7
r
F
C
2
o
K
y
O
G
X
4
x
I
7
o
O
6
R
d
1
7
A
x
7
Z
V
n
r
t
W
M
T
g
n
p
z
U
h
p
Y
5
8
0
C
e
V
l
h
N
V
p
t
o
4
X
2
l
m
x
v
3
l
P
t
K
e
6
2
5
j
+
g
f
G
K
i
J
W
4
I
f
Y
v
3
T
T
z
v
z
p
V
i
8
Q
A
J
7
o
G
Q
T
W
l
m
l
H
V
M
e
N
S
6
K
6
o
m
9
t
f
q
p
L
k
k
B
K
n
c
J
/
i
G
W
G
m
l
d
M
+
2
1
q
T
6
9
p
V
b
3
0
d
f
9
O
Z
i
l
V
7
Z
n
I
L
v
K
t
b
0
o
C
9
n
+
O
c
B
a
1
D
x
3
M
d
7
/
y
o
W
j
s
1
o
y
5
j
F
3
s
4
o
H
k
e
o
4
Y
6
G
m
i
S
9
x
C
P
e
M
K
z
V
b
d
i
q
7
D
u
P
l
O
t
k
t
H
s
4
N
u
y
H
j
4
A
4
e
i
P
o
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
r
q
G
F
K
Y
8
X
a
m
w
i
b
o
g
c
7
E
6
T
+
P
M
3
g
+
0
=
"
>
A
A
A
C
x
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
/
q
u
u
n
Q
T
L
I
K
r
k
I
i
g
y
6
K
b
L
i
v
a
B
9
Q
i
y
X
R
a
h
+
Z
F
M
l
F
K
E
f
w
B
t
/
p
p
4
h
/
o
X
3
h
n
n
I
J
a
R
C
c
k
O
X
P
u
P
W
f
m
3
h
u
k
o
c
i
l
6
7
6
W
r
L
n
5
h
c
W
l
8
v
L
K
6
t
r
6
x
m
Z
l
a
7
u
V
J
0
X
G
e
J
M
l
Y
Z
J
1
A
j
/
n
o
Y
h
5
U
w
o
Z
8
k
6
a
c
T
8
K
Q
t
4
O
R
m
c
q
3
r
7
l
W
S
6
S
+
F
K
O
U
9
6
L
/
G
E
s
B
o
L
5
k
q
g
L
x
3
G
u
K
1
X
X
c
f
W
y
Z
4
F
n
Q
B
V
m
N
Z
L
K
C
6
7
Q
R
w
K
G
A
h
E
4
Y
k
j
C
I
X
z
k
9
H
T
h
w
U
V
K
X
A
8
T
4
j
J
C
Q
s
c
5
7
r
F
C
2
o
K
y
O
G
X
4
x
I
7
o
O
6
R
d
1
7
A
x
7
Z
V
n
r
t
W
M
T
g
n
p
z
U
h
p
Y
5
8
0
C
e
V
l
h
N
V
p
t
o
4
X
2
l
m
x
v
3
l
P
t
K
e
6
2
5
j
+
g
f
G
K
i
J
W
4
I
f
Y
v
3
T
T
z
v
z
p
V
i
8
Q
A
J
7
o
G
Q
T
W
l
m
l
H
V
M
e
N
S
6
K
6
o
m
9
t
f
q
p
L
k
k
B
K
n
c
J
/
i
G
W
G
m
l
d
M
+
2
1
q
T
6
9
p
V
b
3
0
d
f
9
O
Z
i
l
V
7
Z
n
I
L
v
K
t
b
0
o
C
9
n
+
O
c
B
a
1
D
x
3
M
d
7
/
y
o
W
j
s
1
o
y
5
j
F
3
s
4
o
H
k
e
o
4
Y
6
G
m
i
S
9
x
C
P
e
M
K
z
V
b
d
i
q
7
D
u
P
l
O
t
k
t
H
s
4
N
u
y
H
j
4
A
4
e
i
P
o
w
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
7
k
V
D
1
b
z
t
f
l
q
X
q
y
6
u
e
1
p
P
j
T
m
2
Y
K
4
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
k
U
x
G
U
L
9
g
G
1
S
J
J
O
a
+
g
0
C
T
M
T
o
R
T
9
A
b
f
6
b
e
I
f
6
F
9
4
Z
5
y
C
W
k
Q
n
J
D
l
z
7
j
1
n
5
t
4
b
Z
j
y
W
y
v
N
e
C
8
7
C
4
t
L
y
S
n
G
1
t
L
a
+
s
b
l
V
3
t
5
p
y
T
Q
X
E
W
t
G
K
U
9
F
J
w
w
k
4
3
H
C
m
i
p
W
n
H
U
y
w
Y
J
x
y
F
k
7
H
J
3
r
e
P
u
O
C
R
m
n
y
Z
W
a
Z
K
w
3
D
o
Z
J
P
I
i
j
Q
B
H
V
6
N
+
U
K
1
7
V
M
8
u
d
B
7
4
F
F
d
h
V
T
8
s
v
u
E
Y
f
K
S
L
k
G
I
M
h
g
S
L
M
E
U
D
S
0
4
U
P
D
x
l
x
P
U
y
J
E
4
R
i
E
2
e
4
R
4
m
0
O
W
U
x
y
g
i
I
H
d
F
3
S
L
u
u
Z
R
P
a
a
0
9
p
1
B
G
d
w
u
k
V
p
H
R
x
Q
J
q
U
8
g
R
h
f
Z
p
r
4
r
l
x
1
u
x
v
3
l
P
j
q
e
8
2
o
X
9
o
v
c
b
E
K
t
w
S
+
5
d
u
l
v
l
f
n
a
5
F
Y
Y
B
T
U
0
N
M
N
W
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
I
0
7
j
P
s
U
F
4
c
g
o
Z
3
1
2
j
U
a
a
2
n
V
v
A
x
N
/
M
5
m
a
1
f
v
I
5
u
Z
4
1
7
e
k
A
f
s
/
x
z
k
P
W
k
d
V
3
6
v
6
j
e
N
K
7
c
y
O
u
o
g
9
7
O
O
Q
5
n
m
C
G
i
5
R
R
9
N
4
P
+
I
J
z
8
6
F
w
x
3
p
5
J
+
p
T
s
F
q
d
v
F
t
O
Q
8
f
O
5
y
P
a
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
7
k
V
D
1
b
z
t
f
l
q
X
q
y
6
u
e
1
p
P
j
T
m
2
Y
K
4
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
k
U
x
G
U
L
9
g
G
1
S
J
J
O
a
+
g
0
C
T
M
T
o
R
T
9
A
b
f
6
b
e
I
f
6
F
9
4
Z
5
y
C
W
k
Q
n
J
D
l
z
7
j
1
n
5
t
4
b
Z
j
y
W
y
v
N
e
C
8
7
C
4
t
L
y
S
n
G
1
t
L
a
+
s
b
l
V
3
t
5
p
y
T
Q
X
E
W
t
G
K
U
9
F
J
w
w
k
4
3
H
C
m
i
p
W
n
H
U
y
w
Y
J
x
y
F
k
7
H
J
3
r
e
P
u
O
C
R
m
n
y
Z
W
a
Z
K
w
3
D
o
Z
J
P
I
i
j
Q
B
H
V
6
N
+
U
K
1
7
V
M
8
u
d
B
7
4
F
F
d
h
V
T
8
s
v
u
E
Y
f
K
S
L
k
G
I
M
h
g
S
L
M
E
U
D
S
0
4
U
P
D
x
l
x
P
U
y
J
E
4
R
i
E
2
e
4
R
4
m
0
O
W
U
x
y
g
i
I
H
d
F
3
S
L
u
u
Z
R
P
a
a
0
9
p
1
B
G
d
w
u
k
V
p
H
R
x
Q
J
q
U
8
g
R
h
f
Z
p
r
4
r
l
x
1
u
x
v
3
l
P
j
q
e
8
2
o
X
9
o
v
c
b
E
K
t
w
S
+
5
d
u
l
v
l
f
n
a
5
F
Y
Y
B
T
U
0
N
M
N
W
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
I
0
7
j
P
s
U
F
4
c
g
o
Z
3
1
2
j
U
a
a
2
n
V
v
A
x
N
/
M
5
m
a
1
f
v
I
5
u
Z
4
1
7
e
k
A
f
s
/
x
z
k
P
W
k
d
V
3
6
v
6
j
e
N
K
7
c
y
O
u
o
g
9
7
O
O
Q
5
n
m
C
G
i
5
R
R
9
N
4
P
+
I
J
z
8
6
F
w
x
3
p
5
J
+
p
T
s
F
q
d
v
F
t
O
Q
8
f
O
5
y
P
a
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
7
k
V
D
1
b
z
t
f
l
q
X
q
y
6
u
e
1
p
P
j
T
m
2
Y
K
4
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
k
U
x
G
U
L
9
g
G
1
S
J
J
O
a
+
g
0
C
T
M
T
o
R
T
9
A
b
f
6
b
e
I
f
6
F
9
4
Z
5
y
C
W
k
Q
n
J
D
l
z
7
j
1
n
5
t
4
b
Z
j
y
W
y
v
N
e
C
8
7
C
4
t
L
y
S
n
G
1
t
L
a
+
s
b
l
V
3
t
5
p
y
T
Q
X
E
W
t
G
K
U
9
F
J
w
w
k
4
3
H
C
m
i
p
W
n
H
U
y
w
Y
J
x
y
F
k
7
H
J
3
r
e
P
u
O
C
R
m
n
y
Z
W
a
Z
K
w
3
D
o
Z
J
P
I
i
j
Q
B
H
V
6
N
+
U
K
1
7
V
M
8
u
d
B
7
4
F
F
d
h
V
T
8
s
v
u
E
Y
f
K
S
L
k
G
I
M
h
g
S
L
M
E
U
D
S
0
4
U
P
D
x
l
x
P
U
y
J
E
4
R
i
E
2
e
4
R
4
m
0
O
W
U
x
y
g
i
I
H
d
F
3
S
L
u
u
Z
R
P
a
a
0
9
p
1
B
G
d
w
u
k
V
p
H
R
x
Q
J
q
U
8
g
R
h
f
Z
p
r
4
r
l
x
1
u
x
v
3
l
P
j
q
e
8
2
o
X
9
o
v
c
b
E
K
t
w
S
+
5
d
u
l
v
l
f
n
a
5
F
Y
Y
B
T
U
0
N
M
N
W
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
I
0
7
j
P
s
U
F
4
c
g
o
Z
3
1
2
j
U
a
a
2
n
V
v
A
x
N
/
M
5
m
a
1
f
v
I
5
u
Z
4
1
7
e
k
A
f
s
/
x
z
k
P
W
k
d
V
3
6
v
6
j
e
N
K
7
c
y
O
u
o
g
9
7
O
O
Q
5
n
m
C
G
i
5
R
R
9
N
4
P
+
I
J
z
8
6
F
w
x
3
p
5
J
+
p
T
s
F
q
d
v
F
t
O
Q
8
f
O
5
y
P
a
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
7
k
V
D
1
b
z
t
f
l
q
X
q
y
6
u
e
1
p
P
j
T
m
2
Y
K
4
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
k
U
x
G
U
L
9
g
G
1
S
J
J
O
a
+
g
0
C
T
M
T
o
R
T
9
A
b
f
6
b
e
I
f
6
F
9
4
Z
5
y
C
W
k
Q
n
J
D
l
z
7
j
1
n
5
t
4
b
Z
j
y
W
y
v
N
e
C
8
7
C
4
t
L
y
S
n
G
1
t
L
a
+
s
b
l
V
3
t
5
p
y
T
Q
X
E
W
t
G
K
U
9
F
J
w
w
k
4
3
H
C
m
i
p
W
n
H
U
y
w
Y
J
x
y
F
k
7
H
J
3
r
e
P
u
O
C
R
m
n
y
Z
W
a
Z
K
w
3
D
o
Z
J
P
I
i
j
Q
B
H
V
6
N
+
U
K
1
7
V
M
8
u
d
B
7
4
F
F
d
h
V
T
8
s
v
u
E
Y
f
K
S
L
k
G
I
M
h
g
S
L
M
E
U
D
S
0
4
U
P
D
x
l
x
P
U
y
J
E
4
R
i
E
2
e
4
R
4
m
0
O
W
U
x
y
g
i
I
H
d
F
3
S
L
u
u
Z
R
P
a
a
0
9
p
1
B
G
d
w
u
k
V
p
H
R
x
Q
J
q
U
8
g
R
h
f
Z
p
r
4
r
l
x
1
u
x
v
3
l
P
j
q
e
8
2
o
X
9
o
v
c
b
E
K
t
w
S
+
5
d
u
l
v
l
f
n
a
5
F
Y
Y
B
T
U
0
N
M
N
W
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
I
0
7
j
P
s
U
F
4
c
g
o
Z
3
1
2
j
U
a
a
2
n
V
v
A
x
N
/
M
5
m
a
1
f
v
I
5
u
Z
4
1
7
e
k
A
f
s
/
x
z
k
P
W
k
d
V
3
6
v
6
j
e
N
K
7
c
y
O
u
o
g
9
7
O
O
Q
5
n
m
C
G
i
5
R
R
9
N
4
P
+
I
J
z
8
6
F
w
x
3
p
5
J
+
p
T
s
F
q
d
v
F
t
O
Q
8
f
O
5
y
P
a
Q
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
m
y
N
i
8
R
r
Z
U
/
l
U
0
B
7
Z
l
F
1
N
y
A
t
W
6
M
=
"
>
A
A
A
C
x
X
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
0
V
R
I
R
d
F
l
0
o
c
s
q
9
g
G
1
S
D
K
d
1
q
F
5
M
Z
k
U
S
h
F
/
w
K
3
+
m
v
g
H
+
h
f
e
G
V
N
Q
i
+
i
E
J
G
f
O
v
e
f
M
3
H
v
9
J
B
C
p
c
p
z
X
g
j
U
3
v
7
C
4
V
F
w
u
r
a
y
u
r
W
+
U
N
7
e
a
a
Z
x
J
x
h
s
s
D
m
L
Z
9
r
2
U
B
y
L
i
D
S
V
U
w
N
u
J
5
F
7
o
B
7
z
l
D
8
9
0
v
D
X
i
M
h
V
x
d
K
3
G
C
e
+
G
3
i
A
S
f
c
E
8
R
d
R
V
b
/
+
2
X
H
G
q
j
l
n
2
L
H
B
z
U
E
G
+
6
n
H
5
B
T
f
o
I
Q
Z
D
h
h
A
c
E
R
T
h
A
B
5
S
e
j
p
w
4
S
A
h
r
o
s
J
c
Z
K
Q
M
H
G
O
e
5
R
I
m
1
E
W
p
w
y
P
2
C
F
9
B
7
T
r
5
G
x
E
e
+
2
Z
G
j
W
j
U
w
J
6
J
S
l
t
7
J
E
m
p
j
x
J
W
J
9
m
m
3
h
m
n
D
X
7
m
/
f
E
e
O
q
7
j
e
n
v
5
1
4
h
s
Q
p
3
x
P
6
l
m
2
b
+
V
6
d
r
U
e
j
j
x
N
Q
g
q
K
b
E
M
L
o
6
l
r
t
k
p
i
v
6
5
v
a
X
q
h
Q
5
J
M
R
p
3
K
O
4
J
M
y
M
c
t
p
n
2
2
h
S
U
7
v
u
r
W
f
i
b
y
Z
T
s
3
r
P
8
t
w
M
7
/
q
W
N
G
D
3
5
z
h
n
Q
f
O
w
6
j
p
V
9
/
K
o
U
j
v
N
R
1
3
E
D
n
Z
x
Q
P
M
8
R
g
0
X
q
K
N
B
3
n
0
8
4
g
n
P
1
r
k
V
W
s
o
a
f
a
Z
a
h
V
y
z
j
W
/
L
e
v
g
A
v
m
C
P
m
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
m
y
N
i
8
R
r
Z
U
/
l
U
0
B
7
Z
l
F
1
N
y
A
t
W
6
M
=
"
>
A
A
A
C
x
X
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
0
V
R
I
R
d
F
l
0
o
c
s
q
9
g
G
1
S
D
K
d
1
q
F
5
M
Z
k
U
S
h
F
/
w
K
3
+
m
v
g
H
+
h
f
e
G
V
N
Q
i
+
i
E
J
G
f
O
v
e
f
M
3
H
v
9
J
B
C
p
c
p
z
X
g
j
U
3
v
7
C
4
V
F
w
u
r
a
y
u
r
W
+
U
N
7
e
a
a
Z
x
J
x
h
s
s
D
m
L
Z
9
r
2
U
B
y
L
i
D
S
V
U
w
N
u
J
5
F
7
o
B
7
z
l
D
8
9
0
v
D
X
i
M
h
V
x
d
K
3
G
C
e
+
G
3
i
A
S
f
c
E
8
R
d
R
V
b
/
+
2
X
H
G
q
j
l
n
2
L
H
B
z
U
E
G
+
6
n
H
5
B
T
f
o
I
Q
Z
D
h
h
A
c
E
R
T
h
A
B
5
S
e
j
p
w
4
S
A
h
r
o
s
J
c
Z
K
Q
M
H
G
O
e
5
R
I
m
1
E
W
p
w
y
P
2
C
F
9
B
7
T
r
5
G
x
E
e
+
2
Z
G
j
W
j
U
w
J
6
J
S
l
t
7
J
E
m
p
j
x
J
W
J
9
m
m
3
h
m
n
D
X
7
m
/
f
E
e
O
q
7
j
e
n
v
5
1
4
h
s
Q
p
3
x
P
6
l
m
2
b
+
V
6
d
r
U
e
j
j
x
N
Q
g
q
K
b
E
M
L
o
6
l
r
t
k
p
i
v
6
5
v
a
X
q
h
Q
5
J
M
R
p
3
K
O
4
J
M
y
M
c
t
p
n
2
2
h
S
U
7
v
u
r
W
f
i
b
y
Z
T
s
3
r
P
8
t
w
M
7
/
q
W
N
G
D
3
5
z
h
n
Q
f
O
w
6
j
p
V
9
/
K
o
U
j
v
N
R
1
3
E
D
n
Z
x
Q
P
M
8
R
g
0
X
q
K
N
B
3
n
0
8
4
g
n
P
1
r
k
V
W
s
o
a
f
a
Z
a
h
V
y
z
j
W
/
L
e
v
g
A
v
m
C
P
m
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
m
y
N
i
8
R
r
Z
U
/
l
U
0
B
7
Z
l
F
1
N
y
A
t
W
6
M
=
"
>
A
A
A
C
x
X
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
0
V
R
I
R
d
F
l
0
o
c
s
q
9
g
G
1
S
D
K
d
1
q
F
5
M
Z
k
U
S
h
F
/
w
K
3
+
m
v
g
H
+
h
f
e
G
V
N
Q
i
+
i
E
J
G
f
O
v
e
f
M
3
H
v
9
J
B
C
p
c
p
z
X
g
j
U
3
v
7
C
4
V
F
w
u
r
a
y
u
r
W
+
U
N
7
e
a
a
Z
x
J
x
h
s
s
D
m
L
Z
9
r
2
U
B
y
L
i
D
S
V
U
w
N
u
J
5
F
7
o
B
7
z
l
D
8
9
0
v
D
X
i
M
h
V
x
d
K
3
G
C
e
+
G
3
i
A
S
f
c
E
8
R
d
R
V
b
/
+
2
X
H
G
q
j
l
n
2
L
H
B
z
U
E
G
+
6
n
H
5
B
T
f
o
I
Q
Z
D
h
h
A
c
E
R
T
h
A
B
5
S
e
j
p
w
4
S
A
h
r
o
s
J
c
Z
K
Q
M
H
G
O
e
5
R
I
m
1
E
W
p
w
y
P
2
C
F
9
B
7
T
r
5
G
x
E
e
+
2
Z
G
j
W
j
U
w
J
6
J
S
l
t
7
J
E
m
p
j
x
J
W
J
9
m
m
3
h
m
n
D
X
7
m
/
f
E
e
O
q
7
j
e
n
v
5
1
4
h
s
Q
p
3
x
P
6
l
m
2
b
+
V
6
d
r
U
e
j
j
x
N
Q
g
q
K
b
E
M
L
o
6
l
r
t
k
p
i
v
6
5
v
a
X
q
h
Q
5
J
M
R
p
3
K
O
4
J
M
y
M
c
t
p
n
2
2
h
S
U
7
v
u
r
W
f
i
b
y
Z
T
s
3
r
P
8
t
w
M
7
/
q
W
N
G
D
3
5
z
h
n
Q
f
O
w
6
j
p
V
9
/
K
o
U
j
v
N
R
1
3
E
D
n
Z
x
Q
P
M
8
R
g
0
X
q
K
N
B
3
n
0
8
4
g
n
P
1
r
k
V
W
s
o
a
f
a
Z
a
h
V
y
z
j
W
/
L
e
v
g
A
v
m
C
P
m
g
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
2
m
y
N
i
8
R
r
Z
U
/
l
U
0
B
7
Z
l
F
1
N
y
A
t
W
6
M
=
"
>
A
A
A
C
x
X
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
0
V
R
I
R
d
F
l
0
o
c
s
q
9
g
G
1
S
D
K
d
1
q
F
5
M
Z
k
U
S
h
F
/
w
K
3
+
m
v
g
H
+
h
f
e
G
V
N
Q
i
+
i
E
J
G
f
O
v
e
f
M
3
H
v
9
J
B
C
p
c
p
z
X
g
j
U
3
v
7
C
4
V
F
w
u
r
a
y
u
r
W
+
U
N
7
e
a
a
Z
x
J
x
h
s
s
D
m
L
Z
9
r
2
U
B
y
L
i
D
S
V
U
w
N
u
J
5
F
7
o
B
7
z
l
D
8
9
0
v
D
X
i
M
h
V
x
d
K
3
G
C
e
+
G
3
i
A
S
f
c
E
8
R
d
R
V
b
/
+
2
X
H
G
q
j
l
n
2
L
H
B
z
U
E
G
+
6
n
H
5
B
T
f
o
I
Q
Z
D
h
h
A
c
E
R
T
h
A
B
5
S
e
j
p
w
4
S
A
h
r
o
s
J
c
Z
K
Q
M
H
G
O
e
5
R
I
m
1
E
W
p
w
y
P
2
C
F
9
B
7
T
r
5
G
x
E
e
+
2
Z
G
j
W
j
U
w
J
6
J
S
l
t
7
J
E
m
p
j
x
J
W
J
9
m
m
3
h
m
n
D
X
7
m
/
f
E
e
O
q
7
j
e
n
v
5
1
4
h
s
Q
p
3
x
P
6
l
m
2
b
+
V
6
d
r
U
e
j
j
x
N
Q
g
q
K
b
E
M
L
o
6
l
r
t
k
p
i
v
6
5
v
a
X
q
h
Q
5
J
M
R
p
3
K
O
4
J
M
y
M
c
t
p
n
2
2
h
S
U
7
v
u
r
W
f
i
b
y
Z
T
s
3
r
P
8
t
w
M
7
/
q
W
N
G
D
3
5
z
h
n
Q
f
O
w
6
j
p
V
9
/
K
o
U
j
v
N
R
1
3
E
D
n
Z
x
Q
P
M
8
R
g
0
X
q
K
N
B
3
n
0
8
4
g
n
P
1
r
k
V
W
s
o
a
f
a
Z
a
h
V
y
z
j
W
/
L
e
v
g
A
v
m
C
P
m
g
=
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
O
o
B
p
Z
x
w
o
O
7
Q
2
1
G
T
d
J
t
X
z
L
S
r
8
w
Q
w
=
"
>
A
A
A
C
x
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
V
R
I
R
d
F
l
U
x
G
U
L
9
g
F
a
J
E
m
n
d
e
g
0
C
Z
O
J
U
I
r
+
g
F
v
9
N
v
E
P
9
C
+
8
M
0
5
B
L
a
I
T
k
p
w
5
9
5
4
z
c
+
8
N
U
8
E
z
5
X
m
v
B
W
d
u
f
m
F
x
q
b
h
c
W
l
l
d
W
9
8
o
b
2
6
1
s
i
S
X
E
W
t
G
i
U
h
k
J
w
w
y
J
n
j
M
m
o
o
r
w
T
q
p
Z
M
E
o
F
K
w
d
D
k
9
1
v
H
3
H
Z
M
a
T
+
F
K
N
U
9
Y
d
B
Y
O
Y
9
3
k
U
K
K
I
a
Z
z
f
l
i
l
f
1
z
H
J
n
g
W
9
B
B
X
b
V
k
/
I
L
r
t
F
D
g
g
g
5
R
m
C
I
o
Q
g
L
B
M
j
o
u
Y
I
P
D
y
l
x
X
U
y
I
k
4
S
4
i
T
P
c
o
0
T
a
n
L
I
Y
Z
Q
T
E
D
u
k
7
o
N
2
V
Z
W
P
a
a
8
/
M
q
C
M
6
R
d
A
r
S
e
l
i
j
z
Q
J
5
U
n
C
+
j
T
X
x
H
P
j
r
N
n
f
v
C
f
G
U
9
9
t
T
P
/
Q
e
o
2
I
V
b
g
l
9
i
/
d
N
P
O
/
O
l
2
L
Q
h
/
H
p
g
Z
O
N
a
W
G
0
d
V
F
1
i
U
3
X
d
E
3
d
7
9
U
p
c
g
h
J
U
7
j
H
s
U
l
4
c
g
o
p
3
1
2
j
S
Y
z
t
e
v
e
B
i
b
+
Z
j
I
1
q
/
e
R
z
c
3
x
r
m
9
J
A
/
Z
/
j
n
M
W
t
A
6
q
v
l
f
1
G
4
e
V
2
o
k
d
d
R
E
7
2
M
U
+
z
f
M
I
N
V
y
g
j
q
b
x
f
s
Q
T
n
p
1
z
R
z
i
Z
k
3
+
m
O
g
W
r
2
c
a
3
5
T
x
8
A
O
+
N
j
0
k
=
<
/
l
a
t
e
x
i
t
>
acted by LeakyReLU [22] in our experiments. Thus, we can
learn and model the complex inter-relationships of the nodes
by stacking multiple GCN layers. For more details, we refer
interested readers to [14].

3.3. GCN for Multi label Recognition

Our ML-GCN is built upon GCN. GCN was proposed for
semi-supervised classiﬁcation, where the node-level output
is the prediction score of each node. Different from that, we
design the ﬁnal output of each GCN node to be the classiﬁer
of the corresponding label in our task. In addition, the graph
structure (i.e., the correlation matrix) is normally pre-deﬁned
in other tasks, which, however, is not provided in the multi-
label image recognition task. Thus, we need to construct the
correlation matrix from scratch. The overall framework of
our approach is shown in Fig. 2, which is composed of two
main modules, i.e., the image representation learning and
GCN based classiﬁer learning modules.

Image representation learning We can use any CNN
base models to learn the features of an image. In our experi-
ments, following [36, 1, 15, 6], we use ResNet-101 [10] as
the base model in experiments. Thus, if an input image I is
with the 448 × 448 resolution, we can obtain 2048 × 14 × 14
feature maps from the “conv5 x” layer. Then, we employ
global max-pooling to obtain the image-level feature x:

x = fGMP(fcnn(I; θcnn)) ∈ RD,

(3)

where θcnn indicates model parameters and D = 2048.

GCN based classiﬁer learning We learn inter-dependent
object classiﬁers, i.e., W = {wi}C
i=1, from label repre-
sentations via a GCN based mapping function, where C
denotes the number of categories. We use stacked GCNs
where each GCN layer l takes the node representations from
previous layer (H l) as inputs and outputs new node repre-
sentations, i.e., H l+1. For the ﬁrst layer, the input is the
Z ∈ RC×d matrix, where d is the dimensionality of the
label-level word embedding. For the last layer, the output
is W ∈ RC×D with D denoting the dimensionality of the
image representation. By applying the learned classiﬁers to
image representations, we can obtain the predicted scores as

ˆy = W x.

(4)

We assume that the ground truth label of an image is
y ∈ RC , where yi = {0, 1} denotes whether label i appears
in the image or not. The whole network is trained using the
traditional multi-label classiﬁcation loss as follows

L =

CXc=1

yc log(σ(ˆyc)) + (1 − yc) log(1 − σ(ˆyc)),

(5)

Person

0.1

Surfboard

Surfboard

0.75

Person

Figure 3. Illustration of conditional probability between two labels.
As usual, when “surfboard” appears in the image, “person”
will also occur with a high probability. However, in the condition of
“person” appearing, “surfboard” will not necessarily occur.

3.4. Correlation Matrix of ML GCN

GCN works by propagating information between nodes
based on the correlation matrix. Thus, how to build the
correlation matrix A is a crucial problem for GCN. In most
applications, the correlation matrix is pre-deﬁned, which,
however, is not provided in any standard multi-label image
recognition datasets. In this paper, we build this correlation
matrix through a data-driven way. That is, we deﬁne the
correlation between labels via mining their co-occurrence
patterns within the dataset.

We model the label correlation dependency in the form
of conditional probability, i.e., P (Lj|Li) which denotes the
probability of occurrence of label Lj when label Li appears.
As shown in Fig. 3, P (Lj|Li) is not equal to P (Li|Lj).
Thus, the correlation matrix is asymmetrical.

To construct the correlation matrix, ﬁrstly, we count the
occurrence of label pairs in the training set and get the matrix
M ∈ RC×C . Concretely, C is the number of categories,
and Mij denotes the concurring times of Li and Lj . Then,
by using this label co-occurrence matrix, we can get the
conditional probability matrix by

Pi = Mi/Ni,

(6)

where Ni denotes the occurrence times of Li in the training
set, and Pij = P (Lj|Li) means the probability of label Lj
when label Li appears.

However, the simple correlation above may suffer from
two drawbacks. Firstly, the co-occurrence patterns between a
label and the other labels may exhibit a long-tail distribution,
where some rare co-occurrences may be noise. Secondly, the
absolute number of co-occurrences from training and test
may not be completely consistent. A correlation matrix over-
ﬁtted to the training set can hurt the generalization capacity.
Thus, we propose to binarize the correlation P . Speciﬁcally,
we use the threshold τ to ﬁlter noisy edges, and the operation
can be written as

Aij =(0,

1,

if Pij < τ
if Pij ≥ τ

,

(7)

where σ(·) is the sigmoid function.

where A is the binary correlation matrix.

5180

,

(8)

4.3. Experimental Results

Over-smoothing problem From Eq. (2), we can conclude
that after GCN, the feature of a node will be the weighted
sum of its own feature and the adjacent nodes’ features.
Then, a direct problem for the binary correlation matrix is
that it can result in over-smoothing. That is, the node fea-
tures may be over-smoothed such that nodes from different
clusters (e.g., kitchen related vs. living room related) may
become indistinguishable [16]. To alleviate this problem, we
propose the following re-weighted scheme,

p/PC

1 − p,

j=1
i6=j

Aij,

if i 6= j

if i = j

A′

ij =

where A′ is the re-weighted correlation matrix, and p de-
termines the weights assigned to a node itself and other
correlated nodes. By doing this, when updating the node
feature, we will have a ﬁxed weight for the node itself and
the weights for correlated nodes will be determined by the
neighborhood distribution. When p → 1, the feature of a
node itself will not be considered. While, on the other hand,
when p → 0, neighboring information tends to be ignored.

4. Experiments

In this section, we ﬁrst describe the evaluation metrics
and implementation details. Then, we report the empirical
results on two benchmark multi-label image recognition
datasets, i.e., MS-COCO [20] and VOC 2007 [5]. Finally,
visualization analyses are presented.

4.1. Evaluation Metrics

Following conventional settings [28, 6, 36], we report the
average per-class precision (CP), recall (CR), F1 (CF1) and
the average overall precision (OP), recall (OR), F1 (OF1)
for performance evaluation. For each image, the labels are
predicted as positive if the conﬁdences of them are greater
than 0.5. For fair comparisons, we also report the results of
top-3 labels, cf. [36, 6]. In addition, we also compute and
report the mean average precision (mAP). Generally, average
overall F1 (OF1), average per-class F1 (CF1) and mAP are
relatively more important for performance evaluation.

4.2. Implementation Details

Without otherwise stated, our ML-GCN consists of two
GCN layers with output dimensionality of 1024 and 2048,
respectively. For label representations, we adopt 300-dim
GloVe [25] trained on the Wikipedia dataset. For the cate-
gories whose names contain multiple words, we obtain the
label representation as average of embeddings for all words.
For the correlation matrix, without otherwise stated, we set τ
in Eq. (7) to be 0.4 and p in Eq. (8) to be 0.2. In the image rep-
resentation learning branch, we adopt LeakyReLU [22] with
the negative slope of 0.2 as the non-linear activation function,

which leads to faster convergence in experiments. We adopt
ResNet-101 [10] as the feature extraction backbone, which
is pre-trained on ImageNet [4]. During training, the input
images are random cropped and resized into 448 × 448 with
random horizontal ﬂips for data augmentation. For network
optimization, SGD is used as the optimizer. The momentum
is set to be 0.9. Weight decay is 10−4. The initial learning
rate is 0.01, which decays by a factor of 10 for every 40
epochs and the network is trained for 100 epochs in total.
We implement the network based on PyTorch.

In this part, we ﬁrst present our comparisons with state-of-
the-arts on MS-COCO and VOC 2007, respectively. Then,
we conduct ablation studies to evaluate the key aspects of
the proposed approach.

4.3.1 Comparisons with State-of-the-Arts

Results on MS-COCO Microsoft COCO [20] is a widely
used benchmark for multi-label image recognition. It con-
tains 82,081 images as the training set and 40,504 images
as the validation set. The objects are categorized into 80
classes with about 2.9 object labels per image. Since the
ground-truth labels of the test set are not available, we eval-
uate the performance of all the methods on the validation
set. The number of labels of different images also varies
considerably, which makes MS-COCO more challenging.

Quantitative results are reported in Table 1. We compare
with state-of-the-art methods, including CNN-RNN [28],
RNN-Attention [29], Order-Free RNN [1], ML-ZSL [15],
SRN [36], Multi-Evidence [6], etc. For the proposed ML-
GCN, we report the results based on the binary correlation
matrix (“ML-GCN (Binary)”) and the re-weighted correla-
tion matrix (“ML-GCN (Re-weighted)”), respectively. It is
obvious to see that our ML-GCN method based on the binary
correlation matrix obtains worse classiﬁcation performance,
which may be largely due to the over-smoothing problem
discussed in Sec. 3.4. The proposed re-weighted scheme can
alleviate the over-smoothing issue and consequently obtains
superior performance. Comparing with state-of-the-art meth-
ods, our approach with the proposed re-weighted scheme
consistently performs better under almost all metrics, which
shows the effectiveness of our proposed ML-GCN as well
as its corresponding re-weighted scheme.

Results on VOC 2007 PASCAL Visual Object Classes
Challenge (VOC 2007) [5] is another popular dataset for
multi-label recognition. It contains 9,963 images from 20
object categories, which is divided into train, val and test
sets. Following [2, 29], we use the trainval set to train our
model, and evaluate the recognition performance on the test
set. In order to compare with other state-of-the-art methods,

5181

Table 1. Comparisons with state-of-the-art methods on the MS-COCO dataset. The performance of the proposed ML-GCN based on two
types of correlation matrices are reported.“Binary” denotes that we use the binary correlation matrix, cf. Eq. (7). “Re-weighted” means the
correlation matrix generated by the proposed re-weighted scheme is used, cf. Eq. (8).

Methods

CNN-RNN [28]

RNN-Attention [29]
Order-Free RNN [1]

ML-ZSL [15]

SRN [36]

ResNet-101 [10]

Multi-Evidence [6]

ML-GCN (Binary)

ML-GCN (Re-weighted)

mAP

61.2

–
–
–

77.1
77.3

–

80.3
83.0

CP

CR

–
–
–
–

81.6
80.2
80.4

81.1
85.1

–
–
–
–

65.4
66.7
70.2

70.1
72.0

All
CF1

–
–
–
–

71.2
72.8
74.9

75.2
78.0

OP

OR

OF1

–
–
–
–

82.7
83.9
85.2

83.8
85.8

–
–
–
–

69.9
70.8
72.5

74.2
75.4

–
–
–
–

75.8
76.8
78.4

78.7
80.3

CP

66.0
79.1
71.6
74.1
85.2
84.1
84.5

84.9
89.2

CR

55.6
58.7
54.8
64.5
58.8
59.4
62.2

61.3
64.1

Top-3

CF1

60.4
67.4
62.1
69.0
67.4
69.7
70.6

71.2
74.6

OP

69.2
84.0
74.2

–

87.4
89.1
89.1

88.8
90.5

OR

66.4
63.0
62.2

–

62.5
62.8
64.3

65.2
66.5

OF1

67.8
72.0
67.7

–

72.9
73.6
74.7

75.2
76.7

Table 2. Comparisons of AP and mAP with state-of-the-art methods on the VOC 2007 dataset. The meanings of “Binary” and “Re-weighted”
are the same as Table 1.

Methods

aero bike bird boat bottle bus car

cat chair cow table dog horse motor person plant sheep sofa train tv mAP

CNN-RNN [28]

RLSD [34]

VeryDeep [26]
ResNet-101 [10]

FeV+LV [33]

HCP [31]

RNN-Attention [29]
Atten-Reinforce [2]

96.7 83.1 94.2 92.8 61.2 82.1 89.1 94.2 64.2 83.6 70.0 92.4 91.7
96.4 92.7 93.8 94.1 71.2 92.5 94.2 95.7 74.3 90.0 74.2 95.4 96.2
98.9 95.0 96.8 95.4 69.7 90.4 93.5 96.0 74.2 86.6 87.8 96.0 96.3
99.5 97.7 97.8 96.4 65.7 91.8 96.1 97.6 74.2 80.9 85.0 98.4 96.5
97.9 97.0 96.6 94.6 73.6 93.9 96.5 95.5 73.7 90.3 82.8 95.4 97.7
98.6 97.1 98.0 95.6 75.3 94.7 95.8 97.3 73.1 90.2 80.0 97.3 96.1
98.6 97.4 96.3 96.2 75.2 92.4 96.5 97.1 76.5 92.0 87.7 96.8 97.5
98.6 97.1 97.1 95.5 75.6 92.8 96.8 97.3 78.3 92.2 87.6 96.9 96.5

VGG (Binary)

98.3 97.1 96.1 96.7 75.0 91.4 95.8 95.4 76.7 92.1 85.1 96.7 96.0
99.4 97.4 98.0 97.0 77.9 92.4 96.8 97.8 80.8 93.4 87.2 98.0 97.3
99.6 98.3 97.9 97.6 78.2 92.3 97.4 97.4 79.2 94.4 86.5 97.4 97.9
ML-GCN (Re-weighted) 99.5 98.5 98.6 98.1 80.8 94.6 97.2 98.2 82.3 95.7 86.4 98.2 98.4

VGG (Re-weighted)
ML-GCN (Binary)

84.2
92.1
93.1
95.9
95.9
94.9
93.8
93.6

95.3
95.8
97.1
96.7

93.7
97.9
97.2
98.4
98.6
96.3
98.5
98.5

97.8
98.8
98.7
99.0

59.8 93.2 75.3 99.7 78.6 84.0
66.9 93.5 73.7 97.5 87.6 88.5
70.0 92.1 80.3 98.1 87.0 89.7
70.1 88.3 80.2 98.9 89.2 89.9
77.6 88.7 78.0 98.3 89.0 90.6
78.3 94.7 76.2 97.9 91.5 90.9
81.6 93.7 82.8 98.6 89.3 91.9
81.6 93.1 83.2 98.5 89.3 92.0

77.4 93.1 79.7 97.9 89.3 91.1
79.4 95.3 82.2 99.1 91.4 92.8
84.6 95.3 83.0 98.6 90.4 93.1
84.7 96.7 84.3 98.9 93.7 94.0

we report the results of average precision (AP) and mean
average precision (mAP).

The results of VOC 2007 are presented in Table 2. Be-
cause the results of many previous works on VOC 2007 are
based on the VGG model [26]. For fair comparisons, we also
report the results using VGG models as the base model. It is
apparent to see that, our proposed method observes improve-
ments upon the previous methods. Concretely, the proposed
ML-GCN with our re-weighted scheme obtains 94.0% mAP,
which outperforms state-of-the-art by 2%. Even using VGG
model as the base model, we can still achieve better results
(+0.8%). Also, consistent with the results on MS-COCO,
the re-weighed scheme enjoys better performance than the
binary correlation matrix on VOC as well.

4.3.2 Ablation Studies

In this section, we perform ablation studies from four differ-
ent aspects, including the sensitivity of ML-GCN to different
types of word embeddings, effects of τ in correlation matrix
binarization, effects of p for correlation matrix re-weighting,
and the depths of GCN.

which serves as the inputs of the stacked GCNs for learn-
ing the object classiﬁers.
In this part, we evaluate the
performance of ML-GCN under other types popular word
representations. Speciﬁcally, we investigate four different
word embedding methods, including GloVe [25], Google-
News [24], FastText [13] and the simple one-hot word em-
bedding. Fig. 4 shows the results using different word em-
beddings on MS-COCO and VOC 2007. As shown, we can
see that when using different word embeddings as GCN’s
inputs, the multi-label recognition accuracy will not be af-
fected signiﬁcantly. In addition, the observations (especially
the results of one-hot) justify that the accuracy improvements
achieved by our method do not absolutely come from the
semantic meanings derived from word embeddings. Fur-
thermore, using powerful word embeddings could lead to
better performance. One possible reason may be that the
word embeddings [25, 24, 13] learned from large text corpus
maintain some semantic topology. That is, for semantic re-
lated concepts, their embeddings are close in the embedding
space. Our model can employ these implicit dependencies,
and further beneﬁt multi-label image recognition.

ML-GCN under different types of word embeddings
By default, we use Glove [25] as label representations,

Effects of different threshold values τ We vary the val-
ues of the threshold τ in Eq. (7) for correlation matrix bina-
rization, and show the results in Fig. 5. Note that, if we do

5182

100

)

%
(
e
c
n
a
m
r
o
f
r
e
P

80

60

40

20

MS-COCO

100

VOC

80

60

40

20

mAP

CF1

OF1

CF1-3

OF1-3

mAPmAP

Figure 4. Effects of different word embedding approaches. It is
clear to see that, different word embeddings will hardly affect the
accuracy, which reveals our improvements do not absolutely come
from the semantic meanings derived from word embeddings, rather
than our ML-GCN.

84

83

82

81

80

79

78

77

76

75

)

%

(
P
A
m

 mAP

95

94

93

92

91

90

89

88

87

86

85

)

%

(
P
A
m

 mAP

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Threshold

Threshold

(a) Comparisons on MS-COCO.
Figure 5. Accuracy comparisons with different values of τ .

(b) Comparisons on VOC 2007.

not ﬁlter any edges, the model will not converge. Thus, there
is no result for τ = 0 in that ﬁgure. As shown, when ﬁltering
out the edges of small probabilities (i.e., noisy edges), the
multi-label recognition accuracy is boosted. However, when
too many edges are ﬁltered out, the accuracy drops since
correlated neighbors will be ignored as well. The optimal
value of τ is 0.4 for both MS-COCO and VOC 2007.

Effects of different p for correlation matrix re-weighting
To explore the effects of different values of p in Eq. (8) on
multi-label classiﬁcation accuracy, we change the values of
p in a set of {0, 0.1, 0.2, . . . , 0.9, 1}, as depicted in Fig. 6.
Generally, this ﬁgure shows the importance of balancing
the weights between a node itself and the neighborhood
when updating the node feature in GCN. In experiments, we
choose the optimal value of p by cross-validations. We can
see that when p = 0.2, it can achieve the best performance
on both MS-COCO and VOC 2007. If p is too small, nodes
(labels) of the graph can not get sufﬁcient information from
correlated nodes (labels). While, if p is too large, it will lead
to over-smoothing.

Another interesting observation is that, when p = 0, we

85

80

75

70

65

)

%

(
P
A
m

 mAP

95

90

85

80

75

)

%

(
P
A
m

 mAP

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Proportion

Proportion

(a) Comparisons on MS-COCO.

(b) Comparisons on VOC 2007.

Figure 6. Accuracy comparisons with different values of p. Note
that, when p = 1, the model does not converge.

Table 3. Comparisons with different depths of GCN in our model.

♯ Layer

2-layer
3-layer
4-layer

mAP
83.0
82.1
81.1

MS-COCO

All
CF1
78.0
76.9
76.4

OF1
80.3
79.7
79.4

Top-3

CF1
74.6
73.7
72.5

OF1
76.7
76.2
75.8

VOC
All
mAP
94.0
93.6
93.0

can obtain mAPs of 81.67% on MS-COCO and 93.15% on
VOC 2007, which still outperforms existing methods. Note
that when p = 0, we essentially do not explicitly incorporate
the label correlations. The improvement is beneﬁted from
that our ML-GCN model learns the object classiﬁers from
the prior label representations through a shared GCN based
mapping function, which implicitly models label dependen-
cies as discussed in Sec. 3.1

The deeper, the better? We show the performance results
with different numbers of GCN layers for our model in Ta-
ble 3. For the three-layer model, the output dimensionalities
are 1024, 1024 and 2048 for the sequential layers, respec-
tively. For the four-layer model, the dimensionalities are
1024, 1024, 1024 and 2048. As shown, when the number of
graph convolution layers increases, multi-label recognition
performance drops on both datasets. The possible reason for
the performance drop may be that when using more GCN
layers, the propagation between nodes will be accumulated,
which can result in over-smoothing.

4.4. Classiﬁer Visualization

The effectiveness of our approach has been quantitatively
evaluated through comparisons to existing methods and de-
tailed ablation studies.
In this section, we visualize the
learned inter-dependent classiﬁers to show if meaningful
semantic topology can be maintained.

In Fig. 8, we adopt the t-SNE [23] to visualize the classi-
ﬁers learned by our proposed ML-GCN, as well the classi-
ﬁers learned through vanilla ResNet (i.e., parameters of the
last fully-connected layer). It is clear to see that, the classi-
ﬁers learned by our method maintain meaningful semantic

5183

Query

Our Method

Vanilla ResNet

Bicycle, Dog, Person

Bicycle, Dog

Person

Bicycle, Dog, 

Bicycle, Dog, 

Person, Backpack

Person, Backpack

Bicycle, Dog, 

Person, Backpack

Bicycle, Dog

Bicycle, Backpack,

Bicycle, Dog, 

Bicycle, Handbag, 

Bicycle,  Person, 

Car, Person, Stop Sign

Person, Backpack

Person

Traffic Light

Bicycle,  Person, 

Potted Plant

Person, Snowboard

Person, SnowboardPerson, Snowboard, 

Person, Snowboard Person, Snowboard

Person, Kite

Person, Snowboard

Kite

Person, Kite

Car

Stop Sign

(a)

(b)

(c)

Person, Remote

Person, Remote

Person, Remote

Person, Remote

Person, Remote

Person, Remote

Person, Cell Phone Person, Cell Phone

Person, Remote

Person, Cell Phone Person, Cell Phone

Figure 7. Top-5 returned images with the query image. The returned results on the left are based on our proposed ML-GCN, while the results
on the right are vanilla ResNet. All results are sorted in the ascending order according to the distance from the query image.

topology. Speciﬁcally, the learned classiﬁers exhibit cluster
patterns. Classiﬁers (of “car” and “truck”) within one
super concept (“transportation”), tend to be close in
the classiﬁer space. This is consistent with common sense,
which indicates that the classiﬁers learned by our approach
may not be limited to the dataset where the classiﬁers are
learned, but may enjoy generalization capacities. On the
contrary, the classiﬁers learned through vanilla ResNet uni-
formly distribute in the space and do not shown any mean-
ingful topology. This visualization further shows the effec-
tiveness of our approach in modeling label dependencies.

4.5. Performance on image retrieval

Apart from analyzing the learned classiﬁers, we further
evaluate if our model can learn better image representations.
We conduct an image retrieval experiment to verify this.
Speciﬁcally, we use the k-NN algorithm to perform content-
based image retrieval to validate the discriminative ability of
image representations learned by our model. Still, we choose
the features from vanilla ResNet as the baseline. We show
the top-5 images returned by k-NN. The retrieval results are
presented in Fig. 7. For each query image, the corresponding
returned images are sorted in the ascending order according
to the distance to the query image. We can clearly observe
that our retrieval results are obviously better than the vanilla
ResNet baseline. For example, in Fig. 7 (c), the labels of the
images returned by our approach almost exactly match the
labels of the query image. It can demonstrate that our ML-
GCN can not only effectively capture label dependencies to
learn better classiﬁers, but can beneﬁt image representation
learning as well in multi-label recognition.

5. Conclusion

Capturing label dependencies is one crucial issue for
multi-label image recognition. In order to model and explore
this important information, we proposed a GCN based model

snowboard     

skis          

vase          

potted plant  

chair         

dining table  

couch         

remote        

tv            

keyboard      

mouse         

laptop        

cow           

sheep         

bird          

cat           

dog           

book          

horse         

zebra         

elephant      

giraffe       

bear          

teddy bear    

tie           

wine glass    

bottle        

cup           

bed           

sink          

hair drier    

toilet        

toothbrush    

bowl          

spoon         

cake          

cell phone    

fork          

knife         

umbrella      

clock         

motorcycle    

bicycle       

skateboard    

orange        

apple         

banana        

carrot        

broccoli      

kite          

tennis racket 

sports ball   

baseball glove

baseball bat  

suitcase      

handbag       

backpack      

bench         

stop sign     

parking meter 

surfboard     

person        

fire hydrant  

bus           

car           

truck         

traffic light 

boat          

train         

frisbee       

airplane      

scissors      

toaster       

microwave     

refrigerator  

(a) t-SNE on the learned inter-dependent classiﬁers by our model.

donut         

hot dog       

sandwich      

pizza         

oven          

dog           

frisbee       

toilet        

bed           

book          

tie           

chair         

bird          

banana        

couch         

train         

teddy bear    

surfboard     

bear          

tv            

tennis racket 

pizza         

vase          

potted plant  

snowboard     

keyboard      

sink          

horse         

mouse         

person        

cat           

stop sign     

oven          

microwave     

parking meter 

cup           

wine glass    

sheep         

suitcase      

handbag       

orange        

fire hydrant  

backpack      

cell phone    

toaster       

hair drier    

scissors      

fork          

knife         

cake          

toothbrush    

spoon         

hot dog       

broccoli      

kite          

bench         

dining table  

baseball bat  

refrigerator  

baseball glove

sports ball   

giraffe       

sandwich      

apple         

carrot        

elephant      

bowl          

donut         

zebra         

umbrella      

remote        

bus           

skateboard    

laptop        

skis          

cow           

truck         

bottle        

clock         

traffic light 

airplane      

boat          

motorcycle    

bicycle       

car           

(b) t-SNE on the classiﬁers by the vanilla ResNet.

Transportation Kitchen

Animal

Food

Fruit

Washroom Living Room

Sport

Electric
Appliance

Person

Others

Figure 8. Visualization of the learned inter-dependent classiﬁers by
our model and vanillia classiﬁers of ResNet on MS-COCO.

to learn inter-dependent object classiﬁers from prior label
representations, e.g., word embeddings. To explicitly model
the label dependencies, we designed a novel re-weighted
scheme to construct the correlation matrix for GCN by bal-
ancing the weights between a node and its neighborhood for
node feature update. This scheme can effectively alleviate
over-ﬁtting and over-smoothing, which are two key factors
hampering the performance of GCN. Both quantitative and
qualitative results validated the advantages of our ML-GCN.

5184

References

[1] Shang-Fu Chen, Yi-Chen Chen, Chih-Kuan Yeh, and Yu-
Chiang Frank Wang. Order-Free RNN with visual attention
for multi-label classiﬁcation.
In AAAI, pages 6714–6721,
2018. 4, 5, 6

[2] Tianshui Chen, Zhouxia Wang, Guanbin Li, and Liang Lin.
Recurrent attentional reinforcement learning for multi-label
image recognition. In AAAI, pages 6730–6737, 2018. 5, 6

[3] Franc¸ois Chollet. Xception: Deep learning with depthwise
separable convolutions. In CVPR, pages 1251–1258, 2017. 2
[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. ImageNet: A large-scale hierarchical image database.
In CVPR, pages 248–255, 2009. 2, 5

[18] Xin Li, Feipeng Zhao, and Yuhong Guo. Multi-label image
classiﬁcation with a probabilistic label enhancement model.
In UAI, pages 1–10, 2014. 1, 2

[19] Yining Li, Chen Huang, Chen Change Loy, and Xiaoou Tang.
Human attribute recognition by deep hierarchical contexts. In
ECCV, pages 684–700, 2016. 1

[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
ECCV, pages 740–755, 2014. 2, 5

[21] L. Liu, P. Wang, C. Shen, L. Wang, A. v. d. Hengel, C. Wang,
and H. T. Shen. Compositional model based ﬁsher vector
coding for image classiﬁcation. IEEE TPAMI, 39:2335–2348,
2017. 1

[5] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (VOC) challenge. IJCV, 88(2):303–338, 2010. 2, 5

[22] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Recti-
ﬁer nonlinearities improve neural network acoustic models.
In ICML, pages 1–6, 2013. 3, 5

[6] Weifeng Ge, Sibei Yang, and Yizhou Yu. Multi-evidence
ﬁltering and fusion for multi-label classiﬁcation, object detec-
tion and semantic segmentation based on weakly supervised
learning. In CVPR, pages 1277–1286, 2018. 4, 5, 6

[7] Zongyuan Ge, Dwarikanath Mahapatra, Suman Sedai, and
Rajib Chakravorty. Chest X-rays classiﬁcation: A multi-label
and ﬁne-grained problem. arXiv preprint arXiv:1807.07247,
2018. 1

[8] Marian George and Christian Floerkemeier. Recognizing
products: A per-exemplar multi-label image classiﬁcation
approach. In ECCV, pages 440–455, 2014. 1

[9] Yunchao Gong, Yangqing Jia, Thomas Leung, Alexander To-
shev, and Sergey Ioffe. Deep convolutional ranking for multi-
label image annotation. arXiv preprint arXiv:1312.4894,
2013. 2

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR, pages
770–778, 2016. 1, 2, 4, 5, 6

[11] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation

networks. In CVPR, pages 7132–7141, 2018. 2

[12] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional networks.
In CVPR, pages 4700–4708, 2017. 1

[13] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs
Douze, H´erve J´egou, and Tomas Mikolov. Fasttext.zip:
Compressing text classiﬁcation models.
arXiv preprint
arXiv:1612.03651, 2016. 6

[14] Thomas N Kipf and Max Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. In ICLR, pages
1–10, 2017. 3

[15] Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, and Yu-
Chiang Frank Wang. Multi-label zero-shot learning with
structured knowledge graphs. In CVPR, pages 1576–1585,
2018. 2, 4, 5, 6

[16] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights
into graph convolutional networks for semi-supervised learn-
ing. In AAAI, pages 3538–3545, 2018. 5

[17] Qiang Li, Maoying Qiao, Wei Bian, and Dacheng Tao. Con-
ditional graphical lasso for multi-label image classiﬁcation.
In CVPR, pages 2977–2986, 2016. 1, 2

[23] Laurens van der Maaten and Geoffrey Hinton. Visualizing

data using t-sne. JMLR, 9(11):2579–2605, 2008. 7

[24] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
Efﬁcient estimation of word representations in vector space.
In ICLR, pages 1–12, 2013. 6

[25] Jeffrey Pennington, Richard Socher, and Christopher Man-
In

ning. GloVe: Global vectors for word representation.
EMNLP, pages 1532–1543, 2014. 5, 6

[26] Karen Simonyan and Andrew Zisserman. Very deep convolu-
tional networks for large-scale image recognition. In ICLR,
pages 1–8, 2015. 1, 6

[27] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, pages 2818–2826,
2016. 1

[28] Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang
Huang, and Wei Xu. CNN-RNN: A uniﬁed framework for
multi-label image classiﬁcation. In CVPR, pages 2285–2294,
2016. 1, 2, 5, 6

[29] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and
Liang Lin. Multi-label image recognition by recurrently dis-
covering attentional regions. In ICCV, pages 464–472, 2017.
2, 5, 6

[30] Xiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, and Lingqiao
Liu. RPC: A large-scale retail product checkout dataset. arXiv
preprint arXiv:1901.07249, 2019. 1

[31] Yunchao Wei, Wei Xia, Min Lin, Junshi Huang, Bingbing Ni,
Jian Dong, Yao Zhao, and Shuicheng Yan. HCP: A ﬂexible
cnn framework for multi-label image classiﬁcation. IEEE
TPAMI, 38(9):1901–1907, 2016. 6

[32] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In CVPR, pages 5987–5995, 2017. 2

[33] Hao Yang, Joey Tianyi Zhou, Yu Zhang, Bin-Bin Gao, Jianxin
Wu, and Jianfei Cai. Exploit bounding box annotations for
multi-label object recognition.
In CVPR, pages 280–288,
2016. 6

[34] Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, and Jian-
feng Lu. Multi-label image classiﬁcation with regional latent
semantic dependencies. IEEE TMM, 20(10):2801–2813, 2018.
6

5185

[35] Xiangyu Zhang, Xinyu Zhou, Mengxiao Li, and Jian Sun.
ShufﬂeNet: an extremely efﬁcient convolutional neural net-
work for mobile devices. In CVPR, pages 6848–6856, 2018.
2

[36] Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and
Xiaogang Wang. Learning spatial regularization with image-
level supervisions for multi-label image classiﬁcation.
In
CVPR, pages 5513–5522, 2017. 2, 4, 5, 6

5186

