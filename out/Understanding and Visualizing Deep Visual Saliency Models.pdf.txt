Understanding and Visualizing Deep Visual Saliency Models

Sen He1, Hamed R. Tavakoli2, Ali Borji3, Yang Mi1, and Nicolas Pugeault1

1University of Exeter, 2Aalto University, 3MarkableAI

Abstract

Table 1: Five state-of-the-art deep saliency models and their
NSS scores [6] over the MIT300 saliency benchmark [5].

Recently, data-driven deep saliency models have
achieved high performance and have outperformed classi-
cal saliency models, as demonstrated by results on datasets
such as the MIT300 and SALICON. Yet, there remains a
large gap between the performance of these models and
the inter-human baseline. Some outstanding questions in-
clude what have these models learned, how and where they
fail, and how they can be improved. This article attempts
to answer these questions by analyzing the representations
learned by individual neurons located at the intermediate
layers of deep saliency models. To this end, we follow the
steps of existing deep saliency models, that is borrowing a
pre-trained model of object recognition to encode the vi-
sual features and learning a decoder to infer the saliency.
We consider two cases when the encoder is used as a ﬁxed
feature extractor and when it is ﬁne-tuned, and compare
the inner representations of the network. To study how the
learned representations depend on the task, we ﬁne-tune the
same network using the same image set but for two differ-
ent tasks: saliency prediction versus scene classiﬁcation.
Our analyses reveal that: 1) some visual salient regions
(e.g. head, text, symbol, vehicle) are already encoded within
various layers of the network pre-trained for object recog-
nition, 2) using modern datasets, we ﬁnd that ﬁne-tuning
pre-trained models for saliency prediction makes them fa-
vor some categories (e.g. head) over some others (e.g. text),
3) although deep models of saliency outperform classical
models on natural images, the converse is true for synthetic
stimuli (e.g. pop-out search arrays), an evidence of signif-
icant difference between human and data-driven saliency
models, and 4) we conﬁrm that, after-ﬁne tuning, the change
in inner-representations is mostly due to the task and not the
domain shift in the data.

1. Introduction

The human visual system routinely handles vast amounts
of information at about 108 to 109 bits per second [2, 3, 4,

Model

Backbone

Fine
tuning

NSS

VGG-19 [23]

ResNet-50 [11]/VGG-16 √ 2.34/2.30

Deep gaze II [19]

SAM [8]

Deepﬁx [18]

SALICON [14]

PDP [16]
Human IO

VGG-16 [23]

VGG-16
VGG-16

-

×
√
√
√
-

2.34

2.26
2.12
2.05
3.29

15]. An essential mechanism that allows the human visual
system to process this amount of information in real time
is its ability to selectively focus attention on salient parts
of a scene. Which parts of a scene and what individual pat-
terns particularly attract the viewer’s eyes (e.g. salient areas)
have been the subject of psychological research for decades,
and designing computational models for predicting salient
areas is a longstanding problem in computer vision. In re-
cent years, we have observed a surge in the development of
data-driven models of saliency based on deep neural net-
works. Such deep models have demonstrated signiﬁcant
performance improvements in comparison to classical mod-
els, which are based on hand-crafted features or psycho-
logical assumptions, outperforming them on most bench-
marks. However, while there still remains a relatively large
gap between deep models and the human visual system (see
Table 1), the performance of deep models appears to have
reached a ceiling. This raises the question of what is learned
by deep models that drives their superior performance over
classical models, and what are the remaining and missing
ingredients to attain human-like performance. Internal rep-
resentations of deep object recognition models have been
visualized and analyzed extensively in recent years. Such
efforts, however, are missing for saliency models and it is
unclear how saliency models do what they do.

In this work, we shed light on what is learned by deep
saliency models by analyzing their internal representations.
Our contribution are as follows:

i10206

• We annotate 3 datasets for analyzing the relationship
between the deep model’s inner representation and the vi-
sual saliency in the image.

• A new dataset based on synthetic pop-out search arrays
is proposed to compare deep and classical saliency models.
• We investigate what and how saliency information is
encoded in a pre-trained deep model and look into the effect
of ﬁne-tuning on inner-representations of the deep saliency
models.

• Finally, we study the effect of the task type on the in-
ner representations of a deep model by comparing a model
ﬁne-tuned for saliency prediction with a model ﬁne-tuned
for scene recognition.

2. Related Work

2.1. Deep saliency models

The SALICON challenge [17], by offering the ﬁrst large
scale dataset for saliency, facilitated the development of
deep saliency models. Several such models learn a map-
ping from deep feature space to the saliency space, where
a pre-trained object recognition network acts as the feature
encoder. The encoder is then ﬁne-tuned for the saliency
task. For example, DeepNet [22] learns saliency using
8 convolutional layers, where only the ﬁrst 3 layers are
initialized from a pre-trained image classiﬁcation model.
PDP [16] treats the saliency map as a small scale proba-
bility map, and investigates different loss functions for gaze
prediction. They also suggested the use of Bhattacharyya
distance when the gaze map is treated as a small scale
probability map. The SALICON [14] model uses multi-
resolution inputs, and combines feature representations in
the deep layers for saliency prediction. Deepﬁx [18] com-
bines deep architectures of VGG, GoogleNet [24], and Di-
lated convolutions [29] in a network and adds a central
bias, to achieve a higher performance than previous mod-
els. SalGAN [21] uses an encoder-decoder architecture and
proposes the binary cross entropy (BCE) loss function to
perform pixel-wise (rather than image-wise) saliency esti-
mation. After pre-training the encoder-decoder, it uses a
Generative Adversarial Network (GAN) [9] to boost per-
formance. DVA [26] uses multiple layer’s representations,
builds a decoder for each layer, and fuses them at the ﬁ-
nal stage for pixel-wise gaze prediction. SAM [8] uses an
attention module and a LSTM [13] network to attend to dif-
ferent salient regions in the image. DeepGaze II [19] uses
the features at different layers of a pre-trained deep model
and combines them with the prior knowledge (i.e. center-
bias). DSCLRCN [20] uses multiple inputs by adding a
contextual information stream, and concatenates the orig-
inal representation and the contextual representation into a
LSTM network for the ﬁnal prediction.

2.2. Visualizing deep neural networks

The success of deep convolutional neural networks has
raised the question of what representations are learned by
neurons located in intermediate and deep layers. One ap-
proach towards understanding how CNNs work and learn
is to visualize individual neurons’ activations and receptive
ﬁelds. Zeiler and Fergus [30] proposed a deconvolution net-
work in order to visualize the original patterns that activate
the corresponding activation maps. A deconvolution net-
work consists of the three steps of unpooling, transposed
convolution, and the ReLU operation. Yosinski et al. [28]
developed two tools for understanding deep convolutional
neural networks. The ﬁrst of these tools is designed to visu-
alize the activation maps at different layers for a given input
image. The second tool aims to estimate the input pattern
which a network is maximally attuned to for a given object
class. In practice, the last layer of a deep neural network
typically consists of one neuron per object class. Yosinski
et al. proposed to use gradient ascent (with regularization)
to ﬁnd the input image that maximizes the output of a spe-
ciﬁc neuron in regard to a speciﬁc object class. Hence, they
derive the optimum input that appeals to the network for a
speciﬁc class.

Both visualization methods discussed above are essen-
tially qualitative.
In contrast, Bau et al. [1] proposed a
quantitative method to give each activation map a seman-
tic meaning.
In their work, they proposed a dataset with
6 image categories and 63,305 images for network dissec-
tion, where each image is labeled with pixel-wise semantic
meaning. At ﬁrst, they forward all images in the dataset into
a pre-trained deep model. For each activation map inside the
model, different inputs have different patterns. Then, they
compute the distribution of each unit activation map over
the whole dataset, and determine a threshold for each unit
based on its activation distribution. With the threshold for
each unit, the activation map for each input image is quan-
tized to a binary map. Finally, they compute the intersection
over union (IOU) between the quantized activation map and
the labeled ground truth to determine what objects or object
parts a unit is detecting.

The aforementioned approaches provide useful insight
into the internals of deep neural networks trained on Ima-
geNet for the classiﬁcation task. However, our understand-
ing of the internal representations of deep saliency predic-
tion models is somewhat limited. Bylinskii et al. [7] tried
to understand deep models for saliency prediction. But their
study was mostly focused on where models fails, rather than
how they compute saliency. To our best of knowledge, our
work is the ﬁrst to study the representations learned by deep
saliency models1.

1All the codes, data, models and other details in the paper can be found

at https://github.com/SenHe/uavdvsm

10207

(a) image

(b) OSIE

(c) OSIE-SR

Figure 1: (a) An example image from the OSIE dataset, (b)
OSIE annotation, and (c) the re-annotated OSIE-SR labels.

Figure 2: Example synthetic pop out search arrays

use this database to compare the effect of saliency predic-
tion and scene recognition on learned inner representations.

3. Data and Annotation

We ﬁrst introduce the data used in our experiments as

4. Methods

well as our proposed annotated dataset.

4.1. Modiﬁed NSS score

SALICON: SALICON [17] is the largest database for
saliency prediction at the moment. It contains 10, 000 train-
ing images, 5, 000 validation images and 5, 000 testing im-
ages. Here, we use it to ﬁne-tune a pre-trained model for
saliency prediction.

OSIE-SR: This acronym stands for the “OSIE Saliency
Re-annotated” (i.e. annotations of salient regions). The
original OSIE dataset [27] has 700 images with rich se-
mantics. It contains eye movements of 15 subjects for each
image recorded during free-viewing, and also has the an-
notated masks for objects in the image according to 12 at-
tributes. For our analysis, we extract clusters of ﬁxation
locations, called salient regions. We, then, manually an-
notate each salient region as belonging to one of the 12
saliency categories, including: person head, person part, an-
imal head, animal part, object, text, symbol, vehicle, food,
drink, plant, and other. Similar categories have been ex-
ploited in previous research (e.g. [7, 27]). Fig. 1 provides
an example where each annotated region has a label accord-
ing to its salient category. The re-annotated data is used to
measure the association between inner representations (acti-
vation maps) in the deep model (pre-trained and ﬁne-tuned)
and each salient category.

Synthetic Images: We selected 80 synthetic search ar-
rays [12], often used in pop-out experiments (Fig. 2). This
dataset contains various pop-out patterns where a target
stands out from the rest of the items in terms of color, ori-
entation, density, curvature, etc. We provide mask anno-
tation for the salient (i.e. pop-out) region in each image.
This database is used to compare deep models and classical
models on their ability to detect targets that pop-out in sim-
ple scenes. We also use it to study inner representations of
deep models over synthetic patterns.

SALICON-SAL-SCE: We select a subset of images
from the SALICON dataset and annotate each image with
a scene category label based on the categories of the Place-
CNN dataset [32]. We removed images belonging to the
scene categories with fewer than 50 images. Eventually, we
are left with 6,107 images with both ﬁxation maps and scene
category labels (26 categories in total, see supplement). We

We propose using normalized scanpath score (NSS)
within salient regions as a method to interpret the inner rep-
resentations. We, thus, can look into the association be-
tween the activation maps and the salient regions of the im-
age for analyzing the inner representations of the deep vi-
sual saliency models. To implement this, we ﬁrst forward
each image into the deep model and extract the activation
maps from different layers in the model. Then, the associ-
ation between each activation map (actm) and each salient
region (sr) in the image is computed as:

Assoc(actmij , srlk) = N SS(actmij , f ixl · masklk)

(1)

where actmij is the jth activation map from the ith layer for
the input lth image, and srlk is the kth salient region in the
lth image. f ixl is the ﬁxation on the lth image, and masklk
is the annotated polygon mask in lth image for region k.
It is worth noting that all activation maps were normalized
and reshaped to the size of the input image.

4.2. Saliency model

For our analysis, we develop a saliency model using the
convolutional part of VGG-16 (conv1-1 to conv5-3) and a
simple 1×1 convolutional layer on top of the conv5-3 layer.
The model has a single resolution with input of size 224 ×
224 and is optimized with −N SS as the loss function. We
consider 2 setups to analyze the representations.

• Setup I: We ﬁrst look into the inner representations
without ﬁne-tuning the VGG part, i.e. we only learn the
last 1 × 1 convolution layer, motivated by the performance
of some of the existing models that achieve state-of-the-art
performance without ﬁne-tuning. This can be observed in
Table 1. In other words, we analyze what types of saliency
information exist in the pre-trained VGG model and how it
is distributed within different layers. In this setup, we an-
alyze conv4-1 to conv5-3 layers, which correspond to the
last two blocks in VGG-16. The activation maps from lay-
ers below conv4-1 are sensitive to edge-like patterns and do
not correspond to annotated regions. We, thus, do not in-
clude them.

10208

Inner representations in the pre-trained visual
Table 2:
saliency model (setup I; i.e. training the 1 × 1 convolution
layer) at different layers for all types of saliency categories
(Please see text for details).

Inner representations in the last convolutional
Table 3:
layer (conv5-3) before and after ﬁne-tuning for all types of
saliency categories. 0 in number of ﬁne tuned layers indi-
cate setup I, otherwise setup II.

r
e
y
a
l

d
a
e
h
n
o
s
r
e
p

t
r
a
p
n
o
s
r
e
p

d
a
e
h

l
a
m
i
n
a

t
r
a
p

l
a
m
i
n
a

t
c
e
j
b
o

t
x
e
t

l
o
b
m
y
s

e
l
c
i
h
e
v

d
o
o
f

t
n
a
l
p

k
n
i
r
d

r
e
h
t
o

d
e
n
u
t

s
r
e
y
a
l

#

d
a
e
h
n
o
s
r
e
p

t
r
a
p
n
o
s
r
e
p

d
a
e
h

l
a
m
i
n
a

t
r
a
p

l
a
m
i
n
a

t
c
e
j
b
o

t
x
e
t

l
o
b
m
y
s

e
l
c
i
h
e
v

d
o
o
f

t
n
a
l
p

k
n
i
r
d

r
e
h
t
o

mean NSS for top 10 activation maps

mean NSS for top 10 activation maps

conv5-3 1.21 0.98 2.04 1.6 0.69 1.1 1.07 1.53 1.25 1.21 1.48 0.57
conv5-2 2.57 1.64 2.89 1.91 1.07 1.59 1.91 2.18 1.59 2.05 2.13 0.92
conv5-1 2.95 1.58 2.6 1.67 1.16 1.91 2.06 2.48 1.8 1.96 1.85
conv4-3 3.46 1.67 2.93 1.5 1.27 2.4 2.37 2.71 1.54 1.62 1.88 0.91
conv4-2 2.85 1.78 2.63 1.39 1.19 2.04 2.05 2.33 1.48 1.59 1.53 0.74
conv4-1 2.08 1.56 1.89 1.18 1.11 2.17 1.99 1.93 1.38 1.58 1.33 0.72

1

# activation maps above threshold (T = 1.5)

conv5-3
conv5-2
conv5-1
conv4-3
conv4-2
conv4-1

2
41
27
35
35
23

1
11
7
9
12
5

21
50
35
40
18
18

7
21
7
4
2
0

0
0
0
1
0
0

2
5
9
12
11
18

1
19
16
20
14
17

4
33
30
30
23
21

1
8
9
3
4
3

2
12
11
5
6
5

4
15
23
13
4
2

0
0
0
0
0
0

• Setup II: We then look into the ﬁne-tuned model. In
this setup, we learn the last 1 × 1 convolution layer and
ﬁne-tune the VGG part of the model for different number of
layers (each time from scratch) and examine the responses
of neurons in the conv5-3 layer.

4.3. Local saliency statistics

In the OSIE-SR dataset, each region corresponds to one
saliency category (i.e. has one label). To compute the statis-
tics for saliency category c in each layer, we compute the
mean value of the top 10 activation maps with high mean
NSS values in Eq. (1) for all the regions of category c. We
also compute the number of activation maps whose mean
NSS value is above a threshold (T) in Eq. (1) for all regions
for category c.

1.21 0.98 2.04 1.6 0.69 1.1 1.07 1.53 1.25 1.21 1.48 0.57
0
2.61 1.63 2.44 1.7 1.31 1.17 1.56 2.21 1.97 1.89 1.99 1.09
1
3.25 1.77 2.75 1.78 1.25 1.32 1.56 1.91 1.89 1.66 1.65 1.12
2
3.28 1.78 3.04 1.79 1.28 1.34 1.57 1.94 1.96 1.81 1.68 1.2
3
3.38 1.83 3.08 1.78 1.24 1.25 1.47 1.97 1.87 1.73 1.63 1.12
4
3.32 1.78 2.84 1.82 1.28 1.38 1.59 2.1 1.95 1.75 1.77 1.17
5
3.08 1.83 2.57 1.78 1.25 1.25 1.45 2.03 1.99 1.8 1.66 1.11
6
all 2.85 1.75 2.36 1.65 1.14 1.2 1.41 1.72 1.84 1.71 1.33 1.07

0
1
2
3
4
5
6
all

2
30
35
43
53
71
56
27

1
11
11
16
18
28
30

11

2

# activation maps above threshold (T = 1.5)
21
31
36
54
63
77
68
27

7
11
15
21
19
37
27
10

1
6
7
10
1
25
0
0

4
15
19
17
25
46
38
14

1
10
11
16
14
24
27

2
11
8
14
16
20
23

0
0
0
0
0
0
0
0

0
0
0
0
0
0
0

11

11

4
12
11
16
16
28
25
0

0
1
0
0
0
0
0
0

I). From Table 2, we can see that many visual saliency cat-
egories, including: person head, animal head, text, sym-
bol, vehicle and drink, have been encoded in the pre-trained
CNN features. We observe not only high mean NSS scores,
but also a large number of active response maps for each
saliency category. We also observe that the visual saliency
information is encoded within various layers, e.g. person
head, animal head, and text are present in conv4-3. Fig. 3
visualizes some examples of the activation maps in the
model without ﬁne-tuning the VGG features. As depicted,
there is a relatively high association between salient regions
and activation maps.

5. Analysis of Learned Representations

5.2. Saliency representation after ﬁne tuning

How does ﬁne-tuning the VGG neurons for saliency pre-
diction affect the network inner representation? To answer
this question, we train two saliency models, one keeping the
CNN features ﬁxed during the training and the other one
ﬁne-tuning the CNN features in conjunction with the 1 × 1
convolution layer for saliency prediction (corresponding to
the two setups mentioned above).

5.1. Saliency representation before ﬁne tuning

Table 2 depicts the statistics of the inner representations
within different layers of the deep visual saliency model,
where the convolution part has not been ﬁne-tuned (setup

Table 3 reports the statistics of visual saliency in layer
conv5-3, which is directly used for saliency prediction, af-
ter ﬁne-tuning different number of layers in the model (0
indicates that only the ﬁnal 1 × 1 convolutional layer above
the pre-trained model for regression was trained on saliency
data, without ﬁne-tuning the pre-trained model; 1 means
ﬁne-tuning the layer conv5-3, 2 means ﬁne-tuning the lay-
ers conv5-3 and conv5-2, and so on).

From Table 3, we can see that after ﬁne-tuning, the acti-
vation maps became more selective to visual saliency, as the
mean NSS values improve for all saliency categories. The
improvements are, however, uneven as can be seen in Fig. 4;

10209

Figure 3: An example of the pre-trained model inner representation (Setup I). Top row: the original image, and the activation
maps with highest activation at salient regions from layer conv5-1. Bottom row: the image with masked salient regions
(1,2,3,4) and the activation maps that best respond to the salient regions of conv4-3.

Figure 4: Top 10 mean NSS improvements for each cate-
gory when ﬁne-tuning different numbers of layers

Figure 5: The 288th activation map in layer conv5-3 be-
fore and after ﬁne-tuning for an input image. Notice that it
becomes attuned to person head after ﬁne-tuning.

(a)

(b)

(c)

(d)

Figure 6: The 512th activation map in layer conv5-3 before
and after ﬁne-tuning (tuning 3 layers).
(a) the input im-
age, image overlapped with the activation map before ﬁne-
tuning (b), after ﬁne-tuning (c), and the activation map after
ﬁne-tuning (d). After ﬁne-tuning, despite the fact that this
activation map has the highest mean NSS score for all re-
gions annotated as text in the dataset, it still favors heads.

i.e. some categories improve more. For example, person
head improves the most from 1.21 to 3.38 when ﬁne-tuning
4 layers (see Fig. 5), while for text, the improvement is rel-
atively small (at most from 1.1 to 1.38 when ﬁne-tuning 5
layers). Similar effects can be seen in Fig. 6, where activa-
tion maps which are most selective to texture regions in the
image after ﬁne-tuning still respond to head regions in the
image, and with high activation values. It is withstanding
that an ANOVA test and multiple comparisons indicate per-
son head, animal head, and other are signiﬁcantly different
than all other categories and each other.

An interesting observation is that by ﬁne-tuning more
layers, i.e. more than 3 layers (and especially when ﬁne-
tuning all the layers), the mean NSS values or the number
of activation maps for some saliency categories start to de-
crease. We also do not gain more saliency prediction im-
provement by ﬁne-tuning more layers. We speculate that
one reason behind this observation might be the quality and
quantity of the current saliency data, which is biased to-
wards speciﬁc salient objects and regions.

10210

1234123456allnumber of layers fine-tuned050100150200improvement (percentage)person headotherobjectperson partfoodplantsymbolvehicledrinkanimal headtextanimal headwhere, fcj (predi) = N SS(predi, maski,cj
GTi is the ground truth saliency map.

· f ixi), and

The results are shown in Fig. 7. As depicted in the top
left of this ﬁgure, the model’s saliency output is correlated
to the saliency of inner representations. In other words, we
can see that if a category is more salient in the model’s in-
ner representation, it is also more salient in the model’s out-
put (Spearman’s correlation coefﬁcient: rs = 0.96). The
salient categories in inner representations and output, how-
ever, migh be different. The output salience category dif-
ference is lower for less salient inner categories and higher
for more salient inner ones. For example, as depicted in
top right panel of Fig. 7, the head category has more salient
inner representations and higher output salient category dif-
ference (ANOVA test with measurements and clusters as
factors showing signiﬁcant difference p = 9e−9 < 0.05).
In other words, the model has learned to ﬁre on faces, ir-
respective of whether they are salient in the context of the
given image.

Figure 7: Top row: the relationship between the inner repre-
sentation saliency and output saliency (left) and the output
prediction difference (right). Bottom row: an example im-
age, ground truth ﬁxation map, and model prediction.

6. The Relationship between Intermediate and

Final Representations

7. Model Performance and Representations

What is the relation between a model’s inner representa-
tion and its output? In other words, are the categories that
are more salient in the inner representations also more ac-
tive in the output saliency map? To what extend the salient
categories within the inner representations agree with the
ground truth salient categories? To answer these questions,
we deﬁne the the inner saliency for each category as the
mean NSS value of the top 10 activation maps for it, which
is the same as the values in Table 3. Following steps of [25]
for ﬁne-grained contextual analysis, we assign the output
saliency of a model, deﬁned as OSc for category c, as the
output’s mean NSS score of all salient regions in the OSIE-
SR dataset that belong to category c,

OSc =

1
A

N

M

X

X

i=1

j=1

N SS(predi, maski,cj

· f ixi)

(2)

where, A is the total number of salient regions for category
c, N is the total number of images, and M is total number
of regions for category c in ith image, predi is the model
prediction for ith image, maski,cj is the annotated mask
for the jth region in ith image for category c (if category c
exists in the image) and f ixi is the ﬁxation location on the
ith image.

To measure the relation between salient categories and
representations, we deﬁne the concept of output salient cat-
egory difference, denoted as ODc. It measures the mean
difference of the NSS score between a model prediction and
the ground truth with respect to the salient categories,

ODc =

1
A

A

M

X

X

i=1

j=1

|fcj (predi) − fcj (GTi)|

(3)

over Synthetic Search Arrays

How do deep saliency models perform over the synthetic
images? We compare the performance of deep and classical
saliency models on a set of synthetic images. These images
are designed to simulate the feature pop-out, and have been
extensively used to study human attention, but they have
not been considered for evaluating deep saliency models.
There exists no eye-ﬁxation on such images, but is it easy
to locate the target/salient item in the array (in fact we man-
ually labeled the images). we assess the performance of the
models using Normalized Mean value under the annotated
Mask (NMM) for each image:

N M Mi = meanmaski (

predi − µ(predi)

σ(predi)

)

(4)

where maski and predi are the annotated mask and model
prediction for the ith image, respectively. Table 4 shows

Table 4: Performance comparison between deep models
and classical models on synthetic images in terms of NMM.

Deep models

Classical models

DeepGaze II [19] SAM [8] DVA [26] GBVS [10] BMS [31]

1.66

1.25

1.19

2.57

3.65

the performance of deep and classic models on the synthetic
images. Surprisingly, although deep models achieve state-
of-the-art performance on the MIT300 benchmark, they
completely fail on synthetic images and are outclassed by
classical models, moreover, despite the DVA [26] has con-
nections between shallow layer and output layer in their

10211

1.01.52.02.53.03.5Inner saliency0.51.01.52.02.53.03.54.04.55.0Prediction saliencyInner saliency to prediction saliency1.01.52.02.53.03.5Inner saliency0.81.01.21.41.61.82.0Prediction differenceHead(person and animal)text,symbol,etcInner saliency to prediction differenceFigure 8: Some qualitative examples for deep and classical
models on synthetic images, from left to right: image, pre-
dictions from DeepGaze II, SAM, DVA, GBVS, and BMS.

model, their performance is still not good on synthetic im-
ages. Fig. 8 illustrates saliency predictions on example
stimuli by the considered methods. To investigate the rea-
son behind this failure, we looked into the effect of ﬁne-
tuning on the inner representations and neuron responses to
synthetic patterns. The results in Table 5 show that deep
models do indeed capture the salient patterns within the
middle layers of the architecture (e.g. conv4-1 layer). Some
examples (including curvature, orientation, etc) are shown
in Fig. 9. Nevertheless, as indicated in Table 6, no mat-
ter how many layers are ﬁne-tuned, the output of the deep
saliency model never highlights such salient patterns. One
possible reason might be that the current large databases,
e.g. SALICON, are biased towards natural scenes contain-
ing daily objects (text, faces, animals, cars, etc) and do not
include any image similar to synthetic patters. The models,
thus, do not learn anything about simple pop-out.

Table 5: The mean NMM for top 10 activation maps in each
layer (from conv4-1 to conv5-3) in the pre-trained model for
synthetic images.

conv5-3 conv5-2 conv5-1 conv4-3 conv4-2 conv4-1

0.38

0.94

1.47

1.96

1.94

2.12

Figure 9: Top row: synthetic search arrays with their anno-
tated masks. Bottom row: the activation maps from layer
conv4-1 that best correlated with the masked regions in the
synthetic images.

Table 6: The mean NMM score for top 10 activation maps
in the last convolutional layer for synthetic images, when
ﬁne-tuning different number of layers.

# layers ﬁne-tuned

0

0.38

1
0.8

2

3

4

5

6

0.98

0.92

0.65

0.86

0.83

all
0.73

Figure 10: Our architecture to study task-dependency of
representations (saliency prediction vs.
scene classiﬁca-
tion). We use the same data to ﬁne-tune the pre-trained
model for different tasks.

8. The Inﬂuence of Task on the Learned Rep-

resentations

What is the driving cause for the observed change in rep-
resentations after ﬁne-tuning in previous sections? Is it due
to the network being ﬁne-tuned to a new task (saliency pre-
diction) or is it the network being ﬁne-tuned to a different
set of data (images from a saliency prediction dataset)? To
ﬁgure out, we compare two tasks of saliency prediction and
scene recognition on SALICON-SAL-SCE, which provides
saliency information and scene type labels. Note that in
both cases, the images used for ﬁne-tuning are the same,
therefore if the observed shift in representation is only due
to the data, the inner representations should be similar in

both tasks; conversely, if the task is what is driving the
change, the representations should differ.

We check three CNN trunks, including, 1) a pre-traind
CNN based on VGG network for scene recognition (pt), 2)
a CNN ﬁne-tuned for saliency prediction (sp), and 3) a CNN
ﬁne-tuned for scene recognition (sr). The saliency predic-
tion is the same as explained above. The scene recognition
network consists of the VGG and 3 fully connected layers
(see Fig. 10). Due to the data limitation, we only ﬁne-tuned
1 layer of the pre-trained model (layer conv5-3) for both
tasks, weight balance was used when ﬁne-tuning the scene
recognition model to compensate imbalanced categories.

Table 7 shows how the inner representation changes with

10212

Ski ResortVGG-16conv1-1 to conv5-3Input ImageVGG-16conv1-1 to conv5-3Fully Connected Layer1 by 1 Convolutional LayerTable 7: Inner representation for different tasks (saliency
prediction and scene recognition) before and after ﬁne-
tuning (pt: pre-traind CNN based on VGG network scene
recognition, sp: CNN ﬁne-tuned for saliency prediction, sr:
CNN ﬁne-tuned for scene recognition).

d
a
e
h
n
o
s
r
e
p

t
r
a
p
n
o
s
r
e
p

d
a
e
h

l
a
m
i
n
a

t
r
a
p

l
a
m
i
n
a

t
c
e
j
b
o

t
x
e
t

l
o
b
m
y
s

e
l
c
i
h
e
v

d
o
o
f

t
n
a
l
p

k
n
i
r
d

r
e
h
t
o

mean NSS for top 10 activation maps

pt 1.21 0.98 2.04 1.6 0.69 1.1 1.07 1.53 1.25 1.21 1.48 0.57

sp 2.51 1.6 2.32 1.7

1.3 1.44 1.74 2.29 1.86 1.58 2.01 1.13

sr 0.47 0.57 1.54 1.24 0.57 0.83 0.61 0.88 0.91 0.97 1.28 0.41

# activation maps above threshold (T = 1.5)

pt

sp

sr

2

44

0

1

6

0

21

27

4

7

10

1

0

1

0

2

2

0

1

10

0

4

19

0

1

16

0

2

7

2

4

1

3

0

0

0

respect to saliency categories, once the CNN is ﬁne-tuned
for different tasks using the same data. The results show that
ﬁne-tuning for saliency prediction drives the inner represen-
tations to became more selective to salient categories, while
ﬁne-tuning for scene recognition leads to less selectivity to
salient categories and inhibition of some other salient re-
gions. Examples of the activation map change for each task
is provided in Fig. 11.

Figure 11: From left to right: Original image, image over-
lapped with the ground-truth ﬁxation map, overlapped with
the activation map by the pre-trained model, overlapped
with the activation map after ﬁne-tuning for scene recog-
nition, overlapped with the activation map after ﬁne-tuning
for saliency prediction.

Table 8: The NSS scores of mean activation maps for cor-
rect and wrong prediction in scene recognition task.

correct prediction wrong prediction

0.12

0.11

human may ﬁnd salient? We investigate this by computing
the NSS score between the attention of model (the mean of
512 activation maps in layer conv5-3) and the human ﬁxa-
tion on the image. The results are summarized in Table 8,
showing that the NSS score is small irrespective of whether
the model’s prediction is correct or not. In other words, the
model’s attention in scene recognition is different from hu-
man attention in free-viewing.

To summarize, the above results indicate that the inner
representations, during ﬁne-tuning the same CNN for the
two different tasks of saliency prediction and scene recog-
nition on the same data, mostly change because of the task
and not the data.

9. Discussion and Conclusion

In this work, we analyzed the internals of deep saliency
models. To this end, we annotated 3 datasets and conducted
several experiments to unveil the secrets of deep saliency
models. Our analysis on this data revealed that a deep neural
network pre-trained for image recognition already encodes
some visual saliency in the image. Fine-tuning this pre-
trained model for saliency prediction produces a model with
uneven response to saliency categories, e.g. neurons sensi-
tive to textual input start attending more to human head.
We showed that although deep models do capture synthetic
pop-out stimuli within their inner layers, they fail to pre-
dict such salient patterns in their output, contrary to classi-
cal models of saliency prediction. We also conﬁrmed that
the observed change in the inner representations after ﬁne-
tuning is mainly due to ﬁne-tuning for the task and not the
data. In our study, ﬁne-tuning the model for saliency pre-
diction resulted in more selective responses to salient re-
gions, though uneven. On the other hand, ﬁne-tuning the
model for scene recognition had inhibitory effect and the
inner representations were losing their selectivity to some
of the existing salient patterns.

To conclude, pushing the development of better data-
driven deep visual saliency models further may require del-
icate attention to the diversity of salient categories within
images. In other words, we may need not only a large scale
dataset, but also a dataset with rich saliency categories to
ensure generalization.

Acknowledgements

This research was supported by the EPSRC project
DEVA EP/N035399/1. Dr Pugeault acknowledges fund-
ing from the Alan Turing Institute (EP/N510129/1). H. R.
Tavakoli acknowledges NVIDIA for the donation of GPUs
used in his research.

To what degree the inner representations in the scene
recognition network is consistent with human attention?
Does the scene recognition model attend to the locations a

References

[1] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and
Antonio Torralba. Network dissection: Quantifying inter-

10213

pretability of deep visual representations. In Computer Vi-
sion and Pattern Recognition, 2017.

[2] Ali Borji. Saliency prediction in the deep learning era: An
empirical investigation. arXiv preprint arXiv:1810.03716,
2018.

[3] Ali Borji and Laurent Itti. State-of-the-art in visual atten-
IEEE transactions on pattern analysis and

tion modeling.
machine intelligence, 35(1):185–207, 2013.

[4] Ali Borji, Dicky N Sihite, and Laurent Itti. Quantitative anal-
ysis of human-model agreement in visual saliency modeling:
A comparative study. IEEE Transactions on Image Process-
ing, 22(1):55–69, 2013.

[5] Zoya Bylinskii, Tilke Judd, Ali Borji, Laurent Itti, Fr´edo Du-
rand, Aude Oliva, and Antonio Torralba. Mit saliency bench-
mark.

[6] Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba,
and Fr´edo Durand. What do different evaluation metrics tell
us about saliency models?
IEEE transactions on pattern
analysis and machine intelligence, 2018.

[7] Zoya Bylinskii, Adri`a Recasens, Ali Borji, Aude Oliva, An-
tonio Torralba, and Fr´edo Durand. Where should saliency
models look next?
In European Conference on Computer
Vision, pages 809–824. Springer, 2016.

[8] Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita
Cucchiara. Predicting Human Eye Fixations via an LSTM-
based Saliency Attentive Model. IEEE Transactions on Im-
age Processing, 27(10):5142–5154, 2018.

[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014.

[10] Jonathan Harel, Christof Koch, and Pietro Perona. Graph-
In Advances in neural information

based visual saliency.
processing systems, pages 545–552, 2007.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[12] Christopher Healey and James Enns. Attention and visual
memory in visualization and computer graphics. IEEE trans-
actions on visualization and computer graphics, 18(7):1170–
1188, 2012.

[13] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term

memory. Neural computation, 9(8):1735–1780, 1997.

[14] Xun Huang, Chengyao Shen, Xavier Boix, and Qi Zhao. Sal-
icon: Reducing the semantic gap in saliency prediction by
adapting deep neural networks. In Proceedings of the IEEE
International Conference on Computer Vision, pages 262–
270, 2015.

[15] Laurent Itti, Christof Koch, and Ernst Niebur. A model
of saliency-based visual attention for rapid scene analysis.
IEEE Transactions on Pattern Analysis & Machine Intelli-
gence, (11):1254–1259, 1998.

[16] Saumya Jetley, Naila Murray, and Eleonora Vig. End-to-
end saliency mapping via probability distribution prediction.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5753–5761, 2016.

[17] Ming Jiang, Shengsheng Huang, Juanyong Duan, and Qi
Zhao. Salicon: Saliency in context. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1072–1080, 2015.

[18] Srinivas SS Kruthiventi, Kumar Ayush, and R Venkatesh
Babu. Deepﬁx: A fully convolutional neural network for
predicting human eye ﬁxations. IEEE Transactions on Im-
age Processing, 26(9):4446–4456, 2017.

[19] Matthias K¨ummerer, Thomas SA Wallis, Leon A Gatys, and
Matthias Bethge. Understanding low-and high-level contri-
butions to ﬁxation prediction.
In 2017 IEEE International
Conference on Computer Vision, pages 4799–4808, 2017.

[20] Nian Liu and Junwei Han. A deep spatial contextual long-
term recurrent convolutional network for saliency detection.
IEEE Transactions on Image Processing, 27(7):3264–3274,
2018.

[21] Junting Pan, Cristian Canton Ferrer, Kevin McGuinness,
Noel E O’Connor, Jordi Torres, Elisa Sayrol, and Xavier
Giro-i Nieto. Salgan: Visual saliency prediction with genera-
tive adversarial networks. arXiv preprint arXiv:1701.01081,
2017.

[22] Junting Pan, Elisa Sayrol, Xavier Giro-i Nieto, Kevin
McGuinness, and Noel E O’Connor. Shallow and deep con-
volutional networks for saliency prediction. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 598–606, 2016.

[23] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[24] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1–9, 2015.

[25] Hamed R. Tavakoli, Fawad Ahmed, Ali Borji, and Jorma
Laaksonen. Saliency revisited: Analysis of mouse move-
ments versus ﬁxations.
In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2017.

[26] Wenguan Wang and Jianbing Shen. Deep visual attention
IEEE Trans. Image Process, 27(5):2368–2378,

prediction.
2018.

[27] Juan Xu, Ming Jiang, Shuo Wang, Mohan S Kankanhalli,
and Qi Zhao. Predicting human gaze beyond pixels. Journal
of vision, 14(1):28–28, 2014.

[28] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and
Hod Lipson. Understanding neural networks through deep
visualization. arXiv preprint arXiv:1506.06579, 2015.

[29] Fisher Yu and Vladlen Koltun.

aggregation by dilated convolutions.
arXiv:1511.07122, 2015.

Multi-scale context
arXiv preprint

[30] Matthew D Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In European conference on
computer vision, pages 818–833. Springer, 2014.

[31] Jianming Zhang and Stan Sclaroff. Saliency detection: A
boolean map approach. In Proceedings of the IEEE interna-
tional conference on computer vision, pages 153–160, 2013.

10214

[32] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 2017.

10215

