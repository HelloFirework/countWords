Graph Attention Convolution for Point Cloud Semantic Segmentation

Lei Wang1, Yuchun Huang1 ∗, Yaolin Hou1, Shenman Zhang1, Jie Shan2∗

1Wuhan University, China

2Purdue University, USA

{wlei, hycwhu, houyaolin, smzhang}@whu.edu.cn, jshan@purdue.edu

Abstract

Standard convolution is inherently limited for semantic
segmentation of point cloud due to its isotropy about fea-
tures. It neglects the structure of an object, results in poor
object delineation and small spurious regions in the seg-
mentation result. This paper proposes a novel graph at-
tention convolution (GAC), whose kernels can be dynami-
cally carved into speciﬁc shapes to adapt to the structure
of an object. Speciﬁcally, by assigning proper attentional
weights to different neighboring points, GAC is designed to
selectively focus on the most relevant part of them accord-
ing to their dynamically learned features. The shape of the
convolution kernel is then determined by the learned dis-
tribution of the attentional weights. Though simple, GAC
can capture the structured features of point clouds for ﬁne-
grained segmentation and avoid feature contamination be-
tween objects. Theoretically, we provided a thorough anal-
ysis on the expressive capabilities of GAC to show how it
can learn about the features of point clouds. Empirically,
we evaluated the proposed GAC on challenging indoor and
outdoor datasets and achieved the state-of-the-art results in
both scenarios.

1. Introduction

Semantic segmentation of point clouds aims to assign
a category label to each point, which is an important yet
challenging task for 3D understanding. Recent approaches
have attempted to generalize convolutional neural network
(CNN) from grid domains (i.e., speech signals, images, and
video data) to unorganized point clouds [34, 45, 33, 35, 44,
36, 26, 14]. However, due to the isotropy of their convolu-
tion kernels about the neighboring points’ feature attributes,
these works are inherently limited for the semantic point
cloud segmentation. Intuitively, the learned features for the
points at the boundary of two objects (i.e., point 1 in Fig-
ure 1) are actually from both objects rather than the object
they truly belong to, which results in ambiguous label as-

∗Corresponding author.

5

6

4

2

3

1

table
6

table

5

table

4

chair

2

1
table

chair

3

Figure 1. Illustration of the standard convolution and GAC on a
subgraph of a point cloud. Left: The weights of standard convo-
lution are determined by the neighbors’ spatial positions, and the
learned feature at point 1 characterizes all of its neighbors indistin-
guishably. Right: In GAC, the attentional weights on “chair” (the
brown dotted arrows) are masked, so that the convolution kernel
can focus on the “table” points.

signment.

In fact, standard convolution kernels work in a regular
receptive ﬁeld for feature response, and the convolution
weights are ﬁxed at speciﬁc positions within the convolu-
tion window. This kind of position-determined weights re-
sults in the isotropy of the convolution kernel about the fea-
ture attributes of neighboring points. For instance, in Fig-
ure 1 the learned feature at point 1 characterizes its neigh-
boring “table” and “chair” indistinguishably. This limita-
tion of the standard convolution neglects the structural con-
nection between points belonging to the same object, and
results in poor object delineation and small spurious regions
in the segmentation result.

To address this problem, the key idea of this work is as
follows. Based on the position-determined weights of the
standard convolution, we learn to mask or weaken part of
the convolution weights according to the neighbors’ feature
attributes, so that the actual receptive ﬁeld of our convolu-
tion kernel for point clouds is no longer a regular 3D box
but has its own shape to dynamically adapt to the structure
of the objects.

In this paper, we implement this idea by proposing a
novel GAC to selectively focus on the most relevant part
of the neighbors in the receptive ﬁeld. Speciﬁcally, inspired
by the idea of attention mechanism [4, 13, 47], GAC is de-
signed to dynamically assign proper attentional weights to
different neighboring points by combining their spatial po-

10296

sitions and feature attributes. The shape of the convolution
kernel is then determined by the learned distribution of the
attentional weights.

Finally, like the standard convolution in grid domain, our
GAC can also be efﬁciently implemented on the graph rep-
resentation of a point cloud. Referring to image segmenta-
tion network, we train an end-to-end graph attention con-
volution network (GACNet) with the proposed GAC for se-
mantic point cloud segmentation.

Notably, postprocessing of CNN’s outputs using condi-
tional random ﬁeld (CRF) has practically become a de facto
standard in semantic segmentation [45, 5, 9, 2]. However,
by combining the spatial and feature constraints for atten-
tional weights generation, GAC shares the same proper-
ties as CRF, which encourages the label agreement between
similar points. Thus, CRF is no longer needed in the pro-
posed GACNet.

Our contributions are as follows:
• We propose a novel graph attention convolution with
learnable kernel shapes to dynamically adapt to the
structure of the objects;

• We provide thorough theoretical and empirical analy-
ses on the capability and effectiveness of the proposed
graph attention convolution;

• We train an end-to-end graph attention convolution
network for point cloud semantic segmentation with
the proposed GAC and experimentally demonstrate its
effectiveness.

2. Related Works

This section will discuss the related prior works in three
main aspects: deep learning on point clouds, convolution on
graphs, and CRF in deep learning.

Deep learning on point clouds. While deep learning
has been successfully used in 2D images, there are still
many challenges to exploring its feature learning power
for 3D point clouds with irregular data structures. Re-
cent researches on this issue can be mainly summarized
as voxelization-based [25, 49], multi-view-based [43, 24],
graph-based [7, 51, 42] and set-based methods [33, 35].

The voxelization-based method [50, 30] aims to dis-
cretize the point cloud space into regular volumetric oc-
cupancy grids, so that the 3D convolution can be applied
similarly as the image. These full-voxel-based methods in-
evitably lead to information loss, as well as memory and
computational consumption as it increases cubically with
respect to the voxel’s resolution. To reduce the computa-
tional cost of these full-voxel-based methods, OctNet [38]
and Kd-Net [20] were designed to resolve them by skipping
the computations on empty voxels and focusing on infor-
mative voxels. The multi-view-based method [43, 24, 18]
represents the point cloud as a set of images rendered from

multiple views. However, it is still unclear how to deter-
mine the number and distribution of the views to cover the
3D objects while avoiding mutual occlusions.

The graph-based method [7, 51] ﬁrst represents the point
cloud as a graph according to their spatial neighbors, and
then generalizes the standard CNN to adapt to the graph-
structural data. Shen et al. [40] deﬁned a point-set kernel as
a set of learnable 3D points that jointly respond to the neigh-
boring points according to their geometric afﬁnities mea-
sured by the kernel correlation. 3DGNN [36] applied graph
neural network to RGBD data. However, due to the isotropy
of its aggregation function, 3DGNN can hardly adapt to ob-
jects with different structures. ECC [42] and SPG [23] pro-
posed to generate the convolution ﬁlters according to the
edge labels (weights), so that the information can propagate
in a speciﬁc direction on the graph. Nevertheless, ECC and
SPG can only capture some speciﬁc structures since these
edge labels (weights) are predeﬁned.

Beneﬁting from the development of deep learning on
sets [33, 52, 37], researchers recently constructed effective
and simple architecture to directly learn on point sets by ﬁrst
computing individual point features from per-point multi-
layer perceptron (MLP) and then aggregating all the fea-
tures as a global presentation of a point cloud [35, 12]. The
set-based method can be used directly on the point level and
is robust to the rigid transformation. However, it neglects
the spatial neighboring relation between points, which con-
tains ﬁne-grained structural information for semantic seg-
mentation.

Convolution on Graphs. Related works about convolu-
tion on graphs can be categorized as spectral approaches
and non-spectral approaches. Spectral approaches work
with a spectral representation of graphs that relies on the
eigen-decomposition of their Laplacian matrix [19, 10].
The corresponding eigenvectors can be regarded as the
Fourier bases in the harmonic analysis of spectral graph the-
ory. The spectral convolution can then be deﬁned as the
element-wise product of two signals’ Fourier transform on
the graph [8]. This spectral convolution does not guaran-
tee the spatial localization of the ﬁlter and thus requires ex-
pensive computations [41, 17]. In addition, as spectral ap-
proaches are associated with their corresponding Laplacian
matrix, a spectral CNN model learned on one graph cannot
be transferred to another graph that has a different Laplacian
matrix.

Non-spectral approaches aim to deﬁne convolution di-
rectly on a graph with local neighbors in a spatial or man-
ifold domain. The key to non-spectral approaches is to de-
ﬁne a set of sharing weights applied to the neighbors of each
vertex [3, 48]. Duvenaud et al. [11] computed a weight ma-
trix for each vertex and multiplied it to its neighbors fol-
lowing a sum operation. Niepert et al. [32] proposed select-
ing and ordering the neighbors of each vertex heuristically

10297

Figure 2. Left: Illustration of GAC on a subgraph of a point cloud. The output is a weighted combination of the neighbors of point 1.
Right: The attention mechanism employed in GAC for dynamically attentional weights generating. It receives the neighboring vertices’
spatial positions and features as input, and then maps them to normalized attentional weights.

so that the 1D CNN can be used. Monti et al. [31] pro-
posed a uniﬁed framework that allows the generalization of
CNN architecture to graph using ﬁxed local polar pseudo-
coordinates around each vertex. Hamilton et al. [16] intro-
duced an inductive framework by applying a speciﬁc aggre-
gator over the neighbors, such as the max/mean operator or
a recurrent neural network (RNN). However, their convo-
lution weights are mainly generated according to the prede-
ﬁned local coordinate system, while neglecting the structure
of the objects for semantic segmentation.

CRF in Deep Learning. CRF [22] possesses ﬁne-
grained probabilistic modeling capability, while CNN has
powerful feature learning capability. The combination of
CRF and CNN has been proposed in many image segmenta-
tion works [5, 9, 2, 29]. Recently, referring to the mean-ﬁeld
algorithm [21], the iteration of CRF inference was modeled
as a stack of CNN layers [53, 28]. For 3D point cloud, fol-
lowing CRF-RNN [53], SegCloud [45] extends the imple-
mentation of CRF into 3D point clouds after a fully con-
nected CNN. However, since CRF is applied as an indi-
vidual part following the CNN, it is difﬁcult to explore the
power of their combination.

3. Method

We propose a novel graph attention convolution (GAC)
for structured feature learning of 3D point cloud and
demonstrate its theoretical advantage (Section 3.1). After-
wards, we construct an end-to-end point cloud segmentation
framework (Section 3.2) with the proposed GAC. The de-
tails of converting point cloud into our needed graph pyra-
mid are provided in Section 3.3.

3.1. Graph attention convolution

Consider a graph G(V, E) constructed from a given
point cloud P = {p1, p2, ..., pN } ∈ R3 according to their
spatial neighbors, where V = {1, 2, ..., N } and E ⊆
|V | × |V | represent the set of vertices and edges respec-
tively and N is the number of vertices (points). Denote
N (i) = {j : (i, j) ∈ E} ∪ {i} (including itself) as the
neighbor set of vertex i. Let H = {h1, h2, ..., hN } be a set

of input vertex features, each feature hi ∈ RF is associated
with a corresponding graph vertex i ∈ V , where F is the
feature dimension of each vertex.

′

′

′

′

1, h

= {h

2, ..., h

Our GAC is designed to learn a function g : RF → RK ,
which maps the input features H to a new set of vertex fea-
i ∈ RK , while main-
tures H
taining the structural connection between these output fea-
tures. Meanwhile, unlike the relatively ﬁxed neighboring
relation in image domain, the proposed GAC should also
be able to handle the unordered and size-varying neighbors
while retaining the weight sharing property.

N } with h

′

To this end, we construct a sharing attention mechanism
α : R3+F → RK to focus on the most relevant part of the
neighbors for feature learning, so that the convolution ker-
nel of GAC can dynamically adapt to the structure of the
objects. Speciﬁcally, the attentional weight of each neigh-
boring vertex is computed as follows:

˜aij = α(∆pij, ∆hij), j ∈ N (i)

(1)

where ˜aij = [˜aij,1, ˜aij,2, ..., ˜aij,K] ∈ RK indicates the at-
tentional weight vector of vertex j to vertex i. ∆pij = pj −
pi, and ∆hij = Mg(hj) − Mg(hi), where Mg : RF → RK
is a feature mapping function applied on each vertex, i.e.,
Mg is a multilayer perceptron. The ﬁrst term of α indicates
the spatial relations of the neighboring vertices, which helps
to span the unordered neighbors to meaningful surface. The
second term measures the feature difference between vertex
pairs, which guides us to assign more attention to the sim-
ilar neighbors. The sharing attention mechanism α can be
implemented with any differentiable architecture, we use a
multilayer perceptron in this work (as shown in Figure 2),
which can be formulated as follows:

α(∆pij, ∆hij) = Mα([∆pij||∆hij])

(2)

where || is the concatenation operation, Mα indicates the
applied multilayer perceptron.

In addition, to handle the size-varying neighbors across
different vertices and spatial scales, the attentional weights
are normalized across all the neighbors of vertex i as fol-

10298

 $

 !

 "

 #

 !

 "

 !

 "

 $

: Graph pooling

: Feature interpolation

: Graph attention convolution

interpolation

concatenation

1(cid:215)1 convolution

Skip connection

Figure 3. GACNet architecture. Our GACNet is constructed on the graph pyramid of a point cloud. On each scale of the graph pyramid, the
proposed GAC is applied for local feature learning, followed by the graph pooling for resolution reducing in each feature channel. After
that, the learned features are interpolated back to the ﬁnest scale layer by layer for point-wise label assignment.

lows:

aij,k =

exp(˜aij,k)

Pl∈N (i) exp(˜ail,k)

(3)

based on the feature differences rather than the concatena-
tion of two neighboring features, which is more efﬁcient
and explicit to characterize the feature relation.

where ˜aij,k is the attentional weight of vertex j to vertex i
at the k-th feature channel.

Therefore, the ﬁnal output of the proposed GAC can be

formulated as follows:

h

′

i = X
j∈N (i)

aij ∗ Mg(hj) + bi

(4)

where * represents the Hadamard product, which produces
the element-wise production of two vectors, and bi ∈ RK
is a learnable bias.

Relationship to standard convolution. The convolu-
tion weights of a standard convolution in the grid domain
are determined by the neighbors’ local spatial positions. In
our GAC, the attentional weights are generated according
to not only the neighbors’ spatial positions but also their
dynamically learned features. Additionally, as GAC is de-
signed on the spatial neighbors of points, it also retains the
key properties of the standard convolution in grid domain:
weight sharing and locality.

Relationship to prior works. The proposed GAC is re-
lated to several prior works, mainly including GAT [47] and
PointNet [33].

Although we are inspired by the idea of attention mech-
anism as GAT [47], our GAC is different: 1) GAC assigns
proper attentional weights to not only different neighboring
points but also different feature channels, as the features at
different channels are hopefully independent; 2) Compared
to GAT, GAC incorporates the local spatial relationship be-
tween neighboring points, which plays an important role in
3D shape analysis; 3) We generate the attentional weights

PointNet [33] and its variations [35] have achieved
promising results for point cloud analysis by directly learn-
ing on point sets. Key to PointNet is the use of a max-
pooling operator (including a MLP). It can be seen as an
extreme case of GAC as “max attention”, which aggregates
the neighboring features by taking the max value at each
feature channel. The max operator tends to capture the most
“special” features, which damages the structural connec-
tions between the points of an object and becomes sensitive
to noise. Comparatively, the proposed GAC aggregates the
neighboring features by assigning them proper attentional
weights, maintaining the structure of the objects for ﬁne-
grained point cloud segmentation.

Theoretical analysis.

In this section, we explore the
expressive capabilities of our GAC to further understand
how GAC can efﬁciently learn the features of point clouds.
Speciﬁcally, we consider whether GAC can learn to pre-
cisely represent the neighboring features of each vertex.

Suppose the input vertex features H are bounded, i.e.,
H ⊆ [a, b]F , where a and b indicate the lower and upper
bound respectively. In fact, we can show that the proposed
GAC is capable of aggregating the entire neighbor infor-
mation of any vertex on the graph G(V, E) to an arbitrary
precision:

Theorem 1. Let X = {S : S ⊆ [a, b]F and S is ﬁnite},
f : X → R is a continuous set function w.r.t. Hausdorff
distance dH (·, ·). Denote Si = {hj : j ∈ N (i) ∈ X } as
the set of neighboring points of vertex i ∈ V with arbitrary
order. ∀ǫ > 0, ∃K ∈ Z and parameter θ of GAC, such that

10299

for any i ∈ V ,

|f (Si) − γ(gθ(Si))| < ǫ

(5)

where γ is a continuous function, and gθ(Si) ∈ RK is the
output of GAC.

The full proof is provided in the Appendix. Similar to
PointNet, in the worst case, our GAC can learn to divide
the point cloud into a volumetric representation. In Point-
Net, the representation capability is limited by the output
dimension K. However, as the attention mechanism in our
GAC actually acts as a feature encoder, GAC is capable of
approximating the set function f even when K is not sufﬁ-
ciently large.

3.2. Graph attention convolution network

We follow the common image segmentation architecture
to organize our network for point cloud semantic segmen-
tation, coined graph attention convolution network (GAC-
Net). The difference is that, our GACNet is implemented on
the graph pyramid of a point cloud, as shown in Figure 3. At
each scale of the graph pyramid, GAC is applied for local
feature learning. Then a graph pooling operation is used to
reduce the resolution of point clouds in each feature chan-
nel. After that, the learned features are interpolated back to
the ﬁnest scale layer by layer. Inspired by [27], features at
the same scale are skip-connected. Finally, considering the
loss of feature ﬁdelity caused by the multiple graph pooling
and feature interpolation layers, an additional GAC layer is
applied at the ﬁnest scale for feature reﬁnement.

′

Graph pooling. Graph pooling aims to output the aggre-
gated features on the vertices of a coarsened graph. Denote
H
l as the output feature set at the l-th scale of the graph
pyramid, the input feature set Hl+1 of the (l+1)-th scale is
calculated as follows:

hv = pooling{h

′

j : j ∈ Nl(v)}

(6)

where hv ∈ Hl+1 and Nl(v) indicates the neighbors of ver-
tex v at the l-th scale. The pooling function can be a max
or mean function, which corresponds to the max and mean
pooling, respectively [42].

′

Feature interpolation. To ﬁnally obtain the feature map
that has the same number of points as the original input, we
must interpolate the learned features from the coarsest scale
to the original scale layer by layer. Let H
l be the learned
feature set at the l-th scale of the graph pyramid, Pl and
Pl−1 are the spatial coordinates set of the l-th and (l-1)-th
scales, respectively. To obtain the features of the (l-1)-th
scale, we simply search the three nearest neighbors of Pl−1
in Pl and calculate the weighted sum of their features. The
combination weights are calculated according to the neigh-
bors’ normalized spatial distances [35].

GACNet vs. CRF. CRF has practically become a de
facto standard as the postprocessing of the CNN’s outputs

in semantic segmentation tasks. The key idea of CRF is
to encourage similar points to share consistent labels. In-
tuitively, spatially close and appearance-similar points are
encouraged to be assigned the same label.

In fact, our GAC shares the same characteristics as the
CRF model. Speciﬁcally, GAC assigns neighbors proper
attentional weights according to both their spatial positions
and feature attributes. The spatial position term encour-
ages the spatially close points to share similar features,
whereas the feature attribute term aims at leading the infor-
mation propagating between points with similar attributes
(i.e., low-level local features or high-level semantic labels).
Therefore, the CRF model is no longer needed in GACNet.
Notably, compared to formulating the CRF model as a
recurrent network [53], the proposed GACNet has several
compelling advantages. First, rather than using CRF for a
postprocessing which is independent of the CNN, GACNet
is equivalent to unfolding the recurrent network of CRF into
each layer of the network, which directly guides the learned
features to maintain the structure of an object for seman-
tic segmentation. Second, compared to the simple message
passing and compatibility transform in the class-probability
space of CRF [21, 53], GAC also has the capability to map
the input signals into a hidden feature space for further fea-
ture extraction. We experimentally evaluate these claims in
Section 4.3.

3.3. Graph pyramid construction on a point cloud

This section describes how we construct the graph pyra-
mid on point clouds. Speciﬁcally, we search the spatial
neighbors for all points and link them as a graph. The graph
pyramid with different spatial scales is constructed by al-
ternately applying graph construction and coarsening tech-
niques. Additionally, the covariance matrix of each point’s
neighbors at the ﬁnest scale are recorded during the graph
construction process, and its eigenvalues are used as local
geometrical feature (geo-feature). The initial feature vector
of a point is composed of height, RGB, and geo-feature.

Graph construction on a point cloud. For given point
cloud P , which records the spatial coordinates of the points,
we construct a directed graph G(V, E). Here, each vertex
is associated with a point, and the edges are added between
the point and its KG neighbors.
In our experiments, the
KG neighbors are randomly sampled within radius ρ, which
shows better performance than searching their KG nearest
neighbors as it is unrelated to the density of the point cloud.
Graph coarsening. Similar to pyramid construction
in the image domain, we subsample the input point cloud
P with a set of ratios using the furthest point sampling
algorithm [35]. Denote the subsampled point clouds as
P = {P0, P1, ..., PL}, where L is the number of scales
for subsampling and P0 = P . For each Pl(l = 0, ..., L),
a corresponding graph Gl(Vl, El) can be constructed as de-

10300

Method

OA mIoU ceiling ﬂoor wall beam column window door chair

table bookcase sofa board clutter

PointNet [33]
SegCloud [45]

41.09 88.80 97.33 69.80 0.05
3.92
48.92 90.06 96.05 69.86 0.00 18.37
86.38 58.04 89.35 96.87 78.12 0.00 42.81
GACNet(ours) 87.79 62.85 92.28 98.27 81.90 0.00 20.35

SPG [23]

-
-

46.26 10.76 52.61 58.93
38.35 23.12 75.89 70.40
48.93 61.58 84.66 75.41
59.07 40.85 78.54 85.80

40.28
58.42
69.84
61.70

5.85 26.38 33.22
40.88 12.96 41.60
52.60 2.10 52.22
70.75 74.66 52.82

Table 1. Results on the S3DIS dataset (testing on Area 5 and training on the rest ﬁve areas).

Method

OA mIoU

man-made

terrain

natural
terrain

high

low

vegetation

vegetation

buildings hard scape

scanning
artefacts

SnapNet [6]

SegCloud [45]
RF MSSF [46]

88.6 59.1
88.1 61.3
90.3 62.7
MSDeepVoxNet [39] 88.4 65.3
94.0 73.2
91.9 70.8

GACNet(ours)

SPG [23]

82.0
83.9
87.6
83.0
97.4
86.4

77.3
66.0
80.3
67.2
92.6
77.7

79.7
86.0
81.8
83.8
87.9
88.5

22.9
40.5
36.4
36.7
44.0
60.6

91.1
91.1
92.2
92.4
93.2
94.2

18.4
30.9
24.1
31.3
31.0
37.3

37.3
27.5
42.6
50.0
63.5
43.5

cars

64.4
64.3
56.6
78.2
76.2
77.8

Table 2. Results on the Semantic3D dataset (reduced-8 challenge).

scribed above.

4. Experiments

In this section, we evaluate the proposed GACNet on
various 3D point cloud segmentation benchmarks, includ-
ing the Stanford Large-Scale 3D Indoor Spaces (S3DIS) [1]
dataset and the Semantic3D [15] dataset. The performance
is quantitatively evaluated with three metrics, including the
per-class intersection over union (IoU), mean IoU of each
class (mIoU), and overall accuracy (OA). In addition, the
performance of several key components of GAC is further
analyzed.

4.1. Indoor segmentation on the S3DIS dataset

The S3DIS dataset contains 3D RGB point clouds from
six indoor areas of three different buildings. Each point
is annotated with one of the semantic labels from 13 cat-
egories. For a principled evaluation, we follow [45, 33, 23]
to choose Area 5 as our testing set and train our GACNet
on the rest to ensure that the training model does not see
any part of the testing area. Notably, Area 5 is not in the
same building as other areas and there exist some differ-
ences between the objects in Area 5 and other areas. This
across-building experimental setup is better for measuring
the model’s generalizability, while also brings challenges to
the segmentation task.

To prepare our training data, we ﬁrst split the dataset
room by room and then sample them into 1.2m by 1.2m
blocks with a 0.1m buffer area on each side. Points lying
in the buffer area are regarded as the contextual information
and are not linked to the loss function for model training or
class prediction. In addition, for training convenience, the
points in each block are sampled into a uniform number of
4,096 points. During the testing phase, blocks can be any
size depending on the memory of the computing device. In

this experiment, we slice our test room into 3.6m by 3.6m
blocks with a maximum of 4,096×9 points. Each block is
individually constructed as a graph pyramid according to
Section 3.3 for training or testing.

The quantitative evaluations of the experimental results
are provided in Table 1. We can see that the proposed GAC-
Net performs better than other competitive methods in most
classes. In particular, we achieve considerable gains in win-
dow, table, sofa, and board. In the S3DIS dataset, the board
and window are pasted onto the wall and difﬁcult to delin-
eate geometrically, but our GACNet can still segment them
out according to their color features. As the convolution
weights of GAC are assigned according to not only the spa-
tial positions but also the feature attributes of the neighbor-
ing points, the proposed GACNet is able to capture the dis-
criminative features of point clouds even though the spatial
geometry is lost or weak.

4.2. Outdoor segmentation on the Semantic3D

dataset

The Semantic3D dataset is currently the largest avail-
able LiDAR dataset, with over 4 billion points from a va-
riety of urban and rural scenes. Each point has RGB and
intensity values and is labeled with one of 8 categories:
man-made terrain, natural terrain, high vegetation, low veg-
etation, buildings, hard scape, scanning artefacts, and cars.
Different from the S3DIS dataset, the Semantic3D dataset
contains outdoor scenes which have relatively larger ob-
jects. To adapt to the size of objects, the sampled blocks for
the Semantic3D dataset is set to be 4m by 4m while main-
taining the same maximum number of 4,096 points. We
provide the evaluation results on the reduced-8 challenge of
the benchmark in Table 2.

Additionally, we list the overall accuracy and mean IoU
of our GACNet compared to other state-of-the-art methods.

10301

natural terrain

man-made terrain

Figure 4. Illustration of an easily-confused area (red ovals) close
to the scanning station (yellow stars) and similar to natural terrain
in color and geometry but is actually man-made terrain. Note, this
kind of area does not appear in our training set and is therefore
difﬁcult to segment.

In general, our performance is on par with or better than
other competitive methods for many classes. Notably, in
the semantic3D dataset, most of the objects, such as cars,
hard scape, buildings, and low/high-vegetation, are frag-
mented and incomplete due to the mutual occlusion among
points. However, our GACNet can still learn to capture their
discriminative features for segmentation owing to the pow-
erful structured feature learning capability of GAC. Mean-
while, we also notice that the man-made terrain and the nat-
ural terrain are relatively difﬁcult to separate for GACNet
in this experiment, as there are a large number of points in
an easily-confused area (as shown in Figure 4) that does not
appear in the training set.

4.3. Ablation studies and analysis

To better understand the inﬂuence of various design
choices made in the proposed framework, we further con-
duct several ablation studies to demonstrate the effective-
ness of GAC, explore the effect of spatial positions and fea-
ture attributes in GAC, compare GAC with CRF-RNN [53],
and investigate the effect of initial features.

Effectiveness of GAC. To further understand the effec-
tiveness of the proposed GAC, we compare it with the max
operator (including a MLP) of PointNet [33], which has
achieved promising results by directly learning on point
sets. Speciﬁcally, we only replace the attention mechanism
in GAC with the max operator while keeping the rest un-
changed in our GACNet. The testing results on the S3DIS
dataset are provided in Table 3. We can see that the mean
IoU of GAC is 4.43% higher than the max operator, which
shows that GAC has more advantages in discriminative fea-
ture learning than the max operator. Actually, the max oper-
ator in PointNet [33] acts as a “max attention” mechanism
that tends to characterize the contour of point sets in the
feature space while damaging the structural connections be-
tween the points of an object. It results in the max operator
being good at the object classiﬁcation task but poor at seg-
mentation where the border of the object needs to be ﬁnely
delineated.

Ablation studies

OA mIoU

Max operator

85.47
87.44
Spatial positions only
87.28
Feature attributes only
87.12
CRF-RNN (1 iteration)
CRF-RNN (3 iterations) 87.86
CRF-RNN (5 iterations) 87.46
86.06
86.17
83.56
87.79

No geo-feature

Height only

No RGB

GACNet

58.42
60.41
60.25
61.70
61.97
61.83
60.16
60.37
58.96
62.85

Table 3. Ablation studies on the S3DIS test set.

Spatial positions and feature attributes. In GAC, the
neighboring points’ spatial positions and feature attributes
serve as spatial and feature guides to dynamically generate
their attentional weights. To explore their respective roles,
we designed two other variations of GAC that only use the
spatial positions and the feature attributes. Their testing re-
sults on the S3DIS dataset are reported in Table 3 for com-
paring convenience. The experimental results show that,
both spatial positions and feature attributes have played im-
portant roles in GAC for semantic point cloud segmenta-
tion. The spatial positions span the unordered neighboring
points to meaningful object surfaces, while the feature at-
tributes further guide GAC to adapt to the structure of an
object by assigning proper weights to different neighbors.
Without the constraint of the spatial positions, points will
only exchange information with neighbors with similar ini-
tial features, which causes the ﬁnal features to be piece-
meal and difﬁcult to form meaningful objects. Without the
guidance of the feature attributes, convolution kernels can
hardly distinguish where the object’s border is, and the cur-
rent points are easily contaminated by the neighboring ob-
jects (as shown in Figure 5).

CRF-RNN. As described in Section 3.2, our GAC-
Net actually shares the same characteristics with the CRF
model, which encourages feature and label agreement be-
tween similar points. To experimentally verify this claim,
we remove the last GAC layer in our GACNet and replace
it with the CRF-RNN [53] using different iterations. Specif-
ically, we use the Gaussian kernels from [21] for the pair-
wise potentials of CRF. Their testing results on the S3DIS
dataset are also provided in Table 3 for comparing conve-
nience. We can see that, with one iteration the CRF-RNN
has basically converged and more iterations do not result
in considerably increased accuracy. Since our GACNet has
shared the same characteristics of CRF in each layer of the
network (Section 3.2), the recurrence of CRF is no longer
needed.

Effect of initial features.

In the above experiments on
the S3DIS dataset, the initial feature vector of each point
is composed of height, RGB, and geo-feature. In this sec-

10302

Figure 5. Illustration of the role of feature attributes in GAC. Figures from left to right are the input point cloud, the predicted result by
GACNet, the predicted result by GACNet without the feature attributes, and the ground truth. We can see that, with the guidance of the
feature attributes, the objects are more clearly and regularly delineated.

)

%
(
y
c
a
r
u
c
c
A

100

95

90

85

80

75

70

65

GACNet

NoGeo-feature

100
99
98
97
96
95

40

43

46

49

1

4

7 1 0 1 3 1 6 1 9 2 2 2 5 2 8 3 1 3 4 3 7 4 0 4 3 4 6 4 9

Training epoch

Figure 6. Training accuracy with or without geo-feature.

tion, we provide additional ablation studies to further under-
stand the performance of our GACNet with different initial
input features. We design three comparison experiments
where we remove the RGB information, the geo-feature,
and both of them, respectively. The testing results on the
S3DIS dataset are provided in Table 3. By comparison,
the mIoU drops by 2.69%, 2.48%, and 3.89% respectively.
However, compared to the relatively large accuracy differ-
ence in the testing phase, we also notice that the training
accuracy without geo-feature actually shows little differ-
ence from our standard GACNet (as shown in Figure 6).
The initial geo-feature serves as the low-level universal fea-
tures and is designed according to priori knowledge, which
is useful for the generalization of the network.

4.4. Robustness test and stress test

We compare GAC with the max operator [33] on ro-
bustness against random Gaussian noise, and resistance on
missing data. However, as additional noise will change the
class attribute of a point in the segmentation task, we turn
to a classiﬁcation task for our robustness and stress test. We
implement this work on the ModelNet40 [50] shape clas-
siﬁcation benchmark. There are 12,311 CAD models from
40 man-made object categories, splitting them into 9,843
for training and 2,468 for testing. We uniformly sample
1,024 points on their mesh and normalize them into a unit
sphere as inputs for our network. Our classiﬁcation network
is built by simply replacing the feature interpolation layers
in GACNet with a global pooling layer, and the input of the

)

%
(
y
c
a
r
u
c
c
A

100
90
80
70
60
50
40
30
20
10
0

0

GAC
Max

)

%
(
y
c
a
r
u
c
c
A

100
90
80
70
60
50
40
30
20
10
0

GAC
Max

0. 9

0

0. 02

0. 04

0. 06

0. 08

0. 1

Perturbation noise std 

0. 4

0. 2
0. 8
Missing data ratio

0. 6

Figure 7. Robustness and stress test. GAC and Max indicate that
we use graph attention convolution and the max operator in the
classiﬁcation network respectively.

network is just the height information of each point. All
models are trained without data augmentation. During the
robustness test, input points are added with Gaussian noise
with a series of standard deviations and zero means. For
the stress test, a series of ratios of input points are randomly
dropped out. From Figure 7, we can see that the max op-
erator is more sensitive to noise because it actually tends to
capture the most “special” features (probably noise), while
GAC is robust to noise owing to its spatial and feature con-
straints. For missing data, the accuracy of GAC drops by
13.66% when the missing ratio is 40%, while the max oper-
ator drops by 26.48%.

5. Conclusion

We propose a novel graph attention convolution (GAC)
with learnable kernel shape for structured feature learning
of 3D point cloud. Our GAC is a universal and simple mod-
ule maintaining the weight sharing property of the standard
convolution and can be efﬁciently implemented on graph
data. We have applied GAC to train an end-to-end net-
work for semantic point cloud segmentation. Both theo-
retical analysis and empirical experiments have shown the
effectiveness and advantage of the proposed GAC.

Acknowledgments.
This work was supported by the
NSFC (No. 41671419, No. 51208392), and the inter-
disciplinary research program of Wuhan University (No.
2042017kf0204). We are thankful to anonymous reviewers
for their helpful comments and suggestions.

10303

References

[1] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioan-
nis Brilakis, Martin Fischer, and Silvio Savarese. 3D se-
mantic parsing of large-scale indoor spaces. In CVPR, pages
1534–1543, 2016.

[2] Anurag Arnab, Sadeep Jayasumana, Shuai Zheng, and Philip
H S Torr. Higher order conditional random ﬁelds in deep
neural networks. In ECCV, 2016.

[3] James Atwood and Don Towsley. Diffusion-convolutional

neural networks. In NIPS, 2015.

[4] Dzmitry Baddanau, KyungHyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. In ICLR, 2015.

[5] Linchao Bao, Yibing Song, Qingxiong Yang, and Narendra
Ahuja. An edge-preserving ﬁltering framework for visibility
restoration. In ICPR, 2012.

[6] Alexandre Boulch, Joris Guerry, Bertrand Le Saux, and
Nicolas Audebert. SnapNet: 3D point cloud semantic la-
beling with 2D deep segmentation networks. Computers and
Graphics (Pergamon), pages 189–198, 2018.

[7] Michael M. Bronstein, Joan Bruna, Yann Lecun, Arthur
Szlam, and Pierre Vandergheynst. Geometric deep learning:
Going beyond euclidean data. IEEE Signal Processing Mag-
azine, 34(4):18–42, 2017.

[8] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Le-
Cun. Spectral networks and locally connected networks on
graphs. arXiv:1312.6203, 2013.

[9] Liang Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L. Yuille. DeepLab: Semantic im-
age segmentation with deep convolutional nets, atrous con-
volution, and fully connected CRFs. TPAMI, 40(4):834–848,
2018.

[10] Micha¨el Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. Convolutional neural networks on graphs with
fast localized spectral ﬁltering. In NIPS, 2016.

[11] David Duvenaud, Dougal Maclaurin,

Jorge Aguilera-
Iparraguirre, Rafael G´omez-Bombarelli, Timothy Hirzel,
and Al´an Aspuru-Guzik. Convolutional networks on graphs
for learning molecular ﬁngerprints. In NIPS, 2015.

[12] Francis Engelmann, Theodora Kontogianni, Alexander Her-
mans, and Bastian Leibe. Exploring spatial context for 3D
semantic segmentation of point clouds. In ICCV Workshop,
pages 716–724, 2017.

[13] Jonas Gehring, Michael Auli, David Grangier, and Yann N.
Dauphin. A convolutional encoder model for neural machine
translation. arXiv:1611.02344, 2016.

[14] Benjamin Graham, Martin Engelcke, and Laurens van der
Maaten. 3D semantic segmentation with submanifold sparse
convolutional networks. In CVPR, 2018.

[15] T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner, K.
Schindler, and M. Pollefeys. Semantic3D.net: A new large-
scale point cloud classiﬁcation benchmark. In ISPRS Annals
of the Photogrammetry, Remote Sensing and Spatial Infor-
mation Sciences, 2017.

[16] William L. Hamilton, Rex Ying, and Jure Leskovec. Induc-
tive representation learning on large graphs. In NIPS, 2017.

[17] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolu-
tional networks on graph-structured data. arXiv:1506.05163,
2015.

[18] Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji,
and Siddhartha Chaudhuri. 3D shape segmentation with pro-
jective convolutional networks. In CVPR, pages 6630–6639,
2017.

[19] Thomas N Kipf and Max Welling. Semi-supervised calssiﬁ-
cation with graph-concoluational neural networks. In ICML,
2017.

[20] Roman Klokov and Victor Lempitsky. Escape from cells:
Deep kd-networks for the recognition of 3D point cloud
models. In CVPR, pages 863–872, 2017.

[21] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference
in fully connected CRFs with gaussian edge potentials. In
NIPS, pages 109–117, 2012.

[22] John Lafferty, Andrew McCallum, and Fernando C N
Pereira. Conditional random ﬁelds: Probabilistic models
for segmenting and labeling sequence data. In ICML, pages
282–289, 2001.

[23] Loic Landrieu and Martin Simonovsky. Large-scale point
In

cloud semantic segmentation with superpoint graphs.
CVPR, 2018.

[24] Truc Le, Giang Bui, and Ye Duan. A multi-view recurrent
neural network for 3D mesh segmentation. Computers and
Graphics (Pergamon), 2017.

[25] Truc Le and Ye Duan. PointGrid: A deep network for 3D

shape understandings. In CVPR, 2018.

[26] Yangyan Li, Soeren Pirk, Hao Su, Charles R. Qi, and
Leonidas J. Guibas. FPNN: Field probing neural networks
for 3D data. In NIPS, 2016.

[27] Tsung-Yi Lin, Piotr Doll, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection.
In CVPR, pages 936–944,
2017.

[28] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and
Xiaoou Tang. Deep learning markov random ﬁeld for se-
mantic segmentation. TPAMI, 40(8):1814–1828, 2018.

[29] Ping Luo, Xiaogang Wang, and Xiaoou Tang. Pedestrian
parsing via deep decompositional network. In ICCV, pages
2648–2655, 2013.

[30] Daniel Maturana and Sebastian Scherer. VoxNet: A 3D con-
volutional neural network for real-time object recognition. In
IROS, pages 922–928, 2015.

[31] Federico Monti, Davide Boscaini,

Jonathan Masci,
Emanuele Rodol`a, Jan Svoboda, and Michael M. Bronstein.
Geometric deep learning on graphs and manifolds using
mixture model CNNs. In CVPR, pages 5425–5434, 2017.

[32] Mathias Niepert, Mohamed Ahmed,

and Konstantin
Kutzkov. Learning convolutional neural networks for graphs.
In ICML, 2016.

[33] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas.
PointNet: Deep learning on point sets for 3D classiﬁcation
and segmentation. In CVPR, pages 77–85, 2017.

[34] Charles R. Qi, Hao Su, Matthias Niessner, Angela Dai,
Mengyuan Yan, and Leonidas J. Guibas. Volumetric and
multi-view CNNs for object classiﬁcation on 3D data.
In
CVPR, pages 5648–5656, 2016.

10304

[35] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Point-
Net++: Deep hierarchical feature learning on point sets in a
metric space. In NIPS, 2017.

[52] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barn-
abas Poczos, Ruslan Salakhutdinov, and Alexander Smola.
Deep sets. In NIPS, 2017.

[53] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang
Huang, and Philip H. S. Torr. Conditional random ﬁelds
as recurrent neural networks. In ICCV, pages 1529 – 1537,
2015.

[36] Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel
Urtasun. 3D graph neural networks for RGBD semantic seg-
mentation. In ICCV, page 5209–5218, 2017.

[37] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos.
Deep learning with sets and point clouds. arXiv:1611.04500,
2016.

[38] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.
OctNet: Learning deep 3D representations at high resolu-
tions. In CVPR, pages 6620–6629, 2017.

[39] Xavier Roynard, Jean-Emmanuel Deschaud, and Franc¸ois
Goulette. Classiﬁcation of point cloud scenes with multi-
scale voxel deep network. arXiv:1804.03583, 2018.

[40] Yiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian. Min-
ing point cloud local structures by kernel correlation and
graph pooling. In CVPR, 2018.

[41] David I. Shuman, Sunil K. Narang, Pascal Frossard, Anto-
nio Ortega, and Pierre Vandergheynst. The emerging ﬁeld
of signal processing on graphs: Extending high-dimensional
data analysis to networks and other irregular domains. IEEE
Signal Processing Magazine, 30(3):83–98, 2012.

[42] Martin Simonovsky and Nikos Komodakis. Dynamic edge-
conditioned ﬁlters in convolutional neural networks on
graphs. In CVPR, pages 29–38, 2017.

[43] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik
Learned-Miller. Multi-view convolutional neural networks
for 3D shape recognition. In ICCV, pages 945–953, 2015.

[44] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-
Yi Zhou. Tangent convolutions for dense prediction in 3D.
In CVPR, 2018.

[45] Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunY-
oung Gwak, and Silvio Savarese. SEGCloud: Semantic seg-
mentation of 3D point clouds. In 3DV, pages 537–547, 2017.

[46] Hugues Thomas, Jean-Emmanuel Deschaud, Beatriz Mar-
cotegui, Franc¸ois Goulette, and Yann Le Gall. Semantic
classiﬁcation of 3D point clouds with multiscale spherical
neighborhoods. arXiv:1808.00495, 2018.

[47] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph at-
tention networks. In ICLR, 2018.

[48] Nitika Verma, Edmond Boyer, and Jakob Verbeek. Dynamic
ﬁlters in graph convolutional networks. arXiv:1706.05206,
2017.

[49] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,
and Xin Tong. O-CNN: Octree-based convolutional neu-
ral networks for 3D shape analysis. ACM Transactions on
Graphics, 36(4), 2017.

[50] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, and Xiaoou Tang. 3D ShapeNets: A deep
representation for volumetric shapes. In CVPR, pages 1912–
1920, 2015.

[51] Li Yi, Hao Su, Xingwen Guo, and Leonidas Guibas. Sync-
SpecCNN: Synchronized spectral CNN for 3D shape seg-
mentation. In CVPR, pages 6584–6592, 2017.

10305

