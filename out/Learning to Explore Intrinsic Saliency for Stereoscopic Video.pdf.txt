Learning to Explore Intrinsic Saliency for Stereoscopic Video

Qiudan Zhang1

,

2 Xu Wang1 ∗ Shiqi Wang2 Shikai Li1 Sam Kwong2

Jianmin Jiang1

1College of Computer Science and Software Engineering, Shenzhen University

2Department of Computer Science, City University of Hong Kong

Abstract

The human visual system excels at biasing the stereo-
scopic visual signals by the attention mechanisms. Tradi-
tional methods relying on the low-level features and depth
relevant information for stereoscopic video saliency predic-
tion have fundamental limitations. For example, it is cum-
bersome to model the interactions between multiple visual
cues including spatial, temporal, and depth information as
a result of the sophistication. In this paper, we argue that
the high-level features are crucial and resort to the deep
learning framework to learn the saliency map of stereo-
scopic videos. Driven by spatio-temporal coherence from
consecutive frames, the model ﬁrst imitates the mechanism
of saliency by taking advantage of the 3D convolutional
neural network. Subsequently, the saliency originated from
the intrinsic depth is derived based on the correlations be-
tween left and right views in a data-driven manner. Finally,
a Convolutional Long Short-Term Memory (Conv-LSTM)
based fusion network is developed to model the instanta-
neous interactions between spatio-temporal and depth at-
tributes, such that the ultimate stereoscopic saliency maps
over time are produced. Moreover, we establish a new
large-scale stereoscopic video saliency dataset (SVS) in-
cluding 175 stereoscopic video sequences and their ﬁxation
density annotations, aiming to comprehensively study the
intrinsic attributes for stereoscopic video saliency detec-
tion. Extensive experiments show that our proposed model
can achieve superior performance compared to the state-of-
the-art methods on the newly built dataset for stereoscopic
videos.

1. Introduction

In recent years, we have witnessed the strong growth
of 3D content and fast development of 3D display tech-
nologies, such that the automatic prediction of saliency on
stereoscopic videos has become ever important. Stereo-
scopic video saliency prediction (see Fig. 1), which at-

∗Corresponding author. wangxu@szu.edu.cn

Figure 1. Examples of left view clip (including ﬁve consecutive
frames), ground truth and their saliency results obtained with our
model on the newly built SVS dataset.

For example,

tempts to distinguish salient regions or objects in diverse,
dynamic and immersive scenes, is challenging yet reward-
ing.
it can serve as the perceptual pre-
processing for numerous stereoscopic applications, such as
stereoscopic video coding, quality assessment, medical im-
age analysis and robot vision. However, the saliency predic-
tion of stereoscopic videos is quite complex as non-intuitive
interactions between the video pair and depth clues are in-
volved, making it difﬁcult to directly apply the traditional
2D saliency prediction algorithm.

With a variety of deep learning architectures available,
signiﬁcant advances have been achieved in static image
saliency prediction due to the successful deployment of con-
volutional neural networks (ConvNet) [5, 15]. The features
learned based on ConvNet are commendable for saliency in-
ference, as they are capable of mining underlying cues and
heuristic semantic priors to better interpret the image con-
tent compared to traditional hand-crafted features. Never-
theless, it is not feasible to infer dynamic video saliency by
straightforwardly applying static image saliency models, es-
pecially for stereoscopic videos. In particular, since videos
are composed of consecutive frames, temporal coherence is
an important element in stereoscopic video saliency predic-
tion. While recent studies [36, 9, 27] mostly adopt optical
ﬂow to explore saliency in temporal domain for dynamic
scenes, it will induce non-salient features and lead to more
time consumed during inferring. Moreover, depth serves
as an important component for holistic stereoscopic video

19749

perception, and in essence depth cue is another signiﬁcant
ingredient which can inﬂuence locations of ﬁxation points
during the 3D rendering.

A series of 3D saliency detection models have also been
proposed by considering low-level features and depth rel-
evant information. Fang et al. [10] proposed a visual at-
tention model for stereoscopic video based on the Gestalt
theory, in which feature contrast and motion contrast were
calculated to estimate spatial and temporal saliency, respec-
tively. However, the ignorance of high-level semantic fea-
tures leads to the limited success of the above method.
Moreover, how to determine the interactions between spa-
tial, temporal and depth cues is also non-trivial and chal-
lenging for stereoscopic video saliency prediction.

In this paper, we propose a saliency prediction model
for stereoscopic video by resorting to deep learning, which
infers stereoscopic saliency by automatically exploring
saliency-related features in terms of spatio-temporal coher-
ence and intrinsic depth. The obtained saliency distributions
on spatio-temporal and depth cues are ultimately combined
based on a Conv-LSTM fusion network to produce ﬁnal
saliency maps. To facilitate the training and evaluation on
the proposed method, a large dataset with 175 videos, in
which multiple visual stimulus are contained, is built. Ex-
perimental results show that our proposed model has signif-
icantly outperformed the state-of-art saliency models over
the created dataset. The contributions of this work are as
follows,

• We propose a new deep learning based attention model
for stereoscopic videos by singling out the contribu-
tions of spatio-temporal and depth cues, in an effort to
explore the intrinsic stereoscopic video saliency with a
data-driven strategy. The high-level semantic features
are learned correspondingly, and the ﬁnal attention
model is developed based on pyramid spatio-temporal
saliency prediction, intrinsic-depth saliency estimation
and Conv-LSTM based fusion.

• We create a new challenging dataset, stereoscopic
video saliency dataset (SVS), for the further research
and evaluation towards stereoscopic video saliency es-
timation. This dataset contains both natural and syn-
thetic scenes and will be made publicly available. Our
proposed stereoscopic video saliency model has been
validated using this new dataset, showing competitive
performance.

• We carry out analyses to investigate the inﬂuence of
image content, temporal characteristics and depth cues
on the stereoscopic video saliency prediction. We be-
lieve these analyses are capable of providing useful in-
sights to facilitate the future research of comprehen-
sive attention models for stereoscopic video saliency
estimation.

2. Related Work

In the literature, various biological characteristics in-
spired visual saliency computational models have been pro-
posed for 2D or 3D images. Inspired by the behavior and
neuronal architecture of the visual system of primates, the
pioneer saliency model proposed by Itti and Koch et al. [17]
calculated saliency map from multi-scale center-surround
feature contrast based on the underlying features of an im-
age (e.g., color, luminance, texture and orientation). Harel
et al. [13] built a complete bottom-up saliency model by
employing the dissimilarity measure based on graph theory
to evaluate saliency across distinct feature activation maps.
Goferman et al. [12] designed a context-aware saliency de-
tection model based on four visual saliency principles, such
that the signiﬁcant regions of scenes can be detected to rep-
resent the saliency. Hou et al. [14] further proposed a spec-
tral residual based visual saliency detection model that con-
structs saliency maps by the log-spectrum of image.

The key for further improving the performance of static
visual saliency model is to extract meaningful features to
capture the attention relevant information. The superiority
of deep neural network in terms of feature extraction pro-
vides new opportunities, and several learning based static
visual saliency detection models were proposed to locate
human eye ﬁxations. For example, Vig et al. [30] at-
tempted to build a visual saliency detection model based
upon ensembles of deep neural network. Later, K¨ummerer
et al. [22] developed a saliency model relying on extracted
deep learning features. In [15], the gap between model pre-
diction and human behavior was narrowed by a deep neural
network (DNN) based saliency prediction method, which
was achieved by ﬁne-tuning the DNNs model with respect
to an objective function based upon the saliency evalua-
tion metrics and integrating information at different image
scales. Subsequently, Cornia et al. [5] proposed a saliency
attentive model for ﬁxations prediction on natural images.
In this study, an attentive Conv-LSTM model was designed
to sequentially enhance saliency prediction.

For video saliency prediction, previous research works
focus on exploiting the saliency relevant feature representa-
tions from spatial and temporal perspectives. For instance,
Tu et al. [29] proposed a video saliency detection method
in the compression domain on the basic of discrete cosine
transformation (DCT) coefﬁcients and motion information.
In [29], H.264/AVC video bitstream was used to extract the
corresponding information. Xu et al. [34] designed a learn-
ing based video saliency model that utilizes the support
vector machine to fuse spatio-temporal features extracted
by High Efﬁciency Video Coding (HEVC). Kalboussi et
al. [20] introduced a video saliency model that integrates
a static map and dynamic map by the Gestalt principle of
ﬁgure-ground segregation.
In that study, a dense optical
ﬂow was used to represent the motion information. In order

9750

Figure 2. The overall architecture of the proposed stereoscopic video saliency prediction model.

to avoid the time-consuming optical ﬂow calculation, Wang
et al. [32] proposed a video saliency model based on fully
convolutional networks that is capable of directly producing
the spatio-temporal saliency inference by incorporating the
spatial saliency estimation into the dynamic saliency model.

With the development of stereoscopic display technolo-
gies, it is necessary to study visual saliency in binocu-
lar domain. For example, Bruce et al. [2] proposed a
stereo saliency model by expanding the existing 2D at-
tention model to the binocular domain. Region-of-interest
(ROI) extraction method was proposed for adaptive render-
ing as well [4]. Chamaret et al. used the disparity infor-
mation to weight 2D saliency map for producing saliency
map of stereoscopic image [4].
In addition, Potapova et
al. [25] proposed a stereoscopic saliency detection model by
integrating the top-down cues into the bottom-up saliency
detection model. Eye tracking experiments were also per-
formed on 2D and 3D images for depth saliency analysis in
[23], in which stereoscopic saliency map was computed by
extending the previous 2D attention models. More specif-
ically, stereo saliency of an image region was computed
based on the distance between its perceived location and the
comfort zone. Fang et al. [8] proposed to calculate the con-
trast among the color, intensity, texture, and depth features
to produce stereoscopic saliency maps. Zhang et al. [35]
proposed a deep learning based visual saliency model for
3D image. In that study, color and depth features were ex-
tracted by a pretrained convolutional neural network model
to infer saliency value of regions. Kim et al. [21] proposed
a stereoscopic video saliency model that generates the ﬁnal
saliency map by involving low-level features, motion and
depth attributes as well as the high level scene type.

In essence, automatically predicting the saliency for
stereoscopic videos is a very challenging task, especially

when considering the sophisticated interactions between
multiple clues such as spatial, temporal and depth infor-
mation. The traditional hand-crafted features adopted in
the existing methods largely limit the accuracy of saliency
prediction due to the absence of semantic information.
In this paper, we single out the important contributions
of stereoscopic cues and develop a learning based stereo-
scopic video saliency model based on 2D and 3D convo-
lutional neural networks, leading to enhanced saliency ori-
ented spatio-temporal and depth representations. A Conv-
LSTM based fusion network is designed to ﬁnally produce
ultimate saliency map by exploring the high-level feature
representations, leading to superior performance compared
to the state-of-the-art methods.

3. The Proposed Model

As shown in Fig. 2, the architecture of our proposed
model consists of three modules, including pyramid spatio-
temporal coherence based saliency prediction,
intrinsic-
depth saliency estimation and Conv-LSTM based fusion.
More speciﬁcally, the pyramid spatio-temporal coherence
based saliency prediction module ﬁrst generates a series
of temporally consistent saliency maps from consecutive
frames based on 3D ConvNet. Subsequently, the intrinsic-
depth saliency estimation module is applied to obtain the
depth guided saliency map between left and right views
based on the deep ConvNet. Finally, the acquired spatio-
temporal and depth coherence saliency maps are consecu-
tively fed into the Conv-LSTM based fusion network for
the stereoscopic saliency inference.

3.1. Pyramid Spatio Temporal Saliency Prediction

Herein,

the efﬁcient saliency detection
model for stereoscopic videos, we propose a pyramid

to construct

9751

spatio-temporal saliency prediction network termed as
PySTSP-Net to exploit the spatio-temporal intrinsic coher-
ence. The overall architecture is shown in Fig. 2. In partic-
ular, our proposed PySTSP-Net module is based on the 3D
ConvNet [28] that enhances the saliency related feature rep-
resentations among multiple contiguous frames from both
spatial and temporal perspectives, composing of the input
layer, feature encoding and decoding layers evolving from
3D residual blocks. The feature encoding layers consist of
one 3D convolution block with the kernel size 7 × 7 × 3
and four 3D residual blocks, whereas the feature decod-
ing layers are composed of two 3D deconvolution blocks.
Meanwhile, the kernel size of four 3D residual blocks is
3 × 3 × 3, as suggested in [26] that stacked smaller ker-
nels can achieve better classiﬁcation performance compared
with larger kernels. Each convolution layer is followed with
the batch normalization layer and scale layer to speed up the
training convergence process [16].

Compare to the traditional 2D convolutional neural net-
work for spatial feature extraction, the distinct difference of
3D convolutional neural network is to add an extra temporal
dimension in both convolution kernel and input tensor. In
this paper, we pack n consecutive frames (In = {i1, ..., in})
as the input of PySTSP-Net, which represents the frame in-
dex, height and width of the video frame and the number of
channels (denoted as t × h × w × c). The extra temporal
dimension in the 3D convolution kernel enables to capture
temporally consistent visual representations from a segment
of n consecutive frames rather than a single video frame. As
such, the j-th feature map in the i-th 3D convolution layer
is given by

Fi,j = σ BN(γ,β) Xm

wi,j,m ∗ f(i−1),m + bi,j!! ,

(1)
where wi,j,m and bi,j denote the weight and bias of 3D con-
volution kernel connected to the m-th feature map in the
previous convolution layer, and BNγ,β indicates the batch
normalization [16] with the trainable parameters γ and β. σ
represents the nonlinear activation layer.

According to the cognitive research [33], human visual
system follows the coarse to ﬁne strategy in terms of the
scale when viewing natural scenes. After resizing the fea-
tures to the same scale by the 3D deconvolution operation,
we cascade the output features of the third, fourth and ﬁfth
3D residual blocks to construct multi-scale pyramid features
in the PySTSP-Net module,

F p = [F3, F4 ↑2, F5 ↑4],

(2)

where F3, F4 and F5 represent output features of the third,
fourth and ﬁfth 3D residual blocks, respectively. Moreover,
↑x refers to the x times up-sampling operation and F p de-

notes the multi-scale pyramid features. [·] indicates the con-
catenation operation.

Finally, the multi-scale pyramid features are further fed
into feature decoding layers that consist of two 3D decon-
volution blocks, in which the spatio-temporal coherence
guided saliency map can be accurately predicted by eval-
uating local, neighboring and background representations.
The deconvolution layer has recently been adopted to re-
construct features into pixel space in image processing like
semantic segmentation [24], and meanwhile it also plays an
important role in upsampling the obtained features to the
original image size. As such, we employ the 3D decon-
volution layers to reconstruct saliency maps by altering the
stride in spatial and temporal dimensions. The relevance
between parameters of 3D deconvolution layer and upsam-
pling factor k is deﬁned as follows,

ks = kd = k ∗ 2 − k%2, ss = ts = k,

p = (k − 1)/2,

(3)

(4)

where ks and kd are the kernel size and depth. The param-
eters ss and ts refer to spatial stride and temporal stride in
3D deconvolution layer, and p denotes additional padding
added to the feature maps. Finally, the reconstructed n con-
tinuous saliency maps Sst = {S1
st, · · · , Sn
st} can be ob-
tained after the two 3D deconvolution blocks.

3.2. Saliency Estimation Based on Intrinsic Depth

The binocular depth is an important cue in determin-
ing the saliency of stereoscopic videos. However, infer-
ring precise depth information based on stereoscopic pairs
is inherently a non-trivial task.
In this work, we design
an intrinsic-depth saliency estimation network (IDSE-Net),
which serves to automatically explore the depth-oriented
saliency between left and right views from the stereoscopic
video. As shown in Fig. 2, our proposed IDSE-Net con-
sists of 2D convolution based feature extraction, displace-
ment correlation calculation between left and right views,
and depth-oriented saliency reconstruction modules.

The IDSE-Net in Fig. 2 ﬁrst produces meaningful fea-
tures of left and right views separately by three convolution
blocks, each of which consists of one convolution layer fol-
lowed by a relu activation layer. The feature maps in the
i-th convolution layer is given by

F l
i = max(0, wi ∗ f l

(i−1) + bi),

F r
i = max(0, wi ∗ f r

(i−1) + bi),

(5)

(6)

i and F r

where F l
i represent the feature maps of left and right
views. The parameters wi and bi denote the weight vector
and bias of the convolution kernel.

The displacement between left and right views is evalu-
ated based on two feature vectors F l
i with a specif-
ically designed correlation layer. This resembles the stereo

i and F r

9752

and 128. Subsequently, the feature vector F n
v is treated
as the input of Conv-LSTM layers to generate the ﬁnal
saliency map. The long-short term correlations between the
input feature vectors are acquired through the memory cells
(M n−1
) of the
two Conv-LSTM layers at the last frame. The LSTM cells
at n-th frame is given by,

) and the hidden states (H n−1

, M n−1

, H n−1

1

2

1

2

I n
m = σ(W h
F n
m = σ(W h
m = σ(W h
m = tanh (W h
m = F n
m = On

On
Gn
M n
H n

i ∗ (H n−1
f ∗ (H n−1
o ∗ (H n−1

m ◦ Qh
m ◦ Qh
m ◦ Qh

i ) + W f
f ) + W f
o ) + W f

i ∗ (F n
f ∗ (F n
o ∗ (F n

v ◦ Qf
v ◦ Qf
v ◦ Qf
g ∗ (F n

i ) + Bi),
f ) + Bf ),
o ) + Bo),

v ◦ Qf

g ) + Bg),

g ∗ (H n−1
m ◦ M n−1
m + I n
m ◦ tanh (M n
m),

g ) + W f
m ◦ Qh
m ◦ Gn
m,

Figure 3. Illustration of the Conv-LSTM based fusion network.

matching which aims to effectively identify the correspond-
ing pixel pairs. The introduced correlation layer is inspired
by [6], aiming to discover displacement D by performing
multiplicative path comparisons between two feature maps,

(7)

(10)

D = c(cid:0)F l

3, F r

3(cid:1) ,

where c(·) indicates the correlation operation [6]. More-
over, the spatial features F L of the left view are further
concatenated with D, and the concatenated feature vector
F c is then fed into a serials of convolution layers with the
kernel size 3 × 3. As such, the depth-oriented saliency fea-
tures F d
s can be obtained. This process can be formulated
as follows,

F c = [F L, D],

F d
s = max(0, wi ∗ F c

i−1 + bi),

(8)

(9)

where F c denotes the concatenated spatial and depth fea-
tures. Finally, the depth-oriented saliency map Sd can be
reconstructed by the deconvolution layers.

3.3. Conv LSTM Based Fusion

The obtained saliency maps from PySTSP-Net and
IDSE-Net are characterized by the individual components
in the stereoscopic video. However, the spatial, temporal
and depth information can jointly determine the human eye
ﬁxation locations with varying degrees [10]. Hence, fus-
ing the obtained saliency maps Sd and Sst is a crucial pro-
cess to achieve the ultimate saliency prediction. Moreover,
due to the dynamic transitions of attention across continu-
ous video frames [19], considering the dynamic coherence
when merging the obtained saliency maps Sd and Sst with
the feature contrast is a meaningful exploration. As such,
we develop a Conv-LSTM based fusion network with the
target of learning to produce the ﬁnal stereoscopic saliency
maps of a video clip. The architecture is illustrated in Fig. 3.
More speciﬁcally, the saliency maps Sn
st for n-th
frame are fed into two convolution blocks after cascading
into a vector V n
c . Each of the convolution block is com-
posed of a convolution layer with the kernel size 3 × 3,
a batch normalization layer, a relu layer and a max pool-
ing layer. The numbers of output channels are set to 256

d and Sn

m, F n

m and On

m and H n

where I n
m denote the gate of input, forget and
output for n-th frame at the m-th Conv-LSTM layer. Gn
m,
M n
m are the candidate memory, memory cell and
o , Qh
hidden state, respectively. Moreover, {Qh
g }
and {Qf
g } are two sets of random masks for
the hidden states and input features before the convolution
operation [19]. Consequently, the two deconvolution layers
with kernel size 4 × 4 are employed to generate ﬁnal stereo-
scopic saliency map Sn
3d for n-th frame by reconstructing
the output hidden states of the last Conv-LSTM layer.

i , Qf

f , Qh

i , Qh

f , Qf

o , Qf

3.4. Implementation Details

The training of proposed whole framework is not in an
end-to-end manner. PySTSPNet and IDSE-Net are sepa-
rately trained using the eye ﬁxation map as the groundtruth,
such that the outputs of these two networks are desired to
be saliency maps. The parameters of these two modules
are ﬁxed during the training of the Conv-LSTM based fu-
sion network. To train the PySTSP-Net and IDSE-Net, we
initialize the kernel parameters of convolution layers by em-
ploying the pretrained models in [28, 6], respectively. The
inv policy [18] is adopted to control the learning rate while
the initialized learning rate is set to be 0.01. We also uti-
lize Adadelta gradient descent (AGD) with momentum 0.9
and a weight decay of 0.0005 to minimize L1 loss between
prediction and eye ﬁxation density map during training pro-
cess.

Regarding the Conv-LSTM based fusion, the kernel pa-
rameters of convolution layers are initialized with a trun-
cated normal distribution. For the training stage, the ini-
tialized learning rate is set to 10−5. The Xavier initializer
is adopted to initialize the kernel parameters in each LSTM
cell, while the memory cells and hidden states are initialized
as zeros. The training model is constrained and updated by
the minimization of the Kullback-Leibler (KL) divergence
based loss function between the prediction and ground truth.

9753

Figure 4. Illustration of complex scenes in our dataset. (a) Clut-
tered background; (b) Low contrast; (c) Multiple objects.

4. Stereoscopic Video Eye Fixation Database

To our best knowledge, only a few eye ﬁxation datasets
for stereoscopic videos [11, 7] are public available, with in
total 84 video sequences included. However, large-scale eye
ﬁxation database for stereoscopic video is critical for learn-
ing the meaningful features with diversity of video con-
tent. Moreover, the SAVAM dataset [11] only presented
the left view to subjects, such that the acquired eye ﬁxa-
tion data cannot be adopted to investigate the stereoscopic
video saliency. In addition, the dataset in [7] did not pro-
vided the left and right gaze point maps that are important
for the research of stereoscopic saliency prediction.

In this paper, we have constructed a new challenging
dataset termed as the SVS dataset. The SVS dataset in-
cludes 175 stereoscopic video sequences with resolution
1920 × 1080, and each stereoscopic sequence is composed
of left and right views. The videos in the dataset span a vari-
ety of real-world scenarios ranging from natural to synthetic
scenes. We also select 77 video sequences from the dataset
in [11] and [7]. During our data collection, the duration of
the video is set randomly instead of strictly ﬁxing identi-
cal duration for all video sequences. Moreover, the dataset
also includes some exceptional circumstances to better re-
ﬂect the real-world scenes, such as low contrast, multiple
objects and cluttered background, and examples are shown
in Fig. 4.

4.1. Procedures of Data Collection

Each stereoscopic video sequence was displayed on a 27-
inch LG screen in 3D left-right pattern with the resolution
of 1920 × 1080. The viewing distance is set to 63cm in our
experiment. The Tobii Pro X3-120 is used to capture the
eye gaze data and the sampling rate is 120 Hz. The frame
rate of each stereoscopic video sequence is 25 fps. Subjects
wearing a pair of passive polarized glasses were allowed
to view the stereoscopic video sequences. Due to the free-

Figure 5. Fixation density annotations from left and right views.

view setting in our experiment, the subjects were allowed
to freely move their heads such that normal human viewing
behaviors could be better simulated.

All the stereoscopic video sequences were randomly di-
vided into eight groups and presented in a random order
for the subjective viewing. Such randomness also ensures
that two similar videos will not play continuously to mini-
mize the impact of dependencies. The calibration test was
performed before playing each group of video sequences.
The subjects were asked to readjust their watching position
to guarantee that they can maintain the position stable while
watching each group of videos. Each video sequence would
provide subjects with a 3-seconds buffer time before play-
ing. During the viewing test, subjects were asked for a 3-
minutes break at the beginning of each video group to avoid
the feeling of fatigue. There were 28 subjects participated
in the experiment ranging from 18-25 years old. Subjects
corrected to normal visual acuity were required to keep their
glasses clean to ensure the accuracy of gaze data. They have
also no experience about saliency prediction and are naive
to the purpose of the experiment.

4.2. Data Processing and Outlier Removal

After collecting the eye gaze points for left and right
views by the eye tracker, we create two gaze point maps
for left and right views by using the coordinates of the eye
ﬁxation locations. Nevertheless, it is quite difﬁcult for the
saliency method to model the consistency of discrete gaze
points. As such, we perform the Gaussian blur on the two
gaze point maps to simulate the nonuniform distribution of
the photoreceptors on the retina. Finally, we can obtain two
ﬁxation density maps as the ground truth for stereoscopic
video saliency prediction. Some samples of the stereo-
scopic video frames and the corresponding ﬁxation density
maps are provided in Fig. 5.

9754

Figure 6. Comparisons of saliency maps generated from the seven different methods. The ground truth is shown in the last column.

5. Experimental Results

5.1. Training and Testing Datasets

is
Our proposed stereoscopic video saliency model
trained and validated on the newly built SVS dataset.
In
particular, the SVS dataset is randomly divided into train-
ing and testing sets according to the ratio of 9:1. For the
PySTSP-Net which deals with one view only, the data from
DHF1K [31] are combined with the training set of SVS
to jointly improve the diversity of training data, such that
860 video sequences in total are used for training. All of
these video sequences are segmented into clips with n con-
secutive frames, denoted as In = {i1, ..., in}. The spatial
resolution of video clips are downsampled into 112 × 112.
We also allow overlaps as the data augmentation method
for training. Moreover, extremely long or short duration of
video clip may affect the exploration of spatio-temporal co-
herence. Empirically, the length of video clip is set to 16 in
our experiment.

Moreover, the extracted training set including 158 video
sequences is utilized to train the IDSE-Net. The left and
right views are fed into the input layer to estimate the in-
trinsic depth. For the Conv-LSTM based fusion network,
the outputs from PySTSP-Net and IDSE-Net modules are
concatenated as the input. It is worth mentioning that the
left eye ﬁxation density map serves as the ground-truth dur-
ing the training stage for all the three modules.

5.2. Comparisons with State of the Art Methods

In this paper, ﬁve metrics are employed to measure the
accuracy and similarity [3] of saliency detection models, in-

cluding two variants of area under the ROC curve (AUC)
(denoted as AUC Jud, AUC Borji), correlation coefﬁcient
(CC), similarity metric (SIM) and normalized scan-path
saliency (NSS).

To validate the performance of the proposed visual
saliency model on stereoscopic videos, we perform the
comparisons between our proposed method with six ex-
isting state-of-the-art saliency detection models, including
Itti et al.’s method [17], GBVS [13], OMCNN [19], Static-
3D [8], LBVS-3D [1] and Dynamic-3D [10]. Among these
models, Itti et al.’s method [17] and GBVS [13] are pro-
posed for 2D static images. The method OMCNN [19] fo-
cuses on the 2D video. The Static-3D [8] is a saliency model
towards 3D stereoscopic images. Besides, LBVS-3D [1]
and Dynamic-3D [10] aim to predict saliency distributions
for stereoscopic videos.

The comparison results on the SVS dataset are shown in
Table 1. The strong competitiveness of our proposed model
against the state-of-art saliency prediction approaches is
clearly observed. Moreover, Static-3D [8] performs rela-
tively better than traditional methods such as Itti et al.’s
method [17] and GBVS [13]. This may be explained by
the extra depth attribute utilized in Static-3D [8], which
also provides useful evidence regarding the necessity of
depth information for stereoscopic image and video.
In
addition, OMCNN [19] is a deep learning based spatio-
temporal video saliency detection model, which obtains bet-
ter performance than other benchmark models except for
our proposed model. This might originate from the fact
that other benchmark models all exploit hand-crafted fea-
tures to encode image content without incorporating com-

9755

Model
Itti et al.’s method [17]
GBVS[13]
OMCNN[19]
Static-3D [8]
LBVS-3D [1]
Dynamic-3D [10]
Proposed-ST
Proposed-Depth
Proposed

AUC Jud ⇑ AUC Borji ⇑

0.7592
0.8547
0.9066
0.8743
0.7376
0.8334
0.8836
0.8444
0.9201

0.7518
0.8268
0.8244
0.8632
0.7248
0.8066
0.8088
0.7903
0.8390

CC ⇑
0.2454
0.3432
0.5184
0.3987
0.2646
0.2987
0.6096
0.4767
0.6339

SIM ⇑
0.2037
0.2640
0.4068
0.2568
0.1784
0.2356
0.4922
0.3704
0.5171

NSS ⇑
1.1173
1.5990
2.6336
1.8490
1.2574
1.4004
3.2075
2.2068
3.2320

Table 1. Performance evaluations on the SVS dataset.

plex semantic features during the saliency inference. Em-
ploying the learning based motion information could also
be another reason that explains the superior performance
of OMCNN [19]. The results in terms of ﬁve evaluation
criteria provided in Table 1 show that our proposed model
achieves the highest performance, which demonstrates that
our learning based saliency method is capable of produc-
ing saliency distributions for stereoscopic videos better than
other benchmarks.

To better illustrate the advantages of our proposed
model, we provide comparison results with the state-of-art
saliency models in terms of saliency maps, which are de-
picted in Fig. 6. In particular, our proposed model accu-
rately predicts the human eye ﬁxation locations than other
saliency models. We can also discover that traditional 2D
static methods (Itti et al.’s method [17] and GBVS [13])
only detect the blurred outline information of salient ob-
jects. Besides, we can clearly see that some background
information is mistaken as saliency in these two models.
For the learning based 2D dynamic method OMCNN [19],
the prediction accuracy is still lacking. This also veriﬁes
that depth information can affect human eye ﬁxation lo-
cations. As shown in Fig. 6, it is obvious that the Static-
3D [8], LBVS-3D [1] and Dynamic-3D [10] may treat the
background information as salient. By contrast, our pro-
posed model learns to combine spatio-temporal and intrin-
sic depth saliency distributions to produce more accurate
saliency maps than other models.

5.3. Ablation Study

We perform ablation study to evaluate the relative im-
pact of each component in our proposed model. In Table 1,
we report ﬁve evaluation metrics to estimate these com-
ponents, including only spatio-temporal component (de-
noted as Proposed-ST), only depth component (denoted as
Proposed-Depth) and our proposed full version model. We
can discover that the performance of Proposed-ST is bet-
ter than Proposed-Depth. As such, color and motion infor-
mation could play more important roles in predicting the
stereoscopic video saliency. However, this does not im-

ply that depth information is incompetent during the stereo-
scopic video saliency inference process. Compared with the
performance of Proposed-ST and Proposed-Depth, our pro-
posed full version model achieves better performance than
the models contain only single component. This suggests
that color, motion and depth information interact to inﬂu-
ence the ﬁnal prediction performance for stereoscopic video
saliency.

6. Conclusion

In this paper, we propose a learning based visual atten-
tion model for stereoscopic videos by singling out intrinsic
cues in terms of spatio-temporal, intrinsic-depth attributes,
as well as the interaction between each other. In order to
acquire the saliency from spatio-temporal perspective, we
design a pyramid 3D ConvNet to investigate saliency dis-
tributions from spatial and temporal feature channels. Im-
plicitly estimating depth indication between left and right
views also enables our proposed model to effectively in-
fer saliency inﬂuenced by depth information. The ultimate
saliency map is predicted by combining the saliency distri-
butions in spatio-temporal and depth cues by a Conv-LSTM
based fusion network. Experimental results demonstrate
that our proposed model outperforms all existing state-of-
art saliency detection algorithms on the newly built SVS
dataset.

Acknowledgement

This work was supported in part by the National Natural
Science Foundation of China under Grant 61871270, 61672443
and 61620106008, in part by the Hong Kong RGC Early Ca-
reer Scheme under Grant 9048122 (CityU 21211018), in part by
the Guangdong Nature Science Foundation of China under Grant
2016A030310058, in part by the Natural Science Foundation of
SZU (grant no. 827000144), and in part by the National Engi-
neering Laboratory for Big Data System Computing Technology
of China.

9756

References

[1] A. Banitalebi-Dehkordi, M. T. Pourazad, and P. Nasiopou-
los. A learning-based visual saliency prediction model for
stereoscopic 3D video (LBVS-3D). Multimedia Tools and
Applications, 76(22):23859–23890, 2017. 7, 8

[2] N. D. Bruce and J. K. Tsotsos. An attentional framework for
stereo vision. In Proc. 2nd IEEE Canadian Conf. Comput.
Robot Vis., pages 88–95, 2005. 3

[3] Z. Bylinskii, T. Judd, A. Oliva, A. Torralba, and F. Durand.
What do different evaluation metrics tell us about saliency
models? IEEE Trans. Pattern Anal. Mach. Intell., 41(3):740–
757, 2019. 7

[4] C. Chamaret, S. Godeffroy, P. Lopez, and O. Le Meur. Adap-
tive 3D rendering based on region-of-interest. 7524:75240V,
2010. 3

[5] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara. Predict-
ing human eye ﬁxations via an LSTM-based saliency atten-
tive model. IEEE Trans. Image Process., 27(10):5142–5154,
2018. 1, 2

[6] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,
V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.
Flownet: Learning optical ﬂow with convolutional networks.
In Proc. ICCV, pages 2758–2766, 2015. 5

[7] Y. Fang, J. Wang, J. Li, R. P´epion, and P. Le Callet. An
eye tracking database for stereoscopic video.
In Proc. Int.
Workshop Quality Multimedia Exper., pages 51–52, 2014. 6
[8] Y. Fang, J. Wang, M. Narwaria, P. Le Callet, and W. Lin.
Saliency detection for stereoscopic images. IEEE Trans. Im-
age Process., 23(6):2625–2636, 2014. 3, 7, 8

[9] Y. Fang, Z. Wang, W. Lin, and Z. Fang. Video saliency in-
corporating spatiotemporal cues and uncertainty weighting.
IEEE Trans. Image Process., 23(9):3910–3921, 2014. 1

[10] Y. Fang, C. Zhang, J. Li, J. Lei, M. P. Da Silva, and P.
Le Callet. Visual attention modeling for stereoscopic video:
a benchmark and computational model. IEEE Trans. Image
Process., 26(10):4684–4696, 2017. 2, 5, 7, 8

[11] Y. Gitman, M. Erofeev, D. Vatolin, B. Andrey, and F. Alexey.
Semiautomatic visual-attention modeling and its application
to video compression.
In Proc. ICIP, pages 1105–1109,
2014. 6

[12] S. Goferman, L. Zelnik-Manor, and A. Tal. Context-aware
saliency detection. IEEE Trans. Pattern Anal. Mach. Intell,
34(10):1915–1926, 2012. 2

[13] J. Harel, C. Koch, and P. Perona. Graph-based visual

saliency. In Proc. NIPS, pages 545–552, 2007. 2, 7, 8

[14] X. Hou and L. Zhang. Saliency detection: A spectral residual

approach. In Proc. CVPR, pages 1–8, 2007. 2

[15] X. Huang, C. Shen, X. Boix, and Q. Zhao. Salicon: Reduc-
ing the semantic gap in saliency prediction by adapting deep
neural networks. In Proc. ICCV, pages 262–270, 2015. 1, 2
[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
Proc. ICML, pages 448–456, 2015. 4

[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R.
Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. In Proceed-
ings of the 22nd ACM international conference on Multime-
dia, pages 675–678, 2014. 5

[19] L. Jiang, M. Xu, T. Liu, M. Qiao, and Z. Wang. Deepvs: A
deep learning based video saliency prediction approach. In
Proc. ECCV, pages 602–617, 2018. 5, 7, 8

[20] R. Kalboussi, M. Abdellaoui, and A. Douik. A spatiotempo-
ral model for video saliency detection. In Proc. IEEE Conf.
IPAS, pages 1–6, 2016. 2

[21] H. Kim, S. Lee, and A. C. Bovik.

Saliency predic-
IEEE Trans. Image Process.,

tion on stereoscopic videos.
23(4):1476–1490, 2014. 3

[22] M. K¨ummerer, L. Theis, and M. Bethge. Deep gaze i: Boost-
ing saliency prediction with feature maps trained on ima-
genet. Proc. ICLR Workshop, 2015. 2

[23] C. Lang, T. V. Nguyen, H. Katti, K. Yadati, M. Kankanhalli,
and S. Yan. Depth matters: Inﬂuence of depth cues on visual
saliency. In Proc. ECCV, pages 101–115, 2012. 3

[24] H. Noh, S. Hong, and B. Han. Learning deconvolution
In Proc. ICCV, pages

network for semantic segmentation.
1520–1528, 2015. 4

[25] E. Potapova, M. Zillich, and M. Vincze. Learning what mat-
ters: combining probabilistic models of 2D and 3D saliency
cues. Proc. 8th Int. Comput. Vis. Syst., pages 132–142, 2011.
3

[26] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. Proc. ICLR,
2015. 4

[27] Y. Tang, L. Ma, W. Liu, and W. Zheng. Long-Term human
motion prediction by modeling motion context and enhanc-
ing motion dynamics. Proc. IJCAI, pages 935–941, 2018.
1

[28] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3D convolutional net-
works. In Proc. ICCV, pages 4489–4497, 2015. 4, 5

[29] Q. Tu, A. Men, Z. Jiang, F. Ye, and J. Xu. Video saliency
detection incorporating temporal information in compressed
domain. Signal Process. Image Commun., 38:32–44, 2015.
2

[30] E. Vig, M. Dorr, and D. Cox. Large-scale optimization of hi-
erarchical features for saliency prediction in natural images.
In Proc. CVPR, pages 2798–2805, 2014. 2

[31] W. Wang, J. Shen, F. Guo, M.-M. Cheng, and A. Borji. Re-
visiting video saliency: A large-scale benchmark and a new
model. In Proc. CVPR, pages 4894–4903, 2018. 7

[32] W. Wang, J. Shen, and L. Shao. Video salient object detec-
IEEE Trans. Image

tion via fully convolutional networks.
Process., 27(1):38–49, 2018. 3

[33] R. Watt. Scanning from coarse to ﬁne spatial scales in the
human visual system after the onset of a stimulus. Journal of
the Optical Society of America, 4(10):2006–2021, 1987. 4

[17] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
visual attention for rapid scene analysis. IEEE Trans. Pattern
Anal. Mach. Intell., 20(11):1254–1259, 1998. 2, 7, 8

[34] M. Xu, L. Jiang, X. Sun, Z. Ye, and Z. Wang. Learning
to detect video saliency with HEVC features. IEEE Trans.
Image Process., 26(1):369–385, 2017. 2

9757

[35] Q. Zhang, X. Wang, J. Jiang, and L. Ma. Deep learning
features inspired saliency detection of 3D images. In Paciﬁc
Rim Conference on Multimedia, pages 580–589, 2016. 3

[36] S. Zhong, Y. Liu, F. Ren, J. Zhang, and T. Ren. Video
saliency detection via dynamic consistent spatio-temporal at-
tention modelling. In Proc. AAAI, pages 1063–1069, 2013.
1

9758

