Argoverse: 3D Tracking and Forecasting with Rich Maps

Ming-Fang Chang∗ 1,2, John Lambert∗1,3, Patsorn Sangkloy∗1,3, Jagjeet Singh∗1, Sławomir B ˛ak1,
Andrew Hartnett1, De Wang1, Peter Carr1, Simon Lucey1,2, Deva Ramanan1,2, and James Hays1,3

1Argo AI, 2Carnegie Mellon University, 3Georgia Institute of Technology

Figure 1: We introduce a dataset for 3D tracking and forecasting with rich maps for autonomous driving. Our dataset
contains sequences of LiDAR measurements, 360◦ RGB video, front-facing stereo (middle-right), and 6-dof localization.
All sequences are aligned with maps containing lane center lines (magenta), driveable region (orange), and ground height.
Sequences are annotated with 3D cuboid tracks (green). A wider map view is shown in the bottom-right.

Abstract

We present Argoverse, a dataset designed to support au-
tonomous vehicle perception tasks including 3D tracking
and motion forecasting. Argoverse includes sensor data
collected by a ﬂeet of autonomous vehicles in Pittsburgh
and Miami as well as 3D tracking annotations, 300k ex-
tracted interesting vehicle trajectories, and rich semantic
maps. The sensor data consists of 360◦ images from 7 cam-
eras with overlapping ﬁelds of view, forward-facing stereo
imagery, 3D point clouds from long range LiDAR, and 6-dof
pose. Our 290km of mapped lanes contain rich geometric
and semantic metadata which are not currently available
in any public dataset. All data is released under a Cre-
ative Commons license at Argoverse.org.
In baseline ex-
periments, we use map information such as lane direction,
driveable area, and ground height to improve the accuracy
of 3D object tracking. We use 3D object tracking to “mine”
for more than 300k interesting vehicle trajectories to cre-
ate a trajectory forecasting benchmark. Motion forecast-
ing experiments ranging in complexity from classical meth-
ods (k-NN) to LSTMs demonstrate that using detailed “vec-
tor maps” with lane-level information substantially reduces

*Equal contribution

prediction error. Our tracking and forecasting experiments
represent only a superﬁcial exploration of the potential of
rich maps in robotic perception. We hope that Argoverse
will enable the research community to explore these prob-
lems in greater depth.

1. Introduction

Datasets and benchmarks for a variety of perception
tasks in autonomous driving have been hugely inﬂuential
to the computer vision community over the last few years.
We are particularly inspired by the impact KITTI [10] has
had in opening new research directions. However, pub-
licly available datasets for autonomous driving rarely in-
clude map data, even though detailed maps are critical to
the development real world autonomous systems. Publicly
available maps, e.g. OpenStreetMap, can be useful, but
have limited detail and accuracy.

Intuitively, 3D scene understanding would be easier if
maps directly told us which 3D points belong to the road,
which belong to static buildings, what lane a tracked ob-
ject is in, what the speed limit for that lane is, how far it
is to the next intersection, etc. But since publicly available
datasets don’t contain such rich mapped attributes it is an

18748

open research question of how to represent and utilize these
features. Argoverse is the ﬁrst large autonomous driving
dataset with such detailed maps. We examine the potential
utility of these new map features on two tasks – 3D tracking
and motion forecasting, and we offer a signiﬁcant amount
of real-world, annotated data to enable new benchmarks for
these problems.

Our contributions in this paper include:
• We release a large scale dataset with synchronized data
from LiDAR, 360◦ and stereo cameras sampled across
two cities and varied conditions.

• We provide ground truth tracking annotation of objects
in 3D, with ten times more tracks than the KITTI [10]
tracking benchmark.

• We create a large scale forecasting benchmark of tra-
jectories capturing scenarios like turns at intersections,
driving with many vehicles nearby, and lane changes.
• We release map data and an API which can be used
to develop map-based perception algorithms. To our
knowledge, there is no publicly available equivalent
to our semantic vector map of road infrastructure and
trafﬁc rules.

• We examine the inﬂuence of map context in 3D track-

ing and trajectory forecasting.

• We release the ﬁrst large-scale dataset suitable for
training and benchmarking automatic map creation,
often known as map automation.

• We release the ﬁrst fully panoramic, high-frame rate
large-scale dataset collected outdoors on a vehicle,
opening new possibilities for city-scale reconstruction
with photometric-based direct methods.

2. Related Work
Autonomous Driving Datasets with Map Information.
Until recently,
it was rare to ﬁnd datasets that provide
detailed map information associated with annotated data.
Works such as TorontoCity [36] and ApolloScape [18] fo-
cus on map construction tasks but without 3D annotation for
dynamic objects. The nuScenes dataset [5] contains maps in
the form of binary, rasterized, top-down indicators of region
of interest (where region of interest is the union of drive-
able area and sidewalk). This map information is provided
for 1000 annotated vehicle log segments (or “scenes”) in
Singapore and Boston. Like nuScenes, Argoverse includes
maps of driveable area, but we also include ground height
and a “vector map” of lane centerlines and their connectiv-
ity.
Autonomous Driving Datasets with 3D Tracking Anno-
tations. Many existing datasets for object tracking focus on
pedestrian tracking from image/video sequences [32, 28, 2].
Several datasets provide raw data from self driving car sen-
sors, but without any object annotations [27, 30, 33]. The

ApolloCar3D dataset [34] is oriented toward 3D semantic
object keypoint detection instead of tracking. KITTI [10]
and H3D [31] offers 3D bounding box and track annotations
but does not provide a map and the camera ﬁeld of view is
frontal, rather than 360◦. nuScenes [5] currently provides
360◦ data and a benchmark for 3D object detection, with
tracking annotation also available. The Argoverse-Tracking
dataset contains 360◦ track annotations in 3D space aligned
with detailed map information. See Table 1 for a compari-
son between 3D autonomous vehicle datasets.
Autonomous Driving Datasets with Mined Trajectory
Data. TrafﬁcPredict [26] also uses sensor-equipped vehi-
cles to observe driving trajectories in the wild and build a
forecasting benchmark. The TrafﬁcPredict dataset consists
of 155 minutes of observations compared to 320 hours of
observations in Argoverse.
Using Maps for Self-driving Tasks. While high deﬁnition
(HD) maps are widely used by motion planning systems,
few works explore the use of this strong prior in percep-
tion systems [38] despite the fact that the three winning
entries of the 2007 DARPA Urban Challenge relied on a
DARPA-supplied map – the Route Network Deﬁnition File
(RNDF) [29, 35, 3]. Hecker et al. [13] show that end-to-
end route planning can be improved by processing raster-
ized maps from OpenStreetMap and TomTom. Liang et
al. [22] demonstrate that using road centerlines and inter-
section polygons from OpenStreetMap can help infer cross-
walk location and direction. Yang et al.
[38] show that
incorporating ground height and road segment into LiDAR
points can improve 3D object detection. Suraj et al. [25] use
dashboard-mounted monocular cameras on a ﬂeet of vehi-
cles to build a 3D map via city-scale structure-from-motion
for localization of ego-vehicles and trajectory extraction.
3D Object Tracking. In traditional approaches for point
cloud tracking, segments of points can be accumulated us-
ing clustering algorithms such as DBSCAN [9, 20] or con-
nected components of an occupancy grid [21, 17], and
then associated based on some distance function using
the Hungarian algorithm. Held et al. utilize probabilis-
tic approaches to point cloud segmentation and tracking
[14, 16, 15]. Recent work demonstrates how 3D instance
segmentation and 3D motion (in the form of 3D scene ﬂow,
or per-point velocity vectors) can be estimated directly on
point cloud input with deep networks [37, 23]. Our dataset
enables 3D tracking with sensor fusion in a 360◦ frame.
Trajectory Forecasting: Spatial context and social inter-
actions can inﬂuence the future path of pedestrians and cars.
Social-LSTM[1] proposes a novel pooling layer to capture
social interaction of pedestrians. Social-GAN [11] attempts
to model the multimodal nature of the predictions. How-
ever, both have only been tested on pedestrian trajectories,
with no use of static context (e.g. a map). Deo et al. [8]
propose a convolutional social pooling approach wherein

28749

DATASET NAME

KITTI [10]

Oxford RobotCar [27]

H3D [31]

nuScenes v1.0 [5]

Argoverse-Tracking-Beta

(human annotated)

Argoverse-Forecasting

(mined trajectories)

MAP
TYPE

None
None
None
Raster

Vector
+Raster

Vector
+Raster

EXTENT OF
ANNOTATED

LANES

DRIVEABLE

AREA

COVERAGE

0 km
0 km
0 km
0 km

0 m2
0 m2
0 m2

1,115,844 m2

CAMERA
FRAME
RATE

10 Hz
11/16Hz
30 Hz
12 Hz

204 km (MIA)
+86 km (PIT)

204 km (MIA)
+86 km (PIT)

1,074,614 m2

30 Hz

1,074,614 m2

-

360◦

CAMERAS

INCLUDE
STEREO

# TRACKED
OBJECTS

no
no
no
X

X

no

X
no
no
no

X

no

917
0

13,763
64,386

10,572

16.4M

Table 1: Public self-driving datasets. We compare recent, publicly available self-driving datasets with 3D object annotations
for tracking. Coverage area for nuScenes is based on its road and sidewalk raster map. Argoverse coverage area is based on
our driveable area raster map.

they ﬁrst predict the maneuver and then the trajectory con-
ditioned on that maneuver. In the self-driving domain, the
use of spatial context is of utmost importance and it can
be efﬁciently leveraged from the maps. Chen et al. [7] use
a feature-driven approach for social and spatial context by
mapping the input image to a small number affordances of
a road/trafﬁc state. However, they limit their experiments
to a simulation environment. IntentNet [6] extends the joint
detection and prediction approach of Luo et al. [24] by dis-
cretizing the prediction space and attempting to predict one
of eight common driving maneuvers. DESIRE [19] demon-
strates a forecasting model capturing both social interaction
and spatial context. The authors note that the beneﬁts from
these two additional components are small on the KITTI
dataset, attributing this to the minimal inter-vehicle interac-
tions in the data.

3. The Argoverse Dataset

Our sensor data, maps, and annotations are the primary
contribution of this work. We also develop an API which
helps connect the map data with sensor information e.g.
ground point removal, nearest centerline queries, and lane
graph connectivity; see Supplemental Material for more de-
tails. Our data, annotations, and API are available under a
Creative Commons license at Argoverse.org.

We collect raw data from a ﬂeet of autonomous vehi-
cles in Pittsburgh, Pennsylvania, USA and Miami, Florida,
USA. These cities have distinct climate, architecture, in-
frastructure, and behavior patterns. The captured data spans
different seasons, weather conditions, and times of day. The
data used for our dataset traverses nearly 300km of mapped
road lanes and comes from a subset of our ﬂeet operating
area.
Sensors. Our cars are equipped with two roof-mounted
VLP-32 LiDAR sensors with an overlapping 40◦ vertical
ﬁeld of view and a range of 200m, roughly twice that as the
sensors used in nuScenes and KITTI. On average, our Li-

Figure 2: 3D visualization of an Argoverse scene. Left:
we accumulate LiDAR points and project them to a virtual
image plane. Right: using our map, LiDAR points beyond
driveable area are dimmed and points near the ground are
highlighted in cyan. Cuboid object annotations and road
center lines are shown in pink and yellow.

DAR sensors produce a point cloud at each sweep with three
times the density of the LiDAR sweeps in the nuScenes [5]
dataset (ours ∼ 107, 000 points vs. nuScenes’ ∼ 35, 000
points). The vehicles have 7 high-resolution ring cameras
(1920 × 1200) recording at 30 Hz with overlapped ﬁeld of
view providing 360◦ coverage. In addition there are 2 front-
facing stereo cameras (2056×2464) sampled at 5 Hz. Faces
and license plates are procedurally blurred in camera data
to maintain privacy. Finally, 6-DOF localization for each
timestamp comes from a combination of GPS-based and
sensor-based localization. Vehicle localization and maps
use a city-speciﬁc coordinate system described in more de-
tail in the Supplemental Material. Sensor measurements for
particular driving sessions are stored in “logs”, and we pro-
vide intrinsic and extrinsic calibration data for the LiDAR
sensors and all 9 cameras for each log. Figure 2 visualizes
our sensor data in 3D. Similar to [33], we place the origin
of the vehicle coordinate system at the center of the rear
axle. All sensors are roof-mounted, with a LiDAR sensor
surrounded by 7 “ring” cameras (clockwise: facing front
center, front right, side right, rear right, rear left, side left,
and front left) and 2 stereo cameras. Figure 3 visualizes the
geometric arrangement of our sensors.

38750

Figure 3: Car sensor schematic. Three reference coordi-
nate systems are displayed: (1) the vehicle frame, with Xv
forward, Yv left, and Zv up, (2) the camera frame, with Xc
across imager, Yc down imager, and Zc along optical axis.
(3) the LiDAR frame, with XL forward, YL left, and ZL up.
Positive rotations RX , RY , RZ are deﬁned for each coordi-
nate system as rotation about the respective axis following
the right-hand rule.

3.1. Maps

Argoverse contains three distinct maps – (1) a vector
map of lane centerlines and their attributes, (2) a rasterized
map of ground height, and (3) a rasterized map of driveable
area and region of interest (ROI).
Vector Map of Lane Geometry. Our vector map consists
of semantic road data represented as a localized graph rather
than rasterized into discrete samples. The vector map we
release is a simpliﬁcation of the map used in ﬂeet opera-
tions. In our vector map, we offer lane centerlines, split into
lane segments. We observe that vehicle trajectories gener-
ally follow the center of a lane so this is a useful prior for
tracking and forecasting.

A lane segment is a segment of road where cars drive
in single-ﬁle fashion in a single direction. Multiple lane
segments may occupy the same physical space (e.g. in an
intersection). Turning lanes which allow trafﬁc to ﬂow in
either direction would be represented by two different lanes
that occupy the same physical space.

For each lane centerline, we provide a number of seman-
tic attributes. These lane attributes describe whether a lane
is located within an intersection or has an associated trafﬁc
control measure (Boolean values that are not mutually in-
clusive). Other semantic attributes include the lane’s turn
direction (left, right, or none) and the unique identiﬁers for
the lane’s predecessors (lane segments that come before)
and successors (lane segments that come after) of which
there can be multiple (for merges and splits, respectively).
Centerlines are provided as “polylines”, i.e. a sequence of
straight segments. Each straight segment is deﬁned by 2
vertices: (x, y, z) start and (x, y, z) end. Thus, curved lanes
are approximated with a set of straight lines.

We observe that in Miami, lane segments that could be
used for route planning are on average 3.84m ± 0.89 wide.

Figure 4: Map-based ground removal example. Some Ar-
goverse scenes contain uneven ground, which is challenging
to remove with simple heuristics (e.g. assuming ground is
planar). Above, the projected LiDAR points are colored by
surface normal. The ground surface normal color is non-
uniform in the birds-eye-view projection (left). The green
color on the slope (middle column) differs from other parts
of ground (right column). The lower row uses our map tools
to remove ground points and points beyond driveable area.

In Pittsburgh, the average width is 3.97m ± 1.04 in width.
Other types of lane segments that would not be suitable for
self-driving, e.g. bike lanes, can be as narrow as 0.97m in
Miami and as narrow as 1.06m in Pittsburgh.
Rasterized Driveable Area Map. Our maps include binary
driveable area labels at 1 meter grid resolution. A driveable
area is an area where it is possible for a vehicle to drive
(though not necessarily legal). Driveable areas can encom-
pass a road’s shoulder in addition to the normal driveable
area that is represented by a lane segment. Our track annota-
tions (Section 3.2) extend to 5 meters beyond the driveable
area. We call this larger area our region of interest (ROI).
Rasterized Ground Height Map. Finally, our maps in-
clude real-valued ground height at 1 meter resolution.
Knowledge of ground height can be used to remove LiDAR
returns on static ground surfaces and thus makes the 3D de-
tection of dynamic objects easier. Figure 4 demonstrates the
use of our ground height map to remove LiDAR points on
the road.

3.2. 3D Track Annotations

Argoverse-Tracking-Beta1 contains 100 vehicle log seg-
ments with human-annotated data 3D tracks. These 100
segments vary in length from 15 to 60 seconds and collec-
tively contain 10,572 tracked objects. We compare this to
other datasets in Table 1. For each log segment, we annotate
all objects of interest (both dynamic and static) with bound-
ing cuboids which follow the 3D LiDAR returns associated
with each object over time. We only annotate objects within
5 meters of the driveable area as deﬁned by our map. For
objects that are not visible for the entire segment duration,
tracks are instantiated as soon as the object becomes vis-
ible in the LiDAR point cloud and tracks are terminated

1We refer to our tracking data as beta in anticipation of minor reﬁne-

ments or expansion to this dataset before ﬁnal benchmark release.

48751

Figure 5: Distribution of object classes. This plot shows,
in log scale, the number of objects annotated for each class
in the 100 log segments in Argoverse-Tracking-Beta.

when the object ceases to be visible. We mark objects as
“occluded” whenever they become invisible within the se-
quence. Each object is labeled with one of 17 categories, in-
cluding OTHER_STATIC and OTHER_MOVER for static
and dynamic objects that do not ﬁt into other predeﬁned cat-
egories. More than 70% of tracked objects are vehicles, but
we also observe pedestrians, bicycles, mopeds, and more.
Figure 5 show the distribution of classes for annotated ob-
jects. All track labels pass through a manual quality as-
surance review process. Figures 1 and 2 show qualitative
examples of our human annotated labels. We divide our an-
notated tracking data into 60 training, 20 validation, and 20
testing sequences.

3.3. Mined Trajectories for Motion Forecasting

We are also interested in studying the task of motion fore-
casting in which we predict the location of a tracked object
some time in the future. Motion forecasts can be critical
to safe autonomous vehicle motion planning. While our
human-annotated 3D tracks are suitable training and test
data for motion forecasting, the motion of many of vehicles
is relatively uninteresting – in a given frame, most cars are
either parked or traveling at nearly constant velocity. Such
tracks are hardly a representation of real forecasting chal-
lenges. We would like a benchmark with more diverse sce-
narios e.g. managing an intersection, slowing for a merg-
ing vehicle, accelerating after a turn, stopping for a pedes-
trian on the road, etc. To sample enough of these inter-
esting scenarios we track objects from 1006 driving hours
across both Miami and Pittsburgh and ﬁnd vehicles with in-
teresting behavior in 320 of those hours. In particular, we
look for vehicles that are either (1) at intersections (2) tak-
ing left or right turns (3) changing to adjacent lanes or (4)
in dense trafﬁc. In total, we collect 333,441 ﬁve second se-
quences and use them in the forecasting benchmark. Each
sequence contains the 2D, birds-eye-view centroid of each
tracked object sampled at 10hz. Figure 6 shows the geo-

Figure 6: Distribution of mined trajectories. The colors
indicate the number of mined trajectories across the maps
of Miami (left) and Pittsburgh (right). The heuristics to ﬁnd
interesting vehicle behavior lead to higher concentrations
in intersections and on busy roads such as Liberty and Penn
Ave (southeast roads in bottom right inset).

graphic distribution of these sequences. In Section 5, we
do not evaluate motion forecasts for pedestrians and sta-
tionary vehicles, but still retain their trajectories for context
in “social” forecasting models. The 333,441 sequences are
split into 211,691 train, 41,146 validation, and 80,604 test
sequences. Each sequence has one challenging trajectory
which is the focus of our forecasting benchmark. The train,
val, and test sequences are taken from disjoint parts of our
cities, i.e. roughly one eighth and one quarter of each city
is set aside as validation and test data, respectively. This
dataset is far larger than what could be mined from publicly
available autonomous driving datasets and we have the ad-
vantage of using our maps to make it easier to track objects.
While data of this scale is appealing because it allows us to
see rare behaviors and train complex models, it is too large
to exhaustively verify the accuracy of the mined trajectories
and thus there is some noise and error inherent in the data.

4. 3D Object Tracking

In this section, we examine how various baseline track-
ing methods perform on the Argoverse 3D tracking bench-
mark. Our baseline methods are LiDAR-centric and operate
directly in 3D. In addition to measuring the baseline difﬁ-
culty of our benchmark, we measure how some simple map-
based heuristics can inﬂuence tracking accuracy. For these
baselines, our tracking and evaluation is limited to vehicles
only.

Given a sequence of F frames, each frame contains set
of 3D points from LiDAR {Pi | i = 1, ..., N }, where Pi
∈ R3 of x, y, z coordinates, we want to determine a set
of track hypothesis {Tj
| j = 1, ..., n} where n is the
number of unique objects in the whole sequence, and Tj
contains the set of object center locations at frames f for
f = {fstart, ..., fend}, the range of frames where the ob-
ject is visible. We usually have a dynamic observer as our

58752

>3500 3000 2500 20001500 1000 <500 >3500 3000 2500 20001500 1000 <500 car is in motion more often than not. The tracked vehicles
in the scene around us can be static or moving.

Our baseline tracking pipeline clusters LiDAR returns to
detect potential objects, uses Mask R-CNN [12] to prune
non-vehicle LiDAR returns, associates clusters over time
using the Hungarian algorithm, estimating transformations
between clusters with ICP, and estimates vehicle pose with
a Kalman Filter. More details are provided in the Supple-
mental Material.

The tracker uses the following map attributes:

Driveable area. Since our baseline is focused on vehicle
tracking, we constrain our tracker to the driveable area as
speciﬁed by the map. This covers any region where it is
possible for vehicle to drive (see Section 3.1). This reduces
the opportunities for false positives.
Ground removal. We use map information to perform
ground-removal. In contrast to local ground-plane estima-
tion methods, the map-based approach is effective in slop-
ing and uneven environments.
Lane Direction. Determining the vehicle orientation from
LiDAR alone is a challenging task even for humans due to
LiDAR sparsity and partial views. We observe that vehicle
orientation rarely violates lane direction, especially so out-
side of intersections. Fortunately, such information is avail-
able in our dataset, so we adjust vehicle orientation based on
lane direction whenever the vehicle is not at the intersection
and contains too few LiDAR points.

4.1. Evaluation

We use standard evaluation metrics commonly used for
multiple object trackers (MOT) [28, 4]. The MOT metric re-
lies on a distance/similarity function between ground truth
and predicted objects to determine an optimal assignment.
Instead of IoU (Intersection-over-Union) which is more
commonly used in tracking literature, we use Euclidean dis-
tance between object centroids (threshold for missed track
at 2.25 meters, which is half of an average family car length
in US). We follow the original deﬁnition in CLEAR MOT
[4] for MOTP (the lower the better). The tracking metrics
are explained in the Supplementary Material in detail.

In the experiments, we run our tracker over the 20 logs
in the Argoverse-Tracking-Beta test set. We are also inter-
ested in the relationship between tracking performance and
distance. We apply a threshold (30,50,100 m) to the dis-
tance between vehicles and our ego vehicle and only eval-
uate annotations and tracker output within that range. The
results in Table 2 show that our tracker performs quite well
at short range where the LiDAR sampling density is higher,
but struggles for objects beyond 50 meters.

We compare our baseline tracker with three ablations
that exclude: 1) Mask R-CNN as pre-ﬁltering for LiDAR
2) lane direction information from the map and 3) map-
based ground removal. The results in Table 2 show that
Mask-RCNN dramatically improves our detection perfor-

(a) without lane information

(b) with lane information

Figure 7: Tracking with orientation snapping. Using lane
direction information helps to determine the vehicle orien-
tation for detection and tracking. Ground truth cuboids are
green.

mance by reducing false positives. Map-based ground re-
moval leads to slightly better detection performance (higher
MOTA) than a plane-ﬁtting approach at longer ranges. On
the other hand, lane direction from the map doesn’t affect
our metrics (based on centroid distance), but it helps initial-
ize vehicle direction, as shown in Figure 7.

We have used relatively simple baselines to track objects
in 3D. We believe that our data opens possibilities in map-
based and multimodal tracking research.

5. Forecasting

In this section, we describe our pipeline for trajectory

forecasting baselines.

1. Preprocessing: As described in Section 3.3, we ﬁrst
mine for “interesting” sequences and then ﬁlter out station-
ary cars from those. Each sequence contains the centroids
of tracked objects over 5 seconds.

Forecasting Coordinate System and Normalization.
The coordinate system we use for trajectory forecasting is a
top-down, bird eye view (BEV). There are three reference
coordinate frames of interest to forecasting: (1) The raw
trajectory data is stored and evaluated in the city coordinate
system (See Section 1.1. of the Supp. Material). (2) For
models using lane centerlines as a reference path, we de-
ﬁne a 2-d curvilinear coordinate system with axes tangen-
tial and perpendicular to the lane centerline. (3) For models
without the reference path (without a map), we align every-
thing such that the observed portion of the trajectory starts
at the origin and ends somewhere on the positive x axis. If
i ) represent coordinates of trajectory Vi at timestep t,
(xt
then this makes sure yTobs
= 0, where Tobs is last observed
timestep of the trajectory (Section 5.1). We ﬁnd this nor-
malization works better than leaving trajectories in absolute
map coordinates or absolute orientations.

i, yt

i

2. Feature Engineering: We deﬁne additional features
to capture social and/or spatial context. For social context,
we use minimum distance to the objects in front, in back,
and the number of neighbors. Such heuristics are meant to
capture the social interaction between vehicles. For spatial

68753

RANGE

USE

USE

THRESHOLD

MASK-RCNN

MAP LANE

GROUND
REMOVAL

MOTA

MOTP

IDF1

MT(%)

ML(%)

# FP

#FN

IDSW

#FRAG

100 m

50 m

30 m

Y
N
Y
Y

Y
N
Y
Y

Y
N
Y
Y

Y
Y
N
Y

Y
Y
N
Y

Y
Y
N
Y

map
map
map

plane-ﬁtting

map
map
map

plane-ﬁtting

map
map
map

plane-ﬁtting

37.98
16.42
37.95
37.36

52.74
21.53
52.70
52.05

73.02
23.28
72.99
72.82

0.52
0.54
0.52
0.53

0.52
0.54
0.52
0.53

0.53
0.56
0.53
0.53

0.46
0.46
0.46
0.46

0.58
0.55
0.58
0.58

0.73
0.63
0.73
0.74

0.10
0.16
0.10
0.10

0.22
0.38
0.22
0.20

0.66
0.78
0.66
0.66

0.51
0.41
0.51
0.53

0.29
0.18
0.29
0.31

0.08
0.04
0.09
0.09

105.40
1339.95
105.30
105.20

99.70
1197.30
99.50
98.10

92.80
837.45
92.80
92.00

2455.30
1972.95
2454.85
2484.00

1308.25
897.90
1307.75
1335.65

350.50
238.80
349.90
363.35

32.55
43.30
32.35
31.10

31.60
37.85
31.40
30.15

19.75
19.10
19.65
19.75

22.35
29.65
22.45
21.25

21.65
24.60
21.75
20.45

12.80
11.25
12.95
12.85

Table 2: Tracking accuracy at different ranges. From top to bottom, accuracy for objects within 100m, 50m, and 30m.

i, yt

context, we compute everything in the lane segment coordi-
nate system. We compute the lane centerline corresponding
to each trajectory and then map (xt
i ) coordinates to dis-
i) and offset from the centerline
tance along the centerline (at
i). In the subsequent sections, we denote social features
(ot
and map features for trajectory Vi at timestep t by st
i and
mt

i, respectively.
3. Prediction Algorithm: We implement weighted
Nearest Neighbors and LSTM Encoder-Deconder models
using different combinations of features. The results are
analyzed in Section 5.3.

5.1. Problem Description

i, yt

i, yt

The forecasting task is framed as: given the past input
coordinates of a vehicle trajectory Vi as Xi = (xt
i ) for
time steps t = {1, . . . , Tobs}, predict the future coordinates
i ) for time steps {t = Tobs+1, . . . , Tpred}. For
Yi = (xt
a car, 5 seconds is sufﬁcient to capture the required part of
trajectory, e.g. crossing an intersection. Furthermore, it is
unlikely for a typical driving maneuver to last more than
5 seconds. In this paper, we deﬁne the forecasting task as
observing 20 past frames (2 seconds) and then predicting
10-30 frames (1-3 seconds) into the future. Each trajectory
can leverage the trajectories of other vehicles in the same
sequence to capture the social context and map information
for spatial context.

5.2. Multimodal Evaluation

Predicting the future is difﬁcult. Often, there are sev-
eral plausible future actions for a given observation. In the
case of autonomous vehicles, it is important to predict many
plausible outcomes and not simply the most likely outcome.
While some prior works have evaluated forecasting in a de-
terministic, unimodal way, we believe a better approach is
to follow the evaluation methods of DESIRE [19] and So-
cial GAN [11] and encourage algorithms to output multiple
predictions.

Our vector map is a semantic graph. The ﬁrst step in
prediction with a vector map is to localize oneself on the
semantic graph. We deﬁne two subsequent phases: (1) a
hypothesis phase and (2) a generation phase. The seman-
tic graph makes the generation phase trivial because we

can quickly generate hypothesized trajectories via Breadth-
First-Search on the semantic graph. However, the hypothe-
sis phase is still challenging due to the multimodal nature of
the problem, e.g. it’s difﬁcult to know which lane segment
a vehicle will follow in an intersection.

Among the variety of metrics evaluated in DESIRE was
the oracle error over top K number of samples metric,
where K = 50. We follow the same approach and use top-
K Average Displacement Error (ADE) and Final Displace-
ment Error (FDE) as our metrics. The map-based baselines
that we report have access to a semantic vector map. As
such, they can generate K different hypotheses based on the
branching of the road network along a particular observed
trajectory. On average, our heuristics generate K = 5.9
hypotheses. We generate more than 25 hypotheses for less
than 2% of the scenarios. Our map gives us an easy way
to produce a compact yet diverse set of forecasts. Other
baselines don’t have such an option and are restricted to
a single prediction. We also provide an oracle version of
the map-based baselines wherein the model produces the
best possible hypothesis by having access to (xt
i ) for
t = {Tobs+1, . . . , Tpred}, along with the observed trajec-
tory. Note that an oracle-based hypothesis can still generate
an imperfect trajectory, e.g.
if a car wasn’t following any
lane.

i, yt

5.3. Results

In this section, we evaluate the effect of adding social
context and spatial context (from the vector map) to im-
prove trajectory forecasting over horizons of 1 and 3 sec-
onds into the future. We evaluate teh following models:

• Constant Velocity:

Compute the mean velocity
(vxi, vyi) from t = {1, . . . , Tobs} and then forecast
i ) for t = {Tobs+1, . . . , Tpred} using (vxi, vyi)
(xt
as the constant velocity.

i, yt

jectories are queried by (xt

• NN: Weighted Nearest Neighbor regression where tra-
i ) for t = {1, . . . , Tobs}.
• NN+map(oracle): Weighted Nearest Neighbor regres-
i) for t =

sion where trajectories are queried by (at
i, ot
{1, . . . , Tobs} obtained from oracle centerline.

i, yt

• NN+map: Similar to NN+map(oracle) but uses top-K

78754

BASELINE

Constant Velocity
NN
NN+map(oracle)
NN+map
LSTM ED
LSTM ED+social
LSTM ED+map(oracle)
LSTM ED+map
LSTM ED+social+map(oracle)

1 SECOND

ADE

FDE

3 SECONDS

ADE

FDE

1.04
0.75
0.82
0.72
0.68
0.69
0.82
0.80
0.89

1.89
1.28
1.39
1.33
1.78
1.20
1.38
1.35
1.48

3.55
2.46
2.39
2.28
2.27
2.29
2.32
2.25
2.46

7.89
5.60
5.05
4.80
5.19
5.22
4.82
4.67
5.09

Table 3: Forecasting Errors for different prediction horizons

hypothesized centerlines.

• LSTM ED: LSTM Encoder-Decoder model where the
i ) for t = {1, . . . , Tobs} and output is

i, yt

input is (xt
(xt

i, yt

i ) for t = {Tobs+1, . . . , Tpred}

• LSTM ED+social: Similar to LSTM ED but with input

as (xt

i, yt

i , st

i), where st

i denotes social features

i, ot

• LSTM ED+map(oracle): Similar to LSTM ED but with
input as (at
i), where mt
i
denotes the map features obtained from oracle center-
line. Distances (at
i ) for
evaluation.

i) are then mapped to (xt

i) and output as (at

i, mt

i, yt

i, ot

i, ot

• LSTM ED+map: Similar to LSTM ED+map(oracle)

but uses top-K hypothesized centerlines.

• LSTM ED+social+map (oracle): Similar to LSTM
features being

ED+map(oracle) but with input
(at

i, ot

i, st

i, mt

i) .

The results of these baselines are reported in Table 3.
Below, we focus on the ADE and FDE for a prediction
horizon of 3 seconds to understand which baselines are less
impacted by accumulating errors. Constant Velocity is out-
performed by all the other baselines because it cannot cap-
ture typical driving behaviors like acceleration, decelera-
tion, turns etc. NN+map has lower ADE and FDE than NN
because it is leveraging useful cues from the vector map.
NN+map has lower error than NN+map(oracle) as well,
emphasizing the multimodal nature of predictions. LSTM
ED does better than NN. LSTM ED+social performs simi-
lar to LSTM ED, implying that the social context does not
add signiﬁcant value to forecasting. A similar observation
was made on KITTI [10] in DESIRE [19], wherein their
model with social interaction couldn’t outperform the one
without it. We observe that LSTM ED+map outperforms all
the other baselines for a prediction horizon of 3 sec. This
proves the importance of having a vector map for distant fu-
ture prediction and making multimodal predictions. More-
over, NN+map has a lower FDE than LSTM ED+social and
LSTM ED for higher prediction horizon (3 secs). This sug-
gests that even a shallow model working on top of a vector
map works better than a deep model with social features
and no vector map. Figure 8 shows qualitative forecasting
results from our best performing model.

Figure 8: Qualitative results from LSTM ED+map fore-
casting baseline. Top left: the model correctly predicts that
the car will go straight at the intersection. Top right: the
model correctly predicts a smooth right turn never going out
of the lane, which might have been difﬁcult if there were no
map. Bottom left: demonstration of the multimodal nature
of predictions, where the model considers all top-K possi-
bilities. Bottom right: the predictions are on a non-typical
lane which takes a slight left and then a slight right. Again,
this is hard to predict without a map.

6. Discussion

Argoverse is a large dataset for autonomous driving
research. Unique among such datasets, Argoverse con-
tains rich map information such as lane centerlines, ground
height, and driveable area. We examine baseline methods
for 3D tracking with map-derived context. We also mine
one thousand hours of ﬂeet logs to ﬁnd diverse, real-world
object trajectories which constitute our motion forecasting
benchmark. We examine baseline forecasting methods and
see that map data signiﬁcantly improves accuracy. We will
maintain a public leaderboard for 3D object tracking and
motion forecasting. The sensor data, map data, annota-
tions, and code which make up Argoverse are available at
Argoverse.org.

Acknowledgements. We thank our Argo AI colleagues
– Ben Ballard, Brett Browning, Alex Bury, Dave Chekan,
Kunal Desai, Patrick Gray, Larry Jackson, Etienne Jacques,
Gang Pan, Kevin Player, Peter Rander, Bryan Salesky,
Philip Tsai, Ian Volkwein, Ersin Yumer and many more –
for their invaluable assistance in supporting Argoverse.

Patsorn Sangkloy is supported by a a Royal Thai Government
Scholarship. James Hays receives research funding from Argo AI,
which is developing products related to the research described in
this paper.
In addition, the author serves as a Staff Scientist to
Argo AI. The terms of this arrangement have been reviewed and
approved by Georgia Tech in accordance with its conﬂict of inter-
est policies.

88755

References

[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,
Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So-
cial lstm: Human trajectory prediction in crowded spaces.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.

[2] Mykhaylo Andriluka, Stefan Roth, and Bernt Schiele.
People-tracking-by-detection
people-detection-by-
tracking. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2008.

and

[3] Andrew Bacha, Cheryl Bauman, Ruel Faruque, Michael
Fleming, Chris Terwelp, Charles Reinholtz, Dennis Hong,
Al Wicks, Thomas Alberi, David Anderson, Stephen Cacci-
ola, Patrick Currier, Aaron Dalton, Jesse Farmer, Jesse Hur-
dus, Shawn Kimmel, Peter King, Andrew Taylor, David Van
Covern, and Mike Webster. Odin: Team victortango’s entry
in the darpa urban challenge. J. Field Robot., 25(8):467–492,
Aug. 2008.

[4] Keni Bernardin and Rainer Stiefelhagen. Evaluating mul-
tiple object tracking performance: The clear mot metrics.
EURASIP J. Image and Video Processing, 2008.

[5] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,
Giancarlo Baldan, and Oscar Beijbom. nuscenes: A mul-
timodal dataset for autonomous driving.
arXiv preprint
arXiv:1903.11027, 2019.

[6] Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet:
Learning to predict intention from raw sensor data. In Aude
Billard, Anca Dragan, Jan Peters, and Jun Morimoto, edi-
tors, Proceedings of The 2nd Conference on Robot Learning,
volume 87 of Proceedings of Machine Learning Research,
pages 947–956. PMLR, 29–31 Oct 2018.

[7] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong
Xiao. Deepdriving: Learning affordance for direct percep-
tion in autonomous driving. In The IEEE International Con-
ference on Computer Vision (ICCV), 2015.

[8] Nachiket Deo and Mohan M Trivedi. Convolutional so-
cial pooling for vehicle trajectory prediction. arXiv preprint
arXiv:1805.06771, 2018.

[9] Martin Ester, Hans peter Kriegel, JÃ˝urg Sander, and Xiaowei
Xu. A density-based algorithm for discovering clusters in
large spatial databases with noise. In KDD Proceedings of
the Second International Conference on Knowledge Discov-
ery and Data Mining, pages 226–231. AAAI Press, 1996.

[10] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The KITTI dataset. The
International Journal of Robotics Research, 32(11):1231–
1237, 2013.

[11] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese,
and Alexandre Alahi. Social gan: Socially acceptable trajec-
tories with generative adversarial networks.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.

[12] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-cnn.
In 2017 IEEE International Conference on Computer Vision
(ICCV), pages 2980–2988, Oct 2017.

[13] Simon Hecker, Dengxin Dai, and Luc Van Gool. End-to-end
learning of driving models with surround-view cameras and
route planners. In European Conference on Computer Vision
(ECCV), 2018.

[14] David Held, Devin Guillory, Brice Rebsamen, Sebastian
Thrun, and Silvio Savarese. A probabilistic framework for
real-time 3d segmentation using spatial, temporal, and se-
mantic cues. In Proceedings of Robotics: Science and Sys-
tems, 2016.

[15] David Held, Jesse Levinson, and Sebastian Thrun. Precision
tracking with sparse 3d and dense color 2d data. In ICRA,
2013.

[16] David Held, Jesse Levinson, Sebastian Thrun, and Silvio
Savarese. Combining 3d shape, color, and motion for ro-
bust anytime tracking. In Proceedings of Robotics: Science
and Systems, Berkeley, USA, July 2014.

[17] M. Himmelsbach and H. j. WuÌ´Lnsche. Lidar-based 3d ob-
In Proceedings of 1st International Work-

ject perception.
shop on Cognition for Technical Systems, 2008.

[18] Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao,
Dingfu Zhou, Peng Wang, Yuanqing Lin, and Ruigang
Yang. The apolloscape dataset for autonomous driving. In
arXiv:1803.06184, 2018.

[19] Namhoon Lee, Wongun Choi, Paul Vernaza, Christo-
pher Bongsoo Choy, Philip H. S. Torr, and Manmo-
han Krishna Chandraker. DESIRE: distant future predic-
tion in dynamic scenes with interacting agents. CoRR,
abs/1704.04394, 2017.

[20] John Leonard, Jonathan How, Seth Teller, Mitch Berger, Ste-
fan Campbell, Gaston Fiore, Luke Fletcher, Emilio Fraz-
zoli, Albert Huang, Sertac Karaman, Olivier Koch, Yoshi-
aki Kuwata, David Moore, Edwin Olson, Steve Peters, Justin
Teo, Robert Truax, Matthew Walter, David Barrett, Alexan-
der Epstein, Keoni Maheloni, Katy Moyer, Troy Jones, Ryan
Buckley, Matthew Antone, Robert Galejs, Siddhartha Krish-
namurthy, and Jonathan Williams. A perception-driven au-
tonomous urban vehicle. J. Field Robot., 25(10):727–774,
Oct. 2008.

[21] Jesse Levinson, Jake Askeland, Jan Becker, Jennifer Dolson,
David Held, Sören Kammel, J. Zico Kolter, Dirk Langer,
Oliver Pink, Vaughan R. Pratt, Michael Sokolsky, Ganymed
Stanek, David Michael Stavens, Alex Teichman, Moritz
Werling, and Sebastian Thrun. Towards fully autonomous
driving: Systems and algorithms.
In IEEE Intelligent Ve-
hicles Symposium (IV), 2011, Baden-Baden, Germany, June
5-9, 2011, pages 163–168, 2011.

[22] Justin Liang and Raquel Urtasun. End-to-end deep structured
models for drawing crosswalks. In The European Conference
on Computer Vision (ECCV), September 2018.

[23] Xingyu Liu, Charles R Qi, and Leonidas J Guibas.
Flownet3d: Learning scene ﬂow in 3d point clouds. arXiv
preprint arXiv:1806.01411, 2019.

[24] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furious:
Real time end-to-end 3d detection, tracking and motion fore-
casting with a single convolutional net. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2018.

98756

urban challenge. Technical report, Carnegie Mellon Univer-
sity, Pittsburgh, PA, April 2007.

[36] Shenlong Wang, Min Bai, Gellert Mattyus, Hang Chu, Wen-
jie Luo, Bin Yang, Justin Liang, Joel Cheverie, Sanja Fidler,
and Raquel Urtasun. Torontocity: Seeing the world with
a million eyes. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2017.

[37] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neu-
mann. Sgpn: Similarity group proposal network for 3d point
cloud instance segmentation. In CVPR, 2018.

[38] Bin Yang, Ming Liang, and Raquel Urtasun. Hdnet: Exploit-
ing hd maps for 3d object detection. In Aude Billard, Anca
Dragan, Jan Peters, and Jun Morimoto, editors, Proceedings
of The 2nd Conference on Robot Learning, volume 87 of
Proceedings of Machine Learning Research, pages 146–155.
PMLR, 29–31 Oct 2018.

[25] Suraj M S, Hugo Grimmett, Lukas Platinsky, and Peter On-
druska. Visual vehicle tracking through noise and occlusions
using crowd-sourced maps.
In Intelligent Robots and Sys-
tems (IROS), 2018 IEEE international conference on, pages
4531–4538. IEEE, 2018.

[26] Yuexin Ma, Xinge Zhu, Sibo Zhang, Ruigang Yang, Wen-
ping Wang, and Dinesh Manocha. Trafﬁcpredict: Trajectory
prediction for heterogeneous trafﬁc-agents. In Proceedings
of the 33rd National Conference on Artiﬁcal Intelligence,
AAAI’19. AAAI Press, 2019.

[27] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul
Newman. 1 year, 1000 km: The Oxford Robotcar dataset.
The International Journal of Robotics Research, 36(1):3–15,
2017.

[28] A. Milan, L. Leal-Taixé,

I. Reid, S. Roth, and K.
Schindler. MOT16: A benchmark for multi-object tracking.
arXiv:1603.00831 [cs], Mar. 2016. arXiv: 1603.00831.

[29] Michael Montemerlo, Jan Becker, Suhrid Bhat, Hendrik
Dahlkamp, Dmitri Dolgov, Scott Ettinger, Dirk Haehnel,
Tim Hilden, Gabe Hoffmann, Burkhard Huhnke, Doug John-
ston, Stefan Klumpp, Dirk Langer, Anthony Levandowski,
Jesse Levinson, Julien Marcil, David Orenstein, Johannes
Paefgen, Isaac Penny, Anna Petrovskaya, Mike Pﬂueger,
Ganymed Stanek, David Stavens, Antone Vogt, and Sebas-
tian Thrun. Junior: The stanford entry in the urban challenge.
J. Field Robot., 25(9):569–597, Sept. 2008.

[30] Gaurav Pandey, James R Mcbride, and Ryan M Eustice.
Int. J. Rob. Res.,

Ford campus vision and lidar data set.
30(13):1543–1552, Nov. 2011.

[31] Abhishek Patil, Srikanth Malla, Haiming Gang, and Yi-Ting
Chen. The h3d dataset for full-surround 3d multi-object de-
tection and tracking in crowded urban scenes.
In Interna-
tional Conference on Robotics and Automation, 2019.

[32] Luis Patino, Tom Cane, Alain Vallee, and James Ferryman.
Pets 2016: Dataset and challenge.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition Workshops, pages 1–8, 2016.

[33] Akshay Rangesh, Kevan Yuen, Ravi Kumar Satzoda,
Rakesh Nattoji Rajaram, Pujitha Gunaratne, and Mohan M.
Trivedi. A multimodal, full-surround vehicular testbed for
naturalistic studies and benchmarking: Design, calibration
and deployment. CoRR, abs/1709.07502, 2017.

[34] Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye
Guan, Yuchao Dai, Hao Su, Hongdong Li, and Ruigang
Yang. Apollocar3d: A large 3d car instance understanding
benchmark for autonomous driving. CoRR, abs/1811.12222,
2018.

[35] Christopher Urmson, Joshua Anhalt, J. Andrew (Drew) Bag-
nell, Christopher R. Baker, Robert E. Bittner, John M. Dolan,
David Duggins, David Ferguson, Tugrul Galatali, Hartmut
Geyer, Michele Gittleman, Sam Harbaugh, Martial Hebert,
Thomas Howard, Alonzo Kelly, David Kohanbash, Maxim
Likhachev, Nick Miller, Kevin Peterson, Raj Rajkumar,
Paul Rybski, Bryan Salesky, Sebastian Scherer, Young-Woo
Seo, Reid Simmons, Sanjiv Singh, Jarrod M. Snider, An-
thony (Tony) Stentz, William (Red) L. Whittaker, and Jason
Ziglar. Tartan racing: A multi-modal approach to the darpa

108757

