Marginalized Latent Semantic Encoder for Zero-Shot Learning

Zhengming Ding† and Hongfu Liu‡

†Department of CIT, Indiana University-Purdue University Indianapolis, IN, USA

‡Michtom School of Computer Science, Brandeis University, MA, USA

zd2@iu.edu, hongfuliu@brandeis.edu

Abstract

Zero-shot learning has been well explored to precisely
identify new unobserved classes through a visual-semantic
function obtained from the existing objects. However, there
exist two challenging obstacles: one is that the human-
annotated semantics are insufﬁcient to fully describe the vi-
sual samples; the other is the domain shift across existing
and new classes. In this paper, we attempt to exploit the
intrinsic relationship in the semantic manifold when given
semantics are not enough to describe the visual objects, and
enhance the generalization ability of the visual-semantic
function with marginalized strategy. Speciﬁcally, we design
a Marginalized Latent Semantic Encoder (MLSE), which
is learned on the augmented seen visual features and the
latent semantic representation. Meanwhile, latent seman-
tics are discovered under an adaptive graph reconstruction
scheme based on the provided semantics. Consequently, our
proposed algorithm could enrich visual characteristics from
seen classes, and well generalize to unobserved classes.
Experimental results on zero-shot benchmarks demonstrate
that the proposed model delivers superior performance over
the state-of-the-art zero-shot learning approaches.

1. Introduction

Visual data analytic has achieved tremendous improve-
ments recently, as the rapid explosion of data scales and
continuously-improved learning models. Traditional visual
recognition systems almost pursue the supervised strategies,
that require a great number of well-annotated instances to
seek a high-performance model. Unfortunately, it is expen-
sive and even prohibitive to collect enough training samples
for an effective model, especially when these samples need
ﬁne-grained annotations. Hence, it is appealing and essen-
tial to build such recognition systems that can identify novel
categories in the test stage with limited or even no instances
accessible in the training process.

(cid:2)(cid:8)(cid:10)(cid:9)

(cid:6)(cid:13)(cid:17)(cid:10)(cid:18)(cid:23)(cid:15)(cid:11)(cid:26)(cid:2)(cid:18)(cid:11)(cid:19)(cid:12)(cid:13)(cid:21)

(cid:3)(cid:8)(cid:7)(cid:6)(cid:12)

(cid:2)(cid:8)(cid:7)(cid:6)(cid:12)

(cid:2)(cid:6)(cid:10)(cid:11)(cid:4)(cid:12)(cid:5) (cid:1)(cid:10)(cid:13)(cid:7)(cid:4)(cid:12)

(cid:7)(cid:15)(cid:22)(cid:24)(cid:10)(cid:16) (cid:3)(cid:13)(cid:10)(cid:23)(cid:24)(cid:21)(cid:13) (cid:8)

(cid:1)(cid:12)(cid:10)(cid:20)(cid:23)(cid:15)(cid:25)(cid:13) (cid:4)(cid:21)(cid:10)(cid:20)(cid:14) (cid:6)

(cid:3)(cid:5)(cid:12)(cid:14)

(cid:2)(cid:17)(cid:16)(cid:12)(cid:15)(cid:8)(cid:7)

(cid:1)(cid:12)(cid:10)

(cid:4)(cid:11)(cid:12)(cid:17)(cid:8)

(cid:1)(cid:14)(cid:5)(cid:6)(cid:13)

(cid:9)(cid:18)(cid:16)

(cid:6)(cid:13)(cid:17)(cid:10)(cid:18)(cid:23)(cid:15)(cid:11)(cid:26)(cid:5)(cid:13)(cid:20)(cid:21)(cid:13)(cid:22)(cid:13)(cid:18)(cid:23)(cid:10)(cid:23)(cid:15)(cid:19)(cid:18)(cid:26)(cid:9)

Figure 1. Illustration of our marginalized latent semantic encoder,
in which a semantic encoder is built to bridge visual features in
marginalized corruption E( ˜X) and the latent semantics Z with
W E( ˜X) ≈ Z. Furthermore, latent semantics are learned over
the given semantics A through an adaptive graph (Z ≈ AS).

in generalizing knowledge from observed objects to un-
seen objects [22, 8, 33, 11, 6, 17, 14, 31, 3, 26, 7, 28].
In fact, ZSL is motivated by the human cognitive learning
mechanism in identifying unknown classes. ZSL attempts
to discover the intrinsic visual-semantic mapping from ob-
served objects and generalizes it for unobserved categories.
One of the most frequently-adopted approaches is to embed
visual features and their corresponding semantics of seen
classes into the same common space to couple the semantic
gap across two, which expects that the unseen classes with
their semantics and visual samples are also embedded in the
same space. Most existing ZSL models focus on seeking
the visual-semantic function only relying on the provided
visual data and its semantics [4, 22, 12, 13]. The visual-
semantic function could simply be a linear mapping [23],
or dual linear mappings [4], or even complex non-linear
functions, including dictionary learning [8], auto-encoder
[12, 17, 3, 34], and generative models [38, 31, 9, 7], where
generative models are usually promising to augment the
space of seen classes and more likely to cover that of un-
seen classes in the training stage.

Zero-shot learning (ZSL) has been surging recently,
which catches great attention for its promising performance

Although the existing ZSL methods achieve some
promising results in generalizing the seen knowledge to un-

6191

seen ones [12, 17, 3, 34], there still remains two degenerat-
ing points. First of all, it presents a general challenge from
no training data for the unseen classes, which leads to the
difﬁculties for model selection. The domain shift across
seen and unseen classes would prevent the generalization
ability of the learned visual-semantic function. Thus, how
to learn an effective and compatible visual-semantic map-
ping on the observed objects is the key problem in ZSL
problem. Secondly, the information based on observed sam-
ples is not always sufﬁcient to learn the visual-semantic
mapping. On one hand, the semantic attributes are sub-
jective to be annotated and not enough to span the visual
feature space; on the other hand, the visual-semantic map-
ping is learned only on the seen categories, where the dif-
ferent visual distribution on seen and unseen categories ob-
stacles the effective generalization in the test stage. To this
end, tremendous efforts have been taken to handle the above
challenges[38, 31, 9, 7]; however, most of them ignore the
huge potential in the latent semantic representation for a
more generic visual-semantic mapping learning.

In this paper, we develop a novel Marginalized Latent
Semantic Encoder (MLSE) to deal with the previously-
mentioned two zero-shot obstacles (Figure 1). Our main
assumption is that the latent semantic representation could
better describe the visual samples compared with human-
annotated ones, and generic semantic encoder is able to
better capture the unseen knowledge by augmenting visual
space of the seen classes through marginalized denoising
strategy. Moreover, we exploit a sparse residual constraint
to purse a meaningful semantic embedding space and guide
the latent semantic representation learning. To sum up, we
highlight our contributions as:

• First of all, we derive a generic encoder to adapt
the intrinsic knowledge and shared features from the
observed classes under a marginalized augmentation.
Therefore, a generic semantic encoder could cover
more knowledge for the unseen categories, and thus
generalize well in the test stage.

• Second, we automatically learn new latent semantics
to seek more efﬁcient prototypes from known classes
through an adaptive graph reconstruction strategy over
given semantics. Hence, our model is able to learn
more effective information with the given human-
annotated semantics.

• Finally, we further adopt a sparse reguralizer to con-
strain the adaptive graph learning with preserving the
original intrinsic information and removing the out-
liers and noising factors. Therefore, our model is able
to effectively learn the latent semantics.

2. Related Work

visual knowledge from such unknown evaluation classes is
inaccessible in the training process, ZSL needs external se-
mantics to compensate for the unknown visual information.
So far, attribute-based descriptions are widely used to deﬁne
the shared characteristics across various categories [20, 21],
which is an intermediate domain to link the visual features
with their semantics.

Early ZSL explores the attributes within a two-stage ap-
proach to predict the label of a given image from the unseen
classes. Generally speaking, the attributes of any given im-
age are assigned in the ﬁrst stage, then its class label is in-
ferred by searching the class-attribute table using the near-
est neighbor classiﬁer. Direct Attribute Prediction (DAP)
and Indirect attribute prediction (IAP) are two pioneering
studies, which adopt the hidden layer of attributes as vari-
ables decoupling the images from the layer of labels [15].
However, such two-stage approaches suffer from distribu-
tion difference between the intermediate and target task,
since target task is to assign the class label while intermedi-
ate task would consider to obtain attribute classiﬁers.

Recent advances of ZSL seek a direct mapping from a
visual feature space to a semantic space. Along this line,
Akata et al. optimized the structural SVM loss to achieve
the bilinear compatibility [2]. Furthermore, they proposed
to build a bilinear compatibility function across the visual
and the semantics via a ranking loss [1]. On the other hand,
Romera-Paredes et al. exploited the square loss to obtain the
bilinear compatibility and explicitly regularizes the objec-
tive [23]. Recently, Jiang et al. also employed a dictionary
learning framework to seek the latent attributes, which was
not only discriminative but also semantic-preserving [11].
Liu et al. explored a semantic auto-encoder with rank con-
straint on the projection matrix to preserve more intrinsic
structure [17]. Some generative models are proposed by
seeking a generator as the visual-semantic mapping func-
tion [38, 31, 9]. They mainly explore the conditioned gen-
erator on semantics to synthesize more visual features for
seen classes, and thus they have a better chance to mitigate
the domain shift in visual space between seen and unseen
classes. However, generative models are usually hard to
train due to its min-max optimization.

Moreover, another ZSL direction is to embed both the vi-
sual and semantic features into a shared intermediate space.
Following this, Zhang et al. mapped visual features and
semantic features into two different latent spaces, and mea-
sured their similarity through seeking one bilinear compati-
bility function [36]. Besides, Changpinyo et al. explored
a hybrid model and constructed the classiﬁers of unseen
classes by taking the linear combinations of base classiﬁers,
which are trained in a discriminative learning framework
from seen classes[5].

Zero-shot learning (ZSL) targets at learning models of
visual concepts with no evaluation data of the concepts. As

Unfortunately, most existing ZSL approaches pay less
attention to discriminative information for the unknown cat-

6192

egories considering the high within-class variability, and
therefore, they would fail to uncover the common semantics
cross seen and unseen classes. Differently, we assume the
provided semantics are not enough to describe the visual ob-
jects and thus aim to seek a better latent semantic represen-
tation. Simultaneously, we learn a generic semantic encoder
with marginalized augmentation strategy to effectively han-
dle the domain shift and discover the shared discriminative
features across the seen and unseen categories.

3. The Proposed Algorithm

In this part, we discuss our novel marginalized seman-
tic encoder with latent semantic representation for effective
zero-shot learning.

3.1. Preliminaries & Motivation

Considering there are C seen categories with n labeled
instances D = {X, A, y} and Cu unseen categories with
nu unlabeled instances Du = {Xu, Au, yu}. Each instance
is represented with a d-dimensional visual feature vector.
y ∈ Rn and yu ∈ Rnu denote class labels for the seen and

unseen categories, respectively. More speciﬁcally, the seen
and unseen categories are non-overlapped in term of cate-
gory information, that is, y ∩ yu = ∅. Thus, semantic repre-
sentations make up for this challenge, where A ∈ Rm×n
and Au ∈ Rm×nu are the m-dimensional semantics for
seen and unseen categories, respectively. For the seen cate-
gories, A is given for visual feature X, which is labeled by
either binary or continuous attributes representing its cor-
responding class label y. By comparison, Au has to be
predicted as the unseen categories are not annotated. The
intuition of ZSL is to learn a visual-semantic function to
discover the relationship across the visual features and the
individual dimensions of the semantic features. Due to the
distribution divergence across seen and unseen categories,
it is essential to mitigate this challenge during the visual-
semantic function learning.

Since seen categories X and unseen categories Xu are
sampled from various visual feature spaces; fortunately,
A and Au compensate by sharing some common seman-
tics with each other. Take attribute-based semantics as
an example, both seen and unseen categories can be de-
scribed with human-annotated attributes in various values
either binary or continuous. Besides, we notice the human-
provided semantics are not sufﬁcient to comprehensively
describe the visual samples. To this end, we propose our
marginalized latent semantic encoder to handle these two
challenges. First, we explore to diversify the feature space
of seen classes during model training by using the marginal-
ized desnoising strategy. Second, latent semantic represen-
tation is sought to better describe the visual samples jointly
with an adaptive graph learning.

3.2. Generic Semantic Encoder Learning

A natural way to enhance the generalization of a visual-
semantic function using a corrupting distribution is to ex-
plore the spirit of [18] by selecting each element of the
training samples and corrupting it k times. For seen visual
features X, this results in corresponding corrupted observa-

tions ˜Xl (with l = 1,··· , k). Thus, we propose a semantic
encoder to encode each corrupted ˜Xl with semantic repre-
sentation A as follows:

k

min
W

1
k

Pl=1kW ˜Xl − Ak2

F, s.t. W ⊤W = Im,

(1)

where k · kF is the Frobenius norm and ˜Xi is the l-th cor-
rupted version of X. Note that the orthogonal constraint
W ⊤W = Im (Im ∈ Rm×m) is imposed to avoid trivial

solutions.

Although attribute semantics are widely-used in the clas-
siﬁcation problem, two issues need to be taken into account.
First of all, the human-annotated attributes do not always
achieve the similar importance for discrimination, hence, it
would be not desirable to seek more enriched semantics.
Secondly, there are correlations among different attributes;
hence, it is improper to learn every attribute individually.
In other words, it is too strong to enforce A to be semantics
output. Thus, we explore to learn new latent semantics to re-
lax the constraint. Furthermore, we introduce the marginal-
ized denoising strategy to consider the limiting case when
k tends to be ∞. To this end, we explore the weak law of
large numbers and reformulate 1
F to its
expectation formula:

l=1 kW ˜Xl − Ak2

k Pk
F(cid:17) + αkZ − Ak2

F,

min
W,Z

E(cid:16)kW ˜X − Zk2

s.t. W ⊤W = Im,

(2)

where E(·) is the expectation operator and α is the trade-off
parameter. The second constraint enforces Z to be similar
to the given semantics A, which could help ensure that the
learned semantic encoder depicts visual-semantic relation.
The exception loss minimization results in data augmenta-
tion in learning the semantic encoder, which would improve
the generalization ability of the proposed model, especially
when dealing with zero-shot learning problem.

3.3. Adaptive Graph Guided Latent Semantics

Considering the phenomenon that samples from one cat-
egory are lying over a complicated manifold, e.g., crescent
manifold, it is clearly not a proper way to directly use its
center as its prototype or exemplar to that class. More-
over, human-annotated semantics are usually not enough to
comprehensively describe the visual samples. However, se-
mantics across different categories should be shared. So,
we adopt the manifold learning idea to uncover the latent

6193

semantic representation based on a new reconstruction of
provided semantics constrained a graph manifold. Thus,
semantics of some instances lying in the same local man-
ifold could compensate with each other in latent semantics
learning.

To uncover more semantic knowledge, we propose an
adaptive graph reconstruction term to align the latent se-
mantics Z and the given semantics A as follows:

F

(3)

n S = 1⊤

E(cid:16)kW ˜X − Zk2

min
W,Z,S
s.t. W ⊤W = Im, 1⊤

F(cid:17) + αkZ − ASk2
n S = 1⊤
n , S ≥ 0,
where two constraints, i.e., 1⊤
n (1n is an all-one n-
dim vector) and S ≥ 0, are used to guarantee the validity of
the obtained graph coefﬁcients.
To explore more intrinsic structure on S, we consider a
residual minimization with pre-deﬁned graph weight ma-
trix H, which is calculated from a spectral dual-graph. To
be speciﬁc, we explore the data structures from both visual
features X and its corresponding semantics S. We build
two k-nn graphs Gx and Ga based on visual and seman-
tic features, respectively. We ﬁrst use cosine similarity to
calculate the weights of two graphs, i.e., Hx and Ha, then
exploit a simple fusion strategy to achieve weight matrix H
as H = Hx+Ha
. However, the learned graphs Gx and Ga
may suffer from arbitrary noise from the data. In the worst
case, it would signiﬁcantly affect the learning of latent se-
mantic representation and further the semantic encoder. To
this end, to promote structure information and suppress ef-
fects of noise data points, we ﬁrst explore l1-norm to con-
strain the residual between H and S in order to ﬁgure out
the small number of abnormal weights caused by outliers or
noisy samples. We expect most elements of S to be similar
to H to preserve the original intrinsic structure while some
to be different for outliers. Thus, we achieve a robust graph
guided semantic encoder as follows:

2

min
W,Z,S

E(cid:16)kW ˜X − Zk2

F(cid:17) + αkZ − ASk2
n S = 1⊤

s.t. W ⊤W = Im, 1⊤

F + βkS − Hk1

n , S ≥ 0,

(4)
where β is the trade-off among three components. k · k1 is
the l1 operator of matrix to detect the outliers in the original
dual graph by learning a more effective adaptive graph.

Remark: The objective function in Eq.
(4) simultane-
ously seeks a semantic encoder through marginalized de-
noising strategy and latent semantic representation guided
with adaptive graph reconstruction.
In this way, our se-
mantic encoder can beneﬁt the artiﬁcial data augmentation
to span the visual feature space of seen classes. Also, the
adaptive graph reconstruction scheme could assist learning
more effective latent semantic representation assuming the
give semantics are not enough to describe the visual fea-
tures. Two strategies tend to trigger each other to learn the

semantic encoder with better generalization ability to un-
seen classes.

3.4. Optimization

It is straightforward to observe that three variables W, S
and Z in Eq. (4) are not able to be jointly optimized. To
deal with the issue, we ﬁrst convert it into the augmented
Lagrangian function via involving an extra variable E de-
ﬁned as E = S − H (S ≥ 0):
J = E(cid:16)kW ˜X − Zk2
n S − 1⊤

F + βkEk1
F(cid:17),
2 + kS − H − Ek2

F(cid:17) + αkZ − ASk2
n k2

+ µ(cid:16)k1⊤

(5)

n S = 1⊤

To ﬁght of the constraint 1⊤

n efﬁciently, we
relax the constraint through incorporating a penalty term
µk1⊤
2 into Eq. (5) and µ is a positive parame-
ter. Since the optimization of Eq. (5) is non-smooth and
non-convex, and thus, we design an efﬁcient solver to Eq.
(5) with respect to W, S, Z and E, respectively.

n S − 1⊤

n k2

Learning Semantic Encoder W : Given Z, the objective
function w.r.t. W reduces to:

W = arg min
W ⊤W =Im

F(cid:17),
E(cid:16)kW ˜X − Zk2

(6)

where E( ˜X) can be calculated by following [18]. To ﬁght
off the non-convex problem in Eq. (6) due to the orthogo-
nal constraint W ⊤W = Im, we explore a gradient descent
optimization [29]. In general, we ﬁrst calculate the gradient
of J w.r.t W as
∂J
∂W

= 2W E(cid:0) ˜X ˜X ⊤(cid:1) − 2Z E( ˜X ⊤),

where E(cid:0) ˜X ˜X ⊤(cid:1) and E( ˜X ⊤) can be calculated by follow-

ing [18]. After that, we calculate the skew-symmetric ma-
trix and update W until we reach the Armijo-Wolfe condi-
tions [27].

Learning Adaptive Graph S: Given Z, E, we relax the
non-negative constraint and rewrite the objective function
w.r.t. S as:

J = k ¯Z − ¯ASk2

F + tr(ΓS⊤),

(7)

where ¯Z = [√αZ,√µ1n,√µ(H + E)] and ¯A =
[√αA,√µ1n,√µIn]. For constraint S ≥ 0, we introduce

the Lagrange multiplier Γ, which is an extra variable. For-
tunately, we can mitigate the optimization of Γ through the
following deduction. To be speciﬁc, we obtain the partial
derivative of J over S and set it to zero as:

∂J
∂S

= 2 ¯A⊤( ¯AS − ¯Z) + Γ = 0.

6194

Algorithm 1 Solving the problem in Eq. (5)

Input: X, A, H, α, β
Initialization: µm = 106, µ0 = 10−1, ρ = 1.3, τ = 0.
while not converged do

1. Update Z via Eq. (10) with others ﬁxed.

2. Update S, E via Eqs. (8) and (9) with others ﬁxed.

3. Update W via Eq. (6) with others ﬁxed.
4. Update penalty µτ +1 = min(ρµτ , µm).
5. Check the convergence condition |Jτ +1 − Jτ | < 10−3.
6. τ = τ + 1.
end while

output: W, S, E, Z.

Through the KKT condition Γ ⊙ S = 0 (⊙ means the
Hadamard product), we can obtain the following formula-
tion:

h2 ¯A⊤( ¯AS − ¯Z) + Γi ⊙ S = 0.

Following [37], we obtain the updating rule for S:

S = S ⊙r ¯A⊤ ¯AS

¯A⊤ ¯Z

,

(8)

where we mitigate the optimization of Γ.

After we optimize S, E could be further updated with

the following l1-optimization problem:

E = arg min

E

βkEk1 + µkS − H − Ek2
β
2µ

= sign(cid:16)S − H(cid:17)max(cid:16)|S − H| −

F

, 0(cid:17).

(9)

Learning Latent Semantics Z: Given W, S, we can update
Z by minimizing Eq. (10) w.r.t Z:

Z = arg min

E(cid:16)kW ˜X − Zk2
α + 1(cid:16)W E( ˜X) + αAS(cid:17).

Z
1

=

F(cid:17) + αkZ − ASk2

F

(10)
For better clarity, we present the optimization details in
Algorithm 1, where we list the initialization of some vari-
ables. To ensure a good convergence, we initialize W with
the mapping between X and A. Other variables are initial-
ized with random matrices for simplicity. α and β are two
hyper parameters, which would be selected based on the
validation set.

In ZSL tasks, there are different cases to do evaluation.
For zero-shot recognition, we are to predict their class la-
bel given any reference visual data. Considering a test data
xi
t, we could ﬁrst calculate its predicted semantic embed-
ding with W xi
t using semantic encoder, then compare with
the ground-truth semantic representation At with Ct classes
(Ct would cover both seen classes C and unseen classes
Cu). For zero-shot annotation, we just exploit the predicted
the semantics to search its attributes through several largest

Table 1. Statistics of four ZSL benchmarks.

Dataset

aP&aY

AwA2

CUB

SUN

#Training Categories

#Test Categories

20

12

40

10

150

50

645

72

#Samples

15,339

37,322

11,788

14,340

#Semantics

64

85

312

102

#Training Samples

5,932

23,527

7,057

10,320

#Test Seen Samples

1,483

5,882

1,764

2,580

#Test Unseen Samples

7,924

7,913

2,967

1,440

values. For zero-shot retrieval, we would adopt the given
semantics at to search the most similar visual samples over
predicted semantics W Xt.

4. Experiment

In this part, we conduct experiments on four ZSL bench-
marks, by comparing our proposed approach with state-of-
the-art ZSL from conventional and generalized ZSL tasks.

4.1. Dataset & Experimental Setting

Four zero-shot learning benchmarks are evaluated in our
experiments including SUN attribute dataset1, Animals with
Attributes 2 (AwA2)2, Caltech-UCSD Birds 2011 (CUB)3
and aPascal-aYahoo (aP&aY)4. Their statistics are provided
in Table 1. All these benchmarks are served with annotated
attributes.

Due to some unseen test categories in the original splits
for those four benchmarks belong to part of ImageNet [24],
Xian et al. recently proposed a new split protocol [32], tar-
geting at a true zero-shot evaluation. In our experiments,
we strictly follow the split protocol and adopt the 2048-D
ResNet-101 features for all four benchmarks [32]. More-
over, we utilize the continuous attributes for better ZSL per-
formance.

For our model with the k-nn graph, we adopt k = 10 as
default across various ZSL tasks simply. The trade-off pa-
rameters are chosen from the range [10−2, 102] according
to the evaluation performance on the labeled samples from
the seen categories in the validation set. Later on, we di-
rectly utilize the selected parameters to conduct evaluation
on the original seen and unseen classes. Because different
initializations would result in different optimal solutions for
our proposed model, and we run ﬁve times of our model and
report the average results per task.

Baselines: The comparisons with the state-of-the-art in-
clude DAP/IAP [16], CONSE [19], CMT [25], SSE [35],

1http://cs.brown.edu/˜gmpatter/sunattributes.

html

2https://cvml.ist.ac.at/AwA2/
3http://www.vision.caltech.edu/visipedia/

CUB-200-2011.html

4http://vision.cs.uiuc.edu/attributes/

6195

Table 2. Conventional zero-shot recognition in terms of top-1 ac-
curacy (%) on SUN, CUB, AWA2 and aP&aY benchmarks with
ResNet visual features.

Method

SUN CUB AwA2

aP&aY

DAP [16]
IAP [16]

CONSE [19]

CMT [25]
SSE [35]

LATEM [30]

ALE [1]

DEVISE [10]

SJE [2]

ESZSL [23]
SYNC [5]
SAE [12]
PSR [3]

ZSKL [34]

Ours

39.9
19.4
38.8
39.9
51.5
55.3
58.1
56.5
53.7
54.5
56.3
40.3
61.4
61.7
62.8

40.0
24.0
34.3
34.6
43.9
49.3
54.9
52.0
53.9
53.9
55.6
33.3
63.8
51.7
64.2

46.1
35.9
44.5
37.9
61.0
55.8
62.5
59.7
61.9
58.6
46.6
54.1
56.0
70.5

67.8

33.8
36.6
26.9
28.0
34.0
35.2
39.7
39.8
32.9
38.3
23.9
8.3
38.4
45.3
46.2

LATEM [30], ALE [1], DEVISE [10], SJE [2], ESZSL [23],
SYNC [5], SAE [12], PSR [3], and ZSKL [34]. The last
two are the most recently proposed ZSL algorithms. PSR
also aims to explore the relation structure by mining the
most similar and dissimilar pairs, thus could learn a more
discriminative metric. ZSKL attempts to learn a non-linear
mapping across the visual feature and attribute spaces by
exploring kernel functions. Note that results are directly
copied from other published papers, i.e., [32, 3, 14], since
we explore the exactly same protocol and the same set of
data. Moreover, the approaches encompass a wide range in
zero-shot learning area.

Evaluation Metric: Top-1 accuracy is widely-used to mea-
sure single-label classiﬁcation accuracy. That is the pre-
diction is correct for the assignment class label equals to
the ground-truth one.
In zero-shot learning, top-1 accu-
racy per-class is more valued, since high performance is en-
couraged in both densely and sparsely populated categories.
Hence, we average the accurate predictions independently
for each category before dividing their cumulative sum, w.r.t
the number of categories [32].

For generalized zero-shot learning (GZSL) scenario, the
search space during the evaluation stage is not only re-
stricted to the unseen classes (U), but also consists of the
seen classes (S). Thus, the harmonic mean5 is more popular
to measure the GZSL performance by calculating the aver-
age per-class top-1 accuracy on training and test categories
[32]. This strategy is able to ﬂag up those ZSL models over-
ﬁtting to either seen or unseen classes.

5https://en.wikipedia.org/wiki/Harmonic_mean

4.2. Conventional Zero shot Recognition

This section reports the comparison results (Table 2) on
conventional zero-shot recognition in terms of top-1 accu-
racy. From Table 2, we witness that our proposed algorithm
is able to obtain better performance by comparing with oth-
ers. This veriﬁes that our approach learns a more effec-
tive visual-semantic relation from seen data for unseen data
analysis. The obtained improvements are very consistent
according to the complexity of visual images of each bench-
mark, which e can observe from the well-known compli-
cated CUB dataset. On other hand, our model still performs
very well on SUN benchmark, which contains more classes
and relatively fewer training instances per class. For AwA2,
only class-wise attributes are provided, thus it is challenging
for our model to recover the missing attributes by exploring
the relation across different instances and categories.

Compared with PSR and ZSKL, which explore non-
linear neural networks or kernel functions to link visual and
semantics, our model also preserves such non-linear prop-
erty. Since we attempt to learn a latent semantic represen-
tation, it builds a bridge to link the visual features and pro-
vided semantics. Especially, we utilize an adaptive graph
to reconstruct the latent semantics. All these provide more
ﬂexibility to the learned generic encoder and thus is able to
improve the generalizability on unseen classes.

Furthermore, qualitative results are reported for our de-
signed model. We aim to list what kinds of visual infor-
mation our algorithm is able to capture only given the se-
mantic representation for the unseen categories. Figure 2
reports 10 out of 50 unseen categories in CUB dataset, in
which we show top-3 accurately-retrieved samples (middle
row in red) while the top-3 misclassiﬁed samples (last row
in blue) into each unseen class. Observing from the top im-
ages, the proposed model reasonably discovers discrimina-
tive visual information for each unseen category only using
its semantic representation. We further notice that the mis-
classiﬁed visual images have much different visual appear-
ances to that of assigned class. Hence, it is hard to recognize
them, even for humans.

4.3. Generalized Zero shot Recognition

In a more general application, we are not sure if the test
image belongs to the seen categories or totally unseen cat-
egories, which is more interesting from a practical point of
view. In this sense, a lot of research efforts focus on the gen-
eralized zero-shot challenge, in which the test set are built
on both seen and unseen category data.

Table 3 reports the generalized ZSL performance of all
comparisons, where U → U + S and S → U + S repre-
sent two types of GZSL that evaluate if learned unseen/seen
models are confused to each other. H denotes the harmonic
mean. From Table 3, we can easily notice that generalized
ZSL results are signiﬁcantly lower than conventional ZSL

6196

Groove 
Billed Ani

Yellow headed 

Brandt 

Blackbird

Cormorant

Bronzed
Cowbird

Brown
Creeper

Black billed

Cuckoo

Yellow billed

Yellow bellied

Cuckoo

Flycatcher

Northern
Fulmar

Boat tailed

Grackle

Figure 2. Qualitative evaluation of our proposed model on CUB benchmark, in which 10 unseen category labels are listed on the top. Then,
we report the top-3 samples assigned to each category in the middle. Finally, the last row shows the top-3 misclassiﬁed instances.

Table 3. Generalized ZSL recognition performance (%) across four benchmarks.

Method

DAP [16]

IAP [16]

CONSE [19]

CMT [25]

SSE [35]

LATEM [30]

ALE [1]

DEVISE [10]

SJE [2]

ESZSL [23]

SYNC [5]

SAE[12]

PSR[3]

ZSKL [34]

Ours

SUN

CUB

U→S+U

S→S+U

4.2

1.0

6.8

8.1

2.1

14.7

21.8

16.9

14.7

11.0

7.9

8.8

20.8

20.1

20.7

25.1

37.8

39.9

21.8

36.4

28.8

33.1

27.4

30.5

27.9

43.3

18.0

37.2

31.4

36.4

H

7.2

1.8

11.6

11.8

4.0

19.5

26.3

20.9

19.8

15.8

13.4

11.8

26.7

24.5

26.4

U→S+U

S→S+U

1.7

0.2

1.6

7.2

8.5

15.2

23.7

23.8

23.5

12.6

11.5

7.8

20.7

21.6

22.3

67.9

72.8

72.2

49.8

46.9

57.3

62.8

53.0

59.2

63.8

70.9

54.0

73.8

52.8

71.6

H

3.3

0.4

3.1

12.6

14.4

24.0

34.4

32.8

33.6

21.0

19.8

13.6

32.3

30.6

34.0

AwA2

U→S+U

S→S+U

0.0

0.9

0.5

0.5

8.1

11.5

14.0

17.1

8.0

5.9

10.0

1.1

24.6

18.9

23.8

84.7

87.6

90.6

90.0

82.5

77.3

81.8

74.7

73.9

77.8

90.5

82.2

54.3

82.7

83.2

aP&aY

U→S+U

S→S+U

4.8

5.7

0.0

1.4

0.2

0.1

4.6

4.9

3.7

2.4

7.4

0.4

13.5

10.5

12.7

78.3

65.6

91.2

85.2

78.9

73.0

73.7

76.9

55.7

70.1

66.3

80.9

51.4

76.2

74.3

H

9.0

10.4

0.0

2.8

0.4

0.2

8.7

9.2

6.9

4.6

13.3

0.9

21.4

18.5

21.7

H

0.0

1.8

1.0

1.0

14.8

20.0

23.9

27.8

14.4

11.0

18.0

2.2

33.9

30.8

37.0

ones. This results from the fact that seen categories are in-
cluded in the search space, that play as distractors for the
unseen samples.

An interesting phenomenon is that compatibility learn-
ing algorithms, e.g., DEVISE, ALE and SJE, are able to
obtain good performance on unseen classes. However,
these approaches perform well on seen classes, since they
seek independent attribute or object classiﬁers, e.g., DAP

and CONSE. Compared with these methods, our proposed
model also achieves very competitive results in each metric,
especially in harmonic mean measurement. In terms of the
harmonic mean measurement, our proposed approach per-
forms the best on SUN, AwA2, and aP&aY datasets while
the second best on CUB dataset, where ALE outperforms
others). This also veriﬁes the effectiveness of our proposed
approach in generalized ZSL tasks.

6197

)

%

(
 
y
c
a
r
u
c
c
A
1
p
o
T

 

70

60

50

40

30

20

MSLE-I
MLSE-A
MLSE-G
MLSE-L
MLSE

SUN

CUB

AwA2

aP&aY

(a)

l

e
u
a
V
 
e
v
i
t
c
e
b
O

j

9000

8000

7000

6000

5000

4000

3000

2000

1000

0

AwA2
SUN
CUB
aP&aY

70

65

60

55

50

)

%

(
 
y
c
a
r
u
c
c
A
1
p
o
T

 

45
2

1.5

1

0.5

0.1

5

10

15

20
(b)

25

30

35

40

45

50

Iterations

log(,)

-2

-1.5

-1

-0.5

0.1

0.5

-0.1
0
log(-)

0
-0.1

-0.5

-1

-1.5

-2

2

1

1.5
(c)

Figure 3. (a) evaluation on different variants; (b) convergence curves of our proposed algorithm for four ZSL tasks; (c) parameter inﬂuence
of α and β using CUB ZSL task.

4.4. Empirical Analysis

First of all, we evaluate on several variants of our pro-
posed MLSE to dive deeper to the efﬁcacy of each compo-
nent. 1). MLSE-L denotes we use F-norm to replace the
l1-norm in Eq. (4); 2). MLSE-G means Eq. (3); 3). MLSE-
A represents we use a pre-deﬁned graph G instead in Eq.
(3) (i.e., β = 0); 4). MLSE-I is the version that we set S as
the identity matrix. Then, we conduct experiments on four
benchmarks and report the comparison results in Figure
3(a), where we notice that the performance drops signiﬁcant
when we directly enforce latent semantics Z to be close to
the given A. The performance increases a lot with a graph
reconstruction format, which denotes the graph reconstruc-
tion is capable of compensating the attributes across various
samples and categories. Moreover, the adaptive graph could
contribute to enhancing the performance over different ZSL
tasks, which means the adaptive graph is able to automati-
cally capture the relationship across the latent semantics and
the given semantics. Finally, we also witness the improve-
ments with a sparse l1-sparse regularizer.

Secondly, we show our model’s convergence from ex-
perimental side empirically. The convergence curves of
four benchmarks on our proposed algorithm are presented
in Figure 3 (b), where we observe that our model has a good
convergence after several iterations, especially after 40 iter-
ations. The experimental results show our model can con-
verge well.

Thirdly, we testify the parameter inﬂuence in terms of
recognition performance to evaluate the two novel regu-
larizers. We jointly evaluate α and β on CUB tasks with
ResNet features. From Figure 3 (c), we notice that the
recognition performance would increase with the values of
α and β becoming larger, which indicate both parameters
play an important role in our semantic encoder.

Finally, we visualize 10 unseen AwA2 categories with
their learned latent semantics Z using ResNet features as
the input. To be speciﬁc, we explore t-SNE6 to embed the
learned latent semantics of the unseen data points to a 2-D

6https://lvdmaaten.github.io/tsne/

100

80

60

40

20

0

-20

-40

-60

-80

-100

-80

-60

-40

-20

0

20

40

60

80

100

Figure 4. Visualization of 10 unseen class data points from AwA2
with the learned semantics Z. The same color denotes the same
category data points.

plane in Figure 4. From the results, we notice there are most
classes are well separate, while some samples from unseen
classes are overlapped. This indicates our model is valid in
generalizing to unseen classes.

5. Conclusion

In this paper, we developed a novel zero-shot learning
algorithm through learning adaptive latent semantic repre-
sentation. To be speciﬁc, we presented an effective knowl-
edge transfer model by jointly seeking a generic semantic
encoder and learning latent semantic representation. To
augment the visual space of seen classes, we exploited a
marginalized denoising strategy to cover the unseen classes.
Furthermore, we sought an adaptive reconstruction coef-
ﬁcient to learn the latent semantic representation by cap-
turing more intrinsic information from the given semantics.
Conventional and generalized ZSL evaluations on four ZSL
benchmarks were testiﬁed to demonstrate the effectiveness
of our proposed marginalzied semantic encoder.

6198

References

[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-
embedding for image classiﬁcation. TPAMI, 38(7):1425–
1438, 2016. 2, 6, 7

[2] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-
uation of output embeddings for ﬁne-grained image classiﬁ-
cation. In CVPR, pages 2927–2936, 2015. 2, 6, 7

[3] Y. Annadani and S. Biswas. Preserving semantic relations
for zero-shot learning. In CVPR, pages 7603–7612, 2018. 1,
2, 6, 7

[4] M. Bucher, S. Herbin, and F. Jurie. Improving semantic em-
bedding consistency by metric learning for zero-shot classif-
ﬁcation. In ECCV, pages 730–746. Springer, 2016. 1

[5] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Syn-
thesized classiﬁers for zero-shot learning. In CVPR, pages
5327–5336, 2016. 2, 6, 7

[6] S. Changpinyo, W.-L. Chao, and F. Sha. Predicting visual
exemplars of unseen classes for zero-shot learning. In ICCV,
pages 3476–3485, 2017. 1

[7] L. Chen, H. Zhang, J. Xiao, W. Liu, and S.-F. Chang. Zero-
shot visual recognition using semantics-preserving adversar-
ial embedding network. In CVPR, volume 2, 2018. 1, 2

[8] Z. Ding, M. Shao, and Y. Fu. Low-rank embedded ensemble
semantic dictionary for zero-shot learning. In CVPR, pages
2050–2058, 2017. 1

[9] Z. Ding, M. Shao, and Y. Fu. Generative zero-shot learning
via low-rank embedded semantic dictionary. TPAMI, 2018.
1, 2

[10] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
T. Mikolov, et al. Devise: A deep visual-semantic embed-
ding model. In NeurIPS, pages 2121–2129, 2013. 6, 7

[11] H. Jiang, R. Wang, S. Shan, Y. Yang, and X. Chen. Learning
discriminative latent attributes for zero-shot classiﬁcation. In
CVPR, pages 4223–4232, 2017. 1, 2

[12] E. Kodirov, T. Xiang, and S. Gong. Semantic autoencoder
for zero-shot learning. In CVPR, pages 4447–4456, 2017. 1,
2, 6, 7

[13] S. Kolouri, M. Rostami, Y. Owechko, and K. Kim.

Joint
In AAAI, pages 3431–

dictionaries for zero-shot learning.
3439, 2018. 1

[14] V. Kumar Verma, G. Arora, A. Mishra, and P. Rai. General-
ized zero-shot learning via synthesized examples. In CVPR,
pages 4281–4289, 2018. 1, 6

[15] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to
detect unseen object classes by between-class attribute trans-
fer. In CVPR, pages 951–958, 2009. 2

[16] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-
based classiﬁcation for zero-shot visual object categoriza-
tion. TPAMI, 36(3):453–465, 2014. 5, 6, 7

[17] Y. Liu, Q. Gao, J. Li, J. Han, and L. Shao. Zero shot learn-
ing via low-rank embedded semantic autoencoder. In IJCAI,
pages 2490–2496, 2018. 1, 2

[18] L. Maaten, M. Chen, S. Tyree, and K. Weinberger. Learning
with marginalized corrupted features. In ICML, pages 410–
418, 2013. 3, 4

[19] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,
A. Frome, G. Corrado, and J. Dean. Zero-shot learning by
convex combination of semantic embeddings. 2014. 5, 6, 7

[20] D. Parikh and K. Grauman. Relative attributes.

In ICCV,

pages 503–510. IEEE, 2011. 2

[21] P. Peng, Y. Tian, T. Xiang, Y. Wang, and T. Huang. Joint
learning of semantic and latent attributes. In ECCV, pages
336–353. Springer, 2016. 2

[22] G.-J. Qi, W. Liu, C. Aggarwal, and T. Huang. Joint inter-
modal and intramodal label transfers for extremely rare or
unseen classes. TPAMI, 39(7):1360–1373, 2017. 1

[23] B. Romera-Paredes and P. Torr. An embarrassingly sim-
In Proceedings of The
ple approach to zero-shot learning.
32nd International Conference on Machine Learning, pages
2152–2161, 2015. 1, 2, 6, 7

[24] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
IJCV, 115(3):211–252, 2015. 5

[25] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot
In NeurIPS, pages

learning through cross-modal transfer.
935–943, 2013. 5, 6, 7

[26] J. Song, C. Shen, J. Lei, A.-X. Zeng, K. Ou, D. Tao, and
M. Song. Selective zero-shot classiﬁcation with augmented
attributes. In ECCV, pages 468–483, 2018. 1

[27] W. Sun and Y.-X. Yuan. Optimization theory and meth-
ods: nonlinear programming, volume 1. Springer Science
& Business Media, 2006. 4

[28] X. Wang, Y. Ye, and A. Gupta. Zero-shot recognition via se-
mantic embeddings and knowledge graphs. In CVPR, pages
6857–6866, 2018. 1

[29] Z. Wen and W. Yin. A feasible method for optimization
with orthogonality constraints. Mathematical Programming,
142(1-2):397–434, 2013. 4

[30] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and
B. Schiele. Latent embeddings for zero-shot classiﬁcation.
In CVPR, pages 69–77, 2016. 6, 7

[31] Y. Xian, T. Lorenz, B. Schiele, and Z. Akata. Feature gener-
ating networks for zero-shot learning. In CVPR, pages 5542–
5551, 2018. 1, 2

[32] Y. Xian, B. Schiele, and Z. Akata. Zero-shot learning-the
In CVPR, pages 3077–3086,

good, the bad and the ugly.
2017. 5, 6

[33] X. Xu, F. Shen, Y. Yang, D. Zhang, H. T. Shen, and J. Song.
Matrix tri-factorization with manifold regularizations for
zero-shot learning. In CVPR, pages 3798–3807, 2017. 1
[34] H. Zhang and P. Koniusz. Zero-shot kernel learning.

In

CVPR, pages 7670–7679, 2018. 1, 2, 6, 7

[35] Z. Zhang and V. Saligrama. Zero-shot learning via semantic
similarity embedding. In ICCV, pages 4166–4174, 2015. 5,
6, 7

[36] Z. Zhang and V. Saligrama. Zero-shot learning via joint
In CVPR, pages 6034–6042,

latent similarity embedding.
2016. 2

[37] H. Zhao, Z. Ding, and Y. Fu. Multi-view clustering via deep

matrix factorization. In AAAI, pages 2921–2927, 2017. 5

[38] Y. Zhu, M. Elhoseiny, B. Liu, X. Peng, and A. Elgammal. A
generative adversarial approach for zero-shot learning from
noisy texts. In CVPR, pages 1004–1013, 2018. 1, 2

6199

