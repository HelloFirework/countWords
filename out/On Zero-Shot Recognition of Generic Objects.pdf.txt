On zero-shot recognition of generic objects

Tristan Hascoet

Yasuo Ariki

Tetsuya Takiguchi

tristan.hascoet@gmail.com

ariki@kobe-u.ac.jp

takigu@kobe-u.ac.jp

Graduate School of System Informatics, Kobe University, Japan

Abstract

Many recent advances in computer vision are the result
of a healthy competition among researchers on high qual-
ity, task-speciﬁc, benchmarks. After a decade of active re-
search, zero-shot learning (ZSL) models accuracy on the
Imagenet benchmark remains far too low to be considered
for practical object recognition applications.
In this pa-
per, we argue that the main reason behind this apparent
lack of progress is the poor quality of this benchmark. We
highlight major structural ﬂaws of the current benchmark
and analyze different factors impacting the accuracy of ZSL
models. We show that the actual classiﬁcation accuracy of
existing ZSL models is signiﬁcantly higher than was previ-
ously thought as we account for these ﬂaws. We then intro-
duce the notion of structural bias speciﬁc to ZSL datasets.
We discuss how the presence of this new form of bias al-
lows for a trivial solution to the standard benchmark and
conclude on the need for a new benchmark. We then de-
tail the semi-automated construction of a new benchmark
to address these ﬂaws.

1. Introduction

Datasets play a leading role in computer vision research.
Perhaps the most striking example of the impact a dataset
can have on research has been the introduction of Imagenet
[2]. The new scale and granularity of Imagenet’s coverage
of the visual world has paved the way for the success and
wide spread adoption of CNN [8, 11] that have revolution-
ized generic object recognition.

The current best-practice for the development of a prac-
tical object recognition solution consists in collecting and
annotating application-speciﬁc training data to ﬁne-tune a
large Imagenet-pretrained CNN on. This data annotation
process can be prohibitively expensive for many applica-
tions which hinders the wide-spread usage of these tech-
nologies. ZSL models generalize the recognition ability of
traditional image classiﬁers to unknown classes, for which
no image sample is available for training. The promise of

ZSL for generic object recognition is huge: to scale up the
recognition capacity of image classiﬁers beyond the set of
annotated training classes. Hence ZSL has the potential to
be of great practical impact as they would considerably ease
the deployment of object recognition technologies by elim-
inating the need for expensive task-speciﬁc data collection
and ﬁne-tuning processes.

Despite its great promise, and after a decade of active
research [10], the accuracy of ZSL models on the standard
Imagenet benchmark [3] remain far too low for practical
applications. To better understand this lack of progress, we
analyzed the errors of several ZSL baselines. Our analysis
leads us to identify two main factors impacting the accuracy
of ZSL models: structural ﬂaws in the standard evaluation
protocol and poor quality of both semantic and visual sam-
ples. On the bright side of things, we show that once these
ﬂaws are taken into account, the actual accuracy of existing
ZSL models is much higher than was previously thought.

On the other hand, we show that a trivial solution outper-
forms most existing ZSL models by a large margin, which
is upsetting. To explain this phenomenon, we introduce the
notion of structural bias in ZSL datasets. We argue that
ZSL models should aim to develop compositional reason-
ing abilities, but the presence of structural bias in the Ima-
genet benchmark favors solutions based on a trivial one to
one mapping between training and test classes. We come to
the conclusion that a new benchmark is needed to address
the different problems identiﬁed by our analysis and, in the
last section of this paper, we detail the semi-automated con-
struction of a new benchmark we propose.

To structure our discussion, we ﬁrst brieﬂy review pre-
liminaries on ZSL in Section 3. Section 4 details our anal-
ysis of the different factors impacting the accuracy of ZSL
models on the standard benchmark. Section 5 introduces
the notions of structural bias, and propose a way to measure
and minimize its impact in the construction of a new bench-
mark. Finally, Section 6 summarizes the construction of our
proposed benchmark. For space constraint, we only include
the main results of our analysis in the body of this paper.
We refer interested readers to the supplementary material
for additional results and details of our analysis.

43219553

2. Related Work

2.1. ZSL datasets

Early research on ZSL has been carried out on relatively
small scale or domain speciﬁc benchmarks [9, 14, 19], for
which human-annotated visual attributes are proposed as se-
mantic representations of the visual classes. On the one
hand, these benchmarks have provided a controlled setup
for the development of theoretical models and the accurate
tracking of ZSL progress. On the other hand, it is unclear
whether approaches developed on such dataset would gen-
eralize to the more practical setting of zero-shot generic ob-
ject recognition. For instance, in generic object recognition,
manually annotating each and every possible visual class of
interest with a set of visual attributes is impractical due to
the diversity and complexity of the visual world.

The Imagenet dataset [2] consists of more than 13 mil-
lion images scattered among 21,845 visual classes.
Ima-
genet relies on Wordnet [12] to structure its classes: each
visual class in Imagenet corresponds to a concept in Word-
net. Frome et al. [3] proposed a benchmark for ZS generic
object recognition based on the Imagenet dataset, which has
been widely adopted as the standard evaluation benchmark
by recent works [13, 20, 15, 1, 21, 7, 18]. Using word
embeddings as semantic representations, they use the 1000
classes of the ILSVRC dataset as training classes and pro-
pose different test splits drawn from the remaining 20,845
classes of the Imagenet dataset based on their distance to the
training classes within the Wordnet hierarchy: the 2-hops,
3-hops and all test splits.

Careful inspection of these test splits revealed a confu-
sion in their name: The 2-hops test split actually consists of
the set of 1589 test classes directly connected to the train-
ing set classes in Wordnet, i.e; within 1 hop of the training
set. Similarly, the 3-hops test set actually corresponds to the
test classes within 2-hops. In this paper, we will refer to the
standard test splits by the name of their true conﬁguration:
1-hop, 2-hops and all, as illustrated in Figure 1.

2.2. Dataset bias

Bias in datasets can take many forms, depending on the
speciﬁc target task. Torralba et al. [17] investigates bias
in generic object recognition. The notion of structural bias
we introduce in Section 5 is closely related to the notion of
negative set bias they analyze.

As more complex tasks are being considered, more in-
sidious forms of bias sneak into our datasets. In VQA, the
impressive results of early baseline models have later been
shown to be largely due to statistical biases in the ques-
tion/answers pairs [4, 6, 5]. Similar to these works, we will
show that a trivial solution leveraging structural bias in the
Imagenet ZSL benchmark outperforms early ZSL baselines.
Xian et al. [21] identify structural incoherences in small-

scale ZSL benchmarks and proposes new test splits to rem-
edy them. Closely related to our work, they also observe a
correlation between test class sample population and clas-
siﬁcation accuracy in the Imagenet ZSL benchmark. How-
ever, their analysis mainly focuses on small-scale bench-
marks and the comparison of existing ZSL models, while
we analyze the ZSL benchmark for generic object recogni-
tion in more depth.

3. Preliminaries

ZSL models aim to recognize unseen classes, for which
no image sample is available to learn from. To do so, ZSL
models use descriptions of the visual classes, i.e., represen-
tations of the visual classes in a semantic space shared by
both training and test classes. To evaluate the out-of-sample
recognition ability of models, ZSL benchmarks split the full
set of classes C into disjoint training and test sets. ZSL
benchmarks are fully deﬁned by three components: a set of
training and test classes (Ctr, Cte), a set of labeled images
X, and a set of semantic representations Y :

Ctr ∪ Cte ⊂ C
Ctr ∩ Cte = ∅
Y = {yc ∈ Rd ∀c ∈ C}
X = {(x, c) ∈ R3×h×w × C}
T r = {(x, yc) | c ∈ Ctr}
T e = {(x, yc) | c ∈ Cte}

(1a)

(1b)

(1c)

(1d)

(1e)

(1f)

ZSL models are typically trained to minimize a loss
function L over a similarity score E between image and
semantic features of the training sample set with respect to
the model parameters θ.

θ∗ = argminθ E(x,y)∈T rL(Eθ(x, y) + Ω(θ))

(2)

In the standard ZSL setting, test samples xte are classiﬁed
among the set of unseen test classes by retrieving the class
description y of highest similarity score:

c = argmaxc∈Cte E(xte, yc)

(3)

In the generalized ZSL setting, test samples are classiﬁed
among the full set of training and test classes:

c = argmaxc∈CE(xte, yc)

(4)

Xian et al. [20] have shown that many ZSL models can
be formulated within a same linear model framework, with
different training objectives and regularization terms. More
recently, Wang et al. [18] have proposed a Graph Convolu-
tional Network (GCN) model that has shown impressive im-
provements over the previous state of the art. In our study,
we will present results obtained with both a baseline linear
model [15] and a state of the art GCN model [18, 7].

43229554

4. Error analysis

In the previous section, we have mentioned that ZSL
benchmarks are fully deﬁned by three components: a set of
labeled images X, a set of semantic representations Y , and
the set of training and test classes (Ctr, Cte). In this section,
we analyze each of the standard benchmark components in-
dividually: We ﬁrst highlight inconsistencies in the conﬁg-
uration of the different test splits and show that these incon-
sistencies lead to many false negatives in the reported evalu-
ation of ZSL models outputs. Next, we identify a number of
factors impacting the quality of the word embeddings of vi-
sual classes and argue that visual classes with poor semantic
representations should be excluded from ZSL benchmarks.
We then observe that the Imagenet dataset contains many
ambiguous image samples. We deﬁne what a good image
sample means in the context of ZSL and propose a method
to automatically select such images.

4.1. Structural ﬂaws

Figure 1 illustrates the conﬁguration of test classes of
the standard test splits within the Wordnet hierarchy. This
conﬁguration leads to an obvious contradiction: test sets in-
clude visual classes of both parents and their children con-
cepts. Consider the problem of classifying images of birds
within the hop-1 test split as in Figure 1. The standard test
splits give rise to two possibly inconsistent scenarios:

Figure 1. Illustration of the standard test splits conﬁguration

A ZSL model may classify an image of the children class
Cathartid as its parent class Raptor. The standard bench-
mark considers such cases as classiﬁcation errors, while the
classiﬁcation is semantically correct.

A ZSL model may classify an image of the parent class
Raptor as one of its children class: Cathartid. Classiﬁcation
may be semantically correct or incorrect, depending on the
speciﬁc breed of raptor in the image, but we have no way to
automatically assess it without additional annotation. The
standard benchmark considers such cases as classiﬁcation
errors, while the classiﬁcation is semantically undeﬁned.

We refer to both of the above cases as false negatives.
Figure 2 illustrates the distribution of ZSL classiﬁcation

Figure 2. Distribution of the classiﬁcation outputs of different
ZSL models on the 1-hop test split. An image x can be either be
classiﬁed into its actual label c, the parent class of c, one of its chil-
dren class, or an unrelated class. Only the latter case constitutes a
deﬁnitive error.

outputs among these different scenarios on the 1-hop test
split. On the standard ZSL task for instance, the reported
accuracy of the GCN model is 21.8% while the actual (se-
mantically correct) accuracy should be somewhere in be-
tween 27.8% and 40.4%.

The ratio of false negatives per accuracy increases dra-
matically in the generalized ZSL setting. The linear base-
line reported accuracy is only 1.9%, while the actual
(semantically correct) accuracy lies between 16.0% and
41.1%. This is due to the fact that ZSL models tend to clas-
sify test images into their parent or children training class:
for example, Cathartid images tend to be classiﬁed as Vul-
ture. Appendix A of the supplementary material presents
results on the other standard splits on which we show that
the ratio of false negative per reported accuracy further in-
creases with with larger test splits.

4.2. Word embeddings

In this section, we identify two factors impacting the
quality of word embeddings and analyse their affect on ZSL
accuracy: polysemy and occurrence frequency. These prob-
lems naturally arise in the deﬁnition of large scale object
categories so they are inherent problems of ZS recognition
of generic objects. However, we argue that ZSL bench-
marks should provide a curated environment with high qual-
ity, unambiguous, semantic representations and that solu-
tions to tackle the special case of polysemous and rare
words should be separately investigated in the future.

43239555

4.2.1 Occurrence frequency

Word embeddings are learned in an unsupervised manner
from the co-occurrence statistics of words in large text cor-
pora. Common words are learned from plentiful statistics
so we expect them to provide more semantically meaning-
ful representations than rare words, which are learned from
scarce co-occurrence statistics. We found many Imagenet
class labels to be rare words (see Appendix B of the supple-
mentary materials), with as many as 33.7% of label words
appearing less than 50 times in Wikipedia. Here, we ques-
tion whether the few co-occurrence statistics from which
such rare word embeddings are learned actually provide any
visually discriminative information for ZSL.

To answer this question, we evaluate ZSL models on
different test splits of 100 classes: we split the Imagenet
classes into different subsets based on the occurrence fre-
quency of their label word. We independently evaluate the
accuracy of our model on each of these splits and report the
ZSL accuracy with respect to the average occurrence fre-
quency of the visual class labels in Figure 3.

word label with other visual classes. Figure 4 illustrates
the example of the word ”cairn”. Two visual classes share
the same label ”cairn”: One relates to the meaning of cairn
as a stone memorial, while the other refers to a dog breed.
This is problematic as both of these visual classes share the
same representation in the label space, so they are essen-
tially deﬁned as the same class although they correspond to
two visually very distinct concepts.

To deal with polysemy, we assume that all words have
one primary meaning, with possibly several secondary
meanings. We consider word embeddings to reﬂect the se-
mantics of their primary meaning exclusively, and discard
visual classes associated with the secondary meanings of
their word label. To automatically identify the ﬁrst mean-
ing of visual class labels. we implement a solution based
on both Wordnet and word embeddings statistics detailed in
the supplementary material.

Figure 3. Each dot in these ﬁgures represent the top-1 accuracy
(y-axis) of a 100 classes test split with respect to the test split char-
acteristics (x-axis): Left: Mean occurrence frequency of the test
class labels. Right: test classes of primary meaning, such as cairn
(monument), or secondary meaning, such as cairn (dog)

Our results highlight a strong correlation (r = 0.89) be-
tween word frequency and the Linear baseline accuracy as
test splits made of rare words strikingly under-perform test
splits made of more common words, although accuracy re-
mains well above chance (1%), even for test sets of very rare
words. Results are more nuanced for the GCN model (cor-
relation coefﬁcient r = 0.74), which can be explained by
the fact that GCN uses the Wordnet hierarchy information
in addition to word embeddings.

4.2.2 Polysemy

The English language contains many polysemous words,
which makes it difﬁcult to uniquely identify a visual class
with a single word. We found that half of the ImageNet
word labels are shared with at least one other Wordnet con-
cept, and that 38% of ImageNet classes share at least one

Figure 4. Illustration of polysemous words. Each color represents
the 100 nearest neighbors of a given word. ”Cairn” and its closest
neighbors are clustered around the stone and monument related
vocabulary, far away from dog-related vocabulary so we assign
the top visual class as primary meaning of the word cairn.

We conduct an experiment to assess both the impact of
polysemy on ZSL accuracy and the efﬁciency of our so-
lution. As in the previous section, we evaluate our ZSL
models on different test splits of 100 classes: We separately
evaluate test classes identiﬁed as the primary meaning of
their word label and test classes corresponding to the sec-
ondary meaning of their word label. Figure 3 reports the
accuracy obtained on these different test splits. We can see
a signiﬁcant boost in the ZSL accuracy of test classes whose
word labels are identiﬁed as primary meanings.
In com-
parison, test splits made exclusively of secondary meanings
performed poorly. This conﬁrms that polysemy does in-
deed impact ZSL accuracy, and suggests that our solution
for primary meaning identiﬁcation allows addressing this
problem.

4.3. Image samples

The ILSVRC dataset consists of a high-quality curated
subset of the Imagenet dataset. The current ZSL bench-
mark uses ILSVRC classes as training classes and classes
drawn from the remainder of the Imagenet dataset as test

43249556

would like to further ﬁlter quality test images sample-wise.
But what makes a good candidate image for a ZSL bench-
mark? How can we measure the quality of a sample? We
argue that ZSL benchmarks should only reﬂect the zero-shot
ability of models: ZSL benchmarks should evaluate the ac-
curacy of ZSL models relatively to the accuracy of standard
non-ZSL models. Hence, we deﬁne a good ZSL sample as
an image unambiguous enough to be correctly classiﬁed by
standard image classiﬁers trained in a supervised manner.

To automatically ﬁlter such quality samples, we ﬁne-tune
and evaluate a standard CNN in a supervised manner on the
set of candidate test classes. We consider consistently miss-
classiﬁed samples to be too ambiguous for ZSL and only
select samples that were correctly classiﬁed by the CNN
Details of this selection process are presented in Appendix
C of the supplementary material.

4.4. Dataset Summary

Figure 6 summarizes the impact of the different factors
we analyzed on the top-1 classiﬁcation error of both our
baseline models on the ”1-hop” test split. The error rate of
the Linear model on the standard ZSL setting drops from
86% to 61% after removing ambiguous images, semantic
samples, and structural ﬂaws. The error rate of the GCN
model on the generalized setting drops from 90% to 47%.

sets, assuming similar standards of quality from these test
classes. Upon closer inspection, we found these test classes
to contain many inconsistencies and ambiguities.
In this
section, we detail a solution to automatically ﬁlter out am-
biguous samples so as to only select quality samples for our
proposed benchmark.

4.3.1 Class-wise selection

Xian et al. [20] have ﬁrst identiﬁed a correlation between
the sample population of visual classes and their classi-
ﬁcation accuracy. They conjecture that small population
classes are harder to classify because they correspond to
ﬁne-grained visual concepts, while large population classes
correspond to easier, coarse-grained concepts. Manual in-
spection of these classes lead us to a different interpretation:
First, we found no signiﬁcant correlation between sample
population and concept granularity (Appendix C). For ex-
ample, ﬁne-grained concepts such as speciﬁc species of
birds or dogs tend to have high sample populations. On the
other hand, we found many visually ambiguous concepts
such as ”ringer”, ”covering” or ”chair of state” to have low
sample populations. Such visually ambiguous concepts are
harder for crowd-sourced annotators to reach consensus on
labeling, resulting in lower population counts.

In Figure 5, we report the ZSL accuracy of our models on
different test splits with respect to their average population
counts. This ﬁgure shows a clear correlation between the
sample population and the accuracy of both models, with
low accuracy for low sample population classes. We use
the sample population as a rough indicator to quickly ﬁlter
out ambiguous visual classes and only consider classes with
sample population superior to 300 images as valid candidate
classes in our proposed dataset.

Figure 5. ZSL accuracy with respect to sample population sizes.
Left: Distribution of Imagenet class population size. 6.1% of Ima-
genet classes have less than 10 samples, 21.1% have less than 100
samples. Right: ZSL accuracy of different test splits with respect
to their mean sample population size.

4.3.2 Sample-wise selection

Even among the selected classes, we found many inconsis-
tent and ambiguous images to remain (Appendix C), so we

Figure 6. Estimation of the impact of different factors on the
reported error of existing models on the 1-hop test split

The GCN model is particularly sensitive to the structural
ﬂaws of the standard benchmark, but less sensitive to noisy
word embeddings than the linear baseline. This can be eas-
ily explained by the fact that GCN models rely on the ex-
plicit Wordnet hierarchy information as semantic data in ad-
dition to word embeddings. Additional results and details
on the methodology of our analysis are given in Appendix
D of the supplementary material.

43259557

5. Structural bias

ZSL models are inspired by the human ability to rec-
ognize unknown objects from a mere description, as it is
often illustrated by the following example: Without having
ever seen a zebra, a person would be able to recognize one,
knowing that zebras look like horses covered in black and
white stripes. This example illustrates the human capacity
to compose visual features of different known objects to de-
ﬁne and recognize previously unknown object categories.

Standard image classiﬁers encode class labels as local
representations (one-hot embeddings), in which each di-
mension represents a different visual class, as illustrated in
Figure 8. As such, no information is shared among classes
in the label space: visual class embeddings are equally dis-
tant and orthogonal to each other. The main idea behind
ZSL models is to instead embed visual classes into dis-
tributed representations: In label space, visual classes are
deﬁned by multiple visual features (horse-ish shape, stripes,
colors) shared among classes. Distributed representations
allow to deﬁne and recognize unknown classes by composi-
tion of visual features shared with known classes, in a sim-
ilar manner as the human ability described above.

The embedding of visual classes into distributed feature
representations is especially powerful since it allows to de-
ﬁne a combinatorial number of test classes by composition
of a possibly small set of features learned from a given set of
training classes. Hence, we argue that the key challenge be-
hind ZSL is to achieve ZS recognition of unknown classes
by composition of known visual features, following their
original inspiration of the human ability, and as made pos-
sible by distributed feature representations. In this section,
we will see that not all ZSL problems require such kind of
compositional ability. On the standard benchmark, we show
that a trivial solution based on local representations of vi-
sual classes outperform existing approaches based on word
embeddings. We show that this trivial solution is made pos-
sible by the speciﬁc conﬁguration of the standard test splits
and introduce the notion of structural bias to refer to the
existence of such trivial solutions in ZSL datasets.

5.1. Toy example

Figure 7 illustrates a toy ZSL problem in which, given
a training set of Horse and TV monitor images, the goal is
to classify images of Zebra and PC laptop. Let’s consider
training an image classiﬁer on the training set and directly
applying it to images from the test set. We can safely as-
sume that most zebra images will be classiﬁed as horses,
and most laptop samples as TV monitors. Hence, a triv-
ial solution to this problem consists in deﬁning a one to
one mapping between test classes and their closest train-
ing class: Horse=Zebra and TV monitor=PC laptop. This
example makes it fairly obvious that not all ZSL problems
require the ability to compose visual features to solve.

Figure 7. Illustration of the toy example. Left: Wordnet-like class
hierarchy. Training classes are shown in red and test class in green.
Right: Illustration of image samples. The black captions represent
the distance between classes as their shortest path length.

Classiﬁcation problems deﬁne a close-world assump-
tion: As all test samples are known to belong to one of the
test classes, classifying an image x into a given test class c
means that x is more likely to belong to c than other classes
of the test set. In other words, classiﬁcation is performed
relatively to a negative set of classes [17]. What made this
trivial ZSL solution possible is the fact that test classes of
our toy example are very similar to one of the training class,
relatively to their negative set. This allowed us to identify a
one-to-one mapping by similarity between training and test
classes. We refer to this trivial solution as a similarity-based
solution, in opposition to solutions based on the composi-
tion of visual features.

Figure 8. Illustration of local (one-hot, on the left) and distributed
(right) representations of visual classes. The similarity-based solu-
tion encodes both training and test classes as local representations.
Composition-based solutions need distributed representations.

As illustrated in Figure 8, the similarity mapping be-
tween test and training classes can be directly embedded in
the semantic space using local representations. The trivial
solution consists in assigning to test classes the exact same
semantic representation as their most similar training class.
Consider applying these semantic embeddings within a ZSL
framework to our toy problem: classifying a test image x as
a Horse relatively to the negative set of TV within the train-
ing set becomes strictly equivalent to classifying x as Zebra
relatively to its negative set PC within the test set. Hence,

43269558

any existing ZSL model using these local embeddings in-
stead of distributed representations like word embeddings
Y would converge to the same solution.

5.2. Standard benchmark

Besides our toy example, how well would this trivial so-
lution perform on the standard benchmark? To implement
it, we used the Linear baseline model [15] with local rep-
resentations inferred from the Wordnet hierarchy (see Ap-
pendix E), but any model would essentially converge to a
similar solution. Table 1 compares the accuracy of this triv-
ial solution to state of the art models as reported in [21, 7].
The trivial similarity-based solution outperforms existing
ZSL models by a signiﬁcant margin. Only GCN-based
models [7], which we discuss in the next section, seem to
outperform our trivial solution.

Table 1. Top-1 accuracy on the standard test splits (top) as reported
for linear baselines in [21], (middle) as reported for GCN-based
models in [7] and (down) obtained by our trivial solution

model

SYNC [1]

CONSE [13]
ESZSL [15]
LATEM [20]
DEVISE[3]
CMT [16]
GCNZ [18]
ADGPM [7]

Trivial

1-hop
9.26
7.63
6.35
5.45
5.25
2.88
19.8
26.6
20.27

2-hops

2.29
2.18
1.51
1.32
1.29
0.67
4.1
6.3
3.59

all
0.96
0.95
0.62
0.5
0.49
0.29
1.8
3.0
1.53

5.3. Measuring structural bias

In our toy example, we have hinted at the fact that struc-
tural bias emerges for test sets in which test classes are rel-
atively similar to training classes, while being comparably
more dissimilar to each other (to their negative set). To con-
ﬁrm this intuition, we deﬁne the following structural ratio:

r(c) =

R(Cte) =

minc′∈Ctr d(c, c′)
minc′∈Cted(c, c′)
1
|Cte| X

r(c)

c∈Cte

(5a)

(5b)

In which c represents a visual class, Cte and Ctr repre-
sent test and training sets respectively, and d is a distance
reﬂecting similarity between two classes. Here, r(c) repre-
sents the ratio of the distance between c and its closest train-
ing class to the distance between c and its closest test class.
In our experiments, we use the the shortest path length be-
tween two classes in the Wordnet hierarchy as a measure of
distance d, although different metrics would be interesting
to investigate as well. We compute the structural ratio of a

test set R(Cte) as the mean structural ratio of its individ-
ual classes. Figure 9 shows the top-1 accuracy achieved by
baseline models on different test sets with respect to their
structural ratio R. As for previous experiments, we report
our results on test splits of 100 classes.

Figure 9. ZSL accuracy on different test sets with respect to their
structural ratio R(Cte).

On test splits of low structural ratio, the trivial solution
performs remarkably well, on par with the state of the art
GCN model. Such test splits are similar to the toy example
in which each test class is closely related to a training class
while being far away from other test classes in the Wordnet
hierarchy. As an example, the structural ratio of the test split
in our toy example is R(Cte) = 1/2 × (2/4 + 2/4) = 0.5,
which corresponds to the highest accuracies achieved by the
trivial solution. We say that such test split is structurally
biased towards similarity-based trivial solutions.

However, the accuracy of the similarity-based trivial so-
lution decreases sharply with the structural ratio until it
reaches near chance accuracy for the highest ratios. Hence
maximizing the structural ratio of test splits seems to be an
efﬁcient way to minimize structural bias. Although their
accuracy decrease with larger structural ratios, both GCN
and Linear models remain well above chance. These results
suggest that ZSL models based on word embeddings are in-
deed capable of compositional reasoning. At the very least,
they are able to perform more complex ZSL tasks than the
trivial similarity-based solution. Interestingly, as the triv-
ial solution converges towards chance accuracy, the GCN
model accuracy seems to converge towards the accuracy of
the ZSL baseline. This suggests that the main reason behind
the success of GCN models is that they efﬁciently leverage
the Wordnet hierarchy to exploit structural bias.

The 1-hop and 2-hops test splits of the standard bench-
mark consist of the set of test classes closest to the train-
ing classes within the Wordnet hierarchy. This leads to test
splits of very low structural ratio, similar to our toy exam-
ple. For instance, the 1-hop test split has a structural ratio of
0.55. It is an example of structural bias even more extreme
than our toy example as test classes are either children or
parent classes of a training class. In the next section, we

43279559

propose a new benchmark with maximal structural ratio in
order to minimize structural bias.

6. New Benchmark

6.1. Proposed Benchmark

In this section, we brieﬂy detail the semi-automated con-
struction of a new benchmark designed to ﬁx the different
ﬂaws of the current benchmark highlighted by our analy-
sis. For space constraints, a number of minor considerations
could not be properly presented in this paper. We detail
these additional considerations in Appendix F of the sup-
plementary material. Appendix F also provides additional
details regarding the different parameters and the level of
automation of each of the construction process. Appendix
G provides details on the code and data we release. Follow-
ing Frome et al. [3], we use the ILSVRC dataset as training
set, and propose a new test set. The selection of this new
test set proceeds in two steps:

In a ﬁrst step, we select a subset of candidate test classes
C ′ ⊂ C from the remaining 20,845 Imagenet classes based
on the statistics of image samples and word labels: We ﬁrst
ﬁlter out semantic samples Y ′ ⊂ Y corresponding to rare or
polysemous words of secondary meaning (Section 4.2). We
then discard visual classes of low sample population and ﬁl-
ter out ambiguous image samples using supervised learning
to select X ′ ⊂ X (Section 4.3). The set of candidate test
classes is the subset of visual classes C ′ ⊂ C for which
sufﬁciently high quality image and semantic samples were
selected.

In a second step, we deﬁne the test split Cte ⊂ C ′ as a
structurally consistent set of minimal structural bias: The
test set was carefully selected so as to contain no overlap
among its own classes nor with the training classes in order
to provide a structurally consistent test set for the gener-
alized ZSL setting. This test set consists of 500 classes of
maximal structural ratio R(Cte) so as to minimize structural
bias.

6.2. Evaluation

Table 2. Evaluation on the proposed benchmark. Accuracy in
the generalized ZSL setting are reported as harmonic means over
training and test accuracy following [21]

Model

Trivial

CONSE [13]
DEVISE [3]
ESZSL [15]
GCN-6 [18]
GCN-2 [7]
ADGPM [7]

ZSL

G-ZSL

@1
1.2

10.65
11.15
13.54
9.58
14.09
14.10

@5 @1 @5
3.9

0

0

25.10
29.52
32.61
27.19
35.12
36.03

0.12
7.87
4.59
4.81
4.96
4.90

19.34
26.10
25.53
23.35
30.35
29.96

Table 2 presents the evaluation of a number of base-
line models on the newly proposed benchmarks. A few
notable results stand out from this table: First, different
from the standard benchmark, CONSE [13] performs worse
than DEVISE [3]. The relatively high accuracy reported
by the CONSE model on the standard benchmark is most
likely due to the fact that word embeddings of test classes
are statistically close to the word embedding of their par-
ent/children test classes so that CONSE results more closely
ﬁt the trivial similarity-based trivial solution. We expect
model averaging methods to beneﬁt the most from the struc-
tural bias in the standard benchmark.

Second, the impressive improvements reported by GCN-
based models over linear baselines are signiﬁcantly re-
duced, although GCN models still outperform linear base-
lines. This result corroborates the observation, in Section
5, that GCN models tend to converge towards the results of
linear baseline models for high structural ratio.

7. Conclusion and Discussion

ZSL has the potential to be of great practical impact for
object recognition. However, as for any computer vision
task, the availability of a high quality benchmark is a pre-
requisite for progress. In this paper, we have shown major
ﬂaws in the standard generic object ZSL benchmark and
proposed a new benchmark to address these ﬂaws. More
importantly, we introduced the notion of structural bias in
ZSL dataset that allows trivial solutions based on simple
similarity matching in semantic space. We encourage re-
searchers to evaluate their past and future models on our
proposed benchmark. It seems likely that sound ideas may
have been discarded for their poor performance relative to
baseline models that beneﬁted most from structural bias.
Some of these ideas may be worth revisiting today.

Finally, we believe that a deeper discussion on the goals
and the deﬁnition of ZSL is still very much needed. There is
a risk in developing complex models to address poorly char-
acterized problems: Mathematical complexity can act as a
smokescreen of complexity that obfuscates the real prob-
lems and key challenges behind ZSL. Instead, we believe
that practical considerations grounded in common sense are
still very much needed at this stage of ZSL research. The
identiﬁcation of structural bias is a ﬁrst step towards a sound
characterization of ZSL problems. One practical way to
continue this discussion would be to investigate structural
bias in other ZSL benchmarks.

Aknowledgement

This work was supported in part by JSPS KAKENHI

(Grant No. JP17K00236 and No. JP17H01995).

43289560

[17] A. Torralba and A. A. Efros. Unbiased look at dataset bias.
In Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, pages 1521–1528. IEEE, 2011.

[18] X. Wang, Y. Ye, and A. Gupta. Zero-shot recognition via
semantic embeddings and knowledge graphs.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 6857–6866, 2018.

[19] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-

longie, and P. Perona. Caltech-ucsd birds 200. 2010.

[20] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and
B. Schiele. Latent embeddings for zero-shot classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 69–77, 2016.

[21] Y. Xian, B. Schiele, and Z. Akata. Zero-shot learning-
arXiv preprint

the bad and the ugly.

the good,
arXiv:1703.04394, 2017.

References

[1] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthe-
sized classiﬁers for zero-shot learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5327–5336, 2016.

[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. Ieee, 2009.

[3] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
T. Mikolov, et al. Devise: A deep visual-semantic embed-
ding model. In Advances in neural information processing
systems, pages 2121–2129, 2013.

[4] A. Jabri, A. Joulin, and L. van der Maaten. Revisiting visual
In European conference on

question answering baselines.
computer vision, pages 727–739. Springer, 2016.

[5] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei,
C. L. Zitnick, and R. Girshick. Clevr: A diagnostic dataset
for compositional language and elementary visual reasoning.
In Computer Vision and Pattern Recognition (CVPR), 2017
IEEE Conference on, pages 1988–1997. IEEE, 2017.

[6] K. Kaﬂe and C. Kanan. An analysis of visual question an-
swering algorithms. In Computer Vision (ICCV), 2017 IEEE
International Conference on, pages 1983–1991. IEEE, 2017.
[7] M. Kampffmeyer, Y. Chen, X. Liang, H. Wang, Y. Zhang,
and E. P. Xing. Rethinking knowledge graph propagation for
zero-shot learning. arXiv preprint arXiv:1805.11724, 2018.
Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.

[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

[9] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning
to detect unseen object classes by between-class attribute
transfer. In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 951–958. IEEE,
2009.

[10] H. Larochelle, D. Erhan, and Y. Bengio. Zero-data learning

of new tasks. In AAAI, volume 1, page 3, 2008.

[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[12] G. A. Miller. Wordnet: a lexical database for english. Com-

munications of the ACM, 38(11):39–41, 1995.

[13] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,
A. Frome, G. S. Corrado, and J. Dean. Zero-shot learning by
convex combination of semantic embeddings. arXiv preprint
arXiv:1312.5650, 2013.

[14] G. Patterson and J. Hays. Sun attribute database: Discover-
ing, annotating, and recognizing scene attributes. In Com-
puter Vision and Pattern Recognition (CVPR), 2012 IEEE
Conference on, pages 2751–2758. IEEE, 2012.

[15] B. Romera-Paredes and P. Torr. An embarrassingly simple
approach to zero-shot learning. In International Conference
on Machine Learning, pages 2152–2161, 2015.

[16] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot
learning through cross-modal transfer. In Advances in neural
information processing systems, pages 935–943, 2013.

43299561

