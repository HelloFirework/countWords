Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset

Tianyu Wang1

,

2∗ Xin Yang1

,

2∗ Ke Xu1

2

,

Shaozhe Chen1 Qiang Zhang1 Rynson W.H. Lau2†

1Dalian University of Technology

2City University of Hong Kong

Abstract

Removing rain streaks from a single image has been
drawing considerable attention as rain streaks can severely
degrade the image quality and affect the performance of ex-
isting outdoor vision tasks. While recent CNN-based de-
rainers have reported promising performances, deraining
remains an open problem for two reasons. First, existing
synthesized rain datasets have only limited realism, in terms
of modeling real rain characteristics such as rain shape, di-
rection and intensity. Second, there are no public bench-
marks for quantitative comparisons on real rain images,
which makes the current evaluation less objective. The core
challenge is that real world rain/clean image pairs can-
not be captured at the same time.
In this paper, we ad-
dress the single image rain removal problem in two ways.
First, we propose a semi-automatic method that incorpo-
rates temporal priors and human supervision to generate a
high-quality clean image from each input sequence of real
rain images. Using this method, we construct a large-scale
dataset of ∼29.5K rain/rain-free image pairs that covers a
wide range of natural rain scenes. Second, to better cover
the stochastic distribution of real rain streaks, we propose
a novel SPatial Attentive Network (SPANet) to remove rain
streaks in a local-to-global manner. Extensive experiments
demonstrate that our network performs favorably against
the state-of-the-art deraining methods.

1. Introduction

Images taken under various rain conditions often show
low visibility, which can signiﬁcantly affect the perfor-
mance of some outdoor vision tasks, e.g., pedestrian detec-
tion [30], visual tracking [37], or road sign recognition [48].
Hence, removing rain streaks from input rain images is an
important research problem. In this paper, we focus on the
single-image rain removal problem.

In the last decade, we have witnessed a continuous
progress on rain removal research with many methods pro-
posed [20, 29, 26, 5, 47, 9], through carefully modeling

∗ Joint ﬁrst authors. † Rynson Lau is the corresponding author, and

he led this project.

(a) Rain image

(b) Clean image

(c) SPANet

(d) DDN [11]

(e) DID-MDN [42]

(f) RESCAN [25]

Figure 1. We address the single-image rain removal problem in
two ways. First, we generate a high-quality rain/clean image pair
((a) and (b)) from each sequence of real rain images, to form a
dataset. Second, we propose a novel SPANet to take full advantage
of the proposed dataset. (c) to (f) compare the visual results from
SPANet and from state-of-the-art derainers.

the physical characteristics of rain streaks. Beneﬁted from
large-scale training data, recent deep-learning-based derain-
ers [10, 11, 40, 42, 25, 45, 15] achieve further promising
performances. Nonetheless, the single-image rain removal
problem remains open in two ways, as discussed below.

Lack of real training data. As real rain/clean image
pairs are unavailable, existing derainers typically rely on
synthesized datasets to train their models. They usually
start with a clean image and add synthetic rain on it to form
a rain/clean image pair. Although some works have been
done to study the physical characteristics of rain, e.g., rain
direction [40] and rain density [42], their datasets still lack
the ability to model a large range of real world rain streaks.
For example, it is often very difﬁcult to classify the rain
density into one of the three levels (i.e., light, medium and
heavy) as in [42], and any misclassiﬁcation would certainly
affect the deraining performance. To simulate global rain
effects, some methods adopt the nonlinear “screen blend
mode” from Adobe Photoshop, or additionally superimpose
haze on the synthesized rain images. However, these global
settings can only be used in certain types of rain, or the
background may be darkened, with the details lost.

12270

Lack of a real benchmark. Currently, researchers
mainly rely on qualitatively evaluating the deraining per-
formance on real rain images through visual comparisons.
Fan et al. [45] also use an object detection task to help
evaluate the deraining performance. Nevertheless, a high-
quality real deraining benchmark is still much needed for
quantitative evaluation of deraining methods.

In this paper, we address the single-image rain removal
problem in two ways, as summarized in Figure 1. First, we
address the lack of real training/evaluation datasets based
on two observations:
(1) as random rain drops fall in
high velocities, they unlikely cover the same pixel all the
time [13, 44], and (2) the intensity of a pixel covered by
rain ﬂuctuates above the true background radiance across
a sequence of images. These two observations imply that
we can generate one clean image from a sequence of rain
images, where individual pixels of the clean image may
be coming from different images of the sequence. Hence,
we propose a semi-automatic method that incorporates rain
temporal properties as well as human supervision to con-
struct a large-scale real rain dataset. We show that it can sig-
niﬁcantly improve the performance of state-of-the-art de-
rainers on real world rain images.

Second, we observe that real rain streaks can exhibit
highly diverse appearance properties (e.g., rain shape and
direction) within a single image, which challenges existing
derainers as they lack the ability to identify real rain streaks
accurately. To address this limitation, we exploit a spatial
attentive network (SPANet), which ﬁrst leverages horizon-
tal/vertical neighborhood information to model the physical
properties of rain streaks, and then remove them by further
considering the non-local contextual information.
In this
way, the discriminative features for rain streak removal can
be learned in a two-stage local-to-global manner. Exten-
sive evaluations show that the proposed network performs
favorably against the state-of-the-art derainers.

To summarize, this work has the following contributions:

1. We present a semi-automatic method that incorporates
temporal properties of rain streaks and human super-
vision to generate a high quality clean image from a
sequence of real rain images.

2. We construct a large-scale dataset of ∼29.5K high-
resolution rain/clean image pairs, which covers a wide
range of natural rain scenes. We show that it can sig-
niﬁcantly improve the performance of state-of-the-art
derainers on real rain images.

3. We design a novel SPANet to effectively learn discrim-
inative deraining features in a local-to-global attentive
manner. SPANet achieves superior performance over
state-of-the-art derainers.

2. Related works

Single-image rain removal. This problem is extremely

challenging due to the ill-posed deraining formulation as:

B = O − R,

(1)

where O, R and B are the input rain image, the rain streak
image, and the output derained image, respectively.

Kang et al. [20] propose to ﬁrst decompose the rain im-
age into high-/low-frequency layers and remove rain streaks
in the high frequency layer via dictionary learning. Kim et
al. [21] propose to use non-local mean ﬁlters to ﬁlter out
rain streaks. Luo et al. [29] propose a sparse coding based
method to separate rain streaks from the background. Li et
al. [26] propose to use Gaussian mixture models to model
rain streaks and background separately for rain removal.
Chang et al. [5] propose to ﬁrst afﬁne transform the rain
image into a space where rain streaks have vertical appear-
ances and then utilize the low-rank property to remove rain
streaks. Zhu et al. [47] exploit rain streak directions to
ﬁrst determine the rain-dominant regions, which are used
to guide the process of separating rain streaks from back-
ground details based on rain-dominant patch statistics.

In [11, 10], deep learning is applied to single image de-
raining and achieves a signiﬁcant performance boost. They
model rain streaks as “residuals” between the input/output
of the networks in an end-to-end manner. Yang et al. [40]
propose to decompose the rain layer into a series of sub-
layers representing rain streaks of different directions and
shapes, and jointly detect and remove rain streaks using a
recurrent network. In [43], Zhang et al. propose to remove
rain streaks and recover the background via the Conditional
GAN. Recently, Zhang and Patel [42] propose to classify
rain density to guide the rain removal step. Li et al. [25]
propose a recurrent network with a squeeze-and-excitation
block [17] to remove rain streaks in multiple stages. How-
ever, the performances of CNN-based derainers on real rain
images are largely limited by being trained only on syn-
thetic datasets. These derainers also lack the ability to at-
tend to rain spatial distributions. In this paper, we propose
to leverage real training data as well as a spatial attentive
mechanism to address the single image deraining problem.
Multi-image rain removal. Unlike single-image de-
raining, rich temporal information can be derived from a se-
quence of images to provide additional constraints for rain
removal. Pioneering works [12, 13] propose to apply pho-
tometric properties to detect rain streaks and estimate the
corresponding background intensities by averaging the ir-
radiance of temporal or spatial neighboring pixels. Subse-
quently, more intrinsic properties of rain streaks, such as
chromatic property, are explored by [44, 28, 36]. Recent
works [4, 8, 6, 21, 19, 35, 39, 24, 27] focus on removing
the rain streaks from the background with moving objects.

12271

Chen et al. [7] further propose a spatial-temporal content
alignment algorithm to handle fast camera motion and dy-
namic scene contents, and a CNN to reconstruct high fre-
quency background details.

However, these methods cannot be applied for our pur-
pose of generating high-quality rain-free images. This is
because if their assumptions (e.g., low-rank [8, 39, 24]) are
violated, over-/under-deraining can happen to the entire se-
quence and further bury the true background radiance, i.e.,
the clean background pixels may not exist in this sequence.
Hence, in this paper, we propose to use the original se-
quence of rain images to generate a clean image, and rely
on human judgements on the qualities of generated rain-free
images.

Generating the ground truth from real noisy images.
One typical strategy [2, 33] to obtain a noise/noise-free im-
age pair is to photograph the scene with a high ISO value
and a short exposure time for the noise image, and a low
ISO value and a long exposure time for the noise-free im-
age. However, this strategy cannot be used here to capture
rain-free images. As rain drops fall at a high speed, in-
creasing the exposure time will enlarge the rain streaks, not
removing them. Another approach to obtain a ground truth
noise-free image is multi-frame fusion [46, 32, 1], which
performs weighted averaging of a pre-aligned sequence of
images taken from a static scene with a ﬁxed camera set-
ting. However, as rain streaks have brighter appearances
and larger shapes than random noise, this approach is not
able to accurately remove rain from the rain pixels. In con-
trast, we propose to reﬁne the rain pixels based on the ob-
servation that the intensity values of the pixels covered by
rain ﬂuctuate above their true background intensities.

3. Real Rain Image Dataset

Pixels

(a) A rain image sequence

(b) Intensity histogram of a 

pixel cross an image seqeuence

Figure 2. We trace the intensity of one pixel across an image se-
quence in (a). We ask a user to identify if this pixel in each frame
is covered by rain (in red) or not (in blue). The intensity distribu-
tion of this pixel over all frames is show in (b). It shows that the
intensity of the pixel tends to ﬂuctuate in a smaller range if it is
not covered by rain, as compared with that covered by rain.

We ﬁrst conduct an experiment on how to select a suit-
able background value ob from a collection of pixel values
Ol = {o1l, ..., oN l} at spatial position l from a sequence of
N rain images. We capture a video of a rain scene over
a static background, as shown in Figure 2, and then ask

a person to indicate (or predict) when a particular pixel is
covered by rain and when it is not, across the N frames.
We have observed two phenomena. First, rain streaks do
not always cover the same pixel (the temporal property of
video deraining [44]). Second, humans typically predict if
a pixel is covered by rain or not based on the pixel inten-
sity. If the intensity of the pixel is lower at a certain frame
compared with the other frames, humans would predict that
it is not covered by rain. This is because rain streaks tend
to brighten the background. These two observations imply
that, given a sequence of N consecutive rain images, we
can approximate the true background radiance Bl at pixel l
based on these human predicted rain-free pixel values (i.e.,
the blue region of the histogram in Figure 2(b)). If we as-
sume that the ambient light is constant during this time span,
we can then use the value that appears most frequently (i.e.,
mode in statistics) to approximate the background radiance.
Background approximation. Referring to Figure 3,
given a set of pixel values Ol at position l from a sequence
of N rain images, we ﬁrst compute the mode of Ol as:

φl = Φ(Ol),

(2)

where Φ is the mode operation. However, since Eq. 2 does
not consider the neighborhood information when comput-
ing φl, the resulting images tend to be noisy in dense rain
streaks. So, we identify the percentile range (Rmin
, Rmax
)
of the computed φl in Ol based on their intensity values as:

l

l

Rmin

l

=

100%

N

N

!

i=1

{1|oil < φl},

Rmax

l

=

100%

N

N

!

i=1

{1|oil > φl}.

(3)

Figure 3(c) shows an example. Instead of using polyg-
onal lines to connect the mode values φl at all spatial po-
sitions, we can determine a suitable percentile ˆp so that
it crosses the highest number of percentile ranges (the red
dash line in Figure 3(c)). In this way, the estimated back-
ground image is globally smoothed by computing ˆp as:

ˆp = arg max

p

M −1

({

!

l=0

{1|Rmin

l

< p < Rmax

l

}}100

p=0),

(4)

where M is the number of pixels in a frame. Figure 4(e)
shows an example that using the mode leads to noisy result,
while our method in Figure 4(f) produces a cleaner image.
Selection of N for different rain scenes. Recall that
we aim to generate one clean image from a sequence of
N rain images. Our method assumes that for each pixel
of the output clean image, we are able to ﬁnd some in-
put frames where the pixel is not covered by rain. To
satisfy this assumption, we need to adjust N according

12272

 Percentile Prediction

> =50

 Rainy 
Image Sequence

Percentile

Filter

Estimated 
Clean Image 

(a) Pipeline of Background Approximations

20%

50%

9

9

10

10

10

11

12

34

34

157

10

20% (cid:1239) 50% 

A Pixel Sequence

Mode

Percentile Range

(b) Computing Percentile Range for Mode Value

Pixelwise Percentile Range

Pi,j

Pixels

Mode

Percentile 

Range

100

e

l
i
t
n
e
c
r
e
P

80

60
50
40

20

0

Percentile Rank 
Percentile Range

Image Pixel Indexes

(c) Percentile Prediction

Figure 3. Overview of our clean image generation pipeline (a). Given a sequence of rain images, we compute the mode for each pixel based
on its intensity changes over time, and the percentile range of its mode (b). We then consider the global spatial smoothness by ﬁnding a
percentile rank that can cross most of the percentile ranges (c).

(a) Input

(b) Jiang [19]

(c) Wei [39]

(d) Li [24]

(e) Mode ﬁlter

(f) Ours

(g) Ground Truth

Figure 4. A deraining example using a synthetic rain video of 100 frames. We show the best result of each method here. Refer to the
supplementary for more results.

to the amount of rain as follows. First, we empirically
set N to be {20, 100, 200} depending on whether the rain
is {sparse, normal, dense}, respectively, and generate an
output image using our method. Second, we ask users to
evaluate the image as humans are sensitive to rain streaks
as well as other artifacts such as noise. If the image fails
in the user evaluation, we adjust N by adding {10, 20, 50}
frames for {sparse, normal, dense} rain streaks and then
ask the users to evaluate the new output image again. We
ﬁnd that while 20 and 100 frames are usually large enough
to obtain a clean image for sparse and normal rain streaks,
N may go from 200 to 300 frames for dense rain streaks.
We deliberately start with smaller numbers of frames be-
cause we ﬁnd that the more frames that we use, the higher
chance that the video may contain noise, blur and shaking.
Discussion. An intuitive alternative to obtaining a
rain-free image is to use a state-of-the-art video deraining
method to ﬁrst generate a sequence of derained results from
the input rain sequence, and then average them or select the
best result from them to produce a single ﬁnal rain-free im-
age. Unfortunately, there is no guarantee that rain streaks
can be completely removed by the video deraining method,
as shown in Figure 4(b)-(d). On the contrary, we rely on hu-
man judgements to generate high-quality rain-free images.
We show a comparison between our method and three state-
of-the-art video deraining methods [19, 39, 24] in Table 1
on 10 synthesized rain videos (10 black-background rain
videos bought from [31] are imposed on 10 different back-
ground images), which clearly demonstrates the effective-
ness of our method.

Dataset description. We construct a large-scale dataset
using 170 real rain videos, of which 84 scenes are cap-

Methods

Input

Jiang et al. [19] Wei et al. [39]

Li et al. [24]

Ours

27.30 (25.71)

32.79 (29.82)

25.40
0.7228

0.8827 (0.8566)

51.40
PSNR
0.9907
SSIM
Table 1. Comparison with the state-of-the-art video deraining
methods. In each method, we select the frame of highest PSNR
for comparison. The average PSNR/SSIM are in brackets.

0.9458 (0.9387)

0.9043 (0.8911)

32.59 (30.59)

tured by us using an iPhone X or iPhone 6SP and 86 scenes
are collected from StoryBlocks or YouTube. These videos
cover common urban scenes (e.g., buildings, avenues), sub-
urb scenes (e.g., streets, parks), and some outdoor ﬁelds
(e.g., forests). When capturing rain scenes, we also con-
trol the exposure durations as well as the ISO parameter
to cover different lengths of rain streaks and illumination
conditions. Using the aforementioned method, we generate
29, 500 high-quality rain/clean image pairs, which are split
into 28, 500 for training and 1, 000 for testing. Our experi-
ments show that this dataset helps improve the performance
of state-of-the-art derainers.

4. Proposed Model

As real rain streaks may have highly diverse appearances
across the image, we propose the SPANet to detect and re-
move rain streaks in a local-to-global manner, as shown in
Figure 5(a). It is a fully convolutional network that takes
one rain image as input and outputs a derained image.

4.1. Spatial Attentive Block

Review on IRNN architecture. Recurrent neural net-
works with ReLU and identity matrix initialization (IRNN)
for natural language processing [23] have been shown to
be easy to train, good at modeling long-range dependen-

12273

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
y
y
B
B
Z
E
T
l
Y
5
U
f
q
l
y
Y
q
N
p
8
T
9
L
h
/
r
0
=
"
>
A
A
A
C
y
3
i
c
j
V
H
L
T
s
J
A
F
D
3
U
F
+
I
L
d
e
m
m
E
U
x
c
k
Z
a
N
L
o
l
u
3
J
h
g
I
o
8
E
0
b
R
l
g
I
a
+
M
j
M
1
Q
W
T
p
D
7
j
V
/
z
L
+
g
f
6
F
d
8
a
S
q
M
T
o
N
G
3
P
n
H
v
O
n
b
n
3
u
k
n
g
C
2
l
Z
r
z
l
j
Y
X
F
p
e
S
W
/
W
l
h
b
3
9
j
c
K
m
7
v
N
E
W
c
c
o
8
1
v
D
i
I
e
d
t
1
B
A
v
8
i
D
W
k
L
w
P
W
T
j
h
z
Q
j
d
g
L
X
d
0
q
u
K
t
W
8
a
F
H
0
e
X
c
p
y
w
b
u
g
M
I
r
/
v
e
4
4
k
q
l
1
O
r
i
d
y
O
C
3
f
F
E
t
W
x
d
L
L
n
A
d
2
B
k
r
I
V
j
0
u
v
u
A
K
P
c
T
w
k
C
I
E
Q
w
R
J
O
I
A
D
Q
U
8
H
N
i
w
k
x
H
U
x
I
Y
4
T
8
n
W
c
Y
Y
o
C
e
V
N
S
M
V
I
4
x
I
7
o
O
6
B
d
J
2
M
j
2
q
u
c
Q
r
s
9
O
i
W
g
l
5
P
T
x
A
F
5
Y
t
J
x
w
u
o
0
U
8
d
T
n
V
m
x
v
+
W
e
6
J
z
q
b
m
P
6
u
1
m
u
k
F
i
J
I
b
F
/
+
W
b
K
/
/
p
U
L
R
J
9
H
O
s
a
f
K
o
p
0
Y
y
q
z
s
u
y
p
L
o
r
6
u
b
m
l
6
o
k
Z
U
i
I
U
7
h
H
c
U
7
Y
0
8
5
Z
n
0
3
t
E
b
p
2
1
V
t
H
x
9
+
0
U
r
F
q
7
2
X
a
F
O
/
q
l
j
R
g
+
+
c
4
5
0
G
z
W
r
G
t
i
n
1
R
L
d
V
O
s
l
H
n
s
Y
d
9
H
N
I
8
j
1
D
D
G
e
p
o
6
D
k
+
4
g
n
P
x
r
k
h
j
D
v
j
/
l
N
q
5
D
L
P
L
r
4
t
4
+
E
D
T
B
a
S
M
w
=
=
<
/
l
a
t
e
x
i
t
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
t
g
K
W
c
Y
R
X
w
M
t
v
5
r
b
s
W
9
m
F
w
Z
e
0
r
3
4
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
U
R
Q
Z
d
F
N
y
5
c
V
L
A
P
q
K
U
k
0
2
k
d
T
Z
M
w
m
Q
g
l
d
O
c
P
u
N
U
P
E
/
9
A
/
8
I
7
Y
w
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
8
f
B
y
J
R
j
v
N
a
s
O
b
m
F
x
a
X
i
s
u
l
l
d
W
1
9
Y
3
y
5
l
Y
z
i
V
L
J
e
I
N
F
Q
S
T
b
v
p
f
w
Q
I
S
8
o
Y
Q
K
e
D
u
W
3
B
v
5
A
W
/
5
d
2
c
6
3
r
r
n
M
h
F
R
e
K
X
G
M
e
+
O
v
G
E
o
B
o
J
5
i
q
h
W
v
Z
e
J
g
9
t
J
r
1
x
x
q
o
5
Z
9
i
x
w
c
1
B
B
v
u
p
R
+
Q
X
X
6
C
M
C
Q
4
o
R
O
E
I
o
w
g
E
8
J
P
R
0
4
M
J
B
T
F
w
X
G
X
G
S
k
D
B
x
j
g
l
K
5
E
1
J
x
U
n
h
E
X
t
H
3
y
H
t
O
j
k
b
0
l
7
n
T
I
y
b
0
S
k
B
v
Z
K
c
N
v
b
I
E
5
F
O
E
t
a
n
2
S
a
e
m
s
y
a
/
S
1
3
Z
n
L
q
u
4
3
p
7
+
e
5
R
s
Q
q
3
B
D
7
l
2
+
q
/
K
9
P
1
6
I
w
w
I
m
p
Q
V
B
N
s
W
F
0
d
S
z
P
k
p
q
u
6
J
v
b
X
6
p
S
l
C
E
m
T
u
M
+
x
S
V
h
Z
p
z
T
P
t
v
G
k
5
j
a
d
W
8
9
E
3
8
z
S
s
3
q
P
c
u
1
K
d
7
1
L
W
n
A
7
s
9
x
z
o
L
m
Y
d
V
1
q
u
7
l
U
a
V
2
m
o
+
6
i
B
3
s
Y
p
/
m
e
Y
w
a
z
l
F
H
w
1
T
5
i
C
c
8
W
x
e
W
t
M
Z
W
9
i
m
1
C
r
l
n
G
9
+
W
9
f
A
B
g
v
i
R
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
t
g
K
W
c
Y
R
X
w
M
t
v
5
r
b
s
W
9
m
F
w
Z
e
0
r
3
4
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
U
R
Q
Z
d
F
N
y
5
c
V
L
A
P
q
K
U
k
0
2
k
d
T
Z
M
w
m
Q
g
l
d
O
c
P
u
N
U
P
E
/
9
A
/
8
I
7
Y
w
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
8
f
B
y
J
R
j
v
N
a
s
O
b
m
F
x
a
X
i
s
u
l
l
d
W
1
9
Y
3
y
5
l
Y
z
i
V
L
J
e
I
N
F
Q
S
T
b
v
p
f
w
Q
I
S
8
o
Y
Q
K
e
D
u
W
3
B
v
5
A
W
/
5
d
2
c
6
3
r
r
n
M
h
F
R
e
K
X
G
M
e
+
O
v
G
E
o
B
o
J
5
i
q
h
W
v
Z
e
J
g
9
t
J
r
1
x
x
q
o
5
Z
9
i
x
w
c
1
B
B
v
u
p
R
+
Q
X
X
6
C
M
C
Q
4
o
R
O
E
I
o
w
g
E
8
J
P
R
0
4
M
J
B
T
F
w
X
G
X
G
S
k
D
B
x
j
g
l
K
5
E
1
J
x
U
n
h
E
X
t
H
3
y
H
t
O
j
k
b
0
l
7
n
T
I
y
b
0
S
k
B
v
Z
K
c
N
v
b
I
E
5
F
O
E
t
a
n
2
S
a
e
m
s
y
a
/
S
1
3
Z
n
L
q
u
4
3
p
7
+
e
5
R
s
Q
q
3
B
D
7
l
2
+
q
/
K
9
P
1
6
I
w
w
I
m
p
Q
V
B
N
s
W
F
0
d
S
z
P
k
p
q
u
6
J
v
b
X
6
p
S
l
C
E
m
T
u
M
+
x
S
V
h
Z
p
z
T
P
t
v
G
k
5
j
a
d
W
8
9
E
3
8
z
S
s
3
q
P
c
u
1
K
d
7
1
L
W
n
A
7
s
9
x
z
o
L
m
Y
d
V
1
q
u
7
l
U
a
V
2
m
o
+
6
i
B
3
s
Y
p
/
m
e
Y
w
a
z
l
F
H
w
1
T
5
i
C
c
8
W
x
e
W
t
M
Z
W
9
i
m
1
C
r
l
n
G
9
+
W
9
f
A
B
g
v
i
R
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
t
g
K
W
c
Y
R
X
w
M
t
v
5
r
b
s
W
9
m
F
w
Z
e
0
r
3
4
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
U
R
Q
Z
d
F
N
y
5
c
V
L
A
P
q
K
U
k
0
2
k
d
T
Z
M
w
m
Q
g
l
d
O
c
P
u
N
U
P
E
/
9
A
/
8
I
7
Y
w
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
8
f
B
y
J
R
j
v
N
a
s
O
b
m
F
x
a
X
i
s
u
l
l
d
W
1
9
Y
3
y
5
l
Y
z
i
V
L
J
e
I
N
F
Q
S
T
b
v
p
f
w
Q
I
S
8
o
Y
Q
K
e
D
u
W
3
B
v
5
A
W
/
5
d
2
c
6
3
r
r
n
M
h
F
R
e
K
X
G
M
e
+
O
v
G
E
o
B
o
J
5
i
q
h
W
v
Z
e
J
g
9
t
J
r
1
x
x
q
o
5
Z
9
i
x
w
c
1
B
B
v
u
p
R
+
Q
X
X
6
C
M
C
Q
4
o
R
O
E
I
o
w
g
E
8
J
P
R
0
4
M
J
B
T
F
w
X
G
X
G
S
k
D
B
x
j
g
l
K
5
E
1
J
x
U
n
h
E
X
t
H
3
y
H
t
O
j
k
b
0
l
7
n
T
I
y
b
0
S
k
B
v
Z
K
c
N
v
b
I
E
5
F
O
E
t
a
n
2
S
a
e
m
s
y
a
/
S
1
3
Z
n
L
q
u
4
3
p
7
+
e
5
R
s
Q
q
3
B
D
7
l
2
+
q
/
K
9
P
1
6
I
w
w
I
m
p
Q
V
B
N
s
W
F
0
d
S
z
P
k
p
q
u
6
J
v
b
X
6
p
S
l
C
E
m
T
u
M
+
x
S
V
h
Z
p
z
T
P
t
v
G
k
5
j
a
d
W
8
9
E
3
8
z
S
s
3
q
P
c
u
1
K
d
7
1
L
W
n
A
7
s
9
x
z
o
L
m
Y
d
V
1
q
u
7
l
U
a
V
2
m
o
+
6
i
B
3
s
Y
p
/
m
e
Y
w
a
z
l
F
H
w
1
T
5
i
C
c
8
W
x
e
W
t
M
Z
W
9
i
m
1
C
r
l
n
G
9
+
W
9
f
A
B
g
v
i
R
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
G
3
0
n
v
n
J
k
y
K
B
l
K
c
C
y
m
N
n
r
q
T
x
J
g
V
4
=
"
>
A
A
A
C
t
X
i
c
j
V
L
L
S
g
M
x
F
D
0
d
X
7
V
W
r
W
s
3
g
0
V
w
V
T
J
u
d
C
n
o
w
m
U
F
+
4
B
a
Z
C
Z
N
a
+
y
8
T
D
J
C
K
f
6
A
W
z
9
O
/
A
P
9
C
2
/
i
C
G
o
R
z
T
A
z
J
+
f
e
c
5
K
b
m
y
i
P
p
T
a
M
v
V
S
8
p
e
W
V
1
b
X
q
e
m
2
j
X
t
v
c
2
m
7
U
u
z
o
r
F
B
c
d
n
s
W
Z
6
k
e
h
F
r
F
M
R
c
d
I
E
4
t
+
r
k
S
Y
R
L
H
o
R
d
N
T
G
+
/
d
C
6
V
l
l
l
6
a
W
S
6
G
S
T
h
J
5
V
j
y
0
B
D
V
v
m
4
0
W
Y
u
5
4
S
+
C
o
A
R
N
l
C
N
r
P
O
M
K
I
2
T
g
K
J
B
A
I
I
U
h
H
C
O
E
p
m
e
A
A
A
w
5
c
U
P
M
i
V
O
E
p
I
s
L
P
K
B
G
2
o
K
y
B
G
W
E
x
E
7
p
O
6
H
Z
o
G
R
T
m
l
t
P
7
d
S
c
V
o
n
p
V
a
T
0
s
U
+
a
j
P
I
U
Y
b
u
a
7
+
K
F
c
7
b
s
b
9
5
z
5
2
n
3
N
q
N
/
V
H
o
l
x
B
r
c
E
P
u
X
7
j
P
z
v
z
p
b
i
8
E
Y
x
6
4
G
S
T
X
l
j
r
H
V
8
d
K
l
c
K
d
i
d
+
5
/
q
c
q
Q
Q
0
6
c
x
S
O
K
K
8
L
c
K
T
/
P
2
X
c
a
7
W
q
3
Z
x
u
6
+
K
v
L
t
K
y
d
8
z
K
3
w
J
v
d
J
f
U
3
+
N
n
N
R
d
A
9
b
A
W
s
F
V
w
w
V
L
G
L
P
R
x
Q
G
4
9
w
g
n
O
0
0
S
H
L
E
R
7
x
5
J
1
5
t
9
7
d
x
z
3
w
K
u
W
F
2
M
G
3
4
e
l
3
4
Y
W
M
3
A
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
m
M
p
Z
K
G
a
x
l
t
0
S
E
T
t
a
c
x
/
K
2
6
U
X
P
U
=
"
>
A
A
A
C
v
3
i
c
j
V
L
L
S
s
N
A
F
D
2
N
r
1
q
r
V
r
d
u
g
k
V
w
I
S
V
x
Y
5
e
C
G
x
c
u
K
t
g
H
1
F
K
S
6
b
S
O
z
Y
v
J
R
C
i
h
H
+
F
W
P
0
z
8
A
/
0
L
7
4
w
p
q
E
V
0
Q
p
I
z
5
9
5
z
Z
8
7
c
8
Z
N
A
p
M
p
x
X
k
v
W
y
u
r
a
+
k
Z
5
s
7
J
V
3
d
7
Z
r
e
1
V
O
2
m
c
S
c
b
b
L
A
5
i
2
f
O
9
l
A
c
i
4
m
0
l
V
M
B
7
i
e
R
e
6
A
e
8
6
0
8
v
d
L
z
7
w
G
U
q
4
u
h
G
z
R
I
+
C
L
1
J
J
M
a
C
e
Y
q
o
b
m
u
Y
i
5
P
7
+
b
B
W
d
x
q
O
G
f
Y
y
c
A
t
Q
R
z
F
a
c
e
0
F
t
x
g
h
B
k
O
G
E
B
w
R
F
O
E
A
H
l
J
6
+
n
D
h
I
C
F
u
g
J
w
4
S
U
i
Y
O
M
c
c
F
d
J
m
l
M
U
p
w
y
N
2
S
t
8
J
z
f
o
F
G
9
F
c
1
0
y
N
m
t
E
q
A
b
2
S
l
D
a
O
S
B
N
T
n
i
S
s
V
7
N
N
P
D
O
V
N
f
t
b
7
d
z
U
1
H
u
b
0
d
8
v
a
o
X
E
K
t
w
R
+
5
d
u
k
f
l
f
n
f
a
i
M
E
b
T
e
B
D
k
K
T
G
M
d
s
e
K
K
p
k
5
F
b
1
z
+
4
s
r
R
R
U
S
4
j
Q
e
U
V
w
S
Z
k
a
5
O
G
f
b
a
F
L
j
X
Z
+
t
Z
+
J
v
J
l
O
z
e
s
6
K
3
A
z
v
e
p
f
U
Y
P
d
n
O
5
d
B
5
7
T
h
O
g
3
3
2
k
E
Z
B
z
j
E
M
b
X
x
D
O
e
4
R
A
t
t
Y
+
4
R
T
3
i
2
r
i
x
p
z
T
6
v
g
l
U
q
7
s
Q
+
v
g
0
r
/
w
A
x
9
J
C
x
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
j
m
M
p
Z
K
G
a
x
l
t
0
S
E
T
t
a
c
x
/
K
2
6
U
X
P
U
=
"
>
A
A
A
C
v
3
i
c
j
V
L
L
S
s
N
A
F
D
2
N
r
1
q
r
V
r
d
u
g
k
V
w
I
S
V
x
Y
5
e
C
G
x
c
u
K
t
g
H
1
F
K
S
6
b
S
O
z
Y
v
J
R
C
i
h
H
+
F
W
P
0
z
8
A
/
0
L
7
4
w
p
q
E
V
0
Q
p
I
z
5
9
5
z
Z
8
7
c
8
Z
N
A
p
M
p
x
X
k
v
W
y
u
r
a
+
k
Z
5
s
7
J
V
3
d
7
Z
r
e
1
V
O
2
m
c
S
c
b
b
L
A
5
i
2
f
O
9
l
A
c
i
4
m
0
l
V
M
B
7
i
e
R
e
6
A
e
8
6
0
8
v
d
L
z
7
w
G
U
q
4
u
h
G
z
R
I
+
C
L
1
J
J
M
a
C
e
Y
q
o
b
m
u
Y
i
5
P
7
+
b
B
W
d
x
q
O
G
f
Y
y
c
A
t
Q
R
z
F
a
c
e
0
F
t
x
g
h
B
k
O
G
E
B
w
R
F
O
E
A
H
l
J
6
+
n
D
h
I
C
F
u
g
J
w
4
S
U
i
Y
O
M
c
c
F
d
J
m
l
M
U
p
w
y
N
2
S
t
8
J
z
f
o
F
G
9
F
c
1
0
y
N
m
t
E
q
A
b
2
S
l
D
a
O
S
B
N
T
n
i
S
s
V
7
N
N
P
D
O
V
N
f
t
b
7
d
z
U
1
H
u
b
0
d
8
v
a
o
X
E
K
t
w
R
+
5
d
u
k
f
l
f
n
f
a
i
M
E
b
T
e
B
D
k
K
T
G
M
d
s
e
K
K
p
k
5
F
b
1
z
+
4
s
r
R
R
U
S
4
j
Q
e
U
V
w
S
Z
k
a
5
O
G
f
b
a
F
L
j
X
Z
+
t
Z
+
J
v
J
l
O
z
e
s
6
K
3
A
z
v
e
p
f
U
Y
P
d
n
O
5
d
B
5
7
T
h
O
g
3
3
2
k
E
Z
B
z
j
E
M
b
X
x
D
O
e
4
R
A
t
t
Y
+
4
R
T
3
i
2
r
i
x
p
z
T
6
v
g
l
U
q
7
s
Q
+
v
g
0
r
/
w
A
x
9
J
C
x
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
u
S
Q
U
9
8
0
o
d
K
y
K
s
Q
d
7
s
R
Q
q
U
r
c
T
m
N
I
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
V
x
o
8
u
i
G
x
c
u
K
t
g
H
1
F
K
S
d
F
r
H
p
k
m
Y
T
I
Q
S
u
v
M
H
3
O
q
H
i
X
+
g
f
+
G
d
c
Q
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
+
f
h
D
y
V
j
v
N
a
s
B
Y
W
l
5
Z
X
i
q
u
l
t
f
W
N
z
a
3
y
9
k
4
z
j
T
M
R
s
E
Y
Q
h
7
F
o
+
1
7
K
Q
h
6
x
h
u
Q
y
Z
O
1
E
M
G
/
s
h
6
z
l
j
8
5
V
v
H
X
P
R
M
r
j
6
F
p
O
E
t
Y
d
e
8
O
I
D
3
j
g
S
a
J
a
9
V
7
O
j
+
6
m
v
X
L
F
q
T
p
6
2
f
P
A
N
a
A
C
s
+
p
x
+
Q
U
3
6
C
N
G
g
A
x
j
M
E
S
Q
h
E
N
4
S
O
n
p
w
I
W
D
h
L
g
u
c
u
I
E
I
a
7
j
D
F
O
U
y
J
u
R
i
p
H
C
I
3
Z
E
3
y
H
t
O
o
a
N
a
K
9
y
p
t
o
d
0
C
k
h
v
Y
K
c
N
g
7
I
E
5
N
O
E
F
a
n
2
T
q
e
6
c
y
K
/
S
1
3
r
n
O
q
u
0
3
o
7
5
t
c
Y
2
I
l
b
o
n
9
y
z
d
T
/
t
e
n
a
p
E
Y
4
F
T
X
w
K
m
m
R
D
O
q
u
s
B
k
y
X
R
X
1
M
3
t
L
1
V
J
y
p
A
Q
p
3
C
f
4
o
J
w
o
J
2
z
P
t
v
a
k
+
r
a
V
W
8
9
H
X
/
T
S
s
W
q
f
W
C
0
G
d
7
V
L
W
n
A
7
s
9
x
z
o
P
m
c
d
V
1
q
u
6
V
U
6
m
d
m
V
E
X
s
Y
d
9
H
N
I
8
T
1
D
D
B
e
p
o
6
C
o
f
8
Y
R
n
6
9
I
S
1
s
T
K
P
6
V
W
w
X
h
2
8
W
1
Z
D
x
+
B
u
J
H
j
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
t
g
K
W
c
Y
R
X
w
M
t
v
5
r
b
s
W
9
m
F
w
Z
e
0
r
3
4
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
U
R
Q
Z
d
F
N
y
5
c
V
L
A
P
q
K
U
k
0
2
k
d
T
Z
M
w
m
Q
g
l
d
O
c
P
u
N
U
P
E
/
9
A
/
8
I
7
Y
w
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
8
f
B
y
J
R
j
v
N
a
s
O
b
m
F
x
a
X
i
s
u
l
l
d
W
1
9
Y
3
y
5
l
Y
z
i
V
L
J
e
I
N
F
Q
S
T
b
v
p
f
w
Q
I
S
8
o
Y
Q
K
e
D
u
W
3
B
v
5
A
W
/
5
d
2
c
6
3
r
r
n
M
h
F
R
e
K
X
G
M
e
+
O
v
G
E
o
B
o
J
5
i
q
h
W
v
Z
e
J
g
9
t
J
r
1
x
x
q
o
5
Z
9
i
x
w
c
1
B
B
v
u
p
R
+
Q
X
X
6
C
M
C
Q
4
o
R
O
E
I
o
w
g
E
8
J
P
R
0
4
M
J
B
T
F
w
X
G
X
G
S
k
D
B
x
j
g
l
K
5
E
1
J
x
U
n
h
E
X
t
H
3
y
H
t
O
j
k
b
0
l
7
n
T
I
y
b
0
S
k
B
v
Z
K
c
N
v
b
I
E
5
F
O
E
t
a
n
2
S
a
e
m
s
y
a
/
S
1
3
Z
n
L
q
u
4
3
p
7
+
e
5
R
s
Q
q
3
B
D
7
l
2
+
q
/
K
9
P
1
6
I
w
w
I
m
p
Q
V
B
N
s
W
F
0
d
S
z
P
k
p
q
u
6
J
v
b
X
6
p
S
l
C
E
m
T
u
M
+
x
S
V
h
Z
p
z
T
P
t
v
G
k
5
j
a
d
W
8
9
E
3
8
z
S
s
3
q
P
c
u
1
K
d
7
1
L
W
n
A
7
s
9
x
z
o
L
m
Y
d
V
1
q
u
7
l
U
a
V
2
m
o
+
6
i
B
3
s
Y
p
/
m
e
Y
w
a
z
l
F
H
w
1
T
5
i
C
c
8
W
x
e
W
t
M
Z
W
9
i
m
1
C
r
l
n
G
9
+
W
9
f
A
B
g
v
i
R
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
t
g
K
W
c
Y
R
X
w
M
t
v
5
r
b
s
W
9
m
F
w
Z
e
0
r
3
4
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
U
R
Q
Z
d
F
N
y
5
c
V
L
A
P
q
K
U
k
0
2
k
d
T
Z
M
w
m
Q
g
l
d
O
c
P
u
N
U
P
E
/
9
A
/
8
I
7
Y
w
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
8
f
B
y
J
R
j
v
N
a
s
O
b
m
F
x
a
X
i
s
u
l
l
d
W
1
9
Y
3
y
5
l
Y
z
i
V
L
J
e
I
N
F
Q
S
T
b
v
p
f
w
Q
I
S
8
o
Y
Q
K
e
D
u
W
3
B
v
5
A
W
/
5
d
2
c
6
3
r
r
n
M
h
F
R
e
K
X
G
M
e
+
O
v
G
E
o
B
o
J
5
i
q
h
W
v
Z
e
J
g
9
t
J
r
1
x
x
q
o
5
Z
9
i
x
w
c
1
B
B
v
u
p
R
+
Q
X
X
6
C
M
C
Q
4
o
R
O
E
I
o
w
g
E
8
J
P
R
0
4
M
J
B
T
F
w
X
G
X
G
S
k
D
B
x
j
g
l
K
5
E
1
J
x
U
n
h
E
X
t
H
3
y
H
t
O
j
k
b
0
l
7
n
T
I
y
b
0
S
k
B
v
Z
K
c
N
v
b
I
E
5
F
O
E
t
a
n
2
S
a
e
m
s
y
a
/
S
1
3
Z
n
L
q
u
4
3
p
7
+
e
5
R
s
Q
q
3
B
D
7
l
2
+
q
/
K
9
P
1
6
I
w
w
I
m
p
Q
V
B
N
s
W
F
0
d
S
z
P
k
p
q
u
6
J
v
b
X
6
p
S
l
C
E
m
T
u
M
+
x
S
V
h
Z
p
z
T
P
t
v
G
k
5
j
a
d
W
8
9
E
3
8
z
S
s
3
q
P
c
u
1
K
d
7
1
L
W
n
A
7
s
9
x
z
o
L
m
Y
d
V
1
q
u
7
l
U
a
V
2
m
o
+
6
i
B
3
s
Y
p
/
m
e
Y
w
a
z
l
F
H
w
1
T
5
i
C
c
8
W
x
e
W
t
M
Z
W
9
i
m
1
C
r
l
n
G
9
+
W
9
f
A
B
g
v
i
R
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
t
g
K
W
c
Y
R
X
w
M
t
v
5
r
b
s
W
9
m
F
w
Z
e
0
r
3
4
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
U
R
Q
Z
d
F
N
y
5
c
V
L
A
P
q
K
U
k
0
2
k
d
T
Z
M
w
m
Q
g
l
d
O
c
P
u
N
U
P
E
/
9
A
/
8
I
7
Y
w
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
8
f
B
y
J
R
j
v
N
a
s
O
b
m
F
x
a
X
i
s
u
l
l
d
W
1
9
Y
3
y
5
l
Y
z
i
V
L
J
e
I
N
F
Q
S
T
b
v
p
f
w
Q
I
S
8
o
Y
Q
K
e
D
u
W
3
B
v
5
A
W
/
5
d
2
c
6
3
r
r
n
M
h
F
R
e
K
X
G
M
e
+
O
v
G
E
o
B
o
J
5
i
q
h
W
v
Z
e
J
g
9
t
J
r
1
x
x
q
o
5
Z
9
i
x
w
c
1
B
B
v
u
p
R
+
Q
X
X
6
C
M
C
Q
4
o
R
O
E
I
o
w
g
E
8
J
P
R
0
4
M
J
B
T
F
w
X
G
X
G
S
k
D
B
x
j
g
l
K
5
E
1
J
x
U
n
h
E
X
t
H
3
y
H
t
O
j
k
b
0
l
7
n
T
I
y
b
0
S
k
B
v
Z
K
c
N
v
b
I
E
5
F
O
E
t
a
n
2
S
a
e
m
s
y
a
/
S
1
3
Z
n
L
q
u
4
3
p
7
+
e
5
R
s
Q
q
3
B
D
7
l
2
+
q
/
K
9
P
1
6
I
w
w
I
m
p
Q
V
B
N
s
W
F
0
d
S
z
P
k
p
q
u
6
J
v
b
X
6
p
S
l
C
E
m
T
u
M
+
x
S
V
h
Z
p
z
T
P
t
v
G
k
5
j
a
d
W
8
9
E
3
8
z
S
s
3
q
P
c
u
1
K
d
7
1
L
W
n
A
7
s
9
x
z
o
L
m
Y
d
V
1
q
u
7
l
U
a
V
2
m
o
+
6
i
B
3
s
Y
p
/
m
e
Y
w
a
z
l
F
H
w
1
T
5
i
C
c
8
W
x
e
W
t
M
Z
W
9
i
m
1
C
r
l
n
G
9
+
W
9
f
A
B
g
v
i
R
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
t
g
K
W
c
Y
R
X
w
M
t
v
5
r
b
s
W
9
m
F
w
Z
e
0
r
3
4
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
U
R
Q
Z
d
F
N
y
5
c
V
L
A
P
q
K
U
k
0
2
k
d
T
Z
M
w
m
Q
g
l
d
O
c
P
u
N
U
P
E
/
9
A
/
8
I
7
Y
w
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
8
f
B
y
J
R
j
v
N
a
s
O
b
m
F
x
a
X
i
s
u
l
l
d
W
1
9
Y
3
y
5
l
Y
z
i
V
L
J
e
I
N
F
Q
S
T
b
v
p
f
w
Q
I
S
8
o
Y
Q
K
e
D
u
W
3
B
v
5
A
W
/
5
d
2
c
6
3
r
r
n
M
h
F
R
e
K
X
G
M
e
+
O
v
G
E
o
B
o
J
5
i
q
h
W
v
Z
e
J
g
9
t
J
r
1
x
x
q
o
5
Z
9
i
x
w
c
1
B
B
v
u
p
R
+
Q
X
X
6
C
M
C
Q
4
o
R
O
E
I
o
w
g
E
8
J
P
R
0
4
M
J
B
T
F
w
X
G
X
G
S
k
D
B
x
j
g
l
K
5
E
1
J
x
U
n
h
E
X
t
H
3
y
H
t
O
j
k
b
0
l
7
n
T
I
y
b
0
S
k
B
v
Z
K
c
N
v
b
I
E
5
F
O
E
t
a
n
2
S
a
e
m
s
y
a
/
S
1
3
Z
n
L
q
u
4
3
p
7
+
e
5
R
s
Q
q
3
B
D
7
l
2
+
q
/
K
9
P
1
6
I
w
w
I
m
p
Q
V
B
N
s
W
F
0
d
S
z
P
k
p
q
u
6
J
v
b
X
6
p
S
l
C
E
m
T
u
M
+
x
S
V
h
Z
p
z
T
P
t
v
G
k
5
j
a
d
W
8
9
E
3
8
z
S
s
3
q
P
c
u
1
K
d
7
1
L
W
n
A
7
s
9
x
z
o
L
m
Y
d
V
1
q
u
7
l
U
a
V
2
m
o
+
6
i
B
3
s
Y
p
/
m
e
Y
w
a
z
l
F
H
w
1
T
5
i
C
c
8
W
x
e
W
t
M
Z
W
9
i
m
1
C
r
l
n
G
9
+
W
9
f
A
B
g
v
i
R
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
t
g
K
W
c
Y
R
X
w
M
t
v
5
r
b
s
W
9
m
F
w
Z
e
0
r
3
4
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
U
R
Q
Z
d
F
N
y
5
c
V
L
A
P
q
K
U
k
0
2
k
d
T
Z
M
w
m
Q
g
l
d
O
c
P
u
N
U
P
E
/
9
A
/
8
I
7
Y
w
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
8
f
B
y
J
R
j
v
N
a
s
O
b
m
F
x
a
X
i
s
u
l
l
d
W
1
9
Y
3
y
5
l
Y
z
i
V
L
J
e
I
N
F
Q
S
T
b
v
p
f
w
Q
I
S
8
o
Y
Q
K
e
D
u
W
3
B
v
5
A
W
/
5
d
2
c
6
3
r
r
n
M
h
F
R
e
K
X
G
M
e
+
O
v
G
E
o
B
o
J
5
i
q
h
W
v
Z
e
J
g
9
t
J
r
1
x
x
q
o
5
Z
9
i
x
w
c
1
B
B
v
u
p
R
+
Q
X
X
6
C
M
C
Q
4
o
R
O
E
I
o
w
g
E
8
J
P
R
0
4
M
J
B
T
F
w
X
G
X
G
S
k
D
B
x
j
g
l
K
5
E
1
J
x
U
n
h
E
X
t
H
3
y
H
t
O
j
k
b
0
l
7
n
T
I
y
b
0
S
k
B
v
Z
K
c
N
v
b
I
E
5
F
O
E
t
a
n
2
S
a
e
m
s
y
a
/
S
1
3
Z
n
L
q
u
4
3
p
7
+
e
5
R
s
Q
q
3
B
D
7
l
2
+
q
/
K
9
P
1
6
I
w
w
I
m
p
Q
V
B
N
s
W
F
0
d
S
z
P
k
p
q
u
6
J
v
b
X
6
p
S
l
C
E
m
T
u
M
+
x
S
V
h
Z
p
z
T
P
t
v
G
k
5
j
a
d
W
8
9
E
3
8
z
S
s
3
q
P
c
u
1
K
d
7
1
L
W
n
A
7
s
9
x
z
o
L
m
Y
d
V
1
q
u
7
l
U
a
V
2
m
o
+
6
i
B
3
s
Y
p
/
m
e
Y
w
a
z
l
F
H
w
1
T
5
i
C
c
8
W
x
e
W
t
M
Z
W
9
i
m
1
C
r
l
n
G
9
+
W
9
f
A
B
g
v
i
R
5
w
=
=
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
t
g
K
W
c
Y
R
X
w
M
t
v
5
r
b
s
W
9
m
F
w
Z
e
0
r
3
4
=
"
>
A
A
A
C
y
n
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
k
V
w
I
S
U
R
Q
Z
d
F
N
y
5
c
V
L
A
P
q
K
U
k
0
2
k
d
T
Z
M
w
m
Q
g
l
d
O
c
P
u
N
U
P
E
/
9
A
/
8
I
7
Y
w
p
q
E
Z
2
Q
5
M
y
5
5
9
y
Z
e
6
8
f
B
y
J
R
j
v
N
a
s
O
b
m
F
x
a
X
i
s
u
l
l
d
W
1
9
Y
3
y
5
l
Y
z
i
V
L
J
e
I
N
F
Q
S
T
b
v
p
f
w
Q
I
S
8
o
Y
Q
K
e
D
u
W
3
B
v
5
A
W
/
5
d
2
c
6
3
r
r
n
M
h
F
R
e
K
X
G
M
e
+
O
v
G
E
o
B
o
J
5
i
q
h
W
v
Z
e
J
g
9
t
J
r
1
x
x
q
o
5
Z
9
i
x
w
c
1
B
B
v
u
p
R
+
Q
X
X
6
C
M
C
Q
4
o
R
O
E
I
o
w
g
E
8
J
P
R
0
4
M
J
B
T
F
w
X
G
X
G
S
k
D
B
x
j
g
l
K
5
E
1
J
x
U
n
h
E
X
t
H
3
y
H
t
O
j
k
b
0
l
7
n
T
I
y
b
0
S
k
B
v
Z
K
c
N
v
b
I
E
5
F
O
E
t
a
n
2
S
a
e
m
s
y
a
/
S
1
3
Z
n
L
q
u
4
3
p
7
+
e
5
R
s
Q
q
3
B
D
7
l
2
+
q
/
K
9
P
1
6
I
w
w
I
m
p
Q
V
B
N
s
W
F
0
d
S
z
P
k
p
q
u
6
J
v
b
X
6
p
S
l
C
E
m
T
u
M
+
x
S
V
h
Z
p
z
T
P
t
v
G
k
5
j
a
d
W
8
9
E
3
8
z
S
s
3
q
P
c
u
1
K
d
7
1
L
W
n
A
7
s
9
x
z
o
L
m
Y
d
V
1
q
u
7
l
U
a
V
2
m
o
+
6
i
B
3
s
Y
p
/
m
e
Y
w
a
z
l
F
H
w
1
T
5
i
C
c
8
W
x
e
W
t
M
Z
W
9
i
m
1
C
r
l
n
G
9
+
W
9
f
A
B
g
v
i
R
5
w
=
=
<
/
l
a
t
e
x
i
t
>
V
N
O
C

U
L
e
R

Input

B
R

B
R

B
R

B
A
S

B
A
S

B
A
S

B
A
S

B
R

B
R

V
N
O
C

Output

Attention Map

M
A
S

B
R
A
S

B
R
A
S

B
R
A
S

(b) Spatial Attentive Block (SAB)

v
n
o
c

U
L
e
R

v
n
o
c

U
L
e
R

v
n
o
c

U
L
e
R

Attention Map

(a) Spatial Attentive Network (SPANet)

v
n
o
c

U
L
e
R

v
n
o
c

U
L
e
R

v
n
o
c

d
o
m
g
S

i

(share)

>(share)

v
n
o
c

Input

feature map

t
a
c
n
o
c

v
n
o
c

t
a
c
n
o
c

v
n
o
c

U
L
e
R

v
n
o
c

d
o
m
g
S

i

Output

Attention Map

(c) Spatial Attentive Residual Block (SARB)

(d) Spatial Attentive Module (SAM)

Figure 5. The architecture of the proposed SPANet (a). It adopts three standard residual blocks (RBs) [16] to extract features, four spatial
attentive blocks (SABs) to identify rain streaks progressively in four stages, and two residual blocks to reconstruct a clean background. A
SAB (b) contains three spatial attentive residual blocks (SARBs) (c) and one spatial attentive module (SAM) (d). Dilation convolutions [41]
are used in RBs and SARBs.

cies as well as efﬁcient. When applied to computer vision
problems, their key advantage is that information can be ef-
ﬁciently propagated across the entire image to accumulate
long range varying contextual information, by stacking at
least two RNN layers. In [3], a two-round four-directional
IRNN architecture is used to exploit contextual informa-
tion to improve small object detection. While the ﬁrst
round IRNN aims to produce the feature maps that summa-
rize the neighboring contexts for each position of the input
image, the second round IRNN further gathers non-local
contextual information for producing global aware feature
maps. Recently, Hu et al. [18] also exploit this two-round
four-directional IRNN architecture to detect shadow regions
based on the observation that directions play an important
role in ﬁnding strong cues between shadow/non-shadow re-
gions. They design a direction-aware attention mechanism
to generate more discriminative contextual features.

We summarize the four-directional IRNN operation for

computing feature hi,j at location (i, j) as:

hi,j ← max (αdir hi,j−1 + hi,j, 0) ,

(5)

where αdir denotes the weight parameter in the recurrent
convolution layer for each direction. Figure 6 illustrates
how a two-round four-directional IRNN architecture accu-
mulates global contextual information. Here, we extend the
two-round four-directional IRNN model to the single-image
rain removal problem, for the purpose of handling the sig-
niﬁcant appearance variations of real rain streaks.

Spatial attentive module (SAM). We build SAM based
on the aforementioned two-round four-directional IRNN ar-
chitecture. We use the IRNN model to project the rain
streaks to the four main directions. Another branch is
added to capture the spatial contextual information in or-
der to selectively highlight the projected rain features, as
shown in Figure 5(d). Unlike [18] that implicitly learns

Input Feature Map

st
1-stage Feature Map

Output Feature Map

Figure 6. Illustration of how the two-round four-directional IRNN
architecture accumulates global contextual information in two
stages. In the ﬁrst stage, for each position at the input feature map,
four-directional (up, left, down, right) recurrent convolutional op-
erations are performed to collect horizontal and vertical neighbor-
hood information. In the second stage, by repeating the previous
operations, the contextual information from the entire input feature
map are obtained.

direction-aware features in the embedding space, we fur-
ther use additional convolutions and sigmoid activations to
explicitly generate the attention map through explicit super-
vision. The attention map indicates rain spatial distributions
and is used to guide the following deraining process. Fig-
ure 7 shows the input rain images in (a) and our SPANet
derained results in (c). We also visualize the attention maps
produced by SAM in (b). We can see that SAM can ef-
fectively identify the regions affected by rain streaks, even
though the rain streaks exhibit signiﬁcant appearance varia-
tions (i.e., smooth and blurry in the ﬁrst scene and sharp in
the second scene).

Removal-via-detection. As shown in Figure 5(a),
given an input rain image, three standard residual blocks
(RBs) [16] are ﬁrst used to extract features. We feed these
features into a spatial attentive block (SAB) (Figure 5(b)),
which uses a SAM to generate an attention map to guide
three subsequent spatial attentive residual blocks (SARBs)
(Figure 5(c)) to remove rain streaks via the learned negative
residuals. The SAB is repeated four times. (Note that the

12274

<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
z
D
a
X
i
M
d
0
5
h
V
S
7
k
i
k
C
B
S
l
/
U
e
r
L
t
w
=
"
>
A
A
A
C
0
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
q
3
g
o
p
S
k
C
L
o
s
u
H
F
Z
x
T
6
g
l
p
K
k
0
x
q
c
J
n
E
y
E
U
s
p
4
t
Y
f
c
K
t
f
J
f
6
B
/
o
V
3
x
h
T
U
I
j
o
h
y
Z
l
z
7
z
k
z
9
1
4
3
4
n
4
s
L
e
s
1
Y
8
z
N
L
y
w
u
Z
Z
d
z
K
6
t
r
6
x
v
5
z
a
1
G
H
C
b
C
Y
3
U
v
5
K
F
o
u
U
7
M
u
B
+
w
u
v
Q
l
Z
6
1
I
M
G
f
o
c
t
Z
0
r
4
5
V
v
H
n
D
R
O
y
H
w
b
k
c
R
a
w
z
d
A
a
B
3
/
c
9
R
x
L
V
K
T
a
7
4
6
T
E
S
7
2
S
m
B
S
7
+
Y
J
V
t
v
Q
y
Z
4
G
d
g
g
L
S
V
Q
v
z
L
7
h
A
D
y
E
8
J
B
i
C
I
Y
A
k
z
O
E
g
p
q
c
N
G
x
Y
i
4
j
o
Y
E
y
c
I
+
T
r
O
M
E
G
O
t
A
l
l
M
c
p
w
i
L
2
i
7
4
B
2
7
Z
Q
N
a
K
8
8
Y
6
3
2
6
B
R
O
r
y
C
l
i
T
3
S
h
J
Q
n
C
K
v
T
T
B
1
P
t
L
N
i
f
/
M
e
a
0
9
1
t
x
H
9
3
d
R
r
S
K
z
E
J
b
F
/
6
a
a
Z
/
9
W
p
W
i
T
6
O
N
I
1
+
F
R
T
p
B
l
V
n
Z
e
6
J
L
o
r
6
u
b
m
l
6
o
k
O
U
T
E
K
d
y
j
u
C
D
s
a
e
W
0
z
6
b
W
x
L
p
2
1
V
t
H
x
9
9
0
p
m
L
V
3
k
t
z
E
7
y
r
W
9
K
A
7
Z
/
j
n
A
W
N
S
t
m
2
y
v
Z
p
p
V
A
9
S
E
e
d
x
Q
5
2
s
U
/
z
P
E
Q
V
J
6
i
h
T
t
7
X
e
M
Q
T
n
o
0
z
4
9
a
4
M
+
4
/
U
4
1
M
q
t
n
G
t
2
U
8
f
A
A
c
+
p
O
e
<
/
l
a
t
e
x
i
t
>
<
l
a
t
e
x
i
t
 
s
h
a
1
_
b
a
s
e
6
4
=
"
z
D
a
X
i
M
d
0
5
h
V
S
7
k
i
k
C
B
S
l
/
U
e
r
L
t
w
=
"
>
A
A
A
C
0
H
i
c
j
V
H
L
S
s
N
A
F
D
2
N
r
1
p
f
V
Z
d
u
g
q
3
g
o
p
S
k
C
L
o
s
u
H
F
Z
x
T
6
g
l
p
K
k
0
x
q
c
J
n
E
y
E
U
s
p
4
t
Y
f
c
K
t
f
J
f
6
B
/
o
V
3
x
h
T
U
I
j
o
h
y
Z
l
z
7
z
k
z
9
1
4
3
4
n
4
s
L
e
s
1
Y
8
z
N
L
y
w
u
Z
Z
d
z
K
6
t
r
6
x
v
5
z
a
1
G
H
C
b
C
Y
3
U
v
5
K
F
o
u
U
7
M
u
B
+
w
u
v
Q
l
Z
6
1
I
M
G
f
o
c
t
Z
0
r
4
5
V
v
H
n
D
R
O
y
H
w
b
k
c
R
a
w
z
d
A
a
B
3
/
c
9
R
x
L
V
K
T
a
7
4
6
T
E
S
7
2
S
m
B
S
7
+
Y
J
V
t
v
Q
y
Z
4
G
d
g
g
L
S
V
Q
v
z
L
7
h
A
D
y
E
8
J
B
i
C
I
Y
A
k
z
O
E
g
p
q
c
N
G
x
Y
i
4
j
o
Y
E
y
c
I
+
T
r
O
M
E
G
O
t
A
l
l
M
c
p
w
i
L
2
i
7
4
B
2
7
Z
Q
N
a
K
8
8
Y
6
3
2
6
B
R
O
r
y
C
l
i
T
3
S
h
J
Q
n
C
K
v
T
T
B
1
P
t
L
N
i
f
/
M
e
a
0
9
1
t
x
H
9
3
d
R
r
S
K
z
E
J
b
F
/
6
a
a
Z
/
9
W
p
W
i
T
6
O
N
I
1
+
F
R
T
p
B
l
V
n
Z
e
6
J
L
o
r
6
u
b
m
l
6
o
k
O
U
T
E
K
d
y
j
u
C
D
s
a
e
W
0
z
6
b
W
x
L
p
2
1
V
t
H
x
9
9
0
p
m
L
V
3
k
t
z
E
7
y
r
W
9
K
A
7
Z
/
j
n
A
W
N
S
t
m
2
y
v
Z
p
p
V
A
9
S
E
e
d
x
Q
5
2
s
U
/
z
P
E
Q
V
J
6
i
h
T
t
7
X
e
M
Q
T
n
o
0
z
4
9
a
4
M
+
4
/
U
4
1
M
q
t
n
G
t
2
U
8
f
A
A
c
+
p
O
e
<
/
l
a
t
e
x
i
t
Low Probability

High Probability

5. Experiments

(a) Rain Image

(b) Attention Map

(c) SPANet Result

Figure 7. Visualization of the attention map. (a) shows one real
rain image. (b) shows the corresponding attention map produced
by SAM. Red color indicates pixels that are highly likely covered
by rain. (c) shows the corresponding derained result by the pro-
posed SPANet. This demonstrates the effectiveness of SAM in
handling signiﬁcant appearance variations of rain streaks. Refer to
the supplementary for more results.

weights of the SAM in the four SABs are shared.) Finally,
the resulting feature maps are fed to two standard residual
blocks to reconstruct the ﬁnal clean background image.

4.2. Training Details

Loss function. We adopt the following loss function to

train SPANet:

Ltotal = L1 + LSSIM + LAtt.

(6)

We use the standard L1 loss to measure the per-pixel re-
construction accuracy. Lssim [38] is used to constrain the
structural similarities, and is deﬁned as: 1 − SSIM (P, C),
where P is the predicted result and C is the clean image. We
further apply the attention loss Latt as:

Latt = $A − M$2
2 ,

(7)

where A is the attention map from the ﬁrst SAM in the net-
work and M is the binary map of the rain streaks, which is
computed by thresholding the difference between the rain
image and clean image. In this binary map, a 1 indicates
that the pixel is covered by rain and 0 otherwise.

Implementation details. SPANet is implemented using
the PyTorch [34] framework on a PC with a E5-2640 v4
2.4GHz CPU and 8 NVIDIA Titan V GPUs. For loss op-
timization, we adopt the Adam optimizer [22] with a batch
size of 32. We adopt scaling and cropping to augment the
diversity of rain streaks. The learning rate is initialized at
0.005 and divided by 10 after 5K, 15K, 30K and 50K itera-
tions. We train the network for 60K iterations.

In this section, We ﬁrst evaluate the effectiveness of the
proposed dataset on existing CNN-based single-image de-
rainers, and then compare the proposed SPANet to the state-
of-the-art single-image deraining methods. Finally, we pro-
vide internal analysis to study the contributions of individ-
ual components of SPANet. Refer to the supplementary for
more results.

Evaluation on the proposed dataset. The perfor-
mances of existing CNN-based derainers [11, 40, 42, 25]
trained on our dataset are shown in Table 2.
It demon-
strates that our real dataset can signiﬁcantly improve the
performance of CNN-based methods on real images. This
is mainly due to the fact that existing synthesized datasets
lack the ability to represent highly varying rain streaks. One
visual example is given in Figure 9, from which we can
see that the retrained derainers can produce cleaner images
with more details compared to those trained on synthetic
datasets. Note that we use their original codes for evalua-
tion and retraining.

We also show the performance of non-CNN-based state-
of-the-art methods in Table 2. We have an interesting ob-
servation here that the input rain images have similar or
even higher average PSNR and SSIM scores compared with
those of the derained results by the state-of-the-art derain-
ers. As demonstrated in Figure 8, it is mainly caused by
over deraining. Even though [29] is less dependent on train-
ing data (but still depends on a learned dictionary) as the
deep learning methods ([40, 42, 25]), it fails when the rain
exhibits unseen appearances and mistakenly removes the
structures that are similar to rain streaks.

Rain
34.2

DSC [29]

JORDER [40]

DID-MDN [42]

RESCAN [25]

30.9

27.2

23.0

31.1

Clean
PSNR

Figure 8. The difference maps (red boxes shown at the top-right)
between the input rain image and results by deraining methods that
suffer a PSNR drop. (Brighter indicates a higher difference.) We
can see that [29, 40, 42, 25] tend to over-derain the image.

Evaluation on the proposed SPANet. Table 2 reports
the performance of our SPANet, trained on the proposed
dataset. It achieves a superior deraining performance com-

Methods

Rain

Images

DSC [29]
(ICCV’15)

LP [26]

(CVPR’16)

SILS [14]
(ICCV’17)

Clear [10]
(TIP’17)

DDN [11]
(CVPR’17)

JORDER [40]

DID-MDN [42]

RESCAN [25]

Our

(CVPR’17)

(CVPR’18)

(ECCV18)

SPANet

PSNR

32.64

32.33

32.99

33.40

31.31

33.28 (34.88)

32.16 (35.72)

24.91 (28.96)

30.36 (35.19)

38.06

SSIM

0.9475

0.9335

0.9315

0.9867
Table 2. Quantitative results for benchmarking the proposed SPANet and the state-of-the-art derainers on the proposed test set. The original
codes of all these derainers are used for evaluation. We have also trained CNN-based state-of-the-art methods [11, 40, 42, 25] on our dataset,
and results are marked in red. The best performance is marked in bold. Note that due to the lack of density labels for the rain images in
our dataset, we only ﬁne-tune the pre-trained model of DID-MDN [42] without the re-training label classiﬁcation network.

0.9414 (0.9727)

0.9327 (0.9776)

0.9553 (0.9784)

0.8895(0.9457)

0.9304

0.9528

12275

(a) Rain / Clean Image

(b) DDN [11]

(c) JORDER [40]

(d) DID-MDN [42]

(e) RESCAN [25]

33.53 / 0.9372

37.27 / 0.9631

36.67 / 0.9657

22.86 / 0.8721

35.80 / 0.9538

(f) Our SPANet

43.49 / 0.9938

(g) DDN [11]

38.36 / 0.9668

(h) JORDER [40]

(i) DID-MDN [42]

(j) RESCAN [25]

40.49 / 0.9834

26.54 / 0.9625

39.29 / 0.9771

Figure 9. Visual comparison of the state-of-the-art CNN-based derainers trained on the original/proposed datasets. Methods in red mean
that they are retrained on the proposed dataset. PSNR/SSIM results are included for reference.

(a) Rain / Clean Image

(b) DSC [29]

(c) LP [26]

31.06 / 0.9108

34.49 / 0.9316

34.42 / 0.9488

(d) SILS [14]

33.20 / 0.9463

(e) Clearing [10]

31.82 / 0.9353

(f) Our SPANet

38.22 / 0.9764

(g) DDN [11]

33.94 / 0.9460

(h) JORDER [40]

(i) DID-MDN [42]

(j) RESCAN [25]

35.09 / 0.9495

21.69 / 0.8018

34.35 / 0.9265

Figure 10. Visual comparison of SPANet with the state-of-the-art derainers. PSNR/SSIM results are included for reference.

pared to the state-of-the-art derainers. This is because
SPANet can identify the rain streak regions and remove
them accurately. Figure 10 shows a visual example from our
test set. We can see that while methods (b)−(e) tend to leave
rain streaks unremoved and methods (g)−(j) tend to corrupt
the background, the proposed SPANet (f) can produce much
cleaner result. We also show some deraining examples on
rain images collected from previous derain papers and the
Internet in Figure 11. While existing derainers fail to re-
move the rain streaks and some of them tend to darken or
blur the background, our SPANet can handle different kinds
of rain streaks and preserve more details. Table 3 compares
the performances of SPANet with the state-of-the-art de-
rainers on the synthetic test set from [42], demonstrating
the effectiveness of SPANet.

Internal analysis. We verify the importance of the spa-
tial attentive module (SAM) and different ways of using it

in Table 4. Ba is a basic Resnet-like network that does not
use SAM. Bb, Bc, and Bf represent three variants of us-
ing only one SAM for four times (recall that we have four
SAB blocks), four SAMs, and four SAMs that share the
same weights for all operations, respectively. While we can
see that all variants of incorporating the SAM improve the
performance, Bf performs the best, as sharing the weights
makes the deraining process inter-dependent on the four
SAB blocks, which allows more attention to be put to the
challenging real rain streak distributions. Bd is the SPANet
but without the above attention branch in SAM. The com-
parison between Bd and Bf shows that attention branch is
effective in leveraging the local contextual information ag-
gregated from different directions. Be is a variant that re-
moves the attention loss supervision.
It demonstrates the
importance of providing explicit supervision on the atten-
tion map generation process.

12276

(a) Rain

(b) DDN [11]

(c) JORDER [40]

(d) DID-MDN [42]

(e) RESCAN [25]

(f) Our SPANet

Figure 11. Visual comparison of SPANet with the state-of-the-art CNN-based derainers on some real rain images collected from previous
derain papers and from the Internet.

Methods

Input

DSC [29]

LP [26]

Clear[10]

JORDER [40]

DDN [11]

JBO[47]

DID-MDN[42] Our SPANet

DID-MDN Test Set

0.7781/21.15
Table 3. Comparison on the test set from [42]. SPANet is trained on the synthetic dataset from [42].

0.8978/ 27.33

0.8622/24.32

0.8522/23.05

0.8422/22.07

0.7896/21.44

0.8352/22.75

0.9087/ 27.95

0.9342/30.05

Methods

Resnet

Single SAM

Ba

!

4 SAMs w/o shared weights
4 SAMs w/ shared weights

Self-Attention branch

Attention Loss

Bb

!

!

!

!

Bc

!

!

!

!

Bd

!

!

!

Be

!

!

!

Bf

!

!

!

!

PSNR
SSIM

38.06
0.9867
Table 4. Internal analysis of the proposed SPANet. The best per-
formance is marked in bold.

37.70
0.9858

37.39
0.9856

37.43
0.9856

37.43
0.9854

37.47
0.9854

(a) Input

(b) JORDER

(c) DID-MDN (d) Our SPANet

Figure 12. Failure case. Our method fails to remove extremely
dense rain streaks.

6. Conclusion and Future Work

In this paper, we have presented a method to produce a
high-quality clean image from a sequence of real rain im-
ages, by considering temporal priors together with human
supervision. Based on this method, we have constructed a
large-scale dataset of ∼29.5K rain/clean image pairs that
cover a wide range of natural rain scenes. Experiments
show that the performances of state-of-the-art CNN-based
derainers can be signiﬁcantly improved by training on the
proposed dataset. We have also benchmarked state-of-the-

art derainers on the proposed test set. We ﬁnd that the
stochastic distributions of real rain streaks, especially the
varying appearances of rain streaks, often fail these meth-
ods. To this end, we present a novel spatial attentive net-
work (SPANet) that can learn to identify and remove rain
streaks in a local-to-global spatial attentive manner. Exten-
sive evaluations demonstrate the superiority of the proposed
method over the state-of-the-art derainers.

Our method does have limitations. One example is given
in Figure 12, which shows that our method fails when pro-
cessing haze-like heavy rain.
It is because the proposed
dataset generation method fails to select clean pixels from
the misty video frames. As a result, the proposed network
produces a haze-like result.

Currently, our dataset generation method relies on hu-
man judgements. This is partly due to the fact that there are
no existing metrics that can assess the generated rain-free
images, without clean images for reference.
It would be
interesting to develop an unsupervised mechanism for this
purpose in the future.

Acknowledgement.

This work was supported by
NSFC (#91748104, #U1811463, #61632006, #61425002,
#61751203), Key Research and Development Program of
China (#2018YFC0910506), and the Open Project Program
of the State Key Lab of CAD&CG (#A1901).

12277

References

[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael
Brown. A high-quality denoising dataset for smartphone
cameras. In CVPR, 2018. 3

[2] Josue Anaya and Adrian Barbu. Renoir - a benchmark
dataset for real noise reduction evaluation. arXiv:1409.8230,
2014. 3

[3] Sean Bell, C. Lawrence Zitnick, Kavita Bala, and Ross Gir-
shick. Inside-outside net: Detecting objects in context with
skip pooling and recurrent neural networks. In CVPR, 2016.
5

[4] J´er´emie Bossu, Nicolas Hauti`ere, and Jean-Philippe Tarel.
Rain or snow detection in image sequences through use of a
histogram of orientation of streaks. IJCV, 2011. 2

[5] Yi Chang, Luxin Yan, and Sheng Zhong. Transformed low-
rank model for line pattern noise removal. In ICCV, 2017. 1,
2

[6] Jie Chen and Lap-Pui Chau. A rain pixel recovery algorithm

for videos with highly dynamic scenes. IEEE TIP, 2014. 2

[7] Jie Chen, Cheen-Hau Tan, Junhui Hou, Lap-Pui Chau, and
He Li. Robust video content alignment and compensation
for rain removal in a cnn framework. In CVPR, 2018. 3

[8] Yi Lei Chen and Chiou Ting Hsu. A generalized low-rank ap-
pearance model for spatio-temporally correlated rain streaks.
In ICCV, 2013. 2, 3

[9] Shuangli Du, Yiguang Liu, Mao Ye, Zhenyu Xu, Jie Li, and
Jianguo Liu. Single image deraining via decorrelating the
rain streaks and background scene in gradient domain. PR,
2018. 1

[10] Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao,
and John Paisley. Clearing the skies: A deep network ar-
chitecture for single-image rain streaks removal. IEEE TIP,
2017. 1, 2, 6, 7, 8

[11] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao
Ding, and John Paisley. Removing rain from single images
via a deep detail network. In CVPR, 2017. 1, 2, 6, 7, 8

[12] Kshitiz Garg and Shree K. Nayar. Detection and removal of

rain from videos. In CVPR, 2004. 2

[13] Kshitiz Garg and Shree K Nayar. Vision and rain.

IJCV,

2007. 2

[14] Shuhang Gu, Deyu Meng, Wangmeng Zuo, and Lei Zhang.
Joint convolutional analysis and synthesis sparse representa-
In ICCV, 2017. 6,
tion for single image layer separation.
7

[15] Wei Zhang Huiyou Chang Le Dong Liang Lin Guanbin Li,
Xiang He. Non-locally enhanced encoder-decoder network
for single image de-raining. In ACM MM, 2018. 1

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 5

[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

works. In CVPR, 2018. 2

[19] Tai-Xiang Jiang, Ting-Zhu Huang, Xi-Le Zhao, Liang-Jian
Deng, and Yao Wang. A novel tensor-based video rain
streaks removal approach via utilizing discriminatively in-
trinsic priors. In CVPR, 2017. 2, 4

[20] Li-Wei Kang, Chia-Wen Lin, and Yu-Hsiang Fu. Automatic
single-image-based rain streaks removal via image decom-
position. IEEE TIP, 2012. 1, 2

[21] Jin-Hwan Kim, Jae-Young Sim, and Chang-Su Kim. Video
deraining and desnowing using temporal correlation and
low-rank matrix completion. IEEE TIP, 2015. 2

[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015. 6

[23] Quoc Le, Navdeep Jaitly, and Geoffrey Hinton. A simple
way to initialize recurrent networks of rectiﬁed linear units.
arXiv:1504.00941, 2015. 4

[24] Minghan Li, Qi Xie, Qian Zhao, Wei Wei, Shuhang Gu, Jing
Tao, and Deyu Meng. Video rain streak removal by mul-
tiscale convolutional sparse coding. In CVPR, 2018. 2, 3,
4

[25] Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, and Hongbin
Zha. Recurrent squeeze-and-excitation context aggregation
net for single image deraining. In ECCV, 2018. 1, 2, 6, 7, 8
[26] Yu Li, Robby T Tan, Xiaojie Guo, Jiangbo Lu, and Michael
Brown. Rain streak removal using layer priors. In CVPR,
2016. 1, 2, 6, 7, 8

[27] Jiaying Liu, Wenhan Yang, Shuai Yang, and Zongming Guo.
Erase or ﬁll? deep joint recurrent rain removal and recon-
struction in videos. In CVPR, 2018. 2

[28] Peng Liu, Jing Xu, Jiafeng Liu, and Xianglong Tang. Pixel
Based Temporal Analysis Using Chromatic Property for Re-
moving Rain from Videos. CIS, 2009. 2

[29] Yu Luo, Yong Xu, and Hui Ji. Removing rain from a single
image via discriminative sparse coding. In ICCV, 2015. 1,
2, 6, 7, 8

[30] Jiayuan Mao, Tete Xiao, Yuning Jiang, and Zhimin Cao.

What can help pedestrian detection? In CVPR, 2017. 1

[31] motionvfx.

https://www.motionvfx.com/mplugs-48.html,

2014. 4

[32] Seonghyeon Nam, Youngbae Hwang, Yasuyuki Matsushita,
and Seon Joo Kim. A holistic approach to cross-channel im-
age noise modeling and its application to image denoising.
In CVPR, 2016. 3

[33] Tobias Plotz and Stefan Roth. Benchmarking denoising al-

gorithms with real photographs. In CVPR, 2017. 3

[34] PyTorch. http://pytorch.org. 6
[35] Weihong Ren, Jiandong Tian, Zhi Han, Antoni Chan, and
Yandong Tang. Video desnowing and deraining based on
matrix decomposition. In CVPR, 2017. 2

[36] Varun Santhaseelan and Vijayan K Asari. Utilizing local
phase information to remove rain from video. IJCV, 2015. 2
[37] Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao
Bao, Wangmeng Zuo, Chunhua Shen, Rynson W.H. Lau,
and Ming-Hsuan Yang. Vital: Visual tracking via adversarial
learning. In CVPR, 2018. 1

[18] Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and Pheng-
Ann Heng. Direction-aware spatial context features for
shadow detection. In CVPR, 2018. 5

[38] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
Simoncelli. Image quality assessment - from error visibility
to structural similarity. IEEE TIP, 2004. 6

12278

[39] Wei Wei, Lixuan Yi, Qi Xie, Qian Zhao, Deyu Meng, and
Zongben Xu. Should we encode rain streaks in video as de-
terministic or stochastic? In ICCV, 2017. 2, 3, 4

[40] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zong-
ming Guo, and Shuicheng Yan. Deep joint rain detection and
removal from a single image. In CVPR, 2017. 1, 2, 6, 7, 8

[41] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-

tion by dilated convolutions. In ICLR, 2016. 5

[42] He Zhang and Vishal M. Patel. Density-aware single image
In CVPR,

de-raining using a multi-stream dense network.
2018. 1, 2, 6, 7, 8

[43] He Zhang, Vishwanath Sindagi, and Vishal Patel. Image de-
raining using a conditional generative adversarial network.
arXiv:1701.05957, 2017. 2

[44] Xiaopeng Zhang, Hao Li, Yingyi Qi, Wee Kheng Leow, and
Teck Khim Ng. Rain removal in video by combining tempo-
ral and chromatic properties. In ICME, 2006. 2, 3

[45] Xueyang Fu Yue Huang Xinghao Ding Zhiwen Fan,
Huafeng Wu. Residual-guide feature fusion network for sin-
gle image deraining. In ACM MM, 2018. 1, 2

[46] Fengyuan Zhu, Guangyong Chen, and Pheng-Ann Heng.
From noise modeling to blind image denoising. In CVPR,
2016. 3

[47] Lei Zhu, Chi-Wing Fu, Dani Lischinski, and Pheng-Ann
Joint bi-layer optimization for single-image rain

Heng.
streak removal. In ICCV, 2017. 1, 2, 8

[48] Zhe Zhu, Dun Liang, Songhai Zhang, Xiaolei Huang, Baoli
Li, and Shimin Hu. Trafﬁc-sign detection and classiﬁcation
in the wild. In CVPR, 2016. 1

12279

