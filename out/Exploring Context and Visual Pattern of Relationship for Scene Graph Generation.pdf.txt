Exploring Context and Visual Pattern of Relationship for

Scene Graph Generation

Wenbin Wang1,2, Ruiping Wang1,2, Shiguang Shan1,2,3, Xilin Chen1,2

1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),

Institute of Computing Technology, CAS, Beijing, 100190, China

2University of Chinese Academy of Sciences, Beijing, 100049, China

3Peng Cheng Laboratory, Shenzhen, 518055, China

wenbin.wang@vipl.ict.ac.cn, {wangruiping, sgshan, xlchen}@ict.ac.cn

Abstract

Relationship is the core of scene graph, but its predic-
tion is far from satisfying because of its complex visual di-
versity. To alleviate this problem, we treat relationship as
an abstract object, exploring not only signiﬁcative visual
pattern but contextual information for it, which are two key
aspects when considering object recognition. Our obser-
vation on current datasets reveals that there exists intimate
association among relationships. Therefore, inspired by the
successful application of context to object-oriented tasks,
we especially construct context for relationships where all
of them are gathered so that the recognition could bene-
ﬁt from their association. Moreover, accurate recognition
needs discriminative visual pattern for object, and so does
relationship. In order to discover effective pattern for rela-
tionship, traditional relationship feature extraction methods
such as using union region or combination of subject-object
feature pairs are replaced with our proposed intersection
region which focuses on more essential parts. Therefore, we
present our so-called Relationship Context - InterSeCtion
Region (CISC) method. Experiments for scene graph gen-
eration on Visual Genome dataset and visual relationship
prediction on VRD dataset indicate that both the relation-
ship context and intersection region improve performances
and realize anticipated functions.

1. Introduction

Scene graph helps higher level scene understanding. Re-
cently, a number of works [41, 20, 48, 42, 19, 27, 7, 18,
49, 21, 23, 51, 43], have focused on discovering relation-
ships between objects or generating a graph representation
for a scene, which contains objects as nodes and their re-
lationships as edges. Besides, scene graph has evolved as
a promising alternative for high-level intelligence vision

Figure 1. Examples of scene graph. All the scene graphs
are generated from our baseline method [41] given ground
truth objects. Dotted arrow means that the model misses
this relationship, while solid one is detected correctly.

tasks, such as image captioning [24, 40, 45], image gen-
eration [13], and visual question answering [2, 38, 39, 40].
However, scene graph generation still remains a challenging
problem due to complexity of predicting pair-wise relation-
ships even if object categories and locations are all given.
Although previous works have proposed a series of tech-
niques to improve relationship prediction, visual pattern and
contextual information, two key aspects in object recogni-
tion, are still not considered profoundly for relationship.

Let’s ﬁrstly pay attention to contextual

information,
which is never exploited for relationship. Why should we
consider it? In Fig.1, the relationships between glass and
counter, cat and face, as well as cat and head are missed.
In fact, a number of same relationships (e.g. bottle on
counter, cat has ear) nearby have been detected correctly.
In other words, while predicting a speciﬁc relationship, cur-
rent methods only focus on the pair of regions with which
it relates, but ignore other relationships which may be help-
ful for reasoning itself. Once the corresponding object pairs
could not provide strong enough evidence for relationship
inference, the methods would fail.

8188

counterbottleglassononfaceheadcatearpawtailhashashashashasco-occurrence [4, 5, 48, 22] and the approaches of mod-
eling object context varies. Different from objects, direct
and explicit association between two relationships is not
easy to model, thus we hope to gather all the relationships
information to make themselves establish implicit connec-
tion. By this way, semantics and visual patterns of relation-
ships could be reasoned and improved respectively under
the guidance of mutual inﬂuences. Inspired by [4], we use
memory to construct such context for relationships, where
all relationships information is stored and reasoning process
happens. We will show that the relationship context indeed
functions and captures the frequent repetition law.

Apart from contextual information, visual pattern is an-
other key aspect for object, and so does relationship. To the
best of our knowledge, all of current works obtain relation-
ship features either from union region [41, 18], which is the
minimal closure of subject and object region, or combina-
tion of subject and object features [49]. Such combination-
like representations may not expose the real visual pattern
of relationship and mainly have two drawbacks. Firstly, a
large number of union regions overlap with each other [19].
The left image in Fig.3(a) gives an example. The relation-
ship features are too similar for the models to distinguish.
On the other hand, the subject and object areas contain too
much object information. As a result, the models may in-
fer the relationship mainly depending on the objects instead
of relationship pattern itself [43]. However, relationships
especially geometric predicates (e.g. on, in) are almost not
dependent on object categories. In this work, we hope to
separate visual pattern of relationship from object as much
as possible. We propose a simple but effective region, inter-
section region, for relationship feature extraction. As shown
in the right two images in Fig.3(a) (two boxes are intersec-
tant) and three images in Fig.3(b) (two boxes are disjoint),
the interactive parts of subjects and objects are more likely
to reveal the visual pattern of relationship because although
the objects vary, the visual patterns in these regions are sim-
ilar. Experiments on VG and VRD dataset [23] demonstrate
the effectiveness of our method.

2. Related Works

Scene Graph Generation. Scene graph is ﬁrstly men-
tioned in [14] for image retrieval. Recently, a number of
approaches [23, 7, 18, 49, 50, 21, 51, 27, 47, 31, 41, 20,
48, 42, 19, 43] are proposed to detect objects and predict
relationships concurrently. Most of them shed light on mes-
sage passing [41] between two related objects or the ob-
ject and its corresponding relationships. The effectiveness
of this message passing mechanism as well as its variants
[42, 19, 20, 18, 48] is proven. In our work, we especially
focus on message passing between relationships, creatively
exploiting implicit association among relationships which
is helpful for prediction.

8189

Figure 2. The fraction of images in which a relationship
appears no less than N times, denoted by RR@N . Green
bars are for N = 2 while blue bars are for N = 3.

(a)

(b)

Figure 3. (a) The left image shows the high probability of
overlap of union regions. The union regions of man (yellow
box) and hat (blue box), man and glove (pink box) are the
same (red box). Images on right show a case of intersection
region (yellow mask) when two boxes are intersectant. (b)
A case of intersection region when two boxes are disjoint.

To further expose the underlying occurrence pattern of
relationship, we examine the fraction of images in which a
relationship appears no less than N times, denoted by “Rep-
etition Rate (RR@N )”, using Visual Genome dataset [15]
(VG) which contains more than 40k annotated unique rela-
tionships for over 100k images. As long tail distribution ex-
ists and most infrequent relationships hardly appear in nor-
mal scenes, we investigate 50 most common relationships.
As shown in Fig.2, repetition is a ubiquitous phenomenon
for relationships in manual annotations. What leads to this
fact? As we know, the number of object categories are much
more than that of relationship categories. Different object
pairs could be described with the same relationship as long
as they share similar visual patterns. On the other hand,
there exist lots of ﬁxed phrase structures such as “object1-
has-object2” (e.g. “elephant-has-head”, and “elephant-has-
ear”), which is also indicated in [48]. As a result, many
relationships tend to repeat in images. These observations
are consistent with humans language habbits.

From the above observations, there exists strong associa-
tion among relationships, which encourages us to make use
of context to capture it. Context has been widely utilized for
object-oriented tasks in the form of comprehensible object

0.00.20.40.60.81.0RR@Nonwearinghasinofbelonging towearswithholdingridingpart ofeatingmounted onplayingsitting onnearbehindflying inwalking onattached tobetweenpainted onwalking ingrowing oncarryingwatchingstanding onatlaying onsayslying onparked onunderforabovehanging fromin front oflooking aton back ofcovered inusingalongmade ofovercoveringandagainstacrossfromtoN=2N=3(man-wearing-hat)(man-holding-glove)(elephant-on-land)(cattle-on-land)(zebra1-near-zerbra2)(giraffe1-near-giraffe2)(sheep1-near-sheep2)Context Modeling. Context modeling and reasoning
[16, 28, 30, 32, 8, 44, 34, 3, 12, 37] is one of the most
helpful approaches for scene or object recognition. A vari-
ety of previous works on scene understanding [36, 17], ob-
ject recognition [4, 5, 22, 11], attributes reasoning [9, 29],
human-object interaction [44], action recognition [25], have
beneﬁted from context. However, context is seldom consid-
ered in scene graph generation or visual relationship detec-
tion tasks. Zellers et al. [48] makes an early attempt to use
object context for scene graph generation. While our pro-
posed relationship context, is totally different from object
context used in [48]. We take a further step to demonstrate
that relationship context is as nonnegligible as object con-
text and even plays a more signiﬁcant role in relationship-
centric tasks compared with object context.

Relationship Feature Extraction. Almost all pub-
lished scene graph generation or visual realtionship detec-
tion methods have to construct initial feature for relation-
ship. The most general approaches include computing the
union region [41, 18, 7] and feeding it to a local feature
extraction module (e.g. RoI pooling layer [10]), or com-
bining the subject and object features [49]. These methods
are intuitive and work but either lack discrimination or rely
heavily on object information. Our proposed intersection
region concentrates on more essential part and is closer to
the real visual pattern of relationship.

3. Approach

Our goal is not only to especically construct relationship
context apart from object context to capture the hidden asso-
ciation among relationships, but also to discover more dis-
criminative visual pattern for them. To this end, our method,
Relationship Context - InterSeCtion Region (CISC), is de-
vised which will be described in following subsections.

3.1. Basic Scene Graph Model

Our framework is based on a basic scene graph model
which reﬁnes representations for objects and relationships
with explicit message passing mechanism. Therefore, we
start by describing a general skeleton for it.1

In a basic scene graph model, objects and relationships
are modelled separately for |C| object classes and |R| re-
lationship classes. They can be regarded as nodes v in

a virtual graph G = (V = V O S V R, E), as shown
in the inner dotted box in Fig.4(a), where vO ∈ V O de-
notes object, vR ∈ V R denotes relationship, and edge e =
(vO
ij) ∈ E means that if object i
and j are related, there are edges between vO
j , vO
i
and vR
ij . Each node has its own feature
f and broadcasts its message to neighbors to instruct them

j )S(vO
ij , as well as vO

ij)S(vO
j and vR

i and vO

i , vO

i , vR

j , vR

to reﬁne features. The brown dotted bidirectional arrows in
Fig.4(a) demonstrate the message passing process.

i ∈ RD and f O

In practice, supposing f O

j ∈ RD are fea-
tures2 of two object candidates (obtained from region pro-
posal methods, e.g. RPN [33]) associated with vO
i and vO
j ,
ij ∈ RD represents the relationship feature associated
and f R
with vR
ij , the message passing procedure can be written as:

M O→O(f O

j ), X

M R→O(f R

j∈N O
i ), M O→R(f O

i

j )(cid:1) ,

ij )
,

(1)

(2)

(3)

i = GO 

mO

 X

j∈N O

i

ij = GR (cid:0)M O→R(f O
mR
f O
i ← U O(f O
i ),
f R
ij ← U R(f R
ij),
i ∈ RD and mR
i and vR

i , mO
ij , mR

ij respectively. N O
i

(4)
ij ∈ RD denote messages received
where mO
by node vO
stands for neighbors
i . M O→O, M R→O, and M O→R are message pro-
of vO
cessing functions that extract useful information from node
features. Their superscripts indicate the direction of mes-
sage passing (e.g. R → O denotes “from relationship to
object” ). GO and GR represent gathering functions which
integrate messages from sources. U O and U R are update
functions for object and relationship respectively. After
message passing process, the reﬁned features could be used
to make predictions. In next subsection, we will build con-
text based on this universal skeleton.

3.2. Relationship Context Construction

With representations of objects and relationships ob-
tained from basic scene graph model, context is able to be
constructed. However, different from objects which have
co-occurrence association, it is difﬁcult to model explicit
and interpretable association between any two relationships.
Therefore, the most feasible way is to construct relation-
ship context implicitly, which can be considered as a rem-
edy of missing of message passing between relationships in
basic scene graph model. On the other hand, we also hope
the relationship context to keep the 2-dimensional spatial
structure of an image so that a speciﬁc relationship can be
affected by surrounding similar relationships if there exist.
Memory [4, 5] meets our demand. In [4, 5], memory is used
for object context construction. Information of previously
detected objects is saved into the memory, which provides
context for further object reasoning. Supposing there are
N object instances O = [O1, O2, . . . , ON ] to be detected
given an image I. Then an iterative detection model M is
expected to maximize the log-likelihood:

LO ≈ log P(cid:16)O(t)

1:N , M, I(cid:17) ,

1:N(cid:12)(cid:12)S (t−1)

(5)

1We do not differentiate “subject” and “object” but use “ob-

ject” uniformly instead. We use “predicate” to refer to a certain relation.

2The feature is a 1-dimensional vector with size D when referring to
message passing or prediction process, while it is a tensor with spatial size
in memory updating process, except that there is extra explanation.

8190

Figure 4. (a) Details of a basic scene graph model. Message passing described in Eq.(1-4) and prediction are executed
through the constructed virtual graph G shown in the inner dotted box. (b) Architecture of context reasoning network. (c)
The framework of our method. After obtaining initial visual features for objects and relationships, the initial iteration is
triggered and produces predicted information. In each round of later memory iterations, the predicted information together
with their initial visual features are used to update the memories. Then two memories take the responsibility, which will
conduct context reasoning process and provide updated features for further predictions.

where O(t)
1:N stands for the prediction of all objects at
timestep t and memory S (t−1)
1:N encodes information of all
objects at last timestep, t − 1. S (0)
1:N is an empty mem-
ory. In practice, S is a three-dimensional tensor with shape
h × w × c. h and w are the same as the spatial size of the
feature map of image I processed by a feature extraction
network. c is depth size so that the memory stores extra
useful information at each spatial location.

Naturally, we consider constructing relationship context
with memory. Let C denote relationship memory. K rela-
tionships R = [R1, R2, R3, . . . , RK] need to be classiﬁed.
C(t)
1:K encodes information of all relationships at timestep t
and C(0)
1:K is empty. We extend the detection model M to
our whole framework. The relationship prediction part of
M is expected to maximize:
LR ≈ log P(cid:16)R(t)

1:K , M, I(cid:17) .

(6)

1:K(cid:12)(cid:12)C(t−1)

Next we describe the whole framework, which is de-
picted in Fig.4(c). The object representations f O
i obtained
from the front-end object detector and features f R
ij of re-
lated pairs of objects acquired with relationship feature
extraction methods are instantly fed into the basic scene
graph model and make predictions as shown in Fig.4(a).
The predicted information includes the object class scores
pO ∈ RN ×|C| and locations lO ∈ RN ×4|C| (4 indicates
four coordinates of bounding boxes), and relationship class
scores pR ∈ RK×|R|. This is the initial iteration.

tion as possible in memories. Therefore, the inputs to mem-
ories f Oinp , f Rinp should contain ﬁxed initial visual features
together with predicted information (denoted by four “in-
put” arrows in Fig.4(c)):

f Oinp = ReLU(Fc(pO) + Fc(lO) + Conv1×1(f O)), (7)
f Rinp = ReLU(Fc(pR) + Conv1×1(f R)),

(8)
where the fully connected layers and convolutional layers
are used to unify dimensions. Secondly, let’s consider de-
tails of memory updating (denoted by four “update” arrows
in Fig.4(c)). Since the memories should not forget previ-
ously obtained information, we exploit the update mecha-
nism of GRU [6] which is a kind of RNN. Thus we regard
the memories as GRU cells. In a GRU cell, previously ac-
quired information is stored as internal state, which can also
inﬂuence the output. Similarly, the states of memories, de-
noted by f Osta and f Rsta , are obtained by applying RoI pool-
ing operation to the memories. Finally the new features are
computed with GRU and memories are updated with inverse
RoI pooling opearation (similar to the operation mentioned
in [46], which puts the features back to their original spatial
positions):
f •new = z∗f •sta+(1−z)∗σ(WU f •inp +WH (r∗f •sta )), (9)

S = InvRoIP(f Onew ), C = InvRoIP(f Rnew )

(10)
where • stands for O or R, z and r are update and reset gate
in standard GRU, WU and WH are learnable convolutional
parameters, σ is sigmoid function and ∗ denotes element-
wise product. InvRoIP denotes inverse RoI Pooling.

The later memory iterations begin with memory updat-
ing. Firstly, we hope to remember as much known informa-

Now the object memory and relationship memory take
the responsibility, where context reasoning process is con-

8191

detectorbasic SG modelcontext initial iterationmemory iteration(1st)memory iteration(2nd)FCFCobject features(maps/vectors) f𝑖𝑂context (c) Framework (Take union region to extract relationship features for example).relationship features(maps/vectors) f𝑖𝑗𝑅clsbboxrelcls(a) Details of basic scene graph(SG) model.Conv3x3/512RoIPoolingobject 𝒗𝒊𝑶relationship𝒗𝒊𝒋𝑹f𝑖𝑂f𝑖𝑗𝑅(b) Details of context reasoning.inputinputinputinputmemoryedge 𝒆message passing𝒮𝒞p𝒊𝑶, l𝒊𝑶p𝒊𝒋𝑹Conv3x3/512Conv3x3/512ReLUReLUbasic SG modelf𝑶p𝑶,  l𝑶f𝑶inpf𝑹f𝑹inpp𝑹𝒮𝒞updateupdateupdateupdatef𝑶staf𝑹staFigure 5. Four cases of intersection region. Red solid boxes
are object boxes while the blue dotted boxes are our de-
signed intersection region. Case 1 contains two situations.

ducted. As the memories contain both semantic and visual
information, convolution is used to help integrate them and
spread information of a certain object or relationship to sur-
roundings. Similar to [5], context reasoning is realized with
three 3 × 3 convolutions and residual structure, as shown
in Fig.4(b). Especially for relationships, this process makes
use of a large number of similar visual patterns and helps
the model learn better representations. After context rea-
soning, object and relationship features are obtained from
these two memories and used for further predictions.

3.3. Intersection Region

As introduced in Sec.1, current methods for extracting
relationship features are either lacking in discrimination or
seriously dependent on objects. We propose intersection re-
gion shown in Fig.5 which focuses on more essential part
and reduces distractive object information. We elaborately
devise it considering both the intersectant and disjoint cases.
2, yj
2]
for two objects i and j where x1, x2 are horizontal bound-
aries and y1, y2 are vertical boundaries, we ﬁrstly judge
their relative position. Let (ci
y) be center
points of two boxes, wi, hi, wj , hj be widths and heights.
We give two auxiliary conditions for judgement:

Given bounding boxes[xi

2] and [xj

y) and (cj

1, xj

1, yj

1, xi

1, yi

2, yi

x, cj

x, ci

|ci

x − cj

x| ≥

|ci

y − cj

y| ≥

wi + wj

2

hi + hj

2

(11)

(12)

There are four cases:

1. Intersectant. The intersection box is directly obtained:

1, xj
2, xj

Bisc = [ max(xi
min(xi

1, yj
1),
2, yj
2)]
2. Disjoint, satisﬁes condition (11) and (12):
y, cj
y),
y, cj
y)]

Bisc = [ min(ci
max(ci

1), max(yi
2), min(yi

x), min(ci
x), max(ci

x, cj
x, cj

3. Disjoint, satisﬁes condition (11) but violates (12):

Bisc = [ min(ci
max(ci

x, cj
x, cj

x), min(yi
x), max(yi

1, yj
1),
2, yj
2)]

(13)

(14)

(15)

4. Disjoint, satisﬁes condition (12) but violates (11):

Bisc = [ min(xi
max(xi

1, xj
2, xj

1), min(ci
2), max(ci

y, cj
y),
y, cj

y)]

(16)

In the experiment section, we will introduce how to use

and evaluate our intersection region in practice.

4. Experiments

In following subsections we ﬁrstly clarify experimental
settings including datasets, evaluation metrics, and imple-
mentation details. Then we show the experiment results.

4.1. Experiment Settings

Datasets. Visual Genome is the largest dataset annotated
with scene graphs. However, different splits are used in pre-
vious works. We follow the split in [41] which is the most
common used. The split contains 75,651 images for training
and 32,422 images for testing. The most frequent 50 rela-
tionship categories and 150 object categories are selected to
be the predicted targets. Besides, VRD [23] is a standard
dataset for visual relationship detection, containing 4,000
images for training and 1,000 images for testing. 100 object
categories and 70 relationship categories are considered.

Evaluation. We adopt three universal evaluation tasks
(1) predicate classiﬁcation
for scene graph generation:
(PREDCLS): given ground truth categories and locations of
any two objects, predict their relationship, (2) scene graph
classiﬁcation (SGCLS): given ground truth locations of any
two objects, predict their categories and relationship, and
(3) scene graph generation (SGGEN): detect objects and
predict pair-wise relationships, and objects who have at
least 0.5 IoU overlap with their ground truth boxes are con-
sidered to be correctly detected. All evaluation modes use
recall@K metrics, where K maybe 20, 50 or 100.

4.2. Implementation Details

Choice of Basic Scene Graph Model.

In Sec.3.1 we
give a general skeleton of basic scene graph model. In prac-
tice, a model can be selected as long as the message passing
mechanism described by Eq.(1-4) is applicable. We choose
the model proposed in [41] for its favorable performance,
great popularity and easy implementation.

Models and Training Details. In the experiments, we
compare the results between union region and intersection
region. Besides, in order to explore for a better perfor-
mance, we further try to combine these two types of fea-
tures. Faster-RCNN [33] with VGG-16 [35] backbone is
selected as our front-end object detector for fair compari-
son. After the detector is trained and its layers are frozen,
the whole framework is then trained on ground truth scene
graph annotations. Furthermore, we also try to assemble the
predictions from each iteration with attention mechanism

8192

(a) Case 1, intersectant(inclusive or overlap).(b) Case 2, disjoint.(c) Case 3, disjoint.(d) Case 4, disjoint.[26]. Therefore, when making a prediction in each itera-
tion, an extra attention weight is predicted at the same time.
More details can be found in supplementary materials. The
source codes are implemented with Tensorﬂow3 [1].

4.3. Quantitative Results

We compare the following models and present main
quantitative results in Table 1. Mem: Our context-utilized
model.
It uses union region to extract relationship fea-
tures. Mem+Isc: Our context-utilized model which replaces
union region with our intersection region. Mem+Mix: Our
proposed full model which combines two types of rela-
tionship features. Mem+Mix+Attention: Based on model
Mem+Mix, we further assemble predictions from each iter-
ation with predicted attention weights in order to obtain a
best result. IMP [41]: Our baseline which uses union re-
gion to extract relationship features. We reimplement this
model and re-train it using our object detector. In Table 1,
the results of this model reported in [41] and [42] are pre-
sented together with ours. IMP+Isc and IMP+Mix: Re-
place the union region used in IMP with intersection region
or combination version respectively. Graph-RCNN [42]:
It is also a scene graph generation model based on message
passing. VRD [23]: We present its scene graph generation
results reported in [41]. Pixel2Graph [27]: We report its
results according to [48]. MSDN [20]: The VG split it uses
is different from ours. We train and evaluate it on our data
split and report the original and our reimplemented results.
From Table 1, results of our reimplemented IMP model
are close to or better than those of original version and
reimplemented version by [42] under most metrics, which
means that our reimplementation is correct and the improve-
ments mentioned below are from our proposed method.
Firstly, through the comparisons between Mem and IMP**,
Mem+Isc and IMP**+Isc, Mem+Mix and IMP**+Mix,
it preliminarily indicates that the usage of context is ef-
fective in helping the model recognize objects and rela-
tionships. We will further compare the importance be-
tween relationship context and object context and evalu-
ate the function of relationship context in following sub-
sections. On the other hand, IMP**+Isc performs better
than IMP**, and Mem+Isc outperforms Mem under most
metrics. It shows effectiveness of our intersection region.
Finally, the assembled models, IMP**+Mix, Mem+Mix,
and Mem+Mix+Attention, further boost the performance.
It is noteworthy that since our basic scene graph model
is IMP which limits the upper bound of performance, our
models cannot surpass some methods like Graph-RCNN or
Pixel2Graph under some metrics. However, results of our
assembled model, Mem+Mix+Attention, are close to them
and even better under some metrics.

3Our source codes are available at http://vipl.ict.ac.cn/

resources/codes.

(a)

(b)

Figure 6. (a) Results of using various iterations for model
Mem. (b) Performances of utilizing different methods to
extract relationship features. Results of three tasks are
shown under R@100 metric.

Figure 7. The per-type recall@5 of classifying individual
predicate tested on VG dataset. The predicates are listed in
descending order from left to right according to their repe-
tition rates(RR@2).

4.4. Evaluation of Memory

Ablation Study. To compare the importance of object
memory and relationship memory, we consider ablation ex-
periments in Table 1. Mem\relmem and Mem\objmem
stands for dropping the relationship memory module and
object memory module from Mem respectively. The results
suggest that the removal of relationship memory does more
harm to performance than removal of object memory.
It
implies that the association among relationships is nonneg-
ligible and even more important than that among objects for
scene graph generation tasks.

Multiple Iterations Analysis. We investigate the perfor-
mances of using various iterations for model Mem as shown
in Fig.6(a). We ﬁnd that 3 iterations are the best. Since
the memories are empty at the ﬁrst iteration, it actually only
takes 2 iterations for memories to capture the context. More
iterations may enhance noise.

Predicate Prediction. In order to explore what context
the relationship memory module actually captures, we eval-
uate per-type recall@5 of classifying individual predicate,
following [41]. In Fig.7, the per-type recall rates for IMP**
and Mem tested on VG dataset are listed in descending or-
der from left to right according to the predicate repetition
rate (RR@2). We can ﬁnd that the relationship memory
improves most results of predicates which have higher rep-
etition rate (near the left side in Fig.7) despite a few outliers.
For these predicates, it is easier for the memory module to
extract similar patterns and learn stronger representations.
While for some predicates with low repetition rate, the con-
tribution of relationship memory is limited. And on some
outliers, e.g. “mounted on”, “parked on ”, it fails mainly be-

8193

9.810.29.7SGGenR @ 100 (%)iter=2iter=3iter=427.528.027.2SGCls51.852.951.2PredCls7.58.38.710.610.5SGGenR @ 100 (%)IMP**+DROPIMP**IMP**+IscIMP**+Isc+DROPIMP**+Mix2424.724.825.325.5SGCls48.148.950.85151.7PredClsonwearinghasinofbelonging towearswithholdingridingpart ofeatingmounted onplayingsitting onnearbehindflying inwalking onattached tobetweenpainted onwalking ingrowing oncarryingwatchingstanding onatlaying onsayslying onparked onunderforabovehanging fromin front oflooking aton back ofcovered inusingalongmade ofovercoveringandagainstacrossfromto0.0%20.0%40.0%60.0%80.0%100.0%Evaluation of Memory: Recall@5 for different predicates in VG datasetIMP**Mems
l
e
d
o
m

Model
VRD [23]
IMP [41]
IMP* [42]
Pixel2Graph [27, 48]
MSDN [20]
MSDN* [20]
Graph-RCNN [42]

IMP**(baseline)
IMP**+Isc
IMP**+Mix
Mem
Mem+Isc
Mem+Mix
Mem+Mix+Attention

s Mem\relmem
Mem\objmem
Mem

n
o
i
t
a
l
b
A

Scene Graph Generation

Scene Graph Classiﬁcation

Predicate Classiﬁcation

R@20 R@50 R@100 R@20 R@50 R@100 R@20 R@50 R@100

-
-
-

6.5

-
-
-

4.2
4.7
5.3
4.8
5.0
6.0
7.7

4.5
4.8
4.8

0.3
3.4
6.4
8.1
10.7
11.1
11.4

6.8
7.3
8.0
7.6
7.9
9.4
11.4

7.3
7.4
7.6

0.5
4.2
8.0
8.2
14.2
14.0
13.7

8.3
8.7
10.5
10.2
10.5
11.9
13.9

9.7
10.0
10.2

-
-
-

18.2

-
-
-

15.7
16.6
17.3
19.5
19.4
19.7
23.3

19.0
19.3
19.5

11.8
21.7
20.6
21.8
24.3

-

29.6

21.4
21.9
22.6
25.0
25.0
25.0
27.8

24.5
25.0
25.0

14.1
24.4
22.4
22.6
26.5

-

31.6

24.7
24.8
25.5
28.0
28.0
27.7
29.5

27.7
27.9
28.0

-
-
-

47.9

-
-
-

29.9
31.0
31.7
32.3
31.9
33.3
42.1

31.9
32.0
32.3

27.9
44.8
40.8
54.1
67.0

-

54.2

40.4
43.0
44.2
44.9
45.2
45.9
53.2

44.0
44.6
44.9

35.0
53.0
45.2
55.4
71.0

-

59.1

48.9
50.8
51.7
52.9
52.4
53.0
57.9

51.9
52.5
52.9

Table 1. Results table on Visual Genome test set. All numbers are in %. IMP*: results reimplemented by [42]. IMP**:
results reimplemented by us. MSDN*: The results reimplemented by us on our VG data split. Evaluation details about
PREDCLS and SGCLS in MSDN are not released.

Predicate Classiﬁcation

model

DrNet [7]

R@50

80.78

78.12
DrNet*
DrNet*+Isc
78.37
DrNet*+Mix 78.78

R@100

81.90

79.01
79.43
79.62

Table 2. Results on VRD test set. DrNet* denotes our re-
implementation using union region. DrNet*+Isc and Dr-
Net*+Mix use intersection region or mixture version.

cause these predicates are overshadowed seriously by other
semantically similar predicates, which may be ascribed to
annotation bias. Despite this, It is undeniable that the rela-
tionship memory captures the repetition law sucessfully and
helps the model learn better representations on most predi-
cates with high repetition rate and unambiguous semantics.

4.5. Evaluation of Intersection Region

(a) IMP** and IMP**+Isc tested on VG.

(b) DrNet* and DrNet*+Isc tested on VRD.

Figure 8. The per-type recall@5 of classifying individual
predicate. The predicates are listed from left to right accord-
ing to their degree of dependence to certain subject-object
pairs (left side means less dependence).

Results in Table 1 have shown effectiveness of intersec-
tion region. To further validate the universality, we conduct
another experiment for visual relatonship detection on VRD
dataset using model in [7]. We reimplement part of this
model, feeding it with ground truth objects and only predict-
ing the relations. The original model contains several mod-
ules which are trained separately. We train it end-to-end.
Results are shown in Table 2. Although the improvement is
not so obvious because of lots of ambiguous predicates, it
still proves the universality of intersection region.

Predicate Prediction. We explore the effect of inter-
section region on each predicate. We ﬁrstly compute the
number of subject-object pairs that each predicate asso-
ciates with. The predicate with larger number means that
it can be used to describe relationships for more types of
subject-object pairs, and thus has less dependence on a cer-
tain type of pair. The per-type recall@5 rates for compar-
isons between (IMP**, IMP**+Isc), and between (DrNet*,
DrNet*+Isc) are shown in Fig.8. The predicates are listed in
a descending order from left to right according to the num-

8194

onnearhasinbehindwithaboveofunderin front ofholdingsitting onoverwearingattached toforstanding onlooking atandatbetweenwearswatchinghanging fromlaying oncarryingbelonging toridingcoveringpart ofon back ofagainsttoalonglying onfromeatingacrossusingwalking onmounted onpainted onwalking incovered inmade ofplayinggrowing onparked onsaysflying in0.0%20.0%40.0%60.0%80.0%100.0%IMP**IMP**+Iscnext tonearbehindonabovein the front ofunderbesidebelowhasbyon the left ofoveron the right ofinbeneathon the top ofholdwithlooktouchsit oncarrystand oninsidestand next towearpark nextacrossattach toadjacent totaller thanridecontainsit next toagainstcoverpark onlying onatwalk topullwatchlean ondriveuserest onfacestand behinddrive onoutside offeedsleep onflyplay withstand underfolloweatskate onwalk next tosleep next tosit undersit behindwalkwalk pastwalk besidetalkpark behindhitkick0.0%20.0%40.0%60.0%80.0%100.0%DrNet*DrNet*+IscFigure 9. Examples of scene graph under the setting of
PREDCLS metric. All arrows (including dotted and solid
types) are ground truth relationships and detected correctly
by Mem. Dotted arrows stand for missed ones of IMP**.

Figure 10. Scene graph generation examples under the set-
ting of SGGEN metric for comparing IMP** with Mem. In
each row, the left image and scene graph are generated by
IMP** while the right ones are generated by Mem. In im-
ages and scene graphs, red boxes are predicted and overlap
with the ground truth, yellow boxes are ground truth with
no match. In scene graphs, red edges are true positives, or-
ange edges are false negatives, purple boxes and edges are
false positives. Some yellow boxes in scene graphs which
do not exist in images mean that they are detected correctly
but the model fails in detecting their relationships with any
other objects.

ber mentioned above. No matter in the VG or VRD dataset,
the predicates with less dependence are almost all geomet-
ric types. The intersection region especially contributes to
prediction of these predicates because features from inter-
section region are closer to the real visual patterns of predi-
cates and are less likely to be distracted by object informa-
tion, and at the same time geometric predicates rely less on
object categories compared with semantic predicates.

Feature-level Ablation Study. Since the traditional
union region indeed covers our intersection region, it’s nat-
ural to ask a question:
is it the intersection region that
plays a signiﬁcant role in relationship prediction? We con-
duct the feature-level ablation study. Apart from the model
IMP**, IMP**+Isc, and IMP**+Mix mentioned above, we
further evaluate another two models. One is to drop fea-
tures in intersection region from union region by setting the

features in intersection region to 0 (IMP**+DROP) and
the other one is to combine features in intersection region
with DROP (IMP**+Isc+DROP). The results are shown
in Fig.6(b).
IMP**+DROP declines from IMP** while
IMP**+Isc+DROP performs similarly to IMP**+Mix.
It
further justiﬁes the key importance of intersection region.

4.6. Qualitative Results

Qualitative examples for comparing the IMP** and Mem
under the setting of PREDCLS task are shown in Fig.9. The
results show higher predicate recall rates of our method.
What’s more, it is obvious that although the object cate-
gories with which a relationship associates are different, the
visual patterns of the relationship are similar. The relation-
ship context gathers these similar patterns to improve rela-
tionship representations and enhance recognition capability.
In Fig.10 we show some generated scene graphs using Mem
and IMP** on VG test images for contrastive analysis. It
shows that our method obtains higher recall with the help
of context. More qualitative results can be found in sup-
plementary materials, where we also provide examples for
comparing IMP** and IMP**+Isc to show the superiority
of intersection region.

5. Conclusion

In this work, we regard relationships as abstract objects
in scene graph generation task, considering their visual pat-
terns and contextual information. We discover that rep-
etition is a ubiquitous phenomenon among relationships,
hence we construct context for relationships apart from ob-
jects. Experiments show that the relationship context indeed
captures the repetition law and even more helpful for gen-
erating scene graphs compared with object context. What’s
more, intersection region is proposed to help recognize rela-
tionships relying more on their own visual patterns instead
of object information. From our evaluations, our methods
are universal and have potential to be used with other bet-
ter basic scene graph models. Despite our efforts on solv-
ing this task, there still exist some problems which are wor-
thy of discussions. Firstly, the performance of scene graph
models are sensitive to the quality of the front-end detector.
When the detector misses some objects, the relationships
will be missed, too. Another problem is the serious imbal-
ance in VG dataset, which makes it hard to improve the un-
derstanding of semantic relationships. It may be alleviated
by utilizing external language priors.

Acknowledgements. This work is partially supported by
973 Program under contract No. 2015CB351802, Nat-
ural Science Foundation of China under contracts Nos.
61390511, 61772500, CAS Frontier Science Key Research
Project No. QYZDJ-SSWJSC009, and Youth Innovation
Promotion Association No. 2015085.

8195

counterbottleglasswearingcatheadfaceearpawtailhashashashashaswoman1hair1bootcoatsidewalkwoman2jeanjackethair2hashasonononwearingwearinghasmanjacketwomanshirtwearingwearingdogeye-2noseheadeye-1earhashashashashasdogeye-2noseheadeye-1earhashashashashasmanshirteartienosearmhashashashashasglasshairhandheadneckpaperdeskhaswearingwearinghaswearingatononmanshirteartienosearmhashashashashasglasshairhandheadneckpaperdeskhaswearingwearinghaswearingatononReferences

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-
ﬂow: a system for large-scale machine learning. In USENIX
Symposium on Operating Systems Design and Implementa-
tion (OSDI), volume 16, pages 265–283, 2016. 6

[2] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,
C. Lawrence Zitnick, and D. Parikh. Vqa: Visual question
answering. In Proceedings of the IEEE International Confer-
ence on Computer Vision (ICCV), pages 2425–2433, 2015. 1

[3] S. Belongie, J. Malik, and J. Puzicha. Shape context: A
new descriptor for shape matching and object recognition. In
Advances in Neural Information Processing Systems (NIPS),
pages 831–837, 2001. 3

[4] X. Chen and A. Gupta. Spatial memory for context reasoning
in object detection. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pages 4106–4116,
2017. 2, 3

[5] X. Chen, L.-J. Li, L. Fei-Fei, and A. Gupta.

Iterative vi-
sual reasoning beyond convolutions. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 7239–7248, 2018. 2, 3, 5

[6] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical
evaluation of gated recurrent neural networks on sequence
modeling. arXiv preprint arXiv:1412.3555, 2014. 4

[7] B. Dai, Y. Zhang, and D. Lin. Detecting visual relation-
ships with deep relational networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 3298–3308, 2017. 1, 2, 3, 7

[8] S. K. Divvala, D. Hoiem, J. H. Hays, A. A. Efros, and
M. Hebert. An empirical study of context in object detec-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 1271–1278,
2009. 3

[9] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describ-
ing objects by their attributes. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 1778–1785, 2009. 3

[10] R. Girshick. Fast r-cnn.

In Proceedings of the IEEE In-
ternational Conference on Computer Vision (ICCV), pages
1440–1448, 2015. 3

[11] G. Heitz and D. Koller. Learning spatial context: Using
stuff to ﬁnd things.
In Proceedings of European Confer-
ence on Computer Vision (ECCV), volume 5302, pages 30–
43. Springer, 2008. 3

[12] D. Hoiem, A. A. Efros, and M. Hebert. Geometric context
from a single image. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision (ICCV), pages 654–
661, 2005. 3

[13] J. Johnson, A. Gupta, and L. Fei-Fei.

Image generation
from scene graphs. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
1219–1228, 2018. 1

Vision and Pattern Recognition (CVPR), pages 3668–3678,
2015. 2

[15] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bern-
stein, and L. Fei-Fei. Visual genome: Connecting language
and vision using crowdsourced dense image annotations. In-
ternational Journal of Computer Vision (IJCV), 123(1):32–
73, 2017. 2

[16] L. Ladicky, C. Russell, P. Kohli, and P. H. Torr. Graph cut
In Proceed-
based inference with co-occurrence statistics.
ings of European Conference on Computer Vision (ECCV),
volume 6315, pages 239–253. Springer, 2010. 3

[17] L.-J. Li, H. Su, L. Fei-Fei, and E. P. Xing. Object bank:
A high-level image representation for scene classiﬁcation &
semantic feature sparsiﬁcation. In Advances in Neural Infor-
mation Processing Systems (NIPS), pages 1378–1386, 2010.
3

[18] Y. Li, W. Ouyang, X. Wang, and X. Tang. Vip-cnn: Vi-
In Pro-
sual phrase guided convolutional neural network.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 7244–7253, 2017. 1, 2,
3

[19] Y. Li, W. Ouyang, B. Zhou, J. Shi, C. Zhang, and X. Wang.
Factorizable net: an efﬁcient subgraph-based framework for
scene graph generation. In Proceedings of European Con-
ference on Computer Vision (ECCV), volume 11205, pages
346–363. Springer, 2018. 1, 2

[20] Y. Li, W. Ouyang, B. Zhou, K. Wang, and X. Wang. Scene
graph generation from objects, phrases and region captions.
In Proceedings of the IEEE International Conference on
Computer Vision (ICCV), pages 1261–1270, 2017. 1, 2, 6, 7
[21] X. Liang, L. Lee, and E. P. Xing. Deep variation-structured
reinforcement learning for visual relationship and attribute
detection. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 4408–
4417, 2017. 1, 2

[22] Y. Liu, R. Wang, S. Shan, and X. Chen. Structure inference
net: Object detection using scene-level context and instance-
level relationships. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
6985–6994, 2018. 2, 3

[23] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual re-
lationship detection with language priors. In Proceedings of
European Conference on Computer Vision (ECCV), volume
9905, pages 852–869. Springer, 2016. 1, 2, 5, 6, 7

[24] J. Lu, J. Yang, D. Batra, and D. Parikh. Neural baby talk.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 7219–7228, 2018. 1
[25] M. Marszalek, I. Laptev, and C. Schmid. Actions in context.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2929–2936, 2009. 3
[26] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of vi-
sual attention. In Advances in Neural Information Processing
Systems (NIPS), pages 2204–2212, 2014. 6

[14] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. Shamma,
M. Bernstein, and L. Fei-Fei.
Image retrieval using scene
graphs. In Proceedings of the IEEE conference on Computer

[27] A. Newell and J. Deng. Pixels to graphs by associative em-
bedding. In Advances in Neural Information Processing Sys-
tems (NIPS), pages 2171–2180, 2017. 1, 2, 6, 7

8196

Conference on Computer Vision (ECCV), volume 11205,
pages 690–706. Springer, 2018. 1, 2, 6, 7

[43] X. Yang, H. Zhang, and J. Cai.

Shufﬂe-then-assemble:
Learning object-agnostic visual relationship features.
In
Proceedings of European Conference on Computer Vision
(ECCV), volume 11216, pages 38–54, 2018. 1, 2

[44] B. Yao and L. Fei-Fei. Modeling mutual context of object
and human pose in human-object interaction activities.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 17–24, 2010. 3

[45] T. Yao, Y. Pan, Y. Li, and T. Mei. Exploring visual relation-
ship for image captioning. In Proceedings of European Con-
ference on Computer Vision (ECCV), volume 11218, pages
711–727. Springer, 2018. 1

[46] G. Yin, L. Sheng, B. Liu, N. Yu, X. Wang, J. Shao, and C. C.
Loy. Zoom-net: Mining deep feature interactions for visual
relationship recognition. In Proceedings of European Con-
ference on Computer Vision (ECCV), volume 11207, pages
330–347. Springer, 2018. 4

[47] R. Yu, A. Li, V. I. Morariu, and L. S. Davis. Visual relation-
ship detection with internal and external linguistic knowl-
edge distillation. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pages 1974–1982,
2017. 2

[48] R. Zellers, M. Yatskar, S. Thomson, and Y. Choi. Neural
motifs: Scene graph parsing with global context.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 5831–5840, 2018. 1, 2,
3, 6, 7

[49] H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua. Visual
translation embedding network for visual relation detection.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 5532–5540, 2017.
1, 2, 3

[50] H. Zhang, Z. Kyaw, J. Yu, and S.-F. Chang. Ppr-fcn: Weakly
supervised visual relation detection via parallel pairwise r-
fcn.
In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pages 4233–4241, 2017. 2

[51] J. Zhang, M. Elhoseiny, S. Cohen, W. Chang, and A. M.
Elgammal. Relationship proposal networks.
In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 5678–5686, 2017. 1, 2

[28] A. Oliva and A. Torralba. The role of context in object recog-
nition. Trends in cognitive sciences, 11(12):520–527, 2007.
3

[29] D. Parikh and K. Grauman. Relative attributes. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion (ICCV), pages 503–510, 2011. 3

[30] D. Parikh, C. L. Zitnick, and T. Chen. From appearance to
context-based recognition: Dense labeling in small images.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 1–8, 2008. 3

[31] J. Peyre, I. Laptev, C. Schmid, and J. Sivic. Weakly-
supervised learning of visual relations. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV),
pages 5179–5188, 2017. 2

[32] A. Rabinovich, A. Vedaldi, C. Galleguillos, E. Wiewiora,
and S. Belongie. Objects in context. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV),
pages 1–8, 2007. 3

[33] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in Neural Information Processing Systems (NIPS),
pages 91–99, 2015. 3, 5

[34] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Texton-
boost: Joint appearance, shape and context modeling for
multi-class object recognition and segmentation. In Proceed-
ings of European Conference on Computer Vision (ECCV),
volume 3951, pages 1–15. Springer, 2006. 3

[35] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 5

[36] A. Torralba, K. P. Murphy, W. T. Freeman, and M. A. Rubin.
Context-based vision system for place and object recogni-
tion. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pages 273–280, 2003. 3

[37] A. Torralba and P. Sinha. Statistical context priming for ob-
ject detection.
In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pages 763–770,
2001. 3

[38] P. Wang, Q. Wu, C. Shen, A. Dick, and A. van den Hengel.
Fvqa: Fact-based visual question answering. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
40(10):2413–2427, 2018. 1

[39] P. Wang, Q. Wu, C. Shen, and A. van den Hengel. The vqa-
machine: Learning how to use existing vision algorithms to
answer new questions. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 3909–3918, 2017. 1

[40] Q. Wu, C. Shen, P. Wang, A. Dick, and A. van den Hen-
gel. Image captioning and visual question answering based
on attributes and external knowledge.
IEEE Transactions
on Pattern Analysis and Machine Intelligence (TPAMI),
40(6):1367–1381, 2018. 1

[41] D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei. Scene graph gen-
eration by iterative message passing. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 5410–5419, 2017. 1, 2, 3, 5, 6, 7

[42] J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh. Graph r-
cnn for scene graph generation. In Proceedings of European

8197

