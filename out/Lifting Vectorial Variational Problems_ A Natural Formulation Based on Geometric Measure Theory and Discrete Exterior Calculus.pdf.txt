Lifting Vectorial Variational Problems: A Natural Formulation based on

Geometric Measure Theory and Discrete Exterior Calculus

Thomas Möllenhoff and Daniel Cremers

Technical University of Munich

{thomas.moellenhoff,cremers}@tum.de

Abstract

Numerous tasks in imaging and vision can be formu-
lated as variational problems over vector-valued maps. We
approach the relaxation and convexiﬁcation of such vecto-
rial variational problems via a lifting to the space of cur-
rents. To that end, we recall that functionals with poly-
convex Lagrangians can be reparametrized as convex one-
homogeneous functionals on the graph of the function.
This leads to an equivalent shape optimization problem
over oriented surfaces in the product space of domain and
codomain. A convex formulation is then obtained by relax-
ing the search space from oriented surfaces to more gen-
eral currents. We propose a discretization of the resulting
inﬁnite-dimensional optimization problem using Whitney
forms, which also generalizes recent “sublabel-accurate”
multilabeling approaches.

1. Introduction

We consider functionals of C 1-mappings f : X → Y

c (x, f (x), ∇f (x)) dx,

(1)

E(f ) =ZX

where X ⊂ Rn, Y ⊂ RN are bounded and open. The cost
function c ≡ c(x, y, ξ) is assumed to be a nonnegative (pos-
sibly nonconvex) continuous function on X × Y × RN ×n
that is polyconvex (see Def. 2) in the Jacobian matrix ξ.

This work is concerned with relaxation and global op-
timization of (1) when, both, dimension and codimension
are possibly larger than one (n > 1, N > 1). This is ex-
pected to be difﬁcult: In the discrete setting problems with
n = 1 or N = 1 typically correspond to polynomial-time
solvable shortest path (n = 1) or graph cut (N = 1) prob-
lems [11, 60, 24, 53], whereas for n, N > 1, the arising
multilabel problems with unordered label spaces are known
to be NP-hard - see [35]. Nevertheless, heuristic strategies
have been shown to yield excellent results in tasks such
as optical ﬂow [10] or shape matching [55, 9].
In con-
trast to such well-established Markov random ﬁeld (MRF)

works [30, 31, 29, 55, 39, 9, 10, 14] we consider the way
less explored continuous (inﬁnite-dimensional) setting.

Our motivation partly stems from the fact that formula-
tions in function space are very general and admit a variety
of discretizations. Finite difference discretizations of con-
tinuous relaxations often lead to models that are reminis-
cent of MRFs [70], while piecewise-linear approximations
are related to discrete-continuous MRFs [71], see [17, 40].
More recently, for the Kantorovich relaxation in optimal
transport, approximations with deep neural networks were
considered and achieved promising performance, for exam-
ple in generative modeling [2, 54].

We further argue that fractional (non-integer) solutions
to a careful discretization of the continuous model can
implicitly approximate an “integer” continuous solution.
Therefore one can achieve accuracies that go substantially
beyond the mesh size. The resulting models would be
difﬁcult to interpret and derive from a ﬁnite-dimensional
viewpoint such that the continuous considerations are re-
quired for the ﬁnal implementation. Also, formulations
arising from continuous relaxations allow one to introduce
isotropic smoothness potentials without reverting to higher-
order terms in the cost, and, as we show in this work, one
can impose general polyconvex regularizations using only
local constraints. An example of a polyconvex function
(which is in general nonconvex) is the surface area of the
graph, sometimes referred to as “Beltrami regularization”
in the image processing community, see e.g., [28].

In contrast to the discrete multi-labeling setting, an im-
portant question is whether variational problems involving
the energy (1) admit a minimizer. A fruitful approach to
address this question is to suitably relax the notion of solu-
tion, thereby enlarging the search space of admissible can-
didates (“lifting the problem to a larger space”). The ori-
gins of this idea can be traced back1 to the turn of the
century, see Hilbert’s twentieth problem [21]. An exam-
ple of that principle is the celebrated Kantorovich relax-
ation [26] of Monge’s transportation problem [42]. There,

1We refer

the interested reader

to the historical

remarks

in

L. C. Young’s book on the calculus of variations [69, pp. 122–123].

111117

the search over maps f : X → Y is relaxed to one over
probability measures on the product space X × Y. Each
map can be identiﬁed in that extended space with a mea-
sure concentrated on its graph. Existence of optimal trans-
portation plans follows directly due to good compactness
properties of the larger space. Furthermore, the nonlinearly
constrained and nonconvex optimization problem is trans-
formed into one of linear programming, leading to rich du-
ality theories and fast numerical algorithms [47].

One may ask whether the relaxed solution in the ex-
tended space has certain regularity properties, for example
whether it is the graph of a (sufﬁciently regular) map and
thus can be considered a solution to the original (“unlifted”)
problem. In the case of optimal transport, such regularity
theory can be guaranteed under some assumptions [63, 52].
Establishing existence and regularity for problems in which
the cost additionally depends on the Jacobian (for example
minimal surface problems) has been a driving factor in the
development of geometric measure theory, see [44] for an
introduction. In this work, we will use ideas from geomet-
ric measure theory to pursue the above relaxation and lifting
principle for the energy (1). The main idea is to reformu-
late the original variational problem as a shape optimization
problem over oriented manifolds representing the graph of
the map f : X → Y in the product space X × Y . To ob-
tain a convex formulation we enlarge the search space from
oriented manifolds to currents.

1.1. Related Work

A common strategy to solve problems involving (1) is to
revert to local gradient descent minimization based on the
Euler-Lagrange equations. But for nonconvex problems so-
lutions might depend on the initialization and the computed
stationary points may be quite suboptimal. Therefore, we
pursue the aforementioned lifting of the energy (1) to cur-
rents. This lifting has been previously considered in geo-
metric measure theory to establish the aforementioned ex-
istence and regularity theory for vectorial variational prob-
lems in a very broad setting, see e.g., [15, 16, 5]. In contrast
to such impressive theoretical achievements, this paper is
concerned with a discretization and implementation.

There is also a variety of related applied works. The pa-
per [68] tackles the problem of bijective and smooth shape
matching using linear programming. Similar to the present
work, the authors also look for graph surfaces in X × Y but
they consider the discrete setting and use a different notion
of boundary operator. We study the continuous setting, but
also our discrete formulation is quite different.

For N = 1, the proposed continuous formulation spe-
cializes to [1, 48]. To tackle the setting of N > 1 in a
memory efﬁcient manner, Strekalovskiy et al. [58, 19, 59]
keep a collection of N surfaces with codimension one un-
der the factorization assumption that Y = Y1 × . . . × YN .

In contrast, we consider only one surface of codimension
N , we do not require an assumption on Y, our approach is
applicable to a larger class of functionals and we expect it
to yield a tighter relaxation. The lifting approaches [34, 20]
also tackle vectorial problems by considering the full prod-
uct space, but are limited to total variation regularization
(with the former allowing Y to be a manifold). The recent
work [67] is most related to the present one, however their
relaxation considers a speciﬁc instance of (1). Moreover,
the above works are based on ﬁnite difference discretiza-
tions of the continuous model. In contrast, the proposed dis-
cretization using discrete exterior calculus yields solutions
beyond the mesh accuracy as in recent sublabel-accurate ap-
proaches. The latter are restricted to N = 1 [41, 40] or to-
tal variation regularization [33]. Recent works also include
extensions to total generalized variation or Laplacian regu-
larization [57, 64, 36].

Recent approaches in shape analysis [56, 62, 61] also
operate in the product space X × Y. However, these are
based on local minimizations of the Gromov-Wasserstein
distance [37] and spectral variants thereof [38] which leads
to (nonconvex) quadratic assignment problems. While the
goal to ﬁnd a smooth (possibly bijective) map is similar, the
formulations appear to be quite different. To alleviate the
increased cost of the product space formulation, computa-
tionally efﬁcient representations of densities in X × Y have
been studied in the context of functional maps [46, 51].

2. Notation and Preliminaries

Throughout this paper we will introduce notions from
geometric measure theory, as they are not commonly used
in the vision community. While the subject is rather techni-
cal, our aim is to keep the presentation light and to focus on
the geometric intuition and aspects which are important for
a practical implementation. We invite the reader to consult
chapter 4 in the book [44] and the chapter on exterior calcu-
lus in [13], which both contain many illuminating illustra-
tions. For a more technical treatment we refer to [15, 32].

In the following, we denote a basis in Rd as {e1, . . . , ed}
with dual basis {dx1, . . . , dxd} where dxi : Rd → R is the
linear functional that maps every x = (x1, . . . , xd) to the i-
th component xi. Given an integer k ≤ d, I(d, k) are the
multi-indices i = (i1, . . . , ik) with 1 ≤ i1 < . . . < ik ≤ d.
As we will consider n-surfaces in X ×Y ⊂ Rn+N , most
of the time we set d = n+N and k = n. To further simplify
notation, we denote the basis vectors {en+1, . . . , en+N }
by {ε1, . . . , εN } and similarly refer to the dual basis as
{dx1, . . . dxn, dy1, . . . , dyN }. When it is clear from the
context, we treat vectors ei ∈ Rn and εi ∈ RN in the sense
that ei ≃ (ei, 0N ) ∈ Rn+N , εi ≃ (0n, εi) ∈ Rn+N . As an
example, for ∇f (x) ∈ RN ×n we can deﬁne the expression
ei + ∇f (x)ei and read it as (ei, ∇f (x)ei) ∈ Rn+N .

11118

2.1. Convex Analysis

The extended reals are denoted by R = R ∪ {+∞}. For
a ﬁnite-dimensional real vector space V and Ψ : V → R
we denote the convex conjugate as Ψ∗ : V ∗ → R and the
biconjugate as Ψ∗∗ : V → R. Ψ∗∗ is the largest lower-
semicontinuous convex function below Ψ. In our notation,
for functions with several arguments, the conjugate is al-
ways taken only in the last argument. As a general reference
to convex analysis, we refer the reader to the books [23, 50].

2.2. Multilinear Algebra

The formalism of multi-vectors we introduce in this sec-
tion is central to this work, as the idea of the relaxation is to
represent the oriented graph of f by a k-vectorﬁeld (more
precisely: a k-current) in the product space X × Y. Basi-
cally, one can multiply vi ∈ Rd to obtain an object

v = v1 ∧ . . . ∧ vk,

(2)

vi · ei,

called a simple k-vector in Rd. The geometric intuition of
simple k-vectors is, that they describe the k-dimensional
space spanned by the {vi}, together with an orientation and
the area of the parallelotope given by the {vi}. Thus, sim-
ple k-vectors can be thought of oriented parallelotopes as
shown in orange in Fig. 1. In general, k-vectors are deﬁned
to be formal sums

for coefﬁcients vi ∈ R. They form the vector space ΛkRd,

vi · ei1 ∧ . . . ∧ eik = Xi∈I(d,k)

v = Xi∈I(d,k)
k(cid:1).
which has dimension(cid:0)d
(and also for k-covectors) v = Pi viei, w = Pi wiei an
inner product hv, wi =Pi viwi and norm |v| =phv, vi.

The dual space ΛkRd of k-covectors is deﬁned analo-
gously, with hdxi, eji = δij. We deﬁne for two k-vectors

k-vectors (elements of ΛkRd) are called simple, if they
can be written as the wedge product of 1-vectors as in (2).
Unfortunately, for 1 < k < d − 1, not all k-vectors are
simple and the set of simple k-vectors is a nonconvex cone
in ΛkRd, called the Grassmann cone [7]. This is one aspect
why the setting of n > 1 and N > 1 is more challenging.

(3)

Later on, we will consider a relaxation from the noncon-
vex set of simple k-vectors to general k-vectors. Naturally,
for the relaxation to be good, we want the convex energy
to be as large as possible on non-simple k-vectors. For the
Euclidean norm, a good convex extension is the mass norm

kvk = inf(Xi

|ξi| : ξi are simple, v =Xi

ξi) .

(4)

The dual norm is the comass norm given by:

kwk∗ = sup {hw, vi : v is simple , |v| ≤ 1} .

(5)

The mass norm can be understood as the largest norm that
agrees with the Euclidean norm on simple k-vectors.

Y ⊂ R

z

TzGf

e1 + ξe1
(e1 + ξe1) ∧ (e2 + ξe2) ∈ Λ2R3

e2 + ξe2

Gf ⊂ X × Y

e2
e1
e1 ∧ e2 ∈ Λ2R3

X ⊂ R2

Figure 1: Illustration for the setting of n = 2, N = 1.
The graph Gf of the C 1-map f : X → Y is a smooth ori-
ented manifold embedded in the product space X × Y. The
tangent space at z = (x, f (x)) is spanned by the simple n-
vector (e1 + ξe1) ∧ . . . ∧ (en + ξen) ∈ ΛnRn+N , where
ξ = ∇f (x) ∈ RN ×n is the Jacobian.

3. Lifting to Graphs in the Product Space

With the necessary preliminaries in mind, our goal is
now to reparametrize the original energy (1) to the graph
Gf ⊂ X × Y. As shown in Fig. 1, the graph is an oriented
n-dimensional manifold in the product space with global
parametrization u(x) = (x, f (x)).

Deﬁnition 1 (Orientation). If M ⊂ Rd is a k-dimensional
smooth manifold in Rd (possibly with boundary), an orien-
tation of M is a continuous map τM : M → ΛkRd such
that τM(z) is a simple k-vector with unit norm that spans
the tangent space TzM at every point z ∈ M.

From differential geometry we know that the tangent
space TzGf at z = (x, f (x)) is spanned by ∂iu(u−1(z)) =
ei + ∇f (x)ei. Therefore, an orientation of Gf is given by

τGf (z) =

M (∇f (π1z))
|M (∇f (π1z))|

,

(6)

where the map M : RN ×n → ΛnRn+N is given by

M (ξ) = (e1 + ξe1) ∧ . . . ∧ (en + ξen),

(7)

and π1 : X × Y → X is the canonical projection onto the
ﬁrst argument. In order to derive the reparametrization, we
have to connect a simple n-vector (representing an oriented
tangent plane of the graph) with the Jacobian of the original
energy. For that, we need an inverse of the map given in (7).
To derive such an inverse, we ﬁrst introduce further help-
ful notations. For i ∈ I(m, l) we denote by ¯i ∈ I(m, m − l)
the element which complements i in {1, 2, . . . , m} in in-
creasing order, denote ¯0 = {1, . . . , m} and 0 as the empty

11119

multi-index. Every v ∈ ΛnRn+N can be written as

(8)

vi,jei ∧ εj,

v = X|i|+|j|=n
2(cid:1) = 10 coefﬁcients of a 2-vector v ∈ Λ2R5

example, the(cid:0)5

according to the notation (8) are:

where i ∈ I(n, l), j ∈ I(N, l′), l + l′ = n. To give an

v¯0,0
v1,1
v1,2
v1,3

v2,1
v2,2
v2,3

v0,(1,2)
v0,(1,3)

v0,(2,3),

(9)

where we highlighted the N × n coefﬁcients with |j| = 1.
Now note that the vector v = M (ξ) is by construction a
simple n-vector with ﬁrst component v¯0,0 = 1. To any
v ∈ ΛnRn+N with v¯0,0 = 1 we associate ξ(v) ∈ RN ×n
given by

[ξ(v)]j,i = (−1)n−iv¯i,j.

(10)

If and only if v ∈ ΛnRn+N is simple with ﬁrst component
v¯0,0 = 1 then v = M (ξ(v)). A proof is given in [18, Vol. I,
Ch. 2.1, Prop. 1]. Thus, on the set of simple n-vectors with
ﬁrst component v¯0,0 = 1,

Σ1 = {v ∈ ΛnRn+N : v = M (ξ) for ξ ∈ RN ×n}, (11)

the inverse of the map (7) is given by (10).

Using the above notations, we can deﬁne a generalized
notion of convexity, which essentially states that there is a
convex reformulation on k-vectors.

Deﬁnition 2 (Polyconvexity). A map c : RN ×n → R is
polyconvex if there is a convex function ¯c : ΛnRn+N → R
such that we have

c(ξ) = ¯c(M (ξ)) for all ξ ∈ RN ×n.

(12)

Equivalently one has that c(ξ(v)) = ¯c(v) for all v ∈ Σ1.
We also refer to the convex function ¯c as a polyconvex ex-
tension.

In general, the polyconvex extension is not unique. Any
convex function has an obvious polyconvex extension by
(10), but as discussed in the previous section we would like
the convex extension to be as large as possible for v /∈ Σ1.
The largest polyconvex extension which agrees with the
original function on Σ1 can be formally deﬁned using the
convex biconjugate, but is often hard to explicitly compute.
The mass norm (4) corresponds to such a construction.

Nevertheless, given any polyconvex extension, we can
now reparametrize the original energy (1) on the oriented
graph Gf , as we show in the following central proposition.

Proposition 1. Let ¯c : X × Y × ΛnRn+N → R be a poly-
convex extension of the original cost c in the last argument.
Deﬁne the function Ψ : X × Y × ΛnRn+N → R,

Ψ(z, v) =(v¯0,0¯c(π1z, π2z, v/v¯0,0),

+∞,

if v¯0,0 > 0,
otherwise,

(13)

where π1 : X × Y → X and π2 : X × Y → Y are the
canonical projections onto the ﬁrst and second argument.
Then we can reparametrize (1) as follows:

ZX

c(x, f (x), ∇f (x)) dLn(x)

Ψ(z, τGf (z)) dHn(z),

(14)

=ZGf

where the second integral is the standard Lebesgue inte-
gral with respect to the n-dimensional Hausdorff measure
on Rn+N restricted to the graph Gf .

Proof. We directly calculate:

c (x, f (x), ∇f (x)) dLn(x)

ZX
=ZX
=ZGf
=ZGf

(15)

(16)

(18)

Ψ (x, f (x), M (∇f (x))) dLn(x)

Ψ (z, M (∇f (π1z)))

1

|M (∇f (π1z))|

dHn(z) (17)

Ψ(cid:0)z, τGf (z)(cid:1) dHn(z).

The step from (15) to (16) uses that ¯c is a polyconvex ex-
tension (so that we can apply (12)) and the fact that for
v = M (∇f (x)) we have v¯0,0 = 1. To arrive at (17), an
application of the area formula [32, Corollary 5.1.13] suf-
ﬁces and for (18) we used positive one-homogenity of Ψ
and the deﬁnition of τGf in (6).

Interestingly,

the function (13) is convex and one-
homogeneous in the last argument, as it is the perspective
of a convex function. However, the search space of oriented
graphs of C 1 mappings is nonconvex. Therefore we relax
from oriented graphs to the larger set of currents, which we
will introduce in the following section. Since currents form
a vector space, we therefore obtain a convex functional over
a convex domain.

4. From Oriented Graphs to Currents

Throughout this section, let U ⊂ Rd be an open set,
which will later be a neighbourhood of X × Y ⊂ Rn+N ,
where X = cl(X ), Y = cl(Y) are the closures of X , Y.
The main idea of our relaxation and the geometric intuitions
of pushforward and boundary operator we introduce in this
section are summarized in the following Fig. 2. Currents
are deﬁned in duality with differential forms, which we will
brieﬂy introduce in the following section.

11120

π2♯T = JY K

spt ∂T ⊂ (∂X) × Y

∂T

T = JGf K

π1♯T = JXK

(a) Graph of diffeomorphism f

(b) Graph of function with jumps

(c) “Stitched” graph

(d) Current which is not a graph

Figure 2: The idea of our relaxation is to move from oriented graphs in the product space to the larger set of currents. These
include oriented graphs as special cases, as shown in (a). For a diffeomorphism, the pushforwards π1♯T , π2♯T yield currents
induced by domain and codomain, which will be a linear constraint in the relaxed problem. In (b) we show the current given
by the graph of a discontinuous function. Since it has holes, the boundary operator ∂T has support inside the domain. We
will constrain the support of the boundary to exclude such cases. (c) Stitching jumps yields a current with vertical parts at the
jump points, which corresponds to the limiting case in the perspective function (13). To obtain an overall convex formulation,
we will also allow currents (d) which don’t necessarily concentrate on the graph of a function.

4.1. Differential Forms

A differential form of order k (short: k-form) is a map
ω : U → ΛkRd. The support of a differential form spt ω
is deﬁned as the closure of {z ∈ U : ω(z) 6= 0}. Integra-
tion of a k-form over an oriented k-dimensional manifold is
deﬁned by

ZM

ω :=ZM

hω(z), τM(z)i dHk(z).

(19)

A notion of derivative for k-forms is the exterior derivative
dω, which is a (k + 1)-form given by:

hdω(z), v1 ∧ . . . ∧ vk+1i = lim
h→0

1

hk+1Z∂P

ω,

(20)

where ∂P is the oriented boundary of the parallelotope
spanned by the {hvi} at point z. To get an intuition, note
that for k = 0 this reduces to the familiar directional deriva-
1
h (ω(x + hv1) − ω(x)). With
tive hdω(x), v1i = limh→0
(19) and (20) in mind, one sees why Stokes’ theorem

ZM

dω =Z∂M

ω.

(21)

should hold intuitively. Given a map π : Rd → Rq, the
pullback π♯ω of the k-form ω is determined by

hπ♯ω, v1 ∧ .. ∧ vki = hω ◦ π, Dv1 π ∧ .. ∧ Dvk πi,

(22)

where Dvi π = ∇π · vi and ∇π ∈ Rq×d is the Jacobian.

4.2. Currents

Denote the space of smooth k-forms with compact sup-
port on U as Dk(U ). Currents are elements of the dual
space Dk(U ) = Dk(U )′, i.e., linear functionals acting on

differential forms. As shown in Fig. 2a, an oriented k-
dimensional manifold M ⊂ U induces a current by

JMK(ω) =ZM

ω.

(23)

However, since Dk(U ) is a vector space, not all elements
look like k-dimensional manifolds, see Fig. 2d. The bound-
ary of the k-current T ∈ Dk(U ) is the (k − 1)-current
∂T ∈ Dk−1(U ) deﬁned via the exterior derivative:
∂T (ω) = T (dω), for all ω ∈ Dk−1(U ).

(24)

Stokes’ theorem (21) ensures that for currents which are
given by k-dimensional oriented manifolds, the boundary
of the current agrees with the usual notion, see Fig. 2b.

The support of a current, denoted by spt T , is the com-

plement of the biggest open set V such that

T (ω) = 0 whenever spt(ω) ⊂ V.

(25)

Given a map π : Rd → Rq the pushforward π♯T of the
k-current T ∈ Dk(U ) is given by

π♯T (ω) = T (π♯ω), for all ω ∈ Dk(Rq).

(26)

Intuitively, it transforms the current using the map π, as il-
lustrated in Fig. 2a. The mass of a current T ∈ Dk(U ) is

M(T ) = sup(cid:8)T (ω) : ω ∈ Dk(U ), kω(z)k∗ ≤ 1(cid:9) ,

and as expected M(JMK) = Hk(M). We denote the
space of k-currents with ﬁnite mass and compact support
by Mk(U ). These are representable by integration, mean-
ing there is a measure kT k on U and a map ~T : U → ΛkRd
such that k ~T (z)k = 1 for kT k-almost all z such that

(27)

T (ω) =Z hω(z), ~T (z)i dkT k(z).

(28)

The decomposition (28) is crucial, and we will use it to de-
ﬁne the relaxation in the next section.

11121

4.3. The Relaxed Energy

We lift the original energy (1) to the space of ﬁnite mass

currents T ∈ Mn(U ) with spt T ⊂ X × Y as follows:

E(T ) =Z Ψ∗∗(cid:16)π1z, π2z, ~T (z)(cid:17) dkT k(z).

(29)

Since for T = JGf K we have ~T = τGf , kT k = Hn¬Gf the
desirable property E(JGf K) = E(f ) holds due to Prop. 1.

Note that in (29) we use the lower-semicontinuous reg-
ularization Ψ∗∗ which extends (13) at v¯0,0 = 0 with the
correct value.
Interestingly, this point corresponds to the
situation when the graph has vertical parts, which cannot
occur for C 1 functions but can happen for general currents,
see Fig. 2c. In [43] it was shown that one can penalize such
jumps in a way depending on the jump distance and direc-
tion. We will not consider such additional regularization
due to space limitations, but remark that they could be in-
tegrated by adding further constraints to the following dual
representation, which is a consequence of [18, Vol. II, Sec.
1.3.1, Thm. 2].

Proposition 2. For T ∈ Mn(U ) with spt T ⊂ X × Y , we
have the dual representation

E(T ) = sup
ω∈K

T (ω),

where the constraint is the closed and convex set

K =nω ∈ C 0

c (U, ΛnRn+N ) :

Ψ∗(π1z, π2z, ω(z)) ≤ 0, ∀z ∈ X × Yo.

The ﬁnal relaxed optimization problem for (1) reads

(30)

(31)

inf

T ∈Mn(U )

E(T ), s.t. spt T ⊂ X × Y, T ∈ C.

(32)

Depending on the kind of problem one wishes to solve, a
different convex constraint set C should be considered. For
example, in the case of variational problems with Dirichlet
boundary conditions, we set

(33)

C =(cid:8)T : π1♯T = JXK, ∂T = S(cid:9),

where S ∈ Mn−1(U ) is a given boundary datum. In case of
Neumann boundary conditions, one constrains the support
of the boundary to be zero inside the domain

C =(cid:8)T : π1♯T = JXK, spt ∂T ⊂ (∂X) × Y(cid:9),

to exclude surfaces with holes, but allow the boundary to be
freely chosen on (∂X) × Y . In case n = N , one can also
consider the constraint set

(34)

C =(cid:8)T : π1♯T = JXK, π2♯T = JY K,
spt ∂T ⊂ ∂(X × Y )(cid:9),

where the additional pushforward constraint encourages bi-
jectivity. Notice also the similarity of (32) together with
(35) to the Kantorovich relaxation in optimal transport.

Existence of minimizing currents to a similar problem as
(32) in a certain space of currents (real ﬂat chains) is shown
in [16, §3.9]. For dimension n = 1 or codimension N = 1,
the inﬁmum is actually realized by a surface (integral ﬂat
chain) [16, §5.10, §5.12]. An adaptation of such theoretical
considerations to our setting and conditions under which the
relaxation is tight in the scenario n > 1, N > 1 is a major
open challenge and left for future work.

5. Discrete Formulation

In this section we present an implementation of the con-
tinuous model (32) using discrete exterior calculus [22]. We
will base our discretization on cubes since they are easy to
work with in high dimensions, but one could also use sim-
plices. To deﬁne cubical meshes, we adopt some notations
from computational homology [25].

Deﬁnition 3 (Elementary interval and cube). An elemen-
tary interval is an interval I ⊂ R of the form I = [l, l + 1]
or I = {l} for l ∈ Z. Intervals that consist of a single point
are degenerate. An elementary cube is given by a product
κ = I1 × . . . × Id, where each Ii is an elementary interval.
The set of elementary cubes in Rd is denoted by K d.

For κ ∈ K d, denote by dim κ ∈ {1, . . . , d} the number
of nondegenerate intervals. We denote i(κ) ∈ I(d, dim κ)
as the multi-index referencing the nondegenerate intervals.

Deﬁnition 4 (Cubical set). A set Q ⊂ Rd is a cubical set if
it can be written as a ﬁnite union of elementary cubes.

Let K d

k (Q) = {κ ∈ K d : κ ⊂ Q, dim κ = k} be
the set of k-dimensional cubes contained in Q ⊂ Rd. A
map φ : Q → X × Y will transform the cubical set to our
domain. As we work with images, it will just be a mesh
spacing, i.e., we set φ(z) = (h1z1, . . . , hdzd).

Deﬁnition 5 (k-chains, k-cochains). We denote the space
of ﬁnite formal sums of elements in K d
k (Q) with real coefﬁ-
cients as Ck(Q), called (real) k-chains. We denote the dual
as Ck(Q)∗ = Ck(Q) and call the elements k-cochains.

Deﬁnition 6 (Boundary). For κ ∈ K d
k (Q), denote the pri-
mary faces obtained by collapsing the j-th non-degenerate
to the lower respectively upper boundary as
interval
κ−
j , κ+
k−1. The boundary of an elementary cube
κ ∈ K d

j ∈ K d
k (Q) is the (k − 1)-chain,

∂κ =

kXj=1

(−1)j−1(κ+

j − κ−

j ) ∈ Ck−1(Q).

(36)

(35)

The boundary operator is given by the extension to a linear
map ∂ : Ck(Q) → Ck−1(Q).

11122

Y

Input

Finite differences

Discrete exterior calculus

X

Figure 3: Minimization of the Brachistochrone energy on a
25 × 14 cubical set (gray squares). The proposed discretiza-
tion yields a diffuse current (black vector ﬁeld), whose cen-
ter of mass (black, dashed) however is faithful to the analyt-
ical cycloid solution (orange) far beyond the mesh accuracy.

A k-chain T =Pκ Tκκ ∈ Ck(Q) can be identiﬁed with
a k-current T ′ ∈ Dk(U ) by T ′ =Pκ Tκφ♯JκK. The above

discrete notion of boundary is deﬁned in analogy to the con-
tinuous deﬁnition (24).

In our discretization, we will use the dual representation
of the lifted energy from Prop. 2. To implement differential
forms, we introduce an interpolation operator.

Deﬁnition 7 (Whitney map). The Whitney map extends a
k-cochain ω to a k-form (Wω) : X × Y → ΛkRd,

where ωκ ∈ R are the coefﬁcients of the k-cochain,

(Wω)(x) = Xκ∈K d
cW(x, κ) = dxi(κ) Yi∈i(κ)

ωκcW(φ−1(x), κ),

k (Q)

max{0, 1 − |xi − Ii(κ)|},

(37)

(38)

and Ii(κ) ∈ Z is the element in the degenerate interval.

Interestingly, the Whitney map (for simplicial meshes)
ﬁrst appeared in [66, Eq. 27.12] but specializes to lowest-
order Raviart-Thomas [49] (k = 2,d = 3) and Nédélec [45]
elements (for k = 1, d = 3), see [3, 4]. Differential forms
of type (37) are called Whitney forms.

We also deﬁne a weighted inner product h·, ·iφ between
chains and cochains by plugging the Whitney form asso-
ciated to the k-cochain into the current corresponding to
the k-chain. As both are constant on each k-cube, a quick

calculation shows: hT, ωiφ = Pκ TκωκHk(φ(κ)), where

Hk(φ(κ)) is simply the volume of the k-cube under the
mesh spacing φ.

Using the dual representation (30), and approximating
the current by a k-chain and the differential forms with

Finite differences

Discrete exterior calculus

Figure 4: Total variation minimization. Top: The proposed
DEC discretization yields solutions with better isotropy and
sharper discontinuities. Bottom: In that stereo matching
example, we enforce the continuous constraints Wω ∈ K
between the discretization points (here 8 labels), which
leads to more precise (sublabel-accurate) solutions com-
pared to the naive ﬁnite-difference approach.

k-cochains we arrive at the following ﬁnite-dimensional
convex-concave saddle-point problem on Q ⊂ Rn+N :

min

T ∈Cn(Q)

max

ω∈Cn(Q)

ϕ∈Cn−1(Q)

hT, ωiφ + h∂T − S, ϕiφ,

subject to
potentially π2♯T = 1 in case n = N.

π1♯T = 1, Wω ∈ K,

(39)

S ∈ Cn−1(Q) is a given boundary datum, for free boundary
conditions we replace the inner product hS, ϕi with an indi-
cator function S : Cn−1 → R forcing ϕ to be zero on the
boundary. The pushforwards π1♯, π2♯ are linear constraints
on the coefﬁcients of the k-chain T .

6. Numerical Implementation

In practice we solve (39) with the ﬁrst order primal-dual
algorithm [8]. For the local constraints Wω ∈ K usu-
ally no closed form projection exists.
In some situations
(N = 1) they can be implemented exactly, see [41, 40].
In the general setting, we resort to implementing them at
sampled points. To enforce the constraint Wω ∈ K at
samples Z = {z1, z2, . . .} ⊂ X × Y we add another pri-
mal variable λ : Z → ΛnRn+N and the additional term

Pz∈Z Ψ∗∗(z, λ(z)) − hλ(z), (Wω)(z)i to the saddle-point

Finally, one requires the proximal operator of the per-
spective function Ψ∗∗. These can be implemented using
epigraphical projections as in [48]. For an overview over
proximal operators of perspective functions we refer to [12].

formulation (39).

11123

X

Y

id −f

id −f −1

left-to-right slice of kT k

right-to-left slice of kT k

Figure 5: Global registration of X and Y . Top: Our method
yields dense pointwise correspondences that are smooth in
both directions and correspond to the correct transforma-
tion. Bottom: 2-D slices through the 4-D density kT k at
the single black pixel. We empirically verify (also at the
other points) that the current concentrated near a surface,
therefore the recovered solution is near the global minimum
of the original nonconvex problem.

6.1. Properties of the Discretization

c(x, y, ξ) =q 1+ξ2

As a ﬁrst example we solve the Brachistochrone [6], ar-
guably the ﬁrst variational approach. The cost is given by
2gy where g > 0 is the gravitational con-
stant. Dirichlet boundary conditions are enforced using the
boundary operator. In Fig. 3 we show the resulting current,
which concentrates on the graph of the closed-form solution
to the problem, which is a cycloid. The unlifted result is ob-
tained by taking the center of mass of the ﬁrst component
T ¯0,0 of the current by summing over the horizontal edges
in the 1-chain. The obtained result nearly coincides with
the exact cycloid. Instead, solutions from MRF approaches
would invariably be conﬁned to the vertices or edges of the
rather coarse grid.

In Fig. 4 we solve total variation regularized problems
which corresponds to setting c(x, y, ξ) = ρ(x, y) + |ξ| for
some data ρ. The data is either a quadratic or a stereo match-
ing cost in that example. The proposed approach based on
discrete exterior calculus has better isotropy and leads to
sharper discontinuities than the common forward difference
approach used in literature. Furthermore, by enforcing the
constraints Wω ∈ K also between the discretization points
one can achieve “sublabel-accurate” results as demonstrated
in the stereo matching example.

6.2. Global Registration

As an example of n > 1, N > 1 with polycon-
vex regularization, we tackle the problem of orientation
preserving diffeomorphic registration between two shapes

X, Y ⊂ R2 with boundary. We use the cost c(x, y, ξ) =

(ρ(x, y) + ε)pdet (I + ξ⊤ξ), which penalizes the surface

area in the product space and favors local isometry. The
parameter ε > 0 models the trade-off between data and
smoothness. In the example considered in Fig. 5 the data
is given by ρ(x, y) = kI1(x) − I2(y)k, where I1, I2 are the
shown color images. A polyconvex extension of the above
cost, which is large for non-simple vectors is given by the
(weighted) mass norm (4). The 4-D cubical set Q is the
product space between the two shapes X and Y , which are
given as quads (pixels). We impose the constraints Wω ∈ K
at the 16 vertices of each four dimensional hypercube. The
proximal operator of the mass norm is computed as in [67].
Note that the required 4 × 4 real Schur decomposition can
be reduced to a 2 × 2 SVD using a few Givens rotations,
see [65]. We further impose T ¯0,0 ≥ 0 and T 0,¯0 ≥ 0, and
boundary conditions ensure that ∂X is matched to ∂Y . Bi-
jectivity of the matching is encouraged by the pushforward
constraints π1♯T = 1, π2♯T = 1. After solving (39) we ob-
tain the ﬁnal pointwise correspondences f : X → Y from
the 2-chain T ∈ C2(Q) by taking its center of mass.

In Fig. 5 we visualize f (x) = Py y |(WT )(x, y)|,
f −1(y) = Px x |(WT )(x, y)|. As one can see, the maps

f and f −1 are smooth and inverse to each other. Despite
n > 1, N > 1, the current apparently concentrated near
a surface and the computed solutions are therefore near the
global optimum of the original nonconvex problem.

7. Discussion and Limitations

In this work, we introduced a novel approach to vectorial
variational problems based on geometric measure theory,
along with a natural discretization using concepts from dis-
crete exterior calculus. Though observed in practice, we do
not have theoretical guarantees that the minimizing current
will concentrate on a surface. In case of multiple global so-
lutions, one might get a convex combination of minimizers.
Some mechanism to select an extreme point of the convex
solution set would therefore be desirable. The main draw-
back over MRFs, for which very efﬁcient solvers exist [27],
is that we had to resort to the generic algorithm [8] with
O(1/ε) convergence. Yet, solutions with high numerical
accuracy are typically not required in practice and the algo-
rithm parallelizes well on GPUs. To conclude, we believe
that the present work is a step towards making continuous
approaches an attractive alternative to MRFs, especially in
scenarios where faithfulness to certain geometric properties
of the underlying continuous model is desirable.

Acknowledgements. The work was partially supported
by the German Research Foundation (DFG); project
394737018 “Functional Lifting 2.0 – Efﬁcient Convexiﬁca-
tions for Imaging and Vision”.

11124

References

[1] G. Alberti, G. Bouchitté, and G. Dal Maso. The cali-
bration method for the Mumford-Shah functional and free-
discontinuity problems. Calc. Var. Partial Differential Equa-
tions, 16(3):299–333, 2003. 2

[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gen-
erative adversarial networks. In International Conference on
Machine Learning (ICML), pages 214–223, 2017. 1

[3] D. N. Arnold and G. Awanou. Finite element differential
forms on cubical meshes. Mathematics of Computation,
83(288):1551–1570, 2014. 7

[4] D. N. Arnold, R. S. Falk, and R. Winther. Finite element
exterior calculus, homological techniques, and applications.
Acta numerica, 15:1–155, 2006. 7

[5] P. Aviles and Y. Giga. Variational integrals on mappings of
bounded variation and their lower semicontinuity. Arch. Ra-
tion. Mech. Anal., 115(3):201–255, 1991. 2

[6] J. Bernoulli. Problema novum ad cujus solutionem mathe-

matici invitantur. Acta Eruditorum, 18(269), 1696. 8

[7] H. Busemann, G. Ewald, and G. C. Shephard. Convex bodies
and convexity on Grassmann cones. Math. Ann., 151(1):1–
41, 1963. 3

[8] A. Chambolle and T. Pock. A ﬁrst-order primal-dual algo-
rithm for convex problems with applications to imaging. J.
Math. Imaging Vis., 40:120–145, 2011. 7, 8

[9] Q. Chen and V. Koltun. Robust nonrigid registration by con-
vex optimization. In International Conference on Computer
Vision (ICCV), 2015. 1

[10] Q. Chen and V. Koltun. Full ﬂow: Optical ﬂow estimation
by global optimization over regular grids. In Conference on
Computer Vision and Pattern Recognition (CVPR), 2016. 1
[11] L. D. Cohen and R. Kimmel. Global minimum for active
International

contour models: A minimal path approach.
Journal of Computer Vision, 24(1):57–78, 1997. 1

[12] P. L. Combettes and C. L. Müller. Perspective functions:
Proximal calculus and applications in high-dimensional
statistics. J. Math. Anal. Appl., 457(2):1283–1306, 2018. 7

[13] K. Crane. Discrete differential geometry: An applied intro-

duction, 2019. 2

[14] C. Domokos, F. R. Schmidt, and D. Cremers. MRF optimiza-
tion with separable convex prior on partially ordered labels.
In European Conference on Computer Vision (ECCV), 2018.
1

[15] H. Federer. Geometric Measure Theory. Springer, 1969. 2
[16] H. Federer. Real ﬂat chains, cochains and variational prob-

lems. Indiana Univ. Math. J., 24(4):351–407, 1974. 2, 6

[17] A. Fix and S. Agarwal. Duality and the continuous graph-
In European Conference on Computer Vision

ical model.
(ECCV), 2014. 1

[18] M. Giaquinta, G. Modica, and J. Souˇcek. Cartesian currents
in the calculus of variations I, II., volume 37-38 of Ergeb-
nisse der Mathematik und ihrer Grenzgebiete. 3. Springer,
1998. 4, 6

[19] B. Goldluecke, E. Strekalovskiy, and D. Cremers. Tight con-
vex relaxations for vector-valued labeling. SIAM J. Imaging
Sciences, 6(3):1626–1664, 2013. 2

[20] T. Goldstein, X. Bresson, and S. Osher. Global minimization
of Markov random ﬁelds with applications to optical ﬂow.
Inverse Problems & Imaging, 6(4):623–644, 2012. 2

[21] D. Hilbert. Mathematische Probleme. Nachrichten von der
Königl. Gesellschaft der Wiss. zu Göttingen, pages 253–297,
1900. 1

[22] A. N. Hirani. Discrete exterior calculus. PhD thesis, Cali-

fornia Institute of Technology, 2003. 6

[23] J.-B. Hiriart-Urruty and C. Lemaréchal. Fundamentals of

convex analysis. Springer, 2012. 3

[24] H. Ishikawa. Exact optimization for Markov random ﬁelds
with convex priors. IEEE Trans. Pattern Anal. Mach. Intell.,
25(10):1333–1336, 2003. 1

[25] T. Kaczynski, K. Mischaikow, and M. Mrozek. Computa-

tional Homology. Springer, 2006. 6

[26] L. V. Kantorovich. Mathematical methods of organizing and
planning production. Management Science, 6(4):366–422,
1960. 1

[27] J. Kappes, B. Andres, F. Hamprecht, C. Schnorr, S. Nowozin,
D. Batra, S. Kim, B. Kausler, J. Lellmann, N. Komodakis,
et al. A comparative study of modern inference techniques
for discrete energy minimization problems.
In Conference
on Computer Vision and Pattern Recognition (CVPR), pages
1328–1335, 2013. 8

[28] R. Kimmel, R. Malladi, and N. Sochen. Images as embedded
maps and minimal surfaces: Movies, color, texture, and vol-
umetric medical images. International Journal of Computer
Vision 39(2), 2000. 1

[29] P. Kohli, A. Shekhovtsov, C. Rother, V. Kolmogorov, and
P. Torr. On partial optimality in multi-label mrfs.
In In-
ternational Conference on Machine learning (ICML), pages
480–487, 2008. 1

[30] V. Kolmogorov. Convergent tree-reweighted message pass-
ing for energy minimization. IEEE Transactions on Pattern
Analysis and Machine Intelligence (TPAMI), 28(10):1568–
1583, 2006. 1

[31] V. Kolmogorov and C. Rother. Minimizing nonsubmodular
functions with graph cuts - a review. IEEE Transactions on
Pattern Analysis and Machine Intelligence (TPAMI), 29(7),
2007. 1

[32] S. G. Krantz and H. R. Parks. Geometric Integration Theory.

Birkhäuser Boston, 2008. 2, 4

[33] E. Laude, T. Möllenhoff, M. Moeller, J. Lellmann, and
D. Cremers. Sublabel-accurate convex relaxation of vectorial
multilabel energies. In European Conference on Computer
Vision (ECCV), 2016. 2

[34] J. Lellmann, E. Strekalovskiy, S. Koetter, and D. Cremers.
Total variation regularization for functions with values in a
manifold. In International Conference on Computer Vision
(ICCV), 2013. 2

[35] M. Li, A. Shekhovtsov, and D. Huber. Complexity of discrete
energy minimization problems. In European Conference on
Computer Vision (ECCV), 2016. 1

[36] B. Loewenhauser and J. Lellmann. Functional lifting for
variational problems with higher-order regularization. Imag-
ing, Vision and Learning Based on Optimization and PDEs,
pages 101–120, 2018. 2

11125

[37] F. Mémoli. On the use of Gromov-Hausdorff distances for
In Eurographics Symposium on Point-

shape comparison.
Based Graphics. The Eurographics Association, 2007. 2

[38] F. Mémoli.

Spectral Gromov-Wasserstein distances for
shape matching. In International Conference on Computer
Vision Workshops (ICCV Workshops), 2009. 2

[39] M. Menze, C. Heipke, and A. Geiger. Discrete optimization
for optical ﬂow. In German Conference on Pattern Recogni-
tion (GCPR), 2015. 1

[40] T. Möllenhoff and D. Cremers. Sublabel-accurate discretiza-
tion of nonconvex free-discontinuity problems. In Interna-
tional Conference on Computer Vision (ICCV), 2017. 1, 2,
7

[41] T. Möllenhoff, E. Laude, M. Moeller, J. Lellmann, and
Sublabel-accurate relaxation of nonconvex
In Conference on Computer Vision and Pattern

D. Cremers.
energies.
Recognition (CVPR), 2016. 2, 7

[42] G. Monge. Mémoire sur la théorie des déblais et des rem-
blais. Histoire de l’Académie Royale des Sciences de Paris,
1781. 1

[43] M. G. Mora. The calibration method for free-discontinuity
problems on vector-valued maps. J. Convex Anal., 9(1):1–29,
2002. 6

[44] F. Morgan. Geometric Measure Theory: A Beginner’s Guide.

Academic Press, 5th edition, 2016. 2

[45] J.-C. Nédélec. Mixed ﬁnite elements in R3. Numerische

Mathematik, 35(3):315–341, 1980. 7

[46] M. Ovsjanikov, M. Ben-Chen, J. Solomon, A. Butscher,
and L. Guibas. Functional maps: a ﬂexible representation
of maps between shapes. ACM Transactions on Graphics
(TOG), 31(4):30, 2012. 2

[55] A. Shekhovtsov, I. Kovtun, and V. Hlaváˇc. Efﬁcient MRF
deformation model for non-rigid image matching. Com-
puter Vision and Image Understanding (CVIU), 112(1):91–
99, 2008. 1

[56] J. Solomon, G. Peyré, V. G. Kim, and S. Sra. Entropic metric
alignment for correspondence problems. ACM Transactions
on Graphics (TOG), 35(4):72, 2016. 2

[57] M. Strecke and B. Goldluecke. Sublabel-accurate convex
relaxation with total generalized variation regularization. In
German Conference on Pattern Recognition (GCPR), 2018.
2

[58] E. Strekalovskiy, A. Chambolle, and D. Cremers. A convex
representation for the vectorial Mumford-Shah functional.
In Conference on Computer Vision and Pattern Recognition
(CVPR), 2012. 2

[59] E. Strekalovskiy, A. Chambolle, and D. Cremers. Convex
relaxation of vectorial problems with coupled regularization.
SIAM J. Imaging Sci., 7(1):294–336, 2014. 2

[60] J. N. Tsitsiklis.

Efﬁcient algorithms for globally opti-
mal trajectories. IEEE Transactions on Automatic Control,
40(9):1528–1538, 1995. 1

[61] M. Vestner, Z. Lähner, A. Boyarski, O. Litany, R. Slossberg,
T. Remez, E. Rodola, A. Bronstein, M. Bronstein, R. Kim-
mel, and D. Cremers. Efﬁcient deformable shape correspon-
dence via kernel matching. In International Conference on
3D Vision (3DV), 2017. 2

[62] M. Vestner, R. Litman, E. Rodolà, A. M. Bronstein, and
D. Cremers. Product manifold ﬁlter: Non-rigid shape corre-
spondence via kernel density estimation in the product space.
In Conference on Computer Vision and Pattern Recognition
(CVPR), pages 6681–6690, 2017. 2

[63] C. Villani. Optimal Transport: Old and New. Springer, 2008.

[47] G. Peyré and M. Cuturi. Computational optimal transport.

2

arXiv:1803.00567, 2018. 2

[48] T. Pock, D. Cremers, H. Bischof, and A. Chambolle. Global
solutions of variational models with convex regularization.
SIAM J. Imaging Sci., 3(4):1122–1145, 2010. 2, 7

[49] P.-A. Raviart and J.-M. Thomas. A mixed ﬁnite element
method for 2nd order elliptic problems.
In Mathematical
aspects of ﬁnite element methods, pages 292–315. Springer,
1977. 7

[50] R. T. Rockafellar. Convex Analysis. Princeton University

Press, 1996. 3

[51] E. Rodolà, Z. Lähner, A. M. Bronstein, M. M. Bronstein,
and J. Solomon. Functional maps representation on product
manifolds. In Computer Graphics Forum, volume 38, pages
678–689, 2019. 2

[52] F. Santambrogio. Optimal Transport for Applied Mathemati-

cians. Birkhäuser, New York, 2015. 2

[53] T. Schoenemann and D. Cremers. A combinatorial solution
for model-based image segmentation and real-time tracking.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence (TPAMI), 32(7):1153–1164, 2010. 1

[54] V. Seguy, B. B. Damodaran, R. Flamary, N. Courty, A. Rolet,
and M. Blondel. Large-scale optimal transport and mapping
estimation. In International Conference on Learning Repre-
sentations (ICLR), 2018. 1

[64] T. Vogt and J. Lellmann.

liftings of vec-
torial variational problems with Laplacian regularization.
arXiv:1904.00898, 2019. 2

Functional

[65] R. C. Ward and L. J. Gray. Eigensystem computation for
skew-symmetric matrices and a class of symmetric matrices.
Technical report, Oak Ridge National Lab, 1976. 8

[66] H. Whitney. Geometric Integration Theory. Princeton Uni-

versity Press, 1957. 7

[67] T. Windheuser and D. Cremers. A convex solution to
spatially-regularized correspondence problems. In European
Conference on Computer Vision (ECCV), 2016. 2, 8

[68] T. Windheuser, U. Schlickewei, F. R. Schmidt, and D. Cre-
mers. Geometrically consistent elastic matching of 3D
shapes: A linear programming solution.
In International
Conference on Computer Vision (ICCV), 2011. 2

[69] L. C. Young. Lectures on the Calculus of Variations and
Optimal Control Theory. Chelsea Publishing Company, New
York, second edition, 1980. 1

[70] C. Zach, C. Haene, and M. Pollefeys. What is optimized
in tight convex relaxations for multi-label problems?
In
Conference on Computer Vision and Pattern Recognition
(CVPR), 2012. 1

[71] C. Zach and P. Kohli. A convex discrete-continuous ap-
proach for Markov random ﬁelds. In European Conference
on Computer Vision (ECCV), 2014. 1

11126

