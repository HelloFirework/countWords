LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks

Sudhakar Kumawat and Shanmuganathan Raman

Indian Institute of Technology Gandhinagar

Gandhinagar, Gujarat, India

{sudhakar.kumawat, shanmuga}@iitgn.ac.in

Abstract

Traditional 3D Convolutional Neural Networks (CNNs)
are computationally expensive, memory intensive, prone to
overﬁt, and most importantly, there is a need to improve
their feature learning capabilities. To address these issues,
we propose Rectiﬁed Local Phase Volume (ReLPV) block,
an efﬁcient alternative to the standard 3D convolutional
layer. The ReLPV block extracts the phase in a 3D local
neighborhood (e.g., 3 × 3 × 3) of each position of the in-
put map to obtain the feature maps. The phase is extracted
by computing 3D Short Term Fourier Transform (STFT) at
multiple ﬁxed low frequency points in the 3D local neigh-
borhood of each position. These feature maps at different
frequency points are then linearly combined after passing
them through an activation function. The ReLPV block pro-
vides signiﬁcant parameter savings of at least, 33 to 133
times compared to the standard 3D convolutional layer with
the ﬁlter sizes 3 × 3 × 3 to 13 × 13 × 13, respectively. We
show that the feature learning capabilities of the ReLPV
block are signiﬁcantly better than the standard 3D convo-
lutional layer. Furthermore, it produces consistently bet-
ter results across different 3D data representations. We
achieve state-of-the-art accuracy on the volumetric Mod-
elNet10 and ModelNet40 datasets while utilizing only 11%
parameters of the current state-of-the-art. We also improve
the state-of-the-art on the UCF-101 split-1 action recog-
nition dataset by 5.68% (when trained from scratch) while
using only 15% of the parameters of the state-of-the-art.

1. Introduction

Over the past few years, research in the area of 2D CNNs
has led to unprecedented advances in a number of computer
vision tasks such as image classiﬁcation, semantic segmen-
tation, and image super-resolution. Apart from performance
results, 2D CNNs have also made good progress in other
complementary areas such as network compression, bina-
rization, quantization, regularization, etc. Unfortunately,
unlike their 2D counterparts, 3D CNNs have not enjoyed

the same level of performance jumps on the problems in
their domain e.g., video classiﬁcation and progress in the
above mentioned complementary areas. Recent works such
as [44] and [11], list down some of the fundamental barri-
ers in modeling and training of deep 3D CNNs such as (1)
they are computationally very expensive, (2) they result in
large model size, both in terms of memory usage and disk
space, (3) they are prone to overﬁtting, due to a large num-
ber of parameters, (4) and there is a need to improve their
feature learning capabilities which may require fundamen-
tal changes to their network architecture or the standard 3D
convolutional layer [44, 25, 39]. Despite the above chal-
lenges, the current trend in the literature of deep 3D CNNs
is to train computationally expensive, memory intensive,
and very deep networks in order to achieve state-of-the-art
results [2, 8, 11].

In this work, we take a detour from this trend by propos-
ing an alternative to the fundamental building block of the
3D CNNs, the 3D convolutional layer, which is the pri-
mary source of high space-time complexity in 3D CNNs.
More precisely, we propose Rectiﬁed Local Phase Volume
(ReLPV) block, an efﬁcient alternative to the standard 3D
convolutional layer in 3D CNNs. The ReLPV block com-
prises of a local phase module, the ReLU activation function
and a set of trainable linear weights. The local phase mod-
ule extracts the local phase information by computing 3D
Short Term Fourier Transform (STFT) [15] (at multiple low
frequency points) in a local n×n×n (e.g., 3×3×3) neigh-
borhood/volume of each position of the input feature map.
The output of the local phase module is then passed through
the ReLU activation function in order to obtain the acti-
vated response maps of the local phase information at the
ﬁxed low frequency points. Finally, a set of trainable linear
weights computes the weighted combinations of these ac-
tivated response maps. The ReLPV block provides signiﬁ-
cant parameter savings along with computational and mem-
ory savings. The ReLPV block based 3D CNNs have much
lower model complexity and are less prone to overﬁtting.
Most importantly, its feature learning capabilities are sig-
niﬁcantly better than the standard 3D convolutional layer.

4903

Our major contributions in this work are as follows.

• We propose ReLPV block, an efﬁcient alternative to
the standard 3D convolutional layer. The ReLPV block
signiﬁcantly reduces the number of trainable parame-
ters, at least 33 to 133 times compared to the standard
3D convolutional layer with the ﬁlter sizes 3 × 3 × 3
to 13 × 13 × 13, respectively.

• We show that the ReLPV block achieves consistently
better results on different 3D data representations. We
show this on the volumetric ModelNet10 and Model-
Net40 datasets by achieving state-of-the-art accuracy
using just 11% parameters of the current state-of-the-
art. Moreover, we provide results on the spatiotem-
poral image sequences. In particular, on the UCF-101
split-1 action recognition dataset, improving the cur-
rent state-of-the-art by 5.68% while using just 15% pa-
rameters of the state-of-the-art.

• We present detailed ablation and performance studies
of the proposed ReLPV block by varying its various
hyperparameters. The analysis will be beneﬁcial for
designing ReLPV block based 3D CNNs in future.

2. Related Work

Recently, 2D CNNs have achieved state-of-the-art re-
sults in most of the computer vision problems [9]. More-
over, they have also made signiﬁcant progress in other com-
plementary areas such as network compression [16, 48], bi-
narization [7, 6, 31, 21], quantization [50, 18], regulariza-
tion [5, 17, 45, 32], etc. Therefore, not surprisingly, there
have been many recent attempts to extend this success to
the problems in the domain of 3D CNNs e.g., video classi-
ﬁcation [1], 3D object recognition [26, 2] and MRI volume
segmentation [27, 3]. Unfortunately, 3D CNNs are com-
putationally expensive and require large memory and disk
space. Furthermore, they overﬁt very easily owing to the
large number of parameters involved. Therefore, there has
been recent interest in more efﬁcient variants of 3D CNNs.
Inspired from the progress of network binarization tech-
niques in 2D CNNs such as BinaryConnect [7], BinaryNet
[6], and XNORNet [31], Ma et al. in [25] introduced BV-
CNNs, where they fully binarized some of the state-of-the-
art 3D CNN models introduced for recognizing voxelized
3D CAD models from the ModelNet datasets [42]. The
binarized version of the 3D CNNs saves signiﬁcant com-
putation and memory requirements when compared to the
ﬂoating point baselines. However, this comes at the cost of
reduced performance. Furthermore, the binarized network
takes binarized inputs only which restricts its application for
other 3D data representations such as video classiﬁcation.

Another way to reduce the model complexity of 3D
CNNs is to replace the 3D convolutions with separable con-

volutions. This technique has been explored recently in a
number of 3D CNN architectures proposed for the task of
video classiﬁcation. The idea of separable convolutions is to
ﬁrst convolve spatially in 2D and then convolve temporally
in 1D. This factorization is similar in spirit to the depth-
wise separable convolutions used in [43], except that here
the idea is to apply it to the temporal dimension instead of
the feature dimension. The idea has been used in a variety of
recent works, including R(2+1)D networks[39], separable-
3D CNNs [44], Pseudo-3D networks [30], and factorized
spatio-temporal CNNs [36]. The 3D CNNs based on the
idea of separable convolutions achieve competitive results
compared to the state-of-the-art on the task of video classi-
ﬁcation at a reduced space-time complexity.

3. Method

Notation. We denote the feature map output by a layer in

a 3D CNN network with the tensor I ∈ Rc×d×h×w where
h, w, d, and c are the height, width, depth, and number of
channels of the feature map, respectively.
The ReLPV Block Architecture. The ReLPV block is a
four-layer alternative representation of the standard 3D con-
volutional layer. Fig. 1 illustrates the architecture of the
ReLPV block.
Layer 1. This layer is the standard 3D convolutional layer
with a single ﬁlter of size 1 × 1 × 1. It takes a feature map
of size c × d × h × w as input from the previous layer,
and converts it into a single channel feature map of size
1 × d × h × w. This layer prepares the input for the 3D
STFT operation which is computed in Layer 2. Let f (x) be
the feature map output of Layer 1 with size 1 × d × h × w.
Here, x is a variable denoting positions on the feature map
f (x).
Layer 2. Local phase has been successfully used in im-
ages to detect edges and contours for feature extraction [23].
Phase represents the local coherence of different spatial fre-
quencies. Edges and skeletons in image are expressed by
their coherence and play a signiﬁcant role in image under-
standing [47]. Same property holds true for 3D data rep-
resentations too. e.g., videos [29]. There are many meth-
ods for extracting local phase in multiple dimensions [14].
Our method is inspired from [29]. Layer 2 extracts the lo-
cal phase spectra of f (x) by computing the 3D Short Term
Fourier Transform (STFT) in a local n×n×n neighborhood
Nx at each position x of f (x) using Equation 1.
f (x − y) exp−j2πvT y

F (v, x) = X
y∈Nx

(1)

Here, v ∈ R3 is a frequency variable and j = √−1. Using

vector notation [20], we can rewrite Equation 1 as shown in
Equation 2.

F (v, x) = wT

v fx

(2)

4904

c x d x h x w 

1 x d x h x w 

26 x d x h x w 

26 x d x h x w 

f x d x h x w 

previous
 

layer
 

 conv3D (1x1x1,1)

3D STFT (k = 1/n) &
local phase extraction

Layer 1

Layer 2

ReLU 

 

Layer 3

 conv3D (1x1x1,f)

next
layer

Layer 4

Figure 1: The ReLPV block architecture.

Here, wv is the basis vector of the 3D STFT at frequency
variable v and fx is a vector containing all the positions from
the neighborhood Nx. Note that, due to the separability of
the basis functions, 3D STFT can be computed efﬁciently
for all the positions x in f (x) by using simple 1D convo-
lutions for each dimension. In this work, we consider 13
lowest non-zero frequency variables which are deﬁned as
below. The selected frequency variables are shown as red

v1 = [k, 0, 0]T ,
v4 = [0, k, 0]T ,
v7 = [k, k, 0]T ,
v10 = [k, −k, 0]T ,
v13 = [0, 0, k]T ,

draw.io

v2 = [k, 0, k]T ,
v5 = [0, k, k]T ,
v8 = [k, k, k]T ,
v11 = [k, −k, k]T ,
where k = 1/n

v3 = [k, 0, −k]T ,
v6 = [0, k, −k]T ,
v9 = [k, k, −k]T ,
v12 = [k, −k, −k]T ,

V2

V1

V3

1/n

1/n

1/n

Figure 2: Frequency points used to compute the 3D
STFT. The selected frequency points are marked as red
dots. The other frequency points in the green dots are ig-
nored, as they are the complex conjugates of the selected
ones.

dots in Fig. 2. Low frequency variables are used because
they usually contain most of the information, and there-
fore they have better signal-to-noise ratio than the high fre-
quency components [14]. Let

W = [ℜ{wv1 , wv2 , . . . , wv13},ℑ{wv1 , wv2 , . . . , wv13}]T

(3)

Here, W is a 26 × n3 transformation matrix corresponding
to the 13 frequency variables. ℜ{·} and ℑ{·} return the real
and the imaginary parts of a complex number, respectively.
Hence, from Equation 2 & 3, the vector form of 3D STFT
for all the 13 frequency points v1, v2, . . . ,v13 can be written
as shown in Equation 4.

Fx = Wfx

(4)

Since, Fx is computed for all positions x of the input f (x),
it results in an output feature map with size 26 × d × h ×
w. A more detailed mathematical formulation of Layer 2 is
provided in the supplementary document.
Layer 3. Applying non-linearity to the local phase informa-
tion enables the network to learn complex representations.
This layer creates activated response maps of the feature
maps obtained from Layer 2 by using an activation func-
tion. We use the ReLU activation function for better efﬁ-
ciency and faster convergence [28].
Layer 4. This layer is the standard 3D convolutional layer
with f ﬁlters each of size 1 × 1 × 1 which takes a feature
map of size 26×d×h×w as input from Layer 3 and outputs
a feature map of size f × d× h× w. Note that, Layer 1 and
4 get learned during the training phase of the 3D CNN.
We shall use the notation ReLPV(n, f ) for the ReLPV
block, where n and f are its hyperparameters. Here n de-
notes the size of the local 3D neighborhood from Layer 2
and f is the number of 1 × 1 × 1 ﬁlters used in Layer 4.
Importance of using STFT and Local Phase. STFT in
multidimensional space was ﬁrst studied by Hinman et al.
in [15] as an efﬁcient tool for image encoding. It has two
important properties which make it useful for our purpose:
(1) Natural images are often composed of objects with sharp
edge features. It has been observed that the Fourier phase
information accurately represents these edge features. Since
STFT in 3D space is simply a windowed Fourier transform,
the same property applies [15]. Thus, the local phase has the
ability to accurately capture the local features in the same
way as done by the convolutional ﬁlters. (2) STFT decor-
relates the input signal [15]. Regularization is key for deep
learning since it allows training of more complex models
while keeping lower levels of overﬁtting and achieves better
generalization. Decorrelation of features, representations,

chrome-extension://pebppomjfocnoigkeepgbmcifnnlndla/index.html

1/1

4905

and hidden activations has been an active area of research
for better regularization of deep neural nets, with a variety
of novel regularizers proposed such as DeCov [5], Decorre-
lated Batch Normalization (DBN) [17], Structured Decorre-
lation Constraint (SDC) [45] and OrthoReg [32]. As STFT
decorrelates the input representations and due to the re-
duced number of learnable parameters, the ReLPV block
based 3D CNNs are less prone to overﬁtting and generalize
better (for results see Section 5.2).
Forward-Backward Propagation in the ReLPV Block.
The end-to-end training of a 3D CNN network with the
ReLPV blocks instead of the standard 3D convolutional lay-
ers is straightforward. The steps of forward and backward
propagation through the Layers 1, 3 and 4 of the ReLPV
block are standard operations in all deep learning libraries.
Back propagation in the Layer 2 is similar to propagating
gradients through layers without learnable parameters (e.g.
Add, Multiply etc.) as it involves applying the ﬁxed basis
matrix W to the input. Note that, during training, only the
1 × 1 × 1 ﬁlters in Layers 1 and 4 are updated while the
weights in the matrix W remain unaffected.
Parameter analysis of the ReLPV Block. The ReLPV
block uses signiﬁcantly less trainable parameters when
compared to the standard 3D convolutional layer with the
same ﬁlter size/volume and number of input-output chan-
nels. Consider a standard 3D convolutional layer with c in-
put and f output channels. Let n×n×n be the size/volume
of the ﬁlters. Thus, the total number of trainable parameters
in a standard 3D convolutional layer is c· n3 · f . An ReLPV
block with c input channels and f output channels consists
of just c · 1 + f · 26 trainable parameters. Thus, the ratio of
the number of trainable parameters in a standard 3D convo-
lutional layer and the proposed ReLPV block is calculated
as below.

# params. in 3D conv. layer

# params. in ReLPV block

=

c · n3 · f
c · 1 + f · 26

(5)

For simplicity, let us assume f = c, i.e., the number of input
and output channels are same. Furthermore, in practice, in
most deep 3D CNNs f ≥ 27. Therefore, let f = 27. This
reduces the above ratio to n3. Thus, for a ﬁlter of size 3 ×
3 × 3 in the standard 3D convolutional layer, the ReLPV
block uses 27 times less trainable parameters. Therefore,
numerically, ReLPV block saves atleast 27×, 125×, 343×,
729×, 1331×, and 2197× parameters during learning for
3 × 3 × 3, 5 × 5 × 5, 7 × 7 × 7, 9 × 9 × 9, 11 × 11 × 11,
and 13 × 13 × 13 3D convolutional ﬁlters, respectively.
4. Experiments

In this section, we show that the proposed ReLPV block
produces consistently better results on different 3D data
representations compared to the standard 3D convolutional

layer. We demonstrate this on voxelized 3D CAD models
and on spatiotemporal image sequences.

4.1. Experiments and Results on 3D CAD models

Datasets. ModelNet [42] is a large 3D repository of clean
CAD models (shapes). The ModelNet10 with 4,899 shapes
(train: 3991, test: 908) and ModelNet40 with 12,311 shapes
(train: 9843, test: 2468) are commonly used as benchmark-
ing datasets and consist of 10 and 40 categories, respec-
tively. Each model is aligned to a canonical frame and then
rotated at 12 and 24 evenly-sampled orientations about the
z-axis (Az×12 and Az×24 augmentation). These rotated
models are then voxelized to a 32 × 32 × 32 grid. We use
the voxelized versions of [26]. The task here is to classify a
given voxelized 3D model into its corresponding class.

4.1.1 ModelNet: Comparison with the baselines

Baselines. We start our experiments by replacing the stan-
dard 3D convolutional layer with the proposed ReLPV
block (with skip connections) in the baseline networks
VoxNet [26], VoxNetPlus [25] and LightNet [49], and call
these new networks as LP-VoxNet, LP-VoxNetPlus, and
LP-LightNet, respectively. Here LP stands for Local Phase.
The standard 3D convolutional layer is replaced with the
ReLPV block in a straightforward manner. For example
the VoxNet network [26] has the following architecture:
conv3D(5, 32, 2)− conv3D(3, 32, 1)− MP(2)− FC(128)−
FC(K). Here, conv3D(n, f, s) is the standard 3D convolu-
tional layer with f ﬁlters each of size n × n × n applied
with stride s. MP denotes Max Pooling. FC stands for fully
connected layer. K is the number of classes. The equiva-
lent local phase version of VoxNet is: ReLPV(5, 32, 2) −
ReLPV(3, 32, 1)− MP(2)− FC(128)− FC(K). In our ear-
lier discussion on the architecture of the ReLPV block, we
focused only on the important hyperparameters and did not
discuss other hyperparameters that are commonly used in
the standard 3D conv layer, such as the stride information.
Such information can easily be incorporated in the ReLPV
architecture. Similar procedure is followed while preparing
LP-VoxNetPlus and LP-LightNet networks.
Training. We train these new networks using SGD as op-
timizer with momentum 0.9 and categorical crossentropy
as loss. During training, we start with a learning rate of
0.008 and reduce it by a factor of 2 if the validation loss
plateaus. For LP-VoxNet and LP-VoxNetPlus networks,
following [26, 25], we ﬁrst train them on ModelNet40 and
then ﬁne-tune on ModelNet10. The opposite is done on LP-
LightNet network as done in [49]. Following [26, 25, 49],
all networks were trained on 12 evenly-sampled rotations of
each instance about the z-axis (Az × 12 augmentation). No
data augmentation was done on the test data.
Results. Table 1 presents the comparison of the new net-

4906

input layer

input layer

prev layer

prev layer

ReLPV 
(3, 128)

ReLPV 
(3, 128)

ReLPV 
(5, 128)

ReLPV 
(5, 128)

channel concat

channel concat

next layer

next layer

(a) Block 1

ReLPV 
(3, 128)

ReLPV 
(3, 128)

ReLPV 
(5, 128)

ReLPV 
(5, 128)

conv3D
 
(1, 128)

conv3D
 
(1, 128)

channel concat

channel concat
+

+

next layer

next layer

(b) Block 2

2x2x2

32x32x32

16x16x16

8x8x8

4x4x4

Input
layer

Block 1 Block 2 Average
Pooling

Block 2 Average
Pooling

Block 2 Average
Pooling

Block 2

conv3D
 
(1, 256)

Average
Pooling

FC
(512)

FC
(512)

FC

(40/10)

Figure 3: Experiments and comparison with the state-of-the-art. LP-3DCNN network building blocks and architecture.

(c) LP-3DCNN network architecture

Network
VoxNet [26] (baseline)
Binary VoxNet [25]
LP-VoxNet (ours)
VoxNetPlus [25] (baseline)
Binary VoxNetPlus [25]
LP-VoxNetPlus (ours)
LightNet [49] (baseline)
Binary LightNet [25]
LP-LightNet (ours)

ModelNet40 (%) ModelNet10 (%)

83

81.63
86.26
83.91
85.47
88.1
86.90
84.24
87.5

92

90.69
92.24
93.36
92.32
93.4
93.39
92.36
92.95

Table 1: Comparison of the baseline networks with the
Local Phase and binarized versions. The Local Phase ver-
sion outperforms the baseline and their binarized versions.

chrome-extension://pebppomjfocnoigkeepgbmcifnnlndla/index.html

chrome-extension://pebppomjfocnoigkeepgbmcifnnlndla/index.html

works with their corresponding baselines. We also compare
the new networks with the binarized version of the baselines
[25] (as described in section 2). The local phase version
clearly outperforms the corresponding baselines and their
binarized versions on both the ModelNet10 and the Model-
Net40 datasets.

4.1.2 ModelNet: Comparison with the state-of-the-art

Network Architecture. We follow ideas from the
Voxception-ResNet (VRN) architecture of [2] which adopts
a simple inception-style architecture with ResNet-style skip
connections. The intuition behind this design is to have a
maximum number of possible pathways for information to
ﬂow through the network. For the ﬁrst non-downsampling
block that follows the input layer (Fig. 3a), we concatenate
an equal number (128) of feature maps from two ReLPV
blocks with different local phase volume sizes (3×3×3 and
5×5×5). For other non-downsampling blocks, we augment
the above structure with an additional 1×1×1 convolutional
layer that outputs the same number (128) of feature maps as

chrome-extension://pebppomjfocnoigkeepgbmcifnnlndla/index.html

the ReLPV blocks and concatenate it with the other feature
maps as shown in Fig. 3b. This architecture allows the net-
work to choose between taking a weighted average of the
feature maps in the previous layer (i.e. by heavily weight-
ing the 1 × 1 × 1 convolutions) or focusing on local phase
information (i.e., by heavily weighting the ReLPV blocks).
Along with this, skip connections are added as shown in
Fig. 3b for smoother ﬂow of the gradients to the previous
layers. For downsampling, we use average pooling with
pool size 2 and stride 2. Our ﬁnal model is shown in Fig. 3c
with ﬁve non-downsampling blocks, followed by two fully
connected layers each of size 512, and a ﬁnal softmax layer
for classiﬁcation. All non-downsampling layers (after batch
normalization) and fully connected layers are followed by
the ReLU activation function. The layer conv3D(1, 256) is
used after the ﬁnal non-downsampling layer to reduce the
1/1
number of parameters in the fully connected layers.
Training and Testing. The input to our network are voxels
of size 32× 32× 32 from the ModelNet datasets. Following
[2], we change the binary voxel range from {0,1} to {-1,5}
to encourage the network to pay more attention to positive
entries. Network is trained using SGD as optimizer with
momentum 0.9 and categorical crossentropy as loss. During
training, we start with a learning rate of 0.008 and reduce it
by a factor of 5 if the validation loss plateaus. All weights
are initialized using orthogonal initialization. The network
is ﬁrst trained on the Az×12 augmented data, then it is ﬁne-
tuned on the Az × 24 augmented data at low learning rate.
No data augmentation was done on the test data. Apart from
rotations, the data is augmented by adding noise, random
translations and horizontal ﬂips to each training example,
as done in [26, 2].
Results. Table 2 compares our results with other methods
that use voxelized/volumetric ModelNet datasets as input.

1/1

1/1

4907

Network
3D ShapeNets [42]
Beam Search [46]
3D-GAN [41]
VoxNet [26]
LightNet [49]
ORION [33]
VRN [2]
LP-3DCNN (ours)
LP-3DCNN (ours)
FusionNet [13]
VRN Ensemble [2]

Framework

Augmentation

Parameters (Millions) ModelNet40 (%) ModelNet10 (%)

Single, Volumetric
Single, Volumetric
Single, Volumetric
Single, Volumetric
Single, Volumetric
Single, Volumetric
Single, Volumetric
Single, Volumetric
Single, Volumetric

Az×12

-
-

Az×12
Az×12
Az×12
Az×24
Az×12
Az×24

Ensemble, Vol.+ Mul.
Ensemble, Volumetric

(Az, El)×60

Az×24

≈ 38
≈ 0.08

≈11

≈ 0.92
≈ 0.30
≈ 0.91
≈ 18
≈ 2
≈ 2
≈118
≈ 108

77

81.26
83.3
83

86.90

-

91.33
89.4
92.1
90.8
95.54

83.5
88
91
92

93.39
93.8
93.61
93.76
94.4
93.11
97.14

Table 2: Performance results on the ModelNet datasets. Az stands for azimuth rotation and El stands for elevation rotation.
“-” means that information is not provided for the item in the paper. Vol. stands for volumetric, Mul. stands for multi-view.

In order to make a fair comparison, we only consider vol-
umetric network frameworks in this work. We do not in-
clude multi-view networks or point cloud-based networks.
In single network framework, our proposed network outper-
forms all the previous networks on both the ModelNet10
and the ModelNet40 datasets. Furthermore, it uses just 2
million parameters compared to the current state-of-the-art,
the VRN network, that uses 18 million parameters. In the
ensemble framework, the VRN achieves the best perfor-
mance on both the ModelNet10 and ModelNet40 datasets.
However, it has the most complex network architecture with
up to 45 layers and 108 million parameters, taking almost
6 days to train. In ensemble framework, our network out-
performs FusionNet [13] while using almost 59 times less
parameters and signiﬁcantly less data augmentation.

4.2. Experiments and Results on Spatiotemporal

Image Sequences

Dataset. We use the UCF-101 split-1 action recognition
dataset [34]. The dataset has been used as a benchmark
dataset in [37, 38, 8] for the performance studies and for
searching 3D CNN network architectures and hyperparam-
eters for action recognition tasks.
Baseline. We use the experimental 3D CNN network pro-
posed by [37] for action recognition as baseline which is
a smaller version of the C3D network [37]. For simplic-
ity, we call this network as mini C3D network or mC3D.
The mC3D network with ﬁlter size n × n × n denoted as
mC3Dn has the following architecture: conv3D(n, 64) −
MP(2) − conv3D(n, 128) − MP(2) − conv3D(n, 256) −
MP(2) − conv3D(n, 256) − MP(2) − conv3D(n, 256) −
MP(2) − FC(2048) − FC(2048) − FC(101). Each 3D con-
volutional and fully connected layer is followed by a ReLU
activation function. All the convolution layers are applied
with appropriate padding and stride 1 such that there is no
change in size of the tensor from the input to the output of
these layers. Following [37], the input to the network are
videos of dimension 3 × 16 × 112 × 112.

The equivalent local phase version of the above network,

denoted as LP-mC3Dn, is prepared by replacing the stan-
dard 3D convolutional layers with the ReLPV blocks as
done in Section 4.1.1. Here, n denotes the size of the lo-
cal 3D neighborhood in which STFT is computed.
Training.
Following [37], we use SGD as optimizer
with Nesterov momentum with value 0.9 and categorical
crossentropy as loss. We train the networks for 16 epochs
starting with a learning rate of 0.003 and decreasing it by
a factor of 10 after every 4 epochs. Note that all the net-
works are trained from scratch. No data augmentation such
as frame translation, rotation, or scaling is used. We re-
trained all the baseline networks (for n = 3, 5, 7). The re-
sults were found to be consistent with Fig.2 in [37].
Results. Early works such as [37, 22] showed that train-
ing relatively shallow 3D CNNs from scratch on the UCF-
101 split-1 dataset achieve performance between 41− 44%.
Recent works such as [38, 8] use deep 3D Residual Con-
vNet architectures to achieve better results. Table 3 reports
our results on the UCF-101 split-1 dataset. We improve
the state-of-the-art by 5.68% while using just ﬁve ReLPV

Network

2D-ResNet 18 [12, 38]
2D-ResNet 34 [12, 38]
3D-ResNet 18 [38]
3D-ResNet 34 [38]
3D-ResNet 101 [8]
3D STC-ResNet 101 [8]
mC3D3 [37] (baseline)
LP-mC3D3 (ours)
mC3D5 [37] (baseline)
LP-mC3D5 (ours)
mC3D7 [37] (baseline)
LP-mC3D7 (ours)
mC3D9 (baseline)
LP-mC3D9 (ours)

Parameters Model Size
(Millions)

(Mb)

FLOP

(Millions)

≈ 11.2
≈ 21.5
≈ 33.2
≈ 63.5
≈ 86.06

-

≈ 18
≈ 13

≈ 34.32

≈ 13

≈ 71.88

≈ 13

≈ 138.34

≈ 13

-
-

254
485
657

-

139.6
106.2
274.6
106.2
575
106.2
1100
106.2

-
-
-
-
-
-

34.88
26.072
68.64
26.077
143.72
26.08
276.68
26.083

Acc.
(%)
42.2
42.2
45.6
45.9
46.7
47.9
44

53.58
42.5
51.44
42.3
50.54
36.17
48.99

Table 3: Performance results on the UCF-101 split-1 ac-
tion recognition dataset. Comparison of the ReLPV block
based 3D CNNs with their corresponding baselines and
other state-of-the-art networks. All the networks are trained
from scratch.

4908

blocks. Our network uses 13 million parameters compared
to the 3D STC-ResNet 101 network [8], which is built on
the top of 3D ResNet 101 network and uses more than 86
million parameters. Furthermore, all the local phase ver-
sions with different local phase volumes signiﬁcantly out-
perform the corresponding baseline networks.

5. Discussion and Analysis

In this section, we present detailed ablation and perfor-
mance studies of the ReLPV block. Furthermore, we dis-
cuss some statistical advantages afforded by the ReLPV
block over the standard 3D convolutional layer.

5.1. Space time Complexity of the ReLPV block

Model size. Table 3 shows that the ReLPV block based
3D CNNs use less parameters and occupy less disk space
when compared to the corresponding baselines. Further-
more, with an increase in the local phase volume (while
keeping other hyperparameters constant) from 3 to 9, there
is no change in the number of trainable parameters or model
size in the ReLPV block based networks. In contrast, there
is a signiﬁcant rise in the number of parameters and model
size in baseline networks with an increase in ﬁlter size. We
believe this feature of the ReLPV block can be of huge ben-
eﬁt for 3D CNNs in resource constraint environment.
Computational cost. We discussed in Section 3 that due to
the separability of the basis functions, STFT can be com-
puted efﬁciently by using simple 1D convolutions for each
dimension. This technique of computing 3D STFT using
separable convolutions saves huge computational costs and
has been of recent interest in 3D CNNs as discussed in Sec-
tion 2. Table 3 reports the computation cost in terms of the
number of Floating Point Operations (FLOP) of the models.
The FLOP values of the ReLPV block based 3D CNN are
less when compared to the corresponding baselines. Fur-
thermore, they vary very little with an increase in the lo-
cal phase volume. However, for the baseline networks, the
FLOP values increase by almost 8 times with an increase of
ﬁlter size from 3 to 9.

5.2. Statistical advantages of the ReLPV block

As discussed earlier, one of the major challenges in train-
ing deep 3D CNNs is to avoid overﬁtting [37, 38, 11]. A re-
cent study by Hara et al. in [11] shows that even a relatively
shallow 3D CNN such as 3D ResNet-18 tends to overﬁt sig-
niﬁcantly on action recognition datasets such as UCF-101
[34] and HMDB-51 [24]. This is partly due to the large
number of trainable parameters in 3D CNNs in comparison
to their 2D counterparts and partly due to the unavailabil-
ity of large scale 3D datasets [38, 11]. These pose a ma-
jor bottleneck in training deep 3D CNNs. In order to curb
overﬁtting, various training methods such as data augmenta-
tion, training shallow networks, and novel regularizers such

Figure 4: Overﬁtting results on the UCF-101 split-1
dataset. The LP-mC3D3 network overﬁts less and gener-
alizes signiﬁcantly better compared to the baseline mC3D3
network.

Figure 5: ReLPV block STFT volume search. LP-mC3D3
network with STFT volume of 3 × 3 × 3 performs the best.

as Dropout [35], DropConnect [40], and Maxout [10] have
been introduced. While regularizers such as [40, 35, 10]
have been proposed to regularize the fully connected lay-
ers of the network, recent works such as [4, 19, 35] show
that regularizing the convolutional layers of the network
is equally important. Our ReLPV block when used in the
place of the standard 3D convolutional layer in deep 3D
CNNs, naturally regularizes the network due to its use of
signiﬁcantly less trainable parameters and due to the decor-
relation property of STFT (see Section 3). Fig. 4 reports
our result on the overﬁtting experiment. The LP-mC3D3,
network clearly overﬁts less and generalizes signiﬁcantly
better when compared to the baseline mC3D3 network.

5.3. Exploring the Local Phase Volume of the

ReLPV block

As described earlier, the ReLPV block takes two hyper-
parameters as input, one of which is the size of the local vol-
ume in which the STFT is computed (and the local phase is
extracted) for each position of the input feature map.
In

4909

this section, we explore this hyperparameter. We exper-
iment with different sizes of local volumes, in particular
from 3 × 3 × 3 to 9 × 9 × 9. We found that the perfor-
mance of the ReLPV block decreases with an increase in
the STFT volume. Fig. 5 presents the clip accuracy of the
LP-mC3Dn network on the UCF-101 test split-1 dataset for
various STFT volumes ranging from 3 × 3 × 3 to 9 × 9 × 9
over 16 epochs. The LP-mC3D3 network with STFT vol-
ume of 3 × 3 × 3 performs the best while the LP-mC3D9
network performs the worst. Note that, an analogous study
was carried out in [37] for the standard 3D convolutional
layer where it was found that the 3D CNNs with 3 × 3 × 3
convolutional kernels in all the layers perform the best.

5.4. Exploring the number of feature maps output

by the ReLPV block

In this section, we explore another hyperparameter, the
number of feature maps output by the ReLPV block.
In
simple words, we explore the effect of varying the num-
ber of 1 × 1 × 1 ﬁlters in the Layer 4 of the ReLPV block
(see Section 3). For this, we use a modiﬁed version of the
LP-mC3Dn network and experiment with different pairs of
ReLPV block hyperparameters (n, f ). Let LP-mC3Dn,f
be our experimental network with the following architec-
ture: Input layer−ReLPV(n, f )−MP(2)−ReLPV(n, f )−
MP(2)− ReLPV(n, f )− MP(2)− ReLPV(n, f )− MP(2)−
ReLPV(n, f ) − conv3D(1 × 1 × 1, 256) − MP(2) −
FC(2048) − FC(2048) − FC(101). The layer Conv3D(1 ×
1 × 1, 256) is used after the last ReLPV block so that the
number of parameters in the fully-connected layers does not
vary across different networks. Table 4 presents our results
of the experiment on the UCF-101 split-1 test set. We ob-
serve that, for a ﬁxed value of the local STFT volume (the
hyperparameter n), performance improves with an increase
in the number of 1 × 1 × 1 ﬁlters (the hyperparameter f ).
Another important observation is that the model size and the
number of trainable parameters vary by a very small amount
with an increase in the value of the hyperparameter f .

5.5. ReLPV Block based Hybrid 3D CNN Models

In this section, we explore the performance effects of us-
ing ReLPV blocks and the standard 3D convolutional lay-
ers in a single 3D CNN network. We call such networks as
hybrid 3D CNNs. We experiment with two types of varia-
tions. In the ﬁrst variation, we replace the top few layers
(following the input layer) of a traditional 3D CNN net-
work (baseline mC3D3) with the ReLPV blocks such that
the feature maps learned by the ReLPV blocks are input to
the later standard 3D convolutional layers. In the second
variation, the bottom layers are replaced with the ReLPV
blocks such that the feature maps learned by the standard
3D convolutional layers are input to the later ReLPV blocks.
We use the notation mC3D3(Bl/Tl) to denote that l bot-

Network
LP-mC3D3,64
LP-mC3D3,128
LP-mC3D3,256
LP-mC3D5,64
LP-mC3D5,128
LP-mC3D5,256
LP-mC3D7,64
LP-mC3D7,128
LP-mC3D7,256

Parameters (Milions) Model Size (Mb)

≈ 12.84
≈ 12.93
≈ 13.20
≈ 12.84
≈ 12.93
≈ 13.20
≈ 12.84
≈ 12.93
≈ 13.20

104.2
104.9
107.1
104.2
104.9
107.1
104.2
104.9
107.1

Acc.
50.96
51.84
53.50
50.29
51.10
53.22
47.66
50.10
51.14

Table 4: Exploring the number of feature maps output
by the ReLPV block. Performance improves with increase
in value of f .

Network
mC3D3(T1)
mC3D3(T2)
mC3D3(T3)
mC3D3(B1)
mC3D3(B2)
mC3D3(B3)

Parameters (Millions) Model Size (Mb)

≈ 17.44
≈ 16.13
≈ 13.20
≈ 15.82
≈ 14.13
≈ 13.30

139.9
138.5
132.1
126.9
113.7
107.3

Acc.
51.51
47.67
43.95
35.1
36.47
40.84

Table 5: Results on hybrid 3D CNN architectures. Per-
formance results on the UCF-101 split-1 test set.

tom/top successive 3D conv layers of mC3D3 are replaced
with the ReLPV block. Table 5 reports our results of the
experiments. We observe that replacing standard 3D convo-
lutional layers with the ReLPV blocks at the top of a tradi-
tional 3D CNN network improves its performance while the
opposite happens when ReLPV blocks are added in the bot-
tom layers. However, the hybrid 3D CNNs do not outper-
form the LP-mC3D3 network where all layers are replaced
with the ReLPV block (Table 3).

6. Conclusion

In this work, we have proposed ReLPV block, an efﬁ-
cient alternative to the standard 3D convolutional layer, in
order to reduce the high space-time and model complexity
of the traditional 3D CNNs. The ReLPV block when used
in place of the standard 3D convolutional layer in traditional
3D CNNs, signiﬁcantly improves the performance of the
baseline architectures. Furthermore, they produces consis-
tently better results across different 3D data representations.
Our proposed ReLPV block based 3D CNN architectures
achieve state-of-the-art results on the ModelNet and UCF-
101 split-1 action recognition datasets. We plan to apply
ReLPV block in 3D CNN architectures for other 3D data
representations and tasks such as 3D MRI segmentation.

Acknowledgments. The authors gratefully acknowledge
the travel grant support from Google Research India. Sud-
hakar Kumawat was supported by TCS Research Fellow-
ship. Shanmuganathan Raman was supported by SERB
Core Research Grant and Imprint 2 Grant. We thank Man-
isha Verma for early contribution and technical discussion.

4910

References

[1] Maryam Asadi-Aghbolaghi, Albert Clapes, Marco Bellanto-
nio, Hugo Jair Escalante, V´ıctor Ponce-L´opez, Xavier Bar´o,
Isabelle Guyon, Shohreh Kasaei, and Sergio Escalera. A sur-
vey on deep learning based approaches for action and gesture
recognition in image sequences. In Automatic Face & Ges-
ture Recognition, pages 476–483. IEEE, 2017.

[2] Andrew Brock, Theodore Lim, James M Ritchie, and
Nick Weston. Generative and discriminative voxel mod-
eling with convolutional neural networks. arXiv preprint
arXiv:1608.04236, 2016.
¨Ozg¨un C¸ ic¸ek, Ahmed Abdulkadir, Soeren S Lienkamp,
learning
Thomas Brox, and Olaf Ronneberger. 3d u-net:
dense volumetric segmentation from sparse annotation.
In
MICCAI, pages 424–432, 2016.

[3]

[4] Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochre-
iter. Fast and accurate deep network learning by exponential
linear units (elus). In ICLR, 2016.

[5] Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zit-
nick, and Dhruv Batra. Reducing overﬁtting in deep net-
works by decorrelating representations. In ICLR, 2016.

[6] M Courbariaux and Y Bengio. Binarynet: Training deep
neural networks with weights and activations constrained to+
1 or- 1. arXiv: 1602.02830, 2017.

[7] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
David. Binaryconnect: Training deep neural networks with
binary weights during propagations. In NIPS, pages 3123–
3131, 2015.

[8] Ali Diba, Mohsen Fayyaz, Vivek Sharma, M Mahdi Arzani,
Rahman Yousefzadeh, Juergen Gall, and Luc Van Gool.
Spatio-temporal channel correlation networks for action
classiﬁcation. In ECCV, 2018.

[9] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and
Yoshua Bengio. Deep learning, volume 1. MIT press Cam-
bridge, 2016.

[10] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron
Courville, and Yoshua Bengio. Maxout networks. In ICML,
pages III–1319, 2013.

[11] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can
spatiotemporal 3d cnns retrace the history of 2d cnns and
imagenet. In CVPR, pages 18–22, 2018.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016.

[13] Vishakh Hegde and Reza Zadeh. Fusionnet: 3d object clas-
siﬁcation using multiple data representations. arXiv preprint
arXiv:1607.05695, 2016.

[14] Janne Heikkila and Ville Ojansivu. Methods for local phase
In LNLA,

quantization in blur-insensitive image analysis.
pages 104–111, 2009.

tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.

[17] Lei Huang, Dawei Yang, Bo Lang, and Jia Deng. Decorre-

lated batch normalization. In CVPR, 2018.

[18] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-
Yaniv, and Yoshua Bengio. Quantized neural networks:
Training neural networks with low precision weights and
activations. The Journal of Machine Learning Research,
18(1):6869–6898, 2017.

[19] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, pages 448–456, 2015.

[20] Anil K Jain. Fundamentals of digital image processing. En-

glewood Cliffs, NJ: Prentice Hall,, 1989.

[21] Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Sav-
vides. Local binary convolutional neural networks. In CVPR,
volume 1, 2017.

[22] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video
classiﬁcation with convolutional neural networks. In CVPR,
pages 1725–1732, 2014.

[23] P Kovesi. Image features from phase congruency. Videre: A

Journal of Computer Vision Research, 1(3):1–26, 1999.

[24] Hildegard Kuehne, Hueihan Jhuang, Est´ıbaliz Garrote,
Tomaso Poggio, and Thomas Serre. Hmdb: a large video
database for human motion recognition.
In ICCV, pages
2556–2563, 2011.

[25] Chao Ma, Wei An, Yinjie Lei, and Yulan Guo. BV-
CNNs: Binary volumetric convolutional networks for 3d ob-
ject recognition. In BMVC, 2017.

[26] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-
volutional neural network for real-time object recognition. In
IROS, 2015.

[27] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 3DV, pages 565–571, 2016.

[28] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units im-
prove restricted boltzmann machines. In ICML, pages 807–
814, 2010.

[29] Juhani P¨aiv¨arinta, Esa Rahtu, and Janne Heikkil¨a. Volume
local phase quantization for blur-insensitive dynamic texture
classiﬁcation. In Scandinavian Conference on Image Analy-
sis, pages 360–369, 2011.

[30] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-
temporal representation with pseudo-3d residual networks.
In ICCV, pages 5534–5542, 2017.

[31] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi-
nary convolutional neural networks. In ECCV, pages 525–
542, 2016.

[15] B Hinman, Jared Bernstein, and D Staelin. Short-space
fourier transform image processing. In ICASSP, volume 9,
pages 166–169, 1984.

[32] Pau Rodr´ıguez, Jordi Gonzalez, Guillem Cucurull, Josep M
Gonfaus, and Xavier Roca. Regularizing cnns with locally
constrained decorrelations. In ICLR, 2017.

[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-

[33] N Sedaghat, M Zolfaghari, E Amiri,

and T Brox.
Orientation-boosted voxel nets for 3d object recognition. In
BMVC, 2017.

4911

[34] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402, 2012.

[50] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong
Chen. Incremental network quantization: Towards lossless
cnns with low-precision weights. ICLR, 2017.

[35] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958, 2014.

[36] Lin Sun, Kui Jia, Dit-Yan Yeung, and Bertram E Shi. Human
action recognition using factorized spatio-temporal convolu-
tional networks. In ICCV, pages 4597–4605, 2015.

[37] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torre-
sani, and Manohar Paluri. Learning spatiotemporal features
with 3d convolutional networks. In ICCV, pages 4489–4497,
2015.

[38] Du Tran, Jamie Ray, Zheng Shou, Shih-Fu Chang, and
Manohar Paluri. Convnet architecture search for spatiotem-
poral feature learning. arXiv preprint arXiv:1708.05038,
2017.

[39] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. In CVPR, pages 6450–
6459, 2018.

[40] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and
Rob Fergus. Regularization of neural networks using drop-
connect. In ICML, pages 1058–1066, 2013.

[41] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and
Josh Tenenbaum. Learning a probabilistic latent space of ob-
ject shapes via 3d generative-adversarial modeling. In NIPS,
pages 82–90, 2016.

[42] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao.
3d
shapenets: A deep representation for volumetric shapes. In
CVPR, pages 1912–1920, 2015.

[43] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In CVPR, pages 5987–5995, 2017.

[44] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and
Kevin Murphy. Rethinking spatiotemporal feature learning:
Speed-accuracy trade-offs in video classiﬁcation. In ECCV,
pages 305–321, 2018.

[45] Wei Xiong, Bo Du, Lefei Zhang, Ruimin Hu, and Dacheng
Tao. Regularizing deep convolutional neural networks with
a structured decorrelation constraint. In ICDM, pages 519–
528, 2016.

[46] Xu Xu and Sinisa Todorovic. Beam search for learning a
deep convolutional neural network of 3d shapes. In ICPR,
pages 3506–3511, 2016.

[47] Ido Zachevsky and Yehoshua Y Zeevi. Modelling lo-
cal phase of images and textures with applications in
phase denoising and phase retrieval.
arXiv preprint
arXiv:1810.00403, 2018.

[48] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. In CVPR, 2018.

[49] Shuaifeng Zhi, Yongxiang Liu, Xiang Li, and Yulan Guo.
Lightnet: A lightweight 3d convolutional neural network for
real-time 3d object recognition. In Eurographics Workshop
on 3D Object Retrieval, 2017.

4912

