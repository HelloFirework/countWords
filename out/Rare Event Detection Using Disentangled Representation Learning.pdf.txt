Rare Event Detection using Disentangled Representation Learning

Ryuhei Hamaguchi, Ken Sakurada, and Ryosuke Nakamura

National Institute of Advanced Industrial Science and Technology (AIST)

{ryuhei.hamaguchi, k.sakurada, r.nakamura}@aist.go.jp

Abstract

This paper presents a novel method for rare event de-
tection from an image pair with class-imbalanced datasets.
A straightforward approach for event detection tasks is to
train a detection network from a large-scale dataset in an
end-to-end manner. However, in many applications such
as building change detection on satellite images, few posi-
tive samples are available for the training. Moreover, scene
image pairs contain many trivial events, such as in illumi-
nation changes or background motions. These many trivial
events and the class imbalance problem lead to false alarms
for rare event detection. In order to overcome these difﬁ-
culties, we propose a novel method to learn disentangled
representations from only low-cost negative samples. The
proposed method disentangles different aspects in a pair
of observations: variant and invariant factors that repre-
sent trivial events and image contents, respectively. The
effectiveness of the proposed approach is veriﬁed by the
quantitative evaluations on four change detection datasets,
and the qualitative analysis shows that the proposed method
can acquire the representations that disentangle rare events
from trivial ones.

1. Introduction

In the ﬁeld of computer vision, event detection from an
image pair has been comprehensively studied as image sim-
ilarity estimation. Similarity estimation between images is
one of the fundamental problems, which can be applied for
many tasks, such as change detection [11, 14, 20, 25], image
retrieval and matching [3, 23, 33], identiﬁcation [26, 31],
and stereo matching [9, 34]. Thanks to the recent success
of deep features, the image comparison methods have sub-
stantially progressed. However, a general draw back is that
they require a large amount of dataset to fully utilize the
representational power of the deep features.

In the context of image similarity estimation, this paper
considers a particular task of detecting rare events from an
image pair, such as detecting building changes on a pair
of satellite images, or detecting manufacturing defects by

Representation learning

Fine-tuning

𝑠𝑐
𝑐𝑠

Enc

Enc

Negative
samples

𝑐𝑐

Cls

Dec

Enc

Similarity

Dec

Enc

Positive/Negative

samples

Figure 1. The overall concept of the proposed model. From
the negative image pairs, the representation learning model (left)
learns features that are invariant to trivial events. The rare event
detector (right) is then trained on the learned invariant features.

comparing images of products. One challenge of the task
lies in the difﬁculty of collecting training samples. Because
ﬁnding rare samples is labor intensive task, the training
dataset often includes few positive samples. Additionally,
image pairs often contain many cumbersome events that are
not of interest (e.g., illumination changes, registration er-
ror of images, shadow changes, background motion, or sea-
sonal changes). These many trivial events and the class im-
balance problem lead to false alarms for trivial events, or
overlooking the rare events.

In order to overcome these difﬁculties, we propose a
novel network architecture for disentangled representation
learning using only low-cost negative image pairs. Figure 1
demonstrates the overall concept of the proposed method.
The proposed network is trained to encode each image into
the two separated features, speciﬁc and common, by intro-
ducing a similarity constraint between the image contents.
The common features represent image contents that is in-
variant to trivial events, and the speciﬁc features represent a
mixture of information related to trivial events (e.g., illumi-
nation, shadows, or background motion). This disentangle-
ment can be learned using only low-cost negative samples
because negative samples contain rich information about
trivial events. Once we have acquired the common features,
we can build rare event detectors on the learned representa-
tions using small amount of training samples.

9327

The effectiveness of the proposed method on the class-
imbalance scenario is veriﬁed by the quantitative evalua-
tions on four change detection datasets, including in-the-
wild datasets.
In addition, the qualitative analysis shows
that the proposed method successfully learns the disentan-
gled representation for both rare events and trivial ones in
the image pairs. The contributions of this work are as fol-
lows:

• We propose a novel solution to the class imbalance
problem in rare event detection tasks, which has not
been fully studied in the past literature.

• We propose a novel representation learning method
that only requires pairs of observations to learn dis-
entangled representations.

• We create a new large-scale change detection dataset

from the open data repository of Washington D.C.

2. Related Work

In change detection tasks, several works have attempted
to overcome the difﬁculties of data collection and cumber-
some trivial events as described in the previous section. In
order to save the cost of annotation, [13] proposed a weakly
supervised method that requires only image-level labels to
train their change segmentation models. Although their
work saves the pixel-level annotation cost, it still requires
image-level labels, which are still difﬁcult to collect for rare
change events. To address trivial events, several works on
video surveillance tasks [4, 24] utilize background model-
ing techniques in which foreground changes are detected as
outliers. However, these works assume a continuous frame
as the input, and their application is limited to change de-
tection in video frames. [12] proposed a semi-supervised
method to detect damaged areas from pairs of satellite im-
ages. In their method, a bag-of-visual-words vector is ex-
tracted for hierarchical shape descriptors and a support vec-
tor machine classiﬁer is trained on the extracted features.
Since their method is based on the carefully chosen feature
descriptors specialized for their task, the method lacks gen-
eralizability for application in other domains.

Disentangled representation learning is an actively stud-
ied ﬁeld.
[27] proposed a generative adversarial network
(GAN) framework to learn disentangled representation for
pose and identity of a face using encoder-decoder archi-
tecture with auxiliary variables inserted in its latent code.
[18] proposed a GAN model that can generate synthetic im-
ages conditioned on category labels. [22] proposed a semi-
supervised method to learn disentangled representation by
introducing graphical model structure between the encoder
and decoder of a standard variational auto-encoder (VAE).
A drawback of these methods is that during training, they

require explicit labels for the target factor of variation. As
for an unsupervised approach, [8] proposed a method that
learns disentangled representation by maximizing mutual
information between a small subset of latent codes and a
generated image. However, this method cannot control the
disentanglement so that the desired factor of variations is
represented in a certain latent code. Some works utilize
groups of observations as weak supervision. [16] trains a
target latent unit on grouped mini-batches that include only
one factor of variation. [5] and [17] proposed a method that
effectively disentangles intra-class and inter-class variations
using groups of images sharing the same class labels. Our
work is similar to the three works mentioned above. The
difference is that our work assumes weaker conditions; that
is, our method only requires pairs of observations and does
not require aligned observations or class labels. Recently,
multi-view image generation method that only use a paired
observation for feature disentanglement is proposed in [7].
The method that is most related to our work is Domain
Separation Network (DSN) [6]. DSN decomposes an im-
age into a common and a speciﬁc factors between two dif-
ferent image domains. To learn the disentanglement, DSN
penalizes distance between marginals of common features:
D(p(zA) k p(zB)), where p(zA) = E
p(xA)[p(zA|xA)] and
p(xB )[p(zB|xB)]. While the method is effective
p(zB) = E
on domain adaptation tasks, it is not applicable to image
comparison tasks such as rare event detection. This is be-
cause the image comparison tasks do not assume domain
bias across p(xA) and p(xB) that is essential for DSN to
learn the disentanglement. On the other hand, our method
penalizes the distance between the posteriors instead of the
marginals; that is, D(p(zA|xA) k p(zB|xB)). Since the
loss does not involve the expectation of p(xA) and p(xB),
our method is applicable regardless of the existence of do-
main bias across p(xA) and p(xB).

3. Methods

3.1. Overview

Figure 2 shows a schematic of the proposed model. The
model consists of two branches of VAEs that share parame-
ters each other. Each VAE extracts two types of feature rep-
resentations: common and speciﬁc. They represent different
aspects of an input image pair, invariant and variant factors,
respectively. In the context of rare event detection, the spe-
ciﬁc features represent trivial events, and the common fea-
tures represent image contents that are invariant to trivial
events. In order to achieve the disentanglement, we intro-
duce a similarity constraint between common features. This
constraint promotes common features to lie in a shared la-
tent space of paired images. The key aspect of the common
features is that they are invariant to trivial events, which
should be helpful to distinguish target events from trivial

9328

Encoder

𝑆ℎ𝑎𝑟𝑒𝑑

Encoder

𝑥𝐴

𝑥𝐵

𝓛𝑽𝑨𝑬𝑨
𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐
𝜇𝐴𝑠
𝜎𝐴𝑠
𝐶𝑜𝑚𝑚𝑜𝑛
𝜇𝐴𝑐
𝜎𝐴𝑐
𝓛𝒔𝒊𝒎
𝐶𝑜𝑚𝑚𝑜𝑛
𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐 𝜇𝐵𝑐
𝜎𝐵𝑐
𝜇𝐵𝑠
𝜎𝐵𝑠
𝓛𝑽𝑨𝑬𝑩

𝓛𝒂𝒄𝒕𝑨
𝓛𝒂𝒄𝒕𝑩

𝑧𝐴𝑐
𝑧𝐴𝑠
𝑧𝐵𝑐
𝑧𝐵𝑠

Decoder

𝑆ℎ𝑎𝑟𝑒𝑑

Decoder

෤𝑥𝐴

෤𝑥𝐵

Figure 2. Schematics of the proposed representation learning method. The model takes a pair of images xA and xB as input. For each
image, the encoder extracts common and speciﬁc features, and the decoder reconstructs the input. The key feature of the model is the
similarity loss Lsim. This loss constrains the common features to extract invariant factors between xA and xB. Another feature is the
c) to be activated, which avoids a trivial solution –
activation loss Lact. This loss encourages the mean vector of the common features (µ
(σ

c) = (1, 0) – for any input.

c, µ

events. In the successive ﬁne-tuning phase, an event detec-
tor is trained on the learned common features using a small
number of positive and negative samples.

The contents of this section are as follows.

In Sec-
tion 3.2, we present a brief introduction to VAEs. In Sec-
tion 3.3, the proposed method of representation learning is
explained in detail. Finally, in Section 3.4, the ﬁne-tuning
phase of the event detector is explained.

3.2. Variational Auto encoder

A variational auto-encoder [15, 19] is a kind of deep gen-
erative model that deﬁnes the joint distribution of an in-
put x ∈ X and a latent variable z ∈ Z as pθ(x, z) =
pθ(x|z)p(z). p(z) is often set to be Gaussian distribution
with zero mean and unit variance. The generative distribu-
tion pθ(x|z) is modeled by a deep neural network (decoder)
with parameters θ, and the model parameters are trained by
maximizing the marginal likelihood pθ(x) = Pz pθ(x, z).
However, in the case that pθ(x|z) is a neural network, the
marginal likelihood becomes intractable. Therefore, the fol-
lowing variational lower bound is used instead:

LV AE = E

qφ(z|x)[log pθ(x|z)]

−DKL(qφ(z|x) k p(z)) ≤ log pθ(x)

(1)

In the above equation, qφ(z|x) is another deep neural net-
work (encoder) that approximates the posterior distribution
pθ(z|x). The ﬁrst term of Eq. (1) can be seen as the re-
construction error of a classical auto-encoder, and the sec-
ond term can be seen as the regularization term. In order to

make the lower bound differentiable in terms of the encoder
parameters, a technique called reparameterization is used:

z = µφ(x) + σφ(x) ⊙ ǫ where

ǫ ∼ N (0, 1)

(2)

Here, ⊙ represents an element-wise product. In this case,
the encoder becomes a deep neural network that outputs
mean and variance of the posterior distribution.

3.3. Representation Learning

VAE provides an unsupervised method to learn latent
representations. Given input x, the latent representation can
be inferred using the encoder distribution qφ(z|x). The ob-
jective here is to learn the encoder distribution qφ(zc, zs|x)
in which latent variables are disentangled such that zc and
zs respectively represent invariant and variant factors in a
given image pair. For this, we build a model with two
branches of VAEs that share parameters each other. As
shown in Figure 2, the input images xA, xB ∈ X are fed
into different VAE branches, and the latent variables zc and
zs are extracted from each branch. The parameters of VAEs
are trained using following loss function.

L = LA

V AE + LB

V AE + λ1Lsim + λ2Lact

(3)

V AE, LB

where LA
V AE are VAE losses for input images xA
and xB, respectively. Lsim is a similarity loss function that
constrains common features to represent invariant factors
between paired images. Lact is an activation loss function
that encourages activation of common features to avoid a
trivial solution. λ1 and λ2 are the coefﬁcients of similarity

9329

and activation loss, respectively. The following explains
the details about each terms of the loss.

Variational auto-encoder loss.
of each VAE branch becomes

The joint distribution

pθ(x, zc, zs) = pθ(x|zc, zs)p(zc)p(zs)

(4)

The generative distribution pθ(x|zc, zs) is set as a Gaus-
sian distribution with its mean given by the decoder out-
put. The priors p(zc) and p(zs) are both Gaussian distribu-
tions with zero mean and unit variance. Then, the inference
model becomes

qφ(zc, zs|x) = qφ(zc|x)qφ(zs|x)

(5)

The posteriors for zc and zs are set to Gaussian distri-
φ(x)) and qφ(zs|x) =
butions qφ(zc|x) = N (µc
N (µs
φ(x)) whose mean and variance are given by
the outputs of encoder networks. Then, the loss function of
the VAE becomes

φ(x), σs

φ(x), σc

LV AE = E

qφ(zc,zs|x)[log pθ(x|zc, zs)]

Figure 3. Illustration of posterior distributions in 2D case. Even if
the distances in each dimension D1, D2 are the same, distributions
are farther away in z2 axis because of the smaller variance.

there exists a trivial solution.

Activation loss.
One problem with the similarity
constraints is that
The
constraints can be completely satisﬁed by setting the mean
vectors of common features to all zeros. In this case, all
the information in the input are encoded by the speciﬁc
features and the common features do not represent any
information. To avoid this, we introduce another loss to
encourage activation of the common features:

Lact = Lsparsity + Linvmax

(9)

+DKL(qφ(zc|x) k p(zc)) + DKL(qφ(zs|x) k p(zs))
(6)

Activation loss consists of two parts: sparsity loss and in-
vmax loss:

Similarity loss.
In order to make common features
encode invariant factors in an input image pair, we in-
troduce the following similarity loss between the pair of
common features extracted from xA and xB:

Lsim = D(qφ(zc|xA) k qφ(zc|xB))

(7)

where D deﬁnes a statistical distance between latent vari-
ables. There are various types of similarity metric that can
be used for D. A simple candidate is L2 or L1 distance
between centroids µc(xA), µc(xB) of the two posteriors.
However, as shown in Figure 3, when the posterior distribu-
tions have different variance along each latent dimension,
the distance between centroids do not reﬂect the distance
between distributions. Therefore, we used a kind of Maha-
lanobis distance as follows:

Lsim =

1
M

M

X

i=1

(µc

i (xA) − µc
σc
i (xA)σc

i (xB))2
i (xB)

(8)

i and σc

Here, µc
i represent the i-th element of the mean and
the standard deviation of the posterior distribution, and M
is the dimension of the latent variable. The metric measures
scaled distance along each latent dimensions according to
its variances. The experimental results and a comparison
between various kind of distance metrics are shown in
Section 4.5.

Lsparsity =

d

X

i=1

(s log mi + (1 − s) log (1 − mi))

(10)

Linvmax =

1
B

B

X

k=1

(max

i

|µk

i |)−1

(11)

Here, µk
i is an i-th element of the mean vector, and k repre-
sents sample index in a mini-batch. mi is the average of |µk
i |
in the mini-batch, i.e., mi = PB
i |. Lsparsity means
that through a mini-batch, every unit should be activated to
s in average (s is a hyperparameter), and Linvmax means
that at least one unit should be activated for each sample.

k=1 |µk

3.4. Fine tuning

Now we have acquired the encoder for extracting com-
mon and speciﬁc features separately. As the next step, we
build a event detector network Cψ on the learned common
features µc

B extracted from each image in a pair.

A and µc

y = Cψ(µc) where µc = [µc

A, µc
B]

(12)

Here, [*,*] represents a concatenation of two vectors. We
used cross-entropy loss to train the classiﬁer on a ground
truth label t.

Lf ine = t log y + (1 − t) log (1 − y)

(13)

In the ﬁne-tuning phase, classiﬁer parameters ψ and en-
coder parameters φ are jointly trained. Because common

9330

𝑞𝜙(𝒛|𝒙𝐵)𝑧1𝑧2𝐷1𝐷2𝜎1(𝒙𝐴)𝜎2(𝒙𝐴)𝜎2(𝒙𝐵)𝜎1(𝒙𝐵)𝑞𝜙(𝒛|𝒙𝐴)𝐷1=𝑧1,𝐴−𝑧1,𝐵2𝐷2=𝑧2,𝐴−𝑧2,𝐵2ℒ=𝐷1+𝐷2ℒ=𝐷1𝜎1,𝐴𝜎2,𝐵+𝐷2𝜎2,𝐴𝜎2,𝐵features represents image contents that are invariant to triv-
ial events, a robust event detector can be effectively trained
even with a small amount of labels. During the ﬁne-tuning
phase, negative samples are randomly under-sampled to
have the same number of samples as positives.

4. Experiments

In this section, we veriﬁed the effectiveness of our
method on four change detection datasets: Augmented
MNIST, ABCD, PCD, and WDC dataset. We used Aug-
mented MNIST for comparing our method with other meth-
ods such as Mathieu et al. [17] which are commonly evalu-
ated on relatively simple datasets (e.g., MNIST). After that
we evaluated our method on in-the-wild datasets (ABCD,
PCD, and WDC dataset). While all the datasets originally
contained many positive samples, we limited the available
positive samples to simulate a class-imbalance scenario.
The numbers of positive and negative samples used in the
experiments are listed in Table 1. In Section 4.4, we con-
ducted the qualitative evaluation by visualizing learned fea-
tures. Finally, in Section 4.5, we investigated several design
choices of our model.

4.1. Datasets

Augmented MNIST. To validate the proposed model, we
set a problem of detecting a change of digit from a pair of
samples in MNIST. An input image pair is labeled as posi-
tive if the digits in the pair are different and labeled as neg-
ative if they are the same. For source images, we use three
variants of MNIST [29]: MNIST with rotation (MNIST-R),
background clutter (MNIST-B), and both (MNIST-R-B).

ABCD dataset. The ABCD dataset [10] is a dataset for
detecting changes in buildings from a pair of aerial images
taken before and after a tsunami disaster. The task is to clas-
sify whether the target buildings were washed away by the
tsunami or not. Training and test patches were resized and
cropped in advance such that the target buildings were in
the center (i.e., we used “resized” patches as used in [10]).

PCD dataset. The PCD dataset [21] is a dataset for detect-
ing scene changes from a pair of street view panorama im-
ages. For each pair, pixel-wise change masks are provided
as ground truth. In this work, we solved the change mask
estimation problem by conducting patch-based classiﬁca-
tion. First, input patch pairs of size 112 × 112 were cropped
from original images, then they were labeled as positive if
the center area of size 14×14 was purely changed pixels and
labeled as negative if the center area was purely unchanged
pixels. In the testing phase, we cropped the patch pairs in a
sliding manner, and overlaid the classiﬁer outputs to create
a heatmap of change probabilities. The heatmap was then

Table 1. Number of positive and negative samples used in each
dataset. All the negative samples were used for representation
learning. In ﬁne-tuning, both the negative and positive samples
were used.

Training

Testing

#negatives

#positives

#negatives

#positives

Aug. MNIST

ABCD
PCD
WDC

100,000

3374
56718
250,000

50 / 500 / 32,000

50,000

50,000

5 / 50 / 3378

50

50 / 500

847

-

1934

845

-

1934

Table 2. Comparison to the anomaly detection methods on Aug-
mented MNIST dataset. For all models, only negative samples
were used during training.

MNIST-R MNIST-B MNIST-R-B

AE-rec [32]
VAE-rec [1]
CAE-l2 [2]
MLVAE [5]

Mathieu et al. [17]

VAE w/o sim.

VAE w/ sim. (ours)

54.27
57.24
55.14
60.72
58.34
54.95
71.66

54.48
53.27
55.74
59.70
60.31
56.44
82.55

51.36
50.7
50.29
52.75
52.16
52.02
62.23

binarized using a threshold of 0.5, which results in change
mask estimation.

WDC dataset. In order to evaluate our method on a more
large scale dataset, we prepared a new change detection
dataset. This dataset is for detecting newly constructed or
destructed buildings from a pair of aerial images of Wash-
ington D.C. area. The dataset contains images of multi-
ple years (1995, 1999, 2002, 2005, 2008, 2010, 2013 and
2015). They have 16 cm resolution, and covers over 200
km2 for each year. We automatically annotated changes
in buildings by comparing the building footprints produced
at different years. All the images and the footprints are ac-
quired from open data repository hosted by the Government
of District of Columbia [28]. For more detail about the
dataset, please refer to the supplementary material.

4.2. Experimental setup

Baselines. For comparison, we built several baseline mod-
els for handling the class-imbalance problem. (1) Random
under/over-sampling: a straightforward approach for class-
imbalance problem is under-sampling of major class in-
stances or over-sampling of minor class instances. For each
sampling schemes, we trained a siamese CNN (the state-of-
the-art architecture for image comparison tasks). (2) Trans-
fer learning: transfer learning is considered to be effective
when the number of available labels are limited. We trans-
ferred weights from the ImageNet pre-trained models, and
ﬁne-tuned it with under-sampling scheme.
(3) Disentan-
gled representation learning methods: For comparison with
the state-of-the-art representation learning models, we tried
[5, 17] to acquire common features. In the original formu-

9331

Table 3. Change detection accuracies on Augmented MNIST dataset. The number of positive samples were varied from 50 to 32,000. Each
result is given in terms of the mean and standard deviation obtained by 10 training runs using different training subsets.

#Labels Under samp. Over samp. MLVAE [5] Mathieu et al. [17] VAE w/o sim. VAE w/ sim. (ours)

MNIST-R

MNIST-B

MNIST-R-B

50
500

32000

50
500

32000

50
500

32000

50.63(0.31)
60.05(3.10)
94.82(0.21)
50.69(0.61)
52.04(1.52)
94.92(0.21)
50.30(0.11)
50.35(0.12)
79.04(0.25)

50.47(0.44)
61.84(1.37)
95.49(0.15)
50.38(0.16)
52.27(2.80)
93.28(0.15)
50.37(0.08)
50.47(0.19)
75.94(0.80)

57.22(1.39)
79.15(0.90)
95.68(0.17)
59.33(2.25)
72.26(0.96)
95.67(0.10)
51.61(0.67)
56.21(0.27)
78.73(0.26)

61.09(1.20)
77.78(0.74)
95.85(0.23)
58.79(2.66)
75.16(1.09)
94.47(0.29)
51.19(0.51)
53.10(0.93)
78.55(1.17)

51.55(0.43)
64.74(1.31)
95.76(0.09)
52.67(1.44)
73.56(2.24)
96.25(0.06)
50.32(0.28)
52.39(0.49)
80.92(0.41)

79.65(4.42)
89.73(0.56)
95.94(0.15)
82.16(0.37)
84.69(0.42)
96.05(0.13)
60.58(1.60)
62.68(0.46)
81.54(0.57)

Table 4. Change detection accuracies on the ABCD, WDC and PCD dataset. On the column of ABCD and WDC dataset, accuracies are
presented for different numbers of positive samples. On PCD dataset, the performance is reported for three evaluation metrics (Accuracy,
mIoU, and IoU for positive class). The number of positive samples used for PCD dataset is 50. Each result is given in terms of the mean
and standard deviation obtained by 10 training runs using different training subsets.
WDC

ABCD

Under samp.
Over samp.

Transfer

MLVAE [5]

Mathieu et al. [17]

VLAE w/o sim.

VLAE w/ sim. (ours)

#Labels 5

61.14(11.61)
60.88(13.58)
77.39(7.30)
65.36(5.19)
64.73(5.41)
67.32(6.51)
78.52(5.01)

50

All

64.05(17.16)
54.05(11.78)
88.17(0.75)
86.31(1.80)
77.66(2.11)
86.69(1.79)
89.70(0.77)

95.24(0.20)
92.91(0.39)
96.03(0.19)
95.33(0.19)
91.79(0.21)
95.18(0.14)
95.60(0.14)

#Labels 50
53.12(4.56)
52.02(3.37)
61.32(1.73)
63.58(1.59)
60.54(2.80)
59.41(1.68)
63.25(0.86)

500

Acc.

51.72(3.03)
52.09(4.80)
71.07(3.04)
74.70(0.77)
71.55(0.69)
74.17(1.05)
75.70(0.66)

73.28(3.10)
80.52(3.48)
75.59(2.58)
76.88(1.22)
73.71(3.55)
77.22(1.75)
78.20(1.96)

56.27(3.32)
60.88(3.68)
58.74(2.77)
60.13(1.50)
56.63(3.59)
60.49(2.27)
61.66(2.23)

IoU

47.95(2.20)
44.92(3.49)
49.60(2.18)
50.55(1.75)
48.02(2.13)
50.73(2.70)
51.77(1.84)

PCD
mIoU

lation of [17], the discriminator requires class labels as its
additional input. However, since we have no access to the
class labels, we used image pair instead (i.e., discriminate
real-generated and real-real pairs). (4) Anomaly detection
methods: we also tried several anomaly detection methods
from [1, 2, 32]. To apply the methods, images in each pair
are concatenated and regarded as a single data point. The
models are trained using only negative (i.e. normal) data,
and rare events are detected as outliers.

Model architecture for representation learning. We built
two architectures: one for Augmented MNIST dataset and
another for the rest of the datasets. For Augmented MNIST
dataset, the encoder had a simple architecture of “C-P-C-
P-C-H”, where C, P, and H represent convolution, max-
pooling, and hidden layer, respectively. Here, the hidden
layer consists of four branches of convolutional layers, each
of which extract mean and log-variance of speciﬁc and com-
mon features. For the rest of the dataset, in order to model
complex real-world scenes, we used a hierarchical latent
variable model proposed in [35], where a particular image
is modeled by a combination of multiple latent variables
with different levels of abstraction. Speciﬁcally, we used a
model with 5 hidden layers in the experiments. Because tar-
get events are often related to high-level image contents, the
common features were extracted only on the top two hidden
layers. For both architectures above, the decoder part was
set to be symmetric to its encoder. For the detailed archi-
tecture and the hyper-parameter settings, please refer to the

supplementary materials.

Model architecture for ﬁne-tuning.
In the ﬁne-tuning
phase, we attached an event detector consisting of three
fully-connected layers. The dimensions of the layers were
100-100-2 for Augmented MNIST dataset and 2048-2048-2
for the rest of the datasets. During ﬁne-tuning, the learning
rate of the pre-trained encoder part was down-weighted by
a factor of 10.

4.3. Quantitative results

Table 3 shows the results for Augmented MNIST dataset.
When labels were scarce,
the proposed method outper-
formed the other models by a large margin. By compar-
ing the models with and without similarity loss (“VAE w/o
sim.” and “VAE w/ sim.”), we can conclude that the pro-
posed similarity loss is essential to learn better representa-
tions for the change detection task. The performance im-
provement is especially remarkable with 50 labels, where
the proposed model improved by approximately 20-30%
compared to the baselines.

In Table 2, we also compare our method to several
anomaly detection methods. In this case, we did not train
the event detector. Instead, we detected change events by
applying k-means clustering to the distance between com-
mon features.
In the table, the proposed method outper-
forms the other models.

Table 4 shows the results for the ABCD, WDC and PCD
datasets, respectively. Also, for these in-the-wild datasets,

9332

(a) Source images and ground truth

(b) Estimated change mask

(c) Source images and ground truth

(d) Estimated change mask

Figure 4. Examples of mask estimation results on the PCD dataset. From top to bottom, the ﬁgures in the columns b and d shows the result
of “Under samp.”, “Transfer”, and “VLAE w/ sim. (ours)”, respectively.

Interpolate common

Interpolate specific

Figure 5. Results of feature interpolation analysis. (a) Interpolation of common features. (b) Interpolation of speciﬁc features.

(a)

(b)

Common

Specific

Common

Specific

Common

Specific

(a) MNIST-R

(b) MNIST-B

(c) MNIST-R-B

Figure 6. Results of t-SNE visualization for common and speciﬁc features. The color of each plot represents digit classes.

the proposed method outperformed the other baselines. Fig-
ure 4 compares the estimated change mask for baseline
models and that of the proposed model. We see that the
baseline models are sensitive to illumination changes or
registration errors in roads or buildings. Clearly, they suf-
fer from false alarms created by trivial events. On the
other hand, much of the false alarms were successfully sup-
pressed in the output of the proposed model.

4.4. Visualization of Latent Variables

In this subsection, we investigate what is encoded in
common features and speciﬁc features by visualizing them.
Interpolation: we generated a sequence of images by lin-
early interpolating image representations between pairs of
images. To independently investigate the learned semantics
of common and speciﬁc features, the features were interpo-
lated one at a time while ﬁxing the others. Figure 5 shows
the result of visualization on Augmented MNIST dataset.
When common features were interpolated between differ-

ent digits, the digit classes in the generated sequences grad-
ually changed accordingly, while the other factors (i.e., ro-
tation, styles, and background) were unchanged. On the
other hand, when speciﬁc features are interpolated, rota-
tion angles or background patterns are changed accordingly,
while the digit classes remained the same. The result shows
that the common features extract information about digit
classes, but they are invariant to the variation observed in
the same digit pairs. 2D visualization: we visualized the
learned features by t-SNE [30]. Figure 6 shows the visual-
ization results for common and speciﬁc features. In this ﬁg-
ure, the same color plots correspond to the same digits. We
see that the common features are more informative about
digit classes compared to the speciﬁc features.

We also conducted the above visualization for the rest
of the datasets. However, for the real-world complicated
scenes, it was difﬁcult to achieve clear disentanglement.
Speciﬁcally, we observed that the activations of the units
in the common features are degenerated to a certain value.

9333

sparsity+invmax

sparsity

w/o activation loss

]

%

[
 
y
c
a
r
u
c
c
A

85

80

75

70

65

]

%

[
 

y
c
a
r
u
c
c
A

95

90

85

80

75

84.24

85.04

84.59

89.45

90.01

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

1

2

3

4

5

Sparsity parameter

Number  of hidden  layers

Figure 7. Analysis of the effect of activation loss. Results for dif-
ferent sparsity parameter values are shown with red plots (with-
out invmax-loss) and blue plots (with invmax-loss). The error bar
shows the standard deviations of the accuracies for 10 runs with
different training subsets

Figure 8. Sensitivity analysis of the number of hidden layers. The
error bar shows the standard deviations of the accuracies for 10
runs with different training subsets.

Table 5. Comparison of the choices of distance function used in
similarity loss.

4.5. Ablation Study

Effect of activation loss. To investigate the effect of ac-
tivation loss (Eq. (9)), we conducted sensitivity analysis on
the sparsity parameter and existence of Linvmax. Figure 7
shows the results for MNIST-R. From these results, we
can draw several conclusions. First, a sparsity parameter
of approximately 0.5 seems to be suitable because the
performance becomes unstable in terms of the choice of
parameter value at larger than 0.5. Secondly, the use of
Linvmax boosts performance. Lastly and most importantly,
regardless of the parameter choices, the presence of activa-
tion loss improves the performance.

Choice of the distance function for similarity loss.
Here, we investigate several choices of distance function
in Eq. (7). Table 5 compares six types of distance function
evaluated on the MNIST-R and ABCD datasets. We found
that both Mahalanobis distance and Jeffreys divergence
are suitable choices. This result supports our intuition that
we should consider not only the mean vector of the latent
distribution but also the shape of the distribution.

latent variables.

Importance of hierarchical
Here,
we investigate the effect of using hierarchical latent vari-
able models for representation learning.
In this analysis,
the hidden layers were eliminated one by one from the
proposed model in order of the lowest layer to the highest.
Figure 8 shows the results on the ABCD dataset.
In the
ﬁgure, the models with 4 and 5 hidden layers perform
better. This result shows the importance of extracting
hierarchical latent variables for rare event detection of
complicated real-world scenes.

5. Conclusion

We proposed a novel representation learning method to
overcome the class-imbalance problem in rare event detec-
tion tasks. The proposed network learns the two separated

L2
L1

Cosine
MMD

Jeffrey’s Divergence

Mahalanobis

MNIST-R
82.16(0.64)
79.14(0.90)
60.94(0.97)
62.30(0.50)
86.90(0.32)
89.73(0.56)

ABCD

90.13(1.31)
89.01(1.44)
89.63(1.51)
89.69(1.35)
89.85(0.98)
89.70(0.77)

features related to image contents and other nuisance factors
from only low-cost negative samples by introducing a simi-
larity constraint between the image contents. The learned
features are utilized in the subsequent ﬁne-tuning phase,
where rare event detectors are learned robustly. The effec-
tiveness of the proposed method was veriﬁed by the quanti-
tative evaluations on the four change detection datasets. For
the evaluations, we created a large-scale change detection
dataset using publicly available data repository. In addition,
the qualitative analysis on Augmented MNIST showed that
the model successfully learns the desired disentanglement.
The disentanglement of the proposed method is still in-
sufﬁcient for complicated scenes in the real world, due to
degenerated solution observed in the common features. The
performance of our method will be greatly improved with
the clearer feature disentanglement. A possible next step to
achieve this is to avoid degenerated solution by introducing
adversarial training as used in [17], or maximizing mutual
information between the common feature and input images
[8]. Also, in the future, we intend to apply the learned in-
variant features to various types of event detection tasks in-
cluding change mask estimation and change localization.

Acknowledgement

This paper is based on results obtained from a project
commissioned by the New Energy and Industrial Technol-
ogy Development Organization (NEDO). We thank Nevrez
Imamoglu for the discussions and suggestions about the dis-
tance metrics. Also, we thank Motoki Kimura for his help
in preparing WDC dataset.

9334

[20] K. Sakurada and T. Okatani. Change Detection from a Street
Image Pair using CNN Features and Superpixel Segmenta-
tion. In BMVC, 2015. 1

[21] K. Sakurada and T. Okatani. Change Detection from a Street
Image Pair using CNN Features and Superpixel Segmenta-
tion. BMVC, 2015. 5

[22] N. Siddharth, B. Paige, J.-W. van de Meent, A. Desmai-
son, N. D. Goodman, P. Kohli, F. Wood, and P. H. S.
Torr. Learning Disentangled Representations with Semi-
Supervised Deep Generative Models. NIPS, 2017. 2

[23] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and
F. Moreno-Noguer. Discriminative learning of deep convo-
lutional feature point descriptors. In ICCV, 2015. 1

[24] C. Stauffer and W. E. L. Grimson. Adaptive background mix-

ture models for real-time tracking. CVPR, 1999. 2

[25] S. Stent, R. Gherardi, B. Stenger, and R. Cipolla. Detecting
Change for Multi-View, Long-Term Surface Inspection. In
BMVC, 2015. 1

[26] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
pages 1988–1996. Curran Associates, Inc., 2014. 1

[27] L. Tran, X. Yin, and X. Liu. Disentangled Representation
Learning GAN for Pose-Invariant Face Recognition. CVPR,
2017. 2

[28] http://opendata.dc.gov/pages/

dc-from-above. DC GIS program. 5

[29] http://www.iro.umontreal.ca/˜lisa/twiki/
bin/view.cgi/Public/MnistVariations. Aug-
mented MNIST. 5

[30] L. van der Maaten and G. Hinton. Visualizing Data using t-
SNE. Journal of Machine Learning Research, 9:2579–2605,
2008. 7

[31] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In ECCV,
pages 499–515. Springer, 2016. 1

[32] Y. Xia, X. Cao, F. Wen, G. Hua, and J. Sun. Learning dis-
criminative reconstructions for unsupervised outlier removal.
In ICCV, 2015. 5, 6

[33] S. Zagoruyko and N. Komodakis. Learning to compare im-
In CVPR,

age patches via convolutional neural networks.
2015. 1

[34] J. Zbontar and Y. LeCun. Stereo matching by training a con-
volutional neural network to compare image patches. JMLR,
17(1), 2016. 1

[35] S. Zhao, J. Song, and S. Ermon. Learning Hierarchical Fea-

tures from Generative Models. ICML, 2017. 6

References

[1] J. An and S. Cho. Variational autoencoder based anomaly
detection using reconstruction probability. SNU Data Mining
Center, Tech. Rep., 2015. 5, 6

[2] C. Aytekin, X. Ni, F. Cricri, and E. Aksu. Clustering and un-
supervised anomaly detection with l2 normalized deep auto-
encoder representations. arXiv preprint arXiv:1802.00187,
2018. 5, 6

[3] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky.
Neural codes for image retrieval. In ECCV, pages 584–599.
Springer, 2014. 1

[4] O. Barnich and M. Van Droogenbroeck. ViBe: A universal
background subtraction algorithm for video sequences. IEEE
Transactions on Image Processing, 20(6):1709–1724, 2011.
2

[5] D. Bouchacourt, R. Tomioka, and S. Nowozin. Multi-
Level Variational Autoencoder: Learning Disentangled Rep-
resentations from Grouped Observations.
arXiv preprint
arXiv:1705.08841, 2017. 2, 5, 6

[6] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and

D. Erhan. Domain Separation Networks. NIPS, 2016. 2

[7] M. Chen, L. Denoyer, and T. Artieres. Multi-view data gen-

eration without view supervision. ICLR, 2018. 2

[8] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel.
InfoGAN: Interpretable Representation
Learning by Information Maximizing Generative Adversar-
ial Nets. NIPS, 2016. 2, 8

[9] Z. Chen, X. Sun, L. Wang, Y. Yu, and C. Huang. A deep
visual correspondence embedding model for stereo matching
costs. In ICCV, 2015. 1

[10] A. Fujita, K. Sakurada, and T. Imaizumi. Damage Detec-
tion from Aerial Images via Convolutional Neural Networks.
MVA, 2017. 5

[11] A. Fujita, K. Sakurada, T. Imaizumi, R. Ito, S. Hikosaka,
and R. Nakamura. Damage Detection from Aerial Umages
via Convolutional Neural Networks. In MVA, 2017. 1

[12] L. Gueguen and G. Street. Large-Scale Damage Detection

Using Satellite Imagery. CVPR, 2015. 2

[13] S. Khan, X. He, F. Porikli, M. Bennamoun, F. Sohel, and
R. Togneri. Learning deep structured network for weakly
supervised change detection. IJCAI, 2017. 2

[14] S. H. Khan, X. He, F. Porikli, M. Bennamoun, F. Sohel, and
R. Togneri. Learning Deep Structured Network for Weakly
Supervised Change Detection. In IJCAI, 2017. 1

[15] D. P. Kingma and M. Welling. Auto-Encoding Variational

Bayes. ICLR, 2014. 3

[16] T. Kulkarni, W. Whitney, P. Kohli, and J. Tenenbaum. Deep

Convolutional Inverse Graphics Network. NIPS, 2015. 2

[17] M. Mathieu, J. Zhao, P. Sprechmann, A. Ramesh, and Y. Le-
Cun. Disentangling factors of variation in deep representa-
tions using adversarial training. NIPS, 2016. 2, 5, 6, 8

[18] A. Odena, C. Olah, and J. Shlens. Conditional Image
Synthesis With Auxiliary Classiﬁer GANs. arXiv preprint
arXiv:1610.09585, 2016. 2

[19] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic
backpropagation and approximate inference in deep genera-
tive models. arXiv preprint arXiv:1401.4082, 2014. 3

9335

