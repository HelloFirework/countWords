Deep Transfer Learning for Multiple Class Novelty Detection

Pramuditha Perera and Vishal M. Patel

Department of Electrical and Computer Engineering

Johns Hopkins University, Baltimore, MD 21218, USA

pperera3@jhu.edu, vpatel36@rutgers.edu

∗

Abstract

We propose a transfer learning-based solution for the
problem of multiple class novelty detection. In particular,
we propose an end-to-end deep-learning based approach in
which we investigate how the knowledge contained in an
external, out-of-distribution dataset can be used to improve
the performance of a deep network for visual novelty de-
tection. Our solution differs from the standard deep clas-
siﬁcation networks on two accounts. First, we use a novel
loss function, membership loss, in addition to the classi-
cal cross-entropy loss for training networks. Secondly, we
use the knowledge from the external dataset more effec-
tively to learn globally negative ﬁlters, ﬁlters that respond
to generic objects outside the known class set. We show that
thresholding the maximal activation of the proposed net-
work can be used to identify novel objects effectively. Ex-
tensive experiments on four publicly available novelty de-
tection datasets show that the proposed method achieves
signiﬁcant improvements over the state-of-the-art methods.

1. Introduction

In recent years, intelligent systems powered by artiﬁ-
cial intelligence and computer vision that perform visual
recognition have gained much attention [8],[12],[1],[25].
These systems observe instances and labels of known ob-
ject classes during training and learn association patterns
that can be used during inference. A practical visual recog-
nition system should ﬁrst determine whether an observed
instance is from a known class. If it is from a known class,
then the identity of the instance is queried through classiﬁ-
cation.

The former process is commonly known as novelty de-
tection (or novel class detection) [14] in the literature.
Given a set of image instances from known classes, the
goal of novelty detection is to determine whether an ob-
served image during inference belongs to one of the known

∗This work was supported by the NSF grant 1801435.

Figure 1. Novelty detection in dog-breed classiﬁcation. Left:
Sample images. Right: Feature representation. Both known (ﬁrst
row) and novel (second row) images are images of dogs. Given
known images, the goal of novelty detection is to reject novel im-
ages. In order to do so, the knowledge of out-of-distribution im-
ages (ﬁnal row), in this case non-dog images, are used to learn a
suitable representation.

classes. Novelty detection is generally a more challenging
task than out-of-distribution detection [29], [9] since novel
object samples are expected to be from a similar distribution
to that of known samples.

In practice, the knowledge of unknown classes is not
entirely absent. Given a set of known classes from a cer-
tain problem domain, generally unknown class data from
the same problem domain is unavailable. However, in
some cases it is possible to obtain data outside the known
class from different problem domains, which we refer to as
out-of-distribution samples. For example, for a dog-breed
recognition application, ImageNet dataset [21] that contains
images of objects may be considered as out-of-distribution
data as shown as in Figure 1. However, since the out-of-
distribution data are from a different problem domain, they
do not approximate the distribution of the novel samples
well.

Nevertheless, since the deep-models produce generaliz-
able features, the knowledge of out-of-distribution samples
can be transferred to the original problem to aid novelty de-
tection. When the problem considered is a c class prob-
lem, and when the out-of-distribution data of C classes are
available, the following three strategies are used to transfer
knowledge for novelty detection in the literature:

111544

Known Classes (Dogs)Novel classes (Dogs)Out-of-distributional classesNovelty DetectorKnownNovelOut-of-distributionsamples1. Fine-tuning: Network is ﬁrst pre-trained on the out-of-
distribution data and later ﬁne-tuned on the training data of
the given domain. Novelty is queried by thresholding the
ﬁnal activation score [2].
2. Feature Extraction: Conventional novelty detection
techniques [4],[13],[24] are used based on the ﬁne-tuned
features.
3. Fine-tune (c+C): Network is ﬁrst pre-trained on the out-
of-distribution data. Both the training data and the out-of-
distribution data are used to perform ﬁne-tuning in (c + C)
classes together. Novelty is determined in the same way as
in approach 1.

We note that in all these baselines, the out-of-distribution
data is employed in the training process. In fact, any nov-
elty detection method operating on the pre-trained/ﬁnetuned
deep features are implicitly making use of the out-of-
distribution data. In this work, we introduce a new frame-
work to perform novelty detection based on transfer learn-
ing. First, we show that using cross-entropy loss alone for
training is not optimal for the novelty detection task. Sec-
ondly, we empirically show that the out-of-distribution data
can be used more effectively in training to produce better
novelty detection performance with respect to considered
baseline solutions. Speciﬁcally, we make following primary
contributions in this paper.
1. We propose an end-to-end novelty detection framework
based on deep learning. To the best of our knowledge, this is
one of the ﬁrst end-to-end deep learning solutions targeting
visual novelty detection.
2. We introduce a new loss function, membership loss
which has a similar functionality to that of the cross-entropy
loss but encourages an embedding that produces high acti-
vation for known object classes consistently.
3. We propose to take advantage of large-scale external
datasets to learn the globally negative ﬁlters to reduce high
activations caused by the novel images.
4. We show empirically, that the proposed method outper-
forms the baseline novelty detection methods across four
publicly available datasets.

2. Related Work

Object classiﬁcation schemes are often equipped with
a suitable mechanism to detect novel objects. For exam-
ple, Eigenfaces [28] was accompanied by a reconstruc-
tion error-based novel object detection method. In sparse
representation-based classiﬁcation (SRC) algorithm [30],
Sparsity Concentration Index (SCI) was proposed for the
same purpose. In contrast, there is no formal novelty de-
tection mechanism proposed for deep-learning based clas-
siﬁcation. In its absence, thresholding the highest class ac-
tivation score of the deep model has been used as a base-
line in the literature [2]. As an alternative, several recent
works have proposed novelty detection schemes based on

deep features [2],[24]. In the same spirit, it is also a possi-
bility to use classical novelty detection tools such as Kernel
PCA [10], Kernel null space-based novelty detection (KN-
FST) [4] and its variants [3],[13] on deep features. KNFST
operating on deep-features produces the current state of the
art performance in visual novelty detection [13]. However,
advantages of deep-learning are not properly exploited in
all of these approaches due to the absence of an end-to-end
learning framework.

On the other hand, novelty detection problem has
a close resemblance to both anomaly detection [17],
[19],[20], [18], [5],[16], and open-set recognition problems
[22],[2],[7],[15]. Therefore, it is possible to solve anomaly
detection using tools proposed in these alternative domains.
In anomaly detection, given a single normal class, the ob-
jective is to detect out-of-class instances. One-class SVM
[23] and SVDD [27] are two of the most widely used tools
in anomaly detection. Novelty detection can be viewed as
an anomaly detection problem if all known classes are con-
sidered as a single augmented class. On the other hand,
objective in open-set recognition (OSR) is similar to that
of novelty detection. But in addition, OSR requires correct
classiﬁcation of samples detected as known samples. There-
fore, it is also possible to use open-set recognition tools to
perform novelty detection. However, we note that due to
subtle differences in objectives, OSR algorithms are not op-
timal for novelty detection.

In the proposed framework, maximal activation of the ﬁ-
nal layer of a deep network is considered as a statistic to per-
form novelty detection. We design the network and choose
loss functions appropriately so that this statistic is low for
novel objects compared to the known object classes.

3. Background

In this section, we brieﬂy review how deep networks
produce activations in response to input stimuli. Based on
this foundation, we introduce the notion of positive ﬁlters
and negative ﬁlters. Consider a c class fully-supervised
object classiﬁcation problem with a training image set
x = x1, x2, . . . , xn and the corresponding labels y =
y1, y2, . . . , yn where yi ∈ {1, 2, . . . c}. Deep convolutional
neural networks (CNNs) seek to learn a hierarchical, convo-
lutional ﬁlter bank with ﬁlters that respond to visual stimuli
In c class classiﬁcation, the top most
of different levels.
convolutional ﬁlter activation g is subjected to a non-linear
transformation to generate the ﬁnal activation vector f ∈ Rc
(for example, g is the conv5-3 layer in VGG16 and conv5c
in Resnet50. f is the fc8 and fc1000 layers in the respective
networks). In a supervised setting, network parameters are
learned such that arg max f = yi for ∀i ∈ {1, 2, . . . , n}.
This is conventionally done by optimizing the network pa-
rameters based on the cross-entropy loss.

If there exist k ﬁlters in the top most convolution ﬁlter

11545

negative ﬁlters for the considered class using the DeepVis
toolbox [31] (these are the images that are most likely to ac-
tivate the corresponding ﬁlters). By observation, we notice
that the top positive ﬁlters are activated when the network
observes structures similar to snakes. On the other hand, the
top negative ﬁlters are unrelated to the appearance of sand
snakes.

4. Deep Novelty Detection

Based on the above background, we propose to learn the
distributions of known object classes using a CNN frame-
work with the objective of performing joint classiﬁcation
and novelty detection. In our formulation, assuming each
known class has a unique single label, we force the ﬁnal ac-
tivation vector f to model the probability distribution vec-
tor of known classes. Formally, for a given data-label pair
(xi, yi), we expect fi = 1 and fj = 0, ∀j 6= i. Once such a
representation is learned, arg max f returns the most-likely
class of an observed sample. Then, max f yields the likeli-
ness of the sample belonging to the most likely class. Sim-
ilar to binary classiﬁcation, identity I of a test instance can
be queried using hard thresholding. In order to learn a rep-
resentation suitable for the stated objective, we use conven-
tional classiﬁcation networks as the foundation of our work
and propose the following two alternations.
1. Membership loss. Assuming each known class has a
unique single label, if the observed image is from a known
class, only a single positive activation should appear in f .
We observe that when cross-entropy loss is used, this is not
the case. To alleviate this, we introduce a new loss called
membership loss in addition to the cross-entropy loss.
2. Globally negative ﬁlters. In a classiﬁcation setting, a
negative ﬁlter of a certain class is also a positive ﬁlter of an-
other class. In other words, there exist no explicit negative
ﬁlters. In our formulation, we propose to generate globally
negative ﬁlters (ﬁlters that generate negative evidence for all
known classes) to reduce the possibility of a novel sample
registering high activation scores.

4.1. Limitations of Cross Entropy Loss

Pl=1

When a classiﬁcation network is trained, each element
fi of the activation vector f is ﬁrst normalized using the
softmax function to arrive at a normalized activation vector
˜f as in, ˜fj = efj /
efl . When it is assumed that all image

c

classes appearing during inference are known ahead of time,
jth element of vector ˜f is interpreted as the likelihood of the
input image xi belonging to the jth class. Neural network-
based classiﬁcation systems are learned by minimizing the
cross-entropy loss which is the negative log likelihood of
the correct class ˜f . However, since this is a relative measure,
the learned representation deviates from our objective due
to the following reasons.

11546

Figure 2. Positive and negative ﬁlters of the sand snake class in the
Resnet50 trained on ILSVRC12 dataset. Top: weights of the fully
connected layer corresponding to the sand snake class. We call
ﬁlters associated with positive weights as positive ﬁlters of sand
snake class. All other ﬁlters are named as negative ﬁlters. Bottom:
Visualization of top negative and positive ﬁlters. These patterns
are likely to produce high activation in these ﬁlters. We note top
positive ﬁlters are activated by snake-like structures.

bank, its output g is a set of k number of activation maps.
The ﬁnal activation vector of the network f is a function of
g. For a given class i, there exists some ki ﬁlters in the ﬁlter
bank (1 ≤ ki ≤ k) that generally generates positive activa-
tion values. These activations provide supporting (positive)
evidence that an observed image is from class i. Conversely,
all remaining ﬁlters provide evidence against this hypothe-
sis. Activation score of each class in f is determined by
taking into account the evidence for and against each class.
For the remainder of the paper, we call ﬁlters that provide
evidence for and against a particular class as positive ﬁlters
and negative ﬁlters of the class, respectively.

This concept can be easily explained by taking the
Resnet architecture [8] as an example.
In Resnet, ﬁnal
convolution output g is subjected to global average pool-
ing followed by a fully connected layer. Therefore, the ith
component of the ﬁnal activation vector f can be written as
fi = Wi × GAP (g), where GAP is global average pooling
operation (mean of ﬁlter map) and W is the weight matrix
of the fully connected layer. Here, activation of the ith class
is a weighted summation of mean feature maps found in g.
From the above deﬁnition, ﬁlters associated with positive
weights for a given class in W can be identiﬁed as positive
ﬁlters for that particular class. Conversely, ﬁlters associ-
ated with the negative weights become negative ﬁlters of
the class.

For example consider the Sand Snake class appearing in
the ILSVRC12 dataset [21]. Shown in Figure 2 (top) are
the weights associated with the Sand Snake class in the ﬁ-
nal fully connected layer of the Resnet50 network trained
on the ILSVRC12 dataset. We recognize ﬁlters associated
with positive and negative weights as positive and negative
ﬁlters, respectively for the given class. In Figure 2 (bottom)
we visualize per-unit visualization of top positive and top

0.431  0.3030.247-0.076 -0.07585389981Top Positive FiltersTop NegativeFilters-0.0942075791483Figure 3. (a) Activations of known (Calculator) and unknown samples (Playing Cards) in a VGG16 model. In conventional CNN, both
known and unknown samples activates similar conv5-3 ﬁlters and results in a similar fc8 activation map. Novelty detection of the novel
sample fails due to high activation scores present in fc8 layer.
In the proposed method, Calculator object activates ﬁlters related to
Calculators whereas top activated ﬁlters in Playing Cards is unrelated to known classes (globally negative). Since all activations in fc8
are very small for the Playing Cards object, it can be detected as a novel sample by thresholding. (b) Top positive ﬁlters and top globally
negative ﬁlters of the calculator class.

Firstly, even a low activation of the ground truth class
could yield a low cross-entropy provided that the activa-
tions of all other (non-matching) classes are very low. As
a result, lower score values may not get heavily penalized
during training. Therefore, a model trained using the cross-
entropy loss may end up producing low activation scores
for known classes during inference. In closed set classiﬁ-
cation, this behavior will not cause complications as long
as the correct class records the highest score. However, in
threshold-based novelty detection, this poses a problem as
having low scores for the positive class will result in false
negatives. Secondly, the cross-entropy loss does not neces-
sarily drive activations of unrelated classes below zero. As
a result, inaccurate cross-class relationships are encouraged
during training.

In order to illustrate this point, we trained a VGG16 [26]
based CNN classiﬁcation network using the ﬁrst 128 classes
of the Caltech256 dataset. For the considered example, the
Calculator class (indexed at 27) is a known class and the
Playing Cards class (indexed at 163) is a novel class. Shown
in Figure 3 are the activations of conv5-3 and fc8 layers of
the network for two inputs of the two classes. As can be
seen from this ﬁgure, when the network observes a calcula-
tor object (known object), it correctly associates the highest
score in f to the correct class (class 27). However, there
is also a signiﬁcant miss-association between the calculator
class and coin (class 43), keyboard (class 45), dice (class
55) and joystick classes (class 120).

4.2. Membership Loss

In our approach, we ﬁrst independently translate each
activation score value fi into the range 0 − 1 using the
sigmoid(σ) function. We interpret each transformed acti-
vation score as the probability of the input image belong-
ing to each individual class. If the ground truth label of a

given observation x is y, we aim at learning a function that
produces absolute probabilities for the membership of each
class as follows

P(y = i) = σ(f (x)i) ∀i ∈ {1, 2, . . . c}.

(1)
Ideally, the learned transformation will produce σ(f (x)i) =
1 for i = y and σ(f (x)i) = 0, otherwise. We denote
the risk of associating a higher score with a wrong class
(σ(f (x)i) = 1 for i 6= y ) as RW 1 and risk of associating
a low score with the correct class (σ(f (x)i) = 0 for i = y)
as RC 0. We deﬁne the membership loss LM as the risk of
classiﬁcation as

LM (x, y) = RC 0(x, y) + RW 1(x, y),

(2)
where λ is a positive scalar. With our formulation, we deﬁne
RW 1(x, y) = [1 − P(y = 1)]2 = [1 − σ(f (x)y)]2. Here,
the quadratic term is introduced to impose a heavy penalty
on very high deviations. Similarly, RC 0(x, y) becomes,

RC 0(x, y) =

1

c − 1

=

1

c − 1

By substitution, we get

c

[P(i = 1)]2

[σ(f (x)i)]2.

Xi=1,i6=y
Xi=1,i6=y

c

LM (x, y) = λ[1 − σ(f (x)y)]2 +

1

c − 1

c

Xi=1,i6=y

[σ(f (x)i)]2.

(3)
Here, the parameter λ controls relative weight given to each
risk source. In our experiments, we set λ = 5. Taking the
partial derivative of the membership loss yields the follow-
ing back-propagation formula
∂LM (x, y)

=(−2λ[1 − σ(f (x)i)] × σ(f (x)i)′

c−1 σ(f (x)i) × σ(f (x)i)′

∂f (x)i

for i = y
for i 6= y,

2

where, σ(f (x)i)′ = σ(f (x)i)(1 − σ(f (x)i)).

The proposed membership loss does not operate on the

11547

conv5_3fc8conv5_3fc8Conventional CNNCalculatorPlaying CardsProposed Method(a)Positive Filters (Calculator)Globally Negative Filters(b)closed-set assumption. It takes individual score values into
account in an absolute sense. Therefore, when the member-
ship loss is used, known samples that produce small activa-
tions will be penalized regardless of the score values of the
other classes. When the membership loss is used together
with the cross-entropy loss, the network learns a represen-
tation that produces relatively higher activation scores for
the correct class. For example, consider the f c8 activation
map of the proposed method for the Calculator object input
shown in Figure 3. There, we observe that the correct class
(indexed at 27) produces a large positive score whereas all
other classes produce negative scores.

4.3. Globally Negative Filters

When a conventional classiﬁcation network is used,
novel images are often able to produce very high activation
scores there by leading to false positive detections. Such
an example is shown in Figure 3(bottom) where a Playing
Cards instance has produced a very high activation score
in the index corresponding to the Calculator class (indexed
at 27). Final activation score of a class is generated based
on the responses of the positive and negative ﬁlters as dis-
cussed in Section 3. Once the network is trained, given an
input of a particular known class, the input stimulates some
positive ﬁlters and negative ﬁlters associated with the class.
If the model is well trained, the response of the positive ﬁl-
ters exceeds the response of the negative ﬁlters to produce a
high positive activation score.

Given this background, it is interesting to investigate
how a novel sample is able to produce a high activation
score. Let us revisit activations of Playing Cards image
(novel image) shown in Figure 3 (bottom). In this exam-
ple, Playing Cards image has stimulated some positive ﬁl-
ters of the Calculator class despite the differences in con-
tent. At the same time, by chance, it has not produced suf-
ﬁcient stimulation in negative ﬁlters of the Calculator class,
thereby producing a large positive activation in f . This can
be clearly observed in Figure 3 where both the Calculator
and the Playing Cards images have activated similar ﬁlters
in the conv5-3 layer.

To this end, we make the following proposal. We wish
to learn a set of ﬁlters that are stimulated generally by natu-
ral images and produce evidence against all known classes.
In other words, these ﬁlters are negative ﬁlters with respect
to all known classes - hence we call them globally negative
ﬁlters. If any of such ﬁlters are stimulated during inference,
it would prove greater evidence that the observed image is
novel. However, this proposal will succeed only if the glob-
ally negative ﬁlters are stimulated by arbitrary images out-
side the known class set.

In order to learn the globally negative ﬁlters, we pro-
pose a joint-learning network structure. In addition to the
known object dataset, we use the out-of-distribution data

samples in training. For the remainder of the paper we re-
fer the out-of-distribution dataset as the reference dataset.
We learn features that can perform classiﬁcation in both the
known dataset and the reference dataset. If the reference
dataset has C classes, once trained, the ﬁlter bank will con-
tain positive ﬁlters of all c + C classes. Filters associated
with the reference dataset will likely act as negative ﬁlters
for all classes in the known dataset, thereby be globally neg-
ative.
In this framework, the globally negative ﬁlters are
likely to respond to arbitrary natural images provided that
the reference dataset is a large-scale diverse dataset.

In Figure 3, we show the impact of using the globally
negative ﬁlters. Visualization of top activated ﬁlters for the
Calculator class are shown at the top in Figure 3(b). As
can be seen from this ﬁgure, these ﬁlters are positively co-
related with the Calculator class. With the new formulation,
we observe that playing cards object activates some extra
ﬁlters which are not in common with the calculator class
(highlighted in red). At the bottom of Figure 3(b) we visu-
alize ﬁlters with the highest activation for the Playing Cards
object. By inspection, these two visualizations look arbi-
trary and do not have an obvious association with any of the
Caltech256 classes. We interpret these ﬁlters as instances
of the globally negative ﬁlters. Due to the availability of
more negative evidence, the overall activation value of the
playing cards object has been drastically reduced.

4.4. Training Procedure

We propose a network architecture and a training mecha-
nism to ensure that the network learns the globally negative
ﬁlters. For this purpose, we use an external multi-class la-
beled dataset which we refer to as the reference dataset.

We ﬁrst select a CNN backbone of choice (this could
be a simple network such as Alexnet [12] or a very
deep/complex structure such as DenseNet [11]). Two par-
allel CNN networks of the selected backbone are used for
training as shown in Figure 4(a). The only difference be-
tween the two parallel networks is the ﬁnal fully-connected
layer where the number of outputs is equal to the number
of classes present in either dataset. For the purpose of our
discussion, we refer the sub-network up to the penultimate
layer of the CNN as the feature extraction sub-network.

Initially, weights of the two feature extraction sub-
networks are initialized with identical weights and they are
kept identical during training. Weights are not shared be-
tween the ﬁnal layer of two parallel networks. During train-
ing, two mini batches from two datasets (reference dataset
(R) and known classes (T)) are considered and they are
fed into the two branches independently. We calculate the
cross-entropy loss (Lce) with respect to the samples of the
reference dataset and both the membership loss (Lm) and
the cross-entropy loss with respect to the samples of known
classes. The cumulative loss of the network then becomes a

11548

linear combination of the two losses as follows,
CumulativeLoss = Lce(R) + α1 Lce(T ) + α2 Lm(T ).
(4)
In our experiments, we keep α1, α2 = 1. The cumulative
loss is back-propagated to learn the weights of the two CNN
branches. Reducing membership loss and cross-entropy
loss with respect to the known-class dataset increases the
potential of performing novelty detection in addition to clas-
siﬁcation as discussed in the preceding sub-section. On the
other hand, having good performance (low cross-entropy
loss) in the reference dataset suggests the existence of ﬁl-
ters that are responsive to generic objects provided that the
reference dataset is sufﬁciently diverse. When classes ap-
pearing in the reference dataset do not intersect with known
classes, these ﬁlters serve as the globally negative ﬁlters.

Figure 4. Proposed architecture for novelty detection. We use an
external multi-class dataset (reference dataset (R)) in addition to
the known object dataset (T). Two parallel CNN networks with
identical structure and weights are used to extract features from
both datasets. We train separate classiﬁer networks operating on
the same feature to perform classiﬁcation in either dataset. Dur-
ing inference, novelty detection is performed by thresholding the
maximal activation of the bottom branch of the network.

4.5. Testing (Novelty Detection)

During inference, we propose to use the setup shown
in Figure 4(b) where we only consider the bottom CNN
branch of the training network. Given a test image x, we
perform a forward pass using the learned CNN network to
obtain the ﬁnal feature f(x). The largest element of σ(f(x)),
max σ(f(x)) is thresholded using a predetermined threshold
γ to arrive at the identity of the test image. If the yielded
score is below the threshold γ, we identify the test sample
In a practical system, threshold γ is chosen
to be novel.
considering the percentile of the matched score distribution
(for example threshold can be chosen to be 95th percentile
if the accepted false negative rate is 5%) . In addition to
the novelty detection procedure, the same network struc-
ture can be used to perform classiﬁcation as well. Here,
arg max σ(f(x)) yields the identity of the predicted class
for the test sample x. We note that this step is identical
to the classiﬁcation procedure used in the standard CNN-
based classiﬁcation.

5. Experimental Setup and Results

In this section, we present experimental results for the
novelty detection task. We ﬁrst describe the baseline meth-
ods used for comparison. Then, we introduce the four
datasets used for evaluation. Finally, we discuss the ob-
tained results followed by the analysis of the proposed
method.

5.1. Baseline Methods

We evaluate the proposed method on four novelty de-
tection databases and we compare its performance with the
standard novelty detection schemes. We use the following
baseline comparisons based on the AlexNet [12] and the
VGG16 [26] features ﬁne-tuned on the given dataset.
1. Finetune [26]: f c8 feature scores of the trained deep
model are thresholded to detect novel samples.
2. One-class SVM [23]: A one-class SVM classiﬁer is
trained for all known classes. The maximum SVM score
is considered during the inference.
3. KNFST [4], [13]: Deep features are normalized and his-
togram intersection kernel method is used to generate inner
products between the samples.
4. Local KNFST [3]: Deep features with histogram inter-
section kernel is considered with 600 local regions.
5. OpenMax [2]: Activations of penultimate layer of a
deep model are used to construct a single channel class-
wise mean activation vectors (MAV) and the corresponding
Weibull distributions.
6. K-extremes [24]: Mean activations of the VGG16 f c7
features are considered for each class and top 0.1 activation
indexes are binarized to arrive at the Extreme Value Signa-
tures.
7. Finetune(c+C): A (c+C) class CNN is trained by treat-
ing classes of the reference dataset as the additional class.
In addition, we evaluate the performance based on the pre-
trained deep features (trained on the ILSVRC12 database)
for KNFST and local KNFST methods. Whenever pre-
trained features are use they are denoted by the sufﬁx pre.

5.2. Datasets

We use four publicly available multi-class datasets to
evaluate the novelty detection performance of the proposed
method.

Figure 5. Sample images from the evaluation datasets. Each col-
umn contains images taken from a single class of each dataset.

11549

Caltech-256CUB-200DogsFounderType-200Caltech256 Dataset. The Caltech256 dataset is a fully an-
notated dataset which consists of 30607 images from 256
object classes. Following the protocol presented in [13], we
ﬁrst sorted the class names alphabetically and picked the
ﬁrst 128 classes as the known classes and considered the
images from the remaining 128 classes as the novel images.
Caltech-UCSD Birds 200 (CUB 200) Dataset. The CUB-
200 dataset includes 6033 images belonging to 200 distinct
bird categories. Ground truth labels for each image are pro-
vided. In our experiment, we sorted names of the bird cat-
egories alphabetically and used the ﬁrst 100 classes as the
known classes. The remaining classes were used to repre-
sent novel images.
Stanford Dogs Dataset. This dataset is a subset of the Im-
ageNet dataset and was originally intended for ﬁne-grain
classiﬁcation. There are 20580 images belonging to 120
different dog breeds in this dataset. We considered the ﬁrst
60 classes as the known classes and treated the remaining
classes as the novel classes during performance evaluation.
FounderType-200 Dataset. This dataset is a collection
of Chinese character images in different font types. The
dataset is organized based on the font-type. In total there are
200 different font-types with 6763 images from each class
in this dataset. Following the same convention as before, we
picked the ﬁrst 100 classes to represent the enrolled classes.
The remaining 100 classes were used to simulate the novel
images.

In all datasets, following the protocol in [13], images
of the enrolled classes were randomly split into two even
sets to form training and testing datasets of the enrolled
classes. Images of the novel classes were used only during
testing. When ﬁnetuning/extracting features from the cal-
tech256 dataset following [6], we used the pretrained model
trained on the Places365 dataset [32]. For all other tasks, we
used the pretrained model trained on the ILSVRC12 dataset.
Accordingly, the validation sets of Places365 was used as
the reference dataset for Caltech256. For all other tasks the
validation set of ILSVRC12 was considered.

5.3. Results

We evaluated all methods based on the VGG16 and the
AlexNet features. We used the training codes provided by
the authors when evaluating the KNFST [4] and the local
KNFST [3] methods. Performance of each method is eval-
uated using the area under the receiver operating character-
istics (AUC) curve. Obtained AUC values for each method
are tabulated in Table 1 for all datasets1.

When baseline methods are considered, a variance in
performance can be observed across datasets. In general,
K-extremes has reported below-par performances compared
to the other methods. When the number of enrolled classes

1Source code of

the proposed method is made available at

https://github.com/PramuPerera/TransferLearningNovelty

Table 1. Novelty detection results (AUC of the ROC curve) on the
evaluation datasets. The best performing method for each dataset
is shown in bold. Second best method is shown in italics.
Method

FounderType

Caltech-256

CUB-200

Dogs

Finetune[26], [12]
One-class SVM[23]
KNFST pre[4]
KNFST[4], [13]
Local KNFST pre[3]
Local KNFST[3]
K-extremes[24]
OpenMax[2]
Finetune(c + C)
Deep Novelty (ours)

VGG16
0.827
0.576
0.727
0.743
0.657
0.712
0.546
0.831
0.848
0.869

AlexNet VGG16
0.785
0.561
0.672
0.688
0.600
0.628
0.521
0.787
0.788
0.807

0.931
0.554
0.842
0.891
0.780
0.820
0.520
0.935
0.921
0.958

AlexNet VGG16
0.909
0.532
0.710
0.748
0.717
0.690
0.514
0.915
0.899
0.947

0.766
0.542
0.649
0.633
0.652
0.626
0.610
0.776
0.780
0.825

AlexNet VGG16
0.702
0.520
0.619
0.602
0.589
0.600
0.592
0.711
0.692
0.748

0.841
0.627
0.590
0.870
0.549
0.673
0.557
0.852
0.754
0.893

AlexNet
0.650
0.612
0.655
0.678
0.523
0.633
0.512
0.667
0.723
0.741

are very high, the mean activation signature of a class looses
its uniqueness. This is why K-extremes method fails when
very large number of classes are enrolled as suggested in
[24]. In the Caltech-256 and CUB-200 datasets, threshold-
ing deep activations and OpenMax has yielded better results
among the baseline methods. In Caltech256, this has im-
proved marginally when the reference dataset (ILSVRC12)
is incorporated. This method has performed reasonably
well in the FounderType-200 dataset but it’s performance
in the Standford Dogs dataset is not convincing. In general,
KNFST has out-performed local KNFST except for in the
Standford Dogs dataset. KNFST (and local KNFST) oper-
ating on the ﬁnetuned deep features have performed better
in general compared to the pre-trained deep features. This
trend has changed only in the Standford Dogs dataset. Here
we note that none of the baseline methods have yielded con-
sistent performance across datasets.

In comparison, the proposed method is able to produce
the best performance across all datasets. When AlexNet is
used as the back-bone network, there is an improvement of
about 3.0% over the baselines in the CUB-200 and Stand-
ford Dogs datasets. In the other two datasets this margin is
2.0%. In the Caltech256, CUB-200 and FounderType-200
datasets, the improvements in AUC are in excess of 2.0%
for the VGG16 model. In the Standford Dogs dataset, the
proposed method is able to introduce a signiﬁcant advance-
ment of more than 7.0% in AUC compared with the base-
line methods.
In general, we note that in datasets where
the baseline performance is already very good, as in the
CUB-200 and FounderType 200 datasets, the improvement
of the proposed method is relatively small. On the other
hand, when the baseline performance is poor, the proposed
method is able to generate a signiﬁcant improvement in the
performance.

5.4. Ablation Study

In this subsection, we investigate the impact of each in-
dividual component of the proposed framework. For the
purpose of the ablation study, we use the validation dataset
of the ILSVRC12 dataset as the reference dataset. It should
be noted that ﬁgures reported in this subsection are differ-
ent from Table 1 due to this reason. Starting from the tra-
ditional CNN architecture, we added one component of the

11550

proposed framework at a time and evaluated the novelty de-
tection performance on the Caltech-256 dataset as a case
study. Testing protocol presented in the preceding subsec-
tion was followed in all cases. Considered cases are as fol-
lows.
a) Single CNN with the cross-entropy loss (AUC 0.854).
This is the CNN baseline where a CNN is trained using the
enrolled classes conventionally.
b) Single CNN with the cross-entropy loss+membership
loss (AUC 0.865). The network architecture is the same
as in case (a).
In addition to the cross-entropy loss, the
membership loss is calculated with respect to the enrolled
dataset.
c) Two Parallel CNNs with cross-entropy loss (AUC
0.864). The network structure proposed in Figure 4(a) is
used. In contrast, only the cross-entropy loss is used in the
bottom sub-network.
d) Proposed method (AUC 0.906).
Figure 4(a) is used for training.

Proposed structure

In the proposed method, we introduced membership loss
and a parallel network structure as contributions. From the
case study conducted, it appears that the novelty detection
performance improves compared to the baseline even when
one of the contributions are used. Moreover, we observe
that the two contributions compliment each other and gen-
erate even better results when combined together.

5.5. Impact of the Reference Dataset

In the proposed method, we assumed the availability of a
reference dataset with large number of classes. In this sub-
section, we investigate the impact of the reference dataset
by varying the reference dataset of choice.
In particular,
we use the ILSVRC12, Caltech-256 and Standford Dogs
datasets as the reference datasets to perform novelty detec-
tion using the proposed method in the CUB-200 dataset.
Results obtained are tabulated in Table 2. Here we have in-
cluded the performance of the best baseline method for the
CUB-200 dataset (Finetune) from Table 1 as a baseline.

Compared to ILSVRC12, when Caltech-256 is used as
the reference dataset, AUC drops by 0.005%. This fur-
ther drops by 0.008% when the Standford Dogs dataset is
used. The ILSVRC12 dataset contains 1000 image classes
and has signiﬁcant variance in images within each class.
Caltech-256 is a similar multi-class dataset but with fewer
classes. Both of these datasets contain natural images.
However since ILSVRC12 has more classes and more intra-
class variance, we expect it to generate globally negative
ﬁlters better. Therefore, the performance drop of Caltech-
256 compared to ILSVRC12 is expected. On the other
hand, the Standford Dogs dataset only contains images of
dogs. Therefore, ﬁlters learned using this dataset may not
be generic to get stimulated by the arbitrary inputs. There-
fore, the drop in the performance is justiﬁed. In conclusion,

we note that the proposed method is able to out-perform
baseline novelty detection methods even when the reference
dataset is varied. However, better results are obtained when
a larger dataset with high degree of intra-class variation is
used as the reference dataset.

Table 2. Impact of the reference dataset used. Results of the case
study conducted on the CUB-200 dataset by varying the reference
dataset.

Novelty Detection AUC 0.931

Baseline

ILSVRC12
0.958

Caltech-256
0.953

Dogs
0.945

5.6. Impact on Classiﬁcation Accuracy

When a test image is present, the proposed method pro-
duces a set of class activation scores. It is still possible to
perform classiﬁcation using the same system by associat-
ing the test image with the class containing the highest ac-
tivation. In what follows, we consider test samples of the
known classes and perform closed-set classiﬁcation in the
same experimental setup described in Section 5.3. In other
words, we do not consider novel samples for the purpose
of this study. Obtained classiﬁcation accuracies for the four
datasets are tabulated in Table 3. Although the proposed
method is designed for the purpose of novelty detection,
we note that the proposed changes have contributed towards
increasing the classiﬁcation accuracy of the system as well.
This is because the membership loss explicitly enforces cor-
rect class to have a high score and all other classes to have
scores closer to zero.

Table 3. Classiﬁcation accuracy obtained for conventional ﬁne-
tuning and the proposed method for the four evaluation datasets.
FounderType
0.945
0.950

Caltech-256
0.908
0.939

VGG16
Proposed Method

CUB-200
0.988
0.990

Dogs
0.730
0.801

6. Conclusion

We presented an end-to-end deep learning-based solu-
tion for image novelty detection. We build up on the con-
ventional classiﬁcation networks and introduce two novel
contributions; namely, membership loss and a training pro-
cedure that produces globally negative ﬁlters. In the pro-
posed method, novelty is quarried simply by thresholding
the highest activation of the output vector. We demonstrate
the effectiveness of the proposed method on four publicly
available multi-class image datasets and obtain state-of-the-
art results.

References

[1] M. Abavisani, H. R. Vaezi Joze, and V. Patel. Improving the
performance of unimodal dynamic hand-gesture recognition
with multimodal training. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2019. 1

11551

[2] A. Bendale and T. E. Boult. Towards open set deep networks.
In 2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30,
2016, pages 1563–1572, 2016. 2, 6, 7

[3] P. Bodesheim, A. Freytag, E. Rodner, and J. Denzler. Lo-
cal novelty detection in multi-class recognition problems. In
2015 IEEE Winter Conference on Applications of Computer
Vision, pages 813–820, 2015. 2, 6, 7

[4] P. Bodesheim, A. Freytag, E. Rodner, M. Kemmler, and
J. Denzler. Kernel null space methods for novelty detection.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2013. 2, 6, 7

[5] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detec-
tion: A survey. ACM Comput. Surv., 41(3):15:1–15:58, 2009.
2

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR09, 2009. 7

[7] A. R. Dhamija, M. G¨unther, and T. Boult. Reducing network
agnostophobia. In Advances in Neural Information Process-
ing Systems 31, pages 9157–9168. 2018. 2

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
In IEEE Conference on Computer
for image recognition.
Vision and Pattern Recognition, pages 770–778, June 2016.
1, 3

[9] D. Hendrycks and K. Gimpel. A baseline for detecting
misclassiﬁed and out-of-distribution examples in neural net-
works. Proceedings of International Conference on Learning
Representations, 2017. 1

[10] H. Hoffmann. Kernel pca for novelty detection. Pattern

Recognition, 40(3):863 – 874, 2007. 2

[11] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017. 5

[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-
siﬁcation with deep convolutional neural networks. In Ad-
vances in Neural Information Processing Systems 25, pages
1097–1105, 2012. 1, 5, 6, 7

[13] J. Liu, Z. Lian, Y. Wang, and J. Xiao. Incremental kernel null
space discriminant analysis for novelty detection.
In 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 4123–4131, July 2017. 2, 6, 7

[14] M. Markou and S. Singh. Novelty detection: a review – part
1: statistical approaches. Signal Processing, 83(12):2481 –
2497, 2003. 1

[15] P. Oza and V. Patel. C2ae: Class conditioned auto-encoder
for open-set recognition. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2019. 2

[16] P. Oza and V. M. Patel. Active authentication using an au-
toencoder regularized cnn-based one-class classiﬁer. In 2019
14th IEEE International Conference on Automatic Face &
Gesture Recognition (FG 2019). IEEE, 2019. 2

[17] P. Oza and V. M. Patel. One-class convolutional neural net-
work. IEEE Signal Processing Letters, 26(2):277–281, 2019.
2

[18] P. Perera, R. Nallapati, and B. Xiang. Ocgan: One-class
novelty detection using gans with constrained latent repre-
sentations. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019. 2

[19] P. Perera and V. M. Patel. Learning Deep Features for One-

Class Classiﬁcation. ArXiv e-prints. 2

[20] P. Perera and V. M. Patel. Dual-minimax prob-ability ma-
chines for one-class mobile active authentication. In IEEE
Conference on Biometrics: Theory, Applications,and Sys-
tems (BTAS), September 2018. 2

[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog-
nition challenge.
Int. J. Comput. Vision, 115(3):211–252,
Dec. 2015. 1, 3

[22] W. J. Scheirer, A. Rocha, A. Sapkota, and T. E. Boult. To-
wards open set recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence (T-PAMI), 36, July 2013.
2

[23] B. Sch¨olkopf, J. C. Platt, J. C. Shawe-Taylor, A. J. Smola,
and R. C. Williamson. Estimating the support of a high-
dimensional distribution. Neural Comput., 13(7):1443–
1471, 2001. 2, 6, 7

[24] A. Schultheiss, C. K¨ading, A. Freytag, and J. Denzler. Find-
ing the unknown: Novelty detection with extreme value sig-
natures of deep neural activations.
In Pattern Recognition
- 39th German Conference, Proceedings, pages 226–238,
2017. 2, 6, 7

[25] K. Simonyan and A. Zisserman.

Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 1

[26] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. CoRR, 2014. 4,
6, 7

[27] D. M. J. Tax and R. P. W. Duin. Support vector data descrip-

tion. Mach. Learn., 54(1):45–66, 2004. 2

[28] M. Turk and A. Pentland. Eigenfaces for recognition. Jour-

nal of Cognitive Neuroscience, 3(1):71–86, 1991. 2

[29] A. Vyas, N. Jammalamadaka, X. Zhu, D. Das, B. Kaul, and
T. L. Willke. Out-of-distribution detection using an ensemble
of self supervised leave-out classiﬁers. In Computer Vision -
ECCV 2018 - 15th European Conference, Munich, Germany,
September 8-14, 2018, Proceedings, Part VIII, pages 560–
574, 2018. 1

[30] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma.
Robust face recognition via sparse representation.
IEEE
Trans. Pattern Anal. Mach. Intell., 31(2):210–227, Feb.
2009. 2

[31] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson.
Understanding neural networks through deep visualization.
In Deep Learning Workshop, International Conference on
Machine Learning (ICML), 2015. 3

[32] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba.
Places: A 10 million image database for scene recognition.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2017. 7

11552

