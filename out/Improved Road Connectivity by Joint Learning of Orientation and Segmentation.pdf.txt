Improved Road Connectivity by Joint Learning of Orientation and Segmentation

Anil Batra ∗1 Suriya Singh ∗ †2 Guan Pang3 Saikat Basu3 C.V. Jawahar1 Manohar Paluri3

1IIIT Hyderabad 2MILA / Polytechnique Montr´eal

3Facebook

Abstract

Road network extraction from satellite images often pro-
duce fragmented road segments leading to road maps unﬁt
for real applications. Pixel-wise classiﬁcation fails to pre-
dict topologically correct and connected road masks due to
the absence of connectivity supervision and difﬁculty in en-
forcing topological constraints. In this paper, we propose
a connectivity task called Orientation Learning, motivated
by the human behavior of annotating roads by tracing it
at a speciﬁc orientation. We also develop a stacked multi-
branch convolutional module to effectively utilize the mu-
tual information between orientation learning and segmen-
tation tasks. These contributions ensure that the model pre-
dicts topologically correct and connected road masks. We
also propose Connectivity Reﬁnement approach to further
enhance the estimated road networks. The reﬁnement model
is pre-trained to connect and reﬁne the corrupted ground-
truth masks and later ﬁne-tuned to enhance the predicted
road masks. We demonstrate the advantages of our ap-
proach on two diverse road extraction datasets SpaceNet
[30] and DeepGlobe [11]. Our approach improves over the
state-of-the-art techniques by 9% and 7.5% in road topol-
ogy metric on SpaceNet and DeepGlobe, respectively.

1. Introduction

A mapped road network provides routing information to
ﬁnd the traversable paths, which are important for planning
in various applications such as navigation and disaster man-
agement. Example of a connected road network is shown
in Figure 1a. Manual mapping of a complex road network
is time consuming and requires intensive human effort. Au-
tomatic extraction of road networks from satellite imagery
has been proposed [2, 6, 18, 29, 33], where recently, deep
learning based techniques have shown high quality mapping
results in diverse scenarios [3, 8, 10, 19, 21–23, 28, 31, 35].
However, the extracted road networks often produce frag-

∗Equal Contribution
†Work partially done as Research Fellow at IIIT Hyderabad
Code

available

https://github.com/anil-2185/

at:
road-connectivity

(a)

(b)

(c)

(d)

Figure 1: Road network extraction formulated as binary seg-
mentation fails to produce topologically correct road map due to
change in road appearance. (a) Annotators trace lines (highlighted
nodes) along the center of roads with a traversable shortest path
(a, c, d, e, b) for a → b. (b) Fragmented road network estimated
using segmentation resulting in path (a, c, f, g, h, b) for a → b.
(c) Tracing roads with orientation to achieve connectivity. (d) We
extract connected and topologically correct road networks using
segmentation and orientation.

mented road segments, and therefore, are unﬁt for real ap-
plications (Figure 1b). Satellite images pose difﬁculties in
the extraction of roads due to (a) shadows of clouds and
trees, (b) diverse appearance and illumination condition due
to terrain, weather, geography, etc., and, (c) similarity of
road texture with other materials. Label scarcity [28] as well
as omission and registration noise in road ground-truths
[22] also inhibit the accurate estimation of road maps.

Road network extraction is explored in [8, 10, 19, 21,
22], where the problem is posed as segmentation followed
by post-processing steps to reﬁne and couple the missing

110385

abedcfghabedcfghabedcfghconnections. The pixel-wise classiﬁcation supervision does
not constrain the model to learn representations for con-
nected road segments [23], leading to poor estimation of
road topology. Predicting masks with accurate topology
is a challenging task due to difﬁculty in enforcing topo-
logical constraints via a loss function [20, 23] or during
post-processing [19]. To measure deviations in topology,
Mosinska et al. [23] rely on higher-level abstract features of
ground-truth and predicted road masks whereas M´attyus et
al. [20] employ an adversarial matching paradigm. To im-
prove road connectivity, M´attyus et al. [19] proposed post-
processing steps to reason for missing connection hypothe-
ses while Bastani et al. [3] and Ventura et al. [31] iteratively
connect road segments in the neighbouring image patches.
Our focus is on improving connectivity in road network
extraction from binary segmentation of overhead imagery.
Characterizing connectivity supervision in the way human
annotates road maps requires topological and structural in-
formation of roads. We build our approach on the intuition
that to annotate road maps human trace lines along the road
orientation to connect the fragmented road segments. Con-
sider Figure 1b, tracing lines c → b via d and e can connect
the broken roads. This motivates us to design a connectivity
task using available road labels to predict road orientation
angle along with the road segmentation (Figure 1c).

In this paper, we propose to learn a road orientations
jointly with per-pixel road segmentation in multi-branch
CNN model (Figure 2). We also propose connectivity re-
ﬁnement which connect small gaps and reduces false posi-
tives in the prediction. The connectivity reﬁnement model is
pre-trained to restore the corrupted road ground-truth masks
(Figure 2 and 4). This allows the model to effectively cor-
rect diverse failure scenarios. Similar to Mosinska et al.
[23], our connectivity reﬁnement model can be employed
in an iterative manner, however, our reﬁnement approach
focuses on improving connectivity with the help of pre-
training in addition to segmentation improvement. Lastly,
we design a joint learning module by stacking multi-branch
encoder-decoder structure (Figure 5 and 6). This module is
a variant of stacked hourglass network [24], however our
motivation is different i.e., ﬂow of information between
the related tasks to improve the performance of individ-
ual task in a multi-task learning framework. In contrast to
[3, 19, 22, 28], our segmentation model inherently captures
the information of connected road segments in the interme-
diate representation, leading to an accurate topology in road
network estimation (Figure 1d).

Contributions:

1. We design an orientation learning task and demon-
strate that the joint learning of orientation and segmen-
tation improves the connectivity of road network.

2. We propose a connectivity reﬁnement approach pre-

trained with corrupted road ground-truth masks and
ﬁne-tuned with segmentation outputs to iteratively en-
hance the topology of the estimated road networks.

3. We design a stacked multi-branch module to effec-
tively utilize the dual supervision. We show that the
proposed module enables the ﬂow of information be-
tween the tasks and helps in boosting the connectivity.

2. Related Work

Road Network Extraction: Numerous techniques have
been developed in literature to extract road networks from
satellite images. Traditional methods impose connectivity
by incorporating contextual priors such as road geometry
[18], higher order CRF formulation [33], marked point pro-
cesses [6, 29], and solving integer programming on road
graphs [2]. These methods utilized hand designed fea-
tures and optimized for complex objectives. In recent deep
learning based techniques, road extraction is formulated
as segmentation problem [19, 21–23, 28] using convolu-
tional encoder-decoder structured models, which are able
to capture large spatial context. Different from segmenta-
tion based approaches, Bastani et al. [3] introduced graph
based methodology to predict road line strings. In the cur-
rent scope, we focus on segmentation based approaches.
Mnih et al. [21] learn road classiﬁcation by CNN model
in multiple stages (to reduce false negative rate due to la-
bel noise), operating on the image patches. M´attyus et al.
[19] propose encoder-decoder structure model and pose it as
multi-class (roads, building and background) seg-
mentation. The model performs well in segmentation, how-
ever, fails to predict connected roads, and missing roads are
connected using shortest path algorithms in the post pro-
cessing steps to improve the connectivity. M´attyus et al.
[19] further use a binary decision classiﬁer to predict the
correctness of connections. We found that [19] face difﬁ-
culty in correctly adding and classifying the missing road
connections in regions with high road density, ambiguous
road appearance, occlusions, and complex road topology
present in the datasets (SpaceNet [30] and DeepGlobe [11])
we validate our methods on.

The other well admired encoder-decoder structure to
learn thin curvilinear road structures are U-Net [27] and
LinkNet [7]. Their variants are proposed to learn the road
segmentation in [8, 10]. LinkNet34 [7] has been primarily
utilized to segment the roads in DeepGlobe challenge [11].
Nevertheless, connectivity is achieved with more heuristic
based post-processing in these methods.
In contrast, we
propose joint learning of connectivity task and road seg-
mentation with a stacked encoder-decoder structure. The
most recent work of Mosinska et al. [23] combine pixel-
wise classiﬁcation and perceptual losses [12] to learn road
topology in U-Net [27]. Mosinska et al. [23] also proposed

10386

Figure 2: Our approach for extracting connected road topology from satellite images. Annotations in the form of line strings, are converted
to (a) orientation groundtruth and (b) road groundtruth masks. We use encoder-decoder structure with (c) stacked multi-branch module to
jointly learn (d) orientation and (e) segmentation, providing dual supervision to the model. The orientation task is designed to improve the
road connectivity. Finally, a connectivity reﬁnement network, (f) pre-trained with corrupted groundtruths to remove false roads and further
improve the road connectivity, is (g) ﬁne-tuned with road segmentation output to iteratively enhance the estimated road networks.

iterative reﬁnement to ﬁll the small gaps in road segments.
The introduced loss term favors the road like structures but
is inefﬁcient in connecting the road segments.

Multi-Task Learning (MTL):
It is a learning mecha-
nism [4], inspired from human beings to acquire knowledge
of complex tasks by performing different shared sub-tasks
simultaneously. Multi-task learning improves the perfor-
mance by inducing mutual information of the tasks in the
learning process. MTL has been applied successfully in var-
ious domains such as speech recognition, natural language
processing [9] and computer vision [17]. Readers are sug-
gested to read survey [34] on multi-task learning.

Humans perform two related tasks while annotating the
roads i.e.
identify the road pixels and trace lines to con-
nect them. In our work, we use multi-task learning to incor-
porate road annotation as two tasks i.e. while labeling the
satellite images, humans recognize roads and connect them
by tracing lines, inherently identifying the orientation. We
show that these related tasks improve the connectivity with
improved encoded representation in the encoder.

3. Method

Road extraction from overhead images via segmentation
based methods produce disconnected road segments. To ad-
dress this, we develop an orientation task from the road line
strings (Section 3.1) and use it as an auxiliary loss along
with pixel-wise segmentation loss. The motivation of ori-
entation loss is to capture the relational information be-
tween the neighboring pixels through explicit learning of
orientations between them. We formulate the problem as
a two stage process: (a) joint learning of road orientation

and segmentation in multi-task fashion, and (b) a connec-
tivity reﬁnement using a pre-trained CNN model (Section
3.2). We ﬁrst present our novel inductive task followed by
a connectivity reﬁnement technique. Finally, we outline the
proposed end-to-end joint learning pipeline with two stacks
of multi-branch encoder-decoder which can ﬂow the infor-
mation across the tasks (Section 3.3).

3.1. Orientation Learning

The pixel level annotation of roads is a computationally
costly and time consuming task. To reduce the human ef-
fort, roads are preferably annotated with line strings con-
necting 2D points. We visualize each road line string as
a directional vector between two consecutive points in 2D
image plane (see Figure 3). The directional vector provides
the orientation (tracing angle) of each road segment.

The orientation learning task is partly inspired from Part
Afﬁnity Fields [36] and bears resemblance with the deep
watershed technique for instance segmentation [1].
Intu-
itively, representations learned for instance (road segments)
segmentation would lead to improved connectivity in the
estimated road network. However, road segments, unlike
object instances or human body parts, do not have de-
ﬁned boundary between them and are rather interconnected.
Therefore, instead of predicting orientation from the object
boundary towards its centroid [36], we encode and predict
the unit vector pointing towards the next pixel in the same or
the connected adjacent road segment. Learning orientation
with a pixel based cross-entropy loss poses a connectivity
constraint in the encoded representation as learning of road
orientations favors the connected road segments and joint
learning of related tasks often leads to more generalizable

10387

Linestrings0  (11 10), (11 15), (12 17)..1  (19 00), (19 01), (19 21)......21  (250 30), (250 95), (212,..22  (252 91), (101 91), (020,..Linestrings0  (11 10), (11 15), (12 17)..1  (19 00), (19 01), (19 21)......21  (250 30), (250 95), (212,..22  (252 91), (101 91), (020,..Figure 3: Road Orientations. Top left: road line strings annota-
tions. Bottom left:
two consecutive points to compute the ori-
entation angle. Top right: Ground-truth road orientation vectors.
Bottom right: Road orientation ground-truth in an image patch.

features [4, 17]. Orientation learning can be extended to
applications like automatic segmentation along the object
boundary [3, 5], connect the occluded lanes in lane detec-
tion, connect broken alphabets in OCR, etc.

We now describe the process to generate the orientation
ground-truths from line strings. Consider an image shown
in Figure 3 with road line strings {l1, l2, . . . , lm} and each
line string lk consists of 2D points {p1, p2, . . . , pn}. We
assume undirected road network, ignoring driving direction
of the roads. We sort the coordinates of the points of each
line string such that most of the directional vectors point
from left to right and top to bottom, which we ﬁnd to be ap-
propriate for the neural network to learn and focus on con-
nected road representation. We compute a unit directional
vector |~v(x, y)| ∈ [−1, 1] between two consecutive point
pairs {(p1, p2), . . . , (pn−1, pn)} of lk using (1) and convert
it into polar domain to obtain orientation angle or using (2).
For each point pair (pi, pj) using (3), the pixels lying within
the threshold width λorient along the perpendicular direc-
tion of lk, are assigned the same orientation value; for all
other pixels non-road orientation angle ob is assigned.

~vij(x, y) =

pi(x, y) − pj(x, y)

||pi(x, y) − pj(x, y)||2

2

~vij(x, y) ≡ h1 ∡ori

olk (m) =(or

ob

−−−−−−→
(m − p1)| < λorient

if |~v⊥ ·
otherwise.

(1)

(2)

(3)

where ||pi − pj||2
2 is the total length between the consecu-
tive points, v⊥ is a vector perpendicular to unit directional
vector, (x, y) are the coordinates of points and o is ground

Figure 4: Connectivity Reﬁnement. We pre-train the encoder-
decoder CNN to remove false roads with pre-text task of correct-
ing the corrupted road ground-truth masks. The model is later
ﬁne-tuned to reﬁne the road segmentation outputs.

truth for orientations. We ignore non-road orientation angle
during plotting of the vectors in Figure 2 and 3.

3.2. Connectivity Reﬁnement

The orientation supervision improves the connectivity in
the estimated road network. However, complex and dense
road topology such as bridges and parking lots leads to fail-
ure in orientation prediction. The model also hallucinates
roads in regions with similar textures e.g. road like patterns
in farms. To further improve the prediction topology and
suppress false positives, we employ the connectivity reﬁne-
ment (see Figure 4). Motivated by the success of restoring
the images from corruption [25, 28], we interpret missing
and spurious road segments as corrupted road ground-truth
mask. We ﬁrst pre-train the reﬁnement network to restore
the corrupted masks allowing the model to learn connectiv-
ity pattern as well as remove false roads. Note that, we opt
for weight initialization and do not train the connectivity
reﬁnement using segmentation outputs and corrupted GT
simultaneously to avoid overﬁtting to a single distribution
of corruptions [14]. In pre-training stage, we concatenate
satellite image X, corrupted ground-truth y′ along with pre-
vious road prediction ¯yt−1 (where ¯y0 = y′) and feed it as
input to the reﬁnement model g(·).

¯yt = g(cid:16)[X, y′, ¯yt−1](cid:17) t = 1, . . . , T

(4)

At the end of pre-training stage the neural network learns to
effectively encode the available contexts and ﬁlls the miss-
ing road segments. The pre-trained model is further ﬁne-
tuned to improve the road segmentation.
In ﬁne tuning
stage, we replace the manually corrupted ground truth mask
with the output of segmentation network.

ˆyt = g(cid:16)[X, ˆy, ˆyt−1](cid:17) t = 1, . . . , T

(5)

where ˆy = fseg(X), ˆy0 = ˆy, and [·] denotes concatenation
along channel axis. We use T = 3 and identical encoder-
decoder architectures for g(·) and fseg(·).

10388

p2p1mFigure 5: Architecture of n-stacked multi-branch CNN to learn road orientation and segmentation simultaneously. The stacked module is

capable to calculate losses Lseg & Lorient at different scales(cid:0){ 1

4 , 1

4 . . . n times}, 1

2 and 1(cid:1) to optimize the CNN. We use two stacks of

multi-branch module (Figure 6) with features fusion in ﬁrst stack only. Refer to supplementary material for additional architectural details.

Figure 6: A multi-branch module. The intermediate output is ex-
tracted from each branch using 1 × 1 convolution and are merged
using a fusion block.

3.3. Stacked Multi branch Module

The stacked multi-branch module as shown,

in Fig-
ure 5 is composed of three blocks:
(a) shared encoder,
(b) iterative fusion with multi-branch, and (c) prediction
branches for orientation and segmentation. The proposed
CNN model performs the following tasks simultaneously:
(a) learn a robust common representation for connected
road segments in the shared encoder, (b) predicts road orien-
tation and road segmentation, and (c) allows the information
ﬂow between the tasks to encourage road connectivity.

The shared encoder takes the input image X and learns a
mapping function E, which projects the input to an encoded
representation for both tasks. The encoding z = E(X) is
fed to the stacked multi-branch module to learn the coarse
predictions. The motivation for n-stack multi-branch mod-
ule is three fold: (a) large receptive ﬁeld to capture the spa-
tial context, (b) mini encoder-decoder structure learns to re-
calibrate features and coarse predictions in a repetitive fash-
ion, and (c) it allows the information to ﬂow from previous
stack to the subsequent stack and reﬁne the coarse predic-
tions. We denote the stacking with a function Hn, where
n is number of stacked multi-branch modules and coarse
predictions with ¯o for orientation and ¯y for roads in (6).

To learn reﬁne predictions ˆo and ˆy from the coarse pre-
dictions ¯on and ¯yn, we create two symmetric branches for

each task. Each branch learns to up-sample the predictions
using decoder networks consisting of two transposed con-
volutions followed by a pixel-wise convolutional classiﬁer.

¯on, ¯yn =(Hn(¯on−1 + ¯yn−1 + z)

H(z)

if n > 1
if n = 1

(6)

Loss Function: The proposed network is capable of
yielding the intermediate outputs at different scales, n out-
puts from each stack of multi-branch module at 1
4 scale
and two from successive transposed convolution at 1
2 and 1.
Hence, this allows to use multi-scale loss to guide the net-
work while training. Let (X, y, o) be a given labeled sample
from the dataset and f (·) denotes the prediction function us-
ing our model. We optimize the following loss functions:

Lseg(ˆy, y) = −SoftIoU(cid:16)fseg(X), y(cid:17)
oc log(cid:16)forient(X)(cid:17)

Lorient(ˆo, o) = −

ol

Xc=0
Loss =Xs (cid:0)Ls

seg + Ls

orient(cid:1)

(7)

(8)

(9)

where SoftIoU is differentiable IoU loss function [19], ol
is the number of bins in the quantized orientation, and s is
scale having values { 1

4 , . . . n times}, 1

2 and 1.

4 , 1

4. Evaluation Metrics

Pixel Based Metrics: We evaluate the performance of
our approach for road segmentation using intersection over
union (IoU ) and F 1-score metrics. The groundtruth for
road segmentation is obtained by rasterizing the road line
strings with constant width in SpaceNet dataset [30]. The
constant road mask for varying road widths can adversely
affect the pixel based metrics. Thus, we use the relaxed

10389

64 x 6432 x 3216 x 168 x 8Road OrientationRoad SegmentationFusionResidual BlockUpsample1 x 1 ConvMax PoolSumFusion Blockmetrics, suggested by Mnih et al. [22] with a buffer of 4
pixels in our evaluations.

Graph Based Metric: To measure the estimated topol-
ogy and road connectivity, we use the Average Path Length
Similarity (AP LS) [30] as the evaluation metric. The met-
ric captures the deviations in shortest path distances be-
tween all pair of nodes in a graph. The ground-truth and
predicted road network graphs are obtained from y and ˆy,
respectively. SP →T (10) measures the sum of difference of
shortest path for each node pair in groundtruth graph G =
(V, E) and estimated graph ˆG = ( ˆV , ˆE). To penalize the
false positives, symmetric term ST →P is added to AP LS
metric which considers predicted graph as groundtruth and
true graph as prediction.

SP →T = 1 −

AP LS =

1

|V | X min 1,
N X(y,ˆy) 

SP →T (G, ˆG)

1

1

|L(a, b) − L(ˆa, ˆb)|

L(a, b)

! (10)

1
+

ST →P ( ˆG,G)!

1

(11)

where a, b ∈ V , ˆa, ˆb ∈ ˆV , |V | is number of nodes in
groundtruth graph, and N is number of images. L(a, b) and
L(ˆa, ˆb) are the path length of a → b and ˆa → ˆb, respec-
tively.

5. Experiments and Results

5.1. Dataset

We perform our experiments on SpaceNet [30] and
DeepGlobe [11] datasets using only 3-band RGB images.
We follow the experimental protocols and dataset splits of
[28]. We evaluate and report the road connectivity metrics
on full resolution images at inference time for each dataset.

SpaceNet [30]: This dataset provides imagery from four
different cities with ground resolution of 30cm/pixel and
pixel resolution of 1300×1300. Annotations are provided in
the form of line strings, representing centerline of the roads.
The dataset consists of 2780 images and, following [28], we
split the dataset into 2213 images for training and 567 for
testing. To augment the training dataset we create crops of
650 × 650 with overlapping region of 215 pixels, thus pro-
viding ∼32K images. For validation we use the crops of
same size without overlap.

DeepGlobe [11]: This dataset
includes imagery from
three different regions with pixel level annotations. The
ground resolution is 50cm/pixel and pixel resolution is
1024 × 1024. Following [28], we create splits of 4696 im-
ages for training and 1530 for validation. We augment it by
creating crops of size 512 × 512 with overlapping region of
256 pixels, yielding ∼42K images for training phase. We
compute the road line string ground-truths by skeletonizing
the pixel level annotations and smoothing it using Ramer-
Douglas-Peucker algorithm [13, 26].

Method

ResNet18

ResNet18 + Orientation

ResNet18 + Junctions

LinkNet34

LinkNet34 + Orientation

LinkNet34 + Junctions

SpaceNet

DeepGlobe

road IoU a

AP LS

road IoU a

AP LS

59.04

61.90

58.41

60.33

62.45

60.72

52.65

59.06

52.76

55.69

60.77

55.91

62.12

64.77

63.54

62.75

64.72

63.79

63.31

68.93

66.20

65.33

68.71

67.42

Table 1: Comparison of orientation and junction learning auxiliary
tasks for road connectivity. It shows that improvement in the road
connectivity is due to orientation learning. road IoU a: accurate
pixel based intersection over union. AP LS: average path length
similarity on the extracted graph from road segmentation.

5.2. Implementation Details

Dataset Preprocessing: Similar to [28], we generate road
heatmaps using Euclidean distance transform along the cen-
ter line of roads and create binary masks with threshold of
0.76. We use narrower road ground-truth masks, as com-
pared to threshold of 0.4 in [28], to avoid merging of lanes
and nearby roads. This step is crucial to obtain maps with
high connectivity and accurate topology (ablation studies in
the supplementary material). We set λorient = 12 pixels in
(3) as orientation width along the roads which is approxi-
mately equal to the width of road masks.

Training Details: We use random crops of size 256×256
from the image followed by mean subtraction. To improve
the generalization of network, random horizontal ﬂip, mir-
roring and rotation is employed as data augmentation. We
train the joint network with a batch size of 32 for 120
epochs. We use SGD optimizer with momentum = 0.9,
weight decay = 0.0005 and initial learning rate of 10−2
with step scheduler having drop factor of 10 at epochs {60,
90, 110}. We perform simple graph processing to remove
small hanging road segments and graph smoothing. Follow-
ing [32], we formulate the regression for road orientations
as classiﬁcation task as direct regression tends to smoothen
predictions to the mean [15, 32]. We quantize road orien-
tation angles into bins of 10◦ (refer to the supplementary
material for ablation studies on quantization levels).

5.3. Results

Orientation Learning: We choose two architectures
ResNet18 [16] and LinkNet34 [7] to study the performance
of orientation learning. We modify both architectures with
dual and identical decoders having shared encoder. The re-
sults in Table 1 shows that our proposed task for road con-
nectivity generalizes to different architectures. Incorporat-
ing the orientation learning as an auxiliary loss improves the
AP LS for both CNN architectures by 6.41% and 5.08%
for SpaceNet [30], respectively. This suggests that multi-
task learning of two related task improves the intermediate
representation, leading to better generalization. To study

10390

Method

SpaceNet

DeepGlobe

road IoU a AP LS road IoU a AP LS

ResNet18 [16] + Orientation

LinkNet34 [7] + Orientation

Unet [27] + Orientation

Multi-branch(1 Stack) + Orientation

Multi-branch(2 Stack) + Orientation

Multi-branch(3 Stack) + Orientation

61.90

62.45

60.12

63.26

63.75

63.73

59.06

60.77

58.59

60.92

63.65

62.89

64.77

64.72

65.21

65.60

67.21

66.61

68.93

68.71

67.81

70.23

73.21

72.48

Table 2: Comparison of joint learning modules with orienta-
tion learning employed for road segmentation. It shows that our
stacked multi-branch module improves the AP LS by 2.7%.

Multi- Orientation Feature Connectivity

SpaceNet

DeepGlobe

Scale

Learning

Fusion

Reﬁne

IoU a AP LS IoU a AP LS
67.98
61.51

64.23

58.70

✓

✓

✓

✓

✓

✓

✓

✓

✓

✓

61.80

63.44

63.75

63.76

58.49

61.78

63.65

63.79

64.44

66.81

67.21

67.02

67.92

72.03

73.21

73.20

Table 3: Step-wise improvement with multi-scale loss, orientation
learning, and cross task information ﬂow by feature fusion.

Figure 8: Feature maps for different stages in the proposed model.
Shared: feature map from the shared encoder and before the ﬁrst
stack. Segmentation / Orientation: feature map from segmentation
/ orientation branch of the ﬁrst stack of multi-branch module be-
fore fusion. Fused: additive fusion of all feature maps which is fed
to the second stack of multi-branch module.

of multi-branch modules improve the road connectivity over
the single encoder-decoder modules by ∼ 2.5%.

We study the incremental improvement as a result of
our contributions and show the results in Table 3. Initially,
we hypothesize that knowledge of road orientation helps in
tracing lines to connect the broken road segments, which
we achieve by cross information ﬂow between the tasks in
stacked multi-branch module. We discover that adding the
orientation features with segmentation performs better. This
conﬁrms that the neural network utilize the orientation in-
formation to connect the broken road segments and improve
AP LS by 1.87% and 1.18% on respective datasets.

Performing connectivity reﬁnement on the segmenta-
tion output of stacked multi-branch model improves AP LS
marginally. We hypothesize that the n-stack multi branch

10391

Figure 7: Quantitative improvement with orientation learning and
connectivity reﬁnement. R18, L34: ResNet18 and LinkNet34
based encoder-decoder as joint learning model. S and D: denote
the SpaceNet and DeepGlobe dataset. Horizontal axes show road
IoU (top) and AP LS improvements (bottom).

the signiﬁcance of orientation task in road connectivity as
an auxiliary loss, we compare it with another shared task
of predicting road junctions in multi-task learning frame-
work. In the interest of space, we defer details of Junction
Learning to the supplementary material. The results in Ta-
ble 1 shows that the connectivity metric AP LS improve
with orientation task and not due to the multi-task learning.
This validates the efﬁcacy of the orientation task in predict-
ing the connected road topology.

Connectivity Reﬁnement:
In contrast to [23], we pre-
train the connectivity reﬁnement model with corrupted road
ground-truth masks. We analyze different manipulations of
road masks such as erasing regions with block structures of
different sizes and linear structures of different lengths and
thickness. We also add false roads randomly to the road
masks with the same structure. We found that manipula-
tions with linear structures appear similar to the real seg-
mentation outputs, as it blends in with the linear road struc-
tures thus, we report results for only such manipulations.
Figure 7 shows the improvement with connectivity reﬁne-
ment and marginal improvement in road IoU . This shows
that the proposed reﬁnement is able to connect gaps and re-
move the false roads, rather than enhancing road width.

Stacked multi-branch module: We perform experiments
to compare our proposed stacked multi-branch module as
joint learning module with the state-of-art CNN models
commonly used for segmentation of thin structures. We
compare the number (n) of multi-branch modules in the
model and found that performance stabilizes with two mod-
ules. Hence, we employ two stacks of multi-branch mod-
ules in our ﬁnal pipeline. We hypothesize that with more
training data, it would be beneﬁcial to add more multi-
branch modules which also makes the network deeper with-
out overﬁtting. The results in Table 2 shows that stacking

ImageSharedSegmentationOrientationFusedMethod

SpaceNet

DeepGlobe

Precision

Recall

F1

DeepRoadMapper (segmentation) [19]

DeepRoadMapper (full) [19]

Topology Loss (with BCE) [23]

Topology Loss (with SoftIoU) [23]

LinkNet34 [7]

LinkNet34 [7] + Orientation (Ours)

MatAN [20]

RoadCNN (segmentation) [3]

Ours (full)

60.61

57.57

50.35

52.94

61.30

63.82

49.84

62.82

64.65

60.80

58.29

50.32

52.86

61.45

63.96

50.16

63.09

64.77

60.71

57.93

50.34

52.90

61.39

63.89

50.01

62.95

64.71

IoU r
43.58

40.77

33.63

35.96

44.27

46.94

33.34

45.94

47.83

IoU a
59.99

N/A

56.29

57.69

60.33

62.45

52.86

62.34

63.75

AP LS

Precision

Recall

F1

54.25

50.59

49.00

51.99

55.69

60.76

46.44

58.41

63.65

79.82

77.15

76.69

79.63

78.34

81.24

57.59

82.85

83.79

80.31

77.48

75.76

79.88

78.85

81.73

56.96

83.73

84.14

80.07

77.32

76.22

79.75

78.59

81.48

57.28

83.29

83.97

IoU r
66.76

63.02

61.58

66.32

64.73

68.75

40.13

71.36

72.37

IoU a
62.58

N/A

64.95

64.94

62.75

64.71

46.88

67.61

67.21

AP LS

65.56

61.66

56.91

65.96

65.33

68.71

47.15

69.65

73.12

Table 4: Comparison of our technique with the state-of-the-art road network extraction techniques. IoU r and IoU a refers to relaxed and
accurate road IoU . Ours (full) include the proposed stacked multi-branch module with orientation learning. We use implementation from
[3] for DeepRoadMapper [19] and our own implementation for [23].

modules enhance the representation during fusion (Figure
8) in a similar way as connectivity reﬁnement iteratively en-
hance the predictions. The second multi-branch module in-
herently reﬁne the road connectivity, which functions upon
the fused feature space. In the end, joint learning and fusion
improve the road IoU by ∼ 2.5% and AP LS by ∼ 5% on
both datasets over the pixel-wise classiﬁcation supervision.

Effect of fusion: We perform ablation study on fusion
strategies to enable the information ﬂow and report the re-
sults in Table 5. We discover that feature fusion by adding
the orientation features with segmentation performs better.
It shows that the simple feature addition improve the AP LS
by 1.87% and 1.18% over the no fusion on both datasets.

Fusion

SpaceNet

DeepGlobe

No Fusion

Sum

Concatenate

IoU a
63.44

63.75

63.53

AP LS

61.78

63.65

63.01

IoU a
66.81

67.21

66.59

AP LS

72.03

73.21

72.23

Table 5: Effect of different fusion strategies in our proposed mod-
ule to allow the information ﬂow between orientation learning and
segmentation tasks in the ﬁrst stack.

Comparisons with state-of-the-art results: We compare
the effectiveness of the proposed methods with state-of-art
segmentation based methods [19], [20] and [23] (see Table
4 and Figure 9). M´attyus et al. [19] hypothesize the con-
nections with shortest path algorithms between the nodes
of road graph and validates the connection with a classiﬁer.
We found that the classiﬁer is unable to detect the false con-
nections in cases with densely connected roads which leads
to a decrease in AP LS after post-processing. Mosinka et
al. [23] introduce the topology loss term with recursive re-
ﬁnement. However, it also face challenges in predicting the
roads in densely connected areas, and unpaved roads. In-
spite of large diversity in both datasets, our approach sig-
niﬁcantly improves the connectivity in the extracted road
graph against the baselines. However, the proposed tech-

Figure 9: Qualitative Comparisons with state-of-the-art methods
— DRM [19], TL [23], L34 [7], and MatAN [20].

nique faces challenges to accurately connect roads under
the bridges as well as in the presence of large occlusion
(see row #4 in Figure 9). We also observe the false road
detection in farm outlines due to it’s visual similarity with
unpaved roads and parking lots on top of buildings due to
the absence of relative depth cues. We show additional qual-
itative results in the supplementary material.

6. Conclusion

In this paper, we propose a novel task of orientation
learning that constrain the model to produce connected and
topologically accurate road networks. We show that pixel-
wise classiﬁcation supervision leads to road networks with
fragmented road segments and poor connectivity. Our ex-
periments show that the joint learning of orientation and
segmentation followed by connectivity reﬁnement leads to
a signiﬁcant improvement in the road connectivity. We
also show the effectiveness of the stacked encoder-decoder
structure model as a joint learning module, which can efﬁ-
ciently utilize the information from related tasks.

Acknowledgement We thank NSERC for partially sup-
porting the work.

10392

[19] Gell´ert M´attyus, Wenjie Luo, and Raquel Urtasun. Deep-
roadmapper: Extracting road topology from aerial images.
In ICCV, 2017. 1, 2, 5, 8

[20] Gell´ert M´attyus and Raquel Urtasun. Matching adversarial

networks. In CVPR, 2018. 2, 8

[21] Volodymyr Mnih and Geoffrey E Hinton. Learning to detect
roads in high-resolution aerial images. In ECCV, 2010. 1, 2
[22] Volodymyr Mnih and Geoffrey E Hinton. Learning to label

aerial images from noisy data. In ICML, 2012. 1, 2, 6

[23] Agata Mosinska, Pablo M´arquez-Neila, Mateusz Kozinski,
and Pascal Fua. Beyond the pixel-wise loss for topology-
aware delineation. In CVPR, 2018. 1, 2, 7, 8

[24] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In ECCV, 2016.
2

[25] Deepak Pathak, Philipp Kr¨ahenb¨uhl, Jeff Donahue, Trevor
Darrell, and Alexei Efros. Context encoders: Feature learn-
ing by inpainting. In CVPR, 2016. 4

[26] Urs Ramer. An iterative procedure for the polygonal approx-
imation of plane curves. Computer graphics and image pro-
cessing, 1972. 6

[27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 2, 7

[28] Suriya Singh, Anil Batra, Guan Pang, Lorenzo Torresani,
Saikat Basu, Manohar Paluri, and C. V. Jawahar. Self-
supervised feature learning for semantic segmentation of
overhead imagery. In BMVC, 2018. 1, 2, 4, 6

[29] Radu Stoica, Xavier Descombes, and Josiane Zerubia. A
gibbs point process for road extraction from remotely sensed
images. IJCV, 2004. 1, 2

[30] Adam Van Etten, Dave Lindenbaum, and Todd M Bacastow.
Spacenet: A remote sensing dataset and challenge series.
arXiv preprint arXiv:1807.01232, 2018. 1, 2, 5, 6

[31] Carles Ventura, Jordi Pont-Tuset, Sergi Caelles, Kevis-
Kokitsi Maninis, and Luc Van Gool. Iterative deep learning
for road topology extraction. In BMVC, 2018. 1, 2

[32] Jacob Walker, Abhinav Gupta, and Martial Hebert. Dense
optical ﬂow prediction from a static image. In ICCV, 2015.
6

[33] Jan D Wegner, Javier A Montoya-Zegarra, and Konrad
Schindler. A higher-order crf model for road network ex-
traction. In CVPR, 2013. 1, 2

[34] Yu Zhang and Qiang Yang. A survey on multi-task learning.

CoRR, abs/1707.08114, 2017. 3

[35] Zhengxin Zhang, Qingjie Liu, and Yunhong Wang. Road

extraction by deep residual u-net. IGRSL, 2018. 1

[36] Cao Zhe, Simon Tomas, Wei Shih-En, and Sheikh Yaser.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In CVPR, 2017. 3

References

[1] Min Bai and Raquel Urtasun. Deep watershed transform for

instance segmentation. In CVPR, 2017. 3

[2] Meir Barzohar and David B Cooper. Automatic ﬁnding of
main roads in aerial images by using geometric-stochastic
models and estimation. PAMI, 1996. 1, 2

[3] Favyen Bastani, Songtao He, Soﬁane Abbar, Mohammad Al-
izadeh, Hari Balakrishnan, Sanjay Chawla, Sam Madden,
and David DeWitt. Roadtracer: Automatic extraction of road
networks from aerial images. In CVPR, 2018. 1, 2, 4, 8

[4] R Caruna. Multitask learning: A knowledge-based source of

inductive bias. In ICML, 1993. 3, 4

[5] Lluıs Castrej´on, Kaustav Kundu, Raquel Urtasun, and Sanja
Fidler. Annotating object instances with a polygon-rnn. In
CVPR, 2017. 4

[6] Dengfeng Chai, Wolfgang Forstner, and Florent Lafarge.
Recovering line-networks in images by junction-point pro-
cesses. In CVPR, 2013. 1, 2

[7] Abhishek Chaurasia and Eugenio Culurciello. Linknet: Ex-
ploiting encoder representations for efﬁcient semantic seg-
mentation. In VCIP, 2017. 2, 6, 7, 8

[8] Guangliang Cheng, Ying Wang, Shibiao Xu, Hongzhen
Wang, Shiming Xiang, and Chunhong Pan. Automatic road
detection and centerline extraction via cascaded end-to-end
convolutional neural network. Transactions on Geoscience
and Remote Sensing, 2017. 1, 2

[9] Ronan Collobert and Jason Weston. A uniﬁed architecture
for natural language processing: Deep neural networks with
multitask learning. In ICML, 2008. 3

[10] Dragos Costea and Marius Leordeanu. Aerial image geolo-
calization from recognition and matching of roads and inter-
sections. arXiv preprint arXiv:1605.08323, 2016. 1, 2

[11] Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan
Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia,
and Ramesh Raskar. Deepglobe 2018: A challenge to parse
the earth through satellite images. In CVPRW, 2018. 1, 2, 6
[12] Alexey Dosovitskiy and Thomas Brox. Generating images
with perceptual similarity metrics based on deep networks.
In NIPS, 2016. 2

[13] David H Douglas and Thomas K Peucker. Algorithms for
the reduction of the number of points required to represent
a digitized line or its caricature. Cartographica: The Inter-
national Journal for Geographic Information and Geovisu-
alization, 1973. 6

[14] Ruohan Gao and Kristen Grauman. On-demand learning for

deep image restoration. In CVPR, 2017. 4

[15] Ruohan Gao, Bo Xiong, and Kristen Grauman.

Im2ﬂow:
Motion hallucination from static images for action recogni-
tion. In CVPR, 2018. 6

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 6, 7

[17] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geome-
try and semantics. In CVPR, 2018. 3, 4

[18] Ivan Laptev, Helmut Mayer, Tony Lindeberg, Wolfgang Eck-
stein, Carsten Steger, and Albert Baumgartner. Automatic
extraction of roads from aerial images based on scale space
and snakes. MVA, 2000. 1, 2

10393

