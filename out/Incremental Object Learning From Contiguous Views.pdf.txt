Incremental Object Learning from Contiguous Views

Stefan Stojanov1, Samarth Mishra∗1, Ngoc Anh Thai∗1, Nikhil Dhanda1, Ahmad Humayun1,

Chen Yu2, Linda B. Smith2, James M. Rehg1

Georgia Institute of Technology1
Indiana University Bloomington2

{sstojanov, smishra, athai6, nn3, ahmadh, rehg}@gatech.edu

{chenyu, smith4}@indiana.edu

Abstract

Pick up

Put down

Object Naming Event

In this work, we present CRIB (Continual Recognition
Inspired by Babies), a synthetic incremental object learn-
ing environment that can produce data that models visual
imagery produced by object exploration in early infancy.
CRIB is coupled with a new 3D object dataset, Toys-200,
that contains 200 unique toy-like object instances, and is
also compatible with existing 3D datasets. Through ex-
tensive empirical evaluation of state-of-the-art incremental
learning algorithms, we ﬁnd the novel empirical result that
repetition can signiﬁcantly ameliorate the effects of catas-
trophic forgetting. Furthermore, we ﬁnd that in certain
cases repetition allows for performance approaching that
of batch learning algorithms. Finally, we propose an unsu-
pervised incremental learning task with intriguing baseline
results.

1. Introduction

Children are amazing learning machines.1

Infants ac-
quire extensive object knowledge through self-directed play
with minimal supervision, a fact which is remarkable in
contrast to the quantity of labeled data required by current
deep learning methods. During play, infants pick up, exam-
ine, and put down toys of their own volition. The moments
in which a supervisory signal is available, for example when
an adult names an object, are extremely rare in compari-
son to the huge volume of unlabeled perceptual inputs. See
Fig. 1 for a schematic of this play process.

Research in child development [8, 49, 22] has identiﬁed
ﬁve key properties of infants’ play experiences. First, while
infants become experts at object categorization, the bulk of
their early visual experience involves object instances, in

∗Equal contribution.
1In the domain of word learning, for example, children acquire an aver-
age of 8 to 10 new words per day and reach a vocabulary of 60,000 words
by adulthood [34].

Time

Object 7

Object 4

Object 7

Figure 1: Schematic of incremental object learning based
on infant play. Objects occur sequentially as exposures con-
sisting of sets of frames with contiguous viewpoints. A
sparse and noisy supervisory signal for category learning
(naming events), accompanies the wealth of visual data.

the form of toys and everyday objects. Second, their ex-
posure to object instances is highly repetitive, with many
objects (e.g. a favorite sippy cup) recurring over and over
again [8]. Third, when infants hold and manipulate objects,
they generate extended, contiguous views that may help in
revealing 3D object shape [49, 21, 37]. Fourth, infant learn-
ing is fundamentally incremental, as objects are held and
examined in sequence, and once an object has been put
down, its imagery is no longer available for learning. Fifth,
infants must provide their own supervision when learning
about instances, and leverage a sparse, noisy, and unsyn-
chronized supervisory signal when learning object names.2
These properties of the infant learning environment stand
in stark contrast to current methods for object learning
in computer vision, which are based on processing mini-
batches of randomly-sampled, labelled frames that cover
a signiﬁcant subset of the label space. This approach en-
sures that gradient updates do not favor one class over
another in moving collectively towards higher accuracy.
However, when data is processed incrementally in standard
deep learning architectures, the result is catastrophic for-
getting, in which object representations developed early in
training are forgotten at the expense of more recent exam-
ples [14, 17]. Recent works on incremental learning have

2While there is a debate in developmental science about the extent to
which children’s knowledge is innate versus learned, in this paper we focus
on the task of learning from visual experience.

18777

Figure 2: A rendering of approximately one third of the 3D models in Toys-200.

developed methods using distillation loss [29] and exem-
plars [38, 6] to address the catastrophic forgetting problem,
and they represent a valuable point of contact with infant
learning. Crucially, however, these prior works have not
incorporated repetition, which we will demonstrate to be
critical for effective incremental learning (see Sec. 4.3).

This paper introduces a developmentally-motivated en-
vironment for object learning known as CRIB (Continual
Recognition Inspired by Babies), which supports incremen-
tal learning of object instances (and categories) from con-
tiguous views with repetition, in both supervised and un-
supervised settings. CRIB is an ideal testbed for research
in incremental learning, as it provides convenient access to
unlimited data with the ability to precisely manipulate key
dimensions of the learning task and ensure reproducibility.
CRIB comes with a novel dataset, Toys-200, consisting of
3D models of 200 diverse and developmentally-appropriate
object instances. Our experiments with CRIB have uncov-
ered some intriguing empirical properties of incremental
learning tasks which have not been observed in prior work.
Speciﬁcally, we show that in incremental learning with rep-
etition it is possible to ameliorate the effects of catastrophic
forgetting, with the performance of pre-trained models ap-
proaching that of batch-learning. These ﬁndings hold for
both instance and category learning across a diversity of
datasets (Toys-200, ShapeNet [7], and CIFAR[26]).

CRIB is implemented as an API that can easily be incor-
porated into data loaders for standard deep learning frame-
works like PyTorch and TensorFlow, and will be made
freely-available to the research community.
It supports
the paradigm illustrated in Figure 1, in which the learner
receives a sequence of object learning exposures, each
one consisting of a set of frames corresponding to a con-
tiguous sequence of views of a particular object instance.
CRIB supports three different incremental learning tasks,
and we provide baseline results and extensive experimen-
tal results for each in Sec. 4. We hope that CRIB will en-

able new lines of attack on both incremental learning and
developmentally-motivated object learning problems.
In
summary, this work makes the following contributions:

• The CRIB environment for developmentally-inspired
object learning along with the Toys-200 dataset of
developmentally-plausible 3D object instances

• A freely-available data generator which integrates into
standard deep learning platforms, supports existing 3D
datasets, and is capable of generating unlimited data
for incremental instance and category learning

• The identiﬁcation of incremental learning with repeti-
tion as a key learning task which makes it possible to
ameliorate the effects of catastrophic forgetting

• An extensive evaluation of the effects of distillation
loss, explicit exemplar memory and repetitions on
both supervised and unsupervised incremental learn-
ing tasks3

2. Related Work

This paper is most closely related to prior work on incre-
mental learning using deep models, and our experiments
leverage existing algorithms for learning without forget-
ting [29], iCARL [38], and E2EIL [6].
In comparison
to these works, we provide a novel learning environment
(CRIB with Toys-200) and several novel tasks, as well as
extensive experiments on multiple datasets that illuminate
important aspects of incremental learning approaches, such
as the role of repeated exposures, distillation loss, and the
impact of exemplar set size, on incremental learning per-
formance. In contrast, prior works [29, 24, 28, 32, 38, 6]
did not address instance learning or the use of 3D models
to learn from contiguous viewpoints. They addressed only

3All resources needed to reproduce the experimental results in this pa-
per and any subsequent releases of software and data will be available at
https://iolfcv.github.io/

8778

the single exposure paradigm for category learning using
existing image datasets of a ﬁxed size.

Another related body of work is open world recognition
(of which representative citations are [2, 3, 10]). It is rel-
evant due to its emphasis on self-supervision. Our exper-
iments on weakly-supervised learning from sequential ob-
ject exposures in Sec. 4.4 are a point of contact with this
literature, although our speciﬁc paradigm and methods dif-
fer from this prior work.

Our development of CRIB is part of an on-going effort
to explore the use of computer graphics rendering and sim-
ulation environments to investigate machine learning top-
ics in controlled settings and address the large scale data
requirements of deep learning. Examples include purpose-
built autonomous driving simulators such as TORCS [47]
and CARLA [13], and efforts to leverage commercial video
games [25, 40, 39]. Multiple synthetic optical ﬂow datsets
[33, 5, 46] have led to performance improvements, as have
generated 3D car assets from [35]. Although the Active Vi-
sion Dataset [1] is not synthetic, it is a dense collection of
RGB-D images of real scenes that can simulate the visual
information perceived by a robot moving through an envi-
ronment. We are not aware of any prior work on simulation
environments which speciﬁcally target the learning tasks or
synthetic data generation goals addressed by CRIB.

Our work on Toys-200 is related to other efforts in curat-
ing datasets of objects for recognition tasks. Prior work on
collecting real image datasets of 3D objects, such as NORB
[27], COIL [36], and more recently, CORe50 [31], are less
relevant to this work. More closely-related are works that
created synthetic 3D object datasets, such as ShapeNet [7]
and Sculptures [45], which have led to signiﬁcant progress
in the domain [41]. In comparison, Toys-200 contains fewer
instances (307 for Sculptures and 51K for ShapeNet). How-
ever, it occupies a sweet-spot in terms of size and diversity,
as the Toys-200 objects are highly diverse in comparison to
both Sculptures and ShapeNet and were designed to reﬂect
the types of toys and everyday objects that infants would
be likely to encounter. In conjunction with CRIB, we can
support a much wider range of data generation approaches
than any prior works, as summarized in Table ?? of the Ap-
pendix.

This paper is also connected to a long line of research on
developmentally-inspired approaches to robotics and learn-
ing (e.g. venues such as [9]). Works such as Gepperth
et. al. [15] and Kanan et. al. [23] connect to our interest
in biologically-inspired incremental learning. Recent work
by Haber et. al. [18] shares our interest in play behavior.
Other works have developed speciﬁc computational models
for children’s cognitive processes (see [30] for a recent ex-
ample). None of these works address the speciﬁc tasks or
settings which characterize our paper.

3. Approach

In order to achieve our goal of exploring the behavior of
incremental object recognition algorithms in a developmen-
tally plausible setting, we require a visual learning environ-
ment with the following characteristics:
Unlimited Data: The ability to efﬁciently generate unlim-
ited visual data for each of our objects is critical because
it allows us to vary the amount of repetition and generate
arbitrarily long experimental runs while ensuring that the
learning algorithm continues to receive novel inputs.
Developmental relevance: Our goal is to generate visual
data which simulates the object exploration behaviors in
early infancy. This requires the use of developmentally-
plausible object sets and the ability to generate sequences
of contiguous object views.
Integration: To facilitate rapid experimentation, it must
be easy to integrate our learning environment with existing
data loading mechanisms in modern deep learning frame-
works.

We develop CRIB (Continual Recognition Inspired by
Babies)—a synthetic visual learning environment that ful-
ﬁlls these requirements. CRIB can generate unlimited
learning exposures in the form of contiguous views of ob-
ject instances. Since CRIB is implemented as a Python
API it is directly compatible with all popular deep learning
frameworks. CRIB is built using the free and cross platform
3D graphics software Blender and uses the Cycles ray trac-
ing engine for rendering. The following section describes
how CRIB provides a novel environment for incremental
learning experiments.

3.1. CRIB Learning Environment

In this section we describe the process by which we cre-
ated the Toys-200 dataset and the details of the object ren-
dering approach in CRIB.

3.1.1

3D Object Models for Toys-200

A highly diverse set of toy-like objects is central to gen-
erating developmentally plausible object instance data for
visual recognition. We collected the Toys-200 dataset of
200 unique toy object models from Blendswap [4], select-
ing models that were freely-available under a CC license.
We began by targeting a core set of 30 speciﬁc object cate-
gories [49] that are frequently used in research with infants,
identifying the best 3D model instance for each one.
In
order to build a challenging and visually-diverse (See Fig-
ure 2) dataset, we supplemented this initial set with addi-
tional toy-like objects. The criterion of ”toy-like” was im-
plemented by selecting objects which were similar to the
core objects in terms of their level of detail in shape and
appearance, and their plausibility for being a child’s toy. A
speciﬁc material shader was developed to give Toys-200 by

8779

Figure 3: Steps in generating visual exposures using CRIB (top to bottom): 1. Foreground object rendering, 2. background
scene selection, 3. foreground and background compositing. A visual exposure consists of multiple clips corresponding to
arcs on view sphere.

combining basic Blender material shaders, set up to give the
objects surface texture and reﬂectance properties of plastic,
toy-like objects. For our experiments involving category
learning, we used the well-known ShapeNet [7] dataset,
with appropriate modiﬁcations to incorporate it into CRIB.
Refer to the Appendix for more details.

3.1.2 Generating Learning Exposures

A learning exposure is a sequence of images obtained by
rotating an object of interest relative to a ﬁxed camera, de-
signed to simulate the kind of object views that children are
known to generate during object play [21]. An exposure
consists of a sequence of short video clips, where each clip
is generated by rendering a sequence of images of the object
as its pose is linearly interpolated between two ﬁxed poses.
See Figure 3 for an illustration of one sequence. The ﬁnal
object pose for one sequence is the starting pose for the next,
and the images from each sequence are concatenated into a
single contiguous sequence to form the learning exposure.

CRIB generates learning exposures from 3D object mod-
els using a set of user-speciﬁed API parameters. Below we
discuss key aspects of the rendering process, and a detailed
technical description of all the steps to generate a learning
exposure can be found in the Appendix.

Lighting: In the API, the user speciﬁes a lighting setting
of either four point or three rod light sources placed above
the object. Further characteristics are deﬁned by parameters
for pose, temperature and intensity.

Object rotation: Object rotation in CRIB is generated
by linearly interpolating between object poses (azimuth, ro-
tation, elevation, scale) over a number of frames. To spec-
ify the qualitative characteristics of object rotation in the
learning exposure, the user speciﬁes parameters for the to-
tal number of frames in the learning exposure and the total
number of different object poses for interpolation.

the following fully-automated process proceeds: The target
object is imported into Blender, its center of mass is esti-
mated and it is positioned in the center of the camera frame.
The object is then appropriately scaled so that it remains
remains inside the camera ﬁeld of view during the rotating
motion around its center of mass and the change in scale.

Foreground rendering: The speciﬁed light sources are
instantiated and the sequence of frames is rendered with-
out a background. At this step, instance segmentations and
bounding boxes are collected for the foreground object.

Background rendering: Backgrounds are image se-
quences of objects from Toys-200, which are distributed
over the ﬂoor to create a cluttered background. The cam-
era above the objects moves slightly over time to emulate
head motion (such background frames are illustrated in Fig-
ure 3). This results in a dynamic, cluttered background en-
vironment which makes the recognition task more challeng-
ing and simulates real-life play scenarios in which a child
interacts with a set of toys (e.g. dumped from a toybox).
We ensure that the foreground object is not also present in
the background. Once the background sequence has been
rendered, the ﬁnal step is to composite the foreground and
background layers in each frame, and add a small amount
of pixel-wise noise.

Testing image generation: For evaluation purposes,
CRIB can also generate single images of a target object at a
random rotation, elevation and scale, with random lighting
conditions and backgrounds.

3.2. Learning Tasks in CRIB

CRIB supports three different incremental learning sce-
narios, two of which are novel. In each case, CRIB provides
learning exposures which are combined with stored past im-
ages in forming minibatches which are used for training.
The details vary with the task and the architecture, and are
detailed in the following sections.

Preprocessing: Once the API parameters are speciﬁed,

Supervised Single Exposure: This is the standard in-

8780

Supervised Repeated Exposure:

cremental learning task, in which classes or instances are
presented sequentially to the learner. The key property is
that the learner sees each object exactly once. This leads to
catastrophic forgetting in all of the cases that we evaluated.
In this novel task,
classes or instances are presented to the learner sequentially
with repetition. At random, the learner is given new learn-
ing exposures for previously-seen objects. Our experiments
demonstrate that allowing a limited amount of repetition
(e.g. 10 exposures each for 200 object instances) allows
existing algorithms to approach the batch performance.4

Unsupervised Repeated Exposure: In this task, learn-
ers receive repeated exposures to a sequence of objects, but
no labels are provided. This is similar to discriminative in-
cremental clustering [16]. This very challenging task re-
quires the learner to identify learning instances correspond-
ing to novel objects, and re-identify previously-seen ob-
jects. It mirrors the challenge infants face during play, as
most of their learning exposures will not be accompanied
by an object name.

4. Experiments

In this section, we introduce the baseline algorithms in
our study, and present novel experimental results for the
three incremental learning tasks from Sec. 3.2. Performance
is measured using incremental accuracy as in [38]: Follow-
ing training on each learning exposure, the classiﬁcation
accuracy is computed on unseen test samples from all in-
stances or categories the learner has seen up to that point.

All learning exposures generated with CRIB are 100
frames long and interpolate between three randomly-chosen
points on the view sphere, with scale smoothly varying
from 0.3 to 1.1. Light source position is jittered at random
and light intensity is randomly-sampled from 4000-6000K
(indoor lighting temperature range). 100 random testing
frames are generated for each object. Refer to the appendix
for additional details.

4.1. Incremental Learning Methods

We produced our own implementations of three recent
CNN-based incremental learning algorithms [29, 38, 6].
Differences in our implementation from the original are
described below and in the appendix. All methods use
ResNet-34 [19] as the backbone architecture.

LwF [29] addresses catastrophic forgetting by modify-
ing the loss function used to train a standard CNN incre-
mentally. Each time a new class is introduced, the fully
connected layer of the CNN is expanded by adding an out-
put sigmoid unit for the new class. Distillation loss [20] is

4Note that in experiments with 3D rendered data, such as Toys-200, it
is still the case that each rendered image is used only once, as each new
learning instance will correspond to a new trajectory on the view sphere.

applied to the outputs for the other classes in attempt to pre-
serve the information they encode and prevent signiﬁcant
changes due to backprop on the current class. Our LwF dif-
fers from [29] in using sigmoid units rather than a softmax
layer for classiﬁcation, and in performing additional data
augmentation.

iCaRL [38] builds on LwF by including explicit mem-
ory in the form of an exemplar set managed by the learn-
ing algorithm. The exemplars are used to perform nearest
exemplar mean classiﬁcation in feature space. The infer-
ence procedure consists of computing normalized exemplar
mean features per class using the CNN, and then classifying
by determining the nearest exemplar mean from the normal-
ized features of each testing sample. Training follows LwF
when distillation loss is used, otherwise it is standard CNN
training. Our iCaRL [38] implementation uses additional
data augmentation.

E2EIL [6] builds on the previous two methods. During
training, the loss takes into account ground truth labels of
the samples from the other classes as well as the current
class. Unlike iCaRL, training is end-to-end since the net-
work outputs are used for classiﬁcation. E2EIL adds bal-
anced ﬁne-tuning which targets the case when the number
of samples from the other classes is signiﬁcantly lower than
the number of samples for the current class. Exemplar set
construction follows iCaRL, but is done twice: after training
and after balanced ﬁne-tuning. Our implementation adopts
a temperature-squared weighting [20] for distillation loss,
computes distillation loss over all seen classes, and uses a
different data augmentation scheme.

Our experiments include both training from scratch and
initializing weights from a pre-trained ILSVRC-2014 [11]
architecture. Based on prior transfer learning results [44,
48, 12], we would expect that starting from a pretrained ar-
chitecture should yield better performance, and our results
conﬁrm this. We also train with and without distillation
loss, in order to quantify its beneﬁt. Note that when dis-
tillation loss is not used, we apply the classiﬁcation loss to
all output nodes and use the exemplar labels.

Our naming convention: in iCaRL-PT-ND, PT indicates
starting with a pre-trained backbone architecture, and ND
means that distillation loss is not used, whereas iCaRL-S-D
refers to training from scratch (i.e. random weight initial-
ization) and using distillation loss.

4.2. Single Exposure Yields Catastrophic Forgetting

In this section we demonstrate that the single exposure
task leads to catastrophic forgetting for all of our baseline
methods in two datasets: Toys-200 and CIFAR-100.
In
comparison to prior work [29, 38, 6], our Toys-200 experi-
ments are the ﬁrst demonstration of catastrophic forgetting
in instance learning, and our CIFAR-100 experiments differ
in that we present classes one-at-a-time rather than two or

8781

(a)

(b)

Figure 4: (a) Performance of iCaRL, E2EIL and LwF when presented with a single exposure for each object instance from
CRIB-Toys-200. (b) shows performance of the same methods with repeated exposure.

more. For the Toys-200 experiment, we use 200 exposures,
one for each object instance. E2EIL and iCaRL use an ex-
emplar set size of 600 images, or 3% of the total data (as
compared to 4% in [38, 6]). We computed standard error
bars by repeating each experiment 3 times.

As evident in Figure 4a, all results for Toys-200 have
a general downward trend, which is similar to the results
in [38, 29, 6] and is attributed to catastrophic forgetting of
the instances which were seen early in the sequence. The re-
sults for iCaRL-PT-(D/ND) show that training with distilla-
tion is not favorable in this task, while the results for E2EIL-
PT-(D/ND) show that distillation loss does not make a dif-
ference. We ﬁnd that test accuracy can be easily improved
by using a pre-trained model, aligning with [44, 48, 12].
In addition, we tested on CIFAR-100 [26] with iCaRL-S-
ND and iCaRL-PT-ND (see Figure 5) with single classes
presented sequentially. Further experiments using random
initialization are included in the Appendix.

Figure 5: Performance of iCaRL-S-ND and iCaRL-PT-ND
on CIFAR-100 conﬁrm catastrophic forgetting. Both algo-
rithms have an exemplar set size of 2000.

4.3. Repetition Reduces Catastrophic Forgetting

using 2000 learning exposures (each object appearing ten
times), with an explicit memory of 600 exemplars. For ev-
ery experimental run, we generate a random sequence of
object instances such that all methods experience the same
number of objects by each time step, but not the same in-
stances in the same order. Figure 4b shows the results.
E2EIL and iCaRL-PT-ND achieve an accuracy close to the
batch learning algorithm, whereas iCaRL-PT-D does not
show an improvement. Note that the performance gap be-
tween iCaRL-PT-D and iCaRL-PT-ND in this task is larger
in comparison to the single exposure task. This poten-
tially indicates that distillation loss is hindering the ability
to leverage repeated exposures for iCaRL, and highlights
the advantage of simply using the exemplar labels. Results
for algorithms trained from random initialization in the Ap-
pendix further conﬁrm this ﬁnding.

We perform further experiments using three datasets:
CRIB-Toys, CRIB-ShapeNet [7] and CIFAR [26] to (1)
evaluate whether incremental learning with repeated expo-
sures can allow incremental algorithms to get close to the
performance of batch algorithms beyond an instance learn-
ing task (2) evaluate the importance of the number of ex-
emplars on the accuracy gains from repeated exposure. We
perform the following experiments:

1. CRIB-Toys-50: 50 objects over 500 learning expo-

sures (each object is shown 10 times).

2. CRIB-ShapeNet-20: 20 categories over 500 learn-
ing exposures. (25 instances from each category are
shown)

3. CIFAR-20: 20 categories over 1000 learning expo-

sures (each category is shown 50 times).

In this section, we demonstrate that introducing repe-
titions during incremental learning ameliorates the effects
of catastrophic forgetting, resulting in improvements in ac-
curacy and enabling the majority of tested algorithms to
eventually approach the performance of a pre-trained batch
learning method. We also examine the effect of the number
of exemplars.

Our ﬁrst experiments with repetition are with Toys-200,

Figure 6 contains the results for this experiment. It is
evident that the same trend applies to all three datasets: the
performance of the algorithms declines at ﬁrst before in-
creasing as they get more repeated exposures of previously
seen objects, and towards the end gets close to the perfor-
mance of a batch learning algorithm. We believe these are
the ﬁrst ﬁndings for learning with repetitions in incremental
learning.

8782

020406080100120140160180200Number of Learning Exposures0102030405060708090100% Test Accuracy over Seen ObjectsGround-truth UOSBatch-PTE2EIL-PT-DE2EIL-PT-NDiCaRL-PT-DiCaRL-PT-NDLwF-PT020406080100120140160180200Unique Objects Seen (UOS)CRIB-Toys-200 Single Exposures010020030040050060070080090010001100120013001400150016001700180019002000Number of Learning Exposures0102030405060708090100% Test accuracy over seen objectsGround-truth UOSBatch-SE2EIL-PT-DE2EIL-PT-NDiCaRL-PT-DiCaRL-PT-NDLwF-PT04080120160200Unique objects seen (UOS)CRIB-Toys-200 Repeated Exposures0102030405060708090100Number of Learning Exposures0102030405060708090100% Test accuracy over seen objectsGround-truth UOSBatch-PTiCaRL-PT-NDiCaRL-S-ND0102030405060708090100Unique objects seen (UOS)CIFAR-100 Single Exposures(a)

(c)

(b)

(d)

Figure 6: Top : Performance of (a) iCaRL-PT-ND and (b) E2EIL-PT-ND with different number of exemplars on 50 objects
of CRIB-Toys. Bottom : (c) Performance of iCaRL-PT-ND (400 exemplars) on 20 categories of CIFAR (d) Performance of
iCaRL-PT-ND (1500 exemplars) on 20 categories of CRIB-ShapeNet. (Best viewed with zoom)

For CRIB-Toys-50, as evident from Figures 6a and 6b,
both iCaRL-PT-ND and E2EIL-PT-ND maintain the up-
ward trend ﬁrst observed in Figure 4b. Additionally, this
experiment demonstrates that for an instance task, regard-
less of the total number of exemplars (18%, 12%, 3% or
1% of the total data), given sufﬁcient repetition, the accu-
racy of iCaRL-PT-ND is close to pre-trained batch perfor-
mance. While E2EIL-PT-ND maintains an increasing trend,
the variants trained with small numbers of exemplars are not
as close to the pre-trained batch model after 500 exposures.

CRIB-ShapeNet-20 is a categorization task where re-
peated exposures are different instances from the same cat-
egory. 25 instances for training and 15 for testing are cho-
sen randomly from 20 categories of the ShapeNet Core55
dataset [7]. Learning exposures generated with CRIB for
each instance are provided over 500 exposures and testing is
done on 100 frames of random object views, scale and light-
ing for each instance in the test set for a seen category. The
performance (Figure 6d), shows that repeated exposures to
new instances in the same category leads to improvements
for categorization. This result extends our initial ﬁnding of
improvement towards batch performance via repeated ex-
posures to a different task and dataset.

For CIFAR-20, we sample with replacement 100 images
from a total of 500 images per category for each learning
exposure. This exposes the algorithm to images repeatedly
during different exposures. The performance on iCaRL-PT-
ND decreases at ﬁrst and starts to go up after all 20 objects
have been seen (Figure 6c). Coverage for each category is

the portion of unique images seen within a category, with
the mean over all categories shown on the plot. The accu-
racy improvement rate decreases after 100% of the data is
shown to iCaRL-PT-ND. The ﬁnal incremental accuracy is
on par with a batch algorithm, showing that improvements
due to repetition of concepts are not unique to CRIB.

4.4. Incremental Learning Without Supervision

In this task a learning algorithm needs to do novelty
detection—to determine whether or not an exposure comes
from a novel instance, and recognition—to determine which
previously-seen instance it belongs to, prior to updating pa-
rameters.

Prior work on open set and open world recognition
[42, 43] tackles the subproblem of novelty detection by
thresholding on known class scores to detect whether a new
data point belongs to a class that has not been encountered.
Drawing from these works, we use the following algorithm
for our straightforward baseline based on iCaRL-PT-ND. At
any given exposure, in unit normalized feature space, the
algorithm ﬁrst ﬁnds the distance of the images from a learn-
ing exposure to all the exemplar means. This is followed by
ﬁnding the mean distance of the images in the exposure, and
using it as a score to determine whether the current object
has been previously seen. If the minimum distance-score is
more than a given threshold, the exposure labeled as com-
ing from a new object instance, otherwise it gets classiﬁed
as the previously seen instance with the minimum distance-
score. The threshold used was found as the optimal oper-

8783

050100150200250300350400450500Number of learning exposures0102030405060708090100% Test accuracy over seen objectsGround-truth UOSBatch-PT900 exemplars600 exemplars150 exemplars50 exemplars01020304050Unique objects seen (UOS)CRIB-Toys-50 Repeated Exposures E2EIL-PT-ND050100150200250300350400450500Number of learning exposures0102030405060708090100% Test accuracy over seen objectsGround-truth UOSBatch-PT900 exemplars600 exemplars150 exemplars50 exemplars01020304050Unique objects seen (UOS)CRIB-Toys-50 Repeated Exposures iCaRL-PT-ND01002003004005006007008009001000Number of learning exposures0102030405060708090100% Test accuracy over seen objects% Class coverageBatch-PT-20Average coverageGround-truth UOSiCaRL05101520Unique objects seen (UOS)CIFAR-20 Repeated Exposures0255075100125150175200225250275300325350375400425450475500Number of Learning Exposures0102030405060708090100% Test accuracy over seen objectsBatch-PTiCaRL-PT-ND test accGround-truth UCSAverage percentage of instances seen02468101214161820Unique categories seen (UCS)CRIB-ShapeNet-20 Repeated Exposures 

 

0
0
1
=
h
t
g
n
e
l
 
e
r
u
s
o
p
x
e
 
g
n
i
n
r
a
e
L

 

 

0
1
=
h
t
g
n
e
l
 
e
r
u
s
o
p
x
e
 
g
n
i
n
r
a
e
L

s
e
s
s
a
l
c
 

 

n
e
e
s
 
r
e
v
o
y
c
a
r
u
c
c
A
%

 

d
e
r
e
v
o
c
s
i
d
/
n
e
e
s
 
s
t
c
e
j
b
o

 
e
g
a
t
n
e
c
r
e
P

Number of Learning Exposures

Figure 7: Performance of iCaRL-PT-ND (600 exemplars) on unsupervised repeated exposures of CRIB-Toys-50, 100, 150,
200 (left to right) with learning exposure lengths of 10 and 100. The 10 length learning exposures are the ﬁrst 10 frames of
the 100 length learning exposure. (Best viewed with zoom)

ating point from a precision-recall analysis over the binary
classiﬁcation problem of novelty detection.

We evaluate this baseline algorithm on different learn-
ing exposure lengths and number of repeated exposure tasks
with CRIB. After each learning exposure, testing is done on
random views of ground truth seen objects. Since the labels
given by a learning algorithm in this task need not have any
correspondence with the ground truth labels, a one-to-one
correspondence is ﬁrst established between these sets of la-
bels based on a maximum accuracy matching, and then the
learning algorithm’s test accuracy is computed.

Figure 7 contains the results of this study. For 100 learn-
ing exposure length, the algorithm’s accuracy is constant
or decreases with a greater number of objects and four re-
peated exposures. Further, for the same learning exposure
length, the proportion of objects discovered compared to the
ground truth number of unique objects seen decreases with
greater object set sizes. Across all experiments with differ-
ent total numbers of objects, there is a consistent trend that
a smaller learning exposure length results in a lower ﬁnal
accuracy. Furthermore, a lower learning exposure length re-
sults in higher variability in performance over multiple runs
with different order of objects encountered.

categories and instances, based on rendering learning expo-
sures from 3D object models. CRIB models the kinds of
object views generated by infants during play. We intro-
duce a novel instance learning dataset called Toys-200. We
use CRIB to study three incremental learning scenarios and
demonstrate that allowing repeated exposures dramatically
improves the performance of state-of-the-art methods, al-
lowing them to converge to the batch learning accuracy in
many cases. Finally, we show intriguing results on the chal-
lenging new task of incremental learning without supervi-
sion. Our CRIB enviornment and data is freely-available,
and we hope that this work will enable and motivate the de-
velopment of new incremental learning methodology.

6. Acknowledgement

We would like to thank all the reviewers and Qian Shao
for his early contributions to the incremental learning ex-
periments. This work was supported by NSF Awards BCS-
1524565 and BCS-1523982. In addition, this work was par-
tially supported by the Indiana University Areas of Emer-
gent Research initiative in Learning: Brains, Machines,
Children.

5. Conclusion

References

We introduce CRIB, a novel environment for generating
unlimited training data for incremental learning of object

[1] P. Ammirato, A. C. Berg, and J. Kosecka. Active vision
dataset benchmark. In Proceedings of the IEEE Conference

8784

on Computer Vision and Pattern Recognition Workshops,
pages 2046–2049, 2018. 3

aware agents. In Advances in Neural Information Processing
Systems, pages 8398–8409, 2018. 3

[2] A. Bendale and T. Boult. Towards open world recognition.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 1893–1902, 2015. 3

[3] A. Bendale and T. E. Boult. Towards open set deep networks.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 1563–1572, 2016. 3
[4] blendswap.com. https://blendswap.com. 3
[5] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation. In
European Conference on Computer Vision, pages 611–625.
Springer, 2012. 3

[6] F. M. Castro, M. J. Marin-Jimenez, N. Guil, C. Schmid, and
K. Alahari. End-to-end incremental learning. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 233–248, 2018. 2, 5, 6

[7] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015. 2, 3, 4, 6, 7

[8] E. M. Clerkin, E. Hart, J. M. Rehg, C. Yu, and L. B. Smith.
Real-world visual statistics and infants’ ﬁrst-learned object
names. Phil. Trans. R. Soc. B, 372(1711):20160055, 2017. 1
http://www.

[9] Conference.

ICDL-EPIROB.

icdl-epirob.org/. 3

[10] R. De Rosa, T. Mensink, and B. Caputo. Online open world

recognition. arXiv preprint arXiv:1604.02275, 2016. 3

[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 248–255, 2009. 5

[12] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In International
Conference on Machine Learning, pages 647–655, 2014. 5,
6

[13] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and
V. Koltun. CARLA: An open urban driving simulator.
In
Proceedings of the 1st Annual Conference on Robot Learn-
ing, pages 1–16, 2017. 3

[14] R. M. French. Catastrophic Forgetting in Connectionist Net-
works. Trends in Cognitive Sciences, 3(4):128–135, 1999.
1

[15] A. Gepperth and C. Karaoguz. A Bio-inspired Incremen-
tal Learning Architecture for Applied Perceptual Problems.
Cognitive Computation, 8(5):924–934, 2016. 3

[16] R. Gomes, M. Welling, and P. Perona. Incremental learning
of nonparametric bayesian mixture models. In Computer Vi-
sion and Pattern Recognition, 2008. CVPR 2008. IEEE Con-
ference on, pages 1–8. IEEE, 2008. 5

[17] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and
Y. Bengio. An empiritcal investigation of catastrophic for-
getting in gradient-based neural networks. In International
Conference on Learning Representations (ICLR), 2014. 1

[18] N. Haber, D. Mrowca, S. Wang, L. F. Fei-Fei, and D. L.
Yamins. Learning to play with intrinsically-motivated, self-

[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning
for Image Recognition. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 770–778, 2016.
5

[20] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
5

[21] K. H. James, S. S. Jones, L. B. Smith, and S. N.
Swain. Young children’s self-generated object views and
object recognition. Journal of Cognition and Development,
15(3):393–401, 2014. 1, 4

[22] P. J. Kellman and M. E. Arterberry. The cradle of knowledge:

Development of perception in infancy. MIT press, 2000. 1

[23] R. Kemker and C. Kanan. FearNet: Brain-Inspired Model
In International Conference on

for Incremental Learning.
Learning Representations (ICLR), Apr 2018. 3

[24] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Des-
jardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho,
A. Grabska-Barwinska, et al. Overcoming Catastrophic For-
getting in Neural Networks. Proceedings of the National
Academy of Sciences, page 201611835, 2017. 2

[25] P. Kr¨ahenb¨uhl. Free supervision from video games.

In
Proceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR 18), pages
2955–2964, 2018. 3

[26] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Technical report, Citeseer, 2009.
2, 6

[27] Y. LeCun, F. J. Huang, and L. Bottou. Learning Methods
for Generic Object Recognition with Invariance to Pose and
Lighting.
In IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition (CVPR), volume 2,
pages II–104, 2004. 3

[28] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang.
Overcoming Catastrophic Forgetting by Incremental Mo-
ment Matching. In Advances in Neural Information Process-
ing Systems 30 (NIPS), pages 4652–4662. 2017. 2

[29] Z. Li and D. Hoiem. Learning without Forgetting. In Euro-
pean Conference on Computer Vision (ECCV), pages 614–
629, 2016. 2, 5, 6

[30] S. Liu, T. D. Ullman, J. B. Tenenbaum, and E. S. Spelke.
Ten-month-old infants infer the value of goals from the costs
of actions. Science, 358(6366):1038–1041, 2017. 3

[31] V. Lomonaco and D. Maltoni. CORe50: a New Dataset and
Benchmark for Continuous Object Recognition. In Proceed-
ings of the 1st Annual Conference on Robot Learning, 2017.
3

[32] D. Lopez-Paz and M. A. Ranzato. Gradient Episodic Mem-
ory for Continual Learning. In Advances in Neural Informa-
tion Processing Systems 30 (NIPS), pages 6467–6476. 2017.
2

[33] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers,
A. Dosovitskiy, and T. Brox. A large dataset to train con-
volutional networks for disparity, optical ﬂow, and scene
ﬂow estimation.
In Proceedings of the IEEE Conference

8785

on Computer Vision and Pattern Recognition, pages 4040–
4048, 2016. 3

[34] B. McMurray. Defusing the Childhood Vocabulary Explo-

sion. Science, 317(5838):631–631, 2007. 1

[35] Y. Movshovitz-Attias, T. Kanade, and Y. Sheikh. How Use-
ful is Photo-realistic Rendering for Visual Learning? In Eu-
ropean Conference on Computer Vision (ECCV), pages 202–
217, 2016. 3

[36] S. Nayar, S. Nene, and H. Murase. Columbia Object Im-
age Library (COIL 100). Department of Comp. Science,
Columbia University, Tech. Rep. CUCS-006-96, 1996. 3

[37] A. F. Pereira, K. H. James, S. S. Jones, and L. B. Smith. Early
biases and developmental changes in self-generated object
views. Journal of vision, 10(11):22–22, 2010. 1

[38] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert.
iCaRL: Incremental Classiﬁer and Representation Learning.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 5533–5542, July 2017. 2, 5, 6

[39] S. R. Richter, Z. Hayder, and V. Koltun. Playing for bench-
marks. In Proceedings of the IEEE International Conference
on Computer Vision, pages 2213–2222, 2017. 3

[40] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing
for data: Ground truth from computer games. In European
Conference on Computer Vision, pages 102–118, 2016. 3

[41] M. Savva, F. Yu, H. Su, A. Kanezaki, T. Furuya, R. Ohbuchi,
Z. Zhou, R. Yu, S. Bai, X. Bai, et al. Shrec17 track large-
scale 3d shape retrieval from shapenet core55. 3

[42] W. J. Scheirer, A. de Rezende Rocha, A. Sapkota, and T. E.
Boult. Toward open set recognition. IEEE transactions on
pattern analysis and machine intelligence, 35(7):1757–1772,
2013. 7

[43] W. J. Scheirer, L. P. Jain, and T. E. Boult. Probability mod-
els for open set recognition. IEEE transactions on pattern
analysis and machine intelligence, 36(11):2317–2324, 2014.
7

[44] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. Cnn features off-the-shelf: an astounding baseline for
recognition. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition workshops, pages 806–
813, 2014. 5, 6

[45] O. Wiles and A. Zisserman. SilNet : Single- and Multi-View
In BMVC.

Reconstruction by Learning from Silhouettes.
BMVA Press, 2017. 3

[46] J. Wulff, D. J. Butler, G. B. Stanley, and M. J. Black. Lessons
and insights from creating a synthetic optical ﬂow bench-
mark. In European Conference on Computer Vision, pages
168–177. Springer, 2012. 3

[47] B. Wymann. TORCS - The Open Racing Car Simulator. 3
[48] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In Advances
in neural information processing systems, pages 3320–3328,
2014. 5, 6

[49] D. Yurovsky, L. B. Smith, and C. Yu. Statistical Word Learn-
ing at Scale: The Baby’s View is Better. Developmental Sci-
ence, 16(6):959–966, 2013. 1, 3

8786

