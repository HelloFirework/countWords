PoseFix: Model-agnostic General Human Pose Reﬁnement Network

Gyeongsik Moon

Department of ECE, ASRI
Seoul National University

mks0601@snu.ac.kr

Ju Yong Chang
Department of EI

Kwangwoon University
juyong.chang@gmail.com

Kyoung Mu Lee

Department of ECE, ASRI
Seoul National University

kyoungmu@snu.ac.kr

Abstract

Multi-person pose estimation from a 2D image is an es-
sential technique for human behavior understanding.
In
this paper, we propose a human pose reﬁnement network
that estimates a reﬁned pose from a tuple of an input image
and input pose. The pose reﬁnement was performed mainly
through an end-to-end trainable multi-stage architecture in
previous methods. However, they are highly dependent on
pose estimation models and require careful model design.
By contrast, we propose a model-agnostic pose reﬁnement
method. According to a recent study, state-of-the-art 2D
human pose estimation methods have similar error distri-
butions. We use this error statistics as prior information
to generate synthetic poses and use the synthesized poses
to train our model.
In the testing stage, pose estimation
results of any other methods can be input to the proposed
method. Moreover, the proposed model does not require
code or knowledge about other methods, which allows it to
be easily used in the post-processing step. We show that
the proposed approach achieves better performance than
the conventional multi-stage reﬁnement models and consis-
tently improves the performance of various state-of-the-art
pose estimation methods on the commonly used benchmark.
The code is available in 1.

1. Introduction

The goal of human pose estimation is to localize seman-
tic keypoints of a human body. It is an essential technique
for human behavior understanding and human-computer
interaction. Recently, many methods [4, 6, 9, 12, 13, 16,
18, 20, 21, 24, 28] utilize deep convolutional neural net-
works (CNNs) and achieved noticeable performance im-
provement. They are also updating performance limits in
annual competitions for 2D human keypoint detection such
as MS COCO keypoint detection challenge [17].

In this paper, we propose a human pose reﬁnement net-

1https://github.com/mks0601/PoseFix_RELEASE

Figure 1: Testing pipeline of the PoseFix.
It takes pose
estimation results of any other method with an input image
and outputs a reﬁned pose. Note that the PoseFix does not
require any code or knowledge about other methods.

work that estimates a reﬁned pose from a tuple of an in-
put image and a pose. Conventionally, the pose reﬁne-
ment has been mainly performed by multi-stage architec-
tures [3,6,19,27]. In other words, the initial pose and image
features generated in the ﬁrst stage go through subsequent
stages, and each stage outputs a reﬁned pose. These multi-
stage architectures are usually trained in an end-to-end man-
ner. However, the conventional multi-stage architecture-
based reﬁnement approach is highly dependent on the pose
estimation model and requires careful design for successful
reﬁnement. By contrast, in this work, we propose a model-
agnostic pose reﬁnement method that does not depend on
the pose estimation model.

Recent research by Ronchi et al. [22] gave us a clue on
how to design a general model-agnostic pose reﬁner. They
analyzed the results of the MS COCO 2016 keypoint detec-
tion challenge winners [4, 20] by using new pose estimation
evaluation metrics, i.e., keypoint similarity (KS) and object
keypoint similarity (OKS). They taxonomized pose estima-
tion errors into several types such as jitter, inversion, swap,

7773

keypoints: (x1, y1),(x2, y2),(x3, y3), .. (xN, yN)Pose result file of method AInput pose Refined poseInput imagekeypoints: (x1, y1),(x2, y2),(x3, y3), .. (xN, yN)Pose result file of method Bkeypoints: (x1, y1),(x2, y2),(x3, y3), .. (xN, yN)Pose result file of method C(a) Pose estimation results of any methods(b) Pose error fix procedure of the PoseFixPoseFixand miss and described how frequently these errors occur
and how much they can negatively affect performance. Al-
though the winners [4, 20] used very different approaches,
their pose error distributions are very similar, which indi-
cates that common issues exist for more accurate pose esti-
mation.

Our basic idea is to use this error statistics as prior infor-
mation to generate synthetic poses and use the synthesized
poses to train the proposed pose reﬁnement model (Pose-
Fix). To train our model, we generate each type of the er-
rors (i.e., jitter, inversion, swap, and miss) based on the pose
error distributions from Ronchi et al. [22], and construct di-
verse and realistic poses. The generated input pose is fed to
the PoseFix with the input image, and the PoseFix learns to
reﬁne the pose. We design our PoseFix as a single-stage ar-
chitecture with a coarse-to-ﬁne estimation pipeline. It takes
the input pose in a coarse form and estimates the reﬁned
pose in a ﬁner form. The coarse input pose enables the pro-
posed model to focus not only on an exact location of the
input pose but also around it, allowing our model to ﬁx the
error of the input pose. Furthermore, the ﬁner form of the
output pose enables the proposed model to localize the loca-
tion of the pose more exactly compared to existing methods.
After training, our PoseFix can be applied to and reﬁne the
pose estimation results of any single- or multi-person pose
estimation method. Figure 1 shows such a pose reﬁnement
pipeline of the proposed PoseFix.

Our contributions can be summarized as follows.

• We show that model-agnostic general pose reﬁnement
is possible. The PoseFix is trained independently of
the pose estimation model. Instead, it is based on error
statistics obtained through empirical analysis.

• Our PoseFix can take the pose estimation result of any
pose detection method as the input. As the PoseFix
does not require any code or knowledge about other
methods, our model has very high ﬂexibility and ac-
cessibility.

• We design the PoseFix as a coarse-to-ﬁne estimation
system. We empirically observed that this coarse-to-
ﬁne pipeline is crucial for successful pose reﬁnement.

• Our PoseFix achieves a better result than the conven-
tional multi-stage architecture-based reﬁnement meth-
ods. Also, the PoseFix consistently improves the per-
formance of various state-of-the-art pose estimation
methods on the commonly used benchmark.

2. Related works

Single-person pose estimation. Toshev et al. [26] di-
rectly estimated the Cartesian coordinates of body joints
by using a multi-stage deep network and achieved state-of-
the-art performance. Tompson et al. [25] jointly trained

a CNN and a graphical model. The CNN estimated 2D
heatmaps for each joint, and they were used as the unary
term for the graphical model. Liu et al. [27] used multi-
stage CNN which progressively enlarges receptive ﬁelds
and reﬁnes the pose estimation result. Newell et al. [19]
proposed a stacked hourglass network which repeats down-
sampling and upsampling to exploit multi-scale informa-
tion effectively. Carreria et al. [5] proposed an iterative er-
ror feedback-based human pose estimation system. Chu et
al. [7] enhanced the stacked hourglass network [19] by in-
tegrating it with a multi-context attention mechanism. Ke et
al. [14] proposed a multi-scale structure-aware network
which achieved leading position in the publicly available
human pose estimation benchmark [2].

Multi-person pose estimation. There are two main ap-
proaches in the multi-person pose estimation. The ﬁrst one,
top-down approach, relies on a human detector that predicts
bounding boxes of humans. The detected human image is
cropped and fed to the pose estimation network. The second
one, bottom-up approach, localizes all human body key-
points in an input image and assembles them using proposed
clustering algorithms in each work.

[6, 9, 12, 20, 24, 28] are based on the top-down approach.
He et al. [9] proposed Mask R-CNN that can perform hu-
man detection and keypoint localization in a single model.
Instead of cropping the detected humans in the input image,
it crops human features from a feature map via the differen-
tiable RoIAlign layer. Chen et al. [6] proposed a cascaded
pyramid network (CPN) which consists of two networks.
The ﬁrst one, GlobalNet, is based on deep backbone net-
work and upsampling layers with skip connections. The
second one, ReﬁneNet, is built to reﬁne the estimation re-
sults from the GlobalNet by focusing on hard keypoints.
Xiao et al. [28] used a simple pose estimation network that
consists of a deep backbone network and several upsam-
pling layers. Although it is based on a simple network ar-
chitecture, it achieved state-of-the-art performance on the
commonly used benchmark [17].

[4, 13, 16, 18, 21] are based on the bottom-up approach.
DeepCut [21] assigned the detected keypoints to each per-
son in an image by formulating the assignment problem as
an integer linear program. DeeperCut [21] improves the
DeepCut [21] by introducing image-conditioned pair-wise
terms. Cao et al. [4] proposed part afﬁnity ﬁelds (PAFs)
that directly expose the association between human body
keypoints. They assembled the localized keypoints of all
persons in the input image by using the estimated PAFs.
Newell et al. [18] introduced a pixel-wise tag value to as-
sign localized keypoints to a certain human. Kocabas et
al. [16] proposed a pose residual network to assign detected
keypoints to each person. Their model can jointly handle
person detection, keypoint detection, and person segmenta-
tion.

7774

Figure 2: Overall pipeline of the PoseFix. In the training stage, the input pose is generated by synthesizing the pose errors
based on the real pose error distributions on the groundtruth pose. In the testing stage, pose estimation results of any other
methods become the input pose. The heatmaps are visualized by performing max pooling along the channel axis.

Human pose reﬁnement. Many methods attempted
to reﬁne the estimated keypoint for more accurate perfor-
mance. Newell et al. [19], Bulat and Tzimiropoulos [3],
Liu et al. [27], and Chen et al. [6] utilized an end-to-
end trainable multi-stage architecture-based network. Each
stage tries to reﬁne the pose estimation results of the previ-
ous stage via end-to-end learning. Carreria et al. [5] itera-
tively estimated error feedback from a shared weight model.
The output error feedback of the previous iteration is trans-
formed into the input pose of the next iteration, which is
repeated several times for progressive pose reﬁnement. All
of these methods combine pose estimation and reﬁnement
into a single model, and each reﬁnement module is depen-
dent on estimation. Therefore, the reﬁnement modules have
different structures, and they are not guaranteed to work
successfully when they are combined with other estimation
methods. On the other hand, our pose reﬁnement method
is independent of the estimation, and therefore the results
can be consistently improved regardless of the prior pose
estimation method.

Recently, Fieraru et al. [8] proposed a post-processing
network to reﬁne the pose estimation results of other meth-
ods, which is conceptually similar to ours. They synthe-
sized pose for training and employed simple network ar-
chitecture that estimates reﬁned heatmaps and offset vec-
tors for each joint. While their method follows ad-hoc rules
to generate input pose, our method is based on actual er-
ror statistics obtained through empirical analysis. Also,
our network with coarse-to-ﬁne structure achieves a much

stronger reﬁnement performance than their simpler one.

3. Overview of the proposed model

The goal of the PoseFix is to reﬁne the input 2D coordi-
nates of the human body keypoints of all persons in an input
image. To address this problem, our system is constructed
based on the top-down pipeline which processes a tuple of
a cropped human image and a given pose estimation result
of that human instead of processing an entire image includ-
ing multiple persons. In the training stage, the input pose
is synthesized on the groundtruth pose realistically and di-
versely. In the testing stage, pose estimation results of any
other methods can be the input pose to our system. The
overall pipeline of the PoseFix is illustrated in Figure 2.

4. Synthesizing poses for training

To train the PoseFix, we generate synthesized poses us-
ing the groundtruth poses. As the PoseFix should cover
different pose estimation results from various methods in
the testing stage, synthesized poses need to be diverse and
realistic. To satisfy these properties, we generate synthe-
sized poses randomly based on the error distributions of
real poses as described in [22]. The distributions include
the frequency of each pose error (i.e., jitter, inversion, swap,
and miss) according to the joint type, number of visible key-
points, and overlap in the input image. There may also be
joints that do not have any error, which should be synthe-
sized very close to the groundtruth to simulate correct esti-

7775

Image Input pose (P)Backbone + Upsamplerx-axis meshgridy-axis meshgrid1  2  3  ..  w1  2  3  ..  w1  2  3  ..  w..1  2  3  ..  w1  1  1  ..  1   2  2  2  ..  2  3  3  3  ..  3  ..h  h  h  ..  hRefined pose (C)Refined pose (H)Nose:JitterHip: GoodElbow: Jitter        ..Knee: InvPose result file of any methodsGT poseTestingSynthesized errorsTraining~Error distributionProbError typekeypoints(x1, y1),(x2, y2),(x3, y3), .. (xN, yN)Figure 3: Visualization of synthesized pose errors for each
type. The keypoint with pose error is highlighted by a yel-
low rectangle, and the groundtruth keypoints are drawn in a
yellow circle.

mations. Ronchi et al. [22] called this status good. Consid-
ering most of the empirical distributions in [22], we com-
pute the probability that each joint will have one of the pose
errors or be in the good status.

synthesis procedure on each
The detailed error
groundtruth keypoint θp
j of joint j which belongs to a per-
son p is described in below. For more clear description, we
deﬁne j ′ as a left/right inverted joint from the j, and p′ as a
different person from the p in the input image. Also, dk
j is
deﬁned as a L2 distance that makes KS with the groundtruth
keypoint becomes k for joint j. Note that dk
j depends on the
type of joint j because the error distribution of each joint
has different scale [22]. For example, eyes require more
precise localization than hips to obtain the same KS k. Fig-
ure 3 visualizes examples of synthesized pose errors of each
type.

Good. Good status is deﬁned as a very small dis-
placement from the groundtruth keypoint. An offset vector
whose angle and length are uniformly sampled from [0, 2π)
), respectively, is added to the groundtruth θp
and [0, d0.85
j .
The synthesized keypoint position should be closer to the
original groundtruth θp

j , and θp′
j ′ .

j than θp

j ′ , θp′

j

j

j

,d0.5

Jitter. Jitter error is deﬁned as a small displacement from
the groundtruth keypoint. An offset vector whose angle and
length are uniformly sampled from [0, 2π) and [d0.85
),
respectively, is added to the groundtruth θp
j . Similar to the
good status, the synthesized keypoint position should be
j , and θp′
closer to the original groundtruth θp
j ′ .
Inversion. Inversion error occurs when a pose estima-
tion model is confused between semantically similar parts
that belong to the same instance. We restrict the inversion
error to the left/right body part confusion following [22].
The jitter error is added to θp
j ′ . The synthesized keypoint
position should be closer to the θp

j than θp

j ′ , θp′

j , and θp′
j ′ .

j ′ than θp

j , θp′

Swap. Swap error represents a confusion between the
same or similar parts which belong to different persons. The
jitter is added to θp′
j ′ . The closest keypoint from the
synthesized keypoint should be θp′
j and
θp
j ′ .

j ′ , not any of θp

j or θp′

j or θp′

Figure 4: Visualization of the groundtruths and synthesized
input poses. The synthesized poses are generated by adding
errors to the groundtruth poses, which are used for training
PoseFix.

Miss. Miss error represents a large displacement from
the groundtruth keypoint position. An offset vector whose
angle and length are uniformly sampled from [0, 2π) and
[d0.5
j ′ , and
j
θp′
j ′ . The synthesized keypoint position should be at least
d0.5
j

), respectively, is added to one of θp

away from all of θp

j , and θp′
j ′ .

j ′ , θp′

j , θp′

j , θp

j , θp

,d0.1

j

Some examples of synthesized input poses are shown in

Figure 4.

5. Architecture and learning of PoseFix

5.1. Model design

We design the PoseFix to directly estimate a reﬁned pose
from a tuple of an input image and an input pose as shown in
Figure 2. The input image and the input pose provide con-
textual and structured information to the PoseFix, respec-
tively, and the PoseFix learns to use these information to ﬁx
pose errors in the input pose. Although some errors exist in
the input pose, it still provides useful structured information
because, as indicated by Ronchi et al. [22], most keypoints
in the input pose are in good status or have jitter error which
represent a small displacement from the groundtruth pose.
This rough structured information acts like attention which
tells the PoseFix where to focus on at the human body.

We observed that by learning to ﬁx the pose errors in
the input pose, the PoseFix learns where to focus on at the
human body as in Figure 5. As it shows, although some
errors exist in the input pose, the PoseFix initially focuses
well on the reliable keypoint locations of the input pose.
And then, it successfully localizes correct keypoints without
being inﬂuenced by the errors of the input pose.

7776

(a) Jitter(b) Inversion(c) Swap(d) Miss(a) GT pose(b) Input pose in training stage+ synthesized errorThis Gaussian heatmap representation is suitable for sub-
sequent convolutional operations because it is pixel-wise
aligned with the input image. Moreover, as the input pose
can contain some errors, non-zero values around the center
of the blob can be used to encourage the PoseFix to focus
not only on the exact location of the input pose, but also
around it.

From the input Gaussian heatmap in a coarse form, the
proposed network generates the heatmap Hn and the key-
point coordinates Cn for the nth keypoint, sequentially. To
make Hn a ﬁner form, we supervise it using a one-hot vec-
tor. Then, soft-argmax operation [24] is applied to Hn
to generate Cn in a differentiable manner. Soft-argmax
is deﬁned as the element-wise product between the input
heatmap and the meshgrid followed by the summation, as
shown in Figure 2. More precisely, the 2D coordinates are
calculated from Hn as follows:

w

Cn = 
Xi=1


h

Xj=1

iHn(i, j),

w

h

Xi=1

Xj=1

T

jHn(i, j)


,

(2)

where w and h are the width and height of Hn, respectively.
Our network is trained by minimizing the cross-entropy-
based integral loss [24], which is deﬁned as follows:

L = LH + LC ,

(3)

where L is the cross-entropy-based integral loss, and two
losses LH and LC are described below.

The LH is a cross-entropy loss which is calculated after
applying the softmax function to the output heatmap along
the spatial axis. The deﬁnition of the LH is as follows:

LH = −

1
N

N

Xn=1Xi,j

H ∗

n(i, j) log Hn(i, j),

(4)

respectively.
n is a one-hot vector

where H ∗
n and Hn are the groundtruth and estimated
The
heatmaps with softmax applied,
groundtruth heatmap H ∗
the
groundtruth keypoint coordinates are integers. Otherwise,
two grids for each x and y axis are selected by ﬂoor and
ceil operations and are ﬁlled with probabilities by linear ex-
trapolation. The LC is the sum of all L1 losses applied to
the coordinates as follows:

if

LC =

1
N

N

Xn=1

kC ∗

n − Cnk1,

(5)

where C ∗
n is the groundtruth coordinates vector for nth key-
point. The LH forces the PoseFix to select a single grid
point in the estimated heatmap, and the LC enables the
PoseFix to localize keypoints more precisely because it is
calculated in the continuous space which is free from quan-
tization errors.

7777

Figure 5: Visualization of feature maps and ﬁnal heatmaps
of the PoseFix. The feature maps and heatmaps are reduced
into one channel by max pooling along the channel axis for
visualization. The order of the feature maps and heatmaps
in the ﬁgure is the same with that of the feedforward.

5.2. Coarse to ﬁne estimation

To make it more robust to errors, we design the proposed
PoseFix to operate in a coarse-to-ﬁne manner. We use the
terms “coarse” and “ﬁne” by the degree of uncertainty in
representing the pose. For example, in representing the po-
sition of each joint constituting a pose, a Gaussian blob has
a high uncertainty as much as the size of its standard de-
viation. On the other hand, a one-hot vector has relatively
low uncertainty up to the size of a quantized grid. The co-
ordinates of a keypoint has the least amount of uncertainty
because it provides the exact information about the location
itself. Therefore, in our work, the coarse-to-ﬁne estimation
implies that the coarse input pose (P = {Pn}N
n=1) repre-
sented by the set of Gaussian blobs is fed to the network,
producing the ﬁner pose in the form of the one-hot vector
(H = {Hn}N
n=1), and then the ﬁnest pose in terms of the
keypoint coordinates (C = {Cn}N
n=1) is generated as the ﬁ-
nal output as illustrated in Figure 2. N denotes the number
of keypoints. In this subsection, we describe this coarse-to-
ﬁne estimation in more detail.

The input pose is constructed in a coarse form by a

single-mode Gaussian heatmap representation as follows:

Pn(i, j) = exp(cid:18)−

(i − in)2 + (j − jn)2

2σ2

(cid:19) ,

(1)

where Pn and (in,jn) are the input heatmap and 2D coor-
dinates of nth keypoint, respectively, and σ is the standard
deviation of the Gaussian peak. The generated input pose is
concatenated with the input image and fed into the PoseFix.

Input poseRefined poseImageFeature mapsHeatmap5.3. Network architecture

We used network architecture of Xiao et al. [28] which
consists of a deep backbone network (i.e., ResNet [11]) and
several upsampling layers. The ﬁnal upsampling layer be-
comes heatmaps (H) after applying the softmax function.
The soft-argmax operation extracts coordinates (C) from
the heatmaps (H), and it becomes the ﬁnal estimation of
the PoseFix.

6. Implementation details

Training. The proposed PoseFix is trained in an end-to-
end manner. The weights of the backbone part are initial-
ized with the publicly released ResNet model pre-trained on
the ImageNet dataset [23], and the weights of the remaining
part are initialized from the zero-mean Gaussian distribu-
tion with σ = 0.01 and as in He et al. [10]. The weights are
updated by Adam optimizer [15] with a mini-batch size of
128. The initial learning rate is set to 5×10−4 and reduced
by a factor of 10 at 90 and 120th epoch. We perform data
augmentation including scaling (±30%), rotation (±40◦),
and ﬂip. To crop humans from an input image, groundtruth
human bounding boxes are extended to a ﬁxed aspect ratio
(i.e., height:width = 4:3) and then cropped without distort-
ing the aspect ratio. The cropped bounding box is resized
to a ﬁxed size, which becomes the input image. We train
the PoseFix 140 epochs with four NVIDIA 1080 Ti GPUs,
which took two days.

Testing. In the testing stage, the pose estimation result
of other pose estimation methods becomes the input pose.
To crop human bounding box from an image with multiple
persons, we calculate bounding box coordinates from the
keypoints coordinates of the input pose. Following [6, 19],
we used testing time ﬂip augmentation.

Our model is implemented using TensorFlow [1] deep

learning framework.

7. Experiment

7.1. Dataset and evaluation metric

The proposed PoseFix is trained and tested on the MS
COCO [17] 2017 keypoint detection dataset, which consists
of training, validation, and test-dev sets. The training set
includes 57K images and 150K person instances. The vali-
dation set and the test-dev sets include 5K and 20K images,
respectively. The OKS-based AP metric is used to evaluate
the accuracy of the keypoint localization.

7.2. Ablation study

To validate each component of the PoseFix, we tested
the PoseFix on the validation set. The backbone of all the
models are ResNet-50, and the size of the input image is

Methods

E2E-reﬁne

MA-reﬁne (Ours)

AP AP.50 AP.75 APM APL
76.3
70.1
(+0.2)
(+0.4)
78.2
72.1
(+2.4)
(+2.1)

87.3
(-1.0)
88.5
(+0.2)

76.8
(-0.2)
78.3
(+1.3)

66.8
(+0.6)
68.6
(+2.4)

Table 1: AP comparison between the conventional end-
to-end trainable multi-stage reﬁnement model (E2E-reﬁne)
and the proposed model-agnostic reﬁnement model (MA-
reﬁne) on the validation set. The number in the parenthesis
denotes the AP change from the input pose (i.e., CPN).

CPN

P
A
m

72.5

72

71.5

71

70.5

70

69.5

69

68.5

68

F2F

C2F (Ours) C2F-L

C

C2F-L

H

C2C

Figure 6: mAP comparison of various pipelines. The mAP
is calculated on the validation set.

set to 256×192. We used the CPN [6] which is a state-of-
the-art human pose estimation method to generate the input
poses.

Model-agnostic pose reﬁnement. We compared the ac-
curacy of the conventional end-to-end trainable multi-stage
architecture-based pose reﬁnement model (E2E-reﬁne) and
the proposed model-agnostic reﬁnement model (MA-reﬁne)
in Table 1. To train the E2E-reﬁne, we added a reﬁnement
module which has the same network architecture as the
PoseFix at the end part of the pre-trained CPN. And then,
we ﬁne-tuned it by additionally giving the cross-entropy-
based integral loss to the added module in an end-to-end
manner. Both the input image and the output pose of the
CPN are fed into the reﬁnement module similarly to the
PoseFix. We used a pre-trained CPN instead of training
it from scratch because ﬁne-tuning the pre-trained model
yielded better performance.

As Table 1 shows, the MA-reﬁne trained in a model-
agnostic manner improves the accuracy greatly more than
the conventional reﬁnement model does. We believe that
this is because the added reﬁnement module can be easily
overﬁtted to the output pose of the CPN when training the
E2E-reﬁne. In contrast, various input poses that are realisti-
cally synthesized in the training stage of the PoseFix lead to

7778

1

0.9

0.8

0.7

0.6

e
s
o
p
 
d
e
n
i
f
e
r
 
e
h
t
 
f
o
 
S
K
O

0.5

0.5

AE
Mask R-CNN
CPN

0.6

0.7

0.8

0.9

1

OKS of the input pose

Figure 7: OKS change when the PoseFix is applied to state-
of-the-art methods. The dotted line denotes identity func-
tion. OKS is calculated on the validation set.

Figure 8: Frequency of each error type change when the
PoseFix is applied to the CPN. The frequency is calculated
on the validation set.

the effect of data augmentation, which makes the PoseFix
more robust to unseen input poses in the testing stage.

Instead of using the same network architecture of the
PoseFix like in the E2E-reﬁne, one can design their own
reﬁnement module. However, this approach requires care-
ful network design because the amount of GPU memory
available at one time is limited. By contrast, since the Pose-
Fix is a decoupled model in both of the training and testing
stages, it can serve as an add-on module, and thus provides
more ﬂexibility when building pose estimation models.

This analysis clearly demonstrates the beneﬁts of us-
ing the model-agnostic pose reﬁnement model compared
with the conventional end-to-end trainable multi-stage
architecture-based ones.

Coarse-to-ﬁne estimation. To demonstrate the validity
of the coarse-to-ﬁne estimation, we compared the perfor-

mance of ﬁne-to-ﬁne (i.e., F2F), coarse-to-ﬁne (i.e., C2F,
ours), and coarse-to-coarse (i.e., C2C) estimation pipelines
in Figure 6. As described in Section 5.2, the Gaussian
heatmap and one-hot vector are used as coarse and ﬁne
forms of the input pose, respectively. To estimate the re-
ﬁned pose in a coarse form, the model learns to estimate the
Gaussian heatmap by minimizing mean square error follow-
ing [6, 19, 27]. For the ﬁne-form estimation, cross-entropy-
based integral loss is used as a loss function like ours.

As Figure 6 shows, the C2F (i.e., ours) exhibits a more
accurate performance than F2F, which indicates that coarse
input pose representation is more beneﬁcial than ﬁne input
pose representation. Also, C2C fails to improve the input
pose whereas F2F and C2F successfully reﬁne the input
pose. These results indicate that the ﬁne-form estimation is
crucial for a successful reﬁnement.

To further analyze the beneﬁt of the ﬁne-form estima-
tion, we additionally trained two models (C2F-LH and
C2F-LC ). Instead of using both of the LC and LH like the
C2F does, they are trained by minimizing only either LH
or LC . The C2F-LH learns to estimate one-hot vector (H)
by minimizing LH , and C2F-LC is supervised to estimate
coordinate (C) by minimizing LC . Among C2C, C2F-LH ,
and C2F-LC , the target form of the C2C is the most coarse
representation. On the other hand, that of C2F-LC is the
ﬁnest representation as described in Section 5.2. Figure 6
shows that C2C yields the worst performance while C2F-
LC achieves the best among them. This ﬁnding shows that
as the output representation of the PoseFix becomes a ﬁner
form, the performance improves. Thus, by integrating the
two loss functions (i.e., LH and LC ) together, we can im-
prove the performance much, as C2F shows.

This analysis clearly shows the beneﬁt of the coarse-to-

ﬁne estimation pipeline.

7.3. Performance improvement of the state of the 

art methods by PoseFix

We report the performance improvement when the Pose-
Fix is applied to the recent state-of-the-art human pose es-
timation methods. PAFs [4], AE [18], Mask R-CNN [9],
CPN [6], and Simple [28] are used to generate the input
pose. To obtain the pose estimation results of the previ-
ous methods, we used their released codes and pre-trained
models. We tested them by ourselves without ensembling
and testing time augmentation. We also trained a pose esti-
mation model (IntegralPose) with the same network archi-
tecture and loss function with the PoseFix to show that the
PoseFix can improve a model trained from the same archi-
tecture. To analyze how the PoseFix changes the OKS and
frequency of each error type, we tested the PoseFix on the
validation set. We also report how much the PoseFix im-
proves AP on the test-dev set. The ResNet-152 is used as
the backbone of the PoseFix, and the size of the input image

7779

Methods

AE [18]
+ PoseFix (Ours)

PAFs [4]
+ PoseFix (Ours)

Mask R-CNN (ResNet-50) [9]
+ PoseFix (Ours)

Mask R-CNN (ResNet-101)
+ PoseFix (Ours)

Mask R-CNN (ResNeXt-101-64)
+ PoseFix (Ours)

Mask R-CNN (ResNeXt-101-32)
+ PoseFix (Ours)

IntegralPose
+ PoseFix (Ours)

CPN (ResNet-50) [6]
+ PoseFix (Ours)

CPN (ResNet-101)
+ PoseFix (Ours)

Simple (ResNet-50) [28]
+ PoseFix (Ours)

Simple (ResNet-101)
+ PoseFix (Ours)

Simple (ResNet-152)
+ PoseFix (Ours)

AP

56.6
63.9

61.7
66.7

62.9
67.2

63.4
67.5

64.9
68.7

64.9
68.5

66.3
69.5

68.6
71.8

69.6
72.6

69.4
72.5

70.5
73.3

71.1
73.6

AP.50 AP.75

APM

APL

81.7
83.6

84.9
85.7

87.1
88.0

87.5
88.4

88.6
89.3

88.4
88.9

87.6
88.3

89.6
89.8

89.9
90.2

90.1
90.5

90.7
90.8

90.7
90.8

62.1
70.0

67.4
72.9

68.9
73.5

69.4
73.8

71.0
75.2

70.9
75.0

72.9
75.9

76.7
78.9

77.6
79.7

77.4
79.6

78.8
80.7

79.4
81.0

48.1
56.9

57.1
62.9

57.6
62.5

57.8
62.6

59.6
64.1

59.5
64.0

62.7
65.7

65.3
68.3

66.3
69.0

66.2
68.9

67.5
69.8

68.0
70.3

69.4
73.7

68.1
72.3

71.3
75.1

72.0
75.5

73.3
76.4

73.2
76.2

72.7
76.1

74.6
78.1

75.6
78.9

75.5
79.0

76.3
79.8

76.9
79.8

AR

62.5
69.1

66.5
71.3

69.7
74.0

70.2
74.3

71.4
75.2

71.3
75.0

73.2
75.9

75.6
78.2

76.6
78.9

75.1
78.0

76.2
78.7

76.8
79.0

AR.50 AR.75 ARM

ARL

84.9
86.6

87.2
88.0

91.3
92.2

91.8
92.6

92.4
93.1

92.2
92.9

91.8
92.4

93.7
93.9

93.9
94.1

93.9
94.1

94.3
94.4

94.4

94.4

67.2
74.2

71.7
76.7

75.1
79.6

75.6
79.9

76.8
80.9

76.7
80.7

79.1
81.8

82.6
84.3

83.5
85.0

82.4
84.4

83.7
85.3

84.3
85.7

52.2
61.1

60.5
66.3

63.9
68.8

64.3
69.1

65.9
70.3

65.8
70.1

68.3
71.1

70.8
73.5

72.0
74.2

70.8
73.4

72.1
74.3

72.6
74.8

76.5
79.9

74.6
78.1

77.6
81.1

78.2
81.4

78.9
81.9

78.9
81.8

79.8
82.5

82.0
84.6

82.9
85.1

81.0
84.1

81.9
84.8

82.4
84.9

Table 2: Improvement of APs when the PoseFix is applied to the state-of-the-art methods. The APs are calculated on the
test-dev set.

is set to 384×288.

8. Conclusion

OKS change. The graph in Figure 7 shows the change of
the OKS of the same instance when the PoseFix is applied
to the baseline state-of-the-art methods.

Error frequency change. Figure 8 shows how the fre-
quency of each status or error type changes when the Pose-
Fix is applied to the CPN.

AP improvement. Table 2 shows the improvements in
AP when the PoseFix is applied to the recent state-of-the-
art human pose estimation methods. We also included the
results of using different backbone networks [11,29] for the
Mask R-CNN, CPN, and Simple.

As Figures 7, 8 and Table 2 show, the PoseFix con-
sistently improves the performance of the state-of-the-art
methods. The PoseFix corrects not only the small displace-
ment error (i.e., jitter), but also the large displacement errors
(i.e., inversion, miss, and swap) as in Figure 8. Taking into
account the fact that the state-of-the-art methods used in the
experiments vary in structure and learning strategies, we be-
lieve that our model has generalizability that can be applied
to other pose estimation methods. It is also noticeable that
the PoseFix does not require any code or knowledge of the
pose estimation methods, which makes our model very easy
and convenient to use in practice.

We proposed a novel and powerful network, PoseFix,
for human pose reﬁnement. Unlike conventional end-to-
end multi-stage architecture models, the proposed PoseFix
is a model-agnostic pose reﬁnement network. To train the
PoseFix, we generate the input pose by synthesizing pose
errors according to empirical pose error distributions on
the groundtruth pose. The PoseFix takes an input pose
in a coarse form and estimates the reﬁned pose in a ﬁner
form. Since PoseFix is model-agnostic, it does not require
any code or knowledge about the target models. So, it can
be used as a post-processing add-on module conveniently.
We showed that the PoseFix achieves better performance
than the conventional multi-stage architecture-based pose
reﬁnement module. Furthermore, the PoseFix consistently
improves the accuracy of other methods on the commonly
used pose estimation benchmark.

Acknowledgments

This work was partially supported by the Visual Turing
Test project (IITP-2017-0-01780) from the Ministry of Sci-
ence and ICT of Korea.

7780

[20] George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander
Toshev, Jonathan Tompson, Chris Bregler, and Kevin Mur-
phy. Towards accurate multi-person pose estimation in the
wild. In CVPR, 2017.

[21] Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern
Andres, Mykhaylo Andriluka, Peter V Gehler, and Bernt
Schiele. Deepcut: Joint subset partition and labeling for
multi person pose estimation. In CVPR, 2016.

[22] Matteo Ruggero Ronchi and Pietro Perona. Benchmarking
In

and error diagnosis in multi-instance pose estimation.
ICCV, 2017.

[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. IJCV, 2015.

[24] Xiao Sun, Bin Xiao, Shuang Liang, and Yichen Wei. Integral

human pose regression. ECCV, 2018.

[25] Jonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph
Joint training of a convolutional network and a
Bregler.
graphical model for human pose estimation. In NIPS, 2014.
[26] Alexander Toshev and Christian Szegedy. Deeppose: Human

pose estimation via deep neural networks. In CVPR, 2014.

[27] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser

Sheikh. Convolutional pose machines. In CVPR, 2016.

[28] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines

for human pose estimation and tracking. ECCV, 2018.

[29] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In CVPR, 2017.

References

[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow:
a system for large-scale machine learning. In OSDI, 2016.

[2] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and
Bernt Schiele. 2d human pose estimation: New benchmark
and state of the art analysis. In CVPR, 2014.

[3] Adrian Bulat and Georgios Tzimiropoulos. Human pose esti-
mation via convolutional part heatmap regression. In ECCV,
2016.

[4] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. CVPR, 2017.

[5] Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Ji-
tendra Malik. Human pose estimation with iterative error
feedback. In CVPR, 2016.

[6] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang
Zhang, Gang Yu, and Jian Sun. Cascaded pyramid network
for multi-person pose estimation. CVPR, 2018.

[7] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L
Yuille, and Xiaogang Wang. Multi-context attention for hu-
man pose estimation. In CVPR, 2017.

[8] Mihai Fieraru, Anna Khoreva, Leonid Pishchulin, and Bernt
Schiele. Learning to reﬁne human pose estimation. CVPRW,
2018.

[9] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-

shick. Mask r-cnn. In ICCV, 2017.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation. In ICCV, 2015.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[12] Shaoli Huang, Mingming Gong, and Dacheng Tao. A coarse-

ﬁne network for keypoint localization. In ICCV, 2017.

[13] Eldar

Insafutdinov, Leonid Pishchulin, Bjoern Andres,
Mykhaylo Andriluka, and Bernt Schiele. Deepercut: A
deeper, stronger, and faster multi-person pose estimation
model. In ECCV, 2016.

[14] Lipeng Ke, Ming-Ching Chang, Honggang Qi, and Siwei
Lyu. Multi-scale structure-aware network for human pose
estimation. ECCV, 2018.

[15] Diederik P Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. ICLR, 2014.

[16] Muhammed Kocabas, Salih Karagoz, and Emre Akbas. Mul-
tiposenet: Fast multi-person pose estimation using pose
residual network. In ECCV, 2018.

[17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, 2014.

[18] Alejandro Newell, Zhiao Huang, and Jia Deng. Associa-
tive embedding: End-to-end learning for joint detection and
grouping. In NIPS, 2017.

[19] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In ECCV, 2016.

7781

