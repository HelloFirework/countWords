MFAS: Multimodal Fusion Architecture Search

Juan-Manuel P´erez-R´ua1,3∗

Valentin Vielzeuf1,2∗

St´ephane Pateux1

Moez Baccouche1

Frederic Jurie2

1Orange Labs, Cesson-S´evign´e, France
2Universit´e Caen Normandie, France
3Samsung AI Centre, Cambridge, UK

Abstract

We tackle the problem of ﬁnding good architectures for
multimodal classiﬁcation problems. We propose a novel and
generic search space that spans a large number of possi-
ble fusion architectures. In order to ﬁnd an optimal archi-
tecture for a given dataset in the proposed search space,
we leverage an efﬁcient sequential model-based exploration
approach that is tailored for the problem. We demonstrate
the value of posing multimodal fusion as a neural architec-
ture search problem by extensive experimentation on a toy
dataset and two other real multimodal datasets. We dis-
cover fusion architectures that exhibit state-of-the-art per-
formance for problems with different domain and dataset
size, including the NTU RGB+D dataset, the largest multi-
modal action recognition dataset available.

1. Introduction

Deep neural networks have demonstrated to be effective
models for solving a large variety of problems in several
domains, including image [22] and video [5] classiﬁcation,
speech recognition [15], and machine translation [44], to
name a few.
In a multimodal setting, it is very common
to transfer models trained on the individual modalities and
merge them at a single point. It can be at the deepest lay-
ers, known in the literature as late fusion, which is relatively
successful on a number of multimodal tasks [40]. However,
fusing modalities at their respective deepest features is not
necessarily the most optimal way to solve a given multi-
modal problem. We argue in this paper that considering
features extracted from all the hidden layers of independent
modalities could potentially increase performance with re-
spect to only using a single combination of late (or early)
features. Thus, this work tackles the problem of ﬁnding

∗Assert joint ﬁrst authorship. This work was done while JMPR was

with Orange Labs.

good ways to combine multimodal features to better exploit
the information embedded at different layers in deep learn-
ing models for classiﬁcation.

Our hypothesis is in line with a common interpretation
of deep neural models considering that features learned in
a convolutional neural network carry varying levels of se-
mantic meanings. In vision, for example, lower layers are
known to serve as edge detectors with different orientations
and extent, while further layers capture more complex infor-
mation such as semantic concepts, like faces, trees, animals,
etc. Evidently, it is difﬁcult to determine by hand what is the
most optimal way of mixing features with varying levels of
semantic meaning when solving for multimodal classiﬁca-
tion problems. For example, learning to classify furry an-
imals might require analysis of lower level visual features
that can be used to build up the concept of fur, whereas
classes like chirping birds or growling might require analy-
sis of more complex audiovisual attributes. Indeed, features
from different layers at different modalities can give differ-
ent insights from the input data. A similar idea is exploited
by unimodal ResNets [14], where features from different
depths are utilized by later layers through skip connections.
In this line of thought, a few recent works analyzed other
possible combinations from input modalities [38, 42]. How-
ever, those methods fall short, as the model designer needs
to choose empirically which intermediate features to con-
sider. Evaluating all of the possibilities by hand would be
extremely intensive or simply intractable. Indeed, the more
modalities and the deeper they are, the more complicated it
is to choose a mixture. This is all the more true when en-
abling nested combinations of multimodal features. It is in
fact a large combinatorial problem.

In order to handle this issue, the aforementioned com-
binatorial problem has to be tackled by an efﬁcient search
method. Luckily, the underlying structure of this prob-
lem makes it specially amenable to sequential search al-
gorithms. We propose in this paper to rely on a sequen-
tial model-based optimization (SMBO) [19] scheme, which

16966

has previously been applied to the related problem of neu-
ral architecture search or AutoML [26, 34]. In a few words,
we tackle the problem of multimodal classiﬁcation by di-
rectly posing the problem as a combinatorial search. To the
best of our knowledge, this is a completely new approach to
the multimodal fusion problem, which, as shown by thor-
ough experimentation, improves the state-of-the-art on sev-
eral multimodal classiﬁcation datasets.

This paper brings four main contributions: i) an empir-
ical evidence of the importance of searching for optimal
multimodal feature fusion on a synthetic toy database.
ii)
The deﬁnition of a search space adapted to multimodal fu-
sion problems, which is a superset of modern fusion ap-
proaches.
iii) An adaptation of an automatic search ap-
proach for accurate fusion of deep modalities on the deﬁned
search space. iv) Three automatically-found state-of-the-art
fusion architectures for different known and well studied
multimodal problems encompassing ﬁve types of modali-
ties.

The rest of this paper is organized as follows. In Sec-
tion 2, we describe the work that is related to ours, including
multimodal fusion for classiﬁcation and neural architecture
search. Next, in Section 3 we explain our search space and
methodology. In Section 4, we present an experimental val-
idation of our approach. Finally, in Section 5, we give ﬁnal
comments and conclusions.

2. Related work

Current design strategies of neural architectures for gen-
eral classiﬁcation (multimodal or not) and other learn-
ing problems consider the importance of the informa-
tion encoded at various layers along a deep neural net-
work. Indeed, advances in image classiﬁcation like resid-
ual nets [14] and densely connected nets [18] are related
to this idea. Similarly, for the problem of pose estimation,
stacked hourglass networks [32] connect encoder and de-
coder parts of an autoencoder by short-circuit convolutions,
allowing the ﬁnal classiﬁers to ponder features from bottom
layers. However, it is commonly accepted that manually-
designed architectures are not necessarily optimally solving
the task [48].
In fact, looking at the type of neural net-
works that are automatically designed by search algorithms,
it seems that convoluted architectures with many cross-layer
connections and different convolutional operations are pre-
ferred [9, 48].

Interestingly, Escorcia et al. argued that the visual at-
tributes learned by a neural network are distributed across
the entire neural network [11]. Similarly, it is commonly
understood that neural networks encode features in a hier-
archical manner, starting from low-level to higher-level fea-
tures as one goes deeper along them. These ideas motivate
well our take on the problem of multimodal classiﬁcation.
This is, trying to establish an optimal way to connect and

fuse multimodal features. To the best of our knowledge, this
work is the ﬁrst one to directly tackle multimodal fusion for
classiﬁcation as an architecture search problem.

In the following, we give an overview of the multimodal
fusion problem for classiﬁcation as a whole. We then con-
tinue with a short discussion on relevant methods for archi-
tecture search, since it appears at the core of our method.

Multimodal fusion. To categorize the different recent ap-
proaches of deep multimodal fusion, we can deﬁne two
main paths of research: architectures and constraints.

The ﬁrst path focuses on building best possible fusion
architectures e.g. by ﬁnding at which depths the unimodal
layers should be fused. Early works distinguished early and
late fusion methods [4], respectively fusing low-level fea-
tures and prediction-level features. As reported by [40], late
fusion performs slightly better in many cases, but for others,
it is largely outperformed by the early fusion. Late fusion is
often deﬁned by the combination of the ﬁnal scores of each
unimodal branch. This combination can be a simple [39]
or weighted [29] score average, a bilinear product [8], or
a more robust one such as rank minimization [46]. Thus,
methods such as multiple kernel learning [6] and super-
kernel learning [43] may be seen as examples of late fu-
sion. Closer to early fusion, Zhou et al. [47] propose to use
a Multiple Discriminant Analysis on concatenated features,
while Neverova et al [31] apply a heuristic consisting of
fusing similar modalities earlier than the others. Recently,
to take advantage of both low-level and high-level features,
Yang et al. [45] leverage boosting for fusion across all lay-
ers. To avoid overﬁtting due to large number of parameters
in multilayer approches, multimodal regularization meth-
ods [1, 13, 21] are also investigated. Another architecture
approach for multimodal fusion could be grouped under the
idea of attention mechanisms, which decides how to ponder
different modalities by contextual information. The mixture
of experts by [20] can be viewed as a ﬁrst work in this di-
rection. The authors proposed a gated model that picks an
expert network for a given input. As an extension, Arevalo
et al. [3], proposed Gated Multimodal Units, allowing to ap-
ply this fusion strategy anywhere in the model and not only
at prediction-level. In the same spirit, multimodal attention
can also be integrated to temporal approaches [16, 27].

The second category of multimodal fusion methods pro-
poses to deﬁne constraints in order to control the relation-
ship between unimodal features and/or the structure of the
weights. Ngiam et al. [33], proposed a bimodal autoen-
coder, forcing the hidden shared representation to be able
to reconstruct both modalities, even in the absence of one
of them. Andrew et al. [2], adapted Canonical Correlation
Analysis to deep neural networks, maximizing correlation
between representations. Shahroudy et al. [38], use cascad-
ing factorization layers to ﬁnd shared representations be-

6967

tween modalities and isolate modality-speciﬁc information.
To ensure similarity between unimodal features, Engilberge
et al. [10] minimize their cosine distance. Structural con-
straints can also be applied on the very weights of the neu-
ral networks. In addition to modality dropping, Neverova
et al. [30] propose to zero-mask the cross-modal blocks
of the weight matrix in early stages of training. Extend-
ing the idea of modality dropping, Li et al. [25], propose
to learn a stochastic mask. Another structure constraint as
done through tensor factorization was proposed by [8].

Neural architecture search. The last couple of years
have seen an increased interest on AutoML methods [9,
26, 34, 35, 48]. Most of these methods rely somehow on
a neural module at the core of their respective search ap-
proaches. This is now known in the literature as neural
architecture search (NAS). Neural-based or not, AutoML
methods were traditionally reserved for expensive hardware
conﬁgurations with hundreds of available GPUs [26, 48].

Very recently, progressive exploration approaches and
weight-sharing schemes have allowed to tremendously re-
duce the necessary computing power to effectively perform
architecture search on sizeable datasets. Another advantage
of progressive search methods [26, 34] is that they leverage
the intrinsic structure of the search space, by sequentially
increasing the complexity of sampled architectures. In this
paper, we start from a sequential method with weight shar-
ing [34] and adapt it to the problem of multimodal classiﬁ-
cation. In particular, we design a search space that is prone
to sequential search and which is a superset of previously
introduced fusion schemes, e.g., [42]. This is an impor-
tant aspect of our contribution. As demonstrated by [49],
constraining the search space is a key element for afford-
able architecture search. It turns out that directly tackling
multimodal datasets by automatic architecture search with-
out designing a constrained, but meaningful, search space
would not be tractable. We demonstrate the value of our
approach and the importance of optimizing neural architec-
tures for multimodal classiﬁcation tasks by tackling three
challenging datasets.

3. Methodology

In this work, as in many others addressing multimodal
fusion, we start from the assumption of having an off-the-
shelf multi-layer feature extractor for each one of the in-
volved modalities.
In practice, this means that we start
from a multi-layer neural network for each modality, which
we assume to be already pre-trained. However, the reader
should consider that our fusion approach is in fact not lim-
ited to neural networks as primary feature extractors.

Without loss of conceptual generality, we assume from
now on that we will deal with two modalities. The multi-
modal dataset is composed by pairs of input and output data

x1

x2

x3

xM

f1(x)

f2(x1)

f3(x2)

fM (·)

ˆzx

x

f (x)

g(y)

ˆzx,y

y1

y2

y3

yN

g1(y)

g2(y1)

g3(y2)

gN (·)

ˆzy

y

Figure 1. General structure of a bi-modal fusion network. Top:
A neural network with several hidden layers (grey boxes) with in-
put x, and output ˆzx. Bottom: A second network with input y,
and output ˆzy. In this work we focus on ﬁnding efﬁcient fusion
schemes (yellow box and dotted lines).

(x, y; z), where x accounts for the ﬁrst modality, y for the
second one, and z for the supervision labels. Now, we as-
sume that there exists two functions f (x) and g(y) which
take x and y as inputs, and output ˆzx and ˆzy, which are
estimates of the ground-truth labels z.

Furthermore, functions f and g are composed of M
and N layers, respectively, subfunctions denoted by fl and
gl. With a slight abuse of notation, we write for layer l,
xl = (fl ◦ fl−1 · · · ◦ f1)(x), and yl = (gl ◦ gl−1 · · · ◦ g1)(y).
See Fig. 1 for a visual representation. Examples of subfunc-
tions when dealing with standard neural networks are oper-
ations like convolution, pooling, multiplication by a matrix,
non-linearity, etc. The outputs of these subfunctions are the
features we want to fuse across modalities. The problem is
then to choose which features to fuse and how to mix them.

3.1. Multimodal fusion search space

In our approach, data fusion is introduced through a third
neural network (see Fig. 2 for some illustrations). Each fu-
sion layer l combines three inputs: the output of the previ-
ous fusion layer and one output from each modality. This is
done according to the following equation:

hl = σγp

(1)

l 


Wl


l

xγm
yγn
hl−1

l







l , γp

, γn

where γ l = (γm
l ) is a triplet of variable indices es-
l
tablishing, respectively, which feature from the ﬁrst modal-
ity, which feature from the second modality, and which
non-linearity is applied. Also, γm
l ∈
{1, · · · , N }, and γp
l ∈ {1, · · · , P }. For the ﬁrst fusion
layer (l = 1), the fusion operation is deﬁned as:

l ∈ {1, · · · , M }, γn

6968

......x1

x3

f

g

y4

ReLU

y2

x

y

Sigm

ˆzx,y

x

y

x3

y3

Figure 2. Two realizations of our
[(γm
2 = 3, γn

1 = 1), (γm

1 = 1, γn

1 = 2, γp

search space on a small bimodal network.
1 = 3, γn

2 = 2)]. Right: network deﬁned by [(γm

2 = 4, γp

1 = 3, γp

Left:
1 = 2)].

f

Sigm
g

ˆzx,y

network deﬁned by

h1 = σγp

1  W1"xγm
1#!

yγn

1

(2)

The number of possible fusion layers, a search parame-
ter, is denoted by L, so that l ∈ {1, · · · , L}. The fusion
layer weight matrix Wl is trainable. Note that we estab-
lish feature concatenation as ﬁxed strategy to process and
fuse features. In fact, this could be replaced by a weighted
sum of input features. However, during our experiments,
we noticed that fusion networks with weighted sum of fea-
tures were almost never chosen, and almost always reduced
ﬁnal classiﬁcation performance with respect to concatena-
tion. Thus, we decided to simply ﬁx the fusion operation to
concatenation.

An illustrative example for M = N = 4, and P = 2
(p = 1 : ReLU; p = 2 : Sigmoid) is shown in Fig. 2.
We can observe a couple of realizations of the search space
for modalities of four hidden layers and two possible non-
linearities. On the right, a fusion scheme with a single fu-
sion at the third layer of ﬁrst and second modalities. On
the left, two composed fusions. A composed fusion scheme
is deﬁned then by a vector of triplets: [γ l]l∈{1,··· ,L}. We
denote the set of all possible triplets with L layers as ΓL.

Observe that this design enables our space to contain a
large number of possible fusion architectures, including the
networks deﬁned in, for example, CentralNet [42]. The size
of the search space is exponential on the number of fusion
layers L, and is expressed by: (M × N × P )L. If we were
to tackle a multimodal problem where the number of layers
of the feature extractor is only a portion of the depth that
modern neural networks exhibit, say M = N = 16, and
only considered two possible non-linearities P = 2, a fu-
sion scheme with L = 5 would result in a search space of
dimension ∼ 3, 51 × 1013.

Exhaustively exploring all

these possibilities is in-
tractable. In particular, consider that evaluation of a single
sample in this space corresponds to training and evaluating a
multimodal architecture, which can take from several hours

to a few days, depending on the problem at hand. This is
the reason why we focus on an exploration method that has
shown to be sampling-efﬁcient for the related problem of
neural architecture search. This is, sequential model-based
optimization (SMBO), as used by [26, 34]. In their works,
the authors showed that progressively exploring a search
space by dividing it into “complexity levels”, ends up pro-
viding architectures that perform as well as the ones discov-
ered by a more direct exploration approach, as in [48, 49],
while sampling fewer architectures. SMBO is well ﬁt to ﬁnd
optimal architectures in the search space designed by [49].
This is because the space is naturally divided by complexity
levels that can be interpreted as progression steps (blocks in
the “micro space” [26, 34, 49]). SMBO sequentially un-
folds the complexity of the sampled architectures starting
from the simplest one. Luckily, our search space shares a
similar structure. We can interpret the number of fusion
layers L as a hook for progression.

It is worth noting that the constrained search space that
we propose exhibits certain desirable properties. Assum-
ing that the unimodal feature extractor networks are avail-
able greatly reduces search burden as they do not need to be
trained during search, and the complexity of the problem is
limited to a manageable magnitude.

3.2. Search algorithm

In SMBO, a model predicting accuracy of sampled ar-
chitectures lies at the core of the method. This model, or
surrogate function is trained during progressive exploration
of the search space, and it is used to reduce the amount of
neural networks that have to be trained and evaluated by
predicting performance of unseen architectures. In our case,
having a variable-length description of the multimodal ar-
chitectures [γ l]l∈{1,··· ,L}, as described in previous subsec-
tion, naturally results in using a recurrent model as surro-
gate. Let us denote this recurrent function by π. The pa-
rameters of π are updated at iteration l by stochastic gra-
dient descent (SGD) training on a subset of Γl with real
valued accuracies Al.

6969

Algorithm 1 Multimodal
(MFAS)

fusion architecture search

1: procedure (f , g, L, Esearch, Etrain, K, Strain, Sval, Tmax, Tmin)
2: L: max number of fusion layers
3: Esearch: number of search iterations
4: Etrain: number of training epochs
5: K: number of sampled fusion architectures
6: Strain, Sval: training and validation sets
7: Tmax, Tmin: sampling temperature range
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

S1 ← Γ1 // Set of fusion architectures with L = 1
M1 ← descToFusionNet(S1, f , g) // Build fusion nets
C1 ← train(M1, Strain, Etrain) // Train fusion nets
A1 ← evaluate(C1, Sval) // Get real accuracies for them
B, A ← B ∪ S1, A ∪ A1 // Keep track of sampled archs.
π ← update(S1, A1) // Train surrogate
for l = 2 · · · L do

T ← Tmax // Set temperature
B, A ← {} // Initialize corresponding sets of arhcs. and accuracy
for e = 1 · · · Esearch do

l, T ) // Compute sampling probs.

l , π) // Predict with surrogate

l , Pl, K) // Sample K fusion archs

l ← addLayer(Sl−1, Γl) // Unfold 1 more fusion layer
l ← pred(S ′

S ′
ˆA′
Pl ← computeProbs( ˆA′
Sl ← sampleK(S ′
Ml ← descToFusionNet(Sl, f , g) // Build fusion net.
Cl ← train(Ml, Strain, Etrain) // Train
Al ← evaluate(Cl, Sval) // Calculate accuracies
B, A ← B ∪ Sl, A ∪ Al // Keep track of sampled archs.
π ← update(Sl, Al) // Update surrogate
T ← updateTemperature(T, Tmax, Tmin)

19:

20:
21:
22:
23:
24:
25:
26:
27:
28:
29:

end for

end for

return topK(B, A, K) // Return best K from all sampled archs.

30: end procedure

Our procedure, named multimodal fusion architecture
search (MFAS), and based on [26], is laid out in Alg. 1.
From lines 11 to 16, the progressive algorithm starts at the
smallest fusion network complexity level, i.e., L = 1. Then,
the next complexity levels unroll one after the other by
sampling K architectures with a probability that is a func-
tion of the surrogate model predictions in lines 20 and 21.
The fusion architecture search is effectively guided by how
new architectures are sampled. Observe that we implement
search iterations (Esearch) and temperature-based sampling
(Tmax, Tmin) as in EPNAS [34]. This is done so the surro-
gate function does not guide the search with biased assump-
tions made from partial observations of the search space
at early iterations. By using temperature-based sampling,
the surrogate function is only trusted as the exploration ad-
vances (by reducing the temperature in line 27). This is
complemented by training sampled architectures with very
few epochs as in ENAS [35], and implementing weight-
sharing among sampled architectures to counterweight the
main bottleneck of neural architecture search: training sam-
pled architectures to completion. This aspect is of particular
importance for multimodal networks, which tend to have a
large memory footprint and computing times.

Another aspect where our search algorithm differs from

the original algorithm [26] and from [34] is that we assume
the existence of pre-trained modal functions f and g. These
functions are used to build a multimodal network from a
description of the fusion scheme Sl with l layers (line 12
and line 22). At the end of the iterative progressive search,
MFAS returns the best K from the set of all sampled archi-
tectures B.

Final architecture. From Alg. 1, we obtain a set of K fu-
sion architectures. One could think of using the surrogate
function after its last update to predict the very best fusion
scheme from those. However, in this paper we train the best
ﬁve of the ﬁnal K architectures to completion, and simply
pick the absolute best one from the obtained validation ac-
curacies. During this last training step we also evaluate the
performance of the chosen architectures with a larger size
of matrices Wl. The reduced size is used during search to
improve sampling speed and to reduce memory costs.

Loss function. During the search, the weights of the fea-
ture extractors f and g are frozen. Because of it, only the
fusion softmax ˆzx,y is used for the loss function. Found ar-
chitectures are initially trained for a few epochs with frozen
f and g functions. A second training step with more epochs
involves a multitask loss on ˆzx, ˆzy, ˆzx,y, and unfrozen f and
g functions. A categorical cross-entropy loss is used in all
the reported experiments unless otherwise noted.

Handling arbitrary tensor dimensions. A practical is-
sue during the creation of a multimodal neural network from
f and g is that subfunctions might deliver tensors with ar-
bitrary dimensions, hindering fusion of arbitrary modalities
and layer positions. To deal with this in a generic way, we
perform global pooling along the channel dimension of 2D
and 3D convolutions, while leaving linear layer outputs as
they are.

As a side note, observe in Eq. 1 that our default layer
type for fusion is fully connected. We experimented with
several forms of 1D convolutions without noticing any im-
provements.

Weight sharing of fusion layers.
In our implementation
of Alg. 1, multimodal neural networks are not trained in
parallel. Instead, sampled fusion networks are trained se-
quentially for a small number of epochs (Etrain = 2 in
all of our experiments). For two sample indices s and s′,
where s′ > s, we keep track of the weight matrix Ws
l for
layer l, so Ws′
l if sizeof(Ws
l ) =
l
sizeof(Ws′
l ). Please note that weights are only shared
among matrices in the same layer l.

is initialized from Ws

6970

Table 1. Evaluation of our
search method on the AV-
MNIST dataset. The fusion architectures described by arrays of
numbers are instances of our search space with M = 3, N =
5, P = 2. Validation accuracy is reported.

Method

Modalities

Acc

[(1, 1, 2), (4, 3, 1), (5, 2, 1)]

[(3, 3, 2), (5, 3, 2)]

Top-5 found architectures by random search
image + spect.
image + spect.
image + spect.
image + spect.
image + spect.

[(5, 3, 1), (4, 1, 2)]
[(5, 2, 1), (5, 3, 1)]

0.9174
0.9190
0.9196
0.9224
0.9222

[(5, 3, 1)]
Mean (Std)

0.9203 (0.0021)

Top-5 found architectures by MFAS

[(3, 3, 2), (5, 2, 1), (1, 3, 1), (1, 1, 2)]

[(5, 2, 1), (5, 2, 2), (5, 1, 1)]
[(5, 3, 1), (4, 2, 1), (5, 3, 1)]
[(5, 3, 1), (4, 2, 1), (3, 3, 2)]

[(4, 3, 1), (5, 3, 1), (4, 3, 1), (5, 3, 1)]

image + spect.
image + spect.
image + spect.
image + spect.
image + spect.

0.9258
0.9260
0.9270
0.9266
0.9268

Mean (Std)

0.9264 (0.0004)

4. Experiments

In this section we present an extensive experimental
validation of our claims. We ﬁrst start by presenting
experiments on a synthetic toy dataset, namely the AV-
MNIST dataset [42]. We then continue our experimental
work by directly tackling two other multimodal datasets.
These are i) the visual-textual multilabel movie genre clas-
siﬁcation dataset by [3] (MM-IMDB) and ii) the multimodal
action recognition dataset by [37] (NTU RGB+D).

For each dataset, we provide a short description of the
task as well as the experimental set-up, and then discuss on
the results.

AV-MNIST dataset. This is a simple audio-visual dataset
artiﬁcially assembled from independent visual and audio
datasets. The ﬁrst modality corresponds to 28 × 28 MNIST
images, with 75% of their energy removed by PCA. The au-
dio modality is made of audio samples on which we have
computed 112 × 112 spectrograms. The audio samples
are 25,102 pronounced digits of the Tidigits database aug-
mented by adding randomly chosen noise samples from the
ESC-50 dataset [36]. Contaminated audio samples are ran-
domly paired, accordingly with labels, with MNIST digits
in order to reach 55,000 pairs for training and 10,000 pairs
for testing. For validation we take 5000 samples from the
training set. The digit energy removal and audio contami-
nation are intentionally done to increase the difﬁculty of the
task (otherwise unimodal networks would achieve almost
perfect results and data fusion would not be necessary).

In here, f function is a modiﬁed LeNet network [23] with
ﬁve convolutional layers and a global pooling softmax pro-
cessing spoken digits. Similarly g is a modiﬁed LeNet with
three convolutional layers. We limit the subfunctions of f
and g to convolutional layers with ReLU activation, so we

f : audio
x5

x4

· · ·

· · ·

· · ·

x

y

x

y

ReLU

ReLU

ReLU

ˆzx,y

· · ·

· · ·

y2 y3
g : image

· · ·

f : image
· · · x4· · · x8

· · ·

Sigm

Sigm

ˆzx,y

· · · y1 · · ·

g : text

Figure 3. Structure of the found fusion architectures. First: AV-
MNIST. Second: MM-IMDB.

Table 2. Evaluation of multiple fusion architectures on the AV-
MNIST dataset. Test accuracy is reported.
Modalities

Acc (%)

Method

Unimodal baselines for fusion

LeNet-3 [23]
LeNet-5 [23]

image

spectrogram

Explicit fusion

Two-stream [39]
CentralNet [42]

Ours Top 1

image + spect.
image + spect.
image + spect.

74.52
66.06

87.78
87.86
88.38

hook global pooling to each one of them: three for the writ-
ten digit modality (N = 3), and ﬁve for the spectrogram
one (M = 5). For this experiment we let P = 2 by al-
lowing the activation functions of fusion layers to be either
ReLU or Sigmoid.

In Table 1, we show results for two exploration ap-
proaches: a purely random one (upper part), and MFAS
(bottom). Both exploration approaches are allowed to sam-
ple 180 architectures. We show validation accuracy for the
top ﬁve randomly sampled architectures on the proposed
search space (top of Table 1). The large standard deviation
is a testament to the usefulness of multimodal fusion archi-
tecture search. From these results we can infer that some
feature combinations provide better insights from data than
some other mixtures. At the lower part of Table 1 we can
see that in contrast to random search, the top ﬁve found ar-
chitectures with our search method present scores with less
variability. Furthermore, the best performing architecture
on the validation set (in bold) is found by our method.

Test accuracy for baselines and competing fusion archi-
tectures are reported in Table 2. We report test score of our
best found architecture according to Table 1. It can be ob-

6971

Table 3. Evaluation of multiple methods on the MM-
IMDB
dataset [37]. Weighted F1 (F1-W) and Macro F1
(F1-M) are reported for each method.
Modalities

F1-W F1-M

Method

Unimodal baselines for fusion

Maxout MLP [12]

VGG Transfer

text

image

0.5754
0.4921

0.4598
0.3350

Two-stream [39]

GMU [3]

CentralNet [42]

Ours Top 1

Explicit fusion
image + text
image + text
image + text
image + text

0.6081
0.6170
0.6223
0.6250

0.5049
0.5410
0.5344
0.5568

served that all multimodal fusion networks largely improve
over the unimodal networks, but our automatically found
fusion architecture is the one with the best overall score.
This was found after three iterations of progressive search
and L = 4. The success on this toy (but not trivial) dataset
is a ﬁrst milestone in the validation of our contributions.

MM-IMDB dataset. This multimodal dataset comprises
25,959 movie titles and metadata from the Internet Movie
Database1 [3]. Movie data is formed by their plots, posters
(RGB images), genres, and many more metadata ﬁelds in-
cluding director, writer, picture format, etc. The task in
this dataset is to predict movie genres from posters and
movie descriptions. Since very often a movie is assigned to
more than one genre, the classiﬁcation is multi-label. The
loss function used for training is binary cross-entropy with
weights to balance the dataset.

The original split of the dataset is used in our experi-
ments: 15,552 movies are used for training, 7,799 for test-
ing, and 2,608 for validation. The genres to predict in-
clude drama, comedy, documentary, sport, western, ﬁlm-
noir, etc., for a total of 23 non-mutually exclusive classes.

Performance of unimodal networks is given at the top
of Table 3. Using these unimodal networks as a basis,
we implemented Two-stream fusion [39], CentralNet [42],
GMU [3], and our best found architecture. One can note
that our method gives the best results among the four fu-
sion strategies, once again validating our choices on search
space design and fusion scheme 2.

The search space for the MM-IMDB dataset is formed
by eight convolutional layers of a VGG-19 image network,
and two text Maxout-MLP features. The number of possible
fusion conﬁgurations available from these features (we set
N = 2, and M = 8) and the three possible non-linearities
(ReLU, Sigmoid,and LeakyReLU) is of 110,592. Our best
conﬁguration can be seen in Fig. 3.

1https://www.imdb.com/
2The original Central-Net paper considers only the last feature layer for
each modality (as pre-computed by the original authors [3]). Intermediate
layers were not available to us. Consequently, we did not start with the
exact same unimodal baselines and re-implemented all methods in order to
allow fair comparison.

Table 4. Evaluation of multiple methods on the NTU
RGB+D
The reported numbers are the aver-
age accuracy over the different action subjects (cross-subject
measure).

dataset [37].

Method

Modalities

Acc (%)

Single modality

LSTM [37]

part-LSTM [37]

Spatio-temp. attention [41]

pose
pose
pose
Multiple modalities

Shahroudy et al. [38]
Shahroudy et al. [38]
Bilinear Learning [17]
Bilinear Learning [17]
2D/3D Multitask [28]

video + pose
video + pose
video + pose

video + pose + depth

video + pose
Unimodal baselines for fusion

Inﬂated ResNet-50 [7]

Co-occurrence [24]

video
pose

Explicit fusion

Two-stream [39]

GMU [3]

CentralNet [42]

Ours Top 1

video + pose
video + pose
video + pose
video + pose

f : skeleton

· · ·

x2

· · ·

x4

· · ·

ReLU

Sigm

Sigm

· · ·

· · ·

· · ·

y4
y2
g : video

x

y

60.69
62.93
73.40

74.86
74.86
83.30
85.40
85.50

83.91
85.24

88.60
85.80
89.36

90.04±0.6

ReLU

ˆzx,y

Figure 4. Structure of found fusion architectures. NTU RGB+D.

NTU RGB+D dataset. This dataset was ﬁrst introduced by
Shahroudy et al., [37] in 2016. With 56,880 samples, to
the best of our knowledge, it is the largest color and depth
multimodal dataset. Capturing 40 subjects from 80 view-
points performing 60 classes of activities NTU RGB+D is a
very challenging dataset with the particularity that it pro-
vides dynamic skeleton-based pose data on the top of RGB
video sequences. The target activities include drinking, eat-
ing, falling down, and even subject interactions like hug-
ging, shaking hands, punching, etc.

Network

0
1
2
3

# of fusion Parameters Acc (%)
0.9327
0.9289
0.9301
0.9346

2,229,248
2,196,480
1,737,728
2,163,712

Table 5. Top 4 found architectures on NTU RGB+D according to
validation accuracy during search.

In our work, we focus on the cross-subject evaluation,
splitting the 40 subjects into training, validation, and testing

6972

Table 6. Search timings and hardware conﬁgurations.

Dataset

GPUs Esearch ∗ L Search time Avg. step
(P100)

time (hours)

(hours)

(steps)

Figure 5. Left: Error progression during search. Each plot point
represents the validation error of a sampled fusion architecture at
a given step of our search algorithm on the AV-MNIST set, where
the total number of steps is Esearch ∗ L. Mean error and standard
deviation per step are represented with stars and plot shadow, re-
spectively. Right: search temperature schedule.

groups. The subject IDs for training during search are: 1,
4, 8, 13, 15, 17, 19. For validation we use: 2, 5, 9, and,
14. During ﬁnal training of the found architectures we use
the same splitting originally proposed by [37]. We report
results on the testing set to objectively compare our found
architectures with manually designed fusion strategies from
the state-of-the-art.

Results on the NTU RGB+D dataset are summarized in
Table 4. We report accuracy in percentages for several
methods. The ﬁrst group of methods are models process-
ing single modalities as reported by the authors themselves.
The second group of results are by methods from the state-
of-the-art processing and fusing several modalities (video,
pose, and/or depth). Then, we provide the score as com-
puted by us of methods processing single modalities. For
video, we tested the Inﬂated ResNet-50 used by [7]; and for
pose, we leverage the deep co-occurrence model by [24].
The reported numbers in this group are our departing point
and baselines. Finally, the last group of methods perform
explicit fusion of modalities and are our main competitors.
Observe that our scores are the highest in Table 4. We
report 90.04% average accuracy over four runs with a vari-
ance of 0.6, which is a signiﬁcant improvement over all
baselines and competing methods. This is achieved by per-
forming fusion search on the convolutional and fully con-
nected features of the Inﬂated ResNet-50 and deep Co-
occurrence baselines. We start from four possible features
for each modality (M = N = 4) and three non-linearities,
i.e., ReLU, Sigmoid,and LeakyReLU. This means,
the
search space for the NTU RGB+D dataset is of dimension
5, 308, 416. The best found conﬁguration is shown in Fig. 4.
In Table 5 we report validation accuracy during search for
the ﬁnal top four architectures. Observe that the best archi-
tecture is not necessarily the largest one.

Multimodal fusion search behaviour.
In Fig. 5 (top) we
display the behaviour of our search procedure by plotting
validation errors of sampled architectures.
It can be ob-
served that, overall, sampled architectures are more and
more stable error-wise as the search progresses. The stabi-
lization of sampled errors originates from two sources: ﬁrst,

AV-MNIST
MM-IMDB
NTU RGB+D

1
1
4

3 ∗ 4 = 12
5 ∗ 3 = 15
3 ∗ 4 = 12

3.42
9.24

150.91

0.285
0.616
12.57

the shared fusion weights have been more reﬁned at the ﬁ-
nal steps of the search, and second, the search is driven with
more conﬁdence by the predictions of the surrogate func-
tion. Indeed, at the last few steps, mean error is signiﬁcantly
lower than the initial ones.

Another interesting effect of our search method and fu-
sion scheme is the fact that even at the initial search steps
it is possible to sample architectures that display relatively
small validation errors. Since the fusion weights of sam-
pled architectures are trained only for a few epochs, this
effect is not necessarily a positive reﬂection of how good
or bad the sampled architecture is.
Indeed, it is possible
to sample a simple fusion scheme on very deep uni-modal
features (which have been pretrained ofﬂine) and outper-
form other sampled architectures that might actually per-
form better when its weights are revisited at later search
steps. In this sense, our temperature-driven sampling of ar-
chitectures offers a way to escape the fake local minima that
originate from this phenomenon. This all boils down to the
fact that it is important, in order to avoid getting trapped by
initial biased evidence, to trust the surrogate function only
after exploration has advanced. We use an inverse expo-
nential schedule for the sampling temperature, as shown at
the bottom of Fig. 5, since we observed a better outcome in
comparison to a linear temperature schedule.

Search timings.
In Table 6 we provide the hardware set-
tings and timings for the search on all the reported datasets.
Multi GPU training through data parallelism was necessary
on the NTU RGB+D. Search times on NTU RGB+D are
much larger than on the MM-IMDB dataset due to model
complexity and larger search space.

5. Conclusion

This work tackles the problem of ﬁnding accurate fusion
architectures for multimodal classiﬁcation. We propose a
novel multimodal search space and exploration algorithm
to solve the task in an efﬁcient yet effective manner. The
proposed search space is constrained in such a way that
it allows convoluted architectures to take place while also
containing the complexity of the problem to reasonable lev-
els. We experimentally demonstrated on three datasets the
validity of our method, discovering several fusion schemes
that provide state-of-the-art results on those datasets. Fu-
ture research directions include improving the search space
so the composition of fusion layers is even more ﬂexible.

6973

123456789101112steps0.00.10.20.30.40.50.60.70.80.91.0error123456789101112steps0.0000.6531.3071.9602.6133.2673.9204.5735.2275.8806.5337.1877.8408.4939.1479.800temperature ()References

[1] M. R. Amer, T. Shields, B. Siddiquie, A. Tamrakar, A. Di-
vakaran, and S. Chai. Deep multimodal fusion: A hybrid
approach. In IJCV. Springer, 2018.

[2] G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep

canonical correlation analysis. In ICML, 2013.

[3] J. Arevalo, T. Solorio, M. Montes-y G´omez, and F. A.
Gonz´alez. Gated multimodal units for information fusion.
In ICLR Workshop, 2017.

[4] P. K. Atrey, M. A. Hossain, A. El Saddik, and M. S. Kankan-
halli. Multimodal fusion for multimedia analysis: a survey.
Multimedia Systems, 16(6):345–379, 2010.

[5] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and
Sequential deep learning for human action
In ECCV Workshop, pages 29–39. Springer,

A. Baskurt.
recognition.
2011.

[6] F. R. Bach, G. R. Lanckriet, and M. I. Jordan. Multiple kernel
In ICML.

learning, conic duality, and the smo algorithm.
ACM, 2004.

[7] F. Baradel, C. Wolf, J. Mille, and G. W. Taylor. Glimpse
clouds: Human activity recognition from unstructured fea-
ture points. In CVPR, volume 3, 2018.

[8] H. Ben-Younes, R. Cadene, M. Cord, and N. Thome. Mutan:
Multimodal tucker fusion for visual question answering. In
ICCV, volume 3, 2017.

[9] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Smash:
one-shot model architecture search through hypernetworks.
In ICLR, 2017.

[10] M. Engilberge, L. Chevallier, P. P´erez, and M. Cord. Find-
ing beans in burgers: Deep semantic-visual embedding with
localization. In CVPR, 2018.

[11] V. Escorcia, J. Carlos Niebles, and B. Ghanem. On the re-
lationship between visual attributes and convolutional net-
works. In CVPR, pages 1256–1264, 2015.

In International Conference on Learning and Intelli-

tion.
gent Optimization, pages 507–523. Springer, 2011.

[20] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hin-
ton. Adaptive mixtures of local experts. Neural computation,
1991.

[21] Y.-G. Jiang, Z. Wu, J. Wang, X. Xue, and S.-F. Chang. Ex-
ploiting feature and class relationships in video categoriza-
tion with regularized deep neural networks. T-PAMI, 2018.

[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, pages 1097–1105, 2012.

Imagenet
In

[23] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. E. Hubbard, and L. D. Jackel. Handwritten digit
recognition with a back-propagation network. In NIPS, 1990.
[24] C. Li, Q. Zhong, D. Xie, and S. Pu. Co-occurrence fea-
ture learning from skeleton data for action recognition and
detection with hierarchical aggregation.
arXiv preprint
arXiv:1804.06055, 2018.

[25] F. Li, N. Neverova, C. Wolf, and G. Taylor. Modout: Learn-
ing to fuse modalities via stochastic regularization. In CVIS,
2016.

[26] C. Liu, B. Zoph, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei,
A. Yuille, J. Huang, and K. Murphy. Progressive neural ar-
chitecture search. In ECCV, 2018.

[27] X. Long, C. Gan, G. de Melo, X. Liu, Y. Li, F. Li, and
S. Wen. Multimodal keyless attention fusion for video clas-
siﬁcation. In AAAI, 2018.

[28] D. C. Luvizon, D. Picard, and H. Tabia. 2d/3d pose estima-
tion and action recognition using multitask deep learning. In
CVPR, volume 2, 2018.

[29] P. Natarajan, S. Wu, S. Vitaladevuni, X. Zhuang, S. Tsaka-
lidis, U. Park, R. Prasad, and P. Natarajan. Multimodal
feature fusion for robust event detection in web videos. In
CVPR, 2012.

[30] N. Neverova, C. Wolf, G. Taylor, and F. Nebout. Moddrop:

[12] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville,

adaptive multi-modal gesture recognition. T-PAMI, 2016.

and Y. Bengio. Maxout networks. In ICML, 2013.

[13] Z. Gu, B. Lang, T. Yue, and L. Huang. Learning joint multi-
modal representation based on multi-fusion deep neural net-
works. In ICONIP, 2017.

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, pages 770–778, 2016.

[15] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed,
N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath,
et al. Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. Sig-
nal Processing Magazine, 29(6):82–97, 2012.

[16] C. Hori, T. Hori, T.-Y. Lee, Z. Zhang, B. Harsham, J. R. Her-
shey, T. K. Marks, and K. Sumi. Attention-based multimodal
fusion for video description. In ICCV, 2017.

[17] J.-F. Hu, W.-S. Zheng, J. Pan, J. Lai, and J. Zhang. Deep bi-
linear learning for rgb-d action recognition. In ECCV, pages
335–351, 2018.

[18] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
Densely connected convolutional networks. In CVPR, 2017.
[19] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential
model-based optimization for general algorithm conﬁgura-

[31] N. Neverova, C. Wolf, G. W. Taylor, and F. Nebout. Multi-
scale deep learning for gesture detection and localization. In
ECCV Workshop, pages 474–490. Springer, 2014.

[32] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In ECCV, pages 483–499.
Springer, 2016.

[33] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng.

Multimodal deep learning. In ICML, 2011.

[34] J.-M. P´erez-R´ua, M. Baccouche, and S. Pateux. Efﬁcient

progressive neural architecture search. In BMVC, 2018.

[35] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean. Ef-
ﬁcient neural architecture search via parameter sharing. In
ICML, 2018.

[36] K. J. Piczak. Esc: Dataset for environmental sound classiﬁ-

cation. In International Conference on Multimedia, 2015.

[37] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang. Ntu rgb+ d: A
large scale dataset for 3d human activity analysis. In CVPR,
pages 1010–1019, 2016.

[38] A. Shahroudy, T.-T. Ng, Y. Gong, and G. Wang. Deep
multimodal feature analysis for action recognition in rgb+
d videos. T-PAMI, 2017.

6974

[39] K. Simonyan and A. Zisserman. Two-stream convolutional
In NIPS, pages

networks for action recognition in videos.
568–576, 2014.

[40] C. G. Snoek, M. Worring, and A. W. Smeulders. Early versus

late fusion in semantic video analysis. In ACMM, 2005.

[41] S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu. An end-to-
end spatio-temporal attention model for human action recog-
nition from skeleton data. In AAAI, volume 1, pages 4263–
4270, 2017.

[42] V. Vielzeuf, A. Lechervy, S. Pateux, and F. Jurie. Central-
net: a multilayer approach for multimodal fusion. In ECCV
Workshop, 2018.

[43] Y. Wu, E. Y. Chang, K. C.-C. Chang, and J. R. Smith. Op-
In

timal multimodal fusion for multimedia data analysis.
ACMM, 2004.

[44] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi,
W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,
et al. Google’s neural machine translation system: Bridg-
ing the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.

[45] X. Yang, P. Molchanov, and J. Kautz. Multilayer and mul-
timodal fusion of deep neural networks for video classiﬁca-
tion. In ACMM, pages 978–987, 2016.

[46] G. Ye, D. Liu, I.-H. Jhuo, and S.-F. Chang. Robust late fusion

with rank minimization. In CVPR, 2012.

[47] X. Zhou and B. Bhanu. Feature fusion of side face and gait
for video-based human identiﬁcation. Pattern Recognition,
2008.

[48] B. Zoph and Q. V. Le. Neural architecture search with rein-

forcement learning. In ICLR, 2017.

[49] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning
transferable architectures for scalable image recognition. In
CVPR, 2018.

6975

