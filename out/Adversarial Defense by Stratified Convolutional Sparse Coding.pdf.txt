Adversarial Defense by Stratiﬁed Convolutional Sparse Coding

Bo Sun

Nian-hsuan Tsai

Fangchen Liu Ronald Yu Hao Su

Peking University

National Tsinghua University

UC San Diego

bosun@pku.edu.cn

nianhsuan@gmail.com

{fliu,ronaldyu,haosu}@eng.ucsd.edu

Abstract

We propose an adversarial defense method that achieves
state-of-the-art performance among attack-agnostic adver-
sarial defense methods while also maintaining robustness
to input resolution, scale of adversarial perturbation, and
scale of dataset size. Based on convolutional sparse cod-
ing, we construct a stratiﬁed low-dimensional quasi-natural
image space that faithfully approximates the natural im-
age space while also removing adversarial perturbations.
We introduce a novel Sparse Transformation Layer (STL)
between the input image and the ﬁrst layer of the neural
network to efﬁciently project images into our quasi-natural
image space. Our experiments show state-of-the-art perfor-
mance of our method compared to other attack-agnostic ad-
versarial defense methods in various adversarial settings.

1. Introduction

Existing defense mechanisms against adversarial attacks,
although able to achieve robustness in certain adversarial
settings, are still unable to achieve true robustness to all ad-
versarial inputs. The most effective existing defense meth-
ods modify the network training process to improve robust-
ness against adversarial examples [18, 24, 51, 34]. How-
ever, they are trained to defend a speciﬁed attack for a speci-
ﬁed model, limiting their real-world applications and claims
of robustness to all adversarial inputs. Ideally, our defense
mechanism should be attack agnostic and model agnostic.

Instead of modifying the network and training process,
another line of existing methods achieve the desired prop-
erty of being attack-agnostic and model-agnostic by mod-
ifying adversarial inputs to resemble clean inputs [12, 16,
36, 54, 40, 46, 32, 43]. However, these methods show weak-
nesses in other adversarial settings such as being unable to
handle larger perturbations, unable to simultaneously han-
dle many different resolutions, and not scalable to large
datasets.

In this paper, we present an input-transformation based
defense method that achieves state-of-the-art performance
when compared to previous attack-agnostic and model-
agnostic defense methods. Moreover, our method is also

!"

!#$%

T(!")

T(!#$%)

(off manifold)
!#$%

!"
(on manifold)

T(!#$%)

T(!")

Natural Image Space

Quasi-natural Image Space

Neural Network

Neural Network

far

Feature Space

Feature Space

e

s

o

l

c

Figure 1: Comparison of feature extraction between natural
image space and our learned quasi-natural image space. In
the natural image space, a neural network trained from natu-
ral images may assign different labels to the adversarial ex-
ample and the clean image, since they can be far from each
other in the feature space. After projection to the quasi-
natural image space, they tend to lie closely together in the
feature space.

far more simultaneously robust to scale of attack pertur-
bation, a variety of different input resolutions, and dataset
scale. We achieve our high level of robustness by project-
ing both clean and adversarially attacked input images into
a low-dimensional quasi-natural image space that faithfully
approximates the natural image space while also removing
adversarial perturbations so that adversarial examples will
be close to their original inputs in feature space.

We construct the quasi-natural image space in an unsu-
pervised manner using a convolutional dictionary learning-
based [4, 22, 8, 53] method, and we project the input im-
ages into our quasi-natural image space by introducing a
novel Sparse Transformation Layer (STL) between the in-
put and ﬁrst layer of the network. We can further enhance
the robustness of our pipeline by retraining a classiﬁer on
the quasi-natural images.

Experimentally, we demonstrate that our method

11447

achieves a signiﬁcant robustness improvement in a vari-
ety of different adversarial settings compared with state-
of-the-art attack-agnostic defense methods. We also show
that our quasi-natural image space is able to provide a bet-
ter blend of preservation of image details and ability to
remove adversarial perturbations compared to other input-
transformation-based adversarial defense methods.

In summary, our contributions are:

• We propose a novel and effective attack-agnostic ad-
versarial defensive method that uses a novel Sparse
Transformer Layer to transform images so that cor-
responding clean and adversarial images lie close to-
gether both in our quasi-natural image space and fea-
ture space.

• We demonstrate that our defense method achieves
state-of-the-art performance among attack-agnostic
adversarial defense methods.

• Compared to previous state-of-the-art, our defense
method is far more capable of effectively handling a
variety of image resolutions, large and small image
perturbations, and large-scaled datasets.

• Among image-transformation-based adversarial de-
fenses, our image projection onto quasi-natural image
space achieves the best blend of image detail preserva-
tion and ability to remove adversarial perturbations.

2. Related Works

Adversarial Attacks Adversarial attacks are inputs that
are intentionally slightly perturbed to fool machine learning
models. Szegedy et al. [49] ﬁrst introduce adversarial exam-
ples and generate them with the box-constrained L-BFGS
method. Goodfellow et al. [18] propose an efﬁcient single
step attack called FGSM based on network linearity. Ku-
rakin et al. [25] apply FGSM iteratively and propose BIM.
DeepFool [33] ﬁnds the smallest perturbation crossing the
model decision boundary. CW [7] solves an optimization
problem which minimizes both the objective function and
difference between adversarial and clean images. Liu et
al. [27] generate strongly transferable adversarial examples
with an ensemble-based approach. Non-gradient based at-
tacks such as one pixel attack [47] and Zoo [9] do not re-
quire knowledge of network parameters and architecture.

Adversarial Defense via Network Modiﬁcation This
type of defense aims to improve the robustness of the tar-
get model against adversarial examples. The most com-
mon method is adversarial training [18, 24, 51, 34] which
adds adversarial examples into training data. This class of
methods effectively enhances robustness to the adversari-
ally trained attacks but has poor generalizability to unknown
attacks. Other methods like feature squeezing [55], network
distillation [39], region-based classiﬁer [6] and saturating

networks [35] modify the learning strategy based on gra-
dient masking [38] and smooth the decision boundary, but
they are still vulnerable to black-box attacks [7, 37].

Adversarial Defense via Input Transformation Input-
transformation defenses aim to remove adversarial pertur-
bation transforming inputs before feeding them to the target
network. Some previous methods treat adversarial pertur-
bation as high frequency noise and resort to traditional de-
[12, 16]
noising methods to smooth small perturbations.
study the effect of JPEG compression on removing adver-
sarial noise. Osadchy et al. [36] apply a set of ﬁlters such
as median ﬁlter and averaging ﬁlter to remove perturbation.
Guo et al. [20] test ﬁve transformations and ﬁnd total varia-
tion minimization and image quilting obtain good defensive
performance. These denoising methods only ﬁx small per-
turbations and suffer from information loss.

More recently, other works have tried to purify adversar-
ial images through generative models. Meng et al. [32] pro-
pose a two-pronged defense mechanism and use a denoising
auto-encoder to remove adversarial perturbation on MNIST
digits [26]. Song et al. [46] transform adversarial images
into clean images using PixelCNN [42]. Although they
achieve good performance on small datasets, these methods
do not scale well to higher-resolution or larger datasets.

Pixel manipulation methods are also used to remove
small adversarial perturbations. Xie et al. [54] utilize ran-
dom resizing and padding to mitigate adversarial effects.
Prakash et al. [40] locally corrupt adversarial images by re-
distributing pixel values via a process we term pixel deﬂec-
tion. However, these methods suffer when they encounter
perturbations that are not extremely small.

Most similar to our method is D3 [29], which denoises
adversarial images by replacing patches with a sparse com-
bination of natural images patches. Further discussion of
D3 is reserved for Section 4.

Convolutional Dictionary Learning Convolutional
sparse representations are a form of sparse representation
learning [31] with a dictionary that has a structure that is
equivalent to convolution with a set of linear ﬁlters [17, 4].
It is widely and successfully used in signal processing and
computational imaging [19, 28, 41, 56, 57, 44]. Many
efﬁcient algorithms [22, 4, 8, 53, 11] have been developed
to solve this problems. Sung et al. also recently introduced
a method that used a deep neural network to learn sparse
dictionaries for 3D point clouds [48].

3. Approach

3.1. Method Overview

Let X be the image space and Y be the label space.
fθ(·) : X → Y is a classiﬁer parameterized by θ. Given

11448

the classiﬁer fθ(·) and a clean image x0, an adversarial ex-
ample xadv = x0 + η is an image slightly different from
x0 but confuses f :

d(xadv, x0) < ǫ but fθ(xadv) 6= fθ(x0),

(1)

where d(·, ·) is a distance function between the clean and
adversarial images. ǫ is the perturbation scale which is often
set to a small number to get almost imperceptible difference
between xadv and x0.

Adversarial examples xadv are fabricated images and
usually lie out of the natural image manifold. This may
cause the network trained from natural images, even with
adversarial data augmentation, to map xadv far away from
x0 (Figure 1 Left). Our idea is thus to recover x0 as much
as possible by projecting xadv to the natural image mani-
fold. However, parameterizing the true natural image mani-
fold is practically infeasible. We instead leverage manifold
learning to build a low-dimensional space that approximates
the natural image space, which we dub the quasi-natural
space P in this paper. Along with P, there is a transforma-
tion T that maps an image (natural or spoofed) to P. We
require that T satisﬁes the following constraints:

1. fθ(T (xadv)) = fθ(T (x0)) = yx0 ;

2. d(T (xadv), T (x0)) ≪ ǫ.

Condition 1 requires that the classiﬁer f assigns the same
groundtruth label to xadv and x0, which is our ﬁnal goal.
To guarantee Condition 1, other than learning f to optimize
classiﬁcation accuracy, we also introduce Condition 2 (Fig-
ure 1 Right). Condition 2 requires that xadv and x0 should
be situated closely in P, so that we can learn a quite smooth
function f satisfying Condition 1. This is important since
our f is a neural network, and learning a smoother map
would endow the it better generalization power.

We take an unsupervised approach to build the quasi-
natural image space. This space is constructed by stitching
multiple low-dimensional linear subspaces together. Prac-
tically, we cluster the training data into a few groups and
we learn a linear subspace for each group by convolutional
sparse coding algorithm [22, 11]. With this quasi-natural
space constructed, we are able to project any image to this
space by the sparse transformation layer introduced in Sec-
tion 3.2, which will remove a signiﬁcant amount of adver-
sarial perturbations. Then in this quasi-natural image space
we can retrain a classiﬁer to allow robust prediction over
adversarial examples (Section 3.5).

3.2. Sparse Transformation Layer (STL)

Given a classiﬁcation network f , we add a Sparse Trans-
formation Layer (STL) between the input image and the ﬁrst
layer of f . This STL layer projects the input (adversarial or

clean) onto a quasi-natural space, which removes nuisances
including adversarial perturbations in the appearance.

Let the projection of x be T (x) (assume that x is an im-
age of C channels). The projection in our STL layer follows
from the Convolutional Sparse Coding algorithm [11]. This
algorithm learns a dictionary in a convolutional manner by
solving the following optimization problem:

minimize
{fi,c},{zi}

1
2

C

X

c=1

kxc − T (x)ck2

2 + λ

K

X

i=1

kzik1

subject to

T (x)c =

(2)

fi,c ⊗ zi

K

X

i=1

kfi,ck2

2 = 1, 1 ≤ i ≤ K, 1 ≤ c ≤ C

where ⊗ indicates the convolution operator, C is the number
of input channels, K is the number of ﬁlters for each input
channel, fi,c|i=1,...,K;c=1,...,C denotes a set of ﬁlters, and
zi|i=1,...,K are the feature maps for each ﬁlter.

Different from standard sparse coding, which learns a
dictionary and code for the whole image, as shown by [50],
Problem (2) learns to reconstruct image patches by local
dictionaries and codes. Here, the local dictionary contains
the set of ﬁlters fi,c, and local codes are stored in the fea-
ture map zi. The convolution operation in the constraint
essentially computes the linear combination of local ﬁlters.
In vanilla sparse coding, a small set of bases are selected to
reconstruct the image. Similarly, in the convolutional sparse
coding formulation, a small set of ﬁlters should be selected
to reconstruct a local patch. To achieve the ﬁlter selection
goal, we have to enforce the feature map zi to be sparse by
adding the ℓ1 regularization term.

In practice, we prefer to use a small number of ﬁlters.
This forces ﬁlters to learn major and expressive local pat-
terns on the natural image manifold. Moreover, from our
observation, having too many ﬁlters may cause extra ﬁl-
ters to learn high frequency components, which can be used
to reconstruct arbitrary image patches including adversarial
perturbation that should be removed.

3.3. Learning Filters and Feature Maps

Plugging the constraint in Problem (2) into the objective
function, we see that Problem (2) is biconvex in fi,c and zi.
To solve this biconvex problem, we alternate between (1)
learning shared ﬁlters from clean images, and (2) learning
sparse feature maps for each input image with ﬁxed ﬁlters.
Next we brieﬂy introduce these two stages.

Dictionary Learning. Given feature maps, Problem (2)
becomes convex in fi,c. To solve this problem efﬁciently,
we transform to the Fourier domain [52] and use ADMM
algorithm as the solver following the framework of [11].

Sparse feature map (code) learning. Given ﬁxed ﬁlters
{fi,c}, our objective function is again a convex optimiza-

11449

Latent Perceptual Space

!" #:

!" $%:

…

DAE

'()*

Filters

Sparse
Feature
Maps

⨂

+

⨂

+

⨂

⨂…

STL: Joint Optimization

+('()*)

Figure 2: Pipeline of our defensive method. We ﬁrst feed an image to a pre-trained Denoising Auto-Encoder and ﬁnd the
cluster the image should belong to. Then we select the dictionary corresponding to the selected cluster and jointly optimize
the sparse feature maps and ﬁlters in this dictionary. In this way, we can project the input to the quasi-natural image space.

tion problem in zi. The problem is also known as Convolu-
tional Basis Pursuit DeNoising (CBPDN) [10] and we use
ADMM algorithm [3] to solve it.

3.4. Stratiﬁed Quasi Natural Image Space

Due to the high inherent variation of natural images, it
is hard to well reconstruct all images using just a small
dictionary. However, as we discussed at the end of Sec-
tion 3.2, we also do not want to employ a too big dictionary,
because the big dictionary will span an excessively high-
dimensional space, inevitably covering a signiﬁcant amount
of non-natural images. This would reduce the power of our
algorithm to ﬁlter out adversarial perturbations.

To circumvent the challenge, we split the data manifold
into several regions and learn an individual small dictio-
nary for each region. In this way, each image is still recon-
structed by a small dictionary, but we can still reconstruct
all images well using their corresponding dictionaries.

In practice, we partition the image space by clustering
natural image samples based on their perceptual features.
Generative models can learn perceptual features by recon-
struction loss. In particular, we ﬁnd that Denoising Auto-
Encoder (DAE) [2] ﬁts the adversarial setting well because
it is trained with noisy input and the feature extraction pro-
cess can modestly tolerate input noise. Speciﬁcally, we train

a DAE on both natural images and their noise-perturbed ver-
sions (Gaussian noise). In practice we ﬁnd that the original
image and adversarial attacked version usually live closely
in the latent space learned by DAE. We then use the K-
means algorithm to cluster training data [13].

The clusters allow us to partition the natural image man-
ifold. Given an arbitrary input image (adversarial or clean),
we can obtain its latent features from the DAE and ﬁnd the
k-nearest neighbors in the training image dataset. Then we
vote for the cluster the image should belong to. Once we
have found the cluster, we can either update the ﬁlters and
features maps for dictionary learning, or compute the pro-
jection of the image for classiﬁcation network training/test.

3.5. Classiﬁer Training in the Quasi Natural Space

To train a classiﬁer for image categorization, we map all
the clean training images to P. We simply use their recon-
structed version T (x0) to train a user-selected classiﬁcation
network (e.g., AlexNet). To perform defense at test time, we
apply the trained classiﬁer on the T -transformed version of
the testing image (clean or adversarial).

After projection to P, T (xadv) and T (x0) share close
perceptual and semantic features. Therefore, decisions
made in this quasi-natural space P tend to be more reliable
for adversarial examples compared to the original space.

11450

4. Discussion

In this section, we discuss our unique advantages over
existing adversarial defenses and then analyze possible rea-
sons behind the effectiveness of our method against popular
gradient-based attack methods.

Relationship With Existing Methods
In contrast to ad-
versarial learning methods [18, 24, 51] that rely on direct
knowledge of the attack method and model type, our algo-
rithm only relies on the clean training data at hand. Built
without any explicit prior knowledge of the attacker, our
design does not overﬁt to any speciﬁc attack strategy and
tends to be a generic tool.

Recent attack-agnostic defense methods use genera-
tive models to transform images into a low-dimensional
space [32, 46, 43]. We choose not to use a network to build
our low-dimensional space, since the generative network it-
self is vulnerable to adversarial attacks. Another disadvan-
tage of these methods is that the limited expressive power of
generative models restricts the domain of these methods to
datasets small in resolution and scale such as MNIST [26]
and CIFAR-10 [23]. Pixel manipulation methods [40, 54]
can work on large datasets, but they only achieve good per-
formance under extremely small perturbations. Our method
works uniquely well on large adversarial perturbation, com-
plicated datasets, and higher resolutions.

The D3 algorithm proposed in [29] is the most simi-
lar to ours.
It replaces noisy adversarial image patches
by a sparse combination of natural image patches. How-
ever, our method provides several advantages. First, D3
reconstructs images poorly on low-resolution datasets like
CIFAR-10 [23]. Second, the size of the natural patch dic-
tionary is very large (10K-40K) while we only need a small
number of ﬁlters (typically 64). The large size of their patch
dictionary has two main drawbacks: the excessive number
of dictionary elements may lead the dictionary to learn high
frequency components, which can be used to wrongly re-
construct adversarial perturbations, and the generic dictio-
nary elements are not as expressive as ours, so D3 generates
images that are not as sharp as ours as veriﬁed in our exper-
iments.

Robustness to Gradient-Based Attacks There are two
main concepts behind the effectiveness of our method
against gradient-based attacks: (1) Gradient Obfuscation:
Obtaining the numerical gradient of the STL is likely to be
challenging, because the output of the STL is the solution to
a non-convex optimization problem (has the arg min form
of the input image). Without the gradient of the STL, de-
signing gradient-based attack becomes difﬁcult. (2) High-
frequency Perturbation Removal: Existing gradient-based
attack mechanisms often introduce high-frequency pertur-
bations. With a small dictionary and the sparsity constraint
in Problem (2), the learned ﬁlters tend to be quite smooth

(Figure 2), which could ﬁlter out the high-frequency pertur-
bation patterns.

5. Experiments

In this section, we ﬁrst introduce our experimental set-
tings, and then show a quantitative and qualitative compar-
ison with other attack-agnostic adversarial defenses. We
demonstrate that our method outperforms the state-of-the-
art. Lastly, we perform an analysis of the intrinsic trade-off
between projection image quality and defense robustness of
transformation-based defenses.

5.1. Settings

We conduct experiments on CIFAR-10 [23],

Ima-
geNet [14], and ImageNet-10, where we manually choose
10 coarse-grained classes from the whole dataset, e.g. bird,
car, cat, etc. Every class contains 8000 training and 2000
testing images.

We evaluate our method on VGG-16 [45] and ResNet-
50 [21] to defend against FGSM [18], BIM [25], Deep-
Fool [33], and CW [7]. We constrain the perturbation scale
kηk2 = kxadv−x0k
to 0.04 (FGSM-0.04) and 0.08 (FGSM-
0.08) for FGSM and to 0.04 for BIM, DeepFool, and CW.

kx0k

By default, we set the ﬁlter number K = 64, ﬁlter size
S = 8, and sparse constraint λ = 0.2. We ﬁrst downsample
images to 32×32 to train a DAE, and split the latent space to
4 clusters for CIFAR-10 and ImageNet-10, and 10 clusters
for ImageNet.

5.2. Adversarial Defense

We evaluate the defensive effectiveness of our method of
retraining a classiﬁer on quasi-natural images and then pro-
jecting adversarial examples onto the quasi-natural image
space as described in Section 3.5.

Classiﬁcation accuracy comparison results are in Table
1 for CIFAR-10, Table 2 for ImageNet-10 and Table 3 for
ImageNet. In Table 1 and Table 2 we follow our setting as
described in Section 5.1. In Table 3 we follow the experi-
mental setting in [20] and [29]. Although we compare with
other methods in their preferred resolution and datasets for a
fair comparison, we note that one of the unique advantages
of our method is that it performs well in various resolutions
(in our experiments, from 32 to 224), while others can only
work on a limited range of resolutions.

Comparison results show that our method signiﬁcantly
improves the classiﬁcation robustness against unknown
black-box attacks and outperforms state-of-the-art methods
in most types of attacks with a large margin. Moreover, our
retrained model achieves high accuracy on clean data and is
comparable to the clean model, which means we preserve
rich ﬁne details that allow the network to learn discrimina-
tive features. Furthermore, we also compare our method

11451

Adv

TVM

Quilting

TVM+Quilting

D3

PDeflect

STL (Ours)

Clean

Figure 3: Qualitative comparison of image reconstruction results on ImageNet. The ﬁrst column is input the adversarial
examples generated by FGSM [18] attack with L2 dissimilarity 0.08. The last column is the corresponding clean images.
Visually, our method outperforms others on removing adversarial perturbations and keeping input details. D3 refers to [29]
and PDeﬂect refers to [40].

Table 1: CIFAR-10 classiﬁcation accuracy for adversarial
examples on VGG-16 after defense by methods in compari-
son. All methods are trained and tested on their transformed
data. “Clean” means accuracy of transformed clean data on
each method. “STL” denotes STL transformation with a
single universal set of ﬁlters. “STL (cluster)” denotes STL
ﬁlters are chosen through latent space clustering.

Table 2:
ImageNet-10 classiﬁcation accuracy for adver-
sarial examples on VGG-16 after defense by methods in
comparison at resolution 64 (Table 2.A) and 128 (Table
2.B). All methods are trained and tested on their trans-
formed data by their defense method. Here Crop-Ens de-
notes Crop+TVM+Quilting in [20] and PD-Ens denotes
PD+R-CAM+DWT in [40].

Defense

Clean

FGSM-0.08

FGSM-0.04

BIM DeepFool

CW

No Defense
MagNet[32]

PixelDefend[46]

STL

STL (cluster)

0.9298
0.9206
0.9041
0.9002
0.9011

0.5816
0.7393
0.8316
0.8515
0.8567

0.6523
0.8552
0.8799
0.8732
0.8715

0.1803
0.7707
0.8515
0.8754
0.8803

0.1760
0.8770
0.8827
0.8838
0.8890

0.0936
0.8594
0.8845
0.8880
0.8904

with the widely used adversarial training [24] and show that
we achieve better results on unknown attacks (Appendix B).

5.3. White Box Attacks

Our defense is designed primarily for black/grey-box at-
tacks, and like other methods, is highly susceptible to white-
box attacks, especially on ImageNet [14]. Nevertheless,
we show that our method is signiﬁcantly less susceptible
to the white-box attack Backward Pass Differentiable Ap-
proximation (BPDA) on CIFAR-10 [1]. BPDA speciﬁcally
targets defenses in which the gradient does not optimize
the loss; this is the case for our method since our STL
is non-differentiable. Table 4 shows that although our de-
fense accuracy is hurt by obfuscated gradient-based attacks,

Table 2.A Resolution 64.

Defense

Clean

FGSM-0.08

FGSM-0.04

BIM DeepFool

CW

No Defense
TVM[20]

Quilting[20]
Crop-Ens[20]
PD-Ens[40]

STL

STL (cluster)

0.8665
0.7555
0.7741
0.7508
0.8250
0.8438
0.8421

0.2816
0.5997
0.7304
0.6968
0.6634
0.7275
0.7514
Table 2.B Resolution 128.

0.1883
0.7156
0.7642
0.7369
0.7903
0.8164
0.8103

0.3080
0.6930
0.7418
0.7221
0.7607
0.8002
0.8038

0.0811
0.7210
0.7646
0.7401
0.7955
0.8163
0.8221

0.0751
0.7187
0.7662
0.7304
0.7813
0.8058
0.8122

Defense

Clean

FGSM-0.08

FGSM-0.04

BIM DeepFool

CW

No Defense
TVM[20]

Quilting[20]
Crop-Ens[20]
PD-Ens[40]

STL

STL (cluster)

0.8991
0.8567
0.8354
0.8382
0.8603
0.8784
0.8721

0.2123
0.7302
0.7612
0.7640
0.6740
0.7202
0.7421

0.2409
0.8181
0.7914
0.7969
0.8011
0.8308
0.8356

0.1790
0.8183
0.8048
0.8033
0.8273
0.8320
0.8385

0.0584
0.8221
0.8164
0.8071
0.8320
0.8560
0.8494

0.0504
0.8101
0.8093
0.7955
0.8262
0.8449
0.8421

it is much more robust than other defenses with this phe-
nomenon on CIFAR-10 dataset.

On ImageNet [14], all defense methods in their case
study ([20] and [54]) get 0% defense accuracy. Under the

11452

Table 3: Top-1 ImagetNet classiﬁcation accuracy for adver-
sarial examples on ResNet-50 after defense by methods in
comparison. We follow experimental settings in [20] and
[29] where all attacks are in an average normalized L2-
dissimilarity of 0.06. All methods are trained and tested
on their transformed data.

Defense

Clean

FGSM BIM DeepFool

CW UAP

No Defense

quilt[20]

TVM+quilt[20]
Crop-Ens[20]
D3 (40K-5)[29]
D3 (10K-5)[29]
D3 (10K-4)[29]

PD-Ens[40]
STL (cluster)

0.761
0.701
0.724
0.721
0.718
0.708
0.690
0.719
0.721

0.107
0.655
0.657
0.667
0.686
0.683
0.671
0.637
0.693

0.012
0.656
0.658
0.670

-
-
-

0.633
0.678

0.010
0.652
0.658
0.671
0.631
0.646
0.648
0.638
0.685

0.019
0.641
0.640
0.635

-
-
-

0.643
0.677

0.133

-
-
-

0.715
0.703
0.689
0.667
0.712

Table 4: Backward Pass Differentiable Approximation
(BPDA) [1] attack results on CIFAR-10, VGG-16. All
methods are attacked at distance L∞ = 0.031. Defenses
denoted with ∗ propose combining adversarial training.

Defense
Accuracy

SAP [15] TE [5] LID [30]

PD [46] MagNet [32]

0.00

0.00*

0.05

0.09*

0.10

STL
0.38*

STL (cluster)

0.42*

same settings, our defense accuracy similarly collapses to
1%. We further analyze our method’s robustness to other
simple custom-made white-box attacks with full knowl-
edge of our model (including dictionary coefﬁcients) in Ap-
pendix C.

5.4. Input Transformation Effectiveness

Since STL has a strong reconstruction capacity, the pro-
jected images still faithfully preserve information from the
input data space. This is a useful property since it allows us
to use a vanilla model to partially defend against adversarial
examples when we are not able train our own classiﬁer on
quasi-natural images due to limitations such as access to the
entire dataset.

Hence, we also evaluate the accuracy of using STL to
project adversarial examples of a vanilla model that was
pre-trained only on clean data. To perform the defense, we
simply project the input into quasi-natural space and feed
the projected image back into the vanilla model.

We compare with other input-transformation methods
applied to attacked vanilla models in Table 5 for CIFAR-10,
Table 6 for ImageNet-10 and Table 7). Qualitative com-
parisons of our input transformations are shown in Figure 4
for CIFAR-10 and Figure 3 for ImageNet. More results are
in Appendix E.

Under relatively large perturbations (e.g. FGSM-0.08),
all competing methods fail to successfully overcome adver-
sarial attacks while our method signiﬁcantly outperforms
them. On slightly perturbed adversarial examples (e.g.
DeepFool and CW), we achieve a strong defense and also
maintain accuracy on clean data. We see that our method

Adv

MagNet

PD

STL (Ours)

Clean

Figure 4: Qualitative comparison on CIFAR-10 [23] with
MagNet [32] and PixelDefend (PD) [46]. The ﬁrst col-
umn is adversarial images generated by FGSM [18] with
L2-dissimilarity = 0.08. The last column is corresponding
clean images. We can observe that MagNet cannot fully
remove adversarial perturbation, while PixelDefend over-
smooths images, causing large information loss and some-
times introducing colorful artifacts.

Table 5: CIFAR-10 classiﬁcation accuracy of transformed
clean and adversarial examples on the attacked vanilla
VGG-16 model.

Defense

Clean

FGSM-0.08

FGSM-0.04

BIM DeepFool

CW

No Defense
MagNet[32]

PixelDefend[46]

STL

STL (cluster)

0.9298
0.9035
0.8526
0.8285
0.8360

0.5816
0.6145
0.6810
0.7099
0.7103

0.6523
0.6521
0.7329
0.7487
0.7547

0.1803
0.4312
0.7729
0.7462
0.7531

0.1760
0.6535
0.7414
0.7854
0.7959

0.0936
0.4845
0.7579
0.7765
0.7906

can effectively defend against adversarial attacks even us-
ing a vanilla clean model.

5.5. Trade off Between Quality and Robustness

In transformation-based adversarial defenses, we typi-
cally aim to remove adversarial perturbations while preserv-
ing useful details. However, this is hard to achieve, as im-
portant details and adversarial perturbations are usually re-
moved together. Thus, we examine the inherent trade-off
between transformation quality and defensive robustness in
our method.

In our method, the key parameter controlling the pro-
jection quality is the sparsity constraint weight λ:
a
larger λ causes more blurry results. We gradually in-

11453

ImageNet-10 classiﬁcation accuracy of trans-
Table 6:
formed clean and adversarial examples on an attacked
vanilla VGG-16 model at resolution 64 (Table 6.A) and 128
(Table 6.B).

Table 6.A Resolution 64.

Defense

Clean

FGSM-0.08

FGSM-0.04

BIM DeepFool

CW

No Defense
TVM[20]

Quilting[20]
Crop-Ens[20]
PD-Ens[40]

STL

STL (cluster)

0.8665
0.8172
0.6318
0.5590
0.7946
0.7925
0.8017

0.2816
0.3403
0.4541
0.4570
0.3388
0.5472
0.5729
Table 6.B Resolution 128.

0.1883
0.6595
0.5696
0.5369
0.6568
0.7245
0.7234

0.3080
0.4744
0.5312
0.5328
0.5526
0.6825
0.6914

0.0811
0.6943
0.5436
0.5429
0.6919
0.7562
0.7652

0.0751
0.6823
0.5563
0.5320
0.6827
0.7414
0.7521

Defense

Clean

FGSM-0.08

FGSM-0.04

BIM DeepFool

CW

No Defense
TVM[20]

Quilting[20]
Crop-Ens[20]
PD-Ens[40]

STL

STL (cluster)

0.8991
0.8591
0.8149
0.7730
0.8789
0.8654
0.8759

0.2123
0.2568
0.3903
0.4622
0.2333
0.4552
0.4733

0.2409
0.4386
0.5889
0.6447
0.4286
0.6418
0.6606

0.1790
0.6586
0.6434
0.6876
0.7221
0.7332
0.7323

0.0584
0.6360
0.6242
0.7060
0.7359
0.7308
0.7301

0.0504
0.6129
0.5922
0.6888
0.7272
0.7212
0.7432

Table 7: Top-1 ImageNet classiﬁcation accuracy of trans-
formed clean and adversarial examples on an attacked
vanilla ResNet-50 model.

Defense

Clean

FGSM-0.08

FGSM-0.04

BIM DeepFool

CW

No Defense
TVM[20]

Quilting[20]
Crop-Ens[20]
PD-Ens[40]

STL

STL (cluster)

0.7613
0.6205
0.4168
0.6432
0.6821
0.6728
0.6921

0.0862
0.3123
0.3787
0.4623
0.3846
0.5348
0.5588

0.1140
0.4256
0.3865
0.5546
0.5691
0.6032
0.6053

0.0131
0.4923
0.3823
0.5965
0.6089
0.6253
0.6348

0.0106
0.5232
0.3859
0.6023
0.6220
0.6233
0.6468

0.0201
0.5012
0.3783
0.5980
0.6371
0.6158
0.6220

crease λ and explore this trade-off (Figure 5). We de-
note Acc(x) as the accuracy on the vanilla model of in-
put x. Higher Acc(T (x0)) means higher transformation
quality because the projected images still preserve useful
information. Small kAcc(T (x0)) − Acc(T (xadv))k means
the clean and adversarial examples are similar in feature
space. The decision can be robust if both Acc(T (x0))
and Acc(T (xadv)) are high. In Figure 5, we see that as
λ increases, Acc(T (x0)) decreases and the gap between
Acc(T (x0)) and Acc(T (xadv)) shrinks.

We additionally propose a metric to measure this trade-
off. Speciﬁcally, we use PSNR between T (xadv) and
x0 to measure reconstruction quality. For each method
let a0 = Acc(T (x0)) and aadv =
in comparison,
Acc(T (xadv)), then we associate it with a characteristic in-
terval [min(a0, aadv), max(a0, aadv)] to represent its over-
all prediction quality. Apparaently, a strong method should
have an interval that is short (good robustness) and high
(good accuracy). We plot a 2D PSNR vs. prediction quality
map, where the top right corner indicates highest robust-
ness and prediction quality. In Figure 6, we show compar-
ison results of occupied regions on this map. Our method
achieves both the highest PSNR and most preferable charac-

(a).

!(#$)

!(#&'()

(b).

0.8

0.6

0.4

y
c
a
r
u
c
c
A

! #$
! #&'(

0.0

0.1

0.2

0.3

0.4

0.5

)

Figure 5: Intrinsic tradeoff between image reconstruction
quality and defensive robustness. (a). Transformation re-
sults of each corresponding λ. (b). Accuracy of T (xadv
and T (x0) on attacked vanilla model.
(Setting: FGSM-
0.08, ImageNet-10, VGG-16, resolution 64).

0.85

0.80

0.75

0.70

0.65

0.60

0.55

0.50

y
c
a
r
u
c
c
A

STL
Pixel Deflection
TVM
Quilting
TVM+Quilting

23

24

25

26

27

28

PSNR

Figure 6: The PSNR, Acc(T (xadv)) and Acc(T (x0))
of different methods (Setting: FGSM-0.08, ImageNet-10,
VGG-16, resolution 64). For both axes, the higher number
the better. And less difference between Acc(T (xadv)) and
Acc(T (x0)) means higher robustness.

teristic interval, demonstrating its superior ability to achieve
robustness, accuracy, and maintain image quality.

6. Conclusion

We have proposed a novel state-of-the-art attack-
agnostic adversarial defense method with additional in-
creased robustness to input resolution, perturbation scale,
and dataset scale.
Inspired by convolutional sparse cod-
ing, we design a novel sparse transformation layer (STL) to
project the inputs to a low-dimensional quasi-natural space,
wherein a retrained classiﬁer can make more reliable deci-
sions. We evaluate the proposed method on CIFAR-10 and
ImageNet and show that our defense mechanism provide
state-of-the-art results. We have also provided an analysis
of the trade-off between the projection image quality and
defense robustness.

Acknowledgements We thank Bo Li for providing key
discussions on white box attacks.

11454

References

[1] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradi-
ents give a false sense of security: Circumventing defenses
to adversarial examples, 2018. In ICML.

[2] Y. Bengio, L. Yao, G. Alain, and P. Vincent. Generalized de-
noising auto-encoders as generative models, 2013. In NIPS.
[3] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Dis-
tributed optimization and statistical learning via the alternat-
ing direction method of multipliers. Foundations and Trends
in Machine Learning, 3(1):1–122,, 2010.

[4] H. Bristow, A. Eriksson, and S. Lucey. Fast convolutional

sparse coding, 2013. In CVPR.

[5] J. Buckman, A. Roy, C. Raffel, and I. Goodfellow. Ther-
mometer encoding: One hot way to resist adversarial exam-
ples. In International Conference on Learning Representa-
tions(ICLR), 2018.

[6] X. Cao and N. Z. Gong. Mitigating evasion attacks to deep
neural networks via region-based classication, 2017. In Pro-
ceedings of the 33rd Annual Computer Security Applications
Conference. ACM.

[7] N. Carlini and D. A. Wagner. Towards evaluating the robust-
ness of neural networks, 2017. 2017 IEEE Symposium on
Security and Privacy (SP).

[8] R. Chalasani, J. C. Principe, and N. Ramakrishnan. A fast
proximal method for convolutional sparse coding, 2013. In
IJCNN.

[9] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh. Zoo:
Zeroth order optimization based black-box attacks to deep
neural networks without training substitute models, 2017.
arXiv preprint arXiv:1708.03999.

[10] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic
SIAM J. Sci. Comput.,

decomposition by basis pursuit.
20(1):31–61, 1998.

[11] B. Choudhury, R. Swanson, F. Heide, G. Wetzstein, and
W. Heidrich. Consensus convolutional sparse coding, 2017.
In ICCV.

[12] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, L. Chen,
M. E. Kounavis, and D. H. Chau. Keeping the bad guys out:
Protecting and vaccinating deep learning with jpeg compres-
sion, 2017. arXiv preprint arXiv:1705.02900.

[13] A. David and S. Vassilvitskii. k-means++: The advantages of
careful seeding, 2007. Proceedings of the eighteenth annual
ACM-SIAM symposium on Discrete algorithms.

[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F. fei Li.
Imagenet: A large-scale hierarchical image database, 2009.
In CVPR.

[15] G. S. Dhillon, K. Azizzadenesheli, J. D. Bernstein, J. Kos-
saiﬁ, A. Khanna, Z. C. Lipton, and A. Anandkumar. Stochas-
tic activation pruning for robust adversarial defense. In In-
ternational Conference on Learning Representations(ICLR),
2018.

[16] G. K. Dziugaite, Z. Ghahramani, and D. M. Roy. A study of
the effect of jpg compression on adversarial images, 2016.
CoRR, abs/1608.00853.

[17] C. Garcia-Cardona and B. Wohlberg. Convolutional dictio-
nary learning: A comparative review and new algorithms,
2018. arXiv preprint arXiv:1709.02893.

[18] I. J. Goodfellow, J. Shlens, and C. Szegedy.

Explain-
ing and harnessing adversarial examples, 2014. CoRR,
abs/1412.6572.

[19] S. Gu, W. Zuo, Q. Xie, D. Meng, X. Feng, and L. Zhang.
Convolutional sparse coding for image super-resolution,
2015. In ICCV.

[20] C. Guo, M. Rana, M. Cisse, and L. van der Maaten. Coun-
tering adversarial images using input transformations, 2018.
In ICLR.

[21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition, 2017. In CVPR.

[22] F. Heide, W. Heidrich, and G. Wetzstein. Fast and exible

convolutional sparse coding, 2015. In CVPR.

[23] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian

institute for advanced research).

[24] A. Kurakin, I. Goodfellow, and S. Bengio. Adversar-
arXiv preprint

ial machine learning at scale., 2016.
arXiv:1611.01236.

[25] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial ex-
amples in the physical world. CoRR, abs/1607.02533, 2016.

[26] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition., 1998. Pro-
ceedings of the IEEE, 86(11):22782324.

[27] Y. Liu, X. Chen, C. Liu, and D. X. Song. Delving into trans-
ferable adversarial examples and black-box attacks. CoRR,
abs/1611.02770, 2016.

[28] Y. Liu, X. Chen, R. K. Ward, and Z. J. Wang. Image fusion
with convolutional sparse representation, 2016. IEEE Signal
Process. Lett.

[29] S.-M. M.-Dezfooli, A. Shrivastava, and O. Tuzel. Divide,
denoise, and defend against adversarial attacks, 2018. arXiv
preprint, arXiv:1802.06806v1.

[30] X. Ma, B. Li, Y. Wang, S. M. Erfani, S. Wijewickrema,
G. Schoenebeck, M. E. Houle, D. Song, and J. Bailey. Char-
acterizing adversarial subspaces using local intrinsic dimen-
sionality.
In International Conference on Learning Repre-
sentations(ICLR), 2018.

[31] J. Mairal, F. Bach, and J. Ponce. Sparse modeling for image
and vision processing. Foundations and Trends in Computer
Graphics and Vision, 8(2-3):85–283, 2014.

[32] D. Meng and H. Chen. Magnet: A two-pronged defense

against adversarial examples, 2017. In CCS.

[33] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deep-
fool: A simple and accurate method to fool deep neural net-
works, 2016. In CVPR.

[34] T. Na, J. H. Ko, and S. Mukhopadhyay. Cascade adversar-
ial machine learning regularized with a uniﬁed embedding,
2018. In ICLR.

[35] A. Nayebi and S. Ganguli. Biologically inspired protec-
tion of deep networks from adversarial attacks, 2017. arXiv
preprint arXiv:1703.09202.

[36] M. Osadchy, J. Hernandez-Castro, S. Gibson, O. Dunkel-
man, and D. Perez-Cabo. No bot expects the deepcaptcha!
introducing immutable adversarial examples with applica-
tions to captcha, 2017. In IEEE Transactions on Information
Forensics and Security.

11455

In Network and Distributed Systems Security Symposium
(NDSS).

[56] H. Zhang and V. Patel. Convolutional sparse coding-based

image decomposition, 2016. In BMVC.

[57] H. Zhang and V. M. Patel. Convolutional sparse and low-

rank coding-based rain streak removal, 2017. In WACV.

[37] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik,
and A. Swami. Practical black-box attacks against machine
learning, 2017. In ACM Asia Conference on Computer and
Communications Security,.

[38] N. Papernot, P. McDaniel, A. Sinha, and M. Wellman. To-
wards the science of security and privacy in machine learn-
ing, 2016. arXiv preprint arXiv:1611.03814.

[39] N. Papernot, P. D. McDaniel, X. Wu, S. Jha, and A. Swami.
Distillation as a defense to adversarial perturbations against
deep neural networks, 2016. 2016 IEEE Symposium on Se-
curity and Privacy (SP).

[40] A. Prakash, N. Moran, S. Garber, A. DiLillo, and J. Storer.
In

Deecting adversarial attacks with pixel deection, 2018.
CVPR 2018.

[41] T. M. Quan and W.-K. Jeong. Compressed sensing re-
construction of dynamic contrast enhanced mri using gpu-
accelerated convolutional sparse coding, 2016. In ISBI.

[42] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma.
Improving the pixelcnn with discretized lo-
Pixelcnn++:
gistic mixture likelihood and other modiﬁcations, 2017.
arXiv:1701.05517.

[43] P. Samangouei, M. Kabkab, and R. Chellappa. Defense-gan:
Protecting classiﬁers against adversarial attacks using gener-
ative models, 2018. In ICLR.

[44] A. Serrano, F. Heide, D. Gutierrez, G. Wetzstein, and B. Ma-
sia. Convolutional sparse coding for high dynamic range
imaging. Computer Graphics Forum, 35(2):153–163, 2016.
[45] K. Simonyan and A. Zisserman. Very deep convolutional
arXiv

networks for large-scale image recognition, 2014.
preprint arXiv:1409.1556.

[46] Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman.
Pixeldefend: Leveraging generative models to understand
and defend against adversarial examples, 2018. In ICLR.

[47] J. Su, D. V. Vargas, and K. Sakurai. One pixel attack for
fooling deep neural networks. CoRR, abs/1710.08864, 2017.
[48] M. Sung, H. Su, R. Yu, and L. Guibas. Deep functional
dictionaries: Learning consistent semantic structures on 3d
models from functions. arXiv preprint arXiv:1805.09957,
2018.

[49] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. J. Goodfellow, and R. Fergus. Intriguing properties of neu-
ral networks, 2014. In ICLR.

[50] I. Tosic and P. Frossard. Dictionary learning. IEEE Signal

Process. Mag., 28(2):27–38, 2011.

[51] F. Tramr, A. Kurakin, N. P. abd D. Boneh, and P. D. Mc-
Daniel. Ensemble adversarial training: Attacks and de-
fenses., 2018. In ICLR.

[52] B. Wohlberg. Efﬁcient convolutional sparse coding. 2014
IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 7173–7177, 2014.

[53] B. Wohlberg. Efcient convolutional sparse coding, 2014. In

ICASSP.

[54] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille. Mitigating
adversarial effects through randomization. In International
Conference on Learning Representations, 2018.

[55] W. Xu, D. Evans, and Y. Qi. Feature squeezing: Detect-
ing adversarial examples in deep neural networks, 2018.

11456

