Dense Intrinsic Appearance Flow for Human Pose Transfer

Yining Li1

Chen Huang2

Chen Change Loy3

1CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong

2Robotics Institute, Carnegie Mellon University

3School of Computer Science and Engineering, Nanyang Technological University

ly015@ie.cuhk.edu.hk

chenh2@andrew.cmu.edu

ccloy@ntu.edu.sg

Abstract

We present a novel approach for the task of human pose
transfer, which aims at synthesizing a new image of a person
from an input image of that person and a target pose. Unlike
existing methods, we propose to estimate dense and intrin-
sic 3D appearance ﬂow to better guide the transfer of pix-
els between poses. In particular, we wish to generate the 3D
ﬂow from just the reference and target poses. Training a net-
work for this purpose is non-trivial, especially when the an-
notations for 3D appearance ﬂow are scarce by nature. We
address this problem through a ﬂow synthesis stage. This is
achieved by ﬁtting a 3D model to the given pose pair and
project them back to the 2D plane to compute the dense ap-
pearance ﬂow for training. The synthesized ground-truths
are then used to train a feedforward network for efﬁcient
mapping from the input and target skeleton poses to the
3D appearance ﬂow. With the appearance ﬂow, we per-
form feature warping on the input image and generate a
photorealistic image of the target pose. Extensive results
on DeepFashion and Market-1501 datasets demonstrate the
effectiveness of our approach over existing methods. Our
code is available at http://mmlab.ie.cuhk.edu.
hk/projects/pose-transfer/

1. Introduction

The ability to predict what an object will look like from a
new viewpoint is fundamental to intelligence. Human pose
transfer [26] is an important instantiation of such view syn-
thesis task. Given a single view/pose of one person, the goal
is to synthesize an image of that person in arbitrary poses.
This task is of great value to a wide range of applications in
computer vision and graphics. Examples include video syn-
thesis and editing and data augmentation for problems like
person re-identiﬁcation where it is hard to acquire enough
same-person images from different cameras.

Despite the rapid progress in deep generative models
like Generative Adversarial Networks (GAN) [6] and Varia-

tional Auto Encoders (VAE) [17], human image generation
between poses is still exceedingly difﬁcult. The main chal-
lenge is to model the large variations in 2D appearance due
to the change in 3D pose. This is further compounded by
human body self-occlusion that induces ambiguities in in-
ferring unobserved pixels for the target pose.
In general,
successful human pose transfer requires a good representa-
tion or disentangling of human pose and appearance, which
is non-trivial to learn from data. The ability to infer invisible
parts is also necessary. Moreover, the image visual quality
largely depends on whether the high frequency details can
be preserved, e.g. in cloth or face regions.

Most existing methods for human pose transfer [1, 5, 18,
23, 24, 27, 28, 47] employ an encoder-decoder architecture
to learn the appearance transformation from an input im-
age, guided by the input and target 2D pose encoded with
some keypoints of the human-body joints. However, such
keypoint-based representation is only able to capture rough
spatial deformations, but not ﬁne-grained ones. As a re-
sult, distortions or unrealistic details are often produced,
especially in the presence of large pose change with non-
rigid body deformations. Recent advances either decom-
pose the overall deformation by a set of local afﬁne trans-
formations [34], or use a more detailed pose representation
than the keypoint-based one. The latter is to enable ‘dense
appearance ﬂow’ computation that more accurately speci-
ﬁes how to move pixels from the input pose. Neverova et
al. [26] showed that the surface-based pose representation
via DensePose [7] serves as a better alternative. Zanﬁr et
al. [44] turned to ﬁt a 3D model to both input and target
images, and then perform appearance transfer between the
corresponding vertices. The resulting appearance ﬂow with
3D geometry supervision is more ideal, but the 3D model
ﬁtting would incur too much burden at inference time.

In this paper, we propose a novel approach to human
pose transfer that integrates implicit reasoning about 3D ge-
ometry from 2D representations only. This allows us to
share the beneﬁts of using 3D geometry for accurate pose
transfer but at much faster speed. Our key idea is to recover

3693

Figure 1: The proposed human pose transfer method with dense intrinsic 3D appearance ﬂow generates higher quality images
in comparison to baselines. (Left) The core of our method is a ﬂow regression module (the green box) that can transform the
reference and target poses into a 3D appearance ﬂow map and a visibility map.

from training image pairs (along with their pose keypoints)
the underlying 3D models, which when projected back to
2D image plane can provide the ground-truth appearance
ﬂow for us to learn from. Such dense and intrinsic appear-
ance ﬂow implicitly encodes the 3D structures of human
body. Then we train an appearance ﬂow generation module,
represented by the traditional feedforward network, which
directly regresses from a pair of 2D poses to the correspond-
ing appearance ﬂow. This module helps us to bypass the ex-
pensive 3D model ﬁtting at test time, and predict the intrin-
sic pixel-wise correspondence pretty fast without requiring
explicit access to 3D geometry.

Figure 1 (left) illustrates our overall image generation
framework. Given a reference image (and its pose) and the
target pose, we ﬁrst use a variant of U-Net [29] to encode
the image and target pose respectively. Then our appear-
ance ﬂow module generates a 3D ﬂow map from the pose
pair, and further generates a visibility map to account for the
missing pixels in the target pose due to self-occlusions. The
visibility map proves necessary for our network to synthe-
size missing pixels at the correct locations. To render the ﬁ-
nal image in target pose, the encoded image features are ﬁrst
warped through the generated ﬂow map, and then passed
to a gating module guided by the visibility map. Finally,
our pose decoder concatenates such processed image fea-
tures to generate the image. Our U-Net-type image gener-
ator and appearance ﬂow module are trained end-to-end so
as to optimize a combination of reconstruction, adversarial
and perceptual losses. Our approach is able to generate high
quality images on DeepFashion [20] and Market-1501 [48]

datasets, showing consistent improvements over existing
image generators based on keypoints or other pose repre-
sentations. Our method also achieves compelling quantita-
tive results.

The main contributions of this paper can be summarized

as follows:

• A feedforward appearance ﬂow generation module is
proposed to efﬁciently encode the dense and intrinsic
correspondences in 3D space for human pose transfer.

• An end-to-end image generation framework is learned
to move pixels with the appearance ﬂow map and han-
dle self-occlusions with a visibility map.

• State-of-the-art performance and high quality images

are produced on DeepFashion dataset.

2. Related Work

Deep generative image models. Recent years have seen a
breakthrough of deep generative methods for image gener-
ation, using Generative Adversarial Networks (GAN) [6],
Variational Autoencoder (VAE) [17] and so on. Among
these, GAN has drawn a great attention due to its ca-
pability of generating realistic images. Follow-up works
make GANs conditional, generating images based on ex-
tra inputs like class labels [25], natural language descrip-
tions [33, 45, 46] or images from another domain [12]
that leads to an image-to-image domain transfer frame-
work. Adversarial learning has also shown its effectiveness
in many other tasks like image super-resolution [19, 39, 40]
and texture generation [42].

3694

3D Human Model3D Human ModelVertex Matching&Visibility AnalysisReference ImageVisibility MapFlow MapTarget ImageReference PoseTarget PoseFlow Regression ModuleImage GeneratorReference ImageTarget PoseReference PoseGenerated ImageTarget 3D ModelReference 3D ModelData SynthesisTrainingTestingFlow and Visibility MapsHuman pose transfer. Generating human-centric images
is an important sub-area of image synthesis. Example tasks
range from generating full human body in clothing [18] to
generating human action sequences [3]. Ma et al. [23] are
the ﬁrst ones to approach the task of human pose transfer,
which aims to generate a person image in a target pose if a
reference image of that person is given beforehand. The
pose comprised of 18 keypoints, is represented as a 18-
channel keypoint heatmap. Then it is concatenated with the
reference image and fed into a two-stage CNN for adversar-
ial training. Zhao et al. [47] adopted a similar coarse-to-ﬁne
approach to generate new images, but conditioned on the
target view rather than target pose with multiple keypoints.
To better handle the non-rigid body deformation in large
pose transfer, Siarohin et al. [34] proposed Deformable
GAN to decompose the overall deformation by a set of local
afﬁne transformations. Another line of works [5, 24, 27, 28]
focus on disentangling human appearance and pose with
weak supervision. With only single image rather than a pair
as input, these methods try to distill appearance informa-
tion in a separate embedding, sometimes with the help of
cycle-consistent penalty [27].

Geometry-based pose transfer. Some recent works inte-
grate geometric constraints of human body to improve pose
transfer. Neverova et al. [26] proposed a surface-based pose
representation on top of DensePose [7]. This allows to
map the body pixels to a meaningful UV-coordinate space,
where surface interpolation and inpainting can happen be-
fore warping back to the image space. Zanﬁr [44] on the
other hand, proposed to leverage 3D human model to ex-
plicitly capture the body deformations. Speciﬁcally, they ﬁt
a 3D human model [21] to both source and target images us-
ing the method in [43], where a human body is represented
by 6890 surface vertices. Then the pixels on overlapping
vertices are directly transfered to the target image, while
the invisible vertices in source image are hallucinated using
a neural network. The main drawback of this work is that
3D model ﬁtting is computationally expensive and is not
always accurate. Our method avoids the costly 3D model
ﬁtting at test time, and instead learns to predict the 2D ap-
pearance ﬂow map and visibility map deﬁned by 3D cor-
respondences in order to guide pixel transfer. This enables
implicit reasoning about 3D geometry without requiring ac-
cess to it.

Appearance ﬂow for view synthesis. Optical ﬂow [9] pro-
vides dense pixel-to-pixel correspondence between two im-
ages, and has been proved useful in tasks like action recog-
nition in video [35]. Appearance ﬂow [49] also speciﬁes
dense correspondence often between images with different
view-points, which is closer to our setting. However, pre-
vious works mainly estimate appearance ﬂow from simple
view transformations (e.g., a global rotation) or rigid ob-
jects (e.g., a car). Whereas our appearance ﬂow module

deals with the articulated human body with arbitrary pose
transformation.

3. Methodology

3.1. Problem Formulation and Notations

Given a reference person image x and a target pose p,
our goal is to generate a photorealistic image ˆx for that
person but in pose p. For arbitrary pose transfer, we sim-
ply adopt the commonly-used pose representation to guide
such transfer. Speciﬁcally, we use 18 human keypoints ex-
tracted by a pose estimator [2] as in [23, 34]. The keypoints
are encoded into a 18-channel binary heatmap, where each
channel is ﬁlled with 1 within a radius of 8 pixels around
the corresponding keypoint and 0 elsewhere. During train-
ing, we consider the image pair (x1, x2) (source and target)
with their corresponding poses (p1, p2). The model takes
the triplet (x1, p1, p2) as inputs and tries to generate ˆx2 with
small error versus target image x2 in pose p2.

The proposed dense intrinsic appearance ﬂow consists of
two components, namely a ﬂow map F(x1,x2) and a visibil-
ity map V(x1,x2) between image pair (x1, x2) to jointly rep-
resent their pixel-wise correspondence in 3D space. In the
following, we omit the subscript and brief them as F and V
for simplicity. Note F and V have the same spatial dimen-
sions as the target image x2. Assume that u′
i and ui are the
2D coordinates in images x1 and x2 that are projected from
the same 3D body point hi, F and V can be deﬁned as:

fi =F (ui) = u′
vi =V (ui) = visibility(hi, x1),

i − ui,

(1)

where visibility(hi, x1) is a function that indicates whether
hi is invisible (due to self-occlusion or out of the image
plane) in x1. It outputs 3 discrete values (representing vis-
ible, invisible or background) which are color-coded in a
visibility map V (see an example in Fig. 3).

3.2. Overall Framework

Figure. 2 illustrates our human pose transfer framework.
Given the input image x1 and its extracted pose p1, together
with the target pose p2, the ﬂow regression module ﬁrst
predicts from (p1, p2) the intrinsic 3D appearance ﬂow F
and visibility map V by Eq. (1). Then we use the tuple
(x1, p2, F, V ) for image generation. Note the input image
x1 and target pose p2 are likely misaligned spatially, there-
fore if we want to directly concatenate and feed them into
a single convolutional network to generate the target image,
we can suffer from sub-optimal results. Part of the reason
is that the convolutional layers (especially those low-level
ones) in one single network may have limited receptive ﬁeld
to capture the large spatial displacements. Some unique net-
work architecture is introduced in [23] to address this.

3695

Figure 2: Overview of our human pose transfer framework. With the input image x1, its extracted pose p1, and the target
pose p2, the goal is to render a new image in pose p2. Our ﬂow regression module ﬁrst generates the intrinsic appearance
ﬂow map F and visibility map V , which are used to warp the encoded features {ck
a} from reference image x1. Such warped
features {ck
p} can then go through a decoder Gd to produce an image ex2. This result is further
reﬁned by a pixel warping module to generate the ﬁnal result ˆx2. Our training objectives include using the PatchGAN [12]
to discriminate between (x1, p2, x2) and (x1, p2, ˆx2), as well as reconstruction and perceptual losses.

aw} and target pose features {ck

Inspired by [34, 47], we choose to use a dual-path U-
Net [29] to separately model the image and pose informa-
tion. Concretely, an appearance encoder Gea and pose en-
coder Gep are employed to encode image x1 and target pose
p2 into the feature pyramids {ck
p}. Then a feature
warping module is proposed to handle the spatial misalign-
ment issue during pose transfer. This module warps the ap-
pearance features ck
a according to our generated ﬂow map
F . Meanwhile, some potentially missing pixels in target
pose are also implicitly considered by including the visibil-
ity map V . Our feature warping function is deﬁned as:

a}, {ck

ck
aw = WF (ck

a, F, V ),

(2)

where WF is the warping operation detailed in Sec. 3.4, and
ck
aw denotes the warped features at feature level k. Then we
concatenate warped features {ck
aw} and target pose features
{ck
p} hierarchically, which are fed to the image decoder Gd
through skip connections to generate the target image ex2.
Lastly, ex2 is further enhanced by a pixel warping module
(Sec. 3.5) to obtain the ﬁnial output ˆx2.

One of our training objectives is the adversarial loss. We
adopt the PatchGAN [12] to score the realism of synthe-
sized image patches. The input patches to the PatchGAN
discriminator is either from (x1, p2, x2) or (x1, p2, ˆx2). We
found the concatenation of (x1, p2) provides good condi-
tioning for GAN training.

3.3. Flow Regression Module

Our key module for 3D appearance ﬂow regression is
shown in Fig. 3. It is a feedforward CNN that predicts the

Figure 3: Our appearance ﬂow regression module adopts
a U-Net architecture to predict the intrinsic 3D appearance
ﬂow map F and visibility map V from the given pose pair
(p1, p2). This module is jointly trained with an End-Point-
Error (EPE) loss on F and a cross-entropy loss on V .

required appearance ﬂow map F and visibility map V from
the pose pair (p1, p2). This is similar to the optical ﬂow
prediction [4, 10], but differs in that our ﬂow and visibility
maps aim to encode 3D dense correspondences not 2D ones
in optical ﬂow. For accurate prediction of these two maps,
we leverage a 3D human model to synthesize their ground-
truth for training.
Ground-truth generation.
For this purpose, we ran-
domly sample the same-person image pairs (x1, x2) from
the DeepFashion dataset [20]. We then ﬁt a 3D hu-
man model [21] to both images, using the state-of-the-art
method [15]. The 3D model represents the human body
as a mesh with 6,890 vertices and 13,766 faces. After 3D
model ﬁtting, we project them back to the 2D image plane
using an image renderer [22]. As indicated by Eq. (1), for
the projected 2D coordinate uj in image x2, we can identify
its exact belonging mesh face in 3D and hence compute the
corresponding 2D coordinate u′
j in image x1 via barycen-

3696

       Feature Warping   Flow Regression ModuleAppearance Encoder    Pose Encoder    Decoder    Intrinsic Appearance FlowPixelWarping      +Weighted Sum     Discriminator          OR                                EPE LossCross Entropy Lossappearance
feature 

visible  part

invisible  part

C

C

+

flow   

visibility   

STN

Gating

Conv

warped

feature     

Concat

Add

Figure 5: The architecture of feature warping module.

the overall pose transfer framework. At test time, our ﬂow
regression module generalizes well to the given pose p.

3.4. Flow Guided Feature Warping

Recall that our 3D appearance ﬂow and visibility maps
are generated to align the reference image to the target pose
and inpaint the invisible pixels therein. We achieve this by
warping the input image features guided by our two maps.
The architecture of our feature warping module is illustrated
in Fig. 5. The inputs are the image features ck
a (at feature
level k) and the ﬂow and visibility maps (Fk, Vk) resized to
match the ck
a dimensions. We ﬁrst warp the input features ck
a
by the ﬂow map Fk using a spatial transformer layer [13].
The warped features are then fed into a spatial gating layer,
which divides the feature maps into mutually exclusive re-
gions according to the visibility map Vk. Here we do not
simply ﬁlter out the invisible feature map pixels because
they may still contain useful information, like clothing style
or body shape. The gated feature maps are passed through
two convolutional layers with residual path to get the ﬁnal
warped features ck
aw. Our feature warping module is differ-
entiable allowing for end-to-end training.

3.5. Pixel Warping

As shown in Fig. 2, given the warped features {ck

aw}, we
concatenate them with the target pose features {ck
p} hierar-
chically. They are both fed to the image decoder Gd through

iments, we found high frequency details are sometimes lost

skip connections to render the target image ex2. In our exper-
in ex2, indicating the inefﬁciency of image warping only at
feature level. To this end, we propose to further enhance ex2

at pixel level. Similarly, a pixel warping module is adopted
to warp the pixels in input image x1 to the target pose using
our 3D appearance ﬂow.

Speciﬁcally, we warp x1 according to the full resolution
ﬂow map F to get the warped image xw. Note xw con-
tains the required image details from input x1, but may be
distorted because of the coarse ﬂow map and body occlu-
sions. Therefore, we train another U-Net to weigh between

the warped output xw and ex2 at pixel- and feature-level re-
spectively. This weighting network takes xw, ex2, F and V

as inputs and outputs a soft weighting map z with the same
resolution of xw and x2. The map z is normalized to the
range of (0, 1) with sigmoid function. Then the ﬁnal output

3697

Figure 4: Example 3D appearance ﬂow maps and visi-
bility maps: generated ground-truth (middle) and predic-
tion from our ﬂow regression module (right). The ground-
truth rendered from 3D model ﬁtting has occasional er-
rors, e.g., around the overlapping legs in the last row. While
our ﬂow regression module can correct the error by predict-
ing from the given pose.

tric transformation. The resulting ﬂow vector is computed
as fj = u′
j − uj . In addition, we can obtain the visibility
of each mesh face and thus the entire visibility map V from
the image renderer. Fig. 4 (middle) shows some examples of
the generated groundtruth ﬂow map and visibility map. One
by-product of the 2D image projection is that we can obtain
the corresponding 2D pose from the image renderer [15].

We denote such rendered pose as ep, and will elaborate its

use next.
Network architecture and training. Figure. 3 demon-
strates how to train the 3D appearance ﬂow regression mod-
ule with a U-Net architecture. It takes a pose pair (p1, p2)
as input and is trained to simultaneously predict the ﬂow
map F and visibility map V under the end-point-error
(EPE) loss and cross entropy loss, respectively. We noticed
that the 3D model ﬁtting process will sometimes cause er-
rors, e.g., when human legs are overlapped with each other,
see Fig. 4 (middle, last row). In this case, the synthesized
ﬂow and visibility maps {F, V } from image-based 3D ﬁt-
ting is not consistent with the groundtruth pose (p1, p2)
anymore. Hence it is erroneous to train the ﬂow regres-
sion from (p1, p2) to the un-matched {F, V }. Fortunately,

as mentioned before, we have pose {ep1, ep2} rendered from

the 2D projection process that leads to the corresponding
maps {F, V }. Therefore, we choose to perform regression

from the rendered pose (ep1, ep2) to {F, V }, rather than from

the potentially un-matched ground-truth pose (p1, p2). We
found such trained regressor between the rendered pose-
ﬂow pair works surprisingly well even when the 3D model
is not ﬁtted perfectly. Once our appearance ﬂow regression
module ﬁnishes training, it is frozen during the training of

ReferenceImageTargetImageFlow Map and Visibility Map(generated using 3D model)Flow Map and Visibility Map(predicted by the flow regression module)Therefore we also enforce an L1 constraint between the
generated image and the target image as:

LL1(G) = ||ˆx2 − x2||1.

(5)

Perceptual loss. The work in [14] shows that penalizing
L2-distance between feature maps extracted from two im-
ages by a pretrained CNN could encourage image structure
similarity. We adopt a VGG19 network [36] pretrained on
ImageNet [30] as the feature extractor, and use multi-level
feature maps φj to compute perceptual loss as:

Lperceptual(G) =

NX

j=1

||φj(ˆx2) − φj(x2)||2
2.

(6)

Our ﬁnal loss function for image generation is a weighted
sum of above terms:

L(G) = λ1Ladv + λ2LL1 + λ3Lperceptual.

(7)

4. Experiments

4.1. Dataset and Implementation Details

Dataset. We evaluate our method on DeepFashion dataset
(In-shop Clothes Retrieval Benchmark) [20], which con-
tains 52,712 in-shop clothes images and 200,000 cross-
pose/scale pairs. The images have a resolution of 256 × 256
pixels. Following the setting in [34], we select 89,262 pairs
for training and 12,000 pairs for testing. We perform addi-
tional experiments on Market-1501 dataset [48] and show
results in the supplementary material.
Network architecture. Our generator uses a U-Net archi-
tecture of N = 7 levels. At each feature level, the en-
coder has two cascaded residual blocks [8] followed by a
stride-2 convolution layer for downsampling, while the de-
coder has a symmetric structure of an upsampling layer fol-
lowed by two residual blocks. The upsampling layer is im-
plemented as a convolutional layer followed by pixel shuf-
ﬂing operation [32]. There are skip connections between the
corresponding residual blocks in the encoder and decoder,
and batch normalization [11] is used after each convolu-
tional layer (except the last one). Our discriminator uses
the PatchGAN [12] network with a patch size of 70 × 70
pixels.
Training. We use the Adam optimizer [16] (β1 = 0.5,β2 =
0.999) in all experiments. We adopt a batch size of 8 and
a learning rate of 2e-4 (except for the discriminator which
uses learning rate 2e-5). In our experiments we noticed that
optimizing the image generator and the pixel warping mod-
ule separately yields better performance. Therefore we ﬁrst
train the image generator for 10 epochs and freeze it af-
terwards. Then we add the pixel warping module into the
framework and train the full model for another 2 epochs. To
stabilize the training, LGAN is not used in the ﬁrst 5 epochs.

3698

Figure 6: Pixel warping examples. From left to right: the
pixel warped image xw, weighting map z, feature-warped

image ex2, ﬁnal image ˆx2 fused with pixel warping, and the

ground-truth target image x2.

x∗

2 is computed as a weighted sum of xw and ex2 as:

ˆx2 = z · xw + (1 − z) · ex2.

(3)

Figure. 6 validates the effect of pixel warping. We can
see that pixel warping is indeed able to add some high-
frequency details that can not be recovered well by our fea-
ture warping results. The added details are simply copied
from reference image using our intrinsic appearance ﬂow.

3.6. Loss Functions

The goal of our model is to achieve accurate human
pose transfer to an arbitrary pose, generating a photoreal-
istic pose-transferred image. This task is challenging due to
the large non-rigid deformation during pose transfer and the
complex details in human images. Previous works on con-
ditional image generation [12, 38] and human pose trans-
fer [23, 26, 34] utilize multiple loss functions to jointly su-
pervise the training process. In this work we similarly use a
combination of three loss functions, namely an adversarial
loss Ladv, an L1 reconstruction loss LL1, and a perceptual
loss Lperceptual. They are detailed as follows.
Adversarial loss. We adopt a vanilla GAN loss in the con-
ditional setting in our task, which is deﬁned as:

Ladv(G, D) =Ex1,x2 [logD(x2|x1, p2)]

+Ex1,x2 [log(1 − D(G(x1, p2)|x1, p2))].

(4)
L1 loss. Previous work [12] shows L1 loss can stabilize
the training process when a target groundtruth is available.

Warped Image (  )Weight Map( )Before Pixel Warping (   )After Pixel Warping (   )Target Image (  )Table 1: Comparison against previous works on DeepFash-
ion dataset. † indicates the model is unsupervised (no image
pairs used in training). ∗ indicates the results are obtained
using different data splits, thus cannot be directly compared
to ours.

Model

SSIM

IS

FashionIS

UPIS [27]†∗
DPT [26]∗
DPIG [24]†
VUnet [5]†
PG2 [23]
DSC [34]

Ours

Real Image

0.747
0.796
0.614
0.786
0.762
0.756
0.778
1.000

2.97
3.71
3.228
3.087
3.090
3.439
3.338
3.962

-
-
-
-

2.639
3.804
4.898
6.518

AttrRec-k(%)
k=5
k=20

-
-
-
-

-
-
-
-

13.560
19.017
21.065
24.780

30.193
43.812
49.044
61.626

4.2. Evaluation Metrics

Previous works use Structure Similarity (SSIM) [41] and
Inception Score (IS) [31] to evaluate the quality of gener-
ated images. We report these metrics too in our experi-
ments. However, SSIM is noticed to favor blurry images
which are less photorealistic [23]. While IS computed us-
ing a classiﬁer trained on ImageNet [30] is not suitable in
the scenario where the images have a different distribution
than ImageNet images. For these reasons, we introduce two
complementary metrics described below.
Fashion inception score. Following the deﬁnition in [31],
we calculate the inception score using a fashion item clas-
siﬁer, which we refer as FashionIS. Speciﬁcally, we ﬁne-
tune an Inception Model [37] on clothing type classiﬁcation
task on [20], which has no domain gap to the images in our
human pose transfer experiments. We argue that Fashio-
nIS can better evaluate the image quality in our experiments
compared to the original IS.
Clothing attribute retaining rate. The human pose trans-
fer model should be able to preserve the appearance details
in the reference image, like the clothing attributes like color,
texture, fabric and style. To evaluate the model performance
from this aspect, we train a clothing attribute recognition
model on DeepFashion [20] to recognize clothing attributes
from the generated images. Since the groundtruth attribute
label of the test image is available, we directly use the top-k
recall rate as the metric, denoted as AttrRec-k.

4.3. Quantitative Results

We compare our proposed method against recent works
in Table. 1. For SSIM and IS we directly use the results
reported in the original papers. We calculate their Fashio-
nIS and AttRec-k results using the images generated by the
publicly released codes and models. Note that the data splits
used in [26, 27] are different from our setting, thus these re-
sults are not directly comparable. The results show that our

Figure 7: Qualitative comparison between our method and
previous works.

proposed method outperforms others in terms of both Fash-
ionIS and AttrRec-k metrics by a signiﬁcant margin. This
proves that our method can generate more realistic images
with better preserved details. In terms of SSIM and IS, we
also achieve compelling results compared to the state-of-
the-art methods.

4.4. Qualitative Results

We further visualize some qualitative results in Fig. 7 to
show the effectiveness of our proposed method. Because of
the introduced 3D intrinsic appearance ﬂow and visibility
map, the large spatial displacements and deformations are
successfully recovered by our method during pose transfer.
We can see that our model generates realistic human image
in arbitrary poses and is able to restore detailed appearance
attributes like clothing textures.

4.5. User Study

We conduct a user study with 30 users to compare the
visual results from our method and the state-of-the-art base-
line [34]. The user study consists of two tests. The ﬁrst one
is a ”real or fake” test, following the protocol in [23, 34].

3699

ReferenceImageNIPS 2017PG2 [23]CVPR 2018DSC [33]OursTargetImageTargetPoseTable 2: User study (%) on DeepFashion. R2G indicates the
percentage of real images rated as fake, and G2R means the
opposite. ’Judged as better’ indicates the wining percentage
in the comparison test.

Model

DSC [34]

Ours

R2G
9.55
10.01

G2R
9.24
31.71

Judged as better

9.47
90.53

For each method, we show the user 55 real images and 55
fake images in an random order. Each image is shown for
1 second and user will determine whether it is real or fake.
The ﬁrst 10 images are for practice and are ignored when
computing results. The second one is a comparison test, in
which we show the user 55 image pairs, generated by our
method and baseline respectively with the same reference
image and target pose, and the user is asked to pick one im-
age with better quality from each pair. The reference image
is also shown to make the user aware of the groundtruth ap-
pearance. Similar to the ﬁrst test, the ﬁrst 5 pairs are for
practice. All samples in user study is randomly selected
from our test set and shown with full resolution. The results
in Table. 2 show that our method generates images with con-
sistently better quality than the baseline, which are confused
with real images more often by human judeges.

4.6. Ablation Study

In this section we perform ablation study to further an-
alyze the impact of each component in our model. We
ﬁrst describe the variants obtained by incrementally remov-
ing components form the full framework. All variants are
trained using the same protocol described in Sec. 4.1.
w/o. dual encoder. This is similar to PG2 [23] that has a U-
Net architecture with single encoder and no ﬂow regression
module. x1 and p2 are concatenated before being fed into
the model.
w/o. ﬂow. This model has a dual-path U-Net architecture
but without feature warping module. Appearance features
{ck
p} are directly concatenated at
corresponding level before sent into the decoder.
w/o. visibility. This model adopts dual-path U-Net gen-
erator with a simpliﬁed feature warping module, where the
gating layer and the ﬁrst convolution layer in Fig. 5 are re-
placed with a normal residual block that is unaware of the
visibility map V .

a} and pose features {ck

Table 3: Ablation study.

Model

SSIM

IS FashionIS

w/o. dual encoder

w/o. ﬂow

w/o. visibility

w/o. pixel warping

Full

0.780
0.783
0.778
0.776
0.778

3.173
3.319
3.260
3.281
3.338

3.927
4.119
4.491
4.800
4.898

AttrRec-k(%)
k=20
k=5
43.377
44.656
46.591
48.391
49.044

19.085
19.716
20.297
20.942
21.065

Figure 8: Visualization of ablation study

w/o. pixel warping. This model uses the full generators in
Fig. 2 without pixel warping module.
Full. This is the full framework as shown in Fig. 2.

Table. 3 and Fig. 8 show the quantitative and qualitative
results of the ablation study. We can observe that all mod-
els perform well on generating correct body poses, realis-
tic faces and plausible color style, which yield high SSIM
scores. However, our proposed ﬂow guided feature warping
signiﬁcantly improves the capability of preserving detailed
appearance attributes like clothing layout and complex tex-
tures, which also leads a large increase of FashionIS and
AttrRec-k. The pixel warping module further helps to han-
dle some special clothing patterns that are not well recon-
structed by the convolutional generator.

5. Conclusion

In this paper we propose a new human pose transfer
method with implicit reasoning about 3D geometry of hu-
man body. We generate the intrinsic appearance ﬂow map
and visibility map leveraging the 3D human model, so as to
learn how to move pixels and hallucinate invisible ones in
the target pose. A feedforward neural network is trained to
rapidly predict both maps, which are used to warp and gate
image features respectively for high-ﬁdelity image genera-
tion. Both qualitative and quantitative results on the Deep-
Fashion dataset show that our method is able to synthesize
human images in arbitrary pose with realistic details and
preserved attributes. Our approach also signiﬁcantly out-
performs existing pose- or keypoint-based image generators
and other alternatives.
Acknowledgement: This work is supported by Sense-
Time Group Limited, the General Research Fund sponsored
by the Research Grants Council of the Hong Kong SAR
(CUHK 14241716, 14224316. 14209217), and Singapore
MOE AcRF Tier 1 (M4012082.020).

3700

Reference ImageTarget ImageFullw/o. pixel warpingw/o. visibilityw/o. floww/o. dual encoderReferences

[1] Guha Balakrishnan, Amy Zhao, Adrian V Dalca, Fredo Du-
rand, and John Guttag. Synthesizing images of humans in
unseen poses. In CVPR, 2018. 1

[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In CVPR, 2017. 3

[3] Caroline Chan, Shiry Ginosar, Tinghui Zhou,

and
arXiv preprint

Alexei A Efros. Everybody dance now.
arXiv:1808.07371, 2018. 3

[4] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical ﬂow with convolutional networks. In ICCV,
2015. 4

[5] Patrick Esser, Ekaterina Sutter, and Bj¨orn Ommer. A varia-
tional u-net for conditional appearance and shape generation.
In CVPR, 2018. 1, 3, 7

[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
1, 2

[7] Riza Alp G¨uler, Natalia Neverova, and Iasonas Kokkinos.
In

Densepose: Dense human pose estimation in the wild.
CVPR, 2018. 1, 3

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In ECCV,

Identity mappings in deep residual networks.
2016. 6

[9] Berthold KP Horn and Brian G Schunck. Determining opti-

cal ﬂow. Artiﬁcial intelligence, 17(1-3):185–203, 1981. 3

[10] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-
tion of optical ﬂow estimation with deep networks. In CVPR,
2017. 4

[11] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. arXiv preprint arXiv:1502.03167, 2015. 6

[12] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, 2017. 2, 4, 6

[13] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.

Spatial transformer networks. In NIPS, 2015. 5

[14] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
In

losses for real-time style transfer and super-resolution.
ECCV, 2016. 6

[15] Angjoo Kanazawa, Michael J Black, David W Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In CVPR, 2018. 4, 5

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014. 6

[17] Diederik P Kingma and Max Welling. Auto-encoding varia-

tional bayes. arXiv preprint arXiv:1312.6114, 2013. 1, 2

[19] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In CVPR, 2017. 2

[20] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou
Tang. Deepfashion: Powering robust clothes recognition and
retrieval with rich annotations. In CVPR, 2016. 2, 4, 6, 7

[21] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. ACM TOG, 34(6):248, 2015. 3, 4

[22] Matthew M Loper and Michael J Black. Opendr: An approx-

imate differentiable renderer. In ECCV, 2014. 4

[23] Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuyte-
laars, and Luc Van Gool. Pose guided person image genera-
tion. In NIPS, 2017. 1, 3, 6, 7, 8

[24] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc
Van Gool, Bernt Schiele, and Mario Fritz. Disentangled per-
son image generation. In CVPR, 2018. 1, 3, 7

[25] Mehdi Mirza and Simon Osindero. Conditional generative

adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 2

[26] Natalia Neverova, Rıza Alp G¨uler, and Iasonas Kokkinos.

Dense pose transfer. In ECCV, 2018. 1, 3, 6, 7

[27] Albert Pumarola, Antonio Agudo, Alberto Sanfeliu, and
Francesc Moreno-Noguer. Unsupervised person image syn-
thesis in arbitrary poses. In CVPR, 2018. 1, 3, 7

[28] Amit Raj, Patsorn Sangkloy, Huiwen Chang, James Hays,
Duygu Ceylan, and Jingwan Lu. Swapnet: Image based gar-
ment transfer. In ECCV, 2018. 1, 3

[29] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 2, 4

[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Imagenet large
Aditya Khosla, Michael Bernstein, et al.
scale visual recognition challenge.
IJCV, 115(3):211–252,
2015. 6, 7

[31] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In NIPS, 2016. 7

[32] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In
CVPR, 2016. 6

[33] Raquel Urtasun Dahua Lin Chen Change Loy Shizhan Zhu,
Sanja Fidler. Be your own prada: Fashion synthesis with
structural coherence. In ICCV, 2017. 2

[34] Aliaksandr Siarohin, Enver Sangineto, St´ephane Lathuili`ere,
and Nicu Sebe. Deformable gans for pose-based human im-
age generation. In CVPR, 2018. 1, 3, 4, 6, 7, 8

[35] Karen Simonyan and Andrew Zisserman. Two-stream con-
In

volutional networks for action recognition in videos.
NIPS, 2014. 3

[18] Christoph Lassner, Gerard Pons-Moll, and Peter V Gehler.
A generative model of people in clothing. In ICCV, 2017. 1,
3

[36] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 6

3701

[37] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, 2016. 7

[38] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In
CVPR, 2018. 6

[39] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.
Recovering realistic texture in image super-resolution by
deep spatial feature transform. In CVPR, 2018. 2

[40] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,
Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-
hanced super-resolution generative adversarial networks. In
ECCV, 2018. 2

[41] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing,
13(4):600–612, 2004. 7

[42] Varun Agrawal Amit Raj Jingwan Lu Chen Fang Fisher
Yu James Hays Wenqi Xian, Patsorn Sangkloy. Texture-
gan: Controlling deep image synthesis with texture patches.
CVPR, 2018. 2

[43] Andrei Zanﬁr, Elisabeta Marinoiu, and Cristian Sminchis-
escu. Monocular 3d pose and shape estimation of multiple
people in natural scenes–the importance of multiple scene
constraints. In CVPR, 2018. 3

[44] Mihai Zanﬁr, Alin-Ionut Popa, Andrei Zanﬁr, and Cristian
Sminchisescu. Human appearance transfer. In CVPR, 2018.
1, 3

[45] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xi-
aogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stack-
gan++: Realistic image synthesis with stacked generative ad-
versarial networks. arXiv preprint arXiv:1710.10916, 2017.
2

[46] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In ICCV, 2017. 2

[47] Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Zequn Jie, and
Jiashi Feng. Multi-view image generation from a single-
view. In ACMMM, 2018. 1, 3, 4

[48] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiﬁcation:
A benchmark. In ICCV, 2015. 2, 6

[49] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Ma-
lik, and Alexei A Efros. View synthesis by appearance ﬂow.
In ECCV, 2016. 3

3702

