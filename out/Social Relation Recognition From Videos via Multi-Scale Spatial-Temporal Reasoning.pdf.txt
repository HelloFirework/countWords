Social Relation Recognition from Videos via Multi-scale

Spatial-Temporal Reasoning

Xinchen Liu†, Wu Liu†, Meng Zhang†, Jingwen Chen§, Lianli Gao¶, Chenggang Yan§, and Tao Mei†

†JD AI Research, Beijing, China

§Hangzhou Dianzi University, Hangzhou, China

¶University of Electronic Science and Technology of China, Chengdu, China

{liuxinchen1, zhangmeng1208}@jd.com, liuwu@live.cn

{jingwenchen, cgyan}@hdu.edu.cn, lianli.gao@uestc.edu.cn, tmei@live.com

Abstract

Discovering social relations, e.g., kinship, friendship,
etc., from visual contents can make machines better inter-
pret the behaviors and emotions of human beings. Existing
studies mainly focus on recognizing social relations from
still images while neglecting another important media—
video. On the one hand, the actions and storylines in videos
provide more important cues for social relation recogni-
tion. On the other hand, the key persons may appear at
arbitrary spatial-temporal locations, even not in one same
image from beginning to the end. To overcome these chal-
lenges, we propose a Multi-scale Spatial-Temporal Reason-
ing (MSTR) framework to recognize social relations from
videos. For the spatial representation, we not only adopt a
temporal segment network to learn global action and scene
information, but also design a Triple Graphs model to cap-
ture visual relations between persons and objects. For the
temporal domain, we propose a Pyramid Graph Convolu-
tional Network to perform temporal reasoning with multi-
scale receptive ﬁelds, which can obtain both long-term and
short-term storylines in videos. By this means, MSTR can
comprehensively explore the multi-scale actions and story-
lines in spatial-temporal dimensions for social relation rea-
soning in videos. Extensive experiments on a new large-
scale Video Social Relation dataset demonstrate the effec-
tiveness of the proposed framework. Our dataset is avail-
able on https://lxc86739795.github.io.

1. Introduction

Social relation is the close association between multiple
individual persons and forms the basic structure of our soci-
ety. Recognizing social relations from images or videos can
empower machines to better understand the behaviors or e-
motions of human beings. However, compared to image-

Figure 1. How do we recognize colleagues or couples from a
video? The appearance of persons, interactions between person-
s, and the scenes with contextual objects are key cues for social
relation recognition.

based social relation recognition [32], the video-based sce-
nario is an important but frontier topic which is often ne-
glected by the community. It has many potential applica-
tions such as family video search on mobile phones [27]
and product recommendation to groups of customers in s-
tores [21].

Sociological analytics from visual content has been a
popular area during the last decade [27, 31]. Existing social
recognition studies mainly focus on image-based condition,
in which algorithms recognize social relations between per-
sons in a single image. Appearance and face attributes of
persons and contextual objects are explored to distinguish
different social relations [26, 30, 33]. Although discovering
social networks [5], communities [6], roles [23, 24], and

13566

CoupleWoman AMan AHatBagGrassMan AWoman AHatTreeMowerWoman AMan AHatGrassStandingSmilingHuggingOutdoorsColleagueWoman AMan AWoman BDeskChairDoorDocumentCupMan AWoman BDoorWoman AMan AWoman BDoorDocumentDocumentStandingTalkingWritingMeeting Roomgroup activity [1, 2] in videos or movies has been widely
studied, explicit recognition of social relations from video
clips attract far less attention. Recent methods only con-
sider video-based social relation recognition as a general
video classiﬁcation task [8], which take RGB frames, op-
tical ﬂows, or audio of a video as the input and categorize
video clips into pre-deﬁned types [20]. However, such a
general model is obviously over-simpliﬁed, which neglect-
s the appearance of persons, interactions between persons,
and the scenes with contextual objects as shown in Figure 1.
Social relation recognition from videos faces unique
challenges. Firstly, compared to social community discov-
ery, social relations are more ﬁne-grained and ambiguous in
different scenes. The models must discriminate very similar
social relations such as friends and colleagues through vi-
sual contents, which might be very difﬁcult even for human
beings. Moreover, in contrast to image-based social relation
recognition, persons and objects could appear in arbitrary
frames or even separate frames. This makes persons and
objects extremely varied in sequential frames. Therefore,
image-based methods cannot be directly adopted for video-
based scenarios. Furthermore, videos provide dynamics of
persons or objects in the temporal domain than still images.
However, the location and duration of a key action for dis-
criminating a social relation are uncertain in a video. Mod-
eling the latent correlation between the varied dynamics of
persons and social relations remains great challenging.

To this end, we propose a Multi-scale Spatial-Temporal
Reasoning (MSTR) framework for social relation recogni-
tion from videos. The multi-scale reasoning is two-fold. In
the spatial domain, we consider both global cues and se-
mantic regions like persons and contextual objects. In par-
ticular, we adopt Temporal Segment Network (TSN) [28]
to learn global features from scenes and backgrounds in the
full frames. Moreover, we also design a Triple Graphs mod-
el to represent the visual relations between persons and ob-
jects. The multi-scale spatial features can provide comple-
mentary visual information for social relation recognition.
For the temporal domain, we propose a Pyramid Graph
Convolution Network (PGCN) to perform temporal reason-
ing on the Triple Graphs. Speciﬁcally, we apply multi-scale
receptive ﬁelds in the graph convolution block, which can
capture temporal features from both long-term and short-
term dynamics in videos. Finally, our MSTR framework
achieves social relation recognition by spatial-temporal rea-
soning from comprehensive information in videos.

In summary, the contributions of this paper include:

• We propose a Multi-scale Spatial-Temporal Reason-
ing framework to recognize social relations from
videos with global and local information in the spatial-
temporal domain.

• We design a novel Triple Graphs model to represent

visual relations of persons and objects. By combining
with TSN for global features, our framework can learn
multi-scale spatial features from video frames.

• To effectively capture both the long-term and short-
term temporal cues in videos, we propose a PGC-
N which performs relation reasoning with multi-scale
temporal receptive ﬁelds.

In addition, to validate our framework and facilitate re-
search, we build a large-scale Video Social Relation dataset,
named ViSR. It not only contains over 8,000 video clip-
s labeled with eight common social relations in daily life,
but also has diverse scenes, environments and backgrounds.
Extensive experiments on the ViSR dataset show the effec-
tiveness of the proposed framework.

2. Related Work

Social relation discovery in visual content. The in-
terdisciplinary study of sociology and computer vision has
been a popular area in the last decade [5, 25, 26, 27]. The
main topics include social networks discovery [6, 31], key
actors detection [23, 24], multi-person tracking [1], and
group activity recognition [2].

In recent years, explicit recognizing social recognition
from visual content has attracted attention from researcher-
s [12, 26, 30, 32]. Existing methods are mainly focused on
still images. For example, Zhang et al. proposed to learn
social relation traits from face images by a Convolutional
Neural Network (CNN) [32]. Sun et al. proposed a social
relation dataset based on the social domain theory [3] and
adopted a CNN to recognize social relations from a group of
semantic attributes [26]. Li et al. proposed to a dual-glance
model for social relationship recognition, where the ﬁrst
glance focused persons of interest and the second glance ap-
plied attention mechanism to discover contextual cues. [12].
Wang et al. proposed to represent the persons and objects
in an image as a graph and perform social relation reason-
ing by a Gated Graph Neural Network [30]. For the video-
based condition, social relation recognition is only consid-
ered as a video classiﬁcation task. For example, Lv et al.
exploited the Temporal Segment Networks [28] to classify a
video using the RGB frames, optical ﬂows, and audio of the
video [20]. They also built a Social Relation In Video (S-
RIV) dataset which contained about 3,000 video clips with
multi-label annotation. However, this method only consid-
ered global and coarse features while neglecting the person-
s, objects, and scenes in videos. Therefore, we propose to
embed the spatial and temporal features of persons and ob-
jects into a Triple Graphs model, on which social relation
reasoning is performed.

Graph model in computer vision. In the computer vi-
sion ﬁeld, pixels, regions, concepts, and prior knowledge
can be represented as graphs to model their relations for

23567

Figure 2. The overall framework of the Multi-scale Spatial-Temporal Reasoning framework.

different tasks such as object shape detection [14], image
segmentation [7], image retrieval [17], vehicle search [19],
etc. In recent years, researchers of machine learning have
studied message propagation in graphs by end-to-end train-
able networks, such as Graph Convolutional Networks (GC-
N) [4, 11]. Most recently, these models have been adopt-
ed to computer vision tasks [13, 22, 29, 30]. For example,
Liang et al. proposed a Graph Long Short-Term Memo-
ry to propagate information in graphs built on super-pixels
for semantic object parsing [13]. Qi et al. proposed a 3D
Graph Neural Network to build a k-nearest neighbor graph
on 3D point cloud and predict the semantic class of each
pixel for RGBD data [22]. Wang et al. proposed to rep-
resent a video as a space-time region graph by the persons
and objects in videos and adopted a GCN to learn video-
level features for action recognition [29]. Inspired by above
studies, we propose to represent the actions and interactions
of persons and objects in videos as graphs, on which reason-
ing is performed by a novel pyramid graph convolutional
network for social relation recognition.

3. The Proposed Framework

3.1. Overview

The overall architecture of the Multi-scale Spatial-
Temporal framework mainly contains two parts as shown
in Figure 2. The ﬁrst part is the construction of the Triple
Graphs structure. The framework takes as input one video
clip which is sampled into F frames for efﬁciency. To cap-
ture local details from regions of interest, the persons and
objects are ﬁrst cropped from frames with Mask R-CNN [9]
pre-trained on MS-COCO dataset [15]. To model the spa-
tial and temporal representation of persons and objects, we
build an Intra-Person Graph (IntraG) for the same person,

an Inter-Person Graph (InterG) for different persons, and a
Person-Object Graph (POG) to capture the co-existence of
persons and contextual objects. The ResNet [10] is adopted
to extract the spatial features of persons and objects. The
second module adopts the PGCN to perform relation rea-
soning by message propagation in each graph. In PGCN,
multi-scale temporal receptive ﬁelds are explored to learn
dynamics in varied temporal range. The node-level features
are fused into a normalized graph-level representation for
each graph. Besides, a global video classiﬁcation network,
such as TSN [28] or T-C3D [16], is exploited to learn global
features by taking as input of full frames. Finally, the social
relation in a video is predicted by integrating the global fea-
ture from TSN and the reasoning feature from the PGCN.
Next, we will present the details of the construction of the
Triple Graphs model and relation reasoning by PGCN.

3.2. Triple Graphs Model

We can recognize the social relationships in videos by
observing the actions of persons, the interactions between
persons, and the co-existence of persons and contextual ob-
jects in the scenes, as shown in Figure 1. Graph mod-
el has been found effective to represent the spatial, tem-
poral, conceptual, or similarity relations of objects in vi-
sual content [17, 18, 29]. Therefore, we design a Triple
Graphs model, which includes three types of graphs, to
model the visual relations of persons and objects for so-
cial relation reasoning in videos, as shown in Figure 2.
To build the Triple Graphs, we ﬁrst detect bounding box-
es of persons and objects as P = {p1, p2, ..., pN } and
O = {o1, o2, ..., oM } from the sampled F frames by Mask
R-CNN [9]. To balance the accuracy and efﬁciency, we re-
main ﬁxed N persons and M objects for each video by con-
ﬁdence scores. The feature of each bounding box is extract-

33568

Video ClipSampled FramesTriple GraphsIntraGInterGPOGPyramidGraphConvolutionalNetworksPyramidGraphConvolutionalNetworksTSNVideo-level Feature by Pooling over  NodesWeighted ConsensusClassificationColleagueed by the backbone network f (·), i.e., ResNet [10]. These
bounding boxes are adopted as the nodes to build the graph-
s, while the features of each node will be utilized in graph
convolution for social relation reasoning.

Intra-Person Graph. We model the appearance vari-
ance of the same person through the video by the Intra-
Person Graph (IntraG). The IntraG is represented by an ad-
jacent matrix As ∈ RNp×NP , in which the indexes of rows
and columns correspond to the temporal order of bounding
boxes in the video. To match the same person in different
frames, we measure the visual similarity between each pair
of persons, (pi, pj), in two neighboring frames. Therefore,
the adjacent matrix As is ﬁlled by:

As(pi, pj) = (cid:26) 1

0

dist(pi, pj) < τ ,
othersise,

(1)

where dist(pi, pj) = 1 − f (pi)T f (pj )
tance of pi and pj , τ is a hyper-parameter.

||f (pi)||·||f (pj )|| is the cosine dis-

Inter-Person Graph. To capture the interactions be-
tween different persons in videos, we build the Inter-Person
Graph (InterG) by estimating the distances of persons in one
frame and its neighboring frame. For the adjacent matrix of
InterG, Ad ∈ RNp×NP , we directly set Ad(pi, pj) = 1, if
pi and pj are two persons in one frame. For pi and pj in
neighboring frames, we set

Ad(pi, pj) = (cid:26) 1

0

dist(pi, pj) >= τ ,
othersise,

(2)

where dist(pi, pj) is also the cosine distance of pi and pj .
Person-Object Graph. The contextual objects in the
scene are vital information for social relation recognition.
However, due to the shot changes, the persons and contex-
tual objects may become varied in different frames, which
makes it difﬁcult to capture the interactions between per-
sons and contextual objects through one video. Therefore,
different from IntraG and InterG for persons, the Person-
Object Graph (POG) is designed to model the co-existence
of persons and contextual objects. The adjacent matrix of
POG, Ao ∈ R(Np+No)×(NP +No), represents the relation
between each person and the objects that exist in the one
frame. Therefore, we set Ao(pk, ol) = 1, if pk and ol are
from the same frame, and Ao(pk, ol) = 0, otherwise.

To this end, the Triple Graphs are built to represent the
visual relations of persons and objects in videos, i.e., the
appearance and actions of each person, the interactions be-
tween different person, and the co-existence of persons and
objects. In particular, the indexes of adjacent matrixes cor-
respond to the temporal order of bounding boxes in the
video, by which the temporal information is implicitly em-
bedded in the graphs. Next we present how to perform so-
cial relation reasoning from visual features embedded in the
graphs by the Pyramid Graph Convolutional Network.

Figure 3. Pyramid Graph Convolution Block with multi-scale re-
ceptive ﬁelds in the temporal domain. Here we use A to represent
the normalized adjacent matrix ˜D−

2 for simplicity.

2 ˜A ˜D−

1

1

3.3. Reasoning by Pyramid GCN

Graph Convolutional Network. Traditional Convolu-
tional Neural Networks usually apply 2-D or 3-D ﬁlters on
images or videos to abstract visual features from low-level
space to high-level space [10]. In contrast, Graph Convo-
lutional Network (GCN) performs relational reasoning by
performing message propagation from nodes to its neigh-
bors in the graphs [11]. Therefore, we can apply GCNs on
the Triple Graphs to achieve social relation reasoning.

As in [11], given a graph with N nodes in which each n-
ode has a d-length feature vector, the operation of one graph
convolution layer can be formulated as:

X (l+1) = σ( ˜D− 1

2 ˜A ˜D− 1

2 X (l)W (l)),

(3)

where ˜A ∈ RN ×N is the adjacent matrix of the graph,
˜D ∈ RN ×N is the degree matrix of ˜A, X (l) ∈ RN ×d
is the output of the (l − 1)-th layer, W (l) ∈ Rd×d′
is
the learned parameters, and σ(·) is a non-linear activation
function like ReLU. In particular, in our social relation
reasoning framework, the adjacent matrixes of the Triple
Graphs are As, Ad, and Ao as deﬁned in Section 3.2. The
indexes of adjacent matrixes are arranged by the tempo-
ral order of the nodes in a video, by which the tempo-
ral information is implicitly embedded in the built graph-
s. X (0) = [f (x1), f (x2), ..., f (xN )]T is the initial feature
matrix, where f (xi) is the column vector extracted from
the nodes {xi}N like persons or objects in videos. The ﬁnal
outputs of the GCNs are updated features of nodes, X (L),
in the graphs, which can be aggregated into a video level
feature vector for social relation prediction.

Pyramid Graph Convolutional Network. GCN per-

43569

W(l)1W(l)2Scale 1Scale 2Scale k...W(l)kAdjacent Matrix ANodeFeatures X(l)Network Parameters W(l)NodeFeatures X(l+1)××××××××Stride S H WTable 1. The descriptions of social relations in the ViSR dataset based on the domain theory [3].

Domain
Attachment
Mating

Hierarchical power

Reciprocity

Coalitional groups

Relation
Parent-offspring
Couple
Leader-subordinate
Service
Sibling
Friend
Colleague
Opponent

Examples

Parent-child, Grandparent-grandchild
Husband-wife, boyfriend-girlfriend

Teacher-student, team leader-member

Passenger-driver, Customer-waiter

Brothers, sisters

Friends in general scenes

Co-worker, school mate, teammate

Enemy, competitor, disputant

forms operations on all nodes in one graph together as well
as the full temporal range of a video, which means GCN
can capture a global view in the temporal domain. Howev-
er, the key factor for social relation recognition such as a
speciﬁc action of a person may appear in local temporal po-
sition which may be overwhelmed by unimportant informa-
tion. Therefore, we design a Pyramid Graph Convolutional
Network (PGCN) to learn both long-term and short-term in-
formation by a pyramid of temporal receptive ﬁelds.

Figure 3 illustrates the structure of one pyramid graph
convolution block in PGCN. Each block contains multiple
parallel branches with different receptive ﬁelds. Scale 1 is
the standard GCN which performs graph convolution on the
whole adjacent matrix and covers all nodes in the graph. S-
cale 2 gives an example of graph convolution with a smaller
temporal receptive ﬁeld, while Scale K is a more gener-
al illustration. For each scale, the activations of all sliding
windows are aggregated into one feature matrix which has
the same shape with the output of the standard GCN. By
sliding the receptive ﬁeld along the diagonal of the adja-
cent matrix, the model can learn the relatively short-term
features from the start to the end of a video. At last, the
outputs of multiple scales are merged by average pooling to
generate the feature matrix, X (l+1), for the next PGCN lay-
er. The pyramid graph convolution block is end-to-end dif-
ferentiable and can be inserted into other video-based GCN
models for action recognition or video classiﬁcation [29].

2 × N

In our implementation, we stack two pyramid graph con-
volution layers of which the scales of the parameter matrix
W (l) are 2048 × 512 and 512 × 128. In each pyramid graph
convolution block, we adopt two scales of ﬁlters. The ﬁrst
scale has N ×N ﬁlter, while the second scale has N
2 ﬁl-
ter and stride S = N
4 . After forward propagation of PGCN,
the ﬁnal feature matrix X (L) ∈ RN ×128 is aggregated into
a 128-D video-level feature vector. The video-level feature
is fed into a fully connected layer to classify the video in-
to one social relation class. In our framework, the pyramid
temporal reasoning is performed by PGCNs on IntraG, In-
terG, and POG separately. The three branches generate a
weighted consensus after the softmax layers in each branch.
Moreover, to learn more global visual information about
the scenes, environments, and backgrounds, we adopt the

TSN [28] to directly take as input all sampled frames from
a video. At last, the scores of PGCN and TSN are combined
by weighted fusion for the ﬁnal prediction.

4. Experiments

4.1. The ViSR Dataset

Existing datasets for social relation recognition are main-
ly based on still images [12, 26, 32]. The social relation-
s of these datasets are deﬁned by different psychological
or sociological theories. For example, the social relation
dataset in [32] is mainly focused on psychological or emo-
tional traits. Therefore, the images in this dataset are anno-
tated with attributes of faces like expressions. The People
in Photo Album (PIPA) dataset [26] and People in Social
Context (PISC) [12] dataset are both deﬁned on sociolog-
ical theories. The labels of PIPA are based on the social
domain theory [3], in which social life is partitioned into
ﬁve domains and 16 social relations. The PISC dataset con-
tains several common social relations in daily life, which
have a hierarchy of three coarse-level relationships and six
ﬁne-level relationships.

However, video-based dataset labeled with explicit so-
cial relations is rare. One of the largest is the Social Re-
lation in Video (SRIV) dataset which contains about 3000
video clips collected from 69 movies [20].
It is annotat-
ed with eight subjective relations which are similar to the
social relation traits in [32], and eight objective relations
which are derived from the domain-based relations in [3].
There are three main limitations in SRIV: 1) the volume of
the dataset is relatively small for the scalability of the mod-
els especially for CNN; 2) the videos are labeled by multiple
labels, which makes the relation in a video ambiguous; 3)
the social relations are very unbalanced especially for the
objective relations.

To facilitate related research and validate our proposed
framework, we build a large-scale and high-quality Video
based Social Relation dataset, dubbed as ViSR. For our
dataset, we deﬁne eight types of social relation derived from
the domain-based theory [3], as listed in Table 1. The con-
struction process contains three main steps: 1) We ﬁrst col-
lect more than 200 movies which have a wide variety of
types such as adventure, family, comedy, drama, crime, ro-

53570

Figure 4. The statistics of video clip length in ViSR dataset.

Figure 5. The distribution of social relations in the ViSR dataset.

mance, action, biography but exclude surreal types like fan-
tasy and Sci-Fi. 2) We then ask ten annotators to segment
video clips from the movies. The length of each clip is lim-
ited in 10 ∼ 30 seconds. At least two persons that have
interactions must exist in one clip. The scene in one clip
should be ﬁxed. By this means, we obtain about 10,000
candidate video clips for annotation. 3) At last, each can-
didate video clip is labeled by at least ﬁve annotators by
maximum voting to guarantee the quality. The clip will be
discarded if all its labels are less than three votes.

Through elaborate annotation, the ViSR has several fea-
tured properties. First of all, the dataset contains more than
8,000 valid video clips, which can make the algorithms
more scalable than existing datasets. Moreover, due to the
variety of source movies, our dataset not only covers most
common social relations in daily life with balanced class
distribution as shown in Figure 4, but also contains various
scenes, environments, and backgrounds, which makes ViS-
R a challenging dataset. Furthermore, as shown in Figure 5,
the length of most clips is limited in 30 seconds to keep
the stable scenes, which reduces the ambiguity of relation-
s in videos. Figure 6 shows some examples of video clips
in our dataset. In the experiments, we randomly split the
dataset into training, validation, and testing subsets by the
ratio 7 : 1 : 2. The top-1 accuracy on each relation class
and the mean Average Precision (mAP) over all classes are
calculated to evaluate the performance of methods.

4.2. Implementation Details

This section presents the details on the construction of

Triple Graphs and training strategy of the networks.

Figure 6. Some examples of videos in the ViSR dataset.

Triple Graphs Building. The Triple Graphs model is
built as in Section 3.2. We uniformly partition an input
video into 20 segments, in which one frame is randomly
sampled to obtain 20 frames for one video. From the sam-
pled frames, we adopt Mask R-CNN to obtain at most 40
bounding boxes of persons and 20 bounding boxes of ob-
jects. For construction of IntraG and InterG, the person
similarity threshold τ in Equ. 1 and Equ. 2 are set to 0.2.

Networks Training. In our framework, the PGCN and
TSN are trained separately.
In each pyramid graph con-
volution After the construction of the Triple Graphs, three
PGCNs of IntraG, InterG, and POG are pre-trained on the
training set separately with the learning rate lr = 0.01. Af-
ter 30 epochs, the three PGCNs are trained together for 120
epochs in which the learning rate starts from 0.001 and mul-
tiply 0.1 by every 30 epochs. The TSN is trained by the
standard strategy as in [28]. The segment number is set to
20. The base learning rate is 0.001 and multiplies 0.1 by
every 20 epochs until 80 epochs. For testing, the fusion
weights for the results of PGCN and TSN are 0.6 and 0.4,
respectively.

4.3. Comparison with the State of the art Methods

To validate the effectiveness of the proposed Pyramid
Temporal Reasoning framework, we compare it with sever-
al state-of-the-art methods on the ViSR dataset. The details
of methods are as follows:

63571

  00.050.10.150.20.25RatioTypesLeader-subordinateColleagueServiceParent-offspringSiblingCoupleFriendOpponentTable 2. Comparison to the state-of-the-art methods.

Top-1 Accuracy

.

b
u
S
-
r
e
d
a
e
L

e
u
g
a
e
l
l
o
C

GRM [30]
TSN-Spatial [20]
TSN-ST [20]
GCN
PGCN
MSRT

48.67

55.48
41.05
56.16
54.11
57.53

42.93
33.33
49.46
54.89
51.09

.
s
f
f
o
-
t
n
e
r
a
P

g
n
i
l
b
i
S

0.00

35.20
32.83
36.80
40.80
45.60

34.83
45.78
41.57
34.83
39.33

e
c
i
v
r
e
S

6.67
30.00
30.00
27.14
25.71
30.00

e
l
p
u
o
C

4.17
39.78
29.17
34.41
33.33
38.71

d
n
e
i
r
F

0.67
48.75
63.76
39.80
45.27
53.23

t
n
e
n
o
p
p
O

30.13
37.07
32.87
50.00
48.28
47.41

P
A
m

16.69
42.38
43.23
43.46
44.73
47.75

Table 3. Ablation study on the proposed framework.

Module

Top-1 Accuracy

G
a
r
t
n
I

G
r
e
t
n
I

G
O
P

.

b
u
S
-
r
e
d
a
e
L

X

X X

49.32
52.74
X X X 56.16
X
52.74
53.42
X X X 54.11

X X

e
u
g
a
e
l
l
o
C

44.57
48.91
49.46
52.17
51.63
54.89

e
c
i
v
r
e
S

25.71
25.71
27.14
25.71
27.14
25.71

.
s
f
f
o
-
t
n
e
r
a
P

38.40
38.40
36.80
45.60
43.20
40.80

g
n
i
l
b
i
S

38.20
42.70
41.57
40.45
38.20
34.83

e
l
p
u
o
C

26.88
29.03
34.41
34.41
39.78
33.33

d
n
e
i
r
F

44.78
42.29
39.80
38.81
41.29
45.27

t
n
e
n
o
p
p
O

43.97
44.83
50.00
40.52
40.52
48.28

P
A
m

41.02
42.48
43.46
43.07
43.65
44.73

GCN

PGCN

1) Temporal Segment Network using Spatial features
(TSN-Spatial) [20]. This method uses only the RGB
frames of videos as the input and adopts TSN to learn s-
patial features for social relation recognition. We use the
parameters and training strategy as in [20]. We modify the
original multi-label classiﬁcation setting on their dataset to
single-label classiﬁcation task for our dataset.

2) Temporal Segment Network using Spatial-
Temporal features (TSN-ST) [20]. This method uses the
same framework with TSN-Spatial except that the optical
ﬂow is also taken as the input of TSN to learn both spatial
and temporal features from videos. The implementation
is the same as that in [20]. Because this paper is mainly
focused on vision based methods, we do not use any audio
information as in [20]. Therefore we consider this model as
the state-of-the-art method on the SRIV dataset.

3) Graph Reasoning Model (GRM) [30]. This is the
state-of-the model for image-based social relation recogni-
tion on two public datasets, i.e., PIPA [26] and PISC [12].
We apply GRM on each frame in a video. The results on
all sampled frames are integrated by late fusion for video-
based social relation prediction.

4) Graph Convolution Networks (GCN). In this model,
we only adopt the standard GCN to perform reasoning on
the Triple Graphs.

5) Pyramid Graph Convolution Networks (PGCN). In

this model, we insert the temporal pyramid branches into
each graph convolutional layer in GCNs.

6) Multi-scale Spatial-Temporal Reasoning (MSTR).
This is the complete Pyramid Temporal Reasoning frame-
work, which adopts PGCN to learn multi-scale dynamics
of persons from the Triple Graphs and TSN to learn glob-
al spatial features. Finally, the social relation reasoning is
achieved by weighted fusion of PGCN and TSN.

The results of these methods are listed in Table 2. We
ﬁrst ﬁnd that the image-based method, GRM, obtains poor
results on the video-based dataset. The reason is that image-
based methods require the co-existence of two or more per-
sons in one image, while in the video base condition there
may be only one person in a frame. Therefore image-based
method cannot be directly adopted to the video-based sce-
nario. Moreover, by comparison of global feature based
models, i.e., TSN-Spatial and TSN-ST, and graph-based
methods, i.e., GCN and PGCN, we can ﬁnd that global in-
formation and local regions are both effective for social re-
lation recognition. Overall GCN and PGCN are better, be-
cause the detailed appearance and actions of persons and
objects can provide more signiﬁcant features for social re-
lations. Furthermore, the combination of TSN and PGC-
N, i.e., MSTR obtain the best performance, which demon-
strates the complementary effect of multi-scale spatial and
temporal representation.

73572

Figure 7. The results under different τ for building graphs.

Figure 9. The normalized fusion matrix of the MSTR framework.

Figure 8. The results of different frame numbers for MSTR.

4.5. Discussion

4.4. Ablation Study

Signiﬁcance of Triple Graphs Here we explore the ef-
fect of each graph and the pyramid graph convolution block
in the PGCN. Table 3 lists the results of models with dif-
ferent graph combinations for both GCN and PGCN. From
the results, we can ﬁnd that the overall accuracy of PGC-
N is higher than that of GCN, which demonstrates that the
multi-scale receptive ﬁelds can capture useful features from
long-term and short-term ranges. Moreover, for each net-
work architecture, the mAP increases by incorporating In-
traG, InterG, and POG. This validates the signiﬁcance of ac-
tions, interactions between persons, and co-existence of per-
sons and contextual objects for social relation recognition.
We also observe that three graphs show different effects on
different social relations. For examples, the POG brings
signiﬁcant boost on work relations, i.e., leader-subordinate
and colleague. This reﬂects the importance of contextual
objects in work scenes like ofﬁce or meeting room.

Analysis on Hyper Parameters We explore the impact
of two hyper parameters, i.e., the sampled frame number
F and threshold τ in Section 3.2. We ﬁrst set τ = 0.1 to
0.2 for graph construction in both GCN and PGCN. The
results are shown in Figure 7. The curves are stable un-
der different τ , which shows the robustness of our Triple
Graphs model. For sampled frame number, we compare the
results of GCN, PGCN, and MSTR for F = 5, 10, 15, 20.
Figure 8 shows that the mAP increases with the growth of
input frames. This demonstrates that our graphs not only
exploit more useful information from more frames, but also
are robust to the noise from extra data.

From the experimental results, we can observe that ex-
plicit recognition of social relations from video clips is a
challenging task. Figure 9 shows the confusion matrix of
our MSTR framework.
It is difﬁcult to distinguish very
similar relations only using visual content. For example,
friend, sibling, and service may be very ambiguous if we
only focus on the persons in the video. In this condition, the
context like the scenes, backgrounds, objects may be more
important for relation reasoning. Currently, we only simply
adopt a standard TSN model to learn context cues. In future
work, contextual information should be further mined for
social relation recognition in videos.

5. Conclusion

In this paper, we propose a Multi-scale Spatial-Temporal
Reasoning framework to recognize social relations from
videos. The MSTR can learn robust representation which
exploits multi-scale features in both spatial and temporal
domains. To represent the appearance and actions of per-
sons and objects, we propose a Triple Graphs model to cap-
ture the visual relations of nodes. By combining global fea-
tures learned by TSN, our framework can learn multi-scale
spatial features from video frames. To learn both long-term
and short-term temporal cues in videos, we propose a Pyra-
mid Graph Convolutional Network which performs relation
reasoning with multi-scale temporal receptive ﬁelds. Ex-
tensive experiments on a large-scale and high-quality video
social relation dataset demonstrate the effectiveness of the
proposed framework.

83573

 30.0035.0040.0045.0050.000.100.120.140.160.180.20mAPτGCNPGCN 35.0040.0045.0050.005101520mAPFrame NumberGCNPGCNMSTRReferences

[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,
Alexandre Robicquet, Fei-Fei Li, and Silvio Savarese. So-
cial LSTM: human trajectory prediction in crowded spaces.
In CVPR, pages 961–971, 2016. 2

[2] Timur M. Bagautdinov, Alexandre Alahi, Franc¸ois Fleuret,
Pascal Fua, and Silvio Savarese. Social scene understand-
ing: End-to-end multi-person action localization and collec-
tive activity recognition. In CVPR, pages 3425–3434, 2017.
2

[17] Wei Liu, Yu-Gang Jiang, Jiebo Luo, and Shih-Fu Chang.
Noise resistant graph ranking for improved web image
search. In CVPR, pages 849–856, 2011. 3

[18] Xinchen Liu, Wu Liu, Tao Mei, and Huadong Ma. A
deep learning-based approach to progressive vehicle re-
identiﬁcation for urban surveillance. In ECCV, pages 869–
884. Springer, 2016. 3

[19] Xinchen Liu, Wu Liu, Tao Mei, and Huadong Ma. Provid:
Progressive and multimodal vehicle reidentiﬁcation for
large-scale urban surveillance. IEEE Transactions on Multi-
media, 20(3):645–658, 2018. 3

[3] Daphne Blunt Bugental. Acquisition of the algorithms of so-
cial life: A domain-based approach. Psychological Bulletin,
126(2):187, 2000. 2, 5

[20] Jinna Lv, Wu Liu, Lili Zhou, Bin Wu, and Huadong Ma.
Multi-stream fusion model for social relation recognition
from videos. In MMM, pages 355–368, 2018. 2, 5, 7

[4] Micha¨el Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. Convolutional neural networks on graphs with
fast localized spectral ﬁltering. In NIPS, pages 3837–3845,
2016. 3

[5] Lei Ding and Alper Yilmaz. Learning relations among movie
characters: A social network perspective. In ECCV, pages
410–423, 2010. 1, 2

[6] Lei Ding and Alper Yilmaz. Inferring social relations from

visual concepts. In ICCV, pages 699–706, 2011. 1, 2

[7] Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Efﬁcient
International Journal of

graph-based image segmentation.
Computer Vision, 59(2):167–181, 2004. 3

[8] Chuang Gan, Boqing Gong, Kun Liu, Hao Su, and
Leonidas J. Guibas. Geometry guided convolutional neural
networks for self-supervised video representation learning.
In CVPR, pages 5589–5597, 2018. 2

[9] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross B.
Girshick. Mask R-CNN. In ICCV, pages 2980–2988, 2017.
3

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016. 3, 4

[11] Thomas N. Kipf and Max Welling. Semi-supervised clas-
siﬁcation with graph convolutional networks. CoRR, ab-
s/1609.02907, 2016. 3, 4

[12] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S.
Kankanhalli. Dual-glance model for deciphering social re-
lationships. In ICCV, pages 2669–2678, 2017. 2, 5, 7

[13] Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, and
Shuicheng Yan. Semantic object parsing with graph LSTM.
In ECCV, pages 125–143, 2016. 3

[14] Liang Lin, Xiaolong Wang, Wei Yang, and Jian-Huang
Lai. Discriminatively trained and-or graph models for ob-
ject shape detection. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 37(5):959–972, 2015. 3

[15] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and
C. Lawrence Zitnick. Microsoft COCO: common objects in
context. In ECCV, pages 740–755, 2014. 3

[16] Kun Liu, Wu Liu, Chuang Gan, Mingkui Tan, and Huadong
Ma. T-C3D: temporal convolutional 3d network for real-time
action recognition. In AAAI, pages 7138–7145, 2018. 3

[21] You-Jin Park and Kun-Nyeong Chang.

Individual and
group behavior-based customer proﬁle model for personal-
ized product recommendation. Expert Systems with Applica-
tions, 36(2):1932–1939, 2009. 1

[22] Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel
Urtasun. 3d graph neural networks for RGBD semantic seg-
mentation. In ICCV, pages 5209–5218, 2017. 3

[23] Vignesh Ramanathan, Jonathan Huang, Sami Abu-El-Haija,
Alexander N. Gorban, Kevin Murphy, and Li Fei-Fei. De-
tecting events and key actors in multi-person videos.
In
CVPR, pages 3043–3053, 2016. 1, 2

[24] Vignesh Ramanathan, Bangpeng Yao, and Fei-Fei Li. Social
role discovery in human events. In CVPR, pages 2475–2482,
2013. 1, 2

[25] Ashtosh Sapru and Herv´e Bourlard. Automatic recognition
of emergent social roles in small group interactions. IEEE
Transactions on Multimedia, 17(5):746–760, 2015. 2

[26] Qianru Sun, Bernt Schiele, and Mario Fritz. A domain based
approach to social relation recognition. In CVPR, pages 435–
444, 2017. 1, 2, 5, 7

[27] Gang Wang, Andrew C. Gallagher, Jiebo Luo, and David A.
Forsyth. Seeing people in social context: Recognizing peo-
ple and social relationships. In ECCV, pages 169–182, 2010.
1, 2

[28] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment
networks: Towards good practices for deep action recogni-
tion. In ECCV, pages 20–36, 2016. 2, 3, 5, 6

[29] Xiaolong Wang and Abhinav Gupta. Videos as space-time

region graphs. In ECCV, pages 413–431, 2018. 3, 5

[30] Zhouxia Wang, Tianshui Chen, Jimmy S. J. Ren, Weihao Yu,
Hui Cheng, and Liang Lin. Deep reasoning with knowledge
graph for social relationship understanding. In IJCAI, pages
1021–1028, 2018. 1, 2, 3, 7

[31] Ting Yu, Ser-Nam Lim, Kedar A. Patwardhan, and Nils
Krahnstoever. Monitoring, recognizing and discovering so-
cial networks. In CVPR, pages 1462–1469, 2009. 1, 2

[32] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou
Tang. Learning social relation traits from face images. In
ICCV, pages 3631–3639, 2015. 1, 2, 5

[33] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou
Tang. From facial expression recognition to interpersonal re-
lation prediction. International Journal of Computer Vision,
126(5):550–569, 2018. 1

93574

