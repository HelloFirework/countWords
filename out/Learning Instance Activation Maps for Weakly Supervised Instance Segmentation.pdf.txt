Learning Instance Activation Maps

for Weakly Supervised Instance Segmentation

Yi Zhu1, Yanzhao Zhou1, Huijuan Xu2, Qixiang Ye1, David Doermann3, Jianbin Jiao1

1University of Chinese Academy of Sciences 2University of California, Berkeley 3University at Buffalo

{zhuyi215,zhouyanzhao215}@mails.ucas.ac.cn

huijuan@berkeley.edu

{jiaojb,qxye}@ucas.ac.cn

doermann@buffalo.edu

Abstract

Pseudo GT Sampling

Discriminative region responses residing inside an ob-
ject instance can be extracted from networks trained with
image-level label supervision. However, learning the full
extent of pixel-level instance response in a weakly super-
vised manner remains unexplored. In this work, we tackle
this challenging problem by using a novel instance extent
ﬁlling approach. We ﬁrst design a process to selectively col-
lect pseudo supervision from noisy segment proposals ob-
tained with previously published techniques. The pseudo
supervision is used to learn a differentiable ﬁlling module
that predicts a class-agnostic activation map for each in-
stance given the image and an incomplete region response.
We refer to the above maps as Instance Activation Maps
(IAMs), which provide a ﬁne-grained instance-level rep-
resentation and allow instance masks to be extracted by
lightweight CRF. Extensive experiments on the PASCAL
VOC12 dataset show that our approach beats the state-
of-the-art weakly supervised instance segmentation meth-
ods by a signiﬁcant margin and increases the inference
speed by an order of magnitude. Our method also gen-
eralizes well across domains and to unseen object cate-
gories. Without ﬁne-tuning for the speciﬁc tasks, our model
trained on VOC12 dataset (20 classes) obtains top per-
formance for weakly supervised object localization on the
CUB dataset (200 classes) and achieves competitive results
on three widely used salient object detection benchmarks.

1. Introduction

Powered by the recent advances of Deep Convolu-
tional Neural Networks (DCNNs), instance segmentation
has made remarkable progress [13, 6, 25]. Deep learn-
ing approaches, however, typically require large amounts of
data for training and rely on detailed ground truth (GT) in
the form of instance masks which often requires extensive

Image

GT Mask 
(Not used)

PRM

Instance Extent Filling

IAM(Ours)

Figure 1: Peak Response Maps [43] from classiﬁcation net-
works can only identify the discriminative parts of each in-
stance. Our approach collects pseudo ground-truth masks
from noisy segment proposals obtained with off-the-shelf
techniques and learns an extent ﬁlling module. The result-
ing Instance Activation Maps (IAMs) effectively localize
instance-level spatial extent.

human effort. For example, in the CityScapes dataset [9],
ﬁne-detailed pixel-level annotation typically requires more
than 1.5 hours for a single image. In contrast, image-level
labels, i.e., the presence or absence of object categories in
an image, are much easier to deﬁne and can even be auto-
matically collected from the Internet.

Learning instance segmentation with image-level labels
is a challenging task as the annotation does not inform the
location or spatial extent of objects in an image. Zhou et
al. [43] took the ﬁrst step in addressing this task by extract-
ing instance-aware visual cues from classiﬁcation networks.
They produce response maps for each object category, i.e.,
Class Activation Maps (CAMs) [42], to indicate essential
receptive ﬁelds used by the network when identifying the
object class. The peaks, i.e., local maximas, of CAMs are
stimulated and back-propagated to generate Peak Response
Maps (PRMs) that highlight informative regions residing
inside each object instance. PRMs could identify discrim-
inative parts of each object yet failed to localize other re-
gions. As shown in Fig. 2, PRMs highlight the dog head

13116

while ignoring the dog’s body. The reason may lie in the
fact that the body region could be obscured when identify-
ing “dog” but the head is essential for classiﬁcation. There-
fore, PRMs do not complete the instance’s extent. This lim-
its its performance and reduces its inference efﬁciency due
to its dependence on a costly instance mask generation strat-
egy, i.e., retrieving segment object proposals obtained with
low-level vision techniques [33, 29, 28].

In this paper, we address the problem of learning in-
stance extent in a weakly supervised manner by developing
a novel instance extent ﬁlling approach. We ﬁrst leverage
incomplete region responses obtained with the previously
developed PRM method [43] to collect pseudo ground-truth
(GT) masks from noisy object segment proposals. The
pseudo GT masks are then used to learn a differentiable ﬁll-
ing module that predicts a class-agnostic activation map for
each instance conditioned on the image and an incomplete
region response. The result is an Instance Activation Map
(IAM) that speciﬁes both spatial layout and ﬁne-detailed in-
stance boundaries, Fig. 1. This allows instance masks to
be directly extracted with the lightweight and GPU-friendly
dense CRF post-processing [19, 32]. As a result, we signif-
icantly improve the state-of-the-art weakly supervised in-
stance segmentation performance as well as increase the in-
ference speed by an order of magnitude.

Our approach learns instance extent knowledge from
image-level labels and noisy segment proposals. We also
show that the learned knowledge generalizes well across
domains and to unseen object categories. This extends the
application of the proposed approach to many other object
extent related visual tasks. Our model obtains competitive
performance on weakly supervised object localization and
salient object detection benchmarks without ﬁne-tuning the
extent ﬁlling module for the speciﬁc tasks.

The main contributions of this paper include:

• The development of an instance extent ﬁlling approach
to tackle the challenging problem of weakly super-
vised instance segmentation task by collecting pseudo
GT masks from noisy segment proposals, and then
train a differentiable ﬁlling module to learn common
knowledge of class-agnostic object extent.

• An implementation of our approach with popular DC-
NNs, e.g., ResNet50, that demonstrate substantial im-
provement over the state-of-the-art with respect to both
performance and inference speed.

• A demonstration of the fact that the extent knowledge
learned by the proposed approach generalizes well and
achieves a performance that matches or exceeds state-
of-the-art on object extent related tasks such as weakly
supervised object localization and salient object detec-
tion without ﬁne-tuning for the speciﬁc tasks.

dog

dog

dog

potted plant

potted plant

potted plant

Image

CAM

PRM

IAM (ours)

Figure 2: The activation maps from three methods, Class
Activation Map (CAM) [42], Peak Response Map (PRM)
[43] and our Instance Activation Map (IAM), shown in se-
quence for two example images. Our IAM covers full ob-
ject extent while the other two methods only show coarse
location or discriminative object parts.

2. Related Work

Weakly supervised instance segmentation: As one of
the most challenging problems in computer vision, instance
segmentation has been extensively investigated [22, 10, 2,
13, 6, 25]. Nevertheless, many of these works require strong
supervision in the form of human annotated instance masks
which limits their application on large-scale datasets with
weaker forms of labeling. Weakly supervised instance seg-
mentation tries to break this limitation. To perform in-
stance segmentation with few annotations, partial supervi-
sion [16] performs instance segmentation on datasets where
that a subset of classes have instance mask annotations dur-
ing training. The remaining classes have only bounding box
annotations. Weakly supervised instance segmentation with
object bounding box supervision [18] uses object bounding
boxes to construct pseudo GT masks to train instance seg-
mentation models.

Although these methods have relaxed their reliance on
accurate pixel-level masks, they still require instance-level
labeling, which requires the location of each object. In [43],
Zhou et al. for the ﬁrst time proposed to tackle weakly
supervised instance segmentation by exploiting the class
peak response of classiﬁcation networks to extract instance-
aware visual cues. The cues were then used to retrieve pro-
posals as instance masks. Nevertheless, as the ﬁlters learned
for image classiﬁcation typically corresponds to discrimi-
native object parts, this approach failed to locate the full
object extent and thus misled the instance mask generation.
We aim to address this issue and extract complete instance-
level representations by compensating for the missing ex-
tent information by learning knowledge of object extent
from segment proposals off-the-shelf. As more areas of the
instance are activated in the proposed Instance Activation
Maps (IAMs), our method can better improve instance seg-
mentation performance and increase the inference speed via
lightweight post-processing strategy.

3117

Figure 3: An overview of learning Instance Activation Maps for weakly supervised instance segmentation. The images are
ﬁrst fed to a classiﬁcation network to generate deep feature pyramids and Peak Response Maps (PRMs) which highlight object
parts. The deep features are used to construct the weights for an Instance Extent Filling module which recover instance extent
from the PRMs. During training, the ﬁlling module collects common knowledge of object extent from pseudo GT masks.

Learning pixel-level afﬁnity: Our work is also related
to the approaches which leverage/learn pixel-level afﬁn-
ity for image segmentation [8, 3, 34, 1]. Some of these
approaches use semantic segmentation labels to estimate
a pixel-level afﬁnity matrix of an image by training de-
convolutional networks [3] or reﬁnement modules, such as
dense CRFs [8]. In contrast, our goal is to ﬁll object re-
gions by leveraging the sparse and partial visual cues under
weak supervision. With solely image-level categories avail-
able, synthetic labels extracted from class response maps
are employed to train a network which learns pairwise se-
mantic afﬁnity [1]. Our approach learns instance-aware
afﬁnity which extends beyond the semantic afﬁnity. Our
approach is also related to the Spatial Propagation Net-
work [24] which learns semantically-aware afﬁnity values
for high-level vision tasks. The difference lies that [24] re-
lies on GT masks while our approach uses image-level la-
bels and inaccurate class-agnostic proposals off-the-shelf.

Region proposal: Due to the lack of object mask an-
notations, weakly supervised methods typically introduce
object priors from region proposals. Classical region pro-
posal methods [33, 46, 29, 11] hypothesize object candi-
dates based on class-agnostic low-level features, e.g., color,
texture, edge, and contours. Therefore, proposal techniques
typically generalize well and can be used off-the-shelf with-
out introducing human labeling efforts for each speciﬁc
task. The pre-computed proposals could be used to nar-
row the solution space in the pre-processing stage [4, 35]
or to reﬁne prediction boundaries during post-processing
[31, 43]. We random sample noisy proposals to construct
pseudo GT masks during training and statistically learn a
differentiable instance extent ﬁlling module.

3. Method

In this section, we ﬁrst revisit the previously published
method [43] that we use to extract incomplete instance re-
gion responses from CNNs trained with image-level class
labels. We then introduce the proposed instance extent ﬁll-
ing approach, starting with the process of collecting pseudo
GT masks and followed by the design of the extent ﬁlling
module. Finally, we discuss the insights of the method and
specify the implementation details. The overall architecture
of our approach is illustrated in Fig. 3.

3.1. Revisiting Peak Response Mapping

We use the technique in [43] to extract Peak Response
Maps (PRMs) from classiﬁcation networks. The network
is ﬁrst converted to a fully convolutional network (FCN)
by removing the global pooling layer and transforming the
weights of the fully connected layers to 1x1 convolutional
ﬁlters. The FCN outputs CAMs M ∈ RC×N ×N with a
single forward pass, where C denotes the number of image
classes, and N × N is the spatial size of the maps. The
class peak responses (local maximums) of the c-th class re-
sponse map M c, are then detected and averaged to predict
a conﬁdence score for the c-th image class.

During training, the classiﬁcation loss drives the network
to learn multiple discriminative class peaks, and the learned
peaks can be back-propagated to PRMs by leveraging the
top-down relevance between spatial locations of adjacent
layers as:

X

P (U k

pq|V c

ij)P (V c

ij),

(1)

P (U k

pq) = X
k∈Ic

where P (U k

pq|V c

(p,q)∈Mk
ij
ij) = P(i,j)∈N c

pq

Zpq × U k
ij

ˆWpq. U, V are

3118

sheepsheepInstanceActivationMapsheepsheepPeakResponseMapInputImageFlowinTraining&InferenceFlowinTrainingphaseONLYPseudoGT SamplingExtractobjectproposalsMask PredictionDenseCRFPixel-wiseLossClassification networkInstanceExtentFillingModuleFeature PyramidsheepsheepImage-level SupervisionSheepClassification LossConvEncoderConvDecoderNoisySegmentproposals𝐺𝑘Figure 4: Illustration of the ﬁlling process. The value of
each pixel is ﬁlled with the value from its neighbors. The
process is performed in a convolutional manner to facilitate
Cuda acceleration.

outputs of two adjacent layers. Ic is a set of feature maps
connected to V c. Hk
ij is the set of locations in U that con-
ij via non-negative convolutional weights. N c
nect to V c
pq
is the set of locations in V that connected to U c
pq via non-
negative convolutional weights ˆW . The negative weights
are discarded as the commonly used ReLU activation layer
prevents them from contributing to the ﬁnal response. Zpq is
a normalization factor which guarantees the transition prob-
abilities sum to one. With the probability propagation de-
ﬁned by Eq. 1 and an initial probability map that only a peak
location is 1.0, we can identify which locations in the bot-
tom layer (pixel space) contributes to the speciﬁc class peak
response from the top layer (semantic space), and generate
Peak Response Maps (PRMs), M, to highlight discrimina-
tive instance regions, Fig. 3. Note that each PRM M i ∈ M
is a map with the same shape as the input image, and its pre-
dicted class label is the channel index of the corresponding
class peak response. Before further processing, we com-
pute mean across its channel dimension and normalize it by
dividing the sum.

3.2. Learning Instance Activation Maps

Instance Activation Maps are generated by recovering
the full object extent from incomplete PRMs using an ex-
tent ﬁlling process.

Collecting pseudo supervision: Low-level vision ob-
ject proposal methods often use the hypothesis that an ob-
ject has consistent color, texture, and/or closed boundary to
estimate class-agnostic object masks, Fig. 3. Although in-
complete and noisy, redundant segment proposals can sta-
tistically cover the object and are sufﬁcient for learning to
ﬁll the objects extent in local areas.

Given an image, we ﬁrst extract a set of segment pro-
posals S. We then calculate matching scores between each
PRM M i ∈ M and each segment proposal Sj ∈ S of
the image as fij = α · M i ∗ Sj + M i ∗ ˆSj , where ˆSj
is the proposal contour mask computed by morphological
gradient operation. α is a class independent balance fac-
tor. The score comprises both the extent matching and
boundary matching between M i and Sj . After ranking the

Figure 5: Visualization of the ﬁlling weights. The arrows
indicate the ﬁlling from a pixel to its eight adjacent neigh-
bors. Pixels in the ﬂat area (e.g., “bed”) share a ﬁlling direc-
tion with their neighbors. On both sides of the edge, the ar-
rows point in opposite directions; thus preserve the instance
boundary. Best viewed zooming on screen.

match scores for each PRM, we retain the top k proposals
to provide locally correct object extent. When k increases,
more false proposals could be retained to cause large dis-
agreement between the k proposals, which might affect the
model performance. Therefore, we then compute overlaps
between the M and the k proposals and discard proposals
M i ∗ Sj lower than a threshold, i.e., 0.2
with overlap max

i

in our settings.

During training, for each PRM, the approach randomly
samples a proposal from the top k proposal candidates to
construct the pseudo GT mask in each forward pass. Note
that the difference between our approach with the proposal
retrieval procedure in PRM [43] is two-fold: 1) the masks
in our approach are used to learn the instance ﬁlling mod-
ule while those of PRM is used to generate predictions and
2) we use a random strategy to sample multiple candidates
while PRM only retrieves a single candidate with a maxi-
mum score. Therefore, the advantage is also two-fold: 1)
our approach avoids proposals in the inference phase; thus
improve the inference speed (order of magnitude). 2) we
can statistically learn from multiple noisy proposals.

Instance Extent Filling: From each PRM-proposal pair
produced above, we learn common knowledge of object ex-
tent from the segment proposals and image features to re-
cover the instance extent conditioned on the PRM. To this
end, we develop a differentiable extent ﬁlling module with
an encoding-ﬁlling-decoding architecture (Fig. 3). The en-
coding process E(·, θe) is the forward pass of a tiny con-
volutional network including two Conv-BatchNorm-ReLU-
MaxPooling stacks. θe denotes the learnable parameters in
the network. The contracting path of E squeezes the spa-
tial size of the input PRM M i; thus the ﬁlling process can
better capture the long-range dependency between spatial
locations in a computationally efﬁcient way. Moreover, the
encoded instance cues E(M i, θe) are embedded in a fea-
ture space, making the ﬁlling process more stable to the re-
sponse noises in PRMs. The decoding process D(·, θd) is

3119

(𝑖,𝑗)(𝑖,𝑗)𝑟2𝑊𝐺𝑘−1NormalizeReshape𝑁𝑁𝐶𝐶𝐶𝑟𝑟𝐶𝐶𝑁𝑁𝐺𝑘𝑁𝑁ImagePRMFilling weightsIAMbusbusdogdogbusbusalso a forward pass of a network that contains a symmetric
expanding path and decodes the ﬁlling processed features as
an Instance Activation Map (IAM). Note that this encoding-
decoding process is built with standard CNN components;
thus it can pass the gradients to each input. Compare to
commonly used Auto-Encoder architectures, we design a
ﬁlling process to effective model spatial relevance in the
encoded feature space. The ﬁlling process (see Fig. 1) is an
iterative process that consists of N ﬁlling steps F deﬁned
as:

Gk = F (Gk−1, W )
G0 = E(M i, θe),

(2)

where Gk ∈ RC×N ×N are the features after the k-th iter-
ation, 0 < k ≤ N , and W ∈ RC×(N ×N )×(r×r) are the
ﬁlling weights constructed from intermediate features maps
M of the classiﬁcation backbone as W = R(M, θr). R
denotes a feature pyramid structure [23] which sequentially
applies two 1 × 1 convolutions to adapt the feature maps
at different levels. It then upsamples and fuses them from
deep to shallow (Fig. 3). θr is the learnable parameters in
the feature pyramid. As illustrated in Fig. 4, in each step
of the ﬁlling process, locations of the c-th channel of Gk
are ﬁlled according to its neighbors (and itself) and the pre-
dicted ﬁlling weights as

Gk

ij = X
u,v∈Nij

YijWc;i,j;u,vEk−1

uv (M i, θe),

(3)

where Nij denotes the r2 neighbors of coordinate (i, j), Yij
is a normalizer to guarantee Pu,v∈Nij
Wc;i,j;u,v = 1. The
ﬁlling process stops when the maximum number of itera-
tions N is reached. We set N to the size of the encoded
maps to ensure the access to any location on the map.

The examples of learned ﬁlling weights W are shown
in Fig. 5. The eight adjacent neighbors of a pixel are vi-
sualized in the form of vector ﬁelds, where the angle rep-
resents the corresponding neighbors and length represents
the value. We compute the mean of W across channels and
subtract the average from each map to suppress the “ﬂat”
regions that connected with all neighbors. It can be seen
in the zoomed area that the ﬁlling weights clearly identify
the instance boundaries. Interestingly, it can be seen from
the example of the third row that our ﬁlling module suc-
cessfully identiﬁes the boundaries of “bed” even though it
is not a valid object category in the dataset. This demon-
strates that our approach can learn the common knowledge
of object extent that generalizes to unseen object categories.

3.3. Implementation

Training details: Our proposed model is trained with
image-level labels and class-agnostic segment proposals
off-the-shelf. We implement our method based on standard
ResNet50 [14] architecture. We ﬁrst train the backbone net-
work equipped with peak stimulation for image classiﬁca-
tion [43], using the Multi-label Soft Margin Loss and SGD

optimizer, with a learning rate of 0.01. Then we optimize
the ﬁlling module using Binary Cross Entropy loss. The
initial learning rate of the SGD optimizer is set to 0.1. We
use feature maps from res-block 2, 3, 4 of the backbone
ResNet50 to form the feature pyramid. Following [43],
we use the Multi-scale Combinatorial Grouping (MCG)
framework [29] in conjunction with high-quality region hi-
erarchies obtained with Convolutional Oriented Boundaries
[28] to extract segment proposals. Note that our method
does not constrain the choice of proposal technique.

Post-processing: Since our proposed IAMs covers in-
stance extents, we choose the Convolutional Conditional
Random Field (ConvCRF) [32] for further boundary reﬁne-
ment. This is in contrast to previous work [43] that has to
employ computationally intensive proposal retrieval strate-
gies to recover object extents. Experiments show that with
ConvCRF, when the state-of-the-art fails, we maintain top
performance while reducing the inference time by order of
magnitude (0.3s vs. 3.0s per image).

3.4. Discussion

The proposed approach leverages instance-aware cues
from classiﬁcation networks, object prior from proposals,
and instance extent ﬁlling operations to learn the full ob-
ject extent. During the training phase, it actually imple-
ments a special kind of “semantic mosaicking”. It collects
redundant segments, absorbs broken semantic information
into convolutional ﬁlters, and then ﬁts complete semantics
and full object extents. During the test phase, the peak re-
sponse maps act as semantic anchors which correspond to
the most discriminative parts, while the extent ﬁlling mod-
ule produces instance activation maps (IAMs). The pro-
cedure is similar to the classical process of “ﬂood-ﬁlling”
[5]. The difference lies in the ﬂood-ﬁlling is deﬁned for
grey-level stable image regions while IAMs are for seman-
tically stable regions. The emergence of IAMs shows that
the instance-level extent can be learned from redundant and
noisy proposal segments, Fig. 1, which provide fresh insight
for weakly supervised instance segmentation.

4. Experiments

We evaluate the proposed object extent ﬁlling approach
on several popular benchmarks. In Sec. 4.1, we compare
our Instance Activation Maps (IAMs) with state-of-the-art
weakly supervised instance segmentation methods, demon-
strating the effectiveness and efﬁciency of our approach. In
Sec. 4.2, statistical analyses are performed to measure the
quality of IAMs, which shows that our method can gen-
erate accurate instance-aware activations that cover object
extent. In Sec. 4.3, we apply the trained ﬁlling module to
ﬁne-grained object localization and saliency detection with-
out further ﬁne-tuning of the instance extent ﬁlling module,
validating the generalization ability of our method.

3120

Figure 6: Weakly supervised instance segmentation examples. The Instance Activation Maps (2nd row) incorporate complete
instance activation, which is exploited to produce instance-level masks (3rd row). The last column shows typical failure cases.

Method

Ground Truth Box

Rect.
Ellipse
MCG

mAP r

0.25 mAP r
30.2
41.1
38.0

0.5 mAP r
4.5
6.6
12.3

0.75 ABO
47.4
51.9
53.3

78.3
81.6
69.7

Baselines constructed from Weakly Supervised Object Localization

CAM [42]

SPN [45]

MELM [35]

Rect.
Ellipse
MCG
Rect.
Ellipse
MCG
Rect.
Ellipse
MCG

18.7
22.8
20.4
29.2
32.0
26.4
36.0
36.8
36.9

2.5
3.9
7.8
5.2
6.1
12.7
14.6
19.3
22.9

Weakly Supervised Instance Segmentation

PRM [43]
IAM-S1
IAM-S5
IAM-S9

44.3
45.6
45.9
45.7

26.8
28.3
28.8
27.8

0.1
0.1
2.5
0.3
0.3
4.4
1.9
2.4
8.4

9.0
10.4
11.9
10.5

18.9
20.8
23.0
23.0
24.0
27.1
26.4
27.5
32.9

37.6
41.5
41.9
41.7

Table 1: Weakly supervised instance segmentation results
- mean average precision (mAP%) and Average Best Over-
lap (ABO). Our method is evaluated with different random
sampling numbers, i.e., 1, 5, 9.

4.1. Weakly Supervised Instance Segmentation

We compare the performance of the proposed IAM with
some baselines on the PASCAL VOC 2012 [12] segmenta-
tion benchmark.

Numerical results: In Tab. 1, the instance segmentation
results are presented as the mean Average Precision (mAP)
at IoU thresholds of 0.25, 0.5, and 0.75. Our IAM-S5 model
outperforms the state-of-the-art by a margin 1.6%, 2.0%,
and 2.9% respectively. The improvement at a higher IoU
threshold of 0.75 is more signiﬁcant than that 0.25 and 0.5,

Feed-Forward

Proposal Retrieval CRF

Total

PRM [43]
IAM (Ours)

0.05
0.07

3.0 (+8.1)

N/A

N/A 11.15
0.3
0.37

Table 2: Per-image inference time (seconds). The feed-
forward and the CRF modules are tested with a Tesla P100
GPU while the proposal retrieval is tested with the ofﬁcial
code on CPU. It takes 8.1s per image to extract proposals.

which indicates the effectiveness of our approach for gen-
erating high-quality instance activation and the capture of
the ﬁne-detailed object boundary. The Average Best Over-
lap (ABO) [30] score increased by a large margin of 4.3%,
showing the ability of the IAMs to cover full object extents.
Several baselines are constructed from weakly supervised
object localization methods via three reasonable bbox-to-
mask generation strategies [18].

The Effect of the random sampling number k: In the
ﬁlling process, we learn IAMs from pseudo GT Masks ob-
tained by random proposal sampling. The ﬁlling module
summarizes the common knowledge of the object extent
from the top k noisy masks. In Tab. 1, we evaluate the im-
pact of the sampling number k. We ﬁrst set k to 1 to verify
if our model can explore the common object extent cross
instances with only one noisy mask for each PRM. As a
result, the IAM-S1 consistently improves the performance
on the mAP r at 0.25, 0.5, 0.75 as well as the ABO met-
ric. As we increase k to 5, the performance of IAM-S5 is
higher than IAM-S1, showing that our method can summa-
rize the common knowledge of object extent from the noisy
masks corresponding to the same PRM. Despite the ability
to learn object extent from noisy masks, our model would be

3121

ImageIAMPredictionGTClose ClutteredMulti-scaleMulti-classFailure CaseOverspreadMissing instanceHollow objectInterruptionComplex texturepersonbicyclepersonpersonpersonpersonpersonpersonmotor bikecowcarsheepsheepcatbicyclebicycleplantdogcowcowbottle(a) PRM [43]

(b) IAM (Ours)

Figure 8: Per-class mean IoU (%) of PRMs and IAMs.

Figure 7: The density map of samples from PRMs and
IAMs. Darker area indicates more samples are of the corre-
sponding IoU (%) value and object size.

affected when the sampling number increases to 9, as more
and more false proposals could be sampled in the training
procedure, which makes it hard for our model to summarize
the knowledge of object extent.

Inference time: Tab. 2 shows the time cost for the in-
ference period. PRM usually highlights object parts of in-
stances and thus relies on the time-consuming proposal re-
trieval process (3.0s per image plus 8.1s for proposal gener-
ation) to get the instance masks. In contrast, our IAM can
ﬁll the object extent using CRF to reﬁne object boundary,
dramatically improves the inference speed (0.3 vs. 3.0s)
while boosting instance segmentation performance.

Qualitative results: In Fig. 6, we illustrate instance seg-
mentation examples including successful cases and typical
failure cases. In the ﬁrst column, our approach can distin-
guish instances with the complex texture. Examples in the
second and third columns show that our approach performs
well with cluttered or objects close to others. In the fourth
and ﬁfth columns, objects from different scales and differ-
ent classes are well segmented. This shows that our method
can extract both class-aware and instance-aware activation
from classiﬁcation networks. The last column shows fail-
ure cases of IAMs. It could miss an instance without proper
instance-aware cues at ﬁrst. Typically, IAMs can be misled
by differences in color or texture in large areas and some-
times have problems connecting the parts of obscured or
hollow objects. IAM may also fail to identify the boundary
of huddled objects that are similar to each other.

4.2. Statistical Analysis for IAMs

A series of experiments are performed to analyze IAMs
with respect to object size and object category, demonstrat-
ing that our approach outperforms the state-of-the-art ap-
proaches including Peak Response Map (PRM). IAMs are
assigned to GT (ground truth) masks and judged to be over-
lapping or not by measuring the best matching IoU (Inter-
section over Union). To be considered a perfect IAM that
completely coincides with a GT mask, the IoU between the
predicted IAM M and GT masks T must be close to 100%

area(fb(M,θ)∩Ti)
as computed using the metric maxθ,Ti∈T
area(fb(M,θ)∪Ti) ,
where the function fb(M, θ) = M ≥ θ produces the best
matching binary instance masks based on the probabilistic
IAMs over a set of threshold values θ ∈ (0, 1).

Sample distribution over object size: We ﬁrst visualize
the density of the IoU for PRMs and IAMs to see whether
IAMs can cover objects of different sizes (Fig. 7). Fig. 7a
shows that samples from PRMs are predominantly clustered
in the area where the IoU value is less than 50% and failed
to cover large objects. In contrast, in Fig. 7b most of the
IAMs have high IoUs and perform well on large objects.

Sample distribution over object classes: We further
calculate the mean IoU of each class to analyze the impact
of different object categories, Fig. 8. Our method achieves
consistent improvement across all categories. On “bus” and
“cat”, IAM outperforms PRM by a large margin (∼40%).
The reason is that PRMs can highlight the discriminative
parts such as a tire for “bus” and head for “cat”, while IAMs
cover complete object regions.

4.3. Generalization to Unseen Categories

The IAMs trained for weakly supervised instance seg-
mentation are directly applied to localize the full extent of
objects from a ﬁne-grained species and unseen categories.
Without any ﬁne-tuning or re-training of the instance extent
ﬁlling module, this procedure can be seen as unsupervised
domain adaptation.

Localizing objects from ﬁne-grained species: We use
the pre-trained model to localize the bird species in the
CUB-200-2011 dataset [36]. The dataset contains 11788
images over 200 categories of birds. There are 5994 im-
ages for training and 5794 images for testing. We chose
this dataset to validate that the knowledge of object com-
monality learned from PASCAL VOC12 dataset can adapt
to the ﬁne-grained species which contain many unusual bird
objects. Note that there are only 705 images deﬁned for the
bird category in the VOC12 training set. We ﬁrst calculate
IAMs for the images using the pre-trained IAM-S5 model,
then extract bounding boxes using a mean value threshold.
A bounding box is considered to have correct localization
prediction if 1) the predicted class label is correct; 2) the
overlap between the predicted box and ground-truth box is

3122

planebikebirdboatbottlebuscarcatchaircowtabledoghorsemotorpersonplantsheepsofatraintv01020304050607080mIoU (%)3152827204231395202135232521321194019IAMPRMFigure 9: Visualization of object localization from ﬁne-
grained bird species. Green boxes are ground-truth and yel-
low boxes are predictions. Best viewed in color.

Methods GoogLeNet-GAP [42] ACol [40]

SPG [41]

IAM (Ours)

Figure 10: Salient object detection examples. Our method
can ﬁll salient object extent in a class-agnostic manner, even
if the object appearance is unseen during training.

Loc. Err

59.00

54.08

53.36

52.21

Finetune Use GT

Methods

THUR MSRA-B ECSSD

Table 3: Localization error (%) on CUB-200-2011 test set
for weakly supervised methods and our transferable IAMs.
Note that our model is not trained on the target dataset.

higher than 0.5. To make a fair comparison, we use the
scores predicted by GoogLeNet-GAP as the object extent
In Tab. 3, we
predicted by our model is class agnostic.
compare the localization results with weakly supervised lo-
calization methods. We ﬁnd that IAM performs well on
the ﬁne-grained bird species, achieving 52.21% error. This
demonstrates that the model can generalize to objects from
diversiﬁed sub-classes despite it being trained in another do-
main. Fig. 9 shows that IAMs can cover the full object ex-
tent and therefore beneﬁt the bbox localization tasks.

Localizing salient objects from unseen categories:
We humans can tell objects extent even when we don’t
know what the object is, motivating learning class-agnostic
object commonality. To explore whether our approach can
localize objects from unseen categories, we apply it to the
salient object detection task. A Resnet50 classiﬁcation net-
work pre-trained on ImageNet is used to extract instance-
aware visual cues which provide the coarse position of
salient objects. These cues and the image are then fed to
the model trained on VOC12. We obtain saliency detec-
tion results after performing ReLU on IAMs. We evaluated
the performance on three popular saliency datasets includ-
ing THUR [7], MSRA-B [26], and ECSSD [37] using the
F-measure (Fβ) as a performance metric. We compared our
approach with state-of-the-art methods, including three su-
pervised approaches based on deep learning frameworks,
three unsupervised methods based on handcraft features and
two unsupervised models based on deep learning. The re-
sults in Tab. 4 show that our model performs as well as
a generic object extent localizer, despite the fact that it is
not trained for the particular task. Fig. 10 presents some
saliency detection results, which show that IAMs can ﬁll ob-
ject extent even though there is a large gap between the ob-
ject appearance of the target categories and those of VOC12

✓

✓

✓

✓

✓

✗

✗

✗

✗

✓

✓

✓

✗

✗

✗

✗

✗

✗

DSS [15]
NLDF [27]

DC [20]
SBF [38]

Multi-Noise [39]

DRFI [17]
RBD [44]
DSR [21]

IAM (Ours)

0.7081

-

0.6940

-

0.7322
0.5613
0.5221
0.5498
0.7364

0.8941
0.8970
0.8973

-

0.8770
0.7282
0.7508
0.7227
0.8643

0.8796
0.8908
0.8315
0.7870
0.8783
0.6440
0.6518
0.6387
0.8613

Table 4: Mean F-measure (Fβ) on salient object detection.
The ﬁrst column indicates if a ﬁnetuning model is used on
saliency datasets while the second column indicates if the
ground-truth masks are used in the training procedure.

object categories. This further validates the generalization
capability of our approach.

5. Conclusions

We developed a framework which is trained to generate
Instance Activation Maps (IAMs) driven by image-level su-
pervision and prior knowledge from object segment propos-
als off-the-shelf. By extracting instance-aware cues from
the classiﬁcation network and iterative completing the cues
according to the predicted extent ﬁlling weights, IAMs pro-
vide ﬁne-detailed instance-level representation that high-
light the spatial extent for each object. Our approach im-
plements a special kind of “semantic mosaicking” that col-
lects redundant noisy segments, absorbs broken semantic
information into convolutional ﬁlters to learn class-agnostic
object extent knowledge. On commonly used datasets, it
signiﬁcantly improves the state-of-the-art performance, in-
creases inference speed by an order of magnitude, and can
generalize to unseen categories, showing great potential on
instance-level weakly supervised learning problems.

Acknowledgements

The authors are very grateful for the support by NSFC

grant 61836012, 61771447, and 61671427.

3123

SPG mapSPG bboxPRMIAMIAM bboxMSRA-BTHURECSSDImageGTOursReferences

[1] J. Ahn and S. Kwak. Learning pixel-level semantic afﬁnity
with image-level supervision for weakly supervised semantic
segmentation. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 3

[2] A. Arnab and P. H. Torr. Pixelwise instance segmentation
In CVPR, vol-

with a dynamically instantiated network.
ume 1, page 5, 2017. 2

[3] G. Bertasius, L. Torresani, X. Y. Stella, and J. Shi. Convo-
lutional random walk networks for semantic image segmen-
tation. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 6137–6145, 2017. 3

[4] H. Bilen and A. Vedaldi. Weakly supervised deep detection
networks. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 2846–2854, 2016. 3

[5] S. V. Burtsev and Y. P. Kuzmin. An efﬁcient ﬂood-ﬁlling
algorithm. Computers & Graphics, 17(5):549–561, 1993. 5
[6] L.-C. Chen, A. Hermans, G. Papandreou, F. Schroff,
P. Wang, and H. Adam. Masklab: Instance segmentation
by reﬁning object detection with semantic and direction fea-
tures. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 1, 2

[7] M.-M. Cheng, N. J. Mitra, X. Huang, and S.-M. Hu.
Salientshape: Group saliency in image collections. The Vi-
sual Computer, 30(4):443–453, 2014. 8

[8] Y. Cheng, R. Cai, Z. Li, X. Zhao, and K. Huang. Local-
itysensitive deconvolution networks with gated fusion for
rgb-d indoor semantic segmentation.
In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), vol-
ume 3, 2017. 3

[9] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 3213–3223, 2016. 1

[10] J. Dai, K. He, and J. Sun. Instance-aware semantic segmen-
tation via multi-task network cascades. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
3150–3158, 2016. 2

[11] P. Doll´ar and C. L. Zitnick. Fast edge detection using struc-
IEEE Transactions on Pattern Analysis and

tured forests.
Machine Intelligence (TPAMI), 37(8):1558–1570, 2015. 3

[12] M. Everingham, S. M. A. Eslami, L. J. V. Gool, C. K. I.
Williams, J. M. Winn, and A. Zisserman. The pascal vi-
sual object classes challenge: A retrospective. International
Journal of Computer Vision (IJCV), 111(1):98–136, 2015. 6
[13] K. He, G. Gkioxari, P. Doll´ar, and R. B. Girshick. Mask R-
CNN. In IEEE International Conference on Computer Vision
(ICCV), pages 2980–2988, 2017. 1, 2

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 770–778, 2016.
5

[15] Q. Hou, M.-M. Cheng, X. Hu, A. Borji, Z. Tu, and P. Torr.
Deeply supervised salient object detection with short con-
nections. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 5300–5309. IEEE, 2017. 8

[16] R. Hu, P. Doll´ar, K. He, T. Darrell, and R. Girshick. Learning
to segment every thing. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 2

[17] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li.
Salient object detection: A discriminative regional feature
integration approach. In IEEE conference on computer vision
and pattern recognition (CVPR), pages 2083–2090, 2013. 8
[18] A. Khoreva, R. Benenson, J. Hosang, M. Hein, and
B. Schiele. Simple does it: Weakly supervised instance and
semantic segmentation.
In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 1665–1674,
2017. 2, 6

[19] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
connected crfs with gaussian edge potentials. In Advances
in neural information processing systems, pages 109–117,
2011. 2

[20] G. Li and Y. Yu. Deep contrast learning for salient object de-
tection. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 478–487, 2016. 8

[21] X. Li, H. Lu, L. Zhang, X. Ruan, and M.-H. Yang. Saliency
detection via dense and sparse reconstruction. In IEEE In-
ternational Conference on Computer Vision (ICCV), pages
2976–2983, 2013. 8

[22] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei. Fully convolutional
instance-aware semantic segmentation. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
4438–4446, 2017. 2

[23] T.-Y. Lin, P. Doll´ar, R. B. Girshick, K. He, B. Hariharan, and
S. J. Belongie. Feature pyramid networks for object detec-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), volume 1, page 3, 2017. 5

[24] S. Liu, S. D. Mello, J. Gu, G. Zhong, M. Yang, and J. Kautz.
Learning afﬁnity via spatial propagation networks.
In Ad-
vances in Neural Information Processing Systems 30: An-
nual Conference on Neural Information Processing Systems
2017, 4-9 December 2017, Long Beach, CA, USA, pages
1519–1529, 2017. 3

[25] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia. Path aggrega-
tion network for instance segmentation. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
8759–8768, 2018. 1, 2

[26] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and
H.-Y. Shum. Learning to detect a salient object.
IEEE
Transactions on Pattern Analysis and Machine intelligence,
33(2):353–367, 2011. 8

[27] Z. Luo, A. K. Mishra, A. Achkar, J. A. Eichel, S. Li, and
P.-M. Jodoin. Non-local deep features for salient object de-
tection. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), volume 2, page 7, 2017. 8

[28] K.-K. Maninis, J. Pont-Tuset, P. Arbel´aez, and L. Van Gool.
Convolutional oriented boundaries: From image segmenta-
tion to high-level tasks. IEEE transactions on Pattern Anal-
ysis and Machine Intelligence, 40(4):819–833, 2018. 2, 5

[29] J. Pont-Tuset, P. Arbelaez, J. T. Barron, F. Marqu´es, and
J. Malik. Multiscale combinatorial grouping for image seg-
mentation and object proposal generation. IEEE Trans. Pat-
tern Anal. Mach. Intell., 39(1):128–140, 2017. 2, 3, 5

3124

International Conference on Computer Vision (ICCV), pages
1859–1868, 2017. 6

[46] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object
proposals from edges. In European Conference on Computer
Vision (ECCV), pages 391–405, 2014. 3

[30] J. Pont-Tuset and L. Van Gool. Boosting object proposals:
From pascal to coco. In IEEE International Conference on
Computer Vision (ICCV), pages 1546–1554, 2015. 6

[31] X. Qi, Z. Liu, J. Shi, H. Zhao, and J. Jia. Augmented feed-
back in semantic segmentation under image level supervi-
sion. In European Conference on Computer Vision (ECCV),
pages 90–105, 2016. 3

[32] M. T. Teichmann and R. Cipolla. Convolutional crfs for
semantic segmentation. arXiv preprint arXiv:1805.04777,
2018. 2, 5

[33] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and
A. W. M. Smeulders. Selective search for object recog-
nition.
International Journal of Computer Vision (IJCV),
104(2):154–171, 2013. 2, 3

[34] P. Vernaza and M. Chandraker. Learning random-walk label
propagation for weakly-supervised semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), volume 3, page 3, 2017. 3

[35] F. Wan, P. Wei, J. Jiao, Z. Han, and Q. Ye. Min-entropy
latent model for weakly supervised object detection.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2018. 3, 6

[36] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-

longie, and P. Perona. Caltech-ucsd birds 200. 2010. 7

[37] Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency detec-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1155–1162, 2013. 8

[38] D. Zhang, J. Han, and Y. Zhang. Supervision by fusion:
Towards unsupervised learning of deep salient object detec-
tor. In IEEE International Conference on Computer Vision
(ICCV), volume 1, page 3, 2017. 8

[39] J. Zhang, T. Zhang, Y. Dai, M. Harandi, and R. Hartley. Deep
unsupervised saliency detection: A multiple noisy labeling
perspective.
In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 9029–9038, 2018. 8

[40] X. Zhang, Y. Wei, J. Feng, Y. Yang, and T. Huang. Adver-
sarial complementary learning for weakly supervised object
localization.
In IEEE conference on computer vision and
pattern recognition (CVPR), 2018. 8

[41] X. Zhang, Y. Wei, G. Kang, Y. Yang, and T. Huang. Self-
produced guidance for weakly-supervised object localiza-
tion. In European Conference on Computer Vision (ECCV),
2018. 8

[42] B. Zhou, A. Khosla, `A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 2921–2929, 2016. 1, 2, 6, 8

[43] Y. Zhou, Y. Zhu, Q. Ye, Q. Qiu, and J. Jiao. Weakly super-
vised instance segmentation using class peak response.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 1, 2, 3, 4, 5, 6, 7

[44] W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimiza-
tion from robust background detection. In IEEE conference
on computer vision and pattern recognition (CVPR), pages
2814–2821, 2014. 8

[45] Y. Zhu, Y. Zhou, Q. Ye, Q. Qiu, and J. Jiao. Soft proposal
IEEE

networks for weakly supervised object localization.

3125

