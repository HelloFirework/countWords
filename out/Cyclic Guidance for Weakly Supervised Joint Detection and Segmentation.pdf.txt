Cyclic Guidance for Weakly Supervised Joint Detection and Segmentation

Yunhang Shen1, Rongrong Ji1

,

2∗, Yan Wang3, Yongjian Wu4, Liujuan Cao1

1Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science

and Engineering, Xiamen University, 361005, China, 2Peng Cheng Laborotory, China

3Microsoft, Redmond, USA

4BestImage, Tencent Technology (Shanghai) Co.,Ltd, China

shenyunhang01@gmail.com, {rrji, caoliujuan}@xmu.edu.cn, wanyan@microsoft.com, littlekenwu@tencent.com

Abstract

Weakly supervised learning has attracted growing re-
search attention due to the signiﬁcant saving in annotation
cost for tasks that require intra-image annotations, such as
object detection and semantic segmentation. To this end, ex-
isting weakly supervised object detection and semantic seg-
mentation approaches follow an iterative label mining and
model training pipeline. However, such a self-enforcement
pipeline makes both tasks easy to be trapped in local min-
imums. In this paper, we join weakly supervised object de-
tection and segmentation tasks with a multi-task learning
scheme for the ﬁrst time, which uses their respective failure
patterns to complement each other’s learning. Such cross-
task enforcement helps both tasks to leap out of their re-
spective local minimums. In particular, we present an ef-
ﬁcient and effective framework termed Weakly Supervised
Joint Detection and Segmentation (WS-JDS). WS-JDS has
two branches for the above two tasks, which share the same
backbone network. In the learning stage, it uses the same
cyclic training paradigm but with a speciﬁc loss function
such that the two branches beneﬁt each other. Extensive
experiments have been conducted on the widely-used Pas-
cal VOC and COCO benchmarks, which demonstrate that
our model has achieved competitive performance with the
state-of-the-art algorithms.

1. Introduction

In recent years, Deep Convolutional Neural Networks
(DCNNs) have demonstrated outstanding capability in var-
ious computer vision tasks, such as image classiﬁcation [2,
63, 37, 33], object detection [26, 54, 53, 45] and semantic
segmentation [46, 13, 12, 81]. The core of the success lies
in the decade-long effort to construct large-scale annotated
datasets, such as ImageNet [17], PASCAL VOC [20], and

∗Corresponding author.

Figure 1. The core idea of the proposd cyclic guidance learning
framework.
Individually trained object detectors and semantic
segmenters often fail on challenging cases, like the bottom-left
ﬁgure shows. However, we found the failure patterns of object
detection and semantic segmentation are complementary, and thus
propose to train a multi-task model to allow them to beneﬁt each
other in a cyclic way. This ﬁgure is best viewed in color.

COCO [43]. However, along with the great potential and
ﬂexibility in tasks like classiﬁcation, the heavy dependency
on the large-scale annotations had two drawbacks. First, hu-
man labeling can be expensive even with crowd sourcing. It
is especially true for tasks that require pixel-level labeling
such as instance segmentation. Although the community
are aware that an ImageNet-like dataset for those tasks will
be of great beneﬁt, it is still absent, and may be prohibitively
expensive even in the near future. Second, compared with
fully annotated datasets, weakly annotated datasets (i.e.,
only image-level labels are annotated) may be much more
widely available and have much larger scale. And recent ex-
periments show that models trained on these datasets with
noisy and incomplete annotations may perform comparable
or even better than models trained on fully annotated but
smaller datasets [47]. Therefore, weakly supervised learn-

697

ing has attracted growing attention [21, 67, 70, 77, 58], es-
pecially for expensive tasks in terms of annotation such as
semantic segmentation and object detection, which aim at
utilizing the widely available datasets in the literatures (such
as PASCAL VOC and COCO) with only image-level label.

Traditional Weakly Supervised Semantic Segmentation
(WSSS) and Weakly Supervised Object Detection (WSOD)
are considered two separate tasks. Both tasks employ a
framework of two-step iterative learning, with one step min-
ing the labels, and the other training the model using the
mined labels. An obvious problem for the framework is
that the model is easy to get trapped in a local minima
[15, 75, 40, 69]. Therefore, the research of WSSS and
WSOD focuses on the introduction of prior and/or regular-
ization [8, 65, 38, 60, 41, 72, 67, 66, 76, 39]. In this paper,
we conquer this challenge from another aspect. Indeed, by
visualizing the ﬁnal as well as intermediate results of the
trained models, we have some interesting observations and
inspirations in the process of exploring the failure patterns
of popular approaches.

As shown in Fig. 2, a WSSS neural network is often
not able to obtain a label map that is consistent with ob-
ject boundaries. Note how the red regions in the second
column differ from the actual object of interest. This is
exactly the reason why popular WSSS approaches have a
graphical model such as Conditional Random Field (CRF)
following the network to reﬁne the result using additional
signals [49, 50, 76, 52, 42, 34, 66, 1, 78, 1]. While CRFs
demonstrate improvement on the pixel map, the quality of
the ﬁnal result heavily relies on the intermediate pixel map
from the network. And thus aforementioned research is still
mainly on the semantic labeling network.

On the other hand,

to effectively take advantage of
image-level annotations, WSOD approaches usually adopt a
two-stage framework with traditional region proposals fol-
lowed by a classiﬁcation network. Different from WSSS,
the presence of region proposal avoids the case that a
bounding box crossing the boundary of an object most of
the time. However, WSOD also has its own problems. As
shown in the bottom-left subﬁgure of Fig. 1, a typical fail-
ure pattern for WSOD is to mis-recognize multiple objects
in the same class as one single object. In some other cases,
WSOD detectors would output an over-tight bounding box
that only cover part of an object.

Very interestingly,

the failure patterns of WSSS and
WSOD are actually complementary. We argue that when
tackling the problem of weakly supervised learning from
image-level label, especially WSSS and WSOD, a multi-
task learning framework is a necessity. On one hand, the im-
perfect pixel map from semantic segmentation can help the
object detector leap from the local minima of over-merged
or over-tight bounding boxes. On the other hand, the bound-
ing boxes from the object detector do not have the problem

Figure 2. Comparison of different failure pattens for weakly super-
vised semantic segmentor and object detector. The four columns
show the original images, semantation maps from CAM [85], ob-
ject localization maps from our detection branch, and reﬁned ob-
ject detection maps, respectively. It is worth noting that localiza-
tion map provides higher quality background cue than the classi-
ﬁcation maps. The right three columns are drawn with jet color
scale, where red color corresponds to high value and blue color
corresponds to low value. This ﬁgure is best viewed in color.

of crossing object boundaries, and thus can provide a rea-
sonably good seed for the semantic segmentation network.
Actually, there are similar ideas emerging in the ﬁeld of
WSOD in the recent two years. For example, Wei et al. [77]
and Diba et al. [19] both introduced three-stage CNNs, in
which the segmentation stage leverages object localization
cues from the classiﬁcation stage. But the approaches did
not explicitly model the mutual beneﬁt of the object detec-
tion task and the semantic segmentation task, and thus are
essentially different from the proposed approach.

In this paper, we present a Weakly Supervised Joint De-
tection and Segmentation (WS-JDS) framework. The core
is a backbone deep network supporting two branches for ob-
ject detection and semantic segmentation, respectively. Re-
garding to model training, we propose a Cyclic Guidance
Learning (CGL) approach, as illustrated in Fig. 1. Similar
with traditional WSSS and WSOD approaches, CGL iter-
atively does label mining and model training. But when
training the object detection branch, we use the bounding
boxes derived from both the ﬁltered region proposal and the
segmentation pixel map as training data. And the localiza-
tion cues from the object detection branch are also used to
supervise the training of the semantic segmentation branch.

To demonstrate the effectiveness of the proposed net-
work as well as the training scheme, we present detailed

698

Figure 3. Overview of CGL for WS-JDS framework. Typical CNN layers are leveraged to extract the intermediate features of an input
image as the backbone.
In detection branch, features for each object proposal are generated by Spatial Pyramid Pooling (SPP) layer
followed by two fully connected layers. A two stream detector [10] is utilized to discovery object instances under image-level supervision.
The object localization map is extracted through gradient-based saliency method via Back-Propagation (BP). In segmentation branch, the
entire feature map are ﬁrstly fed into a fully convolutional sub-network to predict segmentation mask via Atrous Spatial Pyramid Pooling
(ASPP), and then supervised (Sup.) by the rough segmentation masks produced by object localization map. Meanwhile, based on the
segmentation mask, we evaluate the proposals conﬁdence of contained object instance properly.

evaluations on both tasks of object detection and seman-
tic segmentation. The evaluation of object detection is per-
formed on PASCAL VOC 2007, 2010 and 2012 [20], with
comparison with several state-of-the-art methods [23, 70,
68, 77, 72, 82, 24, 58]. We also evaluate our method on
COCO dataset [43] for object detection and the instance
segmentation tasks. On both tasks, we demonstrate com-
petitive performance with the state-of-the-art methods.

2. Related Work

Weakly Supervised Object Detection. WSOD refers
to learning an object detector with only image-level annota-
tions that indicate the presence of an category. Based on the
optimization objective, WSOD methods can be divided into
two groups, i.e., object discovery and instance reﬁnement.

The object discovery approaches optimize the image-
level classiﬁcation loss based on traditional object proposal
directly, i.e., formulate the WSOD problem with a Multi-
Instance Classiﬁcation (MIC) paradigm. The learning step
of MIC alternates between selecting positive samples and
training an appearance model. A number of different strate-
gies to train the MIC model had been proposed in the litera-
ture [15, 73, 9, 75, 60, 23, 57]. Recent approaches combined
Convolutional Neural Networks (CNNs) and MIC into a
uniﬁed framework [10, 19]. Contextual information was
introduced to achieve promising improvement [38]. Re-
cently, Tang at al. [70] proposed to replace traditional ob-
ject proposal extraction stage by generating and reﬁning ob-
ject proposals in an end-to-end framework. There are some
methods focused on proposal-free paradigms by taking ad-
vantage of deep feature map [7, 5, 87] and class activation
maps [85, 28, 83]. However, this paradigm seriously de-
pends on the quality of feature maps and is hard to dis-
tinguish different instances in challenging scenes. Some
work also used additional annotations and data to improve
the performance, e.g., object size estimation [60], instance

count annotation [22], video motion cue [64] and human
veriﬁcation [48]. Some of the additional data may be from
a different domain. Therefore, knowledge transfer for pro-
gressive cross-domain adaptation was also exploited, e.g.,
data domain adaption [59] and task domain adaption [35].

The instance reﬁnement approaches also follow the
bounding box mining and model training framework. But
instead of optimizing a MIC loss, they optimize the objec-
tive function of instance-level localization. Therefore, from
another prospect, they have an additional instance reﬁne-
ment stage after object discovery by introducing a fully su-
pervised detector. For example, the work in [40, 36, 22, 69]
mined the high-conﬁdence proposals and treated them as
positive samples to train a fully supervised model. Many ef-
forts [84, 24] had been made to mining high-quality bound-
ing boxes. To further improve the robustness, there are
some works that combined the weakly supervised MIC
model and fully supervised detectors. For example, Tang
et al. [69] introduced multiple supervised branches to reﬁne
the result from weakly supervised model. Work in [41, 72]
proposed min-entropy prior to alleviate the ambiguity of re-
sult and used the pseudo ground-truth object to optimize
the objective function of localization. Zhang et al. [82] pro-
posed to estimate sample-wise training difﬁculty to learn a
fully supervised detector in an easy-to-difﬁcult order.

Weakly Supervised Semantic Segmentation. WSSS
methods can also been divided into two groups. The ﬁrst
group [50, 61, 55, 86] leverages CNN built-in pixel-level
cues and constraint priors to learn segmentation masks,
while a common practice for the second group [39, 78, 1,
34] is to treat initial object localization cues, (which is often
produced by classiﬁcation networks,) as pseudo supervision
and train a fully supervised segmentation network.

In the ﬁrst group, Pathak et al. [50] proposed a con-
strained CNN, which applied linear constraints on the struc-
tured output space of pixel labels. Saleh et al. [55] extracted
the built-in masks directly from the hidden layer activation,

699

and incorporated the resulting masks via a weakly super-
vised loss. There are also works that derive category-wise
saliency maps from intermediate feature maps of CNNs to
estimate the segmentation masks [61, 86].

In the second group, popular methods [51, 11, 21] lever-
aged object saliency map or feature activation map to pro-
vide complimentary information. Many priors or regular-
ization [39, 66, 67] were proposed to improve the segmenta-
tion result. Different kinds of supervisions were exploited:
web data [32, 56], bounding boxes [79], scribbles [42],
points [6], etc. There are also work [76, 52] that focused
on improving feature learning in iterative frameworks. Re-
cently, various approaches based on iteratively mining com-
mon feature [74], seeded region growing [34], random-walk
label propagation [71], dilated convolution [78] and pixel-
level semantic afﬁnity [1] were proposed.

Multi-task Learning. Learning detection and segmen-
tation jointly was ﬁrst employed by Hariharan et al. [29] in
fully supervised learning. Although the framework in [29]
was multi-stages, it still showed improvement of perfor-
mance on individual task. He et al. [30] also demonstrated
that box detection can beneﬁt from multi-task learning. Re-
cent works provided more complex mechanism to com-
bine the two tasks with the assistance of direction predic-
tion [12], and information ﬂow boosting [44]. In weakly
supervised learning, some related work used segmentation
masks to boost the performance of detection task [19, 77].
However, different to those works, our method also exploits
to improve segmentation branch with detection result via
CGL. And in weakly supervised setting, ours is the ﬁrst to
join object detection and semantic segmentation tasks.

3. The Proposed Method

Overview: The overall architecture of the proposed ap-
proach is illustrated in Fig. 3. Sharing the same back-
bone, which is VGG16 [63], the proposed model has two
branches, i.e., object detection and semantic segmentation.
In particular, the object detection branch, built on top of
spatial pyramid pooling layer, produces box prediction and
object localization map. Following the previous weakly su-
pervised semantic segmentation approaches [78, 39, 55], we
leverage the inferred localization maps to produce pseudo-
ground-truth of segmentation masks from training images,
which are then used as supervision to train the segmenta-
tion branch. The predicted conﬁdence masks from the seg-
mentation branch are then employed to evaluate object pro-
posals on the likelihood of containing the object instance,
which in turn beneﬁts the object detection branch.

Object Detection Branch: We employ WSDDN [10]
for the object detection branch and further improve the per-
In particular, let I ∈
formance using the CGL scheme.
ℜH×W ×3 be an input image, t ∈ {0, 1}C be the corre-
sponding image-level labels, and C be the total number

of categories. H and W are the image height and width,
respectively. As illustrated by the gray region in Fig. 3,
we ﬁrst extract feature of R object proposals {p1 . . . pR}
from the VGG backbone, and then the feature from the
Spatial Pyramid Pooling (SPP) layer [31] is forked into
two streams, i.e., classiﬁcation stream and detection stream,
producing two score matrices X c, X d ∈ RR×C by two
fully-connected layers, respectively. Both score matrices
are normalized by softmax functions σ(·) over categories
and proposals, respectively. Then the element-wise prod-
uct of the output of the two streams is again a score matrix:
X s = σ(X c) ⊙ σ(X d). To acquire image-level classiﬁ-
cation scores, a sum pooling is applied: yk = PR
rk.
Then we obtain a cross-entropy loss function Ldet:

r=1 X s

Ldet =

C

X

k=1

ntk log yk + (1 − tk) log(1 − yk)o,

(1)

where tk is the ground truth labels of whether an object of
category k is presented in the image I.

Prior Guidance: However, such object discovery opti-
mization lacks prior guidance. Note X s is calculated based
on local information of each individual proposal [58]. The
correlation among instances is typically ignored and the op-
timization might converge to an undesirable local minimum
during MIC learning [3]. Recent work in [38, 77] proposed
to use contextual information as a supervisory guidance,
which enforces the predicted object region to be compati-
ble with its surrounding context. We propose to leverage
knowledge of the learned masks from segmentation branch
to reﬁne the detection via objectness prior [77, 58].

Semantic Segmentation Branch: In order to obtain the
segmentation masks, we ﬁrst collect the intermediate fea-
tures before the last pooling layer of the backbone network.
Then we feed it to the convolutional blocks with multiple
dilated rates to localize object-related regions perceived by
different receptive ﬁelds, similar to DeepLab-ASPP [14],
as illustrated by the blue region in Fig. 3. With the pro-
duced object localization cues, we train the segmentation
branch with pixel-wise loss Lseg, which is widely adopted
by fully supervised schemes. Different from previous lit-
erature [39, 55, 14, 78] of fully/weakly supervised seman-
tic segmentation, when applying Fully Convolutional Net-
works (FCNs) to semantic segmentation, which typically
uses a per-pixel softmax and a multinomial cross-entropy
loss, we use a per-pixel Sigmoid and a binary cross-entropy
loss. Then Lseg is similar to Ldet, but with additional spatial
dimensions:

C, ˆH, ˆW

k,h,w

X

Lseg =

nM k

hw log Sk

hw + (1 − M k

hw)o,
(2)
where M and S denote the rough segmentation masks pro-
duced by detection branch and the predicted masks with C

hw) log(1 − Sk

700

Figure 4. Visualization of the intermediate steps of the proposed CGL. The rows show input image, object localization map from object
detection branch, rough segmentation map derived from the localization map, and output of segmentation branch without CRF post-
processing, respectively. Red, blue, and green pixels in the third row indicate foreground, background and uncertain, respectively.
channels, respectively. And ˆH, ˆW denote the image height
and width of the predicted masks, which is usually 1/16 of
H, W . Besides, we also use a constrain-to-boundary loss
from [39] to encourage segmentation masks to match up
with object boundaries.

Our deﬁnition of Lseg allows the segmentation network
to generate masks for every category without competition
among categories. We rely on the dedicated detection
branch to predict the category label used to select the out-
put masks. As demonstrated in [30], by using this decou-
pled mask and category prediction, once the instance has
been classiﬁed as a whole (by the detection branch), it is
sufﬁcient to predict binary masks without concern for the
categories, which makes the model easier to train.
Cyclic Guidance Learning: Theoretically,

the loss
functions of WSOD and WSSS lead to complementary
failure patterns. On one hand, most works formulate the
WSOD problem with an MIC paradigm. Its explicit penalty
on false positives from negative bags gives WSOD low false
positive rate. However, to prevent self-reinforcing into a lo-
cal minimum, popular loss only penalizes conﬁdent false
negatives (which gives limited pseudo-ground-truth) with
an IoU less than a threshold (which compromises sensitiv-
ity). Consequently, WSOD usually suffers from ambiguous
feature maps around non-discriminative parts of objects. On
the other hand, for WSSS, the loss is deﬁned on pixel-level.
The lack of explicit penalty on false positives often results
in noisy background. But the ﬁne granularity gives better it
precision on ambiguous regions to guide object localizer.

We propose a CGL scheme to exploit complementary
knowledge learned by individual tasks, as illustrated by

the blue dashed line in Fig. 3.
For the detection-to-
segmentation guidance, we leverage the inferred localiza-
tion maps to produce rough segmentation masks M in Eq. 2,
which are then used as supervision to train the segmenta-
tion branch. Different from [19, 77] that introduced extra
saliency detection and classiﬁcation branches to generate
localization maps [85, 50], we produce built-in background
and foreground cues from the detection branch through
gradient-based saliency detection following [62, 39], which
has the beneﬁt of parameter free. In particular, the gradi-
ent of classiﬁcation score ﬂows from detection branch to
the ﬁrst layer of backbone by back-propagation, which is
illustrated in the second row of Fig. 4. On the object local-
ization maps, we assign the pixels with values larger than
a pre-deﬁned normalized threshold (i.e., 0.1) with the cor-
responding category label as the foreground regions. We
also choose pixels with low normalized value (i.e., 0.005)
as background sample. The remaining pixels are marked
as uncertain and ignored during training. The result fore-
ground, background and uncertain pixels from sample im-
ages are illustrated in the third row of Fig. 4.

The last row of Fig. 4 illustrates that the output of seg-
mentation branch is able to generalize object localization
seed to predict uncertain pixels, which in turn provides
guidance to the detector training. For example, a false pos-
itive detection occurs in the ﬁfth column, when the detec-
tion branch fails to discover multiple instance of category
horse existed in the image. And therefore object localiza-
tion map is half-baked (second row). In this case, the image-
level annotation cannot correct this problem, which ends
up with a pseudo ground truth of segmentation with most

701

Table 1. Object detection on PASCAL VOC 2007 in terms of AP (%) on test set.

Method

aero

bicy

bird

boa

bot

bus

car

cat

cha

cow

dtab

dog

hors mbik

pers

plnt

she

sofa

trai

tv

WSDDN [10]
WSDDN∗

WSOD || WSSS
WSOD → WSSS
WSOD ← WSSS
WSOD ⇋ WSSS

39.4
45.9

50.1
52.4
39.8
52.0

50.1
48.1

56.3
63.5
61.4
64.5

31.5
32.4

32.4
28.8
34.6
45.5

16.3
13.3

22.7
16.1
18.1
26.7

12.6
23.0

19.0
27.3
27.3
27.9

64.5
61.7

51.8
58.0
66.1
60.5

42.8
51.1

41.1
55.8
52.9
47.8

42.6
40.7

62.6
41.6
50.8
59.7

10.1
16.8

2.7
22.5
15.6
13.0

35.7
37.9

45.3
47.3
43.0
50.4

24.9
23.8

45.6
14.0
42.4
46.4

38.2
28.4

24.4
25.9
46.1
56.3

34.4
43.1

43.7
8.5
19.4
49.6

55.6
53.0

56.0
55.2
57.9
60.7

9.4
6.5

12.4
18.9
30.5
25.4

14.7
21.1

20.5
22.1
24.2
28.2

30.2
41.2

38.1
46.9
44.0
50.0

40.7
44.0

34.8
45.0
48.2
51.4

54.7
60.6

53.2
54.8
64.8
66.5

46.9
45.9

33.4
49.9
52.9
29.7

Av.

34.8
36.9

37.3
37.7
42.0
45.6

pixels marked as background for the second instance (third
row). However, the segmentation branch is able to predict
a coarse mask properly to overturn the mistake (last row).
Thus the segmentation map provides supervisor guidance
to reﬁne the detector. Another example of object detection
branch beneﬁts segmentation branch is illustrated in the last
column of Fig. 4, when the segmentation branch fails to
predict coarse mask properly of dog existed in the image,
the detection branch provides conservative seed of dog and
clear supervision of background.

For the segmentation-to-detection guidance, we treat the
learned masks as localization prior to reﬁne the proposal
classiﬁcation following [58]. In particular, the masks pro-
vide contextual information for each proposal. Given the
r-th proposal and the k-th category, the conﬁdent weight is
estimated from the masks S:

Wrk =

1

p|pr| X

i,j∈pr

T (Sk

ij) −

1
p|pc

r| X

i,j∈pc

r

T (Sk

ij),

(3)

ij) = 1[Sk

ij ≥ 10−1 · max Sk], pr and pc

where T (Sk
r is the
r-th proposal and the corresponding contextual region. The
contextual region pr is deﬁned as the surrounding regions
of pr by scaling the box by a factor of 1.8 [25]. Therefore,
before computing image-level score with sum pooling, we
reﬁne the predicted proposal score X s with W by element-
wise (Hadamard) product. And we get the reﬁned cross-
entropy loss function as:

Lr

det =

C

X

k=1

ntk log yr

k + (1 − tk) log(1 − yr

k)o,

(4)

k = PR

r=1 WrkX s

where yr
rk. However, mask S is unstable
in early training iterations. Therefore, we also use object
localization map to reﬁne the proposal classiﬁcation.

During the testing stage, we run the box prediction
branch on these proposals, followed by a non-maximum
suppression. At the same time, the mask prediction branch
outputs the segmentation masks for the entire image. Then
we extract the masks of detection boxes.

4. Experimental Evaluation

4.1. Datasets and Evaluation Protocol

Dataset. We evaluate the proposed approach on PAS-
CAL VOC 2007, 2010, 2012 [20] and COCO [43], which
are widely-used benchmark datasets. PASCAL VOC 2007

consists of 2, 501 training images, 2, 510 validation images,
and 4, 092 test images over 20 categories. PASCAL VOC
2010 consists of 4, 998 training images, 5, 105 validation
images, and 9, 637 test images over 20 categories. PASCAL
VOC 2012 consists of 5, 717 training images, 5, 823 vali-
dation images, and 10, 991 test images over 20 categories.
Following the standard settings of weakly supervised object
detection, we use both training and validation sets with only
image-level labels for training. The performance of the lo-
calization task, deﬁned as predicting boxes when categories
are known, is evaluated on the training and validation sets,
and the performance of the detection task, deﬁned as pre-
dicting categories and boxes simultaneously, is evaluated on
the testing set, following common practice [18, 10]. Note
our evaluation settings are more challenging than some pop-
ular approaches, such as [15, 8, 9], which removed the hard
images containing only truncated and difﬁcult objects. We
also evaluate our approach on the MS COCO dataset [43],
which is among the most challenging datasets for instance
segmentation and object detection. It consists of 80 object
categories with pixel-wise instance mask annotations. Our
experiments involve the 115k training set, 5k validation set.
Only image-level annotations are used in training.

Evaluation Protocols. Two protocols are used for evalu-
ation: CorLoc and mean Average Precision (mAP). CorLoc
is a commonly used measurement that quantiﬁes localiza-
tion performance by the percentage of images that contain
at least one object instance with at least 50% overlapped
to the ground-truth. CorLoc indicates the ratio of images
in which a method correctly localizes an object of the tar-
get category according to the PASCAL-criterion. The mAP
follows standard PASCAL VOC protocol to report the mAP
at 50% Intersection-over-Union (IoU) of the detected boxes
with the ground-truth. We evaluate the CorLoc and mAP on
the training/validation and testing splits, respectively. For
MS COCO data, we also report the standard COCO metrics
including AP (averaged over IoU thresholds), AP50, AP75,
APS, APM , and APL (AP at different scales). We use the
superscripts r and b for object detection AP and instance
segmentation AP, respectively.

4.2. Implementation Details

The proposed approach is implemented using Caffe2.
Both Python and C++ interfaces are used. For the backbone
network, we use VGG16 [63] that is initialized with the
weights pretrained on ImageNet [17]. We use WSDDN [10]
as our baseline model for the WSOD branch.

702

Table 2. Object detection on PASCAL VOC 2007 in terms of AP (%) on test set.

Method

aero

bicy

bird

boa

bot

bus

car

cat

cha

cow

dtab

dog

hors mbik

pers

plnt

she

sofa

trai

tv

WSDDN VGG16
WCCN
Jie et al.
OICR-VGG16
SPAM-CAM
TST
TS2C
Ge et al.
Tang et al.
WS-JDS

[10]
[19]
[36]
[69]
[28]
[59]
[77]
[23]
[70]

[69]
OICR FRCNN
[72]
MELM
[82]
ZLDN
[24]
Ge et al.
[84]
W2F
TS2C FRCNN
[77]
Tang et al. FRCNN [70]
WS-JDS FRCNN

39.4
49.5
52.2
58.0

-
-

59.3
49.1
57.9
52.0

65.5
55.6
55.4
64.3
63.5

-

63.0
64.8

50.1
60.6
47.1
62.4

-
-

57.5
53.6
70.5
64.5

67.2
66.9
68.5
68.0

-

69.7
70.7

31.5
38.6
35.0
31.1

-
-

43.7
43.5
37.8
45.5

47.2
34.2
50.1
56.2
50.5

-

40.8
51.5

16.3
29.2
26.7
19.4

-
-

27.3
21.3
5.7
26.7

21.6
29.1
16.8
36.4
31.9

-

11.6
25.1

12.6
16.2
15.4
13.0

-
-

13.5
18.5
21.0
27.9

22.1
16.4
20.8
23.1
14.4

-

27.7
29.0

64.5
70.8
61.3
65.1

-
-

63.9
66.9
66.1
60.5

68.0
68.8
62.7
68.5
72.0

-

70.5
74.1

42.8
56.9
66.0
62.2

-
-

61.7
64.0
69.2
47.8

68.5
68.1
66.8
67.2
67.8

-

74.1
69.7

42.6
42.5
54.3
28.4

-
-

59.9
55.6
59.4
59.7

35.9
43.0
56.5
64.9
73.7

-

58.5
69.6

10.1
10.9
3.0
24.8

-
-

24.1
11.9
3.4
13.0

5.7
25.0
2.1
7.1
23.3

-

10.0
12.7

35.7
44.1
53.6
44.7

-
-

46.9
53.7
57.1
50.4

63.1
65.6
57.8
54.1
53.4

-

66.7
69.5

24.9
29.9
24.7
30.6

-
-

36.7
26.6
57.3
46.4

49.5
45.3
47.5
47.0
49.4

-

60.6
43.9

38.2
42.2
43.6
25.3

-
-

45.6
45.6
35.2
56.3

30.3
53.2
40.1
57.0
65.9

-

34.7
54.9

34.4
47.9
48.4
37.8

-
-

39.9
48.7
64.2
49.6

64.7
49.6
69.7
69.3
57.2

-

75.7
39.3

55.6
64.1
65.8
65.5

-
-

62.6
64.6
68.6
60.7

66.1
68.6
68.2
65.4
67.2

-

70.3
71.3

9.4
13.8
6.6
15.7

-
-

10.3
20.4
32.8
25.4

13.0
2.0
21.6
20.8
27.6

-

25.7
32.6

14.7
23.5
18.8
24.1

-
-

23.6
23.3
28.6
28.2

25.6
25.4
27.2
23.2
23.8

-

26.5
29.8

30.2
45.9
51.9
41.7

-
-

41.7
50.0
50.8
50.0

50.0
52.5
53.4
50.7
51.8

-

55.4
57.0

40.7
54.1
43.6
46.9

-
-

52.4
44.7
49.5
51.4

57.1
56.8
56.1
59.6
58.7

-

56.4
61.0

54.7
60.8
53.6
64.3

-
-

58.7
55.9
41.1
66.5

60.2
62.1
52.5
65.2
64.0

-

55.5
66.6

46.9
54.5
62.4
62.6

-
-

56.6
60.6
30.0
29.7

59.0
57.1
58.2
57.0
62.3

-

54.9
57.4

Table 3. Object localization on PASCAL VOC 2007 in terms of CorLoc (%) on trainval set.

Method

aero

bicy

bird

boa

bot

bus

car

cat

cha

cow

dtab

dog

hors mbik

pers

plnt

she

sofa

trai

tv

WSDDN VGG16
WCCN VGG16
Jie et al.
OICR-VGG16
SP-VGGNet
TST
TS2C
Ge et al.
Tang et al.
WS-JDS

[10]
[19]
[36]
[69]
[87]
[59]
[77]
[23]
[70]

[69]
OICR FRCNN
[82]
ZLDN
W2F
[84]
Tang et al. FRCNN [70]
WS-JDS FRCNN

65.1
83.9
72.7
81.7
85.3

–

84.2
75.9
77.5
82.9

85.8
74.0

–

83.8
79.8

58.8
72.8
55.3
80.4
64.2

–

74.1
67.6
81.2
74.0

82.7
77.8

–

82.7
84.0

58.5
64.5
53.0
48.7
67.0

–

61.3
62.2
55.3
73.4

62.8
65.2

–

60.7
68.3

33.1
44.1
27.8
49.5
42.0

–

52.1
37.3
19.7
47.1

45.2
37.0

–

35.1
40.2

39.8
40.1
35.2
32.8
16.4

–

32.1
36.6
44.3
60.9

43.5
46.7

–

53.8
61.5

68.3
65.7
68.6
81.7
71.0

–

76.7
71.5
80.2
80.4

84.8
75.8

–

82.7
80.5

60.2
82.5
81.9
85.4
64.7

–

82.9
80.2
86.6
77.5

87.0
83.7

–

88.6
85.8

59.6
58.9
60.7
40.1
88.7

–

66.6
63.8
69.5
78.8

46.8
58.8

–

67.4
75.8

34.8
33.7
11.6
40.6
20.7

–

42.3
19.7
10.1
18.6

15.7
17.5

–

22.0
29.7

64.5
72.5
71.6
79.5
63.8

–

70.6
70.6
87.7
70.0

82.2
73.1

–

86.3
77.7

30.5
25.6
29.7
35.7
58.0

–

39.5
32.4
68.4
56.7

51.0
49.0

–

68.8
49.5

43.0
53.7
54.3
33.7
84.1

–

57.0
56.1
52.1
67.0

45.6
51.3

–

50.9
67.4

56.8
67.4
64.3
60.5
84.7

–

61.2
67.8
84.4
64.5

83.7
76.7

–

90.8
58.6

82.4
77.4
88.2
88.8
80.0

–

88.4
81.7
91.6
84.0

91.2
87.4

–

93.6
87.4

25.5
26.8
22.2
21.8
60.0

–
9.3
35.9
57.4
47.0

22.2
30.6

–

44.0
66.2

41.6
49.1
53.7
57.9
29.4

–

54.6
50.9
63.4
50.1

59.7
47.8

–

61.2
46.6

61.5
68.1
72.2
76.3
56.3

–

72.2
73.4
77.3
71.9

75.3
75.0

–

82.5
78.5

55.9
27.9
52.6
59.9
68.1

–

60.0
50.4
58.1
57.6

65.1
62.5

–

65.9
73.7

65.9
64.5
68.9
75.3
77.4

–

65.0
66.0
57.0
83.3

76.8
64.8

–

71.1
84.5

63.7
55.7
75.5
81.4
30.5

–

70.3
66.8
53.8
43.5

78.1
68.8

–

76.7
72.8

Av.

34.8
42.8
41.7
41.2
27.5
33.8
44.3
43.9
45.3
45.6

47.0
47.3
47.6
51.2
52.4
48.0
50.4
52.5

Av.

53.5
56.7
56.1
60.6
60.6
59.5
61.0
58.3
63.8
64.5

64.3
61.2
70.3
68.4
68.6

Table 4. Object detection and localization on PASCAL VOC 2010
and 2012 in terms of mAP (%) and CorLoc (%).

2010

2012

mAP (%)

CorLoc (%) mAP (%)

CorLoc (%)

Method

Multi-Fold MIL
OICR-VGG16
Jie et al.
TS2C
Tang et al.

[16]
[69]
[36]
[77]
[70]

27.4

55.2

–
–
–
–

–
–
–
–

WS-JDS

39.9

63.1

[69]
OICR FRCNN
[72]
MELM
[82]
ZLDN
[84]
W2F
TS2C FRCNN
[77]
Tang et al. FRCNN [70]

–
–
–
–
–
–

–
–
–
–
–
–

WS-JDS FRCNN

45.7

68.1

–

37.9
38.3
40.0
40.8

39.1

42.5
42.4
42.9
47.8
44.4
45.7

46.1

–

62.1
58.8
64.4
64.9

63.5

65.6

–

61.5
69.4

–

69.3

69.5

Training. We use a mini-batch size of 128, learning rate
of 0.001, momentum of 0.9, and dropout rate of 0.5. We use
a step learning rate decay schema with decay weight γ =
0.1 and step size of 20 epochs. In the multi-scale setting, we
use ﬁve scales {480, 576, 688, 864, 1200}. To improve the
robustness, we randomly adjust the exposure and saturation
of the images by up to a factor of 1.5 in the HSV space.
And a random crop with 0.9 of the original images size is
applied. We use MCG [4] to generate object proposals for
all experiments, including our implementation of baseline
methods. We set the max number of region proposals in an
image to be 2, 048. All models are trained for 30 epochs.
We apply Xavier [27] and Gaussian initialization to the new

convolutional and fully-connected layers, respectively.

Testing. The learned detectors are evaluated in two
paradigms, following [40, 19, 69, 24, 77, 58, 70]: The
ﬁrst paradigm directly applies the learned detectors on the
testing images and outputs the scores for each region pro-
posal as the detection results. The second paradigm labels
bounding boxes in training/validation images using WSOD,
and then uses these bounding boxes as pseudo ground-
truth to train the fully supervised detector, which is Fast-
RCNN [26] in our case, for testing.
In this scenario, for
each category, we treat the proposal with maximum detec-
tion score as the pseudo ground-truth bounding box. The
test scores are the average of all scales and ﬂips. Detec-
tion results are post-processed by non-maximum suppres-
sion using a threshold of 0.5 IoU. The predicted masks are
upsampled to match the size of the input image, and then
apply a fully-connected CRF to reﬁne the result.

4.3. Comparison to Baselines

To demonstrate the necessity and beneﬁt of learning
WSSS and WSOD models simultaneously, we compare our
full framework with baseline models with different designs
removed in Tab. 1. The ﬁrst variation (WSOD || WSSS)
trains two tasks independently with the shared backbone
network. The second (WSOD → WSSS) and third (WSOD
← WSSS) variations employ only one direction guidance,
i.e., the detection-to-segmentation or the segmentation-to-
detection guidance, respectively. The fourth (WSOD ⇋

703

Table 5. Instance segmentation and object detection on minival set of COCO.

Method

APr

APr

50

APr

75

APr

S

APr

M APr

L

APb

APb

50

APb

75

APb

S

APb

M APb

L

WSDDN∗

ContextLocNet∗

WS-JDS

BB
ELL
MCG

BB
ELL
MCG

3.4
4.5
5.2

4.2
4.7
5.5

6.1

9.5
10.3
10.7

10.3
10.6
10.9

11.7

2.9
4.3
5.1

4.1
4.4
5.3

5.5

1.0
1.3
1.8

2.1
1.3
2.0

1.5

3.7
4.3
6.3

5.3
5.4
6.7

7.1

9.2
9.1
12.0

10.1
10.0
11.9

9.5

19.2

8.2

2.1

10.4

17.2

9.9

19.4

8.7

2.1

10.8

17.9

12.2

10.5

20.3

9.2

2.2

10.9

18.3

WSSS) one is our full CGL scheme. The performance
of WSOD is only slightly improved in WSOD || WSSS
and WSOD → WSSS compared with WSDDN, which is
mainly because that the WSOD model is trained indepen-
dently without guidance from WSSS in these two base-
lines. The performance boost mainly beneﬁts from sharing
the backbone network of multi-task learning. In WSOD ←
WSSS, the performance of WSOD is signiﬁcantly improved
by exploiting segmentation pixel maps to reﬁne the mined
supervision. Finally, the proposed CGL scheme (WSOD
⇋ WSSS) further improves the performance by combin-
ing guidance from both directions. As comparing to WS-
DDN [10], Tab. 1 shows that our model reaches 45.6% mAP
for weakly supervised object detection. Although our re-
production of WSDDN on VGG16 backbone (WSDDN∗)
is superior to the original WSDDN, our method still out-
performs this baseline with a large margin. Experimental
results demonstrate that the complementary knowledge of
detection and segmentation can beneﬁt individual training.

4.4. Comparison to the State of the Arts

PASCAL VOC. We divide the compared WSOD meth-
ods into two categories: object discovery and instance re-
ﬁnement based methods, as mentioned in Sec. 2 and in the
ﬁrst and second parts in Tab. 2 3. For fair comparison, we
do not include methods that use additional data [60, 64, 22].
For object discovery, we compare our method with the state
of the arts, including ZLDN [82], MELM [72], TS2C [77],
OICR [69] among others. The proposed model reaches
45.6% mAP, and achieves state-of-the-art performance. It
is worth noting that the improvement from our framework
is orthogonal to those works, so the proposed CGL frame-
work can also beneﬁt from all the techniques proposed in
the aforementioned literatures. For instance reﬁnement, we
also train a Fast-RCNN [26] with the pseudo ground-truth
localization extracted from our weakly supervised detec-
tors. We achieve a performance of 52.5% mAP, which is su-
perior to previous work in [40, 80, 69, 72, 82, 58, 77, 70, 84]
with gain of about 0.1 ∼ 5.5% in Tab. 2. We further con-
duct experiments on PASCAL VOC 2010 and 2012. Tab. 4
shows our method consistently achieves competitive perfor-
mance to the state-of-the-art approaches on all metrics.

COCO. With the proposed technique, we perform in-
stance segmentation on the COCO, which is more chal-
lenging than the PASCAL VOC. To the best of our knowl-

edge, this is the ﬁrst work reporting results for image-
level supervised instance segmentation on COCO. We con-
struct several baselines based on object bounding boxes ob-
tained from weakly supervised localization methods follow-
ing [86]. We use three mask extraction strategies: The
ﬁrst strategy uses the entire bounding boxes as the instance
masks (BB). The second strategy ﬁts a maximum ellipse
on the bounding boxes (ELL). The third strategy retrieves a
max overlap segmentation mask in MCG with the bounding
boxes (MCG). As illustrated in Tab. 5, our method achieves
better performance in termed of APr compared with all
other methods in the instance segmentation task. We also
report performance of the object detection task on COCO.
The proposed approach outperforms the baselines methods
by 1.0% and 0.6% in APb, respectively.

5. Conclusion

In this paper, we propose a multi-task learning frame-
work for the problems of weakly supervised object detec-
tion and semantic segmentation. We found that the differ-
ent failure patterns of the two tasks can actually beneﬁt each
other and alleviate the problem of the optimization getting
stuck in local minimum. To leverage the complementary
knowledge learned by the two tasks, we further propose
a Cyclic Guidance Learning scheme. In this scheme, the
detection branch provides a reasonably good seed for seg-
mentation branch, while the learned masks help the detector
to leap from local minimum. On the widely-used bench-
marks of Pascal VOC and COCO, the proposed method
achieves competitive or superior performance to state-of-
the-art methods in both weakly supervised object detection
and instance segmentation tasks.

6. Acknowledgment

This work is supported by the National Key R&D Pro-
gram (No.2017YFC0113000, and No.2016YFB1001503),
Nature Science Foundation of China (No.U1705262,
No.61772443, and No.61572410), Post Doctoral Innova-
tive Talent Support Program under Grant BX201600094,
China Post-Doctoral Science Foundation under Grant
2017M612134, Scientiﬁc Research Project of National
Language Committee of China (Grant No. YB135-49), and
Nature Science Foundation of Fujian Province, China (No.
2017J01125 and No. 2018J01106).

704

References

[1] J. Ahn and S. Kwak. Learning Pixel-level Semantic Afﬁnity
with Image-level Supervision for Weakly Supervised Seman-
tic Segmentation. In CVPR, 2018.

[2] K. Alex, I. Sutskever, and G. E. Hinton.

Imagenet classi-
ﬁcation with deep convolutional neural networks. In NIPS,
2012.

[3] J. Amores. Multiple instance classiﬁcation: Review, taxon-

omy and comparative study. AI, 2013.

[4] P. Arbel´aez, J. Pont-Tuset, J. Barron, F. Marques, and J. Ma-

lik. Multiscale Combinatorial Grouping. In CVPR, 2014.

[5] L. Bazzani, A. Bergamo, D. Anguelov, and L. Torresani.
In

Self-Taught Object Localization with Deep Networks.
WACV, 2016.

[6] A. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei.
What’s the Point: Semantic Segmentation with Point Super-
vision. In ECCV, 2016.

[7] A. J. Bency, H. Kwon, H. Lee, S. Karthikeyan, and B. S.
Manjunath. Weakly Supervised Localization using Deep
Feature Maps. In ECCV, 2016.

[8] H. Bilen, M. Pedersoli, and T. Tuytelaars. Weakly Super-
In

vised Object Detection with Posterior Regularization.
BMVC, 2014.

[9] H. Bilen, M. Pedersoli, and T. Tuytelaars. Weakly supervised

object detection with convex clustering. In CVPR, 2015.

[10] H. Bilen and A. Vedaldi. Weakly Supervised Deep Detection

Networks. In CVPR, 2016.

[11] A. Chaudhry, P. K. Dokania, and P. H. S. Torr. Discovering
Class-Speciﬁc Pixels for Weakly-Supervised Semantic Seg-
mentation. In BMVC, 2017.

[12] L.-C. Chen, A. Hermans, G. Papandreou, F. Schroff,
P. Wang, and H. Adam. MaskLab: Instance Segmentation
by Reﬁning Object Detection with Semantic and Direction
Features. In CVPR, 2018.

[13] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic Image Segmentation with Deep Con-
volutional Nets and Fully Connected CRFs. In ICLR, 2015.

[14] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. DeepLab: Semantic Image Segmentation with
Deep Convolutional Nets, Atrous Convolution, and Fully
Connected CRFs. TPAMI, 2017.

[15] R. G. Cinbis, J. Verbeek, and C. Schmid. Multi-fold MIL
In

Training for Weakly Supervised Object Localization.
CVPR, 2014.

[16] R. G. Cinbis, J. Verbeek, and C. Schmid. Weakly Super-
vised Object Localization with Multi-fold Multiple Instance
Learning. TPAMI, 2015.

[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR, 2009.

[18] T. Deselaers, B. Alexe, and V. Ferrari. Weakly supervised lo-
calization and learning with generic knowledge. IJCV, 2012.

[20] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The Pascal Visual Object Classes (VOC)
Challenge. IJCV, 2010.

[21] R. Fan, Q. Hou, M.-M. Cheng, G. Yu, R. R. Martin, and S.-
M. Hu. Associating Inter-Image Salient Instances for Weakly
Supervised Semantic Segmentation. In ECCV, 2018.

[22] M. Gao, A. Li, R. Yu, V. I. Morariu, and L. S. Davis. C-WSL:
In ECCV,

Count-guided Weakly Supervised Localization.
2018.

[23] C. Ge and J. Wang. Fewer is More : Image Segmentation
Based Weakly Supervised Object Detection with Partial Ag-
gregation. In BMVC, 2018.

[24] W. Ge, S. Yang, and Y. Yu. Multi-Evidence Filtering and
Fusion for Multi-Label Classiﬁcation, Object Detection and
Semantic Segmentation Based on Weakly Supervised Learn-
ing. In CVPR, 2018.

[25] S. Gidaris and N. Komodakis. Object detection via a multi-
In

region & semantic segmentation-aware CNN model.
ICCV, 2015.

[26] R. Girshick. Fast R-CNN. In ICCV, 2015.
[27] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
2010.

[28] A. Gudi, N. van Rosmalen, M. Loog, and J. van Gemert.
Object-Extent Pooling for Weakly Supervised Single-Shot
Localization. In BMVC, 2017.

[29] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Simul-

taneous Detection and Segmentation. In ECCV, 2014.

[30] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask R-

CNN. In ICCV, 2017.

[31] K. He, X. Zhang, S. Ren, and J. Sun. Spatial Pyramid Pool-
ing in Deep Convolutional Networks for Visual Recognition.
In ECCV, 2014.

[32] S. Hong, D. Yeo, S. Kwak, H. Lee, and B. Han. Weakly
Supervised Semantic Segmentation using Web-Crawled
Videos. In CVPR, 2017.

[33] G. Huang, Z. Liu, and K. Q. Weinberger. Densely Connected

Convolutional Networks. In CVPR, 2017.

[34] Z. Huang, X. Wang, J. Wang, W. Liu, and J. Wang.
Weakly-Supervised Semantic Segmentation Network with
Deep Seeded Region Growing. In CVPR, 2018.

[35] N. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa. Cross-
Domain Weakly-Supervised Object Detection through Pro-
gressive Domain Adaptation. In CVPR, 2018.

[36] Z. Jie, Y. Wei, X. Jin, J. Feng, and W. Liu. Deep Self-
Taught Learning for Weakly Supervised Object Localization.
In CVPR, 2017.

[37] Kaiming He, X. Zhang, S. Ren, and J. Sun. Deep Residual

Learning for Image Recognition. In CVPR, 2016.

[38] V. Kantorov, M. Oquab, M. Cho, and I. Laptev. Context-
LocNet: Context-Aware Deep Network Models for Weakly
Supervised Localization. In ECCV, 2016.

[39] A. Kolesnikov and C. H. Lampert. Seed, Expand and Con-
strain: Three Principles for Weakly-Supervised Image Seg-
mentation. In ECCV, 2016.

[19] A. Diba, V. Sharma, A. Pazandeh, H. Pirsiavash, and L. Van
Gool. Weakly Supervised Cascaded Convolutional Net-
works. In CVPR, 2017.

[40] D. Li, J.-B. Huang, Y. Li, S. Wang, and M.-H. Yang. Weakly
Supervised Object Localization with Progressive Domain
Adaptation. In CVPR, 2016.

705

[41] Y. Li, L. Liu, C. Shen, and A. van den Hengel.

Image
Co-localization by Mimicking a Good Detector’s Conﬁdence
Score Distribution. In ECCV, 2016.

[42] D. Lin, J. Dai, J. Jia, K. He, and J. Sun. ScribbleSup:
Scribble-Supervised Convolutional Networks for Semantic
Segmentation. In CVPR, 2016.

[43] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014.

[44] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia. Path Aggregation

Network for Instance Segmentation. In CVPR, 2018.

[45] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
Fu, and A. C. Berg. SSD: Single Shot MultiBox Detector. In
ECCV, 2016.

[46] J. Long, E. Shelhamer, and T. Darrell. Fully Convolutional

Networks for Semantic Segmentation. In CVPR, 2015.

[47] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri,
Y. Li, A. Bharambe, and L. van der Maaten. Exploring the
Limits of Weakly Supervised Pretraining. In ECCV, 2018.

[48] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V. Fer-
rari. We don’t need no bounding-boxes: Training object class
detectors using only human veriﬁcation. In CVPR, 2016.

[49] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille.
Weakly- and Semi-Supervised Learning of a DCNN for Se-
mantic Image Segmentation. In ICCV, 2015.

[50] D. Pathak, P. Kr¨ahenb¨uhl, and T. Darrell. Constrained Con-
volutional Neural Networks for Weakly Supervised Segmen-
tation. In ICCV, 2015.

[51] P. O. Pinheiro and R. Collobert. From Image-level to Pixel-
In CVPR,

level Labeling with Convolutional Networks.
2015.

[52] X. Qi, Z. Liu, J. Shi, H. Zhao, and J. Jia. Augmented feed-
back in semantic segmentation under image level supervi-
sion. In ECCV, 2016.

[53] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
Only Look Once: Uniﬁed, Real-Time Object Detection. In
CVPR, 2016.

[54] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN:
Towards Real-Time Object Detection with Region Proposal
Networks. In NIPS, 2015.

[55] F. Saleh, A. Akbarian, Mohammad, Sadegh, M. Salzmann,
L. Petersson, S. Gould, and J. M. Alvarez. Built-in Fore-
ground/Background Prior for Weakly-Supervised Semantic
Segmentation. In ECCV, 2016.

[56] T. Shen, G. Lin, C. Shen, and I. Reid. Bootstrapping the
Performance of Webly Supervised Semantic Segmentation.
In CVPR, 2018.

[57] Y. Shen, R. Ji, C. Wang, X. Li, and X. Li. Weakly Super-
vised Object Detection via Object-Speciﬁc Pixel Gradient.
TNNLS, 2018.

[58] Y. Shen, R. Ji, S. Zhang, W. Zuo, and Y. Wang. Genera-
tive Adversarial Learning Towards Fast Weakly Supervised
Detection. In CVPR, 2018.

[59] M. Shi, H. Caesar, and V. Ferrari. Weakly Supervised Ob-
ject Localization Using Things and Stuff Transfer. In ICCV,
2017.

[60] M. Shi and V. Ferrari. Weakly Supervised Object Localiza-

tion Using Size Estimates. In ECCV, 2016.

[61] W. Shimoda and K. Y. B. Distinct Class-Speciﬁc Saliency
In

Maps for Weakly Supervised Semantic Segmentation.
ECCV, 2016.

[62] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep Inside
Convolutional Networks: Visualising Image Classiﬁcation
Models and Saliency Maps. In ICLR, 2014.

[63] K. Simonyan and A. Zisserman. Very Deep Convolutional
In ICLR,

Networks for Large-Scale Image Recognition.
2015.

[64] K. K. Singh, F. Xiao, and Y. J. Lee. Track and Transfer:
Watching Videos to Simulate Strong Human Supervision for
Weakly-Supervised Object Detection. In CVPR, 2016.

[65] H. O. Song, Y. J. Lee, S. Jegelka, and T. Darrell. Weakly-
In

supervised Discovery of Visual Pattern Conﬁgurations.
NIPS, 2014.

[66] M. Tang, A. Djelouah, F. Perazzi, Y. Boykov, and
C. Schroers. Normalized Cut Loss for Weakly-supervised
CNN Segmentation. In CVPR, 2018.

[67] M. Tang, F. Perazzi, A. Djelouah, I. B. Ayed, C. Schroers,
On Regularized Losses for Weakly-

and Y. Boykov.
supervised CNN Segmentation. In ECCV, 2018.

[68] P. Tang, X. Wang, S. Bai, W. Shen, X. Bai, W. Liu, and
A. Yuille. PCL: Proposal Cluster Learning for Weakly Su-
pervised Object Detection. TPAMI, 2018.

[69] P. Tang, X. Wang, X. Bai, and W. Liu. Multiple Instance De-
tection Network with Online Instance Classiﬁer Reﬁnement.
In CVPR, 2017.

[70] P. Tang, X. Wang, A. Wang, Y. Yan, W. Liu, J. Huang, and
A. Yuille. Weakly Supervised Region Proposal Network and
Object Detection. In ECCV, 2018.

[71] P. Vernaza and M. Chandraker. Learning Random-Walk La-
bel Propagation for Weakly-Supervised Semantic Segmenta-
tion. In CVPR, 2017.

[72] F. Wan, P. Wei, J. Jiao, Z. Han, and Q. Ye. Min-Entropy
Latent Model for Weakly Supervised Object Detection. In
CVPR, 2018.

[73] C. Wang, W. Ren, K. Huang, and T. Tan. Weakly Super-
vised Object Localization with Latent Category Learning. In
ECCV, 2014.

[74] X. Wang, S. You, X. Li, and H. Ma. Weakly-Supervised Se-
mantic Segmentation by Iteratively Mining Common Object
Features. In CVPR, 2018.

[75] X. Wang, Z. Zhu, C. Yao, and X. Bai. Relaxed Multiple-
In

Instance SVM with Application to Object Discovery.
ICCV, 2015.

[76] Y. Wei, X. Liang, Y. Chen, X. Shen, M.-M. Cheng, J. Feng,
Y. Zhao, and S. Yan. STC: A Simple to Complex Framework
for Weakly-supervised Semantic Segmentation.
TPAMI,
2017.

[77] Y. Wei, Z. Shen, B. Cheng, H. Shi, J. Xiong, J. Feng, and
T. Huang. TS2C: Tight Box Mining with Surrounding Seg-
mentation Context for Weakly Supervised Object Detection.
In ECCV, 2018.

[78] Y. Wei, H. Xiao, H. Shi, Z. Jie, J. Feng, and T. S. Huang.
Revisiting Dilated Convolution: A Simple Approach for

706

Weakly- and Semi- Supervised Semantic Segmentation. In
CVPR, 2018.

[79] J. Xu, A. G. Schwing, and R. Urtasun. Learning to segment

under various forms of weak supervision. In CVPR, 2015.

[80] Z. Yan, J. Liang, W. Pan, J. Li, and C. Zhang. Weakly-
and Semi-Supervised Object Detection with Expectation-
Maximization Algorithm. ArXiv, 2017.

[81] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and
A. Agrawal. Context Encoding for Semantic Segmentation.
In CVPR, 2018.

[82] X. Zhang, J. Feng, H. Xiong, and Q. Tian. Zigzag Learning

for Weakly Supervised Object Detection. In CVPR, 2018.

[83] X. Zhang, Y. Wei, J. Feng, Y. Yang, and T. Huang. Adversar-
ial Complementary Learning for Weakly Supervised Object
Localization. In CVPR, 2018.

[84] Y. Zhang, Y. Li, and B. Ghanem. W2F : A Weakly-
Supervised to Fully-Supervised Framework for Object De-
tection. In CVPR, 2018.

[85] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba.
Learning Deep Features for Discriminative Localization. In
CVPR, 2016.

[86] Y. Zhou, Y. Zhu, Q. Ye, Q. Qiu, and J. Jiao. Weakly Super-
vised Instance Segmentation using Class Peak Response. In
CVPR, 2018.

[87] Y. Zhu, Y. Zhou, Q. Ye, Q. Qiu, and J. Jiao. Soft Proposal
In

Networks for Weakly Supervised Object Localization.
ICCV, 2017.

707

