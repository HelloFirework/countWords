Robustness via curvature regularization, and vice versa

Seyed-Mohsen Moosavi-Dezfooli∗†

seyed.moosavi@epfl.ch

Alhussein Fawzi∗‡
afawzi@google.com

Jonathan Uesato‡
juesato@google.com

Pascal Frossard†

pascal.frossard@epfl.ch

Abstract

State-of-the-art classiﬁers have been shown to be largely
vulnerable to adversarial perturbations. One of the most ef-
fective strategies to improve robustness is adversarial train-
ing. In this paper, we investigate the effect of adversarial
training on the geometry of the classiﬁcation landscape and
decision boundaries. We show in particular that adversar-
ial training leads to a signiﬁcant decrease in the curvature
of the loss surface with respect to inputs, leading to a dras-
tically more “linear” behaviour of the network. Using a
locally quadratic approximation, we provide theoretical ev-
idence on the existence of a strong relation between large
robustness and small curvature. To further show the impor-
tance of reduced curvature for improving the robustness, we
propose a new regularizer that directly minimizes curvature
of the loss surface, and leads to adversarial robustness that
is on par with adversarial training. Besides being a more
efﬁcient and principled alternative to adversarial training,
the proposed regularizer conﬁrms our claims on the impor-
tance of exhibiting quasi-linear behavior in the vicinity of
data points in order to achieve robustness.

1. Introduction

Adversarial training has recently been shown to be one
of the most successful methods for increasing the robust-
ness to adversarial perturbations of deep neural networks
[9, 17, 16]. This approach consists in training the classi-
ﬁer on perturbed samples, with the aim of achieving higher
robustness than a network trained on the original training
set. Despite the importance and popularity of this training
mechanism, the effect of adversarial training on the geo-
metric properties of the classiﬁer – its loss landscape with
respect to the input and decision boundaries – is not well un-

∗The ﬁrst two authors contributed equally to this work.
† ´Ecole Polytechnique F´ed´erale de Lausanne
‡DeepMind

derstood. In particular, how do the decision boundaries and
loss landscapes of adversarially trained models compare to
the ones trained on the original dataset?

In this paper, we analyze such properties and show that
one of the main effects of adversarial training is to induce
a signiﬁcant decrease in the curvature of the loss function
and decision boundaries of the classiﬁer. More than that, we
show that such a geometric implication of adversarial train-
ing allows us to explain the high robustness of adversarially
trained models. To support this claim, we follow a synthe-
sis approach, where a new regularization strategy, Curva-
ture Regularization (CURE), encouraging small curvature
is proposed and shown to achieve robustness levels that are
comparable to that of adversarial training. This highlights
the importance of small curvature for improved robustness.
In more detail, our contributions are summarized as follows:

• We empirically show that adversarial training induces
a signiﬁcant decrease in the curvature of the decision
boundary and loss landscape in the input space.

• Using a quadratic approximation of the loss function,
we establish upper and lower bounds on the robustness
to adversarial perturbations with respect to the curva-
ture of the loss. These bounds conﬁrm the existence of
a relation between low curvature and high robustness.
• Inspired by the implications of adversarially trained
networks on the curvature of the loss function and our
theoretical bounds, we propose an efﬁcient regularizer
that encourages small curvatures. On standard datasets
(CIFAR-10 and SVHN), we show that the proposed
regularizer leads to a signiﬁcant boost of the robust-
ness of neural networks, comparable to that of adver-
sarial training.

The latter step shows that the proposed regularizer can be
seen as a more efﬁcient alternative to adversarial training.
More importantly, it shows that the effect of adversarial
training on the curvature reduction is not a mere by-product,
but rather a driving effect that causes the robustness to in-
crease. We stress here that the main focus of this paper is

9078

mainly on the latter – analyzing the geometry of adversarial
training – rather than outperforming adversarial training.

Related works. The large vulnerability of classiﬁers to
adversarial perturbations has ﬁrst been highlighted in [3,
21]. Many algorithms aiming to improve the robustness
have since then been proposed [9, 20, 16, 4, 1]. In parallel,
there has been a large body of work on designing improved
attacks [17, 16], which have highlighted that many of the
proposed defenses obscure the model rather than make the
model truly robust against all attacks [24, 2]. One defense
however stands out – adversarial training – which has shown
to be empirically robust against all designed attacks. The
goal of this paper is to provide an analysis of this phe-
nomenon, and propose a regularization strategy (CURE),
which mimics the effect of adversarial training. On the
analysis front, many works have analyzed the existence of
adversarial examples, and proposed several hypotheses for
their existence [6, 8, 22, 5, 12]. In [9], it is hypothesized
that networks are not robust as they exhibit a “too linear”
behavior. We show here that linearity of the loss function
with respect to the inputs (that is, small curvature) is, on
the contrary, beneﬁcial for robustness: adversarial training
does lead to much more linear loss functions in the vicinity
of data points, and we verify that this linearity is indeed the
source of increased robustness. We ﬁnally note that prior
works have attempted to improve the robustness using gra-
dient regularization [10, 15, 19]. However, such methods
have not been shown to yield signiﬁcant robustness on com-
plex datasets, or have not been subject to extensive robust-
ness evaluation. Instead, our main focus here is to study the
effect of the second-order properties of the loss landscape,
and show the existence of a strong connection with robust-
ness to adversarial examples.

2. Geometric analysis of adversarial training

We start our analysis by inspecting the effect of adver-
sarial training on the geometric properties of the decision
boundaries of classiﬁers. To do so, we ﬁrst compare quali-
tatively the decision boundaries of classiﬁers with and with-
out adversarial training. Speciﬁcally, we examine the effect
of adversarial ﬁne-tuning, which consists in ﬁne-tuning a
trained network with a few extra epochs on adversarial ex-
amples.1 We consider the CIFAR-10 [14] and SVHN [18]
datasets, and use a ResNet-18 [11] architecture. For ﬁne-
tuning on adversarial examples, we use DeepFool [17].

Fig. 1 illustrates normal cross-sections of the decision
boundaries before and after adversarial ﬁne-tuning for clas-

1While adversarial ﬁne-tuning is distinct from vanilla adversarial train-
ing, which consists in training on adversarial images from scratch, we use
an adversarially ﬁne-tuned network in this paper as it allows to single out
the effect of training on adversarial examples, as opposed to other uncon-
trolled phenomenon happening in the course of vanilla adversarial training.

v

r

(a) Original (CIFAR-10)

(b) Finetuned (CIFAR-10)

(c) Original (SVHN)

(d) Fine-tuned (SVHN)

Figure 1: Random normal cross-sections of the decision
boundary for ResNet-18 classiﬁers trained on CIFAR-10
(ﬁrst row) and SVHN (second row). The ﬁrst column is
for classiﬁers trained on the original dataset, and the second
column shows the boundaries after adversarial ﬁne-tuning
on 20 epochs for CIFAR-10 and 10 epochs for SVHN. The
green and red regions represent the correct class and incor-
rect classes, respectively. The point at the center shows the
datapoint, while the lines represent the different decision
boundaries (note that the red regions can include different
incorrect classes).

siﬁers trained on CIFAR-10 and SVHN datasets. Specif-
ically, the classiﬁcation regions are shown in the plane
spanned by (r, v), where r is the normal to the decision
boundary and v corresponds to a random direction. In ad-
dition to inducing a larger distance between the data point
and the decision boundary (hence resulting in a higher ro-
bustness), observe that the decision regions of ﬁne-tuned
networks are ﬂatter and more regular.
In particular, note
that the curvature of the decision boundaries decreased af-
ter ﬁne-tuning.

To further quantify this phenomenon, we now compute
the curvature proﬁle of the loss function (with respect to
the inputs) before and after adversarial ﬁne-tuning. For-
mally, let ℓ denote the function that represents the loss of
the network with respect to the inputs; e.g., in the case of
cross-entropy, ℓ(x) = XEnt(fθ(x), y), where y is the true
label of image x ∈ Rd, and fθ(x) denotes the logits.2 The
curvature proﬁle corresponds to the set of eigenvalues of the
Hessian matrix

H =(cid:18) ∂2ℓ

∂xi∂xj(cid:19) ∈ Rd×d

2We omit the label y from ℓ for simplicity, as the label can be under-

stood from the context.

9079

(a) CIFAR-10

(b) SVHN

Figure 2: Curvature proﬁles, which correspond to sorted eigenvalues of the Hessian, of the original and the adversarially
ﬁne-tuned networks. Note that the number of eigenvalues is equal to 32 × 32 × 3 = 3072, which corresponds to the number
of input dimensions. The ResNet-18 architecture is used.

where xi, i = 1, . . . , d denote the input pixels. We stress on
the fact that the above Hessian is with respect to the inputs,
and not the weights of the network. To compute these eigen-
values in practice, we note that Hessian vector products are
given by the following for any z;

Hz =

∇ℓ(x + hz) − ∇ℓ(x)

h

for h → 0.

(1)

We then proceed to a ﬁnite difference approximation by
choosing a ﬁnite h in Eq. (1). Besides being more efﬁ-
cient than generating the full Hessian matrix (which would
be prohibitive for high-dimensional datasets), the ﬁnite dif-
ference approach has the beneﬁt of measuring larger-scale
variations of the gradient (where the scale is set using the
parameter h) in the neighborhood of the datapoint, rather
than an inﬁnitesimal point-wise curvature. This is crucial in
the setting of adversarial classiﬁcation, where we analyze
the loss function in a small neighbourhood of data points,
rather than the asymptotic regime h → 0 which might cap-
ture very local (and not relevant) variations of the function.3
Intuitively, small eigenvalues (in absolute value) of H
indicate a small curvature of the graph of ℓ around x, hence
implying that the classiﬁer has a “locally linear” behaviour
in the vicinity of x. In contrast, large eigenvalues (in abso-
lute value) imply a high curvature of the loss function in the
neighbourhood of image x. For example, in the case where
the eigenvalues are exactly zero, the function becomes lo-
cally linear, hence leading to a ﬂat decision surface.

We compute the curvature proﬁle at 100 random test
samples, and show the average curvature in Fig. 2 for
CIFAR-10 and SVHN datasets. Note that adversarial ﬁne-
tuning has led to a strong decrease in the curvature of the

3For example, using ReLU non-linearities result in a piecewise linear
neural network as a function of the inputs. This implies that the Hessian
computed at the logits is exactly 0. This result is however very local; using
the ﬁnite-difference approximation, we focus on larger-scale neighbour-
hoods.

Original
Fine-tuned

PGD(7)

FGSM ℓ∞-DF
38.0% 11.0%
0.5%
61.0% 57.5% 57.2%

PGD(20)

0.2%
56.9%

Table 1: Adversarial accuracies for original and ﬁne-tuned
network on CIFAR-10, where adversarial examples are
computed with different attacks; FGSM [9], DF [17] and
PGD [16]. Perturbations are constrained to have ℓ∞ norm
smaller than ǫ = 4 (images have pixel values in [0, 255]).

loss in the neighborhood of data points. To further illus-
trate qualitatively this signiﬁcant decrease in curvature due
to adversarial training, Fig. 3 shows the loss surface before
and after adversarial training along normal and random di-
rections r and v. Observe that while the original network
has large curvature in certain directions, the effect of adver-
sarial training is to “regularize” the surface, resulting in a
smoother, lower curvature (i.e., linear-like) loss.

We ﬁnally note that this effect of adversarial training on
the loss surface has the following somewhat paradoxical im-
plication: while adversarially trained models are more ro-
bust to adversarial perturbations (compared to original net-
works), they are also easier to fool, in the sense that simple
attacks are as effective as complex ones. This is in stark
contrast with original networks, where complex networks
involving many gradient steps (e.g., PGD(20)) are much
more effective than simple methods (e.g., FGSM). See Ta-
ble 1. The comparatively small gap between the adversar-
ial accuracies for different attacks on adversarially trained
models is a direct consequence of the signiﬁcant decrease of
the curvature of the loss, thereby requiring a small number
of gradient steps to ﬁnd adversarial perturbations.

9080

0-243210-130002500200015001000500OriginalAdversarialEigenvalue profile0-0.51.51.00.50.030002500200015001000500OriginalAdversarialEigenvalue profile(a) Original (CIFAR-10)

(b) Fine-tuned (CIFAR-10)

(c) Original (SVHN)

(d) Fine-tuned (SVHN)

Figure 3: Illustration of the negative of the loss function; i.e., −ℓ(s) for points s belonging to a plane spanned by a normal
direction r to the decision boundary, and random direction v. The original sample is illustrated with a blue dot. The light
blue part of the surface corresponds to low loss (i.e., corresponding to the classiﬁcation region of the sample), and the red
part corresponds to the high loss (i.e., adversarial region).

3. Analysis of the inﬂuence of curvature on ro-

bustness

While our results show that adversarial training leads to
a decrease in the curvature of the loss, the relation between
adversarial robustness and curvature of the loss remains un-
clear. To elucidate this relation, we consider a simple bi-
nary classiﬁcation setting between class 1 and −1. Recall
that ℓ(·, 1) denotes the function that represents the loss of
the network with respect to an input from class 1. For ex-
ample, in the setting where the log-loss is considered, we
have ℓ(x, 1) = − log(p(x)), where p(x) denotes the out-
put of softmax corresponding to class 1. In that setting, x
is classiﬁed as class 1 iff ℓ(x, 1) ≤ log(2). For simplicity,
we assume in our analysis that x belongs to class 1 with-
out loss of generality, and hence omit the second argument
in ℓ in the rest of this section. We assume that the func-
tion ℓ can be locally well approximated using a quadratic
function; that is, for “sufﬁciently small” r, we can write:

ℓ(x + r) ≈ ℓ(x) + ∇ℓ(x)T r +

rT Hr,

1
2

where ∇ℓ(x) and H denote respectively the gradient and
Hessian of ℓ at x. Let x be a point classiﬁed as class 1;
i.e., ℓ(x) ≤ t, where t denotes the loss threshold (e.g., t =
log(2) for the log loss). For this datapoint x, we then deﬁne
r∗ to be the minimal perturbation in the ℓ2 sense4, which
fools the classiﬁer assuming the quadratic approximation
holds; that is,

r∗ := arg min

r

krk s.t. ℓ(x) + ∇ℓ(x)T r +

1
2

rT Hr ≥ t.

In the following result, we provide upper and lower bounds
on the magnitude of r∗ with respect to properties of the loss
function at x.

4We use the ℓ2 norm for simplicity. Using the equivalence of norms in
ﬁnite dimensional spaces, our result allows us to also bound the magnitude
of ℓ∞ adversarial perturbations.

Figure 4: Illustration of upper and lower bounds in Eq. (2)
and (3) on the robustness with respect to curvature ν. We
have set k∇ℓ(x)k = 1, c = 1, ∇ℓ(x)T v = 0.5 in this ex-
ample.

Theorem 1. Let x be such that c := t − ℓ(x) ≥ 0, and let
g = ∇ℓ(x). Assume that ν := λmax(H) ≥ 0, and let u be
the eigenvector corresponding to ν. Then, we have

kgk

ν  s1 +

2νc

kgk2 − 1! ≤ kr∗k

(2)

≤

|gT u|

ν  s1 +

2νc

(gT u)2 − 1!

(3)

The above bounds can further be simpliﬁed to:

c

kgk

− 2ν

c2
kgk3 ≤ kr∗k ≤

c

|gT u|

The proof can be found in the supplemental material.
Remark 1.

Increasing robustness with decreasing
curvature. Note that upper and lower bounds on the robust-

9081

024680.20.40.60.811.21.41.61.82Value of boundsLower boundUpper boundFigure 5: Geometric illustration in 1d of the effect of curva-
ture on the adversarial robustness. Different loss functions
(with varying curvatures) are illustrated at the vicinity of
data point x0, and x(i)
adv indicate the points at which such
losses exceed t (where t is the misclassiﬁcation threshold).
All curves have the same loss and gradient at x0. Note that
increasing curvature leads to smaller adversarial examples
(i.e., smaller |x0 − x(i)

adv|).

ness in Eq. (2), (3) decrease with increasing curvature ν. To
see this, Fig. 4 illustrates the dependence of the bounds on
the curvature ν.
In other words, under the second order
approximation, this shows that small curvature (i.e., small
eigenvalues of the Hessian) is beneﬁcial to obtain classiﬁers
with higher robustness (when the other parameters are kept
ﬁxed). This is in line with our observations from Section 2,
where robust models are observed to have a smaller curva-
ture than networks trained on original data. Fig. 5 provides
intuition to the decreasing robustness with increasing cur-
vature in a one-dimensional example.

Remark 2. Dependence on the gradient. In addition to
the dependence on the curvature ν, note that the upper and
lower bounds depend on the gradient ∇ℓ(x). In particular,
these bounds decrease with the norm k∇ℓ(x)k (for a ﬁxed
direction). Hence, under the second order approximation,
this suggests that the robustness decreases with larger gra-
dients. However, as previously noted in [24, 2], imposing
small gradients might provide a false sense of robustness.
That is, while having small gradients can make it hard for
gradient-based methods to attack the network, the network
can still be intrinsically vulnerable to small perturbations.

Remark 3. Bound tightness. Note that the upper and
lower bounds match (and hence bounds are exact) when
the gradient ∇ℓ(x) is collinear to the largest eigenvector
u. Interestingly, this condition seems to be approximately
satisﬁed in practice, as the average normalized inner prod-
uct |∇ℓ(x)T u|
for CIFAR-10 is equal to 0.43 before adver-
k∇ℓ(x)k2
sarial ﬁne-tuning, and 0.90 after ﬁne-tuning (average over
1000 test points). This inner product is signiﬁcantly larger
than the inner product between two typical vectors uni-
formly sampled from the sphere, which is approximately
1√d
≈ 0.02. Hence, the gradient aligns well with the di-

rection of largest curvature of the loss function in practice,
which leads to approximately tight bounds.

4. Improving robustness through curvature

regularization

While adversarial training leads to a regularity of the loss
in the vicinity of data points, it remains unclear whether this
regularity is the main effect of adversarial training, which
confers robustness to the network, or it is rather a byproduct
of a more sophisticated phenomenon. To answer this ques-
tion, we follow here a synthesis approach, where we derive
a regularizer which mimics the effect of adversarial training
on the loss function – encouraging small curvatures.

Curvature regularization (CURE) method. Recall that
H denotes the Hessian of the loss ℓ at datapoint x. We
denote by λ1, . . . , λd the eigenvalues of H. Our aim is to
penalize large eigenvalues of H; we therefore consider a

regularizer Lr =Pi p(λi), where p is a non-negative func-

tion, which we set to be p(t) = t2 to encourage all eigen-
values to be small. For this choice of p, Lr corresponds to
the Frobenius norm of the matrix H. We further note that

p(λi) = trace(p(H)) = E(zT p(H)z) = EkHzk2,

Lr =Xi

where the expectation is taken over z ∼ N (0, Id). By using
a ﬁnite difference approximation of the Hessian, we have
Hz ≈ ∇ℓ(x+hz)−∇ℓ(x)
, where h denotes the discretization
step, and controls the scale on which we require the varia-
tion of the gradients to be small. Hence, Lr becomes

h

Lr =

1
h2

E k∇ℓ(x + hz) − ∇ℓ(x)k2 .

The above regularizer involves computing an expectation
over z ∼ N (0, Id), and penalizes large curvatures along
all directions equally. Rather than approximating the above
with an empirical expectation of kHzk2 over isotropic di-
rections drawn from N (0, Id), we instead select directions
which are known to lead to high curvature (e.g., [13, 7]), and
minimize the curvature along such chosen directions. The
latter approach is more efﬁcient, as the computation of each
matrix-vector product Hz involves one backward pass; fo-
cusing on high-curvature directions is therefore essential to
minimize the overall curvature without having to go through
each single direction in the input space. This selective ap-
proach is all the more adapted to the very sparse nature of
curvature proﬁles we see in practice (see Fig. 2), where only
a few eigenvalues are large. This provides further motiva-
tion for identifying large curvature directions and penaliz-
ing the curvature along such directions.

Prior works in [7, 13] have identiﬁed gradient directions
In addition, empirical evi-

as high curvature directions.

9082

x0(x0)x(1)advx(2)advx(3)advx(4)advt=log(2)Table 2: Adversarial and clean accuracy for CIFAR-10 for original, regularized and adversarially trained models. Perfor-
mance is reported for ResNet and WideResNet models, and the perturbations are computed using PGD(20). Perturbations
are constrained to have ℓ∞ norm less than ǫ = 8 (where pixel values are in [0, 255]).

ResNet-18

WideResNet-28×10

Clean Adversarial

Clean Adversarial

Normal training
CURE
Adversarial training [16]

94.9%
81.2%
79.4%

0.0%
36.3%
43.7%

94.6%
83.1%
87.3%

0.0%
41.4%
45.8%

dence reported in Section 3 (Remark 3) shows a large in-
ner product between the eigenvector corresponding to max-
imum eigenvalue and the gradient direction; this provides
further indication that the gradient is pointing in high cur-
vature directions, and is therefore a suitable candidate for
z. We set in practice z = sign(∇ℓ(x))
, and ﬁnally consider
ksign(∇ℓ(x))k
the regualizer 5

Lr = k∇ℓ(x + hz) − ∇ℓ(x)k2,

where the 1
h2 is absorbed by the regularization parameter.
Our ﬁne-tuning procedure then corresponds to minimizing
the regularized loss function ℓ + γLr with respect to the
weight parameters, where γ controls the weight of the reg-
ularization relative to the loss term.

We stress that the proposed regularization approach sig-
niﬁcantly departs from adversarial training. In particular,
while adversarial training consists in minimizing the loss
on perturbed points (which involves solving an optimiza-
tion problem), our approach here consists in imposing reg-
ularity of the gradients on a sufﬁciently small scale (i.e.,
determined by h). Previous works [16] have shown that ad-
versarial training using a weak attack (such as FGSM [9],
which involves a single gradient step) does not improve the
robustness. We show that our approach, which rather im-
poses gradient regularity (i.e., small curvature) along such
directions, does lead to a signiﬁcant improvement in the ro-
bustness of the network.

We use two pre-trained networks, ResNet-18 [11]
and WResNet-28x10 [25], on the CIFAR-10 and SVHN
datasets, where the pixel values are in [0, 255]. For the op-
timization of the regularized objective, we use the Adam
optimizer with a decreasing learning rate between [10−4,
10−6] for a duration of 20 epochs starting from a pre-trained
network. We linearly increase the value of h from 0 to 1.5
during the ﬁrst 5 epochs, and from there on, we use a ﬁxed
value of h = 1.5. For γ, we set it to 4 and 8 for ResNet-18
and WResNet-28 respectively.

5The choice of z ∝ ∇ℓ(x) leads to almost identical results. We have
chosen to set z ∝ sign(∇ℓ(x)), as we are testing the robustness of the
classiﬁer to ℓ∞ perturbations. Hence, setting z be the sign of the gradient

Figure 6: Adversarial accuracy versus perturbation mag-
nitude ǫ computed using PGD(20), for ResNet-18 and
WResNet-28x10 trained with CURE on CIFAR-10. See
[16] for the curve corresponding to adversarial training.
Curve generated for 2000 random test points.

Results. We evaluate the regularized networks with a
strong PGD attack of 20 iterations, as it has been shown
to outperform other adversarial attack algorithms [16]. The
adversarial accuracies of the regularized networks are re-
ported in Table 2 for CIFAR-10, and in the supp. material
for SVHN. Moreover, the adversarial accuracy as a function
of the perturbation magnitude ǫ is reported in Fig. 6.

Observe that, while networks trained on the original
dataset are not robust to perturbations as expected, perform-
ing 20 epochs of ﬁne-tuning with the proposed regularizer
leads to a signiﬁcant boost in adversarial performance. In
particular, the performance with the proposed regularizer is
comparable to that of adversarial training reported in [16].
This result hence shows the importance of the curvature de-
crease phenomenon described in this paper in explaining the
success of adversarial training.

In addition to verifying our claim that small curvature
confers robustness to the network (and that it is the underly-
ing effect in adversarial training), we note that the proposed
regularizer has practical value, as it is efﬁcient to compute

is more relevant, as it constrains the z direction to belong to the hypercube
of interest.

9083

00.40.50.60.70.887654321ResNet-18WResNet-28x10Adversarial accuracyFigure 7: Analysis of gradient masking in a network trained
with CURE. Adversarial loss computed with SPSA (y-axis)
vs. adversarial loss with PGD(100) (x-axis) on a batch of
1000 datapoints. Adversarial loss corresponds to the differ-
ence of logits on true and adversarial class. Each point in
the scatter plot corresponds to a single test sample. Nega-
tive loss indicates that the data point is misclassiﬁed. Points
close to the line y = x indicate that both attacks iden-
tiﬁed similar adversarial perturbations. Points below the
line, shown in red, indicate points for which SPSA iden-
tiﬁed stronger adversarial perturbation than PGD. Note that
overall, SPSA and PGD identiﬁed similarly perturbations.

(a) ResNet-18

(b) WideResNet-28

Figure 8: Similar plot to Fig. 3, but where the loss surfaces
of the network obtained with CURE are shown.

and can therefore be used as an alternative to adversarial
training. In fact, the proposed regularizer requires 2 back-
ward passes to compute, and is used in ﬁne-tuning for 20
epochs. In contrast, one needs to run adversarial training
against a strong adversary in order to reach good robust-
ness [16], and start the adversarial training procedure from
scratch. We note that strong adversaries generally require
around 10 backward passes, making the proposed regular-
ization scheme a more efﬁcient alternative. We note how-
ever that the obtained results are slightly worse than adver-
sarial training; we hypothesize that this might be either due

Figure 9: Curvature proﬁle for a network ﬁne-tuned using
adversarial training and CURE. The ResNet-18 architecture
on CIFAR-10 is used. For comparison, we also report the
proﬁle for the original network (same as Fig. 2), where we
clipped the values to ﬁt in the y range.

to higher order effects in adversarial training not captured
with our second order analysis or potentially due to a sub-
optimal choice of hyper-parameters γ and h.

Stronger attacks and verifying the absence of gradient
masking. To provide further evidence on the robustness
of the network ﬁne-tuned with CURE, we attempt to ﬁnd
perturbations for the network with more complex attack al-
gorithms. For the WideResNet-28x10, we obtain an ad-
dversarial accuracy of 41.1% on the test set when using
PGD(40) and PGD(100). This is only slightly worse than
the result reported in Table 2 with PGD(20). This shows
that increasing the complexity of the attack does not lead to
a signiﬁcant decrease in the adversarial accuracy. Moreover,
we evaluate the model against a gradient-free optimization
method (SPSA), similar to the methodology used in [24],
and obtained an adversarial accuracy of 44.5%. We com-
pare moreover in Fig. 7 the adversarial loss (which rep-
resents the difference between the logit scores of the true
and adversarial class) computed using SPSA and PGD for a
batch of test data points. Observe that both methods lead to
comparable adversarial loss (except on a few data points),
hence further justifying that CURE truly improves the ro-
bustness, as opposed to masking or obfuscating gradients.
Hence, just like adversarial training which was shown em-
pirically to lead to networks that are robust to all tested at-
tacks in [24, 2], our experiments show that the regularized
network has similar robustness properties.

Curvature and robustness. We now analyze the network
obtained using CURE ﬁne-tuning, and show that the ob-
tained network has similar geometric properties to the ad-
versarially trained one. Fig. 8 shows the loss surface in a

9084

-6-6-4-20246-4-20Adv. loss using PGDSPSA vs PGD comparison Adv. loss using SPSA246-0.040.060.040.020-0.02030002500200015001000500Adversarial trainingCUREEigenvalue profileOriginalFigure 10: Evolution throughout the course of CURE ﬁne-tuning for a ResNet-18 on CIFAR-10. The curves are averaged
over 1000 datapoints. Left: estimate of Frobenius norm , Middle: kHzk, where z = sign(∇ℓ(x))/ksign(∇ℓ(x))k2 and
Right: adversarial accuracy computed using PGD(20). The Frobenius norm is estimated with kHk2
z∼N (0,I)kHzk2,
where the expectation is approximated with an empirical expectation over 100 samples zi ∼ N (0, I).

F = E

e
g
a
m

i
 
l

i

a
n
g
i
r

O

E
R
U
C

i

g
n
n
a
r
t
 
l

i

a
m
r
o
N

Figure 11: Visualizations of perturbed images and pertur-
bations on SVHN for the ResNet-18 classiﬁer.

plane spanned by (r, v), where r and v denote respectively
a normal to the decision boundary and a random direction.
Note that the loss surface obtained with CURE is quali-
tatively very similar to the one obtained with adversarial
training (Fig. 3), whereby the loss has a more linear behav-
ior in the vicinity of the data point. Quantitatively, Fig. 9
compares the curvature proﬁles for the networks trained
with CURE and adversarial ﬁne-tuning. Observe that both
proﬁles are very similar. We also report the evolution of
the adversarial accuracy and curvature quantities in Fig. 10
during ﬁne-tuning with CURE. Note that throughout the
ﬁne-tuning process, the curvature decreases while the ad-
versarial accuracy increases, which further shows the link
between robustness and curvature. Note also that, while we
explicitly regularized for kHzk (where z is a ﬁxed direction
for each data point) as a proxy for kHkF , the network does
show that the intended target kHkF decreases in the course
of training, hence further suggesting that kHzk acts as an
efﬁcient proxy of the global curvature.

Qualitative evaluation of adversarial perturbations.
We ﬁnally illustrate some adversarial examples in Fig. 11
for networks trained on SVHN. Observe that the network
trained with CURE exhibits visually meaningful adversar-
ial examples, as perturbed images do resemble images from
the adversary class. A similar observation for adversarially
trained models has been made in [23].

5. Conclusion

Guided by the analysis of the geometry of adversarial
training, we have provided empirical and theoretical evi-
dence showing the existence of a strong correlation between
small curvature and robustness. To validate our analysis, we
proposed a new regularizer (CURE), which directly encour-
ages small curvatures (in other words, promotes local lin-
earity). This regularizer is shown to signiﬁcantly improve
the robustness of deep networks and even achieve perfor-
mance that is comparable to adversarial training. In light of
prior works attributing the vulnerability of classiﬁers to the
“linearity of deep networks”, this result is somewhat sur-
prising, as it shows that one needs to decrease the curvature
(and not increase it) to improve the robustness. In addition
to validating the importance of controlling the curvature for
improving the robustness, the proposed regularizer also pro-
vides an efﬁcient alternative to adversarial training. In fu-
ture work, we plan to leverage the proposed regularizer to
train provably robust networks.

Acknowledgements

A.F. would like to thank Neil Rabinowitz and Avraham
Ruderman for the fruitful discussions. S.M and P.F would
like to thank the Google Faculty Research Award, and the
Hasler Foundation, Switzerland, in the framework of the
ROBERT project.

9085

Epochs051015Epochs051015Epochs0510151.02.03.04.05.06.07.00.00.10.20.30.40.50.60.00.050.10.150.20.250.30.35HFAdversarial accuracyHz2In IEEE International Conference on Data Mining (ICDM),
2015.

[16] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. In International
Conference on Learning Representations (ICLR), 2018.

[17] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and
Pascal Frossard. Deepfool: a simple and accurate method
to fool deep neural networks. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2016.

[18] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural
images with unsupervised feature learning. In NIPS work-
shop on deep learning and unsupervised feature learning,
2011.

[19] Andrew Slavin Ross and Finale Doshi-Velez. Improving the
adversarial robustness and interpretability of deep neural net-
works by regularizing their input gradients. In AAAI, 2018.

[20] Uri Shaham, Yutaro Yamada, and Sahand Negahban. Un-
derstanding adversarial training: Increasing local stability
of neural nets through robust optimization. arXiv preprint
arXiv:1511.05432, 2015.

[21] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I.
Goodfellow, and R. Fergus. Intriguing properties of neural
networks. In International Conference on Learning Repre-
sentations (ICLR), 2014.

[22] Thomas Tanay and Lewis Grifﬁn. A boundary tilting
persepective on the phenomenon of adversarial examples.
arXiv preprint arXiv:1608.07690, 2016.

[23] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom,
Alexander Turner, and Aleksander Madry. Robustness may
be at odds with accuracy. arXiv preprint arXiv:1805.12152,
2018.

[24] Jonathan Uesato, Brendan O’Donoghue, Aaron van den
Oord, and Pushmeet Kohli. Adversarial risk and the dangers
of evaluating against weak attacks. In International Confer-
ence on Machine Learning (ICML), 2018.

[25] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-

works. arXiv preprint arXiv:1605.07146, 2016.

References

[1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin
Murphy. Deep variational information bottleneck. In Inter-
national Conference on Learning Representations (ICLR),
2017.

[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfus-
cated gradients give a false sense of security: Circumventing
defenses to adversarial examples. In International Confer-
ence on Machine Learning (ICML), 2018.

[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nel-
son, Nedim ˇSrndi´c, Pavel Laskov, Giorgio Giacinto, and
Fabio Roli. Evasion attacks against machine learning at
test time. In Joint European Conference on Machine Learn-
ing and Knowledge Discovery in Databases, pages 387–402,
2013.

[4] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann
Dauphin, and Nicolas Usunier. Parseval networks: Improv-
ing robustness to adversarial examples. In International Con-
ference on Machine Learning (ICML), 2017.

[5] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adver-
sarial vulnerability for any classiﬁer. In Neural Information
Processing Systems (NIPS), 2018.

[6] A. Fawzi, O. Fawzi, and P. Frossard. Analysis of classi-
ﬁers’ robustness to adversarial perturbations. arXiv preprint
arXiv:1502.02590, 2015.

[7] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal
Frossard, and Stefano Soatto. Empirical study of the topol-
ogy and geometry of deep networks. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018.

[8] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoen-
holz, Maithra Raghu, Martin Wattenberg, and Ian Goodfel-
low. Adversarial spheres.
In International Conference on
Learning Representations (ICLR), 2018.

[9] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples.
In Inter-
national Conference on Learning Representations (ICLR),
2015.

[10] S. Gu and L. Rigazio. Towards deep neural network ar-
chitectures robust to adversarial examples. arXiv preprint
arXiv:1412.5068, 2014.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.

[12] Matthias Hein and Maksym Andriushchenko. Formal guar-
antees on the robustness of a classiﬁer against adversarial
manipulation. In Advances in Neural Information Process-
ing Systems, pages 2263–2273, 2017.

[13] Saumya Jetley, Nicholas Lord, and Philip Torr. With friends
In Neural Information

like these, who needs adversaries?
Processing Systems (NIPS), 2018.

[14] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Master’s thesis, Department of
Computer Science, University of Toronto, 2009.

[15] Chunchuan Lyu, Kaizhu Huang, and Hai-Ning Liang. A uni-
ﬁed gradient regularization family for adversarial examples.

9086

