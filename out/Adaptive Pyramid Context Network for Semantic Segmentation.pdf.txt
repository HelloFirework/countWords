Adaptive Pyramid Context Network for Semantic Segmentation

Junjun He 1

2

,

Zhongying Deng 1

Lei Zhou 1

Yali Wang 1

Yu Qiao∗ 1

3

,

1 Shenzhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab,

Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China

2 Shanghai Jiao Tong University 3 The Chinese University of Hong Kong

Abstract

Recent studies witnessed that context features can signif-
icantly improve the performance of deep semantic segmen-
tation networks. Current context based segmentation meth-
ods differ with each other in how to construct context fea-
tures and perform differently in practice. This paper ﬁrstly
introduces three desirable properties of context features in
segmentation task. Specially, we ﬁnd that Global-guided
Local Afﬁnity (GLA) can play a vital role in constructing ef-
fective context features, while this property has been largely
ignored in previous works. Based on this analysis, this pa-
per proposes Adaptive Pyramid Context Network (APCNet)
for semantic segmentation. APCNet adaptively constructs
multi-scale contextual representations with multiple well-
designed Adaptive Context Modules (ACMs). Speciﬁcally,
each ACM leverages a global image representation as a
guidance to estimate the local afﬁnity coefﬁcients for each
sub-region, and then calculates a context vector with these
afﬁnities. We empirically evaluate our APCNet on three se-
mantic segmentation and scene parsing datasets, including
PASCAL VOC 2012, Pascal-Context, and ADE20K dataset.
Experimental results show that APCNet achieves state-of-
the-art performance on all three benchmarks, and obtains
a new record 84.2% on PASCAL VOC 2012 test set without
MS COCO pre-trained and any post-processing.

1. Introduction

Semantic segmentation, aiming at assigning a category
label for each pixel, is a fundamental yet important prob-
lem in computer vision, with wide applications in scene un-
derstanding, medical imaging, robot vision etc [27] [28].
The challenge of semantic segmentation comes from the
inner content, shape, and scale variations of the same ob-
ject/stuff, as well as the easily confused and ﬁne bound-
aries among different objects/stuff. Current state-of-the-art

∗Yu Qiao is the corresponding author. The author emails are hejun-

jun@sjtu.edu.cn, {zy.deng1, lei.zhou, yl.wang, yu.qiao}@siat.ac.cn.

semantic segmentation methods heavily exploit deep con-
volutional neural networks (CNNs), e.g. Fully Convolu-
tional Network (FCN) [22], U-Net [28], to extract dense se-
mantic representations from input images and predict pixel-
level labels. When trained properly, deep CNNs can capture
rich scene information with multilayer convolutional opera-
tions and nonlinear pooling/activation functions. Due to the
convolutional nature of CNN, however, local convolutional
features usually have limited receptive ﬁelds. Moreover,
even with a large receptive ﬁeld, these features mainly de-
scribe the core region and largely ignore the context around
boundary [23]. On the other hand, local regions from differ-
ent category may share near features, e.g. wood table and
chair may exhibit similar local textures. The precise seman-
tic segmentation always requires context information from
different scales and large regions to release the ambiguity
caused by local regions.

To address this problem, a number of recent works
[4, 40, 20, 10, 13] aggregate context vector to local con-
volutional feature to boost the segmentation performance.
These methods differ with each other in the way to con-
struct context vector, and perform differently on different
datasets. So there is a natural question, what are the optimal
contexts for semantic segmentation. This paper tries to ad-
dress this question by investigating the desirable properties
that the optimal context vector should exhibit. In principle,
the optimal context vector should describe segmentation-
relevant image contents which are complementary to the lo-
cal features, meanwhile, this vector should be compact with
irrelevant information as less as possible. Speciﬁcally, we
summarize three key properties as follows.

Property 1-Multi-scale. For semantic segmentation,
holistic objects/stuff regions yield important cues to deter-
mine the sematic labels of local pixels. Since objects usu-
ally have different sizes and positions, it is necessary to con-
struct multi-scale representations to capture image contents
from different scales. As shown in the ﬁrst row of Figure 1,
method without multi-scale contexts can only capture ob-
jects in single scale and lose details in other scales.

Property 2-Adaptive. Not all areas in input image con-

7519

Figure 1. Illustration of Multi-scale and Global-guided Local Afﬁnity properties. The ﬁrst row: Multi-scale context can capture objects in
different scales. The second row: Global-guided Local Afﬁnity is beneﬁt to segment complete and consist object.

tribute equally to determine the sematic label of a given
pixel. Areas which contain related objects can yield use-
ful information, while others may contribute very few. In
practice, the relevant regions/pixels may exist near around
the given pixel and also can be far away from it, highly de-
pend on the contents and layout of input images. Therefore,
it is important to adaptively identify these important regions
for constructing optimal context vectors.

Property 3-Global-guided Local Afﬁnity (GLA). To
construct effective context vector, one need to aggregate
the features from related pixels or regions.
In practice,
this can be implemented by summarizing their features in a
weighted way. So there is a problem of estimating the afﬁn-
ity weights for aggregation. These weights indicate how dif-
ferent areas contribute to predict the semantic label of a lo-
cal pixel. Previous works [20, 10, 13] mainly estimate these
adaptive weights with local representations of pixels and re-
gions, ignoring the global context. Unlike these works, here
our insight is that both local and global representations are
necessary to estimate robust afﬁnity weights. As shown in
the second row of Figure 1, the legs of horse are small and
exhibit similar texture with snow which belongs to back-
ground class and dominates the whole scene.
It is prone
to classify the legs to background class. Clearly segmen-
tation task can beneﬁt from global representation. We call
this property as Global-guided Local Afﬁnity (GLA), as the
local afﬁnity weights are guided with global representation.

In the next, we make comparison of current context
based semantic segmentation methods from the perspective
of the properties mentioned above. DeepLab [4], ParseNet
[20], and PSPNet [40] utilize ASPP (atrous spatial pyra-
mid pooling), GAP (global average pooling), and PPM
(pyramid pooling module) to obtain context at different
scales, respectively. All these context vectors, however,
only describe contents at ﬁxed locations and are NOT adap-

tive. More recently, DANet [10] encodes global context
with well designed self-attention mechanism. PSANet [13]
learns an adaptive pixel-wise position sensitive spatial at-
tention mask for aggregating contextual features. OCNet
[37] embeds self-attention mechanism to PPM and ASPP
to exploit multi-scale property. But these methods ignore
the Global-guided Local Afﬁnity property discussed in the
above.

As summarized in Table 1, the previous methods can
only account for some of the three properties. Partly in-
spired by this fact,
this paper proposes Adaptive Pyra-
mid Context Network (APCNet) for sematic segmentation,
which effectively constructs the contextual representations
with all the three properties. Speciﬁcally, APCNet designs
pyramid Adaptive Context Modules to capture multi-scale
global representations. The main contributions are as fol-
lows.

Method

MS Adaptive GLA

DeepLab[4]
PSPNet [20]
ParseNet [20]
PSANet [13]
DANet [10]
OCNet [37]
Ours

X

X

X

X

X

X

X

X

X

Table 1. Comparison of different deep context based semantic seg-
mentation methods. MS: multi-scale, GLA: global-guided local
afﬁnity.

• We summarize three desirable properties of context
vectors for semantic segmentation, and compare re-
cent deep context based semantic segmentation meth-
ods from the perspective of these properties.

7520

• We propose Adaptive Context Modules which exploit
GLA property by leveraging local and global repre-
sentation to estimate afﬁnity weights for local regions.
These afﬁnities further allow us to construct adaptive
and multi-scale contextual representations for segmen-
tation task.

• Our method achieves state-of-the-art performance on
three widely used benchmarks,
including PASCAL
VOC 2012, Pascal-Context and ADE20K dataset, and
obtains a new record 84.2% on PASCAL VOC 2012
test set without MS COCO pre-trained and any post-
processing.

2. Related Work

Recently, FCN [22] based approaches have achieved
promising performance on scene parsing and semantic seg-
mentation task, through encoding contextual information.
But most approaches only consider some properties as men-
tioned in table 1.
Multi-scale context. Multi-scale context plays a key role
in semantic segmentation, especially for objects/stuff with
vast variation of scales. Image Pyramid is a common way
to obtain multi-scale context. [9] uses Laplacian pyramid to
scale the input image of a DCNN [14] and merge the fea-
ture maps. SegNet [2], UNet[28], and [5] design Encoder-
Decoder architecture to fuse low-level and high-level fea-
ture map from encoder and decoder, respectively. PSPNet
[40] and DeepLab [4] propose PPM (pyramid pooling mod-
ule) and ASPP (atrous spatial Pyramid pooling) module to
encoding multi-scale context, respectively. These two mod-
ules are effective and efﬁciency to some extent, but they
treat all images regions equally, not in an adaptive way.
Global context. Global context is particularly import for
comprehensive complex scene understanding. ParseNet
[20] proposes a simple but effective method to encoding
global context through GAP (global average pooling) for
semantic segmentation. PSPNet [40] exploits pyramid re-
gion based context aggregation to construct global context
utilizing PPM. These methods cannot encode global context
adaptively for every speciﬁc pixel. DANet [10] and OC-
Net [37] adopt self-attention to capture long-range global
context, which calculate pixel-wise similarity map based on
the pairs semantic features. While PSANet [13] aggregates
global context through learning a pixel-wise position sensi-
tive spatial attention mask to guide information ﬂow. Cal-
culated pixel-wise similarity map and learned pixel-wise at-
tention map are adaptive to every speciﬁc pixel, but these
pairs pixel relations which obtained by calculating pixel-
wise similarity or convolving on a speciﬁc pixel position
are lack of global information. While our method learns
relations guided by local and global information.

Different from all previous work, our proposed method

can generate more powerful multi-scale and global contexts
through aggregating multi-scale features with learned adap-
tive afﬁnities guided by local and global information.

3. Method

Context information is essentially important for com-
plex scene parsing and semantic segmentation. Global con-
text is useful to capture long-range dependency and provide
a comprehensive understanding of the whole scene, while
segmentation of objects with different sizes can beneﬁt from
multi-scale contextual features. In the next, we describe the
proposed Adaptive Pyramid Context Network which adap-
tively constructs multi-scale context vectors with the guid-
ance of global image representation.

3.1. Formulation

To begin with, we describe the mathematical formulation
of our problem as follows. Given an image I for segmen-
tation, we calculate a dense 3D convolution feature cube X
with a backbone CNN, where Xi denotes the convolutional
feature vector at position i. And xi denotes the reduced
convolutional feature vector at position i for efﬁcient com-
putation. The segmentation task can be reduced to predict
a semantic label of a pixel, take i for example. One direct
idea toward this problem is to estimate the semantic label
just with the local feature Xi. However, this idea ignores
the relevant contents in other areas and limit the segmenta-
tion performance. To address this problem, context features
have been successfully exploited to boost segmentation per-
formance in previous works [4, 40, 20, 10, 13]. Mathemati-
cally, we introduce zi = Fcontext(X, i) to denote the context
feature vector for Xi, where Fcontext represents the function
to extract zi from input feature cube at position i. Previous
context segmentation methods differ with each other in how
to deﬁne Fcontext.

As discussed in the Section 1, this paper aims to design a
novel context which satisﬁed the three properties, 1) Multi-
scale, 2) Adaptive, and 3) Global-guided Local Afﬁnity. To-
ward this objective, we ﬁrst transform X into multi-scale
pyramid representations. Then we adaptively construct con-
text vectors for each scale separately. Here we just take one
scale s as an example and the other scales can be processed
in a similar way. For this scale, we divide the feature map
X of image I into s × s subregions, thus transform X into a
set of subregion representations, Ys = [Ys
s×s],
according to this division. For each subregion Ys
j , we sum-
marize its contents with a feature vector ys
j by average pool-
ing and one convolution operation. We introduce afﬁnity
coefﬁcient αs
i,j to denote the degree of how subregion Ys
j
contributes to estimate the sematic label of Xi. Then, the

2, ..., Ys

1, Ys

7521

Figure 2. The pipeline of Adaptive Pyramid Context Network (APCNet). The input image is fed into a backbone CNN to obtain convolu-
tional feature cube X. X is decomposed into multi-scale pyramid representation. The representation of each scale is feed into Adaptive
Context Module (ACM) to estimate adaptive context vectors for each local position. APCNet consists of multiple ACMs organized in par-
allel. Each ACM consists of two branches with one branch to estimate GLA afﬁnity coefﬁcients and the other branch to obtain subregion
representations. The output of these two branches are multiplied to obtain adaptive context vectors. Finally, APCNet concatenates context
vectors from different scales and the original feature cube X for predicting the semantic labels of the input pixels

adaptive context vector can be calculated as,

zs
i =

s×s

X

j=1

ij ys
αs
j ,

(1)

Here the key problem is how to calculate coefﬁcient αs
i,j .
Ideally, αs
i,j should satisfy the GLA property by account-
ing for both local feature from xi and global representa-
tion from X given scale s and position j. Let g(X) denote
the global information representation vector of X and g is
a global information extractor. In this paper, we calculate
αs

i,j = fs(xi, g(X), j). Then Eq. 1 evolves to

zs
i =

s×s

X

j=1

fs(xi, g(X), j)ys
j .

(2)

The above Eq. 2 plays a key role in our design of Adap-

tive Pyramid Context Network.

3.2. Adaptive Context Module

The Adaptive Context Module (ACM) is a key compo-
nent in our Adaptive Pyramid Context Network. In princi-
ple, ACM aims to calculate a context vector for each local

position by leveraging Global-guided Local Afﬁnity. Essen-
tially, ACM implements Eq. 2 with the network architecture
shown in Fig. 2. ACM consists of two branches. The ﬁrst
branch aims to calculate afﬁnity coefﬁcients αs while the
second approach processes single-scale representation ys.
Details are given in the below.

In the ﬁrst branch, we ﬁrst process X with a 1 × 1 con-
volution to get the reduced feature map x, and then obtain
global information representation vector g(X) by applying
spatial global average pooling and one 1 × 1 convolutional
transform on x.
In the next, we integrate both local fea-
tures {xi} and global vector g(X) to calculate an Global-
guided Local Afﬁnity vector for each local positions i. This
is implemented with a 1 × 1 convolution followed by a sig-
moid activation function in our design. One may argue to
exploit large spatial convolution. But this leads to poor per-
formance in experiments, partly due to the complexity of
large ﬁlters. Each afﬁnity vector has a dimension s × s, cor-
responding the number of subregions in this scale. Totally,
we have h ∗ w afﬁnity vectors, which can be reshaped to
an afﬁnity map with size hw × ss. The second branch ap-
plies adaptive average pooling and a 1×1 convolution on X
to obtain ys ∈ Rs×s×512 . Then we reshape ys into size of

7522

s2 ×512 to match that of the afﬁnity map. Then we multiply
them together and reshape the results to obtain the adaptive
context matrix zs composed of {zs
i }. Residual learning is
adopted to ease the training process, and thus we add x to
zs.

3.3. Adaptive Pyramid Context Network

Next, we will describe the proposed Adaptive Pyra-
mid Context Network (APCNet) for semantic segmentation,
whose architecture is shown in Figure 2. APCNet takes a
backbone CNN e.g. ResNet or InceptionNet to calculate
a convolutional feature cube X ∈ Rh×w×c, where h, w, c
represent width, height and channel number respectively.
Then APCNet transforms X into pyramid representations
with S scales in total. Speciﬁcally, for each scale s, we
adopt adaptive average pooling and one 1 × 1 convolution
to transform X to a speciﬁc spatial size s × s and obtain
ys ∈ Rs×s×c. Then each ys together with original X
is processed with an Adaptive Context Module (ACM) to
obtain an adaptive context vector zs
i for each spatial posi-
tion. Totally, APCNet includes multiple ACMs organized
in parallel. In the next, we can concatenate {zs
i } obtained
from different scales into the ﬁnal adaptive context vector
zi = [z1
i ]. Finally, we exploit both local features
{Xi} and their associate context vectors {zi} to predict the
semantic label of each pixel.

i , ..., zS

i , z2

3.4. Relation to Other Approaches

i,j = 1, S=1, and ys

In this subsection, we make comparison between our
Adaptive Pyramid Context Network and other context ap-
proaches for semantic segmentation. ParseNet [20] aggre-
gates global context through global average pooling, which
can be seen as an extreme case of our model if we just set
αs
j = g(X). In PSPNet [40], αs are set
as a ﬁxed bilinear interpolation coefﬁcients for ys. In con-
trast our APCNet estimates αs in an adaptive way with Eq.
2. Recent methods PSANet [13], DANet [10], OCNet [37]
also alleviate this problem by introducing adaptive weights.
These methods calculate pair-wise similarity or learn pixel-
wise attention map. But they all neglect the importance
of global guidance from g(X). Unlike these works, our
APCNet not only takes Global-guided Local Afﬁnity into
account with fs to estimate αs from both local and global
representations, but also exploits multi-scale representation
with feature pyramid.

4. Experiments

We conduct extensive experiments on three challenging
semantic segmentation and scene parsing datasets to eval-
uate our proposed method, including PASCAL VOC 2012
[7], Pascal-Context [24], and ADE20K dataset [42].

4.1. Implementation Details

We adopt ResNet [12] as our backbone which is pre-
trained on ImageNet [29]. Following [36, 4, 38], we re-
move stride and set dilation rate 2 and 4 to the last two
stages of backbone networks respectively, and the output
feature map is 1/8 size of the input image [4, 38, 35]. The
output predictions are bilinear interpolated to target size
for predicting semantic labels of each pixel. We use poly
learning rate policy lr = initial lr × (1 − iter
total iter )power
[4, 5, 38]. The initial learning rate is 0.01 for PASCAL
VOC 2012 [7] and ADE20K dataset [42], 0.001 for Pascal-
Context dataset [7], and the power is 0.9 [38]. Stochastic
gradient descent (SGD) [3] with momentum 0.9 and weight
decay 0.0001 is chosen as optimizer. We train networks for
80 epochs on PASCAL VOC 2012 [7] and Pascal-Context
dataset [24], and 120 epochs on ADE20K dataset [42]. In
practice, appropriately larger crop size can obtain better per-
formance, so we set crop size to 512 on PASCAL VOC
2012 and Pascal-Context dataset, and 576 on ADE20K as
the average image size of ADE20K dataset is larger than
other two datasets [4, 40, 38]. We randomly ﬂip and scale
the input image from 0.5 to 2 as our data augmentation.
Our evaluation metric is mean of class-wise intersection
over union (mIoU). For multi-scale and ﬂip evaluation, we
resize the input image to multiple scales and horizontally
ﬂip them. The predictions are averaged as ﬁnal predictions
[20, 40, 30, 34]. All experiments are implemented based on
PyTorch [26].

4.2. PASCAL VOC 2012

PASCAL VOC 2012 [7] is a benchmark dataset of se-
mantic segmentation, which originally contains 1,464 im-
ages for training, 1,449 for validation, and 1,456 for test.
Totally, there are 20 foreground object classes and one back-
ground class in the original PASCAL VOC 2012 dataset
[7]. The original dataset is augmented to 10,582 images
for training by [11]. Following [4, 38, 5], we use this aug-
mented training set in our experiments.

We conduct experiments with different settings to evalu-
ate the effectiveness of our proposed modules. Our baseline
is dilated ResNet based FCN [4, 22] as mentioned above.

Pyramid scales. ResNet50-based FCN [22] with di-
lated network is adopted as our baseline. We investigate
the performance of APCNet with different setting of pyra-
mid scales (PS). Results are listed in Table 2. From Table
2, we have the following observations. Firstly, compared
with baseline FCN (1st row), all pyramid scales settings
improve the performance signiﬁcantly. Secondly, pyramid
scales of {1,2,3,6} achieve the best result, which improves
the performance of baseline FCN by 8.37% (from 69.83%
to 78.20%). We can infer that properly designed pyramid
scales can help to effectively capture the features of objects
with varied scale. In all the following experiments, we will

7523

adopt pyramid scales of {1,2,3,6}. Finally, deeper back-
bone network, e.g. ResNet101, can further improve the re-
sult.

Backbone

ResNet50
ResNet50
ResNet50
ResNet50
ResNet50
ResNet50
ResNet101

PS

mIoU%

None
{1}
{1,2}
{1,2,3}
{1,2,3,6}

{1,2,3,6,32}

{1,2,3,6}

69.83
77.89
77.48
77.60
78.20
77.29
80.71

Table 2. Investigation of different pyramid scales and backbones.
Baseline is ResNet50-based FCN with dilated network (PS in
none). PS: pyramid scales, {1
32}: bin sizes of pooled
feature, 1 × 1, 2 × 2, 3 × 3, 6 × 6, 32 × 32. The results are evalu-
ated on the validation set of PASCAL VOC 2012 dataset, with the
single-scale input.

2

3

6

,

,

,

,

Figure 3 shows the visualization results of our APCNet
and baseline model FCN. It is obvious that APCNet keeps
more details (1st row) due to its pyramid scale. And it also
introduces less mislabeled pixel (2nd and 3rd row), which
leads to better performance than FCN.

Figure 3. Comparison with baseline method.

To further illustrate the effectiveness of pyramid scales,
we visualize the improvement of different scales in Figure
4.
It can be observed from the ﬁgure that APCNet with
single scale is inferior to multi-scale APCNet since single
scale APCNet can hardly segment the objects with large
scale variations. More speciﬁcally, in the ﬁrst row of Figure
4, APCNet with single scale lacks detailed information of
the boat and fails to segment the person on the boat. With
multi-scale setup, APCNet not only preserves most detailed
information of the boat but also correctly segments the per-
son.
Global-guided Local Afﬁnity (GLA). We conduct exper-
iments w/o GLA with different backbones, to validate the

essential importance of GLA in our APCNet. Table 3 lists
the performance of different backbones w/o GLA on the
validation set of PASCAL VOC 2012 dataset. It is obvious
that GLA consistently increases the performance of differ-
ent backbones.

Backbone GLA mIoU%

ResNet50
ResNet50
ResNet101
ResNet101

X

X

77.68
78.20
80.17
80.71

Table 3. Investigation on the importance of GLA with different
backbone networks, and PS is {1,2,3,6}. GLA: Global-guided
Local Afﬁnity. The results are evaluated on the validation set of
PASCAL VOC 2012 dataset, with the single-scale input.

Also, we visualize the segmentation results to show the
improvement of GLA in Figure 5. The 1st row shows that
APCNet with GLA can lead to more accurate segmentation
(for the dog near the person). The 2nd and 3rd show that
APCNet with GLA can alleviate the problem of segment-
ing an object into different classes. This veriﬁes that global
information introduced by GLA can help better understand-
ing of complex context and more consistent segmentation
of a certain object.
Training and evaluation strategies. The results of differ-
ent training and evaluation strategy are shown in Table 4.
We can observe that 1) deep supervision can optimize the
learning process and further improve the performance, 2)
scaling the input image to multiple scales and ﬂipping the
images left-right for evaluation are useful, 3) ﬁne-tuning the
trained model with original training set boosts the result to
82.67% mIoU on PASCAL VOC 2012 validation set, with-
out MS COCO pre-trained.

Backbone DS Flip MS FT mIoU%

ResNet101
ResNet101 X
ResNet101 X
ResNet101 X
ResNet101 X

X

X

X

X

X

X

80.71
80.93
81.33
81.93
82.67

Table 4. Inﬂuence of different setting in training and evaluation
strategies, and PS is {1,2,3,6}. DS: deep supervised [40], Flip:
horizontally ﬂip input image for evaluation, MS: multi-scale eval-
uation, FT: ﬁne tune the trained model on PASCAL VOC 2012
original training set. The results are evaluated on the validation set
of PASCAL VOC 2012 dataset.

Adaptive. Our proposed model can be reduced as PSP-
Net if removing adaptive and GLA modules. So we reim-
plemented PSPNet with our experimental settings (add
deep supervised) as our baseline with backbone ResNet101
which gets 79.79% mIoU on PASCAL VOC validation set
(single scale). With adaptive and GLA modules, the perfor-
mance is improved clearly as shown in Table 5.

7524

Figure 4. Visualization of segmentation results of single scale and multi-scale.

most all categories of PASCAL VOC 2012. Note that APC-
Net can distinguish categories that look very similar, e.g.
cow (93.7%) and horse (95%). This may owe to the GLA
properties of our methods which take both global and lo-
cal information into consideration. Without pre-trained on
MS COCO dataset [16], APCNet achieves state-of-the-art
performance of 84.2% mIoU, which demonstrates the ef-
fectiveness of our proposed method. With MS COCO pre-
trained, our proposed method also achieves the best perfor-
mance of 87.13% mIoU among the methods based on back-
bone ResNet101.

Figure 5.
Global-guided Local Afﬁnity (GLA).

Visualization of segmentation results with/without

4.3. Pascal Context

Adaptive GLA mIoU (%)

X

X

X

80.19
80.93

Table 5. The improved performance based on PSPNet with adap-
tive and GLA module. PSPNet gets 79.79% mIoU. The results are
evaluated on the validation set of PASCAL VOC 2012 dataset.

For evaluation on PASCAL VOC 2012 [19] test set, we
set pyramid scales to {1,2,3,6} and adopt deep supervised
strategy [40] to train the backbone model on augmented
training set. The backbone model is ResNet101 pre-trained
on ImageNet [29]. Then, we ﬁne tune the trained model on
original training and validation set. After training, multi-
scale and ﬂip are adopted for testing. Final results are sub-
mitted to ofﬁcial server for evaluation and the comparison
to the state-of-the-art methods is shown in Table 6. Clearly,
our APCNet signiﬁcantly outperforms other methods on al-

Pascal-Context dataset [24] is annotated additionally for
PASCAL VOC 2010 [8] with whole scene label. Follow-
ing [38, 17], we train our model on the training set of 4,998
images and evaluate on the test set of 5,105 images, and re-
port our result on 60 classes including 59 foreground classes
and one background class. Table 7 compares the perfor-
mance of the state-of-the-art methods. With the same back-
bone model, our APCNet surpasses DeepLab-v2 [4], Enc-
Net [38], and DANet [10] in a large margin. Moreover,
our APCNet achieves the state-of-the-art performance on
Pascal-Context dataset and thus demonstrates its effective-
ness for semantic segmentation.

4.4. ADE20K

ADE20K dataset [42] is a challenge scene parsing
dataset providing 150 classes dense labels, which consists
of 20K/2K/3K images for training, validation and test, re-
spectively. Due to the diverse and complex scene in this
dataset, it is hard to achieve subtle improvements. Results

7525

Method

aero

bike

bird

boat

bottle

bus

car

cat

chair

cow table

dog

horse mbike

person

plant

sheep

sofa

train

tv mIoU%

76.8
84.4
87.5
89.9
87.7
90.6
94.4
91.8
94.1
95.8

FCN [22]
DeepLabv2 [4]
CRF-RNN [41]
DeconvNet [25]
DPN [21]
Piecewise [18]
ResNet38 [32]
PSPNet [40]
EncNet [38]
Ours
Table 6. Per-class results on PASCAL VOC 2012 test set. Our method outperforms all previous state-of-art methods and achieves 84.2%
without MS COCO dataset pre-trained.

49.4
63.6
64.2
63.9
64.9
67.8
68.8
71.2
76.7
76.0

73.9
80.8
80.6
80.2
82.3
84.8
89.1
89.6
87.9
89.3

60.3
65.9
68.3
68.2
70.3
74.4
78.4
75.8
86.2
80.6

74.7
79.1
80.8
81.2
83.5
85.2
90.0
89.9
90.7
90.0

37.4
50.4
47.8
54.3
53.4
58.2
61.3
64.0
59.0
61.9

68.9
81.5
79.7
79.7
78.4
80.0
94.9
94.7
96.3
84.5

55.1
63.7
67.1
65
65
72.3
78.1
76.3
73.4
79.6

70.9
73.1
78.3
80.7
77.9
80.8
87.7
85.1
86.4
88.9

72.4
82.2
82.8
83.4
83.2
83.2
90.7
89.6
92.6
92.8

45.2
59.7
59.5
58.8
60.5
62.1
71.3
72.8
68.7
75.8

75.3
85.1
87.6
87.4
89.3
92
90.6
95.2
96.3
96.9

77.6
83.4
84.4
86.1
86.1
86.2
92.1
95.9
94.2
96.0

21.4
30.7
30.4
28.5
31.7
39.1
40.1
39.3
38.8
42.0

62.5
74.1
78.2
77.0
79.9
81.2
90.4
90.7
90.7
93.7

46.8
59.8
60.4
62.0
62.6
58.9
71.7
71.7
73.3
75.4

71.8
79
80.5
79.0
81.9
83.8
89.9
90.5
90.0
91.6

76.5
83.2
83.1
83.6
83.5
84.3
91.0
88.8
88.8
90.5

63.9
76.1
77.8
80.3
80.0
83.9
93.7
94.5
92.5
95.0

62.2
71.6
72.0
72.5
74.1
75.3
82.5
82.6
82.9
84.2

34.2
54.5
39.0
39.3
59.4
37.6
72.9
71.9
69.2
75.8

Method

Backbone

mIoU%

Method

Backbone mIoU%

37.8
FCN-8S [22]
39.3
CRF-RNN [41]
40.4
ParseNet [20]
40.5
BoxSup [6]
41.3
HO CRF [1]
43.3
Piecewise [18]
VeryDeep [31]
44.5
DeepLab-v2 [4] ResNet101-COCO 45.7
47.3
ReﬁneNet [17]
MSCI [16]
50.3
51.7
EncNet [38]
52.6
DANet [10]
54.7
Ours

ResNet152
ResNet152
ResNet101
ResNet101
ResNet101

Table 7. Segmentation results on PASCAL-Context dataset of 60
classes with background. Our method outperforms all previous
state-of-art methods with a large margin.

on ADE20K validation set of different methods are summa-
rized in Table 8. Our result outperforms other state-of-the-
art results, even with a shallower backbone networks. We
also submit the test set segmentation results of our method
to ofﬁcial evaluation server. The pixel accuracy is 72.94%,
mIoU is 38.39%, and score is 55.67%, which ranks top on
the leaderboard.

4.5. Summary

Comparing to ParseNet [20] and PSPNet [40], our
method achieves better results on PASCAL VOC 2012,
Pascal-Context and ADE20K dataset. These results demon-
strate the APCNet to adaptively aggregate multi-scale con-
text with the guidance of global representation. Different
from PSANet [13], OCNet [37] and DANet [10] which con-
struct semantic context by calculating semantic correlation
on every pair pixels or convolving on a speciﬁc pixel, our
global-guided local afﬁnity is more reasonable and leads to
higher performance.

5. Conclusion

In this paper, we discuss the properties of context fea-
tures, and propose APCNet to adaptively construct multi-
scale context representation for semantic segmentation and

FCN [22]
SegNet [2]
DilatedNet [35]
CascadeNet [42]
ReﬁneNet [17]
PSPNet [40]
PSPNet [40]
EncNet [38]
SAC [39]
PSANet [13]
UperNet [33]
DSSPN [15]
OCNet [37]
Ours

ResNet152
ResNet101
ResNet269
ResNet101
ResNet101
ResNet101
ResNet101
ResNet101
ResNet101
ResNet101

29.39
21.64
32.31
34.90
40.7
43.29
44.94
44.65
44.30
43.77
42.66
43.68
45.08
45.38

Table 8. Segmentation results on ADE20K validation set. Our
method outperforms all previous methods.

scene parsing. APCNet introduces Adaptive Context Mod-
ules which generate local afﬁnity coefﬁcients with our elab-
orately designed Global-guided Local Afﬁnity. Extensive
experiments show that APCNet can capture different scales
objects, and the predictions of objects are more completely
and consistently. APCNet can not only be embedded to any
FCN based semantic segmentation networks, but also any
layer of the networks which independent of the input feature
map size. APCNet may extend to other scene understanding
tasks, according to the properties and ﬂexibility.

Acknowledgements.

This work is partially sup-
ported by National Natural Science Foundation of
China
Shen-
zhen Research
Program (JCYJ20150925163005055,
CXB201104220032A), the Joint Lab of CAS-HK.

(61876176, U1613211, U1713208),

References

[1] Anurag Arnab, Sadeep Jayasumana, Shuai Zheng, and
Philip HS Torr. Higher order conditional random ﬁelds in
deep neural networks. In European Conference on Computer
Vision, pages 524–540. Springer, 2016.

[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
Segnet: A deep convolutional encoder-decoder architecture

7526

for image segmentation. arXiv preprint arXiv:1511.00561,
2015.

[3] L´eon Bottou. Large-scale machine learning with stochastic
gradient descent. In Proceedings of COMPSTAT’2010, pages
177–186. Springer, 2010.

[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence, 40(4):834–848, 2018.

[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-
rian Schroff, and Hartwig Adam. Encoder-decoder with
atrous separable convolution for semantic image segmenta-
tion. arXiv preprint arXiv:1802.02611, 2018.

[6] Jifeng Dai, Kaiming He, and Jian Sun. Boxsup: Exploit-
ing bounding boxes to supervise convolutional networks for
semantic segmentation.
In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 1635–1643,
2015.

[7] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge.
International journal of computer
vision, 88(2):303–338, 2010.

[8] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge.
International journal of computer
vision, 88(2):303–338, 2010.

[9] Clement Farabet, Camille Couprie, Laurent Najman, and
Yann LeCun. Learning hierarchical features for scene la-
beling. IEEE transactions on pattern analysis and machine
intelligence, 35(8):1915–1929, 2013.

[10] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing
Lu. Dual attention network for scene segmentation. arXiv
preprint arXiv:1809.02983, 2018.

[11] Bharath Hariharan, Pablo Arbel´aez, Ross Girshick, and Ji-
tendra Malik. Hypercolumns for object segmentation and
ﬁne-grained localization. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
447–456, 2015.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

[13] Jiaya Jia. Psanet: Point-wise spatial attention network for

scene parsing. 2018.

[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works.
In Advances in neural information processing sys-
tems, pages 1097–1105, 2012.

[15] Xiaodan Liang, Hongfei Zhou, and Eric Xing. Dynamic-
structured semantic propagation network.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 752–761, 2018.

[16] Di Lin, Yuanfeng Ji, Dani Lischinski, Daniel Cohen-Or, and
Hui Huang. Multi-scale context intertwining for semantic
segmentation. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 603–619, 2018.

[17] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian D
Reid. Reﬁnenet: Multi-path reﬁnement networks for high-
resolution semantic segmentation.
In Cvpr, volume 1,
page 5, 2017.

[18] Guosheng Lin, Chunhua Shen, Anton Van Den Hengel, and
Ian Reid. Efﬁcient piecewise training of deep structured
models for semantic segmentation.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3194–3203, 2016.

[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, 2014.

[20] Wei Liu, Andrew Rabinovich, and Alexander C Berg.
arXiv preprint

Parsenet: Looking wider to see better.
arXiv:1506.04579, 2015.

[21] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen-Change Loy, and
Xiaoou Tang. Semantic image segmentation via deep pars-
ing network. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 1377–1385, 2015.

[22] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 3431–3440, 2015.

[23] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel.
Understanding the effective receptive ﬁeld in deep convolu-
tional neural networks. In Advances in neural information
processing systems, pages 4898–4906, 2016.

[24] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and
Alan Yuille. The role of context for object detection and
semantic segmentation in the wild.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 891–898, 2014.

[25] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
Learning deconvolution network for semantic segmentation.
In Proceedings of the IEEE international conference on com-
puter vision, pages 1520–1528, 2015.

[26] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.

[27] Mengye Ren and Richard S Zemel. End-to-end instance
segmentation with recurrent attention.
In Proceedings of
the 2017 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), Honolulu, HI, USA, pages 21–26,
2017.

[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention, pages 234–241.
Springer, 2015.

[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211–252, 2015.

7527

[30] Gabriel Schwartz and Ko Nishino. Material recognition
arXiv preprint

from local appearance in global context.
arXiv:1611.09394, 2016.

[31] Zifeng Wu, Chunhua Shen, and Anton van den Hengel.
Bridging category-level and instance-level semantic image
segmentation. arXiv preprint arXiv:1605.06885, 2016.

[32] Zifeng Wu, Chunhua Shen, and Anton van den Hengel.
Wider or deeper: Revisiting the resnet model for visual
recognition. arXiv preprint arXiv:1611.10080, 2016.

[33] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Uniﬁed perceptual parsing for scene understand-
ing. arXiv preprint arXiv:1807.10221, 2018.

[34] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,
Gang Yu, and Nong Sang. Learning a discriminative fea-
ture network for semantic segmentation.
arXiv preprint
arXiv:1804.09337, 2018.

[35] Fisher Yu and Vladlen Koltun.

aggregation by dilated convolutions.
arXiv:1511.07122, 2015.

Multi-scale context
arXiv preprint

[36] Fisher Yu, Vladlen Koltun, and Thomas A Funkhouser. Di-

lated residual networks. In CVPR, volume 2, page 3, 2017.

[37] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-
work for scene parsing. arXiv preprint arXiv:1809.00916,
2018.

[38] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang,
Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Con-
text encoding for semantic segmentation.
In The IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018.

[39] Rui Zhang, Sheng Tang, Yongdong Zhang, Jintao Li, and
Shuicheng Yan. Scale-adaptive convolutions for scene pars-
ing. In Proc. 26th Int. Conf. Comput. Vis., pages 2031–2039,
2017.

[40] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network.
In
IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), pages 2881–2890, 2017.

[41] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang
Huang, and Philip HS Torr. Conditional random ﬁelds as
recurrent neural networks. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1529–1537,
2015.

[42] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba.
Scene parsing through
ade20k dataset. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, volume 1, page 4.
IEEE, 2017.

7528

